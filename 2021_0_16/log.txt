program start:
num_rounds= 0
node_emb_dim= 16

----------------------------------------new_epoch--------------------------------------

epoch:  0
iteration : 0
train acc:  0.4375
train loss:  0.7247984409332275
train gradient:  0.28243267551996154
iteration : 1
train acc:  0.453125
train loss:  0.7161509990692139
train gradient:  0.40705113583333963
iteration : 2
train acc:  0.578125
train loss:  0.6950598359107971
train gradient:  0.3648140884462181
iteration : 3
train acc:  0.46875
train loss:  0.7148504257202148
train gradient:  0.21753040822207642
iteration : 4
train acc:  0.546875
train loss:  0.6881036162376404
train gradient:  0.3259768920246272
iteration : 5
train acc:  0.5
train loss:  0.7098019123077393
train gradient:  0.35390854164520913
iteration : 6
train acc:  0.5390625
train loss:  0.6937357783317566
train gradient:  0.22818660767663218
iteration : 7
train acc:  0.609375
train loss:  0.6630963087081909
train gradient:  0.22734982850822932
iteration : 8
train acc:  0.609375
train loss:  0.666748046875
train gradient:  0.2296036629691588
iteration : 9
train acc:  0.5859375
train loss:  0.6623626947402954
train gradient:  0.23277001366148248
iteration : 10
train acc:  0.5546875
train loss:  0.6977825164794922
train gradient:  0.200556942101255
iteration : 11
train acc:  0.640625
train loss:  0.6376546621322632
train gradient:  0.23435235566908935
iteration : 12
train acc:  0.65625
train loss:  0.644885778427124
train gradient:  0.21803074705569653
iteration : 13
train acc:  0.6015625
train loss:  0.6734896898269653
train gradient:  0.20762903386447285
iteration : 14
train acc:  0.6015625
train loss:  0.6612135767936707
train gradient:  0.1910635116040476
iteration : 15
train acc:  0.546875
train loss:  0.7191142439842224
train gradient:  0.24069520657490462
iteration : 16
train acc:  0.546875
train loss:  0.6632802486419678
train gradient:  0.23142070459788097
iteration : 17
train acc:  0.5859375
train loss:  0.7040553092956543
train gradient:  0.26628372924577637
iteration : 18
train acc:  0.6640625
train loss:  0.6143291592597961
train gradient:  0.2795957748617461
iteration : 19
train acc:  0.6171875
train loss:  0.6565871238708496
train gradient:  0.20815626103695944
iteration : 20
train acc:  0.59375
train loss:  0.691726803779602
train gradient:  0.21332313657612798
iteration : 21
train acc:  0.6171875
train loss:  0.6750951409339905
train gradient:  0.1717658232063343
iteration : 22
train acc:  0.640625
train loss:  0.6562932729721069
train gradient:  0.3859052311090461
iteration : 23
train acc:  0.59375
train loss:  0.658384382724762
train gradient:  0.2177320243149711
iteration : 24
train acc:  0.6171875
train loss:  0.669259250164032
train gradient:  0.2944752439416355
iteration : 25
train acc:  0.6328125
train loss:  0.6503024101257324
train gradient:  0.2579969584800845
iteration : 26
train acc:  0.5703125
train loss:  0.6636791229248047
train gradient:  0.2585306619824927
iteration : 27
train acc:  0.6015625
train loss:  0.6526056528091431
train gradient:  0.18038653795211834
iteration : 28
train acc:  0.578125
train loss:  0.6703683137893677
train gradient:  0.25366354684582537
iteration : 29
train acc:  0.5625
train loss:  0.6776795387268066
train gradient:  0.34020478857025294
iteration : 30
train acc:  0.6328125
train loss:  0.6586843729019165
train gradient:  0.2561066684929428
iteration : 31
train acc:  0.6171875
train loss:  0.652255654335022
train gradient:  0.23805153665122977
iteration : 32
train acc:  0.625
train loss:  0.6718993782997131
train gradient:  0.16308760227079938
iteration : 33
train acc:  0.6171875
train loss:  0.640061616897583
train gradient:  0.2985071036405935
iteration : 34
train acc:  0.59375
train loss:  0.6791152954101562
train gradient:  0.24400097364371648
iteration : 35
train acc:  0.6171875
train loss:  0.6590313911437988
train gradient:  0.19492212702610687
iteration : 36
train acc:  0.6171875
train loss:  0.6725665926933289
train gradient:  0.17274055441459166
iteration : 37
train acc:  0.5703125
train loss:  0.6467679738998413
train gradient:  0.15815998570181045
iteration : 38
train acc:  0.59375
train loss:  0.6270138025283813
train gradient:  0.222832188732036
iteration : 39
train acc:  0.609375
train loss:  0.6712734699249268
train gradient:  0.16912023162578171
iteration : 40
train acc:  0.609375
train loss:  0.6483159065246582
train gradient:  0.1781610862285195
iteration : 41
train acc:  0.5546875
train loss:  0.6606468558311462
train gradient:  0.18279320334435648
iteration : 42
train acc:  0.7109375
train loss:  0.5976949334144592
train gradient:  0.18019540996237687
iteration : 43
train acc:  0.6328125
train loss:  0.6339874863624573
train gradient:  0.15878431953790695
iteration : 44
train acc:  0.6484375
train loss:  0.6214045882225037
train gradient:  0.15394767546151655
iteration : 45
train acc:  0.65625
train loss:  0.6523531675338745
train gradient:  0.27397921288621346
iteration : 46
train acc:  0.6953125
train loss:  0.6117298007011414
train gradient:  0.2559076170039515
iteration : 47
train acc:  0.625
train loss:  0.6278664469718933
train gradient:  0.17605016756342168
iteration : 48
train acc:  0.6796875
train loss:  0.6143165826797485
train gradient:  0.17414498287379204
iteration : 49
train acc:  0.6171875
train loss:  0.6669167280197144
train gradient:  0.15441705914327808
iteration : 50
train acc:  0.6640625
train loss:  0.6026870012283325
train gradient:  0.19315769581908307
iteration : 51
train acc:  0.625
train loss:  0.6696920394897461
train gradient:  0.20021502426439952
iteration : 52
train acc:  0.6953125
train loss:  0.5964681506156921
train gradient:  0.14463642404193577
iteration : 53
train acc:  0.5546875
train loss:  0.6687169075012207
train gradient:  0.24551298734706506
iteration : 54
train acc:  0.5390625
train loss:  0.7042347192764282
train gradient:  0.20832153011089388
iteration : 55
train acc:  0.5859375
train loss:  0.6965275406837463
train gradient:  0.18458420679086263
iteration : 56
train acc:  0.65625
train loss:  0.648015558719635
train gradient:  0.19690152560946356
iteration : 57
train acc:  0.5546875
train loss:  0.6964085698127747
train gradient:  0.31141936508895846
iteration : 58
train acc:  0.5859375
train loss:  0.6792068481445312
train gradient:  0.20830923328659232
iteration : 59
train acc:  0.515625
train loss:  0.710864245891571
train gradient:  0.2553817312153855
iteration : 60
train acc:  0.6328125
train loss:  0.6246802806854248
train gradient:  0.15865547622632792
iteration : 61
train acc:  0.6328125
train loss:  0.6668635010719299
train gradient:  0.21112165407062913
iteration : 62
train acc:  0.65625
train loss:  0.6299442648887634
train gradient:  0.15389224462003076
iteration : 63
train acc:  0.640625
train loss:  0.615776538848877
train gradient:  0.2566356505824936
iteration : 64
train acc:  0.609375
train loss:  0.642353355884552
train gradient:  0.17136209896574245
iteration : 65
train acc:  0.671875
train loss:  0.6365693807601929
train gradient:  0.14258889417071174
iteration : 66
train acc:  0.6640625
train loss:  0.6094741821289062
train gradient:  0.1462402194956748
iteration : 67
train acc:  0.6484375
train loss:  0.6334037780761719
train gradient:  0.22281194119137582
iteration : 68
train acc:  0.703125
train loss:  0.5883910655975342
train gradient:  0.1873106955605257
iteration : 69
train acc:  0.671875
train loss:  0.6308939456939697
train gradient:  0.17212799631326955
iteration : 70
train acc:  0.65625
train loss:  0.6349974274635315
train gradient:  0.16137343913334326
iteration : 71
train acc:  0.578125
train loss:  0.6685182452201843
train gradient:  0.14244208954276566
iteration : 72
train acc:  0.578125
train loss:  0.6302490234375
train gradient:  0.2635858085788578
iteration : 73
train acc:  0.671875
train loss:  0.6217502355575562
train gradient:  0.1761375048350493
iteration : 74
train acc:  0.5390625
train loss:  0.6856042146682739
train gradient:  0.1634659051835058
iteration : 75
train acc:  0.59375
train loss:  0.6758990287780762
train gradient:  0.22039947705891974
iteration : 76
train acc:  0.671875
train loss:  0.6206516027450562
train gradient:  0.1296878876738738
iteration : 77
train acc:  0.6796875
train loss:  0.6069758534431458
train gradient:  0.21079934790457605
iteration : 78
train acc:  0.6015625
train loss:  0.619713306427002
train gradient:  0.15423367999592366
iteration : 79
train acc:  0.5859375
train loss:  0.6313714981079102
train gradient:  0.1667561283980507
iteration : 80
train acc:  0.5859375
train loss:  0.6702351570129395
train gradient:  0.17575236449535744
iteration : 81
train acc:  0.6328125
train loss:  0.6549015045166016
train gradient:  0.20009221084790452
iteration : 82
train acc:  0.625
train loss:  0.6179549098014832
train gradient:  0.1715829663173305
iteration : 83
train acc:  0.6640625
train loss:  0.596413254737854
train gradient:  0.1507835937675253
iteration : 84
train acc:  0.671875
train loss:  0.6115637421607971
train gradient:  0.20806462633066725
iteration : 85
train acc:  0.578125
train loss:  0.6670887470245361
train gradient:  0.17210963053305905
iteration : 86
train acc:  0.6171875
train loss:  0.6382861137390137
train gradient:  0.15480676017706818
iteration : 87
train acc:  0.6484375
train loss:  0.6661251783370972
train gradient:  0.23386199444170894
iteration : 88
train acc:  0.6796875
train loss:  0.6103453636169434
train gradient:  0.1801823962132853
iteration : 89
train acc:  0.6171875
train loss:  0.6632112860679626
train gradient:  0.2470288420703646
iteration : 90
train acc:  0.703125
train loss:  0.5891425609588623
train gradient:  0.20624689789773898
iteration : 91
train acc:  0.5234375
train loss:  0.6853916049003601
train gradient:  0.2542537505631316
iteration : 92
train acc:  0.6640625
train loss:  0.6234122514724731
train gradient:  0.15643742517902193
iteration : 93
train acc:  0.6640625
train loss:  0.6068593263626099
train gradient:  0.2101113349224947
iteration : 94
train acc:  0.625
train loss:  0.6313602924346924
train gradient:  0.26348796598829227
iteration : 95
train acc:  0.625
train loss:  0.6221507787704468
train gradient:  0.1330456568664428
iteration : 96
train acc:  0.6875
train loss:  0.5763812065124512
train gradient:  0.15903295559706293
iteration : 97
train acc:  0.6328125
train loss:  0.6093629002571106
train gradient:  0.15772823037042194
iteration : 98
train acc:  0.5625
train loss:  0.6977673172950745
train gradient:  0.1909272984630594
iteration : 99
train acc:  0.59375
train loss:  0.6518120765686035
train gradient:  0.18871999590885968
iteration : 100
train acc:  0.6484375
train loss:  0.6478346586227417
train gradient:  0.2361597092771847
iteration : 101
train acc:  0.6171875
train loss:  0.6370222568511963
train gradient:  0.15064282129484158
iteration : 102
train acc:  0.578125
train loss:  0.652837872505188
train gradient:  0.1784802526716337
iteration : 103
train acc:  0.6640625
train loss:  0.6154441237449646
train gradient:  0.14158473702352953
iteration : 104
train acc:  0.671875
train loss:  0.6180242300033569
train gradient:  0.15509211579095714
iteration : 105
train acc:  0.6640625
train loss:  0.6213258504867554
train gradient:  0.15036955646094116
iteration : 106
train acc:  0.671875
train loss:  0.6282545328140259
train gradient:  0.13843720338608595
iteration : 107
train acc:  0.65625
train loss:  0.6392543315887451
train gradient:  0.21888982660653256
iteration : 108
train acc:  0.6015625
train loss:  0.6346935033798218
train gradient:  0.15955345658587047
iteration : 109
train acc:  0.6171875
train loss:  0.6585922241210938
train gradient:  0.2188141350312563
iteration : 110
train acc:  0.71875
train loss:  0.5625516176223755
train gradient:  0.15321639182662944
iteration : 111
train acc:  0.671875
train loss:  0.6301205158233643
train gradient:  0.1569370998096044
iteration : 112
train acc:  0.6484375
train loss:  0.6268681883811951
train gradient:  0.1656930351030504
iteration : 113
train acc:  0.5546875
train loss:  0.6939781904220581
train gradient:  0.2359319033440537
iteration : 114
train acc:  0.6484375
train loss:  0.6288255453109741
train gradient:  0.18553059115231574
iteration : 115
train acc:  0.6171875
train loss:  0.6390799283981323
train gradient:  0.12687157607307187
iteration : 116
train acc:  0.6484375
train loss:  0.6187794208526611
train gradient:  0.16484447845788086
iteration : 117
train acc:  0.6484375
train loss:  0.6183051466941833
train gradient:  0.2032657828114583
iteration : 118
train acc:  0.65625
train loss:  0.6181597709655762
train gradient:  0.1749626339648201
iteration : 119
train acc:  0.6328125
train loss:  0.6298714876174927
train gradient:  0.17653776193667992
iteration : 120
train acc:  0.625
train loss:  0.6748584508895874
train gradient:  0.22638191502757585
iteration : 121
train acc:  0.65625
train loss:  0.6060172915458679
train gradient:  0.14890130611111735
iteration : 122
train acc:  0.6328125
train loss:  0.6265603303909302
train gradient:  0.16042292540808628
iteration : 123
train acc:  0.609375
train loss:  0.6332954168319702
train gradient:  0.17833936066910322
iteration : 124
train acc:  0.6484375
train loss:  0.627492368221283
train gradient:  0.2045617344011429
iteration : 125
train acc:  0.65625
train loss:  0.5930725932121277
train gradient:  0.22226402137799406
iteration : 126
train acc:  0.6328125
train loss:  0.6558575630187988
train gradient:  0.1849680330322675
iteration : 127
train acc:  0.6875
train loss:  0.5972402095794678
train gradient:  0.16142166340829703
iteration : 128
train acc:  0.703125
train loss:  0.6146751642227173
train gradient:  0.15232265861132066
iteration : 129
train acc:  0.640625
train loss:  0.6367510557174683
train gradient:  0.14945232224746033
iteration : 130
train acc:  0.65625
train loss:  0.6167099475860596
train gradient:  0.1768623868516934
iteration : 131
train acc:  0.703125
train loss:  0.5975478887557983
train gradient:  0.16978127239754004
iteration : 132
train acc:  0.65625
train loss:  0.5754120349884033
train gradient:  0.1638849612084779
iteration : 133
train acc:  0.609375
train loss:  0.6664320230484009
train gradient:  0.20950812322617943
iteration : 134
train acc:  0.5859375
train loss:  0.6792846322059631
train gradient:  0.140606651011332
iteration : 135
train acc:  0.65625
train loss:  0.60813969373703
train gradient:  0.16307880036029324
iteration : 136
train acc:  0.5859375
train loss:  0.628446102142334
train gradient:  0.2502187882214767
iteration : 137
train acc:  0.671875
train loss:  0.6246834397315979
train gradient:  0.1787298860115718
iteration : 138
train acc:  0.6796875
train loss:  0.5884910821914673
train gradient:  0.14454001038331435
iteration : 139
train acc:  0.6484375
train loss:  0.6337850689888
train gradient:  0.20018264107623673
iteration : 140
train acc:  0.6328125
train loss:  0.6416152715682983
train gradient:  0.21930597745889005
iteration : 141
train acc:  0.5859375
train loss:  0.6339090466499329
train gradient:  0.14535954647296548
iteration : 142
train acc:  0.625
train loss:  0.6355677247047424
train gradient:  0.17044664069177726
iteration : 143
train acc:  0.6796875
train loss:  0.6049282550811768
train gradient:  0.16370576295190575
iteration : 144
train acc:  0.703125
train loss:  0.5650681257247925
train gradient:  0.15181246012845293
iteration : 145
train acc:  0.6875
train loss:  0.6106888055801392
train gradient:  0.16365546467876682
iteration : 146
train acc:  0.6796875
train loss:  0.5993441343307495
train gradient:  0.19542077673346914
iteration : 147
train acc:  0.6484375
train loss:  0.6238806247711182
train gradient:  0.18302560489231057
iteration : 148
train acc:  0.640625
train loss:  0.631088376045227
train gradient:  0.14539560128284446
iteration : 149
train acc:  0.671875
train loss:  0.6279592514038086
train gradient:  0.14733571036144066
iteration : 150
train acc:  0.7421875
train loss:  0.5621302127838135
train gradient:  0.19757769774163259
iteration : 151
train acc:  0.65625
train loss:  0.6377928256988525
train gradient:  0.1519681081874533
iteration : 152
train acc:  0.6015625
train loss:  0.6452829837799072
train gradient:  0.1463251563720409
iteration : 153
train acc:  0.625
train loss:  0.6462535262107849
train gradient:  0.22035468227839647
iteration : 154
train acc:  0.640625
train loss:  0.6623038053512573
train gradient:  0.24396691004917298
iteration : 155
train acc:  0.7109375
train loss:  0.5785424113273621
train gradient:  0.16285503238259053
iteration : 156
train acc:  0.671875
train loss:  0.590268611907959
train gradient:  0.13345175136780765
iteration : 157
train acc:  0.671875
train loss:  0.6263729929924011
train gradient:  0.13778958899984145
iteration : 158
train acc:  0.578125
train loss:  0.66612708568573
train gradient:  0.18589204350182967
iteration : 159
train acc:  0.6171875
train loss:  0.6185505390167236
train gradient:  0.1559309484124725
iteration : 160
train acc:  0.640625
train loss:  0.661705493927002
train gradient:  0.19290345036040774
iteration : 161
train acc:  0.6640625
train loss:  0.6313295364379883
train gradient:  0.13614080333904158
iteration : 162
train acc:  0.7265625
train loss:  0.5657845735549927
train gradient:  0.2280458548435719
iteration : 163
train acc:  0.6640625
train loss:  0.5885506868362427
train gradient:  0.12884971170992116
iteration : 164
train acc:  0.7109375
train loss:  0.6165337562561035
train gradient:  0.18644572228462747
iteration : 165
train acc:  0.625
train loss:  0.6612129807472229
train gradient:  0.15674121912061678
iteration : 166
train acc:  0.6484375
train loss:  0.6397818326950073
train gradient:  0.22904547560411131
iteration : 167
train acc:  0.6484375
train loss:  0.6242435574531555
train gradient:  0.1883030235735808
iteration : 168
train acc:  0.671875
train loss:  0.6010981798171997
train gradient:  0.14914174617461196
iteration : 169
train acc:  0.7109375
train loss:  0.5632516145706177
train gradient:  0.16053019787009304
iteration : 170
train acc:  0.65625
train loss:  0.5870270729064941
train gradient:  0.1292019420237658
iteration : 171
train acc:  0.7578125
train loss:  0.5467575788497925
train gradient:  0.17209150138104656
iteration : 172
train acc:  0.6875
train loss:  0.6140772104263306
train gradient:  0.17558142079631617
iteration : 173
train acc:  0.6484375
train loss:  0.626741886138916
train gradient:  0.2024067161178744
iteration : 174
train acc:  0.625
train loss:  0.6192141771316528
train gradient:  0.1695450296933206
iteration : 175
train acc:  0.6875
train loss:  0.5610780119895935
train gradient:  0.12868048549021457
iteration : 176
train acc:  0.625
train loss:  0.6528053879737854
train gradient:  0.18038302918544913
iteration : 177
train acc:  0.640625
train loss:  0.6103653311729431
train gradient:  0.1862206879709529
iteration : 178
train acc:  0.671875
train loss:  0.5878787040710449
train gradient:  0.2517074845361343
iteration : 179
train acc:  0.6015625
train loss:  0.6471828818321228
train gradient:  0.19802489816998908
iteration : 180
train acc:  0.7109375
train loss:  0.5877106189727783
train gradient:  0.203601676812315
iteration : 181
train acc:  0.6640625
train loss:  0.6045352220535278
train gradient:  0.13895062013724993
iteration : 182
train acc:  0.6796875
train loss:  0.60246741771698
train gradient:  0.16335131140346604
iteration : 183
train acc:  0.640625
train loss:  0.6191136240959167
train gradient:  0.1970869218358538
iteration : 184
train acc:  0.5859375
train loss:  0.666695773601532
train gradient:  0.23531251224679003
iteration : 185
train acc:  0.5859375
train loss:  0.6582643985748291
train gradient:  0.1860075116030366
iteration : 186
train acc:  0.6484375
train loss:  0.5875701904296875
train gradient:  0.14582043396082503
iteration : 187
train acc:  0.71875
train loss:  0.5948489308357239
train gradient:  0.17183803237728534
iteration : 188
train acc:  0.6328125
train loss:  0.6180018186569214
train gradient:  0.19280189890203064
iteration : 189
train acc:  0.671875
train loss:  0.635633111000061
train gradient:  0.14882764316806368
iteration : 190
train acc:  0.6328125
train loss:  0.6197931170463562
train gradient:  0.2243160982001547
iteration : 191
train acc:  0.640625
train loss:  0.6453595161437988
train gradient:  0.17503515716294735
iteration : 192
train acc:  0.6640625
train loss:  0.6017251014709473
train gradient:  0.17071127766733526
iteration : 193
train acc:  0.6484375
train loss:  0.6134967803955078
train gradient:  0.23457893179302602
iteration : 194
train acc:  0.703125
train loss:  0.5855776071548462
train gradient:  0.12393529719153992
iteration : 195
train acc:  0.6640625
train loss:  0.601577639579773
train gradient:  0.13507987566670254
iteration : 196
train acc:  0.6328125
train loss:  0.6218191981315613
train gradient:  0.1632973868822898
iteration : 197
train acc:  0.6796875
train loss:  0.6160850524902344
train gradient:  0.17302922559114847
iteration : 198
train acc:  0.6796875
train loss:  0.590076744556427
train gradient:  0.14964405469257147
iteration : 199
train acc:  0.671875
train loss:  0.5707868933677673
train gradient:  0.11744774546251802
iteration : 200
train acc:  0.6171875
train loss:  0.59576815366745
train gradient:  0.13998349581888683
iteration : 201
train acc:  0.5625
train loss:  0.6217115521430969
train gradient:  0.3155570631281363
iteration : 202
train acc:  0.71875
train loss:  0.5850896835327148
train gradient:  0.17139290265363727
iteration : 203
train acc:  0.5703125
train loss:  0.6646633744239807
train gradient:  0.3325447832340632
iteration : 204
train acc:  0.671875
train loss:  0.6161575317382812
train gradient:  0.14105031828987136
iteration : 205
train acc:  0.6875
train loss:  0.5800861716270447
train gradient:  0.12992914216212886
iteration : 206
train acc:  0.71875
train loss:  0.558006227016449
train gradient:  0.19854494944418194
iteration : 207
train acc:  0.625
train loss:  0.6568566560745239
train gradient:  0.21391144653111566
iteration : 208
train acc:  0.578125
train loss:  0.654096245765686
train gradient:  0.14600250924585068
iteration : 209
train acc:  0.625
train loss:  0.6427801847457886
train gradient:  0.1812925517927009
iteration : 210
train acc:  0.65625
train loss:  0.6162166595458984
train gradient:  0.17997920872556739
iteration : 211
train acc:  0.609375
train loss:  0.6571105122566223
train gradient:  0.1994123561864396
iteration : 212
train acc:  0.59375
train loss:  0.6121119856834412
train gradient:  0.14063127547141907
iteration : 213
train acc:  0.609375
train loss:  0.6631410717964172
train gradient:  0.2699918237020819
iteration : 214
train acc:  0.6953125
train loss:  0.5922847986221313
train gradient:  0.1502177457256084
iteration : 215
train acc:  0.734375
train loss:  0.5721508860588074
train gradient:  0.13693453575770242
iteration : 216
train acc:  0.6171875
train loss:  0.629445493221283
train gradient:  0.17026623914956335
iteration : 217
train acc:  0.640625
train loss:  0.6392689943313599
train gradient:  0.15940472849721515
iteration : 218
train acc:  0.7109375
train loss:  0.5746187567710876
train gradient:  0.14290659309821124
iteration : 219
train acc:  0.6640625
train loss:  0.6289500594139099
train gradient:  0.15849919138200888
iteration : 220
train acc:  0.6953125
train loss:  0.6076911687850952
train gradient:  0.15591210489577018
iteration : 221
train acc:  0.703125
train loss:  0.5358731746673584
train gradient:  0.1779025652915482
iteration : 222
train acc:  0.6953125
train loss:  0.6059577465057373
train gradient:  0.20625284326934928
iteration : 223
train acc:  0.6328125
train loss:  0.6095629334449768
train gradient:  0.1917088665704276
iteration : 224
train acc:  0.640625
train loss:  0.6194519996643066
train gradient:  0.17391303053518475
iteration : 225
train acc:  0.640625
train loss:  0.6249895095825195
train gradient:  0.21629119531893748
iteration : 226
train acc:  0.6484375
train loss:  0.5932058095932007
train gradient:  0.14341939769079168
iteration : 227
train acc:  0.640625
train loss:  0.6348257064819336
train gradient:  0.12493352942822165
iteration : 228
train acc:  0.7265625
train loss:  0.5645042061805725
train gradient:  0.16998826325225436
iteration : 229
train acc:  0.609375
train loss:  0.6364383697509766
train gradient:  0.2482041843488463
iteration : 230
train acc:  0.7109375
train loss:  0.5738646984100342
train gradient:  0.20377780321186428
iteration : 231
train acc:  0.5859375
train loss:  0.6433944702148438
train gradient:  0.16289610076454647
iteration : 232
train acc:  0.671875
train loss:  0.6141071319580078
train gradient:  0.1377279330278136
iteration : 233
train acc:  0.6875
train loss:  0.5868645906448364
train gradient:  0.1544407521236476
iteration : 234
train acc:  0.640625
train loss:  0.6482505798339844
train gradient:  0.1621671376712001
iteration : 235
train acc:  0.6171875
train loss:  0.6073881387710571
train gradient:  0.15651917630937723
iteration : 236
train acc:  0.71875
train loss:  0.5887259244918823
train gradient:  0.13970712762367743
iteration : 237
train acc:  0.6484375
train loss:  0.6436498165130615
train gradient:  0.23555958371453822
iteration : 238
train acc:  0.6796875
train loss:  0.5676587820053101
train gradient:  0.17172652797208526
iteration : 239
train acc:  0.671875
train loss:  0.6001427173614502
train gradient:  0.15904393342653356
iteration : 240
train acc:  0.65625
train loss:  0.603306770324707
train gradient:  0.16881711252316714
iteration : 241
train acc:  0.6328125
train loss:  0.6222349405288696
train gradient:  0.18417199803355833
iteration : 242
train acc:  0.6796875
train loss:  0.6038059592247009
train gradient:  0.19059234724946716
iteration : 243
train acc:  0.65625
train loss:  0.624968409538269
train gradient:  0.22491613107753028
iteration : 244
train acc:  0.609375
train loss:  0.6378276348114014
train gradient:  0.16211503985696457
iteration : 245
train acc:  0.703125
train loss:  0.5570752620697021
train gradient:  0.17197639495844816
iteration : 246
train acc:  0.7109375
train loss:  0.597119927406311
train gradient:  0.15504747156396737
iteration : 247
train acc:  0.6484375
train loss:  0.6041055917739868
train gradient:  0.17240819562730897
iteration : 248
train acc:  0.5546875
train loss:  0.6606219410896301
train gradient:  0.20971820320647414
iteration : 249
train acc:  0.6875
train loss:  0.6373671889305115
train gradient:  0.14329607157262658
iteration : 250
train acc:  0.6171875
train loss:  0.6510361433029175
train gradient:  0.17871079327028153
iteration : 251
train acc:  0.7578125
train loss:  0.5423600673675537
train gradient:  0.13902903581398776
iteration : 252
train acc:  0.6796875
train loss:  0.599092960357666
train gradient:  0.13567132317903496
iteration : 253
train acc:  0.6171875
train loss:  0.6297236680984497
train gradient:  0.16771377277308008
iteration : 254
train acc:  0.6328125
train loss:  0.6649448871612549
train gradient:  0.19815558509569076
iteration : 255
train acc:  0.6796875
train loss:  0.5952646732330322
train gradient:  0.2000649253511465
iteration : 256
train acc:  0.7109375
train loss:  0.5844172835350037
train gradient:  0.1851058552688986
iteration : 257
train acc:  0.7109375
train loss:  0.5594608783721924
train gradient:  0.14494985248263687
iteration : 258
train acc:  0.6796875
train loss:  0.5689584016799927
train gradient:  0.22977179208131326
iteration : 259
train acc:  0.7109375
train loss:  0.6009130477905273
train gradient:  0.13201925330758085
iteration : 260
train acc:  0.6328125
train loss:  0.6447001099586487
train gradient:  0.16630419643710154
iteration : 261
train acc:  0.609375
train loss:  0.6117708683013916
train gradient:  0.17258889887028797
iteration : 262
train acc:  0.5703125
train loss:  0.635125994682312
train gradient:  0.1284040121062425
iteration : 263
train acc:  0.6171875
train loss:  0.6156951189041138
train gradient:  0.23346954695794625
iteration : 264
train acc:  0.7109375
train loss:  0.5681989192962646
train gradient:  0.1310072754996835
iteration : 265
train acc:  0.6953125
train loss:  0.6050829291343689
train gradient:  0.17738660482419455
iteration : 266
train acc:  0.6328125
train loss:  0.5922070145606995
train gradient:  0.15703091382813084
iteration : 267
train acc:  0.6796875
train loss:  0.6068055629730225
train gradient:  0.15383577911397683
iteration : 268
train acc:  0.625
train loss:  0.6416950821876526
train gradient:  0.2019329880891515
iteration : 269
train acc:  0.7265625
train loss:  0.563865065574646
train gradient:  0.1812346092281115
iteration : 270
train acc:  0.671875
train loss:  0.6157106757164001
train gradient:  0.13514135747980616
iteration : 271
train acc:  0.65625
train loss:  0.5819977521896362
train gradient:  0.15608658300585682
iteration : 272
train acc:  0.6171875
train loss:  0.6491206884384155
train gradient:  0.19033397241966293
iteration : 273
train acc:  0.640625
train loss:  0.6228176355361938
train gradient:  0.2023370245848215
iteration : 274
train acc:  0.7578125
train loss:  0.5413630604743958
train gradient:  0.1613350939098479
iteration : 275
train acc:  0.71875
train loss:  0.569570779800415
train gradient:  0.1674286250149693
iteration : 276
train acc:  0.640625
train loss:  0.6414796113967896
train gradient:  0.20824990873712385
iteration : 277
train acc:  0.609375
train loss:  0.6724497079849243
train gradient:  0.1794334995915499
iteration : 278
train acc:  0.5859375
train loss:  0.668990969657898
train gradient:  0.20418884347813285
iteration : 279
train acc:  0.59375
train loss:  0.6626114845275879
train gradient:  0.1850755511544426
iteration : 280
train acc:  0.6953125
train loss:  0.5923261642456055
train gradient:  0.17553518731628823
iteration : 281
train acc:  0.703125
train loss:  0.5550936460494995
train gradient:  0.1540531491487721
iteration : 282
train acc:  0.6640625
train loss:  0.6167703866958618
train gradient:  0.2868568577051985
iteration : 283
train acc:  0.6953125
train loss:  0.594288170337677
train gradient:  0.17520762050302427
iteration : 284
train acc:  0.6953125
train loss:  0.587659478187561
train gradient:  0.17710670449433358
iteration : 285
train acc:  0.71875
train loss:  0.5698916912078857
train gradient:  0.1495730938486819
iteration : 286
train acc:  0.6171875
train loss:  0.6404215097427368
train gradient:  0.17598535546931793
iteration : 287
train acc:  0.59375
train loss:  0.6142784953117371
train gradient:  0.21439008101685927
iteration : 288
train acc:  0.6640625
train loss:  0.586453914642334
train gradient:  0.17548436539519735
iteration : 289
train acc:  0.671875
train loss:  0.6057268977165222
train gradient:  0.1569848926348285
iteration : 290
train acc:  0.6328125
train loss:  0.6292312741279602
train gradient:  0.18398246499538787
iteration : 291
train acc:  0.6953125
train loss:  0.5933225750923157
train gradient:  0.13994533343376642
iteration : 292
train acc:  0.6953125
train loss:  0.5704484581947327
train gradient:  0.1321136082041881
iteration : 293
train acc:  0.671875
train loss:  0.6099284887313843
train gradient:  0.1389246653354354
iteration : 294
train acc:  0.75
train loss:  0.5616676211357117
train gradient:  0.20979684621882133
iteration : 295
train acc:  0.6796875
train loss:  0.5707775354385376
train gradient:  0.16350937893430917
iteration : 296
train acc:  0.6484375
train loss:  0.5919774174690247
train gradient:  0.15721525531803718
iteration : 297
train acc:  0.6640625
train loss:  0.5923550128936768
train gradient:  0.18414715917005992
iteration : 298
train acc:  0.6875
train loss:  0.6034459471702576
train gradient:  0.14438216859990846
iteration : 299
train acc:  0.71875
train loss:  0.5524387359619141
train gradient:  0.13631104384964376
iteration : 300
train acc:  0.6171875
train loss:  0.6323615312576294
train gradient:  0.16545386103637963
iteration : 301
train acc:  0.65625
train loss:  0.6132888793945312
train gradient:  0.2236706438931219
iteration : 302
train acc:  0.6015625
train loss:  0.6367006301879883
train gradient:  0.1973967397934401
iteration : 303
train acc:  0.640625
train loss:  0.5879016518592834
train gradient:  0.22802500689832428
iteration : 304
train acc:  0.6484375
train loss:  0.6183424592018127
train gradient:  0.2103549479889619
iteration : 305
train acc:  0.6171875
train loss:  0.6490659117698669
train gradient:  0.2937246757366954
iteration : 306
train acc:  0.6640625
train loss:  0.5946052074432373
train gradient:  0.1669500462136581
iteration : 307
train acc:  0.703125
train loss:  0.6139975190162659
train gradient:  0.21683733218014997
iteration : 308
train acc:  0.6796875
train loss:  0.6205976009368896
train gradient:  0.17023202358351802
iteration : 309
train acc:  0.578125
train loss:  0.6421433687210083
train gradient:  0.19094233960822954
iteration : 310
train acc:  0.7109375
train loss:  0.5889966487884521
train gradient:  0.13915534593036538
iteration : 311
train acc:  0.625
train loss:  0.619511604309082
train gradient:  0.1252657078264608
iteration : 312
train acc:  0.6640625
train loss:  0.6247572898864746
train gradient:  0.1764052449791089
iteration : 313
train acc:  0.5234375
train loss:  0.6945209503173828
train gradient:  0.2379462921605664
iteration : 314
train acc:  0.6328125
train loss:  0.649017870426178
train gradient:  0.1909841219204383
iteration : 315
train acc:  0.6015625
train loss:  0.6343259811401367
train gradient:  0.16593906786653484
iteration : 316
train acc:  0.6484375
train loss:  0.6215483546257019
train gradient:  0.21362315808217014
iteration : 317
train acc:  0.71875
train loss:  0.5711132287979126
train gradient:  0.18256423780189135
iteration : 318
train acc:  0.6328125
train loss:  0.6388002634048462
train gradient:  0.2885755926390967
iteration : 319
train acc:  0.546875
train loss:  0.6973150968551636
train gradient:  0.21791634265615673
iteration : 320
train acc:  0.6875
train loss:  0.6224291920661926
train gradient:  0.1365374171135323
iteration : 321
train acc:  0.6796875
train loss:  0.6253266334533691
train gradient:  0.1862766941064105
iteration : 322
train acc:  0.6484375
train loss:  0.6062203645706177
train gradient:  0.1530164222665156
iteration : 323
train acc:  0.6640625
train loss:  0.5939464569091797
train gradient:  0.16193004475733183
iteration : 324
train acc:  0.6875
train loss:  0.5730652809143066
train gradient:  0.11856348856663022
iteration : 325
train acc:  0.5703125
train loss:  0.6498173475265503
train gradient:  0.19527487086879408
iteration : 326
train acc:  0.6484375
train loss:  0.6113749742507935
train gradient:  0.1787166760851512
iteration : 327
train acc:  0.7109375
train loss:  0.5786599516868591
train gradient:  0.16117116526309122
iteration : 328
train acc:  0.6953125
train loss:  0.6207646131515503
train gradient:  0.1969412044722692
iteration : 329
train acc:  0.625
train loss:  0.6140415668487549
train gradient:  0.15211792776084565
iteration : 330
train acc:  0.640625
train loss:  0.6193856596946716
train gradient:  0.19796074033662714
iteration : 331
train acc:  0.6484375
train loss:  0.6315174102783203
train gradient:  0.12717814108463077
iteration : 332
train acc:  0.6015625
train loss:  0.6247518658638
train gradient:  0.21929198888135934
iteration : 333
train acc:  0.7109375
train loss:  0.5880398750305176
train gradient:  0.1429342473779201
iteration : 334
train acc:  0.6640625
train loss:  0.5947214961051941
train gradient:  0.18579922506871624
iteration : 335
train acc:  0.7421875
train loss:  0.5459827184677124
train gradient:  0.17156443736338334
iteration : 336
train acc:  0.6796875
train loss:  0.6145405769348145
train gradient:  0.18546050059213834
iteration : 337
train acc:  0.6953125
train loss:  0.6025497913360596
train gradient:  0.17409090493108886
iteration : 338
train acc:  0.6953125
train loss:  0.5877870321273804
train gradient:  0.23773744299460425
iteration : 339
train acc:  0.6953125
train loss:  0.6068328619003296
train gradient:  0.17461899312408413
iteration : 340
train acc:  0.5859375
train loss:  0.649831235408783
train gradient:  0.21263839086391445
iteration : 341
train acc:  0.6875
train loss:  0.5922126173973083
train gradient:  0.16013858395336222
iteration : 342
train acc:  0.6328125
train loss:  0.6151230335235596
train gradient:  0.17565854810089354
iteration : 343
train acc:  0.671875
train loss:  0.6034635305404663
train gradient:  0.16158992715787807
iteration : 344
train acc:  0.7109375
train loss:  0.5749461650848389
train gradient:  0.19172300349573476
iteration : 345
train acc:  0.6640625
train loss:  0.6052336096763611
train gradient:  0.19269602723866197
iteration : 346
train acc:  0.703125
train loss:  0.6106871366500854
train gradient:  0.1546134627061253
iteration : 347
train acc:  0.671875
train loss:  0.5962955951690674
train gradient:  0.17138205076516952
iteration : 348
train acc:  0.625
train loss:  0.6488707065582275
train gradient:  0.23282961967486593
iteration : 349
train acc:  0.65625
train loss:  0.6197077631950378
train gradient:  0.1819531634311077
iteration : 350
train acc:  0.5859375
train loss:  0.6528935432434082
train gradient:  0.17471136522023456
iteration : 351
train acc:  0.609375
train loss:  0.6572476625442505
train gradient:  0.2055462892741713
iteration : 352
train acc:  0.734375
train loss:  0.5749935507774353
train gradient:  0.24287246886440358
iteration : 353
train acc:  0.6015625
train loss:  0.6385521292686462
train gradient:  0.22299422414432155
iteration : 354
train acc:  0.625
train loss:  0.6167696118354797
train gradient:  0.20366207360620925
iteration : 355
train acc:  0.6796875
train loss:  0.5853381156921387
train gradient:  0.1528122990763971
iteration : 356
train acc:  0.6796875
train loss:  0.5843931436538696
train gradient:  0.17781180220882103
iteration : 357
train acc:  0.734375
train loss:  0.5386855006217957
train gradient:  0.17469389932038526
iteration : 358
train acc:  0.6875
train loss:  0.5924906134605408
train gradient:  0.17342593987063004
iteration : 359
train acc:  0.6796875
train loss:  0.6189829111099243
train gradient:  0.208643733959712
iteration : 360
train acc:  0.6484375
train loss:  0.6364874243736267
train gradient:  0.15829313575623966
iteration : 361
train acc:  0.6640625
train loss:  0.5903448462486267
train gradient:  0.14154239459178083
iteration : 362
train acc:  0.6953125
train loss:  0.5832326412200928
train gradient:  0.23372059505677356
iteration : 363
train acc:  0.671875
train loss:  0.5826451778411865
train gradient:  0.2686710231755238
iteration : 364
train acc:  0.65625
train loss:  0.593631386756897
train gradient:  0.17087297976576088
iteration : 365
train acc:  0.671875
train loss:  0.6134178042411804
train gradient:  0.15934928592528896
iteration : 366
train acc:  0.6640625
train loss:  0.57377690076828
train gradient:  0.20744987108907398
iteration : 367
train acc:  0.71875
train loss:  0.5903506278991699
train gradient:  0.173252264184386
iteration : 368
train acc:  0.75
train loss:  0.5783087015151978
train gradient:  0.1311419409799485
iteration : 369
train acc:  0.6953125
train loss:  0.5744742155075073
train gradient:  0.1909958973894021
iteration : 370
train acc:  0.6953125
train loss:  0.5948560833930969
train gradient:  0.1281243738653817
iteration : 371
train acc:  0.6328125
train loss:  0.5953160524368286
train gradient:  0.20502928133738035
iteration : 372
train acc:  0.6640625
train loss:  0.5909819602966309
train gradient:  0.17738394672481822
iteration : 373
train acc:  0.6484375
train loss:  0.6140158176422119
train gradient:  0.21100308917112204
iteration : 374
train acc:  0.6640625
train loss:  0.6195270419120789
train gradient:  0.18960691909579686
iteration : 375
train acc:  0.703125
train loss:  0.548727810382843
train gradient:  0.16812976392504908
iteration : 376
train acc:  0.6015625
train loss:  0.6381639242172241
train gradient:  0.2365331583426482
iteration : 377
train acc:  0.625
train loss:  0.6333833336830139
train gradient:  0.16446534817334085
iteration : 378
train acc:  0.71875
train loss:  0.5598933696746826
train gradient:  0.18688254291657003
iteration : 379
train acc:  0.6640625
train loss:  0.5696061253547668
train gradient:  0.1315055496567859
iteration : 380
train acc:  0.6171875
train loss:  0.6283082962036133
train gradient:  0.15996094224636936
iteration : 381
train acc:  0.6953125
train loss:  0.6119617223739624
train gradient:  0.13579586025908424
iteration : 382
train acc:  0.71875
train loss:  0.6062750220298767
train gradient:  0.1774081955469167
iteration : 383
train acc:  0.6640625
train loss:  0.5777714252471924
train gradient:  0.14777681772657036
iteration : 384
train acc:  0.625
train loss:  0.6194630861282349
train gradient:  0.19265773913129253
iteration : 385
train acc:  0.6953125
train loss:  0.5982375144958496
train gradient:  0.13515124643637172
iteration : 386
train acc:  0.671875
train loss:  0.645429790019989
train gradient:  0.22435385746552253
iteration : 387
train acc:  0.6796875
train loss:  0.6013938784599304
train gradient:  0.1447800871970154
iteration : 388
train acc:  0.625
train loss:  0.6418321132659912
train gradient:  0.24279748679171842
iteration : 389
train acc:  0.6328125
train loss:  0.6236226558685303
train gradient:  0.15957838108242822
iteration : 390
train acc:  0.640625
train loss:  0.596989631652832
train gradient:  0.15109151760607353
iteration : 391
train acc:  0.671875
train loss:  0.5974541306495667
train gradient:  0.1703667801275463
iteration : 392
train acc:  0.703125
train loss:  0.5684900283813477
train gradient:  0.15900576406506758
iteration : 393
train acc:  0.5546875
train loss:  0.6458148956298828
train gradient:  0.25044848670803044
iteration : 394
train acc:  0.6640625
train loss:  0.6112833023071289
train gradient:  0.26226410592974553
iteration : 395
train acc:  0.703125
train loss:  0.5676311254501343
train gradient:  0.16080382373070884
iteration : 396
train acc:  0.703125
train loss:  0.5616726875305176
train gradient:  0.12376690426324688
iteration : 397
train acc:  0.6328125
train loss:  0.6174634695053101
train gradient:  0.131770140723121
iteration : 398
train acc:  0.671875
train loss:  0.5929843187332153
train gradient:  0.14887752003478988
iteration : 399
train acc:  0.6953125
train loss:  0.5864191055297852
train gradient:  0.16542863852178724
iteration : 400
train acc:  0.71875
train loss:  0.5508788824081421
train gradient:  0.1488183865318618
iteration : 401
train acc:  0.7109375
train loss:  0.5691090822219849
train gradient:  0.1933870074231272
iteration : 402
train acc:  0.6640625
train loss:  0.5720748901367188
train gradient:  0.14905750088444492
iteration : 403
train acc:  0.640625
train loss:  0.6124452352523804
train gradient:  0.19857893647519448
iteration : 404
train acc:  0.6796875
train loss:  0.5827869176864624
train gradient:  0.1875552067994188
iteration : 405
train acc:  0.7109375
train loss:  0.5875990390777588
train gradient:  0.17771498122673446
iteration : 406
train acc:  0.671875
train loss:  0.6158955097198486
train gradient:  0.19337939817509536
iteration : 407
train acc:  0.6640625
train loss:  0.5855758190155029
train gradient:  0.13015430276955864
iteration : 408
train acc:  0.6640625
train loss:  0.5920621156692505
train gradient:  0.1551310984349529
iteration : 409
train acc:  0.625
train loss:  0.627400815486908
train gradient:  0.15854033028885522
iteration : 410
train acc:  0.6875
train loss:  0.5964208245277405
train gradient:  0.1385238346666754
iteration : 411
train acc:  0.671875
train loss:  0.5875805020332336
train gradient:  0.14672106269576904
iteration : 412
train acc:  0.640625
train loss:  0.612425684928894
train gradient:  0.1364995569792719
iteration : 413
train acc:  0.703125
train loss:  0.565909743309021
train gradient:  0.18581599468249646
iteration : 414
train acc:  0.7109375
train loss:  0.5784015655517578
train gradient:  0.13929795708524495
iteration : 415
train acc:  0.640625
train loss:  0.6273975372314453
train gradient:  0.17059841838246959
iteration : 416
train acc:  0.71875
train loss:  0.5861355066299438
train gradient:  0.16047179967152847
iteration : 417
train acc:  0.7109375
train loss:  0.5763856172561646
train gradient:  0.13472140164515892
iteration : 418
train acc:  0.71875
train loss:  0.5297756791114807
train gradient:  0.1417147433666548
iteration : 419
train acc:  0.6796875
train loss:  0.6027988195419312
train gradient:  0.1693562834455693
iteration : 420
train acc:  0.6796875
train loss:  0.5604634284973145
train gradient:  0.1718284633568133
iteration : 421
train acc:  0.640625
train loss:  0.600735604763031
train gradient:  0.22922867237728567
iteration : 422
train acc:  0.6875
train loss:  0.5552186965942383
train gradient:  0.18861564775469872
iteration : 423
train acc:  0.6640625
train loss:  0.6040867567062378
train gradient:  0.17576980959549798
iteration : 424
train acc:  0.6796875
train loss:  0.590601921081543
train gradient:  0.17496474776775792
iteration : 425
train acc:  0.6640625
train loss:  0.6040536165237427
train gradient:  0.27806500707397125
iteration : 426
train acc:  0.6640625
train loss:  0.5962920188903809
train gradient:  0.17603340410574586
iteration : 427
train acc:  0.578125
train loss:  0.6409944891929626
train gradient:  0.2178703783390511
iteration : 428
train acc:  0.6796875
train loss:  0.5969220995903015
train gradient:  0.19644653883409527
iteration : 429
train acc:  0.65625
train loss:  0.6397663950920105
train gradient:  0.30731030175535334
iteration : 430
train acc:  0.71875
train loss:  0.5714857578277588
train gradient:  0.1591594199094386
iteration : 431
train acc:  0.6796875
train loss:  0.5858829021453857
train gradient:  0.15216450885124336
iteration : 432
train acc:  0.625
train loss:  0.6005429625511169
train gradient:  0.168419450406949
iteration : 433
train acc:  0.65625
train loss:  0.6226301193237305
train gradient:  0.19741217403758549
iteration : 434
train acc:  0.6875
train loss:  0.6049282550811768
train gradient:  0.13710075925181453
iteration : 435
train acc:  0.640625
train loss:  0.6051936149597168
train gradient:  0.1713267599495381
iteration : 436
train acc:  0.6953125
train loss:  0.6032988429069519
train gradient:  0.2151499599549011
iteration : 437
train acc:  0.6796875
train loss:  0.5975266695022583
train gradient:  0.16317426689555375
iteration : 438
train acc:  0.6640625
train loss:  0.5700920820236206
train gradient:  0.12376705956176022
iteration : 439
train acc:  0.640625
train loss:  0.6045411229133606
train gradient:  0.13753018223608804
iteration : 440
train acc:  0.71875
train loss:  0.5621356964111328
train gradient:  0.19890115190135593
iteration : 441
train acc:  0.6953125
train loss:  0.5646117925643921
train gradient:  0.1599576338798799
iteration : 442
train acc:  0.71875
train loss:  0.6025696992874146
train gradient:  0.18831545526013563
iteration : 443
train acc:  0.640625
train loss:  0.5945843458175659
train gradient:  0.13096352301254294
iteration : 444
train acc:  0.703125
train loss:  0.5920105576515198
train gradient:  0.1286760805051525
iteration : 445
train acc:  0.71875
train loss:  0.5353570580482483
train gradient:  0.2547185258404031
iteration : 446
train acc:  0.640625
train loss:  0.6230047941207886
train gradient:  0.18047130333664457
iteration : 447
train acc:  0.6875
train loss:  0.5786735415458679
train gradient:  0.1735154291504633
iteration : 448
train acc:  0.6484375
train loss:  0.5808361768722534
train gradient:  0.1604931048866824
iteration : 449
train acc:  0.71875
train loss:  0.5920858383178711
train gradient:  0.1387261234973176
iteration : 450
train acc:  0.71875
train loss:  0.5918897390365601
train gradient:  0.16171278285245036
iteration : 451
train acc:  0.6796875
train loss:  0.5877877473831177
train gradient:  0.1653953497379913
iteration : 452
train acc:  0.703125
train loss:  0.5812281370162964
train gradient:  0.16315655839815674
iteration : 453
train acc:  0.6328125
train loss:  0.6239141821861267
train gradient:  0.16089982672739692
iteration : 454
train acc:  0.6796875
train loss:  0.639297604560852
train gradient:  0.24212474452599209
iteration : 455
train acc:  0.578125
train loss:  0.6434588432312012
train gradient:  0.18116883246397353
iteration : 456
train acc:  0.6640625
train loss:  0.616173505783081
train gradient:  0.1452162758986016
iteration : 457
train acc:  0.75
train loss:  0.5406322479248047
train gradient:  0.14506045635225223
iteration : 458
train acc:  0.75
train loss:  0.5319239497184753
train gradient:  0.234759038424584
iteration : 459
train acc:  0.671875
train loss:  0.5801388621330261
train gradient:  0.14115683730562728
iteration : 460
train acc:  0.7109375
train loss:  0.60773104429245
train gradient:  0.16832319694485098
iteration : 461
train acc:  0.640625
train loss:  0.620777428150177
train gradient:  0.2360776687710358
iteration : 462
train acc:  0.625
train loss:  0.5893973708152771
train gradient:  0.20852920485186488
iteration : 463
train acc:  0.6640625
train loss:  0.6146748065948486
train gradient:  0.1874135793595661
iteration : 464
train acc:  0.6875
train loss:  0.5946797132492065
train gradient:  0.18279596505518364
iteration : 465
train acc:  0.703125
train loss:  0.5778016448020935
train gradient:  0.14737243920402837
iteration : 466
train acc:  0.6875
train loss:  0.5707100629806519
train gradient:  0.16164369456186894
iteration : 467
train acc:  0.6953125
train loss:  0.5546014904975891
train gradient:  0.17895277938024656
iteration : 468
train acc:  0.6875
train loss:  0.5743695497512817
train gradient:  0.14351253921337728
iteration : 469
train acc:  0.71875
train loss:  0.5378069877624512
train gradient:  0.19106740356966415
iteration : 470
train acc:  0.671875
train loss:  0.6318800449371338
train gradient:  0.17722375426615844
iteration : 471
train acc:  0.6171875
train loss:  0.5885353088378906
train gradient:  0.21026756899237903
iteration : 472
train acc:  0.71875
train loss:  0.5723289847373962
train gradient:  0.17314230274109788
iteration : 473
train acc:  0.6875
train loss:  0.6237788200378418
train gradient:  0.18662196551083948
iteration : 474
train acc:  0.6796875
train loss:  0.5850141644477844
train gradient:  0.1781179750599817
iteration : 475
train acc:  0.6953125
train loss:  0.564775824546814
train gradient:  0.13611512883605925
iteration : 476
train acc:  0.640625
train loss:  0.6182026863098145
train gradient:  0.2399948299903953
iteration : 477
train acc:  0.6640625
train loss:  0.6312399506568909
train gradient:  0.14351399768162587
iteration : 478
train acc:  0.6796875
train loss:  0.5814118385314941
train gradient:  0.22292071687113246
iteration : 479
train acc:  0.6640625
train loss:  0.567385733127594
train gradient:  0.13000225543756966
iteration : 480
train acc:  0.7109375
train loss:  0.5672118067741394
train gradient:  0.15383317960943363
iteration : 481
train acc:  0.7109375
train loss:  0.5541845560073853
train gradient:  0.2330583688537139
iteration : 482
train acc:  0.671875
train loss:  0.6086376905441284
train gradient:  0.158611789882678
iteration : 483
train acc:  0.625
train loss:  0.6121654510498047
train gradient:  0.1909857910834652
iteration : 484
train acc:  0.65625
train loss:  0.6242895722389221
train gradient:  0.21798451206947916
iteration : 485
train acc:  0.6953125
train loss:  0.5803670883178711
train gradient:  0.17558700815728365
iteration : 486
train acc:  0.7109375
train loss:  0.5657429695129395
train gradient:  0.15963509904795176
iteration : 487
train acc:  0.6953125
train loss:  0.5995764136314392
train gradient:  0.1608733443878227
iteration : 488
train acc:  0.7109375
train loss:  0.5434085130691528
train gradient:  0.15290804068323566
iteration : 489
train acc:  0.625
train loss:  0.6172076463699341
train gradient:  0.13671617222600485
iteration : 490
train acc:  0.703125
train loss:  0.5919709205627441
train gradient:  0.20523631808228623
iteration : 491
train acc:  0.703125
train loss:  0.5871492028236389
train gradient:  0.16958366127161403
iteration : 492
train acc:  0.6484375
train loss:  0.6386888027191162
train gradient:  0.16066682919488015
iteration : 493
train acc:  0.6875
train loss:  0.591890811920166
train gradient:  0.1549020550646272
iteration : 494
train acc:  0.640625
train loss:  0.6086287498474121
train gradient:  0.23339030045813436
iteration : 495
train acc:  0.71875
train loss:  0.5838249921798706
train gradient:  0.33795776646861964
iteration : 496
train acc:  0.625
train loss:  0.5998662710189819
train gradient:  0.15921491435341384
iteration : 497
train acc:  0.734375
train loss:  0.5589703321456909
train gradient:  0.16279546195370365
iteration : 498
train acc:  0.703125
train loss:  0.601515531539917
train gradient:  0.15626136087291087
iteration : 499
train acc:  0.6640625
train loss:  0.6120381355285645
train gradient:  0.137188604860134
iteration : 500
train acc:  0.59375
train loss:  0.6540342569351196
train gradient:  0.19360421851870863
iteration : 501
train acc:  0.625
train loss:  0.6103948354721069
train gradient:  0.20554377534716747
iteration : 502
train acc:  0.734375
train loss:  0.5627306699752808
train gradient:  0.17252290255052793
iteration : 503
train acc:  0.65625
train loss:  0.5987333059310913
train gradient:  0.1852009351311254
iteration : 504
train acc:  0.6875
train loss:  0.5810074210166931
train gradient:  0.18238492318053307
iteration : 505
train acc:  0.7578125
train loss:  0.5426831841468811
train gradient:  0.13882877544469652
iteration : 506
train acc:  0.640625
train loss:  0.5856944918632507
train gradient:  0.12223746165183837
iteration : 507
train acc:  0.6484375
train loss:  0.6051732301712036
train gradient:  0.19190351142313772
iteration : 508
train acc:  0.71875
train loss:  0.5783319473266602
train gradient:  0.1343392898895999
iteration : 509
train acc:  0.703125
train loss:  0.5758946537971497
train gradient:  0.15863149821911682
iteration : 510
train acc:  0.703125
train loss:  0.5405026078224182
train gradient:  0.15838970050241513
iteration : 511
train acc:  0.6484375
train loss:  0.5916714668273926
train gradient:  0.1468576448245939
iteration : 512
train acc:  0.71875
train loss:  0.5431516170501709
train gradient:  0.1327641460040258
iteration : 513
train acc:  0.625
train loss:  0.5784127712249756
train gradient:  0.17676630777823593
iteration : 514
train acc:  0.671875
train loss:  0.6045044660568237
train gradient:  0.1596118637700092
iteration : 515
train acc:  0.5859375
train loss:  0.648845374584198
train gradient:  0.1402381719688371
iteration : 516
train acc:  0.734375
train loss:  0.5308651328086853
train gradient:  0.1881780903298108
iteration : 517
train acc:  0.5859375
train loss:  0.6065998077392578
train gradient:  0.21016008896613192
iteration : 518
train acc:  0.65625
train loss:  0.5831760168075562
train gradient:  0.18692213086788145
iteration : 519
train acc:  0.65625
train loss:  0.5773623585700989
train gradient:  0.1503057418358724
iteration : 520
train acc:  0.609375
train loss:  0.633586049079895
train gradient:  0.19117762668629523
iteration : 521
train acc:  0.6484375
train loss:  0.6331411600112915
train gradient:  0.2282233370446562
iteration : 522
train acc:  0.6328125
train loss:  0.5954429507255554
train gradient:  0.21621005640130792
iteration : 523
train acc:  0.6484375
train loss:  0.6130746603012085
train gradient:  0.19390457148311396
iteration : 524
train acc:  0.6953125
train loss:  0.5618495345115662
train gradient:  0.1470417685344368
iteration : 525
train acc:  0.6796875
train loss:  0.6229910850524902
train gradient:  0.18393823901592812
iteration : 526
train acc:  0.75
train loss:  0.5000501275062561
train gradient:  0.1337475466414082
iteration : 527
train acc:  0.6796875
train loss:  0.5696768760681152
train gradient:  0.1530336684421027
iteration : 528
train acc:  0.65625
train loss:  0.6357145309448242
train gradient:  0.24287076715351297
iteration : 529
train acc:  0.7578125
train loss:  0.49047496914863586
train gradient:  0.20939395118663612
iteration : 530
train acc:  0.6328125
train loss:  0.6311148405075073
train gradient:  0.18716855976594404
iteration : 531
train acc:  0.6328125
train loss:  0.6350189447402954
train gradient:  0.14461720307398804
iteration : 532
train acc:  0.671875
train loss:  0.6258711814880371
train gradient:  0.16887737870348649
iteration : 533
train acc:  0.703125
train loss:  0.5392553210258484
train gradient:  0.1831392165806059
iteration : 534
train acc:  0.7734375
train loss:  0.5244146585464478
train gradient:  0.24051113410086195
iteration : 535
train acc:  0.640625
train loss:  0.574083149433136
train gradient:  0.21021738115778354
iteration : 536
train acc:  0.6875
train loss:  0.5864003300666809
train gradient:  0.17311391898490897
iteration : 537
train acc:  0.6953125
train loss:  0.5944952964782715
train gradient:  0.1457193061183241
iteration : 538
train acc:  0.65625
train loss:  0.6141788959503174
train gradient:  0.2541601277203561
iteration : 539
train acc:  0.6953125
train loss:  0.5937832593917847
train gradient:  0.19711511000348586
iteration : 540
train acc:  0.7734375
train loss:  0.5464367866516113
train gradient:  0.12346810072383692
iteration : 541
train acc:  0.671875
train loss:  0.5921962261199951
train gradient:  0.21133101155866785
iteration : 542
train acc:  0.734375
train loss:  0.570175290107727
train gradient:  0.17522900532116698
iteration : 543
train acc:  0.6796875
train loss:  0.576671838760376
train gradient:  0.21674115099276536
iteration : 544
train acc:  0.6484375
train loss:  0.5754982233047485
train gradient:  0.20737775216789225
iteration : 545
train acc:  0.6328125
train loss:  0.6195392608642578
train gradient:  0.14186142818247985
iteration : 546
train acc:  0.640625
train loss:  0.6005755662918091
train gradient:  0.19090233435028903
iteration : 547
train acc:  0.6640625
train loss:  0.6097015738487244
train gradient:  0.2230567728461364
iteration : 548
train acc:  0.671875
train loss:  0.5799388885498047
train gradient:  0.1501812666220516
iteration : 549
train acc:  0.71875
train loss:  0.5716080665588379
train gradient:  0.15502766861271344
iteration : 550
train acc:  0.640625
train loss:  0.5809485912322998
train gradient:  0.14626465560646096
iteration : 551
train acc:  0.609375
train loss:  0.6628167629241943
train gradient:  0.2381566555455547
iteration : 552
train acc:  0.6640625
train loss:  0.5825647711753845
train gradient:  0.1798132962486123
iteration : 553
train acc:  0.671875
train loss:  0.5670289993286133
train gradient:  0.17227983767879815
iteration : 554
train acc:  0.6640625
train loss:  0.6001850366592407
train gradient:  0.1641430785789123
iteration : 555
train acc:  0.6171875
train loss:  0.6135934591293335
train gradient:  0.27996599692224944
iteration : 556
train acc:  0.65625
train loss:  0.6212587356567383
train gradient:  0.18274218748445414
iteration : 557
train acc:  0.7265625
train loss:  0.5555749535560608
train gradient:  0.1743735056226654
iteration : 558
train acc:  0.703125
train loss:  0.5908809900283813
train gradient:  0.20204893496625337
iteration : 559
train acc:  0.6484375
train loss:  0.6127681732177734
train gradient:  0.13969045503037208
iteration : 560
train acc:  0.734375
train loss:  0.553635835647583
train gradient:  0.15182997334252887
iteration : 561
train acc:  0.6328125
train loss:  0.6280863881111145
train gradient:  0.19216748932005345
iteration : 562
train acc:  0.6640625
train loss:  0.563125491142273
train gradient:  0.21477357754193382
iteration : 563
train acc:  0.6484375
train loss:  0.6496283411979675
train gradient:  0.3839176515013314
iteration : 564
train acc:  0.625
train loss:  0.6385396718978882
train gradient:  0.2854693672434918
iteration : 565
train acc:  0.734375
train loss:  0.5431437492370605
train gradient:  0.18569381791826622
iteration : 566
train acc:  0.6328125
train loss:  0.6365643739700317
train gradient:  0.1803876646181236
iteration : 567
train acc:  0.703125
train loss:  0.5675407648086548
train gradient:  0.16852174013946786
iteration : 568
train acc:  0.6484375
train loss:  0.6133667230606079
train gradient:  0.1509299624829068
iteration : 569
train acc:  0.6328125
train loss:  0.5652965307235718
train gradient:  0.15255619821478844
iteration : 570
train acc:  0.6328125
train loss:  0.6508017778396606
train gradient:  0.19595051711277123
iteration : 571
train acc:  0.7265625
train loss:  0.5121938586235046
train gradient:  0.11975294867467642
iteration : 572
train acc:  0.625
train loss:  0.618655800819397
train gradient:  0.1903398258391715
iteration : 573
train acc:  0.71875
train loss:  0.5424079895019531
train gradient:  0.1444897876398851
iteration : 574
train acc:  0.75
train loss:  0.5687544941902161
train gradient:  0.15792480735901016
iteration : 575
train acc:  0.6875
train loss:  0.5989649295806885
train gradient:  0.19167791199903475
iteration : 576
train acc:  0.6328125
train loss:  0.6545881032943726
train gradient:  0.18731915745187494
iteration : 577
train acc:  0.5859375
train loss:  0.6354471445083618
train gradient:  0.1676019366671344
iteration : 578
train acc:  0.6796875
train loss:  0.5889299511909485
train gradient:  0.15768895906649827
iteration : 579
train acc:  0.6015625
train loss:  0.6252906918525696
train gradient:  0.17706436918806767
iteration : 580
train acc:  0.6328125
train loss:  0.6278717517852783
train gradient:  0.20398883340987267
iteration : 581
train acc:  0.6875
train loss:  0.6342105865478516
train gradient:  0.21263330680056752
iteration : 582
train acc:  0.6796875
train loss:  0.5936678647994995
train gradient:  0.14425092292735153
iteration : 583
train acc:  0.6875
train loss:  0.614912748336792
train gradient:  0.2262136739083857
iteration : 584
train acc:  0.671875
train loss:  0.6145102977752686
train gradient:  0.17225149229591957
iteration : 585
train acc:  0.7109375
train loss:  0.5852867960929871
train gradient:  0.12320996276283369
iteration : 586
train acc:  0.609375
train loss:  0.6352947950363159
train gradient:  0.20070470214743114
iteration : 587
train acc:  0.7421875
train loss:  0.5736470818519592
train gradient:  0.1987481040306044
iteration : 588
train acc:  0.71875
train loss:  0.5722471475601196
train gradient:  0.20501852035903334
iteration : 589
train acc:  0.6796875
train loss:  0.5728244781494141
train gradient:  0.16032617746864425
iteration : 590
train acc:  0.671875
train loss:  0.5816012024879456
train gradient:  0.23132702143004333
iteration : 591
train acc:  0.6953125
train loss:  0.5731176733970642
train gradient:  0.12767884977994143
iteration : 592
train acc:  0.7265625
train loss:  0.5687111616134644
train gradient:  0.1348448896463727
iteration : 593
train acc:  0.703125
train loss:  0.5773160457611084
train gradient:  0.12829672491052393
iteration : 594
train acc:  0.671875
train loss:  0.6056040525436401
train gradient:  0.18415191798347208
iteration : 595
train acc:  0.7578125
train loss:  0.5883898735046387
train gradient:  0.14069484083856412
iteration : 596
train acc:  0.7265625
train loss:  0.5253397226333618
train gradient:  0.12376337059514324
iteration : 597
train acc:  0.765625
train loss:  0.5400073528289795
train gradient:  0.11961431507122643
iteration : 598
train acc:  0.6953125
train loss:  0.5896673202514648
train gradient:  0.1852359221170341
iteration : 599
train acc:  0.75
train loss:  0.5550941228866577
train gradient:  0.13151953133317584
iteration : 600
train acc:  0.6796875
train loss:  0.6027420163154602
train gradient:  0.16618357919532045
iteration : 601
train acc:  0.7265625
train loss:  0.5710139870643616
train gradient:  0.12972631994032846
iteration : 602
train acc:  0.6484375
train loss:  0.600222647190094
train gradient:  0.1729612215503899
iteration : 603
train acc:  0.671875
train loss:  0.5797260999679565
train gradient:  0.15041467406271447
iteration : 604
train acc:  0.7109375
train loss:  0.5582582950592041
train gradient:  0.11877322485855808
iteration : 605
train acc:  0.75
train loss:  0.552382230758667
train gradient:  0.15744635845886512
iteration : 606
train acc:  0.6484375
train loss:  0.5959314703941345
train gradient:  0.19925297449225432
iteration : 607
train acc:  0.625
train loss:  0.605930507183075
train gradient:  0.13084292399873218
iteration : 608
train acc:  0.7734375
train loss:  0.5356287956237793
train gradient:  0.10919008907368423
iteration : 609
train acc:  0.6640625
train loss:  0.5785629749298096
train gradient:  0.1597914083047188
iteration : 610
train acc:  0.65625
train loss:  0.5912607312202454
train gradient:  0.16436729338646275
iteration : 611
train acc:  0.640625
train loss:  0.5836354494094849
train gradient:  0.24473459922468996
iteration : 612
train acc:  0.6875
train loss:  0.5960234999656677
train gradient:  0.23474984587153835
iteration : 613
train acc:  0.703125
train loss:  0.5563606023788452
train gradient:  0.14381766155677075
iteration : 614
train acc:  0.71875
train loss:  0.56380695104599
train gradient:  0.19778479983613573
iteration : 615
train acc:  0.640625
train loss:  0.5745463967323303
train gradient:  0.12303432484897393
iteration : 616
train acc:  0.65625
train loss:  0.598743200302124
train gradient:  0.12556610628243658
iteration : 617
train acc:  0.65625
train loss:  0.6492946743965149
train gradient:  0.14934363348904975
iteration : 618
train acc:  0.734375
train loss:  0.5278469920158386
train gradient:  0.11948548985994066
iteration : 619
train acc:  0.7109375
train loss:  0.545896053314209
train gradient:  0.14494267151403667
iteration : 620
train acc:  0.7109375
train loss:  0.5861148834228516
train gradient:  0.17691691427495185
iteration : 621
train acc:  0.6015625
train loss:  0.6315205693244934
train gradient:  0.21688311365874247
iteration : 622
train acc:  0.7109375
train loss:  0.5595270991325378
train gradient:  0.19705225133862428
iteration : 623
train acc:  0.671875
train loss:  0.5924813747406006
train gradient:  0.16023188905199007
iteration : 624
train acc:  0.7421875
train loss:  0.5588915348052979
train gradient:  0.1497743619137233
iteration : 625
train acc:  0.65625
train loss:  0.6031396389007568
train gradient:  0.19132350814002985
iteration : 626
train acc:  0.7109375
train loss:  0.5343863368034363
train gradient:  0.23290349811405675
iteration : 627
train acc:  0.6328125
train loss:  0.6287226676940918
train gradient:  0.21591395908679453
iteration : 628
train acc:  0.6796875
train loss:  0.6070717573165894
train gradient:  0.22973277396863123
iteration : 629
train acc:  0.6484375
train loss:  0.6217427253723145
train gradient:  0.24812360226006197
iteration : 630
train acc:  0.703125
train loss:  0.5357369184494019
train gradient:  0.17494104475566685
iteration : 631
train acc:  0.6953125
train loss:  0.5902198553085327
train gradient:  0.2088955048431631
iteration : 632
train acc:  0.703125
train loss:  0.5390213131904602
train gradient:  0.1435922563339509
iteration : 633
train acc:  0.6875
train loss:  0.6050646901130676
train gradient:  0.1847558888716655
iteration : 634
train acc:  0.71875
train loss:  0.5652564167976379
train gradient:  0.13722460736678163
iteration : 635
train acc:  0.734375
train loss:  0.5624388456344604
train gradient:  0.22683709149616502
iteration : 636
train acc:  0.671875
train loss:  0.5803384780883789
train gradient:  0.11435558102923363
iteration : 637
train acc:  0.703125
train loss:  0.5795042514801025
train gradient:  0.14985121019720027
iteration : 638
train acc:  0.6953125
train loss:  0.5695244073867798
train gradient:  0.12274803542934347
iteration : 639
train acc:  0.6640625
train loss:  0.5801128149032593
train gradient:  0.18253353674367795
iteration : 640
train acc:  0.703125
train loss:  0.603008508682251
train gradient:  0.2447620854841759
iteration : 641
train acc:  0.6875
train loss:  0.6016941666603088
train gradient:  0.18359791412230986
iteration : 642
train acc:  0.6328125
train loss:  0.5837669968605042
train gradient:  0.13638699956054706
iteration : 643
train acc:  0.71875
train loss:  0.5452329516410828
train gradient:  0.13072535259423657
iteration : 644
train acc:  0.6640625
train loss:  0.600783109664917
train gradient:  0.17659040576223045
iteration : 645
train acc:  0.6484375
train loss:  0.6071727871894836
train gradient:  0.1691374472700437
iteration : 646
train acc:  0.6796875
train loss:  0.6064472198486328
train gradient:  0.13258809770309443
iteration : 647
train acc:  0.65625
train loss:  0.5977481603622437
train gradient:  0.14320049195840556
iteration : 648
train acc:  0.6484375
train loss:  0.5940189361572266
train gradient:  0.14082903466466712
iteration : 649
train acc:  0.703125
train loss:  0.5801451206207275
train gradient:  0.15897210569370857
iteration : 650
train acc:  0.640625
train loss:  0.6383277177810669
train gradient:  0.17569817104396393
iteration : 651
train acc:  0.59375
train loss:  0.6181663274765015
train gradient:  0.24300216904092398
iteration : 652
train acc:  0.6875
train loss:  0.5595529675483704
train gradient:  0.1758567738739399
iteration : 653
train acc:  0.6796875
train loss:  0.5867817997932434
train gradient:  0.13543966792823037
iteration : 654
train acc:  0.703125
train loss:  0.5485584735870361
train gradient:  0.213792287576411
iteration : 655
train acc:  0.6640625
train loss:  0.5831649303436279
train gradient:  0.1478103041126781
iteration : 656
train acc:  0.5859375
train loss:  0.6569849848747253
train gradient:  0.23305920545976566
iteration : 657
train acc:  0.65625
train loss:  0.5918307304382324
train gradient:  0.16514415303943886
iteration : 658
train acc:  0.6796875
train loss:  0.5696241855621338
train gradient:  0.1770861020223462
iteration : 659
train acc:  0.6484375
train loss:  0.6568536758422852
train gradient:  0.2115252973542459
iteration : 660
train acc:  0.6328125
train loss:  0.63067626953125
train gradient:  0.15877779336798822
iteration : 661
train acc:  0.703125
train loss:  0.5356767773628235
train gradient:  0.1464858686625684
iteration : 662
train acc:  0.65625
train loss:  0.632070779800415
train gradient:  0.2943503210001674
iteration : 663
train acc:  0.71875
train loss:  0.5582325458526611
train gradient:  0.12231880011097507
iteration : 664
train acc:  0.65625
train loss:  0.5577012896537781
train gradient:  0.16729401914348507
iteration : 665
train acc:  0.734375
train loss:  0.507273256778717
train gradient:  0.19387013707816958
iteration : 666
train acc:  0.65625
train loss:  0.5704376101493835
train gradient:  0.2080275663262833
iteration : 667
train acc:  0.7265625
train loss:  0.5697788000106812
train gradient:  0.15565839947110016
iteration : 668
train acc:  0.6484375
train loss:  0.6022101640701294
train gradient:  0.1786481820051186
iteration : 669
train acc:  0.65625
train loss:  0.5692993402481079
train gradient:  0.22376784323499285
iteration : 670
train acc:  0.671875
train loss:  0.5614834427833557
train gradient:  0.11141647521158099
iteration : 671
train acc:  0.5859375
train loss:  0.6757122278213501
train gradient:  0.22331211472942533
iteration : 672
train acc:  0.6953125
train loss:  0.5848287343978882
train gradient:  0.17362062812053797
iteration : 673
train acc:  0.6796875
train loss:  0.5744398236274719
train gradient:  0.15339900437168888
iteration : 674
train acc:  0.703125
train loss:  0.5656479597091675
train gradient:  0.19480332688478905
iteration : 675
train acc:  0.6484375
train loss:  0.6364635229110718
train gradient:  0.26918371886606546
iteration : 676
train acc:  0.6640625
train loss:  0.5820596218109131
train gradient:  0.2447206641259561
iteration : 677
train acc:  0.7109375
train loss:  0.5905446410179138
train gradient:  0.17051757537148388
iteration : 678
train acc:  0.6953125
train loss:  0.5803606510162354
train gradient:  0.14547164428198475
iteration : 679
train acc:  0.765625
train loss:  0.5272870063781738
train gradient:  0.14044901701302537
iteration : 680
train acc:  0.7109375
train loss:  0.548223614692688
train gradient:  0.17255284946821567
iteration : 681
train acc:  0.6640625
train loss:  0.6086597442626953
train gradient:  0.15260596259663317
iteration : 682
train acc:  0.6484375
train loss:  0.6333960294723511
train gradient:  0.20948158128744124
iteration : 683
train acc:  0.703125
train loss:  0.6029274463653564
train gradient:  0.16006031862089795
iteration : 684
train acc:  0.6875
train loss:  0.5516321063041687
train gradient:  0.14366243335490642
iteration : 685
train acc:  0.7109375
train loss:  0.5457166433334351
train gradient:  0.12620373522256836
iteration : 686
train acc:  0.6484375
train loss:  0.5904145836830139
train gradient:  0.1435953612511922
iteration : 687
train acc:  0.78125
train loss:  0.5188193321228027
train gradient:  0.13250174851746216
iteration : 688
train acc:  0.6796875
train loss:  0.6170514822006226
train gradient:  0.20774837272152913
iteration : 689
train acc:  0.6484375
train loss:  0.5954755544662476
train gradient:  0.14623051358278688
iteration : 690
train acc:  0.6875
train loss:  0.5895934700965881
train gradient:  0.19693216367269464
iteration : 691
train acc:  0.640625
train loss:  0.616527795791626
train gradient:  0.1995829479477652
iteration : 692
train acc:  0.625
train loss:  0.6142035722732544
train gradient:  0.2495856245720494
iteration : 693
train acc:  0.6640625
train loss:  0.6216214299201965
train gradient:  0.18524751944131473
iteration : 694
train acc:  0.703125
train loss:  0.5845257043838501
train gradient:  0.16688227331881458
iteration : 695
train acc:  0.7421875
train loss:  0.5242453813552856
train gradient:  0.14289826265982586
iteration : 696
train acc:  0.703125
train loss:  0.5515103340148926
train gradient:  0.18064057014244433
iteration : 697
train acc:  0.6171875
train loss:  0.610664963722229
train gradient:  0.2575890796948202
iteration : 698
train acc:  0.6953125
train loss:  0.5483486652374268
train gradient:  0.13781555148329053
iteration : 699
train acc:  0.75
train loss:  0.5735880136489868
train gradient:  0.1647851788707814
iteration : 700
train acc:  0.6875
train loss:  0.5886945724487305
train gradient:  0.19121103803096895
iteration : 701
train acc:  0.6484375
train loss:  0.612013578414917
train gradient:  0.15472036894239594
iteration : 702
train acc:  0.671875
train loss:  0.564787745475769
train gradient:  0.19362228413676225
iteration : 703
train acc:  0.671875
train loss:  0.5977929830551147
train gradient:  0.15284009181729008
iteration : 704
train acc:  0.71875
train loss:  0.558077335357666
train gradient:  0.13582089880637882
iteration : 705
train acc:  0.671875
train loss:  0.5692341327667236
train gradient:  0.16385532905968075
iteration : 706
train acc:  0.625
train loss:  0.6323291063308716
train gradient:  0.16580849229291156
iteration : 707
train acc:  0.6875
train loss:  0.575486421585083
train gradient:  0.1859056618337147
iteration : 708
train acc:  0.6640625
train loss:  0.5854049921035767
train gradient:  0.16578874775199004
iteration : 709
train acc:  0.703125
train loss:  0.5568391680717468
train gradient:  0.1450008074447171
iteration : 710
train acc:  0.7109375
train loss:  0.5746954679489136
train gradient:  0.12055422754056652
iteration : 711
train acc:  0.65625
train loss:  0.6190085411071777
train gradient:  0.17522248109240987
iteration : 712
train acc:  0.65625
train loss:  0.5699445009231567
train gradient:  0.1437481873552347
iteration : 713
train acc:  0.703125
train loss:  0.5675919055938721
train gradient:  0.1424547802404566
iteration : 714
train acc:  0.6484375
train loss:  0.593944787979126
train gradient:  0.13676686262619858
iteration : 715
train acc:  0.71875
train loss:  0.5589488744735718
train gradient:  0.15332586355120548
iteration : 716
train acc:  0.734375
train loss:  0.5448222160339355
train gradient:  0.1256655056139746
iteration : 717
train acc:  0.6640625
train loss:  0.5743579864501953
train gradient:  0.304426193239868
iteration : 718
train acc:  0.71875
train loss:  0.5822447538375854
train gradient:  0.1756879431529931
iteration : 719
train acc:  0.671875
train loss:  0.6105574369430542
train gradient:  0.15735916859196086
iteration : 720
train acc:  0.6640625
train loss:  0.5766899585723877
train gradient:  0.15654084475264762
iteration : 721
train acc:  0.703125
train loss:  0.5576062202453613
train gradient:  0.21907501972848137
iteration : 722
train acc:  0.7265625
train loss:  0.5172547101974487
train gradient:  0.14509997407523545
iteration : 723
train acc:  0.6796875
train loss:  0.5888543128967285
train gradient:  0.1612106354027587
iteration : 724
train acc:  0.6484375
train loss:  0.6759581565856934
train gradient:  0.29939854720387865
iteration : 725
train acc:  0.7734375
train loss:  0.5075891017913818
train gradient:  0.14506220973240821
iteration : 726
train acc:  0.640625
train loss:  0.6267877221107483
train gradient:  0.20715591223765653
iteration : 727
train acc:  0.6875
train loss:  0.5858240127563477
train gradient:  0.17121042922123617
iteration : 728
train acc:  0.65625
train loss:  0.5880892276763916
train gradient:  0.16149273476723958
iteration : 729
train acc:  0.6328125
train loss:  0.5814350843429565
train gradient:  0.16637883806668063
iteration : 730
train acc:  0.703125
train loss:  0.5762774348258972
train gradient:  0.1432430692968631
iteration : 731
train acc:  0.625
train loss:  0.6558151245117188
train gradient:  0.3472903279949467
iteration : 732
train acc:  0.734375
train loss:  0.5610496997833252
train gradient:  0.12470261916282363
iteration : 733
train acc:  0.65625
train loss:  0.6226146817207336
train gradient:  0.2426838561854932
iteration : 734
train acc:  0.6796875
train loss:  0.5421720743179321
train gradient:  0.15446254676082505
iteration : 735
train acc:  0.71875
train loss:  0.5431463718414307
train gradient:  0.1832510334514828
iteration : 736
train acc:  0.7421875
train loss:  0.5167062282562256
train gradient:  0.1465713107706591
iteration : 737
train acc:  0.7578125
train loss:  0.5350003242492676
train gradient:  0.15443336358306087
iteration : 738
train acc:  0.71875
train loss:  0.5422903299331665
train gradient:  0.1900319415870041
iteration : 739
train acc:  0.75
train loss:  0.5327569246292114
train gradient:  0.11037179742628679
iteration : 740
train acc:  0.671875
train loss:  0.5807330012321472
train gradient:  0.1341162170006563
iteration : 741
train acc:  0.6171875
train loss:  0.5940110683441162
train gradient:  0.1767818531604907
iteration : 742
train acc:  0.65625
train loss:  0.6204858422279358
train gradient:  0.19292441778257666
iteration : 743
train acc:  0.7734375
train loss:  0.5348573923110962
train gradient:  0.15856579025815415
iteration : 744
train acc:  0.7734375
train loss:  0.48876920342445374
train gradient:  0.1607388008549859
iteration : 745
train acc:  0.6796875
train loss:  0.568210244178772
train gradient:  0.1723055185105528
iteration : 746
train acc:  0.71875
train loss:  0.5380845665931702
train gradient:  0.1309658315414901
iteration : 747
train acc:  0.7265625
train loss:  0.5498631596565247
train gradient:  0.16842667576508338
iteration : 748
train acc:  0.65625
train loss:  0.5820910930633545
train gradient:  0.14207198538508853
iteration : 749
train acc:  0.6640625
train loss:  0.6224946975708008
train gradient:  0.20299183898554163
iteration : 750
train acc:  0.671875
train loss:  0.5949766635894775
train gradient:  0.20467985311992273
iteration : 751
train acc:  0.6015625
train loss:  0.648640513420105
train gradient:  0.1954365925461699
iteration : 752
train acc:  0.6875
train loss:  0.5604615211486816
train gradient:  0.2254233987947968
iteration : 753
train acc:  0.6953125
train loss:  0.6110765933990479
train gradient:  0.1647786099623279
iteration : 754
train acc:  0.703125
train loss:  0.5845068693161011
train gradient:  0.1851340499862626
iteration : 755
train acc:  0.734375
train loss:  0.5421567559242249
train gradient:  0.14331567102112602
iteration : 756
train acc:  0.6640625
train loss:  0.5792642831802368
train gradient:  0.18195962557442832
iteration : 757
train acc:  0.7109375
train loss:  0.5380820035934448
train gradient:  0.18556604848552338
iteration : 758
train acc:  0.6953125
train loss:  0.5824498534202576
train gradient:  0.23313170392191362
iteration : 759
train acc:  0.6796875
train loss:  0.5915827751159668
train gradient:  0.2087509429937834
iteration : 760
train acc:  0.640625
train loss:  0.625999927520752
train gradient:  0.28954724229231743
iteration : 761
train acc:  0.671875
train loss:  0.5979608297348022
train gradient:  0.17353062197541694
iteration : 762
train acc:  0.7265625
train loss:  0.5536164045333862
train gradient:  0.1530744644628067
iteration : 763
train acc:  0.625
train loss:  0.6192107200622559
train gradient:  0.20479116474971204
iteration : 764
train acc:  0.6953125
train loss:  0.5610181093215942
train gradient:  0.22482993780314045
iteration : 765
train acc:  0.6953125
train loss:  0.5705726742744446
train gradient:  0.1975935597803637
iteration : 766
train acc:  0.59375
train loss:  0.6495472192764282
train gradient:  0.19843136456119242
iteration : 767
train acc:  0.671875
train loss:  0.5565734505653381
train gradient:  0.13772777089813468
iteration : 768
train acc:  0.78125
train loss:  0.5357662439346313
train gradient:  0.1387922773944777
iteration : 769
train acc:  0.7109375
train loss:  0.5359643697738647
train gradient:  0.16816648046112256
iteration : 770
train acc:  0.6796875
train loss:  0.5906350612640381
train gradient:  0.2723341513172829
iteration : 771
train acc:  0.6796875
train loss:  0.5562927722930908
train gradient:  0.13138350542369268
iteration : 772
train acc:  0.7109375
train loss:  0.5479835271835327
train gradient:  0.12203618826293769
iteration : 773
train acc:  0.7109375
train loss:  0.5680416822433472
train gradient:  0.1620827428893628
iteration : 774
train acc:  0.734375
train loss:  0.5396360754966736
train gradient:  0.11837658704489912
iteration : 775
train acc:  0.609375
train loss:  0.6043174266815186
train gradient:  0.25018208321336743
iteration : 776
train acc:  0.7265625
train loss:  0.5270635485649109
train gradient:  0.16249285657901102
iteration : 777
train acc:  0.6171875
train loss:  0.6026812791824341
train gradient:  0.16556380690979006
iteration : 778
train acc:  0.734375
train loss:  0.5547288060188293
train gradient:  0.11953445289616293
iteration : 779
train acc:  0.6875
train loss:  0.584026038646698
train gradient:  0.14169299404591776
iteration : 780
train acc:  0.703125
train loss:  0.5525320768356323
train gradient:  0.1463978341717122
iteration : 781
train acc:  0.65625
train loss:  0.5877397060394287
train gradient:  0.1903572954629938
iteration : 782
train acc:  0.6484375
train loss:  0.6301482915878296
train gradient:  0.1892472541278407
iteration : 783
train acc:  0.625
train loss:  0.6386393308639526
train gradient:  0.21972330742678314
iteration : 784
train acc:  0.703125
train loss:  0.5414381623268127
train gradient:  0.17916699322131063
iteration : 785
train acc:  0.65625
train loss:  0.6115498542785645
train gradient:  0.2621889758714912
iteration : 786
train acc:  0.671875
train loss:  0.651701807975769
train gradient:  0.17418289737937415
iteration : 787
train acc:  0.6328125
train loss:  0.6246738433837891
train gradient:  0.27617146249041524
iteration : 788
train acc:  0.6796875
train loss:  0.6280176043510437
train gradient:  0.22501730421514538
iteration : 789
train acc:  0.6640625
train loss:  0.6175940036773682
train gradient:  0.20950708274596785
iteration : 790
train acc:  0.640625
train loss:  0.6081602573394775
train gradient:  0.17713561168164021
iteration : 791
train acc:  0.703125
train loss:  0.5443193316459656
train gradient:  0.14586162138752926
iteration : 792
train acc:  0.703125
train loss:  0.5461433529853821
train gradient:  0.13970822475089117
iteration : 793
train acc:  0.6953125
train loss:  0.6199265122413635
train gradient:  0.18238596530163692
iteration : 794
train acc:  0.65625
train loss:  0.5843554735183716
train gradient:  0.1729959217611313
iteration : 795
train acc:  0.7265625
train loss:  0.5101636648178101
train gradient:  0.1322396841122379
iteration : 796
train acc:  0.625
train loss:  0.6541389226913452
train gradient:  0.22148523384605384
iteration : 797
train acc:  0.7265625
train loss:  0.5284407734870911
train gradient:  0.14828251417564486
iteration : 798
train acc:  0.640625
train loss:  0.6226711869239807
train gradient:  0.16742000085510325
iteration : 799
train acc:  0.6875
train loss:  0.6057903170585632
train gradient:  0.1551421661852287
iteration : 800
train acc:  0.6015625
train loss:  0.6407908201217651
train gradient:  0.20389650049761554
iteration : 801
train acc:  0.640625
train loss:  0.5964466333389282
train gradient:  0.1891178798250831
iteration : 802
train acc:  0.7578125
train loss:  0.5028255581855774
train gradient:  0.12144070497181676
iteration : 803
train acc:  0.6796875
train loss:  0.6225351691246033
train gradient:  0.20605607443677837
iteration : 804
train acc:  0.7265625
train loss:  0.529661238193512
train gradient:  0.2060750362971191
iteration : 805
train acc:  0.6875
train loss:  0.5685951709747314
train gradient:  0.2324318190271542
iteration : 806
train acc:  0.6484375
train loss:  0.6313619613647461
train gradient:  0.1760410830219163
iteration : 807
train acc:  0.703125
train loss:  0.5668306350708008
train gradient:  0.1412948547883437
iteration : 808
train acc:  0.6328125
train loss:  0.5482006072998047
train gradient:  0.14472718623720499
iteration : 809
train acc:  0.671875
train loss:  0.5803714990615845
train gradient:  0.21251342888494362
iteration : 810
train acc:  0.640625
train loss:  0.5579201579093933
train gradient:  0.14989342060704103
iteration : 811
train acc:  0.6328125
train loss:  0.6262907981872559
train gradient:  0.24056561874259202
iteration : 812
train acc:  0.6953125
train loss:  0.603643536567688
train gradient:  0.1714342784339214
iteration : 813
train acc:  0.6875
train loss:  0.5879891514778137
train gradient:  0.15211554454469495
iteration : 814
train acc:  0.75
train loss:  0.5596290230751038
train gradient:  0.1452985935211764
iteration : 815
train acc:  0.65625
train loss:  0.6051493883132935
train gradient:  0.15798134045180162
iteration : 816
train acc:  0.703125
train loss:  0.5742135643959045
train gradient:  0.20152655766598038
iteration : 817
train acc:  0.7109375
train loss:  0.5635896325111389
train gradient:  0.13738318617564094
iteration : 818
train acc:  0.6015625
train loss:  0.5885787010192871
train gradient:  0.22280918770567856
iteration : 819
train acc:  0.703125
train loss:  0.5736309289932251
train gradient:  0.14504280634701616
iteration : 820
train acc:  0.671875
train loss:  0.6228803396224976
train gradient:  0.197047082344429
iteration : 821
train acc:  0.65625
train loss:  0.6041487455368042
train gradient:  0.19307851496069395
iteration : 822
train acc:  0.7578125
train loss:  0.5518757104873657
train gradient:  0.14385068442481147
iteration : 823
train acc:  0.703125
train loss:  0.5716953873634338
train gradient:  0.14126520516369076
iteration : 824
train acc:  0.7265625
train loss:  0.5499167442321777
train gradient:  0.19853996192309892
iteration : 825
train acc:  0.5859375
train loss:  0.6232433319091797
train gradient:  0.18040343671722134
iteration : 826
train acc:  0.734375
train loss:  0.5600294470787048
train gradient:  0.15237347878697938
iteration : 827
train acc:  0.7421875
train loss:  0.5427354574203491
train gradient:  0.24932487425390282
iteration : 828
train acc:  0.609375
train loss:  0.6448310613632202
train gradient:  0.19965683554897123
iteration : 829
train acc:  0.6171875
train loss:  0.6653226613998413
train gradient:  0.26463206912613674
iteration : 830
train acc:  0.65625
train loss:  0.5839677453041077
train gradient:  0.17650831390744354
iteration : 831
train acc:  0.609375
train loss:  0.6887466907501221
train gradient:  0.19812144777940882
iteration : 832
train acc:  0.6953125
train loss:  0.5479679703712463
train gradient:  0.21193663094323456
iteration : 833
train acc:  0.703125
train loss:  0.5461643934249878
train gradient:  0.15912047439055657
iteration : 834
train acc:  0.6875
train loss:  0.6143416166305542
train gradient:  0.1860312210271471
iteration : 835
train acc:  0.6328125
train loss:  0.5833536386489868
train gradient:  0.12752729712243147
iteration : 836
train acc:  0.71875
train loss:  0.5729410648345947
train gradient:  0.12874985310537554
iteration : 837
train acc:  0.6875
train loss:  0.558189868927002
train gradient:  0.17439010759476925
iteration : 838
train acc:  0.6484375
train loss:  0.6112993955612183
train gradient:  0.23679500350956906
iteration : 839
train acc:  0.6640625
train loss:  0.5842198729515076
train gradient:  0.14731535600063989
iteration : 840
train acc:  0.671875
train loss:  0.5605105757713318
train gradient:  0.17639158657352158
iteration : 841
train acc:  0.6953125
train loss:  0.5625824928283691
train gradient:  0.17129857719344327
iteration : 842
train acc:  0.625
train loss:  0.6569836735725403
train gradient:  0.24159190165821298
iteration : 843
train acc:  0.6875
train loss:  0.6250824332237244
train gradient:  0.14030022366372413
iteration : 844
train acc:  0.734375
train loss:  0.5661072731018066
train gradient:  0.1995595598813012
iteration : 845
train acc:  0.65625
train loss:  0.5573116540908813
train gradient:  0.12089707087586596
iteration : 846
train acc:  0.6953125
train loss:  0.5718588829040527
train gradient:  0.15406254127884317
iteration : 847
train acc:  0.65625
train loss:  0.5823383331298828
train gradient:  0.1646597588383239
iteration : 848
train acc:  0.6875
train loss:  0.576815128326416
train gradient:  0.16376924270786097
iteration : 849
train acc:  0.75
train loss:  0.530392050743103
train gradient:  0.11300164685979192
iteration : 850
train acc:  0.7734375
train loss:  0.564994215965271
train gradient:  0.19822737429085552
iteration : 851
train acc:  0.7578125
train loss:  0.5235744714736938
train gradient:  0.1388179846573589
iteration : 852
train acc:  0.71875
train loss:  0.545894980430603
train gradient:  0.20353578212300366
iteration : 853
train acc:  0.6484375
train loss:  0.5871305465698242
train gradient:  0.14752582013579935
iteration : 854
train acc:  0.734375
train loss:  0.5452488660812378
train gradient:  0.21142606526658414
iteration : 855
train acc:  0.6953125
train loss:  0.5680530071258545
train gradient:  0.14351190940192554
iteration : 856
train acc:  0.6484375
train loss:  0.6042450666427612
train gradient:  0.16822229334114341
iteration : 857
train acc:  0.703125
train loss:  0.5766017436981201
train gradient:  0.1584082806545551
iteration : 858
train acc:  0.671875
train loss:  0.5713265538215637
train gradient:  0.13790769136506462
iteration : 859
train acc:  0.625
train loss:  0.627781331539154
train gradient:  0.13950145084128268
iteration : 860
train acc:  0.640625
train loss:  0.5819011926651001
train gradient:  0.1800505370474111
iteration : 861
train acc:  0.7109375
train loss:  0.5808471441268921
train gradient:  0.18253830846990166
iteration : 862
train acc:  0.71875
train loss:  0.5457916259765625
train gradient:  0.2041461001491261
iteration : 863
train acc:  0.7109375
train loss:  0.5712937116622925
train gradient:  0.1299568850506056
iteration : 864
train acc:  0.6875
train loss:  0.5848325490951538
train gradient:  0.20226836930683095
iteration : 865
train acc:  0.65625
train loss:  0.589530348777771
train gradient:  0.20224637693182448
iteration : 866
train acc:  0.6953125
train loss:  0.576581597328186
train gradient:  0.17745607995106866
iteration : 867
train acc:  0.71875
train loss:  0.5728901028633118
train gradient:  0.13609883671958586
iteration : 868
train acc:  0.640625
train loss:  0.6139113306999207
train gradient:  0.1431945898577731
iteration : 869
train acc:  0.6875
train loss:  0.6051952838897705
train gradient:  0.207461090374695
iteration : 870
train acc:  0.6640625
train loss:  0.622322678565979
train gradient:  0.182239084077147
iteration : 871
train acc:  0.6875
train loss:  0.5927033424377441
train gradient:  0.15984091458510188
iteration : 872
train acc:  0.734375
train loss:  0.5418343544006348
train gradient:  0.1456833946829028
iteration : 873
train acc:  0.7265625
train loss:  0.5499505996704102
train gradient:  0.17303212578351457
iteration : 874
train acc:  0.734375
train loss:  0.5383859872817993
train gradient:  0.1344872795059862
iteration : 875
train acc:  0.71875
train loss:  0.5701600313186646
train gradient:  0.14756830457507658
iteration : 876
train acc:  0.640625
train loss:  0.6073312163352966
train gradient:  0.20070985904519542
iteration : 877
train acc:  0.7265625
train loss:  0.5198078155517578
train gradient:  0.11887991388868692
iteration : 878
train acc:  0.6171875
train loss:  0.643837034702301
train gradient:  0.16878836605134917
iteration : 879
train acc:  0.7265625
train loss:  0.5140942335128784
train gradient:  0.1707825507635755
iteration : 880
train acc:  0.6796875
train loss:  0.576624870300293
train gradient:  0.15653585578530804
iteration : 881
train acc:  0.65625
train loss:  0.5772143602371216
train gradient:  0.15209057902085907
iteration : 882
train acc:  0.65625
train loss:  0.6089717149734497
train gradient:  0.1560998643953786
iteration : 883
train acc:  0.6953125
train loss:  0.5751681327819824
train gradient:  0.2723067652034999
iteration : 884
train acc:  0.703125
train loss:  0.5235505104064941
train gradient:  0.15620144855014512
iteration : 885
train acc:  0.65625
train loss:  0.5919654965400696
train gradient:  0.15633202076062766
iteration : 886
train acc:  0.6328125
train loss:  0.6384734511375427
train gradient:  0.19997056495449939
iteration : 887
train acc:  0.71875
train loss:  0.618130087852478
train gradient:  0.1910624460976887
iteration : 888
train acc:  0.625
train loss:  0.6177178025245667
train gradient:  0.20270053466880444
iteration : 889
train acc:  0.6796875
train loss:  0.5830292105674744
train gradient:  0.17265216685854418
iteration : 890
train acc:  0.6796875
train loss:  0.6002324819564819
train gradient:  0.2139844204267863
iteration : 891
train acc:  0.7109375
train loss:  0.5601369142532349
train gradient:  0.1325695311099674
iteration : 892
train acc:  0.7109375
train loss:  0.5614721775054932
train gradient:  0.18914393830343845
iteration : 893
train acc:  0.6640625
train loss:  0.6154690980911255
train gradient:  0.16752076940383084
iteration : 894
train acc:  0.7421875
train loss:  0.5462515950202942
train gradient:  0.14223278789585722
iteration : 895
train acc:  0.6796875
train loss:  0.5945469737052917
train gradient:  0.16904771090178533
iteration : 896
train acc:  0.734375
train loss:  0.547342836856842
train gradient:  0.1563547514360049
iteration : 897
train acc:  0.6875
train loss:  0.588240385055542
train gradient:  0.15598292862380475
iteration : 898
train acc:  0.640625
train loss:  0.5673846006393433
train gradient:  0.1864756698811069
iteration : 899
train acc:  0.609375
train loss:  0.6200708746910095
train gradient:  0.20892554415578732
iteration : 900
train acc:  0.6953125
train loss:  0.5727514624595642
train gradient:  0.12993767304887455
iteration : 901
train acc:  0.6875
train loss:  0.6470791101455688
train gradient:  0.24571094318086767
iteration : 902
train acc:  0.6640625
train loss:  0.5601397156715393
train gradient:  0.18944888064838394
iteration : 903
train acc:  0.7109375
train loss:  0.5741152167320251
train gradient:  0.14235251763628842
iteration : 904
train acc:  0.75
train loss:  0.5127277374267578
train gradient:  0.14584494793153158
iteration : 905
train acc:  0.7265625
train loss:  0.6072304248809814
train gradient:  0.25965512286522174
iteration : 906
train acc:  0.6953125
train loss:  0.5794468522071838
train gradient:  0.21538409021276023
iteration : 907
train acc:  0.6796875
train loss:  0.580242395401001
train gradient:  0.1195587787557232
iteration : 908
train acc:  0.65625
train loss:  0.5941112041473389
train gradient:  0.18950308837879493
iteration : 909
train acc:  0.71875
train loss:  0.5425246953964233
train gradient:  0.14844376397734593
iteration : 910
train acc:  0.671875
train loss:  0.560431957244873
train gradient:  0.16514764359176043
iteration : 911
train acc:  0.6015625
train loss:  0.6027473211288452
train gradient:  0.15155109388651472
iteration : 912
train acc:  0.734375
train loss:  0.5446510314941406
train gradient:  0.17062657103133222
iteration : 913
train acc:  0.6953125
train loss:  0.5317655205726624
train gradient:  0.18792483391792597
iteration : 914
train acc:  0.7421875
train loss:  0.5197342038154602
train gradient:  0.16816277817968595
iteration : 915
train acc:  0.703125
train loss:  0.5731183886528015
train gradient:  0.2799287642116438
iteration : 916
train acc:  0.75
train loss:  0.525075376033783
train gradient:  0.18471859798487641
iteration : 917
train acc:  0.6953125
train loss:  0.5495748519897461
train gradient:  0.1650328373484714
iteration : 918
train acc:  0.6484375
train loss:  0.6234166622161865
train gradient:  0.18430073025141658
iteration : 919
train acc:  0.6796875
train loss:  0.5379589200019836
train gradient:  0.1682181730461612
iteration : 920
train acc:  0.671875
train loss:  0.5973087549209595
train gradient:  0.17067849101691318
iteration : 921
train acc:  0.6796875
train loss:  0.6121945381164551
train gradient:  0.1558109584525647
iteration : 922
train acc:  0.6796875
train loss:  0.6402508020401001
train gradient:  0.17348912692439783
iteration : 923
train acc:  0.6484375
train loss:  0.6122009754180908
train gradient:  0.17995243773635794
iteration : 924
train acc:  0.671875
train loss:  0.5714485049247742
train gradient:  0.17956107745498795
iteration : 925
train acc:  0.734375
train loss:  0.5314732789993286
train gradient:  0.1408786359623725
iteration : 926
train acc:  0.6484375
train loss:  0.5997490882873535
train gradient:  0.139233787788892
iteration : 927
train acc:  0.625
train loss:  0.620449423789978
train gradient:  0.21230543018146936
iteration : 928
train acc:  0.6875
train loss:  0.5687817335128784
train gradient:  0.14468497233507555
iteration : 929
train acc:  0.671875
train loss:  0.5412746071815491
train gradient:  0.16938969778494684
iteration : 930
train acc:  0.71875
train loss:  0.5451229810714722
train gradient:  0.12803492635085884
iteration : 931
train acc:  0.7265625
train loss:  0.5508909821510315
train gradient:  0.14309352689236698
iteration : 932
train acc:  0.7265625
train loss:  0.5374084711074829
train gradient:  0.19616051788068073
iteration : 933
train acc:  0.6328125
train loss:  0.6506825685501099
train gradient:  0.21481555549276166
iteration : 934
train acc:  0.703125
train loss:  0.5832840204238892
train gradient:  0.1698630973334327
iteration : 935
train acc:  0.71875
train loss:  0.5180256366729736
train gradient:  0.13981379198286648
iteration : 936
train acc:  0.6953125
train loss:  0.5363848209381104
train gradient:  0.202637403735553
iteration : 937
train acc:  0.71875
train loss:  0.5504365563392639
train gradient:  0.15635981686435518
iteration : 938
train acc:  0.7421875
train loss:  0.5289275050163269
train gradient:  0.11146618905472513
iteration : 939
train acc:  0.6953125
train loss:  0.5715894103050232
train gradient:  0.14475381378190477
iteration : 940
train acc:  0.6875
train loss:  0.5691640377044678
train gradient:  0.15193302333112357
iteration : 941
train acc:  0.6640625
train loss:  0.5789150595664978
train gradient:  0.19727851827591658
iteration : 942
train acc:  0.65625
train loss:  0.6511526703834534
train gradient:  0.2101965635437881
iteration : 943
train acc:  0.7734375
train loss:  0.5106418132781982
train gradient:  0.12805732620982152
iteration : 944
train acc:  0.671875
train loss:  0.5831164121627808
train gradient:  0.16759214143188095
iteration : 945
train acc:  0.6953125
train loss:  0.5882923603057861
train gradient:  0.18433982641938565
iteration : 946
train acc:  0.6796875
train loss:  0.568943977355957
train gradient:  0.13731955315993166
iteration : 947
train acc:  0.7109375
train loss:  0.5505014657974243
train gradient:  0.17489625397620054
iteration : 948
train acc:  0.6328125
train loss:  0.5935177803039551
train gradient:  0.18265323804150974
iteration : 949
train acc:  0.625
train loss:  0.6038894653320312
train gradient:  0.16940099356269261
iteration : 950
train acc:  0.671875
train loss:  0.5816569328308105
train gradient:  0.17622722945347985
iteration : 951
train acc:  0.65625
train loss:  0.5815271139144897
train gradient:  0.22758862228446858
iteration : 952
train acc:  0.671875
train loss:  0.5614242553710938
train gradient:  0.14976050612555444
iteration : 953
train acc:  0.671875
train loss:  0.5959498286247253
train gradient:  0.13774980182649266
iteration : 954
train acc:  0.6875
train loss:  0.5458886623382568
train gradient:  0.17364088254951088
iteration : 955
train acc:  0.703125
train loss:  0.556891679763794
train gradient:  0.15249103202931052
iteration : 956
train acc:  0.6484375
train loss:  0.6278991103172302
train gradient:  0.20460371739958583
iteration : 957
train acc:  0.6484375
train loss:  0.6187546253204346
train gradient:  0.17570682951344554
iteration : 958
train acc:  0.6875
train loss:  0.6022387742996216
train gradient:  0.23483508904448083
iteration : 959
train acc:  0.65625
train loss:  0.6276934146881104
train gradient:  0.16473747404259628
iteration : 960
train acc:  0.6640625
train loss:  0.5812026858329773
train gradient:  0.14066197162575703
iteration : 961
train acc:  0.640625
train loss:  0.6224942803382874
train gradient:  0.14596952890682655
iteration : 962
train acc:  0.703125
train loss:  0.5676150321960449
train gradient:  0.14296969708685953
iteration : 963
train acc:  0.7109375
train loss:  0.551436722278595
train gradient:  0.17677972692561889
iteration : 964
train acc:  0.7578125
train loss:  0.5538680553436279
train gradient:  0.19321627388703727
iteration : 965
train acc:  0.6640625
train loss:  0.5894595384597778
train gradient:  0.16869834725990634
iteration : 966
train acc:  0.640625
train loss:  0.6262838840484619
train gradient:  0.2229332335323234
iteration : 967
train acc:  0.6796875
train loss:  0.5588422417640686
train gradient:  0.175698084261246
iteration : 968
train acc:  0.6640625
train loss:  0.5978862047195435
train gradient:  0.19454240190284325
iteration : 969
train acc:  0.6484375
train loss:  0.6315552592277527
train gradient:  0.20499436653972386
iteration : 970
train acc:  0.625
train loss:  0.6053590774536133
train gradient:  0.17410336001127477
iteration : 971
train acc:  0.78125
train loss:  0.5444197058677673
train gradient:  0.16259818164331516
iteration : 972
train acc:  0.65625
train loss:  0.5732212066650391
train gradient:  0.20920754256692892
iteration : 973
train acc:  0.6953125
train loss:  0.5700385570526123
train gradient:  0.1763321299738062
iteration : 974
train acc:  0.703125
train loss:  0.5834879875183105
train gradient:  0.21222913876684352
iteration : 975
train acc:  0.6640625
train loss:  0.5665585398674011
train gradient:  0.14660422133911288
iteration : 976
train acc:  0.6953125
train loss:  0.5519310235977173
train gradient:  0.17324704530052457
iteration : 977
train acc:  0.7421875
train loss:  0.5672076940536499
train gradient:  0.147064039495473
iteration : 978
train acc:  0.640625
train loss:  0.5546754598617554
train gradient:  0.2134432075912674
iteration : 979
train acc:  0.6484375
train loss:  0.5812888145446777
train gradient:  0.1969283318432575
iteration : 980
train acc:  0.703125
train loss:  0.5552859902381897
train gradient:  0.23774910974887487
iteration : 981
train acc:  0.6796875
train loss:  0.5880011916160583
train gradient:  0.1443269887377988
iteration : 982
train acc:  0.734375
train loss:  0.5217485427856445
train gradient:  0.14747028924906902
iteration : 983
train acc:  0.671875
train loss:  0.5608301162719727
train gradient:  0.18932888417554383
iteration : 984
train acc:  0.6953125
train loss:  0.535190224647522
train gradient:  0.12661690812699228
iteration : 985
train acc:  0.703125
train loss:  0.5681408643722534
train gradient:  0.10643184910024768
iteration : 986
train acc:  0.6796875
train loss:  0.5356324911117554
train gradient:  0.11443796458382838
iteration : 987
train acc:  0.6796875
train loss:  0.6316720843315125
train gradient:  0.16623955272806154
iteration : 988
train acc:  0.6953125
train loss:  0.5698301792144775
train gradient:  0.14795271012987327
iteration : 989
train acc:  0.7421875
train loss:  0.5414409637451172
train gradient:  0.11519572139596032
iteration : 990
train acc:  0.6796875
train loss:  0.5894713997840881
train gradient:  0.16067133013449994
iteration : 991
train acc:  0.7265625
train loss:  0.5656931400299072
train gradient:  0.15635593561859257
iteration : 992
train acc:  0.7734375
train loss:  0.5085604190826416
train gradient:  0.1378567873892378
iteration : 993
train acc:  0.6015625
train loss:  0.6097689270973206
train gradient:  0.19820774939518315
iteration : 994
train acc:  0.7109375
train loss:  0.5541263818740845
train gradient:  0.19454003740493153
iteration : 995
train acc:  0.640625
train loss:  0.6160921454429626
train gradient:  0.22641975910533924
iteration : 996
train acc:  0.6640625
train loss:  0.5983073711395264
train gradient:  0.15983455191188295
iteration : 997
train acc:  0.6640625
train loss:  0.5624179840087891
train gradient:  0.12946342032670322
iteration : 998
train acc:  0.6796875
train loss:  0.6009756326675415
train gradient:  0.16823145703873899
iteration : 999
train acc:  0.6875
train loss:  0.590768575668335
train gradient:  0.1831734507679489
iteration : 1000
train acc:  0.6875
train loss:  0.6019543409347534
train gradient:  0.1790561053353713
iteration : 1001
train acc:  0.7109375
train loss:  0.5519407987594604
train gradient:  0.18466618885117056
iteration : 1002
train acc:  0.7578125
train loss:  0.5132821798324585
train gradient:  0.20431424721354985
iteration : 1003
train acc:  0.65625
train loss:  0.6147350072860718
train gradient:  0.2565755939317341
iteration : 1004
train acc:  0.7109375
train loss:  0.5499884486198425
train gradient:  0.20416391679953877
iteration : 1005
train acc:  0.703125
train loss:  0.5758808851242065
train gradient:  0.1342426214057003
iteration : 1006
train acc:  0.6640625
train loss:  0.5687349438667297
train gradient:  0.13072722251817018
iteration : 1007
train acc:  0.75
train loss:  0.5341191291809082
train gradient:  0.13835649063551864
iteration : 1008
train acc:  0.734375
train loss:  0.5227349996566772
train gradient:  0.16277421613128654
iteration : 1009
train acc:  0.703125
train loss:  0.5889526605606079
train gradient:  0.20474321982362664
iteration : 1010
train acc:  0.796875
train loss:  0.45495766401290894
train gradient:  0.11388578638414654
iteration : 1011
train acc:  0.7109375
train loss:  0.5380532145500183
train gradient:  0.12992361009268613
iteration : 1012
train acc:  0.7109375
train loss:  0.5433946847915649
train gradient:  0.17712587603203067
iteration : 1013
train acc:  0.734375
train loss:  0.5550092458724976
train gradient:  0.12341972825541815
iteration : 1014
train acc:  0.671875
train loss:  0.5887106657028198
train gradient:  0.21590089156493608
iteration : 1015
train acc:  0.6875
train loss:  0.5924365520477295
train gradient:  0.16707541744363658
iteration : 1016
train acc:  0.6640625
train loss:  0.6002294421195984
train gradient:  0.16909168393212304
iteration : 1017
train acc:  0.7265625
train loss:  0.5475730895996094
train gradient:  0.1946762392818785
iteration : 1018
train acc:  0.6484375
train loss:  0.5856342315673828
train gradient:  0.13634301583014222
iteration : 1019
train acc:  0.59375
train loss:  0.6808401942253113
train gradient:  0.22974110183364269
iteration : 1020
train acc:  0.609375
train loss:  0.6198955774307251
train gradient:  0.18650971661395438
iteration : 1021
train acc:  0.75
train loss:  0.5606962442398071
train gradient:  0.1425137715188624
iteration : 1022
train acc:  0.7578125
train loss:  0.5354912877082825
train gradient:  0.15138725052303328
iteration : 1023
train acc:  0.6796875
train loss:  0.6104707717895508
train gradient:  0.20157582680684194
iteration : 1024
train acc:  0.6953125
train loss:  0.577313244342804
train gradient:  0.150664707070314
iteration : 1025
train acc:  0.6953125
train loss:  0.5773268342018127
train gradient:  0.13042502276498746
iteration : 1026
train acc:  0.6171875
train loss:  0.6121622323989868
train gradient:  0.18470308610706193
iteration : 1027
train acc:  0.7578125
train loss:  0.5191548466682434
train gradient:  0.11574413768018635
iteration : 1028
train acc:  0.6953125
train loss:  0.5702686905860901
train gradient:  0.2521398071290818
iteration : 1029
train acc:  0.765625
train loss:  0.48592185974121094
train gradient:  0.19123672495363273
iteration : 1030
train acc:  0.7109375
train loss:  0.5232594013214111
train gradient:  0.14548236247347296
iteration : 1031
train acc:  0.6875
train loss:  0.5847633481025696
train gradient:  0.16852162100233056
iteration : 1032
train acc:  0.6640625
train loss:  0.601902425289154
train gradient:  0.17771726863321913
iteration : 1033
train acc:  0.703125
train loss:  0.5663406252861023
train gradient:  0.16763914510870812
iteration : 1034
train acc:  0.609375
train loss:  0.6549320816993713
train gradient:  0.1945007256515321
iteration : 1035
train acc:  0.7734375
train loss:  0.5085023641586304
train gradient:  0.13873476544829955
iteration : 1036
train acc:  0.6640625
train loss:  0.5811123251914978
train gradient:  0.17423327824149493
iteration : 1037
train acc:  0.625
train loss:  0.6152819991111755
train gradient:  0.1523527862609922
iteration : 1038
train acc:  0.703125
train loss:  0.5695676207542419
train gradient:  0.14818645718872064
iteration : 1039
train acc:  0.65625
train loss:  0.6091963648796082
train gradient:  0.1912699745538323
iteration : 1040
train acc:  0.6640625
train loss:  0.589664101600647
train gradient:  0.15362205070033363
iteration : 1041
train acc:  0.6796875
train loss:  0.5695850849151611
train gradient:  0.15710616836189834
iteration : 1042
train acc:  0.7421875
train loss:  0.536763072013855
train gradient:  0.1390957734684587
iteration : 1043
train acc:  0.7109375
train loss:  0.5694678425788879
train gradient:  0.12331075271739378
iteration : 1044
train acc:  0.6484375
train loss:  0.6682687997817993
train gradient:  0.26133697163762254
iteration : 1045
train acc:  0.7109375
train loss:  0.5400066375732422
train gradient:  0.16028983494799925
iteration : 1046
train acc:  0.6328125
train loss:  0.6402369737625122
train gradient:  0.2041225190671318
iteration : 1047
train acc:  0.7109375
train loss:  0.5637161731719971
train gradient:  0.18552879389841664
iteration : 1048
train acc:  0.703125
train loss:  0.5773488283157349
train gradient:  0.195697586004615
iteration : 1049
train acc:  0.625
train loss:  0.5717787742614746
train gradient:  0.1760819142437588
iteration : 1050
train acc:  0.625
train loss:  0.5714088678359985
train gradient:  0.19410047542010625
iteration : 1051
train acc:  0.75
train loss:  0.5781254768371582
train gradient:  0.23611813302338316
iteration : 1052
train acc:  0.7265625
train loss:  0.5358486175537109
train gradient:  0.17615155732084326
iteration : 1053
train acc:  0.6953125
train loss:  0.527462899684906
train gradient:  0.14103909845232757
iteration : 1054
train acc:  0.765625
train loss:  0.5349233746528625
train gradient:  0.1410732895639667
iteration : 1055
train acc:  0.6484375
train loss:  0.594855010509491
train gradient:  0.16095719563169933
iteration : 1056
train acc:  0.640625
train loss:  0.5866864919662476
train gradient:  0.23158849266974257
iteration : 1057
train acc:  0.7109375
train loss:  0.5366215705871582
train gradient:  0.12005730707633495
iteration : 1058
train acc:  0.671875
train loss:  0.5746070146560669
train gradient:  0.16150457480495242
iteration : 1059
train acc:  0.703125
train loss:  0.5394781231880188
train gradient:  0.12367018338003512
iteration : 1060
train acc:  0.7734375
train loss:  0.47811436653137207
train gradient:  0.11997463324833323
iteration : 1061
train acc:  0.6796875
train loss:  0.5830099582672119
train gradient:  0.15147544126459783
iteration : 1062
train acc:  0.6953125
train loss:  0.5763924717903137
train gradient:  0.16352624343667138
iteration : 1063
train acc:  0.6875
train loss:  0.5429596304893494
train gradient:  0.12171682913835383
iteration : 1064
train acc:  0.7265625
train loss:  0.5232726335525513
train gradient:  0.24397622602568325
iteration : 1065
train acc:  0.6796875
train loss:  0.580639123916626
train gradient:  0.2183532872974912
iteration : 1066
train acc:  0.6640625
train loss:  0.6003946661949158
train gradient:  0.24726790939035037
iteration : 1067
train acc:  0.6796875
train loss:  0.5998584628105164
train gradient:  0.19899522284997145
iteration : 1068
train acc:  0.75
train loss:  0.5454380512237549
train gradient:  0.11891936442707804
iteration : 1069
train acc:  0.6953125
train loss:  0.5461438894271851
train gradient:  0.16182808242968572
iteration : 1070
train acc:  0.6875
train loss:  0.5539536476135254
train gradient:  0.17592466880595087
iteration : 1071
train acc:  0.7265625
train loss:  0.5161126255989075
train gradient:  0.13181470237147133
iteration : 1072
train acc:  0.75
train loss:  0.5277862548828125
train gradient:  0.14034570462506032
iteration : 1073
train acc:  0.734375
train loss:  0.530917763710022
train gradient:  0.1464888528385552
iteration : 1074
train acc:  0.671875
train loss:  0.5701100826263428
train gradient:  0.2366108674461178
iteration : 1075
train acc:  0.7421875
train loss:  0.5383149981498718
train gradient:  0.13756783906542136
iteration : 1076
train acc:  0.6875
train loss:  0.5534710884094238
train gradient:  0.15087799353087286
iteration : 1077
train acc:  0.625
train loss:  0.5987663269042969
train gradient:  0.20971353085721722
iteration : 1078
train acc:  0.6953125
train loss:  0.5628554821014404
train gradient:  0.1312984454311611
iteration : 1079
train acc:  0.6484375
train loss:  0.5809729099273682
train gradient:  0.1385591167206242
iteration : 1080
train acc:  0.734375
train loss:  0.5650852918624878
train gradient:  0.18395417618771653
iteration : 1081
train acc:  0.609375
train loss:  0.5959571599960327
train gradient:  0.18353460539894137
iteration : 1082
train acc:  0.6953125
train loss:  0.5722965598106384
train gradient:  0.2053093624687531
iteration : 1083
train acc:  0.6953125
train loss:  0.5672227144241333
train gradient:  0.27317628741459776
iteration : 1084
train acc:  0.7109375
train loss:  0.5280190110206604
train gradient:  0.12352646057815182
iteration : 1085
train acc:  0.671875
train loss:  0.5537160634994507
train gradient:  0.16716601546650733
iteration : 1086
train acc:  0.6875
train loss:  0.5291898250579834
train gradient:  0.1453596713062651
iteration : 1087
train acc:  0.7421875
train loss:  0.5443293452262878
train gradient:  0.1378793616971702
iteration : 1088
train acc:  0.6953125
train loss:  0.5374900698661804
train gradient:  0.2522343440254175
iteration : 1089
train acc:  0.6640625
train loss:  0.575462818145752
train gradient:  0.12617568844772692
iteration : 1090
train acc:  0.734375
train loss:  0.5355961322784424
train gradient:  0.2596585608237835
iteration : 1091
train acc:  0.71875
train loss:  0.5457025766372681
train gradient:  0.14999456184249133
iteration : 1092
train acc:  0.6484375
train loss:  0.6050252318382263
train gradient:  0.18897225799743428
iteration : 1093
train acc:  0.7421875
train loss:  0.5493117570877075
train gradient:  0.17916461582930035
iteration : 1094
train acc:  0.71875
train loss:  0.5268403887748718
train gradient:  0.12079235311640299
iteration : 1095
train acc:  0.703125
train loss:  0.5123085379600525
train gradient:  0.11438819716211712
iteration : 1096
train acc:  0.671875
train loss:  0.5392072796821594
train gradient:  0.15542953811270804
iteration : 1097
train acc:  0.6875
train loss:  0.5836225152015686
train gradient:  0.19391627977538456
iteration : 1098
train acc:  0.796875
train loss:  0.4741738438606262
train gradient:  0.1512687085904134
iteration : 1099
train acc:  0.671875
train loss:  0.5580541491508484
train gradient:  0.17716303500364977
iteration : 1100
train acc:  0.6953125
train loss:  0.5455267429351807
train gradient:  0.15970636689465967
iteration : 1101
train acc:  0.7265625
train loss:  0.5384761691093445
train gradient:  0.1258860154633058
iteration : 1102
train acc:  0.6875
train loss:  0.5715354681015015
train gradient:  0.14159972136902726
iteration : 1103
train acc:  0.6796875
train loss:  0.6323327422142029
train gradient:  0.293903145122752
iteration : 1104
train acc:  0.7109375
train loss:  0.5568023920059204
train gradient:  0.19817156768529878
iteration : 1105
train acc:  0.7265625
train loss:  0.5256531238555908
train gradient:  0.14466392623865026
iteration : 1106
train acc:  0.6875
train loss:  0.5579614639282227
train gradient:  0.1574329635553615
iteration : 1107
train acc:  0.75
train loss:  0.5078510046005249
train gradient:  0.1449454649810188
iteration : 1108
train acc:  0.6484375
train loss:  0.5836095809936523
train gradient:  0.16825206734592899
iteration : 1109
train acc:  0.6640625
train loss:  0.6111001968383789
train gradient:  0.2559684622295596
iteration : 1110
train acc:  0.7421875
train loss:  0.5263210535049438
train gradient:  0.14636988100793163
iteration : 1111
train acc:  0.703125
train loss:  0.5328011512756348
train gradient:  0.14742947323824412
iteration : 1112
train acc:  0.6875
train loss:  0.6050955057144165
train gradient:  0.2667937499050783
iteration : 1113
train acc:  0.6796875
train loss:  0.5497547388076782
train gradient:  0.19109796342875363
iteration : 1114
train acc:  0.6796875
train loss:  0.5841372013092041
train gradient:  0.14389411673832891
iteration : 1115
train acc:  0.671875
train loss:  0.5684583783149719
train gradient:  0.17848446240710497
iteration : 1116
train acc:  0.6171875
train loss:  0.6363071203231812
train gradient:  0.16932417671642183
iteration : 1117
train acc:  0.75
train loss:  0.525460422039032
train gradient:  0.12822960872423522
iteration : 1118
train acc:  0.7734375
train loss:  0.5044887661933899
train gradient:  0.16527906513666357
iteration : 1119
train acc:  0.65625
train loss:  0.6025040149688721
train gradient:  0.19314200821614702
iteration : 1120
train acc:  0.75
train loss:  0.520961582660675
train gradient:  0.1864961110932159
iteration : 1121
train acc:  0.6796875
train loss:  0.5773597955703735
train gradient:  0.2154765733611405
iteration : 1122
train acc:  0.71875
train loss:  0.532124936580658
train gradient:  0.18260411983485703
iteration : 1123
train acc:  0.6875
train loss:  0.5792917013168335
train gradient:  0.15253225413496652
iteration : 1124
train acc:  0.703125
train loss:  0.5332748889923096
train gradient:  0.21436001557240253
iteration : 1125
train acc:  0.7265625
train loss:  0.5603609681129456
train gradient:  0.21226401654690386
iteration : 1126
train acc:  0.6875
train loss:  0.5613794326782227
train gradient:  0.16024720781856144
iteration : 1127
train acc:  0.7109375
train loss:  0.5418227314949036
train gradient:  0.1925146406523744
iteration : 1128
train acc:  0.6796875
train loss:  0.5623242855072021
train gradient:  0.1483222953761572
iteration : 1129
train acc:  0.671875
train loss:  0.5858197808265686
train gradient:  0.1342230939019406
iteration : 1130
train acc:  0.71875
train loss:  0.5442115068435669
train gradient:  0.18092797862116927
iteration : 1131
train acc:  0.7578125
train loss:  0.4991210103034973
train gradient:  0.12250948988912126
iteration : 1132
train acc:  0.71875
train loss:  0.5417003035545349
train gradient:  0.18294043061559406
iteration : 1133
train acc:  0.6875
train loss:  0.549601674079895
train gradient:  0.15293539567761072
iteration : 1134
train acc:  0.703125
train loss:  0.5532170534133911
train gradient:  0.1351633058646658
iteration : 1135
train acc:  0.671875
train loss:  0.617794930934906
train gradient:  0.16581408229549272
iteration : 1136
train acc:  0.703125
train loss:  0.5516898036003113
train gradient:  0.15719437845823853
iteration : 1137
train acc:  0.71875
train loss:  0.5306140780448914
train gradient:  0.13841188781415442
iteration : 1138
train acc:  0.6640625
train loss:  0.562049388885498
train gradient:  0.16281202698626518
iteration : 1139
train acc:  0.71875
train loss:  0.5287296175956726
train gradient:  0.1508528628159534
iteration : 1140
train acc:  0.703125
train loss:  0.5516680479049683
train gradient:  0.16078890611578223
iteration : 1141
train acc:  0.6484375
train loss:  0.6115846633911133
train gradient:  0.17396726108951815
iteration : 1142
train acc:  0.65625
train loss:  0.5987622737884521
train gradient:  0.20937652274707935
iteration : 1143
train acc:  0.6796875
train loss:  0.594965934753418
train gradient:  0.14236286140958726
iteration : 1144
train acc:  0.6796875
train loss:  0.5609856843948364
train gradient:  0.14995065670027297
iteration : 1145
train acc:  0.6796875
train loss:  0.6239648461341858
train gradient:  0.22763238530620644
iteration : 1146
train acc:  0.734375
train loss:  0.5189338326454163
train gradient:  0.1505273475626695
iteration : 1147
train acc:  0.703125
train loss:  0.5374470949172974
train gradient:  0.16371652629317976
iteration : 1148
train acc:  0.6640625
train loss:  0.6170077323913574
train gradient:  0.2522985275698745
iteration : 1149
train acc:  0.6171875
train loss:  0.6276666522026062
train gradient:  0.17097988821481558
iteration : 1150
train acc:  0.6796875
train loss:  0.5554120540618896
train gradient:  0.13878659503339585
iteration : 1151
train acc:  0.7265625
train loss:  0.4756525158882141
train gradient:  0.11307390917509479
iteration : 1152
train acc:  0.6484375
train loss:  0.6311248540878296
train gradient:  0.21655308227433162
iteration : 1153
train acc:  0.7421875
train loss:  0.4916953146457672
train gradient:  0.12634558629441828
iteration : 1154
train acc:  0.7421875
train loss:  0.5518704652786255
train gradient:  0.21727145740920764
iteration : 1155
train acc:  0.6328125
train loss:  0.6131672859191895
train gradient:  0.2144912334800547
iteration : 1156
train acc:  0.640625
train loss:  0.5873920917510986
train gradient:  0.23859331497150826
iteration : 1157
train acc:  0.703125
train loss:  0.5829191207885742
train gradient:  0.18787254434369174
iteration : 1158
train acc:  0.6953125
train loss:  0.5143563151359558
train gradient:  0.1422384953847899
iteration : 1159
train acc:  0.7109375
train loss:  0.5254098176956177
train gradient:  0.12711594762948233
iteration : 1160
train acc:  0.6796875
train loss:  0.5655919313430786
train gradient:  0.18992714814089962
iteration : 1161
train acc:  0.7109375
train loss:  0.5182509422302246
train gradient:  0.14125059985909127
iteration : 1162
train acc:  0.6875
train loss:  0.5784370303153992
train gradient:  0.204558156250236
iteration : 1163
train acc:  0.6171875
train loss:  0.6465772390365601
train gradient:  0.1922815740885444
iteration : 1164
train acc:  0.7265625
train loss:  0.5909212827682495
train gradient:  0.15657185217079028
iteration : 1165
train acc:  0.734375
train loss:  0.5054588317871094
train gradient:  0.16855782870161284
iteration : 1166
train acc:  0.640625
train loss:  0.5803408622741699
train gradient:  0.16270129660080684
iteration : 1167
train acc:  0.6875
train loss:  0.5822146534919739
train gradient:  0.1703150320269154
iteration : 1168
train acc:  0.75
train loss:  0.5250037908554077
train gradient:  0.16172245466129093
iteration : 1169
train acc:  0.6640625
train loss:  0.5615667700767517
train gradient:  0.2030881762246618
iteration : 1170
train acc:  0.65625
train loss:  0.5872969031333923
train gradient:  0.1381080914710005
iteration : 1171
train acc:  0.7109375
train loss:  0.5760074257850647
train gradient:  0.2122784878934812
iteration : 1172
train acc:  0.75
train loss:  0.5389453172683716
train gradient:  0.14609534446886369
iteration : 1173
train acc:  0.7421875
train loss:  0.5596014261245728
train gradient:  0.24245053544676815
iteration : 1174
train acc:  0.640625
train loss:  0.5956501960754395
train gradient:  0.20868403207303893
iteration : 1175
train acc:  0.6875
train loss:  0.5768129825592041
train gradient:  0.15056023169278232
iteration : 1176
train acc:  0.6875
train loss:  0.5578556060791016
train gradient:  0.1581866315248253
iteration : 1177
train acc:  0.7265625
train loss:  0.5366878509521484
train gradient:  0.1702522214603937
iteration : 1178
train acc:  0.6640625
train loss:  0.5296224355697632
train gradient:  0.1244464242354144
iteration : 1179
train acc:  0.6796875
train loss:  0.5783321857452393
train gradient:  0.1601257509849212
iteration : 1180
train acc:  0.71875
train loss:  0.5779957175254822
train gradient:  0.16258290912697443
iteration : 1181
train acc:  0.6640625
train loss:  0.6029740571975708
train gradient:  0.1954025634256236
iteration : 1182
train acc:  0.71875
train loss:  0.5415372848510742
train gradient:  0.17936960631567392
iteration : 1183
train acc:  0.7109375
train loss:  0.5909811854362488
train gradient:  0.19061157996035483
iteration : 1184
train acc:  0.7265625
train loss:  0.567218005657196
train gradient:  0.2191023899388852
iteration : 1185
train acc:  0.6640625
train loss:  0.6117578744888306
train gradient:  0.2605146687991349
iteration : 1186
train acc:  0.6875
train loss:  0.5798413753509521
train gradient:  0.14231870628820453
iteration : 1187
train acc:  0.7265625
train loss:  0.5243228673934937
train gradient:  0.13680962689327314
iteration : 1188
train acc:  0.6015625
train loss:  0.6613917946815491
train gradient:  0.2860994170760715
iteration : 1189
train acc:  0.6953125
train loss:  0.5715892314910889
train gradient:  0.1520790565867847
iteration : 1190
train acc:  0.703125
train loss:  0.5757770538330078
train gradient:  0.1652319420698402
iteration : 1191
train acc:  0.6484375
train loss:  0.5882759094238281
train gradient:  0.19477644534110455
iteration : 1192
train acc:  0.6796875
train loss:  0.5704894065856934
train gradient:  0.1800061025417506
iteration : 1193
train acc:  0.6484375
train loss:  0.5552010536193848
train gradient:  0.13032722249226872
iteration : 1194
train acc:  0.78125
train loss:  0.4807676076889038
train gradient:  0.14398546777645244
iteration : 1195
train acc:  0.7734375
train loss:  0.4810168743133545
train gradient:  0.15781479343122473
iteration : 1196
train acc:  0.703125
train loss:  0.5731865763664246
train gradient:  0.21238978455769006
iteration : 1197
train acc:  0.765625
train loss:  0.5608479976654053
train gradient:  0.21573739437568504
iteration : 1198
train acc:  0.640625
train loss:  0.5848299264907837
train gradient:  0.13294563175605928
iteration : 1199
train acc:  0.78125
train loss:  0.508172869682312
train gradient:  0.1348616957269625
iteration : 1200
train acc:  0.6484375
train loss:  0.6020901799201965
train gradient:  0.15617055834257348
iteration : 1201
train acc:  0.703125
train loss:  0.5762932300567627
train gradient:  0.1841572904856615
iteration : 1202
train acc:  0.671875
train loss:  0.5440465211868286
train gradient:  0.12680030300136605
iteration : 1203
train acc:  0.6328125
train loss:  0.596452534198761
train gradient:  0.20346581792542912
iteration : 1204
train acc:  0.671875
train loss:  0.5733964443206787
train gradient:  0.16377398609515242
iteration : 1205
train acc:  0.765625
train loss:  0.5199871063232422
train gradient:  0.14149940744813527
iteration : 1206
train acc:  0.671875
train loss:  0.5665255784988403
train gradient:  0.1814358024996221
iteration : 1207
train acc:  0.796875
train loss:  0.485014408826828
train gradient:  0.1585508625632825
iteration : 1208
train acc:  0.671875
train loss:  0.5740962624549866
train gradient:  0.14970168327752284
iteration : 1209
train acc:  0.6484375
train loss:  0.6006895899772644
train gradient:  0.1562574474944613
iteration : 1210
train acc:  0.640625
train loss:  0.619597315788269
train gradient:  0.21486963333022882
iteration : 1211
train acc:  0.6015625
train loss:  0.7214140892028809
train gradient:  0.31607807852807385
iteration : 1212
train acc:  0.734375
train loss:  0.5149648189544678
train gradient:  0.18818568971338095
iteration : 1213
train acc:  0.7421875
train loss:  0.5752617716789246
train gradient:  0.17808099171192593
iteration : 1214
train acc:  0.6875
train loss:  0.5991920232772827
train gradient:  0.147458708472059
iteration : 1215
train acc:  0.71875
train loss:  0.5930211544036865
train gradient:  0.16898715585272378
iteration : 1216
train acc:  0.734375
train loss:  0.5261163711547852
train gradient:  0.166737617720193
iteration : 1217
train acc:  0.71875
train loss:  0.5665608048439026
train gradient:  0.15525338429353036
iteration : 1218
train acc:  0.6640625
train loss:  0.5861421227455139
train gradient:  0.1505068468320822
iteration : 1219
train acc:  0.65625
train loss:  0.5704981088638306
train gradient:  0.15165501079373772
iteration : 1220
train acc:  0.703125
train loss:  0.533239483833313
train gradient:  0.11981983346685837
iteration : 1221
train acc:  0.71875
train loss:  0.5317028760910034
train gradient:  0.13229546670931946
iteration : 1222
train acc:  0.625
train loss:  0.6077816486358643
train gradient:  0.16656070746333354
iteration : 1223
train acc:  0.6796875
train loss:  0.5553493499755859
train gradient:  0.13830311268620504
iteration : 1224
train acc:  0.6875
train loss:  0.607144832611084
train gradient:  0.1568811654765216
iteration : 1225
train acc:  0.703125
train loss:  0.5853211283683777
train gradient:  0.17548529303180155
iteration : 1226
train acc:  0.6328125
train loss:  0.6676504015922546
train gradient:  0.2822863598872703
iteration : 1227
train acc:  0.65625
train loss:  0.5700218677520752
train gradient:  0.13355700915785734
iteration : 1228
train acc:  0.640625
train loss:  0.6005951762199402
train gradient:  0.21458944245920647
iteration : 1229
train acc:  0.6796875
train loss:  0.6102321147918701
train gradient:  0.20154670861835597
iteration : 1230
train acc:  0.7109375
train loss:  0.5465881824493408
train gradient:  0.13239106771573933
iteration : 1231
train acc:  0.6640625
train loss:  0.5763344168663025
train gradient:  0.17601159188796017
iteration : 1232
train acc:  0.6875
train loss:  0.5615261793136597
train gradient:  0.2575038466815718
iteration : 1233
train acc:  0.6484375
train loss:  0.541062593460083
train gradient:  0.14221424001958138
iteration : 1234
train acc:  0.7578125
train loss:  0.5459705591201782
train gradient:  0.13408453944996818
iteration : 1235
train acc:  0.734375
train loss:  0.4971645176410675
train gradient:  0.1335298338658554
iteration : 1236
train acc:  0.6484375
train loss:  0.5936833620071411
train gradient:  0.17824530970043295
iteration : 1237
train acc:  0.671875
train loss:  0.5806779861450195
train gradient:  0.14946846411191822
iteration : 1238
train acc:  0.703125
train loss:  0.5283400416374207
train gradient:  0.12685519754331914
iteration : 1239
train acc:  0.6875
train loss:  0.584848165512085
train gradient:  0.19773750618836455
iteration : 1240
train acc:  0.6796875
train loss:  0.5546145439147949
train gradient:  0.17864626678841308
iteration : 1241
train acc:  0.7109375
train loss:  0.5771050453186035
train gradient:  0.20391849451279742
iteration : 1242
train acc:  0.65625
train loss:  0.5615936517715454
train gradient:  0.1427165146294236
iteration : 1243
train acc:  0.6953125
train loss:  0.5384054780006409
train gradient:  0.10765552306694857
iteration : 1244
train acc:  0.671875
train loss:  0.6169841289520264
train gradient:  0.25609305530467735
iteration : 1245
train acc:  0.6953125
train loss:  0.5562986135482788
train gradient:  0.14667691587887233
iteration : 1246
train acc:  0.6875
train loss:  0.5941576361656189
train gradient:  0.23564117611530286
iteration : 1247
train acc:  0.6953125
train loss:  0.5723292827606201
train gradient:  0.1659681982970132
iteration : 1248
train acc:  0.65625
train loss:  0.5704697370529175
train gradient:  0.1682082996589493
iteration : 1249
train acc:  0.7265625
train loss:  0.5568042993545532
train gradient:  0.14450562044097287
iteration : 1250
train acc:  0.65625
train loss:  0.6241939067840576
train gradient:  0.2010159787746052
iteration : 1251
train acc:  0.703125
train loss:  0.5671660900115967
train gradient:  0.20647600183606163
iteration : 1252
train acc:  0.6953125
train loss:  0.5581450462341309
train gradient:  0.14945452820509636
iteration : 1253
train acc:  0.6875
train loss:  0.5374589562416077
train gradient:  0.11903518993292578
iteration : 1254
train acc:  0.7109375
train loss:  0.5659856796264648
train gradient:  0.1675420794657298
iteration : 1255
train acc:  0.6171875
train loss:  0.5759314298629761
train gradient:  0.16680835836553531
iteration : 1256
train acc:  0.671875
train loss:  0.5694043636322021
train gradient:  0.18165177408584893
iteration : 1257
train acc:  0.703125
train loss:  0.6139554977416992
train gradient:  0.14863388530635707
iteration : 1258
train acc:  0.7109375
train loss:  0.5578325390815735
train gradient:  0.12448294621854777
iteration : 1259
train acc:  0.7265625
train loss:  0.5498115420341492
train gradient:  0.1362358750566927
iteration : 1260
train acc:  0.6171875
train loss:  0.6298258304595947
train gradient:  0.20517955380275188
iteration : 1261
train acc:  0.6953125
train loss:  0.565158486366272
train gradient:  0.18543059546149415
iteration : 1262
train acc:  0.7421875
train loss:  0.5046148300170898
train gradient:  0.11160824751267183
iteration : 1263
train acc:  0.6015625
train loss:  0.6489148139953613
train gradient:  0.2608706209296947
iteration : 1264
train acc:  0.703125
train loss:  0.58130943775177
train gradient:  0.1627381197038733
iteration : 1265
train acc:  0.625
train loss:  0.599899411201477
train gradient:  0.1848760301427045
iteration : 1266
train acc:  0.6953125
train loss:  0.5755135416984558
train gradient:  0.15470389131105713
iteration : 1267
train acc:  0.6796875
train loss:  0.5432769060134888
train gradient:  0.12115199145907055
iteration : 1268
train acc:  0.703125
train loss:  0.5394487380981445
train gradient:  0.13567156460133445
iteration : 1269
train acc:  0.6796875
train loss:  0.5620786547660828
train gradient:  0.13493686782348102
iteration : 1270
train acc:  0.6640625
train loss:  0.5617668032646179
train gradient:  0.17454160538558194
iteration : 1271
train acc:  0.6875
train loss:  0.58034348487854
train gradient:  0.1466186014258865
iteration : 1272
train acc:  0.671875
train loss:  0.5872560143470764
train gradient:  0.1643981935034906
iteration : 1273
train acc:  0.7578125
train loss:  0.4884006977081299
train gradient:  0.22188790850927181
iteration : 1274
train acc:  0.7109375
train loss:  0.548039972782135
train gradient:  0.17453494221893662
iteration : 1275
train acc:  0.7890625
train loss:  0.47973278164863586
train gradient:  0.18347525729974418
iteration : 1276
train acc:  0.6640625
train loss:  0.5695675611495972
train gradient:  0.1622644003718814
iteration : 1277
train acc:  0.6796875
train loss:  0.5873407125473022
train gradient:  0.19883349723557603
iteration : 1278
train acc:  0.6875
train loss:  0.5587418079376221
train gradient:  0.12502315102147366
iteration : 1279
train acc:  0.75
train loss:  0.5231740474700928
train gradient:  0.24766615220860705
iteration : 1280
train acc:  0.7265625
train loss:  0.5613465905189514
train gradient:  0.15126556805222435
iteration : 1281
train acc:  0.71875
train loss:  0.5599036812782288
train gradient:  0.13393315463153826
iteration : 1282
train acc:  0.7265625
train loss:  0.5413355827331543
train gradient:  0.13734909671523843
iteration : 1283
train acc:  0.6953125
train loss:  0.5537890195846558
train gradient:  0.14087894590383865
iteration : 1284
train acc:  0.6484375
train loss:  0.5904070734977722
train gradient:  0.17509242651956972
iteration : 1285
train acc:  0.703125
train loss:  0.5817168354988098
train gradient:  0.16644879371874066
iteration : 1286
train acc:  0.6328125
train loss:  0.6033486127853394
train gradient:  0.14876426528275777
iteration : 1287
train acc:  0.671875
train loss:  0.574001133441925
train gradient:  0.18553854410693396
iteration : 1288
train acc:  0.7265625
train loss:  0.5256751179695129
train gradient:  0.1516587071443363
iteration : 1289
train acc:  0.71875
train loss:  0.5019012689590454
train gradient:  0.13532756281695152
iteration : 1290
train acc:  0.6875
train loss:  0.5724736452102661
train gradient:  0.15018235126009938
iteration : 1291
train acc:  0.703125
train loss:  0.5749502778053284
train gradient:  0.25019609847732044
iteration : 1292
train acc:  0.75
train loss:  0.5276017785072327
train gradient:  0.14582363601635698
iteration : 1293
train acc:  0.6640625
train loss:  0.5946846008300781
train gradient:  0.18059666655974194
iteration : 1294
train acc:  0.6953125
train loss:  0.5258591175079346
train gradient:  0.17960240128534102
iteration : 1295
train acc:  0.671875
train loss:  0.539128303527832
train gradient:  0.1339682543683282
iteration : 1296
train acc:  0.7421875
train loss:  0.5115291476249695
train gradient:  0.21143687265605204
iteration : 1297
train acc:  0.75
train loss:  0.49229881167411804
train gradient:  0.1272810650241463
iteration : 1298
train acc:  0.703125
train loss:  0.5630881786346436
train gradient:  0.1980287087445689
iteration : 1299
train acc:  0.6953125
train loss:  0.5650448203086853
train gradient:  0.14511464865834356
iteration : 1300
train acc:  0.703125
train loss:  0.5730535984039307
train gradient:  0.16727779096148906
iteration : 1301
train acc:  0.6875
train loss:  0.5978364944458008
train gradient:  0.19803817155328257
iteration : 1302
train acc:  0.7578125
train loss:  0.5513054132461548
train gradient:  0.1883493548999366
iteration : 1303
train acc:  0.734375
train loss:  0.4986359477043152
train gradient:  0.11155386953056261
iteration : 1304
train acc:  0.7109375
train loss:  0.5476934313774109
train gradient:  0.15235872244319548
iteration : 1305
train acc:  0.6640625
train loss:  0.5893533825874329
train gradient:  0.16110930663816947
iteration : 1306
train acc:  0.7109375
train loss:  0.5481393337249756
train gradient:  0.19336979946985916
iteration : 1307
train acc:  0.671875
train loss:  0.5646066665649414
train gradient:  0.14542741597441136
iteration : 1308
train acc:  0.703125
train loss:  0.5430585741996765
train gradient:  0.17971679432603951
iteration : 1309
train acc:  0.71875
train loss:  0.536099910736084
train gradient:  0.20426775447021772
iteration : 1310
train acc:  0.71875
train loss:  0.5647343397140503
train gradient:  0.1616051752912002
iteration : 1311
train acc:  0.6875
train loss:  0.5798822641372681
train gradient:  0.19758470126223107
iteration : 1312
train acc:  0.6796875
train loss:  0.5817482471466064
train gradient:  0.19157234460450875
iteration : 1313
train acc:  0.7109375
train loss:  0.5807108879089355
train gradient:  0.19597894953739703
iteration : 1314
train acc:  0.6796875
train loss:  0.5904442667961121
train gradient:  0.19838852535236473
iteration : 1315
train acc:  0.6796875
train loss:  0.5960904359817505
train gradient:  0.1891584494313363
iteration : 1316
train acc:  0.6484375
train loss:  0.6359253525733948
train gradient:  0.21960395614248826
iteration : 1317
train acc:  0.7734375
train loss:  0.5663313865661621
train gradient:  0.13995375561882498
iteration : 1318
train acc:  0.671875
train loss:  0.5513991117477417
train gradient:  0.14280520160624854
iteration : 1319
train acc:  0.6328125
train loss:  0.6120319962501526
train gradient:  0.20725222478098976
iteration : 1320
train acc:  0.65625
train loss:  0.5934192538261414
train gradient:  0.2023878578193516
iteration : 1321
train acc:  0.71875
train loss:  0.5463849306106567
train gradient:  0.1490974770591874
iteration : 1322
train acc:  0.7421875
train loss:  0.5507471561431885
train gradient:  0.1824519229049717
iteration : 1323
train acc:  0.6640625
train loss:  0.5917314887046814
train gradient:  0.1568656817291699
iteration : 1324
train acc:  0.71875
train loss:  0.5478943586349487
train gradient:  0.1366723124984594
iteration : 1325
train acc:  0.65625
train loss:  0.5414620637893677
train gradient:  0.1252256727469027
iteration : 1326
train acc:  0.65625
train loss:  0.5870732069015503
train gradient:  0.17501404455193686
iteration : 1327
train acc:  0.671875
train loss:  0.577042818069458
train gradient:  0.1539160647043586
iteration : 1328
train acc:  0.71875
train loss:  0.5663084983825684
train gradient:  0.15032426312044295
iteration : 1329
train acc:  0.6328125
train loss:  0.5871669054031372
train gradient:  0.1998708443569238
iteration : 1330
train acc:  0.7109375
train loss:  0.5631385445594788
train gradient:  0.13201279967703505
iteration : 1331
train acc:  0.6953125
train loss:  0.5604787468910217
train gradient:  0.15676124351024956
iteration : 1332
train acc:  0.7265625
train loss:  0.48407092690467834
train gradient:  0.1400449080946149
iteration : 1333
train acc:  0.625
train loss:  0.5851132869720459
train gradient:  0.19820709630149247
iteration : 1334
train acc:  0.6875
train loss:  0.6058212518692017
train gradient:  0.17474506936273154
iteration : 1335
train acc:  0.734375
train loss:  0.5303791761398315
train gradient:  0.12589540189014004
iteration : 1336
train acc:  0.65625
train loss:  0.5552092790603638
train gradient:  0.18837814397100947
iteration : 1337
train acc:  0.7421875
train loss:  0.5146247148513794
train gradient:  0.19278607789139213
iteration : 1338
train acc:  0.6640625
train loss:  0.6454185247421265
train gradient:  0.17917249797709495
iteration : 1339
train acc:  0.6640625
train loss:  0.628441572189331
train gradient:  0.19498739796361536
iteration : 1340
train acc:  0.7734375
train loss:  0.5254136323928833
train gradient:  0.1211245946168932
iteration : 1341
train acc:  0.703125
train loss:  0.5522449016571045
train gradient:  0.17383902767245657
iteration : 1342
train acc:  0.671875
train loss:  0.5891788601875305
train gradient:  0.1971583588922221
iteration : 1343
train acc:  0.7109375
train loss:  0.5401583313941956
train gradient:  0.1475531622909368
iteration : 1344
train acc:  0.671875
train loss:  0.5711068511009216
train gradient:  0.27276484504457893
iteration : 1345
train acc:  0.7734375
train loss:  0.5055544376373291
train gradient:  0.12178858125187005
iteration : 1346
train acc:  0.71875
train loss:  0.5821217894554138
train gradient:  0.2259148669025282
iteration : 1347
train acc:  0.7421875
train loss:  0.4877150356769562
train gradient:  0.1504832111941003
iteration : 1348
train acc:  0.65625
train loss:  0.611231803894043
train gradient:  0.19038293434870274
iteration : 1349
train acc:  0.703125
train loss:  0.5492693185806274
train gradient:  0.19442706675118251
iteration : 1350
train acc:  0.640625
train loss:  0.5849951505661011
train gradient:  0.1847926045993776
iteration : 1351
train acc:  0.7421875
train loss:  0.5829055309295654
train gradient:  0.19553915561031915
iteration : 1352
train acc:  0.6640625
train loss:  0.5945222973823547
train gradient:  0.17073560551599026
iteration : 1353
train acc:  0.7421875
train loss:  0.5606361031532288
train gradient:  0.19502886655119872
iteration : 1354
train acc:  0.7109375
train loss:  0.5189772248268127
train gradient:  0.1649076819664298
iteration : 1355
train acc:  0.671875
train loss:  0.5454227924346924
train gradient:  0.21086156588061783
iteration : 1356
train acc:  0.6796875
train loss:  0.5684402585029602
train gradient:  0.14925611733860328
iteration : 1357
train acc:  0.7578125
train loss:  0.49864253401756287
train gradient:  0.26386306449084984
iteration : 1358
train acc:  0.7265625
train loss:  0.5142171382904053
train gradient:  0.17428188928597566
iteration : 1359
train acc:  0.7109375
train loss:  0.5572951436042786
train gradient:  0.2627047856972462
iteration : 1360
train acc:  0.765625
train loss:  0.5075850486755371
train gradient:  0.1777838482004489
iteration : 1361
train acc:  0.671875
train loss:  0.6259907484054565
train gradient:  0.31595551600158356
iteration : 1362
train acc:  0.6875
train loss:  0.5487753748893738
train gradient:  0.13198077591231067
iteration : 1363
train acc:  0.71875
train loss:  0.565773069858551
train gradient:  0.16771521875016115
iteration : 1364
train acc:  0.71875
train loss:  0.551160454750061
train gradient:  0.1490186484822607
iteration : 1365
train acc:  0.703125
train loss:  0.5176401138305664
train gradient:  0.12616668475437826
iteration : 1366
train acc:  0.65625
train loss:  0.5784198045730591
train gradient:  0.15532070451196495
iteration : 1367
train acc:  0.7265625
train loss:  0.5459455251693726
train gradient:  0.19412269057034667
iteration : 1368
train acc:  0.6328125
train loss:  0.5733293890953064
train gradient:  0.15455601602368965
iteration : 1369
train acc:  0.6953125
train loss:  0.6358039975166321
train gradient:  0.3829195746027167
iteration : 1370
train acc:  0.6796875
train loss:  0.5229278802871704
train gradient:  0.18374012409998258
iteration : 1371
train acc:  0.7265625
train loss:  0.5375313758850098
train gradient:  0.15468672748797832
iteration : 1372
train acc:  0.671875
train loss:  0.6218357086181641
train gradient:  0.19580017512293796
iteration : 1373
train acc:  0.625
train loss:  0.6327127814292908
train gradient:  0.21178361435759113
iteration : 1374
train acc:  0.734375
train loss:  0.5285600423812866
train gradient:  0.15870171499603253
iteration : 1375
train acc:  0.6328125
train loss:  0.5979375243186951
train gradient:  0.22435561526783857
iteration : 1376
train acc:  0.7265625
train loss:  0.5174316763877869
train gradient:  0.12148276913978366
iteration : 1377
train acc:  0.7265625
train loss:  0.5631341934204102
train gradient:  0.16292568814582037
iteration : 1378
train acc:  0.6640625
train loss:  0.6173208951950073
train gradient:  0.152822589392432
iteration : 1379
train acc:  0.75
train loss:  0.5375217199325562
train gradient:  0.15622662881901223
iteration : 1380
train acc:  0.7421875
train loss:  0.5394711494445801
train gradient:  0.1265624169755323
iteration : 1381
train acc:  0.796875
train loss:  0.5064384937286377
train gradient:  0.1504597988391922
iteration : 1382
train acc:  0.71875
train loss:  0.5475225448608398
train gradient:  0.18880403285643493
iteration : 1383
train acc:  0.7421875
train loss:  0.520513653755188
train gradient:  0.16710485174414375
iteration : 1384
train acc:  0.671875
train loss:  0.5606128573417664
train gradient:  0.17209353785136328
iteration : 1385
train acc:  0.7109375
train loss:  0.5584614276885986
train gradient:  0.18069133176173197
iteration : 1386
train acc:  0.671875
train loss:  0.5605128407478333
train gradient:  0.17789838242698625
iteration : 1387
train acc:  0.8046875
train loss:  0.49368172883987427
train gradient:  0.1604759432400391
iteration : 1388
train acc:  0.625
train loss:  0.5842454433441162
train gradient:  0.14848011156180524
iteration : 1389
train acc:  0.65625
train loss:  0.5690159797668457
train gradient:  0.21246696881790497
iteration : 1390
train acc:  0.671875
train loss:  0.5544470548629761
train gradient:  0.14854638613872484
iteration : 1391
train acc:  0.6484375
train loss:  0.5732302665710449
train gradient:  0.16728268416372777
iteration : 1392
train acc:  0.6875
train loss:  0.5485602617263794
train gradient:  0.2051601775827268
iteration : 1393
train acc:  0.703125
train loss:  0.5201722979545593
train gradient:  0.10548858150574843
iteration : 1394
train acc:  0.640625
train loss:  0.5984068512916565
train gradient:  0.2193477429389557
iteration : 1395
train acc:  0.734375
train loss:  0.5473001599311829
train gradient:  0.15021070611969253
iteration : 1396
train acc:  0.7421875
train loss:  0.5236279964447021
train gradient:  0.13782445692210443
iteration : 1397
train acc:  0.7578125
train loss:  0.5113398432731628
train gradient:  0.17810289567600496
iteration : 1398
train acc:  0.65625
train loss:  0.5788441896438599
train gradient:  0.13933555570174058
iteration : 1399
train acc:  0.6640625
train loss:  0.5946733951568604
train gradient:  0.15454044621517343
iteration : 1400
train acc:  0.6953125
train loss:  0.5986111760139465
train gradient:  0.22890655379650265
iteration : 1401
train acc:  0.6953125
train loss:  0.51589035987854
train gradient:  0.1426629004871752
iteration : 1402
train acc:  0.6484375
train loss:  0.6378886699676514
train gradient:  0.1739733574881314
iteration : 1403
train acc:  0.7109375
train loss:  0.6012550592422485
train gradient:  0.1505069054336128
iteration : 1404
train acc:  0.7109375
train loss:  0.5255610942840576
train gradient:  0.16307926228898023
iteration : 1405
train acc:  0.7421875
train loss:  0.5032069683074951
train gradient:  0.2514055515231112
iteration : 1406
train acc:  0.7578125
train loss:  0.49427101016044617
train gradient:  0.17610412183027008
iteration : 1407
train acc:  0.7109375
train loss:  0.5488842725753784
train gradient:  0.19431545710406656
iteration : 1408
train acc:  0.6953125
train loss:  0.5959473848342896
train gradient:  0.24752004950581719
iteration : 1409
train acc:  0.6484375
train loss:  0.6176802515983582
train gradient:  0.35568575212738085
iteration : 1410
train acc:  0.7109375
train loss:  0.566091001033783
train gradient:  0.15378577304218213
iteration : 1411
train acc:  0.6328125
train loss:  0.6100618839263916
train gradient:  0.20777341589122234
iteration : 1412
train acc:  0.75
train loss:  0.5681616067886353
train gradient:  0.1507241527283169
iteration : 1413
train acc:  0.671875
train loss:  0.5705568790435791
train gradient:  0.1480261414391059
iteration : 1414
train acc:  0.6328125
train loss:  0.6339254379272461
train gradient:  0.1911549324913099
iteration : 1415
train acc:  0.6953125
train loss:  0.5592423677444458
train gradient:  0.15182486058525402
iteration : 1416
train acc:  0.78125
train loss:  0.5138123035430908
train gradient:  0.15015745525269528
iteration : 1417
train acc:  0.75
train loss:  0.4710381031036377
train gradient:  0.13079389108695905
iteration : 1418
train acc:  0.71875
train loss:  0.5519062876701355
train gradient:  0.21175586776088617
iteration : 1419
train acc:  0.6796875
train loss:  0.5218895673751831
train gradient:  0.14215234804441246
iteration : 1420
train acc:  0.71875
train loss:  0.5766781568527222
train gradient:  0.17345713161913348
iteration : 1421
train acc:  0.7265625
train loss:  0.5149933099746704
train gradient:  0.15440876859676927
iteration : 1422
train acc:  0.65625
train loss:  0.5870953798294067
train gradient:  0.14709882120937492
iteration : 1423
train acc:  0.703125
train loss:  0.5404069423675537
train gradient:  0.16652040881609248
iteration : 1424
train acc:  0.6875
train loss:  0.5642254948616028
train gradient:  0.18592044756556508
iteration : 1425
train acc:  0.6875
train loss:  0.5860839486122131
train gradient:  0.17376755519734827
iteration : 1426
train acc:  0.65625
train loss:  0.5916743278503418
train gradient:  0.1752117615884683
iteration : 1427
train acc:  0.765625
train loss:  0.5113080143928528
train gradient:  0.14774831973409377
iteration : 1428
train acc:  0.6953125
train loss:  0.5577585697174072
train gradient:  0.1438130937862287
iteration : 1429
train acc:  0.65625
train loss:  0.5708210468292236
train gradient:  0.16742246276018585
iteration : 1430
train acc:  0.71875
train loss:  0.5674795508384705
train gradient:  0.2381228078245935
iteration : 1431
train acc:  0.78125
train loss:  0.4720131456851959
train gradient:  0.16943291496044927
iteration : 1432
train acc:  0.671875
train loss:  0.5504153370857239
train gradient:  0.22535801451019705
iteration : 1433
train acc:  0.671875
train loss:  0.575507402420044
train gradient:  0.16122967831313875
iteration : 1434
train acc:  0.71875
train loss:  0.5149602890014648
train gradient:  0.13001846211461596
iteration : 1435
train acc:  0.6640625
train loss:  0.6137627363204956
train gradient:  0.20998249780185596
iteration : 1436
train acc:  0.6875
train loss:  0.55820631980896
train gradient:  0.1334149333916931
iteration : 1437
train acc:  0.71875
train loss:  0.5585322976112366
train gradient:  0.17049499245993338
iteration : 1438
train acc:  0.6171875
train loss:  0.6960859298706055
train gradient:  0.22122280934364802
iteration : 1439
train acc:  0.7109375
train loss:  0.5137771964073181
train gradient:  0.16124994262136416
iteration : 1440
train acc:  0.703125
train loss:  0.6296943426132202
train gradient:  0.19870126356017934
iteration : 1441
train acc:  0.6953125
train loss:  0.5440417528152466
train gradient:  0.1416277343023494
iteration : 1442
train acc:  0.71875
train loss:  0.5608856678009033
train gradient:  0.1468314780611714
iteration : 1443
train acc:  0.6875
train loss:  0.589993953704834
train gradient:  0.21652269667639107
iteration : 1444
train acc:  0.7109375
train loss:  0.539422869682312
train gradient:  0.17880061059797372
iteration : 1445
train acc:  0.734375
train loss:  0.5379269123077393
train gradient:  0.15173041339107396
iteration : 1446
train acc:  0.671875
train loss:  0.59917151927948
train gradient:  0.20117328232200654
iteration : 1447
train acc:  0.7578125
train loss:  0.5198419094085693
train gradient:  0.1288291826526658
iteration : 1448
train acc:  0.7109375
train loss:  0.5392911434173584
train gradient:  0.12864332863821432
iteration : 1449
train acc:  0.65625
train loss:  0.569758415222168
train gradient:  0.14346020464556383
iteration : 1450
train acc:  0.6640625
train loss:  0.586646556854248
train gradient:  0.22165289068198518
iteration : 1451
train acc:  0.7578125
train loss:  0.5147112607955933
train gradient:  0.1832265807675728
iteration : 1452
train acc:  0.6484375
train loss:  0.6017761826515198
train gradient:  0.17833870963451617
iteration : 1453
train acc:  0.7421875
train loss:  0.5269075632095337
train gradient:  0.16612833607122635
iteration : 1454
train acc:  0.71875
train loss:  0.5617669820785522
train gradient:  0.16157861311008034
iteration : 1455
train acc:  0.7265625
train loss:  0.5852673053741455
train gradient:  0.20534357482129043
iteration : 1456
train acc:  0.703125
train loss:  0.5210891962051392
train gradient:  0.11765444657275503
iteration : 1457
train acc:  0.6875
train loss:  0.5656657814979553
train gradient:  0.20615465048171555
iteration : 1458
train acc:  0.6640625
train loss:  0.6089330911636353
train gradient:  0.15573066542445616
iteration : 1459
train acc:  0.7265625
train loss:  0.5318557024002075
train gradient:  0.14817553347496656
iteration : 1460
train acc:  0.7109375
train loss:  0.5211310982704163
train gradient:  0.14430106244728813
iteration : 1461
train acc:  0.734375
train loss:  0.5112032890319824
train gradient:  0.19986858541677877
iteration : 1462
train acc:  0.65625
train loss:  0.5581482648849487
train gradient:  0.17145190485976675
iteration : 1463
train acc:  0.6640625
train loss:  0.5996495485305786
train gradient:  0.16053954799458173
iteration : 1464
train acc:  0.6953125
train loss:  0.554168701171875
train gradient:  0.15702984231783312
iteration : 1465
train acc:  0.7265625
train loss:  0.5924933552742004
train gradient:  0.17484033762993348
iteration : 1466
train acc:  0.6875
train loss:  0.592892050743103
train gradient:  0.18792906051039934
iteration : 1467
train acc:  0.6640625
train loss:  0.5837799310684204
train gradient:  0.18141695626563265
iteration : 1468
train acc:  0.71875
train loss:  0.5214197635650635
train gradient:  0.12278294919998797
iteration : 1469
train acc:  0.7265625
train loss:  0.5105172395706177
train gradient:  0.14411351824106278
iteration : 1470
train acc:  0.7421875
train loss:  0.51617830991745
train gradient:  0.127458182931741
iteration : 1471
train acc:  0.671875
train loss:  0.5628933310508728
train gradient:  0.17794534461460865
iteration : 1472
train acc:  0.703125
train loss:  0.5478395819664001
train gradient:  0.12120614803735992
iteration : 1473
train acc:  0.75
train loss:  0.5204771757125854
train gradient:  0.1555332631420298
iteration : 1474
train acc:  0.703125
train loss:  0.5510743260383606
train gradient:  0.15721134474882048
iteration : 1475
train acc:  0.640625
train loss:  0.6409939527511597
train gradient:  0.17398027795148147
iteration : 1476
train acc:  0.640625
train loss:  0.6204085350036621
train gradient:  0.20208140082848802
iteration : 1477
train acc:  0.6796875
train loss:  0.5622759461402893
train gradient:  0.15261062300904002
iteration : 1478
train acc:  0.7421875
train loss:  0.5284752249717712
train gradient:  0.1630247161469289
iteration : 1479
train acc:  0.7578125
train loss:  0.5027481317520142
train gradient:  0.13355470241402045
iteration : 1480
train acc:  0.7421875
train loss:  0.5436336994171143
train gradient:  0.1693861702644761
iteration : 1481
train acc:  0.6640625
train loss:  0.5833775997161865
train gradient:  0.1558065789622179
iteration : 1482
train acc:  0.7265625
train loss:  0.5507590770721436
train gradient:  0.14894008081647853
iteration : 1483
train acc:  0.625
train loss:  0.5990570783615112
train gradient:  0.15495391676179432
iteration : 1484
train acc:  0.7890625
train loss:  0.4535682201385498
train gradient:  0.1446893290399312
iteration : 1485
train acc:  0.7109375
train loss:  0.5344241261482239
train gradient:  0.13441361899353638
iteration : 1486
train acc:  0.703125
train loss:  0.5804933309555054
train gradient:  0.1490791585374825
iteration : 1487
train acc:  0.6953125
train loss:  0.558113694190979
train gradient:  0.1479110132689229
iteration : 1488
train acc:  0.6875
train loss:  0.5622618198394775
train gradient:  0.1675840219263423
iteration : 1489
train acc:  0.671875
train loss:  0.5910624265670776
train gradient:  0.14256663550388154
iteration : 1490
train acc:  0.6953125
train loss:  0.5465060472488403
train gradient:  0.15119901672317665
iteration : 1491
train acc:  0.640625
train loss:  0.641187310218811
train gradient:  0.2083679963266626
iteration : 1492
train acc:  0.6953125
train loss:  0.5198789238929749
train gradient:  0.1408846897511903
iteration : 1493
train acc:  0.71875
train loss:  0.5519304275512695
train gradient:  0.1338136161658712
iteration : 1494
train acc:  0.671875
train loss:  0.5428264737129211
train gradient:  0.19893048401658026
iteration : 1495
train acc:  0.6796875
train loss:  0.5544964671134949
train gradient:  0.11947115507114356
iteration : 1496
train acc:  0.671875
train loss:  0.5852115750312805
train gradient:  0.18551825739497568
iteration : 1497
train acc:  0.7265625
train loss:  0.5467828512191772
train gradient:  0.14193780917301818
iteration : 1498
train acc:  0.703125
train loss:  0.5370055437088013
train gradient:  0.18515040734413696
iteration : 1499
train acc:  0.65625
train loss:  0.6127344369888306
train gradient:  0.17005445699529764
iteration : 1500
train acc:  0.75
train loss:  0.5411549210548401
train gradient:  0.20118650339295216
iteration : 1501
train acc:  0.640625
train loss:  0.5928853750228882
train gradient:  0.18409709676924577
iteration : 1502
train acc:  0.734375
train loss:  0.5843864679336548
train gradient:  0.17151526742441625
iteration : 1503
train acc:  0.71875
train loss:  0.5675363540649414
train gradient:  0.16978393340587397
iteration : 1504
train acc:  0.6796875
train loss:  0.5853153467178345
train gradient:  0.14220162782035903
iteration : 1505
train acc:  0.6953125
train loss:  0.6016775965690613
train gradient:  0.22892886339658206
iteration : 1506
train acc:  0.6875
train loss:  0.5639412999153137
train gradient:  0.2082814598214593
iteration : 1507
train acc:  0.7734375
train loss:  0.4924853444099426
train gradient:  0.13449029619440667
iteration : 1508
train acc:  0.703125
train loss:  0.5584375858306885
train gradient:  0.19056046454176584
iteration : 1509
train acc:  0.7734375
train loss:  0.4986606240272522
train gradient:  0.13159782667309905
iteration : 1510
train acc:  0.6875
train loss:  0.5399421453475952
train gradient:  0.13055666727319742
iteration : 1511
train acc:  0.6875
train loss:  0.5311949849128723
train gradient:  0.184727556178241
iteration : 1512
train acc:  0.6640625
train loss:  0.5878154635429382
train gradient:  0.13277511055557134
iteration : 1513
train acc:  0.65625
train loss:  0.5751748085021973
train gradient:  0.19873429239649187
iteration : 1514
train acc:  0.7109375
train loss:  0.5594167709350586
train gradient:  0.2394998665557911
iteration : 1515
train acc:  0.75
train loss:  0.46963465213775635
train gradient:  0.13964781592262088
iteration : 1516
train acc:  0.7890625
train loss:  0.49988412857055664
train gradient:  0.1561653410900579
iteration : 1517
train acc:  0.78125
train loss:  0.5395026206970215
train gradient:  0.1718979604078591
iteration : 1518
train acc:  0.7265625
train loss:  0.5142106413841248
train gradient:  0.21097333625637826
iteration : 1519
train acc:  0.65625
train loss:  0.6090423464775085
train gradient:  0.1790366534176746
iteration : 1520
train acc:  0.7109375
train loss:  0.5739197731018066
train gradient:  0.17637235003465562
iteration : 1521
train acc:  0.75
train loss:  0.5494228005409241
train gradient:  0.22975432535545326
iteration : 1522
train acc:  0.6796875
train loss:  0.5587878227233887
train gradient:  0.15675133885361342
iteration : 1523
train acc:  0.6875
train loss:  0.5639742612838745
train gradient:  0.17290132632393984
iteration : 1524
train acc:  0.765625
train loss:  0.5117433667182922
train gradient:  0.14971312978745036
iteration : 1525
train acc:  0.75
train loss:  0.48790284991264343
train gradient:  0.15987126795034304
iteration : 1526
train acc:  0.65625
train loss:  0.5682774782180786
train gradient:  0.1732114806724942
iteration : 1527
train acc:  0.7421875
train loss:  0.5381683111190796
train gradient:  0.18151332152506533
iteration : 1528
train acc:  0.734375
train loss:  0.5384761691093445
train gradient:  0.14466784501704072
iteration : 1529
train acc:  0.6640625
train loss:  0.6052314639091492
train gradient:  0.16881998798780712
iteration : 1530
train acc:  0.71875
train loss:  0.5377290844917297
train gradient:  0.22180413467449522
iteration : 1531
train acc:  0.6953125
train loss:  0.5376086235046387
train gradient:  0.15139872621705258
iteration : 1532
train acc:  0.734375
train loss:  0.5426045656204224
train gradient:  0.23984016337142444
iteration : 1533
train acc:  0.734375
train loss:  0.5676941871643066
train gradient:  0.15611642777156223
iteration : 1534
train acc:  0.703125
train loss:  0.5439755916595459
train gradient:  0.11579177756723183
iteration : 1535
train acc:  0.6953125
train loss:  0.5986291766166687
train gradient:  0.1611728296046409
iteration : 1536
train acc:  0.6953125
train loss:  0.6309549808502197
train gradient:  0.20576982492532586
iteration : 1537
train acc:  0.75
train loss:  0.5629574060440063
train gradient:  0.18505960594628174
iteration : 1538
train acc:  0.671875
train loss:  0.6139542460441589
train gradient:  0.22294364618566603
iteration : 1539
train acc:  0.7421875
train loss:  0.5320494771003723
train gradient:  0.22292599131134583
iteration : 1540
train acc:  0.7109375
train loss:  0.5620188117027283
train gradient:  0.26249397541873143
iteration : 1541
train acc:  0.625
train loss:  0.6257835626602173
train gradient:  0.2005799514750526
iteration : 1542
train acc:  0.6875
train loss:  0.6031355261802673
train gradient:  0.18019961703582643
iteration : 1543
train acc:  0.734375
train loss:  0.4979592561721802
train gradient:  0.13547233816429727
iteration : 1544
train acc:  0.75
train loss:  0.5043637752532959
train gradient:  0.13983856968472608
iteration : 1545
train acc:  0.6953125
train loss:  0.5273003578186035
train gradient:  0.11159799816258965
iteration : 1546
train acc:  0.6796875
train loss:  0.5865186452865601
train gradient:  0.18589936632133253
iteration : 1547
train acc:  0.75
train loss:  0.4736311435699463
train gradient:  0.09358518410008367
iteration : 1548
train acc:  0.734375
train loss:  0.590621829032898
train gradient:  0.29784385010693115
iteration : 1549
train acc:  0.6953125
train loss:  0.5194783210754395
train gradient:  0.11456681488294188
iteration : 1550
train acc:  0.75
train loss:  0.5523967146873474
train gradient:  0.17822181224903166
iteration : 1551
train acc:  0.7421875
train loss:  0.5773105621337891
train gradient:  0.22928500138843233
iteration : 1552
train acc:  0.5859375
train loss:  0.6861953735351562
train gradient:  0.2500216515726085
iteration : 1553
train acc:  0.6796875
train loss:  0.5741857290267944
train gradient:  0.1698112846935228
iteration : 1554
train acc:  0.7109375
train loss:  0.5276833772659302
train gradient:  0.1381916948220756
iteration : 1555
train acc:  0.7109375
train loss:  0.5568780899047852
train gradient:  0.2749020600279379
iteration : 1556
train acc:  0.75
train loss:  0.49902477860450745
train gradient:  0.19709881457171202
iteration : 1557
train acc:  0.734375
train loss:  0.5209960341453552
train gradient:  0.12389463463781494
iteration : 1558
train acc:  0.78125
train loss:  0.4706721603870392
train gradient:  0.1041497180598723
iteration : 1559
train acc:  0.71875
train loss:  0.5292800664901733
train gradient:  0.18521679593118706
iteration : 1560
train acc:  0.6640625
train loss:  0.5496326088905334
train gradient:  0.14651326855539354
iteration : 1561
train acc:  0.78125
train loss:  0.4698450565338135
train gradient:  0.12565461372313713
iteration : 1562
train acc:  0.7578125
train loss:  0.4944201707839966
train gradient:  0.14411236730605825
iteration : 1563
train acc:  0.703125
train loss:  0.5910972356796265
train gradient:  0.1919338669756179
iteration : 1564
train acc:  0.671875
train loss:  0.5489612817764282
train gradient:  0.14163347901535767
iteration : 1565
train acc:  0.65625
train loss:  0.6016378402709961
train gradient:  0.18928066751128414
iteration : 1566
train acc:  0.796875
train loss:  0.46533939242362976
train gradient:  0.2034217284368952
iteration : 1567
train acc:  0.78125
train loss:  0.4998668432235718
train gradient:  0.11779972366373323
iteration : 1568
train acc:  0.7265625
train loss:  0.5630465149879456
train gradient:  0.1839300198001862
iteration : 1569
train acc:  0.7109375
train loss:  0.541955292224884
train gradient:  0.1880249651210908
iteration : 1570
train acc:  0.75
train loss:  0.532230019569397
train gradient:  0.14327844915343932
iteration : 1571
train acc:  0.6796875
train loss:  0.5430346727371216
train gradient:  0.15487746727317087
iteration : 1572
train acc:  0.7109375
train loss:  0.5572671294212341
train gradient:  0.1186196718551805
iteration : 1573
train acc:  0.65625
train loss:  0.5510968565940857
train gradient:  0.15453057512220975
iteration : 1574
train acc:  0.7265625
train loss:  0.5364686250686646
train gradient:  0.15674980828769552
iteration : 1575
train acc:  0.609375
train loss:  0.635972797870636
train gradient:  0.21230933625328735
iteration : 1576
train acc:  0.6953125
train loss:  0.5508356094360352
train gradient:  0.13691660040526132
iteration : 1577
train acc:  0.703125
train loss:  0.5458463430404663
train gradient:  0.1340503580644637
iteration : 1578
train acc:  0.7109375
train loss:  0.5333866477012634
train gradient:  0.14057628903995556
iteration : 1579
train acc:  0.625
train loss:  0.5979245901107788
train gradient:  0.26320437137374086
iteration : 1580
train acc:  0.6953125
train loss:  0.5508469343185425
train gradient:  0.14209185681300965
iteration : 1581
train acc:  0.6953125
train loss:  0.5920770168304443
train gradient:  0.22208788769442336
iteration : 1582
train acc:  0.6875
train loss:  0.6105568408966064
train gradient:  0.23862196172031802
iteration : 1583
train acc:  0.6875
train loss:  0.5141384601593018
train gradient:  0.15549946624830413
iteration : 1584
train acc:  0.6796875
train loss:  0.5829442143440247
train gradient:  0.20451654078975134
iteration : 1585
train acc:  0.65625
train loss:  0.6103997230529785
train gradient:  0.20010203272612648
iteration : 1586
train acc:  0.71875
train loss:  0.5423108339309692
train gradient:  0.15196928940237947
iteration : 1587
train acc:  0.703125
train loss:  0.5416508913040161
train gradient:  0.1713264758845567
iteration : 1588
train acc:  0.7890625
train loss:  0.47455790638923645
train gradient:  0.15157617566585943
iteration : 1589
train acc:  0.734375
train loss:  0.5238684415817261
train gradient:  0.15659277681427064
iteration : 1590
train acc:  0.7109375
train loss:  0.5306286811828613
train gradient:  0.15781411616320254
iteration : 1591
train acc:  0.6796875
train loss:  0.5653055906295776
train gradient:  0.18401926769990912
iteration : 1592
train acc:  0.765625
train loss:  0.5184847712516785
train gradient:  0.17021643804725745
iteration : 1593
train acc:  0.7734375
train loss:  0.5443467497825623
train gradient:  0.1283437834075709
iteration : 1594
train acc:  0.7109375
train loss:  0.5592421293258667
train gradient:  0.19212128847707582
iteration : 1595
train acc:  0.6796875
train loss:  0.5634056329727173
train gradient:  0.16857355563904616
iteration : 1596
train acc:  0.7734375
train loss:  0.4821528494358063
train gradient:  0.16118948496170354
iteration : 1597
train acc:  0.7109375
train loss:  0.5294044613838196
train gradient:  0.18426800648386793
iteration : 1598
train acc:  0.640625
train loss:  0.5796278119087219
train gradient:  0.17125997178117525
iteration : 1599
train acc:  0.734375
train loss:  0.5290611982345581
train gradient:  0.1216740901374111
iteration : 1600
train acc:  0.75
train loss:  0.5336995720863342
train gradient:  0.14472768221803867
iteration : 1601
train acc:  0.765625
train loss:  0.5070184469223022
train gradient:  0.14769412635700774
iteration : 1602
train acc:  0.6328125
train loss:  0.6001930236816406
train gradient:  0.18538999399729156
iteration : 1603
train acc:  0.6953125
train loss:  0.556988000869751
train gradient:  0.19751161006797907
iteration : 1604
train acc:  0.671875
train loss:  0.573388934135437
train gradient:  0.17621886957891192
iteration : 1605
train acc:  0.7421875
train loss:  0.5289819836616516
train gradient:  0.15406528506193928
iteration : 1606
train acc:  0.7890625
train loss:  0.47469189763069153
train gradient:  0.1233903171163753
iteration : 1607
train acc:  0.6640625
train loss:  0.5732166767120361
train gradient:  0.16855761381793322
iteration : 1608
train acc:  0.7109375
train loss:  0.5803537368774414
train gradient:  0.20567793397326894
iteration : 1609
train acc:  0.7265625
train loss:  0.5034129619598389
train gradient:  0.12833897611297274
iteration : 1610
train acc:  0.671875
train loss:  0.5318252444267273
train gradient:  0.2167853515309241
iteration : 1611
train acc:  0.7109375
train loss:  0.5415661334991455
train gradient:  0.17344784477386868
iteration : 1612
train acc:  0.75
train loss:  0.5512272715568542
train gradient:  0.13310580392917012
iteration : 1613
train acc:  0.671875
train loss:  0.606651246547699
train gradient:  0.20936865071049093
iteration : 1614
train acc:  0.734375
train loss:  0.5319571495056152
train gradient:  0.1458071266152346
iteration : 1615
train acc:  0.828125
train loss:  0.4488987624645233
train gradient:  0.1439365132054068
iteration : 1616
train acc:  0.65625
train loss:  0.5812234878540039
train gradient:  0.18363582434653927
iteration : 1617
train acc:  0.765625
train loss:  0.5157963037490845
train gradient:  0.14300439477498875
iteration : 1618
train acc:  0.6796875
train loss:  0.5738809704780579
train gradient:  0.16663296011116213
iteration : 1619
train acc:  0.625
train loss:  0.6098165512084961
train gradient:  0.17612726154010006
iteration : 1620
train acc:  0.703125
train loss:  0.5535120964050293
train gradient:  0.1429882822356685
iteration : 1621
train acc:  0.7265625
train loss:  0.5167997479438782
train gradient:  0.13717081130819622
iteration : 1622
train acc:  0.6171875
train loss:  0.6244332790374756
train gradient:  0.19903558986022243
iteration : 1623
train acc:  0.6640625
train loss:  0.5979804992675781
train gradient:  0.2437754758363903
iteration : 1624
train acc:  0.7109375
train loss:  0.5542715787887573
train gradient:  0.16897052902769183
iteration : 1625
train acc:  0.7265625
train loss:  0.5012057423591614
train gradient:  0.15946286508359075
iteration : 1626
train acc:  0.640625
train loss:  0.5683141946792603
train gradient:  0.15963154029325585
iteration : 1627
train acc:  0.6796875
train loss:  0.5570811033248901
train gradient:  0.1459547934894131
iteration : 1628
train acc:  0.6875
train loss:  0.5768823027610779
train gradient:  0.2000165132136818
iteration : 1629
train acc:  0.65625
train loss:  0.6096543073654175
train gradient:  0.2940264886550846
iteration : 1630
train acc:  0.6484375
train loss:  0.589169979095459
train gradient:  0.16246284107772518
iteration : 1631
train acc:  0.7578125
train loss:  0.5196840167045593
train gradient:  0.1654560273273574
iteration : 1632
train acc:  0.7265625
train loss:  0.5251176357269287
train gradient:  0.1593252435150356
iteration : 1633
train acc:  0.75
train loss:  0.5216410756111145
train gradient:  0.1639556249512054
iteration : 1634
train acc:  0.6796875
train loss:  0.5450682640075684
train gradient:  0.13572728270362555
iteration : 1635
train acc:  0.7890625
train loss:  0.4795721173286438
train gradient:  0.13478891944176313
iteration : 1636
train acc:  0.7109375
train loss:  0.5679782032966614
train gradient:  0.22778674879725902
iteration : 1637
train acc:  0.6953125
train loss:  0.6030280590057373
train gradient:  0.21373617427143976
iteration : 1638
train acc:  0.6875
train loss:  0.6029320955276489
train gradient:  0.17117123962335984
iteration : 1639
train acc:  0.6875
train loss:  0.5565188527107239
train gradient:  0.14985203346075132
iteration : 1640
train acc:  0.703125
train loss:  0.5889611840248108
train gradient:  0.17498093799430298
iteration : 1641
train acc:  0.7421875
train loss:  0.5664518475532532
train gradient:  0.15203355241191013
iteration : 1642
train acc:  0.6640625
train loss:  0.5596420168876648
train gradient:  0.23711202372831888
iteration : 1643
train acc:  0.65625
train loss:  0.6120152473449707
train gradient:  0.18291339745185242
iteration : 1644
train acc:  0.6875
train loss:  0.5540117621421814
train gradient:  0.17007809630873494
iteration : 1645
train acc:  0.7421875
train loss:  0.4982369840145111
train gradient:  0.20953471678988475
iteration : 1646
train acc:  0.703125
train loss:  0.532056987285614
train gradient:  0.18237604198867013
iteration : 1647
train acc:  0.703125
train loss:  0.5378726720809937
train gradient:  0.16588662421852496
iteration : 1648
train acc:  0.625
train loss:  0.5981849431991577
train gradient:  0.19579632146984935
iteration : 1649
train acc:  0.65625
train loss:  0.5681005716323853
train gradient:  0.17545311015135584
iteration : 1650
train acc:  0.734375
train loss:  0.5654844641685486
train gradient:  0.1552066465545366
iteration : 1651
train acc:  0.7421875
train loss:  0.5461282730102539
train gradient:  0.19976495072327632
iteration : 1652
train acc:  0.71875
train loss:  0.5680040717124939
train gradient:  0.14266503796426067
iteration : 1653
train acc:  0.7421875
train loss:  0.5028179883956909
train gradient:  0.13035409302651013
iteration : 1654
train acc:  0.7578125
train loss:  0.48257702589035034
train gradient:  0.11786444789595109
iteration : 1655
train acc:  0.6875
train loss:  0.5629881620407104
train gradient:  0.15435185819348102
iteration : 1656
train acc:  0.671875
train loss:  0.560131311416626
train gradient:  0.1615507800953123
iteration : 1657
train acc:  0.6875
train loss:  0.5625883936882019
train gradient:  0.15592992996658545
iteration : 1658
train acc:  0.6640625
train loss:  0.5978730916976929
train gradient:  0.2211337244991854
iteration : 1659
train acc:  0.703125
train loss:  0.5326282382011414
train gradient:  0.13937469993444063
iteration : 1660
train acc:  0.765625
train loss:  0.49784159660339355
train gradient:  0.1476946114478509
iteration : 1661
train acc:  0.734375
train loss:  0.5024814605712891
train gradient:  0.16044531530132622
iteration : 1662
train acc:  0.640625
train loss:  0.6163195371627808
train gradient:  0.17327823623622823
iteration : 1663
train acc:  0.6484375
train loss:  0.5619654655456543
train gradient:  0.15763151186205723
iteration : 1664
train acc:  0.671875
train loss:  0.5592191219329834
train gradient:  0.23393949239509254
iteration : 1665
train acc:  0.6640625
train loss:  0.6362872123718262
train gradient:  0.18464353130470487
iteration : 1666
train acc:  0.6953125
train loss:  0.56577068567276
train gradient:  0.1569143742482999
iteration : 1667
train acc:  0.734375
train loss:  0.5557535886764526
train gradient:  0.14389379583458783
iteration : 1668
train acc:  0.6796875
train loss:  0.5942728519439697
train gradient:  0.20646873316332665
iteration : 1669
train acc:  0.75
train loss:  0.5717953443527222
train gradient:  0.17673987530947177
iteration : 1670
train acc:  0.765625
train loss:  0.5593765377998352
train gradient:  0.16669258765900657
iteration : 1671
train acc:  0.7421875
train loss:  0.5312753915786743
train gradient:  0.1506367964909095
iteration : 1672
train acc:  0.734375
train loss:  0.5483670830726624
train gradient:  0.16064495400588952
iteration : 1673
train acc:  0.671875
train loss:  0.5571547746658325
train gradient:  0.23119161850546505
iteration : 1674
train acc:  0.609375
train loss:  0.6083087921142578
train gradient:  0.20672860004494742
iteration : 1675
train acc:  0.6953125
train loss:  0.5706819295883179
train gradient:  0.173196401315725
iteration : 1676
train acc:  0.6875
train loss:  0.542008638381958
train gradient:  0.14583175882955832
iteration : 1677
train acc:  0.75
train loss:  0.509247899055481
train gradient:  0.13439720366919847
iteration : 1678
train acc:  0.6015625
train loss:  0.648680567741394
train gradient:  0.191131191284167
iteration : 1679
train acc:  0.640625
train loss:  0.5784567594528198
train gradient:  0.19683087993238588
iteration : 1680
train acc:  0.7109375
train loss:  0.596905529499054
train gradient:  0.1582337899640356
iteration : 1681
train acc:  0.7734375
train loss:  0.4642494320869446
train gradient:  0.10321602244753059
iteration : 1682
train acc:  0.6953125
train loss:  0.590363621711731
train gradient:  0.2071614459875576
iteration : 1683
train acc:  0.703125
train loss:  0.5606414079666138
train gradient:  0.15570796523359376
iteration : 1684
train acc:  0.734375
train loss:  0.4936360716819763
train gradient:  0.14686594878044132
iteration : 1685
train acc:  0.78125
train loss:  0.5398361682891846
train gradient:  0.1369548704445215
iteration : 1686
train acc:  0.6171875
train loss:  0.5887035131454468
train gradient:  0.16732954033875475
iteration : 1687
train acc:  0.7109375
train loss:  0.5356506109237671
train gradient:  0.12682004201467356
iteration : 1688
train acc:  0.7109375
train loss:  0.5379210114479065
train gradient:  0.23043829521381914
iteration : 1689
train acc:  0.6640625
train loss:  0.5702787041664124
train gradient:  0.20363554964002883
iteration : 1690
train acc:  0.6953125
train loss:  0.5441089868545532
train gradient:  0.15170685232441183
iteration : 1691
train acc:  0.6875
train loss:  0.5765913128852844
train gradient:  0.1931968303000744
iteration : 1692
train acc:  0.65625
train loss:  0.5812332630157471
train gradient:  0.16752976648085213
iteration : 1693
train acc:  0.7578125
train loss:  0.5353519916534424
train gradient:  0.14147384356997664
iteration : 1694
train acc:  0.734375
train loss:  0.5404720306396484
train gradient:  0.1337875274264042
iteration : 1695
train acc:  0.6484375
train loss:  0.6013368368148804
train gradient:  0.2231079752642518
iteration : 1696
train acc:  0.7421875
train loss:  0.5261129140853882
train gradient:  0.13830360044447054
iteration : 1697
train acc:  0.734375
train loss:  0.4879961609840393
train gradient:  0.1595305206911075
iteration : 1698
train acc:  0.6953125
train loss:  0.555009126663208
train gradient:  0.16280869546823729
iteration : 1699
train acc:  0.71875
train loss:  0.5285198092460632
train gradient:  0.1339383720208811
iteration : 1700
train acc:  0.7109375
train loss:  0.5994879007339478
train gradient:  0.18180704590917143
iteration : 1701
train acc:  0.703125
train loss:  0.5136235356330872
train gradient:  0.15116698120296856
iteration : 1702
train acc:  0.7109375
train loss:  0.5019945502281189
train gradient:  0.1563867733372275
iteration : 1703
train acc:  0.65625
train loss:  0.5801427364349365
train gradient:  0.22754875239719186
iteration : 1704
train acc:  0.6875
train loss:  0.5183200240135193
train gradient:  0.1574531990090916
iteration : 1705
train acc:  0.7265625
train loss:  0.516977846622467
train gradient:  0.2129920592543626
iteration : 1706
train acc:  0.7265625
train loss:  0.5591670274734497
train gradient:  0.2084701020676764
iteration : 1707
train acc:  0.71875
train loss:  0.5486012101173401
train gradient:  0.1615597187657402
iteration : 1708
train acc:  0.6875
train loss:  0.5680042505264282
train gradient:  0.20819165603740838
iteration : 1709
train acc:  0.65625
train loss:  0.607650101184845
train gradient:  0.1536823918224881
iteration : 1710
train acc:  0.65625
train loss:  0.5827440023422241
train gradient:  0.186255749731629
iteration : 1711
train acc:  0.71875
train loss:  0.5146877765655518
train gradient:  0.15481447027880524
iteration : 1712
train acc:  0.6796875
train loss:  0.5659418106079102
train gradient:  0.16513498796551748
iteration : 1713
train acc:  0.7421875
train loss:  0.5597988367080688
train gradient:  0.160309886331561
iteration : 1714
train acc:  0.6171875
train loss:  0.6449542045593262
train gradient:  0.20946318356416208
iteration : 1715
train acc:  0.6796875
train loss:  0.5785007476806641
train gradient:  0.2180146315897432
iteration : 1716
train acc:  0.7265625
train loss:  0.5296756029129028
train gradient:  0.1774266879894822
iteration : 1717
train acc:  0.671875
train loss:  0.6202050447463989
train gradient:  0.20049427681450044
iteration : 1718
train acc:  0.7265625
train loss:  0.5447611808776855
train gradient:  0.16512866625969527
iteration : 1719
train acc:  0.7109375
train loss:  0.5342394113540649
train gradient:  0.1816175698540179
iteration : 1720
train acc:  0.7265625
train loss:  0.5074326395988464
train gradient:  0.13479601291082094
iteration : 1721
train acc:  0.703125
train loss:  0.5834424495697021
train gradient:  0.25927450618576814
iteration : 1722
train acc:  0.71875
train loss:  0.5471107363700867
train gradient:  0.18794986193668356
iteration : 1723
train acc:  0.703125
train loss:  0.568734347820282
train gradient:  0.16656005644687072
iteration : 1724
train acc:  0.7265625
train loss:  0.5294692516326904
train gradient:  0.15947023236394697
iteration : 1725
train acc:  0.7109375
train loss:  0.5144839286804199
train gradient:  0.15217759857554003
iteration : 1726
train acc:  0.7890625
train loss:  0.5180746912956238
train gradient:  0.18043880708509635
iteration : 1727
train acc:  0.6953125
train loss:  0.59144127368927
train gradient:  0.23839384803018399
iteration : 1728
train acc:  0.734375
train loss:  0.5098410844802856
train gradient:  0.11590301803687215
iteration : 1729
train acc:  0.7109375
train loss:  0.5472948551177979
train gradient:  0.15543087439176928
iteration : 1730
train acc:  0.71875
train loss:  0.5551981925964355
train gradient:  0.15457647683703585
iteration : 1731
train acc:  0.6953125
train loss:  0.5470331311225891
train gradient:  0.1414072714429898
iteration : 1732
train acc:  0.7578125
train loss:  0.5137091279029846
train gradient:  0.12703235647965272
iteration : 1733
train acc:  0.71875
train loss:  0.5386714935302734
train gradient:  0.21038659946615668
iteration : 1734
train acc:  0.6640625
train loss:  0.567091703414917
train gradient:  0.1769209470873351
iteration : 1735
train acc:  0.6796875
train loss:  0.5987836122512817
train gradient:  0.19672274722888844
iteration : 1736
train acc:  0.703125
train loss:  0.5222676396369934
train gradient:  0.11859107992840708
iteration : 1737
train acc:  0.671875
train loss:  0.5337274074554443
train gradient:  0.14973167992498398
iteration : 1738
train acc:  0.71875
train loss:  0.5497691631317139
train gradient:  0.17561318489294886
iteration : 1739
train acc:  0.6796875
train loss:  0.5477668046951294
train gradient:  0.18683476484336517
iteration : 1740
train acc:  0.703125
train loss:  0.566839337348938
train gradient:  0.19086950662293348
iteration : 1741
train acc:  0.7265625
train loss:  0.5659433603286743
train gradient:  0.14516529456758004
iteration : 1742
train acc:  0.6640625
train loss:  0.53266441822052
train gradient:  0.15615548797508727
iteration : 1743
train acc:  0.734375
train loss:  0.5040130615234375
train gradient:  0.1220691788686469
iteration : 1744
train acc:  0.7890625
train loss:  0.5168750286102295
train gradient:  0.1507617827473708
iteration : 1745
train acc:  0.6796875
train loss:  0.5628616809844971
train gradient:  0.2007830009498782
iteration : 1746
train acc:  0.6640625
train loss:  0.591681718826294
train gradient:  0.2430770403949329
iteration : 1747
train acc:  0.6796875
train loss:  0.557406485080719
train gradient:  0.16783285721135927
iteration : 1748
train acc:  0.703125
train loss:  0.5247805118560791
train gradient:  0.11002368080779935
iteration : 1749
train acc:  0.71875
train loss:  0.5569626092910767
train gradient:  0.15076634066608685
iteration : 1750
train acc:  0.7578125
train loss:  0.5189037322998047
train gradient:  0.13991823973563094
iteration : 1751
train acc:  0.734375
train loss:  0.5199002027511597
train gradient:  0.17914202900605541
iteration : 1752
train acc:  0.609375
train loss:  0.6326583027839661
train gradient:  0.23676631072670176
iteration : 1753
train acc:  0.7109375
train loss:  0.5318235754966736
train gradient:  0.15804017944773113
iteration : 1754
train acc:  0.6875
train loss:  0.5895462036132812
train gradient:  0.22140051701922087
iteration : 1755
train acc:  0.7109375
train loss:  0.5648738741874695
train gradient:  0.14707713088431582
iteration : 1756
train acc:  0.65625
train loss:  0.6069589853286743
train gradient:  0.16606202162972472
iteration : 1757
train acc:  0.75
train loss:  0.5018446445465088
train gradient:  0.14884315740804527
iteration : 1758
train acc:  0.7421875
train loss:  0.49848300218582153
train gradient:  0.10016644816094664
iteration : 1759
train acc:  0.7421875
train loss:  0.4880238175392151
train gradient:  0.10793785211506701
iteration : 1760
train acc:  0.703125
train loss:  0.5243399143218994
train gradient:  0.13948284446555653
iteration : 1761
train acc:  0.6640625
train loss:  0.605317234992981
train gradient:  0.23167849948187796
iteration : 1762
train acc:  0.65625
train loss:  0.545384407043457
train gradient:  0.21073500914310375
iteration : 1763
train acc:  0.7578125
train loss:  0.5231184959411621
train gradient:  0.12193646271876643
iteration : 1764
train acc:  0.6796875
train loss:  0.5353370904922485
train gradient:  0.1694342965085423
iteration : 1765
train acc:  0.6953125
train loss:  0.5921715497970581
train gradient:  0.19519340407943947
iteration : 1766
train acc:  0.6796875
train loss:  0.5893732309341431
train gradient:  0.17133968788561232
iteration : 1767
train acc:  0.6953125
train loss:  0.5506383776664734
train gradient:  0.131590318493305
iteration : 1768
train acc:  0.7265625
train loss:  0.5593849420547485
train gradient:  0.2226687695981785
iteration : 1769
train acc:  0.6953125
train loss:  0.5982518196105957
train gradient:  0.19497533980658527
iteration : 1770
train acc:  0.6796875
train loss:  0.5293415188789368
train gradient:  0.12893602094627288
iteration : 1771
train acc:  0.6640625
train loss:  0.5636841058731079
train gradient:  0.18604429494705144
iteration : 1772
train acc:  0.6953125
train loss:  0.5722492933273315
train gradient:  0.16688669783582452
iteration : 1773
train acc:  0.6484375
train loss:  0.5740339756011963
train gradient:  0.16879359208898137
iteration : 1774
train acc:  0.734375
train loss:  0.5521821975708008
train gradient:  0.2117624563368159
iteration : 1775
train acc:  0.703125
train loss:  0.5379747152328491
train gradient:  0.34146631498481567
iteration : 1776
train acc:  0.71875
train loss:  0.5467488765716553
train gradient:  0.13436292294497906
iteration : 1777
train acc:  0.6875
train loss:  0.5564113855361938
train gradient:  0.16533363015110222
iteration : 1778
train acc:  0.671875
train loss:  0.5807946920394897
train gradient:  0.16130426578919388
iteration : 1779
train acc:  0.7109375
train loss:  0.5604748725891113
train gradient:  0.1508527225718832
iteration : 1780
train acc:  0.703125
train loss:  0.5694799423217773
train gradient:  0.17005572257135393
iteration : 1781
train acc:  0.7265625
train loss:  0.4938490390777588
train gradient:  0.11211728554471409
iteration : 1782
train acc:  0.734375
train loss:  0.5137758255004883
train gradient:  0.14740983761532836
iteration : 1783
train acc:  0.7265625
train loss:  0.5453109741210938
train gradient:  0.17642114481215304
iteration : 1784
train acc:  0.6953125
train loss:  0.5491130352020264
train gradient:  0.14941177282774737
iteration : 1785
train acc:  0.7109375
train loss:  0.5330930352210999
train gradient:  0.11863892967202001
iteration : 1786
train acc:  0.6796875
train loss:  0.6246787309646606
train gradient:  0.2059843068789235
iteration : 1787
train acc:  0.6953125
train loss:  0.5818279385566711
train gradient:  0.13962169331383195
iteration : 1788
train acc:  0.7265625
train loss:  0.5335299372673035
train gradient:  0.19035141688463242
iteration : 1789
train acc:  0.625
train loss:  0.5916681289672852
train gradient:  0.1475868454703898
iteration : 1790
train acc:  0.65625
train loss:  0.5827608108520508
train gradient:  0.20585115610435278
iteration : 1791
train acc:  0.6875
train loss:  0.5634329915046692
train gradient:  0.2731663079271844
iteration : 1792
train acc:  0.7265625
train loss:  0.5061296820640564
train gradient:  0.13891752049210443
iteration : 1793
train acc:  0.6953125
train loss:  0.5660510063171387
train gradient:  0.19610769330806588
iteration : 1794
train acc:  0.6875
train loss:  0.5384509563446045
train gradient:  0.1623660463275388
iteration : 1795
train acc:  0.71875
train loss:  0.5045000910758972
train gradient:  0.11357750488916231
iteration : 1796
train acc:  0.6484375
train loss:  0.5936027765274048
train gradient:  0.18861200735843353
iteration : 1797
train acc:  0.6796875
train loss:  0.5744144916534424
train gradient:  0.17250698386683394
iteration : 1798
train acc:  0.75
train loss:  0.5451229214668274
train gradient:  0.19464701080895874
iteration : 1799
train acc:  0.7578125
train loss:  0.4740685820579529
train gradient:  0.16035243775561003
iteration : 1800
train acc:  0.6875
train loss:  0.5417307615280151
train gradient:  0.239695485748183
iteration : 1801
train acc:  0.7265625
train loss:  0.5532859563827515
train gradient:  0.18154211449473276
iteration : 1802
train acc:  0.703125
train loss:  0.5402159690856934
train gradient:  0.12166893014950035
iteration : 1803
train acc:  0.734375
train loss:  0.5219484567642212
train gradient:  0.13523328281432262
iteration : 1804
train acc:  0.6640625
train loss:  0.5437679886817932
train gradient:  0.13766802018580032
iteration : 1805
train acc:  0.671875
train loss:  0.5385370254516602
train gradient:  0.1165404825576391
iteration : 1806
train acc:  0.7265625
train loss:  0.5073009729385376
train gradient:  0.13328321940592774
iteration : 1807
train acc:  0.640625
train loss:  0.5989744663238525
train gradient:  0.19649763120331176
iteration : 1808
train acc:  0.65625
train loss:  0.6033245325088501
train gradient:  0.2104293499287544
iteration : 1809
train acc:  0.7421875
train loss:  0.5262248516082764
train gradient:  0.35003572795022164
iteration : 1810
train acc:  0.7265625
train loss:  0.540186882019043
train gradient:  0.1832425775503757
iteration : 1811
train acc:  0.7265625
train loss:  0.5263946652412415
train gradient:  0.15859227908952772
iteration : 1812
train acc:  0.6875
train loss:  0.5657609105110168
train gradient:  0.1751208735600147
iteration : 1813
train acc:  0.75
train loss:  0.5285757780075073
train gradient:  0.15616748738680697
iteration : 1814
train acc:  0.6171875
train loss:  0.5626894235610962
train gradient:  0.146200307031342
iteration : 1815
train acc:  0.71875
train loss:  0.510594367980957
train gradient:  0.14839200380689205
iteration : 1816
train acc:  0.7421875
train loss:  0.5139918327331543
train gradient:  0.11583255768009779
iteration : 1817
train acc:  0.640625
train loss:  0.5830422639846802
train gradient:  0.14520523555879006
iteration : 1818
train acc:  0.734375
train loss:  0.559991180896759
train gradient:  0.14200066347667822
iteration : 1819
train acc:  0.6640625
train loss:  0.5960469245910645
train gradient:  0.18682060285120008
iteration : 1820
train acc:  0.6875
train loss:  0.5947661399841309
train gradient:  0.2663062108819258
iteration : 1821
train acc:  0.6953125
train loss:  0.5748516321182251
train gradient:  0.14222369407273397
iteration : 1822
train acc:  0.671875
train loss:  0.5404632091522217
train gradient:  0.15735766178290778
iteration : 1823
train acc:  0.7109375
train loss:  0.521379828453064
train gradient:  0.13911002716728552
iteration : 1824
train acc:  0.7265625
train loss:  0.5254409313201904
train gradient:  0.13067684339798924
iteration : 1825
train acc:  0.7265625
train loss:  0.5554283857345581
train gradient:  0.19775474016483208
iteration : 1826
train acc:  0.6484375
train loss:  0.6261861324310303
train gradient:  0.2732863039521809
iteration : 1827
train acc:  0.7265625
train loss:  0.5391173362731934
train gradient:  0.2602377425524037
iteration : 1828
train acc:  0.65625
train loss:  0.5696456432342529
train gradient:  0.1517839380008773
iteration : 1829
train acc:  0.7421875
train loss:  0.5067166090011597
train gradient:  0.1442224004875425
iteration : 1830
train acc:  0.6640625
train loss:  0.5901631116867065
train gradient:  0.19465026898215282
iteration : 1831
train acc:  0.75
train loss:  0.5132768154144287
train gradient:  0.13906021796737
iteration : 1832
train acc:  0.6484375
train loss:  0.5959912538528442
train gradient:  0.17305031774030877
iteration : 1833
train acc:  0.6953125
train loss:  0.5706626772880554
train gradient:  0.20161317730018102
iteration : 1834
train acc:  0.7890625
train loss:  0.47649943828582764
train gradient:  0.1366100824771045
iteration : 1835
train acc:  0.6875
train loss:  0.5555523633956909
train gradient:  0.20103105444826597
iteration : 1836
train acc:  0.6953125
train loss:  0.5585954189300537
train gradient:  0.1686304138292214
iteration : 1837
train acc:  0.6953125
train loss:  0.5312021374702454
train gradient:  0.14364791215796346
iteration : 1838
train acc:  0.7109375
train loss:  0.5436341762542725
train gradient:  0.172266325142077
iteration : 1839
train acc:  0.71875
train loss:  0.5590924024581909
train gradient:  0.13929355541282892
iteration : 1840
train acc:  0.765625
train loss:  0.5205913782119751
train gradient:  0.16296049750061006
iteration : 1841
train acc:  0.6875
train loss:  0.6057148575782776
train gradient:  0.2012569174864683
iteration : 1842
train acc:  0.71875
train loss:  0.5501798391342163
train gradient:  0.14697324240340498
iteration : 1843
train acc:  0.71875
train loss:  0.5479786992073059
train gradient:  0.1971912671245629
iteration : 1844
train acc:  0.6953125
train loss:  0.5882735848426819
train gradient:  0.188304621676825
iteration : 1845
train acc:  0.71875
train loss:  0.552446722984314
train gradient:  0.18350288316167895
iteration : 1846
train acc:  0.6328125
train loss:  0.596438467502594
train gradient:  0.1822814574374102
iteration : 1847
train acc:  0.703125
train loss:  0.6063064336776733
train gradient:  0.22270229344347642
iteration : 1848
train acc:  0.734375
train loss:  0.5365259647369385
train gradient:  0.14132961459254167
iteration : 1849
train acc:  0.71875
train loss:  0.5363729000091553
train gradient:  0.1379058884076702
iteration : 1850
train acc:  0.7421875
train loss:  0.579897940158844
train gradient:  0.17243456556951864
iteration : 1851
train acc:  0.671875
train loss:  0.5927691459655762
train gradient:  0.23714616071767558
iteration : 1852
train acc:  0.765625
train loss:  0.5330081582069397
train gradient:  0.15835307026590528
iteration : 1853
train acc:  0.7265625
train loss:  0.5213559865951538
train gradient:  0.16572640694608914
iteration : 1854
train acc:  0.6796875
train loss:  0.5968601703643799
train gradient:  0.27402974888731746
iteration : 1855
train acc:  0.7109375
train loss:  0.5105917453765869
train gradient:  0.14377176270445852
iteration : 1856
train acc:  0.640625
train loss:  0.6005796194076538
train gradient:  0.17712696919127346
iteration : 1857
train acc:  0.734375
train loss:  0.4983762502670288
train gradient:  0.13445237186397108
iteration : 1858
train acc:  0.7109375
train loss:  0.5306074023246765
train gradient:  0.26627849330830644
iteration : 1859
train acc:  0.6640625
train loss:  0.5478115081787109
train gradient:  0.1315089131054098
iteration : 1860
train acc:  0.71875
train loss:  0.5869491100311279
train gradient:  0.16362804633841538
iteration : 1861
train acc:  0.6796875
train loss:  0.584017276763916
train gradient:  0.15665563353403017
iteration : 1862
train acc:  0.6484375
train loss:  0.6220654249191284
train gradient:  0.17123399481324003
iteration : 1863
train acc:  0.7421875
train loss:  0.5341857075691223
train gradient:  0.19905470255397328
iteration : 1864
train acc:  0.7109375
train loss:  0.5108950138092041
train gradient:  0.18547533140766986
iteration : 1865
train acc:  0.6953125
train loss:  0.554854154586792
train gradient:  0.2164979920443325
iteration : 1866
train acc:  0.7265625
train loss:  0.5150995254516602
train gradient:  0.16304135423225397
iteration : 1867
train acc:  0.7734375
train loss:  0.49435192346572876
train gradient:  0.14780023963478056
iteration : 1868
train acc:  0.734375
train loss:  0.5460055470466614
train gradient:  0.18152032466134688
iteration : 1869
train acc:  0.7265625
train loss:  0.5159784555435181
train gradient:  0.1489825107671463
iteration : 1870
train acc:  0.7265625
train loss:  0.5350072383880615
train gradient:  0.1604354327340174
iteration : 1871
train acc:  0.6953125
train loss:  0.5539734363555908
train gradient:  0.16067572794403315
iteration : 1872
train acc:  0.640625
train loss:  0.6214599609375
train gradient:  0.3271047553377202
iteration : 1873
train acc:  0.6953125
train loss:  0.5559094548225403
train gradient:  0.1513364126023984
iteration : 1874
train acc:  0.7109375
train loss:  0.5595220923423767
train gradient:  0.13526965361180338
iteration : 1875
train acc:  0.6875
train loss:  0.5281267166137695
train gradient:  0.1236204774405371
iteration : 1876
train acc:  0.6875
train loss:  0.5197895765304565
train gradient:  0.14674982913488965
iteration : 1877
train acc:  0.6640625
train loss:  0.5756189823150635
train gradient:  0.15965141705753427
iteration : 1878
train acc:  0.7890625
train loss:  0.5009400248527527
train gradient:  0.17859068093159586
iteration : 1879
train acc:  0.65625
train loss:  0.5623986721038818
train gradient:  0.1662233318474336
iteration : 1880
train acc:  0.7578125
train loss:  0.5160754919052124
train gradient:  0.13985351562429627
iteration : 1881
train acc:  0.7109375
train loss:  0.5451797246932983
train gradient:  0.18700000963085237
iteration : 1882
train acc:  0.640625
train loss:  0.6143766641616821
train gradient:  0.20215606041278245
iteration : 1883
train acc:  0.703125
train loss:  0.5498348474502563
train gradient:  0.12358780052597458
iteration : 1884
train acc:  0.6640625
train loss:  0.5990071296691895
train gradient:  0.1627576456288732
iteration : 1885
train acc:  0.71875
train loss:  0.5613331198692322
train gradient:  0.15249712417383599
iteration : 1886
train acc:  0.703125
train loss:  0.5681029558181763
train gradient:  0.17778317614975242
iteration : 1887
train acc:  0.7265625
train loss:  0.49093419313430786
train gradient:  0.14046861985783865
iteration : 1888
train acc:  0.6953125
train loss:  0.518254816532135
train gradient:  0.11479231592625001
iteration : 1889
train acc:  0.7265625
train loss:  0.49013692140579224
train gradient:  0.14452332926251701
iteration : 1890
train acc:  0.703125
train loss:  0.5474203824996948
train gradient:  0.14629045844026406
iteration : 1891
train acc:  0.71875
train loss:  0.508013904094696
train gradient:  0.1355786189540469
iteration : 1892
train acc:  0.6484375
train loss:  0.567884087562561
train gradient:  0.16451200189176185
iteration : 1893
train acc:  0.7734375
train loss:  0.5322366952896118
train gradient:  0.12803048446488952
iteration : 1894
train acc:  0.75
train loss:  0.47279855608940125
train gradient:  0.12173916032554624
iteration : 1895
train acc:  0.703125
train loss:  0.5471925735473633
train gradient:  0.18327608995019695
iteration : 1896
train acc:  0.71875
train loss:  0.5142649412155151
train gradient:  0.1451470963790667
iteration : 1897
train acc:  0.6953125
train loss:  0.5744531154632568
train gradient:  0.14340587529243112
iteration : 1898
train acc:  0.7421875
train loss:  0.5035653710365295
train gradient:  0.2197244389063367
iteration : 1899
train acc:  0.703125
train loss:  0.5692579746246338
train gradient:  0.1643568391021543
iteration : 1900
train acc:  0.75
train loss:  0.5132924318313599
train gradient:  0.16466222319929208
iteration : 1901
train acc:  0.75
train loss:  0.5049604177474976
train gradient:  0.14556314062292391
iteration : 1902
train acc:  0.65625
train loss:  0.56484055519104
train gradient:  0.16769305253640138
iteration : 1903
train acc:  0.7109375
train loss:  0.5701109170913696
train gradient:  0.1741789469827466
iteration : 1904
train acc:  0.640625
train loss:  0.6300933957099915
train gradient:  0.18618194094059373
iteration : 1905
train acc:  0.7890625
train loss:  0.48870015144348145
train gradient:  0.1094194880681747
iteration : 1906
train acc:  0.59375
train loss:  0.6490354537963867
train gradient:  0.2183688483232183
iteration : 1907
train acc:  0.6953125
train loss:  0.5645098686218262
train gradient:  0.1638499453546766
iteration : 1908
train acc:  0.7578125
train loss:  0.5310428738594055
train gradient:  0.15507095138810473
iteration : 1909
train acc:  0.6640625
train loss:  0.6023672223091125
train gradient:  0.16599931649331917
iteration : 1910
train acc:  0.71875
train loss:  0.5465158224105835
train gradient:  0.1123155118006048
iteration : 1911
train acc:  0.671875
train loss:  0.5465975999832153
train gradient:  0.16902237474213688
iteration : 1912
train acc:  0.7109375
train loss:  0.5465044975280762
train gradient:  0.1604196178076165
iteration : 1913
train acc:  0.6796875
train loss:  0.6005136966705322
train gradient:  0.20224859502728104
iteration : 1914
train acc:  0.6640625
train loss:  0.5696649551391602
train gradient:  0.15620880501595458
iteration : 1915
train acc:  0.734375
train loss:  0.5426520109176636
train gradient:  0.1869250703877392
iteration : 1916
train acc:  0.7265625
train loss:  0.5239593982696533
train gradient:  0.17539508662746015
iteration : 1917
train acc:  0.6875
train loss:  0.5880714654922485
train gradient:  0.18159650210801592
iteration : 1918
train acc:  0.71875
train loss:  0.5252059698104858
train gradient:  0.14195170040363975
iteration : 1919
train acc:  0.703125
train loss:  0.548291802406311
train gradient:  0.10978161042431879
iteration : 1920
train acc:  0.703125
train loss:  0.49745243787765503
train gradient:  0.18513554499627852
iteration : 1921
train acc:  0.6796875
train loss:  0.5588428378105164
train gradient:  0.1384648409495431
iteration : 1922
train acc:  0.765625
train loss:  0.5075913071632385
train gradient:  0.1295930012178998
iteration : 1923
train acc:  0.6796875
train loss:  0.5884193778038025
train gradient:  0.19096459807551858
iteration : 1924
train acc:  0.7109375
train loss:  0.5525950193405151
train gradient:  0.19600963965349597
iteration : 1925
train acc:  0.734375
train loss:  0.5121257901191711
train gradient:  0.10265234275702531
iteration : 1926
train acc:  0.6953125
train loss:  0.5452035665512085
train gradient:  0.15846768533250838
iteration : 1927
train acc:  0.734375
train loss:  0.5047011971473694
train gradient:  0.15834597246372428
iteration : 1928
train acc:  0.7890625
train loss:  0.48495468497276306
train gradient:  0.11650406073791439
iteration : 1929
train acc:  0.71875
train loss:  0.5542697906494141
train gradient:  0.15903320598602186
iteration : 1930
train acc:  0.7734375
train loss:  0.46219485998153687
train gradient:  0.11433640340207751
iteration : 1931
train acc:  0.734375
train loss:  0.5089714527130127
train gradient:  0.13523669545054012
iteration : 1932
train acc:  0.6484375
train loss:  0.5947449207305908
train gradient:  0.2291662095447632
iteration : 1933
train acc:  0.7109375
train loss:  0.5887655019760132
train gradient:  0.2387125173933185
iteration : 1934
train acc:  0.6953125
train loss:  0.5220839977264404
train gradient:  0.15274718968212347
iteration : 1935
train acc:  0.6328125
train loss:  0.6069143414497375
train gradient:  0.1776350397524345
iteration : 1936
train acc:  0.6953125
train loss:  0.5570753812789917
train gradient:  0.1440331484605341
iteration : 1937
train acc:  0.71875
train loss:  0.5148648023605347
train gradient:  0.14791579510926084
iteration : 1938
train acc:  0.65625
train loss:  0.5692868828773499
train gradient:  0.16926790950902704
iteration : 1939
train acc:  0.7421875
train loss:  0.5346212983131409
train gradient:  0.21185947558553717
iteration : 1940
train acc:  0.6875
train loss:  0.5282822847366333
train gradient:  0.1337053127869828
iteration : 1941
train acc:  0.7265625
train loss:  0.5283815264701843
train gradient:  0.1637220955918724
iteration : 1942
train acc:  0.6953125
train loss:  0.516841471195221
train gradient:  0.13722345969501495
iteration : 1943
train acc:  0.71875
train loss:  0.5093969702720642
train gradient:  0.15668698474631682
iteration : 1944
train acc:  0.7578125
train loss:  0.5076264142990112
train gradient:  0.13777045871347665
iteration : 1945
train acc:  0.6875
train loss:  0.5814605951309204
train gradient:  0.15813906504280212
iteration : 1946
train acc:  0.671875
train loss:  0.580482006072998
train gradient:  0.16342926769805088
iteration : 1947
train acc:  0.6875
train loss:  0.5747129917144775
train gradient:  0.15411888346947436
iteration : 1948
train acc:  0.6875
train loss:  0.5572168827056885
train gradient:  0.1214857426420092
iteration : 1949
train acc:  0.7109375
train loss:  0.526515007019043
train gradient:  0.13783904599063196
iteration : 1950
train acc:  0.75
train loss:  0.5092080235481262
train gradient:  0.13232555438757476
iteration : 1951
train acc:  0.6953125
train loss:  0.5801408290863037
train gradient:  0.16339499686881048
iteration : 1952
train acc:  0.6796875
train loss:  0.5849723815917969
train gradient:  0.26907958863240383
iteration : 1953
train acc:  0.6640625
train loss:  0.6140947341918945
train gradient:  0.2053554568871483
iteration : 1954
train acc:  0.671875
train loss:  0.5653866529464722
train gradient:  0.13270438198901002
iteration : 1955
train acc:  0.671875
train loss:  0.571824312210083
train gradient:  0.17136771553140173
iteration : 1956
train acc:  0.7578125
train loss:  0.4951600134372711
train gradient:  0.13557361421703804
iteration : 1957
train acc:  0.71875
train loss:  0.5303792953491211
train gradient:  0.22002244835152762
iteration : 1958
train acc:  0.6875
train loss:  0.5606197118759155
train gradient:  0.20113965073502266
iteration : 1959
train acc:  0.7265625
train loss:  0.5572506189346313
train gradient:  0.14011841934775152
iteration : 1960
train acc:  0.6328125
train loss:  0.5677134394645691
train gradient:  0.17043561804165144
iteration : 1961
train acc:  0.6484375
train loss:  0.6152964234352112
train gradient:  0.2122494325687632
iteration : 1962
train acc:  0.6328125
train loss:  0.5931271314620972
train gradient:  0.17603314839317868
iteration : 1963
train acc:  0.7421875
train loss:  0.5097631216049194
train gradient:  0.13950320815854164
iteration : 1964
train acc:  0.6953125
train loss:  0.5692475438117981
train gradient:  0.13355054722129692
iteration : 1965
train acc:  0.734375
train loss:  0.5028921365737915
train gradient:  0.1380890554208299
iteration : 1966
train acc:  0.703125
train loss:  0.5694185495376587
train gradient:  0.21709133628256005
iteration : 1967
train acc:  0.703125
train loss:  0.5704982876777649
train gradient:  0.19025364808945355
iteration : 1968
train acc:  0.75
train loss:  0.5049077272415161
train gradient:  0.15933021535947592
iteration : 1969
train acc:  0.7109375
train loss:  0.5664583444595337
train gradient:  0.3787696196150284
iteration : 1970
train acc:  0.8125
train loss:  0.4486790895462036
train gradient:  0.11977603479962803
iteration : 1971
train acc:  0.75
train loss:  0.5062000751495361
train gradient:  0.14208099521295048
iteration : 1972
train acc:  0.7109375
train loss:  0.5140028595924377
train gradient:  0.13739684106063793
iteration : 1973
train acc:  0.7734375
train loss:  0.5042753219604492
train gradient:  0.14818086553836773
iteration : 1974
train acc:  0.78125
train loss:  0.47863543033599854
train gradient:  0.13413478379384566
iteration : 1975
train acc:  0.6328125
train loss:  0.6106074452400208
train gradient:  0.22499138917539974
iteration : 1976
train acc:  0.6640625
train loss:  0.6138428449630737
train gradient:  0.1823766977286404
iteration : 1977
train acc:  0.703125
train loss:  0.5597330927848816
train gradient:  0.15294300984498022
iteration : 1978
train acc:  0.6796875
train loss:  0.5818560123443604
train gradient:  0.15564219243274985
iteration : 1979
train acc:  0.703125
train loss:  0.5860203504562378
train gradient:  0.1952556178368801
iteration : 1980
train acc:  0.6484375
train loss:  0.5662951469421387
train gradient:  0.17607361376949843
iteration : 1981
train acc:  0.65625
train loss:  0.5706791877746582
train gradient:  0.22324318871595916
iteration : 1982
train acc:  0.7109375
train loss:  0.5470929145812988
train gradient:  0.12684316685151492
iteration : 1983
train acc:  0.71875
train loss:  0.535058856010437
train gradient:  0.17421398266821359
iteration : 1984
train acc:  0.765625
train loss:  0.49511122703552246
train gradient:  0.17574545340669945
iteration : 1985
train acc:  0.6953125
train loss:  0.5595791339874268
train gradient:  0.14238725711045092
iteration : 1986
train acc:  0.71875
train loss:  0.5622559785842896
train gradient:  0.20086155317750143
iteration : 1987
train acc:  0.71875
train loss:  0.5290248394012451
train gradient:  0.1357732198593501
iteration : 1988
train acc:  0.7265625
train loss:  0.5397791862487793
train gradient:  0.1447235883698571
iteration : 1989
train acc:  0.7109375
train loss:  0.5491406917572021
train gradient:  0.1678440507875257
iteration : 1990
train acc:  0.6953125
train loss:  0.5631004571914673
train gradient:  0.2543937309102196
iteration : 1991
train acc:  0.7578125
train loss:  0.49296289682388306
train gradient:  0.11486497798807543
iteration : 1992
train acc:  0.7890625
train loss:  0.4549033045768738
train gradient:  0.11806805062071765
iteration : 1993
train acc:  0.6640625
train loss:  0.5710700154304504
train gradient:  0.14723840351149495
iteration : 1994
train acc:  0.6875
train loss:  0.5529825687408447
train gradient:  0.15724464656586945
iteration : 1995
train acc:  0.7109375
train loss:  0.5302073955535889
train gradient:  0.1443346872554932
iteration : 1996
train acc:  0.6875
train loss:  0.6037560701370239
train gradient:  0.18997692165244573
iteration : 1997
train acc:  0.765625
train loss:  0.5227490663528442
train gradient:  0.11953682363642613
iteration : 1998
train acc:  0.6796875
train loss:  0.5370239019393921
train gradient:  0.19581545149889462
iteration : 1999
train acc:  0.6796875
train loss:  0.6100246906280518
train gradient:  0.18258665592507115
iteration : 2000
train acc:  0.71875
train loss:  0.5330901145935059
train gradient:  0.16588701215823287
iteration : 2001
train acc:  0.75
train loss:  0.47682803869247437
train gradient:  0.1582402928925815
iteration : 2002
train acc:  0.671875
train loss:  0.5942700505256653
train gradient:  0.16879278531280245
iteration : 2003
train acc:  0.75
train loss:  0.5363999605178833
train gradient:  0.11046273394863604
iteration : 2004
train acc:  0.703125
train loss:  0.5585266351699829
train gradient:  0.14687381858019455
iteration : 2005
train acc:  0.734375
train loss:  0.5244626998901367
train gradient:  0.16396169189049264
iteration : 2006
train acc:  0.7265625
train loss:  0.5152816772460938
train gradient:  0.1367487309683669
iteration : 2007
train acc:  0.78125
train loss:  0.48554718494415283
train gradient:  0.16647402673113607
iteration : 2008
train acc:  0.640625
train loss:  0.6028878688812256
train gradient:  0.1709421114248489
iteration : 2009
train acc:  0.609375
train loss:  0.6923762559890747
train gradient:  0.2439542950446129
iteration : 2010
train acc:  0.7109375
train loss:  0.5101755857467651
train gradient:  0.14337429767440196
iteration : 2011
train acc:  0.6953125
train loss:  0.5795358419418335
train gradient:  0.15786242871780715
iteration : 2012
train acc:  0.765625
train loss:  0.5115139484405518
train gradient:  0.14827075487137653
iteration : 2013
train acc:  0.734375
train loss:  0.5079956650733948
train gradient:  0.12677627334661343
iteration : 2014
train acc:  0.71875
train loss:  0.5738608837127686
train gradient:  0.1821592076323033
iteration : 2015
train acc:  0.6640625
train loss:  0.5682550668716431
train gradient:  0.21271081158349214
iteration : 2016
train acc:  0.7421875
train loss:  0.5358541011810303
train gradient:  0.11618583326618477
iteration : 2017
train acc:  0.75
train loss:  0.5504838228225708
train gradient:  0.2403123485964896
iteration : 2018
train acc:  0.6796875
train loss:  0.5533057451248169
train gradient:  0.12951125501139446
iteration : 2019
train acc:  0.7265625
train loss:  0.5995775461196899
train gradient:  0.1539460616584406
iteration : 2020
train acc:  0.6171875
train loss:  0.6315459609031677
train gradient:  0.21891596558438145
iteration : 2021
train acc:  0.7265625
train loss:  0.5540745258331299
train gradient:  0.21412414636199273
iteration : 2022
train acc:  0.765625
train loss:  0.48530519008636475
train gradient:  0.16265640796166592
iteration : 2023
train acc:  0.6875
train loss:  0.5154974460601807
train gradient:  0.12906811058945106
iteration : 2024
train acc:  0.71875
train loss:  0.5378220677375793
train gradient:  0.1519603171381516
iteration : 2025
train acc:  0.75
train loss:  0.5054399967193604
train gradient:  0.1465868413762983
iteration : 2026
train acc:  0.6796875
train loss:  0.5661110877990723
train gradient:  0.17213123473523131
iteration : 2027
train acc:  0.6875
train loss:  0.5499371290206909
train gradient:  0.2293545360723785
iteration : 2028
train acc:  0.703125
train loss:  0.5087206363677979
train gradient:  0.13427156394323386
iteration : 2029
train acc:  0.6640625
train loss:  0.5899804830551147
train gradient:  0.1871920852965613
iteration : 2030
train acc:  0.734375
train loss:  0.5302565097808838
train gradient:  0.15881446243949093
iteration : 2031
train acc:  0.65625
train loss:  0.6159354448318481
train gradient:  0.16896143641702427
iteration : 2032
train acc:  0.7890625
train loss:  0.5043998956680298
train gradient:  0.14093048979835215
iteration : 2033
train acc:  0.7421875
train loss:  0.5184205770492554
train gradient:  0.14076628653482737
iteration : 2034
train acc:  0.7109375
train loss:  0.536811113357544
train gradient:  0.11794273396655533
iteration : 2035
train acc:  0.703125
train loss:  0.502210795879364
train gradient:  0.15928125531694798
iteration : 2036
train acc:  0.7109375
train loss:  0.5293562412261963
train gradient:  0.1866148223520788
iteration : 2037
train acc:  0.6875
train loss:  0.5988574624061584
train gradient:  0.19125340994407547
iteration : 2038
train acc:  0.734375
train loss:  0.5319951772689819
train gradient:  0.17694479002114194
iteration : 2039
train acc:  0.671875
train loss:  0.5496256351470947
train gradient:  0.13577105930367106
iteration : 2040
train acc:  0.65625
train loss:  0.5628039836883545
train gradient:  0.1613656436708328
iteration : 2041
train acc:  0.6953125
train loss:  0.5327073335647583
train gradient:  0.16032355744995
iteration : 2042
train acc:  0.8125
train loss:  0.49092984199523926
train gradient:  0.12151585108648805
iteration : 2043
train acc:  0.7265625
train loss:  0.5354304313659668
train gradient:  0.10790977504651379
iteration : 2044
train acc:  0.75
train loss:  0.48424670100212097
train gradient:  0.16824761308339498
iteration : 2045
train acc:  0.6484375
train loss:  0.5875377655029297
train gradient:  0.17715226092028546
iteration : 2046
train acc:  0.734375
train loss:  0.49498286843299866
train gradient:  0.13555203248151174
iteration : 2047
train acc:  0.6640625
train loss:  0.619157612323761
train gradient:  0.24904892981744203
iteration : 2048
train acc:  0.6875
train loss:  0.5240484476089478
train gradient:  0.13700669019760897
iteration : 2049
train acc:  0.6796875
train loss:  0.5520966649055481
train gradient:  0.17473594002209267
iteration : 2050
train acc:  0.7265625
train loss:  0.5350101590156555
train gradient:  0.12745024620369438
iteration : 2051
train acc:  0.6875
train loss:  0.5455813407897949
train gradient:  0.16248387427281372
iteration : 2052
train acc:  0.734375
train loss:  0.5103786587715149
train gradient:  0.1334132832734197
iteration : 2053
train acc:  0.609375
train loss:  0.5932368040084839
train gradient:  0.1732683230982187
iteration : 2054
train acc:  0.7421875
train loss:  0.5337226986885071
train gradient:  0.1953216778777963
iteration : 2055
train acc:  0.6796875
train loss:  0.5400452017784119
train gradient:  0.13051678486881219
iteration : 2056
train acc:  0.7265625
train loss:  0.5052577257156372
train gradient:  0.12148680753031943
iteration : 2057
train acc:  0.75
train loss:  0.5029781460762024
train gradient:  0.12550384484504812
iteration : 2058
train acc:  0.6796875
train loss:  0.5740154981613159
train gradient:  0.22364594567337065
iteration : 2059
train acc:  0.703125
train loss:  0.5319894552230835
train gradient:  0.13494493852005734
iteration : 2060
train acc:  0.703125
train loss:  0.5089570879936218
train gradient:  0.23551726999509592
iteration : 2061
train acc:  0.765625
train loss:  0.5017147064208984
train gradient:  0.12271413396982911
iteration : 2062
train acc:  0.7421875
train loss:  0.5296186208724976
train gradient:  0.1500786307571281
iteration : 2063
train acc:  0.7734375
train loss:  0.4945332109928131
train gradient:  0.1339033274543568
iteration : 2064
train acc:  0.703125
train loss:  0.5541263222694397
train gradient:  0.13972357176369737
iteration : 2065
train acc:  0.6796875
train loss:  0.6012334823608398
train gradient:  0.2512718883793521
iteration : 2066
train acc:  0.65625
train loss:  0.619318962097168
train gradient:  0.20913282170906763
iteration : 2067
train acc:  0.671875
train loss:  0.5640510320663452
train gradient:  0.184007826884434
iteration : 2068
train acc:  0.671875
train loss:  0.5706455707550049
train gradient:  0.17903400154697818
iteration : 2069
train acc:  0.6875
train loss:  0.5060233473777771
train gradient:  0.1551802011889174
iteration : 2070
train acc:  0.6875
train loss:  0.613024115562439
train gradient:  0.21665194448809566
iteration : 2071
train acc:  0.7421875
train loss:  0.4608960747718811
train gradient:  0.13297405620533637
iteration : 2072
train acc:  0.65625
train loss:  0.5646896362304688
train gradient:  0.2436404740212158
iteration : 2073
train acc:  0.7421875
train loss:  0.5426981449127197
train gradient:  0.21842295658619615
iteration : 2074
train acc:  0.6796875
train loss:  0.538330078125
train gradient:  0.17584940663714543
iteration : 2075
train acc:  0.671875
train loss:  0.5256670713424683
train gradient:  0.17078420358968144
iteration : 2076
train acc:  0.7265625
train loss:  0.5932594537734985
train gradient:  0.15843584226947954
iteration : 2077
train acc:  0.65625
train loss:  0.5830821990966797
train gradient:  0.18397511219570484
iteration : 2078
train acc:  0.703125
train loss:  0.5462758541107178
train gradient:  0.14625835647983254
iteration : 2079
train acc:  0.7109375
train loss:  0.5669128894805908
train gradient:  0.1832578828256535
iteration : 2080
train acc:  0.7265625
train loss:  0.5533427000045776
train gradient:  0.20449298635489893
iteration : 2081
train acc:  0.7578125
train loss:  0.5021454095840454
train gradient:  0.1389387835348173
iteration : 2082
train acc:  0.6953125
train loss:  0.5713419318199158
train gradient:  0.13973596440624114
iteration : 2083
train acc:  0.8046875
train loss:  0.48952946066856384
train gradient:  0.14989705866714764
iteration : 2084
train acc:  0.703125
train loss:  0.5941492319107056
train gradient:  0.15888001064904966
iteration : 2085
train acc:  0.6953125
train loss:  0.5205714702606201
train gradient:  0.2138546087349331
iteration : 2086
train acc:  0.71875
train loss:  0.5488282442092896
train gradient:  0.1588490633532987
iteration : 2087
train acc:  0.7109375
train loss:  0.544876754283905
train gradient:  0.15090532346524807
iteration : 2088
train acc:  0.6875
train loss:  0.603675365447998
train gradient:  0.16710100359791752
iteration : 2089
train acc:  0.7109375
train loss:  0.5625985860824585
train gradient:  0.18774529634440784
iteration : 2090
train acc:  0.7109375
train loss:  0.507765531539917
train gradient:  0.12266976388134261
iteration : 2091
train acc:  0.6796875
train loss:  0.5684057474136353
train gradient:  0.17156321477760855
iteration : 2092
train acc:  0.6953125
train loss:  0.5455860495567322
train gradient:  0.1636792815757922
iteration : 2093
train acc:  0.703125
train loss:  0.5018470883369446
train gradient:  0.13609959773497413
iteration : 2094
train acc:  0.6796875
train loss:  0.5768964290618896
train gradient:  0.1783258596841284
iteration : 2095
train acc:  0.7265625
train loss:  0.4975309371948242
train gradient:  0.11601211431568073
iteration : 2096
train acc:  0.7421875
train loss:  0.4974685311317444
train gradient:  0.15038651913047307
iteration : 2097
train acc:  0.765625
train loss:  0.49491024017333984
train gradient:  0.1486443378573915
iteration : 2098
train acc:  0.7265625
train loss:  0.5567342638969421
train gradient:  0.14372951869015196
iteration : 2099
train acc:  0.6484375
train loss:  0.5697649121284485
train gradient:  0.15756591693371347
iteration : 2100
train acc:  0.78125
train loss:  0.46926790475845337
train gradient:  0.1596729103438638
iteration : 2101
train acc:  0.75
train loss:  0.5300028324127197
train gradient:  0.1386470516617484
iteration : 2102
train acc:  0.7265625
train loss:  0.5237233638763428
train gradient:  0.1525605200259182
iteration : 2103
train acc:  0.71875
train loss:  0.5635565519332886
train gradient:  0.24925131551661084
iteration : 2104
train acc:  0.7265625
train loss:  0.5133568644523621
train gradient:  0.14556292204874274
iteration : 2105
train acc:  0.71875
train loss:  0.539370596408844
train gradient:  0.1611885676186794
iteration : 2106
train acc:  0.6953125
train loss:  0.5845627784729004
train gradient:  0.21043768685519726
iteration : 2107
train acc:  0.640625
train loss:  0.559391975402832
train gradient:  0.20392874445623105
iteration : 2108
train acc:  0.75
train loss:  0.4958842992782593
train gradient:  0.17074584479609597
iteration : 2109
train acc:  0.6953125
train loss:  0.6108348369598389
train gradient:  0.2398992040255365
iteration : 2110
train acc:  0.7421875
train loss:  0.5210872888565063
train gradient:  0.1431999185053584
iteration : 2111
train acc:  0.609375
train loss:  0.6399749517440796
train gradient:  0.21924899597936612
iteration : 2112
train acc:  0.703125
train loss:  0.5373003482818604
train gradient:  0.24062793993566925
iteration : 2113
train acc:  0.703125
train loss:  0.5339794158935547
train gradient:  0.14121353821293914
iteration : 2114
train acc:  0.671875
train loss:  0.556317150592804
train gradient:  0.13198794652817175
iteration : 2115
train acc:  0.6796875
train loss:  0.5405477285385132
train gradient:  0.13995097201008797
iteration : 2116
train acc:  0.7109375
train loss:  0.551105260848999
train gradient:  0.15743777930906305
iteration : 2117
train acc:  0.671875
train loss:  0.5795972347259521
train gradient:  0.22699673875404075
iteration : 2118
train acc:  0.7109375
train loss:  0.48481252789497375
train gradient:  0.11886942603488149
iteration : 2119
train acc:  0.65625
train loss:  0.572021484375
train gradient:  0.22335755033829177
iteration : 2120
train acc:  0.6953125
train loss:  0.5696823596954346
train gradient:  0.17374411486287789
iteration : 2121
train acc:  0.7265625
train loss:  0.5261986255645752
train gradient:  0.18623119130661442
iteration : 2122
train acc:  0.65625
train loss:  0.5560473203659058
train gradient:  0.16073094716205966
iteration : 2123
train acc:  0.6875
train loss:  0.5402501821517944
train gradient:  0.21100576649336483
iteration : 2124
train acc:  0.6015625
train loss:  0.6612921953201294
train gradient:  0.2843042092948245
iteration : 2125
train acc:  0.7265625
train loss:  0.5346652269363403
train gradient:  0.16452451408897154
iteration : 2126
train acc:  0.796875
train loss:  0.4783087968826294
train gradient:  0.14511937037746275
iteration : 2127
train acc:  0.6640625
train loss:  0.5362036228179932
train gradient:  0.15809603696665636
iteration : 2128
train acc:  0.7265625
train loss:  0.5263272523880005
train gradient:  0.15278519045858646
iteration : 2129
train acc:  0.71875
train loss:  0.529812216758728
train gradient:  0.14479969034952814
iteration : 2130
train acc:  0.671875
train loss:  0.5615882873535156
train gradient:  0.22102422841816138
iteration : 2131
train acc:  0.703125
train loss:  0.525301456451416
train gradient:  0.17503105817030956
iteration : 2132
train acc:  0.6796875
train loss:  0.5398097634315491
train gradient:  0.15209176312693135
iteration : 2133
train acc:  0.7265625
train loss:  0.5314040780067444
train gradient:  0.14854715495887005
iteration : 2134
train acc:  0.7421875
train loss:  0.4976004958152771
train gradient:  0.14051450803665994
iteration : 2135
train acc:  0.6875
train loss:  0.5856221914291382
train gradient:  0.1884952015371748
iteration : 2136
train acc:  0.7265625
train loss:  0.5215609073638916
train gradient:  0.13422479579904836
iteration : 2137
train acc:  0.7578125
train loss:  0.5086795687675476
train gradient:  0.1409062625453989
iteration : 2138
train acc:  0.75
train loss:  0.5076541900634766
train gradient:  0.11410243012518533
iteration : 2139
train acc:  0.6484375
train loss:  0.570298433303833
train gradient:  0.20335697515999035
iteration : 2140
train acc:  0.734375
train loss:  0.5043976306915283
train gradient:  0.15610045005164117
iteration : 2141
train acc:  0.71875
train loss:  0.5131962299346924
train gradient:  0.13681917258163154
iteration : 2142
train acc:  0.75
train loss:  0.5041559934616089
train gradient:  0.14280259758266836
iteration : 2143
train acc:  0.765625
train loss:  0.5022436380386353
train gradient:  0.14638731310964131
iteration : 2144
train acc:  0.71875
train loss:  0.5247626304626465
train gradient:  0.14258572290834412
iteration : 2145
train acc:  0.6875
train loss:  0.5992761850357056
train gradient:  0.16552769074771415
iteration : 2146
train acc:  0.75
train loss:  0.5245593190193176
train gradient:  0.1821970862516048
iteration : 2147
train acc:  0.765625
train loss:  0.48741820454597473
train gradient:  0.13370322266731172
iteration : 2148
train acc:  0.7109375
train loss:  0.5083038806915283
train gradient:  0.12162200593444815
iteration : 2149
train acc:  0.7109375
train loss:  0.5394503474235535
train gradient:  0.14323187945261456
iteration : 2150
train acc:  0.6875
train loss:  0.5533998012542725
train gradient:  0.17727735583249843
iteration : 2151
train acc:  0.75
train loss:  0.5181574821472168
train gradient:  0.12796839582150588
iteration : 2152
train acc:  0.7265625
train loss:  0.5125675201416016
train gradient:  0.14533916493228438
iteration : 2153
train acc:  0.734375
train loss:  0.524658739566803
train gradient:  0.16075641540732882
iteration : 2154
train acc:  0.671875
train loss:  0.5528967380523682
train gradient:  0.13853815391442686
iteration : 2155
train acc:  0.734375
train loss:  0.5201650857925415
train gradient:  0.14505350450172444
iteration : 2156
train acc:  0.7265625
train loss:  0.5434283018112183
train gradient:  0.14062371624916115
iteration : 2157
train acc:  0.7890625
train loss:  0.47053971886634827
train gradient:  0.10159339961462303
iteration : 2158
train acc:  0.71875
train loss:  0.5582937598228455
train gradient:  0.2235858252730301
iteration : 2159
train acc:  0.7421875
train loss:  0.5208673477172852
train gradient:  0.1300356276314827
iteration : 2160
train acc:  0.7109375
train loss:  0.5209027528762817
train gradient:  0.13893080861583323
iteration : 2161
train acc:  0.7578125
train loss:  0.5117006897926331
train gradient:  0.14646771881142032
iteration : 2162
train acc:  0.7734375
train loss:  0.46248942613601685
train gradient:  0.14589990796991192
iteration : 2163
train acc:  0.609375
train loss:  0.6429455280303955
train gradient:  0.2545183838849587
iteration : 2164
train acc:  0.7421875
train loss:  0.5557032823562622
train gradient:  0.17199036575100468
iteration : 2165
train acc:  0.7109375
train loss:  0.5515512228012085
train gradient:  0.15597697980736674
iteration : 2166
train acc:  0.7265625
train loss:  0.5159254670143127
train gradient:  0.14652791116870809
iteration : 2167
train acc:  0.71875
train loss:  0.49220412969589233
train gradient:  0.14263501601570228
iteration : 2168
train acc:  0.703125
train loss:  0.5723199844360352
train gradient:  0.1615433209189289
iteration : 2169
train acc:  0.7109375
train loss:  0.49168580770492554
train gradient:  0.1311078248061524
iteration : 2170
train acc:  0.71875
train loss:  0.5427697896957397
train gradient:  0.15871911519396095
iteration : 2171
train acc:  0.828125
train loss:  0.45056748390197754
train gradient:  0.12013781469063907
iteration : 2172
train acc:  0.796875
train loss:  0.4798316955566406
train gradient:  0.1296565199065685
iteration : 2173
train acc:  0.7734375
train loss:  0.4745190739631653
train gradient:  0.11423940207350025
iteration : 2174
train acc:  0.7265625
train loss:  0.5844243764877319
train gradient:  0.20396130265287588
iteration : 2175
train acc:  0.71875
train loss:  0.5271078944206238
train gradient:  0.16879446002766257
iteration : 2176
train acc:  0.6953125
train loss:  0.5606881380081177
train gradient:  0.1928913890722504
iteration : 2177
train acc:  0.75
train loss:  0.4939271807670593
train gradient:  0.15243752184774706
iteration : 2178
train acc:  0.671875
train loss:  0.5311540365219116
train gradient:  0.15054955958810928
iteration : 2179
train acc:  0.765625
train loss:  0.4716191291809082
train gradient:  0.1082054629462549
iteration : 2180
train acc:  0.7421875
train loss:  0.5130138993263245
train gradient:  0.1814091347061062
iteration : 2181
train acc:  0.6953125
train loss:  0.5676735639572144
train gradient:  0.18426338059401515
iteration : 2182
train acc:  0.703125
train loss:  0.5594707727432251
train gradient:  0.17026468266688524
iteration : 2183
train acc:  0.6640625
train loss:  0.6032072305679321
train gradient:  0.17953282263287185
iteration : 2184
train acc:  0.671875
train loss:  0.5667487978935242
train gradient:  0.1437939027911589
iteration : 2185
train acc:  0.765625
train loss:  0.5275073051452637
train gradient:  0.167863101151356
iteration : 2186
train acc:  0.71875
train loss:  0.5256017446517944
train gradient:  0.1374612050324284
iteration : 2187
train acc:  0.7109375
train loss:  0.533527135848999
train gradient:  0.18060723343424562
iteration : 2188
train acc:  0.6796875
train loss:  0.5164488554000854
train gradient:  0.12982593015988886
iteration : 2189
train acc:  0.7734375
train loss:  0.4806739389896393
train gradient:  0.15821885699986693
iteration : 2190
train acc:  0.7109375
train loss:  0.518110990524292
train gradient:  0.18340974485118697
iteration : 2191
train acc:  0.671875
train loss:  0.588245689868927
train gradient:  0.20373817583341447
iteration : 2192
train acc:  0.765625
train loss:  0.4919842481613159
train gradient:  0.14343364500743136
iteration : 2193
train acc:  0.734375
train loss:  0.5067088007926941
train gradient:  0.16421220399175257
iteration : 2194
train acc:  0.7265625
train loss:  0.5803669691085815
train gradient:  0.16305657956479597
iteration : 2195
train acc:  0.6953125
train loss:  0.5921990871429443
train gradient:  0.22099230683513815
iteration : 2196
train acc:  0.703125
train loss:  0.5318998694419861
train gradient:  0.16568946209036534
iteration : 2197
train acc:  0.6953125
train loss:  0.5466630458831787
train gradient:  0.17721571829150912
iteration : 2198
train acc:  0.75
train loss:  0.5015147924423218
train gradient:  0.21270155579451025
iteration : 2199
train acc:  0.7421875
train loss:  0.5366426706314087
train gradient:  0.16078680362037456
iteration : 2200
train acc:  0.6953125
train loss:  0.5344812870025635
train gradient:  0.14471352047606922
iteration : 2201
train acc:  0.7265625
train loss:  0.5453169345855713
train gradient:  0.15529181219534804
iteration : 2202
train acc:  0.6796875
train loss:  0.5620507001876831
train gradient:  0.1633826569417764
iteration : 2203
train acc:  0.6484375
train loss:  0.600345253944397
train gradient:  0.21283948762393012
iteration : 2204
train acc:  0.7578125
train loss:  0.5201408863067627
train gradient:  0.14284837844674986
iteration : 2205
train acc:  0.7109375
train loss:  0.5308635234832764
train gradient:  0.24545442360602215
iteration : 2206
train acc:  0.625
train loss:  0.6396799683570862
train gradient:  0.21952828826818443
iteration : 2207
train acc:  0.71875
train loss:  0.5293769836425781
train gradient:  0.13565002287323746
iteration : 2208
train acc:  0.6875
train loss:  0.590919017791748
train gradient:  0.18710126872100363
iteration : 2209
train acc:  0.6953125
train loss:  0.5381932854652405
train gradient:  0.1352277767346371
iteration : 2210
train acc:  0.7109375
train loss:  0.48834460973739624
train gradient:  0.16423359785312214
iteration : 2211
train acc:  0.6875
train loss:  0.537075936794281
train gradient:  0.24634518131218902
iteration : 2212
train acc:  0.671875
train loss:  0.5663284659385681
train gradient:  0.17304478085777802
iteration : 2213
train acc:  0.6875
train loss:  0.5911285281181335
train gradient:  0.3075210978219934
iteration : 2214
train acc:  0.703125
train loss:  0.5436649918556213
train gradient:  0.18009932603328133
iteration : 2215
train acc:  0.6875
train loss:  0.5427188873291016
train gradient:  0.15957763722699397
iteration : 2216
train acc:  0.7109375
train loss:  0.517693042755127
train gradient:  0.15089482087543465
iteration : 2217
train acc:  0.7578125
train loss:  0.5199788212776184
train gradient:  0.16047099425416322
iteration : 2218
train acc:  0.6953125
train loss:  0.597536563873291
train gradient:  0.22404209179663764
iteration : 2219
train acc:  0.8203125
train loss:  0.4436752200126648
train gradient:  0.10334625681956426
iteration : 2220
train acc:  0.6796875
train loss:  0.582221508026123
train gradient:  0.29755542831965986
iteration : 2221
train acc:  0.6796875
train loss:  0.5533789992332458
train gradient:  0.183877889251774
iteration : 2222
train acc:  0.796875
train loss:  0.47777387499809265
train gradient:  0.13777508677387953
iteration : 2223
train acc:  0.7109375
train loss:  0.5265488624572754
train gradient:  0.18345232525805583
iteration : 2224
train acc:  0.6484375
train loss:  0.5872818231582642
train gradient:  0.20170958354823645
iteration : 2225
train acc:  0.7890625
train loss:  0.49247124791145325
train gradient:  0.140163403606536
iteration : 2226
train acc:  0.7890625
train loss:  0.49898049235343933
train gradient:  0.2308155385869928
iteration : 2227
train acc:  0.6875
train loss:  0.5084631443023682
train gradient:  0.158045598978585
iteration : 2228
train acc:  0.6484375
train loss:  0.6076610684394836
train gradient:  0.23115944447510484
iteration : 2229
train acc:  0.7109375
train loss:  0.5413215160369873
train gradient:  0.15305322471621638
iteration : 2230
train acc:  0.7890625
train loss:  0.466122567653656
train gradient:  0.12046932463204489
iteration : 2231
train acc:  0.7734375
train loss:  0.49461135268211365
train gradient:  0.12449180335251164
iteration : 2232
train acc:  0.734375
train loss:  0.5002149343490601
train gradient:  0.16622952617242337
iteration : 2233
train acc:  0.6640625
train loss:  0.5687136054039001
train gradient:  0.1544873331140471
iteration : 2234
train acc:  0.65625
train loss:  0.5490543246269226
train gradient:  0.1846876988916415
iteration : 2235
train acc:  0.7890625
train loss:  0.4719134271144867
train gradient:  0.1187203549774325
iteration : 2236
train acc:  0.6875
train loss:  0.5719509124755859
train gradient:  0.17903148617575365
iteration : 2237
train acc:  0.71875
train loss:  0.5357760190963745
train gradient:  0.13961214083574686
iteration : 2238
train acc:  0.6640625
train loss:  0.6028205156326294
train gradient:  0.19614807374820648
iteration : 2239
train acc:  0.6953125
train loss:  0.5614503026008606
train gradient:  0.2148128748888401
iteration : 2240
train acc:  0.71875
train loss:  0.5784076452255249
train gradient:  0.19505933072884768
iteration : 2241
train acc:  0.703125
train loss:  0.5716416835784912
train gradient:  0.1621942830771644
iteration : 2242
train acc:  0.7421875
train loss:  0.508551836013794
train gradient:  0.1797592075884062
iteration : 2243
train acc:  0.71875
train loss:  0.5044618248939514
train gradient:  0.18659715564312798
iteration : 2244
train acc:  0.7109375
train loss:  0.5657198429107666
train gradient:  0.2690672536192119
iteration : 2245
train acc:  0.6953125
train loss:  0.5551519989967346
train gradient:  0.1646134590360197
iteration : 2246
train acc:  0.75
train loss:  0.48506033420562744
train gradient:  0.1262311048240702
iteration : 2247
train acc:  0.7109375
train loss:  0.5236446857452393
train gradient:  0.15400944726211596
iteration : 2248
train acc:  0.6953125
train loss:  0.5079092979431152
train gradient:  0.15460797949156252
iteration : 2249
train acc:  0.7109375
train loss:  0.5647171139717102
train gradient:  0.17714238314424693
iteration : 2250
train acc:  0.6015625
train loss:  0.6239228248596191
train gradient:  0.3077642224614708
iteration : 2251
train acc:  0.796875
train loss:  0.4676852226257324
train gradient:  0.1454934137457733
iteration : 2252
train acc:  0.7578125
train loss:  0.5122826099395752
train gradient:  0.17247699950251366
iteration : 2253
train acc:  0.7265625
train loss:  0.5283056497573853
train gradient:  0.18581293037416002
iteration : 2254
train acc:  0.625
train loss:  0.6249983906745911
train gradient:  0.19800183484231781
iteration : 2255
train acc:  0.78125
train loss:  0.48692643642425537
train gradient:  0.13059968129859084
iteration : 2256
train acc:  0.7734375
train loss:  0.4869614243507385
train gradient:  0.15559664051745933
iteration : 2257
train acc:  0.703125
train loss:  0.5502923727035522
train gradient:  0.15432500428491755
iteration : 2258
train acc:  0.7265625
train loss:  0.50138258934021
train gradient:  0.19915590231651248
iteration : 2259
train acc:  0.6484375
train loss:  0.6077320575714111
train gradient:  0.211059908246835
iteration : 2260
train acc:  0.7421875
train loss:  0.5342817306518555
train gradient:  0.2073912074784171
iteration : 2261
train acc:  0.6484375
train loss:  0.5799376964569092
train gradient:  0.21471679492640028
iteration : 2262
train acc:  0.7421875
train loss:  0.49554386734962463
train gradient:  0.14667461646541557
iteration : 2263
train acc:  0.7265625
train loss:  0.5164704918861389
train gradient:  0.16177544130155236
iteration : 2264
train acc:  0.71875
train loss:  0.5381156206130981
train gradient:  0.18004099876720492
iteration : 2265
train acc:  0.6171875
train loss:  0.6104243993759155
train gradient:  0.21579929316015717
iteration : 2266
train acc:  0.703125
train loss:  0.5260894298553467
train gradient:  0.14569392712259385
iteration : 2267
train acc:  0.6875
train loss:  0.5745517015457153
train gradient:  0.15318858368824734
iteration : 2268
train acc:  0.7265625
train loss:  0.5358014702796936
train gradient:  0.1864651849436425
iteration : 2269
train acc:  0.6171875
train loss:  0.6428549885749817
train gradient:  0.18706622570030979
iteration : 2270
train acc:  0.6484375
train loss:  0.5291723608970642
train gradient:  0.17153448139546107
iteration : 2271
train acc:  0.671875
train loss:  0.5192232131958008
train gradient:  0.15400130612971397
iteration : 2272
train acc:  0.78125
train loss:  0.5064030289649963
train gradient:  0.18087348363806727
iteration : 2273
train acc:  0.640625
train loss:  0.5957403182983398
train gradient:  0.19627742361392952
iteration : 2274
train acc:  0.75
train loss:  0.518251359462738
train gradient:  0.17270391151457337
iteration : 2275
train acc:  0.7265625
train loss:  0.5089268684387207
train gradient:  0.2440678581223371
iteration : 2276
train acc:  0.7421875
train loss:  0.5377954244613647
train gradient:  0.20349608962787405
iteration : 2277
train acc:  0.7265625
train loss:  0.5493382215499878
train gradient:  0.1971795620739495
iteration : 2278
train acc:  0.6953125
train loss:  0.6146014332771301
train gradient:  0.32250092506790057
iteration : 2279
train acc:  0.6484375
train loss:  0.55411696434021
train gradient:  0.18012504866458273
iteration : 2280
train acc:  0.6640625
train loss:  0.5831458568572998
train gradient:  0.1572587618325263
iteration : 2281
train acc:  0.75
train loss:  0.48206672072410583
train gradient:  0.1250911584889615
iteration : 2282
train acc:  0.7578125
train loss:  0.5199276208877563
train gradient:  0.1313858852615823
iteration : 2283
train acc:  0.6875
train loss:  0.54121994972229
train gradient:  0.1843003597320395
iteration : 2284
train acc:  0.6796875
train loss:  0.5714750289916992
train gradient:  0.24655632406105502
iteration : 2285
train acc:  0.71875
train loss:  0.5720306634902954
train gradient:  0.20146057842441073
iteration : 2286
train acc:  0.671875
train loss:  0.6632375717163086
train gradient:  0.2509307235911401
iteration : 2287
train acc:  0.75
train loss:  0.5161879062652588
train gradient:  0.11597886199169978
iteration : 2288
train acc:  0.7890625
train loss:  0.462093710899353
train gradient:  0.13471031873977918
iteration : 2289
train acc:  0.6875
train loss:  0.5771398544311523
train gradient:  0.17150325453991055
iteration : 2290
train acc:  0.65625
train loss:  0.5904941558837891
train gradient:  0.1559030609948595
iteration : 2291
train acc:  0.734375
train loss:  0.5404485464096069
train gradient:  0.20778193432092545
iteration : 2292
train acc:  0.7421875
train loss:  0.5106250047683716
train gradient:  0.23517966487941874
iteration : 2293
train acc:  0.7578125
train loss:  0.49415844678878784
train gradient:  0.13718326363491357
iteration : 2294
train acc:  0.734375
train loss:  0.4968002438545227
train gradient:  0.12749141803758846
iteration : 2295
train acc:  0.7265625
train loss:  0.5293325185775757
train gradient:  0.1783407427689354
iteration : 2296
train acc:  0.6796875
train loss:  0.5500608086585999
train gradient:  0.15279119200940988
iteration : 2297
train acc:  0.6796875
train loss:  0.5316181182861328
train gradient:  0.16456538410191043
iteration : 2298
train acc:  0.6875
train loss:  0.533490777015686
train gradient:  0.18719690656578142
iteration : 2299
train acc:  0.71875
train loss:  0.5339065790176392
train gradient:  0.23386021483935132
iteration : 2300
train acc:  0.734375
train loss:  0.5517504215240479
train gradient:  0.1654136886990517
iteration : 2301
train acc:  0.6484375
train loss:  0.5863223075866699
train gradient:  0.1594136041717993
iteration : 2302
train acc:  0.765625
train loss:  0.5013532042503357
train gradient:  0.1188136666743411
iteration : 2303
train acc:  0.703125
train loss:  0.5559500455856323
train gradient:  0.17711410228506996
iteration : 2304
train acc:  0.671875
train loss:  0.552249550819397
train gradient:  0.17299281626277985
iteration : 2305
train acc:  0.7109375
train loss:  0.54445880651474
train gradient:  0.14718279914871962
iteration : 2306
train acc:  0.703125
train loss:  0.5282390713691711
train gradient:  0.1614880334376963
iteration : 2307
train acc:  0.625
train loss:  0.6375428438186646
train gradient:  0.2202451594962953
iteration : 2308
train acc:  0.65625
train loss:  0.5870776176452637
train gradient:  0.14761192626829478
iteration : 2309
train acc:  0.7265625
train loss:  0.5528976321220398
train gradient:  0.15718378208081027
iteration : 2310
train acc:  0.7734375
train loss:  0.4939882755279541
train gradient:  0.1548267811548451
iteration : 2311
train acc:  0.765625
train loss:  0.46566712856292725
train gradient:  0.1558385567811581
iteration : 2312
train acc:  0.7109375
train loss:  0.5377814173698425
train gradient:  0.20125290365336462
iteration : 2313
train acc:  0.71875
train loss:  0.5108527541160583
train gradient:  0.15468878831628938
iteration : 2314
train acc:  0.703125
train loss:  0.6051596403121948
train gradient:  0.22996968741141077
iteration : 2315
train acc:  0.7265625
train loss:  0.5672335028648376
train gradient:  0.2301732521317483
iteration : 2316
train acc:  0.703125
train loss:  0.5815901160240173
train gradient:  0.1686041376393725
iteration : 2317
train acc:  0.6640625
train loss:  0.5740349888801575
train gradient:  0.2756702900098236
iteration : 2318
train acc:  0.703125
train loss:  0.5361130237579346
train gradient:  0.13971530634829366
iteration : 2319
train acc:  0.703125
train loss:  0.5519955158233643
train gradient:  0.20153514173259596
iteration : 2320
train acc:  0.7109375
train loss:  0.5390732288360596
train gradient:  0.14050757799270783
iteration : 2321
train acc:  0.671875
train loss:  0.6323981285095215
train gradient:  0.22308268113106317
iteration : 2322
train acc:  0.7265625
train loss:  0.4996744692325592
train gradient:  0.12650685451765492
iteration : 2323
train acc:  0.6875
train loss:  0.5561906099319458
train gradient:  0.19999775895511618
iteration : 2324
train acc:  0.6796875
train loss:  0.5775538086891174
train gradient:  0.1936087789383394
iteration : 2325
train acc:  0.65625
train loss:  0.6134017705917358
train gradient:  0.20420456818297034
iteration : 2326
train acc:  0.71875
train loss:  0.5240528583526611
train gradient:  0.17215865298962074
iteration : 2327
train acc:  0.75
train loss:  0.49506843090057373
train gradient:  0.13322184276559146
iteration : 2328
train acc:  0.7734375
train loss:  0.5027075409889221
train gradient:  0.14087332004165531
iteration : 2329
train acc:  0.6875
train loss:  0.5474185347557068
train gradient:  0.17791687390998612
iteration : 2330
train acc:  0.734375
train loss:  0.5132017135620117
train gradient:  0.12957815778564374
iteration : 2331
train acc:  0.6484375
train loss:  0.5805531740188599
train gradient:  0.20750049135813198
iteration : 2332
train acc:  0.71875
train loss:  0.5079103708267212
train gradient:  0.12088163507279998
iteration : 2333
train acc:  0.6640625
train loss:  0.589799165725708
train gradient:  0.1671565988994933
iteration : 2334
train acc:  0.6953125
train loss:  0.5475575923919678
train gradient:  0.1767195613179267
iteration : 2335
train acc:  0.703125
train loss:  0.5373539924621582
train gradient:  0.1400524485721646
iteration : 2336
train acc:  0.6328125
train loss:  0.5736402869224548
train gradient:  0.17299441247017708
iteration : 2337
train acc:  0.6875
train loss:  0.5244777202606201
train gradient:  0.13934413721581396
iteration : 2338
train acc:  0.734375
train loss:  0.5462982654571533
train gradient:  0.22163182199558476
iteration : 2339
train acc:  0.6640625
train loss:  0.5743341445922852
train gradient:  0.1589430672140998
iteration : 2340
train acc:  0.640625
train loss:  0.6732339859008789
train gradient:  0.3369837368350025
iteration : 2341
train acc:  0.7109375
train loss:  0.5084890723228455
train gradient:  0.14002103021371887
iteration : 2342
train acc:  0.6796875
train loss:  0.5948348641395569
train gradient:  0.18565749021056566
iteration : 2343
train acc:  0.796875
train loss:  0.4521915018558502
train gradient:  0.1306894302956037
iteration : 2344
train acc:  0.7265625
train loss:  0.5697650909423828
train gradient:  0.14967417021089374
iteration : 2345
train acc:  0.7890625
train loss:  0.4451794922351837
train gradient:  0.15041333233479165
iteration : 2346
train acc:  0.6953125
train loss:  0.5567346811294556
train gradient:  0.16422477296647267
iteration : 2347
train acc:  0.7109375
train loss:  0.5288088917732239
train gradient:  0.13314020887461264
iteration : 2348
train acc:  0.6953125
train loss:  0.5506166219711304
train gradient:  0.1588071331365769
iteration : 2349
train acc:  0.6953125
train loss:  0.5557118058204651
train gradient:  0.16988168718757624
iteration : 2350
train acc:  0.6875
train loss:  0.5417816638946533
train gradient:  0.1455070029344818
iteration : 2351
train acc:  0.7890625
train loss:  0.48386698961257935
train gradient:  0.12414330281927637
iteration : 2352
train acc:  0.6875
train loss:  0.533530056476593
train gradient:  0.1564832863547152
iteration : 2353
train acc:  0.734375
train loss:  0.5670799016952515
train gradient:  0.202188414115015
iteration : 2354
train acc:  0.625
train loss:  0.6350377202033997
train gradient:  0.19268560358606995
iteration : 2355
train acc:  0.7109375
train loss:  0.5455838441848755
train gradient:  0.16145111311499227
iteration : 2356
train acc:  0.7109375
train loss:  0.5651324987411499
train gradient:  0.13155089959298388
iteration : 2357
train acc:  0.7734375
train loss:  0.483678936958313
train gradient:  0.1399340595052117
iteration : 2358
train acc:  0.75
train loss:  0.4983195662498474
train gradient:  0.13292064991075714
iteration : 2359
train acc:  0.7109375
train loss:  0.5822046995162964
train gradient:  0.22422168240556892
iteration : 2360
train acc:  0.734375
train loss:  0.4835651218891144
train gradient:  0.11459179652229295
iteration : 2361
train acc:  0.796875
train loss:  0.4461296796798706
train gradient:  0.10574435934876529
iteration : 2362
train acc:  0.7578125
train loss:  0.49542465806007385
train gradient:  0.1256775906727872
iteration : 2363
train acc:  0.6484375
train loss:  0.6061835885047913
train gradient:  0.18319275930046613
iteration : 2364
train acc:  0.734375
train loss:  0.5250735282897949
train gradient:  0.14740667145658054
iteration : 2365
train acc:  0.6875
train loss:  0.5330852270126343
train gradient:  0.14202114432475293
iteration : 2366
train acc:  0.6796875
train loss:  0.5448870062828064
train gradient:  0.1860773025455624
iteration : 2367
train acc:  0.7109375
train loss:  0.5696136951446533
train gradient:  0.17477921807994912
iteration : 2368
train acc:  0.671875
train loss:  0.6525347828865051
train gradient:  0.24466361664744768
iteration : 2369
train acc:  0.7421875
train loss:  0.5361849069595337
train gradient:  0.12599393946769138
iteration : 2370
train acc:  0.7109375
train loss:  0.555828332901001
train gradient:  0.1647621941596349
iteration : 2371
train acc:  0.6640625
train loss:  0.6099509000778198
train gradient:  0.15298993084594598
iteration : 2372
train acc:  0.6875
train loss:  0.5945176482200623
train gradient:  0.179856531854254
iteration : 2373
train acc:  0.765625
train loss:  0.4572353959083557
train gradient:  0.12646367933871325
iteration : 2374
train acc:  0.7421875
train loss:  0.5413922071456909
train gradient:  0.1480398296348861
iteration : 2375
train acc:  0.703125
train loss:  0.5419691801071167
train gradient:  0.1467146582809375
iteration : 2376
train acc:  0.765625
train loss:  0.5065284967422485
train gradient:  0.13711769224128662
iteration : 2377
train acc:  0.7265625
train loss:  0.531921923160553
train gradient:  0.18814540599371848
iteration : 2378
train acc:  0.6875
train loss:  0.55265873670578
train gradient:  0.19067044054355686
iteration : 2379
train acc:  0.7109375
train loss:  0.5749620795249939
train gradient:  0.1879829101688169
iteration : 2380
train acc:  0.78125
train loss:  0.4739105701446533
train gradient:  0.13624777723593873
iteration : 2381
train acc:  0.640625
train loss:  0.6200790405273438
train gradient:  0.21320294125051464
iteration : 2382
train acc:  0.703125
train loss:  0.5406280159950256
train gradient:  0.12467434900595555
iteration : 2383
train acc:  0.6484375
train loss:  0.6180659532546997
train gradient:  0.26982779693522363
iteration : 2384
train acc:  0.7578125
train loss:  0.479573518037796
train gradient:  0.1342765277642688
iteration : 2385
train acc:  0.6875
train loss:  0.5622149705886841
train gradient:  0.14798466785120262
iteration : 2386
train acc:  0.6875
train loss:  0.5263242721557617
train gradient:  0.1636599587946912
iteration : 2387
train acc:  0.6953125
train loss:  0.5812233090400696
train gradient:  0.15780997320064905
iteration : 2388
train acc:  0.75
train loss:  0.5348314046859741
train gradient:  0.15984634404623882
iteration : 2389
train acc:  0.7890625
train loss:  0.47850358486175537
train gradient:  0.1444817204312845
iteration : 2390
train acc:  0.71875
train loss:  0.5143371224403381
train gradient:  0.12169787429466375
iteration : 2391
train acc:  0.7109375
train loss:  0.5110669732093811
train gradient:  0.131083997085646
iteration : 2392
train acc:  0.7109375
train loss:  0.5414219498634338
train gradient:  0.18923525925927084
iteration : 2393
train acc:  0.7109375
train loss:  0.5381547808647156
train gradient:  0.20274754104239562
iteration : 2394
train acc:  0.6953125
train loss:  0.5504312515258789
train gradient:  0.14654434238062924
iteration : 2395
train acc:  0.640625
train loss:  0.552627444267273
train gradient:  0.20902267150896883
iteration : 2396
train acc:  0.734375
train loss:  0.5328118801116943
train gradient:  0.14704967006708808
iteration : 2397
train acc:  0.6640625
train loss:  0.6030157804489136
train gradient:  0.17779852352619885
iteration : 2398
train acc:  0.7578125
train loss:  0.5293271541595459
train gradient:  0.12063376176617067
iteration : 2399
train acc:  0.6640625
train loss:  0.5945736169815063
train gradient:  0.21391770847911057
iteration : 2400
train acc:  0.765625
train loss:  0.5047643184661865
train gradient:  0.15215015517233055
iteration : 2401
train acc:  0.703125
train loss:  0.5493295192718506
train gradient:  0.17360359126282904
iteration : 2402
train acc:  0.6953125
train loss:  0.5807422399520874
train gradient:  0.15058371423048522
iteration : 2403
train acc:  0.765625
train loss:  0.4953344464302063
train gradient:  0.14112355740024038
iteration : 2404
train acc:  0.7265625
train loss:  0.5719950199127197
train gradient:  0.18704824809570703
iteration : 2405
train acc:  0.7421875
train loss:  0.5199868679046631
train gradient:  0.15603040249866224
iteration : 2406
train acc:  0.7421875
train loss:  0.5527342557907104
train gradient:  0.19757924027033147
iteration : 2407
train acc:  0.7109375
train loss:  0.5325222015380859
train gradient:  0.11308003778458652
iteration : 2408
train acc:  0.8203125
train loss:  0.47023725509643555
train gradient:  0.1499216855729894
iteration : 2409
train acc:  0.7421875
train loss:  0.4561823904514313
train gradient:  0.13074961113951317
iteration : 2410
train acc:  0.71875
train loss:  0.5258193612098694
train gradient:  0.1427320302748017
iteration : 2411
train acc:  0.734375
train loss:  0.5429096817970276
train gradient:  0.13872253605139356
iteration : 2412
train acc:  0.703125
train loss:  0.552005410194397
train gradient:  0.20044589037896043
iteration : 2413
train acc:  0.7421875
train loss:  0.4839470088481903
train gradient:  0.1252547066980538
iteration : 2414
train acc:  0.765625
train loss:  0.501907467842102
train gradient:  0.1541990935314416
iteration : 2415
train acc:  0.7578125
train loss:  0.46700847148895264
train gradient:  0.1460184633386273
iteration : 2416
train acc:  0.7578125
train loss:  0.5323643684387207
train gradient:  0.17112885732638
iteration : 2417
train acc:  0.7265625
train loss:  0.5625895261764526
train gradient:  0.15501996249853472
iteration : 2418
train acc:  0.734375
train loss:  0.5075856447219849
train gradient:  0.15063408918013227
iteration : 2419
train acc:  0.734375
train loss:  0.492905855178833
train gradient:  0.14663664530681275
iteration : 2420
train acc:  0.7109375
train loss:  0.5097382068634033
train gradient:  0.1624075797748541
iteration : 2421
train acc:  0.71875
train loss:  0.49174684286117554
train gradient:  0.1558452523293334
iteration : 2422
train acc:  0.703125
train loss:  0.5880019068717957
train gradient:  0.1873500746678715
iteration : 2423
train acc:  0.6953125
train loss:  0.5544610023498535
train gradient:  0.17971698799729913
iteration : 2424
train acc:  0.6484375
train loss:  0.5948636531829834
train gradient:  0.18517728154541765
iteration : 2425
train acc:  0.6953125
train loss:  0.5430532097816467
train gradient:  0.22024448511854566
iteration : 2426
train acc:  0.65625
train loss:  0.5553897619247437
train gradient:  0.23299646473230823
iteration : 2427
train acc:  0.765625
train loss:  0.5038124918937683
train gradient:  0.20643203295770363
iteration : 2428
train acc:  0.7265625
train loss:  0.5105742812156677
train gradient:  0.16214857837910435
iteration : 2429
train acc:  0.765625
train loss:  0.5246225595474243
train gradient:  0.150700164750322
iteration : 2430
train acc:  0.75
train loss:  0.5072846412658691
train gradient:  0.1797746718490092
iteration : 2431
train acc:  0.765625
train loss:  0.5162843465805054
train gradient:  0.1987690269881066
iteration : 2432
train acc:  0.703125
train loss:  0.538314938545227
train gradient:  0.16797836356640952
iteration : 2433
train acc:  0.6953125
train loss:  0.5093083381652832
train gradient:  0.14125470720609445
iteration : 2434
train acc:  0.734375
train loss:  0.5211787223815918
train gradient:  0.17139882411424612
iteration : 2435
train acc:  0.6953125
train loss:  0.5347570180892944
train gradient:  0.162418320452913
iteration : 2436
train acc:  0.7265625
train loss:  0.5734138488769531
train gradient:  0.1762154184124624
iteration : 2437
train acc:  0.6640625
train loss:  0.6230924725532532
train gradient:  0.2707321812039247
iteration : 2438
train acc:  0.734375
train loss:  0.5346983671188354
train gradient:  0.16424076482860403
iteration : 2439
train acc:  0.75
train loss:  0.5446850657463074
train gradient:  0.2305904628369914
iteration : 2440
train acc:  0.640625
train loss:  0.6060078144073486
train gradient:  0.18875887231743826
iteration : 2441
train acc:  0.6796875
train loss:  0.5460157990455627
train gradient:  0.15286541056298353
iteration : 2442
train acc:  0.75
train loss:  0.4889983534812927
train gradient:  0.12345371487081275
iteration : 2443
train acc:  0.7265625
train loss:  0.5373234748840332
train gradient:  0.13738135425742456
iteration : 2444
train acc:  0.71875
train loss:  0.5580214262008667
train gradient:  0.18783570905441838
iteration : 2445
train acc:  0.71875
train loss:  0.6078085899353027
train gradient:  0.16039743390818384
iteration : 2446
train acc:  0.75
train loss:  0.5126542448997498
train gradient:  0.157695014125654
iteration : 2447
train acc:  0.765625
train loss:  0.48449307680130005
train gradient:  0.1145446217809874
iteration : 2448
train acc:  0.7265625
train loss:  0.5257744789123535
train gradient:  0.13191734923490642
iteration : 2449
train acc:  0.75
train loss:  0.5240108966827393
train gradient:  0.2181493439338924
iteration : 2450
train acc:  0.6953125
train loss:  0.5305682420730591
train gradient:  0.18664286124738227
iteration : 2451
train acc:  0.703125
train loss:  0.5590965151786804
train gradient:  0.1538457771140982
iteration : 2452
train acc:  0.7734375
train loss:  0.45813244581222534
train gradient:  0.14016406722608993
iteration : 2453
train acc:  0.71875
train loss:  0.5415674448013306
train gradient:  0.18054071815953487
iteration : 2454
train acc:  0.7421875
train loss:  0.4807729125022888
train gradient:  0.14081172482911675
iteration : 2455
train acc:  0.703125
train loss:  0.5213980078697205
train gradient:  0.15611441166298798
iteration : 2456
train acc:  0.703125
train loss:  0.5509124994277954
train gradient:  0.14384015290960772
iteration : 2457
train acc:  0.7578125
train loss:  0.5254037380218506
train gradient:  0.11250712441907328
iteration : 2458
train acc:  0.671875
train loss:  0.568304717540741
train gradient:  0.16309207362405498
iteration : 2459
train acc:  0.6796875
train loss:  0.5464699864387512
train gradient:  0.16012791723319286
iteration : 2460
train acc:  0.671875
train loss:  0.5832136869430542
train gradient:  0.2619468521461479
iteration : 2461
train acc:  0.7421875
train loss:  0.5015711784362793
train gradient:  0.10628580939934773
iteration : 2462
train acc:  0.7421875
train loss:  0.4847540259361267
train gradient:  0.18694188300729073
iteration : 2463
train acc:  0.765625
train loss:  0.4804162383079529
train gradient:  0.19231285399739284
iteration : 2464
train acc:  0.671875
train loss:  0.5621016025543213
train gradient:  0.17367819019131364
iteration : 2465
train acc:  0.6171875
train loss:  0.6009687185287476
train gradient:  0.18509594170733584
iteration : 2466
train acc:  0.7421875
train loss:  0.5276665091514587
train gradient:  0.18874836684212748
iteration : 2467
train acc:  0.796875
train loss:  0.5254457592964172
train gradient:  0.2930892808128598
iteration : 2468
train acc:  0.625
train loss:  0.5736894607543945
train gradient:  0.14882998671468195
iteration : 2469
train acc:  0.734375
train loss:  0.5595234632492065
train gradient:  0.14948118334513893
iteration : 2470
train acc:  0.703125
train loss:  0.564594030380249
train gradient:  0.17464734712709415
iteration : 2471
train acc:  0.71875
train loss:  0.5208184719085693
train gradient:  0.15544552100629644
iteration : 2472
train acc:  0.703125
train loss:  0.49436795711517334
train gradient:  0.15432253346738195
iteration : 2473
train acc:  0.6953125
train loss:  0.5627027750015259
train gradient:  0.262651077342277
iteration : 2474
train acc:  0.703125
train loss:  0.5555140376091003
train gradient:  0.15018875229512804
iteration : 2475
train acc:  0.7265625
train loss:  0.5333722829818726
train gradient:  0.143407998322301
iteration : 2476
train acc:  0.703125
train loss:  0.5524297952651978
train gradient:  0.191713787431641
iteration : 2477
train acc:  0.7734375
train loss:  0.5090070962905884
train gradient:  0.1786325577678788
iteration : 2478
train acc:  0.765625
train loss:  0.5288544297218323
train gradient:  0.17922500535940417
iteration : 2479
train acc:  0.734375
train loss:  0.528870165348053
train gradient:  0.22347625630161666
iteration : 2480
train acc:  0.6953125
train loss:  0.5866486430168152
train gradient:  0.19363352464228678
iteration : 2481
train acc:  0.71875
train loss:  0.4923040568828583
train gradient:  0.14743370385556764
iteration : 2482
train acc:  0.7109375
train loss:  0.4940478205680847
train gradient:  0.1812988639852539
iteration : 2483
train acc:  0.6328125
train loss:  0.6103394031524658
train gradient:  0.21246962595554825
iteration : 2484
train acc:  0.71875
train loss:  0.5515018701553345
train gradient:  0.18863279794704152
iteration : 2485
train acc:  0.7109375
train loss:  0.5415946245193481
train gradient:  0.15290619662841262
iteration : 2486
train acc:  0.6171875
train loss:  0.5910701751708984
train gradient:  0.2144156708824574
iteration : 2487
train acc:  0.7109375
train loss:  0.5288398265838623
train gradient:  0.1579039136826667
iteration : 2488
train acc:  0.65625
train loss:  0.5597366094589233
train gradient:  0.22537684994747798
iteration : 2489
train acc:  0.734375
train loss:  0.5468189716339111
train gradient:  0.2076490736025431
iteration : 2490
train acc:  0.7109375
train loss:  0.5733098983764648
train gradient:  0.15660509742690426
iteration : 2491
train acc:  0.6640625
train loss:  0.5704032182693481
train gradient:  0.18611375292336407
iteration : 2492
train acc:  0.7109375
train loss:  0.5799760222434998
train gradient:  0.19142220880552258
iteration : 2493
train acc:  0.765625
train loss:  0.5230019688606262
train gradient:  0.2155122454518646
iteration : 2494
train acc:  0.7421875
train loss:  0.5068421363830566
train gradient:  0.17367572189572356
iteration : 2495
train acc:  0.703125
train loss:  0.546330451965332
train gradient:  0.18379010331670603
iteration : 2496
train acc:  0.7109375
train loss:  0.5242847204208374
train gradient:  0.18096323810653012
iteration : 2497
train acc:  0.7265625
train loss:  0.5783346891403198
train gradient:  0.15773918017832597
iteration : 2498
train acc:  0.6953125
train loss:  0.5511881113052368
train gradient:  0.20186603451553492
iteration : 2499
train acc:  0.671875
train loss:  0.5665677785873413
train gradient:  0.2161881828289855
iteration : 2500
train acc:  0.6953125
train loss:  0.5566891431808472
train gradient:  0.1604041564047219
iteration : 2501
train acc:  0.734375
train loss:  0.5173189640045166
train gradient:  0.16067234290361038
iteration : 2502
train acc:  0.71875
train loss:  0.5149011015892029
train gradient:  0.14425455616528704
iteration : 2503
train acc:  0.6640625
train loss:  0.6146912574768066
train gradient:  0.2461826445829833
iteration : 2504
train acc:  0.75
train loss:  0.5001938343048096
train gradient:  0.1565446774943728
iteration : 2505
train acc:  0.75
train loss:  0.5321506261825562
train gradient:  0.15993469172120361
iteration : 2506
train acc:  0.7109375
train loss:  0.5560019016265869
train gradient:  0.2230891359031456
iteration : 2507
train acc:  0.71875
train loss:  0.5866552591323853
train gradient:  0.18634060651677486
iteration : 2508
train acc:  0.75
train loss:  0.5031978487968445
train gradient:  0.15066642814341158
iteration : 2509
train acc:  0.6796875
train loss:  0.5493286848068237
train gradient:  0.22853437778869168
iteration : 2510
train acc:  0.6640625
train loss:  0.5661129951477051
train gradient:  0.16269688238394778
iteration : 2511
train acc:  0.7578125
train loss:  0.48241689801216125
train gradient:  0.1345514230618362
iteration : 2512
train acc:  0.7265625
train loss:  0.5213218927383423
train gradient:  0.1353394172995721
iteration : 2513
train acc:  0.7109375
train loss:  0.5702560544013977
train gradient:  0.14987413601340432
iteration : 2514
train acc:  0.7421875
train loss:  0.5177043080329895
train gradient:  0.1621466731784515
iteration : 2515
train acc:  0.7109375
train loss:  0.5270487070083618
train gradient:  0.23167312441698665
iteration : 2516
train acc:  0.6953125
train loss:  0.5315650701522827
train gradient:  0.15099958483529413
iteration : 2517
train acc:  0.75
train loss:  0.4898494482040405
train gradient:  0.14205176606447512
iteration : 2518
train acc:  0.7109375
train loss:  0.5443111062049866
train gradient:  0.2018228990412484
iteration : 2519
train acc:  0.765625
train loss:  0.49372774362564087
train gradient:  0.17591121248714608
iteration : 2520
train acc:  0.734375
train loss:  0.5473335385322571
train gradient:  0.16353371205129535
iteration : 2521
train acc:  0.65625
train loss:  0.5582471489906311
train gradient:  0.16354457003691425
iteration : 2522
train acc:  0.7578125
train loss:  0.4961127042770386
train gradient:  0.12144983234926715
iteration : 2523
train acc:  0.6953125
train loss:  0.5708962678909302
train gradient:  0.23168615329082753
iteration : 2524
train acc:  0.6875
train loss:  0.581988513469696
train gradient:  0.2393160125864423
iteration : 2525
train acc:  0.671875
train loss:  0.6030988693237305
train gradient:  0.13583104983612349
iteration : 2526
train acc:  0.7734375
train loss:  0.4939920902252197
train gradient:  0.12405070816678636
iteration : 2527
train acc:  0.7578125
train loss:  0.49870067834854126
train gradient:  0.11506088819048346
iteration : 2528
train acc:  0.7265625
train loss:  0.5236796736717224
train gradient:  0.1862886817391356
iteration : 2529
train acc:  0.7265625
train loss:  0.5346256494522095
train gradient:  0.1832573556051908
iteration : 2530
train acc:  0.6171875
train loss:  0.5847294926643372
train gradient:  0.19445621256430118
iteration : 2531
train acc:  0.75
train loss:  0.5300268530845642
train gradient:  0.1804589561560973
iteration : 2532
train acc:  0.6484375
train loss:  0.5697212815284729
train gradient:  0.1370211640749397
iteration : 2533
train acc:  0.703125
train loss:  0.5414177775382996
train gradient:  0.16356274210076027
iteration : 2534
train acc:  0.71875
train loss:  0.5190819501876831
train gradient:  0.191722046986537
iteration : 2535
train acc:  0.734375
train loss:  0.5252368450164795
train gradient:  0.21201395888735763
iteration : 2536
train acc:  0.6484375
train loss:  0.6144615411758423
train gradient:  0.2666824238252673
iteration : 2537
train acc:  0.640625
train loss:  0.6013279557228088
train gradient:  0.1869717008606427
iteration : 2538
train acc:  0.71875
train loss:  0.5369188189506531
train gradient:  0.16643811421411503
iteration : 2539
train acc:  0.71875
train loss:  0.5642246007919312
train gradient:  0.15309629905928876
iteration : 2540
train acc:  0.7421875
train loss:  0.5030725002288818
train gradient:  0.23641900316342124
iteration : 2541
train acc:  0.703125
train loss:  0.5396954417228699
train gradient:  0.169376778379602
iteration : 2542
train acc:  0.671875
train loss:  0.5635840892791748
train gradient:  0.18690529987449062
iteration : 2543
train acc:  0.765625
train loss:  0.5040139555931091
train gradient:  0.15953837486198108
iteration : 2544
train acc:  0.7265625
train loss:  0.5477578043937683
train gradient:  0.19037534279822638
iteration : 2545
train acc:  0.7578125
train loss:  0.5351424813270569
train gradient:  0.14208963723273388
iteration : 2546
train acc:  0.65625
train loss:  0.57525634765625
train gradient:  0.19443345908069215
iteration : 2547
train acc:  0.671875
train loss:  0.5842975378036499
train gradient:  0.25561960357033114
iteration : 2548
train acc:  0.7265625
train loss:  0.5665056109428406
train gradient:  0.22459349136877477
iteration : 2549
train acc:  0.75
train loss:  0.5301305055618286
train gradient:  0.13733437380034796
iteration : 2550
train acc:  0.7421875
train loss:  0.4965718388557434
train gradient:  0.12124089433075652
iteration : 2551
train acc:  0.7734375
train loss:  0.5252569913864136
train gradient:  0.17214884935054436
iteration : 2552
train acc:  0.703125
train loss:  0.5559390783309937
train gradient:  0.1845807147535846
iteration : 2553
train acc:  0.7421875
train loss:  0.5862979888916016
train gradient:  0.17273643660128357
iteration : 2554
train acc:  0.7265625
train loss:  0.4808611273765564
train gradient:  0.10768522446619345
iteration : 2555
train acc:  0.703125
train loss:  0.5639370083808899
train gradient:  0.15962855906378887
iteration : 2556
train acc:  0.71875
train loss:  0.5067987442016602
train gradient:  0.1666592224310905
iteration : 2557
train acc:  0.6328125
train loss:  0.5991750955581665
train gradient:  0.1494959756892302
iteration : 2558
train acc:  0.7265625
train loss:  0.5219061374664307
train gradient:  0.13875159276127824
iteration : 2559
train acc:  0.765625
train loss:  0.4886817932128906
train gradient:  0.16704905009278526
iteration : 2560
train acc:  0.71875
train loss:  0.5281460881233215
train gradient:  0.18710942407052436
iteration : 2561
train acc:  0.71875
train loss:  0.5733466148376465
train gradient:  0.18581881555273178
iteration : 2562
train acc:  0.6875
train loss:  0.5164642930030823
train gradient:  0.14812436696940062
iteration : 2563
train acc:  0.7109375
train loss:  0.5335553884506226
train gradient:  0.13629871751340217
iteration : 2564
train acc:  0.625
train loss:  0.6383680701255798
train gradient:  0.23262150694800982
iteration : 2565
train acc:  0.6875
train loss:  0.5798918008804321
train gradient:  0.24855428268275875
iteration : 2566
train acc:  0.640625
train loss:  0.6164356470108032
train gradient:  0.18736369267751568
iteration : 2567
train acc:  0.6953125
train loss:  0.5651775002479553
train gradient:  0.15757876333043916
iteration : 2568
train acc:  0.75
train loss:  0.5354395508766174
train gradient:  0.13769703258900334
iteration : 2569
train acc:  0.734375
train loss:  0.4768792390823364
train gradient:  0.15472690359645488
iteration : 2570
train acc:  0.671875
train loss:  0.5314886569976807
train gradient:  0.1587628269238957
iteration : 2571
train acc:  0.78125
train loss:  0.45353928208351135
train gradient:  0.12638994678833498
iteration : 2572
train acc:  0.703125
train loss:  0.5346246361732483
train gradient:  0.1800589222263544
iteration : 2573
train acc:  0.734375
train loss:  0.5423734188079834
train gradient:  0.13010518736686105
iteration : 2574
train acc:  0.6953125
train loss:  0.5526735782623291
train gradient:  0.1925428703300059
iteration : 2575
train acc:  0.71875
train loss:  0.5339638590812683
train gradient:  0.15914387382755293
iteration : 2576
train acc:  0.671875
train loss:  0.5468817949295044
train gradient:  0.18594930402574278
iteration : 2577
train acc:  0.7734375
train loss:  0.506604015827179
train gradient:  0.19532518207646893
iteration : 2578
train acc:  0.6875
train loss:  0.5631498098373413
train gradient:  0.17024747105326377
iteration : 2579
train acc:  0.6484375
train loss:  0.5971875786781311
train gradient:  0.16665214133892597
iteration : 2580
train acc:  0.703125
train loss:  0.5402554273605347
train gradient:  0.17053281751704732
iteration : 2581
train acc:  0.7109375
train loss:  0.5140719413757324
train gradient:  0.2226419321692855
iteration : 2582
train acc:  0.6796875
train loss:  0.6125667691230774
train gradient:  0.17777647643611588
iteration : 2583
train acc:  0.6796875
train loss:  0.5608502626419067
train gradient:  0.161963178860549
iteration : 2584
train acc:  0.734375
train loss:  0.5310040712356567
train gradient:  0.15833808165854835
iteration : 2585
train acc:  0.640625
train loss:  0.6321847438812256
train gradient:  0.18893942673468406
iteration : 2586
train acc:  0.6953125
train loss:  0.5796072483062744
train gradient:  0.1441430596240088
iteration : 2587
train acc:  0.7109375
train loss:  0.5341531038284302
train gradient:  0.1449315949190842
iteration : 2588
train acc:  0.7109375
train loss:  0.5116715431213379
train gradient:  0.1358500877999679
iteration : 2589
train acc:  0.6640625
train loss:  0.5877658128738403
train gradient:  0.1997539584858874
iteration : 2590
train acc:  0.765625
train loss:  0.45663487911224365
train gradient:  0.10068690502001303
iteration : 2591
train acc:  0.7265625
train loss:  0.5288834571838379
train gradient:  0.1814215838698513
iteration : 2592
train acc:  0.765625
train loss:  0.4931997060775757
train gradient:  0.1461786924869396
iteration : 2593
train acc:  0.6640625
train loss:  0.5865935683250427
train gradient:  0.17006939997346154
iteration : 2594
train acc:  0.6875
train loss:  0.595702588558197
train gradient:  0.17154401033460212
iteration : 2595
train acc:  0.6875
train loss:  0.575980544090271
train gradient:  0.17233977273520712
iteration : 2596
train acc:  0.7421875
train loss:  0.5364696383476257
train gradient:  0.1682139198051975
iteration : 2597
train acc:  0.71875
train loss:  0.5195485353469849
train gradient:  0.13424352056504912
iteration : 2598
train acc:  0.671875
train loss:  0.5607011318206787
train gradient:  0.1783079245384926
iteration : 2599
train acc:  0.703125
train loss:  0.5956804752349854
train gradient:  0.15830432682975926
iteration : 2600
train acc:  0.6796875
train loss:  0.5997730493545532
train gradient:  0.2204903244259844
iteration : 2601
train acc:  0.6875
train loss:  0.6005523204803467
train gradient:  0.1575930147214354
iteration : 2602
train acc:  0.7890625
train loss:  0.5095983147621155
train gradient:  0.1556888062099544
iteration : 2603
train acc:  0.8046875
train loss:  0.47753089666366577
train gradient:  0.17765995229000192
iteration : 2604
train acc:  0.734375
train loss:  0.5214338302612305
train gradient:  0.1234043540828541
iteration : 2605
train acc:  0.734375
train loss:  0.5306299328804016
train gradient:  0.15602784429007296
iteration : 2606
train acc:  0.7578125
train loss:  0.5161935687065125
train gradient:  0.15529034559853014
iteration : 2607
train acc:  0.75
train loss:  0.4695972800254822
train gradient:  0.12004528569859604
iteration : 2608
train acc:  0.765625
train loss:  0.4866335988044739
train gradient:  0.21152432698877105
iteration : 2609
train acc:  0.6796875
train loss:  0.571916401386261
train gradient:  0.1446639919238481
iteration : 2610
train acc:  0.6875
train loss:  0.5359851121902466
train gradient:  0.16469694420406483
iteration : 2611
train acc:  0.6953125
train loss:  0.5405967831611633
train gradient:  0.12041130775715111
iteration : 2612
train acc:  0.6875
train loss:  0.6195289492607117
train gradient:  0.19593592588103848
iteration : 2613
train acc:  0.71875
train loss:  0.5389487743377686
train gradient:  0.1816571444543039
iteration : 2614
train acc:  0.765625
train loss:  0.49204206466674805
train gradient:  0.14932344037542544
iteration : 2615
train acc:  0.75
train loss:  0.5299272537231445
train gradient:  0.1663443387008826
iteration : 2616
train acc:  0.6953125
train loss:  0.6060988903045654
train gradient:  0.184492627288967
iteration : 2617
train acc:  0.6953125
train loss:  0.6272225379943848
train gradient:  0.31235320652474347
iteration : 2618
train acc:  0.671875
train loss:  0.5649237632751465
train gradient:  0.172117910135373
iteration : 2619
train acc:  0.71875
train loss:  0.5502574443817139
train gradient:  0.1868870237995261
iteration : 2620
train acc:  0.7265625
train loss:  0.5840263962745667
train gradient:  0.15967762173018532
iteration : 2621
train acc:  0.7109375
train loss:  0.5428104996681213
train gradient:  0.14531154704001192
iteration : 2622
train acc:  0.7421875
train loss:  0.521887481212616
train gradient:  0.14070208165620185
iteration : 2623
train acc:  0.75
train loss:  0.5207787156105042
train gradient:  0.15668166190640304
iteration : 2624
train acc:  0.703125
train loss:  0.5349565744400024
train gradient:  0.15576069750312524
iteration : 2625
train acc:  0.7265625
train loss:  0.5655689239501953
train gradient:  0.2219102576917214
iteration : 2626
train acc:  0.6953125
train loss:  0.560479998588562
train gradient:  0.18812282716406076
iteration : 2627
train acc:  0.6484375
train loss:  0.5691482424736023
train gradient:  0.14490125051471
iteration : 2628
train acc:  0.6796875
train loss:  0.5475116968154907
train gradient:  0.1753470788459625
iteration : 2629
train acc:  0.75
train loss:  0.5525609254837036
train gradient:  0.1795813996759993
iteration : 2630
train acc:  0.6640625
train loss:  0.5969778895378113
train gradient:  0.23204635092053918
iteration : 2631
train acc:  0.609375
train loss:  0.6295332312583923
train gradient:  0.1601198606020796
iteration : 2632
train acc:  0.7734375
train loss:  0.5120025873184204
train gradient:  0.14359639670551988
iteration : 2633
train acc:  0.7421875
train loss:  0.49738937616348267
train gradient:  0.12666274518345935
iteration : 2634
train acc:  0.71875
train loss:  0.5308216214179993
train gradient:  0.14829376937557318
iteration : 2635
train acc:  0.7265625
train loss:  0.5682235956192017
train gradient:  0.1737609598264358
iteration : 2636
train acc:  0.703125
train loss:  0.5306986570358276
train gradient:  0.1439663649799571
iteration : 2637
train acc:  0.7109375
train loss:  0.5149292945861816
train gradient:  0.11560366591977578
iteration : 2638
train acc:  0.6953125
train loss:  0.5241341590881348
train gradient:  0.12446149250268637
iteration : 2639
train acc:  0.6875
train loss:  0.5705142617225647
train gradient:  0.1753613315709524
iteration : 2640
train acc:  0.703125
train loss:  0.5249764919281006
train gradient:  0.1291661047102099
iteration : 2641
train acc:  0.703125
train loss:  0.49905455112457275
train gradient:  0.11823802442808104
iteration : 2642
train acc:  0.6875
train loss:  0.5518913269042969
train gradient:  0.1717442351427215
iteration : 2643
train acc:  0.6875
train loss:  0.52493816614151
train gradient:  0.13157179397939686
iteration : 2644
train acc:  0.765625
train loss:  0.4874613881111145
train gradient:  0.1370990719422795
iteration : 2645
train acc:  0.78125
train loss:  0.5148223042488098
train gradient:  0.16103722338641702
iteration : 2646
train acc:  0.765625
train loss:  0.5070070028305054
train gradient:  0.22848393333301537
iteration : 2647
train acc:  0.703125
train loss:  0.5547338724136353
train gradient:  0.1463340122062257
iteration : 2648
train acc:  0.7578125
train loss:  0.5106199383735657
train gradient:  0.12736755029948593
iteration : 2649
train acc:  0.734375
train loss:  0.5088813900947571
train gradient:  0.15399179294081217
iteration : 2650
train acc:  0.671875
train loss:  0.591975212097168
train gradient:  0.2150431867485932
iteration : 2651
train acc:  0.734375
train loss:  0.5018776655197144
train gradient:  0.12388486580595084
iteration : 2652
train acc:  0.6953125
train loss:  0.5924263000488281
train gradient:  0.15680461164971213
iteration : 2653
train acc:  0.6484375
train loss:  0.5954771637916565
train gradient:  0.1889192632187674
iteration : 2654
train acc:  0.7265625
train loss:  0.535186767578125
train gradient:  0.14743376267278913
iteration : 2655
train acc:  0.734375
train loss:  0.5105544924736023
train gradient:  0.14797236402481978
iteration : 2656
train acc:  0.625
train loss:  0.5775631666183472
train gradient:  0.14149502360879718
iteration : 2657
train acc:  0.6953125
train loss:  0.5464459657669067
train gradient:  0.18193822747289273
iteration : 2658
train acc:  0.7109375
train loss:  0.5308234691619873
train gradient:  0.16070445276318052
iteration : 2659
train acc:  0.7421875
train loss:  0.48399603366851807
train gradient:  0.17017234980641288
iteration : 2660
train acc:  0.78125
train loss:  0.4762803912162781
train gradient:  0.13826201226601803
iteration : 2661
train acc:  0.6953125
train loss:  0.620037317276001
train gradient:  0.22061728765897293
iteration : 2662
train acc:  0.6796875
train loss:  0.5657663345336914
train gradient:  0.13740675366381122
iteration : 2663
train acc:  0.71875
train loss:  0.5466398596763611
train gradient:  0.20202541964587234
iteration : 2664
train acc:  0.703125
train loss:  0.5428711175918579
train gradient:  0.1466198469434134
iteration : 2665
train acc:  0.7421875
train loss:  0.4953797459602356
train gradient:  0.10956475192728142
iteration : 2666
train acc:  0.75
train loss:  0.5350103378295898
train gradient:  0.14275428474810087
iteration : 2667
train acc:  0.71875
train loss:  0.5376312732696533
train gradient:  0.16147970357428834
iteration : 2668
train acc:  0.6484375
train loss:  0.5936417579650879
train gradient:  0.19989871790379496
iteration : 2669
train acc:  0.6796875
train loss:  0.544896125793457
train gradient:  0.16397650112858292
iteration : 2670
train acc:  0.703125
train loss:  0.526183009147644
train gradient:  0.18709799441248698
iteration : 2671
train acc:  0.71875
train loss:  0.5309678316116333
train gradient:  0.22449419831126805
iteration : 2672
train acc:  0.671875
train loss:  0.600846529006958
train gradient:  0.16500012546630125
iteration : 2673
train acc:  0.671875
train loss:  0.5912796258926392
train gradient:  0.16020041958752002
iteration : 2674
train acc:  0.7109375
train loss:  0.5259803533554077
train gradient:  0.10676230397639497
iteration : 2675
train acc:  0.703125
train loss:  0.606023907661438
train gradient:  0.2153992905873044
iteration : 2676
train acc:  0.7109375
train loss:  0.5552688837051392
train gradient:  0.17406436500956518
iteration : 2677
train acc:  0.7578125
train loss:  0.49479007720947266
train gradient:  0.14547776747956032
iteration : 2678
train acc:  0.8046875
train loss:  0.45640313625335693
train gradient:  0.1633056468510401
iteration : 2679
train acc:  0.640625
train loss:  0.5770434737205505
train gradient:  0.16949116652282845
iteration : 2680
train acc:  0.75
train loss:  0.49980223178863525
train gradient:  0.1537039920310002
iteration : 2681
train acc:  0.71875
train loss:  0.5695163011550903
train gradient:  0.1360390219859831
iteration : 2682
train acc:  0.703125
train loss:  0.49442166090011597
train gradient:  0.17075479248842235
iteration : 2683
train acc:  0.71875
train loss:  0.5099355578422546
train gradient:  0.1484028595990695
iteration : 2684
train acc:  0.6328125
train loss:  0.6108685731887817
train gradient:  0.2422627818739826
iteration : 2685
train acc:  0.6640625
train loss:  0.5714346766471863
train gradient:  0.1638435937509488
iteration : 2686
train acc:  0.75
train loss:  0.5001096725463867
train gradient:  0.12554680379164312
iteration : 2687
train acc:  0.7421875
train loss:  0.5050358176231384
train gradient:  0.11814448607186954
iteration : 2688
train acc:  0.7109375
train loss:  0.5825166702270508
train gradient:  0.16851132183276485
iteration : 2689
train acc:  0.6875
train loss:  0.5633752942085266
train gradient:  0.17280723197601244
iteration : 2690
train acc:  0.6484375
train loss:  0.5480380058288574
train gradient:  0.23782961516825135
iteration : 2691
train acc:  0.6484375
train loss:  0.5768660306930542
train gradient:  0.14679570469738717
iteration : 2692
train acc:  0.6171875
train loss:  0.6137692928314209
train gradient:  0.2672218293770575
iteration : 2693
train acc:  0.7734375
train loss:  0.4579828381538391
train gradient:  0.0986641915715698
iteration : 2694
train acc:  0.734375
train loss:  0.4910697042942047
train gradient:  0.16078709220623544
iteration : 2695
train acc:  0.6640625
train loss:  0.5411863923072815
train gradient:  0.1423715382960622
iteration : 2696
train acc:  0.734375
train loss:  0.5270582437515259
train gradient:  0.16861636793210694
iteration : 2697
train acc:  0.71875
train loss:  0.579008936882019
train gradient:  0.17059128708202195
iteration : 2698
train acc:  0.7421875
train loss:  0.5426815152168274
train gradient:  0.21174330564735905
iteration : 2699
train acc:  0.671875
train loss:  0.5395766496658325
train gradient:  0.1537380286503533
iteration : 2700
train acc:  0.7734375
train loss:  0.5153063535690308
train gradient:  0.2147066215006153
iteration : 2701
train acc:  0.7421875
train loss:  0.5135044455528259
train gradient:  0.1157665388414492
iteration : 2702
train acc:  0.7265625
train loss:  0.5553853511810303
train gradient:  0.22372495202909462
iteration : 2703
train acc:  0.734375
train loss:  0.5327794551849365
train gradient:  0.1882721802109809
iteration : 2704
train acc:  0.6875
train loss:  0.5714972019195557
train gradient:  0.16945310022533183
iteration : 2705
train acc:  0.6875
train loss:  0.5129557847976685
train gradient:  0.1294164046218616
iteration : 2706
train acc:  0.6796875
train loss:  0.6044895648956299
train gradient:  0.2014913555714376
iteration : 2707
train acc:  0.703125
train loss:  0.54365473985672
train gradient:  0.17927451045544446
iteration : 2708
train acc:  0.703125
train loss:  0.5507549047470093
train gradient:  0.16359560206711588
iteration : 2709
train acc:  0.7265625
train loss:  0.5175457000732422
train gradient:  0.1595477818776359
iteration : 2710
train acc:  0.6875
train loss:  0.579985499382019
train gradient:  0.1569370572613707
iteration : 2711
train acc:  0.71875
train loss:  0.5101715326309204
train gradient:  0.14385852348630554
iteration : 2712
train acc:  0.7421875
train loss:  0.5338079929351807
train gradient:  0.22400843431184103
iteration : 2713
train acc:  0.65625
train loss:  0.6163084506988525
train gradient:  0.21105404362561675
iteration : 2714
train acc:  0.7578125
train loss:  0.5049077272415161
train gradient:  0.12430876028620327
iteration : 2715
train acc:  0.7421875
train loss:  0.5188022255897522
train gradient:  0.19044760963399748
iteration : 2716
train acc:  0.7109375
train loss:  0.5643094778060913
train gradient:  0.13797488227769644
iteration : 2717
train acc:  0.71875
train loss:  0.5403382778167725
train gradient:  0.1523441824308236
iteration : 2718
train acc:  0.7265625
train loss:  0.558889627456665
train gradient:  0.2767760006295258
iteration : 2719
train acc:  0.7265625
train loss:  0.5537465810775757
train gradient:  0.15501715974530178
iteration : 2720
train acc:  0.7109375
train loss:  0.5441533327102661
train gradient:  0.1442010906268436
iteration : 2721
train acc:  0.765625
train loss:  0.48579826951026917
train gradient:  0.12315604050841149
iteration : 2722
train acc:  0.703125
train loss:  0.5606579780578613
train gradient:  0.21223928864885624
iteration : 2723
train acc:  0.6796875
train loss:  0.5785308480262756
train gradient:  0.1596794992286174
iteration : 2724
train acc:  0.6875
train loss:  0.5697431564331055
train gradient:  0.16793871376916852
iteration : 2725
train acc:  0.6875
train loss:  0.5497028231620789
train gradient:  0.17529928087660868
iteration : 2726
train acc:  0.6640625
train loss:  0.5557056665420532
train gradient:  0.17032978199625748
iteration : 2727
train acc:  0.7265625
train loss:  0.5193135738372803
train gradient:  0.1313939694969536
iteration : 2728
train acc:  0.7109375
train loss:  0.5421556234359741
train gradient:  0.20825943217574466
iteration : 2729
train acc:  0.7421875
train loss:  0.5248746275901794
train gradient:  0.12214510171316913
iteration : 2730
train acc:  0.7421875
train loss:  0.5082629919052124
train gradient:  0.1181010454308792
iteration : 2731
train acc:  0.71875
train loss:  0.5181214809417725
train gradient:  0.1832163111653195
iteration : 2732
train acc:  0.7421875
train loss:  0.5216348767280579
train gradient:  0.15543783143025797
iteration : 2733
train acc:  0.796875
train loss:  0.43682175874710083
train gradient:  0.13556765217617525
iteration : 2734
train acc:  0.75
train loss:  0.504845917224884
train gradient:  0.16036080756801224
iteration : 2735
train acc:  0.625
train loss:  0.6374237537384033
train gradient:  0.2018384316230953
iteration : 2736
train acc:  0.6953125
train loss:  0.5796113610267639
train gradient:  0.19506457950545997
iteration : 2737
train acc:  0.75
train loss:  0.5202326774597168
train gradient:  0.14852361136580502
iteration : 2738
train acc:  0.7421875
train loss:  0.5131190419197083
train gradient:  0.14168216798386074
iteration : 2739
train acc:  0.734375
train loss:  0.49089547991752625
train gradient:  0.13811320820061807
iteration : 2740
train acc:  0.7578125
train loss:  0.4669003188610077
train gradient:  0.12613153414152056
iteration : 2741
train acc:  0.6875
train loss:  0.538794219493866
train gradient:  0.10339847943753457
iteration : 2742
train acc:  0.6953125
train loss:  0.560582160949707
train gradient:  0.15399559465784102
iteration : 2743
train acc:  0.6796875
train loss:  0.5478113889694214
train gradient:  0.15254408072040568
iteration : 2744
train acc:  0.703125
train loss:  0.5376136898994446
train gradient:  0.1750484453893651
iteration : 2745
train acc:  0.703125
train loss:  0.5443484783172607
train gradient:  0.14991904253759883
iteration : 2746
train acc:  0.7890625
train loss:  0.4900062680244446
train gradient:  0.1353485267492332
iteration : 2747
train acc:  0.78125
train loss:  0.43932265043258667
train gradient:  0.1272094619314613
iteration : 2748
train acc:  0.703125
train loss:  0.5276211500167847
train gradient:  0.15882264856277462
iteration : 2749
train acc:  0.7734375
train loss:  0.47506609559059143
train gradient:  0.1195990673573084
iteration : 2750
train acc:  0.7578125
train loss:  0.4771147668361664
train gradient:  0.15592075701794372
iteration : 2751
train acc:  0.6640625
train loss:  0.5642091035842896
train gradient:  0.1407475412422997
iteration : 2752
train acc:  0.734375
train loss:  0.4903421401977539
train gradient:  0.11156158228161854
iteration : 2753
train acc:  0.6953125
train loss:  0.5828597545623779
train gradient:  0.15208437985844114
iteration : 2754
train acc:  0.7421875
train loss:  0.51967453956604
train gradient:  0.1228552028688657
iteration : 2755
train acc:  0.796875
train loss:  0.4526251554489136
train gradient:  0.1310061028247297
iteration : 2756
train acc:  0.703125
train loss:  0.5232645273208618
train gradient:  0.19368179372647593
iteration : 2757
train acc:  0.703125
train loss:  0.5523592233657837
train gradient:  0.1390988063965479
iteration : 2758
train acc:  0.71875
train loss:  0.5707376003265381
train gradient:  0.18564780915343687
iteration : 2759
train acc:  0.6953125
train loss:  0.5788908004760742
train gradient:  0.21500304166667294
iteration : 2760
train acc:  0.6953125
train loss:  0.5789662599563599
train gradient:  0.1838771944831829
iteration : 2761
train acc:  0.734375
train loss:  0.5318056344985962
train gradient:  0.15512940346143672
iteration : 2762
train acc:  0.6796875
train loss:  0.5540305376052856
train gradient:  0.1743352598494699
iteration : 2763
train acc:  0.8046875
train loss:  0.4638846814632416
train gradient:  0.1386321889536763
iteration : 2764
train acc:  0.6953125
train loss:  0.5361718535423279
train gradient:  0.16928918226432693
iteration : 2765
train acc:  0.75
train loss:  0.5019030570983887
train gradient:  0.18693317357302536
iteration : 2766
train acc:  0.7578125
train loss:  0.47763240337371826
train gradient:  0.15802211111724823
iteration : 2767
train acc:  0.78125
train loss:  0.4908771216869354
train gradient:  0.11135922024624244
iteration : 2768
train acc:  0.734375
train loss:  0.5562305450439453
train gradient:  0.17583331695384782
iteration : 2769
train acc:  0.75
train loss:  0.4934847950935364
train gradient:  0.17468406918113905
iteration : 2770
train acc:  0.6953125
train loss:  0.5708246827125549
train gradient:  0.21101351740210664
iteration : 2771
train acc:  0.7265625
train loss:  0.5674264430999756
train gradient:  0.1921450648799133
iteration : 2772
train acc:  0.75
train loss:  0.46633440256118774
train gradient:  0.12482159591131574
iteration : 2773
train acc:  0.7734375
train loss:  0.4882708787918091
train gradient:  0.13398527116233366
iteration : 2774
train acc:  0.75
train loss:  0.4702204465866089
train gradient:  0.13617511572366842
iteration : 2775
train acc:  0.7421875
train loss:  0.5391840934753418
train gradient:  0.1360977629526283
iteration : 2776
train acc:  0.734375
train loss:  0.48894938826560974
train gradient:  0.1692494447344915
iteration : 2777
train acc:  0.71875
train loss:  0.5188924670219421
train gradient:  0.18403407413682304
iteration : 2778
train acc:  0.7421875
train loss:  0.5207033157348633
train gradient:  0.15910173793184254
iteration : 2779
train acc:  0.6953125
train loss:  0.4995364844799042
train gradient:  0.22119055086459388
iteration : 2780
train acc:  0.6875
train loss:  0.5436439514160156
train gradient:  0.1362330245088119
iteration : 2781
train acc:  0.703125
train loss:  0.5913033485412598
train gradient:  0.1925548940351154
iteration : 2782
train acc:  0.7265625
train loss:  0.5028263926506042
train gradient:  0.13580798301030475
iteration : 2783
train acc:  0.6796875
train loss:  0.5828871726989746
train gradient:  0.18346058761072903
iteration : 2784
train acc:  0.75
train loss:  0.5088363885879517
train gradient:  0.17403864594591187
iteration : 2785
train acc:  0.7734375
train loss:  0.49625423550605774
train gradient:  0.13334617709230367
iteration : 2786
train acc:  0.7578125
train loss:  0.5149905681610107
train gradient:  0.1503903177230233
iteration : 2787
train acc:  0.6796875
train loss:  0.5378924608230591
train gradient:  0.19691810042483654
iteration : 2788
train acc:  0.7578125
train loss:  0.5110331773757935
train gradient:  0.1591945945201514
iteration : 2789
train acc:  0.6171875
train loss:  0.6193229556083679
train gradient:  0.18422187354841496
iteration : 2790
train acc:  0.671875
train loss:  0.6136372089385986
train gradient:  0.18855375565608218
iteration : 2791
train acc:  0.6640625
train loss:  0.5603902339935303
train gradient:  0.19077897072919392
iteration : 2792
train acc:  0.6875
train loss:  0.5580823421478271
train gradient:  0.16659437086702797
iteration : 2793
train acc:  0.625
train loss:  0.5553306937217712
train gradient:  0.1866109281382144
iteration : 2794
train acc:  0.7109375
train loss:  0.5140889286994934
train gradient:  0.1666257033620393
iteration : 2795
train acc:  0.671875
train loss:  0.5688600540161133
train gradient:  0.13643015594444746
iteration : 2796
train acc:  0.734375
train loss:  0.5418233275413513
train gradient:  0.2263312955244543
iteration : 2797
train acc:  0.671875
train loss:  0.5864604711532593
train gradient:  0.17618947974815818
iteration : 2798
train acc:  0.7734375
train loss:  0.5502100586891174
train gradient:  0.15018877321580953
iteration : 2799
train acc:  0.6875
train loss:  0.5943847894668579
train gradient:  0.19321226111716927
iteration : 2800
train acc:  0.6171875
train loss:  0.5852203369140625
train gradient:  0.15586796962265562
iteration : 2801
train acc:  0.703125
train loss:  0.6091693639755249
train gradient:  0.1712626688528418
iteration : 2802
train acc:  0.703125
train loss:  0.5315309762954712
train gradient:  0.16955621353256964
iteration : 2803
train acc:  0.7265625
train loss:  0.5352208614349365
train gradient:  0.16496342242580925
iteration : 2804
train acc:  0.7421875
train loss:  0.5101238489151001
train gradient:  0.13818130262885536
iteration : 2805
train acc:  0.7109375
train loss:  0.5094966888427734
train gradient:  0.1381085097242849
iteration : 2806
train acc:  0.7265625
train loss:  0.5316364765167236
train gradient:  0.19489195368440523
iteration : 2807
train acc:  0.734375
train loss:  0.5027137994766235
train gradient:  0.12600046116102603
iteration : 2808
train acc:  0.6875
train loss:  0.5476099252700806
train gradient:  0.19776463504941166
iteration : 2809
train acc:  0.7109375
train loss:  0.5145702362060547
train gradient:  0.18542753566634623
iteration : 2810
train acc:  0.6953125
train loss:  0.5330631136894226
train gradient:  0.16128350553561688
iteration : 2811
train acc:  0.75
train loss:  0.49545371532440186
train gradient:  0.12855012230791468
iteration : 2812
train acc:  0.6953125
train loss:  0.5359700918197632
train gradient:  0.13424233297251265
iteration : 2813
train acc:  0.71875
train loss:  0.5322362184524536
train gradient:  0.15546137121538076
iteration : 2814
train acc:  0.6875
train loss:  0.5809387564659119
train gradient:  0.19614614336481906
iteration : 2815
train acc:  0.703125
train loss:  0.559057891368866
train gradient:  0.13032963097821165
iteration : 2816
train acc:  0.75
train loss:  0.4972987771034241
train gradient:  0.1239579443058678
iteration : 2817
train acc:  0.734375
train loss:  0.49870628118515015
train gradient:  0.13521738670690037
iteration : 2818
train acc:  0.6953125
train loss:  0.5326520204544067
train gradient:  0.13479010301816977
iteration : 2819
train acc:  0.734375
train loss:  0.5144861936569214
train gradient:  0.1426123839165569
iteration : 2820
train acc:  0.6171875
train loss:  0.6567273736000061
train gradient:  0.23161792583763416
iteration : 2821
train acc:  0.7265625
train loss:  0.5653483867645264
train gradient:  0.1658556899313196
iteration : 2822
train acc:  0.640625
train loss:  0.6063054800033569
train gradient:  0.24854049249428634
iteration : 2823
train acc:  0.703125
train loss:  0.5491184592247009
train gradient:  0.1827623119043021
iteration : 2824
train acc:  0.7578125
train loss:  0.49632376432418823
train gradient:  0.17425243135317625
iteration : 2825
train acc:  0.703125
train loss:  0.5402493476867676
train gradient:  0.16026602806027224
iteration : 2826
train acc:  0.7265625
train loss:  0.5391393899917603
train gradient:  0.1426763723734556
iteration : 2827
train acc:  0.7265625
train loss:  0.5683422684669495
train gradient:  0.18047200381222067
iteration : 2828
train acc:  0.71875
train loss:  0.5689871311187744
train gradient:  0.19188879617143711
iteration : 2829
train acc:  0.734375
train loss:  0.5245526432991028
train gradient:  0.147329469858116
iteration : 2830
train acc:  0.6875
train loss:  0.5446841716766357
train gradient:  0.11026547966306507
iteration : 2831
train acc:  0.7578125
train loss:  0.4682738780975342
train gradient:  0.15629863739661293
iteration : 2832
train acc:  0.6328125
train loss:  0.6070517301559448
train gradient:  0.19126799819597956
iteration : 2833
train acc:  0.6953125
train loss:  0.581030011177063
train gradient:  0.14736233340265664
iteration : 2834
train acc:  0.796875
train loss:  0.4992592930793762
train gradient:  0.17430329099937297
iteration : 2835
train acc:  0.6875
train loss:  0.5231711268424988
train gradient:  0.16725131079249408
iteration : 2836
train acc:  0.6953125
train loss:  0.5460410118103027
train gradient:  0.14009673927779687
iteration : 2837
train acc:  0.8046875
train loss:  0.41593828797340393
train gradient:  0.10524362054175375
iteration : 2838
train acc:  0.7109375
train loss:  0.5374044179916382
train gradient:  0.1502501126374531
iteration : 2839
train acc:  0.6328125
train loss:  0.624523401260376
train gradient:  0.16983193812826847
iteration : 2840
train acc:  0.6953125
train loss:  0.5525472164154053
train gradient:  0.19348758923225542
iteration : 2841
train acc:  0.671875
train loss:  0.598288893699646
train gradient:  0.2036251070103109
iteration : 2842
train acc:  0.734375
train loss:  0.5248568058013916
train gradient:  0.17187990861403293
iteration : 2843
train acc:  0.734375
train loss:  0.5202714204788208
train gradient:  0.1389216781753203
iteration : 2844
train acc:  0.6953125
train loss:  0.5532069206237793
train gradient:  0.18886951055032014
iteration : 2845
train acc:  0.7265625
train loss:  0.5492380261421204
train gradient:  0.1465531228464062
iteration : 2846
train acc:  0.6796875
train loss:  0.5566554069519043
train gradient:  0.23724367273827446
iteration : 2847
train acc:  0.78125
train loss:  0.4627160429954529
train gradient:  0.10176648949274376
iteration : 2848
train acc:  0.75
train loss:  0.4964245557785034
train gradient:  0.1298962772913084
iteration : 2849
train acc:  0.7734375
train loss:  0.5359390377998352
train gradient:  0.16374227043338369
iteration : 2850
train acc:  0.75
train loss:  0.4988524913787842
train gradient:  0.12866008268590312
iteration : 2851
train acc:  0.765625
train loss:  0.5239648818969727
train gradient:  0.21088188014305342
iteration : 2852
train acc:  0.703125
train loss:  0.5775457620620728
train gradient:  0.16571874029236117
iteration : 2853
train acc:  0.7890625
train loss:  0.4872492253780365
train gradient:  0.1363775012471731
iteration : 2854
train acc:  0.71875
train loss:  0.5172891020774841
train gradient:  0.1299919617223379
iteration : 2855
train acc:  0.6796875
train loss:  0.5700304508209229
train gradient:  0.1550877974049542
iteration : 2856
train acc:  0.6875
train loss:  0.5725587606430054
train gradient:  0.20979304075856575
iteration : 2857
train acc:  0.7109375
train loss:  0.5078073740005493
train gradient:  0.1277597711055229
iteration : 2858
train acc:  0.7421875
train loss:  0.4726938009262085
train gradient:  0.14696016330417636
iteration : 2859
train acc:  0.6484375
train loss:  0.5608006715774536
train gradient:  0.15441541156741462
iteration : 2860
train acc:  0.7578125
train loss:  0.4618581235408783
train gradient:  0.11211536679446997
iteration : 2861
train acc:  0.75
train loss:  0.5078696012496948
train gradient:  0.13583730839238137
iteration : 2862
train acc:  0.734375
train loss:  0.5474511384963989
train gradient:  0.19221366452478636
iteration : 2863
train acc:  0.671875
train loss:  0.5837012529373169
train gradient:  0.18286982891568415
iteration : 2864
train acc:  0.640625
train loss:  0.5993208885192871
train gradient:  0.21198941301566443
iteration : 2865
train acc:  0.7265625
train loss:  0.5382163524627686
train gradient:  0.12867359390549044
iteration : 2866
train acc:  0.703125
train loss:  0.5685189962387085
train gradient:  0.18384683735653168
iteration : 2867
train acc:  0.671875
train loss:  0.5620802640914917
train gradient:  0.17152259275256568
iteration : 2868
train acc:  0.7265625
train loss:  0.47382625937461853
train gradient:  0.15617212821552123
iteration : 2869
train acc:  0.7421875
train loss:  0.49540048837661743
train gradient:  0.1593471171430406
iteration : 2870
train acc:  0.7578125
train loss:  0.4874483644962311
train gradient:  0.12406117011869097
iteration : 2871
train acc:  0.625
train loss:  0.5701903104782104
train gradient:  0.19026582242514134
iteration : 2872
train acc:  0.7421875
train loss:  0.5115198493003845
train gradient:  0.12919231776633566
iteration : 2873
train acc:  0.703125
train loss:  0.5513896942138672
train gradient:  0.15897163847241602
iteration : 2874
train acc:  0.703125
train loss:  0.549435019493103
train gradient:  0.16327050504612317
iteration : 2875
train acc:  0.7734375
train loss:  0.5335824489593506
train gradient:  0.16320380927776074
iteration : 2876
train acc:  0.75
train loss:  0.5471194982528687
train gradient:  0.15822856918883316
iteration : 2877
train acc:  0.6640625
train loss:  0.5708188414573669
train gradient:  0.18636414918351213
iteration : 2878
train acc:  0.7421875
train loss:  0.5037104487419128
train gradient:  0.11409512505859591
iteration : 2879
train acc:  0.7265625
train loss:  0.5585888624191284
train gradient:  0.29250081720921645
iteration : 2880
train acc:  0.703125
train loss:  0.5404067635536194
train gradient:  0.12041357173867122
iteration : 2881
train acc:  0.7578125
train loss:  0.4868691861629486
train gradient:  0.1318933621015004
iteration : 2882
train acc:  0.78125
train loss:  0.4924173951148987
train gradient:  0.13463606884730467
iteration : 2883
train acc:  0.7578125
train loss:  0.49057140946388245
train gradient:  0.13683646713049644
iteration : 2884
train acc:  0.7265625
train loss:  0.5174118280410767
train gradient:  0.1417247415831982
iteration : 2885
train acc:  0.6953125
train loss:  0.607763946056366
train gradient:  0.2219157270419374
iteration : 2886
train acc:  0.75
train loss:  0.5307731628417969
train gradient:  0.14682051525045658
iteration : 2887
train acc:  0.7265625
train loss:  0.5537488460540771
train gradient:  0.16731930687760482
iteration : 2888
train acc:  0.7734375
train loss:  0.5251927375793457
train gradient:  0.15518391237400753
iteration : 2889
train acc:  0.796875
train loss:  0.4718664288520813
train gradient:  0.13300819580053447
iteration : 2890
train acc:  0.703125
train loss:  0.5503576993942261
train gradient:  0.20215559179837658
iteration : 2891
train acc:  0.7109375
train loss:  0.5335347056388855
train gradient:  0.1656785867747109
iteration : 2892
train acc:  0.75
train loss:  0.5215188264846802
train gradient:  0.13004226495560195
iteration : 2893
train acc:  0.6875
train loss:  0.5554713606834412
train gradient:  0.15392350021874118
iteration : 2894
train acc:  0.6796875
train loss:  0.5607049465179443
train gradient:  0.1387356232540431
iteration : 2895
train acc:  0.6953125
train loss:  0.5584282279014587
train gradient:  0.17335157755436684
iteration : 2896
train acc:  0.703125
train loss:  0.5361451506614685
train gradient:  0.14459450032958399
iteration : 2897
train acc:  0.65625
train loss:  0.5829548239707947
train gradient:  0.23325866521486038
iteration : 2898
train acc:  0.7578125
train loss:  0.4911436140537262
train gradient:  0.1311980670813606
iteration : 2899
train acc:  0.71875
train loss:  0.5357540249824524
train gradient:  0.1504110897611025
iteration : 2900
train acc:  0.703125
train loss:  0.5970007181167603
train gradient:  0.23078454681481386
iteration : 2901
train acc:  0.7890625
train loss:  0.47462016344070435
train gradient:  0.13890341218801622
iteration : 2902
train acc:  0.671875
train loss:  0.5450093746185303
train gradient:  0.13916102723454543
iteration : 2903
train acc:  0.6796875
train loss:  0.5885634422302246
train gradient:  0.20687888032907442
iteration : 2904
train acc:  0.6796875
train loss:  0.5901734828948975
train gradient:  0.17613323384580853
iteration : 2905
train acc:  0.7265625
train loss:  0.5250028371810913
train gradient:  0.2805313280977126
iteration : 2906
train acc:  0.7734375
train loss:  0.4482504427433014
train gradient:  0.11880201651316277
iteration : 2907
train acc:  0.671875
train loss:  0.5634658336639404
train gradient:  0.15737854692632441
iteration : 2908
train acc:  0.71875
train loss:  0.4987325668334961
train gradient:  0.17099407201175656
iteration : 2909
train acc:  0.6875
train loss:  0.5617743730545044
train gradient:  0.15308639378305866
iteration : 2910
train acc:  0.75
train loss:  0.5371869206428528
train gradient:  0.1620055911691436
iteration : 2911
train acc:  0.7109375
train loss:  0.5581162571907043
train gradient:  0.17595222172017208
iteration : 2912
train acc:  0.7109375
train loss:  0.5251933932304382
train gradient:  0.17705939973617413
iteration : 2913
train acc:  0.6953125
train loss:  0.5485067367553711
train gradient:  0.16464519534301003
iteration : 2914
train acc:  0.7265625
train loss:  0.5536006689071655
train gradient:  0.1658405721760389
iteration : 2915
train acc:  0.7421875
train loss:  0.5286248326301575
train gradient:  0.17207634838561575
iteration : 2916
train acc:  0.796875
train loss:  0.4593309462070465
train gradient:  0.16028387771557043
iteration : 2917
train acc:  0.734375
train loss:  0.49139273166656494
train gradient:  0.14753773507987247
iteration : 2918
train acc:  0.75
train loss:  0.4899942874908447
train gradient:  0.13478948922678163
iteration : 2919
train acc:  0.640625
train loss:  0.562140703201294
train gradient:  0.15103464151818274
iteration : 2920
train acc:  0.703125
train loss:  0.542136013507843
train gradient:  0.15007083744815253
iteration : 2921
train acc:  0.7265625
train loss:  0.49993807077407837
train gradient:  0.15893372497766378
iteration : 2922
train acc:  0.6796875
train loss:  0.5199592709541321
train gradient:  0.12211122750876835
iteration : 2923
train acc:  0.7265625
train loss:  0.5790155529975891
train gradient:  0.15381444409274964
iteration : 2924
train acc:  0.65625
train loss:  0.5394111275672913
train gradient:  0.17559133962048407
iteration : 2925
train acc:  0.7421875
train loss:  0.554868757724762
train gradient:  0.15746430575130116
iteration : 2926
train acc:  0.6796875
train loss:  0.6080142259597778
train gradient:  0.18731967260778068
iteration : 2927
train acc:  0.7265625
train loss:  0.5062316656112671
train gradient:  0.13972827823761824
iteration : 2928
train acc:  0.640625
train loss:  0.601594090461731
train gradient:  0.1864915184013157
iteration : 2929
train acc:  0.65625
train loss:  0.6006252765655518
train gradient:  0.16564330534098642
iteration : 2930
train acc:  0.671875
train loss:  0.5678572654724121
train gradient:  0.1618728931411344
iteration : 2931
train acc:  0.65625
train loss:  0.5884979963302612
train gradient:  0.23090363944891307
iteration : 2932
train acc:  0.6953125
train loss:  0.5445338487625122
train gradient:  0.17909911144237442
iteration : 2933
train acc:  0.6796875
train loss:  0.5392730832099915
train gradient:  0.13280222908783682
iteration : 2934
train acc:  0.7109375
train loss:  0.5219584703445435
train gradient:  0.15087867555125406
iteration : 2935
train acc:  0.7265625
train loss:  0.6160579919815063
train gradient:  0.18686428984678952
iteration : 2936
train acc:  0.734375
train loss:  0.5135101675987244
train gradient:  0.17355230768632604
iteration : 2937
train acc:  0.7421875
train loss:  0.5634135007858276
train gradient:  0.13963563479339225
iteration : 2938
train acc:  0.6953125
train loss:  0.569454550743103
train gradient:  0.15693459662682852
iteration : 2939
train acc:  0.6328125
train loss:  0.5995922088623047
train gradient:  0.2184142073724616
iteration : 2940
train acc:  0.78125
train loss:  0.520897626876831
train gradient:  0.14279527525192845
iteration : 2941
train acc:  0.78125
train loss:  0.48174208402633667
train gradient:  0.12819134597489287
iteration : 2942
train acc:  0.71875
train loss:  0.5333132147789001
train gradient:  0.14584397645419322
iteration : 2943
train acc:  0.796875
train loss:  0.49903950095176697
train gradient:  0.14781558346862717
iteration : 2944
train acc:  0.71875
train loss:  0.5328523516654968
train gradient:  0.1431166832250852
iteration : 2945
train acc:  0.703125
train loss:  0.5549497604370117
train gradient:  0.16146881256416973
iteration : 2946
train acc:  0.671875
train loss:  0.5926408767700195
train gradient:  0.1967560340054169
iteration : 2947
train acc:  0.71875
train loss:  0.5558761954307556
train gradient:  0.15230249169729726
iteration : 2948
train acc:  0.7421875
train loss:  0.5039536952972412
train gradient:  0.15627890894483054
iteration : 2949
train acc:  0.7578125
train loss:  0.49480998516082764
train gradient:  0.1313562549458358
iteration : 2950
train acc:  0.6171875
train loss:  0.6215038299560547
train gradient:  0.2255506000742994
iteration : 2951
train acc:  0.671875
train loss:  0.5151388645172119
train gradient:  0.13572154539516407
iteration : 2952
train acc:  0.78125
train loss:  0.48640671372413635
train gradient:  0.13859454060146642
iteration : 2953
train acc:  0.8125
train loss:  0.4761018455028534
train gradient:  0.1331216605134238
iteration : 2954
train acc:  0.6640625
train loss:  0.5501166582107544
train gradient:  0.13603764814961208
iteration : 2955
train acc:  0.6796875
train loss:  0.5757017135620117
train gradient:  0.16459786594707507
iteration : 2956
train acc:  0.75
train loss:  0.4976221024990082
train gradient:  0.1584650352363323
iteration : 2957
train acc:  0.7265625
train loss:  0.48809197545051575
train gradient:  0.15991187124193357
iteration : 2958
train acc:  0.71875
train loss:  0.5176783800125122
train gradient:  0.14906860577364434
iteration : 2959
train acc:  0.7265625
train loss:  0.5660247206687927
train gradient:  0.15823201728227887
iteration : 2960
train acc:  0.75
train loss:  0.5249703526496887
train gradient:  0.11933100940294873
iteration : 2961
train acc:  0.734375
train loss:  0.49492403864860535
train gradient:  0.11479740029927135
iteration : 2962
train acc:  0.6640625
train loss:  0.530592679977417
train gradient:  0.15235531619907117
iteration : 2963
train acc:  0.671875
train loss:  0.5664668083190918
train gradient:  0.15850995504855206
iteration : 2964
train acc:  0.6484375
train loss:  0.5654431581497192
train gradient:  0.17279482379902214
iteration : 2965
train acc:  0.71875
train loss:  0.5482884645462036
train gradient:  0.12666765149997633
iteration : 2966
train acc:  0.7734375
train loss:  0.4978718161582947
train gradient:  0.15415009745313113
iteration : 2967
train acc:  0.6796875
train loss:  0.50666344165802
train gradient:  0.12934449305674534
iteration : 2968
train acc:  0.65625
train loss:  0.5673484802246094
train gradient:  0.1681573941509654
iteration : 2969
train acc:  0.671875
train loss:  0.5357617139816284
train gradient:  0.14788355449740037
iteration : 2970
train acc:  0.65625
train loss:  0.548090934753418
train gradient:  0.15853329074781114
iteration : 2971
train acc:  0.75
train loss:  0.5278561115264893
train gradient:  0.1363622171401584
iteration : 2972
train acc:  0.640625
train loss:  0.6183583736419678
train gradient:  0.19329349642334567
iteration : 2973
train acc:  0.75
train loss:  0.5155144333839417
train gradient:  0.1632006713124221
iteration : 2974
train acc:  0.6484375
train loss:  0.5581094622612
train gradient:  0.1507606165354545
iteration : 2975
train acc:  0.6640625
train loss:  0.5577291250228882
train gradient:  0.16428843113474229
iteration : 2976
train acc:  0.7890625
train loss:  0.4655396342277527
train gradient:  0.1344432623908164
iteration : 2977
train acc:  0.734375
train loss:  0.5308201313018799
train gradient:  0.1650378697334946
iteration : 2978
train acc:  0.7421875
train loss:  0.495240181684494
train gradient:  0.16559668875590394
iteration : 2979
train acc:  0.6953125
train loss:  0.5382989645004272
train gradient:  0.1592081866142394
iteration : 2980
train acc:  0.640625
train loss:  0.5981605052947998
train gradient:  0.24746251554973947
iteration : 2981
train acc:  0.765625
train loss:  0.5223324298858643
train gradient:  0.1393739950260865
iteration : 2982
train acc:  0.6640625
train loss:  0.6027299761772156
train gradient:  0.19963270491641089
iteration : 2983
train acc:  0.7109375
train loss:  0.5357157588005066
train gradient:  0.153744907894736
iteration : 2984
train acc:  0.71875
train loss:  0.5105471014976501
train gradient:  0.1155025132325504
iteration : 2985
train acc:  0.6953125
train loss:  0.5429790019989014
train gradient:  0.15870194592320067
iteration : 2986
train acc:  0.6875
train loss:  0.5602649450302124
train gradient:  0.13504297525835326
iteration : 2987
train acc:  0.734375
train loss:  0.5318021774291992
train gradient:  0.13411912922511376
iteration : 2988
train acc:  0.6875
train loss:  0.5693118572235107
train gradient:  0.17359856469682047
iteration : 2989
train acc:  0.6953125
train loss:  0.5603907108306885
train gradient:  0.1920787367653612
iteration : 2990
train acc:  0.703125
train loss:  0.5295252203941345
train gradient:  0.14124979833952792
iteration : 2991
train acc:  0.7578125
train loss:  0.501002311706543
train gradient:  0.19052001387892536
iteration : 2992
train acc:  0.7265625
train loss:  0.5429306626319885
train gradient:  0.17797047815568967
iteration : 2993
train acc:  0.7265625
train loss:  0.5067395567893982
train gradient:  0.1658472684502067
iteration : 2994
train acc:  0.7109375
train loss:  0.5736747980117798
train gradient:  0.1897224362993839
iteration : 2995
train acc:  0.7578125
train loss:  0.4998435378074646
train gradient:  0.14452887852733487
iteration : 2996
train acc:  0.71875
train loss:  0.5368512272834778
train gradient:  0.17538435041548017
iteration : 2997
train acc:  0.75
train loss:  0.5114300847053528
train gradient:  0.13324092530291515
iteration : 2998
train acc:  0.7265625
train loss:  0.49850189685821533
train gradient:  0.19286518067227806
iteration : 2999
train acc:  0.7578125
train loss:  0.502693772315979
train gradient:  0.155078514684447
iteration : 3000
train acc:  0.6953125
train loss:  0.5638463497161865
train gradient:  0.12872877482993755
iteration : 3001
train acc:  0.7890625
train loss:  0.4569793939590454
train gradient:  0.12531244091507932
iteration : 3002
train acc:  0.671875
train loss:  0.5720169544219971
train gradient:  0.1451864302060914
iteration : 3003
train acc:  0.765625
train loss:  0.45966994762420654
train gradient:  0.13499696419472818
iteration : 3004
train acc:  0.7421875
train loss:  0.49543821811676025
train gradient:  0.20291000985916868
iteration : 3005
train acc:  0.65625
train loss:  0.5705225467681885
train gradient:  0.19167921790705855
iteration : 3006
train acc:  0.7734375
train loss:  0.5325450897216797
train gradient:  0.13294961849697035
iteration : 3007
train acc:  0.7265625
train loss:  0.5029717683792114
train gradient:  0.13120838927908762
iteration : 3008
train acc:  0.734375
train loss:  0.5069377422332764
train gradient:  0.11448246999830523
iteration : 3009
train acc:  0.75
train loss:  0.5554964542388916
train gradient:  0.19425421258432313
iteration : 3010
train acc:  0.6328125
train loss:  0.6881707906723022
train gradient:  0.30206351723022734
iteration : 3011
train acc:  0.7265625
train loss:  0.5627387166023254
train gradient:  0.1880515233230478
iteration : 3012
train acc:  0.7421875
train loss:  0.522100031375885
train gradient:  0.15476643517672173
iteration : 3013
train acc:  0.6796875
train loss:  0.5679700374603271
train gradient:  0.20452719969614283
iteration : 3014
train acc:  0.7265625
train loss:  0.5191504955291748
train gradient:  0.20351045445983573
iteration : 3015
train acc:  0.75
train loss:  0.5122216939926147
train gradient:  0.15091376388530964
iteration : 3016
train acc:  0.765625
train loss:  0.4728783369064331
train gradient:  0.13397299367765148
iteration : 3017
train acc:  0.7421875
train loss:  0.4867132306098938
train gradient:  0.15160595493341453
iteration : 3018
train acc:  0.7421875
train loss:  0.509090781211853
train gradient:  0.156069579171554
iteration : 3019
train acc:  0.7265625
train loss:  0.5265862941741943
train gradient:  0.14530390772782026
iteration : 3020
train acc:  0.7421875
train loss:  0.5127399563789368
train gradient:  0.1152610897434639
iteration : 3021
train acc:  0.640625
train loss:  0.595441997051239
train gradient:  0.15812745055952154
iteration : 3022
train acc:  0.75
train loss:  0.4783399701118469
train gradient:  0.12517008985613043
iteration : 3023
train acc:  0.7421875
train loss:  0.5597310066223145
train gradient:  0.1494633488693206
iteration : 3024
train acc:  0.6640625
train loss:  0.6311466693878174
train gradient:  0.21348097872670202
iteration : 3025
train acc:  0.6484375
train loss:  0.5789761543273926
train gradient:  0.16441020702638745
iteration : 3026
train acc:  0.6875
train loss:  0.5802123546600342
train gradient:  0.13339875872835333
iteration : 3027
train acc:  0.75
train loss:  0.47645118832588196
train gradient:  0.13971864692984515
iteration : 3028
train acc:  0.703125
train loss:  0.5238193869590759
train gradient:  0.17629921492442602
iteration : 3029
train acc:  0.765625
train loss:  0.48753467202186584
train gradient:  0.13201787006309373
iteration : 3030
train acc:  0.71875
train loss:  0.5267565846443176
train gradient:  0.1335035638699657
iteration : 3031
train acc:  0.734375
train loss:  0.5240786075592041
train gradient:  0.17422670906807242
iteration : 3032
train acc:  0.765625
train loss:  0.4983151853084564
train gradient:  0.16360799752824923
iteration : 3033
train acc:  0.71875
train loss:  0.5980579257011414
train gradient:  0.17970821141452473
iteration : 3034
train acc:  0.640625
train loss:  0.6230621933937073
train gradient:  0.20799000011196703
iteration : 3035
train acc:  0.7265625
train loss:  0.5182816982269287
train gradient:  0.16481217887272923
iteration : 3036
train acc:  0.6953125
train loss:  0.5161675214767456
train gradient:  0.1571960195988202
iteration : 3037
train acc:  0.71875
train loss:  0.5172946453094482
train gradient:  0.1464292077249251
iteration : 3038
train acc:  0.734375
train loss:  0.5597937107086182
train gradient:  0.21180965011815261
iteration : 3039
train acc:  0.6875
train loss:  0.5418663620948792
train gradient:  0.1594630457225985
iteration : 3040
train acc:  0.7109375
train loss:  0.5216040015220642
train gradient:  0.16991338069470757
iteration : 3041
train acc:  0.71875
train loss:  0.5182693004608154
train gradient:  0.13311020176435912
iteration : 3042
train acc:  0.7265625
train loss:  0.5633273124694824
train gradient:  0.20881100334056302
iteration : 3043
train acc:  0.734375
train loss:  0.4985332489013672
train gradient:  0.15901510154430365
iteration : 3044
train acc:  0.734375
train loss:  0.5489447116851807
train gradient:  0.20533248036070878
iteration : 3045
train acc:  0.703125
train loss:  0.5108654499053955
train gradient:  0.1273450341693709
iteration : 3046
train acc:  0.6953125
train loss:  0.4679158926010132
train gradient:  0.10807432783911455
iteration : 3047
train acc:  0.734375
train loss:  0.4965018630027771
train gradient:  0.13061156557406828
iteration : 3048
train acc:  0.703125
train loss:  0.5259028673171997
train gradient:  0.14863460675076853
iteration : 3049
train acc:  0.6640625
train loss:  0.6084298491477966
train gradient:  0.2014993096876367
iteration : 3050
train acc:  0.703125
train loss:  0.5197238922119141
train gradient:  0.1735502623595674
iteration : 3051
train acc:  0.7890625
train loss:  0.4736178517341614
train gradient:  0.15819876702661217
iteration : 3052
train acc:  0.671875
train loss:  0.5795159935951233
train gradient:  0.16216547064704012
iteration : 3053
train acc:  0.71875
train loss:  0.5096707344055176
train gradient:  0.13960649935575292
iteration : 3054
train acc:  0.765625
train loss:  0.4786481559276581
train gradient:  0.12060961897282003
iteration : 3055
train acc:  0.6875
train loss:  0.5609798431396484
train gradient:  0.14164915932241262
iteration : 3056
train acc:  0.6796875
train loss:  0.587742269039154
train gradient:  0.2069243145779191
iteration : 3057
train acc:  0.7421875
train loss:  0.5315313339233398
train gradient:  0.14539797071480356
iteration : 3058
train acc:  0.7265625
train loss:  0.4820023775100708
train gradient:  0.1366266013573249
iteration : 3059
train acc:  0.671875
train loss:  0.5612592697143555
train gradient:  0.20121052225406938
iteration : 3060
train acc:  0.6796875
train loss:  0.5618849396705627
train gradient:  0.1787911631551681
iteration : 3061
train acc:  0.6484375
train loss:  0.5780194401741028
train gradient:  0.21200637858791677
iteration : 3062
train acc:  0.796875
train loss:  0.46422505378723145
train gradient:  0.13276647829504307
iteration : 3063
train acc:  0.671875
train loss:  0.5299578905105591
train gradient:  0.12369567549753306
iteration : 3064
train acc:  0.703125
train loss:  0.5331701040267944
train gradient:  0.17759155397182205
iteration : 3065
train acc:  0.6796875
train loss:  0.5889405608177185
train gradient:  0.1912869406360907
iteration : 3066
train acc:  0.7421875
train loss:  0.49404624104499817
train gradient:  0.12648000795836106
iteration : 3067
train acc:  0.7578125
train loss:  0.4858236610889435
train gradient:  0.16207712620565407
iteration : 3068
train acc:  0.7578125
train loss:  0.48548460006713867
train gradient:  0.15785443936075044
iteration : 3069
train acc:  0.7421875
train loss:  0.5120978951454163
train gradient:  0.17078460785579672
iteration : 3070
train acc:  0.671875
train loss:  0.547463059425354
train gradient:  0.13979808709081085
iteration : 3071
train acc:  0.75
train loss:  0.5458234548568726
train gradient:  0.17613220111510683
iteration : 3072
train acc:  0.6875
train loss:  0.49679791927337646
train gradient:  0.13772811379433086
iteration : 3073
train acc:  0.671875
train loss:  0.5764492154121399
train gradient:  0.1755074154563567
iteration : 3074
train acc:  0.6796875
train loss:  0.5864579081535339
train gradient:  0.18752398938930875
iteration : 3075
train acc:  0.75
train loss:  0.5072675347328186
train gradient:  0.147952332315806
iteration : 3076
train acc:  0.6640625
train loss:  0.5810208320617676
train gradient:  0.1692897605806935
iteration : 3077
train acc:  0.71875
train loss:  0.533633828163147
train gradient:  0.14883863746794201
iteration : 3078
train acc:  0.6875
train loss:  0.5838613510131836
train gradient:  0.17594474055917256
iteration : 3079
train acc:  0.671875
train loss:  0.5864543914794922
train gradient:  0.17566001811352755
iteration : 3080
train acc:  0.75
train loss:  0.5265306234359741
train gradient:  0.20178093836428015
iteration : 3081
train acc:  0.671875
train loss:  0.5582406520843506
train gradient:  0.13676987369650453
iteration : 3082
train acc:  0.65625
train loss:  0.5843231678009033
train gradient:  0.2868784471238244
iteration : 3083
train acc:  0.6953125
train loss:  0.5663799047470093
train gradient:  0.20210304278717806
iteration : 3084
train acc:  0.671875
train loss:  0.5708724856376648
train gradient:  0.16161973090093895
iteration : 3085
train acc:  0.734375
train loss:  0.5193653106689453
train gradient:  0.1583193725016494
iteration : 3086
train acc:  0.6796875
train loss:  0.5752729177474976
train gradient:  0.1682006463992023
iteration : 3087
train acc:  0.734375
train loss:  0.5203988552093506
train gradient:  0.14306108048905558
iteration : 3088
train acc:  0.671875
train loss:  0.543433666229248
train gradient:  0.17504553769381537
iteration : 3089
train acc:  0.7265625
train loss:  0.5336434841156006
train gradient:  0.2585582928798795
iteration : 3090
train acc:  0.7421875
train loss:  0.5223960876464844
train gradient:  0.12818043414520075
iteration : 3091
train acc:  0.71875
train loss:  0.5016987919807434
train gradient:  0.1681919890029278
iteration : 3092
train acc:  0.7578125
train loss:  0.5654160976409912
train gradient:  0.18338028558070707
iteration : 3093
train acc:  0.6875
train loss:  0.5839188694953918
train gradient:  0.18210862597507388
iteration : 3094
train acc:  0.7265625
train loss:  0.49535754323005676
train gradient:  0.13032278578888784
iteration : 3095
train acc:  0.7265625
train loss:  0.5204126238822937
train gradient:  0.12082600300571364
iteration : 3096
train acc:  0.6640625
train loss:  0.5932652950286865
train gradient:  0.2131236013665935
iteration : 3097
train acc:  0.734375
train loss:  0.5365504622459412
train gradient:  0.1519141880576081
iteration : 3098
train acc:  0.765625
train loss:  0.48573994636535645
train gradient:  0.1546414000976226
iteration : 3099
train acc:  0.71875
train loss:  0.5287978053092957
train gradient:  0.1556042154134711
iteration : 3100
train acc:  0.671875
train loss:  0.5644544363021851
train gradient:  0.23086240359049798
iteration : 3101
train acc:  0.703125
train loss:  0.5106366872787476
train gradient:  0.16494227162088432
iteration : 3102
train acc:  0.71875
train loss:  0.5247505307197571
train gradient:  0.12758742401570514
iteration : 3103
train acc:  0.6953125
train loss:  0.6010400652885437
train gradient:  0.17866383944960715
iteration : 3104
train acc:  0.671875
train loss:  0.5502116680145264
train gradient:  0.18360606504472088
iteration : 3105
train acc:  0.6640625
train loss:  0.5539977550506592
train gradient:  0.13785784988870853
iteration : 3106
train acc:  0.796875
train loss:  0.4536081552505493
train gradient:  0.1296039059926103
iteration : 3107
train acc:  0.765625
train loss:  0.518386960029602
train gradient:  0.17355729767778605
iteration : 3108
train acc:  0.734375
train loss:  0.5487111806869507
train gradient:  0.15830648398886252
iteration : 3109
train acc:  0.703125
train loss:  0.5481219291687012
train gradient:  0.18937542135057056
iteration : 3110
train acc:  0.6875
train loss:  0.5783382654190063
train gradient:  0.21578165430844326
iteration : 3111
train acc:  0.703125
train loss:  0.5533725023269653
train gradient:  0.20864106373262528
iteration : 3112
train acc:  0.7109375
train loss:  0.5372269153594971
train gradient:  0.1533152795301077
iteration : 3113
train acc:  0.6953125
train loss:  0.5642606019973755
train gradient:  0.1326176909071225
iteration : 3114
train acc:  0.734375
train loss:  0.5067297220230103
train gradient:  0.10902047751740938
iteration : 3115
train acc:  0.75
train loss:  0.48168644309043884
train gradient:  0.14372211369261256
iteration : 3116
train acc:  0.71875
train loss:  0.5384039878845215
train gradient:  0.15377807250613107
iteration : 3117
train acc:  0.703125
train loss:  0.5208913683891296
train gradient:  0.13104151963342509
iteration : 3118
train acc:  0.75
train loss:  0.5467145442962646
train gradient:  0.18525834017677248
iteration : 3119
train acc:  0.7421875
train loss:  0.5261987447738647
train gradient:  0.18991772278074998
iteration : 3120
train acc:  0.703125
train loss:  0.5495398640632629
train gradient:  0.16831206828447826
iteration : 3121
train acc:  0.7421875
train loss:  0.47486037015914917
train gradient:  0.1309303295538185
iteration : 3122
train acc:  0.71875
train loss:  0.5629967451095581
train gradient:  0.1745569614401017
iteration : 3123
train acc:  0.7421875
train loss:  0.5312848687171936
train gradient:  0.13963033160313318
iteration : 3124
train acc:  0.7109375
train loss:  0.5603424906730652
train gradient:  0.18365907304038154
iteration : 3125
train acc:  0.7734375
train loss:  0.5414525270462036
train gradient:  0.16320244994164884
iteration : 3126
train acc:  0.6796875
train loss:  0.5391386151313782
train gradient:  0.1573201577165581
iteration : 3127
train acc:  0.7109375
train loss:  0.5230361223220825
train gradient:  0.154770603943292
iteration : 3128
train acc:  0.734375
train loss:  0.5046168565750122
train gradient:  0.16367888183599666
iteration : 3129
train acc:  0.7109375
train loss:  0.536409854888916
train gradient:  0.15023055197776114
iteration : 3130
train acc:  0.6953125
train loss:  0.5598243474960327
train gradient:  0.15050328173889893
iteration : 3131
train acc:  0.6953125
train loss:  0.563238263130188
train gradient:  0.20729503059663273
iteration : 3132
train acc:  0.6875
train loss:  0.534211277961731
train gradient:  0.155590831259822
iteration : 3133
train acc:  0.765625
train loss:  0.4807026982307434
train gradient:  0.13804192749831776
iteration : 3134
train acc:  0.6484375
train loss:  0.5131427049636841
train gradient:  0.14191372284884435
iteration : 3135
train acc:  0.703125
train loss:  0.5478546619415283
train gradient:  0.16401054448402133
iteration : 3136
train acc:  0.734375
train loss:  0.5385012626647949
train gradient:  0.22458002454174478
iteration : 3137
train acc:  0.7734375
train loss:  0.46819594502449036
train gradient:  0.17567276205020077
iteration : 3138
train acc:  0.7265625
train loss:  0.5493529438972473
train gradient:  0.15355042337751865
iteration : 3139
train acc:  0.640625
train loss:  0.5783690214157104
train gradient:  0.1632586581303594
iteration : 3140
train acc:  0.671875
train loss:  0.5519076585769653
train gradient:  0.16500338780041957
iteration : 3141
train acc:  0.6640625
train loss:  0.5531115531921387
train gradient:  0.18826492579422544
iteration : 3142
train acc:  0.7578125
train loss:  0.5619577169418335
train gradient:  0.12850336195023915
iteration : 3143
train acc:  0.6484375
train loss:  0.6000022888183594
train gradient:  0.15535243937823212
iteration : 3144
train acc:  0.7421875
train loss:  0.5178313255310059
train gradient:  0.18244399930204275
iteration : 3145
train acc:  0.7578125
train loss:  0.4825030565261841
train gradient:  0.15108083698426505
iteration : 3146
train acc:  0.7421875
train loss:  0.5488938093185425
train gradient:  0.19223436765872393
iteration : 3147
train acc:  0.6796875
train loss:  0.586388349533081
train gradient:  0.18991474886023885
iteration : 3148
train acc:  0.7109375
train loss:  0.5376230478286743
train gradient:  0.14845040581757057
iteration : 3149
train acc:  0.765625
train loss:  0.5010268092155457
train gradient:  0.21356099836947706
iteration : 3150
train acc:  0.7109375
train loss:  0.5077202320098877
train gradient:  0.14296866587548812
iteration : 3151
train acc:  0.7578125
train loss:  0.49605798721313477
train gradient:  0.1392431194967319
iteration : 3152
train acc:  0.7109375
train loss:  0.5300509929656982
train gradient:  0.12618666803777492
iteration : 3153
train acc:  0.7109375
train loss:  0.5435194969177246
train gradient:  0.20540692413107886
iteration : 3154
train acc:  0.7109375
train loss:  0.5417269468307495
train gradient:  0.11656662273023621
iteration : 3155
train acc:  0.734375
train loss:  0.5147842168807983
train gradient:  0.15787474934141943
iteration : 3156
train acc:  0.7265625
train loss:  0.52796870470047
train gradient:  0.14746624385821605
iteration : 3157
train acc:  0.703125
train loss:  0.516029953956604
train gradient:  0.18868096665364936
iteration : 3158
train acc:  0.6796875
train loss:  0.523073673248291
train gradient:  0.16105989765344714
iteration : 3159
train acc:  0.7109375
train loss:  0.5269979238510132
train gradient:  0.17694363714587316
iteration : 3160
train acc:  0.703125
train loss:  0.6110567450523376
train gradient:  0.19992692387574526
iteration : 3161
train acc:  0.7265625
train loss:  0.5344622135162354
train gradient:  0.17317705431474276
iteration : 3162
train acc:  0.6328125
train loss:  0.5908266305923462
train gradient:  0.20291755808219047
iteration : 3163
train acc:  0.6875
train loss:  0.5712477564811707
train gradient:  0.1851836808945975
iteration : 3164
train acc:  0.71875
train loss:  0.5383057594299316
train gradient:  0.15157570983716245
iteration : 3165
train acc:  0.7421875
train loss:  0.5245053172111511
train gradient:  0.14941372831420288
iteration : 3166
train acc:  0.671875
train loss:  0.5840312242507935
train gradient:  0.17195698605196097
iteration : 3167
train acc:  0.6875
train loss:  0.5570058822631836
train gradient:  0.17194530028197685
iteration : 3168
train acc:  0.7734375
train loss:  0.47250139713287354
train gradient:  0.15479259792539118
iteration : 3169
train acc:  0.734375
train loss:  0.5649735927581787
train gradient:  0.1953072087657834
iteration : 3170
train acc:  0.6875
train loss:  0.5430760979652405
train gradient:  0.12719253367522454
iteration : 3171
train acc:  0.7109375
train loss:  0.4864933490753174
train gradient:  0.12509300145386407
iteration : 3172
train acc:  0.7578125
train loss:  0.49017810821533203
train gradient:  0.14993458945365817
iteration : 3173
train acc:  0.6953125
train loss:  0.5537369251251221
train gradient:  0.19256623108955484
iteration : 3174
train acc:  0.7265625
train loss:  0.5275716185569763
train gradient:  0.1337754360727661
iteration : 3175
train acc:  0.6953125
train loss:  0.508384644985199
train gradient:  0.14577910605799566
iteration : 3176
train acc:  0.703125
train loss:  0.5672907829284668
train gradient:  0.20645669121934387
iteration : 3177
train acc:  0.703125
train loss:  0.5633978843688965
train gradient:  0.18534916718920558
iteration : 3178
train acc:  0.6484375
train loss:  0.5783287286758423
train gradient:  0.20102820516099962
iteration : 3179
train acc:  0.8359375
train loss:  0.4536370635032654
train gradient:  0.1546760563738276
iteration : 3180
train acc:  0.765625
train loss:  0.48957180976867676
train gradient:  0.16449805879355406
iteration : 3181
train acc:  0.734375
train loss:  0.49630001187324524
train gradient:  0.15914561018159024
iteration : 3182
train acc:  0.75
train loss:  0.4999762177467346
train gradient:  0.13430679147303207
iteration : 3183
train acc:  0.6796875
train loss:  0.5701290965080261
train gradient:  0.22317856224281524
iteration : 3184
train acc:  0.6796875
train loss:  0.5780771374702454
train gradient:  0.15621258579776162
iteration : 3185
train acc:  0.796875
train loss:  0.43520110845565796
train gradient:  0.1245516916403697
iteration : 3186
train acc:  0.703125
train loss:  0.585936427116394
train gradient:  0.1891525765447778
iteration : 3187
train acc:  0.7109375
train loss:  0.4933374524116516
train gradient:  0.11474651041387714
iteration : 3188
train acc:  0.6875
train loss:  0.5212360620498657
train gradient:  0.17830674953563552
iteration : 3189
train acc:  0.75
train loss:  0.5319768190383911
train gradient:  0.1655402772888841
iteration : 3190
train acc:  0.7421875
train loss:  0.5134214758872986
train gradient:  0.12904700910403433
iteration : 3191
train acc:  0.65625
train loss:  0.5783222913742065
train gradient:  0.15153342728760866
iteration : 3192
train acc:  0.703125
train loss:  0.5150895118713379
train gradient:  0.1576919131041244
iteration : 3193
train acc:  0.703125
train loss:  0.5474787950515747
train gradient:  0.1607841605512939
iteration : 3194
train acc:  0.6328125
train loss:  0.590613842010498
train gradient:  0.23516899654808987
iteration : 3195
train acc:  0.65625
train loss:  0.5738601088523865
train gradient:  0.18800865283886542
iteration : 3196
train acc:  0.8046875
train loss:  0.4628211259841919
train gradient:  0.16039742387663503
iteration : 3197
train acc:  0.6796875
train loss:  0.5728534460067749
train gradient:  0.22744407650461412
iteration : 3198
train acc:  0.6953125
train loss:  0.5785940289497375
train gradient:  0.15645487769746366
iteration : 3199
train acc:  0.7578125
train loss:  0.5317299365997314
train gradient:  0.20540212270528113
iteration : 3200
train acc:  0.7265625
train loss:  0.5116176605224609
train gradient:  0.1502134635142739
iteration : 3201
train acc:  0.65625
train loss:  0.5695830583572388
train gradient:  0.17494639067293555
iteration : 3202
train acc:  0.71875
train loss:  0.5179398059844971
train gradient:  0.17696677134975575
iteration : 3203
train acc:  0.734375
train loss:  0.5273422002792358
train gradient:  0.14540437826420102
iteration : 3204
train acc:  0.75
train loss:  0.5462314486503601
train gradient:  0.17104750921238931
iteration : 3205
train acc:  0.7578125
train loss:  0.5195737481117249
train gradient:  0.1584405904341012
iteration : 3206
train acc:  0.71875
train loss:  0.5702412128448486
train gradient:  0.1710429295492781
iteration : 3207
train acc:  0.6953125
train loss:  0.5347768068313599
train gradient:  0.18388884647471304
iteration : 3208
train acc:  0.7734375
train loss:  0.47170117497444153
train gradient:  0.13348767924986987
iteration : 3209
train acc:  0.609375
train loss:  0.596626877784729
train gradient:  0.17880231762520826
iteration : 3210
train acc:  0.625
train loss:  0.5716639161109924
train gradient:  0.14959121178081458
iteration : 3211
train acc:  0.7421875
train loss:  0.548686146736145
train gradient:  0.13541564846887152
iteration : 3212
train acc:  0.734375
train loss:  0.5163129568099976
train gradient:  0.14149642219020378
iteration : 3213
train acc:  0.7109375
train loss:  0.528160035610199
train gradient:  0.14158727004987354
iteration : 3214
train acc:  0.71875
train loss:  0.5292913913726807
train gradient:  0.17076759734202926
iteration : 3215
train acc:  0.71875
train loss:  0.5266603231430054
train gradient:  0.16977730975361865
iteration : 3216
train acc:  0.7421875
train loss:  0.49677103757858276
train gradient:  0.10738371737958266
iteration : 3217
train acc:  0.6640625
train loss:  0.621783971786499
train gradient:  0.18611458832991412
iteration : 3218
train acc:  0.703125
train loss:  0.5425974130630493
train gradient:  0.18287617917041674
iteration : 3219
train acc:  0.734375
train loss:  0.5064966678619385
train gradient:  0.1243548715861961
iteration : 3220
train acc:  0.7421875
train loss:  0.5311551094055176
train gradient:  0.15488312130229798
iteration : 3221
train acc:  0.734375
train loss:  0.5329642295837402
train gradient:  0.14829759195779849
iteration : 3222
train acc:  0.7421875
train loss:  0.4740123748779297
train gradient:  0.13507800716899587
iteration : 3223
train acc:  0.6484375
train loss:  0.5682172179222107
train gradient:  0.13541864071771226
iteration : 3224
train acc:  0.6953125
train loss:  0.5835052728652954
train gradient:  0.16231614649149534
iteration : 3225
train acc:  0.703125
train loss:  0.514682412147522
train gradient:  0.16104248159472387
iteration : 3226
train acc:  0.796875
train loss:  0.47136542201042175
train gradient:  0.17229743225200367
iteration : 3227
train acc:  0.703125
train loss:  0.5565383434295654
train gradient:  0.1344060410214498
iteration : 3228
train acc:  0.6640625
train loss:  0.5669332146644592
train gradient:  0.14331558544829254
iteration : 3229
train acc:  0.71875
train loss:  0.5191425085067749
train gradient:  0.13658616296287723
iteration : 3230
train acc:  0.71875
train loss:  0.5534000396728516
train gradient:  0.13446770893559773
iteration : 3231
train acc:  0.6640625
train loss:  0.557475209236145
train gradient:  0.13824387106069425
iteration : 3232
train acc:  0.671875
train loss:  0.5601754188537598
train gradient:  0.14665995286558697
iteration : 3233
train acc:  0.6875
train loss:  0.5569220781326294
train gradient:  0.14967457834328074
iteration : 3234
train acc:  0.703125
train loss:  0.5603463649749756
train gradient:  0.18954406506284954
iteration : 3235
train acc:  0.6640625
train loss:  0.5432760715484619
train gradient:  0.17840333711999068
iteration : 3236
train acc:  0.7109375
train loss:  0.5440194606781006
train gradient:  0.1745815510507989
iteration : 3237
train acc:  0.7734375
train loss:  0.515414297580719
train gradient:  0.14848695046354937
iteration : 3238
train acc:  0.71875
train loss:  0.5203578472137451
train gradient:  0.14441682230669373
iteration : 3239
train acc:  0.734375
train loss:  0.5232915878295898
train gradient:  0.1274684514940948
iteration : 3240
train acc:  0.734375
train loss:  0.5260262489318848
train gradient:  0.14252493429296345
iteration : 3241
train acc:  0.7578125
train loss:  0.4780266284942627
train gradient:  0.12517852913104632
iteration : 3242
train acc:  0.6953125
train loss:  0.5609267950057983
train gradient:  0.17854591939692463
iteration : 3243
train acc:  0.6875
train loss:  0.5729191303253174
train gradient:  0.18382409806347455
iteration : 3244
train acc:  0.7109375
train loss:  0.5015295743942261
train gradient:  0.13907044724339263
iteration : 3245
train acc:  0.7734375
train loss:  0.4818337559700012
train gradient:  0.13141041762638847
iteration : 3246
train acc:  0.734375
train loss:  0.5165068507194519
train gradient:  0.11613996216798066
iteration : 3247
train acc:  0.640625
train loss:  0.6444100141525269
train gradient:  0.223106278328493
iteration : 3248
train acc:  0.6640625
train loss:  0.5834898948669434
train gradient:  0.18865385376662996
iteration : 3249
train acc:  0.671875
train loss:  0.5829732418060303
train gradient:  0.16132927937990932
iteration : 3250
train acc:  0.6875
train loss:  0.5768944025039673
train gradient:  0.18963182268890685
iteration : 3251
train acc:  0.6484375
train loss:  0.6082789897918701
train gradient:  0.23076707009608988
iteration : 3252
train acc:  0.703125
train loss:  0.5442132949829102
train gradient:  0.14738978475806208
iteration : 3253
train acc:  0.7109375
train loss:  0.5093957185745239
train gradient:  0.14864954758555132
iteration : 3254
train acc:  0.6875
train loss:  0.578006386756897
train gradient:  0.17060263053705743
iteration : 3255
train acc:  0.6796875
train loss:  0.5993472337722778
train gradient:  0.1610907293655596
iteration : 3256
train acc:  0.75
train loss:  0.5136026740074158
train gradient:  0.19171907929975823
iteration : 3257
train acc:  0.734375
train loss:  0.474589467048645
train gradient:  0.11058144752498114
iteration : 3258
train acc:  0.7109375
train loss:  0.5456522703170776
train gradient:  0.12659731173177743
iteration : 3259
train acc:  0.625
train loss:  0.6091152429580688
train gradient:  0.19809449965958154
iteration : 3260
train acc:  0.7109375
train loss:  0.5617927312850952
train gradient:  0.137444821090671
iteration : 3261
train acc:  0.7265625
train loss:  0.4732016324996948
train gradient:  0.13773644894632212
iteration : 3262
train acc:  0.7109375
train loss:  0.5435812473297119
train gradient:  0.1807160838547523
iteration : 3263
train acc:  0.765625
train loss:  0.4879568815231323
train gradient:  0.13812584310013193
iteration : 3264
train acc:  0.703125
train loss:  0.5074892044067383
train gradient:  0.1611127185213918
iteration : 3265
train acc:  0.71875
train loss:  0.5638024210929871
train gradient:  0.15141449218740607
iteration : 3266
train acc:  0.6875
train loss:  0.5496618747711182
train gradient:  0.15915744155603812
iteration : 3267
train acc:  0.6875
train loss:  0.5352417826652527
train gradient:  0.14699199622810794
iteration : 3268
train acc:  0.75
train loss:  0.5310230255126953
train gradient:  0.15508719746508348
iteration : 3269
train acc:  0.71875
train loss:  0.5537806749343872
train gradient:  0.15456082357210896
iteration : 3270
train acc:  0.6640625
train loss:  0.5861564874649048
train gradient:  0.177885869671629
iteration : 3271
train acc:  0.7578125
train loss:  0.5043241381645203
train gradient:  0.17603301116533843
iteration : 3272
train acc:  0.6953125
train loss:  0.5202866792678833
train gradient:  0.16925751126997732
iteration : 3273
train acc:  0.703125
train loss:  0.5206817984580994
train gradient:  0.2094059074862385
iteration : 3274
train acc:  0.7734375
train loss:  0.48640042543411255
train gradient:  0.1526936832672501
iteration : 3275
train acc:  0.71875
train loss:  0.5245102643966675
train gradient:  0.16098432853967778
iteration : 3276
train acc:  0.6796875
train loss:  0.5620095729827881
train gradient:  0.14914968991000715
iteration : 3277
train acc:  0.7265625
train loss:  0.4889335632324219
train gradient:  0.12426527421929992
iteration : 3278
train acc:  0.6875
train loss:  0.5990272760391235
train gradient:  0.1730832382829397
iteration : 3279
train acc:  0.75
train loss:  0.5134505033493042
train gradient:  0.15496333004987597
iteration : 3280
train acc:  0.578125
train loss:  0.636691689491272
train gradient:  0.20161671312536905
iteration : 3281
train acc:  0.765625
train loss:  0.49816542863845825
train gradient:  0.15927316183904863
iteration : 3282
train acc:  0.6484375
train loss:  0.6102628111839294
train gradient:  0.21777862656340874
iteration : 3283
train acc:  0.78125
train loss:  0.5148659944534302
train gradient:  0.11021250160518896
iteration : 3284
train acc:  0.71875
train loss:  0.5461313724517822
train gradient:  0.18822065083544648
iteration : 3285
train acc:  0.7265625
train loss:  0.5079489350318909
train gradient:  0.12592170333155556
iteration : 3286
train acc:  0.7421875
train loss:  0.5234576463699341
train gradient:  0.14813615134598695
iteration : 3287
train acc:  0.671875
train loss:  0.5333185195922852
train gradient:  0.21043710982047015
iteration : 3288
train acc:  0.734375
train loss:  0.5488584041595459
train gradient:  0.12779873014807197
iteration : 3289
train acc:  0.703125
train loss:  0.5108989477157593
train gradient:  0.12568297334548223
iteration : 3290
train acc:  0.7109375
train loss:  0.5171512365341187
train gradient:  0.1391208565856512
iteration : 3291
train acc:  0.765625
train loss:  0.4850475788116455
train gradient:  0.18532221899147835
iteration : 3292
train acc:  0.765625
train loss:  0.4928966760635376
train gradient:  0.1211789150312409
iteration : 3293
train acc:  0.734375
train loss:  0.5149123668670654
train gradient:  0.12778023901379115
iteration : 3294
train acc:  0.7265625
train loss:  0.5280588865280151
train gradient:  0.12786601366591802
iteration : 3295
train acc:  0.7421875
train loss:  0.5091848373413086
train gradient:  0.11695480108732607
iteration : 3296
train acc:  0.71875
train loss:  0.5749554634094238
train gradient:  0.17062388420698144
iteration : 3297
train acc:  0.7734375
train loss:  0.5323599576950073
train gradient:  0.16597525868453364
iteration : 3298
train acc:  0.75
train loss:  0.5080747008323669
train gradient:  0.12611609134072085
iteration : 3299
train acc:  0.7421875
train loss:  0.5054209232330322
train gradient:  0.16011889941632412
iteration : 3300
train acc:  0.6796875
train loss:  0.5428717136383057
train gradient:  0.1328068156665726
iteration : 3301
train acc:  0.703125
train loss:  0.5054637789726257
train gradient:  0.1529659868773196
iteration : 3302
train acc:  0.78125
train loss:  0.4689769446849823
train gradient:  0.12319224971272545
iteration : 3303
train acc:  0.7421875
train loss:  0.5250934958457947
train gradient:  0.1480021380448332
iteration : 3304
train acc:  0.734375
train loss:  0.5398319959640503
train gradient:  0.1906387632342573
iteration : 3305
train acc:  0.7421875
train loss:  0.5294166207313538
train gradient:  0.1738408929156587
iteration : 3306
train acc:  0.6953125
train loss:  0.5407716035842896
train gradient:  0.18697487931645462
iteration : 3307
train acc:  0.703125
train loss:  0.572321891784668
train gradient:  0.15751870326223139
iteration : 3308
train acc:  0.671875
train loss:  0.5252629518508911
train gradient:  0.156082930125991
iteration : 3309
train acc:  0.6953125
train loss:  0.5594416260719299
train gradient:  0.16990525948123752
iteration : 3310
train acc:  0.6953125
train loss:  0.5718381404876709
train gradient:  0.16745406284518702
iteration : 3311
train acc:  0.703125
train loss:  0.5409857034683228
train gradient:  0.14510917355333058
iteration : 3312
train acc:  0.75
train loss:  0.5330144166946411
train gradient:  0.15492689640019444
iteration : 3313
train acc:  0.7265625
train loss:  0.5585346221923828
train gradient:  0.1501150144348831
iteration : 3314
train acc:  0.640625
train loss:  0.6788877844810486
train gradient:  0.34238871007515354
iteration : 3315
train acc:  0.7734375
train loss:  0.43350088596343994
train gradient:  0.18779477970555325
iteration : 3316
train acc:  0.7578125
train loss:  0.5244311094284058
train gradient:  0.20710918761288483
iteration : 3317
train acc:  0.71875
train loss:  0.5410958528518677
train gradient:  0.15279429980067216
iteration : 3318
train acc:  0.6875
train loss:  0.5207012891769409
train gradient:  0.1274293793896095
iteration : 3319
train acc:  0.6875
train loss:  0.5626663565635681
train gradient:  0.17325908102851367
iteration : 3320
train acc:  0.671875
train loss:  0.5900290608406067
train gradient:  0.2383257206970394
iteration : 3321
train acc:  0.8359375
train loss:  0.40696144104003906
train gradient:  0.12090856758511212
iteration : 3322
train acc:  0.7265625
train loss:  0.5053744316101074
train gradient:  0.13227113482500602
iteration : 3323
train acc:  0.703125
train loss:  0.5188503265380859
train gradient:  0.13538821228546427
iteration : 3324
train acc:  0.7421875
train loss:  0.5116003751754761
train gradient:  0.16434571466001374
iteration : 3325
train acc:  0.7421875
train loss:  0.5211735963821411
train gradient:  0.14754855042440163
iteration : 3326
train acc:  0.7578125
train loss:  0.4811246991157532
train gradient:  0.13801200297023108
iteration : 3327
train acc:  0.703125
train loss:  0.5063029527664185
train gradient:  0.15508083417599366
iteration : 3328
train acc:  0.65625
train loss:  0.6239047050476074
train gradient:  0.22049433885742709
iteration : 3329
train acc:  0.71875
train loss:  0.5139264464378357
train gradient:  0.16249821317479024
iteration : 3330
train acc:  0.6796875
train loss:  0.5224187970161438
train gradient:  0.15727706549598744
iteration : 3331
train acc:  0.7890625
train loss:  0.459990918636322
train gradient:  0.16978673080709028
iteration : 3332
train acc:  0.7734375
train loss:  0.5346438884735107
train gradient:  0.15287441960865727
iteration : 3333
train acc:  0.7578125
train loss:  0.4688299298286438
train gradient:  0.13276769747683287
iteration : 3334
train acc:  0.71875
train loss:  0.576897144317627
train gradient:  0.17223448405192293
iteration : 3335
train acc:  0.6484375
train loss:  0.5902673006057739
train gradient:  0.1751095183879267
iteration : 3336
train acc:  0.71875
train loss:  0.5074211359024048
train gradient:  0.11356734342732788
iteration : 3337
train acc:  0.7421875
train loss:  0.560672402381897
train gradient:  0.16895479609883535
iteration : 3338
train acc:  0.7109375
train loss:  0.5298651456832886
train gradient:  0.15696131292111742
iteration : 3339
train acc:  0.7265625
train loss:  0.5281158685684204
train gradient:  0.17212399091281944
iteration : 3340
train acc:  0.6875
train loss:  0.5457143783569336
train gradient:  0.18658412280813103
iteration : 3341
train acc:  0.703125
train loss:  0.5074665546417236
train gradient:  0.12774728394037524
iteration : 3342
train acc:  0.6953125
train loss:  0.5460646152496338
train gradient:  0.2006732544550907
iteration : 3343
train acc:  0.7265625
train loss:  0.5107079744338989
train gradient:  0.18518615091660803
iteration : 3344
train acc:  0.6953125
train loss:  0.5394009351730347
train gradient:  0.14081762579770685
iteration : 3345
train acc:  0.7265625
train loss:  0.49677330255508423
train gradient:  0.136239243536054
iteration : 3346
train acc:  0.7265625
train loss:  0.5225692987442017
train gradient:  0.13728670475450452
iteration : 3347
train acc:  0.6640625
train loss:  0.5447027087211609
train gradient:  0.18129853930129242
iteration : 3348
train acc:  0.796875
train loss:  0.48202165961265564
train gradient:  0.11513682486720972
iteration : 3349
train acc:  0.7265625
train loss:  0.5070725679397583
train gradient:  0.1584774678272442
iteration : 3350
train acc:  0.625
train loss:  0.6159439086914062
train gradient:  0.2290179130484472
iteration : 3351
train acc:  0.703125
train loss:  0.5371050238609314
train gradient:  0.18042667367614368
iteration : 3352
train acc:  0.734375
train loss:  0.5053489804267883
train gradient:  0.12310725490705336
iteration : 3353
train acc:  0.765625
train loss:  0.46306541562080383
train gradient:  0.1369348633835502
iteration : 3354
train acc:  0.6875
train loss:  0.5371814370155334
train gradient:  0.18075895569693895
iteration : 3355
train acc:  0.7578125
train loss:  0.5059715509414673
train gradient:  0.13286935808399328
iteration : 3356
train acc:  0.765625
train loss:  0.4748857915401459
train gradient:  0.14940555768070077
iteration : 3357
train acc:  0.7421875
train loss:  0.5338411927223206
train gradient:  0.1242217275900979
iteration : 3358
train acc:  0.6640625
train loss:  0.569796085357666
train gradient:  0.19414687003265446
iteration : 3359
train acc:  0.7265625
train loss:  0.5549533367156982
train gradient:  0.1902601555212558
iteration : 3360
train acc:  0.65625
train loss:  0.5679885149002075
train gradient:  0.15374262035239
iteration : 3361
train acc:  0.734375
train loss:  0.4955406188964844
train gradient:  0.1245382767576892
iteration : 3362
train acc:  0.7890625
train loss:  0.5103543400764465
train gradient:  0.14239286322700634
iteration : 3363
train acc:  0.7421875
train loss:  0.5020003318786621
train gradient:  0.13478771722389413
iteration : 3364
train acc:  0.8046875
train loss:  0.5306079387664795
train gradient:  0.1988658334704766
iteration : 3365
train acc:  0.75
train loss:  0.5103878974914551
train gradient:  0.14975097825889216
iteration : 3366
train acc:  0.71875
train loss:  0.5111921429634094
train gradient:  0.14221543939686448
iteration : 3367
train acc:  0.75
train loss:  0.47636544704437256
train gradient:  0.13586902584450566
iteration : 3368
train acc:  0.7109375
train loss:  0.4951944947242737
train gradient:  0.1638081254322095
iteration : 3369
train acc:  0.6875
train loss:  0.6040554046630859
train gradient:  0.22232751891344188
iteration : 3370
train acc:  0.6875
train loss:  0.5641055107116699
train gradient:  0.17041721073404942
iteration : 3371
train acc:  0.734375
train loss:  0.5157179832458496
train gradient:  0.1537954509495093
iteration : 3372
train acc:  0.7109375
train loss:  0.5361090898513794
train gradient:  0.13970219150785335
iteration : 3373
train acc:  0.7109375
train loss:  0.5388675928115845
train gradient:  0.15723514939929928
iteration : 3374
train acc:  0.71875
train loss:  0.5471827983856201
train gradient:  0.14361766928579228
iteration : 3375
train acc:  0.7421875
train loss:  0.5035014748573303
train gradient:  0.13142113980514897
iteration : 3376
train acc:  0.78125
train loss:  0.4878377318382263
train gradient:  0.12729078492816182
iteration : 3377
train acc:  0.7890625
train loss:  0.46550866961479187
train gradient:  0.1563476957117803
iteration : 3378
train acc:  0.7265625
train loss:  0.47178956866264343
train gradient:  0.13783758848636385
iteration : 3379
train acc:  0.71875
train loss:  0.5221015214920044
train gradient:  0.14011744616869817
iteration : 3380
train acc:  0.765625
train loss:  0.491984486579895
train gradient:  0.1376247304853687
iteration : 3381
train acc:  0.7265625
train loss:  0.5539097785949707
train gradient:  0.19920586362479437
iteration : 3382
train acc:  0.765625
train loss:  0.46984952688217163
train gradient:  0.12006961166749554
iteration : 3383
train acc:  0.6953125
train loss:  0.5486237406730652
train gradient:  0.17562679510991913
iteration : 3384
train acc:  0.6953125
train loss:  0.5377600789070129
train gradient:  0.18105308116745578
iteration : 3385
train acc:  0.78125
train loss:  0.47584831714630127
train gradient:  0.14867296523638435
iteration : 3386
train acc:  0.8125
train loss:  0.4495662450790405
train gradient:  0.11489833199067338
iteration : 3387
train acc:  0.6953125
train loss:  0.538848340511322
train gradient:  0.16490145945803822
iteration : 3388
train acc:  0.625
train loss:  0.5638124942779541
train gradient:  0.1630757371051173
iteration : 3389
train acc:  0.7421875
train loss:  0.5053790211677551
train gradient:  0.12367733149225335
iteration : 3390
train acc:  0.6796875
train loss:  0.5742278695106506
train gradient:  0.18948693819084925
iteration : 3391
train acc:  0.6953125
train loss:  0.541527271270752
train gradient:  0.15932648900059715
iteration : 3392
train acc:  0.671875
train loss:  0.5680204033851624
train gradient:  0.20280248462145833
iteration : 3393
train acc:  0.8125
train loss:  0.4395952820777893
train gradient:  0.1067910897461397
iteration : 3394
train acc:  0.6953125
train loss:  0.5474002361297607
train gradient:  0.15701247081084047
iteration : 3395
train acc:  0.6953125
train loss:  0.6015642881393433
train gradient:  0.22522113388498444
iteration : 3396
train acc:  0.75
train loss:  0.46527761220932007
train gradient:  0.17707338152977287
iteration : 3397
train acc:  0.7265625
train loss:  0.4848698079586029
train gradient:  0.13167026522746594
iteration : 3398
train acc:  0.71875
train loss:  0.5680707693099976
train gradient:  0.159186102343357
iteration : 3399
train acc:  0.6953125
train loss:  0.5524368286132812
train gradient:  0.1844852048420552
iteration : 3400
train acc:  0.6953125
train loss:  0.5699568390846252
train gradient:  0.18677859181436846
iteration : 3401
train acc:  0.671875
train loss:  0.5649729371070862
train gradient:  0.18535889568318326
iteration : 3402
train acc:  0.7421875
train loss:  0.4945991635322571
train gradient:  0.144200120912588
iteration : 3403
train acc:  0.671875
train loss:  0.5815147757530212
train gradient:  0.24165996085483948
iteration : 3404
train acc:  0.703125
train loss:  0.5889774560928345
train gradient:  0.21460016624815162
iteration : 3405
train acc:  0.7109375
train loss:  0.5345818996429443
train gradient:  0.15806255163796828
iteration : 3406
train acc:  0.71875
train loss:  0.545224666595459
train gradient:  0.15092585651578877
iteration : 3407
train acc:  0.765625
train loss:  0.488224059343338
train gradient:  0.1258207430118157
iteration : 3408
train acc:  0.6953125
train loss:  0.5234341621398926
train gradient:  0.12665365678680213
iteration : 3409
train acc:  0.6875
train loss:  0.5769027471542358
train gradient:  0.19485189839809508
iteration : 3410
train acc:  0.6953125
train loss:  0.518256425857544
train gradient:  0.13194237755685495
iteration : 3411
train acc:  0.7578125
train loss:  0.4813064932823181
train gradient:  0.1836654639733858
iteration : 3412
train acc:  0.734375
train loss:  0.5121679306030273
train gradient:  0.14630276039322326
iteration : 3413
train acc:  0.796875
train loss:  0.45488303899765015
train gradient:  0.11412458262840723
iteration : 3414
train acc:  0.640625
train loss:  0.5678727626800537
train gradient:  0.20313411511389773
iteration : 3415
train acc:  0.796875
train loss:  0.4275268316268921
train gradient:  0.10422537094114138
iteration : 3416
train acc:  0.7578125
train loss:  0.49641671776771545
train gradient:  0.16103544999978814
iteration : 3417
train acc:  0.734375
train loss:  0.5511119961738586
train gradient:  0.13885011984822795
iteration : 3418
train acc:  0.7421875
train loss:  0.48444414138793945
train gradient:  0.18433438980636746
iteration : 3419
train acc:  0.7421875
train loss:  0.5054449439048767
train gradient:  0.14420002557446848
iteration : 3420
train acc:  0.7109375
train loss:  0.5335527658462524
train gradient:  0.14143579154209374
iteration : 3421
train acc:  0.75
train loss:  0.4940822124481201
train gradient:  0.1784308398876015
iteration : 3422
train acc:  0.6484375
train loss:  0.6011994481086731
train gradient:  0.22167149614207432
iteration : 3423
train acc:  0.7578125
train loss:  0.5259192585945129
train gradient:  0.1543426103683732
iteration : 3424
train acc:  0.640625
train loss:  0.5933299660682678
train gradient:  0.15747926972746207
iteration : 3425
train acc:  0.71875
train loss:  0.532051682472229
train gradient:  0.13574302657546594
iteration : 3426
train acc:  0.7421875
train loss:  0.45975860953330994
train gradient:  0.12895211086666047
iteration : 3427
train acc:  0.7109375
train loss:  0.5577386617660522
train gradient:  0.20483966287412894
iteration : 3428
train acc:  0.703125
train loss:  0.5333061814308167
train gradient:  0.1520065528734128
iteration : 3429
train acc:  0.7578125
train loss:  0.5057517290115356
train gradient:  0.179217419575654
iteration : 3430
train acc:  0.7109375
train loss:  0.5532141923904419
train gradient:  0.18904269769301224
iteration : 3431
train acc:  0.7265625
train loss:  0.5293117761611938
train gradient:  0.15929838188155712
iteration : 3432
train acc:  0.6875
train loss:  0.5596962571144104
train gradient:  0.20876285257933352
iteration : 3433
train acc:  0.734375
train loss:  0.4806065559387207
train gradient:  0.12945313532599675
iteration : 3434
train acc:  0.640625
train loss:  0.6134402751922607
train gradient:  0.16340584089591492
iteration : 3435
train acc:  0.75
train loss:  0.4758937656879425
train gradient:  0.14484122482602063
iteration : 3436
train acc:  0.71875
train loss:  0.5412303805351257
train gradient:  0.19036830214922026
iteration : 3437
train acc:  0.7265625
train loss:  0.5326714515686035
train gradient:  0.1495454904551701
iteration : 3438
train acc:  0.7265625
train loss:  0.564541757106781
train gradient:  0.2157036860889176
iteration : 3439
train acc:  0.7109375
train loss:  0.5515705943107605
train gradient:  0.13324926145783045
iteration : 3440
train acc:  0.6484375
train loss:  0.5831106901168823
train gradient:  0.14272544693678085
iteration : 3441
train acc:  0.796875
train loss:  0.4705638885498047
train gradient:  0.11583523603703087
iteration : 3442
train acc:  0.7265625
train loss:  0.5227508544921875
train gradient:  0.14510742734462775
iteration : 3443
train acc:  0.6953125
train loss:  0.5412296056747437
train gradient:  0.20695030048830365
iteration : 3444
train acc:  0.7734375
train loss:  0.48638319969177246
train gradient:  0.14031215028791794
iteration : 3445
train acc:  0.7578125
train loss:  0.48149675130844116
train gradient:  0.13854177295251008
iteration : 3446
train acc:  0.6953125
train loss:  0.5580783486366272
train gradient:  0.21671505383188297
iteration : 3447
train acc:  0.6640625
train loss:  0.5593496561050415
train gradient:  0.19106051466107815
iteration : 3448
train acc:  0.6875
train loss:  0.5325748920440674
train gradient:  0.22408295300165906
iteration : 3449
train acc:  0.7265625
train loss:  0.5070452094078064
train gradient:  0.13518128897537135
iteration : 3450
train acc:  0.6640625
train loss:  0.6048063635826111
train gradient:  0.16947248522612185
iteration : 3451
train acc:  0.7734375
train loss:  0.47905489802360535
train gradient:  0.13589901503287066
iteration : 3452
train acc:  0.7265625
train loss:  0.5212963223457336
train gradient:  0.17498907203200792
iteration : 3453
train acc:  0.703125
train loss:  0.5458506941795349
train gradient:  0.17345140867084746
iteration : 3454
train acc:  0.7109375
train loss:  0.5527374148368835
train gradient:  0.1767171000620243
iteration : 3455
train acc:  0.7734375
train loss:  0.4941745400428772
train gradient:  0.13130798136685218
iteration : 3456
train acc:  0.7578125
train loss:  0.5214481353759766
train gradient:  0.16871516499454953
iteration : 3457
train acc:  0.78125
train loss:  0.5198109149932861
train gradient:  0.1545758400005588
iteration : 3458
train acc:  0.7734375
train loss:  0.4574117660522461
train gradient:  0.14156057844662653
iteration : 3459
train acc:  0.71875
train loss:  0.5401507616043091
train gradient:  0.1757045653524708
iteration : 3460
train acc:  0.703125
train loss:  0.5225837230682373
train gradient:  0.18869501159518431
iteration : 3461
train acc:  0.6953125
train loss:  0.5262967944145203
train gradient:  0.21823095354381522
iteration : 3462
train acc:  0.640625
train loss:  0.6046909093856812
train gradient:  0.19750208964941612
iteration : 3463
train acc:  0.6796875
train loss:  0.5784484148025513
train gradient:  0.2015482729800122
iteration : 3464
train acc:  0.7578125
train loss:  0.5115351676940918
train gradient:  0.12586244692692639
iteration : 3465
train acc:  0.71875
train loss:  0.5207573175430298
train gradient:  0.1741706792111501
iteration : 3466
train acc:  0.75
train loss:  0.5268198251724243
train gradient:  0.19974988358901927
iteration : 3467
train acc:  0.7265625
train loss:  0.5107671022415161
train gradient:  0.15259567074329553
iteration : 3468
train acc:  0.7890625
train loss:  0.458804190158844
train gradient:  0.11288852895636588
iteration : 3469
train acc:  0.71875
train loss:  0.5248304605484009
train gradient:  0.16420956214642707
iteration : 3470
train acc:  0.734375
train loss:  0.47965627908706665
train gradient:  0.11251376264286214
iteration : 3471
train acc:  0.71875
train loss:  0.5312447547912598
train gradient:  0.13387692728255057
iteration : 3472
train acc:  0.71875
train loss:  0.532984733581543
train gradient:  0.164448932134581
iteration : 3473
train acc:  0.75
train loss:  0.4829396605491638
train gradient:  0.15154117404248935
iteration : 3474
train acc:  0.6953125
train loss:  0.5235147476196289
train gradient:  0.1450386804323548
iteration : 3475
train acc:  0.6953125
train loss:  0.5199667811393738
train gradient:  0.18670691746647866
iteration : 3476
train acc:  0.65625
train loss:  0.6336709856987
train gradient:  0.2584548936746431
iteration : 3477
train acc:  0.7578125
train loss:  0.48273155093193054
train gradient:  0.1254525208725406
iteration : 3478
train acc:  0.7890625
train loss:  0.4774937629699707
train gradient:  0.13460472208655763
iteration : 3479
train acc:  0.75
train loss:  0.5329844951629639
train gradient:  0.15951803982040244
iteration : 3480
train acc:  0.734375
train loss:  0.4936653971672058
train gradient:  0.14327278227609508
iteration : 3481
train acc:  0.671875
train loss:  0.5794591307640076
train gradient:  0.16732721829944477
iteration : 3482
train acc:  0.7734375
train loss:  0.4933876395225525
train gradient:  0.13671676994986126
iteration : 3483
train acc:  0.6953125
train loss:  0.5619364976882935
train gradient:  0.15277757597861275
iteration : 3484
train acc:  0.75
train loss:  0.567094087600708
train gradient:  0.16550517474811705
iteration : 3485
train acc:  0.71875
train loss:  0.556698203086853
train gradient:  0.14156547909488804
iteration : 3486
train acc:  0.7421875
train loss:  0.5165413618087769
train gradient:  0.15127543049232678
iteration : 3487
train acc:  0.7734375
train loss:  0.5385737419128418
train gradient:  0.15022026120939685
iteration : 3488
train acc:  0.6796875
train loss:  0.539526641368866
train gradient:  0.1854533023374741
iteration : 3489
train acc:  0.6953125
train loss:  0.5136170387268066
train gradient:  0.18484383599205007
iteration : 3490
train acc:  0.703125
train loss:  0.5843584537506104
train gradient:  0.19453018957029172
iteration : 3491
train acc:  0.6953125
train loss:  0.5701695680618286
train gradient:  0.1728555778549723
iteration : 3492
train acc:  0.6953125
train loss:  0.5651313066482544
train gradient:  0.1896667877431451
iteration : 3493
train acc:  0.6328125
train loss:  0.6468287706375122
train gradient:  0.19930495237294787
iteration : 3494
train acc:  0.7421875
train loss:  0.5000804662704468
train gradient:  0.1421507747178642
iteration : 3495
train acc:  0.7421875
train loss:  0.5329338908195496
train gradient:  0.14131689118689955
iteration : 3496
train acc:  0.796875
train loss:  0.45920637249946594
train gradient:  0.13492670120431913
iteration : 3497
train acc:  0.6953125
train loss:  0.5349957346916199
train gradient:  0.14671639736216116
iteration : 3498
train acc:  0.6875
train loss:  0.5997175574302673
train gradient:  0.21371480891062256
iteration : 3499
train acc:  0.5859375
train loss:  0.6567991971969604
train gradient:  0.19225046805308788
iteration : 3500
train acc:  0.71875
train loss:  0.5370811223983765
train gradient:  0.18283012263215234
iteration : 3501
train acc:  0.7109375
train loss:  0.5946616530418396
train gradient:  0.1681172482051479
iteration : 3502
train acc:  0.703125
train loss:  0.5596141815185547
train gradient:  0.16241991807566114
iteration : 3503
train acc:  0.671875
train loss:  0.5463979244232178
train gradient:  0.1282898790772984
iteration : 3504
train acc:  0.765625
train loss:  0.47329938411712646
train gradient:  0.10522397330700665
iteration : 3505
train acc:  0.7109375
train loss:  0.525894045829773
train gradient:  0.14876694587824385
iteration : 3506
train acc:  0.640625
train loss:  0.590606689453125
train gradient:  0.168603306439632
iteration : 3507
train acc:  0.71875
train loss:  0.5662398934364319
train gradient:  0.19821412416766057
iteration : 3508
train acc:  0.703125
train loss:  0.5300883054733276
train gradient:  0.1577973655996052
iteration : 3509
train acc:  0.6796875
train loss:  0.5716776847839355
train gradient:  0.19168448216060813
iteration : 3510
train acc:  0.71875
train loss:  0.5707229375839233
train gradient:  0.18320838689550284
iteration : 3511
train acc:  0.6953125
train loss:  0.534831166267395
train gradient:  0.20860608735121433
iteration : 3512
train acc:  0.7578125
train loss:  0.47721901535987854
train gradient:  0.10222416202015373
iteration : 3513
train acc:  0.75
train loss:  0.4992172420024872
train gradient:  0.1513306504555294
iteration : 3514
train acc:  0.75
train loss:  0.5498838424682617
train gradient:  0.23655240620052154
iteration : 3515
train acc:  0.6875
train loss:  0.5589191317558289
train gradient:  0.1706689818071555
iteration : 3516
train acc:  0.6796875
train loss:  0.5203659534454346
train gradient:  0.14330708065999664
iteration : 3517
train acc:  0.7265625
train loss:  0.5203500986099243
train gradient:  0.16841188202430973
iteration : 3518
train acc:  0.71875
train loss:  0.5302132368087769
train gradient:  0.1506460347592921
iteration : 3519
train acc:  0.7578125
train loss:  0.4998173415660858
train gradient:  0.11947587255769847
iteration : 3520
train acc:  0.734375
train loss:  0.5436643362045288
train gradient:  0.14558956864917807
iteration : 3521
train acc:  0.7890625
train loss:  0.49828040599823
train gradient:  0.12472978984004213
iteration : 3522
train acc:  0.7109375
train loss:  0.5618950128555298
train gradient:  0.17069862774968547
iteration : 3523
train acc:  0.6875
train loss:  0.5762283205986023
train gradient:  0.20255493731699975
iteration : 3524
train acc:  0.6953125
train loss:  0.516707718372345
train gradient:  0.13517534508734347
iteration : 3525
train acc:  0.7109375
train loss:  0.5069899559020996
train gradient:  0.1615471164668163
iteration : 3526
train acc:  0.8046875
train loss:  0.48778998851776123
train gradient:  0.12954202676567395
iteration : 3527
train acc:  0.765625
train loss:  0.4695854187011719
train gradient:  0.11071908687078123
iteration : 3528
train acc:  0.71875
train loss:  0.5271599888801575
train gradient:  0.1544939292921238
iteration : 3529
train acc:  0.7109375
train loss:  0.5545861721038818
train gradient:  0.13916325746500752
iteration : 3530
train acc:  0.75
train loss:  0.4887385666370392
train gradient:  0.14594581178613894
iteration : 3531
train acc:  0.6875
train loss:  0.5956687331199646
train gradient:  0.1737637784171444
iteration : 3532
train acc:  0.75
train loss:  0.4961296319961548
train gradient:  0.11822162227763623
iteration : 3533
train acc:  0.7109375
train loss:  0.5203366279602051
train gradient:  0.12591807841598207
iteration : 3534
train acc:  0.625
train loss:  0.6020709276199341
train gradient:  0.16108147192974903
iteration : 3535
train acc:  0.75
train loss:  0.5290613174438477
train gradient:  0.20323140754895375
iteration : 3536
train acc:  0.7265625
train loss:  0.5223062038421631
train gradient:  0.13807006146498213
iteration : 3537
train acc:  0.765625
train loss:  0.512279748916626
train gradient:  0.15485065966581837
iteration : 3538
train acc:  0.7265625
train loss:  0.5125665664672852
train gradient:  0.1357221710193918
iteration : 3539
train acc:  0.71875
train loss:  0.5624688863754272
train gradient:  0.16478458180350455
iteration : 3540
train acc:  0.6796875
train loss:  0.5496639013290405
train gradient:  0.16767245322365887
iteration : 3541
train acc:  0.75
train loss:  0.531033992767334
train gradient:  0.15819641420524697
iteration : 3542
train acc:  0.7265625
train loss:  0.5021145939826965
train gradient:  0.13456480968293388
iteration : 3543
train acc:  0.65625
train loss:  0.618272066116333
train gradient:  0.19124899454048044
iteration : 3544
train acc:  0.65625
train loss:  0.580110490322113
train gradient:  0.15293860428153422
iteration : 3545
train acc:  0.6328125
train loss:  0.6412719488143921
train gradient:  0.2193563590243795
iteration : 3546
train acc:  0.59375
train loss:  0.5755144953727722
train gradient:  0.19536682599484995
iteration : 3547
train acc:  0.7421875
train loss:  0.5003161430358887
train gradient:  0.1205611722004076
iteration : 3548
train acc:  0.6953125
train loss:  0.555008053779602
train gradient:  0.14771721904439872
iteration : 3549
train acc:  0.7578125
train loss:  0.5207106471061707
train gradient:  0.14463060379291987
iteration : 3550
train acc:  0.7265625
train loss:  0.5208202004432678
train gradient:  0.12449342111759783
iteration : 3551
train acc:  0.7734375
train loss:  0.4592531621456146
train gradient:  0.11833569119308572
iteration : 3552
train acc:  0.71875
train loss:  0.5723206400871277
train gradient:  0.2067387050911263
iteration : 3553
train acc:  0.7265625
train loss:  0.5814911127090454
train gradient:  0.19107750309925425
iteration : 3554
train acc:  0.671875
train loss:  0.600415050983429
train gradient:  0.14810073820408798
iteration : 3555
train acc:  0.6953125
train loss:  0.5471903681755066
train gradient:  0.16421357021504313
iteration : 3556
train acc:  0.65625
train loss:  0.587255597114563
train gradient:  0.14988276289957192
iteration : 3557
train acc:  0.71875
train loss:  0.5605035424232483
train gradient:  0.1940813919659638
iteration : 3558
train acc:  0.8046875
train loss:  0.4466707706451416
train gradient:  0.09700392927506218
iteration : 3559
train acc:  0.703125
train loss:  0.5252567529678345
train gradient:  0.1482500269879176
iteration : 3560
train acc:  0.7109375
train loss:  0.4975849390029907
train gradient:  0.13251402868727025
iteration : 3561
train acc:  0.7109375
train loss:  0.5129518508911133
train gradient:  0.147943815827144
iteration : 3562
train acc:  0.734375
train loss:  0.4830158054828644
train gradient:  0.20002864964947187
iteration : 3563
train acc:  0.8125
train loss:  0.4644528031349182
train gradient:  0.13833928296167175
iteration : 3564
train acc:  0.671875
train loss:  0.5774881839752197
train gradient:  0.16476527840049793
iteration : 3565
train acc:  0.71875
train loss:  0.5274418592453003
train gradient:  0.1544444608328019
iteration : 3566
train acc:  0.6953125
train loss:  0.5451560020446777
train gradient:  0.12351715684629526
iteration : 3567
train acc:  0.8046875
train loss:  0.489344984292984
train gradient:  0.1643396964437681
iteration : 3568
train acc:  0.734375
train loss:  0.509428858757019
train gradient:  0.12063080869156026
iteration : 3569
train acc:  0.75
train loss:  0.46386659145355225
train gradient:  0.09618714186910363
iteration : 3570
train acc:  0.765625
train loss:  0.5229542255401611
train gradient:  0.1567646947828482
iteration : 3571
train acc:  0.6484375
train loss:  0.5738729238510132
train gradient:  0.1242060310672381
iteration : 3572
train acc:  0.7578125
train loss:  0.5436950922012329
train gradient:  0.12288010332466821
iteration : 3573
train acc:  0.6953125
train loss:  0.5310047268867493
train gradient:  0.1368882122593218
iteration : 3574
train acc:  0.6875
train loss:  0.5091454982757568
train gradient:  0.13919321355326794
iteration : 3575
train acc:  0.796875
train loss:  0.5343598127365112
train gradient:  0.18870087628350438
iteration : 3576
train acc:  0.7421875
train loss:  0.558259129524231
train gradient:  0.1519625013484539
iteration : 3577
train acc:  0.6796875
train loss:  0.5340337753295898
train gradient:  0.12780010953028265
iteration : 3578
train acc:  0.703125
train loss:  0.5179733037948608
train gradient:  0.16931807159619447
iteration : 3579
train acc:  0.6875
train loss:  0.5711802244186401
train gradient:  0.16144641108158797
iteration : 3580
train acc:  0.8203125
train loss:  0.47604191303253174
train gradient:  0.12133096015522532
iteration : 3581
train acc:  0.734375
train loss:  0.5124459266662598
train gradient:  0.16775639082440708
iteration : 3582
train acc:  0.7265625
train loss:  0.4957546889781952
train gradient:  0.12971206341341607
iteration : 3583
train acc:  0.6875
train loss:  0.4996366500854492
train gradient:  0.15599632918091877
iteration : 3584
train acc:  0.7421875
train loss:  0.5056025981903076
train gradient:  0.13562555273102547
iteration : 3585
train acc:  0.703125
train loss:  0.5397411584854126
train gradient:  0.19447830336086613
iteration : 3586
train acc:  0.6953125
train loss:  0.5481683015823364
train gradient:  0.1747627163405685
iteration : 3587
train acc:  0.7265625
train loss:  0.498136043548584
train gradient:  0.12171063568895685
iteration : 3588
train acc:  0.7890625
train loss:  0.47518178820610046
train gradient:  0.10722424378581462
iteration : 3589
train acc:  0.6796875
train loss:  0.6003538370132446
train gradient:  0.2040230881353442
iteration : 3590
train acc:  0.671875
train loss:  0.5550150275230408
train gradient:  0.16352820582913719
iteration : 3591
train acc:  0.6875
train loss:  0.5131137371063232
train gradient:  0.11820557955266087
iteration : 3592
train acc:  0.7265625
train loss:  0.5053727626800537
train gradient:  0.13056121842797525
iteration : 3593
train acc:  0.75
train loss:  0.5155243873596191
train gradient:  0.13861147684003106
iteration : 3594
train acc:  0.6875
train loss:  0.565548837184906
train gradient:  0.14930910463292116
iteration : 3595
train acc:  0.6953125
train loss:  0.5460780262947083
train gradient:  0.16069134939955343
iteration : 3596
train acc:  0.7890625
train loss:  0.4915911555290222
train gradient:  0.11702071946112691
iteration : 3597
train acc:  0.71875
train loss:  0.5685235261917114
train gradient:  0.21527241291240928
iteration : 3598
train acc:  0.640625
train loss:  0.5661277770996094
train gradient:  0.16622499599392943
iteration : 3599
train acc:  0.796875
train loss:  0.468071311712265
train gradient:  0.12911995824415773
iteration : 3600
train acc:  0.7578125
train loss:  0.4618595540523529
train gradient:  0.10321951126606106
iteration : 3601
train acc:  0.7265625
train loss:  0.494718074798584
train gradient:  0.19176514992370913
iteration : 3602
train acc:  0.6875
train loss:  0.5778414607048035
train gradient:  0.2107614846285009
iteration : 3603
train acc:  0.8125
train loss:  0.4539834260940552
train gradient:  0.10361058880871706
iteration : 3604
train acc:  0.71875
train loss:  0.5369826555252075
train gradient:  0.17462489815707435
iteration : 3605
train acc:  0.71875
train loss:  0.49363279342651367
train gradient:  0.11687221469615626
iteration : 3606
train acc:  0.6953125
train loss:  0.4992036819458008
train gradient:  0.13333497627346608
iteration : 3607
train acc:  0.703125
train loss:  0.5482380986213684
train gradient:  0.16561852677378636
iteration : 3608
train acc:  0.6953125
train loss:  0.5343989133834839
train gradient:  0.17986186003849425
iteration : 3609
train acc:  0.7421875
train loss:  0.5439378023147583
train gradient:  0.14111530162678781
iteration : 3610
train acc:  0.734375
train loss:  0.5013829469680786
train gradient:  0.13812702380881509
iteration : 3611
train acc:  0.71875
train loss:  0.5381357073783875
train gradient:  0.14147114577990944
iteration : 3612
train acc:  0.7265625
train loss:  0.5266034007072449
train gradient:  0.1445988365178279
iteration : 3613
train acc:  0.734375
train loss:  0.52884840965271
train gradient:  0.2019188774692095
iteration : 3614
train acc:  0.6953125
train loss:  0.5773459076881409
train gradient:  0.14469368883761524
iteration : 3615
train acc:  0.71875
train loss:  0.58726966381073
train gradient:  0.1894662372019547
iteration : 3616
train acc:  0.6953125
train loss:  0.5439939498901367
train gradient:  0.17496311006743442
iteration : 3617
train acc:  0.625
train loss:  0.5991795063018799
train gradient:  0.17324062805907975
iteration : 3618
train acc:  0.765625
train loss:  0.5101597309112549
train gradient:  0.15271625137420886
iteration : 3619
train acc:  0.6875
train loss:  0.5519838333129883
train gradient:  0.14251040638482754
iteration : 3620
train acc:  0.7421875
train loss:  0.5662026405334473
train gradient:  0.19583188184590206
iteration : 3621
train acc:  0.6875
train loss:  0.5562665462493896
train gradient:  0.1529842079458716
iteration : 3622
train acc:  0.75
train loss:  0.4921395480632782
train gradient:  0.12804167608284348
iteration : 3623
train acc:  0.671875
train loss:  0.5529027581214905
train gradient:  0.13769335782375047
iteration : 3624
train acc:  0.7265625
train loss:  0.5277137756347656
train gradient:  0.14892997854489404
iteration : 3625
train acc:  0.6796875
train loss:  0.5564018487930298
train gradient:  0.15901745573388015
iteration : 3626
train acc:  0.7421875
train loss:  0.5154699683189392
train gradient:  0.14383937128599739
iteration : 3627
train acc:  0.765625
train loss:  0.48546740412712097
train gradient:  0.1314894036352583
iteration : 3628
train acc:  0.75
train loss:  0.49091458320617676
train gradient:  0.11414457848077894
iteration : 3629
train acc:  0.6953125
train loss:  0.5132221579551697
train gradient:  0.11885029692187517
iteration : 3630
train acc:  0.6953125
train loss:  0.5132880210876465
train gradient:  0.14875831823329339
iteration : 3631
train acc:  0.703125
train loss:  0.522960901260376
train gradient:  0.18587454631280864
iteration : 3632
train acc:  0.6328125
train loss:  0.6067239046096802
train gradient:  0.21729857188487672
iteration : 3633
train acc:  0.734375
train loss:  0.5266060829162598
train gradient:  0.13081874033648377
iteration : 3634
train acc:  0.734375
train loss:  0.5417044758796692
train gradient:  0.1486667676218837
iteration : 3635
train acc:  0.7421875
train loss:  0.515076756477356
train gradient:  0.11995332558898576
iteration : 3636
train acc:  0.7421875
train loss:  0.5313585996627808
train gradient:  0.15174734654340832
iteration : 3637
train acc:  0.765625
train loss:  0.46187490224838257
train gradient:  0.1117794185058321
iteration : 3638
train acc:  0.6796875
train loss:  0.5412132740020752
train gradient:  0.11719769300031091
iteration : 3639
train acc:  0.7265625
train loss:  0.5004616379737854
train gradient:  0.14150702277671007
iteration : 3640
train acc:  0.71875
train loss:  0.5316852331161499
train gradient:  0.14959810104263258
iteration : 3641
train acc:  0.7734375
train loss:  0.5154461860656738
train gradient:  0.15283277909341192
iteration : 3642
train acc:  0.7421875
train loss:  0.5062761306762695
train gradient:  0.14720049371141963
iteration : 3643
train acc:  0.734375
train loss:  0.5166915059089661
train gradient:  0.15074951048795793
iteration : 3644
train acc:  0.7265625
train loss:  0.511448860168457
train gradient:  0.14755150344467338
iteration : 3645
train acc:  0.7109375
train loss:  0.5136383771896362
train gradient:  0.16589574742229868
iteration : 3646
train acc:  0.6640625
train loss:  0.5520067811012268
train gradient:  0.29691261860989354
iteration : 3647
train acc:  0.765625
train loss:  0.5109729766845703
train gradient:  0.13903140759877736
iteration : 3648
train acc:  0.6875
train loss:  0.5809248685836792
train gradient:  0.18619143430452295
iteration : 3649
train acc:  0.7578125
train loss:  0.5026168823242188
train gradient:  0.1264035459450807
iteration : 3650
train acc:  0.6640625
train loss:  0.5710704326629639
train gradient:  0.16768406082319426
iteration : 3651
train acc:  0.671875
train loss:  0.5060286521911621
train gradient:  0.2302621945494032
iteration : 3652
train acc:  0.6796875
train loss:  0.604331374168396
train gradient:  0.17795083996306155
iteration : 3653
train acc:  0.7265625
train loss:  0.5452800393104553
train gradient:  0.22337693173275858
iteration : 3654
train acc:  0.71875
train loss:  0.5099842548370361
train gradient:  0.12384210805912982
iteration : 3655
train acc:  0.6796875
train loss:  0.5353317260742188
train gradient:  0.19187083329916832
iteration : 3656
train acc:  0.703125
train loss:  0.5464067459106445
train gradient:  0.14463097569236616
iteration : 3657
train acc:  0.7109375
train loss:  0.5035949945449829
train gradient:  0.1545508689612648
iteration : 3658
train acc:  0.703125
train loss:  0.5404400825500488
train gradient:  0.1934660573012949
iteration : 3659
train acc:  0.7109375
train loss:  0.5208532810211182
train gradient:  0.19608101649502074
iteration : 3660
train acc:  0.6953125
train loss:  0.5698685646057129
train gradient:  0.23699586573330234
iteration : 3661
train acc:  0.7109375
train loss:  0.5590097308158875
train gradient:  0.27785434108042034
iteration : 3662
train acc:  0.7265625
train loss:  0.5373704433441162
train gradient:  0.1390510063685093
iteration : 3663
train acc:  0.7109375
train loss:  0.5361677408218384
train gradient:  0.177057494019309
iteration : 3664
train acc:  0.6328125
train loss:  0.5803742408752441
train gradient:  0.16781047618676637
iteration : 3665
train acc:  0.7109375
train loss:  0.5653772354125977
train gradient:  0.17364017437471252
iteration : 3666
train acc:  0.671875
train loss:  0.5729429721832275
train gradient:  0.15338983842854492
iteration : 3667
train acc:  0.765625
train loss:  0.5040334463119507
train gradient:  0.15354726839785
iteration : 3668
train acc:  0.6875
train loss:  0.5263616442680359
train gradient:  0.13785205295565495
iteration : 3669
train acc:  0.703125
train loss:  0.5195928812026978
train gradient:  0.13313066319780814
iteration : 3670
train acc:  0.7109375
train loss:  0.4959299564361572
train gradient:  0.13310211546721293
iteration : 3671
train acc:  0.6953125
train loss:  0.5155550241470337
train gradient:  0.1549864636656422
iteration : 3672
train acc:  0.6953125
train loss:  0.5359761714935303
train gradient:  0.1377589842547534
iteration : 3673
train acc:  0.7421875
train loss:  0.49334901571273804
train gradient:  0.11734822393562766
iteration : 3674
train acc:  0.828125
train loss:  0.46499601006507874
train gradient:  0.1662592890444728
iteration : 3675
train acc:  0.7109375
train loss:  0.5519735813140869
train gradient:  0.15057008351186874
iteration : 3676
train acc:  0.7734375
train loss:  0.4983226954936981
train gradient:  0.1459736572797184
iteration : 3677
train acc:  0.734375
train loss:  0.49811673164367676
train gradient:  0.1382208209722229
iteration : 3678
train acc:  0.734375
train loss:  0.5322331786155701
train gradient:  0.12686535302808477
iteration : 3679
train acc:  0.828125
train loss:  0.44588667154312134
train gradient:  0.11040572657608418
iteration : 3680
train acc:  0.6171875
train loss:  0.5999684929847717
train gradient:  0.16920580780529187
iteration : 3681
train acc:  0.640625
train loss:  0.6442192196846008
train gradient:  0.22943670396217666
iteration : 3682
train acc:  0.703125
train loss:  0.531122624874115
train gradient:  0.1778694236787305
iteration : 3683
train acc:  0.6875
train loss:  0.5548569560050964
train gradient:  0.17459616809151393
iteration : 3684
train acc:  0.703125
train loss:  0.5159873962402344
train gradient:  0.14003399645392717
iteration : 3685
train acc:  0.7109375
train loss:  0.5534043908119202
train gradient:  0.15312652057013135
iteration : 3686
train acc:  0.671875
train loss:  0.5646064281463623
train gradient:  0.15325937907434845
iteration : 3687
train acc:  0.7734375
train loss:  0.5068058967590332
train gradient:  0.15465616332366727
iteration : 3688
train acc:  0.7890625
train loss:  0.4549061357975006
train gradient:  0.11570332628601711
iteration : 3689
train acc:  0.6875
train loss:  0.5234500169754028
train gradient:  0.1644279839567193
iteration : 3690
train acc:  0.765625
train loss:  0.4837544560432434
train gradient:  0.17977840998455852
iteration : 3691
train acc:  0.75
train loss:  0.5128911137580872
train gradient:  0.12516270698230164
iteration : 3692
train acc:  0.6953125
train loss:  0.5387769937515259
train gradient:  0.21261441391341557
iteration : 3693
train acc:  0.7890625
train loss:  0.48186826705932617
train gradient:  0.16560839761948415
iteration : 3694
train acc:  0.7421875
train loss:  0.5107355117797852
train gradient:  0.1376301860180733
iteration : 3695
train acc:  0.75
train loss:  0.46362361311912537
train gradient:  0.11068934691334914
iteration : 3696
train acc:  0.7421875
train loss:  0.501399576663971
train gradient:  0.11845181374981378
iteration : 3697
train acc:  0.75
train loss:  0.481739342212677
train gradient:  0.11848498252359936
iteration : 3698
train acc:  0.734375
train loss:  0.5588086843490601
train gradient:  0.1449206878586624
iteration : 3699
train acc:  0.75
train loss:  0.5448704957962036
train gradient:  0.14516217547278992
iteration : 3700
train acc:  0.671875
train loss:  0.5802898406982422
train gradient:  0.18230773352981477
iteration : 3701
train acc:  0.75
train loss:  0.47022509574890137
train gradient:  0.10902342960677806
iteration : 3702
train acc:  0.6953125
train loss:  0.5868279933929443
train gradient:  0.21964487361329593
iteration : 3703
train acc:  0.7265625
train loss:  0.5365160703659058
train gradient:  0.161305729785866
iteration : 3704
train acc:  0.6640625
train loss:  0.5496293902397156
train gradient:  0.13215640127980505
iteration : 3705
train acc:  0.6875
train loss:  0.536487340927124
train gradient:  0.13236863642594354
iteration : 3706
train acc:  0.6875
train loss:  0.5245914459228516
train gradient:  0.14519367413930387
iteration : 3707
train acc:  0.6953125
train loss:  0.5767092108726501
train gradient:  0.1895359227406207
iteration : 3708
train acc:  0.734375
train loss:  0.5148674249649048
train gradient:  0.1486794963883908
iteration : 3709
train acc:  0.6796875
train loss:  0.5113852024078369
train gradient:  0.1442005240807075
iteration : 3710
train acc:  0.7265625
train loss:  0.5197047591209412
train gradient:  0.11863575190513144
iteration : 3711
train acc:  0.6484375
train loss:  0.624605655670166
train gradient:  0.22996750704226762
iteration : 3712
train acc:  0.765625
train loss:  0.5094033479690552
train gradient:  0.14806826821241287
iteration : 3713
train acc:  0.734375
train loss:  0.49097108840942383
train gradient:  0.1278983872384446
iteration : 3714
train acc:  0.6875
train loss:  0.5823181867599487
train gradient:  0.15050163954277496
iteration : 3715
train acc:  0.75
train loss:  0.4667760729789734
train gradient:  0.12168387928219504
iteration : 3716
train acc:  0.7265625
train loss:  0.5103437900543213
train gradient:  0.16529589859238075
iteration : 3717
train acc:  0.6953125
train loss:  0.6029437780380249
train gradient:  0.1977198810983241
iteration : 3718
train acc:  0.6484375
train loss:  0.5545663237571716
train gradient:  0.1574967707735706
iteration : 3719
train acc:  0.7265625
train loss:  0.49182379245758057
train gradient:  0.14159052366346792
iteration : 3720
train acc:  0.703125
train loss:  0.569122314453125
train gradient:  0.14739643361037702
iteration : 3721
train acc:  0.6796875
train loss:  0.5553913712501526
train gradient:  0.13291419832687507
iteration : 3722
train acc:  0.7265625
train loss:  0.5475342273712158
train gradient:  0.1600433307969294
iteration : 3723
train acc:  0.703125
train loss:  0.584423303604126
train gradient:  0.1585865860634785
iteration : 3724
train acc:  0.6875
train loss:  0.5141283273696899
train gradient:  0.14906527833815494
iteration : 3725
train acc:  0.71875
train loss:  0.503788948059082
train gradient:  0.16150754393995603
iteration : 3726
train acc:  0.6953125
train loss:  0.5004429817199707
train gradient:  0.12936955052923044
iteration : 3727
train acc:  0.703125
train loss:  0.5319938063621521
train gradient:  0.16835235970558124
iteration : 3728
train acc:  0.7265625
train loss:  0.5074452757835388
train gradient:  0.15175262328177408
iteration : 3729
train acc:  0.734375
train loss:  0.5388877391815186
train gradient:  0.18880930050650563
iteration : 3730
train acc:  0.6953125
train loss:  0.5803585648536682
train gradient:  0.18205931050816238
iteration : 3731
train acc:  0.765625
train loss:  0.5260151624679565
train gradient:  0.15649301091699158
iteration : 3732
train acc:  0.75
train loss:  0.5070784091949463
train gradient:  0.13667146213826725
iteration : 3733
train acc:  0.7109375
train loss:  0.5520984530448914
train gradient:  0.1729725234161642
iteration : 3734
train acc:  0.671875
train loss:  0.5737097263336182
train gradient:  0.21989219438165986
iteration : 3735
train acc:  0.765625
train loss:  0.5180951356887817
train gradient:  0.12593438139928032
iteration : 3736
train acc:  0.734375
train loss:  0.5161520838737488
train gradient:  0.12817538898475883
iteration : 3737
train acc:  0.6953125
train loss:  0.5144326686859131
train gradient:  0.14871879556070552
iteration : 3738
train acc:  0.7109375
train loss:  0.565725564956665
train gradient:  0.16631569305713917
iteration : 3739
train acc:  0.6953125
train loss:  0.5249603986740112
train gradient:  0.15222753112759868
iteration : 3740
train acc:  0.703125
train loss:  0.508510172367096
train gradient:  0.1565655371985624
iteration : 3741
train acc:  0.703125
train loss:  0.5254215002059937
train gradient:  0.1590592848337323
iteration : 3742
train acc:  0.7265625
train loss:  0.5267425775527954
train gradient:  0.17123501824797227
iteration : 3743
train acc:  0.6484375
train loss:  0.6311792135238647
train gradient:  0.18613467736184716
iteration : 3744
train acc:  0.6875
train loss:  0.5386257767677307
train gradient:  0.12640401978983218
iteration : 3745
train acc:  0.765625
train loss:  0.49046266078948975
train gradient:  0.14525816186795504
iteration : 3746
train acc:  0.7265625
train loss:  0.5122890472412109
train gradient:  0.15974945946770097
iteration : 3747
train acc:  0.7265625
train loss:  0.5290710926055908
train gradient:  0.12729550990802774
iteration : 3748
train acc:  0.7109375
train loss:  0.5396765470504761
train gradient:  0.16800868977050043
iteration : 3749
train acc:  0.7421875
train loss:  0.5383825302124023
train gradient:  0.14526786298132843
iteration : 3750
train acc:  0.7421875
train loss:  0.5111978054046631
train gradient:  0.16777927719885954
iteration : 3751
train acc:  0.6640625
train loss:  0.5684711933135986
train gradient:  0.22389601711319693
iteration : 3752
train acc:  0.71875
train loss:  0.5615484714508057
train gradient:  0.1748408563606677
iteration : 3753
train acc:  0.6328125
train loss:  0.5975638628005981
train gradient:  0.2156504230646514
iteration : 3754
train acc:  0.75
train loss:  0.4698313772678375
train gradient:  0.13619047118170224
iteration : 3755
train acc:  0.6953125
train loss:  0.5441634654998779
train gradient:  0.16901894044655158
iteration : 3756
train acc:  0.765625
train loss:  0.45110875368118286
train gradient:  0.12979498488626534
iteration : 3757
train acc:  0.6953125
train loss:  0.5911081433296204
train gradient:  0.1919529112442166
iteration : 3758
train acc:  0.6953125
train loss:  0.5426220893859863
train gradient:  0.14214076006951396
iteration : 3759
train acc:  0.7578125
train loss:  0.4823622405529022
train gradient:  0.14067744069321236
iteration : 3760
train acc:  0.7421875
train loss:  0.48752376437187195
train gradient:  0.1540478109762894
iteration : 3761
train acc:  0.7265625
train loss:  0.5113247632980347
train gradient:  0.12853920950497122
iteration : 3762
train acc:  0.71875
train loss:  0.4908807873725891
train gradient:  0.13101036022582496
iteration : 3763
train acc:  0.7265625
train loss:  0.48432284593582153
train gradient:  0.14669991749762323
iteration : 3764
train acc:  0.640625
train loss:  0.6675390005111694
train gradient:  0.23090991321159476
iteration : 3765
train acc:  0.6796875
train loss:  0.5842620134353638
train gradient:  0.22620245816433704
iteration : 3766
train acc:  0.71875
train loss:  0.5084975957870483
train gradient:  0.17125240157582478
iteration : 3767
train acc:  0.6875
train loss:  0.577970027923584
train gradient:  0.15473718059717873
iteration : 3768
train acc:  0.7734375
train loss:  0.4920491576194763
train gradient:  0.1194545148176941
iteration : 3769
train acc:  0.7421875
train loss:  0.5591410994529724
train gradient:  0.137022094910034
iteration : 3770
train acc:  0.71875
train loss:  0.5111920833587646
train gradient:  0.13492974106864106
iteration : 3771
train acc:  0.75
train loss:  0.5033890604972839
train gradient:  0.15775551182220576
iteration : 3772
train acc:  0.65625
train loss:  0.6105477213859558
train gradient:  0.2034553608731226
iteration : 3773
train acc:  0.7109375
train loss:  0.5246957540512085
train gradient:  0.11401567138846033
iteration : 3774
train acc:  0.7421875
train loss:  0.5487149953842163
train gradient:  0.14784063759718336
iteration : 3775
train acc:  0.640625
train loss:  0.5519351959228516
train gradient:  0.17849264510056523
iteration : 3776
train acc:  0.7109375
train loss:  0.5413212776184082
train gradient:  0.18687933608201052
iteration : 3777
train acc:  0.6640625
train loss:  0.5756995677947998
train gradient:  0.21684523594543254
iteration : 3778
train acc:  0.6640625
train loss:  0.5347475409507751
train gradient:  0.20529297890338705
iteration : 3779
train acc:  0.75
train loss:  0.5091840028762817
train gradient:  0.1605073051085681
iteration : 3780
train acc:  0.7109375
train loss:  0.5242676734924316
train gradient:  0.18321924321482042
iteration : 3781
train acc:  0.65625
train loss:  0.5705100297927856
train gradient:  0.15597252013887372
iteration : 3782
train acc:  0.75
train loss:  0.46117764711380005
train gradient:  0.1019370462114822
iteration : 3783
train acc:  0.734375
train loss:  0.535185694694519
train gradient:  0.17133826757663162
iteration : 3784
train acc:  0.7265625
train loss:  0.5672638416290283
train gradient:  0.14452535224978974
iteration : 3785
train acc:  0.71875
train loss:  0.5148630738258362
train gradient:  0.1585217610395595
iteration : 3786
train acc:  0.671875
train loss:  0.6001405715942383
train gradient:  0.20705883905867015
iteration : 3787
train acc:  0.671875
train loss:  0.5871996879577637
train gradient:  0.17795825130077608
iteration : 3788
train acc:  0.7265625
train loss:  0.5214000940322876
train gradient:  0.12634464041118565
iteration : 3789
train acc:  0.6484375
train loss:  0.5778390169143677
train gradient:  0.14254790927971456
iteration : 3790
train acc:  0.71875
train loss:  0.532929539680481
train gradient:  0.1287902746890977
iteration : 3791
train acc:  0.65625
train loss:  0.5629109144210815
train gradient:  0.16123386495520725
iteration : 3792
train acc:  0.765625
train loss:  0.4916577935218811
train gradient:  0.13919102674237516
iteration : 3793
train acc:  0.75
train loss:  0.45695123076438904
train gradient:  0.12450699096008677
iteration : 3794
train acc:  0.796875
train loss:  0.5092509984970093
train gradient:  0.1952641935079537
iteration : 3795
train acc:  0.703125
train loss:  0.5391051173210144
train gradient:  0.19670031772637558
iteration : 3796
train acc:  0.75
train loss:  0.48090019822120667
train gradient:  0.11882227402245775
iteration : 3797
train acc:  0.6171875
train loss:  0.6100606322288513
train gradient:  0.24970839004652368
iteration : 3798
train acc:  0.7890625
train loss:  0.4654143750667572
train gradient:  0.12054882377461221
iteration : 3799
train acc:  0.6953125
train loss:  0.5074781775474548
train gradient:  0.11810226360479723
iteration : 3800
train acc:  0.765625
train loss:  0.4843665659427643
train gradient:  0.1049172743478254
iteration : 3801
train acc:  0.703125
train loss:  0.5539833307266235
train gradient:  0.18863245445376653
iteration : 3802
train acc:  0.6328125
train loss:  0.6289095282554626
train gradient:  0.2837200769403276
iteration : 3803
train acc:  0.6875
train loss:  0.542320966720581
train gradient:  0.18864994633224746
iteration : 3804
train acc:  0.609375
train loss:  0.6132872104644775
train gradient:  0.19104551352699856
iteration : 3805
train acc:  0.75
train loss:  0.4952905774116516
train gradient:  0.12050293690475555
iteration : 3806
train acc:  0.8046875
train loss:  0.4742763042449951
train gradient:  0.1567159019079114
iteration : 3807
train acc:  0.7421875
train loss:  0.4971342086791992
train gradient:  0.15248961131842362
iteration : 3808
train acc:  0.75
train loss:  0.5103086233139038
train gradient:  0.19235659482812556
iteration : 3809
train acc:  0.703125
train loss:  0.5540138483047485
train gradient:  0.1629457571594506
iteration : 3810
train acc:  0.78125
train loss:  0.48651590943336487
train gradient:  0.1405026339030282
iteration : 3811
train acc:  0.6875
train loss:  0.5358083248138428
train gradient:  0.1675701178897453
iteration : 3812
train acc:  0.7265625
train loss:  0.5574111342430115
train gradient:  0.16667330872638914
iteration : 3813
train acc:  0.6875
train loss:  0.5460096597671509
train gradient:  0.18752973823753039
iteration : 3814
train acc:  0.7421875
train loss:  0.5547760128974915
train gradient:  0.21409012567779379
iteration : 3815
train acc:  0.6953125
train loss:  0.5258819460868835
train gradient:  0.15613552343807724
iteration : 3816
train acc:  0.734375
train loss:  0.484881728887558
train gradient:  0.130066995819492
iteration : 3817
train acc:  0.71875
train loss:  0.5230301022529602
train gradient:  0.13622435000818697
iteration : 3818
train acc:  0.734375
train loss:  0.517943263053894
train gradient:  0.12358624242318277
iteration : 3819
train acc:  0.71875
train loss:  0.6181591749191284
train gradient:  0.20664783561427696
iteration : 3820
train acc:  0.7265625
train loss:  0.5182434320449829
train gradient:  0.15272493224137113
iteration : 3821
train acc:  0.75
train loss:  0.5116251111030579
train gradient:  0.1272058583231555
iteration : 3822
train acc:  0.71875
train loss:  0.5227667093276978
train gradient:  0.12806879205934407
iteration : 3823
train acc:  0.7109375
train loss:  0.5255779027938843
train gradient:  0.14411141475305053
iteration : 3824
train acc:  0.6875
train loss:  0.5805033445358276
train gradient:  0.15695053373293644
iteration : 3825
train acc:  0.6875
train loss:  0.5206062197685242
train gradient:  0.11794100173337964
iteration : 3826
train acc:  0.7421875
train loss:  0.5069468021392822
train gradient:  0.15055663660126128
iteration : 3827
train acc:  0.765625
train loss:  0.4924047291278839
train gradient:  0.12935279049427145
iteration : 3828
train acc:  0.7265625
train loss:  0.5426822900772095
train gradient:  0.15770550509210496
iteration : 3829
train acc:  0.6875
train loss:  0.5976353883743286
train gradient:  0.19417328107127138
iteration : 3830
train acc:  0.7421875
train loss:  0.5635532140731812
train gradient:  0.16846467765626108
iteration : 3831
train acc:  0.796875
train loss:  0.4402346611022949
train gradient:  0.11135265149290181
iteration : 3832
train acc:  0.703125
train loss:  0.5343891978263855
train gradient:  0.18693020897163576
iteration : 3833
train acc:  0.671875
train loss:  0.587456464767456
train gradient:  0.17216308422348367
iteration : 3834
train acc:  0.7578125
train loss:  0.4760149419307709
train gradient:  0.13566505268364049
iteration : 3835
train acc:  0.765625
train loss:  0.4794979691505432
train gradient:  0.12946532780226946
iteration : 3836
train acc:  0.7421875
train loss:  0.528307318687439
train gradient:  0.14959554632868202
iteration : 3837
train acc:  0.75
train loss:  0.49956682324409485
train gradient:  0.16126121846926492
iteration : 3838
train acc:  0.6875
train loss:  0.5359898805618286
train gradient:  0.2027329886481268
iteration : 3839
train acc:  0.703125
train loss:  0.5281223654747009
train gradient:  0.1422559267030307
iteration : 3840
train acc:  0.7109375
train loss:  0.553751528263092
train gradient:  0.15768137164333235
iteration : 3841
train acc:  0.7578125
train loss:  0.4952225685119629
train gradient:  0.1295880452834305
iteration : 3842
train acc:  0.71875
train loss:  0.6148130297660828
train gradient:  0.24033673594446053
iteration : 3843
train acc:  0.7109375
train loss:  0.5454921722412109
train gradient:  0.1412713128594803
iteration : 3844
train acc:  0.7109375
train loss:  0.5608788728713989
train gradient:  0.18085110357119652
iteration : 3845
train acc:  0.7578125
train loss:  0.48096591234207153
train gradient:  0.20618157105542856
iteration : 3846
train acc:  0.7890625
train loss:  0.46786004304885864
train gradient:  0.12345014708561135
iteration : 3847
train acc:  0.796875
train loss:  0.4665859043598175
train gradient:  0.1631939823388172
iteration : 3848
train acc:  0.6484375
train loss:  0.6304525136947632
train gradient:  0.289721077584857
iteration : 3849
train acc:  0.7421875
train loss:  0.5187328457832336
train gradient:  0.12301948525503036
iteration : 3850
train acc:  0.65625
train loss:  0.5499789714813232
train gradient:  0.14337762531281822
iteration : 3851
train acc:  0.671875
train loss:  0.5117496848106384
train gradient:  0.1780865366477991
iteration : 3852
train acc:  0.78125
train loss:  0.5115087032318115
train gradient:  0.20938868608582145
iteration : 3853
train acc:  0.6953125
train loss:  0.5730187892913818
train gradient:  0.17575247361181015
iteration : 3854
train acc:  0.7734375
train loss:  0.44881150126457214
train gradient:  0.12146806404125438
iteration : 3855
train acc:  0.8046875
train loss:  0.4505503475666046
train gradient:  0.1074294864065803
iteration : 3856
train acc:  0.671875
train loss:  0.55906742811203
train gradient:  0.1259187171486736
iteration : 3857
train acc:  0.7109375
train loss:  0.537112295627594
train gradient:  0.14064603094059994
iteration : 3858
train acc:  0.671875
train loss:  0.5991925597190857
train gradient:  0.1547826317443692
iteration : 3859
train acc:  0.71875
train loss:  0.5005840063095093
train gradient:  0.13677401621423563
iteration : 3860
train acc:  0.7421875
train loss:  0.4828822612762451
train gradient:  0.1291339010015449
iteration : 3861
train acc:  0.71875
train loss:  0.5086276531219482
train gradient:  0.13247701264132894
iteration : 3862
train acc:  0.7421875
train loss:  0.48450011014938354
train gradient:  0.12908377998705542
iteration : 3863
train acc:  0.765625
train loss:  0.47555121779441833
train gradient:  0.10985825937329832
iteration : 3864
train acc:  0.7421875
train loss:  0.4464271366596222
train gradient:  0.1415141078327503
iteration : 3865
train acc:  0.7265625
train loss:  0.5187245607376099
train gradient:  0.18909502480260942
iteration : 3866
train acc:  0.7421875
train loss:  0.4672762453556061
train gradient:  0.14773887447209783
iteration : 3867
train acc:  0.71875
train loss:  0.5458897948265076
train gradient:  0.14428207455591507
iteration : 3868
train acc:  0.765625
train loss:  0.4763484001159668
train gradient:  0.12243632008616644
iteration : 3869
train acc:  0.8046875
train loss:  0.4487854838371277
train gradient:  0.1449971936825658
iteration : 3870
train acc:  0.71875
train loss:  0.5429158210754395
train gradient:  0.1839895536373567
iteration : 3871
train acc:  0.703125
train loss:  0.5520775318145752
train gradient:  0.15685419219196825
iteration : 3872
train acc:  0.7578125
train loss:  0.5276539921760559
train gradient:  0.13430319419832673
iteration : 3873
train acc:  0.6953125
train loss:  0.5986582040786743
train gradient:  0.1972864740895352
iteration : 3874
train acc:  0.765625
train loss:  0.49697956442832947
train gradient:  0.17430924973103845
iteration : 3875
train acc:  0.65625
train loss:  0.5438528060913086
train gradient:  0.14242686177278102
iteration : 3876
train acc:  0.765625
train loss:  0.5121152997016907
train gradient:  0.14333811285107523
iteration : 3877
train acc:  0.7421875
train loss:  0.5271422863006592
train gradient:  0.13498565017132103
iteration : 3878
train acc:  0.6875
train loss:  0.5606971979141235
train gradient:  0.14342431848566892
iteration : 3879
train acc:  0.7421875
train loss:  0.5471076965332031
train gradient:  0.12202580980383437
iteration : 3880
train acc:  0.6953125
train loss:  0.5461685657501221
train gradient:  0.16941604023656726
iteration : 3881
train acc:  0.7578125
train loss:  0.4537949562072754
train gradient:  0.12023602369581612
iteration : 3882
train acc:  0.734375
train loss:  0.5864717960357666
train gradient:  0.18406201358082466
iteration : 3883
train acc:  0.796875
train loss:  0.46949800848960876
train gradient:  0.12247743261068655
iteration : 3884
train acc:  0.71875
train loss:  0.5294919013977051
train gradient:  0.1303721605262646
iteration : 3885
train acc:  0.7578125
train loss:  0.4785493016242981
train gradient:  0.11815672208306714
iteration : 3886
train acc:  0.7109375
train loss:  0.5329083204269409
train gradient:  0.24179404843129704
iteration : 3887
train acc:  0.7578125
train loss:  0.511485755443573
train gradient:  0.1293182227001644
iteration : 3888
train acc:  0.7421875
train loss:  0.5325964689254761
train gradient:  0.20012820942281442
iteration : 3889
train acc:  0.6953125
train loss:  0.51719069480896
train gradient:  0.1465949280661435
iteration : 3890
train acc:  0.7109375
train loss:  0.5005104541778564
train gradient:  0.11590326234307465
iteration : 3891
train acc:  0.765625
train loss:  0.4797912538051605
train gradient:  0.10572661017016614
iteration : 3892
train acc:  0.7890625
train loss:  0.45377877354621887
train gradient:  0.1370383495319628
iteration : 3893
train acc:  0.7421875
train loss:  0.5027868747711182
train gradient:  0.12152066156115793
iteration : 3894
train acc:  0.78125
train loss:  0.4478401839733124
train gradient:  0.10971368716084307
iteration : 3895
train acc:  0.765625
train loss:  0.5138739347457886
train gradient:  0.14652062559599846
iteration : 3896
train acc:  0.75
train loss:  0.4904922842979431
train gradient:  0.12788820426511263
iteration : 3897
train acc:  0.734375
train loss:  0.5687641501426697
train gradient:  0.2607767480031535
iteration : 3898
train acc:  0.6953125
train loss:  0.5168566107749939
train gradient:  0.1438790844840026
iteration : 3899
train acc:  0.6796875
train loss:  0.5869688987731934
train gradient:  0.21035116959734818
iteration : 3900
train acc:  0.71875
train loss:  0.5217752456665039
train gradient:  0.10972711231157628
iteration : 3901
train acc:  0.765625
train loss:  0.4797307848930359
train gradient:  0.13015286112622315
iteration : 3902
train acc:  0.671875
train loss:  0.5741873979568481
train gradient:  0.15817127442456627
iteration : 3903
train acc:  0.6875
train loss:  0.5146124362945557
train gradient:  0.1536045076743377
iteration : 3904
train acc:  0.7421875
train loss:  0.49509966373443604
train gradient:  0.15524268357419158
iteration : 3905
train acc:  0.6953125
train loss:  0.5198557376861572
train gradient:  0.14442081729335451
iteration : 3906
train acc:  0.734375
train loss:  0.5287268161773682
train gradient:  0.17932868691608023
iteration : 3907
train acc:  0.7421875
train loss:  0.507912278175354
train gradient:  0.12892258488403036
iteration : 3908
train acc:  0.71875
train loss:  0.49575456976890564
train gradient:  0.12664623634899774
iteration : 3909
train acc:  0.6953125
train loss:  0.5550691485404968
train gradient:  0.13024883537830362
iteration : 3910
train acc:  0.7109375
train loss:  0.5244698524475098
train gradient:  0.12696600126548954
iteration : 3911
train acc:  0.7421875
train loss:  0.47816038131713867
train gradient:  0.13075965847871177
iteration : 3912
train acc:  0.71875
train loss:  0.5276513695716858
train gradient:  0.14742025278790172
iteration : 3913
train acc:  0.6953125
train loss:  0.5625833868980408
train gradient:  0.20288345433407906
iteration : 3914
train acc:  0.7734375
train loss:  0.4998099207878113
train gradient:  0.13002293067834272
iteration : 3915
train acc:  0.7265625
train loss:  0.48034173250198364
train gradient:  0.11383096913701475
iteration : 3916
train acc:  0.71875
train loss:  0.5484535098075867
train gradient:  0.19604870684952233
iteration : 3917
train acc:  0.6796875
train loss:  0.5227707028388977
train gradient:  0.16265569582733896
iteration : 3918
train acc:  0.7578125
train loss:  0.4659726619720459
train gradient:  0.16173556435338188
iteration : 3919
train acc:  0.7734375
train loss:  0.5116597414016724
train gradient:  0.1616794205994018
iteration : 3920
train acc:  0.765625
train loss:  0.4680469036102295
train gradient:  0.1299172196665522
iteration : 3921
train acc:  0.78125
train loss:  0.4779239892959595
train gradient:  0.14763720038635092
iteration : 3922
train acc:  0.7578125
train loss:  0.47276532649993896
train gradient:  0.15735393534070458
iteration : 3923
train acc:  0.703125
train loss:  0.5091855525970459
train gradient:  0.12186506546371553
iteration : 3924
train acc:  0.7890625
train loss:  0.4818006753921509
train gradient:  0.1502500600451518
iteration : 3925
train acc:  0.765625
train loss:  0.5065325498580933
train gradient:  0.21528795102771447
iteration : 3926
train acc:  0.703125
train loss:  0.5810463428497314
train gradient:  0.23737294368896117
iteration : 3927
train acc:  0.75
train loss:  0.4989059567451477
train gradient:  0.13688618571995545
iteration : 3928
train acc:  0.7890625
train loss:  0.4600038528442383
train gradient:  0.09939586927199741
iteration : 3929
train acc:  0.734375
train loss:  0.537407398223877
train gradient:  0.18944651801347395
iteration : 3930
train acc:  0.71875
train loss:  0.49681025743484497
train gradient:  0.14001221410880205
iteration : 3931
train acc:  0.6953125
train loss:  0.5367822647094727
train gradient:  0.21360992701747195
iteration : 3932
train acc:  0.6875
train loss:  0.5842058062553406
train gradient:  0.1580126279094673
iteration : 3933
train acc:  0.7890625
train loss:  0.46097490191459656
train gradient:  0.15849017216399536
iteration : 3934
train acc:  0.7421875
train loss:  0.5170366764068604
train gradient:  0.1399132627201134
iteration : 3935
train acc:  0.703125
train loss:  0.528503954410553
train gradient:  0.16315993173489343
iteration : 3936
train acc:  0.71875
train loss:  0.5155583620071411
train gradient:  0.11387334432652348
iteration : 3937
train acc:  0.703125
train loss:  0.5242534875869751
train gradient:  0.14718959030090803
iteration : 3938
train acc:  0.7734375
train loss:  0.4655319154262543
train gradient:  0.11260042990147669
iteration : 3939
train acc:  0.734375
train loss:  0.5378665924072266
train gradient:  0.15425877829828588
iteration : 3940
train acc:  0.703125
train loss:  0.5252606272697449
train gradient:  0.17249751382924572
iteration : 3941
train acc:  0.6796875
train loss:  0.619157612323761
train gradient:  0.22683072638261056
iteration : 3942
train acc:  0.71875
train loss:  0.5174436569213867
train gradient:  0.185243063943909
iteration : 3943
train acc:  0.671875
train loss:  0.5489157438278198
train gradient:  0.14853842814738513
iteration : 3944
train acc:  0.6796875
train loss:  0.5689133405685425
train gradient:  0.17657604931971782
iteration : 3945
train acc:  0.7734375
train loss:  0.45697250962257385
train gradient:  0.1221751772194297
iteration : 3946
train acc:  0.7578125
train loss:  0.48168402910232544
train gradient:  0.12694747218543517
iteration : 3947
train acc:  0.7578125
train loss:  0.49414950609207153
train gradient:  0.12107749898649985
iteration : 3948
train acc:  0.765625
train loss:  0.48997020721435547
train gradient:  0.12614151117627093
iteration : 3949
train acc:  0.7421875
train loss:  0.5099331140518188
train gradient:  0.13988035428248707
iteration : 3950
train acc:  0.71875
train loss:  0.5589699149131775
train gradient:  0.18133954294910487
iteration : 3951
train acc:  0.71875
train loss:  0.5505359768867493
train gradient:  0.16492454761747466
iteration : 3952
train acc:  0.703125
train loss:  0.5202451944351196
train gradient:  0.21469444429458745
iteration : 3953
train acc:  0.6875
train loss:  0.5405276417732239
train gradient:  0.21743207580725238
iteration : 3954
train acc:  0.6640625
train loss:  0.5678120255470276
train gradient:  0.19472072864220663
iteration : 3955
train acc:  0.78125
train loss:  0.45161962509155273
train gradient:  0.15155615841787284
iteration : 3956
train acc:  0.7265625
train loss:  0.5034900903701782
train gradient:  0.14570727115068066
iteration : 3957
train acc:  0.7265625
train loss:  0.4998026490211487
train gradient:  0.14704505291174086
iteration : 3958
train acc:  0.75
train loss:  0.5112813115119934
train gradient:  0.14249912439749002
iteration : 3959
train acc:  0.6484375
train loss:  0.5832562446594238
train gradient:  0.22440162846931183
iteration : 3960
train acc:  0.6953125
train loss:  0.5153123140335083
train gradient:  0.17410617728157185
iteration : 3961
train acc:  0.6875
train loss:  0.5247918367385864
train gradient:  0.19995930169042514
iteration : 3962
train acc:  0.75
train loss:  0.4903053939342499
train gradient:  0.12327020783768929
iteration : 3963
train acc:  0.7578125
train loss:  0.46297454833984375
train gradient:  0.16614768751345305
iteration : 3964
train acc:  0.7109375
train loss:  0.5341582894325256
train gradient:  0.22018341041192296
iteration : 3965
train acc:  0.7265625
train loss:  0.4896799921989441
train gradient:  0.13952629382138088
iteration : 3966
train acc:  0.65625
train loss:  0.614305853843689
train gradient:  0.1921581492794182
iteration : 3967
train acc:  0.71875
train loss:  0.5695679783821106
train gradient:  0.14925704446075191
iteration : 3968
train acc:  0.734375
train loss:  0.5210423469543457
train gradient:  0.10898439676290628
iteration : 3969
train acc:  0.7109375
train loss:  0.5392608046531677
train gradient:  0.1445975141877892
iteration : 3970
train acc:  0.6953125
train loss:  0.5698899030685425
train gradient:  0.2306775773948906
iteration : 3971
train acc:  0.6953125
train loss:  0.5584065318107605
train gradient:  0.2068942691832572
iteration : 3972
train acc:  0.6875
train loss:  0.547415554523468
train gradient:  0.13474049963589382
iteration : 3973
train acc:  0.7109375
train loss:  0.5215295553207397
train gradient:  0.13293012979670427
iteration : 3974
train acc:  0.703125
train loss:  0.5308797955513
train gradient:  0.1321795270611614
iteration : 3975
train acc:  0.7421875
train loss:  0.5287586450576782
train gradient:  0.15272600673312958
iteration : 3976
train acc:  0.7109375
train loss:  0.4912949204444885
train gradient:  0.14184382042361615
iteration : 3977
train acc:  0.7578125
train loss:  0.5254353284835815
train gradient:  0.1759741030609489
iteration : 3978
train acc:  0.7265625
train loss:  0.5023777484893799
train gradient:  0.1799096673392285
iteration : 3979
train acc:  0.75
train loss:  0.5015163421630859
train gradient:  0.13580695324886793
iteration : 3980
train acc:  0.734375
train loss:  0.5074132680892944
train gradient:  0.15189028217620482
iteration : 3981
train acc:  0.75
train loss:  0.5258941054344177
train gradient:  0.20234167169927464
iteration : 3982
train acc:  0.703125
train loss:  0.5893233418464661
train gradient:  0.18659627357669378
iteration : 3983
train acc:  0.7109375
train loss:  0.5255935192108154
train gradient:  0.15079876914113965
iteration : 3984
train acc:  0.6328125
train loss:  0.6085605621337891
train gradient:  0.207788712082707
iteration : 3985
train acc:  0.6796875
train loss:  0.545892596244812
train gradient:  0.1683351313070124
iteration : 3986
train acc:  0.65625
train loss:  0.6523085236549377
train gradient:  0.20723049864471088
iteration : 3987
train acc:  0.65625
train loss:  0.5915459394454956
train gradient:  0.18264524985788863
iteration : 3988
train acc:  0.7265625
train loss:  0.5646920204162598
train gradient:  0.1966912787074403
iteration : 3989
train acc:  0.7578125
train loss:  0.5017340183258057
train gradient:  0.11702126992019893
iteration : 3990
train acc:  0.7578125
train loss:  0.5127019286155701
train gradient:  0.14201523370625205
iteration : 3991
train acc:  0.71875
train loss:  0.5063615441322327
train gradient:  0.20991644884933536
iteration : 3992
train acc:  0.734375
train loss:  0.4990321695804596
train gradient:  0.13960242378003757
iteration : 3993
train acc:  0.7421875
train loss:  0.519287109375
train gradient:  0.14919654966062293
iteration : 3994
train acc:  0.734375
train loss:  0.5167123079299927
train gradient:  0.1505142709937008
iteration : 3995
train acc:  0.703125
train loss:  0.5348119139671326
train gradient:  0.16526389463827357
iteration : 3996
train acc:  0.7421875
train loss:  0.5192089080810547
train gradient:  0.13988062935126672
iteration : 3997
train acc:  0.7578125
train loss:  0.47566747665405273
train gradient:  0.13982379266145595
iteration : 3998
train acc:  0.734375
train loss:  0.5130070447921753
train gradient:  0.1497541834905964
iteration : 3999
train acc:  0.78125
train loss:  0.5115550756454468
train gradient:  0.15357535385618515
iteration : 4000
train acc:  0.6796875
train loss:  0.5439141988754272
train gradient:  0.1519916421145625
iteration : 4001
train acc:  0.640625
train loss:  0.624132513999939
train gradient:  0.18519591492822926
iteration : 4002
train acc:  0.765625
train loss:  0.4867905378341675
train gradient:  0.1701128501233823
iteration : 4003
train acc:  0.7578125
train loss:  0.5052448511123657
train gradient:  0.19590003498132064
iteration : 4004
train acc:  0.6640625
train loss:  0.5874227285385132
train gradient:  0.21311749704443905
iteration : 4005
train acc:  0.7109375
train loss:  0.5065535306930542
train gradient:  0.15042987879830516
iteration : 4006
train acc:  0.6875
train loss:  0.5574458837509155
train gradient:  0.18673505118559883
iteration : 4007
train acc:  0.7421875
train loss:  0.5142837762832642
train gradient:  0.12065599461242489
iteration : 4008
train acc:  0.71875
train loss:  0.5251730680465698
train gradient:  0.15347724204923788
iteration : 4009
train acc:  0.671875
train loss:  0.5693886876106262
train gradient:  0.1729701503947187
iteration : 4010
train acc:  0.6640625
train loss:  0.5754939317703247
train gradient:  0.1817456495041136
iteration : 4011
train acc:  0.6328125
train loss:  0.5955541729927063
train gradient:  0.1790260940851784
iteration : 4012
train acc:  0.7109375
train loss:  0.5400233268737793
train gradient:  0.13795768618412896
iteration : 4013
train acc:  0.6796875
train loss:  0.5939404368400574
train gradient:  0.15142181427862153
iteration : 4014
train acc:  0.703125
train loss:  0.5070961713790894
train gradient:  0.13115948944398337
iteration : 4015
train acc:  0.7734375
train loss:  0.4666818380355835
train gradient:  0.13834187561969957
iteration : 4016
train acc:  0.7734375
train loss:  0.48691749572753906
train gradient:  0.11943044814965027
iteration : 4017
train acc:  0.7421875
train loss:  0.5213130116462708
train gradient:  0.16352761903036694
iteration : 4018
train acc:  0.734375
train loss:  0.5032402873039246
train gradient:  0.1353241549791696
iteration : 4019
train acc:  0.78125
train loss:  0.4950220286846161
train gradient:  0.14718118530402433
iteration : 4020
train acc:  0.765625
train loss:  0.527678370475769
train gradient:  0.13209178085733692
iteration : 4021
train acc:  0.703125
train loss:  0.543000340461731
train gradient:  0.14508762926009416
iteration : 4022
train acc:  0.734375
train loss:  0.5248759388923645
train gradient:  0.15436007943101737
iteration : 4023
train acc:  0.703125
train loss:  0.5281208753585815
train gradient:  0.15439786455454774
iteration : 4024
train acc:  0.7109375
train loss:  0.5295858383178711
train gradient:  0.13903503443319737
iteration : 4025
train acc:  0.7265625
train loss:  0.5085092782974243
train gradient:  0.15536975860981247
iteration : 4026
train acc:  0.6953125
train loss:  0.5647127032279968
train gradient:  0.2161179578570693
iteration : 4027
train acc:  0.734375
train loss:  0.5161702632904053
train gradient:  0.15997540263683935
iteration : 4028
train acc:  0.6953125
train loss:  0.6062352657318115
train gradient:  0.18138754920316782
iteration : 4029
train acc:  0.7421875
train loss:  0.5051127076148987
train gradient:  0.1199841928759817
iteration : 4030
train acc:  0.703125
train loss:  0.5371116399765015
train gradient:  0.1367491023597147
iteration : 4031
train acc:  0.6640625
train loss:  0.602755606174469
train gradient:  0.19533812115206878
iteration : 4032
train acc:  0.6796875
train loss:  0.5969472527503967
train gradient:  0.24071839347527818
iteration : 4033
train acc:  0.71875
train loss:  0.502041220664978
train gradient:  0.16355992413736742
iteration : 4034
train acc:  0.75
train loss:  0.5336820483207703
train gradient:  0.1905997476885075
iteration : 4035
train acc:  0.71875
train loss:  0.5417487621307373
train gradient:  0.1503069418505909
iteration : 4036
train acc:  0.7421875
train loss:  0.5207224488258362
train gradient:  0.18132137692850125
iteration : 4037
train acc:  0.6640625
train loss:  0.6360388994216919
train gradient:  0.21890885037209382
iteration : 4038
train acc:  0.6796875
train loss:  0.4980832636356354
train gradient:  0.13023482824492089
iteration : 4039
train acc:  0.6640625
train loss:  0.5435953140258789
train gradient:  0.20627198748412384
iteration : 4040
train acc:  0.765625
train loss:  0.5177744030952454
train gradient:  0.15493679572273664
iteration : 4041
train acc:  0.7578125
train loss:  0.5254913568496704
train gradient:  0.1571022856920401
iteration : 4042
train acc:  0.734375
train loss:  0.506091833114624
train gradient:  0.13787746221291258
iteration : 4043
train acc:  0.7109375
train loss:  0.5227917432785034
train gradient:  0.12931274725970987
iteration : 4044
train acc:  0.6484375
train loss:  0.5666948556900024
train gradient:  0.19465601417327894
iteration : 4045
train acc:  0.75
train loss:  0.4868881404399872
train gradient:  0.15241912842968
iteration : 4046
train acc:  0.71875
train loss:  0.5319477319717407
train gradient:  0.16011148644940154
iteration : 4047
train acc:  0.71875
train loss:  0.567775547504425
train gradient:  0.209896285069679
iteration : 4048
train acc:  0.7578125
train loss:  0.5307718515396118
train gradient:  0.12324502321496622
iteration : 4049
train acc:  0.6640625
train loss:  0.5636014938354492
train gradient:  0.16430436619855676
iteration : 4050
train acc:  0.75
train loss:  0.49780887365341187
train gradient:  0.17864363507458283
iteration : 4051
train acc:  0.7109375
train loss:  0.5558722019195557
train gradient:  0.1744666615275018
iteration : 4052
train acc:  0.6640625
train loss:  0.6042442917823792
train gradient:  0.2266360435563487
iteration : 4053
train acc:  0.6875
train loss:  0.5438277721405029
train gradient:  0.17191637880930036
iteration : 4054
train acc:  0.7578125
train loss:  0.5116449594497681
train gradient:  0.14186023474478468
iteration : 4055
train acc:  0.71875
train loss:  0.5314785242080688
train gradient:  0.1551901135214442
iteration : 4056
train acc:  0.65625
train loss:  0.567521333694458
train gradient:  0.16933553343787502
iteration : 4057
train acc:  0.7890625
train loss:  0.4996300935745239
train gradient:  0.19708482137054714
iteration : 4058
train acc:  0.65625
train loss:  0.6249078512191772
train gradient:  0.23020013325920968
iteration : 4059
train acc:  0.7734375
train loss:  0.49865537881851196
train gradient:  0.13436094678574173
iteration : 4060
train acc:  0.6640625
train loss:  0.5902101397514343
train gradient:  0.15580967334426715
iteration : 4061
train acc:  0.6796875
train loss:  0.5608581304550171
train gradient:  0.15653125816104457
iteration : 4062
train acc:  0.6640625
train loss:  0.5747151970863342
train gradient:  0.17847011604486254
iteration : 4063
train acc:  0.6640625
train loss:  0.5409444570541382
train gradient:  0.13079742182827558
iteration : 4064
train acc:  0.7265625
train loss:  0.49744856357574463
train gradient:  0.13097993454158086
iteration : 4065
train acc:  0.734375
train loss:  0.536227822303772
train gradient:  0.16693502440957395
iteration : 4066
train acc:  0.6796875
train loss:  0.5781144499778748
train gradient:  0.24417184541659753
iteration : 4067
train acc:  0.7265625
train loss:  0.516055703163147
train gradient:  0.1285920577554439
iteration : 4068
train acc:  0.6171875
train loss:  0.5863180160522461
train gradient:  0.2265054435800382
iteration : 4069
train acc:  0.71875
train loss:  0.5373890995979309
train gradient:  0.23103914350339227
iteration : 4070
train acc:  0.7421875
train loss:  0.5501033663749695
train gradient:  0.13706651041504955
iteration : 4071
train acc:  0.78125
train loss:  0.4466203451156616
train gradient:  0.09933904500962354
iteration : 4072
train acc:  0.734375
train loss:  0.5043075680732727
train gradient:  0.16461835181551993
iteration : 4073
train acc:  0.65625
train loss:  0.5831562280654907
train gradient:  0.19203764551853394
iteration : 4074
train acc:  0.7421875
train loss:  0.5029001235961914
train gradient:  0.12564237630557346
iteration : 4075
train acc:  0.7265625
train loss:  0.49743369221687317
train gradient:  0.17317076576957185
iteration : 4076
train acc:  0.71875
train loss:  0.5371943712234497
train gradient:  0.14214107071647422
iteration : 4077
train acc:  0.75
train loss:  0.5048672556877136
train gradient:  0.18506233592333587
iteration : 4078
train acc:  0.765625
train loss:  0.4824730157852173
train gradient:  0.14257321811609552
iteration : 4079
train acc:  0.75
train loss:  0.5328919887542725
train gradient:  0.1343206102718535
iteration : 4080
train acc:  0.71875
train loss:  0.4958573579788208
train gradient:  0.21111450034015367
iteration : 4081
train acc:  0.75
train loss:  0.4981197714805603
train gradient:  0.13602392624835452
iteration : 4082
train acc:  0.71875
train loss:  0.472800612449646
train gradient:  0.1279194096400143
iteration : 4083
train acc:  0.6796875
train loss:  0.5488830804824829
train gradient:  0.16088627654400808
iteration : 4084
train acc:  0.6328125
train loss:  0.5471841096878052
train gradient:  0.1677130911877951
iteration : 4085
train acc:  0.703125
train loss:  0.5616575479507446
train gradient:  0.13749598333438912
iteration : 4086
train acc:  0.7734375
train loss:  0.5099459886550903
train gradient:  0.1652811769977207
iteration : 4087
train acc:  0.6796875
train loss:  0.5818052887916565
train gradient:  0.17053157317709577
iteration : 4088
train acc:  0.671875
train loss:  0.616195797920227
train gradient:  0.20620810140153223
iteration : 4089
train acc:  0.75
train loss:  0.4700186550617218
train gradient:  0.11501673351150016
iteration : 4090
train acc:  0.7578125
train loss:  0.5009655952453613
train gradient:  0.12997305348024502
iteration : 4091
train acc:  0.8046875
train loss:  0.4716452360153198
train gradient:  0.10958796856831458
iteration : 4092
train acc:  0.7421875
train loss:  0.5016548037528992
train gradient:  0.1630973990515738
iteration : 4093
train acc:  0.7421875
train loss:  0.4780762493610382
train gradient:  0.1342901008177358
iteration : 4094
train acc:  0.7421875
train loss:  0.5235865712165833
train gradient:  0.17295016775837724
iteration : 4095
train acc:  0.6796875
train loss:  0.5101999044418335
train gradient:  0.12374892050942629
iteration : 4096
train acc:  0.7109375
train loss:  0.5273562669754028
train gradient:  0.16045807157463948
iteration : 4097
train acc:  0.8203125
train loss:  0.42978090047836304
train gradient:  0.11063395048834389
iteration : 4098
train acc:  0.734375
train loss:  0.5000923871994019
train gradient:  0.2255092606146037
iteration : 4099
train acc:  0.7109375
train loss:  0.5439862012863159
train gradient:  0.27767712206285683
iteration : 4100
train acc:  0.65625
train loss:  0.6196324229240417
train gradient:  0.1598857432953539
iteration : 4101
train acc:  0.6484375
train loss:  0.5990355610847473
train gradient:  0.2118255974821877
iteration : 4102
train acc:  0.7578125
train loss:  0.4586004316806793
train gradient:  0.10994852901276067
iteration : 4103
train acc:  0.78125
train loss:  0.4806988537311554
train gradient:  0.12856750378265927
iteration : 4104
train acc:  0.71875
train loss:  0.5871703624725342
train gradient:  0.21982034184499744
iteration : 4105
train acc:  0.6796875
train loss:  0.5473211407661438
train gradient:  0.1507806021282445
iteration : 4106
train acc:  0.703125
train loss:  0.5344030261039734
train gradient:  0.1570838369469308
iteration : 4107
train acc:  0.703125
train loss:  0.5596328377723694
train gradient:  0.13504227157998414
iteration : 4108
train acc:  0.671875
train loss:  0.5780771970748901
train gradient:  0.19338237843818112
iteration : 4109
train acc:  0.7734375
train loss:  0.5241149663925171
train gradient:  0.1805798359933353
iteration : 4110
train acc:  0.703125
train loss:  0.5471853017807007
train gradient:  0.17410729190825344
iteration : 4111
train acc:  0.6328125
train loss:  0.5908869504928589
train gradient:  0.16464635397142008
iteration : 4112
train acc:  0.765625
train loss:  0.46632736921310425
train gradient:  0.13714815368639247
iteration : 4113
train acc:  0.703125
train loss:  0.5282092094421387
train gradient:  0.19881507760307765
iteration : 4114
train acc:  0.6640625
train loss:  0.5715644359588623
train gradient:  0.24565761352910287
iteration : 4115
train acc:  0.734375
train loss:  0.49766889214515686
train gradient:  0.15993212310919946
iteration : 4116
train acc:  0.640625
train loss:  0.5728158354759216
train gradient:  0.17588555287193608
iteration : 4117
train acc:  0.7890625
train loss:  0.45157256722450256
train gradient:  0.1683162616387722
iteration : 4118
train acc:  0.71875
train loss:  0.515999436378479
train gradient:  0.16342226887617056
iteration : 4119
train acc:  0.6484375
train loss:  0.5662646293640137
train gradient:  0.1750822975722011
iteration : 4120
train acc:  0.7578125
train loss:  0.46996021270751953
train gradient:  0.11838671713305671
iteration : 4121
train acc:  0.6796875
train loss:  0.5428791046142578
train gradient:  0.25596003340599705
iteration : 4122
train acc:  0.734375
train loss:  0.5466129779815674
train gradient:  0.1326335012603132
iteration : 4123
train acc:  0.78125
train loss:  0.455438494682312
train gradient:  0.18223769374483606
iteration : 4124
train acc:  0.7265625
train loss:  0.5139961838722229
train gradient:  0.1894191432970732
iteration : 4125
train acc:  0.75
train loss:  0.48699235916137695
train gradient:  0.14695092365924056
iteration : 4126
train acc:  0.7578125
train loss:  0.4667191505432129
train gradient:  0.1137135711696339
iteration : 4127
train acc:  0.7421875
train loss:  0.47187328338623047
train gradient:  0.12159380767390378
iteration : 4128
train acc:  0.7734375
train loss:  0.4617358446121216
train gradient:  0.09922323102060723
iteration : 4129
train acc:  0.75
train loss:  0.5425702333450317
train gradient:  0.12294290575660032
iteration : 4130
train acc:  0.7734375
train loss:  0.4806579053401947
train gradient:  0.1394613866457814
iteration : 4131
train acc:  0.7734375
train loss:  0.5094146728515625
train gradient:  0.13874882377446918
iteration : 4132
train acc:  0.71875
train loss:  0.5349515080451965
train gradient:  0.13119031411538068
iteration : 4133
train acc:  0.671875
train loss:  0.5749059915542603
train gradient:  0.1800608049912653
iteration : 4134
train acc:  0.65625
train loss:  0.5401612520217896
train gradient:  0.18528228967901117
iteration : 4135
train acc:  0.7109375
train loss:  0.5175892114639282
train gradient:  0.13388443414769283
iteration : 4136
train acc:  0.78125
train loss:  0.48511436581611633
train gradient:  0.15230410622807986
iteration : 4137
train acc:  0.7734375
train loss:  0.5107458829879761
train gradient:  0.13900009382669246
iteration : 4138
train acc:  0.71875
train loss:  0.5413200259208679
train gradient:  0.13857365200438732
iteration : 4139
train acc:  0.7734375
train loss:  0.4561740756034851
train gradient:  0.0898310289377265
iteration : 4140
train acc:  0.7265625
train loss:  0.5039059519767761
train gradient:  0.1443867373678115
iteration : 4141
train acc:  0.796875
train loss:  0.4537705183029175
train gradient:  0.09324208581509143
iteration : 4142
train acc:  0.7734375
train loss:  0.5489689111709595
train gradient:  0.16768266344438282
iteration : 4143
train acc:  0.734375
train loss:  0.5258880853652954
train gradient:  0.16717012499710687
iteration : 4144
train acc:  0.7421875
train loss:  0.47894608974456787
train gradient:  0.14782012721898202
iteration : 4145
train acc:  0.75
train loss:  0.514549195766449
train gradient:  0.1177208810049528
iteration : 4146
train acc:  0.734375
train loss:  0.5050158500671387
train gradient:  0.14706394632026193
iteration : 4147
train acc:  0.7109375
train loss:  0.5377079248428345
train gradient:  0.20313397342701672
iteration : 4148
train acc:  0.6640625
train loss:  0.5631930828094482
train gradient:  0.20708306160989573
iteration : 4149
train acc:  0.7265625
train loss:  0.5542468428611755
train gradient:  0.14213551526571813
iteration : 4150
train acc:  0.71875
train loss:  0.5366702079772949
train gradient:  0.15185497623570915
iteration : 4151
train acc:  0.7890625
train loss:  0.443600058555603
train gradient:  0.17212399383819793
iteration : 4152
train acc:  0.6484375
train loss:  0.5782743692398071
train gradient:  0.14215487662368154
iteration : 4153
train acc:  0.734375
train loss:  0.4966552257537842
train gradient:  0.11858478350919502
iteration : 4154
train acc:  0.8046875
train loss:  0.4419223368167877
train gradient:  0.11367304321506194
iteration : 4155
train acc:  0.7265625
train loss:  0.5386331081390381
train gradient:  0.11959904786208246
iteration : 4156
train acc:  0.75
train loss:  0.498653382062912
train gradient:  0.147743422179767
iteration : 4157
train acc:  0.6953125
train loss:  0.515854001045227
train gradient:  0.19002829730714055
iteration : 4158
train acc:  0.734375
train loss:  0.5092759728431702
train gradient:  0.12328915109616789
iteration : 4159
train acc:  0.6796875
train loss:  0.5877771377563477
train gradient:  0.19961156146548803
iteration : 4160
train acc:  0.71875
train loss:  0.5174432992935181
train gradient:  0.16118562629054145
iteration : 4161
train acc:  0.8203125
train loss:  0.5030189752578735
train gradient:  0.13658243841384654
iteration : 4162
train acc:  0.671875
train loss:  0.5416934490203857
train gradient:  0.16418307529486878
iteration : 4163
train acc:  0.7890625
train loss:  0.4731729328632355
train gradient:  0.10151125502566521
iteration : 4164
train acc:  0.7421875
train loss:  0.4708418548107147
train gradient:  0.1334638081199655
iteration : 4165
train acc:  0.71875
train loss:  0.5642844438552856
train gradient:  0.19480046821316438
iteration : 4166
train acc:  0.75
train loss:  0.5359044671058655
train gradient:  0.1435174595117715
iteration : 4167
train acc:  0.6875
train loss:  0.531644344329834
train gradient:  0.16008682209597835
iteration : 4168
train acc:  0.6953125
train loss:  0.523112952709198
train gradient:  0.17019158957573066
iteration : 4169
train acc:  0.6875
train loss:  0.5916324853897095
train gradient:  0.2035372665242523
iteration : 4170
train acc:  0.7578125
train loss:  0.4774359464645386
train gradient:  0.11940139065199891
iteration : 4171
train acc:  0.734375
train loss:  0.4845468997955322
train gradient:  0.1278137479252111
iteration : 4172
train acc:  0.71875
train loss:  0.5475910305976868
train gradient:  0.16152888511958235
iteration : 4173
train acc:  0.78125
train loss:  0.4369887113571167
train gradient:  0.14878457119967103
iteration : 4174
train acc:  0.7109375
train loss:  0.5244947075843811
train gradient:  0.14475267175547196
iteration : 4175
train acc:  0.7109375
train loss:  0.5332786440849304
train gradient:  0.14319415756428
iteration : 4176
train acc:  0.78125
train loss:  0.4983813464641571
train gradient:  0.12369714299700667
iteration : 4177
train acc:  0.6953125
train loss:  0.5550130605697632
train gradient:  0.16138420939150472
iteration : 4178
train acc:  0.7421875
train loss:  0.49019497632980347
train gradient:  0.13179235086861482
iteration : 4179
train acc:  0.7109375
train loss:  0.5540226697921753
train gradient:  0.17209305228179028
iteration : 4180
train acc:  0.7734375
train loss:  0.5264954566955566
train gradient:  0.15139831903448664
iteration : 4181
train acc:  0.609375
train loss:  0.673333466053009
train gradient:  0.3942394839316037
iteration : 4182
train acc:  0.7265625
train loss:  0.5453810095787048
train gradient:  0.14099689943871133
iteration : 4183
train acc:  0.6796875
train loss:  0.5859639644622803
train gradient:  0.21916686883099984
iteration : 4184
train acc:  0.765625
train loss:  0.447944700717926
train gradient:  0.1668715854953574
iteration : 4185
train acc:  0.6484375
train loss:  0.5950337648391724
train gradient:  0.17530129866611432
iteration : 4186
train acc:  0.75
train loss:  0.5010149478912354
train gradient:  0.1544945248174877
iteration : 4187
train acc:  0.71875
train loss:  0.5356636047363281
train gradient:  0.12427981778017719
iteration : 4188
train acc:  0.6640625
train loss:  0.6305232048034668
train gradient:  0.21865841251582485
iteration : 4189
train acc:  0.6953125
train loss:  0.5151007175445557
train gradient:  0.1576262433005124
iteration : 4190
train acc:  0.7265625
train loss:  0.5006740093231201
train gradient:  0.13670629826186725
iteration : 4191
train acc:  0.703125
train loss:  0.5445947647094727
train gradient:  0.16651308227804423
iteration : 4192
train acc:  0.6953125
train loss:  0.5590577125549316
train gradient:  0.1834700343653966
iteration : 4193
train acc:  0.6875
train loss:  0.5643237829208374
train gradient:  0.1663310275924367
iteration : 4194
train acc:  0.703125
train loss:  0.5270754098892212
train gradient:  0.21103010737696837
iteration : 4195
train acc:  0.7109375
train loss:  0.522743284702301
train gradient:  0.1615027793758464
iteration : 4196
train acc:  0.75
train loss:  0.539212703704834
train gradient:  0.18104457953187708
iteration : 4197
train acc:  0.734375
train loss:  0.5142679810523987
train gradient:  0.13762344294375675
iteration : 4198
train acc:  0.671875
train loss:  0.5523056387901306
train gradient:  0.13194605021969513
iteration : 4199
train acc:  0.6484375
train loss:  0.5703580379486084
train gradient:  0.14768240092088053
iteration : 4200
train acc:  0.78125
train loss:  0.4736372232437134
train gradient:  0.14135393087616147
iteration : 4201
train acc:  0.7265625
train loss:  0.5135968923568726
train gradient:  0.12421894119853708
iteration : 4202
train acc:  0.75
train loss:  0.4947810769081116
train gradient:  0.12629685964442422
iteration : 4203
train acc:  0.671875
train loss:  0.6161053776741028
train gradient:  0.2065663589149807
iteration : 4204
train acc:  0.7421875
train loss:  0.5318572521209717
train gradient:  0.17493368822466443
iteration : 4205
train acc:  0.7109375
train loss:  0.5412435531616211
train gradient:  0.16804956897112502
iteration : 4206
train acc:  0.7109375
train loss:  0.5859977602958679
train gradient:  0.19156604329182375
iteration : 4207
train acc:  0.71875
train loss:  0.5278670787811279
train gradient:  0.13650626816549855
iteration : 4208
train acc:  0.734375
train loss:  0.536920428276062
train gradient:  0.15008962447308483
iteration : 4209
train acc:  0.7265625
train loss:  0.4992411434650421
train gradient:  0.11605241843701206
iteration : 4210
train acc:  0.7109375
train loss:  0.5475649833679199
train gradient:  0.16004606464204163
iteration : 4211
train acc:  0.6953125
train loss:  0.5302647352218628
train gradient:  0.13261061679420896
iteration : 4212
train acc:  0.6875
train loss:  0.5708944797515869
train gradient:  0.152252744012338
iteration : 4213
train acc:  0.6796875
train loss:  0.5809003114700317
train gradient:  0.21062963946002333
iteration : 4214
train acc:  0.703125
train loss:  0.5310811996459961
train gradient:  0.18301242162138817
iteration : 4215
train acc:  0.796875
train loss:  0.466454416513443
train gradient:  0.19524386407544808
iteration : 4216
train acc:  0.734375
train loss:  0.47990408539772034
train gradient:  0.14421674115948568
iteration : 4217
train acc:  0.734375
train loss:  0.5256356596946716
train gradient:  0.1563643427607646
iteration : 4218
train acc:  0.765625
train loss:  0.4789566397666931
train gradient:  0.12352372108443527
iteration : 4219
train acc:  0.78125
train loss:  0.46216750144958496
train gradient:  0.10912031642350477
iteration : 4220
train acc:  0.671875
train loss:  0.5338414311408997
train gradient:  0.19773103954456617
iteration : 4221
train acc:  0.7109375
train loss:  0.5504447221755981
train gradient:  0.1410057761596359
iteration : 4222
train acc:  0.6953125
train loss:  0.5117834806442261
train gradient:  0.20983736592537733
iteration : 4223
train acc:  0.6796875
train loss:  0.5611972212791443
train gradient:  0.16596896770438588
iteration : 4224
train acc:  0.703125
train loss:  0.5312191843986511
train gradient:  0.13764529167669617
iteration : 4225
train acc:  0.6875
train loss:  0.5166586637496948
train gradient:  0.2540943501385238
iteration : 4226
train acc:  0.6953125
train loss:  0.5296297073364258
train gradient:  0.19014606108602986
iteration : 4227
train acc:  0.7265625
train loss:  0.5520266890525818
train gradient:  0.1767552509574044
iteration : 4228
train acc:  0.7421875
train loss:  0.4831565320491791
train gradient:  0.11746273584752343
iteration : 4229
train acc:  0.7734375
train loss:  0.49438077211380005
train gradient:  0.12538636011609258
iteration : 4230
train acc:  0.765625
train loss:  0.5027071833610535
train gradient:  0.14561046186282267
iteration : 4231
train acc:  0.7109375
train loss:  0.5246390104293823
train gradient:  0.1709440407996729
iteration : 4232
train acc:  0.7890625
train loss:  0.45323410630226135
train gradient:  0.16800980946069333
iteration : 4233
train acc:  0.6484375
train loss:  0.6046162247657776
train gradient:  0.17515412192922386
iteration : 4234
train acc:  0.703125
train loss:  0.5531667470932007
train gradient:  0.1489964918976341
iteration : 4235
train acc:  0.765625
train loss:  0.44507256150245667
train gradient:  0.1364921830830814
iteration : 4236
train acc:  0.71875
train loss:  0.5501844882965088
train gradient:  0.15904914413161034
iteration : 4237
train acc:  0.6953125
train loss:  0.5569429397583008
train gradient:  0.18782622873819982
iteration : 4238
train acc:  0.8046875
train loss:  0.47761785984039307
train gradient:  0.14780727029917412
iteration : 4239
train acc:  0.765625
train loss:  0.4759939908981323
train gradient:  0.11768876169702752
iteration : 4240
train acc:  0.671875
train loss:  0.6140742301940918
train gradient:  0.2117803759198041
iteration : 4241
train acc:  0.6953125
train loss:  0.5300887823104858
train gradient:  0.12258218290710488
iteration : 4242
train acc:  0.75
train loss:  0.48885828256607056
train gradient:  0.1257681919191989
iteration : 4243
train acc:  0.7578125
train loss:  0.4745035171508789
train gradient:  0.13009115099646967
iteration : 4244
train acc:  0.671875
train loss:  0.5559989213943481
train gradient:  0.1481246230900306
iteration : 4245
train acc:  0.71875
train loss:  0.5042364597320557
train gradient:  0.16250333198355782
iteration : 4246
train acc:  0.828125
train loss:  0.46502232551574707
train gradient:  0.27121418737906966
iteration : 4247
train acc:  0.7421875
train loss:  0.46305879950523376
train gradient:  0.11710476472954966
iteration : 4248
train acc:  0.7109375
train loss:  0.5195057392120361
train gradient:  0.17326918392339677
iteration : 4249
train acc:  0.734375
train loss:  0.49615365266799927
train gradient:  0.14778617950603684
iteration : 4250
train acc:  0.6796875
train loss:  0.5736056566238403
train gradient:  0.22363537419763338
iteration : 4251
train acc:  0.765625
train loss:  0.48334571719169617
train gradient:  0.11693736424177952
iteration : 4252
train acc:  0.7421875
train loss:  0.5121084451675415
train gradient:  0.17317493928671146
iteration : 4253
train acc:  0.7265625
train loss:  0.6085107922554016
train gradient:  0.26405897584406995
iteration : 4254
train acc:  0.78125
train loss:  0.5285741090774536
train gradient:  0.16480930184623574
iteration : 4255
train acc:  0.6640625
train loss:  0.5671008229255676
train gradient:  0.1546770365439949
iteration : 4256
train acc:  0.7421875
train loss:  0.5582818984985352
train gradient:  0.1931043253732711
iteration : 4257
train acc:  0.75
train loss:  0.5032187700271606
train gradient:  0.11220837415351334
iteration : 4258
train acc:  0.8359375
train loss:  0.44481632113456726
train gradient:  0.1378018251295812
iteration : 4259
train acc:  0.703125
train loss:  0.5648032426834106
train gradient:  0.16478274407158644
iteration : 4260
train acc:  0.734375
train loss:  0.5430973768234253
train gradient:  0.1754250454157612
iteration : 4261
train acc:  0.765625
train loss:  0.4593835771083832
train gradient:  0.13269348471139358
iteration : 4262
train acc:  0.7109375
train loss:  0.5555120706558228
train gradient:  0.13928295332461244
iteration : 4263
train acc:  0.6796875
train loss:  0.5722329616546631
train gradient:  0.16511786029305045
iteration : 4264
train acc:  0.75
train loss:  0.5393370985984802
train gradient:  0.20347658380017264
iteration : 4265
train acc:  0.6953125
train loss:  0.5495425462722778
train gradient:  0.1430117754937302
iteration : 4266
train acc:  0.7421875
train loss:  0.5336356163024902
train gradient:  0.1911122308573841
iteration : 4267
train acc:  0.71875
train loss:  0.5189404487609863
train gradient:  0.14700753312836085
iteration : 4268
train acc:  0.7109375
train loss:  0.5649209022521973
train gradient:  0.13335496402564873
iteration : 4269
train acc:  0.7734375
train loss:  0.5227921009063721
train gradient:  0.20459190268574604
iteration : 4270
train acc:  0.71875
train loss:  0.5228457450866699
train gradient:  0.14903685569821462
iteration : 4271
train acc:  0.7109375
train loss:  0.5246924757957458
train gradient:  0.1616506790699151
iteration : 4272
train acc:  0.7890625
train loss:  0.4497132897377014
train gradient:  0.11134001361902808
iteration : 4273
train acc:  0.7109375
train loss:  0.5404950380325317
train gradient:  0.18343491273207108
iteration : 4274
train acc:  0.7265625
train loss:  0.4851827621459961
train gradient:  0.13920123116369817
iteration : 4275
train acc:  0.7265625
train loss:  0.5370362401008606
train gradient:  0.17776452935716858
iteration : 4276
train acc:  0.7734375
train loss:  0.4930903911590576
train gradient:  0.1750082105175873
iteration : 4277
train acc:  0.7265625
train loss:  0.5394248962402344
train gradient:  0.1750514204061061
iteration : 4278
train acc:  0.734375
train loss:  0.5152087211608887
train gradient:  0.12376985635363012
iteration : 4279
train acc:  0.7265625
train loss:  0.5106629133224487
train gradient:  0.14153059946557767
iteration : 4280
train acc:  0.6875
train loss:  0.5249910950660706
train gradient:  0.16424923380295783
iteration : 4281
train acc:  0.703125
train loss:  0.4936017692089081
train gradient:  0.11027638445601087
iteration : 4282
train acc:  0.6953125
train loss:  0.5480700731277466
train gradient:  0.15596093766665342
iteration : 4283
train acc:  0.6953125
train loss:  0.49925756454467773
train gradient:  0.13921685702826722
iteration : 4284
train acc:  0.71875
train loss:  0.495541512966156
train gradient:  0.13511924999657998
iteration : 4285
train acc:  0.71875
train loss:  0.5639629364013672
train gradient:  0.17084574672612554
iteration : 4286
train acc:  0.6875
train loss:  0.5171651840209961
train gradient:  0.2035726827918491
iteration : 4287
train acc:  0.7421875
train loss:  0.5316392183303833
train gradient:  0.15814158954538698
iteration : 4288
train acc:  0.7734375
train loss:  0.4876861572265625
train gradient:  0.16849937614519137
iteration : 4289
train acc:  0.734375
train loss:  0.5395549535751343
train gradient:  0.1825579395394601
iteration : 4290
train acc:  0.703125
train loss:  0.5666519999504089
train gradient:  0.1775550400749139
iteration : 4291
train acc:  0.7265625
train loss:  0.48852086067199707
train gradient:  0.12143413767180089
iteration : 4292
train acc:  0.7890625
train loss:  0.4800049662590027
train gradient:  0.13734431578770226
iteration : 4293
train acc:  0.78125
train loss:  0.49800407886505127
train gradient:  0.139030437600425
iteration : 4294
train acc:  0.6953125
train loss:  0.5588626265525818
train gradient:  0.18503468151948094
iteration : 4295
train acc:  0.734375
train loss:  0.48087507486343384
train gradient:  0.16580084532061234
iteration : 4296
train acc:  0.71875
train loss:  0.5307692289352417
train gradient:  0.18391158517644712
iteration : 4297
train acc:  0.7265625
train loss:  0.5816650390625
train gradient:  0.26707461660191084
iteration : 4298
train acc:  0.734375
train loss:  0.49076688289642334
train gradient:  0.162912593251325
iteration : 4299
train acc:  0.7109375
train loss:  0.5310509204864502
train gradient:  0.15062217404391645
iteration : 4300
train acc:  0.71875
train loss:  0.5155449509620667
train gradient:  0.14255830367835426
iteration : 4301
train acc:  0.71875
train loss:  0.507575273513794
train gradient:  0.11782926033392227
iteration : 4302
train acc:  0.7421875
train loss:  0.47448331117630005
train gradient:  0.1564269608118607
iteration : 4303
train acc:  0.7109375
train loss:  0.49976494908332825
train gradient:  0.19834211549108838
iteration : 4304
train acc:  0.6484375
train loss:  0.5689551830291748
train gradient:  0.2041570988723035
iteration : 4305
train acc:  0.828125
train loss:  0.4404139518737793
train gradient:  0.11386418090045684
iteration : 4306
train acc:  0.71875
train loss:  0.4868316650390625
train gradient:  0.1412831558622947
iteration : 4307
train acc:  0.671875
train loss:  0.5789461135864258
train gradient:  0.23805624899389166
iteration : 4308
train acc:  0.765625
train loss:  0.4817567467689514
train gradient:  0.12332752120546422
iteration : 4309
train acc:  0.75
train loss:  0.5478312373161316
train gradient:  0.18449423152565475
iteration : 4310
train acc:  0.796875
train loss:  0.4787009656429291
train gradient:  0.1500663451663987
iteration : 4311
train acc:  0.71875
train loss:  0.5261446237564087
train gradient:  0.1257506180750077
iteration : 4312
train acc:  0.75
train loss:  0.49152278900146484
train gradient:  0.13699505268692405
iteration : 4313
train acc:  0.703125
train loss:  0.5267088413238525
train gradient:  0.16822051149660616
iteration : 4314
train acc:  0.6953125
train loss:  0.5466430187225342
train gradient:  0.13885264085525167
iteration : 4315
train acc:  0.734375
train loss:  0.5225402116775513
train gradient:  0.1818614104340185
iteration : 4316
train acc:  0.734375
train loss:  0.4968675673007965
train gradient:  0.1194337480528876
iteration : 4317
train acc:  0.703125
train loss:  0.5504046082496643
train gradient:  0.15100133107650615
iteration : 4318
train acc:  0.7109375
train loss:  0.5158263444900513
train gradient:  0.144816710712583
iteration : 4319
train acc:  0.7109375
train loss:  0.522914707660675
train gradient:  0.20769992547531252
iteration : 4320
train acc:  0.765625
train loss:  0.5435075163841248
train gradient:  0.2214433915793821
iteration : 4321
train acc:  0.734375
train loss:  0.5253463983535767
train gradient:  0.11904938081859277
iteration : 4322
train acc:  0.640625
train loss:  0.5701688528060913
train gradient:  0.2043489629799896
iteration : 4323
train acc:  0.7109375
train loss:  0.574969470500946
train gradient:  0.22454407116483613
iteration : 4324
train acc:  0.7265625
train loss:  0.5042999982833862
train gradient:  0.14323347104997147
iteration : 4325
train acc:  0.7109375
train loss:  0.5373973250389099
train gradient:  0.12737962662900992
iteration : 4326
train acc:  0.7265625
train loss:  0.5556422472000122
train gradient:  0.18399337910284627
iteration : 4327
train acc:  0.765625
train loss:  0.4572718143463135
train gradient:  0.1267637962703197
iteration : 4328
train acc:  0.6953125
train loss:  0.5173501968383789
train gradient:  0.13339554963351105
iteration : 4329
train acc:  0.71875
train loss:  0.554717481136322
train gradient:  0.1665083529042103
iteration : 4330
train acc:  0.640625
train loss:  0.5799716711044312
train gradient:  0.17411678812948878
iteration : 4331
train acc:  0.71875
train loss:  0.513769805431366
train gradient:  0.14646318803667763
iteration : 4332
train acc:  0.7109375
train loss:  0.54802405834198
train gradient:  0.20343486153759394
iteration : 4333
train acc:  0.734375
train loss:  0.5260950326919556
train gradient:  0.1634284648130754
iteration : 4334
train acc:  0.765625
train loss:  0.5266053676605225
train gradient:  0.16947278223547074
iteration : 4335
train acc:  0.6875
train loss:  0.4911153018474579
train gradient:  0.13463647168772142
iteration : 4336
train acc:  0.71875
train loss:  0.5895003080368042
train gradient:  0.21054579303371368
iteration : 4337
train acc:  0.71875
train loss:  0.5510863065719604
train gradient:  0.16102812222638047
iteration : 4338
train acc:  0.6640625
train loss:  0.5516127347946167
train gradient:  0.20085228712294806
iteration : 4339
train acc:  0.6796875
train loss:  0.5283643007278442
train gradient:  0.16047813953688844
iteration : 4340
train acc:  0.65625
train loss:  0.579236626625061
train gradient:  0.21483587263513637
iteration : 4341
train acc:  0.734375
train loss:  0.5431575775146484
train gradient:  0.18133846372243523
iteration : 4342
train acc:  0.75
train loss:  0.5515010356903076
train gradient:  0.15122376109755162
iteration : 4343
train acc:  0.7421875
train loss:  0.5325512290000916
train gradient:  0.148881817854773
iteration : 4344
train acc:  0.703125
train loss:  0.5352966785430908
train gradient:  0.14105376403899517
iteration : 4345
train acc:  0.6796875
train loss:  0.5619320869445801
train gradient:  0.15274688020639754
iteration : 4346
train acc:  0.7578125
train loss:  0.4467122554779053
train gradient:  0.11236272740300311
iteration : 4347
train acc:  0.703125
train loss:  0.5431747436523438
train gradient:  0.14771087516097767
iteration : 4348
train acc:  0.7109375
train loss:  0.5034754872322083
train gradient:  0.1985528295222941
iteration : 4349
train acc:  0.7578125
train loss:  0.4907136559486389
train gradient:  0.1135672135898517
iteration : 4350
train acc:  0.671875
train loss:  0.579182505607605
train gradient:  0.1822960351483789
iteration : 4351
train acc:  0.7109375
train loss:  0.551689863204956
train gradient:  0.16500481908838294
iteration : 4352
train acc:  0.7734375
train loss:  0.47094255685806274
train gradient:  0.10516882518727727
iteration : 4353
train acc:  0.7265625
train loss:  0.5234084129333496
train gradient:  0.19372698558583978
iteration : 4354
train acc:  0.71875
train loss:  0.5065752267837524
train gradient:  0.21049167651798753
iteration : 4355
train acc:  0.71875
train loss:  0.5442408919334412
train gradient:  0.14696789662808812
iteration : 4356
train acc:  0.7578125
train loss:  0.46413370966911316
train gradient:  0.12167449583670352
iteration : 4357
train acc:  0.71875
train loss:  0.5414692759513855
train gradient:  0.17283826170999683
iteration : 4358
train acc:  0.6484375
train loss:  0.6028358936309814
train gradient:  0.20676225402264276
iteration : 4359
train acc:  0.6796875
train loss:  0.574072003364563
train gradient:  0.1922012463329721
iteration : 4360
train acc:  0.65625
train loss:  0.523533046245575
train gradient:  0.1473713781215579
iteration : 4361
train acc:  0.671875
train loss:  0.5504693984985352
train gradient:  0.13415130076942655
iteration : 4362
train acc:  0.765625
train loss:  0.46137869358062744
train gradient:  0.12927027340674424
iteration : 4363
train acc:  0.734375
train loss:  0.5028896331787109
train gradient:  0.19593222997553728
iteration : 4364
train acc:  0.765625
train loss:  0.4720049500465393
train gradient:  0.1430998882631448
iteration : 4365
train acc:  0.6953125
train loss:  0.5496772527694702
train gradient:  0.18563909243927645
iteration : 4366
train acc:  0.7265625
train loss:  0.5213415622711182
train gradient:  0.1404553348453055
iteration : 4367
train acc:  0.734375
train loss:  0.5119129419326782
train gradient:  0.17272003093509408
iteration : 4368
train acc:  0.7421875
train loss:  0.5400469303131104
train gradient:  0.12233109722103061
iteration : 4369
train acc:  0.71875
train loss:  0.47899049520492554
train gradient:  0.1313956849998112
iteration : 4370
train acc:  0.703125
train loss:  0.5668663382530212
train gradient:  0.15418175499890435
iteration : 4371
train acc:  0.7109375
train loss:  0.6028026342391968
train gradient:  0.1478544889581288
iteration : 4372
train acc:  0.6796875
train loss:  0.5994224548339844
train gradient:  0.201471053923121
iteration : 4373
train acc:  0.6640625
train loss:  0.5855650305747986
train gradient:  0.16160542188705196
iteration : 4374
train acc:  0.734375
train loss:  0.49259787797927856
train gradient:  0.12848684554906095
iteration : 4375
train acc:  0.6796875
train loss:  0.5767406225204468
train gradient:  0.1596884867227587
iteration : 4376
train acc:  0.7421875
train loss:  0.4722532033920288
train gradient:  0.12627156649518823
iteration : 4377
train acc:  0.6328125
train loss:  0.5728821158409119
train gradient:  0.23034519073970802
iteration : 4378
train acc:  0.765625
train loss:  0.5580741167068481
train gradient:  0.19861067933119014
iteration : 4379
train acc:  0.7421875
train loss:  0.5245339870452881
train gradient:  0.13622649246775712
iteration : 4380
train acc:  0.734375
train loss:  0.48263710737228394
train gradient:  0.15371045581345408
iteration : 4381
train acc:  0.6953125
train loss:  0.5474672913551331
train gradient:  0.18659752648525285
iteration : 4382
train acc:  0.7578125
train loss:  0.5121949911117554
train gradient:  0.10873178781397996
iteration : 4383
train acc:  0.71875
train loss:  0.49299895763397217
train gradient:  0.13314803320691865
iteration : 4384
train acc:  0.7109375
train loss:  0.5167019367218018
train gradient:  0.1515136190090111
iteration : 4385
train acc:  0.7109375
train loss:  0.5160359740257263
train gradient:  0.1267128850712512
iteration : 4386
train acc:  0.703125
train loss:  0.5244245529174805
train gradient:  0.23384348269336563
iteration : 4387
train acc:  0.75
train loss:  0.515744686126709
train gradient:  0.174580060001232
iteration : 4388
train acc:  0.75
train loss:  0.4921862781047821
train gradient:  0.13188665042496717
iteration : 4389
train acc:  0.7734375
train loss:  0.48105859756469727
train gradient:  0.12000809381577861
iteration : 4390
train acc:  0.8046875
train loss:  0.45685702562332153
train gradient:  0.11544212834709508
iteration : 4391
train acc:  0.6875
train loss:  0.5272814631462097
train gradient:  0.12668267632943564
iteration : 4392
train acc:  0.8359375
train loss:  0.4424233138561249
train gradient:  0.10000446805501115
iteration : 4393
train acc:  0.765625
train loss:  0.5015459656715393
train gradient:  0.15354705756355416
iteration : 4394
train acc:  0.7421875
train loss:  0.5496792793273926
train gradient:  0.1872465062497033
iteration : 4395
train acc:  0.703125
train loss:  0.5021339654922485
train gradient:  0.11470888996231776
iteration : 4396
train acc:  0.6953125
train loss:  0.5421713590621948
train gradient:  0.11487162625092401
iteration : 4397
train acc:  0.8046875
train loss:  0.5026934742927551
train gradient:  0.15162870489161479
iteration : 4398
train acc:  0.6953125
train loss:  0.5715464353561401
train gradient:  0.15428170386994366
iteration : 4399
train acc:  0.65625
train loss:  0.6278811693191528
train gradient:  0.18503607101912536
iteration : 4400
train acc:  0.671875
train loss:  0.5675797462463379
train gradient:  0.16059916758474763
iteration : 4401
train acc:  0.7734375
train loss:  0.4879692494869232
train gradient:  0.15431381877693312
iteration : 4402
train acc:  0.734375
train loss:  0.5301981568336487
train gradient:  0.12780783481339217
iteration : 4403
train acc:  0.6953125
train loss:  0.55322265625
train gradient:  0.16553575427176973
iteration : 4404
train acc:  0.7890625
train loss:  0.4924754202365875
train gradient:  0.1193592238493523
iteration : 4405
train acc:  0.75
train loss:  0.4875248968601227
train gradient:  0.14952331287597637
iteration : 4406
train acc:  0.6875
train loss:  0.5456526279449463
train gradient:  0.14280973991708806
iteration : 4407
train acc:  0.7890625
train loss:  0.45320776104927063
train gradient:  0.13558419668850635
iteration : 4408
train acc:  0.765625
train loss:  0.4736952483654022
train gradient:  0.1459142309466393
iteration : 4409
train acc:  0.6796875
train loss:  0.5694055557250977
train gradient:  0.16029501990839215
iteration : 4410
train acc:  0.8046875
train loss:  0.44198256731033325
train gradient:  0.14324679055261272
iteration : 4411
train acc:  0.7734375
train loss:  0.4601702094078064
train gradient:  0.11913137071935256
iteration : 4412
train acc:  0.6875
train loss:  0.558376669883728
train gradient:  0.1439948015487904
iteration : 4413
train acc:  0.703125
train loss:  0.579424262046814
train gradient:  0.17593689119822986
iteration : 4414
train acc:  0.7109375
train loss:  0.5233485698699951
train gradient:  0.17551339277256822
iteration : 4415
train acc:  0.6875
train loss:  0.5097226500511169
train gradient:  0.15038995506072952
iteration : 4416
train acc:  0.6640625
train loss:  0.5872790813446045
train gradient:  0.18128996984276455
iteration : 4417
train acc:  0.7421875
train loss:  0.486746221780777
train gradient:  0.10899114678260813
iteration : 4418
train acc:  0.703125
train loss:  0.5548969507217407
train gradient:  0.15521634275420637
iteration : 4419
train acc:  0.671875
train loss:  0.5562841296195984
train gradient:  0.1728828550532821
iteration : 4420
train acc:  0.6875
train loss:  0.5140078067779541
train gradient:  0.16331968381769876
iteration : 4421
train acc:  0.703125
train loss:  0.5359246730804443
train gradient:  0.16074322671225882
iteration : 4422
train acc:  0.71875
train loss:  0.5550267696380615
train gradient:  0.2022508056579424
iteration : 4423
train acc:  0.71875
train loss:  0.4933692216873169
train gradient:  0.15216757467461015
iteration : 4424
train acc:  0.7265625
train loss:  0.4894564151763916
train gradient:  0.1267486461596006
iteration : 4425
train acc:  0.7890625
train loss:  0.4538674056529999
train gradient:  0.11550626958222801
iteration : 4426
train acc:  0.7109375
train loss:  0.48098576068878174
train gradient:  0.1366951339562232
iteration : 4427
train acc:  0.7890625
train loss:  0.428291916847229
train gradient:  0.12183366026642216
iteration : 4428
train acc:  0.7109375
train loss:  0.6182022094726562
train gradient:  0.209927797002591
iteration : 4429
train acc:  0.765625
train loss:  0.4433746337890625
train gradient:  0.12328331398595158
iteration : 4430
train acc:  0.7421875
train loss:  0.4876844882965088
train gradient:  0.16559446375576592
iteration : 4431
train acc:  0.6796875
train loss:  0.5689650774002075
train gradient:  0.19859802356751394
iteration : 4432
train acc:  0.8125
train loss:  0.460731565952301
train gradient:  0.13636716486530154
iteration : 4433
train acc:  0.703125
train loss:  0.5677242279052734
train gradient:  0.1496998450367873
iteration : 4434
train acc:  0.765625
train loss:  0.49624353647232056
train gradient:  0.13555750817740714
iteration : 4435
train acc:  0.734375
train loss:  0.5241022109985352
train gradient:  0.1648683935971784
iteration : 4436
train acc:  0.8125
train loss:  0.43351638317108154
train gradient:  0.11061844587062401
iteration : 4437
train acc:  0.7734375
train loss:  0.4964565634727478
train gradient:  0.12266133283809295
iteration : 4438
train acc:  0.6953125
train loss:  0.536018967628479
train gradient:  0.244867520577437
iteration : 4439
train acc:  0.7578125
train loss:  0.5011441707611084
train gradient:  0.14873251160093254
iteration : 4440
train acc:  0.625
train loss:  0.6117545366287231
train gradient:  0.19929135218557376
iteration : 4441
train acc:  0.7265625
train loss:  0.5008630156517029
train gradient:  0.16628404019946702
iteration : 4442
train acc:  0.6875
train loss:  0.552463173866272
train gradient:  0.1850604737212392
iteration : 4443
train acc:  0.734375
train loss:  0.5043836236000061
train gradient:  0.14225626404509617
iteration : 4444
train acc:  0.671875
train loss:  0.5331229567527771
train gradient:  0.16768624085063918
iteration : 4445
train acc:  0.734375
train loss:  0.49376678466796875
train gradient:  0.14523601086906024
iteration : 4446
train acc:  0.765625
train loss:  0.4997480511665344
train gradient:  0.14501895631597767
iteration : 4447
train acc:  0.765625
train loss:  0.5206098556518555
train gradient:  0.13712676946500224
iteration : 4448
train acc:  0.6953125
train loss:  0.5759439468383789
train gradient:  0.23377310474800667
iteration : 4449
train acc:  0.71875
train loss:  0.548747181892395
train gradient:  0.15278216308105036
iteration : 4450
train acc:  0.78125
train loss:  0.4667738080024719
train gradient:  0.12112086911033115
iteration : 4451
train acc:  0.703125
train loss:  0.5406308770179749
train gradient:  0.16463514703553453
iteration : 4452
train acc:  0.7578125
train loss:  0.46883660554885864
train gradient:  0.11986142049649164
iteration : 4453
train acc:  0.71875
train loss:  0.5395623445510864
train gradient:  0.16774087591447245
iteration : 4454
train acc:  0.7734375
train loss:  0.43911013007164
train gradient:  0.09885556556780709
iteration : 4455
train acc:  0.671875
train loss:  0.5857868194580078
train gradient:  0.1962171013228035
iteration : 4456
train acc:  0.78125
train loss:  0.47841891646385193
train gradient:  0.13461318264027222
iteration : 4457
train acc:  0.734375
train loss:  0.5089327096939087
train gradient:  0.143470772099333
iteration : 4458
train acc:  0.7265625
train loss:  0.4799366891384125
train gradient:  0.11482061089871974
iteration : 4459
train acc:  0.6328125
train loss:  0.6166365742683411
train gradient:  0.17797597586985386
iteration : 4460
train acc:  0.6484375
train loss:  0.5867295265197754
train gradient:  0.1491550081078667
iteration : 4461
train acc:  0.6640625
train loss:  0.56432044506073
train gradient:  0.1529813634327685
iteration : 4462
train acc:  0.6875
train loss:  0.5878960490226746
train gradient:  0.2511660882113112
iteration : 4463
train acc:  0.7109375
train loss:  0.5881121158599854
train gradient:  0.22308081729327367
iteration : 4464
train acc:  0.6953125
train loss:  0.504428505897522
train gradient:  0.1442740985580814
iteration : 4465
train acc:  0.7734375
train loss:  0.5000538229942322
train gradient:  0.14647013340362441
iteration : 4466
train acc:  0.7421875
train loss:  0.49626433849334717
train gradient:  0.14941414580238718
iteration : 4467
train acc:  0.6953125
train loss:  0.5642580986022949
train gradient:  0.1474518200788862
iteration : 4468
train acc:  0.7265625
train loss:  0.5099828243255615
train gradient:  0.11536632862318495
iteration : 4469
train acc:  0.7265625
train loss:  0.4946911931037903
train gradient:  0.17833310624750512
iteration : 4470
train acc:  0.7421875
train loss:  0.48811861872673035
train gradient:  0.14442433563063095
iteration : 4471
train acc:  0.734375
train loss:  0.4999814033508301
train gradient:  0.15848369449340294
iteration : 4472
train acc:  0.65625
train loss:  0.5454772710800171
train gradient:  0.13496023282252845
iteration : 4473
train acc:  0.6796875
train loss:  0.5394862294197083
train gradient:  0.12485121015778065
iteration : 4474
train acc:  0.7265625
train loss:  0.5312066078186035
train gradient:  0.14896843509726387
iteration : 4475
train acc:  0.765625
train loss:  0.47960364818573
train gradient:  0.12528047340527693
iteration : 4476
train acc:  0.7421875
train loss:  0.4796263575553894
train gradient:  0.10877116967147703
iteration : 4477
train acc:  0.6875
train loss:  0.5449694395065308
train gradient:  0.17268225099936213
iteration : 4478
train acc:  0.7578125
train loss:  0.5069987773895264
train gradient:  0.1121921878943488
iteration : 4479
train acc:  0.71875
train loss:  0.5094409584999084
train gradient:  0.12579589301227884
iteration : 4480
train acc:  0.7109375
train loss:  0.5144485831260681
train gradient:  0.13187320119779186
iteration : 4481
train acc:  0.671875
train loss:  0.555144190788269
train gradient:  0.15216037349730765
iteration : 4482
train acc:  0.6796875
train loss:  0.5270904302597046
train gradient:  0.15399844183616163
iteration : 4483
train acc:  0.765625
train loss:  0.4641488790512085
train gradient:  0.1474071011664515
iteration : 4484
train acc:  0.6640625
train loss:  0.5629497766494751
train gradient:  0.21028537015431703
iteration : 4485
train acc:  0.765625
train loss:  0.44367513060569763
train gradient:  0.09306245929871311
iteration : 4486
train acc:  0.734375
train loss:  0.5477070808410645
train gradient:  0.16182033418694122
iteration : 4487
train acc:  0.6953125
train loss:  0.5219936370849609
train gradient:  0.1944281333450047
iteration : 4488
train acc:  0.6796875
train loss:  0.5623286366462708
train gradient:  0.15056702578459857
iteration : 4489
train acc:  0.703125
train loss:  0.5959159731864929
train gradient:  0.18031094087520763
iteration : 4490
train acc:  0.7421875
train loss:  0.5059523582458496
train gradient:  0.15541896099995733
iteration : 4491
train acc:  0.7265625
train loss:  0.5193760395050049
train gradient:  0.13128031711657173
iteration : 4492
train acc:  0.703125
train loss:  0.5509170889854431
train gradient:  0.13861861619045193
iteration : 4493
train acc:  0.7265625
train loss:  0.5070525407791138
train gradient:  0.1925901951539983
iteration : 4494
train acc:  0.6953125
train loss:  0.5396420955657959
train gradient:  0.13095667152614654
iteration : 4495
train acc:  0.6796875
train loss:  0.5152220726013184
train gradient:  0.10783835582964947
iteration : 4496
train acc:  0.7265625
train loss:  0.5466749668121338
train gradient:  0.15767967375783876
iteration : 4497
train acc:  0.71875
train loss:  0.4898235499858856
train gradient:  0.12920514108214762
iteration : 4498
train acc:  0.671875
train loss:  0.5695748329162598
train gradient:  0.13880873809998912
iteration : 4499
train acc:  0.7578125
train loss:  0.4982872009277344
train gradient:  0.15572983875946828
iteration : 4500
train acc:  0.671875
train loss:  0.5476652979850769
train gradient:  0.14853548094218877
iteration : 4501
train acc:  0.71875
train loss:  0.5771465301513672
train gradient:  0.1524412975661859
iteration : 4502
train acc:  0.78125
train loss:  0.4708762764930725
train gradient:  0.12740879799088228
iteration : 4503
train acc:  0.71875
train loss:  0.5019771456718445
train gradient:  0.19281681301357007
iteration : 4504
train acc:  0.71875
train loss:  0.5018446445465088
train gradient:  0.14052193986091996
iteration : 4505
train acc:  0.734375
train loss:  0.5244255065917969
train gradient:  0.1692040293893971
iteration : 4506
train acc:  0.6875
train loss:  0.58167564868927
train gradient:  0.18106160799834203
iteration : 4507
train acc:  0.6953125
train loss:  0.5291193723678589
train gradient:  0.15615967925060675
iteration : 4508
train acc:  0.6796875
train loss:  0.5672860741615295
train gradient:  0.1872946605702805
iteration : 4509
train acc:  0.7421875
train loss:  0.4742545485496521
train gradient:  0.08819050125463115
iteration : 4510
train acc:  0.6640625
train loss:  0.565434455871582
train gradient:  0.15568735955822893
iteration : 4511
train acc:  0.7265625
train loss:  0.4908990263938904
train gradient:  0.12368158520481788
iteration : 4512
train acc:  0.734375
train loss:  0.5280078053474426
train gradient:  0.1405518805487487
iteration : 4513
train acc:  0.703125
train loss:  0.5364716053009033
train gradient:  0.1559600629278568
iteration : 4514
train acc:  0.6953125
train loss:  0.5259160995483398
train gradient:  0.14592905148405885
iteration : 4515
train acc:  0.6875
train loss:  0.576447606086731
train gradient:  0.24734791230083605
iteration : 4516
train acc:  0.71875
train loss:  0.5371591448783875
train gradient:  0.1466805342685834
iteration : 4517
train acc:  0.703125
train loss:  0.49342960119247437
train gradient:  0.11778916559681876
iteration : 4518
train acc:  0.7734375
train loss:  0.5437811613082886
train gradient:  0.1638377139844194
iteration : 4519
train acc:  0.6875
train loss:  0.5792969465255737
train gradient:  0.16936601768182372
iteration : 4520
train acc:  0.6875
train loss:  0.5516573786735535
train gradient:  0.15763726982875517
iteration : 4521
train acc:  0.71875
train loss:  0.5600392818450928
train gradient:  0.1630104879516744
iteration : 4522
train acc:  0.7265625
train loss:  0.5487695932388306
train gradient:  0.18519909271757823
iteration : 4523
train acc:  0.7109375
train loss:  0.5450814962387085
train gradient:  0.17377702404734374
iteration : 4524
train acc:  0.6796875
train loss:  0.6702579259872437
train gradient:  0.2998538689649857
iteration : 4525
train acc:  0.7109375
train loss:  0.5689342021942139
train gradient:  0.1794867499535575
iteration : 4526
train acc:  0.7578125
train loss:  0.4776154160499573
train gradient:  0.10434397767808343
iteration : 4527
train acc:  0.7734375
train loss:  0.4504370391368866
train gradient:  0.11642227940722225
iteration : 4528
train acc:  0.671875
train loss:  0.5848556160926819
train gradient:  0.15822140223031428
iteration : 4529
train acc:  0.6953125
train loss:  0.538459300994873
train gradient:  0.13475668012273995
iteration : 4530
train acc:  0.6875
train loss:  0.5393509864807129
train gradient:  0.180318332160889
iteration : 4531
train acc:  0.7109375
train loss:  0.5078350305557251
train gradient:  0.14222691549762084
iteration : 4532
train acc:  0.734375
train loss:  0.5213416814804077
train gradient:  0.1096999864806789
iteration : 4533
train acc:  0.765625
train loss:  0.4916646480560303
train gradient:  0.16270796767486767
iteration : 4534
train acc:  0.7421875
train loss:  0.47723978757858276
train gradient:  0.12128830007968606
iteration : 4535
train acc:  0.7421875
train loss:  0.5447878837585449
train gradient:  0.2094870220484289
iteration : 4536
train acc:  0.7421875
train loss:  0.49523818492889404
train gradient:  0.11217634273161016
iteration : 4537
train acc:  0.7265625
train loss:  0.5114421844482422
train gradient:  0.11272737120304634
iteration : 4538
train acc:  0.7421875
train loss:  0.5273791551589966
train gradient:  0.13768678587176586
iteration : 4539
train acc:  0.7109375
train loss:  0.5290007591247559
train gradient:  0.15140733783145216
iteration : 4540
train acc:  0.671875
train loss:  0.5225280523300171
train gradient:  0.14295710663938888
iteration : 4541
train acc:  0.734375
train loss:  0.47932711243629456
train gradient:  0.10841754418465094
iteration : 4542
train acc:  0.703125
train loss:  0.5590770244598389
train gradient:  0.1490460675700998
iteration : 4543
train acc:  0.7265625
train loss:  0.5170232057571411
train gradient:  0.11996383856427621
iteration : 4544
train acc:  0.75
train loss:  0.5026856660842896
train gradient:  0.13467033298698056
iteration : 4545
train acc:  0.71875
train loss:  0.5242892503738403
train gradient:  0.16972953223200804
iteration : 4546
train acc:  0.6875
train loss:  0.5955216288566589
train gradient:  0.18474946933323189
iteration : 4547
train acc:  0.7109375
train loss:  0.5402405261993408
train gradient:  0.15915312266365503
iteration : 4548
train acc:  0.7265625
train loss:  0.5415032505989075
train gradient:  0.15857964784566694
iteration : 4549
train acc:  0.75
train loss:  0.5089132189750671
train gradient:  0.1227722230133496
iteration : 4550
train acc:  0.734375
train loss:  0.48974093794822693
train gradient:  0.10326443779201064
iteration : 4551
train acc:  0.75
train loss:  0.5268958806991577
train gradient:  0.16511722297311332
iteration : 4552
train acc:  0.7265625
train loss:  0.5254272222518921
train gradient:  0.1297789515283746
iteration : 4553
train acc:  0.7421875
train loss:  0.5165313482284546
train gradient:  0.14293977491241555
iteration : 4554
train acc:  0.7265625
train loss:  0.51875239610672
train gradient:  0.1254907612843547
iteration : 4555
train acc:  0.75
train loss:  0.4742273688316345
train gradient:  0.1600104037121683
iteration : 4556
train acc:  0.75
train loss:  0.5134696364402771
train gradient:  0.1347945129900145
iteration : 4557
train acc:  0.7734375
train loss:  0.4985499978065491
train gradient:  0.14411353483767758
iteration : 4558
train acc:  0.6484375
train loss:  0.6091161966323853
train gradient:  0.19117587088953603
iteration : 4559
train acc:  0.671875
train loss:  0.5284440517425537
train gradient:  0.12439663538332581
iteration : 4560
train acc:  0.71875
train loss:  0.5457408428192139
train gradient:  0.18032911084218856
iteration : 4561
train acc:  0.7578125
train loss:  0.4919854998588562
train gradient:  0.13677286885197765
iteration : 4562
train acc:  0.71875
train loss:  0.5183027386665344
train gradient:  0.14475300068092822
iteration : 4563
train acc:  0.734375
train loss:  0.522638201713562
train gradient:  0.14504321300894896
iteration : 4564
train acc:  0.703125
train loss:  0.5057531595230103
train gradient:  0.1296759906160561
iteration : 4565
train acc:  0.6875
train loss:  0.5415154695510864
train gradient:  0.11398223331986092
iteration : 4566
train acc:  0.7421875
train loss:  0.5334643125534058
train gradient:  0.12291347945997351
iteration : 4567
train acc:  0.6640625
train loss:  0.5771540403366089
train gradient:  0.18408954012132595
iteration : 4568
train acc:  0.6640625
train loss:  0.5642940402030945
train gradient:  0.1789200375703614
iteration : 4569
train acc:  0.796875
train loss:  0.43113428354263306
train gradient:  0.09592151252614967
iteration : 4570
train acc:  0.7890625
train loss:  0.4792843461036682
train gradient:  0.12881491001631049
iteration : 4571
train acc:  0.7421875
train loss:  0.4986724853515625
train gradient:  0.1873667751630595
iteration : 4572
train acc:  0.6875
train loss:  0.5830351114273071
train gradient:  0.1537780681266892
iteration : 4573
train acc:  0.6953125
train loss:  0.587310791015625
train gradient:  0.1410461418393046
iteration : 4574
train acc:  0.78125
train loss:  0.4684150815010071
train gradient:  0.14116048995342861
iteration : 4575
train acc:  0.625
train loss:  0.6117537021636963
train gradient:  0.17582558295145723
iteration : 4576
train acc:  0.71875
train loss:  0.49028509855270386
train gradient:  0.11240991231551563
iteration : 4577
train acc:  0.734375
train loss:  0.481095552444458
train gradient:  0.15786705209561988
iteration : 4578
train acc:  0.7734375
train loss:  0.46058231592178345
train gradient:  0.15279406232367115
iteration : 4579
train acc:  0.7421875
train loss:  0.47751834988594055
train gradient:  0.11934511303916093
iteration : 4580
train acc:  0.671875
train loss:  0.5415926575660706
train gradient:  0.21405511564665758
iteration : 4581
train acc:  0.734375
train loss:  0.48694154620170593
train gradient:  0.11220970264831957
iteration : 4582
train acc:  0.6640625
train loss:  0.5973120331764221
train gradient:  0.2102086762437648
iteration : 4583
train acc:  0.7421875
train loss:  0.48980003595352173
train gradient:  0.12387450705171349
iteration : 4584
train acc:  0.7578125
train loss:  0.4868161082267761
train gradient:  0.1580349184712629
iteration : 4585
train acc:  0.7109375
train loss:  0.5602887868881226
train gradient:  0.1666492502743923
iteration : 4586
train acc:  0.75
train loss:  0.48032188415527344
train gradient:  0.1333943398369602
iteration : 4587
train acc:  0.7734375
train loss:  0.45736613869667053
train gradient:  0.12740322033353713
iteration : 4588
train acc:  0.7578125
train loss:  0.4965403378009796
train gradient:  0.15191304466007716
iteration : 4589
train acc:  0.8125
train loss:  0.4919949769973755
train gradient:  0.1378929079037417
iteration : 4590
train acc:  0.7109375
train loss:  0.5459973216056824
train gradient:  0.13976404218912775
iteration : 4591
train acc:  0.765625
train loss:  0.4673561751842499
train gradient:  0.1638298178120724
iteration : 4592
train acc:  0.6953125
train loss:  0.5483176112174988
train gradient:  0.16655265111150627
iteration : 4593
train acc:  0.75
train loss:  0.49359130859375
train gradient:  0.1347171575996129
iteration : 4594
train acc:  0.7734375
train loss:  0.495802640914917
train gradient:  0.1356414997961808
iteration : 4595
train acc:  0.671875
train loss:  0.5052912831306458
train gradient:  0.13239520041135838
iteration : 4596
train acc:  0.7578125
train loss:  0.5245996117591858
train gradient:  0.13400014783449446
iteration : 4597
train acc:  0.7265625
train loss:  0.519910454750061
train gradient:  0.1274946727001266
iteration : 4598
train acc:  0.7109375
train loss:  0.5125586986541748
train gradient:  0.15898048598971254
iteration : 4599
train acc:  0.7578125
train loss:  0.4741917550563812
train gradient:  0.12434699187486517
iteration : 4600
train acc:  0.7109375
train loss:  0.5263209342956543
train gradient:  0.19330238710352637
iteration : 4601
train acc:  0.7421875
train loss:  0.47497451305389404
train gradient:  0.13731573459731578
iteration : 4602
train acc:  0.734375
train loss:  0.5754289627075195
train gradient:  0.2100305434750756
iteration : 4603
train acc:  0.703125
train loss:  0.49991732835769653
train gradient:  0.14214589615759315
iteration : 4604
train acc:  0.7421875
train loss:  0.5215736627578735
train gradient:  0.13601035287685775
iteration : 4605
train acc:  0.65625
train loss:  0.570155143737793
train gradient:  0.17132125972855627
iteration : 4606
train acc:  0.734375
train loss:  0.5242789387702942
train gradient:  0.14599244875924688
iteration : 4607
train acc:  0.703125
train loss:  0.534347653388977
train gradient:  0.16264364766037387
iteration : 4608
train acc:  0.7734375
train loss:  0.48923587799072266
train gradient:  0.12864898881798748
iteration : 4609
train acc:  0.765625
train loss:  0.47976037859916687
train gradient:  0.132136812151165
iteration : 4610
train acc:  0.734375
train loss:  0.5097808241844177
train gradient:  0.17077531282041794
iteration : 4611
train acc:  0.75
train loss:  0.4780539274215698
train gradient:  0.17492609711840434
iteration : 4612
train acc:  0.734375
train loss:  0.5511232614517212
train gradient:  0.17105208272112388
iteration : 4613
train acc:  0.7265625
train loss:  0.5282900333404541
train gradient:  0.14850126166315264
iteration : 4614
train acc:  0.75
train loss:  0.5308059453964233
train gradient:  0.1574324318565213
iteration : 4615
train acc:  0.65625
train loss:  0.5708292722702026
train gradient:  0.18891546767915046
iteration : 4616
train acc:  0.75
train loss:  0.45864373445510864
train gradient:  0.13098216716222413
iteration : 4617
train acc:  0.71875
train loss:  0.5551584959030151
train gradient:  0.15307630076765544
iteration : 4618
train acc:  0.7109375
train loss:  0.5377612709999084
train gradient:  0.1304170092040136
iteration : 4619
train acc:  0.7734375
train loss:  0.5039193630218506
train gradient:  0.15328268295216677
iteration : 4620
train acc:  0.765625
train loss:  0.5267316699028015
train gradient:  0.16272694008123748
iteration : 4621
train acc:  0.734375
train loss:  0.47387391328811646
train gradient:  0.10586340066360471
iteration : 4622
train acc:  0.6796875
train loss:  0.5826188921928406
train gradient:  0.1994516965261386
iteration : 4623
train acc:  0.75
train loss:  0.5059255361557007
train gradient:  0.10543224626894668
iteration : 4624
train acc:  0.7421875
train loss:  0.5300991535186768
train gradient:  0.1527456340148366
iteration : 4625
train acc:  0.8125
train loss:  0.4291239380836487
train gradient:  0.09877687721267688
iteration : 4626
train acc:  0.7421875
train loss:  0.5435866117477417
train gradient:  0.15870285378322724
iteration : 4627
train acc:  0.7265625
train loss:  0.5409222841262817
train gradient:  0.17632522201787298
iteration : 4628
train acc:  0.71875
train loss:  0.5096713304519653
train gradient:  0.1508538837345412
iteration : 4629
train acc:  0.75
train loss:  0.48299601674079895
train gradient:  0.10978219470288945
iteration : 4630
train acc:  0.6953125
train loss:  0.547165036201477
train gradient:  0.1516820812155309
iteration : 4631
train acc:  0.78125
train loss:  0.4540537893772125
train gradient:  0.11777929935534796
iteration : 4632
train acc:  0.75
train loss:  0.49180468916893005
train gradient:  0.12007393206773112
iteration : 4633
train acc:  0.703125
train loss:  0.5456802845001221
train gradient:  0.1754432069624996
iteration : 4634
train acc:  0.671875
train loss:  0.5864949226379395
train gradient:  0.16729606531327654
iteration : 4635
train acc:  0.75
train loss:  0.49731284379959106
train gradient:  0.1597520492185921
iteration : 4636
train acc:  0.6796875
train loss:  0.5856472253799438
train gradient:  0.189492099780186
iteration : 4637
train acc:  0.6171875
train loss:  0.5892220139503479
train gradient:  0.2298210824544512
iteration : 4638
train acc:  0.7421875
train loss:  0.5036522746086121
train gradient:  0.13715434497551648
iteration : 4639
train acc:  0.734375
train loss:  0.488980233669281
train gradient:  0.16781792053199596
iteration : 4640
train acc:  0.703125
train loss:  0.561173141002655
train gradient:  0.19243829678283458
iteration : 4641
train acc:  0.796875
train loss:  0.4500496983528137
train gradient:  0.1070982945490454
iteration : 4642
train acc:  0.6796875
train loss:  0.5605217814445496
train gradient:  0.1767455571455537
iteration : 4643
train acc:  0.7734375
train loss:  0.46929484605789185
train gradient:  0.1301508924110732
iteration : 4644
train acc:  0.6953125
train loss:  0.557835042476654
train gradient:  0.14428765172888203
iteration : 4645
train acc:  0.765625
train loss:  0.5087442398071289
train gradient:  0.14457324816762052
iteration : 4646
train acc:  0.75
train loss:  0.49293094873428345
train gradient:  0.13393544260382576
iteration : 4647
train acc:  0.78125
train loss:  0.43364089727401733
train gradient:  0.1252403764902396
iteration : 4648
train acc:  0.765625
train loss:  0.47886186838150024
train gradient:  0.14977710714420658
iteration : 4649
train acc:  0.7734375
train loss:  0.5260723829269409
train gradient:  0.1662054251738348
iteration : 4650
train acc:  0.734375
train loss:  0.4624403715133667
train gradient:  0.17536152383635917
iteration : 4651
train acc:  0.703125
train loss:  0.5437813997268677
train gradient:  0.18587419227988405
iteration : 4652
train acc:  0.703125
train loss:  0.5485018491744995
train gradient:  0.15042144064452137
iteration : 4653
train acc:  0.7421875
train loss:  0.48486751317977905
train gradient:  0.1497644350302894
iteration : 4654
train acc:  0.671875
train loss:  0.5592198967933655
train gradient:  0.15967073729311387
iteration : 4655
train acc:  0.703125
train loss:  0.5577356815338135
train gradient:  0.1675077040950118
iteration : 4656
train acc:  0.7265625
train loss:  0.5290699005126953
train gradient:  0.15437948291650755
iteration : 4657
train acc:  0.796875
train loss:  0.46295619010925293
train gradient:  0.10884701923462244
iteration : 4658
train acc:  0.6796875
train loss:  0.5870177149772644
train gradient:  0.2769997955139414
iteration : 4659
train acc:  0.734375
train loss:  0.5054964423179626
train gradient:  0.1349320299844774
iteration : 4660
train acc:  0.7109375
train loss:  0.5602171421051025
train gradient:  0.13894713064141107
iteration : 4661
train acc:  0.7734375
train loss:  0.47940757870674133
train gradient:  0.16459841074265455
iteration : 4662
train acc:  0.765625
train loss:  0.5155551433563232
train gradient:  0.17907298380352077
iteration : 4663
train acc:  0.765625
train loss:  0.46525460481643677
train gradient:  0.12937486399249049
iteration : 4664
train acc:  0.7109375
train loss:  0.5211197137832642
train gradient:  0.13955649771205292
iteration : 4665
train acc:  0.7578125
train loss:  0.49498283863067627
train gradient:  0.1420601131945352
iteration : 4666
train acc:  0.7734375
train loss:  0.4419589638710022
train gradient:  0.11253980446828753
iteration : 4667
train acc:  0.7421875
train loss:  0.4815834164619446
train gradient:  0.13475595091115633
iteration : 4668
train acc:  0.7421875
train loss:  0.49355465173721313
train gradient:  0.16101521141731603
iteration : 4669
train acc:  0.703125
train loss:  0.5282994508743286
train gradient:  0.18013663347135206
iteration : 4670
train acc:  0.703125
train loss:  0.5397331118583679
train gradient:  0.16062328656088753
iteration : 4671
train acc:  0.7265625
train loss:  0.544278621673584
train gradient:  0.1540610761663884
iteration : 4672
train acc:  0.71875
train loss:  0.5391421318054199
train gradient:  0.15125549133345162
iteration : 4673
train acc:  0.7578125
train loss:  0.5039405822753906
train gradient:  0.12803035883699873
iteration : 4674
train acc:  0.75
train loss:  0.5209110975265503
train gradient:  0.15957488789274385
iteration : 4675
train acc:  0.75
train loss:  0.5254994630813599
train gradient:  0.1768294436443196
iteration : 4676
train acc:  0.703125
train loss:  0.5752441883087158
train gradient:  0.24944260972554272
iteration : 4677
train acc:  0.7578125
train loss:  0.4808274209499359
train gradient:  0.12376911096359985
iteration : 4678
train acc:  0.734375
train loss:  0.5145748853683472
train gradient:  0.13579293643120174
iteration : 4679
train acc:  0.71875
train loss:  0.4779110550880432
train gradient:  0.16007221413913136
iteration : 4680
train acc:  0.7109375
train loss:  0.5717781782150269
train gradient:  0.21787016962501315
iteration : 4681
train acc:  0.703125
train loss:  0.5699856877326965
train gradient:  0.19812184283409143
iteration : 4682
train acc:  0.7890625
train loss:  0.4468533396720886
train gradient:  0.13300167622255263
iteration : 4683
train acc:  0.6796875
train loss:  0.5183975696563721
train gradient:  0.11461222393943746
iteration : 4684
train acc:  0.7265625
train loss:  0.479619562625885
train gradient:  0.12281552620383421
iteration : 4685
train acc:  0.71875
train loss:  0.49577587842941284
train gradient:  0.18014625134899928
iteration : 4686
train acc:  0.734375
train loss:  0.4886144995689392
train gradient:  0.12141945834618308
iteration : 4687
train acc:  0.7578125
train loss:  0.507775068283081
train gradient:  0.20299353305561252
iteration : 4688
train acc:  0.7421875
train loss:  0.528695821762085
train gradient:  0.13404457842981532
iteration : 4689
train acc:  0.78125
train loss:  0.4833100736141205
train gradient:  0.1597701307672467
iteration : 4690
train acc:  0.7578125
train loss:  0.4704667925834656
train gradient:  0.12100213892444692
iteration : 4691
train acc:  0.671875
train loss:  0.5387140512466431
train gradient:  0.16665507007173902
iteration : 4692
train acc:  0.734375
train loss:  0.5169515609741211
train gradient:  0.15201279185406932
iteration : 4693
train acc:  0.703125
train loss:  0.5257667303085327
train gradient:  0.14734382707431884
iteration : 4694
train acc:  0.7421875
train loss:  0.502292275428772
train gradient:  0.161386595538917
iteration : 4695
train acc:  0.78125
train loss:  0.4602619409561157
train gradient:  0.13307230952614701
iteration : 4696
train acc:  0.6953125
train loss:  0.5515434145927429
train gradient:  0.1687413351276259
iteration : 4697
train acc:  0.6953125
train loss:  0.5338575839996338
train gradient:  0.14859761433498372
iteration : 4698
train acc:  0.734375
train loss:  0.4729933738708496
train gradient:  0.14544541800003435
iteration : 4699
train acc:  0.7109375
train loss:  0.5085036754608154
train gradient:  0.13140274884847708
iteration : 4700
train acc:  0.6796875
train loss:  0.5639907121658325
train gradient:  0.19162832712852676
iteration : 4701
train acc:  0.6953125
train loss:  0.5393627285957336
train gradient:  0.15451788866013136
iteration : 4702
train acc:  0.765625
train loss:  0.4961067736148834
train gradient:  0.11259757378583121
iteration : 4703
train acc:  0.765625
train loss:  0.46643227338790894
train gradient:  0.11584052452649858
iteration : 4704
train acc:  0.6796875
train loss:  0.5328954458236694
train gradient:  0.14141053425755673
iteration : 4705
train acc:  0.671875
train loss:  0.6245984435081482
train gradient:  0.20233295059253675
iteration : 4706
train acc:  0.8046875
train loss:  0.42422059178352356
train gradient:  0.12376279926789423
iteration : 4707
train acc:  0.7265625
train loss:  0.501512885093689
train gradient:  0.16519643070239945
iteration : 4708
train acc:  0.7265625
train loss:  0.5191702842712402
train gradient:  0.15373222178864226
iteration : 4709
train acc:  0.7421875
train loss:  0.48681914806365967
train gradient:  0.14198794756173286
iteration : 4710
train acc:  0.7421875
train loss:  0.47452160716056824
train gradient:  0.15438546478970377
iteration : 4711
train acc:  0.734375
train loss:  0.5455852150917053
train gradient:  0.17471506816362656
iteration : 4712
train acc:  0.75
train loss:  0.462197482585907
train gradient:  0.15867839220224986
iteration : 4713
train acc:  0.65625
train loss:  0.5567257404327393
train gradient:  0.20221415686823285
iteration : 4714
train acc:  0.765625
train loss:  0.49777454137802124
train gradient:  0.13177792426039348
iteration : 4715
train acc:  0.78125
train loss:  0.4539448618888855
train gradient:  0.13155963645682778
iteration : 4716
train acc:  0.7109375
train loss:  0.5283571481704712
train gradient:  0.12182487815009911
iteration : 4717
train acc:  0.765625
train loss:  0.5147281885147095
train gradient:  0.13004716692438517
iteration : 4718
train acc:  0.671875
train loss:  0.5537791848182678
train gradient:  0.13405791321242294
iteration : 4719
train acc:  0.6875
train loss:  0.580287516117096
train gradient:  0.24893388573460412
iteration : 4720
train acc:  0.671875
train loss:  0.5910561084747314
train gradient:  0.22954888646509958
iteration : 4721
train acc:  0.7421875
train loss:  0.510089635848999
train gradient:  0.15720276226027344
iteration : 4722
train acc:  0.7890625
train loss:  0.4750343859195709
train gradient:  0.14135357151624348
iteration : 4723
train acc:  0.75
train loss:  0.49911391735076904
train gradient:  0.14098763086489935
iteration : 4724
train acc:  0.734375
train loss:  0.501340389251709
train gradient:  0.17040220361117214
iteration : 4725
train acc:  0.6953125
train loss:  0.5778870582580566
train gradient:  0.17590524977617933
iteration : 4726
train acc:  0.7109375
train loss:  0.44521933794021606
train gradient:  0.11477371185226266
iteration : 4727
train acc:  0.75
train loss:  0.5053279399871826
train gradient:  0.16670664392419293
iteration : 4728
train acc:  0.7578125
train loss:  0.5616285800933838
train gradient:  0.14067418274922766
iteration : 4729
train acc:  0.7265625
train loss:  0.493588924407959
train gradient:  0.13083828687168303
iteration : 4730
train acc:  0.7265625
train loss:  0.4952487349510193
train gradient:  0.12007193061475593
iteration : 4731
train acc:  0.7734375
train loss:  0.45503008365631104
train gradient:  0.12591419572275614
iteration : 4732
train acc:  0.7421875
train loss:  0.48686668276786804
train gradient:  0.1166603332324548
iteration : 4733
train acc:  0.7265625
train loss:  0.5395892262458801
train gradient:  0.23078159883450278
iteration : 4734
train acc:  0.7109375
train loss:  0.5118880867958069
train gradient:  0.18606135524700007
iteration : 4735
train acc:  0.7421875
train loss:  0.4672242999076843
train gradient:  0.12006995427705389
iteration : 4736
train acc:  0.7265625
train loss:  0.5445940494537354
train gradient:  0.1669973551747632
iteration : 4737
train acc:  0.7109375
train loss:  0.5371741652488708
train gradient:  0.23910240803120003
iteration : 4738
train acc:  0.734375
train loss:  0.522791862487793
train gradient:  0.16274378862432543
iteration : 4739
train acc:  0.734375
train loss:  0.5320565700531006
train gradient:  0.13740643799311747
iteration : 4740
train acc:  0.7265625
train loss:  0.4853585958480835
train gradient:  0.12640466625212127
iteration : 4741
train acc:  0.7109375
train loss:  0.5037442445755005
train gradient:  0.15625867559725026
iteration : 4742
train acc:  0.7109375
train loss:  0.5097329616546631
train gradient:  0.1430721389841037
iteration : 4743
train acc:  0.765625
train loss:  0.4876776933670044
train gradient:  0.16310123403449725
iteration : 4744
train acc:  0.6953125
train loss:  0.5749359130859375
train gradient:  0.2349327554938676
iteration : 4745
train acc:  0.6953125
train loss:  0.5816982984542847
train gradient:  0.19464612547401677
iteration : 4746
train acc:  0.765625
train loss:  0.44926726818084717
train gradient:  0.13257589346949566
iteration : 4747
train acc:  0.75
train loss:  0.4665500521659851
train gradient:  0.1329197682755271
iteration : 4748
train acc:  0.625
train loss:  0.6763699650764465
train gradient:  0.2681416657288187
iteration : 4749
train acc:  0.6328125
train loss:  0.6290668249130249
train gradient:  0.19374434539059102
iteration : 4750
train acc:  0.65625
train loss:  0.5806683897972107
train gradient:  0.17563843816730057
iteration : 4751
train acc:  0.7421875
train loss:  0.551986813545227
train gradient:  0.15742079144707216
iteration : 4752
train acc:  0.75
train loss:  0.5266112089157104
train gradient:  0.13401284151538798
iteration : 4753
train acc:  0.765625
train loss:  0.5060577988624573
train gradient:  0.19413742999238906
iteration : 4754
train acc:  0.7265625
train loss:  0.5030654668807983
train gradient:  0.11166518739054342
iteration : 4755
train acc:  0.734375
train loss:  0.5373057126998901
train gradient:  0.1415261719667753
iteration : 4756
train acc:  0.6484375
train loss:  0.5687655210494995
train gradient:  0.19969222820369453
iteration : 4757
train acc:  0.7109375
train loss:  0.5481759905815125
train gradient:  0.17107338899590546
iteration : 4758
train acc:  0.6640625
train loss:  0.5933146476745605
train gradient:  0.16569180094420238
iteration : 4759
train acc:  0.75
train loss:  0.5296444892883301
train gradient:  0.15215757129175342
iteration : 4760
train acc:  0.7578125
train loss:  0.5069843530654907
train gradient:  0.13466739554054535
iteration : 4761
train acc:  0.71875
train loss:  0.5274789929389954
train gradient:  0.1622465802752182
iteration : 4762
train acc:  0.7265625
train loss:  0.5128499269485474
train gradient:  0.13094001996848983
iteration : 4763
train acc:  0.765625
train loss:  0.4992319941520691
train gradient:  0.14978064952516107
iteration : 4764
train acc:  0.7265625
train loss:  0.5299468040466309
train gradient:  0.13938792752220092
iteration : 4765
train acc:  0.6875
train loss:  0.5430116653442383
train gradient:  0.20092144833258174
iteration : 4766
train acc:  0.6796875
train loss:  0.57744300365448
train gradient:  0.1664563820506444
iteration : 4767
train acc:  0.71875
train loss:  0.529940128326416
train gradient:  0.12354074844891867
iteration : 4768
train acc:  0.7109375
train loss:  0.5330646634101868
train gradient:  0.12717235950922576
iteration : 4769
train acc:  0.7265625
train loss:  0.5001041889190674
train gradient:  0.1350237155666003
iteration : 4770
train acc:  0.7890625
train loss:  0.4574759006500244
train gradient:  0.12466557256066113
iteration : 4771
train acc:  0.734375
train loss:  0.5200866460800171
train gradient:  0.14882229048477566
iteration : 4772
train acc:  0.8046875
train loss:  0.3936958312988281
train gradient:  0.09704084613280133
iteration : 4773
train acc:  0.71875
train loss:  0.5447741746902466
train gradient:  0.16768974667790199
iteration : 4774
train acc:  0.703125
train loss:  0.5550050735473633
train gradient:  0.1527335844402402
iteration : 4775
train acc:  0.703125
train loss:  0.5497824549674988
train gradient:  0.1678109444263862
iteration : 4776
train acc:  0.7109375
train loss:  0.5555015802383423
train gradient:  0.1377125349442225
iteration : 4777
train acc:  0.765625
train loss:  0.4633375108242035
train gradient:  0.12775233555221122
iteration : 4778
train acc:  0.7578125
train loss:  0.5208182334899902
train gradient:  0.12377986779947223
iteration : 4779
train acc:  0.7421875
train loss:  0.49678683280944824
train gradient:  0.1428649686545538
iteration : 4780
train acc:  0.7890625
train loss:  0.521965742111206
train gradient:  0.15965311105458801
iteration : 4781
train acc:  0.7109375
train loss:  0.5565145611763
train gradient:  0.1765519060580236
iteration : 4782
train acc:  0.703125
train loss:  0.5293706059455872
train gradient:  0.13249843295689803
iteration : 4783
train acc:  0.6953125
train loss:  0.5206893682479858
train gradient:  0.14007125456754127
iteration : 4784
train acc:  0.6875
train loss:  0.5292176008224487
train gradient:  0.18526358757868702
iteration : 4785
train acc:  0.7734375
train loss:  0.482663094997406
train gradient:  0.14875322824749887
iteration : 4786
train acc:  0.6953125
train loss:  0.536647379398346
train gradient:  0.11606543545869975
iteration : 4787
train acc:  0.703125
train loss:  0.5213162899017334
train gradient:  0.1570988179808585
iteration : 4788
train acc:  0.6640625
train loss:  0.6115987300872803
train gradient:  0.20780822627312054
iteration : 4789
train acc:  0.734375
train loss:  0.558431088924408
train gradient:  0.14787461861369955
iteration : 4790
train acc:  0.7578125
train loss:  0.5248860120773315
train gradient:  0.1688545810162133
iteration : 4791
train acc:  0.6953125
train loss:  0.5440367460250854
train gradient:  0.17903859464655159
iteration : 4792
train acc:  0.71875
train loss:  0.5324841141700745
train gradient:  0.14066876802377337
iteration : 4793
train acc:  0.6796875
train loss:  0.5514527559280396
train gradient:  0.21981710695841342
iteration : 4794
train acc:  0.734375
train loss:  0.5214487910270691
train gradient:  0.16203717360537423
iteration : 4795
train acc:  0.6875
train loss:  0.5488934516906738
train gradient:  0.12804521894645196
iteration : 4796
train acc:  0.7421875
train loss:  0.5061013698577881
train gradient:  0.13164253503889534
iteration : 4797
train acc:  0.7421875
train loss:  0.5604357719421387
train gradient:  0.1693636154852985
iteration : 4798
train acc:  0.7109375
train loss:  0.5516067743301392
train gradient:  0.16449875887502857
iteration : 4799
train acc:  0.765625
train loss:  0.49834510684013367
train gradient:  0.15717881889247676
iteration : 4800
train acc:  0.78125
train loss:  0.5008191466331482
train gradient:  0.12899537579671555
iteration : 4801
train acc:  0.6484375
train loss:  0.6259733438491821
train gradient:  0.207485591425694
iteration : 4802
train acc:  0.6640625
train loss:  0.6166802644729614
train gradient:  0.21680431663344785
iteration : 4803
train acc:  0.75
train loss:  0.4706632196903229
train gradient:  0.15873074442113044
iteration : 4804
train acc:  0.734375
train loss:  0.47985389828681946
train gradient:  0.11323995642280273
iteration : 4805
train acc:  0.75
train loss:  0.4682537615299225
train gradient:  0.11657869184426026
iteration : 4806
train acc:  0.734375
train loss:  0.4930667281150818
train gradient:  0.13753205192056434
iteration : 4807
train acc:  0.7421875
train loss:  0.4856911599636078
train gradient:  0.14447243365386878
iteration : 4808
train acc:  0.6953125
train loss:  0.5155686140060425
train gradient:  0.13094185897477562
iteration : 4809
train acc:  0.7265625
train loss:  0.5368391871452332
train gradient:  0.17603496914167405
iteration : 4810
train acc:  0.7265625
train loss:  0.5265589952468872
train gradient:  0.11510482729714286
iteration : 4811
train acc:  0.7265625
train loss:  0.5113968849182129
train gradient:  0.16367180386022812
iteration : 4812
train acc:  0.75
train loss:  0.4816208779811859
train gradient:  0.15094455109489954
iteration : 4813
train acc:  0.796875
train loss:  0.4512249827384949
train gradient:  0.10721384494117107
iteration : 4814
train acc:  0.65625
train loss:  0.5485341548919678
train gradient:  0.16756415684159126
iteration : 4815
train acc:  0.71875
train loss:  0.4990135133266449
train gradient:  0.14933611276858788
iteration : 4816
train acc:  0.71875
train loss:  0.5598653554916382
train gradient:  0.16380940955530804
iteration : 4817
train acc:  0.78125
train loss:  0.4866620898246765
train gradient:  0.10963635102194667
iteration : 4818
train acc:  0.734375
train loss:  0.5437199473381042
train gradient:  0.19420478947305886
iteration : 4819
train acc:  0.7421875
train loss:  0.5127713680267334
train gradient:  0.15155891880543715
iteration : 4820
train acc:  0.6484375
train loss:  0.6048351526260376
train gradient:  0.17308304302583563
iteration : 4821
train acc:  0.796875
train loss:  0.4559260606765747
train gradient:  0.13571472727358297
iteration : 4822
train acc:  0.8046875
train loss:  0.4857161343097687
train gradient:  0.15780133989049355
iteration : 4823
train acc:  0.78125
train loss:  0.4799951910972595
train gradient:  0.13091455918509654
iteration : 4824
train acc:  0.7734375
train loss:  0.494978129863739
train gradient:  0.17756222144310158
iteration : 4825
train acc:  0.7578125
train loss:  0.46409744024276733
train gradient:  0.10804845225384514
iteration : 4826
train acc:  0.78125
train loss:  0.47049176692962646
train gradient:  0.13287874856342607
iteration : 4827
train acc:  0.7421875
train loss:  0.4866928160190582
train gradient:  0.10886422962292866
iteration : 4828
train acc:  0.703125
train loss:  0.5899488925933838
train gradient:  0.21993788487113852
iteration : 4829
train acc:  0.6953125
train loss:  0.5457270741462708
train gradient:  0.17237157234848174
iteration : 4830
train acc:  0.7265625
train loss:  0.48158425092697144
train gradient:  0.181639149536725
iteration : 4831
train acc:  0.6875
train loss:  0.526137113571167
train gradient:  0.16095686410234078
iteration : 4832
train acc:  0.703125
train loss:  0.5287652015686035
train gradient:  0.15152202276575089
iteration : 4833
train acc:  0.65625
train loss:  0.5605598092079163
train gradient:  0.1716063652792118
iteration : 4834
train acc:  0.6953125
train loss:  0.5417847633361816
train gradient:  0.17188888958314652
iteration : 4835
train acc:  0.7109375
train loss:  0.581611692905426
train gradient:  0.16409101799210776
iteration : 4836
train acc:  0.734375
train loss:  0.5104563236236572
train gradient:  0.14267361484549979
iteration : 4837
train acc:  0.71875
train loss:  0.5722302198410034
train gradient:  0.16150633919650764
iteration : 4838
train acc:  0.7421875
train loss:  0.5648398399353027
train gradient:  0.13630780536966497
iteration : 4839
train acc:  0.7578125
train loss:  0.47756388783454895
train gradient:  0.11465658821884744
iteration : 4840
train acc:  0.78125
train loss:  0.470103919506073
train gradient:  0.12883500497077657
iteration : 4841
train acc:  0.765625
train loss:  0.5089472532272339
train gradient:  0.1462159939021739
iteration : 4842
train acc:  0.6796875
train loss:  0.5704967975616455
train gradient:  0.16741118141018235
iteration : 4843
train acc:  0.7109375
train loss:  0.5273582935333252
train gradient:  0.14660623011948082
iteration : 4844
train acc:  0.71875
train loss:  0.4854345917701721
train gradient:  0.1368744448843967
iteration : 4845
train acc:  0.6953125
train loss:  0.544219434261322
train gradient:  0.1663537133271603
iteration : 4846
train acc:  0.765625
train loss:  0.4716734290122986
train gradient:  0.11987382885466141
iteration : 4847
train acc:  0.75
train loss:  0.48123699426651
train gradient:  0.14416645147871843
iteration : 4848
train acc:  0.703125
train loss:  0.5383999943733215
train gradient:  0.18180118837990544
iteration : 4849
train acc:  0.7890625
train loss:  0.45620661973953247
train gradient:  0.1188279377915453
iteration : 4850
train acc:  0.640625
train loss:  0.6270439624786377
train gradient:  0.2630984836018058
iteration : 4851
train acc:  0.71875
train loss:  0.48374199867248535
train gradient:  0.15003141197433334
iteration : 4852
train acc:  0.7421875
train loss:  0.5262641310691833
train gradient:  0.1282644753530187
iteration : 4853
train acc:  0.7578125
train loss:  0.4414282739162445
train gradient:  0.12392250848496239
iteration : 4854
train acc:  0.7734375
train loss:  0.4576417803764343
train gradient:  0.13728345645345025
iteration : 4855
train acc:  0.75
train loss:  0.4809000492095947
train gradient:  0.11728403304588596
iteration : 4856
train acc:  0.734375
train loss:  0.4737197160720825
train gradient:  0.11241115075918844
iteration : 4857
train acc:  0.7421875
train loss:  0.5112727880477905
train gradient:  0.12036998965470028
iteration : 4858
train acc:  0.671875
train loss:  0.5862841606140137
train gradient:  0.18385571670912101
iteration : 4859
train acc:  0.734375
train loss:  0.5487285852432251
train gradient:  0.20175694613415712
iteration : 4860
train acc:  0.7578125
train loss:  0.5148563385009766
train gradient:  0.16207488329240843
iteration : 4861
train acc:  0.703125
train loss:  0.5199148654937744
train gradient:  0.15764665657005822
iteration : 4862
train acc:  0.75
train loss:  0.5041897296905518
train gradient:  0.13434784472391503
iteration : 4863
train acc:  0.7421875
train loss:  0.5069650411605835
train gradient:  0.11913711711590842
iteration : 4864
train acc:  0.6796875
train loss:  0.5457199811935425
train gradient:  0.18382283068555538
iteration : 4865
train acc:  0.703125
train loss:  0.514049768447876
train gradient:  0.14967661435155044
iteration : 4866
train acc:  0.7109375
train loss:  0.5161644816398621
train gradient:  0.18760112037333937
iteration : 4867
train acc:  0.7421875
train loss:  0.4811244010925293
train gradient:  0.13260499452192068
iteration : 4868
train acc:  0.671875
train loss:  0.5372205972671509
train gradient:  0.18020655283613035
iteration : 4869
train acc:  0.671875
train loss:  0.5164772868156433
train gradient:  0.15216018931928943
iteration : 4870
train acc:  0.734375
train loss:  0.5140897035598755
train gradient:  0.12473290304115517
iteration : 4871
train acc:  0.734375
train loss:  0.5102244019508362
train gradient:  0.1529974357811914
iteration : 4872
train acc:  0.703125
train loss:  0.5495917797088623
train gradient:  0.13139723348083643
iteration : 4873
train acc:  0.7421875
train loss:  0.5004705786705017
train gradient:  0.1092242705666013
iteration : 4874
train acc:  0.8046875
train loss:  0.4600420296192169
train gradient:  0.1056089525493921
iteration : 4875
train acc:  0.7265625
train loss:  0.5264681577682495
train gradient:  0.13027522688827073
iteration : 4876
train acc:  0.75
train loss:  0.478512167930603
train gradient:  0.11430340403072453
iteration : 4877
train acc:  0.6875
train loss:  0.5597611665725708
train gradient:  0.14674662218710272
iteration : 4878
train acc:  0.75
train loss:  0.5105377435684204
train gradient:  0.12613212893875225
iteration : 4879
train acc:  0.625
train loss:  0.5546806454658508
train gradient:  0.1591697982400878
iteration : 4880
train acc:  0.734375
train loss:  0.520095705986023
train gradient:  0.1340566107697524
iteration : 4881
train acc:  0.7421875
train loss:  0.4758795499801636
train gradient:  0.18576635571124425
iteration : 4882
train acc:  0.703125
train loss:  0.5805408954620361
train gradient:  0.16145618736669715
iteration : 4883
train acc:  0.75
train loss:  0.5064921379089355
train gradient:  0.12965926744269318
iteration : 4884
train acc:  0.7578125
train loss:  0.49000927805900574
train gradient:  0.1701013393756311
iteration : 4885
train acc:  0.71875
train loss:  0.5130385756492615
train gradient:  0.11862486060451734
iteration : 4886
train acc:  0.6953125
train loss:  0.5815016031265259
train gradient:  0.1818559417989848
iteration : 4887
train acc:  0.6953125
train loss:  0.5629550218582153
train gradient:  0.1451058524895972
iteration : 4888
train acc:  0.734375
train loss:  0.5296085476875305
train gradient:  0.13838088762028405
iteration : 4889
train acc:  0.7109375
train loss:  0.49616676568984985
train gradient:  0.145203661096986
iteration : 4890
train acc:  0.765625
train loss:  0.4820047616958618
train gradient:  0.19716196616417148
iteration : 4891
train acc:  0.6953125
train loss:  0.5553285479545593
train gradient:  0.16523370377928276
iteration : 4892
train acc:  0.7109375
train loss:  0.5124044418334961
train gradient:  0.11964493004519942
iteration : 4893
train acc:  0.59375
train loss:  0.6081091165542603
train gradient:  0.18859419504368702
iteration : 4894
train acc:  0.7109375
train loss:  0.5258223414421082
train gradient:  0.1726561070618735
iteration : 4895
train acc:  0.6953125
train loss:  0.5414572358131409
train gradient:  0.14180499108066313
iteration : 4896
train acc:  0.765625
train loss:  0.4837266802787781
train gradient:  0.1299996239962492
iteration : 4897
train acc:  0.7265625
train loss:  0.5200756788253784
train gradient:  0.18689292432244942
iteration : 4898
train acc:  0.765625
train loss:  0.46857574582099915
train gradient:  0.13574962318972722
iteration : 4899
train acc:  0.7578125
train loss:  0.479451984167099
train gradient:  0.13778959608767538
iteration : 4900
train acc:  0.7109375
train loss:  0.5504437685012817
train gradient:  0.27823474435277673
iteration : 4901
train acc:  0.765625
train loss:  0.553196907043457
train gradient:  0.164024671227936
iteration : 4902
train acc:  0.71875
train loss:  0.5202420949935913
train gradient:  0.177376593834216
iteration : 4903
train acc:  0.6953125
train loss:  0.5568249225616455
train gradient:  0.17578079179459682
iteration : 4904
train acc:  0.6875
train loss:  0.5665295124053955
train gradient:  0.15049817287583017
iteration : 4905
train acc:  0.7265625
train loss:  0.5356189012527466
train gradient:  0.1682420231932755
iteration : 4906
train acc:  0.796875
train loss:  0.41665759682655334
train gradient:  0.1046892056188224
iteration : 4907
train acc:  0.7109375
train loss:  0.5074881315231323
train gradient:  0.16697254449368593
iteration : 4908
train acc:  0.7109375
train loss:  0.5483627319335938
train gradient:  0.14970623048282186
iteration : 4909
train acc:  0.6484375
train loss:  0.5883365869522095
train gradient:  0.15807547367355645
iteration : 4910
train acc:  0.6640625
train loss:  0.5662517547607422
train gradient:  0.17766377227212737
iteration : 4911
train acc:  0.7109375
train loss:  0.5413840413093567
train gradient:  0.16569706443808913
iteration : 4912
train acc:  0.7890625
train loss:  0.49484431743621826
train gradient:  0.14042175405768476
iteration : 4913
train acc:  0.7265625
train loss:  0.49450284242630005
train gradient:  0.1988944183356033
iteration : 4914
train acc:  0.7265625
train loss:  0.5279456377029419
train gradient:  0.13570673360152052
iteration : 4915
train acc:  0.6640625
train loss:  0.5802726745605469
train gradient:  0.23019679279939703
iteration : 4916
train acc:  0.796875
train loss:  0.4860093593597412
train gradient:  0.10775885126982639
iteration : 4917
train acc:  0.7734375
train loss:  0.464194655418396
train gradient:  0.10952687304202814
iteration : 4918
train acc:  0.7421875
train loss:  0.5423622727394104
train gradient:  0.1784941878924327
iteration : 4919
train acc:  0.796875
train loss:  0.4685666859149933
train gradient:  0.12099045714623753
iteration : 4920
train acc:  0.765625
train loss:  0.472973495721817
train gradient:  0.10325255967652791
iteration : 4921
train acc:  0.7109375
train loss:  0.5472538471221924
train gradient:  0.14580168654627496
iteration : 4922
train acc:  0.7265625
train loss:  0.5710275769233704
train gradient:  0.2027796091989028
iteration : 4923
train acc:  0.703125
train loss:  0.5538277626037598
train gradient:  0.14646305846732086
iteration : 4924
train acc:  0.65625
train loss:  0.5812267065048218
train gradient:  0.19684736457024937
iteration : 4925
train acc:  0.7578125
train loss:  0.5394813418388367
train gradient:  0.17518350403821592
iteration : 4926
train acc:  0.7421875
train loss:  0.5239888429641724
train gradient:  0.15417597045751436
iteration : 4927
train acc:  0.703125
train loss:  0.5312275290489197
train gradient:  0.1705448549468702
iteration : 4928
train acc:  0.734375
train loss:  0.4757397770881653
train gradient:  0.12138005697618319
iteration : 4929
train acc:  0.6484375
train loss:  0.5776433944702148
train gradient:  0.1734934136529943
iteration : 4930
train acc:  0.7109375
train loss:  0.5052670240402222
train gradient:  0.14046911242621868
iteration : 4931
train acc:  0.7578125
train loss:  0.4709743559360504
train gradient:  0.12059494764025391
iteration : 4932
train acc:  0.7109375
train loss:  0.5176420211791992
train gradient:  0.13228941984942005
iteration : 4933
train acc:  0.671875
train loss:  0.5480841398239136
train gradient:  0.14937598635871852
iteration : 4934
train acc:  0.765625
train loss:  0.5063587427139282
train gradient:  0.1489928358069213
iteration : 4935
train acc:  0.7109375
train loss:  0.4766824543476105
train gradient:  0.1282049973570859
iteration : 4936
train acc:  0.671875
train loss:  0.5852910280227661
train gradient:  0.1743234341312624
iteration : 4937
train acc:  0.6796875
train loss:  0.6031848788261414
train gradient:  0.1574407820180316
iteration : 4938
train acc:  0.75
train loss:  0.4903610944747925
train gradient:  0.12105108090523549
iteration : 4939
train acc:  0.765625
train loss:  0.4586176574230194
train gradient:  0.1728526325345724
iteration : 4940
train acc:  0.828125
train loss:  0.4775082468986511
train gradient:  0.12784933194753406
iteration : 4941
train acc:  0.7578125
train loss:  0.5141310691833496
train gradient:  0.14805067507088587
iteration : 4942
train acc:  0.671875
train loss:  0.5999957323074341
train gradient:  0.18428735583494887
iteration : 4943
train acc:  0.7421875
train loss:  0.5024371147155762
train gradient:  0.12772581742470623
iteration : 4944
train acc:  0.7421875
train loss:  0.4912113845348358
train gradient:  0.12071641512693147
iteration : 4945
train acc:  0.75
train loss:  0.5030030608177185
train gradient:  0.1294414737043869
iteration : 4946
train acc:  0.75
train loss:  0.5212374925613403
train gradient:  0.17119559785807947
iteration : 4947
train acc:  0.71875
train loss:  0.525504469871521
train gradient:  0.1334820277571448
iteration : 4948
train acc:  0.734375
train loss:  0.4722636938095093
train gradient:  0.16130327791798726
iteration : 4949
train acc:  0.671875
train loss:  0.5601215362548828
train gradient:  0.15281295344754928
iteration : 4950
train acc:  0.7265625
train loss:  0.5401536226272583
train gradient:  0.18278824534975968
iteration : 4951
train acc:  0.7109375
train loss:  0.5685887932777405
train gradient:  0.1567585874607162
iteration : 4952
train acc:  0.78125
train loss:  0.4480188488960266
train gradient:  0.11367068410907326
iteration : 4953
train acc:  0.7734375
train loss:  0.5079969167709351
train gradient:  0.13409571282897823
iteration : 4954
train acc:  0.6953125
train loss:  0.5294683575630188
train gradient:  0.1295562154571277
iteration : 4955
train acc:  0.7265625
train loss:  0.4743824601173401
train gradient:  0.14822813184703382
iteration : 4956
train acc:  0.6875
train loss:  0.5594689846038818
train gradient:  0.14443531250811914
iteration : 4957
train acc:  0.6953125
train loss:  0.5500041842460632
train gradient:  0.17305931723682205
iteration : 4958
train acc:  0.7578125
train loss:  0.4485589861869812
train gradient:  0.0943856561319157
iteration : 4959
train acc:  0.7734375
train loss:  0.4768984913825989
train gradient:  0.11665312162906129
iteration : 4960
train acc:  0.6640625
train loss:  0.5774028897285461
train gradient:  0.13517724051304852
iteration : 4961
train acc:  0.71875
train loss:  0.498060405254364
train gradient:  0.10401106205535625
iteration : 4962
train acc:  0.7578125
train loss:  0.4834265410900116
train gradient:  0.1361392969471652
iteration : 4963
train acc:  0.7109375
train loss:  0.5629374384880066
train gradient:  0.1883304603279129
iteration : 4964
train acc:  0.7109375
train loss:  0.5437312722206116
train gradient:  0.14865823162683828
iteration : 4965
train acc:  0.75
train loss:  0.48124271631240845
train gradient:  0.11479550625668644
iteration : 4966
train acc:  0.6640625
train loss:  0.5769355893135071
train gradient:  0.150670448417291
iteration : 4967
train acc:  0.75
train loss:  0.4786970615386963
train gradient:  0.13334727453954542
iteration : 4968
train acc:  0.765625
train loss:  0.48213493824005127
train gradient:  0.15287872459700025
iteration : 4969
train acc:  0.6953125
train loss:  0.5220475792884827
train gradient:  0.1297461620977608
iteration : 4970
train acc:  0.7109375
train loss:  0.5323648452758789
train gradient:  0.14236170078197652
iteration : 4971
train acc:  0.75
train loss:  0.506564199924469
train gradient:  0.1444159583176533
iteration : 4972
train acc:  0.7265625
train loss:  0.577223539352417
train gradient:  0.18485248890403702
iteration : 4973
train acc:  0.703125
train loss:  0.541276216506958
train gradient:  0.16832251715794416
iteration : 4974
train acc:  0.75
train loss:  0.4963495135307312
train gradient:  0.1296045456526213
iteration : 4975
train acc:  0.734375
train loss:  0.49689093232154846
train gradient:  0.1605846348143679
iteration : 4976
train acc:  0.7265625
train loss:  0.5448020100593567
train gradient:  0.17843801721516306
iteration : 4977
train acc:  0.75
train loss:  0.5003575086593628
train gradient:  0.11973162654551829
iteration : 4978
train acc:  0.7109375
train loss:  0.5227582454681396
train gradient:  0.15683045335278167
iteration : 4979
train acc:  0.6953125
train loss:  0.5855386257171631
train gradient:  0.1859884544864308
iteration : 4980
train acc:  0.765625
train loss:  0.4903408885002136
train gradient:  0.12588425530396616
iteration : 4981
train acc:  0.71875
train loss:  0.49087071418762207
train gradient:  0.1284368889554043
iteration : 4982
train acc:  0.7578125
train loss:  0.4895239472389221
train gradient:  0.11333996471015953
iteration : 4983
train acc:  0.7890625
train loss:  0.4771674573421478
train gradient:  0.10360741380617415
iteration : 4984
train acc:  0.703125
train loss:  0.5445036888122559
train gradient:  0.14119897444388285
iteration : 4985
train acc:  0.7265625
train loss:  0.46167612075805664
train gradient:  0.14371735182325504
iteration : 4986
train acc:  0.6953125
train loss:  0.5640578269958496
train gradient:  0.14876445377058528
iteration : 4987
train acc:  0.671875
train loss:  0.5416231155395508
train gradient:  0.1987111062747826
iteration : 4988
train acc:  0.7109375
train loss:  0.5233309268951416
train gradient:  0.14484174441223452
iteration : 4989
train acc:  0.6953125
train loss:  0.5797328948974609
train gradient:  0.20181125980366332
iteration : 4990
train acc:  0.75
train loss:  0.4549482464790344
train gradient:  0.1165447135835188
iteration : 4991
train acc:  0.7109375
train loss:  0.5083138942718506
train gradient:  0.107286976999784
iteration : 4992
train acc:  0.78125
train loss:  0.44039490818977356
train gradient:  0.15271762111673565
iteration : 4993
train acc:  0.7734375
train loss:  0.44996458292007446
train gradient:  0.12433267581021525
iteration : 4994
train acc:  0.703125
train loss:  0.5493941307067871
train gradient:  0.13599987084401277
iteration : 4995
train acc:  0.765625
train loss:  0.47677385807037354
train gradient:  0.1448227625936494
iteration : 4996
train acc:  0.6796875
train loss:  0.5678250193595886
train gradient:  0.1916250907187093
iteration : 4997
train acc:  0.703125
train loss:  0.5344236493110657
train gradient:  0.1813033396728304
iteration : 4998
train acc:  0.7265625
train loss:  0.44965749979019165
train gradient:  0.14477350435644598
iteration : 4999
train acc:  0.6875
train loss:  0.4954071044921875
train gradient:  0.14483126663620288
iteration : 5000
train acc:  0.6796875
train loss:  0.5192180871963501
train gradient:  0.16456371002764825
iteration : 5001
train acc:  0.71875
train loss:  0.4869786500930786
train gradient:  0.1163704156416655
iteration : 5002
train acc:  0.7421875
train loss:  0.5084215402603149
train gradient:  0.146564634750219
iteration : 5003
train acc:  0.765625
train loss:  0.46333134174346924
train gradient:  0.11103668558217655
iteration : 5004
train acc:  0.6796875
train loss:  0.5767385959625244
train gradient:  0.2090604037614231
iteration : 5005
train acc:  0.734375
train loss:  0.4632735252380371
train gradient:  0.14129415861668632
iteration : 5006
train acc:  0.7421875
train loss:  0.5039045810699463
train gradient:  0.14280115944737848
iteration : 5007
train acc:  0.6953125
train loss:  0.5213533639907837
train gradient:  0.17919679844705522
iteration : 5008
train acc:  0.7890625
train loss:  0.4659786820411682
train gradient:  0.12890235584845108
iteration : 5009
train acc:  0.7890625
train loss:  0.46096640825271606
train gradient:  0.13175757764919793
iteration : 5010
train acc:  0.78125
train loss:  0.4494588077068329
train gradient:  0.12396341304151183
iteration : 5011
train acc:  0.6640625
train loss:  0.5742163062095642
train gradient:  0.17579396667749697
iteration : 5012
train acc:  0.7421875
train loss:  0.4814014732837677
train gradient:  0.14612222099628014
iteration : 5013
train acc:  0.734375
train loss:  0.5262622237205505
train gradient:  0.15200933182924786
iteration : 5014
train acc:  0.796875
train loss:  0.48953482508659363
train gradient:  0.1391757713728914
iteration : 5015
train acc:  0.6640625
train loss:  0.6014664173126221
train gradient:  0.1851105342064271
iteration : 5016
train acc:  0.65625
train loss:  0.5813392400741577
train gradient:  0.18892710931438522
iteration : 5017
train acc:  0.703125
train loss:  0.5170267820358276
train gradient:  0.16264122992229024
iteration : 5018
train acc:  0.640625
train loss:  0.5700860619544983
train gradient:  0.1823631500338814
iteration : 5019
train acc:  0.734375
train loss:  0.4736891984939575
train gradient:  0.15022754705445157
iteration : 5020
train acc:  0.8125
train loss:  0.4404541254043579
train gradient:  0.12965067218344678
iteration : 5021
train acc:  0.765625
train loss:  0.4820619225502014
train gradient:  0.13327706041079973
iteration : 5022
train acc:  0.6796875
train loss:  0.5538296699523926
train gradient:  0.18965725018011498
iteration : 5023
train acc:  0.7578125
train loss:  0.4870578646659851
train gradient:  0.1604233693590578
iteration : 5024
train acc:  0.6875
train loss:  0.5724509358406067
train gradient:  0.14756141509511667
iteration : 5025
train acc:  0.7421875
train loss:  0.5402305126190186
train gradient:  0.14520974413649795
iteration : 5026
train acc:  0.7578125
train loss:  0.5006991624832153
train gradient:  0.15029469074762425
iteration : 5027
train acc:  0.78125
train loss:  0.4587101936340332
train gradient:  0.14386892601856688
iteration : 5028
train acc:  0.7734375
train loss:  0.4800124764442444
train gradient:  0.13011411843827486
iteration : 5029
train acc:  0.7578125
train loss:  0.5437057614326477
train gradient:  0.21983643130735236
iteration : 5030
train acc:  0.6875
train loss:  0.5680598020553589
train gradient:  0.2197906594646882
iteration : 5031
train acc:  0.7890625
train loss:  0.5060467720031738
train gradient:  0.15450270355203294
iteration : 5032
train acc:  0.7421875
train loss:  0.48576682806015015
train gradient:  0.1270066074839456
iteration : 5033
train acc:  0.7421875
train loss:  0.51034015417099
train gradient:  0.15365604904590208
iteration : 5034
train acc:  0.75
train loss:  0.5273202657699585
train gradient:  0.14846479458894368
iteration : 5035
train acc:  0.7265625
train loss:  0.5411974191665649
train gradient:  0.13285168799796598
iteration : 5036
train acc:  0.734375
train loss:  0.5103499889373779
train gradient:  0.16936198353257642
iteration : 5037
train acc:  0.8125
train loss:  0.4109480082988739
train gradient:  0.11385476661473486
iteration : 5038
train acc:  0.6640625
train loss:  0.5784459114074707
train gradient:  0.1603187303261307
iteration : 5039
train acc:  0.7109375
train loss:  0.5538467764854431
train gradient:  0.1864636946588304
iteration : 5040
train acc:  0.65625
train loss:  0.6385867595672607
train gradient:  0.18306215273897977
iteration : 5041
train acc:  0.765625
train loss:  0.4811304807662964
train gradient:  0.14192660618994152
iteration : 5042
train acc:  0.6796875
train loss:  0.5432640910148621
train gradient:  0.1388250116097564
iteration : 5043
train acc:  0.6796875
train loss:  0.6124840974807739
train gradient:  0.253802048310446
iteration : 5044
train acc:  0.71875
train loss:  0.5487962961196899
train gradient:  0.24547975572144526
iteration : 5045
train acc:  0.734375
train loss:  0.5180504322052002
train gradient:  0.16864486217890762
iteration : 5046
train acc:  0.734375
train loss:  0.5193027853965759
train gradient:  0.1298786509837591
iteration : 5047
train acc:  0.765625
train loss:  0.4926968514919281
train gradient:  0.15848992635598047
iteration : 5048
train acc:  0.78125
train loss:  0.47574445605278015
train gradient:  0.1430010013564062
iteration : 5049
train acc:  0.6953125
train loss:  0.5654664039611816
train gradient:  0.1966924229578012
iteration : 5050
train acc:  0.7421875
train loss:  0.5725740194320679
train gradient:  0.2095402237940578
iteration : 5051
train acc:  0.7421875
train loss:  0.4826178550720215
train gradient:  0.11879170941405742
iteration : 5052
train acc:  0.6796875
train loss:  0.5314875841140747
train gradient:  0.14234430173856538
iteration : 5053
train acc:  0.71875
train loss:  0.46749046444892883
train gradient:  0.10361873254237844
iteration : 5054
train acc:  0.7578125
train loss:  0.4750804305076599
train gradient:  0.13645173802247773
iteration : 5055
train acc:  0.7421875
train loss:  0.5130807757377625
train gradient:  0.16664872808723374
iteration : 5056
train acc:  0.7734375
train loss:  0.4473966360092163
train gradient:  0.11684843187151328
iteration : 5057
train acc:  0.7421875
train loss:  0.5002408623695374
train gradient:  0.11813295875927064
iteration : 5058
train acc:  0.7890625
train loss:  0.45863354206085205
train gradient:  0.1337505367574053
iteration : 5059
train acc:  0.7578125
train loss:  0.5292074680328369
train gradient:  0.1605484544105075
iteration : 5060
train acc:  0.7578125
train loss:  0.538939356803894
train gradient:  0.17987920632947657
iteration : 5061
train acc:  0.75
train loss:  0.4858691096305847
train gradient:  0.12380087806028328
iteration : 5062
train acc:  0.7734375
train loss:  0.45535269379615784
train gradient:  0.11819652735257659
iteration : 5063
train acc:  0.71875
train loss:  0.4906929135322571
train gradient:  0.15561990289417144
iteration : 5064
train acc:  0.75
train loss:  0.5291879773139954
train gradient:  0.16814072112958572
iteration : 5065
train acc:  0.75
train loss:  0.46850714087486267
train gradient:  0.1507945693981717
iteration : 5066
train acc:  0.7734375
train loss:  0.4904485046863556
train gradient:  0.14007901988695204
iteration : 5067
train acc:  0.7421875
train loss:  0.5153936147689819
train gradient:  0.13799412597846863
iteration : 5068
train acc:  0.6875
train loss:  0.5681694746017456
train gradient:  0.20299671459335333
iteration : 5069
train acc:  0.6953125
train loss:  0.612006425857544
train gradient:  0.2380547107013113
iteration : 5070
train acc:  0.71875
train loss:  0.5223740339279175
train gradient:  0.16136174998086994
iteration : 5071
train acc:  0.7421875
train loss:  0.4673597812652588
train gradient:  0.12331246411403683
iteration : 5072
train acc:  0.75
train loss:  0.45577937364578247
train gradient:  0.12305410007879519
iteration : 5073
train acc:  0.7265625
train loss:  0.5442901849746704
train gradient:  0.13054461314009508
iteration : 5074
train acc:  0.71875
train loss:  0.4889436364173889
train gradient:  0.1271111291801696
iteration : 5075
train acc:  0.7421875
train loss:  0.4982680082321167
train gradient:  0.15984631262546806
iteration : 5076
train acc:  0.734375
train loss:  0.4964768588542938
train gradient:  0.16376560067187385
iteration : 5077
train acc:  0.7109375
train loss:  0.5769269466400146
train gradient:  0.18208225723951638
iteration : 5078
train acc:  0.75
train loss:  0.43929699063301086
train gradient:  0.11083456152044528
iteration : 5079
train acc:  0.71875
train loss:  0.4967552423477173
train gradient:  0.14540491287642604
iteration : 5080
train acc:  0.7265625
train loss:  0.5423291921615601
train gradient:  0.18109068869364153
iteration : 5081
train acc:  0.703125
train loss:  0.49653345346450806
train gradient:  0.10550272345412617
iteration : 5082
train acc:  0.734375
train loss:  0.4847146272659302
train gradient:  0.17068533029260827
iteration : 5083
train acc:  0.6875
train loss:  0.5386194586753845
train gradient:  0.16403976483948582
iteration : 5084
train acc:  0.734375
train loss:  0.4972267150878906
train gradient:  0.12658621840811232
iteration : 5085
train acc:  0.8203125
train loss:  0.4447742998600006
train gradient:  0.11675581743258931
iteration : 5086
train acc:  0.7578125
train loss:  0.49586620926856995
train gradient:  0.1663997007465869
iteration : 5087
train acc:  0.7265625
train loss:  0.4940798282623291
train gradient:  0.12331247257871487
iteration : 5088
train acc:  0.7109375
train loss:  0.5014209151268005
train gradient:  0.19415555872612927
iteration : 5089
train acc:  0.703125
train loss:  0.5257441997528076
train gradient:  0.20784526579823626
iteration : 5090
train acc:  0.6875
train loss:  0.5823090672492981
train gradient:  0.15021073469020296
iteration : 5091
train acc:  0.7578125
train loss:  0.4703323543071747
train gradient:  0.124523628576485
iteration : 5092
train acc:  0.796875
train loss:  0.4326571524143219
train gradient:  0.11186580916078613
iteration : 5093
train acc:  0.7734375
train loss:  0.45335841178894043
train gradient:  0.11196373984765731
iteration : 5094
train acc:  0.6953125
train loss:  0.5657948851585388
train gradient:  0.20592303118153493
iteration : 5095
train acc:  0.765625
train loss:  0.5135173797607422
train gradient:  0.15168547139636684
iteration : 5096
train acc:  0.828125
train loss:  0.4160223603248596
train gradient:  0.10921550054801943
iteration : 5097
train acc:  0.6796875
train loss:  0.5353873372077942
train gradient:  0.1468626551896005
iteration : 5098
train acc:  0.7734375
train loss:  0.43544790148735046
train gradient:  0.14689934479682087
iteration : 5099
train acc:  0.75
train loss:  0.4947005808353424
train gradient:  0.1512179924104164
iteration : 5100
train acc:  0.7265625
train loss:  0.5346265435218811
train gradient:  0.15537536904826732
iteration : 5101
train acc:  0.671875
train loss:  0.6276983618736267
train gradient:  0.27560495252368167
iteration : 5102
train acc:  0.71875
train loss:  0.5103787183761597
train gradient:  0.1462071352766644
iteration : 5103
train acc:  0.7421875
train loss:  0.4778980016708374
train gradient:  0.11477199724927105
iteration : 5104
train acc:  0.6875
train loss:  0.5389493703842163
train gradient:  0.12062517279081683
iteration : 5105
train acc:  0.765625
train loss:  0.474787175655365
train gradient:  0.10746194793324564
iteration : 5106
train acc:  0.7890625
train loss:  0.45468395948410034
train gradient:  0.12175733109272892
iteration : 5107
train acc:  0.78125
train loss:  0.466099351644516
train gradient:  0.12000766657670472
iteration : 5108
train acc:  0.6875
train loss:  0.5660862922668457
train gradient:  0.14995310983976182
iteration : 5109
train acc:  0.6796875
train loss:  0.5594255924224854
train gradient:  0.1614521501577416
iteration : 5110
train acc:  0.7890625
train loss:  0.4283050298690796
train gradient:  0.1323296746451097
iteration : 5111
train acc:  0.6875
train loss:  0.5766590237617493
train gradient:  0.16448611676351915
iteration : 5112
train acc:  0.7265625
train loss:  0.5587303638458252
train gradient:  0.19390768540692022
iteration : 5113
train acc:  0.7421875
train loss:  0.507082462310791
train gradient:  0.16983840997154753
iteration : 5114
train acc:  0.71875
train loss:  0.5582677125930786
train gradient:  0.20038493165111226
iteration : 5115
train acc:  0.7265625
train loss:  0.5618774890899658
train gradient:  0.21091861333100648
iteration : 5116
train acc:  0.7578125
train loss:  0.5232285261154175
train gradient:  0.18368012915340715
iteration : 5117
train acc:  0.71875
train loss:  0.5331184267997742
train gradient:  0.18265929726167274
iteration : 5118
train acc:  0.7109375
train loss:  0.523831844329834
train gradient:  0.16693130125635847
iteration : 5119
train acc:  0.7421875
train loss:  0.4920980930328369
train gradient:  0.1386678493145705
iteration : 5120
train acc:  0.671875
train loss:  0.5885486602783203
train gradient:  0.16162675944252686
iteration : 5121
train acc:  0.6796875
train loss:  0.5924466848373413
train gradient:  0.17250274753116018
iteration : 5122
train acc:  0.7265625
train loss:  0.5319294333457947
train gradient:  0.17182789185156638
iteration : 5123
train acc:  0.6796875
train loss:  0.5657639503479004
train gradient:  0.16919733236732992
iteration : 5124
train acc:  0.78125
train loss:  0.49146950244903564
train gradient:  0.12886034344928476
iteration : 5125
train acc:  0.7734375
train loss:  0.49399757385253906
train gradient:  0.12420099897753557
iteration : 5126
train acc:  0.7578125
train loss:  0.47086840867996216
train gradient:  0.12715924426629005
iteration : 5127
train acc:  0.7734375
train loss:  0.4637712240219116
train gradient:  0.11720760427547011
iteration : 5128
train acc:  0.765625
train loss:  0.49087923765182495
train gradient:  0.14588949915738253
iteration : 5129
train acc:  0.7734375
train loss:  0.4965987801551819
train gradient:  0.11114874536599628
iteration : 5130
train acc:  0.7265625
train loss:  0.5262820720672607
train gradient:  0.13727436720839414
iteration : 5131
train acc:  0.6953125
train loss:  0.5282617807388306
train gradient:  0.15388576655854008
iteration : 5132
train acc:  0.75
train loss:  0.49942949414253235
train gradient:  0.1112915818163487
iteration : 5133
train acc:  0.6875
train loss:  0.5337162613868713
train gradient:  0.15349026559251522
iteration : 5134
train acc:  0.7265625
train loss:  0.5248856544494629
train gradient:  0.15106502336783412
iteration : 5135
train acc:  0.6875
train loss:  0.5131749510765076
train gradient:  0.15232320365907948
iteration : 5136
train acc:  0.703125
train loss:  0.5563635230064392
train gradient:  0.14797029046032806
iteration : 5137
train acc:  0.6875
train loss:  0.57239830493927
train gradient:  0.26034128508471155
iteration : 5138
train acc:  0.765625
train loss:  0.4820892810821533
train gradient:  0.131896319663376
iteration : 5139
train acc:  0.71875
train loss:  0.514882504940033
train gradient:  0.18018484999619033
iteration : 5140
train acc:  0.7109375
train loss:  0.542978048324585
train gradient:  0.16202322087755322
iteration : 5141
train acc:  0.734375
train loss:  0.486411452293396
train gradient:  0.11767456837533896
iteration : 5142
train acc:  0.6640625
train loss:  0.5260268449783325
train gradient:  0.13575986362580894
iteration : 5143
train acc:  0.7578125
train loss:  0.4403221309185028
train gradient:  0.1241869522387945
iteration : 5144
train acc:  0.7421875
train loss:  0.5082240104675293
train gradient:  0.15189859591946042
iteration : 5145
train acc:  0.8046875
train loss:  0.44542643427848816
train gradient:  0.10516579543978317
iteration : 5146
train acc:  0.7265625
train loss:  0.5843861103057861
train gradient:  0.1928612042581664
iteration : 5147
train acc:  0.7265625
train loss:  0.5441606044769287
train gradient:  0.1696927017232916
iteration : 5148
train acc:  0.65625
train loss:  0.5817121267318726
train gradient:  0.25232689595364377
iteration : 5149
train acc:  0.71875
train loss:  0.575685977935791
train gradient:  0.1654481887824932
iteration : 5150
train acc:  0.7109375
train loss:  0.5053609013557434
train gradient:  0.17749139961087024
iteration : 5151
train acc:  0.7734375
train loss:  0.43942105770111084
train gradient:  0.13720531687315385
iteration : 5152
train acc:  0.6328125
train loss:  0.6203970909118652
train gradient:  0.22593669877986639
iteration : 5153
train acc:  0.734375
train loss:  0.4955323338508606
train gradient:  0.1476912354502793
iteration : 5154
train acc:  0.703125
train loss:  0.5534213185310364
train gradient:  0.16743978504918688
iteration : 5155
train acc:  0.75
train loss:  0.46579623222351074
train gradient:  0.12950041343302662
iteration : 5156
train acc:  0.6796875
train loss:  0.5873767137527466
train gradient:  0.16518727853941081
iteration : 5157
train acc:  0.7265625
train loss:  0.561627984046936
train gradient:  0.16717080963733463
iteration : 5158
train acc:  0.6796875
train loss:  0.5327785015106201
train gradient:  0.1333753174221784
iteration : 5159
train acc:  0.7734375
train loss:  0.458569198846817
train gradient:  0.1326287799245546
iteration : 5160
train acc:  0.671875
train loss:  0.5527900457382202
train gradient:  0.14977999769192812
iteration : 5161
train acc:  0.7578125
train loss:  0.46744564175605774
train gradient:  0.1132243368113206
iteration : 5162
train acc:  0.71875
train loss:  0.50746750831604
train gradient:  0.13064291064340366
iteration : 5163
train acc:  0.7109375
train loss:  0.4806533455848694
train gradient:  0.15049747065102065
iteration : 5164
train acc:  0.7265625
train loss:  0.5374997854232788
train gradient:  0.18415436821887732
iteration : 5165
train acc:  0.6953125
train loss:  0.5260250568389893
train gradient:  0.13023757760978028
iteration : 5166
train acc:  0.71875
train loss:  0.5173488259315491
train gradient:  0.1732851065856219
iteration : 5167
train acc:  0.7109375
train loss:  0.5496358871459961
train gradient:  0.172338128319046
iteration : 5168
train acc:  0.7265625
train loss:  0.5007736086845398
train gradient:  0.1371200850333097
iteration : 5169
train acc:  0.734375
train loss:  0.5114734172821045
train gradient:  0.15900851354037443
iteration : 5170
train acc:  0.734375
train loss:  0.5323808789253235
train gradient:  0.1741494552109428
iteration : 5171
train acc:  0.7578125
train loss:  0.4705130457878113
train gradient:  0.14983062040520156
iteration : 5172
train acc:  0.7109375
train loss:  0.5103292465209961
train gradient:  0.12405157887175222
iteration : 5173
train acc:  0.703125
train loss:  0.5040780305862427
train gradient:  0.18512239929517443
iteration : 5174
train acc:  0.734375
train loss:  0.4733061194419861
train gradient:  0.1436766474681045
iteration : 5175
train acc:  0.7578125
train loss:  0.4901663362979889
train gradient:  0.14844411247693123
iteration : 5176
train acc:  0.734375
train loss:  0.46766817569732666
train gradient:  0.137198750935418
iteration : 5177
train acc:  0.6796875
train loss:  0.5340334177017212
train gradient:  0.164925466326997
iteration : 5178
train acc:  0.78125
train loss:  0.45778244733810425
train gradient:  0.10703531538929599
iteration : 5179
train acc:  0.6875
train loss:  0.5590643882751465
train gradient:  0.1965692544845429
iteration : 5180
train acc:  0.734375
train loss:  0.5119071006774902
train gradient:  0.15774096162455925
iteration : 5181
train acc:  0.75
train loss:  0.5647809505462646
train gradient:  0.2203168963025155
iteration : 5182
train acc:  0.734375
train loss:  0.4987829923629761
train gradient:  0.1892727710529995
iteration : 5183
train acc:  0.7734375
train loss:  0.4777965545654297
train gradient:  0.11781588185040714
iteration : 5184
train acc:  0.7421875
train loss:  0.5172014236450195
train gradient:  0.1397745241838735
iteration : 5185
train acc:  0.703125
train loss:  0.5545482039451599
train gradient:  0.13495295793452547
iteration : 5186
train acc:  0.671875
train loss:  0.5718892812728882
train gradient:  0.1558046239138122
iteration : 5187
train acc:  0.71875
train loss:  0.5362834334373474
train gradient:  0.17909220175107146
iteration : 5188
train acc:  0.734375
train loss:  0.5139868855476379
train gradient:  0.16487027805647375
iteration : 5189
train acc:  0.71875
train loss:  0.5567615032196045
train gradient:  0.15946013074255483
iteration : 5190
train acc:  0.7109375
train loss:  0.5667088031768799
train gradient:  0.1867083255128278
iteration : 5191
train acc:  0.7421875
train loss:  0.4763084352016449
train gradient:  0.1086666544641882
iteration : 5192
train acc:  0.7421875
train loss:  0.5271700024604797
train gradient:  0.16465949869785906
iteration : 5193
train acc:  0.7578125
train loss:  0.4616333842277527
train gradient:  0.1487833279028555
iteration : 5194
train acc:  0.75
train loss:  0.4813353419303894
train gradient:  0.1454963292726984
iteration : 5195
train acc:  0.65625
train loss:  0.6018354892730713
train gradient:  0.19875299025571325
iteration : 5196
train acc:  0.734375
train loss:  0.4928129017353058
train gradient:  0.14625574836276933
iteration : 5197
train acc:  0.7109375
train loss:  0.5149299502372742
train gradient:  0.21460148678780178
iteration : 5198
train acc:  0.75
train loss:  0.46131622791290283
train gradient:  0.13288263173969742
iteration : 5199
train acc:  0.765625
train loss:  0.4807319939136505
train gradient:  0.12526288829032808
iteration : 5200
train acc:  0.7421875
train loss:  0.4870139956474304
train gradient:  0.10427811036093357
iteration : 5201
train acc:  0.6875
train loss:  0.5370850563049316
train gradient:  0.14789406242111344
iteration : 5202
train acc:  0.734375
train loss:  0.5071533918380737
train gradient:  0.13067704610958603
iteration : 5203
train acc:  0.8046875
train loss:  0.45899373292922974
train gradient:  0.10496703990887282
iteration : 5204
train acc:  0.78125
train loss:  0.47666847705841064
train gradient:  0.12418121920979219
iteration : 5205
train acc:  0.6953125
train loss:  0.551925539970398
train gradient:  0.17427286642279172
iteration : 5206
train acc:  0.7578125
train loss:  0.5022525191307068
train gradient:  0.11640448585024121
iteration : 5207
train acc:  0.6953125
train loss:  0.5489622950553894
train gradient:  0.1797313719248434
iteration : 5208
train acc:  0.75
train loss:  0.517214834690094
train gradient:  0.15317122409953576
iteration : 5209
train acc:  0.7265625
train loss:  0.5091491937637329
train gradient:  0.16432891689433576
iteration : 5210
train acc:  0.7578125
train loss:  0.5370463728904724
train gradient:  0.13863354855275856
iteration : 5211
train acc:  0.6640625
train loss:  0.5963248014450073
train gradient:  0.16808677740117656
iteration : 5212
train acc:  0.6953125
train loss:  0.5307981371879578
train gradient:  0.13654831766225567
iteration : 5213
train acc:  0.6953125
train loss:  0.5592018961906433
train gradient:  0.17147752506434055
iteration : 5214
train acc:  0.7890625
train loss:  0.44124317169189453
train gradient:  0.18276022541630565
iteration : 5215
train acc:  0.75
train loss:  0.4851853847503662
train gradient:  0.128016762583694
iteration : 5216
train acc:  0.78125
train loss:  0.46043556928634644
train gradient:  0.11716596265927166
iteration : 5217
train acc:  0.71875
train loss:  0.49681323766708374
train gradient:  0.12069726650083312
iteration : 5218
train acc:  0.7734375
train loss:  0.4588605761528015
train gradient:  0.10557504781616338
iteration : 5219
train acc:  0.6875
train loss:  0.585726261138916
train gradient:  0.2022365072872584
iteration : 5220
train acc:  0.7578125
train loss:  0.4833364188671112
train gradient:  0.13832061825882008
iteration : 5221
train acc:  0.6953125
train loss:  0.49592676758766174
train gradient:  0.15030049997782213
iteration : 5222
train acc:  0.765625
train loss:  0.43087857961654663
train gradient:  0.11765539047993587
iteration : 5223
train acc:  0.7265625
train loss:  0.5049240589141846
train gradient:  0.13224522453151016
iteration : 5224
train acc:  0.71875
train loss:  0.5103582739830017
train gradient:  0.11799546483632921
iteration : 5225
train acc:  0.8125
train loss:  0.4379235506057739
train gradient:  0.1034922304369197
iteration : 5226
train acc:  0.6875
train loss:  0.5203046798706055
train gradient:  0.13288573603849677
iteration : 5227
train acc:  0.7578125
train loss:  0.4893215298652649
train gradient:  0.13853213108115356
iteration : 5228
train acc:  0.765625
train loss:  0.4739387631416321
train gradient:  0.135111726113953
iteration : 5229
train acc:  0.75
train loss:  0.5663100481033325
train gradient:  0.17557425360952472
iteration : 5230
train acc:  0.6796875
train loss:  0.5604838132858276
train gradient:  0.20145993434806564
iteration : 5231
train acc:  0.7578125
train loss:  0.4432644844055176
train gradient:  0.15577128868685963
iteration : 5232
train acc:  0.7578125
train loss:  0.49783647060394287
train gradient:  0.12719069378369544
iteration : 5233
train acc:  0.7421875
train loss:  0.5018000602722168
train gradient:  0.1705568154204149
iteration : 5234
train acc:  0.671875
train loss:  0.5708911418914795
train gradient:  0.19078084211630209
iteration : 5235
train acc:  0.78125
train loss:  0.5085004568099976
train gradient:  0.14343733832906225
iteration : 5236
train acc:  0.75
train loss:  0.4873505234718323
train gradient:  0.16606081978278114
iteration : 5237
train acc:  0.7578125
train loss:  0.45733046531677246
train gradient:  0.13214818358316127
iteration : 5238
train acc:  0.7734375
train loss:  0.4701218008995056
train gradient:  0.21212257900522588
iteration : 5239
train acc:  0.6796875
train loss:  0.5687230825424194
train gradient:  0.19030265327644377
iteration : 5240
train acc:  0.7734375
train loss:  0.47867313027381897
train gradient:  0.17490212604076166
iteration : 5241
train acc:  0.71875
train loss:  0.5408429503440857
train gradient:  0.2035272074993455
iteration : 5242
train acc:  0.6953125
train loss:  0.560968279838562
train gradient:  0.21134002684457665
iteration : 5243
train acc:  0.75
train loss:  0.4663577079772949
train gradient:  0.11509038108316666
iteration : 5244
train acc:  0.7265625
train loss:  0.49706608057022095
train gradient:  0.1221631439135622
iteration : 5245
train acc:  0.6875
train loss:  0.5666453242301941
train gradient:  0.1827313219592111
iteration : 5246
train acc:  0.78125
train loss:  0.4545557498931885
train gradient:  0.15749196105927873
iteration : 5247
train acc:  0.765625
train loss:  0.4724469482898712
train gradient:  0.13027453561473548
iteration : 5248
train acc:  0.8125
train loss:  0.4025306701660156
train gradient:  0.10789855453489398
iteration : 5249
train acc:  0.6953125
train loss:  0.5161095261573792
train gradient:  0.1865108792991496
iteration : 5250
train acc:  0.78125
train loss:  0.47443950176239014
train gradient:  0.1277287805094508
iteration : 5251
train acc:  0.7109375
train loss:  0.4973696172237396
train gradient:  0.1570429324183732
iteration : 5252
train acc:  0.7109375
train loss:  0.5345640778541565
train gradient:  0.14968389180714403
iteration : 5253
train acc:  0.7890625
train loss:  0.4821031093597412
train gradient:  0.143220526366175
iteration : 5254
train acc:  0.734375
train loss:  0.5126069784164429
train gradient:  0.16325746258059176
iteration : 5255
train acc:  0.7421875
train loss:  0.523505449295044
train gradient:  0.17454805602690554
iteration : 5256
train acc:  0.71875
train loss:  0.5289654731750488
train gradient:  0.1656089914283751
iteration : 5257
train acc:  0.8046875
train loss:  0.4694082736968994
train gradient:  0.14426123705866825
iteration : 5258
train acc:  0.6796875
train loss:  0.580859899520874
train gradient:  0.19790461884608984
iteration : 5259
train acc:  0.6953125
train loss:  0.5360937714576721
train gradient:  0.14189181479708696
iteration : 5260
train acc:  0.7265625
train loss:  0.5008425712585449
train gradient:  0.16733238318314553
iteration : 5261
train acc:  0.7578125
train loss:  0.5135751962661743
train gradient:  0.14003316535321364
iteration : 5262
train acc:  0.75
train loss:  0.4928539991378784
train gradient:  0.13755599441275468
iteration : 5263
train acc:  0.7265625
train loss:  0.5205936431884766
train gradient:  0.193440352515295
iteration : 5264
train acc:  0.7421875
train loss:  0.4687610864639282
train gradient:  0.12248018055042334
iteration : 5265
train acc:  0.7734375
train loss:  0.4973132610321045
train gradient:  0.13016511503887912
iteration : 5266
train acc:  0.7265625
train loss:  0.5149106383323669
train gradient:  0.17593508588733442
iteration : 5267
train acc:  0.7265625
train loss:  0.5144291520118713
train gradient:  0.13692744754428474
iteration : 5268
train acc:  0.6953125
train loss:  0.5284751653671265
train gradient:  0.16670874683108067
iteration : 5269
train acc:  0.734375
train loss:  0.5623388886451721
train gradient:  0.1703901240508016
iteration : 5270
train acc:  0.8046875
train loss:  0.425515741109848
train gradient:  0.1315001748959384
iteration : 5271
train acc:  0.6484375
train loss:  0.6632770299911499
train gradient:  0.2868722245928738
iteration : 5272
train acc:  0.6953125
train loss:  0.5382815599441528
train gradient:  0.17774592870819772
iteration : 5273
train acc:  0.75
train loss:  0.505506157875061
train gradient:  0.13198627652537354
iteration : 5274
train acc:  0.734375
train loss:  0.5446880459785461
train gradient:  0.21498801170002288
iteration : 5275
train acc:  0.7890625
train loss:  0.47552454471588135
train gradient:  0.12040497811847117
iteration : 5276
train acc:  0.7734375
train loss:  0.4550200402736664
train gradient:  0.11971272976516352
iteration : 5277
train acc:  0.7578125
train loss:  0.49788692593574524
train gradient:  0.10075663661910877
iteration : 5278
train acc:  0.671875
train loss:  0.5567405223846436
train gradient:  0.18646194511542274
iteration : 5279
train acc:  0.71875
train loss:  0.5342279672622681
train gradient:  0.15567964143642343
iteration : 5280
train acc:  0.6953125
train loss:  0.5258185863494873
train gradient:  0.1448748148130019
iteration : 5281
train acc:  0.703125
train loss:  0.5483697056770325
train gradient:  0.15744567268293624
iteration : 5282
train acc:  0.765625
train loss:  0.522038459777832
train gradient:  0.1664837472931685
iteration : 5283
train acc:  0.7421875
train loss:  0.4753152132034302
train gradient:  0.14317971332186752
iteration : 5284
train acc:  0.703125
train loss:  0.49636757373809814
train gradient:  0.14011441292624616
iteration : 5285
train acc:  0.7578125
train loss:  0.5131100416183472
train gradient:  0.12751856414970264
iteration : 5286
train acc:  0.640625
train loss:  0.6017193794250488
train gradient:  0.17554587997871302
iteration : 5287
train acc:  0.765625
train loss:  0.4754793643951416
train gradient:  0.1110534421059952
iteration : 5288
train acc:  0.7265625
train loss:  0.5537445545196533
train gradient:  0.16341585061891384
iteration : 5289
train acc:  0.734375
train loss:  0.45877012610435486
train gradient:  0.11548114730520011
iteration : 5290
train acc:  0.7421875
train loss:  0.5437964797019958
train gradient:  0.15267433769171368
iteration : 5291
train acc:  0.765625
train loss:  0.4847218096256256
train gradient:  0.14100464983790667
iteration : 5292
train acc:  0.6796875
train loss:  0.5090975165367126
train gradient:  0.1365581752732463
iteration : 5293
train acc:  0.65625
train loss:  0.5554588437080383
train gradient:  0.19001434918876176
iteration : 5294
train acc:  0.703125
train loss:  0.5593231320381165
train gradient:  0.19837128771181686
iteration : 5295
train acc:  0.7421875
train loss:  0.5006064176559448
train gradient:  0.1265087420447213
iteration : 5296
train acc:  0.6328125
train loss:  0.6035687923431396
train gradient:  0.21197263520395404
iteration : 5297
train acc:  0.671875
train loss:  0.5628765821456909
train gradient:  0.18118030469913468
iteration : 5298
train acc:  0.7578125
train loss:  0.4872400164604187
train gradient:  0.12341506628120026
iteration : 5299
train acc:  0.6953125
train loss:  0.5925726890563965
train gradient:  0.21695488133267057
iteration : 5300
train acc:  0.765625
train loss:  0.44330689311027527
train gradient:  0.10785667456879856
iteration : 5301
train acc:  0.7109375
train loss:  0.5206263661384583
train gradient:  0.15817602445260892
iteration : 5302
train acc:  0.75
train loss:  0.48945242166519165
train gradient:  0.1320970424374574
iteration : 5303
train acc:  0.7265625
train loss:  0.5046910643577576
train gradient:  0.14263877939613712
iteration : 5304
train acc:  0.75
train loss:  0.5236673355102539
train gradient:  0.1394376504298651
iteration : 5305
train acc:  0.7578125
train loss:  0.5273534059524536
train gradient:  0.16148729085699887
iteration : 5306
train acc:  0.65625
train loss:  0.6229192614555359
train gradient:  0.20234960578525957
iteration : 5307
train acc:  0.7109375
train loss:  0.5257440805435181
train gradient:  0.17803961669674978
iteration : 5308
train acc:  0.7734375
train loss:  0.5143142938613892
train gradient:  0.1922456595972753
iteration : 5309
train acc:  0.7578125
train loss:  0.5130472183227539
train gradient:  0.17173037330355417
iteration : 5310
train acc:  0.796875
train loss:  0.49984967708587646
train gradient:  0.12282629736358901
iteration : 5311
train acc:  0.6953125
train loss:  0.5398138761520386
train gradient:  0.13642100319251665
iteration : 5312
train acc:  0.7734375
train loss:  0.47010719776153564
train gradient:  0.12022578626564948
iteration : 5313
train acc:  0.8046875
train loss:  0.4336237609386444
train gradient:  0.10824417103118439
iteration : 5314
train acc:  0.765625
train loss:  0.46550285816192627
train gradient:  0.12279799218386948
iteration : 5315
train acc:  0.7265625
train loss:  0.5130560398101807
train gradient:  0.13704557608971174
iteration : 5316
train acc:  0.640625
train loss:  0.6056373119354248
train gradient:  0.241227804418363
iteration : 5317
train acc:  0.796875
train loss:  0.4132898449897766
train gradient:  0.1160142146517101
iteration : 5318
train acc:  0.75
train loss:  0.49278897047042847
train gradient:  0.1272489041956066
iteration : 5319
train acc:  0.703125
train loss:  0.5023701786994934
train gradient:  0.12744258370453904
iteration : 5320
train acc:  0.6171875
train loss:  0.5918156504631042
train gradient:  0.16698765851688244
iteration : 5321
train acc:  0.7265625
train loss:  0.46754953265190125
train gradient:  0.10618554452250192
iteration : 5322
train acc:  0.75
train loss:  0.4773189425468445
train gradient:  0.12643376344111062
iteration : 5323
train acc:  0.796875
train loss:  0.43437081575393677
train gradient:  0.11901589934038749
iteration : 5324
train acc:  0.7421875
train loss:  0.5007421970367432
train gradient:  0.14412227271700512
iteration : 5325
train acc:  0.6875
train loss:  0.5035252571105957
train gradient:  0.13491509183233716
iteration : 5326
train acc:  0.6953125
train loss:  0.5645502805709839
train gradient:  0.19408482059846946
iteration : 5327
train acc:  0.734375
train loss:  0.5214319229125977
train gradient:  0.24504148071529647
iteration : 5328
train acc:  0.6875
train loss:  0.5603028535842896
train gradient:  0.1606169526609561
iteration : 5329
train acc:  0.75
train loss:  0.4793848395347595
train gradient:  0.12860744829039444
iteration : 5330
train acc:  0.765625
train loss:  0.46550410985946655
train gradient:  0.14499265295574443
iteration : 5331
train acc:  0.7421875
train loss:  0.4757721722126007
train gradient:  0.16791398406905156
iteration : 5332
train acc:  0.65625
train loss:  0.5989229679107666
train gradient:  0.2601196394615653
iteration : 5333
train acc:  0.765625
train loss:  0.4656391143798828
train gradient:  0.12799864304617367
iteration : 5334
train acc:  0.671875
train loss:  0.5661108493804932
train gradient:  0.14158452868362018
iteration : 5335
train acc:  0.75
train loss:  0.4737950265407562
train gradient:  0.14129614020813325
iteration : 5336
train acc:  0.7109375
train loss:  0.5517400503158569
train gradient:  0.157590852206413
iteration : 5337
train acc:  0.75
train loss:  0.4833782911300659
train gradient:  0.13259007155844207
iteration : 5338
train acc:  0.765625
train loss:  0.5049148797988892
train gradient:  0.14825578364852326
iteration : 5339
train acc:  0.6875
train loss:  0.5514441728591919
train gradient:  0.16655975487218014
iteration : 5340
train acc:  0.7578125
train loss:  0.5212814807891846
train gradient:  0.16443462990630608
iteration : 5341
train acc:  0.75
train loss:  0.4841937720775604
train gradient:  0.1535156848998345
iteration : 5342
train acc:  0.7421875
train loss:  0.4586109220981598
train gradient:  0.13493382701147572
iteration : 5343
train acc:  0.7421875
train loss:  0.4852455258369446
train gradient:  0.13428777173551615
iteration : 5344
train acc:  0.6640625
train loss:  0.5967456102371216
train gradient:  0.19313571857290504
iteration : 5345
train acc:  0.671875
train loss:  0.5301731824874878
train gradient:  0.16311616233805964
iteration : 5346
train acc:  0.6953125
train loss:  0.5692823529243469
train gradient:  0.14504979103318297
iteration : 5347
train acc:  0.71875
train loss:  0.4843531548976898
train gradient:  0.1547001036632607
iteration : 5348
train acc:  0.7421875
train loss:  0.4824867248535156
train gradient:  0.14995352512164628
iteration : 5349
train acc:  0.828125
train loss:  0.45301321148872375
train gradient:  0.14385964321058317
iteration : 5350
train acc:  0.7578125
train loss:  0.4797600507736206
train gradient:  0.14756834378563854
iteration : 5351
train acc:  0.6796875
train loss:  0.5824135541915894
train gradient:  0.21507718731294057
iteration : 5352
train acc:  0.71875
train loss:  0.4977158308029175
train gradient:  0.14581949154815577
iteration : 5353
train acc:  0.7109375
train loss:  0.5171768665313721
train gradient:  0.15366156791383412
iteration : 5354
train acc:  0.7421875
train loss:  0.5025951266288757
train gradient:  0.1534377509938116
iteration : 5355
train acc:  0.734375
train loss:  0.5429635047912598
train gradient:  0.13996158969242084
iteration : 5356
train acc:  0.671875
train loss:  0.5726307034492493
train gradient:  0.2046746307644906
iteration : 5357
train acc:  0.734375
train loss:  0.4893991947174072
train gradient:  0.14295004730195268
iteration : 5358
train acc:  0.765625
train loss:  0.47344258427619934
train gradient:  0.1383756297220024
iteration : 5359
train acc:  0.7109375
train loss:  0.5358372926712036
train gradient:  0.14983837916726078
iteration : 5360
train acc:  0.7109375
train loss:  0.5366029739379883
train gradient:  0.19232808639374305
iteration : 5361
train acc:  0.71875
train loss:  0.566409707069397
train gradient:  0.21373766664399257
iteration : 5362
train acc:  0.671875
train loss:  0.5721633434295654
train gradient:  0.15457664907401075
iteration : 5363
train acc:  0.796875
train loss:  0.44563722610473633
train gradient:  0.11655286454923938
iteration : 5364
train acc:  0.7578125
train loss:  0.4736960530281067
train gradient:  0.1517474425587887
iteration : 5365
train acc:  0.7578125
train loss:  0.5231252908706665
train gradient:  0.1806562535124015
iteration : 5366
train acc:  0.6953125
train loss:  0.524042010307312
train gradient:  0.20585935746249268
iteration : 5367
train acc:  0.6796875
train loss:  0.5456879734992981
train gradient:  0.13272460309691453
iteration : 5368
train acc:  0.6796875
train loss:  0.5988049507141113
train gradient:  0.2884494523809984
iteration : 5369
train acc:  0.703125
train loss:  0.5251344442367554
train gradient:  0.14581585354254145
iteration : 5370
train acc:  0.6796875
train loss:  0.5861228704452515
train gradient:  0.16875343787047986
iteration : 5371
train acc:  0.75
train loss:  0.4919833540916443
train gradient:  0.15453310111734828
iteration : 5372
train acc:  0.6640625
train loss:  0.5305541753768921
train gradient:  0.14528160106515237
iteration : 5373
train acc:  0.671875
train loss:  0.5574976801872253
train gradient:  0.18419152075175876
iteration : 5374
train acc:  0.71875
train loss:  0.49306976795196533
train gradient:  0.12996830493488454
iteration : 5375
train acc:  0.7734375
train loss:  0.4720306992530823
train gradient:  0.12915571037629398
iteration : 5376
train acc:  0.75
train loss:  0.4599224925041199
train gradient:  0.11869142871425731
iteration : 5377
train acc:  0.7734375
train loss:  0.5126359462738037
train gradient:  0.17331809450582836
iteration : 5378
train acc:  0.7109375
train loss:  0.4940643906593323
train gradient:  0.11800922559082104
iteration : 5379
train acc:  0.8046875
train loss:  0.4453407824039459
train gradient:  0.11148845230415642
iteration : 5380
train acc:  0.796875
train loss:  0.4571390151977539
train gradient:  0.1369197305243104
iteration : 5381
train acc:  0.7421875
train loss:  0.4848508834838867
train gradient:  0.16198558394813922
iteration : 5382
train acc:  0.71875
train loss:  0.5307185649871826
train gradient:  0.1899531629482897
iteration : 5383
train acc:  0.7421875
train loss:  0.4754570424556732
train gradient:  0.1324758932933407
iteration : 5384
train acc:  0.75
train loss:  0.4865013360977173
train gradient:  0.15001806602658097
iteration : 5385
train acc:  0.71875
train loss:  0.48408475518226624
train gradient:  0.11214021636374912
iteration : 5386
train acc:  0.7265625
train loss:  0.5038280487060547
train gradient:  0.22512219312716716
iteration : 5387
train acc:  0.7578125
train loss:  0.4799272418022156
train gradient:  0.12219363484838515
iteration : 5388
train acc:  0.703125
train loss:  0.5312968492507935
train gradient:  0.22292028103960526
iteration : 5389
train acc:  0.6875
train loss:  0.5666118860244751
train gradient:  0.23728410939542915
iteration : 5390
train acc:  0.7421875
train loss:  0.5170968770980835
train gradient:  0.14631029987498936
iteration : 5391
train acc:  0.65625
train loss:  0.5386990904808044
train gradient:  0.1271435474322935
iteration : 5392
train acc:  0.78125
train loss:  0.4286652207374573
train gradient:  0.1203963147057629
iteration : 5393
train acc:  0.8046875
train loss:  0.46249449253082275
train gradient:  0.12283503832654391
iteration : 5394
train acc:  0.703125
train loss:  0.5601083040237427
train gradient:  0.1656191948263039
iteration : 5395
train acc:  0.71875
train loss:  0.5171484351158142
train gradient:  0.18294344276658986
iteration : 5396
train acc:  0.7421875
train loss:  0.49789196252822876
train gradient:  0.16387777487368438
iteration : 5397
train acc:  0.6796875
train loss:  0.5538727045059204
train gradient:  0.14642738406091538
iteration : 5398
train acc:  0.75
train loss:  0.5095944404602051
train gradient:  0.14527492040006595
iteration : 5399
train acc:  0.7421875
train loss:  0.5045417547225952
train gradient:  0.19038784368010114
iteration : 5400
train acc:  0.703125
train loss:  0.5161824226379395
train gradient:  0.1482735458514307
iteration : 5401
train acc:  0.7890625
train loss:  0.43529585003852844
train gradient:  0.10278998341768951
iteration : 5402
train acc:  0.734375
train loss:  0.48154303431510925
train gradient:  0.15441497874631607
iteration : 5403
train acc:  0.65625
train loss:  0.5575979351997375
train gradient:  0.14816179446761576
iteration : 5404
train acc:  0.796875
train loss:  0.4297422766685486
train gradient:  0.10953812272215954
iteration : 5405
train acc:  0.7734375
train loss:  0.4929124116897583
train gradient:  0.12943790315461073
iteration : 5406
train acc:  0.6953125
train loss:  0.5751256346702576
train gradient:  0.15615766119200286
iteration : 5407
train acc:  0.671875
train loss:  0.5856096744537354
train gradient:  0.20813699437690886
iteration : 5408
train acc:  0.7578125
train loss:  0.5221860408782959
train gradient:  0.16215528695058962
iteration : 5409
train acc:  0.75
train loss:  0.4895631670951843
train gradient:  0.1120796332948646
iteration : 5410
train acc:  0.640625
train loss:  0.5714871287345886
train gradient:  0.16053893164843608
iteration : 5411
train acc:  0.7109375
train loss:  0.52754145860672
train gradient:  0.15323819688122897
iteration : 5412
train acc:  0.671875
train loss:  0.5496192574501038
train gradient:  0.17196928328936667
iteration : 5413
train acc:  0.703125
train loss:  0.511193037033081
train gradient:  0.19062218179394402
iteration : 5414
train acc:  0.71875
train loss:  0.5161665678024292
train gradient:  0.15107981927876885
iteration : 5415
train acc:  0.671875
train loss:  0.5890668630599976
train gradient:  0.16641736717543154
iteration : 5416
train acc:  0.7265625
train loss:  0.5216155052185059
train gradient:  0.17979133301570221
iteration : 5417
train acc:  0.6796875
train loss:  0.525140643119812
train gradient:  0.1472836711250772
iteration : 5418
train acc:  0.625
train loss:  0.6073049306869507
train gradient:  0.2106031311784814
iteration : 5419
train acc:  0.7265625
train loss:  0.5713026523590088
train gradient:  0.14770704862689082
iteration : 5420
train acc:  0.71875
train loss:  0.5174937844276428
train gradient:  0.16157287392512998
iteration : 5421
train acc:  0.78125
train loss:  0.5140062570571899
train gradient:  0.15215978009929845
iteration : 5422
train acc:  0.7421875
train loss:  0.5243031978607178
train gradient:  0.14036833130485077
iteration : 5423
train acc:  0.7421875
train loss:  0.5520254969596863
train gradient:  0.16769907189546635
iteration : 5424
train acc:  0.7734375
train loss:  0.4467172622680664
train gradient:  0.08558370943187325
iteration : 5425
train acc:  0.828125
train loss:  0.40813392400741577
train gradient:  0.09709392085554079
iteration : 5426
train acc:  0.8359375
train loss:  0.4211958050727844
train gradient:  0.10138563209687834
iteration : 5427
train acc:  0.7578125
train loss:  0.5220960378646851
train gradient:  0.17988457109713518
iteration : 5428
train acc:  0.703125
train loss:  0.5169171094894409
train gradient:  0.15496247077699385
iteration : 5429
train acc:  0.7265625
train loss:  0.4968995451927185
train gradient:  0.15294951498426823
iteration : 5430
train acc:  0.734375
train loss:  0.4900713562965393
train gradient:  0.1918044526684739
iteration : 5431
train acc:  0.6796875
train loss:  0.5996519327163696
train gradient:  0.16603178414551012
iteration : 5432
train acc:  0.6328125
train loss:  0.6062368154525757
train gradient:  0.16862461311228572
iteration : 5433
train acc:  0.7578125
train loss:  0.47822514176368713
train gradient:  0.12841319687591393
iteration : 5434
train acc:  0.7109375
train loss:  0.5272067785263062
train gradient:  0.15080328811463373
iteration : 5435
train acc:  0.703125
train loss:  0.5258085131645203
train gradient:  0.15766910116264718
iteration : 5436
train acc:  0.703125
train loss:  0.5569003820419312
train gradient:  0.14453771670942578
iteration : 5437
train acc:  0.7109375
train loss:  0.4968058168888092
train gradient:  0.13029257433677927
iteration : 5438
train acc:  0.7578125
train loss:  0.45639270544052124
train gradient:  0.13960637823510202
iteration : 5439
train acc:  0.6953125
train loss:  0.5326724052429199
train gradient:  0.1521459260481764
iteration : 5440
train acc:  0.7421875
train loss:  0.5496657490730286
train gradient:  0.17767917231684915
iteration : 5441
train acc:  0.75
train loss:  0.5071496367454529
train gradient:  0.1280832906362579
iteration : 5442
train acc:  0.75
train loss:  0.48009729385375977
train gradient:  0.15722319770286236
iteration : 5443
train acc:  0.7734375
train loss:  0.5316393971443176
train gradient:  0.1951955447993421
iteration : 5444
train acc:  0.703125
train loss:  0.5041623115539551
train gradient:  0.13195654675614962
iteration : 5445
train acc:  0.703125
train loss:  0.5105593204498291
train gradient:  0.11729393993901496
iteration : 5446
train acc:  0.7890625
train loss:  0.4581465721130371
train gradient:  0.12484494296929642
iteration : 5447
train acc:  0.7421875
train loss:  0.4467746615409851
train gradient:  0.11603441683457388
iteration : 5448
train acc:  0.7421875
train loss:  0.4993646740913391
train gradient:  0.13717387187911242
iteration : 5449
train acc:  0.6796875
train loss:  0.5632954239845276
train gradient:  0.16366073905999123
iteration : 5450
train acc:  0.7578125
train loss:  0.4955310821533203
train gradient:  0.13551933818246933
iteration : 5451
train acc:  0.7265625
train loss:  0.47914397716522217
train gradient:  0.14900282583028862
iteration : 5452
train acc:  0.7421875
train loss:  0.4944525957107544
train gradient:  0.12708271601743404
iteration : 5453
train acc:  0.75
train loss:  0.4742541015148163
train gradient:  0.11134396855476232
iteration : 5454
train acc:  0.7734375
train loss:  0.4773896634578705
train gradient:  0.10843305722838553
iteration : 5455
train acc:  0.75
train loss:  0.464579701423645
train gradient:  0.12255062729950719
iteration : 5456
train acc:  0.7109375
train loss:  0.5140440464019775
train gradient:  0.1581783394152686
iteration : 5457
train acc:  0.734375
train loss:  0.470776230096817
train gradient:  0.13295579948790598
iteration : 5458
train acc:  0.7421875
train loss:  0.518642783164978
train gradient:  0.14785195156495354
iteration : 5459
train acc:  0.796875
train loss:  0.5016182661056519
train gradient:  0.12178466464899249
iteration : 5460
train acc:  0.7421875
train loss:  0.4725971519947052
train gradient:  0.12554050672394787
iteration : 5461
train acc:  0.6875
train loss:  0.5316753387451172
train gradient:  0.14698640595770723
iteration : 5462
train acc:  0.71875
train loss:  0.5007792711257935
train gradient:  0.13685185606930345
iteration : 5463
train acc:  0.7578125
train loss:  0.4923740327358246
train gradient:  0.15226479336194515
iteration : 5464
train acc:  0.71875
train loss:  0.49226078391075134
train gradient:  0.13401786730435147
iteration : 5465
train acc:  0.7109375
train loss:  0.5358390808105469
train gradient:  0.14469901779553285
iteration : 5466
train acc:  0.75
train loss:  0.5113785862922668
train gradient:  0.13731021714546096
iteration : 5467
train acc:  0.7734375
train loss:  0.49318927526474
train gradient:  0.12000876057490338
iteration : 5468
train acc:  0.703125
train loss:  0.5456503033638
train gradient:  0.1438864568296076
iteration : 5469
train acc:  0.7734375
train loss:  0.4232839345932007
train gradient:  0.11651260346161882
iteration : 5470
train acc:  0.734375
train loss:  0.4915398955345154
train gradient:  0.13531683219178947
iteration : 5471
train acc:  0.71875
train loss:  0.512511670589447
train gradient:  0.15562191148942633
iteration : 5472
train acc:  0.7578125
train loss:  0.48870787024497986
train gradient:  0.1426221114062745
iteration : 5473
train acc:  0.6953125
train loss:  0.5150210857391357
train gradient:  0.17480752184465348
iteration : 5474
train acc:  0.7734375
train loss:  0.4724608063697815
train gradient:  0.09793973051344992
iteration : 5475
train acc:  0.71875
train loss:  0.520393967628479
train gradient:  0.15544287942300428
iteration : 5476
train acc:  0.703125
train loss:  0.5171447992324829
train gradient:  0.1922323175481529
iteration : 5477
train acc:  0.8046875
train loss:  0.46464329957962036
train gradient:  0.10436083933218716
iteration : 5478
train acc:  0.78125
train loss:  0.4729212522506714
train gradient:  0.2018427787631641
iteration : 5479
train acc:  0.6640625
train loss:  0.5282264351844788
train gradient:  0.15232993120717425
iteration : 5480
train acc:  0.7109375
train loss:  0.5440762042999268
train gradient:  0.1685232580994722
iteration : 5481
train acc:  0.7421875
train loss:  0.5136643648147583
train gradient:  0.13825228790704133
iteration : 5482
train acc:  0.6875
train loss:  0.5738012194633484
train gradient:  0.20467532713840889
iteration : 5483
train acc:  0.703125
train loss:  0.518250584602356
train gradient:  0.15278802810109732
iteration : 5484
train acc:  0.7421875
train loss:  0.45973825454711914
train gradient:  0.12619738755796397
iteration : 5485
train acc:  0.78125
train loss:  0.52126145362854
train gradient:  0.16249970588935392
iteration : 5486
train acc:  0.734375
train loss:  0.5118035078048706
train gradient:  0.12261365425028527
iteration : 5487
train acc:  0.7109375
train loss:  0.5371156930923462
train gradient:  0.13790444023664283
iteration : 5488
train acc:  0.734375
train loss:  0.48384249210357666
train gradient:  0.14185827463670964
iteration : 5489
train acc:  0.765625
train loss:  0.49047091603279114
train gradient:  0.14243298196328402
iteration : 5490
train acc:  0.734375
train loss:  0.49828195571899414
train gradient:  0.14307756900353147
iteration : 5491
train acc:  0.734375
train loss:  0.5119553804397583
train gradient:  0.14216217519054167
iteration : 5492
train acc:  0.75
train loss:  0.576392650604248
train gradient:  0.16474921641385204
iteration : 5493
train acc:  0.765625
train loss:  0.5061407089233398
train gradient:  0.15547920282563987
iteration : 5494
train acc:  0.71875
train loss:  0.5356959700584412
train gradient:  0.1866328164974414
iteration : 5495
train acc:  0.7578125
train loss:  0.48629191517829895
train gradient:  0.2085649287373504
iteration : 5496
train acc:  0.65625
train loss:  0.5692254900932312
train gradient:  0.27287936989756434
iteration : 5497
train acc:  0.7109375
train loss:  0.5331703424453735
train gradient:  0.15176759053814476
iteration : 5498
train acc:  0.71875
train loss:  0.49539920687675476
train gradient:  0.17879061943076685
iteration : 5499
train acc:  0.7578125
train loss:  0.45759066939353943
train gradient:  0.13386129845012357
iteration : 5500
train acc:  0.75
train loss:  0.4568807780742645
train gradient:  0.10635896603587916
iteration : 5501
train acc:  0.75
train loss:  0.5003347992897034
train gradient:  0.15394100697347263
iteration : 5502
train acc:  0.7734375
train loss:  0.48911356925964355
train gradient:  0.1300588944578689
iteration : 5503
train acc:  0.7578125
train loss:  0.4592793583869934
train gradient:  0.14313123009182266
iteration : 5504
train acc:  0.7578125
train loss:  0.452568918466568
train gradient:  0.1247158320449213
iteration : 5505
train acc:  0.7578125
train loss:  0.4774940609931946
train gradient:  0.11771344468666123
iteration : 5506
train acc:  0.703125
train loss:  0.5071127414703369
train gradient:  0.16785697888027223
iteration : 5507
train acc:  0.734375
train loss:  0.49663543701171875
train gradient:  0.1501386393234982
iteration : 5508
train acc:  0.671875
train loss:  0.5335239171981812
train gradient:  0.20087065621178368
iteration : 5509
train acc:  0.71875
train loss:  0.49653270840644836
train gradient:  0.14996018848754922
iteration : 5510
train acc:  0.75
train loss:  0.49082812666893005
train gradient:  0.18868568161907884
iteration : 5511
train acc:  0.7734375
train loss:  0.4692501723766327
train gradient:  0.11771614106942138
iteration : 5512
train acc:  0.6796875
train loss:  0.5573022365570068
train gradient:  0.16228798856170695
iteration : 5513
train acc:  0.6796875
train loss:  0.5682235956192017
train gradient:  0.21214680600132674
iteration : 5514
train acc:  0.7421875
train loss:  0.4833441376686096
train gradient:  0.11889652869089101
iteration : 5515
train acc:  0.7421875
train loss:  0.5220651626586914
train gradient:  0.14791596531417178
iteration : 5516
train acc:  0.78125
train loss:  0.45162448287010193
train gradient:  0.1084425439626943
iteration : 5517
train acc:  0.6796875
train loss:  0.5293340682983398
train gradient:  0.1757698864151851
iteration : 5518
train acc:  0.703125
train loss:  0.5304318070411682
train gradient:  0.13374495274216935
iteration : 5519
train acc:  0.671875
train loss:  0.5674290657043457
train gradient:  0.21471916086958848
iteration : 5520
train acc:  0.765625
train loss:  0.4667864441871643
train gradient:  0.12574203690981825
iteration : 5521
train acc:  0.7578125
train loss:  0.5047779083251953
train gradient:  0.12567702658093605
iteration : 5522
train acc:  0.7578125
train loss:  0.5127484798431396
train gradient:  0.13816312061419972
iteration : 5523
train acc:  0.6796875
train loss:  0.5240497589111328
train gradient:  0.1470894878538121
iteration : 5524
train acc:  0.78125
train loss:  0.4566924571990967
train gradient:  0.11823713874505089
iteration : 5525
train acc:  0.7265625
train loss:  0.4926093518733978
train gradient:  0.14376426852253846
iteration : 5526
train acc:  0.7265625
train loss:  0.5775638818740845
train gradient:  0.21382000868549494
iteration : 5527
train acc:  0.671875
train loss:  0.5638138055801392
train gradient:  0.18592798523959503
iteration : 5528
train acc:  0.734375
train loss:  0.5149670839309692
train gradient:  0.1498184327823685
iteration : 5529
train acc:  0.734375
train loss:  0.45983022451400757
train gradient:  0.11937684352849606
iteration : 5530
train acc:  0.7421875
train loss:  0.4617650508880615
train gradient:  0.12345512998492066
iteration : 5531
train acc:  0.7109375
train loss:  0.5197337865829468
train gradient:  0.1612344569844621
iteration : 5532
train acc:  0.6875
train loss:  0.5535117983818054
train gradient:  0.14841208545471624
iteration : 5533
train acc:  0.75
train loss:  0.45817434787750244
train gradient:  0.11097152399230102
iteration : 5534
train acc:  0.71875
train loss:  0.5737758874893188
train gradient:  0.18684289976107615
iteration : 5535
train acc:  0.6875
train loss:  0.5961025953292847
train gradient:  0.17140172127923053
iteration : 5536
train acc:  0.7421875
train loss:  0.48450201749801636
train gradient:  0.12999459960546267
iteration : 5537
train acc:  0.7109375
train loss:  0.5591678619384766
train gradient:  0.15797278161218065
iteration : 5538
train acc:  0.734375
train loss:  0.5023378133773804
train gradient:  0.15121951360761854
iteration : 5539
train acc:  0.6875
train loss:  0.5564250946044922
train gradient:  0.21000666867842832
iteration : 5540
train acc:  0.7265625
train loss:  0.5012293457984924
train gradient:  0.13006577422779242
iteration : 5541
train acc:  0.7265625
train loss:  0.4882398247718811
train gradient:  0.17232671194271376
iteration : 5542
train acc:  0.734375
train loss:  0.519817590713501
train gradient:  0.1253842430407343
iteration : 5543
train acc:  0.7734375
train loss:  0.4392952024936676
train gradient:  0.1006563957447311
iteration : 5544
train acc:  0.703125
train loss:  0.5196630954742432
train gradient:  0.1557386990606613
iteration : 5545
train acc:  0.7109375
train loss:  0.5007110834121704
train gradient:  0.13285086976528254
iteration : 5546
train acc:  0.671875
train loss:  0.5452749729156494
train gradient:  0.1821012498370656
iteration : 5547
train acc:  0.7890625
train loss:  0.46099573373794556
train gradient:  0.12659660586429983
iteration : 5548
train acc:  0.75
train loss:  0.49195384979248047
train gradient:  0.12987916370000824
iteration : 5549
train acc:  0.7109375
train loss:  0.5168059468269348
train gradient:  0.17785874463628285
iteration : 5550
train acc:  0.7109375
train loss:  0.5350172519683838
train gradient:  0.14233340725791846
iteration : 5551
train acc:  0.75
train loss:  0.5183831453323364
train gradient:  0.1721610516231845
iteration : 5552
train acc:  0.75
train loss:  0.4894610047340393
train gradient:  0.14252227153531782
iteration : 5553
train acc:  0.6640625
train loss:  0.6064237356185913
train gradient:  0.19184147532860885
iteration : 5554
train acc:  0.71875
train loss:  0.5367693305015564
train gradient:  0.17959680250831989
iteration : 5555
train acc:  0.6875
train loss:  0.5829638242721558
train gradient:  0.17130651727891455
iteration : 5556
train acc:  0.7578125
train loss:  0.4555237889289856
train gradient:  0.10821246430926061
iteration : 5557
train acc:  0.6484375
train loss:  0.618107795715332
train gradient:  0.2172939316324475
iteration : 5558
train acc:  0.7265625
train loss:  0.4951123893260956
train gradient:  0.14691333628405945
iteration : 5559
train acc:  0.7109375
train loss:  0.5117119550704956
train gradient:  0.1370196149426604
iteration : 5560
train acc:  0.734375
train loss:  0.5087502002716064
train gradient:  0.16385166093855205
iteration : 5561
train acc:  0.7109375
train loss:  0.545639157295227
train gradient:  0.1734778989412988
iteration : 5562
train acc:  0.7265625
train loss:  0.5252530574798584
train gradient:  0.1757089503897298
iteration : 5563
train acc:  0.7265625
train loss:  0.4664124846458435
train gradient:  0.13263704981394023
iteration : 5564
train acc:  0.765625
train loss:  0.4759370684623718
train gradient:  0.16382527341284722
iteration : 5565
train acc:  0.8046875
train loss:  0.4981580078601837
train gradient:  0.12364693933526193
iteration : 5566
train acc:  0.7578125
train loss:  0.48721957206726074
train gradient:  0.1330865729781635
iteration : 5567
train acc:  0.765625
train loss:  0.5085511207580566
train gradient:  0.14133328585167576
iteration : 5568
train acc:  0.703125
train loss:  0.5349534749984741
train gradient:  0.1356081337153958
iteration : 5569
train acc:  0.7578125
train loss:  0.4960532486438751
train gradient:  0.1483833669000105
iteration : 5570
train acc:  0.7734375
train loss:  0.47423362731933594
train gradient:  0.11725741207898192
iteration : 5571
train acc:  0.75
train loss:  0.46145346760749817
train gradient:  0.1114509994076563
iteration : 5572
train acc:  0.75
train loss:  0.4952514171600342
train gradient:  0.16138129414594654
iteration : 5573
train acc:  0.734375
train loss:  0.4942304790019989
train gradient:  0.11244287520637462
iteration : 5574
train acc:  0.7578125
train loss:  0.4798051714897156
train gradient:  0.10447626293612679
iteration : 5575
train acc:  0.7109375
train loss:  0.5345152020454407
train gradient:  0.14413668525206233
iteration : 5576
train acc:  0.734375
train loss:  0.4685567617416382
train gradient:  0.12425475845341474
iteration : 5577
train acc:  0.703125
train loss:  0.5366531610488892
train gradient:  0.15268424656418184
iteration : 5578
train acc:  0.7265625
train loss:  0.5097489356994629
train gradient:  0.137330398450998
iteration : 5579
train acc:  0.75
train loss:  0.4349256157875061
train gradient:  0.09945604103740618
iteration : 5580
train acc:  0.7578125
train loss:  0.476316899061203
train gradient:  0.12469673646082342
iteration : 5581
train acc:  0.7265625
train loss:  0.4945080876350403
train gradient:  0.14824239336341233
iteration : 5582
train acc:  0.7109375
train loss:  0.4874584972858429
train gradient:  0.12160320366093137
iteration : 5583
train acc:  0.75
train loss:  0.5004402995109558
train gradient:  0.17296767546945052
iteration : 5584
train acc:  0.7421875
train loss:  0.5391271114349365
train gradient:  0.15148626281422872
iteration : 5585
train acc:  0.71875
train loss:  0.5701615214347839
train gradient:  0.1511552299317872
iteration : 5586
train acc:  0.7265625
train loss:  0.4987804591655731
train gradient:  0.12914952188490875
iteration : 5587
train acc:  0.765625
train loss:  0.46383944153785706
train gradient:  0.10453358864998456
iteration : 5588
train acc:  0.75
train loss:  0.48563411831855774
train gradient:  0.12300829320891155
iteration : 5589
train acc:  0.765625
train loss:  0.48455774784088135
train gradient:  0.18932937836548636
iteration : 5590
train acc:  0.7421875
train loss:  0.4804595410823822
train gradient:  0.1313904793948848
iteration : 5591
train acc:  0.6953125
train loss:  0.4968802034854889
train gradient:  0.13898484831012778
iteration : 5592
train acc:  0.734375
train loss:  0.47013720870018005
train gradient:  0.1177857503050192
iteration : 5593
train acc:  0.765625
train loss:  0.5032734274864197
train gradient:  0.15810648220364656
iteration : 5594
train acc:  0.6875
train loss:  0.5810374021530151
train gradient:  0.1616495528741869
iteration : 5595
train acc:  0.75
train loss:  0.4794420897960663
train gradient:  0.1840246568154435
iteration : 5596
train acc:  0.7734375
train loss:  0.46547332406044006
train gradient:  0.13449925023741924
iteration : 5597
train acc:  0.6015625
train loss:  0.6625103950500488
train gradient:  0.31420942374692573
iteration : 5598
train acc:  0.6796875
train loss:  0.5394386649131775
train gradient:  0.16259291516427649
iteration : 5599
train acc:  0.6953125
train loss:  0.5221256017684937
train gradient:  0.13782081441193486
iteration : 5600
train acc:  0.75
train loss:  0.4755638837814331
train gradient:  0.12016555918703052
iteration : 5601
train acc:  0.703125
train loss:  0.5111521482467651
train gradient:  0.22086100444972262
iteration : 5602
train acc:  0.7578125
train loss:  0.4874516725540161
train gradient:  0.12849550877290938
iteration : 5603
train acc:  0.78125
train loss:  0.49601805210113525
train gradient:  0.1180371820454976
iteration : 5604
train acc:  0.7421875
train loss:  0.5140098333358765
train gradient:  0.17561591411243954
iteration : 5605
train acc:  0.75
train loss:  0.5089371800422668
train gradient:  0.12919545266104415
iteration : 5606
train acc:  0.7421875
train loss:  0.49800819158554077
train gradient:  0.15657486201429932
iteration : 5607
train acc:  0.6796875
train loss:  0.5713476538658142
train gradient:  0.1603253926470102
iteration : 5608
train acc:  0.71875
train loss:  0.5136086940765381
train gradient:  0.15549440870075198
iteration : 5609
train acc:  0.734375
train loss:  0.5342830419540405
train gradient:  0.14809800959548114
iteration : 5610
train acc:  0.75
train loss:  0.49890559911727905
train gradient:  0.16233676154024462
iteration : 5611
train acc:  0.75
train loss:  0.4946121573448181
train gradient:  0.1342191339585811
iteration : 5612
train acc:  0.7734375
train loss:  0.501498281955719
train gradient:  0.14966668486613768
iteration : 5613
train acc:  0.7421875
train loss:  0.5011261701583862
train gradient:  0.1506856355258473
iteration : 5614
train acc:  0.7578125
train loss:  0.5087070465087891
train gradient:  0.1490900042922631
iteration : 5615
train acc:  0.7578125
train loss:  0.4797799587249756
train gradient:  0.12011472010981473
iteration : 5616
train acc:  0.6875
train loss:  0.540320873260498
train gradient:  0.1724189035599341
iteration : 5617
train acc:  0.7578125
train loss:  0.4836988151073456
train gradient:  0.1311185837494775
iteration : 5618
train acc:  0.6796875
train loss:  0.5969155430793762
train gradient:  0.1822539912583948
iteration : 5619
train acc:  0.765625
train loss:  0.44292446970939636
train gradient:  0.11375963809350313
iteration : 5620
train acc:  0.671875
train loss:  0.5499423742294312
train gradient:  0.170845968473181
iteration : 5621
train acc:  0.765625
train loss:  0.48873862624168396
train gradient:  0.14444110643774408
iteration : 5622
train acc:  0.7421875
train loss:  0.5080975890159607
train gradient:  0.13289182515195191
iteration : 5623
train acc:  0.6796875
train loss:  0.5609926581382751
train gradient:  0.18424211033384882
iteration : 5624
train acc:  0.78125
train loss:  0.5249727964401245
train gradient:  0.14524293764298338
iteration : 5625
train acc:  0.7109375
train loss:  0.5730903148651123
train gradient:  0.1969970251457417
iteration : 5626
train acc:  0.7578125
train loss:  0.46712589263916016
train gradient:  0.14317170603775092
iteration : 5627
train acc:  0.7734375
train loss:  0.49255359172821045
train gradient:  0.1327789293917579
iteration : 5628
train acc:  0.7734375
train loss:  0.49425986409187317
train gradient:  0.14003895879585285
iteration : 5629
train acc:  0.7109375
train loss:  0.5170460939407349
train gradient:  0.1127295733690683
iteration : 5630
train acc:  0.7734375
train loss:  0.4778369665145874
train gradient:  0.1706043624546768
iteration : 5631
train acc:  0.71875
train loss:  0.5209242701530457
train gradient:  0.13500322648873037
iteration : 5632
train acc:  0.671875
train loss:  0.6019184589385986
train gradient:  0.20966249658631259
iteration : 5633
train acc:  0.7421875
train loss:  0.5039973258972168
train gradient:  0.12393843234003257
iteration : 5634
train acc:  0.765625
train loss:  0.44966787099838257
train gradient:  0.12824346352142038
iteration : 5635
train acc:  0.796875
train loss:  0.42720216512680054
train gradient:  0.10315077876327862
iteration : 5636
train acc:  0.7578125
train loss:  0.514331042766571
train gradient:  0.14574473348502975
iteration : 5637
train acc:  0.7578125
train loss:  0.4620513617992401
train gradient:  0.1264234555423161
iteration : 5638
train acc:  0.7734375
train loss:  0.5077873468399048
train gradient:  0.14568342343729437
iteration : 5639
train acc:  0.7109375
train loss:  0.55027174949646
train gradient:  0.17522374306805547
iteration : 5640
train acc:  0.703125
train loss:  0.5408529043197632
train gradient:  0.19465569739406774
iteration : 5641
train acc:  0.6796875
train loss:  0.5886522531509399
train gradient:  0.21025338390444154
iteration : 5642
train acc:  0.7578125
train loss:  0.4984714984893799
train gradient:  0.14397384213831346
iteration : 5643
train acc:  0.796875
train loss:  0.48398807644844055
train gradient:  0.15155599194092978
iteration : 5644
train acc:  0.78125
train loss:  0.4592088758945465
train gradient:  0.12538155076446644
iteration : 5645
train acc:  0.6953125
train loss:  0.5450699925422668
train gradient:  0.20126355657020562
iteration : 5646
train acc:  0.7421875
train loss:  0.48921769857406616
train gradient:  0.1360394839032757
iteration : 5647
train acc:  0.6875
train loss:  0.5201481580734253
train gradient:  0.17434842899058045
iteration : 5648
train acc:  0.8203125
train loss:  0.425411194562912
train gradient:  0.13372495619381272
iteration : 5649
train acc:  0.671875
train loss:  0.5483157634735107
train gradient:  0.15249809380829252
iteration : 5650
train acc:  0.7578125
train loss:  0.48291510343551636
train gradient:  0.15198135360356302
iteration : 5651
train acc:  0.75
train loss:  0.5081136226654053
train gradient:  0.12108268945968488
iteration : 5652
train acc:  0.7421875
train loss:  0.4880247116088867
train gradient:  0.11105412065736509
iteration : 5653
train acc:  0.765625
train loss:  0.48572224378585815
train gradient:  0.1174834257724758
iteration : 5654
train acc:  0.765625
train loss:  0.454670786857605
train gradient:  0.15726569749493724
iteration : 5655
train acc:  0.78125
train loss:  0.48599255084991455
train gradient:  0.15083597655473063
iteration : 5656
train acc:  0.703125
train loss:  0.5007250308990479
train gradient:  0.13798048521238088
iteration : 5657
train acc:  0.6875
train loss:  0.563805341720581
train gradient:  0.1634631254501432
iteration : 5658
train acc:  0.765625
train loss:  0.5111380219459534
train gradient:  0.13802404442808025
iteration : 5659
train acc:  0.765625
train loss:  0.49652284383773804
train gradient:  0.1588765997464031
iteration : 5660
train acc:  0.71875
train loss:  0.5151321887969971
train gradient:  0.17421641797172038
iteration : 5661
train acc:  0.75
train loss:  0.5276866555213928
train gradient:  0.2210668195645641
iteration : 5662
train acc:  0.7578125
train loss:  0.5036811232566833
train gradient:  0.15081825848079816
iteration : 5663
train acc:  0.671875
train loss:  0.5856043100357056
train gradient:  0.17045203630673816
iteration : 5664
train acc:  0.7421875
train loss:  0.5560250878334045
train gradient:  0.14823855479055412
iteration : 5665
train acc:  0.7734375
train loss:  0.47372615337371826
train gradient:  0.12508608324479936
iteration : 5666
train acc:  0.75
train loss:  0.5299661755561829
train gradient:  0.16701694236307948
iteration : 5667
train acc:  0.7421875
train loss:  0.44918400049209595
train gradient:  0.1444243956869172
iteration : 5668
train acc:  0.6875
train loss:  0.5526816844940186
train gradient:  0.22507887386779432
iteration : 5669
train acc:  0.78125
train loss:  0.49993956089019775
train gradient:  0.12204278500726712
iteration : 5670
train acc:  0.671875
train loss:  0.5824941396713257
train gradient:  0.15726624154288366
iteration : 5671
train acc:  0.7421875
train loss:  0.5140767693519592
train gradient:  0.18164823851732645
iteration : 5672
train acc:  0.7578125
train loss:  0.5185720324516296
train gradient:  0.13649222477358533
iteration : 5673
train acc:  0.734375
train loss:  0.5267863273620605
train gradient:  0.12974441706297246
iteration : 5674
train acc:  0.7265625
train loss:  0.5287052392959595
train gradient:  0.12720063073874707
iteration : 5675
train acc:  0.7734375
train loss:  0.44313663244247437
train gradient:  0.12366398304119322
iteration : 5676
train acc:  0.765625
train loss:  0.5218542814254761
train gradient:  0.19037525534873095
iteration : 5677
train acc:  0.796875
train loss:  0.44916635751724243
train gradient:  0.10842594272839824
iteration : 5678
train acc:  0.7265625
train loss:  0.5148839950561523
train gradient:  0.12955138689616774
iteration : 5679
train acc:  0.75
train loss:  0.5606687664985657
train gradient:  0.17825945554677275
iteration : 5680
train acc:  0.734375
train loss:  0.5045132637023926
train gradient:  0.20486342674040103
iteration : 5681
train acc:  0.71875
train loss:  0.4556204080581665
train gradient:  0.11100038953070927
iteration : 5682
train acc:  0.7734375
train loss:  0.46327370405197144
train gradient:  0.12658147268338896
iteration : 5683
train acc:  0.7890625
train loss:  0.46605217456817627
train gradient:  0.11184334961682048
iteration : 5684
train acc:  0.7421875
train loss:  0.5121605396270752
train gradient:  0.1543796134829545
iteration : 5685
train acc:  0.7265625
train loss:  0.5004239082336426
train gradient:  0.10831662932551238
iteration : 5686
train acc:  0.7734375
train loss:  0.49707651138305664
train gradient:  0.14704329834701785
iteration : 5687
train acc:  0.7265625
train loss:  0.5248034000396729
train gradient:  0.14942097212430233
iteration : 5688
train acc:  0.75
train loss:  0.5529510974884033
train gradient:  0.16067397518165893
iteration : 5689
train acc:  0.8046875
train loss:  0.47376078367233276
train gradient:  0.13702987771956754
iteration : 5690
train acc:  0.71875
train loss:  0.49806177616119385
train gradient:  0.18318566920231588
iteration : 5691
train acc:  0.7578125
train loss:  0.49423304200172424
train gradient:  0.14796889288430165
iteration : 5692
train acc:  0.7578125
train loss:  0.4429270029067993
train gradient:  0.14829246176606908
iteration : 5693
train acc:  0.734375
train loss:  0.4860903322696686
train gradient:  0.18348911161873963
iteration : 5694
train acc:  0.7734375
train loss:  0.478177011013031
train gradient:  0.1449696523103446
iteration : 5695
train acc:  0.75
train loss:  0.5032528638839722
train gradient:  0.12839769357230796
iteration : 5696
train acc:  0.7109375
train loss:  0.5312474966049194
train gradient:  0.1294432210096214
iteration : 5697
train acc:  0.7109375
train loss:  0.5221428871154785
train gradient:  0.16383130865549617
iteration : 5698
train acc:  0.703125
train loss:  0.5884488224983215
train gradient:  0.21495987164860475
iteration : 5699
train acc:  0.7890625
train loss:  0.4808870553970337
train gradient:  0.110340684056628
iteration : 5700
train acc:  0.6796875
train loss:  0.5900267362594604
train gradient:  0.17183192805933498
iteration : 5701
train acc:  0.7109375
train loss:  0.4871525168418884
train gradient:  0.14907284951527935
iteration : 5702
train acc:  0.796875
train loss:  0.41918736696243286
train gradient:  0.1009467708226186
iteration : 5703
train acc:  0.796875
train loss:  0.44807571172714233
train gradient:  0.14782966742089723
iteration : 5704
train acc:  0.8125
train loss:  0.4399188756942749
train gradient:  0.11074572877293604
iteration : 5705
train acc:  0.71875
train loss:  0.5518121123313904
train gradient:  0.21447616888515084
iteration : 5706
train acc:  0.7578125
train loss:  0.4981587529182434
train gradient:  0.14678940603993207
iteration : 5707
train acc:  0.6953125
train loss:  0.5368949174880981
train gradient:  0.1644422447318666
iteration : 5708
train acc:  0.7265625
train loss:  0.5324681997299194
train gradient:  0.1548268386257169
iteration : 5709
train acc:  0.7890625
train loss:  0.4630146622657776
train gradient:  0.1272247231329201
iteration : 5710
train acc:  0.75
train loss:  0.4726487398147583
train gradient:  0.14334757797650738
iteration : 5711
train acc:  0.6953125
train loss:  0.497330904006958
train gradient:  0.12957251593673547
iteration : 5712
train acc:  0.796875
train loss:  0.4833817780017853
train gradient:  0.1180560203025519
iteration : 5713
train acc:  0.671875
train loss:  0.5360622406005859
train gradient:  0.15676116787026825
iteration : 5714
train acc:  0.734375
train loss:  0.560085117816925
train gradient:  0.2014034742411635
iteration : 5715
train acc:  0.7578125
train loss:  0.4955291748046875
train gradient:  0.11659189268979081
iteration : 5716
train acc:  0.6796875
train loss:  0.5653318166732788
train gradient:  0.15759065778231018
iteration : 5717
train acc:  0.75
train loss:  0.5014610290527344
train gradient:  0.16162264604576504
iteration : 5718
train acc:  0.75
train loss:  0.4741884469985962
train gradient:  0.10346001864063684
iteration : 5719
train acc:  0.6796875
train loss:  0.5851562023162842
train gradient:  0.18115433524372
iteration : 5720
train acc:  0.7109375
train loss:  0.5218207240104675
train gradient:  0.1367761258694123
iteration : 5721
train acc:  0.7109375
train loss:  0.5548276901245117
train gradient:  0.16576593695862418
iteration : 5722
train acc:  0.71875
train loss:  0.5535147190093994
train gradient:  0.20629819924960108
iteration : 5723
train acc:  0.75
train loss:  0.5159192085266113
train gradient:  0.14267741406771373
iteration : 5724
train acc:  0.75
train loss:  0.4722272753715515
train gradient:  0.13637792013716574
iteration : 5725
train acc:  0.6953125
train loss:  0.5571838021278381
train gradient:  0.17614108955794827
iteration : 5726
train acc:  0.6640625
train loss:  0.5490257143974304
train gradient:  0.1606160615587452
iteration : 5727
train acc:  0.7578125
train loss:  0.4634438157081604
train gradient:  0.14685595024066017
iteration : 5728
train acc:  0.7421875
train loss:  0.5291451215744019
train gradient:  0.14855090301919316
iteration : 5729
train acc:  0.7578125
train loss:  0.4874906837940216
train gradient:  0.15776229583684936
iteration : 5730
train acc:  0.734375
train loss:  0.47046247124671936
train gradient:  0.1616035093084246
iteration : 5731
train acc:  0.6953125
train loss:  0.5211488604545593
train gradient:  0.10925496017336683
iteration : 5732
train acc:  0.8125
train loss:  0.4103243947029114
train gradient:  0.10736548927000039
iteration : 5733
train acc:  0.7734375
train loss:  0.4689217805862427
train gradient:  0.1361771911588201
iteration : 5734
train acc:  0.71875
train loss:  0.5072598457336426
train gradient:  0.14157379065738646
iteration : 5735
train acc:  0.8046875
train loss:  0.42428672313690186
train gradient:  0.1129846519637561
iteration : 5736
train acc:  0.75
train loss:  0.48898935317993164
train gradient:  0.12693063076588704
iteration : 5737
train acc:  0.7734375
train loss:  0.4904136061668396
train gradient:  0.1818349690647716
iteration : 5738
train acc:  0.6796875
train loss:  0.5423179268836975
train gradient:  0.1659102261222002
iteration : 5739
train acc:  0.7109375
train loss:  0.5561016798019409
train gradient:  0.1685881575949224
iteration : 5740
train acc:  0.71875
train loss:  0.5031748414039612
train gradient:  0.15078111232266792
iteration : 5741
train acc:  0.7265625
train loss:  0.4847583770751953
train gradient:  0.17618554311069884
iteration : 5742
train acc:  0.734375
train loss:  0.4746741056442261
train gradient:  0.1633521801045811
iteration : 5743
train acc:  0.796875
train loss:  0.4342893362045288
train gradient:  0.1207740772764934
iteration : 5744
train acc:  0.75
train loss:  0.4811549782752991
train gradient:  0.13186853696437492
iteration : 5745
train acc:  0.71875
train loss:  0.5326088666915894
train gradient:  0.15974705136624207
iteration : 5746
train acc:  0.75
train loss:  0.5176998972892761
train gradient:  0.20041886649601665
iteration : 5747
train acc:  0.7265625
train loss:  0.4827355146408081
train gradient:  0.1136040647997483
iteration : 5748
train acc:  0.7109375
train loss:  0.5265045166015625
train gradient:  0.18558675822631576
iteration : 5749
train acc:  0.71875
train loss:  0.4966273307800293
train gradient:  0.16312107235286324
iteration : 5750
train acc:  0.7265625
train loss:  0.557354748249054
train gradient:  0.15335756770965112
iteration : 5751
train acc:  0.6484375
train loss:  0.6293747425079346
train gradient:  0.1940135580673829
iteration : 5752
train acc:  0.6875
train loss:  0.5642083883285522
train gradient:  0.20547560746032661
iteration : 5753
train acc:  0.78125
train loss:  0.4868343472480774
train gradient:  0.13001596107762614
iteration : 5754
train acc:  0.7109375
train loss:  0.5073780417442322
train gradient:  0.15935883196261852
iteration : 5755
train acc:  0.7265625
train loss:  0.4804045557975769
train gradient:  0.11323634236899985
iteration : 5756
train acc:  0.71875
train loss:  0.5065412521362305
train gradient:  0.14274907816846055
iteration : 5757
train acc:  0.71875
train loss:  0.5274009704589844
train gradient:  0.1503608076874661
iteration : 5758
train acc:  0.7890625
train loss:  0.4257637858390808
train gradient:  0.11392301285857111
iteration : 5759
train acc:  0.7578125
train loss:  0.5521284937858582
train gradient:  0.1638578401648485
iteration : 5760
train acc:  0.7265625
train loss:  0.5066284537315369
train gradient:  0.13122652748008792
iteration : 5761
train acc:  0.7421875
train loss:  0.5260876417160034
train gradient:  0.15191087319624794
iteration : 5762
train acc:  0.7421875
train loss:  0.48844611644744873
train gradient:  0.12850945215799064
iteration : 5763
train acc:  0.7421875
train loss:  0.473608136177063
train gradient:  0.10915300486001242
iteration : 5764
train acc:  0.6953125
train loss:  0.5617436170578003
train gradient:  0.1791369737579117
iteration : 5765
train acc:  0.7265625
train loss:  0.5149478316307068
train gradient:  0.1392376192362068
iteration : 5766
train acc:  0.7265625
train loss:  0.4952925741672516
train gradient:  0.1150810164001002
iteration : 5767
train acc:  0.7109375
train loss:  0.520251452922821
train gradient:  0.22865276996146622
iteration : 5768
train acc:  0.7578125
train loss:  0.4812408685684204
train gradient:  0.12648095031153772
iteration : 5769
train acc:  0.7421875
train loss:  0.5117385387420654
train gradient:  0.11426191138543268
iteration : 5770
train acc:  0.7265625
train loss:  0.4954633116722107
train gradient:  0.11502834720176086
iteration : 5771
train acc:  0.734375
train loss:  0.5145933628082275
train gradient:  0.12781318862104185
iteration : 5772
train acc:  0.7265625
train loss:  0.5126495957374573
train gradient:  0.1387544194380519
iteration : 5773
train acc:  0.671875
train loss:  0.5574653148651123
train gradient:  0.15882373742533695
iteration : 5774
train acc:  0.71875
train loss:  0.5411103963851929
train gradient:  0.14431154467261148
iteration : 5775
train acc:  0.734375
train loss:  0.4956849217414856
train gradient:  0.1207518125122758
iteration : 5776
train acc:  0.8125
train loss:  0.46340101957321167
train gradient:  0.10442699546381448
iteration : 5777
train acc:  0.7421875
train loss:  0.4511176347732544
train gradient:  0.13200989972712865
iteration : 5778
train acc:  0.8359375
train loss:  0.42452389001846313
train gradient:  0.1178162700546161
iteration : 5779
train acc:  0.7421875
train loss:  0.47278326749801636
train gradient:  0.11718963140984355
iteration : 5780
train acc:  0.71875
train loss:  0.5086448192596436
train gradient:  0.11660478728615872
iteration : 5781
train acc:  0.7890625
train loss:  0.43003758788108826
train gradient:  0.12539838167964035
iteration : 5782
train acc:  0.765625
train loss:  0.49088165163993835
train gradient:  0.1541964970205481
iteration : 5783
train acc:  0.765625
train loss:  0.5177509784698486
train gradient:  0.14843820075100284
iteration : 5784
train acc:  0.7421875
train loss:  0.46847355365753174
train gradient:  0.12325529744438703
iteration : 5785
train acc:  0.6796875
train loss:  0.5844865441322327
train gradient:  0.19837846896311856
iteration : 5786
train acc:  0.75
train loss:  0.4844963848590851
train gradient:  0.14321999632519028
iteration : 5787
train acc:  0.7265625
train loss:  0.5367474555969238
train gradient:  0.18455227117823642
iteration : 5788
train acc:  0.765625
train loss:  0.4862459897994995
train gradient:  0.14953953293155964
iteration : 5789
train acc:  0.6796875
train loss:  0.5466763973236084
train gradient:  0.13712809209716542
iteration : 5790
train acc:  0.7578125
train loss:  0.4422798752784729
train gradient:  0.13887012134272791
iteration : 5791
train acc:  0.7265625
train loss:  0.559826135635376
train gradient:  0.14841229904726613
iteration : 5792
train acc:  0.6953125
train loss:  0.580075740814209
train gradient:  0.2702267912312432
iteration : 5793
train acc:  0.7734375
train loss:  0.4678488075733185
train gradient:  0.12361236614361204
iteration : 5794
train acc:  0.7265625
train loss:  0.5071625113487244
train gradient:  0.16357419687354305
iteration : 5795
train acc:  0.765625
train loss:  0.47782522439956665
train gradient:  0.14327879611189123
iteration : 5796
train acc:  0.78125
train loss:  0.4525095224380493
train gradient:  0.12228857243588784
iteration : 5797
train acc:  0.703125
train loss:  0.5532497763633728
train gradient:  0.1585508830238681
iteration : 5798
train acc:  0.765625
train loss:  0.4463561177253723
train gradient:  0.14588286365373965
iteration : 5799
train acc:  0.765625
train loss:  0.5046853423118591
train gradient:  0.23202686472575568
iteration : 5800
train acc:  0.765625
train loss:  0.49701690673828125
train gradient:  0.12820018980724762
iteration : 5801
train acc:  0.7734375
train loss:  0.47866004705429077
train gradient:  0.14115684941545525
iteration : 5802
train acc:  0.7578125
train loss:  0.4549051523208618
train gradient:  0.10907292606516708
iteration : 5803
train acc:  0.75
train loss:  0.5384231805801392
train gradient:  0.1837491436328527
iteration : 5804
train acc:  0.75
train loss:  0.5391353368759155
train gradient:  0.14735739169812628
iteration : 5805
train acc:  0.6953125
train loss:  0.509816586971283
train gradient:  0.15392287326726034
iteration : 5806
train acc:  0.75
train loss:  0.46199071407318115
train gradient:  0.1484769339122884
iteration : 5807
train acc:  0.734375
train loss:  0.45249801874160767
train gradient:  0.12168315016585134
iteration : 5808
train acc:  0.7734375
train loss:  0.4575892388820648
train gradient:  0.15630743368423516
iteration : 5809
train acc:  0.7109375
train loss:  0.5419020652770996
train gradient:  0.20224309932585316
iteration : 5810
train acc:  0.6875
train loss:  0.5478543043136597
train gradient:  0.13584152845746617
iteration : 5811
train acc:  0.6875
train loss:  0.610530436038971
train gradient:  0.1919350774717815
iteration : 5812
train acc:  0.734375
train loss:  0.5293558835983276
train gradient:  0.14927525837049677
iteration : 5813
train acc:  0.7109375
train loss:  0.5567184090614319
train gradient:  0.19721907891343296
iteration : 5814
train acc:  0.75
train loss:  0.495079904794693
train gradient:  0.1267551237993616
iteration : 5815
train acc:  0.734375
train loss:  0.5180268287658691
train gradient:  0.13345490717105302
iteration : 5816
train acc:  0.7265625
train loss:  0.47178301215171814
train gradient:  0.12302426469491266
iteration : 5817
train acc:  0.7734375
train loss:  0.47685402631759644
train gradient:  0.11864590834638677
iteration : 5818
train acc:  0.7109375
train loss:  0.5394162535667419
train gradient:  0.13132023952324196
iteration : 5819
train acc:  0.703125
train loss:  0.5462195873260498
train gradient:  0.18686225035894033
iteration : 5820
train acc:  0.78125
train loss:  0.4905690848827362
train gradient:  0.1606707525960458
iteration : 5821
train acc:  0.765625
train loss:  0.5128596425056458
train gradient:  0.16552997266378455
iteration : 5822
train acc:  0.6953125
train loss:  0.5435513257980347
train gradient:  0.17474748157316963
iteration : 5823
train acc:  0.7578125
train loss:  0.5080077648162842
train gradient:  0.15071880474899912
iteration : 5824
train acc:  0.7890625
train loss:  0.46820276975631714
train gradient:  0.13089393680989708
iteration : 5825
train acc:  0.6796875
train loss:  0.5717028379440308
train gradient:  0.18328029558738082
iteration : 5826
train acc:  0.71875
train loss:  0.5120800137519836
train gradient:  0.17353680706597327
iteration : 5827
train acc:  0.6796875
train loss:  0.5600628852844238
train gradient:  0.15856382293531687
iteration : 5828
train acc:  0.75
train loss:  0.5023089647293091
train gradient:  0.16957950067036748
iteration : 5829
train acc:  0.6796875
train loss:  0.5384482145309448
train gradient:  0.1321583396713768
iteration : 5830
train acc:  0.75
train loss:  0.4887722134590149
train gradient:  0.13738095162896455
iteration : 5831
train acc:  0.65625
train loss:  0.5763498544692993
train gradient:  0.1872533487847739
iteration : 5832
train acc:  0.734375
train loss:  0.5362553596496582
train gradient:  0.15289614655894573
iteration : 5833
train acc:  0.765625
train loss:  0.506923258304596
train gradient:  0.13526686754377107
iteration : 5834
train acc:  0.7109375
train loss:  0.596126139163971
train gradient:  0.19249674908368927
iteration : 5835
train acc:  0.796875
train loss:  0.4209384024143219
train gradient:  0.10858263939983544
iteration : 5836
train acc:  0.7578125
train loss:  0.515198826789856
train gradient:  0.13880311057490174
iteration : 5837
train acc:  0.734375
train loss:  0.5052323937416077
train gradient:  0.14456659885944462
iteration : 5838
train acc:  0.796875
train loss:  0.4173809885978699
train gradient:  0.09835730846694653
iteration : 5839
train acc:  0.765625
train loss:  0.4878973364830017
train gradient:  0.14169855744065393
iteration : 5840
train acc:  0.7265625
train loss:  0.5553402900695801
train gradient:  0.1381981169721079
iteration : 5841
train acc:  0.734375
train loss:  0.5275009274482727
train gradient:  0.15996158694763973
iteration : 5842
train acc:  0.7265625
train loss:  0.49924957752227783
train gradient:  0.13052915291979328
iteration : 5843
train acc:  0.7421875
train loss:  0.4708501994609833
train gradient:  0.11786066118637897
iteration : 5844
train acc:  0.71875
train loss:  0.5486875176429749
train gradient:  0.15006473053854316
iteration : 5845
train acc:  0.765625
train loss:  0.5199968218803406
train gradient:  0.15668738221189046
iteration : 5846
train acc:  0.7734375
train loss:  0.4655283987522125
train gradient:  0.14441653466178395
iteration : 5847
train acc:  0.7578125
train loss:  0.500102162361145
train gradient:  0.1398025101832209
iteration : 5848
train acc:  0.7734375
train loss:  0.4636163115501404
train gradient:  0.12476509511516072
iteration : 5849
train acc:  0.7265625
train loss:  0.4923020005226135
train gradient:  0.14134392745706637
iteration : 5850
train acc:  0.796875
train loss:  0.4639672040939331
train gradient:  0.14128370293068032
iteration : 5851
train acc:  0.71875
train loss:  0.5116365551948547
train gradient:  0.13221639485494552
iteration : 5852
train acc:  0.734375
train loss:  0.49552077054977417
train gradient:  0.16300601303026738
iteration : 5853
train acc:  0.7265625
train loss:  0.556795597076416
train gradient:  0.16498492092510336
iteration : 5854
train acc:  0.765625
train loss:  0.4719696342945099
train gradient:  0.1345474290855057
iteration : 5855
train acc:  0.7578125
train loss:  0.4468355178833008
train gradient:  0.09652468127781104
iteration : 5856
train acc:  0.765625
train loss:  0.46891194581985474
train gradient:  0.12195679666291537
iteration : 5857
train acc:  0.65625
train loss:  0.5520678758621216
train gradient:  0.1416079144719565
iteration : 5858
train acc:  0.7890625
train loss:  0.43842771649360657
train gradient:  0.15772301147355872
iteration : 5859
train acc:  0.75
train loss:  0.48965004086494446
train gradient:  0.12606865160203784
iteration : 5860
train acc:  0.71875
train loss:  0.5954318046569824
train gradient:  0.19541802248907858
iteration : 5861
train acc:  0.6640625
train loss:  0.5987762212753296
train gradient:  0.22534946778210108
iteration : 5862
train acc:  0.703125
train loss:  0.5301498174667358
train gradient:  0.13936506323079345
iteration : 5863
train acc:  0.7734375
train loss:  0.48637449741363525
train gradient:  0.1439076447135002
iteration : 5864
train acc:  0.7421875
train loss:  0.48362845182418823
train gradient:  0.13456971067060391
iteration : 5865
train acc:  0.65625
train loss:  0.5225697755813599
train gradient:  0.12824606055473126
iteration : 5866
train acc:  0.78125
train loss:  0.4810139834880829
train gradient:  0.17611220219539864
iteration : 5867
train acc:  0.7109375
train loss:  0.49752628803253174
train gradient:  0.14916712154095682
iteration : 5868
train acc:  0.7265625
train loss:  0.5329748392105103
train gradient:  0.15207123594270436
iteration : 5869
train acc:  0.6953125
train loss:  0.49860110878944397
train gradient:  0.16665286746864438
iteration : 5870
train acc:  0.765625
train loss:  0.45846205949783325
train gradient:  0.12163266038869683
iteration : 5871
train acc:  0.7578125
train loss:  0.4919606149196625
train gradient:  0.1282505877779817
iteration : 5872
train acc:  0.7578125
train loss:  0.4719579815864563
train gradient:  0.13949254728040117
iteration : 5873
train acc:  0.7109375
train loss:  0.532241702079773
train gradient:  0.1615562458216223
iteration : 5874
train acc:  0.71875
train loss:  0.6006001830101013
train gradient:  0.23170667421441915
iteration : 5875
train acc:  0.71875
train loss:  0.5356101393699646
train gradient:  0.17676524662360749
iteration : 5876
train acc:  0.71875
train loss:  0.4903799891471863
train gradient:  0.12724583085262542
iteration : 5877
train acc:  0.6953125
train loss:  0.5615882277488708
train gradient:  0.1638204641259317
iteration : 5878
train acc:  0.7421875
train loss:  0.5128432512283325
train gradient:  0.159614375847956
iteration : 5879
train acc:  0.6953125
train loss:  0.5159784555435181
train gradient:  0.14286529841393014
iteration : 5880
train acc:  0.734375
train loss:  0.5654556751251221
train gradient:  0.197735443807585
iteration : 5881
train acc:  0.7265625
train loss:  0.5330764651298523
train gradient:  0.18028489504844475
iteration : 5882
train acc:  0.71875
train loss:  0.5196985602378845
train gradient:  0.14138032314909144
iteration : 5883
train acc:  0.7890625
train loss:  0.4437456429004669
train gradient:  0.1242634021011881
iteration : 5884
train acc:  0.734375
train loss:  0.5234681367874146
train gradient:  0.15801478088452714
iteration : 5885
train acc:  0.7578125
train loss:  0.49755457043647766
train gradient:  0.11189025793267844
iteration : 5886
train acc:  0.7265625
train loss:  0.5556704998016357
train gradient:  0.17881754613914191
iteration : 5887
train acc:  0.71875
train loss:  0.526548445224762
train gradient:  0.12847060517556008
iteration : 5888
train acc:  0.734375
train loss:  0.48319756984710693
train gradient:  0.11413748375525505
iteration : 5889
train acc:  0.734375
train loss:  0.5188305974006653
train gradient:  0.19422785671406553
iteration : 5890
train acc:  0.7109375
train loss:  0.5204997658729553
train gradient:  0.15506702746757164
iteration : 5891
train acc:  0.765625
train loss:  0.46331799030303955
train gradient:  0.09858554739176938
iteration : 5892
train acc:  0.6796875
train loss:  0.5078128576278687
train gradient:  0.14900880371813657
iteration : 5893
train acc:  0.71875
train loss:  0.5124046802520752
train gradient:  0.13079737803777236
iteration : 5894
train acc:  0.6953125
train loss:  0.5625584125518799
train gradient:  0.17105693414775824
iteration : 5895
train acc:  0.703125
train loss:  0.524260401725769
train gradient:  0.16697955612125603
iteration : 5896
train acc:  0.7421875
train loss:  0.5258883237838745
train gradient:  0.13509474777261288
iteration : 5897
train acc:  0.796875
train loss:  0.4278237521648407
train gradient:  0.10625051357913434
iteration : 5898
train acc:  0.734375
train loss:  0.49941349029541016
train gradient:  0.13725972404637438
iteration : 5899
train acc:  0.6953125
train loss:  0.5392614006996155
train gradient:  0.1378307592440721
iteration : 5900
train acc:  0.8046875
train loss:  0.46376773715019226
train gradient:  0.12249720443718129
iteration : 5901
train acc:  0.796875
train loss:  0.43855977058410645
train gradient:  0.12332630885391425
iteration : 5902
train acc:  0.65625
train loss:  0.5337849855422974
train gradient:  0.15310935390329605
iteration : 5903
train acc:  0.796875
train loss:  0.4521815776824951
train gradient:  0.11579895554664574
iteration : 5904
train acc:  0.765625
train loss:  0.4244411587715149
train gradient:  0.12856223716248746
iteration : 5905
train acc:  0.75
train loss:  0.5014876127243042
train gradient:  0.17973557780704902
iteration : 5906
train acc:  0.7265625
train loss:  0.46727755665779114
train gradient:  0.11473932440392563
iteration : 5907
train acc:  0.7578125
train loss:  0.47389766573905945
train gradient:  0.14918038543825402
iteration : 5908
train acc:  0.734375
train loss:  0.45964515209198
train gradient:  0.11498618658557962
iteration : 5909
train acc:  0.7265625
train loss:  0.501589298248291
train gradient:  0.1371178189577964
iteration : 5910
train acc:  0.734375
train loss:  0.5014350414276123
train gradient:  0.16727176363605023
iteration : 5911
train acc:  0.78125
train loss:  0.5018491744995117
train gradient:  0.16231936767737143
iteration : 5912
train acc:  0.7421875
train loss:  0.4934333860874176
train gradient:  0.11587537217047521
iteration : 5913
train acc:  0.75
train loss:  0.4899299740791321
train gradient:  0.10723155688230887
iteration : 5914
train acc:  0.65625
train loss:  0.568462610244751
train gradient:  0.15094999537901976
iteration : 5915
train acc:  0.796875
train loss:  0.4637536406517029
train gradient:  0.1400126444959367
iteration : 5916
train acc:  0.7265625
train loss:  0.50066739320755
train gradient:  0.13142911898643655
iteration : 5917
train acc:  0.7265625
train loss:  0.47397077083587646
train gradient:  0.10991636041397942
iteration : 5918
train acc:  0.71875
train loss:  0.6028708219528198
train gradient:  0.1847461567114601
iteration : 5919
train acc:  0.796875
train loss:  0.42230936884880066
train gradient:  0.09973604953362271
iteration : 5920
train acc:  0.734375
train loss:  0.5898624658584595
train gradient:  0.16804806513353648
iteration : 5921
train acc:  0.765625
train loss:  0.5417772531509399
train gradient:  0.18265628826659516
iteration : 5922
train acc:  0.734375
train loss:  0.5057091116905212
train gradient:  0.13244099245445223
iteration : 5923
train acc:  0.6875
train loss:  0.5090211033821106
train gradient:  0.12507328886442304
iteration : 5924
train acc:  0.6953125
train loss:  0.5159631967544556
train gradient:  0.12352431801297986
iteration : 5925
train acc:  0.75
train loss:  0.509009063243866
train gradient:  0.15940841879951861
iteration : 5926
train acc:  0.7578125
train loss:  0.4982585608959198
train gradient:  0.13601805242779613
iteration : 5927
train acc:  0.7421875
train loss:  0.4656766951084137
train gradient:  0.11552658991342758
iteration : 5928
train acc:  0.8046875
train loss:  0.44865405559539795
train gradient:  0.11932377314495822
iteration : 5929
train acc:  0.765625
train loss:  0.48362016677856445
train gradient:  0.12594044248949265
iteration : 5930
train acc:  0.7578125
train loss:  0.5543339252471924
train gradient:  0.1781725855603537
iteration : 5931
train acc:  0.734375
train loss:  0.4734673798084259
train gradient:  0.14426497559855583
iteration : 5932
train acc:  0.765625
train loss:  0.5259763598442078
train gradient:  0.16567431338324085
iteration : 5933
train acc:  0.734375
train loss:  0.45940709114074707
train gradient:  0.10640532353638832
iteration : 5934
train acc:  0.640625
train loss:  0.5836031436920166
train gradient:  0.16761500593512027
iteration : 5935
train acc:  0.71875
train loss:  0.5163772106170654
train gradient:  0.16276299670558855
iteration : 5936
train acc:  0.8125
train loss:  0.4594345986843109
train gradient:  0.15365296455672162
iteration : 5937
train acc:  0.71875
train loss:  0.5144972801208496
train gradient:  0.16770056137143508
iteration : 5938
train acc:  0.7578125
train loss:  0.49770259857177734
train gradient:  0.16223798389528624
iteration : 5939
train acc:  0.7734375
train loss:  0.5322258472442627
train gradient:  0.1675748521432081
iteration : 5940
train acc:  0.7265625
train loss:  0.5383161306381226
train gradient:  0.1551011474465264
iteration : 5941
train acc:  0.75
train loss:  0.4662257730960846
train gradient:  0.11092759499316404
iteration : 5942
train acc:  0.7734375
train loss:  0.5173699259757996
train gradient:  0.13162104105030514
iteration : 5943
train acc:  0.7265625
train loss:  0.5350910425186157
train gradient:  0.15978065656934434
iteration : 5944
train acc:  0.7578125
train loss:  0.48285284638404846
train gradient:  0.123894424432872
iteration : 5945
train acc:  0.71875
train loss:  0.4850670099258423
train gradient:  0.13240237781858966
iteration : 5946
train acc:  0.765625
train loss:  0.4265356659889221
train gradient:  0.09715244154878327
iteration : 5947
train acc:  0.7109375
train loss:  0.5510649085044861
train gradient:  0.1608206090153197
iteration : 5948
train acc:  0.71875
train loss:  0.5322285890579224
train gradient:  0.18436189435826228
iteration : 5949
train acc:  0.734375
train loss:  0.5298939943313599
train gradient:  0.14088938702877357
iteration : 5950
train acc:  0.7109375
train loss:  0.5530973076820374
train gradient:  0.1369247677485857
iteration : 5951
train acc:  0.6796875
train loss:  0.5799380540847778
train gradient:  0.1631585572991368
iteration : 5952
train acc:  0.734375
train loss:  0.49352410435676575
train gradient:  0.12500863030966172
iteration : 5953
train acc:  0.734375
train loss:  0.530181884765625
train gradient:  0.15410595658680917
iteration : 5954
train acc:  0.6953125
train loss:  0.5408526062965393
train gradient:  0.148299101046649
iteration : 5955
train acc:  0.734375
train loss:  0.5124521255493164
train gradient:  0.16100532176194104
iteration : 5956
train acc:  0.6796875
train loss:  0.503267765045166
train gradient:  0.1448363613068675
iteration : 5957
train acc:  0.71875
train loss:  0.5379668474197388
train gradient:  0.2265407707303385
iteration : 5958
train acc:  0.7109375
train loss:  0.5074549317359924
train gradient:  0.1533214787589101
iteration : 5959
train acc:  0.75
train loss:  0.48947203159332275
train gradient:  0.15405342628920055
iteration : 5960
train acc:  0.8203125
train loss:  0.44497546553611755
train gradient:  0.1661589510503778
iteration : 5961
train acc:  0.6796875
train loss:  0.5596135854721069
train gradient:  0.17168369803954364
iteration : 5962
train acc:  0.78125
train loss:  0.4651471972465515
train gradient:  0.13766310484411765
iteration : 5963
train acc:  0.7734375
train loss:  0.5106430649757385
train gradient:  0.16268697404461308
iteration : 5964
train acc:  0.7578125
train loss:  0.4645118713378906
train gradient:  0.10641460566675834
iteration : 5965
train acc:  0.734375
train loss:  0.4603310227394104
train gradient:  0.1292590344884944
iteration : 5966
train acc:  0.7265625
train loss:  0.5489333271980286
train gradient:  0.2158804336565106
iteration : 5967
train acc:  0.7578125
train loss:  0.4760228991508484
train gradient:  0.14393256506948798
iteration : 5968
train acc:  0.7109375
train loss:  0.5400863885879517
train gradient:  0.16619234517573667
iteration : 5969
train acc:  0.6796875
train loss:  0.5299889445304871
train gradient:  0.1449661630042835
iteration : 5970
train acc:  0.7265625
train loss:  0.4957975447177887
train gradient:  0.10506545973559646
iteration : 5971
train acc:  0.6484375
train loss:  0.5468431711196899
train gradient:  0.16132207773026644
iteration : 5972
train acc:  0.75
train loss:  0.44296586513519287
train gradient:  0.14840626447443567
iteration : 5973
train acc:  0.703125
train loss:  0.5758384466171265
train gradient:  0.14087622771069802
iteration : 5974
train acc:  0.6953125
train loss:  0.5804718136787415
train gradient:  0.14555049071550413
iteration : 5975
train acc:  0.6875
train loss:  0.5403308868408203
train gradient:  0.17877864040859043
iteration : 5976
train acc:  0.75
train loss:  0.45902395248413086
train gradient:  0.10619589176016565
iteration : 5977
train acc:  0.7578125
train loss:  0.4559118449687958
train gradient:  0.1180979606611977
iteration : 5978
train acc:  0.7109375
train loss:  0.5794928073883057
train gradient:  0.226594748142795
iteration : 5979
train acc:  0.7265625
train loss:  0.538140594959259
train gradient:  0.14491583449020284
iteration : 5980
train acc:  0.671875
train loss:  0.5577430725097656
train gradient:  0.16616646032916876
iteration : 5981
train acc:  0.6875
train loss:  0.569355845451355
train gradient:  0.21110993249832116
iteration : 5982
train acc:  0.609375
train loss:  0.6107015609741211
train gradient:  0.21164120308294704
iteration : 5983
train acc:  0.7578125
train loss:  0.5242825746536255
train gradient:  0.18236837766991026
iteration : 5984
train acc:  0.75
train loss:  0.476505309343338
train gradient:  0.12238956367977709
iteration : 5985
train acc:  0.7265625
train loss:  0.5259242653846741
train gradient:  0.12532844116421604
iteration : 5986
train acc:  0.703125
train loss:  0.5394850969314575
train gradient:  0.141134699277226
iteration : 5987
train acc:  0.6875
train loss:  0.5333902835845947
train gradient:  0.15357972118346125
iteration : 5988
train acc:  0.7734375
train loss:  0.4638867974281311
train gradient:  0.1206322768715619
iteration : 5989
train acc:  0.765625
train loss:  0.5173105001449585
train gradient:  0.15296221141580407
iteration : 5990
train acc:  0.7109375
train loss:  0.5181782245635986
train gradient:  0.1229217766777552
iteration : 5991
train acc:  0.78125
train loss:  0.5026451349258423
train gradient:  0.12394149781439823
iteration : 5992
train acc:  0.7265625
train loss:  0.45035091042518616
train gradient:  0.09597244220915628
iteration : 5993
train acc:  0.796875
train loss:  0.47017765045166016
train gradient:  0.11818976218567075
iteration : 5994
train acc:  0.7578125
train loss:  0.4815059304237366
train gradient:  0.15206054132655616
iteration : 5995
train acc:  0.6640625
train loss:  0.6001750826835632
train gradient:  0.18955409901563777
iteration : 5996
train acc:  0.7421875
train loss:  0.5064607262611389
train gradient:  0.14224245832427898
iteration : 5997
train acc:  0.7265625
train loss:  0.586532711982727
train gradient:  0.17603957260336095
iteration : 5998
train acc:  0.703125
train loss:  0.5223995447158813
train gradient:  0.13266783830317586
iteration : 5999
train acc:  0.71875
train loss:  0.551256537437439
train gradient:  0.1898753759968623
iteration : 6000
train acc:  0.734375
train loss:  0.4869502782821655
train gradient:  0.12204013684955686
iteration : 6001
train acc:  0.7734375
train loss:  0.5225746631622314
train gradient:  0.15062796162417902
iteration : 6002
train acc:  0.7421875
train loss:  0.5018588304519653
train gradient:  0.19134298290314355
iteration : 6003
train acc:  0.7421875
train loss:  0.46065258979797363
train gradient:  0.09580622487327105
iteration : 6004
train acc:  0.65625
train loss:  0.5519141554832458
train gradient:  0.20265620584338967
iteration : 6005
train acc:  0.703125
train loss:  0.5611599087715149
train gradient:  0.16312825531893144
iteration : 6006
train acc:  0.7265625
train loss:  0.507309079170227
train gradient:  0.17072781081659993
iteration : 6007
train acc:  0.703125
train loss:  0.5362764596939087
train gradient:  0.15169103634360875
iteration : 6008
train acc:  0.6953125
train loss:  0.49076879024505615
train gradient:  0.10897725891300912
iteration : 6009
train acc:  0.71875
train loss:  0.5841530561447144
train gradient:  0.16797991677979823
iteration : 6010
train acc:  0.71875
train loss:  0.5572537183761597
train gradient:  0.1962714387773311
iteration : 6011
train acc:  0.8125
train loss:  0.5027389526367188
train gradient:  0.1542643329919868
iteration : 6012
train acc:  0.7421875
train loss:  0.5147208571434021
train gradient:  0.151361243729826
iteration : 6013
train acc:  0.7421875
train loss:  0.48772966861724854
train gradient:  0.12586099319778793
iteration : 6014
train acc:  0.7734375
train loss:  0.4972938895225525
train gradient:  0.20304947327019707
iteration : 6015
train acc:  0.75
train loss:  0.5187125205993652
train gradient:  0.16560186906089847
iteration : 6016
train acc:  0.7265625
train loss:  0.4946673810482025
train gradient:  0.16788296997280944
iteration : 6017
train acc:  0.7578125
train loss:  0.4687885344028473
train gradient:  0.12701458052164566
iteration : 6018
train acc:  0.75
train loss:  0.49857670068740845
train gradient:  0.13726122640393665
iteration : 6019
train acc:  0.7109375
train loss:  0.47603440284729004
train gradient:  0.14090214853677116
iteration : 6020
train acc:  0.71875
train loss:  0.5150281190872192
train gradient:  0.10749208387149779
iteration : 6021
train acc:  0.6796875
train loss:  0.48992928862571716
train gradient:  0.15711205761867136
iteration : 6022
train acc:  0.703125
train loss:  0.5096228122711182
train gradient:  0.16529375656372064
iteration : 6023
train acc:  0.7109375
train loss:  0.46170681715011597
train gradient:  0.1158260876172941
iteration : 6024
train acc:  0.7578125
train loss:  0.5230015516281128
train gradient:  0.18361199396938893
iteration : 6025
train acc:  0.71875
train loss:  0.5346424579620361
train gradient:  0.16845550763190803
iteration : 6026
train acc:  0.6953125
train loss:  0.5510290861129761
train gradient:  0.20516936270121852
iteration : 6027
train acc:  0.6484375
train loss:  0.5866237282752991
train gradient:  0.17988304145867212
iteration : 6028
train acc:  0.75
train loss:  0.47881054878234863
train gradient:  0.11673480732107437
iteration : 6029
train acc:  0.765625
train loss:  0.48773956298828125
train gradient:  0.12399562294161537
iteration : 6030
train acc:  0.6875
train loss:  0.5867619514465332
train gradient:  0.24144271568703196
iteration : 6031
train acc:  0.7578125
train loss:  0.4841063618659973
train gradient:  0.11225744180137887
iteration : 6032
train acc:  0.71875
train loss:  0.5142568349838257
train gradient:  0.1612187349862808
iteration : 6033
train acc:  0.78125
train loss:  0.49542784690856934
train gradient:  0.1410756961190357
iteration : 6034
train acc:  0.71875
train loss:  0.5055794715881348
train gradient:  0.15029324639260788
iteration : 6035
train acc:  0.7734375
train loss:  0.48558226227760315
train gradient:  0.1184875605378571
iteration : 6036
train acc:  0.6875
train loss:  0.5389876365661621
train gradient:  0.1600700417525232
iteration : 6037
train acc:  0.75
train loss:  0.4647901654243469
train gradient:  0.1114500198958523
iteration : 6038
train acc:  0.71875
train loss:  0.5152377486228943
train gradient:  0.15981314475304287
iteration : 6039
train acc:  0.703125
train loss:  0.5622754096984863
train gradient:  0.1537289703717771
iteration : 6040
train acc:  0.734375
train loss:  0.5557436943054199
train gradient:  0.17736508195720974
iteration : 6041
train acc:  0.71875
train loss:  0.5393669605255127
train gradient:  0.15984006428329472
iteration : 6042
train acc:  0.7109375
train loss:  0.5202072262763977
train gradient:  0.15892728521663357
iteration : 6043
train acc:  0.75
train loss:  0.4801819324493408
train gradient:  0.12850790533827003
iteration : 6044
train acc:  0.703125
train loss:  0.5345999002456665
train gradient:  0.18146130182826653
iteration : 6045
train acc:  0.71875
train loss:  0.47660648822784424
train gradient:  0.1112413458920787
iteration : 6046
train acc:  0.7578125
train loss:  0.47869229316711426
train gradient:  0.10555412028668595
iteration : 6047
train acc:  0.703125
train loss:  0.5274181365966797
train gradient:  0.17690756898218668
iteration : 6048
train acc:  0.6875
train loss:  0.5009894371032715
train gradient:  0.11796184044643152
iteration : 6049
train acc:  0.7109375
train loss:  0.5351108312606812
train gradient:  0.16794400213935462
iteration : 6050
train acc:  0.71875
train loss:  0.5383230447769165
train gradient:  0.16184973569957134
iteration : 6051
train acc:  0.7109375
train loss:  0.5242500305175781
train gradient:  0.1528744874595674
iteration : 6052
train acc:  0.7265625
train loss:  0.5624526739120483
train gradient:  0.2450030804173598
iteration : 6053
train acc:  0.734375
train loss:  0.497670441865921
train gradient:  0.11340787160005365
iteration : 6054
train acc:  0.7421875
train loss:  0.4635656177997589
train gradient:  0.12194356027360274
iteration : 6055
train acc:  0.703125
train loss:  0.6390078067779541
train gradient:  0.21785975576226657
iteration : 6056
train acc:  0.7578125
train loss:  0.450053334236145
train gradient:  0.10020938476417715
iteration : 6057
train acc:  0.6640625
train loss:  0.5879490375518799
train gradient:  0.20896645832540042
iteration : 6058
train acc:  0.7578125
train loss:  0.49911800026893616
train gradient:  0.10784529665861445
iteration : 6059
train acc:  0.765625
train loss:  0.47569382190704346
train gradient:  0.10928598296700003
iteration : 6060
train acc:  0.78125
train loss:  0.4879539906978607
train gradient:  0.11856382695387983
iteration : 6061
train acc:  0.734375
train loss:  0.538330078125
train gradient:  0.13975338264923248
iteration : 6062
train acc:  0.7265625
train loss:  0.4954747259616852
train gradient:  0.14833360198309659
iteration : 6063
train acc:  0.7578125
train loss:  0.4494897425174713
train gradient:  0.10299442938477267
iteration : 6064
train acc:  0.7421875
train loss:  0.5157648324966431
train gradient:  0.16920220359783245
iteration : 6065
train acc:  0.703125
train loss:  0.5133466720581055
train gradient:  0.13416036612037416
iteration : 6066
train acc:  0.6796875
train loss:  0.5604041814804077
train gradient:  0.13675984477644465
iteration : 6067
train acc:  0.8125
train loss:  0.4508246183395386
train gradient:  0.14033295759450948
iteration : 6068
train acc:  0.7578125
train loss:  0.4394414722919464
train gradient:  0.09850935536588099
iteration : 6069
train acc:  0.765625
train loss:  0.4942012429237366
train gradient:  0.11970282047749047
iteration : 6070
train acc:  0.796875
train loss:  0.5223046541213989
train gradient:  0.19633367413450806
iteration : 6071
train acc:  0.7421875
train loss:  0.48096123337745667
train gradient:  0.13706979880212197
iteration : 6072
train acc:  0.7734375
train loss:  0.5043855905532837
train gradient:  0.1448576265793789
iteration : 6073
train acc:  0.75
train loss:  0.49546313285827637
train gradient:  0.12107154192171372
iteration : 6074
train acc:  0.7734375
train loss:  0.481832355260849
train gradient:  0.12656868026697388
iteration : 6075
train acc:  0.7578125
train loss:  0.45012861490249634
train gradient:  0.09492290585126152
iteration : 6076
train acc:  0.7421875
train loss:  0.4879809617996216
train gradient:  0.10674546519224604
iteration : 6077
train acc:  0.65625
train loss:  0.5185106992721558
train gradient:  0.15626088541465277
iteration : 6078
train acc:  0.765625
train loss:  0.4959122836589813
train gradient:  0.12163628690407056
iteration : 6079
train acc:  0.6875
train loss:  0.5508897304534912
train gradient:  0.1589697425718763
iteration : 6080
train acc:  0.71875
train loss:  0.5152032375335693
train gradient:  0.14984059782782905
iteration : 6081
train acc:  0.6796875
train loss:  0.5455674529075623
train gradient:  0.21501682639374295
iteration : 6082
train acc:  0.75
train loss:  0.5599204301834106
train gradient:  0.188562800884436
iteration : 6083
train acc:  0.703125
train loss:  0.5017141103744507
train gradient:  0.11765460661756683
iteration : 6084
train acc:  0.8203125
train loss:  0.46171116828918457
train gradient:  0.1311915684659502
iteration : 6085
train acc:  0.78125
train loss:  0.4160679280757904
train gradient:  0.08928723055770474
iteration : 6086
train acc:  0.6796875
train loss:  0.5688344240188599
train gradient:  0.18351601542938123
iteration : 6087
train acc:  0.71875
train loss:  0.4987538456916809
train gradient:  0.1691629529954905
iteration : 6088
train acc:  0.78125
train loss:  0.48583805561065674
train gradient:  0.12758894003876015
iteration : 6089
train acc:  0.6171875
train loss:  0.5811927318572998
train gradient:  0.16248476303697354
iteration : 6090
train acc:  0.75
train loss:  0.48241373896598816
train gradient:  0.11284808979741796
iteration : 6091
train acc:  0.71875
train loss:  0.530854344367981
train gradient:  0.15658768252015381
iteration : 6092
train acc:  0.7265625
train loss:  0.5026752948760986
train gradient:  0.1417615998206479
iteration : 6093
train acc:  0.78125
train loss:  0.4283331036567688
train gradient:  0.11063793218660357
iteration : 6094
train acc:  0.75
train loss:  0.5395097136497498
train gradient:  0.14327068999517922
iteration : 6095
train acc:  0.7578125
train loss:  0.4502932131290436
train gradient:  0.10486281384421929
iteration : 6096
train acc:  0.78125
train loss:  0.4401441216468811
train gradient:  0.13728414060091926
iteration : 6097
train acc:  0.78125
train loss:  0.46877971291542053
train gradient:  0.12147235865429323
iteration : 6098
train acc:  0.8203125
train loss:  0.4243391156196594
train gradient:  0.12930823751515552
iteration : 6099
train acc:  0.734375
train loss:  0.5455358028411865
train gradient:  0.17827360900990719
iteration : 6100
train acc:  0.765625
train loss:  0.4949854612350464
train gradient:  0.10670799879800724
iteration : 6101
train acc:  0.765625
train loss:  0.46219396591186523
train gradient:  0.0973071111053214
iteration : 6102
train acc:  0.78125
train loss:  0.48062989115715027
train gradient:  0.13617546915856626
iteration : 6103
train acc:  0.7421875
train loss:  0.4631480574607849
train gradient:  0.1244959573182041
iteration : 6104
train acc:  0.7109375
train loss:  0.4994228482246399
train gradient:  0.1417498134373411
iteration : 6105
train acc:  0.6796875
train loss:  0.5264755487442017
train gradient:  0.16204942018061752
iteration : 6106
train acc:  0.65625
train loss:  0.5958800315856934
train gradient:  0.1587981532083592
iteration : 6107
train acc:  0.75
train loss:  0.4876786172389984
train gradient:  0.16979455001490962
iteration : 6108
train acc:  0.6484375
train loss:  0.6286537051200867
train gradient:  0.3098294895282095
iteration : 6109
train acc:  0.75
train loss:  0.4952791929244995
train gradient:  0.1460411143679381
iteration : 6110
train acc:  0.7265625
train loss:  0.4756108522415161
train gradient:  0.1318061250319661
iteration : 6111
train acc:  0.7578125
train loss:  0.47769007086753845
train gradient:  0.11695210867908401
iteration : 6112
train acc:  0.6875
train loss:  0.5284348130226135
train gradient:  0.17100518050374994
iteration : 6113
train acc:  0.7265625
train loss:  0.5284454822540283
train gradient:  0.15955609103990775
iteration : 6114
train acc:  0.6953125
train loss:  0.5442583560943604
train gradient:  0.13908772284860468
iteration : 6115
train acc:  0.703125
train loss:  0.5361765623092651
train gradient:  0.1643879149759731
iteration : 6116
train acc:  0.7421875
train loss:  0.5165165662765503
train gradient:  0.1652815171866307
iteration : 6117
train acc:  0.796875
train loss:  0.4606078863143921
train gradient:  0.1145460619037266
iteration : 6118
train acc:  0.6875
train loss:  0.6038885116577148
train gradient:  0.19338355663442064
iteration : 6119
train acc:  0.7734375
train loss:  0.43853962421417236
train gradient:  0.11931765108054747
iteration : 6120
train acc:  0.703125
train loss:  0.5350887775421143
train gradient:  0.15082913578155246
iteration : 6121
train acc:  0.6640625
train loss:  0.5386027693748474
train gradient:  0.1516230205195021
iteration : 6122
train acc:  0.6328125
train loss:  0.5863871574401855
train gradient:  0.2335245284363161
iteration : 6123
train acc:  0.7109375
train loss:  0.5091495513916016
train gradient:  0.13889325180865025
iteration : 6124
train acc:  0.7265625
train loss:  0.5133391618728638
train gradient:  0.19722633745810794
iteration : 6125
train acc:  0.78125
train loss:  0.5114223957061768
train gradient:  0.16013084614320727
iteration : 6126
train acc:  0.765625
train loss:  0.45867353677749634
train gradient:  0.1519913639208063
iteration : 6127
train acc:  0.7421875
train loss:  0.5134736895561218
train gradient:  0.12918098297968686
iteration : 6128
train acc:  0.7421875
train loss:  0.502937912940979
train gradient:  0.1463297978084495
iteration : 6129
train acc:  0.71875
train loss:  0.5217911005020142
train gradient:  0.14518748260327566
iteration : 6130
train acc:  0.7265625
train loss:  0.5230552554130554
train gradient:  0.14395798351631162
iteration : 6131
train acc:  0.7265625
train loss:  0.5007746815681458
train gradient:  0.1421688960030555
iteration : 6132
train acc:  0.8046875
train loss:  0.4225691556930542
train gradient:  0.1109818322940294
iteration : 6133
train acc:  0.7578125
train loss:  0.450246125459671
train gradient:  0.10460114460413084
iteration : 6134
train acc:  0.78125
train loss:  0.4700799882411957
train gradient:  0.11363139432571445
iteration : 6135
train acc:  0.7421875
train loss:  0.5260393619537354
train gradient:  0.1198868189955831
iteration : 6136
train acc:  0.765625
train loss:  0.493664413690567
train gradient:  0.14244538868675788
iteration : 6137
train acc:  0.7265625
train loss:  0.5009939670562744
train gradient:  0.14340964907049303
iteration : 6138
train acc:  0.7578125
train loss:  0.5327203273773193
train gradient:  0.14397116779598562
iteration : 6139
train acc:  0.703125
train loss:  0.5125512480735779
train gradient:  0.12343913466648147
iteration : 6140
train acc:  0.765625
train loss:  0.5330690145492554
train gradient:  0.16210281090836304
iteration : 6141
train acc:  0.703125
train loss:  0.550233006477356
train gradient:  0.18816702987636014
iteration : 6142
train acc:  0.734375
train loss:  0.5032939910888672
train gradient:  0.22650107969422104
iteration : 6143
train acc:  0.7421875
train loss:  0.4756069481372833
train gradient:  0.11095944859757353
iteration : 6144
train acc:  0.6875
train loss:  0.5017786622047424
train gradient:  0.11939723811196248
iteration : 6145
train acc:  0.8046875
train loss:  0.45557641983032227
train gradient:  0.12135559509469768
iteration : 6146
train acc:  0.7109375
train loss:  0.5343022346496582
train gradient:  0.13150981077478824
iteration : 6147
train acc:  0.796875
train loss:  0.46171170473098755
train gradient:  0.10815579075580684
iteration : 6148
train acc:  0.7734375
train loss:  0.46101754903793335
train gradient:  0.09943359145698169
iteration : 6149
train acc:  0.75
train loss:  0.4906542897224426
train gradient:  0.12995301181332325
iteration : 6150
train acc:  0.765625
train loss:  0.5233396887779236
train gradient:  0.1708500634800857
iteration : 6151
train acc:  0.734375
train loss:  0.5429017543792725
train gradient:  0.20258301966585562
iteration : 6152
train acc:  0.78125
train loss:  0.4278794527053833
train gradient:  0.10596214804542828
iteration : 6153
train acc:  0.8515625
train loss:  0.44588708877563477
train gradient:  0.09184746186884724
iteration : 6154
train acc:  0.6875
train loss:  0.5256490111351013
train gradient:  0.1256779925576223
iteration : 6155
train acc:  0.6640625
train loss:  0.6053218841552734
train gradient:  0.22304487358913883
iteration : 6156
train acc:  0.78125
train loss:  0.5096450448036194
train gradient:  0.12004208223642752
iteration : 6157
train acc:  0.6875
train loss:  0.5710639953613281
train gradient:  0.15210739437261084
iteration : 6158
train acc:  0.7578125
train loss:  0.4796432852745056
train gradient:  0.13966833968765896
iteration : 6159
train acc:  0.703125
train loss:  0.5862381458282471
train gradient:  0.14800854885111864
iteration : 6160
train acc:  0.6640625
train loss:  0.5364664793014526
train gradient:  0.18729278866508958
iteration : 6161
train acc:  0.625
train loss:  0.605694055557251
train gradient:  0.1676796033614758
iteration : 6162
train acc:  0.75
train loss:  0.49247968196868896
train gradient:  0.14976021812646484
iteration : 6163
train acc:  0.7890625
train loss:  0.4479711055755615
train gradient:  0.11615184068064428
iteration : 6164
train acc:  0.7265625
train loss:  0.5704485177993774
train gradient:  0.14109013548795651
iteration : 6165
train acc:  0.71875
train loss:  0.48612096905708313
train gradient:  0.11694851161088259
iteration : 6166
train acc:  0.78125
train loss:  0.47238659858703613
train gradient:  0.13209658111383005
iteration : 6167
train acc:  0.734375
train loss:  0.4685033857822418
train gradient:  0.0977989622897961
iteration : 6168
train acc:  0.765625
train loss:  0.48119279742240906
train gradient:  0.1180513621826002
iteration : 6169
train acc:  0.6875
train loss:  0.4892934560775757
train gradient:  0.10524483918569395
iteration : 6170
train acc:  0.6796875
train loss:  0.5040016174316406
train gradient:  0.1252650467425292
iteration : 6171
train acc:  0.765625
train loss:  0.4597873389720917
train gradient:  0.09698417552424064
iteration : 6172
train acc:  0.71875
train loss:  0.5183383822441101
train gradient:  0.15553982428830657
iteration : 6173
train acc:  0.71875
train loss:  0.5640645623207092
train gradient:  0.2014308855667025
iteration : 6174
train acc:  0.703125
train loss:  0.51271653175354
train gradient:  0.1468783656186612
iteration : 6175
train acc:  0.796875
train loss:  0.45487868785858154
train gradient:  0.12277260591098711
iteration : 6176
train acc:  0.828125
train loss:  0.403648316860199
train gradient:  0.113371715753358
iteration : 6177
train acc:  0.71875
train loss:  0.5304292440414429
train gradient:  0.1267852740370186
iteration : 6178
train acc:  0.703125
train loss:  0.5486663579940796
train gradient:  0.13581262326135585
iteration : 6179
train acc:  0.6953125
train loss:  0.5501048564910889
train gradient:  0.14466916183278444
iteration : 6180
train acc:  0.6640625
train loss:  0.5877870917320251
train gradient:  0.16554768656409452
iteration : 6181
train acc:  0.6953125
train loss:  0.5774295926094055
train gradient:  0.17952261148102244
iteration : 6182
train acc:  0.78125
train loss:  0.5298564434051514
train gradient:  0.14780261780356846
iteration : 6183
train acc:  0.7109375
train loss:  0.5207343101501465
train gradient:  0.139861626742472
iteration : 6184
train acc:  0.7578125
train loss:  0.44024863839149475
train gradient:  0.11194381305163435
iteration : 6185
train acc:  0.7578125
train loss:  0.46852466464042664
train gradient:  0.12568293100249367
iteration : 6186
train acc:  0.71875
train loss:  0.4959062933921814
train gradient:  0.16603715591756246
iteration : 6187
train acc:  0.7734375
train loss:  0.4472215175628662
train gradient:  0.10838222917146273
iteration : 6188
train acc:  0.6953125
train loss:  0.5048282742500305
train gradient:  0.12300224463409723
iteration : 6189
train acc:  0.7734375
train loss:  0.46295875310897827
train gradient:  0.13384739742045382
iteration : 6190
train acc:  0.78125
train loss:  0.46999603509902954
train gradient:  0.1123014665703557
iteration : 6191
train acc:  0.765625
train loss:  0.47595807909965515
train gradient:  0.15423731420276676
iteration : 6192
train acc:  0.765625
train loss:  0.48716220259666443
train gradient:  0.11541111759759419
iteration : 6193
train acc:  0.7265625
train loss:  0.5224794745445251
train gradient:  0.13277706443408344
iteration : 6194
train acc:  0.7421875
train loss:  0.49702826142311096
train gradient:  0.1396154716658205
iteration : 6195
train acc:  0.8203125
train loss:  0.4389656186103821
train gradient:  0.13837312076639902
iteration : 6196
train acc:  0.7109375
train loss:  0.5344542264938354
train gradient:  0.12419849629616915
iteration : 6197
train acc:  0.7578125
train loss:  0.47238099575042725
train gradient:  0.15555108202874723
iteration : 6198
train acc:  0.734375
train loss:  0.4889008104801178
train gradient:  0.1360016582003692
iteration : 6199
train acc:  0.7578125
train loss:  0.49127694964408875
train gradient:  0.10749614079543819
iteration : 6200
train acc:  0.6953125
train loss:  0.5398764610290527
train gradient:  0.1399274049953431
iteration : 6201
train acc:  0.6953125
train loss:  0.5550543069839478
train gradient:  0.19381839261502418
iteration : 6202
train acc:  0.609375
train loss:  0.6050980687141418
train gradient:  0.21702718827955925
iteration : 6203
train acc:  0.7421875
train loss:  0.5069231390953064
train gradient:  0.15481499005800187
iteration : 6204
train acc:  0.7109375
train loss:  0.5201677083969116
train gradient:  0.16117311583229985
iteration : 6205
train acc:  0.7109375
train loss:  0.5104767084121704
train gradient:  0.11253207760572961
iteration : 6206
train acc:  0.765625
train loss:  0.44369786977767944
train gradient:  0.11099799583456176
iteration : 6207
train acc:  0.7734375
train loss:  0.46274542808532715
train gradient:  0.13975934075710994
iteration : 6208
train acc:  0.7109375
train loss:  0.5514752864837646
train gradient:  0.17256110565227334
iteration : 6209
train acc:  0.7421875
train loss:  0.4824484586715698
train gradient:  0.1266914015462528
iteration : 6210
train acc:  0.734375
train loss:  0.5133355855941772
train gradient:  0.15428576191760612
iteration : 6211
train acc:  0.75
train loss:  0.4530637562274933
train gradient:  0.08635392346573074
iteration : 6212
train acc:  0.734375
train loss:  0.4878198206424713
train gradient:  0.14893997685373503
iteration : 6213
train acc:  0.8203125
train loss:  0.43478837609291077
train gradient:  0.09091544707070819
iteration : 6214
train acc:  0.734375
train loss:  0.5377404689788818
train gradient:  0.19627893437643035
iteration : 6215
train acc:  0.7421875
train loss:  0.5456565022468567
train gradient:  0.2757685307117441
iteration : 6216
train acc:  0.8046875
train loss:  0.43179982900619507
train gradient:  0.11470598759966694
iteration : 6217
train acc:  0.734375
train loss:  0.4935147762298584
train gradient:  0.13929299274032964
iteration : 6218
train acc:  0.7734375
train loss:  0.4451237916946411
train gradient:  0.12559097425827948
iteration : 6219
train acc:  0.7578125
train loss:  0.4975736141204834
train gradient:  0.137159163346265
iteration : 6220
train acc:  0.7265625
train loss:  0.5362581014633179
train gradient:  0.15831394861211762
iteration : 6221
train acc:  0.7265625
train loss:  0.5067497491836548
train gradient:  0.15992936398368135
iteration : 6222
train acc:  0.7265625
train loss:  0.5047893524169922
train gradient:  0.1742706385012691
iteration : 6223
train acc:  0.734375
train loss:  0.5169072151184082
train gradient:  0.1423077108097034
iteration : 6224
train acc:  0.703125
train loss:  0.5351061820983887
train gradient:  0.170242324538526
iteration : 6225
train acc:  0.7109375
train loss:  0.5885789394378662
train gradient:  0.14187895335422562
iteration : 6226
train acc:  0.765625
train loss:  0.4889707863330841
train gradient:  0.12433478130363868
iteration : 6227
train acc:  0.7421875
train loss:  0.5263082981109619
train gradient:  0.14621036786080224
iteration : 6228
train acc:  0.6875
train loss:  0.5820305943489075
train gradient:  0.17138876865396563
iteration : 6229
train acc:  0.7890625
train loss:  0.45824718475341797
train gradient:  0.10211863681415911
iteration : 6230
train acc:  0.65625
train loss:  0.5478143095970154
train gradient:  0.16804267782730198
iteration : 6231
train acc:  0.7578125
train loss:  0.4898808002471924
train gradient:  0.10746832807083441
iteration : 6232
train acc:  0.765625
train loss:  0.4445191025733948
train gradient:  0.11257056252125011
iteration : 6233
train acc:  0.7578125
train loss:  0.49673938751220703
train gradient:  0.13759742546604137
iteration : 6234
train acc:  0.7734375
train loss:  0.4714981019496918
train gradient:  0.1367150976600489
iteration : 6235
train acc:  0.78125
train loss:  0.4370836019515991
train gradient:  0.12053817609096891
iteration : 6236
train acc:  0.71875
train loss:  0.4949629008769989
train gradient:  0.12253234631877889
iteration : 6237
train acc:  0.7421875
train loss:  0.4872314929962158
train gradient:  0.15812078937368795
iteration : 6238
train acc:  0.6875
train loss:  0.5216895341873169
train gradient:  0.15886522042437234
iteration : 6239
train acc:  0.765625
train loss:  0.5034345388412476
train gradient:  0.14535663582246833
iteration : 6240
train acc:  0.734375
train loss:  0.5028053522109985
train gradient:  0.15729425321699597
iteration : 6241
train acc:  0.671875
train loss:  0.5806063413619995
train gradient:  0.14323593205756296
iteration : 6242
train acc:  0.796875
train loss:  0.4532574415206909
train gradient:  0.10226615213424885
iteration : 6243
train acc:  0.7265625
train loss:  0.5348162055015564
train gradient:  0.15943697880308233
iteration : 6244
train acc:  0.6484375
train loss:  0.6382569074630737
train gradient:  0.17468781582780213
iteration : 6245
train acc:  0.6953125
train loss:  0.532481849193573
train gradient:  0.14401417282939438
iteration : 6246
train acc:  0.7578125
train loss:  0.4550800919532776
train gradient:  0.105756657570074
iteration : 6247
train acc:  0.7109375
train loss:  0.5490759611129761
train gradient:  0.16939714796303323
iteration : 6248
train acc:  0.65625
train loss:  0.5589289665222168
train gradient:  0.1817433396448775
iteration : 6249
train acc:  0.7734375
train loss:  0.4670490026473999
train gradient:  0.12274386083458147
iteration : 6250
train acc:  0.6640625
train loss:  0.5905948877334595
train gradient:  0.16910217148867152
iteration : 6251
train acc:  0.796875
train loss:  0.432142972946167
train gradient:  0.09356751597146387
iteration : 6252
train acc:  0.734375
train loss:  0.48632580041885376
train gradient:  0.12802779501407363
iteration : 6253
train acc:  0.75
train loss:  0.5224674940109253
train gradient:  0.11894134779406591
iteration : 6254
train acc:  0.7421875
train loss:  0.49632319808006287
train gradient:  0.11670954037246864
iteration : 6255
train acc:  0.703125
train loss:  0.5350983738899231
train gradient:  0.17728139987490005
iteration : 6256
train acc:  0.7421875
train loss:  0.46020713448524475
train gradient:  0.11659675456742256
iteration : 6257
train acc:  0.7421875
train loss:  0.5349822044372559
train gradient:  0.17966982202157972
iteration : 6258
train acc:  0.6953125
train loss:  0.5407716035842896
train gradient:  0.17914605606449543
iteration : 6259
train acc:  0.6953125
train loss:  0.5574783086776733
train gradient:  0.17627203738478053
iteration : 6260
train acc:  0.75
train loss:  0.5016176700592041
train gradient:  0.14798244379967818
iteration : 6261
train acc:  0.6484375
train loss:  0.6034590601921082
train gradient:  0.20663795355021042
iteration : 6262
train acc:  0.78125
train loss:  0.44594407081604004
train gradient:  0.10709221407369052
iteration : 6263
train acc:  0.71875
train loss:  0.5364808440208435
train gradient:  0.1369943187703607
iteration : 6264
train acc:  0.7734375
train loss:  0.4740443229675293
train gradient:  0.11478630335358321
iteration : 6265
train acc:  0.765625
train loss:  0.47803062200546265
train gradient:  0.1364527497422247
iteration : 6266
train acc:  0.796875
train loss:  0.4351012110710144
train gradient:  0.1008982245290693
iteration : 6267
train acc:  0.6796875
train loss:  0.5617462992668152
train gradient:  0.15782695113856987
iteration : 6268
train acc:  0.71875
train loss:  0.4982607960700989
train gradient:  0.11882431522296197
iteration : 6269
train acc:  0.71875
train loss:  0.5410195589065552
train gradient:  0.1277447639305641
iteration : 6270
train acc:  0.7734375
train loss:  0.4670105278491974
train gradient:  0.13504097279880728
iteration : 6271
train acc:  0.765625
train loss:  0.4769846796989441
train gradient:  0.11556820786966841
iteration : 6272
train acc:  0.703125
train loss:  0.5225067138671875
train gradient:  0.15456662871125826
iteration : 6273
train acc:  0.7734375
train loss:  0.4306764602661133
train gradient:  0.10015651360544099
iteration : 6274
train acc:  0.71875
train loss:  0.5630477666854858
train gradient:  0.1801431722066829
iteration : 6275
train acc:  0.7265625
train loss:  0.5440864562988281
train gradient:  0.13392922994350842
iteration : 6276
train acc:  0.75
train loss:  0.4992329180240631
train gradient:  0.14418831646626862
iteration : 6277
train acc:  0.7265625
train loss:  0.5787585377693176
train gradient:  0.2124300086802223
iteration : 6278
train acc:  0.828125
train loss:  0.426309734582901
train gradient:  0.1095483474680637
iteration : 6279
train acc:  0.671875
train loss:  0.605902910232544
train gradient:  0.16980837521116904
iteration : 6280
train acc:  0.7421875
train loss:  0.49992722272872925
train gradient:  0.13195246369207897
iteration : 6281
train acc:  0.6015625
train loss:  0.6027423143386841
train gradient:  0.1801459341902143
iteration : 6282
train acc:  0.671875
train loss:  0.5674310922622681
train gradient:  0.13097256472873642
iteration : 6283
train acc:  0.78125
train loss:  0.4621589779853821
train gradient:  0.11239397524921722
iteration : 6284
train acc:  0.7109375
train loss:  0.515647828578949
train gradient:  0.18089163688191373
iteration : 6285
train acc:  0.78125
train loss:  0.49928876757621765
train gradient:  0.12882265479243968
iteration : 6286
train acc:  0.7265625
train loss:  0.4822738766670227
train gradient:  0.1294135952000271
iteration : 6287
train acc:  0.71875
train loss:  0.5646464824676514
train gradient:  0.19557512097337787
iteration : 6288
train acc:  0.7578125
train loss:  0.46587133407592773
train gradient:  0.11460817996537431
iteration : 6289
train acc:  0.75
train loss:  0.4966965317726135
train gradient:  0.13406166035352637
iteration : 6290
train acc:  0.75
train loss:  0.4769691228866577
train gradient:  0.10948359402010865
iteration : 6291
train acc:  0.7578125
train loss:  0.4941959083080292
train gradient:  0.14473163957860488
iteration : 6292
train acc:  0.703125
train loss:  0.5231521129608154
train gradient:  0.1204577628169991
iteration : 6293
train acc:  0.78125
train loss:  0.46386727690696716
train gradient:  0.11131146742125504
iteration : 6294
train acc:  0.6875
train loss:  0.5700910091400146
train gradient:  0.21985124654765192
iteration : 6295
train acc:  0.734375
train loss:  0.5052083730697632
train gradient:  0.13028916506574104
iteration : 6296
train acc:  0.765625
train loss:  0.46634453535079956
train gradient:  0.10248444417820367
iteration : 6297
train acc:  0.75
train loss:  0.5194849967956543
train gradient:  0.16932851087674772
iteration : 6298
train acc:  0.703125
train loss:  0.5544880032539368
train gradient:  0.1525178091949035
iteration : 6299
train acc:  0.7109375
train loss:  0.5009888410568237
train gradient:  0.11896586333184737
iteration : 6300
train acc:  0.75
train loss:  0.5173414349555969
train gradient:  0.13869396247935778
iteration : 6301
train acc:  0.6875
train loss:  0.5492531061172485
train gradient:  0.14451701851490162
iteration : 6302
train acc:  0.7265625
train loss:  0.47892510890960693
train gradient:  0.09911847734764945
iteration : 6303
train acc:  0.6640625
train loss:  0.6036030650138855
train gradient:  0.16179060098620668
iteration : 6304
train acc:  0.7578125
train loss:  0.46894747018814087
train gradient:  0.11387266894858497
iteration : 6305
train acc:  0.7265625
train loss:  0.5005003213882446
train gradient:  0.16278230369340518
iteration : 6306
train acc:  0.7421875
train loss:  0.46375662088394165
train gradient:  0.11175248364827896
iteration : 6307
train acc:  0.8359375
train loss:  0.47036445140838623
train gradient:  0.13476764595896068
iteration : 6308
train acc:  0.7109375
train loss:  0.534735918045044
train gradient:  0.15067308470158852
iteration : 6309
train acc:  0.734375
train loss:  0.5437264442443848
train gradient:  0.1622804256395477
iteration : 6310
train acc:  0.734375
train loss:  0.5055747032165527
train gradient:  0.1304614160589443
iteration : 6311
train acc:  0.7421875
train loss:  0.4830288589000702
train gradient:  0.10953762689604994
iteration : 6312
train acc:  0.7578125
train loss:  0.49207770824432373
train gradient:  0.106875814762493
iteration : 6313
train acc:  0.65625
train loss:  0.6098313927650452
train gradient:  0.16873712782327385
iteration : 6314
train acc:  0.75
train loss:  0.5189126133918762
train gradient:  0.1710932397350831
iteration : 6315
train acc:  0.7421875
train loss:  0.49963507056236267
train gradient:  0.15555904079078964
iteration : 6316
train acc:  0.78125
train loss:  0.45022720098495483
train gradient:  0.134365758263705
iteration : 6317
train acc:  0.7578125
train loss:  0.44630709290504456
train gradient:  0.12873425789458506
iteration : 6318
train acc:  0.734375
train loss:  0.5256170034408569
train gradient:  0.14564094122117183
iteration : 6319
train acc:  0.734375
train loss:  0.545886218547821
train gradient:  0.17783746712034418
iteration : 6320
train acc:  0.765625
train loss:  0.47701287269592285
train gradient:  0.11891514682411299
iteration : 6321
train acc:  0.734375
train loss:  0.5022085905075073
train gradient:  0.1336120481077198
iteration : 6322
train acc:  0.7578125
train loss:  0.4611431062221527
train gradient:  0.15285421376035524
iteration : 6323
train acc:  0.7734375
train loss:  0.5327993631362915
train gradient:  0.16045857849953774
iteration : 6324
train acc:  0.796875
train loss:  0.4464215338230133
train gradient:  0.11068542686549158
iteration : 6325
train acc:  0.765625
train loss:  0.4497833251953125
train gradient:  0.134891339044319
iteration : 6326
train acc:  0.7265625
train loss:  0.5187942385673523
train gradient:  0.1336565734992855
iteration : 6327
train acc:  0.78125
train loss:  0.43956682085990906
train gradient:  0.10348588653015062
iteration : 6328
train acc:  0.7421875
train loss:  0.4509534239768982
train gradient:  0.11303702678453985
iteration : 6329
train acc:  0.765625
train loss:  0.4720374643802643
train gradient:  0.10674055212445269
iteration : 6330
train acc:  0.6484375
train loss:  0.6244885325431824
train gradient:  0.21716636425027808
iteration : 6331
train acc:  0.7578125
train loss:  0.48097658157348633
train gradient:  0.10360002222518012
iteration : 6332
train acc:  0.6875
train loss:  0.5558131337165833
train gradient:  0.16199624322095468
iteration : 6333
train acc:  0.78125
train loss:  0.4364747107028961
train gradient:  0.11788014544675986
iteration : 6334
train acc:  0.734375
train loss:  0.4952166676521301
train gradient:  0.11939465548230532
iteration : 6335
train acc:  0.7109375
train loss:  0.535179078578949
train gradient:  0.1112940932585319
iteration : 6336
train acc:  0.703125
train loss:  0.5534709692001343
train gradient:  0.1489521370525397
iteration : 6337
train acc:  0.7265625
train loss:  0.5310063362121582
train gradient:  0.19322073176238705
iteration : 6338
train acc:  0.7109375
train loss:  0.5362217426300049
train gradient:  0.17006891431397902
iteration : 6339
train acc:  0.71875
train loss:  0.50477135181427
train gradient:  0.11541748264604026
iteration : 6340
train acc:  0.7578125
train loss:  0.48921826481819153
train gradient:  0.14492820440678156
iteration : 6341
train acc:  0.75
train loss:  0.5347901582717896
train gradient:  0.17051136305047165
iteration : 6342
train acc:  0.6796875
train loss:  0.566994309425354
train gradient:  0.21802170633121654
iteration : 6343
train acc:  0.6484375
train loss:  0.552119791507721
train gradient:  0.13511765292939654
iteration : 6344
train acc:  0.765625
train loss:  0.4599777162075043
train gradient:  0.15817216717933114
iteration : 6345
train acc:  0.7421875
train loss:  0.4792661666870117
train gradient:  0.1338679537589207
iteration : 6346
train acc:  0.703125
train loss:  0.5226883292198181
train gradient:  0.12524343086070217
iteration : 6347
train acc:  0.7265625
train loss:  0.5198332667350769
train gradient:  0.1295874081313761
iteration : 6348
train acc:  0.7109375
train loss:  0.5773974061012268
train gradient:  0.13642030315162146
iteration : 6349
train acc:  0.7109375
train loss:  0.5530234575271606
train gradient:  0.17900576246726657
iteration : 6350
train acc:  0.7109375
train loss:  0.48117053508758545
train gradient:  0.16132696276541028
iteration : 6351
train acc:  0.6796875
train loss:  0.5358269810676575
train gradient:  0.13175624924801552
iteration : 6352
train acc:  0.7578125
train loss:  0.45787811279296875
train gradient:  0.11922057131673942
iteration : 6353
train acc:  0.6796875
train loss:  0.5827337503433228
train gradient:  0.16401450216091518
iteration : 6354
train acc:  0.75
train loss:  0.521190881729126
train gradient:  0.2179595343391282
iteration : 6355
train acc:  0.71875
train loss:  0.5188848972320557
train gradient:  0.1411956418960934
iteration : 6356
train acc:  0.703125
train loss:  0.5301207304000854
train gradient:  0.15796042930566273
iteration : 6357
train acc:  0.6875
train loss:  0.5932064056396484
train gradient:  0.16524627393806723
iteration : 6358
train acc:  0.7578125
train loss:  0.4901595711708069
train gradient:  0.17401723761344384
iteration : 6359
train acc:  0.765625
train loss:  0.47587713599205017
train gradient:  0.1322166112407357
iteration : 6360
train acc:  0.6953125
train loss:  0.5402299165725708
train gradient:  0.17061607057798495
iteration : 6361
train acc:  0.7734375
train loss:  0.49348801374435425
train gradient:  0.12401229880111393
iteration : 6362
train acc:  0.75
train loss:  0.46473461389541626
train gradient:  0.10479210662477395
iteration : 6363
train acc:  0.7109375
train loss:  0.5388382077217102
train gradient:  0.18105867480961976
iteration : 6364
train acc:  0.671875
train loss:  0.5526062250137329
train gradient:  0.19554055618401797
iteration : 6365
train acc:  0.71875
train loss:  0.4884414076805115
train gradient:  0.11757597859807496
iteration : 6366
train acc:  0.6875
train loss:  0.5578955411911011
train gradient:  0.14168553849074478
iteration : 6367
train acc:  0.7265625
train loss:  0.5172460675239563
train gradient:  0.15104113029062693
iteration : 6368
train acc:  0.6640625
train loss:  0.5539939403533936
train gradient:  0.15252873053993377
iteration : 6369
train acc:  0.765625
train loss:  0.4606904685497284
train gradient:  0.10596278097763767
iteration : 6370
train acc:  0.78125
train loss:  0.5065985918045044
train gradient:  0.17187337658036517
iteration : 6371
train acc:  0.7578125
train loss:  0.4816347062587738
train gradient:  0.10038465894773632
iteration : 6372
train acc:  0.765625
train loss:  0.44428306818008423
train gradient:  0.08859030062335897
iteration : 6373
train acc:  0.7265625
train loss:  0.5281199216842651
train gradient:  0.1762079504484164
iteration : 6374
train acc:  0.6953125
train loss:  0.5656816959381104
train gradient:  0.20528125630487493
iteration : 6375
train acc:  0.78125
train loss:  0.4919087886810303
train gradient:  0.1389535183954026
iteration : 6376
train acc:  0.6953125
train loss:  0.5579648017883301
train gradient:  0.14828261092778688
iteration : 6377
train acc:  0.7109375
train loss:  0.5284626483917236
train gradient:  0.16435837404244263
iteration : 6378
train acc:  0.734375
train loss:  0.49330776929855347
train gradient:  0.1418192081111867
iteration : 6379
train acc:  0.65625
train loss:  0.6044199466705322
train gradient:  0.22601327381473987
iteration : 6380
train acc:  0.7109375
train loss:  0.5863672494888306
train gradient:  0.14283836689320684
iteration : 6381
train acc:  0.75
train loss:  0.541499674320221
train gradient:  0.13300828924483798
iteration : 6382
train acc:  0.6953125
train loss:  0.5306290984153748
train gradient:  0.17733610142618642
iteration : 6383
train acc:  0.75
train loss:  0.49425646662712097
train gradient:  0.10866711109016351
iteration : 6384
train acc:  0.6953125
train loss:  0.554863452911377
train gradient:  0.16202799491148215
iteration : 6385
train acc:  0.7109375
train loss:  0.5493823289871216
train gradient:  0.1680144776825181
iteration : 6386
train acc:  0.71875
train loss:  0.46714678406715393
train gradient:  0.11584297018173739
iteration : 6387
train acc:  0.6875
train loss:  0.5191768407821655
train gradient:  0.13565619922546152
iteration : 6388
train acc:  0.7265625
train loss:  0.504159688949585
train gradient:  0.11814577303030943
iteration : 6389
train acc:  0.71875
train loss:  0.5104760527610779
train gradient:  0.10452581466294751
iteration : 6390
train acc:  0.671875
train loss:  0.5474016070365906
train gradient:  0.16010834049971612
iteration : 6391
train acc:  0.7265625
train loss:  0.5327597856521606
train gradient:  0.1433258937269133
iteration : 6392
train acc:  0.8046875
train loss:  0.44245898723602295
train gradient:  0.1248357155535634
iteration : 6393
train acc:  0.6875
train loss:  0.5951540470123291
train gradient:  0.19841318897609622
iteration : 6394
train acc:  0.71875
train loss:  0.5097485184669495
train gradient:  0.16608229658538848
iteration : 6395
train acc:  0.7734375
train loss:  0.4879077076911926
train gradient:  0.1266356083217517
iteration : 6396
train acc:  0.7578125
train loss:  0.4628625810146332
train gradient:  0.1160762514744402
iteration : 6397
train acc:  0.671875
train loss:  0.5574252009391785
train gradient:  0.1772703317243613
iteration : 6398
train acc:  0.8046875
train loss:  0.4397888481616974
train gradient:  0.0898230245560025
iteration : 6399
train acc:  0.7734375
train loss:  0.5264396071434021
train gradient:  0.1432735882112935
iteration : 6400
train acc:  0.734375
train loss:  0.5306078195571899
train gradient:  0.1446102330313515
iteration : 6401
train acc:  0.796875
train loss:  0.4619697332382202
train gradient:  0.13512420467914943
iteration : 6402
train acc:  0.703125
train loss:  0.5269885063171387
train gradient:  0.21506047625213082
iteration : 6403
train acc:  0.78125
train loss:  0.4969920516014099
train gradient:  0.14077983254325466
iteration : 6404
train acc:  0.71875
train loss:  0.5239136815071106
train gradient:  0.1522605991440198
iteration : 6405
train acc:  0.75
train loss:  0.47969794273376465
train gradient:  0.09674104027418434
iteration : 6406
train acc:  0.734375
train loss:  0.5010879039764404
train gradient:  0.12707353537343175
iteration : 6407
train acc:  0.703125
train loss:  0.5266726016998291
train gradient:  0.2008359657084846
iteration : 6408
train acc:  0.6953125
train loss:  0.5118144154548645
train gradient:  0.1577572685009465
iteration : 6409
train acc:  0.7421875
train loss:  0.5172908306121826
train gradient:  0.1188265108096886
iteration : 6410
train acc:  0.75
train loss:  0.51609206199646
train gradient:  0.13355376707862612
iteration : 6411
train acc:  0.75
train loss:  0.5108038187026978
train gradient:  0.11897251740905379
iteration : 6412
train acc:  0.78125
train loss:  0.47262144088745117
train gradient:  0.0991862915161007
iteration : 6413
train acc:  0.6953125
train loss:  0.5274840593338013
train gradient:  0.14411520138942605
iteration : 6414
train acc:  0.75
train loss:  0.4880034923553467
train gradient:  0.13169907158140065
iteration : 6415
train acc:  0.6875
train loss:  0.5418415069580078
train gradient:  0.14262280190842608
iteration : 6416
train acc:  0.71875
train loss:  0.5356046557426453
train gradient:  0.19051480368668905
iteration : 6417
train acc:  0.7265625
train loss:  0.5181612372398376
train gradient:  0.18204572672503394
iteration : 6418
train acc:  0.7578125
train loss:  0.5529527068138123
train gradient:  0.18978207954017362
iteration : 6419
train acc:  0.671875
train loss:  0.5569011569023132
train gradient:  0.12362230328862316
iteration : 6420
train acc:  0.59375
train loss:  0.5959301590919495
train gradient:  0.2531907378527814
iteration : 6421
train acc:  0.65625
train loss:  0.5298848152160645
train gradient:  0.15049808755992045
iteration : 6422
train acc:  0.734375
train loss:  0.5124506950378418
train gradient:  0.19849632757236058
iteration : 6423
train acc:  0.6640625
train loss:  0.5479981899261475
train gradient:  0.1242433705422067
iteration : 6424
train acc:  0.6796875
train loss:  0.5344945192337036
train gradient:  0.14779994918298595
iteration : 6425
train acc:  0.6953125
train loss:  0.5842718482017517
train gradient:  0.16103696376291218
iteration : 6426
train acc:  0.7578125
train loss:  0.4977673292160034
train gradient:  0.1470802329596391
iteration : 6427
train acc:  0.7578125
train loss:  0.5052950978279114
train gradient:  0.1294632055179845
iteration : 6428
train acc:  0.796875
train loss:  0.42497149109840393
train gradient:  0.10067804085113433
iteration : 6429
train acc:  0.703125
train loss:  0.5775451064109802
train gradient:  0.14898842743593077
iteration : 6430
train acc:  0.7109375
train loss:  0.5292154550552368
train gradient:  0.1731383806680178
iteration : 6431
train acc:  0.7265625
train loss:  0.4716702997684479
train gradient:  0.15880047676529574
iteration : 6432
train acc:  0.734375
train loss:  0.511588454246521
train gradient:  0.12507296783220234
iteration : 6433
train acc:  0.7109375
train loss:  0.5137004852294922
train gradient:  0.1469206032184564
iteration : 6434
train acc:  0.7890625
train loss:  0.4384937286376953
train gradient:  0.10312006246167799
iteration : 6435
train acc:  0.7578125
train loss:  0.4580102562904358
train gradient:  0.11270597637523953
iteration : 6436
train acc:  0.6953125
train loss:  0.5324656963348389
train gradient:  0.11850411190250808
iteration : 6437
train acc:  0.7734375
train loss:  0.4750918745994568
train gradient:  0.12275852784375753
iteration : 6438
train acc:  0.7890625
train loss:  0.49342435598373413
train gradient:  0.13534500122558013
iteration : 6439
train acc:  0.75
train loss:  0.4965820908546448
train gradient:  0.1272964532373965
iteration : 6440
train acc:  0.7890625
train loss:  0.45330923795700073
train gradient:  0.10683593298156681
iteration : 6441
train acc:  0.71875
train loss:  0.571161150932312
train gradient:  0.19949842003405308
iteration : 6442
train acc:  0.7109375
train loss:  0.501453161239624
train gradient:  0.15866035211846052
iteration : 6443
train acc:  0.75
train loss:  0.5189383029937744
train gradient:  0.1524453414677097
iteration : 6444
train acc:  0.6875
train loss:  0.5926191806793213
train gradient:  0.21250353547350365
iteration : 6445
train acc:  0.78125
train loss:  0.45710688829421997
train gradient:  0.11248425243682851
iteration : 6446
train acc:  0.7109375
train loss:  0.5074376463890076
train gradient:  0.14321384736816067
iteration : 6447
train acc:  0.765625
train loss:  0.4624702036380768
train gradient:  0.12592679144351165
iteration : 6448
train acc:  0.7578125
train loss:  0.4973407983779907
train gradient:  0.1399005182099422
iteration : 6449
train acc:  0.765625
train loss:  0.4668828248977661
train gradient:  0.12523449499255115
iteration : 6450
train acc:  0.71875
train loss:  0.49229714274406433
train gradient:  0.10274984498087043
iteration : 6451
train acc:  0.703125
train loss:  0.5253837704658508
train gradient:  0.1442855622913758
iteration : 6452
train acc:  0.71875
train loss:  0.4915994107723236
train gradient:  0.12933755332471522
iteration : 6453
train acc:  0.765625
train loss:  0.4632188081741333
train gradient:  0.1022084475870218
iteration : 6454
train acc:  0.8203125
train loss:  0.4188101887702942
train gradient:  0.09190958934249625
iteration : 6455
train acc:  0.6796875
train loss:  0.5931571125984192
train gradient:  0.17450114769434488
iteration : 6456
train acc:  0.71875
train loss:  0.5408389568328857
train gradient:  0.1582776853037608
iteration : 6457
train acc:  0.7421875
train loss:  0.49414175748825073
train gradient:  0.18069592782609062
iteration : 6458
train acc:  0.6953125
train loss:  0.5195221304893494
train gradient:  0.10272967470503061
iteration : 6459
train acc:  0.7265625
train loss:  0.4986984133720398
train gradient:  0.14338971007825257
iteration : 6460
train acc:  0.7421875
train loss:  0.478828102350235
train gradient:  0.15191050585231325
iteration : 6461
train acc:  0.7890625
train loss:  0.4894777834415436
train gradient:  0.14435486033999265
iteration : 6462
train acc:  0.7578125
train loss:  0.46962523460388184
train gradient:  0.08570558022529469
iteration : 6463
train acc:  0.7109375
train loss:  0.5055780410766602
train gradient:  0.10492892976471528
iteration : 6464
train acc:  0.6953125
train loss:  0.5042868256568909
train gradient:  0.11435599208425579
iteration : 6465
train acc:  0.75
train loss:  0.47879868745803833
train gradient:  0.11184339373665218
iteration : 6466
train acc:  0.75
train loss:  0.49993911385536194
train gradient:  0.14319106630771
iteration : 6467
train acc:  0.7890625
train loss:  0.4269331097602844
train gradient:  0.08768526898274545
iteration : 6468
train acc:  0.7109375
train loss:  0.550329864025116
train gradient:  0.14317276067658619
iteration : 6469
train acc:  0.7578125
train loss:  0.5233186483383179
train gradient:  0.17613293853958212
iteration : 6470
train acc:  0.7265625
train loss:  0.5153817534446716
train gradient:  0.11960025670693844
iteration : 6471
train acc:  0.6640625
train loss:  0.53764808177948
train gradient:  0.17850811535797356
iteration : 6472
train acc:  0.7421875
train loss:  0.5292123556137085
train gradient:  0.1297359495969266
iteration : 6473
train acc:  0.765625
train loss:  0.4857165813446045
train gradient:  0.12549675789544418
iteration : 6474
train acc:  0.7734375
train loss:  0.4756683111190796
train gradient:  0.10979575133701419
iteration : 6475
train acc:  0.7109375
train loss:  0.5548467636108398
train gradient:  0.22887807742617522
iteration : 6476
train acc:  0.765625
train loss:  0.46190208196640015
train gradient:  0.12108260971700978
iteration : 6477
train acc:  0.7734375
train loss:  0.4788840413093567
train gradient:  0.11986382008184658
iteration : 6478
train acc:  0.71875
train loss:  0.5608959197998047
train gradient:  0.23963277873270022
iteration : 6479
train acc:  0.8125
train loss:  0.4677373170852661
train gradient:  0.1072013850935041
iteration : 6480
train acc:  0.7421875
train loss:  0.5113250017166138
train gradient:  0.231685465939858
iteration : 6481
train acc:  0.6640625
train loss:  0.5511024594306946
train gradient:  0.14474618830042474
iteration : 6482
train acc:  0.6796875
train loss:  0.5570462942123413
train gradient:  0.17170570609846347
iteration : 6483
train acc:  0.7734375
train loss:  0.45458364486694336
train gradient:  0.1108754138714284
iteration : 6484
train acc:  0.7734375
train loss:  0.47131603956222534
train gradient:  0.11188899817932446
iteration : 6485
train acc:  0.8125
train loss:  0.4531077444553375
train gradient:  0.12723415344274386
iteration : 6486
train acc:  0.734375
train loss:  0.5306919813156128
train gradient:  0.11104430450093122
iteration : 6487
train acc:  0.640625
train loss:  0.6001666784286499
train gradient:  0.17798059701035523
iteration : 6488
train acc:  0.703125
train loss:  0.506786584854126
train gradient:  0.09804467386739875
iteration : 6489
train acc:  0.7578125
train loss:  0.4787546396255493
train gradient:  0.13380527358778765
iteration : 6490
train acc:  0.765625
train loss:  0.45284122228622437
train gradient:  0.10152939200828773
iteration : 6491
train acc:  0.7265625
train loss:  0.5067700743675232
train gradient:  0.13501290785981412
iteration : 6492
train acc:  0.734375
train loss:  0.4842921197414398
train gradient:  0.12901558266549049
iteration : 6493
train acc:  0.7421875
train loss:  0.49796345829963684
train gradient:  0.1433255160795102
iteration : 6494
train acc:  0.765625
train loss:  0.4815908670425415
train gradient:  0.14198964227226885
iteration : 6495
train acc:  0.65625
train loss:  0.5465513467788696
train gradient:  0.13371880299031264
iteration : 6496
train acc:  0.65625
train loss:  0.5842944979667664
train gradient:  0.173115719706732
iteration : 6497
train acc:  0.6640625
train loss:  0.510328471660614
train gradient:  0.14212587131469162
iteration : 6498
train acc:  0.6875
train loss:  0.5894346237182617
train gradient:  0.2121331472137224
iteration : 6499
train acc:  0.75
train loss:  0.43025511503219604
train gradient:  0.08772615580544116
iteration : 6500
train acc:  0.6796875
train loss:  0.5550000667572021
train gradient:  0.18383918891524642
iteration : 6501
train acc:  0.796875
train loss:  0.44329166412353516
train gradient:  0.11216135348841887
iteration : 6502
train acc:  0.7109375
train loss:  0.5098353624343872
train gradient:  0.12650155328304796
iteration : 6503
train acc:  0.6953125
train loss:  0.5642313361167908
train gradient:  0.19933959816331892
iteration : 6504
train acc:  0.7734375
train loss:  0.4674140214920044
train gradient:  0.11799549494887021
iteration : 6505
train acc:  0.7890625
train loss:  0.4296179413795471
train gradient:  0.11477142443265505
iteration : 6506
train acc:  0.671875
train loss:  0.5508778095245361
train gradient:  0.1342149952123222
iteration : 6507
train acc:  0.703125
train loss:  0.5404720902442932
train gradient:  0.15782226531068883
iteration : 6508
train acc:  0.7421875
train loss:  0.4962612986564636
train gradient:  0.12932038042414545
iteration : 6509
train acc:  0.78125
train loss:  0.48009413480758667
train gradient:  0.13730018569532665
iteration : 6510
train acc:  0.75
train loss:  0.5264279842376709
train gradient:  0.17970115180553797
iteration : 6511
train acc:  0.8125
train loss:  0.40775126218795776
train gradient:  0.11086138042087615
iteration : 6512
train acc:  0.6640625
train loss:  0.5524041056632996
train gradient:  0.13882836385467776
iteration : 6513
train acc:  0.71875
train loss:  0.5149841904640198
train gradient:  0.12205039741376628
iteration : 6514
train acc:  0.6953125
train loss:  0.4981687664985657
train gradient:  0.1341478062792012
iteration : 6515
train acc:  0.71875
train loss:  0.5208821892738342
train gradient:  0.10989840159277509
iteration : 6516
train acc:  0.7578125
train loss:  0.5200542211532593
train gradient:  0.13280146177228747
iteration : 6517
train acc:  0.7109375
train loss:  0.5529589653015137
train gradient:  0.16478535426291446
iteration : 6518
train acc:  0.6875
train loss:  0.5998092889785767
train gradient:  0.17502594541478544
iteration : 6519
train acc:  0.6875
train loss:  0.5609338283538818
train gradient:  0.1981299977639619
iteration : 6520
train acc:  0.7109375
train loss:  0.5243626236915588
train gradient:  0.11918414504625875
iteration : 6521
train acc:  0.6953125
train loss:  0.4950355887413025
train gradient:  0.1237368462696511
iteration : 6522
train acc:  0.7265625
train loss:  0.5069836378097534
train gradient:  0.10734599423360291
iteration : 6523
train acc:  0.7578125
train loss:  0.44803309440612793
train gradient:  0.13441761137804073
iteration : 6524
train acc:  0.734375
train loss:  0.5126639604568481
train gradient:  0.16083210750052448
iteration : 6525
train acc:  0.8125
train loss:  0.4202095568180084
train gradient:  0.09939361970476991
iteration : 6526
train acc:  0.765625
train loss:  0.47149932384490967
train gradient:  0.1257862763938159
iteration : 6527
train acc:  0.7421875
train loss:  0.5036493539810181
train gradient:  0.16103147421196376
iteration : 6528
train acc:  0.75
train loss:  0.5092517733573914
train gradient:  0.13229912394428964
iteration : 6529
train acc:  0.796875
train loss:  0.4583805799484253
train gradient:  0.11989325033300716
iteration : 6530
train acc:  0.7265625
train loss:  0.4993212819099426
train gradient:  0.12567491024247163
iteration : 6531
train acc:  0.6953125
train loss:  0.5725628137588501
train gradient:  0.12348252050029312
iteration : 6532
train acc:  0.75
train loss:  0.49793440103530884
train gradient:  0.1112776019376113
iteration : 6533
train acc:  0.640625
train loss:  0.6031813025474548
train gradient:  0.15967989772496582
iteration : 6534
train acc:  0.7265625
train loss:  0.5136220455169678
train gradient:  0.13179499287036972
iteration : 6535
train acc:  0.75
train loss:  0.5472387075424194
train gradient:  0.141937886097172
iteration : 6536
train acc:  0.75
train loss:  0.5318351984024048
train gradient:  0.13469137035199352
iteration : 6537
train acc:  0.8125
train loss:  0.4256066381931305
train gradient:  0.09209743206698712
iteration : 6538
train acc:  0.7265625
train loss:  0.5865911245346069
train gradient:  0.1550511868504756
iteration : 6539
train acc:  0.7421875
train loss:  0.48788920044898987
train gradient:  0.12798207132131093
iteration : 6540
train acc:  0.59375
train loss:  0.674561083316803
train gradient:  0.24431851017748285
iteration : 6541
train acc:  0.765625
train loss:  0.5054451823234558
train gradient:  0.12134575679175473
iteration : 6542
train acc:  0.765625
train loss:  0.4724440574645996
train gradient:  0.10253442846784044
iteration : 6543
train acc:  0.75
train loss:  0.49537762999534607
train gradient:  0.16263500269088504
iteration : 6544
train acc:  0.7890625
train loss:  0.49373483657836914
train gradient:  0.14060329516291276
iteration : 6545
train acc:  0.8046875
train loss:  0.4517859220504761
train gradient:  0.08968924853926681
iteration : 6546
train acc:  0.734375
train loss:  0.5171205997467041
train gradient:  0.12904365591495995
iteration : 6547
train acc:  0.8125
train loss:  0.41818946599960327
train gradient:  0.09587846481466657
iteration : 6548
train acc:  0.7421875
train loss:  0.5103482604026794
train gradient:  0.17678994070175838
iteration : 6549
train acc:  0.7421875
train loss:  0.5182763338088989
train gradient:  0.12935353132577634
iteration : 6550
train acc:  0.7109375
train loss:  0.530835747718811
train gradient:  0.13901283282473711
iteration : 6551
train acc:  0.765625
train loss:  0.48638349771499634
train gradient:  0.11030183627389985
iteration : 6552
train acc:  0.6875
train loss:  0.5378643274307251
train gradient:  0.10886045115231965
iteration : 6553
train acc:  0.734375
train loss:  0.5409788489341736
train gradient:  0.21026156354127
iteration : 6554
train acc:  0.71875
train loss:  0.5240908861160278
train gradient:  0.1707018761847952
iteration : 6555
train acc:  0.78125
train loss:  0.42818981409072876
train gradient:  0.1399027788327267
iteration : 6556
train acc:  0.703125
train loss:  0.5692336559295654
train gradient:  0.14505544815808558
iteration : 6557
train acc:  0.7265625
train loss:  0.5443679094314575
train gradient:  0.1246790323965695
iteration : 6558
train acc:  0.828125
train loss:  0.45372897386550903
train gradient:  0.1034105702249384
iteration : 6559
train acc:  0.8203125
train loss:  0.44189751148223877
train gradient:  0.12401061066170511
iteration : 6560
train acc:  0.6953125
train loss:  0.542371928691864
train gradient:  0.13969106553470986
iteration : 6561
train acc:  0.7890625
train loss:  0.4782411754131317
train gradient:  0.14455841587608564
iteration : 6562
train acc:  0.71875
train loss:  0.4755151867866516
train gradient:  0.12764776753322565
iteration : 6563
train acc:  0.765625
train loss:  0.4873489737510681
train gradient:  0.12588725656693847
iteration : 6564
train acc:  0.7421875
train loss:  0.4999547004699707
train gradient:  0.12241566559827247
iteration : 6565
train acc:  0.671875
train loss:  0.5775930285453796
train gradient:  0.23581676586021763
iteration : 6566
train acc:  0.7578125
train loss:  0.5044912695884705
train gradient:  0.1355027981057883
iteration : 6567
train acc:  0.6953125
train loss:  0.6126922965049744
train gradient:  0.24891715325662528
iteration : 6568
train acc:  0.7734375
train loss:  0.44637569785118103
train gradient:  0.1311837319360778
iteration : 6569
train acc:  0.7265625
train loss:  0.5153151750564575
train gradient:  0.1223902986894229
iteration : 6570
train acc:  0.6875
train loss:  0.5562036633491516
train gradient:  0.14571523384319143
iteration : 6571
train acc:  0.7265625
train loss:  0.5125159025192261
train gradient:  0.1286195308788225
iteration : 6572
train acc:  0.7890625
train loss:  0.4645542502403259
train gradient:  0.09890028742927795
iteration : 6573
train acc:  0.734375
train loss:  0.5351271629333496
train gradient:  0.1487197651231802
iteration : 6574
train acc:  0.6953125
train loss:  0.5230963230133057
train gradient:  0.14719070019826458
iteration : 6575
train acc:  0.78125
train loss:  0.4657100737094879
train gradient:  0.0955657388113829
iteration : 6576
train acc:  0.671875
train loss:  0.5782554149627686
train gradient:  0.22088459445903974
iteration : 6577
train acc:  0.671875
train loss:  0.5742906332015991
train gradient:  0.16886067874707433
iteration : 6578
train acc:  0.703125
train loss:  0.5035049319267273
train gradient:  0.12224091448202264
iteration : 6579
train acc:  0.71875
train loss:  0.5089714527130127
train gradient:  0.11560086344917615
iteration : 6580
train acc:  0.75
train loss:  0.5022435188293457
train gradient:  0.13667937923129897
iteration : 6581
train acc:  0.734375
train loss:  0.5335550308227539
train gradient:  0.1733000200293367
iteration : 6582
train acc:  0.7265625
train loss:  0.4841630756855011
train gradient:  0.11606800102720122
iteration : 6583
train acc:  0.78125
train loss:  0.4687493145465851
train gradient:  0.1159929526460306
iteration : 6584
train acc:  0.78125
train loss:  0.4706932008266449
train gradient:  0.11258447238820725
iteration : 6585
train acc:  0.8515625
train loss:  0.4748765230178833
train gradient:  0.14073522170418543
iteration : 6586
train acc:  0.6875
train loss:  0.5304301977157593
train gradient:  0.14504928517297105
iteration : 6587
train acc:  0.75
train loss:  0.5141600370407104
train gradient:  0.13027873073197094
iteration : 6588
train acc:  0.6640625
train loss:  0.6039578914642334
train gradient:  0.22524128366440632
iteration : 6589
train acc:  0.7734375
train loss:  0.4603814482688904
train gradient:  0.11338237002790141
iteration : 6590
train acc:  0.71875
train loss:  0.519344687461853
train gradient:  0.137422358707296
iteration : 6591
train acc:  0.7265625
train loss:  0.4996517598628998
train gradient:  0.14717384317902227
iteration : 6592
train acc:  0.734375
train loss:  0.5005618333816528
train gradient:  0.15147647807841463
iteration : 6593
train acc:  0.7109375
train loss:  0.5387601852416992
train gradient:  0.16395631300967634
iteration : 6594
train acc:  0.7890625
train loss:  0.420036643743515
train gradient:  0.11732671684127209
iteration : 6595
train acc:  0.78125
train loss:  0.4489489197731018
train gradient:  0.10812082904677152
iteration : 6596
train acc:  0.796875
train loss:  0.4388344883918762
train gradient:  0.11922249469708152
iteration : 6597
train acc:  0.75
train loss:  0.43275186419487
train gradient:  0.10938856109860673
iteration : 6598
train acc:  0.7265625
train loss:  0.538297176361084
train gradient:  0.1761546427715079
iteration : 6599
train acc:  0.6875
train loss:  0.5284112691879272
train gradient:  0.1493962210673066
iteration : 6600
train acc:  0.8125
train loss:  0.45075759291648865
train gradient:  0.11198287229425574
iteration : 6601
train acc:  0.7109375
train loss:  0.5185147523880005
train gradient:  0.132992529944984
iteration : 6602
train acc:  0.7421875
train loss:  0.5027613639831543
train gradient:  0.12955613119678433
iteration : 6603
train acc:  0.7578125
train loss:  0.49515631794929504
train gradient:  0.1433794450456089
iteration : 6604
train acc:  0.7734375
train loss:  0.47431865334510803
train gradient:  0.11654447034391259
iteration : 6605
train acc:  0.75
train loss:  0.48833248019218445
train gradient:  0.12536632740457343
iteration : 6606
train acc:  0.6953125
train loss:  0.5362125635147095
train gradient:  0.1414703987502039
iteration : 6607
train acc:  0.6953125
train loss:  0.5413531064987183
train gradient:  0.13468134256963857
iteration : 6608
train acc:  0.78125
train loss:  0.4675710201263428
train gradient:  0.17881812494280974
iteration : 6609
train acc:  0.7734375
train loss:  0.4529324173927307
train gradient:  0.14102910013058242
iteration : 6610
train acc:  0.703125
train loss:  0.5350459814071655
train gradient:  0.13829644295300025
iteration : 6611
train acc:  0.6953125
train loss:  0.5333397388458252
train gradient:  0.1270233364280925
iteration : 6612
train acc:  0.7734375
train loss:  0.4679974913597107
train gradient:  0.09882435228798274
iteration : 6613
train acc:  0.7265625
train loss:  0.5162187218666077
train gradient:  0.13839644160612274
iteration : 6614
train acc:  0.65625
train loss:  0.6188574433326721
train gradient:  0.2325311790528204
iteration : 6615
train acc:  0.71875
train loss:  0.5518980026245117
train gradient:  0.1627212740906911
iteration : 6616
train acc:  0.75
train loss:  0.49307897686958313
train gradient:  0.13570115113469225
iteration : 6617
train acc:  0.703125
train loss:  0.5903735756874084
train gradient:  0.15166800775945682
iteration : 6618
train acc:  0.765625
train loss:  0.47234803438186646
train gradient:  0.2243338133668547
iteration : 6619
train acc:  0.6953125
train loss:  0.5307906270027161
train gradient:  0.16639384156110776
iteration : 6620
train acc:  0.7890625
train loss:  0.4390617311000824
train gradient:  0.12137961028099462
iteration : 6621
train acc:  0.765625
train loss:  0.5213176012039185
train gradient:  0.15497078733160627
iteration : 6622
train acc:  0.6796875
train loss:  0.5744726061820984
train gradient:  0.15497696343185363
iteration : 6623
train acc:  0.703125
train loss:  0.5341156125068665
train gradient:  0.1767013110715404
iteration : 6624
train acc:  0.7421875
train loss:  0.4836125373840332
train gradient:  0.11935469313240205
iteration : 6625
train acc:  0.6796875
train loss:  0.47281286120414734
train gradient:  0.12442375339257393
iteration : 6626
train acc:  0.7734375
train loss:  0.4298399090766907
train gradient:  0.09159090196046327
iteration : 6627
train acc:  0.7265625
train loss:  0.535342276096344
train gradient:  0.12393532174303762
iteration : 6628
train acc:  0.7578125
train loss:  0.49061018228530884
train gradient:  0.148512251473423
iteration : 6629
train acc:  0.78125
train loss:  0.43482351303100586
train gradient:  0.10595692780348316
iteration : 6630
train acc:  0.8046875
train loss:  0.44262954592704773
train gradient:  0.10610396871841495
iteration : 6631
train acc:  0.7265625
train loss:  0.5217150449752808
train gradient:  0.13194236220433228
iteration : 6632
train acc:  0.7421875
train loss:  0.5344154238700867
train gradient:  0.14853474917351125
iteration : 6633
train acc:  0.71875
train loss:  0.5021572113037109
train gradient:  0.13381405250281875
iteration : 6634
train acc:  0.75
train loss:  0.5470547676086426
train gradient:  0.1757249257663825
iteration : 6635
train acc:  0.765625
train loss:  0.4539485573768616
train gradient:  0.13131965843312343
iteration : 6636
train acc:  0.7578125
train loss:  0.4815906286239624
train gradient:  0.11298037695709948
iteration : 6637
train acc:  0.7734375
train loss:  0.5118554830551147
train gradient:  0.11282367245063014
iteration : 6638
train acc:  0.7265625
train loss:  0.551766574382782
train gradient:  0.1447376978827496
iteration : 6639
train acc:  0.78125
train loss:  0.4704272150993347
train gradient:  0.144193596606398
iteration : 6640
train acc:  0.7578125
train loss:  0.49406957626342773
train gradient:  0.12109937741385816
iteration : 6641
train acc:  0.6953125
train loss:  0.537581205368042
train gradient:  0.14415880641892007
iteration : 6642
train acc:  0.7734375
train loss:  0.4867675006389618
train gradient:  0.1418334941693315
iteration : 6643
train acc:  0.6953125
train loss:  0.6120977401733398
train gradient:  0.19595375866870518
iteration : 6644
train acc:  0.6875
train loss:  0.5738943815231323
train gradient:  0.19856578526181223
iteration : 6645
train acc:  0.78125
train loss:  0.44538456201553345
train gradient:  0.13991235658155576
iteration : 6646
train acc:  0.6953125
train loss:  0.5652036666870117
train gradient:  0.19421332957624146
iteration : 6647
train acc:  0.7734375
train loss:  0.48034757375717163
train gradient:  0.14573996736705971
iteration : 6648
train acc:  0.7109375
train loss:  0.523784339427948
train gradient:  0.17193748176655643
iteration : 6649
train acc:  0.7265625
train loss:  0.49131399393081665
train gradient:  0.1292678318627707
iteration : 6650
train acc:  0.796875
train loss:  0.42997872829437256
train gradient:  0.1155794924576162
iteration : 6651
train acc:  0.7734375
train loss:  0.5150695443153381
train gradient:  0.1071603518773019
iteration : 6652
train acc:  0.71875
train loss:  0.5091623663902283
train gradient:  0.14818334300847963
iteration : 6653
train acc:  0.734375
train loss:  0.49602559208869934
train gradient:  0.1287703108908826
iteration : 6654
train acc:  0.7109375
train loss:  0.4880515933036804
train gradient:  0.15003360687577683
iteration : 6655
train acc:  0.7265625
train loss:  0.47929835319519043
train gradient:  0.13727742117593983
iteration : 6656
train acc:  0.703125
train loss:  0.5470792651176453
train gradient:  0.21793793396932826
iteration : 6657
train acc:  0.7109375
train loss:  0.5115758180618286
train gradient:  0.16010689823813995
iteration : 6658
train acc:  0.703125
train loss:  0.5661898255348206
train gradient:  0.19285734163521734
iteration : 6659
train acc:  0.78125
train loss:  0.45177727937698364
train gradient:  0.14596539570462097
iteration : 6660
train acc:  0.71875
train loss:  0.4526602625846863
train gradient:  0.12003011138239261
iteration : 6661
train acc:  0.7421875
train loss:  0.5018137693405151
train gradient:  0.19112292777815637
iteration : 6662
train acc:  0.7578125
train loss:  0.49088069796562195
train gradient:  0.13523527881419134
iteration : 6663
train acc:  0.703125
train loss:  0.5444594025611877
train gradient:  0.1464139374062402
iteration : 6664
train acc:  0.6953125
train loss:  0.5162537097930908
train gradient:  0.1255516014955891
iteration : 6665
train acc:  0.765625
train loss:  0.5193305015563965
train gradient:  0.1577610726206191
iteration : 6666
train acc:  0.7578125
train loss:  0.510050356388092
train gradient:  0.17967911132070818
iteration : 6667
train acc:  0.7734375
train loss:  0.5038244724273682
train gradient:  0.15036578565750933
iteration : 6668
train acc:  0.703125
train loss:  0.5349235534667969
train gradient:  0.14507838860711408
iteration : 6669
train acc:  0.7578125
train loss:  0.4825419485569
train gradient:  0.13655226989138056
iteration : 6670
train acc:  0.765625
train loss:  0.4600612223148346
train gradient:  0.11049726163025873
iteration : 6671
train acc:  0.7109375
train loss:  0.48106327652931213
train gradient:  0.13205881908269468
iteration : 6672
train acc:  0.7734375
train loss:  0.5277466773986816
train gradient:  0.1878613464182547
iteration : 6673
train acc:  0.7265625
train loss:  0.5312614440917969
train gradient:  0.15678964863239941
iteration : 6674
train acc:  0.78125
train loss:  0.4370282292366028
train gradient:  0.09790029701052619
iteration : 6675
train acc:  0.765625
train loss:  0.5117172598838806
train gradient:  0.10143307147105313
iteration : 6676
train acc:  0.6953125
train loss:  0.5534166097640991
train gradient:  0.16047088220902225
iteration : 6677
train acc:  0.7734375
train loss:  0.45016592741012573
train gradient:  0.1192233702574169
iteration : 6678
train acc:  0.765625
train loss:  0.4930163025856018
train gradient:  0.12371095681204813
iteration : 6679
train acc:  0.7578125
train loss:  0.4680051803588867
train gradient:  0.13144718481970363
iteration : 6680
train acc:  0.7890625
train loss:  0.4529896676540375
train gradient:  0.11136492435748584
iteration : 6681
train acc:  0.734375
train loss:  0.5015308260917664
train gradient:  0.16144870348250095
iteration : 6682
train acc:  0.78125
train loss:  0.4943951368331909
train gradient:  0.1369999435574104
iteration : 6683
train acc:  0.7734375
train loss:  0.4481550455093384
train gradient:  0.12256751318763642
iteration : 6684
train acc:  0.8125
train loss:  0.42101484537124634
train gradient:  0.12108932197929606
iteration : 6685
train acc:  0.7578125
train loss:  0.5325520038604736
train gradient:  0.1259346985285355
iteration : 6686
train acc:  0.84375
train loss:  0.4039892554283142
train gradient:  0.11560245779158555
iteration : 6687
train acc:  0.7734375
train loss:  0.4481564164161682
train gradient:  0.1073605561262787
iteration : 6688
train acc:  0.7265625
train loss:  0.512883186340332
train gradient:  0.13468520510130666
iteration : 6689
train acc:  0.7421875
train loss:  0.5435481071472168
train gradient:  0.17603888756621766
iteration : 6690
train acc:  0.765625
train loss:  0.48007726669311523
train gradient:  0.11931014297639522
iteration : 6691
train acc:  0.7734375
train loss:  0.4022080898284912
train gradient:  0.117122226033779
iteration : 6692
train acc:  0.75
train loss:  0.5135728120803833
train gradient:  0.17095221292379853
iteration : 6693
train acc:  0.8125
train loss:  0.4194216728210449
train gradient:  0.1084098150481596
iteration : 6694
train acc:  0.703125
train loss:  0.5561169385910034
train gradient:  0.17154585255201069
iteration : 6695
train acc:  0.703125
train loss:  0.5555667877197266
train gradient:  0.15226796633284556
iteration : 6696
train acc:  0.7734375
train loss:  0.4346086382865906
train gradient:  0.11217385036587135
iteration : 6697
train acc:  0.7890625
train loss:  0.4433425962924957
train gradient:  0.12926232825669187
iteration : 6698
train acc:  0.7265625
train loss:  0.5564514398574829
train gradient:  0.17834677422727047
iteration : 6699
train acc:  0.703125
train loss:  0.5260225534439087
train gradient:  0.20741702727184608
iteration : 6700
train acc:  0.75
train loss:  0.5104657411575317
train gradient:  0.19687369282379225
iteration : 6701
train acc:  0.7734375
train loss:  0.4530088007450104
train gradient:  0.10587404509912328
iteration : 6702
train acc:  0.7421875
train loss:  0.4987899363040924
train gradient:  0.13816044031441002
iteration : 6703
train acc:  0.7421875
train loss:  0.4772685170173645
train gradient:  0.14009462490573787
iteration : 6704
train acc:  0.7578125
train loss:  0.505196213722229
train gradient:  0.1599039031166773
iteration : 6705
train acc:  0.6796875
train loss:  0.5869027376174927
train gradient:  0.1755768890249182
iteration : 6706
train acc:  0.78125
train loss:  0.4366860091686249
train gradient:  0.10296332430940036
iteration : 6707
train acc:  0.71875
train loss:  0.5241057872772217
train gradient:  0.16594840675124797
iteration : 6708
train acc:  0.7421875
train loss:  0.48359739780426025
train gradient:  0.1213094976289883
iteration : 6709
train acc:  0.6875
train loss:  0.5534911155700684
train gradient:  0.1485544867417082
iteration : 6710
train acc:  0.703125
train loss:  0.5277793407440186
train gradient:  0.13579406875914535
iteration : 6711
train acc:  0.7109375
train loss:  0.5096277594566345
train gradient:  0.13538863256527184
iteration : 6712
train acc:  0.8046875
train loss:  0.4499794542789459
train gradient:  0.11309956693739294
iteration : 6713
train acc:  0.6796875
train loss:  0.5129843354225159
train gradient:  0.17727348934077736
iteration : 6714
train acc:  0.7109375
train loss:  0.5590173602104187
train gradient:  0.19880004257147824
iteration : 6715
train acc:  0.765625
train loss:  0.446332722902298
train gradient:  0.12701326186662582
iteration : 6716
train acc:  0.7578125
train loss:  0.5504733324050903
train gradient:  0.15204948368548132
iteration : 6717
train acc:  0.6796875
train loss:  0.5311287045478821
train gradient:  0.17780496402392704
iteration : 6718
train acc:  0.71875
train loss:  0.5390243530273438
train gradient:  0.15297809278366242
iteration : 6719
train acc:  0.7421875
train loss:  0.4721093773841858
train gradient:  0.14978401020360438
iteration : 6720
train acc:  0.703125
train loss:  0.4914242923259735
train gradient:  0.11846058809641799
iteration : 6721
train acc:  0.7421875
train loss:  0.49037861824035645
train gradient:  0.1598283196918525
iteration : 6722
train acc:  0.703125
train loss:  0.5085297226905823
train gradient:  0.16221532453179258
iteration : 6723
train acc:  0.8125
train loss:  0.44922882318496704
train gradient:  0.09161750713841078
iteration : 6724
train acc:  0.6875
train loss:  0.5561743974685669
train gradient:  0.1547428364753814
iteration : 6725
train acc:  0.7734375
train loss:  0.48636457324028015
train gradient:  0.13939696929863993
iteration : 6726
train acc:  0.7890625
train loss:  0.47091367840766907
train gradient:  0.14319546627839833
iteration : 6727
train acc:  0.703125
train loss:  0.5250505805015564
train gradient:  0.16459520083858353
iteration : 6728
train acc:  0.671875
train loss:  0.6128793954849243
train gradient:  0.25469381621747045
iteration : 6729
train acc:  0.8125
train loss:  0.4422265589237213
train gradient:  0.13598697362708417
iteration : 6730
train acc:  0.75
train loss:  0.4956272542476654
train gradient:  0.13672576994744104
iteration : 6731
train acc:  0.7109375
train loss:  0.5005736947059631
train gradient:  0.1222632852855255
iteration : 6732
train acc:  0.7578125
train loss:  0.48914745450019836
train gradient:  0.14610761268385566
iteration : 6733
train acc:  0.78125
train loss:  0.4849507808685303
train gradient:  0.13427166748903752
iteration : 6734
train acc:  0.7265625
train loss:  0.505569577217102
train gradient:  0.14780597541363927
iteration : 6735
train acc:  0.78125
train loss:  0.4958379566669464
train gradient:  0.14492014576617798
iteration : 6736
train acc:  0.71875
train loss:  0.48317915201187134
train gradient:  0.1282833641487957
iteration : 6737
train acc:  0.6328125
train loss:  0.5990036725997925
train gradient:  0.21294310494221746
iteration : 6738
train acc:  0.734375
train loss:  0.5070645809173584
train gradient:  0.14285026193342526
iteration : 6739
train acc:  0.7578125
train loss:  0.4710477590560913
train gradient:  0.127605005267444
iteration : 6740
train acc:  0.8046875
train loss:  0.4619130492210388
train gradient:  0.096830995240954
iteration : 6741
train acc:  0.6875
train loss:  0.5617992281913757
train gradient:  0.22056037305556214
iteration : 6742
train acc:  0.78125
train loss:  0.448219895362854
train gradient:  0.14367446511155785
iteration : 6743
train acc:  0.6875
train loss:  0.5646509528160095
train gradient:  0.17209094129918254
iteration : 6744
train acc:  0.78125
train loss:  0.4502568542957306
train gradient:  0.13179418563844664
iteration : 6745
train acc:  0.75
train loss:  0.4829685389995575
train gradient:  0.1711691608753596
iteration : 6746
train acc:  0.6875
train loss:  0.5583734512329102
train gradient:  0.1662783249508557
iteration : 6747
train acc:  0.65625
train loss:  0.5319116711616516
train gradient:  0.13069044485277062
iteration : 6748
train acc:  0.765625
train loss:  0.5209958553314209
train gradient:  0.1756653385172831
iteration : 6749
train acc:  0.734375
train loss:  0.5115358829498291
train gradient:  0.16676126468809582
iteration : 6750
train acc:  0.6875
train loss:  0.5603533983230591
train gradient:  0.17959760795291724
iteration : 6751
train acc:  0.7421875
train loss:  0.47549962997436523
train gradient:  0.11470618955048906
iteration : 6752
train acc:  0.796875
train loss:  0.45444315671920776
train gradient:  0.12947103053993475
iteration : 6753
train acc:  0.640625
train loss:  0.6146292686462402
train gradient:  0.20840717818594273
iteration : 6754
train acc:  0.734375
train loss:  0.49180835485458374
train gradient:  0.12124935408716202
iteration : 6755
train acc:  0.7890625
train loss:  0.425640344619751
train gradient:  0.11083052713617422
iteration : 6756
train acc:  0.7890625
train loss:  0.4457014799118042
train gradient:  0.10640877253426514
iteration : 6757
train acc:  0.71875
train loss:  0.5088416934013367
train gradient:  0.1376497733565646
iteration : 6758
train acc:  0.6875
train loss:  0.5368547439575195
train gradient:  0.12390376923207855
iteration : 6759
train acc:  0.7265625
train loss:  0.5491734743118286
train gradient:  0.16290458332262253
iteration : 6760
train acc:  0.71875
train loss:  0.5356548428535461
train gradient:  0.14982273862264012
iteration : 6761
train acc:  0.7265625
train loss:  0.47432005405426025
train gradient:  0.1359416832297613
iteration : 6762
train acc:  0.765625
train loss:  0.4792032241821289
train gradient:  0.12488139620282077
iteration : 6763
train acc:  0.7421875
train loss:  0.47665083408355713
train gradient:  0.11343819045327005
iteration : 6764
train acc:  0.765625
train loss:  0.4670625925064087
train gradient:  0.12568489348865974
iteration : 6765
train acc:  0.703125
train loss:  0.5523829460144043
train gradient:  0.15542950561737756
iteration : 6766
train acc:  0.7578125
train loss:  0.46731728315353394
train gradient:  0.1406858639481151
iteration : 6767
train acc:  0.78125
train loss:  0.46644508838653564
train gradient:  0.16014381469388117
iteration : 6768
train acc:  0.7890625
train loss:  0.4420049786567688
train gradient:  0.08340039886683395
iteration : 6769
train acc:  0.734375
train loss:  0.4552064538002014
train gradient:  0.11754882869326862
iteration : 6770
train acc:  0.6796875
train loss:  0.5435877442359924
train gradient:  0.14912150863760848
iteration : 6771
train acc:  0.78125
train loss:  0.4555758237838745
train gradient:  0.1145783453245304
iteration : 6772
train acc:  0.7265625
train loss:  0.49723589420318604
train gradient:  0.11476856381676286
iteration : 6773
train acc:  0.75
train loss:  0.5082895755767822
train gradient:  0.14350114858282592
iteration : 6774
train acc:  0.734375
train loss:  0.4941859841346741
train gradient:  0.11200582428328636
iteration : 6775
train acc:  0.765625
train loss:  0.5042724609375
train gradient:  0.14107924138434652
iteration : 6776
train acc:  0.7734375
train loss:  0.48613983392715454
train gradient:  0.16671816699246783
iteration : 6777
train acc:  0.6796875
train loss:  0.5744357109069824
train gradient:  0.21157678348817477
iteration : 6778
train acc:  0.7421875
train loss:  0.5016608238220215
train gradient:  0.1351869461784762
iteration : 6779
train acc:  0.6875
train loss:  0.5406839847564697
train gradient:  0.15441972393880055
iteration : 6780
train acc:  0.78125
train loss:  0.5037976503372192
train gradient:  0.18106089755387333
iteration : 6781
train acc:  0.6796875
train loss:  0.5397674441337585
train gradient:  0.17302305658658845
iteration : 6782
train acc:  0.7265625
train loss:  0.5041755437850952
train gradient:  0.1491642785274031
iteration : 6783
train acc:  0.6875
train loss:  0.5587812662124634
train gradient:  0.1787527781807018
iteration : 6784
train acc:  0.7265625
train loss:  0.5006323456764221
train gradient:  0.14040352043071425
iteration : 6785
train acc:  0.7578125
train loss:  0.4752441942691803
train gradient:  0.15377312179299651
iteration : 6786
train acc:  0.78125
train loss:  0.4713901877403259
train gradient:  0.1328850484357265
iteration : 6787
train acc:  0.75
train loss:  0.5158067941665649
train gradient:  0.11507474040787061
iteration : 6788
train acc:  0.703125
train loss:  0.5188588500022888
train gradient:  0.14914400701191372
iteration : 6789
train acc:  0.734375
train loss:  0.5098780393600464
train gradient:  0.11919472740574052
iteration : 6790
train acc:  0.703125
train loss:  0.567404568195343
train gradient:  0.2026152080718307
iteration : 6791
train acc:  0.765625
train loss:  0.49396130442619324
train gradient:  0.12315578837700228
iteration : 6792
train acc:  0.8203125
train loss:  0.4397609233856201
train gradient:  0.12324314901058071
iteration : 6793
train acc:  0.78125
train loss:  0.45075976848602295
train gradient:  0.15119338672384056
iteration : 6794
train acc:  0.703125
train loss:  0.5007244944572449
train gradient:  0.15001334428855212
iteration : 6795
train acc:  0.7421875
train loss:  0.4700952172279358
train gradient:  0.14729117728231228
iteration : 6796
train acc:  0.75
train loss:  0.48216935992240906
train gradient:  0.13004880895353546
iteration : 6797
train acc:  0.734375
train loss:  0.4683358073234558
train gradient:  0.12280913521448765
iteration : 6798
train acc:  0.7421875
train loss:  0.45928332209587097
train gradient:  0.12250121947329075
iteration : 6799
train acc:  0.8046875
train loss:  0.496085524559021
train gradient:  0.13383707212010343
iteration : 6800
train acc:  0.8046875
train loss:  0.43969663977622986
train gradient:  0.1362617455663805
iteration : 6801
train acc:  0.734375
train loss:  0.5343072414398193
train gradient:  0.1408330574270168
iteration : 6802
train acc:  0.7109375
train loss:  0.49469518661499023
train gradient:  0.12539909463902765
iteration : 6803
train acc:  0.7421875
train loss:  0.4653128981590271
train gradient:  0.17116783110320166
iteration : 6804
train acc:  0.75
train loss:  0.45248228311538696
train gradient:  0.12139408062600157
iteration : 6805
train acc:  0.7109375
train loss:  0.5309110879898071
train gradient:  0.22670552640741204
iteration : 6806
train acc:  0.6953125
train loss:  0.5142122507095337
train gradient:  0.15985521792496865
iteration : 6807
train acc:  0.75
train loss:  0.5456280708312988
train gradient:  0.16050323431690522
iteration : 6808
train acc:  0.7265625
train loss:  0.5416532754898071
train gradient:  0.11956476065279673
iteration : 6809
train acc:  0.734375
train loss:  0.5614089369773865
train gradient:  0.15743025537417393
iteration : 6810
train acc:  0.6875
train loss:  0.5322999358177185
train gradient:  0.15197285477250655
iteration : 6811
train acc:  0.765625
train loss:  0.485398530960083
train gradient:  0.12490337627703962
iteration : 6812
train acc:  0.78125
train loss:  0.4273311197757721
train gradient:  0.11506257199156177
iteration : 6813
train acc:  0.6953125
train loss:  0.5288842916488647
train gradient:  0.12459542810842816
iteration : 6814
train acc:  0.703125
train loss:  0.5578219294548035
train gradient:  0.17658532147620126
iteration : 6815
train acc:  0.7265625
train loss:  0.47405898571014404
train gradient:  0.1391163441049721
iteration : 6816
train acc:  0.7578125
train loss:  0.4811565577983856
train gradient:  0.13779958189349353
iteration : 6817
train acc:  0.734375
train loss:  0.4452482759952545
train gradient:  0.11562037574435566
iteration : 6818
train acc:  0.734375
train loss:  0.5046236515045166
train gradient:  0.12454098340749556
iteration : 6819
train acc:  0.75
train loss:  0.49745869636535645
train gradient:  0.14917066950512636
iteration : 6820
train acc:  0.75
train loss:  0.48954617977142334
train gradient:  0.12365715697797912
iteration : 6821
train acc:  0.75
train loss:  0.4995955228805542
train gradient:  0.13808984437957786
iteration : 6822
train acc:  0.7265625
train loss:  0.5024421215057373
train gradient:  0.18731476199110192
iteration : 6823
train acc:  0.765625
train loss:  0.451716810464859
train gradient:  0.13491731745265845
iteration : 6824
train acc:  0.6796875
train loss:  0.5493096113204956
train gradient:  0.2267624908284761
iteration : 6825
train acc:  0.6484375
train loss:  0.6015554666519165
train gradient:  0.21705007060911177
iteration : 6826
train acc:  0.8046875
train loss:  0.4634034335613251
train gradient:  0.10979667592363977
iteration : 6827
train acc:  0.703125
train loss:  0.5284698009490967
train gradient:  0.15068039104077718
iteration : 6828
train acc:  0.7734375
train loss:  0.4744102358818054
train gradient:  0.12075835276863739
iteration : 6829
train acc:  0.734375
train loss:  0.46359267830848694
train gradient:  0.12173159247800518
iteration : 6830
train acc:  0.78125
train loss:  0.45857933163642883
train gradient:  0.1388351387641284
iteration : 6831
train acc:  0.796875
train loss:  0.4577881097793579
train gradient:  0.12204480513108848
iteration : 6832
train acc:  0.7578125
train loss:  0.4815715551376343
train gradient:  0.14748779709676269
iteration : 6833
train acc:  0.6953125
train loss:  0.5146597623825073
train gradient:  0.1557163317819643
iteration : 6834
train acc:  0.6875
train loss:  0.5605639815330505
train gradient:  0.13870876051925438
iteration : 6835
train acc:  0.7578125
train loss:  0.47385740280151367
train gradient:  0.09842099228816828
iteration : 6836
train acc:  0.6484375
train loss:  0.5957370400428772
train gradient:  0.1930018947231894
iteration : 6837
train acc:  0.75
train loss:  0.5088217258453369
train gradient:  0.17502750691665026
iteration : 6838
train acc:  0.6953125
train loss:  0.5711889266967773
train gradient:  0.2231338233812699
iteration : 6839
train acc:  0.7734375
train loss:  0.49534013867378235
train gradient:  0.14648239666620655
iteration : 6840
train acc:  0.75
train loss:  0.5425084829330444
train gradient:  0.1638527352575508
iteration : 6841
train acc:  0.7421875
train loss:  0.48858070373535156
train gradient:  0.12250662138042545
iteration : 6842
train acc:  0.796875
train loss:  0.4322528541088104
train gradient:  0.1237756493074744
iteration : 6843
train acc:  0.7421875
train loss:  0.5516923069953918
train gradient:  0.17189184665309218
iteration : 6844
train acc:  0.703125
train loss:  0.5311136245727539
train gradient:  0.14167714053615169
iteration : 6845
train acc:  0.7890625
train loss:  0.41917720437049866
train gradient:  0.13240441688955665
iteration : 6846
train acc:  0.7421875
train loss:  0.5007772445678711
train gradient:  0.15750780271354675
iteration : 6847
train acc:  0.734375
train loss:  0.5177879333496094
train gradient:  0.12122872237659535
iteration : 6848
train acc:  0.765625
train loss:  0.45898735523223877
train gradient:  0.14786049603678744
iteration : 6849
train acc:  0.75
train loss:  0.47731661796569824
train gradient:  0.11170735482513565
iteration : 6850
train acc:  0.71875
train loss:  0.5398940443992615
train gradient:  0.15917906543512295
iteration : 6851
train acc:  0.734375
train loss:  0.5488312244415283
train gradient:  0.17032170500478816
iteration : 6852
train acc:  0.78125
train loss:  0.47894734144210815
train gradient:  0.13232700857590307
iteration : 6853
train acc:  0.765625
train loss:  0.4307482838630676
train gradient:  0.10672183053010569
iteration : 6854
train acc:  0.7890625
train loss:  0.4532475471496582
train gradient:  0.11046401889322674
iteration : 6855
train acc:  0.8203125
train loss:  0.47555750608444214
train gradient:  0.17504011585951465
iteration : 6856
train acc:  0.7421875
train loss:  0.5065844058990479
train gradient:  0.13992726162927172
iteration : 6857
train acc:  0.7734375
train loss:  0.48797836899757385
train gradient:  0.10627111794984224
iteration : 6858
train acc:  0.7421875
train loss:  0.4717428386211395
train gradient:  0.11633203113126238
iteration : 6859
train acc:  0.7109375
train loss:  0.44879940152168274
train gradient:  0.1354351445778177
iteration : 6860
train acc:  0.7578125
train loss:  0.48095566034317017
train gradient:  0.11659265271314287
iteration : 6861
train acc:  0.7734375
train loss:  0.4432482123374939
train gradient:  0.11745731905732448
iteration : 6862
train acc:  0.7421875
train loss:  0.4555423855781555
train gradient:  0.10720521360228867
iteration : 6863
train acc:  0.7734375
train loss:  0.4479576349258423
train gradient:  0.11794897660173635
iteration : 6864
train acc:  0.734375
train loss:  0.5154213905334473
train gradient:  0.12725010874388706
iteration : 6865
train acc:  0.7109375
train loss:  0.4737693965435028
train gradient:  0.10323335110452424
iteration : 6866
train acc:  0.71875
train loss:  0.5411678552627563
train gradient:  0.13449366788717637
iteration : 6867
train acc:  0.7578125
train loss:  0.44887691736221313
train gradient:  0.1225277822889558
iteration : 6868
train acc:  0.78125
train loss:  0.4405211806297302
train gradient:  0.10793786928167105
iteration : 6869
train acc:  0.6875
train loss:  0.5637367963790894
train gradient:  0.16944442577228813
iteration : 6870
train acc:  0.7578125
train loss:  0.49191299080848694
train gradient:  0.1466191049596695
iteration : 6871
train acc:  0.8125
train loss:  0.42208561301231384
train gradient:  0.11044847587781348
iteration : 6872
train acc:  0.7890625
train loss:  0.4604012370109558
train gradient:  0.13061859701632855
iteration : 6873
train acc:  0.765625
train loss:  0.4715902805328369
train gradient:  0.1476491903244312
iteration : 6874
train acc:  0.6640625
train loss:  0.5546613931655884
train gradient:  0.1407330481974117
iteration : 6875
train acc:  0.703125
train loss:  0.5490922927856445
train gradient:  0.1680179508623416
iteration : 6876
train acc:  0.734375
train loss:  0.4811883270740509
train gradient:  0.12627956662956433
iteration : 6877
train acc:  0.6796875
train loss:  0.5961315631866455
train gradient:  0.18316997783079808
iteration : 6878
train acc:  0.7265625
train loss:  0.550438404083252
train gradient:  0.18289254662194365
iteration : 6879
train acc:  0.75
train loss:  0.48925942182540894
train gradient:  0.11318529251284577
iteration : 6880
train acc:  0.7578125
train loss:  0.4676859974861145
train gradient:  0.13093185327325613
iteration : 6881
train acc:  0.7421875
train loss:  0.5104416608810425
train gradient:  0.1698593191883736
iteration : 6882
train acc:  0.75
train loss:  0.4636927545070648
train gradient:  0.14435438671613005
iteration : 6883
train acc:  0.71875
train loss:  0.4436004161834717
train gradient:  0.1215664880199322
iteration : 6884
train acc:  0.75
train loss:  0.48901528120040894
train gradient:  0.17377451985517972
iteration : 6885
train acc:  0.7109375
train loss:  0.5391494035720825
train gradient:  0.1520377933207157
iteration : 6886
train acc:  0.75
train loss:  0.4907299280166626
train gradient:  0.13135080147863182
iteration : 6887
train acc:  0.734375
train loss:  0.5209476947784424
train gradient:  0.13762772327706843
iteration : 6888
train acc:  0.7421875
train loss:  0.49636703729629517
train gradient:  0.13121528459217155
iteration : 6889
train acc:  0.6953125
train loss:  0.5724081993103027
train gradient:  0.17320397313241
iteration : 6890
train acc:  0.75
train loss:  0.4736372232437134
train gradient:  0.11815041988527573
iteration : 6891
train acc:  0.7109375
train loss:  0.5358374714851379
train gradient:  0.17469158822065547
iteration : 6892
train acc:  0.6484375
train loss:  0.5477747917175293
train gradient:  0.17534800523344293
iteration : 6893
train acc:  0.75
train loss:  0.48277607560157776
train gradient:  0.13786252038212155
iteration : 6894
train acc:  0.75
train loss:  0.4799211621284485
train gradient:  0.10967153559043492
iteration : 6895
train acc:  0.7421875
train loss:  0.4606701731681824
train gradient:  0.12019860488572501
iteration : 6896
train acc:  0.6640625
train loss:  0.5582276582717896
train gradient:  0.19314673415529648
iteration : 6897
train acc:  0.734375
train loss:  0.5199271440505981
train gradient:  0.14958324303427323
iteration : 6898
train acc:  0.7109375
train loss:  0.5264291167259216
train gradient:  0.14660740864194952
iteration : 6899
train acc:  0.71875
train loss:  0.5454875826835632
train gradient:  0.1344297792580466
iteration : 6900
train acc:  0.7265625
train loss:  0.482761412858963
train gradient:  0.14642678651454727
iteration : 6901
train acc:  0.7109375
train loss:  0.5672001838684082
train gradient:  0.18993572531765668
iteration : 6902
train acc:  0.6875
train loss:  0.579779863357544
train gradient:  0.2169336475646052
iteration : 6903
train acc:  0.703125
train loss:  0.5088908672332764
train gradient:  0.1631665784713141
iteration : 6904
train acc:  0.75
train loss:  0.4708372950553894
train gradient:  0.09928769124265269
iteration : 6905
train acc:  0.7734375
train loss:  0.44713008403778076
train gradient:  0.11583296152162666
iteration : 6906
train acc:  0.6953125
train loss:  0.5140103101730347
train gradient:  0.15940315475756978
iteration : 6907
train acc:  0.7421875
train loss:  0.5137412548065186
train gradient:  0.1571224151113837
iteration : 6908
train acc:  0.7734375
train loss:  0.48258766531944275
train gradient:  0.15753046068438903
iteration : 6909
train acc:  0.7734375
train loss:  0.46449071168899536
train gradient:  0.10019511218879015
iteration : 6910
train acc:  0.6875
train loss:  0.5477402210235596
train gradient:  0.14232616299744622
iteration : 6911
train acc:  0.6328125
train loss:  0.6200461387634277
train gradient:  0.20252194675181046
iteration : 6912
train acc:  0.734375
train loss:  0.48477211594581604
train gradient:  0.17438181431298055
iteration : 6913
train acc:  0.7265625
train loss:  0.5133503675460815
train gradient:  0.14343846668335836
iteration : 6914
train acc:  0.6953125
train loss:  0.5160316228866577
train gradient:  0.15067633818136353
iteration : 6915
train acc:  0.6953125
train loss:  0.5041014552116394
train gradient:  0.13588472041586708
iteration : 6916
train acc:  0.8125
train loss:  0.48353931307792664
train gradient:  0.11644921323499866
iteration : 6917
train acc:  0.7265625
train loss:  0.48629504442214966
train gradient:  0.11339185596755218
iteration : 6918
train acc:  0.75
train loss:  0.4948384761810303
train gradient:  0.1141235411205462
iteration : 6919
train acc:  0.7109375
train loss:  0.5440627932548523
train gradient:  0.14296397994124296
iteration : 6920
train acc:  0.7734375
train loss:  0.5020803809165955
train gradient:  0.13003233094242972
iteration : 6921
train acc:  0.703125
train loss:  0.5007480978965759
train gradient:  0.11567469824210086
iteration : 6922
train acc:  0.7734375
train loss:  0.4560031294822693
train gradient:  0.13046576341655208
iteration : 6923
train acc:  0.78125
train loss:  0.4794178605079651
train gradient:  0.11794883114978293
iteration : 6924
train acc:  0.6875
train loss:  0.5238240957260132
train gradient:  0.17248781702346977
iteration : 6925
train acc:  0.71875
train loss:  0.520030677318573
train gradient:  0.13132980772046654
iteration : 6926
train acc:  0.8125
train loss:  0.46159303188323975
train gradient:  0.1192208824873474
iteration : 6927
train acc:  0.703125
train loss:  0.5271158814430237
train gradient:  0.17673132260874114
iteration : 6928
train acc:  0.6640625
train loss:  0.537359356880188
train gradient:  0.170996970985702
iteration : 6929
train acc:  0.671875
train loss:  0.5205473899841309
train gradient:  0.1435477715865178
iteration : 6930
train acc:  0.78125
train loss:  0.45860522985458374
train gradient:  0.1346237576931315
iteration : 6931
train acc:  0.78125
train loss:  0.46894845366477966
train gradient:  0.11876143232650037
iteration : 6932
train acc:  0.7109375
train loss:  0.5668091773986816
train gradient:  0.15475022214647902
iteration : 6933
train acc:  0.71875
train loss:  0.5666406154632568
train gradient:  0.21613211603233762
iteration : 6934
train acc:  0.7421875
train loss:  0.4695497751235962
train gradient:  0.11444327541166233
iteration : 6935
train acc:  0.71875
train loss:  0.5154287219047546
train gradient:  0.1355064081925354
iteration : 6936
train acc:  0.75
train loss:  0.46009641885757446
train gradient:  0.10363996811552637
iteration : 6937
train acc:  0.75
train loss:  0.49569159746170044
train gradient:  0.12706342511561958
iteration : 6938
train acc:  0.6875
train loss:  0.5790301561355591
train gradient:  0.18746937619927262
iteration : 6939
train acc:  0.6953125
train loss:  0.5153224468231201
train gradient:  0.1726204460837289
iteration : 6940
train acc:  0.6875
train loss:  0.5602936148643494
train gradient:  0.20547492223598363
iteration : 6941
train acc:  0.703125
train loss:  0.5255374908447266
train gradient:  0.12981668640299593
iteration : 6942
train acc:  0.734375
train loss:  0.472898006439209
train gradient:  0.12870886166415021
iteration : 6943
train acc:  0.7578125
train loss:  0.45267435908317566
train gradient:  0.09952508392951621
iteration : 6944
train acc:  0.7421875
train loss:  0.4910586178302765
train gradient:  0.1387447568815251
iteration : 6945
train acc:  0.8125
train loss:  0.41468143463134766
train gradient:  0.0866817439768427
iteration : 6946
train acc:  0.734375
train loss:  0.5445258021354675
train gradient:  0.1883748129651077
iteration : 6947
train acc:  0.7109375
train loss:  0.511498212814331
train gradient:  0.12469535347167826
iteration : 6948
train acc:  0.71875
train loss:  0.5733023285865784
train gradient:  0.18989013984659442
iteration : 6949
train acc:  0.6875
train loss:  0.531885027885437
train gradient:  0.22890656004482363
iteration : 6950
train acc:  0.7734375
train loss:  0.5144473314285278
train gradient:  0.15508875199118602
iteration : 6951
train acc:  0.6953125
train loss:  0.5194313526153564
train gradient:  0.16014789039344152
iteration : 6952
train acc:  0.7109375
train loss:  0.4978947639465332
train gradient:  0.14580102553224597
iteration : 6953
train acc:  0.703125
train loss:  0.5513771176338196
train gradient:  0.16237145162484518
iteration : 6954
train acc:  0.7265625
train loss:  0.4658864736557007
train gradient:  0.10972032767069427
iteration : 6955
train acc:  0.6796875
train loss:  0.5771334767341614
train gradient:  0.16722472682923817
iteration : 6956
train acc:  0.8046875
train loss:  0.4622587263584137
train gradient:  0.1232421629492665
iteration : 6957
train acc:  0.796875
train loss:  0.4850965440273285
train gradient:  0.1364933913970347
iteration : 6958
train acc:  0.7890625
train loss:  0.4646126329898834
train gradient:  0.11368234805693796
iteration : 6959
train acc:  0.75
train loss:  0.5216891169548035
train gradient:  0.14181801383168247
iteration : 6960
train acc:  0.7265625
train loss:  0.48947012424468994
train gradient:  0.13984239522861508
iteration : 6961
train acc:  0.7109375
train loss:  0.5371788740158081
train gradient:  0.12563474894889426
iteration : 6962
train acc:  0.7578125
train loss:  0.47400641441345215
train gradient:  0.12158211312139622
iteration : 6963
train acc:  0.75
train loss:  0.47744303941726685
train gradient:  0.1205150395855229
iteration : 6964
train acc:  0.78125
train loss:  0.45736438035964966
train gradient:  0.10145593579731735
iteration : 6965
train acc:  0.6640625
train loss:  0.5812070369720459
train gradient:  0.18411783936986353
iteration : 6966
train acc:  0.7265625
train loss:  0.5162364840507507
train gradient:  0.14935361778567938
iteration : 6967
train acc:  0.7265625
train loss:  0.469717413187027
train gradient:  0.12435046715273629
iteration : 6968
train acc:  0.78125
train loss:  0.4362154006958008
train gradient:  0.09799660992414252
iteration : 6969
train acc:  0.6953125
train loss:  0.5237692594528198
train gradient:  0.16602052914986065
iteration : 6970
train acc:  0.8046875
train loss:  0.4394461214542389
train gradient:  0.11815178158136963
iteration : 6971
train acc:  0.75
train loss:  0.50520259141922
train gradient:  0.14058968514771375
iteration : 6972
train acc:  0.703125
train loss:  0.5596357583999634
train gradient:  0.15879787744250173
iteration : 6973
train acc:  0.71875
train loss:  0.5099953413009644
train gradient:  0.16358751009422084
iteration : 6974
train acc:  0.7265625
train loss:  0.47161561250686646
train gradient:  0.14749680709932445
iteration : 6975
train acc:  0.765625
train loss:  0.47727859020233154
train gradient:  0.12820651421732746
iteration : 6976
train acc:  0.71875
train loss:  0.5376084446907043
train gradient:  0.2108952000264287
iteration : 6977
train acc:  0.7578125
train loss:  0.4620020389556885
train gradient:  0.16069602057819188
iteration : 6978
train acc:  0.6953125
train loss:  0.49563294649124146
train gradient:  0.15072299138925643
iteration : 6979
train acc:  0.671875
train loss:  0.5289149880409241
train gradient:  0.15496019651367454
iteration : 6980
train acc:  0.71875
train loss:  0.5732576847076416
train gradient:  0.2160355742675063
iteration : 6981
train acc:  0.6953125
train loss:  0.5724787712097168
train gradient:  0.16524656321630748
iteration : 6982
train acc:  0.765625
train loss:  0.5031683444976807
train gradient:  0.13816820832408122
iteration : 6983
train acc:  0.7265625
train loss:  0.53581702709198
train gradient:  0.14783066973609466
iteration : 6984
train acc:  0.703125
train loss:  0.5569978952407837
train gradient:  0.14983250427360448
iteration : 6985
train acc:  0.7265625
train loss:  0.45352402329444885
train gradient:  0.1166583736545063
iteration : 6986
train acc:  0.765625
train loss:  0.4270663857460022
train gradient:  0.14100823846131055
iteration : 6987
train acc:  0.734375
train loss:  0.4951850175857544
train gradient:  0.16771874709002627
iteration : 6988
train acc:  0.8046875
train loss:  0.4488297700881958
train gradient:  0.12049384216146286
iteration : 6989
train acc:  0.7578125
train loss:  0.5095958113670349
train gradient:  0.1993146151482545
iteration : 6990
train acc:  0.7890625
train loss:  0.4918774366378784
train gradient:  0.1321680017951472
iteration : 6991
train acc:  0.734375
train loss:  0.5051424503326416
train gradient:  0.11301433801548924
iteration : 6992
train acc:  0.703125
train loss:  0.5866984128952026
train gradient:  0.21193864625308045
iteration : 6993
train acc:  0.765625
train loss:  0.4937206506729126
train gradient:  0.1215015655871294
iteration : 6994
train acc:  0.7421875
train loss:  0.5136151313781738
train gradient:  0.16311885759359868
iteration : 6995
train acc:  0.765625
train loss:  0.4682408571243286
train gradient:  0.10969502062819149
iteration : 6996
train acc:  0.75
train loss:  0.5172661542892456
train gradient:  0.15476753379051586
iteration : 6997
train acc:  0.7421875
train loss:  0.5363689661026001
train gradient:  0.1740150213819535
iteration : 6998
train acc:  0.75
train loss:  0.4615791141986847
train gradient:  0.10804393427102409
iteration : 6999
train acc:  0.828125
train loss:  0.43531662225723267
train gradient:  0.11118616753314677
iteration : 7000
train acc:  0.6640625
train loss:  0.5530946254730225
train gradient:  0.17904830674868982
iteration : 7001
train acc:  0.765625
train loss:  0.4901810884475708
train gradient:  0.11251732954738772
iteration : 7002
train acc:  0.78125
train loss:  0.47090446949005127
train gradient:  0.1261295904316314
iteration : 7003
train acc:  0.734375
train loss:  0.5062237977981567
train gradient:  0.16860839283881512
iteration : 7004
train acc:  0.6796875
train loss:  0.49606144428253174
train gradient:  0.13554902776620564
iteration : 7005
train acc:  0.7734375
train loss:  0.451886385679245
train gradient:  0.12017394099994946
iteration : 7006
train acc:  0.765625
train loss:  0.4611203670501709
train gradient:  0.1251350668824669
iteration : 7007
train acc:  0.8203125
train loss:  0.45027273893356323
train gradient:  0.15793902401187565
iteration : 7008
train acc:  0.703125
train loss:  0.5333832502365112
train gradient:  0.16632531036633696
iteration : 7009
train acc:  0.7265625
train loss:  0.5295016765594482
train gradient:  0.15257177072614442
iteration : 7010
train acc:  0.6875
train loss:  0.5581151247024536
train gradient:  0.1797070764908405
iteration : 7011
train acc:  0.71875
train loss:  0.5801379680633545
train gradient:  0.14722440030221634
iteration : 7012
train acc:  0.7109375
train loss:  0.5524635314941406
train gradient:  0.16315744329001874
iteration : 7013
train acc:  0.8125
train loss:  0.42382103204727173
train gradient:  0.10739466112961027
iteration : 7014
train acc:  0.671875
train loss:  0.4979974329471588
train gradient:  0.1307426250099934
iteration : 7015
train acc:  0.734375
train loss:  0.5181045532226562
train gradient:  0.13090123019739525
iteration : 7016
train acc:  0.8046875
train loss:  0.4441272020339966
train gradient:  0.17243918246405623
iteration : 7017
train acc:  0.7109375
train loss:  0.5440801382064819
train gradient:  0.15414780597977681
iteration : 7018
train acc:  0.7109375
train loss:  0.5127105712890625
train gradient:  0.14866518991192076
iteration : 7019
train acc:  0.7109375
train loss:  0.545569121837616
train gradient:  0.16060768441356932
iteration : 7020
train acc:  0.6875
train loss:  0.5385594367980957
train gradient:  0.15497886781782283
iteration : 7021
train acc:  0.71875
train loss:  0.5663551092147827
train gradient:  0.1572898729353487
iteration : 7022
train acc:  0.7109375
train loss:  0.55556321144104
train gradient:  0.1600341661852699
iteration : 7023
train acc:  0.7734375
train loss:  0.4856819212436676
train gradient:  0.15067663531256165
iteration : 7024
train acc:  0.703125
train loss:  0.4994399845600128
train gradient:  0.1538970499395816
iteration : 7025
train acc:  0.734375
train loss:  0.5628933906555176
train gradient:  0.20428376641215798
iteration : 7026
train acc:  0.78125
train loss:  0.4300335645675659
train gradient:  0.12105127159459576
iteration : 7027
train acc:  0.7734375
train loss:  0.5101273655891418
train gradient:  0.1776058323591585
iteration : 7028
train acc:  0.8125
train loss:  0.42372265458106995
train gradient:  0.08834487113387134
iteration : 7029
train acc:  0.71875
train loss:  0.48339319229125977
train gradient:  0.1413978895441108
iteration : 7030
train acc:  0.7578125
train loss:  0.4637197256088257
train gradient:  0.1316149308850068
iteration : 7031
train acc:  0.828125
train loss:  0.41396668553352356
train gradient:  0.10427734425343736
iteration : 7032
train acc:  0.734375
train loss:  0.4842427372932434
train gradient:  0.1337696450710924
iteration : 7033
train acc:  0.703125
train loss:  0.5439541935920715
train gradient:  0.21669819788424138
iteration : 7034
train acc:  0.75
train loss:  0.5240521430969238
train gradient:  0.14802831604643643
iteration : 7035
train acc:  0.7734375
train loss:  0.4699307680130005
train gradient:  0.13830833543827437
iteration : 7036
train acc:  0.71875
train loss:  0.5121248364448547
train gradient:  0.13210918764568924
iteration : 7037
train acc:  0.734375
train loss:  0.470242977142334
train gradient:  0.0999352195721125
iteration : 7038
train acc:  0.7109375
train loss:  0.49850162863731384
train gradient:  0.19340651330941172
iteration : 7039
train acc:  0.78125
train loss:  0.4424366354942322
train gradient:  0.1116486462011633
iteration : 7040
train acc:  0.6875
train loss:  0.5228188633918762
train gradient:  0.1351185833249008
iteration : 7041
train acc:  0.8125
train loss:  0.4662155210971832
train gradient:  0.12873592015494909
iteration : 7042
train acc:  0.78125
train loss:  0.45758938789367676
train gradient:  0.11437080884798519
iteration : 7043
train acc:  0.8046875
train loss:  0.4372914433479309
train gradient:  0.12881433806949338
iteration : 7044
train acc:  0.734375
train loss:  0.5484639406204224
train gradient:  0.13930976787720983
iteration : 7045
train acc:  0.7421875
train loss:  0.49383848905563354
train gradient:  0.1463513427552453
iteration : 7046
train acc:  0.7890625
train loss:  0.4447096586227417
train gradient:  0.10257506135902328
iteration : 7047
train acc:  0.7578125
train loss:  0.491504043340683
train gradient:  0.1344563811420929
iteration : 7048
train acc:  0.7734375
train loss:  0.45252376794815063
train gradient:  0.1138864219488294
iteration : 7049
train acc:  0.7421875
train loss:  0.5171988010406494
train gradient:  0.1791219189463536
iteration : 7050
train acc:  0.703125
train loss:  0.5321246385574341
train gradient:  0.16967416499789928
iteration : 7051
train acc:  0.765625
train loss:  0.4671291708946228
train gradient:  0.118772084753089
iteration : 7052
train acc:  0.6953125
train loss:  0.5917938947677612
train gradient:  0.1726517763291034
iteration : 7053
train acc:  0.71875
train loss:  0.4973498582839966
train gradient:  0.1360163153014037
iteration : 7054
train acc:  0.796875
train loss:  0.43830662965774536
train gradient:  0.15447252950870743
iteration : 7055
train acc:  0.7578125
train loss:  0.5115159153938293
train gradient:  0.15501679492135115
iteration : 7056
train acc:  0.7109375
train loss:  0.5215773582458496
train gradient:  0.18285506357673065
iteration : 7057
train acc:  0.7734375
train loss:  0.4408479332923889
train gradient:  0.12081329163345636
iteration : 7058
train acc:  0.71875
train loss:  0.581895112991333
train gradient:  0.21543072636013888
iteration : 7059
train acc:  0.7265625
train loss:  0.5452068448066711
train gradient:  0.17363927839277749
iteration : 7060
train acc:  0.6953125
train loss:  0.5313234329223633
train gradient:  0.17690977446637082
iteration : 7061
train acc:  0.7109375
train loss:  0.5488658547401428
train gradient:  0.1576531727108172
iteration : 7062
train acc:  0.703125
train loss:  0.5260266661643982
train gradient:  0.148737035570998
iteration : 7063
train acc:  0.7109375
train loss:  0.5051398873329163
train gradient:  0.15940742794355556
iteration : 7064
train acc:  0.6875
train loss:  0.5393909215927124
train gradient:  0.20941280000540774
iteration : 7065
train acc:  0.828125
train loss:  0.43435412645339966
train gradient:  0.1068345996821213
iteration : 7066
train acc:  0.7421875
train loss:  0.47828131914138794
train gradient:  0.1532307400292522
iteration : 7067
train acc:  0.7890625
train loss:  0.45795726776123047
train gradient:  0.12129425111362777
iteration : 7068
train acc:  0.7578125
train loss:  0.5024961829185486
train gradient:  0.14039634580668925
iteration : 7069
train acc:  0.703125
train loss:  0.5246571898460388
train gradient:  0.1410751631447385
iteration : 7070
train acc:  0.6796875
train loss:  0.554818868637085
train gradient:  0.16734231215244982
iteration : 7071
train acc:  0.71875
train loss:  0.5629515647888184
train gradient:  0.166405567048262
iteration : 7072
train acc:  0.7265625
train loss:  0.45961201190948486
train gradient:  0.10605009300579017
iteration : 7073
train acc:  0.765625
train loss:  0.5365322828292847
train gradient:  0.1537555721549036
iteration : 7074
train acc:  0.640625
train loss:  0.560772180557251
train gradient:  0.20813860029258158
iteration : 7075
train acc:  0.6953125
train loss:  0.4987110495567322
train gradient:  0.12900867081441472
iteration : 7076
train acc:  0.734375
train loss:  0.5002573728561401
train gradient:  0.1593890999592435
iteration : 7077
train acc:  0.78125
train loss:  0.46198320388793945
train gradient:  0.12227897895363034
iteration : 7078
train acc:  0.78125
train loss:  0.4667539596557617
train gradient:  0.11415817802177977
iteration : 7079
train acc:  0.6875
train loss:  0.5779844522476196
train gradient:  0.1831036393816542
iteration : 7080
train acc:  0.734375
train loss:  0.48638013005256653
train gradient:  0.14719199110974052
iteration : 7081
train acc:  0.765625
train loss:  0.459623783826828
train gradient:  0.10391324593282875
iteration : 7082
train acc:  0.75
train loss:  0.46512874960899353
train gradient:  0.11084656781611782
iteration : 7083
train acc:  0.75
train loss:  0.49554532766342163
train gradient:  0.13540610320769858
iteration : 7084
train acc:  0.6640625
train loss:  0.5823402404785156
train gradient:  0.17345252211951306
iteration : 7085
train acc:  0.703125
train loss:  0.5198889970779419
train gradient:  0.13808014332830398
iteration : 7086
train acc:  0.7265625
train loss:  0.5237894654273987
train gradient:  0.12813340246159918
iteration : 7087
train acc:  0.7109375
train loss:  0.49698469042778015
train gradient:  0.17501751465716964
iteration : 7088
train acc:  0.703125
train loss:  0.5434157848358154
train gradient:  0.23377506156152333
iteration : 7089
train acc:  0.6875
train loss:  0.5299814343452454
train gradient:  0.14757352566655108
iteration : 7090
train acc:  0.796875
train loss:  0.4783201515674591
train gradient:  0.1621644839127504
iteration : 7091
train acc:  0.765625
train loss:  0.5131279230117798
train gradient:  0.12787272002972266
iteration : 7092
train acc:  0.734375
train loss:  0.49314558506011963
train gradient:  0.11784906315298216
iteration : 7093
train acc:  0.75
train loss:  0.5090764760971069
train gradient:  0.13925123675271228
iteration : 7094
train acc:  0.78125
train loss:  0.4654795527458191
train gradient:  0.11936062779715505
iteration : 7095
train acc:  0.7265625
train loss:  0.5430902242660522
train gradient:  0.15737840985915438
iteration : 7096
train acc:  0.7265625
train loss:  0.4876038730144501
train gradient:  0.12503284274161813
iteration : 7097
train acc:  0.6953125
train loss:  0.5384018421173096
train gradient:  0.1346549625542357
iteration : 7098
train acc:  0.703125
train loss:  0.5270410776138306
train gradient:  0.16552530903148177
iteration : 7099
train acc:  0.765625
train loss:  0.5214322209358215
train gradient:  0.1367530677138345
iteration : 7100
train acc:  0.6796875
train loss:  0.6087726354598999
train gradient:  0.18556947079288988
iteration : 7101
train acc:  0.6953125
train loss:  0.573363721370697
train gradient:  0.14626645646264982
iteration : 7102
train acc:  0.75
train loss:  0.4998854100704193
train gradient:  0.1235328054080138
iteration : 7103
train acc:  0.734375
train loss:  0.5122311115264893
train gradient:  0.14494211341253688
iteration : 7104
train acc:  0.6953125
train loss:  0.5596698522567749
train gradient:  0.15074180228585266
iteration : 7105
train acc:  0.734375
train loss:  0.5506073236465454
train gradient:  0.16201933229661028
iteration : 7106
train acc:  0.703125
train loss:  0.5103026032447815
train gradient:  0.13122681762790794
iteration : 7107
train acc:  0.734375
train loss:  0.48162841796875
train gradient:  0.12709465832648387
iteration : 7108
train acc:  0.65625
train loss:  0.5409770011901855
train gradient:  0.17036606249473657
iteration : 7109
train acc:  0.6796875
train loss:  0.5360901355743408
train gradient:  0.1508939578260917
iteration : 7110
train acc:  0.703125
train loss:  0.5600773096084595
train gradient:  0.1351332532629339
iteration : 7111
train acc:  0.703125
train loss:  0.5594205856323242
train gradient:  0.22352171495480805
iteration : 7112
train acc:  0.8046875
train loss:  0.4545155167579651
train gradient:  0.1097552662141703
iteration : 7113
train acc:  0.7890625
train loss:  0.4368155598640442
train gradient:  0.09837792617172025
iteration : 7114
train acc:  0.75
train loss:  0.4782429039478302
train gradient:  0.1055526510446486
iteration : 7115
train acc:  0.6953125
train loss:  0.4707704782485962
train gradient:  0.11581387176434887
iteration : 7116
train acc:  0.7421875
train loss:  0.5303641557693481
train gradient:  0.16914029599766994
iteration : 7117
train acc:  0.734375
train loss:  0.4719569981098175
train gradient:  0.1044961711043693
iteration : 7118
train acc:  0.7265625
train loss:  0.4968438744544983
train gradient:  0.1024009794540236
iteration : 7119
train acc:  0.6875
train loss:  0.5771691799163818
train gradient:  0.1824738933856323
iteration : 7120
train acc:  0.8046875
train loss:  0.45159536600112915
train gradient:  0.12074730635017314
iteration : 7121
train acc:  0.75
train loss:  0.47189927101135254
train gradient:  0.10874054300046643
iteration : 7122
train acc:  0.6328125
train loss:  0.6210718154907227
train gradient:  0.2148890286033598
iteration : 7123
train acc:  0.7578125
train loss:  0.4758874475955963
train gradient:  0.13196781940354654
iteration : 7124
train acc:  0.765625
train loss:  0.5007214546203613
train gradient:  0.12952904881612387
iteration : 7125
train acc:  0.84375
train loss:  0.44346505403518677
train gradient:  0.1655569420517817
iteration : 7126
train acc:  0.671875
train loss:  0.5621758103370667
train gradient:  0.18653851057912366
iteration : 7127
train acc:  0.7265625
train loss:  0.5724788904190063
train gradient:  0.15697867539552557
iteration : 7128
train acc:  0.7109375
train loss:  0.5569170713424683
train gradient:  0.17231336521959656
iteration : 7129
train acc:  0.8046875
train loss:  0.4495883584022522
train gradient:  0.14046670157341565
iteration : 7130
train acc:  0.765625
train loss:  0.4849224388599396
train gradient:  0.11369954540059718
iteration : 7131
train acc:  0.6953125
train loss:  0.5474544763565063
train gradient:  0.16858848693200762
iteration : 7132
train acc:  0.8125
train loss:  0.4250648617744446
train gradient:  0.10413058850601016
iteration : 7133
train acc:  0.6796875
train loss:  0.5405005812644958
train gradient:  0.16275748488296599
iteration : 7134
train acc:  0.75
train loss:  0.49156850576400757
train gradient:  0.13723964789389653
iteration : 7135
train acc:  0.75
train loss:  0.5460339784622192
train gradient:  0.1228019207809861
iteration : 7136
train acc:  0.8046875
train loss:  0.43147867918014526
train gradient:  0.09851410751279234
iteration : 7137
train acc:  0.6796875
train loss:  0.5887124538421631
train gradient:  0.1621681372735252
iteration : 7138
train acc:  0.6953125
train loss:  0.5464433431625366
train gradient:  0.1721029317760574
iteration : 7139
train acc:  0.7578125
train loss:  0.4891730844974518
train gradient:  0.12432204239567887
iteration : 7140
train acc:  0.6796875
train loss:  0.5447098612785339
train gradient:  0.1764216443030847
iteration : 7141
train acc:  0.703125
train loss:  0.507982611656189
train gradient:  0.14502470700332412
iteration : 7142
train acc:  0.7265625
train loss:  0.5033737421035767
train gradient:  0.14307140062290882
iteration : 7143
train acc:  0.765625
train loss:  0.460710346698761
train gradient:  0.09868588936806762
iteration : 7144
train acc:  0.75
train loss:  0.5273966193199158
train gradient:  0.13275389437209043
iteration : 7145
train acc:  0.7578125
train loss:  0.45601528882980347
train gradient:  0.12821165094727846
iteration : 7146
train acc:  0.703125
train loss:  0.5792418122291565
train gradient:  0.17495298327734737
iteration : 7147
train acc:  0.6953125
train loss:  0.5357621908187866
train gradient:  0.12879819628316486
iteration : 7148
train acc:  0.6953125
train loss:  0.5368170142173767
train gradient:  0.1343893423218543
iteration : 7149
train acc:  0.78125
train loss:  0.4701816439628601
train gradient:  0.16781738160934534
iteration : 7150
train acc:  0.78125
train loss:  0.4766736626625061
train gradient:  0.1292877925905765
iteration : 7151
train acc:  0.7578125
train loss:  0.4993652105331421
train gradient:  0.15379294523073522
iteration : 7152
train acc:  0.765625
train loss:  0.5089542865753174
train gradient:  0.13665610323493482
iteration : 7153
train acc:  0.7421875
train loss:  0.48337462544441223
train gradient:  0.12062886193175577
iteration : 7154
train acc:  0.796875
train loss:  0.46069201827049255
train gradient:  0.11077272402050946
iteration : 7155
train acc:  0.7109375
train loss:  0.4809282422065735
train gradient:  0.11874114704841697
iteration : 7156
train acc:  0.734375
train loss:  0.4739508032798767
train gradient:  0.11506204637430323
iteration : 7157
train acc:  0.734375
train loss:  0.4847877621650696
train gradient:  0.11973273643290176
iteration : 7158
train acc:  0.84375
train loss:  0.43550071120262146
train gradient:  0.09342625051866699
iteration : 7159
train acc:  0.7421875
train loss:  0.5065116882324219
train gradient:  0.13894658237598406
iteration : 7160
train acc:  0.6953125
train loss:  0.5478813052177429
train gradient:  0.18553881561312618
iteration : 7161
train acc:  0.7421875
train loss:  0.44048455357551575
train gradient:  0.11213345763655255
iteration : 7162
train acc:  0.7421875
train loss:  0.5158467292785645
train gradient:  0.13953609639534148
iteration : 7163
train acc:  0.6796875
train loss:  0.6387165784835815
train gradient:  0.18210466575533807
iteration : 7164
train acc:  0.703125
train loss:  0.5263229608535767
train gradient:  0.13930016278109675
iteration : 7165
train acc:  0.6640625
train loss:  0.5590930581092834
train gradient:  0.13089114985281364
iteration : 7166
train acc:  0.7734375
train loss:  0.496106892824173
train gradient:  0.11158231430002129
iteration : 7167
train acc:  0.7578125
train loss:  0.47899407148361206
train gradient:  0.11038030171823816
iteration : 7168
train acc:  0.75
train loss:  0.49163222312927246
train gradient:  0.1014034034597561
iteration : 7169
train acc:  0.78125
train loss:  0.46590161323547363
train gradient:  0.11613071096755487
iteration : 7170
train acc:  0.75
train loss:  0.4742799401283264
train gradient:  0.1115613731775477
iteration : 7171
train acc:  0.703125
train loss:  0.5427058935165405
train gradient:  0.11608852695390497
iteration : 7172
train acc:  0.7109375
train loss:  0.4797288179397583
train gradient:  0.1196643838438298
iteration : 7173
train acc:  0.7421875
train loss:  0.5199005007743835
train gradient:  0.12175978639945674
iteration : 7174
train acc:  0.6875
train loss:  0.5491820573806763
train gradient:  0.22714464689684605
iteration : 7175
train acc:  0.75
train loss:  0.5051034688949585
train gradient:  0.16702347087112085
iteration : 7176
train acc:  0.765625
train loss:  0.47299376130104065
train gradient:  0.12511461160306794
iteration : 7177
train acc:  0.7265625
train loss:  0.5392096638679504
train gradient:  0.17064966114380475
iteration : 7178
train acc:  0.703125
train loss:  0.5217275619506836
train gradient:  0.16199960895230767
iteration : 7179
train acc:  0.6953125
train loss:  0.513927161693573
train gradient:  0.12460110230802214
iteration : 7180
train acc:  0.7265625
train loss:  0.46368855237960815
train gradient:  0.11220301335856388
iteration : 7181
train acc:  0.7421875
train loss:  0.4808831810951233
train gradient:  0.11415488189202912
iteration : 7182
train acc:  0.8125
train loss:  0.4530521035194397
train gradient:  0.13408021632913225
iteration : 7183
train acc:  0.7265625
train loss:  0.48331403732299805
train gradient:  0.11034826128292625
iteration : 7184
train acc:  0.78125
train loss:  0.46062374114990234
train gradient:  0.12138097897357152
iteration : 7185
train acc:  0.640625
train loss:  0.6509930491447449
train gradient:  0.2317273521528634
iteration : 7186
train acc:  0.7265625
train loss:  0.5144432187080383
train gradient:  0.12643603281591212
iteration : 7187
train acc:  0.7265625
train loss:  0.5488052368164062
train gradient:  0.15203705151422117
iteration : 7188
train acc:  0.6953125
train loss:  0.5362634062767029
train gradient:  0.14455702779349644
iteration : 7189
train acc:  0.703125
train loss:  0.5114850401878357
train gradient:  0.17039936323510246
iteration : 7190
train acc:  0.75
train loss:  0.5114266276359558
train gradient:  0.2101970339155413
iteration : 7191
train acc:  0.734375
train loss:  0.49061888456344604
train gradient:  0.12742216365488046
iteration : 7192
train acc:  0.7890625
train loss:  0.5013676881790161
train gradient:  0.13308354129725586
iteration : 7193
train acc:  0.6875
train loss:  0.5853898525238037
train gradient:  0.1685281413002384
iteration : 7194
train acc:  0.7734375
train loss:  0.45647507905960083
train gradient:  0.10296842834553643
iteration : 7195
train acc:  0.796875
train loss:  0.46096980571746826
train gradient:  0.10380850436442032
iteration : 7196
train acc:  0.7421875
train loss:  0.4565178453922272
train gradient:  0.10646869442313947
iteration : 7197
train acc:  0.765625
train loss:  0.49526384472846985
train gradient:  0.15347198533344897
iteration : 7198
train acc:  0.78125
train loss:  0.442921906709671
train gradient:  0.12044040730494938
iteration : 7199
train acc:  0.75
train loss:  0.4745333790779114
train gradient:  0.11451109077294419
iteration : 7200
train acc:  0.71875
train loss:  0.538355827331543
train gradient:  0.15958750098320523
iteration : 7201
train acc:  0.7421875
train loss:  0.5213521718978882
train gradient:  0.1350955374322545
iteration : 7202
train acc:  0.6875
train loss:  0.5271784067153931
train gradient:  0.14361061477550047
iteration : 7203
train acc:  0.7734375
train loss:  0.4528099596500397
train gradient:  0.1366422096861073
iteration : 7204
train acc:  0.7109375
train loss:  0.5118018388748169
train gradient:  0.1663680811067921
iteration : 7205
train acc:  0.7421875
train loss:  0.48698875308036804
train gradient:  0.15799071853237778
iteration : 7206
train acc:  0.6953125
train loss:  0.5015333890914917
train gradient:  0.12661731868913895
iteration : 7207
train acc:  0.7578125
train loss:  0.47756338119506836
train gradient:  0.13791728056806848
iteration : 7208
train acc:  0.7578125
train loss:  0.4729631245136261
train gradient:  0.11096435289093255
iteration : 7209
train acc:  0.828125
train loss:  0.39152809977531433
train gradient:  0.09104447783568337
iteration : 7210
train acc:  0.71875
train loss:  0.46754300594329834
train gradient:  0.1177053909570951
iteration : 7211
train acc:  0.6796875
train loss:  0.5123162865638733
train gradient:  0.11124986499294895
iteration : 7212
train acc:  0.796875
train loss:  0.4917011260986328
train gradient:  0.1589093341678114
iteration : 7213
train acc:  0.7109375
train loss:  0.5647262334823608
train gradient:  0.24596549907549
iteration : 7214
train acc:  0.734375
train loss:  0.5456489324569702
train gradient:  0.15630785615971882
iteration : 7215
train acc:  0.703125
train loss:  0.525499701499939
train gradient:  0.14333024803630023
iteration : 7216
train acc:  0.7265625
train loss:  0.45453983545303345
train gradient:  0.11864402493500098
iteration : 7217
train acc:  0.7265625
train loss:  0.4719173014163971
train gradient:  0.12676848751307684
iteration : 7218
train acc:  0.7109375
train loss:  0.5361429452896118
train gradient:  0.16792646777210782
iteration : 7219
train acc:  0.8046875
train loss:  0.4215005338191986
train gradient:  0.10393727250356972
iteration : 7220
train acc:  0.7109375
train loss:  0.5155930519104004
train gradient:  0.1546692539018094
iteration : 7221
train acc:  0.671875
train loss:  0.5197975039482117
train gradient:  0.16924308203867494
iteration : 7222
train acc:  0.765625
train loss:  0.4820673167705536
train gradient:  0.11267565835002925
iteration : 7223
train acc:  0.75
train loss:  0.44578230381011963
train gradient:  0.1178491702737319
iteration : 7224
train acc:  0.7421875
train loss:  0.46530959010124207
train gradient:  0.1282330601965328
iteration : 7225
train acc:  0.703125
train loss:  0.5178507566452026
train gradient:  0.14237948252449642
iteration : 7226
train acc:  0.6875
train loss:  0.5322378277778625
train gradient:  0.11582451228418345
iteration : 7227
train acc:  0.8125
train loss:  0.4407085180282593
train gradient:  0.10579376920626382
iteration : 7228
train acc:  0.7265625
train loss:  0.48590269684791565
train gradient:  0.17595873179571941
iteration : 7229
train acc:  0.65625
train loss:  0.5204493403434753
train gradient:  0.14073698682480817
iteration : 7230
train acc:  0.671875
train loss:  0.5264039635658264
train gradient:  0.13135940512278646
iteration : 7231
train acc:  0.65625
train loss:  0.5272412300109863
train gradient:  0.15787447041283592
iteration : 7232
train acc:  0.7109375
train loss:  0.49577170610427856
train gradient:  0.15501217508593962
iteration : 7233
train acc:  0.75
train loss:  0.4602457880973816
train gradient:  0.10565746700815483
iteration : 7234
train acc:  0.71875
train loss:  0.5618025064468384
train gradient:  0.2032066853471992
iteration : 7235
train acc:  0.75
train loss:  0.4783814251422882
train gradient:  0.11743012446393018
iteration : 7236
train acc:  0.78125
train loss:  0.4455016553401947
train gradient:  0.12246987421338579
iteration : 7237
train acc:  0.765625
train loss:  0.4623071551322937
train gradient:  0.15761797076534495
iteration : 7238
train acc:  0.78125
train loss:  0.43440359830856323
train gradient:  0.0960765093895477
iteration : 7239
train acc:  0.734375
train loss:  0.5179498791694641
train gradient:  0.15688737818201748
iteration : 7240
train acc:  0.78125
train loss:  0.46810245513916016
train gradient:  0.1037890351965586
iteration : 7241
train acc:  0.765625
train loss:  0.48068273067474365
train gradient:  0.15412568059170292
iteration : 7242
train acc:  0.7421875
train loss:  0.484605610370636
train gradient:  0.16357903646350078
iteration : 7243
train acc:  0.734375
train loss:  0.4975726306438446
train gradient:  0.12372318341302249
iteration : 7244
train acc:  0.8125
train loss:  0.4569132626056671
train gradient:  0.11396416103358882
iteration : 7245
train acc:  0.8125
train loss:  0.43516993522644043
train gradient:  0.10373927240417433
iteration : 7246
train acc:  0.7578125
train loss:  0.47428280115127563
train gradient:  0.11647308630509098
iteration : 7247
train acc:  0.8046875
train loss:  0.49851298332214355
train gradient:  0.11462979779836367
iteration : 7248
train acc:  0.75
train loss:  0.4561450183391571
train gradient:  0.14808845458479003
iteration : 7249
train acc:  0.71875
train loss:  0.5188100337982178
train gradient:  0.1379848044026884
iteration : 7250
train acc:  0.7265625
train loss:  0.5618350505828857
train gradient:  0.20847133892444358
iteration : 7251
train acc:  0.7265625
train loss:  0.5402107238769531
train gradient:  0.13921395192023106
iteration : 7252
train acc:  0.796875
train loss:  0.4931967258453369
train gradient:  0.1306274128203353
iteration : 7253
train acc:  0.7890625
train loss:  0.4973360002040863
train gradient:  0.16734794575875006
iteration : 7254
train acc:  0.734375
train loss:  0.5571715831756592
train gradient:  0.173141221001097
iteration : 7255
train acc:  0.765625
train loss:  0.420518159866333
train gradient:  0.10096326031061768
iteration : 7256
train acc:  0.71875
train loss:  0.5145626068115234
train gradient:  0.12215967532235497
iteration : 7257
train acc:  0.71875
train loss:  0.5166144967079163
train gradient:  0.12905905945818325
iteration : 7258
train acc:  0.6953125
train loss:  0.5532224774360657
train gradient:  0.15931096590630273
iteration : 7259
train acc:  0.78125
train loss:  0.46137064695358276
train gradient:  0.11766062088979763
iteration : 7260
train acc:  0.734375
train loss:  0.5105338096618652
train gradient:  0.1131850921691922
iteration : 7261
train acc:  0.78125
train loss:  0.4610092341899872
train gradient:  0.10278901165792656
iteration : 7262
train acc:  0.703125
train loss:  0.5628291368484497
train gradient:  0.2870930555354037
iteration : 7263
train acc:  0.703125
train loss:  0.6182667016983032
train gradient:  0.2043828201630404
iteration : 7264
train acc:  0.75
train loss:  0.49962589144706726
train gradient:  0.1432414219595737
iteration : 7265
train acc:  0.734375
train loss:  0.5060853958129883
train gradient:  0.14321456610819072
iteration : 7266
train acc:  0.7734375
train loss:  0.460696816444397
train gradient:  0.10974856192441304
iteration : 7267
train acc:  0.7265625
train loss:  0.4709104001522064
train gradient:  0.08864934788333842
iteration : 7268
train acc:  0.6953125
train loss:  0.527331531047821
train gradient:  0.17037360326211204
iteration : 7269
train acc:  0.7578125
train loss:  0.4565294086933136
train gradient:  0.12221452827962193
iteration : 7270
train acc:  0.84375
train loss:  0.4180149435997009
train gradient:  0.10381093785413412
iteration : 7271
train acc:  0.765625
train loss:  0.47684502601623535
train gradient:  0.13112077545952244
iteration : 7272
train acc:  0.765625
train loss:  0.45339757204055786
train gradient:  0.10481385446286862
iteration : 7273
train acc:  0.734375
train loss:  0.4752868115901947
train gradient:  0.11544458261444918
iteration : 7274
train acc:  0.671875
train loss:  0.5613526105880737
train gradient:  0.1654454807364894
iteration : 7275
train acc:  0.75
train loss:  0.5023608207702637
train gradient:  0.1315746621815349
iteration : 7276
train acc:  0.734375
train loss:  0.4763492941856384
train gradient:  0.14057174333390599
iteration : 7277
train acc:  0.71875
train loss:  0.5191220045089722
train gradient:  0.12229377209274675
iteration : 7278
train acc:  0.703125
train loss:  0.5421994924545288
train gradient:  0.13254083043552
iteration : 7279
train acc:  0.71875
train loss:  0.5235555171966553
train gradient:  0.1135088585776508
iteration : 7280
train acc:  0.796875
train loss:  0.4734918177127838
train gradient:  0.13657516019831584
iteration : 7281
train acc:  0.7109375
train loss:  0.5536538362503052
train gradient:  0.16314841446308856
iteration : 7282
train acc:  0.7421875
train loss:  0.5196892023086548
train gradient:  0.17681242113026716
iteration : 7283
train acc:  0.7265625
train loss:  0.5424398183822632
train gradient:  0.1518797973132428
iteration : 7284
train acc:  0.7265625
train loss:  0.503462016582489
train gradient:  0.1295730304039902
iteration : 7285
train acc:  0.734375
train loss:  0.5505858063697815
train gradient:  0.15880621870208195
iteration : 7286
train acc:  0.7265625
train loss:  0.48017042875289917
train gradient:  0.1360767125718163
iteration : 7287
train acc:  0.75
train loss:  0.4740121066570282
train gradient:  0.11245196069754267
iteration : 7288
train acc:  0.7109375
train loss:  0.47408777475357056
train gradient:  0.10341262857767904
iteration : 7289
train acc:  0.765625
train loss:  0.49808719754219055
train gradient:  0.1750951208245834
iteration : 7290
train acc:  0.765625
train loss:  0.5432610511779785
train gradient:  0.20016448235839146
iteration : 7291
train acc:  0.7890625
train loss:  0.44139304757118225
train gradient:  0.13233922841949727
iteration : 7292
train acc:  0.7734375
train loss:  0.47413915395736694
train gradient:  0.12118401227621751
iteration : 7293
train acc:  0.78125
train loss:  0.4595317840576172
train gradient:  0.1375069175925558
iteration : 7294
train acc:  0.6796875
train loss:  0.6371962428092957
train gradient:  0.23360354775251263
iteration : 7295
train acc:  0.78125
train loss:  0.4770774841308594
train gradient:  0.10813043162795045
iteration : 7296
train acc:  0.703125
train loss:  0.5365082621574402
train gradient:  0.18174265318213956
iteration : 7297
train acc:  0.6953125
train loss:  0.5175572037696838
train gradient:  0.13006004641535873
iteration : 7298
train acc:  0.6328125
train loss:  0.5621650815010071
train gradient:  0.21084759167452666
iteration : 7299
train acc:  0.734375
train loss:  0.537753164768219
train gradient:  0.12743591120416548
iteration : 7300
train acc:  0.765625
train loss:  0.5510154962539673
train gradient:  0.1390611632676296
iteration : 7301
train acc:  0.6875
train loss:  0.4911746680736542
train gradient:  0.1442500834652057
iteration : 7302
train acc:  0.8125
train loss:  0.488323450088501
train gradient:  0.13504450803297646
iteration : 7303
train acc:  0.765625
train loss:  0.41352903842926025
train gradient:  0.08111746196264726
iteration : 7304
train acc:  0.78125
train loss:  0.4283815026283264
train gradient:  0.09837143953726252
iteration : 7305
train acc:  0.71875
train loss:  0.5353625416755676
train gradient:  0.13747600639496083
iteration : 7306
train acc:  0.75
train loss:  0.5123642086982727
train gradient:  0.13499801755569674
iteration : 7307
train acc:  0.6796875
train loss:  0.5716781616210938
train gradient:  0.17761117801627319
iteration : 7308
train acc:  0.71875
train loss:  0.4772833585739136
train gradient:  0.09605949896396247
iteration : 7309
train acc:  0.703125
train loss:  0.533743143081665
train gradient:  0.14367104478562917
iteration : 7310
train acc:  0.7421875
train loss:  0.4764118492603302
train gradient:  0.09986652854108628
iteration : 7311
train acc:  0.71875
train loss:  0.5812422037124634
train gradient:  0.23550976266200851
iteration : 7312
train acc:  0.7734375
train loss:  0.5097074508666992
train gradient:  0.1454841772800806
iteration : 7313
train acc:  0.7578125
train loss:  0.48481571674346924
train gradient:  0.13081210309069746
iteration : 7314
train acc:  0.8046875
train loss:  0.4823382794857025
train gradient:  0.13579669681822598
iteration : 7315
train acc:  0.7109375
train loss:  0.5224030613899231
train gradient:  0.109273916459515
iteration : 7316
train acc:  0.7265625
train loss:  0.46584105491638184
train gradient:  0.09735307076994876
iteration : 7317
train acc:  0.7578125
train loss:  0.5116938948631287
train gradient:  0.14516671208953263
iteration : 7318
train acc:  0.75
train loss:  0.4653908908367157
train gradient:  0.0994966876410978
iteration : 7319
train acc:  0.7265625
train loss:  0.500033974647522
train gradient:  0.10048841841257716
iteration : 7320
train acc:  0.7578125
train loss:  0.4774490296840668
train gradient:  0.11132426793796435
iteration : 7321
train acc:  0.7109375
train loss:  0.5224993228912354
train gradient:  0.13395980659528312
iteration : 7322
train acc:  0.78125
train loss:  0.48995769023895264
train gradient:  0.17629422061204325
iteration : 7323
train acc:  0.6796875
train loss:  0.5828745365142822
train gradient:  0.20590414265417614
iteration : 7324
train acc:  0.6796875
train loss:  0.5708479881286621
train gradient:  0.15434294580373265
iteration : 7325
train acc:  0.765625
train loss:  0.43975862860679626
train gradient:  0.1114383134083072
iteration : 7326
train acc:  0.703125
train loss:  0.5714028477668762
train gradient:  0.18487538375905954
iteration : 7327
train acc:  0.8125
train loss:  0.4280579090118408
train gradient:  0.1023163602495413
iteration : 7328
train acc:  0.7578125
train loss:  0.5169554948806763
train gradient:  0.12976962462570407
iteration : 7329
train acc:  0.7421875
train loss:  0.48724180459976196
train gradient:  0.18347367576211338
iteration : 7330
train acc:  0.78125
train loss:  0.47679463028907776
train gradient:  0.13523595159150426
iteration : 7331
train acc:  0.71875
train loss:  0.4788438081741333
train gradient:  0.10493234730344232
iteration : 7332
train acc:  0.765625
train loss:  0.4749617576599121
train gradient:  0.14650828681418976
iteration : 7333
train acc:  0.71875
train loss:  0.5363355875015259
train gradient:  0.13525388233653984
iteration : 7334
train acc:  0.703125
train loss:  0.5598652958869934
train gradient:  0.21827027677456845
iteration : 7335
train acc:  0.6953125
train loss:  0.558056116104126
train gradient:  0.1870077456796933
iteration : 7336
train acc:  0.6796875
train loss:  0.5470066070556641
train gradient:  0.15043782879806616
iteration : 7337
train acc:  0.7734375
train loss:  0.4765358567237854
train gradient:  0.159405337954274
iteration : 7338
train acc:  0.78125
train loss:  0.4407656192779541
train gradient:  0.12054848009363875
iteration : 7339
train acc:  0.6796875
train loss:  0.5824035406112671
train gradient:  0.20692506443161887
iteration : 7340
train acc:  0.7578125
train loss:  0.4969828426837921
train gradient:  0.13768739895242746
iteration : 7341
train acc:  0.75
train loss:  0.4493595063686371
train gradient:  0.108008847136603
iteration : 7342
train acc:  0.734375
train loss:  0.5360305309295654
train gradient:  0.14255434285875346
iteration : 7343
train acc:  0.765625
train loss:  0.48213645815849304
train gradient:  0.13864049184065796
iteration : 7344
train acc:  0.7109375
train loss:  0.5053170323371887
train gradient:  0.14104949664480487
iteration : 7345
train acc:  0.8125
train loss:  0.41216474771499634
train gradient:  0.10579809771220615
iteration : 7346
train acc:  0.734375
train loss:  0.4970228970050812
train gradient:  0.13170938151091272
iteration : 7347
train acc:  0.703125
train loss:  0.5274765491485596
train gradient:  0.132301767936492
iteration : 7348
train acc:  0.75
train loss:  0.49876999855041504
train gradient:  0.14000449393206096
iteration : 7349
train acc:  0.7578125
train loss:  0.4424648880958557
train gradient:  0.13283899500522273
iteration : 7350
train acc:  0.71875
train loss:  0.5000649690628052
train gradient:  0.1402195845633057
iteration : 7351
train acc:  0.734375
train loss:  0.5130465030670166
train gradient:  0.17630152167208846
iteration : 7352
train acc:  0.7421875
train loss:  0.49698197841644287
train gradient:  0.13318842055162927
iteration : 7353
train acc:  0.703125
train loss:  0.5200316309928894
train gradient:  0.13344680760933433
iteration : 7354
train acc:  0.6875
train loss:  0.506561279296875
train gradient:  0.13333446478400962
iteration : 7355
train acc:  0.6875
train loss:  0.5510604381561279
train gradient:  0.16589767066580585
iteration : 7356
train acc:  0.71875
train loss:  0.5122040510177612
train gradient:  0.12895627645725113
iteration : 7357
train acc:  0.8203125
train loss:  0.4076195955276489
train gradient:  0.11770072472173687
iteration : 7358
train acc:  0.734375
train loss:  0.49091050028800964
train gradient:  0.1681907410611191
iteration : 7359
train acc:  0.78125
train loss:  0.44144248962402344
train gradient:  0.1145942151178171
iteration : 7360
train acc:  0.6953125
train loss:  0.5224826335906982
train gradient:  0.1483430266329048
iteration : 7361
train acc:  0.7265625
train loss:  0.480040967464447
train gradient:  0.1711540666865537
iteration : 7362
train acc:  0.7265625
train loss:  0.45119035243988037
train gradient:  0.10143584804666596
iteration : 7363
train acc:  0.7265625
train loss:  0.5088193416595459
train gradient:  0.13065325126038888
iteration : 7364
train acc:  0.6796875
train loss:  0.5503839254379272
train gradient:  0.15380316518410572
iteration : 7365
train acc:  0.7734375
train loss:  0.47024106979370117
train gradient:  0.13015098316867058
iteration : 7366
train acc:  0.71875
train loss:  0.5315675139427185
train gradient:  0.14957891636432336
iteration : 7367
train acc:  0.6875
train loss:  0.5852552652359009
train gradient:  0.17754165505432168
iteration : 7368
train acc:  0.6796875
train loss:  0.5278162956237793
train gradient:  0.14901779907667712
iteration : 7369
train acc:  0.734375
train loss:  0.49550461769104004
train gradient:  0.1607690866415525
iteration : 7370
train acc:  0.796875
train loss:  0.467048704624176
train gradient:  0.10255719196074274
iteration : 7371
train acc:  0.6796875
train loss:  0.6153845191001892
train gradient:  0.27681616185469693
iteration : 7372
train acc:  0.7734375
train loss:  0.4626718759536743
train gradient:  0.12240844785974353
iteration : 7373
train acc:  0.7890625
train loss:  0.4874154031276703
train gradient:  0.11805322844018207
iteration : 7374
train acc:  0.7109375
train loss:  0.5145454406738281
train gradient:  0.11118246933111799
iteration : 7375
train acc:  0.7890625
train loss:  0.4924869239330292
train gradient:  0.1299006371870024
iteration : 7376
train acc:  0.7421875
train loss:  0.46775299310684204
train gradient:  0.1037061578889396
iteration : 7377
train acc:  0.7421875
train loss:  0.5047757625579834
train gradient:  0.13312134561860267
iteration : 7378
train acc:  0.734375
train loss:  0.5012576580047607
train gradient:  0.13160765287443155
iteration : 7379
train acc:  0.703125
train loss:  0.5562910437583923
train gradient:  0.13820684859600457
iteration : 7380
train acc:  0.78125
train loss:  0.4624800682067871
train gradient:  0.13731040221996543
iteration : 7381
train acc:  0.7265625
train loss:  0.5406086444854736
train gradient:  0.1993708626073124
iteration : 7382
train acc:  0.7578125
train loss:  0.5059671401977539
train gradient:  0.13169180074410713
iteration : 7383
train acc:  0.8203125
train loss:  0.43087831139564514
train gradient:  0.13538877860425041
iteration : 7384
train acc:  0.71875
train loss:  0.542765736579895
train gradient:  0.14387317178012915
iteration : 7385
train acc:  0.75
train loss:  0.5053898096084595
train gradient:  0.14342331024048893
iteration : 7386
train acc:  0.7890625
train loss:  0.4218904972076416
train gradient:  0.11095051254841189
iteration : 7387
train acc:  0.7734375
train loss:  0.4544682502746582
train gradient:  0.12444421974275381
iteration : 7388
train acc:  0.765625
train loss:  0.5067121982574463
train gradient:  0.13768922225354707
iteration : 7389
train acc:  0.75
train loss:  0.49375778436660767
train gradient:  0.15855288982768562
iteration : 7390
train acc:  0.71875
train loss:  0.5100840330123901
train gradient:  0.13624112764839863
iteration : 7391
train acc:  0.734375
train loss:  0.481502890586853
train gradient:  0.12046937293216294
iteration : 7392
train acc:  0.765625
train loss:  0.47162294387817383
train gradient:  0.11049053936733642
iteration : 7393
train acc:  0.734375
train loss:  0.5070434808731079
train gradient:  0.18087909939714356
iteration : 7394
train acc:  0.7265625
train loss:  0.5227404236793518
train gradient:  0.12483618375619504
iteration : 7395
train acc:  0.71875
train loss:  0.4912663996219635
train gradient:  0.11899567175957047
iteration : 7396
train acc:  0.7265625
train loss:  0.526844322681427
train gradient:  0.155613234858697
iteration : 7397
train acc:  0.6875
train loss:  0.5401917695999146
train gradient:  0.17567332536291208
iteration : 7398
train acc:  0.71875
train loss:  0.49355974793434143
train gradient:  0.10720479959338719
iteration : 7399
train acc:  0.71875
train loss:  0.539891242980957
train gradient:  0.15032609040609254
iteration : 7400
train acc:  0.7421875
train loss:  0.5010807514190674
train gradient:  0.13569013526883383
iteration : 7401
train acc:  0.71875
train loss:  0.5204315185546875
train gradient:  0.15903285303941622
iteration : 7402
train acc:  0.7109375
train loss:  0.5315283536911011
train gradient:  0.1228581891421514
iteration : 7403
train acc:  0.7578125
train loss:  0.474026620388031
train gradient:  0.10848902181318988
iteration : 7404
train acc:  0.7890625
train loss:  0.47363460063934326
train gradient:  0.1351237882791741
iteration : 7405
train acc:  0.703125
train loss:  0.5168992877006531
train gradient:  0.14633022826415276
iteration : 7406
train acc:  0.75
train loss:  0.47426527738571167
train gradient:  0.11552276008583097
iteration : 7407
train acc:  0.7578125
train loss:  0.4532957673072815
train gradient:  0.09260544704303238
iteration : 7408
train acc:  0.734375
train loss:  0.5008203983306885
train gradient:  0.14017381902509185
iteration : 7409
train acc:  0.6953125
train loss:  0.5632469058036804
train gradient:  0.13385772934233287
iteration : 7410
train acc:  0.765625
train loss:  0.4965343475341797
train gradient:  0.14358191265775166
iteration : 7411
train acc:  0.734375
train loss:  0.5109506845474243
train gradient:  0.1336102329011728
iteration : 7412
train acc:  0.71875
train loss:  0.4835137724876404
train gradient:  0.11647701871133669
iteration : 7413
train acc:  0.7578125
train loss:  0.4951854944229126
train gradient:  0.13742157522416804
iteration : 7414
train acc:  0.796875
train loss:  0.4638335704803467
train gradient:  0.12695305228845924
iteration : 7415
train acc:  0.7265625
train loss:  0.49907025694847107
train gradient:  0.0955090604026343
iteration : 7416
train acc:  0.640625
train loss:  0.5910854339599609
train gradient:  0.15725809172871896
iteration : 7417
train acc:  0.703125
train loss:  0.5047228336334229
train gradient:  0.13599549272295844
iteration : 7418
train acc:  0.7265625
train loss:  0.5367143154144287
train gradient:  0.14992370110685993
iteration : 7419
train acc:  0.78125
train loss:  0.484277606010437
train gradient:  0.14245217188114073
iteration : 7420
train acc:  0.7109375
train loss:  0.5615716576576233
train gradient:  0.17354535701329415
iteration : 7421
train acc:  0.765625
train loss:  0.5084863305091858
train gradient:  0.1542051911548204
iteration : 7422
train acc:  0.7734375
train loss:  0.47410494089126587
train gradient:  0.14249412513018234
iteration : 7423
train acc:  0.7265625
train loss:  0.5107613801956177
train gradient:  0.1656875117557184
iteration : 7424
train acc:  0.6953125
train loss:  0.5258480906486511
train gradient:  0.17757394402225654
iteration : 7425
train acc:  0.7578125
train loss:  0.5654799938201904
train gradient:  0.18577382882932336
iteration : 7426
train acc:  0.734375
train loss:  0.5084879398345947
train gradient:  0.14515066792858045
iteration : 7427
train acc:  0.7578125
train loss:  0.4316222369670868
train gradient:  0.1204181614373991
iteration : 7428
train acc:  0.78125
train loss:  0.42878514528274536
train gradient:  0.12933411751777119
iteration : 7429
train acc:  0.75
train loss:  0.4853293001651764
train gradient:  0.13201517392144788
iteration : 7430
train acc:  0.71875
train loss:  0.5112536549568176
train gradient:  0.1194624847058901
iteration : 7431
train acc:  0.7109375
train loss:  0.5403561592102051
train gradient:  0.13600312210915294
iteration : 7432
train acc:  0.7578125
train loss:  0.49272915720939636
train gradient:  0.14552057435766153
iteration : 7433
train acc:  0.7890625
train loss:  0.4907034933567047
train gradient:  0.14999693110160137
iteration : 7434
train acc:  0.6875
train loss:  0.5664949417114258
train gradient:  0.15181287843095653
iteration : 7435
train acc:  0.6875
train loss:  0.5142006874084473
train gradient:  0.1445517831656027
iteration : 7436
train acc:  0.71875
train loss:  0.5109062194824219
train gradient:  0.14833921864083116
iteration : 7437
train acc:  0.7265625
train loss:  0.4773634672164917
train gradient:  0.11787613420385414
iteration : 7438
train acc:  0.7109375
train loss:  0.5564489364624023
train gradient:  0.181789471069343
iteration : 7439
train acc:  0.7578125
train loss:  0.5051276087760925
train gradient:  0.16708059856258498
iteration : 7440
train acc:  0.78125
train loss:  0.48091840744018555
train gradient:  0.1339165518146389
iteration : 7441
train acc:  0.75
train loss:  0.5022618174552917
train gradient:  0.12856486699896266
iteration : 7442
train acc:  0.71875
train loss:  0.5738551616668701
train gradient:  0.1642292477992027
iteration : 7443
train acc:  0.7578125
train loss:  0.48126742243766785
train gradient:  0.14247665373380053
iteration : 7444
train acc:  0.8046875
train loss:  0.44852250814437866
train gradient:  0.14250745750075416
iteration : 7445
train acc:  0.7421875
train loss:  0.4931074380874634
train gradient:  0.16145297104671752
iteration : 7446
train acc:  0.7109375
train loss:  0.52641761302948
train gradient:  0.15979863142890982
iteration : 7447
train acc:  0.6640625
train loss:  0.5266729593276978
train gradient:  0.1468817972757433
iteration : 7448
train acc:  0.703125
train loss:  0.5477604269981384
train gradient:  0.13228730918509793
iteration : 7449
train acc:  0.734375
train loss:  0.5433294773101807
train gradient:  0.16578752303272037
iteration : 7450
train acc:  0.7109375
train loss:  0.4657597541809082
train gradient:  0.10388454646965321
iteration : 7451
train acc:  0.7421875
train loss:  0.48928770422935486
train gradient:  0.11543177649421693
iteration : 7452
train acc:  0.75
train loss:  0.5297422409057617
train gradient:  0.17520076797048778
iteration : 7453
train acc:  0.7578125
train loss:  0.5343889594078064
train gradient:  0.13643813877723646
iteration : 7454
train acc:  0.7109375
train loss:  0.5056555271148682
train gradient:  0.21903580200102402
iteration : 7455
train acc:  0.7890625
train loss:  0.4422610402107239
train gradient:  0.1086690401655952
iteration : 7456
train acc:  0.703125
train loss:  0.5539749264717102
train gradient:  0.1379856997838017
iteration : 7457
train acc:  0.6953125
train loss:  0.5176618695259094
train gradient:  0.2033649985906264
iteration : 7458
train acc:  0.6796875
train loss:  0.5136532783508301
train gradient:  0.13481249802850087
iteration : 7459
train acc:  0.6796875
train loss:  0.5523590445518494
train gradient:  0.16714542366984372
iteration : 7460
train acc:  0.7734375
train loss:  0.5062437057495117
train gradient:  0.1706748426185684
iteration : 7461
train acc:  0.78125
train loss:  0.4807908236980438
train gradient:  0.14332253887648638
iteration : 7462
train acc:  0.765625
train loss:  0.45920008420944214
train gradient:  0.12894069807726077
iteration : 7463
train acc:  0.71875
train loss:  0.5110440254211426
train gradient:  0.14963773727985152
iteration : 7464
train acc:  0.71875
train loss:  0.5069746971130371
train gradient:  0.12401657233142722
iteration : 7465
train acc:  0.71875
train loss:  0.549164891242981
train gradient:  0.14077140235407132
iteration : 7466
train acc:  0.7265625
train loss:  0.5560399889945984
train gradient:  0.1941914524391379
iteration : 7467
train acc:  0.765625
train loss:  0.4671716094017029
train gradient:  0.13456083223042417
iteration : 7468
train acc:  0.6875
train loss:  0.5619338750839233
train gradient:  0.15342654435331082
iteration : 7469
train acc:  0.734375
train loss:  0.5090405941009521
train gradient:  0.1304380729976468
iteration : 7470
train acc:  0.8203125
train loss:  0.4372870922088623
train gradient:  0.1185603387878044
iteration : 7471
train acc:  0.7578125
train loss:  0.5078452825546265
train gradient:  0.13333142670776635
iteration : 7472
train acc:  0.7734375
train loss:  0.48221516609191895
train gradient:  0.090964667691221
iteration : 7473
train acc:  0.734375
train loss:  0.47442686557769775
train gradient:  0.11291386445872435
iteration : 7474
train acc:  0.6484375
train loss:  0.5856701731681824
train gradient:  0.15597814879689484
iteration : 7475
train acc:  0.7890625
train loss:  0.4549511969089508
train gradient:  0.08587288222379538
iteration : 7476
train acc:  0.75
train loss:  0.45474666357040405
train gradient:  0.08866443445268347
iteration : 7477
train acc:  0.6875
train loss:  0.5593427419662476
train gradient:  0.14904039893842913
iteration : 7478
train acc:  0.6484375
train loss:  0.5427075624465942
train gradient:  0.14371693858935533
iteration : 7479
train acc:  0.8125
train loss:  0.3890313506126404
train gradient:  0.08365121934453307
iteration : 7480
train acc:  0.7265625
train loss:  0.51450514793396
train gradient:  0.1501881913298716
iteration : 7481
train acc:  0.8203125
train loss:  0.4126793444156647
train gradient:  0.09145193690939349
iteration : 7482
train acc:  0.7109375
train loss:  0.5587279200553894
train gradient:  0.17063566261068766
iteration : 7483
train acc:  0.7265625
train loss:  0.498139351606369
train gradient:  0.11483440707590456
iteration : 7484
train acc:  0.734375
train loss:  0.5105775594711304
train gradient:  0.14947300917700218
iteration : 7485
train acc:  0.6953125
train loss:  0.5229402780532837
train gradient:  0.1414314941646565
iteration : 7486
train acc:  0.7578125
train loss:  0.5116740465164185
train gradient:  0.14334697416474473
iteration : 7487
train acc:  0.7265625
train loss:  0.5095089673995972
train gradient:  0.1347609763375523
iteration : 7488
train acc:  0.703125
train loss:  0.5168396830558777
train gradient:  0.1379887854852488
iteration : 7489
train acc:  0.7109375
train loss:  0.5699727535247803
train gradient:  0.15993656639183784
iteration : 7490
train acc:  0.7109375
train loss:  0.5945570468902588
train gradient:  0.16888333296529145
iteration : 7491
train acc:  0.7109375
train loss:  0.5171474814414978
train gradient:  0.14135418569086824
iteration : 7492
train acc:  0.7109375
train loss:  0.53464275598526
train gradient:  0.12275515206711539
iteration : 7493
train acc:  0.8046875
train loss:  0.45097699761390686
train gradient:  0.11123781923047674
iteration : 7494
train acc:  0.7421875
train loss:  0.4569174647331238
train gradient:  0.1313146615518198
iteration : 7495
train acc:  0.71875
train loss:  0.5533086657524109
train gradient:  0.24077223703323275
iteration : 7496
train acc:  0.7265625
train loss:  0.5098170638084412
train gradient:  0.1600733253984433
iteration : 7497
train acc:  0.734375
train loss:  0.524911642074585
train gradient:  0.1481718165788778
iteration : 7498
train acc:  0.7734375
train loss:  0.4807760715484619
train gradient:  0.14254710217777694
iteration : 7499
train acc:  0.7421875
train loss:  0.5411581993103027
train gradient:  0.1388490455578529
iteration : 7500
train acc:  0.7578125
train loss:  0.4855133891105652
train gradient:  0.121763061957134
iteration : 7501
train acc:  0.78125
train loss:  0.4714548587799072
train gradient:  0.14398200297731398
iteration : 7502
train acc:  0.8203125
train loss:  0.4122997522354126
train gradient:  0.11231438657744514
iteration : 7503
train acc:  0.7734375
train loss:  0.47318214178085327
train gradient:  0.15034669499246822
iteration : 7504
train acc:  0.765625
train loss:  0.5084022283554077
train gradient:  0.10901593197291769
iteration : 7505
train acc:  0.7421875
train loss:  0.5064541101455688
train gradient:  0.11857055438808067
iteration : 7506
train acc:  0.7578125
train loss:  0.49265056848526
train gradient:  0.11438889051594472
iteration : 7507
train acc:  0.7578125
train loss:  0.5162236094474792
train gradient:  0.11920409476841579
iteration : 7508
train acc:  0.734375
train loss:  0.5176802277565002
train gradient:  0.15106033015908166
iteration : 7509
train acc:  0.6953125
train loss:  0.5433118343353271
train gradient:  0.1501435348038661
iteration : 7510
train acc:  0.703125
train loss:  0.5271721482276917
train gradient:  0.11869515956802533
iteration : 7511
train acc:  0.75
train loss:  0.49621766805648804
train gradient:  0.10348303383628069
iteration : 7512
train acc:  0.7890625
train loss:  0.43020421266555786
train gradient:  0.09941176057063948
iteration : 7513
train acc:  0.71875
train loss:  0.5135815143585205
train gradient:  0.1381472222293953
iteration : 7514
train acc:  0.6953125
train loss:  0.5296649932861328
train gradient:  0.14363287842625277
iteration : 7515
train acc:  0.7578125
train loss:  0.46761223673820496
train gradient:  0.11919511336071725
iteration : 7516
train acc:  0.703125
train loss:  0.5663093328475952
train gradient:  0.17698590571917971
iteration : 7517
train acc:  0.7578125
train loss:  0.5057240128517151
train gradient:  0.12514309464819584
iteration : 7518
train acc:  0.7421875
train loss:  0.5138207077980042
train gradient:  0.12822269975874756
iteration : 7519
train acc:  0.7109375
train loss:  0.4952883720397949
train gradient:  0.12996174184271803
iteration : 7520
train acc:  0.75
train loss:  0.49427926540374756
train gradient:  0.1128642570105212
iteration : 7521
train acc:  0.6796875
train loss:  0.5545752048492432
train gradient:  0.1815170783709802
iteration : 7522
train acc:  0.765625
train loss:  0.5041860342025757
train gradient:  0.161337377943301
iteration : 7523
train acc:  0.8203125
train loss:  0.4171348214149475
train gradient:  0.09327124503021117
iteration : 7524
train acc:  0.7578125
train loss:  0.49053955078125
train gradient:  0.12825858926046949
iteration : 7525
train acc:  0.734375
train loss:  0.4997776746749878
train gradient:  0.12535590262016927
iteration : 7526
train acc:  0.6796875
train loss:  0.5382190942764282
train gradient:  0.1242260542001792
iteration : 7527
train acc:  0.640625
train loss:  0.5554349422454834
train gradient:  0.15113493149086862
iteration : 7528
train acc:  0.765625
train loss:  0.4852323532104492
train gradient:  0.1238193264662511
iteration : 7529
train acc:  0.8515625
train loss:  0.3644256591796875
train gradient:  0.10516929369524038
iteration : 7530
train acc:  0.765625
train loss:  0.4689142107963562
train gradient:  0.11750207112313399
iteration : 7531
train acc:  0.75
train loss:  0.48291486501693726
train gradient:  0.12027082127611971
iteration : 7532
train acc:  0.734375
train loss:  0.4738050103187561
train gradient:  0.10389578681507182
iteration : 7533
train acc:  0.71875
train loss:  0.5507361888885498
train gradient:  0.1715605231451448
iteration : 7534
train acc:  0.765625
train loss:  0.49377214908599854
train gradient:  0.14921566257267554
iteration : 7535
train acc:  0.78125
train loss:  0.4454755187034607
train gradient:  0.10473874799507159
iteration : 7536
train acc:  0.7578125
train loss:  0.4575810432434082
train gradient:  0.0971868387416574
iteration : 7537
train acc:  0.7734375
train loss:  0.4718635678291321
train gradient:  0.12500064445022432
iteration : 7538
train acc:  0.75
train loss:  0.49663493037223816
train gradient:  0.1534431682179152
iteration : 7539
train acc:  0.671875
train loss:  0.5476536154747009
train gradient:  0.14236127473023222
iteration : 7540
train acc:  0.734375
train loss:  0.5262656211853027
train gradient:  0.18441344187483244
iteration : 7541
train acc:  0.6796875
train loss:  0.5537046194076538
train gradient:  0.14401162172898868
iteration : 7542
train acc:  0.7578125
train loss:  0.504348874092102
train gradient:  0.18175476012919695
iteration : 7543
train acc:  0.703125
train loss:  0.5270770788192749
train gradient:  0.17743024582439298
iteration : 7544
train acc:  0.65625
train loss:  0.5696909427642822
train gradient:  0.1700120658709192
iteration : 7545
train acc:  0.71875
train loss:  0.531420111656189
train gradient:  0.18467900285245162
iteration : 7546
train acc:  0.7421875
train loss:  0.46447694301605225
train gradient:  0.1349081938440237
iteration : 7547
train acc:  0.8046875
train loss:  0.43011951446533203
train gradient:  0.1086606501955036
iteration : 7548
train acc:  0.6953125
train loss:  0.5300902128219604
train gradient:  0.15651320109931643
iteration : 7549
train acc:  0.703125
train loss:  0.5155130624771118
train gradient:  0.13347645364800864
iteration : 7550
train acc:  0.6875
train loss:  0.5832626819610596
train gradient:  0.20958802985092284
iteration : 7551
train acc:  0.6875
train loss:  0.5221420526504517
train gradient:  0.1482096536026962
iteration : 7552
train acc:  0.671875
train loss:  0.6070213913917542
train gradient:  0.1756867206197137
iteration : 7553
train acc:  0.765625
train loss:  0.47488465905189514
train gradient:  0.12070202867478037
iteration : 7554
train acc:  0.75
train loss:  0.5221590995788574
train gradient:  0.131755147309574
iteration : 7555
train acc:  0.796875
train loss:  0.458824098110199
train gradient:  0.11892644409192922
iteration : 7556
train acc:  0.7578125
train loss:  0.49302470684051514
train gradient:  0.13269975266474981
iteration : 7557
train acc:  0.75
train loss:  0.5378109216690063
train gradient:  0.15445354264108926
iteration : 7558
train acc:  0.7109375
train loss:  0.48806998133659363
train gradient:  0.1288238366373345
iteration : 7559
train acc:  0.7265625
train loss:  0.4937219023704529
train gradient:  0.14372913987507857
iteration : 7560
train acc:  0.7265625
train loss:  0.5088232159614563
train gradient:  0.1277654015927489
iteration : 7561
train acc:  0.6796875
train loss:  0.5269150137901306
train gradient:  0.11651210978333175
iteration : 7562
train acc:  0.7578125
train loss:  0.48784327507019043
train gradient:  0.13005929257319887
iteration : 7563
train acc:  0.7578125
train loss:  0.4876171350479126
train gradient:  0.1350238133156653
iteration : 7564
train acc:  0.6875
train loss:  0.5217016339302063
train gradient:  0.1333605928816962
iteration : 7565
train acc:  0.78125
train loss:  0.45078033208847046
train gradient:  0.09502990410124225
iteration : 7566
train acc:  0.671875
train loss:  0.5563946962356567
train gradient:  0.19182749391807113
iteration : 7567
train acc:  0.6953125
train loss:  0.5118635892868042
train gradient:  0.14216775129587028
iteration : 7568
train acc:  0.71875
train loss:  0.5664077997207642
train gradient:  0.1757285890735238
iteration : 7569
train acc:  0.7265625
train loss:  0.5684961080551147
train gradient:  0.1735613557608428
iteration : 7570
train acc:  0.71875
train loss:  0.517152726650238
train gradient:  0.12820429932494776
iteration : 7571
train acc:  0.7421875
train loss:  0.4764721095561981
train gradient:  0.11792704199360612
iteration : 7572
train acc:  0.8515625
train loss:  0.4304007887840271
train gradient:  0.11126312842333326
iteration : 7573
train acc:  0.75
train loss:  0.4846917390823364
train gradient:  0.12347241968459866
iteration : 7574
train acc:  0.71875
train loss:  0.5168026089668274
train gradient:  0.1703244936928301
iteration : 7575
train acc:  0.7734375
train loss:  0.4481618404388428
train gradient:  0.11352786318157485
iteration : 7576
train acc:  0.8046875
train loss:  0.4539795517921448
train gradient:  0.10354075380179747
iteration : 7577
train acc:  0.7265625
train loss:  0.46800893545150757
train gradient:  0.10900472771189262
iteration : 7578
train acc:  0.75
train loss:  0.5037614703178406
train gradient:  0.12768415018369716
iteration : 7579
train acc:  0.734375
train loss:  0.4891980290412903
train gradient:  0.10789972421568755
iteration : 7580
train acc:  0.734375
train loss:  0.5401422381401062
train gradient:  0.12768940699068396
iteration : 7581
train acc:  0.71875
train loss:  0.5202481746673584
train gradient:  0.1269100058266836
iteration : 7582
train acc:  0.75
train loss:  0.4792046546936035
train gradient:  0.12436984177827674
iteration : 7583
train acc:  0.7734375
train loss:  0.5082588195800781
train gradient:  0.12020281408837606
iteration : 7584
train acc:  0.71875
train loss:  0.4820738732814789
train gradient:  0.09546736885534413
iteration : 7585
train acc:  0.8125
train loss:  0.44554996490478516
train gradient:  0.15225344144330974
iteration : 7586
train acc:  0.78125
train loss:  0.45339956879615784
train gradient:  0.1352763766507566
iteration : 7587
train acc:  0.734375
train loss:  0.485678493976593
train gradient:  0.10481061690068277
iteration : 7588
train acc:  0.734375
train loss:  0.5011733770370483
train gradient:  0.13472834850558157
iteration : 7589
train acc:  0.75
train loss:  0.4490697681903839
train gradient:  0.11754804167905222
iteration : 7590
train acc:  0.6796875
train loss:  0.5914281606674194
train gradient:  0.18392811893432098
iteration : 7591
train acc:  0.7734375
train loss:  0.45767727494239807
train gradient:  0.09730464758666414
iteration : 7592
train acc:  0.71875
train loss:  0.517084538936615
train gradient:  0.1416222871701912
iteration : 7593
train acc:  0.7734375
train loss:  0.49794554710388184
train gradient:  0.12994470570369923
iteration : 7594
train acc:  0.7734375
train loss:  0.45795175433158875
train gradient:  0.11773808004638629
iteration : 7595
train acc:  0.71875
train loss:  0.5338085889816284
train gradient:  0.22357516634715918
iteration : 7596
train acc:  0.71875
train loss:  0.5237265229225159
train gradient:  0.14576395688624133
iteration : 7597
train acc:  0.6875
train loss:  0.5311688184738159
train gradient:  0.16359047728157322
iteration : 7598
train acc:  0.71875
train loss:  0.5819603204727173
train gradient:  0.20906018198825116
iteration : 7599
train acc:  0.8046875
train loss:  0.4525773525238037
train gradient:  0.11174275257471943
iteration : 7600
train acc:  0.734375
train loss:  0.5536189079284668
train gradient:  0.1432089004445261
iteration : 7601
train acc:  0.7109375
train loss:  0.5157268047332764
train gradient:  0.21496026419449776
iteration : 7602
train acc:  0.734375
train loss:  0.49839258193969727
train gradient:  0.12859736838546787
iteration : 7603
train acc:  0.71875
train loss:  0.5013675093650818
train gradient:  0.14293697526397098
iteration : 7604
train acc:  0.6953125
train loss:  0.5394001007080078
train gradient:  0.13456937348278708
iteration : 7605
train acc:  0.7109375
train loss:  0.5231326818466187
train gradient:  0.1779485143223562
iteration : 7606
train acc:  0.734375
train loss:  0.46571770310401917
train gradient:  0.11399431964422936
iteration : 7607
train acc:  0.765625
train loss:  0.4622032642364502
train gradient:  0.11200109821509491
iteration : 7608
train acc:  0.75
train loss:  0.4935923218727112
train gradient:  0.12459026514599612
iteration : 7609
train acc:  0.78125
train loss:  0.4984487295150757
train gradient:  0.14707231498753748
iteration : 7610
train acc:  0.7734375
train loss:  0.4424375295639038
train gradient:  0.11783518222553789
iteration : 7611
train acc:  0.71875
train loss:  0.5098734498023987
train gradient:  0.15896110031833427
iteration : 7612
train acc:  0.796875
train loss:  0.45370954275131226
train gradient:  0.1425480259096441
iteration : 7613
train acc:  0.7578125
train loss:  0.48305702209472656
train gradient:  0.12351545181273815
iteration : 7614
train acc:  0.7109375
train loss:  0.5116668343544006
train gradient:  0.15016358838212984
iteration : 7615
train acc:  0.7265625
train loss:  0.5054188370704651
train gradient:  0.15582812470944324
iteration : 7616
train acc:  0.7421875
train loss:  0.5076715350151062
train gradient:  0.13974325331415388
iteration : 7617
train acc:  0.765625
train loss:  0.4651458263397217
train gradient:  0.09951604831782186
iteration : 7618
train acc:  0.75
train loss:  0.4646953344345093
train gradient:  0.15213637491420373
iteration : 7619
train acc:  0.7890625
train loss:  0.4946146309375763
train gradient:  0.13602582228807422
iteration : 7620
train acc:  0.734375
train loss:  0.46942758560180664
train gradient:  0.16697424814454148
iteration : 7621
train acc:  0.765625
train loss:  0.48357972502708435
train gradient:  0.13200145353064996
iteration : 7622
train acc:  0.765625
train loss:  0.4837108850479126
train gradient:  0.11857984208904924
iteration : 7623
train acc:  0.796875
train loss:  0.4396783113479614
train gradient:  0.09285443628286094
iteration : 7624
train acc:  0.6796875
train loss:  0.5743098258972168
train gradient:  0.16751308605090903
iteration : 7625
train acc:  0.7421875
train loss:  0.4727596640586853
train gradient:  0.15892970337867263
iteration : 7626
train acc:  0.7421875
train loss:  0.5090733766555786
train gradient:  0.11796821557403733
iteration : 7627
train acc:  0.734375
train loss:  0.5231143236160278
train gradient:  0.13945268335110012
iteration : 7628
train acc:  0.765625
train loss:  0.48125267028808594
train gradient:  0.11448938746584297
iteration : 7629
train acc:  0.7734375
train loss:  0.45976725220680237
train gradient:  0.14042544943775093
iteration : 7630
train acc:  0.765625
train loss:  0.43610095977783203
train gradient:  0.10635616907455803
iteration : 7631
train acc:  0.7890625
train loss:  0.45699620246887207
train gradient:  0.10234868882215414
iteration : 7632
train acc:  0.765625
train loss:  0.43354254961013794
train gradient:  0.08505438138094266
iteration : 7633
train acc:  0.765625
train loss:  0.4802497625350952
train gradient:  0.12573759708626914
iteration : 7634
train acc:  0.7265625
train loss:  0.4821769595146179
train gradient:  0.11559249885875987
iteration : 7635
train acc:  0.6953125
train loss:  0.5426996946334839
train gradient:  0.17268620552488512
iteration : 7636
train acc:  0.7734375
train loss:  0.502078652381897
train gradient:  0.09856352285006727
iteration : 7637
train acc:  0.671875
train loss:  0.5433835983276367
train gradient:  0.14229161723779288
iteration : 7638
train acc:  0.7578125
train loss:  0.5199674963951111
train gradient:  0.11318155036111267
iteration : 7639
train acc:  0.7578125
train loss:  0.45142874121665955
train gradient:  0.09936503340504127
iteration : 7640
train acc:  0.734375
train loss:  0.5011166334152222
train gradient:  0.1654077263482704
iteration : 7641
train acc:  0.7734375
train loss:  0.462153822183609
train gradient:  0.12927992121588475
iteration : 7642
train acc:  0.6875
train loss:  0.525156557559967
train gradient:  0.14708305534985544
iteration : 7643
train acc:  0.640625
train loss:  0.5871700048446655
train gradient:  0.17656731728347083
iteration : 7644
train acc:  0.7421875
train loss:  0.47904038429260254
train gradient:  0.11577576768966891
iteration : 7645
train acc:  0.75
train loss:  0.4553315043449402
train gradient:  0.1168470044103634
iteration : 7646
train acc:  0.7578125
train loss:  0.4446409344673157
train gradient:  0.11203689902215572
iteration : 7647
train acc:  0.71875
train loss:  0.5167918801307678
train gradient:  0.15223738225308636
iteration : 7648
train acc:  0.7421875
train loss:  0.5074232816696167
train gradient:  0.19365683525111377
iteration : 7649
train acc:  0.75
train loss:  0.5030467510223389
train gradient:  0.12773047135711502
iteration : 7650
train acc:  0.7421875
train loss:  0.4648284912109375
train gradient:  0.1239692655548349
iteration : 7651
train acc:  0.734375
train loss:  0.5007164478302002
train gradient:  0.1302795158910562
iteration : 7652
train acc:  0.7109375
train loss:  0.49406030774116516
train gradient:  0.13707827537104034
iteration : 7653
train acc:  0.7578125
train loss:  0.4580569267272949
train gradient:  0.12069179634396962
iteration : 7654
train acc:  0.640625
train loss:  0.5961388349533081
train gradient:  0.16880787157220511
iteration : 7655
train acc:  0.71875
train loss:  0.4792160391807556
train gradient:  0.11380525795381091
iteration : 7656
train acc:  0.7578125
train loss:  0.47280243039131165
train gradient:  0.13925332872840712
iteration : 7657
train acc:  0.7578125
train loss:  0.4764827489852905
train gradient:  0.10813711250283599
iteration : 7658
train acc:  0.6640625
train loss:  0.5523595809936523
train gradient:  0.18094856863222486
iteration : 7659
train acc:  0.703125
train loss:  0.5020291805267334
train gradient:  0.1403837373897312
iteration : 7660
train acc:  0.6796875
train loss:  0.5556550621986389
train gradient:  0.2269458398929064
iteration : 7661
train acc:  0.796875
train loss:  0.41609930992126465
train gradient:  0.09326688145847377
iteration : 7662
train acc:  0.71875
train loss:  0.5113562345504761
train gradient:  0.1150214254380259
iteration : 7663
train acc:  0.7578125
train loss:  0.4557560980319977
train gradient:  0.11274521658162101
iteration : 7664
train acc:  0.7578125
train loss:  0.4544404149055481
train gradient:  0.11269577809523479
iteration : 7665
train acc:  0.71875
train loss:  0.4941703677177429
train gradient:  0.13022806105282314
iteration : 7666
train acc:  0.703125
train loss:  0.4950833320617676
train gradient:  0.11733160507123708
iteration : 7667
train acc:  0.8125
train loss:  0.4273056983947754
train gradient:  0.09735616515790045
iteration : 7668
train acc:  0.7109375
train loss:  0.5253163576126099
train gradient:  0.1547853070907798
iteration : 7669
train acc:  0.7421875
train loss:  0.5148831605911255
train gradient:  0.12701314078172832
iteration : 7670
train acc:  0.7890625
train loss:  0.4291948974132538
train gradient:  0.09897623043895958
iteration : 7671
train acc:  0.7734375
train loss:  0.46566230058670044
train gradient:  0.11500935624757719
iteration : 7672
train acc:  0.6953125
train loss:  0.5298495292663574
train gradient:  0.19177459223465598
iteration : 7673
train acc:  0.7734375
train loss:  0.4481678009033203
train gradient:  0.1265865482195236
iteration : 7674
train acc:  0.71875
train loss:  0.4815348982810974
train gradient:  0.12088249125426938
iteration : 7675
train acc:  0.71875
train loss:  0.5251859426498413
train gradient:  0.1336003751500605
iteration : 7676
train acc:  0.703125
train loss:  0.5250964164733887
train gradient:  0.13476582267052228
iteration : 7677
train acc:  0.734375
train loss:  0.44832050800323486
train gradient:  0.13222648774533496
iteration : 7678
train acc:  0.796875
train loss:  0.4774838387966156
train gradient:  0.12267822495615772
iteration : 7679
train acc:  0.7421875
train loss:  0.5177873373031616
train gradient:  0.14862281008874623
iteration : 7680
train acc:  0.703125
train loss:  0.48925018310546875
train gradient:  0.1173769784043841
iteration : 7681
train acc:  0.7421875
train loss:  0.475503534078598
train gradient:  0.13208724396765825
iteration : 7682
train acc:  0.7265625
train loss:  0.48904022574424744
train gradient:  0.15790050766800529
iteration : 7683
train acc:  0.7734375
train loss:  0.503527820110321
train gradient:  0.15055851631913894
iteration : 7684
train acc:  0.78125
train loss:  0.4775503873825073
train gradient:  0.1591864846622385
iteration : 7685
train acc:  0.734375
train loss:  0.5053967833518982
train gradient:  0.1256119895728251
iteration : 7686
train acc:  0.6640625
train loss:  0.5686368346214294
train gradient:  0.23854068745974075
iteration : 7687
train acc:  0.703125
train loss:  0.5415029525756836
train gradient:  0.1666481482503055
iteration : 7688
train acc:  0.75
train loss:  0.509734034538269
train gradient:  0.16183709249971323
iteration : 7689
train acc:  0.78125
train loss:  0.40946289896965027
train gradient:  0.1001147902007195
iteration : 7690
train acc:  0.6796875
train loss:  0.5623398423194885
train gradient:  0.17946010868416484
iteration : 7691
train acc:  0.75
train loss:  0.46215003728866577
train gradient:  0.15444892404910343
iteration : 7692
train acc:  0.734375
train loss:  0.45023369789123535
train gradient:  0.09232515296925296
iteration : 7693
train acc:  0.7265625
train loss:  0.5370208621025085
train gradient:  0.16298294605039676
iteration : 7694
train acc:  0.7890625
train loss:  0.44104698300361633
train gradient:  0.10219244076608208
iteration : 7695
train acc:  0.8125
train loss:  0.44339519739151
train gradient:  0.10830868645075752
iteration : 7696
train acc:  0.7421875
train loss:  0.5059454441070557
train gradient:  0.11670599269979644
iteration : 7697
train acc:  0.7734375
train loss:  0.5004982948303223
train gradient:  0.15414901135749864
iteration : 7698
train acc:  0.703125
train loss:  0.548252522945404
train gradient:  0.15828536531509255
iteration : 7699
train acc:  0.7578125
train loss:  0.48743242025375366
train gradient:  0.149965027213013
iteration : 7700
train acc:  0.671875
train loss:  0.5214470028877258
train gradient:  0.20831763505748796
iteration : 7701
train acc:  0.7734375
train loss:  0.4858160614967346
train gradient:  0.13765543558986476
iteration : 7702
train acc:  0.6875
train loss:  0.6308659911155701
train gradient:  0.1886789940184899
iteration : 7703
train acc:  0.703125
train loss:  0.5011385679244995
train gradient:  0.13506453886455377
iteration : 7704
train acc:  0.7109375
train loss:  0.5253405570983887
train gradient:  0.1431900084658428
iteration : 7705
train acc:  0.7734375
train loss:  0.4951663911342621
train gradient:  0.13234304346583564
iteration : 7706
train acc:  0.7265625
train loss:  0.5320732593536377
train gradient:  0.17925291492813827
iteration : 7707
train acc:  0.734375
train loss:  0.48194968700408936
train gradient:  0.12358068051587663
iteration : 7708
train acc:  0.78125
train loss:  0.4678884744644165
train gradient:  0.10988329069229569
iteration : 7709
train acc:  0.7421875
train loss:  0.4659133553504944
train gradient:  0.09482836499528284
iteration : 7710
train acc:  0.7734375
train loss:  0.48623108863830566
train gradient:  0.13799551742913363
iteration : 7711
train acc:  0.75
train loss:  0.4831589162349701
train gradient:  0.10741041147329476
iteration : 7712
train acc:  0.7265625
train loss:  0.5019662380218506
train gradient:  0.12802728113220915
iteration : 7713
train acc:  0.71875
train loss:  0.5265582799911499
train gradient:  0.1651687183510761
iteration : 7714
train acc:  0.71875
train loss:  0.5308997631072998
train gradient:  0.1372394686726076
iteration : 7715
train acc:  0.7578125
train loss:  0.49712854623794556
train gradient:  0.12734456348068843
iteration : 7716
train acc:  0.71875
train loss:  0.5078901052474976
train gradient:  0.1300084206539832
iteration : 7717
train acc:  0.7421875
train loss:  0.45032745599746704
train gradient:  0.11419542595018121
iteration : 7718
train acc:  0.734375
train loss:  0.5358723998069763
train gradient:  0.13982375057292376
iteration : 7719
train acc:  0.7578125
train loss:  0.45527514815330505
train gradient:  0.11844282537539928
iteration : 7720
train acc:  0.7578125
train loss:  0.46851107478141785
train gradient:  0.11137440945394116
iteration : 7721
train acc:  0.765625
train loss:  0.4644550681114197
train gradient:  0.12986045747011732
iteration : 7722
train acc:  0.796875
train loss:  0.45026811957359314
train gradient:  0.12199676354387615
iteration : 7723
train acc:  0.765625
train loss:  0.48575109243392944
train gradient:  0.11573848264661678
iteration : 7724
train acc:  0.828125
train loss:  0.41775238513946533
train gradient:  0.10645958044222
iteration : 7725
train acc:  0.7421875
train loss:  0.49166542291641235
train gradient:  0.14120810078642357
iteration : 7726
train acc:  0.7890625
train loss:  0.4723234176635742
train gradient:  0.11201329200543177
iteration : 7727
train acc:  0.7421875
train loss:  0.4884765148162842
train gradient:  0.15240923127282066
iteration : 7728
train acc:  0.7421875
train loss:  0.5346754789352417
train gradient:  0.1495696910109935
iteration : 7729
train acc:  0.734375
train loss:  0.5302722454071045
train gradient:  0.17819724442944043
iteration : 7730
train acc:  0.71875
train loss:  0.522915244102478
train gradient:  0.16705157277387012
iteration : 7731
train acc:  0.7421875
train loss:  0.48024386167526245
train gradient:  0.13036857400346621
iteration : 7732
train acc:  0.7578125
train loss:  0.46603190898895264
train gradient:  0.14328678842564735
iteration : 7733
train acc:  0.7734375
train loss:  0.4672732651233673
train gradient:  0.11835657294774386
iteration : 7734
train acc:  0.765625
train loss:  0.4317467510700226
train gradient:  0.11848495117838055
iteration : 7735
train acc:  0.7421875
train loss:  0.46275076270103455
train gradient:  0.12545374590972236
iteration : 7736
train acc:  0.65625
train loss:  0.5530181527137756
train gradient:  0.18561160576979338
iteration : 7737
train acc:  0.78125
train loss:  0.4781262278556824
train gradient:  0.20567604205436568
iteration : 7738
train acc:  0.6875
train loss:  0.6011446714401245
train gradient:  0.19141785567518343
iteration : 7739
train acc:  0.7265625
train loss:  0.5078786611557007
train gradient:  0.17698251458068603
iteration : 7740
train acc:  0.7421875
train loss:  0.5124542117118835
train gradient:  0.1745479827395473
iteration : 7741
train acc:  0.7109375
train loss:  0.5612716674804688
train gradient:  0.22804964723919238
iteration : 7742
train acc:  0.78125
train loss:  0.5033012628555298
train gradient:  0.1356485355640823
iteration : 7743
train acc:  0.765625
train loss:  0.43272143602371216
train gradient:  0.13345920511126658
iteration : 7744
train acc:  0.7109375
train loss:  0.5019630193710327
train gradient:  0.1318292867791642
iteration : 7745
train acc:  0.7578125
train loss:  0.43629831075668335
train gradient:  0.10171213096023782
iteration : 7746
train acc:  0.7890625
train loss:  0.44399505853652954
train gradient:  0.11896793095170229
iteration : 7747
train acc:  0.703125
train loss:  0.5554510354995728
train gradient:  0.1380734772745154
iteration : 7748
train acc:  0.734375
train loss:  0.5070623159408569
train gradient:  0.15056491320553317
iteration : 7749
train acc:  0.734375
train loss:  0.511705756187439
train gradient:  0.1466488163152302
iteration : 7750
train acc:  0.6953125
train loss:  0.5167539715766907
train gradient:  0.14357655972809275
iteration : 7751
train acc:  0.7421875
train loss:  0.5031090974807739
train gradient:  0.1466415128892607
iteration : 7752
train acc:  0.7734375
train loss:  0.4622085690498352
train gradient:  0.1266625760714204
iteration : 7753
train acc:  0.6953125
train loss:  0.49872544407844543
train gradient:  0.12025328677783231
iteration : 7754
train acc:  0.765625
train loss:  0.47241806983947754
train gradient:  0.106327653766271
iteration : 7755
train acc:  0.6875
train loss:  0.5346910953521729
train gradient:  0.15945393129326407
iteration : 7756
train acc:  0.6796875
train loss:  0.610738217830658
train gradient:  0.2116851698486658
iteration : 7757
train acc:  0.7578125
train loss:  0.47938793897628784
train gradient:  0.13933600108877603
iteration : 7758
train acc:  0.65625
train loss:  0.6085464954376221
train gradient:  0.17314857965546948
iteration : 7759
train acc:  0.7734375
train loss:  0.46288758516311646
train gradient:  0.1145986896377458
iteration : 7760
train acc:  0.6953125
train loss:  0.5052468180656433
train gradient:  0.1224319765807448
iteration : 7761
train acc:  0.7421875
train loss:  0.4836682677268982
train gradient:  0.13327858425623318
iteration : 7762
train acc:  0.7734375
train loss:  0.5327236652374268
train gradient:  0.173387576278062
iteration : 7763
train acc:  0.8125
train loss:  0.45041823387145996
train gradient:  0.14912612461647817
iteration : 7764
train acc:  0.765625
train loss:  0.4916831851005554
train gradient:  0.17087557170635664
iteration : 7765
train acc:  0.6953125
train loss:  0.562899112701416
train gradient:  0.1868373537503913
iteration : 7766
train acc:  0.734375
train loss:  0.4776400327682495
train gradient:  0.11604741913687308
iteration : 7767
train acc:  0.734375
train loss:  0.522519588470459
train gradient:  0.14957679956202252
iteration : 7768
train acc:  0.71875
train loss:  0.526725172996521
train gradient:  0.1512252870992368
iteration : 7769
train acc:  0.703125
train loss:  0.5887190103530884
train gradient:  0.22025245999086385
iteration : 7770
train acc:  0.75
train loss:  0.5294594764709473
train gradient:  0.18140019409364483
iteration : 7771
train acc:  0.75
train loss:  0.45083311200141907
train gradient:  0.11250566690408514
iteration : 7772
train acc:  0.84375
train loss:  0.38242053985595703
train gradient:  0.1096408432586675
iteration : 7773
train acc:  0.734375
train loss:  0.5102847814559937
train gradient:  0.21408637228763255
iteration : 7774
train acc:  0.7578125
train loss:  0.4765004813671112
train gradient:  0.17611868940186906
iteration : 7775
train acc:  0.796875
train loss:  0.4496240019798279
train gradient:  0.19891572347430944
iteration : 7776
train acc:  0.765625
train loss:  0.47593799233436584
train gradient:  0.13355866359139495
iteration : 7777
train acc:  0.7578125
train loss:  0.517482578754425
train gradient:  0.13970340583229923
iteration : 7778
train acc:  0.734375
train loss:  0.5476730465888977
train gradient:  0.1567981198956509
iteration : 7779
train acc:  0.734375
train loss:  0.4888123869895935
train gradient:  0.13921115259353345
iteration : 7780
train acc:  0.7109375
train loss:  0.4861057996749878
train gradient:  0.13096629501277876
iteration : 7781
train acc:  0.765625
train loss:  0.4754623770713806
train gradient:  0.17921022696660915
iteration : 7782
train acc:  0.71875
train loss:  0.5499444007873535
train gradient:  0.15700007549998402
iteration : 7783
train acc:  0.75
train loss:  0.45740535855293274
train gradient:  0.11366647709559066
iteration : 7784
train acc:  0.7109375
train loss:  0.5316106081008911
train gradient:  0.16736286725064786
iteration : 7785
train acc:  0.7578125
train loss:  0.46551865339279175
train gradient:  0.12638427811915454
iteration : 7786
train acc:  0.7265625
train loss:  0.4913157820701599
train gradient:  0.13432705298020986
iteration : 7787
train acc:  0.796875
train loss:  0.43425190448760986
train gradient:  0.10657764992065266
iteration : 7788
train acc:  0.6953125
train loss:  0.5285162925720215
train gradient:  0.11520005727196357
iteration : 7789
train acc:  0.7265625
train loss:  0.5529768466949463
train gradient:  0.1785084726954892
iteration : 7790
train acc:  0.7265625
train loss:  0.5162678956985474
train gradient:  0.17278674004905914
iteration : 7791
train acc:  0.7265625
train loss:  0.5164393186569214
train gradient:  0.1367207670540104
iteration : 7792
train acc:  0.6796875
train loss:  0.5414823293685913
train gradient:  0.18338899977110912
iteration : 7793
train acc:  0.75
train loss:  0.49221962690353394
train gradient:  0.1247046957967744
iteration : 7794
train acc:  0.765625
train loss:  0.43302732706069946
train gradient:  0.1788276995736487
iteration : 7795
train acc:  0.7890625
train loss:  0.4463341534137726
train gradient:  0.12446464516860319
iteration : 7796
train acc:  0.703125
train loss:  0.5946207046508789
train gradient:  0.2099594468249165
iteration : 7797
train acc:  0.6796875
train loss:  0.4832600951194763
train gradient:  0.1624033490814008
iteration : 7798
train acc:  0.796875
train loss:  0.43445366621017456
train gradient:  0.11562104444829618
iteration : 7799
train acc:  0.7109375
train loss:  0.537351131439209
train gradient:  0.14055626242664315
iteration : 7800
train acc:  0.6796875
train loss:  0.5786486268043518
train gradient:  0.18634133043586476
iteration : 7801
train acc:  0.6953125
train loss:  0.5562393069267273
train gradient:  0.16660801873233233
iteration : 7802
train acc:  0.6640625
train loss:  0.5739750862121582
train gradient:  0.1519658845624652
iteration : 7803
train acc:  0.6796875
train loss:  0.5649542212486267
train gradient:  0.17262119375549245
iteration : 7804
train acc:  0.703125
train loss:  0.5703370571136475
train gradient:  0.17502841765445948
iteration : 7805
train acc:  0.7109375
train loss:  0.5265093445777893
train gradient:  0.1243462060311967
iteration : 7806
train acc:  0.796875
train loss:  0.45489901304244995
train gradient:  0.10470367872385296
iteration : 7807
train acc:  0.7421875
train loss:  0.5037505626678467
train gradient:  0.15213342571135638
iteration : 7808
train acc:  0.765625
train loss:  0.47149717807769775
train gradient:  0.11613249934542215
iteration : 7809
train acc:  0.7734375
train loss:  0.5094150304794312
train gradient:  0.11079770576331119
iteration : 7810
train acc:  0.703125
train loss:  0.49438413977622986
train gradient:  0.14626079282893112
iteration : 7811
train acc:  0.7265625
train loss:  0.5396124124526978
train gradient:  0.1577416729631091
iteration : 7812
train acc:  0.671875
train loss:  0.5423481464385986
train gradient:  0.13782708735156535
iteration : 7813
train acc:  0.7734375
train loss:  0.5081796646118164
train gradient:  0.13113336571845624
iteration : 7814
train acc:  0.7734375
train loss:  0.4615733325481415
train gradient:  0.10068937691204649
iteration : 7815
train acc:  0.734375
train loss:  0.5095361471176147
train gradient:  0.13391294539554738
iteration : 7816
train acc:  0.6796875
train loss:  0.5261968970298767
train gradient:  0.13271176181773164
iteration : 7817
train acc:  0.78125
train loss:  0.4423847198486328
train gradient:  0.10105160005421765
iteration : 7818
train acc:  0.7265625
train loss:  0.5044755935668945
train gradient:  0.1736436620494557
iteration : 7819
train acc:  0.734375
train loss:  0.5021024942398071
train gradient:  0.1284097162573825
iteration : 7820
train acc:  0.75
train loss:  0.4854293763637543
train gradient:  0.13741272837105734
iteration : 7821
train acc:  0.7890625
train loss:  0.51936936378479
train gradient:  0.16586429064825736
iteration : 7822
train acc:  0.6796875
train loss:  0.5310558080673218
train gradient:  0.1544027288046185
iteration : 7823
train acc:  0.75
train loss:  0.5230202674865723
train gradient:  0.22353404471569632
iteration : 7824
train acc:  0.71875
train loss:  0.48807626962661743
train gradient:  0.1372168028169708
iteration : 7825
train acc:  0.78125
train loss:  0.5121884942054749
train gradient:  0.12407021517647646
iteration : 7826
train acc:  0.625
train loss:  0.6486905813217163
train gradient:  0.1944720575970249
iteration : 7827
train acc:  0.7734375
train loss:  0.5084480047225952
train gradient:  0.1319282956445007
iteration : 7828
train acc:  0.765625
train loss:  0.4335927367210388
train gradient:  0.09596490562816411
iteration : 7829
train acc:  0.6875
train loss:  0.593320369720459
train gradient:  0.21217950598859758
iteration : 7830
train acc:  0.7421875
train loss:  0.488804429769516
train gradient:  0.20697797820451708
iteration : 7831
train acc:  0.7109375
train loss:  0.5231266021728516
train gradient:  0.15270841010285094
iteration : 7832
train acc:  0.765625
train loss:  0.5246180295944214
train gradient:  0.12838823099436328
iteration : 7833
train acc:  0.765625
train loss:  0.5143639445304871
train gradient:  0.13657193630126802
iteration : 7834
train acc:  0.6953125
train loss:  0.49205419421195984
train gradient:  0.13071046873221606
iteration : 7835
train acc:  0.703125
train loss:  0.5093466639518738
train gradient:  0.12200382921549409
iteration : 7836
train acc:  0.7890625
train loss:  0.4329949915409088
train gradient:  0.0982543198856751
iteration : 7837
train acc:  0.78125
train loss:  0.4768555164337158
train gradient:  0.09675728450974334
iteration : 7838
train acc:  0.7421875
train loss:  0.5314772129058838
train gradient:  0.15333083891785415
iteration : 7839
train acc:  0.78125
train loss:  0.45857441425323486
train gradient:  0.13026751304839795
iteration : 7840
train acc:  0.734375
train loss:  0.568461000919342
train gradient:  0.14179991556616756
iteration : 7841
train acc:  0.7421875
train loss:  0.5415896773338318
train gradient:  0.1347502049046822
iteration : 7842
train acc:  0.7265625
train loss:  0.5622957944869995
train gradient:  0.1407380604140568
iteration : 7843
train acc:  0.7578125
train loss:  0.4948732554912567
train gradient:  0.1170852984349712
iteration : 7844
train acc:  0.7578125
train loss:  0.46076658368110657
train gradient:  0.0997708585219089
iteration : 7845
train acc:  0.6640625
train loss:  0.5116748213768005
train gradient:  0.1656803045520016
iteration : 7846
train acc:  0.796875
train loss:  0.5038978457450867
train gradient:  0.14735804996750723
iteration : 7847
train acc:  0.765625
train loss:  0.5018708109855652
train gradient:  0.1129164172309987
iteration : 7848
train acc:  0.7890625
train loss:  0.4273144006729126
train gradient:  0.09783999584327058
iteration : 7849
train acc:  0.71875
train loss:  0.4932420253753662
train gradient:  0.11088153668580693
iteration : 7850
train acc:  0.796875
train loss:  0.45715153217315674
train gradient:  0.10355238686439182
iteration : 7851
train acc:  0.71875
train loss:  0.5101837515830994
train gradient:  0.14501763787363733
iteration : 7852
train acc:  0.7265625
train loss:  0.5322080850601196
train gradient:  0.148998499590747
iteration : 7853
train acc:  0.78125
train loss:  0.47944170236587524
train gradient:  0.1314932124012257
iteration : 7854
train acc:  0.734375
train loss:  0.47185245156288147
train gradient:  0.12540773216032405
iteration : 7855
train acc:  0.7265625
train loss:  0.5163356065750122
train gradient:  0.14959057830208922
iteration : 7856
train acc:  0.765625
train loss:  0.4898846745491028
train gradient:  0.1414231806753775
iteration : 7857
train acc:  0.7578125
train loss:  0.5363293886184692
train gradient:  0.16500108076742248
iteration : 7858
train acc:  0.6953125
train loss:  0.5362134575843811
train gradient:  0.13140405951623577
iteration : 7859
train acc:  0.7421875
train loss:  0.4729287326335907
train gradient:  0.11551652698581133
iteration : 7860
train acc:  0.6640625
train loss:  0.5479713678359985
train gradient:  0.14798916076345142
iteration : 7861
train acc:  0.75
train loss:  0.4811781644821167
train gradient:  0.16967203543509662
iteration : 7862
train acc:  0.7265625
train loss:  0.4880240261554718
train gradient:  0.1109878613598001
iteration : 7863
train acc:  0.8203125
train loss:  0.43315860629081726
train gradient:  0.09925392875088551
iteration : 7864
train acc:  0.71875
train loss:  0.511559247970581
train gradient:  0.12747002157434426
iteration : 7865
train acc:  0.7421875
train loss:  0.5497353076934814
train gradient:  0.14159839211739805
iteration : 7866
train acc:  0.765625
train loss:  0.5072141885757446
train gradient:  0.12367120433466991
iteration : 7867
train acc:  0.7578125
train loss:  0.5282796621322632
train gradient:  0.1440676171449233
iteration : 7868
train acc:  0.7734375
train loss:  0.45505207777023315
train gradient:  0.0975049486970954
iteration : 7869
train acc:  0.7421875
train loss:  0.50902259349823
train gradient:  0.18115601267045928
iteration : 7870
train acc:  0.7734375
train loss:  0.42528364062309265
train gradient:  0.10259114572752895
iteration : 7871
train acc:  0.7421875
train loss:  0.5018454194068909
train gradient:  0.15403051160618725
iteration : 7872
train acc:  0.671875
train loss:  0.5865930914878845
train gradient:  0.1862532838287259
iteration : 7873
train acc:  0.671875
train loss:  0.5393192768096924
train gradient:  0.14283456299176375
iteration : 7874
train acc:  0.734375
train loss:  0.4664067029953003
train gradient:  0.12213355681186866
iteration : 7875
train acc:  0.6328125
train loss:  0.5460174083709717
train gradient:  0.15740152036582503
iteration : 7876
train acc:  0.7578125
train loss:  0.44124579429626465
train gradient:  0.10686706426626533
iteration : 7877
train acc:  0.7421875
train loss:  0.45860379934310913
train gradient:  0.11060433033703133
iteration : 7878
train acc:  0.75
train loss:  0.5077805519104004
train gradient:  0.16353952607517025
iteration : 7879
train acc:  0.78125
train loss:  0.4595852792263031
train gradient:  0.11398325840290764
iteration : 7880
train acc:  0.7578125
train loss:  0.4577883780002594
train gradient:  0.12025634107439127
iteration : 7881
train acc:  0.7421875
train loss:  0.48463407158851624
train gradient:  0.1256177556812551
iteration : 7882
train acc:  0.7734375
train loss:  0.45654743909835815
train gradient:  0.11471992268993571
iteration : 7883
train acc:  0.671875
train loss:  0.581206202507019
train gradient:  0.15223530528251683
iteration : 7884
train acc:  0.734375
train loss:  0.48458966612815857
train gradient:  0.13618179929252652
iteration : 7885
train acc:  0.7421875
train loss:  0.4898265302181244
train gradient:  0.11535260853543668
iteration : 7886
train acc:  0.7109375
train loss:  0.5335469245910645
train gradient:  0.1553669309599422
iteration : 7887
train acc:  0.75
train loss:  0.502446711063385
train gradient:  0.17607122224754546
iteration : 7888
train acc:  0.796875
train loss:  0.463341623544693
train gradient:  0.12051305833994415
iteration : 7889
train acc:  0.734375
train loss:  0.4878630042076111
train gradient:  0.13316085596501276
iteration : 7890
train acc:  0.78125
train loss:  0.47066473960876465
train gradient:  0.11763060247720081
iteration : 7891
train acc:  0.7578125
train loss:  0.5031147003173828
train gradient:  0.14966505003994507
iteration : 7892
train acc:  0.78125
train loss:  0.5394868850708008
train gradient:  0.13452707726960608
iteration : 7893
train acc:  0.765625
train loss:  0.4716840088367462
train gradient:  0.10095922671967152
iteration : 7894
train acc:  0.6875
train loss:  0.5795884728431702
train gradient:  0.14982306706340093
iteration : 7895
train acc:  0.734375
train loss:  0.5314667224884033
train gradient:  0.14247941588051644
iteration : 7896
train acc:  0.796875
train loss:  0.4893513321876526
train gradient:  0.10979286138089299
iteration : 7897
train acc:  0.734375
train loss:  0.48727113008499146
train gradient:  0.1232925131097321
iteration : 7898
train acc:  0.6640625
train loss:  0.5512598752975464
train gradient:  0.1734957641503661
iteration : 7899
train acc:  0.6796875
train loss:  0.5796711444854736
train gradient:  0.17973704522515732
iteration : 7900
train acc:  0.7109375
train loss:  0.5625334978103638
train gradient:  0.14910788550895054
iteration : 7901
train acc:  0.7734375
train loss:  0.47898411750793457
train gradient:  0.13951927582760162
iteration : 7902
train acc:  0.7421875
train loss:  0.48588597774505615
train gradient:  0.12948069109983432
iteration : 7903
train acc:  0.7421875
train loss:  0.46143513917922974
train gradient:  0.09952879996225245
iteration : 7904
train acc:  0.734375
train loss:  0.4829998016357422
train gradient:  0.11324404746489038
iteration : 7905
train acc:  0.75
train loss:  0.4548550844192505
train gradient:  0.0950155089675454
iteration : 7906
train acc:  0.6875
train loss:  0.537757933139801
train gradient:  0.15770679214782019
iteration : 7907
train acc:  0.796875
train loss:  0.4785894751548767
train gradient:  0.13724473683507743
iteration : 7908
train acc:  0.703125
train loss:  0.5121000409126282
train gradient:  0.1512886891951176
iteration : 7909
train acc:  0.7421875
train loss:  0.5008496642112732
train gradient:  0.13071437596770846
iteration : 7910
train acc:  0.7734375
train loss:  0.4864146113395691
train gradient:  0.1091778890343218
iteration : 7911
train acc:  0.7421875
train loss:  0.478759229183197
train gradient:  0.14278020407017167
iteration : 7912
train acc:  0.7734375
train loss:  0.4463444650173187
train gradient:  0.13548479415013848
iteration : 7913
train acc:  0.734375
train loss:  0.4957002103328705
train gradient:  0.13413320180192098
iteration : 7914
train acc:  0.75
train loss:  0.4969969391822815
train gradient:  0.11488205324240401
iteration : 7915
train acc:  0.75
train loss:  0.50897216796875
train gradient:  0.13400594789536485
iteration : 7916
train acc:  0.7734375
train loss:  0.47008413076400757
train gradient:  0.12913525844531237
iteration : 7917
train acc:  0.75
train loss:  0.5020835995674133
train gradient:  0.1300657339938247
iteration : 7918
train acc:  0.7890625
train loss:  0.4859415888786316
train gradient:  0.1297304966581927
iteration : 7919
train acc:  0.7734375
train loss:  0.5004350543022156
train gradient:  0.12808133081328468
iteration : 7920
train acc:  0.7265625
train loss:  0.4859175682067871
train gradient:  0.13569296203440492
iteration : 7921
train acc:  0.7421875
train loss:  0.5093153715133667
train gradient:  0.188874178894679
iteration : 7922
train acc:  0.765625
train loss:  0.4903927445411682
train gradient:  0.15809028404021355
iteration : 7923
train acc:  0.7265625
train loss:  0.5040238499641418
train gradient:  0.15540825913646775
iteration : 7924
train acc:  0.6796875
train loss:  0.6276410818099976
train gradient:  0.19779345457106653
iteration : 7925
train acc:  0.7578125
train loss:  0.5322948694229126
train gradient:  0.16037032874289273
iteration : 7926
train acc:  0.7421875
train loss:  0.47478175163269043
train gradient:  0.13422215067471796
iteration : 7927
train acc:  0.6875
train loss:  0.5774598121643066
train gradient:  0.16752034656832215
iteration : 7928
train acc:  0.7890625
train loss:  0.46127066016197205
train gradient:  0.13049096006091737
iteration : 7929
train acc:  0.75
train loss:  0.4744492173194885
train gradient:  0.12558138856580903
iteration : 7930
train acc:  0.765625
train loss:  0.50848788022995
train gradient:  0.13947994739899816
iteration : 7931
train acc:  0.7265625
train loss:  0.5679165124893188
train gradient:  0.15771658092554486
iteration : 7932
train acc:  0.7734375
train loss:  0.4894181787967682
train gradient:  0.11379591441550159
iteration : 7933
train acc:  0.78125
train loss:  0.4609547257423401
train gradient:  0.10044876523625468
iteration : 7934
train acc:  0.75
train loss:  0.5028623342514038
train gradient:  0.13785634126016097
iteration : 7935
train acc:  0.625
train loss:  0.5592837333679199
train gradient:  0.1606116787791429
iteration : 7936
train acc:  0.7578125
train loss:  0.49572303891181946
train gradient:  0.1860747065537775
iteration : 7937
train acc:  0.71875
train loss:  0.5199539065361023
train gradient:  0.1281206515852363
iteration : 7938
train acc:  0.84375
train loss:  0.41641396284103394
train gradient:  0.1108441463853743
iteration : 7939
train acc:  0.78125
train loss:  0.43716928362846375
train gradient:  0.10515040710192344
iteration : 7940
train acc:  0.7734375
train loss:  0.47299912571907043
train gradient:  0.11169935085633735
iteration : 7941
train acc:  0.734375
train loss:  0.47315484285354614
train gradient:  0.11654420322405086
iteration : 7942
train acc:  0.703125
train loss:  0.5710372924804688
train gradient:  0.18360115011943132
iteration : 7943
train acc:  0.7890625
train loss:  0.4114704728126526
train gradient:  0.09104286110709336
iteration : 7944
train acc:  0.7890625
train loss:  0.4518249034881592
train gradient:  0.09156116033142207
iteration : 7945
train acc:  0.6953125
train loss:  0.5032655000686646
train gradient:  0.15284792729579988
iteration : 7946
train acc:  0.75
train loss:  0.5354350805282593
train gradient:  0.17109484062224073
iteration : 7947
train acc:  0.75
train loss:  0.48596733808517456
train gradient:  0.11261063922210861
iteration : 7948
train acc:  0.6640625
train loss:  0.6002446413040161
train gradient:  0.19148828172298044
iteration : 7949
train acc:  0.703125
train loss:  0.5844075679779053
train gradient:  0.19043428210663693
iteration : 7950
train acc:  0.671875
train loss:  0.5797289609909058
train gradient:  0.16591720295735254
iteration : 7951
train acc:  0.7109375
train loss:  0.4899027943611145
train gradient:  0.13652145490689827
iteration : 7952
train acc:  0.6328125
train loss:  0.5610665678977966
train gradient:  0.160481077130934
iteration : 7953
train acc:  0.7734375
train loss:  0.4708096385002136
train gradient:  0.09579006105190062
iteration : 7954
train acc:  0.671875
train loss:  0.5292906761169434
train gradient:  0.128538260185946
iteration : 7955
train acc:  0.7109375
train loss:  0.512790322303772
train gradient:  0.1395668598230682
iteration : 7956
train acc:  0.6875
train loss:  0.5240955352783203
train gradient:  0.1304205809306177
iteration : 7957
train acc:  0.7578125
train loss:  0.4733614921569824
train gradient:  0.11993492822700999
iteration : 7958
train acc:  0.6796875
train loss:  0.5462888479232788
train gradient:  0.15880231644695447
iteration : 7959
train acc:  0.765625
train loss:  0.5183531045913696
train gradient:  0.1108128678476021
iteration : 7960
train acc:  0.65625
train loss:  0.5301767587661743
train gradient:  0.1313596009578793
iteration : 7961
train acc:  0.7890625
train loss:  0.4582619369029999
train gradient:  0.11583500098726734
iteration : 7962
train acc:  0.7265625
train loss:  0.49942341446876526
train gradient:  0.13215373604862857
iteration : 7963
train acc:  0.7890625
train loss:  0.4207870662212372
train gradient:  0.08486711347363922
iteration : 7964
train acc:  0.8203125
train loss:  0.40367114543914795
train gradient:  0.09091931785813849
iteration : 7965
train acc:  0.796875
train loss:  0.4392163157463074
train gradient:  0.11310712341571601
iteration : 7966
train acc:  0.734375
train loss:  0.46927088499069214
train gradient:  0.12332366873773655
iteration : 7967
train acc:  0.7578125
train loss:  0.4529222846031189
train gradient:  0.08942569544771652
iteration : 7968
train acc:  0.75
train loss:  0.4919061064720154
train gradient:  0.11802527253902027
iteration : 7969
train acc:  0.7734375
train loss:  0.4621792435646057
train gradient:  0.10606893098343335
iteration : 7970
train acc:  0.7578125
train loss:  0.4671637713909149
train gradient:  0.12498904480516952
iteration : 7971
train acc:  0.7421875
train loss:  0.5214233994483948
train gradient:  0.13245550499787745
iteration : 7972
train acc:  0.7109375
train loss:  0.5189757943153381
train gradient:  0.11493299967642555
iteration : 7973
train acc:  0.71875
train loss:  0.5673178434371948
train gradient:  0.1908896476203824
iteration : 7974
train acc:  0.7109375
train loss:  0.5762282609939575
train gradient:  0.1700823368746847
iteration : 7975
train acc:  0.703125
train loss:  0.5304635763168335
train gradient:  0.1659360100726595
iteration : 7976
train acc:  0.7890625
train loss:  0.4913296103477478
train gradient:  0.13412177895381877
iteration : 7977
train acc:  0.7109375
train loss:  0.538195788860321
train gradient:  0.11459252227298508
iteration : 7978
train acc:  0.7421875
train loss:  0.4593827426433563
train gradient:  0.09777091348461342
iteration : 7979
train acc:  0.7109375
train loss:  0.5158005356788635
train gradient:  0.14679519117052753
iteration : 7980
train acc:  0.703125
train loss:  0.500879168510437
train gradient:  0.1268679646009932
iteration : 7981
train acc:  0.734375
train loss:  0.5356500148773193
train gradient:  0.1300594501104011
iteration : 7982
train acc:  0.8125
train loss:  0.4794785976409912
train gradient:  0.12914789565560186
iteration : 7983
train acc:  0.7578125
train loss:  0.4767649173736572
train gradient:  0.16366629269745786
iteration : 7984
train acc:  0.7890625
train loss:  0.4474264085292816
train gradient:  0.09305143600868826
iteration : 7985
train acc:  0.703125
train loss:  0.49683424830436707
train gradient:  0.14599016894891664
iteration : 7986
train acc:  0.71875
train loss:  0.5401185154914856
train gradient:  0.1556618450325386
iteration : 7987
train acc:  0.6953125
train loss:  0.5599513053894043
train gradient:  0.1409775152414986
iteration : 7988
train acc:  0.7734375
train loss:  0.49189290404319763
train gradient:  0.12147997675435368
iteration : 7989
train acc:  0.7421875
train loss:  0.5394706130027771
train gradient:  0.1275310757514797
iteration : 7990
train acc:  0.75
train loss:  0.44488829374313354
train gradient:  0.10627022622639648
iteration : 7991
train acc:  0.7421875
train loss:  0.4816260039806366
train gradient:  0.11240815678996767
iteration : 7992
train acc:  0.71875
train loss:  0.5034927129745483
train gradient:  0.12199328249016242
iteration : 7993
train acc:  0.7265625
train loss:  0.4817346930503845
train gradient:  0.09658567734091861
iteration : 7994
train acc:  0.7265625
train loss:  0.5409235954284668
train gradient:  0.1610070177906337
iteration : 7995
train acc:  0.7578125
train loss:  0.46849146485328674
train gradient:  0.08617429122807094
iteration : 7996
train acc:  0.734375
train loss:  0.5312677621841431
train gradient:  0.16506726932026122
iteration : 7997
train acc:  0.71875
train loss:  0.4851016402244568
train gradient:  0.11518638701226551
iteration : 7998
train acc:  0.796875
train loss:  0.4982551634311676
train gradient:  0.10911302467417688
iteration : 7999
train acc:  0.75
train loss:  0.4850451350212097
train gradient:  0.1067498858306916
iteration : 8000
train acc:  0.7265625
train loss:  0.5643901824951172
train gradient:  0.1805670407851378
iteration : 8001
train acc:  0.7265625
train loss:  0.4896713197231293
train gradient:  0.13026026043641695
iteration : 8002
train acc:  0.7265625
train loss:  0.511277973651886
train gradient:  0.13496629336436333
iteration : 8003
train acc:  0.7109375
train loss:  0.5059632658958435
train gradient:  0.15461135911482313
iteration : 8004
train acc:  0.7109375
train loss:  0.5258587598800659
train gradient:  0.1219749766977228
iteration : 8005
train acc:  0.703125
train loss:  0.5243174433708191
train gradient:  0.19429835257205813
iteration : 8006
train acc:  0.734375
train loss:  0.4832373857498169
train gradient:  0.11568993759443186
iteration : 8007
train acc:  0.765625
train loss:  0.4716784358024597
train gradient:  0.12114331684857406
iteration : 8008
train acc:  0.78125
train loss:  0.48504310846328735
train gradient:  0.1449811955814394
iteration : 8009
train acc:  0.7265625
train loss:  0.49496030807495117
train gradient:  0.12503895320144054
iteration : 8010
train acc:  0.671875
train loss:  0.5127221345901489
train gradient:  0.2068795049356013
iteration : 8011
train acc:  0.75
train loss:  0.46769827604293823
train gradient:  0.15771607295362294
iteration : 8012
train acc:  0.703125
train loss:  0.5842623114585876
train gradient:  0.20658675068371232
iteration : 8013
train acc:  0.7421875
train loss:  0.4814545214176178
train gradient:  0.1313681918585355
iteration : 8014
train acc:  0.6953125
train loss:  0.4961867928504944
train gradient:  0.11254781413366842
iteration : 8015
train acc:  0.7734375
train loss:  0.4460117220878601
train gradient:  0.09705538500866472
iteration : 8016
train acc:  0.75
train loss:  0.49698445200920105
train gradient:  0.11710302507507851
iteration : 8017
train acc:  0.734375
train loss:  0.5689481496810913
train gradient:  0.18111846484066213
iteration : 8018
train acc:  0.75
train loss:  0.49202674627304077
train gradient:  0.1685320482292475
iteration : 8019
train acc:  0.7421875
train loss:  0.516292154788971
train gradient:  0.12485470700874368
iteration : 8020
train acc:  0.71875
train loss:  0.5235239267349243
train gradient:  0.16174735622081654
iteration : 8021
train acc:  0.78125
train loss:  0.5161232948303223
train gradient:  0.15210342451851536
iteration : 8022
train acc:  0.78125
train loss:  0.4929640293121338
train gradient:  0.12035042489067989
iteration : 8023
train acc:  0.796875
train loss:  0.46422091126441956
train gradient:  0.08933012468135801
iteration : 8024
train acc:  0.765625
train loss:  0.4833328425884247
train gradient:  0.11036848279907853
iteration : 8025
train acc:  0.78125
train loss:  0.45830756425857544
train gradient:  0.12931601479937693
iteration : 8026
train acc:  0.78125
train loss:  0.436488538980484
train gradient:  0.10234597787832626
iteration : 8027
train acc:  0.7109375
train loss:  0.5462415218353271
train gradient:  0.13151890660609208
iteration : 8028
train acc:  0.734375
train loss:  0.4972831606864929
train gradient:  0.11660296753460808
iteration : 8029
train acc:  0.765625
train loss:  0.4652397930622101
train gradient:  0.11367121373335136
iteration : 8030
train acc:  0.6875
train loss:  0.5538712739944458
train gradient:  0.14690926126487747
iteration : 8031
train acc:  0.765625
train loss:  0.45442378520965576
train gradient:  0.11279899566496913
iteration : 8032
train acc:  0.78125
train loss:  0.5215227007865906
train gradient:  0.15283932310893256
iteration : 8033
train acc:  0.7890625
train loss:  0.4365217089653015
train gradient:  0.10817468501589436
iteration : 8034
train acc:  0.7109375
train loss:  0.5367403030395508
train gradient:  0.14360278753039907
iteration : 8035
train acc:  0.734375
train loss:  0.507612943649292
train gradient:  0.11154653175788971
iteration : 8036
train acc:  0.765625
train loss:  0.4969368278980255
train gradient:  0.1501678811420984
iteration : 8037
train acc:  0.8046875
train loss:  0.4100860357284546
train gradient:  0.08868139392291746
iteration : 8038
train acc:  0.7890625
train loss:  0.4646695554256439
train gradient:  0.1413837415116297
iteration : 8039
train acc:  0.7265625
train loss:  0.5230000019073486
train gradient:  0.12495623052447738
iteration : 8040
train acc:  0.7265625
train loss:  0.5019690990447998
train gradient:  0.12212421418179122
iteration : 8041
train acc:  0.7734375
train loss:  0.46131378412246704
train gradient:  0.0902485840144999
iteration : 8042
train acc:  0.7578125
train loss:  0.5273460149765015
train gradient:  0.14113354486652285
iteration : 8043
train acc:  0.703125
train loss:  0.5533870458602905
train gradient:  0.14439267325572994
iteration : 8044
train acc:  0.78125
train loss:  0.4767948389053345
train gradient:  0.13165292244296742
iteration : 8045
train acc:  0.75
train loss:  0.4566911458969116
train gradient:  0.13398131402486282
iteration : 8046
train acc:  0.75
train loss:  0.5028016567230225
train gradient:  0.16388713997117826
iteration : 8047
train acc:  0.71875
train loss:  0.5802408456802368
train gradient:  0.17592361523869177
iteration : 8048
train acc:  0.734375
train loss:  0.49825841188430786
train gradient:  0.12990038879937782
iteration : 8049
train acc:  0.8515625
train loss:  0.37524881958961487
train gradient:  0.10733526371633778
iteration : 8050
train acc:  0.765625
train loss:  0.4870036840438843
train gradient:  0.11623585702947022
iteration : 8051
train acc:  0.7109375
train loss:  0.5152602791786194
train gradient:  0.12063216377491255
iteration : 8052
train acc:  0.7265625
train loss:  0.564307451248169
train gradient:  0.17803972904199739
iteration : 8053
train acc:  0.7109375
train loss:  0.5347068309783936
train gradient:  0.16359222397165657
iteration : 8054
train acc:  0.765625
train loss:  0.4684164524078369
train gradient:  0.1093600764641505
iteration : 8055
train acc:  0.734375
train loss:  0.5355554819107056
train gradient:  0.1372461785516574
iteration : 8056
train acc:  0.75
train loss:  0.5546079874038696
train gradient:  0.1526487507407224
iteration : 8057
train acc:  0.703125
train loss:  0.535746693611145
train gradient:  0.13259452246430653
iteration : 8058
train acc:  0.7265625
train loss:  0.48713141679763794
train gradient:  0.11618869658933553
iteration : 8059
train acc:  0.7890625
train loss:  0.46649548411369324
train gradient:  0.10822244116072947
iteration : 8060
train acc:  0.796875
train loss:  0.4386705160140991
train gradient:  0.11414854068381348
iteration : 8061
train acc:  0.78125
train loss:  0.42413583397865295
train gradient:  0.11820036263290704
iteration : 8062
train acc:  0.78125
train loss:  0.5177726745605469
train gradient:  0.15680911789671534
iteration : 8063
train acc:  0.78125
train loss:  0.46476083993911743
train gradient:  0.12504300978876642
iteration : 8064
train acc:  0.7265625
train loss:  0.4859268069267273
train gradient:  0.11499469815884385
iteration : 8065
train acc:  0.7578125
train loss:  0.48921599984169006
train gradient:  0.11815201575330003
iteration : 8066
train acc:  0.765625
train loss:  0.4941117763519287
train gradient:  0.1331489273324783
iteration : 8067
train acc:  0.8671875
train loss:  0.3911999464035034
train gradient:  0.12382959992789956
iteration : 8068
train acc:  0.8046875
train loss:  0.457454651594162
train gradient:  0.12035638337140772
iteration : 8069
train acc:  0.7734375
train loss:  0.5110938549041748
train gradient:  0.14214372289636162
iteration : 8070
train acc:  0.7734375
train loss:  0.5137861967086792
train gradient:  0.1620829688917872
iteration : 8071
train acc:  0.6953125
train loss:  0.585706353187561
train gradient:  0.16744654928433234
iteration : 8072
train acc:  0.7734375
train loss:  0.48648881912231445
train gradient:  0.12823985414786299
iteration : 8073
train acc:  0.75
train loss:  0.5041125416755676
train gradient:  0.11821333853861224
iteration : 8074
train acc:  0.7421875
train loss:  0.47068464756011963
train gradient:  0.10159528021591815
iteration : 8075
train acc:  0.71875
train loss:  0.4945846199989319
train gradient:  0.14236080762297434
iteration : 8076
train acc:  0.7265625
train loss:  0.5773839950561523
train gradient:  0.17198574732342795
iteration : 8077
train acc:  0.6953125
train loss:  0.5303272604942322
train gradient:  0.13949963258217712
iteration : 8078
train acc:  0.6796875
train loss:  0.5555932521820068
train gradient:  0.15443966508499868
iteration : 8079
train acc:  0.71875
train loss:  0.5318605899810791
train gradient:  0.1758383504001898
iteration : 8080
train acc:  0.765625
train loss:  0.44543224573135376
train gradient:  0.11034004689107306
iteration : 8081
train acc:  0.75
train loss:  0.5868399739265442
train gradient:  0.14465149463438753
iteration : 8082
train acc:  0.75
train loss:  0.5216128826141357
train gradient:  0.19705423984246256
iteration : 8083
train acc:  0.734375
train loss:  0.46263301372528076
train gradient:  0.09501212192450352
iteration : 8084
train acc:  0.78125
train loss:  0.4686805307865143
train gradient:  0.12627048197207208
iteration : 8085
train acc:  0.734375
train loss:  0.5254288911819458
train gradient:  0.14330187322251842
iteration : 8086
train acc:  0.7890625
train loss:  0.5311415791511536
train gradient:  0.15320896560034986
iteration : 8087
train acc:  0.75
train loss:  0.5286000967025757
train gradient:  0.15120167309531604
iteration : 8088
train acc:  0.734375
train loss:  0.46309226751327515
train gradient:  0.1237808946166382
iteration : 8089
train acc:  0.7265625
train loss:  0.5389462113380432
train gradient:  0.17945651683328545
iteration : 8090
train acc:  0.765625
train loss:  0.46640700101852417
train gradient:  0.11421208883425402
iteration : 8091
train acc:  0.6875
train loss:  0.5209733843803406
train gradient:  0.11105579298669649
iteration : 8092
train acc:  0.671875
train loss:  0.5485955476760864
train gradient:  0.1621183569153084
iteration : 8093
train acc:  0.7578125
train loss:  0.4547703266143799
train gradient:  0.11020025951592355
iteration : 8094
train acc:  0.78125
train loss:  0.4543871283531189
train gradient:  0.11363580201193867
iteration : 8095
train acc:  0.734375
train loss:  0.5148859024047852
train gradient:  0.1544389301396548
iteration : 8096
train acc:  0.6875
train loss:  0.5769748687744141
train gradient:  0.15190585022172146
iteration : 8097
train acc:  0.703125
train loss:  0.48394134640693665
train gradient:  0.13892181471973541
iteration : 8098
train acc:  0.7421875
train loss:  0.5209391117095947
train gradient:  0.1283262156962377
iteration : 8099
train acc:  0.8125
train loss:  0.41966378688812256
train gradient:  0.10959763515994436
iteration : 8100
train acc:  0.703125
train loss:  0.5095058679580688
train gradient:  0.14080794720779535
iteration : 8101
train acc:  0.78125
train loss:  0.44164949655532837
train gradient:  0.11024294956048031
iteration : 8102
train acc:  0.78125
train loss:  0.4954032003879547
train gradient:  0.14376118243522049
iteration : 8103
train acc:  0.7109375
train loss:  0.530754566192627
train gradient:  0.14170544472521474
iteration : 8104
train acc:  0.6796875
train loss:  0.5403633117675781
train gradient:  0.1419220675305436
iteration : 8105
train acc:  0.734375
train loss:  0.5165705680847168
train gradient:  0.13754766823191011
iteration : 8106
train acc:  0.765625
train loss:  0.4694440960884094
train gradient:  0.10582700848349819
iteration : 8107
train acc:  0.734375
train loss:  0.47264260053634644
train gradient:  0.12294207341842249
iteration : 8108
train acc:  0.796875
train loss:  0.4264304041862488
train gradient:  0.12216814195772914
iteration : 8109
train acc:  0.734375
train loss:  0.4939056932926178
train gradient:  0.15270538411506762
iteration : 8110
train acc:  0.6875
train loss:  0.5181500911712646
train gradient:  0.1324318041853067
iteration : 8111
train acc:  0.765625
train loss:  0.43479490280151367
train gradient:  0.11482225204920468
iteration : 8112
train acc:  0.734375
train loss:  0.5268629193305969
train gradient:  0.14806178522667254
iteration : 8113
train acc:  0.7578125
train loss:  0.4498177170753479
train gradient:  0.14465769332053247
iteration : 8114
train acc:  0.703125
train loss:  0.5119394063949585
train gradient:  0.14095521701846225
iteration : 8115
train acc:  0.8046875
train loss:  0.4093046188354492
train gradient:  0.10131760413592233
iteration : 8116
train acc:  0.7421875
train loss:  0.4954826831817627
train gradient:  0.14768973569402516
iteration : 8117
train acc:  0.7734375
train loss:  0.4605693817138672
train gradient:  0.13417343696779832
iteration : 8118
train acc:  0.8125
train loss:  0.46592628955841064
train gradient:  0.13936477972435046
iteration : 8119
train acc:  0.703125
train loss:  0.5630181431770325
train gradient:  0.15621027925793157
iteration : 8120
train acc:  0.6796875
train loss:  0.5687037706375122
train gradient:  0.17474317794547045
iteration : 8121
train acc:  0.7578125
train loss:  0.5221445560455322
train gradient:  0.16264046868923648
iteration : 8122
train acc:  0.703125
train loss:  0.5285775065422058
train gradient:  0.1383904389527357
iteration : 8123
train acc:  0.7421875
train loss:  0.5538148880004883
train gradient:  0.15208843715400958
iteration : 8124
train acc:  0.65625
train loss:  0.5884808301925659
train gradient:  0.17511301244465044
iteration : 8125
train acc:  0.7734375
train loss:  0.44485199451446533
train gradient:  0.12825331573937526
iteration : 8126
train acc:  0.7734375
train loss:  0.4534400701522827
train gradient:  0.11074535576707939
iteration : 8127
train acc:  0.703125
train loss:  0.5464029908180237
train gradient:  0.17003555525483838
iteration : 8128
train acc:  0.734375
train loss:  0.5289038419723511
train gradient:  0.13580958047523262
iteration : 8129
train acc:  0.7578125
train loss:  0.4355212450027466
train gradient:  0.11058606656007747
iteration : 8130
train acc:  0.75
train loss:  0.47042831778526306
train gradient:  0.13901913685497813
iteration : 8131
train acc:  0.7109375
train loss:  0.4908111095428467
train gradient:  0.14683330678783177
iteration : 8132
train acc:  0.7265625
train loss:  0.5641795992851257
train gradient:  0.16320275538122042
iteration : 8133
train acc:  0.7421875
train loss:  0.4852999150753021
train gradient:  0.1259409130855933
iteration : 8134
train acc:  0.7421875
train loss:  0.4779243469238281
train gradient:  0.11869735696654206
iteration : 8135
train acc:  0.7109375
train loss:  0.503124475479126
train gradient:  0.17442261956223304
iteration : 8136
train acc:  0.796875
train loss:  0.4454894959926605
train gradient:  0.12338943389190263
iteration : 8137
train acc:  0.796875
train loss:  0.4582567811012268
train gradient:  0.12081291292941167
iteration : 8138
train acc:  0.703125
train loss:  0.516938328742981
train gradient:  0.1310263054426541
iteration : 8139
train acc:  0.765625
train loss:  0.4745607376098633
train gradient:  0.1123504639394118
iteration : 8140
train acc:  0.7109375
train loss:  0.4827286899089813
train gradient:  0.11121610146965283
iteration : 8141
train acc:  0.7265625
train loss:  0.5400891900062561
train gradient:  0.1639174360558206
iteration : 8142
train acc:  0.734375
train loss:  0.54243403673172
train gradient:  0.15561150849229627
iteration : 8143
train acc:  0.7265625
train loss:  0.601829469203949
train gradient:  0.2119183740637979
iteration : 8144
train acc:  0.7265625
train loss:  0.544951319694519
train gradient:  0.13622640866150054
iteration : 8145
train acc:  0.7265625
train loss:  0.553924560546875
train gradient:  0.1478511370004617
iteration : 8146
train acc:  0.75
train loss:  0.4644929766654968
train gradient:  0.12040456102797747
iteration : 8147
train acc:  0.6484375
train loss:  0.5700254440307617
train gradient:  0.13464244679582077
iteration : 8148
train acc:  0.7578125
train loss:  0.49357497692108154
train gradient:  0.13245847087666698
iteration : 8149
train acc:  0.703125
train loss:  0.5563478469848633
train gradient:  0.17627174634879952
iteration : 8150
train acc:  0.7890625
train loss:  0.4492359757423401
train gradient:  0.1121678931523277
iteration : 8151
train acc:  0.734375
train loss:  0.483452707529068
train gradient:  0.132716195180609
iteration : 8152
train acc:  0.75
train loss:  0.5150810480117798
train gradient:  0.1352511050276054
iteration : 8153
train acc:  0.734375
train loss:  0.4907988905906677
train gradient:  0.12901780268285096
iteration : 8154
train acc:  0.7578125
train loss:  0.467426598072052
train gradient:  0.16490122673129193
iteration : 8155
train acc:  0.703125
train loss:  0.5459953546524048
train gradient:  0.12549009908111003
iteration : 8156
train acc:  0.734375
train loss:  0.47093087434768677
train gradient:  0.1031512795503521
iteration : 8157
train acc:  0.765625
train loss:  0.4649956524372101
train gradient:  0.09453634069289366
iteration : 8158
train acc:  0.6796875
train loss:  0.5190286040306091
train gradient:  0.1358787129703567
iteration : 8159
train acc:  0.734375
train loss:  0.5128147602081299
train gradient:  0.12121681923218347
iteration : 8160
train acc:  0.71875
train loss:  0.5130636096000671
train gradient:  0.1491650565308984
iteration : 8161
train acc:  0.6953125
train loss:  0.5872423648834229
train gradient:  0.1637367200891506
iteration : 8162
train acc:  0.7578125
train loss:  0.48461341857910156
train gradient:  0.1483479257124704
iteration : 8163
train acc:  0.7265625
train loss:  0.5252931118011475
train gradient:  0.13660711125467578
iteration : 8164
train acc:  0.734375
train loss:  0.5088108777999878
train gradient:  0.12161940231705798
iteration : 8165
train acc:  0.7109375
train loss:  0.5661352872848511
train gradient:  0.13829735971357016
iteration : 8166
train acc:  0.7578125
train loss:  0.5114942789077759
train gradient:  0.12751770565906262
iteration : 8167
train acc:  0.6953125
train loss:  0.5490572452545166
train gradient:  0.14030287735881472
iteration : 8168
train acc:  0.6875
train loss:  0.5694347023963928
train gradient:  0.12443609023594769
iteration : 8169
train acc:  0.7578125
train loss:  0.4804518520832062
train gradient:  0.13428183493005896
iteration : 8170
train acc:  0.703125
train loss:  0.5310437083244324
train gradient:  0.19012751700938735
iteration : 8171
train acc:  0.6953125
train loss:  0.5551906824111938
train gradient:  0.17196331292887768
iteration : 8172
train acc:  0.6953125
train loss:  0.5499545335769653
train gradient:  0.17308000479241153
iteration : 8173
train acc:  0.75
train loss:  0.48055005073547363
train gradient:  0.12723021808685886
iteration : 8174
train acc:  0.7265625
train loss:  0.5215117931365967
train gradient:  0.1503037280638838
iteration : 8175
train acc:  0.78125
train loss:  0.42524099349975586
train gradient:  0.07858914590998457
iteration : 8176
train acc:  0.6796875
train loss:  0.49909183382987976
train gradient:  0.1090638337110822
iteration : 8177
train acc:  0.703125
train loss:  0.526221752166748
train gradient:  0.14488948940681146
iteration : 8178
train acc:  0.7734375
train loss:  0.5039508938789368
train gradient:  0.12394568393555878
iteration : 8179
train acc:  0.6484375
train loss:  0.5112032890319824
train gradient:  0.15864700212778016
iteration : 8180
train acc:  0.7890625
train loss:  0.4805339276790619
train gradient:  0.10409211320024604
iteration : 8181
train acc:  0.7265625
train loss:  0.4974220097064972
train gradient:  0.14513873769760063
iteration : 8182
train acc:  0.765625
train loss:  0.4613976776599884
train gradient:  0.11577246873112981
iteration : 8183
train acc:  0.71875
train loss:  0.5053269267082214
train gradient:  0.13777558369619797
iteration : 8184
train acc:  0.7578125
train loss:  0.4466959238052368
train gradient:  0.09699683376913322
iteration : 8185
train acc:  0.8125
train loss:  0.4749601483345032
train gradient:  0.0983782139000992
iteration : 8186
train acc:  0.7265625
train loss:  0.4865304231643677
train gradient:  0.13816085980074455
iteration : 8187
train acc:  0.703125
train loss:  0.5383545160293579
train gradient:  0.13774922137249293
iteration : 8188
train acc:  0.7890625
train loss:  0.4280608892440796
train gradient:  0.09200950388411021
iteration : 8189
train acc:  0.6953125
train loss:  0.544499933719635
train gradient:  0.15981055706755598
iteration : 8190
train acc:  0.7578125
train loss:  0.4780694246292114
train gradient:  0.1082736585386522
iteration : 8191
train acc:  0.75
train loss:  0.48759013414382935
train gradient:  0.12873220872677915
iteration : 8192
train acc:  0.7734375
train loss:  0.45515748858451843
train gradient:  0.10537239802585287
iteration : 8193
train acc:  0.6875
train loss:  0.5306316614151001
train gradient:  0.1573962290994419
iteration : 8194
train acc:  0.734375
train loss:  0.4879039525985718
train gradient:  0.12257841272530537
iteration : 8195
train acc:  0.7421875
train loss:  0.512035608291626
train gradient:  0.13244321479257143
iteration : 8196
train acc:  0.7421875
train loss:  0.5114266872406006
train gradient:  0.12568861145526883
iteration : 8197
train acc:  0.7265625
train loss:  0.5270623564720154
train gradient:  0.13663522911221598
iteration : 8198
train acc:  0.78125
train loss:  0.45309337973594666
train gradient:  0.10376882245316366
iteration : 8199
train acc:  0.71875
train loss:  0.5039651989936829
train gradient:  0.13118960114136224
iteration : 8200
train acc:  0.7890625
train loss:  0.47037845849990845
train gradient:  0.12119879806569862
iteration : 8201
train acc:  0.71875
train loss:  0.5095244646072388
train gradient:  0.1144285906592711
iteration : 8202
train acc:  0.78125
train loss:  0.43541067838668823
train gradient:  0.08420296961395941
iteration : 8203
train acc:  0.7421875
train loss:  0.45702531933784485
train gradient:  0.11479782870848015
iteration : 8204
train acc:  0.8359375
train loss:  0.4812208414077759
train gradient:  0.18853146578617427
iteration : 8205
train acc:  0.7265625
train loss:  0.502045750617981
train gradient:  0.11673439527998437
iteration : 8206
train acc:  0.7109375
train loss:  0.5466742515563965
train gradient:  0.16147114228755088
iteration : 8207
train acc:  0.703125
train loss:  0.49099016189575195
train gradient:  0.11195649210509552
iteration : 8208
train acc:  0.7734375
train loss:  0.4647842049598694
train gradient:  0.13509301099164905
iteration : 8209
train acc:  0.7890625
train loss:  0.4770163297653198
train gradient:  0.10656695037327939
iteration : 8210
train acc:  0.6953125
train loss:  0.5505241751670837
train gradient:  0.13050256034875818
iteration : 8211
train acc:  0.7734375
train loss:  0.535871684551239
train gradient:  0.1434756760675778
iteration : 8212
train acc:  0.7578125
train loss:  0.4708367586135864
train gradient:  0.1328179514064023
iteration : 8213
train acc:  0.6953125
train loss:  0.559424877166748
train gradient:  0.17132754180722642
iteration : 8214
train acc:  0.8203125
train loss:  0.4472547173500061
train gradient:  0.11159589462130586
iteration : 8215
train acc:  0.7265625
train loss:  0.46532198786735535
train gradient:  0.09043194758319709
iteration : 8216
train acc:  0.7265625
train loss:  0.49501535296440125
train gradient:  0.1085539800938823
iteration : 8217
train acc:  0.734375
train loss:  0.5086850523948669
train gradient:  0.13137338647253677
iteration : 8218
train acc:  0.75
train loss:  0.4711301326751709
train gradient:  0.11691067758883657
iteration : 8219
train acc:  0.703125
train loss:  0.5401100516319275
train gradient:  0.15846664531667268
iteration : 8220
train acc:  0.7109375
train loss:  0.5368795394897461
train gradient:  0.1646611641287803
iteration : 8221
train acc:  0.7421875
train loss:  0.5418909192085266
train gradient:  0.1315591389652496
iteration : 8222
train acc:  0.7421875
train loss:  0.5236422419548035
train gradient:  0.13765171613852953
iteration : 8223
train acc:  0.6875
train loss:  0.5604581236839294
train gradient:  0.1841561928141353
iteration : 8224
train acc:  0.7109375
train loss:  0.5528128147125244
train gradient:  0.170142830243965
iteration : 8225
train acc:  0.7109375
train loss:  0.5771403312683105
train gradient:  0.17039545119553184
iteration : 8226
train acc:  0.6484375
train loss:  0.5606629848480225
train gradient:  0.13869986949589588
iteration : 8227
train acc:  0.7734375
train loss:  0.4372914135456085
train gradient:  0.10844416033777661
iteration : 8228
train acc:  0.8203125
train loss:  0.4034914970397949
train gradient:  0.09286455566239546
iteration : 8229
train acc:  0.7578125
train loss:  0.4465174376964569
train gradient:  0.10739277289445089
iteration : 8230
train acc:  0.703125
train loss:  0.5292028188705444
train gradient:  0.1413525690402085
iteration : 8231
train acc:  0.734375
train loss:  0.5075305104255676
train gradient:  0.11463642873520409
iteration : 8232
train acc:  0.6953125
train loss:  0.500246524810791
train gradient:  0.12331242808747737
iteration : 8233
train acc:  0.7265625
train loss:  0.5255090594291687
train gradient:  0.11710050817273852
iteration : 8234
train acc:  0.8125
train loss:  0.41025710105895996
train gradient:  0.07889559171666352
iteration : 8235
train acc:  0.703125
train loss:  0.5210036039352417
train gradient:  0.10982709308009118
iteration : 8236
train acc:  0.7578125
train loss:  0.4339175820350647
train gradient:  0.11319588812467823
iteration : 8237
train acc:  0.75
train loss:  0.49686455726623535
train gradient:  0.12573137350968272
iteration : 8238
train acc:  0.6953125
train loss:  0.5222560167312622
train gradient:  0.1457560400454169
iteration : 8239
train acc:  0.71875
train loss:  0.512786865234375
train gradient:  0.13344410195357287
iteration : 8240
train acc:  0.7109375
train loss:  0.4951813817024231
train gradient:  0.13833145145062098
iteration : 8241
train acc:  0.75
train loss:  0.49626341462135315
train gradient:  0.13333702590613034
iteration : 8242
train acc:  0.78125
train loss:  0.4529542326927185
train gradient:  0.0970137257812435
iteration : 8243
train acc:  0.796875
train loss:  0.4423130452632904
train gradient:  0.08648247331189647
iteration : 8244
train acc:  0.7734375
train loss:  0.49796557426452637
train gradient:  0.1697066653791289
iteration : 8245
train acc:  0.734375
train loss:  0.5339478850364685
train gradient:  0.1426969430086154
iteration : 8246
train acc:  0.7109375
train loss:  0.4701191186904907
train gradient:  0.13685869258422523
iteration : 8247
train acc:  0.765625
train loss:  0.4587502181529999
train gradient:  0.11216161191338721
iteration : 8248
train acc:  0.6953125
train loss:  0.5280444025993347
train gradient:  0.15548781868143935
iteration : 8249
train acc:  0.7578125
train loss:  0.5090187788009644
train gradient:  0.1388838193249503
iteration : 8250
train acc:  0.6796875
train loss:  0.56923508644104
train gradient:  0.1686887695759189
iteration : 8251
train acc:  0.7578125
train loss:  0.4977703094482422
train gradient:  0.1135739911783893
iteration : 8252
train acc:  0.71875
train loss:  0.5154353380203247
train gradient:  0.11902765906244667
iteration : 8253
train acc:  0.7109375
train loss:  0.5138906240463257
train gradient:  0.1404837951253723
iteration : 8254
train acc:  0.796875
train loss:  0.48458752036094666
train gradient:  0.11451583226157976
iteration : 8255
train acc:  0.765625
train loss:  0.4882880747318268
train gradient:  0.1376159447188014
iteration : 8256
train acc:  0.8203125
train loss:  0.4550720751285553
train gradient:  0.11646497429970679
iteration : 8257
train acc:  0.765625
train loss:  0.4862107038497925
train gradient:  0.15019323408015645
iteration : 8258
train acc:  0.7421875
train loss:  0.47990959882736206
train gradient:  0.14542858329224173
iteration : 8259
train acc:  0.703125
train loss:  0.497592955827713
train gradient:  0.10504267331508675
iteration : 8260
train acc:  0.7421875
train loss:  0.46248751878738403
train gradient:  0.10712442236882572
iteration : 8261
train acc:  0.734375
train loss:  0.546033501625061
train gradient:  0.16268426549522863
iteration : 8262
train acc:  0.75
train loss:  0.46591246128082275
train gradient:  0.12829421338503036
iteration : 8263
train acc:  0.7734375
train loss:  0.42852842807769775
train gradient:  0.08323011585771124
iteration : 8264
train acc:  0.703125
train loss:  0.5505176186561584
train gradient:  0.15548453900016945
iteration : 8265
train acc:  0.78125
train loss:  0.41606277227401733
train gradient:  0.10313436749491425
iteration : 8266
train acc:  0.734375
train loss:  0.5117931365966797
train gradient:  0.15155513494580447
iteration : 8267
train acc:  0.765625
train loss:  0.4689148962497711
train gradient:  0.14500404374872977
iteration : 8268
train acc:  0.7421875
train loss:  0.4773394763469696
train gradient:  0.13608649629789712
iteration : 8269
train acc:  0.7734375
train loss:  0.44384220242500305
train gradient:  0.10540290434488554
iteration : 8270
train acc:  0.6640625
train loss:  0.6305685043334961
train gradient:  0.21739564948389306
iteration : 8271
train acc:  0.7578125
train loss:  0.4883911609649658
train gradient:  0.11094731800129091
iteration : 8272
train acc:  0.7109375
train loss:  0.4855768084526062
train gradient:  0.09578664627449086
iteration : 8273
train acc:  0.75
train loss:  0.491592675447464
train gradient:  0.14761161031110048
iteration : 8274
train acc:  0.7265625
train loss:  0.54532790184021
train gradient:  0.16801764362216062
iteration : 8275
train acc:  0.796875
train loss:  0.47359320521354675
train gradient:  0.14407265252768814
iteration : 8276
train acc:  0.75
train loss:  0.4731924533843994
train gradient:  0.13215667565596417
iteration : 8277
train acc:  0.7421875
train loss:  0.5038620233535767
train gradient:  0.11149568232959206
iteration : 8278
train acc:  0.7421875
train loss:  0.48653078079223633
train gradient:  0.11347175726963997
iteration : 8279
train acc:  0.6796875
train loss:  0.5562703609466553
train gradient:  0.16957241287543234
iteration : 8280
train acc:  0.703125
train loss:  0.5073205232620239
train gradient:  0.1419316858580999
iteration : 8281
train acc:  0.671875
train loss:  0.5763872861862183
train gradient:  0.1538035344847492
iteration : 8282
train acc:  0.734375
train loss:  0.5133519172668457
train gradient:  0.13955191085892923
iteration : 8283
train acc:  0.71875
train loss:  0.4839024245738983
train gradient:  0.11105424721555035
iteration : 8284
train acc:  0.765625
train loss:  0.48028504848480225
train gradient:  0.11948221133372601
iteration : 8285
train acc:  0.7421875
train loss:  0.47989988327026367
train gradient:  0.1094329811537065
iteration : 8286
train acc:  0.6953125
train loss:  0.543820321559906
train gradient:  0.1761024597655284
iteration : 8287
train acc:  0.796875
train loss:  0.4340746998786926
train gradient:  0.09194217569763233
iteration : 8288
train acc:  0.7421875
train loss:  0.5255729556083679
train gradient:  0.1541813673808315
iteration : 8289
train acc:  0.671875
train loss:  0.5395994186401367
train gradient:  0.13006957013978393
iteration : 8290
train acc:  0.78125
train loss:  0.46489131450653076
train gradient:  0.11330236657424839
iteration : 8291
train acc:  0.7421875
train loss:  0.5427403450012207
train gradient:  0.16082875139174113
iteration : 8292
train acc:  0.703125
train loss:  0.5338830947875977
train gradient:  0.15563567790899047
iteration : 8293
train acc:  0.7578125
train loss:  0.4884997308254242
train gradient:  0.13050865064381406
iteration : 8294
train acc:  0.7734375
train loss:  0.4582040309906006
train gradient:  0.09756016471242697
iteration : 8295
train acc:  0.7734375
train loss:  0.49543941020965576
train gradient:  0.12424374654952722
iteration : 8296
train acc:  0.7734375
train loss:  0.46869754791259766
train gradient:  0.1392341783551036
iteration : 8297
train acc:  0.75
train loss:  0.4840504825115204
train gradient:  0.10398976444038477
iteration : 8298
train acc:  0.7421875
train loss:  0.4844646751880646
train gradient:  0.12465264733037676
iteration : 8299
train acc:  0.6953125
train loss:  0.5002762079238892
train gradient:  0.1289215411560759
iteration : 8300
train acc:  0.7109375
train loss:  0.5002574324607849
train gradient:  0.11026851918494157
iteration : 8301
train acc:  0.8125
train loss:  0.4650604724884033
train gradient:  0.13308673305691657
iteration : 8302
train acc:  0.7734375
train loss:  0.4752935767173767
train gradient:  0.1436737674992112
iteration : 8303
train acc:  0.7109375
train loss:  0.5739431381225586
train gradient:  0.1584093103805836
iteration : 8304
train acc:  0.7265625
train loss:  0.5067042708396912
train gradient:  0.22227306066559607
iteration : 8305
train acc:  0.7578125
train loss:  0.4529692828655243
train gradient:  0.1007701771970293
iteration : 8306
train acc:  0.8125
train loss:  0.4262005090713501
train gradient:  0.08923598601540766
iteration : 8307
train acc:  0.75
train loss:  0.49786096811294556
train gradient:  0.11893508775897885
iteration : 8308
train acc:  0.7421875
train loss:  0.5405639410018921
train gradient:  0.14929958039088603
iteration : 8309
train acc:  0.6796875
train loss:  0.5668051242828369
train gradient:  0.1452518204312867
iteration : 8310
train acc:  0.7578125
train loss:  0.5145034790039062
train gradient:  0.1124866021126793
iteration : 8311
train acc:  0.7578125
train loss:  0.45762622356414795
train gradient:  0.09389646417091534
iteration : 8312
train acc:  0.6796875
train loss:  0.5852566361427307
train gradient:  0.15414813553294004
iteration : 8313
train acc:  0.7109375
train loss:  0.5168566703796387
train gradient:  0.1536572372010145
iteration : 8314
train acc:  0.75
train loss:  0.519848108291626
train gradient:  0.12453252848844863
iteration : 8315
train acc:  0.8203125
train loss:  0.40949469804763794
train gradient:  0.11876760148774335
iteration : 8316
train acc:  0.7265625
train loss:  0.5607208013534546
train gradient:  0.20464706998308546
iteration : 8317
train acc:  0.7890625
train loss:  0.4665955603122711
train gradient:  0.12512107646108686
iteration : 8318
train acc:  0.7734375
train loss:  0.4568966329097748
train gradient:  0.10320613669933845
iteration : 8319
train acc:  0.78125
train loss:  0.43166154623031616
train gradient:  0.09148341903165647
iteration : 8320
train acc:  0.8125
train loss:  0.43539488315582275
train gradient:  0.11960632095009187
iteration : 8321
train acc:  0.7421875
train loss:  0.5227959156036377
train gradient:  0.15568532272480545
iteration : 8322
train acc:  0.734375
train loss:  0.5035060048103333
train gradient:  0.11362158488295064
iteration : 8323
train acc:  0.703125
train loss:  0.5104843378067017
train gradient:  0.1588344571515941
iteration : 8324
train acc:  0.7578125
train loss:  0.4521331489086151
train gradient:  0.1287796319932272
iteration : 8325
train acc:  0.734375
train loss:  0.4623096287250519
train gradient:  0.10813129303665121
iteration : 8326
train acc:  0.75
train loss:  0.49276798963546753
train gradient:  0.10508731079263912
iteration : 8327
train acc:  0.7890625
train loss:  0.4594436287879944
train gradient:  0.12829377909393105
iteration : 8328
train acc:  0.7421875
train loss:  0.5571862459182739
train gradient:  0.15080221473006244
iteration : 8329
train acc:  0.7265625
train loss:  0.5146732330322266
train gradient:  0.15121081131849956
iteration : 8330
train acc:  0.703125
train loss:  0.47547274827957153
train gradient:  0.11705280332145222
iteration : 8331
train acc:  0.7734375
train loss:  0.44641804695129395
train gradient:  0.12706900023851814
iteration : 8332
train acc:  0.75
train loss:  0.4851970672607422
train gradient:  0.10757927792731344
iteration : 8333
train acc:  0.7109375
train loss:  0.5545473098754883
train gradient:  0.16181288089518958
iteration : 8334
train acc:  0.7734375
train loss:  0.48707449436187744
train gradient:  0.1675668700628123
iteration : 8335
train acc:  0.75
train loss:  0.4537513852119446
train gradient:  0.12022703627341434
iteration : 8336
train acc:  0.71875
train loss:  0.532679557800293
train gradient:  0.13723703047311425
iteration : 8337
train acc:  0.78125
train loss:  0.5081607699394226
train gradient:  0.122984752570764
iteration : 8338
train acc:  0.734375
train loss:  0.45525187253952026
train gradient:  0.1217412978724014
iteration : 8339
train acc:  0.734375
train loss:  0.5126273036003113
train gradient:  0.11428900805070787
iteration : 8340
train acc:  0.75
train loss:  0.548904299736023
train gradient:  0.15448741977291397
iteration : 8341
train acc:  0.7578125
train loss:  0.47823241353034973
train gradient:  0.09508639740567373
iteration : 8342
train acc:  0.6796875
train loss:  0.5630133152008057
train gradient:  0.14424528670413633
iteration : 8343
train acc:  0.7421875
train loss:  0.5097359418869019
train gradient:  0.10545262588219836
iteration : 8344
train acc:  0.71875
train loss:  0.5074263215065002
train gradient:  0.17716943086126768
iteration : 8345
train acc:  0.703125
train loss:  0.5393122434616089
train gradient:  0.13265911490891852
iteration : 8346
train acc:  0.7421875
train loss:  0.5302283763885498
train gradient:  0.19305034216833405
iteration : 8347
train acc:  0.7421875
train loss:  0.46374520659446716
train gradient:  0.10331165111988216
iteration : 8348
train acc:  0.7421875
train loss:  0.4997527003288269
train gradient:  0.12369346686254647
iteration : 8349
train acc:  0.7734375
train loss:  0.48453909158706665
train gradient:  0.12082193125814283
iteration : 8350
train acc:  0.7109375
train loss:  0.48939329385757446
train gradient:  0.12523297832044084
iteration : 8351
train acc:  0.7265625
train loss:  0.49018779397010803
train gradient:  0.13445879198456523
iteration : 8352
train acc:  0.75
train loss:  0.4929349422454834
train gradient:  0.09949815494293562
iteration : 8353
train acc:  0.71875
train loss:  0.49867457151412964
train gradient:  0.13540604227310565
iteration : 8354
train acc:  0.6953125
train loss:  0.571631908416748
train gradient:  0.12522810778234705
iteration : 8355
train acc:  0.734375
train loss:  0.4977632462978363
train gradient:  0.14369534195682554
iteration : 8356
train acc:  0.7421875
train loss:  0.5219967365264893
train gradient:  0.12888604325800274
iteration : 8357
train acc:  0.7890625
train loss:  0.433588445186615
train gradient:  0.09522666575642055
iteration : 8358
train acc:  0.734375
train loss:  0.5134825110435486
train gradient:  0.17197726231249924
iteration : 8359
train acc:  0.75
train loss:  0.48294079303741455
train gradient:  0.10468903361084235
iteration : 8360
train acc:  0.7421875
train loss:  0.5473031997680664
train gradient:  0.13100195846239637
iteration : 8361
train acc:  0.71875
train loss:  0.536481499671936
train gradient:  0.14205587021254046
iteration : 8362
train acc:  0.765625
train loss:  0.45295199751853943
train gradient:  0.09454157852305767
iteration : 8363
train acc:  0.71875
train loss:  0.5472702980041504
train gradient:  0.14802632001015287
iteration : 8364
train acc:  0.7109375
train loss:  0.5895283222198486
train gradient:  0.23939431063992667
iteration : 8365
train acc:  0.7890625
train loss:  0.47297030687332153
train gradient:  0.12287626580206701
iteration : 8366
train acc:  0.6796875
train loss:  0.5491572022438049
train gradient:  0.16601536265674982
iteration : 8367
train acc:  0.6875
train loss:  0.630336582660675
train gradient:  0.20843568713375588
iteration : 8368
train acc:  0.75
train loss:  0.4791060984134674
train gradient:  0.1054219944593263
iteration : 8369
train acc:  0.703125
train loss:  0.5153430700302124
train gradient:  0.13637682657598582
iteration : 8370
train acc:  0.7265625
train loss:  0.4765896201133728
train gradient:  0.08253749841881612
iteration : 8371
train acc:  0.75
train loss:  0.5022397041320801
train gradient:  0.11225521230416523
iteration : 8372
train acc:  0.75
train loss:  0.49116164445877075
train gradient:  0.1371017671038084
iteration : 8373
train acc:  0.7890625
train loss:  0.4695226848125458
train gradient:  0.09675782290722204
iteration : 8374
train acc:  0.7734375
train loss:  0.48142746090888977
train gradient:  0.1429849134280372
iteration : 8375
train acc:  0.7578125
train loss:  0.5039008259773254
train gradient:  0.14136756590003585
iteration : 8376
train acc:  0.765625
train loss:  0.4902576804161072
train gradient:  0.12125598345612067
iteration : 8377
train acc:  0.7578125
train loss:  0.5023543834686279
train gradient:  0.1222190848175462
iteration : 8378
train acc:  0.7734375
train loss:  0.44970813393592834
train gradient:  0.09181572849190377
iteration : 8379
train acc:  0.7265625
train loss:  0.501705527305603
train gradient:  0.12139961098068575
iteration : 8380
train acc:  0.734375
train loss:  0.5418314933776855
train gradient:  0.15439080236821973
iteration : 8381
train acc:  0.7421875
train loss:  0.5007414817810059
train gradient:  0.14530714947919732
iteration : 8382
train acc:  0.734375
train loss:  0.524349570274353
train gradient:  0.1267511903100848
iteration : 8383
train acc:  0.765625
train loss:  0.47713977098464966
train gradient:  0.15510971656675884
iteration : 8384
train acc:  0.6484375
train loss:  0.5904901623725891
train gradient:  0.15549423518075597
iteration : 8385
train acc:  0.6953125
train loss:  0.543470561504364
train gradient:  0.15696876831948803
iteration : 8386
train acc:  0.75
train loss:  0.4630076289176941
train gradient:  0.10197155997938778
iteration : 8387
train acc:  0.671875
train loss:  0.5983338356018066
train gradient:  0.17311584835735322
iteration : 8388
train acc:  0.7421875
train loss:  0.5500034093856812
train gradient:  0.13880767839397962
iteration : 8389
train acc:  0.6953125
train loss:  0.5375587344169617
train gradient:  0.16249870644109538
iteration : 8390
train acc:  0.6953125
train loss:  0.5858347415924072
train gradient:  0.14605814742631945
iteration : 8391
train acc:  0.734375
train loss:  0.529594361782074
train gradient:  0.15911110415981802
iteration : 8392
train acc:  0.7421875
train loss:  0.5045483708381653
train gradient:  0.1337283958151016
iteration : 8393
train acc:  0.71875
train loss:  0.5096498727798462
train gradient:  0.12399547302033348
iteration : 8394
train acc:  0.7421875
train loss:  0.5594127178192139
train gradient:  0.15599113163576944
iteration : 8395
train acc:  0.7578125
train loss:  0.4839879870414734
train gradient:  0.13857325326787248
iteration : 8396
train acc:  0.7578125
train loss:  0.48002636432647705
train gradient:  0.10704419614979682
iteration : 8397
train acc:  0.7421875
train loss:  0.4964239001274109
train gradient:  0.10909753721090883
iteration : 8398
train acc:  0.75
train loss:  0.506103515625
train gradient:  0.12101043816230247
iteration : 8399
train acc:  0.6875
train loss:  0.5234692096710205
train gradient:  0.12787996357940423
iteration : 8400
train acc:  0.8125
train loss:  0.4261956214904785
train gradient:  0.10085108592332183
iteration : 8401
train acc:  0.7265625
train loss:  0.5326258540153503
train gradient:  0.0991431279328751
iteration : 8402
train acc:  0.7265625
train loss:  0.4756072163581848
train gradient:  0.11583008453538475
iteration : 8403
train acc:  0.71875
train loss:  0.4926178753376007
train gradient:  0.12022599251086655
iteration : 8404
train acc:  0.71875
train loss:  0.4938226640224457
train gradient:  0.12917197370341738
iteration : 8405
train acc:  0.6953125
train loss:  0.5607285499572754
train gradient:  0.15331649559712773
iteration : 8406
train acc:  0.7734375
train loss:  0.46023446321487427
train gradient:  0.08338104641260947
iteration : 8407
train acc:  0.75
train loss:  0.4576396644115448
train gradient:  0.09774265500715794
iteration : 8408
train acc:  0.7265625
train loss:  0.547523021697998
train gradient:  0.1721577389618727
iteration : 8409
train acc:  0.734375
train loss:  0.5144463181495667
train gradient:  0.1380766668370324
iteration : 8410
train acc:  0.8203125
train loss:  0.39863574504852295
train gradient:  0.07695136311444424
iteration : 8411
train acc:  0.7109375
train loss:  0.5092130899429321
train gradient:  0.11230568073354269
iteration : 8412
train acc:  0.6953125
train loss:  0.5297015309333801
train gradient:  0.14518051994583553
iteration : 8413
train acc:  0.75
train loss:  0.4920944571495056
train gradient:  0.141912978123719
iteration : 8414
train acc:  0.78125
train loss:  0.4600376784801483
train gradient:  0.12140719889841141
iteration : 8415
train acc:  0.703125
train loss:  0.4942832291126251
train gradient:  0.135595528022069
iteration : 8416
train acc:  0.78125
train loss:  0.4668387174606323
train gradient:  0.1176067883632748
iteration : 8417
train acc:  0.6875
train loss:  0.5646611452102661
train gradient:  0.14324267997146875
iteration : 8418
train acc:  0.78125
train loss:  0.4395964741706848
train gradient:  0.09619672596338513
iteration : 8419
train acc:  0.75
train loss:  0.47201454639434814
train gradient:  0.10893876680214647
iteration : 8420
train acc:  0.703125
train loss:  0.47682926058769226
train gradient:  0.11268528340109307
iteration : 8421
train acc:  0.734375
train loss:  0.4520197808742523
train gradient:  0.10926189990112672
iteration : 8422
train acc:  0.7734375
train loss:  0.43150368332862854
train gradient:  0.09250723152761108
iteration : 8423
train acc:  0.6875
train loss:  0.5180345773696899
train gradient:  0.1270710041011256
iteration : 8424
train acc:  0.6953125
train loss:  0.5354419946670532
train gradient:  0.15153651169446414
iteration : 8425
train acc:  0.7734375
train loss:  0.47291940450668335
train gradient:  0.1077406096904628
iteration : 8426
train acc:  0.78125
train loss:  0.44150304794311523
train gradient:  0.09920417077581864
iteration : 8427
train acc:  0.75
train loss:  0.46506455540657043
train gradient:  0.12765368620877185
iteration : 8428
train acc:  0.6953125
train loss:  0.5043774843215942
train gradient:  0.1055689457226533
iteration : 8429
train acc:  0.78125
train loss:  0.4615716338157654
train gradient:  0.12601754653571814
iteration : 8430
train acc:  0.7578125
train loss:  0.5059527158737183
train gradient:  0.12654501855998723
iteration : 8431
train acc:  0.7578125
train loss:  0.5136048197746277
train gradient:  0.11653289342969586
iteration : 8432
train acc:  0.7265625
train loss:  0.481338232755661
train gradient:  0.11031288431020382
iteration : 8433
train acc:  0.7109375
train loss:  0.5098699331283569
train gradient:  0.10698462300725349
iteration : 8434
train acc:  0.671875
train loss:  0.5898506045341492
train gradient:  0.13580519718493936
iteration : 8435
train acc:  0.75
train loss:  0.449392169713974
train gradient:  0.10715754181374249
iteration : 8436
train acc:  0.7734375
train loss:  0.4723666310310364
train gradient:  0.14778773060929218
iteration : 8437
train acc:  0.734375
train loss:  0.5059801340103149
train gradient:  0.15834626177166955
iteration : 8438
train acc:  0.75
train loss:  0.48198747634887695
train gradient:  0.11689679456127602
iteration : 8439
train acc:  0.734375
train loss:  0.490310937166214
train gradient:  0.11072764616310417
iteration : 8440
train acc:  0.7734375
train loss:  0.4623412787914276
train gradient:  0.11175473549477481
iteration : 8441
train acc:  0.75
train loss:  0.4928174912929535
train gradient:  0.12267000877343853
iteration : 8442
train acc:  0.734375
train loss:  0.4777202606201172
train gradient:  0.11386343008286409
iteration : 8443
train acc:  0.7578125
train loss:  0.46515071392059326
train gradient:  0.12381641078258049
iteration : 8444
train acc:  0.6875
train loss:  0.5417996048927307
train gradient:  0.1556006303696838
iteration : 8445
train acc:  0.7890625
train loss:  0.4683496952056885
train gradient:  0.1184200647622811
iteration : 8446
train acc:  0.7734375
train loss:  0.45508939027786255
train gradient:  0.10859247743881498
iteration : 8447
train acc:  0.6875
train loss:  0.48775404691696167
train gradient:  0.14004801450721788
iteration : 8448
train acc:  0.75
train loss:  0.47310584783554077
train gradient:  0.10395229473270749
iteration : 8449
train acc:  0.671875
train loss:  0.6044455766677856
train gradient:  0.19085404968065267
iteration : 8450
train acc:  0.71875
train loss:  0.5587102174758911
train gradient:  0.18176372053703965
iteration : 8451
train acc:  0.734375
train loss:  0.48992806673049927
train gradient:  0.12223800405764113
iteration : 8452
train acc:  0.7578125
train loss:  0.47540533542633057
train gradient:  0.14155037617515745
iteration : 8453
train acc:  0.78125
train loss:  0.42164433002471924
train gradient:  0.1068200013722994
iteration : 8454
train acc:  0.75
train loss:  0.5148064494132996
train gradient:  0.16134791224670159
iteration : 8455
train acc:  0.734375
train loss:  0.4733409285545349
train gradient:  0.10038328264993832
iteration : 8456
train acc:  0.7890625
train loss:  0.41430044174194336
train gradient:  0.0883294023061015
iteration : 8457
train acc:  0.7578125
train loss:  0.4687163233757019
train gradient:  0.15689417968588795
iteration : 8458
train acc:  0.8203125
train loss:  0.43169671297073364
train gradient:  0.10143114482982278
iteration : 8459
train acc:  0.75
train loss:  0.5034865140914917
train gradient:  0.13010066643427645
iteration : 8460
train acc:  0.7421875
train loss:  0.4633484482765198
train gradient:  0.1093507947892246
iteration : 8461
train acc:  0.75
train loss:  0.5346338748931885
train gradient:  0.1257059413679793
iteration : 8462
train acc:  0.78125
train loss:  0.4362313449382782
train gradient:  0.11113385503221396
iteration : 8463
train acc:  0.8046875
train loss:  0.47400355339050293
train gradient:  0.11310439972289786
iteration : 8464
train acc:  0.7265625
train loss:  0.4726434051990509
train gradient:  0.11128571232894226
iteration : 8465
train acc:  0.7578125
train loss:  0.484820693731308
train gradient:  0.11294037935906719
iteration : 8466
train acc:  0.71875
train loss:  0.5325419902801514
train gradient:  0.17421340478793462
iteration : 8467
train acc:  0.7265625
train loss:  0.4892789125442505
train gradient:  0.1191290981626996
iteration : 8468
train acc:  0.8359375
train loss:  0.439044326543808
train gradient:  0.1161253300899183
iteration : 8469
train acc:  0.6953125
train loss:  0.5696021318435669
train gradient:  0.19768206657323978
iteration : 8470
train acc:  0.7109375
train loss:  0.497356653213501
train gradient:  0.12316592816034833
iteration : 8471
train acc:  0.78125
train loss:  0.4500983953475952
train gradient:  0.10915583301498041
iteration : 8472
train acc:  0.703125
train loss:  0.53054279088974
train gradient:  0.15439967551368872
iteration : 8473
train acc:  0.7890625
train loss:  0.4868938624858856
train gradient:  0.13048199261063476
iteration : 8474
train acc:  0.765625
train loss:  0.4874069392681122
train gradient:  0.13433662154827083
iteration : 8475
train acc:  0.7265625
train loss:  0.5272529721260071
train gradient:  0.1650196509143871
iteration : 8476
train acc:  0.7265625
train loss:  0.48443037271499634
train gradient:  0.11077558304623897
iteration : 8477
train acc:  0.7109375
train loss:  0.5048866271972656
train gradient:  0.14801739429309702
iteration : 8478
train acc:  0.7734375
train loss:  0.5137912034988403
train gradient:  0.1895483885927427
iteration : 8479
train acc:  0.7421875
train loss:  0.5002464652061462
train gradient:  0.11554538762403814
iteration : 8480
train acc:  0.6796875
train loss:  0.5959103107452393
train gradient:  0.15347649287764598
iteration : 8481
train acc:  0.75
train loss:  0.44422289729118347
train gradient:  0.08329683566099197
iteration : 8482
train acc:  0.7421875
train loss:  0.49555402994155884
train gradient:  0.13539348557725173
iteration : 8483
train acc:  0.7265625
train loss:  0.5742710828781128
train gradient:  0.13682811670906356
iteration : 8484
train acc:  0.71875
train loss:  0.5448991060256958
train gradient:  0.14192747698009783
iteration : 8485
train acc:  0.7734375
train loss:  0.452414333820343
train gradient:  0.1126777967107229
iteration : 8486
train acc:  0.7109375
train loss:  0.5506412386894226
train gradient:  0.15959308401056077
iteration : 8487
train acc:  0.71875
train loss:  0.5536723136901855
train gradient:  0.16336474721079114
iteration : 8488
train acc:  0.7578125
train loss:  0.4719885587692261
train gradient:  0.12797562861953382
iteration : 8489
train acc:  0.703125
train loss:  0.5581470727920532
train gradient:  0.15208912278781112
iteration : 8490
train acc:  0.640625
train loss:  0.584479570388794
train gradient:  0.14017466106967588
iteration : 8491
train acc:  0.84375
train loss:  0.3533865511417389
train gradient:  0.0792910615791718
iteration : 8492
train acc:  0.6640625
train loss:  0.5475701093673706
train gradient:  0.1957428780483106
iteration : 8493
train acc:  0.765625
train loss:  0.46465253829956055
train gradient:  0.12920547153080142
iteration : 8494
train acc:  0.765625
train loss:  0.5263959169387817
train gradient:  0.15791702797874405
iteration : 8495
train acc:  0.7734375
train loss:  0.48514920473098755
train gradient:  0.12390072296348097
iteration : 8496
train acc:  0.7890625
train loss:  0.47524476051330566
train gradient:  0.10573497532427709
iteration : 8497
train acc:  0.78125
train loss:  0.44975754618644714
train gradient:  0.09188102503878677
iteration : 8498
train acc:  0.796875
train loss:  0.5229859352111816
train gradient:  0.14273226677659484
iteration : 8499
train acc:  0.7578125
train loss:  0.48175132274627686
train gradient:  0.1606497316437959
iteration : 8500
train acc:  0.703125
train loss:  0.5301775932312012
train gradient:  0.2029804270626049
iteration : 8501
train acc:  0.734375
train loss:  0.4530274271965027
train gradient:  0.10207070100069233
iteration : 8502
train acc:  0.765625
train loss:  0.4505839943885803
train gradient:  0.0877647506455747
iteration : 8503
train acc:  0.703125
train loss:  0.48496541380882263
train gradient:  0.1335246389333573
iteration : 8504
train acc:  0.734375
train loss:  0.4783105254173279
train gradient:  0.09932062298822845
iteration : 8505
train acc:  0.765625
train loss:  0.4855673313140869
train gradient:  0.10780622264217908
iteration : 8506
train acc:  0.75
train loss:  0.4711199998855591
train gradient:  0.11519648527251886
iteration : 8507
train acc:  0.78125
train loss:  0.47604671120643616
train gradient:  0.12224446388339084
iteration : 8508
train acc:  0.671875
train loss:  0.5412153005599976
train gradient:  0.16125735845427652
iteration : 8509
train acc:  0.7109375
train loss:  0.5237666368484497
train gradient:  0.12676666670325254
iteration : 8510
train acc:  0.71875
train loss:  0.46595728397369385
train gradient:  0.13920825372157694
iteration : 8511
train acc:  0.7421875
train loss:  0.5332595109939575
train gradient:  0.13681418269058726
iteration : 8512
train acc:  0.71875
train loss:  0.5432107448577881
train gradient:  0.19818224710401539
iteration : 8513
train acc:  0.734375
train loss:  0.5117140412330627
train gradient:  0.11029970495878696
iteration : 8514
train acc:  0.8046875
train loss:  0.4577723443508148
train gradient:  0.1110346868043567
iteration : 8515
train acc:  0.7890625
train loss:  0.42802730202674866
train gradient:  0.08334670414647216
iteration : 8516
train acc:  0.7421875
train loss:  0.4816153645515442
train gradient:  0.09100776708174392
iteration : 8517
train acc:  0.734375
train loss:  0.5017112493515015
train gradient:  0.15632021506154087
iteration : 8518
train acc:  0.7578125
train loss:  0.5029850602149963
train gradient:  0.14396756863642168
iteration : 8519
train acc:  0.7109375
train loss:  0.5316462516784668
train gradient:  0.15238190984797267
iteration : 8520
train acc:  0.7578125
train loss:  0.4934121072292328
train gradient:  0.11553980426907869
iteration : 8521
train acc:  0.7421875
train loss:  0.4711032509803772
train gradient:  0.1043745616518359
iteration : 8522
train acc:  0.6953125
train loss:  0.5223783850669861
train gradient:  0.18007265782235
iteration : 8523
train acc:  0.7265625
train loss:  0.49073052406311035
train gradient:  0.14920745053960333
iteration : 8524
train acc:  0.703125
train loss:  0.5294485092163086
train gradient:  0.12410276019169392
iteration : 8525
train acc:  0.7734375
train loss:  0.4571767747402191
train gradient:  0.10237368926347444
iteration : 8526
train acc:  0.796875
train loss:  0.4516026973724365
train gradient:  0.11752188698627468
iteration : 8527
train acc:  0.7109375
train loss:  0.5280312299728394
train gradient:  0.1303538661397235
iteration : 8528
train acc:  0.71875
train loss:  0.5157831311225891
train gradient:  0.14146437143224594
iteration : 8529
train acc:  0.671875
train loss:  0.5813795328140259
train gradient:  0.18556509713707559
iteration : 8530
train acc:  0.7109375
train loss:  0.5569461584091187
train gradient:  0.18070648386175392
iteration : 8531
train acc:  0.75
train loss:  0.4759916365146637
train gradient:  0.12166043305088896
iteration : 8532
train acc:  0.796875
train loss:  0.4411298334598541
train gradient:  0.0929872888238657
iteration : 8533
train acc:  0.640625
train loss:  0.535824179649353
train gradient:  0.14998362072341143
iteration : 8534
train acc:  0.75
train loss:  0.4959920048713684
train gradient:  0.11914328617254703
iteration : 8535
train acc:  0.703125
train loss:  0.5113454461097717
train gradient:  0.1633450730898917
iteration : 8536
train acc:  0.796875
train loss:  0.46461552381515503
train gradient:  0.10797705975556454
iteration : 8537
train acc:  0.7109375
train loss:  0.5715854167938232
train gradient:  0.1552628047321496
iteration : 8538
train acc:  0.828125
train loss:  0.41981029510498047
train gradient:  0.10674605516304553
iteration : 8539
train acc:  0.734375
train loss:  0.49227723479270935
train gradient:  0.09808481823504422
iteration : 8540
train acc:  0.6796875
train loss:  0.5402829647064209
train gradient:  0.13651437995510451
iteration : 8541
train acc:  0.6328125
train loss:  0.5491441488265991
train gradient:  0.13051667169780623
iteration : 8542
train acc:  0.7578125
train loss:  0.5337883234024048
train gradient:  0.13712873768097106
iteration : 8543
train acc:  0.734375
train loss:  0.5209066867828369
train gradient:  0.13609606568484695
iteration : 8544
train acc:  0.6484375
train loss:  0.5806655883789062
train gradient:  0.17514928683047581
iteration : 8545
train acc:  0.6484375
train loss:  0.5962624549865723
train gradient:  0.18929051812488806
iteration : 8546
train acc:  0.765625
train loss:  0.4785601794719696
train gradient:  0.12080328093128903
iteration : 8547
train acc:  0.7578125
train loss:  0.48496413230895996
train gradient:  0.11189305530687921
iteration : 8548
train acc:  0.765625
train loss:  0.4304773807525635
train gradient:  0.09894703209013354
iteration : 8549
train acc:  0.7578125
train loss:  0.438538521528244
train gradient:  0.10115919847552048
iteration : 8550
train acc:  0.75
train loss:  0.5164883136749268
train gradient:  0.12495007894915515
iteration : 8551
train acc:  0.6953125
train loss:  0.5185790061950684
train gradient:  0.1347684376384039
iteration : 8552
train acc:  0.7578125
train loss:  0.561791181564331
train gradient:  0.16289256779158112
iteration : 8553
train acc:  0.703125
train loss:  0.5269837975502014
train gradient:  0.15736827116598273
iteration : 8554
train acc:  0.7734375
train loss:  0.44986557960510254
train gradient:  0.09473841706037221
iteration : 8555
train acc:  0.71875
train loss:  0.4725953936576843
train gradient:  0.10006772240857932
iteration : 8556
train acc:  0.6484375
train loss:  0.5600109100341797
train gradient:  0.1795415736402582
iteration : 8557
train acc:  0.734375
train loss:  0.5073429346084595
train gradient:  0.15998263693577502
iteration : 8558
train acc:  0.6875
train loss:  0.5069506168365479
train gradient:  0.12995270834977674
iteration : 8559
train acc:  0.703125
train loss:  0.5277371406555176
train gradient:  0.12365393616862821
iteration : 8560
train acc:  0.7109375
train loss:  0.5100806355476379
train gradient:  0.1200477205816964
iteration : 8561
train acc:  0.7734375
train loss:  0.448432058095932
train gradient:  0.09418189861789542
iteration : 8562
train acc:  0.7109375
train loss:  0.5044647455215454
train gradient:  0.1210821958666373
iteration : 8563
train acc:  0.7265625
train loss:  0.5344181060791016
train gradient:  0.13087413932561165
iteration : 8564
train acc:  0.7578125
train loss:  0.5174204111099243
train gradient:  0.1699457554332385
iteration : 8565
train acc:  0.75
train loss:  0.45858681201934814
train gradient:  0.12475936281492454
iteration : 8566
train acc:  0.71875
train loss:  0.5341465473175049
train gradient:  0.15285190059612577
iteration : 8567
train acc:  0.703125
train loss:  0.5075907707214355
train gradient:  0.14583205734962446
iteration : 8568
train acc:  0.7734375
train loss:  0.5156724452972412
train gradient:  0.12467933087742983
iteration : 8569
train acc:  0.765625
train loss:  0.4551992416381836
train gradient:  0.1066327973540113
iteration : 8570
train acc:  0.71875
train loss:  0.5136162638664246
train gradient:  0.1466404735892519
iteration : 8571
train acc:  0.703125
train loss:  0.5097106099128723
train gradient:  0.1136715937739056
iteration : 8572
train acc:  0.71875
train loss:  0.5302160382270813
train gradient:  0.1482865238568406
iteration : 8573
train acc:  0.6875
train loss:  0.5935500264167786
train gradient:  0.15148846947488762
iteration : 8574
train acc:  0.6875
train loss:  0.5688503980636597
train gradient:  0.16530759240034537
iteration : 8575
train acc:  0.78125
train loss:  0.44427013397216797
train gradient:  0.11350666835979166
iteration : 8576
train acc:  0.6640625
train loss:  0.5690344572067261
train gradient:  0.18165291411469964
iteration : 8577
train acc:  0.8125
train loss:  0.4463195204734802
train gradient:  0.11356621545569182
iteration : 8578
train acc:  0.7421875
train loss:  0.48149651288986206
train gradient:  0.11760176382505365
iteration : 8579
train acc:  0.75
train loss:  0.4839775860309601
train gradient:  0.12423872293844473
iteration : 8580
train acc:  0.7890625
train loss:  0.44993114471435547
train gradient:  0.08779254007455041
iteration : 8581
train acc:  0.7890625
train loss:  0.4278060793876648
train gradient:  0.08852787615969279
iteration : 8582
train acc:  0.7421875
train loss:  0.5197013020515442
train gradient:  0.2197671365511477
iteration : 8583
train acc:  0.734375
train loss:  0.45870882272720337
train gradient:  0.1243672764181973
iteration : 8584
train acc:  0.703125
train loss:  0.4807533919811249
train gradient:  0.10841458945232214
iteration : 8585
train acc:  0.7734375
train loss:  0.4299536645412445
train gradient:  0.11843889831868445
iteration : 8586
train acc:  0.78125
train loss:  0.41732192039489746
train gradient:  0.09398415273796597
iteration : 8587
train acc:  0.7578125
train loss:  0.460828959941864
train gradient:  0.11702952095886615
iteration : 8588
train acc:  0.78125
train loss:  0.5167012810707092
train gradient:  0.13756133626367906
iteration : 8589
train acc:  0.6796875
train loss:  0.5061923265457153
train gradient:  0.10816826433812014
iteration : 8590
train acc:  0.71875
train loss:  0.4943954348564148
train gradient:  0.10564649477827953
iteration : 8591
train acc:  0.7890625
train loss:  0.4962828457355499
train gradient:  0.1279502487873994
iteration : 8592
train acc:  0.6796875
train loss:  0.6575312614440918
train gradient:  0.19668814770823315
iteration : 8593
train acc:  0.765625
train loss:  0.46278250217437744
train gradient:  0.11917539676074583
iteration : 8594
train acc:  0.75
train loss:  0.491172194480896
train gradient:  0.14672925987423258
iteration : 8595
train acc:  0.7109375
train loss:  0.47736090421676636
train gradient:  0.1254482956798346
iteration : 8596
train acc:  0.703125
train loss:  0.546438992023468
train gradient:  0.15236575723689866
iteration : 8597
train acc:  0.734375
train loss:  0.45909345149993896
train gradient:  0.11920155008925551
iteration : 8598
train acc:  0.8125
train loss:  0.4641413688659668
train gradient:  0.14986325997361583
iteration : 8599
train acc:  0.734375
train loss:  0.4813419580459595
train gradient:  0.12543781881044025
iteration : 8600
train acc:  0.6875
train loss:  0.5692312121391296
train gradient:  0.14373335073645332
iteration : 8601
train acc:  0.796875
train loss:  0.4685269296169281
train gradient:  0.09519043280532673
iteration : 8602
train acc:  0.7109375
train loss:  0.6030459403991699
train gradient:  0.2512674047953938
iteration : 8603
train acc:  0.7265625
train loss:  0.5591049194335938
train gradient:  0.2193086992331958
iteration : 8604
train acc:  0.703125
train loss:  0.49141812324523926
train gradient:  0.14891110145747238
iteration : 8605
train acc:  0.7578125
train loss:  0.4729282259941101
train gradient:  0.11571247863000722
iteration : 8606
train acc:  0.765625
train loss:  0.4807211756706238
train gradient:  0.11935275497970199
iteration : 8607
train acc:  0.7265625
train loss:  0.5035194158554077
train gradient:  0.11762063485224407
iteration : 8608
train acc:  0.7265625
train loss:  0.5379995107650757
train gradient:  0.11563121240474458
iteration : 8609
train acc:  0.734375
train loss:  0.5120570063591003
train gradient:  0.13218089690037377
iteration : 8610
train acc:  0.734375
train loss:  0.5136343836784363
train gradient:  0.1679198258646185
iteration : 8611
train acc:  0.71875
train loss:  0.5077992081642151
train gradient:  0.1492720345001534
iteration : 8612
train acc:  0.75
train loss:  0.4764770269393921
train gradient:  0.13359986914456073
iteration : 8613
train acc:  0.7578125
train loss:  0.4994550049304962
train gradient:  0.1400571135723338
iteration : 8614
train acc:  0.734375
train loss:  0.47750845551490784
train gradient:  0.11878568491871354
iteration : 8615
train acc:  0.6953125
train loss:  0.5529153347015381
train gradient:  0.15731424066585964
iteration : 8616
train acc:  0.8203125
train loss:  0.4612691104412079
train gradient:  0.1501608480217087
iteration : 8617
train acc:  0.796875
train loss:  0.45088937878608704
train gradient:  0.10677445876005147
iteration : 8618
train acc:  0.78125
train loss:  0.4681798219680786
train gradient:  0.11605547314090262
iteration : 8619
train acc:  0.75
train loss:  0.5245703458786011
train gradient:  0.147967141784254
iteration : 8620
train acc:  0.7734375
train loss:  0.4217708110809326
train gradient:  0.1011748761378608
iteration : 8621
train acc:  0.765625
train loss:  0.48953476548194885
train gradient:  0.13651042665514151
iteration : 8622
train acc:  0.7421875
train loss:  0.5140540599822998
train gradient:  0.13418593433400366
iteration : 8623
train acc:  0.7890625
train loss:  0.43058475852012634
train gradient:  0.10606030523050941
iteration : 8624
train acc:  0.7734375
train loss:  0.4889843761920929
train gradient:  0.11769792829069478
iteration : 8625
train acc:  0.703125
train loss:  0.5235421657562256
train gradient:  0.12640248732990955
iteration : 8626
train acc:  0.6875
train loss:  0.5536960363388062
train gradient:  0.15386216834439248
iteration : 8627
train acc:  0.7265625
train loss:  0.47553884983062744
train gradient:  0.10178518749497135
iteration : 8628
train acc:  0.7421875
train loss:  0.5035204887390137
train gradient:  0.17358622650216837
iteration : 8629
train acc:  0.734375
train loss:  0.4833971858024597
train gradient:  0.11026552874131941
iteration : 8630
train acc:  0.75
train loss:  0.49392372369766235
train gradient:  0.1201562597341011
iteration : 8631
train acc:  0.75
train loss:  0.48557811975479126
train gradient:  0.16260875830526794
iteration : 8632
train acc:  0.7578125
train loss:  0.5021212100982666
train gradient:  0.1366987748227645
iteration : 8633
train acc:  0.765625
train loss:  0.4921855926513672
train gradient:  0.10467098114229362
iteration : 8634
train acc:  0.7578125
train loss:  0.4647423028945923
train gradient:  0.107816015654565
iteration : 8635
train acc:  0.703125
train loss:  0.5498501062393188
train gradient:  0.1301222370374227
iteration : 8636
train acc:  0.78125
train loss:  0.5005465149879456
train gradient:  0.11556004347082147
iteration : 8637
train acc:  0.734375
train loss:  0.48007911443710327
train gradient:  0.1133709186906809
iteration : 8638
train acc:  0.7109375
train loss:  0.5477793216705322
train gradient:  0.14447118205910475
iteration : 8639
train acc:  0.75
train loss:  0.42853349447250366
train gradient:  0.09559939528232522
iteration : 8640
train acc:  0.7734375
train loss:  0.49159571528434753
train gradient:  0.12588722623283957
iteration : 8641
train acc:  0.7265625
train loss:  0.526752769947052
train gradient:  0.12904465171957064
iteration : 8642
train acc:  0.8203125
train loss:  0.4186888337135315
train gradient:  0.08044201747052517
iteration : 8643
train acc:  0.7578125
train loss:  0.5083482265472412
train gradient:  0.17321336144041272
iteration : 8644
train acc:  0.765625
train loss:  0.4988381862640381
train gradient:  0.12472240315335063
iteration : 8645
train acc:  0.78125
train loss:  0.4494708776473999
train gradient:  0.09027464798504038
iteration : 8646
train acc:  0.7734375
train loss:  0.4399859607219696
train gradient:  0.13033530407940347
iteration : 8647
train acc:  0.7421875
train loss:  0.5285366773605347
train gradient:  0.1260022617800285
iteration : 8648
train acc:  0.703125
train loss:  0.5536404848098755
train gradient:  0.15005751847016086
iteration : 8649
train acc:  0.78125
train loss:  0.5024613738059998
train gradient:  0.14816948158258517
iteration : 8650
train acc:  0.6796875
train loss:  0.5475234985351562
train gradient:  0.12442317821927515
iteration : 8651
train acc:  0.7265625
train loss:  0.47437745332717896
train gradient:  0.12609134777068032
iteration : 8652
train acc:  0.7578125
train loss:  0.49357709288597107
train gradient:  0.11276695433885509
iteration : 8653
train acc:  0.7109375
train loss:  0.5004420280456543
train gradient:  0.11779123484699153
iteration : 8654
train acc:  0.7421875
train loss:  0.504603385925293
train gradient:  0.11830366438239981
iteration : 8655
train acc:  0.7734375
train loss:  0.4900255799293518
train gradient:  0.15602084047737938
iteration : 8656
train acc:  0.6796875
train loss:  0.5237717628479004
train gradient:  0.15471103116851048
iteration : 8657
train acc:  0.71875
train loss:  0.5184026956558228
train gradient:  0.14134643962498894
iteration : 8658
train acc:  0.796875
train loss:  0.44988614320755005
train gradient:  0.09855391784809332
iteration : 8659
train acc:  0.734375
train loss:  0.4885976314544678
train gradient:  0.1312981306234642
iteration : 8660
train acc:  0.7578125
train loss:  0.4587423801422119
train gradient:  0.14031326469782224
iteration : 8661
train acc:  0.6875
train loss:  0.5429644584655762
train gradient:  0.16758677335132666
iteration : 8662
train acc:  0.7109375
train loss:  0.4932858347892761
train gradient:  0.1076550034180521
iteration : 8663
train acc:  0.75
train loss:  0.4851473569869995
train gradient:  0.13295239005315168
iteration : 8664
train acc:  0.7734375
train loss:  0.49802297353744507
train gradient:  0.17111439531071235
iteration : 8665
train acc:  0.6953125
train loss:  0.6048936247825623
train gradient:  0.15633738732810004
iteration : 8666
train acc:  0.75
train loss:  0.45427846908569336
train gradient:  0.09444169263253176
iteration : 8667
train acc:  0.78125
train loss:  0.4732685685157776
train gradient:  0.11289708550307877
iteration : 8668
train acc:  0.71875
train loss:  0.5284938216209412
train gradient:  0.14132105018326913
iteration : 8669
train acc:  0.6796875
train loss:  0.5752662420272827
train gradient:  0.14587566975279287
iteration : 8670
train acc:  0.828125
train loss:  0.3951062858104706
train gradient:  0.09596835945027059
iteration : 8671
train acc:  0.78125
train loss:  0.46397513151168823
train gradient:  0.11556857810041658
iteration : 8672
train acc:  0.78125
train loss:  0.46549105644226074
train gradient:  0.09482766163588065
iteration : 8673
train acc:  0.78125
train loss:  0.4746916890144348
train gradient:  0.12246951852236147
iteration : 8674
train acc:  0.734375
train loss:  0.5431768894195557
train gradient:  0.17809204904097148
iteration : 8675
train acc:  0.734375
train loss:  0.5134584903717041
train gradient:  0.1288795580472351
iteration : 8676
train acc:  0.6953125
train loss:  0.5341075658798218
train gradient:  0.14116271516136814
iteration : 8677
train acc:  0.7734375
train loss:  0.4889644384384155
train gradient:  0.131622389473333
iteration : 8678
train acc:  0.7265625
train loss:  0.5197570323944092
train gradient:  0.12762063345988878
iteration : 8679
train acc:  0.8046875
train loss:  0.46311113238334656
train gradient:  0.11565968502650234
iteration : 8680
train acc:  0.7734375
train loss:  0.4732177257537842
train gradient:  0.1213617369822448
iteration : 8681
train acc:  0.7578125
train loss:  0.49515020847320557
train gradient:  0.15610796770683133
iteration : 8682
train acc:  0.6640625
train loss:  0.5738047957420349
train gradient:  0.19622946747045694
iteration : 8683
train acc:  0.671875
train loss:  0.5588791966438293
train gradient:  0.1605836414282879
iteration : 8684
train acc:  0.734375
train loss:  0.4750215411186218
train gradient:  0.10533665807500447
iteration : 8685
train acc:  0.765625
train loss:  0.517812192440033
train gradient:  0.1229554314203925
iteration : 8686
train acc:  0.6875
train loss:  0.5359002947807312
train gradient:  0.13469298215336292
iteration : 8687
train acc:  0.7421875
train loss:  0.5572758913040161
train gradient:  0.12482838453262259
iteration : 8688
train acc:  0.71875
train loss:  0.5178534388542175
train gradient:  0.15323190705928263
iteration : 8689
train acc:  0.7421875
train loss:  0.5147742033004761
train gradient:  0.13684344450221247
iteration : 8690
train acc:  0.7734375
train loss:  0.49875620007514954
train gradient:  0.16087162919710984
iteration : 8691
train acc:  0.765625
train loss:  0.4992828965187073
train gradient:  0.1181276049925203
iteration : 8692
train acc:  0.703125
train loss:  0.5362104773521423
train gradient:  0.14316089120148523
iteration : 8693
train acc:  0.8046875
train loss:  0.46242010593414307
train gradient:  0.15668618360385755
iteration : 8694
train acc:  0.7109375
train loss:  0.5117950439453125
train gradient:  0.13388754520931528
iteration : 8695
train acc:  0.7578125
train loss:  0.4870758652687073
train gradient:  0.12119325530540427
iteration : 8696
train acc:  0.796875
train loss:  0.4583641290664673
train gradient:  0.0959637574446841
iteration : 8697
train acc:  0.75
train loss:  0.4478306770324707
train gradient:  0.0947778638044179
iteration : 8698
train acc:  0.7890625
train loss:  0.4740245044231415
train gradient:  0.10694065528406342
iteration : 8699
train acc:  0.7265625
train loss:  0.48320484161376953
train gradient:  0.10910447870713771
iteration : 8700
train acc:  0.7578125
train loss:  0.5072574615478516
train gradient:  0.1326081514053164
iteration : 8701
train acc:  0.6953125
train loss:  0.5949842929840088
train gradient:  0.16617823864786158
iteration : 8702
train acc:  0.734375
train loss:  0.5392224788665771
train gradient:  0.13645734367191004
iteration : 8703
train acc:  0.7890625
train loss:  0.41044408082962036
train gradient:  0.08615177990223152
iteration : 8704
train acc:  0.734375
train loss:  0.5267706513404846
train gradient:  0.12529750406713996
iteration : 8705
train acc:  0.71875
train loss:  0.5375074744224548
train gradient:  0.11233811199766326
iteration : 8706
train acc:  0.7734375
train loss:  0.4898141920566559
train gradient:  0.11719635502049808
iteration : 8707
train acc:  0.734375
train loss:  0.5019145011901855
train gradient:  0.13837975238695815
iteration : 8708
train acc:  0.71875
train loss:  0.5633896589279175
train gradient:  0.12849820492695457
iteration : 8709
train acc:  0.71875
train loss:  0.5038759708404541
train gradient:  0.17329483064485354
iteration : 8710
train acc:  0.7265625
train loss:  0.47126567363739014
train gradient:  0.10907062719222467
iteration : 8711
train acc:  0.734375
train loss:  0.5118588805198669
train gradient:  0.1656673511214633
iteration : 8712
train acc:  0.6875
train loss:  0.5183224678039551
train gradient:  0.2222496532862954
iteration : 8713
train acc:  0.7578125
train loss:  0.46508029103279114
train gradient:  0.10125654914574472
iteration : 8714
train acc:  0.7734375
train loss:  0.48400160670280457
train gradient:  0.09344087613965733
iteration : 8715
train acc:  0.7890625
train loss:  0.4584260880947113
train gradient:  0.11841737232225881
iteration : 8716
train acc:  0.7109375
train loss:  0.533280074596405
train gradient:  0.15789783400933888
iteration : 8717
train acc:  0.765625
train loss:  0.4964035451412201
train gradient:  0.15850303677958785
iteration : 8718
train acc:  0.7578125
train loss:  0.4249517023563385
train gradient:  0.09690939688794402
iteration : 8719
train acc:  0.703125
train loss:  0.5536848306655884
train gradient:  0.14655252924531115
iteration : 8720
train acc:  0.8046875
train loss:  0.45975908637046814
train gradient:  0.10437324439145629
iteration : 8721
train acc:  0.7734375
train loss:  0.427837073802948
train gradient:  0.08803280838886099
iteration : 8722
train acc:  0.7734375
train loss:  0.44732093811035156
train gradient:  0.09765871262913677
iteration : 8723
train acc:  0.703125
train loss:  0.4977390766143799
train gradient:  0.10055396264713264
iteration : 8724
train acc:  0.7578125
train loss:  0.48214462399482727
train gradient:  0.11428093619518033
iteration : 8725
train acc:  0.8046875
train loss:  0.44742944836616516
train gradient:  0.11161448622658053
iteration : 8726
train acc:  0.7421875
train loss:  0.4550759494304657
train gradient:  0.10322136547577554
iteration : 8727
train acc:  0.6640625
train loss:  0.6271458864212036
train gradient:  0.18961047953241467
iteration : 8728
train acc:  0.7421875
train loss:  0.5238667130470276
train gradient:  0.12364466053232101
iteration : 8729
train acc:  0.765625
train loss:  0.48046809434890747
train gradient:  0.11460736615127838
iteration : 8730
train acc:  0.78125
train loss:  0.4627419114112854
train gradient:  0.10219132286148416
iteration : 8731
train acc:  0.7578125
train loss:  0.5361909866333008
train gradient:  0.18068398397389993
iteration : 8732
train acc:  0.71875
train loss:  0.5127657055854797
train gradient:  0.14497292181468352
iteration : 8733
train acc:  0.7890625
train loss:  0.4216323494911194
train gradient:  0.1283417073157567
iteration : 8734
train acc:  0.7109375
train loss:  0.5018126368522644
train gradient:  0.13041511202105557
iteration : 8735
train acc:  0.7890625
train loss:  0.5140377283096313
train gradient:  0.1579940924795571
iteration : 8736
train acc:  0.75
train loss:  0.534243643283844
train gradient:  0.16717562070598693
iteration : 8737
train acc:  0.7578125
train loss:  0.4761134088039398
train gradient:  0.11377738900126719
iteration : 8738
train acc:  0.734375
train loss:  0.4871917963027954
train gradient:  0.09632812180058942
iteration : 8739
train acc:  0.6796875
train loss:  0.5400580167770386
train gradient:  0.13622846482350254
iteration : 8740
train acc:  0.7421875
train loss:  0.46970582008361816
train gradient:  0.11509284274060529
iteration : 8741
train acc:  0.734375
train loss:  0.5520331859588623
train gradient:  0.13824790762844813
iteration : 8742
train acc:  0.6875
train loss:  0.5212863087654114
train gradient:  0.1270535876517495
iteration : 8743
train acc:  0.734375
train loss:  0.5047469139099121
train gradient:  0.1569530070850628
iteration : 8744
train acc:  0.7734375
train loss:  0.4300037622451782
train gradient:  0.12377935499124929
iteration : 8745
train acc:  0.6640625
train loss:  0.5722975730895996
train gradient:  0.16842185701038548
iteration : 8746
train acc:  0.703125
train loss:  0.48338258266448975
train gradient:  0.11590092564457391
iteration : 8747
train acc:  0.8125
train loss:  0.42972463369369507
train gradient:  0.10820508773727841
iteration : 8748
train acc:  0.71875
train loss:  0.5051273107528687
train gradient:  0.15158164564320714
iteration : 8749
train acc:  0.734375
train loss:  0.518775224685669
train gradient:  0.1287926129469587
iteration : 8750
train acc:  0.7421875
train loss:  0.5260743498802185
train gradient:  0.11765932113085029
iteration : 8751
train acc:  0.7578125
train loss:  0.4583709239959717
train gradient:  0.10407150252730554
iteration : 8752
train acc:  0.7578125
train loss:  0.47490304708480835
train gradient:  0.12198538498646841
iteration : 8753
train acc:  0.765625
train loss:  0.5055660009384155
train gradient:  0.14289925365733383
iteration : 8754
train acc:  0.7421875
train loss:  0.5399094820022583
train gradient:  0.17198710070353568
iteration : 8755
train acc:  0.7265625
train loss:  0.5072863101959229
train gradient:  0.13421468585479152
iteration : 8756
train acc:  0.703125
train loss:  0.5436434745788574
train gradient:  0.17941028880133514
iteration : 8757
train acc:  0.84375
train loss:  0.38818803429603577
train gradient:  0.08633236789231823
iteration : 8758
train acc:  0.796875
train loss:  0.46645891666412354
train gradient:  0.10075696011225907
iteration : 8759
train acc:  0.7421875
train loss:  0.49963700771331787
train gradient:  0.13904911634866235
iteration : 8760
train acc:  0.7421875
train loss:  0.4569430947303772
train gradient:  0.10332563327138813
iteration : 8761
train acc:  0.6875
train loss:  0.5373599529266357
train gradient:  0.1322815118999911
iteration : 8762
train acc:  0.765625
train loss:  0.4951193928718567
train gradient:  0.1091549407909705
iteration : 8763
train acc:  0.734375
train loss:  0.4673880338668823
train gradient:  0.11538117724799118
iteration : 8764
train acc:  0.6953125
train loss:  0.5092982053756714
train gradient:  0.13164235246280578
iteration : 8765
train acc:  0.703125
train loss:  0.5220174789428711
train gradient:  0.14311604297819325
iteration : 8766
train acc:  0.75
train loss:  0.5375142097473145
train gradient:  0.1471018640341395
iteration : 8767
train acc:  0.7421875
train loss:  0.5422982573509216
train gradient:  0.12261807006392678
iteration : 8768
train acc:  0.7890625
train loss:  0.46196234226226807
train gradient:  0.13890210115055493
iteration : 8769
train acc:  0.734375
train loss:  0.5151267051696777
train gradient:  0.16265297990261407
iteration : 8770
train acc:  0.7421875
train loss:  0.5140935182571411
train gradient:  0.15463469439320549
iteration : 8771
train acc:  0.78125
train loss:  0.4525470733642578
train gradient:  0.10721675782908423
iteration : 8772
train acc:  0.6796875
train loss:  0.5056948661804199
train gradient:  0.11558720030691694
iteration : 8773
train acc:  0.78125
train loss:  0.5018872022628784
train gradient:  0.12100564627719367
iteration : 8774
train acc:  0.734375
train loss:  0.47708871960639954
train gradient:  0.11683443272696464
iteration : 8775
train acc:  0.7734375
train loss:  0.44514238834381104
train gradient:  0.15813326654730053
iteration : 8776
train acc:  0.78125
train loss:  0.49348682165145874
train gradient:  0.11083039006924741
iteration : 8777
train acc:  0.71875
train loss:  0.4684521555900574
train gradient:  0.10841914255543492
iteration : 8778
train acc:  0.71875
train loss:  0.5396372675895691
train gradient:  0.17149132992040408
iteration : 8779
train acc:  0.7578125
train loss:  0.4797312915325165
train gradient:  0.1171284370635945
iteration : 8780
train acc:  0.6875
train loss:  0.5368850827217102
train gradient:  0.13324436715502463
iteration : 8781
train acc:  0.71875
train loss:  0.5116453170776367
train gradient:  0.11080457181412082
iteration : 8782
train acc:  0.6875
train loss:  0.5073267221450806
train gradient:  0.12561765681053944
iteration : 8783
train acc:  0.7890625
train loss:  0.41119879484176636
train gradient:  0.11002856953397458
iteration : 8784
train acc:  0.7734375
train loss:  0.5424784421920776
train gradient:  0.13213362518731955
iteration : 8785
train acc:  0.7734375
train loss:  0.45304954051971436
train gradient:  0.11142276488431697
iteration : 8786
train acc:  0.703125
train loss:  0.5087499618530273
train gradient:  0.14721513733471794
iteration : 8787
train acc:  0.7578125
train loss:  0.4990924596786499
train gradient:  0.12251655492044704
iteration : 8788
train acc:  0.7890625
train loss:  0.4316987097263336
train gradient:  0.1079751361653642
iteration : 8789
train acc:  0.7421875
train loss:  0.5105518102645874
train gradient:  0.11134042634951208
iteration : 8790
train acc:  0.7109375
train loss:  0.5016319751739502
train gradient:  0.09350358682158592
iteration : 8791
train acc:  0.7265625
train loss:  0.5116496682167053
train gradient:  0.12507337109406985
iteration : 8792
train acc:  0.765625
train loss:  0.5138030052185059
train gradient:  0.14118216130438047
iteration : 8793
train acc:  0.6953125
train loss:  0.5478712320327759
train gradient:  0.14860901057339576
iteration : 8794
train acc:  0.7734375
train loss:  0.4298800826072693
train gradient:  0.10859174670616271
iteration : 8795
train acc:  0.734375
train loss:  0.46112483739852905
train gradient:  0.13043645596451875
iteration : 8796
train acc:  0.671875
train loss:  0.54411381483078
train gradient:  0.14623560936845065
iteration : 8797
train acc:  0.78125
train loss:  0.46566322445869446
train gradient:  0.10120287932550545
iteration : 8798
train acc:  0.65625
train loss:  0.5739331841468811
train gradient:  0.15629799034654834
iteration : 8799
train acc:  0.7265625
train loss:  0.5438703298568726
train gradient:  0.1627367053096011
iteration : 8800
train acc:  0.7109375
train loss:  0.5224146246910095
train gradient:  0.15416863609377007
iteration : 8801
train acc:  0.75
train loss:  0.4744652509689331
train gradient:  0.10583263453782467
iteration : 8802
train acc:  0.703125
train loss:  0.5545289516448975
train gradient:  0.13904184062841835
iteration : 8803
train acc:  0.7578125
train loss:  0.45422208309173584
train gradient:  0.11107992620483553
iteration : 8804
train acc:  0.765625
train loss:  0.49803322553634644
train gradient:  0.11875206852265222
iteration : 8805
train acc:  0.7421875
train loss:  0.48577409982681274
train gradient:  0.10886891844003407
iteration : 8806
train acc:  0.7734375
train loss:  0.4892643690109253
train gradient:  0.10097688873372673
iteration : 8807
train acc:  0.765625
train loss:  0.438455730676651
train gradient:  0.09232585007035694
iteration : 8808
train acc:  0.71875
train loss:  0.48677945137023926
train gradient:  0.12456436285305117
iteration : 8809
train acc:  0.8046875
train loss:  0.4500701129436493
train gradient:  0.11099157089916391
iteration : 8810
train acc:  0.7265625
train loss:  0.5156341791152954
train gradient:  0.12128236356400725
iteration : 8811
train acc:  0.703125
train loss:  0.5392476916313171
train gradient:  0.12688424592100306
iteration : 8812
train acc:  0.8046875
train loss:  0.4330804944038391
train gradient:  0.09222482513477544
iteration : 8813
train acc:  0.7265625
train loss:  0.5009708404541016
train gradient:  0.1642514814973892
iteration : 8814
train acc:  0.6328125
train loss:  0.672871470451355
train gradient:  0.23763822500472892
iteration : 8815
train acc:  0.7578125
train loss:  0.4560578465461731
train gradient:  0.13587434103025745
iteration : 8816
train acc:  0.6953125
train loss:  0.5834153890609741
train gradient:  0.16767362047521644
iteration : 8817
train acc:  0.703125
train loss:  0.4929483234882355
train gradient:  0.13097866800321167
iteration : 8818
train acc:  0.734375
train loss:  0.47596296668052673
train gradient:  0.1185906858996483
iteration : 8819
train acc:  0.75
train loss:  0.472568154335022
train gradient:  0.12764775274482804
iteration : 8820
train acc:  0.78125
train loss:  0.46290212869644165
train gradient:  0.1431819921698695
iteration : 8821
train acc:  0.6875
train loss:  0.537335991859436
train gradient:  0.14475823291266168
iteration : 8822
train acc:  0.7109375
train loss:  0.523283064365387
train gradient:  0.14756528291244814
iteration : 8823
train acc:  0.7578125
train loss:  0.4962523877620697
train gradient:  0.10212879430444542
iteration : 8824
train acc:  0.7109375
train loss:  0.4899871051311493
train gradient:  0.1401341729101465
iteration : 8825
train acc:  0.6875
train loss:  0.531867265701294
train gradient:  0.1320206269217612
iteration : 8826
train acc:  0.734375
train loss:  0.514253556728363
train gradient:  0.14093981907563674
iteration : 8827
train acc:  0.8125
train loss:  0.4236341118812561
train gradient:  0.10948743402613313
iteration : 8828
train acc:  0.7265625
train loss:  0.5005536079406738
train gradient:  0.12219640039595564
iteration : 8829
train acc:  0.75
train loss:  0.4838617444038391
train gradient:  0.16070897135314202
iteration : 8830
train acc:  0.75
train loss:  0.5363507270812988
train gradient:  0.14714462275997647
iteration : 8831
train acc:  0.7578125
train loss:  0.4416021704673767
train gradient:  0.09272579136102019
iteration : 8832
train acc:  0.6796875
train loss:  0.538611650466919
train gradient:  0.13467027266916787
iteration : 8833
train acc:  0.75
train loss:  0.4947734475135803
train gradient:  0.1272635250428879
iteration : 8834
train acc:  0.703125
train loss:  0.5698016285896301
train gradient:  0.16396622215655923
iteration : 8835
train acc:  0.6953125
train loss:  0.5463522672653198
train gradient:  0.22308783613826438
iteration : 8836
train acc:  0.671875
train loss:  0.5404151678085327
train gradient:  0.13906109291355695
iteration : 8837
train acc:  0.7265625
train loss:  0.4689207673072815
train gradient:  0.1275578184221794
iteration : 8838
train acc:  0.71875
train loss:  0.5292673110961914
train gradient:  0.1376769422169659
iteration : 8839
train acc:  0.75
train loss:  0.4727005362510681
train gradient:  0.13275770409851423
iteration : 8840
train acc:  0.765625
train loss:  0.4689677357673645
train gradient:  0.10664313267161174
iteration : 8841
train acc:  0.71875
train loss:  0.49930351972579956
train gradient:  0.12862405478845962
iteration : 8842
train acc:  0.7578125
train loss:  0.5219387412071228
train gradient:  0.14318199570354873
iteration : 8843
train acc:  0.7421875
train loss:  0.49598270654678345
train gradient:  0.1608034758239828
iteration : 8844
train acc:  0.6796875
train loss:  0.5402944087982178
train gradient:  0.13877641517907524
iteration : 8845
train acc:  0.828125
train loss:  0.41065144538879395
train gradient:  0.10531364185326525
iteration : 8846
train acc:  0.7578125
train loss:  0.5350176095962524
train gradient:  0.12622894690317016
iteration : 8847
train acc:  0.734375
train loss:  0.531822919845581
train gradient:  0.15598761144816164
iteration : 8848
train acc:  0.7734375
train loss:  0.45521432161331177
train gradient:  0.10445000754305639
iteration : 8849
train acc:  0.6953125
train loss:  0.6040453910827637
train gradient:  0.18032985461381673
iteration : 8850
train acc:  0.7109375
train loss:  0.5786302089691162
train gradient:  0.1847805163357636
iteration : 8851
train acc:  0.6875
train loss:  0.5416445136070251
train gradient:  0.18794738283723578
iteration : 8852
train acc:  0.65625
train loss:  0.5671368837356567
train gradient:  0.2197429026258129
iteration : 8853
train acc:  0.7109375
train loss:  0.4761374890804291
train gradient:  0.11625526889155544
iteration : 8854
train acc:  0.671875
train loss:  0.5644916296005249
train gradient:  0.1825516139516823
iteration : 8855
train acc:  0.6875
train loss:  0.5333700180053711
train gradient:  0.20344987783683383
iteration : 8856
train acc:  0.7578125
train loss:  0.5265799164772034
train gradient:  0.13240505057903812
iteration : 8857
train acc:  0.75
train loss:  0.5158792734146118
train gradient:  0.1532063342649988
iteration : 8858
train acc:  0.7578125
train loss:  0.46842923760414124
train gradient:  0.09678561633214429
iteration : 8859
train acc:  0.796875
train loss:  0.4769187271595001
train gradient:  0.10705859097275361
iteration : 8860
train acc:  0.7734375
train loss:  0.4600280225276947
train gradient:  0.11557509130803234
iteration : 8861
train acc:  0.7578125
train loss:  0.44614696502685547
train gradient:  0.10061488084932864
iteration : 8862
train acc:  0.6796875
train loss:  0.5514260530471802
train gradient:  0.14156985943799072
iteration : 8863
train acc:  0.71875
train loss:  0.5106568336486816
train gradient:  0.11996078325555933
iteration : 8864
train acc:  0.7421875
train loss:  0.42812103033065796
train gradient:  0.09105634742393087
iteration : 8865
train acc:  0.6953125
train loss:  0.5522751808166504
train gradient:  0.1459884888276518
iteration : 8866
train acc:  0.6640625
train loss:  0.5331758260726929
train gradient:  0.10475759483543304
iteration : 8867
train acc:  0.75
train loss:  0.5484760999679565
train gradient:  0.14776842664675313
iteration : 8868
train acc:  0.75
train loss:  0.4941402077674866
train gradient:  0.14343848384872315
iteration : 8869
train acc:  0.78125
train loss:  0.43321895599365234
train gradient:  0.09845723972682091
iteration : 8870
train acc:  0.71875
train loss:  0.5381568074226379
train gradient:  0.13771982120162124
iteration : 8871
train acc:  0.734375
train loss:  0.4797383248806
train gradient:  0.1299403286845094
iteration : 8872
train acc:  0.703125
train loss:  0.4898921847343445
train gradient:  0.13136572995113413
iteration : 8873
train acc:  0.6875
train loss:  0.5924111604690552
train gradient:  0.1478801855633488
iteration : 8874
train acc:  0.78125
train loss:  0.465626984834671
train gradient:  0.10282720242747767
iteration : 8875
train acc:  0.7421875
train loss:  0.4929502010345459
train gradient:  0.11887197697693183
iteration : 8876
train acc:  0.703125
train loss:  0.5022330284118652
train gradient:  0.11810111025261277
iteration : 8877
train acc:  0.75
train loss:  0.4598465859889984
train gradient:  0.0882689750942226
iteration : 8878
train acc:  0.7578125
train loss:  0.47644224762916565
train gradient:  0.11366194167279754
iteration : 8879
train acc:  0.75
train loss:  0.47683802247047424
train gradient:  0.10650171628437796
iteration : 8880
train acc:  0.8046875
train loss:  0.5204272866249084
train gradient:  0.14762006039321193
iteration : 8881
train acc:  0.7578125
train loss:  0.47052961587905884
train gradient:  0.0986996109557407
iteration : 8882
train acc:  0.734375
train loss:  0.4705228805541992
train gradient:  0.10405890134847565
iteration : 8883
train acc:  0.7578125
train loss:  0.5020318031311035
train gradient:  0.11997166381861689
iteration : 8884
train acc:  0.734375
train loss:  0.5102707147598267
train gradient:  0.1333703177912629
iteration : 8885
train acc:  0.703125
train loss:  0.5544987916946411
train gradient:  0.14574918664600384
iteration : 8886
train acc:  0.71875
train loss:  0.5138683915138245
train gradient:  0.12228289601703375
iteration : 8887
train acc:  0.71875
train loss:  0.48959076404571533
train gradient:  0.12557759901388435
iteration : 8888
train acc:  0.765625
train loss:  0.4379158616065979
train gradient:  0.11233218147957483
iteration : 8889
train acc:  0.7265625
train loss:  0.48795950412750244
train gradient:  0.1272311064668607
iteration : 8890
train acc:  0.78125
train loss:  0.47065454721450806
train gradient:  0.11679247983482807
iteration : 8891
train acc:  0.6953125
train loss:  0.5461758375167847
train gradient:  0.1718711710654741
iteration : 8892
train acc:  0.703125
train loss:  0.5195879340171814
train gradient:  0.12640834250001687
iteration : 8893
train acc:  0.71875
train loss:  0.4917113184928894
train gradient:  0.15371081273245407
iteration : 8894
train acc:  0.75
train loss:  0.48807454109191895
train gradient:  0.12284785166516181
iteration : 8895
train acc:  0.6875
train loss:  0.5327844619750977
train gradient:  0.14037516012465434
iteration : 8896
train acc:  0.7578125
train loss:  0.44561493396759033
train gradient:  0.10890802614806123
iteration : 8897
train acc:  0.765625
train loss:  0.5350080728530884
train gradient:  0.20847757802567674
iteration : 8898
train acc:  0.7734375
train loss:  0.45818692445755005
train gradient:  0.10183955447630391
iteration : 8899
train acc:  0.71875
train loss:  0.5649787783622742
train gradient:  0.16661862401944305
iteration : 8900
train acc:  0.7578125
train loss:  0.4730173945426941
train gradient:  0.14337942408466509
iteration : 8901
train acc:  0.6953125
train loss:  0.569247841835022
train gradient:  0.142112900865714
iteration : 8902
train acc:  0.7890625
train loss:  0.45879748463630676
train gradient:  0.14096403360380214
iteration : 8903
train acc:  0.765625
train loss:  0.4584774374961853
train gradient:  0.09316778475268847
iteration : 8904
train acc:  0.71875
train loss:  0.5326593518257141
train gradient:  0.13370010264270077
iteration : 8905
train acc:  0.7734375
train loss:  0.45865631103515625
train gradient:  0.11701666910249103
iteration : 8906
train acc:  0.78125
train loss:  0.4341581463813782
train gradient:  0.10061052024978824
iteration : 8907
train acc:  0.6953125
train loss:  0.5566879510879517
train gradient:  0.14310123679666048
iteration : 8908
train acc:  0.8359375
train loss:  0.4072837233543396
train gradient:  0.1139391645983556
iteration : 8909
train acc:  0.7265625
train loss:  0.4883559048175812
train gradient:  0.12421800079271109
iteration : 8910
train acc:  0.71875
train loss:  0.5052398443222046
train gradient:  0.13082799742832296
iteration : 8911
train acc:  0.65625
train loss:  0.606521487236023
train gradient:  0.15983271605669797
iteration : 8912
train acc:  0.7578125
train loss:  0.4546952247619629
train gradient:  0.12027766061541481
iteration : 8913
train acc:  0.703125
train loss:  0.5293904542922974
train gradient:  0.17399071264565757
iteration : 8914
train acc:  0.8203125
train loss:  0.41957467794418335
train gradient:  0.10616226584420328
iteration : 8915
train acc:  0.75
train loss:  0.4612974226474762
train gradient:  0.12145063410632287
iteration : 8916
train acc:  0.8046875
train loss:  0.44199493527412415
train gradient:  0.10941874032408545
iteration : 8917
train acc:  0.6875
train loss:  0.5311479568481445
train gradient:  0.17153643083445835
iteration : 8918
train acc:  0.7421875
train loss:  0.44879135489463806
train gradient:  0.10839654990918038
iteration : 8919
train acc:  0.75
train loss:  0.4578910768032074
train gradient:  0.12803341853953265
iteration : 8920
train acc:  0.8203125
train loss:  0.4408976435661316
train gradient:  0.09098401300522616
iteration : 8921
train acc:  0.7578125
train loss:  0.5059858560562134
train gradient:  0.11546679836393296
iteration : 8922
train acc:  0.7578125
train loss:  0.45687973499298096
train gradient:  0.09323566805947059
iteration : 8923
train acc:  0.7734375
train loss:  0.5141113996505737
train gradient:  0.119278077591065
iteration : 8924
train acc:  0.734375
train loss:  0.4885699450969696
train gradient:  0.10818746878314882
iteration : 8925
train acc:  0.7421875
train loss:  0.5059205889701843
train gradient:  0.1754994946480099
iteration : 8926
train acc:  0.7734375
train loss:  0.48256343603134155
train gradient:  0.12363267208561633
iteration : 8927
train acc:  0.734375
train loss:  0.5673952102661133
train gradient:  0.14340904171116658
iteration : 8928
train acc:  0.640625
train loss:  0.5733183026313782
train gradient:  0.18234005028938743
iteration : 8929
train acc:  0.765625
train loss:  0.49711599946022034
train gradient:  0.1494136474465484
iteration : 8930
train acc:  0.703125
train loss:  0.5310793519020081
train gradient:  0.1298182547054073
iteration : 8931
train acc:  0.671875
train loss:  0.5681061744689941
train gradient:  0.13296319069757054
iteration : 8932
train acc:  0.7578125
train loss:  0.4977293312549591
train gradient:  0.13065493121297084
iteration : 8933
train acc:  0.7890625
train loss:  0.4356590509414673
train gradient:  0.09081908629930571
iteration : 8934
train acc:  0.7578125
train loss:  0.48384979367256165
train gradient:  0.1101928913470933
iteration : 8935
train acc:  0.7578125
train loss:  0.469865620136261
train gradient:  0.13160284042626416
iteration : 8936
train acc:  0.734375
train loss:  0.4936460256576538
train gradient:  0.11813799390842689
iteration : 8937
train acc:  0.7578125
train loss:  0.504869818687439
train gradient:  0.12915278604696018
iteration : 8938
train acc:  0.765625
train loss:  0.4921204447746277
train gradient:  0.11004504571433014
iteration : 8939
train acc:  0.6640625
train loss:  0.598659336566925
train gradient:  0.17218347664641515
iteration : 8940
train acc:  0.6953125
train loss:  0.5447350740432739
train gradient:  0.13510303007975305
iteration : 8941
train acc:  0.8046875
train loss:  0.4281959533691406
train gradient:  0.12947176969492302
iteration : 8942
train acc:  0.7265625
train loss:  0.47615835070610046
train gradient:  0.10958612067196861
iteration : 8943
train acc:  0.78125
train loss:  0.4415806233882904
train gradient:  0.11963504459342415
iteration : 8944
train acc:  0.75
train loss:  0.4697367548942566
train gradient:  0.1014099938895612
iteration : 8945
train acc:  0.75
train loss:  0.5117576718330383
train gradient:  0.18027085098141193
iteration : 8946
train acc:  0.78125
train loss:  0.4249805510044098
train gradient:  0.10866141334448128
iteration : 8947
train acc:  0.7265625
train loss:  0.46505671739578247
train gradient:  0.09453432195998708
iteration : 8948
train acc:  0.765625
train loss:  0.4470975697040558
train gradient:  0.11134274667220256
iteration : 8949
train acc:  0.7890625
train loss:  0.4477730393409729
train gradient:  0.12463390234992427
iteration : 8950
train acc:  0.7421875
train loss:  0.5145644545555115
train gradient:  0.1681314879108385
iteration : 8951
train acc:  0.796875
train loss:  0.4552065134048462
train gradient:  0.11658991530518838
iteration : 8952
train acc:  0.671875
train loss:  0.5699276924133301
train gradient:  0.14036483479017647
iteration : 8953
train acc:  0.7734375
train loss:  0.4818871319293976
train gradient:  0.16235997970533247
iteration : 8954
train acc:  0.71875
train loss:  0.5180259943008423
train gradient:  0.11443096910198268
iteration : 8955
train acc:  0.7265625
train loss:  0.526695966720581
train gradient:  0.1557569637071604
iteration : 8956
train acc:  0.734375
train loss:  0.5205113291740417
train gradient:  0.14883142245297212
iteration : 8957
train acc:  0.734375
train loss:  0.4986622631549835
train gradient:  0.14777558842964816
iteration : 8958
train acc:  0.7421875
train loss:  0.4658855199813843
train gradient:  0.1145207416853494
iteration : 8959
train acc:  0.671875
train loss:  0.5927910804748535
train gradient:  0.17334426638183686
iteration : 8960
train acc:  0.7578125
train loss:  0.46164679527282715
train gradient:  0.09862040316782153
iteration : 8961
train acc:  0.7421875
train loss:  0.4627039432525635
train gradient:  0.13157902522495146
iteration : 8962
train acc:  0.75
train loss:  0.5081645250320435
train gradient:  0.18904292646476534
iteration : 8963
train acc:  0.8125
train loss:  0.4207976460456848
train gradient:  0.0882689338384872
iteration : 8964
train acc:  0.7109375
train loss:  0.5602264404296875
train gradient:  0.14907714741985556
iteration : 8965
train acc:  0.7734375
train loss:  0.4725196957588196
train gradient:  0.15368423510981732
iteration : 8966
train acc:  0.6953125
train loss:  0.5767203569412231
train gradient:  0.15689867259347645
iteration : 8967
train acc:  0.75
train loss:  0.5011875629425049
train gradient:  0.1434855521748954
iteration : 8968
train acc:  0.6953125
train loss:  0.5341267585754395
train gradient:  0.132237223075715
iteration : 8969
train acc:  0.671875
train loss:  0.5766741037368774
train gradient:  0.17516183953051867
iteration : 8970
train acc:  0.8125
train loss:  0.4576725959777832
train gradient:  0.0972190424779812
iteration : 8971
train acc:  0.6875
train loss:  0.5769268274307251
train gradient:  0.16155567074423205
iteration : 8972
train acc:  0.6640625
train loss:  0.5032362937927246
train gradient:  0.12407666003096293
iteration : 8973
train acc:  0.78125
train loss:  0.43697652220726013
train gradient:  0.08236159616520129
iteration : 8974
train acc:  0.7421875
train loss:  0.4875037670135498
train gradient:  0.11040078859643652
iteration : 8975
train acc:  0.71875
train loss:  0.47098276019096375
train gradient:  0.134350538015779
iteration : 8976
train acc:  0.7265625
train loss:  0.5200777053833008
train gradient:  0.16849530795593978
iteration : 8977
train acc:  0.7890625
train loss:  0.4255523085594177
train gradient:  0.09729250061228904
iteration : 8978
train acc:  0.7734375
train loss:  0.44079840183258057
train gradient:  0.1008224445920735
iteration : 8979
train acc:  0.75
train loss:  0.49867987632751465
train gradient:  0.12428278383590612
iteration : 8980
train acc:  0.7265625
train loss:  0.4904553294181824
train gradient:  0.14202176693826418
iteration : 8981
train acc:  0.7578125
train loss:  0.47020819783210754
train gradient:  0.15402772247005803
iteration : 8982
train acc:  0.7734375
train loss:  0.5103775262832642
train gradient:  0.12427673956325823
iteration : 8983
train acc:  0.7890625
train loss:  0.45909589529037476
train gradient:  0.16661338539587356
iteration : 8984
train acc:  0.75
train loss:  0.4970538020133972
train gradient:  0.1388023555361385
iteration : 8985
train acc:  0.7109375
train loss:  0.490902304649353
train gradient:  0.14351178654106397
iteration : 8986
train acc:  0.8046875
train loss:  0.4470016658306122
train gradient:  0.10541717426796719
iteration : 8987
train acc:  0.703125
train loss:  0.5479761362075806
train gradient:  0.1464637251209302
iteration : 8988
train acc:  0.7421875
train loss:  0.5169923305511475
train gradient:  0.1430407884305097
iteration : 8989
train acc:  0.6953125
train loss:  0.49788787961006165
train gradient:  0.12488916090155544
iteration : 8990
train acc:  0.734375
train loss:  0.52891606092453
train gradient:  0.1910502375474109
iteration : 8991
train acc:  0.7578125
train loss:  0.44909489154815674
train gradient:  0.11697997645855145
iteration : 8992
train acc:  0.7734375
train loss:  0.4791363477706909
train gradient:  0.14247583342281328
iteration : 8993
train acc:  0.71875
train loss:  0.5139293670654297
train gradient:  0.13821826705643359
iteration : 8994
train acc:  0.78125
train loss:  0.46483421325683594
train gradient:  0.1097482504212315
iteration : 8995
train acc:  0.7578125
train loss:  0.47245076298713684
train gradient:  0.10280379290503339
iteration : 8996
train acc:  0.71875
train loss:  0.5177199840545654
train gradient:  0.13893619167591298
iteration : 8997
train acc:  0.734375
train loss:  0.48594117164611816
train gradient:  0.13142022948483334
iteration : 8998
train acc:  0.7421875
train loss:  0.4696565270423889
train gradient:  0.14573946627549222
iteration : 8999
train acc:  0.7109375
train loss:  0.5642129182815552
train gradient:  0.17694380674708332
iteration : 9000
train acc:  0.828125
train loss:  0.43772831559181213
train gradient:  0.08974619610130702
iteration : 9001
train acc:  0.7421875
train loss:  0.49331241846084595
train gradient:  0.11262775777022521
iteration : 9002
train acc:  0.6875
train loss:  0.5504952073097229
train gradient:  0.13211022887748256
iteration : 9003
train acc:  0.734375
train loss:  0.5062236785888672
train gradient:  0.14486333888350836
iteration : 9004
train acc:  0.8203125
train loss:  0.39927440881729126
train gradient:  0.07994693008492686
iteration : 9005
train acc:  0.7265625
train loss:  0.48892760276794434
train gradient:  0.12031774131233679
iteration : 9006
train acc:  0.7734375
train loss:  0.4858223795890808
train gradient:  0.11746583581128729
iteration : 9007
train acc:  0.7734375
train loss:  0.4650612473487854
train gradient:  0.09569661327998993
iteration : 9008
train acc:  0.7578125
train loss:  0.46916133165359497
train gradient:  0.12390793619776054
iteration : 9009
train acc:  0.671875
train loss:  0.5458956360816956
train gradient:  0.13934903009485572
iteration : 9010
train acc:  0.7578125
train loss:  0.4713827669620514
train gradient:  0.11798282059790619
iteration : 9011
train acc:  0.71875
train loss:  0.5044699907302856
train gradient:  0.1667701825744496
iteration : 9012
train acc:  0.78125
train loss:  0.43981391191482544
train gradient:  0.11013017009471586
iteration : 9013
train acc:  0.8046875
train loss:  0.4383459985256195
train gradient:  0.10391412503484761
iteration : 9014
train acc:  0.734375
train loss:  0.5030463337898254
train gradient:  0.141893569533673
iteration : 9015
train acc:  0.75
train loss:  0.5003315210342407
train gradient:  0.11274958050187087
iteration : 9016
train acc:  0.8046875
train loss:  0.44411155581474304
train gradient:  0.09331704755595768
iteration : 9017
train acc:  0.765625
train loss:  0.45557233691215515
train gradient:  0.1287001325411095
iteration : 9018
train acc:  0.7109375
train loss:  0.5196715593338013
train gradient:  0.14651162255934141
iteration : 9019
train acc:  0.75
train loss:  0.4654867351055145
train gradient:  0.12045799365257268
iteration : 9020
train acc:  0.8125
train loss:  0.4204651415348053
train gradient:  0.10383256134583206
iteration : 9021
train acc:  0.7734375
train loss:  0.4816325008869171
train gradient:  0.13643396011959613
iteration : 9022
train acc:  0.7734375
train loss:  0.459264874458313
train gradient:  0.17054661809215144
iteration : 9023
train acc:  0.8046875
train loss:  0.42637982964515686
train gradient:  0.09639299630321449
iteration : 9024
train acc:  0.7734375
train loss:  0.4614865183830261
train gradient:  0.10739191384380392
iteration : 9025
train acc:  0.78125
train loss:  0.41867825388908386
train gradient:  0.09339246395715448
iteration : 9026
train acc:  0.75
train loss:  0.49973413348197937
train gradient:  0.1493284195051965
iteration : 9027
train acc:  0.71875
train loss:  0.5100778341293335
train gradient:  0.12435063461150173
iteration : 9028
train acc:  0.7578125
train loss:  0.45362451672554016
train gradient:  0.13193339737052937
iteration : 9029
train acc:  0.7578125
train loss:  0.4528079628944397
train gradient:  0.10729018576870125
iteration : 9030
train acc:  0.6875
train loss:  0.538183331489563
train gradient:  0.13383953188065123
iteration : 9031
train acc:  0.7421875
train loss:  0.4753175377845764
train gradient:  0.14234251102197626
iteration : 9032
train acc:  0.7265625
train loss:  0.4810760021209717
train gradient:  0.1247088293366316
iteration : 9033
train acc:  0.7109375
train loss:  0.4990762770175934
train gradient:  0.14815143085287183
iteration : 9034
train acc:  0.7109375
train loss:  0.536004900932312
train gradient:  0.1251989005124228
iteration : 9035
train acc:  0.7421875
train loss:  0.43537575006484985
train gradient:  0.09723267886168627
iteration : 9036
train acc:  0.7265625
train loss:  0.530492901802063
train gradient:  0.1546134812976449
iteration : 9037
train acc:  0.7265625
train loss:  0.488900750875473
train gradient:  0.12382760945878203
iteration : 9038
train acc:  0.6875
train loss:  0.5476260185241699
train gradient:  0.1322228557633635
iteration : 9039
train acc:  0.7734375
train loss:  0.4449079930782318
train gradient:  0.09800196910577447
iteration : 9040
train acc:  0.7890625
train loss:  0.4736819565296173
train gradient:  0.12730914528111437
iteration : 9041
train acc:  0.7421875
train loss:  0.4494674503803253
train gradient:  0.11419177471724334
iteration : 9042
train acc:  0.7421875
train loss:  0.5200687646865845
train gradient:  0.13422172009183891
iteration : 9043
train acc:  0.75
train loss:  0.48287591338157654
train gradient:  0.15442527652591315
iteration : 9044
train acc:  0.734375
train loss:  0.5033590197563171
train gradient:  0.11471158074384342
iteration : 9045
train acc:  0.796875
train loss:  0.4475906491279602
train gradient:  0.12094836651662105
iteration : 9046
train acc:  0.8046875
train loss:  0.4349300265312195
train gradient:  0.10651228130420518
iteration : 9047
train acc:  0.734375
train loss:  0.553379237651825
train gradient:  0.15303866205080305
iteration : 9048
train acc:  0.75
train loss:  0.5085145831108093
train gradient:  0.13702427162133923
iteration : 9049
train acc:  0.75
train loss:  0.45027780532836914
train gradient:  0.12006728608603962
iteration : 9050
train acc:  0.7109375
train loss:  0.569278359413147
train gradient:  0.14932875713899965
iteration : 9051
train acc:  0.7734375
train loss:  0.48036664724349976
train gradient:  0.138396983663994
iteration : 9052
train acc:  0.7734375
train loss:  0.5023155808448792
train gradient:  0.11918792461182459
iteration : 9053
train acc:  0.7734375
train loss:  0.47274380922317505
train gradient:  0.10390149557363566
iteration : 9054
train acc:  0.7578125
train loss:  0.4540931284427643
train gradient:  0.0936143111820354
iteration : 9055
train acc:  0.75
train loss:  0.46544116735458374
train gradient:  0.10309713219515082
iteration : 9056
train acc:  0.6875
train loss:  0.5427677631378174
train gradient:  0.1540443634031276
iteration : 9057
train acc:  0.6640625
train loss:  0.5832279324531555
train gradient:  0.16977337439290058
iteration : 9058
train acc:  0.7265625
train loss:  0.47909075021743774
train gradient:  0.10822945461038555
iteration : 9059
train acc:  0.7890625
train loss:  0.42544275522232056
train gradient:  0.11433102103287723
iteration : 9060
train acc:  0.7421875
train loss:  0.494564414024353
train gradient:  0.10983812213199702
iteration : 9061
train acc:  0.71875
train loss:  0.5561726093292236
train gradient:  0.21963175380071231
iteration : 9062
train acc:  0.7421875
train loss:  0.5038018226623535
train gradient:  0.13996002377292296
iteration : 9063
train acc:  0.7265625
train loss:  0.48810240626335144
train gradient:  0.13397386198034264
iteration : 9064
train acc:  0.7578125
train loss:  0.4671395421028137
train gradient:  0.12230106665766045
iteration : 9065
train acc:  0.7109375
train loss:  0.49808624386787415
train gradient:  0.1251912984216768
iteration : 9066
train acc:  0.75
train loss:  0.5080248117446899
train gradient:  0.12008633273972186
iteration : 9067
train acc:  0.7578125
train loss:  0.49469247460365295
train gradient:  0.10572962808683092
iteration : 9068
train acc:  0.765625
train loss:  0.5015604496002197
train gradient:  0.11483152312860571
iteration : 9069
train acc:  0.765625
train loss:  0.4766046404838562
train gradient:  0.12999772533759735
iteration : 9070
train acc:  0.75
train loss:  0.43088197708129883
train gradient:  0.10341911327393315
iteration : 9071
train acc:  0.71875
train loss:  0.5170885920524597
train gradient:  0.18601773760323703
iteration : 9072
train acc:  0.7265625
train loss:  0.4802559018135071
train gradient:  0.10928016281892948
iteration : 9073
train acc:  0.7421875
train loss:  0.4768912196159363
train gradient:  0.13341625268366902
iteration : 9074
train acc:  0.7421875
train loss:  0.43684548139572144
train gradient:  0.10645188742783825
iteration : 9075
train acc:  0.78125
train loss:  0.41207003593444824
train gradient:  0.16741544701314837
iteration : 9076
train acc:  0.671875
train loss:  0.5321109294891357
train gradient:  0.15552968888765592
iteration : 9077
train acc:  0.6953125
train loss:  0.48175179958343506
train gradient:  0.10723648812113418
iteration : 9078
train acc:  0.75
train loss:  0.4774315357208252
train gradient:  0.13514634979942386
iteration : 9079
train acc:  0.78125
train loss:  0.4329990744590759
train gradient:  0.13608975257497563
iteration : 9080
train acc:  0.703125
train loss:  0.5376098155975342
train gradient:  0.14002262530950252
iteration : 9081
train acc:  0.7890625
train loss:  0.4663769602775574
train gradient:  0.11946966070873145
iteration : 9082
train acc:  0.796875
train loss:  0.4507807493209839
train gradient:  0.1244255620393158
iteration : 9083
train acc:  0.7578125
train loss:  0.4949145019054413
train gradient:  0.13405461477160852
iteration : 9084
train acc:  0.703125
train loss:  0.5052703619003296
train gradient:  0.13140495101055621
iteration : 9085
train acc:  0.71875
train loss:  0.5011370182037354
train gradient:  0.14516728490722303
iteration : 9086
train acc:  0.78125
train loss:  0.43889176845550537
train gradient:  0.0877582966987428
iteration : 9087
train acc:  0.765625
train loss:  0.5005770921707153
train gradient:  0.13965635041184732
iteration : 9088
train acc:  0.765625
train loss:  0.46160608530044556
train gradient:  0.11663005576042032
iteration : 9089
train acc:  0.7265625
train loss:  0.5130467414855957
train gradient:  0.21312021118309243
iteration : 9090
train acc:  0.7109375
train loss:  0.46756988763809204
train gradient:  0.1091498651513072
iteration : 9091
train acc:  0.734375
train loss:  0.48695725202560425
train gradient:  0.13717702966749018
iteration : 9092
train acc:  0.7890625
train loss:  0.4697597026824951
train gradient:  0.13140093749814286
iteration : 9093
train acc:  0.7421875
train loss:  0.48710140585899353
train gradient:  0.11629922857647597
iteration : 9094
train acc:  0.6796875
train loss:  0.5479567646980286
train gradient:  0.12636086728054774
iteration : 9095
train acc:  0.6328125
train loss:  0.6047861576080322
train gradient:  0.18724036519977913
iteration : 9096
train acc:  0.7265625
train loss:  0.5276796221733093
train gradient:  0.13541896841217377
iteration : 9097
train acc:  0.75
train loss:  0.5087703466415405
train gradient:  0.11928179022069205
iteration : 9098
train acc:  0.6796875
train loss:  0.6121506094932556
train gradient:  0.21101566657399312
iteration : 9099
train acc:  0.796875
train loss:  0.4543176293373108
train gradient:  0.15379054844178588
iteration : 9100
train acc:  0.7734375
train loss:  0.47264695167541504
train gradient:  0.09397264371489332
iteration : 9101
train acc:  0.6875
train loss:  0.5559921264648438
train gradient:  0.15389733242645898
iteration : 9102
train acc:  0.7734375
train loss:  0.47177091240882874
train gradient:  0.13874239373756359
iteration : 9103
train acc:  0.78125
train loss:  0.4629957377910614
train gradient:  0.12549630557480385
iteration : 9104
train acc:  0.7421875
train loss:  0.49459385871887207
train gradient:  0.13725585547449065
iteration : 9105
train acc:  0.7421875
train loss:  0.4837036430835724
train gradient:  0.1389377980383681
iteration : 9106
train acc:  0.71875
train loss:  0.508497953414917
train gradient:  0.13652137150553012
iteration : 9107
train acc:  0.75
train loss:  0.4918197691440582
train gradient:  0.1377945914131254
iteration : 9108
train acc:  0.6953125
train loss:  0.5251898169517517
train gradient:  0.1553236710540406
iteration : 9109
train acc:  0.734375
train loss:  0.5019830465316772
train gradient:  0.13465202261734915
iteration : 9110
train acc:  0.75
train loss:  0.5213391780853271
train gradient:  0.13105972611308286
iteration : 9111
train acc:  0.7734375
train loss:  0.4506484866142273
train gradient:  0.08860624241549056
iteration : 9112
train acc:  0.765625
train loss:  0.4819291830062866
train gradient:  0.13119610654745426
iteration : 9113
train acc:  0.7734375
train loss:  0.4632846713066101
train gradient:  0.16402159137427047
iteration : 9114
train acc:  0.75
train loss:  0.49686771631240845
train gradient:  0.14685837083286093
iteration : 9115
train acc:  0.734375
train loss:  0.49995481967926025
train gradient:  0.10814574728059677
iteration : 9116
train acc:  0.7421875
train loss:  0.4870758056640625
train gradient:  0.1488765522464085
iteration : 9117
train acc:  0.7265625
train loss:  0.5133870840072632
train gradient:  0.1116594148893086
iteration : 9118
train acc:  0.8203125
train loss:  0.42194345593452454
train gradient:  0.10535705377822721
iteration : 9119
train acc:  0.7265625
train loss:  0.5315117239952087
train gradient:  0.14649945121568742
iteration : 9120
train acc:  0.6953125
train loss:  0.5091029405593872
train gradient:  0.12286815110713174
iteration : 9121
train acc:  0.7265625
train loss:  0.4727570414543152
train gradient:  0.11604966118084603
iteration : 9122
train acc:  0.703125
train loss:  0.5377753376960754
train gradient:  0.17452624578819184
iteration : 9123
train acc:  0.734375
train loss:  0.562498927116394
train gradient:  0.15348769548922656
iteration : 9124
train acc:  0.6796875
train loss:  0.5274718403816223
train gradient:  0.15013802694708023
iteration : 9125
train acc:  0.78125
train loss:  0.45899438858032227
train gradient:  0.10417351197763325
iteration : 9126
train acc:  0.765625
train loss:  0.45291686058044434
train gradient:  0.1290000880841208
iteration : 9127
train acc:  0.7734375
train loss:  0.4819861054420471
train gradient:  0.11294064079946613
iteration : 9128
train acc:  0.7109375
train loss:  0.5512742400169373
train gradient:  0.146725137816074
iteration : 9129
train acc:  0.703125
train loss:  0.49021491408348083
train gradient:  0.1532750619538849
iteration : 9130
train acc:  0.71875
train loss:  0.502213716506958
train gradient:  0.16193065163427273
iteration : 9131
train acc:  0.828125
train loss:  0.39373543858528137
train gradient:  0.0880404958851104
iteration : 9132
train acc:  0.71875
train loss:  0.4771440029144287
train gradient:  0.15417600719276506
iteration : 9133
train acc:  0.7109375
train loss:  0.5269217491149902
train gradient:  0.16690248483344888
iteration : 9134
train acc:  0.78125
train loss:  0.4702564477920532
train gradient:  0.1143298226131568
iteration : 9135
train acc:  0.6875
train loss:  0.5000874400138855
train gradient:  0.10953591774546956
iteration : 9136
train acc:  0.8125
train loss:  0.43974733352661133
train gradient:  0.10500171668327543
iteration : 9137
train acc:  0.7265625
train loss:  0.4882127642631531
train gradient:  0.12747619177993297
iteration : 9138
train acc:  0.765625
train loss:  0.5124861001968384
train gradient:  0.20097561480358883
iteration : 9139
train acc:  0.6875
train loss:  0.5111901760101318
train gradient:  0.1397441426199264
iteration : 9140
train acc:  0.71875
train loss:  0.5133106708526611
train gradient:  0.1682811407825866
iteration : 9141
train acc:  0.75
train loss:  0.5406347513198853
train gradient:  0.13940319236054455
iteration : 9142
train acc:  0.71875
train loss:  0.4981914758682251
train gradient:  0.13791065420353515
iteration : 9143
train acc:  0.734375
train loss:  0.5490356683731079
train gradient:  0.1945412394578691
iteration : 9144
train acc:  0.78125
train loss:  0.4509884715080261
train gradient:  0.10301731711948589
iteration : 9145
train acc:  0.6640625
train loss:  0.6113974452018738
train gradient:  0.17760653726304484
iteration : 9146
train acc:  0.7578125
train loss:  0.4854488968849182
train gradient:  0.11806827299852825
iteration : 9147
train acc:  0.75
train loss:  0.4598082900047302
train gradient:  0.10949244133896156
iteration : 9148
train acc:  0.7109375
train loss:  0.5299123525619507
train gradient:  0.1334413686709131
iteration : 9149
train acc:  0.828125
train loss:  0.4399806559085846
train gradient:  0.10209319527172474
iteration : 9150
train acc:  0.7109375
train loss:  0.49450764060020447
train gradient:  0.1111096258312828
iteration : 9151
train acc:  0.734375
train loss:  0.48058372735977173
train gradient:  0.15865845896326197
iteration : 9152
train acc:  0.7265625
train loss:  0.5299805998802185
train gradient:  0.18872935910410257
iteration : 9153
train acc:  0.765625
train loss:  0.5101450681686401
train gradient:  0.11168576843588575
iteration : 9154
train acc:  0.7734375
train loss:  0.4482259154319763
train gradient:  0.09324881453066378
iteration : 9155
train acc:  0.6953125
train loss:  0.542782187461853
train gradient:  0.13731703173708348
iteration : 9156
train acc:  0.703125
train loss:  0.49761050939559937
train gradient:  0.11725956233481274
iteration : 9157
train acc:  0.734375
train loss:  0.48687803745269775
train gradient:  0.10616668285402425
iteration : 9158
train acc:  0.7421875
train loss:  0.48657357692718506
train gradient:  0.12977077221873765
iteration : 9159
train acc:  0.7421875
train loss:  0.4813750386238098
train gradient:  0.11018847089123866
iteration : 9160
train acc:  0.78125
train loss:  0.4751076102256775
train gradient:  0.14035542804980783
iteration : 9161
train acc:  0.828125
train loss:  0.40208718180656433
train gradient:  0.0933760794114247
iteration : 9162
train acc:  0.75
train loss:  0.5133135318756104
train gradient:  0.1278997423345454
iteration : 9163
train acc:  0.7734375
train loss:  0.4912634491920471
train gradient:  0.12480252268666114
iteration : 9164
train acc:  0.6796875
train loss:  0.5218193531036377
train gradient:  0.1243566113524617
iteration : 9165
train acc:  0.7421875
train loss:  0.5025334358215332
train gradient:  0.13651894045497737
iteration : 9166
train acc:  0.734375
train loss:  0.48656749725341797
train gradient:  0.14832658847717944
iteration : 9167
train acc:  0.6953125
train loss:  0.5147104263305664
train gradient:  0.16394646758228032
iteration : 9168
train acc:  0.7890625
train loss:  0.4854384660720825
train gradient:  0.1416629495515196
iteration : 9169
train acc:  0.796875
train loss:  0.4711475968360901
train gradient:  0.18098208093077006
iteration : 9170
train acc:  0.71875
train loss:  0.4858730435371399
train gradient:  0.1306691181386228
iteration : 9171
train acc:  0.71875
train loss:  0.5302135348320007
train gradient:  0.14262102096411933
iteration : 9172
train acc:  0.734375
train loss:  0.48938265442848206
train gradient:  0.13609151564664906
iteration : 9173
train acc:  0.7421875
train loss:  0.4837438464164734
train gradient:  0.15502752938427322
iteration : 9174
train acc:  0.8515625
train loss:  0.40604865550994873
train gradient:  0.09135970952125211
iteration : 9175
train acc:  0.828125
train loss:  0.37939053773880005
train gradient:  0.08083616013730466
iteration : 9176
train acc:  0.75
train loss:  0.48962485790252686
train gradient:  0.11823020785423405
iteration : 9177
train acc:  0.8046875
train loss:  0.44488850235939026
train gradient:  0.10636266220765568
iteration : 9178
train acc:  0.7734375
train loss:  0.4807487428188324
train gradient:  0.09875090227576323
iteration : 9179
train acc:  0.796875
train loss:  0.40420660376548767
train gradient:  0.11035173451832482
iteration : 9180
train acc:  0.7109375
train loss:  0.5486717224121094
train gradient:  0.1404776502814271
iteration : 9181
train acc:  0.7421875
train loss:  0.4767696261405945
train gradient:  0.10130755449550721
iteration : 9182
train acc:  0.6796875
train loss:  0.5549125671386719
train gradient:  0.18149338466225184
iteration : 9183
train acc:  0.71875
train loss:  0.5269736051559448
train gradient:  0.1251469303707876
iteration : 9184
train acc:  0.78125
train loss:  0.462380051612854
train gradient:  0.1280226149009896
iteration : 9185
train acc:  0.7578125
train loss:  0.486386239528656
train gradient:  0.12901566952872465
iteration : 9186
train acc:  0.75
train loss:  0.4732173979282379
train gradient:  0.1330018006683194
iteration : 9187
train acc:  0.71875
train loss:  0.5253791213035583
train gradient:  0.15188647787447307
iteration : 9188
train acc:  0.7890625
train loss:  0.43813788890838623
train gradient:  0.08643697557861456
iteration : 9189
train acc:  0.671875
train loss:  0.5761553049087524
train gradient:  0.1448195540042295
iteration : 9190
train acc:  0.75
train loss:  0.4823085367679596
train gradient:  0.12023104314109809
iteration : 9191
train acc:  0.75
train loss:  0.46261823177337646
train gradient:  0.11019717647377225
iteration : 9192
train acc:  0.75
train loss:  0.47318288683891296
train gradient:  0.12693028253382455
iteration : 9193
train acc:  0.6953125
train loss:  0.5217918753623962
train gradient:  0.12288516045070966
iteration : 9194
train acc:  0.7578125
train loss:  0.4527023136615753
train gradient:  0.10912672106233197
iteration : 9195
train acc:  0.7578125
train loss:  0.4782862067222595
train gradient:  0.11029003168227869
iteration : 9196
train acc:  0.6875
train loss:  0.5662894248962402
train gradient:  0.14734805472843476
iteration : 9197
train acc:  0.78125
train loss:  0.47644761204719543
train gradient:  0.1222077338719823
iteration : 9198
train acc:  0.7578125
train loss:  0.4481147527694702
train gradient:  0.13112639112094748
iteration : 9199
train acc:  0.7265625
train loss:  0.5291340351104736
train gradient:  0.14108254374418994
iteration : 9200
train acc:  0.734375
train loss:  0.4824015498161316
train gradient:  0.1307457557968298
iteration : 9201
train acc:  0.671875
train loss:  0.5602658987045288
train gradient:  0.1347155107295071
iteration : 9202
train acc:  0.78125
train loss:  0.47134870290756226
train gradient:  0.13459528353279923
iteration : 9203
train acc:  0.734375
train loss:  0.49242889881134033
train gradient:  0.10729020536411403
iteration : 9204
train acc:  0.7421875
train loss:  0.508213222026825
train gradient:  0.13091686108309647
iteration : 9205
train acc:  0.7109375
train loss:  0.5313884019851685
train gradient:  0.13538145281095082
iteration : 9206
train acc:  0.734375
train loss:  0.5057529807090759
train gradient:  0.13077493160380613
iteration : 9207
train acc:  0.78125
train loss:  0.4440745711326599
train gradient:  0.11457562768049061
iteration : 9208
train acc:  0.734375
train loss:  0.47414880990982056
train gradient:  0.09650556446527862
iteration : 9209
train acc:  0.796875
train loss:  0.4421532452106476
train gradient:  0.12096670098968064
iteration : 9210
train acc:  0.734375
train loss:  0.48023903369903564
train gradient:  0.12363618233994708
iteration : 9211
train acc:  0.7265625
train loss:  0.561260461807251
train gradient:  0.147240867451614
iteration : 9212
train acc:  0.7734375
train loss:  0.45551782846450806
train gradient:  0.12060561263207811
iteration : 9213
train acc:  0.7109375
train loss:  0.5240932703018188
train gradient:  0.16278651115268972
iteration : 9214
train acc:  0.7578125
train loss:  0.5185522437095642
train gradient:  0.18688910669541547
iteration : 9215
train acc:  0.765625
train loss:  0.44550997018814087
train gradient:  0.11592793935943693
iteration : 9216
train acc:  0.78125
train loss:  0.4348575174808502
train gradient:  0.07818125775930876
iteration : 9217
train acc:  0.6953125
train loss:  0.5361163020133972
train gradient:  0.15833089411321016
iteration : 9218
train acc:  0.78125
train loss:  0.4680882394313812
train gradient:  0.12044241691998732
iteration : 9219
train acc:  0.6796875
train loss:  0.5979690551757812
train gradient:  0.18383960573454566
iteration : 9220
train acc:  0.7265625
train loss:  0.48046645522117615
train gradient:  0.14221644227916264
iteration : 9221
train acc:  0.6484375
train loss:  0.5996392965316772
train gradient:  0.22481598618074544
iteration : 9222
train acc:  0.71875
train loss:  0.5337080955505371
train gradient:  0.1774395419329257
iteration : 9223
train acc:  0.71875
train loss:  0.5085344314575195
train gradient:  0.1402381330897254
iteration : 9224
train acc:  0.7890625
train loss:  0.46812593936920166
train gradient:  0.09749894213545346
iteration : 9225
train acc:  0.7421875
train loss:  0.48995766043663025
train gradient:  0.12487459212845141
iteration : 9226
train acc:  0.71875
train loss:  0.5193696022033691
train gradient:  0.16086645770741942
iteration : 9227
train acc:  0.78125
train loss:  0.47942203283309937
train gradient:  0.1138719477420986
iteration : 9228
train acc:  0.75
train loss:  0.5419366359710693
train gradient:  0.1500398241757213
iteration : 9229
train acc:  0.75
train loss:  0.5107076168060303
train gradient:  0.1274192077872816
iteration : 9230
train acc:  0.6796875
train loss:  0.5419832468032837
train gradient:  0.15784155170735736
iteration : 9231
train acc:  0.78125
train loss:  0.4442934989929199
train gradient:  0.13751800904731165
iteration : 9232
train acc:  0.734375
train loss:  0.548117995262146
train gradient:  0.22224392270397
iteration : 9233
train acc:  0.71875
train loss:  0.5190017223358154
train gradient:  0.1450522091012347
iteration : 9234
train acc:  0.7421875
train loss:  0.5342848300933838
train gradient:  0.14707158258989714
iteration : 9235
train acc:  0.7421875
train loss:  0.5518379807472229
train gradient:  0.11621685112600195
iteration : 9236
train acc:  0.7734375
train loss:  0.44109490513801575
train gradient:  0.12817894905335436
iteration : 9237
train acc:  0.7109375
train loss:  0.5283750295639038
train gradient:  0.14569517808713395
iteration : 9238
train acc:  0.7109375
train loss:  0.49797308444976807
train gradient:  0.15338685392022827
iteration : 9239
train acc:  0.6796875
train loss:  0.5219928026199341
train gradient:  0.1359829390848834
iteration : 9240
train acc:  0.7578125
train loss:  0.47959738969802856
train gradient:  0.11771623231921317
iteration : 9241
train acc:  0.640625
train loss:  0.5822725296020508
train gradient:  0.2630693696969666
iteration : 9242
train acc:  0.78125
train loss:  0.4704394042491913
train gradient:  0.11318035207084884
iteration : 9243
train acc:  0.7734375
train loss:  0.41862964630126953
train gradient:  0.10854306611468514
iteration : 9244
train acc:  0.75
train loss:  0.4825105667114258
train gradient:  0.14087896683237416
iteration : 9245
train acc:  0.765625
train loss:  0.46329885721206665
train gradient:  0.1257735348052341
iteration : 9246
train acc:  0.6953125
train loss:  0.5522035360336304
train gradient:  0.19803270832680325
iteration : 9247
train acc:  0.7890625
train loss:  0.41457608342170715
train gradient:  0.09564350108924231
iteration : 9248
train acc:  0.7421875
train loss:  0.5042623281478882
train gradient:  0.1492188778291078
iteration : 9249
train acc:  0.8046875
train loss:  0.46236470341682434
train gradient:  0.09294033466517838
iteration : 9250
train acc:  0.7578125
train loss:  0.48986777663230896
train gradient:  0.1398419230535205
iteration : 9251
train acc:  0.65625
train loss:  0.5561326742172241
train gradient:  0.17868493850766676
iteration : 9252
train acc:  0.71875
train loss:  0.5055099725723267
train gradient:  0.1515667977594912
iteration : 9253
train acc:  0.71875
train loss:  0.49941566586494446
train gradient:  0.1278073650802549
iteration : 9254
train acc:  0.71875
train loss:  0.5272958278656006
train gradient:  0.12878160421777524
iteration : 9255
train acc:  0.7265625
train loss:  0.4604378044605255
train gradient:  0.11394440264230936
iteration : 9256
train acc:  0.7578125
train loss:  0.481289267539978
train gradient:  0.14412981933725819
iteration : 9257
train acc:  0.796875
train loss:  0.4358649253845215
train gradient:  0.10617048806045119
iteration : 9258
train acc:  0.734375
train loss:  0.4827171564102173
train gradient:  0.1190422296988726
iteration : 9259
train acc:  0.828125
train loss:  0.3973146080970764
train gradient:  0.09446464343385982
iteration : 9260
train acc:  0.734375
train loss:  0.48255378007888794
train gradient:  0.11996356206034431
iteration : 9261
train acc:  0.765625
train loss:  0.4549092948436737
train gradient:  0.09869053308463882
iteration : 9262
train acc:  0.796875
train loss:  0.4531920850276947
train gradient:  0.11592097803302373
iteration : 9263
train acc:  0.6953125
train loss:  0.4967167377471924
train gradient:  0.14077474902864076
iteration : 9264
train acc:  0.7578125
train loss:  0.47332286834716797
train gradient:  0.17855545469401113
iteration : 9265
train acc:  0.7734375
train loss:  0.47546669840812683
train gradient:  0.10923084840577424
iteration : 9266
train acc:  0.71875
train loss:  0.49015527963638306
train gradient:  0.15664740944824335
iteration : 9267
train acc:  0.7421875
train loss:  0.5160002112388611
train gradient:  0.15273682481836576
iteration : 9268
train acc:  0.7421875
train loss:  0.5263493657112122
train gradient:  0.1835342760415447
iteration : 9269
train acc:  0.7578125
train loss:  0.5034506320953369
train gradient:  0.13180965939812067
iteration : 9270
train acc:  0.765625
train loss:  0.48045873641967773
train gradient:  0.11510402409238324
iteration : 9271
train acc:  0.71875
train loss:  0.5117933750152588
train gradient:  0.1601733174397471
iteration : 9272
train acc:  0.75
train loss:  0.5574547052383423
train gradient:  0.16415707758051057
iteration : 9273
train acc:  0.765625
train loss:  0.42996156215667725
train gradient:  0.10077704682455317
iteration : 9274
train acc:  0.75
train loss:  0.5028887391090393
train gradient:  0.11973686371236765
iteration : 9275
train acc:  0.703125
train loss:  0.5522564053535461
train gradient:  0.16801058477576075
iteration : 9276
train acc:  0.78125
train loss:  0.4203200042247772
train gradient:  0.09242021228543318
iteration : 9277
train acc:  0.7109375
train loss:  0.5328614711761475
train gradient:  0.14392526422094584
iteration : 9278
train acc:  0.7109375
train loss:  0.520196795463562
train gradient:  0.14350776012841837
iteration : 9279
train acc:  0.7109375
train loss:  0.5126897692680359
train gradient:  0.1270979338518725
iteration : 9280
train acc:  0.6484375
train loss:  0.5606117248535156
train gradient:  0.1575515934003467
iteration : 9281
train acc:  0.7265625
train loss:  0.5092124938964844
train gradient:  0.1387781855999325
iteration : 9282
train acc:  0.7109375
train loss:  0.47457295656204224
train gradient:  0.12487310279407537
iteration : 9283
train acc:  0.765625
train loss:  0.48718690872192383
train gradient:  0.13198372607829267
iteration : 9284
train acc:  0.78125
train loss:  0.4654618799686432
train gradient:  0.10366840028694121
iteration : 9285
train acc:  0.703125
train loss:  0.5531893372535706
train gradient:  0.1433968871887103
iteration : 9286
train acc:  0.6796875
train loss:  0.6139913201332092
train gradient:  0.22666800408319482
iteration : 9287
train acc:  0.7578125
train loss:  0.47384053468704224
train gradient:  0.12336344927911838
iteration : 9288
train acc:  0.765625
train loss:  0.4568518400192261
train gradient:  0.10929961213982052
iteration : 9289
train acc:  0.7421875
train loss:  0.5026430487632751
train gradient:  0.14329604206411714
iteration : 9290
train acc:  0.7734375
train loss:  0.48791199922561646
train gradient:  0.15579330126914082
iteration : 9291
train acc:  0.796875
train loss:  0.4432317614555359
train gradient:  0.09646663826572571
iteration : 9292
train acc:  0.7421875
train loss:  0.48778483271598816
train gradient:  0.14030569849123614
iteration : 9293
train acc:  0.8046875
train loss:  0.41750389337539673
train gradient:  0.09687428872050018
iteration : 9294
train acc:  0.75
train loss:  0.47909873723983765
train gradient:  0.10867184924045584
iteration : 9295
train acc:  0.8046875
train loss:  0.4267558455467224
train gradient:  0.08906335882077505
iteration : 9296
train acc:  0.75
train loss:  0.4394848644733429
train gradient:  0.09794140762680854
iteration : 9297
train acc:  0.78125
train loss:  0.4372439384460449
train gradient:  0.1029140933442154
iteration : 9298
train acc:  0.71875
train loss:  0.570796012878418
train gradient:  0.1300278344847889
iteration : 9299
train acc:  0.7421875
train loss:  0.5076677799224854
train gradient:  0.15648140281771153
iteration : 9300
train acc:  0.75
train loss:  0.5311387181282043
train gradient:  0.13983871406543058
iteration : 9301
train acc:  0.7265625
train loss:  0.49874091148376465
train gradient:  0.136474933396867
iteration : 9302
train acc:  0.734375
train loss:  0.5828546285629272
train gradient:  0.2521819999635453
iteration : 9303
train acc:  0.7421875
train loss:  0.4880876839160919
train gradient:  0.12095780700185096
iteration : 9304
train acc:  0.7578125
train loss:  0.47570788860321045
train gradient:  0.10370796642969321
iteration : 9305
train acc:  0.796875
train loss:  0.44582754373550415
train gradient:  0.10349428035053608
iteration : 9306
train acc:  0.7109375
train loss:  0.5281925797462463
train gradient:  0.15564978242776184
iteration : 9307
train acc:  0.7578125
train loss:  0.4410315752029419
train gradient:  0.08634673336040573
iteration : 9308
train acc:  0.7890625
train loss:  0.4317055344581604
train gradient:  0.096792936380283
iteration : 9309
train acc:  0.7578125
train loss:  0.5092449188232422
train gradient:  0.13819393941897637
iteration : 9310
train acc:  0.6640625
train loss:  0.5742870569229126
train gradient:  0.16181798409314063
iteration : 9311
train acc:  0.7734375
train loss:  0.4575347900390625
train gradient:  0.11301599476707762
iteration : 9312
train acc:  0.6953125
train loss:  0.5178988575935364
train gradient:  0.1260493706623632
iteration : 9313
train acc:  0.7578125
train loss:  0.4841170907020569
train gradient:  0.10537093613602469
iteration : 9314
train acc:  0.7890625
train loss:  0.5195057392120361
train gradient:  0.13454593246482838
iteration : 9315
train acc:  0.7578125
train loss:  0.4538588523864746
train gradient:  0.09081593237047549
iteration : 9316
train acc:  0.7734375
train loss:  0.46759748458862305
train gradient:  0.12610020527088045
iteration : 9317
train acc:  0.7421875
train loss:  0.44785383343696594
train gradient:  0.09821764829756359
iteration : 9318
train acc:  0.6484375
train loss:  0.5607779026031494
train gradient:  0.17608821352315185
iteration : 9319
train acc:  0.703125
train loss:  0.5576062202453613
train gradient:  0.1387763799128079
iteration : 9320
train acc:  0.7265625
train loss:  0.5180385112762451
train gradient:  0.1306862935083149
iteration : 9321
train acc:  0.78125
train loss:  0.48538440465927124
train gradient:  0.11070067577927145
iteration : 9322
train acc:  0.7421875
train loss:  0.5207598209381104
train gradient:  0.14872694943656042
iteration : 9323
train acc:  0.765625
train loss:  0.48391515016555786
train gradient:  0.1284098799902813
iteration : 9324
train acc:  0.6953125
train loss:  0.534188985824585
train gradient:  0.13606385501150342
iteration : 9325
train acc:  0.8046875
train loss:  0.398465096950531
train gradient:  0.09435872451249196
iteration : 9326
train acc:  0.65625
train loss:  0.565214991569519
train gradient:  0.17867906513605375
iteration : 9327
train acc:  0.75
train loss:  0.4771210253238678
train gradient:  0.11420972538811208
iteration : 9328
train acc:  0.765625
train loss:  0.48140811920166016
train gradient:  0.11741932176465597
iteration : 9329
train acc:  0.6953125
train loss:  0.46695834398269653
train gradient:  0.11274758148135065
iteration : 9330
train acc:  0.734375
train loss:  0.49772971868515015
train gradient:  0.11264787932681815
iteration : 9331
train acc:  0.7578125
train loss:  0.5363274812698364
train gradient:  0.16599204060776007
iteration : 9332
train acc:  0.796875
train loss:  0.4543985426425934
train gradient:  0.10499373213345559
iteration : 9333
train acc:  0.7265625
train loss:  0.559938907623291
train gradient:  0.16047941198806964
iteration : 9334
train acc:  0.6796875
train loss:  0.5507287979125977
train gradient:  0.16210321609607392
iteration : 9335
train acc:  0.7421875
train loss:  0.5361381769180298
train gradient:  0.11340873302805018
iteration : 9336
train acc:  0.734375
train loss:  0.468989759683609
train gradient:  0.10709722458282618
iteration : 9337
train acc:  0.71875
train loss:  0.5546746253967285
train gradient:  0.1295508029352448
iteration : 9338
train acc:  0.71875
train loss:  0.5271230936050415
train gradient:  0.16202018351834668
iteration : 9339
train acc:  0.7734375
train loss:  0.4259661138057709
train gradient:  0.09650358889009286
iteration : 9340
train acc:  0.7109375
train loss:  0.5348929166793823
train gradient:  0.14540096250527101
iteration : 9341
train acc:  0.7109375
train loss:  0.5102282166481018
train gradient:  0.13761769628109433
iteration : 9342
train acc:  0.7265625
train loss:  0.5634274482727051
train gradient:  0.2021157136246657
iteration : 9343
train acc:  0.75
train loss:  0.4662272036075592
train gradient:  0.09875250190431874
iteration : 9344
train acc:  0.7265625
train loss:  0.5347278118133545
train gradient:  0.16139380492461158
iteration : 9345
train acc:  0.7421875
train loss:  0.5336962938308716
train gradient:  0.12129335894616985
iteration : 9346
train acc:  0.7734375
train loss:  0.4591107964515686
train gradient:  0.12769021997904756
iteration : 9347
train acc:  0.6171875
train loss:  0.6202090382575989
train gradient:  0.19594401503043693
iteration : 9348
train acc:  0.75
train loss:  0.47336187958717346
train gradient:  0.11575434696716981
iteration : 9349
train acc:  0.75
train loss:  0.5407041311264038
train gradient:  0.13547002287435056
iteration : 9350
train acc:  0.6796875
train loss:  0.5430816411972046
train gradient:  0.19004281725455047
iteration : 9351
train acc:  0.7734375
train loss:  0.44612663984298706
train gradient:  0.07588204922728274
iteration : 9352
train acc:  0.71875
train loss:  0.5399569272994995
train gradient:  0.12293611563065919
iteration : 9353
train acc:  0.734375
train loss:  0.5114607810974121
train gradient:  0.14270086595653111
iteration : 9354
train acc:  0.734375
train loss:  0.4706123173236847
train gradient:  0.10378939358544322
iteration : 9355
train acc:  0.7109375
train loss:  0.5028995871543884
train gradient:  0.10538292051755299
iteration : 9356
train acc:  0.6640625
train loss:  0.5991150140762329
train gradient:  0.16331685642433386
iteration : 9357
train acc:  0.78125
train loss:  0.46909868717193604
train gradient:  0.10453375855515326
iteration : 9358
train acc:  0.8359375
train loss:  0.4063764214515686
train gradient:  0.08406527650626223
iteration : 9359
train acc:  0.765625
train loss:  0.5009584426879883
train gradient:  0.1184107890877462
iteration : 9360
train acc:  0.75
train loss:  0.5075252056121826
train gradient:  0.1455354913697905
iteration : 9361
train acc:  0.6953125
train loss:  0.5170114040374756
train gradient:  0.15225527893237253
iteration : 9362
train acc:  0.7578125
train loss:  0.44284939765930176
train gradient:  0.10174896255747293
iteration : 9363
train acc:  0.71875
train loss:  0.5068849325180054
train gradient:  0.10552425077096259
iteration : 9364
train acc:  0.71875
train loss:  0.48942849040031433
train gradient:  0.132019537111987
iteration : 9365
train acc:  0.765625
train loss:  0.5003194808959961
train gradient:  0.12914354540491682
iteration : 9366
train acc:  0.75
train loss:  0.4614119529724121
train gradient:  0.1116095786092453
iteration : 9367
train acc:  0.6640625
train loss:  0.5321015119552612
train gradient:  0.11987459381216693
iteration : 9368
train acc:  0.7265625
train loss:  0.5628747940063477
train gradient:  0.2116170370471911
iteration : 9369
train acc:  0.7421875
train loss:  0.5224065184593201
train gradient:  0.15113740888361848
iteration : 9370
train acc:  0.7421875
train loss:  0.4580596089363098
train gradient:  0.11052191999232312
iteration : 9371
train acc:  0.7890625
train loss:  0.45982229709625244
train gradient:  0.10939782781620634
iteration : 9372
train acc:  0.7109375
train loss:  0.5191134810447693
train gradient:  0.14216621375024743
iteration : 9373
train acc:  0.7265625
train loss:  0.5194824934005737
train gradient:  0.15094918007616154
iteration : 9374
train acc:  0.7578125
train loss:  0.4909321069717407
train gradient:  0.11141019413077359
iteration : 9375
train acc:  0.703125
train loss:  0.5321805477142334
train gradient:  0.13470760063741877
iteration : 9376
train acc:  0.7578125
train loss:  0.4671928882598877
train gradient:  0.10765113181221773
iteration : 9377
train acc:  0.734375
train loss:  0.5252981185913086
train gradient:  0.145804134491162
iteration : 9378
train acc:  0.8046875
train loss:  0.430814266204834
train gradient:  0.10418555420662433
iteration : 9379
train acc:  0.8125
train loss:  0.4466755986213684
train gradient:  0.1264863520925073
iteration : 9380
train acc:  0.7421875
train loss:  0.47345852851867676
train gradient:  0.1144305737613961
iteration : 9381
train acc:  0.71875
train loss:  0.4949064254760742
train gradient:  0.12744746779725746
iteration : 9382
train acc:  0.8046875
train loss:  0.4152345061302185
train gradient:  0.10062323689946853
iteration : 9383
train acc:  0.6484375
train loss:  0.5133938789367676
train gradient:  0.1060053219487784
iteration : 9384
train acc:  0.7265625
train loss:  0.4933171570301056
train gradient:  0.10346440970478507
iteration : 9385
train acc:  0.734375
train loss:  0.5216580033302307
train gradient:  0.1646811119473473
iteration : 9386
train acc:  0.734375
train loss:  0.5426735877990723
train gradient:  0.12926554893527814
iteration : 9387
train acc:  0.7265625
train loss:  0.5023021101951599
train gradient:  0.11723172818011528
iteration : 9388
train acc:  0.6953125
train loss:  0.4946289658546448
train gradient:  0.13534346054715646
iteration : 9389
train acc:  0.828125
train loss:  0.37978214025497437
train gradient:  0.10169845157849611
iteration : 9390
train acc:  0.7265625
train loss:  0.5406343936920166
train gradient:  0.16986128959899033
iteration : 9391
train acc:  0.734375
train loss:  0.5067199468612671
train gradient:  0.16025290006119075
iteration : 9392
train acc:  0.6875
train loss:  0.5071073770523071
train gradient:  0.12367588466405238
iteration : 9393
train acc:  0.7265625
train loss:  0.5144539475440979
train gradient:  0.14210739366221786
iteration : 9394
train acc:  0.6953125
train loss:  0.5694723129272461
train gradient:  0.1265508213207428
iteration : 9395
train acc:  0.7890625
train loss:  0.47027701139450073
train gradient:  0.10586582818909458
iteration : 9396
train acc:  0.75
train loss:  0.4972672164440155
train gradient:  0.11020975203743791
iteration : 9397
train acc:  0.7578125
train loss:  0.46760696172714233
train gradient:  0.11439602106498424
iteration : 9398
train acc:  0.75
train loss:  0.5124566555023193
train gradient:  0.154962719051605
iteration : 9399
train acc:  0.765625
train loss:  0.4565364718437195
train gradient:  0.1263256002895825
iteration : 9400
train acc:  0.71875
train loss:  0.5376719236373901
train gradient:  0.13164433520455848
iteration : 9401
train acc:  0.78125
train loss:  0.46170076727867126
train gradient:  0.13380608804965932
iteration : 9402
train acc:  0.703125
train loss:  0.5244421362876892
train gradient:  0.16090929566444834
iteration : 9403
train acc:  0.7734375
train loss:  0.42741507291793823
train gradient:  0.10370431857517805
iteration : 9404
train acc:  0.71875
train loss:  0.49517586827278137
train gradient:  0.12043505660934378
iteration : 9405
train acc:  0.78125
train loss:  0.464779257774353
train gradient:  0.11515043848714071
iteration : 9406
train acc:  0.7421875
train loss:  0.5312008857727051
train gradient:  0.16214362753399592
iteration : 9407
train acc:  0.7265625
train loss:  0.5182344913482666
train gradient:  0.13368070721955524
iteration : 9408
train acc:  0.7109375
train loss:  0.5066814422607422
train gradient:  0.12193658706015267
iteration : 9409
train acc:  0.7109375
train loss:  0.48746052384376526
train gradient:  0.10062280063988266
iteration : 9410
train acc:  0.7734375
train loss:  0.4881547689437866
train gradient:  0.1305608247731529
iteration : 9411
train acc:  0.6953125
train loss:  0.5599411129951477
train gradient:  0.23851799231311435
iteration : 9412
train acc:  0.7421875
train loss:  0.4967603087425232
train gradient:  0.11611509241890451
iteration : 9413
train acc:  0.7578125
train loss:  0.4612123668193817
train gradient:  0.12349220274804307
iteration : 9414
train acc:  0.78125
train loss:  0.45423150062561035
train gradient:  0.09979161148646672
iteration : 9415
train acc:  0.7578125
train loss:  0.4659443199634552
train gradient:  0.10558509297627178
iteration : 9416
train acc:  0.7109375
train loss:  0.5251866579055786
train gradient:  0.12520767053327844
iteration : 9417
train acc:  0.6953125
train loss:  0.5911121368408203
train gradient:  0.1568132717732053
iteration : 9418
train acc:  0.7578125
train loss:  0.4390110373497009
train gradient:  0.0886560361751636
iteration : 9419
train acc:  0.6484375
train loss:  0.5998997092247009
train gradient:  0.2044216561813299
iteration : 9420
train acc:  0.6953125
train loss:  0.5356398820877075
train gradient:  0.14198871139531832
iteration : 9421
train acc:  0.7734375
train loss:  0.4466496706008911
train gradient:  0.10468071554040795
iteration : 9422
train acc:  0.7421875
train loss:  0.5158355236053467
train gradient:  0.14277060205216663
iteration : 9423
train acc:  0.6953125
train loss:  0.5587367415428162
train gradient:  0.14990385683044377
iteration : 9424
train acc:  0.71875
train loss:  0.5070326328277588
train gradient:  0.15242820803470278
iteration : 9425
train acc:  0.8125
train loss:  0.4149618148803711
train gradient:  0.09887449297169659
iteration : 9426
train acc:  0.75
train loss:  0.5002476572990417
train gradient:  0.10482074135665247
iteration : 9427
train acc:  0.8046875
train loss:  0.42775267362594604
train gradient:  0.10043616029280786
iteration : 9428
train acc:  0.75
train loss:  0.5239317417144775
train gradient:  0.1384329076499018
iteration : 9429
train acc:  0.796875
train loss:  0.46783849596977234
train gradient:  0.09847443849334168
iteration : 9430
train acc:  0.765625
train loss:  0.5032448172569275
train gradient:  0.12985968687081462
iteration : 9431
train acc:  0.75
train loss:  0.5057958364486694
train gradient:  0.11456140286792225
iteration : 9432
train acc:  0.734375
train loss:  0.5195671319961548
train gradient:  0.16505561293918752
iteration : 9433
train acc:  0.7890625
train loss:  0.4917268753051758
train gradient:  0.12220815879742829
iteration : 9434
train acc:  0.640625
train loss:  0.5193701386451721
train gradient:  0.14016565276234172
iteration : 9435
train acc:  0.8125
train loss:  0.4277108311653137
train gradient:  0.09541989901459791
iteration : 9436
train acc:  0.734375
train loss:  0.4717790484428406
train gradient:  0.11873899529449349
iteration : 9437
train acc:  0.7578125
train loss:  0.5116102695465088
train gradient:  0.11946212288249705
iteration : 9438
train acc:  0.796875
train loss:  0.46196383237838745
train gradient:  0.11086612534552263
iteration : 9439
train acc:  0.6875
train loss:  0.5559251308441162
train gradient:  0.1371729943927211
iteration : 9440
train acc:  0.7109375
train loss:  0.5391581654548645
train gradient:  0.1579214558288301
iteration : 9441
train acc:  0.75
train loss:  0.503598690032959
train gradient:  0.13784257521135496
iteration : 9442
train acc:  0.75
train loss:  0.5164318084716797
train gradient:  0.11776813854348774
iteration : 9443
train acc:  0.75
train loss:  0.47981521487236023
train gradient:  0.10963822880590508
iteration : 9444
train acc:  0.7109375
train loss:  0.5156323909759521
train gradient:  0.11345725810596909
iteration : 9445
train acc:  0.7109375
train loss:  0.485027015209198
train gradient:  0.11654615145256947
iteration : 9446
train acc:  0.71875
train loss:  0.5092098116874695
train gradient:  0.12278081190026577
iteration : 9447
train acc:  0.71875
train loss:  0.4925771951675415
train gradient:  0.11983632182675087
iteration : 9448
train acc:  0.78125
train loss:  0.42477232217788696
train gradient:  0.10736094817393492
iteration : 9449
train acc:  0.65625
train loss:  0.5533697605133057
train gradient:  0.17964977181272548
iteration : 9450
train acc:  0.8203125
train loss:  0.40606164932250977
train gradient:  0.10433393777672305
iteration : 9451
train acc:  0.78125
train loss:  0.4901841878890991
train gradient:  0.11916301058592518
iteration : 9452
train acc:  0.7265625
train loss:  0.5041278600692749
train gradient:  0.14764615429888667
iteration : 9453
train acc:  0.7265625
train loss:  0.48957210779190063
train gradient:  0.14601671122254
iteration : 9454
train acc:  0.734375
train loss:  0.49261054396629333
train gradient:  0.11401521275058445
iteration : 9455
train acc:  0.8125
train loss:  0.4728442132472992
train gradient:  0.11857304185499311
iteration : 9456
train acc:  0.71875
train loss:  0.572868824005127
train gradient:  0.16594936490131898
iteration : 9457
train acc:  0.71875
train loss:  0.5183945894241333
train gradient:  0.14749934964806666
iteration : 9458
train acc:  0.7265625
train loss:  0.5020887851715088
train gradient:  0.15027223421820335
iteration : 9459
train acc:  0.6640625
train loss:  0.5783288478851318
train gradient:  0.18297551364054054
iteration : 9460
train acc:  0.75
train loss:  0.5000197887420654
train gradient:  0.17932815975280467
iteration : 9461
train acc:  0.7890625
train loss:  0.5037992000579834
train gradient:  0.14208906647865696
iteration : 9462
train acc:  0.765625
train loss:  0.4739897549152374
train gradient:  0.15989530785488415
iteration : 9463
train acc:  0.734375
train loss:  0.46703702211380005
train gradient:  0.1053814601589581
iteration : 9464
train acc:  0.734375
train loss:  0.4622872471809387
train gradient:  0.11745501013405679
iteration : 9465
train acc:  0.8125
train loss:  0.464383602142334
train gradient:  0.12362226774591688
iteration : 9466
train acc:  0.671875
train loss:  0.550125777721405
train gradient:  0.13159583880964118
iteration : 9467
train acc:  0.78125
train loss:  0.46536439657211304
train gradient:  0.13012411809804275
iteration : 9468
train acc:  0.7890625
train loss:  0.4196414649486542
train gradient:  0.08483291030499202
iteration : 9469
train acc:  0.6875
train loss:  0.5268129110336304
train gradient:  0.15883168478095117
iteration : 9470
train acc:  0.796875
train loss:  0.46364760398864746
train gradient:  0.10764325021152119
iteration : 9471
train acc:  0.6796875
train loss:  0.5541204214096069
train gradient:  0.1831068630444957
iteration : 9472
train acc:  0.65625
train loss:  0.5576061010360718
train gradient:  0.15198971888395005
iteration : 9473
train acc:  0.78125
train loss:  0.4076249599456787
train gradient:  0.10574451629845968
iteration : 9474
train acc:  0.8203125
train loss:  0.4237845540046692
train gradient:  0.1054150580827994
iteration : 9475
train acc:  0.765625
train loss:  0.4608248770236969
train gradient:  0.11583580195102569
iteration : 9476
train acc:  0.7734375
train loss:  0.4166443943977356
train gradient:  0.12594905993106764
iteration : 9477
train acc:  0.7421875
train loss:  0.5211455821990967
train gradient:  0.1709834344578507
iteration : 9478
train acc:  0.734375
train loss:  0.44974204897880554
train gradient:  0.1017717812768623
iteration : 9479
train acc:  0.7890625
train loss:  0.4582681655883789
train gradient:  0.11862480645714264
iteration : 9480
train acc:  0.6875
train loss:  0.5696433782577515
train gradient:  0.15577549446626027
iteration : 9481
train acc:  0.765625
train loss:  0.44066497683525085
train gradient:  0.13312340162434583
iteration : 9482
train acc:  0.796875
train loss:  0.4010244607925415
train gradient:  0.08468508072553084
iteration : 9483
train acc:  0.7109375
train loss:  0.5704700946807861
train gradient:  0.15000092651467237
iteration : 9484
train acc:  0.734375
train loss:  0.502777636051178
train gradient:  0.15260435497413388
iteration : 9485
train acc:  0.75
train loss:  0.46473804116249084
train gradient:  0.11017499998764133
iteration : 9486
train acc:  0.7890625
train loss:  0.42210671305656433
train gradient:  0.08935525055612469
iteration : 9487
train acc:  0.7109375
train loss:  0.5076190233230591
train gradient:  0.1350478663501983
iteration : 9488
train acc:  0.6953125
train loss:  0.5147660970687866
train gradient:  0.12503104665575254
iteration : 9489
train acc:  0.734375
train loss:  0.5232660174369812
train gradient:  0.11720076260840719
iteration : 9490
train acc:  0.7109375
train loss:  0.5319381952285767
train gradient:  0.14533979005732095
iteration : 9491
train acc:  0.71875
train loss:  0.5029274225234985
train gradient:  0.1139922735524848
iteration : 9492
train acc:  0.75
train loss:  0.5364169478416443
train gradient:  0.1950660549832779
iteration : 9493
train acc:  0.75
train loss:  0.48544013500213623
train gradient:  0.11630785652017253
iteration : 9494
train acc:  0.6484375
train loss:  0.5879185795783997
train gradient:  0.15756737298477835
iteration : 9495
train acc:  0.71875
train loss:  0.5143204927444458
train gradient:  0.12351719583141778
iteration : 9496
train acc:  0.6875
train loss:  0.5446712374687195
train gradient:  0.1514518238745472
iteration : 9497
train acc:  0.6875
train loss:  0.5177129507064819
train gradient:  0.13541361323731926
iteration : 9498
train acc:  0.703125
train loss:  0.5195891261100769
train gradient:  0.12029203984665908
iteration : 9499
train acc:  0.7890625
train loss:  0.4651598632335663
train gradient:  0.21929579576240654
iteration : 9500
train acc:  0.7421875
train loss:  0.5038991570472717
train gradient:  0.13736197653405638
iteration : 9501
train acc:  0.65625
train loss:  0.557181715965271
train gradient:  0.14927915825868981
iteration : 9502
train acc:  0.765625
train loss:  0.48516684770584106
train gradient:  0.11904779134157964
iteration : 9503
train acc:  0.703125
train loss:  0.5530543327331543
train gradient:  0.15905205527031552
iteration : 9504
train acc:  0.8046875
train loss:  0.42166274785995483
train gradient:  0.09466116685254278
iteration : 9505
train acc:  0.7109375
train loss:  0.5346629619598389
train gradient:  0.1428285771851846
iteration : 9506
train acc:  0.71875
train loss:  0.5205974578857422
train gradient:  0.1508288175996636
iteration : 9507
train acc:  0.65625
train loss:  0.5586598515510559
train gradient:  0.18417311038238354
iteration : 9508
train acc:  0.734375
train loss:  0.48408716917037964
train gradient:  0.108098960065955
iteration : 9509
train acc:  0.7578125
train loss:  0.49031364917755127
train gradient:  0.12624809178348598
iteration : 9510
train acc:  0.75
train loss:  0.4682258069515228
train gradient:  0.11218191532497084
iteration : 9511
train acc:  0.75
train loss:  0.4869222939014435
train gradient:  0.1236915960889145
iteration : 9512
train acc:  0.734375
train loss:  0.4465639293193817
train gradient:  0.12089270793747528
iteration : 9513
train acc:  0.7421875
train loss:  0.49896687269210815
train gradient:  0.16322413394578456
iteration : 9514
train acc:  0.734375
train loss:  0.49555495381355286
train gradient:  0.11417091489309658
iteration : 9515
train acc:  0.75
train loss:  0.5220241546630859
train gradient:  0.13800423295532432
iteration : 9516
train acc:  0.75
train loss:  0.45774364471435547
train gradient:  0.10877941966288396
iteration : 9517
train acc:  0.7578125
train loss:  0.46356672048568726
train gradient:  0.12068537635479364
iteration : 9518
train acc:  0.796875
train loss:  0.4721536338329315
train gradient:  0.1254324459497781
iteration : 9519
train acc:  0.7578125
train loss:  0.49610620737075806
train gradient:  0.127173930582458
iteration : 9520
train acc:  0.703125
train loss:  0.577377438545227
train gradient:  0.1442586479515201
iteration : 9521
train acc:  0.671875
train loss:  0.6516541242599487
train gradient:  0.25848766041864457
iteration : 9522
train acc:  0.7421875
train loss:  0.4668455719947815
train gradient:  0.09443415289135708
iteration : 9523
train acc:  0.6953125
train loss:  0.580591082572937
train gradient:  0.15010781446760252
iteration : 9524
train acc:  0.71875
train loss:  0.5037816166877747
train gradient:  0.10569130059974445
iteration : 9525
train acc:  0.765625
train loss:  0.48779943585395813
train gradient:  0.11775585876122054
iteration : 9526
train acc:  0.71875
train loss:  0.5358642339706421
train gradient:  0.1377581346869896
iteration : 9527
train acc:  0.7421875
train loss:  0.4950728118419647
train gradient:  0.1351582503289889
iteration : 9528
train acc:  0.765625
train loss:  0.43668630719184875
train gradient:  0.08892160191435469
iteration : 9529
train acc:  0.7734375
train loss:  0.45466160774230957
train gradient:  0.13437302597584008
iteration : 9530
train acc:  0.7734375
train loss:  0.4801154136657715
train gradient:  0.11347297130667687
iteration : 9531
train acc:  0.75
train loss:  0.4840887188911438
train gradient:  0.10214239669178454
iteration : 9532
train acc:  0.7421875
train loss:  0.5179604291915894
train gradient:  0.1186427223022298
iteration : 9533
train acc:  0.75
train loss:  0.49875691533088684
train gradient:  0.10678734512104442
iteration : 9534
train acc:  0.734375
train loss:  0.5608898997306824
train gradient:  0.13161329932456417
iteration : 9535
train acc:  0.6796875
train loss:  0.5095769166946411
train gradient:  0.11978380978788511
iteration : 9536
train acc:  0.7421875
train loss:  0.4859873056411743
train gradient:  0.13440039032916917
iteration : 9537
train acc:  0.7265625
train loss:  0.5271284580230713
train gradient:  0.12825984692777698
iteration : 9538
train acc:  0.7109375
train loss:  0.5076545476913452
train gradient:  0.10521015348388553
iteration : 9539
train acc:  0.7109375
train loss:  0.5010022521018982
train gradient:  0.12212609764963418
iteration : 9540
train acc:  0.8125
train loss:  0.44305363297462463
train gradient:  0.11984805899001111
iteration : 9541
train acc:  0.8046875
train loss:  0.3985127806663513
train gradient:  0.09662938846616695
iteration : 9542
train acc:  0.671875
train loss:  0.48981571197509766
train gradient:  0.12089891603107031
iteration : 9543
train acc:  0.8046875
train loss:  0.4517977833747864
train gradient:  0.10246742311380251
iteration : 9544
train acc:  0.7421875
train loss:  0.4933026432991028
train gradient:  0.13200416578941132
iteration : 9545
train acc:  0.6875
train loss:  0.5228058099746704
train gradient:  0.12061835447355923
iteration : 9546
train acc:  0.765625
train loss:  0.5602874755859375
train gradient:  0.14930934684480968
iteration : 9547
train acc:  0.75
train loss:  0.4819812774658203
train gradient:  0.10784859363422639
iteration : 9548
train acc:  0.75
train loss:  0.5185503959655762
train gradient:  0.18775492940060368
iteration : 9549
train acc:  0.7265625
train loss:  0.5064270496368408
train gradient:  0.13245248032910528
iteration : 9550
train acc:  0.7265625
train loss:  0.5330521464347839
train gradient:  0.14504588922630918
iteration : 9551
train acc:  0.765625
train loss:  0.45170801877975464
train gradient:  0.12118714371620425
iteration : 9552
train acc:  0.7265625
train loss:  0.5394762754440308
train gradient:  0.13977964832912734
iteration : 9553
train acc:  0.859375
train loss:  0.36662039160728455
train gradient:  0.0948473814606066
iteration : 9554
train acc:  0.734375
train loss:  0.5235829949378967
train gradient:  0.12380671322403596
iteration : 9555
train acc:  0.6875
train loss:  0.5264754891395569
train gradient:  0.11684383993029727
iteration : 9556
train acc:  0.71875
train loss:  0.5862370133399963
train gradient:  0.16886585415127348
iteration : 9557
train acc:  0.765625
train loss:  0.4886783957481384
train gradient:  0.10380728905118046
iteration : 9558
train acc:  0.6875
train loss:  0.5634466409683228
train gradient:  0.1536376230284877
iteration : 9559
train acc:  0.7265625
train loss:  0.5134381055831909
train gradient:  0.1422060152209681
iteration : 9560
train acc:  0.765625
train loss:  0.4942868947982788
train gradient:  0.1343812211171965
iteration : 9561
train acc:  0.6875
train loss:  0.5620574951171875
train gradient:  0.1335032603008789
iteration : 9562
train acc:  0.7265625
train loss:  0.5047346353530884
train gradient:  0.17734629834369256
iteration : 9563
train acc:  0.6796875
train loss:  0.6014276742935181
train gradient:  0.2081810645925219
iteration : 9564
train acc:  0.7109375
train loss:  0.5103403925895691
train gradient:  0.11627913436635492
iteration : 9565
train acc:  0.7734375
train loss:  0.461111456155777
train gradient:  0.124313818024569
iteration : 9566
train acc:  0.671875
train loss:  0.5937389135360718
train gradient:  0.20364141022981633
iteration : 9567
train acc:  0.71875
train loss:  0.49995753169059753
train gradient:  0.10318880124724693
iteration : 9568
train acc:  0.734375
train loss:  0.512834370136261
train gradient:  0.14628703805545162
iteration : 9569
train acc:  0.765625
train loss:  0.4652060270309448
train gradient:  0.15556356780542888
iteration : 9570
train acc:  0.75
train loss:  0.4815046787261963
train gradient:  0.1057470269235893
iteration : 9571
train acc:  0.78125
train loss:  0.4491875171661377
train gradient:  0.12013269383921125
iteration : 9572
train acc:  0.7109375
train loss:  0.5070524215698242
train gradient:  0.13650408222299304
iteration : 9573
train acc:  0.6796875
train loss:  0.5367356538772583
train gradient:  0.1318000999886484
iteration : 9574
train acc:  0.734375
train loss:  0.5152252316474915
train gradient:  0.15274986082950584
iteration : 9575
train acc:  0.6953125
train loss:  0.511363685131073
train gradient:  0.137473530465204
iteration : 9576
train acc:  0.7734375
train loss:  0.44071483612060547
train gradient:  0.11622326394737165
iteration : 9577
train acc:  0.7734375
train loss:  0.4425952434539795
train gradient:  0.10632551678878012
iteration : 9578
train acc:  0.8125
train loss:  0.42185941338539124
train gradient:  0.1068472303838491
iteration : 9579
train acc:  0.7578125
train loss:  0.47527921199798584
train gradient:  0.12879617857958253
iteration : 9580
train acc:  0.671875
train loss:  0.5526010990142822
train gradient:  0.15473267383899775
iteration : 9581
train acc:  0.8046875
train loss:  0.42520636320114136
train gradient:  0.10199202378723113
iteration : 9582
train acc:  0.7109375
train loss:  0.5581202507019043
train gradient:  0.18151429916591078
iteration : 9583
train acc:  0.78125
train loss:  0.4509667158126831
train gradient:  0.12336697928832831
iteration : 9584
train acc:  0.765625
train loss:  0.5152559876441956
train gradient:  0.14400316122105977
iteration : 9585
train acc:  0.6953125
train loss:  0.5785783529281616
train gradient:  0.15622734015191087
iteration : 9586
train acc:  0.75
train loss:  0.44100508093833923
train gradient:  0.0932916794211381
iteration : 9587
train acc:  0.7109375
train loss:  0.5205146074295044
train gradient:  0.12986014551220143
iteration : 9588
train acc:  0.7421875
train loss:  0.4964551031589508
train gradient:  0.11045803501073452
iteration : 9589
train acc:  0.75
train loss:  0.5813188552856445
train gradient:  0.20866744151833477
iteration : 9590
train acc:  0.7109375
train loss:  0.5259056091308594
train gradient:  0.12580785985987714
iteration : 9591
train acc:  0.7265625
train loss:  0.5312172174453735
train gradient:  0.14245381392020856
iteration : 9592
train acc:  0.78125
train loss:  0.43185412883758545
train gradient:  0.10110178152580362
iteration : 9593
train acc:  0.6640625
train loss:  0.5584406852722168
train gradient:  0.15546323450179073
iteration : 9594
train acc:  0.78125
train loss:  0.4845626950263977
train gradient:  0.1197763454974531
iteration : 9595
train acc:  0.7734375
train loss:  0.47439032793045044
train gradient:  0.10660789203052701
iteration : 9596
train acc:  0.78125
train loss:  0.4476073384284973
train gradient:  0.11237974479163812
iteration : 9597
train acc:  0.6953125
train loss:  0.4785194993019104
train gradient:  0.15102828694063958
iteration : 9598
train acc:  0.6875
train loss:  0.5784919261932373
train gradient:  0.15611530083338745
iteration : 9599
train acc:  0.671875
train loss:  0.5863056778907776
train gradient:  0.14808258727428042
iteration : 9600
train acc:  0.7109375
train loss:  0.6172378659248352
train gradient:  0.1936165571900484
iteration : 9601
train acc:  0.71875
train loss:  0.5584481954574585
train gradient:  0.1375604423778377
iteration : 9602
train acc:  0.765625
train loss:  0.4430205821990967
train gradient:  0.10044041145210043
iteration : 9603
train acc:  0.7890625
train loss:  0.5063216090202332
train gradient:  0.09816892896720876
iteration : 9604
train acc:  0.78125
train loss:  0.5145946741104126
train gradient:  0.13364631741444472
iteration : 9605
train acc:  0.7421875
train loss:  0.5238064527511597
train gradient:  0.1502239002592981
iteration : 9606
train acc:  0.75
train loss:  0.5232597589492798
train gradient:  0.11885341659411397
iteration : 9607
train acc:  0.703125
train loss:  0.5225868821144104
train gradient:  0.16964528054098593
iteration : 9608
train acc:  0.7265625
train loss:  0.477008193731308
train gradient:  0.11049521168564402
iteration : 9609
train acc:  0.65625
train loss:  0.5699174404144287
train gradient:  0.14223265353806203
iteration : 9610
train acc:  0.734375
train loss:  0.4870048761367798
train gradient:  0.16601285007631317
iteration : 9611
train acc:  0.796875
train loss:  0.4516172409057617
train gradient:  0.08913039587093732
iteration : 9612
train acc:  0.703125
train loss:  0.5503485798835754
train gradient:  0.13388551296300266
iteration : 9613
train acc:  0.7578125
train loss:  0.45470064878463745
train gradient:  0.13279387590810426
iteration : 9614
train acc:  0.7109375
train loss:  0.4740866422653198
train gradient:  0.10510678958313012
iteration : 9615
train acc:  0.7421875
train loss:  0.4841587543487549
train gradient:  0.1369077279864166
iteration : 9616
train acc:  0.734375
train loss:  0.5206791758537292
train gradient:  0.11232298190904674
iteration : 9617
train acc:  0.7890625
train loss:  0.4933307468891144
train gradient:  0.09595690503387921
iteration : 9618
train acc:  0.7265625
train loss:  0.4910486340522766
train gradient:  0.116305179080986
iteration : 9619
train acc:  0.7734375
train loss:  0.526866614818573
train gradient:  0.14592279844923695
iteration : 9620
train acc:  0.7734375
train loss:  0.44342291355133057
train gradient:  0.09295178989352419
iteration : 9621
train acc:  0.7421875
train loss:  0.5705083608627319
train gradient:  0.1603792897140064
iteration : 9622
train acc:  0.7265625
train loss:  0.46714991331100464
train gradient:  0.12876700168653432
iteration : 9623
train acc:  0.7421875
train loss:  0.4970723092556
train gradient:  0.12071882473095726
iteration : 9624
train acc:  0.703125
train loss:  0.542555570602417
train gradient:  0.1321979811422962
iteration : 9625
train acc:  0.765625
train loss:  0.44986486434936523
train gradient:  0.11128367550868537
iteration : 9626
train acc:  0.7265625
train loss:  0.5072628259658813
train gradient:  0.10514453008518308
iteration : 9627
train acc:  0.703125
train loss:  0.5383045077323914
train gradient:  0.14247440326208002
iteration : 9628
train acc:  0.6796875
train loss:  0.5864171981811523
train gradient:  0.15186953982797236
iteration : 9629
train acc:  0.65625
train loss:  0.5599992275238037
train gradient:  0.14181537137529848
iteration : 9630
train acc:  0.71875
train loss:  0.540947437286377
train gradient:  0.13478115875465324
iteration : 9631
train acc:  0.75
train loss:  0.4532564878463745
train gradient:  0.100261109259781
iteration : 9632
train acc:  0.7421875
train loss:  0.5029173493385315
train gradient:  0.14870174257788588
iteration : 9633
train acc:  0.6875
train loss:  0.5749013423919678
train gradient:  0.14179632705697343
iteration : 9634
train acc:  0.7421875
train loss:  0.5087214708328247
train gradient:  0.13922291381373175
iteration : 9635
train acc:  0.7890625
train loss:  0.46901053190231323
train gradient:  0.11029726915506025
iteration : 9636
train acc:  0.7578125
train loss:  0.45175158977508545
train gradient:  0.11741868297080241
iteration : 9637
train acc:  0.7265625
train loss:  0.510388970375061
train gradient:  0.11985780970952102
iteration : 9638
train acc:  0.6640625
train loss:  0.5822132229804993
train gradient:  0.13900230887642723
iteration : 9639
train acc:  0.734375
train loss:  0.46846091747283936
train gradient:  0.11012028588145353
iteration : 9640
train acc:  0.734375
train loss:  0.48387980461120605
train gradient:  0.11588419990258396
iteration : 9641
train acc:  0.78125
train loss:  0.43795689940452576
train gradient:  0.0986530002319684
iteration : 9642
train acc:  0.7578125
train loss:  0.47714754939079285
train gradient:  0.11327733749848728
iteration : 9643
train acc:  0.765625
train loss:  0.4620892107486725
train gradient:  0.09233061488709894
iteration : 9644
train acc:  0.7734375
train loss:  0.4854399263858795
train gradient:  0.13164159201554515
iteration : 9645
train acc:  0.703125
train loss:  0.5815606117248535
train gradient:  0.19165890639065397
iteration : 9646
train acc:  0.734375
train loss:  0.5122913122177124
train gradient:  0.126716724013607
iteration : 9647
train acc:  0.7890625
train loss:  0.4604440927505493
train gradient:  0.1080157456269367
iteration : 9648
train acc:  0.7421875
train loss:  0.5273710489273071
train gradient:  0.16624338875148864
iteration : 9649
train acc:  0.7265625
train loss:  0.5057100653648376
train gradient:  0.1428812433018113
iteration : 9650
train acc:  0.765625
train loss:  0.4595324993133545
train gradient:  0.11932470098785647
iteration : 9651
train acc:  0.7109375
train loss:  0.547494649887085
train gradient:  0.15492880658740232
iteration : 9652
train acc:  0.6953125
train loss:  0.5655748844146729
train gradient:  0.14572243139297408
iteration : 9653
train acc:  0.6796875
train loss:  0.5801783204078674
train gradient:  0.14247318610096416
iteration : 9654
train acc:  0.71875
train loss:  0.5007928609848022
train gradient:  0.13920544024800496
iteration : 9655
train acc:  0.734375
train loss:  0.5422424077987671
train gradient:  0.1264333353574456
iteration : 9656
train acc:  0.7421875
train loss:  0.5423271656036377
train gradient:  0.13221788437471224
iteration : 9657
train acc:  0.7265625
train loss:  0.4803547263145447
train gradient:  0.11242553949389149
iteration : 9658
train acc:  0.8046875
train loss:  0.45214539766311646
train gradient:  0.10439225852076814
iteration : 9659
train acc:  0.7578125
train loss:  0.4930272400379181
train gradient:  0.11263102589354457
iteration : 9660
train acc:  0.8046875
train loss:  0.41116395592689514
train gradient:  0.09445289547778737
iteration : 9661
train acc:  0.734375
train loss:  0.5539368987083435
train gradient:  0.1779390337739266
iteration : 9662
train acc:  0.703125
train loss:  0.5350131988525391
train gradient:  0.1336039259430942
iteration : 9663
train acc:  0.7109375
train loss:  0.4874454736709595
train gradient:  0.09124477540064924
iteration : 9664
train acc:  0.75
train loss:  0.472614049911499
train gradient:  0.09845987399658009
iteration : 9665
train acc:  0.734375
train loss:  0.5241622924804688
train gradient:  0.11729597802678224
iteration : 9666
train acc:  0.6875
train loss:  0.5420328378677368
train gradient:  0.12399106919983875
iteration : 9667
train acc:  0.71875
train loss:  0.5347120761871338
train gradient:  0.11247715351227179
iteration : 9668
train acc:  0.78125
train loss:  0.4407505393028259
train gradient:  0.1151922225321859
iteration : 9669
train acc:  0.7421875
train loss:  0.46460217237472534
train gradient:  0.09883340652608581
iteration : 9670
train acc:  0.6953125
train loss:  0.5470569133758545
train gradient:  0.1654191969261132
iteration : 9671
train acc:  0.7265625
train loss:  0.500866174697876
train gradient:  0.11232059770122374
iteration : 9672
train acc:  0.734375
train loss:  0.48755356669425964
train gradient:  0.1175774550259807
iteration : 9673
train acc:  0.7265625
train loss:  0.4588935375213623
train gradient:  0.10847310841404818
iteration : 9674
train acc:  0.703125
train loss:  0.50235915184021
train gradient:  0.1268159444011837
iteration : 9675
train acc:  0.7578125
train loss:  0.4995814561843872
train gradient:  0.2202995378656803
iteration : 9676
train acc:  0.65625
train loss:  0.576148509979248
train gradient:  0.17814830591526687
iteration : 9677
train acc:  0.7578125
train loss:  0.46160152554512024
train gradient:  0.08854700496674944
iteration : 9678
train acc:  0.75
train loss:  0.4558646082878113
train gradient:  0.10301811948905217
iteration : 9679
train acc:  0.7578125
train loss:  0.5222780704498291
train gradient:  0.18567746080446995
iteration : 9680
train acc:  0.71875
train loss:  0.504454493522644
train gradient:  0.12300543303654844
iteration : 9681
train acc:  0.7265625
train loss:  0.4971888065338135
train gradient:  0.14975216400615785
iteration : 9682
train acc:  0.65625
train loss:  0.5836613178253174
train gradient:  0.16156717030485357
iteration : 9683
train acc:  0.7734375
train loss:  0.4357892870903015
train gradient:  0.09924311328009845
iteration : 9684
train acc:  0.6875
train loss:  0.5225740671157837
train gradient:  0.12427355933800394
iteration : 9685
train acc:  0.7109375
train loss:  0.5348097681999207
train gradient:  0.11770667894886565
iteration : 9686
train acc:  0.7421875
train loss:  0.5327651500701904
train gradient:  0.14142411852122772
iteration : 9687
train acc:  0.75
train loss:  0.500643253326416
train gradient:  0.12307062252470118
iteration : 9688
train acc:  0.71875
train loss:  0.4730224311351776
train gradient:  0.10188764630393993
iteration : 9689
train acc:  0.7109375
train loss:  0.5750765800476074
train gradient:  0.2167732787558711
iteration : 9690
train acc:  0.6953125
train loss:  0.5201283097267151
train gradient:  0.1056102325386151
iteration : 9691
train acc:  0.796875
train loss:  0.44268712401390076
train gradient:  0.0826513553212101
iteration : 9692
train acc:  0.7265625
train loss:  0.5082927346229553
train gradient:  0.12007818179195584
iteration : 9693
train acc:  0.703125
train loss:  0.5100082159042358
train gradient:  0.1210680658545501
iteration : 9694
train acc:  0.78125
train loss:  0.45260006189346313
train gradient:  0.09205981946266406
iteration : 9695
train acc:  0.765625
train loss:  0.4798364043235779
train gradient:  0.09308857787888383
iteration : 9696
train acc:  0.6875
train loss:  0.5635460615158081
train gradient:  0.13620059185004568
iteration : 9697
train acc:  0.6953125
train loss:  0.5710572004318237
train gradient:  0.14223675734567887
iteration : 9698
train acc:  0.78125
train loss:  0.5143976211547852
train gradient:  0.1388136776803183
iteration : 9699
train acc:  0.78125
train loss:  0.4550243616104126
train gradient:  0.11307743439170326
iteration : 9700
train acc:  0.7734375
train loss:  0.4744170904159546
train gradient:  0.11910804314782193
iteration : 9701
train acc:  0.7578125
train loss:  0.5028673410415649
train gradient:  0.1356970609920249
iteration : 9702
train acc:  0.7734375
train loss:  0.4604097008705139
train gradient:  0.09577630647943515
iteration : 9703
train acc:  0.78125
train loss:  0.4495229423046112
train gradient:  0.10706329889036406
iteration : 9704
train acc:  0.8125
train loss:  0.44088059663772583
train gradient:  0.10668204629387425
iteration : 9705
train acc:  0.7421875
train loss:  0.5044870376586914
train gradient:  0.13185457672985587
iteration : 9706
train acc:  0.6484375
train loss:  0.5762557983398438
train gradient:  0.17385059955386029
iteration : 9707
train acc:  0.6875
train loss:  0.5505181550979614
train gradient:  0.12631697016345958
iteration : 9708
train acc:  0.78125
train loss:  0.4066193699836731
train gradient:  0.08822785799150312
iteration : 9709
train acc:  0.71875
train loss:  0.4763917922973633
train gradient:  0.10593546936718677
iteration : 9710
train acc:  0.7265625
train loss:  0.49652695655822754
train gradient:  0.11218995289474658
iteration : 9711
train acc:  0.75
train loss:  0.5416650772094727
train gradient:  0.1693081800513329
iteration : 9712
train acc:  0.7578125
train loss:  0.47883081436157227
train gradient:  0.11564310941593595
iteration : 9713
train acc:  0.8125
train loss:  0.4221856892108917
train gradient:  0.1264071813644064
iteration : 9714
train acc:  0.765625
train loss:  0.47839340567588806
train gradient:  0.1007360821901137
iteration : 9715
train acc:  0.78125
train loss:  0.45782357454299927
train gradient:  0.10134235816730959
iteration : 9716
train acc:  0.734375
train loss:  0.524684488773346
train gradient:  0.16096851865471784
iteration : 9717
train acc:  0.71875
train loss:  0.527442991733551
train gradient:  0.15786243161378105
iteration : 9718
train acc:  0.7578125
train loss:  0.4690777659416199
train gradient:  0.13015888161476122
iteration : 9719
train acc:  0.6875
train loss:  0.5562260746955872
train gradient:  0.15735394298912223
iteration : 9720
train acc:  0.7578125
train loss:  0.5520575642585754
train gradient:  0.14351948219088206
iteration : 9721
train acc:  0.734375
train loss:  0.45756796002388
train gradient:  0.11631195978531256
iteration : 9722
train acc:  0.75
train loss:  0.5123058557510376
train gradient:  0.14410150311263897
iteration : 9723
train acc:  0.734375
train loss:  0.5258568525314331
train gradient:  0.1516262114921192
iteration : 9724
train acc:  0.78125
train loss:  0.42778629064559937
train gradient:  0.10461571389943909
iteration : 9725
train acc:  0.71875
train loss:  0.51935875415802
train gradient:  0.10902384500592475
iteration : 9726
train acc:  0.7265625
train loss:  0.5036900639533997
train gradient:  0.1201707250522767
iteration : 9727
train acc:  0.796875
train loss:  0.44024544954299927
train gradient:  0.14325474446436495
iteration : 9728
train acc:  0.78125
train loss:  0.4290301501750946
train gradient:  0.11601882260474386
iteration : 9729
train acc:  0.7421875
train loss:  0.4890197217464447
train gradient:  0.11271974484025023
iteration : 9730
train acc:  0.7578125
train loss:  0.5057789087295532
train gradient:  0.10898046411800236
iteration : 9731
train acc:  0.7890625
train loss:  0.4218793511390686
train gradient:  0.08579894386318128
iteration : 9732
train acc:  0.75
train loss:  0.46198177337646484
train gradient:  0.11466292617289968
iteration : 9733
train acc:  0.7421875
train loss:  0.5351359248161316
train gradient:  0.1460149996722876
iteration : 9734
train acc:  0.7734375
train loss:  0.4809626340866089
train gradient:  0.13630901019368213
iteration : 9735
train acc:  0.6875
train loss:  0.5703834295272827
train gradient:  0.1850705876455743
iteration : 9736
train acc:  0.765625
train loss:  0.479483962059021
train gradient:  0.11214711187278091
iteration : 9737
train acc:  0.7578125
train loss:  0.4267403483390808
train gradient:  0.10253898300184786
iteration : 9738
train acc:  0.828125
train loss:  0.42354005575180054
train gradient:  0.10071503237256862
iteration : 9739
train acc:  0.8203125
train loss:  0.4241013526916504
train gradient:  0.08251015044075957
iteration : 9740
train acc:  0.7421875
train loss:  0.4966045916080475
train gradient:  0.16858356465231106
iteration : 9741
train acc:  0.75
train loss:  0.5292559862136841
train gradient:  0.10597770435011204
iteration : 9742
train acc:  0.7265625
train loss:  0.5330798029899597
train gradient:  0.12497369082584461
iteration : 9743
train acc:  0.7578125
train loss:  0.4425065219402313
train gradient:  0.10722791339626152
iteration : 9744
train acc:  0.765625
train loss:  0.4669458866119385
train gradient:  0.0888813968914246
iteration : 9745
train acc:  0.6953125
train loss:  0.5000852346420288
train gradient:  0.09612380642941978
iteration : 9746
train acc:  0.703125
train loss:  0.5096747875213623
train gradient:  0.11987793032469216
iteration : 9747
train acc:  0.7421875
train loss:  0.556187629699707
train gradient:  0.1598603244089637
iteration : 9748
train acc:  0.703125
train loss:  0.5579442977905273
train gradient:  0.15028621583833146
iteration : 9749
train acc:  0.6875
train loss:  0.527320921421051
train gradient:  0.1128312123508974
iteration : 9750
train acc:  0.7890625
train loss:  0.4740172028541565
train gradient:  0.10945124761685737
iteration : 9751
train acc:  0.765625
train loss:  0.5162185430526733
train gradient:  0.1110865834257269
iteration : 9752
train acc:  0.8046875
train loss:  0.4635351598262787
train gradient:  0.09449227462000853
iteration : 9753
train acc:  0.7109375
train loss:  0.5263874530792236
train gradient:  0.13990460396101656
iteration : 9754
train acc:  0.703125
train loss:  0.5208909511566162
train gradient:  0.11159541602083281
iteration : 9755
train acc:  0.734375
train loss:  0.5056822896003723
train gradient:  0.10257186515697804
iteration : 9756
train acc:  0.7421875
train loss:  0.5371078252792358
train gradient:  0.12211689438774789
iteration : 9757
train acc:  0.6796875
train loss:  0.5182056427001953
train gradient:  0.14076361927443987
iteration : 9758
train acc:  0.78125
train loss:  0.4565165936946869
train gradient:  0.1141423438916272
iteration : 9759
train acc:  0.703125
train loss:  0.5934942364692688
train gradient:  0.1546926873832744
iteration : 9760
train acc:  0.7109375
train loss:  0.5149388313293457
train gradient:  0.11523224279084977
iteration : 9761
train acc:  0.671875
train loss:  0.5047920942306519
train gradient:  0.1227247209028751
iteration : 9762
train acc:  0.71875
train loss:  0.5329679846763611
train gradient:  0.1277575851336249
iteration : 9763
train acc:  0.8125
train loss:  0.4194307327270508
train gradient:  0.10212769200688765
iteration : 9764
train acc:  0.7109375
train loss:  0.5309188365936279
train gradient:  0.11755252341563847
iteration : 9765
train acc:  0.703125
train loss:  0.5511659383773804
train gradient:  0.14386285332934895
iteration : 9766
train acc:  0.7578125
train loss:  0.4963986873626709
train gradient:  0.1250116863727408
iteration : 9767
train acc:  0.703125
train loss:  0.5401504039764404
train gradient:  0.1492925174520603
iteration : 9768
train acc:  0.8125
train loss:  0.41077834367752075
train gradient:  0.08551185488746363
iteration : 9769
train acc:  0.71875
train loss:  0.5325239896774292
train gradient:  0.1356932802089936
iteration : 9770
train acc:  0.75
train loss:  0.465578556060791
train gradient:  0.13164773687199663
iteration : 9771
train acc:  0.78125
train loss:  0.4665374159812927
train gradient:  0.10962749277250858
iteration : 9772
train acc:  0.6640625
train loss:  0.5749728679656982
train gradient:  0.16304019565198172
iteration : 9773
train acc:  0.6875
train loss:  0.5625001192092896
train gradient:  0.15858904138906654
iteration : 9774
train acc:  0.765625
train loss:  0.49177587032318115
train gradient:  0.10706312047979866
iteration : 9775
train acc:  0.703125
train loss:  0.5050110816955566
train gradient:  0.130350972108977
iteration : 9776
train acc:  0.71875
train loss:  0.47405126690864563
train gradient:  0.1422097935256016
iteration : 9777
train acc:  0.8125
train loss:  0.44390246272087097
train gradient:  0.1447658369343891
iteration : 9778
train acc:  0.78125
train loss:  0.4726252555847168
train gradient:  0.11908854986373928
iteration : 9779
train acc:  0.765625
train loss:  0.5002257823944092
train gradient:  0.13876926508259946
iteration : 9780
train acc:  0.78125
train loss:  0.45130422711372375
train gradient:  0.11891502539918736
iteration : 9781
train acc:  0.703125
train loss:  0.4966413080692291
train gradient:  0.113462068128477
iteration : 9782
train acc:  0.625
train loss:  0.6209841966629028
train gradient:  0.1655455724541825
iteration : 9783
train acc:  0.703125
train loss:  0.5685266256332397
train gradient:  0.15632927319054085
iteration : 9784
train acc:  0.640625
train loss:  0.61298006772995
train gradient:  0.19665547944784362
iteration : 9785
train acc:  0.71875
train loss:  0.49617236852645874
train gradient:  0.13496188263794126
iteration : 9786
train acc:  0.7578125
train loss:  0.5212533473968506
train gradient:  0.11619986211623919
iteration : 9787
train acc:  0.71875
train loss:  0.5601446628570557
train gradient:  0.1366609563094732
iteration : 9788
train acc:  0.703125
train loss:  0.5609621405601501
train gradient:  0.13648935041314864
iteration : 9789
train acc:  0.7734375
train loss:  0.47067469358444214
train gradient:  0.11267481076955216
iteration : 9790
train acc:  0.7890625
train loss:  0.48130589723587036
train gradient:  0.1280785696735849
iteration : 9791
train acc:  0.703125
train loss:  0.5016794204711914
train gradient:  0.11120799749275767
iteration : 9792
train acc:  0.7265625
train loss:  0.49676215648651123
train gradient:  0.12768666511127513
iteration : 9793
train acc:  0.765625
train loss:  0.4705895483493805
train gradient:  0.0905162573310795
iteration : 9794
train acc:  0.75
train loss:  0.4731011390686035
train gradient:  0.1270805428611942
iteration : 9795
train acc:  0.71875
train loss:  0.4762754738330841
train gradient:  0.129319351530862
iteration : 9796
train acc:  0.78125
train loss:  0.4367982745170593
train gradient:  0.07788553624577207
iteration : 9797
train acc:  0.7734375
train loss:  0.49211448431015015
train gradient:  0.14023078156429783
iteration : 9798
train acc:  0.6953125
train loss:  0.5552693009376526
train gradient:  0.1302264018101979
iteration : 9799
train acc:  0.765625
train loss:  0.4668896496295929
train gradient:  0.1280376193454646
iteration : 9800
train acc:  0.765625
train loss:  0.4908718466758728
train gradient:  0.1600377139680904
iteration : 9801
train acc:  0.7109375
train loss:  0.4835841655731201
train gradient:  0.13115599282971685
iteration : 9802
train acc:  0.7421875
train loss:  0.4894081950187683
train gradient:  0.11871875946924815
iteration : 9803
train acc:  0.734375
train loss:  0.5636629462242126
train gradient:  0.1464342643779084
iteration : 9804
train acc:  0.8046875
train loss:  0.434970498085022
train gradient:  0.10033514536115674
iteration : 9805
train acc:  0.8046875
train loss:  0.41801872849464417
train gradient:  0.08995389578907838
iteration : 9806
train acc:  0.75
train loss:  0.49172264337539673
train gradient:  0.11848631216644391
iteration : 9807
train acc:  0.7421875
train loss:  0.49880945682525635
train gradient:  0.10854545254821261
iteration : 9808
train acc:  0.71875
train loss:  0.5004449486732483
train gradient:  0.11344507649005234
iteration : 9809
train acc:  0.75
train loss:  0.518168568611145
train gradient:  0.1223371430568557
iteration : 9810
train acc:  0.7421875
train loss:  0.5066975951194763
train gradient:  0.15061336295899747
iteration : 9811
train acc:  0.7265625
train loss:  0.47922033071517944
train gradient:  0.11173216357777666
iteration : 9812
train acc:  0.7578125
train loss:  0.47635507583618164
train gradient:  0.1440008805038041
iteration : 9813
train acc:  0.7265625
train loss:  0.5660401582717896
train gradient:  0.14143383595803855
iteration : 9814
train acc:  0.7578125
train loss:  0.45484116673469543
train gradient:  0.10583022601083832
iteration : 9815
train acc:  0.71875
train loss:  0.48901188373565674
train gradient:  0.12504623494590045
iteration : 9816
train acc:  0.75
train loss:  0.47264328598976135
train gradient:  0.12587414349372217
iteration : 9817
train acc:  0.7265625
train loss:  0.5503581762313843
train gradient:  0.18563617816162659
iteration : 9818
train acc:  0.71875
train loss:  0.511944055557251
train gradient:  0.12724637671375527
iteration : 9819
train acc:  0.7421875
train loss:  0.5014036893844604
train gradient:  0.10382730755581536
iteration : 9820
train acc:  0.765625
train loss:  0.5001494288444519
train gradient:  0.1229243045442389
iteration : 9821
train acc:  0.7265625
train loss:  0.4847196340560913
train gradient:  0.1426471610283535
iteration : 9822
train acc:  0.75
train loss:  0.5419260263442993
train gradient:  0.12336248300087034
iteration : 9823
train acc:  0.78125
train loss:  0.4397623836994171
train gradient:  0.0812297679529941
iteration : 9824
train acc:  0.6875
train loss:  0.4893013834953308
train gradient:  0.1137560295252601
iteration : 9825
train acc:  0.7578125
train loss:  0.46302878856658936
train gradient:  0.11360647413950459
iteration : 9826
train acc:  0.703125
train loss:  0.5600951910018921
train gradient:  0.17350879263340435
iteration : 9827
train acc:  0.6875
train loss:  0.5415478944778442
train gradient:  0.1297448577338166
iteration : 9828
train acc:  0.78125
train loss:  0.4526262879371643
train gradient:  0.09477059232138205
iteration : 9829
train acc:  0.8125
train loss:  0.44550976157188416
train gradient:  0.10437352841556623
iteration : 9830
train acc:  0.703125
train loss:  0.46194010972976685
train gradient:  0.11388054738137518
iteration : 9831
train acc:  0.78125
train loss:  0.4446011781692505
train gradient:  0.08170711903431498
iteration : 9832
train acc:  0.7109375
train loss:  0.5796777009963989
train gradient:  0.12363160160082189
iteration : 9833
train acc:  0.765625
train loss:  0.45548686385154724
train gradient:  0.12154424661783117
iteration : 9834
train acc:  0.7734375
train loss:  0.4598408639431
train gradient:  0.11772792870816046
iteration : 9835
train acc:  0.796875
train loss:  0.4688507318496704
train gradient:  0.0940087377975057
iteration : 9836
train acc:  0.6875
train loss:  0.5042984485626221
train gradient:  0.08939728478383616
iteration : 9837
train acc:  0.75
train loss:  0.5307998657226562
train gradient:  0.1369154430239437
iteration : 9838
train acc:  0.7109375
train loss:  0.5138261318206787
train gradient:  0.11471249532619009
iteration : 9839
train acc:  0.7265625
train loss:  0.5107163786888123
train gradient:  0.1238411324433566
iteration : 9840
train acc:  0.6875
train loss:  0.5255417823791504
train gradient:  0.1335856518867645
iteration : 9841
train acc:  0.75
train loss:  0.5063427686691284
train gradient:  0.12616519050244654
iteration : 9842
train acc:  0.78125
train loss:  0.4834086298942566
train gradient:  0.1354544278347704
iteration : 9843
train acc:  0.7578125
train loss:  0.4740053713321686
train gradient:  0.1097178945668745
iteration : 9844
train acc:  0.7734375
train loss:  0.4768022894859314
train gradient:  0.11659902265758006
iteration : 9845
train acc:  0.7578125
train loss:  0.4967637062072754
train gradient:  0.10474684613466503
iteration : 9846
train acc:  0.7421875
train loss:  0.49287813901901245
train gradient:  0.11587994641063419
iteration : 9847
train acc:  0.6796875
train loss:  0.560468316078186
train gradient:  0.17627513692988184
iteration : 9848
train acc:  0.734375
train loss:  0.46883296966552734
train gradient:  0.11335260523095979
iteration : 9849
train acc:  0.75
train loss:  0.4805276095867157
train gradient:  0.09534842351255331
iteration : 9850
train acc:  0.78125
train loss:  0.46707507967948914
train gradient:  0.10405199671112382
iteration : 9851
train acc:  0.6953125
train loss:  0.5716637969017029
train gradient:  0.1383306559087359
iteration : 9852
train acc:  0.734375
train loss:  0.442023903131485
train gradient:  0.09296612407088567
iteration : 9853
train acc:  0.7421875
train loss:  0.4747287929058075
train gradient:  0.13066945770950902
iteration : 9854
train acc:  0.734375
train loss:  0.5112189054489136
train gradient:  0.11222299117611576
iteration : 9855
train acc:  0.7109375
train loss:  0.5138912200927734
train gradient:  0.12244394983469803
iteration : 9856
train acc:  0.7578125
train loss:  0.5103492736816406
train gradient:  0.11690651074997607
iteration : 9857
train acc:  0.8046875
train loss:  0.40673190355300903
train gradient:  0.08494630131544678
iteration : 9858
train acc:  0.7421875
train loss:  0.4611973166465759
train gradient:  0.15863449579154637
iteration : 9859
train acc:  0.7578125
train loss:  0.5044066309928894
train gradient:  0.15975138990090457
iteration : 9860
train acc:  0.6953125
train loss:  0.4991256296634674
train gradient:  0.11139314638297038
iteration : 9861
train acc:  0.6796875
train loss:  0.5446561574935913
train gradient:  0.14222788316679802
iteration : 9862
train acc:  0.703125
train loss:  0.5265425443649292
train gradient:  0.14859876511491552
iteration : 9863
train acc:  0.71875
train loss:  0.5738315582275391
train gradient:  0.16670291283027824
iteration : 9864
train acc:  0.8203125
train loss:  0.45641207695007324
train gradient:  0.11188748427051573
iteration : 9865
train acc:  0.8125
train loss:  0.43206357955932617
train gradient:  0.11596636642803983
iteration : 9866
train acc:  0.7109375
train loss:  0.5014818906784058
train gradient:  0.12099170876038909
iteration : 9867
train acc:  0.7421875
train loss:  0.45921775698661804
train gradient:  0.11874944464033943
iteration : 9868
train acc:  0.765625
train loss:  0.574705958366394
train gradient:  0.16711439488038488
iteration : 9869
train acc:  0.6796875
train loss:  0.5387426614761353
train gradient:  0.14740914580911163
iteration : 9870
train acc:  0.7578125
train loss:  0.4989568889141083
train gradient:  0.11344481046567437
iteration : 9871
train acc:  0.7265625
train loss:  0.5341681838035583
train gradient:  0.10632523506896097
iteration : 9872
train acc:  0.734375
train loss:  0.5473020076751709
train gradient:  0.13339137471498877
iteration : 9873
train acc:  0.7734375
train loss:  0.4690620005130768
train gradient:  0.10081350414886057
iteration : 9874
train acc:  0.78125
train loss:  0.48043200373649597
train gradient:  0.1213641038211819
iteration : 9875
train acc:  0.7265625
train loss:  0.556865394115448
train gradient:  0.13082601033384847
iteration : 9876
train acc:  0.7265625
train loss:  0.49812567234039307
train gradient:  0.12108732617781917
iteration : 9877
train acc:  0.75
train loss:  0.48403656482696533
train gradient:  0.11319189219163434
iteration : 9878
train acc:  0.7890625
train loss:  0.4638766348361969
train gradient:  0.10638387686007929
iteration : 9879
train acc:  0.703125
train loss:  0.5038058161735535
train gradient:  0.10415718332155507
iteration : 9880
train acc:  0.734375
train loss:  0.4954153299331665
train gradient:  0.14971889793030618
iteration : 9881
train acc:  0.6484375
train loss:  0.6175057888031006
train gradient:  0.13529502073488273
iteration : 9882
train acc:  0.765625
train loss:  0.46198195219039917
train gradient:  0.10094758299323539
iteration : 9883
train acc:  0.6796875
train loss:  0.5588873624801636
train gradient:  0.11815431068819557
iteration : 9884
train acc:  0.75
train loss:  0.48905423283576965
train gradient:  0.09468352207173693
iteration : 9885
train acc:  0.7734375
train loss:  0.5236217975616455
train gradient:  0.13382922790191396
iteration : 9886
train acc:  0.7265625
train loss:  0.4567309021949768
train gradient:  0.11457422320585653
iteration : 9887
train acc:  0.8359375
train loss:  0.4199492931365967
train gradient:  0.07801437374308562
iteration : 9888
train acc:  0.78125
train loss:  0.49904894828796387
train gradient:  0.11826755348735915
iteration : 9889
train acc:  0.8046875
train loss:  0.44524112343788147
train gradient:  0.11618542033198077
iteration : 9890
train acc:  0.75
train loss:  0.49848484992980957
train gradient:  0.10377094733116667
iteration : 9891
train acc:  0.7109375
train loss:  0.5037407279014587
train gradient:  0.11502846530883747
iteration : 9892
train acc:  0.7890625
train loss:  0.4764232039451599
train gradient:  0.1261185738547116
iteration : 9893
train acc:  0.7109375
train loss:  0.523769736289978
train gradient:  0.11826393194401727
iteration : 9894
train acc:  0.71875
train loss:  0.49302080273628235
train gradient:  0.0938722319115391
iteration : 9895
train acc:  0.765625
train loss:  0.48128044605255127
train gradient:  0.10956358403489716
iteration : 9896
train acc:  0.8203125
train loss:  0.38364025950431824
train gradient:  0.0918527380484706
iteration : 9897
train acc:  0.6640625
train loss:  0.5589931011199951
train gradient:  0.17887638606511336
iteration : 9898
train acc:  0.71875
train loss:  0.5294249057769775
train gradient:  0.14114011760499356
iteration : 9899
train acc:  0.7890625
train loss:  0.5414683818817139
train gradient:  0.11606152423982337
iteration : 9900
train acc:  0.765625
train loss:  0.5017591714859009
train gradient:  0.11831164683997131
iteration : 9901
train acc:  0.78125
train loss:  0.46850624680519104
train gradient:  0.09438924919432938
iteration : 9902
train acc:  0.71875
train loss:  0.514478862285614
train gradient:  0.11225410364373566
iteration : 9903
train acc:  0.6953125
train loss:  0.5556169748306274
train gradient:  0.14660309776460753
iteration : 9904
train acc:  0.7734375
train loss:  0.4335293769836426
train gradient:  0.11372062672312552
iteration : 9905
train acc:  0.6953125
train loss:  0.5431147813796997
train gradient:  0.1464791338835778
iteration : 9906
train acc:  0.71875
train loss:  0.48711246252059937
train gradient:  0.10093857200556632
iteration : 9907
train acc:  0.7734375
train loss:  0.46606722474098206
train gradient:  0.10916695438448762
iteration : 9908
train acc:  0.8046875
train loss:  0.44415587186813354
train gradient:  0.09918097480169986
iteration : 9909
train acc:  0.7265625
train loss:  0.5305790305137634
train gradient:  0.1492699267663114
iteration : 9910
train acc:  0.75
train loss:  0.4767310619354248
train gradient:  0.11824051945812718
iteration : 9911
train acc:  0.7578125
train loss:  0.4836563169956207
train gradient:  0.13318606265245053
iteration : 9912
train acc:  0.7421875
train loss:  0.46810150146484375
train gradient:  0.12474897775428072
iteration : 9913
train acc:  0.765625
train loss:  0.46118491888046265
train gradient:  0.1054556686794596
iteration : 9914
train acc:  0.6953125
train loss:  0.5407754182815552
train gradient:  0.13133420004359753
iteration : 9915
train acc:  0.7578125
train loss:  0.4508531391620636
train gradient:  0.10103432265242086
iteration : 9916
train acc:  0.7421875
train loss:  0.5196226835250854
train gradient:  0.15786150905176172
iteration : 9917
train acc:  0.7734375
train loss:  0.48056209087371826
train gradient:  0.10969807120614891
iteration : 9918
train acc:  0.78125
train loss:  0.44807982444763184
train gradient:  0.10889118175321058
iteration : 9919
train acc:  0.7734375
train loss:  0.43640345335006714
train gradient:  0.12275482778049951
iteration : 9920
train acc:  0.7265625
train loss:  0.5209708213806152
train gradient:  0.11854106443325593
iteration : 9921
train acc:  0.6953125
train loss:  0.5398088693618774
train gradient:  0.15216613240359186
iteration : 9922
train acc:  0.7421875
train loss:  0.48468470573425293
train gradient:  0.10804126941570322
iteration : 9923
train acc:  0.7890625
train loss:  0.4966728091239929
train gradient:  0.1263248022948047
iteration : 9924
train acc:  0.8125
train loss:  0.4350261390209198
train gradient:  0.10140117220462676
iteration : 9925
train acc:  0.7421875
train loss:  0.5012834072113037
train gradient:  0.1247827249226905
iteration : 9926
train acc:  0.765625
train loss:  0.4641759991645813
train gradient:  0.11606281350850534
iteration : 9927
train acc:  0.7109375
train loss:  0.513586163520813
train gradient:  0.14850430113949303
iteration : 9928
train acc:  0.734375
train loss:  0.4939973056316376
train gradient:  0.11354653021391907
iteration : 9929
train acc:  0.765625
train loss:  0.48608699440956116
train gradient:  0.13629521731310457
iteration : 9930
train acc:  0.75
train loss:  0.4516741931438446
train gradient:  0.10487418260838849
iteration : 9931
train acc:  0.7890625
train loss:  0.44088214635849
train gradient:  0.11008783500326083
iteration : 9932
train acc:  0.6640625
train loss:  0.6215870380401611
train gradient:  0.17950523771801025
iteration : 9933
train acc:  0.7578125
train loss:  0.4510006904602051
train gradient:  0.1113560431399815
iteration : 9934
train acc:  0.7578125
train loss:  0.5044763088226318
train gradient:  0.11679865293539374
iteration : 9935
train acc:  0.7578125
train loss:  0.48300686478614807
train gradient:  0.12543455903672412
iteration : 9936
train acc:  0.7109375
train loss:  0.5247752666473389
train gradient:  0.11812067419169418
iteration : 9937
train acc:  0.6953125
train loss:  0.5499523282051086
train gradient:  0.14238889738123334
iteration : 9938
train acc:  0.71875
train loss:  0.4907165765762329
train gradient:  0.1206318175610673
iteration : 9939
train acc:  0.7421875
train loss:  0.4580976366996765
train gradient:  0.14681128517655367
iteration : 9940
train acc:  0.7578125
train loss:  0.5045908689498901
train gradient:  0.13545398579123494
iteration : 9941
train acc:  0.6875
train loss:  0.5616458654403687
train gradient:  0.15536378093406183
iteration : 9942
train acc:  0.7734375
train loss:  0.48084238171577454
train gradient:  0.12702960598541296
iteration : 9943
train acc:  0.75
train loss:  0.5276309251785278
train gradient:  0.11769081927869199
iteration : 9944
train acc:  0.7578125
train loss:  0.45088496804237366
train gradient:  0.131845557384573
iteration : 9945
train acc:  0.8046875
train loss:  0.45988360047340393
train gradient:  0.12250261427057768
iteration : 9946
train acc:  0.7265625
train loss:  0.49168795347213745
train gradient:  0.11458982493060922
iteration : 9947
train acc:  0.7265625
train loss:  0.47855934500694275
train gradient:  0.12239655263079015
iteration : 9948
train acc:  0.7421875
train loss:  0.47975122928619385
train gradient:  0.10112972863415537
iteration : 9949
train acc:  0.7578125
train loss:  0.5439788699150085
train gradient:  0.12662549992374153
iteration : 9950
train acc:  0.796875
train loss:  0.46790629625320435
train gradient:  0.0928616852343085
iteration : 9951
train acc:  0.7578125
train loss:  0.4706379771232605
train gradient:  0.1317316124464541
iteration : 9952
train acc:  0.8203125
train loss:  0.41258928179740906
train gradient:  0.08612496968995308
iteration : 9953
train acc:  0.765625
train loss:  0.45551323890686035
train gradient:  0.1038092464707661
iteration : 9954
train acc:  0.7734375
train loss:  0.46014532446861267
train gradient:  0.10269231660373494
iteration : 9955
train acc:  0.7578125
train loss:  0.4631197154521942
train gradient:  0.10978742657324486
iteration : 9956
train acc:  0.796875
train loss:  0.45532917976379395
train gradient:  0.11052795686449265
iteration : 9957
train acc:  0.765625
train loss:  0.47272801399230957
train gradient:  0.1481107319746085
iteration : 9958
train acc:  0.6953125
train loss:  0.5759332776069641
train gradient:  0.15123374102386195
iteration : 9959
train acc:  0.7734375
train loss:  0.4663851857185364
train gradient:  0.09874414198642326
iteration : 9960
train acc:  0.7421875
train loss:  0.48019933700561523
train gradient:  0.10628494986734238
iteration : 9961
train acc:  0.6875
train loss:  0.4854261875152588
train gradient:  0.12415851433519225
iteration : 9962
train acc:  0.765625
train loss:  0.4966563284397125
train gradient:  0.13592818279348032
iteration : 9963
train acc:  0.78125
train loss:  0.4522971510887146
train gradient:  0.09450125166154916
iteration : 9964
train acc:  0.7734375
train loss:  0.44692087173461914
train gradient:  0.09301747935463058
iteration : 9965
train acc:  0.703125
train loss:  0.5615289211273193
train gradient:  0.14744598551216187
iteration : 9966
train acc:  0.734375
train loss:  0.4781995713710785
train gradient:  0.1500042942509165
iteration : 9967
train acc:  0.7578125
train loss:  0.46126383543014526
train gradient:  0.10724361784246446
iteration : 9968
train acc:  0.765625
train loss:  0.4546405076980591
train gradient:  0.08768555621993177
iteration : 9969
train acc:  0.765625
train loss:  0.4893522560596466
train gradient:  0.13765761014732605
iteration : 9970
train acc:  0.7578125
train loss:  0.5065869092941284
train gradient:  0.13591563221578545
iteration : 9971
train acc:  0.7265625
train loss:  0.6054979562759399
train gradient:  0.16684011796957565
iteration : 9972
train acc:  0.703125
train loss:  0.5503871440887451
train gradient:  0.1726246899755855
iteration : 9973
train acc:  0.703125
train loss:  0.5262670516967773
train gradient:  0.1434054028493928
iteration : 9974
train acc:  0.8359375
train loss:  0.45750892162323
train gradient:  0.101294221221976
iteration : 9975
train acc:  0.765625
train loss:  0.4389423131942749
train gradient:  0.13546636121882694
iteration : 9976
train acc:  0.7421875
train loss:  0.5013294219970703
train gradient:  0.12022712261552668
iteration : 9977
train acc:  0.8125
train loss:  0.4061282277107239
train gradient:  0.09326993089830173
iteration : 9978
train acc:  0.734375
train loss:  0.5265146493911743
train gradient:  0.1295635241634582
iteration : 9979
train acc:  0.6953125
train loss:  0.5437638163566589
train gradient:  0.15105690093146795
iteration : 9980
train acc:  0.765625
train loss:  0.47724324464797974
train gradient:  0.09864783179152539
iteration : 9981
train acc:  0.7578125
train loss:  0.46824681758880615
train gradient:  0.11463333546670826
iteration : 9982
train acc:  0.7109375
train loss:  0.4827624559402466
train gradient:  0.11436847418472672
iteration : 9983
train acc:  0.71875
train loss:  0.5328868627548218
train gradient:  0.18675724079588935
iteration : 9984
train acc:  0.8125
train loss:  0.42795684933662415
train gradient:  0.11992295920510392
iteration : 9985
train acc:  0.7265625
train loss:  0.5379769802093506
train gradient:  0.18706471436303226
iteration : 9986
train acc:  0.7890625
train loss:  0.482952356338501
train gradient:  0.15879323697314532
iteration : 9987
train acc:  0.7734375
train loss:  0.4754875898361206
train gradient:  0.14225346345768353
iteration : 9988
train acc:  0.671875
train loss:  0.5905004739761353
train gradient:  0.2119405063288605
iteration : 9989
train acc:  0.7265625
train loss:  0.4821777939796448
train gradient:  0.13533275149674967
iteration : 9990
train acc:  0.75
train loss:  0.4801165461540222
train gradient:  0.11261089171314549
iteration : 9991
train acc:  0.6875
train loss:  0.5176728367805481
train gradient:  0.1391069382624428
iteration : 9992
train acc:  0.7421875
train loss:  0.46455609798431396
train gradient:  0.11341862866816793
iteration : 9993
train acc:  0.7265625
train loss:  0.49169889092445374
train gradient:  0.12771820729299896
iteration : 9994
train acc:  0.7734375
train loss:  0.4828054904937744
train gradient:  0.11525650987651025
iteration : 9995
train acc:  0.6953125
train loss:  0.5460038185119629
train gradient:  0.11738266679670153
iteration : 9996
train acc:  0.75
train loss:  0.48996207118034363
train gradient:  0.12334590580068061
iteration : 9997
train acc:  0.7421875
train loss:  0.5213375091552734
train gradient:  0.15680703376039867
iteration : 9998
train acc:  0.7109375
train loss:  0.4680757522583008
train gradient:  0.14114950236716461
iteration : 9999
train acc:  0.75
train loss:  0.4624921381473541
train gradient:  0.12868627929800058
iteration : 10000
train acc:  0.7578125
train loss:  0.5064505338668823
train gradient:  0.11659597568713176
iteration : 10001
train acc:  0.75
train loss:  0.4993612766265869
train gradient:  0.13919890898130843
iteration : 10002
train acc:  0.7578125
train loss:  0.4504236578941345
train gradient:  0.11020922051150565
iteration : 10003
train acc:  0.8046875
train loss:  0.5143038034439087
train gradient:  0.14562753071794138
iteration : 10004
train acc:  0.703125
train loss:  0.49647703766822815
train gradient:  0.14200043317386163
iteration : 10005
train acc:  0.6875
train loss:  0.4870811104774475
train gradient:  0.11527663830042967
iteration : 10006
train acc:  0.7578125
train loss:  0.49525296688079834
train gradient:  0.13279327480451014
iteration : 10007
train acc:  0.7890625
train loss:  0.4480844736099243
train gradient:  0.10643831100336301
iteration : 10008
train acc:  0.703125
train loss:  0.5604167580604553
train gradient:  0.1532267922880836
iteration : 10009
train acc:  0.7890625
train loss:  0.4726870656013489
train gradient:  0.13692181727719616
iteration : 10010
train acc:  0.8046875
train loss:  0.4279484748840332
train gradient:  0.10001076718901605
iteration : 10011
train acc:  0.71875
train loss:  0.5003151893615723
train gradient:  0.11175881529445027
iteration : 10012
train acc:  0.7890625
train loss:  0.4434792995452881
train gradient:  0.12136231357567892
iteration : 10013
train acc:  0.640625
train loss:  0.6090450286865234
train gradient:  0.16676672672503745
iteration : 10014
train acc:  0.78125
train loss:  0.522426187992096
train gradient:  0.15318807664422213
iteration : 10015
train acc:  0.7578125
train loss:  0.46241068840026855
train gradient:  0.1298602149568386
iteration : 10016
train acc:  0.78125
train loss:  0.4479450583457947
train gradient:  0.10862098637341408
iteration : 10017
train acc:  0.7109375
train loss:  0.542953610420227
train gradient:  0.14427288328062454
iteration : 10018
train acc:  0.6875
train loss:  0.5548213720321655
train gradient:  0.1874691002843467
iteration : 10019
train acc:  0.703125
train loss:  0.6073039770126343
train gradient:  0.19400708065061045
iteration : 10020
train acc:  0.796875
train loss:  0.4423726797103882
train gradient:  0.11339033027739609
iteration : 10021
train acc:  0.7578125
train loss:  0.4472087621688843
train gradient:  0.11757501394982724
iteration : 10022
train acc:  0.7578125
train loss:  0.49806493520736694
train gradient:  0.19067145454951315
iteration : 10023
train acc:  0.75
train loss:  0.5353828072547913
train gradient:  0.12842228993794205
iteration : 10024
train acc:  0.8359375
train loss:  0.4229305684566498
train gradient:  0.08817488217659339
iteration : 10025
train acc:  0.8203125
train loss:  0.42633548378944397
train gradient:  0.08860385137274597
iteration : 10026
train acc:  0.7109375
train loss:  0.5816873908042908
train gradient:  0.15838259166647214
iteration : 10027
train acc:  0.7265625
train loss:  0.5660859942436218
train gradient:  0.18909019925424025
iteration : 10028
train acc:  0.6875
train loss:  0.5975363850593567
train gradient:  0.1602668493076208
iteration : 10029
train acc:  0.7109375
train loss:  0.4999582767486572
train gradient:  0.11066773791347598
iteration : 10030
train acc:  0.7890625
train loss:  0.4894922971725464
train gradient:  0.12715438845606064
iteration : 10031
train acc:  0.7734375
train loss:  0.4503212571144104
train gradient:  0.12320127360108211
iteration : 10032
train acc:  0.7421875
train loss:  0.504676103591919
train gradient:  0.10048313157443237
iteration : 10033
train acc:  0.7265625
train loss:  0.543832540512085
train gradient:  0.13620470536412527
iteration : 10034
train acc:  0.796875
train loss:  0.47375044226646423
train gradient:  0.08766945221748972
iteration : 10035
train acc:  0.796875
train loss:  0.42859622836112976
train gradient:  0.12073821079701952
iteration : 10036
train acc:  0.75
train loss:  0.49199527502059937
train gradient:  0.11998750258989252
iteration : 10037
train acc:  0.7734375
train loss:  0.451679527759552
train gradient:  0.1050965222303783
iteration : 10038
train acc:  0.7734375
train loss:  0.4536874294281006
train gradient:  0.10398511631323418
iteration : 10039
train acc:  0.7109375
train loss:  0.49839186668395996
train gradient:  0.11558044407702484
iteration : 10040
train acc:  0.7265625
train loss:  0.49803096055984497
train gradient:  0.11103009249933642
iteration : 10041
train acc:  0.8125
train loss:  0.4215349555015564
train gradient:  0.10576246717096625
iteration : 10042
train acc:  0.8203125
train loss:  0.4376755654811859
train gradient:  0.11211390619096027
iteration : 10043
train acc:  0.71875
train loss:  0.517516016960144
train gradient:  0.11333519808919969
iteration : 10044
train acc:  0.8125
train loss:  0.4254344701766968
train gradient:  0.10416982111123307
iteration : 10045
train acc:  0.7265625
train loss:  0.5124542117118835
train gradient:  0.12207257015480527
iteration : 10046
train acc:  0.8203125
train loss:  0.4079330563545227
train gradient:  0.09881224144148915
iteration : 10047
train acc:  0.7734375
train loss:  0.4633116126060486
train gradient:  0.1043688125466748
iteration : 10048
train acc:  0.6953125
train loss:  0.517848014831543
train gradient:  0.1227736556849351
iteration : 10049
train acc:  0.7421875
train loss:  0.447287380695343
train gradient:  0.10113414266118269
iteration : 10050
train acc:  0.75
train loss:  0.4475114941596985
train gradient:  0.09265488691605848
iteration : 10051
train acc:  0.78125
train loss:  0.4888788163661957
train gradient:  0.1113058208147852
iteration : 10052
train acc:  0.7109375
train loss:  0.5435200929641724
train gradient:  0.15268629655585797
iteration : 10053
train acc:  0.78125
train loss:  0.4416315257549286
train gradient:  0.12937217188451133
iteration : 10054
train acc:  0.7265625
train loss:  0.5016897916793823
train gradient:  0.1015936182212476
iteration : 10055
train acc:  0.71875
train loss:  0.5078816413879395
train gradient:  0.13505087498771773
iteration : 10056
train acc:  0.7421875
train loss:  0.479510098695755
train gradient:  0.11347508405803576
iteration : 10057
train acc:  0.8125
train loss:  0.4083746075630188
train gradient:  0.07603587230223254
iteration : 10058
train acc:  0.71875
train loss:  0.5138623714447021
train gradient:  0.12291949048296431
iteration : 10059
train acc:  0.7578125
train loss:  0.4760948717594147
train gradient:  0.10332491409207671
iteration : 10060
train acc:  0.7734375
train loss:  0.44216442108154297
train gradient:  0.11657050410211567
iteration : 10061
train acc:  0.7109375
train loss:  0.5279145240783691
train gradient:  0.15977193643241286
iteration : 10062
train acc:  0.640625
train loss:  0.6004409790039062
train gradient:  0.17011928501323037
iteration : 10063
train acc:  0.75
train loss:  0.4575931429862976
train gradient:  0.1037626199917099
iteration : 10064
train acc:  0.6875
train loss:  0.5584030747413635
train gradient:  0.13768384012830065
iteration : 10065
train acc:  0.6796875
train loss:  0.5138124227523804
train gradient:  0.12297487632332599
iteration : 10066
train acc:  0.7265625
train loss:  0.5652733445167542
train gradient:  0.17269330061586696
iteration : 10067
train acc:  0.7265625
train loss:  0.5399687886238098
train gradient:  0.15310062413068687
iteration : 10068
train acc:  0.796875
train loss:  0.42765557765960693
train gradient:  0.09285911210380839
iteration : 10069
train acc:  0.7578125
train loss:  0.4600442051887512
train gradient:  0.08994321523030525
iteration : 10070
train acc:  0.71875
train loss:  0.5385878682136536
train gradient:  0.16105231197723124
iteration : 10071
train acc:  0.796875
train loss:  0.4285385012626648
train gradient:  0.12390735977273565
iteration : 10072
train acc:  0.7109375
train loss:  0.503943145275116
train gradient:  0.13509201097029175
iteration : 10073
train acc:  0.78125
train loss:  0.48813921213150024
train gradient:  0.1478590863967758
iteration : 10074
train acc:  0.78125
train loss:  0.4481903314590454
train gradient:  0.09643691867515888
iteration : 10075
train acc:  0.734375
train loss:  0.4784644842147827
train gradient:  0.11365715534592777
iteration : 10076
train acc:  0.7109375
train loss:  0.5382484197616577
train gradient:  0.1381552250498897
iteration : 10077
train acc:  0.703125
train loss:  0.5496846437454224
train gradient:  0.1669178438052224
iteration : 10078
train acc:  0.8125
train loss:  0.39778897166252136
train gradient:  0.08937492037528066
iteration : 10079
train acc:  0.7578125
train loss:  0.5231541395187378
train gradient:  0.13495605684190898
iteration : 10080
train acc:  0.7578125
train loss:  0.4750850796699524
train gradient:  0.10978251246617361
iteration : 10081
train acc:  0.7109375
train loss:  0.5150661468505859
train gradient:  0.13706792403099508
iteration : 10082
train acc:  0.7109375
train loss:  0.5299275517463684
train gradient:  0.1554715201848233
iteration : 10083
train acc:  0.7734375
train loss:  0.4994082450866699
train gradient:  0.1009086743586888
iteration : 10084
train acc:  0.703125
train loss:  0.5952133536338806
train gradient:  0.21327847925088955
iteration : 10085
train acc:  0.7265625
train loss:  0.4937002658843994
train gradient:  0.1256222697385314
iteration : 10086
train acc:  0.7578125
train loss:  0.4878298342227936
train gradient:  0.11017701993195668
iteration : 10087
train acc:  0.7265625
train loss:  0.4715302586555481
train gradient:  0.10676616079347928
iteration : 10088
train acc:  0.7578125
train loss:  0.4606402516365051
train gradient:  0.13251923209541588
iteration : 10089
train acc:  0.7421875
train loss:  0.5022063255310059
train gradient:  0.11198346228714541
iteration : 10090
train acc:  0.75
train loss:  0.487053245306015
train gradient:  0.1499350345918569
iteration : 10091
train acc:  0.8046875
train loss:  0.5023212432861328
train gradient:  0.1199320331394149
iteration : 10092
train acc:  0.7265625
train loss:  0.48222845792770386
train gradient:  0.13096811656735013
iteration : 10093
train acc:  0.78125
train loss:  0.48226702213287354
train gradient:  0.1274415203615754
iteration : 10094
train acc:  0.7109375
train loss:  0.5191131234169006
train gradient:  0.1495012881931616
iteration : 10095
train acc:  0.8125
train loss:  0.4176258444786072
train gradient:  0.07648111370350308
iteration : 10096
train acc:  0.7265625
train loss:  0.5460891723632812
train gradient:  0.15616893049651565
iteration : 10097
train acc:  0.71875
train loss:  0.5050947666168213
train gradient:  0.14561643627607374
iteration : 10098
train acc:  0.7109375
train loss:  0.5009018182754517
train gradient:  0.1572355178999943
iteration : 10099
train acc:  0.78125
train loss:  0.45695871114730835
train gradient:  0.10211710869403204
iteration : 10100
train acc:  0.7109375
train loss:  0.5606532096862793
train gradient:  0.1531688235185717
iteration : 10101
train acc:  0.6953125
train loss:  0.5581579804420471
train gradient:  0.1179728413691564
iteration : 10102
train acc:  0.75
train loss:  0.4922357201576233
train gradient:  0.11096284335749214
iteration : 10103
train acc:  0.6875
train loss:  0.5741370916366577
train gradient:  0.14497036180217823
iteration : 10104
train acc:  0.6953125
train loss:  0.565096914768219
train gradient:  0.16779967102452636
iteration : 10105
train acc:  0.71875
train loss:  0.5284138917922974
train gradient:  0.18125499853058236
iteration : 10106
train acc:  0.765625
train loss:  0.5325500965118408
train gradient:  0.15440874446956915
iteration : 10107
train acc:  0.7265625
train loss:  0.513266921043396
train gradient:  0.1291943115850216
iteration : 10108
train acc:  0.7265625
train loss:  0.539292573928833
train gradient:  0.1620549817071133
iteration : 10109
train acc:  0.71875
train loss:  0.5032602548599243
train gradient:  0.1618486386670877
iteration : 10110
train acc:  0.7578125
train loss:  0.45143550634384155
train gradient:  0.1088689335593403
iteration : 10111
train acc:  0.7265625
train loss:  0.46898213028907776
train gradient:  0.1410701206342151
iteration : 10112
train acc:  0.8046875
train loss:  0.458109587430954
train gradient:  0.10299336242868869
iteration : 10113
train acc:  0.6875
train loss:  0.5559628009796143
train gradient:  0.14151977904762358
iteration : 10114
train acc:  0.6875
train loss:  0.519188642501831
train gradient:  0.12995666612365303
iteration : 10115
train acc:  0.765625
train loss:  0.48173901438713074
train gradient:  0.13002181914793898
iteration : 10116
train acc:  0.7265625
train loss:  0.5031599402427673
train gradient:  0.14971135956971254
iteration : 10117
train acc:  0.78125
train loss:  0.4869268834590912
train gradient:  0.11926731669049606
iteration : 10118
train acc:  0.7265625
train loss:  0.5011482238769531
train gradient:  0.10976327203428259
iteration : 10119
train acc:  0.8125
train loss:  0.48704493045806885
train gradient:  0.13734358685967607
iteration : 10120
train acc:  0.7421875
train loss:  0.547032356262207
train gradient:  0.15146167682469033
iteration : 10121
train acc:  0.703125
train loss:  0.5440951585769653
train gradient:  0.12393986402872931
iteration : 10122
train acc:  0.7265625
train loss:  0.4746929705142975
train gradient:  0.1193513489373448
iteration : 10123
train acc:  0.765625
train loss:  0.4459584951400757
train gradient:  0.09291768920959383
iteration : 10124
train acc:  0.7734375
train loss:  0.4632910490036011
train gradient:  0.11615964934985294
iteration : 10125
train acc:  0.765625
train loss:  0.46203774213790894
train gradient:  0.12452394255532882
iteration : 10126
train acc:  0.765625
train loss:  0.4633074104785919
train gradient:  0.12178456312154358
iteration : 10127
train acc:  0.703125
train loss:  0.5026991367340088
train gradient:  0.13054452572163264
iteration : 10128
train acc:  0.7578125
train loss:  0.4978901445865631
train gradient:  0.10599070422418164
iteration : 10129
train acc:  0.71875
train loss:  0.5479903221130371
train gradient:  0.11334598111765612
iteration : 10130
train acc:  0.78125
train loss:  0.46130117774009705
train gradient:  0.11322157134824616
iteration : 10131
train acc:  0.765625
train loss:  0.4879820644855499
train gradient:  0.13673613972340554
iteration : 10132
train acc:  0.7890625
train loss:  0.4381338953971863
train gradient:  0.09578836301839863
iteration : 10133
train acc:  0.671875
train loss:  0.5143064856529236
train gradient:  0.111950719495827
iteration : 10134
train acc:  0.7265625
train loss:  0.4859658181667328
train gradient:  0.20355499854555803
iteration : 10135
train acc:  0.796875
train loss:  0.4421403110027313
train gradient:  0.12010725341927313
iteration : 10136
train acc:  0.703125
train loss:  0.5264686346054077
train gradient:  0.13935287430096804
iteration : 10137
train acc:  0.7890625
train loss:  0.4371492266654968
train gradient:  0.08899241685211125
iteration : 10138
train acc:  0.6796875
train loss:  0.5473750829696655
train gradient:  0.14977518037221832
iteration : 10139
train acc:  0.8046875
train loss:  0.4456387162208557
train gradient:  0.09632376619237297
iteration : 10140
train acc:  0.734375
train loss:  0.5131425857543945
train gradient:  0.10564453384408848
iteration : 10141
train acc:  0.71875
train loss:  0.4811444878578186
train gradient:  0.14321909171378744
iteration : 10142
train acc:  0.765625
train loss:  0.5024711489677429
train gradient:  0.13889637768329122
iteration : 10143
train acc:  0.8203125
train loss:  0.45317572355270386
train gradient:  0.1039015959126159
iteration : 10144
train acc:  0.7109375
train loss:  0.4959602653980255
train gradient:  0.1114041380886946
iteration : 10145
train acc:  0.7265625
train loss:  0.46080127358436584
train gradient:  0.08187342973127985
iteration : 10146
train acc:  0.734375
train loss:  0.49046996235847473
train gradient:  0.10434665521867904
iteration : 10147
train acc:  0.734375
train loss:  0.5146950483322144
train gradient:  0.13224338927967247
iteration : 10148
train acc:  0.71875
train loss:  0.5045050382614136
train gradient:  0.1505357673003095
iteration : 10149
train acc:  0.71875
train loss:  0.5358988642692566
train gradient:  0.14431441396989103
iteration : 10150
train acc:  0.71875
train loss:  0.5179297924041748
train gradient:  0.12510714889312086
iteration : 10151
train acc:  0.765625
train loss:  0.49220070242881775
train gradient:  0.11479820944546466
iteration : 10152
train acc:  0.7734375
train loss:  0.46638059616088867
train gradient:  0.1253284679404427
iteration : 10153
train acc:  0.6796875
train loss:  0.5880078077316284
train gradient:  0.15804767634370104
iteration : 10154
train acc:  0.734375
train loss:  0.5182737112045288
train gradient:  0.1649040468063937
iteration : 10155
train acc:  0.7890625
train loss:  0.47289204597473145
train gradient:  0.11440107739847158
iteration : 10156
train acc:  0.7265625
train loss:  0.551421582698822
train gradient:  0.15704002373876919
iteration : 10157
train acc:  0.7578125
train loss:  0.44164395332336426
train gradient:  0.09684833416973898
iteration : 10158
train acc:  0.703125
train loss:  0.5154060125350952
train gradient:  0.11716276428616243
iteration : 10159
train acc:  0.8125
train loss:  0.45626676082611084
train gradient:  0.11535119316203578
iteration : 10160
train acc:  0.7421875
train loss:  0.4809485077857971
train gradient:  0.11352795402850745
iteration : 10161
train acc:  0.734375
train loss:  0.4752524495124817
train gradient:  0.11607253009631194
iteration : 10162
train acc:  0.7421875
train loss:  0.5003034472465515
train gradient:  0.15491437843168554
iteration : 10163
train acc:  0.7421875
train loss:  0.4912605881690979
train gradient:  0.10842421865671815
iteration : 10164
train acc:  0.6875
train loss:  0.5781079530715942
train gradient:  0.16447742458823944
iteration : 10165
train acc:  0.7890625
train loss:  0.438819944858551
train gradient:  0.0916414735933683
iteration : 10166
train acc:  0.7578125
train loss:  0.49839991331100464
train gradient:  0.11222045752411053
iteration : 10167
train acc:  0.71875
train loss:  0.5215617418289185
train gradient:  0.1199326128122452
iteration : 10168
train acc:  0.6953125
train loss:  0.5346437692642212
train gradient:  0.13400699726788287
iteration : 10169
train acc:  0.7578125
train loss:  0.4979313313961029
train gradient:  0.15206599662578943
iteration : 10170
train acc:  0.765625
train loss:  0.46213841438293457
train gradient:  0.12886308250552472
iteration : 10171
train acc:  0.734375
train loss:  0.4876633882522583
train gradient:  0.10386990203322023
iteration : 10172
train acc:  0.6953125
train loss:  0.5423204898834229
train gradient:  0.15283839632274426
iteration : 10173
train acc:  0.71875
train loss:  0.5275033712387085
train gradient:  0.1665817225863973
iteration : 10174
train acc:  0.671875
train loss:  0.5474348068237305
train gradient:  0.13897254837033052
iteration : 10175
train acc:  0.7734375
train loss:  0.4453316926956177
train gradient:  0.10161133481624662
iteration : 10176
train acc:  0.796875
train loss:  0.4301172196865082
train gradient:  0.09249780583548964
iteration : 10177
train acc:  0.7734375
train loss:  0.49669694900512695
train gradient:  0.12706745337797024
iteration : 10178
train acc:  0.6015625
train loss:  0.5491564273834229
train gradient:  0.16924852234376814
iteration : 10179
train acc:  0.7421875
train loss:  0.45562100410461426
train gradient:  0.10877052019941941
iteration : 10180
train acc:  0.75
train loss:  0.4626467227935791
train gradient:  0.11115682064840271
iteration : 10181
train acc:  0.8203125
train loss:  0.4429650604724884
train gradient:  0.11714518295480744
iteration : 10182
train acc:  0.75
train loss:  0.48628807067871094
train gradient:  0.12776659703876614
iteration : 10183
train acc:  0.75
train loss:  0.4441056251525879
train gradient:  0.1280916310526944
iteration : 10184
train acc:  0.7734375
train loss:  0.4804837703704834
train gradient:  0.12744746780052899
iteration : 10185
train acc:  0.765625
train loss:  0.4767674207687378
train gradient:  0.10243232684911088
iteration : 10186
train acc:  0.7109375
train loss:  0.5122432112693787
train gradient:  0.12192425077529247
iteration : 10187
train acc:  0.7734375
train loss:  0.48147112131118774
train gradient:  0.17129359548066103
iteration : 10188
train acc:  0.7734375
train loss:  0.478005588054657
train gradient:  0.10496426764510519
iteration : 10189
train acc:  0.8046875
train loss:  0.4371379613876343
train gradient:  0.15056547979164575
iteration : 10190
train acc:  0.71875
train loss:  0.5666818022727966
train gradient:  0.1264745631672014
iteration : 10191
train acc:  0.7578125
train loss:  0.5072802305221558
train gradient:  0.15647833902075595
iteration : 10192
train acc:  0.734375
train loss:  0.5157954096794128
train gradient:  0.17559235511489546
iteration : 10193
train acc:  0.765625
train loss:  0.48942846059799194
train gradient:  0.09857701741742211
iteration : 10194
train acc:  0.7109375
train loss:  0.557445764541626
train gradient:  0.16214655093838598
iteration : 10195
train acc:  0.7734375
train loss:  0.44850873947143555
train gradient:  0.1131891459323753
iteration : 10196
train acc:  0.7578125
train loss:  0.5006636381149292
train gradient:  0.11910218620364126
iteration : 10197
train acc:  0.796875
train loss:  0.45820748805999756
train gradient:  0.1034627988803794
iteration : 10198
train acc:  0.7421875
train loss:  0.4943859875202179
train gradient:  0.10713378903134521
iteration : 10199
train acc:  0.734375
train loss:  0.5740952491760254
train gradient:  0.13864956704936343
iteration : 10200
train acc:  0.765625
train loss:  0.45606282353401184
train gradient:  0.11163427986103228
iteration : 10201
train acc:  0.703125
train loss:  0.5287864804267883
train gradient:  0.13878928066273805
iteration : 10202
train acc:  0.765625
train loss:  0.4559817612171173
train gradient:  0.0968427918937854
iteration : 10203
train acc:  0.75
train loss:  0.46962887048721313
train gradient:  0.10422756203648108
iteration : 10204
train acc:  0.703125
train loss:  0.5322578549385071
train gradient:  0.12244102059214775
iteration : 10205
train acc:  0.75
train loss:  0.4639873206615448
train gradient:  0.12746991719671102
iteration : 10206
train acc:  0.78125
train loss:  0.4563375413417816
train gradient:  0.13847806858171807
iteration : 10207
train acc:  0.71875
train loss:  0.5596562027931213
train gradient:  0.15764754873427667
iteration : 10208
train acc:  0.6875
train loss:  0.5045543909072876
train gradient:  0.15281762268523993
iteration : 10209
train acc:  0.7421875
train loss:  0.4644644558429718
train gradient:  0.10646858703803104
iteration : 10210
train acc:  0.7421875
train loss:  0.5087670087814331
train gradient:  0.11715093435794649
iteration : 10211
train acc:  0.7109375
train loss:  0.5180948972702026
train gradient:  0.1637076243196132
iteration : 10212
train acc:  0.78125
train loss:  0.4664117693901062
train gradient:  0.0985519688600447
iteration : 10213
train acc:  0.671875
train loss:  0.48044493794441223
train gradient:  0.11612136767918402
iteration : 10214
train acc:  0.7734375
train loss:  0.5297552347183228
train gradient:  0.1385903648925506
iteration : 10215
train acc:  0.8203125
train loss:  0.4294629693031311
train gradient:  0.12175059975445954
iteration : 10216
train acc:  0.7578125
train loss:  0.48843061923980713
train gradient:  0.11441785938575445
iteration : 10217
train acc:  0.734375
train loss:  0.4829924702644348
train gradient:  0.12806648020252498
iteration : 10218
train acc:  0.6796875
train loss:  0.4911646842956543
train gradient:  0.1231794700210162
iteration : 10219
train acc:  0.71875
train loss:  0.5155790448188782
train gradient:  0.136911233942405
iteration : 10220
train acc:  0.6796875
train loss:  0.5796095132827759
train gradient:  0.14063841019093112
iteration : 10221
train acc:  0.75
train loss:  0.48843058943748474
train gradient:  0.1578983326947519
iteration : 10222
train acc:  0.7421875
train loss:  0.5073530673980713
train gradient:  0.11170743401904183
iteration : 10223
train acc:  0.7421875
train loss:  0.5297845005989075
train gradient:  0.1234633670916215
iteration : 10224
train acc:  0.75
train loss:  0.49834638833999634
train gradient:  0.10233639793548505
iteration : 10225
train acc:  0.7421875
train loss:  0.4698031544685364
train gradient:  0.14456227688567397
iteration : 10226
train acc:  0.7265625
train loss:  0.5062188506126404
train gradient:  0.1283889639647981
iteration : 10227
train acc:  0.7421875
train loss:  0.475558876991272
train gradient:  0.15534497614340492
iteration : 10228
train acc:  0.71875
train loss:  0.5333144664764404
train gradient:  0.12760407372763927
iteration : 10229
train acc:  0.7421875
train loss:  0.5108340978622437
train gradient:  0.13134473861534157
iteration : 10230
train acc:  0.6796875
train loss:  0.503446638584137
train gradient:  0.17172986544193553
iteration : 10231
train acc:  0.734375
train loss:  0.506230354309082
train gradient:  0.1317766304024065
iteration : 10232
train acc:  0.6875
train loss:  0.5499675273895264
train gradient:  0.1833833640578006
iteration : 10233
train acc:  0.6953125
train loss:  0.523586630821228
train gradient:  0.12086267219948821
iteration : 10234
train acc:  0.71875
train loss:  0.5017601847648621
train gradient:  0.16032301526784076
iteration : 10235
train acc:  0.7734375
train loss:  0.46290475130081177
train gradient:  0.11152913977233343
iteration : 10236
train acc:  0.71875
train loss:  0.5092835426330566
train gradient:  0.09723692299593693
iteration : 10237
train acc:  0.71875
train loss:  0.4553338587284088
train gradient:  0.10235040902682528
iteration : 10238
train acc:  0.734375
train loss:  0.5253896713256836
train gradient:  0.1688229427141688
iteration : 10239
train acc:  0.796875
train loss:  0.4461349546909332
train gradient:  0.09911353631675215
iteration : 10240
train acc:  0.765625
train loss:  0.5150459408760071
train gradient:  0.14276625900256207
iteration : 10241
train acc:  0.7578125
train loss:  0.5096368789672852
train gradient:  0.12462561247384975
iteration : 10242
train acc:  0.7265625
train loss:  0.510058581829071
train gradient:  0.1254534496758229
iteration : 10243
train acc:  0.6875
train loss:  0.536108672618866
train gradient:  0.1388444350639772
iteration : 10244
train acc:  0.6953125
train loss:  0.504402220249176
train gradient:  0.12778500883157973
iteration : 10245
train acc:  0.7421875
train loss:  0.5307184457778931
train gradient:  0.14564902626164505
iteration : 10246
train acc:  0.6875
train loss:  0.5066145658493042
train gradient:  0.14140298090521236
iteration : 10247
train acc:  0.765625
train loss:  0.5048026442527771
train gradient:  0.11113048567542028
iteration : 10248
train acc:  0.765625
train loss:  0.472703218460083
train gradient:  0.11353846945823355
iteration : 10249
train acc:  0.7421875
train loss:  0.529094934463501
train gradient:  0.12740679885822717
iteration : 10250
train acc:  0.6953125
train loss:  0.529015302658081
train gradient:  0.1525473508246476
iteration : 10251
train acc:  0.75
train loss:  0.4528821110725403
train gradient:  0.13562185791813813
iteration : 10252
train acc:  0.703125
train loss:  0.4696018695831299
train gradient:  0.13501623550741787
iteration : 10253
train acc:  0.75
train loss:  0.4945431351661682
train gradient:  0.11359902116906491
iteration : 10254
train acc:  0.75
train loss:  0.4466727375984192
train gradient:  0.11558073734941127
iteration : 10255
train acc:  0.71875
train loss:  0.5306593179702759
train gradient:  0.11697429756427029
iteration : 10256
train acc:  0.8125
train loss:  0.4434366822242737
train gradient:  0.10967257520057222
iteration : 10257
train acc:  0.796875
train loss:  0.4581115245819092
train gradient:  0.11390470478144997
iteration : 10258
train acc:  0.765625
train loss:  0.509389340877533
train gradient:  0.161448963659928
iteration : 10259
train acc:  0.765625
train loss:  0.4851951003074646
train gradient:  0.10452071637398655
iteration : 10260
train acc:  0.7890625
train loss:  0.48220112919807434
train gradient:  0.13977733072013332
iteration : 10261
train acc:  0.7421875
train loss:  0.47926896810531616
train gradient:  0.11749340263891353
iteration : 10262
train acc:  0.75
train loss:  0.46444663405418396
train gradient:  0.1505182193941359
iteration : 10263
train acc:  0.7421875
train loss:  0.5058631896972656
train gradient:  0.12850849148684612
iteration : 10264
train acc:  0.6875
train loss:  0.5453155040740967
train gradient:  0.16192600182810163
iteration : 10265
train acc:  0.6953125
train loss:  0.5649020671844482
train gradient:  0.1481874481578906
iteration : 10266
train acc:  0.7109375
train loss:  0.5208144783973694
train gradient:  0.1061014909552655
iteration : 10267
train acc:  0.75
train loss:  0.46567901968955994
train gradient:  0.09045902233431952
iteration : 10268
train acc:  0.7421875
train loss:  0.5124847888946533
train gradient:  0.11025748731289507
iteration : 10269
train acc:  0.7578125
train loss:  0.4875198006629944
train gradient:  0.12572862645611294
iteration : 10270
train acc:  0.7421875
train loss:  0.5033859014511108
train gradient:  0.1367830511811217
iteration : 10271
train acc:  0.7265625
train loss:  0.48617348074913025
train gradient:  0.13365298409060644
iteration : 10272
train acc:  0.6953125
train loss:  0.6159811615943909
train gradient:  0.20936880608842007
iteration : 10273
train acc:  0.8203125
train loss:  0.4798896312713623
train gradient:  0.12354143016101941
iteration : 10274
train acc:  0.7890625
train loss:  0.4591063857078552
train gradient:  0.09934440788440783
iteration : 10275
train acc:  0.6953125
train loss:  0.49472248554229736
train gradient:  0.1342317365355859
iteration : 10276
train acc:  0.7578125
train loss:  0.49108973145484924
train gradient:  0.09944363016018627
iteration : 10277
train acc:  0.734375
train loss:  0.4688146114349365
train gradient:  0.09643874165611264
iteration : 10278
train acc:  0.6875
train loss:  0.5337604284286499
train gradient:  0.12993596888325534
iteration : 10279
train acc:  0.796875
train loss:  0.39800363779067993
train gradient:  0.08874564210959725
iteration : 10280
train acc:  0.8046875
train loss:  0.4468485116958618
train gradient:  0.10635597445815505
iteration : 10281
train acc:  0.7421875
train loss:  0.4826924800872803
train gradient:  0.1092483039873733
iteration : 10282
train acc:  0.7109375
train loss:  0.5371540784835815
train gradient:  0.1732752004711285
iteration : 10283
train acc:  0.796875
train loss:  0.5010427832603455
train gradient:  0.09500922447136141
iteration : 10284
train acc:  0.8046875
train loss:  0.46536052227020264
train gradient:  0.11138087838446671
iteration : 10285
train acc:  0.6875
train loss:  0.5664016008377075
train gradient:  0.13640325174277926
iteration : 10286
train acc:  0.7265625
train loss:  0.5223784446716309
train gradient:  0.13440331779475798
iteration : 10287
train acc:  0.703125
train loss:  0.5270429253578186
train gradient:  0.13071224202505588
iteration : 10288
train acc:  0.7109375
train loss:  0.5607714653015137
train gradient:  0.18643597830602082
iteration : 10289
train acc:  0.734375
train loss:  0.509069561958313
train gradient:  0.09717041073734436
iteration : 10290
train acc:  0.7890625
train loss:  0.4457910656929016
train gradient:  0.09481069949121941
iteration : 10291
train acc:  0.7578125
train loss:  0.45918601751327515
train gradient:  0.10947841860563096
iteration : 10292
train acc:  0.75
train loss:  0.4551915228366852
train gradient:  0.14963741739528205
iteration : 10293
train acc:  0.7421875
train loss:  0.4921409487724304
train gradient:  0.10458502555819778
iteration : 10294
train acc:  0.7109375
train loss:  0.5528601408004761
train gradient:  0.1660678934019883
iteration : 10295
train acc:  0.6953125
train loss:  0.516741156578064
train gradient:  0.12129680322725005
iteration : 10296
train acc:  0.8203125
train loss:  0.4063720703125
train gradient:  0.08805229984760703
iteration : 10297
train acc:  0.7265625
train loss:  0.4638005495071411
train gradient:  0.10503117440191155
iteration : 10298
train acc:  0.71875
train loss:  0.5207026600837708
train gradient:  0.13646361689202052
iteration : 10299
train acc:  0.6953125
train loss:  0.5344139337539673
train gradient:  0.20812044317421394
iteration : 10300
train acc:  0.734375
train loss:  0.446134477853775
train gradient:  0.08817482040161514
iteration : 10301
train acc:  0.6953125
train loss:  0.5349230766296387
train gradient:  0.14229272031477758
iteration : 10302
train acc:  0.75
train loss:  0.45926350355148315
train gradient:  0.10272425813511292
iteration : 10303
train acc:  0.6875
train loss:  0.5304599404335022
train gradient:  0.10480484806143692
iteration : 10304
train acc:  0.7421875
train loss:  0.4513019621372223
train gradient:  0.14182699346731376
iteration : 10305
train acc:  0.734375
train loss:  0.46743690967559814
train gradient:  0.10741344212557934
iteration : 10306
train acc:  0.6953125
train loss:  0.5087992548942566
train gradient:  0.1215999816149555
iteration : 10307
train acc:  0.765625
train loss:  0.4741014838218689
train gradient:  0.08989268112518502
iteration : 10308
train acc:  0.703125
train loss:  0.5063175559043884
train gradient:  0.12857376161175693
iteration : 10309
train acc:  0.7890625
train loss:  0.4305180311203003
train gradient:  0.09001034356474422
iteration : 10310
train acc:  0.7890625
train loss:  0.4420812129974365
train gradient:  0.08169254912509427
iteration : 10311
train acc:  0.7578125
train loss:  0.4525887668132782
train gradient:  0.10243867279259673
iteration : 10312
train acc:  0.796875
train loss:  0.4808696508407593
train gradient:  0.11693370982345956
iteration : 10313
train acc:  0.734375
train loss:  0.5258840322494507
train gradient:  0.12495162442294816
iteration : 10314
train acc:  0.703125
train loss:  0.5086907148361206
train gradient:  0.1119523684800317
iteration : 10315
train acc:  0.7734375
train loss:  0.4769923686981201
train gradient:  0.1357096320243653
iteration : 10316
train acc:  0.7890625
train loss:  0.44381099939346313
train gradient:  0.10910182558456512
iteration : 10317
train acc:  0.6953125
train loss:  0.5321223735809326
train gradient:  0.14256440373210894
iteration : 10318
train acc:  0.703125
train loss:  0.6502991914749146
train gradient:  0.239275561091207
iteration : 10319
train acc:  0.7421875
train loss:  0.4991150200366974
train gradient:  0.11239458364168387
iteration : 10320
train acc:  0.6796875
train loss:  0.4988558888435364
train gradient:  0.11019229354084187
iteration : 10321
train acc:  0.71875
train loss:  0.516229510307312
train gradient:  0.13156539229288733
iteration : 10322
train acc:  0.8203125
train loss:  0.4330940246582031
train gradient:  0.08724064230031477
iteration : 10323
train acc:  0.7421875
train loss:  0.5062479972839355
train gradient:  0.10553256637694883
iteration : 10324
train acc:  0.7578125
train loss:  0.4617248475551605
train gradient:  0.1104602851059994
iteration : 10325
train acc:  0.7578125
train loss:  0.4468071460723877
train gradient:  0.09740606404408332
iteration : 10326
train acc:  0.734375
train loss:  0.45853444933891296
train gradient:  0.09452788374523557
iteration : 10327
train acc:  0.6640625
train loss:  0.5222154259681702
train gradient:  0.1102414113777024
iteration : 10328
train acc:  0.765625
train loss:  0.4438658356666565
train gradient:  0.09789956165894849
iteration : 10329
train acc:  0.8125
train loss:  0.44754236936569214
train gradient:  0.09031006797155089
iteration : 10330
train acc:  0.7734375
train loss:  0.47801077365875244
train gradient:  0.1230370956084309
iteration : 10331
train acc:  0.7265625
train loss:  0.5062555074691772
train gradient:  0.11411743683213973
iteration : 10332
train acc:  0.7734375
train loss:  0.4141755700111389
train gradient:  0.09902215053276832
iteration : 10333
train acc:  0.75
train loss:  0.49968796968460083
train gradient:  0.20792692560697348
iteration : 10334
train acc:  0.8125
train loss:  0.4116975665092468
train gradient:  0.07451112975157687
iteration : 10335
train acc:  0.671875
train loss:  0.5957227945327759
train gradient:  0.1758620614767265
iteration : 10336
train acc:  0.7421875
train loss:  0.5025466680526733
train gradient:  0.12252202156432664
iteration : 10337
train acc:  0.703125
train loss:  0.5377280712127686
train gradient:  0.1290562214614079
iteration : 10338
train acc:  0.7890625
train loss:  0.48574429750442505
train gradient:  0.11903274030690411
iteration : 10339
train acc:  0.7734375
train loss:  0.40456488728523254
train gradient:  0.08866317363050474
iteration : 10340
train acc:  0.765625
train loss:  0.46268463134765625
train gradient:  0.09624470810512882
iteration : 10341
train acc:  0.78125
train loss:  0.44167888164520264
train gradient:  0.11373703941590411
iteration : 10342
train acc:  0.7265625
train loss:  0.4694022536277771
train gradient:  0.10713936737491443
iteration : 10343
train acc:  0.7421875
train loss:  0.487506240606308
train gradient:  0.12552603567315151
iteration : 10344
train acc:  0.7734375
train loss:  0.47825050354003906
train gradient:  0.10812799883108805
iteration : 10345
train acc:  0.765625
train loss:  0.43669992685317993
train gradient:  0.09291366303356191
iteration : 10346
train acc:  0.765625
train loss:  0.4657432436943054
train gradient:  0.11284538934897827
iteration : 10347
train acc:  0.703125
train loss:  0.5157592296600342
train gradient:  0.13542847573579747
iteration : 10348
train acc:  0.7109375
train loss:  0.5297574400901794
train gradient:  0.13105917725220054
iteration : 10349
train acc:  0.6796875
train loss:  0.5609684586524963
train gradient:  0.15275240144600072
iteration : 10350
train acc:  0.671875
train loss:  0.5672025084495544
train gradient:  0.18151553460767267
iteration : 10351
train acc:  0.7578125
train loss:  0.4386783838272095
train gradient:  0.10591735027057328
iteration : 10352
train acc:  0.765625
train loss:  0.5200110077857971
train gradient:  0.12458850745105682
iteration : 10353
train acc:  0.8125
train loss:  0.4288812279701233
train gradient:  0.1051939275910972
iteration : 10354
train acc:  0.7578125
train loss:  0.47419849038124084
train gradient:  0.09698440484484405
iteration : 10355
train acc:  0.6953125
train loss:  0.5057578086853027
train gradient:  0.12371921639418232
iteration : 10356
train acc:  0.8125
train loss:  0.4333975315093994
train gradient:  0.10510955909986
iteration : 10357
train acc:  0.7734375
train loss:  0.49761083722114563
train gradient:  0.1299663507474229
iteration : 10358
train acc:  0.6875
train loss:  0.5495631694793701
train gradient:  0.18021114332683447
iteration : 10359
train acc:  0.7109375
train loss:  0.4939833879470825
train gradient:  0.11311795632608802
iteration : 10360
train acc:  0.75
train loss:  0.4924912452697754
train gradient:  0.1624527797310794
iteration : 10361
train acc:  0.7265625
train loss:  0.5181839466094971
train gradient:  0.14316677733969985
iteration : 10362
train acc:  0.7421875
train loss:  0.493489146232605
train gradient:  0.1519942446156602
iteration : 10363
train acc:  0.765625
train loss:  0.49079200625419617
train gradient:  0.14729962565948548
iteration : 10364
train acc:  0.7578125
train loss:  0.493127703666687
train gradient:  0.15153980645597281
iteration : 10365
train acc:  0.734375
train loss:  0.5174867510795593
train gradient:  0.13243155438040438
iteration : 10366
train acc:  0.78125
train loss:  0.47681382298469543
train gradient:  0.10276383970628537
iteration : 10367
train acc:  0.7265625
train loss:  0.5107284188270569
train gradient:  0.12115491396439003
iteration : 10368
train acc:  0.796875
train loss:  0.49995553493499756
train gradient:  0.18036779777036355
iteration : 10369
train acc:  0.78125
train loss:  0.49584850668907166
train gradient:  0.12281255625166955
iteration : 10370
train acc:  0.75
train loss:  0.47133055329322815
train gradient:  0.09360774956791505
iteration : 10371
train acc:  0.6953125
train loss:  0.4685218334197998
train gradient:  0.09816487516826329
iteration : 10372
train acc:  0.7109375
train loss:  0.5136995315551758
train gradient:  0.1349912864908357
iteration : 10373
train acc:  0.7734375
train loss:  0.45766735076904297
train gradient:  0.09709782878402343
iteration : 10374
train acc:  0.7265625
train loss:  0.4914426803588867
train gradient:  0.12629854935479184
iteration : 10375
train acc:  0.7578125
train loss:  0.4579557180404663
train gradient:  0.12144957345038873
iteration : 10376
train acc:  0.7265625
train loss:  0.5111685991287231
train gradient:  0.11352155049572644
iteration : 10377
train acc:  0.734375
train loss:  0.4700329899787903
train gradient:  0.1166703296595235
iteration : 10378
train acc:  0.7578125
train loss:  0.5033625364303589
train gradient:  0.14485751467093966
iteration : 10379
train acc:  0.7734375
train loss:  0.47424086928367615
train gradient:  0.09239288891437125
iteration : 10380
train acc:  0.7890625
train loss:  0.4785824418067932
train gradient:  0.14381758668503394
iteration : 10381
train acc:  0.7265625
train loss:  0.5130540728569031
train gradient:  0.11544249527184162
iteration : 10382
train acc:  0.7734375
train loss:  0.5029283165931702
train gradient:  0.12303254688408388
iteration : 10383
train acc:  0.75
train loss:  0.5121825337409973
train gradient:  0.12543498079562795
iteration : 10384
train acc:  0.7109375
train loss:  0.4965331256389618
train gradient:  0.12593722111876715
iteration : 10385
train acc:  0.7734375
train loss:  0.503847599029541
train gradient:  0.11358625087667847
iteration : 10386
train acc:  0.734375
train loss:  0.5675827264785767
train gradient:  0.1530575135578362
iteration : 10387
train acc:  0.75
train loss:  0.4708517789840698
train gradient:  0.12370101168005752
iteration : 10388
train acc:  0.6796875
train loss:  0.5811420679092407
train gradient:  0.16020501671401202
iteration : 10389
train acc:  0.7734375
train loss:  0.4622427821159363
train gradient:  0.09252406340164064
iteration : 10390
train acc:  0.7109375
train loss:  0.5325643420219421
train gradient:  0.15930794231487133
iteration : 10391
train acc:  0.765625
train loss:  0.4652556777000427
train gradient:  0.08599485156569912
iteration : 10392
train acc:  0.8046875
train loss:  0.44770747423171997
train gradient:  0.12608419062112505
iteration : 10393
train acc:  0.7890625
train loss:  0.4250340163707733
train gradient:  0.08972289125502846
iteration : 10394
train acc:  0.71875
train loss:  0.46783778071403503
train gradient:  0.1264778543663267
iteration : 10395
train acc:  0.71875
train loss:  0.5272691249847412
train gradient:  0.1357394837809186
iteration : 10396
train acc:  0.7265625
train loss:  0.45497000217437744
train gradient:  0.10243753875771561
iteration : 10397
train acc:  0.734375
train loss:  0.5101782083511353
train gradient:  0.12859622938445575
iteration : 10398
train acc:  0.7421875
train loss:  0.4951958656311035
train gradient:  0.12797449665299665
iteration : 10399
train acc:  0.7265625
train loss:  0.5188992619514465
train gradient:  0.12418821355214882
iteration : 10400
train acc:  0.7109375
train loss:  0.566392719745636
train gradient:  0.16443231332092323
iteration : 10401
train acc:  0.78125
train loss:  0.44395712018013
train gradient:  0.12541008836442513
iteration : 10402
train acc:  0.7890625
train loss:  0.4275396168231964
train gradient:  0.10000363212690407
iteration : 10403
train acc:  0.734375
train loss:  0.47946077585220337
train gradient:  0.105385505685852
iteration : 10404
train acc:  0.7421875
train loss:  0.5012555718421936
train gradient:  0.11491422030310668
iteration : 10405
train acc:  0.734375
train loss:  0.5008959770202637
train gradient:  0.13837945148005848
iteration : 10406
train acc:  0.84375
train loss:  0.4240588843822479
train gradient:  0.0997356196799565
iteration : 10407
train acc:  0.71875
train loss:  0.4991597533226013
train gradient:  0.12481160419452325
iteration : 10408
train acc:  0.6875
train loss:  0.5685813426971436
train gradient:  0.16442495684521097
iteration : 10409
train acc:  0.78125
train loss:  0.4367932975292206
train gradient:  0.09242417096258623
iteration : 10410
train acc:  0.75
train loss:  0.546295166015625
train gradient:  0.25654935513637256
iteration : 10411
train acc:  0.7265625
train loss:  0.47876009345054626
train gradient:  0.11714583557704855
iteration : 10412
train acc:  0.65625
train loss:  0.6052466630935669
train gradient:  0.18459181786713322
iteration : 10413
train acc:  0.796875
train loss:  0.44160619378089905
train gradient:  0.09279865859826791
iteration : 10414
train acc:  0.6875
train loss:  0.5165563821792603
train gradient:  0.14392763165553363
iteration : 10415
train acc:  0.734375
train loss:  0.4991474747657776
train gradient:  0.12324377610524008
iteration : 10416
train acc:  0.8125
train loss:  0.47805255651474
train gradient:  0.12334001536453813
iteration : 10417
train acc:  0.6796875
train loss:  0.5606603026390076
train gradient:  0.1550360143352589
iteration : 10418
train acc:  0.7265625
train loss:  0.49325111508369446
train gradient:  0.15701101319449268
iteration : 10419
train acc:  0.765625
train loss:  0.49269789457321167
train gradient:  0.1324145825043735
iteration : 10420
train acc:  0.75
train loss:  0.49300822615623474
train gradient:  0.10915709036759844
iteration : 10421
train acc:  0.6875
train loss:  0.5386260151863098
train gradient:  0.1585312511083417
iteration : 10422
train acc:  0.7890625
train loss:  0.45756855607032776
train gradient:  0.09987194829378793
iteration : 10423
train acc:  0.7890625
train loss:  0.4514349699020386
train gradient:  0.10519050550444838
iteration : 10424
train acc:  0.7734375
train loss:  0.46929439902305603
train gradient:  0.10218859839361448
iteration : 10425
train acc:  0.703125
train loss:  0.5420703291893005
train gradient:  0.14424111550047525
iteration : 10426
train acc:  0.78125
train loss:  0.44703176617622375
train gradient:  0.11563526598828647
iteration : 10427
train acc:  0.7890625
train loss:  0.45440220832824707
train gradient:  0.09889344285072517
iteration : 10428
train acc:  0.7578125
train loss:  0.47986093163490295
train gradient:  0.10582166064385287
iteration : 10429
train acc:  0.75
train loss:  0.4884120523929596
train gradient:  0.10158690133736067
iteration : 10430
train acc:  0.8125
train loss:  0.470922589302063
train gradient:  0.1086757660109734
iteration : 10431
train acc:  0.7109375
train loss:  0.5016268491744995
train gradient:  0.1311593533783837
iteration : 10432
train acc:  0.6796875
train loss:  0.5386936664581299
train gradient:  0.12177602230573749
iteration : 10433
train acc:  0.734375
train loss:  0.4898354709148407
train gradient:  0.14122445064976802
iteration : 10434
train acc:  0.7578125
train loss:  0.44823282957077026
train gradient:  0.0920171699645177
iteration : 10435
train acc:  0.734375
train loss:  0.4830602705478668
train gradient:  0.1359864563413357
iteration : 10436
train acc:  0.7421875
train loss:  0.4564836025238037
train gradient:  0.09190475539853701
iteration : 10437
train acc:  0.7265625
train loss:  0.5458596348762512
train gradient:  0.1458542589899529
iteration : 10438
train acc:  0.671875
train loss:  0.5876586437225342
train gradient:  0.1698800477004455
iteration : 10439
train acc:  0.7578125
train loss:  0.48911377787590027
train gradient:  0.1156841765026804
iteration : 10440
train acc:  0.765625
train loss:  0.45558616518974304
train gradient:  0.11221116526984858
iteration : 10441
train acc:  0.7265625
train loss:  0.49456310272216797
train gradient:  0.11140562007698292
iteration : 10442
train acc:  0.6875
train loss:  0.5453614592552185
train gradient:  0.14229524279200906
iteration : 10443
train acc:  0.75
train loss:  0.49134230613708496
train gradient:  0.1380871884774858
iteration : 10444
train acc:  0.703125
train loss:  0.5179182291030884
train gradient:  0.12234082918527657
iteration : 10445
train acc:  0.796875
train loss:  0.42555588483810425
train gradient:  0.11118903930065459
iteration : 10446
train acc:  0.6875
train loss:  0.5396184921264648
train gradient:  0.14792110121181984
iteration : 10447
train acc:  0.71875
train loss:  0.585976779460907
train gradient:  0.17180881291953126
iteration : 10448
train acc:  0.796875
train loss:  0.4688529074192047
train gradient:  0.09578035898264818
iteration : 10449
train acc:  0.6796875
train loss:  0.5923482179641724
train gradient:  0.14781811189957322
iteration : 10450
train acc:  0.7265625
train loss:  0.5488991141319275
train gradient:  0.12554758080014494
iteration : 10451
train acc:  0.65625
train loss:  0.5732836723327637
train gradient:  0.17584978435812132
iteration : 10452
train acc:  0.6796875
train loss:  0.5041942596435547
train gradient:  0.11952628840255597
iteration : 10453
train acc:  0.6953125
train loss:  0.5198801755905151
train gradient:  0.10456843963310201
iteration : 10454
train acc:  0.7109375
train loss:  0.5339274406433105
train gradient:  0.1437218736121884
iteration : 10455
train acc:  0.6640625
train loss:  0.5593779683113098
train gradient:  0.15376168899652481
iteration : 10456
train acc:  0.6953125
train loss:  0.5659882426261902
train gradient:  0.15047235958754207
iteration : 10457
train acc:  0.7421875
train loss:  0.4606918394565582
train gradient:  0.10133635782273427
iteration : 10458
train acc:  0.8125
train loss:  0.4346470832824707
train gradient:  0.10931953974503356
iteration : 10459
train acc:  0.703125
train loss:  0.5094016194343567
train gradient:  0.11012439480139449
iteration : 10460
train acc:  0.7109375
train loss:  0.47439756989479065
train gradient:  0.0824216294789532
iteration : 10461
train acc:  0.765625
train loss:  0.47101104259490967
train gradient:  0.12218015039519078
iteration : 10462
train acc:  0.78125
train loss:  0.4874744415283203
train gradient:  0.1086314337812994
iteration : 10463
train acc:  0.7265625
train loss:  0.5187727808952332
train gradient:  0.10518474510859935
iteration : 10464
train acc:  0.6796875
train loss:  0.5557883977890015
train gradient:  0.1499332441946995
iteration : 10465
train acc:  0.734375
train loss:  0.4865584075450897
train gradient:  0.14422337843226374
iteration : 10466
train acc:  0.7578125
train loss:  0.5500578880310059
train gradient:  0.11678884764962741
iteration : 10467
train acc:  0.7265625
train loss:  0.480716735124588
train gradient:  0.1353044036663048
iteration : 10468
train acc:  0.6953125
train loss:  0.5405691266059875
train gradient:  0.1273163566672475
iteration : 10469
train acc:  0.8125
train loss:  0.4312129020690918
train gradient:  0.0953601698088092
iteration : 10470
train acc:  0.7734375
train loss:  0.43427321314811707
train gradient:  0.09131310763003482
iteration : 10471
train acc:  0.71875
train loss:  0.5221583843231201
train gradient:  0.14262002509661054
iteration : 10472
train acc:  0.8046875
train loss:  0.43698185682296753
train gradient:  0.0979139636551975
iteration : 10473
train acc:  0.7734375
train loss:  0.4845729172229767
train gradient:  0.13661585895601602
iteration : 10474
train acc:  0.6953125
train loss:  0.5105680227279663
train gradient:  0.12850536155789555
iteration : 10475
train acc:  0.75
train loss:  0.4859561026096344
train gradient:  0.11653327058978423
iteration : 10476
train acc:  0.7578125
train loss:  0.4661135673522949
train gradient:  0.1185268581312918
iteration : 10477
train acc:  0.765625
train loss:  0.48050475120544434
train gradient:  0.13297782239220882
iteration : 10478
train acc:  0.6640625
train loss:  0.5544803738594055
train gradient:  0.16638415793333278
iteration : 10479
train acc:  0.78125
train loss:  0.4492548108100891
train gradient:  0.0938638716515769
iteration : 10480
train acc:  0.7421875
train loss:  0.5053003430366516
train gradient:  0.1474371929793877
iteration : 10481
train acc:  0.7109375
train loss:  0.49708062410354614
train gradient:  0.11731861836070041
iteration : 10482
train acc:  0.7265625
train loss:  0.5327210426330566
train gradient:  0.12021359462436464
iteration : 10483
train acc:  0.7265625
train loss:  0.5627729892730713
train gradient:  0.14362818881590422
iteration : 10484
train acc:  0.7578125
train loss:  0.4750030040740967
train gradient:  0.09949508288984613
iteration : 10485
train acc:  0.78125
train loss:  0.42590785026550293
train gradient:  0.11816283687321959
iteration : 10486
train acc:  0.78125
train loss:  0.4835888147354126
train gradient:  0.10992430246089241
iteration : 10487
train acc:  0.7421875
train loss:  0.5104670524597168
train gradient:  0.12823330043934567
iteration : 10488
train acc:  0.75
train loss:  0.48609116673469543
train gradient:  0.11100860521475872
iteration : 10489
train acc:  0.75
train loss:  0.46466654539108276
train gradient:  0.10752520998646817
iteration : 10490
train acc:  0.7890625
train loss:  0.47328805923461914
train gradient:  0.09349203893313261
iteration : 10491
train acc:  0.7578125
train loss:  0.4941093325614929
train gradient:  0.13304004736540628
iteration : 10492
train acc:  0.703125
train loss:  0.5145096778869629
train gradient:  0.13887803207781413
iteration : 10493
train acc:  0.796875
train loss:  0.4718155562877655
train gradient:  0.11374905511766721
iteration : 10494
train acc:  0.71875
train loss:  0.4731473922729492
train gradient:  0.12272294835477242
iteration : 10495
train acc:  0.765625
train loss:  0.47676321864128113
train gradient:  0.10051197567750042
iteration : 10496
train acc:  0.7890625
train loss:  0.48088565468788147
train gradient:  0.11295475337196031
iteration : 10497
train acc:  0.6953125
train loss:  0.5248111486434937
train gradient:  0.14252946859950028
iteration : 10498
train acc:  0.703125
train loss:  0.5143778920173645
train gradient:  0.13128679465957055
iteration : 10499
train acc:  0.7578125
train loss:  0.5113220810890198
train gradient:  0.12925338772500095
iteration : 10500
train acc:  0.7578125
train loss:  0.5062283873558044
train gradient:  0.12662770650067925
iteration : 10501
train acc:  0.7421875
train loss:  0.48360514640808105
train gradient:  0.1296901980211837
iteration : 10502
train acc:  0.671875
train loss:  0.5467918515205383
train gradient:  0.16748844778097
iteration : 10503
train acc:  0.734375
train loss:  0.5242291688919067
train gradient:  0.10583227851539154
iteration : 10504
train acc:  0.75
train loss:  0.4885641038417816
train gradient:  0.11283443568390665
iteration : 10505
train acc:  0.6640625
train loss:  0.5791506171226501
train gradient:  0.14346402676775585
iteration : 10506
train acc:  0.6640625
train loss:  0.5331185460090637
train gradient:  0.17005935534662148
iteration : 10507
train acc:  0.734375
train loss:  0.49155497550964355
train gradient:  0.11414210945861036
iteration : 10508
train acc:  0.75
train loss:  0.5027188062667847
train gradient:  0.1395880951711704
iteration : 10509
train acc:  0.6796875
train loss:  0.5646307468414307
train gradient:  0.13749669632498016
iteration : 10510
train acc:  0.8125
train loss:  0.47981852293014526
train gradient:  0.1397780512730153
iteration : 10511
train acc:  0.8046875
train loss:  0.4294559955596924
train gradient:  0.09582530698500348
iteration : 10512
train acc:  0.7265625
train loss:  0.5164437294006348
train gradient:  0.12517403576352107
iteration : 10513
train acc:  0.7734375
train loss:  0.48316627740859985
train gradient:  0.09539383926111501
iteration : 10514
train acc:  0.7890625
train loss:  0.45683544874191284
train gradient:  0.09119997253979763
iteration : 10515
train acc:  0.71875
train loss:  0.4892663061618805
train gradient:  0.12378923037789039
iteration : 10516
train acc:  0.765625
train loss:  0.43636712431907654
train gradient:  0.09596432589353905
iteration : 10517
train acc:  0.8046875
train loss:  0.4291437268257141
train gradient:  0.08444065263493294
iteration : 10518
train acc:  0.671875
train loss:  0.534773588180542
train gradient:  0.12742963968076101
iteration : 10519
train acc:  0.75
train loss:  0.48986196517944336
train gradient:  0.10032852667191677
iteration : 10520
train acc:  0.7578125
train loss:  0.4510343670845032
train gradient:  0.09181743511914008
iteration : 10521
train acc:  0.78125
train loss:  0.45033442974090576
train gradient:  0.12146132577362553
iteration : 10522
train acc:  0.7265625
train loss:  0.5465589761734009
train gradient:  0.13425503836754485
iteration : 10523
train acc:  0.8046875
train loss:  0.4211207628250122
train gradient:  0.0841005414643179
iteration : 10524
train acc:  0.7734375
train loss:  0.45085638761520386
train gradient:  0.1199887393241789
iteration : 10525
train acc:  0.734375
train loss:  0.4598294496536255
train gradient:  0.10602130577687895
iteration : 10526
train acc:  0.8046875
train loss:  0.44175153970718384
train gradient:  0.1025427025500385
iteration : 10527
train acc:  0.7421875
train loss:  0.4763008952140808
train gradient:  0.10333922757682548
iteration : 10528
train acc:  0.8203125
train loss:  0.40758854150772095
train gradient:  0.08284492547925432
iteration : 10529
train acc:  0.78125
train loss:  0.4809742569923401
train gradient:  0.1140400498995953
iteration : 10530
train acc:  0.7578125
train loss:  0.46633410453796387
train gradient:  0.09984768921201578
iteration : 10531
train acc:  0.6875
train loss:  0.495594322681427
train gradient:  0.13478648289759024
iteration : 10532
train acc:  0.7734375
train loss:  0.45874881744384766
train gradient:  0.09438617871139555
iteration : 10533
train acc:  0.7421875
train loss:  0.5356871485710144
train gradient:  0.1244857568385474
iteration : 10534
train acc:  0.6953125
train loss:  0.5281159281730652
train gradient:  0.13558384186964706
iteration : 10535
train acc:  0.78125
train loss:  0.4398229718208313
train gradient:  0.0915251640750088
iteration : 10536
train acc:  0.703125
train loss:  0.4860934913158417
train gradient:  0.12661546199075818
iteration : 10537
train acc:  0.7734375
train loss:  0.4682214558124542
train gradient:  0.13199544927197668
iteration : 10538
train acc:  0.7578125
train loss:  0.4527033269405365
train gradient:  0.10007798476557751
iteration : 10539
train acc:  0.78125
train loss:  0.4296099543571472
train gradient:  0.09284510322771333
iteration : 10540
train acc:  0.765625
train loss:  0.46767526865005493
train gradient:  0.1486974820972261
iteration : 10541
train acc:  0.7578125
train loss:  0.4509250819683075
train gradient:  0.10049291466373908
iteration : 10542
train acc:  0.8046875
train loss:  0.45356255769729614
train gradient:  0.10047283635204707
iteration : 10543
train acc:  0.7734375
train loss:  0.47487351298332214
train gradient:  0.11597255037715083
iteration : 10544
train acc:  0.671875
train loss:  0.5898311734199524
train gradient:  0.1409355700680596
iteration : 10545
train acc:  0.7421875
train loss:  0.4960336685180664
train gradient:  0.12404160494309631
iteration : 10546
train acc:  0.7578125
train loss:  0.4474320113658905
train gradient:  0.10229812335592604
iteration : 10547
train acc:  0.75
train loss:  0.5034993886947632
train gradient:  0.11682149089758392
iteration : 10548
train acc:  0.734375
train loss:  0.5362619757652283
train gradient:  0.12658775856269988
iteration : 10549
train acc:  0.78125
train loss:  0.4267212450504303
train gradient:  0.11374891374550575
iteration : 10550
train acc:  0.7578125
train loss:  0.5247211456298828
train gradient:  0.10928686873301882
iteration : 10551
train acc:  0.6640625
train loss:  0.5397149324417114
train gradient:  0.14438007507523323
iteration : 10552
train acc:  0.765625
train loss:  0.4512118101119995
train gradient:  0.10430656856139837
iteration : 10553
train acc:  0.7578125
train loss:  0.47097504138946533
train gradient:  0.12633649588854762
iteration : 10554
train acc:  0.8046875
train loss:  0.43490278720855713
train gradient:  0.08781946953012029
iteration : 10555
train acc:  0.796875
train loss:  0.4585219919681549
train gradient:  0.10510814502155257
iteration : 10556
train acc:  0.765625
train loss:  0.48765647411346436
train gradient:  0.12774175889049266
iteration : 10557
train acc:  0.796875
train loss:  0.4221760630607605
train gradient:  0.1117853468921992
iteration : 10558
train acc:  0.6640625
train loss:  0.554442286491394
train gradient:  0.15666216844305766
iteration : 10559
train acc:  0.7578125
train loss:  0.5098097920417786
train gradient:  0.10099631984919032
iteration : 10560
train acc:  0.7421875
train loss:  0.5149748921394348
train gradient:  0.12714902389296218
iteration : 10561
train acc:  0.7734375
train loss:  0.43536871671676636
train gradient:  0.11023529864033023
iteration : 10562
train acc:  0.7734375
train loss:  0.4354704022407532
train gradient:  0.0837238639218287
iteration : 10563
train acc:  0.7265625
train loss:  0.4765450358390808
train gradient:  0.13622730880819348
iteration : 10564
train acc:  0.78125
train loss:  0.49106988310813904
train gradient:  0.1226409631938742
iteration : 10565
train acc:  0.7578125
train loss:  0.48075565695762634
train gradient:  0.11150542570498462
iteration : 10566
train acc:  0.7265625
train loss:  0.4798487424850464
train gradient:  0.12653356709483982
iteration : 10567
train acc:  0.65625
train loss:  0.5633516311645508
train gradient:  0.1826772481591175
iteration : 10568
train acc:  0.8203125
train loss:  0.45583024621009827
train gradient:  0.10683544336033296
iteration : 10569
train acc:  0.765625
train loss:  0.44613680243492126
train gradient:  0.0886744679070312
iteration : 10570
train acc:  0.7734375
train loss:  0.5027368068695068
train gradient:  0.1192885307008746
iteration : 10571
train acc:  0.78125
train loss:  0.4990386962890625
train gradient:  0.1611635433850429
iteration : 10572
train acc:  0.765625
train loss:  0.4610230028629303
train gradient:  0.14230566047435972
iteration : 10573
train acc:  0.7734375
train loss:  0.44461768865585327
train gradient:  0.0995318215781273
iteration : 10574
train acc:  0.75
train loss:  0.45937544107437134
train gradient:  0.09260519583107969
iteration : 10575
train acc:  0.6953125
train loss:  0.5113301873207092
train gradient:  0.12354379643449588
iteration : 10576
train acc:  0.734375
train loss:  0.5012732744216919
train gradient:  0.14003110775006716
iteration : 10577
train acc:  0.7265625
train loss:  0.5198033452033997
train gradient:  0.12707190173252522
iteration : 10578
train acc:  0.7421875
train loss:  0.5003309845924377
train gradient:  0.11712805500081863
iteration : 10579
train acc:  0.828125
train loss:  0.4032762050628662
train gradient:  0.093523126518642
iteration : 10580
train acc:  0.7421875
train loss:  0.5089064836502075
train gradient:  0.10874295046323397
iteration : 10581
train acc:  0.6796875
train loss:  0.5059312582015991
train gradient:  0.11109264431543876
iteration : 10582
train acc:  0.71875
train loss:  0.508385181427002
train gradient:  0.12494794681715415
iteration : 10583
train acc:  0.7734375
train loss:  0.44671839475631714
train gradient:  0.07387658185080268
iteration : 10584
train acc:  0.78125
train loss:  0.4536672830581665
train gradient:  0.1286452596325654
iteration : 10585
train acc:  0.7421875
train loss:  0.47971418499946594
train gradient:  0.11400115539233056
iteration : 10586
train acc:  0.8125
train loss:  0.4625518321990967
train gradient:  0.10469714990745009
iteration : 10587
train acc:  0.734375
train loss:  0.4539257884025574
train gradient:  0.11816961287954865
iteration : 10588
train acc:  0.7734375
train loss:  0.46185797452926636
train gradient:  0.11242047951116012
iteration : 10589
train acc:  0.8046875
train loss:  0.47221848368644714
train gradient:  0.13161407513715764
iteration : 10590
train acc:  0.6328125
train loss:  0.5897641777992249
train gradient:  0.21223682633038385
iteration : 10591
train acc:  0.7109375
train loss:  0.5354552865028381
train gradient:  0.14347101260938128
iteration : 10592
train acc:  0.7109375
train loss:  0.510026216506958
train gradient:  0.11951594292475912
iteration : 10593
train acc:  0.75
train loss:  0.4600825309753418
train gradient:  0.12289642279984929
iteration : 10594
train acc:  0.8125
train loss:  0.439419686794281
train gradient:  0.10315284045513252
iteration : 10595
train acc:  0.7109375
train loss:  0.5410670042037964
train gradient:  0.16572654132081294
iteration : 10596
train acc:  0.7265625
train loss:  0.4726988971233368
train gradient:  0.1268398173559489
iteration : 10597
train acc:  0.7578125
train loss:  0.4982377290725708
train gradient:  0.17063297701603086
iteration : 10598
train acc:  0.7265625
train loss:  0.533299446105957
train gradient:  0.10833196363858248
iteration : 10599
train acc:  0.7734375
train loss:  0.5056999921798706
train gradient:  0.15563329819948798
iteration : 10600
train acc:  0.6875
train loss:  0.6191787123680115
train gradient:  0.15664231188300431
iteration : 10601
train acc:  0.734375
train loss:  0.462321937084198
train gradient:  0.12336288014326982
iteration : 10602
train acc:  0.7578125
train loss:  0.4663239121437073
train gradient:  0.10555564940856842
iteration : 10603
train acc:  0.765625
train loss:  0.4964097738265991
train gradient:  0.12374695481590757
iteration : 10604
train acc:  0.7265625
train loss:  0.5087219476699829
train gradient:  0.1247598479196703
iteration : 10605
train acc:  0.71875
train loss:  0.5086541175842285
train gradient:  0.13247657980261374
iteration : 10606
train acc:  0.6875
train loss:  0.5261189937591553
train gradient:  0.13537734799913098
iteration : 10607
train acc:  0.765625
train loss:  0.4993291199207306
train gradient:  0.13232093163904768
iteration : 10608
train acc:  0.734375
train loss:  0.5221356749534607
train gradient:  0.12299315410575522
iteration : 10609
train acc:  0.71875
train loss:  0.48822179436683655
train gradient:  0.13560386443291159
iteration : 10610
train acc:  0.6953125
train loss:  0.5656442642211914
train gradient:  0.1415080401176228
iteration : 10611
train acc:  0.703125
train loss:  0.5244385600090027
train gradient:  0.1318543789929985
iteration : 10612
train acc:  0.7734375
train loss:  0.4606204926967621
train gradient:  0.130712497188874
iteration : 10613
train acc:  0.7734375
train loss:  0.4877625107765198
train gradient:  0.1116741339966167
iteration : 10614
train acc:  0.78125
train loss:  0.4383808672428131
train gradient:  0.11799815829846788
iteration : 10615
train acc:  0.7265625
train loss:  0.5183959007263184
train gradient:  0.13243988395515455
iteration : 10616
train acc:  0.8125
train loss:  0.47273150086402893
train gradient:  0.14200931954202106
iteration : 10617
train acc:  0.7265625
train loss:  0.5109643936157227
train gradient:  0.13656933912582764
iteration : 10618
train acc:  0.734375
train loss:  0.5093055963516235
train gradient:  0.1280001513618301
iteration : 10619
train acc:  0.7421875
train loss:  0.47427570819854736
train gradient:  0.12592451731421664
iteration : 10620
train acc:  0.796875
train loss:  0.4473876357078552
train gradient:  0.09844451537314361
iteration : 10621
train acc:  0.765625
train loss:  0.4349552392959595
train gradient:  0.12770821883396088
iteration : 10622
train acc:  0.75
train loss:  0.4996037483215332
train gradient:  0.11511312408665701
iteration : 10623
train acc:  0.71875
train loss:  0.4853823184967041
train gradient:  0.1102464691047503
iteration : 10624
train acc:  0.7734375
train loss:  0.4570886492729187
train gradient:  0.10971656556087901
iteration : 10625
train acc:  0.7265625
train loss:  0.4904707670211792
train gradient:  0.13460542230299702
iteration : 10626
train acc:  0.7421875
train loss:  0.4706066846847534
train gradient:  0.10766165049093142
iteration : 10627
train acc:  0.8125
train loss:  0.47419944405555725
train gradient:  0.167046903164569
iteration : 10628
train acc:  0.6484375
train loss:  0.5725007653236389
train gradient:  0.14455027691751646
iteration : 10629
train acc:  0.7578125
train loss:  0.4551653265953064
train gradient:  0.10355710860812814
iteration : 10630
train acc:  0.8671875
train loss:  0.3838295638561249
train gradient:  0.09311353399549258
iteration : 10631
train acc:  0.859375
train loss:  0.3670088052749634
train gradient:  0.10248919047589618
iteration : 10632
train acc:  0.671875
train loss:  0.5495132803916931
train gradient:  0.15085266904469338
iteration : 10633
train acc:  0.6953125
train loss:  0.5416141152381897
train gradient:  0.1147538361062158
iteration : 10634
train acc:  0.734375
train loss:  0.5163048505783081
train gradient:  0.14175036654845363
iteration : 10635
train acc:  0.765625
train loss:  0.43481624126434326
train gradient:  0.09384452214525629
iteration : 10636
train acc:  0.7109375
train loss:  0.5366520285606384
train gradient:  0.12603292312122974
iteration : 10637
train acc:  0.7421875
train loss:  0.5520753860473633
train gradient:  0.14098522102003783
iteration : 10638
train acc:  0.75
train loss:  0.4422968029975891
train gradient:  0.10919041438723714
iteration : 10639
train acc:  0.75
train loss:  0.4972437024116516
train gradient:  0.10133382588801414
iteration : 10640
train acc:  0.7578125
train loss:  0.5193919539451599
train gradient:  0.13604806112940848
iteration : 10641
train acc:  0.7734375
train loss:  0.5092485547065735
train gradient:  0.14900437089493412
iteration : 10642
train acc:  0.7265625
train loss:  0.5246835947036743
train gradient:  0.12714826250635425
iteration : 10643
train acc:  0.7890625
train loss:  0.4732201397418976
train gradient:  0.11886902525743735
iteration : 10644
train acc:  0.6875
train loss:  0.5212973952293396
train gradient:  0.1342551955186187
iteration : 10645
train acc:  0.765625
train loss:  0.45031633973121643
train gradient:  0.08969572957261016
iteration : 10646
train acc:  0.7578125
train loss:  0.46525776386260986
train gradient:  0.07854437673942566
iteration : 10647
train acc:  0.6875
train loss:  0.48054927587509155
train gradient:  0.13332324127934625
iteration : 10648
train acc:  0.828125
train loss:  0.396875262260437
train gradient:  0.07316969543970367
iteration : 10649
train acc:  0.703125
train loss:  0.5412701368331909
train gradient:  0.13068176066996623
iteration : 10650
train acc:  0.7734375
train loss:  0.481342077255249
train gradient:  0.1388515404527239
iteration : 10651
train acc:  0.7421875
train loss:  0.5249147415161133
train gradient:  0.13715647436960493
iteration : 10652
train acc:  0.8125
train loss:  0.43806225061416626
train gradient:  0.09746202790540621
iteration : 10653
train acc:  0.8046875
train loss:  0.4426535367965698
train gradient:  0.10715767210021879
iteration : 10654
train acc:  0.734375
train loss:  0.45863205194473267
train gradient:  0.11788376783535703
iteration : 10655
train acc:  0.7109375
train loss:  0.5372000932693481
train gradient:  0.16193945122036418
iteration : 10656
train acc:  0.7265625
train loss:  0.5069116950035095
train gradient:  0.1228024438686824
iteration : 10657
train acc:  0.7421875
train loss:  0.5020383596420288
train gradient:  0.12625884688283923
iteration : 10658
train acc:  0.7578125
train loss:  0.49372047185897827
train gradient:  0.13986453796516551
iteration : 10659
train acc:  0.75
train loss:  0.48102185130119324
train gradient:  0.10169172384406168
iteration : 10660
train acc:  0.7734375
train loss:  0.48062756657600403
train gradient:  0.08863961519479317
iteration : 10661
train acc:  0.7109375
train loss:  0.513043999671936
train gradient:  0.11251816542316048
iteration : 10662
train acc:  0.7265625
train loss:  0.5360374450683594
train gradient:  0.14662907609346415
iteration : 10663
train acc:  0.71875
train loss:  0.5087437033653259
train gradient:  0.1284312331565619
iteration : 10664
train acc:  0.765625
train loss:  0.46819430589675903
train gradient:  0.1088450059268308
iteration : 10665
train acc:  0.796875
train loss:  0.461928129196167
train gradient:  0.12336027524614857
iteration : 10666
train acc:  0.734375
train loss:  0.5360393524169922
train gradient:  0.14833569695124196
iteration : 10667
train acc:  0.7421875
train loss:  0.4729199707508087
train gradient:  0.15289836976783797
iteration : 10668
train acc:  0.7109375
train loss:  0.47940659523010254
train gradient:  0.12463616359835683
iteration : 10669
train acc:  0.734375
train loss:  0.43570613861083984
train gradient:  0.09294165844111375
iteration : 10670
train acc:  0.6484375
train loss:  0.5819284915924072
train gradient:  0.18992211037862294
iteration : 10671
train acc:  0.8046875
train loss:  0.42033618688583374
train gradient:  0.09997404263921082
iteration : 10672
train acc:  0.7890625
train loss:  0.43110671639442444
train gradient:  0.09119041617804295
iteration : 10673
train acc:  0.78125
train loss:  0.4908714294433594
train gradient:  0.12359261413487242
iteration : 10674
train acc:  0.78125
train loss:  0.4360805153846741
train gradient:  0.11213759308464118
iteration : 10675
train acc:  0.78125
train loss:  0.45137226581573486
train gradient:  0.12353018981711546
iteration : 10676
train acc:  0.78125
train loss:  0.4812327027320862
train gradient:  0.10905508076890448
iteration : 10677
train acc:  0.765625
train loss:  0.4672405421733856
train gradient:  0.11647166706155206
iteration : 10678
train acc:  0.75
train loss:  0.46772637963294983
train gradient:  0.0962169910889739
iteration : 10679
train acc:  0.7109375
train loss:  0.5292617082595825
train gradient:  0.12234130174150235
iteration : 10680
train acc:  0.7109375
train loss:  0.5360414981842041
train gradient:  0.1314399116499067
iteration : 10681
train acc:  0.6875
train loss:  0.5275624990463257
train gradient:  0.11787211362755005
iteration : 10682
train acc:  0.7421875
train loss:  0.4564688801765442
train gradient:  0.11750009244609204
iteration : 10683
train acc:  0.734375
train loss:  0.5292026400566101
train gradient:  0.13502807576839848
iteration : 10684
train acc:  0.6484375
train loss:  0.5428997278213501
train gradient:  0.1238529206211071
iteration : 10685
train acc:  0.765625
train loss:  0.4791969954967499
train gradient:  0.12195070249238202
iteration : 10686
train acc:  0.75
train loss:  0.5464919805526733
train gradient:  0.17897345958795088
iteration : 10687
train acc:  0.765625
train loss:  0.48533910512924194
train gradient:  0.11273185375577274
iteration : 10688
train acc:  0.7421875
train loss:  0.48723334074020386
train gradient:  0.12360450003625066
iteration : 10689
train acc:  0.75
train loss:  0.5205233693122864
train gradient:  0.12366497912908715
iteration : 10690
train acc:  0.75
train loss:  0.48091232776641846
train gradient:  0.09745275267548457
iteration : 10691
train acc:  0.7890625
train loss:  0.4531494081020355
train gradient:  0.13130282783899783
iteration : 10692
train acc:  0.6640625
train loss:  0.5555676221847534
train gradient:  0.13044876116712406
iteration : 10693
train acc:  0.7890625
train loss:  0.4618825316429138
train gradient:  0.10387166380910504
iteration : 10694
train acc:  0.75
train loss:  0.5057568550109863
train gradient:  0.1528768991264759
iteration : 10695
train acc:  0.75
train loss:  0.5010354518890381
train gradient:  0.12511004061007347
iteration : 10696
train acc:  0.75
train loss:  0.5204251408576965
train gradient:  0.14445917677703046
iteration : 10697
train acc:  0.7265625
train loss:  0.4879707992076874
train gradient:  0.12824253325253404
iteration : 10698
train acc:  0.7578125
train loss:  0.48184555768966675
train gradient:  0.10962951241520777
iteration : 10699
train acc:  0.7734375
train loss:  0.4882517158985138
train gradient:  0.14157935439148167
iteration : 10700
train acc:  0.7265625
train loss:  0.5010210275650024
train gradient:  0.12519601069654818
iteration : 10701
train acc:  0.65625
train loss:  0.5800824165344238
train gradient:  0.1392846030027088
iteration : 10702
train acc:  0.71875
train loss:  0.5031884908676147
train gradient:  0.09942234399093554
iteration : 10703
train acc:  0.671875
train loss:  0.5284304022789001
train gradient:  0.10064635595761968
iteration : 10704
train acc:  0.7109375
train loss:  0.508359968662262
train gradient:  0.14618467722568906
iteration : 10705
train acc:  0.7890625
train loss:  0.4399947226047516
train gradient:  0.09796115628030422
iteration : 10706
train acc:  0.7265625
train loss:  0.45575982332229614
train gradient:  0.11121941201737129
iteration : 10707
train acc:  0.71875
train loss:  0.5151132345199585
train gradient:  0.12532819201541284
iteration : 10708
train acc:  0.75
train loss:  0.4920812249183655
train gradient:  0.11463219159657266
iteration : 10709
train acc:  0.703125
train loss:  0.5298352241516113
train gradient:  0.1322918154622728
iteration : 10710
train acc:  0.6953125
train loss:  0.5505395531654358
train gradient:  0.16361868210350608
iteration : 10711
train acc:  0.796875
train loss:  0.43650996685028076
train gradient:  0.10243040091206537
iteration : 10712
train acc:  0.6796875
train loss:  0.5250919461250305
train gradient:  0.14980591336216642
iteration : 10713
train acc:  0.78125
train loss:  0.4742945432662964
train gradient:  0.09628225202470041
iteration : 10714
train acc:  0.7734375
train loss:  0.43307167291641235
train gradient:  0.11696365662452674
iteration : 10715
train acc:  0.734375
train loss:  0.47990021109580994
train gradient:  0.12205784743955478
iteration : 10716
train acc:  0.75
train loss:  0.47084537148475647
train gradient:  0.1120368433871592
iteration : 10717
train acc:  0.71875
train loss:  0.5184240341186523
train gradient:  0.1273554672236355
iteration : 10718
train acc:  0.765625
train loss:  0.4773481786251068
train gradient:  0.0993759306252541
iteration : 10719
train acc:  0.7890625
train loss:  0.4176064133644104
train gradient:  0.13337187817560736
iteration : 10720
train acc:  0.734375
train loss:  0.5301635265350342
train gradient:  0.1547501544157462
iteration : 10721
train acc:  0.765625
train loss:  0.49525344371795654
train gradient:  0.11474236204177053
iteration : 10722
train acc:  0.6875
train loss:  0.6181998252868652
train gradient:  0.18334172308360291
iteration : 10723
train acc:  0.7890625
train loss:  0.4446127116680145
train gradient:  0.11377665246202222
iteration : 10724
train acc:  0.7578125
train loss:  0.47697269916534424
train gradient:  0.12168759381410485
iteration : 10725
train acc:  0.71875
train loss:  0.5094727277755737
train gradient:  0.13971510924811106
iteration : 10726
train acc:  0.7890625
train loss:  0.44251883029937744
train gradient:  0.08785229043155332
iteration : 10727
train acc:  0.7890625
train loss:  0.4690646231174469
train gradient:  0.10351366354413463
iteration : 10728
train acc:  0.734375
train loss:  0.4950723648071289
train gradient:  0.13335143426622673
iteration : 10729
train acc:  0.703125
train loss:  0.5492393970489502
train gradient:  0.14558266607336834
iteration : 10730
train acc:  0.671875
train loss:  0.6096667051315308
train gradient:  0.15971277138747633
iteration : 10731
train acc:  0.7734375
train loss:  0.4741606116294861
train gradient:  0.12305675204204646
iteration : 10732
train acc:  0.75
train loss:  0.4701313078403473
train gradient:  0.12166772069452046
iteration : 10733
train acc:  0.6953125
train loss:  0.5319560170173645
train gradient:  0.14187984216354146
iteration : 10734
train acc:  0.8203125
train loss:  0.40518009662628174
train gradient:  0.09297779241676918
iteration : 10735
train acc:  0.71875
train loss:  0.48291656374931335
train gradient:  0.13875394657498263
iteration : 10736
train acc:  0.78125
train loss:  0.4493095278739929
train gradient:  0.09568038394638344
iteration : 10737
train acc:  0.7578125
train loss:  0.4722774624824524
train gradient:  0.10674829883495641
iteration : 10738
train acc:  0.7421875
train loss:  0.508243203163147
train gradient:  0.11664282032759253
iteration : 10739
train acc:  0.734375
train loss:  0.5280297994613647
train gradient:  0.13710242286259774
iteration : 10740
train acc:  0.6953125
train loss:  0.5737230777740479
train gradient:  0.1976206290750848
iteration : 10741
train acc:  0.734375
train loss:  0.5394755601882935
train gradient:  0.13268729654150074
iteration : 10742
train acc:  0.7734375
train loss:  0.46848994493484497
train gradient:  0.10670359751865367
iteration : 10743
train acc:  0.75
train loss:  0.5009873509407043
train gradient:  0.11336550889235093
iteration : 10744
train acc:  0.6796875
train loss:  0.5396424531936646
train gradient:  0.19394143746983156
iteration : 10745
train acc:  0.7421875
train loss:  0.46275144815444946
train gradient:  0.09512107236459792
iteration : 10746
train acc:  0.7890625
train loss:  0.43452227115631104
train gradient:  0.10993122961403927
iteration : 10747
train acc:  0.7734375
train loss:  0.46324658393859863
train gradient:  0.13258704375721142
iteration : 10748
train acc:  0.6796875
train loss:  0.6108291149139404
train gradient:  0.20873316791356888
iteration : 10749
train acc:  0.7265625
train loss:  0.576951265335083
train gradient:  0.12380153989602014
iteration : 10750
train acc:  0.7265625
train loss:  0.5105053186416626
train gradient:  0.14549021722679892
iteration : 10751
train acc:  0.7109375
train loss:  0.5002541542053223
train gradient:  0.12708332577632997
iteration : 10752
train acc:  0.734375
train loss:  0.5031147599220276
train gradient:  0.11697493297312846
iteration : 10753
train acc:  0.7578125
train loss:  0.53293776512146
train gradient:  0.11671058085112498
iteration : 10754
train acc:  0.703125
train loss:  0.5188175439834595
train gradient:  0.17123594266020903
iteration : 10755
train acc:  0.78125
train loss:  0.4700324535369873
train gradient:  0.09862266942017564
iteration : 10756
train acc:  0.734375
train loss:  0.473092257976532
train gradient:  0.10470551463625861
iteration : 10757
train acc:  0.7890625
train loss:  0.45145636796951294
train gradient:  0.08693604535779546
iteration : 10758
train acc:  0.7421875
train loss:  0.49695050716400146
train gradient:  0.11655580522467408
iteration : 10759
train acc:  0.7265625
train loss:  0.5108222961425781
train gradient:  0.13156167034616123
iteration : 10760
train acc:  0.7109375
train loss:  0.5380653142929077
train gradient:  0.12662515626936255
iteration : 10761
train acc:  0.7890625
train loss:  0.49063897132873535
train gradient:  0.14916432896524434
iteration : 10762
train acc:  0.78125
train loss:  0.4569326639175415
train gradient:  0.11766105106920176
iteration : 10763
train acc:  0.6953125
train loss:  0.5882794260978699
train gradient:  0.15544886779602396
iteration : 10764
train acc:  0.75
train loss:  0.4750346541404724
train gradient:  0.1221895221675661
iteration : 10765
train acc:  0.8203125
train loss:  0.4134940505027771
train gradient:  0.08956696040146983
iteration : 10766
train acc:  0.7578125
train loss:  0.4782724976539612
train gradient:  0.11842733505114074
iteration : 10767
train acc:  0.7265625
train loss:  0.48340776562690735
train gradient:  0.11092085510203278
iteration : 10768
train acc:  0.765625
train loss:  0.47360658645629883
train gradient:  0.1326149776350296
iteration : 10769
train acc:  0.703125
train loss:  0.5410493016242981
train gradient:  0.13037807779536406
iteration : 10770
train acc:  0.7109375
train loss:  0.5047478079795837
train gradient:  0.10913255087059538
iteration : 10771
train acc:  0.6953125
train loss:  0.5178138017654419
train gradient:  0.11600499754591448
iteration : 10772
train acc:  0.7421875
train loss:  0.44470033049583435
train gradient:  0.11884020277770556
iteration : 10773
train acc:  0.703125
train loss:  0.5054821968078613
train gradient:  0.11168563971710192
iteration : 10774
train acc:  0.6875
train loss:  0.4964793920516968
train gradient:  0.1319162606037133
iteration : 10775
train acc:  0.765625
train loss:  0.48335862159729004
train gradient:  0.11364995145761873
iteration : 10776
train acc:  0.71875
train loss:  0.536245584487915
train gradient:  0.15043098260907983
iteration : 10777
train acc:  0.765625
train loss:  0.5173219442367554
train gradient:  0.13264598171491035
iteration : 10778
train acc:  0.7890625
train loss:  0.46206632256507874
train gradient:  0.09587363200375763
iteration : 10779
train acc:  0.6875
train loss:  0.5681217908859253
train gradient:  0.14037105197161587
iteration : 10780
train acc:  0.7109375
train loss:  0.5606834888458252
train gradient:  0.14545891573506323
iteration : 10781
train acc:  0.7578125
train loss:  0.4903258681297302
train gradient:  0.15649973280317203
iteration : 10782
train acc:  0.84375
train loss:  0.4070664644241333
train gradient:  0.07358769578272745
iteration : 10783
train acc:  0.7734375
train loss:  0.45429176092147827
train gradient:  0.10703791233923703
iteration : 10784
train acc:  0.734375
train loss:  0.47945636510849
train gradient:  0.10919446253105862
iteration : 10785
train acc:  0.7109375
train loss:  0.47665491700172424
train gradient:  0.12016637194387055
iteration : 10786
train acc:  0.7109375
train loss:  0.515201985836029
train gradient:  0.15964122301439068
iteration : 10787
train acc:  0.7265625
train loss:  0.5648945569992065
train gradient:  0.18143186420495516
iteration : 10788
train acc:  0.75
train loss:  0.47749409079551697
train gradient:  0.1251541003095201
iteration : 10789
train acc:  0.7265625
train loss:  0.4780065417289734
train gradient:  0.09608812687675658
iteration : 10790
train acc:  0.7578125
train loss:  0.4230407476425171
train gradient:  0.07724904516871205
iteration : 10791
train acc:  0.703125
train loss:  0.5145771503448486
train gradient:  0.11233434623246187
iteration : 10792
train acc:  0.796875
train loss:  0.48132362961769104
train gradient:  0.10849134665701109
iteration : 10793
train acc:  0.7890625
train loss:  0.4283837080001831
train gradient:  0.09266265456537336
iteration : 10794
train acc:  0.75
train loss:  0.4532984793186188
train gradient:  0.08831557434427315
iteration : 10795
train acc:  0.7578125
train loss:  0.47869086265563965
train gradient:  0.09874051676818286
iteration : 10796
train acc:  0.6640625
train loss:  0.5856473445892334
train gradient:  0.14111249651870392
iteration : 10797
train acc:  0.7109375
train loss:  0.49675238132476807
train gradient:  0.1540225305760417
iteration : 10798
train acc:  0.78125
train loss:  0.4681693911552429
train gradient:  0.1320980961770786
iteration : 10799
train acc:  0.703125
train loss:  0.49886488914489746
train gradient:  0.13865883333303425
iteration : 10800
train acc:  0.7109375
train loss:  0.537406325340271
train gradient:  0.137218719528781
iteration : 10801
train acc:  0.765625
train loss:  0.43778449296951294
train gradient:  0.1143377976171111
iteration : 10802
train acc:  0.7265625
train loss:  0.5024054050445557
train gradient:  0.0992413496265508
iteration : 10803
train acc:  0.7578125
train loss:  0.45738857984542847
train gradient:  0.10423208218507048
iteration : 10804
train acc:  0.6875
train loss:  0.5470524430274963
train gradient:  0.17087009813760856
iteration : 10805
train acc:  0.71875
train loss:  0.47656580805778503
train gradient:  0.10126845916697502
iteration : 10806
train acc:  0.7734375
train loss:  0.5374230146408081
train gradient:  0.14017233958527975
iteration : 10807
train acc:  0.7109375
train loss:  0.46985936164855957
train gradient:  0.13807455779621225
iteration : 10808
train acc:  0.75
train loss:  0.47518840432167053
train gradient:  0.12676621682890485
iteration : 10809
train acc:  0.703125
train loss:  0.4991292953491211
train gradient:  0.09767315465034922
iteration : 10810
train acc:  0.7265625
train loss:  0.46659207344055176
train gradient:  0.11470373368120637
iteration : 10811
train acc:  0.7265625
train loss:  0.5491432547569275
train gradient:  0.11831916945688023
iteration : 10812
train acc:  0.7734375
train loss:  0.46658986806869507
train gradient:  0.11080996361848902
iteration : 10813
train acc:  0.765625
train loss:  0.45882296562194824
train gradient:  0.10138004507410502
iteration : 10814
train acc:  0.7265625
train loss:  0.5604605674743652
train gradient:  0.19705416626674913
iteration : 10815
train acc:  0.7578125
train loss:  0.4973706305027008
train gradient:  0.11823543183046742
iteration : 10816
train acc:  0.828125
train loss:  0.41249117255210876
train gradient:  0.09857860407132057
iteration : 10817
train acc:  0.7109375
train loss:  0.5465760231018066
train gradient:  0.11929658682219804
iteration : 10818
train acc:  0.765625
train loss:  0.4727097749710083
train gradient:  0.11535721798764896
iteration : 10819
train acc:  0.703125
train loss:  0.5011886358261108
train gradient:  0.10700597725344033
iteration : 10820
train acc:  0.7578125
train loss:  0.5179590582847595
train gradient:  0.09650042071960675
iteration : 10821
train acc:  0.7890625
train loss:  0.4692937731742859
train gradient:  0.15399050279068233
iteration : 10822
train acc:  0.765625
train loss:  0.4427512288093567
train gradient:  0.0947509163402521
iteration : 10823
train acc:  0.765625
train loss:  0.47118228673934937
train gradient:  0.10382104163051593
iteration : 10824
train acc:  0.7734375
train loss:  0.5034855604171753
train gradient:  0.12660052748256057
iteration : 10825
train acc:  0.6875
train loss:  0.5546705722808838
train gradient:  0.13745681178533375
iteration : 10826
train acc:  0.734375
train loss:  0.5163180828094482
train gradient:  0.15801519278732049
iteration : 10827
train acc:  0.734375
train loss:  0.4973623752593994
train gradient:  0.12071953556910596
iteration : 10828
train acc:  0.703125
train loss:  0.5600959658622742
train gradient:  0.11122799659113902
iteration : 10829
train acc:  0.7421875
train loss:  0.49308526515960693
train gradient:  0.1305869897539942
iteration : 10830
train acc:  0.7890625
train loss:  0.4361262917518616
train gradient:  0.11123953847404225
iteration : 10831
train acc:  0.734375
train loss:  0.46217450499534607
train gradient:  0.09504110129808355
iteration : 10832
train acc:  0.765625
train loss:  0.4750356674194336
train gradient:  0.09997970113760725
iteration : 10833
train acc:  0.7734375
train loss:  0.45870649814605713
train gradient:  0.09694568153386846
iteration : 10834
train acc:  0.7734375
train loss:  0.44870060682296753
train gradient:  0.09608270338603163
iteration : 10835
train acc:  0.6796875
train loss:  0.557222843170166
train gradient:  0.15719678961596972
iteration : 10836
train acc:  0.7109375
train loss:  0.4854743182659149
train gradient:  0.1013210264622649
iteration : 10837
train acc:  0.78125
train loss:  0.46935462951660156
train gradient:  0.14878703789085296
iteration : 10838
train acc:  0.7265625
train loss:  0.4753844439983368
train gradient:  0.09385099714572842
iteration : 10839
train acc:  0.7890625
train loss:  0.4568749666213989
train gradient:  0.09908085336990342
iteration : 10840
train acc:  0.7421875
train loss:  0.48963087797164917
train gradient:  0.11752955488739875
iteration : 10841
train acc:  0.7265625
train loss:  0.5228945016860962
train gradient:  0.10690992110113695
iteration : 10842
train acc:  0.71875
train loss:  0.47905731201171875
train gradient:  0.11576693033837313
iteration : 10843
train acc:  0.7421875
train loss:  0.4610595107078552
train gradient:  0.11684976659509667
iteration : 10844
train acc:  0.7578125
train loss:  0.4610346555709839
train gradient:  0.08767201725902017
iteration : 10845
train acc:  0.7265625
train loss:  0.5097612142562866
train gradient:  0.11182872503713343
iteration : 10846
train acc:  0.7578125
train loss:  0.49325665831565857
train gradient:  0.19489037226538258
iteration : 10847
train acc:  0.765625
train loss:  0.45065340399742126
train gradient:  0.09741468113669599
iteration : 10848
train acc:  0.765625
train loss:  0.471681147813797
train gradient:  0.12308477192018459
iteration : 10849
train acc:  0.7421875
train loss:  0.5354080200195312
train gradient:  0.17036652468427782
iteration : 10850
train acc:  0.671875
train loss:  0.5066150426864624
train gradient:  0.10975123438583163
iteration : 10851
train acc:  0.6953125
train loss:  0.5098974704742432
train gradient:  0.12663206257761656
iteration : 10852
train acc:  0.71875
train loss:  0.5487504005432129
train gradient:  0.135762516097914
iteration : 10853
train acc:  0.7109375
train loss:  0.5497456789016724
train gradient:  0.15355498960931652
iteration : 10854
train acc:  0.7578125
train loss:  0.45887690782546997
train gradient:  0.09765278244846295
iteration : 10855
train acc:  0.6953125
train loss:  0.5705349445343018
train gradient:  0.1593308497732518
iteration : 10856
train acc:  0.796875
train loss:  0.39377737045288086
train gradient:  0.09291312120390254
iteration : 10857
train acc:  0.734375
train loss:  0.48376190662384033
train gradient:  0.10587428946544769
iteration : 10858
train acc:  0.6796875
train loss:  0.48520976305007935
train gradient:  0.1370692641237613
iteration : 10859
train acc:  0.7578125
train loss:  0.4548119306564331
train gradient:  0.10877843298678629
iteration : 10860
train acc:  0.703125
train loss:  0.5488802790641785
train gradient:  0.14181900114759577
iteration : 10861
train acc:  0.8203125
train loss:  0.38933297991752625
train gradient:  0.0822126620805182
iteration : 10862
train acc:  0.78125
train loss:  0.44250011444091797
train gradient:  0.10396259499617967
iteration : 10863
train acc:  0.6875
train loss:  0.5145724415779114
train gradient:  0.10117993856290995
iteration : 10864
train acc:  0.734375
train loss:  0.5032252073287964
train gradient:  0.15286835155934175
iteration : 10865
train acc:  0.7421875
train loss:  0.4898838400840759
train gradient:  0.1070574534689367
iteration : 10866
train acc:  0.71875
train loss:  0.5355864763259888
train gradient:  0.12928868980236013
iteration : 10867
train acc:  0.78125
train loss:  0.4536738395690918
train gradient:  0.09331793236842204
iteration : 10868
train acc:  0.7109375
train loss:  0.5251990556716919
train gradient:  0.1167937901575934
iteration : 10869
train acc:  0.765625
train loss:  0.45931747555732727
train gradient:  0.09701105246321058
iteration : 10870
train acc:  0.6484375
train loss:  0.5365728139877319
train gradient:  0.11945595288432109
iteration : 10871
train acc:  0.828125
train loss:  0.4203556180000305
train gradient:  0.09441074730831205
iteration : 10872
train acc:  0.7109375
train loss:  0.5624783039093018
train gradient:  0.16959264020881287
iteration : 10873
train acc:  0.6796875
train loss:  0.5468862056732178
train gradient:  0.12633385524384294
iteration : 10874
train acc:  0.7265625
train loss:  0.5096507668495178
train gradient:  0.11781276564370578
iteration : 10875
train acc:  0.7578125
train loss:  0.4576718211174011
train gradient:  0.10957506538669746
iteration : 10876
train acc:  0.7734375
train loss:  0.44663557410240173
train gradient:  0.11341977003022571
iteration : 10877
train acc:  0.71875
train loss:  0.49633103609085083
train gradient:  0.12914009408116578
iteration : 10878
train acc:  0.71875
train loss:  0.5393218994140625
train gradient:  0.16781445661378605
iteration : 10879
train acc:  0.765625
train loss:  0.4672946333885193
train gradient:  0.10283875389903407
iteration : 10880
train acc:  0.7265625
train loss:  0.4799312353134155
train gradient:  0.11891413540149841
iteration : 10881
train acc:  0.75
train loss:  0.49958688020706177
train gradient:  0.11774032460118418
iteration : 10882
train acc:  0.7421875
train loss:  0.4924737215042114
train gradient:  0.1589318339408248
iteration : 10883
train acc:  0.7578125
train loss:  0.5093121528625488
train gradient:  0.11815812926768934
iteration : 10884
train acc:  0.7421875
train loss:  0.48171812295913696
train gradient:  0.1205133225615935
iteration : 10885
train acc:  0.71875
train loss:  0.597224771976471
train gradient:  0.2021551713739797
iteration : 10886
train acc:  0.7421875
train loss:  0.47360092401504517
train gradient:  0.10979830580359473
iteration : 10887
train acc:  0.703125
train loss:  0.5556018948554993
train gradient:  0.19121109746966247
iteration : 10888
train acc:  0.734375
train loss:  0.47987616062164307
train gradient:  0.1369376228172282
iteration : 10889
train acc:  0.765625
train loss:  0.48927244544029236
train gradient:  0.11156424054903447
iteration : 10890
train acc:  0.78125
train loss:  0.49346497654914856
train gradient:  0.14503060538231116
iteration : 10891
train acc:  0.7109375
train loss:  0.5175255537033081
train gradient:  0.1374974267994966
iteration : 10892
train acc:  0.7578125
train loss:  0.4977928400039673
train gradient:  0.1601016816616873
iteration : 10893
train acc:  0.78125
train loss:  0.4626425504684448
train gradient:  0.12046406369249751
iteration : 10894
train acc:  0.6875
train loss:  0.5789626240730286
train gradient:  0.15700898705962574
iteration : 10895
train acc:  0.671875
train loss:  0.5285180807113647
train gradient:  0.1513395941718315
iteration : 10896
train acc:  0.7734375
train loss:  0.4183887839317322
train gradient:  0.08063836154778482
iteration : 10897
train acc:  0.78125
train loss:  0.4625243544578552
train gradient:  0.12399464866711996
iteration : 10898
train acc:  0.8203125
train loss:  0.44235843420028687
train gradient:  0.10912487167686315
iteration : 10899
train acc:  0.7890625
train loss:  0.4720062017440796
train gradient:  0.11802413361208543
iteration : 10900
train acc:  0.7109375
train loss:  0.5427602529525757
train gradient:  0.15276351278722225
iteration : 10901
train acc:  0.734375
train loss:  0.47656679153442383
train gradient:  0.12035260770628219
iteration : 10902
train acc:  0.7265625
train loss:  0.4772301912307739
train gradient:  0.10009625442243436
iteration : 10903
train acc:  0.7578125
train loss:  0.46605122089385986
train gradient:  0.10025907162187375
iteration : 10904
train acc:  0.7734375
train loss:  0.4954516589641571
train gradient:  0.11163214485795576
iteration : 10905
train acc:  0.765625
train loss:  0.4909079074859619
train gradient:  0.10480423339184164
iteration : 10906
train acc:  0.7265625
train loss:  0.5349088311195374
train gradient:  0.12214794610718235
iteration : 10907
train acc:  0.65625
train loss:  0.6103694438934326
train gradient:  0.16375540961802373
iteration : 10908
train acc:  0.6953125
train loss:  0.5232354402542114
train gradient:  0.1367934636796173
iteration : 10909
train acc:  0.6796875
train loss:  0.5554718971252441
train gradient:  0.137722824701842
iteration : 10910
train acc:  0.7109375
train loss:  0.5199247598648071
train gradient:  0.13776743954203238
iteration : 10911
train acc:  0.7421875
train loss:  0.5146085619926453
train gradient:  0.12385962402909209
iteration : 10912
train acc:  0.7578125
train loss:  0.48024308681488037
train gradient:  0.1276627229162844
iteration : 10913
train acc:  0.71875
train loss:  0.5595183372497559
train gradient:  0.17669843637826665
iteration : 10914
train acc:  0.71875
train loss:  0.49417680501937866
train gradient:  0.12265018863074754
iteration : 10915
train acc:  0.7265625
train loss:  0.4428118169307709
train gradient:  0.11848118825003433
iteration : 10916
train acc:  0.7265625
train loss:  0.519561231136322
train gradient:  0.16106207942390963
iteration : 10917
train acc:  0.6796875
train loss:  0.5483522415161133
train gradient:  0.15687104171603833
iteration : 10918
train acc:  0.6875
train loss:  0.5298219323158264
train gradient:  0.13803000676148086
iteration : 10919
train acc:  0.75
train loss:  0.5058580040931702
train gradient:  0.14108310436418253
iteration : 10920
train acc:  0.671875
train loss:  0.5458378791809082
train gradient:  0.11059128782443031
iteration : 10921
train acc:  0.703125
train loss:  0.5373630523681641
train gradient:  0.1165580358349955
iteration : 10922
train acc:  0.671875
train loss:  0.5293874740600586
train gradient:  0.11664265177205271
iteration : 10923
train acc:  0.703125
train loss:  0.5516998767852783
train gradient:  0.14265872544399222
iteration : 10924
train acc:  0.78125
train loss:  0.4651029706001282
train gradient:  0.09224035821153424
iteration : 10925
train acc:  0.78125
train loss:  0.5161336064338684
train gradient:  0.12336015742121612
iteration : 10926
train acc:  0.6953125
train loss:  0.5259066820144653
train gradient:  0.1491703064986092
iteration : 10927
train acc:  0.7890625
train loss:  0.482643187046051
train gradient:  0.11495101836750207
iteration : 10928
train acc:  0.7734375
train loss:  0.4456965923309326
train gradient:  0.11397426191916034
iteration : 10929
train acc:  0.78125
train loss:  0.4509112238883972
train gradient:  0.10545552767522856
iteration : 10930
train acc:  0.796875
train loss:  0.438666969537735
train gradient:  0.10851422907923149
iteration : 10931
train acc:  0.7578125
train loss:  0.4788294732570648
train gradient:  0.1424675218721388
iteration : 10932
train acc:  0.6796875
train loss:  0.5324912071228027
train gradient:  0.13894126551566535
iteration : 10933
train acc:  0.78125
train loss:  0.4701932668685913
train gradient:  0.10163838738892882
iteration : 10934
train acc:  0.6640625
train loss:  0.5656531453132629
train gradient:  0.1604990076700722
iteration : 10935
train acc:  0.75
train loss:  0.42378872632980347
train gradient:  0.08136617197582159
iteration : 10936
train acc:  0.796875
train loss:  0.43858182430267334
train gradient:  0.08955259212097218
iteration : 10937
train acc:  0.796875
train loss:  0.4366098642349243
train gradient:  0.0991862448841324
iteration : 10938
train acc:  0.8046875
train loss:  0.41685011982917786
train gradient:  0.0786058135524083
iteration : 10939
train acc:  0.796875
train loss:  0.42609918117523193
train gradient:  0.09005281482410281
iteration : 10940
train acc:  0.7578125
train loss:  0.4968836307525635
train gradient:  0.15366001035633348
iteration : 10941
train acc:  0.78125
train loss:  0.4479452967643738
train gradient:  0.10268711433263059
iteration : 10942
train acc:  0.78125
train loss:  0.4190590977668762
train gradient:  0.10163391244716134
iteration : 10943
train acc:  0.75
train loss:  0.49026671051979065
train gradient:  0.11702741055526007
iteration : 10944
train acc:  0.765625
train loss:  0.45914584398269653
train gradient:  0.12954335557438604
iteration : 10945
train acc:  0.734375
train loss:  0.519830584526062
train gradient:  0.11701550838688682
iteration : 10946
train acc:  0.7421875
train loss:  0.5126492977142334
train gradient:  0.12561158733351385
iteration : 10947
train acc:  0.78125
train loss:  0.4629960060119629
train gradient:  0.13839100431065865
iteration : 10948
train acc:  0.7421875
train loss:  0.5122858285903931
train gradient:  0.12251095129172242
iteration : 10949
train acc:  0.71875
train loss:  0.5228797197341919
train gradient:  0.15203479768032685
iteration : 10950
train acc:  0.6953125
train loss:  0.5227445363998413
train gradient:  0.1539627336328677
iteration : 10951
train acc:  0.71875
train loss:  0.48835626244544983
train gradient:  0.11144615750897854
iteration : 10952
train acc:  0.6953125
train loss:  0.5622081756591797
train gradient:  0.142974978261125
iteration : 10953
train acc:  0.828125
train loss:  0.41635024547576904
train gradient:  0.09041988650562202
iteration : 10954
train acc:  0.7734375
train loss:  0.4708327054977417
train gradient:  0.125939459893046
iteration : 10955
train acc:  0.7734375
train loss:  0.4179133176803589
train gradient:  0.09782795765065376
iteration : 10956
train acc:  0.7421875
train loss:  0.48058655858039856
train gradient:  0.11654951452820617
iteration : 10957
train acc:  0.7265625
train loss:  0.48276567459106445
train gradient:  0.1358659681257294
iteration : 10958
train acc:  0.7109375
train loss:  0.5419456958770752
train gradient:  0.12819874791931818
iteration : 10959
train acc:  0.7421875
train loss:  0.4822391867637634
train gradient:  0.11889143263869803
iteration : 10960
train acc:  0.765625
train loss:  0.46387189626693726
train gradient:  0.1381181509250647
iteration : 10961
train acc:  0.6875
train loss:  0.5105324983596802
train gradient:  0.12894039202884228
iteration : 10962
train acc:  0.703125
train loss:  0.49711161851882935
train gradient:  0.12478523975104358
iteration : 10963
train acc:  0.7578125
train loss:  0.5009458065032959
train gradient:  0.1134045307669005
iteration : 10964
train acc:  0.734375
train loss:  0.5135620832443237
train gradient:  0.15063037414653907
iteration : 10965
train acc:  0.7421875
train loss:  0.5021187663078308
train gradient:  0.1281407227259081
iteration : 10966
train acc:  0.7890625
train loss:  0.46749866008758545
train gradient:  0.11265726218525564
iteration : 10967
train acc:  0.6953125
train loss:  0.5165674686431885
train gradient:  0.15989905557879003
iteration : 10968
train acc:  0.7890625
train loss:  0.4295746684074402
train gradient:  0.09775611372761109
iteration : 10969
train acc:  0.703125
train loss:  0.4983372688293457
train gradient:  0.1071533868686299
iteration : 10970
train acc:  0.7734375
train loss:  0.4379618465900421
train gradient:  0.08529215210128657
iteration : 10971
train acc:  0.78125
train loss:  0.4433046281337738
train gradient:  0.10281835030890507
iteration : 10972
train acc:  0.71875
train loss:  0.4916352331638336
train gradient:  0.11535099124365258
iteration : 10973
train acc:  0.7421875
train loss:  0.5123527646064758
train gradient:  0.19745678775496928
iteration : 10974
train acc:  0.8125
train loss:  0.46668943762779236
train gradient:  0.08729839387648212
iteration : 10975
train acc:  0.7421875
train loss:  0.44522807002067566
train gradient:  0.09832319090040832
iteration : 10976
train acc:  0.7421875
train loss:  0.5164026021957397
train gradient:  0.18884758584988165
iteration : 10977
train acc:  0.7578125
train loss:  0.4559233784675598
train gradient:  0.12490867699702977
iteration : 10978
train acc:  0.78125
train loss:  0.49576377868652344
train gradient:  0.11127039549653134
iteration : 10979
train acc:  0.71875
train loss:  0.5082346200942993
train gradient:  0.14009931285773206
iteration : 10980
train acc:  0.78125
train loss:  0.4380882680416107
train gradient:  0.12553894888732683
iteration : 10981
train acc:  0.7578125
train loss:  0.5229250192642212
train gradient:  0.14790279549482413
iteration : 10982
train acc:  0.765625
train loss:  0.45421266555786133
train gradient:  0.11278987651059168
iteration : 10983
train acc:  0.7578125
train loss:  0.5029036998748779
train gradient:  0.127109660130157
iteration : 10984
train acc:  0.703125
train loss:  0.517128586769104
train gradient:  0.12382054905811675
iteration : 10985
train acc:  0.75
train loss:  0.4868311285972595
train gradient:  0.12397832320818145
iteration : 10986
train acc:  0.7265625
train loss:  0.5403874516487122
train gradient:  0.13488013625059325
iteration : 10987
train acc:  0.75
train loss:  0.5234696865081787
train gradient:  0.14824309183267767
iteration : 10988
train acc:  0.671875
train loss:  0.6046841144561768
train gradient:  0.16396582791447284
iteration : 10989
train acc:  0.7109375
train loss:  0.5034668445587158
train gradient:  0.11723287015674194
iteration : 10990
train acc:  0.7109375
train loss:  0.52390056848526
train gradient:  0.12281407583662673
iteration : 10991
train acc:  0.7890625
train loss:  0.4069896638393402
train gradient:  0.0799462803609694
iteration : 10992
train acc:  0.7421875
train loss:  0.4737622141838074
train gradient:  0.11542459733577409
iteration : 10993
train acc:  0.828125
train loss:  0.39197540283203125
train gradient:  0.0770280092310687
iteration : 10994
train acc:  0.71875
train loss:  0.5110223293304443
train gradient:  0.10759463153535045
iteration : 10995
train acc:  0.703125
train loss:  0.5508641004562378
train gradient:  0.14874232843488172
iteration : 10996
train acc:  0.796875
train loss:  0.445453941822052
train gradient:  0.12154320534526454
iteration : 10997
train acc:  0.7265625
train loss:  0.525795042514801
train gradient:  0.10238978733714332
iteration : 10998
train acc:  0.734375
train loss:  0.5027961730957031
train gradient:  0.12455591466959084
iteration : 10999
train acc:  0.71875
train loss:  0.47353896498680115
train gradient:  0.13264716943643046
iteration : 11000
train acc:  0.7109375
train loss:  0.4544697701931
train gradient:  0.10171878549856987
iteration : 11001
train acc:  0.6953125
train loss:  0.5691584944725037
train gradient:  0.1335899760545448
iteration : 11002
train acc:  0.7265625
train loss:  0.5017135739326477
train gradient:  0.14278114634307304
iteration : 11003
train acc:  0.7421875
train loss:  0.44145628809928894
train gradient:  0.10614689729571292
iteration : 11004
train acc:  0.7109375
train loss:  0.5167229175567627
train gradient:  0.14208270637898518
iteration : 11005
train acc:  0.7890625
train loss:  0.4739065170288086
train gradient:  0.1287478017626263
iteration : 11006
train acc:  0.7109375
train loss:  0.49381643533706665
train gradient:  0.09629036124795104
iteration : 11007
train acc:  0.7265625
train loss:  0.4999014437198639
train gradient:  0.1204177962782888
iteration : 11008
train acc:  0.7421875
train loss:  0.5172131061553955
train gradient:  0.13015331765664656
iteration : 11009
train acc:  0.6796875
train loss:  0.5625080466270447
train gradient:  0.18296859290106066
iteration : 11010
train acc:  0.75
train loss:  0.4204328954219818
train gradient:  0.10073642089440185
iteration : 11011
train acc:  0.6875
train loss:  0.5934206247329712
train gradient:  0.1536945764655132
iteration : 11012
train acc:  0.75
train loss:  0.4891356825828552
train gradient:  0.1221780604070901
iteration : 11013
train acc:  0.7578125
train loss:  0.45579802989959717
train gradient:  0.11486695545768699
iteration : 11014
train acc:  0.7109375
train loss:  0.5270217061042786
train gradient:  0.11715554561382624
iteration : 11015
train acc:  0.7265625
train loss:  0.5447817444801331
train gradient:  0.15147479703214828
iteration : 11016
train acc:  0.75
train loss:  0.4776802659034729
train gradient:  0.12173024691515921
iteration : 11017
train acc:  0.734375
train loss:  0.5329615473747253
train gradient:  0.1344502366345358
iteration : 11018
train acc:  0.7578125
train loss:  0.4817851781845093
train gradient:  0.12272219821327346
iteration : 11019
train acc:  0.7109375
train loss:  0.5514945387840271
train gradient:  0.12201145544465845
iteration : 11020
train acc:  0.734375
train loss:  0.4750439524650574
train gradient:  0.1159133751297062
iteration : 11021
train acc:  0.7890625
train loss:  0.39263033866882324
train gradient:  0.06899313799239859
iteration : 11022
train acc:  0.703125
train loss:  0.5286208391189575
train gradient:  0.1296642378701634
iteration : 11023
train acc:  0.765625
train loss:  0.47135934233665466
train gradient:  0.1088153868684274
iteration : 11024
train acc:  0.6875
train loss:  0.5652950406074524
train gradient:  0.1532959827446957
iteration : 11025
train acc:  0.7890625
train loss:  0.43275851011276245
train gradient:  0.08600455275298242
iteration : 11026
train acc:  0.7890625
train loss:  0.41873472929000854
train gradient:  0.09982773586514501
iteration : 11027
train acc:  0.71875
train loss:  0.5467453598976135
train gradient:  0.13229701341012884
iteration : 11028
train acc:  0.7734375
train loss:  0.4644268751144409
train gradient:  0.10105239045957648
iteration : 11029
train acc:  0.7421875
train loss:  0.47650763392448425
train gradient:  0.09393067476472522
iteration : 11030
train acc:  0.78125
train loss:  0.44465383887290955
train gradient:  0.0934527062507939
iteration : 11031
train acc:  0.7578125
train loss:  0.4648330807685852
train gradient:  0.14280434622386012
iteration : 11032
train acc:  0.7109375
train loss:  0.5252314209938049
train gradient:  0.12567872026457116
iteration : 11033
train acc:  0.765625
train loss:  0.4525717496871948
train gradient:  0.1003857984606999
iteration : 11034
train acc:  0.71875
train loss:  0.4624800682067871
train gradient:  0.1156777021410227
iteration : 11035
train acc:  0.7421875
train loss:  0.5054856538772583
train gradient:  0.1260217588885703
iteration : 11036
train acc:  0.6953125
train loss:  0.5439890623092651
train gradient:  0.17691249237087164
iteration : 11037
train acc:  0.75
train loss:  0.512775182723999
train gradient:  0.17496188831142212
iteration : 11038
train acc:  0.7578125
train loss:  0.44190236926078796
train gradient:  0.11704649477745593
iteration : 11039
train acc:  0.7109375
train loss:  0.5453181266784668
train gradient:  0.16072072452427616
iteration : 11040
train acc:  0.734375
train loss:  0.48473668098449707
train gradient:  0.10799710527532382
iteration : 11041
train acc:  0.703125
train loss:  0.50322425365448
train gradient:  0.1235201482944769
iteration : 11042
train acc:  0.7421875
train loss:  0.4720016121864319
train gradient:  0.11079553528011428
iteration : 11043
train acc:  0.7734375
train loss:  0.501721978187561
train gradient:  0.13159766243770327
iteration : 11044
train acc:  0.7421875
train loss:  0.4978548288345337
train gradient:  0.10449653619200165
iteration : 11045
train acc:  0.7578125
train loss:  0.4583682715892792
train gradient:  0.11306300073702708
iteration : 11046
train acc:  0.78125
train loss:  0.4826759994029999
train gradient:  0.12237329228487179
iteration : 11047
train acc:  0.765625
train loss:  0.48456984758377075
train gradient:  0.11255064207740932
iteration : 11048
train acc:  0.7890625
train loss:  0.4385760426521301
train gradient:  0.10137541098705362
iteration : 11049
train acc:  0.75
train loss:  0.4909723103046417
train gradient:  0.11703968188364702
iteration : 11050
train acc:  0.703125
train loss:  0.52951979637146
train gradient:  0.17273412149642625
iteration : 11051
train acc:  0.6875
train loss:  0.5192661881446838
train gradient:  0.1524067028326037
iteration : 11052
train acc:  0.71875
train loss:  0.46376851201057434
train gradient:  0.12029727803335832
iteration : 11053
train acc:  0.8046875
train loss:  0.440434068441391
train gradient:  0.11588449507082195
iteration : 11054
train acc:  0.7421875
train loss:  0.4357055723667145
train gradient:  0.10110462075376273
iteration : 11055
train acc:  0.734375
train loss:  0.5070009827613831
train gradient:  0.11131607636663626
iteration : 11056
train acc:  0.75
train loss:  0.4373081624507904
train gradient:  0.0916634581957955
iteration : 11057
train acc:  0.7421875
train loss:  0.4946114718914032
train gradient:  0.12527580813988126
iteration : 11058
train acc:  0.703125
train loss:  0.5633256435394287
train gradient:  0.17225183840568214
iteration : 11059
train acc:  0.796875
train loss:  0.38851913809776306
train gradient:  0.08989997406289926
iteration : 11060
train acc:  0.7109375
train loss:  0.5184873938560486
train gradient:  0.1205209357831765
iteration : 11061
train acc:  0.703125
train loss:  0.5326640605926514
train gradient:  0.11588704780757779
iteration : 11062
train acc:  0.8046875
train loss:  0.4174843430519104
train gradient:  0.09648423230693648
iteration : 11063
train acc:  0.75
train loss:  0.45987576246261597
train gradient:  0.09698351327183986
iteration : 11064
train acc:  0.7421875
train loss:  0.551084578037262
train gradient:  0.19199199767944305
iteration : 11065
train acc:  0.75
train loss:  0.4984734058380127
train gradient:  0.12384685681245093
iteration : 11066
train acc:  0.765625
train loss:  0.4493134021759033
train gradient:  0.10876277256042274
iteration : 11067
train acc:  0.7265625
train loss:  0.48163044452667236
train gradient:  0.1334355331338256
iteration : 11068
train acc:  0.765625
train loss:  0.4524855613708496
train gradient:  0.09653112354404789
iteration : 11069
train acc:  0.7578125
train loss:  0.4468936026096344
train gradient:  0.09486774715742179
iteration : 11070
train acc:  0.703125
train loss:  0.5768519043922424
train gradient:  0.1675852551540494
iteration : 11071
train acc:  0.7109375
train loss:  0.5182538032531738
train gradient:  0.15223958459355477
iteration : 11072
train acc:  0.734375
train loss:  0.5064308047294617
train gradient:  0.11984444575437825
iteration : 11073
train acc:  0.734375
train loss:  0.4754444360733032
train gradient:  0.11004120816859186
iteration : 11074
train acc:  0.7578125
train loss:  0.4967731535434723
train gradient:  0.12649146300142816
iteration : 11075
train acc:  0.7578125
train loss:  0.4638608694076538
train gradient:  0.13512369108888905
iteration : 11076
train acc:  0.7578125
train loss:  0.5391647219657898
train gradient:  0.1130019172385544
iteration : 11077
train acc:  0.7265625
train loss:  0.48408615589141846
train gradient:  0.10494081947777961
iteration : 11078
train acc:  0.71875
train loss:  0.5040115118026733
train gradient:  0.12036108573454225
iteration : 11079
train acc:  0.7578125
train loss:  0.4988120198249817
train gradient:  0.0954996402141797
iteration : 11080
train acc:  0.7578125
train loss:  0.49932312965393066
train gradient:  0.1088227950624739
iteration : 11081
train acc:  0.8046875
train loss:  0.47308099269866943
train gradient:  0.12444386587615411
iteration : 11082
train acc:  0.765625
train loss:  0.4398874342441559
train gradient:  0.15736762067471044
iteration : 11083
train acc:  0.7265625
train loss:  0.5289813280105591
train gradient:  0.12594406305193812
iteration : 11084
train acc:  0.7734375
train loss:  0.4841878414154053
train gradient:  0.11186932171927234
iteration : 11085
train acc:  0.796875
train loss:  0.42188549041748047
train gradient:  0.08545300500676296
iteration : 11086
train acc:  0.7421875
train loss:  0.4983125329017639
train gradient:  0.12587229406672362
iteration : 11087
train acc:  0.7734375
train loss:  0.4992603063583374
train gradient:  0.11723855193204939
iteration : 11088
train acc:  0.7578125
train loss:  0.48071324825286865
train gradient:  0.09080987571990007
iteration : 11089
train acc:  0.796875
train loss:  0.4749991297721863
train gradient:  0.13165868165755135
iteration : 11090
train acc:  0.71875
train loss:  0.5124253034591675
train gradient:  0.14335679889292546
iteration : 11091
train acc:  0.7890625
train loss:  0.5136036276817322
train gradient:  0.12680282000975873
iteration : 11092
train acc:  0.7421875
train loss:  0.49016740918159485
train gradient:  0.15178539110931688
iteration : 11093
train acc:  0.7578125
train loss:  0.47103774547576904
train gradient:  0.11059391162194586
iteration : 11094
train acc:  0.7890625
train loss:  0.429317444562912
train gradient:  0.07963393010010478
iteration : 11095
train acc:  0.7890625
train loss:  0.4160851240158081
train gradient:  0.0857455103151551
iteration : 11096
train acc:  0.7890625
train loss:  0.43041110038757324
train gradient:  0.11382789560078722
iteration : 11097
train acc:  0.6953125
train loss:  0.5067747235298157
train gradient:  0.11888836476195633
iteration : 11098
train acc:  0.796875
train loss:  0.44821932911872864
train gradient:  0.10725988221097214
iteration : 11099
train acc:  0.7578125
train loss:  0.4775579571723938
train gradient:  0.12088900675267027
iteration : 11100
train acc:  0.6953125
train loss:  0.5088357329368591
train gradient:  0.12799759319542575
iteration : 11101
train acc:  0.75
train loss:  0.48362064361572266
train gradient:  0.10610537444520778
iteration : 11102
train acc:  0.71875
train loss:  0.512353777885437
train gradient:  0.14313431050586226
iteration : 11103
train acc:  0.765625
train loss:  0.46676963567733765
train gradient:  0.13317427650846736
iteration : 11104
train acc:  0.6796875
train loss:  0.5833278298377991
train gradient:  0.22963078069529289
iteration : 11105
train acc:  0.7578125
train loss:  0.479950487613678
train gradient:  0.10304590210314123
iteration : 11106
train acc:  0.7109375
train loss:  0.48931884765625
train gradient:  0.1263791266173779
iteration : 11107
train acc:  0.734375
train loss:  0.47987550497055054
train gradient:  0.12133520502871703
iteration : 11108
train acc:  0.7265625
train loss:  0.5310294032096863
train gradient:  0.15074972482524884
iteration : 11109
train acc:  0.7890625
train loss:  0.4726499319076538
train gradient:  0.14984352327504832
iteration : 11110
train acc:  0.7265625
train loss:  0.5133177042007446
train gradient:  0.12997298002263036
iteration : 11111
train acc:  0.7109375
train loss:  0.5455999970436096
train gradient:  0.15987397722028063
iteration : 11112
train acc:  0.7265625
train loss:  0.4862269163131714
train gradient:  0.11500994934496787
iteration : 11113
train acc:  0.796875
train loss:  0.4810062646865845
train gradient:  0.11406380493157424
iteration : 11114
train acc:  0.7578125
train loss:  0.5155259966850281
train gradient:  0.12990234328847883
iteration : 11115
train acc:  0.7890625
train loss:  0.44913455843925476
train gradient:  0.15116219752991406
iteration : 11116
train acc:  0.796875
train loss:  0.43761491775512695
train gradient:  0.08153391271273328
iteration : 11117
train acc:  0.75
train loss:  0.46836793422698975
train gradient:  0.11999126038965438
iteration : 11118
train acc:  0.7421875
train loss:  0.4956516921520233
train gradient:  0.11325538966521714
iteration : 11119
train acc:  0.7890625
train loss:  0.4721771478652954
train gradient:  0.0999250922775158
iteration : 11120
train acc:  0.8046875
train loss:  0.4328116178512573
train gradient:  0.09794085400636471
iteration : 11121
train acc:  0.7734375
train loss:  0.4477752149105072
train gradient:  0.09201814105156038
iteration : 11122
train acc:  0.7109375
train loss:  0.4737319052219391
train gradient:  0.10551956150070253
iteration : 11123
train acc:  0.7109375
train loss:  0.5313045978546143
train gradient:  0.135910506964881
iteration : 11124
train acc:  0.71875
train loss:  0.5205093622207642
train gradient:  0.13601869675580647
iteration : 11125
train acc:  0.796875
train loss:  0.4981488883495331
train gradient:  0.1264244698510832
iteration : 11126
train acc:  0.7109375
train loss:  0.535835325717926
train gradient:  0.1574401189434258
iteration : 11127
train acc:  0.7578125
train loss:  0.5161726474761963
train gradient:  0.1827294692263285
iteration : 11128
train acc:  0.8125
train loss:  0.44059640169143677
train gradient:  0.10793209685363638
iteration : 11129
train acc:  0.78125
train loss:  0.46341240406036377
train gradient:  0.12342862148723044
iteration : 11130
train acc:  0.7734375
train loss:  0.43031367659568787
train gradient:  0.09352462626093175
iteration : 11131
train acc:  0.7421875
train loss:  0.5116134881973267
train gradient:  0.17640700127233855
iteration : 11132
train acc:  0.6640625
train loss:  0.6031212210655212
train gradient:  0.17223379182980708
iteration : 11133
train acc:  0.71875
train loss:  0.5132409334182739
train gradient:  0.12895657191102866
iteration : 11134
train acc:  0.6953125
train loss:  0.531987190246582
train gradient:  0.1328156050568321
iteration : 11135
train acc:  0.6875
train loss:  0.5274489521980286
train gradient:  0.1368484890984442
iteration : 11136
train acc:  0.6953125
train loss:  0.5346953868865967
train gradient:  0.13864425534387348
iteration : 11137
train acc:  0.75
train loss:  0.5373616218566895
train gradient:  0.131585087561568
iteration : 11138
train acc:  0.703125
train loss:  0.5554134845733643
train gradient:  0.1489655678386373
iteration : 11139
train acc:  0.7734375
train loss:  0.4723418951034546
train gradient:  0.13512937687440935
iteration : 11140
train acc:  0.7265625
train loss:  0.45691096782684326
train gradient:  0.12347664959075454
iteration : 11141
train acc:  0.71875
train loss:  0.4765181541442871
train gradient:  0.10551677372740481
iteration : 11142
train acc:  0.8046875
train loss:  0.432966411113739
train gradient:  0.07962023674086349
iteration : 11143
train acc:  0.71875
train loss:  0.5470224022865295
train gradient:  0.160393401014369
iteration : 11144
train acc:  0.7421875
train loss:  0.47098708152770996
train gradient:  0.11074741201807375
iteration : 11145
train acc:  0.796875
train loss:  0.41762804985046387
train gradient:  0.0974763435748925
iteration : 11146
train acc:  0.734375
train loss:  0.4880668520927429
train gradient:  0.12165296631582938
iteration : 11147
train acc:  0.7578125
train loss:  0.486047625541687
train gradient:  0.12817591221495633
iteration : 11148
train acc:  0.7265625
train loss:  0.5344398021697998
train gradient:  0.1084757600357519
iteration : 11149
train acc:  0.734375
train loss:  0.5015132427215576
train gradient:  0.1367057238694676
iteration : 11150
train acc:  0.78125
train loss:  0.46607840061187744
train gradient:  0.1259397602435482
iteration : 11151
train acc:  0.7421875
train loss:  0.5375006794929504
train gradient:  0.14642781726379717
iteration : 11152
train acc:  0.78125
train loss:  0.43623197078704834
train gradient:  0.08721122078065746
iteration : 11153
train acc:  0.7265625
train loss:  0.4800359010696411
train gradient:  0.11603322280998395
iteration : 11154
train acc:  0.828125
train loss:  0.41807064414024353
train gradient:  0.11161402024835532
iteration : 11155
train acc:  0.796875
train loss:  0.46391940116882324
train gradient:  0.1439920004068387
iteration : 11156
train acc:  0.78125
train loss:  0.456354022026062
train gradient:  0.11826655839104577
iteration : 11157
train acc:  0.7109375
train loss:  0.5687175393104553
train gradient:  0.16515192851890298
iteration : 11158
train acc:  0.7421875
train loss:  0.47292548418045044
train gradient:  0.09402383903204795
iteration : 11159
train acc:  0.6484375
train loss:  0.5483224391937256
train gradient:  0.16317388029622565
iteration : 11160
train acc:  0.78125
train loss:  0.5382565259933472
train gradient:  0.18974210166248898
iteration : 11161
train acc:  0.734375
train loss:  0.5828373432159424
train gradient:  0.1852190061583752
iteration : 11162
train acc:  0.7109375
train loss:  0.4889754056930542
train gradient:  0.10601154015001403
iteration : 11163
train acc:  0.765625
train loss:  0.4651314616203308
train gradient:  0.12216308462444522
iteration : 11164
train acc:  0.703125
train loss:  0.5218954086303711
train gradient:  0.14349957120105705
iteration : 11165
train acc:  0.75
train loss:  0.44695574045181274
train gradient:  0.08196426564660257
iteration : 11166
train acc:  0.71875
train loss:  0.5391664505004883
train gradient:  0.1479801264588798
iteration : 11167
train acc:  0.828125
train loss:  0.40348339080810547
train gradient:  0.09425289148042504
iteration : 11168
train acc:  0.796875
train loss:  0.4457908272743225
train gradient:  0.13553841537429256
iteration : 11169
train acc:  0.7265625
train loss:  0.5078454613685608
train gradient:  0.12523261764724689
iteration : 11170
train acc:  0.75
train loss:  0.4741717576980591
train gradient:  0.16450827626908904
iteration : 11171
train acc:  0.7734375
train loss:  0.5018053650856018
train gradient:  0.12943228304348908
iteration : 11172
train acc:  0.7578125
train loss:  0.461392879486084
train gradient:  0.09732301352897546
iteration : 11173
train acc:  0.7265625
train loss:  0.5003888607025146
train gradient:  0.1196149561095298
iteration : 11174
train acc:  0.765625
train loss:  0.4897599220275879
train gradient:  0.11853051635796161
iteration : 11175
train acc:  0.78125
train loss:  0.45676371455192566
train gradient:  0.11083978932989995
iteration : 11176
train acc:  0.75
train loss:  0.49422401189804077
train gradient:  0.13849724717917217
iteration : 11177
train acc:  0.6796875
train loss:  0.539633572101593
train gradient:  0.14872449125001275
iteration : 11178
train acc:  0.78125
train loss:  0.4594446122646332
train gradient:  0.10614718503046384
iteration : 11179
train acc:  0.7578125
train loss:  0.503383994102478
train gradient:  0.12359427211233914
iteration : 11180
train acc:  0.796875
train loss:  0.4285740256309509
train gradient:  0.10431269727006524
iteration : 11181
train acc:  0.765625
train loss:  0.4762502610683441
train gradient:  0.12934177457714402
iteration : 11182
train acc:  0.7578125
train loss:  0.43829402327537537
train gradient:  0.11005510378349412
iteration : 11183
train acc:  0.7265625
train loss:  0.5660776495933533
train gradient:  0.1445957790446515
iteration : 11184
train acc:  0.7421875
train loss:  0.5195724368095398
train gradient:  0.14490689015337987
iteration : 11185
train acc:  0.7734375
train loss:  0.5088714361190796
train gradient:  0.14378447439741193
iteration : 11186
train acc:  0.7109375
train loss:  0.5251013040542603
train gradient:  0.14819571056389652
iteration : 11187
train acc:  0.734375
train loss:  0.49506762623786926
train gradient:  0.12470563961447635
iteration : 11188
train acc:  0.796875
train loss:  0.468116819858551
train gradient:  0.1259603222900892
iteration : 11189
train acc:  0.71875
train loss:  0.5305721759796143
train gradient:  0.11610753984360657
iteration : 11190
train acc:  0.6796875
train loss:  0.5277258157730103
train gradient:  0.15382642498087296
iteration : 11191
train acc:  0.7734375
train loss:  0.45007187128067017
train gradient:  0.09350747505544171
iteration : 11192
train acc:  0.734375
train loss:  0.48252183198928833
train gradient:  0.15306425178947103
iteration : 11193
train acc:  0.7109375
train loss:  0.5269167423248291
train gradient:  0.13618632143049625
iteration : 11194
train acc:  0.7578125
train loss:  0.48537346720695496
train gradient:  0.14662017642489028
iteration : 11195
train acc:  0.703125
train loss:  0.5455019474029541
train gradient:  0.17610958777587307
iteration : 11196
train acc:  0.7265625
train loss:  0.5200786590576172
train gradient:  0.16923485795026144
iteration : 11197
train acc:  0.828125
train loss:  0.4309244751930237
train gradient:  0.11716928154309988
iteration : 11198
train acc:  0.7734375
train loss:  0.4666255712509155
train gradient:  0.11406742974896748
iteration : 11199
train acc:  0.7890625
train loss:  0.4553622007369995
train gradient:  0.11119992403901154
iteration : 11200
train acc:  0.71875
train loss:  0.484396368265152
train gradient:  0.12254649430686176
iteration : 11201
train acc:  0.6875
train loss:  0.5339675545692444
train gradient:  0.13505142020266253
iteration : 11202
train acc:  0.671875
train loss:  0.5468258261680603
train gradient:  0.13851653071165945
iteration : 11203
train acc:  0.7109375
train loss:  0.5419156551361084
train gradient:  0.14382576044588888
iteration : 11204
train acc:  0.7734375
train loss:  0.48160451650619507
train gradient:  0.129386472758015
iteration : 11205
train acc:  0.7578125
train loss:  0.45531168580055237
train gradient:  0.12010982114743178
iteration : 11206
train acc:  0.734375
train loss:  0.4879039525985718
train gradient:  0.1295168657421018
iteration : 11207
train acc:  0.8203125
train loss:  0.4358428418636322
train gradient:  0.0864805594322366
iteration : 11208
train acc:  0.796875
train loss:  0.4861387610435486
train gradient:  0.1580721267528854
iteration : 11209
train acc:  0.671875
train loss:  0.5190417766571045
train gradient:  0.13700014908185643
iteration : 11210
train acc:  0.796875
train loss:  0.43363243341445923
train gradient:  0.09178616906989083
iteration : 11211
train acc:  0.7890625
train loss:  0.4570678770542145
train gradient:  0.09978852701276855
iteration : 11212
train acc:  0.703125
train loss:  0.5060240030288696
train gradient:  0.12281420738800716
iteration : 11213
train acc:  0.671875
train loss:  0.515937328338623
train gradient:  0.15471258443787822
iteration : 11214
train acc:  0.7421875
train loss:  0.4904455840587616
train gradient:  0.12631268572619417
iteration : 11215
train acc:  0.703125
train loss:  0.5192917585372925
train gradient:  0.1298975889924351
iteration : 11216
train acc:  0.765625
train loss:  0.4673464894294739
train gradient:  0.11749257582014647
iteration : 11217
train acc:  0.75
train loss:  0.48070770502090454
train gradient:  0.13015495400240074
iteration : 11218
train acc:  0.75
train loss:  0.46992436051368713
train gradient:  0.09909408491872877
iteration : 11219
train acc:  0.7265625
train loss:  0.49772974848747253
train gradient:  0.0981438406462249
iteration : 11220
train acc:  0.7421875
train loss:  0.5247023701667786
train gradient:  0.13545724811650975
iteration : 11221
train acc:  0.6953125
train loss:  0.5535072088241577
train gradient:  0.16353377607499242
iteration : 11222
train acc:  0.7265625
train loss:  0.5212194919586182
train gradient:  0.14532604387198164
iteration : 11223
train acc:  0.765625
train loss:  0.4493716061115265
train gradient:  0.10136390978462559
iteration : 11224
train acc:  0.765625
train loss:  0.48121562600135803
train gradient:  0.1096909723815227
iteration : 11225
train acc:  0.734375
train loss:  0.45792582631111145
train gradient:  0.10034288684569982
iteration : 11226
train acc:  0.7421875
train loss:  0.5107804536819458
train gradient:  0.12532671590958
iteration : 11227
train acc:  0.6953125
train loss:  0.5627633333206177
train gradient:  0.17114387647176765
iteration : 11228
train acc:  0.703125
train loss:  0.46681100130081177
train gradient:  0.09459083366429327
iteration : 11229
train acc:  0.796875
train loss:  0.43635278940200806
train gradient:  0.09721806125087919
iteration : 11230
train acc:  0.765625
train loss:  0.464785635471344
train gradient:  0.12898427889186737
iteration : 11231
train acc:  0.8515625
train loss:  0.34760794043540955
train gradient:  0.08671616272537708
iteration : 11232
train acc:  0.71875
train loss:  0.5076935291290283
train gradient:  0.15023567357751672
iteration : 11233
train acc:  0.8203125
train loss:  0.4338235855102539
train gradient:  0.1027873981733774
iteration : 11234
train acc:  0.8046875
train loss:  0.45052674412727356
train gradient:  0.08720332049533347
iteration : 11235
train acc:  0.7734375
train loss:  0.47173601388931274
train gradient:  0.11973390644554342
iteration : 11236
train acc:  0.796875
train loss:  0.4347704350948334
train gradient:  0.09506826454215847
iteration : 11237
train acc:  0.8515625
train loss:  0.4190215468406677
train gradient:  0.102604610396646
iteration : 11238
train acc:  0.6796875
train loss:  0.514136791229248
train gradient:  0.1073337665484022
iteration : 11239
train acc:  0.75
train loss:  0.4965038597583771
train gradient:  0.11753277345504201
iteration : 11240
train acc:  0.7421875
train loss:  0.457231342792511
train gradient:  0.1160948441468387
iteration : 11241
train acc:  0.7578125
train loss:  0.46123048663139343
train gradient:  0.14316727440475613
iteration : 11242
train acc:  0.75
train loss:  0.43671321868896484
train gradient:  0.10909151444841368
iteration : 11243
train acc:  0.734375
train loss:  0.4670725464820862
train gradient:  0.10344337358492774
iteration : 11244
train acc:  0.7578125
train loss:  0.5283299088478088
train gradient:  0.11383759102146161
iteration : 11245
train acc:  0.8203125
train loss:  0.4370725154876709
train gradient:  0.09861363127821297
iteration : 11246
train acc:  0.7734375
train loss:  0.5157016515731812
train gradient:  0.16528157276285116
iteration : 11247
train acc:  0.734375
train loss:  0.476315975189209
train gradient:  0.11195694912168679
iteration : 11248
train acc:  0.7890625
train loss:  0.4094555675983429
train gradient:  0.09247845746124174
iteration : 11249
train acc:  0.7109375
train loss:  0.5196685791015625
train gradient:  0.1308463228913011
iteration : 11250
train acc:  0.78125
train loss:  0.47294098138809204
train gradient:  0.12783380153583765
iteration : 11251
train acc:  0.7421875
train loss:  0.48379018902778625
train gradient:  0.13155820016093617
iteration : 11252
train acc:  0.765625
train loss:  0.5063210725784302
train gradient:  0.1405444810976304
iteration : 11253
train acc:  0.7734375
train loss:  0.46505212783813477
train gradient:  0.11453575129043907
iteration : 11254
train acc:  0.78125
train loss:  0.4732217788696289
train gradient:  0.1171337394452805
iteration : 11255
train acc:  0.8203125
train loss:  0.40865421295166016
train gradient:  0.09426908005292547
iteration : 11256
train acc:  0.7109375
train loss:  0.5421150922775269
train gradient:  0.15419982978901003
iteration : 11257
train acc:  0.765625
train loss:  0.46592575311660767
train gradient:  0.13077541505663454
iteration : 11258
train acc:  0.734375
train loss:  0.4964713156223297
train gradient:  0.10788707181456351
iteration : 11259
train acc:  0.7734375
train loss:  0.47349685430526733
train gradient:  0.10756437125862481
iteration : 11260
train acc:  0.7421875
train loss:  0.46353599429130554
train gradient:  0.11632810372003259
iteration : 11261
train acc:  0.8359375
train loss:  0.4017813205718994
train gradient:  0.09167322228482586
iteration : 11262
train acc:  0.7109375
train loss:  0.4978342056274414
train gradient:  0.1390449482464054
iteration : 11263
train acc:  0.7734375
train loss:  0.4566091001033783
train gradient:  0.14373958397948577
iteration : 11264
train acc:  0.7890625
train loss:  0.4615980386734009
train gradient:  0.11285213116256301
iteration : 11265
train acc:  0.765625
train loss:  0.46406224370002747
train gradient:  0.10876827845854505
iteration : 11266
train acc:  0.734375
train loss:  0.46583080291748047
train gradient:  0.10094915153846963
iteration : 11267
train acc:  0.765625
train loss:  0.4821060597896576
train gradient:  0.10649417665926177
iteration : 11268
train acc:  0.734375
train loss:  0.4955180287361145
train gradient:  0.1339766906401461
iteration : 11269
train acc:  0.7578125
train loss:  0.46263277530670166
train gradient:  0.10161385509581135
iteration : 11270
train acc:  0.7578125
train loss:  0.4845258891582489
train gradient:  0.10720676451143911
iteration : 11271
train acc:  0.75
train loss:  0.47712230682373047
train gradient:  0.10434299360386573
iteration : 11272
train acc:  0.75
train loss:  0.5464316606521606
train gradient:  0.14797145061811823
iteration : 11273
train acc:  0.6640625
train loss:  0.5122306942939758
train gradient:  0.134813485648547
iteration : 11274
train acc:  0.6953125
train loss:  0.5611353516578674
train gradient:  0.1817365871234905
iteration : 11275
train acc:  0.7265625
train loss:  0.5683720111846924
train gradient:  0.1539441179670818
iteration : 11276
train acc:  0.7109375
train loss:  0.5248438119888306
train gradient:  0.1637304233290311
iteration : 11277
train acc:  0.765625
train loss:  0.45559006929397583
train gradient:  0.1094039047103194
iteration : 11278
train acc:  0.75
train loss:  0.4688052535057068
train gradient:  0.12926047730839085
iteration : 11279
train acc:  0.7109375
train loss:  0.5136017203330994
train gradient:  0.15944903545829464
iteration : 11280
train acc:  0.765625
train loss:  0.48312345147132874
train gradient:  0.10595204401136592
iteration : 11281
train acc:  0.6875
train loss:  0.5599006414413452
train gradient:  0.1528990318433931
iteration : 11282
train acc:  0.8125
train loss:  0.4616069793701172
train gradient:  0.10887668563141832
iteration : 11283
train acc:  0.7109375
train loss:  0.547847330570221
train gradient:  0.14846253386061176
iteration : 11284
train acc:  0.78125
train loss:  0.45398083329200745
train gradient:  0.11001920187278486
iteration : 11285
train acc:  0.65625
train loss:  0.5887693166732788
train gradient:  0.1746907239073705
iteration : 11286
train acc:  0.7734375
train loss:  0.5364264249801636
train gradient:  0.19541795502333964
iteration : 11287
train acc:  0.7109375
train loss:  0.5394153594970703
train gradient:  0.16504898926290565
iteration : 11288
train acc:  0.8359375
train loss:  0.4204051196575165
train gradient:  0.08814218385170479
iteration : 11289
train acc:  0.71875
train loss:  0.4966643452644348
train gradient:  0.1428232405611349
iteration : 11290
train acc:  0.71875
train loss:  0.4574565589427948
train gradient:  0.09716393719976696
iteration : 11291
train acc:  0.6953125
train loss:  0.545843243598938
train gradient:  0.17236337490093892
iteration : 11292
train acc:  0.7421875
train loss:  0.5092802047729492
train gradient:  0.11888481811132039
iteration : 11293
train acc:  0.7734375
train loss:  0.47810328006744385
train gradient:  0.09976951910243405
iteration : 11294
train acc:  0.7421875
train loss:  0.49485135078430176
train gradient:  0.11687621196488114
iteration : 11295
train acc:  0.6875
train loss:  0.5183761715888977
train gradient:  0.14046348476168105
iteration : 11296
train acc:  0.75
train loss:  0.517687201499939
train gradient:  0.1243142531166312
iteration : 11297
train acc:  0.7265625
train loss:  0.4791044592857361
train gradient:  0.12564704544573066
iteration : 11298
train acc:  0.7265625
train loss:  0.5028099417686462
train gradient:  0.12294663870665938
iteration : 11299
train acc:  0.7421875
train loss:  0.4860776960849762
train gradient:  0.15794057385940075
iteration : 11300
train acc:  0.7109375
train loss:  0.5364487171173096
train gradient:  0.12007920548847548
iteration : 11301
train acc:  0.734375
train loss:  0.5033579468727112
train gradient:  0.13561973496906818
iteration : 11302
train acc:  0.7421875
train loss:  0.483523964881897
train gradient:  0.12306000418873805
iteration : 11303
train acc:  0.7421875
train loss:  0.4490964710712433
train gradient:  0.08594673017551763
iteration : 11304
train acc:  0.703125
train loss:  0.5010543465614319
train gradient:  0.10988365831434731
iteration : 11305
train acc:  0.7109375
train loss:  0.5153989791870117
train gradient:  0.13366449715247017
iteration : 11306
train acc:  0.7109375
train loss:  0.5463584065437317
train gradient:  0.15243042283415947
iteration : 11307
train acc:  0.7265625
train loss:  0.5112444162368774
train gradient:  0.14167169949270458
iteration : 11308
train acc:  0.7734375
train loss:  0.5384171009063721
train gradient:  0.1526951594528117
iteration : 11309
train acc:  0.734375
train loss:  0.5639921426773071
train gradient:  0.15497795870650627
iteration : 11310
train acc:  0.7890625
train loss:  0.442227303981781
train gradient:  0.11933521533648521
iteration : 11311
train acc:  0.828125
train loss:  0.42699146270751953
train gradient:  0.12089900392871895
iteration : 11312
train acc:  0.7109375
train loss:  0.567721426486969
train gradient:  0.16450763375272298
iteration : 11313
train acc:  0.7265625
train loss:  0.445625901222229
train gradient:  0.11292450494989385
iteration : 11314
train acc:  0.7734375
train loss:  0.4608277678489685
train gradient:  0.10984592139388666
iteration : 11315
train acc:  0.78125
train loss:  0.4675169587135315
train gradient:  0.16291376008905215
iteration : 11316
train acc:  0.6875
train loss:  0.5602632761001587
train gradient:  0.13883875687812855
iteration : 11317
train acc:  0.703125
train loss:  0.560646653175354
train gradient:  0.1652844220746919
iteration : 11318
train acc:  0.765625
train loss:  0.4772510528564453
train gradient:  0.10084192818129487
iteration : 11319
train acc:  0.6875
train loss:  0.5438461303710938
train gradient:  0.11971019644780694
iteration : 11320
train acc:  0.71875
train loss:  0.47311052680015564
train gradient:  0.10508357104194935
iteration : 11321
train acc:  0.7578125
train loss:  0.5072247982025146
train gradient:  0.1384949095040856
iteration : 11322
train acc:  0.6640625
train loss:  0.5565846562385559
train gradient:  0.15737372721415205
iteration : 11323
train acc:  0.7578125
train loss:  0.48611924052238464
train gradient:  0.11629821880714013
iteration : 11324
train acc:  0.7890625
train loss:  0.44320717453956604
train gradient:  0.13916195188413327
iteration : 11325
train acc:  0.7578125
train loss:  0.4806428849697113
train gradient:  0.11881221512132702
iteration : 11326
train acc:  0.7578125
train loss:  0.4614989161491394
train gradient:  0.09897329483914213
iteration : 11327
train acc:  0.7265625
train loss:  0.5110141038894653
train gradient:  0.154343689642033
iteration : 11328
train acc:  0.734375
train loss:  0.5190448760986328
train gradient:  0.14103829456588451
iteration : 11329
train acc:  0.6953125
train loss:  0.6004741191864014
train gradient:  0.16213603229566126
iteration : 11330
train acc:  0.703125
train loss:  0.4540284276008606
train gradient:  0.0909833761464005
iteration : 11331
train acc:  0.765625
train loss:  0.4483790099620819
train gradient:  0.10375632184971323
iteration : 11332
train acc:  0.6796875
train loss:  0.5697920918464661
train gradient:  0.14889500031751024
iteration : 11333
train acc:  0.7578125
train loss:  0.45151734352111816
train gradient:  0.11992948376000807
iteration : 11334
train acc:  0.71875
train loss:  0.5190427899360657
train gradient:  0.113240759633714
iteration : 11335
train acc:  0.7421875
train loss:  0.49505355954170227
train gradient:  0.13198464596399612
iteration : 11336
train acc:  0.734375
train loss:  0.4844805598258972
train gradient:  0.1147032701567001
iteration : 11337
train acc:  0.7890625
train loss:  0.48028653860092163
train gradient:  0.12322069016965488
iteration : 11338
train acc:  0.78125
train loss:  0.4898114800453186
train gradient:  0.13575454363421113
iteration : 11339
train acc:  0.8046875
train loss:  0.467231810092926
train gradient:  0.11453068992308471
iteration : 11340
train acc:  0.734375
train loss:  0.5118050575256348
train gradient:  0.1207239284625669
iteration : 11341
train acc:  0.75
train loss:  0.4663541316986084
train gradient:  0.1325721308187569
iteration : 11342
train acc:  0.734375
train loss:  0.49454137682914734
train gradient:  0.10795556371339979
iteration : 11343
train acc:  0.7734375
train loss:  0.43244022130966187
train gradient:  0.0872566722392102
iteration : 11344
train acc:  0.7421875
train loss:  0.49520355463027954
train gradient:  0.11409301528775945
iteration : 11345
train acc:  0.734375
train loss:  0.5056421756744385
train gradient:  0.11843304735439032
iteration : 11346
train acc:  0.7890625
train loss:  0.42488327622413635
train gradient:  0.11068858859181091
iteration : 11347
train acc:  0.78125
train loss:  0.45204538106918335
train gradient:  0.10152443379140455
iteration : 11348
train acc:  0.875
train loss:  0.3389027714729309
train gradient:  0.08798698585275326
iteration : 11349
train acc:  0.75
train loss:  0.47714245319366455
train gradient:  0.13989026197849536
iteration : 11350
train acc:  0.78125
train loss:  0.4379766583442688
train gradient:  0.10980201603018606
iteration : 11351
train acc:  0.8125
train loss:  0.4358752369880676
train gradient:  0.0973998629573704
iteration : 11352
train acc:  0.7265625
train loss:  0.5221819281578064
train gradient:  0.15262183810268598
iteration : 11353
train acc:  0.6875
train loss:  0.5017670392990112
train gradient:  0.13612490077320794
iteration : 11354
train acc:  0.765625
train loss:  0.4563797116279602
train gradient:  0.10954296555505992
iteration : 11355
train acc:  0.671875
train loss:  0.5905487537384033
train gradient:  0.15162374045059895
iteration : 11356
train acc:  0.625
train loss:  0.6481235027313232
train gradient:  0.18606661381598527
iteration : 11357
train acc:  0.8046875
train loss:  0.43376290798187256
train gradient:  0.09253382102189336
iteration : 11358
train acc:  0.78125
train loss:  0.4274791479110718
train gradient:  0.08088524002366239
iteration : 11359
train acc:  0.8046875
train loss:  0.4402061700820923
train gradient:  0.113246175217277
iteration : 11360
train acc:  0.8125
train loss:  0.44790396094322205
train gradient:  0.16279182978274093
iteration : 11361
train acc:  0.7421875
train loss:  0.44722893834114075
train gradient:  0.10507112472427947
iteration : 11362
train acc:  0.7578125
train loss:  0.4972696900367737
train gradient:  0.11104636288078855
iteration : 11363
train acc:  0.7578125
train loss:  0.4732453525066376
train gradient:  0.12445647194663415
iteration : 11364
train acc:  0.703125
train loss:  0.5256177186965942
train gradient:  0.11859652568719904
iteration : 11365
train acc:  0.7734375
train loss:  0.42627593874931335
train gradient:  0.0848598128329489
iteration : 11366
train acc:  0.7578125
train loss:  0.47144657373428345
train gradient:  0.14047742143294817
iteration : 11367
train acc:  0.8046875
train loss:  0.4252607226371765
train gradient:  0.09236581940511668
iteration : 11368
train acc:  0.78125
train loss:  0.47162580490112305
train gradient:  0.1044387834306352
iteration : 11369
train acc:  0.796875
train loss:  0.4375891387462616
train gradient:  0.09225529750947808
iteration : 11370
train acc:  0.7734375
train loss:  0.4505544900894165
train gradient:  0.11078503222431689
iteration : 11371
train acc:  0.6953125
train loss:  0.543034553527832
train gradient:  0.14028616357438128
iteration : 11372
train acc:  0.7890625
train loss:  0.45567071437835693
train gradient:  0.10445207047998126
iteration : 11373
train acc:  0.7890625
train loss:  0.4540095925331116
train gradient:  0.09157355312859237
iteration : 11374
train acc:  0.765625
train loss:  0.44550588726997375
train gradient:  0.09295010325863522
iteration : 11375
train acc:  0.7109375
train loss:  0.5484994649887085
train gradient:  0.1995790079304421
iteration : 11376
train acc:  0.6953125
train loss:  0.5292963981628418
train gradient:  0.1363592857023931
iteration : 11377
train acc:  0.7265625
train loss:  0.48987850546836853
train gradient:  0.10800197623201538
iteration : 11378
train acc:  0.6953125
train loss:  0.4623825252056122
train gradient:  0.10963661410712415
iteration : 11379
train acc:  0.7265625
train loss:  0.5051409602165222
train gradient:  0.13135508764077186
iteration : 11380
train acc:  0.7421875
train loss:  0.445059210062027
train gradient:  0.1073764052490549
iteration : 11381
train acc:  0.7421875
train loss:  0.5154922008514404
train gradient:  0.13090782371856513
iteration : 11382
train acc:  0.8125
train loss:  0.4243696928024292
train gradient:  0.10619662084550409
iteration : 11383
train acc:  0.7578125
train loss:  0.504875898361206
train gradient:  0.16843128622368342
iteration : 11384
train acc:  0.6796875
train loss:  0.5839471817016602
train gradient:  0.16763808332640018
iteration : 11385
train acc:  0.765625
train loss:  0.4751878082752228
train gradient:  0.13167000069161625
iteration : 11386
train acc:  0.7890625
train loss:  0.4636550545692444
train gradient:  0.10816303355977347
iteration : 11387
train acc:  0.78125
train loss:  0.49291330575942993
train gradient:  0.12155489943909051
iteration : 11388
train acc:  0.7734375
train loss:  0.4573015570640564
train gradient:  0.11133227760056874
iteration : 11389
train acc:  0.671875
train loss:  0.6126039028167725
train gradient:  0.15167236186914831
iteration : 11390
train acc:  0.734375
train loss:  0.4836954176425934
train gradient:  0.126155960405947
iteration : 11391
train acc:  0.7734375
train loss:  0.4578089118003845
train gradient:  0.1384739197316605
iteration : 11392
train acc:  0.765625
train loss:  0.42681145668029785
train gradient:  0.10478873969954747
iteration : 11393
train acc:  0.71875
train loss:  0.5619273781776428
train gradient:  0.16009068960893452
iteration : 11394
train acc:  0.7578125
train loss:  0.5563889741897583
train gradient:  0.13757408120361825
iteration : 11395
train acc:  0.78125
train loss:  0.4492068290710449
train gradient:  0.12463975158784812
iteration : 11396
train acc:  0.7421875
train loss:  0.46741655468940735
train gradient:  0.14220379236779696
iteration : 11397
train acc:  0.765625
train loss:  0.4655681550502777
train gradient:  0.12434300359537685
iteration : 11398
train acc:  0.828125
train loss:  0.42284637689590454
train gradient:  0.11893568397502506
iteration : 11399
train acc:  0.75
train loss:  0.5061659812927246
train gradient:  0.12858511838830355
iteration : 11400
train acc:  0.7421875
train loss:  0.49960190057754517
train gradient:  0.13504261685624558
iteration : 11401
train acc:  0.7890625
train loss:  0.4321780502796173
train gradient:  0.08344583402132132
iteration : 11402
train acc:  0.71875
train loss:  0.573285698890686
train gradient:  0.16371846320183742
iteration : 11403
train acc:  0.765625
train loss:  0.474037230014801
train gradient:  0.10298835506060933
iteration : 11404
train acc:  0.7109375
train loss:  0.5119948387145996
train gradient:  0.14077252526047934
iteration : 11405
train acc:  0.6796875
train loss:  0.5162566900253296
train gradient:  0.20028622976214255
iteration : 11406
train acc:  0.8046875
train loss:  0.43245744705200195
train gradient:  0.09113954814063165
iteration : 11407
train acc:  0.734375
train loss:  0.4730052947998047
train gradient:  0.14459641944741602
iteration : 11408
train acc:  0.765625
train loss:  0.5090755224227905
train gradient:  0.13013175416702646
iteration : 11409
train acc:  0.71875
train loss:  0.5102174878120422
train gradient:  0.13559049560914493
iteration : 11410
train acc:  0.7734375
train loss:  0.44218188524246216
train gradient:  0.08609421602035439
iteration : 11411
train acc:  0.796875
train loss:  0.45522192120552063
train gradient:  0.13026449615680855
iteration : 11412
train acc:  0.7890625
train loss:  0.41287779808044434
train gradient:  0.08066284230424657
iteration : 11413
train acc:  0.734375
train loss:  0.5090324878692627
train gradient:  0.13538301278548542
iteration : 11414
train acc:  0.6875
train loss:  0.4980328381061554
train gradient:  0.12404564841024063
iteration : 11415
train acc:  0.703125
train loss:  0.5477190613746643
train gradient:  0.17555800483673759
iteration : 11416
train acc:  0.796875
train loss:  0.47217991948127747
train gradient:  0.13325558775108126
iteration : 11417
train acc:  0.765625
train loss:  0.4781855344772339
train gradient:  0.14750513649692942
iteration : 11418
train acc:  0.796875
train loss:  0.44870737195014954
train gradient:  0.12564040890440548
iteration : 11419
train acc:  0.7734375
train loss:  0.4771755635738373
train gradient:  0.11252438682891218
iteration : 11420
train acc:  0.6875
train loss:  0.5580731630325317
train gradient:  0.15576144440258374
iteration : 11421
train acc:  0.6875
train loss:  0.5391657948493958
train gradient:  0.12955937173424362
iteration : 11422
train acc:  0.6796875
train loss:  0.6106066703796387
train gradient:  0.16222064235408518
iteration : 11423
train acc:  0.8125
train loss:  0.4438301920890808
train gradient:  0.12913718962919843
iteration : 11424
train acc:  0.7890625
train loss:  0.4206539988517761
train gradient:  0.09821548832835017
iteration : 11425
train acc:  0.75
train loss:  0.46558406949043274
train gradient:  0.11956942830523992
iteration : 11426
train acc:  0.7578125
train loss:  0.494689404964447
train gradient:  0.11139612791886577
iteration : 11427
train acc:  0.7578125
train loss:  0.46859127283096313
train gradient:  0.11399310912376544
iteration : 11428
train acc:  0.75
train loss:  0.43989264965057373
train gradient:  0.10286996526482893
iteration : 11429
train acc:  0.7265625
train loss:  0.5093299746513367
train gradient:  0.12501534909916803
iteration : 11430
train acc:  0.71875
train loss:  0.49719181656837463
train gradient:  0.17200300658377438
iteration : 11431
train acc:  0.7890625
train loss:  0.42258843779563904
train gradient:  0.0966836433705098
iteration : 11432
train acc:  0.703125
train loss:  0.5004792213439941
train gradient:  0.11026439767302434
iteration : 11433
train acc:  0.765625
train loss:  0.4627295434474945
train gradient:  0.09075881708559654
iteration : 11434
train acc:  0.7421875
train loss:  0.5577225685119629
train gradient:  0.15445873225974643
iteration : 11435
train acc:  0.7421875
train loss:  0.48408347368240356
train gradient:  0.10954771416576893
iteration : 11436
train acc:  0.75
train loss:  0.44690194725990295
train gradient:  0.11021423885819741
iteration : 11437
train acc:  0.8125
train loss:  0.42704036831855774
train gradient:  0.11721101330208
iteration : 11438
train acc:  0.734375
train loss:  0.5132565498352051
train gradient:  0.1677216887678693
iteration : 11439
train acc:  0.7421875
train loss:  0.5296635627746582
train gradient:  0.1346834508387876
iteration : 11440
train acc:  0.7734375
train loss:  0.4808424115180969
train gradient:  0.15671705286995502
iteration : 11441
train acc:  0.703125
train loss:  0.5482029914855957
train gradient:  0.1989019563659186
iteration : 11442
train acc:  0.7421875
train loss:  0.5000467300415039
train gradient:  0.14678793474507232
iteration : 11443
train acc:  0.734375
train loss:  0.4596492648124695
train gradient:  0.11909744085525166
iteration : 11444
train acc:  0.7109375
train loss:  0.4714987874031067
train gradient:  0.11230888211326413
iteration : 11445
train acc:  0.734375
train loss:  0.4812397360801697
train gradient:  0.13681099328344787
iteration : 11446
train acc:  0.734375
train loss:  0.538379967212677
train gradient:  0.1482917546382105
iteration : 11447
train acc:  0.7109375
train loss:  0.5379549264907837
train gradient:  0.15438346854985502
iteration : 11448
train acc:  0.7265625
train loss:  0.5699336528778076
train gradient:  0.18161117171386804
iteration : 11449
train acc:  0.6953125
train loss:  0.5440595149993896
train gradient:  0.15076596769106693
iteration : 11450
train acc:  0.7734375
train loss:  0.49066415429115295
train gradient:  0.11511959651067905
iteration : 11451
train acc:  0.7421875
train loss:  0.49273937940597534
train gradient:  0.1186145986712644
iteration : 11452
train acc:  0.7578125
train loss:  0.44618886709213257
train gradient:  0.0943535255719169
iteration : 11453
train acc:  0.7421875
train loss:  0.4734921455383301
train gradient:  0.11125310478859325
iteration : 11454
train acc:  0.7890625
train loss:  0.4461277723312378
train gradient:  0.10611493040766203
iteration : 11455
train acc:  0.734375
train loss:  0.5181879997253418
train gradient:  0.14097775468274065
iteration : 11456
train acc:  0.734375
train loss:  0.4829680323600769
train gradient:  0.137817294027141
iteration : 11457
train acc:  0.75
train loss:  0.4909658432006836
train gradient:  0.12807914840672846
iteration : 11458
train acc:  0.6953125
train loss:  0.5183238983154297
train gradient:  0.16056338085713878
iteration : 11459
train acc:  0.8203125
train loss:  0.43911463022232056
train gradient:  0.11240670529802341
iteration : 11460
train acc:  0.78125
train loss:  0.44073885679244995
train gradient:  0.09658582097390427
iteration : 11461
train acc:  0.703125
train loss:  0.46093353629112244
train gradient:  0.10056325350350964
iteration : 11462
train acc:  0.7265625
train loss:  0.5125422477722168
train gradient:  0.12094619852582242
iteration : 11463
train acc:  0.7890625
train loss:  0.4429326057434082
train gradient:  0.10166647879515354
iteration : 11464
train acc:  0.7890625
train loss:  0.40805917978286743
train gradient:  0.09212791260280524
iteration : 11465
train acc:  0.734375
train loss:  0.5288118720054626
train gradient:  0.11961512377652705
iteration : 11466
train acc:  0.8203125
train loss:  0.4180678725242615
train gradient:  0.07940030339049355
iteration : 11467
train acc:  0.7421875
train loss:  0.4598124027252197
train gradient:  0.11558501191224026
iteration : 11468
train acc:  0.7578125
train loss:  0.48699504137039185
train gradient:  0.1376944202614392
iteration : 11469
train acc:  0.765625
train loss:  0.4822744131088257
train gradient:  0.11816045610185544
iteration : 11470
train acc:  0.7421875
train loss:  0.5029469728469849
train gradient:  0.13605759109639898
iteration : 11471
train acc:  0.71875
train loss:  0.5909641981124878
train gradient:  0.14965145357436244
iteration : 11472
train acc:  0.734375
train loss:  0.4634951055049896
train gradient:  0.10333818375284497
iteration : 11473
train acc:  0.765625
train loss:  0.43289536237716675
train gradient:  0.09715625265435035
iteration : 11474
train acc:  0.7265625
train loss:  0.6142094731330872
train gradient:  0.185761381553659
iteration : 11475
train acc:  0.7109375
train loss:  0.47590917348861694
train gradient:  0.11156669183517122
iteration : 11476
train acc:  0.6953125
train loss:  0.5343374609947205
train gradient:  0.14272141223161683
iteration : 11477
train acc:  0.8125
train loss:  0.43712517619132996
train gradient:  0.09769262426470014
iteration : 11478
train acc:  0.71875
train loss:  0.4796735644340515
train gradient:  0.11958137863468897
iteration : 11479
train acc:  0.7421875
train loss:  0.45344865322113037
train gradient:  0.10572391897799911
iteration : 11480
train acc:  0.7578125
train loss:  0.4647175669670105
train gradient:  0.08547192005826434
iteration : 11481
train acc:  0.703125
train loss:  0.5541028380393982
train gradient:  0.12554778145207504
iteration : 11482
train acc:  0.7734375
train loss:  0.48796218633651733
train gradient:  0.11427366845876233
iteration : 11483
train acc:  0.7265625
train loss:  0.4817124605178833
train gradient:  0.1704877630440207
iteration : 11484
train acc:  0.6953125
train loss:  0.6040887236595154
train gradient:  0.20909814770429028
iteration : 11485
train acc:  0.796875
train loss:  0.46841728687286377
train gradient:  0.11748792839800913
iteration : 11486
train acc:  0.7734375
train loss:  0.46364325284957886
train gradient:  0.1111738226723178
iteration : 11487
train acc:  0.8125
train loss:  0.4544834792613983
train gradient:  0.10083544802823255
iteration : 11488
train acc:  0.765625
train loss:  0.489439994096756
train gradient:  0.12239342365027274
iteration : 11489
train acc:  0.7421875
train loss:  0.46938788890838623
train gradient:  0.1266173113540684
iteration : 11490
train acc:  0.78125
train loss:  0.4840889573097229
train gradient:  0.15218239592797195
iteration : 11491
train acc:  0.7109375
train loss:  0.49493587017059326
train gradient:  0.14828584552742843
iteration : 11492
train acc:  0.78125
train loss:  0.4201120138168335
train gradient:  0.07630837604184457
iteration : 11493
train acc:  0.78125
train loss:  0.4512471556663513
train gradient:  0.11726467927566946
iteration : 11494
train acc:  0.78125
train loss:  0.4549059271812439
train gradient:  0.10743824506696065
iteration : 11495
train acc:  0.7578125
train loss:  0.5369619131088257
train gradient:  0.1298471628272816
iteration : 11496
train acc:  0.6875
train loss:  0.54300856590271
train gradient:  0.14496768677125033
iteration : 11497
train acc:  0.71875
train loss:  0.5279948711395264
train gradient:  0.11989159649198773
iteration : 11498
train acc:  0.78125
train loss:  0.4824763536453247
train gradient:  0.19462803698765246
iteration : 11499
train acc:  0.78125
train loss:  0.4256893992424011
train gradient:  0.09341432026961555
iteration : 11500
train acc:  0.7421875
train loss:  0.5504586100578308
train gradient:  0.1725854259978543
iteration : 11501
train acc:  0.7421875
train loss:  0.5293850898742676
train gradient:  0.15592297970935584
iteration : 11502
train acc:  0.7890625
train loss:  0.4811047613620758
train gradient:  0.13085827777837028
iteration : 11503
train acc:  0.7109375
train loss:  0.5812197923660278
train gradient:  0.16458475660420785
iteration : 11504
train acc:  0.734375
train loss:  0.4957672357559204
train gradient:  0.130065296102951
iteration : 11505
train acc:  0.7890625
train loss:  0.49069297313690186
train gradient:  0.10964878177583491
iteration : 11506
train acc:  0.78125
train loss:  0.41826796531677246
train gradient:  0.11329826107560992
iteration : 11507
train acc:  0.703125
train loss:  0.48789533972740173
train gradient:  0.11663456345361412
iteration : 11508
train acc:  0.7421875
train loss:  0.4434751570224762
train gradient:  0.09431689151493479
iteration : 11509
train acc:  0.75
train loss:  0.4877837300300598
train gradient:  0.12981036252657874
iteration : 11510
train acc:  0.75
train loss:  0.47378093004226685
train gradient:  0.13561985791310707
iteration : 11511
train acc:  0.765625
train loss:  0.474896103143692
train gradient:  0.09914828164356382
iteration : 11512
train acc:  0.6953125
train loss:  0.4997546076774597
train gradient:  0.10852947243502496
iteration : 11513
train acc:  0.6953125
train loss:  0.5451830625534058
train gradient:  0.1341881136495096
iteration : 11514
train acc:  0.7265625
train loss:  0.47417372465133667
train gradient:  0.12981072990648915
iteration : 11515
train acc:  0.703125
train loss:  0.5669869184494019
train gradient:  0.22675910562254065
iteration : 11516
train acc:  0.703125
train loss:  0.5567161440849304
train gradient:  0.139252107211278
iteration : 11517
train acc:  0.7578125
train loss:  0.49816590547561646
train gradient:  0.13962045556746672
iteration : 11518
train acc:  0.734375
train loss:  0.4825209379196167
train gradient:  0.14673581355364007
iteration : 11519
train acc:  0.7578125
train loss:  0.4845021963119507
train gradient:  0.13945125539805003
iteration : 11520
train acc:  0.734375
train loss:  0.4652957320213318
train gradient:  0.11752966422773423
iteration : 11521
train acc:  0.75
train loss:  0.5167148113250732
train gradient:  0.13467165316577445
iteration : 11522
train acc:  0.734375
train loss:  0.4380895495414734
train gradient:  0.10641828362880862
iteration : 11523
train acc:  0.796875
train loss:  0.4218003749847412
train gradient:  0.09297083863688733
iteration : 11524
train acc:  0.75
train loss:  0.5401428937911987
train gradient:  0.18532686673946308
iteration : 11525
train acc:  0.7734375
train loss:  0.4652877151966095
train gradient:  0.11854381272168937
iteration : 11526
train acc:  0.71875
train loss:  0.477858304977417
train gradient:  0.132917350978329
iteration : 11527
train acc:  0.75
train loss:  0.5196130275726318
train gradient:  0.14326532393250552
iteration : 11528
train acc:  0.734375
train loss:  0.5162478685379028
train gradient:  0.11296167667361974
iteration : 11529
train acc:  0.7734375
train loss:  0.48216280341148376
train gradient:  0.10393243884353648
iteration : 11530
train acc:  0.7421875
train loss:  0.5394595861434937
train gradient:  0.13105712746509782
iteration : 11531
train acc:  0.71875
train loss:  0.5139588117599487
train gradient:  0.10607385882607091
iteration : 11532
train acc:  0.8046875
train loss:  0.414423406124115
train gradient:  0.11448060933941531
iteration : 11533
train acc:  0.734375
train loss:  0.5371464490890503
train gradient:  0.1691513327670882
iteration : 11534
train acc:  0.7265625
train loss:  0.5141100883483887
train gradient:  0.10081404942867622
iteration : 11535
train acc:  0.7578125
train loss:  0.4739520847797394
train gradient:  0.10086836744096112
iteration : 11536
train acc:  0.7109375
train loss:  0.5428107976913452
train gradient:  0.151125884401711
iteration : 11537
train acc:  0.7265625
train loss:  0.5506019592285156
train gradient:  0.17589684138663675
iteration : 11538
train acc:  0.734375
train loss:  0.46381425857543945
train gradient:  0.12642731668669133
iteration : 11539
train acc:  0.7421875
train loss:  0.475281298160553
train gradient:  0.12497678781929981
iteration : 11540
train acc:  0.734375
train loss:  0.5089958906173706
train gradient:  0.11319600740952558
iteration : 11541
train acc:  0.7265625
train loss:  0.4919487535953522
train gradient:  0.1216514242213669
iteration : 11542
train acc:  0.84375
train loss:  0.38080891966819763
train gradient:  0.06930423013901524
iteration : 11543
train acc:  0.78125
train loss:  0.4294390380382538
train gradient:  0.10312511362753378
iteration : 11544
train acc:  0.7265625
train loss:  0.5476330518722534
train gradient:  0.13121762166139292
iteration : 11545
train acc:  0.7890625
train loss:  0.45614898204803467
train gradient:  0.12587890546913405
iteration : 11546
train acc:  0.796875
train loss:  0.486034631729126
train gradient:  0.10672721469277535
iteration : 11547
train acc:  0.8203125
train loss:  0.3973386883735657
train gradient:  0.08004327523873353
iteration : 11548
train acc:  0.7109375
train loss:  0.5599345564842224
train gradient:  0.1853601243689415
iteration : 11549
train acc:  0.71875
train loss:  0.510077714920044
train gradient:  0.10699695439707715
iteration : 11550
train acc:  0.8046875
train loss:  0.4605691730976105
train gradient:  0.120848069838495
iteration : 11551
train acc:  0.7890625
train loss:  0.4507031738758087
train gradient:  0.09049350759980862
iteration : 11552
train acc:  0.7421875
train loss:  0.4363439083099365
train gradient:  0.12613866412399444
iteration : 11553
train acc:  0.7109375
train loss:  0.5161779522895813
train gradient:  0.12785241988391824
iteration : 11554
train acc:  0.7421875
train loss:  0.5149874091148376
train gradient:  0.1154959221836719
iteration : 11555
train acc:  0.7109375
train loss:  0.5922064781188965
train gradient:  0.1845353555951183
iteration : 11556
train acc:  0.734375
train loss:  0.5058461427688599
train gradient:  0.13671547382216886
iteration : 11557
train acc:  0.7109375
train loss:  0.6010406017303467
train gradient:  0.16263200579835324
iteration : 11558
train acc:  0.78125
train loss:  0.4264379143714905
train gradient:  0.11329185450286448
iteration : 11559
train acc:  0.796875
train loss:  0.48905038833618164
train gradient:  0.1602701104947129
iteration : 11560
train acc:  0.7734375
train loss:  0.4816672205924988
train gradient:  0.09670788690309641
iteration : 11561
train acc:  0.7421875
train loss:  0.4882411062717438
train gradient:  0.1301201818368095
iteration : 11562
train acc:  0.8203125
train loss:  0.40284448862075806
train gradient:  0.08439527069677916
iteration : 11563
train acc:  0.734375
train loss:  0.4875270128250122
train gradient:  0.15931384623052314
iteration : 11564
train acc:  0.75
train loss:  0.499405175447464
train gradient:  0.11283076139691092
iteration : 11565
train acc:  0.7109375
train loss:  0.6013742089271545
train gradient:  0.14984327357223345
iteration : 11566
train acc:  0.7109375
train loss:  0.49090445041656494
train gradient:  0.1392790280038029
iteration : 11567
train acc:  0.6640625
train loss:  0.5312920808792114
train gradient:  0.18815203812585257
iteration : 11568
train acc:  0.7265625
train loss:  0.5381207466125488
train gradient:  0.14793861003650174
iteration : 11569
train acc:  0.6953125
train loss:  0.5873212218284607
train gradient:  0.13009333434709525
iteration : 11570
train acc:  0.8203125
train loss:  0.4388471245765686
train gradient:  0.09684842964959109
iteration : 11571
train acc:  0.765625
train loss:  0.45355838537216187
train gradient:  0.10097342173172966
iteration : 11572
train acc:  0.7734375
train loss:  0.47796058654785156
train gradient:  0.10923515400195556
iteration : 11573
train acc:  0.7421875
train loss:  0.4963751435279846
train gradient:  0.11451236672847648
iteration : 11574
train acc:  0.703125
train loss:  0.5255115032196045
train gradient:  0.14314364413737196
iteration : 11575
train acc:  0.734375
train loss:  0.4532417356967926
train gradient:  0.11162705730861251
iteration : 11576
train acc:  0.765625
train loss:  0.4471698999404907
train gradient:  0.12071873635152071
iteration : 11577
train acc:  0.75
train loss:  0.478489488363266
train gradient:  0.11175728102509783
iteration : 11578
train acc:  0.71875
train loss:  0.5651917457580566
train gradient:  0.14720810711299526
iteration : 11579
train acc:  0.78125
train loss:  0.4524296820163727
train gradient:  0.10700574998527597
iteration : 11580
train acc:  0.7265625
train loss:  0.4975319802761078
train gradient:  0.1210065721322084
iteration : 11581
train acc:  0.7265625
train loss:  0.4762675166130066
train gradient:  0.10406846058014924
iteration : 11582
train acc:  0.7265625
train loss:  0.514858603477478
train gradient:  0.17279195861256022
iteration : 11583
train acc:  0.65625
train loss:  0.539808452129364
train gradient:  0.12257606410200975
iteration : 11584
train acc:  0.7265625
train loss:  0.5436087846755981
train gradient:  0.14993797347790863
iteration : 11585
train acc:  0.75
train loss:  0.47445982694625854
train gradient:  0.09249025422265586
iteration : 11586
train acc:  0.7421875
train loss:  0.4905245006084442
train gradient:  0.1204033254547657
iteration : 11587
train acc:  0.734375
train loss:  0.5258691906929016
train gradient:  0.13513261286660566
iteration : 11588
train acc:  0.7421875
train loss:  0.5060372352600098
train gradient:  0.14315842928057093
iteration : 11589
train acc:  0.7421875
train loss:  0.534348726272583
train gradient:  0.12952049029314355
iteration : 11590
train acc:  0.7890625
train loss:  0.4324333965778351
train gradient:  0.09848514095320944
iteration : 11591
train acc:  0.78125
train loss:  0.4837022125720978
train gradient:  0.13443459585355985
iteration : 11592
train acc:  0.765625
train loss:  0.4513801336288452
train gradient:  0.11864726411680726
iteration : 11593
train acc:  0.78125
train loss:  0.41814762353897095
train gradient:  0.08857887083932976
iteration : 11594
train acc:  0.6953125
train loss:  0.5191022157669067
train gradient:  0.1393217022764241
iteration : 11595
train acc:  0.71875
train loss:  0.5508723258972168
train gradient:  0.16730634284265944
iteration : 11596
train acc:  0.7265625
train loss:  0.4651017189025879
train gradient:  0.08837775583020821
iteration : 11597
train acc:  0.75
train loss:  0.46238380670547485
train gradient:  0.09192514438844886
iteration : 11598
train acc:  0.703125
train loss:  0.5247865915298462
train gradient:  0.12918246677488066
iteration : 11599
train acc:  0.6953125
train loss:  0.4983130991458893
train gradient:  0.14334055588022404
iteration : 11600
train acc:  0.71875
train loss:  0.522178590297699
train gradient:  0.11980862062700864
iteration : 11601
train acc:  0.734375
train loss:  0.5263569951057434
train gradient:  0.12566103839732035
iteration : 11602
train acc:  0.7578125
train loss:  0.4799911379814148
train gradient:  0.1010359733830995
iteration : 11603
train acc:  0.7421875
train loss:  0.47128647565841675
train gradient:  0.11959017860341145
iteration : 11604
train acc:  0.7578125
train loss:  0.45174485445022583
train gradient:  0.1169326400711216
iteration : 11605
train acc:  0.78125
train loss:  0.4588760733604431
train gradient:  0.11316650690456836
iteration : 11606
train acc:  0.6796875
train loss:  0.5385271310806274
train gradient:  0.15717295646651458
iteration : 11607
train acc:  0.8046875
train loss:  0.4420560896396637
train gradient:  0.09001276145376556
iteration : 11608
train acc:  0.6953125
train loss:  0.5368277430534363
train gradient:  0.15159516782039512
iteration : 11609
train acc:  0.765625
train loss:  0.4925333857536316
train gradient:  0.11513699511054461
iteration : 11610
train acc:  0.78125
train loss:  0.4235929846763611
train gradient:  0.16551559340213912
iteration : 11611
train acc:  0.75
train loss:  0.484355092048645
train gradient:  0.12157870084690126
iteration : 11612
train acc:  0.8125
train loss:  0.45344990491867065
train gradient:  0.10514279450283112
iteration : 11613
train acc:  0.75
train loss:  0.44770509004592896
train gradient:  0.11836354898480124
iteration : 11614
train acc:  0.6796875
train loss:  0.5855515003204346
train gradient:  0.14015796992052576
iteration : 11615
train acc:  0.7578125
train loss:  0.5002879500389099
train gradient:  0.1208772150393554
iteration : 11616
train acc:  0.7578125
train loss:  0.45558106899261475
train gradient:  0.11757811420711219
iteration : 11617
train acc:  0.71875
train loss:  0.47754108905792236
train gradient:  0.13458141830310644
iteration : 11618
train acc:  0.765625
train loss:  0.4617915153503418
train gradient:  0.10623113067036756
iteration : 11619
train acc:  0.734375
train loss:  0.5056334733963013
train gradient:  0.10909984247482496
iteration : 11620
train acc:  0.703125
train loss:  0.5681794881820679
train gradient:  0.18181289844488438
iteration : 11621
train acc:  0.7890625
train loss:  0.5145111680030823
train gradient:  0.17893087006274033
iteration : 11622
train acc:  0.7421875
train loss:  0.5549300312995911
train gradient:  0.14826384287065855
iteration : 11623
train acc:  0.84375
train loss:  0.44250917434692383
train gradient:  0.0942952161866868
iteration : 11624
train acc:  0.7265625
train loss:  0.4834485352039337
train gradient:  0.1346750674834195
iteration : 11625
train acc:  0.78125
train loss:  0.46764662861824036
train gradient:  0.11772490747798263
iteration : 11626
train acc:  0.8046875
train loss:  0.4355918765068054
train gradient:  0.11239438901116888
iteration : 11627
train acc:  0.75
train loss:  0.5076847076416016
train gradient:  0.12396755001650914
iteration : 11628
train acc:  0.6953125
train loss:  0.5228163003921509
train gradient:  0.14077418256071156
iteration : 11629
train acc:  0.765625
train loss:  0.43640682101249695
train gradient:  0.09046882504818687
iteration : 11630
train acc:  0.7578125
train loss:  0.5142266750335693
train gradient:  0.11466007611746969
iteration : 11631
train acc:  0.71875
train loss:  0.5242860317230225
train gradient:  0.14420779927478078
iteration : 11632
train acc:  0.7890625
train loss:  0.4765297472476959
train gradient:  0.1146429868859239
iteration : 11633
train acc:  0.75
train loss:  0.5220775008201599
train gradient:  0.11870426166816848
iteration : 11634
train acc:  0.734375
train loss:  0.47335073351860046
train gradient:  0.12466281159065734
iteration : 11635
train acc:  0.7890625
train loss:  0.4450511336326599
train gradient:  0.09974230910687092
iteration : 11636
train acc:  0.78125
train loss:  0.5082226991653442
train gradient:  0.12622621446770604
iteration : 11637
train acc:  0.78125
train loss:  0.4172609746456146
train gradient:  0.09491186343863339
iteration : 11638
train acc:  0.75
train loss:  0.4985327124595642
train gradient:  0.12732430627548208
iteration : 11639
train acc:  0.7890625
train loss:  0.46513232588768005
train gradient:  0.1314937722007723
iteration : 11640
train acc:  0.7421875
train loss:  0.490913450717926
train gradient:  0.12172835832408167
iteration : 11641
train acc:  0.734375
train loss:  0.540276288986206
train gradient:  0.12631325788483205
iteration : 11642
train acc:  0.75
train loss:  0.4779214560985565
train gradient:  0.1207523984586471
iteration : 11643
train acc:  0.75
train loss:  0.4518700838088989
train gradient:  0.10648639368244027
iteration : 11644
train acc:  0.7109375
train loss:  0.5361596345901489
train gradient:  0.14161024569165048
iteration : 11645
train acc:  0.7109375
train loss:  0.5454860925674438
train gradient:  0.15439334419220382
iteration : 11646
train acc:  0.7734375
train loss:  0.41548019647598267
train gradient:  0.10241494821352007
iteration : 11647
train acc:  0.75
train loss:  0.471137136220932
train gradient:  0.1105000674576673
iteration : 11648
train acc:  0.78125
train loss:  0.5118746757507324
train gradient:  0.12262517990714375
iteration : 11649
train acc:  0.6953125
train loss:  0.5074493288993835
train gradient:  0.12802026396840452
iteration : 11650
train acc:  0.7421875
train loss:  0.46933627128601074
train gradient:  0.10591512170144018
iteration : 11651
train acc:  0.703125
train loss:  0.4533320963382721
train gradient:  0.09830784876192336
iteration : 11652
train acc:  0.8046875
train loss:  0.43816977739334106
train gradient:  0.10906081780306466
iteration : 11653
train acc:  0.75
train loss:  0.4891514778137207
train gradient:  0.12096259139104444
iteration : 11654
train acc:  0.7265625
train loss:  0.502311110496521
train gradient:  0.2224745421834522
iteration : 11655
train acc:  0.6875
train loss:  0.5492328405380249
train gradient:  0.14034581272457344
iteration : 11656
train acc:  0.71875
train loss:  0.524743914604187
train gradient:  0.14219022600471395
iteration : 11657
train acc:  0.7109375
train loss:  0.5492210388183594
train gradient:  0.1375771031262672
iteration : 11658
train acc:  0.7734375
train loss:  0.4781060516834259
train gradient:  0.12435483027936088
iteration : 11659
train acc:  0.78125
train loss:  0.46347302198410034
train gradient:  0.13202525665929854
iteration : 11660
train acc:  0.71875
train loss:  0.5225754976272583
train gradient:  0.10258077467808303
iteration : 11661
train acc:  0.6875
train loss:  0.5434137582778931
train gradient:  0.16361724473838626
iteration : 11662
train acc:  0.71875
train loss:  0.47964048385620117
train gradient:  0.13370522269665358
iteration : 11663
train acc:  0.703125
train loss:  0.5523779392242432
train gradient:  0.1717756467109661
iteration : 11664
train acc:  0.7734375
train loss:  0.48902982473373413
train gradient:  0.12634988526810217
iteration : 11665
train acc:  0.78125
train loss:  0.5038118362426758
train gradient:  0.13681478703333783
iteration : 11666
train acc:  0.7265625
train loss:  0.5401714444160461
train gradient:  0.129987049296457
iteration : 11667
train acc:  0.7578125
train loss:  0.4769095778465271
train gradient:  0.10124895775209597
iteration : 11668
train acc:  0.78125
train loss:  0.4335862994194031
train gradient:  0.10573218580330311
iteration : 11669
train acc:  0.7265625
train loss:  0.5076024532318115
train gradient:  0.14886281168056048
iteration : 11670
train acc:  0.8203125
train loss:  0.46961456537246704
train gradient:  0.10574854760677696
iteration : 11671
train acc:  0.734375
train loss:  0.5035167336463928
train gradient:  0.12152290359885602
iteration : 11672
train acc:  0.796875
train loss:  0.4391975700855255
train gradient:  0.09682466710374062
iteration : 11673
train acc:  0.7421875
train loss:  0.5087523460388184
train gradient:  0.1322074896384221
iteration : 11674
train acc:  0.71875
train loss:  0.48431146144866943
train gradient:  0.10971838880291589
iteration : 11675
train acc:  0.8046875
train loss:  0.5013232231140137
train gradient:  0.18152601076010041
iteration : 11676
train acc:  0.7734375
train loss:  0.461630642414093
train gradient:  0.12331468184843132
iteration : 11677
train acc:  0.7265625
train loss:  0.4724612832069397
train gradient:  0.1302846684404119
iteration : 11678
train acc:  0.796875
train loss:  0.4277893006801605
train gradient:  0.10001098223919792
iteration : 11679
train acc:  0.6796875
train loss:  0.5576391220092773
train gradient:  0.1423909411121207
iteration : 11680
train acc:  0.7890625
train loss:  0.42619290947914124
train gradient:  0.08636740762093204
iteration : 11681
train acc:  0.78125
train loss:  0.4854794442653656
train gradient:  0.12130256532428996
iteration : 11682
train acc:  0.78125
train loss:  0.44319289922714233
train gradient:  0.15034563801997097
iteration : 11683
train acc:  0.71875
train loss:  0.5281431078910828
train gradient:  0.15175197423318104
iteration : 11684
train acc:  0.7109375
train loss:  0.5203284621238708
train gradient:  0.12127602690249425
iteration : 11685
train acc:  0.75
train loss:  0.46932467818260193
train gradient:  0.1380394847539324
iteration : 11686
train acc:  0.734375
train loss:  0.4580910801887512
train gradient:  0.11654429232152869
iteration : 11687
train acc:  0.7734375
train loss:  0.49945759773254395
train gradient:  0.10731083868909914
iteration : 11688
train acc:  0.734375
train loss:  0.47417008876800537
train gradient:  0.10186782277131655
iteration : 11689
train acc:  0.734375
train loss:  0.4591406583786011
train gradient:  0.08303086936620672
iteration : 11690
train acc:  0.796875
train loss:  0.43384382128715515
train gradient:  0.10494050549973162
iteration : 11691
train acc:  0.71875
train loss:  0.5011340379714966
train gradient:  0.12884890238317764
iteration : 11692
train acc:  0.734375
train loss:  0.5236328840255737
train gradient:  0.10528987097123578
iteration : 11693
train acc:  0.8046875
train loss:  0.4315757751464844
train gradient:  0.08656262480685727
iteration : 11694
train acc:  0.75
train loss:  0.5210875868797302
train gradient:  0.16855728327757197
iteration : 11695
train acc:  0.7578125
train loss:  0.443083256483078
train gradient:  0.10223172794640746
iteration : 11696
train acc:  0.7734375
train loss:  0.43900221586227417
train gradient:  0.08841465381977652
iteration : 11697
train acc:  0.703125
train loss:  0.5416488647460938
train gradient:  0.14532795170201823
iteration : 11698
train acc:  0.75
train loss:  0.4966179430484772
train gradient:  0.1139603733732155
iteration : 11699
train acc:  0.75
train loss:  0.4957980513572693
train gradient:  0.10211053015925597
iteration : 11700
train acc:  0.765625
train loss:  0.5266591310501099
train gradient:  0.11734292400389029
iteration : 11701
train acc:  0.78125
train loss:  0.45567935705184937
train gradient:  0.1034144637376622
iteration : 11702
train acc:  0.8125
train loss:  0.45838549733161926
train gradient:  0.09499046213633339
iteration : 11703
train acc:  0.8203125
train loss:  0.4701169729232788
train gradient:  0.11678359770887137
iteration : 11704
train acc:  0.7265625
train loss:  0.49855005741119385
train gradient:  0.12499239164998743
iteration : 11705
train acc:  0.765625
train loss:  0.4626261591911316
train gradient:  0.09789644346387809
iteration : 11706
train acc:  0.7265625
train loss:  0.5116223692893982
train gradient:  0.12900510274420127
iteration : 11707
train acc:  0.6953125
train loss:  0.525348424911499
train gradient:  0.1524905755120801
iteration : 11708
train acc:  0.7265625
train loss:  0.4795420169830322
train gradient:  0.10593162744745402
iteration : 11709
train acc:  0.6796875
train loss:  0.5546551942825317
train gradient:  0.14229252459274555
iteration : 11710
train acc:  0.7734375
train loss:  0.4588741660118103
train gradient:  0.09553159504230617
iteration : 11711
train acc:  0.734375
train loss:  0.4565191864967346
train gradient:  0.12601265822134183
iteration : 11712
train acc:  0.7578125
train loss:  0.4627504348754883
train gradient:  0.11473079775617534
iteration : 11713
train acc:  0.671875
train loss:  0.5786906480789185
train gradient:  0.1714680150273558
iteration : 11714
train acc:  0.7109375
train loss:  0.5008100867271423
train gradient:  0.15866080252487624
iteration : 11715
train acc:  0.6796875
train loss:  0.5543174743652344
train gradient:  0.16568513563713233
iteration : 11716
train acc:  0.7890625
train loss:  0.41151002049446106
train gradient:  0.08296104993655852
iteration : 11717
train acc:  0.75
train loss:  0.5268645286560059
train gradient:  0.225500648116636
iteration : 11718
train acc:  0.7578125
train loss:  0.5042393207550049
train gradient:  0.12094394226099545
iteration : 11719
train acc:  0.8125
train loss:  0.4154270887374878
train gradient:  0.0866882577345808
iteration : 11720
train acc:  0.7265625
train loss:  0.4726563096046448
train gradient:  0.12552493701469666
iteration : 11721
train acc:  0.71875
train loss:  0.532241940498352
train gradient:  0.18711517118475188
iteration : 11722
train acc:  0.75
train loss:  0.47304999828338623
train gradient:  0.13548229728795735
iteration : 11723
train acc:  0.7890625
train loss:  0.43464553356170654
train gradient:  0.11179433063190321
iteration : 11724
train acc:  0.7578125
train loss:  0.5009700655937195
train gradient:  0.1582617434970221
iteration : 11725
train acc:  0.8046875
train loss:  0.46750178933143616
train gradient:  0.12112797587861443
iteration : 11726
train acc:  0.7109375
train loss:  0.5311782360076904
train gradient:  0.14339831138985304
iteration : 11727
train acc:  0.78125
train loss:  0.4786737263202667
train gradient:  0.10182174416233024
iteration : 11728
train acc:  0.75
train loss:  0.5011807680130005
train gradient:  0.1344043589083664
iteration : 11729
train acc:  0.75
train loss:  0.5168074369430542
train gradient:  0.14242246382757612
iteration : 11730
train acc:  0.6953125
train loss:  0.5836248397827148
train gradient:  0.16149208472236826
iteration : 11731
train acc:  0.8125
train loss:  0.44763755798339844
train gradient:  0.09996179617137414
iteration : 11732
train acc:  0.75
train loss:  0.4402172565460205
train gradient:  0.1267773851391864
iteration : 11733
train acc:  0.7265625
train loss:  0.550332248210907
train gradient:  0.1746824910384212
iteration : 11734
train acc:  0.7265625
train loss:  0.4733045697212219
train gradient:  0.1186153750902304
iteration : 11735
train acc:  0.734375
train loss:  0.4857824444770813
train gradient:  0.1159450985407718
iteration : 11736
train acc:  0.75
train loss:  0.44300317764282227
train gradient:  0.1161295247069682
iteration : 11737
train acc:  0.703125
train loss:  0.4654166102409363
train gradient:  0.11025162560551105
iteration : 11738
train acc:  0.71875
train loss:  0.5405991673469543
train gradient:  0.15142538215059942
iteration : 11739
train acc:  0.71875
train loss:  0.5051189661026001
train gradient:  0.13642362985297357
iteration : 11740
train acc:  0.671875
train loss:  0.5352752208709717
train gradient:  0.14509813490915763
iteration : 11741
train acc:  0.703125
train loss:  0.5648272633552551
train gradient:  0.20255193115850517
iteration : 11742
train acc:  0.796875
train loss:  0.47916319966316223
train gradient:  0.14968288579904354
iteration : 11743
train acc:  0.7421875
train loss:  0.49282658100128174
train gradient:  0.130508530893653
iteration : 11744
train acc:  0.8203125
train loss:  0.4035351574420929
train gradient:  0.09317708043535232
iteration : 11745
train acc:  0.7421875
train loss:  0.5820128917694092
train gradient:  0.14678151496480007
iteration : 11746
train acc:  0.734375
train loss:  0.4932725131511688
train gradient:  0.11898843635096464
iteration : 11747
train acc:  0.7109375
train loss:  0.5282871723175049
train gradient:  0.1319838354144336
iteration : 11748
train acc:  0.765625
train loss:  0.49335038661956787
train gradient:  0.12479861189292288
iteration : 11749
train acc:  0.8125
train loss:  0.3958967328071594
train gradient:  0.0784305370405288
iteration : 11750
train acc:  0.75
train loss:  0.5379579663276672
train gradient:  0.13714185590131223
iteration : 11751
train acc:  0.75
train loss:  0.45879465341567993
train gradient:  0.12720656343085116
iteration : 11752
train acc:  0.7890625
train loss:  0.4229639768600464
train gradient:  0.10400218403236594
iteration : 11753
train acc:  0.703125
train loss:  0.5463029742240906
train gradient:  0.14100822217439996
iteration : 11754
train acc:  0.75
train loss:  0.5041395425796509
train gradient:  0.11567899382786143
iteration : 11755
train acc:  0.734375
train loss:  0.47042617201805115
train gradient:  0.10435820747383244
iteration : 11756
train acc:  0.7421875
train loss:  0.49443697929382324
train gradient:  0.1126457728451635
iteration : 11757
train acc:  0.7421875
train loss:  0.4910051226615906
train gradient:  0.11277176595122491
iteration : 11758
train acc:  0.765625
train loss:  0.4736396372318268
train gradient:  0.12555828478675096
iteration : 11759
train acc:  0.6953125
train loss:  0.517998456954956
train gradient:  0.11743111888805247
iteration : 11760
train acc:  0.734375
train loss:  0.5397237539291382
train gradient:  0.12904197225813807
iteration : 11761
train acc:  0.7578125
train loss:  0.4816848635673523
train gradient:  0.12687331905877006
iteration : 11762
train acc:  0.7734375
train loss:  0.5230971574783325
train gradient:  0.1289163520084902
iteration : 11763
train acc:  0.7421875
train loss:  0.5446317791938782
train gradient:  0.1486196587173478
iteration : 11764
train acc:  0.703125
train loss:  0.5084881782531738
train gradient:  0.13027968564812784
iteration : 11765
train acc:  0.75
train loss:  0.4602125585079193
train gradient:  0.10487586829808136
iteration : 11766
train acc:  0.7265625
train loss:  0.5021899938583374
train gradient:  0.1161159822140726
iteration : 11767
train acc:  0.78125
train loss:  0.4272772967815399
train gradient:  0.10311953265259283
iteration : 11768
train acc:  0.7265625
train loss:  0.5400909185409546
train gradient:  0.17460351315898698
iteration : 11769
train acc:  0.671875
train loss:  0.5734726190567017
train gradient:  0.20800518464551596
iteration : 11770
train acc:  0.7265625
train loss:  0.46877521276474
train gradient:  0.1032826108935271
iteration : 11771
train acc:  0.7109375
train loss:  0.51478111743927
train gradient:  0.13385035043219567
iteration : 11772
train acc:  0.765625
train loss:  0.46207213401794434
train gradient:  0.11372694008451564
iteration : 11773
train acc:  0.796875
train loss:  0.420048326253891
train gradient:  0.09385906996529013
iteration : 11774
train acc:  0.8125
train loss:  0.44578561186790466
train gradient:  0.092463713742022
iteration : 11775
train acc:  0.8203125
train loss:  0.43089979887008667
train gradient:  0.0979153890460767
iteration : 11776
train acc:  0.71875
train loss:  0.4710885286331177
train gradient:  0.1066592631228982
iteration : 11777
train acc:  0.71875
train loss:  0.5505977869033813
train gradient:  0.1689361283152001
iteration : 11778
train acc:  0.7578125
train loss:  0.44238388538360596
train gradient:  0.10139341316390184
iteration : 11779
train acc:  0.7109375
train loss:  0.514862060546875
train gradient:  0.127923839849432
iteration : 11780
train acc:  0.828125
train loss:  0.4336526393890381
train gradient:  0.11333566310150756
iteration : 11781
train acc:  0.7109375
train loss:  0.4880150556564331
train gradient:  0.14472537363727583
iteration : 11782
train acc:  0.7734375
train loss:  0.4817793369293213
train gradient:  0.10705130617289342
iteration : 11783
train acc:  0.71875
train loss:  0.5057589411735535
train gradient:  0.12363028083795322
iteration : 11784
train acc:  0.7109375
train loss:  0.5154702663421631
train gradient:  0.13948583090333938
iteration : 11785
train acc:  0.765625
train loss:  0.46787095069885254
train gradient:  0.13682412311972914
iteration : 11786
train acc:  0.7734375
train loss:  0.4530106782913208
train gradient:  0.08572678339595327
iteration : 11787
train acc:  0.7578125
train loss:  0.49875813722610474
train gradient:  0.10593352528651945
iteration : 11788
train acc:  0.7421875
train loss:  0.5135520100593567
train gradient:  0.11614935108640695
iteration : 11789
train acc:  0.75
train loss:  0.4360152781009674
train gradient:  0.1015085037253171
iteration : 11790
train acc:  0.6875
train loss:  0.4967167377471924
train gradient:  0.1088659660438018
iteration : 11791
train acc:  0.7109375
train loss:  0.5428900122642517
train gradient:  0.12428134823730441
iteration : 11792
train acc:  0.78125
train loss:  0.5274056196212769
train gradient:  0.14900763818274765
iteration : 11793
train acc:  0.75
train loss:  0.5170480608940125
train gradient:  0.1286602476149558
iteration : 11794
train acc:  0.7890625
train loss:  0.4548330307006836
train gradient:  0.11081541435956797
iteration : 11795
train acc:  0.703125
train loss:  0.5389511585235596
train gradient:  0.13597927056836404
iteration : 11796
train acc:  0.703125
train loss:  0.5354349613189697
train gradient:  0.13866765773435047
iteration : 11797
train acc:  0.6953125
train loss:  0.5417425036430359
train gradient:  0.1529807708022029
iteration : 11798
train acc:  0.7734375
train loss:  0.5178056955337524
train gradient:  0.14940005825116018
iteration : 11799
train acc:  0.75
train loss:  0.4772709012031555
train gradient:  0.10730254061375602
iteration : 11800
train acc:  0.6953125
train loss:  0.522635817527771
train gradient:  0.1425067506788643
iteration : 11801
train acc:  0.7421875
train loss:  0.5521703958511353
train gradient:  0.14887533308531012
iteration : 11802
train acc:  0.7265625
train loss:  0.4889024496078491
train gradient:  0.12382306932421369
iteration : 11803
train acc:  0.6953125
train loss:  0.5744158029556274
train gradient:  0.1746392438792884
iteration : 11804
train acc:  0.8203125
train loss:  0.44421887397766113
train gradient:  0.10463624474356092
iteration : 11805
train acc:  0.7890625
train loss:  0.4785553216934204
train gradient:  0.11548883493759823
iteration : 11806
train acc:  0.78125
train loss:  0.4660688042640686
train gradient:  0.09624549203505878
iteration : 11807
train acc:  0.7890625
train loss:  0.4854891896247864
train gradient:  0.10956109378193776
iteration : 11808
train acc:  0.7265625
train loss:  0.523996114730835
train gradient:  0.13629063183403084
iteration : 11809
train acc:  0.75
train loss:  0.4657239615917206
train gradient:  0.09367093172503066
iteration : 11810
train acc:  0.7578125
train loss:  0.4619179666042328
train gradient:  0.10135049114628296
iteration : 11811
train acc:  0.6796875
train loss:  0.5773653388023376
train gradient:  0.18637193932417329
iteration : 11812
train acc:  0.734375
train loss:  0.483697772026062
train gradient:  0.13209512934988835
iteration : 11813
train acc:  0.765625
train loss:  0.5103896260261536
train gradient:  0.14493313693227267
iteration : 11814
train acc:  0.796875
train loss:  0.4727938175201416
train gradient:  0.1469198417193997
iteration : 11815
train acc:  0.8046875
train loss:  0.44908568263053894
train gradient:  0.11052735007895009
iteration : 11816
train acc:  0.8359375
train loss:  0.39745795726776123
train gradient:  0.09301615522841687
iteration : 11817
train acc:  0.71875
train loss:  0.5475563406944275
train gradient:  0.14864934485847087
iteration : 11818
train acc:  0.6953125
train loss:  0.5117279887199402
train gradient:  0.1265357400688472
iteration : 11819
train acc:  0.7734375
train loss:  0.4568495452404022
train gradient:  0.10934283503869993
iteration : 11820
train acc:  0.7421875
train loss:  0.4504425525665283
train gradient:  0.09323491586379529
iteration : 11821
train acc:  0.71875
train loss:  0.5726010799407959
train gradient:  0.15439242362190406
iteration : 11822
train acc:  0.6796875
train loss:  0.530555009841919
train gradient:  0.1516409242117174
iteration : 11823
train acc:  0.7890625
train loss:  0.4563566744327545
train gradient:  0.08426577299349104
iteration : 11824
train acc:  0.734375
train loss:  0.45385217666625977
train gradient:  0.11012571159278234
iteration : 11825
train acc:  0.6328125
train loss:  0.5842742919921875
train gradient:  0.179995356356048
iteration : 11826
train acc:  0.78125
train loss:  0.4493798315525055
train gradient:  0.10070159369893504
iteration : 11827
train acc:  0.765625
train loss:  0.46962565183639526
train gradient:  0.1597711317551821
iteration : 11828
train acc:  0.7578125
train loss:  0.4776384234428406
train gradient:  0.10103340912267689
iteration : 11829
train acc:  0.734375
train loss:  0.48005008697509766
train gradient:  0.12296737209207641
iteration : 11830
train acc:  0.7734375
train loss:  0.4329523742198944
train gradient:  0.12728290630754047
iteration : 11831
train acc:  0.7421875
train loss:  0.4496408700942993
train gradient:  0.10651605922777396
iteration : 11832
train acc:  0.8046875
train loss:  0.4051249623298645
train gradient:  0.12602934464652227
iteration : 11833
train acc:  0.7890625
train loss:  0.43843796849250793
train gradient:  0.07875370216825843
iteration : 11834
train acc:  0.7421875
train loss:  0.5069718360900879
train gradient:  0.10949551071918523
iteration : 11835
train acc:  0.765625
train loss:  0.5182593464851379
train gradient:  0.15360999062650993
iteration : 11836
train acc:  0.7421875
train loss:  0.4736567735671997
train gradient:  0.10242266118999264
iteration : 11837
train acc:  0.78125
train loss:  0.42641645669937134
train gradient:  0.09835584043726306
iteration : 11838
train acc:  0.734375
train loss:  0.4647061824798584
train gradient:  0.09505757622815185
iteration : 11839
train acc:  0.7890625
train loss:  0.44917231798171997
train gradient:  0.1315775223157435
iteration : 11840
train acc:  0.734375
train loss:  0.46236491203308105
train gradient:  0.11222214883932237
iteration : 11841
train acc:  0.7734375
train loss:  0.458967924118042
train gradient:  0.10055767520239249
iteration : 11842
train acc:  0.796875
train loss:  0.4817180931568146
train gradient:  0.09250620558933133
iteration : 11843
train acc:  0.7265625
train loss:  0.5451703071594238
train gradient:  0.13197425179005318
iteration : 11844
train acc:  0.7421875
train loss:  0.49637570977211
train gradient:  0.13089744154414268
iteration : 11845
train acc:  0.78125
train loss:  0.489188015460968
train gradient:  0.11074435056914449
iteration : 11846
train acc:  0.71875
train loss:  0.5408107042312622
train gradient:  0.17186113192734792
iteration : 11847
train acc:  0.7109375
train loss:  0.5248705148696899
train gradient:  0.1577832198777258
iteration : 11848
train acc:  0.7578125
train loss:  0.5129983425140381
train gradient:  0.12667051463493623
iteration : 11849
train acc:  0.703125
train loss:  0.5477259159088135
train gradient:  0.1325594544753645
iteration : 11850
train acc:  0.734375
train loss:  0.47665199637413025
train gradient:  0.11566273832574808
iteration : 11851
train acc:  0.6953125
train loss:  0.5118586421012878
train gradient:  0.13113393599881726
iteration : 11852
train acc:  0.7265625
train loss:  0.46970391273498535
train gradient:  0.11925822762945136
iteration : 11853
train acc:  0.765625
train loss:  0.4411587119102478
train gradient:  0.08530837722280145
iteration : 11854
train acc:  0.7421875
train loss:  0.5741446018218994
train gradient:  0.15984055531823918
iteration : 11855
train acc:  0.6953125
train loss:  0.5142694711685181
train gradient:  0.17702908558819386
iteration : 11856
train acc:  0.7421875
train loss:  0.5021985769271851
train gradient:  0.10191976482177983
iteration : 11857
train acc:  0.7109375
train loss:  0.5104480385780334
train gradient:  0.1302708873356178
iteration : 11858
train acc:  0.765625
train loss:  0.49799996614456177
train gradient:  0.13355790996274366
iteration : 11859
train acc:  0.7890625
train loss:  0.4559796154499054
train gradient:  0.10951290511438816
iteration : 11860
train acc:  0.71875
train loss:  0.5186483860015869
train gradient:  0.15028605879291076
iteration : 11861
train acc:  0.734375
train loss:  0.48163923621177673
train gradient:  0.09423221612286856
iteration : 11862
train acc:  0.703125
train loss:  0.5046760439872742
train gradient:  0.12972302175745232
iteration : 11863
train acc:  0.703125
train loss:  0.5435173511505127
train gradient:  0.14130231406999996
iteration : 11864
train acc:  0.703125
train loss:  0.4994848370552063
train gradient:  0.13446081051179287
iteration : 11865
train acc:  0.7265625
train loss:  0.4811302721500397
train gradient:  0.12324555072735408
iteration : 11866
train acc:  0.796875
train loss:  0.43887224793434143
train gradient:  0.10617481735505477
iteration : 11867
train acc:  0.7109375
train loss:  0.5350027680397034
train gradient:  0.1035934691345823
iteration : 11868
train acc:  0.7265625
train loss:  0.4904845356941223
train gradient:  0.14825063291354232
iteration : 11869
train acc:  0.796875
train loss:  0.4430542290210724
train gradient:  0.10615792141263172
iteration : 11870
train acc:  0.7734375
train loss:  0.5236272215843201
train gradient:  0.16084664702193785
iteration : 11871
train acc:  0.7734375
train loss:  0.5213850736618042
train gradient:  0.12702817222560972
iteration : 11872
train acc:  0.765625
train loss:  0.4635772705078125
train gradient:  0.10533632598081846
iteration : 11873
train acc:  0.71875
train loss:  0.46646302938461304
train gradient:  0.08518397454308503
iteration : 11874
train acc:  0.78125
train loss:  0.428117573261261
train gradient:  0.10511054331367188
iteration : 11875
train acc:  0.7734375
train loss:  0.46632736921310425
train gradient:  0.1309470210979478
iteration : 11876
train acc:  0.6953125
train loss:  0.5349542498588562
train gradient:  0.13108006419882484
iteration : 11877
train acc:  0.8203125
train loss:  0.4453122615814209
train gradient:  0.0921918319532953
iteration : 11878
train acc:  0.71875
train loss:  0.5692111253738403
train gradient:  0.12482912961675076
iteration : 11879
train acc:  0.78125
train loss:  0.4435693621635437
train gradient:  0.10749687781079306
iteration : 11880
train acc:  0.75
train loss:  0.4560723900794983
train gradient:  0.1111838210111708
iteration : 11881
train acc:  0.7109375
train loss:  0.5259149074554443
train gradient:  0.11546008150362969
iteration : 11882
train acc:  0.71875
train loss:  0.524633526802063
train gradient:  0.14460019209019417
iteration : 11883
train acc:  0.7578125
train loss:  0.4834234416484833
train gradient:  0.12164319588423222
iteration : 11884
train acc:  0.765625
train loss:  0.4968585968017578
train gradient:  0.13120694914858352
iteration : 11885
train acc:  0.671875
train loss:  0.553994357585907
train gradient:  0.14033492901398797
iteration : 11886
train acc:  0.7109375
train loss:  0.5010459423065186
train gradient:  0.11772412446424299
iteration : 11887
train acc:  0.6875
train loss:  0.5627631545066833
train gradient:  0.13425030418011785
iteration : 11888
train acc:  0.765625
train loss:  0.49376872181892395
train gradient:  0.13979866137229913
iteration : 11889
train acc:  0.78125
train loss:  0.4417341947555542
train gradient:  0.09634477276957285
iteration : 11890
train acc:  0.765625
train loss:  0.4742667078971863
train gradient:  0.11831126871910631
iteration : 11891
train acc:  0.8046875
train loss:  0.4545530676841736
train gradient:  0.09226210627355118
iteration : 11892
train acc:  0.8046875
train loss:  0.47678524255752563
train gradient:  0.10993065530604797
iteration : 11893
train acc:  0.71875
train loss:  0.5000520348548889
train gradient:  0.14491250907942965
iteration : 11894
train acc:  0.796875
train loss:  0.45355480909347534
train gradient:  0.1083839711321092
iteration : 11895
train acc:  0.7734375
train loss:  0.48186230659484863
train gradient:  0.146591796975167
iteration : 11896
train acc:  0.7578125
train loss:  0.43323346972465515
train gradient:  0.08189940571822196
iteration : 11897
train acc:  0.7109375
train loss:  0.5133281946182251
train gradient:  0.12586777699314827
iteration : 11898
train acc:  0.78125
train loss:  0.4910356402397156
train gradient:  0.10680670124753064
iteration : 11899
train acc:  0.765625
train loss:  0.4613933861255646
train gradient:  0.11222991110239759
iteration : 11900
train acc:  0.734375
train loss:  0.49872052669525146
train gradient:  0.14305942619381629
iteration : 11901
train acc:  0.765625
train loss:  0.5016114711761475
train gradient:  0.1218655934381139
iteration : 11902
train acc:  0.7265625
train loss:  0.5182412266731262
train gradient:  0.1607297522162598
iteration : 11903
train acc:  0.7734375
train loss:  0.46235525608062744
train gradient:  0.09894198134292913
iteration : 11904
train acc:  0.6875
train loss:  0.5454045534133911
train gradient:  0.14169192580156903
iteration : 11905
train acc:  0.703125
train loss:  0.48618781566619873
train gradient:  0.15049249948007193
iteration : 11906
train acc:  0.7421875
train loss:  0.48030561208724976
train gradient:  0.14462929416488313
iteration : 11907
train acc:  0.7734375
train loss:  0.4879985451698303
train gradient:  0.11354766392424771
iteration : 11908
train acc:  0.7734375
train loss:  0.5163244605064392
train gradient:  0.1326283125247134
iteration : 11909
train acc:  0.6953125
train loss:  0.5489562749862671
train gradient:  0.1319058078675536
iteration : 11910
train acc:  0.7109375
train loss:  0.5339195728302002
train gradient:  0.1471519247868654
iteration : 11911
train acc:  0.734375
train loss:  0.48557206988334656
train gradient:  0.12224540513623722
iteration : 11912
train acc:  0.75
train loss:  0.5405082106590271
train gradient:  0.12479684395829001
iteration : 11913
train acc:  0.78125
train loss:  0.41138797998428345
train gradient:  0.13743371891833575
iteration : 11914
train acc:  0.765625
train loss:  0.49084511399269104
train gradient:  0.12033743421305271
iteration : 11915
train acc:  0.8359375
train loss:  0.3964391350746155
train gradient:  0.07969557228395278
iteration : 11916
train acc:  0.7109375
train loss:  0.47349783778190613
train gradient:  0.11104779445298263
iteration : 11917
train acc:  0.7734375
train loss:  0.45145368576049805
train gradient:  0.11112114712526877
iteration : 11918
train acc:  0.7421875
train loss:  0.47440654039382935
train gradient:  0.12189366298677883
iteration : 11919
train acc:  0.7265625
train loss:  0.5334235429763794
train gradient:  0.1357247810345974
iteration : 11920
train acc:  0.796875
train loss:  0.4235915541648865
train gradient:  0.0981546035032832
iteration : 11921
train acc:  0.796875
train loss:  0.41427409648895264
train gradient:  0.07470896247468066
iteration : 11922
train acc:  0.8125
train loss:  0.39886927604675293
train gradient:  0.07841325028198894
iteration : 11923
train acc:  0.6953125
train loss:  0.5264585018157959
train gradient:  0.18076578251507974
iteration : 11924
train acc:  0.8046875
train loss:  0.450687050819397
train gradient:  0.11627412965911574
iteration : 11925
train acc:  0.78125
train loss:  0.4823840856552124
train gradient:  0.10624276591067243
iteration : 11926
train acc:  0.6875
train loss:  0.5769715905189514
train gradient:  0.1854383502528657
iteration : 11927
train acc:  0.703125
train loss:  0.5334447026252747
train gradient:  0.133749130100279
iteration : 11928
train acc:  0.6796875
train loss:  0.5935673713684082
train gradient:  0.12437091729394731
iteration : 11929
train acc:  0.75
train loss:  0.5122044086456299
train gradient:  0.13052527654360216
iteration : 11930
train acc:  0.71875
train loss:  0.5202516913414001
train gradient:  0.13294674358084033
iteration : 11931
train acc:  0.7265625
train loss:  0.4586586058139801
train gradient:  0.1350914620261225
iteration : 11932
train acc:  0.7109375
train loss:  0.5251021385192871
train gradient:  0.14728624433798454
iteration : 11933
train acc:  0.7578125
train loss:  0.48107823729515076
train gradient:  0.13007771943282953
iteration : 11934
train acc:  0.7265625
train loss:  0.5038662552833557
train gradient:  0.1211245734301585
iteration : 11935
train acc:  0.7734375
train loss:  0.5196335315704346
train gradient:  0.11049538704427844
iteration : 11936
train acc:  0.765625
train loss:  0.4679720103740692
train gradient:  0.123724310535185
iteration : 11937
train acc:  0.7421875
train loss:  0.42191141843795776
train gradient:  0.07765060856651368
iteration : 11938
train acc:  0.75
train loss:  0.49293261766433716
train gradient:  0.15595832491997252
iteration : 11939
train acc:  0.828125
train loss:  0.45132434368133545
train gradient:  0.09389206151742249
iteration : 11940
train acc:  0.78125
train loss:  0.48717477917671204
train gradient:  0.11450583614179037
iteration : 11941
train acc:  0.765625
train loss:  0.4497241973876953
train gradient:  0.10852981890714107
iteration : 11942
train acc:  0.6875
train loss:  0.5716128349304199
train gradient:  0.1631251296121135
iteration : 11943
train acc:  0.765625
train loss:  0.4633033871650696
train gradient:  0.11405557750966608
iteration : 11944
train acc:  0.796875
train loss:  0.49764469265937805
train gradient:  0.1522176448864358
iteration : 11945
train acc:  0.7109375
train loss:  0.522645115852356
train gradient:  0.1659418497187587
iteration : 11946
train acc:  0.7421875
train loss:  0.5031198859214783
train gradient:  0.13047283460015363
iteration : 11947
train acc:  0.7421875
train loss:  0.45474371314048767
train gradient:  0.09545124705018398
iteration : 11948
train acc:  0.7109375
train loss:  0.5693901777267456
train gradient:  0.14687791796909289
iteration : 11949
train acc:  0.7578125
train loss:  0.44589847326278687
train gradient:  0.09597808056135443
iteration : 11950
train acc:  0.6953125
train loss:  0.5485883951187134
train gradient:  0.11443373878501618
iteration : 11951
train acc:  0.796875
train loss:  0.4695892632007599
train gradient:  0.12513313435536544
iteration : 11952
train acc:  0.7109375
train loss:  0.49605321884155273
train gradient:  0.11211365079279542
iteration : 11953
train acc:  0.6484375
train loss:  0.5666675567626953
train gradient:  0.13707579941070705
iteration : 11954
train acc:  0.75
train loss:  0.5008291602134705
train gradient:  0.11077994738393343
iteration : 11955
train acc:  0.7265625
train loss:  0.5038623809814453
train gradient:  0.09404645213422909
iteration : 11956
train acc:  0.78125
train loss:  0.4489003121852875
train gradient:  0.09225674890738605
iteration : 11957
train acc:  0.734375
train loss:  0.45603927969932556
train gradient:  0.11012762938112902
iteration : 11958
train acc:  0.7109375
train loss:  0.4615093469619751
train gradient:  0.12592554078148113
iteration : 11959
train acc:  0.8125
train loss:  0.48884713649749756
train gradient:  0.1091045856640905
iteration : 11960
train acc:  0.703125
train loss:  0.502023458480835
train gradient:  0.12071854955564913
iteration : 11961
train acc:  0.7421875
train loss:  0.5364123582839966
train gradient:  0.11796758954612652
iteration : 11962
train acc:  0.765625
train loss:  0.45502740144729614
train gradient:  0.11981071778799662
iteration : 11963
train acc:  0.734375
train loss:  0.5134799480438232
train gradient:  0.11221900636324106
iteration : 11964
train acc:  0.796875
train loss:  0.44591832160949707
train gradient:  0.10376789148854354
iteration : 11965
train acc:  0.703125
train loss:  0.5239341855049133
train gradient:  0.1393054363910363
iteration : 11966
train acc:  0.7265625
train loss:  0.51622474193573
train gradient:  0.14870500716816448
iteration : 11967
train acc:  0.7265625
train loss:  0.49889689683914185
train gradient:  0.13377119387349556
iteration : 11968
train acc:  0.7421875
train loss:  0.5148125290870667
train gradient:  0.11820851240181242
iteration : 11969
train acc:  0.8359375
train loss:  0.39888572692871094
train gradient:  0.09939697470238726
iteration : 11970
train acc:  0.7890625
train loss:  0.4631020128726959
train gradient:  0.10631194997211565
iteration : 11971
train acc:  0.734375
train loss:  0.4871961176395416
train gradient:  0.11411838343588901
iteration : 11972
train acc:  0.796875
train loss:  0.43112707138061523
train gradient:  0.0862858178170071
iteration : 11973
train acc:  0.7578125
train loss:  0.42032569646835327
train gradient:  0.08094676609319494
iteration : 11974
train acc:  0.7578125
train loss:  0.4788432717323303
train gradient:  0.11525976229775263
iteration : 11975
train acc:  0.7109375
train loss:  0.5132868885993958
train gradient:  0.1214115192188248
iteration : 11976
train acc:  0.734375
train loss:  0.48135673999786377
train gradient:  0.09343848094044868
iteration : 11977
train acc:  0.796875
train loss:  0.45146894454956055
train gradient:  0.09789589958549214
iteration : 11978
train acc:  0.7734375
train loss:  0.4419906735420227
train gradient:  0.1123958509105517
iteration : 11979
train acc:  0.78125
train loss:  0.47726207971572876
train gradient:  0.08794873331116186
iteration : 11980
train acc:  0.7890625
train loss:  0.4719129800796509
train gradient:  0.12464890439534516
iteration : 11981
train acc:  0.796875
train loss:  0.45861589908599854
train gradient:  0.12102102691800112
iteration : 11982
train acc:  0.78125
train loss:  0.45196595788002014
train gradient:  0.09229977296970504
iteration : 11983
train acc:  0.71875
train loss:  0.5013747215270996
train gradient:  0.13548790716797246
iteration : 11984
train acc:  0.7578125
train loss:  0.5006357431411743
train gradient:  0.148003421509069
iteration : 11985
train acc:  0.7890625
train loss:  0.48511743545532227
train gradient:  0.10798744062140933
iteration : 11986
train acc:  0.796875
train loss:  0.41985422372817993
train gradient:  0.1074908029974625
iteration : 11987
train acc:  0.75
train loss:  0.5516490340232849
train gradient:  0.137501235903528
iteration : 11988
train acc:  0.7109375
train loss:  0.5431888699531555
train gradient:  0.12452226195038031
iteration : 11989
train acc:  0.6875
train loss:  0.5784732103347778
train gradient:  0.161475740597728
iteration : 11990
train acc:  0.734375
train loss:  0.4547913372516632
train gradient:  0.11881627274497328
iteration : 11991
train acc:  0.7734375
train loss:  0.4472009837627411
train gradient:  0.10372941136332751
iteration : 11992
train acc:  0.78125
train loss:  0.44180935621261597
train gradient:  0.08768195119971611
iteration : 11993
train acc:  0.734375
train loss:  0.4903801679611206
train gradient:  0.13681128906294518
iteration : 11994
train acc:  0.7265625
train loss:  0.4924277067184448
train gradient:  0.1429134639243128
iteration : 11995
train acc:  0.7578125
train loss:  0.4797002077102661
train gradient:  0.11385485118292847
iteration : 11996
train acc:  0.7265625
train loss:  0.47034895420074463
train gradient:  0.11550617217170225
iteration : 11997
train acc:  0.75
train loss:  0.5029024481773376
train gradient:  0.1395358341188696
iteration : 11998
train acc:  0.71875
train loss:  0.48429355025291443
train gradient:  0.09429398221844108
iteration : 11999
train acc:  0.8203125
train loss:  0.4449816346168518
train gradient:  0.10204829289538672
iteration : 12000
train acc:  0.7109375
train loss:  0.525623619556427
train gradient:  0.11817733072361711
iteration : 12001
train acc:  0.765625
train loss:  0.5326157212257385
train gradient:  0.13977440672868738
iteration : 12002
train acc:  0.7109375
train loss:  0.5470656752586365
train gradient:  0.12233817115062251
iteration : 12003
train acc:  0.7421875
train loss:  0.5413264036178589
train gradient:  0.16131426954867856
iteration : 12004
train acc:  0.7578125
train loss:  0.5207416415214539
train gradient:  0.13577620355659722
iteration : 12005
train acc:  0.78125
train loss:  0.4558996558189392
train gradient:  0.0973512269064293
iteration : 12006
train acc:  0.7421875
train loss:  0.49546384811401367
train gradient:  0.13877075465039984
iteration : 12007
train acc:  0.7109375
train loss:  0.4735109806060791
train gradient:  0.08706070430386292
iteration : 12008
train acc:  0.7265625
train loss:  0.4547294080257416
train gradient:  0.093875554848603
iteration : 12009
train acc:  0.78125
train loss:  0.46293824911117554
train gradient:  0.0966187763944326
iteration : 12010
train acc:  0.765625
train loss:  0.517736554145813
train gradient:  0.11019788693848011
iteration : 12011
train acc:  0.7734375
train loss:  0.46033626794815063
train gradient:  0.10294742024939746
iteration : 12012
train acc:  0.7421875
train loss:  0.4947816729545593
train gradient:  0.12411361255315673
iteration : 12013
train acc:  0.734375
train loss:  0.45858779549598694
train gradient:  0.10208124776511301
iteration : 12014
train acc:  0.75
train loss:  0.447528600692749
train gradient:  0.11095862874955695
iteration : 12015
train acc:  0.6796875
train loss:  0.5287636518478394
train gradient:  0.10687490873610665
iteration : 12016
train acc:  0.734375
train loss:  0.4589267373085022
train gradient:  0.12791217649436137
iteration : 12017
train acc:  0.7421875
train loss:  0.4660670757293701
train gradient:  0.10434462547377274
iteration : 12018
train acc:  0.71875
train loss:  0.516459047794342
train gradient:  0.14142569204830646
iteration : 12019
train acc:  0.7890625
train loss:  0.44721749424934387
train gradient:  0.10017640533155867
iteration : 12020
train acc:  0.71875
train loss:  0.512447714805603
train gradient:  0.15436537318668558
iteration : 12021
train acc:  0.796875
train loss:  0.42143261432647705
train gradient:  0.09057853447007519
iteration : 12022
train acc:  0.71875
train loss:  0.5079424381256104
train gradient:  0.14883132613675115
iteration : 12023
train acc:  0.75
train loss:  0.5107392072677612
train gradient:  0.11933874012753788
iteration : 12024
train acc:  0.71875
train loss:  0.5165225267410278
train gradient:  0.14844683568033784
iteration : 12025
train acc:  0.7890625
train loss:  0.4206176996231079
train gradient:  0.08521310877359899
iteration : 12026
train acc:  0.71875
train loss:  0.4791102111339569
train gradient:  0.14171536716979385
iteration : 12027
train acc:  0.7265625
train loss:  0.49171459674835205
train gradient:  0.13248608732841316
iteration : 12028
train acc:  0.7265625
train loss:  0.4887741208076477
train gradient:  0.12569542871956468
iteration : 12029
train acc:  0.7421875
train loss:  0.4874265491962433
train gradient:  0.10651902896284302
iteration : 12030
train acc:  0.6953125
train loss:  0.4881386160850525
train gradient:  0.1453404371855233
iteration : 12031
train acc:  0.7578125
train loss:  0.45372140407562256
train gradient:  0.10546255796436126
iteration : 12032
train acc:  0.765625
train loss:  0.5306861400604248
train gradient:  0.18271605108862937
iteration : 12033
train acc:  0.734375
train loss:  0.495033323764801
train gradient:  0.09350718818146843
iteration : 12034
train acc:  0.7265625
train loss:  0.4822766184806824
train gradient:  0.10608975574870322
iteration : 12035
train acc:  0.7265625
train loss:  0.4681277573108673
train gradient:  0.14540617353958293
iteration : 12036
train acc:  0.765625
train loss:  0.4320032596588135
train gradient:  0.08331786288970043
iteration : 12037
train acc:  0.828125
train loss:  0.434188574552536
train gradient:  0.09294088869825864
iteration : 12038
train acc:  0.7265625
train loss:  0.5073754787445068
train gradient:  0.1365675935901824
iteration : 12039
train acc:  0.7265625
train loss:  0.47212862968444824
train gradient:  0.13444052344471752
iteration : 12040
train acc:  0.796875
train loss:  0.43632274866104126
train gradient:  0.08871717213755952
iteration : 12041
train acc:  0.7734375
train loss:  0.4485161602497101
train gradient:  0.11853534910369455
iteration : 12042
train acc:  0.703125
train loss:  0.5530585050582886
train gradient:  0.15710741039301213
iteration : 12043
train acc:  0.7578125
train loss:  0.45174384117126465
train gradient:  0.09876540554350655
iteration : 12044
train acc:  0.7734375
train loss:  0.4523795247077942
train gradient:  0.10763200207613231
iteration : 12045
train acc:  0.8125
train loss:  0.45096880197525024
train gradient:  0.10515831667525326
iteration : 12046
train acc:  0.7421875
train loss:  0.4869760274887085
train gradient:  0.1016345345684763
iteration : 12047
train acc:  0.7265625
train loss:  0.4707840383052826
train gradient:  0.10954704376770466
iteration : 12048
train acc:  0.71875
train loss:  0.49440744519233704
train gradient:  0.10436399299463688
iteration : 12049
train acc:  0.7421875
train loss:  0.4921761155128479
train gradient:  0.14592085091670576
iteration : 12050
train acc:  0.828125
train loss:  0.38765963912010193
train gradient:  0.0936643154586756
iteration : 12051
train acc:  0.734375
train loss:  0.4244076609611511
train gradient:  0.12308438241201496
iteration : 12052
train acc:  0.8046875
train loss:  0.46511951088905334
train gradient:  0.09418133086414053
iteration : 12053
train acc:  0.7890625
train loss:  0.4745267629623413
train gradient:  0.099922212062522
iteration : 12054
train acc:  0.703125
train loss:  0.525651216506958
train gradient:  0.14002304766185023
iteration : 12055
train acc:  0.6953125
train loss:  0.5644638538360596
train gradient:  0.17376567205459365
iteration : 12056
train acc:  0.7890625
train loss:  0.456967830657959
train gradient:  0.10443707835363142
iteration : 12057
train acc:  0.671875
train loss:  0.5423629283905029
train gradient:  0.15116649398865428
iteration : 12058
train acc:  0.75
train loss:  0.4778813421726227
train gradient:  0.13135817126931967
iteration : 12059
train acc:  0.7578125
train loss:  0.4644832909107208
train gradient:  0.10168738175186605
iteration : 12060
train acc:  0.7578125
train loss:  0.492990106344223
train gradient:  0.11913555629027507
iteration : 12061
train acc:  0.8359375
train loss:  0.4309528172016144
train gradient:  0.0982004458756649
iteration : 12062
train acc:  0.7109375
train loss:  0.4892263412475586
train gradient:  0.12634191144583448
iteration : 12063
train acc:  0.7109375
train loss:  0.5122740268707275
train gradient:  0.13588625114506836
iteration : 12064
train acc:  0.75
train loss:  0.5030940175056458
train gradient:  0.11023558965736464
iteration : 12065
train acc:  0.78125
train loss:  0.4807475209236145
train gradient:  0.12934749794569383
iteration : 12066
train acc:  0.734375
train loss:  0.4868871867656708
train gradient:  0.17725783860359728
iteration : 12067
train acc:  0.7890625
train loss:  0.4298747777938843
train gradient:  0.09808926282609634
iteration : 12068
train acc:  0.75
train loss:  0.4683719277381897
train gradient:  0.09361250491783649
iteration : 12069
train acc:  0.7265625
train loss:  0.4411517381668091
train gradient:  0.08730152539422208
iteration : 12070
train acc:  0.71875
train loss:  0.48956185579299927
train gradient:  0.12320301759620628
iteration : 12071
train acc:  0.796875
train loss:  0.4781629145145416
train gradient:  0.14611152523474552
iteration : 12072
train acc:  0.75
train loss:  0.49852877855300903
train gradient:  0.10229644564314005
iteration : 12073
train acc:  0.7109375
train loss:  0.4332007169723511
train gradient:  0.09793025518007051
iteration : 12074
train acc:  0.7734375
train loss:  0.476146399974823
train gradient:  0.13731296183385505
iteration : 12075
train acc:  0.6953125
train loss:  0.5495333671569824
train gradient:  0.14459006920960854
iteration : 12076
train acc:  0.796875
train loss:  0.5219753980636597
train gradient:  0.11407980308345507
iteration : 12077
train acc:  0.8046875
train loss:  0.43383604288101196
train gradient:  0.10737142960399643
iteration : 12078
train acc:  0.7890625
train loss:  0.45703598856925964
train gradient:  0.13593606041295808
iteration : 12079
train acc:  0.71875
train loss:  0.5632297992706299
train gradient:  0.16133735366108282
iteration : 12080
train acc:  0.75
train loss:  0.5018554925918579
train gradient:  0.12914724800336877
iteration : 12081
train acc:  0.75
train loss:  0.4895893633365631
train gradient:  0.13787245908635176
iteration : 12082
train acc:  0.7421875
train loss:  0.5127248764038086
train gradient:  0.14256675484088385
iteration : 12083
train acc:  0.7421875
train loss:  0.4468286335468292
train gradient:  0.0876030536637712
iteration : 12084
train acc:  0.6953125
train loss:  0.5418484210968018
train gradient:  0.17578060040712895
iteration : 12085
train acc:  0.765625
train loss:  0.4885502755641937
train gradient:  0.1226482301884544
iteration : 12086
train acc:  0.75
train loss:  0.4749019145965576
train gradient:  0.11803566786258449
iteration : 12087
train acc:  0.8125
train loss:  0.45405203104019165
train gradient:  0.1631658900581128
iteration : 12088
train acc:  0.8046875
train loss:  0.40685299038887024
train gradient:  0.0998815033972525
iteration : 12089
train acc:  0.75
train loss:  0.4960029721260071
train gradient:  0.1236958980896972
iteration : 12090
train acc:  0.75
train loss:  0.5305253267288208
train gradient:  0.18210740683308435
iteration : 12091
train acc:  0.71875
train loss:  0.4844747483730316
train gradient:  0.11504470522189231
iteration : 12092
train acc:  0.765625
train loss:  0.4809677302837372
train gradient:  0.11206175128170588
iteration : 12093
train acc:  0.7421875
train loss:  0.45628517866134644
train gradient:  0.11827307763948657
iteration : 12094
train acc:  0.796875
train loss:  0.43989402055740356
train gradient:  0.11383339020533505
iteration : 12095
train acc:  0.765625
train loss:  0.5062547922134399
train gradient:  0.11822154158281162
iteration : 12096
train acc:  0.765625
train loss:  0.4800674617290497
train gradient:  0.1571414740473845
iteration : 12097
train acc:  0.7734375
train loss:  0.4696187674999237
train gradient:  0.10795338057177985
iteration : 12098
train acc:  0.7578125
train loss:  0.5080844163894653
train gradient:  0.14423230699916537
iteration : 12099
train acc:  0.734375
train loss:  0.5086272954940796
train gradient:  0.12027856478286915
iteration : 12100
train acc:  0.796875
train loss:  0.44141340255737305
train gradient:  0.10091429727822498
iteration : 12101
train acc:  0.703125
train loss:  0.49410420656204224
train gradient:  0.11842675540313957
iteration : 12102
train acc:  0.8046875
train loss:  0.44947993755340576
train gradient:  0.09558713722070894
iteration : 12103
train acc:  0.765625
train loss:  0.43442976474761963
train gradient:  0.13076459629824932
iteration : 12104
train acc:  0.6953125
train loss:  0.5192651748657227
train gradient:  0.11601711380269106
iteration : 12105
train acc:  0.7109375
train loss:  0.5791728496551514
train gradient:  0.19983913660678393
iteration : 12106
train acc:  0.75
train loss:  0.4949036240577698
train gradient:  0.12771784021855956
iteration : 12107
train acc:  0.7421875
train loss:  0.47853320837020874
train gradient:  0.1326640242094066
iteration : 12108
train acc:  0.7578125
train loss:  0.43731093406677246
train gradient:  0.08498274391889495
iteration : 12109
train acc:  0.7578125
train loss:  0.5114391446113586
train gradient:  0.13501195720853118
iteration : 12110
train acc:  0.7578125
train loss:  0.5013142824172974
train gradient:  0.124591847050903
iteration : 12111
train acc:  0.796875
train loss:  0.427680641412735
train gradient:  0.07541292401835883
iteration : 12112
train acc:  0.6640625
train loss:  0.5647141933441162
train gradient:  0.1450661559406018
iteration : 12113
train acc:  0.7578125
train loss:  0.4720618724822998
train gradient:  0.11286983214360388
iteration : 12114
train acc:  0.75
train loss:  0.45126062631607056
train gradient:  0.09232138896463066
iteration : 12115
train acc:  0.7109375
train loss:  0.46882420778274536
train gradient:  0.09826625753972776
iteration : 12116
train acc:  0.671875
train loss:  0.582252025604248
train gradient:  0.1775848231807673
iteration : 12117
train acc:  0.7109375
train loss:  0.5434662699699402
train gradient:  0.12057560896653721
iteration : 12118
train acc:  0.734375
train loss:  0.49074557423591614
train gradient:  0.1255244940434485
iteration : 12119
train acc:  0.71875
train loss:  0.5666174292564392
train gradient:  0.15678647816668753
iteration : 12120
train acc:  0.765625
train loss:  0.47272568941116333
train gradient:  0.11363761717905205
iteration : 12121
train acc:  0.734375
train loss:  0.5552157163619995
train gradient:  0.1640321950291464
iteration : 12122
train acc:  0.8203125
train loss:  0.42689332365989685
train gradient:  0.0865717232942364
iteration : 12123
train acc:  0.71875
train loss:  0.5391431450843811
train gradient:  0.15510964368767793
iteration : 12124
train acc:  0.671875
train loss:  0.6358838677406311
train gradient:  0.1599863069650586
iteration : 12125
train acc:  0.765625
train loss:  0.4607052206993103
train gradient:  0.11939050270847569
iteration : 12126
train acc:  0.7734375
train loss:  0.479806125164032
train gradient:  0.12003364862450891
iteration : 12127
train acc:  0.7109375
train loss:  0.5069429278373718
train gradient:  0.13167628944034418
iteration : 12128
train acc:  0.7109375
train loss:  0.5157252550125122
train gradient:  0.11796471323992186
iteration : 12129
train acc:  0.7109375
train loss:  0.5515543818473816
train gradient:  0.16962385424986637
iteration : 12130
train acc:  0.7421875
train loss:  0.5151385068893433
train gradient:  0.12065915297049225
iteration : 12131
train acc:  0.7109375
train loss:  0.5043089985847473
train gradient:  0.141689073453086
iteration : 12132
train acc:  0.765625
train loss:  0.4822837710380554
train gradient:  0.11618044228802593
iteration : 12133
train acc:  0.7265625
train loss:  0.49546104669570923
train gradient:  0.10578050461121313
iteration : 12134
train acc:  0.703125
train loss:  0.5008984804153442
train gradient:  0.11230651073865845
iteration : 12135
train acc:  0.71875
train loss:  0.572069525718689
train gradient:  0.1580586964508488
iteration : 12136
train acc:  0.7421875
train loss:  0.49886420369148254
train gradient:  0.11212855092731579
iteration : 12137
train acc:  0.734375
train loss:  0.4583790600299835
train gradient:  0.09180648418362149
iteration : 12138
train acc:  0.7265625
train loss:  0.4841732382774353
train gradient:  0.08905610932949565
iteration : 12139
train acc:  0.703125
train loss:  0.5377517938613892
train gradient:  0.1417282483652041
iteration : 12140
train acc:  0.734375
train loss:  0.5339463949203491
train gradient:  0.135457724493909
iteration : 12141
train acc:  0.7109375
train loss:  0.5294270515441895
train gradient:  0.12099554822765334
iteration : 12142
train acc:  0.71875
train loss:  0.4943636655807495
train gradient:  0.14326949361730967
iteration : 12143
train acc:  0.734375
train loss:  0.4699215888977051
train gradient:  0.12348748324931763
iteration : 12144
train acc:  0.7734375
train loss:  0.5264772772789001
train gradient:  0.12053027445749528
iteration : 12145
train acc:  0.7421875
train loss:  0.47041672468185425
train gradient:  0.12052310650546864
iteration : 12146
train acc:  0.796875
train loss:  0.4478309750556946
train gradient:  0.08489163505248233
iteration : 12147
train acc:  0.8203125
train loss:  0.39007434248924255
train gradient:  0.0723745653326417
iteration : 12148
train acc:  0.78125
train loss:  0.4518486559391022
train gradient:  0.1299471259515099
iteration : 12149
train acc:  0.703125
train loss:  0.5250161290168762
train gradient:  0.1394245671960569
iteration : 12150
train acc:  0.7734375
train loss:  0.46381479501724243
train gradient:  0.09016318213180984
iteration : 12151
train acc:  0.7421875
train loss:  0.5008096098899841
train gradient:  0.10224125083138232
iteration : 12152
train acc:  0.7109375
train loss:  0.5480360984802246
train gradient:  0.14795205362416308
iteration : 12153
train acc:  0.7734375
train loss:  0.4767962098121643
train gradient:  0.11001982233166503
iteration : 12154
train acc:  0.75
train loss:  0.5374812483787537
train gradient:  0.15593864355728804
iteration : 12155
train acc:  0.8046875
train loss:  0.45219874382019043
train gradient:  0.10428048979583665
iteration : 12156
train acc:  0.7109375
train loss:  0.5772262811660767
train gradient:  0.16716558200716028
iteration : 12157
train acc:  0.7734375
train loss:  0.43345606327056885
train gradient:  0.1074069449724215
iteration : 12158
train acc:  0.7109375
train loss:  0.4982631802558899
train gradient:  0.15302000483638734
iteration : 12159
train acc:  0.8125
train loss:  0.41457048058509827
train gradient:  0.07707647854415872
iteration : 12160
train acc:  0.7734375
train loss:  0.43666136264801025
train gradient:  0.08906856046052357
iteration : 12161
train acc:  0.65625
train loss:  0.5985890626907349
train gradient:  0.1502169434227116
iteration : 12162
train acc:  0.7734375
train loss:  0.4652864933013916
train gradient:  0.07206407952457744
iteration : 12163
train acc:  0.796875
train loss:  0.4674079418182373
train gradient:  0.1116294719870575
iteration : 12164
train acc:  0.7421875
train loss:  0.5044108629226685
train gradient:  0.12351528662439047
iteration : 12165
train acc:  0.671875
train loss:  0.5343892574310303
train gradient:  0.12187258744935552
iteration : 12166
train acc:  0.7265625
train loss:  0.47100359201431274
train gradient:  0.11241099426125525
iteration : 12167
train acc:  0.7890625
train loss:  0.4561787247657776
train gradient:  0.11986817553753151
iteration : 12168
train acc:  0.75
train loss:  0.4801318645477295
train gradient:  0.12380702249639026
iteration : 12169
train acc:  0.828125
train loss:  0.39207082986831665
train gradient:  0.07965099308347372
iteration : 12170
train acc:  0.703125
train loss:  0.5252457857131958
train gradient:  0.1453706920482778
iteration : 12171
train acc:  0.7734375
train loss:  0.4368526041507721
train gradient:  0.09715128310983165
iteration : 12172
train acc:  0.6796875
train loss:  0.5012853145599365
train gradient:  0.13663828556340418
iteration : 12173
train acc:  0.7734375
train loss:  0.4625217914581299
train gradient:  0.07638171478013239
iteration : 12174
train acc:  0.6796875
train loss:  0.5861687064170837
train gradient:  0.1715667795199305
iteration : 12175
train acc:  0.6953125
train loss:  0.5199163556098938
train gradient:  0.17151558633032354
iteration : 12176
train acc:  0.6953125
train loss:  0.5266153812408447
train gradient:  0.1337395852294644
iteration : 12177
train acc:  0.7421875
train loss:  0.5111430287361145
train gradient:  0.1302420173562393
iteration : 12178
train acc:  0.7578125
train loss:  0.45095232129096985
train gradient:  0.09005320267869409
iteration : 12179
train acc:  0.78125
train loss:  0.4269644618034363
train gradient:  0.09871056026665512
iteration : 12180
train acc:  0.7578125
train loss:  0.4897034466266632
train gradient:  0.10143909385218544
iteration : 12181
train acc:  0.6875
train loss:  0.5526716113090515
train gradient:  0.1460124567421041
iteration : 12182
train acc:  0.75
train loss:  0.49373337626457214
train gradient:  0.10741578761155161
iteration : 12183
train acc:  0.8046875
train loss:  0.445110023021698
train gradient:  0.08870219745303454
iteration : 12184
train acc:  0.7578125
train loss:  0.4775122106075287
train gradient:  0.10150825089350902
iteration : 12185
train acc:  0.7265625
train loss:  0.491315633058548
train gradient:  0.11842540533257391
iteration : 12186
train acc:  0.6796875
train loss:  0.5252398252487183
train gradient:  0.14361852127383654
iteration : 12187
train acc:  0.7421875
train loss:  0.4664607048034668
train gradient:  0.1341896781986926
iteration : 12188
train acc:  0.7265625
train loss:  0.5123941898345947
train gradient:  0.1479949611654512
iteration : 12189
train acc:  0.7421875
train loss:  0.47115176916122437
train gradient:  0.10803694538068948
iteration : 12190
train acc:  0.7734375
train loss:  0.4754211902618408
train gradient:  0.11568432317714183
iteration : 12191
train acc:  0.71875
train loss:  0.5356122255325317
train gradient:  0.11700694123819197
iteration : 12192
train acc:  0.75
train loss:  0.4478139579296112
train gradient:  0.10150669092668167
iteration : 12193
train acc:  0.8046875
train loss:  0.4341232180595398
train gradient:  0.09516466672577756
iteration : 12194
train acc:  0.75
train loss:  0.48186448216438293
train gradient:  0.11126200014729273
iteration : 12195
train acc:  0.78125
train loss:  0.4714142680168152
train gradient:  0.1139784429015289
iteration : 12196
train acc:  0.7109375
train loss:  0.5059006810188293
train gradient:  0.15648684202580418
iteration : 12197
train acc:  0.765625
train loss:  0.488228440284729
train gradient:  0.10418574825046209
iteration : 12198
train acc:  0.734375
train loss:  0.4994884133338928
train gradient:  0.11030416000500459
iteration : 12199
train acc:  0.8046875
train loss:  0.4281883239746094
train gradient:  0.10826647580750011
iteration : 12200
train acc:  0.765625
train loss:  0.41887366771698
train gradient:  0.07496351168094396
iteration : 12201
train acc:  0.7890625
train loss:  0.44846290349960327
train gradient:  0.0936544952710089
iteration : 12202
train acc:  0.7265625
train loss:  0.5483736991882324
train gradient:  0.15205850616182007
iteration : 12203
train acc:  0.8046875
train loss:  0.4342383146286011
train gradient:  0.10284899770233136
iteration : 12204
train acc:  0.78125
train loss:  0.4462660551071167
train gradient:  0.1120624105186614
iteration : 12205
train acc:  0.65625
train loss:  0.5352955460548401
train gradient:  0.12266754690583401
iteration : 12206
train acc:  0.65625
train loss:  0.5448429584503174
train gradient:  0.1606925167482311
iteration : 12207
train acc:  0.65625
train loss:  0.5841760635375977
train gradient:  0.15137150692461437
iteration : 12208
train acc:  0.7890625
train loss:  0.44079652428627014
train gradient:  0.09295756007949504
iteration : 12209
train acc:  0.7578125
train loss:  0.4533424973487854
train gradient:  0.08766928502105584
iteration : 12210
train acc:  0.6875
train loss:  0.5513521432876587
train gradient:  0.1680217968918728
iteration : 12211
train acc:  0.796875
train loss:  0.44522908329963684
train gradient:  0.09327880964672451
iteration : 12212
train acc:  0.7265625
train loss:  0.4870143234729767
train gradient:  0.09423863740623084
iteration : 12213
train acc:  0.78125
train loss:  0.4456658959388733
train gradient:  0.09641828223683953
iteration : 12214
train acc:  0.8515625
train loss:  0.37405022978782654
train gradient:  0.08042932089095686
iteration : 12215
train acc:  0.7265625
train loss:  0.48092013597488403
train gradient:  0.11572168473889602
iteration : 12216
train acc:  0.7265625
train loss:  0.5586910843849182
train gradient:  0.16799412487878887
iteration : 12217
train acc:  0.71875
train loss:  0.5099236369132996
train gradient:  0.10465308036644685
iteration : 12218
train acc:  0.796875
train loss:  0.4438020884990692
train gradient:  0.14844775504942637
iteration : 12219
train acc:  0.734375
train loss:  0.5379667282104492
train gradient:  0.11475849014013721
iteration : 12220
train acc:  0.765625
train loss:  0.4398878812789917
train gradient:  0.09663490451608768
iteration : 12221
train acc:  0.6953125
train loss:  0.5387651920318604
train gradient:  0.1535353146353023
iteration : 12222
train acc:  0.7734375
train loss:  0.44862234592437744
train gradient:  0.12950208506938649
iteration : 12223
train acc:  0.6875
train loss:  0.4951980710029602
train gradient:  0.11017138458559485
iteration : 12224
train acc:  0.703125
train loss:  0.542807936668396
train gradient:  0.1694785882939276
iteration : 12225
train acc:  0.7890625
train loss:  0.43426713347435
train gradient:  0.08633448945709615
iteration : 12226
train acc:  0.78125
train loss:  0.46514469385147095
train gradient:  0.11595303401742647
iteration : 12227
train acc:  0.78125
train loss:  0.4549408555030823
train gradient:  0.10428831184588183
iteration : 12228
train acc:  0.734375
train loss:  0.5173237323760986
train gradient:  0.1432962334813968
iteration : 12229
train acc:  0.765625
train loss:  0.48119300603866577
train gradient:  0.11912130022054382
iteration : 12230
train acc:  0.796875
train loss:  0.4155713617801666
train gradient:  0.10639412506883802
iteration : 12231
train acc:  0.734375
train loss:  0.4779319167137146
train gradient:  0.11613947870445016
iteration : 12232
train acc:  0.7578125
train loss:  0.48024657368659973
train gradient:  0.12085182678627884
iteration : 12233
train acc:  0.7265625
train loss:  0.467586874961853
train gradient:  0.10079502230599272
iteration : 12234
train acc:  0.7421875
train loss:  0.45539695024490356
train gradient:  0.11000152163607486
iteration : 12235
train acc:  0.734375
train loss:  0.5241973400115967
train gradient:  0.16295708578182283
iteration : 12236
train acc:  0.78125
train loss:  0.4551597237586975
train gradient:  0.11884311978171114
iteration : 12237
train acc:  0.7109375
train loss:  0.4994076192378998
train gradient:  0.14198599967779593
iteration : 12238
train acc:  0.78125
train loss:  0.45246946811676025
train gradient:  0.12205159792031064
iteration : 12239
train acc:  0.765625
train loss:  0.4809172749519348
train gradient:  0.12704065975120077
iteration : 12240
train acc:  0.734375
train loss:  0.47944873571395874
train gradient:  0.11554044926866515
iteration : 12241
train acc:  0.7890625
train loss:  0.45019131898880005
train gradient:  0.09422587720870054
iteration : 12242
train acc:  0.7890625
train loss:  0.43786147236824036
train gradient:  0.10004626675529991
iteration : 12243
train acc:  0.7890625
train loss:  0.48080483078956604
train gradient:  0.106605428958035
iteration : 12244
train acc:  0.7265625
train loss:  0.5276502370834351
train gradient:  0.18804719412583132
iteration : 12245
train acc:  0.703125
train loss:  0.49744313955307007
train gradient:  0.13812373041682796
iteration : 12246
train acc:  0.6875
train loss:  0.530820369720459
train gradient:  0.12788315924455507
iteration : 12247
train acc:  0.734375
train loss:  0.45934033393859863
train gradient:  0.11054270140718854
iteration : 12248
train acc:  0.8046875
train loss:  0.43374472856521606
train gradient:  0.11650341332212431
iteration : 12249
train acc:  0.7578125
train loss:  0.42101138830184937
train gradient:  0.09626570173099201
iteration : 12250
train acc:  0.7265625
train loss:  0.5637752413749695
train gradient:  0.14171988823597637
iteration : 12251
train acc:  0.734375
train loss:  0.4849835932254791
train gradient:  0.1294260002901937
iteration : 12252
train acc:  0.7734375
train loss:  0.49958229064941406
train gradient:  0.142160506285261
iteration : 12253
train acc:  0.7109375
train loss:  0.5100463628768921
train gradient:  0.1149982333273719
iteration : 12254
train acc:  0.7265625
train loss:  0.5706692337989807
train gradient:  0.1367611595533747
iteration : 12255
train acc:  0.734375
train loss:  0.45883315801620483
train gradient:  0.11175512560840561
iteration : 12256
train acc:  0.7734375
train loss:  0.5206984877586365
train gradient:  0.11145121964778446
iteration : 12257
train acc:  0.6796875
train loss:  0.5517591238021851
train gradient:  0.13349828224971827
iteration : 12258
train acc:  0.75
train loss:  0.4785471558570862
train gradient:  0.11183242762503755
iteration : 12259
train acc:  0.65625
train loss:  0.5734803676605225
train gradient:  0.16954233276966577
iteration : 12260
train acc:  0.7109375
train loss:  0.5353342294692993
train gradient:  0.14208742609674618
iteration : 12261
train acc:  0.7421875
train loss:  0.4834570288658142
train gradient:  0.12255486455079732
iteration : 12262
train acc:  0.7421875
train loss:  0.4634271264076233
train gradient:  0.11839682206169161
iteration : 12263
train acc:  0.7109375
train loss:  0.5632030963897705
train gradient:  0.1512094381303623
iteration : 12264
train acc:  0.7265625
train loss:  0.5034197568893433
train gradient:  0.12390420408887245
iteration : 12265
train acc:  0.7265625
train loss:  0.506159245967865
train gradient:  0.1275085721532349
iteration : 12266
train acc:  0.78125
train loss:  0.4334679841995239
train gradient:  0.09622992632373632
iteration : 12267
train acc:  0.7265625
train loss:  0.5095226168632507
train gradient:  0.13091517206356748
iteration : 12268
train acc:  0.7109375
train loss:  0.5359082221984863
train gradient:  0.12955560452301967
iteration : 12269
train acc:  0.75
train loss:  0.521709680557251
train gradient:  0.12225485740079535
iteration : 12270
train acc:  0.7421875
train loss:  0.4657241702079773
train gradient:  0.09863902213809232
iteration : 12271
train acc:  0.7578125
train loss:  0.4421248435974121
train gradient:  0.11776589149291557
iteration : 12272
train acc:  0.7109375
train loss:  0.5126644372940063
train gradient:  0.1091474845228985
iteration : 12273
train acc:  0.7890625
train loss:  0.4639074206352234
train gradient:  0.10390395511534438
iteration : 12274
train acc:  0.7421875
train loss:  0.4733383059501648
train gradient:  0.1415213840181314
iteration : 12275
train acc:  0.671875
train loss:  0.6232589483261108
train gradient:  0.1588864221618021
iteration : 12276
train acc:  0.7265625
train loss:  0.4898572564125061
train gradient:  0.1272784189916371
iteration : 12277
train acc:  0.7421875
train loss:  0.4880320429801941
train gradient:  0.1125278955760999
iteration : 12278
train acc:  0.71875
train loss:  0.5258237719535828
train gradient:  0.15194491929994886
iteration : 12279
train acc:  0.7890625
train loss:  0.4305918216705322
train gradient:  0.09673997555727389
iteration : 12280
train acc:  0.7734375
train loss:  0.43294745683670044
train gradient:  0.08234918468151327
iteration : 12281
train acc:  0.796875
train loss:  0.4393097162246704
train gradient:  0.09782914621378089
iteration : 12282
train acc:  0.6953125
train loss:  0.5395185351371765
train gradient:  0.22420901051376319
iteration : 12283
train acc:  0.7734375
train loss:  0.4742739796638489
train gradient:  0.1490057877346322
iteration : 12284
train acc:  0.796875
train loss:  0.46272528171539307
train gradient:  0.12770705426687198
iteration : 12285
train acc:  0.734375
train loss:  0.46541720628738403
train gradient:  0.10061450834552714
iteration : 12286
train acc:  0.75
train loss:  0.5213813781738281
train gradient:  0.13954103864224787
iteration : 12287
train acc:  0.7421875
train loss:  0.4925990700721741
train gradient:  0.11011409565219085
iteration : 12288
train acc:  0.7265625
train loss:  0.4635581076145172
train gradient:  0.10507998029290447
iteration : 12289
train acc:  0.7265625
train loss:  0.5197287201881409
train gradient:  0.17380740631021296
iteration : 12290
train acc:  0.78125
train loss:  0.44673365354537964
train gradient:  0.09156738719647593
iteration : 12291
train acc:  0.7421875
train loss:  0.47719305753707886
train gradient:  0.10950920930417701
iteration : 12292
train acc:  0.75
train loss:  0.46948957443237305
train gradient:  0.11306934890002414
iteration : 12293
train acc:  0.8125
train loss:  0.4124549627304077
train gradient:  0.09028261002400195
iteration : 12294
train acc:  0.7265625
train loss:  0.4755588173866272
train gradient:  0.09761202679954582
iteration : 12295
train acc:  0.734375
train loss:  0.4930277466773987
train gradient:  0.11536727614333254
iteration : 12296
train acc:  0.6875
train loss:  0.5068579316139221
train gradient:  0.10365083577992124
iteration : 12297
train acc:  0.7578125
train loss:  0.48764997720718384
train gradient:  0.09682973545763357
iteration : 12298
train acc:  0.7109375
train loss:  0.5073059797286987
train gradient:  0.10384143056313325
iteration : 12299
train acc:  0.7734375
train loss:  0.45429784059524536
train gradient:  0.09654025015710281
iteration : 12300
train acc:  0.7578125
train loss:  0.4769219160079956
train gradient:  0.1148894027805129
iteration : 12301
train acc:  0.71875
train loss:  0.48758190870285034
train gradient:  0.1236740271573631
iteration : 12302
train acc:  0.734375
train loss:  0.5138726234436035
train gradient:  0.11968144331401744
iteration : 12303
train acc:  0.6796875
train loss:  0.5868335366249084
train gradient:  0.17426109979368815
iteration : 12304
train acc:  0.78125
train loss:  0.4418419301509857
train gradient:  0.09714146892676008
iteration : 12305
train acc:  0.8203125
train loss:  0.4213183522224426
train gradient:  0.11995432995878295
iteration : 12306
train acc:  0.78125
train loss:  0.4713822603225708
train gradient:  0.1533515158770541
iteration : 12307
train acc:  0.7421875
train loss:  0.46702611446380615
train gradient:  0.159115250857927
iteration : 12308
train acc:  0.71875
train loss:  0.4746361970901489
train gradient:  0.09554098744684365
iteration : 12309
train acc:  0.765625
train loss:  0.4821891188621521
train gradient:  0.09796501920149535
iteration : 12310
train acc:  0.7890625
train loss:  0.47097599506378174
train gradient:  0.10786314650322108
iteration : 12311
train acc:  0.765625
train loss:  0.5200600624084473
train gradient:  0.1853931610892574
iteration : 12312
train acc:  0.703125
train loss:  0.564934492111206
train gradient:  0.14736869864766713
iteration : 12313
train acc:  0.703125
train loss:  0.49496370553970337
train gradient:  0.10747193774109132
iteration : 12314
train acc:  0.7734375
train loss:  0.4994182586669922
train gradient:  0.14070463819376
iteration : 12315
train acc:  0.7421875
train loss:  0.47850126028060913
train gradient:  0.13489624464930589
iteration : 12316
train acc:  0.7265625
train loss:  0.4916168749332428
train gradient:  0.1109009783201745
iteration : 12317
train acc:  0.796875
train loss:  0.43824517726898193
train gradient:  0.10689758207182633
iteration : 12318
train acc:  0.7109375
train loss:  0.5752770900726318
train gradient:  0.16776184172737357
iteration : 12319
train acc:  0.7734375
train loss:  0.40849053859710693
train gradient:  0.08238341980137952
iteration : 12320
train acc:  0.7265625
train loss:  0.5191102027893066
train gradient:  0.12511113601987206
iteration : 12321
train acc:  0.7265625
train loss:  0.49871009588241577
train gradient:  0.12900889402417792
iteration : 12322
train acc:  0.8125
train loss:  0.39883601665496826
train gradient:  0.07211531569063495
iteration : 12323
train acc:  0.71875
train loss:  0.5039423108100891
train gradient:  0.132333356816692
iteration : 12324
train acc:  0.7890625
train loss:  0.5013402700424194
train gradient:  0.12144490025519306
iteration : 12325
train acc:  0.7265625
train loss:  0.5236615538597107
train gradient:  0.12551110083464184
iteration : 12326
train acc:  0.7265625
train loss:  0.5152931809425354
train gradient:  0.1120602874976621
iteration : 12327
train acc:  0.7109375
train loss:  0.5286546945571899
train gradient:  0.1470473320651423
iteration : 12328
train acc:  0.7265625
train loss:  0.5146879553794861
train gradient:  0.12565639823089625
iteration : 12329
train acc:  0.703125
train loss:  0.5220972299575806
train gradient:  0.152367736859088
iteration : 12330
train acc:  0.6953125
train loss:  0.5298410058021545
train gradient:  0.11010628911336974
iteration : 12331
train acc:  0.75
train loss:  0.4550437331199646
train gradient:  0.10455815047820423
iteration : 12332
train acc:  0.7265625
train loss:  0.47588151693344116
train gradient:  0.143878989459863
iteration : 12333
train acc:  0.71875
train loss:  0.49111229181289673
train gradient:  0.1203666746807135
iteration : 12334
train acc:  0.7421875
train loss:  0.5202728509902954
train gradient:  0.1253016672672519
iteration : 12335
train acc:  0.7265625
train loss:  0.5378841757774353
train gradient:  0.1684741506596521
iteration : 12336
train acc:  0.7734375
train loss:  0.4527812898159027
train gradient:  0.10971531336476575
iteration : 12337
train acc:  0.75
train loss:  0.4823024272918701
train gradient:  0.13534977485892374
iteration : 12338
train acc:  0.71875
train loss:  0.48605072498321533
train gradient:  0.0978229044958815
iteration : 12339
train acc:  0.7734375
train loss:  0.5009232759475708
train gradient:  0.1265238970436835
iteration : 12340
train acc:  0.78125
train loss:  0.4786524474620819
train gradient:  0.13240154545072946
iteration : 12341
train acc:  0.7890625
train loss:  0.4798656105995178
train gradient:  0.09805034809857786
iteration : 12342
train acc:  0.703125
train loss:  0.5048742294311523
train gradient:  0.12921822409181996
iteration : 12343
train acc:  0.7265625
train loss:  0.5139791965484619
train gradient:  0.1176958967160068
iteration : 12344
train acc:  0.7265625
train loss:  0.5466405153274536
train gradient:  0.14553714469069068
iteration : 12345
train acc:  0.703125
train loss:  0.5795438885688782
train gradient:  0.16749994209668595
iteration : 12346
train acc:  0.8125
train loss:  0.44422775506973267
train gradient:  0.08947280101900712
iteration : 12347
train acc:  0.765625
train loss:  0.4337022304534912
train gradient:  0.08595620669613391
iteration : 12348
train acc:  0.765625
train loss:  0.4745670557022095
train gradient:  0.10380043010411043
iteration : 12349
train acc:  0.7890625
train loss:  0.46167027950286865
train gradient:  0.08035715838516401
iteration : 12350
train acc:  0.75
train loss:  0.5548479557037354
train gradient:  0.1877971404964448
iteration : 12351
train acc:  0.703125
train loss:  0.4959769546985626
train gradient:  0.09867264695230825
iteration : 12352
train acc:  0.7890625
train loss:  0.4484909474849701
train gradient:  0.09219845355982453
iteration : 12353
train acc:  0.765625
train loss:  0.4748677611351013
train gradient:  0.11269184887831014
iteration : 12354
train acc:  0.765625
train loss:  0.47014904022216797
train gradient:  0.10755839600854754
iteration : 12355
train acc:  0.7265625
train loss:  0.5023880004882812
train gradient:  0.11186133596898279
iteration : 12356
train acc:  0.7421875
train loss:  0.4583205580711365
train gradient:  0.09776784813157288
iteration : 12357
train acc:  0.7421875
train loss:  0.47306951880455017
train gradient:  0.10449210648196268
iteration : 12358
train acc:  0.8046875
train loss:  0.4183782637119293
train gradient:  0.09213764383897725
iteration : 12359
train acc:  0.7421875
train loss:  0.4747859239578247
train gradient:  0.10966172636030065
iteration : 12360
train acc:  0.7109375
train loss:  0.5174741148948669
train gradient:  0.12617062188030895
iteration : 12361
train acc:  0.7578125
train loss:  0.467515766620636
train gradient:  0.09836424645348732
iteration : 12362
train acc:  0.6640625
train loss:  0.6317503452301025
train gradient:  0.15085972920881824
iteration : 12363
train acc:  0.6953125
train loss:  0.48117178678512573
train gradient:  0.1330465653312497
iteration : 12364
train acc:  0.6796875
train loss:  0.5554518699645996
train gradient:  0.11371176435401956
iteration : 12365
train acc:  0.6953125
train loss:  0.5274046659469604
train gradient:  0.16126974373648673
iteration : 12366
train acc:  0.78125
train loss:  0.4289814829826355
train gradient:  0.09650548844923447
iteration : 12367
train acc:  0.6875
train loss:  0.5939277410507202
train gradient:  0.14512372392628115
iteration : 12368
train acc:  0.75
train loss:  0.48524659872055054
train gradient:  0.08511989490451727
iteration : 12369
train acc:  0.75
train loss:  0.4981054663658142
train gradient:  0.08730174915196917
iteration : 12370
train acc:  0.6875
train loss:  0.5205373764038086
train gradient:  0.1463416909292869
iteration : 12371
train acc:  0.765625
train loss:  0.4613277316093445
train gradient:  0.08936003210703436
iteration : 12372
train acc:  0.7421875
train loss:  0.48866844177246094
train gradient:  0.1145741658162444
iteration : 12373
train acc:  0.7578125
train loss:  0.5470752120018005
train gradient:  0.16348823139207405
iteration : 12374
train acc:  0.7734375
train loss:  0.4514991044998169
train gradient:  0.09728770649435432
iteration : 12375
train acc:  0.7109375
train loss:  0.5033445358276367
train gradient:  0.1494055467976233
iteration : 12376
train acc:  0.703125
train loss:  0.48252320289611816
train gradient:  0.10921798014968855
iteration : 12377
train acc:  0.8203125
train loss:  0.39791178703308105
train gradient:  0.07543444695536385
iteration : 12378
train acc:  0.6953125
train loss:  0.5168406963348389
train gradient:  0.11346436055198736
iteration : 12379
train acc:  0.71875
train loss:  0.48521941900253296
train gradient:  0.10213790845177062
iteration : 12380
train acc:  0.71875
train loss:  0.5532187223434448
train gradient:  0.1602332317151598
iteration : 12381
train acc:  0.7109375
train loss:  0.5215470194816589
train gradient:  0.12705507891058065
iteration : 12382
train acc:  0.8125
train loss:  0.43560999631881714
train gradient:  0.10272346448386849
iteration : 12383
train acc:  0.7578125
train loss:  0.47641825675964355
train gradient:  0.10737737402427382
iteration : 12384
train acc:  0.7265625
train loss:  0.5140928626060486
train gradient:  0.12882996816231318
iteration : 12385
train acc:  0.8203125
train loss:  0.4077679514884949
train gradient:  0.0843907561892827
iteration : 12386
train acc:  0.734375
train loss:  0.5193657875061035
train gradient:  0.1464418700703044
iteration : 12387
train acc:  0.7734375
train loss:  0.4858575463294983
train gradient:  0.11639503924153038
iteration : 12388
train acc:  0.7265625
train loss:  0.5274426937103271
train gradient:  0.13211885586205308
iteration : 12389
train acc:  0.7265625
train loss:  0.4928703010082245
train gradient:  0.09571555633022225
iteration : 12390
train acc:  0.7265625
train loss:  0.5289540886878967
train gradient:  0.13097617118986155
iteration : 12391
train acc:  0.71875
train loss:  0.5146983861923218
train gradient:  0.11955131329652077
iteration : 12392
train acc:  0.7109375
train loss:  0.49416041374206543
train gradient:  0.11599763591624322
iteration : 12393
train acc:  0.734375
train loss:  0.5239498019218445
train gradient:  0.17164568825841614
iteration : 12394
train acc:  0.7578125
train loss:  0.45387524366378784
train gradient:  0.1167738446268638
iteration : 12395
train acc:  0.7890625
train loss:  0.4359707534313202
train gradient:  0.11130006051866953
iteration : 12396
train acc:  0.75
train loss:  0.49570977687835693
train gradient:  0.11159561390558687
iteration : 12397
train acc:  0.765625
train loss:  0.45584824681282043
train gradient:  0.10741246493957676
iteration : 12398
train acc:  0.75
train loss:  0.4709319472312927
train gradient:  0.09380524757830053
iteration : 12399
train acc:  0.6640625
train loss:  0.5260447263717651
train gradient:  0.11241363344703563
iteration : 12400
train acc:  0.71875
train loss:  0.5188171863555908
train gradient:  0.18039115218741092
iteration : 12401
train acc:  0.78125
train loss:  0.4593747854232788
train gradient:  0.11855526314580526
iteration : 12402
train acc:  0.765625
train loss:  0.46504682302474976
train gradient:  0.09991343107651708
iteration : 12403
train acc:  0.75
train loss:  0.47241300344467163
train gradient:  0.11229265806495864
iteration : 12404
train acc:  0.734375
train loss:  0.49866780638694763
train gradient:  0.10926010257553877
iteration : 12405
train acc:  0.6953125
train loss:  0.4804393947124481
train gradient:  0.10494904320117367
iteration : 12406
train acc:  0.703125
train loss:  0.5225543975830078
train gradient:  0.16907314576429536
iteration : 12407
train acc:  0.734375
train loss:  0.4785510301589966
train gradient:  0.1285633763763588
iteration : 12408
train acc:  0.6953125
train loss:  0.5560128092765808
train gradient:  0.16266550208526748
iteration : 12409
train acc:  0.6875
train loss:  0.5577951073646545
train gradient:  0.14425385833077947
iteration : 12410
train acc:  0.8359375
train loss:  0.43241292238235474
train gradient:  0.08540879727184869
iteration : 12411
train acc:  0.796875
train loss:  0.4334043264389038
train gradient:  0.09529572688574876
iteration : 12412
train acc:  0.7734375
train loss:  0.4304248094558716
train gradient:  0.08337280764695056
iteration : 12413
train acc:  0.7890625
train loss:  0.4404662847518921
train gradient:  0.08738802261755976
iteration : 12414
train acc:  0.6953125
train loss:  0.5408441424369812
train gradient:  0.12925894000565258
iteration : 12415
train acc:  0.7265625
train loss:  0.46410778164863586
train gradient:  0.08223154195671276
iteration : 12416
train acc:  0.7578125
train loss:  0.4828200340270996
train gradient:  0.10979941060468337
iteration : 12417
train acc:  0.6796875
train loss:  0.5574577450752258
train gradient:  0.12601040515499354
iteration : 12418
train acc:  0.8125
train loss:  0.4420045018196106
train gradient:  0.09130180213553603
iteration : 12419
train acc:  0.7890625
train loss:  0.42305755615234375
train gradient:  0.09239971106461382
iteration : 12420
train acc:  0.7265625
train loss:  0.5114842057228088
train gradient:  0.13335324432763299
iteration : 12421
train acc:  0.765625
train loss:  0.4863051474094391
train gradient:  0.10402436674754846
iteration : 12422
train acc:  0.734375
train loss:  0.5048996806144714
train gradient:  0.12665489453986925
iteration : 12423
train acc:  0.7421875
train loss:  0.5197916030883789
train gradient:  0.1322926969974932
iteration : 12424
train acc:  0.65625
train loss:  0.5775853395462036
train gradient:  0.14758975174157263
iteration : 12425
train acc:  0.75
train loss:  0.49040913581848145
train gradient:  0.09620256904975269
iteration : 12426
train acc:  0.7890625
train loss:  0.4759442210197449
train gradient:  0.13793555920344708
iteration : 12427
train acc:  0.703125
train loss:  0.5366829633712769
train gradient:  0.14368284863810338
iteration : 12428
train acc:  0.6640625
train loss:  0.5625936985015869
train gradient:  0.18047457872718173
iteration : 12429
train acc:  0.7578125
train loss:  0.45315107703208923
train gradient:  0.09813364422382014
iteration : 12430
train acc:  0.7578125
train loss:  0.4841178357601166
train gradient:  0.10253293912368135
iteration : 12431
train acc:  0.75
train loss:  0.5369171500205994
train gradient:  0.14332847442684077
iteration : 12432
train acc:  0.765625
train loss:  0.47203272581100464
train gradient:  0.09819213943123825
iteration : 12433
train acc:  0.7578125
train loss:  0.4783572256565094
train gradient:  0.1003002936247439
iteration : 12434
train acc:  0.7578125
train loss:  0.4750703275203705
train gradient:  0.09756191650283205
iteration : 12435
train acc:  0.765625
train loss:  0.45624032616615295
train gradient:  0.09755369651714915
iteration : 12436
train acc:  0.6953125
train loss:  0.5561195015907288
train gradient:  0.1797364568803547
iteration : 12437
train acc:  0.734375
train loss:  0.5285515189170837
train gradient:  0.10922072512398583
iteration : 12438
train acc:  0.6875
train loss:  0.5257314443588257
train gradient:  0.1263856861371431
iteration : 12439
train acc:  0.7734375
train loss:  0.46195921301841736
train gradient:  0.0789037428133177
iteration : 12440
train acc:  0.734375
train loss:  0.541754961013794
train gradient:  0.1490524640913526
iteration : 12441
train acc:  0.7578125
train loss:  0.4909117817878723
train gradient:  0.10224607701307296
iteration : 12442
train acc:  0.6796875
train loss:  0.5568735599517822
train gradient:  0.12162891490172334
iteration : 12443
train acc:  0.7890625
train loss:  0.44371914863586426
train gradient:  0.12315219357165078
iteration : 12444
train acc:  0.7265625
train loss:  0.4701152741909027
train gradient:  0.11117396613485984
iteration : 12445
train acc:  0.6953125
train loss:  0.4912814497947693
train gradient:  0.09976825778131838
iteration : 12446
train acc:  0.7890625
train loss:  0.4354459047317505
train gradient:  0.0959070565392434
iteration : 12447
train acc:  0.7109375
train loss:  0.5805947780609131
train gradient:  0.1612602779289109
iteration : 12448
train acc:  0.7421875
train loss:  0.5173007249832153
train gradient:  0.12882629002966794
iteration : 12449
train acc:  0.8046875
train loss:  0.4541579484939575
train gradient:  0.08364088898271396
iteration : 12450
train acc:  0.703125
train loss:  0.5441550016403198
train gradient:  0.14770158172615594
iteration : 12451
train acc:  0.7265625
train loss:  0.5442055463790894
train gradient:  0.13410501379936868
iteration : 12452
train acc:  0.6953125
train loss:  0.5108068585395813
train gradient:  0.12354737956777945
iteration : 12453
train acc:  0.7265625
train loss:  0.4936770796775818
train gradient:  0.11169225284208191
iteration : 12454
train acc:  0.796875
train loss:  0.4171331524848938
train gradient:  0.08813764332171414
iteration : 12455
train acc:  0.75
train loss:  0.4753936529159546
train gradient:  0.10437290148072347
iteration : 12456
train acc:  0.7734375
train loss:  0.5041890740394592
train gradient:  0.15343223184140914
iteration : 12457
train acc:  0.7265625
train loss:  0.4838714301586151
train gradient:  0.10637859097502043
iteration : 12458
train acc:  0.7421875
train loss:  0.4507097005844116
train gradient:  0.09983537897005935
iteration : 12459
train acc:  0.765625
train loss:  0.45924997329711914
train gradient:  0.09934899718819132
iteration : 12460
train acc:  0.7421875
train loss:  0.5906702280044556
train gradient:  0.12916128060053153
iteration : 12461
train acc:  0.703125
train loss:  0.5136104226112366
train gradient:  0.14103079655401057
iteration : 12462
train acc:  0.7265625
train loss:  0.4997214376926422
train gradient:  0.10938964779256999
iteration : 12463
train acc:  0.7734375
train loss:  0.45989274978637695
train gradient:  0.09507934044258139
iteration : 12464
train acc:  0.75
train loss:  0.4506949484348297
train gradient:  0.10681778271376594
iteration : 12465
train acc:  0.7734375
train loss:  0.4389217793941498
train gradient:  0.09919692774572887
iteration : 12466
train acc:  0.7421875
train loss:  0.4904012084007263
train gradient:  0.11515070320936012
iteration : 12467
train acc:  0.7421875
train loss:  0.48951882123947144
train gradient:  0.12636260310862718
iteration : 12468
train acc:  0.765625
train loss:  0.45866864919662476
train gradient:  0.11452177276087463
iteration : 12469
train acc:  0.828125
train loss:  0.4301276206970215
train gradient:  0.11562318419144355
iteration : 12470
train acc:  0.7734375
train loss:  0.5197747945785522
train gradient:  0.11524267450246452
iteration : 12471
train acc:  0.78125
train loss:  0.5154837965965271
train gradient:  0.1343751000536232
iteration : 12472
train acc:  0.6953125
train loss:  0.49975860118865967
train gradient:  0.09475898031614864
iteration : 12473
train acc:  0.71875
train loss:  0.4817847013473511
train gradient:  0.09936421437293544
iteration : 12474
train acc:  0.671875
train loss:  0.6537721157073975
train gradient:  0.2092945657032723
iteration : 12475
train acc:  0.8828125
train loss:  0.3893619179725647
train gradient:  0.0722602516778629
iteration : 12476
train acc:  0.7734375
train loss:  0.426604688167572
train gradient:  0.0816374928216236
iteration : 12477
train acc:  0.703125
train loss:  0.507083535194397
train gradient:  0.11727987770177933
iteration : 12478
train acc:  0.7890625
train loss:  0.45078521966934204
train gradient:  0.10338440971903393
iteration : 12479
train acc:  0.6796875
train loss:  0.5295224189758301
train gradient:  0.11150287327712946
iteration : 12480
train acc:  0.7109375
train loss:  0.561505913734436
train gradient:  0.1286819958698955
iteration : 12481
train acc:  0.734375
train loss:  0.489658385515213
train gradient:  0.10819170714732064
iteration : 12482
train acc:  0.6953125
train loss:  0.516529381275177
train gradient:  0.11898565833992
iteration : 12483
train acc:  0.7265625
train loss:  0.5174834132194519
train gradient:  0.12864637940807572
iteration : 12484
train acc:  0.7421875
train loss:  0.5006474852561951
train gradient:  0.1361906551422455
iteration : 12485
train acc:  0.7265625
train loss:  0.5561302900314331
train gradient:  0.14448890456880803
iteration : 12486
train acc:  0.7421875
train loss:  0.46462440490722656
train gradient:  0.08393543053653223
iteration : 12487
train acc:  0.6953125
train loss:  0.49888908863067627
train gradient:  0.10871360223807512
iteration : 12488
train acc:  0.7421875
train loss:  0.47524693608283997
train gradient:  0.0885541376962632
iteration : 12489
train acc:  0.7578125
train loss:  0.4649433493614197
train gradient:  0.08205019324102833
iteration : 12490
train acc:  0.7421875
train loss:  0.5005308389663696
train gradient:  0.11854503851116795
iteration : 12491
train acc:  0.7421875
train loss:  0.483285129070282
train gradient:  0.10081754617650197
iteration : 12492
train acc:  0.7421875
train loss:  0.4686201512813568
train gradient:  0.10552774119554936
iteration : 12493
train acc:  0.734375
train loss:  0.5217345952987671
train gradient:  0.10896256257610484
iteration : 12494
train acc:  0.6953125
train loss:  0.5667424201965332
train gradient:  0.16511482643020553
iteration : 12495
train acc:  0.8046875
train loss:  0.43568557500839233
train gradient:  0.08094231950216804
iteration : 12496
train acc:  0.7734375
train loss:  0.4694046378135681
train gradient:  0.11444046091228946
iteration : 12497
train acc:  0.7578125
train loss:  0.442569375038147
train gradient:  0.10217709219200971
iteration : 12498
train acc:  0.765625
train loss:  0.4978315234184265
train gradient:  0.11415393987634515
iteration : 12499
train acc:  0.765625
train loss:  0.46162325143814087
train gradient:  0.09059377052731127
iteration : 12500
train acc:  0.765625
train loss:  0.4636209309101105
train gradient:  0.08667300898825171
iteration : 12501
train acc:  0.7734375
train loss:  0.49253061413764954
train gradient:  0.1479857268022505
iteration : 12502
train acc:  0.734375
train loss:  0.4936445951461792
train gradient:  0.10693554716569531
iteration : 12503
train acc:  0.78125
train loss:  0.4238698184490204
train gradient:  0.10220566707041474
iteration : 12504
train acc:  0.734375
train loss:  0.4992140531539917
train gradient:  0.11302001201920107
iteration : 12505
train acc:  0.7421875
train loss:  0.5017815828323364
train gradient:  0.12664660882393688
iteration : 12506
train acc:  0.765625
train loss:  0.4874228239059448
train gradient:  0.0984668720166508
iteration : 12507
train acc:  0.703125
train loss:  0.47066256403923035
train gradient:  0.1044359945217645
iteration : 12508
train acc:  0.7421875
train loss:  0.4891657531261444
train gradient:  0.09556434472867541
iteration : 12509
train acc:  0.8125
train loss:  0.45138710737228394
train gradient:  0.1145623853486971
iteration : 12510
train acc:  0.703125
train loss:  0.5082632303237915
train gradient:  0.099131245502429
iteration : 12511
train acc:  0.6953125
train loss:  0.5259799361228943
train gradient:  0.12026442989929227
iteration : 12512
train acc:  0.7421875
train loss:  0.440289169549942
train gradient:  0.09202912584186794
iteration : 12513
train acc:  0.7421875
train loss:  0.47997626662254333
train gradient:  0.1159285813081424
iteration : 12514
train acc:  0.7890625
train loss:  0.44630080461502075
train gradient:  0.1148063148332174
iteration : 12515
train acc:  0.6328125
train loss:  0.587383508682251
train gradient:  0.15102320076183656
iteration : 12516
train acc:  0.7109375
train loss:  0.5216083526611328
train gradient:  0.1361994937758142
iteration : 12517
train acc:  0.7734375
train loss:  0.4985630512237549
train gradient:  0.12739172520288045
iteration : 12518
train acc:  0.75
train loss:  0.48887500166893005
train gradient:  0.11024582483031566
iteration : 12519
train acc:  0.75
train loss:  0.4740700125694275
train gradient:  0.0787366578886998
iteration : 12520
train acc:  0.7734375
train loss:  0.46819189190864563
train gradient:  0.0947882271463998
iteration : 12521
train acc:  0.6953125
train loss:  0.5246309041976929
train gradient:  0.13532975153076987
iteration : 12522
train acc:  0.765625
train loss:  0.5514337420463562
train gradient:  0.14150540993532923
iteration : 12523
train acc:  0.703125
train loss:  0.47313952445983887
train gradient:  0.10653542583785647
iteration : 12524
train acc:  0.7421875
train loss:  0.5231830477714539
train gradient:  0.149561635659572
iteration : 12525
train acc:  0.734375
train loss:  0.4792678952217102
train gradient:  0.10387978764018774
iteration : 12526
train acc:  0.7578125
train loss:  0.43952566385269165
train gradient:  0.08813886913859505
iteration : 12527
train acc:  0.78125
train loss:  0.4304097294807434
train gradient:  0.08558410482549421
iteration : 12528
train acc:  0.7265625
train loss:  0.5303500890731812
train gradient:  0.12905642530059838
iteration : 12529
train acc:  0.703125
train loss:  0.5269157886505127
train gradient:  0.13167063506526122
iteration : 12530
train acc:  0.765625
train loss:  0.47811394929885864
train gradient:  0.1176063003178524
iteration : 12531
train acc:  0.7578125
train loss:  0.48052898049354553
train gradient:  0.1001215291087051
iteration : 12532
train acc:  0.7265625
train loss:  0.5026440620422363
train gradient:  0.1151026729403461
iteration : 12533
train acc:  0.78125
train loss:  0.4225476384162903
train gradient:  0.0959589227273537
iteration : 12534
train acc:  0.8046875
train loss:  0.44326066970825195
train gradient:  0.08714659199031836
iteration : 12535
train acc:  0.7734375
train loss:  0.4161245822906494
train gradient:  0.06824764526838151
iteration : 12536
train acc:  0.8203125
train loss:  0.4209011495113373
train gradient:  0.1016695067014517
iteration : 12537
train acc:  0.8125
train loss:  0.39707258343696594
train gradient:  0.09514244366510882
iteration : 12538
train acc:  0.7421875
train loss:  0.5255555510520935
train gradient:  0.1718649362260804
iteration : 12539
train acc:  0.7265625
train loss:  0.515411376953125
train gradient:  0.13961691325653786
iteration : 12540
train acc:  0.7890625
train loss:  0.43099290132522583
train gradient:  0.11252226679358916
iteration : 12541
train acc:  0.7109375
train loss:  0.512350857257843
train gradient:  0.1541894918055019
iteration : 12542
train acc:  0.75
train loss:  0.46626806259155273
train gradient:  0.10029812466094994
iteration : 12543
train acc:  0.8203125
train loss:  0.40281325578689575
train gradient:  0.0794484634901476
iteration : 12544
train acc:  0.765625
train loss:  0.465725302696228
train gradient:  0.09462948514935382
iteration : 12545
train acc:  0.6953125
train loss:  0.5303441286087036
train gradient:  0.13193563351653403
iteration : 12546
train acc:  0.71875
train loss:  0.48970335721969604
train gradient:  0.11907115668236747
iteration : 12547
train acc:  0.703125
train loss:  0.50199294090271
train gradient:  0.10452970934264225
iteration : 12548
train acc:  0.75
train loss:  0.45118361711502075
train gradient:  0.09022865418961382
iteration : 12549
train acc:  0.8203125
train loss:  0.39581093192100525
train gradient:  0.07485485759212274
iteration : 12550
train acc:  0.7734375
train loss:  0.4273052215576172
train gradient:  0.09967063559620563
iteration : 12551
train acc:  0.71875
train loss:  0.597277045249939
train gradient:  0.2085074521283287
iteration : 12552
train acc:  0.734375
train loss:  0.5371197462081909
train gradient:  0.17430621341971891
iteration : 12553
train acc:  0.703125
train loss:  0.515349805355072
train gradient:  0.12013532029877212
iteration : 12554
train acc:  0.8359375
train loss:  0.4085276424884796
train gradient:  0.09342030692564388
iteration : 12555
train acc:  0.8046875
train loss:  0.4437345266342163
train gradient:  0.11851265054721816
iteration : 12556
train acc:  0.6484375
train loss:  0.5491087436676025
train gradient:  0.16750140011431347
iteration : 12557
train acc:  0.7265625
train loss:  0.44995006918907166
train gradient:  0.08577448568122387
iteration : 12558
train acc:  0.7578125
train loss:  0.5148051977157593
train gradient:  0.12172547552848759
iteration : 12559
train acc:  0.7578125
train loss:  0.4771779179573059
train gradient:  0.1663747582732254
iteration : 12560
train acc:  0.7734375
train loss:  0.4241914749145508
train gradient:  0.0841016756164432
iteration : 12561
train acc:  0.765625
train loss:  0.4714551568031311
train gradient:  0.12217320702962348
iteration : 12562
train acc:  0.7578125
train loss:  0.5009446144104004
train gradient:  0.11538394607987959
iteration : 12563
train acc:  0.7734375
train loss:  0.4567749500274658
train gradient:  0.11694480892540561
iteration : 12564
train acc:  0.7265625
train loss:  0.48316675424575806
train gradient:  0.09791708676010893
iteration : 12565
train acc:  0.7578125
train loss:  0.4867593050003052
train gradient:  0.13110868061867753
iteration : 12566
train acc:  0.828125
train loss:  0.43878230452537537
train gradient:  0.10754912366421976
iteration : 12567
train acc:  0.671875
train loss:  0.5651307702064514
train gradient:  0.1735077300259789
iteration : 12568
train acc:  0.7578125
train loss:  0.5017828941345215
train gradient:  0.14487023489757722
iteration : 12569
train acc:  0.7890625
train loss:  0.450492262840271
train gradient:  0.10606186885464551
iteration : 12570
train acc:  0.7734375
train loss:  0.48069119453430176
train gradient:  0.11765030400370587
iteration : 12571
train acc:  0.75
train loss:  0.4961574673652649
train gradient:  0.11603856936483811
iteration : 12572
train acc:  0.7578125
train loss:  0.43263882398605347
train gradient:  0.09226186829401437
iteration : 12573
train acc:  0.7734375
train loss:  0.46252456307411194
train gradient:  0.10762975388865888
iteration : 12574
train acc:  0.7421875
train loss:  0.4651404619216919
train gradient:  0.0996752670911486
iteration : 12575
train acc:  0.6875
train loss:  0.5223482847213745
train gradient:  0.11291901556907748
iteration : 12576
train acc:  0.7578125
train loss:  0.5195938348770142
train gradient:  0.1209192225763722
iteration : 12577
train acc:  0.7421875
train loss:  0.453689306974411
train gradient:  0.1148550945733711
iteration : 12578
train acc:  0.765625
train loss:  0.4585127830505371
train gradient:  0.11390779438355918
iteration : 12579
train acc:  0.7421875
train loss:  0.5338611602783203
train gradient:  0.13057839435027466
iteration : 12580
train acc:  0.7265625
train loss:  0.465704083442688
train gradient:  0.09378025598316035
iteration : 12581
train acc:  0.6875
train loss:  0.5308225154876709
train gradient:  0.17769927343472697
iteration : 12582
train acc:  0.6875
train loss:  0.5178720355033875
train gradient:  0.1081019494572832
iteration : 12583
train acc:  0.7109375
train loss:  0.5004332661628723
train gradient:  0.12793943518413664
iteration : 12584
train acc:  0.7578125
train loss:  0.4924590587615967
train gradient:  0.12211594092208397
iteration : 12585
train acc:  0.765625
train loss:  0.4930226504802704
train gradient:  0.12481789848881815
iteration : 12586
train acc:  0.703125
train loss:  0.5017380118370056
train gradient:  0.1397258212379923
iteration : 12587
train acc:  0.765625
train loss:  0.40909039974212646
train gradient:  0.09817309106736104
iteration : 12588
train acc:  0.734375
train loss:  0.5156040191650391
train gradient:  0.11900413645551894
iteration : 12589
train acc:  0.796875
train loss:  0.41111838817596436
train gradient:  0.08291433724491956
iteration : 12590
train acc:  0.828125
train loss:  0.443120539188385
train gradient:  0.09819161293875854
iteration : 12591
train acc:  0.734375
train loss:  0.5119104385375977
train gradient:  0.15078588206425417
iteration : 12592
train acc:  0.765625
train loss:  0.450001984834671
train gradient:  0.09749738046716791
iteration : 12593
train acc:  0.7890625
train loss:  0.4617706537246704
train gradient:  0.11158791571768541
iteration : 12594
train acc:  0.734375
train loss:  0.48862236738204956
train gradient:  0.1332684748404378
iteration : 12595
train acc:  0.7578125
train loss:  0.5183849334716797
train gradient:  0.11714858010816517
iteration : 12596
train acc:  0.6796875
train loss:  0.5615706443786621
train gradient:  0.17038124436312685
iteration : 12597
train acc:  0.703125
train loss:  0.5284776091575623
train gradient:  0.1542266033331281
iteration : 12598
train acc:  0.78125
train loss:  0.46770310401916504
train gradient:  0.09953121581536749
iteration : 12599
train acc:  0.7421875
train loss:  0.4525962471961975
train gradient:  0.10348118555741553
iteration : 12600
train acc:  0.78125
train loss:  0.43661174178123474
train gradient:  0.10179602615596055
iteration : 12601
train acc:  0.7734375
train loss:  0.5191202163696289
train gradient:  0.10947261071250403
iteration : 12602
train acc:  0.71875
train loss:  0.5861196517944336
train gradient:  0.20588200643075724
iteration : 12603
train acc:  0.75
train loss:  0.524106502532959
train gradient:  0.13202001214693249
iteration : 12604
train acc:  0.7421875
train loss:  0.49976879358291626
train gradient:  0.12599917389742754
iteration : 12605
train acc:  0.6484375
train loss:  0.5196099877357483
train gradient:  0.10936724790414415
iteration : 12606
train acc:  0.6875
train loss:  0.5515972971916199
train gradient:  0.15051143087125685
iteration : 12607
train acc:  0.71875
train loss:  0.5403109788894653
train gradient:  0.17040979824937258
iteration : 12608
train acc:  0.7421875
train loss:  0.5691741108894348
train gradient:  0.14434655656329373
iteration : 12609
train acc:  0.7421875
train loss:  0.5083364248275757
train gradient:  0.11216415200668432
iteration : 12610
train acc:  0.7265625
train loss:  0.4943946897983551
train gradient:  0.1590704543081736
iteration : 12611
train acc:  0.75
train loss:  0.47472822666168213
train gradient:  0.11489416316637409
iteration : 12612
train acc:  0.78125
train loss:  0.4799363911151886
train gradient:  0.12824908744521069
iteration : 12613
train acc:  0.6875
train loss:  0.5293593406677246
train gradient:  0.14506382948437901
iteration : 12614
train acc:  0.6875
train loss:  0.517963171005249
train gradient:  0.12434721504636181
iteration : 12615
train acc:  0.78125
train loss:  0.46663621068000793
train gradient:  0.1097627519867049
iteration : 12616
train acc:  0.640625
train loss:  0.664595365524292
train gradient:  0.2534442727327393
iteration : 12617
train acc:  0.7734375
train loss:  0.5078259706497192
train gradient:  0.14598016420458448
iteration : 12618
train acc:  0.765625
train loss:  0.4187953770160675
train gradient:  0.0894546353300617
iteration : 12619
train acc:  0.7578125
train loss:  0.5024926662445068
train gradient:  0.10990540951077565
iteration : 12620
train acc:  0.8359375
train loss:  0.3916771411895752
train gradient:  0.0737937556508906
iteration : 12621
train acc:  0.796875
train loss:  0.4408494234085083
train gradient:  0.10887650240045287
iteration : 12622
train acc:  0.734375
train loss:  0.5361441373825073
train gradient:  0.11343233279196985
iteration : 12623
train acc:  0.7734375
train loss:  0.45513594150543213
train gradient:  0.09734401967507247
iteration : 12624
train acc:  0.7578125
train loss:  0.504507839679718
train gradient:  0.1242987137433488
iteration : 12625
train acc:  0.796875
train loss:  0.4839645028114319
train gradient:  0.10794676159820261
iteration : 12626
train acc:  0.765625
train loss:  0.5673544406890869
train gradient:  0.17294617948873642
iteration : 12627
train acc:  0.75
train loss:  0.49098455905914307
train gradient:  0.13705524757921023
iteration : 12628
train acc:  0.71875
train loss:  0.5150912404060364
train gradient:  0.10213311101486837
iteration : 12629
train acc:  0.71875
train loss:  0.5154699683189392
train gradient:  0.12734238603098474
iteration : 12630
train acc:  0.8046875
train loss:  0.41504156589508057
train gradient:  0.07739826052469656
iteration : 12631
train acc:  0.7109375
train loss:  0.5249693989753723
train gradient:  0.14301395284799834
iteration : 12632
train acc:  0.7265625
train loss:  0.5770341157913208
train gradient:  0.13608936773320335
iteration : 12633
train acc:  0.7890625
train loss:  0.4557284116744995
train gradient:  0.11268279405842088
iteration : 12634
train acc:  0.7421875
train loss:  0.48378610610961914
train gradient:  0.12449299059201817
iteration : 12635
train acc:  0.65625
train loss:  0.587356686592102
train gradient:  0.13409478985106005
iteration : 12636
train acc:  0.734375
train loss:  0.5070657730102539
train gradient:  0.13235805394899983
iteration : 12637
train acc:  0.78125
train loss:  0.47694242000579834
train gradient:  0.11076081446975089
iteration : 12638
train acc:  0.78125
train loss:  0.47016775608062744
train gradient:  0.08509850604438839
iteration : 12639
train acc:  0.7734375
train loss:  0.4569779634475708
train gradient:  0.10221901038921885
iteration : 12640
train acc:  0.7109375
train loss:  0.5633617043495178
train gradient:  0.15241499998228175
iteration : 12641
train acc:  0.796875
train loss:  0.4549012780189514
train gradient:  0.09424499946044006
iteration : 12642
train acc:  0.75
train loss:  0.48542320728302
train gradient:  0.11614862960856771
iteration : 12643
train acc:  0.7578125
train loss:  0.4707740247249603
train gradient:  0.0950578047912655
iteration : 12644
train acc:  0.71875
train loss:  0.5533831119537354
train gradient:  0.17710220283006534
iteration : 12645
train acc:  0.7421875
train loss:  0.4728805422782898
train gradient:  0.12170470890133105
iteration : 12646
train acc:  0.7421875
train loss:  0.4640498757362366
train gradient:  0.1364322239067396
iteration : 12647
train acc:  0.796875
train loss:  0.3991611897945404
train gradient:  0.08862495341768649
iteration : 12648
train acc:  0.7734375
train loss:  0.5011897087097168
train gradient:  0.08938891682731005
iteration : 12649
train acc:  0.7734375
train loss:  0.4193609952926636
train gradient:  0.1126383303910077
iteration : 12650
train acc:  0.6875
train loss:  0.5433019399642944
train gradient:  0.1221699081582386
iteration : 12651
train acc:  0.7265625
train loss:  0.4535609781742096
train gradient:  0.11240962992232405
iteration : 12652
train acc:  0.734375
train loss:  0.45141804218292236
train gradient:  0.09297808619960947
iteration : 12653
train acc:  0.796875
train loss:  0.459868460893631
train gradient:  0.1327594852133564
iteration : 12654
train acc:  0.7578125
train loss:  0.5402768850326538
train gradient:  0.1535683708649237
iteration : 12655
train acc:  0.7421875
train loss:  0.46733608841896057
train gradient:  0.10412359619698083
iteration : 12656
train acc:  0.7109375
train loss:  0.5213940143585205
train gradient:  0.16753517466314705
iteration : 12657
train acc:  0.765625
train loss:  0.4327523410320282
train gradient:  0.08425131142650275
iteration : 12658
train acc:  0.6796875
train loss:  0.49622488021850586
train gradient:  0.13073132518275413
iteration : 12659
train acc:  0.75
train loss:  0.46108707785606384
train gradient:  0.13573843406492303
iteration : 12660
train acc:  0.7734375
train loss:  0.45116934180259705
train gradient:  0.12663600029698074
iteration : 12661
train acc:  0.7734375
train loss:  0.4831644892692566
train gradient:  0.1433341091173671
iteration : 12662
train acc:  0.8203125
train loss:  0.4292551279067993
train gradient:  0.08844172773424491
iteration : 12663
train acc:  0.7578125
train loss:  0.4778274893760681
train gradient:  0.10145322164509482
iteration : 12664
train acc:  0.703125
train loss:  0.5355381965637207
train gradient:  0.12498412336104152
iteration : 12665
train acc:  0.7109375
train loss:  0.5595536231994629
train gradient:  0.13957753163207698
iteration : 12666
train acc:  0.8203125
train loss:  0.4312972128391266
train gradient:  0.09318021898260885
iteration : 12667
train acc:  0.7109375
train loss:  0.543583333492279
train gradient:  0.15203173497019318
iteration : 12668
train acc:  0.7890625
train loss:  0.4606874883174896
train gradient:  0.10807083068641792
iteration : 12669
train acc:  0.71875
train loss:  0.5517536997795105
train gradient:  0.13562217694913623
iteration : 12670
train acc:  0.7578125
train loss:  0.49467048048973083
train gradient:  0.14804536827297268
iteration : 12671
train acc:  0.75
train loss:  0.46216142177581787
train gradient:  0.10296239835862972
iteration : 12672
train acc:  0.7109375
train loss:  0.5626596212387085
train gradient:  0.14846719745442646
iteration : 12673
train acc:  0.75
train loss:  0.5007076263427734
train gradient:  0.11827818511546298
iteration : 12674
train acc:  0.6796875
train loss:  0.5485321283340454
train gradient:  0.13440960425059123
iteration : 12675
train acc:  0.78125
train loss:  0.44744133949279785
train gradient:  0.10258904840592283
iteration : 12676
train acc:  0.84375
train loss:  0.4203348457813263
train gradient:  0.09162844840097559
iteration : 12677
train acc:  0.765625
train loss:  0.5349881052970886
train gradient:  0.1399837022385254
iteration : 12678
train acc:  0.7421875
train loss:  0.5130126476287842
train gradient:  0.1142555751275166
iteration : 12679
train acc:  0.7265625
train loss:  0.48501527309417725
train gradient:  0.09179009088033327
iteration : 12680
train acc:  0.78125
train loss:  0.44631427526474
train gradient:  0.09852867991637261
iteration : 12681
train acc:  0.765625
train loss:  0.4915551543235779
train gradient:  0.11026890489436562
iteration : 12682
train acc:  0.6875
train loss:  0.5378704071044922
train gradient:  0.15605205792898566
iteration : 12683
train acc:  0.7421875
train loss:  0.48360657691955566
train gradient:  0.09650107339335479
iteration : 12684
train acc:  0.7578125
train loss:  0.4707084894180298
train gradient:  0.10325595578632996
iteration : 12685
train acc:  0.765625
train loss:  0.47020775079727173
train gradient:  0.09946565409152645
iteration : 12686
train acc:  0.765625
train loss:  0.48594218492507935
train gradient:  0.11077708077802723
iteration : 12687
train acc:  0.734375
train loss:  0.4810028076171875
train gradient:  0.09741793287140589
iteration : 12688
train acc:  0.7109375
train loss:  0.555465579032898
train gradient:  0.1487525017573776
iteration : 12689
train acc:  0.75
train loss:  0.4631025195121765
train gradient:  0.0995610828446051
iteration : 12690
train acc:  0.734375
train loss:  0.4829813241958618
train gradient:  0.10549608534794722
iteration : 12691
train acc:  0.8046875
train loss:  0.44477033615112305
train gradient:  0.09211089918415513
iteration : 12692
train acc:  0.7578125
train loss:  0.4910126328468323
train gradient:  0.11735202858037601
iteration : 12693
train acc:  0.7734375
train loss:  0.5060622096061707
train gradient:  0.15759870213479416
iteration : 12694
train acc:  0.703125
train loss:  0.538648247718811
train gradient:  0.1469191916291906
iteration : 12695
train acc:  0.7578125
train loss:  0.45429807901382446
train gradient:  0.09106624080015424
iteration : 12696
train acc:  0.8125
train loss:  0.45152366161346436
train gradient:  0.09885376077945143
iteration : 12697
train acc:  0.734375
train loss:  0.4668706953525543
train gradient:  0.12966794023149178
iteration : 12698
train acc:  0.765625
train loss:  0.5108534693717957
train gradient:  0.13256376965781314
iteration : 12699
train acc:  0.7421875
train loss:  0.5146448016166687
train gradient:  0.1763383053414765
iteration : 12700
train acc:  0.7109375
train loss:  0.4806167781352997
train gradient:  0.11336522340372257
iteration : 12701
train acc:  0.6796875
train loss:  0.5895761251449585
train gradient:  0.16878989804620842
iteration : 12702
train acc:  0.7734375
train loss:  0.4561630189418793
train gradient:  0.08860844764291192
iteration : 12703
train acc:  0.7265625
train loss:  0.4522410035133362
train gradient:  0.08774712452066841
iteration : 12704
train acc:  0.796875
train loss:  0.4997081160545349
train gradient:  0.1271142031483722
iteration : 12705
train acc:  0.734375
train loss:  0.49947038292884827
train gradient:  0.11389887177413757
iteration : 12706
train acc:  0.6875
train loss:  0.5404518246650696
train gradient:  0.1262385499918196
iteration : 12707
train acc:  0.75
train loss:  0.53264319896698
train gradient:  0.12259870285156976
iteration : 12708
train acc:  0.7421875
train loss:  0.4634161591529846
train gradient:  0.09931048548796811
iteration : 12709
train acc:  0.796875
train loss:  0.4604918956756592
train gradient:  0.08281377782808996
iteration : 12710
train acc:  0.6328125
train loss:  0.6033322215080261
train gradient:  0.17569929060599407
iteration : 12711
train acc:  0.796875
train loss:  0.45463037490844727
train gradient:  0.13511976880068327
iteration : 12712
train acc:  0.734375
train loss:  0.4857980012893677
train gradient:  0.11121337802485723
iteration : 12713
train acc:  0.765625
train loss:  0.48636484146118164
train gradient:  0.11823956268396017
iteration : 12714
train acc:  0.7578125
train loss:  0.5065727233886719
train gradient:  0.13288445129412152
iteration : 12715
train acc:  0.6953125
train loss:  0.5009844899177551
train gradient:  0.11513999379380668
iteration : 12716
train acc:  0.7421875
train loss:  0.48943382501602173
train gradient:  0.10923700711198116
iteration : 12717
train acc:  0.7578125
train loss:  0.4883063733577728
train gradient:  0.10729209305775496
iteration : 12718
train acc:  0.7734375
train loss:  0.4368782043457031
train gradient:  0.09464536908541785
iteration : 12719
train acc:  0.7578125
train loss:  0.4396110475063324
train gradient:  0.08756359136406867
iteration : 12720
train acc:  0.7265625
train loss:  0.512824535369873
train gradient:  0.11115119872287378
iteration : 12721
train acc:  0.796875
train loss:  0.46108150482177734
train gradient:  0.09457036003979065
iteration : 12722
train acc:  0.703125
train loss:  0.49923303723335266
train gradient:  0.12300837595738878
iteration : 12723
train acc:  0.75
train loss:  0.4531720280647278
train gradient:  0.11127347287260785
iteration : 12724
train acc:  0.7578125
train loss:  0.4595552682876587
train gradient:  0.08941287743958483
iteration : 12725
train acc:  0.734375
train loss:  0.5026121735572815
train gradient:  0.12309193722498235
iteration : 12726
train acc:  0.6875
train loss:  0.5835103988647461
train gradient:  0.17861570377715338
iteration : 12727
train acc:  0.7109375
train loss:  0.5231668949127197
train gradient:  0.1483044777195141
iteration : 12728
train acc:  0.71875
train loss:  0.5079136490821838
train gradient:  0.10473699546115635
iteration : 12729
train acc:  0.734375
train loss:  0.5144631266593933
train gradient:  0.12039639871423194
iteration : 12730
train acc:  0.765625
train loss:  0.4845348298549652
train gradient:  0.15286571714005703
iteration : 12731
train acc:  0.7421875
train loss:  0.5076494216918945
train gradient:  0.1081004872730229
iteration : 12732
train acc:  0.703125
train loss:  0.5796926021575928
train gradient:  0.15479033190707325
iteration : 12733
train acc:  0.765625
train loss:  0.42142215371131897
train gradient:  0.10445938718379578
iteration : 12734
train acc:  0.7265625
train loss:  0.5110582709312439
train gradient:  0.1266030348728007
iteration : 12735
train acc:  0.71875
train loss:  0.5373700261116028
train gradient:  0.13829216485576257
iteration : 12736
train acc:  0.6953125
train loss:  0.5167791247367859
train gradient:  0.12567725207389308
iteration : 12737
train acc:  0.78125
train loss:  0.4485248327255249
train gradient:  0.10424547856198942
iteration : 12738
train acc:  0.7421875
train loss:  0.509097695350647
train gradient:  0.11620858800002357
iteration : 12739
train acc:  0.7890625
train loss:  0.4810459315776825
train gradient:  0.12911334185254758
iteration : 12740
train acc:  0.7734375
train loss:  0.4409756362438202
train gradient:  0.07992835720297707
iteration : 12741
train acc:  0.7578125
train loss:  0.4528328478336334
train gradient:  0.082369781578746
iteration : 12742
train acc:  0.78125
train loss:  0.44522547721862793
train gradient:  0.08174016961680178
iteration : 12743
train acc:  0.75
train loss:  0.45759502053260803
train gradient:  0.09451691663891636
iteration : 12744
train acc:  0.765625
train loss:  0.5469973683357239
train gradient:  0.23414867868426914
iteration : 12745
train acc:  0.7109375
train loss:  0.5490535497665405
train gradient:  0.15345976021589258
iteration : 12746
train acc:  0.78125
train loss:  0.4863467812538147
train gradient:  0.12405563033942507
iteration : 12747
train acc:  0.75
train loss:  0.44014275074005127
train gradient:  0.1102235156354476
iteration : 12748
train acc:  0.6953125
train loss:  0.5288415551185608
train gradient:  0.13038344429298437
iteration : 12749
train acc:  0.7421875
train loss:  0.4641074240207672
train gradient:  0.10918097480552712
iteration : 12750
train acc:  0.734375
train loss:  0.49882030487060547
train gradient:  0.1098613540982929
iteration : 12751
train acc:  0.7734375
train loss:  0.4273502230644226
train gradient:  0.10880876496352014
iteration : 12752
train acc:  0.7890625
train loss:  0.46792158484458923
train gradient:  0.09115422669157215
iteration : 12753
train acc:  0.6875
train loss:  0.5566920042037964
train gradient:  0.1686591439084591
iteration : 12754
train acc:  0.7109375
train loss:  0.543559193611145
train gradient:  0.1451242373709315
iteration : 12755
train acc:  0.734375
train loss:  0.517605721950531
train gradient:  0.15144669324484483
iteration : 12756
train acc:  0.7109375
train loss:  0.5323805809020996
train gradient:  0.14431712038699268
iteration : 12757
train acc:  0.71875
train loss:  0.5078720450401306
train gradient:  0.12030880740500689
iteration : 12758
train acc:  0.703125
train loss:  0.5780008435249329
train gradient:  0.15905281998468818
iteration : 12759
train acc:  0.75
train loss:  0.49546679854393005
train gradient:  0.10892560196779569
iteration : 12760
train acc:  0.8359375
train loss:  0.41297757625579834
train gradient:  0.09444814761411204
iteration : 12761
train acc:  0.6953125
train loss:  0.5194737911224365
train gradient:  0.13632368003743534
iteration : 12762
train acc:  0.7109375
train loss:  0.5729422569274902
train gradient:  0.13745904208720802
iteration : 12763
train acc:  0.8046875
train loss:  0.4822021722793579
train gradient:  0.10525354369688711
iteration : 12764
train acc:  0.6640625
train loss:  0.6167361736297607
train gradient:  0.1968600005732459
iteration : 12765
train acc:  0.75
train loss:  0.5339500308036804
train gradient:  0.13617228064069448
iteration : 12766
train acc:  0.703125
train loss:  0.5265541076660156
train gradient:  0.15840950721454153
iteration : 12767
train acc:  0.7109375
train loss:  0.5945870876312256
train gradient:  0.16788960021950416
iteration : 12768
train acc:  0.765625
train loss:  0.455593466758728
train gradient:  0.10219464891808591
iteration : 12769
train acc:  0.7265625
train loss:  0.5042526721954346
train gradient:  0.11392612718108133
iteration : 12770
train acc:  0.7265625
train loss:  0.528468132019043
train gradient:  0.1342009500597903
iteration : 12771
train acc:  0.78125
train loss:  0.47234010696411133
train gradient:  0.09836542365605523
iteration : 12772
train acc:  0.7265625
train loss:  0.49916163086891174
train gradient:  0.13629346142055612
iteration : 12773
train acc:  0.734375
train loss:  0.46783578395843506
train gradient:  0.11099988120269666
iteration : 12774
train acc:  0.8125
train loss:  0.4328475296497345
train gradient:  0.09943425825265427
iteration : 12775
train acc:  0.7890625
train loss:  0.45885926485061646
train gradient:  0.08267910477128672
iteration : 12776
train acc:  0.765625
train loss:  0.46173906326293945
train gradient:  0.11432216903825987
iteration : 12777
train acc:  0.7109375
train loss:  0.528442919254303
train gradient:  0.11883429796756885
iteration : 12778
train acc:  0.765625
train loss:  0.44803619384765625
train gradient:  0.1010853598818236
iteration : 12779
train acc:  0.78125
train loss:  0.4558242857456207
train gradient:  0.11557730779379756
iteration : 12780
train acc:  0.75
train loss:  0.45360058546066284
train gradient:  0.0992741921098835
iteration : 12781
train acc:  0.7578125
train loss:  0.5153721570968628
train gradient:  0.12191171717129469
iteration : 12782
train acc:  0.734375
train loss:  0.5240607857704163
train gradient:  0.14094206014418958
iteration : 12783
train acc:  0.7890625
train loss:  0.422488808631897
train gradient:  0.07386457076529246
iteration : 12784
train acc:  0.8046875
train loss:  0.4564715027809143
train gradient:  0.12431818158758769
iteration : 12785
train acc:  0.6875
train loss:  0.5474629402160645
train gradient:  0.1420992581190506
iteration : 12786
train acc:  0.6796875
train loss:  0.5121481418609619
train gradient:  0.12411203712178284
iteration : 12787
train acc:  0.6953125
train loss:  0.5523898601531982
train gradient:  0.1454237955787615
iteration : 12788
train acc:  0.765625
train loss:  0.42908257246017456
train gradient:  0.12079586440537121
iteration : 12789
train acc:  0.8359375
train loss:  0.47165921330451965
train gradient:  0.10492274985179631
iteration : 12790
train acc:  0.7109375
train loss:  0.5503630638122559
train gradient:  0.13335368900008668
iteration : 12791
train acc:  0.7890625
train loss:  0.4556901454925537
train gradient:  0.10350597618877533
iteration : 12792
train acc:  0.7421875
train loss:  0.4602155387401581
train gradient:  0.11899852134312681
iteration : 12793
train acc:  0.765625
train loss:  0.42850708961486816
train gradient:  0.07930673240099358
iteration : 12794
train acc:  0.7578125
train loss:  0.4823720455169678
train gradient:  0.12372339216389577
iteration : 12795
train acc:  0.7734375
train loss:  0.42388981580734253
train gradient:  0.09612676030526349
iteration : 12796
train acc:  0.8046875
train loss:  0.48551681637763977
train gradient:  0.1433431317548914
iteration : 12797
train acc:  0.7890625
train loss:  0.4659830629825592
train gradient:  0.1098700512934244
iteration : 12798
train acc:  0.78125
train loss:  0.4708743691444397
train gradient:  0.12462699292799416
iteration : 12799
train acc:  0.8203125
train loss:  0.4312970042228699
train gradient:  0.11500091301146949
iteration : 12800
train acc:  0.75
train loss:  0.5232534408569336
train gradient:  0.12579562669651914
iteration : 12801
train acc:  0.7421875
train loss:  0.4965397119522095
train gradient:  0.10361359065537759
iteration : 12802
train acc:  0.75
train loss:  0.5031638145446777
train gradient:  0.13119080302394248
iteration : 12803
train acc:  0.75
train loss:  0.5255231261253357
train gradient:  0.163537486774633
iteration : 12804
train acc:  0.796875
train loss:  0.43472838401794434
train gradient:  0.07968219913812666
iteration : 12805
train acc:  0.7578125
train loss:  0.5311990976333618
train gradient:  0.12343143084648565
iteration : 12806
train acc:  0.640625
train loss:  0.5378109812736511
train gradient:  0.12590619106742065
iteration : 12807
train acc:  0.6875
train loss:  0.5960133671760559
train gradient:  0.16634330814477544
iteration : 12808
train acc:  0.765625
train loss:  0.5106858611106873
train gradient:  0.11995404904646521
iteration : 12809
train acc:  0.75
train loss:  0.5112830996513367
train gradient:  0.12257946083670372
iteration : 12810
train acc:  0.7578125
train loss:  0.5126694440841675
train gradient:  0.1317758993394989
iteration : 12811
train acc:  0.6875
train loss:  0.5334903001785278
train gradient:  0.11563505581559143
iteration : 12812
train acc:  0.7265625
train loss:  0.5680464506149292
train gradient:  0.14883065006455254
iteration : 12813
train acc:  0.6953125
train loss:  0.5216619968414307
train gradient:  0.13230478000892515
iteration : 12814
train acc:  0.6640625
train loss:  0.5671417713165283
train gradient:  0.1377862657751639
iteration : 12815
train acc:  0.7578125
train loss:  0.4804379642009735
train gradient:  0.10642779822600536
iteration : 12816
train acc:  0.703125
train loss:  0.5283111929893494
train gradient:  0.13455955918994217
iteration : 12817
train acc:  0.7734375
train loss:  0.47495782375335693
train gradient:  0.11823246370035498
iteration : 12818
train acc:  0.7890625
train loss:  0.424759179353714
train gradient:  0.0809255554577934
iteration : 12819
train acc:  0.75
train loss:  0.42143669724464417
train gradient:  0.084276425152875
iteration : 12820
train acc:  0.78125
train loss:  0.4530579745769501
train gradient:  0.10518858975237151
iteration : 12821
train acc:  0.7578125
train loss:  0.5080146193504333
train gradient:  0.09903365072842142
iteration : 12822
train acc:  0.75
train loss:  0.5212987065315247
train gradient:  0.12430954957990957
iteration : 12823
train acc:  0.7265625
train loss:  0.4634885787963867
train gradient:  0.10960775362411983
iteration : 12824
train acc:  0.6796875
train loss:  0.54265296459198
train gradient:  0.14968296997941022
iteration : 12825
train acc:  0.8125
train loss:  0.4625622034072876
train gradient:  0.16148956104849316
iteration : 12826
train acc:  0.7734375
train loss:  0.4266922175884247
train gradient:  0.0886775493021643
iteration : 12827
train acc:  0.71875
train loss:  0.545535147190094
train gradient:  0.12413158041871497
iteration : 12828
train acc:  0.78125
train loss:  0.4345654547214508
train gradient:  0.11067609015187374
iteration : 12829
train acc:  0.84375
train loss:  0.41221338510513306
train gradient:  0.09467537975088755
iteration : 12830
train acc:  0.7578125
train loss:  0.43499940633773804
train gradient:  0.09310733458509388
iteration : 12831
train acc:  0.7734375
train loss:  0.45423561334609985
train gradient:  0.09850389787711593
iteration : 12832
train acc:  0.7734375
train loss:  0.4288042187690735
train gradient:  0.09584906531110571
iteration : 12833
train acc:  0.7421875
train loss:  0.49963611364364624
train gradient:  0.13397486201812658
iteration : 12834
train acc:  0.7109375
train loss:  0.5699702501296997
train gradient:  0.12824535426060268
iteration : 12835
train acc:  0.7265625
train loss:  0.4882015585899353
train gradient:  0.1069539510948662
iteration : 12836
train acc:  0.7421875
train loss:  0.45647478103637695
train gradient:  0.09205384074634693
iteration : 12837
train acc:  0.7421875
train loss:  0.49758440256118774
train gradient:  0.11241628438114405
iteration : 12838
train acc:  0.65625
train loss:  0.5825823545455933
train gradient:  0.18217660746891895
iteration : 12839
train acc:  0.7734375
train loss:  0.5025820732116699
train gradient:  0.15042813506621372
iteration : 12840
train acc:  0.7421875
train loss:  0.4967045485973358
train gradient:  0.1279986014413838
iteration : 12841
train acc:  0.7421875
train loss:  0.45788684487342834
train gradient:  0.10053338631494407
iteration : 12842
train acc:  0.8125
train loss:  0.4285145401954651
train gradient:  0.11232157876966628
iteration : 12843
train acc:  0.75
train loss:  0.5435934662818909
train gradient:  0.1440128761254404
iteration : 12844
train acc:  0.7265625
train loss:  0.5222622752189636
train gradient:  0.1516239486538148
iteration : 12845
train acc:  0.78125
train loss:  0.4362533688545227
train gradient:  0.09209574526577102
iteration : 12846
train acc:  0.7578125
train loss:  0.4668768346309662
train gradient:  0.10024507245189503
iteration : 12847
train acc:  0.7890625
train loss:  0.4761732220649719
train gradient:  0.11853998158452259
iteration : 12848
train acc:  0.75
train loss:  0.44875574111938477
train gradient:  0.11360473446489015
iteration : 12849
train acc:  0.7734375
train loss:  0.4448927044868469
train gradient:  0.09786108535358917
iteration : 12850
train acc:  0.703125
train loss:  0.5064519047737122
train gradient:  0.11537788286742114
iteration : 12851
train acc:  0.765625
train loss:  0.43602293729782104
train gradient:  0.11031468098660735
iteration : 12852
train acc:  0.796875
train loss:  0.44591519236564636
train gradient:  0.11734566983013288
iteration : 12853
train acc:  0.8203125
train loss:  0.3831477761268616
train gradient:  0.07899091505044721
iteration : 12854
train acc:  0.75
train loss:  0.49998292326927185
train gradient:  0.08698277560450361
iteration : 12855
train acc:  0.765625
train loss:  0.4827615022659302
train gradient:  0.10696584636509862
iteration : 12856
train acc:  0.71875
train loss:  0.488874226808548
train gradient:  0.10583589956978763
iteration : 12857
train acc:  0.71875
train loss:  0.5352447628974915
train gradient:  0.12651763880373412
iteration : 12858
train acc:  0.78125
train loss:  0.4804779589176178
train gradient:  0.10707482240358933
iteration : 12859
train acc:  0.765625
train loss:  0.45832744240760803
train gradient:  0.10496938217444411
iteration : 12860
train acc:  0.75
train loss:  0.43752118945121765
train gradient:  0.09621955941694899
iteration : 12861
train acc:  0.7421875
train loss:  0.48671168088912964
train gradient:  0.15063900117035178
iteration : 12862
train acc:  0.640625
train loss:  0.5285981893539429
train gradient:  0.16726349121350553
iteration : 12863
train acc:  0.828125
train loss:  0.3655221462249756
train gradient:  0.11368743059986594
iteration : 12864
train acc:  0.6796875
train loss:  0.5908421277999878
train gradient:  0.17325872342008802
iteration : 12865
train acc:  0.734375
train loss:  0.4647841155529022
train gradient:  0.09519643526482661
iteration : 12866
train acc:  0.6875
train loss:  0.5666406750679016
train gradient:  0.14990322875813905
iteration : 12867
train acc:  0.8046875
train loss:  0.45375409722328186
train gradient:  0.10150978519639094
iteration : 12868
train acc:  0.734375
train loss:  0.5142265558242798
train gradient:  0.12399701243018502
iteration : 12869
train acc:  0.7734375
train loss:  0.44018590450286865
train gradient:  0.1097002922522403
iteration : 12870
train acc:  0.8203125
train loss:  0.44860804080963135
train gradient:  0.09164846961835627
iteration : 12871
train acc:  0.8203125
train loss:  0.4291057884693146
train gradient:  0.10398311555977631
iteration : 12872
train acc:  0.7421875
train loss:  0.4563363194465637
train gradient:  0.09503218225499575
iteration : 12873
train acc:  0.78125
train loss:  0.4271124303340912
train gradient:  0.08385496469520037
iteration : 12874
train acc:  0.6796875
train loss:  0.5290011167526245
train gradient:  0.12409902326938262
iteration : 12875
train acc:  0.71875
train loss:  0.5356712341308594
train gradient:  0.14825971188707043
iteration : 12876
train acc:  0.765625
train loss:  0.462633341550827
train gradient:  0.10833055914199466
iteration : 12877
train acc:  0.7578125
train loss:  0.49190807342529297
train gradient:  0.10402535935369257
iteration : 12878
train acc:  0.75
train loss:  0.44932860136032104
train gradient:  0.09119161995510466
iteration : 12879
train acc:  0.75
train loss:  0.46533697843551636
train gradient:  0.09897751933047746
iteration : 12880
train acc:  0.765625
train loss:  0.4716135263442993
train gradient:  0.110695345397233
iteration : 12881
train acc:  0.7734375
train loss:  0.4726622700691223
train gradient:  0.12572392792503975
iteration : 12882
train acc:  0.6953125
train loss:  0.5494167804718018
train gradient:  0.1432644255249662
iteration : 12883
train acc:  0.7421875
train loss:  0.5122203826904297
train gradient:  0.14434142422358276
iteration : 12884
train acc:  0.734375
train loss:  0.5309760570526123
train gradient:  0.13873444378871308
iteration : 12885
train acc:  0.703125
train loss:  0.5443673133850098
train gradient:  0.12370564109244185
iteration : 12886
train acc:  0.78125
train loss:  0.4369783401489258
train gradient:  0.11514337148023623
iteration : 12887
train acc:  0.78125
train loss:  0.4741019010543823
train gradient:  0.09650297862548461
iteration : 12888
train acc:  0.765625
train loss:  0.4816802740097046
train gradient:  0.13804931074051618
iteration : 12889
train acc:  0.78125
train loss:  0.4621276259422302
train gradient:  0.10313714091094645
iteration : 12890
train acc:  0.7734375
train loss:  0.5041312575340271
train gradient:  0.1307596340042938
iteration : 12891
train acc:  0.765625
train loss:  0.48349303007125854
train gradient:  0.1398790661946389
iteration : 12892
train acc:  0.6171875
train loss:  0.6249134540557861
train gradient:  0.19699833787899673
iteration : 12893
train acc:  0.7734375
train loss:  0.48248887062072754
train gradient:  0.1055957548999971
iteration : 12894
train acc:  0.7109375
train loss:  0.5314379930496216
train gradient:  0.13712193064133776
iteration : 12895
train acc:  0.7421875
train loss:  0.45225298404693604
train gradient:  0.11474598611101602
iteration : 12896
train acc:  0.75
train loss:  0.47469764947891235
train gradient:  0.1230015856646551
iteration : 12897
train acc:  0.7734375
train loss:  0.43469756841659546
train gradient:  0.09046075403285699
iteration : 12898
train acc:  0.6953125
train loss:  0.5209665298461914
train gradient:  0.16577502178901213
iteration : 12899
train acc:  0.7578125
train loss:  0.48263928294181824
train gradient:  0.09711483045233228
iteration : 12900
train acc:  0.796875
train loss:  0.44676274061203003
train gradient:  0.12493913852392803
iteration : 12901
train acc:  0.7265625
train loss:  0.4793650507926941
train gradient:  0.1390804655068406
iteration : 12902
train acc:  0.734375
train loss:  0.45771780610084534
train gradient:  0.10424933491650648
iteration : 12903
train acc:  0.734375
train loss:  0.4658288359642029
train gradient:  0.12006827932636185
iteration : 12904
train acc:  0.796875
train loss:  0.48318707942962646
train gradient:  0.13266295993078736
iteration : 12905
train acc:  0.78125
train loss:  0.4375944435596466
train gradient:  0.09090311076319073
iteration : 12906
train acc:  0.7890625
train loss:  0.443909227848053
train gradient:  0.09209255693557758
iteration : 12907
train acc:  0.7265625
train loss:  0.4994036555290222
train gradient:  0.10953518967957675
iteration : 12908
train acc:  0.734375
train loss:  0.5056180953979492
train gradient:  0.14227264288296682
iteration : 12909
train acc:  0.7578125
train loss:  0.48647862672805786
train gradient:  0.13764943527635193
iteration : 12910
train acc:  0.8359375
train loss:  0.39918649196624756
train gradient:  0.08332371156295469
iteration : 12911
train acc:  0.7109375
train loss:  0.48683470487594604
train gradient:  0.10823659500401685
iteration : 12912
train acc:  0.7265625
train loss:  0.5028176307678223
train gradient:  0.10560819184024699
iteration : 12913
train acc:  0.734375
train loss:  0.5102177262306213
train gradient:  0.12438006027768601
iteration : 12914
train acc:  0.7578125
train loss:  0.4515947103500366
train gradient:  0.08318514826510254
iteration : 12915
train acc:  0.7421875
train loss:  0.506024956703186
train gradient:  0.1280007095518484
iteration : 12916
train acc:  0.71875
train loss:  0.5469560623168945
train gradient:  0.12929494002435213
iteration : 12917
train acc:  0.7421875
train loss:  0.4886063039302826
train gradient:  0.10584758632807102
iteration : 12918
train acc:  0.8046875
train loss:  0.4463964104652405
train gradient:  0.10871782984168113
iteration : 12919
train acc:  0.71875
train loss:  0.5201961398124695
train gradient:  0.14714852133982376
iteration : 12920
train acc:  0.7734375
train loss:  0.43533623218536377
train gradient:  0.09557390852370469
iteration : 12921
train acc:  0.7578125
train loss:  0.471265584230423
train gradient:  0.11298613636717905
iteration : 12922
train acc:  0.734375
train loss:  0.5203592777252197
train gradient:  0.14148942733073427
iteration : 12923
train acc:  0.7734375
train loss:  0.45731768012046814
train gradient:  0.1046299241045408
iteration : 12924
train acc:  0.7890625
train loss:  0.4674883186817169
train gradient:  0.1323350145903034
iteration : 12925
train acc:  0.7578125
train loss:  0.5057356357574463
train gradient:  0.11471719947925244
iteration : 12926
train acc:  0.828125
train loss:  0.38251733779907227
train gradient:  0.0749673441370158
iteration : 12927
train acc:  0.796875
train loss:  0.4508346915245056
train gradient:  0.10915968184830022
iteration : 12928
train acc:  0.7421875
train loss:  0.5428599715232849
train gradient:  0.12483121167248953
iteration : 12929
train acc:  0.7578125
train loss:  0.42774438858032227
train gradient:  0.08610041033686337
iteration : 12930
train acc:  0.8046875
train loss:  0.4295216202735901
train gradient:  0.10726940514222573
iteration : 12931
train acc:  0.703125
train loss:  0.4982159435749054
train gradient:  0.11296794569102099
iteration : 12932
train acc:  0.78125
train loss:  0.45094233751296997
train gradient:  0.11518469833171577
iteration : 12933
train acc:  0.8046875
train loss:  0.4337243437767029
train gradient:  0.08154189953780921
iteration : 12934
train acc:  0.7578125
train loss:  0.4767796993255615
train gradient:  0.1280721476619703
iteration : 12935
train acc:  0.7734375
train loss:  0.4917495846748352
train gradient:  0.13302303376593375
iteration : 12936
train acc:  0.6796875
train loss:  0.5270752310752869
train gradient:  0.12113520533620555
iteration : 12937
train acc:  0.6953125
train loss:  0.5949106812477112
train gradient:  0.19175145647561959
iteration : 12938
train acc:  0.78125
train loss:  0.44831782579421997
train gradient:  0.11081099532746824
iteration : 12939
train acc:  0.7265625
train loss:  0.4712345600128174
train gradient:  0.11185058196246747
iteration : 12940
train acc:  0.6875
train loss:  0.5429328083992004
train gradient:  0.1317790707625894
iteration : 12941
train acc:  0.734375
train loss:  0.5058874487876892
train gradient:  0.13518715519279217
iteration : 12942
train acc:  0.7734375
train loss:  0.47415196895599365
train gradient:  0.1187664451673541
iteration : 12943
train acc:  0.7109375
train loss:  0.5016133189201355
train gradient:  0.1010087365789086
iteration : 12944
train acc:  0.7265625
train loss:  0.5422918200492859
train gradient:  0.15205725551761784
iteration : 12945
train acc:  0.7421875
train loss:  0.46984902024269104
train gradient:  0.11029755997775811
iteration : 12946
train acc:  0.7734375
train loss:  0.5347901582717896
train gradient:  0.1326962547591797
iteration : 12947
train acc:  0.75
train loss:  0.48875507712364197
train gradient:  0.12777909896048834
iteration : 12948
train acc:  0.6953125
train loss:  0.5404631495475769
train gradient:  0.16287847932367944
iteration : 12949
train acc:  0.7421875
train loss:  0.5172821283340454
train gradient:  0.12779900964617985
iteration : 12950
train acc:  0.6796875
train loss:  0.59134441614151
train gradient:  0.20009024305478568
iteration : 12951
train acc:  0.8125
train loss:  0.4005293846130371
train gradient:  0.07589311830315514
iteration : 12952
train acc:  0.734375
train loss:  0.5461877584457397
train gradient:  0.17236741670040945
iteration : 12953
train acc:  0.71875
train loss:  0.5311417579650879
train gradient:  0.1299932894020957
iteration : 12954
train acc:  0.8046875
train loss:  0.44334226846694946
train gradient:  0.08739471647948224
iteration : 12955
train acc:  0.78125
train loss:  0.4988093078136444
train gradient:  0.1303545828679703
iteration : 12956
train acc:  0.734375
train loss:  0.5037420988082886
train gradient:  0.14932186096545466
iteration : 12957
train acc:  0.703125
train loss:  0.5530861616134644
train gradient:  0.13047505282926963
iteration : 12958
train acc:  0.71875
train loss:  0.5449143052101135
train gradient:  0.1592367872501671
iteration : 12959
train acc:  0.78125
train loss:  0.4773344397544861
train gradient:  0.12493937545084428
iteration : 12960
train acc:  0.78125
train loss:  0.45477789640426636
train gradient:  0.09870363485185005
iteration : 12961
train acc:  0.7421875
train loss:  0.5290588736534119
train gradient:  0.1727697283057633
iteration : 12962
train acc:  0.7109375
train loss:  0.514015793800354
train gradient:  0.13734229070334278
iteration : 12963
train acc:  0.8359375
train loss:  0.3607693314552307
train gradient:  0.0731006558435743
iteration : 12964
train acc:  0.6953125
train loss:  0.5817408561706543
train gradient:  0.1576321029717553
iteration : 12965
train acc:  0.8125
train loss:  0.45595768094062805
train gradient:  0.10541887418510643
iteration : 12966
train acc:  0.6953125
train loss:  0.4937784671783447
train gradient:  0.13046437279082346
iteration : 12967
train acc:  0.7578125
train loss:  0.4928009510040283
train gradient:  0.12197976834615464
iteration : 12968
train acc:  0.7578125
train loss:  0.43011006712913513
train gradient:  0.09609029000437327
iteration : 12969
train acc:  0.734375
train loss:  0.5159870386123657
train gradient:  0.11088100671452555
iteration : 12970
train acc:  0.8046875
train loss:  0.405383825302124
train gradient:  0.10480624845612427
iteration : 12971
train acc:  0.71875
train loss:  0.5808717012405396
train gradient:  0.14428510970400807
iteration : 12972
train acc:  0.7890625
train loss:  0.48290759325027466
train gradient:  0.0983339135108084
iteration : 12973
train acc:  0.7578125
train loss:  0.4767371714115143
train gradient:  0.11488469910854907
iteration : 12974
train acc:  0.703125
train loss:  0.5566204190254211
train gradient:  0.15684855599091535
iteration : 12975
train acc:  0.78125
train loss:  0.4477623403072357
train gradient:  0.09721617033696339
iteration : 12976
train acc:  0.7265625
train loss:  0.5240017771720886
train gradient:  0.14039049907977966
iteration : 12977
train acc:  0.6953125
train loss:  0.5581508278846741
train gradient:  0.1559471832179392
iteration : 12978
train acc:  0.7421875
train loss:  0.45369699597358704
train gradient:  0.10581515910328361
iteration : 12979
train acc:  0.7734375
train loss:  0.4510113298892975
train gradient:  0.12576210452823794
iteration : 12980
train acc:  0.7890625
train loss:  0.46038639545440674
train gradient:  0.10470828662732495
iteration : 12981
train acc:  0.7578125
train loss:  0.508467972278595
train gradient:  0.13151537710951702
iteration : 12982
train acc:  0.703125
train loss:  0.5200743675231934
train gradient:  0.13091532284041163
iteration : 12983
train acc:  0.796875
train loss:  0.40636974573135376
train gradient:  0.08587570749383332
iteration : 12984
train acc:  0.7734375
train loss:  0.4877331256866455
train gradient:  0.11374084038771153
iteration : 12985
train acc:  0.765625
train loss:  0.5251172780990601
train gradient:  0.1299232849309166
iteration : 12986
train acc:  0.7109375
train loss:  0.512876033782959
train gradient:  0.12632768302232822
iteration : 12987
train acc:  0.7734375
train loss:  0.5062831044197083
train gradient:  0.10671598445972301
iteration : 12988
train acc:  0.7265625
train loss:  0.4811711013317108
train gradient:  0.12057882484450429
iteration : 12989
train acc:  0.7265625
train loss:  0.48497021198272705
train gradient:  0.1193464006205999
iteration : 12990
train acc:  0.7265625
train loss:  0.49143528938293457
train gradient:  0.130991748924013
iteration : 12991
train acc:  0.71875
train loss:  0.4959871470928192
train gradient:  0.12355749243179709
iteration : 12992
train acc:  0.71875
train loss:  0.49782460927963257
train gradient:  0.10314846970770696
iteration : 12993
train acc:  0.7265625
train loss:  0.4771265983581543
train gradient:  0.11851968961691493
iteration : 12994
train acc:  0.703125
train loss:  0.49540799856185913
train gradient:  0.11984344581388527
iteration : 12995
train acc:  0.796875
train loss:  0.4113541543483734
train gradient:  0.10785029488485935
iteration : 12996
train acc:  0.7421875
train loss:  0.5079599618911743
train gradient:  0.1276684328169388
iteration : 12997
train acc:  0.765625
train loss:  0.45502161979675293
train gradient:  0.09939167753639508
iteration : 12998
train acc:  0.734375
train loss:  0.5086489915847778
train gradient:  0.11336850962346388
iteration : 12999
train acc:  0.6875
train loss:  0.5895337462425232
train gradient:  0.12388629226365842
iteration : 13000
train acc:  0.8046875
train loss:  0.43370720744132996
train gradient:  0.0895684394368498
iteration : 13001
train acc:  0.75
train loss:  0.4771392345428467
train gradient:  0.11097601389675975
iteration : 13002
train acc:  0.7109375
train loss:  0.5375335216522217
train gradient:  0.16195791830715983
iteration : 13003
train acc:  0.6796875
train loss:  0.5208102464675903
train gradient:  0.13515223006190302
iteration : 13004
train acc:  0.734375
train loss:  0.4765453338623047
train gradient:  0.10603515504326357
iteration : 13005
train acc:  0.71875
train loss:  0.48141032457351685
train gradient:  0.10981024397091838
iteration : 13006
train acc:  0.7109375
train loss:  0.5234814882278442
train gradient:  0.16015716010153652
iteration : 13007
train acc:  0.6875
train loss:  0.56474769115448
train gradient:  0.1450029173027929
iteration : 13008
train acc:  0.7421875
train loss:  0.48984408378601074
train gradient:  0.11945675867094978
iteration : 13009
train acc:  0.7734375
train loss:  0.4963756799697876
train gradient:  0.11527345468088138
iteration : 13010
train acc:  0.7890625
train loss:  0.422395795583725
train gradient:  0.09983762333013746
iteration : 13011
train acc:  0.7890625
train loss:  0.4330448508262634
train gradient:  0.08507089012841747
iteration : 13012
train acc:  0.8125
train loss:  0.4319010376930237
train gradient:  0.13090845204071072
iteration : 13013
train acc:  0.734375
train loss:  0.4819391667842865
train gradient:  0.09128181424994863
iteration : 13014
train acc:  0.65625
train loss:  0.5385314226150513
train gradient:  0.1995026784658637
iteration : 13015
train acc:  0.7890625
train loss:  0.475524365901947
train gradient:  0.1073331757001149
iteration : 13016
train acc:  0.7734375
train loss:  0.44550999999046326
train gradient:  0.09286403427862852
iteration : 13017
train acc:  0.7734375
train loss:  0.4732941687107086
train gradient:  0.1062277639261496
iteration : 13018
train acc:  0.734375
train loss:  0.516271710395813
train gradient:  0.127810458131852
iteration : 13019
train acc:  0.65625
train loss:  0.5844343900680542
train gradient:  0.1372079144986562
iteration : 13020
train acc:  0.78125
train loss:  0.4775219261646271
train gradient:  0.09536445698719143
iteration : 13021
train acc:  0.765625
train loss:  0.46811774373054504
train gradient:  0.11040548710619001
iteration : 13022
train acc:  0.7734375
train loss:  0.4766848087310791
train gradient:  0.13433490931545844
iteration : 13023
train acc:  0.6953125
train loss:  0.476550817489624
train gradient:  0.10057318194922575
iteration : 13024
train acc:  0.6875
train loss:  0.6150927543640137
train gradient:  0.17439320363849597
iteration : 13025
train acc:  0.765625
train loss:  0.5026679635047913
train gradient:  0.14778161046518595
iteration : 13026
train acc:  0.78125
train loss:  0.42421621084213257
train gradient:  0.09737989615680666
iteration : 13027
train acc:  0.6953125
train loss:  0.524665892124176
train gradient:  0.10971458687575271
iteration : 13028
train acc:  0.75
train loss:  0.49489331245422363
train gradient:  0.12146398428315786
iteration : 13029
train acc:  0.78125
train loss:  0.4862072467803955
train gradient:  0.1452574048477115
iteration : 13030
train acc:  0.7109375
train loss:  0.5443876385688782
train gradient:  0.11149506170135509
iteration : 13031
train acc:  0.7578125
train loss:  0.5002857446670532
train gradient:  0.13686578998821558
iteration : 13032
train acc:  0.734375
train loss:  0.4692956805229187
train gradient:  0.10063177581372214
iteration : 13033
train acc:  0.6640625
train loss:  0.5801500082015991
train gradient:  0.1487793102507988
iteration : 13034
train acc:  0.7578125
train loss:  0.4289158582687378
train gradient:  0.10052602435613593
iteration : 13035
train acc:  0.71875
train loss:  0.5012311339378357
train gradient:  0.12122600919883428
iteration : 13036
train acc:  0.734375
train loss:  0.5010414719581604
train gradient:  0.10788183962274914
iteration : 13037
train acc:  0.8125
train loss:  0.4244416058063507
train gradient:  0.08086019354149067
iteration : 13038
train acc:  0.7265625
train loss:  0.5048731565475464
train gradient:  0.11634577297525761
iteration : 13039
train acc:  0.7421875
train loss:  0.4784184694290161
train gradient:  0.13887471674975682
iteration : 13040
train acc:  0.6796875
train loss:  0.5271649360656738
train gradient:  0.13540341217547408
iteration : 13041
train acc:  0.7421875
train loss:  0.4570651650428772
train gradient:  0.08568151191783645
iteration : 13042
train acc:  0.7265625
train loss:  0.534130871295929
train gradient:  0.12397994920846192
iteration : 13043
train acc:  0.7265625
train loss:  0.5197893381118774
train gradient:  0.11380890295025732
iteration : 13044
train acc:  0.7578125
train loss:  0.5274715423583984
train gradient:  0.11547252105495223
iteration : 13045
train acc:  0.7421875
train loss:  0.5161224603652954
train gradient:  0.13639174864471132
iteration : 13046
train acc:  0.734375
train loss:  0.49893319606781006
train gradient:  0.13929361463126178
iteration : 13047
train acc:  0.6875
train loss:  0.5334063768386841
train gradient:  0.13504302165195875
iteration : 13048
train acc:  0.7578125
train loss:  0.4536575973033905
train gradient:  0.10038873255694443
iteration : 13049
train acc:  0.7578125
train loss:  0.45273759961128235
train gradient:  0.10523469222059173
iteration : 13050
train acc:  0.75
train loss:  0.5033011436462402
train gradient:  0.13306585165945217
iteration : 13051
train acc:  0.7265625
train loss:  0.4948357939720154
train gradient:  0.1272446679974129
iteration : 13052
train acc:  0.7578125
train loss:  0.5175658464431763
train gradient:  0.13925221725753023
iteration : 13053
train acc:  0.75
train loss:  0.4726792573928833
train gradient:  0.086032203735857
iteration : 13054
train acc:  0.796875
train loss:  0.4535207748413086
train gradient:  0.11619812401369094
iteration : 13055
train acc:  0.7734375
train loss:  0.4856448173522949
train gradient:  0.1084055131504576
iteration : 13056
train acc:  0.6953125
train loss:  0.5106829404830933
train gradient:  0.10027061825594961
iteration : 13057
train acc:  0.7578125
train loss:  0.5006850957870483
train gradient:  0.16029876647902513
iteration : 13058
train acc:  0.7890625
train loss:  0.4886043667793274
train gradient:  0.10899950692611168
iteration : 13059
train acc:  0.796875
train loss:  0.41693389415740967
train gradient:  0.10909591646215049
iteration : 13060
train acc:  0.6953125
train loss:  0.5238763093948364
train gradient:  0.11015742391516439
iteration : 13061
train acc:  0.78125
train loss:  0.4670795202255249
train gradient:  0.11685037374595748
iteration : 13062
train acc:  0.7265625
train loss:  0.5566459894180298
train gradient:  0.15870795550306835
iteration : 13063
train acc:  0.75
train loss:  0.5174434185028076
train gradient:  0.14280312733462533
iteration : 13064
train acc:  0.8125
train loss:  0.4627578854560852
train gradient:  0.11878995169807903
iteration : 13065
train acc:  0.6875
train loss:  0.5114953517913818
train gradient:  0.13982553465052755
iteration : 13066
train acc:  0.71875
train loss:  0.498126357793808
train gradient:  0.10807095681494348
iteration : 13067
train acc:  0.765625
train loss:  0.5092944502830505
train gradient:  0.13064326157243805
iteration : 13068
train acc:  0.734375
train loss:  0.5214436054229736
train gradient:  0.11507417686802363
iteration : 13069
train acc:  0.734375
train loss:  0.5299442410469055
train gradient:  0.12429746003803482
iteration : 13070
train acc:  0.7109375
train loss:  0.48150932788848877
train gradient:  0.09810273174191105
iteration : 13071
train acc:  0.7734375
train loss:  0.48353472352027893
train gradient:  0.12008597941778246
iteration : 13072
train acc:  0.7734375
train loss:  0.4424148499965668
train gradient:  0.09478999185074864
iteration : 13073
train acc:  0.8359375
train loss:  0.4079948365688324
train gradient:  0.0782273598199987
iteration : 13074
train acc:  0.828125
train loss:  0.4181348979473114
train gradient:  0.10180020943700119
iteration : 13075
train acc:  0.703125
train loss:  0.4905768930912018
train gradient:  0.13906535180515886
iteration : 13076
train acc:  0.75
train loss:  0.5034729242324829
train gradient:  0.1155243180991245
iteration : 13077
train acc:  0.7578125
train loss:  0.4732314348220825
train gradient:  0.11008101436174157
iteration : 13078
train acc:  0.765625
train loss:  0.42684847116470337
train gradient:  0.08325712261097068
iteration : 13079
train acc:  0.7109375
train loss:  0.5539878010749817
train gradient:  0.17977218039258339
iteration : 13080
train acc:  0.6953125
train loss:  0.5699640512466431
train gradient:  0.1258424244342129
iteration : 13081
train acc:  0.7890625
train loss:  0.44989851117134094
train gradient:  0.09719737360845357
iteration : 13082
train acc:  0.671875
train loss:  0.5816702842712402
train gradient:  0.19615900030600353
iteration : 13083
train acc:  0.71875
train loss:  0.5132795572280884
train gradient:  0.10265044869410579
iteration : 13084
train acc:  0.71875
train loss:  0.5047235488891602
train gradient:  0.130062808851226
iteration : 13085
train acc:  0.765625
train loss:  0.47197240591049194
train gradient:  0.15913413093600437
iteration : 13086
train acc:  0.8046875
train loss:  0.4701145887374878
train gradient:  0.10655438793276366
iteration : 13087
train acc:  0.7890625
train loss:  0.4495418071746826
train gradient:  0.10067097188306566
iteration : 13088
train acc:  0.7421875
train loss:  0.44774603843688965
train gradient:  0.0789917999024305
iteration : 13089
train acc:  0.765625
train loss:  0.45725882053375244
train gradient:  0.1256648839245021
iteration : 13090
train acc:  0.7890625
train loss:  0.4827480912208557
train gradient:  0.1281959839343827
iteration : 13091
train acc:  0.78125
train loss:  0.4405052661895752
train gradient:  0.10092418287232365
iteration : 13092
train acc:  0.65625
train loss:  0.5845476984977722
train gradient:  0.15265862822089865
iteration : 13093
train acc:  0.75
train loss:  0.4408877491950989
train gradient:  0.07565493234291626
iteration : 13094
train acc:  0.7265625
train loss:  0.49807608127593994
train gradient:  0.13364929000411468
iteration : 13095
train acc:  0.7734375
train loss:  0.4816988706588745
train gradient:  0.10601345177080974
iteration : 13096
train acc:  0.7578125
train loss:  0.4243282079696655
train gradient:  0.10050061394438875
iteration : 13097
train acc:  0.7734375
train loss:  0.40127357840538025
train gradient:  0.09441263698331054
iteration : 13098
train acc:  0.71875
train loss:  0.5213151574134827
train gradient:  0.1296970987399919
iteration : 13099
train acc:  0.765625
train loss:  0.4897492229938507
train gradient:  0.09359014629680111
iteration : 13100
train acc:  0.8125
train loss:  0.43247610330581665
train gradient:  0.10954271558130814
iteration : 13101
train acc:  0.7265625
train loss:  0.4899563193321228
train gradient:  0.13020917236866317
iteration : 13102
train acc:  0.7734375
train loss:  0.49981167912483215
train gradient:  0.1129910157056381
iteration : 13103
train acc:  0.7578125
train loss:  0.4493624269962311
train gradient:  0.09051212035399989
iteration : 13104
train acc:  0.7890625
train loss:  0.4530736804008484
train gradient:  0.09658282298828365
iteration : 13105
train acc:  0.8125
train loss:  0.3983662724494934
train gradient:  0.07949735174739969
iteration : 13106
train acc:  0.71875
train loss:  0.5375345945358276
train gradient:  0.13351286956466463
iteration : 13107
train acc:  0.7109375
train loss:  0.565606951713562
train gradient:  0.13529933168138092
iteration : 13108
train acc:  0.671875
train loss:  0.5563689470291138
train gradient:  0.17100629607872087
iteration : 13109
train acc:  0.734375
train loss:  0.48207852244377136
train gradient:  0.09116967481350553
iteration : 13110
train acc:  0.7890625
train loss:  0.4395650327205658
train gradient:  0.10629047057095059
iteration : 13111
train acc:  0.75
train loss:  0.4626690447330475
train gradient:  0.1086586131610909
iteration : 13112
train acc:  0.7265625
train loss:  0.5218977928161621
train gradient:  0.12745292605829645
iteration : 13113
train acc:  0.71875
train loss:  0.4925609529018402
train gradient:  0.13939108372198888
iteration : 13114
train acc:  0.7890625
train loss:  0.4604721665382385
train gradient:  0.09476743994922719
iteration : 13115
train acc:  0.7890625
train loss:  0.3929411768913269
train gradient:  0.07544908660562487
iteration : 13116
train acc:  0.703125
train loss:  0.5419895648956299
train gradient:  0.14988979939261954
iteration : 13117
train acc:  0.734375
train loss:  0.47965574264526367
train gradient:  0.11383789911779335
iteration : 13118
train acc:  0.8203125
train loss:  0.45582282543182373
train gradient:  0.09322355427990585
iteration : 13119
train acc:  0.71875
train loss:  0.5531209707260132
train gradient:  0.15286895060400735
iteration : 13120
train acc:  0.78125
train loss:  0.4325500428676605
train gradient:  0.09326205902564495
iteration : 13121
train acc:  0.6796875
train loss:  0.5551894903182983
train gradient:  0.12968682074636773
iteration : 13122
train acc:  0.71875
train loss:  0.46248477697372437
train gradient:  0.08215028030578835
iteration : 13123
train acc:  0.7890625
train loss:  0.4635545313358307
train gradient:  0.12672862459132797
iteration : 13124
train acc:  0.75
train loss:  0.4668397903442383
train gradient:  0.13803526844010827
iteration : 13125
train acc:  0.765625
train loss:  0.49661123752593994
train gradient:  0.09826364392389678
iteration : 13126
train acc:  0.765625
train loss:  0.4615558385848999
train gradient:  0.11402483336682107
iteration : 13127
train acc:  0.765625
train loss:  0.4619869291782379
train gradient:  0.09847619268540019
iteration : 13128
train acc:  0.7265625
train loss:  0.5358855128288269
train gradient:  0.1591817532135882
iteration : 13129
train acc:  0.75
train loss:  0.47851595282554626
train gradient:  0.11748682586890771
iteration : 13130
train acc:  0.734375
train loss:  0.49786972999572754
train gradient:  0.12420458337436958
iteration : 13131
train acc:  0.7578125
train loss:  0.46842798590660095
train gradient:  0.1083506355839492
iteration : 13132
train acc:  0.828125
train loss:  0.39495018124580383
train gradient:  0.09512604340260894
iteration : 13133
train acc:  0.7265625
train loss:  0.548581063747406
train gradient:  0.14386984142833445
iteration : 13134
train acc:  0.7578125
train loss:  0.46883127093315125
train gradient:  0.09822877121376233
iteration : 13135
train acc:  0.7265625
train loss:  0.48813292384147644
train gradient:  0.10677162925395835
iteration : 13136
train acc:  0.703125
train loss:  0.5360162854194641
train gradient:  0.1593971514720946
iteration : 13137
train acc:  0.6953125
train loss:  0.506482720375061
train gradient:  0.1206979369346409
iteration : 13138
train acc:  0.765625
train loss:  0.47152990102767944
train gradient:  0.10355232344031588
iteration : 13139
train acc:  0.7890625
train loss:  0.4367392957210541
train gradient:  0.10341087886356437
iteration : 13140
train acc:  0.7265625
train loss:  0.507380485534668
train gradient:  0.1179673022790825
iteration : 13141
train acc:  0.734375
train loss:  0.47811368107795715
train gradient:  0.08587803044147346
iteration : 13142
train acc:  0.796875
train loss:  0.4216651916503906
train gradient:  0.09634790707210907
iteration : 13143
train acc:  0.6796875
train loss:  0.5528237819671631
train gradient:  0.15802629681612934
iteration : 13144
train acc:  0.7734375
train loss:  0.4844702184200287
train gradient:  0.16058344060819418
iteration : 13145
train acc:  0.6796875
train loss:  0.5591495037078857
train gradient:  0.17078094829680862
iteration : 13146
train acc:  0.7734375
train loss:  0.4324137568473816
train gradient:  0.10372817918224286
iteration : 13147
train acc:  0.8203125
train loss:  0.3934895992279053
train gradient:  0.0930557551958344
iteration : 13148
train acc:  0.7421875
train loss:  0.5048298835754395
train gradient:  0.12301710862375603
iteration : 13149
train acc:  0.7109375
train loss:  0.5152192711830139
train gradient:  0.11776907841227961
iteration : 13150
train acc:  0.7421875
train loss:  0.4760423004627228
train gradient:  0.1406354989684968
iteration : 13151
train acc:  0.7734375
train loss:  0.4327331781387329
train gradient:  0.09256141805501968
iteration : 13152
train acc:  0.7578125
train loss:  0.5211093425750732
train gradient:  0.12156001644605703
iteration : 13153
train acc:  0.75
train loss:  0.48987334966659546
train gradient:  0.13242457183019182
iteration : 13154
train acc:  0.8203125
train loss:  0.4163272976875305
train gradient:  0.06321479867884199
iteration : 13155
train acc:  0.765625
train loss:  0.4571608901023865
train gradient:  0.10424447189815807
iteration : 13156
train acc:  0.7734375
train loss:  0.4618225693702698
train gradient:  0.10655802460548656
iteration : 13157
train acc:  0.765625
train loss:  0.4827421009540558
train gradient:  0.11393433627672685
iteration : 13158
train acc:  0.75
train loss:  0.4623957872390747
train gradient:  0.10401249633372416
iteration : 13159
train acc:  0.7421875
train loss:  0.5060765147209167
train gradient:  0.12094725933418493
iteration : 13160
train acc:  0.7890625
train loss:  0.46548402309417725
train gradient:  0.12064034410134912
iteration : 13161
train acc:  0.703125
train loss:  0.5390205383300781
train gradient:  0.15764891452131574
iteration : 13162
train acc:  0.78125
train loss:  0.469728946685791
train gradient:  0.09970600861281129
iteration : 13163
train acc:  0.8125
train loss:  0.40771448612213135
train gradient:  0.08701825193507448
iteration : 13164
train acc:  0.7578125
train loss:  0.4916419982910156
train gradient:  0.10776974059572982
iteration : 13165
train acc:  0.84375
train loss:  0.4134671688079834
train gradient:  0.10576085274043638
iteration : 13166
train acc:  0.7890625
train loss:  0.48651641607284546
train gradient:  0.11612695428938054
iteration : 13167
train acc:  0.796875
train loss:  0.4581495523452759
train gradient:  0.13202573916158522
iteration : 13168
train acc:  0.75
train loss:  0.5078709125518799
train gradient:  0.14358595409521335
iteration : 13169
train acc:  0.765625
train loss:  0.4341771602630615
train gradient:  0.09395281545402887
iteration : 13170
train acc:  0.7578125
train loss:  0.47529518604278564
train gradient:  0.09578642921203288
iteration : 13171
train acc:  0.75
train loss:  0.4981554448604584
train gradient:  0.14199103514221997
iteration : 13172
train acc:  0.8125
train loss:  0.41399428248405457
train gradient:  0.06709137125731235
iteration : 13173
train acc:  0.75
train loss:  0.4996268153190613
train gradient:  0.145786955765494
iteration : 13174
train acc:  0.6953125
train loss:  0.5119746327400208
train gradient:  0.12936093810774807
iteration : 13175
train acc:  0.75
train loss:  0.45283618569374084
train gradient:  0.07556992535575918
iteration : 13176
train acc:  0.765625
train loss:  0.47357141971588135
train gradient:  0.10864934142786925
iteration : 13177
train acc:  0.7265625
train loss:  0.47774767875671387
train gradient:  0.09274756414386712
iteration : 13178
train acc:  0.7109375
train loss:  0.5401625037193298
train gradient:  0.13804549310129663
iteration : 13179
train acc:  0.7734375
train loss:  0.46314531564712524
train gradient:  0.09580625671512769
iteration : 13180
train acc:  0.765625
train loss:  0.43098777532577515
train gradient:  0.09772601673609345
iteration : 13181
train acc:  0.7265625
train loss:  0.5237259864807129
train gradient:  0.13725098203880715
iteration : 13182
train acc:  0.7890625
train loss:  0.45820701122283936
train gradient:  0.08938156427654381
iteration : 13183
train acc:  0.734375
train loss:  0.527730405330658
train gradient:  0.11506992522651224
iteration : 13184
train acc:  0.7890625
train loss:  0.45784515142440796
train gradient:  0.10276470104408927
iteration : 13185
train acc:  0.7265625
train loss:  0.5222047567367554
train gradient:  0.11631469068504145
iteration : 13186
train acc:  0.75
train loss:  0.47423648834228516
train gradient:  0.10520636325568386
iteration : 13187
train acc:  0.78125
train loss:  0.4475701153278351
train gradient:  0.1215464046922061
iteration : 13188
train acc:  0.75
train loss:  0.4790528416633606
train gradient:  0.10579727694797474
iteration : 13189
train acc:  0.703125
train loss:  0.49588990211486816
train gradient:  0.1169537208708249
iteration : 13190
train acc:  0.7578125
train loss:  0.482613742351532
train gradient:  0.16122327772721323
iteration : 13191
train acc:  0.765625
train loss:  0.4353874623775482
train gradient:  0.11450041994846505
iteration : 13192
train acc:  0.71875
train loss:  0.5737756490707397
train gradient:  0.15252301973009563
iteration : 13193
train acc:  0.71875
train loss:  0.5065415501594543
train gradient:  0.1821047132273365
iteration : 13194
train acc:  0.7578125
train loss:  0.5126278400421143
train gradient:  0.13085568825536403
iteration : 13195
train acc:  0.703125
train loss:  0.5404959321022034
train gradient:  0.13209241250162063
iteration : 13196
train acc:  0.703125
train loss:  0.5238988995552063
train gradient:  0.13612945217453754
iteration : 13197
train acc:  0.734375
train loss:  0.5089088678359985
train gradient:  0.1292521730332411
iteration : 13198
train acc:  0.71875
train loss:  0.4950832426548004
train gradient:  0.1433621767807136
iteration : 13199
train acc:  0.6953125
train loss:  0.5374447107315063
train gradient:  0.13754241957132007
iteration : 13200
train acc:  0.7265625
train loss:  0.4513154625892639
train gradient:  0.09007611368463114
iteration : 13201
train acc:  0.734375
train loss:  0.5743132829666138
train gradient:  0.15232008781369855
iteration : 13202
train acc:  0.703125
train loss:  0.5668115615844727
train gradient:  0.18600899272064045
iteration : 13203
train acc:  0.671875
train loss:  0.5298434495925903
train gradient:  0.10964446254487424
iteration : 13204
train acc:  0.78125
train loss:  0.4424232244491577
train gradient:  0.10274456241418357
iteration : 13205
train acc:  0.6953125
train loss:  0.514700710773468
train gradient:  0.17827879281246617
iteration : 13206
train acc:  0.8125
train loss:  0.4481443762779236
train gradient:  0.08434258963010795
iteration : 13207
train acc:  0.7578125
train loss:  0.5080848932266235
train gradient:  0.16095588225455182
iteration : 13208
train acc:  0.7265625
train loss:  0.45844870805740356
train gradient:  0.09564566087666472
iteration : 13209
train acc:  0.703125
train loss:  0.4792013168334961
train gradient:  0.10280250487058797
iteration : 13210
train acc:  0.7578125
train loss:  0.4378955364227295
train gradient:  0.09389725728547992
iteration : 13211
train acc:  0.7421875
train loss:  0.4855632781982422
train gradient:  0.10403478629250672
iteration : 13212
train acc:  0.703125
train loss:  0.5262308120727539
train gradient:  0.12248515842866413
iteration : 13213
train acc:  0.7109375
train loss:  0.5429481267929077
train gradient:  0.14944951600448414
iteration : 13214
train acc:  0.7734375
train loss:  0.4518020451068878
train gradient:  0.11507813853454545
iteration : 13215
train acc:  0.703125
train loss:  0.5080065727233887
train gradient:  0.13434817893739076
iteration : 13216
train acc:  0.71875
train loss:  0.536827802658081
train gradient:  0.15862577222343954
iteration : 13217
train acc:  0.7578125
train loss:  0.4402517080307007
train gradient:  0.1032358222224782
iteration : 13218
train acc:  0.734375
train loss:  0.5197241306304932
train gradient:  0.14357652695364842
iteration : 13219
train acc:  0.7578125
train loss:  0.4630798101425171
train gradient:  0.11924224322557025
iteration : 13220
train acc:  0.7890625
train loss:  0.45198190212249756
train gradient:  0.10719401165061535
iteration : 13221
train acc:  0.71875
train loss:  0.480070561170578
train gradient:  0.14141044225910643
iteration : 13222
train acc:  0.7890625
train loss:  0.4236273169517517
train gradient:  0.09149986512559195
iteration : 13223
train acc:  0.7734375
train loss:  0.42390599846839905
train gradient:  0.09317479210500698
iteration : 13224
train acc:  0.71875
train loss:  0.47469204664230347
train gradient:  0.12442400333041534
iteration : 13225
train acc:  0.796875
train loss:  0.41043397784233093
train gradient:  0.11265801183445003
iteration : 13226
train acc:  0.7734375
train loss:  0.43805623054504395
train gradient:  0.09419697009202241
iteration : 13227
train acc:  0.75
train loss:  0.4815453588962555
train gradient:  0.10199020641647545
iteration : 13228
train acc:  0.765625
train loss:  0.4886270761489868
train gradient:  0.12982802279259623
iteration : 13229
train acc:  0.75
train loss:  0.4820344150066376
train gradient:  0.11412929920052478
iteration : 13230
train acc:  0.71875
train loss:  0.4908456802368164
train gradient:  0.12185302122255585
iteration : 13231
train acc:  0.765625
train loss:  0.506531834602356
train gradient:  0.11693064431358761
iteration : 13232
train acc:  0.796875
train loss:  0.434594988822937
train gradient:  0.1116840984593932
iteration : 13233
train acc:  0.703125
train loss:  0.5523167848587036
train gradient:  0.1326976154110821
iteration : 13234
train acc:  0.7578125
train loss:  0.4495668411254883
train gradient:  0.1134302811571104
iteration : 13235
train acc:  0.7421875
train loss:  0.44259750843048096
train gradient:  0.12047488141013607
iteration : 13236
train acc:  0.7265625
train loss:  0.5631117820739746
train gradient:  0.17336949152952164
iteration : 13237
train acc:  0.7578125
train loss:  0.4468975067138672
train gradient:  0.11350710631663384
iteration : 13238
train acc:  0.78125
train loss:  0.45149198174476624
train gradient:  0.08785255190310805
iteration : 13239
train acc:  0.7578125
train loss:  0.5289098024368286
train gradient:  0.09716379668352725
iteration : 13240
train acc:  0.71875
train loss:  0.5040972828865051
train gradient:  0.11343669972615518
iteration : 13241
train acc:  0.71875
train loss:  0.602502703666687
train gradient:  0.1379503412197488
iteration : 13242
train acc:  0.7890625
train loss:  0.4790472388267517
train gradient:  0.10660515873343113
iteration : 13243
train acc:  0.71875
train loss:  0.45128297805786133
train gradient:  0.11673268765778173
iteration : 13244
train acc:  0.7265625
train loss:  0.5339508652687073
train gradient:  0.12959977441945406
iteration : 13245
train acc:  0.8046875
train loss:  0.4165482223033905
train gradient:  0.0929067880205899
iteration : 13246
train acc:  0.8203125
train loss:  0.4902743697166443
train gradient:  0.14167208091192332
iteration : 13247
train acc:  0.78125
train loss:  0.42131054401397705
train gradient:  0.08596841865904754
iteration : 13248
train acc:  0.78125
train loss:  0.4732930660247803
train gradient:  0.10021092290478038
iteration : 13249
train acc:  0.75
train loss:  0.46572890877723694
train gradient:  0.1276052612808383
iteration : 13250
train acc:  0.78125
train loss:  0.45444053411483765
train gradient:  0.10894482677978534
iteration : 13251
train acc:  0.7109375
train loss:  0.5473731160163879
train gradient:  0.14857601978063845
iteration : 13252
train acc:  0.703125
train loss:  0.49943244457244873
train gradient:  0.09628275542275593
iteration : 13253
train acc:  0.6796875
train loss:  0.5568618178367615
train gradient:  0.125957360811026
iteration : 13254
train acc:  0.7578125
train loss:  0.47032466530799866
train gradient:  0.12239031194029884
iteration : 13255
train acc:  0.703125
train loss:  0.5515075922012329
train gradient:  0.13181981098675194
iteration : 13256
train acc:  0.8203125
train loss:  0.4178066849708557
train gradient:  0.10351797686201872
iteration : 13257
train acc:  0.7421875
train loss:  0.45517048239707947
train gradient:  0.09792125647257069
iteration : 13258
train acc:  0.84375
train loss:  0.38097527623176575
train gradient:  0.07726220694104435
iteration : 13259
train acc:  0.7265625
train loss:  0.44746509194374084
train gradient:  0.10623996500166906
iteration : 13260
train acc:  0.75
train loss:  0.4631790518760681
train gradient:  0.11293974202075399
iteration : 13261
train acc:  0.7578125
train loss:  0.4785505533218384
train gradient:  0.11944576361387085
iteration : 13262
train acc:  0.7578125
train loss:  0.49417534470558167
train gradient:  0.12314161948017588
iteration : 13263
train acc:  0.75
train loss:  0.4894910752773285
train gradient:  0.1038262135490923
iteration : 13264
train acc:  0.8046875
train loss:  0.4143218398094177
train gradient:  0.09282245517160444
iteration : 13265
train acc:  0.796875
train loss:  0.46701163053512573
train gradient:  0.14295040835692915
iteration : 13266
train acc:  0.7421875
train loss:  0.483695924282074
train gradient:  0.11810725610017107
iteration : 13267
train acc:  0.7734375
train loss:  0.5097343921661377
train gradient:  0.11683101159843122
iteration : 13268
train acc:  0.75
train loss:  0.4982627034187317
train gradient:  0.1349544588846393
iteration : 13269
train acc:  0.8046875
train loss:  0.4896020293235779
train gradient:  0.1429672304458293
iteration : 13270
train acc:  0.8359375
train loss:  0.43136724829673767
train gradient:  0.11046558474927112
iteration : 13271
train acc:  0.734375
train loss:  0.4639742374420166
train gradient:  0.13122999804369256
iteration : 13272
train acc:  0.7734375
train loss:  0.45014017820358276
train gradient:  0.08837694731751529
iteration : 13273
train acc:  0.828125
train loss:  0.3812394440174103
train gradient:  0.0774753261674975
iteration : 13274
train acc:  0.7109375
train loss:  0.5226763486862183
train gradient:  0.11847651726865892
iteration : 13275
train acc:  0.71875
train loss:  0.5228927731513977
train gradient:  0.11171786292757971
iteration : 13276
train acc:  0.7890625
train loss:  0.4427196979522705
train gradient:  0.11692140709190367
iteration : 13277
train acc:  0.7109375
train loss:  0.5348504185676575
train gradient:  0.1655693542930342
iteration : 13278
train acc:  0.7734375
train loss:  0.46544235944747925
train gradient:  0.09658390781843192
iteration : 13279
train acc:  0.65625
train loss:  0.6207329034805298
train gradient:  0.1813710770953733
iteration : 13280
train acc:  0.7578125
train loss:  0.49961787462234497
train gradient:  0.10856097268618765
iteration : 13281
train acc:  0.7734375
train loss:  0.4482148289680481
train gradient:  0.09478249016451129
iteration : 13282
train acc:  0.75
train loss:  0.4554787874221802
train gradient:  0.1000599911960037
iteration : 13283
train acc:  0.75
train loss:  0.5135159492492676
train gradient:  0.12771631291968977
iteration : 13284
train acc:  0.7890625
train loss:  0.45282942056655884
train gradient:  0.08447733833360098
iteration : 13285
train acc:  0.765625
train loss:  0.4666655659675598
train gradient:  0.10865477977874696
iteration : 13286
train acc:  0.7265625
train loss:  0.5247024893760681
train gradient:  0.11938600008060368
iteration : 13287
train acc:  0.75
train loss:  0.4737110733985901
train gradient:  0.1166205445292771
iteration : 13288
train acc:  0.71875
train loss:  0.5284403562545776
train gradient:  0.12312573448604189
iteration : 13289
train acc:  0.6953125
train loss:  0.5623309016227722
train gradient:  0.16311713425346214
iteration : 13290
train acc:  0.703125
train loss:  0.5384327173233032
train gradient:  0.12201453878170905
iteration : 13291
train acc:  0.7421875
train loss:  0.4941433370113373
train gradient:  0.11966447158468695
iteration : 13292
train acc:  0.8125
train loss:  0.4460561275482178
train gradient:  0.0852971424854326
iteration : 13293
train acc:  0.828125
train loss:  0.39170053601264954
train gradient:  0.08076850997758003
iteration : 13294
train acc:  0.7578125
train loss:  0.47600114345550537
train gradient:  0.09657030179327099
iteration : 13295
train acc:  0.7421875
train loss:  0.502554714679718
train gradient:  0.11946368910044139
iteration : 13296
train acc:  0.734375
train loss:  0.5283186435699463
train gradient:  0.1236420733278524
iteration : 13297
train acc:  0.7421875
train loss:  0.5082330703735352
train gradient:  0.11634451109857094
iteration : 13298
train acc:  0.7421875
train loss:  0.468329519033432
train gradient:  0.1360265730893121
iteration : 13299
train acc:  0.71875
train loss:  0.5708922147750854
train gradient:  0.1485191476950192
iteration : 13300
train acc:  0.6484375
train loss:  0.6080759763717651
train gradient:  0.2621950706139538
iteration : 13301
train acc:  0.7578125
train loss:  0.4881642460823059
train gradient:  0.13029324829570865
iteration : 13302
train acc:  0.71875
train loss:  0.5314856767654419
train gradient:  0.1153441673999694
iteration : 13303
train acc:  0.734375
train loss:  0.5501890182495117
train gradient:  0.1407394090950545
iteration : 13304
train acc:  0.7109375
train loss:  0.48273640871047974
train gradient:  0.12078345226590774
iteration : 13305
train acc:  0.703125
train loss:  0.5131357312202454
train gradient:  0.1304840463351446
iteration : 13306
train acc:  0.8125
train loss:  0.47046250104904175
train gradient:  0.1256875854309008
iteration : 13307
train acc:  0.7578125
train loss:  0.5068340301513672
train gradient:  0.12784040613784275
iteration : 13308
train acc:  0.765625
train loss:  0.468183308839798
train gradient:  0.11838545646812151
iteration : 13309
train acc:  0.7421875
train loss:  0.4989844858646393
train gradient:  0.10837967959799694
iteration : 13310
train acc:  0.734375
train loss:  0.5086843371391296
train gradient:  0.1467067082320873
iteration : 13311
train acc:  0.75
train loss:  0.5137616395950317
train gradient:  0.15280989397559175
iteration : 13312
train acc:  0.7578125
train loss:  0.4418576657772064
train gradient:  0.10082087907308916
iteration : 13313
train acc:  0.7734375
train loss:  0.4983827769756317
train gradient:  0.10934382365169172
iteration : 13314
train acc:  0.7734375
train loss:  0.526627242565155
train gradient:  0.14321609691025372
iteration : 13315
train acc:  0.7265625
train loss:  0.533694863319397
train gradient:  0.11213086310289175
iteration : 13316
train acc:  0.7265625
train loss:  0.5054749250411987
train gradient:  0.12570172590186723
iteration : 13317
train acc:  0.7421875
train loss:  0.45587605237960815
train gradient:  0.08668379859113909
iteration : 13318
train acc:  0.6875
train loss:  0.5318572521209717
train gradient:  0.1313243348702733
iteration : 13319
train acc:  0.7109375
train loss:  0.4559057056903839
train gradient:  0.09261697446348391
iteration : 13320
train acc:  0.75
train loss:  0.5145986676216125
train gradient:  0.13477850932092605
iteration : 13321
train acc:  0.734375
train loss:  0.4509070813655853
train gradient:  0.10355699285142612
iteration : 13322
train acc:  0.765625
train loss:  0.48870882391929626
train gradient:  0.1570481638979032
iteration : 13323
train acc:  0.671875
train loss:  0.558129072189331
train gradient:  0.13009563458674014
iteration : 13324
train acc:  0.7734375
train loss:  0.4509046673774719
train gradient:  0.10232729972737514
iteration : 13325
train acc:  0.7890625
train loss:  0.47750455141067505
train gradient:  0.10069809717421252
iteration : 13326
train acc:  0.8125
train loss:  0.4262796640396118
train gradient:  0.0928396266123024
iteration : 13327
train acc:  0.75
train loss:  0.465179443359375
train gradient:  0.11980731844285683
iteration : 13328
train acc:  0.71875
train loss:  0.5120628476142883
train gradient:  0.11589210063335258
iteration : 13329
train acc:  0.71875
train loss:  0.5183016061782837
train gradient:  0.11233602664019053
iteration : 13330
train acc:  0.796875
train loss:  0.4598914384841919
train gradient:  0.13783252028603907
iteration : 13331
train acc:  0.6796875
train loss:  0.5227709412574768
train gradient:  0.10542831325855016
iteration : 13332
train acc:  0.765625
train loss:  0.4476892650127411
train gradient:  0.10044109522466899
iteration : 13333
train acc:  0.75
train loss:  0.45296841859817505
train gradient:  0.12651271737983522
iteration : 13334
train acc:  0.7421875
train loss:  0.4822777509689331
train gradient:  0.11726317593167979
iteration : 13335
train acc:  0.796875
train loss:  0.40194493532180786
train gradient:  0.09485416563633647
iteration : 13336
train acc:  0.78125
train loss:  0.40522998571395874
train gradient:  0.08351180398183741
iteration : 13337
train acc:  0.796875
train loss:  0.4655378758907318
train gradient:  0.1254867610661547
iteration : 13338
train acc:  0.796875
train loss:  0.43727993965148926
train gradient:  0.10171543312635796
iteration : 13339
train acc:  0.78125
train loss:  0.4924442172050476
train gradient:  0.11871698453360494
iteration : 13340
train acc:  0.75
train loss:  0.48803722858428955
train gradient:  0.10658693175052489
iteration : 13341
train acc:  0.734375
train loss:  0.48234474658966064
train gradient:  0.0997273488458459
iteration : 13342
train acc:  0.6953125
train loss:  0.5310705900192261
train gradient:  0.122243124198935
iteration : 13343
train acc:  0.7265625
train loss:  0.5223426818847656
train gradient:  0.1619372855598595
iteration : 13344
train acc:  0.78125
train loss:  0.436204195022583
train gradient:  0.09932785427457637
iteration : 13345
train acc:  0.734375
train loss:  0.482012540102005
train gradient:  0.11614446627390583
iteration : 13346
train acc:  0.734375
train loss:  0.49284812808036804
train gradient:  0.11005228802007523
iteration : 13347
train acc:  0.6953125
train loss:  0.5829170942306519
train gradient:  0.1785942036792445
iteration : 13348
train acc:  0.6796875
train loss:  0.5107982754707336
train gradient:  0.10040772085015487
iteration : 13349
train acc:  0.8203125
train loss:  0.44275420904159546
train gradient:  0.1092759246521207
iteration : 13350
train acc:  0.75
train loss:  0.4476510286331177
train gradient:  0.09307666542110078
iteration : 13351
train acc:  0.796875
train loss:  0.4535592794418335
train gradient:  0.09270634346936771
iteration : 13352
train acc:  0.7109375
train loss:  0.48013293743133545
train gradient:  0.09226693759841997
iteration : 13353
train acc:  0.765625
train loss:  0.4838681221008301
train gradient:  0.11747551208029922
iteration : 13354
train acc:  0.6875
train loss:  0.4855231046676636
train gradient:  0.09458887897456038
iteration : 13355
train acc:  0.765625
train loss:  0.4536237418651581
train gradient:  0.11200310761468805
iteration : 13356
train acc:  0.7109375
train loss:  0.5348379611968994
train gradient:  0.13808265970904082
iteration : 13357
train acc:  0.75
train loss:  0.4585292935371399
train gradient:  0.093790923307773
iteration : 13358
train acc:  0.734375
train loss:  0.4674980640411377
train gradient:  0.1242025196985644
iteration : 13359
train acc:  0.7890625
train loss:  0.4311104714870453
train gradient:  0.08400840554017656
iteration : 13360
train acc:  0.734375
train loss:  0.4785401225090027
train gradient:  0.13635084287058052
iteration : 13361
train acc:  0.765625
train loss:  0.4881519079208374
train gradient:  0.14850670114163678
iteration : 13362
train acc:  0.84375
train loss:  0.38720986247062683
train gradient:  0.0768609955470113
iteration : 13363
train acc:  0.75
train loss:  0.45862525701522827
train gradient:  0.12163325138527932
iteration : 13364
train acc:  0.7421875
train loss:  0.47230350971221924
train gradient:  0.1397636812280446
iteration : 13365
train acc:  0.71875
train loss:  0.4771105647087097
train gradient:  0.10448577375592971
iteration : 13366
train acc:  0.8359375
train loss:  0.41740015149116516
train gradient:  0.09608162211019548
iteration : 13367
train acc:  0.765625
train loss:  0.49743902683258057
train gradient:  0.12378562828109131
iteration : 13368
train acc:  0.7734375
train loss:  0.4265731871128082
train gradient:  0.0920263165689803
iteration : 13369
train acc:  0.703125
train loss:  0.5272854566574097
train gradient:  0.15601999052251103
iteration : 13370
train acc:  0.8359375
train loss:  0.3969089090824127
train gradient:  0.08199222812099986
iteration : 13371
train acc:  0.7734375
train loss:  0.47665178775787354
train gradient:  0.09947792430563553
iteration : 13372
train acc:  0.734375
train loss:  0.4651358425617218
train gradient:  0.11929869585383117
iteration : 13373
train acc:  0.75
train loss:  0.46986669301986694
train gradient:  0.10171462834497587
iteration : 13374
train acc:  0.6640625
train loss:  0.5732784271240234
train gradient:  0.17978077184016286
iteration : 13375
train acc:  0.6484375
train loss:  0.5404804944992065
train gradient:  0.1393480966395941
iteration : 13376
train acc:  0.75
train loss:  0.48563307523727417
train gradient:  0.1373007543627358
iteration : 13377
train acc:  0.703125
train loss:  0.4672855734825134
train gradient:  0.11162221907914374
iteration : 13378
train acc:  0.7578125
train loss:  0.48173466324806213
train gradient:  0.11294749370172541
iteration : 13379
train acc:  0.828125
train loss:  0.4491806924343109
train gradient:  0.09408472055506178
iteration : 13380
train acc:  0.7421875
train loss:  0.4860740900039673
train gradient:  0.09883188034087985
iteration : 13381
train acc:  0.7109375
train loss:  0.5277288556098938
train gradient:  0.14532044316428105
iteration : 13382
train acc:  0.6640625
train loss:  0.5545492768287659
train gradient:  0.1842156749714762
iteration : 13383
train acc:  0.7578125
train loss:  0.496194064617157
train gradient:  0.12059418934503839
iteration : 13384
train acc:  0.6875
train loss:  0.554262638092041
train gradient:  0.15579626072293418
iteration : 13385
train acc:  0.7265625
train loss:  0.5074381232261658
train gradient:  0.13094423944424477
iteration : 13386
train acc:  0.75
train loss:  0.5221715569496155
train gradient:  0.11497629339953766
iteration : 13387
train acc:  0.671875
train loss:  0.5997582674026489
train gradient:  0.2093270903301378
iteration : 13388
train acc:  0.6953125
train loss:  0.5052115321159363
train gradient:  0.15565905190121992
iteration : 13389
train acc:  0.78125
train loss:  0.4316903352737427
train gradient:  0.10283298983674771
iteration : 13390
train acc:  0.78125
train loss:  0.4971953332424164
train gradient:  0.13457669518861748
iteration : 13391
train acc:  0.734375
train loss:  0.47673681378364563
train gradient:  0.09641414776940485
iteration : 13392
train acc:  0.71875
train loss:  0.510140597820282
train gradient:  0.1440139028852941
iteration : 13393
train acc:  0.71875
train loss:  0.48537302017211914
train gradient:  0.11976058034373063
iteration : 13394
train acc:  0.71875
train loss:  0.5616785287857056
train gradient:  0.1810150779065796
iteration : 13395
train acc:  0.75
train loss:  0.46952056884765625
train gradient:  0.10433001136507494
iteration : 13396
train acc:  0.7890625
train loss:  0.43996262550354004
train gradient:  0.09921327333431056
iteration : 13397
train acc:  0.765625
train loss:  0.4495427906513214
train gradient:  0.09827571424879908
iteration : 13398
train acc:  0.7734375
train loss:  0.5021005868911743
train gradient:  0.10901240435280805
iteration : 13399
train acc:  0.7421875
train loss:  0.5540080070495605
train gradient:  0.14362052221515725
iteration : 13400
train acc:  0.796875
train loss:  0.4287126064300537
train gradient:  0.09480057118307661
iteration : 13401
train acc:  0.7109375
train loss:  0.5187729597091675
train gradient:  0.11969464908995715
iteration : 13402
train acc:  0.75
train loss:  0.4932742416858673
train gradient:  0.1211495345648756
iteration : 13403
train acc:  0.75
train loss:  0.4714213013648987
train gradient:  0.14089710770811195
iteration : 13404
train acc:  0.7734375
train loss:  0.4998318552970886
train gradient:  0.12472250378649546
iteration : 13405
train acc:  0.7421875
train loss:  0.5143965482711792
train gradient:  0.15370852816401936
iteration : 13406
train acc:  0.7578125
train loss:  0.4433531165122986
train gradient:  0.1061003610656614
iteration : 13407
train acc:  0.796875
train loss:  0.4294857978820801
train gradient:  0.0902043520412117
iteration : 13408
train acc:  0.7734375
train loss:  0.5228314399719238
train gradient:  0.13990263913930792
iteration : 13409
train acc:  0.7734375
train loss:  0.41866177320480347
train gradient:  0.09421469936504155
iteration : 13410
train acc:  0.8125
train loss:  0.40630489587783813
train gradient:  0.07430583024764854
iteration : 13411
train acc:  0.7890625
train loss:  0.45804330706596375
train gradient:  0.10163114054619422
iteration : 13412
train acc:  0.7890625
train loss:  0.45814821124076843
train gradient:  0.0914988250849
iteration : 13413
train acc:  0.765625
train loss:  0.5152055621147156
train gradient:  0.11230205985225715
iteration : 13414
train acc:  0.75
train loss:  0.5107995271682739
train gradient:  0.12861107010363737
iteration : 13415
train acc:  0.7109375
train loss:  0.4922220706939697
train gradient:  0.11409042754669474
iteration : 13416
train acc:  0.78125
train loss:  0.4846973121166229
train gradient:  0.12413145134447837
iteration : 13417
train acc:  0.71875
train loss:  0.5063334703445435
train gradient:  0.14005668750014003
iteration : 13418
train acc:  0.734375
train loss:  0.5388000011444092
train gradient:  0.1408080981357877
iteration : 13419
train acc:  0.765625
train loss:  0.47971272468566895
train gradient:  0.11315084689807398
iteration : 13420
train acc:  0.7734375
train loss:  0.5777292251586914
train gradient:  0.21445839351953105
iteration : 13421
train acc:  0.7734375
train loss:  0.43939051032066345
train gradient:  0.11202375327077954
iteration : 13422
train acc:  0.765625
train loss:  0.425679475069046
train gradient:  0.09032434416918736
iteration : 13423
train acc:  0.78125
train loss:  0.4359648823738098
train gradient:  0.09956333990578362
iteration : 13424
train acc:  0.765625
train loss:  0.4670887887477875
train gradient:  0.11008767270999002
iteration : 13425
train acc:  0.6796875
train loss:  0.5226317644119263
train gradient:  0.13104270495841894
iteration : 13426
train acc:  0.7421875
train loss:  0.4732983708381653
train gradient:  0.10527588122982247
iteration : 13427
train acc:  0.765625
train loss:  0.4297201633453369
train gradient:  0.08388284062894953
iteration : 13428
train acc:  0.78125
train loss:  0.5079017877578735
train gradient:  0.13831187884237467
iteration : 13429
train acc:  0.7734375
train loss:  0.4453510642051697
train gradient:  0.10713280060083281
iteration : 13430
train acc:  0.6953125
train loss:  0.5942513942718506
train gradient:  0.16269244623603518
iteration : 13431
train acc:  0.765625
train loss:  0.4739138185977936
train gradient:  0.10851433743254724
iteration : 13432
train acc:  0.734375
train loss:  0.45200619101524353
train gradient:  0.10176011900246956
iteration : 13433
train acc:  0.75
train loss:  0.44787928462028503
train gradient:  0.10778445812805358
iteration : 13434
train acc:  0.734375
train loss:  0.5080925226211548
train gradient:  0.09493468405546893
iteration : 13435
train acc:  0.71875
train loss:  0.4527769088745117
train gradient:  0.12652442237260558
iteration : 13436
train acc:  0.671875
train loss:  0.5473847389221191
train gradient:  0.13971473899989528
iteration : 13437
train acc:  0.71875
train loss:  0.49912354350090027
train gradient:  0.12812529117413932
iteration : 13438
train acc:  0.7421875
train loss:  0.49137169122695923
train gradient:  0.09582837950883019
iteration : 13439
train acc:  0.78125
train loss:  0.46997132897377014
train gradient:  0.11868426966996434
iteration : 13440
train acc:  0.734375
train loss:  0.4675399959087372
train gradient:  0.11843559973025682
iteration : 13441
train acc:  0.7734375
train loss:  0.5244671106338501
train gradient:  0.10822578038029076
iteration : 13442
train acc:  0.671875
train loss:  0.5609241724014282
train gradient:  0.18816703963894604
iteration : 13443
train acc:  0.765625
train loss:  0.4717579483985901
train gradient:  0.10170653661704383
iteration : 13444
train acc:  0.7890625
train loss:  0.4559260606765747
train gradient:  0.09029572346024134
iteration : 13445
train acc:  0.703125
train loss:  0.48687759041786194
train gradient:  0.13537952977057965
iteration : 13446
train acc:  0.8203125
train loss:  0.44222164154052734
train gradient:  0.09502225722949081
iteration : 13447
train acc:  0.7265625
train loss:  0.5793136358261108
train gradient:  0.1376822994911902
iteration : 13448
train acc:  0.7578125
train loss:  0.5086956024169922
train gradient:  0.11803968461472317
iteration : 13449
train acc:  0.6875
train loss:  0.5226002335548401
train gradient:  0.13909092153131414
iteration : 13450
train acc:  0.7421875
train loss:  0.4950263500213623
train gradient:  0.1098567188159761
iteration : 13451
train acc:  0.7578125
train loss:  0.4843037724494934
train gradient:  0.11107295959264125
iteration : 13452
train acc:  0.7734375
train loss:  0.46636122465133667
train gradient:  0.08651516827948288
iteration : 13453
train acc:  0.8125
train loss:  0.4280928671360016
train gradient:  0.08632359116445763
iteration : 13454
train acc:  0.7578125
train loss:  0.4982059597969055
train gradient:  0.12345772616161807
iteration : 13455
train acc:  0.71875
train loss:  0.5234248638153076
train gradient:  0.12027781295985529
iteration : 13456
train acc:  0.7265625
train loss:  0.5110782980918884
train gradient:  0.13276717953681139
iteration : 13457
train acc:  0.78125
train loss:  0.46124979853630066
train gradient:  0.09261099808569728
iteration : 13458
train acc:  0.7109375
train loss:  0.4932594299316406
train gradient:  0.12676911112581413
iteration : 13459
train acc:  0.703125
train loss:  0.5460653305053711
train gradient:  0.12793049327200612
iteration : 13460
train acc:  0.765625
train loss:  0.4697017967700958
train gradient:  0.1360086299240799
iteration : 13461
train acc:  0.734375
train loss:  0.5073490738868713
train gradient:  0.11462388762328615
iteration : 13462
train acc:  0.8359375
train loss:  0.39053016901016235
train gradient:  0.09053230934916988
iteration : 13463
train acc:  0.7578125
train loss:  0.4787118434906006
train gradient:  0.10182444578704358
iteration : 13464
train acc:  0.734375
train loss:  0.4928523302078247
train gradient:  0.13385680570711156
iteration : 13465
train acc:  0.71875
train loss:  0.49702078104019165
train gradient:  0.1201443433581249
iteration : 13466
train acc:  0.7890625
train loss:  0.4867219626903534
train gradient:  0.11272179333967744
iteration : 13467
train acc:  0.7578125
train loss:  0.4976673424243927
train gradient:  0.16085217460617818
iteration : 13468
train acc:  0.7578125
train loss:  0.4688408374786377
train gradient:  0.08984576818202245
iteration : 13469
train acc:  0.7421875
train loss:  0.5176147818565369
train gradient:  0.13274564169051023
iteration : 13470
train acc:  0.75
train loss:  0.4517503082752228
train gradient:  0.0834153189425434
iteration : 13471
train acc:  0.734375
train loss:  0.47054582834243774
train gradient:  0.11533805024703811
iteration : 13472
train acc:  0.78125
train loss:  0.44766977429389954
train gradient:  0.08893748038835776
iteration : 13473
train acc:  0.7890625
train loss:  0.4272095561027527
train gradient:  0.08808023344159749
iteration : 13474
train acc:  0.7734375
train loss:  0.4547252953052521
train gradient:  0.0851237579261791
iteration : 13475
train acc:  0.7890625
train loss:  0.47354820370674133
train gradient:  0.09397984123514183
iteration : 13476
train acc:  0.75
train loss:  0.46390625834465027
train gradient:  0.09970908275319329
iteration : 13477
train acc:  0.6953125
train loss:  0.5000879168510437
train gradient:  0.11319497954495822
iteration : 13478
train acc:  0.7578125
train loss:  0.49811649322509766
train gradient:  0.1324596805099309
iteration : 13479
train acc:  0.7109375
train loss:  0.5036584734916687
train gradient:  0.10982661510813677
iteration : 13480
train acc:  0.7890625
train loss:  0.42174023389816284
train gradient:  0.07445954946933632
iteration : 13481
train acc:  0.6796875
train loss:  0.5180845260620117
train gradient:  0.16853741569207636
iteration : 13482
train acc:  0.765625
train loss:  0.4653797149658203
train gradient:  0.11944941442578538
iteration : 13483
train acc:  0.7265625
train loss:  0.4871571362018585
train gradient:  0.12905345404875634
iteration : 13484
train acc:  0.7109375
train loss:  0.5097461938858032
train gradient:  0.11902561885589165
iteration : 13485
train acc:  0.7421875
train loss:  0.4610421657562256
train gradient:  0.09202236692830101
iteration : 13486
train acc:  0.7734375
train loss:  0.4493865668773651
train gradient:  0.08481257681092452
iteration : 13487
train acc:  0.796875
train loss:  0.4352089464664459
train gradient:  0.0908558786841319
iteration : 13488
train acc:  0.7421875
train loss:  0.5384987592697144
train gradient:  0.13353502881931112
iteration : 13489
train acc:  0.734375
train loss:  0.5076980590820312
train gradient:  0.10297461172849703
iteration : 13490
train acc:  0.6875
train loss:  0.5028381943702698
train gradient:  0.11457985065213917
iteration : 13491
train acc:  0.75
train loss:  0.4857654571533203
train gradient:  0.11223900694435032
iteration : 13492
train acc:  0.8203125
train loss:  0.4030825197696686
train gradient:  0.0756355211800764
iteration : 13493
train acc:  0.7578125
train loss:  0.4479084014892578
train gradient:  0.11376095672155072
iteration : 13494
train acc:  0.703125
train loss:  0.5017151236534119
train gradient:  0.1228017894702017
iteration : 13495
train acc:  0.796875
train loss:  0.46047818660736084
train gradient:  0.11964045375425901
iteration : 13496
train acc:  0.75
train loss:  0.46664369106292725
train gradient:  0.1290325156647017
iteration : 13497
train acc:  0.828125
train loss:  0.4200754165649414
train gradient:  0.10919485291696761
iteration : 13498
train acc:  0.75
train loss:  0.4910251498222351
train gradient:  0.10707828009994187
iteration : 13499
train acc:  0.7890625
train loss:  0.43088245391845703
train gradient:  0.10599462198497203
iteration : 13500
train acc:  0.7421875
train loss:  0.45935478806495667
train gradient:  0.10918047633774093
iteration : 13501
train acc:  0.71875
train loss:  0.522962749004364
train gradient:  0.1425091711521666
iteration : 13502
train acc:  0.6640625
train loss:  0.5502626299858093
train gradient:  0.16648527414667869
iteration : 13503
train acc:  0.7578125
train loss:  0.4769747853279114
train gradient:  0.17722520946507514
iteration : 13504
train acc:  0.765625
train loss:  0.46999791264533997
train gradient:  0.1282499354150669
iteration : 13505
train acc:  0.7265625
train loss:  0.4744521975517273
train gradient:  0.10019836194327084
iteration : 13506
train acc:  0.765625
train loss:  0.5107206702232361
train gradient:  0.14423786617155782
iteration : 13507
train acc:  0.75
train loss:  0.4559532105922699
train gradient:  0.12356970501056062
iteration : 13508
train acc:  0.734375
train loss:  0.4987833797931671
train gradient:  0.14403032985689102
iteration : 13509
train acc:  0.75
train loss:  0.5562529563903809
train gradient:  0.15734354611779772
iteration : 13510
train acc:  0.71875
train loss:  0.4879646599292755
train gradient:  0.1512187586110546
iteration : 13511
train acc:  0.8046875
train loss:  0.444463312625885
train gradient:  0.09804175907111332
iteration : 13512
train acc:  0.7109375
train loss:  0.5228177905082703
train gradient:  0.16636263208836133
iteration : 13513
train acc:  0.7578125
train loss:  0.45420998334884644
train gradient:  0.09854111239483554
iteration : 13514
train acc:  0.734375
train loss:  0.4515899419784546
train gradient:  0.11598012359376338
iteration : 13515
train acc:  0.7421875
train loss:  0.4926958382129669
train gradient:  0.10445410276934075
iteration : 13516
train acc:  0.8046875
train loss:  0.40977323055267334
train gradient:  0.09578741318150112
iteration : 13517
train acc:  0.7265625
train loss:  0.48156100511550903
train gradient:  0.11881055395332357
iteration : 13518
train acc:  0.78125
train loss:  0.45318764448165894
train gradient:  0.14094604428139912
iteration : 13519
train acc:  0.734375
train loss:  0.4580186605453491
train gradient:  0.126685777114208
iteration : 13520
train acc:  0.890625
train loss:  0.36851638555526733
train gradient:  0.09316515250236168
iteration : 13521
train acc:  0.7265625
train loss:  0.5260736346244812
train gradient:  0.14766595110933953
iteration : 13522
train acc:  0.7265625
train loss:  0.48012566566467285
train gradient:  0.12396096067339413
iteration : 13523
train acc:  0.6875
train loss:  0.5595464706420898
train gradient:  0.14050660936011547
iteration : 13524
train acc:  0.7265625
train loss:  0.4968074560165405
train gradient:  0.13273252399259106
iteration : 13525
train acc:  0.8046875
train loss:  0.4881074130535126
train gradient:  0.1056551374738691
iteration : 13526
train acc:  0.765625
train loss:  0.44868218898773193
train gradient:  0.12655240927231373
iteration : 13527
train acc:  0.75
train loss:  0.48635029792785645
train gradient:  0.12505428782799846
iteration : 13528
train acc:  0.796875
train loss:  0.4058857858181
train gradient:  0.09797619025487614
iteration : 13529
train acc:  0.7109375
train loss:  0.5780666470527649
train gradient:  0.1589041457677559
iteration : 13530
train acc:  0.7734375
train loss:  0.4587933421134949
train gradient:  0.10030431635286471
iteration : 13531
train acc:  0.65625
train loss:  0.566849946975708
train gradient:  0.16591729227079044
iteration : 13532
train acc:  0.734375
train loss:  0.47616371512413025
train gradient:  0.1279277231313924
iteration : 13533
train acc:  0.75
train loss:  0.48902884125709534
train gradient:  0.11003142749759068
iteration : 13534
train acc:  0.8125
train loss:  0.44520607590675354
train gradient:  0.1396941774332477
iteration : 13535
train acc:  0.828125
train loss:  0.41793787479400635
train gradient:  0.08376867301647184
iteration : 13536
train acc:  0.7578125
train loss:  0.5131993293762207
train gradient:  0.1236320396549219
iteration : 13537
train acc:  0.75
train loss:  0.4929499328136444
train gradient:  0.1226667847778141
iteration : 13538
train acc:  0.8046875
train loss:  0.5001998543739319
train gradient:  0.12180889949025328
iteration : 13539
train acc:  0.75
train loss:  0.49960821866989136
train gradient:  0.13016709370044655
iteration : 13540
train acc:  0.796875
train loss:  0.4530675709247589
train gradient:  0.08731744436754947
iteration : 13541
train acc:  0.7421875
train loss:  0.5253865718841553
train gradient:  0.11283064802906231
iteration : 13542
train acc:  0.734375
train loss:  0.4882102310657501
train gradient:  0.14537957653754294
iteration : 13543
train acc:  0.78125
train loss:  0.421791136264801
train gradient:  0.09847778994415293
iteration : 13544
train acc:  0.765625
train loss:  0.48970645666122437
train gradient:  0.12895308934806848
iteration : 13545
train acc:  0.71875
train loss:  0.4723243713378906
train gradient:  0.11012360128659732
iteration : 13546
train acc:  0.796875
train loss:  0.42660170793533325
train gradient:  0.08233833678807877
iteration : 13547
train acc:  0.765625
train loss:  0.43840980529785156
train gradient:  0.10874201370941995
iteration : 13548
train acc:  0.7578125
train loss:  0.4814103841781616
train gradient:  0.12999794284756255
iteration : 13549
train acc:  0.78125
train loss:  0.4333384931087494
train gradient:  0.09926933629210563
iteration : 13550
train acc:  0.765625
train loss:  0.4923003315925598
train gradient:  0.11891004741853427
iteration : 13551
train acc:  0.75
train loss:  0.4890716075897217
train gradient:  0.120654421922066
iteration : 13552
train acc:  0.7421875
train loss:  0.527145504951477
train gradient:  0.13199312201858043
iteration : 13553
train acc:  0.703125
train loss:  0.5162079334259033
train gradient:  0.12998627683985525
iteration : 13554
train acc:  0.71875
train loss:  0.47672587633132935
train gradient:  0.12448349983157128
iteration : 13555
train acc:  0.7734375
train loss:  0.4572350084781647
train gradient:  0.13070975877392232
iteration : 13556
train acc:  0.6953125
train loss:  0.5120416879653931
train gradient:  0.15631389345005461
iteration : 13557
train acc:  0.8046875
train loss:  0.4658536911010742
train gradient:  0.10755924223394207
iteration : 13558
train acc:  0.7421875
train loss:  0.48274779319763184
train gradient:  0.12670406206996684
iteration : 13559
train acc:  0.8046875
train loss:  0.4170752167701721
train gradient:  0.08181765993686202
iteration : 13560
train acc:  0.734375
train loss:  0.4610136151313782
train gradient:  0.11013042144425124
iteration : 13561
train acc:  0.75
train loss:  0.45477432012557983
train gradient:  0.121106609445954
iteration : 13562
train acc:  0.7265625
train loss:  0.5115435123443604
train gradient:  0.12948371952571558
iteration : 13563
train acc:  0.78125
train loss:  0.46076083183288574
train gradient:  0.11204976856624596
iteration : 13564
train acc:  0.796875
train loss:  0.4332994818687439
train gradient:  0.09889735319836135
iteration : 13565
train acc:  0.796875
train loss:  0.4271942675113678
train gradient:  0.11580034757525634
iteration : 13566
train acc:  0.734375
train loss:  0.583444356918335
train gradient:  0.18846080985330735
iteration : 13567
train acc:  0.7734375
train loss:  0.43043145537376404
train gradient:  0.11657706004031199
iteration : 13568
train acc:  0.765625
train loss:  0.4739619195461273
train gradient:  0.12905650893193205
iteration : 13569
train acc:  0.7578125
train loss:  0.5201135873794556
train gradient:  0.12034668824334602
iteration : 13570
train acc:  0.71875
train loss:  0.5135945081710815
train gradient:  0.12320052898363272
iteration : 13571
train acc:  0.6953125
train loss:  0.48602646589279175
train gradient:  0.12270254914334915
iteration : 13572
train acc:  0.765625
train loss:  0.4729921221733093
train gradient:  0.1371878482070903
iteration : 13573
train acc:  0.75
train loss:  0.533297061920166
train gradient:  0.13302600034860657
iteration : 13574
train acc:  0.7421875
train loss:  0.4633980989456177
train gradient:  0.11831536637091442
iteration : 13575
train acc:  0.75
train loss:  0.4891265034675598
train gradient:  0.09533996888226198
iteration : 13576
train acc:  0.765625
train loss:  0.4513322710990906
train gradient:  0.10135544852116692
iteration : 13577
train acc:  0.703125
train loss:  0.5529627203941345
train gradient:  0.12600623843436573
iteration : 13578
train acc:  0.7734375
train loss:  0.4720951020717621
train gradient:  0.1084509496539906
iteration : 13579
train acc:  0.71875
train loss:  0.48363396525382996
train gradient:  0.1284808245983936
iteration : 13580
train acc:  0.8046875
train loss:  0.41983988881111145
train gradient:  0.0974752232069422
iteration : 13581
train acc:  0.75
train loss:  0.4907200336456299
train gradient:  0.1260818400478279
iteration : 13582
train acc:  0.7578125
train loss:  0.5332133769989014
train gradient:  0.1456082362660674
iteration : 13583
train acc:  0.6875
train loss:  0.5284678936004639
train gradient:  0.13964850287448533
iteration : 13584
train acc:  0.7578125
train loss:  0.481423556804657
train gradient:  0.1018253376693841
iteration : 13585
train acc:  0.7578125
train loss:  0.48128998279571533
train gradient:  0.12210270659373501
iteration : 13586
train acc:  0.7734375
train loss:  0.4390753209590912
train gradient:  0.08872949203789672
iteration : 13587
train acc:  0.75
train loss:  0.49363550543785095
train gradient:  0.11710378662011386
iteration : 13588
train acc:  0.78125
train loss:  0.4269372820854187
train gradient:  0.14044996561483244
iteration : 13589
train acc:  0.703125
train loss:  0.5586028099060059
train gradient:  0.1588357051719454
iteration : 13590
train acc:  0.8203125
train loss:  0.3868929147720337
train gradient:  0.08918648273866508
iteration : 13591
train acc:  0.75
train loss:  0.4870678782463074
train gradient:  0.1120874541705573
iteration : 13592
train acc:  0.703125
train loss:  0.5422685742378235
train gradient:  0.17256075902461537
iteration : 13593
train acc:  0.8046875
train loss:  0.44296401739120483
train gradient:  0.11762130811803974
iteration : 13594
train acc:  0.7890625
train loss:  0.47462135553359985
train gradient:  0.11701653338892863
iteration : 13595
train acc:  0.7578125
train loss:  0.4779042899608612
train gradient:  0.11690501148212493
iteration : 13596
train acc:  0.78125
train loss:  0.4440962076187134
train gradient:  0.10884001927733784
iteration : 13597
train acc:  0.6796875
train loss:  0.50379478931427
train gradient:  0.1125544945954281
iteration : 13598
train acc:  0.7578125
train loss:  0.49972885847091675
train gradient:  0.1321619504725972
iteration : 13599
train acc:  0.7421875
train loss:  0.4986550509929657
train gradient:  0.11856492421726017
iteration : 13600
train acc:  0.75
train loss:  0.5003251433372498
train gradient:  0.11326310413068623
iteration : 13601
train acc:  0.7421875
train loss:  0.5273399949073792
train gradient:  0.12394092151157023
iteration : 13602
train acc:  0.7421875
train loss:  0.4888935685157776
train gradient:  0.13836591427033937
iteration : 13603
train acc:  0.7109375
train loss:  0.5105370879173279
train gradient:  0.11108717632786744
iteration : 13604
train acc:  0.7890625
train loss:  0.47797030210494995
train gradient:  0.1057213568900538
iteration : 13605
train acc:  0.8046875
train loss:  0.42076730728149414
train gradient:  0.09037983969683236
iteration : 13606
train acc:  0.8125
train loss:  0.4191688299179077
train gradient:  0.08961110460956308
iteration : 13607
train acc:  0.7734375
train loss:  0.45767852663993835
train gradient:  0.10836094301242488
iteration : 13608
train acc:  0.75
train loss:  0.5159469842910767
train gradient:  0.13388097733667473
iteration : 13609
train acc:  0.7109375
train loss:  0.5262458920478821
train gradient:  0.12694510408291965
iteration : 13610
train acc:  0.6875
train loss:  0.5461618900299072
train gradient:  0.15310824380188426
iteration : 13611
train acc:  0.7109375
train loss:  0.5364673137664795
train gradient:  0.17872939468132604
iteration : 13612
train acc:  0.65625
train loss:  0.5462880730628967
train gradient:  0.15612288422489873
iteration : 13613
train acc:  0.7421875
train loss:  0.458538293838501
train gradient:  0.11948484500383921
iteration : 13614
train acc:  0.6640625
train loss:  0.5241086483001709
train gradient:  0.13555927759273412
iteration : 13615
train acc:  0.796875
train loss:  0.43863460421562195
train gradient:  0.11854310714563032
iteration : 13616
train acc:  0.734375
train loss:  0.5194958448410034
train gradient:  0.15037007899745122
iteration : 13617
train acc:  0.84375
train loss:  0.40998005867004395
train gradient:  0.09267647482289723
iteration : 13618
train acc:  0.7265625
train loss:  0.4877433776855469
train gradient:  0.13832639388347528
iteration : 13619
train acc:  0.796875
train loss:  0.43387526273727417
train gradient:  0.10937202150281711
iteration : 13620
train acc:  0.703125
train loss:  0.5622597932815552
train gradient:  0.14144399844084543
iteration : 13621
train acc:  0.703125
train loss:  0.46840986609458923
train gradient:  0.0988560858563179
iteration : 13622
train acc:  0.828125
train loss:  0.42486757040023804
train gradient:  0.08660497417580226
iteration : 13623
train acc:  0.765625
train loss:  0.4856134355068207
train gradient:  0.11597932200627419
iteration : 13624
train acc:  0.734375
train loss:  0.4545612931251526
train gradient:  0.09867593372111227
iteration : 13625
train acc:  0.828125
train loss:  0.4151882827281952
train gradient:  0.08871614124531495
iteration : 13626
train acc:  0.8515625
train loss:  0.4188140630722046
train gradient:  0.11929960880934438
iteration : 13627
train acc:  0.8203125
train loss:  0.4459071755409241
train gradient:  0.13021773263791817
iteration : 13628
train acc:  0.734375
train loss:  0.5103986859321594
train gradient:  0.151884055311569
iteration : 13629
train acc:  0.7421875
train loss:  0.49586978554725647
train gradient:  0.1244455837800159
iteration : 13630
train acc:  0.7265625
train loss:  0.5154204368591309
train gradient:  0.11157912688967171
iteration : 13631
train acc:  0.7421875
train loss:  0.47828352451324463
train gradient:  0.11665610981751005
iteration : 13632
train acc:  0.7109375
train loss:  0.5707002878189087
train gradient:  0.15970770246771615
iteration : 13633
train acc:  0.8046875
train loss:  0.4568427503108978
train gradient:  0.0886755196825706
iteration : 13634
train acc:  0.7109375
train loss:  0.5239164233207703
train gradient:  0.12684464380034965
iteration : 13635
train acc:  0.7890625
train loss:  0.4389364719390869
train gradient:  0.09332474720967832
iteration : 13636
train acc:  0.7578125
train loss:  0.4880707859992981
train gradient:  0.11538451562073844
iteration : 13637
train acc:  0.75
train loss:  0.4489314556121826
train gradient:  0.09962423091219555
iteration : 13638
train acc:  0.7265625
train loss:  0.5468313694000244
train gradient:  0.1257002997503029
iteration : 13639
train acc:  0.75
train loss:  0.5027244091033936
train gradient:  0.14189542684374168
iteration : 13640
train acc:  0.734375
train loss:  0.5027341842651367
train gradient:  0.11282096656868755
iteration : 13641
train acc:  0.71875
train loss:  0.5192722082138062
train gradient:  0.12008132644593901
iteration : 13642
train acc:  0.78125
train loss:  0.4300423264503479
train gradient:  0.101596898909511
iteration : 13643
train acc:  0.7890625
train loss:  0.4844721555709839
train gradient:  0.0996828199966662
iteration : 13644
train acc:  0.7890625
train loss:  0.4841424226760864
train gradient:  0.13474465045626816
iteration : 13645
train acc:  0.640625
train loss:  0.5695946216583252
train gradient:  0.17436092345699583
iteration : 13646
train acc:  0.7578125
train loss:  0.4904910922050476
train gradient:  0.12132273620175339
iteration : 13647
train acc:  0.8046875
train loss:  0.4701653718948364
train gradient:  0.11090089424872808
iteration : 13648
train acc:  0.7890625
train loss:  0.449729323387146
train gradient:  0.09382911019009539
iteration : 13649
train acc:  0.7421875
train loss:  0.45834818482398987
train gradient:  0.08522517617680421
iteration : 13650
train acc:  0.7578125
train loss:  0.45382699370384216
train gradient:  0.11019293008913343
iteration : 13651
train acc:  0.7890625
train loss:  0.44809573888778687
train gradient:  0.09739670408913892
iteration : 13652
train acc:  0.7109375
train loss:  0.5330547094345093
train gradient:  0.1297092654985992
iteration : 13653
train acc:  0.7421875
train loss:  0.48809927701950073
train gradient:  0.10278211892383331
iteration : 13654
train acc:  0.71875
train loss:  0.5909321308135986
train gradient:  0.16013915783665827
iteration : 13655
train acc:  0.7265625
train loss:  0.5581387281417847
train gradient:  0.13927261799812346
iteration : 13656
train acc:  0.7265625
train loss:  0.503287136554718
train gradient:  0.12777261552224267
iteration : 13657
train acc:  0.78125
train loss:  0.4366064965724945
train gradient:  0.13020973523115612
iteration : 13658
train acc:  0.78125
train loss:  0.45557263493537903
train gradient:  0.12782899874373613
iteration : 13659
train acc:  0.7109375
train loss:  0.5270413756370544
train gradient:  0.12590673990734896
iteration : 13660
train acc:  0.671875
train loss:  0.606084942817688
train gradient:  0.16672923217051344
iteration : 13661
train acc:  0.7578125
train loss:  0.4491901397705078
train gradient:  0.09090376901967512
iteration : 13662
train acc:  0.734375
train loss:  0.5201141238212585
train gradient:  0.1127086172016471
iteration : 13663
train acc:  0.703125
train loss:  0.5546187162399292
train gradient:  0.15144945868049203
iteration : 13664
train acc:  0.71875
train loss:  0.48216450214385986
train gradient:  0.09812861304709347
iteration : 13665
train acc:  0.7265625
train loss:  0.4979626536369324
train gradient:  0.1135723183427068
iteration : 13666
train acc:  0.65625
train loss:  0.5814483165740967
train gradient:  0.16365299388428228
iteration : 13667
train acc:  0.7890625
train loss:  0.48249977827072144
train gradient:  0.11837679810838284
iteration : 13668
train acc:  0.765625
train loss:  0.46548154950141907
train gradient:  0.10065628661710929
iteration : 13669
train acc:  0.703125
train loss:  0.5080001950263977
train gradient:  0.10757002075373515
iteration : 13670
train acc:  0.8125
train loss:  0.43836188316345215
train gradient:  0.09908856231390828
iteration : 13671
train acc:  0.75
train loss:  0.47222715616226196
train gradient:  0.10545223021249779
iteration : 13672
train acc:  0.734375
train loss:  0.5230774879455566
train gradient:  0.12716820614595808
iteration : 13673
train acc:  0.7890625
train loss:  0.42049217224121094
train gradient:  0.08949804680119591
iteration : 13674
train acc:  0.7265625
train loss:  0.5460094213485718
train gradient:  0.1325543067696438
iteration : 13675
train acc:  0.7578125
train loss:  0.48492226004600525
train gradient:  0.12442739323802157
iteration : 13676
train acc:  0.8125
train loss:  0.4400622844696045
train gradient:  0.1235890591246495
iteration : 13677
train acc:  0.7265625
train loss:  0.5099669694900513
train gradient:  0.12299867920765434
iteration : 13678
train acc:  0.8046875
train loss:  0.4193401336669922
train gradient:  0.12590371532894148
iteration : 13679
train acc:  0.7734375
train loss:  0.42184147238731384
train gradient:  0.1157691964564459
iteration : 13680
train acc:  0.765625
train loss:  0.47661852836608887
train gradient:  0.12295652995491375
iteration : 13681
train acc:  0.734375
train loss:  0.5129544734954834
train gradient:  0.13774429655284093
iteration : 13682
train acc:  0.7109375
train loss:  0.5134401917457581
train gradient:  0.14379322059594946
iteration : 13683
train acc:  0.6796875
train loss:  0.5449650287628174
train gradient:  0.1371348607827379
iteration : 13684
train acc:  0.8046875
train loss:  0.4232267737388611
train gradient:  0.06842890079131955
iteration : 13685
train acc:  0.7421875
train loss:  0.4489535093307495
train gradient:  0.09952389138880781
iteration : 13686
train acc:  0.8125
train loss:  0.38207244873046875
train gradient:  0.07163585526926931
iteration : 13687
train acc:  0.7890625
train loss:  0.4883352220058441
train gradient:  0.1117701716100487
iteration : 13688
train acc:  0.7734375
train loss:  0.47283464670181274
train gradient:  0.10477883695284124
iteration : 13689
train acc:  0.8046875
train loss:  0.4218757152557373
train gradient:  0.08103346387494885
iteration : 13690
train acc:  0.734375
train loss:  0.4748680591583252
train gradient:  0.1700627930057534
iteration : 13691
train acc:  0.7421875
train loss:  0.49231594800949097
train gradient:  0.1169263786116712
iteration : 13692
train acc:  0.703125
train loss:  0.5541016459465027
train gradient:  0.1709600431093998
iteration : 13693
train acc:  0.734375
train loss:  0.4893524944782257
train gradient:  0.13399708214080874
iteration : 13694
train acc:  0.7734375
train loss:  0.4660073518753052
train gradient:  0.09413322687986246
iteration : 13695
train acc:  0.6875
train loss:  0.5515755414962769
train gradient:  0.2380009227706326
iteration : 13696
train acc:  0.7265625
train loss:  0.5045058727264404
train gradient:  0.12220823547349181
iteration : 13697
train acc:  0.7734375
train loss:  0.46507513523101807
train gradient:  0.09767091272255567
iteration : 13698
train acc:  0.6953125
train loss:  0.50502610206604
train gradient:  0.13181350006882173
iteration : 13699
train acc:  0.71875
train loss:  0.5876672267913818
train gradient:  0.2099132962680555
iteration : 13700
train acc:  0.8046875
train loss:  0.4729456305503845
train gradient:  0.09807999789126726
iteration : 13701
train acc:  0.7734375
train loss:  0.5483300089836121
train gradient:  0.14355907446393856
iteration : 13702
train acc:  0.6796875
train loss:  0.5141173601150513
train gradient:  0.12253351258529296
iteration : 13703
train acc:  0.75
train loss:  0.4771864414215088
train gradient:  0.09394680137389642
iteration : 13704
train acc:  0.7578125
train loss:  0.47969290614128113
train gradient:  0.1199500073214835
iteration : 13705
train acc:  0.765625
train loss:  0.47043538093566895
train gradient:  0.12732648464749022
iteration : 13706
train acc:  0.6953125
train loss:  0.5188308358192444
train gradient:  0.13866244603214783
iteration : 13707
train acc:  0.734375
train loss:  0.5666042566299438
train gradient:  0.13078489689206363
iteration : 13708
train acc:  0.71875
train loss:  0.5272895097732544
train gradient:  0.15246615865585744
iteration : 13709
train acc:  0.75
train loss:  0.5105737447738647
train gradient:  0.1317893179093675
iteration : 13710
train acc:  0.7734375
train loss:  0.45808953046798706
train gradient:  0.09199584660614142
iteration : 13711
train acc:  0.8125
train loss:  0.4296872019767761
train gradient:  0.08168855070566296
iteration : 13712
train acc:  0.7734375
train loss:  0.49123549461364746
train gradient:  0.1293358597097803
iteration : 13713
train acc:  0.7890625
train loss:  0.4808282256126404
train gradient:  0.09736599897333903
iteration : 13714
train acc:  0.765625
train loss:  0.47313597798347473
train gradient:  0.12977269969822505
iteration : 13715
train acc:  0.6953125
train loss:  0.4826171100139618
train gradient:  0.11444851211780503
iteration : 13716
train acc:  0.65625
train loss:  0.5358002185821533
train gradient:  0.148952691699911
iteration : 13717
train acc:  0.7421875
train loss:  0.4082481265068054
train gradient:  0.07910020989730367
iteration : 13718
train acc:  0.765625
train loss:  0.47702115774154663
train gradient:  0.09156459959552143
iteration : 13719
train acc:  0.7890625
train loss:  0.4335377514362335
train gradient:  0.09387354935185321
iteration : 13720
train acc:  0.7265625
train loss:  0.46976280212402344
train gradient:  0.10869031916424857
iteration : 13721
train acc:  0.75
train loss:  0.42474550008773804
train gradient:  0.1049540326488119
iteration : 13722
train acc:  0.765625
train loss:  0.47763317823410034
train gradient:  0.10002511874460535
iteration : 13723
train acc:  0.765625
train loss:  0.5293625593185425
train gradient:  0.15005012790706745
iteration : 13724
train acc:  0.78125
train loss:  0.4252331852912903
train gradient:  0.10698666340427121
iteration : 13725
train acc:  0.75
train loss:  0.539722204208374
train gradient:  0.16719268583961827
iteration : 13726
train acc:  0.7109375
train loss:  0.5021107196807861
train gradient:  0.11271334908455732
iteration : 13727
train acc:  0.75
train loss:  0.5125477910041809
train gradient:  0.12403355115762635
iteration : 13728
train acc:  0.6953125
train loss:  0.5487534403800964
train gradient:  0.1585472834600066
iteration : 13729
train acc:  0.75
train loss:  0.5335218906402588
train gradient:  0.11700523894840907
iteration : 13730
train acc:  0.7734375
train loss:  0.45937567949295044
train gradient:  0.09748866717082208
iteration : 13731
train acc:  0.6796875
train loss:  0.48627784848213196
train gradient:  0.12152637552527992
iteration : 13732
train acc:  0.6875
train loss:  0.5446938276290894
train gradient:  0.1237104162038258
iteration : 13733
train acc:  0.734375
train loss:  0.4768190383911133
train gradient:  0.10405402639424394
iteration : 13734
train acc:  0.7890625
train loss:  0.4621129035949707
train gradient:  0.12412629619684794
iteration : 13735
train acc:  0.75
train loss:  0.501846194267273
train gradient:  0.12412451452850932
iteration : 13736
train acc:  0.765625
train loss:  0.4251062870025635
train gradient:  0.0891875183905076
iteration : 13737
train acc:  0.734375
train loss:  0.43134161829948425
train gradient:  0.09608638444675903
iteration : 13738
train acc:  0.6875
train loss:  0.5327893495559692
train gradient:  0.1581015613190776
iteration : 13739
train acc:  0.796875
train loss:  0.423206627368927
train gradient:  0.07522997384194564
iteration : 13740
train acc:  0.828125
train loss:  0.44946491718292236
train gradient:  0.13943671995370743
iteration : 13741
train acc:  0.7578125
train loss:  0.43766045570373535
train gradient:  0.09065842644723822
iteration : 13742
train acc:  0.796875
train loss:  0.4048337936401367
train gradient:  0.08103811315591064
iteration : 13743
train acc:  0.765625
train loss:  0.4832839369773865
train gradient:  0.13622495104682697
iteration : 13744
train acc:  0.7890625
train loss:  0.48642420768737793
train gradient:  0.13485057533985814
iteration : 13745
train acc:  0.7734375
train loss:  0.4540509581565857
train gradient:  0.12220731823394214
iteration : 13746
train acc:  0.71875
train loss:  0.509678840637207
train gradient:  0.13776361461756476
iteration : 13747
train acc:  0.7578125
train loss:  0.48457351326942444
train gradient:  0.12092048958923428
iteration : 13748
train acc:  0.84375
train loss:  0.422649621963501
train gradient:  0.10734649375880115
iteration : 13749
train acc:  0.703125
train loss:  0.5126614570617676
train gradient:  0.11763440937861477
iteration : 13750
train acc:  0.8046875
train loss:  0.43209511041641235
train gradient:  0.08732890349170483
iteration : 13751
train acc:  0.6640625
train loss:  0.5577569603919983
train gradient:  0.13952226271325754
iteration : 13752
train acc:  0.7421875
train loss:  0.49855613708496094
train gradient:  0.1390952385406571
iteration : 13753
train acc:  0.703125
train loss:  0.4856417179107666
train gradient:  0.10128947934244349
iteration : 13754
train acc:  0.7421875
train loss:  0.5339093208312988
train gradient:  0.15911455504326327
iteration : 13755
train acc:  0.7109375
train loss:  0.5428958535194397
train gradient:  0.17642354461041784
iteration : 13756
train acc:  0.703125
train loss:  0.5483419895172119
train gradient:  0.16634007258432673
iteration : 13757
train acc:  0.6875
train loss:  0.6112223267555237
train gradient:  0.16728482685103355
iteration : 13758
train acc:  0.7421875
train loss:  0.45993512868881226
train gradient:  0.09838376030890739
iteration : 13759
train acc:  0.78125
train loss:  0.46911656856536865
train gradient:  0.11153533003843065
iteration : 13760
train acc:  0.765625
train loss:  0.496142715215683
train gradient:  0.12054816744273876
iteration : 13761
train acc:  0.7578125
train loss:  0.48221510648727417
train gradient:  0.12187874150406924
iteration : 13762
train acc:  0.75
train loss:  0.4871242344379425
train gradient:  0.10656907244640978
iteration : 13763
train acc:  0.6875
train loss:  0.5699295997619629
train gradient:  0.17492367014937432
iteration : 13764
train acc:  0.78125
train loss:  0.5097098350524902
train gradient:  0.11320160331780861
iteration : 13765
train acc:  0.703125
train loss:  0.49028730392456055
train gradient:  0.11220391639258119
iteration : 13766
train acc:  0.75
train loss:  0.48177075386047363
train gradient:  0.12479962625425471
iteration : 13767
train acc:  0.75
train loss:  0.42452067136764526
train gradient:  0.10693908603340774
iteration : 13768
train acc:  0.7734375
train loss:  0.41003984212875366
train gradient:  0.09244797402733744
iteration : 13769
train acc:  0.734375
train loss:  0.5084147453308105
train gradient:  0.13053801277802637
iteration : 13770
train acc:  0.7109375
train loss:  0.5709474086761475
train gradient:  0.17091676270353656
iteration : 13771
train acc:  0.734375
train loss:  0.49041515588760376
train gradient:  0.15623227569591225
iteration : 13772
train acc:  0.7421875
train loss:  0.49801966547966003
train gradient:  0.12067007656711895
iteration : 13773
train acc:  0.7265625
train loss:  0.5099952220916748
train gradient:  0.13708947823232562
iteration : 13774
train acc:  0.7578125
train loss:  0.5114731788635254
train gradient:  0.13254156071470868
iteration : 13775
train acc:  0.78125
train loss:  0.4754835367202759
train gradient:  0.1099985723350814
iteration : 13776
train acc:  0.7421875
train loss:  0.476871132850647
train gradient:  0.11671131878776077
iteration : 13777
train acc:  0.7109375
train loss:  0.6089242696762085
train gradient:  0.16098385900804352
iteration : 13778
train acc:  0.7890625
train loss:  0.42958009243011475
train gradient:  0.11784571229073891
iteration : 13779
train acc:  0.7734375
train loss:  0.4595192074775696
train gradient:  0.11334734560601754
iteration : 13780
train acc:  0.765625
train loss:  0.45843398571014404
train gradient:  0.09421603584033542
iteration : 13781
train acc:  0.6796875
train loss:  0.5408509969711304
train gradient:  0.1188382417581404
iteration : 13782
train acc:  0.7890625
train loss:  0.4744376540184021
train gradient:  0.10255464713567897
iteration : 13783
train acc:  0.734375
train loss:  0.5623549222946167
train gradient:  0.12930291687734324
iteration : 13784
train acc:  0.7734375
train loss:  0.47409066557884216
train gradient:  0.07778012527837604
iteration : 13785
train acc:  0.8203125
train loss:  0.409138560295105
train gradient:  0.08973339091440229
iteration : 13786
train acc:  0.734375
train loss:  0.5012997984886169
train gradient:  0.1337091766633044
iteration : 13787
train acc:  0.7578125
train loss:  0.43241697549819946
train gradient:  0.0868552732445621
iteration : 13788
train acc:  0.7578125
train loss:  0.45117300748825073
train gradient:  0.10470596527584124
iteration : 13789
train acc:  0.734375
train loss:  0.5383340120315552
train gradient:  0.12558624451919342
iteration : 13790
train acc:  0.8046875
train loss:  0.47247210144996643
train gradient:  0.10309569396218346
iteration : 13791
train acc:  0.7109375
train loss:  0.5014586448669434
train gradient:  0.10573518386338593
iteration : 13792
train acc:  0.765625
train loss:  0.4610496759414673
train gradient:  0.12268263226196728
iteration : 13793
train acc:  0.7265625
train loss:  0.5229991674423218
train gradient:  0.1466181868447134
iteration : 13794
train acc:  0.765625
train loss:  0.4779684245586395
train gradient:  0.1175486314589609
iteration : 13795
train acc:  0.734375
train loss:  0.5307790040969849
train gradient:  0.11970234633405281
iteration : 13796
train acc:  0.7421875
train loss:  0.48244839906692505
train gradient:  0.11647596764893431
iteration : 13797
train acc:  0.78125
train loss:  0.4932311773300171
train gradient:  0.09860510330290714
iteration : 13798
train acc:  0.8125
train loss:  0.3937830626964569
train gradient:  0.06852929956054957
iteration : 13799
train acc:  0.6796875
train loss:  0.5620655417442322
train gradient:  0.1463978712595277
iteration : 13800
train acc:  0.71875
train loss:  0.49537143111228943
train gradient:  0.10523329566214602
iteration : 13801
train acc:  0.75
train loss:  0.4961336553096771
train gradient:  0.15200177322218314
iteration : 13802
train acc:  0.796875
train loss:  0.41279858350753784
train gradient:  0.06717008666779707
iteration : 13803
train acc:  0.7265625
train loss:  0.462762713432312
train gradient:  0.0932315025500705
iteration : 13804
train acc:  0.71875
train loss:  0.5258979201316833
train gradient:  0.11721353352875404
iteration : 13805
train acc:  0.7265625
train loss:  0.4624693989753723
train gradient:  0.09611606617859414
iteration : 13806
train acc:  0.7734375
train loss:  0.49453580379486084
train gradient:  0.10394526841035125
iteration : 13807
train acc:  0.78125
train loss:  0.49747225642204285
train gradient:  0.14743867353322615
iteration : 13808
train acc:  0.7265625
train loss:  0.5364208221435547
train gradient:  0.12837870992525052
iteration : 13809
train acc:  0.71875
train loss:  0.5345986485481262
train gradient:  0.2131472853348307
iteration : 13810
train acc:  0.7109375
train loss:  0.4864747226238251
train gradient:  0.11553216506823252
iteration : 13811
train acc:  0.734375
train loss:  0.49410200119018555
train gradient:  0.12925166041771
iteration : 13812
train acc:  0.7578125
train loss:  0.47583484649658203
train gradient:  0.11559388625189095
iteration : 13813
train acc:  0.703125
train loss:  0.505766749382019
train gradient:  0.09250954607067743
iteration : 13814
train acc:  0.703125
train loss:  0.4805411100387573
train gradient:  0.11163992700021412
iteration : 13815
train acc:  0.7421875
train loss:  0.4931323826313019
train gradient:  0.12500806474775258
iteration : 13816
train acc:  0.7265625
train loss:  0.4589700698852539
train gradient:  0.09436850645074567
iteration : 13817
train acc:  0.7109375
train loss:  0.5119445323944092
train gradient:  0.11728403801374444
iteration : 13818
train acc:  0.7734375
train loss:  0.45332884788513184
train gradient:  0.09289316571738743
iteration : 13819
train acc:  0.7421875
train loss:  0.48382073640823364
train gradient:  0.11337646857514876
iteration : 13820
train acc:  0.6328125
train loss:  0.6185359954833984
train gradient:  0.1565879197024172
iteration : 13821
train acc:  0.71875
train loss:  0.46484804153442383
train gradient:  0.12968185969503793
iteration : 13822
train acc:  0.7890625
train loss:  0.4378501772880554
train gradient:  0.09005655797513068
iteration : 13823
train acc:  0.7734375
train loss:  0.4641094207763672
train gradient:  0.11450827256511932
iteration : 13824
train acc:  0.734375
train loss:  0.5384910702705383
train gradient:  0.17120209333570616
iteration : 13825
train acc:  0.6875
train loss:  0.5667518377304077
train gradient:  0.16307642048576987
iteration : 13826
train acc:  0.71875
train loss:  0.5001839399337769
train gradient:  0.1282012238484527
iteration : 13827
train acc:  0.796875
train loss:  0.4166272282600403
train gradient:  0.09679958588739561
iteration : 13828
train acc:  0.7265625
train loss:  0.5264566540718079
train gradient:  0.14297070568315684
iteration : 13829
train acc:  0.78125
train loss:  0.4383200407028198
train gradient:  0.09486360355250786
iteration : 13830
train acc:  0.7109375
train loss:  0.5601115226745605
train gradient:  0.14957352469813665
iteration : 13831
train acc:  0.6640625
train loss:  0.581527590751648
train gradient:  0.14217447180828363
iteration : 13832
train acc:  0.7421875
train loss:  0.45698609948158264
train gradient:  0.13663655124575314
iteration : 13833
train acc:  0.78125
train loss:  0.4436872601509094
train gradient:  0.09319989903922048
iteration : 13834
train acc:  0.7265625
train loss:  0.4868927001953125
train gradient:  0.09903394536238769
iteration : 13835
train acc:  0.703125
train loss:  0.5127665996551514
train gradient:  0.11283967135700544
iteration : 13836
train acc:  0.734375
train loss:  0.46267080307006836
train gradient:  0.09519436482299186
iteration : 13837
train acc:  0.7265625
train loss:  0.49057096242904663
train gradient:  0.11544783430832867
iteration : 13838
train acc:  0.703125
train loss:  0.5278369188308716
train gradient:  0.11815966102650872
iteration : 13839
train acc:  0.7734375
train loss:  0.4479259252548218
train gradient:  0.09999170485674516
iteration : 13840
train acc:  0.7734375
train loss:  0.5380444526672363
train gradient:  0.14077812902023892
iteration : 13841
train acc:  0.8515625
train loss:  0.3538452684879303
train gradient:  0.07649165117848523
iteration : 13842
train acc:  0.7734375
train loss:  0.46286913752555847
train gradient:  0.11035243713149609
iteration : 13843
train acc:  0.7578125
train loss:  0.5515694618225098
train gradient:  0.1490401204886317
iteration : 13844
train acc:  0.765625
train loss:  0.46591392159461975
train gradient:  0.09560745214772672
iteration : 13845
train acc:  0.734375
train loss:  0.48263102769851685
train gradient:  0.10073551381244329
iteration : 13846
train acc:  0.7421875
train loss:  0.5182127952575684
train gradient:  0.15421507267284423
iteration : 13847
train acc:  0.71875
train loss:  0.5067960023880005
train gradient:  0.12349607028373726
iteration : 13848
train acc:  0.78125
train loss:  0.4662412405014038
train gradient:  0.12636546183365976
iteration : 13849
train acc:  0.75
train loss:  0.49144813418388367
train gradient:  0.1027021936816764
iteration : 13850
train acc:  0.671875
train loss:  0.5655250549316406
train gradient:  0.13917927005570005
iteration : 13851
train acc:  0.75
train loss:  0.4870769679546356
train gradient:  0.12019221439067397
iteration : 13852
train acc:  0.7265625
train loss:  0.5535411834716797
train gradient:  0.14790950201553393
iteration : 13853
train acc:  0.796875
train loss:  0.4354817867279053
train gradient:  0.08770760669272826
iteration : 13854
train acc:  0.71875
train loss:  0.4986807107925415
train gradient:  0.14211608751505356
iteration : 13855
train acc:  0.7265625
train loss:  0.5221083164215088
train gradient:  0.12580678769708908
iteration : 13856
train acc:  0.7421875
train loss:  0.46749311685562134
train gradient:  0.10765419921060126
iteration : 13857
train acc:  0.6484375
train loss:  0.5541118383407593
train gradient:  0.12299306121239935
iteration : 13858
train acc:  0.7890625
train loss:  0.46907538175582886
train gradient:  0.11298277149716023
iteration : 13859
train acc:  0.7734375
train loss:  0.43564271926879883
train gradient:  0.0886989562542902
iteration : 13860
train acc:  0.7734375
train loss:  0.4644024074077606
train gradient:  0.1007599291811528
iteration : 13861
train acc:  0.6875
train loss:  0.5198836326599121
train gradient:  0.12493791603768231
iteration : 13862
train acc:  0.796875
train loss:  0.4657760262489319
train gradient:  0.09001857054215003
iteration : 13863
train acc:  0.78125
train loss:  0.4484202563762665
train gradient:  0.09671578958465135
iteration : 13864
train acc:  0.7734375
train loss:  0.48852744698524475
train gradient:  0.1603792443665037
iteration : 13865
train acc:  0.625
train loss:  0.5994195938110352
train gradient:  0.16331527581721128
iteration : 13866
train acc:  0.765625
train loss:  0.48715388774871826
train gradient:  0.11527911803275132
iteration : 13867
train acc:  0.7578125
train loss:  0.4599999189376831
train gradient:  0.094763549591092
iteration : 13868
train acc:  0.734375
train loss:  0.513365626335144
train gradient:  0.1560530225947917
iteration : 13869
train acc:  0.7578125
train loss:  0.44919735193252563
train gradient:  0.1009114559382497
iteration : 13870
train acc:  0.7734375
train loss:  0.45300227403640747
train gradient:  0.09596023194792642
iteration : 13871
train acc:  0.6953125
train loss:  0.5010015368461609
train gradient:  0.09645214399727121
iteration : 13872
train acc:  0.7421875
train loss:  0.509225606918335
train gradient:  0.1453692696007281
iteration : 13873
train acc:  0.7421875
train loss:  0.49764078855514526
train gradient:  0.1392336547172202
iteration : 13874
train acc:  0.71875
train loss:  0.45214956998825073
train gradient:  0.09114615997777253
iteration : 13875
train acc:  0.7578125
train loss:  0.42539072036743164
train gradient:  0.09830100003422695
iteration : 13876
train acc:  0.7734375
train loss:  0.44653674960136414
train gradient:  0.08760266506377784
iteration : 13877
train acc:  0.796875
train loss:  0.4137355089187622
train gradient:  0.0923421966480065
iteration : 13878
train acc:  0.6640625
train loss:  0.5391803979873657
train gradient:  0.18295376432052918
iteration : 13879
train acc:  0.765625
train loss:  0.4597102105617523
train gradient:  0.12996410541704634
iteration : 13880
train acc:  0.734375
train loss:  0.4733496904373169
train gradient:  0.1068402084706275
iteration : 13881
train acc:  0.6875
train loss:  0.5233965516090393
train gradient:  0.1251594815425497
iteration : 13882
train acc:  0.7734375
train loss:  0.4400160312652588
train gradient:  0.12881345082586199
iteration : 13883
train acc:  0.7890625
train loss:  0.42200154066085815
train gradient:  0.08990369488871061
iteration : 13884
train acc:  0.7890625
train loss:  0.45930129289627075
train gradient:  0.09358085666962354
iteration : 13885
train acc:  0.796875
train loss:  0.4653448164463043
train gradient:  0.1087473669563344
iteration : 13886
train acc:  0.7421875
train loss:  0.5165754556655884
train gradient:  0.10957448883293991
iteration : 13887
train acc:  0.7421875
train loss:  0.5092220902442932
train gradient:  0.11267627152991838
iteration : 13888
train acc:  0.75
train loss:  0.4692216217517853
train gradient:  0.1463414519125527
iteration : 13889
train acc:  0.7421875
train loss:  0.5459166765213013
train gradient:  0.10928256744206742
iteration : 13890
train acc:  0.7734375
train loss:  0.4462827444076538
train gradient:  0.11466518226507097
iteration : 13891
train acc:  0.8125
train loss:  0.38660430908203125
train gradient:  0.08590420392036087
iteration : 13892
train acc:  0.71875
train loss:  0.5144352912902832
train gradient:  0.1393145480107067
iteration : 13893
train acc:  0.7421875
train loss:  0.5314345359802246
train gradient:  0.14081743882050995
iteration : 13894
train acc:  0.734375
train loss:  0.5005954504013062
train gradient:  0.10698741246720024
iteration : 13895
train acc:  0.6953125
train loss:  0.5159855484962463
train gradient:  0.128910968591313
iteration : 13896
train acc:  0.75
train loss:  0.5179029703140259
train gradient:  0.12107872725988006
iteration : 13897
train acc:  0.609375
train loss:  0.6301575303077698
train gradient:  0.17258527578849292
iteration : 13898
train acc:  0.71875
train loss:  0.5490401983261108
train gradient:  0.11700990040081102
iteration : 13899
train acc:  0.7578125
train loss:  0.4807571768760681
train gradient:  0.09441474036608907
iteration : 13900
train acc:  0.6953125
train loss:  0.5083397030830383
train gradient:  0.12937690334623553
iteration : 13901
train acc:  0.7265625
train loss:  0.4920741319656372
train gradient:  0.1129674637004073
iteration : 13902
train acc:  0.7421875
train loss:  0.4965633153915405
train gradient:  0.10929002637363851
iteration : 13903
train acc:  0.6875
train loss:  0.5151603817939758
train gradient:  0.1103514055756958
iteration : 13904
train acc:  0.7265625
train loss:  0.46142342686653137
train gradient:  0.08459903075551076
iteration : 13905
train acc:  0.796875
train loss:  0.49200332164764404
train gradient:  0.12948897613614763
iteration : 13906
train acc:  0.7265625
train loss:  0.4602002501487732
train gradient:  0.11303328298850857
iteration : 13907
train acc:  0.7890625
train loss:  0.4083269238471985
train gradient:  0.09782591081081282
iteration : 13908
train acc:  0.7421875
train loss:  0.465824693441391
train gradient:  0.09320835087468433
iteration : 13909
train acc:  0.765625
train loss:  0.43200671672821045
train gradient:  0.0944563487854639
iteration : 13910
train acc:  0.6953125
train loss:  0.5565533638000488
train gradient:  0.14150991922561035
iteration : 13911
train acc:  0.7890625
train loss:  0.46489572525024414
train gradient:  0.09716388756114935
iteration : 13912
train acc:  0.765625
train loss:  0.4854755401611328
train gradient:  0.11089612138624493
iteration : 13913
train acc:  0.75
train loss:  0.560909628868103
train gradient:  0.1422270792852648
iteration : 13914
train acc:  0.7734375
train loss:  0.4549759328365326
train gradient:  0.09578890292051025
iteration : 13915
train acc:  0.765625
train loss:  0.47512680292129517
train gradient:  0.12885496837150245
iteration : 13916
train acc:  0.7421875
train loss:  0.5025515556335449
train gradient:  0.13842659333811236
iteration : 13917
train acc:  0.7578125
train loss:  0.46457070112228394
train gradient:  0.09489723291524839
iteration : 13918
train acc:  0.671875
train loss:  0.5457766056060791
train gradient:  0.12473392989237929
iteration : 13919
train acc:  0.765625
train loss:  0.4565354287624359
train gradient:  0.0966247697194284
iteration : 13920
train acc:  0.765625
train loss:  0.5009427666664124
train gradient:  0.12044900048692686
iteration : 13921
train acc:  0.78125
train loss:  0.46192312240600586
train gradient:  0.10023319520245819
iteration : 13922
train acc:  0.7734375
train loss:  0.46005454659461975
train gradient:  0.11729842753306945
iteration : 13923
train acc:  0.71875
train loss:  0.5339301824569702
train gradient:  0.1328836423488345
iteration : 13924
train acc:  0.75
train loss:  0.4856265187263489
train gradient:  0.10829249311477583
iteration : 13925
train acc:  0.78125
train loss:  0.4624672532081604
train gradient:  0.09923534702914323
iteration : 13926
train acc:  0.7578125
train loss:  0.48665738105773926
train gradient:  0.11303342979541181
iteration : 13927
train acc:  0.796875
train loss:  0.42648500204086304
train gradient:  0.11161627547152657
iteration : 13928
train acc:  0.71875
train loss:  0.5077697038650513
train gradient:  0.1355051237110272
iteration : 13929
train acc:  0.734375
train loss:  0.4822503328323364
train gradient:  0.10812286169296328
iteration : 13930
train acc:  0.7890625
train loss:  0.4385607838630676
train gradient:  0.10185661152508295
iteration : 13931
train acc:  0.75
train loss:  0.4566681981086731
train gradient:  0.08735137022651583
iteration : 13932
train acc:  0.8046875
train loss:  0.3971395492553711
train gradient:  0.05858045308727834
iteration : 13933
train acc:  0.78125
train loss:  0.4108886122703552
train gradient:  0.0798339887572319
iteration : 13934
train acc:  0.75
train loss:  0.42090851068496704
train gradient:  0.09693051973308545
iteration : 13935
train acc:  0.7265625
train loss:  0.4743500351905823
train gradient:  0.10663217492302689
iteration : 13936
train acc:  0.8125
train loss:  0.44042497873306274
train gradient:  0.0915636351271388
iteration : 13937
train acc:  0.796875
train loss:  0.47025224566459656
train gradient:  0.09709541471992858
iteration : 13938
train acc:  0.7421875
train loss:  0.4745572805404663
train gradient:  0.10263818605368799
iteration : 13939
train acc:  0.6328125
train loss:  0.6456543207168579
train gradient:  0.20244310197959287
iteration : 13940
train acc:  0.75
train loss:  0.4463821053504944
train gradient:  0.0849559871691025
iteration : 13941
train acc:  0.8125
train loss:  0.42605558037757874
train gradient:  0.08211182667051352
iteration : 13942
train acc:  0.7890625
train loss:  0.46983152627944946
train gradient:  0.11194545537920199
iteration : 13943
train acc:  0.6796875
train loss:  0.5335409641265869
train gradient:  0.10226268759682401
iteration : 13944
train acc:  0.765625
train loss:  0.47372543811798096
train gradient:  0.11541415419665632
iteration : 13945
train acc:  0.7734375
train loss:  0.47226059436798096
train gradient:  0.11927098366695774
iteration : 13946
train acc:  0.734375
train loss:  0.4905961751937866
train gradient:  0.14133244144882556
iteration : 13947
train acc:  0.71875
train loss:  0.5175250768661499
train gradient:  0.1330734839583241
iteration : 13948
train acc:  0.7578125
train loss:  0.46164196729660034
train gradient:  0.10151996553395626
iteration : 13949
train acc:  0.7734375
train loss:  0.47021371126174927
train gradient:  0.11707589805689489
iteration : 13950
train acc:  0.7421875
train loss:  0.5652114152908325
train gradient:  0.13744168736884654
iteration : 13951
train acc:  0.7109375
train loss:  0.5237337350845337
train gradient:  0.1341680183436308
iteration : 13952
train acc:  0.75
train loss:  0.4817498028278351
train gradient:  0.1028883179803442
iteration : 13953
train acc:  0.75
train loss:  0.4963858723640442
train gradient:  0.12002905202815578
iteration : 13954
train acc:  0.7265625
train loss:  0.47103017568588257
train gradient:  0.09953184554117722
iteration : 13955
train acc:  0.796875
train loss:  0.4909013509750366
train gradient:  0.10768819387673792
iteration : 13956
train acc:  0.7265625
train loss:  0.5266940593719482
train gradient:  0.11700738347269139
iteration : 13957
train acc:  0.6953125
train loss:  0.5045059323310852
train gradient:  0.14752588990287194
iteration : 13958
train acc:  0.796875
train loss:  0.42188650369644165
train gradient:  0.1034814437169021
iteration : 13959
train acc:  0.765625
train loss:  0.4997580051422119
train gradient:  0.13287378754244977
iteration : 13960
train acc:  0.671875
train loss:  0.5730468034744263
train gradient:  0.18361936310069654
iteration : 13961
train acc:  0.75
train loss:  0.4722205102443695
train gradient:  0.08950610412502386
iteration : 13962
train acc:  0.75
train loss:  0.4915783405303955
train gradient:  0.10557062046217791
iteration : 13963
train acc:  0.765625
train loss:  0.4753997325897217
train gradient:  0.10677557756468217
iteration : 13964
train acc:  0.7109375
train loss:  0.4921049475669861
train gradient:  0.10908654371958083
iteration : 13965
train acc:  0.75
train loss:  0.501663088798523
train gradient:  0.12309669729722247
iteration : 13966
train acc:  0.75
train loss:  0.46874862909317017
train gradient:  0.10639341906760229
iteration : 13967
train acc:  0.7734375
train loss:  0.4856494665145874
train gradient:  0.12509837602694207
iteration : 13968
train acc:  0.7890625
train loss:  0.427166223526001
train gradient:  0.08212725511624294
iteration : 13969
train acc:  0.6875
train loss:  0.5375749468803406
train gradient:  0.1448092193790146
iteration : 13970
train acc:  0.7265625
train loss:  0.5253247022628784
train gradient:  0.11205532655081366
iteration : 13971
train acc:  0.703125
train loss:  0.4944339394569397
train gradient:  0.10925545446988134
iteration : 13972
train acc:  0.7734375
train loss:  0.45753583312034607
train gradient:  0.08905664350828427
iteration : 13973
train acc:  0.75
train loss:  0.4788915514945984
train gradient:  0.09357739334884461
iteration : 13974
train acc:  0.8046875
train loss:  0.465750515460968
train gradient:  0.11028467295735737
iteration : 13975
train acc:  0.7421875
train loss:  0.5069144368171692
train gradient:  0.12668196529088763
iteration : 13976
train acc:  0.6875
train loss:  0.5824993848800659
train gradient:  0.1870240046658186
iteration : 13977
train acc:  0.765625
train loss:  0.4973162114620209
train gradient:  0.1812364408233902
iteration : 13978
train acc:  0.734375
train loss:  0.5532853603363037
train gradient:  0.16878663563916035
iteration : 13979
train acc:  0.671875
train loss:  0.5333155989646912
train gradient:  0.1348037949851854
iteration : 13980
train acc:  0.8046875
train loss:  0.42248639464378357
train gradient:  0.08449878695537738
iteration : 13981
train acc:  0.765625
train loss:  0.46679985523223877
train gradient:  0.11799001660238391
iteration : 13982
train acc:  0.8046875
train loss:  0.3999684453010559
train gradient:  0.07863603989705831
iteration : 13983
train acc:  0.734375
train loss:  0.4786003530025482
train gradient:  0.09943164271296973
iteration : 13984
train acc:  0.71875
train loss:  0.4751243591308594
train gradient:  0.09837380569005373
iteration : 13985
train acc:  0.7265625
train loss:  0.48333585262298584
train gradient:  0.12971777080534255
iteration : 13986
train acc:  0.78125
train loss:  0.4514274001121521
train gradient:  0.1199131319176836
iteration : 13987
train acc:  0.84375
train loss:  0.4067767560482025
train gradient:  0.07172658395667136
iteration : 13988
train acc:  0.75
train loss:  0.4532984495162964
train gradient:  0.08693545669264216
iteration : 13989
train acc:  0.7578125
train loss:  0.4732491970062256
train gradient:  0.10408423305044869
iteration : 13990
train acc:  0.7890625
train loss:  0.4579973816871643
train gradient:  0.08877856610592777
iteration : 13991
train acc:  0.734375
train loss:  0.5333059430122375
train gradient:  0.13680450543704462
iteration : 13992
train acc:  0.71875
train loss:  0.45517218112945557
train gradient:  0.0943731149998524
iteration : 13993
train acc:  0.7421875
train loss:  0.4563805162906647
train gradient:  0.11768622686299708
iteration : 13994
train acc:  0.6796875
train loss:  0.5626528263092041
train gradient:  0.15899437985111592
iteration : 13995
train acc:  0.7578125
train loss:  0.45259106159210205
train gradient:  0.10500916829367014
iteration : 13996
train acc:  0.8046875
train loss:  0.4253219962120056
train gradient:  0.09292973596036191
iteration : 13997
train acc:  0.703125
train loss:  0.5561836957931519
train gradient:  0.11585177010262597
iteration : 13998
train acc:  0.78125
train loss:  0.41796696186065674
train gradient:  0.10269288360165621
iteration : 13999
train acc:  0.75
train loss:  0.49517935514450073
train gradient:  0.1066721280589284
iteration : 14000
train acc:  0.796875
train loss:  0.4447054862976074
train gradient:  0.10890049912617814
iteration : 14001
train acc:  0.7421875
train loss:  0.5339967012405396
train gradient:  0.1436222154732127
iteration : 14002
train acc:  0.7578125
train loss:  0.49528640508651733
train gradient:  0.10876223748391121
iteration : 14003
train acc:  0.6875
train loss:  0.5487250089645386
train gradient:  0.14442995369608297
iteration : 14004
train acc:  0.7265625
train loss:  0.49117955565452576
train gradient:  0.10839848438356012
iteration : 14005
train acc:  0.7578125
train loss:  0.5059013366699219
train gradient:  0.13713743366136533
iteration : 14006
train acc:  0.71875
train loss:  0.5466402769088745
train gradient:  0.14551928463634028
iteration : 14007
train acc:  0.765625
train loss:  0.4502210021018982
train gradient:  0.0803775510749478
iteration : 14008
train acc:  0.765625
train loss:  0.46750181913375854
train gradient:  0.09623796567270637
iteration : 14009
train acc:  0.65625
train loss:  0.5655523538589478
train gradient:  0.14955621453916884
iteration : 14010
train acc:  0.8046875
train loss:  0.39734169840812683
train gradient:  0.09359031206799436
iteration : 14011
train acc:  0.7109375
train loss:  0.5833439826965332
train gradient:  0.1547214426503029
iteration : 14012
train acc:  0.7421875
train loss:  0.5007054805755615
train gradient:  0.11905494025505257
iteration : 14013
train acc:  0.734375
train loss:  0.4957602918148041
train gradient:  0.12263273185443137
iteration : 14014
train acc:  0.765625
train loss:  0.4334508180618286
train gradient:  0.11252477181958465
iteration : 14015
train acc:  0.7578125
train loss:  0.5072048902511597
train gradient:  0.14401756763497053
iteration : 14016
train acc:  0.7890625
train loss:  0.42603617906570435
train gradient:  0.08889228490868675
iteration : 14017
train acc:  0.7734375
train loss:  0.4667316675186157
train gradient:  0.10243668419610195
iteration : 14018
train acc:  0.78125
train loss:  0.4604400098323822
train gradient:  0.10147672033456695
iteration : 14019
train acc:  0.734375
train loss:  0.47937583923339844
train gradient:  0.09851472766064268
iteration : 14020
train acc:  0.71875
train loss:  0.5396769046783447
train gradient:  0.16381306079709182
iteration : 14021
train acc:  0.703125
train loss:  0.47782227396965027
train gradient:  0.15413664698104557
iteration : 14022
train acc:  0.6953125
train loss:  0.5248633623123169
train gradient:  0.16504236930246574
iteration : 14023
train acc:  0.7890625
train loss:  0.4540616273880005
train gradient:  0.09691277722162547
iteration : 14024
train acc:  0.7890625
train loss:  0.4652816355228424
train gradient:  0.10035047406313441
iteration : 14025
train acc:  0.78125
train loss:  0.4566956162452698
train gradient:  0.10072450790788505
iteration : 14026
train acc:  0.8125
train loss:  0.4297919273376465
train gradient:  0.08043773436637212
iteration : 14027
train acc:  0.796875
train loss:  0.41725295782089233
train gradient:  0.0778922518935105
iteration : 14028
train acc:  0.8125
train loss:  0.46903538703918457
train gradient:  0.10179095066232062
iteration : 14029
train acc:  0.7265625
train loss:  0.48257553577423096
train gradient:  0.08021854028430155
iteration : 14030
train acc:  0.78125
train loss:  0.45871400833129883
train gradient:  0.0976757524878217
iteration : 14031
train acc:  0.75
train loss:  0.49033814668655396
train gradient:  0.12437330112415172
iteration : 14032
train acc:  0.7109375
train loss:  0.47138357162475586
train gradient:  0.1330464849231932
iteration : 14033
train acc:  0.7578125
train loss:  0.4804506003856659
train gradient:  0.09397037490659975
iteration : 14034
train acc:  0.7734375
train loss:  0.513953685760498
train gradient:  0.12295782074784256
iteration : 14035
train acc:  0.78125
train loss:  0.5255619287490845
train gradient:  0.09903027299989124
iteration : 14036
train acc:  0.7265625
train loss:  0.4497453570365906
train gradient:  0.10976561878134398
iteration : 14037
train acc:  0.71875
train loss:  0.5464409589767456
train gradient:  0.14315187217656858
iteration : 14038
train acc:  0.734375
train loss:  0.5433403849601746
train gradient:  0.14294121675441085
iteration : 14039
train acc:  0.8203125
train loss:  0.4160093069076538
train gradient:  0.09124050170312065
iteration : 14040
train acc:  0.8046875
train loss:  0.469013512134552
train gradient:  0.11778298536503963
iteration : 14041
train acc:  0.796875
train loss:  0.4408316910266876
train gradient:  0.08505091476638486
iteration : 14042
train acc:  0.796875
train loss:  0.4557002782821655
train gradient:  0.11713269605841103
iteration : 14043
train acc:  0.7890625
train loss:  0.4390510618686676
train gradient:  0.10391702864357545
iteration : 14044
train acc:  0.6796875
train loss:  0.5629734992980957
train gradient:  0.1377625470370354
iteration : 14045
train acc:  0.71875
train loss:  0.5021992921829224
train gradient:  0.11619104377465347
iteration : 14046
train acc:  0.7578125
train loss:  0.4978569746017456
train gradient:  0.12824628452452475
iteration : 14047
train acc:  0.7421875
train loss:  0.4796449542045593
train gradient:  0.10965843357096429
iteration : 14048
train acc:  0.7109375
train loss:  0.5497044920921326
train gradient:  0.1581283170768934
iteration : 14049
train acc:  0.7421875
train loss:  0.4871557354927063
train gradient:  0.13680685780630092
iteration : 14050
train acc:  0.7734375
train loss:  0.45711368322372437
train gradient:  0.10386809590091695
iteration : 14051
train acc:  0.765625
train loss:  0.5066606998443604
train gradient:  0.14516955488310107
iteration : 14052
train acc:  0.7734375
train loss:  0.49502602219581604
train gradient:  0.14772324251064134
iteration : 14053
train acc:  0.75
train loss:  0.5089826583862305
train gradient:  0.12635365488751066
iteration : 14054
train acc:  0.7578125
train loss:  0.4777461886405945
train gradient:  0.10069728262404934
iteration : 14055
train acc:  0.7109375
train loss:  0.5303873419761658
train gradient:  0.14064036044717926
iteration : 14056
train acc:  0.8046875
train loss:  0.45943117141723633
train gradient:  0.1423935692031173
iteration : 14057
train acc:  0.7109375
train loss:  0.5520163774490356
train gradient:  0.1327292082352253
iteration : 14058
train acc:  0.6484375
train loss:  0.6038734912872314
train gradient:  0.17690720728310544
iteration : 14059
train acc:  0.71875
train loss:  0.5151616334915161
train gradient:  0.1409106794324808
iteration : 14060
train acc:  0.734375
train loss:  0.45371606945991516
train gradient:  0.1142659701941855
iteration : 14061
train acc:  0.78125
train loss:  0.4243793785572052
train gradient:  0.1081915933419812
iteration : 14062
train acc:  0.671875
train loss:  0.5212799310684204
train gradient:  0.1539870860525969
iteration : 14063
train acc:  0.78125
train loss:  0.47656992077827454
train gradient:  0.10433542827687181
iteration : 14064
train acc:  0.7578125
train loss:  0.4181300401687622
train gradient:  0.10809484582731348
iteration : 14065
train acc:  0.8203125
train loss:  0.4366987347602844
train gradient:  0.10581224386562138
iteration : 14066
train acc:  0.734375
train loss:  0.49329674243927
train gradient:  0.11315558690425027
iteration : 14067
train acc:  0.7578125
train loss:  0.4626776874065399
train gradient:  0.10544113846267453
iteration : 14068
train acc:  0.75
train loss:  0.43926334381103516
train gradient:  0.09232939278321706
iteration : 14069
train acc:  0.71875
train loss:  0.5156060457229614
train gradient:  0.12769401612502895
iteration : 14070
train acc:  0.765625
train loss:  0.5022842884063721
train gradient:  0.14986088132807546
iteration : 14071
train acc:  0.7265625
train loss:  0.5440059900283813
train gradient:  0.11860359754624311
iteration : 14072
train acc:  0.75
train loss:  0.47273263335227966
train gradient:  0.12072859287474443
iteration : 14073
train acc:  0.734375
train loss:  0.49836230278015137
train gradient:  0.15024186967692627
iteration : 14074
train acc:  0.7734375
train loss:  0.4492454528808594
train gradient:  0.10008265767365229
iteration : 14075
train acc:  0.7109375
train loss:  0.5214160680770874
train gradient:  0.15039698151480602
iteration : 14076
train acc:  0.7421875
train loss:  0.5153043270111084
train gradient:  0.10658712484218096
iteration : 14077
train acc:  0.734375
train loss:  0.48582911491394043
train gradient:  0.09779559661660242
iteration : 14078
train acc:  0.7890625
train loss:  0.44251328706741333
train gradient:  0.08647061572099195
iteration : 14079
train acc:  0.7421875
train loss:  0.5239242315292358
train gradient:  0.11786693453049917
iteration : 14080
train acc:  0.6328125
train loss:  0.6298177242279053
train gradient:  0.1594373123208925
iteration : 14081
train acc:  0.734375
train loss:  0.48088952898979187
train gradient:  0.1340457977379675
iteration : 14082
train acc:  0.6796875
train loss:  0.5503132343292236
train gradient:  0.1289065050144766
iteration : 14083
train acc:  0.75
train loss:  0.486080527305603
train gradient:  0.11129609867058529
iteration : 14084
train acc:  0.71875
train loss:  0.512592077255249
train gradient:  0.10831975129117793
iteration : 14085
train acc:  0.765625
train loss:  0.4699549376964569
train gradient:  0.10996678590665175
iteration : 14086
train acc:  0.8046875
train loss:  0.4265749752521515
train gradient:  0.08959055050611921
iteration : 14087
train acc:  0.671875
train loss:  0.5434216260910034
train gradient:  0.12481178479314065
iteration : 14088
train acc:  0.734375
train loss:  0.47709667682647705
train gradient:  0.11114298569918624
iteration : 14089
train acc:  0.734375
train loss:  0.5243210196495056
train gradient:  0.11874152266354135
iteration : 14090
train acc:  0.75
train loss:  0.4304748475551605
train gradient:  0.09296427479827325
iteration : 14091
train acc:  0.734375
train loss:  0.5080682039260864
train gradient:  0.10181699262027766
iteration : 14092
train acc:  0.734375
train loss:  0.48098981380462646
train gradient:  0.11175211055686927
iteration : 14093
train acc:  0.8125
train loss:  0.39404210448265076
train gradient:  0.07471022840754572
iteration : 14094
train acc:  0.6875
train loss:  0.5558338761329651
train gradient:  0.1712139555594183
iteration : 14095
train acc:  0.7578125
train loss:  0.47441595792770386
train gradient:  0.10082634661083581
iteration : 14096
train acc:  0.7265625
train loss:  0.5070852637290955
train gradient:  0.11934848558079482
iteration : 14097
train acc:  0.7109375
train loss:  0.5417523384094238
train gradient:  0.11625366082535994
iteration : 14098
train acc:  0.71875
train loss:  0.4916706383228302
train gradient:  0.10420750819126452
iteration : 14099
train acc:  0.796875
train loss:  0.46803510189056396
train gradient:  0.11732646561466688
iteration : 14100
train acc:  0.796875
train loss:  0.4199208915233612
train gradient:  0.08134623464944409
iteration : 14101
train acc:  0.671875
train loss:  0.5940407514572144
train gradient:  0.13956357463122787
iteration : 14102
train acc:  0.8046875
train loss:  0.4426841139793396
train gradient:  0.10232855071835162
iteration : 14103
train acc:  0.7265625
train loss:  0.518591582775116
train gradient:  0.14315997704781055
iteration : 14104
train acc:  0.734375
train loss:  0.5166648030281067
train gradient:  0.13768789063306514
iteration : 14105
train acc:  0.7421875
train loss:  0.48425808548927307
train gradient:  0.11484428397383044
iteration : 14106
train acc:  0.765625
train loss:  0.45015794038772583
train gradient:  0.097063010532046
iteration : 14107
train acc:  0.8125
train loss:  0.4681631922721863
train gradient:  0.10195718462489338
iteration : 14108
train acc:  0.75
train loss:  0.4972913861274719
train gradient:  0.12367288116393575
iteration : 14109
train acc:  0.8203125
train loss:  0.45466095209121704
train gradient:  0.13484061861925373
iteration : 14110
train acc:  0.8125
train loss:  0.451709508895874
train gradient:  0.09639299076683078
iteration : 14111
train acc:  0.6953125
train loss:  0.5532704591751099
train gradient:  0.12735343555579542
iteration : 14112
train acc:  0.8046875
train loss:  0.4106675982475281
train gradient:  0.08742691533934939
iteration : 14113
train acc:  0.7578125
train loss:  0.47482508420944214
train gradient:  0.10131786412347492
iteration : 14114
train acc:  0.7265625
train loss:  0.4860610365867615
train gradient:  0.07952453597887815
iteration : 14115
train acc:  0.7890625
train loss:  0.43114322423934937
train gradient:  0.0942368095860213
iteration : 14116
train acc:  0.84375
train loss:  0.414785772562027
train gradient:  0.08880524866194771
iteration : 14117
train acc:  0.7109375
train loss:  0.4806719422340393
train gradient:  0.10113186533589373
iteration : 14118
train acc:  0.7265625
train loss:  0.4936850070953369
train gradient:  0.12051338197480953
iteration : 14119
train acc:  0.7578125
train loss:  0.48100361227989197
train gradient:  0.11211797759251459
iteration : 14120
train acc:  0.796875
train loss:  0.4675118327140808
train gradient:  0.12515309826466967
iteration : 14121
train acc:  0.7890625
train loss:  0.45118892192840576
train gradient:  0.08882986995852332
iteration : 14122
train acc:  0.7578125
train loss:  0.5059579610824585
train gradient:  0.12996573635979705
iteration : 14123
train acc:  0.734375
train loss:  0.45122793316841125
train gradient:  0.11391794917856715
iteration : 14124
train acc:  0.75
train loss:  0.46141624450683594
train gradient:  0.12044100149071384
iteration : 14125
train acc:  0.703125
train loss:  0.4808419942855835
train gradient:  0.12666024117504426
iteration : 14126
train acc:  0.7890625
train loss:  0.40103381872177124
train gradient:  0.11060014250850975
iteration : 14127
train acc:  0.7734375
train loss:  0.49517419934272766
train gradient:  0.10814971593900481
iteration : 14128
train acc:  0.7578125
train loss:  0.5002012252807617
train gradient:  0.10664595568063098
iteration : 14129
train acc:  0.7109375
train loss:  0.5260571241378784
train gradient:  0.15474472628214775
iteration : 14130
train acc:  0.7578125
train loss:  0.4214969575405121
train gradient:  0.09067669379104894
iteration : 14131
train acc:  0.8203125
train loss:  0.4321492910385132
train gradient:  0.09278470392650863
iteration : 14132
train acc:  0.6875
train loss:  0.5305884480476379
train gradient:  0.15055053333258556
iteration : 14133
train acc:  0.765625
train loss:  0.43881648778915405
train gradient:  0.08612204465555431
iteration : 14134
train acc:  0.8203125
train loss:  0.4550805687904358
train gradient:  0.0947499356284299
iteration : 14135
train acc:  0.7421875
train loss:  0.5210870504379272
train gradient:  0.12811447166502626
iteration : 14136
train acc:  0.7265625
train loss:  0.44123294949531555
train gradient:  0.10872930278587965
iteration : 14137
train acc:  0.8359375
train loss:  0.44100069999694824
train gradient:  0.07736377364242608
iteration : 14138
train acc:  0.765625
train loss:  0.49421319365501404
train gradient:  0.1117436846369852
iteration : 14139
train acc:  0.7109375
train loss:  0.5124925971031189
train gradient:  0.11606178038801794
iteration : 14140
train acc:  0.7265625
train loss:  0.5106346607208252
train gradient:  0.12118916733114482
iteration : 14141
train acc:  0.7265625
train loss:  0.5081067085266113
train gradient:  0.12157268053066558
iteration : 14142
train acc:  0.71875
train loss:  0.5187873840332031
train gradient:  0.13577023195207275
iteration : 14143
train acc:  0.828125
train loss:  0.3935876488685608
train gradient:  0.0784926729886624
iteration : 14144
train acc:  0.7265625
train loss:  0.5202694535255432
train gradient:  0.1299369672971344
iteration : 14145
train acc:  0.71875
train loss:  0.4811806082725525
train gradient:  0.09848795828311363
iteration : 14146
train acc:  0.7578125
train loss:  0.45393943786621094
train gradient:  0.11511089622169116
iteration : 14147
train acc:  0.6953125
train loss:  0.5239506363868713
train gradient:  0.14944203864982525
iteration : 14148
train acc:  0.7421875
train loss:  0.5098103284835815
train gradient:  0.1141553719864661
iteration : 14149
train acc:  0.71875
train loss:  0.5612820386886597
train gradient:  0.14269675660388742
iteration : 14150
train acc:  0.8046875
train loss:  0.42092233896255493
train gradient:  0.09318627802017579
iteration : 14151
train acc:  0.7578125
train loss:  0.5342977046966553
train gradient:  0.13569545926285428
iteration : 14152
train acc:  0.765625
train loss:  0.5003031492233276
train gradient:  0.11722914506939847
iteration : 14153
train acc:  0.7109375
train loss:  0.5307655334472656
train gradient:  0.12315323659379177
iteration : 14154
train acc:  0.6875
train loss:  0.5353212952613831
train gradient:  0.12998484419438633
iteration : 14155
train acc:  0.8203125
train loss:  0.4255949556827545
train gradient:  0.07852085309220033
iteration : 14156
train acc:  0.765625
train loss:  0.4993583858013153
train gradient:  0.09140283977235178
iteration : 14157
train acc:  0.7109375
train loss:  0.4824235439300537
train gradient:  0.09558358460355297
iteration : 14158
train acc:  0.8046875
train loss:  0.4514096975326538
train gradient:  0.08473630157540368
iteration : 14159
train acc:  0.765625
train loss:  0.4944555163383484
train gradient:  0.15601087494236948
iteration : 14160
train acc:  0.71875
train loss:  0.5307022333145142
train gradient:  0.13164607229514258
iteration : 14161
train acc:  0.78125
train loss:  0.4644775986671448
train gradient:  0.11174139780198782
iteration : 14162
train acc:  0.703125
train loss:  0.5133256912231445
train gradient:  0.12174903984524507
iteration : 14163
train acc:  0.734375
train loss:  0.5260133743286133
train gradient:  0.16002473778332343
iteration : 14164
train acc:  0.734375
train loss:  0.5182598233222961
train gradient:  0.17998737595141068
iteration : 14165
train acc:  0.703125
train loss:  0.5299161672592163
train gradient:  0.14580602642924276
iteration : 14166
train acc:  0.765625
train loss:  0.5270694494247437
train gradient:  0.11166600671668511
iteration : 14167
train acc:  0.8046875
train loss:  0.47405290603637695
train gradient:  0.1328409806474013
iteration : 14168
train acc:  0.765625
train loss:  0.4550924301147461
train gradient:  0.11695986801138589
iteration : 14169
train acc:  0.8203125
train loss:  0.4473038911819458
train gradient:  0.13286357140226868
iteration : 14170
train acc:  0.7265625
train loss:  0.6407026052474976
train gradient:  0.20263599886192254
iteration : 14171
train acc:  0.6953125
train loss:  0.5339851379394531
train gradient:  0.11617465183094482
iteration : 14172
train acc:  0.7421875
train loss:  0.45069849491119385
train gradient:  0.07773692361059967
iteration : 14173
train acc:  0.7578125
train loss:  0.4558164179325104
train gradient:  0.08144937941786237
iteration : 14174
train acc:  0.7890625
train loss:  0.42013001441955566
train gradient:  0.08713417426298747
iteration : 14175
train acc:  0.6953125
train loss:  0.5159032344818115
train gradient:  0.11607911226019196
iteration : 14176
train acc:  0.7890625
train loss:  0.43802839517593384
train gradient:  0.09906888905211599
iteration : 14177
train acc:  0.8046875
train loss:  0.40111100673675537
train gradient:  0.06668084406771814
iteration : 14178
train acc:  0.7109375
train loss:  0.5139694213867188
train gradient:  0.1535201822414342
iteration : 14179
train acc:  0.703125
train loss:  0.5279994606971741
train gradient:  0.1341168088263948
iteration : 14180
train acc:  0.703125
train loss:  0.5426150560379028
train gradient:  0.1337583196254662
iteration : 14181
train acc:  0.71875
train loss:  0.5281244516372681
train gradient:  0.14420603227286777
iteration : 14182
train acc:  0.7734375
train loss:  0.4627458453178406
train gradient:  0.1250060036159635
iteration : 14183
train acc:  0.6796875
train loss:  0.5040397644042969
train gradient:  0.11759292334361858
iteration : 14184
train acc:  0.8203125
train loss:  0.4358854293823242
train gradient:  0.13443169141264832
iteration : 14185
train acc:  0.8515625
train loss:  0.40073227882385254
train gradient:  0.08563058025079143
iteration : 14186
train acc:  0.8888888888888888
train loss:  0.4617413580417633
train gradient:  0.7129318430235869
val acc:  0.7349102525098874
val f1:  0.7630043653869691
val confusion matrix:  [[60780 37830]
 [14451 84159]]

----------------------------------------new_epoch--------------------------------------

epoch:  1
iteration : 0
train acc:  0.75
train loss:  0.5061556100845337
train gradient:  0.11949066986948312
iteration : 1
train acc:  0.7421875
train loss:  0.4995921850204468
train gradient:  0.1079424060031044
iteration : 2
train acc:  0.703125
train loss:  0.5438995361328125
train gradient:  0.1561861705435883
iteration : 3
train acc:  0.6953125
train loss:  0.5075443387031555
train gradient:  0.1260180655493141
iteration : 4
train acc:  0.8203125
train loss:  0.40569570660591125
train gradient:  0.07153637380589856
iteration : 5
train acc:  0.796875
train loss:  0.42846301198005676
train gradient:  0.10571894322856944
iteration : 6
train acc:  0.7578125
train loss:  0.4762189984321594
train gradient:  0.11792993360680355
iteration : 7
train acc:  0.7578125
train loss:  0.48895370960235596
train gradient:  0.10461399821324657
iteration : 8
train acc:  0.71875
train loss:  0.480900377035141
train gradient:  0.13310914825870768
iteration : 9
train acc:  0.78125
train loss:  0.4580436050891876
train gradient:  0.10011140204577583
iteration : 10
train acc:  0.8125
train loss:  0.4345855116844177
train gradient:  0.10140004502336636
iteration : 11
train acc:  0.7578125
train loss:  0.5014855265617371
train gradient:  0.11522678020407082
iteration : 12
train acc:  0.71875
train loss:  0.47911739349365234
train gradient:  0.10866417719155637
iteration : 13
train acc:  0.734375
train loss:  0.4609324336051941
train gradient:  0.09865877039600988
iteration : 14
train acc:  0.78125
train loss:  0.4669169783592224
train gradient:  0.08995201267907482
iteration : 15
train acc:  0.7734375
train loss:  0.4526126980781555
train gradient:  0.09772218083826657
iteration : 16
train acc:  0.71875
train loss:  0.570284366607666
train gradient:  0.1241601773577011
iteration : 17
train acc:  0.7578125
train loss:  0.4677238464355469
train gradient:  0.09646774526382039
iteration : 18
train acc:  0.671875
train loss:  0.5745897889137268
train gradient:  0.19056390273334262
iteration : 19
train acc:  0.6484375
train loss:  0.6169480085372925
train gradient:  0.2058469068726127
iteration : 20
train acc:  0.7421875
train loss:  0.46308633685112
train gradient:  0.10231639337663674
iteration : 21
train acc:  0.7578125
train loss:  0.45118191838264465
train gradient:  0.1151643719543941
iteration : 22
train acc:  0.7578125
train loss:  0.42601048946380615
train gradient:  0.08064794151586743
iteration : 23
train acc:  0.7734375
train loss:  0.4439822733402252
train gradient:  0.09850304147664303
iteration : 24
train acc:  0.8046875
train loss:  0.4714457392692566
train gradient:  0.11319919509318796
iteration : 25
train acc:  0.8046875
train loss:  0.47097331285476685
train gradient:  0.10706309926876016
iteration : 26
train acc:  0.765625
train loss:  0.45792892575263977
train gradient:  0.11339396848338146
iteration : 27
train acc:  0.7421875
train loss:  0.488754004240036
train gradient:  0.11521455145461675
iteration : 28
train acc:  0.71875
train loss:  0.4909868836402893
train gradient:  0.11382310913996298
iteration : 29
train acc:  0.6953125
train loss:  0.4773615598678589
train gradient:  0.09988083724891911
iteration : 30
train acc:  0.7578125
train loss:  0.48076131939888
train gradient:  0.1188467402392675
iteration : 31
train acc:  0.78125
train loss:  0.4672996401786804
train gradient:  0.10179579620219083
iteration : 32
train acc:  0.6796875
train loss:  0.48882293701171875
train gradient:  0.10957051460094216
iteration : 33
train acc:  0.75
train loss:  0.46573004126548767
train gradient:  0.09691610741452251
iteration : 34
train acc:  0.7421875
train loss:  0.494283527135849
train gradient:  0.13981125121460325
iteration : 35
train acc:  0.71875
train loss:  0.5787654519081116
train gradient:  0.1587688115680203
iteration : 36
train acc:  0.7421875
train loss:  0.4923900365829468
train gradient:  0.09736037060196247
iteration : 37
train acc:  0.7734375
train loss:  0.3899768888950348
train gradient:  0.092990679040324
iteration : 38
train acc:  0.7578125
train loss:  0.4535999000072479
train gradient:  0.1000094789391573
iteration : 39
train acc:  0.75
train loss:  0.44776538014411926
train gradient:  0.09060401051647428
iteration : 40
train acc:  0.71875
train loss:  0.5367330312728882
train gradient:  0.1408531884329675
iteration : 41
train acc:  0.7265625
train loss:  0.5447326302528381
train gradient:  0.15590778386141937
iteration : 42
train acc:  0.7421875
train loss:  0.43057674169540405
train gradient:  0.09477268669973382
iteration : 43
train acc:  0.8125
train loss:  0.41776779294013977
train gradient:  0.08622253432724288
iteration : 44
train acc:  0.7109375
train loss:  0.5176371335983276
train gradient:  0.16660218515581807
iteration : 45
train acc:  0.7421875
train loss:  0.4575609564781189
train gradient:  0.10706422319159127
iteration : 46
train acc:  0.6875
train loss:  0.5316638946533203
train gradient:  0.1255633124140037
iteration : 47
train acc:  0.75
train loss:  0.50593501329422
train gradient:  0.13045473871689217
iteration : 48
train acc:  0.7109375
train loss:  0.5002180933952332
train gradient:  0.11762836362009556
iteration : 49
train acc:  0.7734375
train loss:  0.5138919353485107
train gradient:  0.13067748323845835
iteration : 50
train acc:  0.734375
train loss:  0.5026108622550964
train gradient:  0.11899776774351191
iteration : 51
train acc:  0.7734375
train loss:  0.5146709680557251
train gradient:  0.0940880575473929
iteration : 52
train acc:  0.796875
train loss:  0.4496653378009796
train gradient:  0.10069240230372707
iteration : 53
train acc:  0.78125
train loss:  0.4518037438392639
train gradient:  0.09414558210971631
iteration : 54
train acc:  0.71875
train loss:  0.4816368818283081
train gradient:  0.11041684717332202
iteration : 55
train acc:  0.75
train loss:  0.5579731464385986
train gradient:  0.11921657831957981
iteration : 56
train acc:  0.796875
train loss:  0.4129300117492676
train gradient:  0.08181419021495384
iteration : 57
train acc:  0.765625
train loss:  0.47876816987991333
train gradient:  0.11467402339349358
iteration : 58
train acc:  0.71875
train loss:  0.5223504900932312
train gradient:  0.1106534327073572
iteration : 59
train acc:  0.7890625
train loss:  0.4086652994155884
train gradient:  0.09003053399954317
iteration : 60
train acc:  0.7265625
train loss:  0.4803258180618286
train gradient:  0.124785982413393
iteration : 61
train acc:  0.734375
train loss:  0.5137605667114258
train gradient:  0.11217839869338413
iteration : 62
train acc:  0.765625
train loss:  0.4713675081729889
train gradient:  0.11443581703255021
iteration : 63
train acc:  0.703125
train loss:  0.5742181539535522
train gradient:  0.2067569894684606
iteration : 64
train acc:  0.75
train loss:  0.4909394085407257
train gradient:  0.12401145726452435
iteration : 65
train acc:  0.7421875
train loss:  0.4927285313606262
train gradient:  0.12352546182111819
iteration : 66
train acc:  0.75
train loss:  0.5059422254562378
train gradient:  0.14039062965044735
iteration : 67
train acc:  0.7109375
train loss:  0.5128782987594604
train gradient:  0.10485737649756482
iteration : 68
train acc:  0.7265625
train loss:  0.4789525866508484
train gradient:  0.10640844868558545
iteration : 69
train acc:  0.8046875
train loss:  0.4447234272956848
train gradient:  0.10305596440085467
iteration : 70
train acc:  0.7578125
train loss:  0.46914127469062805
train gradient:  0.1020446994220811
iteration : 71
train acc:  0.703125
train loss:  0.5287529230117798
train gradient:  0.15295467877093816
iteration : 72
train acc:  0.7890625
train loss:  0.44172823429107666
train gradient:  0.08114259056853529
iteration : 73
train acc:  0.734375
train loss:  0.5187439918518066
train gradient:  0.12912135119684626
iteration : 74
train acc:  0.7265625
train loss:  0.49277263879776
train gradient:  0.13447377256040804
iteration : 75
train acc:  0.765625
train loss:  0.4837111234664917
train gradient:  0.09274248743114237
iteration : 76
train acc:  0.7421875
train loss:  0.46804502606391907
train gradient:  0.09266572169150196
iteration : 77
train acc:  0.703125
train loss:  0.5038719177246094
train gradient:  0.1177410818723428
iteration : 78
train acc:  0.7265625
train loss:  0.4816473126411438
train gradient:  0.11554857065709619
iteration : 79
train acc:  0.71875
train loss:  0.49498602747917175
train gradient:  0.12470079919350485
iteration : 80
train acc:  0.7578125
train loss:  0.4643871784210205
train gradient:  0.10824323591847004
iteration : 81
train acc:  0.796875
train loss:  0.4285435080528259
train gradient:  0.08673320181270551
iteration : 82
train acc:  0.7734375
train loss:  0.46913594007492065
train gradient:  0.1043489906524655
iteration : 83
train acc:  0.765625
train loss:  0.46391332149505615
train gradient:  0.09877127780377036
iteration : 84
train acc:  0.6953125
train loss:  0.48971033096313477
train gradient:  0.12601326673243796
iteration : 85
train acc:  0.75
train loss:  0.46542757749557495
train gradient:  0.12026357005644527
iteration : 86
train acc:  0.7109375
train loss:  0.5323859453201294
train gradient:  0.11855200249904922
iteration : 87
train acc:  0.7109375
train loss:  0.5617063641548157
train gradient:  0.13383152607314552
iteration : 88
train acc:  0.7890625
train loss:  0.4348107874393463
train gradient:  0.11950139890369581
iteration : 89
train acc:  0.734375
train loss:  0.4310189485549927
train gradient:  0.07957999927302091
iteration : 90
train acc:  0.7734375
train loss:  0.5007006525993347
train gradient:  0.11061211367632949
iteration : 91
train acc:  0.7421875
train loss:  0.4717583954334259
train gradient:  0.09749626744207979
iteration : 92
train acc:  0.7421875
train loss:  0.4670032262802124
train gradient:  0.09686664428725329
iteration : 93
train acc:  0.6953125
train loss:  0.5037729740142822
train gradient:  0.12317007553518639
iteration : 94
train acc:  0.7890625
train loss:  0.4352908730506897
train gradient:  0.09713354258676081
iteration : 95
train acc:  0.796875
train loss:  0.4795822501182556
train gradient:  0.0980548913803225
iteration : 96
train acc:  0.734375
train loss:  0.5112629532814026
train gradient:  0.13757096596694737
iteration : 97
train acc:  0.7734375
train loss:  0.5012549161911011
train gradient:  0.1040692845623593
iteration : 98
train acc:  0.7734375
train loss:  0.47625046968460083
train gradient:  0.11133265800831266
iteration : 99
train acc:  0.6796875
train loss:  0.5207614898681641
train gradient:  0.10782479116117916
iteration : 100
train acc:  0.796875
train loss:  0.4454168975353241
train gradient:  0.10955049692094247
iteration : 101
train acc:  0.734375
train loss:  0.503508448600769
train gradient:  0.11957632959217669
iteration : 102
train acc:  0.7890625
train loss:  0.4252592921257019
train gradient:  0.08016431849414146
iteration : 103
train acc:  0.765625
train loss:  0.48941904306411743
train gradient:  0.11172815540212397
iteration : 104
train acc:  0.7734375
train loss:  0.4710339307785034
train gradient:  0.10691645243460897
iteration : 105
train acc:  0.734375
train loss:  0.5399185419082642
train gradient:  0.1532637354377204
iteration : 106
train acc:  0.71875
train loss:  0.5425795316696167
train gradient:  0.10681638287180364
iteration : 107
train acc:  0.7421875
train loss:  0.5354633331298828
train gradient:  0.13252674018683103
iteration : 108
train acc:  0.7890625
train loss:  0.4243077039718628
train gradient:  0.07403013490742745
iteration : 109
train acc:  0.8125
train loss:  0.4302259683609009
train gradient:  0.08835218662915886
iteration : 110
train acc:  0.765625
train loss:  0.47566449642181396
train gradient:  0.12015352476480326
iteration : 111
train acc:  0.65625
train loss:  0.5850497484207153
train gradient:  0.14427908711003912
iteration : 112
train acc:  0.6640625
train loss:  0.5426957011222839
train gradient:  0.13518110436917408
iteration : 113
train acc:  0.796875
train loss:  0.4338844120502472
train gradient:  0.08337117576084305
iteration : 114
train acc:  0.6953125
train loss:  0.5231457948684692
train gradient:  0.12890873269039851
iteration : 115
train acc:  0.703125
train loss:  0.5173002481460571
train gradient:  0.1466908750348795
iteration : 116
train acc:  0.7109375
train loss:  0.5144730806350708
train gradient:  0.11755472788656353
iteration : 117
train acc:  0.78125
train loss:  0.44812366366386414
train gradient:  0.11427599216029975
iteration : 118
train acc:  0.7890625
train loss:  0.4498744308948517
train gradient:  0.09936449779395391
iteration : 119
train acc:  0.7265625
train loss:  0.5211496353149414
train gradient:  0.11498730815185103
iteration : 120
train acc:  0.765625
train loss:  0.46120476722717285
train gradient:  0.10124707730605116
iteration : 121
train acc:  0.7890625
train loss:  0.47655123472213745
train gradient:  0.10247396880080555
iteration : 122
train acc:  0.734375
train loss:  0.4802117943763733
train gradient:  0.11703167301506012
iteration : 123
train acc:  0.7578125
train loss:  0.47996997833251953
train gradient:  0.11153791275956615
iteration : 124
train acc:  0.734375
train loss:  0.5001853108406067
train gradient:  0.1357638937116301
iteration : 125
train acc:  0.765625
train loss:  0.5363780856132507
train gradient:  0.15795811757454392
iteration : 126
train acc:  0.7734375
train loss:  0.4364885985851288
train gradient:  0.08545692591122943
iteration : 127
train acc:  0.7890625
train loss:  0.44527965784072876
train gradient:  0.10553353667245724
iteration : 128
train acc:  0.765625
train loss:  0.4489060342311859
train gradient:  0.10289940544005734
iteration : 129
train acc:  0.765625
train loss:  0.49634402990341187
train gradient:  0.12527044026409623
iteration : 130
train acc:  0.7734375
train loss:  0.4392580986022949
train gradient:  0.11640142430307336
iteration : 131
train acc:  0.8046875
train loss:  0.4240368604660034
train gradient:  0.07964917518264085
iteration : 132
train acc:  0.78125
train loss:  0.4591103196144104
train gradient:  0.11363272153096432
iteration : 133
train acc:  0.734375
train loss:  0.5388267040252686
train gradient:  0.13796037802908773
iteration : 134
train acc:  0.7734375
train loss:  0.4351206421852112
train gradient:  0.110031229535281
iteration : 135
train acc:  0.7265625
train loss:  0.5085352659225464
train gradient:  0.14881879097212977
iteration : 136
train acc:  0.75
train loss:  0.4702147841453552
train gradient:  0.10771764245303642
iteration : 137
train acc:  0.796875
train loss:  0.5006957054138184
train gradient:  0.15688538595355506
iteration : 138
train acc:  0.734375
train loss:  0.5195020437240601
train gradient:  0.1399092829033674
iteration : 139
train acc:  0.8125
train loss:  0.4228959083557129
train gradient:  0.09336363205102688
iteration : 140
train acc:  0.7890625
train loss:  0.45435455441474915
train gradient:  0.11297272808661311
iteration : 141
train acc:  0.7734375
train loss:  0.4637545645236969
train gradient:  0.1093707238517621
iteration : 142
train acc:  0.7421875
train loss:  0.49918270111083984
train gradient:  0.12989691768536205
iteration : 143
train acc:  0.7421875
train loss:  0.47161585092544556
train gradient:  0.10955687689052966
iteration : 144
train acc:  0.765625
train loss:  0.46321049332618713
train gradient:  0.11168709564745
iteration : 145
train acc:  0.734375
train loss:  0.493713915348053
train gradient:  0.13265626487640858
iteration : 146
train acc:  0.7421875
train loss:  0.4902769923210144
train gradient:  0.12980781873279848
iteration : 147
train acc:  0.71875
train loss:  0.46397262811660767
train gradient:  0.11590836709981076
iteration : 148
train acc:  0.7265625
train loss:  0.5119283199310303
train gradient:  0.10872546185225057
iteration : 149
train acc:  0.6875
train loss:  0.5473380088806152
train gradient:  0.17006171554981336
iteration : 150
train acc:  0.75
train loss:  0.4884088635444641
train gradient:  0.1494311345457111
iteration : 151
train acc:  0.75
train loss:  0.4829746186733246
train gradient:  0.11509862413866416
iteration : 152
train acc:  0.765625
train loss:  0.5077219009399414
train gradient:  0.12750101874093778
iteration : 153
train acc:  0.78125
train loss:  0.46491295099258423
train gradient:  0.1065885103075351
iteration : 154
train acc:  0.8203125
train loss:  0.41194015741348267
train gradient:  0.07656099235025632
iteration : 155
train acc:  0.734375
train loss:  0.510258674621582
train gradient:  0.14830788932201372
iteration : 156
train acc:  0.796875
train loss:  0.46921467781066895
train gradient:  0.13121256563944128
iteration : 157
train acc:  0.7421875
train loss:  0.474143385887146
train gradient:  0.1133901925751839
iteration : 158
train acc:  0.7578125
train loss:  0.4446369707584381
train gradient:  0.09865829508729704
iteration : 159
train acc:  0.7734375
train loss:  0.43799397349357605
train gradient:  0.08802360884952071
iteration : 160
train acc:  0.7265625
train loss:  0.4533809423446655
train gradient:  0.12195583199644791
iteration : 161
train acc:  0.7265625
train loss:  0.46049022674560547
train gradient:  0.09797954808951673
iteration : 162
train acc:  0.8046875
train loss:  0.39624014496803284
train gradient:  0.07436749751696611
iteration : 163
train acc:  0.765625
train loss:  0.4624177813529968
train gradient:  0.09464228026371573
iteration : 164
train acc:  0.75
train loss:  0.5124454498291016
train gradient:  0.12135101962690718
iteration : 165
train acc:  0.734375
train loss:  0.4801708161830902
train gradient:  0.09238565189505223
iteration : 166
train acc:  0.7578125
train loss:  0.4945649206638336
train gradient:  0.12050755939613671
iteration : 167
train acc:  0.7734375
train loss:  0.4907037913799286
train gradient:  0.08894843506619407
iteration : 168
train acc:  0.7734375
train loss:  0.4491191506385803
train gradient:  0.11771817721662549
iteration : 169
train acc:  0.765625
train loss:  0.44025230407714844
train gradient:  0.09613811206771283
iteration : 170
train acc:  0.7109375
train loss:  0.5504080653190613
train gradient:  0.1382918210196657
iteration : 171
train acc:  0.75
train loss:  0.4771471917629242
train gradient:  0.10429388914529354
iteration : 172
train acc:  0.734375
train loss:  0.48832499980926514
train gradient:  0.11793145935742944
iteration : 173
train acc:  0.765625
train loss:  0.46349239349365234
train gradient:  0.12796987340687663
iteration : 174
train acc:  0.84375
train loss:  0.40691542625427246
train gradient:  0.07569473552961142
iteration : 175
train acc:  0.75
train loss:  0.4537445604801178
train gradient:  0.13587870443003794
iteration : 176
train acc:  0.75
train loss:  0.5002365708351135
train gradient:  0.13802153659029578
iteration : 177
train acc:  0.7109375
train loss:  0.5011698007583618
train gradient:  0.1175717536443463
iteration : 178
train acc:  0.7734375
train loss:  0.4670864939689636
train gradient:  0.10925391997911132
iteration : 179
train acc:  0.7265625
train loss:  0.509638786315918
train gradient:  0.11368510363829958
iteration : 180
train acc:  0.71875
train loss:  0.5067425966262817
train gradient:  0.1558298943885037
iteration : 181
train acc:  0.765625
train loss:  0.45186930894851685
train gradient:  0.07800193058899423
iteration : 182
train acc:  0.734375
train loss:  0.5047049522399902
train gradient:  0.12513038080459893
iteration : 183
train acc:  0.8046875
train loss:  0.45143842697143555
train gradient:  0.11455498966838853
iteration : 184
train acc:  0.734375
train loss:  0.44005247950553894
train gradient:  0.0909560590941728
iteration : 185
train acc:  0.6953125
train loss:  0.49125632643699646
train gradient:  0.10858567877844036
iteration : 186
train acc:  0.765625
train loss:  0.45764559507369995
train gradient:  0.1018294464294456
iteration : 187
train acc:  0.765625
train loss:  0.5700111389160156
train gradient:  0.13171438715571024
iteration : 188
train acc:  0.765625
train loss:  0.49556124210357666
train gradient:  0.10054412286480818
iteration : 189
train acc:  0.78125
train loss:  0.431818425655365
train gradient:  0.11645927882617324
iteration : 190
train acc:  0.7265625
train loss:  0.5227212309837341
train gradient:  0.1346236710424022
iteration : 191
train acc:  0.75
train loss:  0.4792262315750122
train gradient:  0.11903679192538333
iteration : 192
train acc:  0.7421875
train loss:  0.4688500165939331
train gradient:  0.0929887219861903
iteration : 193
train acc:  0.7734375
train loss:  0.4725691080093384
train gradient:  0.09839829274636537
iteration : 194
train acc:  0.765625
train loss:  0.4663236141204834
train gradient:  0.11624009490021378
iteration : 195
train acc:  0.6953125
train loss:  0.5153237581253052
train gradient:  0.13646396473855532
iteration : 196
train acc:  0.734375
train loss:  0.4732363820075989
train gradient:  0.10764742629133715
iteration : 197
train acc:  0.6796875
train loss:  0.5291385054588318
train gradient:  0.12313688096379097
iteration : 198
train acc:  0.78125
train loss:  0.4398311972618103
train gradient:  0.09284050711174431
iteration : 199
train acc:  0.6875
train loss:  0.5473294258117676
train gradient:  0.13454081368932327
iteration : 200
train acc:  0.734375
train loss:  0.543988823890686
train gradient:  0.1282033358556055
iteration : 201
train acc:  0.8125
train loss:  0.41646286845207214
train gradient:  0.09649203120560206
iteration : 202
train acc:  0.7421875
train loss:  0.5063501596450806
train gradient:  0.12806571828539737
iteration : 203
train acc:  0.78125
train loss:  0.4649888873100281
train gradient:  0.15326699370476035
iteration : 204
train acc:  0.828125
train loss:  0.432554692029953
train gradient:  0.09426560670514406
iteration : 205
train acc:  0.7578125
train loss:  0.5417622327804565
train gradient:  0.13559253117705278
iteration : 206
train acc:  0.7109375
train loss:  0.5317224860191345
train gradient:  0.13713473109753477
iteration : 207
train acc:  0.7734375
train loss:  0.45913445949554443
train gradient:  0.11451243688149558
iteration : 208
train acc:  0.734375
train loss:  0.5381402969360352
train gradient:  0.2107212843986534
iteration : 209
train acc:  0.734375
train loss:  0.461559921503067
train gradient:  0.12373395944803926
iteration : 210
train acc:  0.7421875
train loss:  0.4858410954475403
train gradient:  0.13188039955240327
iteration : 211
train acc:  0.75
train loss:  0.49279436469078064
train gradient:  0.09678324462049952
iteration : 212
train acc:  0.734375
train loss:  0.4890340566635132
train gradient:  0.1155312847195213
iteration : 213
train acc:  0.765625
train loss:  0.4435192346572876
train gradient:  0.07094551989977035
iteration : 214
train acc:  0.765625
train loss:  0.46207180619239807
train gradient:  0.11839394968898108
iteration : 215
train acc:  0.765625
train loss:  0.4703129827976227
train gradient:  0.12586141433654754
iteration : 216
train acc:  0.765625
train loss:  0.43074706196784973
train gradient:  0.09509369878977732
iteration : 217
train acc:  0.703125
train loss:  0.49369290471076965
train gradient:  0.10217979817751527
iteration : 218
train acc:  0.8046875
train loss:  0.4687304198741913
train gradient:  0.09101426582394966
iteration : 219
train acc:  0.75
train loss:  0.5199160575866699
train gradient:  0.11333984268561832
iteration : 220
train acc:  0.75
train loss:  0.45834600925445557
train gradient:  0.09303520727897359
iteration : 221
train acc:  0.7578125
train loss:  0.4781211316585541
train gradient:  0.1359360408504009
iteration : 222
train acc:  0.7421875
train loss:  0.48391589522361755
train gradient:  0.1110311948972891
iteration : 223
train acc:  0.7890625
train loss:  0.4165932834148407
train gradient:  0.08022767350058004
iteration : 224
train acc:  0.7421875
train loss:  0.4779983162879944
train gradient:  0.10624816140266205
iteration : 225
train acc:  0.7578125
train loss:  0.4455256164073944
train gradient:  0.09604579426649337
iteration : 226
train acc:  0.71875
train loss:  0.5098012089729309
train gradient:  0.11577335010726
iteration : 227
train acc:  0.7109375
train loss:  0.5165829658508301
train gradient:  0.13032194769588784
iteration : 228
train acc:  0.7890625
train loss:  0.47453027963638306
train gradient:  0.10656605070130785
iteration : 229
train acc:  0.71875
train loss:  0.46711596846580505
train gradient:  0.09082039720557027
iteration : 230
train acc:  0.8125
train loss:  0.43330118060112
train gradient:  0.08915015302431155
iteration : 231
train acc:  0.78125
train loss:  0.4557860493659973
train gradient:  0.10293607493609463
iteration : 232
train acc:  0.828125
train loss:  0.4078975319862366
train gradient:  0.09804376180341708
iteration : 233
train acc:  0.7421875
train loss:  0.49411851167678833
train gradient:  0.10337160549868508
iteration : 234
train acc:  0.7109375
train loss:  0.502889096736908
train gradient:  0.15633425862616124
iteration : 235
train acc:  0.703125
train loss:  0.5497795343399048
train gradient:  0.14517408993863892
iteration : 236
train acc:  0.71875
train loss:  0.4988858997821808
train gradient:  0.14582133246195494
iteration : 237
train acc:  0.7734375
train loss:  0.4270675480365753
train gradient:  0.08377643144976302
iteration : 238
train acc:  0.78125
train loss:  0.4813633859157562
train gradient:  0.10700005457957015
iteration : 239
train acc:  0.78125
train loss:  0.44696247577667236
train gradient:  0.09596572529679573
iteration : 240
train acc:  0.7109375
train loss:  0.4995426833629608
train gradient:  0.11822258671824315
iteration : 241
train acc:  0.78125
train loss:  0.45921260118484497
train gradient:  0.10139100066068178
iteration : 242
train acc:  0.796875
train loss:  0.4070567488670349
train gradient:  0.08866678792187842
iteration : 243
train acc:  0.8515625
train loss:  0.4064106047153473
train gradient:  0.08711831443896327
iteration : 244
train acc:  0.7890625
train loss:  0.44889676570892334
train gradient:  0.08788079239275978
iteration : 245
train acc:  0.71875
train loss:  0.5053895711898804
train gradient:  0.11101354369258742
iteration : 246
train acc:  0.75
train loss:  0.4320944547653198
train gradient:  0.09031379687682355
iteration : 247
train acc:  0.7734375
train loss:  0.4448786973953247
train gradient:  0.08063413230735114
iteration : 248
train acc:  0.75
train loss:  0.4776763916015625
train gradient:  0.1044829745044735
iteration : 249
train acc:  0.7578125
train loss:  0.4743357002735138
train gradient:  0.08945715310640538
iteration : 250
train acc:  0.734375
train loss:  0.5141466856002808
train gradient:  0.11441126810247751
iteration : 251
train acc:  0.703125
train loss:  0.5725036263465881
train gradient:  0.1389562880887631
iteration : 252
train acc:  0.7890625
train loss:  0.4064246714115143
train gradient:  0.0891192866004931
iteration : 253
train acc:  0.734375
train loss:  0.43870311975479126
train gradient:  0.10249252428100693
iteration : 254
train acc:  0.78125
train loss:  0.42760467529296875
train gradient:  0.08220786371922166
iteration : 255
train acc:  0.78125
train loss:  0.45328420400619507
train gradient:  0.11088436670118672
iteration : 256
train acc:  0.796875
train loss:  0.4527013301849365
train gradient:  0.12498031219170348
iteration : 257
train acc:  0.78125
train loss:  0.47251108288764954
train gradient:  0.13194520531722165
iteration : 258
train acc:  0.7109375
train loss:  0.5635678768157959
train gradient:  0.1257630980618635
iteration : 259
train acc:  0.765625
train loss:  0.5442542433738708
train gradient:  0.16153486483775692
iteration : 260
train acc:  0.7265625
train loss:  0.5179048180580139
train gradient:  0.11169936156367205
iteration : 261
train acc:  0.71875
train loss:  0.4660031497478485
train gradient:  0.10882225616786145
iteration : 262
train acc:  0.734375
train loss:  0.45262306928634644
train gradient:  0.10504750401347869
iteration : 263
train acc:  0.78125
train loss:  0.4473629891872406
train gradient:  0.10959230504957511
iteration : 264
train acc:  0.765625
train loss:  0.4603344202041626
train gradient:  0.09720114658934544
iteration : 265
train acc:  0.75
train loss:  0.4726274609565735
train gradient:  0.12448394316227676
iteration : 266
train acc:  0.7578125
train loss:  0.44097989797592163
train gradient:  0.09082358143504747
iteration : 267
train acc:  0.7265625
train loss:  0.48981690406799316
train gradient:  0.11173135004943231
iteration : 268
train acc:  0.828125
train loss:  0.43655925989151
train gradient:  0.1331598709141236
iteration : 269
train acc:  0.7421875
train loss:  0.4511038661003113
train gradient:  0.10005415011822377
iteration : 270
train acc:  0.7265625
train loss:  0.480436235666275
train gradient:  0.10851112453340085
iteration : 271
train acc:  0.8203125
train loss:  0.42901402711868286
train gradient:  0.10179603877955036
iteration : 272
train acc:  0.6953125
train loss:  0.5152112245559692
train gradient:  0.12888666129984916
iteration : 273
train acc:  0.8046875
train loss:  0.42798474431037903
train gradient:  0.091531634364329
iteration : 274
train acc:  0.8515625
train loss:  0.36748117208480835
train gradient:  0.08587846739730529
iteration : 275
train acc:  0.6953125
train loss:  0.5583860874176025
train gradient:  0.15582639943193943
iteration : 276
train acc:  0.7265625
train loss:  0.45613574981689453
train gradient:  0.0988676067979384
iteration : 277
train acc:  0.703125
train loss:  0.49930787086486816
train gradient:  0.11082180230298494
iteration : 278
train acc:  0.703125
train loss:  0.5187180042266846
train gradient:  0.11540735305404096
iteration : 279
train acc:  0.75
train loss:  0.45130014419555664
train gradient:  0.10143486198666732
iteration : 280
train acc:  0.734375
train loss:  0.5339592695236206
train gradient:  0.11652863713855928
iteration : 281
train acc:  0.75
train loss:  0.49872392416000366
train gradient:  0.1342712041678577
iteration : 282
train acc:  0.765625
train loss:  0.4373449683189392
train gradient:  0.09584311739205019
iteration : 283
train acc:  0.7578125
train loss:  0.471635103225708
train gradient:  0.12720114687080408
iteration : 284
train acc:  0.6640625
train loss:  0.5763195753097534
train gradient:  0.14661239856075176
iteration : 285
train acc:  0.765625
train loss:  0.4691830277442932
train gradient:  0.11280155368336117
iteration : 286
train acc:  0.765625
train loss:  0.474062979221344
train gradient:  0.12546658104671027
iteration : 287
train acc:  0.7578125
train loss:  0.47351038455963135
train gradient:  0.11271467461190075
iteration : 288
train acc:  0.765625
train loss:  0.42903056740760803
train gradient:  0.08688120389243588
iteration : 289
train acc:  0.71875
train loss:  0.4736868143081665
train gradient:  0.09848477816393236
iteration : 290
train acc:  0.6875
train loss:  0.5552021265029907
train gradient:  0.14311163810699912
iteration : 291
train acc:  0.78125
train loss:  0.45897209644317627
train gradient:  0.11658767595907943
iteration : 292
train acc:  0.75
train loss:  0.44665324687957764
train gradient:  0.10763601361430214
iteration : 293
train acc:  0.7734375
train loss:  0.47197091579437256
train gradient:  0.10702019909094226
iteration : 294
train acc:  0.7734375
train loss:  0.45931702852249146
train gradient:  0.09603692381510621
iteration : 295
train acc:  0.734375
train loss:  0.4973081350326538
train gradient:  0.10902608255275056
iteration : 296
train acc:  0.7265625
train loss:  0.5134528279304504
train gradient:  0.13616873346261704
iteration : 297
train acc:  0.7734375
train loss:  0.4560362696647644
train gradient:  0.1209433828723562
iteration : 298
train acc:  0.7265625
train loss:  0.5364598035812378
train gradient:  0.116646952336569
iteration : 299
train acc:  0.796875
train loss:  0.42998382449150085
train gradient:  0.09989546692656251
iteration : 300
train acc:  0.7421875
train loss:  0.5130056142807007
train gradient:  0.1380526125862415
iteration : 301
train acc:  0.7421875
train loss:  0.5013812780380249
train gradient:  0.11864279554873743
iteration : 302
train acc:  0.75
train loss:  0.4599017798900604
train gradient:  0.08448729459885473
iteration : 303
train acc:  0.71875
train loss:  0.522342324256897
train gradient:  0.1232454150460294
iteration : 304
train acc:  0.671875
train loss:  0.4992729425430298
train gradient:  0.09954465231758068
iteration : 305
train acc:  0.7578125
train loss:  0.44632938504219055
train gradient:  0.1145203792610105
iteration : 306
train acc:  0.71875
train loss:  0.5126888751983643
train gradient:  0.13161571655125642
iteration : 307
train acc:  0.7265625
train loss:  0.4944959878921509
train gradient:  0.10813783127992886
iteration : 308
train acc:  0.7578125
train loss:  0.4881797432899475
train gradient:  0.11360965744131257
iteration : 309
train acc:  0.7890625
train loss:  0.44280683994293213
train gradient:  0.09515263694208398
iteration : 310
train acc:  0.734375
train loss:  0.5015661716461182
train gradient:  0.11386382914702864
iteration : 311
train acc:  0.78125
train loss:  0.4459276497364044
train gradient:  0.1119712055084523
iteration : 312
train acc:  0.75
train loss:  0.4963289797306061
train gradient:  0.12778209457191803
iteration : 313
train acc:  0.71875
train loss:  0.48561546206474304
train gradient:  0.11110231558648279
iteration : 314
train acc:  0.796875
train loss:  0.43195661902427673
train gradient:  0.09623862610420109
iteration : 315
train acc:  0.75
train loss:  0.5225235223770142
train gradient:  0.14992193266057555
iteration : 316
train acc:  0.71875
train loss:  0.5220631957054138
train gradient:  0.11272636890889813
iteration : 317
train acc:  0.75
train loss:  0.42218708992004395
train gradient:  0.09008171198508931
iteration : 318
train acc:  0.7109375
train loss:  0.5051102638244629
train gradient:  0.10698856073246953
iteration : 319
train acc:  0.7734375
train loss:  0.42803919315338135
train gradient:  0.08929253257103005
iteration : 320
train acc:  0.8125
train loss:  0.4137171506881714
train gradient:  0.09187004993154302
iteration : 321
train acc:  0.7265625
train loss:  0.46989402174949646
train gradient:  0.0982325476347863
iteration : 322
train acc:  0.765625
train loss:  0.4671388566493988
train gradient:  0.12743021410295502
iteration : 323
train acc:  0.734375
train loss:  0.4908556044101715
train gradient:  0.12539009125954842
iteration : 324
train acc:  0.71875
train loss:  0.5345582365989685
train gradient:  0.17394279763547674
iteration : 325
train acc:  0.7265625
train loss:  0.5069848895072937
train gradient:  0.1259051377003871
iteration : 326
train acc:  0.6640625
train loss:  0.5013137459754944
train gradient:  0.1340732550588627
iteration : 327
train acc:  0.703125
train loss:  0.47802281379699707
train gradient:  0.10407963987221572
iteration : 328
train acc:  0.7578125
train loss:  0.5120851993560791
train gradient:  0.1320053839147225
iteration : 329
train acc:  0.7578125
train loss:  0.45275065302848816
train gradient:  0.11981952915289426
iteration : 330
train acc:  0.8046875
train loss:  0.44416627287864685
train gradient:  0.09146102782933589
iteration : 331
train acc:  0.703125
train loss:  0.5302720069885254
train gradient:  0.12961153727044233
iteration : 332
train acc:  0.7421875
train loss:  0.4910529851913452
train gradient:  0.11187881748496811
iteration : 333
train acc:  0.8046875
train loss:  0.39080336689949036
train gradient:  0.09530852561778651
iteration : 334
train acc:  0.7890625
train loss:  0.4279455542564392
train gradient:  0.1028187738916876
iteration : 335
train acc:  0.7109375
train loss:  0.4874635338783264
train gradient:  0.12980647432292985
iteration : 336
train acc:  0.703125
train loss:  0.5093404650688171
train gradient:  0.16089684921118458
iteration : 337
train acc:  0.7265625
train loss:  0.5131708383560181
train gradient:  0.1408379888963946
iteration : 338
train acc:  0.734375
train loss:  0.5212067365646362
train gradient:  0.10785606646691079
iteration : 339
train acc:  0.7421875
train loss:  0.5516347289085388
train gradient:  0.16921452361893066
iteration : 340
train acc:  0.7734375
train loss:  0.4309118688106537
train gradient:  0.10660115985045203
iteration : 341
train acc:  0.7578125
train loss:  0.4498290717601776
train gradient:  0.0830414186677891
iteration : 342
train acc:  0.7734375
train loss:  0.4569176435470581
train gradient:  0.10558285973722513
iteration : 343
train acc:  0.71875
train loss:  0.5275219082832336
train gradient:  0.12138059730467395
iteration : 344
train acc:  0.7421875
train loss:  0.5327482223510742
train gradient:  0.13967338889158276
iteration : 345
train acc:  0.7109375
train loss:  0.519955039024353
train gradient:  0.1182895050888094
iteration : 346
train acc:  0.671875
train loss:  0.546826958656311
train gradient:  0.17332283525444436
iteration : 347
train acc:  0.7265625
train loss:  0.4756069481372833
train gradient:  0.10814796495867297
iteration : 348
train acc:  0.8203125
train loss:  0.40360552072525024
train gradient:  0.08887015301727103
iteration : 349
train acc:  0.7421875
train loss:  0.46530088782310486
train gradient:  0.10015874076028777
iteration : 350
train acc:  0.78125
train loss:  0.505167543888092
train gradient:  0.13275786092769137
iteration : 351
train acc:  0.71875
train loss:  0.4636343717575073
train gradient:  0.10125761722342223
iteration : 352
train acc:  0.71875
train loss:  0.486854612827301
train gradient:  0.09579163698489071
iteration : 353
train acc:  0.71875
train loss:  0.5759600400924683
train gradient:  0.18311169450229903
iteration : 354
train acc:  0.78125
train loss:  0.4164637327194214
train gradient:  0.106584031246483
iteration : 355
train acc:  0.7265625
train loss:  0.5029338598251343
train gradient:  0.12443103618414819
iteration : 356
train acc:  0.734375
train loss:  0.44581007957458496
train gradient:  0.12375433795010617
iteration : 357
train acc:  0.796875
train loss:  0.44751447439193726
train gradient:  0.09528656121764828
iteration : 358
train acc:  0.7109375
train loss:  0.5152927041053772
train gradient:  0.1606494747682718
iteration : 359
train acc:  0.71875
train loss:  0.500941276550293
train gradient:  0.12669486842412458
iteration : 360
train acc:  0.7734375
train loss:  0.4820479154586792
train gradient:  0.10342027326909023
iteration : 361
train acc:  0.703125
train loss:  0.5732731223106384
train gradient:  0.15183085369713428
iteration : 362
train acc:  0.765625
train loss:  0.4902329742908478
train gradient:  0.12144639021194129
iteration : 363
train acc:  0.703125
train loss:  0.515398383140564
train gradient:  0.1188617295471156
iteration : 364
train acc:  0.7109375
train loss:  0.53766930103302
train gradient:  0.12695226607962523
iteration : 365
train acc:  0.71875
train loss:  0.5001398921012878
train gradient:  0.11696756103523874
iteration : 366
train acc:  0.7890625
train loss:  0.4323732256889343
train gradient:  0.08988124118282628
iteration : 367
train acc:  0.7109375
train loss:  0.5155396461486816
train gradient:  0.11121303667178846
iteration : 368
train acc:  0.78125
train loss:  0.43434810638427734
train gradient:  0.10015223518381612
iteration : 369
train acc:  0.859375
train loss:  0.4227103590965271
train gradient:  0.09401281395059415
iteration : 370
train acc:  0.8046875
train loss:  0.45019519329071045
train gradient:  0.10085759986949198
iteration : 371
train acc:  0.75
train loss:  0.46639445424079895
train gradient:  0.09651294357559476
iteration : 372
train acc:  0.75
train loss:  0.5042653679847717
train gradient:  0.11570826993056603
iteration : 373
train acc:  0.703125
train loss:  0.5453178882598877
train gradient:  0.15013827524615803
iteration : 374
train acc:  0.75
train loss:  0.45664769411087036
train gradient:  0.10603887810981283
iteration : 375
train acc:  0.765625
train loss:  0.4562053680419922
train gradient:  0.11789565787231161
iteration : 376
train acc:  0.71875
train loss:  0.5163516402244568
train gradient:  0.13805679797141648
iteration : 377
train acc:  0.75
train loss:  0.541235625743866
train gradient:  0.13755377674041833
iteration : 378
train acc:  0.7109375
train loss:  0.5053712725639343
train gradient:  0.10174743507440129
iteration : 379
train acc:  0.765625
train loss:  0.5399609208106995
train gradient:  0.13594208709883854
iteration : 380
train acc:  0.7734375
train loss:  0.49551457166671753
train gradient:  0.11029567714020515
iteration : 381
train acc:  0.7890625
train loss:  0.454205721616745
train gradient:  0.09134778491449115
iteration : 382
train acc:  0.7578125
train loss:  0.4641401171684265
train gradient:  0.12157511806047397
iteration : 383
train acc:  0.703125
train loss:  0.5330142378807068
train gradient:  0.13621761236354402
iteration : 384
train acc:  0.671875
train loss:  0.5343414545059204
train gradient:  0.15977173347120532
iteration : 385
train acc:  0.75
train loss:  0.44542038440704346
train gradient:  0.10145053895325058
iteration : 386
train acc:  0.7578125
train loss:  0.4365611672401428
train gradient:  0.09055071871933813
iteration : 387
train acc:  0.71875
train loss:  0.470378577709198
train gradient:  0.09865275807374448
iteration : 388
train acc:  0.71875
train loss:  0.5297271013259888
train gradient:  0.14403726571030903
iteration : 389
train acc:  0.78125
train loss:  0.46616244316101074
train gradient:  0.0986546749158996
iteration : 390
train acc:  0.765625
train loss:  0.4499938488006592
train gradient:  0.09945467573119329
iteration : 391
train acc:  0.78125
train loss:  0.42559149861335754
train gradient:  0.07410802320144942
iteration : 392
train acc:  0.75
train loss:  0.46120181679725647
train gradient:  0.10167507629573758
iteration : 393
train acc:  0.6875
train loss:  0.5554488897323608
train gradient:  0.1574752731081205
iteration : 394
train acc:  0.671875
train loss:  0.6175431609153748
train gradient:  0.1548175798528562
iteration : 395
train acc:  0.7109375
train loss:  0.515825629234314
train gradient:  0.1266117826912376
iteration : 396
train acc:  0.765625
train loss:  0.4710092544555664
train gradient:  0.1057576716048359
iteration : 397
train acc:  0.71875
train loss:  0.50858074426651
train gradient:  0.14323081830953047
iteration : 398
train acc:  0.765625
train loss:  0.4487350583076477
train gradient:  0.090843197589515
iteration : 399
train acc:  0.7109375
train loss:  0.46426278352737427
train gradient:  0.10057952048443615
iteration : 400
train acc:  0.7265625
train loss:  0.4786732792854309
train gradient:  0.09559107185330187
iteration : 401
train acc:  0.71875
train loss:  0.4966636598110199
train gradient:  0.10639959919264745
iteration : 402
train acc:  0.7578125
train loss:  0.4772728979587555
train gradient:  0.13279641465673941
iteration : 403
train acc:  0.71875
train loss:  0.5190544128417969
train gradient:  0.12526072839479788
iteration : 404
train acc:  0.7578125
train loss:  0.5122991800308228
train gradient:  0.14090431469762604
iteration : 405
train acc:  0.78125
train loss:  0.4466615915298462
train gradient:  0.10877520696255577
iteration : 406
train acc:  0.71875
train loss:  0.5216525793075562
train gradient:  0.11522165823432207
iteration : 407
train acc:  0.7734375
train loss:  0.4958042502403259
train gradient:  0.1557033314789918
iteration : 408
train acc:  0.7578125
train loss:  0.4745604395866394
train gradient:  0.10371237477209758
iteration : 409
train acc:  0.71875
train loss:  0.5109801292419434
train gradient:  0.1045631951782541
iteration : 410
train acc:  0.734375
train loss:  0.4840303659439087
train gradient:  0.12045214545034326
iteration : 411
train acc:  0.7578125
train loss:  0.46968233585357666
train gradient:  0.10953568367956709
iteration : 412
train acc:  0.7109375
train loss:  0.5328618884086609
train gradient:  0.10637709245215748
iteration : 413
train acc:  0.7265625
train loss:  0.4533778727054596
train gradient:  0.09565104641955176
iteration : 414
train acc:  0.734375
train loss:  0.5411827564239502
train gradient:  0.14373813524060858
iteration : 415
train acc:  0.7734375
train loss:  0.46789807081222534
train gradient:  0.10448808587905749
iteration : 416
train acc:  0.7109375
train loss:  0.5140880942344666
train gradient:  0.11186221430884295
iteration : 417
train acc:  0.765625
train loss:  0.47999006509780884
train gradient:  0.10702993631046483
iteration : 418
train acc:  0.75
train loss:  0.5025100111961365
train gradient:  0.12769418391102982
iteration : 419
train acc:  0.7734375
train loss:  0.4290412664413452
train gradient:  0.07807770755972249
iteration : 420
train acc:  0.7265625
train loss:  0.49142688512802124
train gradient:  0.11474705665773584
iteration : 421
train acc:  0.8046875
train loss:  0.3975900709629059
train gradient:  0.06758926130539336
iteration : 422
train acc:  0.765625
train loss:  0.47655972838401794
train gradient:  0.09469778386853017
iteration : 423
train acc:  0.703125
train loss:  0.5179738998413086
train gradient:  0.12297733878555563
iteration : 424
train acc:  0.765625
train loss:  0.5086513757705688
train gradient:  0.146922321222307
iteration : 425
train acc:  0.6953125
train loss:  0.5446012020111084
train gradient:  0.12448207617354609
iteration : 426
train acc:  0.75
train loss:  0.4640325903892517
train gradient:  0.09262021332884138
iteration : 427
train acc:  0.859375
train loss:  0.384626567363739
train gradient:  0.08746115728998877
iteration : 428
train acc:  0.7578125
train loss:  0.46909695863723755
train gradient:  0.11336518248986145
iteration : 429
train acc:  0.78125
train loss:  0.437488317489624
train gradient:  0.08486569936583602
iteration : 430
train acc:  0.71875
train loss:  0.4905889332294464
train gradient:  0.09681853073312056
iteration : 431
train acc:  0.8671875
train loss:  0.39501750469207764
train gradient:  0.07579128588671229
iteration : 432
train acc:  0.75
train loss:  0.46633756160736084
train gradient:  0.10768760035603656
iteration : 433
train acc:  0.7734375
train loss:  0.4829597473144531
train gradient:  0.08210144175237233
iteration : 434
train acc:  0.78125
train loss:  0.4321542978286743
train gradient:  0.09261141842155272
iteration : 435
train acc:  0.7734375
train loss:  0.41492125391960144
train gradient:  0.08756240028569598
iteration : 436
train acc:  0.7578125
train loss:  0.4478908181190491
train gradient:  0.09995124905194584
iteration : 437
train acc:  0.7578125
train loss:  0.452178955078125
train gradient:  0.08921836217931452
iteration : 438
train acc:  0.8203125
train loss:  0.3758210837841034
train gradient:  0.08968754774200043
iteration : 439
train acc:  0.734375
train loss:  0.5415371060371399
train gradient:  0.13617363517460881
iteration : 440
train acc:  0.703125
train loss:  0.48480263352394104
train gradient:  0.11155916870412758
iteration : 441
train acc:  0.7734375
train loss:  0.4851270914077759
train gradient:  0.11733258397201723
iteration : 442
train acc:  0.765625
train loss:  0.42168867588043213
train gradient:  0.12418793166378748
iteration : 443
train acc:  0.8203125
train loss:  0.4055280089378357
train gradient:  0.09140149102727611
iteration : 444
train acc:  0.7890625
train loss:  0.4477307200431824
train gradient:  0.11229272209159967
iteration : 445
train acc:  0.7109375
train loss:  0.50584876537323
train gradient:  0.12548346370299335
iteration : 446
train acc:  0.7734375
train loss:  0.4305254817008972
train gradient:  0.0805239947961551
iteration : 447
train acc:  0.703125
train loss:  0.5506277084350586
train gradient:  0.13058519330845958
iteration : 448
train acc:  0.765625
train loss:  0.4424460828304291
train gradient:  0.08306034752823066
iteration : 449
train acc:  0.7109375
train loss:  0.5629584789276123
train gradient:  0.14703092304279647
iteration : 450
train acc:  0.71875
train loss:  0.4777180552482605
train gradient:  0.11964220082653192
iteration : 451
train acc:  0.7109375
train loss:  0.5012113451957703
train gradient:  0.11625034425745344
iteration : 452
train acc:  0.828125
train loss:  0.3922134041786194
train gradient:  0.08054271410078798
iteration : 453
train acc:  0.6875
train loss:  0.5186148881912231
train gradient:  0.1366368795337803
iteration : 454
train acc:  0.7890625
train loss:  0.4321976900100708
train gradient:  0.10630450466047477
iteration : 455
train acc:  0.7109375
train loss:  0.5483002066612244
train gradient:  0.1335018396748381
iteration : 456
train acc:  0.6875
train loss:  0.5368024706840515
train gradient:  0.17365174977120695
iteration : 457
train acc:  0.796875
train loss:  0.45053017139434814
train gradient:  0.13599336713837984
iteration : 458
train acc:  0.7421875
train loss:  0.4872145652770996
train gradient:  0.10219667183070111
iteration : 459
train acc:  0.7578125
train loss:  0.4520653188228607
train gradient:  0.09546278752267434
iteration : 460
train acc:  0.8125
train loss:  0.4462655186653137
train gradient:  0.10591278837287
iteration : 461
train acc:  0.78125
train loss:  0.4516012370586395
train gradient:  0.10054470921289904
iteration : 462
train acc:  0.8046875
train loss:  0.4362278878688812
train gradient:  0.09577231503565005
iteration : 463
train acc:  0.828125
train loss:  0.43725359439849854
train gradient:  0.08000959587224613
iteration : 464
train acc:  0.6796875
train loss:  0.5673471093177795
train gradient:  0.1298637073860138
iteration : 465
train acc:  0.7421875
train loss:  0.47288084030151367
train gradient:  0.13772030359196985
iteration : 466
train acc:  0.75
train loss:  0.4411025643348694
train gradient:  0.10742904813990388
iteration : 467
train acc:  0.7421875
train loss:  0.5520380139350891
train gradient:  0.2139744228175816
iteration : 468
train acc:  0.7109375
train loss:  0.49910256266593933
train gradient:  0.11221327511035757
iteration : 469
train acc:  0.7578125
train loss:  0.4615262448787689
train gradient:  0.09121957770134337
iteration : 470
train acc:  0.78125
train loss:  0.44997262954711914
train gradient:  0.09645254050779237
iteration : 471
train acc:  0.71875
train loss:  0.514815092086792
train gradient:  0.1358572020430073
iteration : 472
train acc:  0.7578125
train loss:  0.4655013084411621
train gradient:  0.08440987994911761
iteration : 473
train acc:  0.7578125
train loss:  0.45629802346229553
train gradient:  0.10739426458902038
iteration : 474
train acc:  0.71875
train loss:  0.504785418510437
train gradient:  0.10247599102768337
iteration : 475
train acc:  0.78125
train loss:  0.45893973112106323
train gradient:  0.10806483174071362
iteration : 476
train acc:  0.75
train loss:  0.4581376612186432
train gradient:  0.09143221553940314
iteration : 477
train acc:  0.8046875
train loss:  0.4128735363483429
train gradient:  0.0955067554708279
iteration : 478
train acc:  0.7421875
train loss:  0.4918574392795563
train gradient:  0.10871262204090222
iteration : 479
train acc:  0.7578125
train loss:  0.49433988332748413
train gradient:  0.13771985801378867
iteration : 480
train acc:  0.71875
train loss:  0.46315884590148926
train gradient:  0.10315569425021294
iteration : 481
train acc:  0.78125
train loss:  0.4471699595451355
train gradient:  0.09528246375097466
iteration : 482
train acc:  0.734375
train loss:  0.44527101516723633
train gradient:  0.10373462994916485
iteration : 483
train acc:  0.7421875
train loss:  0.5333462357521057
train gradient:  0.12160934204061401
iteration : 484
train acc:  0.7578125
train loss:  0.4752931296825409
train gradient:  0.10118799475231403
iteration : 485
train acc:  0.78125
train loss:  0.462784081697464
train gradient:  0.13378726319923978
iteration : 486
train acc:  0.6953125
train loss:  0.5556132793426514
train gradient:  0.14796108033792874
iteration : 487
train acc:  0.78125
train loss:  0.41137444972991943
train gradient:  0.09107875920128261
iteration : 488
train acc:  0.78125
train loss:  0.45887690782546997
train gradient:  0.1200837833468232
iteration : 489
train acc:  0.796875
train loss:  0.4756045937538147
train gradient:  0.11731696215234977
iteration : 490
train acc:  0.71875
train loss:  0.531112551689148
train gradient:  0.1684215262134971
iteration : 491
train acc:  0.7890625
train loss:  0.42232656478881836
train gradient:  0.106029691137276
iteration : 492
train acc:  0.7734375
train loss:  0.43816304206848145
train gradient:  0.09224436578205744
iteration : 493
train acc:  0.7265625
train loss:  0.4870273768901825
train gradient:  0.11796516855133224
iteration : 494
train acc:  0.734375
train loss:  0.4895203709602356
train gradient:  0.11483175283977584
iteration : 495
train acc:  0.7265625
train loss:  0.5350800156593323
train gradient:  0.13960875403614004
iteration : 496
train acc:  0.7421875
train loss:  0.5535653829574585
train gradient:  0.1680410530815873
iteration : 497
train acc:  0.6953125
train loss:  0.5110276937484741
train gradient:  0.12930469176234308
iteration : 498
train acc:  0.78125
train loss:  0.4659370183944702
train gradient:  0.10185112405068719
iteration : 499
train acc:  0.78125
train loss:  0.46598654985427856
train gradient:  0.11285021463098444
iteration : 500
train acc:  0.7421875
train loss:  0.47257500886917114
train gradient:  0.11636723569986954
iteration : 501
train acc:  0.796875
train loss:  0.4571361541748047
train gradient:  0.11035634929546392
iteration : 502
train acc:  0.7421875
train loss:  0.5448113679885864
train gradient:  0.14185755004110562
iteration : 503
train acc:  0.7109375
train loss:  0.5162677764892578
train gradient:  0.1222634775979599
iteration : 504
train acc:  0.75
train loss:  0.47791725397109985
train gradient:  0.15477297352938812
iteration : 505
train acc:  0.7421875
train loss:  0.47159120440483093
train gradient:  0.11446300894213525
iteration : 506
train acc:  0.7109375
train loss:  0.5195653438568115
train gradient:  0.13621840699535442
iteration : 507
train acc:  0.8125
train loss:  0.3929045498371124
train gradient:  0.07024982237033234
iteration : 508
train acc:  0.765625
train loss:  0.4276478588581085
train gradient:  0.09706702608216108
iteration : 509
train acc:  0.6796875
train loss:  0.5496804118156433
train gradient:  0.14573893144876257
iteration : 510
train acc:  0.765625
train loss:  0.4562605023384094
train gradient:  0.10076019923507692
iteration : 511
train acc:  0.8125
train loss:  0.4811912775039673
train gradient:  0.13626851241371196
iteration : 512
train acc:  0.7265625
train loss:  0.5085893869400024
train gradient:  0.10954413854545691
iteration : 513
train acc:  0.7734375
train loss:  0.4139084815979004
train gradient:  0.07920262773959386
iteration : 514
train acc:  0.7109375
train loss:  0.5595515966415405
train gradient:  0.14830666948604576
iteration : 515
train acc:  0.703125
train loss:  0.620997965335846
train gradient:  0.1621212294718795
iteration : 516
train acc:  0.734375
train loss:  0.5494258999824524
train gradient:  0.1647876235422424
iteration : 517
train acc:  0.7109375
train loss:  0.5373287796974182
train gradient:  0.14589535953616684
iteration : 518
train acc:  0.7109375
train loss:  0.4971488118171692
train gradient:  0.12040261639264999
iteration : 519
train acc:  0.78125
train loss:  0.42029812932014465
train gradient:  0.0940054524048405
iteration : 520
train acc:  0.75
train loss:  0.4479176998138428
train gradient:  0.09370643280410787
iteration : 521
train acc:  0.8046875
train loss:  0.4076201021671295
train gradient:  0.14407782621917253
iteration : 522
train acc:  0.765625
train loss:  0.4797627329826355
train gradient:  0.11868046516909794
iteration : 523
train acc:  0.7890625
train loss:  0.4195193350315094
train gradient:  0.07309548992784151
iteration : 524
train acc:  0.71875
train loss:  0.45652931928634644
train gradient:  0.08820544083136395
iteration : 525
train acc:  0.75
train loss:  0.4898880422115326
train gradient:  0.13442162347446207
iteration : 526
train acc:  0.75
train loss:  0.42830580472946167
train gradient:  0.09364086473604709
iteration : 527
train acc:  0.671875
train loss:  0.569058895111084
train gradient:  0.16687615761094865
iteration : 528
train acc:  0.734375
train loss:  0.5105617046356201
train gradient:  0.11591688272607589
iteration : 529
train acc:  0.7734375
train loss:  0.4993707835674286
train gradient:  0.11415349527594389
iteration : 530
train acc:  0.7578125
train loss:  0.510962963104248
train gradient:  0.1351186814059785
iteration : 531
train acc:  0.796875
train loss:  0.4373633563518524
train gradient:  0.08676891291197045
iteration : 532
train acc:  0.7890625
train loss:  0.48036980628967285
train gradient:  0.12044971017828879
iteration : 533
train acc:  0.7421875
train loss:  0.47388091683387756
train gradient:  0.1039530174395305
iteration : 534
train acc:  0.7734375
train loss:  0.4633524417877197
train gradient:  0.12660319610248688
iteration : 535
train acc:  0.7109375
train loss:  0.46722614765167236
train gradient:  0.11896755445326351
iteration : 536
train acc:  0.7734375
train loss:  0.46266189217567444
train gradient:  0.08206996916847902
iteration : 537
train acc:  0.7734375
train loss:  0.4434416592121124
train gradient:  0.11122156155293349
iteration : 538
train acc:  0.7734375
train loss:  0.49214527010917664
train gradient:  0.1201265483126994
iteration : 539
train acc:  0.8046875
train loss:  0.4090704917907715
train gradient:  0.08457883945385572
iteration : 540
train acc:  0.71875
train loss:  0.49428144097328186
train gradient:  0.13549697515423398
iteration : 541
train acc:  0.6953125
train loss:  0.5536279678344727
train gradient:  0.14587816374356982
iteration : 542
train acc:  0.734375
train loss:  0.47308748960494995
train gradient:  0.09311854103792985
iteration : 543
train acc:  0.765625
train loss:  0.4198841154575348
train gradient:  0.09273626042858732
iteration : 544
train acc:  0.7421875
train loss:  0.4538816809654236
train gradient:  0.1070907425538555
iteration : 545
train acc:  0.75
train loss:  0.5021384954452515
train gradient:  0.10863861347293471
iteration : 546
train acc:  0.8046875
train loss:  0.39139753580093384
train gradient:  0.09246330650491583
iteration : 547
train acc:  0.7421875
train loss:  0.4363100528717041
train gradient:  0.0792819823193538
iteration : 548
train acc:  0.7421875
train loss:  0.4959768056869507
train gradient:  0.1378451684935872
iteration : 549
train acc:  0.765625
train loss:  0.495259165763855
train gradient:  0.11074703744365152
iteration : 550
train acc:  0.7109375
train loss:  0.5722566246986389
train gradient:  0.1374944677053803
iteration : 551
train acc:  0.75
train loss:  0.4555095434188843
train gradient:  0.09633888397203708
iteration : 552
train acc:  0.7265625
train loss:  0.5070273876190186
train gradient:  0.10602223392499403
iteration : 553
train acc:  0.6875
train loss:  0.546172022819519
train gradient:  0.1355249317432855
iteration : 554
train acc:  0.7734375
train loss:  0.4949369430541992
train gradient:  0.12060715889171032
iteration : 555
train acc:  0.71875
train loss:  0.5408905744552612
train gradient:  0.14400353677600702
iteration : 556
train acc:  0.75
train loss:  0.47191208600997925
train gradient:  0.09828755113082287
iteration : 557
train acc:  0.71875
train loss:  0.4937590956687927
train gradient:  0.12510609949628906
iteration : 558
train acc:  0.765625
train loss:  0.42979228496551514
train gradient:  0.08161282632288686
iteration : 559
train acc:  0.765625
train loss:  0.4133005142211914
train gradient:  0.09077004260237242
iteration : 560
train acc:  0.8359375
train loss:  0.3826786279678345
train gradient:  0.09287447558165518
iteration : 561
train acc:  0.71875
train loss:  0.4910920560359955
train gradient:  0.11501618242217052
iteration : 562
train acc:  0.7265625
train loss:  0.5406582951545715
train gradient:  0.14944942110274906
iteration : 563
train acc:  0.796875
train loss:  0.45673948526382446
train gradient:  0.11640148565704879
iteration : 564
train acc:  0.765625
train loss:  0.44483715295791626
train gradient:  0.08616814900070922
iteration : 565
train acc:  0.7734375
train loss:  0.4551061987876892
train gradient:  0.14076624624898698
iteration : 566
train acc:  0.71875
train loss:  0.5261691808700562
train gradient:  0.15949139207302315
iteration : 567
train acc:  0.7578125
train loss:  0.46471402049064636
train gradient:  0.11965923237266074
iteration : 568
train acc:  0.6875
train loss:  0.5290858745574951
train gradient:  0.14012821082067947
iteration : 569
train acc:  0.734375
train loss:  0.47589951753616333
train gradient:  0.10523772428164754
iteration : 570
train acc:  0.703125
train loss:  0.5355457067489624
train gradient:  0.1362204069171578
iteration : 571
train acc:  0.671875
train loss:  0.584564745426178
train gradient:  0.16410967777235247
iteration : 572
train acc:  0.8046875
train loss:  0.44392824172973633
train gradient:  0.12305648528060001
iteration : 573
train acc:  0.7890625
train loss:  0.404732346534729
train gradient:  0.09014678213524563
iteration : 574
train acc:  0.78125
train loss:  0.477533221244812
train gradient:  0.12823557546676223
iteration : 575
train acc:  0.7734375
train loss:  0.49160099029541016
train gradient:  0.12726915633019592
iteration : 576
train acc:  0.796875
train loss:  0.4623146057128906
train gradient:  0.11599141849019624
iteration : 577
train acc:  0.7578125
train loss:  0.49310654401779175
train gradient:  0.10240310266152959
iteration : 578
train acc:  0.8046875
train loss:  0.4677415192127228
train gradient:  0.11193598945607262
iteration : 579
train acc:  0.78125
train loss:  0.44281503558158875
train gradient:  0.1169627919770647
iteration : 580
train acc:  0.7265625
train loss:  0.4873940944671631
train gradient:  0.14061881984820823
iteration : 581
train acc:  0.765625
train loss:  0.469463586807251
train gradient:  0.0963161054759195
iteration : 582
train acc:  0.671875
train loss:  0.5914493203163147
train gradient:  0.17400988123648584
iteration : 583
train acc:  0.7578125
train loss:  0.47126486897468567
train gradient:  0.12550583879639374
iteration : 584
train acc:  0.671875
train loss:  0.5254196524620056
train gradient:  0.15319840634988918
iteration : 585
train acc:  0.7578125
train loss:  0.49071091413497925
train gradient:  0.10958839760775928
iteration : 586
train acc:  0.6484375
train loss:  0.6048952341079712
train gradient:  0.16837701700286545
iteration : 587
train acc:  0.6796875
train loss:  0.5845786333084106
train gradient:  0.13960909694446344
iteration : 588
train acc:  0.7890625
train loss:  0.43837499618530273
train gradient:  0.11249420808400916
iteration : 589
train acc:  0.7109375
train loss:  0.47693145275115967
train gradient:  0.12778841994825815
iteration : 590
train acc:  0.7578125
train loss:  0.41019773483276367
train gradient:  0.10886397372259778
iteration : 591
train acc:  0.8046875
train loss:  0.42913252115249634
train gradient:  0.09924647656024178
iteration : 592
train acc:  0.7734375
train loss:  0.4614288806915283
train gradient:  0.09571949226225046
iteration : 593
train acc:  0.6953125
train loss:  0.5679979920387268
train gradient:  0.21026492088795792
iteration : 594
train acc:  0.7734375
train loss:  0.5192552804946899
train gradient:  0.13276910991890303
iteration : 595
train acc:  0.7421875
train loss:  0.532303512096405
train gradient:  0.1507010994758336
iteration : 596
train acc:  0.765625
train loss:  0.4771304130554199
train gradient:  0.0916098830048013
iteration : 597
train acc:  0.71875
train loss:  0.5216474533081055
train gradient:  0.14814953959770544
iteration : 598
train acc:  0.6875
train loss:  0.5799698829650879
train gradient:  0.1413269042353241
iteration : 599
train acc:  0.75
train loss:  0.44147592782974243
train gradient:  0.1006253079941337
iteration : 600
train acc:  0.78125
train loss:  0.43607228994369507
train gradient:  0.09040579485491135
iteration : 601
train acc:  0.78125
train loss:  0.4534674286842346
train gradient:  0.10374108536514812
iteration : 602
train acc:  0.7421875
train loss:  0.4875483512878418
train gradient:  0.12239695680793665
iteration : 603
train acc:  0.765625
train loss:  0.44251304864883423
train gradient:  0.12069530991837069
iteration : 604
train acc:  0.7578125
train loss:  0.5213830471038818
train gradient:  0.11988107557912937
iteration : 605
train acc:  0.7421875
train loss:  0.4924449622631073
train gradient:  0.14154725928844814
iteration : 606
train acc:  0.75
train loss:  0.4822791516780853
train gradient:  0.12485355298971824
iteration : 607
train acc:  0.7421875
train loss:  0.5077463388442993
train gradient:  0.13202286219417014
iteration : 608
train acc:  0.703125
train loss:  0.5111464858055115
train gradient:  0.1120183733205057
iteration : 609
train acc:  0.71875
train loss:  0.4811843931674957
train gradient:  0.11476061428082845
iteration : 610
train acc:  0.7109375
train loss:  0.514005184173584
train gradient:  0.11037733579443962
iteration : 611
train acc:  0.75
train loss:  0.48487067222595215
train gradient:  0.09754053641894424
iteration : 612
train acc:  0.671875
train loss:  0.5153557658195496
train gradient:  0.1301472164993403
iteration : 613
train acc:  0.7421875
train loss:  0.4640894830226898
train gradient:  0.0961930829555123
iteration : 614
train acc:  0.7265625
train loss:  0.5266133546829224
train gradient:  0.11342168410431012
iteration : 615
train acc:  0.7578125
train loss:  0.48764193058013916
train gradient:  0.11001837452720131
iteration : 616
train acc:  0.796875
train loss:  0.45742011070251465
train gradient:  0.10819328822720368
iteration : 617
train acc:  0.7421875
train loss:  0.5526622533798218
train gradient:  0.1560216748308177
iteration : 618
train acc:  0.734375
train loss:  0.49065735936164856
train gradient:  0.1174637813627766
iteration : 619
train acc:  0.7109375
train loss:  0.500186562538147
train gradient:  0.11705690737597663
iteration : 620
train acc:  0.8046875
train loss:  0.43174219131469727
train gradient:  0.11084421463666545
iteration : 621
train acc:  0.7578125
train loss:  0.5214051604270935
train gradient:  0.18595893933184085
iteration : 622
train acc:  0.6640625
train loss:  0.5803960561752319
train gradient:  0.15826678823304424
iteration : 623
train acc:  0.8046875
train loss:  0.4137924313545227
train gradient:  0.07594979786902382
iteration : 624
train acc:  0.703125
train loss:  0.505800724029541
train gradient:  0.11923382185204666
iteration : 625
train acc:  0.7109375
train loss:  0.49998268485069275
train gradient:  0.11717789895246983
iteration : 626
train acc:  0.7890625
train loss:  0.46331143379211426
train gradient:  0.1059811842491784
iteration : 627
train acc:  0.8203125
train loss:  0.45679566264152527
train gradient:  0.10749457154289135
iteration : 628
train acc:  0.7421875
train loss:  0.45043855905532837
train gradient:  0.09567876267831901
iteration : 629
train acc:  0.734375
train loss:  0.4583702087402344
train gradient:  0.13398268956025083
iteration : 630
train acc:  0.7734375
train loss:  0.45273131132125854
train gradient:  0.1169566982845575
iteration : 631
train acc:  0.765625
train loss:  0.4974541962146759
train gradient:  0.12792537377361396
iteration : 632
train acc:  0.7578125
train loss:  0.4251152575016022
train gradient:  0.08736197951549417
iteration : 633
train acc:  0.75
train loss:  0.4781811535358429
train gradient:  0.13135563905177877
iteration : 634
train acc:  0.75
train loss:  0.448635071516037
train gradient:  0.09830177492592336
iteration : 635
train acc:  0.75
train loss:  0.4662766456604004
train gradient:  0.09709045682238412
iteration : 636
train acc:  0.7421875
train loss:  0.4684121012687683
train gradient:  0.09019986694636632
iteration : 637
train acc:  0.7421875
train loss:  0.4900710880756378
train gradient:  0.1434249294220816
iteration : 638
train acc:  0.6796875
train loss:  0.5156475901603699
train gradient:  0.13986724967530267
iteration : 639
train acc:  0.71875
train loss:  0.474932998418808
train gradient:  0.11097428601472097
iteration : 640
train acc:  0.7578125
train loss:  0.46165338158607483
train gradient:  0.09226970667735361
iteration : 641
train acc:  0.6875
train loss:  0.5774336457252502
train gradient:  0.13567280791611128
iteration : 642
train acc:  0.671875
train loss:  0.5149223804473877
train gradient:  0.12055982332677442
iteration : 643
train acc:  0.75
train loss:  0.4494675099849701
train gradient:  0.09339606913147572
iteration : 644
train acc:  0.6875
train loss:  0.5831764936447144
train gradient:  0.21106214409700325
iteration : 645
train acc:  0.75
train loss:  0.4873010814189911
train gradient:  0.11457215579015087
iteration : 646
train acc:  0.7890625
train loss:  0.4278468191623688
train gradient:  0.08676737775665427
iteration : 647
train acc:  0.7890625
train loss:  0.44884341955184937
train gradient:  0.09256401219018183
iteration : 648
train acc:  0.7265625
train loss:  0.49557286500930786
train gradient:  0.10958172221893256
iteration : 649
train acc:  0.765625
train loss:  0.4737906754016876
train gradient:  0.11704517516075275
iteration : 650
train acc:  0.703125
train loss:  0.5321446657180786
train gradient:  0.13443353904224875
iteration : 651
train acc:  0.7734375
train loss:  0.40669184923171997
train gradient:  0.11237955538630685
iteration : 652
train acc:  0.734375
train loss:  0.5243984460830688
train gradient:  0.11460566186970009
iteration : 653
train acc:  0.796875
train loss:  0.4445129930973053
train gradient:  0.1047402934250861
iteration : 654
train acc:  0.75
train loss:  0.4685962200164795
train gradient:  0.09513638079344809
iteration : 655
train acc:  0.78125
train loss:  0.49941515922546387
train gradient:  0.1143711826576622
iteration : 656
train acc:  0.734375
train loss:  0.5116499066352844
train gradient:  0.12118483986013566
iteration : 657
train acc:  0.765625
train loss:  0.4551149606704712
train gradient:  0.08533364287859131
iteration : 658
train acc:  0.734375
train loss:  0.5272021293640137
train gradient:  0.1142207971386882
iteration : 659
train acc:  0.6953125
train loss:  0.5138359069824219
train gradient:  0.11398043763722243
iteration : 660
train acc:  0.8125
train loss:  0.43784981966018677
train gradient:  0.10059122582132467
iteration : 661
train acc:  0.75
train loss:  0.48108649253845215
train gradient:  0.12362995263624195
iteration : 662
train acc:  0.7265625
train loss:  0.4533175826072693
train gradient:  0.12387674327422862
iteration : 663
train acc:  0.7421875
train loss:  0.4898206889629364
train gradient:  0.10108449706712827
iteration : 664
train acc:  0.7109375
train loss:  0.4831516742706299
train gradient:  0.14488742445766883
iteration : 665
train acc:  0.7265625
train loss:  0.5349022746086121
train gradient:  0.10793466503237002
iteration : 666
train acc:  0.71875
train loss:  0.5033597946166992
train gradient:  0.107898379588633
iteration : 667
train acc:  0.671875
train loss:  0.5050897598266602
train gradient:  0.13470153426161252
iteration : 668
train acc:  0.7578125
train loss:  0.47326192259788513
train gradient:  0.12036154317239871
iteration : 669
train acc:  0.71875
train loss:  0.5194758176803589
train gradient:  0.12870754644597726
iteration : 670
train acc:  0.71875
train loss:  0.4944518804550171
train gradient:  0.09135871915885736
iteration : 671
train acc:  0.78125
train loss:  0.46511727571487427
train gradient:  0.13419741677041647
iteration : 672
train acc:  0.7265625
train loss:  0.4510123133659363
train gradient:  0.09584306089807228
iteration : 673
train acc:  0.7109375
train loss:  0.5359159111976624
train gradient:  0.13247030743134183
iteration : 674
train acc:  0.7421875
train loss:  0.46415644884109497
train gradient:  0.10870107788652904
iteration : 675
train acc:  0.765625
train loss:  0.4393399953842163
train gradient:  0.08765063095885177
iteration : 676
train acc:  0.8046875
train loss:  0.4435245394706726
train gradient:  0.08690271845111404
iteration : 677
train acc:  0.6796875
train loss:  0.5351966619491577
train gradient:  0.11117360070178407
iteration : 678
train acc:  0.7734375
train loss:  0.48805713653564453
train gradient:  0.11001087500733668
iteration : 679
train acc:  0.75
train loss:  0.49660828709602356
train gradient:  0.1428822821375954
iteration : 680
train acc:  0.7421875
train loss:  0.47672030329704285
train gradient:  0.13529799604038067
iteration : 681
train acc:  0.71875
train loss:  0.4774785041809082
train gradient:  0.11020618333193057
iteration : 682
train acc:  0.765625
train loss:  0.4973154067993164
train gradient:  0.11097504346632232
iteration : 683
train acc:  0.75
train loss:  0.5047352313995361
train gradient:  0.13446467018807756
iteration : 684
train acc:  0.765625
train loss:  0.4304867386817932
train gradient:  0.10728675386273784
iteration : 685
train acc:  0.7421875
train loss:  0.47640424966812134
train gradient:  0.10508363541439417
iteration : 686
train acc:  0.765625
train loss:  0.5005719065666199
train gradient:  0.10980046125454752
iteration : 687
train acc:  0.6796875
train loss:  0.49426645040512085
train gradient:  0.0949567115045477
iteration : 688
train acc:  0.828125
train loss:  0.39413946866989136
train gradient:  0.07187818406791985
iteration : 689
train acc:  0.7578125
train loss:  0.44223839044570923
train gradient:  0.0946852767855951
iteration : 690
train acc:  0.7421875
train loss:  0.46935319900512695
train gradient:  0.09147544501116105
iteration : 691
train acc:  0.734375
train loss:  0.4424636960029602
train gradient:  0.08555651037965689
iteration : 692
train acc:  0.71875
train loss:  0.47335681319236755
train gradient:  0.11492800591418019
iteration : 693
train acc:  0.671875
train loss:  0.5668580532073975
train gradient:  0.14832190040332927
iteration : 694
train acc:  0.78125
train loss:  0.45259079337120056
train gradient:  0.12865032545731586
iteration : 695
train acc:  0.765625
train loss:  0.5111997723579407
train gradient:  0.12418929034097628
iteration : 696
train acc:  0.7421875
train loss:  0.4765074849128723
train gradient:  0.10716450512671942
iteration : 697
train acc:  0.7265625
train loss:  0.5142532587051392
train gradient:  0.11613373356303985
iteration : 698
train acc:  0.734375
train loss:  0.5004903078079224
train gradient:  0.13784954743618966
iteration : 699
train acc:  0.71875
train loss:  0.5335713624954224
train gradient:  0.14152661647471887
iteration : 700
train acc:  0.7734375
train loss:  0.513882040977478
train gradient:  0.1366352929416949
iteration : 701
train acc:  0.6953125
train loss:  0.5537549257278442
train gradient:  0.13805433371815884
iteration : 702
train acc:  0.75
train loss:  0.45717379450798035
train gradient:  0.089185531898739
iteration : 703
train acc:  0.796875
train loss:  0.44951143860816956
train gradient:  0.10340499220841089
iteration : 704
train acc:  0.7421875
train loss:  0.4385742247104645
train gradient:  0.11884317253924724
iteration : 705
train acc:  0.7265625
train loss:  0.4765644073486328
train gradient:  0.12352573076358712
iteration : 706
train acc:  0.796875
train loss:  0.4181683659553528
train gradient:  0.11847079489586468
iteration : 707
train acc:  0.75
train loss:  0.5136657953262329
train gradient:  0.12375956905951842
iteration : 708
train acc:  0.703125
train loss:  0.53947913646698
train gradient:  0.11285365076864336
iteration : 709
train acc:  0.71875
train loss:  0.49503129720687866
train gradient:  0.13055899717845088
iteration : 710
train acc:  0.7421875
train loss:  0.5206337571144104
train gradient:  0.11565543095510486
iteration : 711
train acc:  0.7109375
train loss:  0.5474898815155029
train gradient:  0.15503479005669227
iteration : 712
train acc:  0.75
train loss:  0.4535619020462036
train gradient:  0.10039289647166443
iteration : 713
train acc:  0.734375
train loss:  0.4748445749282837
train gradient:  0.10917687122903895
iteration : 714
train acc:  0.7265625
train loss:  0.5569210648536682
train gradient:  0.14372947111575762
iteration : 715
train acc:  0.75
train loss:  0.42361336946487427
train gradient:  0.08024169097820812
iteration : 716
train acc:  0.7734375
train loss:  0.4433180093765259
train gradient:  0.09795226155136932
iteration : 717
train acc:  0.78125
train loss:  0.4412916898727417
train gradient:  0.0911309777630039
iteration : 718
train acc:  0.78125
train loss:  0.43545103073120117
train gradient:  0.09830100295033349
iteration : 719
train acc:  0.7734375
train loss:  0.4667292833328247
train gradient:  0.1391689213273276
iteration : 720
train acc:  0.734375
train loss:  0.48651811480522156
train gradient:  0.11124432880889112
iteration : 721
train acc:  0.78125
train loss:  0.488480806350708
train gradient:  0.13674586748976325
iteration : 722
train acc:  0.7265625
train loss:  0.4896048307418823
train gradient:  0.1381911145901184
iteration : 723
train acc:  0.75
train loss:  0.48668527603149414
train gradient:  0.11844193441347459
iteration : 724
train acc:  0.734375
train loss:  0.5186876058578491
train gradient:  0.12160939459384837
iteration : 725
train acc:  0.8046875
train loss:  0.4197186827659607
train gradient:  0.09250275719573506
iteration : 726
train acc:  0.71875
train loss:  0.4911807179450989
train gradient:  0.1297541444995337
iteration : 727
train acc:  0.75
train loss:  0.4624740481376648
train gradient:  0.09512908718095994
iteration : 728
train acc:  0.6953125
train loss:  0.5785737037658691
train gradient:  0.1499390697839023
iteration : 729
train acc:  0.7734375
train loss:  0.4234185814857483
train gradient:  0.07330275175004938
iteration : 730
train acc:  0.8046875
train loss:  0.39999955892562866
train gradient:  0.09564438685498983
iteration : 731
train acc:  0.7265625
train loss:  0.49163010716438293
train gradient:  0.1118728945681418
iteration : 732
train acc:  0.7265625
train loss:  0.4724811315536499
train gradient:  0.08930716511193637
iteration : 733
train acc:  0.6875
train loss:  0.5188427567481995
train gradient:  0.12500987722542023
iteration : 734
train acc:  0.71875
train loss:  0.5104886293411255
train gradient:  0.12194762005222473
iteration : 735
train acc:  0.8046875
train loss:  0.36464375257492065
train gradient:  0.06902926421906797
iteration : 736
train acc:  0.7421875
train loss:  0.5252270698547363
train gradient:  0.13755474676874352
iteration : 737
train acc:  0.75
train loss:  0.5116400122642517
train gradient:  0.11446945716936698
iteration : 738
train acc:  0.796875
train loss:  0.4421849250793457
train gradient:  0.09699206463732722
iteration : 739
train acc:  0.7421875
train loss:  0.464378297328949
train gradient:  0.09727237840451987
iteration : 740
train acc:  0.7734375
train loss:  0.44539281725883484
train gradient:  0.1196466096801643
iteration : 741
train acc:  0.765625
train loss:  0.47060200572013855
train gradient:  0.10868630943068146
iteration : 742
train acc:  0.7421875
train loss:  0.47177863121032715
train gradient:  0.11073264800251331
iteration : 743
train acc:  0.6796875
train loss:  0.5458211898803711
train gradient:  0.12780042697719918
iteration : 744
train acc:  0.6875
train loss:  0.5276049971580505
train gradient:  0.14420847442166274
iteration : 745
train acc:  0.6953125
train loss:  0.5048407912254333
train gradient:  0.10034376639561429
iteration : 746
train acc:  0.734375
train loss:  0.47054576873779297
train gradient:  0.10510705677765486
iteration : 747
train acc:  0.65625
train loss:  0.5701782703399658
train gradient:  0.16350285057339325
iteration : 748
train acc:  0.765625
train loss:  0.5063809156417847
train gradient:  0.143763973522039
iteration : 749
train acc:  0.7890625
train loss:  0.4416252374649048
train gradient:  0.11027052532266007
iteration : 750
train acc:  0.7734375
train loss:  0.4478209912776947
train gradient:  0.09306190204683547
iteration : 751
train acc:  0.7578125
train loss:  0.4466509521007538
train gradient:  0.08870068627393432
iteration : 752
train acc:  0.75
train loss:  0.5060495734214783
train gradient:  0.153065263372963
iteration : 753
train acc:  0.7734375
train loss:  0.41823434829711914
train gradient:  0.07408500767004327
iteration : 754
train acc:  0.78125
train loss:  0.4706185460090637
train gradient:  0.10006161372749289
iteration : 755
train acc:  0.6640625
train loss:  0.5005607604980469
train gradient:  0.10835251589312718
iteration : 756
train acc:  0.7265625
train loss:  0.5094653367996216
train gradient:  0.11137843808387034
iteration : 757
train acc:  0.7265625
train loss:  0.5227640867233276
train gradient:  0.1230857601440885
iteration : 758
train acc:  0.6953125
train loss:  0.5311688184738159
train gradient:  0.13109306623513534
iteration : 759
train acc:  0.7265625
train loss:  0.49613767862319946
train gradient:  0.11033613833293394
iteration : 760
train acc:  0.75
train loss:  0.47791752219200134
train gradient:  0.11097476247969466
iteration : 761
train acc:  0.734375
train loss:  0.46747446060180664
train gradient:  0.12573478056533727
iteration : 762
train acc:  0.7578125
train loss:  0.44751375913619995
train gradient:  0.12466788246718652
iteration : 763
train acc:  0.734375
train loss:  0.5149675011634827
train gradient:  0.15916869200810596
iteration : 764
train acc:  0.71875
train loss:  0.5162261724472046
train gradient:  0.12491559231971873
iteration : 765
train acc:  0.7734375
train loss:  0.4611818194389343
train gradient:  0.10845059483584409
iteration : 766
train acc:  0.765625
train loss:  0.4716711640357971
train gradient:  0.10670023316375774
iteration : 767
train acc:  0.7265625
train loss:  0.5235904455184937
train gradient:  0.1511343908839307
iteration : 768
train acc:  0.7265625
train loss:  0.4709656834602356
train gradient:  0.11163891184749013
iteration : 769
train acc:  0.7421875
train loss:  0.582251250743866
train gradient:  0.15881223185339846
iteration : 770
train acc:  0.765625
train loss:  0.4703461229801178
train gradient:  0.11342286708445162
iteration : 771
train acc:  0.8046875
train loss:  0.4444184899330139
train gradient:  0.10079142495303736
iteration : 772
train acc:  0.7578125
train loss:  0.4719215929508209
train gradient:  0.11800429790678164
iteration : 773
train acc:  0.7578125
train loss:  0.4785272777080536
train gradient:  0.10596761728453642
iteration : 774
train acc:  0.7734375
train loss:  0.44291675090789795
train gradient:  0.09257934610873267
iteration : 775
train acc:  0.7578125
train loss:  0.49231576919555664
train gradient:  0.11039439014983968
iteration : 776
train acc:  0.703125
train loss:  0.49259454011917114
train gradient:  0.10033681554400745
iteration : 777
train acc:  0.7265625
train loss:  0.48317205905914307
train gradient:  0.09537592345293511
iteration : 778
train acc:  0.71875
train loss:  0.5141960978507996
train gradient:  0.10857899613239598
iteration : 779
train acc:  0.78125
train loss:  0.48290613293647766
train gradient:  0.12440279473992517
iteration : 780
train acc:  0.6953125
train loss:  0.5227392315864563
train gradient:  0.14537308701299154
iteration : 781
train acc:  0.7734375
train loss:  0.46396422386169434
train gradient:  0.10019040618450566
iteration : 782
train acc:  0.703125
train loss:  0.5733494162559509
train gradient:  0.17123491042666314
iteration : 783
train acc:  0.75
train loss:  0.45428746938705444
train gradient:  0.11975045673563489
iteration : 784
train acc:  0.7421875
train loss:  0.45737719535827637
train gradient:  0.11519136069721879
iteration : 785
train acc:  0.6953125
train loss:  0.59739089012146
train gradient:  0.1508787607195698
iteration : 786
train acc:  0.7578125
train loss:  0.4613632559776306
train gradient:  0.10377958954991205
iteration : 787
train acc:  0.7109375
train loss:  0.4958376884460449
train gradient:  0.11915100408602586
iteration : 788
train acc:  0.6953125
train loss:  0.5316926836967468
train gradient:  0.13057708267699886
iteration : 789
train acc:  0.6796875
train loss:  0.5533196926116943
train gradient:  0.16643673050934754
iteration : 790
train acc:  0.8046875
train loss:  0.4394013285636902
train gradient:  0.09383626622385723
iteration : 791
train acc:  0.71875
train loss:  0.5274682641029358
train gradient:  0.13877215694528514
iteration : 792
train acc:  0.6953125
train loss:  0.5572752356529236
train gradient:  0.1414844052224798
iteration : 793
train acc:  0.6328125
train loss:  0.5743852853775024
train gradient:  0.14978498820374403
iteration : 794
train acc:  0.7109375
train loss:  0.5144093632698059
train gradient:  0.11419074385159407
iteration : 795
train acc:  0.734375
train loss:  0.5092312097549438
train gradient:  0.13816304151706205
iteration : 796
train acc:  0.75
train loss:  0.5244317054748535
train gradient:  0.09911532667835429
iteration : 797
train acc:  0.6796875
train loss:  0.5572471618652344
train gradient:  0.14018647040707566
iteration : 798
train acc:  0.75
train loss:  0.5416420698165894
train gradient:  0.15113116333191784
iteration : 799
train acc:  0.734375
train loss:  0.46257808804512024
train gradient:  0.09400537784858153
iteration : 800
train acc:  0.71875
train loss:  0.5780941247940063
train gradient:  0.23753940737791723
iteration : 801
train acc:  0.8203125
train loss:  0.4093658924102783
train gradient:  0.07774336385193585
iteration : 802
train acc:  0.75
train loss:  0.47955209016799927
train gradient:  0.08701018648814972
iteration : 803
train acc:  0.734375
train loss:  0.495426744222641
train gradient:  0.12279900963707802
iteration : 804
train acc:  0.7109375
train loss:  0.5155057907104492
train gradient:  0.1312887583891147
iteration : 805
train acc:  0.7890625
train loss:  0.44292908906936646
train gradient:  0.09367130428564271
iteration : 806
train acc:  0.78125
train loss:  0.44769012928009033
train gradient:  0.0930731209456295
iteration : 807
train acc:  0.7890625
train loss:  0.4223514199256897
train gradient:  0.07675588821025404
iteration : 808
train acc:  0.8046875
train loss:  0.43724700808525085
train gradient:  0.09673126781240242
iteration : 809
train acc:  0.7109375
train loss:  0.4983813464641571
train gradient:  0.10723915016121466
iteration : 810
train acc:  0.7265625
train loss:  0.47985658049583435
train gradient:  0.10559751921293917
iteration : 811
train acc:  0.7421875
train loss:  0.4984542429447174
train gradient:  0.1136658825554069
iteration : 812
train acc:  0.734375
train loss:  0.5311456322669983
train gradient:  0.11016134213853576
iteration : 813
train acc:  0.75
train loss:  0.4576573371887207
train gradient:  0.11659784731874771
iteration : 814
train acc:  0.765625
train loss:  0.4460489749908447
train gradient:  0.09824883992974183
iteration : 815
train acc:  0.78125
train loss:  0.4264677166938782
train gradient:  0.07537958987344129
iteration : 816
train acc:  0.78125
train loss:  0.4862474799156189
train gradient:  0.11563014658455463
iteration : 817
train acc:  0.8515625
train loss:  0.371925950050354
train gradient:  0.06887418543169069
iteration : 818
train acc:  0.78125
train loss:  0.39756035804748535
train gradient:  0.0614113546889703
iteration : 819
train acc:  0.7734375
train loss:  0.4669863283634186
train gradient:  0.10409992835114253
iteration : 820
train acc:  0.828125
train loss:  0.43985676765441895
train gradient:  0.08905485615812705
iteration : 821
train acc:  0.7578125
train loss:  0.48861396312713623
train gradient:  0.129748187683139
iteration : 822
train acc:  0.7109375
train loss:  0.5083792209625244
train gradient:  0.12787180140380439
iteration : 823
train acc:  0.8125
train loss:  0.43219631910324097
train gradient:  0.08646923097185574
iteration : 824
train acc:  0.765625
train loss:  0.44610655307769775
train gradient:  0.09432775094414154
iteration : 825
train acc:  0.8046875
train loss:  0.4106762707233429
train gradient:  0.0851847208930572
iteration : 826
train acc:  0.7734375
train loss:  0.4823724925518036
train gradient:  0.14634379427792482
iteration : 827
train acc:  0.78125
train loss:  0.47156673669815063
train gradient:  0.0911996762642068
iteration : 828
train acc:  0.6953125
train loss:  0.5317866802215576
train gradient:  0.15141802606685772
iteration : 829
train acc:  0.859375
train loss:  0.36791539192199707
train gradient:  0.06587290304900717
iteration : 830
train acc:  0.75
train loss:  0.5106452703475952
train gradient:  0.13323952446651427
iteration : 831
train acc:  0.8203125
train loss:  0.41482168436050415
train gradient:  0.08944681647036029
iteration : 832
train acc:  0.734375
train loss:  0.47614189982414246
train gradient:  0.115281946195622
iteration : 833
train acc:  0.7421875
train loss:  0.5176149606704712
train gradient:  0.13653203143356685
iteration : 834
train acc:  0.7109375
train loss:  0.5067862272262573
train gradient:  0.12155672592143507
iteration : 835
train acc:  0.7421875
train loss:  0.5043654441833496
train gradient:  0.11355898642933898
iteration : 836
train acc:  0.7421875
train loss:  0.4734927713871002
train gradient:  0.11684023763735371
iteration : 837
train acc:  0.7578125
train loss:  0.45111554861068726
train gradient:  0.09723743114658702
iteration : 838
train acc:  0.71875
train loss:  0.5488650798797607
train gradient:  0.15486759212353668
iteration : 839
train acc:  0.7421875
train loss:  0.4939444065093994
train gradient:  0.11701326953194142
iteration : 840
train acc:  0.734375
train loss:  0.5057466626167297
train gradient:  0.1372987071478688
iteration : 841
train acc:  0.6796875
train loss:  0.5418248176574707
train gradient:  0.1641152475769931
iteration : 842
train acc:  0.78125
train loss:  0.441008597612381
train gradient:  0.09171368999888271
iteration : 843
train acc:  0.71875
train loss:  0.5155293345451355
train gradient:  0.16792167318499984
iteration : 844
train acc:  0.7265625
train loss:  0.46914392709732056
train gradient:  0.09974136737080394
iteration : 845
train acc:  0.7578125
train loss:  0.4758284091949463
train gradient:  0.11208092931782139
iteration : 846
train acc:  0.7421875
train loss:  0.47014129161834717
train gradient:  0.10454310532309136
iteration : 847
train acc:  0.7265625
train loss:  0.483733594417572
train gradient:  0.1134848688954979
iteration : 848
train acc:  0.8125
train loss:  0.4076061546802521
train gradient:  0.08430418302707937
iteration : 849
train acc:  0.671875
train loss:  0.5691468715667725
train gradient:  0.2009849952164043
iteration : 850
train acc:  0.734375
train loss:  0.4826590418815613
train gradient:  0.1168880653008486
iteration : 851
train acc:  0.796875
train loss:  0.42861026525497437
train gradient:  0.08292724312329702
iteration : 852
train acc:  0.7734375
train loss:  0.438406765460968
train gradient:  0.11972438677661322
iteration : 853
train acc:  0.734375
train loss:  0.46973171830177307
train gradient:  0.10325237690145306
iteration : 854
train acc:  0.78125
train loss:  0.4332501292228699
train gradient:  0.08300126079439271
iteration : 855
train acc:  0.75
train loss:  0.4595791697502136
train gradient:  0.10615716296084493
iteration : 856
train acc:  0.734375
train loss:  0.475909560918808
train gradient:  0.10862062998736051
iteration : 857
train acc:  0.765625
train loss:  0.46910691261291504
train gradient:  0.0876557794570433
iteration : 858
train acc:  0.765625
train loss:  0.43656831979751587
train gradient:  0.08987607867135708
iteration : 859
train acc:  0.7109375
train loss:  0.5152525901794434
train gradient:  0.11709977979187326
iteration : 860
train acc:  0.765625
train loss:  0.44283002614974976
train gradient:  0.08603262072558461
iteration : 861
train acc:  0.796875
train loss:  0.44123899936676025
train gradient:  0.09965349688230016
iteration : 862
train acc:  0.8046875
train loss:  0.4483415484428406
train gradient:  0.08926459396748085
iteration : 863
train acc:  0.78125
train loss:  0.4463942348957062
train gradient:  0.11151935694337752
iteration : 864
train acc:  0.7734375
train loss:  0.46628397703170776
train gradient:  0.11755309320211144
iteration : 865
train acc:  0.71875
train loss:  0.5034577250480652
train gradient:  0.13070803499132028
iteration : 866
train acc:  0.7578125
train loss:  0.4836256802082062
train gradient:  0.09620889671065305
iteration : 867
train acc:  0.7109375
train loss:  0.5228978395462036
train gradient:  0.13765880954975007
iteration : 868
train acc:  0.7265625
train loss:  0.49127352237701416
train gradient:  0.12375223094714363
iteration : 869
train acc:  0.8125
train loss:  0.4450451135635376
train gradient:  0.07741559265864063
iteration : 870
train acc:  0.7265625
train loss:  0.5120640993118286
train gradient:  0.1241145112058717
iteration : 871
train acc:  0.8203125
train loss:  0.38955098390579224
train gradient:  0.0985663598048279
iteration : 872
train acc:  0.765625
train loss:  0.440947949886322
train gradient:  0.11454085922774061
iteration : 873
train acc:  0.765625
train loss:  0.45763325691223145
train gradient:  0.11176797909604586
iteration : 874
train acc:  0.7890625
train loss:  0.40021249651908875
train gradient:  0.09163194963318125
iteration : 875
train acc:  0.765625
train loss:  0.42713794112205505
train gradient:  0.09789078831182844
iteration : 876
train acc:  0.734375
train loss:  0.510968804359436
train gradient:  0.11669393317190616
iteration : 877
train acc:  0.6953125
train loss:  0.5126384496688843
train gradient:  0.10908120752343069
iteration : 878
train acc:  0.75
train loss:  0.5111722350120544
train gradient:  0.14467071152217664
iteration : 879
train acc:  0.796875
train loss:  0.4587029814720154
train gradient:  0.09872125404190941
iteration : 880
train acc:  0.71875
train loss:  0.518036961555481
train gradient:  0.13759274498710822
iteration : 881
train acc:  0.7109375
train loss:  0.5168157815933228
train gradient:  0.15051417621053748
iteration : 882
train acc:  0.75
train loss:  0.45553094148635864
train gradient:  0.09849676294757462
iteration : 883
train acc:  0.8203125
train loss:  0.43804723024368286
train gradient:  0.11019082506427513
iteration : 884
train acc:  0.7578125
train loss:  0.467331200838089
train gradient:  0.10316150280149161
iteration : 885
train acc:  0.7890625
train loss:  0.427927702665329
train gradient:  0.09307577761923182
iteration : 886
train acc:  0.7421875
train loss:  0.48350489139556885
train gradient:  0.1323329631862361
iteration : 887
train acc:  0.78125
train loss:  0.45317012071609497
train gradient:  0.10731363649354304
iteration : 888
train acc:  0.7734375
train loss:  0.49266672134399414
train gradient:  0.1346771480763551
iteration : 889
train acc:  0.7890625
train loss:  0.446916401386261
train gradient:  0.11368632914366716
iteration : 890
train acc:  0.78125
train loss:  0.44894498586654663
train gradient:  0.09218164053344083
iteration : 891
train acc:  0.7890625
train loss:  0.4469008147716522
train gradient:  0.0905258964997241
iteration : 892
train acc:  0.765625
train loss:  0.4366553723812103
train gradient:  0.09533184197611078
iteration : 893
train acc:  0.6953125
train loss:  0.5425963401794434
train gradient:  0.13173430314358958
iteration : 894
train acc:  0.71875
train loss:  0.4690544307231903
train gradient:  0.11152150292502845
iteration : 895
train acc:  0.71875
train loss:  0.5091649293899536
train gradient:  0.14025249840926446
iteration : 896
train acc:  0.765625
train loss:  0.44470155239105225
train gradient:  0.09751025983677823
iteration : 897
train acc:  0.7265625
train loss:  0.47666439414024353
train gradient:  0.1228840251359079
iteration : 898
train acc:  0.671875
train loss:  0.5693026185035706
train gradient:  0.1564697467913634
iteration : 899
train acc:  0.7734375
train loss:  0.45149871706962585
train gradient:  0.11076720923087771
iteration : 900
train acc:  0.78125
train loss:  0.4727873206138611
train gradient:  0.12976138452709216
iteration : 901
train acc:  0.671875
train loss:  0.5612796545028687
train gradient:  0.15487744745416931
iteration : 902
train acc:  0.765625
train loss:  0.4674431085586548
train gradient:  0.11706821383516784
iteration : 903
train acc:  0.734375
train loss:  0.48882877826690674
train gradient:  0.11147624259870004
iteration : 904
train acc:  0.75
train loss:  0.48147475719451904
train gradient:  0.09401316324328311
iteration : 905
train acc:  0.7578125
train loss:  0.45280763506889343
train gradient:  0.09504560721504211
iteration : 906
train acc:  0.796875
train loss:  0.43354761600494385
train gradient:  0.12267003592165747
iteration : 907
train acc:  0.7890625
train loss:  0.46358051896095276
train gradient:  0.1247353409112695
iteration : 908
train acc:  0.765625
train loss:  0.492611289024353
train gradient:  0.10542917204665052
iteration : 909
train acc:  0.78125
train loss:  0.4676384925842285
train gradient:  0.10822376432638137
iteration : 910
train acc:  0.7265625
train loss:  0.533349871635437
train gradient:  0.1488527433607678
iteration : 911
train acc:  0.703125
train loss:  0.5774800777435303
train gradient:  0.1787006310959371
iteration : 912
train acc:  0.765625
train loss:  0.46911877393722534
train gradient:  0.10719735442944762
iteration : 913
train acc:  0.7890625
train loss:  0.4292224645614624
train gradient:  0.08592256757715491
iteration : 914
train acc:  0.75
train loss:  0.45304352045059204
train gradient:  0.09485526221240637
iteration : 915
train acc:  0.8046875
train loss:  0.4333949685096741
train gradient:  0.09829245115893898
iteration : 916
train acc:  0.7265625
train loss:  0.5171351432800293
train gradient:  0.15629285366215462
iteration : 917
train acc:  0.78125
train loss:  0.4632716774940491
train gradient:  0.09964437098346263
iteration : 918
train acc:  0.7265625
train loss:  0.49158555269241333
train gradient:  0.11237896061063067
iteration : 919
train acc:  0.734375
train loss:  0.5514992475509644
train gradient:  0.15423683648145323
iteration : 920
train acc:  0.8125
train loss:  0.4241657555103302
train gradient:  0.1012674992187747
iteration : 921
train acc:  0.7890625
train loss:  0.4530562460422516
train gradient:  0.12862908391226638
iteration : 922
train acc:  0.7421875
train loss:  0.5018665194511414
train gradient:  0.1345088394342543
iteration : 923
train acc:  0.765625
train loss:  0.4328913390636444
train gradient:  0.0954158252555229
iteration : 924
train acc:  0.828125
train loss:  0.4205071032047272
train gradient:  0.10879494965619192
iteration : 925
train acc:  0.7890625
train loss:  0.4268925189971924
train gradient:  0.08724325794922003
iteration : 926
train acc:  0.78125
train loss:  0.4679907262325287
train gradient:  0.1040035097218601
iteration : 927
train acc:  0.78125
train loss:  0.437255859375
train gradient:  0.11630118588817107
iteration : 928
train acc:  0.7734375
train loss:  0.44672998785972595
train gradient:  0.13125297169192673
iteration : 929
train acc:  0.703125
train loss:  0.5759517550468445
train gradient:  0.15289666177311984
iteration : 930
train acc:  0.75
train loss:  0.43033623695373535
train gradient:  0.10169927996298155
iteration : 931
train acc:  0.734375
train loss:  0.5485064387321472
train gradient:  0.15061561125381367
iteration : 932
train acc:  0.7734375
train loss:  0.43293285369873047
train gradient:  0.10839559496049277
iteration : 933
train acc:  0.71875
train loss:  0.4915696382522583
train gradient:  0.1266359706666958
iteration : 934
train acc:  0.734375
train loss:  0.44425466656684875
train gradient:  0.09111988941029801
iteration : 935
train acc:  0.734375
train loss:  0.5127565860748291
train gradient:  0.14670027751425657
iteration : 936
train acc:  0.7890625
train loss:  0.43696826696395874
train gradient:  0.12123504194234022
iteration : 937
train acc:  0.71875
train loss:  0.49086761474609375
train gradient:  0.1142828469667627
iteration : 938
train acc:  0.7578125
train loss:  0.5114502310752869
train gradient:  0.13000953905631338
iteration : 939
train acc:  0.7734375
train loss:  0.4216729402542114
train gradient:  0.09387986029114119
iteration : 940
train acc:  0.8359375
train loss:  0.441689133644104
train gradient:  0.11354800109716484
iteration : 941
train acc:  0.78125
train loss:  0.45191657543182373
train gradient:  0.0973187542606932
iteration : 942
train acc:  0.8203125
train loss:  0.4227837324142456
train gradient:  0.08746208687687947
iteration : 943
train acc:  0.734375
train loss:  0.5363845825195312
train gradient:  0.13441987303883085
iteration : 944
train acc:  0.7421875
train loss:  0.5132009983062744
train gradient:  0.14453136606172767
iteration : 945
train acc:  0.75
train loss:  0.4475337862968445
train gradient:  0.14034332928259713
iteration : 946
train acc:  0.765625
train loss:  0.43155181407928467
train gradient:  0.09644810864708005
iteration : 947
train acc:  0.75
train loss:  0.4844930171966553
train gradient:  0.10137825712705407
iteration : 948
train acc:  0.78125
train loss:  0.4253535866737366
train gradient:  0.09457433189222442
iteration : 949
train acc:  0.7421875
train loss:  0.43741458654403687
train gradient:  0.09017497914550017
iteration : 950
train acc:  0.7734375
train loss:  0.47648465633392334
train gradient:  0.09511685212220092
iteration : 951
train acc:  0.7265625
train loss:  0.5565033555030823
train gradient:  0.1268344685273164
iteration : 952
train acc:  0.78125
train loss:  0.43769076466560364
train gradient:  0.09390809086580333
iteration : 953
train acc:  0.8046875
train loss:  0.44446295499801636
train gradient:  0.11608278662681615
iteration : 954
train acc:  0.75
train loss:  0.5214905142784119
train gradient:  0.1499454759043679
iteration : 955
train acc:  0.765625
train loss:  0.5089736580848694
train gradient:  0.14637004811028284
iteration : 956
train acc:  0.8359375
train loss:  0.4149605929851532
train gradient:  0.1166711113165391
iteration : 957
train acc:  0.6953125
train loss:  0.5511695742607117
train gradient:  0.1560648159113455
iteration : 958
train acc:  0.765625
train loss:  0.450518935918808
train gradient:  0.0847981360139959
iteration : 959
train acc:  0.7421875
train loss:  0.48160138726234436
train gradient:  0.14525188994467209
iteration : 960
train acc:  0.7734375
train loss:  0.42648375034332275
train gradient:  0.11923440043688077
iteration : 961
train acc:  0.734375
train loss:  0.48800069093704224
train gradient:  0.1309844937792586
iteration : 962
train acc:  0.71875
train loss:  0.5030108094215393
train gradient:  0.1398177427735725
iteration : 963
train acc:  0.78125
train loss:  0.4968540668487549
train gradient:  0.13086729772767208
iteration : 964
train acc:  0.7109375
train loss:  0.5363212823867798
train gradient:  0.13832227751330262
iteration : 965
train acc:  0.78125
train loss:  0.4122721552848816
train gradient:  0.09162867157152804
iteration : 966
train acc:  0.7109375
train loss:  0.44024890661239624
train gradient:  0.09774609222243809
iteration : 967
train acc:  0.7109375
train loss:  0.5115861892700195
train gradient:  0.15062526968576556
iteration : 968
train acc:  0.6875
train loss:  0.5870146751403809
train gradient:  0.1798185740521869
iteration : 969
train acc:  0.71875
train loss:  0.5043575167655945
train gradient:  0.1410925374960835
iteration : 970
train acc:  0.6875
train loss:  0.5593689680099487
train gradient:  0.17234137098171776
iteration : 971
train acc:  0.7265625
train loss:  0.4875904619693756
train gradient:  0.12949677564237103
iteration : 972
train acc:  0.703125
train loss:  0.5868160724639893
train gradient:  0.15551350879152187
iteration : 973
train acc:  0.75
train loss:  0.4706413149833679
train gradient:  0.09319785841392812
iteration : 974
train acc:  0.7109375
train loss:  0.5297309160232544
train gradient:  0.12065634025051221
iteration : 975
train acc:  0.7734375
train loss:  0.4679502844810486
train gradient:  0.09197047976068423
iteration : 976
train acc:  0.734375
train loss:  0.46189868450164795
train gradient:  0.10269363193654382
iteration : 977
train acc:  0.71875
train loss:  0.5717974305152893
train gradient:  0.1746477052867071
iteration : 978
train acc:  0.7734375
train loss:  0.43156957626342773
train gradient:  0.12411975086975885
iteration : 979
train acc:  0.7734375
train loss:  0.4516526460647583
train gradient:  0.08236405959566465
iteration : 980
train acc:  0.8046875
train loss:  0.40622493624687195
train gradient:  0.07469767425035852
iteration : 981
train acc:  0.734375
train loss:  0.4417653977870941
train gradient:  0.09667920948498267
iteration : 982
train acc:  0.6953125
train loss:  0.489644855260849
train gradient:  0.11906299224403537
iteration : 983
train acc:  0.75
train loss:  0.46589395403862
train gradient:  0.09977652366933687
iteration : 984
train acc:  0.71875
train loss:  0.49832266569137573
train gradient:  0.15853921490173117
iteration : 985
train acc:  0.7734375
train loss:  0.42461830377578735
train gradient:  0.10259466864195481
iteration : 986
train acc:  0.75
train loss:  0.4552914500236511
train gradient:  0.11411752661390163
iteration : 987
train acc:  0.734375
train loss:  0.5086904764175415
train gradient:  0.1265244881221843
iteration : 988
train acc:  0.765625
train loss:  0.4908375144004822
train gradient:  0.14638624497296487
iteration : 989
train acc:  0.7734375
train loss:  0.49079978466033936
train gradient:  0.11108093040771326
iteration : 990
train acc:  0.7578125
train loss:  0.4492161273956299
train gradient:  0.08878915655130346
iteration : 991
train acc:  0.7109375
train loss:  0.495688259601593
train gradient:  0.127651214692522
iteration : 992
train acc:  0.71875
train loss:  0.5358588695526123
train gradient:  0.13684233638597657
iteration : 993
train acc:  0.78125
train loss:  0.4767647087574005
train gradient:  0.13556881960429717
iteration : 994
train acc:  0.734375
train loss:  0.5289775133132935
train gradient:  0.15394975976656183
iteration : 995
train acc:  0.6953125
train loss:  0.5647062063217163
train gradient:  0.18861687367464952
iteration : 996
train acc:  0.796875
train loss:  0.42625242471694946
train gradient:  0.09975863090954777
iteration : 997
train acc:  0.7578125
train loss:  0.45811450481414795
train gradient:  0.10105036375095446
iteration : 998
train acc:  0.7734375
train loss:  0.5008341670036316
train gradient:  0.11693661340525781
iteration : 999
train acc:  0.765625
train loss:  0.47994065284729004
train gradient:  0.1542079900959431
iteration : 1000
train acc:  0.8125
train loss:  0.4213940501213074
train gradient:  0.13110020931920213
iteration : 1001
train acc:  0.75
train loss:  0.44350284337997437
train gradient:  0.10227442503490183
iteration : 1002
train acc:  0.7421875
train loss:  0.4398955702781677
train gradient:  0.09977582535056179
iteration : 1003
train acc:  0.6875
train loss:  0.5386661291122437
train gradient:  0.16758094005604957
iteration : 1004
train acc:  0.7890625
train loss:  0.4812403917312622
train gradient:  0.13205775419317092
iteration : 1005
train acc:  0.6875
train loss:  0.5227733254432678
train gradient:  0.11743364906379197
iteration : 1006
train acc:  0.6953125
train loss:  0.5071778297424316
train gradient:  0.1399867752014483
iteration : 1007
train acc:  0.7734375
train loss:  0.3998347520828247
train gradient:  0.09371129543327646
iteration : 1008
train acc:  0.6796875
train loss:  0.5389197468757629
train gradient:  0.1603467071713484
iteration : 1009
train acc:  0.8203125
train loss:  0.422958105802536
train gradient:  0.08214886689801376
iteration : 1010
train acc:  0.6640625
train loss:  0.5690479874610901
train gradient:  0.19393664498749186
iteration : 1011
train acc:  0.8046875
train loss:  0.39350563287734985
train gradient:  0.06766523894942283
iteration : 1012
train acc:  0.7421875
train loss:  0.5110114216804504
train gradient:  0.1511934500660707
iteration : 1013
train acc:  0.734375
train loss:  0.531627357006073
train gradient:  0.12745574873321602
iteration : 1014
train acc:  0.703125
train loss:  0.5576179027557373
train gradient:  0.1447434350190409
iteration : 1015
train acc:  0.734375
train loss:  0.45710256695747375
train gradient:  0.11418982529349919
iteration : 1016
train acc:  0.75
train loss:  0.5026135444641113
train gradient:  0.13287379825804913
iteration : 1017
train acc:  0.75
train loss:  0.5022710561752319
train gradient:  0.12415241386268684
iteration : 1018
train acc:  0.8203125
train loss:  0.4097708463668823
train gradient:  0.09119874558284746
iteration : 1019
train acc:  0.65625
train loss:  0.528513491153717
train gradient:  0.12097344672027567
iteration : 1020
train acc:  0.7578125
train loss:  0.5148358941078186
train gradient:  0.14283685877078584
iteration : 1021
train acc:  0.75
train loss:  0.4794486463069916
train gradient:  0.12098665633989898
iteration : 1022
train acc:  0.78125
train loss:  0.43932273983955383
train gradient:  0.12516541108124637
iteration : 1023
train acc:  0.7421875
train loss:  0.4864347577095032
train gradient:  0.17721509503193578
iteration : 1024
train acc:  0.8125
train loss:  0.4081641435623169
train gradient:  0.0900650574453254
iteration : 1025
train acc:  0.75
train loss:  0.46315059065818787
train gradient:  0.0984081062680031
iteration : 1026
train acc:  0.7734375
train loss:  0.4865989685058594
train gradient:  0.10615348805883995
iteration : 1027
train acc:  0.765625
train loss:  0.4482511281967163
train gradient:  0.11826137407000717
iteration : 1028
train acc:  0.6875
train loss:  0.5639339685440063
train gradient:  0.1349924488691966
iteration : 1029
train acc:  0.7421875
train loss:  0.45283111929893494
train gradient:  0.11176726780094186
iteration : 1030
train acc:  0.7421875
train loss:  0.46495717763900757
train gradient:  0.10639234231528401
iteration : 1031
train acc:  0.671875
train loss:  0.5474679470062256
train gradient:  0.1423367812715416
iteration : 1032
train acc:  0.7578125
train loss:  0.4640022814273834
train gradient:  0.11125665211928763
iteration : 1033
train acc:  0.78125
train loss:  0.45453596115112305
train gradient:  0.13642251075103526
iteration : 1034
train acc:  0.7265625
train loss:  0.5054849982261658
train gradient:  0.13812566426360479
iteration : 1035
train acc:  0.7265625
train loss:  0.5117439031600952
train gradient:  0.14525621434439645
iteration : 1036
train acc:  0.765625
train loss:  0.45283377170562744
train gradient:  0.09801021186227594
iteration : 1037
train acc:  0.671875
train loss:  0.5792275071144104
train gradient:  0.17249268542665258
iteration : 1038
train acc:  0.7109375
train loss:  0.5911383628845215
train gradient:  0.1685398778532582
iteration : 1039
train acc:  0.703125
train loss:  0.4881734251976013
train gradient:  0.13784690620350454
iteration : 1040
train acc:  0.75
train loss:  0.4743732511997223
train gradient:  0.12889933879463128
iteration : 1041
train acc:  0.7109375
train loss:  0.5154474377632141
train gradient:  0.13189583187965354
iteration : 1042
train acc:  0.75
train loss:  0.4891171157360077
train gradient:  0.14738431329458535
iteration : 1043
train acc:  0.7421875
train loss:  0.4972653388977051
train gradient:  0.13285057919861504
iteration : 1044
train acc:  0.7890625
train loss:  0.49137306213378906
train gradient:  0.10378415064865205
iteration : 1045
train acc:  0.7421875
train loss:  0.4814095199108124
train gradient:  0.13649217670452057
iteration : 1046
train acc:  0.7421875
train loss:  0.5264201164245605
train gradient:  0.1333094766906055
iteration : 1047
train acc:  0.78125
train loss:  0.46618223190307617
train gradient:  0.11056837958095442
iteration : 1048
train acc:  0.7265625
train loss:  0.4899948835372925
train gradient:  0.09997232964853806
iteration : 1049
train acc:  0.7265625
train loss:  0.4742642045021057
train gradient:  0.10107755211743111
iteration : 1050
train acc:  0.796875
train loss:  0.4402661919593811
train gradient:  0.12458874880255891
iteration : 1051
train acc:  0.765625
train loss:  0.4672236442565918
train gradient:  0.1290507348181777
iteration : 1052
train acc:  0.703125
train loss:  0.5619790554046631
train gradient:  0.1812942428723504
iteration : 1053
train acc:  0.703125
train loss:  0.5397891998291016
train gradient:  0.11579302512475328
iteration : 1054
train acc:  0.7109375
train loss:  0.5358044505119324
train gradient:  0.1475465750502655
iteration : 1055
train acc:  0.7578125
train loss:  0.5047221183776855
train gradient:  0.12327591031342888
iteration : 1056
train acc:  0.734375
train loss:  0.5680314302444458
train gradient:  0.18454026859983141
iteration : 1057
train acc:  0.765625
train loss:  0.4345945417881012
train gradient:  0.08180125660883535
iteration : 1058
train acc:  0.8046875
train loss:  0.47294551134109497
train gradient:  0.10866366793034528
iteration : 1059
train acc:  0.78125
train loss:  0.480980783700943
train gradient:  0.10530902336975259
iteration : 1060
train acc:  0.78125
train loss:  0.4364674687385559
train gradient:  0.09057255947618899
iteration : 1061
train acc:  0.78125
train loss:  0.4603883922100067
train gradient:  0.10936143517162028
iteration : 1062
train acc:  0.84375
train loss:  0.39698538184165955
train gradient:  0.07388779009365685
iteration : 1063
train acc:  0.7890625
train loss:  0.4339413046836853
train gradient:  0.1106307548195141
iteration : 1064
train acc:  0.796875
train loss:  0.4827961027622223
train gradient:  0.09734547752981156
iteration : 1065
train acc:  0.734375
train loss:  0.4570970833301544
train gradient:  0.09747603381916382
iteration : 1066
train acc:  0.7578125
train loss:  0.5310333967208862
train gradient:  0.1326920859334632
iteration : 1067
train acc:  0.6953125
train loss:  0.4805086553096771
train gradient:  0.10735261676991845
iteration : 1068
train acc:  0.7421875
train loss:  0.465643972158432
train gradient:  0.11684588122777549
iteration : 1069
train acc:  0.796875
train loss:  0.4374076724052429
train gradient:  0.1087518436706973
iteration : 1070
train acc:  0.7890625
train loss:  0.4509296417236328
train gradient:  0.11404846162506828
iteration : 1071
train acc:  0.8046875
train loss:  0.4381081759929657
train gradient:  0.08484076515394827
iteration : 1072
train acc:  0.703125
train loss:  0.5356219410896301
train gradient:  0.13899741972733626
iteration : 1073
train acc:  0.75
train loss:  0.44541627168655396
train gradient:  0.07695978537311901
iteration : 1074
train acc:  0.703125
train loss:  0.550174355506897
train gradient:  0.1554152503111293
iteration : 1075
train acc:  0.734375
train loss:  0.4985320568084717
train gradient:  0.15638307887624886
iteration : 1076
train acc:  0.7265625
train loss:  0.5048950910568237
train gradient:  0.1438459572481991
iteration : 1077
train acc:  0.765625
train loss:  0.510893702507019
train gradient:  0.12185366730192021
iteration : 1078
train acc:  0.734375
train loss:  0.5288214087486267
train gradient:  0.16395696542014704
iteration : 1079
train acc:  0.7578125
train loss:  0.4623543620109558
train gradient:  0.10467725512827708
iteration : 1080
train acc:  0.75
train loss:  0.47085875272750854
train gradient:  0.10954445620787798
iteration : 1081
train acc:  0.7421875
train loss:  0.4856993854045868
train gradient:  0.09879948905266803
iteration : 1082
train acc:  0.7578125
train loss:  0.4538945257663727
train gradient:  0.10225759665334339
iteration : 1083
train acc:  0.7578125
train loss:  0.49482619762420654
train gradient:  0.12945571551390767
iteration : 1084
train acc:  0.828125
train loss:  0.40697070956230164
train gradient:  0.0905908986845869
iteration : 1085
train acc:  0.75
train loss:  0.4997289478778839
train gradient:  0.12556415726674003
iteration : 1086
train acc:  0.7421875
train loss:  0.4559832811355591
train gradient:  0.12012361228880596
iteration : 1087
train acc:  0.75
train loss:  0.48154735565185547
train gradient:  0.10373724162975896
iteration : 1088
train acc:  0.796875
train loss:  0.48028531670570374
train gradient:  0.1159895354940737
iteration : 1089
train acc:  0.71875
train loss:  0.49232348799705505
train gradient:  0.1596458808392648
iteration : 1090
train acc:  0.75
train loss:  0.47013580799102783
train gradient:  0.11962743250792202
iteration : 1091
train acc:  0.765625
train loss:  0.46219968795776367
train gradient:  0.10223980319265691
iteration : 1092
train acc:  0.7421875
train loss:  0.47712916135787964
train gradient:  0.12324256921020303
iteration : 1093
train acc:  0.7265625
train loss:  0.48998019099235535
train gradient:  0.14263735228743968
iteration : 1094
train acc:  0.7421875
train loss:  0.48384448885917664
train gradient:  0.11167762260933457
iteration : 1095
train acc:  0.7421875
train loss:  0.49054205417633057
train gradient:  0.1168299875966419
iteration : 1096
train acc:  0.7578125
train loss:  0.5090985298156738
train gradient:  0.13124054483019057
iteration : 1097
train acc:  0.75
train loss:  0.501522421836853
train gradient:  0.10787230584853072
iteration : 1098
train acc:  0.671875
train loss:  0.573930025100708
train gradient:  0.14264714379129245
iteration : 1099
train acc:  0.75
train loss:  0.508367657661438
train gradient:  0.17819905524609014
iteration : 1100
train acc:  0.78125
train loss:  0.4336398243904114
train gradient:  0.1230242681285766
iteration : 1101
train acc:  0.734375
train loss:  0.5503332018852234
train gradient:  0.15743583615129148
iteration : 1102
train acc:  0.75
train loss:  0.5398522615432739
train gradient:  0.14352036853230996
iteration : 1103
train acc:  0.734375
train loss:  0.48692208528518677
train gradient:  0.1498550757088336
iteration : 1104
train acc:  0.7265625
train loss:  0.4763072729110718
train gradient:  0.10793722248320692
iteration : 1105
train acc:  0.734375
train loss:  0.49789464473724365
train gradient:  0.11206359244320638
iteration : 1106
train acc:  0.7578125
train loss:  0.46371525526046753
train gradient:  0.10458661351717939
iteration : 1107
train acc:  0.8046875
train loss:  0.41960346698760986
train gradient:  0.09618415276554033
iteration : 1108
train acc:  0.703125
train loss:  0.49908721446990967
train gradient:  0.14679138917728712
iteration : 1109
train acc:  0.7578125
train loss:  0.4340057969093323
train gradient:  0.09022445678034614
iteration : 1110
train acc:  0.765625
train loss:  0.5024659037590027
train gradient:  0.1363850056476707
iteration : 1111
train acc:  0.734375
train loss:  0.4650692641735077
train gradient:  0.10344824327440544
iteration : 1112
train acc:  0.734375
train loss:  0.5633320212364197
train gradient:  0.1400811091848111
iteration : 1113
train acc:  0.7734375
train loss:  0.4515284299850464
train gradient:  0.10663585019178055
iteration : 1114
train acc:  0.734375
train loss:  0.45441168546676636
train gradient:  0.09497742656726126
iteration : 1115
train acc:  0.7421875
train loss:  0.47277504205703735
train gradient:  0.10582950371992923
iteration : 1116
train acc:  0.7578125
train loss:  0.45379823446273804
train gradient:  0.10492901555730653
iteration : 1117
train acc:  0.7421875
train loss:  0.4577707052230835
train gradient:  0.10511350703681141
iteration : 1118
train acc:  0.8203125
train loss:  0.41611212491989136
train gradient:  0.08709819479755349
iteration : 1119
train acc:  0.78125
train loss:  0.4788639545440674
train gradient:  0.12042541085484276
iteration : 1120
train acc:  0.7265625
train loss:  0.4919728636741638
train gradient:  0.11323317687558956
iteration : 1121
train acc:  0.75
train loss:  0.46158304810523987
train gradient:  0.09238544890962447
iteration : 1122
train acc:  0.71875
train loss:  0.47770869731903076
train gradient:  0.14576360041814151
iteration : 1123
train acc:  0.8359375
train loss:  0.4157567620277405
train gradient:  0.09555151512806873
iteration : 1124
train acc:  0.828125
train loss:  0.43564337491989136
train gradient:  0.10267506566047442
iteration : 1125
train acc:  0.7578125
train loss:  0.48686307668685913
train gradient:  0.1361862208795853
iteration : 1126
train acc:  0.8125
train loss:  0.4468494951725006
train gradient:  0.0894443251356179
iteration : 1127
train acc:  0.7734375
train loss:  0.4375327229499817
train gradient:  0.1008350655380367
iteration : 1128
train acc:  0.765625
train loss:  0.4385967254638672
train gradient:  0.09511738179645246
iteration : 1129
train acc:  0.78125
train loss:  0.5147566795349121
train gradient:  0.17525376477669688
iteration : 1130
train acc:  0.75
train loss:  0.4978834390640259
train gradient:  0.16386126309636767
iteration : 1131
train acc:  0.7109375
train loss:  0.533169150352478
train gradient:  0.13888037701369993
iteration : 1132
train acc:  0.734375
train loss:  0.4488220810890198
train gradient:  0.11435266358329066
iteration : 1133
train acc:  0.734375
train loss:  0.4827035069465637
train gradient:  0.12117740695332523
iteration : 1134
train acc:  0.8046875
train loss:  0.403672456741333
train gradient:  0.12501895096914115
iteration : 1135
train acc:  0.7890625
train loss:  0.43995195627212524
train gradient:  0.1219704209354459
iteration : 1136
train acc:  0.734375
train loss:  0.4845554828643799
train gradient:  0.12510004103142341
iteration : 1137
train acc:  0.7578125
train loss:  0.49424344301223755
train gradient:  0.12245210410438168
iteration : 1138
train acc:  0.7421875
train loss:  0.4999096393585205
train gradient:  0.12760949308225095
iteration : 1139
train acc:  0.796875
train loss:  0.437148779630661
train gradient:  0.11426160834347274
iteration : 1140
train acc:  0.7265625
train loss:  0.5392314195632935
train gradient:  0.1254629900757665
iteration : 1141
train acc:  0.7890625
train loss:  0.4326167106628418
train gradient:  0.0997833746623987
iteration : 1142
train acc:  0.7421875
train loss:  0.4542442560195923
train gradient:  0.10094242462790418
iteration : 1143
train acc:  0.7890625
train loss:  0.46962785720825195
train gradient:  0.12580681956511097
iteration : 1144
train acc:  0.8046875
train loss:  0.39702966809272766
train gradient:  0.08371529992232393
iteration : 1145
train acc:  0.8046875
train loss:  0.4197191596031189
train gradient:  0.10712656069375505
iteration : 1146
train acc:  0.71875
train loss:  0.49009138345718384
train gradient:  0.13312957142404103
iteration : 1147
train acc:  0.7421875
train loss:  0.4866331219673157
train gradient:  0.10454563103822541
iteration : 1148
train acc:  0.75
train loss:  0.5087975859642029
train gradient:  0.1369163422230879
iteration : 1149
train acc:  0.75
train loss:  0.46170997619628906
train gradient:  0.08941869172604214
iteration : 1150
train acc:  0.6953125
train loss:  0.5435872077941895
train gradient:  0.12745781035271508
iteration : 1151
train acc:  0.71875
train loss:  0.48210933804512024
train gradient:  0.11558266802444317
iteration : 1152
train acc:  0.7890625
train loss:  0.45367732644081116
train gradient:  0.0874665968592103
iteration : 1153
train acc:  0.7890625
train loss:  0.4135063886642456
train gradient:  0.08401258413890791
iteration : 1154
train acc:  0.828125
train loss:  0.43091294169425964
train gradient:  0.11042266870160074
iteration : 1155
train acc:  0.7578125
train loss:  0.4656447768211365
train gradient:  0.10216520198413695
iteration : 1156
train acc:  0.734375
train loss:  0.5195467472076416
train gradient:  0.16913115512006427
iteration : 1157
train acc:  0.6796875
train loss:  0.5388929843902588
train gradient:  0.1300448129604117
iteration : 1158
train acc:  0.7421875
train loss:  0.4902057647705078
train gradient:  0.11216982448460831
iteration : 1159
train acc:  0.7109375
train loss:  0.5636634826660156
train gradient:  0.14044917019357628
iteration : 1160
train acc:  0.78125
train loss:  0.4577995836734772
train gradient:  0.10737313790228314
iteration : 1161
train acc:  0.8125
train loss:  0.37593817710876465
train gradient:  0.08918501243655923
iteration : 1162
train acc:  0.7890625
train loss:  0.4063977301120758
train gradient:  0.07930465004520387
iteration : 1163
train acc:  0.8125
train loss:  0.40467071533203125
train gradient:  0.07566790198287456
iteration : 1164
train acc:  0.734375
train loss:  0.4666717052459717
train gradient:  0.11864323840151855
iteration : 1165
train acc:  0.7109375
train loss:  0.49313050508499146
train gradient:  0.10895516339118233
iteration : 1166
train acc:  0.75
train loss:  0.5326755046844482
train gradient:  0.15373042728087064
iteration : 1167
train acc:  0.765625
train loss:  0.5354909300804138
train gradient:  0.1290706392267244
iteration : 1168
train acc:  0.703125
train loss:  0.5133123397827148
train gradient:  0.15275556505268345
iteration : 1169
train acc:  0.7578125
train loss:  0.46151331067085266
train gradient:  0.09972762393285742
iteration : 1170
train acc:  0.7421875
train loss:  0.4539012312889099
train gradient:  0.12023567612567217
iteration : 1171
train acc:  0.75
train loss:  0.43120643496513367
train gradient:  0.09477639460562003
iteration : 1172
train acc:  0.71875
train loss:  0.4830920994281769
train gradient:  0.10532877373129455
iteration : 1173
train acc:  0.8359375
train loss:  0.4184475541114807
train gradient:  0.0947843743779175
iteration : 1174
train acc:  0.78125
train loss:  0.4392091631889343
train gradient:  0.09047443296469021
iteration : 1175
train acc:  0.75
train loss:  0.45705848932266235
train gradient:  0.12289438958723865
iteration : 1176
train acc:  0.7578125
train loss:  0.4697129726409912
train gradient:  0.13417554997189352
iteration : 1177
train acc:  0.7265625
train loss:  0.4651663303375244
train gradient:  0.09468236982920304
iteration : 1178
train acc:  0.765625
train loss:  0.46452850103378296
train gradient:  0.12739919117843956
iteration : 1179
train acc:  0.703125
train loss:  0.5898077487945557
train gradient:  0.14713257188741943
iteration : 1180
train acc:  0.765625
train loss:  0.44917964935302734
train gradient:  0.10870433829483682
iteration : 1181
train acc:  0.7109375
train loss:  0.5795902013778687
train gradient:  0.17444531233843835
iteration : 1182
train acc:  0.703125
train loss:  0.5106726288795471
train gradient:  0.11350249149734346
iteration : 1183
train acc:  0.78125
train loss:  0.4474998712539673
train gradient:  0.10189507633044126
iteration : 1184
train acc:  0.7890625
train loss:  0.41718125343322754
train gradient:  0.09889465869307257
iteration : 1185
train acc:  0.796875
train loss:  0.4724978506565094
train gradient:  0.09956319379259995
iteration : 1186
train acc:  0.7734375
train loss:  0.4179854989051819
train gradient:  0.10583619084408549
iteration : 1187
train acc:  0.78125
train loss:  0.45036864280700684
train gradient:  0.11450818616169299
iteration : 1188
train acc:  0.8125
train loss:  0.41559797525405884
train gradient:  0.08085389153780628
iteration : 1189
train acc:  0.7265625
train loss:  0.543021559715271
train gradient:  0.12205288394743835
iteration : 1190
train acc:  0.7578125
train loss:  0.49664998054504395
train gradient:  0.09671436303753664
iteration : 1191
train acc:  0.7109375
train loss:  0.5488333702087402
train gradient:  0.13585156170081028
iteration : 1192
train acc:  0.6796875
train loss:  0.5660590529441833
train gradient:  0.1882656780905423
iteration : 1193
train acc:  0.765625
train loss:  0.5019961595535278
train gradient:  0.12498540875110188
iteration : 1194
train acc:  0.65625
train loss:  0.5636084079742432
train gradient:  0.14282548631279074
iteration : 1195
train acc:  0.7890625
train loss:  0.4342290163040161
train gradient:  0.10066496172314882
iteration : 1196
train acc:  0.8046875
train loss:  0.4150780737400055
train gradient:  0.09249920435806432
iteration : 1197
train acc:  0.8359375
train loss:  0.4227818250656128
train gradient:  0.1032745712789034
iteration : 1198
train acc:  0.7421875
train loss:  0.4516589641571045
train gradient:  0.10000507475449572
iteration : 1199
train acc:  0.7578125
train loss:  0.4835001230239868
train gradient:  0.12738626044098134
iteration : 1200
train acc:  0.8046875
train loss:  0.4238014817237854
train gradient:  0.07358811971157295
iteration : 1201
train acc:  0.734375
train loss:  0.48670291900634766
train gradient:  0.1378533404649256
iteration : 1202
train acc:  0.75
train loss:  0.6009513139724731
train gradient:  0.15229460403726713
iteration : 1203
train acc:  0.71875
train loss:  0.5439287424087524
train gradient:  0.14364597249415795
iteration : 1204
train acc:  0.71875
train loss:  0.5791283249855042
train gradient:  0.13655283830816267
iteration : 1205
train acc:  0.78125
train loss:  0.5265579223632812
train gradient:  0.15502161688869198
iteration : 1206
train acc:  0.7890625
train loss:  0.47303521633148193
train gradient:  0.1456434029124002
iteration : 1207
train acc:  0.7734375
train loss:  0.45087748765945435
train gradient:  0.09221086464180803
iteration : 1208
train acc:  0.7421875
train loss:  0.5007132291793823
train gradient:  0.10982561795606015
iteration : 1209
train acc:  0.765625
train loss:  0.44069352746009827
train gradient:  0.083929315861543
iteration : 1210
train acc:  0.734375
train loss:  0.47025641798973083
train gradient:  0.10817415758456735
iteration : 1211
train acc:  0.75
train loss:  0.46418634057044983
train gradient:  0.11603621540052117
iteration : 1212
train acc:  0.7734375
train loss:  0.4319496154785156
train gradient:  0.12058020739371302
iteration : 1213
train acc:  0.7890625
train loss:  0.3753649592399597
train gradient:  0.07318604561405999
iteration : 1214
train acc:  0.78125
train loss:  0.4292405843734741
train gradient:  0.0862137596501279
iteration : 1215
train acc:  0.7734375
train loss:  0.4592651128768921
train gradient:  0.11140032240687853
iteration : 1216
train acc:  0.6875
train loss:  0.4804946482181549
train gradient:  0.15623534772345177
iteration : 1217
train acc:  0.828125
train loss:  0.4382553696632385
train gradient:  0.10388783497800752
iteration : 1218
train acc:  0.7734375
train loss:  0.4638221859931946
train gradient:  0.1365589022873912
iteration : 1219
train acc:  0.75
train loss:  0.43865084648132324
train gradient:  0.09377650339226463
iteration : 1220
train acc:  0.75
train loss:  0.5001002550125122
train gradient:  0.11447348065660332
iteration : 1221
train acc:  0.765625
train loss:  0.47163766622543335
train gradient:  0.10010346093928325
iteration : 1222
train acc:  0.7421875
train loss:  0.4688115417957306
train gradient:  0.0954618354912526
iteration : 1223
train acc:  0.7578125
train loss:  0.5414629578590393
train gradient:  0.1420265765020537
iteration : 1224
train acc:  0.7265625
train loss:  0.47644731402397156
train gradient:  0.09507940242112554
iteration : 1225
train acc:  0.6875
train loss:  0.5179933309555054
train gradient:  0.10713013093217333
iteration : 1226
train acc:  0.7890625
train loss:  0.41807618737220764
train gradient:  0.09425097800434985
iteration : 1227
train acc:  0.7890625
train loss:  0.4609324634075165
train gradient:  0.12220019077790645
iteration : 1228
train acc:  0.734375
train loss:  0.4804275333881378
train gradient:  0.11515050743736091
iteration : 1229
train acc:  0.765625
train loss:  0.44145768880844116
train gradient:  0.10694264290194137
iteration : 1230
train acc:  0.7421875
train loss:  0.4531364142894745
train gradient:  0.11187025988998443
iteration : 1231
train acc:  0.8125
train loss:  0.4436919689178467
train gradient:  0.09573824429693244
iteration : 1232
train acc:  0.796875
train loss:  0.45157426595687866
train gradient:  0.12022880414316114
iteration : 1233
train acc:  0.71875
train loss:  0.5421419143676758
train gradient:  0.1386345615397478
iteration : 1234
train acc:  0.765625
train loss:  0.4773710072040558
train gradient:  0.11320376655227851
iteration : 1235
train acc:  0.6796875
train loss:  0.5644075274467468
train gradient:  0.13560824637438967
iteration : 1236
train acc:  0.7109375
train loss:  0.48868516087532043
train gradient:  0.12709295534230053
iteration : 1237
train acc:  0.78125
train loss:  0.4690935015678406
train gradient:  0.13480219992535566
iteration : 1238
train acc:  0.75
train loss:  0.4895938038825989
train gradient:  0.11636115905604565
iteration : 1239
train acc:  0.7734375
train loss:  0.43817993998527527
train gradient:  0.10294545623108262
iteration : 1240
train acc:  0.7578125
train loss:  0.5154590606689453
train gradient:  0.10798386075432552
iteration : 1241
train acc:  0.8203125
train loss:  0.40856534242630005
train gradient:  0.07886300496462197
iteration : 1242
train acc:  0.765625
train loss:  0.48341548442840576
train gradient:  0.11994327333777915
iteration : 1243
train acc:  0.7578125
train loss:  0.5619386434555054
train gradient:  0.1254049605272747
iteration : 1244
train acc:  0.71875
train loss:  0.446269690990448
train gradient:  0.10221117740689925
iteration : 1245
train acc:  0.6953125
train loss:  0.5355592966079712
train gradient:  0.15463765110707167
iteration : 1246
train acc:  0.75
train loss:  0.47513845562934875
train gradient:  0.10965467751786506
iteration : 1247
train acc:  0.8046875
train loss:  0.4722174406051636
train gradient:  0.12125587921689167
iteration : 1248
train acc:  0.7734375
train loss:  0.42734861373901367
train gradient:  0.1125765339519532
iteration : 1249
train acc:  0.7890625
train loss:  0.40144431591033936
train gradient:  0.08902432655097962
iteration : 1250
train acc:  0.75
train loss:  0.4779905676841736
train gradient:  0.12808274678349318
iteration : 1251
train acc:  0.7421875
train loss:  0.5128980278968811
train gradient:  0.11770145610805796
iteration : 1252
train acc:  0.78125
train loss:  0.42468535900115967
train gradient:  0.09010871694749437
iteration : 1253
train acc:  0.75
train loss:  0.47470787167549133
train gradient:  0.1080920848538122
iteration : 1254
train acc:  0.8125
train loss:  0.41904112696647644
train gradient:  0.08884727622594102
iteration : 1255
train acc:  0.75
train loss:  0.4971482753753662
train gradient:  0.14363045781331296
iteration : 1256
train acc:  0.7890625
train loss:  0.39989686012268066
train gradient:  0.11518142156649862
iteration : 1257
train acc:  0.734375
train loss:  0.5327567458152771
train gradient:  0.13649118949286299
iteration : 1258
train acc:  0.7265625
train loss:  0.5054060816764832
train gradient:  0.12715289567484084
iteration : 1259
train acc:  0.7265625
train loss:  0.4825429916381836
train gradient:  0.10358407291670015
iteration : 1260
train acc:  0.7421875
train loss:  0.4396902918815613
train gradient:  0.10384798600045556
iteration : 1261
train acc:  0.7109375
train loss:  0.48213309049606323
train gradient:  0.11943247902519363
iteration : 1262
train acc:  0.7578125
train loss:  0.45662444829940796
train gradient:  0.10173291249812588
iteration : 1263
train acc:  0.75
train loss:  0.49730169773101807
train gradient:  0.12313567306891325
iteration : 1264
train acc:  0.765625
train loss:  0.48606836795806885
train gradient:  0.13232420323630356
iteration : 1265
train acc:  0.8046875
train loss:  0.47267261147499084
train gradient:  0.11369490772371556
iteration : 1266
train acc:  0.734375
train loss:  0.49915987253189087
train gradient:  0.1464807860363946
iteration : 1267
train acc:  0.796875
train loss:  0.41508615016937256
train gradient:  0.09291835062413453
iteration : 1268
train acc:  0.703125
train loss:  0.5636380314826965
train gradient:  0.1387549264484999
iteration : 1269
train acc:  0.8046875
train loss:  0.4216355085372925
train gradient:  0.08927497833114681
iteration : 1270
train acc:  0.75
train loss:  0.501825213432312
train gradient:  0.13803983982598367
iteration : 1271
train acc:  0.6875
train loss:  0.567903995513916
train gradient:  0.1654848446866305
iteration : 1272
train acc:  0.6953125
train loss:  0.5277410745620728
train gradient:  0.12719845187819218
iteration : 1273
train acc:  0.796875
train loss:  0.4285162687301636
train gradient:  0.11485374738613166
iteration : 1274
train acc:  0.7265625
train loss:  0.534619152545929
train gradient:  0.14944231301884678
iteration : 1275
train acc:  0.78125
train loss:  0.4501206874847412
train gradient:  0.10055932665656638
iteration : 1276
train acc:  0.8125
train loss:  0.42063337564468384
train gradient:  0.07940226489538758
iteration : 1277
train acc:  0.7734375
train loss:  0.516061544418335
train gradient:  0.1508652142756992
iteration : 1278
train acc:  0.828125
train loss:  0.3958549499511719
train gradient:  0.08705100733896351
iteration : 1279
train acc:  0.671875
train loss:  0.5989711284637451
train gradient:  0.15605670412543604
iteration : 1280
train acc:  0.6953125
train loss:  0.4909307360649109
train gradient:  0.08903371614928918
iteration : 1281
train acc:  0.796875
train loss:  0.4127063751220703
train gradient:  0.07762362840852023
iteration : 1282
train acc:  0.765625
train loss:  0.4488781988620758
train gradient:  0.09496144441457172
iteration : 1283
train acc:  0.75
train loss:  0.45446884632110596
train gradient:  0.09816166216371204
iteration : 1284
train acc:  0.7734375
train loss:  0.4362650513648987
train gradient:  0.10422378689823344
iteration : 1285
train acc:  0.765625
train loss:  0.4296220541000366
train gradient:  0.08824765402419234
iteration : 1286
train acc:  0.7421875
train loss:  0.47315818071365356
train gradient:  0.13335471220517675
iteration : 1287
train acc:  0.7109375
train loss:  0.5604960918426514
train gradient:  0.14642599036982568
iteration : 1288
train acc:  0.6875
train loss:  0.5544832348823547
train gradient:  0.12507591582013305
iteration : 1289
train acc:  0.7578125
train loss:  0.4708981513977051
train gradient:  0.12627881246217088
iteration : 1290
train acc:  0.7734375
train loss:  0.4712159037590027
train gradient:  0.09651496219749656
iteration : 1291
train acc:  0.7890625
train loss:  0.44321638345718384
train gradient:  0.09612500285079388
iteration : 1292
train acc:  0.78125
train loss:  0.497724711894989
train gradient:  0.17592464025540958
iteration : 1293
train acc:  0.8046875
train loss:  0.44626757502555847
train gradient:  0.12557989418616902
iteration : 1294
train acc:  0.7421875
train loss:  0.45333635807037354
train gradient:  0.10604529400988995
iteration : 1295
train acc:  0.78125
train loss:  0.4119560122489929
train gradient:  0.10186410109994581
iteration : 1296
train acc:  0.7421875
train loss:  0.4629814326763153
train gradient:  0.10866957895124965
iteration : 1297
train acc:  0.7578125
train loss:  0.4103921055793762
train gradient:  0.09404380885660402
iteration : 1298
train acc:  0.7734375
train loss:  0.5023574829101562
train gradient:  0.1254297862230624
iteration : 1299
train acc:  0.7109375
train loss:  0.4820454716682434
train gradient:  0.08468771943603061
iteration : 1300
train acc:  0.734375
train loss:  0.5465903282165527
train gradient:  0.16451368427582058
iteration : 1301
train acc:  0.8046875
train loss:  0.45129263401031494
train gradient:  0.09700349690626688
iteration : 1302
train acc:  0.7421875
train loss:  0.488943487405777
train gradient:  0.12068724784337691
iteration : 1303
train acc:  0.7890625
train loss:  0.40340733528137207
train gradient:  0.07240639987509022
iteration : 1304
train acc:  0.796875
train loss:  0.4466015696525574
train gradient:  0.10160154569241793
iteration : 1305
train acc:  0.734375
train loss:  0.5199043154716492
train gradient:  0.14604215848430052
iteration : 1306
train acc:  0.75
train loss:  0.4877770245075226
train gradient:  0.1460305241782556
iteration : 1307
train acc:  0.828125
train loss:  0.3574306070804596
train gradient:  0.06715528909078272
iteration : 1308
train acc:  0.8125
train loss:  0.4217762053012848
train gradient:  0.09015847116337025
iteration : 1309
train acc:  0.796875
train loss:  0.48660874366760254
train gradient:  0.10288925182646336
iteration : 1310
train acc:  0.765625
train loss:  0.47649839520454407
train gradient:  0.16487467744663997
iteration : 1311
train acc:  0.734375
train loss:  0.45511308312416077
train gradient:  0.1331050204429531
iteration : 1312
train acc:  0.75
train loss:  0.46361976861953735
train gradient:  0.09067940976865846
iteration : 1313
train acc:  0.7578125
train loss:  0.45440709590911865
train gradient:  0.11185725445810542
iteration : 1314
train acc:  0.7578125
train loss:  0.478630006313324
train gradient:  0.12360341969571836
iteration : 1315
train acc:  0.7265625
train loss:  0.5130394697189331
train gradient:  0.1287524264736338
iteration : 1316
train acc:  0.8046875
train loss:  0.4775514602661133
train gradient:  0.1433620719877424
iteration : 1317
train acc:  0.7421875
train loss:  0.4610695242881775
train gradient:  0.11848490460142136
iteration : 1318
train acc:  0.78125
train loss:  0.4610917866230011
train gradient:  0.13665939033997446
iteration : 1319
train acc:  0.7734375
train loss:  0.44478726387023926
train gradient:  0.10494676638742874
iteration : 1320
train acc:  0.703125
train loss:  0.4947378635406494
train gradient:  0.13385415077579102
iteration : 1321
train acc:  0.7734375
train loss:  0.46044835448265076
train gradient:  0.08685104958749401
iteration : 1322
train acc:  0.734375
train loss:  0.4995102286338806
train gradient:  0.14721843819211206
iteration : 1323
train acc:  0.8125
train loss:  0.39511772990226746
train gradient:  0.08553582600179634
iteration : 1324
train acc:  0.7421875
train loss:  0.48571738600730896
train gradient:  0.12146496336521068
iteration : 1325
train acc:  0.78125
train loss:  0.47057706117630005
train gradient:  0.10549676844274831
iteration : 1326
train acc:  0.7734375
train loss:  0.42974820733070374
train gradient:  0.08781126146290287
iteration : 1327
train acc:  0.7578125
train loss:  0.45941683650016785
train gradient:  0.12073022658978816
iteration : 1328
train acc:  0.7421875
train loss:  0.4791333079338074
train gradient:  0.12034106008160628
iteration : 1329
train acc:  0.7890625
train loss:  0.47481971979141235
train gradient:  0.1161445161316496
iteration : 1330
train acc:  0.7265625
train loss:  0.5019538998603821
train gradient:  0.11468207113824512
iteration : 1331
train acc:  0.796875
train loss:  0.44496315717697144
train gradient:  0.08565253848425447
iteration : 1332
train acc:  0.7421875
train loss:  0.4488704800605774
train gradient:  0.12244452644805134
iteration : 1333
train acc:  0.796875
train loss:  0.3959633708000183
train gradient:  0.11086104365706637
iteration : 1334
train acc:  0.734375
train loss:  0.47271624207496643
train gradient:  0.11728723497858627
iteration : 1335
train acc:  0.828125
train loss:  0.4362325072288513
train gradient:  0.1169580095795575
iteration : 1336
train acc:  0.71875
train loss:  0.5119553804397583
train gradient:  0.15997320036768087
iteration : 1337
train acc:  0.7578125
train loss:  0.42177659273147583
train gradient:  0.11235569892267115
iteration : 1338
train acc:  0.8125
train loss:  0.4041815996170044
train gradient:  0.09258232339609695
iteration : 1339
train acc:  0.78125
train loss:  0.44485318660736084
train gradient:  0.09347468853463818
iteration : 1340
train acc:  0.796875
train loss:  0.4453529119491577
train gradient:  0.10326079946167153
iteration : 1341
train acc:  0.7734375
train loss:  0.47251608967781067
train gradient:  0.11571425964678438
iteration : 1342
train acc:  0.75
train loss:  0.4882160425186157
train gradient:  0.12262138667875855
iteration : 1343
train acc:  0.734375
train loss:  0.5069863796234131
train gradient:  0.14381077753469046
iteration : 1344
train acc:  0.734375
train loss:  0.4967343807220459
train gradient:  0.12267082180578531
iteration : 1345
train acc:  0.6796875
train loss:  0.5443153381347656
train gradient:  0.13992989681957624
iteration : 1346
train acc:  0.6875
train loss:  0.5694757699966431
train gradient:  0.16670171616449275
iteration : 1347
train acc:  0.7109375
train loss:  0.5066704750061035
train gradient:  0.1579418213401495
iteration : 1348
train acc:  0.734375
train loss:  0.5355815887451172
train gradient:  0.14698168417021418
iteration : 1349
train acc:  0.8203125
train loss:  0.41025441884994507
train gradient:  0.07821895495841978
iteration : 1350
train acc:  0.7578125
train loss:  0.4325751066207886
train gradient:  0.0981228915513486
iteration : 1351
train acc:  0.75
train loss:  0.5285043716430664
train gradient:  0.13203607519638771
iteration : 1352
train acc:  0.75
train loss:  0.4426073133945465
train gradient:  0.10532449103582295
iteration : 1353
train acc:  0.765625
train loss:  0.4446130394935608
train gradient:  0.10690534692829687
iteration : 1354
train acc:  0.7109375
train loss:  0.5014174580574036
train gradient:  0.11054219908012114
iteration : 1355
train acc:  0.71875
train loss:  0.5228800177574158
train gradient:  0.12165014444541888
iteration : 1356
train acc:  0.7890625
train loss:  0.4353271424770355
train gradient:  0.09375037702919004
iteration : 1357
train acc:  0.734375
train loss:  0.4905308485031128
train gradient:  0.12194097926975041
iteration : 1358
train acc:  0.7421875
train loss:  0.47957664728164673
train gradient:  0.11399939461315224
iteration : 1359
train acc:  0.765625
train loss:  0.4438219964504242
train gradient:  0.08388774231874604
iteration : 1360
train acc:  0.78125
train loss:  0.4469042420387268
train gradient:  0.13684128839698406
iteration : 1361
train acc:  0.734375
train loss:  0.5149977207183838
train gradient:  0.14258970925325964
iteration : 1362
train acc:  0.7265625
train loss:  0.5242275595664978
train gradient:  0.12099172588980328
iteration : 1363
train acc:  0.78125
train loss:  0.49750542640686035
train gradient:  0.11516436131032896
iteration : 1364
train acc:  0.7109375
train loss:  0.5011007785797119
train gradient:  0.11168682605360263
iteration : 1365
train acc:  0.703125
train loss:  0.5066177845001221
train gradient:  0.10487456182072956
iteration : 1366
train acc:  0.8046875
train loss:  0.44833871722221375
train gradient:  0.09007726008616386
iteration : 1367
train acc:  0.75
train loss:  0.49110299348831177
train gradient:  0.13154861323768918
iteration : 1368
train acc:  0.859375
train loss:  0.4181855320930481
train gradient:  0.1130408754020339
iteration : 1369
train acc:  0.8046875
train loss:  0.4464004933834076
train gradient:  0.1046157776770969
iteration : 1370
train acc:  0.7578125
train loss:  0.4498272240161896
train gradient:  0.09710195183129647
iteration : 1371
train acc:  0.7578125
train loss:  0.4482942819595337
train gradient:  0.11261149821170155
iteration : 1372
train acc:  0.765625
train loss:  0.48109135031700134
train gradient:  0.0865890970230185
iteration : 1373
train acc:  0.8046875
train loss:  0.4882873296737671
train gradient:  0.12309423475818919
iteration : 1374
train acc:  0.75
train loss:  0.47707152366638184
train gradient:  0.10679144956509277
iteration : 1375
train acc:  0.7421875
train loss:  0.512298583984375
train gradient:  0.115598769173135
iteration : 1376
train acc:  0.6796875
train loss:  0.5280229449272156
train gradient:  0.12316114380580293
iteration : 1377
train acc:  0.71875
train loss:  0.47027599811553955
train gradient:  0.09957234217668437
iteration : 1378
train acc:  0.71875
train loss:  0.515221357345581
train gradient:  0.14589317522250134
iteration : 1379
train acc:  0.828125
train loss:  0.41149967908859253
train gradient:  0.09109260431643776
iteration : 1380
train acc:  0.7109375
train loss:  0.5534340143203735
train gradient:  0.14027191506825276
iteration : 1381
train acc:  0.7421875
train loss:  0.5053207874298096
train gradient:  0.12512295491165415
iteration : 1382
train acc:  0.8203125
train loss:  0.45079630613327026
train gradient:  0.09227763992454066
iteration : 1383
train acc:  0.765625
train loss:  0.4587138295173645
train gradient:  0.08574756849310253
iteration : 1384
train acc:  0.7421875
train loss:  0.4912559390068054
train gradient:  0.10038047309557592
iteration : 1385
train acc:  0.7578125
train loss:  0.4416278898715973
train gradient:  0.09664329038729509
iteration : 1386
train acc:  0.7421875
train loss:  0.4719155430793762
train gradient:  0.1101243169038944
iteration : 1387
train acc:  0.734375
train loss:  0.45956671237945557
train gradient:  0.13411904166905203
iteration : 1388
train acc:  0.7109375
train loss:  0.48752081394195557
train gradient:  0.1052599315466246
iteration : 1389
train acc:  0.75
train loss:  0.46346715092658997
train gradient:  0.09144985181749081
iteration : 1390
train acc:  0.75
train loss:  0.48719292879104614
train gradient:  0.12957482305345108
iteration : 1391
train acc:  0.7265625
train loss:  0.4717866778373718
train gradient:  0.09173999412807396
iteration : 1392
train acc:  0.71875
train loss:  0.4959961771965027
train gradient:  0.12680231770028488
iteration : 1393
train acc:  0.7421875
train loss:  0.46976056694984436
train gradient:  0.10259025742640389
iteration : 1394
train acc:  0.7265625
train loss:  0.5269367098808289
train gradient:  0.1336013818504054
iteration : 1395
train acc:  0.734375
train loss:  0.4861077070236206
train gradient:  0.1389916074337592
iteration : 1396
train acc:  0.7265625
train loss:  0.4597324728965759
train gradient:  0.12228502723740475
iteration : 1397
train acc:  0.734375
train loss:  0.49406886100769043
train gradient:  0.12209910985070803
iteration : 1398
train acc:  0.7109375
train loss:  0.530134916305542
train gradient:  0.11790204392039656
iteration : 1399
train acc:  0.703125
train loss:  0.5819352865219116
train gradient:  0.16932470419084467
iteration : 1400
train acc:  0.71875
train loss:  0.5327937006950378
train gradient:  0.14771089543581345
iteration : 1401
train acc:  0.75
train loss:  0.455236554145813
train gradient:  0.10720186253439425
iteration : 1402
train acc:  0.703125
train loss:  0.5294577479362488
train gradient:  0.1385065702879586
iteration : 1403
train acc:  0.7421875
train loss:  0.4356100559234619
train gradient:  0.10703061814606994
iteration : 1404
train acc:  0.7265625
train loss:  0.5529086589813232
train gradient:  0.1890654819993461
iteration : 1405
train acc:  0.8125
train loss:  0.44270196557044983
train gradient:  0.13939893401816328
iteration : 1406
train acc:  0.734375
train loss:  0.4837909936904907
train gradient:  0.13254206259222354
iteration : 1407
train acc:  0.6796875
train loss:  0.5297268629074097
train gradient:  0.1360385331605076
iteration : 1408
train acc:  0.703125
train loss:  0.5084062218666077
train gradient:  0.13385090088034401
iteration : 1409
train acc:  0.7578125
train loss:  0.49130284786224365
train gradient:  0.13844588351716308
iteration : 1410
train acc:  0.6875
train loss:  0.537397563457489
train gradient:  0.1527764876491482
iteration : 1411
train acc:  0.7109375
train loss:  0.5445227026939392
train gradient:  0.17275342491414328
iteration : 1412
train acc:  0.7421875
train loss:  0.4960712194442749
train gradient:  0.10628397540403978
iteration : 1413
train acc:  0.78125
train loss:  0.48457127809524536
train gradient:  0.10337948451549099
iteration : 1414
train acc:  0.765625
train loss:  0.4527932107448578
train gradient:  0.12084427280621782
iteration : 1415
train acc:  0.828125
train loss:  0.43463632464408875
train gradient:  0.13090909664378436
iteration : 1416
train acc:  0.7734375
train loss:  0.4473748803138733
train gradient:  0.12732825143612736
iteration : 1417
train acc:  0.7578125
train loss:  0.43946993350982666
train gradient:  0.08473483126882159
iteration : 1418
train acc:  0.8046875
train loss:  0.4229215085506439
train gradient:  0.09935613379724514
iteration : 1419
train acc:  0.734375
train loss:  0.5233365297317505
train gradient:  0.12395025274565379
iteration : 1420
train acc:  0.78125
train loss:  0.45189571380615234
train gradient:  0.11885975511193786
iteration : 1421
train acc:  0.75
train loss:  0.44965338706970215
train gradient:  0.10463072695291532
iteration : 1422
train acc:  0.828125
train loss:  0.37774384021759033
train gradient:  0.10933099301018383
iteration : 1423
train acc:  0.7421875
train loss:  0.5206115245819092
train gradient:  0.12202051326949766
iteration : 1424
train acc:  0.71875
train loss:  0.5041043162345886
train gradient:  0.11245389318540092
iteration : 1425
train acc:  0.7265625
train loss:  0.4664335548877716
train gradient:  0.11970352375853782
iteration : 1426
train acc:  0.765625
train loss:  0.4538727402687073
train gradient:  0.11324636922953953
iteration : 1427
train acc:  0.6875
train loss:  0.5068873167037964
train gradient:  0.10760683417453297
iteration : 1428
train acc:  0.796875
train loss:  0.4099617898464203
train gradient:  0.10835547063689237
iteration : 1429
train acc:  0.7578125
train loss:  0.45774996280670166
train gradient:  0.10458111702627423
iteration : 1430
train acc:  0.796875
train loss:  0.41439419984817505
train gradient:  0.08154198549963035
iteration : 1431
train acc:  0.7265625
train loss:  0.5349108576774597
train gradient:  0.14057202572444708
iteration : 1432
train acc:  0.7734375
train loss:  0.468750923871994
train gradient:  0.1177016921549579
iteration : 1433
train acc:  0.75
train loss:  0.5140889883041382
train gradient:  0.14526808867536095
iteration : 1434
train acc:  0.7421875
train loss:  0.419823557138443
train gradient:  0.12160917489945851
iteration : 1435
train acc:  0.765625
train loss:  0.4650834798812866
train gradient:  0.10390632345332473
iteration : 1436
train acc:  0.703125
train loss:  0.46865686774253845
train gradient:  0.14554662647360994
iteration : 1437
train acc:  0.6953125
train loss:  0.5439323782920837
train gradient:  0.14164342196209373
iteration : 1438
train acc:  0.7265625
train loss:  0.5063587427139282
train gradient:  0.09966893645202933
iteration : 1439
train acc:  0.765625
train loss:  0.492623507976532
train gradient:  0.10160641930649941
iteration : 1440
train acc:  0.78125
train loss:  0.435945600271225
train gradient:  0.09547825844398569
iteration : 1441
train acc:  0.7109375
train loss:  0.5585823655128479
train gradient:  0.14734800410000903
iteration : 1442
train acc:  0.7734375
train loss:  0.40797895193099976
train gradient:  0.10300733929191655
iteration : 1443
train acc:  0.8046875
train loss:  0.42932769656181335
train gradient:  0.1013913927620006
iteration : 1444
train acc:  0.765625
train loss:  0.4564052224159241
train gradient:  0.12246607068435436
iteration : 1445
train acc:  0.8359375
train loss:  0.4028714895248413
train gradient:  0.08925078208079408
iteration : 1446
train acc:  0.703125
train loss:  0.4719063341617584
train gradient:  0.11538444621545998
iteration : 1447
train acc:  0.78125
train loss:  0.45699572563171387
train gradient:  0.12571391810215105
iteration : 1448
train acc:  0.7265625
train loss:  0.5141982436180115
train gradient:  0.1332491219477409
iteration : 1449
train acc:  0.7109375
train loss:  0.48679959774017334
train gradient:  0.11032228225284019
iteration : 1450
train acc:  0.7421875
train loss:  0.43919235467910767
train gradient:  0.10930305479291147
iteration : 1451
train acc:  0.796875
train loss:  0.441902756690979
train gradient:  0.10504482121600967
iteration : 1452
train acc:  0.7421875
train loss:  0.4538753032684326
train gradient:  0.09438680285629016
iteration : 1453
train acc:  0.734375
train loss:  0.5304304957389832
train gradient:  0.16105546993052017
iteration : 1454
train acc:  0.75
train loss:  0.45936834812164307
train gradient:  0.12191737694322377
iteration : 1455
train acc:  0.703125
train loss:  0.5214790105819702
train gradient:  0.11842567097732958
iteration : 1456
train acc:  0.765625
train loss:  0.46948152780532837
train gradient:  0.12930037851979675
iteration : 1457
train acc:  0.7734375
train loss:  0.4986020624637604
train gradient:  0.13198127087153028
iteration : 1458
train acc:  0.734375
train loss:  0.4614616632461548
train gradient:  0.09798550619727162
iteration : 1459
train acc:  0.7109375
train loss:  0.5352580547332764
train gradient:  0.16049800861535315
iteration : 1460
train acc:  0.7578125
train loss:  0.47340893745422363
train gradient:  0.12022575371257446
iteration : 1461
train acc:  0.734375
train loss:  0.5502109527587891
train gradient:  0.13348116371580898
iteration : 1462
train acc:  0.7421875
train loss:  0.47944560647010803
train gradient:  0.10983468366785758
iteration : 1463
train acc:  0.7578125
train loss:  0.49241912364959717
train gradient:  0.11992982520065543
iteration : 1464
train acc:  0.734375
train loss:  0.5108985304832458
train gradient:  0.13041505793541958
iteration : 1465
train acc:  0.671875
train loss:  0.5470278263092041
train gradient:  0.11876740945692098
iteration : 1466
train acc:  0.7109375
train loss:  0.4923887848854065
train gradient:  0.11358109010728874
iteration : 1467
train acc:  0.796875
train loss:  0.5067819356918335
train gradient:  0.1178299859231289
iteration : 1468
train acc:  0.7265625
train loss:  0.5516942739486694
train gradient:  0.15552396628567855
iteration : 1469
train acc:  0.6875
train loss:  0.4893653094768524
train gradient:  0.09141975232034613
iteration : 1470
train acc:  0.71875
train loss:  0.51821368932724
train gradient:  0.12432291661929151
iteration : 1471
train acc:  0.734375
train loss:  0.4801710546016693
train gradient:  0.10092667247536725
iteration : 1472
train acc:  0.7734375
train loss:  0.4180868864059448
train gradient:  0.08008369979708577
iteration : 1473
train acc:  0.7265625
train loss:  0.4395062327384949
train gradient:  0.09377658086142077
iteration : 1474
train acc:  0.8046875
train loss:  0.4889792203903198
train gradient:  0.11113115737808513
iteration : 1475
train acc:  0.71875
train loss:  0.4804585576057434
train gradient:  0.13011865218827934
iteration : 1476
train acc:  0.7578125
train loss:  0.429874986410141
train gradient:  0.08393529689986853
iteration : 1477
train acc:  0.7578125
train loss:  0.4689972996711731
train gradient:  0.13132089514387563
iteration : 1478
train acc:  0.6953125
train loss:  0.5160946846008301
train gradient:  0.14246679229283674
iteration : 1479
train acc:  0.7421875
train loss:  0.5098176002502441
train gradient:  0.1143981540711624
iteration : 1480
train acc:  0.7265625
train loss:  0.5538953542709351
train gradient:  0.12871814390651276
iteration : 1481
train acc:  0.7890625
train loss:  0.39193642139434814
train gradient:  0.07956617456875187
iteration : 1482
train acc:  0.703125
train loss:  0.524233341217041
train gradient:  0.13872600598267026
iteration : 1483
train acc:  0.7890625
train loss:  0.42595958709716797
train gradient:  0.08808278943963245
iteration : 1484
train acc:  0.7421875
train loss:  0.45834529399871826
train gradient:  0.09596371052965218
iteration : 1485
train acc:  0.7421875
train loss:  0.452206552028656
train gradient:  0.10796263079595447
iteration : 1486
train acc:  0.71875
train loss:  0.4965610206127167
train gradient:  0.1372552677099403
iteration : 1487
train acc:  0.7265625
train loss:  0.4855209290981293
train gradient:  0.1352649457186757
iteration : 1488
train acc:  0.7421875
train loss:  0.4227890968322754
train gradient:  0.0818528647847592
iteration : 1489
train acc:  0.75
train loss:  0.5530524253845215
train gradient:  0.185407878470771
iteration : 1490
train acc:  0.78125
train loss:  0.4851785898208618
train gradient:  0.0998763922260638
iteration : 1491
train acc:  0.78125
train loss:  0.4160420000553131
train gradient:  0.09313722907858546
iteration : 1492
train acc:  0.78125
train loss:  0.41914433240890503
train gradient:  0.09243310897405377
iteration : 1493
train acc:  0.7890625
train loss:  0.4353848099708557
train gradient:  0.10189893921119557
iteration : 1494
train acc:  0.7421875
train loss:  0.5106115341186523
train gradient:  0.1134175882737274
iteration : 1495
train acc:  0.796875
train loss:  0.4416292905807495
train gradient:  0.0907745443503839
iteration : 1496
train acc:  0.765625
train loss:  0.46012574434280396
train gradient:  0.11725682650369947
iteration : 1497
train acc:  0.8203125
train loss:  0.4037643074989319
train gradient:  0.0793680723578201
iteration : 1498
train acc:  0.734375
train loss:  0.4715232253074646
train gradient:  0.1044683464987251
iteration : 1499
train acc:  0.7421875
train loss:  0.48003512620925903
train gradient:  0.09536811506181718
iteration : 1500
train acc:  0.75
train loss:  0.4531696140766144
train gradient:  0.08319997367434026
iteration : 1501
train acc:  0.8046875
train loss:  0.4107295870780945
train gradient:  0.1003619197175625
iteration : 1502
train acc:  0.75
train loss:  0.4633389115333557
train gradient:  0.0897995072954609
iteration : 1503
train acc:  0.7421875
train loss:  0.4427666664123535
train gradient:  0.07876187236101002
iteration : 1504
train acc:  0.7421875
train loss:  0.46222126483917236
train gradient:  0.10426953733825298
iteration : 1505
train acc:  0.7421875
train loss:  0.4836229383945465
train gradient:  0.09966945153749766
iteration : 1506
train acc:  0.765625
train loss:  0.4451999068260193
train gradient:  0.09566007208015534
iteration : 1507
train acc:  0.7578125
train loss:  0.5335280895233154
train gradient:  0.14618327758344674
iteration : 1508
train acc:  0.7890625
train loss:  0.45224523544311523
train gradient:  0.11003866736801692
iteration : 1509
train acc:  0.78125
train loss:  0.470598042011261
train gradient:  0.16997199888471337
iteration : 1510
train acc:  0.6953125
train loss:  0.5218217372894287
train gradient:  0.11953124560424301
iteration : 1511
train acc:  0.6953125
train loss:  0.5034371018409729
train gradient:  0.12583823415015377
iteration : 1512
train acc:  0.7578125
train loss:  0.4784071445465088
train gradient:  0.10143163588332635
iteration : 1513
train acc:  0.75
train loss:  0.5063616633415222
train gradient:  0.10116992534519742
iteration : 1514
train acc:  0.7265625
train loss:  0.47612839937210083
train gradient:  0.1013183819538551
iteration : 1515
train acc:  0.7265625
train loss:  0.46550682187080383
train gradient:  0.1016261608478108
iteration : 1516
train acc:  0.6796875
train loss:  0.5280123949050903
train gradient:  0.15918409436830558
iteration : 1517
train acc:  0.7109375
train loss:  0.5454269647598267
train gradient:  0.17502682310178874
iteration : 1518
train acc:  0.7578125
train loss:  0.4441092014312744
train gradient:  0.10460811331406411
iteration : 1519
train acc:  0.671875
train loss:  0.5069983601570129
train gradient:  0.11620457894307569
iteration : 1520
train acc:  0.703125
train loss:  0.4875400960445404
train gradient:  0.12420197650792655
iteration : 1521
train acc:  0.8125
train loss:  0.3969884216785431
train gradient:  0.08730123174174842
iteration : 1522
train acc:  0.7890625
train loss:  0.42831194400787354
train gradient:  0.08727031146224001
iteration : 1523
train acc:  0.703125
train loss:  0.5018460154533386
train gradient:  0.15168874248821912
iteration : 1524
train acc:  0.8046875
train loss:  0.4282914400100708
train gradient:  0.10428334896681307
iteration : 1525
train acc:  0.7734375
train loss:  0.4551805257797241
train gradient:  0.1221867663451722
iteration : 1526
train acc:  0.765625
train loss:  0.4361926317214966
train gradient:  0.09894459892058874
iteration : 1527
train acc:  0.7421875
train loss:  0.47581344842910767
train gradient:  0.1027623100563667
iteration : 1528
train acc:  0.75
train loss:  0.4645615220069885
train gradient:  0.11559168893735462
iteration : 1529
train acc:  0.7109375
train loss:  0.5125457048416138
train gradient:  0.15060824114361088
iteration : 1530
train acc:  0.7421875
train loss:  0.4781252145767212
train gradient:  0.14030328828697086
iteration : 1531
train acc:  0.71875
train loss:  0.49462249875068665
train gradient:  0.14335445045765915
iteration : 1532
train acc:  0.703125
train loss:  0.5233099460601807
train gradient:  0.14405431196816504
iteration : 1533
train acc:  0.7734375
train loss:  0.4371638894081116
train gradient:  0.09418054234290991
iteration : 1534
train acc:  0.6796875
train loss:  0.5034186244010925
train gradient:  0.12117118383352542
iteration : 1535
train acc:  0.796875
train loss:  0.44313955307006836
train gradient:  0.0905188367876164
iteration : 1536
train acc:  0.6875
train loss:  0.6404503583908081
train gradient:  0.20124750442933453
iteration : 1537
train acc:  0.765625
train loss:  0.47715309262275696
train gradient:  0.13211582446959969
iteration : 1538
train acc:  0.671875
train loss:  0.5586891174316406
train gradient:  0.16753201216547048
iteration : 1539
train acc:  0.796875
train loss:  0.422446072101593
train gradient:  0.09964045518241238
iteration : 1540
train acc:  0.734375
train loss:  0.5005172491073608
train gradient:  0.1391998640244548
iteration : 1541
train acc:  0.7734375
train loss:  0.45950978994369507
train gradient:  0.12597309271089818
iteration : 1542
train acc:  0.6953125
train loss:  0.5383914709091187
train gradient:  0.17393524631484558
iteration : 1543
train acc:  0.703125
train loss:  0.518828272819519
train gradient:  0.1594656445077035
iteration : 1544
train acc:  0.8046875
train loss:  0.4251033663749695
train gradient:  0.09392549873996335
iteration : 1545
train acc:  0.765625
train loss:  0.4903373718261719
train gradient:  0.1207719543830331
iteration : 1546
train acc:  0.796875
train loss:  0.426277220249176
train gradient:  0.09175373130936568
iteration : 1547
train acc:  0.7734375
train loss:  0.4494907259941101
train gradient:  0.11470722027019592
iteration : 1548
train acc:  0.7890625
train loss:  0.4225621223449707
train gradient:  0.10337978644435379
iteration : 1549
train acc:  0.8125
train loss:  0.4362594783306122
train gradient:  0.11727693378755269
iteration : 1550
train acc:  0.7578125
train loss:  0.46403735876083374
train gradient:  0.10655804585253098
iteration : 1551
train acc:  0.734375
train loss:  0.5653775930404663
train gradient:  0.12144681949503928
iteration : 1552
train acc:  0.7265625
train loss:  0.5093073844909668
train gradient:  0.1293545643138475
iteration : 1553
train acc:  0.7265625
train loss:  0.45986613631248474
train gradient:  0.1085980071075615
iteration : 1554
train acc:  0.75
train loss:  0.436421275138855
train gradient:  0.08626475378635935
iteration : 1555
train acc:  0.796875
train loss:  0.4533120393753052
train gradient:  0.11232560541668
iteration : 1556
train acc:  0.7421875
train loss:  0.4678136706352234
train gradient:  0.11271433380180473
iteration : 1557
train acc:  0.7421875
train loss:  0.5291436910629272
train gradient:  0.15991741012639954
iteration : 1558
train acc:  0.75
train loss:  0.5120505094528198
train gradient:  0.12941359860120683
iteration : 1559
train acc:  0.7421875
train loss:  0.5164855122566223
train gradient:  0.11844360942961428
iteration : 1560
train acc:  0.796875
train loss:  0.47061365842819214
train gradient:  0.14243424113706332
iteration : 1561
train acc:  0.6796875
train loss:  0.5619029402732849
train gradient:  0.13487177650748877
iteration : 1562
train acc:  0.7109375
train loss:  0.5106339454650879
train gradient:  0.22072300052559313
iteration : 1563
train acc:  0.78125
train loss:  0.4326309859752655
train gradient:  0.09111241356378501
iteration : 1564
train acc:  0.765625
train loss:  0.44102632999420166
train gradient:  0.10666579025227294
iteration : 1565
train acc:  0.8203125
train loss:  0.47749602794647217
train gradient:  0.12402231918097213
iteration : 1566
train acc:  0.75
train loss:  0.44614139199256897
train gradient:  0.12304321117053406
iteration : 1567
train acc:  0.765625
train loss:  0.46528905630111694
train gradient:  0.11834987381617514
iteration : 1568
train acc:  0.640625
train loss:  0.652180552482605
train gradient:  0.24111083581191886
iteration : 1569
train acc:  0.7734375
train loss:  0.4446464776992798
train gradient:  0.1175499504439092
iteration : 1570
train acc:  0.796875
train loss:  0.4087733328342438
train gradient:  0.07699567288124659
iteration : 1571
train acc:  0.7734375
train loss:  0.482112854719162
train gradient:  0.1395758631159912
iteration : 1572
train acc:  0.7265625
train loss:  0.556433916091919
train gradient:  0.17385330912153155
iteration : 1573
train acc:  0.78125
train loss:  0.47222623229026794
train gradient:  0.12786482872060626
iteration : 1574
train acc:  0.7578125
train loss:  0.5701015591621399
train gradient:  0.1788247117457228
iteration : 1575
train acc:  0.7890625
train loss:  0.4290681481361389
train gradient:  0.08651411318567276
iteration : 1576
train acc:  0.8046875
train loss:  0.5049561858177185
train gradient:  0.16291101095773652
iteration : 1577
train acc:  0.71875
train loss:  0.5018790364265442
train gradient:  0.12135444917363554
iteration : 1578
train acc:  0.78125
train loss:  0.431723415851593
train gradient:  0.11319060251815886
iteration : 1579
train acc:  0.734375
train loss:  0.5072685480117798
train gradient:  0.12665448464227114
iteration : 1580
train acc:  0.703125
train loss:  0.5544289946556091
train gradient:  0.1546184093927914
iteration : 1581
train acc:  0.8046875
train loss:  0.4085502624511719
train gradient:  0.11279199436259299
iteration : 1582
train acc:  0.7734375
train loss:  0.4279711842536926
train gradient:  0.10623517901857407
iteration : 1583
train acc:  0.7734375
train loss:  0.4240550696849823
train gradient:  0.09226201305944667
iteration : 1584
train acc:  0.671875
train loss:  0.5558121800422668
train gradient:  0.16764284101569107
iteration : 1585
train acc:  0.75
train loss:  0.4644312858581543
train gradient:  0.14228223732566447
iteration : 1586
train acc:  0.6796875
train loss:  0.49389252066612244
train gradient:  0.11765895961504291
iteration : 1587
train acc:  0.796875
train loss:  0.47332876920700073
train gradient:  0.10934200622278845
iteration : 1588
train acc:  0.734375
train loss:  0.4960024654865265
train gradient:  0.10933069080098984
iteration : 1589
train acc:  0.7265625
train loss:  0.522523045539856
train gradient:  0.1429560342130996
iteration : 1590
train acc:  0.765625
train loss:  0.4626762866973877
train gradient:  0.10752382924673298
iteration : 1591
train acc:  0.796875
train loss:  0.44456058740615845
train gradient:  0.080016914889975
iteration : 1592
train acc:  0.734375
train loss:  0.5208793878555298
train gradient:  0.17365310362524194
iteration : 1593
train acc:  0.7890625
train loss:  0.44832462072372437
train gradient:  0.09133028062486645
iteration : 1594
train acc:  0.6875
train loss:  0.5442084074020386
train gradient:  0.14271444564365265
iteration : 1595
train acc:  0.859375
train loss:  0.372718870639801
train gradient:  0.0766795192660253
iteration : 1596
train acc:  0.6953125
train loss:  0.5053649544715881
train gradient:  0.13473260924491065
iteration : 1597
train acc:  0.78125
train loss:  0.4691999554634094
train gradient:  0.12079107460200127
iteration : 1598
train acc:  0.7578125
train loss:  0.46751415729522705
train gradient:  0.1357650127240928
iteration : 1599
train acc:  0.75
train loss:  0.47232764959335327
train gradient:  0.11686208066998752
iteration : 1600
train acc:  0.6796875
train loss:  0.5096657276153564
train gradient:  0.12027792183330153
iteration : 1601
train acc:  0.75
train loss:  0.49919670820236206
train gradient:  0.13651282918981297
iteration : 1602
train acc:  0.78125
train loss:  0.4179774522781372
train gradient:  0.07321065232384405
iteration : 1603
train acc:  0.7421875
train loss:  0.5413128733634949
train gradient:  0.15685802085290948
iteration : 1604
train acc:  0.7265625
train loss:  0.49588775634765625
train gradient:  0.1367064101463029
iteration : 1605
train acc:  0.78125
train loss:  0.47744718194007874
train gradient:  0.10394923304516579
iteration : 1606
train acc:  0.7578125
train loss:  0.45614561438560486
train gradient:  0.10966418850898293
iteration : 1607
train acc:  0.7578125
train loss:  0.5152592658996582
train gradient:  0.12971765508168598
iteration : 1608
train acc:  0.7578125
train loss:  0.43680107593536377
train gradient:  0.10261715654108532
iteration : 1609
train acc:  0.7734375
train loss:  0.4455293118953705
train gradient:  0.1118217985699001
iteration : 1610
train acc:  0.65625
train loss:  0.5298583507537842
train gradient:  0.13036683185737025
iteration : 1611
train acc:  0.6875
train loss:  0.5775696039199829
train gradient:  0.21430820054210153
iteration : 1612
train acc:  0.8046875
train loss:  0.43619316816329956
train gradient:  0.0761545098555583
iteration : 1613
train acc:  0.6875
train loss:  0.5493286848068237
train gradient:  0.165840647325457
iteration : 1614
train acc:  0.734375
train loss:  0.5081393718719482
train gradient:  0.1279784859085505
iteration : 1615
train acc:  0.7734375
train loss:  0.4797492027282715
train gradient:  0.12325157039756769
iteration : 1616
train acc:  0.78125
train loss:  0.43484407663345337
train gradient:  0.0813037469033106
iteration : 1617
train acc:  0.7578125
train loss:  0.42194825410842896
train gradient:  0.0890785798474485
iteration : 1618
train acc:  0.75
train loss:  0.5761876702308655
train gradient:  0.14777373971007254
iteration : 1619
train acc:  0.7578125
train loss:  0.5106911659240723
train gradient:  0.14477711221483114
iteration : 1620
train acc:  0.7265625
train loss:  0.49747204780578613
train gradient:  0.12188418943766116
iteration : 1621
train acc:  0.6484375
train loss:  0.4930463433265686
train gradient:  0.1213651794573579
iteration : 1622
train acc:  0.7109375
train loss:  0.5010836124420166
train gradient:  0.11468334433008051
iteration : 1623
train acc:  0.78125
train loss:  0.4248366355895996
train gradient:  0.09586046064890116
iteration : 1624
train acc:  0.8046875
train loss:  0.42611488699913025
train gradient:  0.1012035135516155
iteration : 1625
train acc:  0.7734375
train loss:  0.431027352809906
train gradient:  0.13028401211847954
iteration : 1626
train acc:  0.671875
train loss:  0.515077531337738
train gradient:  0.12001776421791438
iteration : 1627
train acc:  0.75
train loss:  0.4903486669063568
train gradient:  0.1212451448687153
iteration : 1628
train acc:  0.71875
train loss:  0.4964424669742584
train gradient:  0.14230568785901798
iteration : 1629
train acc:  0.7109375
train loss:  0.5072124004364014
train gradient:  0.1202869395804597
iteration : 1630
train acc:  0.734375
train loss:  0.47478511929512024
train gradient:  0.09577870856847427
iteration : 1631
train acc:  0.75
train loss:  0.45413938164711
train gradient:  0.09423156913912198
iteration : 1632
train acc:  0.7734375
train loss:  0.4513256847858429
train gradient:  0.11169656517287671
iteration : 1633
train acc:  0.7734375
train loss:  0.48809874057769775
train gradient:  0.12730604169538645
iteration : 1634
train acc:  0.7109375
train loss:  0.5427125692367554
train gradient:  0.15469727347057333
iteration : 1635
train acc:  0.765625
train loss:  0.48733168840408325
train gradient:  0.11456649826726764
iteration : 1636
train acc:  0.7734375
train loss:  0.47712165117263794
train gradient:  0.11588634282325543
iteration : 1637
train acc:  0.7734375
train loss:  0.4782187044620514
train gradient:  0.10802621262652186
iteration : 1638
train acc:  0.7265625
train loss:  0.4978593587875366
train gradient:  0.11589995107378846
iteration : 1639
train acc:  0.7734375
train loss:  0.5358045101165771
train gradient:  0.1604845997335867
iteration : 1640
train acc:  0.75
train loss:  0.5070154666900635
train gradient:  0.15036558650731335
iteration : 1641
train acc:  0.7890625
train loss:  0.45314696431159973
train gradient:  0.1111503595073104
iteration : 1642
train acc:  0.7890625
train loss:  0.47429585456848145
train gradient:  0.10439761808893445
iteration : 1643
train acc:  0.78125
train loss:  0.41865286231040955
train gradient:  0.10515167155693271
iteration : 1644
train acc:  0.7421875
train loss:  0.4981103241443634
train gradient:  0.13195306840279253
iteration : 1645
train acc:  0.75
train loss:  0.5208451151847839
train gradient:  0.1289933107276947
iteration : 1646
train acc:  0.78125
train loss:  0.46413248777389526
train gradient:  0.1139258818087901
iteration : 1647
train acc:  0.734375
train loss:  0.5129508972167969
train gradient:  0.14095080387500475
iteration : 1648
train acc:  0.7421875
train loss:  0.4744653105735779
train gradient:  0.13092545456297072
iteration : 1649
train acc:  0.78125
train loss:  0.4514855444431305
train gradient:  0.12164838615787192
iteration : 1650
train acc:  0.796875
train loss:  0.46802613139152527
train gradient:  0.09726486995307805
iteration : 1651
train acc:  0.7890625
train loss:  0.39714640378952026
train gradient:  0.06351756862024932
iteration : 1652
train acc:  0.7734375
train loss:  0.4492303431034088
train gradient:  0.1137907758068437
iteration : 1653
train acc:  0.8203125
train loss:  0.46099355816841125
train gradient:  0.12655713549039818
iteration : 1654
train acc:  0.7890625
train loss:  0.4176691472530365
train gradient:  0.06633941672090161
iteration : 1655
train acc:  0.703125
train loss:  0.48938846588134766
train gradient:  0.13261398809114894
iteration : 1656
train acc:  0.734375
train loss:  0.45294201374053955
train gradient:  0.14568554025125563
iteration : 1657
train acc:  0.75
train loss:  0.4199693202972412
train gradient:  0.10297880211918287
iteration : 1658
train acc:  0.8125
train loss:  0.4523114860057831
train gradient:  0.09837597432961322
iteration : 1659
train acc:  0.7578125
train loss:  0.5227810740470886
train gradient:  0.13436589992839573
iteration : 1660
train acc:  0.8046875
train loss:  0.42113834619522095
train gradient:  0.11570454536066921
iteration : 1661
train acc:  0.7421875
train loss:  0.46392470598220825
train gradient:  0.11520473093213145
iteration : 1662
train acc:  0.8359375
train loss:  0.3962430953979492
train gradient:  0.0866033717541721
iteration : 1663
train acc:  0.7265625
train loss:  0.5157221555709839
train gradient:  0.13520493104095288
iteration : 1664
train acc:  0.7734375
train loss:  0.4500153362751007
train gradient:  0.08856413755939586
iteration : 1665
train acc:  0.6953125
train loss:  0.5157442092895508
train gradient:  0.13022908520712748
iteration : 1666
train acc:  0.7265625
train loss:  0.47362053394317627
train gradient:  0.1547627309254096
iteration : 1667
train acc:  0.7421875
train loss:  0.4266398549079895
train gradient:  0.09798849542476624
iteration : 1668
train acc:  0.7890625
train loss:  0.4385617971420288
train gradient:  0.09402752307479029
iteration : 1669
train acc:  0.734375
train loss:  0.4772588908672333
train gradient:  0.10754978666378402
iteration : 1670
train acc:  0.7421875
train loss:  0.4965299665927887
train gradient:  0.11512655490075496
iteration : 1671
train acc:  0.7734375
train loss:  0.4276532530784607
train gradient:  0.08804124103207325
iteration : 1672
train acc:  0.828125
train loss:  0.4003247022628784
train gradient:  0.08796173614971944
iteration : 1673
train acc:  0.765625
train loss:  0.47636550664901733
train gradient:  0.12790743858343667
iteration : 1674
train acc:  0.7265625
train loss:  0.4726695120334625
train gradient:  0.1400891649071595
iteration : 1675
train acc:  0.7578125
train loss:  0.47234660387039185
train gradient:  0.11836657427671773
iteration : 1676
train acc:  0.765625
train loss:  0.47274020314216614
train gradient:  0.1100567296648608
iteration : 1677
train acc:  0.71875
train loss:  0.516481876373291
train gradient:  0.1345696230486771
iteration : 1678
train acc:  0.7265625
train loss:  0.5235447287559509
train gradient:  0.12560468977129577
iteration : 1679
train acc:  0.7421875
train loss:  0.5112577080726624
train gradient:  0.14476146730914463
iteration : 1680
train acc:  0.71875
train loss:  0.5119023323059082
train gradient:  0.11111003786933131
iteration : 1681
train acc:  0.78125
train loss:  0.458324670791626
train gradient:  0.09631086107776896
iteration : 1682
train acc:  0.7421875
train loss:  0.5154917240142822
train gradient:  0.12845102722803642
iteration : 1683
train acc:  0.75
train loss:  0.5119266510009766
train gradient:  0.10873376316941585
iteration : 1684
train acc:  0.71875
train loss:  0.4907251298427582
train gradient:  0.10934431178418996
iteration : 1685
train acc:  0.7265625
train loss:  0.48257607221603394
train gradient:  0.12213017173316039
iteration : 1686
train acc:  0.8046875
train loss:  0.4134185314178467
train gradient:  0.08570074093675298
iteration : 1687
train acc:  0.65625
train loss:  0.5818386077880859
train gradient:  0.1926004683537134
iteration : 1688
train acc:  0.78125
train loss:  0.41087883710861206
train gradient:  0.10135592535304935
iteration : 1689
train acc:  0.7734375
train loss:  0.4801047742366791
train gradient:  0.11474105661141891
iteration : 1690
train acc:  0.734375
train loss:  0.48667681217193604
train gradient:  0.1275863113225313
iteration : 1691
train acc:  0.6953125
train loss:  0.5695978999137878
train gradient:  0.1761559943964644
iteration : 1692
train acc:  0.7421875
train loss:  0.4894278049468994
train gradient:  0.12105107595652054
iteration : 1693
train acc:  0.75
train loss:  0.5378978848457336
train gradient:  0.17539680351997988
iteration : 1694
train acc:  0.796875
train loss:  0.4073752760887146
train gradient:  0.09236744422742775
iteration : 1695
train acc:  0.75
train loss:  0.4529552757740021
train gradient:  0.08602158306240812
iteration : 1696
train acc:  0.8203125
train loss:  0.42543283104896545
train gradient:  0.08677001817076924
iteration : 1697
train acc:  0.7421875
train loss:  0.4604063034057617
train gradient:  0.10879102553192771
iteration : 1698
train acc:  0.75
train loss:  0.5324898958206177
train gradient:  0.14050579779420203
iteration : 1699
train acc:  0.71875
train loss:  0.5462071895599365
train gradient:  0.1856237449921053
iteration : 1700
train acc:  0.703125
train loss:  0.5398503541946411
train gradient:  0.12695740676853617
iteration : 1701
train acc:  0.796875
train loss:  0.45084020495414734
train gradient:  0.10098154168189012
iteration : 1702
train acc:  0.8125
train loss:  0.4446253776550293
train gradient:  0.10606755692314107
iteration : 1703
train acc:  0.765625
train loss:  0.4435003697872162
train gradient:  0.10057362849497747
iteration : 1704
train acc:  0.75
train loss:  0.39913082122802734
train gradient:  0.09010413312694865
iteration : 1705
train acc:  0.7890625
train loss:  0.47015857696533203
train gradient:  0.10529703979146303
iteration : 1706
train acc:  0.7578125
train loss:  0.45819100737571716
train gradient:  0.12185945990472101
iteration : 1707
train acc:  0.75
train loss:  0.46814173460006714
train gradient:  0.10355406504449473
iteration : 1708
train acc:  0.7734375
train loss:  0.4618798792362213
train gradient:  0.10750876529481437
iteration : 1709
train acc:  0.78125
train loss:  0.4471365511417389
train gradient:  0.1086520065028862
iteration : 1710
train acc:  0.765625
train loss:  0.49704691767692566
train gradient:  0.13090125186345747
iteration : 1711
train acc:  0.7734375
train loss:  0.4863435626029968
train gradient:  0.14633244332881173
iteration : 1712
train acc:  0.796875
train loss:  0.3924158215522766
train gradient:  0.0833709553938857
iteration : 1713
train acc:  0.796875
train loss:  0.4010692238807678
train gradient:  0.09133552910079297
iteration : 1714
train acc:  0.7109375
train loss:  0.5158627033233643
train gradient:  0.12207733509360884
iteration : 1715
train acc:  0.765625
train loss:  0.509682834148407
train gradient:  0.12687743748336006
iteration : 1716
train acc:  0.75
train loss:  0.46555817127227783
train gradient:  0.09546389937437735
iteration : 1717
train acc:  0.75
train loss:  0.5230025053024292
train gradient:  0.15361761920978287
iteration : 1718
train acc:  0.7421875
train loss:  0.4758659601211548
train gradient:  0.13233105295304015
iteration : 1719
train acc:  0.7265625
train loss:  0.49006372690200806
train gradient:  0.0989702982439416
iteration : 1720
train acc:  0.703125
train loss:  0.5581099987030029
train gradient:  0.1797066085146386
iteration : 1721
train acc:  0.734375
train loss:  0.5342391729354858
train gradient:  0.1700942092322904
iteration : 1722
train acc:  0.703125
train loss:  0.5125853419303894
train gradient:  0.1131153575576105
iteration : 1723
train acc:  0.796875
train loss:  0.42566484212875366
train gradient:  0.09513513811057037
iteration : 1724
train acc:  0.7109375
train loss:  0.5251922011375427
train gradient:  0.143343837092063
iteration : 1725
train acc:  0.734375
train loss:  0.4696994721889496
train gradient:  0.11457282479239186
iteration : 1726
train acc:  0.7734375
train loss:  0.5252076983451843
train gradient:  0.11584019100809029
iteration : 1727
train acc:  0.7421875
train loss:  0.5200875997543335
train gradient:  0.15902522044426404
iteration : 1728
train acc:  0.7109375
train loss:  0.49972450733184814
train gradient:  0.12356709037201957
iteration : 1729
train acc:  0.8359375
train loss:  0.39947161078453064
train gradient:  0.09774500450357362
iteration : 1730
train acc:  0.7578125
train loss:  0.5059322714805603
train gradient:  0.11403598270691061
iteration : 1731
train acc:  0.828125
train loss:  0.39217549562454224
train gradient:  0.0856156278327518
iteration : 1732
train acc:  0.7890625
train loss:  0.4783177375793457
train gradient:  0.13954668164624823
iteration : 1733
train acc:  0.828125
train loss:  0.3928505778312683
train gradient:  0.08123644331425635
iteration : 1734
train acc:  0.7578125
train loss:  0.4933640956878662
train gradient:  0.13878912090769133
iteration : 1735
train acc:  0.765625
train loss:  0.4633457064628601
train gradient:  0.10008264293740456
iteration : 1736
train acc:  0.8203125
train loss:  0.4259798526763916
train gradient:  0.09633550573831634
iteration : 1737
train acc:  0.8046875
train loss:  0.4140753149986267
train gradient:  0.09506152459966336
iteration : 1738
train acc:  0.796875
train loss:  0.46693509817123413
train gradient:  0.10109203115373383
iteration : 1739
train acc:  0.7265625
train loss:  0.5319926738739014
train gradient:  0.15051796738264134
iteration : 1740
train acc:  0.703125
train loss:  0.4912903606891632
train gradient:  0.1063214512257595
iteration : 1741
train acc:  0.7578125
train loss:  0.4227791428565979
train gradient:  0.0869584186428947
iteration : 1742
train acc:  0.7109375
train loss:  0.5116443634033203
train gradient:  0.12761136919642155
iteration : 1743
train acc:  0.7265625
train loss:  0.4818735718727112
train gradient:  0.11803193520922584
iteration : 1744
train acc:  0.765625
train loss:  0.41603875160217285
train gradient:  0.09267594973422041
iteration : 1745
train acc:  0.7578125
train loss:  0.452956885099411
train gradient:  0.09310395268376642
iteration : 1746
train acc:  0.6875
train loss:  0.5461446046829224
train gradient:  0.17225578312780268
iteration : 1747
train acc:  0.78125
train loss:  0.45922720432281494
train gradient:  0.09862396140093274
iteration : 1748
train acc:  0.7890625
train loss:  0.49380233883857727
train gradient:  0.1192060811978621
iteration : 1749
train acc:  0.71875
train loss:  0.5388298034667969
train gradient:  0.13793968269161183
iteration : 1750
train acc:  0.7890625
train loss:  0.47505098581314087
train gradient:  0.12010372461419891
iteration : 1751
train acc:  0.7265625
train loss:  0.49794381856918335
train gradient:  0.12145019836171501
iteration : 1752
train acc:  0.71875
train loss:  0.5075036287307739
train gradient:  0.1103246130099339
iteration : 1753
train acc:  0.8359375
train loss:  0.39697808027267456
train gradient:  0.08954564014961944
iteration : 1754
train acc:  0.7421875
train loss:  0.4963626265525818
train gradient:  0.12475764985721001
iteration : 1755
train acc:  0.7265625
train loss:  0.44489148259162903
train gradient:  0.09818127166858932
iteration : 1756
train acc:  0.71875
train loss:  0.4665612578392029
train gradient:  0.10883492330575942
iteration : 1757
train acc:  0.78125
train loss:  0.45866262912750244
train gradient:  0.11415695086004177
iteration : 1758
train acc:  0.703125
train loss:  0.5423059463500977
train gradient:  0.1534643642544391
iteration : 1759
train acc:  0.8203125
train loss:  0.4281805455684662
train gradient:  0.11384831478050728
iteration : 1760
train acc:  0.734375
train loss:  0.5490686893463135
train gradient:  0.10955477903598274
iteration : 1761
train acc:  0.6640625
train loss:  0.4977518320083618
train gradient:  0.17607153758486133
iteration : 1762
train acc:  0.7734375
train loss:  0.4458906054496765
train gradient:  0.09825166334893122
iteration : 1763
train acc:  0.7890625
train loss:  0.4738200306892395
train gradient:  0.1549915615233385
iteration : 1764
train acc:  0.7734375
train loss:  0.4405345320701599
train gradient:  0.10212977508232556
iteration : 1765
train acc:  0.75
train loss:  0.5107829570770264
train gradient:  0.14399681885249505
iteration : 1766
train acc:  0.78125
train loss:  0.43236902356147766
train gradient:  0.11420510566229115
iteration : 1767
train acc:  0.6875
train loss:  0.5114132165908813
train gradient:  0.13205855178831954
iteration : 1768
train acc:  0.765625
train loss:  0.45433783531188965
train gradient:  0.1006165355582062
iteration : 1769
train acc:  0.7265625
train loss:  0.5202574729919434
train gradient:  0.11504774441492968
iteration : 1770
train acc:  0.8046875
train loss:  0.3998870253562927
train gradient:  0.08620171173023013
iteration : 1771
train acc:  0.734375
train loss:  0.47940585017204285
train gradient:  0.10216774908913005
iteration : 1772
train acc:  0.7890625
train loss:  0.4473875164985657
train gradient:  0.09719916390071418
iteration : 1773
train acc:  0.8359375
train loss:  0.40192312002182007
train gradient:  0.08058043757231671
iteration : 1774
train acc:  0.734375
train loss:  0.5077406167984009
train gradient:  0.10812510792021517
iteration : 1775
train acc:  0.75
train loss:  0.48621729016304016
train gradient:  0.12969093018664868
iteration : 1776
train acc:  0.7890625
train loss:  0.4934995770454407
train gradient:  0.14771604108376107
iteration : 1777
train acc:  0.78125
train loss:  0.4765712320804596
train gradient:  0.11418535780666264
iteration : 1778
train acc:  0.734375
train loss:  0.49578166007995605
train gradient:  0.16273930183740673
iteration : 1779
train acc:  0.7578125
train loss:  0.4714176058769226
train gradient:  0.12050187274363114
iteration : 1780
train acc:  0.7578125
train loss:  0.48798608779907227
train gradient:  0.1141356495007735
iteration : 1781
train acc:  0.7734375
train loss:  0.4587307572364807
train gradient:  0.1271431746976182
iteration : 1782
train acc:  0.71875
train loss:  0.4961385428905487
train gradient:  0.16380415745329507
iteration : 1783
train acc:  0.7421875
train loss:  0.4638260006904602
train gradient:  0.12313713613464716
iteration : 1784
train acc:  0.75
train loss:  0.4840232729911804
train gradient:  0.11419061464931682
iteration : 1785
train acc:  0.7890625
train loss:  0.4877949357032776
train gradient:  0.1057966583072085
iteration : 1786
train acc:  0.7890625
train loss:  0.4410017728805542
train gradient:  0.1140916808356833
iteration : 1787
train acc:  0.765625
train loss:  0.49686142802238464
train gradient:  0.12160733562623524
iteration : 1788
train acc:  0.8359375
train loss:  0.44774121046066284
train gradient:  0.1071575974520595
iteration : 1789
train acc:  0.7421875
train loss:  0.4856942296028137
train gradient:  0.13854367808941043
iteration : 1790
train acc:  0.765625
train loss:  0.4247090816497803
train gradient:  0.09247926146833932
iteration : 1791
train acc:  0.734375
train loss:  0.48995620012283325
train gradient:  0.11832648256629534
iteration : 1792
train acc:  0.734375
train loss:  0.46946167945861816
train gradient:  0.10610173835835161
iteration : 1793
train acc:  0.734375
train loss:  0.5197737812995911
train gradient:  0.12119292045184817
iteration : 1794
train acc:  0.7109375
train loss:  0.46859854459762573
train gradient:  0.11103151949939329
iteration : 1795
train acc:  0.7421875
train loss:  0.4608209729194641
train gradient:  0.10763442800852652
iteration : 1796
train acc:  0.796875
train loss:  0.39917290210723877
train gradient:  0.08448756286468045
iteration : 1797
train acc:  0.7890625
train loss:  0.4477308988571167
train gradient:  0.08747368501079436
iteration : 1798
train acc:  0.75
train loss:  0.4758317768573761
train gradient:  0.1081369742069978
iteration : 1799
train acc:  0.734375
train loss:  0.47008413076400757
train gradient:  0.12562900625996265
iteration : 1800
train acc:  0.75
train loss:  0.45954906940460205
train gradient:  0.1049086322739549
iteration : 1801
train acc:  0.703125
train loss:  0.5296669006347656
train gradient:  0.14246462921373942
iteration : 1802
train acc:  0.6953125
train loss:  0.5394105911254883
train gradient:  0.15047126461435517
iteration : 1803
train acc:  0.7265625
train loss:  0.5662632584571838
train gradient:  0.14196036089112707
iteration : 1804
train acc:  0.7578125
train loss:  0.4448355436325073
train gradient:  0.09710397348071245
iteration : 1805
train acc:  0.765625
train loss:  0.46146661043167114
train gradient:  0.12240611463095032
iteration : 1806
train acc:  0.7734375
train loss:  0.4475530982017517
train gradient:  0.10463773947788253
iteration : 1807
train acc:  0.75
train loss:  0.4762168526649475
train gradient:  0.1048969195171941
iteration : 1808
train acc:  0.765625
train loss:  0.4653390049934387
train gradient:  0.10682925754757024
iteration : 1809
train acc:  0.7265625
train loss:  0.5206281542778015
train gradient:  0.13524253216314086
iteration : 1810
train acc:  0.734375
train loss:  0.5379223227500916
train gradient:  0.1866623194277867
iteration : 1811
train acc:  0.8046875
train loss:  0.4073193073272705
train gradient:  0.10284931719108482
iteration : 1812
train acc:  0.8046875
train loss:  0.3973192572593689
train gradient:  0.08052403207453183
iteration : 1813
train acc:  0.7265625
train loss:  0.5107539892196655
train gradient:  0.13680942081982134
iteration : 1814
train acc:  0.765625
train loss:  0.4555599093437195
train gradient:  0.10824943839401287
iteration : 1815
train acc:  0.7421875
train loss:  0.4724932312965393
train gradient:  0.11550060027366907
iteration : 1816
train acc:  0.7890625
train loss:  0.4986673593521118
train gradient:  0.1264651088718799
iteration : 1817
train acc:  0.78125
train loss:  0.41956689953804016
train gradient:  0.10022763767846168
iteration : 1818
train acc:  0.78125
train loss:  0.46175616979599
train gradient:  0.10487618882961268
iteration : 1819
train acc:  0.796875
train loss:  0.4763808250427246
train gradient:  0.1065261015698088
iteration : 1820
train acc:  0.6484375
train loss:  0.6669491529464722
train gradient:  0.20692360935922438
iteration : 1821
train acc:  0.78125
train loss:  0.5023418664932251
train gradient:  0.15912168277579808
iteration : 1822
train acc:  0.765625
train loss:  0.42563989758491516
train gradient:  0.11147073494129697
iteration : 1823
train acc:  0.8359375
train loss:  0.44289058446884155
train gradient:  0.11264302536638547
iteration : 1824
train acc:  0.75
train loss:  0.4325675964355469
train gradient:  0.0999815200882191
iteration : 1825
train acc:  0.765625
train loss:  0.42126017808914185
train gradient:  0.09647251528792154
iteration : 1826
train acc:  0.8125
train loss:  0.42703092098236084
train gradient:  0.0922655383227412
iteration : 1827
train acc:  0.75
train loss:  0.4577394723892212
train gradient:  0.13605650040334494
iteration : 1828
train acc:  0.7421875
train loss:  0.4774564504623413
train gradient:  0.16241306457829868
iteration : 1829
train acc:  0.75
train loss:  0.41558903455734253
train gradient:  0.08792223749127849
iteration : 1830
train acc:  0.703125
train loss:  0.4887559413909912
train gradient:  0.12415948600618695
iteration : 1831
train acc:  0.7578125
train loss:  0.5355632901191711
train gradient:  0.14250070008550964
iteration : 1832
train acc:  0.7578125
train loss:  0.4789259731769562
train gradient:  0.09788837814481197
iteration : 1833
train acc:  0.7421875
train loss:  0.46927058696746826
train gradient:  0.1049810951573869
iteration : 1834
train acc:  0.75
train loss:  0.45920971035957336
train gradient:  0.09729630732333404
iteration : 1835
train acc:  0.796875
train loss:  0.4439573287963867
train gradient:  0.10156812117810458
iteration : 1836
train acc:  0.7109375
train loss:  0.5570828914642334
train gradient:  0.1668257061385947
iteration : 1837
train acc:  0.71875
train loss:  0.4951634705066681
train gradient:  0.10664853480854054
iteration : 1838
train acc:  0.7421875
train loss:  0.49874454736709595
train gradient:  0.10499725890071343
iteration : 1839
train acc:  0.734375
train loss:  0.4839889705181122
train gradient:  0.09979251601744919
iteration : 1840
train acc:  0.8046875
train loss:  0.412112832069397
train gradient:  0.09449664073537889
iteration : 1841
train acc:  0.8046875
train loss:  0.440878301858902
train gradient:  0.12226579498827246
iteration : 1842
train acc:  0.78125
train loss:  0.4580806493759155
train gradient:  0.14024031123345576
iteration : 1843
train acc:  0.84375
train loss:  0.38727903366088867
train gradient:  0.09409945154516659
iteration : 1844
train acc:  0.828125
train loss:  0.41273942589759827
train gradient:  0.08436810887303886
iteration : 1845
train acc:  0.7734375
train loss:  0.5029424428939819
train gradient:  0.1293885023865562
iteration : 1846
train acc:  0.7109375
train loss:  0.4803212881088257
train gradient:  0.14220874058261232
iteration : 1847
train acc:  0.765625
train loss:  0.42552450299263
train gradient:  0.08492271667150184
iteration : 1848
train acc:  0.7734375
train loss:  0.4494454264640808
train gradient:  0.08471866584171993
iteration : 1849
train acc:  0.765625
train loss:  0.4668891429901123
train gradient:  0.11584961368806425
iteration : 1850
train acc:  0.65625
train loss:  0.5401325821876526
train gradient:  0.1381519781941076
iteration : 1851
train acc:  0.765625
train loss:  0.4586271047592163
train gradient:  0.08779171168033392
iteration : 1852
train acc:  0.796875
train loss:  0.43849891424179077
train gradient:  0.09727530227140002
iteration : 1853
train acc:  0.7578125
train loss:  0.496870219707489
train gradient:  0.13195250816512594
iteration : 1854
train acc:  0.6875
train loss:  0.5191152691841125
train gradient:  0.15037322646206486
iteration : 1855
train acc:  0.8046875
train loss:  0.4081535041332245
train gradient:  0.10952032858429014
iteration : 1856
train acc:  0.828125
train loss:  0.4222131669521332
train gradient:  0.1052856911126254
iteration : 1857
train acc:  0.71875
train loss:  0.48900657892227173
train gradient:  0.09627797648427487
iteration : 1858
train acc:  0.765625
train loss:  0.43742626905441284
train gradient:  0.08366045486621543
iteration : 1859
train acc:  0.71875
train loss:  0.548214316368103
train gradient:  0.1378009599411955
iteration : 1860
train acc:  0.7265625
train loss:  0.5008037686347961
train gradient:  0.10547706286476732
iteration : 1861
train acc:  0.828125
train loss:  0.42211517691612244
train gradient:  0.09575731301524532
iteration : 1862
train acc:  0.7890625
train loss:  0.527938723564148
train gradient:  0.1537226674171996
iteration : 1863
train acc:  0.7578125
train loss:  0.5239332318305969
train gradient:  0.1333088374014994
iteration : 1864
train acc:  0.7421875
train loss:  0.4801362156867981
train gradient:  0.16260323482862543
iteration : 1865
train acc:  0.78125
train loss:  0.44981619715690613
train gradient:  0.1295721839304465
iteration : 1866
train acc:  0.7421875
train loss:  0.48384106159210205
train gradient:  0.14052331749015068
iteration : 1867
train acc:  0.796875
train loss:  0.4298762083053589
train gradient:  0.08500363976000432
iteration : 1868
train acc:  0.7890625
train loss:  0.4950282871723175
train gradient:  0.12287834427660484
iteration : 1869
train acc:  0.765625
train loss:  0.5002923011779785
train gradient:  0.1426140945195985
iteration : 1870
train acc:  0.65625
train loss:  0.6010196208953857
train gradient:  0.1375944297066393
iteration : 1871
train acc:  0.7109375
train loss:  0.5723936557769775
train gradient:  0.14345244837498738
iteration : 1872
train acc:  0.7421875
train loss:  0.533433198928833
train gradient:  0.1699281358211513
iteration : 1873
train acc:  0.7265625
train loss:  0.43459272384643555
train gradient:  0.08662153088677417
iteration : 1874
train acc:  0.7578125
train loss:  0.4885767698287964
train gradient:  0.1531665702651474
iteration : 1875
train acc:  0.765625
train loss:  0.4607471227645874
train gradient:  0.10898684937072324
iteration : 1876
train acc:  0.796875
train loss:  0.4240412712097168
train gradient:  0.09931189680193321
iteration : 1877
train acc:  0.796875
train loss:  0.45651382207870483
train gradient:  0.12057967530403359
iteration : 1878
train acc:  0.7578125
train loss:  0.403922975063324
train gradient:  0.09278970418877537
iteration : 1879
train acc:  0.75
train loss:  0.5054651498794556
train gradient:  0.12830485264402056
iteration : 1880
train acc:  0.734375
train loss:  0.4809243679046631
train gradient:  0.11213213511155452
iteration : 1881
train acc:  0.671875
train loss:  0.5794592499732971
train gradient:  0.17362833986266205
iteration : 1882
train acc:  0.7421875
train loss:  0.47452086210250854
train gradient:  0.1139849517836279
iteration : 1883
train acc:  0.7265625
train loss:  0.48981773853302
train gradient:  0.12742356031461582
iteration : 1884
train acc:  0.7421875
train loss:  0.486020565032959
train gradient:  0.1225853057803669
iteration : 1885
train acc:  0.828125
train loss:  0.4712698757648468
train gradient:  0.12799232048296919
iteration : 1886
train acc:  0.78125
train loss:  0.5340465307235718
train gradient:  0.1231144924389393
iteration : 1887
train acc:  0.7890625
train loss:  0.502817690372467
train gradient:  0.17881948983581836
iteration : 1888
train acc:  0.75
train loss:  0.44192907214164734
train gradient:  0.14846054609111803
iteration : 1889
train acc:  0.7890625
train loss:  0.4963172674179077
train gradient:  0.13894234306841044
iteration : 1890
train acc:  0.734375
train loss:  0.5139759182929993
train gradient:  0.1146576927241371
iteration : 1891
train acc:  0.7421875
train loss:  0.5160139799118042
train gradient:  0.1199017317031301
iteration : 1892
train acc:  0.8046875
train loss:  0.40154749155044556
train gradient:  0.08620108258429904
iteration : 1893
train acc:  0.7421875
train loss:  0.5266467332839966
train gradient:  0.11283088544949112
iteration : 1894
train acc:  0.75
train loss:  0.5078386664390564
train gradient:  0.10677397107267685
iteration : 1895
train acc:  0.703125
train loss:  0.4675801396369934
train gradient:  0.11837217906562074
iteration : 1896
train acc:  0.671875
train loss:  0.5872057676315308
train gradient:  0.1753194028607487
iteration : 1897
train acc:  0.7109375
train loss:  0.5383360981941223
train gradient:  0.13048543786508726
iteration : 1898
train acc:  0.8046875
train loss:  0.43083420395851135
train gradient:  0.12498496081215817
iteration : 1899
train acc:  0.6875
train loss:  0.5278201103210449
train gradient:  0.15016986643786473
iteration : 1900
train acc:  0.7421875
train loss:  0.47688204050064087
train gradient:  0.1138287093521437
iteration : 1901
train acc:  0.734375
train loss:  0.5180959105491638
train gradient:  0.14118495505880874
iteration : 1902
train acc:  0.765625
train loss:  0.4794415831565857
train gradient:  0.13438749465283065
iteration : 1903
train acc:  0.7578125
train loss:  0.48445671796798706
train gradient:  0.12063702708980821
iteration : 1904
train acc:  0.7578125
train loss:  0.4765787124633789
train gradient:  0.1332589137256528
iteration : 1905
train acc:  0.8359375
train loss:  0.40879714488983154
train gradient:  0.09178506530539167
iteration : 1906
train acc:  0.734375
train loss:  0.48561882972717285
train gradient:  0.12847071490954598
iteration : 1907
train acc:  0.8046875
train loss:  0.47190016508102417
train gradient:  0.1165534257963424
iteration : 1908
train acc:  0.7421875
train loss:  0.49195346236228943
train gradient:  0.1014862094248739
iteration : 1909
train acc:  0.8203125
train loss:  0.4661886692047119
train gradient:  0.13557518566941087
iteration : 1910
train acc:  0.78125
train loss:  0.4505254626274109
train gradient:  0.09556247882454395
iteration : 1911
train acc:  0.7109375
train loss:  0.48951199650764465
train gradient:  0.10349831370212549
iteration : 1912
train acc:  0.71875
train loss:  0.47166186571121216
train gradient:  0.09856287092742738
iteration : 1913
train acc:  0.734375
train loss:  0.4637184143066406
train gradient:  0.10486387476629074
iteration : 1914
train acc:  0.78125
train loss:  0.4975103735923767
train gradient:  0.11475429227429851
iteration : 1915
train acc:  0.78125
train loss:  0.4370158314704895
train gradient:  0.08390671322148553
iteration : 1916
train acc:  0.7109375
train loss:  0.5238558053970337
train gradient:  0.12807295608106667
iteration : 1917
train acc:  0.7421875
train loss:  0.46829479932785034
train gradient:  0.11735494366832408
iteration : 1918
train acc:  0.8203125
train loss:  0.3943300247192383
train gradient:  0.08457892616478731
iteration : 1919
train acc:  0.7578125
train loss:  0.4844617545604706
train gradient:  0.12584545900155916
iteration : 1920
train acc:  0.7890625
train loss:  0.4459129571914673
train gradient:  0.11353682665806267
iteration : 1921
train acc:  0.671875
train loss:  0.6157315373420715
train gradient:  0.17851441958508463
iteration : 1922
train acc:  0.765625
train loss:  0.4452098309993744
train gradient:  0.10943599193845634
iteration : 1923
train acc:  0.828125
train loss:  0.4267158508300781
train gradient:  0.0982148895179641
iteration : 1924
train acc:  0.7890625
train loss:  0.4383828043937683
train gradient:  0.10543636480660488
iteration : 1925
train acc:  0.7734375
train loss:  0.5010034441947937
train gradient:  0.10101166112766165
iteration : 1926
train acc:  0.75
train loss:  0.47132378816604614
train gradient:  0.09609886155334434
iteration : 1927
train acc:  0.6953125
train loss:  0.5891209840774536
train gradient:  0.157573882279286
iteration : 1928
train acc:  0.734375
train loss:  0.5245034098625183
train gradient:  0.10344544690025793
iteration : 1929
train acc:  0.7734375
train loss:  0.4945127069950104
train gradient:  0.13525831164053975
iteration : 1930
train acc:  0.7890625
train loss:  0.4989387094974518
train gradient:  0.1185132071722453
iteration : 1931
train acc:  0.765625
train loss:  0.44969654083251953
train gradient:  0.08846868200203603
iteration : 1932
train acc:  0.8125
train loss:  0.45118534564971924
train gradient:  0.127398350454551
iteration : 1933
train acc:  0.7109375
train loss:  0.5222029685974121
train gradient:  0.11739967887574207
iteration : 1934
train acc:  0.703125
train loss:  0.5684114694595337
train gradient:  0.1314432031076106
iteration : 1935
train acc:  0.7890625
train loss:  0.423904687166214
train gradient:  0.08953265029531655
iteration : 1936
train acc:  0.71875
train loss:  0.4793156087398529
train gradient:  0.1030845935927895
iteration : 1937
train acc:  0.7421875
train loss:  0.47964638471603394
train gradient:  0.12271691863311168
iteration : 1938
train acc:  0.7578125
train loss:  0.4512639343738556
train gradient:  0.14906563453531663
iteration : 1939
train acc:  0.7890625
train loss:  0.4433680772781372
train gradient:  0.10576111173342588
iteration : 1940
train acc:  0.6953125
train loss:  0.53499835729599
train gradient:  0.12995705836407961
iteration : 1941
train acc:  0.75
train loss:  0.5146101713180542
train gradient:  0.11490513477824185
iteration : 1942
train acc:  0.734375
train loss:  0.46220383048057556
train gradient:  0.14124163416764118
iteration : 1943
train acc:  0.7578125
train loss:  0.4838436543941498
train gradient:  0.10597877555608863
iteration : 1944
train acc:  0.734375
train loss:  0.4930092692375183
train gradient:  0.13816145000065083
iteration : 1945
train acc:  0.71875
train loss:  0.547937273979187
train gradient:  0.14875158662835727
iteration : 1946
train acc:  0.71875
train loss:  0.5169174075126648
train gradient:  0.1643961612557193
iteration : 1947
train acc:  0.7578125
train loss:  0.48813119530677795
train gradient:  0.11293281816412222
iteration : 1948
train acc:  0.7109375
train loss:  0.4472784399986267
train gradient:  0.11010493692860142
iteration : 1949
train acc:  0.734375
train loss:  0.5389763116836548
train gradient:  0.1349749592984208
iteration : 1950
train acc:  0.6875
train loss:  0.5127690434455872
train gradient:  0.13419273998840253
iteration : 1951
train acc:  0.6796875
train loss:  0.5593476295471191
train gradient:  0.13344277283451556
iteration : 1952
train acc:  0.6484375
train loss:  0.5419175624847412
train gradient:  0.12657436653019577
iteration : 1953
train acc:  0.765625
train loss:  0.4387972950935364
train gradient:  0.09519348023411849
iteration : 1954
train acc:  0.7265625
train loss:  0.4698350429534912
train gradient:  0.120758024859132
iteration : 1955
train acc:  0.765625
train loss:  0.4218961000442505
train gradient:  0.08363289099756412
iteration : 1956
train acc:  0.84375
train loss:  0.4119860529899597
train gradient:  0.09539241975944764
iteration : 1957
train acc:  0.78125
train loss:  0.5046234726905823
train gradient:  0.11256613161373051
iteration : 1958
train acc:  0.7734375
train loss:  0.46153923869132996
train gradient:  0.10725275968928473
iteration : 1959
train acc:  0.765625
train loss:  0.4834451377391815
train gradient:  0.11286157347714544
iteration : 1960
train acc:  0.703125
train loss:  0.537878155708313
train gradient:  0.15137537443018328
iteration : 1961
train acc:  0.6796875
train loss:  0.5361955761909485
train gradient:  0.15565066392447002
iteration : 1962
train acc:  0.75
train loss:  0.4818296730518341
train gradient:  0.10774495375708493
iteration : 1963
train acc:  0.75
train loss:  0.4386938512325287
train gradient:  0.08806467295035021
iteration : 1964
train acc:  0.8203125
train loss:  0.4287072718143463
train gradient:  0.10096780575990316
iteration : 1965
train acc:  0.7421875
train loss:  0.5103131532669067
train gradient:  0.1556625695276348
iteration : 1966
train acc:  0.78125
train loss:  0.4720391035079956
train gradient:  0.11399150668040692
iteration : 1967
train acc:  0.7734375
train loss:  0.45049822330474854
train gradient:  0.11974988566908829
iteration : 1968
train acc:  0.734375
train loss:  0.5119831562042236
train gradient:  0.106131505679179
iteration : 1969
train acc:  0.7734375
train loss:  0.46098682284355164
train gradient:  0.1246012953782327
iteration : 1970
train acc:  0.8046875
train loss:  0.4289093613624573
train gradient:  0.08736477073243112
iteration : 1971
train acc:  0.6796875
train loss:  0.5277918577194214
train gradient:  0.12980508876877944
iteration : 1972
train acc:  0.8046875
train loss:  0.42641621828079224
train gradient:  0.09556472592562337
iteration : 1973
train acc:  0.7265625
train loss:  0.48242199420928955
train gradient:  0.10620258012599318
iteration : 1974
train acc:  0.796875
train loss:  0.4409784972667694
train gradient:  0.11073309574566027
iteration : 1975
train acc:  0.6953125
train loss:  0.507190465927124
train gradient:  0.10137717421746235
iteration : 1976
train acc:  0.71875
train loss:  0.5014640092849731
train gradient:  0.13385161726946265
iteration : 1977
train acc:  0.71875
train loss:  0.4361928701400757
train gradient:  0.10754200615730757
iteration : 1978
train acc:  0.7421875
train loss:  0.4832611083984375
train gradient:  0.14635664867067774
iteration : 1979
train acc:  0.7578125
train loss:  0.5135161280632019
train gradient:  0.1223488861170129
iteration : 1980
train acc:  0.7578125
train loss:  0.47972649335861206
train gradient:  0.09131513004076688
iteration : 1981
train acc:  0.71875
train loss:  0.5100316405296326
train gradient:  0.10891180770794227
iteration : 1982
train acc:  0.6796875
train loss:  0.6163821220397949
train gradient:  0.1980002465097273
iteration : 1983
train acc:  0.8671875
train loss:  0.3860939145088196
train gradient:  0.07004445289418007
iteration : 1984
train acc:  0.6875
train loss:  0.5166290998458862
train gradient:  0.13828529402553014
iteration : 1985
train acc:  0.7109375
train loss:  0.5141214728355408
train gradient:  0.15067013785086175
iteration : 1986
train acc:  0.75
train loss:  0.4876769483089447
train gradient:  0.11647191611153722
iteration : 1987
train acc:  0.75
train loss:  0.5232149958610535
train gradient:  0.11565014700503592
iteration : 1988
train acc:  0.71875
train loss:  0.48615261912345886
train gradient:  0.11012969194920338
iteration : 1989
train acc:  0.796875
train loss:  0.42532098293304443
train gradient:  0.0862858600268987
iteration : 1990
train acc:  0.7578125
train loss:  0.3887230157852173
train gradient:  0.06688766505254344
iteration : 1991
train acc:  0.7578125
train loss:  0.4329451322555542
train gradient:  0.08115670799826853
iteration : 1992
train acc:  0.8046875
train loss:  0.45885413885116577
train gradient:  0.1200018166567771
iteration : 1993
train acc:  0.734375
train loss:  0.4898643493652344
train gradient:  0.09521238010897361
iteration : 1994
train acc:  0.71875
train loss:  0.48306548595428467
train gradient:  0.12041325203810897
iteration : 1995
train acc:  0.765625
train loss:  0.4523981213569641
train gradient:  0.09401063289661948
iteration : 1996
train acc:  0.7734375
train loss:  0.45601457357406616
train gradient:  0.09028445175747161
iteration : 1997
train acc:  0.7734375
train loss:  0.420332670211792
train gradient:  0.093613966981284
iteration : 1998
train acc:  0.7109375
train loss:  0.5148847699165344
train gradient:  0.18499211519150432
iteration : 1999
train acc:  0.75
train loss:  0.5179085731506348
train gradient:  0.1389812377253574
iteration : 2000
train acc:  0.796875
train loss:  0.3832424283027649
train gradient:  0.09682373071836584
iteration : 2001
train acc:  0.796875
train loss:  0.4115604758262634
train gradient:  0.09239922480600823
iteration : 2002
train acc:  0.7890625
train loss:  0.43223801255226135
train gradient:  0.09652038367131192
iteration : 2003
train acc:  0.7734375
train loss:  0.4655498266220093
train gradient:  0.11021274600793841
iteration : 2004
train acc:  0.765625
train loss:  0.4493103623390198
train gradient:  0.11986138186091816
iteration : 2005
train acc:  0.71875
train loss:  0.6010228395462036
train gradient:  0.1545274161103119
iteration : 2006
train acc:  0.78125
train loss:  0.4225044250488281
train gradient:  0.0703940613574469
iteration : 2007
train acc:  0.84375
train loss:  0.4324027895927429
train gradient:  0.09216115391044843
iteration : 2008
train acc:  0.6953125
train loss:  0.5218431353569031
train gradient:  0.1278638222991391
iteration : 2009
train acc:  0.7734375
train loss:  0.42859962582588196
train gradient:  0.0991594711871691
iteration : 2010
train acc:  0.796875
train loss:  0.45464658737182617
train gradient:  0.09436180493385438
iteration : 2011
train acc:  0.765625
train loss:  0.4911074638366699
train gradient:  0.1056882165595303
iteration : 2012
train acc:  0.78125
train loss:  0.4485020041465759
train gradient:  0.11043465331681078
iteration : 2013
train acc:  0.7734375
train loss:  0.44059205055236816
train gradient:  0.09966735895380867
iteration : 2014
train acc:  0.7578125
train loss:  0.43285632133483887
train gradient:  0.11274180972494927
iteration : 2015
train acc:  0.8125
train loss:  0.4372059106826782
train gradient:  0.0938848584302052
iteration : 2016
train acc:  0.7265625
train loss:  0.4923132658004761
train gradient:  0.1244040141531813
iteration : 2017
train acc:  0.765625
train loss:  0.47416403889656067
train gradient:  0.10994607685145495
iteration : 2018
train acc:  0.796875
train loss:  0.4441656470298767
train gradient:  0.10057823993356231
iteration : 2019
train acc:  0.765625
train loss:  0.4058436453342438
train gradient:  0.0834896493502951
iteration : 2020
train acc:  0.7578125
train loss:  0.4915795922279358
train gradient:  0.13052987547170064
iteration : 2021
train acc:  0.8203125
train loss:  0.3845658302307129
train gradient:  0.07640280544427237
iteration : 2022
train acc:  0.7734375
train loss:  0.437188982963562
train gradient:  0.08226511978588619
iteration : 2023
train acc:  0.7578125
train loss:  0.4515102207660675
train gradient:  0.10712313051433096
iteration : 2024
train acc:  0.7109375
train loss:  0.5282691121101379
train gradient:  0.14802315404157013
iteration : 2025
train acc:  0.75
train loss:  0.45245903730392456
train gradient:  0.08713839258491472
iteration : 2026
train acc:  0.8203125
train loss:  0.3776642680168152
train gradient:  0.09516536846949536
iteration : 2027
train acc:  0.765625
train loss:  0.42993441224098206
train gradient:  0.09081706421557637
iteration : 2028
train acc:  0.7421875
train loss:  0.4656549394130707
train gradient:  0.12109918353010449
iteration : 2029
train acc:  0.8046875
train loss:  0.44281095266342163
train gradient:  0.09241765724444449
iteration : 2030
train acc:  0.7578125
train loss:  0.46303266286849976
train gradient:  0.1133837603536966
iteration : 2031
train acc:  0.796875
train loss:  0.4334527850151062
train gradient:  0.08719552647900251
iteration : 2032
train acc:  0.8203125
train loss:  0.39045459032058716
train gradient:  0.08962379597943107
iteration : 2033
train acc:  0.796875
train loss:  0.40739625692367554
train gradient:  0.07141146651096607
iteration : 2034
train acc:  0.828125
train loss:  0.4255563020706177
train gradient:  0.09476130398502501
iteration : 2035
train acc:  0.7578125
train loss:  0.5159663558006287
train gradient:  0.15104007618766646
iteration : 2036
train acc:  0.765625
train loss:  0.4592514634132385
train gradient:  0.10430863628732802
iteration : 2037
train acc:  0.71875
train loss:  0.5184062123298645
train gradient:  0.16855448148793678
iteration : 2038
train acc:  0.75
train loss:  0.4753413796424866
train gradient:  0.1116949571039025
iteration : 2039
train acc:  0.703125
train loss:  0.47941258549690247
train gradient:  0.09954444968154523
iteration : 2040
train acc:  0.828125
train loss:  0.4355294704437256
train gradient:  0.09860935296889782
iteration : 2041
train acc:  0.7578125
train loss:  0.4876437187194824
train gradient:  0.15087533209401918
iteration : 2042
train acc:  0.7578125
train loss:  0.4436042606830597
train gradient:  0.09538219774760943
iteration : 2043
train acc:  0.6875
train loss:  0.5790971517562866
train gradient:  0.14195649099935878
iteration : 2044
train acc:  0.7265625
train loss:  0.4553191661834717
train gradient:  0.10038554712805789
iteration : 2045
train acc:  0.8046875
train loss:  0.4029236435890198
train gradient:  0.10822140654848636
iteration : 2046
train acc:  0.734375
train loss:  0.4446696937084198
train gradient:  0.1074677594635328
iteration : 2047
train acc:  0.734375
train loss:  0.4330587387084961
train gradient:  0.09003065959150909
iteration : 2048
train acc:  0.78125
train loss:  0.4690772294998169
train gradient:  0.11017375452087885
iteration : 2049
train acc:  0.75
train loss:  0.5227106809616089
train gradient:  0.13247059405108794
iteration : 2050
train acc:  0.765625
train loss:  0.487282395362854
train gradient:  0.15821527581575584
iteration : 2051
train acc:  0.7734375
train loss:  0.47237375378608704
train gradient:  0.14058048552943786
iteration : 2052
train acc:  0.765625
train loss:  0.4989585280418396
train gradient:  0.16171609179629975
iteration : 2053
train acc:  0.75
train loss:  0.4771598279476166
train gradient:  0.109293830427858
iteration : 2054
train acc:  0.8046875
train loss:  0.45935529470443726
train gradient:  0.1314463894525859
iteration : 2055
train acc:  0.7265625
train loss:  0.4916877746582031
train gradient:  0.12063719479990302
iteration : 2056
train acc:  0.7578125
train loss:  0.5064886212348938
train gradient:  0.133043305583821
iteration : 2057
train acc:  0.71875
train loss:  0.5441752672195435
train gradient:  0.14296492429253083
iteration : 2058
train acc:  0.7109375
train loss:  0.48479995131492615
train gradient:  0.1150317060660952
iteration : 2059
train acc:  0.7265625
train loss:  0.504328191280365
train gradient:  0.12244370142928183
iteration : 2060
train acc:  0.765625
train loss:  0.45658212900161743
train gradient:  0.12088811680187628
iteration : 2061
train acc:  0.6953125
train loss:  0.5350948572158813
train gradient:  0.10165501257888065
iteration : 2062
train acc:  0.71875
train loss:  0.5311426520347595
train gradient:  0.1501065440213925
iteration : 2063
train acc:  0.78125
train loss:  0.4110173285007477
train gradient:  0.0923023168386191
iteration : 2064
train acc:  0.671875
train loss:  0.5223674774169922
train gradient:  0.15437629618142112
iteration : 2065
train acc:  0.765625
train loss:  0.4658891558647156
train gradient:  0.11905423277004823
iteration : 2066
train acc:  0.703125
train loss:  0.5163782835006714
train gradient:  0.1258316565198329
iteration : 2067
train acc:  0.7421875
train loss:  0.5307201147079468
train gradient:  0.13656782339243073
iteration : 2068
train acc:  0.7421875
train loss:  0.47922253608703613
train gradient:  0.12253273267481318
iteration : 2069
train acc:  0.75
train loss:  0.4938613474369049
train gradient:  0.10119656683800823
iteration : 2070
train acc:  0.7578125
train loss:  0.46098625659942627
train gradient:  0.13990016177489947
iteration : 2071
train acc:  0.8359375
train loss:  0.4096490740776062
train gradient:  0.12170659379465934
iteration : 2072
train acc:  0.7734375
train loss:  0.4952836334705353
train gradient:  0.10683940065991059
iteration : 2073
train acc:  0.78125
train loss:  0.47186893224716187
train gradient:  0.11741749988877322
iteration : 2074
train acc:  0.7578125
train loss:  0.43284207582473755
train gradient:  0.11766057090809981
iteration : 2075
train acc:  0.7890625
train loss:  0.42152899503707886
train gradient:  0.10433560684079402
iteration : 2076
train acc:  0.8046875
train loss:  0.4645131826400757
train gradient:  0.13830111179716678
iteration : 2077
train acc:  0.78125
train loss:  0.4216262400150299
train gradient:  0.11021733790418894
iteration : 2078
train acc:  0.7109375
train loss:  0.5794098377227783
train gradient:  0.15877638656214405
iteration : 2079
train acc:  0.7734375
train loss:  0.46453243494033813
train gradient:  0.13943289774410045
iteration : 2080
train acc:  0.8203125
train loss:  0.4374755620956421
train gradient:  0.11666244552651078
iteration : 2081
train acc:  0.78125
train loss:  0.46317410469055176
train gradient:  0.09358455493896438
iteration : 2082
train acc:  0.75
train loss:  0.4799550473690033
train gradient:  0.12186333544957895
iteration : 2083
train acc:  0.734375
train loss:  0.4947718381881714
train gradient:  0.1240358372506976
iteration : 2084
train acc:  0.734375
train loss:  0.5111473798751831
train gradient:  0.09111852984161666
iteration : 2085
train acc:  0.75
train loss:  0.5064854621887207
train gradient:  0.12900957983796085
iteration : 2086
train acc:  0.7265625
train loss:  0.5250898599624634
train gradient:  0.15726934615358618
iteration : 2087
train acc:  0.6875
train loss:  0.5043836832046509
train gradient:  0.10968333004099758
iteration : 2088
train acc:  0.7890625
train loss:  0.4289402961730957
train gradient:  0.08618797636646755
iteration : 2089
train acc:  0.78125
train loss:  0.4540676474571228
train gradient:  0.11204150299429774
iteration : 2090
train acc:  0.7421875
train loss:  0.4597581923007965
train gradient:  0.10386097112898597
iteration : 2091
train acc:  0.8046875
train loss:  0.4162062406539917
train gradient:  0.08968805764476642
iteration : 2092
train acc:  0.6953125
train loss:  0.5318633317947388
train gradient:  0.14064690023334347
iteration : 2093
train acc:  0.71875
train loss:  0.4850091338157654
train gradient:  0.1274552072396331
iteration : 2094
train acc:  0.7734375
train loss:  0.47849035263061523
train gradient:  0.13192567105922104
iteration : 2095
train acc:  0.7734375
train loss:  0.4384278357028961
train gradient:  0.08972436859824819
iteration : 2096
train acc:  0.8359375
train loss:  0.3805597722530365
train gradient:  0.10716847153870973
iteration : 2097
train acc:  0.7578125
train loss:  0.5114585161209106
train gradient:  0.16878389374354358
iteration : 2098
train acc:  0.78125
train loss:  0.49390411376953125
train gradient:  0.10882149062510542
iteration : 2099
train acc:  0.6953125
train loss:  0.5467875003814697
train gradient:  0.12958653754365806
iteration : 2100
train acc:  0.7109375
train loss:  0.49270525574684143
train gradient:  0.10614677333532241
iteration : 2101
train acc:  0.6875
train loss:  0.5856137275695801
train gradient:  0.18663233037251103
iteration : 2102
train acc:  0.703125
train loss:  0.5284684896469116
train gradient:  0.15772712198589328
iteration : 2103
train acc:  0.6640625
train loss:  0.5643188953399658
train gradient:  0.14993835285103313
iteration : 2104
train acc:  0.6796875
train loss:  0.5368776321411133
train gradient:  0.14728119121220484
iteration : 2105
train acc:  0.6953125
train loss:  0.5382217168807983
train gradient:  0.14083744661934777
iteration : 2106
train acc:  0.8203125
train loss:  0.4230402708053589
train gradient:  0.09997163259879772
iteration : 2107
train acc:  0.7734375
train loss:  0.4356592893600464
train gradient:  0.08624566915831089
iteration : 2108
train acc:  0.6953125
train loss:  0.4916820526123047
train gradient:  0.12267729358471896
iteration : 2109
train acc:  0.78125
train loss:  0.43960070610046387
train gradient:  0.10299827154811321
iteration : 2110
train acc:  0.7578125
train loss:  0.44995802640914917
train gradient:  0.10240836587091813
iteration : 2111
train acc:  0.7265625
train loss:  0.5075180530548096
train gradient:  0.1186465898173288
iteration : 2112
train acc:  0.7265625
train loss:  0.46825915575027466
train gradient:  0.15372820076664284
iteration : 2113
train acc:  0.78125
train loss:  0.47180426120758057
train gradient:  0.1204730674300907
iteration : 2114
train acc:  0.8125
train loss:  0.41800999641418457
train gradient:  0.10265249270396028
iteration : 2115
train acc:  0.71875
train loss:  0.5545914173126221
train gradient:  0.15116819114358498
iteration : 2116
train acc:  0.7890625
train loss:  0.48718878626823425
train gradient:  0.12833043973388022
iteration : 2117
train acc:  0.7421875
train loss:  0.5400559902191162
train gradient:  0.1469379307952693
iteration : 2118
train acc:  0.7265625
train loss:  0.4826851487159729
train gradient:  0.10919652813207503
iteration : 2119
train acc:  0.7734375
train loss:  0.4623076319694519
train gradient:  0.09378788462731434
iteration : 2120
train acc:  0.71875
train loss:  0.5088744759559631
train gradient:  0.14845150905631116
iteration : 2121
train acc:  0.78125
train loss:  0.4159321188926697
train gradient:  0.08589334130851208
iteration : 2122
train acc:  0.796875
train loss:  0.44335466623306274
train gradient:  0.09717779534629462
iteration : 2123
train acc:  0.828125
train loss:  0.4458225667476654
train gradient:  0.1165385553328898
iteration : 2124
train acc:  0.7890625
train loss:  0.4485342502593994
train gradient:  0.09878554810743621
iteration : 2125
train acc:  0.78125
train loss:  0.4308331608772278
train gradient:  0.10172999732309555
iteration : 2126
train acc:  0.6953125
train loss:  0.5391025543212891
train gradient:  0.14555445220369656
iteration : 2127
train acc:  0.765625
train loss:  0.41987234354019165
train gradient:  0.08772310020628594
iteration : 2128
train acc:  0.71875
train loss:  0.4636061191558838
train gradient:  0.1029424870815913
iteration : 2129
train acc:  0.8046875
train loss:  0.4775429368019104
train gradient:  0.09254687502716556
iteration : 2130
train acc:  0.6875
train loss:  0.5120810866355896
train gradient:  0.15351791052405062
iteration : 2131
train acc:  0.7421875
train loss:  0.4903023838996887
train gradient:  0.12105901225650047
iteration : 2132
train acc:  0.8046875
train loss:  0.4587414264678955
train gradient:  0.11454665394696725
iteration : 2133
train acc:  0.7265625
train loss:  0.4689794182777405
train gradient:  0.11966067170369483
iteration : 2134
train acc:  0.75
train loss:  0.44081151485443115
train gradient:  0.09766034453321655
iteration : 2135
train acc:  0.7734375
train loss:  0.4750334620475769
train gradient:  0.09371864185523017
iteration : 2136
train acc:  0.7421875
train loss:  0.484893262386322
train gradient:  0.18262966340502654
iteration : 2137
train acc:  0.71875
train loss:  0.526692807674408
train gradient:  0.1390953496614982
iteration : 2138
train acc:  0.7421875
train loss:  0.46776822209358215
train gradient:  0.13045091915374674
iteration : 2139
train acc:  0.703125
train loss:  0.5584429502487183
train gradient:  0.18026894149918643
iteration : 2140
train acc:  0.7265625
train loss:  0.5147086381912231
train gradient:  0.15114956053902562
iteration : 2141
train acc:  0.734375
train loss:  0.48027142882347107
train gradient:  0.09564704798462656
iteration : 2142
train acc:  0.75
train loss:  0.49205470085144043
train gradient:  0.12562668113935416
iteration : 2143
train acc:  0.796875
train loss:  0.44063127040863037
train gradient:  0.0863308859719349
iteration : 2144
train acc:  0.734375
train loss:  0.46131157875061035
train gradient:  0.1011563701839255
iteration : 2145
train acc:  0.8125
train loss:  0.41781407594680786
train gradient:  0.09888243144009055
iteration : 2146
train acc:  0.6640625
train loss:  0.5283765196800232
train gradient:  0.13875047668285628
iteration : 2147
train acc:  0.75
train loss:  0.47698116302490234
train gradient:  0.1276079450061672
iteration : 2148
train acc:  0.7421875
train loss:  0.4425465166568756
train gradient:  0.10128445716095778
iteration : 2149
train acc:  0.734375
train loss:  0.4900553822517395
train gradient:  0.1182801084503235
iteration : 2150
train acc:  0.71875
train loss:  0.49707090854644775
train gradient:  0.13669791038962087
iteration : 2151
train acc:  0.75
train loss:  0.43678659200668335
train gradient:  0.09058782683988047
iteration : 2152
train acc:  0.75
train loss:  0.49502864480018616
train gradient:  0.13389869101515348
iteration : 2153
train acc:  0.71875
train loss:  0.5033378601074219
train gradient:  0.12867559239450232
iteration : 2154
train acc:  0.765625
train loss:  0.46391427516937256
train gradient:  0.11650990456280008
iteration : 2155
train acc:  0.7578125
train loss:  0.4518584609031677
train gradient:  0.10245037730009798
iteration : 2156
train acc:  0.7421875
train loss:  0.517025887966156
train gradient:  0.115754920160071
iteration : 2157
train acc:  0.6640625
train loss:  0.5549793243408203
train gradient:  0.1950717308417707
iteration : 2158
train acc:  0.7578125
train loss:  0.46711671352386475
train gradient:  0.11027816561237433
iteration : 2159
train acc:  0.78125
train loss:  0.4747186005115509
train gradient:  0.10063735679567387
iteration : 2160
train acc:  0.7109375
train loss:  0.510391354560852
train gradient:  0.18076519579321113
iteration : 2161
train acc:  0.75
train loss:  0.5248011946678162
train gradient:  0.13814413975719614
iteration : 2162
train acc:  0.7578125
train loss:  0.43916088342666626
train gradient:  0.08107589418440554
iteration : 2163
train acc:  0.8125
train loss:  0.4337916374206543
train gradient:  0.1122992449761374
iteration : 2164
train acc:  0.75
train loss:  0.46948039531707764
train gradient:  0.09821106133317228
iteration : 2165
train acc:  0.71875
train loss:  0.5128469467163086
train gradient:  0.15817839043957638
iteration : 2166
train acc:  0.671875
train loss:  0.4866147041320801
train gradient:  0.11366372478249072
iteration : 2167
train acc:  0.71875
train loss:  0.5122379064559937
train gradient:  0.16592307723548205
iteration : 2168
train acc:  0.703125
train loss:  0.5159990787506104
train gradient:  0.14719618266583068
iteration : 2169
train acc:  0.734375
train loss:  0.490018755197525
train gradient:  0.12096035250071781
iteration : 2170
train acc:  0.75
train loss:  0.48465240001678467
train gradient:  0.13205015134587478
iteration : 2171
train acc:  0.6796875
train loss:  0.5076494216918945
train gradient:  0.12035690980121587
iteration : 2172
train acc:  0.765625
train loss:  0.4556059241294861
train gradient:  0.10547380815127587
iteration : 2173
train acc:  0.7421875
train loss:  0.5180991888046265
train gradient:  0.10504835118851968
iteration : 2174
train acc:  0.7578125
train loss:  0.4942995309829712
train gradient:  0.1014801446593912
iteration : 2175
train acc:  0.734375
train loss:  0.4666998088359833
train gradient:  0.10159031426912958
iteration : 2176
train acc:  0.7265625
train loss:  0.5255953669548035
train gradient:  0.14247723078899688
iteration : 2177
train acc:  0.6640625
train loss:  0.5665552020072937
train gradient:  0.15206049163383334
iteration : 2178
train acc:  0.75
train loss:  0.48196882009506226
train gradient:  0.1419475264683976
iteration : 2179
train acc:  0.7109375
train loss:  0.4875369668006897
train gradient:  0.16968663566756387
iteration : 2180
train acc:  0.78125
train loss:  0.43971869349479675
train gradient:  0.11687921418539209
iteration : 2181
train acc:  0.71875
train loss:  0.5208857655525208
train gradient:  0.139806235096629
iteration : 2182
train acc:  0.75
train loss:  0.4633868932723999
train gradient:  0.07927486881477497
iteration : 2183
train acc:  0.7109375
train loss:  0.5836684703826904
train gradient:  0.17677004800741752
iteration : 2184
train acc:  0.6484375
train loss:  0.5405580997467041
train gradient:  0.18110302490000257
iteration : 2185
train acc:  0.703125
train loss:  0.5333553552627563
train gradient:  0.13230901408853046
iteration : 2186
train acc:  0.7421875
train loss:  0.47048789262771606
train gradient:  0.10381767630046643
iteration : 2187
train acc:  0.7421875
train loss:  0.45217111706733704
train gradient:  0.08897996068902989
iteration : 2188
train acc:  0.7734375
train loss:  0.46664613485336304
train gradient:  0.09150033678022333
iteration : 2189
train acc:  0.765625
train loss:  0.4954864978790283
train gradient:  0.14192231041606346
iteration : 2190
train acc:  0.765625
train loss:  0.4750567376613617
train gradient:  0.10249997094847524
iteration : 2191
train acc:  0.7578125
train loss:  0.5068985223770142
train gradient:  0.10201062050621525
iteration : 2192
train acc:  0.7890625
train loss:  0.4443519115447998
train gradient:  0.09960458800539242
iteration : 2193
train acc:  0.8203125
train loss:  0.3802386522293091
train gradient:  0.07820532644740176
iteration : 2194
train acc:  0.7265625
train loss:  0.4620424211025238
train gradient:  0.09155612146967317
iteration : 2195
train acc:  0.7890625
train loss:  0.4207751750946045
train gradient:  0.10700180340829064
iteration : 2196
train acc:  0.796875
train loss:  0.41310566663742065
train gradient:  0.08070831661391117
iteration : 2197
train acc:  0.7734375
train loss:  0.440419465303421
train gradient:  0.08925459326143846
iteration : 2198
train acc:  0.7109375
train loss:  0.544976532459259
train gradient:  0.1426598955239627
iteration : 2199
train acc:  0.7421875
train loss:  0.48599907755851746
train gradient:  0.09946669071574601
iteration : 2200
train acc:  0.7109375
train loss:  0.5222334265708923
train gradient:  0.1263809034190121
iteration : 2201
train acc:  0.7421875
train loss:  0.4644891619682312
train gradient:  0.1020905273785724
iteration : 2202
train acc:  0.703125
train loss:  0.48764967918395996
train gradient:  0.11374101776888539
iteration : 2203
train acc:  0.71875
train loss:  0.4419846534729004
train gradient:  0.09659236113982314
iteration : 2204
train acc:  0.796875
train loss:  0.4002145528793335
train gradient:  0.08829009715478155
iteration : 2205
train acc:  0.7890625
train loss:  0.4261813759803772
train gradient:  0.106507223328525
iteration : 2206
train acc:  0.7578125
train loss:  0.4690074920654297
train gradient:  0.13021243221368983
iteration : 2207
train acc:  0.7265625
train loss:  0.4936496615409851
train gradient:  0.1182421137228981
iteration : 2208
train acc:  0.828125
train loss:  0.3977869153022766
train gradient:  0.0797212488265123
iteration : 2209
train acc:  0.75
train loss:  0.4890313744544983
train gradient:  0.13766164699744168
iteration : 2210
train acc:  0.6875
train loss:  0.6206110119819641
train gradient:  0.165605770110137
iteration : 2211
train acc:  0.6953125
train loss:  0.4945673942565918
train gradient:  0.12239940458248953
iteration : 2212
train acc:  0.78125
train loss:  0.4355197548866272
train gradient:  0.10232000333234333
iteration : 2213
train acc:  0.7421875
train loss:  0.5119915008544922
train gradient:  0.1519077111961513
iteration : 2214
train acc:  0.734375
train loss:  0.49413996934890747
train gradient:  0.1122562436748572
iteration : 2215
train acc:  0.7734375
train loss:  0.4403429329395294
train gradient:  0.11103040200586602
iteration : 2216
train acc:  0.6953125
train loss:  0.5168989896774292
train gradient:  0.15206480323833027
iteration : 2217
train acc:  0.78125
train loss:  0.43833932280540466
train gradient:  0.08591075571428267
iteration : 2218
train acc:  0.734375
train loss:  0.5548045635223389
train gradient:  0.14293220746448176
iteration : 2219
train acc:  0.7421875
train loss:  0.5033382177352905
train gradient:  0.14377758531599238
iteration : 2220
train acc:  0.7109375
train loss:  0.5324134826660156
train gradient:  0.11293685118822469
iteration : 2221
train acc:  0.75
train loss:  0.49491646885871887
train gradient:  0.11549305032469981
iteration : 2222
train acc:  0.7734375
train loss:  0.4502195417881012
train gradient:  0.13435603814215563
iteration : 2223
train acc:  0.7421875
train loss:  0.47418177127838135
train gradient:  0.13685694488879252
iteration : 2224
train acc:  0.7265625
train loss:  0.45550772547721863
train gradient:  0.11747184811166062
iteration : 2225
train acc:  0.734375
train loss:  0.5877755880355835
train gradient:  0.15696969209430517
iteration : 2226
train acc:  0.7578125
train loss:  0.4918593764305115
train gradient:  0.11829386321034621
iteration : 2227
train acc:  0.7421875
train loss:  0.47196531295776367
train gradient:  0.1180520981381482
iteration : 2228
train acc:  0.703125
train loss:  0.49458765983581543
train gradient:  0.11653063576351853
iteration : 2229
train acc:  0.8203125
train loss:  0.40989723801612854
train gradient:  0.07871872760691269
iteration : 2230
train acc:  0.75
train loss:  0.42456522583961487
train gradient:  0.0874350016537821
iteration : 2231
train acc:  0.796875
train loss:  0.39259010553359985
train gradient:  0.07904093470785256
iteration : 2232
train acc:  0.6796875
train loss:  0.5332401394844055
train gradient:  0.17248053122615264
iteration : 2233
train acc:  0.703125
train loss:  0.47293344140052795
train gradient:  0.08353401107580875
iteration : 2234
train acc:  0.7734375
train loss:  0.42651668190956116
train gradient:  0.09874047002713102
iteration : 2235
train acc:  0.7734375
train loss:  0.4523765444755554
train gradient:  0.1018046659901304
iteration : 2236
train acc:  0.765625
train loss:  0.4665333330631256
train gradient:  0.123437951180504
iteration : 2237
train acc:  0.7421875
train loss:  0.49176740646362305
train gradient:  0.13318938190868684
iteration : 2238
train acc:  0.765625
train loss:  0.4846971333026886
train gradient:  0.10146968144498025
iteration : 2239
train acc:  0.7578125
train loss:  0.45988690853118896
train gradient:  0.10016816526478842
iteration : 2240
train acc:  0.6953125
train loss:  0.5311110019683838
train gradient:  0.17206142883827452
iteration : 2241
train acc:  0.765625
train loss:  0.4335412085056305
train gradient:  0.079480450258101
iteration : 2242
train acc:  0.71875
train loss:  0.49869924783706665
train gradient:  0.10785929818093472
iteration : 2243
train acc:  0.75
train loss:  0.4774920344352722
train gradient:  0.12552108914135232
iteration : 2244
train acc:  0.734375
train loss:  0.4927905797958374
train gradient:  0.11878837714831385
iteration : 2245
train acc:  0.796875
train loss:  0.46161073446273804
train gradient:  0.14482803585664278
iteration : 2246
train acc:  0.7734375
train loss:  0.49287861585617065
train gradient:  0.1477667177510446
iteration : 2247
train acc:  0.7421875
train loss:  0.47017961740493774
train gradient:  0.09808521276574597
iteration : 2248
train acc:  0.8203125
train loss:  0.43355125188827515
train gradient:  0.11887729920014962
iteration : 2249
train acc:  0.75
train loss:  0.5047441124916077
train gradient:  0.12267586958278957
iteration : 2250
train acc:  0.765625
train loss:  0.4656410217285156
train gradient:  0.09426142677647403
iteration : 2251
train acc:  0.7734375
train loss:  0.44990986585617065
train gradient:  0.10225915679280455
iteration : 2252
train acc:  0.7890625
train loss:  0.4501916170120239
train gradient:  0.12393652474449944
iteration : 2253
train acc:  0.65625
train loss:  0.5438821315765381
train gradient:  0.13220175503939274
iteration : 2254
train acc:  0.7890625
train loss:  0.4782711863517761
train gradient:  0.0988939697612016
iteration : 2255
train acc:  0.8359375
train loss:  0.4288061559200287
train gradient:  0.09930920318379698
iteration : 2256
train acc:  0.7109375
train loss:  0.4986567497253418
train gradient:  0.1391055996284487
iteration : 2257
train acc:  0.765625
train loss:  0.4778306484222412
train gradient:  0.1128054637208404
iteration : 2258
train acc:  0.7578125
train loss:  0.4545983672142029
train gradient:  0.07444075802992114
iteration : 2259
train acc:  0.6796875
train loss:  0.546405017375946
train gradient:  0.1470112228687152
iteration : 2260
train acc:  0.75
train loss:  0.4275657832622528
train gradient:  0.08282282288399505
iteration : 2261
train acc:  0.75
train loss:  0.46070045232772827
train gradient:  0.09737083188109112
iteration : 2262
train acc:  0.78125
train loss:  0.4891153573989868
train gradient:  0.12531663970808202
iteration : 2263
train acc:  0.671875
train loss:  0.5348514914512634
train gradient:  0.14974778775003478
iteration : 2264
train acc:  0.734375
train loss:  0.4696634113788605
train gradient:  0.08888256360670245
iteration : 2265
train acc:  0.6953125
train loss:  0.5901955366134644
train gradient:  0.18982429158816566
iteration : 2266
train acc:  0.7265625
train loss:  0.47393283247947693
train gradient:  0.12191034834786153
iteration : 2267
train acc:  0.7578125
train loss:  0.47173774242401123
train gradient:  0.14066478157303192
iteration : 2268
train acc:  0.7890625
train loss:  0.4542014002799988
train gradient:  0.11298521920992889
iteration : 2269
train acc:  0.796875
train loss:  0.4790398180484772
train gradient:  0.1117544162063867
iteration : 2270
train acc:  0.7734375
train loss:  0.42924022674560547
train gradient:  0.08017746045722103
iteration : 2271
train acc:  0.7890625
train loss:  0.450256884098053
train gradient:  0.09096216880130933
iteration : 2272
train acc:  0.75
train loss:  0.4846743941307068
train gradient:  0.12132135782388132
iteration : 2273
train acc:  0.75
train loss:  0.47586295008659363
train gradient:  0.11278332490657211
iteration : 2274
train acc:  0.796875
train loss:  0.41776397824287415
train gradient:  0.09405174322618512
iteration : 2275
train acc:  0.7578125
train loss:  0.45650312304496765
train gradient:  0.09114360234680123
iteration : 2276
train acc:  0.703125
train loss:  0.5267760157585144
train gradient:  0.12499552399558543
iteration : 2277
train acc:  0.765625
train loss:  0.4432387053966522
train gradient:  0.1013386427001524
iteration : 2278
train acc:  0.765625
train loss:  0.44974786043167114
train gradient:  0.08377057653789972
iteration : 2279
train acc:  0.703125
train loss:  0.4986106753349304
train gradient:  0.13761572544610737
iteration : 2280
train acc:  0.8046875
train loss:  0.4156806468963623
train gradient:  0.08557314151373009
iteration : 2281
train acc:  0.765625
train loss:  0.5046863555908203
train gradient:  0.12412569046128823
iteration : 2282
train acc:  0.8671875
train loss:  0.3520708680152893
train gradient:  0.07336759113920995
iteration : 2283
train acc:  0.734375
train loss:  0.50676029920578
train gradient:  0.11952312185747606
iteration : 2284
train acc:  0.78125
train loss:  0.45972156524658203
train gradient:  0.1267274863805267
iteration : 2285
train acc:  0.703125
train loss:  0.47861021757125854
train gradient:  0.10842529797487763
iteration : 2286
train acc:  0.8125
train loss:  0.4407889246940613
train gradient:  0.1139323417658903
iteration : 2287
train acc:  0.71875
train loss:  0.5389313101768494
train gradient:  0.15218750859204494
iteration : 2288
train acc:  0.7578125
train loss:  0.47260499000549316
train gradient:  0.10736560143126565
iteration : 2289
train acc:  0.7421875
train loss:  0.45217764377593994
train gradient:  0.13824346879283173
iteration : 2290
train acc:  0.71875
train loss:  0.5693734288215637
train gradient:  0.15321258266755527
iteration : 2291
train acc:  0.765625
train loss:  0.47337669134140015
train gradient:  0.12478777851002347
iteration : 2292
train acc:  0.75
train loss:  0.5159636735916138
train gradient:  0.13301015454708626
iteration : 2293
train acc:  0.765625
train loss:  0.4152926802635193
train gradient:  0.11421993355476058
iteration : 2294
train acc:  0.7734375
train loss:  0.47157132625579834
train gradient:  0.1069228324667201
iteration : 2295
train acc:  0.7265625
train loss:  0.5505013465881348
train gradient:  0.11647353501025298
iteration : 2296
train acc:  0.8046875
train loss:  0.4270477294921875
train gradient:  0.11847133094498226
iteration : 2297
train acc:  0.8046875
train loss:  0.4006756544113159
train gradient:  0.06662309914279219
iteration : 2298
train acc:  0.796875
train loss:  0.40896421670913696
train gradient:  0.08764604007779261
iteration : 2299
train acc:  0.71875
train loss:  0.45361658930778503
train gradient:  0.1332575932360297
iteration : 2300
train acc:  0.703125
train loss:  0.5125120282173157
train gradient:  0.11387861198372214
iteration : 2301
train acc:  0.71875
train loss:  0.5179387331008911
train gradient:  0.1327896894904174
iteration : 2302
train acc:  0.7265625
train loss:  0.4890877604484558
train gradient:  0.14419614910751516
iteration : 2303
train acc:  0.7890625
train loss:  0.4407597482204437
train gradient:  0.09937144724039476
iteration : 2304
train acc:  0.7578125
train loss:  0.4459083676338196
train gradient:  0.09244721743242094
iteration : 2305
train acc:  0.7421875
train loss:  0.4981445074081421
train gradient:  0.13082031300156843
iteration : 2306
train acc:  0.7265625
train loss:  0.521655797958374
train gradient:  0.11311555056011237
iteration : 2307
train acc:  0.7734375
train loss:  0.4619778096675873
train gradient:  0.10897622302468596
iteration : 2308
train acc:  0.765625
train loss:  0.5277154445648193
train gradient:  0.14233629165936607
iteration : 2309
train acc:  0.8046875
train loss:  0.39536309242248535
train gradient:  0.06960299484601269
iteration : 2310
train acc:  0.734375
train loss:  0.5086331367492676
train gradient:  0.13423229045395152
iteration : 2311
train acc:  0.7890625
train loss:  0.4462766647338867
train gradient:  0.08893152017154453
iteration : 2312
train acc:  0.7109375
train loss:  0.5667442679405212
train gradient:  0.14009638806701924
iteration : 2313
train acc:  0.796875
train loss:  0.46102094650268555
train gradient:  0.08402496912820402
iteration : 2314
train acc:  0.7421875
train loss:  0.4956282377243042
train gradient:  0.12758235143424995
iteration : 2315
train acc:  0.8125
train loss:  0.5065202713012695
train gradient:  0.1503755657835822
iteration : 2316
train acc:  0.78125
train loss:  0.45600438117980957
train gradient:  0.1313426766305415
iteration : 2317
train acc:  0.7578125
train loss:  0.4574923813343048
train gradient:  0.09635006889241562
iteration : 2318
train acc:  0.78125
train loss:  0.42212826013565063
train gradient:  0.08721452734500748
iteration : 2319
train acc:  0.7734375
train loss:  0.4812318682670593
train gradient:  0.13881645589306993
iteration : 2320
train acc:  0.765625
train loss:  0.4232044816017151
train gradient:  0.08612128133875983
iteration : 2321
train acc:  0.7578125
train loss:  0.45390230417251587
train gradient:  0.11499115080657325
iteration : 2322
train acc:  0.7578125
train loss:  0.47450360655784607
train gradient:  0.10134523623350536
iteration : 2323
train acc:  0.7578125
train loss:  0.4604846239089966
train gradient:  0.09172367598740466
iteration : 2324
train acc:  0.7734375
train loss:  0.5150070786476135
train gradient:  0.1378416951772128
iteration : 2325
train acc:  0.703125
train loss:  0.5727639198303223
train gradient:  0.15992759438618398
iteration : 2326
train acc:  0.6796875
train loss:  0.5731642246246338
train gradient:  0.1708468589151982
iteration : 2327
train acc:  0.7421875
train loss:  0.4763748049736023
train gradient:  0.12381348692055735
iteration : 2328
train acc:  0.7578125
train loss:  0.44571781158447266
train gradient:  0.10901555302276764
iteration : 2329
train acc:  0.640625
train loss:  0.6031962633132935
train gradient:  0.15686206160324528
iteration : 2330
train acc:  0.7578125
train loss:  0.4829588532447815
train gradient:  0.13536572598455754
iteration : 2331
train acc:  0.71875
train loss:  0.5227501392364502
train gradient:  0.13923600786781626
iteration : 2332
train acc:  0.7734375
train loss:  0.44820404052734375
train gradient:  0.09351688201933388
iteration : 2333
train acc:  0.7734375
train loss:  0.5152818560600281
train gradient:  0.12988745196337417
iteration : 2334
train acc:  0.7421875
train loss:  0.48159319162368774
train gradient:  0.11438619647273908
iteration : 2335
train acc:  0.7734375
train loss:  0.425514817237854
train gradient:  0.118065233923313
iteration : 2336
train acc:  0.765625
train loss:  0.443633496761322
train gradient:  0.10141041304853605
iteration : 2337
train acc:  0.7578125
train loss:  0.45130455493927
train gradient:  0.11226164165743491
iteration : 2338
train acc:  0.796875
train loss:  0.4471187889575958
train gradient:  0.09581814082385486
iteration : 2339
train acc:  0.765625
train loss:  0.4274718761444092
train gradient:  0.10155988191313452
iteration : 2340
train acc:  0.8203125
train loss:  0.3954762816429138
train gradient:  0.08456745904737002
iteration : 2341
train acc:  0.75
train loss:  0.4445726275444031
train gradient:  0.12108926529876227
iteration : 2342
train acc:  0.7578125
train loss:  0.4811820387840271
train gradient:  0.10275797695918183
iteration : 2343
train acc:  0.7421875
train loss:  0.49044352769851685
train gradient:  0.12125918490004639
iteration : 2344
train acc:  0.7734375
train loss:  0.49182745814323425
train gradient:  0.12987314000595862
iteration : 2345
train acc:  0.65625
train loss:  0.6198702454566956
train gradient:  0.1606847742322946
iteration : 2346
train acc:  0.7265625
train loss:  0.5460020303726196
train gradient:  0.22183587789078069
iteration : 2347
train acc:  0.7578125
train loss:  0.4895724654197693
train gradient:  0.1255711387222566
iteration : 2348
train acc:  0.765625
train loss:  0.48500263690948486
train gradient:  0.09727284080055323
iteration : 2349
train acc:  0.828125
train loss:  0.42336517572402954
train gradient:  0.08987816772426692
iteration : 2350
train acc:  0.765625
train loss:  0.5674470663070679
train gradient:  0.18429866021465396
iteration : 2351
train acc:  0.78125
train loss:  0.4759107828140259
train gradient:  0.1093724107641988
iteration : 2352
train acc:  0.7734375
train loss:  0.4628273844718933
train gradient:  0.10384833794884117
iteration : 2353
train acc:  0.6953125
train loss:  0.49146947264671326
train gradient:  0.09994326816498962
iteration : 2354
train acc:  0.7890625
train loss:  0.43453288078308105
train gradient:  0.10779031101036489
iteration : 2355
train acc:  0.8125
train loss:  0.430930048227