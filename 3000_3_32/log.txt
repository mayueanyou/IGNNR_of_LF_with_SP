program start:
num_rounds= 3
node_emb_dim= 32

----------------------------------------new_epoch--------------------------------------

epoch:  0
iteration : 0
train acc:  0.4296875
train loss:  0.7073176503181458
train gradient:  2.3418345141712837
iteration : 1
train acc:  0.578125
train loss:  0.6953295469284058
train gradient:  3.543645927457515
iteration : 2
train acc:  0.5390625
train loss:  0.6781424283981323
train gradient:  1.9338553167818522
iteration : 3
train acc:  0.484375
train loss:  0.6952883005142212
train gradient:  4.874778713670027
iteration : 4
train acc:  0.4921875
train loss:  0.7340947389602661
train gradient:  3.894080736843449
iteration : 5
train acc:  0.5546875
train loss:  0.6982274055480957
train gradient:  3.5249076049913217
iteration : 6
train acc:  0.5390625
train loss:  0.6743349432945251
train gradient:  1.4751750826082175
iteration : 7
train acc:  0.5546875
train loss:  0.6906331181526184
train gradient:  1.5527338611971058
iteration : 8
train acc:  0.4921875
train loss:  0.6810591816902161
train gradient:  1.2113433860703946
iteration : 9
train acc:  0.609375
train loss:  0.6828359961509705
train gradient:  1.4359416141316883
iteration : 10
train acc:  0.5234375
train loss:  0.7141684889793396
train gradient:  1.9121171850563485
iteration : 11
train acc:  0.5546875
train loss:  0.6737834215164185
train gradient:  1.3093111388098435
iteration : 12
train acc:  0.6015625
train loss:  0.6706504225730896
train gradient:  1.20030041121358
iteration : 13
train acc:  0.5234375
train loss:  0.6933023929595947
train gradient:  0.9526774086017197
iteration : 14
train acc:  0.578125
train loss:  0.6545050144195557
train gradient:  0.6896778023650575
iteration : 15
train acc:  0.625
train loss:  0.6641305685043335
train gradient:  0.9084149908281031
iteration : 16
train acc:  0.5703125
train loss:  0.7035180330276489
train gradient:  0.6017447391903143
iteration : 17
train acc:  0.4453125
train loss:  0.720689058303833
train gradient:  0.8750210291979152
iteration : 18
train acc:  0.5859375
train loss:  0.6638933420181274
train gradient:  0.6453041960519295
iteration : 19
train acc:  0.5078125
train loss:  0.7074837684631348
train gradient:  2.1822093239528044
iteration : 20
train acc:  0.5078125
train loss:  0.6901595592498779
train gradient:  0.5011049333220847
iteration : 21
train acc:  0.53125
train loss:  0.7189598679542542
train gradient:  1.4683278242449207
iteration : 22
train acc:  0.6171875
train loss:  0.6593008041381836
train gradient:  0.892301548068628
iteration : 23
train acc:  0.59375
train loss:  0.6727063059806824
train gradient:  0.8781865140923555
iteration : 24
train acc:  0.546875
train loss:  0.7040591239929199
train gradient:  1.1990147658620234
iteration : 25
train acc:  0.546875
train loss:  0.7014120221138
train gradient:  0.44023576213086063
iteration : 26
train acc:  0.6015625
train loss:  0.6604452133178711
train gradient:  0.552816829397896
iteration : 27
train acc:  0.625
train loss:  0.6405367255210876
train gradient:  0.21295630150985212
iteration : 28
train acc:  0.6015625
train loss:  0.6829535961151123
train gradient:  1.3468078024696997
iteration : 29
train acc:  0.6328125
train loss:  0.6929165124893188
train gradient:  0.9408473513955745
iteration : 30
train acc:  0.578125
train loss:  0.6895645260810852
train gradient:  0.576253023890137
iteration : 31
train acc:  0.546875
train loss:  0.7535317540168762
train gradient:  1.7396306742337613
iteration : 32
train acc:  0.5859375
train loss:  0.687342643737793
train gradient:  0.5139190279628811
iteration : 33
train acc:  0.5859375
train loss:  0.6636403799057007
train gradient:  0.4899711640653996
iteration : 34
train acc:  0.5390625
train loss:  0.6956808567047119
train gradient:  0.9966654071024786
iteration : 35
train acc:  0.6171875
train loss:  0.6677924990653992
train gradient:  0.434211053396321
iteration : 36
train acc:  0.5546875
train loss:  0.663770318031311
train gradient:  0.53803631738641
iteration : 37
train acc:  0.625
train loss:  0.6600661277770996
train gradient:  0.5181441618077083
iteration : 38
train acc:  0.5390625
train loss:  0.7009166479110718
train gradient:  0.7808185643593923
iteration : 39
train acc:  0.625
train loss:  0.666949987411499
train gradient:  0.689724120134018
iteration : 40
train acc:  0.625
train loss:  0.6546854972839355
train gradient:  0.6561629024629526
iteration : 41
train acc:  0.59375
train loss:  0.6693516373634338
train gradient:  0.5986506572630087
iteration : 42
train acc:  0.6171875
train loss:  0.6495677828788757
train gradient:  0.3260691505205967
iteration : 43
train acc:  0.5625
train loss:  0.6798622608184814
train gradient:  0.6616082266478407
iteration : 44
train acc:  0.59375
train loss:  0.6635594367980957
train gradient:  0.4302334495531846
iteration : 45
train acc:  0.6015625
train loss:  0.6598623991012573
train gradient:  0.34477010483580206
iteration : 46
train acc:  0.6953125
train loss:  0.6350317001342773
train gradient:  0.25108897879369946
iteration : 47
train acc:  0.640625
train loss:  0.6487138867378235
train gradient:  0.29347786834529865
iteration : 48
train acc:  0.65625
train loss:  0.6470998525619507
train gradient:  0.5405685335899733
iteration : 49
train acc:  0.6328125
train loss:  0.6404997110366821
train gradient:  0.18598206548053065
iteration : 50
train acc:  0.53125
train loss:  0.665928304195404
train gradient:  0.3344692284724414
iteration : 51
train acc:  0.6640625
train loss:  0.6402372717857361
train gradient:  0.3378388465977662
iteration : 52
train acc:  0.5546875
train loss:  0.6860659718513489
train gradient:  0.4488624367292436
iteration : 53
train acc:  0.578125
train loss:  0.6993179321289062
train gradient:  1.7540284077531212
iteration : 54
train acc:  0.6328125
train loss:  0.6592181921005249
train gradient:  0.598531945942957
iteration : 55
train acc:  0.65625
train loss:  0.6684368848800659
train gradient:  0.8881958933931806
iteration : 56
train acc:  0.578125
train loss:  0.672254741191864
train gradient:  0.3832594005801804
iteration : 57
train acc:  0.5625
train loss:  0.6870248317718506
train gradient:  0.8797172354516541
iteration : 58
train acc:  0.5546875
train loss:  0.6879072785377502
train gradient:  0.8119218239436863
iteration : 59
train acc:  0.5703125
train loss:  0.6501829028129578
train gradient:  0.2867433288882427
iteration : 60
train acc:  0.5625
train loss:  0.7243639826774597
train gradient:  0.7223381472825192
iteration : 61
train acc:  0.6875
train loss:  0.6177361607551575
train gradient:  0.468307784007875
iteration : 62
train acc:  0.6328125
train loss:  0.6351860761642456
train gradient:  0.23633517515199215
iteration : 63
train acc:  0.6171875
train loss:  0.6427429914474487
train gradient:  0.4192688858519465
iteration : 64
train acc:  0.59375
train loss:  0.6571688652038574
train gradient:  0.37380361845703136
iteration : 65
train acc:  0.609375
train loss:  0.6639660596847534
train gradient:  0.20290357120762936
iteration : 66
train acc:  0.65625
train loss:  0.6401187181472778
train gradient:  0.16195126255941047
iteration : 67
train acc:  0.5546875
train loss:  0.6778346300125122
train gradient:  0.5299954681970633
iteration : 68
train acc:  0.609375
train loss:  0.656623363494873
train gradient:  0.1871918257451972
iteration : 69
train acc:  0.609375
train loss:  0.6652645468711853
train gradient:  0.4346990736382333
iteration : 70
train acc:  0.5703125
train loss:  0.6713892221450806
train gradient:  0.3136836598209406
iteration : 71
train acc:  0.609375
train loss:  0.6746845841407776
train gradient:  0.6284125642399039
iteration : 72
train acc:  0.59375
train loss:  0.6578236222267151
train gradient:  0.3069480985675682
iteration : 73
train acc:  0.5703125
train loss:  0.6639172434806824
train gradient:  0.5908298812840442
iteration : 74
train acc:  0.6875
train loss:  0.6382879018783569
train gradient:  0.2648827318062469
iteration : 75
train acc:  0.609375
train loss:  0.6450508832931519
train gradient:  0.253355801945034
iteration : 76
train acc:  0.546875
train loss:  0.7207930684089661
train gradient:  0.8141108374968999
iteration : 77
train acc:  0.640625
train loss:  0.6304934620857239
train gradient:  0.4204969660327136
iteration : 78
train acc:  0.6328125
train loss:  0.6549023389816284
train gradient:  0.6463730207356064
iteration : 79
train acc:  0.625
train loss:  0.6479880809783936
train gradient:  0.44272610319860933
iteration : 80
train acc:  0.5703125
train loss:  0.6694808006286621
train gradient:  0.6577347495719804
iteration : 81
train acc:  0.5703125
train loss:  0.6755897998809814
train gradient:  0.5754899293737805
iteration : 82
train acc:  0.6171875
train loss:  0.6602928638458252
train gradient:  0.5342435882055038
iteration : 83
train acc:  0.6015625
train loss:  0.657079815864563
train gradient:  0.2918355013725136
iteration : 84
train acc:  0.703125
train loss:  0.5954712629318237
train gradient:  0.44314400262368076
iteration : 85
train acc:  0.609375
train loss:  0.6762295365333557
train gradient:  0.5049237157187465
iteration : 86
train acc:  0.609375
train loss:  0.6580324172973633
train gradient:  0.44597975493320785
iteration : 87
train acc:  0.5390625
train loss:  0.7028785943984985
train gradient:  0.9110883416988218
iteration : 88
train acc:  0.625
train loss:  0.6548242568969727
train gradient:  0.6956008916206612
iteration : 89
train acc:  0.5703125
train loss:  0.6817845702171326
train gradient:  0.7527691700162161
iteration : 90
train acc:  0.5703125
train loss:  0.7082543969154358
train gradient:  0.7002823072995237
iteration : 91
train acc:  0.6484375
train loss:  0.6279216408729553
train gradient:  0.45770194786006485
iteration : 92
train acc:  0.6484375
train loss:  0.6257002353668213
train gradient:  0.3642902807820405
iteration : 93
train acc:  0.5703125
train loss:  0.662301778793335
train gradient:  0.45236008574796716
iteration : 94
train acc:  0.609375
train loss:  0.6351082921028137
train gradient:  0.24293713376212
iteration : 95
train acc:  0.6484375
train loss:  0.6290903091430664
train gradient:  0.3124552152741618
iteration : 96
train acc:  0.5078125
train loss:  0.7064660787582397
train gradient:  0.6150937731327426
iteration : 97
train acc:  0.625
train loss:  0.6278475522994995
train gradient:  0.34662310113399736
iteration : 98
train acc:  0.6484375
train loss:  0.6199702024459839
train gradient:  0.32223900731214156
iteration : 99
train acc:  0.65625
train loss:  0.6521202921867371
train gradient:  0.6508272428108648
iteration : 100
train acc:  0.609375
train loss:  0.6688714027404785
train gradient:  0.7629152789643773
iteration : 101
train acc:  0.5546875
train loss:  0.6715357303619385
train gradient:  0.8222499672724675
iteration : 102
train acc:  0.6484375
train loss:  0.6220908164978027
train gradient:  0.48084799197351874
iteration : 103
train acc:  0.625
train loss:  0.6463376879692078
train gradient:  0.42045182027978956
iteration : 104
train acc:  0.671875
train loss:  0.6187158823013306
train gradient:  0.4181814519976928
iteration : 105
train acc:  0.6484375
train loss:  0.635368287563324
train gradient:  0.4659045144886456
iteration : 106
train acc:  0.5703125
train loss:  0.6430187225341797
train gradient:  0.4484618837167675
iteration : 107
train acc:  0.546875
train loss:  0.6953310370445251
train gradient:  0.9743844802813743
iteration : 108
train acc:  0.671875
train loss:  0.638839602470398
train gradient:  0.5153111610984663
iteration : 109
train acc:  0.65625
train loss:  0.628955066204071
train gradient:  0.29087242476153624
iteration : 110
train acc:  0.6171875
train loss:  0.6693425178527832
train gradient:  0.7231905842712761
iteration : 111
train acc:  0.59375
train loss:  0.6540895700454712
train gradient:  0.7843872813596657
iteration : 112
train acc:  0.578125
train loss:  0.6538554430007935
train gradient:  0.41968716007190954
iteration : 113
train acc:  0.703125
train loss:  0.5906660556793213
train gradient:  0.6167053014731102
iteration : 114
train acc:  0.671875
train loss:  0.6387749910354614
train gradient:  0.3600389944233897
iteration : 115
train acc:  0.6328125
train loss:  0.6354825496673584
train gradient:  0.4805479358343015
iteration : 116
train acc:  0.6328125
train loss:  0.6623312830924988
train gradient:  0.5382545650081297
iteration : 117
train acc:  0.609375
train loss:  0.63398277759552
train gradient:  0.5629186225703267
iteration : 118
train acc:  0.6640625
train loss:  0.6264956593513489
train gradient:  0.5986852614775524
iteration : 119
train acc:  0.6875
train loss:  0.6049618721008301
train gradient:  0.581489739191009
iteration : 120
train acc:  0.6328125
train loss:  0.6646648645401001
train gradient:  0.48652678650900816
iteration : 121
train acc:  0.6171875
train loss:  0.6203879714012146
train gradient:  0.3464603449666506
iteration : 122
train acc:  0.6328125
train loss:  0.6233452558517456
train gradient:  0.21111472748066895
iteration : 123
train acc:  0.7109375
train loss:  0.6004337072372437
train gradient:  0.3513533100596339
iteration : 124
train acc:  0.7109375
train loss:  0.5854318141937256
train gradient:  0.37005593617192173
iteration : 125
train acc:  0.6171875
train loss:  0.6637855768203735
train gradient:  0.5044627110854388
iteration : 126
train acc:  0.6328125
train loss:  0.6344293355941772
train gradient:  0.512880500171754
iteration : 127
train acc:  0.6953125
train loss:  0.5813730359077454
train gradient:  0.4467469379698387
iteration : 128
train acc:  0.609375
train loss:  0.6138969659805298
train gradient:  0.33983569518383544
iteration : 129
train acc:  0.671875
train loss:  0.5859914422035217
train gradient:  0.5019861688851261
iteration : 130
train acc:  0.6015625
train loss:  0.6707661151885986
train gradient:  0.6591740901164987
iteration : 131
train acc:  0.6796875
train loss:  0.6135821342468262
train gradient:  1.0772269719300758
iteration : 132
train acc:  0.671875
train loss:  0.612474799156189
train gradient:  0.5177449978258586
iteration : 133
train acc:  0.6953125
train loss:  0.6055721640586853
train gradient:  0.652455378073838
iteration : 134
train acc:  0.6015625
train loss:  0.6768359541893005
train gradient:  1.0746780664193736
iteration : 135
train acc:  0.640625
train loss:  0.6051216125488281
train gradient:  0.33913283082928825
iteration : 136
train acc:  0.65625
train loss:  0.5977227687835693
train gradient:  0.611115727867436
iteration : 137
train acc:  0.6640625
train loss:  0.6240150332450867
train gradient:  0.8343547263424501
iteration : 138
train acc:  0.6640625
train loss:  0.6297609806060791
train gradient:  0.8218465040352216
iteration : 139
train acc:  0.6640625
train loss:  0.608380913734436
train gradient:  0.5177265813794361
iteration : 140
train acc:  0.640625
train loss:  0.6263517141342163
train gradient:  0.6979404283474471
iteration : 141
train acc:  0.5859375
train loss:  0.6776815056800842
train gradient:  0.6285557288473387
iteration : 142
train acc:  0.625
train loss:  0.647403359413147
train gradient:  0.6703189955844062
iteration : 143
train acc:  0.6640625
train loss:  0.6334353089332581
train gradient:  0.728558713805574
iteration : 144
train acc:  0.6796875
train loss:  0.5973000526428223
train gradient:  0.4812992993455536
iteration : 145
train acc:  0.671875
train loss:  0.5901939272880554
train gradient:  0.4927863389670415
iteration : 146
train acc:  0.734375
train loss:  0.5818520784378052
train gradient:  0.4625306556969371
iteration : 147
train acc:  0.7109375
train loss:  0.6030654907226562
train gradient:  0.447672643512033
iteration : 148
train acc:  0.6640625
train loss:  0.5935091972351074
train gradient:  0.4926580315191818
iteration : 149
train acc:  0.609375
train loss:  0.6464236974716187
train gradient:  0.6923630518468664
iteration : 150
train acc:  0.6640625
train loss:  0.6136226654052734
train gradient:  1.0438851700268665
iteration : 151
train acc:  0.671875
train loss:  0.5705333948135376
train gradient:  0.46357283165627877
iteration : 152
train acc:  0.59375
train loss:  0.6271770000457764
train gradient:  0.41546828189698193
iteration : 153
train acc:  0.6484375
train loss:  0.6184546947479248
train gradient:  0.38317038313701984
iteration : 154
train acc:  0.5859375
train loss:  0.6385400295257568
train gradient:  0.5957214197456802
iteration : 155
train acc:  0.625
train loss:  0.6154700517654419
train gradient:  0.6568322159213644
iteration : 156
train acc:  0.6796875
train loss:  0.5730017423629761
train gradient:  0.4281083591890955
iteration : 157
train acc:  0.625
train loss:  0.6352182030677795
train gradient:  0.6282078184195844
iteration : 158
train acc:  0.6484375
train loss:  0.6229309439659119
train gradient:  0.7619838283168978
iteration : 159
train acc:  0.6171875
train loss:  0.6671295762062073
train gradient:  0.972297787529333
iteration : 160
train acc:  0.59375
train loss:  0.6885294318199158
train gradient:  1.03076368018092
iteration : 161
train acc:  0.6796875
train loss:  0.6135069131851196
train gradient:  0.4231226364560125
iteration : 162
train acc:  0.6484375
train loss:  0.6304492354393005
train gradient:  0.5253597214488497
iteration : 163
train acc:  0.671875
train loss:  0.5782639980316162
train gradient:  0.5156366481392547
iteration : 164
train acc:  0.7265625
train loss:  0.5474523305892944
train gradient:  0.436514730610034
iteration : 165
train acc:  0.609375
train loss:  0.6434855461120605
train gradient:  0.7946835122457215
iteration : 166
train acc:  0.671875
train loss:  0.5852981209754944
train gradient:  0.43665465006776444
iteration : 167
train acc:  0.7421875
train loss:  0.5621317625045776
train gradient:  0.4599524199155421
iteration : 168
train acc:  0.6953125
train loss:  0.5922528505325317
train gradient:  0.7450428014245909
iteration : 169
train acc:  0.6640625
train loss:  0.6219764947891235
train gradient:  0.2917898507234305
iteration : 170
train acc:  0.609375
train loss:  0.6459625363349915
train gradient:  0.5547796662602679
iteration : 171
train acc:  0.734375
train loss:  0.5603575706481934
train gradient:  0.4370538379807304
iteration : 172
train acc:  0.625
train loss:  0.6045259833335876
train gradient:  0.4170760266124436
iteration : 173
train acc:  0.6484375
train loss:  0.5950609445571899
train gradient:  0.9873279644153667
iteration : 174
train acc:  0.6640625
train loss:  0.6215023994445801
train gradient:  0.6864466971878049
iteration : 175
train acc:  0.6015625
train loss:  0.6560302972793579
train gradient:  0.6663930211036256
iteration : 176
train acc:  0.6484375
train loss:  0.5882177352905273
train gradient:  0.5543960194209323
iteration : 177
train acc:  0.671875
train loss:  0.5952771306037903
train gradient:  0.7844694642567371
iteration : 178
train acc:  0.6796875
train loss:  0.5855433344841003
train gradient:  0.7200781555419427
iteration : 179
train acc:  0.6875
train loss:  0.5796166658401489
train gradient:  0.4855040313878507
iteration : 180
train acc:  0.65625
train loss:  0.600761890411377
train gradient:  0.35328249672257056
iteration : 181
train acc:  0.6171875
train loss:  0.60673987865448
train gradient:  0.3603008821690174
iteration : 182
train acc:  0.671875
train loss:  0.5775139331817627
train gradient:  0.6444737675628084
iteration : 183
train acc:  0.6796875
train loss:  0.6124789714813232
train gradient:  1.0355490056814496
iteration : 184
train acc:  0.7109375
train loss:  0.5938633680343628
train gradient:  0.5150883136328334
iteration : 185
train acc:  0.6328125
train loss:  0.6269820928573608
train gradient:  0.7382257619615897
iteration : 186
train acc:  0.671875
train loss:  0.635909914970398
train gradient:  1.6523897595506376
iteration : 187
train acc:  0.6796875
train loss:  0.6117421388626099
train gradient:  0.7637743654446268
iteration : 188
train acc:  0.6484375
train loss:  0.615270733833313
train gradient:  0.5638854292149339
iteration : 189
train acc:  0.6953125
train loss:  0.5994284152984619
train gradient:  0.6712564585744041
iteration : 190
train acc:  0.6640625
train loss:  0.6129645109176636
train gradient:  0.7584134389243306
iteration : 191
train acc:  0.546875
train loss:  0.681021511554718
train gradient:  0.7250109210979301
iteration : 192
train acc:  0.6640625
train loss:  0.5808433294296265
train gradient:  0.8485239126380806
iteration : 193
train acc:  0.609375
train loss:  0.6462364196777344
train gradient:  0.6247869509020273
iteration : 194
train acc:  0.7109375
train loss:  0.5480880737304688
train gradient:  0.9243538268892296
iteration : 195
train acc:  0.6640625
train loss:  0.5758404731750488
train gradient:  0.7117205820809795
iteration : 196
train acc:  0.6875
train loss:  0.5747982263565063
train gradient:  0.6503458837911715
iteration : 197
train acc:  0.65625
train loss:  0.6055443286895752
train gradient:  0.5823733455052438
iteration : 198
train acc:  0.6328125
train loss:  0.6487249135971069
train gradient:  0.745139280961944
iteration : 199
train acc:  0.640625
train loss:  0.5976440906524658
train gradient:  0.4760457762662633
iteration : 200
train acc:  0.6328125
train loss:  0.5991483926773071
train gradient:  0.538298332857095
iteration : 201
train acc:  0.71875
train loss:  0.5453163385391235
train gradient:  0.6605017906042454
iteration : 202
train acc:  0.7265625
train loss:  0.5794029235839844
train gradient:  0.5117627161422786
iteration : 203
train acc:  0.7421875
train loss:  0.5335406064987183
train gradient:  0.5404667731250724
iteration : 204
train acc:  0.6875
train loss:  0.636825442314148
train gradient:  0.8449377431901667
iteration : 205
train acc:  0.7265625
train loss:  0.6045993566513062
train gradient:  0.6064914593681551
iteration : 206
train acc:  0.6484375
train loss:  0.6143090128898621
train gradient:  0.6815951197478474
iteration : 207
train acc:  0.6484375
train loss:  0.627724826335907
train gradient:  0.5560446346798447
iteration : 208
train acc:  0.640625
train loss:  0.6539931297302246
train gradient:  0.4449223563796646
iteration : 209
train acc:  0.71875
train loss:  0.5912660360336304
train gradient:  0.46480651313933913
iteration : 210
train acc:  0.6484375
train loss:  0.6583762764930725
train gradient:  0.9077566970562362
iteration : 211
train acc:  0.6640625
train loss:  0.5563678741455078
train gradient:  0.43147930753491076
iteration : 212
train acc:  0.734375
train loss:  0.5792972445487976
train gradient:  0.51132261589602
iteration : 213
train acc:  0.734375
train loss:  0.5543094873428345
train gradient:  0.8629698881127572
iteration : 214
train acc:  0.6640625
train loss:  0.5817042589187622
train gradient:  0.6209768316430786
iteration : 215
train acc:  0.7265625
train loss:  0.5266116857528687
train gradient:  0.45694756039894563
iteration : 216
train acc:  0.5859375
train loss:  0.6828092336654663
train gradient:  0.864982737727281
iteration : 217
train acc:  0.6640625
train loss:  0.5990362167358398
train gradient:  0.4684248300589439
iteration : 218
train acc:  0.6875
train loss:  0.6061640381813049
train gradient:  0.8848946863118554
iteration : 219
train acc:  0.6015625
train loss:  0.6405821442604065
train gradient:  0.7735460711245066
iteration : 220
train acc:  0.7109375
train loss:  0.5427546501159668
train gradient:  0.4053378177239756
iteration : 221
train acc:  0.7265625
train loss:  0.5811865329742432
train gradient:  0.8547603347998213
iteration : 222
train acc:  0.6796875
train loss:  0.5896812677383423
train gradient:  0.6645443011221911
iteration : 223
train acc:  0.6953125
train loss:  0.5922287702560425
train gradient:  0.566622003780613
iteration : 224
train acc:  0.734375
train loss:  0.5495820641517639
train gradient:  0.4669132580709953
iteration : 225
train acc:  0.71875
train loss:  0.5564032793045044
train gradient:  0.4602019484183859
iteration : 226
train acc:  0.6875
train loss:  0.5789424777030945
train gradient:  0.8030417892244675
iteration : 227
train acc:  0.671875
train loss:  0.5704208016395569
train gradient:  0.5401058786392267
iteration : 228
train acc:  0.6640625
train loss:  0.5735986828804016
train gradient:  0.47077878543115714
iteration : 229
train acc:  0.65625
train loss:  0.6076338887214661
train gradient:  0.7759067232041416
iteration : 230
train acc:  0.703125
train loss:  0.6008615493774414
train gradient:  0.5701035900259583
iteration : 231
train acc:  0.703125
train loss:  0.5928521156311035
train gradient:  0.7566542562788071
iteration : 232
train acc:  0.703125
train loss:  0.5866208076477051
train gradient:  0.4372875257466213
iteration : 233
train acc:  0.703125
train loss:  0.5655170679092407
train gradient:  0.6729111705345678
iteration : 234
train acc:  0.7265625
train loss:  0.5472422242164612
train gradient:  0.5667089710309801
iteration : 235
train acc:  0.609375
train loss:  0.6233508586883545
train gradient:  0.939006308071562
iteration : 236
train acc:  0.625
train loss:  0.5958384275436401
train gradient:  0.6033762258130164
iteration : 237
train acc:  0.640625
train loss:  0.6324464082717896
train gradient:  0.7315635495230735
iteration : 238
train acc:  0.625
train loss:  0.6148995757102966
train gradient:  0.8451223890098531
iteration : 239
train acc:  0.6953125
train loss:  0.6067703366279602
train gradient:  0.669798504776158
iteration : 240
train acc:  0.6953125
train loss:  0.5482335090637207
train gradient:  0.47858884651216205
iteration : 241
train acc:  0.796875
train loss:  0.4970148801803589
train gradient:  0.3984027977709298
iteration : 242
train acc:  0.6484375
train loss:  0.6349173784255981
train gradient:  0.6403952840931806
iteration : 243
train acc:  0.7421875
train loss:  0.5470667481422424
train gradient:  0.43173823710209586
iteration : 244
train acc:  0.6875
train loss:  0.5776727199554443
train gradient:  0.5603299608390664
iteration : 245
train acc:  0.6484375
train loss:  0.5926169157028198
train gradient:  0.5908350737221845
iteration : 246
train acc:  0.625
train loss:  0.6276564598083496
train gradient:  0.5521255643308857
iteration : 247
train acc:  0.734375
train loss:  0.5382792949676514
train gradient:  0.41553163228805123
iteration : 248
train acc:  0.640625
train loss:  0.6051494479179382
train gradient:  0.4061974863353115
iteration : 249
train acc:  0.671875
train loss:  0.5659141540527344
train gradient:  0.48314480910859453
iteration : 250
train acc:  0.6875
train loss:  0.5865287184715271
train gradient:  0.4843117338448236
iteration : 251
train acc:  0.7421875
train loss:  0.5338221192359924
train gradient:  0.4872532073249522
iteration : 252
train acc:  0.6953125
train loss:  0.5515762567520142
train gradient:  0.49233873328161326
iteration : 253
train acc:  0.640625
train loss:  0.6122004985809326
train gradient:  0.475408697595675
iteration : 254
train acc:  0.6796875
train loss:  0.60563725233078
train gradient:  0.6726340298327883
iteration : 255
train acc:  0.703125
train loss:  0.5566431879997253
train gradient:  0.5391658706803627
iteration : 256
train acc:  0.671875
train loss:  0.6034520268440247
train gradient:  0.514097368412692
iteration : 257
train acc:  0.7421875
train loss:  0.5319476127624512
train gradient:  0.5254172481566302
iteration : 258
train acc:  0.671875
train loss:  0.5722300410270691
train gradient:  0.6060333025923679
iteration : 259
train acc:  0.71875
train loss:  0.5715611577033997
train gradient:  0.6852087620177153
iteration : 260
train acc:  0.75
train loss:  0.6157612204551697
train gradient:  3.8431075656266525
iteration : 261
train acc:  0.7265625
train loss:  0.507474422454834
train gradient:  0.35244831815127636
iteration : 262
train acc:  0.6875
train loss:  0.5477458238601685
train gradient:  0.5637751441195014
iteration : 263
train acc:  0.6484375
train loss:  0.5649281740188599
train gradient:  0.42639052423181895
iteration : 264
train acc:  0.734375
train loss:  0.5607270002365112
train gradient:  0.5294817002586691
iteration : 265
train acc:  0.7265625
train loss:  0.5555256605148315
train gradient:  0.3883124010724232
iteration : 266
train acc:  0.734375
train loss:  0.5350732803344727
train gradient:  0.39491523599119405
iteration : 267
train acc:  0.6484375
train loss:  0.6096228957176208
train gradient:  0.7386670967854383
iteration : 268
train acc:  0.6953125
train loss:  0.5888369083404541
train gradient:  0.577613186759903
iteration : 269
train acc:  0.7109375
train loss:  0.5471560955047607
train gradient:  0.4477235056683359
iteration : 270
train acc:  0.75
train loss:  0.5352550745010376
train gradient:  0.4644998109440056
iteration : 271
train acc:  0.6484375
train loss:  0.5804387331008911
train gradient:  0.7267529812282039
iteration : 272
train acc:  0.75
train loss:  0.5282003879547119
train gradient:  0.6114036227266539
iteration : 273
train acc:  0.78125
train loss:  0.5046868324279785
train gradient:  0.33510886141621543
iteration : 274
train acc:  0.7109375
train loss:  0.5571041107177734
train gradient:  0.4936493211666524
iteration : 275
train acc:  0.7421875
train loss:  0.5249332785606384
train gradient:  0.7095773560589721
iteration : 276
train acc:  0.75
train loss:  0.5185719728469849
train gradient:  0.6482934235540496
iteration : 277
train acc:  0.6953125
train loss:  0.585874080657959
train gradient:  0.46086333941555585
iteration : 278
train acc:  0.6875
train loss:  0.5539721250534058
train gradient:  0.6508882562903555
iteration : 279
train acc:  0.65625
train loss:  0.6053750514984131
train gradient:  0.6807863806216554
iteration : 280
train acc:  0.7265625
train loss:  0.5645958185195923
train gradient:  0.39144976644072715
iteration : 281
train acc:  0.6171875
train loss:  0.6707776784896851
train gradient:  0.8729415318606591
iteration : 282
train acc:  0.5546875
train loss:  0.6468878984451294
train gradient:  1.0728756804564987
iteration : 283
train acc:  0.6953125
train loss:  0.6251707077026367
train gradient:  0.8761631319171544
iteration : 284
train acc:  0.6875
train loss:  0.5994659662246704
train gradient:  0.8466057520068959
iteration : 285
train acc:  0.75
train loss:  0.5180070400238037
train gradient:  0.4535159207445731
iteration : 286
train acc:  0.7890625
train loss:  0.4831885099411011
train gradient:  0.40111078614919987
iteration : 287
train acc:  0.65625
train loss:  0.6024789810180664
train gradient:  0.4899766073659778
iteration : 288
train acc:  0.703125
train loss:  0.5660156607627869
train gradient:  0.4133832708067127
iteration : 289
train acc:  0.6953125
train loss:  0.5782991647720337
train gradient:  0.7578834937076833
iteration : 290
train acc:  0.75
train loss:  0.5582929253578186
train gradient:  0.4348817920316066
iteration : 291
train acc:  0.6640625
train loss:  0.5983240604400635
train gradient:  0.7581286860435063
iteration : 292
train acc:  0.7578125
train loss:  0.537501335144043
train gradient:  0.3916410870004769
iteration : 293
train acc:  0.671875
train loss:  0.580607533454895
train gradient:  0.30015029223939615
iteration : 294
train acc:  0.671875
train loss:  0.619530439376831
train gradient:  0.7272576703356652
iteration : 295
train acc:  0.6953125
train loss:  0.5918139815330505
train gradient:  0.6194411224952138
iteration : 296
train acc:  0.6875
train loss:  0.5697286128997803
train gradient:  0.5795620284039129
iteration : 297
train acc:  0.6953125
train loss:  0.6220716238021851
train gradient:  1.2683363747126606
iteration : 298
train acc:  0.734375
train loss:  0.5778827667236328
train gradient:  0.407261763313314
iteration : 299
train acc:  0.7265625
train loss:  0.5180708169937134
train gradient:  0.43005682860001154
iteration : 300
train acc:  0.765625
train loss:  0.4784051775932312
train gradient:  0.5524049991877602
iteration : 301
train acc:  0.6953125
train loss:  0.6067530512809753
train gradient:  1.231769404247087
iteration : 302
train acc:  0.7578125
train loss:  0.5241252183914185
train gradient:  0.3326941256841622
iteration : 303
train acc:  0.6953125
train loss:  0.569844126701355
train gradient:  0.6914988017586942
iteration : 304
train acc:  0.703125
train loss:  0.5728268623352051
train gradient:  0.7236653216164144
iteration : 305
train acc:  0.71875
train loss:  0.5619505643844604
train gradient:  0.7847409804120716
iteration : 306
train acc:  0.6328125
train loss:  0.5970921516418457
train gradient:  0.6611388222582701
iteration : 307
train acc:  0.703125
train loss:  0.5522873997688293
train gradient:  0.8620481754617983
iteration : 308
train acc:  0.578125
train loss:  0.6601150631904602
train gradient:  1.0544505002290425
iteration : 309
train acc:  0.7109375
train loss:  0.5468571186065674
train gradient:  0.6039856414054408
iteration : 310
train acc:  0.6953125
train loss:  0.5809196829795837
train gradient:  0.8594767365546697
iteration : 311
train acc:  0.671875
train loss:  0.5565158724784851
train gradient:  0.48763745694315774
iteration : 312
train acc:  0.7109375
train loss:  0.590070903301239
train gradient:  0.5483864646182561
iteration : 313
train acc:  0.671875
train loss:  0.604282796382904
train gradient:  0.6058686369593065
iteration : 314
train acc:  0.7421875
train loss:  0.5062872171401978
train gradient:  0.3048244277281614
iteration : 315
train acc:  0.7421875
train loss:  0.5185499787330627
train gradient:  0.5193880629774672
iteration : 316
train acc:  0.6796875
train loss:  0.5727749466896057
train gradient:  0.4493785139151635
iteration : 317
train acc:  0.7265625
train loss:  0.5378016829490662
train gradient:  0.7012929229710246
iteration : 318
train acc:  0.703125
train loss:  0.543454110622406
train gradient:  0.5949207317096902
iteration : 319
train acc:  0.640625
train loss:  0.6050136089324951
train gradient:  0.7534499016590424
iteration : 320
train acc:  0.6796875
train loss:  0.5992906093597412
train gradient:  0.9617453898745734
iteration : 321
train acc:  0.7109375
train loss:  0.5564422011375427
train gradient:  0.40672689111205496
iteration : 322
train acc:  0.6875
train loss:  0.5504634380340576
train gradient:  0.5005924589753727
iteration : 323
train acc:  0.703125
train loss:  0.5779197216033936
train gradient:  0.6271033264636974
iteration : 324
train acc:  0.71875
train loss:  0.53285151720047
train gradient:  0.4718743575303156
iteration : 325
train acc:  0.7578125
train loss:  0.5267451405525208
train gradient:  0.5696207792149911
iteration : 326
train acc:  0.671875
train loss:  0.6013453006744385
train gradient:  0.5268748827173207
iteration : 327
train acc:  0.6640625
train loss:  0.6398634910583496
train gradient:  0.7196895909725868
iteration : 328
train acc:  0.71875
train loss:  0.5696970224380493
train gradient:  0.6682318670044854
iteration : 329
train acc:  0.6953125
train loss:  0.5380798578262329
train gradient:  0.5153800789466276
iteration : 330
train acc:  0.7734375
train loss:  0.5295052528381348
train gradient:  0.7694811922872555
iteration : 331
train acc:  0.6953125
train loss:  0.5502545237541199
train gradient:  0.567771858285855
iteration : 332
train acc:  0.6953125
train loss:  0.5632137060165405
train gradient:  0.6582944368802639
iteration : 333
train acc:  0.71875
train loss:  0.5591776371002197
train gradient:  0.46927719021776143
iteration : 334
train acc:  0.640625
train loss:  0.619305431842804
train gradient:  0.6141366624886248
iteration : 335
train acc:  0.625
train loss:  0.6073964238166809
train gradient:  0.8117423221616301
iteration : 336
train acc:  0.6953125
train loss:  0.5527811646461487
train gradient:  0.49978083796439515
iteration : 337
train acc:  0.8046875
train loss:  0.46905404329299927
train gradient:  0.42125893310145995
iteration : 338
train acc:  0.703125
train loss:  0.518940806388855
train gradient:  0.5264838612303842
iteration : 339
train acc:  0.6875
train loss:  0.6158417463302612
train gradient:  0.8143908487379914
iteration : 340
train acc:  0.734375
train loss:  0.5300400257110596
train gradient:  0.6360744360415476
iteration : 341
train acc:  0.7578125
train loss:  0.5460230112075806
train gradient:  0.4068452746745365
iteration : 342
train acc:  0.6640625
train loss:  0.5697476863861084
train gradient:  0.9385437482969363
iteration : 343
train acc:  0.6953125
train loss:  0.5696855187416077
train gradient:  0.9074669104176664
iteration : 344
train acc:  0.65625
train loss:  0.5773441195487976
train gradient:  0.5332722642778138
iteration : 345
train acc:  0.703125
train loss:  0.5355437397956848
train gradient:  0.6053039088885167
iteration : 346
train acc:  0.75
train loss:  0.5243262052536011
train gradient:  0.407469290192824
iteration : 347
train acc:  0.6953125
train loss:  0.5569156408309937
train gradient:  0.3951945799351803
iteration : 348
train acc:  0.6796875
train loss:  0.5562194585800171
train gradient:  0.6138106599520562
iteration : 349
train acc:  0.6484375
train loss:  0.6139944791793823
train gradient:  0.8366614199266288
iteration : 350
train acc:  0.7265625
train loss:  0.5273528099060059
train gradient:  0.47364067460625564
iteration : 351
train acc:  0.6328125
train loss:  0.6286976337432861
train gradient:  0.7121408370317089
iteration : 352
train acc:  0.6875
train loss:  0.5940178632736206
train gradient:  0.9478374300873948
iteration : 353
train acc:  0.65625
train loss:  0.6219528913497925
train gradient:  0.5011088233819806
iteration : 354
train acc:  0.7109375
train loss:  0.5295500755310059
train gradient:  0.7315021824791403
iteration : 355
train acc:  0.75
train loss:  0.5248602628707886
train gradient:  0.3806262802313808
iteration : 356
train acc:  0.7421875
train loss:  0.5229378938674927
train gradient:  0.4529311768941349
iteration : 357
train acc:  0.7109375
train loss:  0.5564610958099365
train gradient:  0.49424249043263613
iteration : 358
train acc:  0.6953125
train loss:  0.5231476426124573
train gradient:  0.4842663464754176
iteration : 359
train acc:  0.8125
train loss:  0.5030885934829712
train gradient:  0.41440030078743406
iteration : 360
train acc:  0.625
train loss:  0.6413496136665344
train gradient:  0.7188770092875096
iteration : 361
train acc:  0.6796875
train loss:  0.6074709892272949
train gradient:  1.129465899284094
iteration : 362
train acc:  0.765625
train loss:  0.48786070942878723
train gradient:  0.3554379261023246
iteration : 363
train acc:  0.703125
train loss:  0.5520191192626953
train gradient:  0.5881325501015802
iteration : 364
train acc:  0.6953125
train loss:  0.5536137819290161
train gradient:  0.5786067004452304
iteration : 365
train acc:  0.71875
train loss:  0.4854307770729065
train gradient:  0.6217825681312993
iteration : 366
train acc:  0.75
train loss:  0.49131083488464355
train gradient:  0.48638002085223425
iteration : 367
train acc:  0.734375
train loss:  0.5681670904159546
train gradient:  1.0007660471014657
iteration : 368
train acc:  0.671875
train loss:  0.6094756126403809
train gradient:  0.5305950133904433
iteration : 369
train acc:  0.65625
train loss:  0.5498844385147095
train gradient:  0.7181174121914308
iteration : 370
train acc:  0.7734375
train loss:  0.46092650294303894
train gradient:  0.541128426680606
iteration : 371
train acc:  0.734375
train loss:  0.5571367740631104
train gradient:  0.44661041493045683
iteration : 372
train acc:  0.6484375
train loss:  0.6209322214126587
train gradient:  0.621182953753326
iteration : 373
train acc:  0.6875
train loss:  0.531028151512146
train gradient:  0.3707414437550022
iteration : 374
train acc:  0.78125
train loss:  0.5025584101676941
train gradient:  0.5338873325026788
iteration : 375
train acc:  0.734375
train loss:  0.5310225486755371
train gradient:  0.565259870884356
iteration : 376
train acc:  0.7109375
train loss:  0.5125842094421387
train gradient:  0.5475460955952134
iteration : 377
train acc:  0.7421875
train loss:  0.5686701536178589
train gradient:  0.5157437256323562
iteration : 378
train acc:  0.6640625
train loss:  0.5818212032318115
train gradient:  0.6502534035968207
iteration : 379
train acc:  0.71875
train loss:  0.5478734970092773
train gradient:  0.6502258924032207
iteration : 380
train acc:  0.71875
train loss:  0.5816636085510254
train gradient:  0.5061888542832577
iteration : 381
train acc:  0.75
train loss:  0.5149672031402588
train gradient:  0.3779640343054144
iteration : 382
train acc:  0.7421875
train loss:  0.5359061360359192
train gradient:  0.5206491554795922
iteration : 383
train acc:  0.734375
train loss:  0.4988098740577698
train gradient:  0.5046152337799477
iteration : 384
train acc:  0.78125
train loss:  0.4795167148113251
train gradient:  0.44491862967118573
iteration : 385
train acc:  0.6953125
train loss:  0.5256792306900024
train gradient:  0.520485330264391
iteration : 386
train acc:  0.671875
train loss:  0.5655322074890137
train gradient:  0.68566624308184
iteration : 387
train acc:  0.6328125
train loss:  0.6299835443496704
train gradient:  0.9328689340303205
iteration : 388
train acc:  0.7421875
train loss:  0.5303083658218384
train gradient:  0.5266327761976339
iteration : 389
train acc:  0.75
train loss:  0.5004079341888428
train gradient:  0.4328735024485226
iteration : 390
train acc:  0.71875
train loss:  0.5430288314819336
train gradient:  0.6222311360164332
iteration : 391
train acc:  0.703125
train loss:  0.5901815891265869
train gradient:  0.5471523228648998
iteration : 392
train acc:  0.78125
train loss:  0.5113527774810791
train gradient:  0.3797820165120295
iteration : 393
train acc:  0.6953125
train loss:  0.5796346664428711
train gradient:  0.6827185020446336
iteration : 394
train acc:  0.734375
train loss:  0.5450121164321899
train gradient:  0.42832234812867165
iteration : 395
train acc:  0.78125
train loss:  0.49142253398895264
train gradient:  0.46722114192988484
iteration : 396
train acc:  0.6875
train loss:  0.5604814291000366
train gradient:  0.4978080454176888
iteration : 397
train acc:  0.625
train loss:  0.622199535369873
train gradient:  0.6131922301724324
iteration : 398
train acc:  0.6953125
train loss:  0.5573612451553345
train gradient:  0.47125659919007656
iteration : 399
train acc:  0.6953125
train loss:  0.5615119934082031
train gradient:  0.47883823817582255
iteration : 400
train acc:  0.7421875
train loss:  0.518682599067688
train gradient:  0.6459080460451893
iteration : 401
train acc:  0.7734375
train loss:  0.5184462070465088
train gradient:  0.5923459120250711
iteration : 402
train acc:  0.75
train loss:  0.5489305853843689
train gradient:  0.8654407844734799
iteration : 403
train acc:  0.75
train loss:  0.5269036889076233
train gradient:  0.5645181954556235
iteration : 404
train acc:  0.6875
train loss:  0.5644039511680603
train gradient:  0.5898757867561707
iteration : 405
train acc:  0.75
train loss:  0.5582797527313232
train gradient:  0.5943752466997483
iteration : 406
train acc:  0.703125
train loss:  0.5820838212966919
train gradient:  0.6745017957020997
iteration : 407
train acc:  0.703125
train loss:  0.5341504216194153
train gradient:  0.6516584139510814
iteration : 408
train acc:  0.75
train loss:  0.5293658971786499
train gradient:  0.4292704817164988
iteration : 409
train acc:  0.703125
train loss:  0.5311342477798462
train gradient:  0.5937140852730085
iteration : 410
train acc:  0.7109375
train loss:  0.5630871057510376
train gradient:  0.6089883468846626
iteration : 411
train acc:  0.75
train loss:  0.5692536234855652
train gradient:  0.5463802505542203
iteration : 412
train acc:  0.671875
train loss:  0.608599066734314
train gradient:  0.7179632555771327
iteration : 413
train acc:  0.7265625
train loss:  0.5100114345550537
train gradient:  0.5815697191158917
iteration : 414
train acc:  0.71875
train loss:  0.5609668493270874
train gradient:  0.5447275294660257
iteration : 415
train acc:  0.8046875
train loss:  0.49932122230529785
train gradient:  0.5832829018072612
iteration : 416
train acc:  0.7109375
train loss:  0.5941968560218811
train gradient:  0.7191981875534589
iteration : 417
train acc:  0.7265625
train loss:  0.50396329164505
train gradient:  0.5740097786693001
iteration : 418
train acc:  0.7265625
train loss:  0.5571548938751221
train gradient:  0.4760250061235304
iteration : 419
train acc:  0.703125
train loss:  0.5771574378013611
train gradient:  0.8661821483395568
iteration : 420
train acc:  0.734375
train loss:  0.5417810082435608
train gradient:  0.5771260322031668
iteration : 421
train acc:  0.7109375
train loss:  0.5218762159347534
train gradient:  0.46206440259553244
iteration : 422
train acc:  0.6796875
train loss:  0.5706141591072083
train gradient:  0.5811485146169443
iteration : 423
train acc:  0.8125
train loss:  0.4628893733024597
train gradient:  0.46657975322531897
iteration : 424
train acc:  0.71875
train loss:  0.5588477849960327
train gradient:  0.45689680785140185
iteration : 425
train acc:  0.6875
train loss:  0.5473041534423828
train gradient:  0.4190350525861061
iteration : 426
train acc:  0.75
train loss:  0.5147445201873779
train gradient:  0.552328594656764
iteration : 427
train acc:  0.7265625
train loss:  0.5109214782714844
train gradient:  0.574631494599843
iteration : 428
train acc:  0.7890625
train loss:  0.4706067740917206
train gradient:  0.4533971900122663
iteration : 429
train acc:  0.765625
train loss:  0.48451048135757446
train gradient:  0.3604788514890772
iteration : 430
train acc:  0.6796875
train loss:  0.6217628717422485
train gradient:  0.8190284846965502
iteration : 431
train acc:  0.7265625
train loss:  0.49995002150535583
train gradient:  0.3818927705145685
iteration : 432
train acc:  0.8125
train loss:  0.46213245391845703
train gradient:  0.4361870397653167
iteration : 433
train acc:  0.7421875
train loss:  0.5098980665206909
train gradient:  0.635725397632104
iteration : 434
train acc:  0.7578125
train loss:  0.5237563848495483
train gradient:  0.46827148879876895
iteration : 435
train acc:  0.6796875
train loss:  0.5378139019012451
train gradient:  0.38091828120183474
iteration : 436
train acc:  0.71875
train loss:  0.5415120720863342
train gradient:  0.5643877552211148
iteration : 437
train acc:  0.7421875
train loss:  0.5384831428527832
train gradient:  0.785300102366257
iteration : 438
train acc:  0.75
train loss:  0.5235310196876526
train gradient:  0.7437806513683527
iteration : 439
train acc:  0.7734375
train loss:  0.474901407957077
train gradient:  0.39341133847012694
iteration : 440
train acc:  0.7578125
train loss:  0.50515216588974
train gradient:  0.6717497174566112
iteration : 441
train acc:  0.8125
train loss:  0.46911996603012085
train gradient:  0.5964771929455308
iteration : 442
train acc:  0.71875
train loss:  0.5631797313690186
train gradient:  0.6704716977448748
iteration : 443
train acc:  0.7734375
train loss:  0.47538459300994873
train gradient:  0.42234835057524617
iteration : 444
train acc:  0.796875
train loss:  0.5051990747451782
train gradient:  0.5027312747394383
iteration : 445
train acc:  0.7421875
train loss:  0.5548794865608215
train gradient:  0.612782138857679
iteration : 446
train acc:  0.75
train loss:  0.5487399101257324
train gradient:  0.5701580232746519
iteration : 447
train acc:  0.7109375
train loss:  0.5494011640548706
train gradient:  0.46369972890469763
iteration : 448
train acc:  0.734375
train loss:  0.5191028118133545
train gradient:  0.517913347152379
iteration : 449
train acc:  0.75
train loss:  0.5199728012084961
train gradient:  0.52458848213549
iteration : 450
train acc:  0.7578125
train loss:  0.5196874141693115
train gradient:  0.5076720513918657
iteration : 451
train acc:  0.7421875
train loss:  0.5299724340438843
train gradient:  0.9885994254390178
iteration : 452
train acc:  0.640625
train loss:  0.6262634992599487
train gradient:  0.9265601513409687
iteration : 453
train acc:  0.71875
train loss:  0.5308831930160522
train gradient:  0.8080518533033301
iteration : 454
train acc:  0.6796875
train loss:  0.5848411321640015
train gradient:  0.7241034895360636
iteration : 455
train acc:  0.84375
train loss:  0.47065919637680054
train gradient:  0.5043368387881053
iteration : 456
train acc:  0.6796875
train loss:  0.5847468972206116
train gradient:  0.6857587838564502
iteration : 457
train acc:  0.6953125
train loss:  0.5551108121871948
train gradient:  0.8376632838097879
iteration : 458
train acc:  0.7421875
train loss:  0.5864531993865967
train gradient:  0.7321612351813285
iteration : 459
train acc:  0.7734375
train loss:  0.4843791425228119
train gradient:  0.6232288227624414
iteration : 460
train acc:  0.765625
train loss:  0.45325005054473877
train gradient:  0.526270894874668
iteration : 461
train acc:  0.7109375
train loss:  0.5177580118179321
train gradient:  0.508608255974328
iteration : 462
train acc:  0.78125
train loss:  0.5043506622314453
train gradient:  0.42313430331170426
iteration : 463
train acc:  0.7109375
train loss:  0.5626997351646423
train gradient:  0.531136975506571
iteration : 464
train acc:  0.75
train loss:  0.5511765480041504
train gradient:  0.6725282172220673
iteration : 465
train acc:  0.765625
train loss:  0.4691806435585022
train gradient:  0.5309691651210136
iteration : 466
train acc:  0.7109375
train loss:  0.5791109800338745
train gradient:  0.808203514760475
iteration : 467
train acc:  0.734375
train loss:  0.5349975228309631
train gradient:  0.5620599653446856
iteration : 468
train acc:  0.734375
train loss:  0.5481669902801514
train gradient:  0.6043252568432573
iteration : 469
train acc:  0.7734375
train loss:  0.5080753564834595
train gradient:  0.4663293297442549
iteration : 470
train acc:  0.71875
train loss:  0.5138643980026245
train gradient:  0.6007936086083473
iteration : 471
train acc:  0.734375
train loss:  0.5422421097755432
train gradient:  0.47150479817584545
iteration : 472
train acc:  0.703125
train loss:  0.5524511337280273
train gradient:  0.585737455725588
iteration : 473
train acc:  0.6796875
train loss:  0.5791402459144592
train gradient:  0.5418832479048022
iteration : 474
train acc:  0.6796875
train loss:  0.6008249521255493
train gradient:  0.738704931941705
iteration : 475
train acc:  0.7109375
train loss:  0.5758506059646606
train gradient:  0.7096681847181683
iteration : 476
train acc:  0.7734375
train loss:  0.46675390005111694
train gradient:  0.36902181599061745
iteration : 477
train acc:  0.765625
train loss:  0.4744325280189514
train gradient:  0.4919738401573128
iteration : 478
train acc:  0.7734375
train loss:  0.5039074420928955
train gradient:  0.7438157964462944
iteration : 479
train acc:  0.7578125
train loss:  0.5613733530044556
train gradient:  0.5431240867460735
iteration : 480
train acc:  0.7421875
train loss:  0.5150330066680908
train gradient:  0.6682240374150115
iteration : 481
train acc:  0.65625
train loss:  0.6143661737442017
train gradient:  0.7703383748510051
iteration : 482
train acc:  0.7109375
train loss:  0.4916786551475525
train gradient:  0.46197054317027647
iteration : 483
train acc:  0.7265625
train loss:  0.47875022888183594
train gradient:  0.5734759322668875
iteration : 484
train acc:  0.8125
train loss:  0.4640427231788635
train gradient:  0.3703664243837766
iteration : 485
train acc:  0.7578125
train loss:  0.5515173673629761
train gradient:  0.6121436655873952
iteration : 486
train acc:  0.71875
train loss:  0.53072190284729
train gradient:  0.6493657373437339
iteration : 487
train acc:  0.7734375
train loss:  0.4960835874080658
train gradient:  0.4639759928960063
iteration : 488
train acc:  0.6796875
train loss:  0.5527907013893127
train gradient:  0.6116442842285617
iteration : 489
train acc:  0.796875
train loss:  0.45573538541793823
train gradient:  0.5463211034028168
iteration : 490
train acc:  0.7734375
train loss:  0.47703856229782104
train gradient:  0.7030526738493272
iteration : 491
train acc:  0.703125
train loss:  0.5371294021606445
train gradient:  0.5144532716820639
iteration : 492
train acc:  0.7109375
train loss:  0.5581079721450806
train gradient:  0.4525924135251241
iteration : 493
train acc:  0.7265625
train loss:  0.5581358671188354
train gradient:  0.704295814908681
iteration : 494
train acc:  0.6796875
train loss:  0.5625413656234741
train gradient:  0.5738492657005431
iteration : 495
train acc:  0.71875
train loss:  0.5703446269035339
train gradient:  0.6703229096404808
iteration : 496
train acc:  0.71875
train loss:  0.5188623666763306
train gradient:  0.46961774069826967
iteration : 497
train acc:  0.7734375
train loss:  0.46311673521995544
train gradient:  0.40131314430243187
iteration : 498
train acc:  0.75
train loss:  0.49676066637039185
train gradient:  0.5609291430410495
iteration : 499
train acc:  0.71875
train loss:  0.4954509139060974
train gradient:  0.5550483660510296
iteration : 500
train acc:  0.796875
train loss:  0.4622271955013275
train gradient:  0.3975537718253496
iteration : 501
train acc:  0.6640625
train loss:  0.6342769861221313
train gradient:  0.838369743478961
iteration : 502
train acc:  0.6796875
train loss:  0.5646564960479736
train gradient:  0.6723744234730108
iteration : 503
train acc:  0.75
train loss:  0.4897499084472656
train gradient:  0.5654710493452134
iteration : 504
train acc:  0.71875
train loss:  0.5182006359100342
train gradient:  0.5957790592942918
iteration : 505
train acc:  0.7109375
train loss:  0.5630578994750977
train gradient:  0.6214888279023194
iteration : 506
train acc:  0.7890625
train loss:  0.45412200689315796
train gradient:  0.4772361840874756
iteration : 507
train acc:  0.8125
train loss:  0.4453731179237366
train gradient:  0.2863390691931654
iteration : 508
train acc:  0.7578125
train loss:  0.5242165923118591
train gradient:  0.5826786287265222
iteration : 509
train acc:  0.75
train loss:  0.5078415870666504
train gradient:  0.5036531839497902
iteration : 510
train acc:  0.6953125
train loss:  0.5710320472717285
train gradient:  0.5079163307629213
iteration : 511
train acc:  0.7734375
train loss:  0.5447471141815186
train gradient:  0.4417314077368224
iteration : 512
train acc:  0.7265625
train loss:  0.5667085647583008
train gradient:  0.7162581076837464
iteration : 513
train acc:  0.734375
train loss:  0.5472087860107422
train gradient:  0.5253129193652808
iteration : 514
train acc:  0.7734375
train loss:  0.4792449176311493
train gradient:  0.46353078325388164
iteration : 515
train acc:  0.75
train loss:  0.511661171913147
train gradient:  0.48988018633387775
iteration : 516
train acc:  0.7578125
train loss:  0.498568594455719
train gradient:  0.38089135947844444
iteration : 517
train acc:  0.7265625
train loss:  0.5492976307868958
train gradient:  0.6651400227109857
iteration : 518
train acc:  0.75
train loss:  0.521793782711029
train gradient:  0.5033758350389725
iteration : 519
train acc:  0.7734375
train loss:  0.5473541021347046
train gradient:  1.2718786893099547
iteration : 520
train acc:  0.734375
train loss:  0.5379414558410645
train gradient:  0.5952881536263281
iteration : 521
train acc:  0.703125
train loss:  0.5235490202903748
train gradient:  0.4584922241999348
iteration : 522
train acc:  0.71875
train loss:  0.5201553106307983
train gradient:  0.594124445312179
iteration : 523
train acc:  0.6953125
train loss:  0.5494178533554077
train gradient:  0.7611459790044365
iteration : 524
train acc:  0.7265625
train loss:  0.5295966863632202
train gradient:  0.5658559160259136
iteration : 525
train acc:  0.75
train loss:  0.5493581891059875
train gradient:  0.5269871224325613
iteration : 526
train acc:  0.703125
train loss:  0.5351905226707458
train gradient:  0.6277341034698054
iteration : 527
train acc:  0.7421875
train loss:  0.4935699999332428
train gradient:  0.555121954601055
iteration : 528
train acc:  0.671875
train loss:  0.6258121728897095
train gradient:  0.7709446906527908
iteration : 529
train acc:  0.7265625
train loss:  0.5251352787017822
train gradient:  0.6001677447556588
iteration : 530
train acc:  0.7421875
train loss:  0.512430727481842
train gradient:  0.5430304221094597
iteration : 531
train acc:  0.6953125
train loss:  0.5647334456443787
train gradient:  0.6368685093857441
iteration : 532
train acc:  0.7734375
train loss:  0.5464924573898315
train gradient:  0.7354021158374009
iteration : 533
train acc:  0.7421875
train loss:  0.49764326214790344
train gradient:  0.5937701266132382
iteration : 534
train acc:  0.734375
train loss:  0.49460721015930176
train gradient:  0.5250923325174575
iteration : 535
train acc:  0.796875
train loss:  0.4653353989124298
train gradient:  0.4052272192478502
iteration : 536
train acc:  0.7421875
train loss:  0.5195757150650024
train gradient:  0.519772756040545
iteration : 537
train acc:  0.75
train loss:  0.5265278816223145
train gradient:  0.39248409237708237
iteration : 538
train acc:  0.703125
train loss:  0.5764981508255005
train gradient:  0.5301446155535021
iteration : 539
train acc:  0.6953125
train loss:  0.5418633222579956
train gradient:  0.47317431311973607
iteration : 540
train acc:  0.7734375
train loss:  0.4779035449028015
train gradient:  0.41130236795692876
iteration : 541
train acc:  0.7421875
train loss:  0.5411939024925232
train gradient:  0.6602497127537269
iteration : 542
train acc:  0.703125
train loss:  0.5491325855255127
train gradient:  0.5855130791951636
iteration : 543
train acc:  0.703125
train loss:  0.530767560005188
train gradient:  0.5557905190434571
iteration : 544
train acc:  0.765625
train loss:  0.5341944694519043
train gradient:  0.4946953634307543
iteration : 545
train acc:  0.6875
train loss:  0.5901148319244385
train gradient:  0.5558545700568294
iteration : 546
train acc:  0.7109375
train loss:  0.5635817646980286
train gradient:  0.392353592845263
iteration : 547
train acc:  0.6640625
train loss:  0.6164459586143494
train gradient:  0.8965526270461892
iteration : 548
train acc:  0.7109375
train loss:  0.5511449575424194
train gradient:  0.5002675063473276
iteration : 549
train acc:  0.75
train loss:  0.5646989941596985
train gradient:  0.5152011761660578
iteration : 550
train acc:  0.6953125
train loss:  0.5455916523933411
train gradient:  0.5533831354708727
iteration : 551
train acc:  0.8359375
train loss:  0.4274371266365051
train gradient:  0.32601700575909864
iteration : 552
train acc:  0.7890625
train loss:  0.46984803676605225
train gradient:  0.3842842591618354
iteration : 553
train acc:  0.6640625
train loss:  0.5804707407951355
train gradient:  0.5062592222444754
iteration : 554
train acc:  0.7109375
train loss:  0.5414155125617981
train gradient:  0.49340422375763093
iteration : 555
train acc:  0.71875
train loss:  0.5690277814865112
train gradient:  0.7220272071094878
iteration : 556
train acc:  0.8046875
train loss:  0.4620618224143982
train gradient:  0.28411385375222525
iteration : 557
train acc:  0.796875
train loss:  0.4536168575286865
train gradient:  0.3426399481181117
iteration : 558
train acc:  0.7734375
train loss:  0.46720466017723083
train gradient:  0.43777773645982526
iteration : 559
train acc:  0.734375
train loss:  0.5341289043426514
train gradient:  0.5365481791254609
iteration : 560
train acc:  0.7578125
train loss:  0.5141515135765076
train gradient:  0.3668759325054628
iteration : 561
train acc:  0.8125
train loss:  0.45272380113601685
train gradient:  0.6689377745407734
iteration : 562
train acc:  0.75
train loss:  0.5096455812454224
train gradient:  0.5722047162786181
iteration : 563
train acc:  0.7265625
train loss:  0.4987109899520874
train gradient:  1.152312510443964
iteration : 564
train acc:  0.7109375
train loss:  0.5416720509529114
train gradient:  0.46003401124082904
iteration : 565
train acc:  0.78125
train loss:  0.5254747867584229
train gradient:  0.589145117777232
iteration : 566
train acc:  0.7578125
train loss:  0.5268384218215942
train gradient:  0.6050067224087026
iteration : 567
train acc:  0.7421875
train loss:  0.5268149375915527
train gradient:  0.699965552069287
iteration : 568
train acc:  0.7421875
train loss:  0.5070802569389343
train gradient:  0.5641577150444392
iteration : 569
train acc:  0.765625
train loss:  0.5286648273468018
train gradient:  0.7129582082990278
iteration : 570
train acc:  0.796875
train loss:  0.45457595586776733
train gradient:  0.3951347688272146
iteration : 571
train acc:  0.71875
train loss:  0.5170047283172607
train gradient:  0.6131869579308085
iteration : 572
train acc:  0.8125
train loss:  0.45383647084236145
train gradient:  0.36947397587674513
iteration : 573
train acc:  0.7890625
train loss:  0.4939163625240326
train gradient:  0.5762773974182173
iteration : 574
train acc:  0.7265625
train loss:  0.5382887721061707
train gradient:  0.6030178821242397
iteration : 575
train acc:  0.765625
train loss:  0.4875388741493225
train gradient:  0.415583514937522
iteration : 576
train acc:  0.671875
train loss:  0.5977757573127747
train gradient:  0.6230039361471736
iteration : 577
train acc:  0.78125
train loss:  0.48689574003219604
train gradient:  0.47684808278319873
iteration : 578
train acc:  0.7265625
train loss:  0.579757034778595
train gradient:  0.8061850565527685
iteration : 579
train acc:  0.765625
train loss:  0.47477033734321594
train gradient:  0.5752181031694648
iteration : 580
train acc:  0.734375
train loss:  0.49803364276885986
train gradient:  0.5540835644505923
iteration : 581
train acc:  0.6953125
train loss:  0.5819971561431885
train gradient:  0.7179591565597071
iteration : 582
train acc:  0.7734375
train loss:  0.503028392791748
train gradient:  0.660580970911775
iteration : 583
train acc:  0.7109375
train loss:  0.5333893895149231
train gradient:  0.46459500921855074
iteration : 584
train acc:  0.796875
train loss:  0.45909446477890015
train gradient:  0.5768026234061028
iteration : 585
train acc:  0.7578125
train loss:  0.511306881904602
train gradient:  0.429251758171915
iteration : 586
train acc:  0.703125
train loss:  0.6074477434158325
train gradient:  0.7509301532437302
iteration : 587
train acc:  0.7734375
train loss:  0.4779605567455292
train gradient:  0.6533097419394869
iteration : 588
train acc:  0.7421875
train loss:  0.535391092300415
train gradient:  0.61888772336453
iteration : 589
train acc:  0.671875
train loss:  0.5957807302474976
train gradient:  0.745184914869053
iteration : 590
train acc:  0.7265625
train loss:  0.5707940459251404
train gradient:  0.6525396297820939
iteration : 591
train acc:  0.7734375
train loss:  0.4522339403629303
train gradient:  0.47790095706707536
iteration : 592
train acc:  0.796875
train loss:  0.4673398435115814
train gradient:  0.6344731488403204
iteration : 593
train acc:  0.796875
train loss:  0.4454973340034485
train gradient:  0.5826238128961143
iteration : 594
train acc:  0.734375
train loss:  0.5378705263137817
train gradient:  0.5044841473313204
iteration : 595
train acc:  0.796875
train loss:  0.4585609436035156
train gradient:  0.42743415947637736
iteration : 596
train acc:  0.7734375
train loss:  0.46689340472221375
train gradient:  0.45623516435389105
iteration : 597
train acc:  0.765625
train loss:  0.5088487267494202
train gradient:  0.47182969336961067
iteration : 598
train acc:  0.7421875
train loss:  0.5122883319854736
train gradient:  0.45023369383847195
iteration : 599
train acc:  0.75
train loss:  0.45569348335266113
train gradient:  0.5885340065072107
iteration : 600
train acc:  0.7265625
train loss:  0.5144063234329224
train gradient:  0.5842594640763461
iteration : 601
train acc:  0.765625
train loss:  0.46144190430641174
train gradient:  0.40982761892045205
iteration : 602
train acc:  0.6953125
train loss:  0.5511864423751831
train gradient:  0.6343211766800723
iteration : 603
train acc:  0.7734375
train loss:  0.47765830159187317
train gradient:  0.4638606368475789
iteration : 604
train acc:  0.6640625
train loss:  0.6078624129295349
train gradient:  0.7789585014293724
iteration : 605
train acc:  0.7578125
train loss:  0.5203704833984375
train gradient:  0.5289824078430694
iteration : 606
train acc:  0.7265625
train loss:  0.5280375480651855
train gradient:  0.599834487899424
iteration : 607
train acc:  0.8125
train loss:  0.41670459508895874
train gradient:  0.29351593070388476
iteration : 608
train acc:  0.7734375
train loss:  0.5315602421760559
train gradient:  1.2435319408342391
iteration : 609
train acc:  0.7890625
train loss:  0.471130907535553
train gradient:  0.4677282967793779
iteration : 610
train acc:  0.6796875
train loss:  0.5729159116744995
train gradient:  0.8138604998106684
iteration : 611
train acc:  0.8125
train loss:  0.4300113916397095
train gradient:  0.4183621245883054
iteration : 612
train acc:  0.7421875
train loss:  0.5114052295684814
train gradient:  0.47159560687574986
iteration : 613
train acc:  0.7734375
train loss:  0.5144011378288269
train gradient:  1.3963755987452338
iteration : 614
train acc:  0.8203125
train loss:  0.3930683732032776
train gradient:  0.38475465826372873
iteration : 615
train acc:  0.734375
train loss:  0.49803096055984497
train gradient:  0.3549342339177743
iteration : 616
train acc:  0.8046875
train loss:  0.456970751285553
train gradient:  0.4688406584113087
iteration : 617
train acc:  0.78125
train loss:  0.4866596460342407
train gradient:  0.5174119892640434
iteration : 618
train acc:  0.8515625
train loss:  0.4462386667728424
train gradient:  0.6382588584525043
iteration : 619
train acc:  0.78125
train loss:  0.5062693357467651
train gradient:  0.6141870759448342
iteration : 620
train acc:  0.7734375
train loss:  0.4923889935016632
train gradient:  0.5303407706959814
iteration : 621
train acc:  0.8125
train loss:  0.48717015981674194
train gradient:  0.5586149843768535
iteration : 622
train acc:  0.7578125
train loss:  0.4976425766944885
train gradient:  0.7541251111692028
iteration : 623
train acc:  0.765625
train loss:  0.4699065685272217
train gradient:  0.5318374812144819
iteration : 624
train acc:  0.7421875
train loss:  0.44448426365852356
train gradient:  0.574438367732904
iteration : 625
train acc:  0.8046875
train loss:  0.4634072184562683
train gradient:  0.5735811700666054
iteration : 626
train acc:  0.7734375
train loss:  0.4779771566390991
train gradient:  0.6006630919309665
iteration : 627
train acc:  0.7890625
train loss:  0.44109833240509033
train gradient:  0.5961742092869216
iteration : 628
train acc:  0.6953125
train loss:  0.5876435041427612
train gradient:  0.7372736249463694
iteration : 629
train acc:  0.765625
train loss:  0.48089420795440674
train gradient:  0.6117012794293502
iteration : 630
train acc:  0.734375
train loss:  0.5442818403244019
train gradient:  0.7102556450872388
iteration : 631
train acc:  0.75
train loss:  0.5194931030273438
train gradient:  0.5518012357627158
iteration : 632
train acc:  0.7578125
train loss:  0.5150534510612488
train gradient:  0.6505290817570825
iteration : 633
train acc:  0.765625
train loss:  0.45727699995040894
train gradient:  0.6208848007164893
iteration : 634
train acc:  0.7890625
train loss:  0.5032734870910645
train gradient:  0.6449041005245779
iteration : 635
train acc:  0.78125
train loss:  0.46004506945610046
train gradient:  0.5476147393201285
iteration : 636
train acc:  0.75
train loss:  0.560706377029419
train gradient:  1.006293237980933
iteration : 637
train acc:  0.7734375
train loss:  0.5016379952430725
train gradient:  0.6712145242258132
iteration : 638
train acc:  0.671875
train loss:  0.6221165657043457
train gradient:  0.9305349099841251
iteration : 639
train acc:  0.78125
train loss:  0.4673802852630615
train gradient:  0.6263082958806925
iteration : 640
train acc:  0.78125
train loss:  0.5015361309051514
train gradient:  0.4759309113232893
iteration : 641
train acc:  0.7734375
train loss:  0.4740930497646332
train gradient:  0.6357438658043151
iteration : 642
train acc:  0.75
train loss:  0.5147398114204407
train gradient:  0.5946138711460498
iteration : 643
train acc:  0.7421875
train loss:  0.5072439312934875
train gradient:  0.6833969631750884
iteration : 644
train acc:  0.828125
train loss:  0.4359981417655945
train gradient:  0.6075954227392906
iteration : 645
train acc:  0.8125
train loss:  0.40721797943115234
train gradient:  0.5654809220659223
iteration : 646
train acc:  0.7109375
train loss:  0.5457865595817566
train gradient:  0.8678327179874783
iteration : 647
train acc:  0.7578125
train loss:  0.4976843595504761
train gradient:  0.522826947318738
iteration : 648
train acc:  0.7265625
train loss:  0.5271872878074646
train gradient:  0.8652745107072611
iteration : 649
train acc:  0.84375
train loss:  0.40224048495292664
train gradient:  0.5385964072322544
iteration : 650
train acc:  0.78125
train loss:  0.46257859468460083
train gradient:  0.6331129258839849
iteration : 651
train acc:  0.8125
train loss:  0.4465731680393219
train gradient:  0.6423983298046874
iteration : 652
train acc:  0.7734375
train loss:  0.47693532705307007
train gradient:  0.7761867751149817
iteration : 653
train acc:  0.78125
train loss:  0.4679510295391083
train gradient:  0.5117821377714462
iteration : 654
train acc:  0.703125
train loss:  0.546929121017456
train gradient:  0.7159320529724381
iteration : 655
train acc:  0.7734375
train loss:  0.5353491902351379
train gradient:  0.5873636327479816
iteration : 656
train acc:  0.7109375
train loss:  0.5814909934997559
train gradient:  0.7145941208647527
iteration : 657
train acc:  0.8046875
train loss:  0.44998908042907715
train gradient:  0.4817184629398962
iteration : 658
train acc:  0.7734375
train loss:  0.43711841106414795
train gradient:  0.4761551853854951
iteration : 659
train acc:  0.71875
train loss:  0.5353702306747437
train gradient:  0.9390372841367228
iteration : 660
train acc:  0.796875
train loss:  0.44501373171806335
train gradient:  0.46826288292866575
iteration : 661
train acc:  0.765625
train loss:  0.5290343761444092
train gradient:  0.6143902224418107
iteration : 662
train acc:  0.796875
train loss:  0.445899099111557
train gradient:  0.4950595712334598
iteration : 663
train acc:  0.8359375
train loss:  0.4157538414001465
train gradient:  0.3360446783159555
iteration : 664
train acc:  0.7734375
train loss:  0.5039912462234497
train gradient:  0.6058887356973289
iteration : 665
train acc:  0.7734375
train loss:  0.5319139957427979
train gradient:  0.8396292906514715
iteration : 666
train acc:  0.7890625
train loss:  0.4860609769821167
train gradient:  0.5490095183015087
iteration : 667
train acc:  0.7578125
train loss:  0.514943540096283
train gradient:  0.6474140669463886
iteration : 668
train acc:  0.7578125
train loss:  0.4859849214553833
train gradient:  0.598116834020861
iteration : 669
train acc:  0.8046875
train loss:  0.4082655906677246
train gradient:  0.5023863429524684
iteration : 670
train acc:  0.8046875
train loss:  0.4568367004394531
train gradient:  0.4616934584661481
iteration : 671
train acc:  0.7734375
train loss:  0.49286341667175293
train gradient:  0.7102776707037658
iteration : 672
train acc:  0.75
train loss:  0.5239632725715637
train gradient:  0.8192283921900365
iteration : 673
train acc:  0.765625
train loss:  0.4445289969444275
train gradient:  0.4838749994071156
iteration : 674
train acc:  0.734375
train loss:  0.5293072462081909
train gradient:  0.8605352757593276
iteration : 675
train acc:  0.7578125
train loss:  0.49516886472702026
train gradient:  0.6729240027572133
iteration : 676
train acc:  0.7265625
train loss:  0.5378183126449585
train gradient:  0.67802113562362
iteration : 677
train acc:  0.78125
train loss:  0.4495095908641815
train gradient:  0.4416360940272601
iteration : 678
train acc:  0.71875
train loss:  0.507741391658783
train gradient:  0.7680149603479269
iteration : 679
train acc:  0.7734375
train loss:  0.48019301891326904
train gradient:  0.5692966164074265
iteration : 680
train acc:  0.71875
train loss:  0.5454114079475403
train gradient:  0.820796111673426
iteration : 681
train acc:  0.7890625
train loss:  0.5002959370613098
train gradient:  0.8613698932806445
iteration : 682
train acc:  0.75
train loss:  0.5113980174064636
train gradient:  0.7032198308778737
iteration : 683
train acc:  0.71875
train loss:  0.534184992313385
train gradient:  0.8691518401205443
iteration : 684
train acc:  0.7265625
train loss:  0.5460440516471863
train gradient:  0.7249808369751867
iteration : 685
train acc:  0.796875
train loss:  0.42167043685913086
train gradient:  0.36885372335860994
iteration : 686
train acc:  0.75
train loss:  0.4935376048088074
train gradient:  0.7643689121608921
iteration : 687
train acc:  0.765625
train loss:  0.45138269662857056
train gradient:  0.47063811895409896
iteration : 688
train acc:  0.78125
train loss:  0.46411460638046265
train gradient:  0.5160254460735669
iteration : 689
train acc:  0.765625
train loss:  0.5270817279815674
train gradient:  0.5179039000463057
iteration : 690
train acc:  0.765625
train loss:  0.4806601107120514
train gradient:  0.45876351794543346
iteration : 691
train acc:  0.7421875
train loss:  0.5270482301712036
train gradient:  0.7395110627554315
iteration : 692
train acc:  0.7421875
train loss:  0.5174617767333984
train gradient:  0.6955686915320095
iteration : 693
train acc:  0.75
train loss:  0.46698734164237976
train gradient:  0.5285058095065249
iteration : 694
train acc:  0.84375
train loss:  0.4165869355201721
train gradient:  0.5395202827749896
iteration : 695
train acc:  0.71875
train loss:  0.5471839308738708
train gradient:  0.6878996290849719
iteration : 696
train acc:  0.78125
train loss:  0.4453554153442383
train gradient:  0.43561857395500814
iteration : 697
train acc:  0.765625
train loss:  0.49287718534469604
train gradient:  0.49584018771092037
iteration : 698
train acc:  0.7734375
train loss:  0.5424389243125916
train gradient:  0.7490802244722357
iteration : 699
train acc:  0.7890625
train loss:  0.5158671140670776
train gradient:  0.6001010462754319
iteration : 700
train acc:  0.6796875
train loss:  0.5740922093391418
train gradient:  0.8098131791788713
iteration : 701
train acc:  0.796875
train loss:  0.45402413606643677
train gradient:  0.6028711279772089
iteration : 702
train acc:  0.75
train loss:  0.47156864404678345
train gradient:  0.5115664518862262
iteration : 703
train acc:  0.671875
train loss:  0.5888586044311523
train gradient:  0.7327534683758365
iteration : 704
train acc:  0.765625
train loss:  0.5041334629058838
train gradient:  0.6453754042779507
iteration : 705
train acc:  0.6796875
train loss:  0.5821477174758911
train gradient:  0.693567494652412
iteration : 706
train acc:  0.765625
train loss:  0.48966550827026367
train gradient:  0.5975577105940373
iteration : 707
train acc:  0.7890625
train loss:  0.4635378122329712
train gradient:  0.458030962018021
iteration : 708
train acc:  0.7578125
train loss:  0.4638471305370331
train gradient:  0.6497872996968527
iteration : 709
train acc:  0.7265625
train loss:  0.4816693067550659
train gradient:  0.6303413606460073
iteration : 710
train acc:  0.796875
train loss:  0.47268474102020264
train gradient:  0.6597887350288212
iteration : 711
train acc:  0.71875
train loss:  0.489518404006958
train gradient:  0.6229060939100759
iteration : 712
train acc:  0.7890625
train loss:  0.5286824703216553
train gradient:  0.8085729468928381
iteration : 713
train acc:  0.7109375
train loss:  0.529894232749939
train gradient:  0.6472586111691633
iteration : 714
train acc:  0.796875
train loss:  0.434895396232605
train gradient:  0.5907305488462944
iteration : 715
train acc:  0.7734375
train loss:  0.47664061188697815
train gradient:  0.6873424124276832
iteration : 716
train acc:  0.765625
train loss:  0.5068269371986389
train gradient:  0.5464092293918291
iteration : 717
train acc:  0.8203125
train loss:  0.39354586601257324
train gradient:  0.3146738625832084
iteration : 718
train acc:  0.8125
train loss:  0.41698282957077026
train gradient:  0.37409313690188806
iteration : 719
train acc:  0.7421875
train loss:  0.5327093005180359
train gradient:  0.6359794195377807
iteration : 720
train acc:  0.703125
train loss:  0.5668121576309204
train gradient:  0.8613148220200979
iteration : 721
train acc:  0.71875
train loss:  0.502179741859436
train gradient:  0.801082267784716
iteration : 722
train acc:  0.6953125
train loss:  0.5777504444122314
train gradient:  0.5416529503001224
iteration : 723
train acc:  0.75
train loss:  0.47254377603530884
train gradient:  0.5374776344896228
iteration : 724
train acc:  0.7265625
train loss:  0.5354121923446655
train gradient:  0.5289570755098028
iteration : 725
train acc:  0.7734375
train loss:  0.46754220128059387
train gradient:  0.34074729862261205
iteration : 726
train acc:  0.8515625
train loss:  0.39185234904289246
train gradient:  0.3508593589929596
iteration : 727
train acc:  0.6640625
train loss:  0.5340731143951416
train gradient:  0.5919438626898595
iteration : 728
train acc:  0.75
train loss:  0.5343127250671387
train gradient:  0.710976168410747
iteration : 729
train acc:  0.7421875
train loss:  0.5208702683448792
train gradient:  0.6593702318893481
iteration : 730
train acc:  0.7421875
train loss:  0.5232100486755371
train gradient:  0.5693210175228894
iteration : 731
train acc:  0.7890625
train loss:  0.4832858741283417
train gradient:  0.5251015498346108
iteration : 732
train acc:  0.7734375
train loss:  0.4902347922325134
train gradient:  0.6359293609873147
iteration : 733
train acc:  0.78125
train loss:  0.4690585732460022
train gradient:  0.49072501982715
iteration : 734
train acc:  0.8359375
train loss:  0.380931556224823
train gradient:  0.4228112275426973
iteration : 735
train acc:  0.78125
train loss:  0.45857250690460205
train gradient:  0.5339147034812384
iteration : 736
train acc:  0.7734375
train loss:  0.4837486743927002
train gradient:  0.5974164048812911
iteration : 737
train acc:  0.7578125
train loss:  0.5020287036895752
train gradient:  0.4000073437605027
iteration : 738
train acc:  0.796875
train loss:  0.4743664562702179
train gradient:  0.4400466009381289
iteration : 739
train acc:  0.71875
train loss:  0.5463088154792786
train gradient:  0.7798090491855731
iteration : 740
train acc:  0.8203125
train loss:  0.42058345675468445
train gradient:  0.5554553140585343
iteration : 741
train acc:  0.7734375
train loss:  0.5280177593231201
train gradient:  0.5335838556413063
iteration : 742
train acc:  0.78125
train loss:  0.43938326835632324
train gradient:  0.48843232962427463
iteration : 743
train acc:  0.7265625
train loss:  0.4936288893222809
train gradient:  0.5755652678571096
iteration : 744
train acc:  0.796875
train loss:  0.49458593130111694
train gradient:  0.6935456190139107
iteration : 745
train acc:  0.7890625
train loss:  0.5115785598754883
train gradient:  0.6286692262191083
iteration : 746
train acc:  0.78125
train loss:  0.4820502996444702
train gradient:  0.47916067922008604
iteration : 747
train acc:  0.75
train loss:  0.474956214427948
train gradient:  0.4958584037106598
iteration : 748
train acc:  0.734375
train loss:  0.48474636673927307
train gradient:  0.6354065988009089
iteration : 749
train acc:  0.8046875
train loss:  0.49421215057373047
train gradient:  0.6098475149040652
iteration : 750
train acc:  0.8125
train loss:  0.4386823773384094
train gradient:  0.45726275741833483
iteration : 751
train acc:  0.8203125
train loss:  0.4431785047054291
train gradient:  0.5388945856156784
iteration : 752
train acc:  0.7421875
train loss:  0.5111055970191956
train gradient:  0.5658610586371274
iteration : 753
train acc:  0.7265625
train loss:  0.5152181386947632
train gradient:  0.4375714664136216
iteration : 754
train acc:  0.7421875
train loss:  0.4835486114025116
train gradient:  0.5092290840209801
iteration : 755
train acc:  0.7109375
train loss:  0.5160268545150757
train gradient:  0.6543520489974733
iteration : 756
train acc:  0.7421875
train loss:  0.5413119792938232
train gradient:  0.5468662178868269
iteration : 757
train acc:  0.7734375
train loss:  0.48344355821609497
train gradient:  0.39986908036894686
iteration : 758
train acc:  0.8203125
train loss:  0.4034537076950073
train gradient:  0.48287120534504735
iteration : 759
train acc:  0.734375
train loss:  0.5076721906661987
train gradient:  0.6544906959336929
iteration : 760
train acc:  0.765625
train loss:  0.4931490421295166
train gradient:  0.4693746531894069
iteration : 761
train acc:  0.8125
train loss:  0.4022284746170044
train gradient:  0.3748554763801933
iteration : 762
train acc:  0.7890625
train loss:  0.4954366087913513
train gradient:  0.6300319208492693
iteration : 763
train acc:  0.8203125
train loss:  0.42971935868263245
train gradient:  1.4466310926273007
iteration : 764
train acc:  0.765625
train loss:  0.5072110295295715
train gradient:  0.46673458692765557
iteration : 765
train acc:  0.7734375
train loss:  0.4477829933166504
train gradient:  0.40648019313477785
iteration : 766
train acc:  0.8125
train loss:  0.40401798486709595
train gradient:  0.40563092399545725
iteration : 767
train acc:  0.78125
train loss:  0.47801998257637024
train gradient:  0.5574798323930634
iteration : 768
train acc:  0.75
train loss:  0.5363171100616455
train gradient:  0.6376218953700448
iteration : 769
train acc:  0.734375
train loss:  0.5245144367218018
train gradient:  0.8351815064497042
iteration : 770
train acc:  0.8359375
train loss:  0.4743959307670593
train gradient:  0.35059327035417387
iteration : 771
train acc:  0.7734375
train loss:  0.48353201150894165
train gradient:  0.43865102158402425
iteration : 772
train acc:  0.734375
train loss:  0.49927523732185364
train gradient:  0.5440355149332129
iteration : 773
train acc:  0.8359375
train loss:  0.4322405755519867
train gradient:  0.48685159249828275
iteration : 774
train acc:  0.765625
train loss:  0.47781842947006226
train gradient:  0.5076761284069682
iteration : 775
train acc:  0.765625
train loss:  0.47106799483299255
train gradient:  0.6940173749691071
iteration : 776
train acc:  0.7734375
train loss:  0.46593326330184937
train gradient:  0.4783894368402342
iteration : 777
train acc:  0.8125
train loss:  0.4275827407836914
train gradient:  0.5542522924957601
iteration : 778
train acc:  0.734375
train loss:  0.45126235485076904
train gradient:  0.4859998764539331
iteration : 779
train acc:  0.7890625
train loss:  0.43570053577423096
train gradient:  0.4328265709495831
iteration : 780
train acc:  0.78125
train loss:  0.46796277165412903
train gradient:  0.5456535297401544
iteration : 781
train acc:  0.7265625
train loss:  0.5007773637771606
train gradient:  0.6125699112048244
iteration : 782
train acc:  0.8203125
train loss:  0.48108047246932983
train gradient:  0.6301412896437835
iteration : 783
train acc:  0.7265625
train loss:  0.5861867070198059
train gradient:  0.8998579728185193
iteration : 784
train acc:  0.796875
train loss:  0.46716251969337463
train gradient:  0.48348643431335553
iteration : 785
train acc:  0.7734375
train loss:  0.4725393056869507
train gradient:  0.5955653777940368
iteration : 786
train acc:  0.8046875
train loss:  0.4429396688938141
train gradient:  0.51190242276652
iteration : 787
train acc:  0.796875
train loss:  0.46830442547798157
train gradient:  0.8439377140742891
iteration : 788
train acc:  0.796875
train loss:  0.4973461925983429
train gradient:  0.5608525333159151
iteration : 789
train acc:  0.8046875
train loss:  0.43795523047447205
train gradient:  0.5933120615904861
iteration : 790
train acc:  0.7890625
train loss:  0.43574297428131104
train gradient:  0.37598245573276684
iteration : 791
train acc:  0.765625
train loss:  0.47938287258148193
train gradient:  0.5635986974759475
iteration : 792
train acc:  0.734375
train loss:  0.49722617864608765
train gradient:  0.5163086957504539
iteration : 793
train acc:  0.7421875
train loss:  0.4962528944015503
train gradient:  0.5501235677328545
iteration : 794
train acc:  0.75
train loss:  0.5068129897117615
train gradient:  0.5214498155035372
iteration : 795
train acc:  0.7734375
train loss:  0.48911985754966736
train gradient:  0.6592191276527268
iteration : 796
train acc:  0.734375
train loss:  0.5169124603271484
train gradient:  0.7407105707245845
iteration : 797
train acc:  0.8125
train loss:  0.4655371606349945
train gradient:  0.7418863007105416
iteration : 798
train acc:  0.828125
train loss:  0.41339007019996643
train gradient:  0.5891721903463787
iteration : 799
train acc:  0.78125
train loss:  0.45523545145988464
train gradient:  0.5288987890645631
iteration : 800
train acc:  0.78125
train loss:  0.513245701789856
train gradient:  0.49921779972883623
iteration : 801
train acc:  0.7734375
train loss:  0.4950138330459595
train gradient:  0.5300429054169887
iteration : 802
train acc:  0.78125
train loss:  0.45615848898887634
train gradient:  0.4483520044189826
iteration : 803
train acc:  0.7578125
train loss:  0.49714037775993347
train gradient:  0.48563473874602203
iteration : 804
train acc:  0.7890625
train loss:  0.4571608304977417
train gradient:  0.5535416391128276
iteration : 805
train acc:  0.828125
train loss:  0.49236929416656494
train gradient:  0.6232866941191755
iteration : 806
train acc:  0.6796875
train loss:  0.5830485820770264
train gradient:  0.8721779399417696
iteration : 807
train acc:  0.8125
train loss:  0.4491734206676483
train gradient:  0.7029752328041754
iteration : 808
train acc:  0.734375
train loss:  0.45814377069473267
train gradient:  0.47317130463610596
iteration : 809
train acc:  0.796875
train loss:  0.46181097626686096
train gradient:  0.5564681782567855
iteration : 810
train acc:  0.796875
train loss:  0.4123842120170593
train gradient:  0.5352141118608753
iteration : 811
train acc:  0.765625
train loss:  0.485565721988678
train gradient:  0.4570365288943701
iteration : 812
train acc:  0.796875
train loss:  0.43829846382141113
train gradient:  0.5451831613833691
iteration : 813
train acc:  0.734375
train loss:  0.5163741111755371
train gradient:  0.5254928975476028
iteration : 814
train acc:  0.7890625
train loss:  0.46055570244789124
train gradient:  0.5540287377785771
iteration : 815
train acc:  0.796875
train loss:  0.4516809582710266
train gradient:  0.5413324530248641
iteration : 816
train acc:  0.7734375
train loss:  0.5001169443130493
train gradient:  0.6844279190491956
iteration : 817
train acc:  0.7890625
train loss:  0.4264307916164398
train gradient:  0.3150895072220453
iteration : 818
train acc:  0.828125
train loss:  0.41324353218078613
train gradient:  0.37357979029877847
iteration : 819
train acc:  0.78125
train loss:  0.43823322653770447
train gradient:  0.493364859843505
iteration : 820
train acc:  0.7734375
train loss:  0.4567117691040039
train gradient:  0.5169978311568812
iteration : 821
train acc:  0.7421875
train loss:  0.5281479358673096
train gradient:  0.7275501651925309
iteration : 822
train acc:  0.765625
train loss:  0.5183584094047546
train gradient:  0.7393123884487293
iteration : 823
train acc:  0.8125
train loss:  0.4471287727355957
train gradient:  0.47384875794759745
iteration : 824
train acc:  0.8515625
train loss:  0.34185683727264404
train gradient:  0.3706166613486003
iteration : 825
train acc:  0.75
train loss:  0.497525691986084
train gradient:  0.4889239233384722
iteration : 826
train acc:  0.7421875
train loss:  0.46722620725631714
train gradient:  0.4520520365755424
iteration : 827
train acc:  0.7734375
train loss:  0.47875672578811646
train gradient:  0.5620610123265664
iteration : 828
train acc:  0.734375
train loss:  0.5551325678825378
train gradient:  0.7769288947998253
iteration : 829
train acc:  0.71875
train loss:  0.5322290658950806
train gradient:  0.41906404570987144
iteration : 830
train acc:  0.828125
train loss:  0.41159090399742126
train gradient:  0.37588444398597626
iteration : 831
train acc:  0.75
train loss:  0.4804942309856415
train gradient:  0.7758841838145667
iteration : 832
train acc:  0.828125
train loss:  0.41651400923728943
train gradient:  0.3698295945987328
iteration : 833
train acc:  0.703125
train loss:  0.5270629525184631
train gradient:  0.5728028417414015
iteration : 834
train acc:  0.7734375
train loss:  0.4992942810058594
train gradient:  0.6551927219320668
iteration : 835
train acc:  0.8125
train loss:  0.48008060455322266
train gradient:  0.8920661492403199
iteration : 836
train acc:  0.7578125
train loss:  0.461223840713501
train gradient:  0.5395351728765325
iteration : 837
train acc:  0.828125
train loss:  0.4046972692012787
train gradient:  0.38110893042150074
iteration : 838
train acc:  0.7578125
train loss:  0.4734031856060028
train gradient:  0.5908601670856407
iteration : 839
train acc:  0.75
train loss:  0.49260202050209045
train gradient:  0.5657981400588159
iteration : 840
train acc:  0.6953125
train loss:  0.5301229953765869
train gradient:  0.5170475357444946
iteration : 841
train acc:  0.7421875
train loss:  0.5257986783981323
train gradient:  0.6472617582463984
iteration : 842
train acc:  0.796875
train loss:  0.4235173463821411
train gradient:  0.7911850627225567
iteration : 843
train acc:  0.7734375
train loss:  0.5137103796005249
train gradient:  0.704996128780325
iteration : 844
train acc:  0.7734375
train loss:  0.5354506969451904
train gradient:  0.5760950898725168
iteration : 845
train acc:  0.7734375
train loss:  0.492473840713501
train gradient:  0.5711393950941147
iteration : 846
train acc:  0.7578125
train loss:  0.4669877290725708
train gradient:  0.6325411522001477
iteration : 847
train acc:  0.765625
train loss:  0.500030517578125
train gradient:  0.5969457939302202
iteration : 848
train acc:  0.734375
train loss:  0.5668402910232544
train gradient:  0.9075292274129911
iteration : 849
train acc:  0.8125
train loss:  0.4236755967140198
train gradient:  0.5027278318186688
iteration : 850
train acc:  0.8203125
train loss:  0.45012927055358887
train gradient:  0.45254450457976964
iteration : 851
train acc:  0.8203125
train loss:  0.39733874797821045
train gradient:  0.6722022646439774
iteration : 852
train acc:  0.7265625
train loss:  0.5276305675506592
train gradient:  0.7674219501626808
iteration : 853
train acc:  0.8203125
train loss:  0.39473360776901245
train gradient:  0.3800096346076307
iteration : 854
train acc:  0.8359375
train loss:  0.4138164818286896
train gradient:  0.4439991375096159
iteration : 855
train acc:  0.71875
train loss:  0.5218598246574402
train gradient:  0.6704091029765372
iteration : 856
train acc:  0.7734375
train loss:  0.5316357612609863
train gradient:  0.5123144117120294
iteration : 857
train acc:  0.796875
train loss:  0.4241121709346771
train gradient:  0.4223229603944249
iteration : 858
train acc:  0.765625
train loss:  0.4971262216567993
train gradient:  0.5217578631757839
iteration : 859
train acc:  0.6953125
train loss:  0.6073247194290161
train gradient:  0.798810712533345
iteration : 860
train acc:  0.78125
train loss:  0.45062685012817383
train gradient:  0.6347274396074997
iteration : 861
train acc:  0.75
train loss:  0.504694938659668
train gradient:  0.4822349158522837
iteration : 862
train acc:  0.796875
train loss:  0.412034809589386
train gradient:  0.3811675315194312
iteration : 863
train acc:  0.78125
train loss:  0.4369502067565918
train gradient:  0.49105356485981194
iteration : 864
train acc:  0.8203125
train loss:  0.40089696645736694
train gradient:  0.5392520837561126
iteration : 865
train acc:  0.765625
train loss:  0.5179905891418457
train gradient:  0.7825828426598873
iteration : 866
train acc:  0.8125
train loss:  0.4449624717235565
train gradient:  0.5147586432394907
iteration : 867
train acc:  0.734375
train loss:  0.553321361541748
train gradient:  0.737147394583818
iteration : 868
train acc:  0.828125
train loss:  0.39102715253829956
train gradient:  0.5088208638762133
iteration : 869
train acc:  0.75
train loss:  0.5683629512786865
train gradient:  0.8214812590187113
iteration : 870
train acc:  0.7734375
train loss:  0.46434977650642395
train gradient:  0.4576828992605341
iteration : 871
train acc:  0.75
train loss:  0.5080406665802002
train gradient:  0.46963044851211394
iteration : 872
train acc:  0.765625
train loss:  0.5061930418014526
train gradient:  0.7345918139959553
iteration : 873
train acc:  0.796875
train loss:  0.4500557780265808
train gradient:  0.4115444916245756
iteration : 874
train acc:  0.8046875
train loss:  0.46711403131484985
train gradient:  0.4302214797946954
iteration : 875
train acc:  0.7734375
train loss:  0.43579915165901184
train gradient:  0.40624164668388957
iteration : 876
train acc:  0.78125
train loss:  0.4606422781944275
train gradient:  0.538262410618628
iteration : 877
train acc:  0.7734375
train loss:  0.45023113489151
train gradient:  0.4016217893897697
iteration : 878
train acc:  0.8125
train loss:  0.45009487867355347
train gradient:  0.2680109459155728
iteration : 879
train acc:  0.703125
train loss:  0.5630770325660706
train gradient:  0.9862816813139432
iteration : 880
train acc:  0.7421875
train loss:  0.4700532555580139
train gradient:  0.6053218339687916
iteration : 881
train acc:  0.7578125
train loss:  0.4781196117401123
train gradient:  0.49531573811187496
iteration : 882
train acc:  0.75
train loss:  0.5271852612495422
train gradient:  0.6207755649505761
iteration : 883
train acc:  0.7578125
train loss:  0.49490636587142944
train gradient:  0.640633001198907
iteration : 884
train acc:  0.7890625
train loss:  0.4534546136856079
train gradient:  0.4662161775988821
iteration : 885
train acc:  0.828125
train loss:  0.44289451837539673
train gradient:  0.3764764604692189
iteration : 886
train acc:  0.734375
train loss:  0.5131688117980957
train gradient:  0.5219586924623165
iteration : 887
train acc:  0.875
train loss:  0.3621405065059662
train gradient:  0.25022243994458315
iteration : 888
train acc:  0.78125
train loss:  0.503797173500061
train gradient:  0.520475942628517
iteration : 889
train acc:  0.796875
train loss:  0.485038161277771
train gradient:  0.5929497263318991
iteration : 890
train acc:  0.78125
train loss:  0.4995911419391632
train gradient:  0.4677423605751075
iteration : 891
train acc:  0.8359375
train loss:  0.4293857514858246
train gradient:  0.43901882999472874
iteration : 892
train acc:  0.7890625
train loss:  0.4602939486503601
train gradient:  0.42199364418287827
iteration : 893
train acc:  0.8359375
train loss:  0.4170394241809845
train gradient:  0.4269385228298168
iteration : 894
train acc:  0.8203125
train loss:  0.41920173168182373
train gradient:  0.44401118347307056
iteration : 895
train acc:  0.859375
train loss:  0.34861984848976135
train gradient:  0.37237096436350126
iteration : 896
train acc:  0.8203125
train loss:  0.4081546366214752
train gradient:  0.49931831396932286
iteration : 897
train acc:  0.796875
train loss:  0.44204264879226685
train gradient:  0.47897621289787096
iteration : 898
train acc:  0.7890625
train loss:  0.4621269106864929
train gradient:  0.4460362753097075
iteration : 899
train acc:  0.8046875
train loss:  0.4811495542526245
train gradient:  0.5176509455477338
iteration : 900
train acc:  0.78125
train loss:  0.4938463568687439
train gradient:  0.7255267812117455
iteration : 901
train acc:  0.7890625
train loss:  0.4460877776145935
train gradient:  0.5850265906122802
iteration : 902
train acc:  0.765625
train loss:  0.4440828263759613
train gradient:  0.4419731176567286
iteration : 903
train acc:  0.75
train loss:  0.5123215317726135
train gradient:  0.5903258810798852
iteration : 904
train acc:  0.828125
train loss:  0.4294302463531494
train gradient:  0.5994356251069228
iteration : 905
train acc:  0.796875
train loss:  0.45680728554725647
train gradient:  0.5405310271877792
iteration : 906
train acc:  0.7109375
train loss:  0.5511316061019897
train gradient:  0.82527369563323
iteration : 907
train acc:  0.78125
train loss:  0.45752865076065063
train gradient:  0.5730269655350435
iteration : 908
train acc:  0.765625
train loss:  0.44526010751724243
train gradient:  0.5166973145175757
iteration : 909
train acc:  0.8125
train loss:  0.5166811943054199
train gradient:  0.4443954038647691
iteration : 910
train acc:  0.859375
train loss:  0.3925154209136963
train gradient:  0.46684804362787424
iteration : 911
train acc:  0.78125
train loss:  0.4391287565231323
train gradient:  0.37615374510941624
iteration : 912
train acc:  0.8515625
train loss:  0.3672236204147339
train gradient:  0.5521209339610449
iteration : 913
train acc:  0.6875
train loss:  0.5168266892433167
train gradient:  0.7602911342618214
iteration : 914
train acc:  0.78125
train loss:  0.4804496169090271
train gradient:  0.7724187196426879
iteration : 915
train acc:  0.75
train loss:  0.5138763189315796
train gradient:  0.656455385476687
iteration : 916
train acc:  0.71875
train loss:  0.6108066439628601
train gradient:  1.002060772343511
iteration : 917
train acc:  0.7734375
train loss:  0.4466785788536072
train gradient:  0.5577870274620864
iteration : 918
train acc:  0.8203125
train loss:  0.38555365800857544
train gradient:  0.3619750546972137
iteration : 919
train acc:  0.78125
train loss:  0.47971272468566895
train gradient:  0.5608997087673248
iteration : 920
train acc:  0.7734375
train loss:  0.49566978216171265
train gradient:  0.9314084454369749
iteration : 921
train acc:  0.8046875
train loss:  0.4500342607498169
train gradient:  0.3844979147734218
iteration : 922
train acc:  0.8125
train loss:  0.42199617624282837
train gradient:  0.49814289329144545
iteration : 923
train acc:  0.7734375
train loss:  0.3997332453727722
train gradient:  0.434733300502905
iteration : 924
train acc:  0.78125
train loss:  0.5085960626602173
train gradient:  0.7810411591125921
iteration : 925
train acc:  0.78125
train loss:  0.4587603211402893
train gradient:  0.4291245186892351
iteration : 926
train acc:  0.78125
train loss:  0.4955800175666809
train gradient:  0.9989006174346149
iteration : 927
train acc:  0.8515625
train loss:  0.38080286979675293
train gradient:  0.3114814774636259
iteration : 928
train acc:  0.7890625
train loss:  0.4588887691497803
train gradient:  0.5568182595135219
iteration : 929
train acc:  0.75
train loss:  0.5104360580444336
train gradient:  0.5924134471810767
iteration : 930
train acc:  0.8203125
train loss:  0.4267149865627289
train gradient:  0.4055126995879469
iteration : 931
train acc:  0.734375
train loss:  0.5094130635261536
train gradient:  0.6547556624064661
iteration : 932
train acc:  0.7578125
train loss:  0.520535945892334
train gradient:  0.6326759222094809
iteration : 933
train acc:  0.796875
train loss:  0.46488791704177856
train gradient:  0.6402819049347428
iteration : 934
train acc:  0.765625
train loss:  0.478828102350235
train gradient:  0.46146002441552897
iteration : 935
train acc:  0.828125
train loss:  0.4155764877796173
train gradient:  0.42629860270274433
iteration : 936
train acc:  0.78125
train loss:  0.47312289476394653
train gradient:  0.5265701993426118
iteration : 937
train acc:  0.75
train loss:  0.5024471282958984
train gradient:  0.5551708590481358
iteration : 938
train acc:  0.7734375
train loss:  0.44066715240478516
train gradient:  0.48511117928019876
iteration : 939
train acc:  0.7890625
train loss:  0.4632115960121155
train gradient:  0.33332428760036154
iteration : 940
train acc:  0.7890625
train loss:  0.44209566712379456
train gradient:  0.44690490225145063
iteration : 941
train acc:  0.7890625
train loss:  0.4890783131122589
train gradient:  0.5436165830052858
iteration : 942
train acc:  0.828125
train loss:  0.403685063123703
train gradient:  0.43174663650346456
iteration : 943
train acc:  0.7734375
train loss:  0.43666011095046997
train gradient:  0.5799421154312027
iteration : 944
train acc:  0.84375
train loss:  0.3780933618545532
train gradient:  0.4236766303064561
iteration : 945
train acc:  0.7890625
train loss:  0.4422306418418884
train gradient:  0.4138137532116903
iteration : 946
train acc:  0.8515625
train loss:  0.40030232071876526
train gradient:  0.44667363347873495
iteration : 947
train acc:  0.78125
train loss:  0.4875680208206177
train gradient:  0.5636723423394218
iteration : 948
train acc:  0.7578125
train loss:  0.4468437731266022
train gradient:  0.4541094513712113
iteration : 949
train acc:  0.7734375
train loss:  0.48384687304496765
train gradient:  0.5391765641288574
iteration : 950
train acc:  0.84375
train loss:  0.3953055739402771
train gradient:  0.4962821528230346
iteration : 951
train acc:  0.8125
train loss:  0.38064855337142944
train gradient:  0.446065606751094
iteration : 952
train acc:  0.8046875
train loss:  0.3989051580429077
train gradient:  0.42682398608905364
iteration : 953
train acc:  0.8046875
train loss:  0.38086792826652527
train gradient:  0.4020300647685668
iteration : 954
train acc:  0.7890625
train loss:  0.48943448066711426
train gradient:  0.7281283598616172
iteration : 955
train acc:  0.7734375
train loss:  0.526255190372467
train gradient:  0.7312388926913552
iteration : 956
train acc:  0.765625
train loss:  0.45907312631607056
train gradient:  0.47419022602061517
iteration : 957
train acc:  0.8046875
train loss:  0.41702747344970703
train gradient:  0.5208515133696929
iteration : 958
train acc:  0.8125
train loss:  0.4108934700489044
train gradient:  0.4164374411683512
iteration : 959
train acc:  0.796875
train loss:  0.44709312915802
train gradient:  0.6551228349292841
iteration : 960
train acc:  0.8125
train loss:  0.3859137296676636
train gradient:  0.460168043304297
iteration : 961
train acc:  0.78125
train loss:  0.4166712760925293
train gradient:  0.5072897223942776
iteration : 962
train acc:  0.796875
train loss:  0.4372186064720154
train gradient:  0.5167857704517955
iteration : 963
train acc:  0.78125
train loss:  0.5083954334259033
train gradient:  0.8163494011907232
iteration : 964
train acc:  0.84375
train loss:  0.36789852380752563
train gradient:  0.5367927699860536
iteration : 965
train acc:  0.765625
train loss:  0.47973352670669556
train gradient:  0.8474356861698513
iteration : 966
train acc:  0.7734375
train loss:  0.4611426591873169
train gradient:  0.6505057107594179
iteration : 967
train acc:  0.78125
train loss:  0.47746890783309937
train gradient:  0.567330270090546
iteration : 968
train acc:  0.8125
train loss:  0.45142805576324463
train gradient:  0.4639157880107753
iteration : 969
train acc:  0.84375
train loss:  0.38984471559524536
train gradient:  0.46168440650558096
iteration : 970
train acc:  0.7890625
train loss:  0.4643143117427826
train gradient:  0.49536552212558566
iteration : 971
train acc:  0.828125
train loss:  0.39716053009033203
train gradient:  0.6032912928107117
iteration : 972
train acc:  0.8359375
train loss:  0.38709351420402527
train gradient:  0.42989075821723344
iteration : 973
train acc:  0.78125
train loss:  0.4352290630340576
train gradient:  0.6533567187495488
iteration : 974
train acc:  0.7890625
train loss:  0.43268734216690063
train gradient:  0.6628891345174879
iteration : 975
train acc:  0.703125
train loss:  0.5471951961517334
train gradient:  0.7957438068794954
iteration : 976
train acc:  0.8359375
train loss:  0.46638941764831543
train gradient:  0.6013103632706287
iteration : 977
train acc:  0.7890625
train loss:  0.4223954975605011
train gradient:  0.599082210164451
iteration : 978
train acc:  0.84375
train loss:  0.41036272048950195
train gradient:  0.5163227345616289
iteration : 979
train acc:  0.7578125
train loss:  0.48511701822280884
train gradient:  0.6465906988761583
iteration : 980
train acc:  0.796875
train loss:  0.4458251893520355
train gradient:  0.3906455308746541
iteration : 981
train acc:  0.75
train loss:  0.5085527300834656
train gradient:  0.5562344243787953
iteration : 982
train acc:  0.8046875
train loss:  0.4071158170700073
train gradient:  0.4296439034022309
iteration : 983
train acc:  0.7421875
train loss:  0.4964471757411957
train gradient:  0.6422178513560752
iteration : 984
train acc:  0.796875
train loss:  0.5025508403778076
train gradient:  0.7672497493491017
iteration : 985
train acc:  0.71875
train loss:  0.505924642086029
train gradient:  0.6352269723814923
iteration : 986
train acc:  0.7265625
train loss:  0.526143491268158
train gradient:  0.715358721226839
iteration : 987
train acc:  0.7578125
train loss:  0.5048418641090393
train gradient:  0.5475129565110752
iteration : 988
train acc:  0.796875
train loss:  0.42966780066490173
train gradient:  0.5224695690157924
iteration : 989
train acc:  0.78125
train loss:  0.4862492084503174
train gradient:  0.5349037540314849
iteration : 990
train acc:  0.78125
train loss:  0.4772725999355316
train gradient:  0.4855368080579891
iteration : 991
train acc:  0.8125
train loss:  0.4565214514732361
train gradient:  0.536332860330486
iteration : 992
train acc:  0.84375
train loss:  0.4297916293144226
train gradient:  0.5924317151044854
iteration : 993
train acc:  0.7734375
train loss:  0.48887836933135986
train gradient:  0.5446686988419863
iteration : 994
train acc:  0.7578125
train loss:  0.47307494282722473
train gradient:  0.43832690018757897
iteration : 995
train acc:  0.7578125
train loss:  0.4964359700679779
train gradient:  0.5098282891742231
iteration : 996
train acc:  0.7421875
train loss:  0.5209640264511108
train gradient:  0.5473944526348511
iteration : 997
train acc:  0.75
train loss:  0.47304457426071167
train gradient:  0.6146843197533567
iteration : 998
train acc:  0.8046875
train loss:  0.4275928735733032
train gradient:  0.4570294042422683
iteration : 999
train acc:  0.7734375
train loss:  0.46277058124542236
train gradient:  0.4992011420945064
iteration : 1000
train acc:  0.8203125
train loss:  0.40923231840133667
train gradient:  0.46606687367926425
iteration : 1001
train acc:  0.734375
train loss:  0.5583665370941162
train gradient:  0.6775674686541355
iteration : 1002
train acc:  0.8125
train loss:  0.42246532440185547
train gradient:  0.4345742961740351
iteration : 1003
train acc:  0.8125
train loss:  0.4216499626636505
train gradient:  0.4195299397740304
iteration : 1004
train acc:  0.796875
train loss:  0.442438542842865
train gradient:  0.43514793336658636
iteration : 1005
train acc:  0.859375
train loss:  0.35806018114089966
train gradient:  0.3482760632944793
iteration : 1006
train acc:  0.828125
train loss:  0.383224219083786
train gradient:  0.3502018029019478
iteration : 1007
train acc:  0.8125
train loss:  0.4756844937801361
train gradient:  0.42693128255937685
iteration : 1008
train acc:  0.8125
train loss:  0.3992859423160553
train gradient:  0.5752054243970776
iteration : 1009
train acc:  0.7734375
train loss:  0.44965678453445435
train gradient:  0.4110840302385576
iteration : 1010
train acc:  0.8359375
train loss:  0.41508758068084717
train gradient:  0.3806359208813975
iteration : 1011
train acc:  0.7421875
train loss:  0.511483907699585
train gradient:  0.8964448172520463
iteration : 1012
train acc:  0.8203125
train loss:  0.40136563777923584
train gradient:  0.2998749529375439
iteration : 1013
train acc:  0.7265625
train loss:  0.5293674468994141
train gradient:  0.7014849477937689
iteration : 1014
train acc:  0.8046875
train loss:  0.4352934956550598
train gradient:  0.4316618883038755
iteration : 1015
train acc:  0.8125
train loss:  0.4176905155181885
train gradient:  0.4531216514220163
iteration : 1016
train acc:  0.796875
train loss:  0.4451541304588318
train gradient:  0.49940321532032883
iteration : 1017
train acc:  0.7421875
train loss:  0.5153216123580933
train gradient:  0.48810241748033506
iteration : 1018
train acc:  0.734375
train loss:  0.47876086831092834
train gradient:  0.4673822794554335
iteration : 1019
train acc:  0.765625
train loss:  0.456059992313385
train gradient:  0.3669400321023107
iteration : 1020
train acc:  0.78125
train loss:  0.4844791293144226
train gradient:  0.41431429366483263
iteration : 1021
train acc:  0.78125
train loss:  0.4800119996070862
train gradient:  0.6097134794038307
iteration : 1022
train acc:  0.796875
train loss:  0.43004846572875977
train gradient:  0.3560121592172278
iteration : 1023
train acc:  0.71875
train loss:  0.5284797549247742
train gradient:  0.5617096391636998
iteration : 1024
train acc:  0.671875
train loss:  0.5968806743621826
train gradient:  0.7542446949569939
iteration : 1025
train acc:  0.7421875
train loss:  0.5739336609840393
train gradient:  0.9107724034478621
iteration : 1026
train acc:  0.78125
train loss:  0.44953763484954834
train gradient:  0.48954860338986134
iteration : 1027
train acc:  0.734375
train loss:  0.4882967174053192
train gradient:  0.45205842123930506
iteration : 1028
train acc:  0.765625
train loss:  0.4945777654647827
train gradient:  0.5926223105929702
iteration : 1029
train acc:  0.859375
train loss:  0.3645338714122772
train gradient:  0.5273199597078688
iteration : 1030
train acc:  0.734375
train loss:  0.5241972804069519
train gradient:  0.8638259342465724
iteration : 1031
train acc:  0.7578125
train loss:  0.4771646559238434
train gradient:  0.5884255415748871
iteration : 1032
train acc:  0.828125
train loss:  0.41694918274879456
train gradient:  0.4101902024932682
iteration : 1033
train acc:  0.8359375
train loss:  0.3625403642654419
train gradient:  0.3081779454115867
iteration : 1034
train acc:  0.734375
train loss:  0.5053791403770447
train gradient:  0.533286301917025
iteration : 1035
train acc:  0.765625
train loss:  0.449718177318573
train gradient:  0.4354640896843021
iteration : 1036
train acc:  0.8046875
train loss:  0.4047204256057739
train gradient:  0.467599942162444
iteration : 1037
train acc:  0.7734375
train loss:  0.4638236165046692
train gradient:  0.5208236754446255
iteration : 1038
train acc:  0.75
train loss:  0.5056290626525879
train gradient:  0.7258292939388924
iteration : 1039
train acc:  0.7578125
train loss:  0.46321284770965576
train gradient:  0.5578229370772483
iteration : 1040
train acc:  0.796875
train loss:  0.4591095745563507
train gradient:  0.4428878792173324
iteration : 1041
train acc:  0.78125
train loss:  0.48300644755363464
train gradient:  0.5376333866028102
iteration : 1042
train acc:  0.796875
train loss:  0.43206989765167236
train gradient:  0.5148280275058685
iteration : 1043
train acc:  0.75
train loss:  0.520114541053772
train gradient:  0.6311154316843876
iteration : 1044
train acc:  0.8046875
train loss:  0.38812997937202454
train gradient:  0.3514446554220305
iteration : 1045
train acc:  0.828125
train loss:  0.4887627959251404
train gradient:  0.5120518422970473
iteration : 1046
train acc:  0.8125
train loss:  0.40422582626342773
train gradient:  0.3932999449592734
iteration : 1047
train acc:  0.8125
train loss:  0.47717374563217163
train gradient:  0.7465701817654318
iteration : 1048
train acc:  0.8515625
train loss:  0.39243587851524353
train gradient:  0.4264030546852408
iteration : 1049
train acc:  0.8125
train loss:  0.4444551467895508
train gradient:  0.5605184451051619
iteration : 1050
train acc:  0.75
train loss:  0.4644005298614502
train gradient:  0.4165679340396478
iteration : 1051
train acc:  0.796875
train loss:  0.43258994817733765
train gradient:  0.7227617976661
iteration : 1052
train acc:  0.78125
train loss:  0.45583653450012207
train gradient:  0.6531338228873595
iteration : 1053
train acc:  0.796875
train loss:  0.45525071024894714
train gradient:  0.47881992501705745
iteration : 1054
train acc:  0.8125
train loss:  0.41770631074905396
train gradient:  0.4761202937599478
iteration : 1055
train acc:  0.796875
train loss:  0.4296630620956421
train gradient:  0.4808528561233596
iteration : 1056
train acc:  0.7890625
train loss:  0.4979894757270813
train gradient:  0.6213339462772539
iteration : 1057
train acc:  0.796875
train loss:  0.4361501932144165
train gradient:  0.5869816492981901
iteration : 1058
train acc:  0.828125
train loss:  0.40745508670806885
train gradient:  0.38764502190798994
iteration : 1059
train acc:  0.8515625
train loss:  0.4010467529296875
train gradient:  0.3671088061148347
iteration : 1060
train acc:  0.8515625
train loss:  0.37930548191070557
train gradient:  0.4016989991156994
iteration : 1061
train acc:  0.8046875
train loss:  0.4618648886680603
train gradient:  0.6308533450186714
iteration : 1062
train acc:  0.78125
train loss:  0.4683092534542084
train gradient:  0.6121559892149857
iteration : 1063
train acc:  0.8125
train loss:  0.42595458030700684
train gradient:  0.3505454488922511
iteration : 1064
train acc:  0.8203125
train loss:  0.4144042730331421
train gradient:  0.5537054090327558
iteration : 1065
train acc:  0.8046875
train loss:  0.46419405937194824
train gradient:  0.5760404258915017
iteration : 1066
train acc:  0.78125
train loss:  0.5140185356140137
train gradient:  0.5982151868919908
iteration : 1067
train acc:  0.78125
train loss:  0.48658373951911926
train gradient:  0.3958355889478937
iteration : 1068
train acc:  0.8046875
train loss:  0.387518048286438
train gradient:  0.4877406231773916
iteration : 1069
train acc:  0.7890625
train loss:  0.4626930356025696
train gradient:  0.7112094165916135
iteration : 1070
train acc:  0.78125
train loss:  0.46662747859954834
train gradient:  0.4109396608536766
iteration : 1071
train acc:  0.8125
train loss:  0.43730172514915466
train gradient:  0.3982841223226286
iteration : 1072
train acc:  0.828125
train loss:  0.37966346740722656
train gradient:  0.45011399032368726
iteration : 1073
train acc:  0.734375
train loss:  0.575425386428833
train gradient:  0.6419552895608915
iteration : 1074
train acc:  0.7578125
train loss:  0.43532317876815796
train gradient:  0.43799616174939293
iteration : 1075
train acc:  0.8203125
train loss:  0.3985217809677124
train gradient:  0.5745951590460611
iteration : 1076
train acc:  0.7890625
train loss:  0.4107680916786194
train gradient:  0.4223240990503805
iteration : 1077
train acc:  0.7421875
train loss:  0.479099303483963
train gradient:  0.7176233884469709
iteration : 1078
train acc:  0.796875
train loss:  0.4412105083465576
train gradient:  0.5719200821727374
iteration : 1079
train acc:  0.78125
train loss:  0.5158833861351013
train gradient:  0.5827631017912369
iteration : 1080
train acc:  0.765625
train loss:  0.5294585824012756
train gradient:  0.5777423415637103
iteration : 1081
train acc:  0.75
train loss:  0.4749298691749573
train gradient:  0.47049850055864145
iteration : 1082
train acc:  0.828125
train loss:  0.4385554790496826
train gradient:  0.5477606332831608
iteration : 1083
train acc:  0.78125
train loss:  0.39382702112197876
train gradient:  0.3489815499798976
iteration : 1084
train acc:  0.8203125
train loss:  0.38859206438064575
train gradient:  0.32164059614245855
iteration : 1085
train acc:  0.796875
train loss:  0.48546212911605835
train gradient:  0.5849067590781667
iteration : 1086
train acc:  0.7109375
train loss:  0.4679962992668152
train gradient:  0.5054324919060242
iteration : 1087
train acc:  0.7578125
train loss:  0.48454856872558594
train gradient:  0.45530522942440843
iteration : 1088
train acc:  0.8125
train loss:  0.4316999018192291
train gradient:  0.36495768813693735
iteration : 1089
train acc:  0.8515625
train loss:  0.42988765239715576
train gradient:  0.5840430163452882
iteration : 1090
train acc:  0.828125
train loss:  0.3929533362388611
train gradient:  0.48561739563023926
iteration : 1091
train acc:  0.765625
train loss:  0.4852096736431122
train gradient:  0.42564284717137907
iteration : 1092
train acc:  0.7421875
train loss:  0.49448809027671814
train gradient:  0.4171031728068409
iteration : 1093
train acc:  0.8125
train loss:  0.4204148054122925
train gradient:  0.37053176062522725
iteration : 1094
train acc:  0.75
train loss:  0.4648107886314392
train gradient:  0.38745274750974285
iteration : 1095
train acc:  0.75
train loss:  0.46269211173057556
train gradient:  0.513421032187906
iteration : 1096
train acc:  0.8359375
train loss:  0.3642439842224121
train gradient:  0.3333507604024186
iteration : 1097
train acc:  0.78125
train loss:  0.4450923800468445
train gradient:  0.6380573550961611
iteration : 1098
train acc:  0.734375
train loss:  0.468915730714798
train gradient:  0.48253052010719505
iteration : 1099
train acc:  0.796875
train loss:  0.4299621880054474
train gradient:  0.3420893838558672
iteration : 1100
train acc:  0.8515625
train loss:  0.41647499799728394
train gradient:  0.3526506514195568
iteration : 1101
train acc:  0.78125
train loss:  0.5003269910812378
train gradient:  0.6409980784399107
iteration : 1102
train acc:  0.84375
train loss:  0.3893952965736389
train gradient:  0.4343982869394699
iteration : 1103
train acc:  0.8515625
train loss:  0.38311249017715454
train gradient:  0.36730992817933067
iteration : 1104
train acc:  0.78125
train loss:  0.4026518762111664
train gradient:  0.5050029628471503
iteration : 1105
train acc:  0.7734375
train loss:  0.436376690864563
train gradient:  0.6146978964553181
iteration : 1106
train acc:  0.7734375
train loss:  0.4517991244792938
train gradient:  0.36085983764724466
iteration : 1107
train acc:  0.78125
train loss:  0.4267061650753021
train gradient:  0.5676990846859034
iteration : 1108
train acc:  0.7890625
train loss:  0.45385152101516724
train gradient:  0.6382079065033732
iteration : 1109
train acc:  0.8046875
train loss:  0.4151379466056824
train gradient:  0.4380385519911072
iteration : 1110
train acc:  0.7578125
train loss:  0.4643644094467163
train gradient:  0.666302491975538
iteration : 1111
train acc:  0.8359375
train loss:  0.3805012106895447
train gradient:  0.28827000165382405
iteration : 1112
train acc:  0.765625
train loss:  0.4168344736099243
train gradient:  0.6933485811979418
iteration : 1113
train acc:  0.796875
train loss:  0.4483964741230011
train gradient:  0.4645624205963568
iteration : 1114
train acc:  0.8203125
train loss:  0.40456652641296387
train gradient:  0.40086000155947216
iteration : 1115
train acc:  0.75
train loss:  0.4890340566635132
train gradient:  0.47343438144504174
iteration : 1116
train acc:  0.7890625
train loss:  0.43716123700141907
train gradient:  0.3712077128612878
iteration : 1117
train acc:  0.78125
train loss:  0.4464682340621948
train gradient:  0.6591461203889197
iteration : 1118
train acc:  0.8359375
train loss:  0.3494754135608673
train gradient:  0.3837814552406139
iteration : 1119
train acc:  0.796875
train loss:  0.46385639905929565
train gradient:  0.5422919684087302
iteration : 1120
train acc:  0.8203125
train loss:  0.4356166124343872
train gradient:  0.4634618953860754
iteration : 1121
train acc:  0.734375
train loss:  0.5507224798202515
train gradient:  0.8185301658849493
iteration : 1122
train acc:  0.8515625
train loss:  0.3922852575778961
train gradient:  0.3565075586123549
iteration : 1123
train acc:  0.8125
train loss:  0.46555522084236145
train gradient:  0.6408944771349642
iteration : 1124
train acc:  0.7890625
train loss:  0.45744380354881287
train gradient:  0.7459513491361688
iteration : 1125
train acc:  0.8203125
train loss:  0.44128209352493286
train gradient:  0.471151650374714
iteration : 1126
train acc:  0.78125
train loss:  0.46133488416671753
train gradient:  0.4408619583493497
iteration : 1127
train acc:  0.765625
train loss:  0.4578123688697815
train gradient:  0.5739829502439879
iteration : 1128
train acc:  0.734375
train loss:  0.5058976411819458
train gradient:  0.5833808378566468
iteration : 1129
train acc:  0.7421875
train loss:  0.5112975835800171
train gradient:  0.8189919552046647
iteration : 1130
train acc:  0.7890625
train loss:  0.4722484052181244
train gradient:  0.5027084699532561
iteration : 1131
train acc:  0.7890625
train loss:  0.44095540046691895
train gradient:  0.4409555196393267
iteration : 1132
train acc:  0.7421875
train loss:  0.48280686140060425
train gradient:  0.5642972905325215
iteration : 1133
train acc:  0.8125
train loss:  0.41037601232528687
train gradient:  0.415101346424977
iteration : 1134
train acc:  0.7421875
train loss:  0.48335984349250793
train gradient:  0.7261646010673912
iteration : 1135
train acc:  0.765625
train loss:  0.44402024149894714
train gradient:  0.6099677910030938
iteration : 1136
train acc:  0.75
train loss:  0.4421910345554352
train gradient:  0.40534783007854663
iteration : 1137
train acc:  0.7734375
train loss:  0.4842487573623657
train gradient:  0.4231116790324808
iteration : 1138
train acc:  0.7578125
train loss:  0.4692423939704895
train gradient:  0.3427380644028432
iteration : 1139
train acc:  0.71875
train loss:  0.5404484272003174
train gradient:  0.5691690051713023
iteration : 1140
train acc:  0.7265625
train loss:  0.522922933101654
train gradient:  0.9276415897073643
iteration : 1141
train acc:  0.796875
train loss:  0.45185983180999756
train gradient:  0.5345548049249271
iteration : 1142
train acc:  0.7890625
train loss:  0.46787717938423157
train gradient:  0.6254105025004268
iteration : 1143
train acc:  0.7890625
train loss:  0.4670897424221039
train gradient:  0.44637958514021514
iteration : 1144
train acc:  0.8046875
train loss:  0.41936615109443665
train gradient:  0.6468000360947199
iteration : 1145
train acc:  0.7265625
train loss:  0.49492859840393066
train gradient:  0.5977253571905611
iteration : 1146
train acc:  0.8046875
train loss:  0.4352297782897949
train gradient:  0.48906570920370346
iteration : 1147
train acc:  0.765625
train loss:  0.45753297209739685
train gradient:  0.40282718271004253
iteration : 1148
train acc:  0.7421875
train loss:  0.519557774066925
train gradient:  0.6185254466753809
iteration : 1149
train acc:  0.7890625
train loss:  0.42703527212142944
train gradient:  0.3078397622755307
iteration : 1150
train acc:  0.78125
train loss:  0.48340916633605957
train gradient:  0.48760306042341156
iteration : 1151
train acc:  0.7578125
train loss:  0.48969873785972595
train gradient:  0.5531455544484167
iteration : 1152
train acc:  0.7890625
train loss:  0.4370262026786804
train gradient:  0.5210376197484907
iteration : 1153
train acc:  0.8515625
train loss:  0.38652268052101135
train gradient:  0.3763355994624996
iteration : 1154
train acc:  0.84375
train loss:  0.3873421549797058
train gradient:  0.33279645731639423
iteration : 1155
train acc:  0.8359375
train loss:  0.3685205578804016
train gradient:  0.3006160592271343
iteration : 1156
train acc:  0.78125
train loss:  0.4413832426071167
train gradient:  0.4060668196887684
iteration : 1157
train acc:  0.890625
train loss:  0.38719406723976135
train gradient:  0.4612524183953672
iteration : 1158
train acc:  0.7421875
train loss:  0.4666392505168915
train gradient:  0.4908731280743991
iteration : 1159
train acc:  0.78125
train loss:  0.4528627395629883
train gradient:  0.4780716137958878
iteration : 1160
train acc:  0.8203125
train loss:  0.4104524850845337
train gradient:  0.2846275165683288
iteration : 1161
train acc:  0.8515625
train loss:  0.3945823311805725
train gradient:  0.40413093112386295
iteration : 1162
train acc:  0.828125
train loss:  0.4845404624938965
train gradient:  0.6407998983972332
iteration : 1163
train acc:  0.7734375
train loss:  0.45340824127197266
train gradient:  0.48064349200505674
iteration : 1164
train acc:  0.75
train loss:  0.47709837555885315
train gradient:  0.45466217085867094
iteration : 1165
train acc:  0.8046875
train loss:  0.4312201738357544
train gradient:  0.31893868398577163
iteration : 1166
train acc:  0.8203125
train loss:  0.38077545166015625
train gradient:  0.39958076320755004
iteration : 1167
train acc:  0.765625
train loss:  0.4264666736125946
train gradient:  0.5085837831581799
iteration : 1168
train acc:  0.84375
train loss:  0.37978628277778625
train gradient:  0.27271689162653245
iteration : 1169
train acc:  0.8515625
train loss:  0.3620801270008087
train gradient:  0.32749403293367324
iteration : 1170
train acc:  0.8046875
train loss:  0.43995362520217896
train gradient:  0.7622593329422812
iteration : 1171
train acc:  0.7890625
train loss:  0.46753770112991333
train gradient:  0.739864103598131
iteration : 1172
train acc:  0.8046875
train loss:  0.4033444821834564
train gradient:  0.4236122566519726
iteration : 1173
train acc:  0.765625
train loss:  0.4391123652458191
train gradient:  0.3966001766463093
iteration : 1174
train acc:  0.796875
train loss:  0.4711216688156128
train gradient:  0.44588275069227634
iteration : 1175
train acc:  0.8125
train loss:  0.3798273503780365
train gradient:  0.45269274317318936
iteration : 1176
train acc:  0.796875
train loss:  0.4871589243412018
train gradient:  0.8391585297570199
iteration : 1177
train acc:  0.796875
train loss:  0.40460965037345886
train gradient:  0.4594930468738711
iteration : 1178
train acc:  0.796875
train loss:  0.42284178733825684
train gradient:  0.5010053694886423
iteration : 1179
train acc:  0.84375
train loss:  0.40886181592941284
train gradient:  0.5244673312942255
iteration : 1180
train acc:  0.8125
train loss:  0.42793697118759155
train gradient:  0.4218983542319139
iteration : 1181
train acc:  0.7265625
train loss:  0.5053740739822388
train gradient:  0.5554440403562926
iteration : 1182
train acc:  0.7890625
train loss:  0.4405995011329651
train gradient:  0.663323337449349
iteration : 1183
train acc:  0.78125
train loss:  0.46182289719581604
train gradient:  0.5287018030397419
iteration : 1184
train acc:  0.7890625
train loss:  0.4755787253379822
train gradient:  0.5288432872852489
iteration : 1185
train acc:  0.71875
train loss:  0.5457932353019714
train gradient:  0.8783342139000799
iteration : 1186
train acc:  0.7890625
train loss:  0.4805217385292053
train gradient:  0.5298678290615193
iteration : 1187
train acc:  0.8125
train loss:  0.48470044136047363
train gradient:  0.977252157765454
iteration : 1188
train acc:  0.796875
train loss:  0.41458460688591003
train gradient:  0.3929437675960478
iteration : 1189
train acc:  0.7890625
train loss:  0.42862948775291443
train gradient:  0.5618760932812754
iteration : 1190
train acc:  0.765625
train loss:  0.4747210144996643
train gradient:  0.587989834851766
iteration : 1191
train acc:  0.78125
train loss:  0.4188370108604431
train gradient:  0.5495816472979987
iteration : 1192
train acc:  0.78125
train loss:  0.4471862316131592
train gradient:  0.4504681354842524
iteration : 1193
train acc:  0.859375
train loss:  0.3592604398727417
train gradient:  0.3396934052496336
iteration : 1194
train acc:  0.78125
train loss:  0.4892641603946686
train gradient:  0.4405319958541266
iteration : 1195
train acc:  0.7890625
train loss:  0.4912993013858795
train gradient:  0.5170734265220034
iteration : 1196
train acc:  0.8359375
train loss:  0.38939034938812256
train gradient:  0.3834438123983304
iteration : 1197
train acc:  0.8046875
train loss:  0.38789603114128113
train gradient:  0.36674736139658615
iteration : 1198
train acc:  0.8359375
train loss:  0.40085798501968384
train gradient:  0.28946546432158476
iteration : 1199
train acc:  0.7890625
train loss:  0.462427020072937
train gradient:  0.4639799928303275
iteration : 1200
train acc:  0.7734375
train loss:  0.45816051959991455
train gradient:  0.7177767672353036
iteration : 1201
train acc:  0.8046875
train loss:  0.4038199782371521
train gradient:  0.5615765984577343
iteration : 1202
train acc:  0.8125
train loss:  0.3970681428909302
train gradient:  0.4704997854119863
iteration : 1203
train acc:  0.7734375
train loss:  0.4962307810783386
train gradient:  0.5317963956443497
iteration : 1204
train acc:  0.8359375
train loss:  0.3845558166503906
train gradient:  0.4428413060626687
iteration : 1205
train acc:  0.7578125
train loss:  0.43726062774658203
train gradient:  0.4392149882827212
iteration : 1206
train acc:  0.765625
train loss:  0.45503437519073486
train gradient:  0.45806139128899503
iteration : 1207
train acc:  0.7890625
train loss:  0.48678967356681824
train gradient:  0.47998958452604346
iteration : 1208
train acc:  0.8203125
train loss:  0.40256553888320923
train gradient:  0.3018187221772866
iteration : 1209
train acc:  0.859375
train loss:  0.4164421558380127
train gradient:  0.42399765074681267
iteration : 1210
train acc:  0.8203125
train loss:  0.3937801122665405
train gradient:  0.46056523813611727
iteration : 1211
train acc:  0.8203125
train loss:  0.46984338760375977
train gradient:  0.731810240862975
iteration : 1212
train acc:  0.7578125
train loss:  0.524988055229187
train gradient:  0.5996609943605403
iteration : 1213
train acc:  0.7734375
train loss:  0.4460522532463074
train gradient:  0.5192455669727442
iteration : 1214
train acc:  0.796875
train loss:  0.47704124450683594
train gradient:  0.5763686376602641
iteration : 1215
train acc:  0.7890625
train loss:  0.4176626205444336
train gradient:  0.45553316667238863
iteration : 1216
train acc:  0.78125
train loss:  0.4675971567630768
train gradient:  0.56109262649791
iteration : 1217
train acc:  0.84375
train loss:  0.4205451011657715
train gradient:  0.4042900913426435
iteration : 1218
train acc:  0.7578125
train loss:  0.4909343719482422
train gradient:  0.5572142049629552
iteration : 1219
train acc:  0.7421875
train loss:  0.4837839603424072
train gradient:  0.7594653162396914
iteration : 1220
train acc:  0.859375
train loss:  0.3843095898628235
train gradient:  0.4454636804284915
iteration : 1221
train acc:  0.8125
train loss:  0.4428147077560425
train gradient:  0.5159902806810647
iteration : 1222
train acc:  0.765625
train loss:  0.4972657561302185
train gradient:  0.5799219173208741
iteration : 1223
train acc:  0.8046875
train loss:  0.41869211196899414
train gradient:  0.4784971006751219
iteration : 1224
train acc:  0.8046875
train loss:  0.4167032837867737
train gradient:  0.4083583203381996
iteration : 1225
train acc:  0.796875
train loss:  0.45498764514923096
train gradient:  0.7951112224100398
iteration : 1226
train acc:  0.8203125
train loss:  0.40705370903015137
train gradient:  0.39005720210649303
iteration : 1227
train acc:  0.859375
train loss:  0.39672791957855225
train gradient:  0.3636656678532186
iteration : 1228
train acc:  0.8359375
train loss:  0.35061410069465637
train gradient:  0.345445376420761
iteration : 1229
train acc:  0.8515625
train loss:  0.39966949820518494
train gradient:  0.6160641974473666
iteration : 1230
train acc:  0.734375
train loss:  0.48283135890960693
train gradient:  0.3970312299696971
iteration : 1231
train acc:  0.8046875
train loss:  0.4396822154521942
train gradient:  0.4211970173034465
iteration : 1232
train acc:  0.828125
train loss:  0.3449576497077942
train gradient:  0.3290977979094026
iteration : 1233
train acc:  0.7734375
train loss:  0.452377051115036
train gradient:  0.5729483810156341
iteration : 1234
train acc:  0.734375
train loss:  0.48483186960220337
train gradient:  0.5684718236027386
iteration : 1235
train acc:  0.8359375
train loss:  0.38076990842819214
train gradient:  0.38023621778101674
iteration : 1236
train acc:  0.8203125
train loss:  0.4048079252243042
train gradient:  0.4167408984331705
iteration : 1237
train acc:  0.84375
train loss:  0.38461631536483765
train gradient:  0.40997158389244875
iteration : 1238
train acc:  0.828125
train loss:  0.3935239911079407
train gradient:  0.39863152695706183
iteration : 1239
train acc:  0.796875
train loss:  0.4362792372703552
train gradient:  0.42030292156313337
iteration : 1240
train acc:  0.859375
train loss:  0.38868746161460876
train gradient:  0.39701881507032866
iteration : 1241
train acc:  0.8359375
train loss:  0.40385040640830994
train gradient:  0.5680034717905219
iteration : 1242
train acc:  0.8203125
train loss:  0.4276559352874756
train gradient:  0.4964232684720347
iteration : 1243
train acc:  0.796875
train loss:  0.41966795921325684
train gradient:  0.5628227638971851
iteration : 1244
train acc:  0.796875
train loss:  0.4621453285217285
train gradient:  0.5101804738521303
iteration : 1245
train acc:  0.78125
train loss:  0.42549189925193787
train gradient:  0.5410898963353648
iteration : 1246
train acc:  0.828125
train loss:  0.46537965536117554
train gradient:  0.5403724763475783
iteration : 1247
train acc:  0.7890625
train loss:  0.4437800645828247
train gradient:  0.3867381601533769
iteration : 1248
train acc:  0.796875
train loss:  0.4610138237476349
train gradient:  0.6824064109435528
iteration : 1249
train acc:  0.7890625
train loss:  0.47158992290496826
train gradient:  0.6579644909810973
iteration : 1250
train acc:  0.8046875
train loss:  0.39497286081314087
train gradient:  0.5102312046977777
iteration : 1251
train acc:  0.8359375
train loss:  0.4024296998977661
train gradient:  0.5106377142024396
iteration : 1252
train acc:  0.765625
train loss:  0.45057275891304016
train gradient:  0.6704323301417182
iteration : 1253
train acc:  0.828125
train loss:  0.430943101644516
train gradient:  0.49926555295113667
iteration : 1254
train acc:  0.8125
train loss:  0.44089967012405396
train gradient:  0.45910848403159243
iteration : 1255
train acc:  0.8046875
train loss:  0.45902732014656067
train gradient:  0.5616719076789118
iteration : 1256
train acc:  0.75
train loss:  0.4308009147644043
train gradient:  0.47545147914051494
iteration : 1257
train acc:  0.7578125
train loss:  0.4632176160812378
train gradient:  0.7135022339375936
iteration : 1258
train acc:  0.7734375
train loss:  0.4891485273838043
train gradient:  0.6083368421591725
iteration : 1259
train acc:  0.8359375
train loss:  0.37817803025245667
train gradient:  0.437375684717688
iteration : 1260
train acc:  0.7890625
train loss:  0.41174906492233276
train gradient:  0.37638774038125633
iteration : 1261
train acc:  0.7890625
train loss:  0.3999030292034149
train gradient:  0.47915216948533335
iteration : 1262
train acc:  0.7421875
train loss:  0.4627300202846527
train gradient:  0.41409668270488853
iteration : 1263
train acc:  0.78125
train loss:  0.48295140266418457
train gradient:  0.5932666273215883
iteration : 1264
train acc:  0.8515625
train loss:  0.35947078466415405
train gradient:  0.48523844493474116
iteration : 1265
train acc:  0.8359375
train loss:  0.42669233679771423
train gradient:  0.2808104133823965
iteration : 1266
train acc:  0.796875
train loss:  0.4932781159877777
train gradient:  0.4457660307170389
iteration : 1267
train acc:  0.828125
train loss:  0.36952492594718933
train gradient:  0.39163359605654685
iteration : 1268
train acc:  0.8359375
train loss:  0.39513617753982544
train gradient:  0.3881055362983708
iteration : 1269
train acc:  0.796875
train loss:  0.5031627416610718
train gradient:  0.7888214453588603
iteration : 1270
train acc:  0.8515625
train loss:  0.3637370467185974
train gradient:  0.28537943412855016
iteration : 1271
train acc:  0.7734375
train loss:  0.4073534309864044
train gradient:  0.3593812783973855
iteration : 1272
train acc:  0.78125
train loss:  0.44485706090927124
train gradient:  0.41645029488684254
iteration : 1273
train acc:  0.7421875
train loss:  0.4898374676704407
train gradient:  0.4898171615686146
iteration : 1274
train acc:  0.8671875
train loss:  0.35833460092544556
train gradient:  0.4138270541950289
iteration : 1275
train acc:  0.75
train loss:  0.4593706727027893
train gradient:  0.5587918188588483
iteration : 1276
train acc:  0.765625
train loss:  0.5172126293182373
train gradient:  0.6700904395363669
iteration : 1277
train acc:  0.796875
train loss:  0.4095085859298706
train gradient:  0.4498065879112951
iteration : 1278
train acc:  0.7734375
train loss:  0.4625037610530853
train gradient:  0.41858340314261255
iteration : 1279
train acc:  0.8359375
train loss:  0.4011813998222351
train gradient:  0.45197066125705837
iteration : 1280
train acc:  0.78125
train loss:  0.4489758014678955
train gradient:  0.4297080443020251
iteration : 1281
train acc:  0.8359375
train loss:  0.38364410400390625
train gradient:  0.40592139262354054
iteration : 1282
train acc:  0.7890625
train loss:  0.46626919507980347
train gradient:  0.4239823781511381
iteration : 1283
train acc:  0.7890625
train loss:  0.4213060736656189
train gradient:  0.4869360957460149
iteration : 1284
train acc:  0.7890625
train loss:  0.43187108635902405
train gradient:  0.4562020070183519
iteration : 1285
train acc:  0.8125
train loss:  0.45172715187072754
train gradient:  0.8194274337921397
iteration : 1286
train acc:  0.7734375
train loss:  0.4087828993797302
train gradient:  0.5821936041686411
iteration : 1287
train acc:  0.78125
train loss:  0.48892366886138916
train gradient:  0.6725217688587446
iteration : 1288
train acc:  0.7890625
train loss:  0.45764899253845215
train gradient:  0.38222266384220877
iteration : 1289
train acc:  0.828125
train loss:  0.4210360050201416
train gradient:  0.5651730298014995
iteration : 1290
train acc:  0.734375
train loss:  0.4828774333000183
train gradient:  0.532329454755399
iteration : 1291
train acc:  0.8359375
train loss:  0.42726951837539673
train gradient:  0.5184762165625116
iteration : 1292
train acc:  0.7734375
train loss:  0.44047367572784424
train gradient:  0.4712646012133547
iteration : 1293
train acc:  0.765625
train loss:  0.4634946584701538
train gradient:  0.35300535702534325
iteration : 1294
train acc:  0.796875
train loss:  0.37832343578338623
train gradient:  0.36955054312308283
iteration : 1295
train acc:  0.765625
train loss:  0.4125971794128418
train gradient:  0.5686801564432634
iteration : 1296
train acc:  0.75
train loss:  0.4647356867790222
train gradient:  0.5079565710151976
iteration : 1297
train acc:  0.78125
train loss:  0.5355677604675293
train gradient:  0.6437046198005991
iteration : 1298
train acc:  0.8359375
train loss:  0.3674548864364624
train gradient:  0.27028625220121005
iteration : 1299
train acc:  0.8046875
train loss:  0.42949897050857544
train gradient:  0.47637433029008835
iteration : 1300
train acc:  0.7578125
train loss:  0.5109028220176697
train gradient:  0.6692338320019525
iteration : 1301
train acc:  0.8046875
train loss:  0.4540177583694458
train gradient:  0.5266293476690431
iteration : 1302
train acc:  0.8046875
train loss:  0.4526621103286743
train gradient:  0.5022474449259368
iteration : 1303
train acc:  0.7890625
train loss:  0.41120645403862
train gradient:  0.4055295324112323
iteration : 1304
train acc:  0.8046875
train loss:  0.41726988554000854
train gradient:  0.3785594260742585
iteration : 1305
train acc:  0.7578125
train loss:  0.4882340133190155
train gradient:  0.5025771822304657
iteration : 1306
train acc:  0.7890625
train loss:  0.4670177102088928
train gradient:  0.6161612958224056
iteration : 1307
train acc:  0.796875
train loss:  0.4491017758846283
train gradient:  0.5830159019085589
iteration : 1308
train acc:  0.8125
train loss:  0.45599210262298584
train gradient:  0.41508887131843836
iteration : 1309
train acc:  0.78125
train loss:  0.4731196165084839
train gradient:  0.46473246440726274
iteration : 1310
train acc:  0.8359375
train loss:  0.36812645196914673
train gradient:  0.3482374505190593
iteration : 1311
train acc:  0.8359375
train loss:  0.3972470760345459
train gradient:  0.4367557024986538
iteration : 1312
train acc:  0.8203125
train loss:  0.4452834725379944
train gradient:  0.3712652744686904
iteration : 1313
train acc:  0.8046875
train loss:  0.4091060757637024
train gradient:  0.47599867119129274
iteration : 1314
train acc:  0.7890625
train loss:  0.39275476336479187
train gradient:  0.4118887355495668
iteration : 1315
train acc:  0.8046875
train loss:  0.42074254155158997
train gradient:  0.4110393413738663
iteration : 1316
train acc:  0.8203125
train loss:  0.4212331771850586
train gradient:  0.4422189831119397
iteration : 1317
train acc:  0.828125
train loss:  0.38443613052368164
train gradient:  0.40030082086285557
iteration : 1318
train acc:  0.765625
train loss:  0.46250951290130615
train gradient:  0.6551446241239771
iteration : 1319
train acc:  0.75
train loss:  0.4630530774593353
train gradient:  0.536452077611055
iteration : 1320
train acc:  0.7578125
train loss:  0.45939159393310547
train gradient:  0.5071146672345725
iteration : 1321
train acc:  0.7421875
train loss:  0.4950200617313385
train gradient:  0.6154168060744697
iteration : 1322
train acc:  0.8125
train loss:  0.3883182406425476
train gradient:  0.4937883109252745
iteration : 1323
train acc:  0.7578125
train loss:  0.4120546579360962
train gradient:  0.5896654565278996
iteration : 1324
train acc:  0.765625
train loss:  0.458335280418396
train gradient:  0.5045191003711295
iteration : 1325
train acc:  0.84375
train loss:  0.35933804512023926
train gradient:  0.4159387137765756
iteration : 1326
train acc:  0.8359375
train loss:  0.4352114796638489
train gradient:  0.37041624344625884
iteration : 1327
train acc:  0.8203125
train loss:  0.39730459451675415
train gradient:  0.34581955136964826
iteration : 1328
train acc:  0.7421875
train loss:  0.48161211609840393
train gradient:  0.532402501957987
iteration : 1329
train acc:  0.8046875
train loss:  0.43710801005363464
train gradient:  0.36496165562136573
iteration : 1330
train acc:  0.765625
train loss:  0.4842638373374939
train gradient:  0.5586605447653705
iteration : 1331
train acc:  0.78125
train loss:  0.45383596420288086
train gradient:  0.3769908801590772
iteration : 1332
train acc:  0.8125
train loss:  0.42210453748703003
train gradient:  0.3568483468838402
iteration : 1333
train acc:  0.765625
train loss:  0.459605872631073
train gradient:  0.3716081567238206
iteration : 1334
train acc:  0.796875
train loss:  0.4789143204689026
train gradient:  0.44856827087293355
iteration : 1335
train acc:  0.8125
train loss:  0.41678643226623535
train gradient:  0.39200406365187174
iteration : 1336
train acc:  0.78125
train loss:  0.5285534858703613
train gradient:  0.5776871239618496
iteration : 1337
train acc:  0.8046875
train loss:  0.4348434805870056
train gradient:  0.41920864368771044
iteration : 1338
train acc:  0.828125
train loss:  0.44273391366004944
train gradient:  0.5331716465690737
iteration : 1339
train acc:  0.859375
train loss:  0.3570060729980469
train gradient:  0.32992274435044894
iteration : 1340
train acc:  0.8046875
train loss:  0.46266722679138184
train gradient:  0.4262265964896249
iteration : 1341
train acc:  0.875
train loss:  0.3887265920639038
train gradient:  0.3711047788137645
iteration : 1342
train acc:  0.78125
train loss:  0.47071248292922974
train gradient:  0.45886607879510555
iteration : 1343
train acc:  0.7734375
train loss:  0.4845760464668274
train gradient:  0.37717776750894516
iteration : 1344
train acc:  0.8125
train loss:  0.41758590936660767
train gradient:  0.4032957751546625
iteration : 1345
train acc:  0.8671875
train loss:  0.3540565073490143
train gradient:  0.32262919536981943
iteration : 1346
train acc:  0.828125
train loss:  0.4685429334640503
train gradient:  0.47064993288238255
iteration : 1347
train acc:  0.7890625
train loss:  0.4477190375328064
train gradient:  0.5383546756220952
iteration : 1348
train acc:  0.8125
train loss:  0.3727931082248688
train gradient:  0.37249312426856684
iteration : 1349
train acc:  0.8203125
train loss:  0.43179041147232056
train gradient:  0.4328829233978413
iteration : 1350
train acc:  0.8125
train loss:  0.403134286403656
train gradient:  0.39869087528554215
iteration : 1351
train acc:  0.78125
train loss:  0.4278726577758789
train gradient:  0.43760024717987506
iteration : 1352
train acc:  0.8046875
train loss:  0.3661916255950928
train gradient:  0.35558154899024547
iteration : 1353
train acc:  0.828125
train loss:  0.3935015797615051
train gradient:  0.32639845112287047
iteration : 1354
train acc:  0.828125
train loss:  0.39706891775131226
train gradient:  0.42570812179721146
iteration : 1355
train acc:  0.7734375
train loss:  0.4677681624889374
train gradient:  0.6433730025514534
iteration : 1356
train acc:  0.796875
train loss:  0.3949654698371887
train gradient:  0.6307328279720636
iteration : 1357
train acc:  0.8359375
train loss:  0.35528016090393066
train gradient:  0.3491818880150482
iteration : 1358
train acc:  0.796875
train loss:  0.404579758644104
train gradient:  0.35285545780967204
iteration : 1359
train acc:  0.765625
train loss:  0.44332027435302734
train gradient:  0.47355767700170787
iteration : 1360
train acc:  0.796875
train loss:  0.4061388671398163
train gradient:  0.4640835807141127
iteration : 1361
train acc:  0.7890625
train loss:  0.45728224515914917
train gradient:  0.4738564850121598
iteration : 1362
train acc:  0.7578125
train loss:  0.40583115816116333
train gradient:  0.3734962624785055
iteration : 1363
train acc:  0.7734375
train loss:  0.4671197831630707
train gradient:  0.4965400519301082
iteration : 1364
train acc:  0.8125
train loss:  0.3918505311012268
train gradient:  0.49177962086218613
iteration : 1365
train acc:  0.8046875
train loss:  0.4947090446949005
train gradient:  0.7036496131629036
iteration : 1366
train acc:  0.8046875
train loss:  0.4233452081680298
train gradient:  0.5208659194741239
iteration : 1367
train acc:  0.7734375
train loss:  0.4892163872718811
train gradient:  0.5385333930340472
iteration : 1368
train acc:  0.7890625
train loss:  0.46551910042762756
train gradient:  0.48769444192021544
iteration : 1369
train acc:  0.78125
train loss:  0.44462883472442627
train gradient:  0.5062760943144067
iteration : 1370
train acc:  0.8515625
train loss:  0.3615094721317291
train gradient:  0.39499790144003255
iteration : 1371
train acc:  0.8046875
train loss:  0.40724989771842957
train gradient:  0.36427306400542475
iteration : 1372
train acc:  0.8203125
train loss:  0.43011224269866943
train gradient:  0.4777327667982782
iteration : 1373
train acc:  0.8046875
train loss:  0.43169844150543213
train gradient:  0.5371315476618232
iteration : 1374
train acc:  0.8125
train loss:  0.404524028301239
train gradient:  0.35377077145322144
iteration : 1375
train acc:  0.765625
train loss:  0.4886547923088074
train gradient:  0.4454838781704698
iteration : 1376
train acc:  0.78125
train loss:  0.4322739541530609
train gradient:  0.33794394949798007
iteration : 1377
train acc:  0.75
train loss:  0.4946504533290863
train gradient:  0.5732410729403432
iteration : 1378
train acc:  0.828125
train loss:  0.44911259412765503
train gradient:  0.5285407381381881
iteration : 1379
train acc:  0.7265625
train loss:  0.5044514536857605
train gradient:  0.552089376824143
iteration : 1380
train acc:  0.8203125
train loss:  0.4344569444656372
train gradient:  0.4794615251536483
iteration : 1381
train acc:  0.8046875
train loss:  0.4553656578063965
train gradient:  0.47027232855742
iteration : 1382
train acc:  0.875
train loss:  0.34156447649002075
train gradient:  0.3245641240685269
iteration : 1383
train acc:  0.84375
train loss:  0.3826598525047302
train gradient:  0.42728786560585297
iteration : 1384
train acc:  0.7734375
train loss:  0.4330380856990814
train gradient:  0.4489231150406844
iteration : 1385
train acc:  0.796875
train loss:  0.4373362958431244
train gradient:  0.5408680295919536
iteration : 1386
train acc:  0.7265625
train loss:  0.5616989135742188
train gradient:  0.7690887591384805
iteration : 1387
train acc:  0.796875
train loss:  0.4207603335380554
train gradient:  0.4488754345691722
iteration : 1388
train acc:  0.8125
train loss:  0.4423733949661255
train gradient:  0.4865495987553121
iteration : 1389
train acc:  0.8125
train loss:  0.4142557978630066
train gradient:  0.4935786905538328
iteration : 1390
train acc:  0.78125
train loss:  0.4800168573856354
train gradient:  0.502472624619241
iteration : 1391
train acc:  0.828125
train loss:  0.36210888624191284
train gradient:  0.4176545104480715
iteration : 1392
train acc:  0.8828125
train loss:  0.3323090970516205
train gradient:  0.4050481233802568
iteration : 1393
train acc:  0.7734375
train loss:  0.48052704334259033
train gradient:  0.46671285077817876
iteration : 1394
train acc:  0.796875
train loss:  0.38437074422836304
train gradient:  0.39994795124400945
iteration : 1395
train acc:  0.8046875
train loss:  0.46831607818603516
train gradient:  0.7822843667355622
iteration : 1396
train acc:  0.7265625
train loss:  0.545005202293396
train gradient:  0.6065540285192675
iteration : 1397
train acc:  0.8515625
train loss:  0.358396977186203
train gradient:  0.4591526981893272
iteration : 1398
train acc:  0.7578125
train loss:  0.4701021909713745
train gradient:  0.6074873412061836
iteration : 1399
train acc:  0.7890625
train loss:  0.4809173345565796
train gradient:  0.7263715529949607
iteration : 1400
train acc:  0.7734375
train loss:  0.4542591869831085
train gradient:  0.4318044117427244
iteration : 1401
train acc:  0.765625
train loss:  0.45644500851631165
train gradient:  0.5545245550847298
iteration : 1402
train acc:  0.8515625
train loss:  0.34598296880722046
train gradient:  0.372645332251925
iteration : 1403
train acc:  0.7578125
train loss:  0.4826110303401947
train gradient:  0.6137588285849723
iteration : 1404
train acc:  0.7421875
train loss:  0.5041103959083557
train gradient:  0.5073007352552029
iteration : 1405
train acc:  0.796875
train loss:  0.48460817337036133
train gradient:  0.5017503650171592
iteration : 1406
train acc:  0.828125
train loss:  0.41755425930023193
train gradient:  0.4132821072105834
iteration : 1407
train acc:  0.84375
train loss:  0.3699831962585449
train gradient:  0.27911957193579023
iteration : 1408
train acc:  0.765625
train loss:  0.4568656086921692
train gradient:  0.41599362311406046
iteration : 1409
train acc:  0.7734375
train loss:  0.4552251100540161
train gradient:  0.5322263680835609
iteration : 1410
train acc:  0.8515625
train loss:  0.3827750086784363
train gradient:  0.45521605553738614
iteration : 1411
train acc:  0.828125
train loss:  0.43879640102386475
train gradient:  0.5915216043723839
iteration : 1412
train acc:  0.859375
train loss:  0.3733835816383362
train gradient:  0.3855653989117359
iteration : 1413
train acc:  0.796875
train loss:  0.496672123670578
train gradient:  0.575939861962613
iteration : 1414
train acc:  0.7421875
train loss:  0.47515869140625
train gradient:  0.5888625713630491
iteration : 1415
train acc:  0.7734375
train loss:  0.46820148825645447
train gradient:  0.6527738658813254
iteration : 1416
train acc:  0.8203125
train loss:  0.4141462445259094
train gradient:  0.5961987644318952
iteration : 1417
train acc:  0.7734375
train loss:  0.48004230856895447
train gradient:  0.698266228413522
iteration : 1418
train acc:  0.78125
train loss:  0.461283415555954
train gradient:  0.4820266982387728
iteration : 1419
train acc:  0.796875
train loss:  0.4442460536956787
train gradient:  0.6625016538537608
iteration : 1420
train acc:  0.8203125
train loss:  0.48506665229797363
train gradient:  0.7692428270257666
iteration : 1421
train acc:  0.8046875
train loss:  0.4315462112426758
train gradient:  0.379673381132511
iteration : 1422
train acc:  0.8359375
train loss:  0.39421147108078003
train gradient:  0.434670901631083
iteration : 1423
train acc:  0.78125
train loss:  0.4743543267250061
train gradient:  0.5908866750568074
iteration : 1424
train acc:  0.78125
train loss:  0.4631038308143616
train gradient:  0.6003454437281625
iteration : 1425
train acc:  0.7890625
train loss:  0.4750358760356903
train gradient:  0.5800116009820028
iteration : 1426
train acc:  0.7421875
train loss:  0.5048715472221375
train gradient:  0.5593944918075935
iteration : 1427
train acc:  0.859375
train loss:  0.3438054919242859
train gradient:  0.3887466208754307
iteration : 1428
train acc:  0.78125
train loss:  0.4069957435131073
train gradient:  0.3422321494574309
iteration : 1429
train acc:  0.8515625
train loss:  0.4061419665813446
train gradient:  0.4304680641733633
iteration : 1430
train acc:  0.7890625
train loss:  0.4426477253437042
train gradient:  0.4582127460317654
iteration : 1431
train acc:  0.8046875
train loss:  0.4212801158428192
train gradient:  0.49593291877515555
iteration : 1432
train acc:  0.828125
train loss:  0.3753245770931244
train gradient:  0.41342175123426417
iteration : 1433
train acc:  0.7890625
train loss:  0.45629337430000305
train gradient:  0.538303313848191
iteration : 1434
train acc:  0.7578125
train loss:  0.4704834818840027
train gradient:  0.4150783470247814
iteration : 1435
train acc:  0.7578125
train loss:  0.4835056960582733
train gradient:  0.4778521713901671
iteration : 1436
train acc:  0.84375
train loss:  0.40201812982559204
train gradient:  0.4441416531010326
iteration : 1437
train acc:  0.828125
train loss:  0.4163450300693512
train gradient:  0.5564924507905898
iteration : 1438
train acc:  0.84375
train loss:  0.3514370322227478
train gradient:  0.3411559446828017
iteration : 1439
train acc:  0.8359375
train loss:  0.409556120634079
train gradient:  0.5307558201520222
iteration : 1440
train acc:  0.8359375
train loss:  0.3892155885696411
train gradient:  0.2888245036151487
iteration : 1441
train acc:  0.765625
train loss:  0.4380406141281128
train gradient:  0.43839161791004144
iteration : 1442
train acc:  0.78125
train loss:  0.4062865972518921
train gradient:  0.39552786255857375
iteration : 1443
train acc:  0.859375
train loss:  0.38474810123443604
train gradient:  0.3724202086081382
iteration : 1444
train acc:  0.71875
train loss:  0.474661648273468
train gradient:  0.47406404224812937
iteration : 1445
train acc:  0.7890625
train loss:  0.42314988374710083
train gradient:  0.6515270755016116
iteration : 1446
train acc:  0.7578125
train loss:  0.4564412236213684
train gradient:  0.4260502819941271
iteration : 1447
train acc:  0.765625
train loss:  0.4541693329811096
train gradient:  0.6422596606287487
iteration : 1448
train acc:  0.7734375
train loss:  0.4651715159416199
train gradient:  0.3810327211729838
iteration : 1449
train acc:  0.7734375
train loss:  0.42605432868003845
train gradient:  0.5169840058669399
iteration : 1450
train acc:  0.84375
train loss:  0.3804176449775696
train gradient:  0.3869670942312799
iteration : 1451
train acc:  0.8203125
train loss:  0.39370566606521606
train gradient:  0.5460215098566835
iteration : 1452
train acc:  0.796875
train loss:  0.45486775040626526
train gradient:  0.7107337127330756
iteration : 1453
train acc:  0.8515625
train loss:  0.33677470684051514
train gradient:  0.3154915684328567
iteration : 1454
train acc:  0.796875
train loss:  0.44372841715812683
train gradient:  0.5015793222785467
iteration : 1455
train acc:  0.8359375
train loss:  0.36860746145248413
train gradient:  0.3029233942457041
iteration : 1456
train acc:  0.828125
train loss:  0.38418105244636536
train gradient:  0.5148694325142988
iteration : 1457
train acc:  0.859375
train loss:  0.3736129105091095
train gradient:  0.3496269665331534
iteration : 1458
train acc:  0.84375
train loss:  0.451452374458313
train gradient:  0.48728219638971
iteration : 1459
train acc:  0.8203125
train loss:  0.46593546867370605
train gradient:  0.5618269176235661
iteration : 1460
train acc:  0.7734375
train loss:  0.42762354016304016
train gradient:  0.38127803321034565
iteration : 1461
train acc:  0.7578125
train loss:  0.5000903606414795
train gradient:  0.8353160443668394
iteration : 1462
train acc:  0.7734375
train loss:  0.4238768517971039
train gradient:  0.48424764119978253
iteration : 1463
train acc:  0.8984375
train loss:  0.33795398473739624
train gradient:  0.5132454829733295
iteration : 1464
train acc:  0.7890625
train loss:  0.49631935358047485
train gradient:  0.741188705416442
iteration : 1465
train acc:  0.8203125
train loss:  0.41692477464675903
train gradient:  0.4714991458766539
iteration : 1466
train acc:  0.796875
train loss:  0.4493344724178314
train gradient:  0.5403183770626467
iteration : 1467
train acc:  0.8203125
train loss:  0.4015771150588989
train gradient:  0.5361231375750927
iteration : 1468
train acc:  0.7578125
train loss:  0.4933103919029236
train gradient:  0.6680975911205702
iteration : 1469
train acc:  0.7578125
train loss:  0.4131186902523041
train gradient:  0.5101389461628151
iteration : 1470
train acc:  0.8359375
train loss:  0.3793165683746338
train gradient:  0.39046794507133803
iteration : 1471
train acc:  0.7890625
train loss:  0.4586057662963867
train gradient:  0.4760953469894395
iteration : 1472
train acc:  0.8671875
train loss:  0.36145609617233276
train gradient:  0.3041486068780477
iteration : 1473
train acc:  0.828125
train loss:  0.3711756765842438
train gradient:  0.41879277494030936
iteration : 1474
train acc:  0.8125
train loss:  0.3663322627544403
train gradient:  0.31712002561296204
iteration : 1475
train acc:  0.8203125
train loss:  0.3770486116409302
train gradient:  0.4424516352656242
iteration : 1476
train acc:  0.78125
train loss:  0.4764705300331116
train gradient:  0.6591423854279295
iteration : 1477
train acc:  0.796875
train loss:  0.4261811673641205
train gradient:  0.3532392782152579
iteration : 1478
train acc:  0.734375
train loss:  0.4801364541053772
train gradient:  0.6604117100227603
iteration : 1479
train acc:  0.8125
train loss:  0.4222039580345154
train gradient:  0.3860581439852121
iteration : 1480
train acc:  0.734375
train loss:  0.4823954701423645
train gradient:  0.6351674541976211
iteration : 1481
train acc:  0.796875
train loss:  0.43145257234573364
train gradient:  0.49650114642724225
iteration : 1482
train acc:  0.7734375
train loss:  0.43134090304374695
train gradient:  0.4061211412200818
iteration : 1483
train acc:  0.75
train loss:  0.508657693862915
train gradient:  0.5873916620829825
iteration : 1484
train acc:  0.765625
train loss:  0.4430273175239563
train gradient:  0.49173373271986626
iteration : 1485
train acc:  0.796875
train loss:  0.4051463305950165
train gradient:  0.512616738256098
iteration : 1486
train acc:  0.78125
train loss:  0.43209779262542725
train gradient:  0.43521188995311993
iteration : 1487
train acc:  0.765625
train loss:  0.48827844858169556
train gradient:  0.523791036651388
iteration : 1488
train acc:  0.7734375
train loss:  0.43604713678359985
train gradient:  0.30430129870173034
iteration : 1489
train acc:  0.765625
train loss:  0.4834085702896118
train gradient:  0.541020907213384
iteration : 1490
train acc:  0.8125
train loss:  0.43324148654937744
train gradient:  0.42862638806486825
iteration : 1491
train acc:  0.7890625
train loss:  0.44914186000823975
train gradient:  0.5140812100342274
iteration : 1492
train acc:  0.796875
train loss:  0.39795035123825073
train gradient:  0.3264620441547899
iteration : 1493
train acc:  0.8046875
train loss:  0.4999033510684967
train gradient:  0.6451537785256046
iteration : 1494
train acc:  0.890625
train loss:  0.3162778317928314
train gradient:  0.3454103849156663
iteration : 1495
train acc:  0.84375
train loss:  0.4433556795120239
train gradient:  0.41455942177464106
iteration : 1496
train acc:  0.765625
train loss:  0.4819979965686798
train gradient:  0.38919611726876296
iteration : 1497
train acc:  0.7890625
train loss:  0.4648284316062927
train gradient:  0.5076114163568145
iteration : 1498
train acc:  0.765625
train loss:  0.42727625370025635
train gradient:  0.34695268663977996
iteration : 1499
train acc:  0.765625
train loss:  0.4321342706680298
train gradient:  0.3310801682807879
iteration : 1500
train acc:  0.7890625
train loss:  0.43239402770996094
train gradient:  0.3443107830902632
iteration : 1501
train acc:  0.828125
train loss:  0.3894720673561096
train gradient:  0.3666453134128599
iteration : 1502
train acc:  0.8984375
train loss:  0.30841752886772156
train gradient:  0.2474866785154814
iteration : 1503
train acc:  0.7890625
train loss:  0.4240607023239136
train gradient:  0.32953722969229426
iteration : 1504
train acc:  0.8203125
train loss:  0.3541203737258911
train gradient:  0.3017332289321114
iteration : 1505
train acc:  0.8359375
train loss:  0.3736479878425598
train gradient:  0.3405246148928917
iteration : 1506
train acc:  0.7890625
train loss:  0.44651806354522705
train gradient:  0.4969151188736984
iteration : 1507
train acc:  0.8515625
train loss:  0.3817847669124603
train gradient:  0.4812101319168473
iteration : 1508
train acc:  0.8046875
train loss:  0.4237651228904724
train gradient:  0.5535491855222401
iteration : 1509
train acc:  0.828125
train loss:  0.4124949872493744
train gradient:  0.2999933760327675
iteration : 1510
train acc:  0.8359375
train loss:  0.35683292150497437
train gradient:  0.36947565666856463
iteration : 1511
train acc:  0.7890625
train loss:  0.45377957820892334
train gradient:  0.500572220788368
iteration : 1512
train acc:  0.8203125
train loss:  0.39230361580848694
train gradient:  0.338689929306109
iteration : 1513
train acc:  0.8359375
train loss:  0.4227876663208008
train gradient:  0.32407256209507285
iteration : 1514
train acc:  0.796875
train loss:  0.4231789708137512
train gradient:  0.542312506779196
iteration : 1515
train acc:  0.8515625
train loss:  0.377210795879364
train gradient:  0.40416609679224624
iteration : 1516
train acc:  0.7890625
train loss:  0.3795284926891327
train gradient:  0.5135869605954584
iteration : 1517
train acc:  0.734375
train loss:  0.4082277715206146
train gradient:  0.5065580097004726
iteration : 1518
train acc:  0.8125
train loss:  0.4086633622646332
train gradient:  0.38876512767221727
iteration : 1519
train acc:  0.8125
train loss:  0.41063734889030457
train gradient:  0.45471182980778285
iteration : 1520
train acc:  0.8125
train loss:  0.41592055559158325
train gradient:  0.5654218903997303
iteration : 1521
train acc:  0.8671875
train loss:  0.3336498439311981
train gradient:  0.2551569713572329
iteration : 1522
train acc:  0.75
train loss:  0.5148517489433289
train gradient:  0.7192460034053951
iteration : 1523
train acc:  0.796875
train loss:  0.3906223773956299
train gradient:  0.3722566429645584
iteration : 1524
train acc:  0.796875
train loss:  0.401530921459198
train gradient:  0.3884306149450215
iteration : 1525
train acc:  0.796875
train loss:  0.48159492015838623
train gradient:  0.8363574914358579
iteration : 1526
train acc:  0.796875
train loss:  0.3593800365924835
train gradient:  0.40433825677338014
iteration : 1527
train acc:  0.875
train loss:  0.3298168480396271
train gradient:  0.5049041590410301
iteration : 1528
train acc:  0.8515625
train loss:  0.36416110396385193
train gradient:  0.3242981324169412
iteration : 1529
train acc:  0.8515625
train loss:  0.3338521718978882
train gradient:  0.4383752437429904
iteration : 1530
train acc:  0.8203125
train loss:  0.4205665588378906
train gradient:  0.4758791093377353
iteration : 1531
train acc:  0.78125
train loss:  0.481056809425354
train gradient:  0.7455429266274768
iteration : 1532
train acc:  0.78125
train loss:  0.39186933636665344
train gradient:  0.6948110366958614
iteration : 1533
train acc:  0.7890625
train loss:  0.3984076678752899
train gradient:  0.39605853003398506
iteration : 1534
train acc:  0.8515625
train loss:  0.3670027256011963
train gradient:  0.34696564731460233
iteration : 1535
train acc:  0.8359375
train loss:  0.41841739416122437
train gradient:  0.5588021793620548
iteration : 1536
train acc:  0.8046875
train loss:  0.43133825063705444
train gradient:  0.4773926407517535
iteration : 1537
train acc:  0.7890625
train loss:  0.46893298625946045
train gradient:  0.6492375261305655
iteration : 1538
train acc:  0.8359375
train loss:  0.4090258777141571
train gradient:  0.8066419818647346
iteration : 1539
train acc:  0.8359375
train loss:  0.38033026456832886
train gradient:  0.4793767773491865
iteration : 1540
train acc:  0.8984375
train loss:  0.28979578614234924
train gradient:  0.3006404693520361
iteration : 1541
train acc:  0.7890625
train loss:  0.40875548124313354
train gradient:  0.48051379783720205
iteration : 1542
train acc:  0.78125
train loss:  0.4247003197669983
train gradient:  0.6643518692669699
iteration : 1543
train acc:  0.765625
train loss:  0.43054598569869995
train gradient:  0.4756556773460541
iteration : 1544
train acc:  0.8125
train loss:  0.4389742612838745
train gradient:  0.43823009544453223
iteration : 1545
train acc:  0.71875
train loss:  0.46266287565231323
train gradient:  0.8099089369575472
iteration : 1546
train acc:  0.8125
train loss:  0.4195020794868469
train gradient:  0.4803460420790267
iteration : 1547
train acc:  0.8515625
train loss:  0.4205858111381531
train gradient:  0.42581757436605433
iteration : 1548
train acc:  0.8125
train loss:  0.39219093322753906
train gradient:  0.6741616129683918
iteration : 1549
train acc:  0.8359375
train loss:  0.38807934522628784
train gradient:  0.4675120141531984
iteration : 1550
train acc:  0.828125
train loss:  0.39453020691871643
train gradient:  0.5170352601112229
iteration : 1551
train acc:  0.7578125
train loss:  0.5636832118034363
train gradient:  1.208518752526157
iteration : 1552
train acc:  0.8359375
train loss:  0.4062528610229492
train gradient:  0.4572955297443875
iteration : 1553
train acc:  0.7734375
train loss:  0.45325344800949097
train gradient:  0.47938083580006613
iteration : 1554
train acc:  0.84375
train loss:  0.3753395676612854
train gradient:  0.45901457214429836
iteration : 1555
train acc:  0.828125
train loss:  0.3661102056503296
train gradient:  0.3920023514039622
iteration : 1556
train acc:  0.8203125
train loss:  0.4146411418914795
train gradient:  0.6459608549425655
iteration : 1557
train acc:  0.765625
train loss:  0.45154786109924316
train gradient:  0.4384886816527795
iteration : 1558
train acc:  0.84375
train loss:  0.3768814206123352
train gradient:  0.4428994477907655
iteration : 1559
train acc:  0.859375
train loss:  0.3179068863391876
train gradient:  0.24253100172070508
iteration : 1560
train acc:  0.8046875
train loss:  0.4319193959236145
train gradient:  0.38336158965483813
iteration : 1561
train acc:  0.796875
train loss:  0.4415583312511444
train gradient:  0.4582721171953662
iteration : 1562
train acc:  0.828125
train loss:  0.3902261257171631
train gradient:  0.36801564138440057
iteration : 1563
train acc:  0.828125
train loss:  0.39830565452575684
train gradient:  0.5131329450681417
iteration : 1564
train acc:  0.8203125
train loss:  0.40767350792884827
train gradient:  0.43601407085284827
iteration : 1565
train acc:  0.796875
train loss:  0.4965246617794037
train gradient:  0.5731300020100984
iteration : 1566
train acc:  0.71875
train loss:  0.5168951749801636
train gradient:  0.7751392349097814
iteration : 1567
train acc:  0.8125
train loss:  0.3609682321548462
train gradient:  0.43172592664399195
iteration : 1568
train acc:  0.7421875
train loss:  0.5215129256248474
train gradient:  0.7497979245244638
iteration : 1569
train acc:  0.8125
train loss:  0.36136314272880554
train gradient:  0.4334043207953375
iteration : 1570
train acc:  0.7890625
train loss:  0.42877620458602905
train gradient:  0.4565577935934833
iteration : 1571
train acc:  0.8359375
train loss:  0.38210830092430115
train gradient:  0.43113804502335723
iteration : 1572
train acc:  0.78125
train loss:  0.4038460850715637
train gradient:  0.48079119655910546
iteration : 1573
train acc:  0.78125
train loss:  0.4250377118587494
train gradient:  0.604284008508918
iteration : 1574
train acc:  0.75
train loss:  0.4341852068901062
train gradient:  0.36387658944227375
iteration : 1575
train acc:  0.875
train loss:  0.33747872710227966
train gradient:  0.42568158340236795
iteration : 1576
train acc:  0.7890625
train loss:  0.4494350850582123
train gradient:  0.6360198815331846
iteration : 1577
train acc:  0.7734375
train loss:  0.4794040024280548
train gradient:  0.6238555023883696
iteration : 1578
train acc:  0.8203125
train loss:  0.4039928913116455
train gradient:  0.44797902347258745
iteration : 1579
train acc:  0.8359375
train loss:  0.3822559118270874
train gradient:  0.5467874783751563
iteration : 1580
train acc:  0.7578125
train loss:  0.45937639474868774
train gradient:  0.5642897157127178
iteration : 1581
train acc:  0.796875
train loss:  0.42409011721611023
train gradient:  0.5904604382259857
iteration : 1582
train acc:  0.828125
train loss:  0.3894212245941162
train gradient:  0.39272976073322985
iteration : 1583
train acc:  0.796875
train loss:  0.42544126510620117
train gradient:  0.6465288439074879
iteration : 1584
train acc:  0.765625
train loss:  0.5011631846427917
train gradient:  0.5357962010957122
iteration : 1585
train acc:  0.8125
train loss:  0.4740695655345917
train gradient:  0.658175699661961
iteration : 1586
train acc:  0.828125
train loss:  0.40794458985328674
train gradient:  0.4899933188723885
iteration : 1587
train acc:  0.8359375
train loss:  0.4122779667377472
train gradient:  0.44687991971149066
iteration : 1588
train acc:  0.828125
train loss:  0.3784360885620117
train gradient:  0.33507994296497373
iteration : 1589
train acc:  0.8359375
train loss:  0.36234813928604126
train gradient:  0.39268616625105895
iteration : 1590
train acc:  0.796875
train loss:  0.44429725408554077
train gradient:  0.5739704121232077
iteration : 1591
train acc:  0.734375
train loss:  0.49348071217536926
train gradient:  0.5433899732536004
iteration : 1592
train acc:  0.8046875
train loss:  0.3969016969203949
train gradient:  0.387386076287456
iteration : 1593
train acc:  0.7890625
train loss:  0.44760578870773315
train gradient:  0.4784819270137183
iteration : 1594
train acc:  0.7734375
train loss:  0.4137820601463318
train gradient:  0.45432898898266544
iteration : 1595
train acc:  0.796875
train loss:  0.44350960850715637
train gradient:  0.45465913205936953
iteration : 1596
train acc:  0.8515625
train loss:  0.36679214239120483
train gradient:  0.4424746438780603
iteration : 1597
train acc:  0.8125
train loss:  0.4261026680469513
train gradient:  0.37896078242801506
iteration : 1598
train acc:  0.7578125
train loss:  0.44424939155578613
train gradient:  0.49999250101724074
iteration : 1599
train acc:  0.7734375
train loss:  0.4428045153617859
train gradient:  0.5124051999443469
iteration : 1600
train acc:  0.875
train loss:  0.31198304891586304
train gradient:  0.282160547413764
iteration : 1601
train acc:  0.765625
train loss:  0.47136881947517395
train gradient:  0.6612123442915243
iteration : 1602
train acc:  0.859375
train loss:  0.35615575313568115
train gradient:  0.3245788449792313
iteration : 1603
train acc:  0.7578125
train loss:  0.43518972396850586
train gradient:  0.4757090900689578
iteration : 1604
train acc:  0.8203125
train loss:  0.424729585647583
train gradient:  0.3828617353007163
iteration : 1605
train acc:  0.8203125
train loss:  0.39645159244537354
train gradient:  0.4045712478372615
iteration : 1606
train acc:  0.84375
train loss:  0.4006624221801758
train gradient:  0.3275835227898674
iteration : 1607
train acc:  0.78125
train loss:  0.4692215323448181
train gradient:  0.5443371002744444
iteration : 1608
train acc:  0.828125
train loss:  0.42350083589553833
train gradient:  0.48448429881364674
iteration : 1609
train acc:  0.75
train loss:  0.5086046457290649
train gradient:  0.6019977427993158
iteration : 1610
train acc:  0.875
train loss:  0.3360396921634674
train gradient:  0.35338034149283465
iteration : 1611
train acc:  0.796875
train loss:  0.43196314573287964
train gradient:  0.38611579535892976
iteration : 1612
train acc:  0.8125
train loss:  0.4265170395374298
train gradient:  0.5415054211556407
iteration : 1613
train acc:  0.75
train loss:  0.4733092188835144
train gradient:  0.6122669912948935
iteration : 1614
train acc:  0.8203125
train loss:  0.4422834515571594
train gradient:  0.5651214731848897
iteration : 1615
train acc:  0.78125
train loss:  0.4648779034614563
train gradient:  0.6517048569927021
iteration : 1616
train acc:  0.796875
train loss:  0.4015713930130005
train gradient:  0.3713378644627457
iteration : 1617
train acc:  0.8203125
train loss:  0.3937568962574005
train gradient:  0.5284825426510096
iteration : 1618
train acc:  0.875
train loss:  0.35392048954963684
train gradient:  0.3345270344776513
iteration : 1619
train acc:  0.78125
train loss:  0.4419247508049011
train gradient:  0.5895797632279397
iteration : 1620
train acc:  0.7734375
train loss:  0.4963609278202057
train gradient:  0.5365291814450195
iteration : 1621
train acc:  0.7890625
train loss:  0.405564546585083
train gradient:  0.4117532984905723
iteration : 1622
train acc:  0.7734375
train loss:  0.4332566261291504
train gradient:  0.556666849422485
iteration : 1623
train acc:  0.7890625
train loss:  0.4428236782550812
train gradient:  0.4716114569406325
iteration : 1624
train acc:  0.8046875
train loss:  0.4036376476287842
train gradient:  0.30160618012130885
iteration : 1625
train acc:  0.7734375
train loss:  0.44423165917396545
train gradient:  0.4080010117410526
iteration : 1626
train acc:  0.875
train loss:  0.35949236154556274
train gradient:  0.34125624528170817
iteration : 1627
train acc:  0.8046875
train loss:  0.5047812461853027
train gradient:  0.5006661229376861
iteration : 1628
train acc:  0.8515625
train loss:  0.3624420166015625
train gradient:  0.4455751350303129
iteration : 1629
train acc:  0.7578125
train loss:  0.4522443413734436
train gradient:  0.4306021599334981
iteration : 1630
train acc:  0.75
train loss:  0.5379953384399414
train gradient:  0.6996478805151843
iteration : 1631
train acc:  0.796875
train loss:  0.4424985647201538
train gradient:  0.411017575286301
iteration : 1632
train acc:  0.84375
train loss:  0.41634929180145264
train gradient:  0.41306215888401815
iteration : 1633
train acc:  0.765625
train loss:  0.41415125131607056
train gradient:  0.47484449307043997
iteration : 1634
train acc:  0.7890625
train loss:  0.4206051528453827
train gradient:  0.5174580939916997
iteration : 1635
train acc:  0.765625
train loss:  0.520767331123352
train gradient:  0.7007115524375876
iteration : 1636
train acc:  0.7890625
train loss:  0.4188348054885864
train gradient:  0.7548178809330897
iteration : 1637
train acc:  0.7265625
train loss:  0.475705623626709
train gradient:  0.44498558390399323
iteration : 1638
train acc:  0.8046875
train loss:  0.44910991191864014
train gradient:  0.5717131428218197
iteration : 1639
train acc:  0.7421875
train loss:  0.49173063039779663
train gradient:  0.5345539940029967
iteration : 1640
train acc:  0.765625
train loss:  0.48825520277023315
train gradient:  0.46484061459820775
iteration : 1641
train acc:  0.8046875
train loss:  0.4439741373062134
train gradient:  0.41713979156665953
iteration : 1642
train acc:  0.8515625
train loss:  0.36227715015411377
train gradient:  0.28881123423104704
iteration : 1643
train acc:  0.8203125
train loss:  0.4107063412666321
train gradient:  0.4171357627056969
iteration : 1644
train acc:  0.8125
train loss:  0.4914793372154236
train gradient:  0.4623346617042966
iteration : 1645
train acc:  0.8203125
train loss:  0.38036489486694336
train gradient:  0.2687661968714348
iteration : 1646
train acc:  0.8203125
train loss:  0.3684672713279724
train gradient:  0.238239532797805
iteration : 1647
train acc:  0.7734375
train loss:  0.4743805229663849
train gradient:  0.5052696231122766
iteration : 1648
train acc:  0.8671875
train loss:  0.3493908941745758
train gradient:  0.24641995413105566
iteration : 1649
train acc:  0.78125
train loss:  0.48439866304397583
train gradient:  0.5046322175293476
iteration : 1650
train acc:  0.84375
train loss:  0.37775373458862305
train gradient:  0.2242909291791521
iteration : 1651
train acc:  0.828125
train loss:  0.3916081190109253
train gradient:  0.31322456221215184
iteration : 1652
train acc:  0.7734375
train loss:  0.4294421374797821
train gradient:  0.4223982374953619
iteration : 1653
train acc:  0.796875
train loss:  0.4052910804748535
train gradient:  0.3002909739871689
iteration : 1654
train acc:  0.84375
train loss:  0.43525925278663635
train gradient:  0.5127555241765928
iteration : 1655
train acc:  0.828125
train loss:  0.4177568554878235
train gradient:  0.31577620649862614
iteration : 1656
train acc:  0.7890625
train loss:  0.4942016005516052
train gradient:  0.62856120702834
iteration : 1657
train acc:  0.796875
train loss:  0.43233245611190796
train gradient:  0.3416557615828997
iteration : 1658
train acc:  0.8046875
train loss:  0.42980626225471497
train gradient:  0.36736835185014066
iteration : 1659
train acc:  0.78125
train loss:  0.45293301343917847
train gradient:  0.43925986772147196
iteration : 1660
train acc:  0.8046875
train loss:  0.4099915623664856
train gradient:  0.41593369867701546
iteration : 1661
train acc:  0.84375
train loss:  0.3955020308494568
train gradient:  0.39168339153625736
iteration : 1662
train acc:  0.8203125
train loss:  0.4381830394268036
train gradient:  0.3979303486741763
iteration : 1663
train acc:  0.8359375
train loss:  0.3798919916152954
train gradient:  0.3606221868076329
iteration : 1664
train acc:  0.78125
train loss:  0.4730430841445923
train gradient:  0.5307920364282162
iteration : 1665
train acc:  0.7578125
train loss:  0.4468984007835388
train gradient:  0.4089114702863847
iteration : 1666
train acc:  0.765625
train loss:  0.47353366017341614
train gradient:  0.49100741503273054
iteration : 1667
train acc:  0.859375
train loss:  0.40201255679130554
train gradient:  0.4151823718514851
iteration : 1668
train acc:  0.796875
train loss:  0.3932279348373413
train gradient:  0.27416018465296316
iteration : 1669
train acc:  0.7421875
train loss:  0.5055515170097351
train gradient:  0.6957224397584413
iteration : 1670
train acc:  0.875
train loss:  0.3654048442840576
train gradient:  0.3496686539140238
iteration : 1671
train acc:  0.8125
train loss:  0.3836015462875366
train gradient:  0.3054691956977064
iteration : 1672
train acc:  0.796875
train loss:  0.4407223165035248
train gradient:  0.4042019320772474
iteration : 1673
train acc:  0.765625
train loss:  0.4179055690765381
train gradient:  0.39304643418378876
iteration : 1674
train acc:  0.7890625
train loss:  0.44949451088905334
train gradient:  0.48178741697508504
iteration : 1675
train acc:  0.7890625
train loss:  0.4267445504665375
train gradient:  0.5765839101785335
iteration : 1676
train acc:  0.7578125
train loss:  0.46570223569869995
train gradient:  0.731813479628024
iteration : 1677
train acc:  0.8125
train loss:  0.4128456711769104
train gradient:  0.4459670993763987
iteration : 1678
train acc:  0.7734375
train loss:  0.5050036907196045
train gradient:  0.49889927503846787
iteration : 1679
train acc:  0.828125
train loss:  0.36114615201950073
train gradient:  0.3914025093050585
iteration : 1680
train acc:  0.8515625
train loss:  0.3896711468696594
train gradient:  0.42853172545044116
iteration : 1681
train acc:  0.8046875
train loss:  0.4487517178058624
train gradient:  0.34209750438084185
iteration : 1682
train acc:  0.796875
train loss:  0.39974647760391235
train gradient:  0.445423077467723
iteration : 1683
train acc:  0.8671875
train loss:  0.33434081077575684
train gradient:  0.30895810749345576
iteration : 1684
train acc:  0.765625
train loss:  0.45374220609664917
train gradient:  0.5269028954659528
iteration : 1685
train acc:  0.7734375
train loss:  0.48622995615005493
train gradient:  0.43638760022808104
iteration : 1686
train acc:  0.8515625
train loss:  0.411988228559494
train gradient:  0.49459081066790833
iteration : 1687
train acc:  0.8046875
train loss:  0.4755089282989502
train gradient:  0.7737446201864395
iteration : 1688
train acc:  0.7421875
train loss:  0.4542657732963562
train gradient:  0.5947662269330357
iteration : 1689
train acc:  0.8046875
train loss:  0.5095335245132446
train gradient:  0.5522282929744509
iteration : 1690
train acc:  0.8046875
train loss:  0.4045713245868683
train gradient:  0.43164962945147417
iteration : 1691
train acc:  0.8125
train loss:  0.4093615412712097
train gradient:  0.3671557775391079
iteration : 1692
train acc:  0.8359375
train loss:  0.38133731484413147
train gradient:  0.45317206549046624
iteration : 1693
train acc:  0.7421875
train loss:  0.4593644440174103
train gradient:  0.5012288044309241
iteration : 1694
train acc:  0.8671875
train loss:  0.3427068293094635
train gradient:  0.2525897463824161
iteration : 1695
train acc:  0.7890625
train loss:  0.4631531536579132
train gradient:  0.4995036009864858
iteration : 1696
train acc:  0.7890625
train loss:  0.4081168472766876
train gradient:  0.3770314095971954
iteration : 1697
train acc:  0.8203125
train loss:  0.409699022769928
train gradient:  0.3391492050536727
iteration : 1698
train acc:  0.8828125
train loss:  0.3361054062843323
train gradient:  0.26694381615723606
iteration : 1699
train acc:  0.78125
train loss:  0.4201423227787018
train gradient:  0.4398046681173343
iteration : 1700
train acc:  0.796875
train loss:  0.40849897265434265
train gradient:  0.3670120897163369
iteration : 1701
train acc:  0.796875
train loss:  0.46426987648010254
train gradient:  0.3825441926889091
iteration : 1702
train acc:  0.859375
train loss:  0.354465514421463
train gradient:  0.3701521729840683
iteration : 1703
train acc:  0.84375
train loss:  0.35336342453956604
train gradient:  0.463600063974774
iteration : 1704
train acc:  0.75
train loss:  0.4609672725200653
train gradient:  0.46347188217720414
iteration : 1705
train acc:  0.859375
train loss:  0.3673524558544159
train gradient:  0.4029283354042758
iteration : 1706
train acc:  0.8984375
train loss:  0.37523025274276733
train gradient:  0.39422425517425674
iteration : 1707
train acc:  0.75
train loss:  0.4313889145851135
train gradient:  0.4386041974411119
iteration : 1708
train acc:  0.7734375
train loss:  0.4787697494029999
train gradient:  0.5531143775970266
iteration : 1709
train acc:  0.78125
train loss:  0.4485444128513336
train gradient:  0.7272015317164362
iteration : 1710
train acc:  0.7734375
train loss:  0.4561837911605835
train gradient:  0.598557326961169
iteration : 1711
train acc:  0.8046875
train loss:  0.4228639304637909
train gradient:  0.46294081244281815
iteration : 1712
train acc:  0.8125
train loss:  0.3937680125236511
train gradient:  0.40696989084452817
iteration : 1713
train acc:  0.8203125
train loss:  0.4092918038368225
train gradient:  0.3858690949270554
iteration : 1714
train acc:  0.8359375
train loss:  0.36587822437286377
train gradient:  0.3807352331554988
iteration : 1715
train acc:  0.8203125
train loss:  0.3940734565258026
train gradient:  0.43542600744819704
iteration : 1716
train acc:  0.765625
train loss:  0.5665102005004883
train gradient:  0.8860572442522271
iteration : 1717
train acc:  0.8203125
train loss:  0.40794405341148376
train gradient:  0.5326753310076813
iteration : 1718
train acc:  0.78125
train loss:  0.451534628868103
train gradient:  0.3953139770819631
iteration : 1719
train acc:  0.78125
train loss:  0.3855030834674835
train gradient:  0.48188619259016635
iteration : 1720
train acc:  0.84375
train loss:  0.36560818552970886
train gradient:  0.3487024542636198
iteration : 1721
train acc:  0.796875
train loss:  0.48481452465057373
train gradient:  0.6113154196022548
iteration : 1722
train acc:  0.828125
train loss:  0.37135422229766846
train gradient:  0.4302246591248816
iteration : 1723
train acc:  0.734375
train loss:  0.49009519815444946
train gradient:  0.5113692961496028
iteration : 1724
train acc:  0.7890625
train loss:  0.40217137336730957
train gradient:  0.4441891277896926
iteration : 1725
train acc:  0.8515625
train loss:  0.3686367869377136
train gradient:  0.4357107407830003
iteration : 1726
train acc:  0.7890625
train loss:  0.43075263500213623
train gradient:  0.5287882238587553
iteration : 1727
train acc:  0.90625
train loss:  0.30466529726982117
train gradient:  0.31620730235341016
iteration : 1728
train acc:  0.8203125
train loss:  0.377588152885437
train gradient:  0.35876754701278096
iteration : 1729
train acc:  0.828125
train loss:  0.396850049495697
train gradient:  0.5685906189456004
iteration : 1730
train acc:  0.828125
train loss:  0.4300651550292969
train gradient:  0.4820652319837421
iteration : 1731
train acc:  0.8203125
train loss:  0.4184795022010803
train gradient:  0.40961307562497407
iteration : 1732
train acc:  0.8359375
train loss:  0.37009531259536743
train gradient:  0.3171357074259834
iteration : 1733
train acc:  0.828125
train loss:  0.3925342559814453
train gradient:  0.3092406728233628
iteration : 1734
train acc:  0.8203125
train loss:  0.3860147297382355
train gradient:  0.3110474630149585
iteration : 1735
train acc:  0.7421875
train loss:  0.4864290654659271
train gradient:  0.44920422342427563
iteration : 1736
train acc:  0.8828125
train loss:  0.3635633885860443
train gradient:  0.34883867249136846
iteration : 1737
train acc:  0.75
train loss:  0.47273802757263184
train gradient:  0.6112198427046609
iteration : 1738
train acc:  0.8984375
train loss:  0.29335278272628784
train gradient:  0.2823431035720821
iteration : 1739
train acc:  0.78125
train loss:  0.4708521366119385
train gradient:  0.48963937153879394
iteration : 1740
train acc:  0.828125
train loss:  0.3707889914512634
train gradient:  0.297061037320208
iteration : 1741
train acc:  0.796875
train loss:  0.5219099521636963
train gradient:  0.5193680539184673
iteration : 1742
train acc:  0.7890625
train loss:  0.43288546800613403
train gradient:  0.31502895866659486
iteration : 1743
train acc:  0.859375
train loss:  0.3545161485671997
train gradient:  0.298504143221576
iteration : 1744
train acc:  0.875
train loss:  0.27361011505126953
train gradient:  0.21961464891585836
iteration : 1745
train acc:  0.78125
train loss:  0.49835288524627686
train gradient:  0.49473258007186927
iteration : 1746
train acc:  0.8125
train loss:  0.43730586767196655
train gradient:  0.4368004343706342
iteration : 1747
train acc:  0.765625
train loss:  0.49186426401138306
train gradient:  0.4583311643947283
iteration : 1748
train acc:  0.8984375
train loss:  0.3134053349494934
train gradient:  0.2706328417979777
iteration : 1749
train acc:  0.828125
train loss:  0.4010090231895447
train gradient:  0.3874639745147935
iteration : 1750
train acc:  0.8359375
train loss:  0.38550427556037903
train gradient:  0.3774048386749301
iteration : 1751
train acc:  0.8515625
train loss:  0.3704848885536194
train gradient:  0.2662253988424747
iteration : 1752
train acc:  0.828125
train loss:  0.4473009705543518
train gradient:  0.41295721450834044
iteration : 1753
train acc:  0.796875
train loss:  0.432014524936676
train gradient:  0.39537548410353074
iteration : 1754
train acc:  0.8046875
train loss:  0.45515894889831543
train gradient:  0.5262700216336389
iteration : 1755
train acc:  0.8046875
train loss:  0.4340989291667938
train gradient:  0.40846531335080594
iteration : 1756
train acc:  0.796875
train loss:  0.40637487173080444
train gradient:  0.49906432407590295
iteration : 1757
train acc:  0.796875
train loss:  0.4979514479637146
train gradient:  0.657337901042655
iteration : 1758
train acc:  0.828125
train loss:  0.42692020535469055
train gradient:  0.3336237631817011
iteration : 1759
train acc:  0.78125
train loss:  0.45496004819869995
train gradient:  0.39535122526180566
iteration : 1760
train acc:  0.8359375
train loss:  0.38866865634918213
train gradient:  0.3265860184230839
iteration : 1761
train acc:  0.796875
train loss:  0.3765273094177246
train gradient:  0.2730954275153208
iteration : 1762
train acc:  0.7578125
train loss:  0.48906973004341125
train gradient:  0.38172639921001067
iteration : 1763
train acc:  0.8046875
train loss:  0.4441923201084137
train gradient:  0.47644595599347667
iteration : 1764
train acc:  0.828125
train loss:  0.4048973321914673
train gradient:  0.4739274750698007
iteration : 1765
train acc:  0.8046875
train loss:  0.4546584486961365
train gradient:  0.5340500367917655
iteration : 1766
train acc:  0.796875
train loss:  0.4272322654724121
train gradient:  0.44732867917133273
iteration : 1767
train acc:  0.8359375
train loss:  0.3721890449523926
train gradient:  0.25075493720381403
iteration : 1768
train acc:  0.78125
train loss:  0.46424445509910583
train gradient:  0.40396808035016063
iteration : 1769
train acc:  0.7421875
train loss:  0.4558573067188263
train gradient:  0.4854093149308619
iteration : 1770
train acc:  0.84375
train loss:  0.3369099497795105
train gradient:  0.25697869352824243
iteration : 1771
train acc:  0.8203125
train loss:  0.41972649097442627
train gradient:  0.34561811471700676
iteration : 1772
train acc:  0.84375
train loss:  0.39276015758514404
train gradient:  0.4450385585636514
iteration : 1773
train acc:  0.8671875
train loss:  0.34981679916381836
train gradient:  0.35680612842478765
iteration : 1774
train acc:  0.8515625
train loss:  0.38024190068244934
train gradient:  0.27669349714752883
iteration : 1775
train acc:  0.828125
train loss:  0.3633662462234497
train gradient:  0.2522072816610676
iteration : 1776
train acc:  0.8046875
train loss:  0.3959883153438568
train gradient:  0.3415773802570294
iteration : 1777
train acc:  0.796875
train loss:  0.45552974939346313
train gradient:  0.3191027049427259
iteration : 1778
train acc:  0.8125
train loss:  0.4341466724872589
train gradient:  0.3727670292154138
iteration : 1779
train acc:  0.7734375
train loss:  0.49367856979370117
train gradient:  0.5160163110293932
iteration : 1780
train acc:  0.828125
train loss:  0.43026643991470337
train gradient:  0.40887942591563087
iteration : 1781
train acc:  0.7890625
train loss:  0.4251542389392853
train gradient:  0.4142079017365806
iteration : 1782
train acc:  0.84375
train loss:  0.38340240716934204
train gradient:  0.34532182060196726
iteration : 1783
train acc:  0.8125
train loss:  0.42943787574768066
train gradient:  0.5741048667497896
iteration : 1784
train acc:  0.8203125
train loss:  0.38314560055732727
train gradient:  0.47824107700983376
iteration : 1785
train acc:  0.8203125
train loss:  0.4212201237678528
train gradient:  0.3145796947187983
iteration : 1786
train acc:  0.84375
train loss:  0.35783374309539795
train gradient:  0.3401922817488551
iteration : 1787
train acc:  0.78125
train loss:  0.39137741923332214
train gradient:  0.5098805945277018
iteration : 1788
train acc:  0.734375
train loss:  0.43799924850463867
train gradient:  0.4239809242011013
iteration : 1789
train acc:  0.8125
train loss:  0.3840984106063843
train gradient:  0.3092715895724422
iteration : 1790
train acc:  0.875
train loss:  0.35865727066993713
train gradient:  0.3526095776713723
iteration : 1791
train acc:  0.8203125
train loss:  0.4220571517944336
train gradient:  0.3707761625568462
iteration : 1792
train acc:  0.7265625
train loss:  0.4861376881599426
train gradient:  0.6256361690845488
iteration : 1793
train acc:  0.8125
train loss:  0.4434981942176819
train gradient:  0.34403282779097183
iteration : 1794
train acc:  0.8359375
train loss:  0.38844507932662964
train gradient:  0.4385513062336511
iteration : 1795
train acc:  0.84375
train loss:  0.3825300335884094
train gradient:  0.3782532024683929
iteration : 1796
train acc:  0.7578125
train loss:  0.4711720943450928
train gradient:  0.5292068303777144
iteration : 1797
train acc:  0.7734375
train loss:  0.49688756465911865
train gradient:  0.6021940823452048
iteration : 1798
train acc:  0.8359375
train loss:  0.40548425912857056
train gradient:  0.4023189677055611
iteration : 1799
train acc:  0.84375
train loss:  0.351104736328125
train gradient:  0.4101148095479717
iteration : 1800
train acc:  0.7734375
train loss:  0.4289901852607727
train gradient:  0.46965719821861096
iteration : 1801
train acc:  0.8125
train loss:  0.3786921203136444
train gradient:  0.3234497409599408
iteration : 1802
train acc:  0.7890625
train loss:  0.43097758293151855
train gradient:  0.7119091224118097
iteration : 1803
train acc:  0.8125
train loss:  0.43638917803764343
train gradient:  0.33855127640943045
iteration : 1804
train acc:  0.75
train loss:  0.49114957451820374
train gradient:  0.5803855349277472
iteration : 1805
train acc:  0.8671875
train loss:  0.3741883337497711
train gradient:  0.394794895096066
iteration : 1806
train acc:  0.84375
train loss:  0.39159470796585083
train gradient:  0.35817253138658217
iteration : 1807
train acc:  0.765625
train loss:  0.43568652868270874
train gradient:  0.416321893225505
iteration : 1808
train acc:  0.890625
train loss:  0.3662990927696228
train gradient:  0.27300267743565654
iteration : 1809
train acc:  0.703125
train loss:  0.5584237575531006
train gradient:  0.6668794342728235
iteration : 1810
train acc:  0.8046875
train loss:  0.40460264682769775
train gradient:  0.3750560174195365
iteration : 1811
train acc:  0.8359375
train loss:  0.41769397258758545
train gradient:  0.47300722396491185
iteration : 1812
train acc:  0.8359375
train loss:  0.40803200006484985
train gradient:  0.434033874966659
iteration : 1813
train acc:  0.828125
train loss:  0.3992040157318115
train gradient:  0.4511783658839092
iteration : 1814
train acc:  0.875
train loss:  0.3161003589630127
train gradient:  0.32708315473646216
iteration : 1815
train acc:  0.84375
train loss:  0.3751177191734314
train gradient:  0.3746276771082167
iteration : 1816
train acc:  0.859375
train loss:  0.3671859800815582
train gradient:  0.30075965245119396
iteration : 1817
train acc:  0.828125
train loss:  0.38044270873069763
train gradient:  0.2893614906040732
iteration : 1818
train acc:  0.7890625
train loss:  0.39417505264282227
train gradient:  0.332832545970294
iteration : 1819
train acc:  0.7734375
train loss:  0.48875588178634644
train gradient:  0.562050770923066
iteration : 1820
train acc:  0.7890625
train loss:  0.4445175528526306
train gradient:  0.4432482392485761
iteration : 1821
train acc:  0.8046875
train loss:  0.47580498456954956
train gradient:  0.49669382103313314
iteration : 1822
train acc:  0.90625
train loss:  0.2954656779766083
train gradient:  0.21538119478515866
iteration : 1823
train acc:  0.7890625
train loss:  0.4364352822303772
train gradient:  0.5337192753878881
iteration : 1824
train acc:  0.75
train loss:  0.5046204328536987
train gradient:  0.5965073082360298
iteration : 1825
train acc:  0.78125
train loss:  0.46626245975494385
train gradient:  2.872958354368754
iteration : 1826
train acc:  0.7890625
train loss:  0.45760443806648254
train gradient:  0.5233196521741307
iteration : 1827
train acc:  0.78125
train loss:  0.45778244733810425
train gradient:  0.4957401875961767
iteration : 1828
train acc:  0.8515625
train loss:  0.33813849091529846
train gradient:  0.2547016196625737
iteration : 1829
train acc:  0.8046875
train loss:  0.4401799440383911
train gradient:  0.4740365381215405
iteration : 1830
train acc:  0.8359375
train loss:  0.3779715895652771
train gradient:  0.4209150353048906
iteration : 1831
train acc:  0.84375
train loss:  0.37085670232772827
train gradient:  0.3312143749659939
iteration : 1832
train acc:  0.828125
train loss:  0.4022487998008728
train gradient:  0.4065919421833609
iteration : 1833
train acc:  0.8203125
train loss:  0.3775748610496521
train gradient:  0.3416662428735998
iteration : 1834
train acc:  0.875
train loss:  0.32198795676231384
train gradient:  0.24226821681528227
iteration : 1835
train acc:  0.796875
train loss:  0.44140589237213135
train gradient:  0.4223825859493087
iteration : 1836
train acc:  0.8125
train loss:  0.4144391715526581
train gradient:  0.38762433045896943
iteration : 1837
train acc:  0.828125
train loss:  0.42004191875457764
train gradient:  0.5266548997170374
iteration : 1838
train acc:  0.7578125
train loss:  0.45752307772636414
train gradient:  0.44597353993465855
iteration : 1839
train acc:  0.734375
train loss:  0.49170053005218506
train gradient:  0.5697674350834459
iteration : 1840
train acc:  0.8203125
train loss:  0.41192427277565
train gradient:  0.26549002838354196
iteration : 1841
train acc:  0.8125
train loss:  0.3871941566467285
train gradient:  0.41425992499927616
iteration : 1842
train acc:  0.7890625
train loss:  0.37987053394317627
train gradient:  0.4329947745829439
iteration : 1843
train acc:  0.859375
train loss:  0.32184508442878723
train gradient:  0.33436632927828924
iteration : 1844
train acc:  0.8125
train loss:  0.43155938386917114
train gradient:  0.38375420157164736
iteration : 1845
train acc:  0.8046875
train loss:  0.4345053732395172
train gradient:  0.46331005022661736
iteration : 1846
train acc:  0.8671875
train loss:  0.33226829767227173
train gradient:  0.28736479946938126
iteration : 1847
train acc:  0.8125
train loss:  0.39596736431121826
train gradient:  0.49397825911516074
iteration : 1848
train acc:  0.8515625
train loss:  0.3683452010154724
train gradient:  0.29546519830024115
iteration : 1849
train acc:  0.8671875
train loss:  0.3364337086677551
train gradient:  0.2721848528944012
iteration : 1850
train acc:  0.875
train loss:  0.33326491713523865
train gradient:  0.2837744674001405
iteration : 1851
train acc:  0.8515625
train loss:  0.35943055152893066
train gradient:  0.3856746915756983
iteration : 1852
train acc:  0.7578125
train loss:  0.43719056248664856
train gradient:  0.4710715154257673
iteration : 1853
train acc:  0.7578125
train loss:  0.5135772824287415
train gradient:  0.45831299732258735
iteration : 1854
train acc:  0.859375
train loss:  0.36563748121261597
train gradient:  0.43699070434676424
iteration : 1855
train acc:  0.8125
train loss:  0.3624200224876404
train gradient:  0.4851888501226285
iteration : 1856
train acc:  0.7890625
train loss:  0.4558265209197998
train gradient:  0.581195190018873
iteration : 1857
train acc:  0.8515625
train loss:  0.404691219329834
train gradient:  0.3344109283997962
iteration : 1858
train acc:  0.875
train loss:  0.35777533054351807
train gradient:  0.41796244886438666
iteration : 1859
train acc:  0.890625
train loss:  0.3141178488731384
train gradient:  0.3557172234742228
iteration : 1860
train acc:  0.8125
train loss:  0.4217444360256195
train gradient:  0.565167408273095
iteration : 1861
train acc:  0.7890625
train loss:  0.4512181878089905
train gradient:  0.47170067224610496
iteration : 1862
train acc:  0.828125
train loss:  0.39438217878341675
train gradient:  0.3040463322650044
iteration : 1863
train acc:  0.78125
train loss:  0.4271295666694641
train gradient:  0.5842734324829708
iteration : 1864
train acc:  0.84375
train loss:  0.36559993028640747
train gradient:  0.5364645500329046
iteration : 1865
train acc:  0.859375
train loss:  0.35814404487609863
train gradient:  0.34001232991636354
iteration : 1866
train acc:  0.7890625
train loss:  0.4490331709384918
train gradient:  0.5044889765291294
iteration : 1867
train acc:  0.7890625
train loss:  0.4502408504486084
train gradient:  0.5301931453848034
iteration : 1868
train acc:  0.7734375
train loss:  0.39028358459472656
train gradient:  0.4969040124222901
iteration : 1869
train acc:  0.8125
train loss:  0.41550469398498535
train gradient:  0.37105968761814373
iteration : 1870
train acc:  0.7421875
train loss:  0.48038843274116516
train gradient:  0.5408109128212684
iteration : 1871
train acc:  0.78125
train loss:  0.3810538649559021
train gradient:  0.553349370306183
iteration : 1872
train acc:  0.796875
train loss:  0.4301581382751465
train gradient:  0.5032005665932432
iteration : 1873
train acc:  0.8359375
train loss:  0.39184343814849854
train gradient:  0.3328646768830099
iteration : 1874
train acc:  0.7734375
train loss:  0.46190911531448364
train gradient:  0.5733210432734638
iteration : 1875
train acc:  0.7890625
train loss:  0.46204960346221924
train gradient:  0.4668074990677053
iteration : 1876
train acc:  0.828125
train loss:  0.416856050491333
train gradient:  0.3267207365393523
iteration : 1877
train acc:  0.7734375
train loss:  0.4804573655128479
train gradient:  0.6221738867480312
iteration : 1878
train acc:  0.765625
train loss:  0.4521852433681488
train gradient:  0.5161559994528058
iteration : 1879
train acc:  0.796875
train loss:  0.3870031237602234
train gradient:  0.23845364403252642
iteration : 1880
train acc:  0.796875
train loss:  0.3903299570083618
train gradient:  0.4056558186281393
iteration : 1881
train acc:  0.828125
train loss:  0.4336780905723572
train gradient:  0.5211409293645182
iteration : 1882
train acc:  0.8203125
train loss:  0.4576679766178131
train gradient:  0.6403132680224428
iteration : 1883
train acc:  0.765625
train loss:  0.4716145396232605
train gradient:  0.5707917388432467
iteration : 1884
train acc:  0.8046875
train loss:  0.3984740674495697
train gradient:  0.473549802616697
iteration : 1885
train acc:  0.75
train loss:  0.3995175361633301
train gradient:  0.4528345449172626
iteration : 1886
train acc:  0.859375
train loss:  0.3243977725505829
train gradient:  0.40651839277788787
iteration : 1887
train acc:  0.828125
train loss:  0.37906867265701294
train gradient:  0.332776813617152
iteration : 1888
train acc:  0.7578125
train loss:  0.4958938658237457
train gradient:  0.5686431993561198
iteration : 1889
train acc:  0.796875
train loss:  0.4166533350944519
train gradient:  0.4778888172837697
iteration : 1890
train acc:  0.8203125
train loss:  0.4855225384235382
train gradient:  0.6052400383677966
iteration : 1891
train acc:  0.765625
train loss:  0.4757121205329895
train gradient:  0.7352745602904756
iteration : 1892
train acc:  0.75
train loss:  0.5123523473739624
train gradient:  0.7123095654382394
iteration : 1893
train acc:  0.8203125
train loss:  0.3961736261844635
train gradient:  0.30247807260880577
iteration : 1894
train acc:  0.84375
train loss:  0.35881507396698
train gradient:  0.3476143219427688
iteration : 1895
train acc:  0.828125
train loss:  0.4015483260154724
train gradient:  0.40118310865488827
iteration : 1896
train acc:  0.75
train loss:  0.48058462142944336
train gradient:  0.47804624914659816
iteration : 1897
train acc:  0.828125
train loss:  0.35074517130851746
train gradient:  0.25316692924493917
iteration : 1898
train acc:  0.7734375
train loss:  0.43680065870285034
train gradient:  0.42114927069078506
iteration : 1899
train acc:  0.84375
train loss:  0.4022826552391052
train gradient:  0.4247202865119738
iteration : 1900
train acc:  0.8203125
train loss:  0.42333564162254333
train gradient:  0.39176881221985
iteration : 1901
train acc:  0.78125
train loss:  0.4574945569038391
train gradient:  0.3832186570123444
iteration : 1902
train acc:  0.8671875
train loss:  0.3323768973350525
train gradient:  0.2055756599853732
iteration : 1903
train acc:  0.8515625
train loss:  0.35275784134864807
train gradient:  0.3069686848109958
iteration : 1904
train acc:  0.7265625
train loss:  0.5310376882553101
train gradient:  0.614635672309746
iteration : 1905
train acc:  0.8359375
train loss:  0.38539624214172363
train gradient:  0.3937951400739652
iteration : 1906
train acc:  0.7890625
train loss:  0.46085408329963684
train gradient:  0.37929241037195066
iteration : 1907
train acc:  0.828125
train loss:  0.38018977642059326
train gradient:  0.29046092082914826
iteration : 1908
train acc:  0.8203125
train loss:  0.4020681381225586
train gradient:  0.425492521206943
iteration : 1909
train acc:  0.8046875
train loss:  0.43497133255004883
train gradient:  0.4205954123280898
iteration : 1910
train acc:  0.8515625
train loss:  0.3865819275379181
train gradient:  0.37194282830120856
iteration : 1911
train acc:  0.8046875
train loss:  0.39495494961738586
train gradient:  0.39164633867480786
iteration : 1912
train acc:  0.796875
train loss:  0.45621457695961
train gradient:  0.6095812843192672
iteration : 1913
train acc:  0.8203125
train loss:  0.4164677560329437
train gradient:  0.363112602875271
iteration : 1914
train acc:  0.8125
train loss:  0.38611283898353577
train gradient:  0.3165314818680657
iteration : 1915
train acc:  0.8515625
train loss:  0.35849064588546753
train gradient:  0.2917708085729841
iteration : 1916
train acc:  0.8203125
train loss:  0.37783512473106384
train gradient:  0.4371799649015474
iteration : 1917
train acc:  0.84375
train loss:  0.35521870851516724
train gradient:  0.33664994072031024
iteration : 1918
train acc:  0.8203125
train loss:  0.39815953373908997
train gradient:  0.45413785686326885
iteration : 1919
train acc:  0.765625
train loss:  0.5038856267929077
train gradient:  0.5634722244364508
iteration : 1920
train acc:  0.7578125
train loss:  0.4718332290649414
train gradient:  0.4760808079205689
iteration : 1921
train acc:  0.8046875
train loss:  0.4438677430152893
train gradient:  0.27001119369883303
iteration : 1922
train acc:  0.7421875
train loss:  0.5461020469665527
train gradient:  0.7270183294837208
iteration : 1923
train acc:  0.7734375
train loss:  0.449767142534256
train gradient:  0.3692627781860352
iteration : 1924
train acc:  0.8203125
train loss:  0.3957822322845459
train gradient:  0.31981323169026993
iteration : 1925
train acc:  0.828125
train loss:  0.3773711621761322
train gradient:  0.44655956459424007
iteration : 1926
train acc:  0.8125
train loss:  0.42550337314605713
train gradient:  0.4868344917240629
iteration : 1927
train acc:  0.8046875
train loss:  0.3885924816131592
train gradient:  0.34503509381627206
iteration : 1928
train acc:  0.796875
train loss:  0.4393737316131592
train gradient:  0.3820889034598142
iteration : 1929
train acc:  0.78125
train loss:  0.39028841257095337
train gradient:  0.272762283166425
iteration : 1930
train acc:  0.796875
train loss:  0.42038583755493164
train gradient:  0.35558090659836
iteration : 1931
train acc:  0.8125
train loss:  0.4634758532047272
train gradient:  0.3847889830760267
iteration : 1932
train acc:  0.8203125
train loss:  0.41391927003860474
train gradient:  0.3643714173163318
iteration : 1933
train acc:  0.765625
train loss:  0.5114513039588928
train gradient:  0.5758218095797177
iteration : 1934
train acc:  0.7890625
train loss:  0.4111717939376831
train gradient:  0.3094052864699789
iteration : 1935
train acc:  0.7890625
train loss:  0.4195224344730377
train gradient:  0.31913822454770713
iteration : 1936
train acc:  0.8203125
train loss:  0.3954697847366333
train gradient:  0.38065553441703875
iteration : 1937
train acc:  0.7890625
train loss:  0.4298046827316284
train gradient:  0.3704493441425269
iteration : 1938
train acc:  0.859375
train loss:  0.34612926840782166
train gradient:  0.23369250124965885
iteration : 1939
train acc:  0.765625
train loss:  0.500029444694519
train gradient:  0.5167442283148838
iteration : 1940
train acc:  0.8359375
train loss:  0.3824612498283386
train gradient:  0.36057902124751257
iteration : 1941
train acc:  0.84375
train loss:  0.3644951581954956
train gradient:  0.2630486679202119
iteration : 1942
train acc:  0.796875
train loss:  0.40076571702957153
train gradient:  0.3583231947119577
iteration : 1943
train acc:  0.8046875
train loss:  0.40705186128616333
train gradient:  0.4557485024602457
iteration : 1944
train acc:  0.84375
train loss:  0.3735274374485016
train gradient:  0.27475038658136736
iteration : 1945
train acc:  0.8125
train loss:  0.38915133476257324
train gradient:  0.4174425286978959
iteration : 1946
train acc:  0.796875
train loss:  0.4511612057685852
train gradient:  0.8855386317407976
iteration : 1947
train acc:  0.78125
train loss:  0.44537946581840515
train gradient:  0.3415894744931052
iteration : 1948
train acc:  0.84375
train loss:  0.39338183403015137
train gradient:  0.33447613881523836
iteration : 1949
train acc:  0.78125
train loss:  0.4544418454170227
train gradient:  0.42454225555487746
iteration : 1950
train acc:  0.8203125
train loss:  0.37020236253738403
train gradient:  0.21408816998183136
iteration : 1951
train acc:  0.8046875
train loss:  0.40548768639564514
train gradient:  0.33130945712120574
iteration : 1952
train acc:  0.828125
train loss:  0.40200909972190857
train gradient:  0.344743071185293
iteration : 1953
train acc:  0.8203125
train loss:  0.37260526418685913
train gradient:  0.3831533743330147
iteration : 1954
train acc:  0.796875
train loss:  0.4317139685153961
train gradient:  0.5077558134260918
iteration : 1955
train acc:  0.8984375
train loss:  0.30760037899017334
train gradient:  0.25075826522056677
iteration : 1956
train acc:  0.84375
train loss:  0.4085029363632202
train gradient:  0.41186724151476356
iteration : 1957
train acc:  0.84375
train loss:  0.3739497661590576
train gradient:  0.26487305196001876
iteration : 1958
train acc:  0.859375
train loss:  0.35203635692596436
train gradient:  0.270276041057713
iteration : 1959
train acc:  0.7734375
train loss:  0.4638201594352722
train gradient:  0.5616190177177042
iteration : 1960
train acc:  0.828125
train loss:  0.36597940325737
train gradient:  0.2722427812532586
iteration : 1961
train acc:  0.859375
train loss:  0.4143504202365875
train gradient:  0.56380496504252
iteration : 1962
train acc:  0.8125
train loss:  0.44124636054039
train gradient:  0.5267252418541568
iteration : 1963
train acc:  0.8203125
train loss:  0.4285951852798462
train gradient:  0.4043464724569177
iteration : 1964
train acc:  0.8515625
train loss:  0.3430401682853699
train gradient:  0.36553240450781066
iteration : 1965
train acc:  0.8125
train loss:  0.3979315757751465
train gradient:  0.5544876888949718
iteration : 1966
train acc:  0.8359375
train loss:  0.3742220401763916
train gradient:  0.4229324508595481
iteration : 1967
train acc:  0.796875
train loss:  0.41604191064834595
train gradient:  0.46576495256440453
iteration : 1968
train acc:  0.7734375
train loss:  0.448650598526001
train gradient:  0.6262462103684758
iteration : 1969
train acc:  0.84375
train loss:  0.42970508337020874
train gradient:  0.4197469708262362
iteration : 1970
train acc:  0.8828125
train loss:  0.39720791578292847
train gradient:  0.4120100504501505
iteration : 1971
train acc:  0.8359375
train loss:  0.37211135029792786
train gradient:  0.4190739124164248
iteration : 1972
train acc:  0.7890625
train loss:  0.40454912185668945
train gradient:  0.4546517208070406
iteration : 1973
train acc:  0.8125
train loss:  0.4012675881385803
train gradient:  0.4217245599889201
iteration : 1974
train acc:  0.8203125
train loss:  0.426736056804657
train gradient:  0.6483131369028285
iteration : 1975
train acc:  0.78125
train loss:  0.414634644985199
train gradient:  0.46405365689514494
iteration : 1976
train acc:  0.875
train loss:  0.32758742570877075
train gradient:  0.22367402198770214
iteration : 1977
train acc:  0.765625
train loss:  0.45230990648269653
train gradient:  0.5333707879112242
iteration : 1978
train acc:  0.8203125
train loss:  0.43021610379219055
train gradient:  0.5433499089367266
iteration : 1979
train acc:  0.8125
train loss:  0.4129541218280792
train gradient:  0.35707871787880124
iteration : 1980
train acc:  0.7734375
train loss:  0.43688493967056274
train gradient:  0.42336713575848167
iteration : 1981
train acc:  0.8359375
train loss:  0.40808993577957153
train gradient:  0.3685871172926735
iteration : 1982
train acc:  0.875
train loss:  0.31986290216445923
train gradient:  0.44020818746796914
iteration : 1983
train acc:  0.8203125
train loss:  0.42511802911758423
train gradient:  0.33259235265109477
iteration : 1984
train acc:  0.828125
train loss:  0.4093794822692871
train gradient:  0.5476031224500287
iteration : 1985
train acc:  0.78125
train loss:  0.4178096354007721
train gradient:  0.3758280387988597
iteration : 1986
train acc:  0.8046875
train loss:  0.46529820561408997
train gradient:  0.7797885234155058
iteration : 1987
train acc:  0.8125
train loss:  0.4150821566581726
train gradient:  0.40344892547303646
iteration : 1988
train acc:  0.8671875
train loss:  0.33589380979537964
train gradient:  0.28911154688598606
iteration : 1989
train acc:  0.75
train loss:  0.5618388056755066
train gradient:  0.7112381784613135
iteration : 1990
train acc:  0.828125
train loss:  0.41383999586105347
train gradient:  0.4708136540114038
iteration : 1991
train acc:  0.8203125
train loss:  0.43018412590026855
train gradient:  0.46803501165216893
iteration : 1992
train acc:  0.7734375
train loss:  0.45217207074165344
train gradient:  0.528883480306925
iteration : 1993
train acc:  0.75
train loss:  0.5156621932983398
train gradient:  0.6087014384582611
iteration : 1994
train acc:  0.796875
train loss:  0.41835373640060425
train gradient:  0.5046350582732714
iteration : 1995
train acc:  0.8125
train loss:  0.41367340087890625
train gradient:  0.34675921208128413
iteration : 1996
train acc:  0.8515625
train loss:  0.34210604429244995
train gradient:  0.3251374577375985
iteration : 1997
train acc:  0.8515625
train loss:  0.3737761080265045
train gradient:  0.3857327107010747
iteration : 1998
train acc:  0.75
train loss:  0.5104405879974365
train gradient:  0.724073352825707
iteration : 1999
train acc:  0.7578125
train loss:  0.49224022030830383
train gradient:  0.6137825558384764
iteration : 2000
train acc:  0.84375
train loss:  0.37136566638946533
train gradient:  0.34575546687790604
iteration : 2001
train acc:  0.8671875
train loss:  0.33436480164527893
train gradient:  0.2630653167698953
iteration : 2002
train acc:  0.796875
train loss:  0.44049447774887085
train gradient:  0.3290828937021794
iteration : 2003
train acc:  0.875
train loss:  0.3256376385688782
train gradient:  0.28932970104976685
iteration : 2004
train acc:  0.796875
train loss:  0.4309362471103668
train gradient:  0.39363385662722056
iteration : 2005
train acc:  0.8125
train loss:  0.42571741342544556
train gradient:  0.35963884113702776
iteration : 2006
train acc:  0.8125
train loss:  0.3885687589645386
train gradient:  0.3144391931239405
iteration : 2007
train acc:  0.828125
train loss:  0.38360363245010376
train gradient:  0.3567131996495721
iteration : 2008
train acc:  0.8515625
train loss:  0.41889747977256775
train gradient:  0.42191795770610574
iteration : 2009
train acc:  0.8359375
train loss:  0.36213600635528564
train gradient:  0.35572983385283036
iteration : 2010
train acc:  0.828125
train loss:  0.42226964235305786
train gradient:  0.2961284401396905
iteration : 2011
train acc:  0.828125
train loss:  0.4232679009437561
train gradient:  0.45137646851811386
iteration : 2012
train acc:  0.796875
train loss:  0.4140256643295288
train gradient:  0.6751686862846543
iteration : 2013
train acc:  0.8203125
train loss:  0.3716576099395752
train gradient:  0.30746786889339045
iteration : 2014
train acc:  0.8515625
train loss:  0.36967262625694275
train gradient:  0.2743173634542645
iteration : 2015
train acc:  0.7734375
train loss:  0.41973432898521423
train gradient:  0.46870312028238514
iteration : 2016
train acc:  0.8515625
train loss:  0.3304489850997925
train gradient:  0.3378938219677326
iteration : 2017
train acc:  0.8359375
train loss:  0.401471883058548
train gradient:  0.3464519450214873
iteration : 2018
train acc:  0.7578125
train loss:  0.48952263593673706
train gradient:  0.5995711860285695
iteration : 2019
train acc:  0.7734375
train loss:  0.4659452736377716
train gradient:  0.5651421528698919
iteration : 2020
train acc:  0.8203125
train loss:  0.44502514600753784
train gradient:  0.3826417345229846
iteration : 2021
train acc:  0.8046875
train loss:  0.43032175302505493
train gradient:  0.3694649084202951
iteration : 2022
train acc:  0.8359375
train loss:  0.37779736518859863
train gradient:  0.3551511228850562
iteration : 2023
train acc:  0.796875
train loss:  0.4235931932926178
train gradient:  0.464825929528071
iteration : 2024
train acc:  0.8515625
train loss:  0.3212888240814209
train gradient:  0.3242029836611531
iteration : 2025
train acc:  0.796875
train loss:  0.4651581943035126
train gradient:  0.47566798327937915
iteration : 2026
train acc:  0.75
train loss:  0.5237921476364136
train gradient:  0.6599434658801944
iteration : 2027
train acc:  0.7734375
train loss:  0.474775105714798
train gradient:  0.49922212534120336
iteration : 2028
train acc:  0.7734375
train loss:  0.44128137826919556
train gradient:  0.41844074209026566
iteration : 2029
train acc:  0.765625
train loss:  0.4863271415233612
train gradient:  0.5512295658114222
iteration : 2030
train acc:  0.828125
train loss:  0.35736846923828125
train gradient:  0.2648492036980441
iteration : 2031
train acc:  0.875
train loss:  0.38996535539627075
train gradient:  0.4917978736438548
iteration : 2032
train acc:  0.7578125
train loss:  0.4749511778354645
train gradient:  0.406462739254019
iteration : 2033
train acc:  0.8046875
train loss:  0.37782397866249084
train gradient:  0.449587382598529
iteration : 2034
train acc:  0.796875
train loss:  0.4240516722202301
train gradient:  0.4249923336844453
iteration : 2035
train acc:  0.7890625
train loss:  0.48140421509742737
train gradient:  0.4919566676049492
iteration : 2036
train acc:  0.7578125
train loss:  0.5226253867149353
train gradient:  0.7192010292001554
iteration : 2037
train acc:  0.796875
train loss:  0.40376928448677063
train gradient:  0.3018327400258165
iteration : 2038
train acc:  0.8671875
train loss:  0.3249050974845886
train gradient:  0.2606220831482931
iteration : 2039
train acc:  0.8671875
train loss:  0.3512578010559082
train gradient:  0.298637036930794
iteration : 2040
train acc:  0.859375
train loss:  0.33475589752197266
train gradient:  0.23989249318836767
iteration : 2041
train acc:  0.7734375
train loss:  0.4996125102043152
train gradient:  0.4617504437155053
iteration : 2042
train acc:  0.828125
train loss:  0.4749213755130768
train gradient:  0.42136218594824726
iteration : 2043
train acc:  0.859375
train loss:  0.3372601270675659
train gradient:  0.31947493702070456
iteration : 2044
train acc:  0.8046875
train loss:  0.45359858870506287
train gradient:  0.479910682105532
iteration : 2045
train acc:  0.7890625
train loss:  0.4005160927772522
train gradient:  0.3308075836316677
iteration : 2046
train acc:  0.75
train loss:  0.4941599369049072
train gradient:  0.6000631275173183
iteration : 2047
train acc:  0.8046875
train loss:  0.44332364201545715
train gradient:  0.3358066411646689
iteration : 2048
train acc:  0.828125
train loss:  0.3851972818374634
train gradient:  0.2974226807697725
iteration : 2049
train acc:  0.8203125
train loss:  0.37436026334762573
train gradient:  0.3432710533374697
iteration : 2050
train acc:  0.796875
train loss:  0.3972207307815552
train gradient:  0.31338570086881373
iteration : 2051
train acc:  0.796875
train loss:  0.4908679127693176
train gradient:  0.5741414266543106
iteration : 2052
train acc:  0.84375
train loss:  0.3969059884548187
train gradient:  0.3670719733239375
iteration : 2053
train acc:  0.8359375
train loss:  0.3855520784854889
train gradient:  0.2791383341628207
iteration : 2054
train acc:  0.859375
train loss:  0.33118146657943726
train gradient:  0.3504829103791691
iteration : 2055
train acc:  0.828125
train loss:  0.39012229442596436
train gradient:  0.34195975380160043
iteration : 2056
train acc:  0.78125
train loss:  0.4230327308177948
train gradient:  0.44215632168109786
iteration : 2057
train acc:  0.7734375
train loss:  0.46445232629776
train gradient:  0.425394729261019
iteration : 2058
train acc:  0.8203125
train loss:  0.4040473997592926
train gradient:  0.3709858038022305
iteration : 2059
train acc:  0.7578125
train loss:  0.4478423595428467
train gradient:  0.3955814244003711
iteration : 2060
train acc:  0.8359375
train loss:  0.36973756551742554
train gradient:  0.2718053478062323
iteration : 2061
train acc:  0.8359375
train loss:  0.42999690771102905
train gradient:  0.43442287801863033
iteration : 2062
train acc:  0.8046875
train loss:  0.4194989800453186
train gradient:  0.4163148072476501
iteration : 2063
train acc:  0.765625
train loss:  0.5127720832824707
train gradient:  0.49375041642414547
iteration : 2064
train acc:  0.828125
train loss:  0.35829824209213257
train gradient:  0.2230942147885835
iteration : 2065
train acc:  0.78125
train loss:  0.43529972434043884
train gradient:  0.39954017207091963
iteration : 2066
train acc:  0.8125
train loss:  0.4033452272415161
train gradient:  0.4178498623204824
iteration : 2067
train acc:  0.78125
train loss:  0.4327743947505951
train gradient:  0.4040517905235798
iteration : 2068
train acc:  0.8671875
train loss:  0.34089574217796326
train gradient:  0.4493145862752913
iteration : 2069
train acc:  0.75
train loss:  0.47827571630477905
train gradient:  0.5182225969408758
iteration : 2070
train acc:  0.8515625
train loss:  0.3861597776412964
train gradient:  0.254821916171717
iteration : 2071
train acc:  0.7109375
train loss:  0.5149605870246887
train gradient:  0.5706988628100176
iteration : 2072
train acc:  0.8203125
train loss:  0.41164523363113403
train gradient:  0.3404072594989806
iteration : 2073
train acc:  0.828125
train loss:  0.35727977752685547
train gradient:  0.3473980251726649
iteration : 2074
train acc:  0.796875
train loss:  0.4757823348045349
train gradient:  0.5835569676346593
iteration : 2075
train acc:  0.8359375
train loss:  0.3985585570335388
train gradient:  0.432090878181202
iteration : 2076
train acc:  0.8125
train loss:  0.45896613597869873
train gradient:  0.43595874027923703
iteration : 2077
train acc:  0.828125
train loss:  0.41668015718460083
train gradient:  0.3191968606878085
iteration : 2078
train acc:  0.7890625
train loss:  0.447068989276886
train gradient:  0.5667827660386103
iteration : 2079
train acc:  0.7578125
train loss:  0.4239718019962311
train gradient:  0.28951421800305394
iteration : 2080
train acc:  0.8046875
train loss:  0.45277148485183716
train gradient:  0.5031450586285535
iteration : 2081
train acc:  0.8359375
train loss:  0.387484610080719
train gradient:  0.3813389747939274
iteration : 2082
train acc:  0.8125
train loss:  0.4331972599029541
train gradient:  0.39844402031820536
iteration : 2083
train acc:  0.7734375
train loss:  0.48010966181755066
train gradient:  0.5513764155052461
iteration : 2084
train acc:  0.8359375
train loss:  0.3943253755569458
train gradient:  0.27468681715071674
iteration : 2085
train acc:  0.8515625
train loss:  0.32520610094070435
train gradient:  0.27398828571862643
iteration : 2086
train acc:  0.84375
train loss:  0.3824189305305481
train gradient:  0.3653362167709124
iteration : 2087
train acc:  0.8125
train loss:  0.40235865116119385
train gradient:  0.3960499644028469
iteration : 2088
train acc:  0.8359375
train loss:  0.4431470036506653
train gradient:  0.45579647989454175
iteration : 2089
train acc:  0.8203125
train loss:  0.38141846656799316
train gradient:  0.30673443184635324
iteration : 2090
train acc:  0.8125
train loss:  0.39460626244544983
train gradient:  0.4274586715675001
iteration : 2091
train acc:  0.8046875
train loss:  0.3789275884628296
train gradient:  0.3514157371828973
iteration : 2092
train acc:  0.78125
train loss:  0.4371691644191742
train gradient:  0.4106785059788137
iteration : 2093
train acc:  0.8203125
train loss:  0.436191201210022
train gradient:  0.3830656865583334
iteration : 2094
train acc:  0.8359375
train loss:  0.40926408767700195
train gradient:  0.4145235961105174
iteration : 2095
train acc:  0.84375
train loss:  0.32311326265335083
train gradient:  0.2733501498727112
iteration : 2096
train acc:  0.796875
train loss:  0.4001126289367676
train gradient:  0.438784529105859
iteration : 2097
train acc:  0.8203125
train loss:  0.4316026568412781
train gradient:  0.3324437091066445
iteration : 2098
train acc:  0.8515625
train loss:  0.3897436857223511
train gradient:  0.2662375421657404
iteration : 2099
train acc:  0.765625
train loss:  0.5041601657867432
train gradient:  0.5715376141322697
iteration : 2100
train acc:  0.828125
train loss:  0.37704259157180786
train gradient:  0.43948058555476543
iteration : 2101
train acc:  0.7734375
train loss:  0.40020883083343506
train gradient:  0.3566306607973445
iteration : 2102
train acc:  0.7734375
train loss:  0.45030781626701355
train gradient:  0.41427646681852615
iteration : 2103
train acc:  0.7734375
train loss:  0.45042335987091064
train gradient:  0.47752451444655547
iteration : 2104
train acc:  0.75
train loss:  0.45715874433517456
train gradient:  0.5405994966126608
iteration : 2105
train acc:  0.734375
train loss:  0.469497948884964
train gradient:  0.41696781314199416
iteration : 2106
train acc:  0.78125
train loss:  0.43322232365608215
train gradient:  0.3597668509324864
iteration : 2107
train acc:  0.84375
train loss:  0.3628430962562561
train gradient:  0.24661836026932527
iteration : 2108
train acc:  0.7890625
train loss:  0.3897101879119873
train gradient:  0.3482943236212991
iteration : 2109
train acc:  0.84375
train loss:  0.38842886686325073
train gradient:  0.458587230206108
iteration : 2110
train acc:  0.8125
train loss:  0.3956814706325531
train gradient:  0.3547509573148103
iteration : 2111
train acc:  0.8125
train loss:  0.3829768896102905
train gradient:  0.29429834545781747
iteration : 2112
train acc:  0.8125
train loss:  0.3578423857688904
train gradient:  0.4457186366531544
iteration : 2113
train acc:  0.859375
train loss:  0.3523995876312256
train gradient:  0.36094056340203107
iteration : 2114
train acc:  0.8125
train loss:  0.393753319978714
train gradient:  0.3473400635929813
iteration : 2115
train acc:  0.78125
train loss:  0.4101583659648895
train gradient:  0.415132180977853
iteration : 2116
train acc:  0.8125
train loss:  0.42254990339279175
train gradient:  0.38544534311591017
iteration : 2117
train acc:  0.8203125
train loss:  0.3817920684814453
train gradient:  0.4679908143518329
iteration : 2118
train acc:  0.8046875
train loss:  0.4732421040534973
train gradient:  0.5472795865401664
iteration : 2119
train acc:  0.8515625
train loss:  0.34624505043029785
train gradient:  0.3695702550059409
iteration : 2120
train acc:  0.890625
train loss:  0.33754390478134155
train gradient:  0.24787113912722264
iteration : 2121
train acc:  0.796875
train loss:  0.3825516104698181
train gradient:  0.2878231542328691
iteration : 2122
train acc:  0.828125
train loss:  0.3570929765701294
train gradient:  0.4221350800056052
iteration : 2123
train acc:  0.8203125
train loss:  0.395179808139801
train gradient:  0.36501484813806523
iteration : 2124
train acc:  0.8046875
train loss:  0.40136823058128357
train gradient:  0.28729862563066355
iteration : 2125
train acc:  0.84375
train loss:  0.37970322370529175
train gradient:  0.4145298848731811
iteration : 2126
train acc:  0.8359375
train loss:  0.4555026888847351
train gradient:  0.5448396766937431
iteration : 2127
train acc:  0.859375
train loss:  0.3580496907234192
train gradient:  0.3962140521569806
iteration : 2128
train acc:  0.8203125
train loss:  0.4044671654701233
train gradient:  0.4150390476896099
iteration : 2129
train acc:  0.859375
train loss:  0.34740132093429565
train gradient:  0.3217811567283424
iteration : 2130
train acc:  0.8515625
train loss:  0.3470577895641327
train gradient:  0.253634708753034
iteration : 2131
train acc:  0.8046875
train loss:  0.44194144010543823
train gradient:  0.4332334702795804
iteration : 2132
train acc:  0.796875
train loss:  0.4366525113582611
train gradient:  0.40658808004595254
iteration : 2133
train acc:  0.84375
train loss:  0.3631981909275055
train gradient:  0.25873281396077363
iteration : 2134
train acc:  0.7890625
train loss:  0.45455142855644226
train gradient:  0.4065303789420191
iteration : 2135
train acc:  0.7890625
train loss:  0.4721081852912903
train gradient:  0.4976515816632849
iteration : 2136
train acc:  0.765625
train loss:  0.388082355260849
train gradient:  0.37086404024309394
iteration : 2137
train acc:  0.8515625
train loss:  0.42190176248550415
train gradient:  0.4752916777817921
iteration : 2138
train acc:  0.8515625
train loss:  0.33069783449172974
train gradient:  0.2657987002424481
iteration : 2139
train acc:  0.796875
train loss:  0.41623759269714355
train gradient:  0.5151441233725346
iteration : 2140
train acc:  0.828125
train loss:  0.37924668192863464
train gradient:  0.38371483179146865
iteration : 2141
train acc:  0.796875
train loss:  0.4137914776802063
train gradient:  0.36654825818148407
iteration : 2142
train acc:  0.8359375
train loss:  0.3626335561275482
train gradient:  0.3105635088231453
iteration : 2143
train acc:  0.7890625
train loss:  0.4449072480201721
train gradient:  0.6211043349242811
iteration : 2144
train acc:  0.75
train loss:  0.5145134329795837
train gradient:  0.5432330038904822
iteration : 2145
train acc:  0.7890625
train loss:  0.38847899436950684
train gradient:  0.3437000825276572
iteration : 2146
train acc:  0.7578125
train loss:  0.4746035933494568
train gradient:  0.4227765286636578
iteration : 2147
train acc:  0.8125
train loss:  0.37180209159851074
train gradient:  0.47238122212592415
iteration : 2148
train acc:  0.8125
train loss:  0.38312405347824097
train gradient:  0.40146327017285127
iteration : 2149
train acc:  0.765625
train loss:  0.4349334239959717
train gradient:  0.4559506637932284
iteration : 2150
train acc:  0.8515625
train loss:  0.3380396068096161
train gradient:  0.31133920385756764
iteration : 2151
train acc:  0.8125
train loss:  0.3992503583431244
train gradient:  0.3497781600375764
iteration : 2152
train acc:  0.78125
train loss:  0.40511226654052734
train gradient:  0.4474833721002716
iteration : 2153
train acc:  0.7890625
train loss:  0.41102802753448486
train gradient:  0.4673219704713899
iteration : 2154
train acc:  0.796875
train loss:  0.43738871812820435
train gradient:  0.5191352710210454
iteration : 2155
train acc:  0.84375
train loss:  0.3926728069782257
train gradient:  0.39552955819008084
iteration : 2156
train acc:  0.84375
train loss:  0.36618274450302124
train gradient:  0.28475327334806927
iteration : 2157
train acc:  0.8671875
train loss:  0.358686238527298
train gradient:  0.267667916335762
iteration : 2158
train acc:  0.8203125
train loss:  0.4021044373512268
train gradient:  0.38024992353423315
iteration : 2159
train acc:  0.8671875
train loss:  0.35287991166114807
train gradient:  0.35118535213457225
iteration : 2160
train acc:  0.8671875
train loss:  0.36841505765914917
train gradient:  0.40476689371786856
iteration : 2161
train acc:  0.796875
train loss:  0.43969017267227173
train gradient:  0.38421457782542234
iteration : 2162
train acc:  0.8046875
train loss:  0.42213156819343567
train gradient:  0.3986992956427238
iteration : 2163
train acc:  0.7890625
train loss:  0.4115101099014282
train gradient:  0.4150837714559083
iteration : 2164
train acc:  0.8359375
train loss:  0.40490955114364624
train gradient:  0.3784172233950863
iteration : 2165
train acc:  0.8125
train loss:  0.4087381064891815
train gradient:  0.38826222061546606
iteration : 2166
train acc:  0.7265625
train loss:  0.5528601408004761
train gradient:  0.6399429911747113
iteration : 2167
train acc:  0.8125
train loss:  0.4328985810279846
train gradient:  0.46941584526887
iteration : 2168
train acc:  0.84375
train loss:  0.3613928556442261
train gradient:  0.33597689037764894
iteration : 2169
train acc:  0.859375
train loss:  0.3688530921936035
train gradient:  0.3763169906405285
iteration : 2170
train acc:  0.796875
train loss:  0.4015377163887024
train gradient:  0.4866956179843236
iteration : 2171
train acc:  0.8046875
train loss:  0.36786559224128723
train gradient:  0.3228566860860849
iteration : 2172
train acc:  0.8671875
train loss:  0.34784647822380066
train gradient:  0.3737700323765807
iteration : 2173
train acc:  0.78125
train loss:  0.4894680976867676
train gradient:  0.5853862813269808
iteration : 2174
train acc:  0.796875
train loss:  0.41362857818603516
train gradient:  0.32766127248404614
iteration : 2175
train acc:  0.8671875
train loss:  0.3326096534729004
train gradient:  0.40158356182295013
iteration : 2176
train acc:  0.8515625
train loss:  0.38236987590789795
train gradient:  0.43083727071982
iteration : 2177
train acc:  0.84375
train loss:  0.4160768985748291
train gradient:  0.39025666147752164
iteration : 2178
train acc:  0.84375
train loss:  0.41426339745521545
train gradient:  0.42244215396674345
iteration : 2179
train acc:  0.796875
train loss:  0.4013306498527527
train gradient:  0.3328527168152133
iteration : 2180
train acc:  0.875
train loss:  0.33502382040023804
train gradient:  0.32097292745720934
iteration : 2181
train acc:  0.7578125
train loss:  0.4590209722518921
train gradient:  0.4269360857437115
iteration : 2182
train acc:  0.7890625
train loss:  0.46474599838256836
train gradient:  0.46694901417233936
iteration : 2183
train acc:  0.8203125
train loss:  0.3673803210258484
train gradient:  0.2892164071637558
iteration : 2184
train acc:  0.828125
train loss:  0.3803030848503113
train gradient:  0.3236563289341379
iteration : 2185
train acc:  0.7890625
train loss:  0.4182265102863312
train gradient:  0.506344028946335
iteration : 2186
train acc:  0.90625
train loss:  0.32497861981391907
train gradient:  0.25926202960262007
iteration : 2187
train acc:  0.796875
train loss:  0.4634297490119934
train gradient:  0.7443929342035911
iteration : 2188
train acc:  0.859375
train loss:  0.2988176941871643
train gradient:  0.24152106271559
iteration : 2189
train acc:  0.7578125
train loss:  0.4572492241859436
train gradient:  0.4738618479612769
iteration : 2190
train acc:  0.7890625
train loss:  0.49878886342048645
train gradient:  0.5695088977366268
iteration : 2191
train acc:  0.8671875
train loss:  0.327376127243042
train gradient:  0.2795694638913506
iteration : 2192
train acc:  0.828125
train loss:  0.4646407663822174
train gradient:  0.4653410383473331
iteration : 2193
train acc:  0.7890625
train loss:  0.46754124760627747
train gradient:  0.4750332660291634
iteration : 2194
train acc:  0.9296875
train loss:  0.25445908308029175
train gradient:  0.2805359810144005
iteration : 2195
train acc:  0.8203125
train loss:  0.38254445791244507
train gradient:  0.3242481382113565
iteration : 2196
train acc:  0.828125
train loss:  0.4270029664039612
train gradient:  0.49472161674596843
iteration : 2197
train acc:  0.7890625
train loss:  0.41794121265411377
train gradient:  0.3478592462721683
iteration : 2198
train acc:  0.859375
train loss:  0.3469739556312561
train gradient:  0.3048345644648548
iteration : 2199
train acc:  0.7734375
train loss:  0.4332139790058136
train gradient:  0.4530425914642203
iteration : 2200
train acc:  0.8359375
train loss:  0.39869189262390137
train gradient:  0.3743696128437922
iteration : 2201
train acc:  0.9140625
train loss:  0.29480743408203125
train gradient:  0.2041139847193752
iteration : 2202
train acc:  0.8046875
train loss:  0.39805904030799866
train gradient:  0.3919593923373862
iteration : 2203
train acc:  0.78125
train loss:  0.4194580912590027
train gradient:  0.3207944421014146
iteration : 2204
train acc:  0.8125
train loss:  0.38256391882896423
train gradient:  0.36154300044411974
iteration : 2205
train acc:  0.8203125
train loss:  0.44040268659591675
train gradient:  0.41547744181007196
iteration : 2206
train acc:  0.8359375
train loss:  0.39364469051361084
train gradient:  0.26987256655383046
iteration : 2207
train acc:  0.8671875
train loss:  0.33782804012298584
train gradient:  0.2522754441938668
iteration : 2208
train acc:  0.8203125
train loss:  0.40393349528312683
train gradient:  0.30975155213704664
iteration : 2209
train acc:  0.78125
train loss:  0.45224353671073914
train gradient:  0.5446564701562889
iteration : 2210
train acc:  0.7734375
train loss:  0.4403386116027832
train gradient:  0.4965046373186433
iteration : 2211
train acc:  0.8515625
train loss:  0.36727771162986755
train gradient:  0.41610707679401415
iteration : 2212
train acc:  0.8046875
train loss:  0.46643444895744324
train gradient:  0.46632056593139554
iteration : 2213
train acc:  0.8359375
train loss:  0.3627314567565918
train gradient:  0.2600058190359963
iteration : 2214
train acc:  0.7734375
train loss:  0.4683522582054138
train gradient:  0.5997374783192577
iteration : 2215
train acc:  0.78125
train loss:  0.4648544192314148
train gradient:  0.5178292874399402
iteration : 2216
train acc:  0.7734375
train loss:  0.5297532081604004
train gradient:  0.6796548254868667
iteration : 2217
train acc:  0.8203125
train loss:  0.3498743772506714
train gradient:  0.2619649798472854
iteration : 2218
train acc:  0.8671875
train loss:  0.30622434616088867
train gradient:  0.2407345908310817
iteration : 2219
train acc:  0.8125
train loss:  0.4270162880420685
train gradient:  0.587519579211778
iteration : 2220
train acc:  0.828125
train loss:  0.3632695972919464
train gradient:  0.24304902181121552
iteration : 2221
train acc:  0.859375
train loss:  0.31493908166885376
train gradient:  0.2493192067434986
iteration : 2222
train acc:  0.8515625
train loss:  0.4020632803440094
train gradient:  0.38443973876800636
iteration : 2223
train acc:  0.78125
train loss:  0.42481717467308044
train gradient:  0.4656269307141931
iteration : 2224
train acc:  0.84375
train loss:  0.3896480202674866
train gradient:  0.4401783724974278
iteration : 2225
train acc:  0.796875
train loss:  0.4296596050262451
train gradient:  0.41837684368450917
iteration : 2226
train acc:  0.8671875
train loss:  0.3581116199493408
train gradient:  0.31573364965493517
iteration : 2227
train acc:  0.8203125
train loss:  0.43020129203796387
train gradient:  0.39863239938519734
iteration : 2228
train acc:  0.8203125
train loss:  0.4482831358909607
train gradient:  0.43599802194066917
iteration : 2229
train acc:  0.8125
train loss:  0.4286866784095764
train gradient:  0.4516340150106093
iteration : 2230
train acc:  0.78125
train loss:  0.4155900478363037
train gradient:  0.4586734547060393
iteration : 2231
train acc:  0.7890625
train loss:  0.4395420551300049
train gradient:  0.5425961858143642
iteration : 2232
train acc:  0.7890625
train loss:  0.42267486453056335
train gradient:  0.38476337819194095
iteration : 2233
train acc:  0.8046875
train loss:  0.39796021580696106
train gradient:  0.429979639346425
iteration : 2234
train acc:  0.8125
train loss:  0.4004703760147095
train gradient:  0.40591501552531195
iteration : 2235
train acc:  0.859375
train loss:  0.34165140986442566
train gradient:  0.4075156433342082
iteration : 2236
train acc:  0.8671875
train loss:  0.3883346915245056
train gradient:  0.3479721241687998
iteration : 2237
train acc:  0.8671875
train loss:  0.3846985697746277
train gradient:  0.2541647957298354
iteration : 2238
train acc:  0.78125
train loss:  0.4175654649734497
train gradient:  0.5712646533985088
iteration : 2239
train acc:  0.8515625
train loss:  0.34973859786987305
train gradient:  0.3084996966361088
iteration : 2240
train acc:  0.8359375
train loss:  0.39349234104156494
train gradient:  0.3562675331168461
iteration : 2241
train acc:  0.78125
train loss:  0.47207319736480713
train gradient:  0.462920330280103
iteration : 2242
train acc:  0.828125
train loss:  0.401517391204834
train gradient:  0.37986908166103367
iteration : 2243
train acc:  0.84375
train loss:  0.37640678882598877
train gradient:  0.2782644008230426
iteration : 2244
train acc:  0.8671875
train loss:  0.36306971311569214
train gradient:  0.32785538939720404
iteration : 2245
train acc:  0.828125
train loss:  0.3798249363899231
train gradient:  0.5019274620792544
iteration : 2246
train acc:  0.859375
train loss:  0.3559741973876953
train gradient:  0.3142306789905469
iteration : 2247
train acc:  0.859375
train loss:  0.35166341066360474
train gradient:  0.31554742583871503
iteration : 2248
train acc:  0.8203125
train loss:  0.406311571598053
train gradient:  0.4368787944178454
iteration : 2249
train acc:  0.796875
train loss:  0.40668433904647827
train gradient:  0.4095873605836601
iteration : 2250
train acc:  0.8203125
train loss:  0.37890440225601196
train gradient:  0.3254952021746751
iteration : 2251
train acc:  0.8359375
train loss:  0.3573593199253082
train gradient:  0.3149887973990102
iteration : 2252
train acc:  0.8046875
train loss:  0.3999267518520355
train gradient:  0.29389035995012225
iteration : 2253
train acc:  0.765625
train loss:  0.47399160265922546
train gradient:  0.34524133227248177
iteration : 2254
train acc:  0.7734375
train loss:  0.4285714030265808
train gradient:  0.48343687412471226
iteration : 2255
train acc:  0.7890625
train loss:  0.4180121421813965
train gradient:  0.34622123043043695
iteration : 2256
train acc:  0.859375
train loss:  0.3104473352432251
train gradient:  0.5257216991150686
iteration : 2257
train acc:  0.8515625
train loss:  0.3768971562385559
train gradient:  0.26042368946080663
iteration : 2258
train acc:  0.7890625
train loss:  0.40351617336273193
train gradient:  0.3920704232572292
iteration : 2259
train acc:  0.8359375
train loss:  0.36397784948349
train gradient:  0.2823390572626145
iteration : 2260
train acc:  0.8046875
train loss:  0.4101688861846924
train gradient:  0.32591939536298875
iteration : 2261
train acc:  0.765625
train loss:  0.43789809942245483
train gradient:  0.6830251237970937
iteration : 2262
train acc:  0.84375
train loss:  0.3487084209918976
train gradient:  0.3172487194544915
iteration : 2263
train acc:  0.828125
train loss:  0.36797910928726196
train gradient:  0.3699461502544809
iteration : 2264
train acc:  0.859375
train loss:  0.35312238335609436
train gradient:  0.38058763536179724
iteration : 2265
train acc:  0.828125
train loss:  0.37457984685897827
train gradient:  0.33317781769954147
iteration : 2266
train acc:  0.8046875
train loss:  0.4040892720222473
train gradient:  0.36875675583718415
iteration : 2267
train acc:  0.7578125
train loss:  0.5075687170028687
train gradient:  0.5861899945953166
iteration : 2268
train acc:  0.875
train loss:  0.3491617739200592
train gradient:  0.27568296791102476
iteration : 2269
train acc:  0.71875
train loss:  0.5545638799667358
train gradient:  0.6575382379651384
iteration : 2270
train acc:  0.8046875
train loss:  0.40041384100914
train gradient:  0.3617019012589069
iteration : 2271
train acc:  0.8359375
train loss:  0.3353031277656555
train gradient:  0.36781523502465663
iteration : 2272
train acc:  0.8046875
train loss:  0.37306681275367737
train gradient:  0.4978922997366165
iteration : 2273
train acc:  0.7734375
train loss:  0.46253228187561035
train gradient:  0.5241223420987651
iteration : 2274
train acc:  0.8046875
train loss:  0.4030003547668457
train gradient:  0.38365158139037103
iteration : 2275
train acc:  0.7734375
train loss:  0.45817336440086365
train gradient:  0.49331436448078403
iteration : 2276
train acc:  0.8046875
train loss:  0.4643581211566925
train gradient:  0.6126131499643896
iteration : 2277
train acc:  0.8125
train loss:  0.39161357283592224
train gradient:  0.37503177628582673
iteration : 2278
train acc:  0.8203125
train loss:  0.38920462131500244
train gradient:  0.3567367482873515
iteration : 2279
train acc:  0.8359375
train loss:  0.3991924822330475
train gradient:  0.3235296317790219
iteration : 2280
train acc:  0.8125
train loss:  0.36802709102630615
train gradient:  0.4564686428711155
iteration : 2281
train acc:  0.7734375
train loss:  0.49297896027565
train gradient:  0.701897767958134
iteration : 2282
train acc:  0.8515625
train loss:  0.41026341915130615
train gradient:  0.3771734420499813
iteration : 2283
train acc:  0.8046875
train loss:  0.48044466972351074
train gradient:  0.42433554322021655
iteration : 2284
train acc:  0.8203125
train loss:  0.43381449580192566
train gradient:  0.40709834659311706
iteration : 2285
train acc:  0.84375
train loss:  0.4093928933143616
train gradient:  0.416735902398589
iteration : 2286
train acc:  0.7890625
train loss:  0.4299947917461395
train gradient:  0.309098495927126
iteration : 2287
train acc:  0.84375
train loss:  0.3528203070163727
train gradient:  0.3853504180774146
iteration : 2288
train acc:  0.875
train loss:  0.29303765296936035
train gradient:  0.24864145238657043
iteration : 2289
train acc:  0.828125
train loss:  0.3790223002433777
train gradient:  0.31061393031195655
iteration : 2290
train acc:  0.796875
train loss:  0.42644837498664856
train gradient:  0.39578588965029304
iteration : 2291
train acc:  0.8359375
train loss:  0.3989887237548828
train gradient:  0.5219017712773116
iteration : 2292
train acc:  0.8203125
train loss:  0.3981974124908447
train gradient:  0.37418545454708585
iteration : 2293
train acc:  0.8046875
train loss:  0.4112245440483093
train gradient:  0.2824544482914376
iteration : 2294
train acc:  0.84375
train loss:  0.3850231170654297
train gradient:  0.27217897706293914
iteration : 2295
train acc:  0.8359375
train loss:  0.38764703273773193
train gradient:  0.4028007732344389
iteration : 2296
train acc:  0.84375
train loss:  0.36776435375213623
train gradient:  0.4281310274830431
iteration : 2297
train acc:  0.84375
train loss:  0.39281535148620605
train gradient:  0.46063110164969884
iteration : 2298
train acc:  0.859375
train loss:  0.34369659423828125
train gradient:  0.292828476605389
iteration : 2299
train acc:  0.8125
train loss:  0.41088712215423584
train gradient:  0.40741096142594124
iteration : 2300
train acc:  0.7890625
train loss:  0.4456208050251007
train gradient:  0.31847169351752586
iteration : 2301
train acc:  0.7734375
train loss:  0.46620291471481323
train gradient:  0.4530428155830126
iteration : 2302
train acc:  0.796875
train loss:  0.4142691195011139
train gradient:  0.39280007652159143
iteration : 2303
train acc:  0.8359375
train loss:  0.41823190450668335
train gradient:  0.37262564989979596
iteration : 2304
train acc:  0.8046875
train loss:  0.40135592222213745
train gradient:  0.26785927191139436
iteration : 2305
train acc:  0.7734375
train loss:  0.512763261795044
train gradient:  0.5101748559704307
iteration : 2306
train acc:  0.8671875
train loss:  0.3356621265411377
train gradient:  0.26562849098420943
iteration : 2307
train acc:  0.8046875
train loss:  0.3938264846801758
train gradient:  0.3216655036945163
iteration : 2308
train acc:  0.8203125
train loss:  0.38192635774612427
train gradient:  0.35119452442854787
iteration : 2309
train acc:  0.796875
train loss:  0.4794304668903351
train gradient:  0.5763664213876589
iteration : 2310
train acc:  0.78125
train loss:  0.5058491230010986
train gradient:  0.5633512095612305
iteration : 2311
train acc:  0.8125
train loss:  0.4471728205680847
train gradient:  0.48871800861214404
iteration : 2312
train acc:  0.859375
train loss:  0.39173635840415955
train gradient:  0.36134858538225845
iteration : 2313
train acc:  0.875
train loss:  0.3525228500366211
train gradient:  0.3226570780119218
iteration : 2314
train acc:  0.8125
train loss:  0.3869617283344269
train gradient:  0.28131953760041273
iteration : 2315
train acc:  0.8046875
train loss:  0.3840515613555908
train gradient:  0.33726098919526787
iteration : 2316
train acc:  0.8359375
train loss:  0.4551781117916107
train gradient:  0.4653165202414303
iteration : 2317
train acc:  0.7890625
train loss:  0.4385284185409546
train gradient:  0.27222622470590224
iteration : 2318
train acc:  0.875
train loss:  0.3566701114177704
train gradient:  0.38496323487032963
iteration : 2319
train acc:  0.7578125
train loss:  0.4703916013240814
train gradient:  0.4324699179452369
iteration : 2320
train acc:  0.7890625
train loss:  0.4339463710784912
train gradient:  0.4194118144874883
iteration : 2321
train acc:  0.8359375
train loss:  0.38720470666885376
train gradient:  0.2863269603550579
iteration : 2322
train acc:  0.796875
train loss:  0.4554261565208435
train gradient:  0.3896177590302814
iteration : 2323
train acc:  0.7578125
train loss:  0.4885055720806122
train gradient:  0.4588763824947057
iteration : 2324
train acc:  0.8671875
train loss:  0.36092400550842285
train gradient:  0.38901134793903586
iteration : 2325
train acc:  0.8046875
train loss:  0.3870747685432434
train gradient:  0.2847849400867609
iteration : 2326
train acc:  0.7578125
train loss:  0.4617135226726532
train gradient:  0.5802265305815341
iteration : 2327
train acc:  0.78125
train loss:  0.48497000336647034
train gradient:  0.46350773781445703
iteration : 2328
train acc:  0.8125
train loss:  0.3747929334640503
train gradient:  0.31063607245200675
iteration : 2329
train acc:  0.796875
train loss:  0.44870465993881226
train gradient:  0.3582006170586191
iteration : 2330
train acc:  0.796875
train loss:  0.4491453170776367
train gradient:  0.3085856127403784
iteration : 2331
train acc:  0.859375
train loss:  0.3609430193901062
train gradient:  0.27542635094242574
iteration : 2332
train acc:  0.8046875
train loss:  0.38880062103271484
train gradient:  0.3609745838041656
iteration : 2333
train acc:  0.8515625
train loss:  0.39742031693458557
train gradient:  0.3165991018887371
iteration : 2334
train acc:  0.7890625
train loss:  0.4175401031970978
train gradient:  0.3574886695444673
iteration : 2335
train acc:  0.859375
train loss:  0.3840867877006531
train gradient:  0.3804881982165413
iteration : 2336
train acc:  0.828125
train loss:  0.3636435270309448
train gradient:  0.27752468228917343
iteration : 2337
train acc:  0.8203125
train loss:  0.3751686215400696
train gradient:  0.28934747037729186
iteration : 2338
train acc:  0.7734375
train loss:  0.4352678060531616
train gradient:  0.6166036028465729
iteration : 2339
train acc:  0.796875
train loss:  0.46454858779907227
train gradient:  0.3997712989998492
iteration : 2340
train acc:  0.7890625
train loss:  0.39952319860458374
train gradient:  0.4151884200787285
iteration : 2341
train acc:  0.859375
train loss:  0.31961026787757874
train gradient:  0.2766551512977362
iteration : 2342
train acc:  0.78125
train loss:  0.46520113945007324
train gradient:  0.5340651876883027
iteration : 2343
train acc:  0.875
train loss:  0.3636125326156616
train gradient:  0.7960247514818073
iteration : 2344
train acc:  0.828125
train loss:  0.38527151942253113
train gradient:  0.31911496406434775
iteration : 2345
train acc:  0.890625
train loss:  0.3273945748806
train gradient:  0.24004591149407456
iteration : 2346
train acc:  0.8203125
train loss:  0.3959200382232666
train gradient:  0.43381399680015065
iteration : 2347
train acc:  0.828125
train loss:  0.3948698043823242
train gradient:  0.3811410188781048
iteration : 2348
train acc:  0.78125
train loss:  0.45471134781837463
train gradient:  0.34189008328224374
iteration : 2349
train acc:  0.8671875
train loss:  0.3420715928077698
train gradient:  0.2792130657399013
iteration : 2350
train acc:  0.8125
train loss:  0.43558424711227417
train gradient:  0.39327282793693386
iteration : 2351
train acc:  0.8125
train loss:  0.3767636716365814
train gradient:  0.3238012034695554
iteration : 2352
train acc:  0.78125
train loss:  0.44177931547164917
train gradient:  0.43724184759624335
iteration : 2353
train acc:  0.78125
train loss:  0.45522651076316833
train gradient:  0.6086937891587614
iteration : 2354
train acc:  0.7890625
train loss:  0.4231235384941101
train gradient:  0.43276656422128584
iteration : 2355
train acc:  0.828125
train loss:  0.4443519413471222
train gradient:  0.3663125057152979
iteration : 2356
train acc:  0.84375
train loss:  0.37190550565719604
train gradient:  0.3838950224499304
iteration : 2357
train acc:  0.796875
train loss:  0.38643383979797363
train gradient:  0.3147519073711043
iteration : 2358
train acc:  0.78125
train loss:  0.4336462616920471
train gradient:  0.45009446762308936
iteration : 2359
train acc:  0.8359375
train loss:  0.31608718633651733
train gradient:  0.24547351372358672
iteration : 2360
train acc:  0.859375
train loss:  0.425534188747406
train gradient:  0.34449503311584345
iteration : 2361
train acc:  0.8203125
train loss:  0.385807603597641
train gradient:  0.3450962203935708
iteration : 2362
train acc:  0.8828125
train loss:  0.3341626524925232
train gradient:  0.26549680148372007
iteration : 2363
train acc:  0.8515625
train loss:  0.39617085456848145
train gradient:  0.45786615491091004
iteration : 2364
train acc:  0.8203125
train loss:  0.40866029262542725
train gradient:  0.3132241100841203
iteration : 2365
train acc:  0.8359375
train loss:  0.3704667389392853
train gradient:  0.3016414058786725
iteration : 2366
train acc:  0.8984375
train loss:  0.3144577145576477
train gradient:  0.36725061879829646
iteration : 2367
train acc:  0.8984375
train loss:  0.3244858980178833
train gradient:  0.29426977284363703
iteration : 2368
train acc:  0.8671875
train loss:  0.34082210063934326
train gradient:  0.26539936510770573
iteration : 2369
train acc:  0.796875
train loss:  0.457192599773407
train gradient:  0.49897530766113146
iteration : 2370
train acc:  0.8359375
train loss:  0.3307182192802429
train gradient:  0.32128643688146563
iteration : 2371
train acc:  0.875
train loss:  0.3323279619216919
train gradient:  0.21547171421981948
iteration : 2372
train acc:  0.859375
train loss:  0.3759332001209259
train gradient:  0.34375007231697513
iteration : 2373
train acc:  0.828125
train loss:  0.3480483889579773
train gradient:  0.29779868964292894
iteration : 2374
train acc:  0.8515625
train loss:  0.38228097558021545
train gradient:  0.29433765388567246
iteration : 2375
train acc:  0.7890625
train loss:  0.4448109567165375
train gradient:  0.5843313825611913
iteration : 2376
train acc:  0.8359375
train loss:  0.42770135402679443
train gradient:  0.40891196754301334
iteration : 2377
train acc:  0.8671875
train loss:  0.31007444858551025
train gradient:  0.23401713811232314
iteration : 2378
train acc:  0.90625
train loss:  0.31271442770957947
train gradient:  0.19834926386390764
iteration : 2379
train acc:  0.84375
train loss:  0.3370102643966675
train gradient:  0.3281518915020223
iteration : 2380
train acc:  0.859375
train loss:  0.3597942888736725
train gradient:  0.4187316378719588
iteration : 2381
train acc:  0.8203125
train loss:  0.36640429496765137
train gradient:  0.31928507130511535
iteration : 2382
train acc:  0.7421875
train loss:  0.48297518491744995
train gradient:  0.4541194915424804
iteration : 2383
train acc:  0.859375
train loss:  0.3251491189002991
train gradient:  0.26039503104125167
iteration : 2384
train acc:  0.84375
train loss:  0.446206271648407
train gradient:  0.4538655269580896
iteration : 2385
train acc:  0.8203125
train loss:  0.44134005904197693
train gradient:  0.5027830131664581
iteration : 2386
train acc:  0.8515625
train loss:  0.38628479838371277
train gradient:  0.41581128003342305
iteration : 2387
train acc:  0.8515625
train loss:  0.3994436264038086
train gradient:  0.4304118228220121
iteration : 2388
train acc:  0.84375
train loss:  0.35305291414260864
train gradient:  0.49770100996821504
iteration : 2389
train acc:  0.8203125
train loss:  0.43082889914512634
train gradient:  0.3713466980569331
iteration : 2390
train acc:  0.8515625
train loss:  0.3587301969528198
train gradient:  0.30436699598248873
iteration : 2391
train acc:  0.8359375
train loss:  0.36957821249961853
train gradient:  0.42347219536876557
iteration : 2392
train acc:  0.828125
train loss:  0.36402592062950134
train gradient:  0.4592163005286058
iteration : 2393
train acc:  0.8671875
train loss:  0.3216354250907898
train gradient:  0.2634181161065699
iteration : 2394
train acc:  0.8515625
train loss:  0.3735412359237671
train gradient:  0.36342139561380127
iteration : 2395
train acc:  0.84375
train loss:  0.3247206211090088
train gradient:  0.3139179027624183
iteration : 2396
train acc:  0.8515625
train loss:  0.35215139389038086
train gradient:  0.3461021860625858
iteration : 2397
train acc:  0.828125
train loss:  0.3877531588077545
train gradient:  0.3928556413011184
iteration : 2398
train acc:  0.8359375
train loss:  0.40007859468460083
train gradient:  0.48695912864235513
iteration : 2399
train acc:  0.8203125
train loss:  0.35886049270629883
train gradient:  0.39625502441947724
iteration : 2400
train acc:  0.828125
train loss:  0.3472754955291748
train gradient:  0.3706352281938434
iteration : 2401
train acc:  0.796875
train loss:  0.4096428155899048
train gradient:  0.4154853424424413
iteration : 2402
train acc:  0.7734375
train loss:  0.5191004276275635
train gradient:  0.5837904447987554
iteration : 2403
train acc:  0.796875
train loss:  0.3952258229255676
train gradient:  0.33439573068599826
iteration : 2404
train acc:  0.796875
train loss:  0.4136852025985718
train gradient:  0.44072605417701266
iteration : 2405
train acc:  0.78125
train loss:  0.4236209988594055
train gradient:  0.4928936256913773
iteration : 2406
train acc:  0.796875
train loss:  0.42351478338241577
train gradient:  0.479210824829191
iteration : 2407
train acc:  0.8515625
train loss:  0.3513573408126831
train gradient:  0.3020164379508977
iteration : 2408
train acc:  0.8125
train loss:  0.4248920679092407
train gradient:  0.3549244352495378
iteration : 2409
train acc:  0.7421875
train loss:  0.512961745262146
train gradient:  0.5881662634648993
iteration : 2410
train acc:  0.8125
train loss:  0.36745595932006836
train gradient:  0.4283689796026107
iteration : 2411
train acc:  0.796875
train loss:  0.3870142698287964
train gradient:  0.4546306960108964
iteration : 2412
train acc:  0.828125
train loss:  0.3474702537059784
train gradient:  0.33502285457634673
iteration : 2413
train acc:  0.8046875
train loss:  0.418476939201355
train gradient:  0.49217181096097196
iteration : 2414
train acc:  0.8125
train loss:  0.4088006913661957
train gradient:  0.3487409575985744
iteration : 2415
train acc:  0.890625
train loss:  0.2902337908744812
train gradient:  0.21523792824327584
iteration : 2416
train acc:  0.78125
train loss:  0.4558796286582947
train gradient:  0.5933977309641205
iteration : 2417
train acc:  0.8203125
train loss:  0.3937470316886902
train gradient:  0.426484001647443
iteration : 2418
train acc:  0.78125
train loss:  0.45687708258628845
train gradient:  0.436859012925757
iteration : 2419
train acc:  0.8203125
train loss:  0.3586726784706116
train gradient:  0.3822290004452105
iteration : 2420
train acc:  0.8671875
train loss:  0.3489341139793396
train gradient:  0.346410173347595
iteration : 2421
train acc:  0.78125
train loss:  0.45722776651382446
train gradient:  0.41217466245452666
iteration : 2422
train acc:  0.8125
train loss:  0.3788975477218628
train gradient:  0.24173645040787248
iteration : 2423
train acc:  0.8125
train loss:  0.4223897457122803
train gradient:  0.4500673664765579
iteration : 2424
train acc:  0.8046875
train loss:  0.4058803915977478
train gradient:  0.3202124165478069
iteration : 2425
train acc:  0.7734375
train loss:  0.4225413203239441
train gradient:  0.2871037847492513
iteration : 2426
train acc:  0.8203125
train loss:  0.3739034831523895
train gradient:  0.3715255573411195
iteration : 2427
train acc:  0.859375
train loss:  0.380575954914093
train gradient:  0.2951172491739044
iteration : 2428
train acc:  0.8125
train loss:  0.4142470955848694
train gradient:  0.33821362639045927
iteration : 2429
train acc:  0.7734375
train loss:  0.4254102110862732
train gradient:  0.5952397557813761
iteration : 2430
train acc:  0.8359375
train loss:  0.4081276059150696
train gradient:  0.29711449931899214
iteration : 2431
train acc:  0.828125
train loss:  0.36639684438705444
train gradient:  0.3547126240152852
iteration : 2432
train acc:  0.859375
train loss:  0.31562796235084534
train gradient:  0.20646304482813943
iteration : 2433
train acc:  0.796875
train loss:  0.44394439458847046
train gradient:  0.31060682357553815
iteration : 2434
train acc:  0.890625
train loss:  0.27059876918792725
train gradient:  0.24576033800375405
iteration : 2435
train acc:  0.796875
train loss:  0.414248526096344
train gradient:  0.3725280473786193
iteration : 2436
train acc:  0.8828125
train loss:  0.3228728175163269
train gradient:  0.30246950978750065
iteration : 2437
train acc:  0.8046875
train loss:  0.46762827038764954
train gradient:  0.4482996170918116
iteration : 2438
train acc:  0.8515625
train loss:  0.3110973834991455
train gradient:  0.24133528423526904
iteration : 2439
train acc:  0.84375
train loss:  0.36393433809280396
train gradient:  0.24318389421130135
iteration : 2440
train acc:  0.8125
train loss:  0.432195782661438
train gradient:  0.5243425334820755
iteration : 2441
train acc:  0.8828125
train loss:  0.3214544951915741
train gradient:  0.4053548830905403
iteration : 2442
train acc:  0.8046875
train loss:  0.46425479650497437
train gradient:  0.4119032313404091
iteration : 2443
train acc:  0.8203125
train loss:  0.3764832615852356
train gradient:  0.3527068544367398
iteration : 2444
train acc:  0.8203125
train loss:  0.3752017021179199
train gradient:  0.40834134895565727
iteration : 2445
train acc:  0.859375
train loss:  0.40601596236228943
train gradient:  0.4204993182544487
iteration : 2446
train acc:  0.8203125
train loss:  0.4397926926612854
train gradient:  0.5158379641782127
iteration : 2447
train acc:  0.7890625
train loss:  0.4032095670700073
train gradient:  0.2920905779956732
iteration : 2448
train acc:  0.78125
train loss:  0.5005397796630859
train gradient:  0.7843720491816053
iteration : 2449
train acc:  0.8046875
train loss:  0.40464550256729126
train gradient:  0.45468471586290626
iteration : 2450
train acc:  0.8046875
train loss:  0.4119192659854889
train gradient:  0.4958658838283809
iteration : 2451
train acc:  0.8359375
train loss:  0.3446274697780609
train gradient:  0.2209247708722238
iteration : 2452
train acc:  0.8359375
train loss:  0.3528077006340027
train gradient:  0.37304020896792034
iteration : 2453
train acc:  0.8671875
train loss:  0.3533164858818054
train gradient:  0.2793618338941337
iteration : 2454
train acc:  0.8359375
train loss:  0.3797009587287903
train gradient:  0.36036651967020306
iteration : 2455
train acc:  0.8515625
train loss:  0.31645774841308594
train gradient:  0.24995534936857153
iteration : 2456
train acc:  0.7890625
train loss:  0.42378926277160645
train gradient:  0.36912768005350344
iteration : 2457
train acc:  0.8125
train loss:  0.4295167326927185
train gradient:  0.4339038315133706
iteration : 2458
train acc:  0.78125
train loss:  0.4211304783821106
train gradient:  0.37968103657386065
iteration : 2459
train acc:  0.7890625
train loss:  0.4223058819770813
train gradient:  0.34156022132104696
iteration : 2460
train acc:  0.8046875
train loss:  0.37478190660476685
train gradient:  0.36899937585006304
iteration : 2461
train acc:  0.8359375
train loss:  0.37505388259887695
train gradient:  0.3363865542929677
iteration : 2462
train acc:  0.84375
train loss:  0.4215887784957886
train gradient:  0.5141502179523122
iteration : 2463
train acc:  0.8203125
train loss:  0.3750287890434265
train gradient:  0.2974268148853378
iteration : 2464
train acc:  0.75
train loss:  0.4586973190307617
train gradient:  0.47495604137286124
iteration : 2465
train acc:  0.8046875
train loss:  0.3966703414916992
train gradient:  0.3799515930467945
iteration : 2466
train acc:  0.8203125
train loss:  0.4345128834247589
train gradient:  0.6182035877502448
iteration : 2467
train acc:  0.7890625
train loss:  0.41213876008987427
train gradient:  0.5445451448521407
iteration : 2468
train acc:  0.859375
train loss:  0.393044650554657
train gradient:  0.39621574068724247
iteration : 2469
train acc:  0.8203125
train loss:  0.3953394293785095
train gradient:  0.44629489418509666
iteration : 2470
train acc:  0.8515625
train loss:  0.36359861493110657
train gradient:  0.4282859159134776
iteration : 2471
train acc:  0.8203125
train loss:  0.4512817859649658
train gradient:  0.3460192204807665
iteration : 2472
train acc:  0.8515625
train loss:  0.31693461537361145
train gradient:  0.2911557872660095
iteration : 2473
train acc:  0.8203125
train loss:  0.4192788898944855
train gradient:  0.4616441683368634
iteration : 2474
train acc:  0.828125
train loss:  0.42555761337280273
train gradient:  0.4262522058511773
iteration : 2475
train acc:  0.765625
train loss:  0.4418821930885315
train gradient:  0.628060136410083
iteration : 2476
train acc:  0.8203125
train loss:  0.42613211274147034
train gradient:  0.4816230230247257
iteration : 2477
train acc:  0.828125
train loss:  0.38339871168136597
train gradient:  0.3918885002512495
iteration : 2478
train acc:  0.796875
train loss:  0.41606736183166504
train gradient:  0.3389781066875417
iteration : 2479
train acc:  0.828125
train loss:  0.42158395051956177
train gradient:  0.31183112315411243
iteration : 2480
train acc:  0.8515625
train loss:  0.37297892570495605
train gradient:  0.36757734225922156
iteration : 2481
train acc:  0.78125
train loss:  0.46788227558135986
train gradient:  0.5345767114169371
iteration : 2482
train acc:  0.8125
train loss:  0.3841341435909271
train gradient:  0.5177062168717563
iteration : 2483
train acc:  0.8203125
train loss:  0.3879486322402954
train gradient:  0.3836842959797924
iteration : 2484
train acc:  0.796875
train loss:  0.39132505655288696
train gradient:  0.31767708425375984
iteration : 2485
train acc:  0.8203125
train loss:  0.36212843656539917
train gradient:  0.3150429628032491
iteration : 2486
train acc:  0.7890625
train loss:  0.395227313041687
train gradient:  0.4325322801132698
iteration : 2487
train acc:  0.8359375
train loss:  0.37875303626060486
train gradient:  0.415066635459108
iteration : 2488
train acc:  0.796875
train loss:  0.44473379850387573
train gradient:  0.4441900916295616
iteration : 2489
train acc:  0.765625
train loss:  0.4605010449886322
train gradient:  0.5114809117377135
iteration : 2490
train acc:  0.7890625
train loss:  0.4489342272281647
train gradient:  0.5238772149959211
iteration : 2491
train acc:  0.84375
train loss:  0.381351113319397
train gradient:  0.2822206667297373
iteration : 2492
train acc:  0.75
train loss:  0.4453399181365967
train gradient:  0.5734426820688532
iteration : 2493
train acc:  0.8359375
train loss:  0.41264668107032776
train gradient:  0.3009651719481649
iteration : 2494
train acc:  0.8125
train loss:  0.4167376160621643
train gradient:  0.3627735431452394
iteration : 2495
train acc:  0.8046875
train loss:  0.43253037333488464
train gradient:  0.3402262031222602
iteration : 2496
train acc:  0.8515625
train loss:  0.35822170972824097
train gradient:  0.3078207431355956
iteration : 2497
train acc:  0.796875
train loss:  0.4104476571083069
train gradient:  0.3264022755888393
iteration : 2498
train acc:  0.84375
train loss:  0.3114030957221985
train gradient:  0.26195488048545434
iteration : 2499
train acc:  0.859375
train loss:  0.37973079085350037
train gradient:  0.3141538997216827
iteration : 2500
train acc:  0.78125
train loss:  0.429818332195282
train gradient:  0.501034979694426
iteration : 2501
train acc:  0.8125
train loss:  0.43806689977645874
train gradient:  0.4031632484963539
iteration : 2502
train acc:  0.8046875
train loss:  0.36595618724823
train gradient:  0.3492156037685879
iteration : 2503
train acc:  0.859375
train loss:  0.3145086169242859
train gradient:  0.23964266150430225
iteration : 2504
train acc:  0.8359375
train loss:  0.3589893579483032
train gradient:  0.3022520762300523
iteration : 2505
train acc:  0.8515625
train loss:  0.3474065065383911
train gradient:  0.21797213895818562
iteration : 2506
train acc:  0.8046875
train loss:  0.4058116674423218
train gradient:  0.44387123161181075
iteration : 2507
train acc:  0.78125
train loss:  0.41421276330947876
train gradient:  0.37345261234571314
iteration : 2508
train acc:  0.828125
train loss:  0.37498193979263306
train gradient:  0.2918395538330622
iteration : 2509
train acc:  0.890625
train loss:  0.3109295964241028
train gradient:  0.29540643569080177
iteration : 2510
train acc:  0.8359375
train loss:  0.33970344066619873
train gradient:  0.3103208254694081
iteration : 2511
train acc:  0.890625
train loss:  0.34843599796295166
train gradient:  0.33502746505817615
iteration : 2512
train acc:  0.828125
train loss:  0.3224330544471741
train gradient:  0.30346430563916327
iteration : 2513
train acc:  0.75
train loss:  0.4834825396537781
train gradient:  0.6868262595554399
iteration : 2514
train acc:  0.7421875
train loss:  0.4566074013710022
train gradient:  0.50321274212925
iteration : 2515
train acc:  0.8203125
train loss:  0.44164326786994934
train gradient:  0.5139944924919024
iteration : 2516
train acc:  0.796875
train loss:  0.3919526934623718
train gradient:  0.39427673178079764
iteration : 2517
train acc:  0.84375
train loss:  0.3865482807159424
train gradient:  0.3407849398858988
iteration : 2518
train acc:  0.8046875
train loss:  0.4123380482196808
train gradient:  0.3970087986382629
iteration : 2519
train acc:  0.875
train loss:  0.3135839104652405
train gradient:  0.2254418679990872
iteration : 2520
train acc:  0.8203125
train loss:  0.3891263008117676
train gradient:  0.35414804944432615
iteration : 2521
train acc:  0.8671875
train loss:  0.3275876045227051
train gradient:  0.33451282881564187
iteration : 2522
train acc:  0.8046875
train loss:  0.38388076424598694
train gradient:  0.39331085623510054
iteration : 2523
train acc:  0.84375
train loss:  0.3668186068534851
train gradient:  0.26899163322518216
iteration : 2524
train acc:  0.828125
train loss:  0.31302064657211304
train gradient:  0.24005127559955708
iteration : 2525
train acc:  0.828125
train loss:  0.368956595659256
train gradient:  0.5075426187124975
iteration : 2526
train acc:  0.8515625
train loss:  0.35979846119880676
train gradient:  0.3247980002508773
iteration : 2527
train acc:  0.828125
train loss:  0.3737844228744507
train gradient:  0.3138474493078757
iteration : 2528
train acc:  0.875
train loss:  0.3025955557823181
train gradient:  0.20471041733256257
iteration : 2529
train acc:  0.8046875
train loss:  0.4239528179168701
train gradient:  0.45294806580854574
iteration : 2530
train acc:  0.7734375
train loss:  0.49490833282470703
train gradient:  0.3596182215175607
iteration : 2531
train acc:  0.8203125
train loss:  0.3837447166442871
train gradient:  0.5454575536527129
iteration : 2532
train acc:  0.8359375
train loss:  0.43637168407440186
train gradient:  0.4314424673263221
iteration : 2533
train acc:  0.875
train loss:  0.32072997093200684
train gradient:  0.20885416284158526
iteration : 2534
train acc:  0.8515625
train loss:  0.3201133608818054
train gradient:  0.3678780576180625
iteration : 2535
train acc:  0.7890625
train loss:  0.48220908641815186
train gradient:  0.45171018177396083
iteration : 2536
train acc:  0.828125
train loss:  0.3609592020511627
train gradient:  0.48895185467235636
iteration : 2537
train acc:  0.8046875
train loss:  0.37008771300315857
train gradient:  0.37761763139296944
iteration : 2538
train acc:  0.796875
train loss:  0.41695332527160645
train gradient:  0.40142262911642534
iteration : 2539
train acc:  0.8359375
train loss:  0.44267505407333374
train gradient:  0.4512058961145661
iteration : 2540
train acc:  0.8671875
train loss:  0.3014245331287384
train gradient:  0.23706822916670134
iteration : 2541
train acc:  0.828125
train loss:  0.37081179022789
train gradient:  0.3905753066862512
iteration : 2542
train acc:  0.8125
train loss:  0.38032054901123047
train gradient:  0.4403093754722356
iteration : 2543
train acc:  0.8203125
train loss:  0.4106065630912781
train gradient:  0.44499273700663305
iteration : 2544
train acc:  0.828125
train loss:  0.3333974778652191
train gradient:  0.3178861353203717
iteration : 2545
train acc:  0.8515625
train loss:  0.3903399407863617
train gradient:  0.4394512900046429
iteration : 2546
train acc:  0.8125
train loss:  0.3826431930065155
train gradient:  0.2759654418445798
iteration : 2547
train acc:  0.8515625
train loss:  0.3694682717323303
train gradient:  0.4253367213742137
iteration : 2548
train acc:  0.8046875
train loss:  0.3989221453666687
train gradient:  0.5724121585344575
iteration : 2549
train acc:  0.8359375
train loss:  0.31882375478744507
train gradient:  0.3556243495516282
iteration : 2550
train acc:  0.8203125
train loss:  0.3494085371494293
train gradient:  0.2820364173899103
iteration : 2551
train acc:  0.8203125
train loss:  0.35636186599731445
train gradient:  0.36641036861754295
iteration : 2552
train acc:  0.796875
train loss:  0.42279255390167236
train gradient:  0.47977648091157415
iteration : 2553
train acc:  0.8125
train loss:  0.4121479392051697
train gradient:  0.39242045166170764
iteration : 2554
train acc:  0.8359375
train loss:  0.38025927543640137
train gradient:  0.4608864493430674
iteration : 2555
train acc:  0.8046875
train loss:  0.4260687232017517
train gradient:  0.5055528685933006
iteration : 2556
train acc:  0.7109375
train loss:  0.496712863445282
train gradient:  0.4779738522290282
iteration : 2557
train acc:  0.8359375
train loss:  0.43664979934692383
train gradient:  0.4987479409905249
iteration : 2558
train acc:  0.8046875
train loss:  0.38233524560928345
train gradient:  0.45245306119584255
iteration : 2559
train acc:  0.8203125
train loss:  0.4649352729320526
train gradient:  0.5473074637391251
iteration : 2560
train acc:  0.8203125
train loss:  0.3700341582298279
train gradient:  0.43857951211371865
iteration : 2561
train acc:  0.859375
train loss:  0.34811219573020935
train gradient:  0.25849877911494024
iteration : 2562
train acc:  0.7890625
train loss:  0.4185883700847626
train gradient:  0.3709997962765639
iteration : 2563
train acc:  0.8515625
train loss:  0.351923406124115
train gradient:  0.30578233900369733
iteration : 2564
train acc:  0.8203125
train loss:  0.3722350597381592
train gradient:  0.24440331655282285
iteration : 2565
train acc:  0.8671875
train loss:  0.36256998777389526
train gradient:  0.39881520312928714
iteration : 2566
train acc:  0.8203125
train loss:  0.45632144808769226
train gradient:  0.49160382812340814
iteration : 2567
train acc:  0.8359375
train loss:  0.3600006103515625
train gradient:  0.3100770006830766
iteration : 2568
train acc:  0.875
train loss:  0.34627461433410645
train gradient:  0.26775340049362
iteration : 2569
train acc:  0.8515625
train loss:  0.3805265426635742
train gradient:  0.37390260280866866
iteration : 2570
train acc:  0.8828125
train loss:  0.33296918869018555
train gradient:  0.47739281102666126
iteration : 2571
train acc:  0.8359375
train loss:  0.36685100197792053
train gradient:  0.243345322979384
iteration : 2572
train acc:  0.8203125
train loss:  0.4216800332069397
train gradient:  0.5217755402782076
iteration : 2573
train acc:  0.8125
train loss:  0.36670824885368347
train gradient:  0.40675376691710974
iteration : 2574
train acc:  0.8515625
train loss:  0.3960533142089844
train gradient:  0.3223656228660044
iteration : 2575
train acc:  0.84375
train loss:  0.43428516387939453
train gradient:  0.48717865735525867
iteration : 2576
train acc:  0.8828125
train loss:  0.33980920910835266
train gradient:  0.31276329767244215
iteration : 2577
train acc:  0.8203125
train loss:  0.388498455286026
train gradient:  0.38019084844231527
iteration : 2578
train acc:  0.8359375
train loss:  0.3599148392677307
train gradient:  0.6249742729546247
iteration : 2579
train acc:  0.8515625
train loss:  0.32863250374794006
train gradient:  0.22024771307960506
iteration : 2580
train acc:  0.8359375
train loss:  0.40469276905059814
train gradient:  0.35041954436599393
iteration : 2581
train acc:  0.796875
train loss:  0.48753392696380615
train gradient:  0.49286312219273254
iteration : 2582
train acc:  0.8203125
train loss:  0.3908483386039734
train gradient:  0.36235405610094124
iteration : 2583
train acc:  0.8515625
train loss:  0.3591254949569702
train gradient:  0.32285622702194705
iteration : 2584
train acc:  0.8046875
train loss:  0.4344649612903595
train gradient:  0.4053785948475358
iteration : 2585
train acc:  0.8359375
train loss:  0.4034842848777771
train gradient:  0.4747920945522867
iteration : 2586
train acc:  0.8671875
train loss:  0.304362416267395
train gradient:  0.2898392048716322
iteration : 2587
train acc:  0.7890625
train loss:  0.4310743808746338
train gradient:  0.35707169634411745
iteration : 2588
train acc:  0.8359375
train loss:  0.33082395792007446
train gradient:  0.24297967113005342
iteration : 2589
train acc:  0.7578125
train loss:  0.45916229486465454
train gradient:  0.4776655197338639
iteration : 2590
train acc:  0.8203125
train loss:  0.3955574631690979
train gradient:  0.4052731475958604
iteration : 2591
train acc:  0.8515625
train loss:  0.3176881968975067
train gradient:  0.3794636320193777
iteration : 2592
train acc:  0.7421875
train loss:  0.47987207770347595
train gradient:  0.5562173491036666
iteration : 2593
train acc:  0.8515625
train loss:  0.33822721242904663
train gradient:  0.26610348458325783
iteration : 2594
train acc:  0.828125
train loss:  0.36949944496154785
train gradient:  0.3823877336142945
iteration : 2595
train acc:  0.828125
train loss:  0.42185544967651367
train gradient:  0.3169511182800765
iteration : 2596
train acc:  0.7578125
train loss:  0.4692252576351166
train gradient:  0.4879115093375705
iteration : 2597
train acc:  0.8515625
train loss:  0.3908451795578003
train gradient:  0.3903475193757289
iteration : 2598
train acc:  0.859375
train loss:  0.32771071791648865
train gradient:  0.3723159496986593
iteration : 2599
train acc:  0.7734375
train loss:  0.4300112724304199
train gradient:  0.6550244580460916
iteration : 2600
train acc:  0.8515625
train loss:  0.3572874665260315
train gradient:  0.38078684741581437
iteration : 2601
train acc:  0.84375
train loss:  0.41517373919487
train gradient:  0.3986034850780659
iteration : 2602
train acc:  0.8125
train loss:  0.40965279936790466
train gradient:  0.4142753304045901
iteration : 2603
train acc:  0.8515625
train loss:  0.3190639913082123
train gradient:  0.2794516045088018
iteration : 2604
train acc:  0.828125
train loss:  0.38909751176834106
train gradient:  0.49379843204715956
iteration : 2605
train acc:  0.84375
train loss:  0.35357069969177246
train gradient:  0.30725364457907084
iteration : 2606
train acc:  0.78125
train loss:  0.4147692620754242
train gradient:  0.279162345658291
iteration : 2607
train acc:  0.8359375
train loss:  0.36990612745285034
train gradient:  0.3022266544870308
iteration : 2608
train acc:  0.7890625
train loss:  0.41754239797592163
train gradient:  0.39660401733771145
iteration : 2609
train acc:  0.8359375
train loss:  0.349373459815979
train gradient:  0.28372107363804383
iteration : 2610
train acc:  0.828125
train loss:  0.3278827667236328
train gradient:  0.40403861912074984
iteration : 2611
train acc:  0.8984375
train loss:  0.28586632013320923
train gradient:  0.3389914906945336
iteration : 2612
train acc:  0.7734375
train loss:  0.48546460270881653
train gradient:  0.6187066962508361
iteration : 2613
train acc:  0.8515625
train loss:  0.3443235158920288
train gradient:  0.3850114933320649
iteration : 2614
train acc:  0.8125
train loss:  0.47410857677459717
train gradient:  0.5016536266667759
iteration : 2615
train acc:  0.7890625
train loss:  0.4335467517375946
train gradient:  0.34593960607693186
iteration : 2616
train acc:  0.8359375
train loss:  0.3575543761253357
train gradient:  0.2750966047691025
iteration : 2617
train acc:  0.890625
train loss:  0.3211047053337097
train gradient:  0.30918108201889494
iteration : 2618
train acc:  0.75
train loss:  0.4871257543563843
train gradient:  0.6306560277965927
iteration : 2619
train acc:  0.828125
train loss:  0.3980792760848999
train gradient:  0.3800990119699871
iteration : 2620
train acc:  0.796875
train loss:  0.4691948890686035
train gradient:  0.4668591743166166
iteration : 2621
train acc:  0.8671875
train loss:  0.3723207116127014
train gradient:  0.27522653220304916
iteration : 2622
train acc:  0.8203125
train loss:  0.35362714529037476
train gradient:  0.3096377541552085
iteration : 2623
train acc:  0.8125
train loss:  0.34332865476608276
train gradient:  0.2668614225835359
iteration : 2624
train acc:  0.828125
train loss:  0.46409541368484497
train gradient:  0.4691608743103178
iteration : 2625
train acc:  0.734375
train loss:  0.4967920780181885
train gradient:  0.44105242073203793
iteration : 2626
train acc:  0.84375
train loss:  0.4276777505874634
train gradient:  0.4891732943458777
iteration : 2627
train acc:  0.859375
train loss:  0.36161908507347107
train gradient:  0.2687510963677674
iteration : 2628
train acc:  0.84375
train loss:  0.3714700937271118
train gradient:  0.3380935971526601
iteration : 2629
train acc:  0.796875
train loss:  0.4180874228477478
train gradient:  0.41987794260615785
iteration : 2630
train acc:  0.859375
train loss:  0.3480430245399475
train gradient:  0.32165635048215885
iteration : 2631
train acc:  0.8203125
train loss:  0.3902263343334198
train gradient:  0.3398639534021685
iteration : 2632
train acc:  0.84375
train loss:  0.33599644899368286
train gradient:  0.23852944943436957
iteration : 2633
train acc:  0.8359375
train loss:  0.4236838221549988
train gradient:  0.6470829219416815
iteration : 2634
train acc:  0.7734375
train loss:  0.4493250846862793
train gradient:  0.3288963894098589
iteration : 2635
train acc:  0.7734375
train loss:  0.4256609082221985
train gradient:  0.4118573621335614
iteration : 2636
train acc:  0.8515625
train loss:  0.36573344469070435
train gradient:  0.26507042449416374
iteration : 2637
train acc:  0.8046875
train loss:  0.4159780740737915
train gradient:  0.4786229415280594
iteration : 2638
train acc:  0.8125
train loss:  0.3950589895248413
train gradient:  0.35488437592269895
iteration : 2639
train acc:  0.8515625
train loss:  0.3358461558818817
train gradient:  0.2667942187785152
iteration : 2640
train acc:  0.8359375
train loss:  0.363128662109375
train gradient:  0.28509380984411337
iteration : 2641
train acc:  0.859375
train loss:  0.3311225473880768
train gradient:  0.2774348580468243
iteration : 2642
train acc:  0.765625
train loss:  0.41906648874282837
train gradient:  0.6202314343504971
iteration : 2643
train acc:  0.8125
train loss:  0.4126119613647461
train gradient:  0.5041306460968921
iteration : 2644
train acc:  0.84375
train loss:  0.3262241780757904
train gradient:  0.4874485417036709
iteration : 2645
train acc:  0.8359375
train loss:  0.42334163188934326
train gradient:  0.3404748067927191
iteration : 2646
train acc:  0.8203125
train loss:  0.4707797169685364
train gradient:  0.49019152616849093
iteration : 2647
train acc:  0.875
train loss:  0.319672167301178
train gradient:  0.30101126011347884
iteration : 2648
train acc:  0.828125
train loss:  0.38207098841667175
train gradient:  0.33349047769442375
iteration : 2649
train acc:  0.828125
train loss:  0.34074917435646057
train gradient:  0.2718087395108162
iteration : 2650
train acc:  0.84375
train loss:  0.378192275762558
train gradient:  0.2699485510881683
iteration : 2651
train acc:  0.90625
train loss:  0.3224015235900879
train gradient:  0.2826973303804514
iteration : 2652
train acc:  0.828125
train loss:  0.3879871964454651
train gradient:  0.4535171814929854
iteration : 2653
train acc:  0.8046875
train loss:  0.4638044536113739
train gradient:  0.3626849730403586
iteration : 2654
train acc:  0.8671875
train loss:  0.3067595660686493
train gradient:  0.1910113524845987
iteration : 2655
train acc:  0.796875
train loss:  0.4155721664428711
train gradient:  0.37170487840276883
iteration : 2656
train acc:  0.8515625
train loss:  0.3311828374862671
train gradient:  0.3029375411501369
iteration : 2657
train acc:  0.875
train loss:  0.3796195387840271
train gradient:  0.2569301925083227
iteration : 2658
train acc:  0.796875
train loss:  0.4590795636177063
train gradient:  0.49205079011319697
iteration : 2659
train acc:  0.8984375
train loss:  0.3411683738231659
train gradient:  0.27554517650231825
iteration : 2660
train acc:  0.8125
train loss:  0.40467673540115356
train gradient:  0.3406655390965131
iteration : 2661
train acc:  0.875
train loss:  0.3611309826374054
train gradient:  0.37297274725373464
iteration : 2662
train acc:  0.8671875
train loss:  0.375842422246933
train gradient:  0.3893474404790799
iteration : 2663
train acc:  0.828125
train loss:  0.38818594813346863
train gradient:  0.40923849845860005
iteration : 2664
train acc:  0.84375
train loss:  0.33860844373703003
train gradient:  0.3050767641618516
iteration : 2665
train acc:  0.8203125
train loss:  0.4121673107147217
train gradient:  0.4992891843641515
iteration : 2666
train acc:  0.8828125
train loss:  0.2910733222961426
train gradient:  0.34549575929285753
iteration : 2667
train acc:  0.828125
train loss:  0.3776445686817169
train gradient:  0.33114389945983674
iteration : 2668
train acc:  0.8203125
train loss:  0.38890087604522705
train gradient:  0.30302088153474266
iteration : 2669
train acc:  0.875
train loss:  0.3344663381576538
train gradient:  0.318305820694213
iteration : 2670
train acc:  0.8125
train loss:  0.3819498121738434
train gradient:  0.5042828372420123
iteration : 2671
train acc:  0.8359375
train loss:  0.3237442970275879
train gradient:  0.28437653064469154
iteration : 2672
train acc:  0.859375
train loss:  0.31477922201156616
train gradient:  0.3025728606564625
iteration : 2673
train acc:  0.796875
train loss:  0.4416424036026001
train gradient:  0.48006890286341447
iteration : 2674
train acc:  0.828125
train loss:  0.3567398488521576
train gradient:  0.3155570475797547
iteration : 2675
train acc:  0.8125
train loss:  0.4169420599937439
train gradient:  0.4424408993611409
iteration : 2676
train acc:  0.8125
train loss:  0.3829463720321655
train gradient:  0.3718806392358118
iteration : 2677
train acc:  0.78125
train loss:  0.43738219141960144
train gradient:  0.44485220223667366
iteration : 2678
train acc:  0.84375
train loss:  0.3688673675060272
train gradient:  0.3333013268118405
iteration : 2679
train acc:  0.7578125
train loss:  0.4580845832824707
train gradient:  0.6298542608543833
iteration : 2680
train acc:  0.7890625
train loss:  0.48222970962524414
train gradient:  0.5494332827778727
iteration : 2681
train acc:  0.8515625
train loss:  0.3098045289516449
train gradient:  0.3578703938481738
iteration : 2682
train acc:  0.7890625
train loss:  0.41998350620269775
train gradient:  0.44142175296851244
iteration : 2683
train acc:  0.828125
train loss:  0.40220001339912415
train gradient:  0.361530476390936
iteration : 2684
train acc:  0.828125
train loss:  0.3469260334968567
train gradient:  0.3440539358736389
iteration : 2685
train acc:  0.8671875
train loss:  0.36394983530044556
train gradient:  0.30630587026158224
iteration : 2686
train acc:  0.84375
train loss:  0.3512069880962372
train gradient:  0.3310459658550749
iteration : 2687
train acc:  0.84375
train loss:  0.4105222225189209
train gradient:  0.5316788945246018
iteration : 2688
train acc:  0.8203125
train loss:  0.4096626043319702
train gradient:  0.29462061574290904
iteration : 2689
train acc:  0.859375
train loss:  0.3535125255584717
train gradient:  0.3890056256049483
iteration : 2690
train acc:  0.75
train loss:  0.5815417766571045
train gradient:  0.6922023001890273
iteration : 2691
train acc:  0.8203125
train loss:  0.3161751925945282
train gradient:  0.30923786800080144
iteration : 2692
train acc:  0.75
train loss:  0.4323778450489044
train gradient:  0.31898474573548957
iteration : 2693
train acc:  0.84375
train loss:  0.33363571763038635
train gradient:  0.26045082555462884
iteration : 2694
train acc:  0.8046875
train loss:  0.3905044496059418
train gradient:  0.41193851580339674
iteration : 2695
train acc:  0.8046875
train loss:  0.41422319412231445
train gradient:  0.5223588382282474
iteration : 2696
train acc:  0.8203125
train loss:  0.43782007694244385
train gradient:  0.47067154126610705
iteration : 2697
train acc:  0.7890625
train loss:  0.42785075306892395
train gradient:  0.5817718444313822
iteration : 2698
train acc:  0.8125
train loss:  0.38595372438430786
train gradient:  0.31030663654122986
iteration : 2699
train acc:  0.78125
train loss:  0.4382554292678833
train gradient:  0.3777099559516768
iteration : 2700
train acc:  0.796875
train loss:  0.40070509910583496
train gradient:  0.428153865854753
iteration : 2701
train acc:  0.8203125
train loss:  0.4082473814487457
train gradient:  0.34328268753293495
iteration : 2702
train acc:  0.8515625
train loss:  0.4140223264694214
train gradient:  0.405151438894657
iteration : 2703
train acc:  0.875
train loss:  0.3236609399318695
train gradient:  0.2671081887997247
iteration : 2704
train acc:  0.8046875
train loss:  0.3878598213195801
train gradient:  0.3127168652722591
iteration : 2705
train acc:  0.7734375
train loss:  0.4883933663368225
train gradient:  0.5510279234981537
iteration : 2706
train acc:  0.796875
train loss:  0.405217707157135
train gradient:  0.46018396736145767
iteration : 2707
train acc:  0.8203125
train loss:  0.4181583523750305
train gradient:  0.2901204788726139
iteration : 2708
train acc:  0.78125
train loss:  0.43470141291618347
train gradient:  0.4340367246650708
iteration : 2709
train acc:  0.859375
train loss:  0.39099714159965515
train gradient:  0.2617615403391223
iteration : 2710
train acc:  0.8515625
train loss:  0.3461545705795288
train gradient:  0.2234279377822513
iteration : 2711
train acc:  0.8515625
train loss:  0.3427281677722931
train gradient:  0.2565065004438705
iteration : 2712
train acc:  0.75
train loss:  0.4455088973045349
train gradient:  0.39354295901557884
iteration : 2713
train acc:  0.8671875
train loss:  0.3993063271045685
train gradient:  0.4104844508869241
iteration : 2714
train acc:  0.7890625
train loss:  0.35630932450294495
train gradient:  0.30225337891967313
iteration : 2715
train acc:  0.7890625
train loss:  0.5090643763542175
train gradient:  0.5495004240666943
iteration : 2716
train acc:  0.7890625
train loss:  0.47181323170661926
train gradient:  0.4019576146164841
iteration : 2717
train acc:  0.8046875
train loss:  0.40805038809776306
train gradient:  0.31681421997199133
iteration : 2718
train acc:  0.859375
train loss:  0.3478044271469116
train gradient:  0.22445524775841982
iteration : 2719
train acc:  0.84375
train loss:  0.3523561358451843
train gradient:  0.2370961379348575
iteration : 2720
train acc:  0.84375
train loss:  0.32053661346435547
train gradient:  0.22463378699315195
iteration : 2721
train acc:  0.828125
train loss:  0.36370423436164856
train gradient:  0.31050749070727784
iteration : 2722
train acc:  0.8359375
train loss:  0.3517850637435913
train gradient:  0.28193258714679353
iteration : 2723
train acc:  0.8203125
train loss:  0.4013614058494568
train gradient:  0.30459544417809176
iteration : 2724
train acc:  0.8125
train loss:  0.35925543308258057
train gradient:  0.336201670627431
iteration : 2725
train acc:  0.8515625
train loss:  0.331795871257782
train gradient:  0.23067942076308462
iteration : 2726
train acc:  0.8125
train loss:  0.3992355167865753
train gradient:  0.37623800703992133
iteration : 2727
train acc:  0.8671875
train loss:  0.3715917766094208
train gradient:  0.2243698424322155
iteration : 2728
train acc:  0.859375
train loss:  0.3424443006515503
train gradient:  0.27958691860978024
iteration : 2729
train acc:  0.859375
train loss:  0.33976125717163086
train gradient:  0.18590093684541625
iteration : 2730
train acc:  0.828125
train loss:  0.3540777564048767
train gradient:  0.23832299852671873
iteration : 2731
train acc:  0.828125
train loss:  0.40985697507858276
train gradient:  0.4494889563865086
iteration : 2732
train acc:  0.8125
train loss:  0.45288458466529846
train gradient:  0.44398317275102395
iteration : 2733
train acc:  0.84375
train loss:  0.35099053382873535
train gradient:  0.2948971004577184
iteration : 2734
train acc:  0.8046875
train loss:  0.44541794061660767
train gradient:  0.3839709315123945
iteration : 2735
train acc:  0.8671875
train loss:  0.29337289929389954
train gradient:  0.19566599400630014
iteration : 2736
train acc:  0.859375
train loss:  0.3061443865299225
train gradient:  0.19239622240166016
iteration : 2737
train acc:  0.828125
train loss:  0.3684495687484741
train gradient:  0.31157408293425054
iteration : 2738
train acc:  0.8125
train loss:  0.43532049655914307
train gradient:  0.5185641512523473
iteration : 2739
train acc:  0.8515625
train loss:  0.3463376760482788
train gradient:  0.3349147761037547
iteration : 2740
train acc:  0.8203125
train loss:  0.4192565381526947
train gradient:  0.38952072137425003
iteration : 2741
train acc:  0.875
train loss:  0.31419679522514343
train gradient:  0.2386919429117371
iteration : 2742
train acc:  0.8515625
train loss:  0.34600600600242615
train gradient:  0.2720852037304254
iteration : 2743
train acc:  0.8203125
train loss:  0.43174275755882263
train gradient:  0.3210910800858628
iteration : 2744
train acc:  0.8359375
train loss:  0.3926566541194916
train gradient:  0.2870251989304389
iteration : 2745
train acc:  0.796875
train loss:  0.3844262659549713
train gradient:  0.37515226566033866
iteration : 2746
train acc:  0.8671875
train loss:  0.36118537187576294
train gradient:  0.315473993477773
iteration : 2747
train acc:  0.828125
train loss:  0.40093809366226196
train gradient:  0.4293341689318671
iteration : 2748
train acc:  0.8359375
train loss:  0.3787447512149811
train gradient:  0.34482785795907084
iteration : 2749
train acc:  0.8671875
train loss:  0.37164825201034546
train gradient:  0.3113995323233242
iteration : 2750
train acc:  0.859375
train loss:  0.3496204614639282
train gradient:  0.2620074061293192
iteration : 2751
train acc:  0.8515625
train loss:  0.4125235676765442
train gradient:  0.4451388250375882
iteration : 2752
train acc:  0.828125
train loss:  0.3919757604598999
train gradient:  0.40618269531651313
iteration : 2753
train acc:  0.875
train loss:  0.3463040590286255
train gradient:  0.23938724089737515
iteration : 2754
train acc:  0.8671875
train loss:  0.33948248624801636
train gradient:  0.29704140028112264
iteration : 2755
train acc:  0.859375
train loss:  0.36536863446235657
train gradient:  0.27108457245874934
iteration : 2756
train acc:  0.8515625
train loss:  0.3686688542366028
train gradient:  0.33029038703876723
iteration : 2757
train acc:  0.8515625
train loss:  0.33953940868377686
train gradient:  0.2771989685103906
iteration : 2758
train acc:  0.7734375
train loss:  0.4473060667514801
train gradient:  0.5348091876638487
iteration : 2759
train acc:  0.78125
train loss:  0.4654117524623871
train gradient:  0.5583703041201126
iteration : 2760
train acc:  0.859375
train loss:  0.36301708221435547
train gradient:  0.30484207952091946
iteration : 2761
train acc:  0.859375
train loss:  0.4248431921005249
train gradient:  0.3754940516793461
iteration : 2762
train acc:  0.796875
train loss:  0.4083441495895386
train gradient:  0.38605775598637954
iteration : 2763
train acc:  0.7578125
train loss:  0.4785667657852173
train gradient:  0.46604196950173793
iteration : 2764
train acc:  0.84375
train loss:  0.35783812403678894
train gradient:  0.41591236027497225
iteration : 2765
train acc:  0.8671875
train loss:  0.3147848844528198
train gradient:  0.23732490643567647
iteration : 2766
train acc:  0.828125
train loss:  0.36607682704925537
train gradient:  0.3783350327499525
iteration : 2767
train acc:  0.8828125
train loss:  0.28676995635032654
train gradient:  0.23437978499536524
iteration : 2768
train acc:  0.84375
train loss:  0.3566223978996277
train gradient:  0.33210611212096375
iteration : 2769
train acc:  0.8046875
train loss:  0.39557138085365295
train gradient:  0.32103403795886454
iteration : 2770
train acc:  0.828125
train loss:  0.37734490633010864
train gradient:  0.37654929169376894
iteration : 2771
train acc:  0.796875
train loss:  0.4811531901359558
train gradient:  0.6421801474830651
iteration : 2772
train acc:  0.8828125
train loss:  0.34764987230300903
train gradient:  0.2913728953876974
iteration : 2773
train acc:  0.828125
train loss:  0.3923666179180145
train gradient:  0.35863536301665305
iteration : 2774
train acc:  0.890625
train loss:  0.31750667095184326
train gradient:  0.2984102267238436
iteration : 2775
train acc:  0.7890625
train loss:  0.43967390060424805
train gradient:  0.6453196961305354
iteration : 2776
train acc:  0.84375
train loss:  0.42745548486709595
train gradient:  0.4771901643739615
iteration : 2777
train acc:  0.8125
train loss:  0.40844589471817017
train gradient:  0.5287923850024198
iteration : 2778
train acc:  0.8203125
train loss:  0.39100781083106995
train gradient:  0.3805048902996234
iteration : 2779
train acc:  0.859375
train loss:  0.34500694274902344
train gradient:  0.2867945050074735
iteration : 2780
train acc:  0.828125
train loss:  0.3831137716770172
train gradient:  0.4219696821166114
iteration : 2781
train acc:  0.78125
train loss:  0.4478808641433716
train gradient:  0.5079566437467493
iteration : 2782
train acc:  0.8359375
train loss:  0.39225471019744873
train gradient:  0.3269636497543347
iteration : 2783
train acc:  0.8828125
train loss:  0.2753176689147949
train gradient:  0.23274791209158027
iteration : 2784
train acc:  0.84375
train loss:  0.3668287694454193
train gradient:  0.46407452613969596
iteration : 2785
train acc:  0.78125
train loss:  0.41818395256996155
train gradient:  0.3948118106580396
iteration : 2786
train acc:  0.828125
train loss:  0.4031935930252075
train gradient:  0.3252451198043926
iteration : 2787
train acc:  0.8515625
train loss:  0.3386099338531494
train gradient:  0.27356577762604006
iteration : 2788
train acc:  0.8203125
train loss:  0.42050665616989136
train gradient:  0.3251424057992932
iteration : 2789
train acc:  0.8046875
train loss:  0.41835325956344604
train gradient:  0.48064709799813754
iteration : 2790
train acc:  0.796875
train loss:  0.38141247630119324
train gradient:  0.380230440339037
iteration : 2791
train acc:  0.8671875
train loss:  0.35514697432518005
train gradient:  0.3148671034810599
iteration : 2792
train acc:  0.84375
train loss:  0.40499377250671387
train gradient:  0.35425827803459575
iteration : 2793
train acc:  0.796875
train loss:  0.4391566216945648
train gradient:  0.3595269830966505
iteration : 2794
train acc:  0.8125
train loss:  0.37897586822509766
train gradient:  0.33912817924415384
iteration : 2795
train acc:  0.8046875
train loss:  0.44944918155670166
train gradient:  0.41805001458189145
iteration : 2796
train acc:  0.84375
train loss:  0.3634079396724701
train gradient:  0.26197238881794943
iteration : 2797
train acc:  0.8515625
train loss:  0.32041090726852417
train gradient:  0.21515218986606172
iteration : 2798
train acc:  0.8046875
train loss:  0.3607024550437927
train gradient:  0.4298023000661183
iteration : 2799
train acc:  0.796875
train loss:  0.4064965844154358
train gradient:  0.42958326679460246
iteration : 2800
train acc:  0.84375
train loss:  0.37825530767440796
train gradient:  0.36114115839945427
iteration : 2801
train acc:  0.8125
train loss:  0.3993164896965027
train gradient:  0.472355171452207
iteration : 2802
train acc:  0.8671875
train loss:  0.32479727268218994
train gradient:  0.2506420558153639
iteration : 2803
train acc:  0.859375
train loss:  0.33250147104263306
train gradient:  0.2892243197656338
iteration : 2804
train acc:  0.8125
train loss:  0.4094945788383484
train gradient:  0.34946263152550516
iteration : 2805
train acc:  0.8671875
train loss:  0.3213486671447754
train gradient:  0.3047295585344896
iteration : 2806
train acc:  0.828125
train loss:  0.4097316265106201
train gradient:  0.37190719951186724
iteration : 2807
train acc:  0.8515625
train loss:  0.4356958270072937
train gradient:  0.4443884026379334
iteration : 2808
train acc:  0.8125
train loss:  0.435743123292923
train gradient:  0.6798771981825911
iteration : 2809
train acc:  0.78125
train loss:  0.4128739833831787
train gradient:  0.48456859710461997
iteration : 2810
train acc:  0.8828125
train loss:  0.3342859745025635
train gradient:  0.3310988868434574
iteration : 2811
train acc:  0.8203125
train loss:  0.3976684510707855
train gradient:  0.27495705805345666
iteration : 2812
train acc:  0.8515625
train loss:  0.3529515266418457
train gradient:  0.30873439664047064
iteration : 2813
train acc:  0.859375
train loss:  0.357453852891922
train gradient:  0.251678437478318
iteration : 2814
train acc:  0.828125
train loss:  0.4333523213863373
train gradient:  0.4375292125385179
iteration : 2815
train acc:  0.8125
train loss:  0.42733901739120483
train gradient:  0.48323326840733705
iteration : 2816
train acc:  0.7890625
train loss:  0.4063454866409302
train gradient:  0.34382369757800463
iteration : 2817
train acc:  0.828125
train loss:  0.38611969351768494
train gradient:  0.40277058494679274
iteration : 2818
train acc:  0.828125
train loss:  0.4003142714500427
train gradient:  0.36952760831867754
iteration : 2819
train acc:  0.78125
train loss:  0.38227033615112305
train gradient:  0.32913129479059666
iteration : 2820
train acc:  0.8125
train loss:  0.3836989104747772
train gradient:  0.3480498537494021
iteration : 2821
train acc:  0.8515625
train loss:  0.3245852589607239
train gradient:  0.22320999577415512
iteration : 2822
train acc:  0.84375
train loss:  0.33470532298088074
train gradient:  0.24835753423783946
iteration : 2823
train acc:  0.8359375
train loss:  0.3624382019042969
train gradient:  0.31050990610239054
iteration : 2824
train acc:  0.8671875
train loss:  0.31835678219795227
train gradient:  0.2246386504328972
iteration : 2825
train acc:  0.84375
train loss:  0.35669654607772827
train gradient:  0.2114588514062951
iteration : 2826
train acc:  0.84375
train loss:  0.39287739992141724
train gradient:  0.3748697745700594
iteration : 2827
train acc:  0.765625
train loss:  0.39082929491996765
train gradient:  0.36321434439546224
iteration : 2828
train acc:  0.8671875
train loss:  0.3277626633644104
train gradient:  0.34709993225003577
iteration : 2829
train acc:  0.859375
train loss:  0.3973291516304016
train gradient:  0.3628648191770146
iteration : 2830
train acc:  0.796875
train loss:  0.48076432943344116
train gradient:  0.5131835238254249
iteration : 2831
train acc:  0.8828125
train loss:  0.34103456139564514
train gradient:  0.35349536585369484
iteration : 2832
train acc:  0.8203125
train loss:  0.3638930916786194
train gradient:  0.4052859172344179
iteration : 2833
train acc:  0.7734375
train loss:  0.5062923431396484
train gradient:  0.5924808998734716
iteration : 2834
train acc:  0.7890625
train loss:  0.3611077070236206
train gradient:  0.4343896454214655
iteration : 2835
train acc:  0.8046875
train loss:  0.3756350874900818
train gradient:  0.29245750508452517
iteration : 2836
train acc:  0.8125
train loss:  0.4091604948043823
train gradient:  0.47824615420550964
iteration : 2837
train acc:  0.8125
train loss:  0.41250330209732056
train gradient:  0.35797607088983063
iteration : 2838
train acc:  0.8125
train loss:  0.46224358677864075
train gradient:  0.5809124449973924
iteration : 2839
train acc:  0.8515625
train loss:  0.39296212792396545
train gradient:  0.3962069967507104
iteration : 2840
train acc:  0.8203125
train loss:  0.41293615102767944
train gradient:  0.549617169488638
iteration : 2841
train acc:  0.84375
train loss:  0.3664686381816864
train gradient:  0.33334209994145314
iteration : 2842
train acc:  0.8359375
train loss:  0.378495454788208
train gradient:  0.40129929036396067
iteration : 2843
train acc:  0.8203125
train loss:  0.4248160719871521
train gradient:  0.38530153358156144
iteration : 2844
train acc:  0.8515625
train loss:  0.34916889667510986
train gradient:  0.31866122699485816
iteration : 2845
train acc:  0.8671875
train loss:  0.322257936000824
train gradient:  0.3447042082129859
iteration : 2846
train acc:  0.8203125
train loss:  0.36450114846229553
train gradient:  0.5146432098720137
iteration : 2847
train acc:  0.8984375
train loss:  0.3265514373779297
train gradient:  0.26213331650296495
iteration : 2848
train acc:  0.828125
train loss:  0.4015887379646301
train gradient:  0.2884783385222474
iteration : 2849
train acc:  0.828125
train loss:  0.34994372725486755
train gradient:  0.3344080952993701
iteration : 2850
train acc:  0.8359375
train loss:  0.4060196876525879
train gradient:  0.3521852126050351
iteration : 2851
train acc:  0.828125
train loss:  0.35936251282691956
train gradient:  0.23557376547847902
iteration : 2852
train acc:  0.828125
train loss:  0.35044294595718384
train gradient:  0.28729689783936296
iteration : 2853
train acc:  0.7734375
train loss:  0.46199116110801697
train gradient:  0.8005145358296921
iteration : 2854
train acc:  0.8515625
train loss:  0.36380329728126526
train gradient:  0.3081602345955923
iteration : 2855
train acc:  0.859375
train loss:  0.3244767189025879
train gradient:  0.20590983509234667
iteration : 2856
train acc:  0.828125
train loss:  0.3568806052207947
train gradient:  0.3773309661865955
iteration : 2857
train acc:  0.8203125
train loss:  0.4058476686477661
train gradient:  0.5507053447145493
iteration : 2858
train acc:  0.828125
train loss:  0.36208996176719666
train gradient:  0.2867563874752915
iteration : 2859
train acc:  0.859375
train loss:  0.34259897470474243
train gradient:  0.3693197729335775
iteration : 2860
train acc:  0.78125
train loss:  0.40197205543518066
train gradient:  0.4312223951434628
iteration : 2861
train acc:  0.8125
train loss:  0.43289756774902344
train gradient:  0.40424963029840816
iteration : 2862
train acc:  0.8359375
train loss:  0.34465301036834717
train gradient:  0.24282364996252026
iteration : 2863
train acc:  0.84375
train loss:  0.3913700580596924
train gradient:  0.4572581710562766
iteration : 2864
train acc:  0.7734375
train loss:  0.4023796319961548
train gradient:  0.312646265114219
iteration : 2865
train acc:  0.84375
train loss:  0.42296919226646423
train gradient:  0.47498609639718586
iteration : 2866
train acc:  0.84375
train loss:  0.3577767610549927
train gradient:  0.4938318861604125
iteration : 2867
train acc:  0.8046875
train loss:  0.42105862498283386
train gradient:  0.5497819423565768
iteration : 2868
train acc:  0.8515625
train loss:  0.3583216071128845
train gradient:  0.26485289266390133
iteration : 2869
train acc:  0.875
train loss:  0.3569093644618988
train gradient:  0.5085003863546524
iteration : 2870
train acc:  0.8828125
train loss:  0.33784082531929016
train gradient:  0.29676092861606795
iteration : 2871
train acc:  0.84375
train loss:  0.4107818603515625
train gradient:  0.3468480411087882
iteration : 2872
train acc:  0.859375
train loss:  0.3209701180458069
train gradient:  0.3233668225067693
iteration : 2873
train acc:  0.78125
train loss:  0.40357115864753723
train gradient:  0.44277316511687254
iteration : 2874
train acc:  0.796875
train loss:  0.40111035108566284
train gradient:  0.42482696211030696
iteration : 2875
train acc:  0.859375
train loss:  0.32936668395996094
train gradient:  0.32866581526572863
iteration : 2876
train acc:  0.8125
train loss:  0.3686811327934265
train gradient:  0.29437561018439035
iteration : 2877
train acc:  0.84375
train loss:  0.31635987758636475
train gradient:  0.23312755434596738
iteration : 2878
train acc:  0.7890625
train loss:  0.3888576924800873
train gradient:  0.48238140937491336
iteration : 2879
train acc:  0.8203125
train loss:  0.34315162897109985
train gradient:  0.2312342916733578
iteration : 2880
train acc:  0.78125
train loss:  0.4220860004425049
train gradient:  0.40486448094677874
iteration : 2881
train acc:  0.828125
train loss:  0.36587363481521606
train gradient:  0.336548440647821
iteration : 2882
train acc:  0.8125
train loss:  0.4341511130332947
train gradient:  0.4134085061633239
iteration : 2883
train acc:  0.8125
train loss:  0.3694673776626587
train gradient:  0.22390955993166198
iteration : 2884
train acc:  0.84375
train loss:  0.3980463147163391
train gradient:  0.37382860469667484
iteration : 2885
train acc:  0.8828125
train loss:  0.32804015278816223
train gradient:  0.3035420522316163
iteration : 2886
train acc:  0.828125
train loss:  0.3792085647583008
train gradient:  0.41987690279553475
iteration : 2887
train acc:  0.7890625
train loss:  0.4327716827392578
train gradient:  0.5387693504823208
iteration : 2888
train acc:  0.84375
train loss:  0.33691108226776123
train gradient:  0.24719193277843157
iteration : 2889
train acc:  0.78125
train loss:  0.42501914501190186
train gradient:  0.5270624485902162
iteration : 2890
train acc:  0.8515625
train loss:  0.39040273427963257
train gradient:  0.3455028655529889
iteration : 2891
train acc:  0.8125
train loss:  0.38661572337150574
train gradient:  0.3398166763533954
iteration : 2892
train acc:  0.8046875
train loss:  0.3506655991077423
train gradient:  0.24767418171767913
iteration : 2893
train acc:  0.8671875
train loss:  0.30472853779792786
train gradient:  0.302495094664529
iteration : 2894
train acc:  0.7578125
train loss:  0.4675365090370178
train gradient:  0.4643381465848436
iteration : 2895
train acc:  0.828125
train loss:  0.36018288135528564
train gradient:  0.46610245299178926
iteration : 2896
train acc:  0.84375
train loss:  0.40724021196365356
train gradient:  0.29622074529145326
iteration : 2897
train acc:  0.7890625
train loss:  0.41190874576568604
train gradient:  0.2852897322255477
iteration : 2898
train acc:  0.75
train loss:  0.524303674697876
train gradient:  0.5488360273177635
iteration : 2899
train acc:  0.8515625
train loss:  0.34671616554260254
train gradient:  0.3472310371580059
iteration : 2900
train acc:  0.8046875
train loss:  0.43591004610061646
train gradient:  0.36220091956599487
iteration : 2901
train acc:  0.828125
train loss:  0.33532917499542236
train gradient:  0.3006093467155806
iteration : 2902
train acc:  0.7734375
train loss:  0.3894885182380676
train gradient:  0.3369288029330003
iteration : 2903
train acc:  0.828125
train loss:  0.4158971309661865
train gradient:  0.5052612030072254
iteration : 2904
train acc:  0.7734375
train loss:  0.4623708724975586
train gradient:  0.35608599072849667
iteration : 2905
train acc:  0.828125
train loss:  0.34933507442474365
train gradient:  0.25651992912824123
iteration : 2906
train acc:  0.8359375
train loss:  0.3932105302810669
train gradient:  0.3262877468022553
iteration : 2907
train acc:  0.875
train loss:  0.3463466167449951
train gradient:  0.299299391021394
iteration : 2908
train acc:  0.8046875
train loss:  0.42892539501190186
train gradient:  0.7245599186533431
iteration : 2909
train acc:  0.7890625
train loss:  0.39351001381874084
train gradient:  0.37051430025887394
iteration : 2910
train acc:  0.8125
train loss:  0.3838740885257721
train gradient:  0.2730988021902624
iteration : 2911
train acc:  0.8203125
train loss:  0.4035998582839966
train gradient:  0.328085297184512
iteration : 2912
train acc:  0.765625
train loss:  0.4093680679798126
train gradient:  0.312104942736517
iteration : 2913
train acc:  0.8515625
train loss:  0.3462093472480774
train gradient:  0.2769711229339178
iteration : 2914
train acc:  0.8671875
train loss:  0.3647308647632599
train gradient:  0.2765085873104239
iteration : 2915
train acc:  0.84375
train loss:  0.34700292348861694
train gradient:  0.23908828546023697
iteration : 2916
train acc:  0.875
train loss:  0.33704859018325806
train gradient:  0.25226451503708724
iteration : 2917
train acc:  0.8046875
train loss:  0.3949563503265381
train gradient:  0.3688186527131559
iteration : 2918
train acc:  0.8515625
train loss:  0.3251906931400299
train gradient:  0.300479628598313
iteration : 2919
train acc:  0.890625
train loss:  0.32606667280197144
train gradient:  0.23698150878229254
iteration : 2920
train acc:  0.8515625
train loss:  0.30770188570022583
train gradient:  0.38669041719160524
iteration : 2921
train acc:  0.8359375
train loss:  0.369564950466156
train gradient:  0.4248841118516257
iteration : 2922
train acc:  0.8671875
train loss:  0.3539576232433319
train gradient:  0.26803869190103446
iteration : 2923
train acc:  0.84375
train loss:  0.40112730860710144
train gradient:  0.27829920703499367
iteration : 2924
train acc:  0.828125
train loss:  0.420931875705719
train gradient:  0.4099389291548754
iteration : 2925
train acc:  0.8515625
train loss:  0.41058582067489624
train gradient:  0.41925273916952216
iteration : 2926
train acc:  0.859375
train loss:  0.37167078256607056
train gradient:  0.38831402308084095
iteration : 2927
train acc:  0.8046875
train loss:  0.41177767515182495
train gradient:  0.4285736894131152
iteration : 2928
train acc:  0.8515625
train loss:  0.3718315064907074
train gradient:  0.3122183934989274
iteration : 2929
train acc:  0.859375
train loss:  0.3294985890388489
train gradient:  0.324982598033962
iteration : 2930
train acc:  0.796875
train loss:  0.42584896087646484
train gradient:  0.4343206681299752
iteration : 2931
train acc:  0.7578125
train loss:  0.5285447835922241
train gradient:  0.8160900958261943
iteration : 2932
train acc:  0.828125
train loss:  0.36943671107292175
train gradient:  0.2815899157152419
iteration : 2933
train acc:  0.859375
train loss:  0.310693621635437
train gradient:  0.35812356989171157
iteration : 2934
train acc:  0.8515625
train loss:  0.37436896562576294
train gradient:  0.3495257545061448
iteration : 2935
train acc:  0.8203125
train loss:  0.3515499532222748
train gradient:  0.5019204338958645
iteration : 2936
train acc:  0.84375
train loss:  0.35871922969818115
train gradient:  0.35023635400410824
iteration : 2937
train acc:  0.828125
train loss:  0.3396010994911194
train gradient:  0.2996745930298492
iteration : 2938
train acc:  0.828125
train loss:  0.39523178339004517
train gradient:  0.39315813313338177
iteration : 2939
train acc:  0.875
train loss:  0.32368436455726624
train gradient:  0.29924142641566226
iteration : 2940
train acc:  0.8203125
train loss:  0.383955180644989
train gradient:  0.30405335839316694
iteration : 2941
train acc:  0.828125
train loss:  0.42755135893821716
train gradient:  0.44811742740610694
iteration : 2942
train acc:  0.84375
train loss:  0.3706868290901184
train gradient:  0.3542599613840813
iteration : 2943
train acc:  0.8359375
train loss:  0.39629411697387695
train gradient:  0.3702877536761764
iteration : 2944
train acc:  0.84375
train loss:  0.3826218843460083
train gradient:  0.47657913320142853
iteration : 2945
train acc:  0.78125
train loss:  0.4306008815765381
train gradient:  0.3772376733545758
iteration : 2946
train acc:  0.796875
train loss:  0.48332780599594116
train gradient:  0.5536870411260346
iteration : 2947
train acc:  0.7890625
train loss:  0.43017494678497314
train gradient:  0.3796716608047436
iteration : 2948
train acc:  0.828125
train loss:  0.382113516330719
train gradient:  0.3368048507365673
iteration : 2949
train acc:  0.84375
train loss:  0.38890016078948975
train gradient:  0.2735206058772495
iteration : 2950
train acc:  0.8515625
train loss:  0.3653649687767029
train gradient:  0.2857960311010989
iteration : 2951
train acc:  0.8125
train loss:  0.3790362477302551
train gradient:  0.34783221660763003
iteration : 2952
train acc:  0.8671875
train loss:  0.33596059679985046
train gradient:  0.23733131496536675
iteration : 2953
train acc:  0.890625
train loss:  0.34098169207572937
train gradient:  0.2684484523450792
iteration : 2954
train acc:  0.8203125
train loss:  0.3788703680038452
train gradient:  0.5128370009180826
iteration : 2955
train acc:  0.859375
train loss:  0.33521246910095215
train gradient:  0.23285848281677443
iteration : 2956
train acc:  0.8828125
train loss:  0.29500478506088257
train gradient:  0.2615401042990058
iteration : 2957
train acc:  0.859375
train loss:  0.32236456871032715
train gradient:  0.2852213524878873
iteration : 2958
train acc:  0.765625
train loss:  0.43160736560821533
train gradient:  0.5460781688696337
iteration : 2959
train acc:  0.828125
train loss:  0.3879796266555786
train gradient:  0.3525697630236809
iteration : 2960
train acc:  0.859375
train loss:  0.3445667624473572
train gradient:  0.41109408792691304
iteration : 2961
train acc:  0.859375
train loss:  0.3590736389160156
train gradient:  0.2971385610205487
iteration : 2962
train acc:  0.8125
train loss:  0.3710373044013977
train gradient:  0.3857455429324459
iteration : 2963
train acc:  0.8515625
train loss:  0.3943226933479309
train gradient:  0.23996332946987992
iteration : 2964
train acc:  0.8359375
train loss:  0.369489848613739
train gradient:  0.4392940362708738
iteration : 2965
train acc:  0.875
train loss:  0.40199869871139526
train gradient:  0.34496554939380114
iteration : 2966
train acc:  0.84375
train loss:  0.3411572277545929
train gradient:  0.3604274822814674
iteration : 2967
train acc:  0.8515625
train loss:  0.3481990694999695
train gradient:  0.25001843691263576
iteration : 2968
train acc:  0.8515625
train loss:  0.3471072018146515
train gradient:  0.4094049066522084
iteration : 2969
train acc:  0.8125
train loss:  0.3966432809829712
train gradient:  0.3363574668422424
iteration : 2970
train acc:  0.8515625
train loss:  0.3190786838531494
train gradient:  0.3028803680923163
iteration : 2971
train acc:  0.8203125
train loss:  0.4179002046585083
train gradient:  0.4565180241812321
iteration : 2972
train acc:  0.796875
train loss:  0.38413572311401367
train gradient:  0.3211015998006348
iteration : 2973
train acc:  0.8671875
train loss:  0.35446372628211975
train gradient:  0.327022744897979
iteration : 2974
train acc:  0.859375
train loss:  0.41721421480178833
train gradient:  0.44633515010571007
iteration : 2975
train acc:  0.8515625
train loss:  0.377200722694397
train gradient:  0.42737649622368723
iteration : 2976
train acc:  0.84375
train loss:  0.37590402364730835
train gradient:  0.3151619607735904
iteration : 2977
train acc:  0.796875
train loss:  0.4423566460609436
train gradient:  0.4245664360961433
iteration : 2978
train acc:  0.7734375
train loss:  0.46245306730270386
train gradient:  0.5132271824623778
iteration : 2979
train acc:  0.8125
train loss:  0.34790170192718506
train gradient:  0.3315530007962552
iteration : 2980
train acc:  0.8359375
train loss:  0.36508965492248535
train gradient:  0.2983839872220404
iteration : 2981
train acc:  0.796875
train loss:  0.4318159222602844
train gradient:  0.6308190485747771
iteration : 2982
train acc:  0.921875
train loss:  0.24022462964057922
train gradient:  0.2707044048804644
iteration : 2983
train acc:  0.8515625
train loss:  0.3724786639213562
train gradient:  0.36482019430122775
iteration : 2984
train acc:  0.75
train loss:  0.4244384765625
train gradient:  0.4353030778951315
iteration : 2985
train acc:  0.8046875
train loss:  0.42318931221961975
train gradient:  0.40461269503086794
iteration : 2986
train acc:  0.8359375
train loss:  0.38507840037345886
train gradient:  0.3514033221909452
iteration : 2987
train acc:  0.890625
train loss:  0.3239893913269043
train gradient:  0.24898737479989896
iteration : 2988
train acc:  0.8203125
train loss:  0.3666203022003174
train gradient:  0.5663193529293121
iteration : 2989
train acc:  0.8125
train loss:  0.408588707447052
train gradient:  0.3558185197310695
iteration : 2990
train acc:  0.84375
train loss:  0.40696173906326294
train gradient:  0.3475682328834613
iteration : 2991
train acc:  0.796875
train loss:  0.4060172140598297
train gradient:  0.3469018874829785
iteration : 2992
train acc:  0.84375
train loss:  0.3981313109397888
train gradient:  0.35922168273559735
iteration : 2993
train acc:  0.84375
train loss:  0.3198801875114441
train gradient:  0.2622601156317139
iteration : 2994
train acc:  0.7890625
train loss:  0.41964319348335266
train gradient:  0.48748188397001807
iteration : 2995
train acc:  0.8515625
train loss:  0.4025140404701233
train gradient:  0.5155956747080503
iteration : 2996
train acc:  0.859375
train loss:  0.35398319363594055
train gradient:  0.4389049458889345
iteration : 2997
train acc:  0.8203125
train loss:  0.39192289113998413
train gradient:  0.46114246794785924
iteration : 2998
train acc:  0.8984375
train loss:  0.2827565371990204
train gradient:  0.2266952959328603
iteration : 2999
train acc:  0.828125
train loss:  0.3622114956378937
train gradient:  0.4445616653350797
iteration : 3000
train acc:  0.84375
train loss:  0.3984789550304413
train gradient:  0.3415385685647115
iteration : 3001
train acc:  0.8125
train loss:  0.412558913230896
train gradient:  0.4690757084007136
iteration : 3002
train acc:  0.8359375
train loss:  0.38799911737442017
train gradient:  0.3595699702517704
iteration : 3003
train acc:  0.84375
train loss:  0.37709492444992065
train gradient:  0.39248085632053953
iteration : 3004
train acc:  0.8125
train loss:  0.41041600704193115
train gradient:  0.33258019281877227
iteration : 3005
train acc:  0.7890625
train loss:  0.42487314343452454
train gradient:  0.3819946586607369
iteration : 3006
train acc:  0.8828125
train loss:  0.2884555459022522
train gradient:  0.19664700702035726
iteration : 3007
train acc:  0.875
train loss:  0.31913283467292786
train gradient:  0.2816633130804242
iteration : 3008
train acc:  0.84375
train loss:  0.3116697669029236
train gradient:  0.30058313491474276
iteration : 3009
train acc:  0.8125
train loss:  0.4385083019733429
train gradient:  0.4007176715260293
iteration : 3010
train acc:  0.859375
train loss:  0.3526161313056946
train gradient:  0.2821672032880087
iteration : 3011
train acc:  0.7421875
train loss:  0.4630313217639923
train gradient:  0.4090880516473466
iteration : 3012
train acc:  0.796875
train loss:  0.42353561520576477
train gradient:  0.41486262398569035
iteration : 3013
train acc:  0.921875
train loss:  0.27832937240600586
train gradient:  0.24643864559323575
iteration : 3014
train acc:  0.796875
train loss:  0.41158831119537354
train gradient:  0.42878598401428253
iteration : 3015
train acc:  0.8046875
train loss:  0.4495285749435425
train gradient:  0.44175307693138277
iteration : 3016
train acc:  0.8046875
train loss:  0.42935094237327576
train gradient:  0.3859800817374131
iteration : 3017
train acc:  0.859375
train loss:  0.3501139283180237
train gradient:  0.3944083618315033
iteration : 3018
train acc:  0.796875
train loss:  0.37116414308547974
train gradient:  0.3420797300414096
iteration : 3019
train acc:  0.828125
train loss:  0.4149884879589081
train gradient:  0.38625852481674344
iteration : 3020
train acc:  0.8046875
train loss:  0.42446815967559814
train gradient:  0.3802641670863205
iteration : 3021
train acc:  0.84375
train loss:  0.3862334191799164
train gradient:  0.36780408016983074
iteration : 3022
train acc:  0.84375
train loss:  0.3184729814529419
train gradient:  0.2866758124943159
iteration : 3023
train acc:  0.796875
train loss:  0.36104246973991394
train gradient:  0.30552125395533375
iteration : 3024
train acc:  0.8125
train loss:  0.3779599070549011
train gradient:  0.35686678744572486
iteration : 3025
train acc:  0.828125
train loss:  0.39402687549591064
train gradient:  0.4256361407152814
iteration : 3026
train acc:  0.84375
train loss:  0.399970144033432
train gradient:  0.48798926401076625
iteration : 3027
train acc:  0.8828125
train loss:  0.28434211015701294
train gradient:  0.19047030964116385
iteration : 3028
train acc:  0.859375
train loss:  0.3035311698913574
train gradient:  0.28757537052740695
iteration : 3029
train acc:  0.7734375
train loss:  0.4556702673435211
train gradient:  0.4157037365370621
iteration : 3030
train acc:  0.828125
train loss:  0.4477231800556183
train gradient:  0.4911542254346483
iteration : 3031
train acc:  0.8046875
train loss:  0.4566503167152405
train gradient:  0.48285307646013287
iteration : 3032
train acc:  0.828125
train loss:  0.3778250217437744
train gradient:  0.21869842815986762
iteration : 3033
train acc:  0.859375
train loss:  0.3440720736980438
train gradient:  0.19707109659680497
iteration : 3034
train acc:  0.8671875
train loss:  0.3597768247127533
train gradient:  0.24350243217699252
iteration : 3035
train acc:  0.78125
train loss:  0.45246052742004395
train gradient:  0.5302151448198363
iteration : 3036
train acc:  0.8203125
train loss:  0.43403375148773193
train gradient:  0.4340332809987825
iteration : 3037
train acc:  0.9140625
train loss:  0.3052161931991577
train gradient:  0.2624604880541161
iteration : 3038
train acc:  0.7421875
train loss:  0.5136064887046814
train gradient:  0.8162674183514498
iteration : 3039
train acc:  0.84375
train loss:  0.34475934505462646
train gradient:  0.22875454446166915
iteration : 3040
train acc:  0.7734375
train loss:  0.4453054964542389
train gradient:  0.40682465936241147
iteration : 3041
train acc:  0.8203125
train loss:  0.35973048210144043
train gradient:  0.3800476325152902
iteration : 3042
train acc:  0.796875
train loss:  0.42122042179107666
train gradient:  0.3697482274781991
iteration : 3043
train acc:  0.765625
train loss:  0.5151268839836121
train gradient:  0.4699263175447211
iteration : 3044
train acc:  0.8359375
train loss:  0.3264913558959961
train gradient:  0.2854036840781812
iteration : 3045
train acc:  0.84375
train loss:  0.36572715640068054
train gradient:  0.2499267650804452
iteration : 3046
train acc:  0.8046875
train loss:  0.38830721378326416
train gradient:  0.27176784632277207
iteration : 3047
train acc:  0.8046875
train loss:  0.39477115869522095
train gradient:  0.3254165503555262
iteration : 3048
train acc:  0.8671875
train loss:  0.27626121044158936
train gradient:  0.5632349324536284
iteration : 3049
train acc:  0.828125
train loss:  0.3726122975349426
train gradient:  0.2561186387634263
iteration : 3050
train acc:  0.8515625
train loss:  0.378780722618103
train gradient:  0.2965061961406901
iteration : 3051
train acc:  0.796875
train loss:  0.4421955347061157
train gradient:  0.33064958435779795
iteration : 3052
train acc:  0.8671875
train loss:  0.34055668115615845
train gradient:  0.2396835994710062
iteration : 3053
train acc:  0.765625
train loss:  0.5315223932266235
train gradient:  0.49207975125129216
iteration : 3054
train acc:  0.8203125
train loss:  0.4049460291862488
train gradient:  0.3314316735055274
iteration : 3055
train acc:  0.875
train loss:  0.35728031396865845
train gradient:  0.3003475613087295
iteration : 3056
train acc:  0.8203125
train loss:  0.37028366327285767
train gradient:  0.25563979538648174
iteration : 3057
train acc:  0.828125
train loss:  0.3516961336135864
train gradient:  0.27646103333078653
iteration : 3058
train acc:  0.8515625
train loss:  0.33834630250930786
train gradient:  0.25840369724780216
iteration : 3059
train acc:  0.8203125
train loss:  0.4045928418636322
train gradient:  0.3129723290284127
iteration : 3060
train acc:  0.8359375
train loss:  0.3672158718109131
train gradient:  0.28341774430303607
iteration : 3061
train acc:  0.765625
train loss:  0.5042030811309814
train gradient:  0.4987086130642906
iteration : 3062
train acc:  0.84375
train loss:  0.3511010408401489
train gradient:  0.2741648431621297
iteration : 3063
train acc:  0.7578125
train loss:  0.4006103575229645
train gradient:  0.41170056080735973
iteration : 3064
train acc:  0.796875
train loss:  0.3893691897392273
train gradient:  0.3085598389558871
iteration : 3065
train acc:  0.890625
train loss:  0.26836490631103516
train gradient:  0.2586496992337037
iteration : 3066
train acc:  0.8046875
train loss:  0.36924731731414795
train gradient:  0.31201320434362084
iteration : 3067
train acc:  0.859375
train loss:  0.3205593228340149
train gradient:  0.24834651954876885
iteration : 3068
train acc:  0.8203125
train loss:  0.40460389852523804
train gradient:  0.40689516045664237
iteration : 3069
train acc:  0.8203125
train loss:  0.4502927362918854
train gradient:  0.427331741471306
iteration : 3070
train acc:  0.859375
train loss:  0.3699988126754761
train gradient:  0.3083309854030769
iteration : 3071
train acc:  0.7734375
train loss:  0.4211866855621338
train gradient:  0.29660269676275347
iteration : 3072
train acc:  0.765625
train loss:  0.40216168761253357
train gradient:  0.3723227282613187
iteration : 3073
train acc:  0.8515625
train loss:  0.3299959599971771
train gradient:  0.31839520739307514
iteration : 3074
train acc:  0.859375
train loss:  0.3256535828113556
train gradient:  0.3161554709336318
iteration : 3075
train acc:  0.8125
train loss:  0.3877849578857422
train gradient:  0.2939470640021216
iteration : 3076
train acc:  0.8359375
train loss:  0.4454260468482971
train gradient:  0.3902807074368007
iteration : 3077
train acc:  0.8359375
train loss:  0.4117411971092224
train gradient:  0.3272924678235928
iteration : 3078
train acc:  0.7890625
train loss:  0.3989207148551941
train gradient:  0.3717740058348554
iteration : 3079
train acc:  0.796875
train loss:  0.39289525151252747
train gradient:  0.47643978408125437
iteration : 3080
train acc:  0.7890625
train loss:  0.4677408039569855
train gradient:  0.3492064607407215
iteration : 3081
train acc:  0.859375
train loss:  0.33972907066345215
train gradient:  0.2611292980534635
iteration : 3082
train acc:  0.7890625
train loss:  0.40947604179382324
train gradient:  0.36478772146510346
iteration : 3083
train acc:  0.8515625
train loss:  0.41961416602134705
train gradient:  0.34182355841392786
iteration : 3084
train acc:  0.8671875
train loss:  0.3313044607639313
train gradient:  0.250284014974985
iteration : 3085
train acc:  0.8359375
train loss:  0.3447163701057434
train gradient:  0.20682163626620415
iteration : 3086
train acc:  0.8203125
train loss:  0.40382394194602966
train gradient:  0.30934699531345267
iteration : 3087
train acc:  0.7734375
train loss:  0.4627557396888733
train gradient:  0.42854011957009414
iteration : 3088
train acc:  0.8203125
train loss:  0.41511860489845276
train gradient:  0.358801143823887
iteration : 3089
train acc:  0.78125
train loss:  0.4180893301963806
train gradient:  0.44619867020128745
iteration : 3090
train acc:  0.8515625
train loss:  0.3788606524467468
train gradient:  0.3519680485044025
iteration : 3091
train acc:  0.8671875
train loss:  0.33183616399765015
train gradient:  0.2251887915512865
iteration : 3092
train acc:  0.8515625
train loss:  0.4020681381225586
train gradient:  0.2733691333526688
iteration : 3093
train acc:  0.859375
train loss:  0.34425410628318787
train gradient:  0.22127709008866456
iteration : 3094
train acc:  0.890625
train loss:  0.34039050340652466
train gradient:  0.2375077138250123
iteration : 3095
train acc:  0.828125
train loss:  0.4204898774623871
train gradient:  0.49379640566883093
iteration : 3096
train acc:  0.8203125
train loss:  0.40961024165153503
train gradient:  0.4396529455668541
iteration : 3097
train acc:  0.7421875
train loss:  0.47985416650772095
train gradient:  0.5100123517741842
iteration : 3098
train acc:  0.875
train loss:  0.3273269534111023
train gradient:  0.20771161490182774
iteration : 3099
train acc:  0.828125
train loss:  0.3679690957069397
train gradient:  0.2590500590842342
iteration : 3100
train acc:  0.859375
train loss:  0.34475213289260864
train gradient:  0.311340510486497
iteration : 3101
train acc:  0.828125
train loss:  0.4069691300392151
train gradient:  0.33127970926053496
iteration : 3102
train acc:  0.859375
train loss:  0.318209171295166
train gradient:  0.29573762277674415
iteration : 3103
train acc:  0.7890625
train loss:  0.3854970335960388
train gradient:  0.30402009738314023
iteration : 3104
train acc:  0.828125
train loss:  0.34538328647613525
train gradient:  0.22438882715337244
iteration : 3105
train acc:  0.796875
train loss:  0.4588639438152313
train gradient:  0.36548919777102623
iteration : 3106
train acc:  0.796875
train loss:  0.4462882876396179
train gradient:  0.5389223431903175
iteration : 3107
train acc:  0.84375
train loss:  0.34230804443359375
train gradient:  0.28567049753706275
iteration : 3108
train acc:  0.8046875
train loss:  0.4060804545879364
train gradient:  0.39334808935435006
iteration : 3109
train acc:  0.8515625
train loss:  0.3262704014778137
train gradient:  0.2400966931387009
iteration : 3110
train acc:  0.8671875
train loss:  0.3081390857696533
train gradient:  0.25363324259234665
iteration : 3111
train acc:  0.8203125
train loss:  0.3712460994720459
train gradient:  0.3431724217606251
iteration : 3112
train acc:  0.7734375
train loss:  0.4484397768974304
train gradient:  0.5952088898850274
iteration : 3113
train acc:  0.84375
train loss:  0.35323941707611084
train gradient:  0.2665186130316274
iteration : 3114
train acc:  0.7734375
train loss:  0.4772642254829407
train gradient:  0.41059619161862176
iteration : 3115
train acc:  0.8125
train loss:  0.40858781337738037
train gradient:  0.3552563282919452
iteration : 3116
train acc:  0.796875
train loss:  0.4321388602256775
train gradient:  0.37749220763921754
iteration : 3117
train acc:  0.8515625
train loss:  0.3674721419811249
train gradient:  0.30710153690298225
iteration : 3118
train acc:  0.8046875
train loss:  0.42855745553970337
train gradient:  0.31054909236198064
iteration : 3119
train acc:  0.8359375
train loss:  0.3987565338611603
train gradient:  0.3702150846038673
iteration : 3120
train acc:  0.8359375
train loss:  0.44622179865837097
train gradient:  0.44226509877340386
iteration : 3121
train acc:  0.8203125
train loss:  0.37912577390670776
train gradient:  0.2336960010363835
iteration : 3122
train acc:  0.84375
train loss:  0.3599943518638611
train gradient:  0.24535367845416017
iteration : 3123
train acc:  0.8203125
train loss:  0.4444870352745056
train gradient:  0.3679020950519864
iteration : 3124
train acc:  0.8125
train loss:  0.3609342575073242
train gradient:  0.2962961993328452
iteration : 3125
train acc:  0.7890625
train loss:  0.4659721255302429
train gradient:  0.4110938217534996
iteration : 3126
train acc:  0.875
train loss:  0.28397876024246216
train gradient:  0.16292341155790666
iteration : 3127
train acc:  0.7578125
train loss:  0.45958173274993896
train gradient:  0.7524519777474821
iteration : 3128
train acc:  0.84375
train loss:  0.34104108810424805
train gradient:  0.26545497999049833
iteration : 3129
train acc:  0.8828125
train loss:  0.3220813572406769
train gradient:  0.3526257622948266
iteration : 3130
train acc:  0.8203125
train loss:  0.391182005405426
train gradient:  0.4668318356649306
iteration : 3131
train acc:  0.8515625
train loss:  0.3820449113845825
train gradient:  0.3041927475566283
iteration : 3132
train acc:  0.8125
train loss:  0.3556092381477356
train gradient:  0.33781480852531953
iteration : 3133
train acc:  0.8125
train loss:  0.47188469767570496
train gradient:  0.4152320920037803
iteration : 3134
train acc:  0.7734375
train loss:  0.41208532452583313
train gradient:  0.4078230467790842
iteration : 3135
train acc:  0.84375
train loss:  0.33890414237976074
train gradient:  0.25724936206517957
iteration : 3136
train acc:  0.859375
train loss:  0.36350011825561523
train gradient:  0.20268330424003206
iteration : 3137
train acc:  0.796875
train loss:  0.4195929169654846
train gradient:  0.27763663697173213
iteration : 3138
train acc:  0.875
train loss:  0.29253408312797546
train gradient:  0.1899470031722583
iteration : 3139
train acc:  0.8125
train loss:  0.4348057508468628
train gradient:  0.35326162850437537
iteration : 3140
train acc:  0.8125
train loss:  0.40988606214523315
train gradient:  0.4359153671389557
iteration : 3141
train acc:  0.8125
train loss:  0.4202383756637573
train gradient:  0.3071356446110531
iteration : 3142
train acc:  0.7890625
train loss:  0.4880096912384033
train gradient:  0.3722102417875228
iteration : 3143
train acc:  0.8125
train loss:  0.3291834890842438
train gradient:  0.23948648017827007
iteration : 3144
train acc:  0.78125
train loss:  0.4785643219947815
train gradient:  0.3711190455806469
iteration : 3145
train acc:  0.875
train loss:  0.33191967010498047
train gradient:  0.24226627016903451
iteration : 3146
train acc:  0.8828125
train loss:  0.3646731376647949
train gradient:  0.3521189209334237
iteration : 3147
train acc:  0.8671875
train loss:  0.32937246561050415
train gradient:  0.2384051626710064
iteration : 3148
train acc:  0.8125
train loss:  0.4405907392501831
train gradient:  0.3568915542966188
iteration : 3149
train acc:  0.84375
train loss:  0.40717625617980957
train gradient:  0.3518734217512128
iteration : 3150
train acc:  0.828125
train loss:  0.3784444332122803
train gradient:  0.2675999625348741
iteration : 3151
train acc:  0.8125
train loss:  0.46575191617012024
train gradient:  0.37998090012737623
iteration : 3152
train acc:  0.875
train loss:  0.36160945892333984
train gradient:  0.2516501395345352
iteration : 3153
train acc:  0.828125
train loss:  0.43434423208236694
train gradient:  0.38292891061569634
iteration : 3154
train acc:  0.8203125
train loss:  0.38842034339904785
train gradient:  0.3136383031144227
iteration : 3155
train acc:  0.8359375
train loss:  0.3770647346973419
train gradient:  0.3219015230486728
iteration : 3156
train acc:  0.8515625
train loss:  0.3534679710865021
train gradient:  0.25804809861518385
iteration : 3157
train acc:  0.7890625
train loss:  0.4081547260284424
train gradient:  0.33325715824906965
iteration : 3158
train acc:  0.8203125
train loss:  0.3723621964454651
train gradient:  0.3585776360705291
iteration : 3159
train acc:  0.828125
train loss:  0.4039657711982727
train gradient:  0.3165352405485815
iteration : 3160
train acc:  0.8125
train loss:  0.42371097207069397
train gradient:  0.39858950895694784
iteration : 3161
train acc:  0.8515625
train loss:  0.33764857053756714
train gradient:  0.2812589361087167
iteration : 3162
train acc:  0.859375
train loss:  0.3549420237541199
train gradient:  0.22619171206916378
iteration : 3163
train acc:  0.8203125
train loss:  0.37896692752838135
train gradient:  0.3337703148655513
iteration : 3164
train acc:  0.828125
train loss:  0.4485127925872803
train gradient:  0.329645873541944
iteration : 3165
train acc:  0.7890625
train loss:  0.4290406107902527
train gradient:  0.35859644676501734
iteration : 3166
train acc:  0.796875
train loss:  0.4249136447906494
train gradient:  0.3321188178306828
iteration : 3167
train acc:  0.75
train loss:  0.4590104818344116
train gradient:  0.33562380006864095
iteration : 3168
train acc:  0.796875
train loss:  0.3821532428264618
train gradient:  0.2799993838549652
iteration : 3169
train acc:  0.78125
train loss:  0.46912792325019836
train gradient:  0.4350533889173562
iteration : 3170
train acc:  0.8203125
train loss:  0.38275253772735596
train gradient:  0.205373113368921
iteration : 3171
train acc:  0.8125
train loss:  0.38110458850860596
train gradient:  0.21286478968506958
iteration : 3172
train acc:  0.8359375
train loss:  0.3574710488319397
train gradient:  0.19391576840263794
iteration : 3173
train acc:  0.84375
train loss:  0.3063582479953766
train gradient:  0.22313353423466614
iteration : 3174
train acc:  0.859375
train loss:  0.341429740190506
train gradient:  0.22445062642112756
iteration : 3175
train acc:  0.8203125
train loss:  0.42116254568099976
train gradient:  0.32821209403055807
iteration : 3176
train acc:  0.859375
train loss:  0.3690323233604431
train gradient:  0.2349536342998226
iteration : 3177
train acc:  0.8125
train loss:  0.36336493492126465
train gradient:  0.22146266588231
iteration : 3178
train acc:  0.84375
train loss:  0.3317503333091736
train gradient:  0.22458584858040082
iteration : 3179
train acc:  0.796875
train loss:  0.4429825246334076
train gradient:  0.32287368025162494
iteration : 3180
train acc:  0.8359375
train loss:  0.33267688751220703
train gradient:  0.3439907247820297
iteration : 3181
train acc:  0.890625
train loss:  0.2980925738811493
train gradient:  0.26675299236674715
iteration : 3182
train acc:  0.875
train loss:  0.3245496451854706
train gradient:  0.2211411124246569
iteration : 3183
train acc:  0.828125
train loss:  0.42564213275909424
train gradient:  0.2967498519726224
iteration : 3184
train acc:  0.859375
train loss:  0.32510730624198914
train gradient:  0.2817145334532108
iteration : 3185
train acc:  0.8984375
train loss:  0.2762797176837921
train gradient:  0.1457223042359634
iteration : 3186
train acc:  0.84375
train loss:  0.37616345286369324
train gradient:  0.3272765570814257
iteration : 3187
train acc:  0.8125
train loss:  0.43941470980644226
train gradient:  0.36958184451159926
iteration : 3188
train acc:  0.7734375
train loss:  0.44315680861473083
train gradient:  0.36881173362543307
iteration : 3189
train acc:  0.8515625
train loss:  0.32857269048690796
train gradient:  0.2521065953461108
iteration : 3190
train acc:  0.78125
train loss:  0.4596203565597534
train gradient:  0.3498663889151082
iteration : 3191
train acc:  0.875
train loss:  0.33915233612060547
train gradient:  0.3286057291849274
iteration : 3192
train acc:  0.8125
train loss:  0.4283464550971985
train gradient:  0.2705487700279069
iteration : 3193
train acc:  0.875
train loss:  0.30825939774513245
train gradient:  0.23203831662394342
iteration : 3194
train acc:  0.8359375
train loss:  0.31782427430152893
train gradient:  0.2537412479703439
iteration : 3195
train acc:  0.8203125
train loss:  0.4524182975292206
train gradient:  0.33709655037458
iteration : 3196
train acc:  0.8828125
train loss:  0.3245660066604614
train gradient:  0.27052763780807415
iteration : 3197
train acc:  0.859375
train loss:  0.33986926078796387
train gradient:  0.2904494044007742
iteration : 3198
train acc:  0.796875
train loss:  0.4618949294090271
train gradient:  0.5362354592296938
iteration : 3199
train acc:  0.8671875
train loss:  0.344743549823761
train gradient:  0.20672395888335227
iteration : 3200
train acc:  0.828125
train loss:  0.38226959109306335
train gradient:  0.394332156015286
iteration : 3201
train acc:  0.8984375
train loss:  0.2811107039451599
train gradient:  0.22146847397874558
iteration : 3202
train acc:  0.90625
train loss:  0.2992790937423706
train gradient:  0.16273786392462716
iteration : 3203
train acc:  0.8046875
train loss:  0.4382329285144806
train gradient:  0.3758557087541631
iteration : 3204
train acc:  0.8359375
train loss:  0.3416997194290161
train gradient:  0.3118576990177538
iteration : 3205
train acc:  0.8671875
train loss:  0.30554869771003723
train gradient:  0.21591460478642532
iteration : 3206
train acc:  0.796875
train loss:  0.41965237259864807
train gradient:  0.6192529927747186
iteration : 3207
train acc:  0.8515625
train loss:  0.3587557375431061
train gradient:  0.21882027540947455
iteration : 3208
train acc:  0.8046875
train loss:  0.4073929786682129
train gradient:  0.38584908153868996
iteration : 3209
train acc:  0.875
train loss:  0.29646727442741394
train gradient:  0.16284355617459437
iteration : 3210
train acc:  0.8046875
train loss:  0.41734302043914795
train gradient:  0.3641098062837829
iteration : 3211
train acc:  0.875
train loss:  0.3597780466079712
train gradient:  0.2664811196598769
iteration : 3212
train acc:  0.828125
train loss:  0.374570369720459
train gradient:  0.2612414079744758
iteration : 3213
train acc:  0.8828125
train loss:  0.2675919234752655
train gradient:  0.21823418957362312
iteration : 3214
train acc:  0.875
train loss:  0.2915668487548828
train gradient:  0.3037504822016329
iteration : 3215
train acc:  0.7890625
train loss:  0.43122562766075134
train gradient:  0.5222738092097716
iteration : 3216
train acc:  0.8515625
train loss:  0.36394888162612915
train gradient:  0.2474612049605049
iteration : 3217
train acc:  0.8671875
train loss:  0.2949894368648529
train gradient:  0.22697836285516323
iteration : 3218
train acc:  0.828125
train loss:  0.3502546548843384
train gradient:  0.37094269444932204
iteration : 3219
train acc:  0.8125
train loss:  0.4059431552886963
train gradient:  0.374160869508822
iteration : 3220
train acc:  0.796875
train loss:  0.43415582180023193
train gradient:  0.3963062028020282
iteration : 3221
train acc:  0.859375
train loss:  0.31899315118789673
train gradient:  0.276375907000416
iteration : 3222
train acc:  0.859375
train loss:  0.3051234483718872
train gradient:  0.26009683831512664
iteration : 3223
train acc:  0.859375
train loss:  0.36845529079437256
train gradient:  0.2282722409483163
iteration : 3224
train acc:  0.796875
train loss:  0.453900545835495
train gradient:  0.5174534807111608
iteration : 3225
train acc:  0.828125
train loss:  0.3092019557952881
train gradient:  0.3442293212312132
iteration : 3226
train acc:  0.8828125
train loss:  0.31149375438690186
train gradient:  0.22793618936341303
iteration : 3227
train acc:  0.8125
train loss:  0.36171936988830566
train gradient:  0.26564864655315135
iteration : 3228
train acc:  0.8203125
train loss:  0.40317147970199585
train gradient:  0.32510988213696945
iteration : 3229
train acc:  0.8515625
train loss:  0.29548466205596924
train gradient:  0.22069637922931093
iteration : 3230
train acc:  0.890625
train loss:  0.29130488634109497
train gradient:  0.28226037368825124
iteration : 3231
train acc:  0.8359375
train loss:  0.367308646440506
train gradient:  0.2708804299325124
iteration : 3232
train acc:  0.8203125
train loss:  0.4006105065345764
train gradient:  0.40797831690061104
iteration : 3233
train acc:  0.8125
train loss:  0.4523268938064575
train gradient:  0.4037151882043368
iteration : 3234
train acc:  0.828125
train loss:  0.39871102571487427
train gradient:  0.33562994132734997
iteration : 3235
train acc:  0.8515625
train loss:  0.3533673584461212
train gradient:  0.37025971184038325
iteration : 3236
train acc:  0.828125
train loss:  0.3973141312599182
train gradient:  0.2783786245530335
iteration : 3237
train acc:  0.84375
train loss:  0.3246748447418213
train gradient:  0.21942673644905222
iteration : 3238
train acc:  0.8359375
train loss:  0.4490601718425751
train gradient:  0.4701376871618151
iteration : 3239
train acc:  0.890625
train loss:  0.3135288953781128
train gradient:  0.251844626361369
iteration : 3240
train acc:  0.8671875
train loss:  0.30229246616363525
train gradient:  0.22397394930798697
iteration : 3241
train acc:  0.890625
train loss:  0.3331698179244995
train gradient:  0.33500341056980887
iteration : 3242
train acc:  0.78125
train loss:  0.5241810083389282
train gradient:  0.9071684056648905
iteration : 3243
train acc:  0.875
train loss:  0.32541772723197937
train gradient:  0.2157853587417477
iteration : 3244
train acc:  0.90625
train loss:  0.23382456600666046
train gradient:  0.22190223331172154
iteration : 3245
train acc:  0.7734375
train loss:  0.5577699542045593
train gradient:  0.5760120304399107
iteration : 3246
train acc:  0.8671875
train loss:  0.3211767077445984
train gradient:  0.3321547376727686
iteration : 3247
train acc:  0.8203125
train loss:  0.3749045431613922
train gradient:  0.3073731395734872
iteration : 3248
train acc:  0.8515625
train loss:  0.35290998220443726
train gradient:  0.32218555684792854
iteration : 3249
train acc:  0.8515625
train loss:  0.35710522532463074
train gradient:  0.27580118120428504
iteration : 3250
train acc:  0.7890625
train loss:  0.4251195788383484
train gradient:  0.3768149204781724
iteration : 3251
train acc:  0.8203125
train loss:  0.3512114882469177
train gradient:  0.27187527577563086
iteration : 3252
train acc:  0.8203125
train loss:  0.42097097635269165
train gradient:  0.3892994492895515
iteration : 3253
train acc:  0.8671875
train loss:  0.31580233573913574
train gradient:  0.28064667903624874
iteration : 3254
train acc:  0.796875
train loss:  0.39048081636428833
train gradient:  0.35552549540111417
iteration : 3255
train acc:  0.8515625
train loss:  0.34649893641471863
train gradient:  0.27380511759583726
iteration : 3256
train acc:  0.859375
train loss:  0.35165563225746155
train gradient:  0.2790759939678677
iteration : 3257
train acc:  0.7578125
train loss:  0.48117876052856445
train gradient:  0.9189989827230054
iteration : 3258
train acc:  0.84375
train loss:  0.349056214094162
train gradient:  0.24479524171483963
iteration : 3259
train acc:  0.8203125
train loss:  0.42925357818603516
train gradient:  0.3380183128297604
iteration : 3260
train acc:  0.84375
train loss:  0.37713944911956787
train gradient:  0.2803998529585147
iteration : 3261
train acc:  0.7890625
train loss:  0.43233221769332886
train gradient:  0.48952798932759484
iteration : 3262
train acc:  0.8359375
train loss:  0.37038207054138184
train gradient:  0.31084318833683744
iteration : 3263
train acc:  0.828125
train loss:  0.37788471579551697
train gradient:  0.29046745219411435
iteration : 3264
train acc:  0.8671875
train loss:  0.32217326760292053
train gradient:  0.2998538936272557
iteration : 3265
train acc:  0.828125
train loss:  0.3568253517150879
train gradient:  0.2329740834280849
iteration : 3266
train acc:  0.84375
train loss:  0.3441455364227295
train gradient:  0.2600493332499229
iteration : 3267
train acc:  0.8046875
train loss:  0.3894202411174774
train gradient:  0.4314040037628485
iteration : 3268
train acc:  0.8359375
train loss:  0.38649988174438477
train gradient:  0.307973314758313
iteration : 3269
train acc:  0.828125
train loss:  0.3911232054233551
train gradient:  0.3452264242381102
iteration : 3270
train acc:  0.8515625
train loss:  0.338927686214447
train gradient:  0.3954003426663616
iteration : 3271
train acc:  0.859375
train loss:  0.3453516364097595
train gradient:  0.3025349559104552
iteration : 3272
train acc:  0.828125
train loss:  0.36569756269454956
train gradient:  0.214558931026313
iteration : 3273
train acc:  0.8671875
train loss:  0.32286185026168823
train gradient:  0.23467172573534387
iteration : 3274
train acc:  0.7890625
train loss:  0.4242151975631714
train gradient:  0.3663827646163155
iteration : 3275
train acc:  0.8125
train loss:  0.420318067073822
train gradient:  0.34805058872439215
iteration : 3276
train acc:  0.8828125
train loss:  0.29100272059440613
train gradient:  0.2503692022930741
iteration : 3277
train acc:  0.8203125
train loss:  0.38646093010902405
train gradient:  0.3827866041440812
iteration : 3278
train acc:  0.8359375
train loss:  0.43350136280059814
train gradient:  0.4746839216489177
iteration : 3279
train acc:  0.8359375
train loss:  0.4221441149711609
train gradient:  0.29426352395101685
iteration : 3280
train acc:  0.828125
train loss:  0.36084258556365967
train gradient:  0.24733696775997657
iteration : 3281
train acc:  0.8203125
train loss:  0.3712512254714966
train gradient:  0.3558476593374248
iteration : 3282
train acc:  0.828125
train loss:  0.32653266191482544
train gradient:  0.22166613927102308
iteration : 3283
train acc:  0.828125
train loss:  0.3757612705230713
train gradient:  0.3837112093870434
iteration : 3284
train acc:  0.828125
train loss:  0.37087684869766235
train gradient:  0.3581705629618757
iteration : 3285
train acc:  0.875
train loss:  0.30970999598503113
train gradient:  0.23428745447802202
iteration : 3286
train acc:  0.75
train loss:  0.5380780100822449
train gradient:  0.7046256500566095
iteration : 3287
train acc:  0.828125
train loss:  0.4002356231212616
train gradient:  0.2931108371691282
iteration : 3288
train acc:  0.8125
train loss:  0.38994115591049194
train gradient:  0.3010574482412805
iteration : 3289
train acc:  0.7734375
train loss:  0.44223713874816895
train gradient:  0.2807735666625072
iteration : 3290
train acc:  0.859375
train loss:  0.3083542287349701
train gradient:  0.23127747268989496
iteration : 3291
train acc:  0.7734375
train loss:  0.4170364737510681
train gradient:  0.4237629395197982
iteration : 3292
train acc:  0.78125
train loss:  0.43076199293136597
train gradient:  0.29222997652056715
iteration : 3293
train acc:  0.875
train loss:  0.31094229221343994
train gradient:  0.2171438488088722
iteration : 3294
train acc:  0.8046875
train loss:  0.3951151371002197
train gradient:  0.44060883744435375
iteration : 3295
train acc:  0.859375
train loss:  0.3360528349876404
train gradient:  0.3041542435316528
iteration : 3296
train acc:  0.8125
train loss:  0.3766404986381531
train gradient:  0.383212283271295
iteration : 3297
train acc:  0.8515625
train loss:  0.35069704055786133
train gradient:  0.38333966625564564
iteration : 3298
train acc:  0.8671875
train loss:  0.3057366609573364
train gradient:  0.3494146833245493
iteration : 3299
train acc:  0.890625
train loss:  0.31143707036972046
train gradient:  0.32070871895587255
iteration : 3300
train acc:  0.8515625
train loss:  0.3572271466255188
train gradient:  0.37122667478076204
iteration : 3301
train acc:  0.8203125
train loss:  0.41246694326400757
train gradient:  0.3158117894683683
iteration : 3302
train acc:  0.796875
train loss:  0.4651055634021759
train gradient:  0.3644253009257731
iteration : 3303
train acc:  0.8046875
train loss:  0.42122387886047363
train gradient:  0.45926631011212904
iteration : 3304
train acc:  0.7578125
train loss:  0.4202243685722351
train gradient:  0.31459669915365607
iteration : 3305
train acc:  0.875
train loss:  0.2810547947883606
train gradient:  0.23171265777581967
iteration : 3306
train acc:  0.8125
train loss:  0.42694926261901855
train gradient:  0.35558336304722526
iteration : 3307
train acc:  0.7890625
train loss:  0.44555380940437317
train gradient:  0.43205151044691126
iteration : 3308
train acc:  0.875
train loss:  0.34876954555511475
train gradient:  0.2381456458443268
iteration : 3309
train acc:  0.796875
train loss:  0.4269556999206543
train gradient:  0.36186345939987974
iteration : 3310
train acc:  0.8515625
train loss:  0.36317452788352966
train gradient:  0.2660731145374839
iteration : 3311
train acc:  0.84375
train loss:  0.33972829580307007
train gradient:  0.2927023425493975
iteration : 3312
train acc:  0.8515625
train loss:  0.35977667570114136
train gradient:  0.2890050344243021
iteration : 3313
train acc:  0.796875
train loss:  0.41739341616630554
train gradient:  0.2582240871358209
iteration : 3314
train acc:  0.8515625
train loss:  0.3747013509273529
train gradient:  0.22079948822671208
iteration : 3315
train acc:  0.8203125
train loss:  0.3824119567871094
train gradient:  0.27679920374915096
iteration : 3316
train acc:  0.8046875
train loss:  0.33700796961784363
train gradient:  0.2866843816228889
iteration : 3317
train acc:  0.75
train loss:  0.5137739181518555
train gradient:  0.5298399611189984
iteration : 3318
train acc:  0.828125
train loss:  0.3463665246963501
train gradient:  0.24495533163626873
iteration : 3319
train acc:  0.84375
train loss:  0.35253459215164185
train gradient:  0.27074970871319115
iteration : 3320
train acc:  0.8046875
train loss:  0.407833456993103
train gradient:  0.3708794833492882
iteration : 3321
train acc:  0.828125
train loss:  0.413746178150177
train gradient:  0.3044452992798081
iteration : 3322
train acc:  0.8515625
train loss:  0.3148908019065857
train gradient:  0.3966001461593295
iteration : 3323
train acc:  0.828125
train loss:  0.3482866883277893
train gradient:  0.30851182727579374
iteration : 3324
train acc:  0.828125
train loss:  0.3651980757713318
train gradient:  0.22388951111609853
iteration : 3325
train acc:  0.84375
train loss:  0.3489062190055847
train gradient:  0.2767093798302043
iteration : 3326
train acc:  0.78125
train loss:  0.47295692563056946
train gradient:  0.40968103941107176
iteration : 3327
train acc:  0.8125
train loss:  0.42464548349380493
train gradient:  0.3464604551241458
iteration : 3328
train acc:  0.828125
train loss:  0.3728017807006836
train gradient:  0.3547268778210478
iteration : 3329
train acc:  0.8046875
train loss:  0.4083414375782013
train gradient:  0.3141350394108456
iteration : 3330
train acc:  0.8359375
train loss:  0.38448864221572876
train gradient:  0.3800441497627661
iteration : 3331
train acc:  0.859375
train loss:  0.3632339537143707
train gradient:  0.1826810278076132
iteration : 3332
train acc:  0.8515625
train loss:  0.3473810851573944
train gradient:  0.22796564851342763
iteration : 3333
train acc:  0.8671875
train loss:  0.33690762519836426
train gradient:  0.24733854485190299
iteration : 3334
train acc:  0.796875
train loss:  0.4227878153324127
train gradient:  0.40911817160125313
iteration : 3335
train acc:  0.859375
train loss:  0.3194255530834198
train gradient:  0.24827644158326861
iteration : 3336
train acc:  0.859375
train loss:  0.3422885239124298
train gradient:  0.26529730502295995
iteration : 3337
train acc:  0.84375
train loss:  0.375993937253952
train gradient:  0.2354904466930572
iteration : 3338
train acc:  0.8671875
train loss:  0.3391708731651306
train gradient:  0.2735385583871659
iteration : 3339
train acc:  0.8046875
train loss:  0.3787505328655243
train gradient:  0.3013563476809679
iteration : 3340
train acc:  0.8359375
train loss:  0.3346678614616394
train gradient:  0.2306978110343884
iteration : 3341
train acc:  0.8671875
train loss:  0.33872491121292114
train gradient:  0.22332265096784826
iteration : 3342
train acc:  0.796875
train loss:  0.436779648065567
train gradient:  0.4382103238581927
iteration : 3343
train acc:  0.828125
train loss:  0.3897557258605957
train gradient:  0.2833824102680498
iteration : 3344
train acc:  0.7578125
train loss:  0.48446038365364075
train gradient:  0.5014731081508546
iteration : 3345
train acc:  0.8125
train loss:  0.39275744557380676
train gradient:  0.2946412522022794
iteration : 3346
train acc:  0.8203125
train loss:  0.3796895146369934
train gradient:  0.37990770415335573
iteration : 3347
train acc:  0.84375
train loss:  0.32801491022109985
train gradient:  0.2211039447327729
iteration : 3348
train acc:  0.8125
train loss:  0.4112323820590973
train gradient:  0.3807253388498855
iteration : 3349
train acc:  0.8828125
train loss:  0.327353835105896
train gradient:  0.2303818060394277
iteration : 3350
train acc:  0.859375
train loss:  0.3182280659675598
train gradient:  0.2289818981868001
iteration : 3351
train acc:  0.8203125
train loss:  0.39726367592811584
train gradient:  0.3181057650099649
iteration : 3352
train acc:  0.8515625
train loss:  0.40211766958236694
train gradient:  0.35841042550267954
iteration : 3353
train acc:  0.8515625
train loss:  0.3495718240737915
train gradient:  0.26216197550807957
iteration : 3354
train acc:  0.8046875
train loss:  0.4172481298446655
train gradient:  0.4069959838603273
iteration : 3355
train acc:  0.8125
train loss:  0.37336617708206177
train gradient:  0.27494653203456076
iteration : 3356
train acc:  0.8671875
train loss:  0.3722963035106659
train gradient:  0.2828109292028841
iteration : 3357
train acc:  0.8125
train loss:  0.38080838322639465
train gradient:  0.4506237366738321
iteration : 3358
train acc:  0.8125
train loss:  0.39160478115081787
train gradient:  0.4022222308772144
iteration : 3359
train acc:  0.8828125
train loss:  0.3536885678768158
train gradient:  0.2817190696760935
iteration : 3360
train acc:  0.8359375
train loss:  0.37421709299087524
train gradient:  0.3165923490608696
iteration : 3361
train acc:  0.765625
train loss:  0.4762161076068878
train gradient:  0.4141657949616063
iteration : 3362
train acc:  0.8125
train loss:  0.3930387496948242
train gradient:  0.3386865670810224
iteration : 3363
train acc:  0.8046875
train loss:  0.44351711869239807
train gradient:  0.32843692416398423
iteration : 3364
train acc:  0.84375
train loss:  0.38721615076065063
train gradient:  0.3254295677845508
iteration : 3365
train acc:  0.8203125
train loss:  0.4602210521697998
train gradient:  0.4172208482084294
iteration : 3366
train acc:  0.78125
train loss:  0.4272725582122803
train gradient:  0.3491033010388254
iteration : 3367
train acc:  0.8125
train loss:  0.4072282314300537
train gradient:  0.27034649088511303
iteration : 3368
train acc:  0.84375
train loss:  0.3239206075668335
train gradient:  0.16707152884611803
iteration : 3369
train acc:  0.8671875
train loss:  0.36370643973350525
train gradient:  0.3041839586176933
iteration : 3370
train acc:  0.8515625
train loss:  0.34847480058670044
train gradient:  0.22447546773813123
iteration : 3371
train acc:  0.859375
train loss:  0.3379565477371216
train gradient:  0.2754685996273771
iteration : 3372
train acc:  0.8203125
train loss:  0.4428742825984955
train gradient:  0.49775310741887685
iteration : 3373
train acc:  0.8359375
train loss:  0.3967098593711853
train gradient:  0.2861906790916658
iteration : 3374
train acc:  0.8515625
train loss:  0.32977429032325745
train gradient:  0.25775880480417707
iteration : 3375
train acc:  0.7890625
train loss:  0.4727712869644165
train gradient:  0.4414652855780193
iteration : 3376
train acc:  0.859375
train loss:  0.34196633100509644
train gradient:  0.30845147560521596
iteration : 3377
train acc:  0.875
train loss:  0.3051983118057251
train gradient:  0.18700966510637243
iteration : 3378
train acc:  0.8359375
train loss:  0.33355897665023804
train gradient:  0.2296186710934947
iteration : 3379
train acc:  0.828125
train loss:  0.32869064807891846
train gradient:  0.1859081121085485
iteration : 3380
train acc:  0.859375
train loss:  0.35216835141181946
train gradient:  0.25362244085944863
iteration : 3381
train acc:  0.8203125
train loss:  0.33149033784866333
train gradient:  0.20556794811824727
iteration : 3382
train acc:  0.875
train loss:  0.323307603597641
train gradient:  0.21511857348971855
iteration : 3383
train acc:  0.8515625
train loss:  0.3368980884552002
train gradient:  0.22694987134090125
iteration : 3384
train acc:  0.8125
train loss:  0.4067906141281128
train gradient:  0.32531694417931034
iteration : 3385
train acc:  0.796875
train loss:  0.3890325427055359
train gradient:  0.3399623596628853
iteration : 3386
train acc:  0.796875
train loss:  0.4115285277366638
train gradient:  0.34219956660196255
iteration : 3387
train acc:  0.8046875
train loss:  0.4170367121696472
train gradient:  0.3177427700013328
iteration : 3388
train acc:  0.8671875
train loss:  0.3154200613498688
train gradient:  0.22691543129505048
iteration : 3389
train acc:  0.796875
train loss:  0.38718950748443604
train gradient:  0.23773615105242007
iteration : 3390
train acc:  0.859375
train loss:  0.3815065622329712
train gradient:  0.3295571257597314
iteration : 3391
train acc:  0.8125
train loss:  0.4086124300956726
train gradient:  0.25600579740401086
iteration : 3392
train acc:  0.8671875
train loss:  0.3606952130794525
train gradient:  0.2734173275171755
iteration : 3393
train acc:  0.8359375
train loss:  0.37439078092575073
train gradient:  0.22035759333960128
iteration : 3394
train acc:  0.78125
train loss:  0.4294421076774597
train gradient:  0.45694176396177866
iteration : 3395
train acc:  0.8125
train loss:  0.39239978790283203
train gradient:  0.3216040669694608
iteration : 3396
train acc:  0.78125
train loss:  0.38876616954803467
train gradient:  0.2842873224956494
iteration : 3397
train acc:  0.8125
train loss:  0.3806799650192261
train gradient:  0.2692003578763828
iteration : 3398
train acc:  0.8515625
train loss:  0.37003207206726074
train gradient:  0.30991727370974537
iteration : 3399
train acc:  0.8359375
train loss:  0.36785441637039185
train gradient:  0.43836603428776644
iteration : 3400
train acc:  0.859375
train loss:  0.33682262897491455
train gradient:  0.24124986440914992
iteration : 3401
train acc:  0.8046875
train loss:  0.43234553933143616
train gradient:  0.3429991679933337
iteration : 3402
train acc:  0.8203125
train loss:  0.3399788737297058
train gradient:  0.3327506518817967
iteration : 3403
train acc:  0.8515625
train loss:  0.3245088756084442
train gradient:  0.25149386013108466
iteration : 3404
train acc:  0.828125
train loss:  0.3828304409980774
train gradient:  0.4810125647156532
iteration : 3405
train acc:  0.8671875
train loss:  0.32636529207229614
train gradient:  0.34358962398583853
iteration : 3406
train acc:  0.8671875
train loss:  0.2731798589229584
train gradient:  0.20870455625191947
iteration : 3407
train acc:  0.7734375
train loss:  0.42993009090423584
train gradient:  0.4511849968198982
iteration : 3408
train acc:  0.8359375
train loss:  0.3193429708480835
train gradient:  0.292248429838876
iteration : 3409
train acc:  0.75
train loss:  0.46583908796310425
train gradient:  0.461486359947581
iteration : 3410
train acc:  0.859375
train loss:  0.35271117091178894
train gradient:  0.2657401518619007
iteration : 3411
train acc:  0.8515625
train loss:  0.40614503622055054
train gradient:  0.33502989563395325
iteration : 3412
train acc:  0.8671875
train loss:  0.32479244470596313
train gradient:  0.3041505444638813
iteration : 3413
train acc:  0.78125
train loss:  0.4987720251083374
train gradient:  0.41739165089591235
iteration : 3414
train acc:  0.8203125
train loss:  0.37154436111450195
train gradient:  0.3550721719187893
iteration : 3415
train acc:  0.890625
train loss:  0.34979045391082764
train gradient:  0.29950149016978356
iteration : 3416
train acc:  0.890625
train loss:  0.31207364797592163
train gradient:  0.19520103066724398
iteration : 3417
train acc:  0.828125
train loss:  0.40335220098495483
train gradient:  0.3397626564531362
iteration : 3418
train acc:  0.859375
train loss:  0.30640217661857605
train gradient:  0.2776419007850528
iteration : 3419
train acc:  0.7890625
train loss:  0.4272567629814148
train gradient:  0.4462995761671265
iteration : 3420
train acc:  0.890625
train loss:  0.2902185916900635
train gradient:  0.20761933638373403
iteration : 3421
train acc:  0.8125
train loss:  0.4411553740501404
train gradient:  0.6617083817025997
iteration : 3422
train acc:  0.859375
train loss:  0.3101416230201721
train gradient:  0.2930076306032662
iteration : 3423
train acc:  0.8203125
train loss:  0.3956851363182068
train gradient:  0.3974561287832477
iteration : 3424
train acc:  0.8671875
train loss:  0.360901802778244
train gradient:  0.23540911841274492
iteration : 3425
train acc:  0.890625
train loss:  0.31374669075012207
train gradient:  0.304978714287317
iteration : 3426
train acc:  0.8203125
train loss:  0.41490352153778076
train gradient:  0.38293336868024624
iteration : 3427
train acc:  0.8046875
train loss:  0.41389936208724976
train gradient:  0.4232747832614496
iteration : 3428
train acc:  0.8203125
train loss:  0.44190895557403564
train gradient:  0.5491346597596629
iteration : 3429
train acc:  0.8046875
train loss:  0.4217381477355957
train gradient:  0.3974047035606591
iteration : 3430
train acc:  0.875
train loss:  0.34233802556991577
train gradient:  0.3854655803353336
iteration : 3431
train acc:  0.828125
train loss:  0.33707913756370544
train gradient:  0.21231885321716348
iteration : 3432
train acc:  0.8125
train loss:  0.4118709862232208
train gradient:  0.4347354921744963
iteration : 3433
train acc:  0.8359375
train loss:  0.4593527317047119
train gradient:  0.4302992342880047
iteration : 3434
train acc:  0.828125
train loss:  0.36114367842674255
train gradient:  0.4047331893792213
iteration : 3435
train acc:  0.8671875
train loss:  0.36096861958503723
train gradient:  0.2990865780575516
iteration : 3436
train acc:  0.828125
train loss:  0.3779478371143341
train gradient:  0.2820831541195116
iteration : 3437
train acc:  0.7890625
train loss:  0.44338563084602356
train gradient:  0.5298332389896545
iteration : 3438
train acc:  0.7890625
train loss:  0.4872746467590332
train gradient:  0.4411455337154263
iteration : 3439
train acc:  0.8125
train loss:  0.3673549294471741
train gradient:  0.30782845002401427
iteration : 3440
train acc:  0.8671875
train loss:  0.35487982630729675
train gradient:  0.2306353651895578
iteration : 3441
train acc:  0.7890625
train loss:  0.4153245687484741
train gradient:  0.4298261679280217
iteration : 3442
train acc:  0.8515625
train loss:  0.3755982518196106
train gradient:  0.4295144315787622
iteration : 3443
train acc:  0.828125
train loss:  0.4166853427886963
train gradient:  0.2526428991971924
iteration : 3444
train acc:  0.828125
train loss:  0.3837939500808716
train gradient:  0.3378643595737866
iteration : 3445
train acc:  0.765625
train loss:  0.42301130294799805
train gradient:  0.43212216042559404
iteration : 3446
train acc:  0.8359375
train loss:  0.3840108811855316
train gradient:  0.29909475663399127
iteration : 3447
train acc:  0.8046875
train loss:  0.3650848865509033
train gradient:  0.3761066037357085
iteration : 3448
train acc:  0.8359375
train loss:  0.3238894045352936
train gradient:  0.26523402662240775
iteration : 3449
train acc:  0.859375
train loss:  0.33194616436958313
train gradient:  0.2845565115056532
iteration : 3450
train acc:  0.765625
train loss:  0.5110378861427307
train gradient:  0.5890746499075462
iteration : 3451
train acc:  0.78125
train loss:  0.4502706527709961
train gradient:  0.4224660418826489
iteration : 3452
train acc:  0.8515625
train loss:  0.38171225786209106
train gradient:  0.2867808385592869
iteration : 3453
train acc:  0.890625
train loss:  0.3061434328556061
train gradient:  0.4880472368504155
iteration : 3454
train acc:  0.8515625
train loss:  0.3389953672885895
train gradient:  0.2980540924138959
iteration : 3455
train acc:  0.8359375
train loss:  0.45777761936187744
train gradient:  0.46151280794749117
iteration : 3456
train acc:  0.7578125
train loss:  0.41795212030410767
train gradient:  0.34748014436837493
iteration : 3457
train acc:  0.875
train loss:  0.3252890408039093
train gradient:  0.27442272563474274
iteration : 3458
train acc:  0.796875
train loss:  0.4556225538253784
train gradient:  0.33047267953071174
iteration : 3459
train acc:  0.84375
train loss:  0.36525392532348633
train gradient:  0.35593687577525596
iteration : 3460
train acc:  0.828125
train loss:  0.3350380063056946
train gradient:  0.33435229891200263
iteration : 3461
train acc:  0.8046875
train loss:  0.3533482253551483
train gradient:  0.2908129402570882
iteration : 3462
train acc:  0.890625
train loss:  0.306852251291275
train gradient:  0.34938603268795015
iteration : 3463
train acc:  0.8125
train loss:  0.39739522337913513
train gradient:  0.368429470989767
iteration : 3464
train acc:  0.8515625
train loss:  0.3449646830558777
train gradient:  0.22294510380647448
iteration : 3465
train acc:  0.8671875
train loss:  0.32696977257728577
train gradient:  0.20297680261426765
iteration : 3466
train acc:  0.828125
train loss:  0.3842240869998932
train gradient:  0.3182051604770242
iteration : 3467
train acc:  0.8671875
train loss:  0.37217527627944946
train gradient:  0.28406442733778553
iteration : 3468
train acc:  0.7578125
train loss:  0.41526341438293457
train gradient:  0.2798155998685057
iteration : 3469
train acc:  0.8203125
train loss:  0.37306278944015503
train gradient:  0.3552228689705394
iteration : 3470
train acc:  0.875
train loss:  0.3301433026790619
train gradient:  0.24800047113131013
iteration : 3471
train acc:  0.84375
train loss:  0.4098876118659973
train gradient:  0.3506435064452836
iteration : 3472
train acc:  0.8671875
train loss:  0.34342989325523376
train gradient:  0.29023128414518073
iteration : 3473
train acc:  0.8203125
train loss:  0.3667834997177124
train gradient:  0.29040620844346837
iteration : 3474
train acc:  0.8828125
train loss:  0.25729918479919434
train gradient:  0.2205441827782041
iteration : 3475
train acc:  0.8125
train loss:  0.43261393904685974
train gradient:  0.35456640093297104
iteration : 3476
train acc:  0.8125
train loss:  0.4041360020637512
train gradient:  0.3374208593219633
iteration : 3477
train acc:  0.84375
train loss:  0.3767626881599426
train gradient:  0.26653614926501606
iteration : 3478
train acc:  0.7421875
train loss:  0.49954044818878174
train gradient:  0.47793560134105056
iteration : 3479
train acc:  0.7734375
train loss:  0.4663408398628235
train gradient:  0.43086808083005723
iteration : 3480
train acc:  0.859375
train loss:  0.27580106258392334
train gradient:  0.22947423342383408
iteration : 3481
train acc:  0.8203125
train loss:  0.384803831577301
train gradient:  0.2918326320196731
iteration : 3482
train acc:  0.7890625
train loss:  0.3871089220046997
train gradient:  0.43042856693441855
iteration : 3483
train acc:  0.84375
train loss:  0.3779950737953186
train gradient:  0.28496894446211357
iteration : 3484
train acc:  0.7734375
train loss:  0.4594041407108307
train gradient:  0.44234662953387377
iteration : 3485
train acc:  0.8671875
train loss:  0.3106901943683624
train gradient:  0.23272971627712283
iteration : 3486
train acc:  0.8203125
train loss:  0.42135903239250183
train gradient:  0.38999994127397547
iteration : 3487
train acc:  0.796875
train loss:  0.3669484853744507
train gradient:  0.2605664466470404
iteration : 3488
train acc:  0.8046875
train loss:  0.38846683502197266
train gradient:  0.4203578190860288
iteration : 3489
train acc:  0.828125
train loss:  0.347368061542511
train gradient:  0.298988439605676
iteration : 3490
train acc:  0.8125
train loss:  0.3917396068572998
train gradient:  0.33505828806856086
iteration : 3491
train acc:  0.859375
train loss:  0.3407549262046814
train gradient:  0.286869099197714
iteration : 3492
train acc:  0.8359375
train loss:  0.3443385362625122
train gradient:  0.31793255030684897
iteration : 3493
train acc:  0.8828125
train loss:  0.31381335854530334
train gradient:  0.217131837556674
iteration : 3494
train acc:  0.84375
train loss:  0.38897228240966797
train gradient:  0.33829156887935874
iteration : 3495
train acc:  0.84375
train loss:  0.39453911781311035
train gradient:  0.2846573409028688
iteration : 3496
train acc:  0.8515625
train loss:  0.3582054078578949
train gradient:  0.2526629133619975
iteration : 3497
train acc:  0.7890625
train loss:  0.4014516770839691
train gradient:  0.37026081534949035
iteration : 3498
train acc:  0.859375
train loss:  0.3644446134567261
train gradient:  0.2728881207003374
iteration : 3499
train acc:  0.90625
train loss:  0.2964703142642975
train gradient:  0.21815972939998668
iteration : 3500
train acc:  0.828125
train loss:  0.37142789363861084
train gradient:  0.372206712321469
iteration : 3501
train acc:  0.8515625
train loss:  0.3128753900527954
train gradient:  0.26942461610220547
iteration : 3502
train acc:  0.859375
train loss:  0.3141774535179138
train gradient:  0.2658860292816292
iteration : 3503
train acc:  0.8203125
train loss:  0.405831515789032
train gradient:  0.47917941880174075
iteration : 3504
train acc:  0.75
train loss:  0.5631952881813049
train gradient:  0.4954167082149211
iteration : 3505
train acc:  0.8125
train loss:  0.42671188712120056
train gradient:  0.36617851239429033
iteration : 3506
train acc:  0.796875
train loss:  0.40306997299194336
train gradient:  0.34533894372374174
iteration : 3507
train acc:  0.8984375
train loss:  0.3111790418624878
train gradient:  0.20273573515546472
iteration : 3508
train acc:  0.828125
train loss:  0.3614397943019867
train gradient:  0.30174057401395804
iteration : 3509
train acc:  0.84375
train loss:  0.38757163286209106
train gradient:  0.3075333627484738
iteration : 3510
train acc:  0.7734375
train loss:  0.43536168336868286
train gradient:  0.40885661139471025
iteration : 3511
train acc:  0.8671875
train loss:  0.37691664695739746
train gradient:  0.2320847448022926
iteration : 3512
train acc:  0.875
train loss:  0.3261847496032715
train gradient:  0.2756783873402825
iteration : 3513
train acc:  0.828125
train loss:  0.3258780837059021
train gradient:  0.19667287784057774
iteration : 3514
train acc:  0.8515625
train loss:  0.349942684173584
train gradient:  0.34488310865699345
iteration : 3515
train acc:  0.859375
train loss:  0.34406059980392456
train gradient:  0.31135115750916015
iteration : 3516
train acc:  0.7890625
train loss:  0.4387850761413574
train gradient:  0.4924118609514181
iteration : 3517
train acc:  0.8125
train loss:  0.47951698303222656
train gradient:  0.4277679696093568
iteration : 3518
train acc:  0.84375
train loss:  0.4180618226528168
train gradient:  0.49664606334118333
iteration : 3519
train acc:  0.8515625
train loss:  0.3386181890964508
train gradient:  0.291457843713664
iteration : 3520
train acc:  0.84375
train loss:  0.3142070770263672
train gradient:  0.2367417824831272
iteration : 3521
train acc:  0.859375
train loss:  0.32379600405693054
train gradient:  0.3473110148196899
iteration : 3522
train acc:  0.796875
train loss:  0.35089895129203796
train gradient:  0.49862321549642596
iteration : 3523
train acc:  0.78125
train loss:  0.4468228220939636
train gradient:  0.3761027344680422
iteration : 3524
train acc:  0.84375
train loss:  0.3616443872451782
train gradient:  0.43284734203726727
iteration : 3525
train acc:  0.859375
train loss:  0.3415968418121338
train gradient:  0.317477963368967
iteration : 3526
train acc:  0.8046875
train loss:  0.40156957507133484
train gradient:  0.3893246747262112
iteration : 3527
train acc:  0.84375
train loss:  0.31344330310821533
train gradient:  0.19806412373144144
iteration : 3528
train acc:  0.875
train loss:  0.33889544010162354
train gradient:  0.3233036901565987
iteration : 3529
train acc:  0.84375
train loss:  0.4326707720756531
train gradient:  0.3640175418922648
iteration : 3530
train acc:  0.8203125
train loss:  0.3493700325489044
train gradient:  0.31533818338737635
iteration : 3531
train acc:  0.78125
train loss:  0.49265584349632263
train gradient:  0.5709812675438315
iteration : 3532
train acc:  0.828125
train loss:  0.3789878487586975
train gradient:  0.3611487442436809
iteration : 3533
train acc:  0.8359375
train loss:  0.4020957946777344
train gradient:  0.4536194921426232
iteration : 3534
train acc:  0.8671875
train loss:  0.38139551877975464
train gradient:  0.3185607803955459
iteration : 3535
train acc:  0.8828125
train loss:  0.32289713621139526
train gradient:  0.21806035058032824
iteration : 3536
train acc:  0.8359375
train loss:  0.39274320006370544
train gradient:  0.27750872572117546
iteration : 3537
train acc:  0.84375
train loss:  0.39701545238494873
train gradient:  0.27789826302614123
iteration : 3538
train acc:  0.8203125
train loss:  0.3916858434677124
train gradient:  0.3469967015345718
iteration : 3539
train acc:  0.828125
train loss:  0.44380393624305725
train gradient:  0.3516963573884429
iteration : 3540
train acc:  0.8515625
train loss:  0.3607713580131531
train gradient:  0.3321745724975281
iteration : 3541
train acc:  0.796875
train loss:  0.37629878520965576
train gradient:  0.2828718678219808
iteration : 3542
train acc:  0.8515625
train loss:  0.32499492168426514
train gradient:  0.28102539043495184
iteration : 3543
train acc:  0.828125
train loss:  0.41656607389450073
train gradient:  0.37896658716896986
iteration : 3544
train acc:  0.859375
train loss:  0.34604713320732117
train gradient:  0.201522978467239
iteration : 3545
train acc:  0.8984375
train loss:  0.3058492839336395
train gradient:  0.2399940236272166
iteration : 3546
train acc:  0.859375
train loss:  0.30436205863952637
train gradient:  0.2608187872193971
iteration : 3547
train acc:  0.8671875
train loss:  0.3244335949420929
train gradient:  0.2573926364038375
iteration : 3548
train acc:  0.84375
train loss:  0.3599414825439453
train gradient:  0.28334297441093714
iteration : 3549
train acc:  0.8046875
train loss:  0.4414036273956299
train gradient:  0.4671831794823911
iteration : 3550
train acc:  0.828125
train loss:  0.3439827561378479
train gradient:  0.25749303987338684
iteration : 3551
train acc:  0.828125
train loss:  0.4128227233886719
train gradient:  0.3668004643449353
iteration : 3552
train acc:  0.8203125
train loss:  0.382479727268219
train gradient:  0.2817642200380094
iteration : 3553
train acc:  0.7734375
train loss:  0.45173120498657227
train gradient:  0.4177787415644046
iteration : 3554
train acc:  0.8671875
train loss:  0.32834768295288086
train gradient:  0.19053171848973105
iteration : 3555
train acc:  0.828125
train loss:  0.3757433295249939
train gradient:  0.23503291916784513
iteration : 3556
train acc:  0.8359375
train loss:  0.3863946795463562
train gradient:  0.33043601224423813
iteration : 3557
train acc:  0.8984375
train loss:  0.2860892415046692
train gradient:  0.2375853832669263
iteration : 3558
train acc:  0.8671875
train loss:  0.28913578391075134
train gradient:  0.187144111382756
iteration : 3559
train acc:  0.8125
train loss:  0.39525753259658813
train gradient:  0.2646392091914575
iteration : 3560
train acc:  0.890625
train loss:  0.38268575072288513
train gradient:  0.2208270292676877
iteration : 3561
train acc:  0.8359375
train loss:  0.34929096698760986
train gradient:  0.34168893791308536
iteration : 3562
train acc:  0.8203125
train loss:  0.3662385642528534
train gradient:  0.2142494810967203
iteration : 3563
train acc:  0.8203125
train loss:  0.40281927585601807
train gradient:  0.27774639508426147
iteration : 3564
train acc:  0.8046875
train loss:  0.3885837495326996
train gradient:  0.3804913730082801
iteration : 3565
train acc:  0.8359375
train loss:  0.35065069794654846
train gradient:  0.3145272944223406
iteration : 3566
train acc:  0.8359375
train loss:  0.36879628896713257
train gradient:  0.33481058309032974
iteration : 3567
train acc:  0.828125
train loss:  0.34134480357170105
train gradient:  0.20508067404969685
iteration : 3568
train acc:  0.8515625
train loss:  0.34889930486679077
train gradient:  0.3560050334535619
iteration : 3569
train acc:  0.8359375
train loss:  0.364975243806839
train gradient:  0.3415970783841627
iteration : 3570
train acc:  0.8515625
train loss:  0.319659024477005
train gradient:  0.21904601420162945
iteration : 3571
train acc:  0.84375
train loss:  0.3613583445549011
train gradient:  0.26959354272486175
iteration : 3572
train acc:  0.828125
train loss:  0.38224124908447266
train gradient:  0.24429686951529234
iteration : 3573
train acc:  0.8515625
train loss:  0.37904250621795654
train gradient:  0.2971517134310021
iteration : 3574
train acc:  0.859375
train loss:  0.30834752321243286
train gradient:  0.1999583856493781
iteration : 3575
train acc:  0.8671875
train loss:  0.2939850687980652
train gradient:  0.2619482239134117
iteration : 3576
train acc:  0.90625
train loss:  0.2612455189228058
train gradient:  0.16602086454484022
iteration : 3577
train acc:  0.859375
train loss:  0.3710216283798218
train gradient:  0.33498535121161965
iteration : 3578
train acc:  0.859375
train loss:  0.33643725514411926
train gradient:  0.2692438102960295
iteration : 3579
train acc:  0.8359375
train loss:  0.37337726354599
train gradient:  0.27405136727779084
iteration : 3580
train acc:  0.8203125
train loss:  0.46137869358062744
train gradient:  0.4441744183994093
iteration : 3581
train acc:  0.828125
train loss:  0.37542790174484253
train gradient:  0.24136668381638407
iteration : 3582
train acc:  0.8515625
train loss:  0.36359351873397827
train gradient:  0.23272825948373285
iteration : 3583
train acc:  0.84375
train loss:  0.36374613642692566
train gradient:  0.31350619181287503
iteration : 3584
train acc:  0.890625
train loss:  0.3269934058189392
train gradient:  0.25660351745375287
iteration : 3585
train acc:  0.828125
train loss:  0.418358713388443
train gradient:  0.3981424403089154
iteration : 3586
train acc:  0.8359375
train loss:  0.35463470220565796
train gradient:  0.3023149406674004
iteration : 3587
train acc:  0.75
train loss:  0.5643930435180664
train gradient:  0.6207282784191661
iteration : 3588
train acc:  0.84375
train loss:  0.3025727868080139
train gradient:  0.22223139810301298
iteration : 3589
train acc:  0.828125
train loss:  0.4665548801422119
train gradient:  0.46515958519627293
iteration : 3590
train acc:  0.78125
train loss:  0.4733484983444214
train gradient:  0.5591195586918578
iteration : 3591
train acc:  0.8046875
train loss:  0.40932950377464294
train gradient:  0.41974567766812815
iteration : 3592
train acc:  0.828125
train loss:  0.36947259306907654
train gradient:  0.3998790611881806
iteration : 3593
train acc:  0.875
train loss:  0.34913304448127747
train gradient:  0.2473060578733459
iteration : 3594
train acc:  0.75
train loss:  0.4420909285545349
train gradient:  0.5598814785168259
iteration : 3595
train acc:  0.84375
train loss:  0.35092541575431824
train gradient:  0.33905996193497767
iteration : 3596
train acc:  0.8671875
train loss:  0.3045533001422882
train gradient:  0.21432149797104375
iteration : 3597
train acc:  0.765625
train loss:  0.43310922384262085
train gradient:  0.3191044804295068
iteration : 3598
train acc:  0.796875
train loss:  0.45529916882514954
train gradient:  0.41000767694145734
iteration : 3599
train acc:  0.84375
train loss:  0.3175426125526428
train gradient:  0.23358821139350597
iteration : 3600
train acc:  0.8671875
train loss:  0.3067786395549774
train gradient:  0.26765574186786406
iteration : 3601
train acc:  0.8046875
train loss:  0.4459991455078125
train gradient:  0.5296735250466149
iteration : 3602
train acc:  0.8046875
train loss:  0.4028177261352539
train gradient:  0.3927990818416569
iteration : 3603
train acc:  0.8671875
train loss:  0.3009525537490845
train gradient:  0.2551875905364295
iteration : 3604
train acc:  0.84375
train loss:  0.39501696825027466
train gradient:  0.3208404766687645
iteration : 3605
train acc:  0.796875
train loss:  0.40880000591278076
train gradient:  0.38497415644450356
iteration : 3606
train acc:  0.8671875
train loss:  0.2926689088344574
train gradient:  0.3949222528038535
iteration : 3607
train acc:  0.828125
train loss:  0.38081371784210205
train gradient:  0.3060924055675808
iteration : 3608
train acc:  0.828125
train loss:  0.39713531732559204
train gradient:  0.29188866343730807
iteration : 3609
train acc:  0.7890625
train loss:  0.40694355964660645
train gradient:  0.42811499510522366
iteration : 3610
train acc:  0.8359375
train loss:  0.3837723433971405
train gradient:  0.4096844807420751
iteration : 3611
train acc:  0.796875
train loss:  0.3994746208190918
train gradient:  0.4483066481941729
iteration : 3612
train acc:  0.828125
train loss:  0.3924729824066162
train gradient:  0.34876256447340787
iteration : 3613
train acc:  0.9140625
train loss:  0.30804145336151123
train gradient:  0.18005959330047458
iteration : 3614
train acc:  0.8125
train loss:  0.3924657702445984
train gradient:  0.4561482095856035
iteration : 3615
train acc:  0.890625
train loss:  0.31126075983047485
train gradient:  0.2615330467671759
iteration : 3616
train acc:  0.84375
train loss:  0.3586881160736084
train gradient:  0.30180210039642524
iteration : 3617
train acc:  0.8359375
train loss:  0.3724381923675537
train gradient:  0.35985905856402967
iteration : 3618
train acc:  0.859375
train loss:  0.33524632453918457
train gradient:  0.24359673161487755
iteration : 3619
train acc:  0.890625
train loss:  0.3305673599243164
train gradient:  0.27272439771656243
iteration : 3620
train acc:  0.859375
train loss:  0.34872299432754517
train gradient:  0.39795172987464855
iteration : 3621
train acc:  0.8125
train loss:  0.37299489974975586
train gradient:  0.3337196132720661
iteration : 3622
train acc:  0.890625
train loss:  0.27963289618492126
train gradient:  0.21456902282074178
iteration : 3623
train acc:  0.8671875
train loss:  0.3274214267730713
train gradient:  0.22885085989029436
iteration : 3624
train acc:  0.8046875
train loss:  0.41118139028549194
train gradient:  0.44273980987179534
iteration : 3625
train acc:  0.8671875
train loss:  0.33186155557632446
train gradient:  0.25598958655787346
iteration : 3626
train acc:  0.7734375
train loss:  0.5000501871109009
train gradient:  0.7363119778908963
iteration : 3627
train acc:  0.765625
train loss:  0.4909696578979492
train gradient:  0.415922670642788
iteration : 3628
train acc:  0.875
train loss:  0.32673758268356323
train gradient:  0.2363542280229657
iteration : 3629
train acc:  0.8359375
train loss:  0.3480212092399597
train gradient:  0.25602714640715074
iteration : 3630
train acc:  0.7890625
train loss:  0.4436108469963074
train gradient:  0.3554859249888122
iteration : 3631
train acc:  0.8515625
train loss:  0.3292377293109894
train gradient:  0.313215528404701
iteration : 3632
train acc:  0.84375
train loss:  0.4194048047065735
train gradient:  0.3490366819223662
iteration : 3633
train acc:  0.8359375
train loss:  0.35923731327056885
train gradient:  0.2549175360884269
iteration : 3634
train acc:  0.84375
train loss:  0.3659173548221588
train gradient:  0.2083987015931903
iteration : 3635
train acc:  0.8828125
train loss:  0.3200288712978363
train gradient:  0.2322654431128373
iteration : 3636
train acc:  0.859375
train loss:  0.33728712797164917
train gradient:  0.2819703735345588
iteration : 3637
train acc:  0.8359375
train loss:  0.40264853835105896
train gradient:  0.29393991897002314
iteration : 3638
train acc:  0.78125
train loss:  0.39015477895736694
train gradient:  0.3106352031870014
iteration : 3639
train acc:  0.8671875
train loss:  0.39178332686424255
train gradient:  0.3234438793255066
iteration : 3640
train acc:  0.796875
train loss:  0.38934147357940674
train gradient:  0.40172680398867866
iteration : 3641
train acc:  0.8046875
train loss:  0.35882678627967834
train gradient:  0.3363727579831706
iteration : 3642
train acc:  0.875
train loss:  0.34525734186172485
train gradient:  0.300359975085067
iteration : 3643
train acc:  0.828125
train loss:  0.4121679663658142
train gradient:  0.35970856903589804
iteration : 3644
train acc:  0.8671875
train loss:  0.29663777351379395
train gradient:  0.17966621600720245
iteration : 3645
train acc:  0.8359375
train loss:  0.36528879404067993
train gradient:  0.40782765654185865
iteration : 3646
train acc:  0.8203125
train loss:  0.32162755727767944
train gradient:  0.1922455766166375
iteration : 3647
train acc:  0.8203125
train loss:  0.3412468433380127
train gradient:  0.3232269516462618
iteration : 3648
train acc:  0.8984375
train loss:  0.32306838035583496
train gradient:  0.3879792825719826
iteration : 3649
train acc:  0.7890625
train loss:  0.43062400817871094
train gradient:  0.38209670212130586
iteration : 3650
train acc:  0.8046875
train loss:  0.42210322618484497
train gradient:  0.351750798724658
iteration : 3651
train acc:  0.828125
train loss:  0.48862606287002563
train gradient:  0.5293611718507791
iteration : 3652
train acc:  0.859375
train loss:  0.35907965898513794
train gradient:  0.2666930781008439
iteration : 3653
train acc:  0.8984375
train loss:  0.27957552671432495
train gradient:  0.24038430429801216
iteration : 3654
train acc:  0.84375
train loss:  0.45186054706573486
train gradient:  0.379317937059836
iteration : 3655
train acc:  0.8828125
train loss:  0.31619274616241455
train gradient:  0.22179170314962726
iteration : 3656
train acc:  0.828125
train loss:  0.3920769989490509
train gradient:  0.3122436666282591
iteration : 3657
train acc:  0.84375
train loss:  0.3844025731086731
train gradient:  0.26837887294581664
iteration : 3658
train acc:  0.796875
train loss:  0.36722126603126526
train gradient:  0.32570760884655653
iteration : 3659
train acc:  0.8125
train loss:  0.4850906729698181
train gradient:  0.5217203787368341
iteration : 3660
train acc:  0.8671875
train loss:  0.282939612865448
train gradient:  0.2097244739138034
iteration : 3661
train acc:  0.8984375
train loss:  0.34352463483810425
train gradient:  0.2259201286792283
iteration : 3662
train acc:  0.8515625
train loss:  0.3713773787021637
train gradient:  0.3848310874008656
iteration : 3663
train acc:  0.8671875
train loss:  0.35067617893218994
train gradient:  0.32240402188938644
iteration : 3664
train acc:  0.8046875
train loss:  0.4223368763923645
train gradient:  0.3436507965712605
iteration : 3665
train acc:  0.8359375
train loss:  0.345209002494812
train gradient:  0.20614560181961203
iteration : 3666
train acc:  0.7734375
train loss:  0.4826175272464752
train gradient:  0.5190319338983431
iteration : 3667
train acc:  0.796875
train loss:  0.42540523409843445
train gradient:  0.42105193121641377
iteration : 3668
train acc:  0.78125
train loss:  0.41317039728164673
train gradient:  0.3167652375910618
iteration : 3669
train acc:  0.8125
train loss:  0.4012576937675476
train gradient:  0.28706318882880666
iteration : 3670
train acc:  0.828125
train loss:  0.38993632793426514
train gradient:  0.3106698638442959
iteration : 3671
train acc:  0.875
train loss:  0.3105792999267578
train gradient:  0.21381018464998908
iteration : 3672
train acc:  0.796875
train loss:  0.39430922269821167
train gradient:  0.32614623973751017
iteration : 3673
train acc:  0.84375
train loss:  0.32344403862953186
train gradient:  0.21461461804946563
iteration : 3674
train acc:  0.8515625
train loss:  0.35021501779556274
train gradient:  0.3094811431217314
iteration : 3675
train acc:  0.8203125
train loss:  0.3566955327987671
train gradient:  0.30042286708766264
iteration : 3676
train acc:  0.7890625
train loss:  0.48235392570495605
train gradient:  0.4095515155637133
iteration : 3677
train acc:  0.859375
train loss:  0.36956530809402466
train gradient:  0.2504205231298094
iteration : 3678
train acc:  0.828125
train loss:  0.461284875869751
train gradient:  0.3737347531618336
iteration : 3679
train acc:  0.7890625
train loss:  0.4716717600822449
train gradient:  0.43808309208432744
iteration : 3680
train acc:  0.8125
train loss:  0.4126202464103699
train gradient:  0.4055581232072896
iteration : 3681
train acc:  0.828125
train loss:  0.4606586992740631
train gradient:  0.49376218400422367
iteration : 3682
train acc:  0.875
train loss:  0.31875020265579224
train gradient:  0.2958828212391642
iteration : 3683
train acc:  0.8046875
train loss:  0.40961673855781555
train gradient:  0.31504217435025417
iteration : 3684
train acc:  0.7890625
train loss:  0.41826897859573364
train gradient:  0.41707010341614914
iteration : 3685
train acc:  0.8203125
train loss:  0.35082972049713135
train gradient:  0.23431461162067083
iteration : 3686
train acc:  0.8203125
train loss:  0.36687737703323364
train gradient:  0.33010353704736983
iteration : 3687
train acc:  0.8515625
train loss:  0.3489655554294586
train gradient:  0.19627466401560056
iteration : 3688
train acc:  0.8359375
train loss:  0.40763235092163086
train gradient:  0.31539867580709646
iteration : 3689
train acc:  0.84375
train loss:  0.38281601667404175
train gradient:  0.5175072021362435
iteration : 3690
train acc:  0.7734375
train loss:  0.4662938714027405
train gradient:  0.49821333718396166
iteration : 3691
train acc:  0.84375
train loss:  0.3393537402153015
train gradient:  0.25174346871666586
iteration : 3692
train acc:  0.8046875
train loss:  0.3860262632369995
train gradient:  0.3264105078890983
iteration : 3693
train acc:  0.7734375
train loss:  0.41205817461013794
train gradient:  0.32991308480771464
iteration : 3694
train acc:  0.8828125
train loss:  0.31558936834335327
train gradient:  0.17844507286999733
iteration : 3695
train acc:  0.8046875
train loss:  0.4268656075000763
train gradient:  0.5136425614379001
iteration : 3696
train acc:  0.8671875
train loss:  0.3344433307647705
train gradient:  0.24227502523143923
iteration : 3697
train acc:  0.828125
train loss:  0.38193196058273315
train gradient:  0.33781694604049034
iteration : 3698
train acc:  0.8359375
train loss:  0.36363309621810913
train gradient:  0.215865602824165
iteration : 3699
train acc:  0.8671875
train loss:  0.3206135034561157
train gradient:  0.1822263890943575
iteration : 3700
train acc:  0.78125
train loss:  0.4441111385822296
train gradient:  0.38900301890700867
iteration : 3701
train acc:  0.875
train loss:  0.318159818649292
train gradient:  0.22748338819942346
iteration : 3702
train acc:  0.84375
train loss:  0.39862188696861267
train gradient:  0.2455355432173955
iteration : 3703
train acc:  0.8828125
train loss:  0.3194957673549652
train gradient:  0.18089203699398765
iteration : 3704
train acc:  0.796875
train loss:  0.3949629068374634
train gradient:  0.2906453190862642
iteration : 3705
train acc:  0.859375
train loss:  0.34157609939575195
train gradient:  0.2475397086212399
iteration : 3706
train acc:  0.8046875
train loss:  0.42489781975746155
train gradient:  0.33771777853319207
iteration : 3707
train acc:  0.8828125
train loss:  0.296764075756073
train gradient:  0.24194508980216606
iteration : 3708
train acc:  0.875
train loss:  0.3544703722000122
train gradient:  0.2278290467910728
iteration : 3709
train acc:  0.890625
train loss:  0.3122979402542114
train gradient:  0.2154996575485327
iteration : 3710
train acc:  0.84375
train loss:  0.3296331763267517
train gradient:  0.2327477332238796
iteration : 3711
train acc:  0.8125
train loss:  0.4438517093658447
train gradient:  0.276430259037171
iteration : 3712
train acc:  0.8515625
train loss:  0.34273722767829895
train gradient:  0.23874056115754602
iteration : 3713
train acc:  0.8359375
train loss:  0.38893765211105347
train gradient:  0.25747069540555384
iteration : 3714
train acc:  0.7734375
train loss:  0.542447566986084
train gradient:  0.6647213104869937
iteration : 3715
train acc:  0.7734375
train loss:  0.4038192927837372
train gradient:  0.3175027215481003
iteration : 3716
train acc:  0.859375
train loss:  0.34967875480651855
train gradient:  0.297120042619068
iteration : 3717
train acc:  0.8203125
train loss:  0.35685354471206665
train gradient:  0.3004021338113049
iteration : 3718
train acc:  0.78125
train loss:  0.4294114112854004
train gradient:  0.4164886095099627
iteration : 3719
train acc:  0.8671875
train loss:  0.3183932900428772
train gradient:  0.39770498703721163
iteration : 3720
train acc:  0.8359375
train loss:  0.32961830496788025
train gradient:  0.1833782582718748
iteration : 3721
train acc:  0.828125
train loss:  0.38837385177612305
train gradient:  0.3284247203921862
iteration : 3722
train acc:  0.7890625
train loss:  0.3780587613582611
train gradient:  0.27889826317115934
iteration : 3723
train acc:  0.859375
train loss:  0.3089601993560791
train gradient:  0.2279646353150277
iteration : 3724
train acc:  0.8984375
train loss:  0.2894696593284607
train gradient:  0.15888067064859382
iteration : 3725
train acc:  0.84375
train loss:  0.3912261724472046
train gradient:  0.32160934454028633
iteration : 3726
train acc:  0.8046875
train loss:  0.3901967704296112
train gradient:  0.34453095096422726
iteration : 3727
train acc:  0.8125
train loss:  0.41910630464553833
train gradient:  0.2810039364378356
iteration : 3728
train acc:  0.8671875
train loss:  0.31042036414146423
train gradient:  0.2100742979150854
iteration : 3729
train acc:  0.7890625
train loss:  0.43595167994499207
train gradient:  0.33151996919738863
iteration : 3730
train acc:  0.8515625
train loss:  0.351507306098938
train gradient:  0.2008875686620702
iteration : 3731
train acc:  0.84375
train loss:  0.34839463233947754
train gradient:  0.25024285640755695
iteration : 3732
train acc:  0.75
train loss:  0.5318881273269653
train gradient:  0.46550221468652353
iteration : 3733
train acc:  0.84375
train loss:  0.3880837559700012
train gradient:  0.31016913103200167
iteration : 3734
train acc:  0.7890625
train loss:  0.44079506397247314
train gradient:  0.4234019635900171
iteration : 3735
train acc:  0.8515625
train loss:  0.3158244490623474
train gradient:  0.20887589935680126
iteration : 3736
train acc:  0.8515625
train loss:  0.33109939098358154
train gradient:  0.2598075838867666
iteration : 3737
train acc:  0.8671875
train loss:  0.3337123990058899
train gradient:  0.22695380875572507
iteration : 3738
train acc:  0.8828125
train loss:  0.3178272247314453
train gradient:  0.22063434529788434
iteration : 3739
train acc:  0.78125
train loss:  0.40915971994400024
train gradient:  0.3301738377755561
iteration : 3740
train acc:  0.84375
train loss:  0.3439745008945465
train gradient:  0.2766134892907515
iteration : 3741
train acc:  0.84375
train loss:  0.40543264150619507
train gradient:  0.35077134715558794
iteration : 3742
train acc:  0.8125
train loss:  0.42715775966644287
train gradient:  0.3285529566022026
iteration : 3743
train acc:  0.828125
train loss:  0.35841572284698486
train gradient:  0.2604619953915469
iteration : 3744
train acc:  0.8828125
train loss:  0.29633015394210815
train gradient:  0.1825541023310716
iteration : 3745
train acc:  0.765625
train loss:  0.41407737135887146
train gradient:  0.3774564888374472
iteration : 3746
train acc:  0.8125
train loss:  0.39600253105163574
train gradient:  0.3134190463457386
iteration : 3747
train acc:  0.8671875
train loss:  0.36061590909957886
train gradient:  0.23919407515669044
iteration : 3748
train acc:  0.8359375
train loss:  0.3637811541557312
train gradient:  0.24325740468054574
iteration : 3749
train acc:  0.8046875
train loss:  0.38081544637680054
train gradient:  0.2922561373344409
iteration : 3750
train acc:  0.8203125
train loss:  0.3909078538417816
train gradient:  0.33294830971393125
iteration : 3751
train acc:  0.7890625
train loss:  0.44905123114585876
train gradient:  0.5784808965979593
iteration : 3752
train acc:  0.859375
train loss:  0.30493199825286865
train gradient:  0.20245107996781053
iteration : 3753
train acc:  0.859375
train loss:  0.36773914098739624
train gradient:  0.26382992440463815
iteration : 3754
train acc:  0.8046875
train loss:  0.3962165117263794
train gradient:  0.3650521607738484
iteration : 3755
train acc:  0.84375
train loss:  0.3136492371559143
train gradient:  0.20892635160986078
iteration : 3756
train acc:  0.8671875
train loss:  0.2916070222854614
train gradient:  0.21633719836547358
iteration : 3757
train acc:  0.8125
train loss:  0.37544822692871094
train gradient:  0.25092605225745096
iteration : 3758
train acc:  0.8359375
train loss:  0.36580294370651245
train gradient:  0.5962130995819986
iteration : 3759
train acc:  0.8203125
train loss:  0.37506303191185
train gradient:  0.29468046243752183
iteration : 3760
train acc:  0.8515625
train loss:  0.3830965757369995
train gradient:  0.3170125708891089
iteration : 3761
train acc:  0.8828125
train loss:  0.35279738903045654
train gradient:  0.21664931188116326
iteration : 3762
train acc:  0.84375
train loss:  0.31369486451148987
train gradient:  0.1354568126315545
iteration : 3763
train acc:  0.828125
train loss:  0.3514622449874878
train gradient:  0.2965127443565596
iteration : 3764
train acc:  0.8671875
train loss:  0.3138663172721863
train gradient:  0.23454715906783746
iteration : 3765
train acc:  0.7890625
train loss:  0.39937442541122437
train gradient:  0.307383035854882
iteration : 3766
train acc:  0.8203125
train loss:  0.3481709659099579
train gradient:  0.2422278603758408
iteration : 3767
train acc:  0.7578125
train loss:  0.47108548879623413
train gradient:  0.3987540390748079
iteration : 3768
train acc:  0.8203125
train loss:  0.42998993396759033
train gradient:  0.2794831202724479
iteration : 3769
train acc:  0.890625
train loss:  0.3143540620803833
train gradient:  0.22491382352654857
iteration : 3770
train acc:  0.7890625
train loss:  0.42407578229904175
train gradient:  0.3495702081361688
iteration : 3771
train acc:  0.84375
train loss:  0.38620591163635254
train gradient:  0.36020712341088523
iteration : 3772
train acc:  0.8203125
train loss:  0.3497415781021118
train gradient:  0.3900242934369017
iteration : 3773
train acc:  0.78125
train loss:  0.46689945459365845
train gradient:  0.3304816037518437
iteration : 3774
train acc:  0.890625
train loss:  0.28791165351867676
train gradient:  0.16078620652268766
iteration : 3775
train acc:  0.8046875
train loss:  0.41370874643325806
train gradient:  0.2922212921695656
iteration : 3776
train acc:  0.8046875
train loss:  0.41199344396591187
train gradient:  0.42564488336605
iteration : 3777
train acc:  0.8203125
train loss:  0.3765385150909424
train gradient:  0.3128707273485077
iteration : 3778
train acc:  0.8359375
train loss:  0.3350760042667389
train gradient:  0.2641418260237068
iteration : 3779
train acc:  0.828125
train loss:  0.38104063272476196
train gradient:  0.2967671968026491
iteration : 3780
train acc:  0.875
train loss:  0.33032816648483276
train gradient:  0.15338487195458658
iteration : 3781
train acc:  0.7421875
train loss:  0.45350170135498047
train gradient:  0.3501300822285989
iteration : 3782
train acc:  0.78125
train loss:  0.48488277196884155
train gradient:  0.4390481567868426
iteration : 3783
train acc:  0.828125
train loss:  0.3913011848926544
train gradient:  0.36300826742392445
iteration : 3784
train acc:  0.8515625
train loss:  0.38102900981903076
train gradient:  0.2914218838254657
iteration : 3785
train acc:  0.78125
train loss:  0.4021969139575958
train gradient:  0.38912930381253324
iteration : 3786
train acc:  0.796875
train loss:  0.43544596433639526
train gradient:  0.343478300059995
iteration : 3787
train acc:  0.8203125
train loss:  0.3763270974159241
train gradient:  0.2915296930351927
iteration : 3788
train acc:  0.8984375
train loss:  0.3113008737564087
train gradient:  0.20616377755327173
iteration : 3789
train acc:  0.890625
train loss:  0.2907542586326599
train gradient:  0.2176861585206109
iteration : 3790
train acc:  0.8125
train loss:  0.40319621562957764
train gradient:  0.2831356449670142
iteration : 3791
train acc:  0.875
train loss:  0.3285244107246399
train gradient:  0.20905628077501584
iteration : 3792
train acc:  0.890625
train loss:  0.2873115837574005
train gradient:  0.21508895077417087
iteration : 3793
train acc:  0.8125
train loss:  0.35421398282051086
train gradient:  0.2611551943083737
iteration : 3794
train acc:  0.859375
train loss:  0.3526412844657898
train gradient:  0.2816338452905085
iteration : 3795
train acc:  0.8828125
train loss:  0.3329710364341736
train gradient:  0.3746405193396211
iteration : 3796
train acc:  0.796875
train loss:  0.4135986268520355
train gradient:  0.41651390497327
iteration : 3797
train acc:  0.7890625
train loss:  0.40389788150787354
train gradient:  0.28648863281980075
iteration : 3798
train acc:  0.8515625
train loss:  0.3418966233730316
train gradient:  0.23932113395077953
iteration : 3799
train acc:  0.828125
train loss:  0.38465994596481323
train gradient:  0.32568409627935124
iteration : 3800
train acc:  0.8203125
train loss:  0.38678908348083496
train gradient:  0.3174564143108483
iteration : 3801
train acc:  0.7734375
train loss:  0.4069189131259918
train gradient:  0.3740661603363979
iteration : 3802
train acc:  0.78125
train loss:  0.39160168170928955
train gradient:  0.3438892914559952
iteration : 3803
train acc:  0.859375
train loss:  0.35508739948272705
train gradient:  0.4265192981066789
iteration : 3804
train acc:  0.8671875
train loss:  0.31612834334373474
train gradient:  0.1908487108238281
iteration : 3805
train acc:  0.84375
train loss:  0.33810490369796753
train gradient:  0.2651988753264061
iteration : 3806
train acc:  0.8359375
train loss:  0.4021434187889099
train gradient:  0.3235124844623845
iteration : 3807
train acc:  0.8515625
train loss:  0.3397403955459595
train gradient:  0.2344904800040391
iteration : 3808
train acc:  0.78125
train loss:  0.39029228687286377
train gradient:  0.27113539637596146
iteration : 3809
train acc:  0.875
train loss:  0.2998059391975403
train gradient:  0.24484148910245418
iteration : 3810
train acc:  0.796875
train loss:  0.44549280405044556
train gradient:  0.45794499367158736
iteration : 3811
train acc:  0.8671875
train loss:  0.28148937225341797
train gradient:  0.25498993667621045
iteration : 3812
train acc:  0.90625
train loss:  0.2945072054862976
train gradient:  0.2175023318757146
iteration : 3813
train acc:  0.8984375
train loss:  0.2786421775817871
train gradient:  0.20498038213416475
iteration : 3814
train acc:  0.875
train loss:  0.3042663633823395
train gradient:  0.27289027562080115
iteration : 3815
train acc:  0.84375
train loss:  0.36010032892227173
train gradient:  0.26939828729953
iteration : 3816
train acc:  0.90625
train loss:  0.2764592170715332
train gradient:  0.16836808171952222
iteration : 3817
train acc:  0.875
train loss:  0.31966716051101685
train gradient:  0.26290603626846526
iteration : 3818
train acc:  0.8671875
train loss:  0.39040976762771606
train gradient:  0.33620347779853355
iteration : 3819
train acc:  0.890625
train loss:  0.27238598465919495
train gradient:  0.13850893857347343
iteration : 3820
train acc:  0.8125
train loss:  0.38304054737091064
train gradient:  0.27023020395945274
iteration : 3821
train acc:  0.890625
train loss:  0.31928807497024536
train gradient:  0.26230160467431946
iteration : 3822
train acc:  0.859375
train loss:  0.30420562624931335
train gradient:  0.21368382922502493
iteration : 3823
train acc:  0.84375
train loss:  0.31728851795196533
train gradient:  0.22836734338346637
iteration : 3824
train acc:  0.8125
train loss:  0.3566858172416687
train gradient:  0.3659364646212257
iteration : 3825
train acc:  0.9140625
train loss:  0.29317787289619446
train gradient:  0.15538678843460596
iteration : 3826
train acc:  0.7734375
train loss:  0.428490549325943
train gradient:  0.5366025124227365
iteration : 3827
train acc:  0.7734375
train loss:  0.3959265351295471
train gradient:  0.4621333898986196
iteration : 3828
train acc:  0.890625
train loss:  0.28805363178253174
train gradient:  0.7354434105743582
iteration : 3829
train acc:  0.8515625
train loss:  0.3741700351238251
train gradient:  0.390341158930081
iteration : 3830
train acc:  0.8125
train loss:  0.391623318195343
train gradient:  0.27667976982618014
iteration : 3831
train acc:  0.9140625
train loss:  0.2723528742790222
train gradient:  0.21863343171103006
iteration : 3832
train acc:  0.84375
train loss:  0.33890974521636963
train gradient:  0.31259152544558155
iteration : 3833
train acc:  0.84375
train loss:  0.35269027948379517
train gradient:  0.30157443738083994
iteration : 3834
train acc:  0.859375
train loss:  0.3977087736129761
train gradient:  0.401335731892968
iteration : 3835
train acc:  0.8203125
train loss:  0.4117630422115326
train gradient:  0.5045827735649933
iteration : 3836
train acc:  0.90625
train loss:  0.30040299892425537
train gradient:  0.31033736931705774
iteration : 3837
train acc:  0.84375
train loss:  0.3533225655555725
train gradient:  0.4049943021185868
iteration : 3838
train acc:  0.8359375
train loss:  0.35002338886260986
train gradient:  0.2609839775240968
iteration : 3839
train acc:  0.859375
train loss:  0.31475210189819336
train gradient:  0.2162291610842448
iteration : 3840
train acc:  0.8515625
train loss:  0.30605968832969666
train gradient:  0.26804128744352274
iteration : 3841
train acc:  0.8515625
train loss:  0.3350968062877655
train gradient:  0.2658487315580769
iteration : 3842
train acc:  0.828125
train loss:  0.3866581916809082
train gradient:  0.4257172977817496
iteration : 3843
train acc:  0.8671875
train loss:  0.3764915466308594
train gradient:  0.3326614030985109
iteration : 3844
train acc:  0.8515625
train loss:  0.3700111210346222
train gradient:  0.37320627714308535
iteration : 3845
train acc:  0.8203125
train loss:  0.3687366247177124
train gradient:  0.33159641504918175
iteration : 3846
train acc:  0.8203125
train loss:  0.4387437701225281
train gradient:  0.3550019973697796
iteration : 3847
train acc:  0.796875
train loss:  0.425021231174469
train gradient:  0.38882063631859803
iteration : 3848
train acc:  0.828125
train loss:  0.36329972743988037
train gradient:  0.24652154133377685
iteration : 3849
train acc:  0.8828125
train loss:  0.3270481526851654
train gradient:  0.3394499667445879
iteration : 3850
train acc:  0.9140625
train loss:  0.25532928109169006
train gradient:  0.14453865967355697
iteration : 3851
train acc:  0.8671875
train loss:  0.3040773868560791
train gradient:  0.22902079321609164
iteration : 3852
train acc:  0.84375
train loss:  0.37232255935668945
train gradient:  0.32704427770379624
iteration : 3853
train acc:  0.765625
train loss:  0.4262049198150635
train gradient:  0.35682798069503285
iteration : 3854
train acc:  0.78125
train loss:  0.3939252495765686
train gradient:  0.5440778384008461
iteration : 3855
train acc:  0.8828125
train loss:  0.2943626046180725
train gradient:  0.5960073827521588
iteration : 3856
train acc:  0.859375
train loss:  0.34170567989349365
train gradient:  0.28607306198209964
iteration : 3857
train acc:  0.8984375
train loss:  0.2654683589935303
train gradient:  0.13898777198976597
iteration : 3858
train acc:  0.8046875
train loss:  0.3907056748867035
train gradient:  0.3305879240776618
iteration : 3859
train acc:  0.8984375
train loss:  0.28194335103034973
train gradient:  0.1580494115126834
iteration : 3860
train acc:  0.8046875
train loss:  0.4008798599243164
train gradient:  0.42185826017308237
iteration : 3861
train acc:  0.828125
train loss:  0.31957924365997314
train gradient:  0.30082746230047797
iteration : 3862
train acc:  0.8125
train loss:  0.42122071981430054
train gradient:  0.38456555746705035
iteration : 3863
train acc:  0.8203125
train loss:  0.3820705711841583
train gradient:  0.3567266266269478
iteration : 3864
train acc:  0.8359375
train loss:  0.35324257612228394
train gradient:  0.2394456780835219
iteration : 3865
train acc:  0.8359375
train loss:  0.35911256074905396
train gradient:  0.2864014158836625
iteration : 3866
train acc:  0.8828125
train loss:  0.32429367303848267
train gradient:  0.1889164598520529
iteration : 3867
train acc:  0.8359375
train loss:  0.3371288478374481
train gradient:  0.2051543917824805
iteration : 3868
train acc:  0.7734375
train loss:  0.5043988227844238
train gradient:  0.3929760336539266
iteration : 3869
train acc:  0.84375
train loss:  0.30751973390579224
train gradient:  0.3284013698471586
iteration : 3870
train acc:  0.828125
train loss:  0.375404953956604
train gradient:  0.24247052315209827
iteration : 3871
train acc:  0.8359375
train loss:  0.3453625440597534
train gradient:  0.3382468086208586
iteration : 3872
train acc:  0.84375
train loss:  0.3727145195007324
train gradient:  0.2681119117974057
iteration : 3873
train acc:  0.8359375
train loss:  0.41564929485321045
train gradient:  0.38842381849091123
iteration : 3874
train acc:  0.84375
train loss:  0.40208861231803894
train gradient:  0.5247251839137826
iteration : 3875
train acc:  0.8046875
train loss:  0.47982603311538696
train gradient:  0.6374545002004074
iteration : 3876
train acc:  0.8125
train loss:  0.41243213415145874
train gradient:  0.40268515540643396
iteration : 3877
train acc:  0.8671875
train loss:  0.2839986979961395
train gradient:  0.20304461503318588
iteration : 3878
train acc:  0.8359375
train loss:  0.35805097222328186
train gradient:  0.3617134795791389
iteration : 3879
train acc:  0.7734375
train loss:  0.4546300768852234
train gradient:  0.44840394992033483
iteration : 3880
train acc:  0.8984375
train loss:  0.3039681911468506
train gradient:  0.21296098901424246
iteration : 3881
train acc:  0.84375
train loss:  0.30194124579429626
train gradient:  0.2493745811997557
iteration : 3882
train acc:  0.828125
train loss:  0.37610530853271484
train gradient:  0.36964912017296253
iteration : 3883
train acc:  0.8359375
train loss:  0.3909684717655182
train gradient:  0.40513454388806946
iteration : 3884
train acc:  0.8203125
train loss:  0.4132552742958069
train gradient:  0.32389179529774187
iteration : 3885
train acc:  0.8203125
train loss:  0.3664133548736572
train gradient:  0.2574454192332785
iteration : 3886
train acc:  0.828125
train loss:  0.36611467599868774
train gradient:  0.382440120011805
iteration : 3887
train acc:  0.8203125
train loss:  0.4489569067955017
train gradient:  0.4793811913579728
iteration : 3888
train acc:  0.8515625
train loss:  0.35057759284973145
train gradient:  0.24810661055086572
iteration : 3889
train acc:  0.890625
train loss:  0.28117096424102783
train gradient:  0.24005626313882297
iteration : 3890
train acc:  0.828125
train loss:  0.37856167554855347
train gradient:  0.22603007228073846
iteration : 3891
train acc:  0.90625
train loss:  0.2535269856452942
train gradient:  0.16389682385992227
iteration : 3892
train acc:  0.84375
train loss:  0.36173373460769653
train gradient:  0.39674060692324375
iteration : 3893
train acc:  0.84375
train loss:  0.3783854842185974
train gradient:  0.2846633484318699
iteration : 3894
train acc:  0.765625
train loss:  0.45915472507476807
train gradient:  0.33673385445978954
iteration : 3895
train acc:  0.8125
train loss:  0.4200216829776764
train gradient:  0.3088444186288744
iteration : 3896
train acc:  0.796875
train loss:  0.43217411637306213
train gradient:  0.4407940741110995
iteration : 3897
train acc:  0.859375
train loss:  0.3174220323562622
train gradient:  0.2810983421355872
iteration : 3898
train acc:  0.8125
train loss:  0.41784653067588806
train gradient:  0.3371765551968802
iteration : 3899
train acc:  0.8515625
train loss:  0.31159406900405884
train gradient:  0.30161554452080785
iteration : 3900
train acc:  0.8671875
train loss:  0.28981566429138184
train gradient:  0.1947577456787073
iteration : 3901
train acc:  0.890625
train loss:  0.29273366928100586
train gradient:  0.27956329853207434
iteration : 3902
train acc:  0.8671875
train loss:  0.32315242290496826
train gradient:  0.2937901687073879
iteration : 3903
train acc:  0.890625
train loss:  0.30720072984695435
train gradient:  0.283589727877079
iteration : 3904
train acc:  0.7890625
train loss:  0.44069337844848633
train gradient:  0.383949398679939
iteration : 3905
train acc:  0.8671875
train loss:  0.3210182785987854
train gradient:  0.23397733912135948
iteration : 3906
train acc:  0.890625
train loss:  0.3106367588043213
train gradient:  0.16979076397977858
iteration : 3907
train acc:  0.8515625
train loss:  0.31083598732948303
train gradient:  0.1980153861590142
iteration : 3908
train acc:  0.8046875
train loss:  0.39806365966796875
train gradient:  0.4149726721014089
iteration : 3909
train acc:  0.8828125
train loss:  0.3121355175971985
train gradient:  0.19710147376379722
iteration : 3910
train acc:  0.859375
train loss:  0.3378497362136841
train gradient:  0.3134723467954284
iteration : 3911
train acc:  0.84375
train loss:  0.393638551235199
train gradient:  0.31170003835429344
iteration : 3912
train acc:  0.8984375
train loss:  0.30749672651290894
train gradient:  0.42765919523722856
iteration : 3913
train acc:  0.8984375
train loss:  0.2910583019256592
train gradient:  0.39313755767721154
iteration : 3914
train acc:  0.8203125
train loss:  0.4233030080795288
train gradient:  0.33526654794229377
iteration : 3915
train acc:  0.7890625
train loss:  0.4610438942909241
train gradient:  0.5559986273554445
iteration : 3916
train acc:  0.8515625
train loss:  0.369809627532959
train gradient:  0.35083862651612224
iteration : 3917
train acc:  0.84375
train loss:  0.35368338227272034
train gradient:  0.2379157308319999
iteration : 3918
train acc:  0.8515625
train loss:  0.37698838114738464
train gradient:  0.27454320313396524
iteration : 3919
train acc:  0.8125
train loss:  0.38753679394721985
train gradient:  0.32267256462936006
iteration : 3920
train acc:  0.7890625
train loss:  0.4491327702999115
train gradient:  0.584386846565463
iteration : 3921
train acc:  0.8359375
train loss:  0.41145801544189453
train gradient:  0.5380657754096256
iteration : 3922
train acc:  0.8515625
train loss:  0.3382871150970459
train gradient:  0.22592324973937106
iteration : 3923
train acc:  0.84375
train loss:  0.36546942591667175
train gradient:  0.34516615385777016
iteration : 3924
train acc:  0.796875
train loss:  0.5015371441841125
train gradient:  0.7322182602172426
iteration : 3925
train acc:  0.8359375
train loss:  0.3841066360473633
train gradient:  0.3110621501554283
iteration : 3926
train acc:  0.7890625
train loss:  0.39972519874572754
train gradient:  0.41914225411706174
iteration : 3927
train acc:  0.8359375
train loss:  0.3492046594619751
train gradient:  0.2421029523841846
iteration : 3928
train acc:  0.8828125
train loss:  0.2665761113166809
train gradient:  0.1293052826106031
iteration : 3929
train acc:  0.7890625
train loss:  0.38849714398384094
train gradient:  0.3689146214946473
iteration : 3930
train acc:  0.7734375
train loss:  0.4640243351459503
train gradient:  0.48706057527638585
iteration : 3931
train acc:  0.8671875
train loss:  0.31078922748565674
train gradient:  0.17929220852219058
iteration : 3932
train acc:  0.8125
train loss:  0.3935273289680481
train gradient:  0.35755700424221387
iteration : 3933
train acc:  0.78125
train loss:  0.4627711772918701
train gradient:  0.4101331772579336
iteration : 3934
train acc:  0.8515625
train loss:  0.34728410840034485
train gradient:  0.33331604071543797
iteration : 3935
train acc:  0.8515625
train loss:  0.3875070810317993
train gradient:  0.3335659678866137
iteration : 3936
train acc:  0.8828125
train loss:  0.3211515545845032
train gradient:  0.19719944918086413
iteration : 3937
train acc:  0.7734375
train loss:  0.4602509140968323
train gradient:  0.35539659360382675
iteration : 3938
train acc:  0.8203125
train loss:  0.3688949942588806
train gradient:  0.2116550944440505
iteration : 3939
train acc:  0.8046875
train loss:  0.40796759724617004
train gradient:  0.33394990363578975
iteration : 3940
train acc:  0.828125
train loss:  0.3578163981437683
train gradient:  0.2812672374533436
iteration : 3941
train acc:  0.859375
train loss:  0.33821362257003784
train gradient:  0.309847298356609
iteration : 3942
train acc:  0.8359375
train loss:  0.4121043086051941
train gradient:  0.30220715038685686
iteration : 3943
train acc:  0.859375
train loss:  0.35178273916244507
train gradient:  0.4460924249109936
iteration : 3944
train acc:  0.875
train loss:  0.3032754063606262
train gradient:  0.1760799157275305
iteration : 3945
train acc:  0.8515625
train loss:  0.33743804693222046
train gradient:  0.2180623380774388
iteration : 3946
train acc:  0.875
train loss:  0.3438330590724945
train gradient:  0.2439029437480309
iteration : 3947
train acc:  0.84375
train loss:  0.39096730947494507
train gradient:  0.2871303061459679
iteration : 3948
train acc:  0.8515625
train loss:  0.3496759533882141
train gradient:  0.2403361273963902
iteration : 3949
train acc:  0.8359375
train loss:  0.3728784918785095
train gradient:  0.22134311942225232
iteration : 3950
train acc:  0.8828125
train loss:  0.27839523553848267
train gradient:  0.18626268184488884
iteration : 3951
train acc:  0.828125
train loss:  0.4172266125679016
train gradient:  0.32445246111878606
iteration : 3952
train acc:  0.8671875
train loss:  0.30860376358032227
train gradient:  0.20336684254537216
iteration : 3953
train acc:  0.859375
train loss:  0.32164692878723145
train gradient:  0.2055926745783013
iteration : 3954
train acc:  0.875
train loss:  0.35258370637893677
train gradient:  0.2596009347034887
iteration : 3955
train acc:  0.828125
train loss:  0.4163961410522461
train gradient:  0.2871526698219695
iteration : 3956
train acc:  0.8125
train loss:  0.35103654861450195
train gradient:  0.2546189722792105
iteration : 3957
train acc:  0.8515625
train loss:  0.35237184166908264
train gradient:  0.27264006045076694
iteration : 3958
train acc:  0.8125
train loss:  0.4382369816303253
train gradient:  0.32008510582137817
iteration : 3959
train acc:  0.8984375
train loss:  0.33768153190612793
train gradient:  0.3384207783514014
iteration : 3960
train acc:  0.8359375
train loss:  0.3669026494026184
train gradient:  0.2145151033216935
iteration : 3961
train acc:  0.8203125
train loss:  0.4012366533279419
train gradient:  0.27605732635912805
iteration : 3962
train acc:  0.7890625
train loss:  0.4720532298088074
train gradient:  0.4218590987667212
iteration : 3963
train acc:  0.84375
train loss:  0.3743477463722229
train gradient:  0.24157019821145645
iteration : 3964
train acc:  0.8671875
train loss:  0.38759005069732666
train gradient:  0.19656130891061704
iteration : 3965
train acc:  0.8203125
train loss:  0.3990495502948761
train gradient:  0.3610015604566522
iteration : 3966
train acc:  0.84375
train loss:  0.3781672716140747
train gradient:  0.2672034044232676
iteration : 3967
train acc:  0.8515625
train loss:  0.31589680910110474
train gradient:  0.18027267950157996
iteration : 3968
train acc:  0.875
train loss:  0.3496766686439514
train gradient:  0.27871441593575313
iteration : 3969
train acc:  0.8671875
train loss:  0.3254115581512451
train gradient:  0.1876164501820285
iteration : 3970
train acc:  0.8203125
train loss:  0.34447333216667175
train gradient:  0.3417303041048248
iteration : 3971
train acc:  0.8125
train loss:  0.436257541179657
train gradient:  0.31085713396263565
iteration : 3972
train acc:  0.8359375
train loss:  0.37786996364593506
train gradient:  0.32868306377547696
iteration : 3973
train acc:  0.859375
train loss:  0.32822996377944946
train gradient:  0.1782268292818003
iteration : 3974
train acc:  0.828125
train loss:  0.3878624737262726
train gradient:  0.3099943911592636
iteration : 3975
train acc:  0.8828125
train loss:  0.3148602247238159
train gradient:  0.19079754503003138
iteration : 3976
train acc:  0.8046875
train loss:  0.327269971370697
train gradient:  0.2653097153232388
iteration : 3977
train acc:  0.84375
train loss:  0.34860244393348694
train gradient:  0.2683826985652944
iteration : 3978
train acc:  0.8828125
train loss:  0.27370357513427734
train gradient:  0.14271823242355441
iteration : 3979
train acc:  0.8515625
train loss:  0.3639967739582062
train gradient:  0.23908338335035856
iteration : 3980
train acc:  0.8359375
train loss:  0.3889724016189575
train gradient:  0.23451057343855247
iteration : 3981
train acc:  0.828125
train loss:  0.35553768277168274
train gradient:  0.257732314826884
iteration : 3982
train acc:  0.859375
train loss:  0.38474947214126587
train gradient:  0.22201631338404376
iteration : 3983
train acc:  0.8125
train loss:  0.36699819564819336
train gradient:  0.16291482652528647
iteration : 3984
train acc:  0.828125
train loss:  0.3580402731895447
train gradient:  0.24319033082147595
iteration : 3985
train acc:  0.875
train loss:  0.2896832823753357
train gradient:  0.21116546064130687
iteration : 3986
train acc:  0.78125
train loss:  0.46206051111221313
train gradient:  0.3525739074208096
iteration : 3987
train acc:  0.8125
train loss:  0.42716899514198303
train gradient:  0.3361128717216003
iteration : 3988
train acc:  0.8046875
train loss:  0.4156962037086487
train gradient:  0.31356378318495054
iteration : 3989
train acc:  0.859375
train loss:  0.32398247718811035
train gradient:  0.24687482846669273
iteration : 3990
train acc:  0.859375
train loss:  0.29235997796058655
train gradient:  0.17837491857079102
iteration : 3991
train acc:  0.8359375
train loss:  0.35541674494743347
train gradient:  0.3072453317843226
iteration : 3992
train acc:  0.828125
train loss:  0.3774455785751343
train gradient:  0.33965785795747555
iteration : 3993
train acc:  0.8203125
train loss:  0.3441435694694519
train gradient:  0.17630013045422824
iteration : 3994
train acc:  0.875
train loss:  0.32509732246398926
train gradient:  0.22987506556344126
iteration : 3995
train acc:  0.859375
train loss:  0.38048654794692993
train gradient:  0.3732644398565257
iteration : 3996
train acc:  0.84375
train loss:  0.46496614813804626
train gradient:  0.48873593662362197
iteration : 3997
train acc:  0.8359375
train loss:  0.36591196060180664
train gradient:  0.33308074191436715
iteration : 3998
train acc:  0.828125
train loss:  0.3564431071281433
train gradient:  0.24211244290729192
iteration : 3999
train acc:  0.828125
train loss:  0.38548049330711365
train gradient:  0.23473335403133588
iteration : 4000
train acc:  0.84375
train loss:  0.3400788903236389
train gradient:  0.1867817257409965
iteration : 4001
train acc:  0.84375
train loss:  0.37003806233406067
train gradient:  0.29144477634625016
iteration : 4002
train acc:  0.828125
train loss:  0.3644814193248749
train gradient:  0.2632113976973502
iteration : 4003
train acc:  0.8359375
train loss:  0.3638756275177002
train gradient:  0.3534108274969097
iteration : 4004
train acc:  0.859375
train loss:  0.3138440251350403
train gradient:  0.17392814157396688
iteration : 4005
train acc:  0.828125
train loss:  0.336584210395813
train gradient:  0.24166220190829185
iteration : 4006
train acc:  0.8359375
train loss:  0.37121379375457764
train gradient:  0.410759786788968
iteration : 4007
train acc:  0.8203125
train loss:  0.35768938064575195
train gradient:  0.3195397876261434
iteration : 4008
train acc:  0.796875
train loss:  0.40756291151046753
train gradient:  0.31776793299249617
iteration : 4009
train acc:  0.859375
train loss:  0.34463533759117126
train gradient:  0.28311399919482755
iteration : 4010
train acc:  0.828125
train loss:  0.4822748303413391
train gradient:  0.34011639213703804
iteration : 4011
train acc:  0.90625
train loss:  0.30307817459106445
train gradient:  0.5662165990760049
iteration : 4012
train acc:  0.78125
train loss:  0.42497777938842773
train gradient:  0.3210660864097511
iteration : 4013
train acc:  0.8671875
train loss:  0.3073977828025818
train gradient:  0.14907826093990476
iteration : 4014
train acc:  0.9140625
train loss:  0.23512285947799683
train gradient:  0.15296607908914323
iteration : 4015
train acc:  0.8359375
train loss:  0.34785905480384827
train gradient:  0.25018871779499663
iteration : 4016
train acc:  0.859375
train loss:  0.3259321451187134
train gradient:  0.26769850090702335
iteration : 4017
train acc:  0.8203125
train loss:  0.44010162353515625
train gradient:  0.49579696873759693
iteration : 4018
train acc:  0.8671875
train loss:  0.31354737281799316
train gradient:  0.18145126326815803
iteration : 4019
train acc:  0.859375
train loss:  0.3243933320045471
train gradient:  0.25543363694659615
iteration : 4020
train acc:  0.8125
train loss:  0.36464402079582214
train gradient:  0.22445709775571393
iteration : 4021
train acc:  0.859375
train loss:  0.3890748620033264
train gradient:  0.3311944585043619
iteration : 4022
train acc:  0.828125
train loss:  0.3761233389377594
train gradient:  0.27050157599608804
iteration : 4023
train acc:  0.8125
train loss:  0.36930274963378906
train gradient:  0.3265225040281904
iteration : 4024
train acc:  0.859375
train loss:  0.35723090171813965
train gradient:  0.43768715599867264
iteration : 4025
train acc:  0.8515625
train loss:  0.36754554510116577
train gradient:  0.31150459185955076
iteration : 4026
train acc:  0.84375
train loss:  0.3734636902809143
train gradient:  0.27689242524133917
iteration : 4027
train acc:  0.84375
train loss:  0.3496013879776001
train gradient:  0.2994764057162581
iteration : 4028
train acc:  0.875
train loss:  0.3241739273071289
train gradient:  0.22634093790142715
iteration : 4029
train acc:  0.84375
train loss:  0.3736003041267395
train gradient:  0.3099991926433884
iteration : 4030
train acc:  0.8125
train loss:  0.38041484355926514
train gradient:  0.22982265267896074
iteration : 4031
train acc:  0.859375
train loss:  0.3449987471103668
train gradient:  0.358981215542602
iteration : 4032
train acc:  0.7890625
train loss:  0.40179067850112915
train gradient:  0.36878710703089757
iteration : 4033
train acc:  0.8515625
train loss:  0.3417435586452484
train gradient:  0.23886178068654118
iteration : 4034
train acc:  0.90625
train loss:  0.255746066570282
train gradient:  0.35634529531343534
iteration : 4035
train acc:  0.859375
train loss:  0.3566761612892151
train gradient:  0.26210869955769345
iteration : 4036
train acc:  0.8359375
train loss:  0.394998162984848
train gradient:  0.29310925913346414
iteration : 4037
train acc:  0.859375
train loss:  0.39643150568008423
train gradient:  0.6354255363972388
iteration : 4038
train acc:  0.8359375
train loss:  0.42677944898605347
train gradient:  0.3647095936511104
iteration : 4039
train acc:  0.8984375
train loss:  0.2751580476760864
train gradient:  0.21748097339578903
iteration : 4040
train acc:  0.8515625
train loss:  0.3478931188583374
train gradient:  0.31589036859898734
iteration : 4041
train acc:  0.8828125
train loss:  0.2958256006240845
train gradient:  0.23776299573083223
iteration : 4042
train acc:  0.8984375
train loss:  0.2808331847190857
train gradient:  0.23593258061575717
iteration : 4043
train acc:  0.8359375
train loss:  0.37571606040000916
train gradient:  0.298605292688198
iteration : 4044
train acc:  0.8359375
train loss:  0.37940728664398193
train gradient:  0.3933702331662892
iteration : 4045
train acc:  0.859375
train loss:  0.3598453402519226
train gradient:  0.41568215126972136
iteration : 4046
train acc:  0.8671875
train loss:  0.38709840178489685
train gradient:  0.34375829193354623
iteration : 4047
train acc:  0.8125
train loss:  0.3737207055091858
train gradient:  0.22924779769422923
iteration : 4048
train acc:  0.8046875
train loss:  0.41663673520088196
train gradient:  0.41330977279040576
iteration : 4049
train acc:  0.8515625
train loss:  0.3523308038711548
train gradient:  0.23982725679446973
iteration : 4050
train acc:  0.8203125
train loss:  0.40886375308036804
train gradient:  0.31690790122638457
iteration : 4051
train acc:  0.8125
train loss:  0.3864176273345947
train gradient:  0.25767867300935493
iteration : 4052
train acc:  0.828125
train loss:  0.344620019197464
train gradient:  0.2913147454509822
iteration : 4053
train acc:  0.828125
train loss:  0.3857877254486084
train gradient:  0.3207594108353724
iteration : 4054
train acc:  0.8203125
train loss:  0.42354539036750793
train gradient:  0.32018988934574044
iteration : 4055
train acc:  0.8671875
train loss:  0.298561155796051
train gradient:  0.26469463978038466
iteration : 4056
train acc:  0.828125
train loss:  0.3735232949256897
train gradient:  0.2881191755472394
iteration : 4057
train acc:  0.8828125
train loss:  0.2986796498298645
train gradient:  0.19810604753944733
iteration : 4058
train acc:  0.828125
train loss:  0.3310561180114746
train gradient:  0.2547675683185996
iteration : 4059
train acc:  0.90625
train loss:  0.2831224799156189
train gradient:  0.18469962556825145
iteration : 4060
train acc:  0.8203125
train loss:  0.368267297744751
train gradient:  0.38314334838770264
iteration : 4061
train acc:  0.8671875
train loss:  0.30376136302948
train gradient:  0.247583332441105
iteration : 4062
train acc:  0.84375
train loss:  0.37351953983306885
train gradient:  0.2869477791025952
iteration : 4063
train acc:  0.828125
train loss:  0.35180947184562683
train gradient:  0.2621626159248474
iteration : 4064
train acc:  0.828125
train loss:  0.3422614336013794
train gradient:  0.3544742193039621
iteration : 4065
train acc:  0.8359375
train loss:  0.3523644208908081
train gradient:  0.2775772774745158
iteration : 4066
train acc:  0.859375
train loss:  0.2793092429637909
train gradient:  0.21510003263643612
iteration : 4067
train acc:  0.8046875
train loss:  0.4100787043571472
train gradient:  0.35403701817656485
iteration : 4068
train acc:  0.8515625
train loss:  0.3636825680732727
train gradient:  0.2949944301219651
iteration : 4069
train acc:  0.8828125
train loss:  0.31236279010772705
train gradient:  0.2723153896981027
iteration : 4070
train acc:  0.8828125
train loss:  0.3040822446346283
train gradient:  0.2327121901413721
iteration : 4071
train acc:  0.8671875
train loss:  0.3313763737678528
train gradient:  0.28182125884310394
iteration : 4072
train acc:  0.8671875
train loss:  0.32479599118232727
train gradient:  0.2694705925519973
iteration : 4073
train acc:  0.8359375
train loss:  0.3952842950820923
train gradient:  0.28672128185528534
iteration : 4074
train acc:  0.84375
train loss:  0.3844081461429596
train gradient:  0.35490018025517234
iteration : 4075
train acc:  0.8203125
train loss:  0.3577669858932495
train gradient:  0.3235115968731179
iteration : 4076
train acc:  0.859375
train loss:  0.33667391538619995
train gradient:  0.33948885090896613
iteration : 4077
train acc:  0.8046875
train loss:  0.40695518255233765
train gradient:  0.4866644736524437
iteration : 4078
train acc:  0.8515625
train loss:  0.378049373626709
train gradient:  0.30210682727886945
iteration : 4079
train acc:  0.8203125
train loss:  0.40709763765335083
train gradient:  0.3620118483798216
iteration : 4080
train acc:  0.8046875
train loss:  0.4217761754989624
train gradient:  0.3870762689408336
iteration : 4081
train acc:  0.8359375
train loss:  0.364188551902771
train gradient:  0.32467172283652407
iteration : 4082
train acc:  0.90625
train loss:  0.24451197683811188
train gradient:  0.15688729858010766
iteration : 4083
train acc:  0.859375
train loss:  0.3000911772251129
train gradient:  0.23986718288387615
iteration : 4084
train acc:  0.8359375
train loss:  0.30763596296310425
train gradient:  0.2104164807344783
iteration : 4085
train acc:  0.8046875
train loss:  0.39396601915359497
train gradient:  0.3394582233995696
iteration : 4086
train acc:  0.796875
train loss:  0.41310930252075195
train gradient:  0.49057812203162754
iteration : 4087
train acc:  0.8515625
train loss:  0.3485789895057678
train gradient:  0.2771933443284631
iteration : 4088
train acc:  0.9140625
train loss:  0.30672162771224976
train gradient:  0.29430941839633973
iteration : 4089
train acc:  0.875
train loss:  0.3095163404941559
train gradient:  0.17102330707215274
iteration : 4090
train acc:  0.90625
train loss:  0.24848037958145142
train gradient:  0.19346233475135868
iteration : 4091
train acc:  0.84375
train loss:  0.34808480739593506
train gradient:  0.26838173827564205
iteration : 4092
train acc:  0.84375
train loss:  0.4462125301361084
train gradient:  0.3301238123511947
iteration : 4093
train acc:  0.8359375
train loss:  0.39999544620513916
train gradient:  0.35830269428070716
iteration : 4094
train acc:  0.8046875
train loss:  0.4082268476486206
train gradient:  0.3359022026268948
iteration : 4095
train acc:  0.8671875
train loss:  0.30535125732421875
train gradient:  0.21054958670110424
iteration : 4096
train acc:  0.8984375
train loss:  0.3028581738471985
train gradient:  0.2649611998134652
iteration : 4097
train acc:  0.8203125
train loss:  0.44508400559425354
train gradient:  0.551122739528717
iteration : 4098
train acc:  0.8515625
train loss:  0.3243478536605835
train gradient:  0.2736142758237048
iteration : 4099
train acc:  0.8984375
train loss:  0.2641240656375885
train gradient:  0.2009144982647248
iteration : 4100
train acc:  0.8515625
train loss:  0.3190217614173889
train gradient:  0.24275317865921742
iteration : 4101
train acc:  0.8203125
train loss:  0.3984640836715698
train gradient:  0.45678437485656065
iteration : 4102
train acc:  0.828125
train loss:  0.34563618898391724
train gradient:  0.2639833182795436
iteration : 4103
train acc:  0.7421875
train loss:  0.49718624353408813
train gradient:  0.3646686034076516
iteration : 4104
train acc:  0.875
train loss:  0.3466123640537262
train gradient:  0.27223977195522603
iteration : 4105
train acc:  0.796875
train loss:  0.40921467542648315
train gradient:  0.38961678562931024
iteration : 4106
train acc:  0.8671875
train loss:  0.37481552362442017
train gradient:  0.34963034405321286
iteration : 4107
train acc:  0.828125
train loss:  0.3949233591556549
train gradient:  0.38409990668528365
iteration : 4108
train acc:  0.8828125
train loss:  0.3306315839290619
train gradient:  0.2822338015349636
iteration : 4109
train acc:  0.8359375
train loss:  0.381712406873703
train gradient:  0.3582070877330191
iteration : 4110
train acc:  0.8671875
train loss:  0.36373019218444824
train gradient:  0.2961665048374132
iteration : 4111
train acc:  0.84375
train loss:  0.3344339430332184
train gradient:  0.24498160462413215
iteration : 4112
train acc:  0.875
train loss:  0.32320529222488403
train gradient:  0.19524866320602507
iteration : 4113
train acc:  0.8828125
train loss:  0.29311931133270264
train gradient:  0.2738862241495085
iteration : 4114
train acc:  0.8203125
train loss:  0.3287643492221832
train gradient:  0.2041025084483007
iteration : 4115
train acc:  0.796875
train loss:  0.4298020601272583
train gradient:  0.30099756387738025
iteration : 4116
train acc:  0.828125
train loss:  0.49725621938705444
train gradient:  0.7927514833526752
iteration : 4117
train acc:  0.8515625
train loss:  0.30891191959381104
train gradient:  0.25904658664498775
iteration : 4118
train acc:  0.890625
train loss:  0.2676432132720947
train gradient:  0.16490398368962123
iteration : 4119
train acc:  0.8203125
train loss:  0.3458479940891266
train gradient:  0.28357369737686655
iteration : 4120
train acc:  0.796875
train loss:  0.4366565942764282
train gradient:  0.39905859205082866
iteration : 4121
train acc:  0.90625
train loss:  0.23887647688388824
train gradient:  0.1833454843475849
iteration : 4122
train acc:  0.8515625
train loss:  0.30913373827934265
train gradient:  0.24943018639586137
iteration : 4123
train acc:  0.8671875
train loss:  0.3210631012916565
train gradient:  0.18028761280436822
iteration : 4124
train acc:  0.859375
train loss:  0.3461363613605499
train gradient:  0.36785084500179155
iteration : 4125
train acc:  0.9140625
train loss:  0.2711154520511627
train gradient:  0.18462189925113834
iteration : 4126
train acc:  0.828125
train loss:  0.4071023464202881
train gradient:  0.26536926170332287
iteration : 4127
train acc:  0.84375
train loss:  0.36933451890945435
train gradient:  0.25030643866439145
iteration : 4128
train acc:  0.859375
train loss:  0.34663069248199463
train gradient:  0.2616272970014171
iteration : 4129
train acc:  0.84375
train loss:  0.3389171361923218
train gradient:  0.2988170381822897
iteration : 4130
train acc:  0.828125
train loss:  0.4498031735420227
train gradient:  0.39685416039012983
iteration : 4131
train acc:  0.859375
train loss:  0.2997574806213379
train gradient:  0.2322013429188287
iteration : 4132
train acc:  0.8515625
train loss:  0.3809530436992645
train gradient:  0.3060175253199763
iteration : 4133
train acc:  0.84375
train loss:  0.3834652304649353
train gradient:  0.28092358722959887
iteration : 4134
train acc:  0.8203125
train loss:  0.38031134009361267
train gradient:  0.2858585605942264
iteration : 4135
train acc:  0.8046875
train loss:  0.3910461664199829
train gradient:  0.2662934768007782
iteration : 4136
train acc:  0.828125
train loss:  0.4259788990020752
train gradient:  0.35242573226285945
iteration : 4137
train acc:  0.8828125
train loss:  0.29750773310661316
train gradient:  0.19447400933957965
iteration : 4138
train acc:  0.8203125
train loss:  0.375168114900589
train gradient:  0.2633822675792594
iteration : 4139
train acc:  0.84375
train loss:  0.3740096092224121
train gradient:  0.21869702982903655
iteration : 4140
train acc:  0.84375
train loss:  0.39205724000930786
train gradient:  0.28836166771364197
iteration : 4141
train acc:  0.875
train loss:  0.2974003553390503
train gradient:  0.18370609114138936
iteration : 4142
train acc:  0.8203125
train loss:  0.33070284128189087
train gradient:  0.2660702748803005
iteration : 4143
train acc:  0.8203125
train loss:  0.3520064949989319
train gradient:  0.29004822023586374
iteration : 4144
train acc:  0.8359375
train loss:  0.3592424988746643
train gradient:  0.2639691054856173
iteration : 4145
train acc:  0.8828125
train loss:  0.296293705701828
train gradient:  0.17588368842050806
iteration : 4146
train acc:  0.8515625
train loss:  0.3532771170139313
train gradient:  0.2293887500178366
iteration : 4147
train acc:  0.875
train loss:  0.2959361672401428
train gradient:  0.22389670198819384
iteration : 4148
train acc:  0.8515625
train loss:  0.3647591471672058
train gradient:  0.23570997343405714
iteration : 4149
train acc:  0.8671875
train loss:  0.33510589599609375
train gradient:  0.20249030094990186
iteration : 4150
train acc:  0.8828125
train loss:  0.3661069869995117
train gradient:  0.3534020720433212
iteration : 4151
train acc:  0.890625
train loss:  0.2694738507270813
train gradient:  0.18676216494784503
iteration : 4152
train acc:  0.8359375
train loss:  0.395165354013443
train gradient:  0.38624596924820737
iteration : 4153
train acc:  0.796875
train loss:  0.464382141828537
train gradient:  0.4086994903012255
iteration : 4154
train acc:  0.8515625
train loss:  0.3788074851036072
train gradient:  0.288882143177622
iteration : 4155
train acc:  0.8359375
train loss:  0.32467496395111084
train gradient:  0.17168831150959551
iteration : 4156
train acc:  0.8671875
train loss:  0.31933480501174927
train gradient:  0.1798700490368995
iteration : 4157
train acc:  0.9140625
train loss:  0.2550882399082184
train gradient:  0.15707045487747262
iteration : 4158
train acc:  0.8046875
train loss:  0.45734548568725586
train gradient:  0.44770219370734254
iteration : 4159
train acc:  0.8203125
train loss:  0.4179055094718933
train gradient:  0.28737573584940307
iteration : 4160
train acc:  0.7890625
train loss:  0.41770845651626587
train gradient:  0.397151826313925
iteration : 4161
train acc:  0.828125
train loss:  0.4299240708351135
train gradient:  0.37693527387739423
iteration : 4162
train acc:  0.8359375
train loss:  0.32607534527778625
train gradient:  0.2523785184972583
iteration : 4163
train acc:  0.8515625
train loss:  0.2978699803352356
train gradient:  0.1827925635388916
iteration : 4164
train acc:  0.8125
train loss:  0.4137958288192749
train gradient:  0.42534804796533104
iteration : 4165
train acc:  0.8359375
train loss:  0.33548271656036377
train gradient:  0.2296588835846763
iteration : 4166
train acc:  0.8359375
train loss:  0.35514211654663086
train gradient:  0.2327272764414357
iteration : 4167
train acc:  0.84375
train loss:  0.4107547998428345
train gradient:  0.3856682733606279
iteration : 4168
train acc:  0.875
train loss:  0.36918413639068604
train gradient:  0.24419230663101876
iteration : 4169
train acc:  0.8046875
train loss:  0.4114760458469391
train gradient:  0.48957892081181575
iteration : 4170
train acc:  0.890625
train loss:  0.28842419385910034
train gradient:  0.20291863773711147
iteration : 4171
train acc:  0.828125
train loss:  0.375256210565567
train gradient:  0.27855591730278434
iteration : 4172
train acc:  0.875
train loss:  0.3391363024711609
train gradient:  0.2717484100511594
iteration : 4173
train acc:  0.875
train loss:  0.33110448718070984
train gradient:  0.31196435510571224
iteration : 4174
train acc:  0.7734375
train loss:  0.4473656117916107
train gradient:  0.48861196761865794
iteration : 4175
train acc:  0.8515625
train loss:  0.37099769711494446
train gradient:  0.374928364174289
iteration : 4176
train acc:  0.8203125
train loss:  0.42894938588142395
train gradient:  0.4242644055351656
iteration : 4177
train acc:  0.8515625
train loss:  0.33211231231689453
train gradient:  0.2786454643483498
iteration : 4178
train acc:  0.859375
train loss:  0.34602731466293335
train gradient:  0.27100029909385326
iteration : 4179
train acc:  0.875
train loss:  0.3165942430496216
train gradient:  0.19889661528671992
iteration : 4180
train acc:  0.8046875
train loss:  0.36877790093421936
train gradient:  0.22347962628182144
iteration : 4181
train acc:  0.90625
train loss:  0.27675461769104004
train gradient:  0.2043052607213805
iteration : 4182
train acc:  0.78125
train loss:  0.42997032403945923
train gradient:  0.5281740047129614
iteration : 4183
train acc:  0.7578125
train loss:  0.4859697222709656
train gradient:  0.49928165811990377
iteration : 4184
train acc:  0.859375
train loss:  0.3677015006542206
train gradient:  0.3206656169329963
iteration : 4185
train acc:  0.796875
train loss:  0.43358421325683594
train gradient:  0.28908511727582686
iteration : 4186
train acc:  0.828125
train loss:  0.36744236946105957
train gradient:  0.21995762006606032
iteration : 4187
train acc:  0.828125
train loss:  0.36397039890289307
train gradient:  0.34587364815536686
iteration : 4188
train acc:  0.8671875
train loss:  0.35878264904022217
train gradient:  0.2968781994266441
iteration : 4189
train acc:  0.8671875
train loss:  0.29127997159957886
train gradient:  0.17752296147952407
iteration : 4190
train acc:  0.8046875
train loss:  0.3651498854160309
train gradient:  0.2397904995887835
iteration : 4191
train acc:  0.7734375
train loss:  0.41660481691360474
train gradient:  0.2930581967997446
iteration : 4192
train acc:  0.8515625
train loss:  0.3145367503166199
train gradient:  0.22573458938076582
iteration : 4193
train acc:  0.8203125
train loss:  0.38029158115386963
train gradient:  0.3299694958282797
iteration : 4194
train acc:  0.859375
train loss:  0.35445940494537354
train gradient:  0.2685588602370814
iteration : 4195
train acc:  0.796875
train loss:  0.3884452283382416
train gradient:  0.24150130854743226
iteration : 4196
train acc:  0.8359375
train loss:  0.32214415073394775
train gradient:  0.2812274873826277
iteration : 4197
train acc:  0.875
train loss:  0.31329166889190674
train gradient:  0.211178155316116
iteration : 4198
train acc:  0.8125
train loss:  0.4408489167690277
train gradient:  0.40824052027547114
iteration : 4199
train acc:  0.7734375
train loss:  0.39935359358787537
train gradient:  0.24650346448962457
iteration : 4200
train acc:  0.8828125
train loss:  0.3080872893333435
train gradient:  0.2262141534799689
iteration : 4201
train acc:  0.859375
train loss:  0.30980682373046875
train gradient:  0.2568695140323905
iteration : 4202
train acc:  0.8359375
train loss:  0.37462228536605835
train gradient:  0.36051044433630186
iteration : 4203
train acc:  0.8359375
train loss:  0.3662649989128113
train gradient:  0.23962230678591095
iteration : 4204
train acc:  0.859375
train loss:  0.3346916735172272
train gradient:  0.29439050789280896
iteration : 4205
train acc:  0.78125
train loss:  0.45575010776519775
train gradient:  0.3185795859583887
iteration : 4206
train acc:  0.8046875
train loss:  0.472157746553421
train gradient:  0.3850782478521311
iteration : 4207
train acc:  0.8671875
train loss:  0.36353766918182373
train gradient:  0.22769639277247672
iteration : 4208
train acc:  0.8203125
train loss:  0.4015456438064575
train gradient:  0.34827671023830675
iteration : 4209
train acc:  0.8515625
train loss:  0.3261287808418274
train gradient:  0.19714838165402132
iteration : 4210
train acc:  0.8984375
train loss:  0.3482351005077362
train gradient:  0.2991970813492864
iteration : 4211
train acc:  0.75
train loss:  0.5021080374717712
train gradient:  0.42807537709379595
iteration : 4212
train acc:  0.890625
train loss:  0.36347758769989014
train gradient:  0.19978011453442174
iteration : 4213
train acc:  0.828125
train loss:  0.37223488092422485
train gradient:  0.33056649357640644
iteration : 4214
train acc:  0.84375
train loss:  0.3969351351261139
train gradient:  0.24775595422208152
iteration : 4215
train acc:  0.8515625
train loss:  0.342257559299469
train gradient:  0.17794348750347616
iteration : 4216
train acc:  0.859375
train loss:  0.3413587510585785
train gradient:  0.2033602688325086
iteration : 4217
train acc:  0.78125
train loss:  0.38750413060188293
train gradient:  0.33136341283513104
iteration : 4218
train acc:  0.8359375
train loss:  0.3304711580276489
train gradient:  0.25499980327570204
iteration : 4219
train acc:  0.828125
train loss:  0.3590623140335083
train gradient:  0.2384808910497198
iteration : 4220
train acc:  0.8125
train loss:  0.4388694763183594
train gradient:  0.39045110742470174
iteration : 4221
train acc:  0.8984375
train loss:  0.2614511251449585
train gradient:  0.12041968644994583
iteration : 4222
train acc:  0.8984375
train loss:  0.29947489500045776
train gradient:  0.2441916938929124
iteration : 4223
train acc:  0.84375
train loss:  0.33683860301971436
train gradient:  0.22810513804008659
iteration : 4224
train acc:  0.828125
train loss:  0.3583998680114746
train gradient:  0.23666686433166123
iteration : 4225
train acc:  0.84375
train loss:  0.3607364296913147
train gradient:  0.2436242873582286
iteration : 4226
train acc:  0.8046875
train loss:  0.36925339698791504
train gradient:  0.26077607297205685
iteration : 4227
train acc:  0.8125
train loss:  0.4018467664718628
train gradient:  0.2466540542684551
iteration : 4228
train acc:  0.84375
train loss:  0.34634584188461304
train gradient:  0.28763529944901367
iteration : 4229
train acc:  0.8046875
train loss:  0.4043499529361725
train gradient:  0.34993916042965323
iteration : 4230
train acc:  0.859375
train loss:  0.31801581382751465
train gradient:  0.1960774721126937
iteration : 4231
train acc:  0.84375
train loss:  0.33177220821380615
train gradient:  0.26096616169126885
iteration : 4232
train acc:  0.828125
train loss:  0.38601142168045044
train gradient:  0.30360181934498975
iteration : 4233
train acc:  0.828125
train loss:  0.4333752393722534
train gradient:  0.324135361460477
iteration : 4234
train acc:  0.8359375
train loss:  0.37199151515960693
train gradient:  0.3150149655170533
iteration : 4235
train acc:  0.8671875
train loss:  0.332610547542572
train gradient:  0.25220096349293986
iteration : 4236
train acc:  0.8515625
train loss:  0.3266347646713257
train gradient:  0.27466445927508976
iteration : 4237
train acc:  0.828125
train loss:  0.3952851891517639
train gradient:  0.3251107196005974
iteration : 4238
train acc:  0.859375
train loss:  0.33401668071746826
train gradient:  0.18018897376400173
iteration : 4239
train acc:  0.8828125
train loss:  0.30164778232574463
train gradient:  0.17144250156293325
iteration : 4240
train acc:  0.8203125
train loss:  0.3909708559513092
train gradient:  0.2770195395429493
iteration : 4241
train acc:  0.890625
train loss:  0.25418418645858765
train gradient:  0.170498793147048
iteration : 4242
train acc:  0.78125
train loss:  0.4395967125892639
train gradient:  0.5143590130681592
iteration : 4243
train acc:  0.859375
train loss:  0.3420494496822357
train gradient:  0.21920186898303207
iteration : 4244
train acc:  0.859375
train loss:  0.3316284716129303
train gradient:  0.31902935555850054
iteration : 4245
train acc:  0.8359375
train loss:  0.3950274884700775
train gradient:  0.44907564964067476
iteration : 4246
train acc:  0.859375
train loss:  0.33195507526397705
train gradient:  0.19925580068905124
iteration : 4247
train acc:  0.8125
train loss:  0.39134204387664795
train gradient:  0.24075863922245844
iteration : 4248
train acc:  0.8203125
train loss:  0.3479968309402466
train gradient:  0.2417137215769516
iteration : 4249
train acc:  0.8515625
train loss:  0.33761101961135864
train gradient:  0.2616881009496813
iteration : 4250
train acc:  0.7890625
train loss:  0.43939679861068726
train gradient:  0.3703341935916294
iteration : 4251
train acc:  0.8203125
train loss:  0.40317803621292114
train gradient:  0.28906195503469245
iteration : 4252
train acc:  0.84375
train loss:  0.3404783606529236
train gradient:  0.24222079607959762
iteration : 4253
train acc:  0.84375
train loss:  0.3441780209541321
train gradient:  0.220683493004888
iteration : 4254
train acc:  0.90625
train loss:  0.24808363616466522
train gradient:  0.14366092250855442
iteration : 4255
train acc:  0.8359375
train loss:  0.40516337752342224
train gradient:  0.22689149814484405
iteration : 4256
train acc:  0.828125
train loss:  0.4195541739463806
train gradient:  0.45228670246783265
iteration : 4257
train acc:  0.859375
train loss:  0.3398851752281189
train gradient:  0.18453807421498508
iteration : 4258
train acc:  0.8203125
train loss:  0.39380955696105957
train gradient:  0.3178759164913185
iteration : 4259
train acc:  0.8984375
train loss:  0.32319772243499756
train gradient:  0.24851649929152397
iteration : 4260
train acc:  0.8671875
train loss:  0.344490647315979
train gradient:  0.3398633795084106
iteration : 4261
train acc:  0.8828125
train loss:  0.30203819274902344
train gradient:  0.21817299899051434
iteration : 4262
train acc:  0.75
train loss:  0.4584481418132782
train gradient:  0.5525692083006253
iteration : 4263
train acc:  0.8203125
train loss:  0.3730872869491577
train gradient:  0.2857869607500912
iteration : 4264
train acc:  0.8046875
train loss:  0.4285467863082886
train gradient:  0.435784382780478
iteration : 4265
train acc:  0.8046875
train loss:  0.3515404462814331
train gradient:  0.22760972492909196
iteration : 4266
train acc:  0.8203125
train loss:  0.36139383912086487
train gradient:  0.3193119076443848
iteration : 4267
train acc:  0.8359375
train loss:  0.3747835159301758
train gradient:  0.27096105536212683
iteration : 4268
train acc:  0.8203125
train loss:  0.37872493267059326
train gradient:  0.4313166065702696
iteration : 4269
train acc:  0.8515625
train loss:  0.3425499200820923
train gradient:  0.23624235310079084
iteration : 4270
train acc:  0.859375
train loss:  0.3411034941673279
train gradient:  0.24863616958552925
iteration : 4271
train acc:  0.8359375
train loss:  0.30085569620132446
train gradient:  0.2785136947495147
iteration : 4272
train acc:  0.8125
train loss:  0.37656545639038086
train gradient:  0.3333125491873168
iteration : 4273
train acc:  0.8203125
train loss:  0.3754897117614746
train gradient:  0.24235183810244204
iteration : 4274
train acc:  0.78125
train loss:  0.4762442708015442
train gradient:  0.37522006455743995
iteration : 4275
train acc:  0.8515625
train loss:  0.36599624156951904
train gradient:  0.19818196448002035
iteration : 4276
train acc:  0.84375
train loss:  0.37240591645240784
train gradient:  0.25836207757821783
iteration : 4277
train acc:  0.828125
train loss:  0.36289671063423157
train gradient:  0.3164387408613783
iteration : 4278
train acc:  0.828125
train loss:  0.36727020144462585
train gradient:  0.24284343681689896
iteration : 4279
train acc:  0.7734375
train loss:  0.4303375482559204
train gradient:  0.4489023634423338
iteration : 4280
train acc:  0.859375
train loss:  0.3249477744102478
train gradient:  0.2470696641097809
iteration : 4281
train acc:  0.84375
train loss:  0.3204219341278076
train gradient:  0.32287835157921957
iteration : 4282
train acc:  0.8828125
train loss:  0.3608331084251404
train gradient:  0.2589030404064843
iteration : 4283
train acc:  0.8671875
train loss:  0.3468327224254608
train gradient:  0.2690892232986201
iteration : 4284
train acc:  0.875
train loss:  0.2749536335468292
train gradient:  0.18662281283645854
iteration : 4285
train acc:  0.8828125
train loss:  0.32655394077301025
train gradient:  0.3320797917812695
iteration : 4286
train acc:  0.84375
train loss:  0.3538210391998291
train gradient:  0.2722794821857667
iteration : 4287
train acc:  0.8515625
train loss:  0.3819672465324402
train gradient:  0.2868715421114822
iteration : 4288
train acc:  0.796875
train loss:  0.4435446262359619
train gradient:  0.3145182282459227
iteration : 4289
train acc:  0.84375
train loss:  0.34552544355392456
train gradient:  0.263648481587199
iteration : 4290
train acc:  0.859375
train loss:  0.319246768951416
train gradient:  0.2497401487366171
iteration : 4291
train acc:  0.859375
train loss:  0.29335832595825195
train gradient:  0.19852943241914647
iteration : 4292
train acc:  0.796875
train loss:  0.39703983068466187
train gradient:  0.363824072616036
iteration : 4293
train acc:  0.8671875
train loss:  0.33503180742263794
train gradient:  0.24056935961322257
iteration : 4294
train acc:  0.8828125
train loss:  0.30818161368370056
train gradient:  0.23176170297106324
iteration : 4295
train acc:  0.8046875
train loss:  0.3688415288925171
train gradient:  0.28269278955942145
iteration : 4296
train acc:  0.875
train loss:  0.3191310465335846
train gradient:  0.20294943858790215
iteration : 4297
train acc:  0.828125
train loss:  0.33496588468551636
train gradient:  0.17081458268124378
iteration : 4298
train acc:  0.8203125
train loss:  0.3823195695877075
train gradient:  0.2601855522707942
iteration : 4299
train acc:  0.859375
train loss:  0.32541459798812866
train gradient:  0.19123485889921726
iteration : 4300
train acc:  0.8671875
train loss:  0.2986242175102234
train gradient:  0.23181135758446333
iteration : 4301
train acc:  0.8359375
train loss:  0.38806062936782837
train gradient:  0.3448833405770963
iteration : 4302
train acc:  0.8984375
train loss:  0.37023845314979553
train gradient:  0.21980745648943495
iteration : 4303
train acc:  0.859375
train loss:  0.27644041180610657
train gradient:  0.17786629491427083
iteration : 4304
train acc:  0.8828125
train loss:  0.3081516623497009
train gradient:  0.17580090156765144
iteration : 4305
train acc:  0.8359375
train loss:  0.3357731103897095
train gradient:  0.3208200016996126
iteration : 4306
train acc:  0.7890625
train loss:  0.41211655735969543
train gradient:  0.48713673465163493
iteration : 4307
train acc:  0.8125
train loss:  0.41012153029441833
train gradient:  0.31813856420126296
iteration : 4308
train acc:  0.84375
train loss:  0.4254663288593292
train gradient:  0.34640486036611334
iteration : 4309
train acc:  0.90625
train loss:  0.28479769825935364
train gradient:  0.1472720695600157
iteration : 4310
train acc:  0.8046875
train loss:  0.43398669362068176
train gradient:  0.4636199513440643
iteration : 4311
train acc:  0.78125
train loss:  0.40192800760269165
train gradient:  0.3189353007960061
iteration : 4312
train acc:  0.875
train loss:  0.3238586187362671
train gradient:  0.1946363184809637
iteration : 4313
train acc:  0.828125
train loss:  0.3839631676673889
train gradient:  0.2617027812954106
iteration : 4314
train acc:  0.90625
train loss:  0.2827356159687042
train gradient:  0.19207824163235354
iteration : 4315
train acc:  0.859375
train loss:  0.289450466632843
train gradient:  0.22632057221193147
iteration : 4316
train acc:  0.875
train loss:  0.33337104320526123
train gradient:  0.24609988481825829
iteration : 4317
train acc:  0.828125
train loss:  0.4346928596496582
train gradient:  0.416302081441331
iteration : 4318
train acc:  0.78125
train loss:  0.4403485655784607
train gradient:  0.3537261035924075
iteration : 4319
train acc:  0.78125
train loss:  0.4841660261154175
train gradient:  0.556178405041176
iteration : 4320
train acc:  0.875
train loss:  0.3716893792152405
train gradient:  0.4713010666947644
iteration : 4321
train acc:  0.8203125
train loss:  0.4090246856212616
train gradient:  0.37320263646297125
iteration : 4322
train acc:  0.8359375
train loss:  0.3927396833896637
train gradient:  0.3347842006811596
iteration : 4323
train acc:  0.8515625
train loss:  0.34946951270103455
train gradient:  0.7311264189538684
iteration : 4324
train acc:  0.8671875
train loss:  0.2751753330230713
train gradient:  0.21378597885379413
iteration : 4325
train acc:  0.8203125
train loss:  0.3908536732196808
train gradient:  0.32779072740304876
iteration : 4326
train acc:  0.875
train loss:  0.3113369643688202
train gradient:  0.1770776019415022
iteration : 4327
train acc:  0.78125
train loss:  0.38440507650375366
train gradient:  0.31215180716483987
iteration : 4328
train acc:  0.7890625
train loss:  0.4670199453830719
train gradient:  0.7117027439615318
iteration : 4329
train acc:  0.8671875
train loss:  0.39028576016426086
train gradient:  0.38496395899396085
iteration : 4330
train acc:  0.8515625
train loss:  0.3196406364440918
train gradient:  0.2843481713920409
iteration : 4331
train acc:  0.8125
train loss:  0.45379748940467834
train gradient:  0.4227474684704371
iteration : 4332
train acc:  0.796875
train loss:  0.47942185401916504
train gradient:  0.38793021998325083
iteration : 4333
train acc:  0.84375
train loss:  0.30020803213119507
train gradient:  0.2748035045330529
iteration : 4334
train acc:  0.78125
train loss:  0.42626529932022095
train gradient:  0.4867752702691918
iteration : 4335
train acc:  0.828125
train loss:  0.37065237760543823
train gradient:  0.26641743519323086
iteration : 4336
train acc:  0.8828125
train loss:  0.33101004362106323
train gradient:  0.24423255468784438
iteration : 4337
train acc:  0.828125
train loss:  0.4151776134967804
train gradient:  0.3600908674001409
iteration : 4338
train acc:  0.8125
train loss:  0.3590306043624878
train gradient:  0.2574550023641067
iteration : 4339
train acc:  0.890625
train loss:  0.296781986951828
train gradient:  0.2889144543085395
iteration : 4340
train acc:  0.859375
train loss:  0.3316599726676941
train gradient:  0.17979433510279236
iteration : 4341
train acc:  0.8671875
train loss:  0.31020963191986084
train gradient:  0.25805057153207517
iteration : 4342
train acc:  0.8125
train loss:  0.42473798990249634
train gradient:  0.312642435980435
iteration : 4343
train acc:  0.859375
train loss:  0.4036199450492859
train gradient:  0.2799027892081206
iteration : 4344
train acc:  0.890625
train loss:  0.28672128915786743
train gradient:  0.20946581302569373
iteration : 4345
train acc:  0.8125
train loss:  0.3403538167476654
train gradient:  0.32464245383495954
iteration : 4346
train acc:  0.8359375
train loss:  0.4004361033439636
train gradient:  0.4141742652819906
iteration : 4347
train acc:  0.8203125
train loss:  0.3834843933582306
train gradient:  0.2658457346689653
iteration : 4348
train acc:  0.875
train loss:  0.31448352336883545
train gradient:  0.25974114161404666
iteration : 4349
train acc:  0.8671875
train loss:  0.35630959272384644
train gradient:  0.2351333379753843
iteration : 4350
train acc:  0.8359375
train loss:  0.3695281147956848
train gradient:  0.31445439434979605
iteration : 4351
train acc:  0.8515625
train loss:  0.35378602147102356
train gradient:  0.2705248229153969
iteration : 4352
train acc:  0.890625
train loss:  0.3257371187210083
train gradient:  0.26465795964486555
iteration : 4353
train acc:  0.8359375
train loss:  0.39943405985832214
train gradient:  0.3815653176132067
iteration : 4354
train acc:  0.859375
train loss:  0.2982090711593628
train gradient:  0.3022141371121147
iteration : 4355
train acc:  0.765625
train loss:  0.4636945128440857
train gradient:  0.4040319137047237
iteration : 4356
train acc:  0.8671875
train loss:  0.40441077947616577
train gradient:  0.28128222453308793
iteration : 4357
train acc:  0.9296875
train loss:  0.2564656436443329
train gradient:  0.16699094743704107
iteration : 4358
train acc:  0.8984375
train loss:  0.2484302520751953
train gradient:  0.17218469056752145
iteration : 4359
train acc:  0.8359375
train loss:  0.3279135823249817
train gradient:  0.21774055205027304
iteration : 4360
train acc:  0.8203125
train loss:  0.40384477376937866
train gradient:  0.3935976719366814
iteration : 4361
train acc:  0.828125
train loss:  0.3406493365764618
train gradient:  0.2777543874482309
iteration : 4362
train acc:  0.78125
train loss:  0.42422908544540405
train gradient:  0.28637082162974636
iteration : 4363
train acc:  0.8671875
train loss:  0.32593753933906555
train gradient:  0.22959407141111576
iteration : 4364
train acc:  0.8671875
train loss:  0.35049670934677124
train gradient:  0.2349547584397107
iteration : 4365
train acc:  0.8046875
train loss:  0.44374287128448486
train gradient:  0.3379008493635397
iteration : 4366
train acc:  0.8125
train loss:  0.38640689849853516
train gradient:  0.2289040794110204
iteration : 4367
train acc:  0.859375
train loss:  0.3520348370075226
train gradient:  0.2906420712945634
iteration : 4368
train acc:  0.84375
train loss:  0.36398109793663025
train gradient:  0.2505894230125129
iteration : 4369
train acc:  0.8515625
train loss:  0.3183298110961914
train gradient:  0.24564824693631748
iteration : 4370
train acc:  0.8125
train loss:  0.4230441153049469
train gradient:  0.3240649597477989
iteration : 4371
train acc:  0.7734375
train loss:  0.3860715627670288
train gradient:  0.35209169644088917
iteration : 4372
train acc:  0.84375
train loss:  0.35804885625839233
train gradient:  0.3039078560615449
iteration : 4373
train acc:  0.84375
train loss:  0.38955962657928467
train gradient:  0.33722004688324386
iteration : 4374
train acc:  0.8671875
train loss:  0.3080185055732727
train gradient:  0.2013714286371195
iteration : 4375
train acc:  0.8203125
train loss:  0.3862611949443817
train gradient:  1.3854829666242494
iteration : 4376
train acc:  0.8359375
train loss:  0.34426993131637573
train gradient:  0.19841527760455174
iteration : 4377
train acc:  0.8359375
train loss:  0.37236472964286804
train gradient:  0.22515349283885983
iteration : 4378
train acc:  0.8828125
train loss:  0.2971224784851074
train gradient:  0.19673406939360394
iteration : 4379
train acc:  0.8203125
train loss:  0.36416763067245483
train gradient:  0.3652846898476199
iteration : 4380
train acc:  0.8203125
train loss:  0.31653720140457153
train gradient:  0.25832615328190073
iteration : 4381
train acc:  0.828125
train loss:  0.3522527515888214
train gradient:  0.19355991495993488
iteration : 4382
train acc:  0.859375
train loss:  0.3310033082962036
train gradient:  0.1906296504601192
iteration : 4383
train acc:  0.8671875
train loss:  0.35565081238746643
train gradient:  0.17850923188638132
iteration : 4384
train acc:  0.8671875
train loss:  0.30337488651275635
train gradient:  0.1522742322305264
iteration : 4385
train acc:  0.8671875
train loss:  0.29369962215423584
train gradient:  0.23144461921011256
iteration : 4386
train acc:  0.859375
train loss:  0.36643415689468384
train gradient:  0.23165494150795926
iteration : 4387
train acc:  0.8203125
train loss:  0.40909963846206665
train gradient:  0.6230490544121642
iteration : 4388
train acc:  0.8515625
train loss:  0.3397623896598816
train gradient:  0.21296999402883235
iteration : 4389
train acc:  0.890625
train loss:  0.28928858041763306
train gradient:  0.2248771720957785
iteration : 4390
train acc:  0.859375
train loss:  0.3527095913887024
train gradient:  0.2601755466920132
iteration : 4391
train acc:  0.875
train loss:  0.32635214924812317
train gradient:  0.2852849983074522
iteration : 4392
train acc:  0.8203125
train loss:  0.3425729274749756
train gradient:  0.30722416980098444
iteration : 4393
train acc:  0.7890625
train loss:  0.41905516386032104
train gradient:  0.2509060135038433
iteration : 4394
train acc:  0.7890625
train loss:  0.43654173612594604
train gradient:  0.5269861784628445
iteration : 4395
train acc:  0.796875
train loss:  0.35614830255508423
train gradient:  0.21671865287741493
iteration : 4396
train acc:  0.84375
train loss:  0.3483514189720154
train gradient:  0.3145923650022703
iteration : 4397
train acc:  0.7890625
train loss:  0.4246973395347595
train gradient:  0.3931012037219884
iteration : 4398
train acc:  0.8359375
train loss:  0.3340664803981781
train gradient:  0.2422901738396554
iteration : 4399
train acc:  0.796875
train loss:  0.3861130475997925
train gradient:  0.31215518837425893
iteration : 4400
train acc:  0.7890625
train loss:  0.3785315752029419
train gradient:  0.3378448721483134
iteration : 4401
train acc:  0.890625
train loss:  0.34623628854751587
train gradient:  0.30293610474361327
iteration : 4402
train acc:  0.8359375
train loss:  0.3536357879638672
train gradient:  0.26474739078981163
iteration : 4403
train acc:  0.84375
train loss:  0.33041107654571533
train gradient:  0.22018988749072993
iteration : 4404
train acc:  0.84375
train loss:  0.340086966753006
train gradient:  0.24724804365388964
iteration : 4405
train acc:  0.8125
train loss:  0.39495646953582764
train gradient:  0.4094640148836856
iteration : 4406
train acc:  0.8671875
train loss:  0.28892916440963745
train gradient:  0.22866070185711396
iteration : 4407
train acc:  0.8515625
train loss:  0.3241051137447357
train gradient:  0.22429762563416392
iteration : 4408
train acc:  0.8359375
train loss:  0.3288426697254181
train gradient:  0.22145414851612635
iteration : 4409
train acc:  0.8671875
train loss:  0.31082993745803833
train gradient:  0.23867205910437544
iteration : 4410
train acc:  0.8359375
train loss:  0.410472571849823
train gradient:  0.39214022130288295
iteration : 4411
train acc:  0.796875
train loss:  0.37765127420425415
train gradient:  0.38715091296561704
iteration : 4412
train acc:  0.828125
train loss:  0.3409900665283203
train gradient:  0.24235324430890265
iteration : 4413
train acc:  0.8046875
train loss:  0.388457715511322
train gradient:  0.30088028415930185
iteration : 4414
train acc:  0.8046875
train loss:  0.37052077054977417
train gradient:  0.3203683456748533
iteration : 4415
train acc:  0.8671875
train loss:  0.3191989064216614
train gradient:  0.12887065729568878
iteration : 4416
train acc:  0.8515625
train loss:  0.34818708896636963
train gradient:  0.28342837866630327
iteration : 4417
train acc:  0.859375
train loss:  0.33355724811553955
train gradient:  0.22814948443949623
iteration : 4418
train acc:  0.8359375
train loss:  0.3257312774658203
train gradient:  0.2219108688130063
iteration : 4419
train acc:  0.8359375
train loss:  0.3965493440628052
train gradient:  0.24611133425077514
iteration : 4420
train acc:  0.84375
train loss:  0.337742418050766
train gradient:  0.2544301191755645
iteration : 4421
train acc:  0.8828125
train loss:  0.3187927007675171
train gradient:  0.21064193850120153
iteration : 4422
train acc:  0.8359375
train loss:  0.4008355140686035
train gradient:  0.2546912089944855
iteration : 4423
train acc:  0.8203125
train loss:  0.39463359117507935
train gradient:  0.28845076806074843
iteration : 4424
train acc:  0.7890625
train loss:  0.42508435249328613
train gradient:  0.33294666326946987
iteration : 4425
train acc:  0.8203125
train loss:  0.36404019594192505
train gradient:  0.20982599205815217
iteration : 4426
train acc:  0.90625
train loss:  0.25457850098609924
train gradient:  0.19730342168573267
iteration : 4427
train acc:  0.8046875
train loss:  0.40172362327575684
train gradient:  0.6843077132433965
iteration : 4428
train acc:  0.828125
train loss:  0.3676898181438446
train gradient:  0.36176879378966575
iteration : 4429
train acc:  0.828125
train loss:  0.3657049536705017
train gradient:  0.31405725671011364
iteration : 4430
train acc:  0.8515625
train loss:  0.36773404479026794
train gradient:  0.2574493264740917
iteration : 4431
train acc:  0.84375
train loss:  0.4961557686328888
train gradient:  0.3536302956163551
iteration : 4432
train acc:  0.8203125
train loss:  0.3614063262939453
train gradient:  0.23825371445411964
iteration : 4433
train acc:  0.8125
train loss:  0.40231162309646606
train gradient:  0.2591389858356769
iteration : 4434
train acc:  0.8515625
train loss:  0.36826232075691223
train gradient:  0.40983222067770697
iteration : 4435
train acc:  0.7734375
train loss:  0.37871697545051575
train gradient:  0.2716705098630253
iteration : 4436
train acc:  0.8203125
train loss:  0.3696290850639343
train gradient:  0.2351222109026066
iteration : 4437
train acc:  0.8359375
train loss:  0.3384990692138672
train gradient:  0.28539009508044444
iteration : 4438
train acc:  0.8515625
train loss:  0.3547825515270233
train gradient:  0.3579409436368862
iteration : 4439
train acc:  0.8828125
train loss:  0.3032611310482025
train gradient:  0.19444474778806775
iteration : 4440
train acc:  0.875
train loss:  0.3027772307395935
train gradient:  0.188982971235012
iteration : 4441
train acc:  0.7890625
train loss:  0.42072343826293945
train gradient:  0.3144113411973759
iteration : 4442
train acc:  0.84375
train loss:  0.38744908571243286
train gradient:  0.341503585379093
iteration : 4443
train acc:  0.8203125
train loss:  0.37672650814056396
train gradient:  0.26086362016985365
iteration : 4444
train acc:  0.8203125
train loss:  0.3925454020500183
train gradient:  0.27703504490075515
iteration : 4445
train acc:  0.875
train loss:  0.3102600574493408
train gradient:  0.16966291494398728
iteration : 4446
train acc:  0.8359375
train loss:  0.3269224464893341
train gradient:  0.25399934153200193
iteration : 4447
train acc:  0.8203125
train loss:  0.41339653730392456
train gradient:  0.2597477393649181
iteration : 4448
train acc:  0.765625
train loss:  0.42479944229125977
train gradient:  0.3326163181411671
iteration : 4449
train acc:  0.8203125
train loss:  0.36726969480514526
train gradient:  0.3092507142250024
iteration : 4450
train acc:  0.8203125
train loss:  0.4327768087387085
train gradient:  0.40320045205964383
iteration : 4451
train acc:  0.859375
train loss:  0.35359200835227966
train gradient:  0.3316535778516082
iteration : 4452
train acc:  0.859375
train loss:  0.3100319504737854
train gradient:  0.27547166912521
iteration : 4453
train acc:  0.84375
train loss:  0.3545157313346863
train gradient:  0.18701449027309117
iteration : 4454
train acc:  0.8046875
train loss:  0.35159850120544434
train gradient:  0.28547013834434326
iteration : 4455
train acc:  0.8359375
train loss:  0.3859449028968811
train gradient:  0.3160858366985088
iteration : 4456
train acc:  0.859375
train loss:  0.3310611844062805
train gradient:  0.1958349325316121
iteration : 4457
train acc:  0.890625
train loss:  0.36706531047821045
train gradient:  0.2014519476436863
iteration : 4458
train acc:  0.8046875
train loss:  0.3890451192855835
train gradient:  0.28397252180324206
iteration : 4459
train acc:  0.8671875
train loss:  0.33425143361091614
train gradient:  0.17792399447543766
iteration : 4460
train acc:  0.796875
train loss:  0.4320644736289978
train gradient:  0.2900181008677821
iteration : 4461
train acc:  0.8515625
train loss:  0.3527095913887024
train gradient:  0.5138936663151215
iteration : 4462
train acc:  0.8203125
train loss:  0.36388319730758667
train gradient:  0.2516809103787012
iteration : 4463
train acc:  0.8671875
train loss:  0.350901335477829
train gradient:  0.23143343179384102
iteration : 4464
train acc:  0.796875
train loss:  0.374423086643219
train gradient:  0.29081318100009845
iteration : 4465
train acc:  0.828125
train loss:  0.4179266095161438
train gradient:  0.3696425856803013
iteration : 4466
train acc:  0.84375
train loss:  0.35064974427223206
train gradient:  0.1929212751280157
iteration : 4467
train acc:  0.78125
train loss:  0.4056210517883301
train gradient:  0.4127684239488697
iteration : 4468
train acc:  0.8125
train loss:  0.39782464504241943
train gradient:  0.21552765274403052
iteration : 4469
train acc:  0.75
train loss:  0.4978061020374298
train gradient:  0.3958483032081483
iteration : 4470
train acc:  0.875
train loss:  0.3112225830554962
train gradient:  0.152741324348565
iteration : 4471
train acc:  0.84375
train loss:  0.35946840047836304
train gradient:  0.2119167373753853
iteration : 4472
train acc:  0.84375
train loss:  0.3359251320362091
train gradient:  0.17650804699840875
iteration : 4473
train acc:  0.8359375
train loss:  0.3330470323562622
train gradient:  0.33335314025496865
iteration : 4474
train acc:  0.90625
train loss:  0.2622492015361786
train gradient:  0.12135282502303006
iteration : 4475
train acc:  0.828125
train loss:  0.3656603693962097
train gradient:  0.2970834749512096
iteration : 4476
train acc:  0.8125
train loss:  0.35286015272140503
train gradient:  0.3133324136935013
iteration : 4477
train acc:  0.859375
train loss:  0.3619866371154785
train gradient:  0.34320420686720776
iteration : 4478
train acc:  0.859375
train loss:  0.35285434126853943
train gradient:  0.2173817663587459
iteration : 4479
train acc:  0.8046875
train loss:  0.36665111780166626
train gradient:  0.289395115603036
iteration : 4480
train acc:  0.9140625
train loss:  0.28720182180404663
train gradient:  0.14811033002975862
iteration : 4481
train acc:  0.8671875
train loss:  0.3040771484375
train gradient:  0.24774387691427296
iteration : 4482
train acc:  0.8046875
train loss:  0.3700920343399048
train gradient:  0.24054112207432857
iteration : 4483
train acc:  0.8515625
train loss:  0.33588895201683044
train gradient:  0.20637014909693235
iteration : 4484
train acc:  0.875
train loss:  0.35230329632759094
train gradient:  0.3476042425737999
iteration : 4485
train acc:  0.828125
train loss:  0.38298556208610535
train gradient:  0.3115630567378821
iteration : 4486
train acc:  0.8671875
train loss:  0.32538384199142456
train gradient:  0.2176334462538576
iteration : 4487
train acc:  0.8515625
train loss:  0.33263903856277466
train gradient:  0.20375330257600754
iteration : 4488
train acc:  0.859375
train loss:  0.3995341658592224
train gradient:  0.3013494196405984
iteration : 4489
train acc:  0.84375
train loss:  0.3996206223964691
train gradient:  0.332300402164605
iteration : 4490
train acc:  0.859375
train loss:  0.3376060128211975
train gradient:  0.18804423228859818
iteration : 4491
train acc:  0.8671875
train loss:  0.3009132742881775
train gradient:  0.22032431588377055
iteration : 4492
train acc:  0.84375
train loss:  0.34335577487945557
train gradient:  0.5261263747985603
iteration : 4493
train acc:  0.875
train loss:  0.31595176458358765
train gradient:  0.21823297284680576
iteration : 4494
train acc:  0.90625
train loss:  0.27003538608551025
train gradient:  0.21206266165242257
iteration : 4495
train acc:  0.875
train loss:  0.33970049023628235
train gradient:  0.211083789994143
iteration : 4496
train acc:  0.84375
train loss:  0.3969937562942505
train gradient:  0.4890126106818468
iteration : 4497
train acc:  0.8125
train loss:  0.37143704295158386
train gradient:  0.34786103545874436
iteration : 4498
train acc:  0.7109375
train loss:  0.5652340650558472
train gradient:  0.5757714359370618
iteration : 4499
train acc:  0.8125
train loss:  0.4076407551765442
train gradient:  0.3037812192671029
iteration : 4500
train acc:  0.8125
train loss:  0.3925471305847168
train gradient:  0.4260582381904535
iteration : 4501
train acc:  0.8359375
train loss:  0.3263302445411682
train gradient:  0.2955940795081309
iteration : 4502
train acc:  0.796875
train loss:  0.43440887331962585
train gradient:  0.33750316891279974
iteration : 4503
train acc:  0.859375
train loss:  0.36128750443458557
train gradient:  0.2841465264413504
iteration : 4504
train acc:  0.875
train loss:  0.362231969833374
train gradient:  0.2780085331877839
iteration : 4505
train acc:  0.890625
train loss:  0.3268989026546478
train gradient:  0.2301870755125514
iteration : 4506
train acc:  0.8515625
train loss:  0.3763197660446167
train gradient:  0.33688885970691573
iteration : 4507
train acc:  0.7578125
train loss:  0.44998347759246826
train gradient:  0.43964881081220103
iteration : 4508
train acc:  0.8671875
train loss:  0.33398041129112244
train gradient:  0.2413374445098591
iteration : 4509
train acc:  0.84375
train loss:  0.3239437937736511
train gradient:  0.1832110589242833
iteration : 4510
train acc:  0.84375
train loss:  0.38560789823532104
train gradient:  0.33782368763883563
iteration : 4511
train acc:  0.875
train loss:  0.29136425256729126
train gradient:  0.21470551362927234
iteration : 4512
train acc:  0.8515625
train loss:  0.3243981599807739
train gradient:  0.19739475457343125
iteration : 4513
train acc:  0.8125
train loss:  0.3669237494468689
train gradient:  0.3406938985918515
iteration : 4514
train acc:  0.8515625
train loss:  0.30689316987991333
train gradient:  0.18335118390690353
iteration : 4515
train acc:  0.84375
train loss:  0.3707374334335327
train gradient:  0.20755594869634997
iteration : 4516
train acc:  0.8359375
train loss:  0.39882177114486694
train gradient:  0.2958378285347951
iteration : 4517
train acc:  0.78125
train loss:  0.488694429397583
train gradient:  0.4324360426212881
iteration : 4518
train acc:  0.8671875
train loss:  0.3508155345916748
train gradient:  0.25574313082989486
iteration : 4519
train acc:  0.8984375
train loss:  0.28749191761016846
train gradient:  0.23460748103994286
iteration : 4520
train acc:  0.765625
train loss:  0.4568922221660614
train gradient:  0.37717243257345845
iteration : 4521
train acc:  0.8828125
train loss:  0.34132933616638184
train gradient:  0.26602231252764524
iteration : 4522
train acc:  0.890625
train loss:  0.31672707200050354
train gradient:  0.18475892362582547
iteration : 4523
train acc:  0.828125
train loss:  0.3505684435367584
train gradient:  0.24348136915713017
iteration : 4524
train acc:  0.7890625
train loss:  0.5076041221618652
train gradient:  0.35578962278674287
iteration : 4525
train acc:  0.859375
train loss:  0.34224873781204224
train gradient:  0.21504393642883735
iteration : 4526
train acc:  0.796875
train loss:  0.3896324634552002
train gradient:  0.21549233803879464
iteration : 4527
train acc:  0.8984375
train loss:  0.27406302094459534
train gradient:  0.3966228668697472
iteration : 4528
train acc:  0.8125
train loss:  0.44619956612586975
train gradient:  0.3575431025767778
iteration : 4529
train acc:  0.8046875
train loss:  0.4404398500919342
train gradient:  0.3819579942877685
iteration : 4530
train acc:  0.890625
train loss:  0.26790666580200195
train gradient:  0.22319374187489244
iteration : 4531
train acc:  0.84375
train loss:  0.3735576868057251
train gradient:  0.3210241004626955
iteration : 4532
train acc:  0.796875
train loss:  0.41795289516448975
train gradient:  0.3158734733686468
iteration : 4533
train acc:  0.8203125
train loss:  0.3841557204723358
train gradient:  0.28175535418635095
iteration : 4534
train acc:  0.8125
train loss:  0.35486114025115967
train gradient:  0.21776145601408464
iteration : 4535
train acc:  0.875
train loss:  0.3318653702735901
train gradient:  0.4015622871960361
iteration : 4536
train acc:  0.8515625
train loss:  0.37308141589164734
train gradient:  0.25208835801871965
iteration : 4537
train acc:  0.890625
train loss:  0.28312474489212036
train gradient:  0.16524049197725632
iteration : 4538
train acc:  0.828125
train loss:  0.40087413787841797
train gradient:  0.43288020174012554
iteration : 4539
train acc:  0.8671875
train loss:  0.35366135835647583
train gradient:  0.25892253859108144
iteration : 4540
train acc:  0.84375
train loss:  0.385285884141922
train gradient:  0.4626394004387834
iteration : 4541
train acc:  0.7890625
train loss:  0.4174770414829254
train gradient:  0.3965481708490263
iteration : 4542
train acc:  0.7734375
train loss:  0.3732072114944458
train gradient:  0.2736640167010325
iteration : 4543
train acc:  0.9140625
train loss:  0.274056613445282
train gradient:  0.2660972596938386
iteration : 4544
train acc:  0.828125
train loss:  0.3621823787689209
train gradient:  0.21086657431111705
iteration : 4545
train acc:  0.7734375
train loss:  0.44868600368499756
train gradient:  0.5155063396692352
iteration : 4546
train acc:  0.8671875
train loss:  0.39888158440589905
train gradient:  0.2443085887365105
iteration : 4547
train acc:  0.84375
train loss:  0.37412986159324646
train gradient:  0.2169153598066252
iteration : 4548
train acc:  0.7734375
train loss:  0.4149004817008972
train gradient:  0.3855300537362729
iteration : 4549
train acc:  0.78125
train loss:  0.41622084379196167
train gradient:  0.3238395794601113
iteration : 4550
train acc:  0.8515625
train loss:  0.3425171673297882
train gradient:  0.21661957088031375
iteration : 4551
train acc:  0.890625
train loss:  0.3130813539028168
train gradient:  0.19995159220550943
iteration : 4552
train acc:  0.859375
train loss:  0.3524972200393677
train gradient:  0.17623114738647164
iteration : 4553
train acc:  0.8828125
train loss:  0.29917094111442566
train gradient:  0.21575011109646164
iteration : 4554
train acc:  0.8203125
train loss:  0.3523385524749756
train gradient:  0.39746933391303024
iteration : 4555
train acc:  0.8828125
train loss:  0.3068384528160095
train gradient:  0.17991130426010152
iteration : 4556
train acc:  0.8046875
train loss:  0.4122995138168335
train gradient:  0.32010182544424615
iteration : 4557
train acc:  0.828125
train loss:  0.4112759530544281
train gradient:  0.29685595825604566
iteration : 4558
train acc:  0.8671875
train loss:  0.3583155870437622
train gradient:  0.22692691412259647
iteration : 4559
train acc:  0.8984375
train loss:  0.30574798583984375
train gradient:  0.18705195754313572
iteration : 4560
train acc:  0.859375
train loss:  0.32073521614074707
train gradient:  0.16239597121620664
iteration : 4561
train acc:  0.8671875
train loss:  0.31923070549964905
train gradient:  0.2528801621935809
iteration : 4562
train acc:  0.8671875
train loss:  0.3405371904373169
train gradient:  0.2706430566586362
iteration : 4563
train acc:  0.8515625
train loss:  0.30965709686279297
train gradient:  0.19773404446895393
iteration : 4564
train acc:  0.859375
train loss:  0.3317158818244934
train gradient:  0.18645380028563888
iteration : 4565
train acc:  0.8984375
train loss:  0.28906798362731934
train gradient:  0.15602609257959787
iteration : 4566
train acc:  0.859375
train loss:  0.33359092473983765
train gradient:  0.262292767814191
iteration : 4567
train acc:  0.859375
train loss:  0.3334921896457672
train gradient:  0.3605252937869386
iteration : 4568
train acc:  0.8671875
train loss:  0.3154253363609314
train gradient:  0.1747595895511023
iteration : 4569
train acc:  0.8125
train loss:  0.4312080442905426
train gradient:  0.3674621479728402
iteration : 4570
train acc:  0.796875
train loss:  0.3928917646408081
train gradient:  0.33368958184703895
iteration : 4571
train acc:  0.8125
train loss:  0.3433491587638855
train gradient:  0.23910347680831173
iteration : 4572
train acc:  0.7890625
train loss:  0.5171071887016296
train gradient:  0.36418254944065026
iteration : 4573
train acc:  0.8515625
train loss:  0.35167473554611206
train gradient:  0.22919641256978485
iteration : 4574
train acc:  0.8828125
train loss:  0.3028264343738556
train gradient:  0.2095463543677076
iteration : 4575
train acc:  0.875
train loss:  0.29713746905326843
train gradient:  0.269024695231024
iteration : 4576
train acc:  0.8125
train loss:  0.3640097975730896
train gradient:  0.310271782949008
iteration : 4577
train acc:  0.859375
train loss:  0.3423668146133423
train gradient:  0.25111783787457737
iteration : 4578
train acc:  0.8828125
train loss:  0.2804049253463745
train gradient:  0.24307562404060212
iteration : 4579
train acc:  0.8671875
train loss:  0.35110998153686523
train gradient:  0.2970763621245461
iteration : 4580
train acc:  0.8671875
train loss:  0.3240790069103241
train gradient:  0.19057525434948452
iteration : 4581
train acc:  0.78125
train loss:  0.46029022336006165
train gradient:  0.4586245138739092
iteration : 4582
train acc:  0.8359375
train loss:  0.33946335315704346
train gradient:  0.3025611963758985
iteration : 4583
train acc:  0.8203125
train loss:  0.3773542046546936
train gradient:  0.26683096585235244
iteration : 4584
train acc:  0.8359375
train loss:  0.34342020750045776
train gradient:  0.27465303295412663
iteration : 4585
train acc:  0.8984375
train loss:  0.2922915816307068
train gradient:  0.268752492663604
iteration : 4586
train acc:  0.84375
train loss:  0.3421192765235901
train gradient:  0.24109576698646545
iteration : 4587
train acc:  0.8125
train loss:  0.35273927450180054
train gradient:  0.3224297837957555
iteration : 4588
train acc:  0.8046875
train loss:  0.3778977394104004
train gradient:  0.3346205266681406
iteration : 4589
train acc:  0.8671875
train loss:  0.33467331528663635
train gradient:  0.28181078127149456
iteration : 4590
train acc:  0.8359375
train loss:  0.3315926790237427
train gradient:  0.23632528113914658
iteration : 4591
train acc:  0.890625
train loss:  0.2814738154411316
train gradient:  0.23485996312960455
iteration : 4592
train acc:  0.828125
train loss:  0.3705134987831116
train gradient:  0.2364047495559379
iteration : 4593
train acc:  0.8046875
train loss:  0.40229976177215576
train gradient:  0.27343070359219074
iteration : 4594
train acc:  0.8046875
train loss:  0.4342634081840515
train gradient:  0.40274773732251884
iteration : 4595
train acc:  0.828125
train loss:  0.3764251470565796
train gradient:  0.4455228993843957
iteration : 4596
train acc:  0.859375
train loss:  0.325160950422287
train gradient:  0.20667683673625997
iteration : 4597
train acc:  0.875
train loss:  0.3262227475643158
train gradient:  0.29405474741743426
iteration : 4598
train acc:  0.8359375
train loss:  0.37790119647979736
train gradient:  0.33604506531337125
iteration : 4599
train acc:  0.859375
train loss:  0.38638830184936523
train gradient:  0.2726361472850776
iteration : 4600
train acc:  0.859375
train loss:  0.33039677143096924
train gradient:  0.1972890185914454
iteration : 4601
train acc:  0.875
train loss:  0.31504377722740173
train gradient:  0.21656013993601914
iteration : 4602
train acc:  0.8984375
train loss:  0.27904367446899414
train gradient:  0.1863902121291672
iteration : 4603
train acc:  0.84375
train loss:  0.3638416826725006
train gradient:  0.371402083379091
iteration : 4604
train acc:  0.8359375
train loss:  0.3534715175628662
train gradient:  0.5151552224178975
iteration : 4605
train acc:  0.84375
train loss:  0.333028107881546
train gradient:  0.295244938522635
iteration : 4606
train acc:  0.8203125
train loss:  0.4051937460899353
train gradient:  0.3249897873091829
iteration : 4607
train acc:  0.8828125
train loss:  0.3152470290660858
train gradient:  0.2313032862892901
iteration : 4608
train acc:  0.796875
train loss:  0.3924391269683838
train gradient:  0.3330148892207476
iteration : 4609
train acc:  0.8359375
train loss:  0.3333485722541809
train gradient:  0.20995377055595227
iteration : 4610
train acc:  0.8046875
train loss:  0.409233421087265
train gradient:  0.29817764912485856
iteration : 4611
train acc:  0.8359375
train loss:  0.36013856530189514
train gradient:  0.25758158611154175
iteration : 4612
train acc:  0.8125
train loss:  0.44082486629486084
train gradient:  0.4142590016478291
iteration : 4613
train acc:  0.8046875
train loss:  0.38953620195388794
train gradient:  0.3502819504125218
iteration : 4614
train acc:  0.859375
train loss:  0.369861364364624
train gradient:  0.42502545156435323
iteration : 4615
train acc:  0.8515625
train loss:  0.3275342285633087
train gradient:  0.35991368382097144
iteration : 4616
train acc:  0.84375
train loss:  0.29869168996810913
train gradient:  0.25460210586479187
iteration : 4617
train acc:  0.765625
train loss:  0.4572218358516693
train gradient:  0.594064681037008
iteration : 4618
train acc:  0.7890625
train loss:  0.39464253187179565
train gradient:  0.3708396664791104
iteration : 4619
train acc:  0.8828125
train loss:  0.29548317193984985
train gradient:  0.2497439586203794
iteration : 4620
train acc:  0.8359375
train loss:  0.3597069978713989
train gradient:  0.41212933491876497
iteration : 4621
train acc:  0.8671875
train loss:  0.30789536237716675
train gradient:  0.19938814006019845
iteration : 4622
train acc:  0.859375
train loss:  0.32082492113113403
train gradient:  0.251188424474049
iteration : 4623
train acc:  0.8984375
train loss:  0.26128676533699036
train gradient:  0.1777497030886147
iteration : 4624
train acc:  0.8671875
train loss:  0.31426802277565
train gradient:  0.2125790098221953
iteration : 4625
train acc:  0.859375
train loss:  0.29808148741722107
train gradient:  0.23863270974166123
iteration : 4626
train acc:  0.875
train loss:  0.34572625160217285
train gradient:  0.25121906341338884
iteration : 4627
train acc:  0.828125
train loss:  0.35630306601524353
train gradient:  0.22352103075179508
iteration : 4628
train acc:  0.8359375
train loss:  0.3423328995704651
train gradient:  0.2631989555785999
iteration : 4629
train acc:  0.859375
train loss:  0.3778424859046936
train gradient:  0.2901070753840865
iteration : 4630
train acc:  0.8359375
train loss:  0.42414188385009766
train gradient:  0.2690001080731942
iteration : 4631
train acc:  0.8359375
train loss:  0.3623083829879761
train gradient:  0.23813521330747542
iteration : 4632
train acc:  0.765625
train loss:  0.47532057762145996
train gradient:  0.35280650626475557
iteration : 4633
train acc:  0.9296875
train loss:  0.24812322854995728
train gradient:  0.13063942252757021
iteration : 4634
train acc:  0.8671875
train loss:  0.3276439905166626
train gradient:  0.18233698391238518
iteration : 4635
train acc:  0.796875
train loss:  0.4169083833694458
train gradient:  0.26728075169932963
iteration : 4636
train acc:  0.8046875
train loss:  0.4028646647930145
train gradient:  0.4328131127401554
iteration : 4637
train acc:  0.7578125
train loss:  0.5806760787963867
train gradient:  0.8404478087036179
iteration : 4638
train acc:  0.8828125
train loss:  0.27603691816329956
train gradient:  0.14919644699830747
iteration : 4639
train acc:  0.84375
train loss:  0.31238892674446106
train gradient:  0.18865005078850847
iteration : 4640
train acc:  0.8359375
train loss:  0.38847848773002625
train gradient:  0.3545479187241456
iteration : 4641
train acc:  0.859375
train loss:  0.35032135248184204
train gradient:  0.25065560720392716
iteration : 4642
train acc:  0.8046875
train loss:  0.3555362820625305
train gradient:  0.462872860323988
iteration : 4643
train acc:  0.875
train loss:  0.297477126121521
train gradient:  0.18692197846549194
iteration : 4644
train acc:  0.828125
train loss:  0.3819495439529419
train gradient:  0.2819674407323268
iteration : 4645
train acc:  0.796875
train loss:  0.38729947805404663
train gradient:  0.2841501209304669
iteration : 4646
train acc:  0.859375
train loss:  0.3300934433937073
train gradient:  0.2119626954035887
iteration : 4647
train acc:  0.8671875
train loss:  0.3408055305480957
train gradient:  0.19305756157166087
iteration : 4648
train acc:  0.828125
train loss:  0.42414581775665283
train gradient:  0.36493741508197186
iteration : 4649
train acc:  0.84375
train loss:  0.3657352924346924
train gradient:  0.2145699566518843
iteration : 4650
train acc:  0.8359375
train loss:  0.40134066343307495
train gradient:  0.24990594342156577
iteration : 4651
train acc:  0.8203125
train loss:  0.4088374972343445
train gradient:  0.29316915762408646
iteration : 4652
train acc:  0.859375
train loss:  0.31017762422561646
train gradient:  0.19825833195322054
iteration : 4653
train acc:  0.890625
train loss:  0.3029094934463501
train gradient:  0.1886202542374023
iteration : 4654
train acc:  0.8359375
train loss:  0.41109499335289
train gradient:  0.2599404695240011
iteration : 4655
train acc:  0.859375
train loss:  0.33263319730758667
train gradient:  0.2102865263330102
iteration : 4656
train acc:  0.8828125
train loss:  0.32449644804000854
train gradient:  0.3004103330164753
iteration : 4657
train acc:  0.8125
train loss:  0.4218287467956543
train gradient:  0.3239395738259211
iteration : 4658
train acc:  0.8203125
train loss:  0.3893095850944519
train gradient:  0.2601712973923079
iteration : 4659
train acc:  0.8828125
train loss:  0.3143516182899475
train gradient:  0.20936376084688338
iteration : 4660
train acc:  0.8671875
train loss:  0.28636521100997925
train gradient:  0.2346311558911694
iteration : 4661
train acc:  0.84375
train loss:  0.3964041471481323
train gradient:  0.2678235961628951
iteration : 4662
train acc:  0.8203125
train loss:  0.3783355951309204
train gradient:  0.2574647732908808
iteration : 4663
train acc:  0.8203125
train loss:  0.4272228479385376
train gradient:  0.39577601362218434
iteration : 4664
train acc:  0.8359375
train loss:  0.37877896428108215
train gradient:  0.3872430265081086
iteration : 4665
train acc:  0.875
train loss:  0.34752020239830017
train gradient:  0.2496268808224974
iteration : 4666
train acc:  0.84375
train loss:  0.32964250445365906
train gradient:  0.21778324095051632
iteration : 4667
train acc:  0.859375
train loss:  0.3009665310382843
train gradient:  0.20203464479753896
iteration : 4668
train acc:  0.8203125
train loss:  0.39499449729919434
train gradient:  0.2763471702978874
iteration : 4669
train acc:  0.8671875
train loss:  0.39283210039138794
train gradient:  0.30450716791226673
iteration : 4670
train acc:  0.8671875
train loss:  0.30203235149383545
train gradient:  0.203990236632963
iteration : 4671
train acc:  0.890625
train loss:  0.288985937833786
train gradient:  0.15779823581849106
iteration : 4672
train acc:  0.8515625
train loss:  0.334076464176178
train gradient:  0.24305290181860723
iteration : 4673
train acc:  0.828125
train loss:  0.3221163749694824
train gradient:  0.2424687394522561
iteration : 4674
train acc:  0.84375
train loss:  0.3970082998275757
train gradient:  0.2978325591125294
iteration : 4675
train acc:  0.875
train loss:  0.3230750262737274
train gradient:  0.26668166147841527
iteration : 4676
train acc:  0.828125
train loss:  0.4126475155353546
train gradient:  0.3219400105501521
iteration : 4677
train acc:  0.796875
train loss:  0.3761460483074188
train gradient:  0.37518537239798666
iteration : 4678
train acc:  0.859375
train loss:  0.3590206801891327
train gradient:  0.2341211679112954
iteration : 4679
train acc:  0.8359375
train loss:  0.3626416325569153
train gradient:  0.2004224366032376
iteration : 4680
train acc:  0.828125
train loss:  0.38661932945251465
train gradient:  0.3643452970752995
iteration : 4681
train acc:  0.8203125
train loss:  0.38320738077163696
train gradient:  0.31219583671359485
iteration : 4682
train acc:  0.8203125
train loss:  0.4148172438144684
train gradient:  0.3270778643429675
iteration : 4683
train acc:  0.84375
train loss:  0.32627665996551514
train gradient:  0.27468260943569195
iteration : 4684
train acc:  0.8671875
train loss:  0.321380615234375
train gradient:  0.16464894999602625
iteration : 4685
train acc:  0.8828125
train loss:  0.2942199409008026
train gradient:  0.1807922735151995
iteration : 4686
train acc:  0.859375
train loss:  0.319804310798645
train gradient:  0.2394326702823122
iteration : 4687
train acc:  0.8203125
train loss:  0.41103100776672363
train gradient:  0.27443258315839186
iteration : 4688
train acc:  0.828125
train loss:  0.38833051919937134
train gradient:  0.27518856713293527
iteration : 4689
train acc:  0.765625
train loss:  0.5025477409362793
train gradient:  0.6117929645598179
iteration : 4690
train acc:  0.8515625
train loss:  0.35823068022727966
train gradient:  0.4367174112998841
iteration : 4691
train acc:  0.828125
train loss:  0.4008585810661316
train gradient:  0.2048941154977387
iteration : 4692
train acc:  0.8671875
train loss:  0.29515403509140015
train gradient:  0.22338420072274828
iteration : 4693
train acc:  0.8828125
train loss:  0.3247983753681183
train gradient:  0.16645636113644405
iteration : 4694
train acc:  0.796875
train loss:  0.3906763195991516
train gradient:  0.4051955428183863
iteration : 4695
train acc:  0.859375
train loss:  0.3432774841785431
train gradient:  0.2959010208797295
iteration : 4696
train acc:  0.8671875
train loss:  0.2817581295967102
train gradient:  0.2432055413612655
iteration : 4697
train acc:  0.8515625
train loss:  0.31116384267807007
train gradient:  0.25215811827894746
iteration : 4698
train acc:  0.84375
train loss:  0.316824346780777
train gradient:  0.24817345898658655
iteration : 4699
train acc:  0.8359375
train loss:  0.35433468222618103
train gradient:  0.2396783288657509
iteration : 4700
train acc:  0.8125
train loss:  0.45043396949768066
train gradient:  0.35026351481699924
iteration : 4701
train acc:  0.859375
train loss:  0.3305012583732605
train gradient:  0.24831309420036138
iteration : 4702
train acc:  0.890625
train loss:  0.2867683470249176
train gradient:  0.16341721647325202
iteration : 4703
train acc:  0.859375
train loss:  0.30511218309402466
train gradient:  0.18235911614052613
iteration : 4704
train acc:  0.828125
train loss:  0.36141523718833923
train gradient:  0.21102940880801696
iteration : 4705
train acc:  0.8984375
train loss:  0.3074589967727661
train gradient:  0.26768679779569504
iteration : 4706
train acc:  0.84375
train loss:  0.33750787377357483
train gradient:  0.27860562330048033
iteration : 4707
train acc:  0.796875
train loss:  0.39680156111717224
train gradient:  0.30162500043709695
iteration : 4708
train acc:  0.828125
train loss:  0.36897972226142883
train gradient:  0.717711355354426
iteration : 4709
train acc:  0.8125
train loss:  0.3234461843967438
train gradient:  0.22279607282943603
iteration : 4710
train acc:  0.890625
train loss:  0.2708996534347534
train gradient:  0.13420648108180433
iteration : 4711
train acc:  0.796875
train loss:  0.4007272720336914
train gradient:  0.2704457904965416
iteration : 4712
train acc:  0.859375
train loss:  0.34813472628593445
train gradient:  0.2816993198168174
iteration : 4713
train acc:  0.8828125
train loss:  0.288329154253006
train gradient:  0.17900960594392382
iteration : 4714
train acc:  0.859375
train loss:  0.35660862922668457
train gradient:  0.3044748924888125
iteration : 4715
train acc:  0.8203125
train loss:  0.36232298612594604
train gradient:  0.36756402298377977
iteration : 4716
train acc:  0.8203125
train loss:  0.3872913122177124
train gradient:  0.45378308251702565
iteration : 4717
train acc:  0.8828125
train loss:  0.31133782863616943
train gradient:  0.2600203779968031
iteration : 4718
train acc:  0.828125
train loss:  0.42566582560539246
train gradient:  0.3438190358439551
iteration : 4719
train acc:  0.8671875
train loss:  0.4111974835395813
train gradient:  0.36413149427710917
iteration : 4720
train acc:  0.7890625
train loss:  0.40539979934692383
train gradient:  0.28714940457592913
iteration : 4721
train acc:  0.859375
train loss:  0.3091890215873718
train gradient:  0.2113286935367034
iteration : 4722
train acc:  0.875
train loss:  0.2917771637439728
train gradient:  0.24122421203569505
iteration : 4723
train acc:  0.8828125
train loss:  0.3366985321044922
train gradient:  0.23932316009402707
iteration : 4724
train acc:  0.8515625
train loss:  0.3328624367713928
train gradient:  0.41166547165737577
iteration : 4725
train acc:  0.84375
train loss:  0.3328759968280792
train gradient:  0.2744698362258492
iteration : 4726
train acc:  0.890625
train loss:  0.2974478006362915
train gradient:  0.3137202511467057
iteration : 4727
train acc:  0.796875
train loss:  0.3727286458015442
train gradient:  0.447288883989392
iteration : 4728
train acc:  0.8671875
train loss:  0.30222058296203613
train gradient:  0.3379757970542559
iteration : 4729
train acc:  0.828125
train loss:  0.3954335153102875
train gradient:  0.3685514963718031
iteration : 4730
train acc:  0.90625
train loss:  0.3245863616466522
train gradient:  0.32767847641905956
iteration : 4731
train acc:  0.828125
train loss:  0.3742266297340393
train gradient:  0.327839934389337
iteration : 4732
train acc:  0.8671875
train loss:  0.3482576012611389
train gradient:  0.24824525134473102
iteration : 4733
train acc:  0.8515625
train loss:  0.37716084718704224
train gradient:  0.30640224804534427
iteration : 4734
train acc:  0.84375
train loss:  0.3639228940010071
train gradient:  0.24782092966669886
iteration : 4735
train acc:  0.8046875
train loss:  0.38277995586395264
train gradient:  0.21826240798564078
iteration : 4736
train acc:  0.84375
train loss:  0.3317541182041168
train gradient:  0.32792397950807295
iteration : 4737
train acc:  0.90625
train loss:  0.25906652212142944
train gradient:  0.17624237140043225
iteration : 4738
train acc:  0.859375
train loss:  0.35907143354415894
train gradient:  0.2812542920641326
iteration : 4739
train acc:  0.828125
train loss:  0.3723163902759552
train gradient:  0.3492976160965434
iteration : 4740
train acc:  0.84375
train loss:  0.3563756048679352
train gradient:  0.1778975136769601
iteration : 4741
train acc:  0.875
train loss:  0.3286151587963104
train gradient:  0.28191604641400236
iteration : 4742
train acc:  0.8671875
train loss:  0.3629300892353058
train gradient:  0.25326981293555173
iteration : 4743
train acc:  0.8359375
train loss:  0.4108820855617523
train gradient:  0.4327319460382883
iteration : 4744
train acc:  0.8046875
train loss:  0.42641884088516235
train gradient:  0.38138531418361715
iteration : 4745
train acc:  0.8125
train loss:  0.40072351694107056
train gradient:  0.3813762394505445
iteration : 4746
train acc:  0.84375
train loss:  0.4140546917915344
train gradient:  0.390126750093005
iteration : 4747
train acc:  0.84375
train loss:  0.404388427734375
train gradient:  0.30755763198112057
iteration : 4748
train acc:  0.8359375
train loss:  0.39148950576782227
train gradient:  0.42242644037895877
iteration : 4749
train acc:  0.796875
train loss:  0.4451906979084015
train gradient:  0.5025852229697676
iteration : 4750
train acc:  0.8203125
train loss:  0.4717671871185303
train gradient:  0.3464644628179793
iteration : 4751
train acc:  0.859375
train loss:  0.29712891578674316
train gradient:  0.22049227930409615
iteration : 4752
train acc:  0.84375
train loss:  0.4569739103317261
train gradient:  0.371574512561569
iteration : 4753
train acc:  0.8984375
train loss:  0.2913183867931366
train gradient:  0.20486197891954538
iteration : 4754
train acc:  0.7890625
train loss:  0.3940209746360779
train gradient:  0.21649855626828937
iteration : 4755
train acc:  0.8671875
train loss:  0.2959510385990143
train gradient:  0.19058894931606937
iteration : 4756
train acc:  0.7890625
train loss:  0.39336684346199036
train gradient:  0.3084569653920324
iteration : 4757
train acc:  0.7734375
train loss:  0.4230250120162964
train gradient:  0.36191829538050346
iteration : 4758
train acc:  0.859375
train loss:  0.31656885147094727
train gradient:  0.4937174989956077
iteration : 4759
train acc:  0.84375
train loss:  0.36881130933761597
train gradient:  0.28488023252410266
iteration : 4760
train acc:  0.84375
train loss:  0.3505271375179291
train gradient:  0.299001627323089
iteration : 4761
train acc:  0.84375
train loss:  0.31663277745246887
train gradient:  0.258415065603624
iteration : 4762
train acc:  0.828125
train loss:  0.3579031825065613
train gradient:  0.3187190610153327
iteration : 4763
train acc:  0.8671875
train loss:  0.3226065933704376
train gradient:  0.23389018537970646
iteration : 4764
train acc:  0.8046875
train loss:  0.35648906230926514
train gradient:  0.25227653614855716
iteration : 4765
train acc:  0.8984375
train loss:  0.2997420132160187
train gradient:  0.24229985873977075
iteration : 4766
train acc:  0.8359375
train loss:  0.37551984190940857
train gradient:  0.32635630240540037
iteration : 4767
train acc:  0.8671875
train loss:  0.279462993144989
train gradient:  0.15243563253672587
iteration : 4768
train acc:  0.875
train loss:  0.3030574321746826
train gradient:  0.17740588194996063
iteration : 4769
train acc:  0.859375
train loss:  0.3547373116016388
train gradient:  0.35923211492581325
iteration : 4770
train acc:  0.8671875
train loss:  0.27250194549560547
train gradient:  0.20897613451807698
iteration : 4771
train acc:  0.8046875
train loss:  0.42056894302368164
train gradient:  0.30960312654824085
iteration : 4772
train acc:  0.875
train loss:  0.31902939081192017
train gradient:  0.28167575878388423
iteration : 4773
train acc:  0.8671875
train loss:  0.27210479974746704
train gradient:  0.232599485983915
iteration : 4774
train acc:  0.8359375
train loss:  0.376430481672287
train gradient:  0.26454202075180483
iteration : 4775
train acc:  0.828125
train loss:  0.39075565338134766
train gradient:  0.43645472763638155
iteration : 4776
train acc:  0.8046875
train loss:  0.42702949047088623
train gradient:  0.5416042634336414
iteration : 4777
train acc:  0.84375
train loss:  0.3321780562400818
train gradient:  0.3086772820107216
iteration : 4778
train acc:  0.875
train loss:  0.2753360867500305
train gradient:  0.4493699154061692
iteration : 4779
train acc:  0.859375
train loss:  0.3675623834133148
train gradient:  0.2092000035103259
iteration : 4780
train acc:  0.8671875
train loss:  0.30533355474472046
train gradient:  0.20784091078805694
iteration : 4781
train acc:  0.84375
train loss:  0.3490997850894928
train gradient:  0.35261818683128926
iteration : 4782
train acc:  0.875
train loss:  0.3233259916305542
train gradient:  0.3245719521427516
iteration : 4783
train acc:  0.8359375
train loss:  0.3248485326766968
train gradient:  0.3339781235105019
iteration : 4784
train acc:  0.84375
train loss:  0.3822789788246155
train gradient:  0.30451023509309344
iteration : 4785
train acc:  0.875
train loss:  0.30154451727867126
train gradient:  0.20247423957986146
iteration : 4786
train acc:  0.875
train loss:  0.31833815574645996
train gradient:  0.22214475144914678
iteration : 4787
train acc:  0.859375
train loss:  0.3526645302772522
train gradient:  0.30434064105649633
iteration : 4788
train acc:  0.84375
train loss:  0.35939085483551025
train gradient:  0.2629698016619401
iteration : 4789
train acc:  0.8515625
train loss:  0.3087790310382843
train gradient:  0.2459984084353708
iteration : 4790
train acc:  0.828125
train loss:  0.4172547161579132
train gradient:  0.2984993078275914
iteration : 4791
train acc:  0.8515625
train loss:  0.3829655051231384
train gradient:  0.3472619023767083
iteration : 4792
train acc:  0.8515625
train loss:  0.42636001110076904
train gradient:  0.4374716699927966
iteration : 4793
train acc:  0.8828125
train loss:  0.33344370126724243
train gradient:  0.2662072246329613
iteration : 4794
train acc:  0.828125
train loss:  0.3426319360733032
train gradient:  0.3057222257737829
iteration : 4795
train acc:  0.84375
train loss:  0.37531009316444397
train gradient:  0.3237507114556581
iteration : 4796
train acc:  0.8359375
train loss:  0.40569576621055603
train gradient:  0.38951132226409596
iteration : 4797
train acc:  0.875
train loss:  0.2818126678466797
train gradient:  0.28407631593201427
iteration : 4798
train acc:  0.8203125
train loss:  0.35986223816871643
train gradient:  0.43512119202389893
iteration : 4799
train acc:  0.859375
train loss:  0.3477579355239868
train gradient:  0.2996398985644419
iteration : 4800
train acc:  0.8359375
train loss:  0.3324548602104187
train gradient:  0.3098652859684426
iteration : 4801
train acc:  0.8984375
train loss:  0.29087239503860474
train gradient:  0.2628533337653762
iteration : 4802
train acc:  0.8203125
train loss:  0.3872206211090088
train gradient:  0.44860864778888065
iteration : 4803
train acc:  0.765625
train loss:  0.453898161649704
train gradient:  0.3203270131891993
iteration : 4804
train acc:  0.8203125
train loss:  0.40522873401641846
train gradient:  0.37397336066990106
iteration : 4805
train acc:  0.8046875
train loss:  0.3851289749145508
train gradient:  0.30271538113065943
iteration : 4806
train acc:  0.875
train loss:  0.3306822180747986
train gradient:  0.3734236337327319
iteration : 4807
train acc:  0.8515625
train loss:  0.3935962915420532
train gradient:  0.2600417016903045
iteration : 4808
train acc:  0.859375
train loss:  0.31392669677734375
train gradient:  0.2170483578788296
iteration : 4809
train acc:  0.9140625
train loss:  0.27972477674484253
train gradient:  0.187894304628244
iteration : 4810
train acc:  0.859375
train loss:  0.32530367374420166
train gradient:  0.2692668943147674
iteration : 4811
train acc:  0.84375
train loss:  0.3052510917186737
train gradient:  0.2590692298724523
iteration : 4812
train acc:  0.796875
train loss:  0.403997004032135
train gradient:  0.2767731400702028
iteration : 4813
train acc:  0.8046875
train loss:  0.43630605936050415
train gradient:  0.2622299662323231
iteration : 4814
train acc:  0.8046875
train loss:  0.4311832785606384
train gradient:  0.40502224376568385
iteration : 4815
train acc:  0.8203125
train loss:  0.2895994782447815
train gradient:  0.17847200500065558
iteration : 4816
train acc:  0.875
train loss:  0.27508509159088135
train gradient:  0.20826977585383075
iteration : 4817
train acc:  0.859375
train loss:  0.4143837094306946
train gradient:  0.3299754359619347
iteration : 4818
train acc:  0.8671875
train loss:  0.29046830534935
train gradient:  0.38984358318089746
iteration : 4819
train acc:  0.875
train loss:  0.30734866857528687
train gradient:  0.24848377340783823
iteration : 4820
train acc:  0.8515625
train loss:  0.35796982049942017
train gradient:  0.24944794782114313
iteration : 4821
train acc:  0.84375
train loss:  0.2922568917274475
train gradient:  0.19391621463743103
iteration : 4822
train acc:  0.859375
train loss:  0.363347589969635
train gradient:  0.2905257527970731
iteration : 4823
train acc:  0.8515625
train loss:  0.36352062225341797
train gradient:  0.360894288960282
iteration : 4824
train acc:  0.78125
train loss:  0.4173036813735962
train gradient:  0.37916950162555163
iteration : 4825
train acc:  0.8125
train loss:  0.38745027780532837
train gradient:  0.3040838607985237
iteration : 4826
train acc:  0.8671875
train loss:  0.3228565454483032
train gradient:  0.19701647555968793
iteration : 4827
train acc:  0.8203125
train loss:  0.37035542726516724
train gradient:  0.3817378944878446
iteration : 4828
train acc:  0.8359375
train loss:  0.3546265661716461
train gradient:  0.2523724257778443
iteration : 4829
train acc:  0.84375
train loss:  0.373329758644104
train gradient:  0.29248142168768465
iteration : 4830
train acc:  0.828125
train loss:  0.3801262080669403
train gradient:  0.27418774523724926
iteration : 4831
train acc:  0.8203125
train loss:  0.4103298783302307
train gradient:  0.3200963838418522
iteration : 4832
train acc:  0.8203125
train loss:  0.3801328241825104
train gradient:  0.4606272681782801
iteration : 4833
train acc:  0.84375
train loss:  0.3489696979522705
train gradient:  0.3593450569546772
iteration : 4834
train acc:  0.7734375
train loss:  0.43171289563179016
train gradient:  0.3457571874942611
iteration : 4835
train acc:  0.8515625
train loss:  0.3825034499168396
train gradient:  0.2954236126050124
iteration : 4836
train acc:  0.828125
train loss:  0.3397330045700073
train gradient:  0.23239316317348524
iteration : 4837
train acc:  0.8828125
train loss:  0.3255021572113037
train gradient:  0.2422436660217399
iteration : 4838
train acc:  0.84375
train loss:  0.37711000442504883
train gradient:  0.2551722847877287
iteration : 4839
train acc:  0.8125
train loss:  0.38366973400115967
train gradient:  0.33705858501507174
iteration : 4840
train acc:  0.921875
train loss:  0.22205853462219238
train gradient:  0.11947044838288552
iteration : 4841
train acc:  0.8359375
train loss:  0.35674184560775757
train gradient:  0.348626784479128
iteration : 4842
train acc:  0.828125
train loss:  0.3525621294975281
train gradient:  0.3231729950522812
iteration : 4843
train acc:  0.8359375
train loss:  0.42039647698402405
train gradient:  0.2990463887459984
iteration : 4844
train acc:  0.859375
train loss:  0.31680941581726074
train gradient:  0.13939788433432357
iteration : 4845
train acc:  0.7890625
train loss:  0.46993765234947205
train gradient:  0.3106239885818657
iteration : 4846
train acc:  0.859375
train loss:  0.32178300619125366
train gradient:  0.17950663351624344
iteration : 4847
train acc:  0.84375
train loss:  0.3849354386329651
train gradient:  0.23636726754031157
iteration : 4848
train acc:  0.859375
train loss:  0.3634849190711975
train gradient:  0.2231074709771298
iteration : 4849
train acc:  0.8125
train loss:  0.39705371856689453
train gradient:  0.35081528062645095
iteration : 4850
train acc:  0.8359375
train loss:  0.32178589701652527
train gradient:  0.2862886015941607
iteration : 4851
train acc:  0.8671875
train loss:  0.3247765302658081
train gradient:  0.21053055410917018
iteration : 4852
train acc:  0.78125
train loss:  0.44466519355773926
train gradient:  0.31815077837856187
iteration : 4853
train acc:  0.8359375
train loss:  0.33531564474105835
train gradient:  0.2300963211267255
iteration : 4854
train acc:  0.8515625
train loss:  0.3231450915336609
train gradient:  0.2025860423890226
iteration : 4855
train acc:  0.8359375
train loss:  0.35320210456848145
train gradient:  0.19223493044807896
iteration : 4856
train acc:  0.8125
train loss:  0.4001193046569824
train gradient:  0.2880485588884221
iteration : 4857
train acc:  0.8359375
train loss:  0.33562201261520386
train gradient:  0.20407879845691085
iteration : 4858
train acc:  0.875
train loss:  0.34083592891693115
train gradient:  0.2370326938476072
iteration : 4859
train acc:  0.8671875
train loss:  0.37792152166366577
train gradient:  0.2540123577849728
iteration : 4860
train acc:  0.8984375
train loss:  0.29131996631622314
train gradient:  0.22768159580380898
iteration : 4861
train acc:  0.8125
train loss:  0.3854221701622009
train gradient:  0.38437819371438403
iteration : 4862
train acc:  0.796875
train loss:  0.38377517461776733
train gradient:  0.3043847899526179
iteration : 4863
train acc:  0.78125
train loss:  0.4664420783519745
train gradient:  0.3653326366412556
iteration : 4864
train acc:  0.84375
train loss:  0.3481940031051636
train gradient:  0.26425379613884104
iteration : 4865
train acc:  0.828125
train loss:  0.3540807366371155
train gradient:  0.20930587941275222
iteration : 4866
train acc:  0.890625
train loss:  0.2690531313419342
train gradient:  0.14720475905605157
iteration : 4867
train acc:  0.8046875
train loss:  0.3910404443740845
train gradient:  0.294532887814636
iteration : 4868
train acc:  0.796875
train loss:  0.37529468536376953
train gradient:  0.28719707943394984
iteration : 4869
train acc:  0.84375
train loss:  0.4079993963241577
train gradient:  0.30397698340697527
iteration : 4870
train acc:  0.875
train loss:  0.31396329402923584
train gradient:  0.18937911578071215
iteration : 4871
train acc:  0.8984375
train loss:  0.2542300820350647
train gradient:  0.17268307854541115
iteration : 4872
train acc:  0.859375
train loss:  0.3580080270767212
train gradient:  0.28970552631315644
iteration : 4873
train acc:  0.8125
train loss:  0.3925632834434509
train gradient:  0.24982685826490347
iteration : 4874
train acc:  0.8515625
train loss:  0.34521767497062683
train gradient:  0.2866439396593579
iteration : 4875
train acc:  0.828125
train loss:  0.4516223669052124
train gradient:  0.3326849062172307
iteration : 4876
train acc:  0.796875
train loss:  0.3847542405128479
train gradient:  0.3101701113754401
iteration : 4877
train acc:  0.8125
train loss:  0.3387586772441864
train gradient:  0.23317420742687917
iteration : 4878
train acc:  0.8046875
train loss:  0.37301117181777954
train gradient:  0.2471871566997646
iteration : 4879
train acc:  0.8671875
train loss:  0.3234703540802002
train gradient:  0.22174380265205734
iteration : 4880
train acc:  0.828125
train loss:  0.4001803696155548
train gradient:  0.28179656311773493
iteration : 4881
train acc:  0.8203125
train loss:  0.3867422938346863
train gradient:  0.2522614692690932
iteration : 4882
train acc:  0.8515625
train loss:  0.33785441517829895
train gradient:  0.30324433063691214
iteration : 4883
train acc:  0.84375
train loss:  0.39342737197875977
train gradient:  0.3861208611124972
iteration : 4884
train acc:  0.8671875
train loss:  0.2917919158935547
train gradient:  0.1735516209883814
iteration : 4885
train acc:  0.8515625
train loss:  0.3740955889225006
train gradient:  0.2688784944973253
iteration : 4886
train acc:  0.859375
train loss:  0.3052726089954376
train gradient:  0.2247829617059122
iteration : 4887
train acc:  0.796875
train loss:  0.3788679540157318
train gradient:  0.23415791611511563
iteration : 4888
train acc:  0.8125
train loss:  0.3481857180595398
train gradient:  0.2491336610146458
iteration : 4889
train acc:  0.8125
train loss:  0.3850906491279602
train gradient:  0.1911207628475651
iteration : 4890
train acc:  0.890625
train loss:  0.3567849397659302
train gradient:  0.32049204535393777
iteration : 4891
train acc:  0.8671875
train loss:  0.3381298780441284
train gradient:  0.2794797981628737
iteration : 4892
train acc:  0.8515625
train loss:  0.33819344639778137
train gradient:  0.24646330251718163
iteration : 4893
train acc:  0.8828125
train loss:  0.31043052673339844
train gradient:  0.23786808375755245
iteration : 4894
train acc:  0.8125
train loss:  0.4194104075431824
train gradient:  0.5147942020754452
iteration : 4895
train acc:  0.8046875
train loss:  0.44036588072776794
train gradient:  0.3369543694929899
iteration : 4896
train acc:  0.8671875
train loss:  0.33560654520988464
train gradient:  0.2212759459653818
iteration : 4897
train acc:  0.8984375
train loss:  0.25683528184890747
train gradient:  0.1637763417746868
iteration : 4898
train acc:  0.84375
train loss:  0.33538568019866943
train gradient:  0.2136468450863237
iteration : 4899
train acc:  0.859375
train loss:  0.31221073865890503
train gradient:  0.18994897587045004
iteration : 4900
train acc:  0.890625
train loss:  0.273052453994751
train gradient:  0.1744904977177898
iteration : 4901
train acc:  0.8359375
train loss:  0.3039243221282959
train gradient:  0.16452798453020284
iteration : 4902
train acc:  0.8359375
train loss:  0.3402571678161621
train gradient:  0.2737658308311801
iteration : 4903
train acc:  0.8125
train loss:  0.3667054772377014
train gradient:  0.25825947772124064
iteration : 4904
train acc:  0.8125
train loss:  0.38744035363197327
train gradient:  0.2734704036454766
iteration : 4905
train acc:  0.8671875
train loss:  0.2973870038986206
train gradient:  0.15660662390482136
iteration : 4906
train acc:  0.890625
train loss:  0.3183021545410156
train gradient:  0.1936332716374441
iteration : 4907
train acc:  0.8125
train loss:  0.36898356676101685
train gradient:  0.21604108476014652
iteration : 4908
train acc:  0.8515625
train loss:  0.3513696789741516
train gradient:  0.19025094903106443
iteration : 4909
train acc:  0.8359375
train loss:  0.3944314420223236
train gradient:  0.37164118088255405
iteration : 4910
train acc:  0.859375
train loss:  0.3224138021469116
train gradient:  0.30374454814797375
iteration : 4911
train acc:  0.859375
train loss:  0.3374476730823517
train gradient:  0.2639348724574363
iteration : 4912
train acc:  0.8359375
train loss:  0.40481036901474
train gradient:  0.3026101459940504
iteration : 4913
train acc:  0.8203125
train loss:  0.3602513372898102
train gradient:  0.2682269479902393
iteration : 4914
train acc:  0.7890625
train loss:  0.4499512314796448
train gradient:  0.3612539590601092
iteration : 4915
train acc:  0.8515625
train loss:  0.3512967824935913
train gradient:  0.24751987318402902
iteration : 4916
train acc:  0.8359375
train loss:  0.3554989695549011
train gradient:  0.46160489339450356
iteration : 4917
train acc:  0.8984375
train loss:  0.3124542236328125
train gradient:  0.2744036000865549
iteration : 4918
train acc:  0.8515625
train loss:  0.3599405884742737
train gradient:  0.364399548394003
iteration : 4919
train acc:  0.828125
train loss:  0.3754320740699768
train gradient:  0.2555066746026026
iteration : 4920
train acc:  0.8359375
train loss:  0.3736966848373413
train gradient:  0.3920402378191985
iteration : 4921
train acc:  0.859375
train loss:  0.34024345874786377
train gradient:  0.26651748291303906
iteration : 4922
train acc:  0.84375
train loss:  0.4260322153568268
train gradient:  0.37082513379024407
iteration : 4923
train acc:  0.8203125
train loss:  0.3702051341533661
train gradient:  0.3022506754700719
iteration : 4924
train acc:  0.859375
train loss:  0.33314406871795654
train gradient:  0.24568375344774016
iteration : 4925
train acc:  0.84375
train loss:  0.33937329053878784
train gradient:  0.22837028895054418
iteration : 4926
train acc:  0.8125
train loss:  0.39010900259017944
train gradient:  0.25419399793392633
iteration : 4927
train acc:  0.859375
train loss:  0.33331525325775146
train gradient:  0.253653360031923
iteration : 4928
train acc:  0.8046875
train loss:  0.4392908215522766
train gradient:  0.3880172895425242
iteration : 4929
train acc:  0.8125
train loss:  0.368932843208313
train gradient:  0.2561841490307177
iteration : 4930
train acc:  0.8515625
train loss:  0.3987111747264862
train gradient:  0.28199217664174897
iteration : 4931
train acc:  0.90625
train loss:  0.30817490816116333
train gradient:  0.25218846117836713
iteration : 4932
train acc:  0.8125
train loss:  0.38035041093826294
train gradient:  0.3268707848770958
iteration : 4933
train acc:  0.8515625
train loss:  0.3817712962627411
train gradient:  0.39432803774315467
iteration : 4934
train acc:  0.8515625
train loss:  0.36440208554267883
train gradient:  0.24481934488121376
iteration : 4935
train acc:  0.84375
train loss:  0.37352174520492554
train gradient:  0.41360607480466144
iteration : 4936
train acc:  0.734375
train loss:  0.4791678190231323
train gradient:  0.3565716233571121
iteration : 4937
train acc:  0.859375
train loss:  0.328706830739975
train gradient:  0.26956033517226746
iteration : 4938
train acc:  0.8671875
train loss:  0.3135935962200165
train gradient:  0.16052517491701496
iteration : 4939
train acc:  0.8515625
train loss:  0.32827937602996826
train gradient:  0.2270674316969572
iteration : 4940
train acc:  0.8671875
train loss:  0.3251296281814575
train gradient:  0.244134020737511
iteration : 4941
train acc:  0.8046875
train loss:  0.427883505821228
train gradient:  0.33896419905278
iteration : 4942
train acc:  0.8359375
train loss:  0.3660368323326111
train gradient:  0.2521759134843224
iteration : 4943
train acc:  0.8203125
train loss:  0.40947628021240234
train gradient:  0.24192172636005077
iteration : 4944
train acc:  0.8671875
train loss:  0.2687453031539917
train gradient:  0.21491116093706628
iteration : 4945
train acc:  0.875
train loss:  0.3423519730567932
train gradient:  0.2741376710328317
iteration : 4946
train acc:  0.84375
train loss:  0.34429121017456055
train gradient:  0.22839860923197697
iteration : 4947
train acc:  0.859375
train loss:  0.38676851987838745
train gradient:  0.2110316807301672
iteration : 4948
train acc:  0.859375
train loss:  0.3196086287498474
train gradient:  0.20289180298786313
iteration : 4949
train acc:  0.875
train loss:  0.32816949486732483
train gradient:  0.3038080893482241
iteration : 4950
train acc:  0.796875
train loss:  0.40094271302223206
train gradient:  0.23949240257802315
iteration : 4951
train acc:  0.875
train loss:  0.3022053837776184
train gradient:  0.18934640997844954
iteration : 4952
train acc:  0.8359375
train loss:  0.32219821214675903
train gradient:  0.17605800699280164
iteration : 4953
train acc:  0.890625
train loss:  0.27778083086013794
train gradient:  0.17003774555467144
iteration : 4954
train acc:  0.8515625
train loss:  0.37692564725875854
train gradient:  0.24852828595615872
iteration : 4955
train acc:  0.8046875
train loss:  0.462091326713562
train gradient:  0.34150608099955004
iteration : 4956
train acc:  0.8984375
train loss:  0.32602351903915405
train gradient:  0.2307887925938284
iteration : 4957
train acc:  0.875
train loss:  0.3027121424674988
train gradient:  0.18970265676927783
iteration : 4958
train acc:  0.796875
train loss:  0.45795702934265137
train gradient:  0.4414101588116245
iteration : 4959
train acc:  0.8359375
train loss:  0.37835240364074707
train gradient:  0.2909035002636968
iteration : 4960
train acc:  0.796875
train loss:  0.46439462900161743
train gradient:  0.2920326907931149
iteration : 4961
train acc:  0.84375
train loss:  0.3978966474533081
train gradient:  0.3457279875683207
iteration : 4962
train acc:  0.828125
train loss:  0.4034162759780884
train gradient:  0.24309860063187988
iteration : 4963
train acc:  0.84375
train loss:  0.3552786409854889
train gradient:  0.31614143979291137
iteration : 4964
train acc:  0.8984375
train loss:  0.28815189003944397
train gradient:  0.2000989281673714
iteration : 4965
train acc:  0.8828125
train loss:  0.34231093525886536
train gradient:  0.22033872517870146
iteration : 4966
train acc:  0.796875
train loss:  0.46768105030059814
train gradient:  0.5201436997485205
iteration : 4967
train acc:  0.8671875
train loss:  0.31704163551330566
train gradient:  0.20683589216950538
iteration : 4968
train acc:  0.84375
train loss:  0.3457850217819214
train gradient:  0.2142335294440279
iteration : 4969
train acc:  0.8359375
train loss:  0.3457028865814209
train gradient:  0.2055345662961444
iteration : 4970
train acc:  0.8359375
train loss:  0.36347484588623047
train gradient:  0.21357955516242033
iteration : 4971
train acc:  0.875
train loss:  0.30287012457847595
train gradient:  0.12509406269399836
iteration : 4972
train acc:  0.828125
train loss:  0.3910583555698395
train gradient:  0.24054600972945073
iteration : 4973
train acc:  0.8359375
train loss:  0.3105868697166443
train gradient:  0.15863306436350155
iteration : 4974
train acc:  0.8359375
train loss:  0.4000142216682434
train gradient:  0.2636729007961405
iteration : 4975
train acc:  0.859375
train loss:  0.35989800095558167
train gradient:  0.29485243363650976
iteration : 4976
train acc:  0.8515625
train loss:  0.2540680170059204
train gradient:  0.18113802950817534
iteration : 4977
train acc:  0.8828125
train loss:  0.31698358058929443
train gradient:  0.19154967884997437
iteration : 4978
train acc:  0.828125
train loss:  0.36805588006973267
train gradient:  0.26795476642811394
iteration : 4979
train acc:  0.8515625
train loss:  0.3709585666656494
train gradient:  0.18271682703338044
iteration : 4980
train acc:  0.8203125
train loss:  0.3842780590057373
train gradient:  0.20456308997638592
iteration : 4981
train acc:  0.8359375
train loss:  0.3702797293663025
train gradient:  0.19475195913825066
iteration : 4982
train acc:  0.796875
train loss:  0.44502371549606323
train gradient:  0.3220183216949434
iteration : 4983
train acc:  0.8203125
train loss:  0.36232608556747437
train gradient:  0.22874964468920017
iteration : 4984
train acc:  0.890625
train loss:  0.2923225164413452
train gradient:  0.22830478844631913
iteration : 4985
train acc:  0.8515625
train loss:  0.29245302081108093
train gradient:  0.22919590757882563
iteration : 4986
train acc:  0.828125
train loss:  0.3456045985221863
train gradient:  0.24993641842753112
iteration : 4987
train acc:  0.8671875
train loss:  0.3031073808670044
train gradient:  0.1373146316513109
iteration : 4988
train acc:  0.8125
train loss:  0.3953222632408142
train gradient:  0.2890601990166521
iteration : 4989
train acc:  0.78125
train loss:  0.4309670925140381
train gradient:  0.26613883875606725
iteration : 4990
train acc:  0.8671875
train loss:  0.31631845235824585
train gradient:  0.14298461136051485
iteration : 4991
train acc:  0.8671875
train loss:  0.3133898079395294
train gradient:  0.2327463332410959
iteration : 4992
train acc:  0.8984375
train loss:  0.3169999122619629
train gradient:  0.15720185917030793
iteration : 4993
train acc:  0.828125
train loss:  0.41115307807922363
train gradient:  0.28593659808180394
iteration : 4994
train acc:  0.8828125
train loss:  0.32857611775398254
train gradient:  0.24057799315146866
iteration : 4995
train acc:  0.8203125
train loss:  0.3461834192276001
train gradient:  0.2471550277643274
iteration : 4996
train acc:  0.859375
train loss:  0.3273352384567261
train gradient:  0.2041458848973264
iteration : 4997
train acc:  0.828125
train loss:  0.39724862575531006
train gradient:  0.3277875998876804
iteration : 4998
train acc:  0.8984375
train loss:  0.28377681970596313
train gradient:  0.17752186519155982
iteration : 4999
train acc:  0.8515625
train loss:  0.36573565006256104
train gradient:  0.25198935087564606
iteration : 5000
train acc:  0.765625
train loss:  0.47906771302223206
train gradient:  0.3115688719299601
iteration : 5001
train acc:  0.8203125
train loss:  0.36725741624832153
train gradient:  0.3108522091066393
iteration : 5002
train acc:  0.796875
train loss:  0.4567248225212097
train gradient:  0.4236048274443761
iteration : 5003
train acc:  0.8828125
train loss:  0.31769847869873047
train gradient:  0.19885368008708854
iteration : 5004
train acc:  0.8671875
train loss:  0.37476247549057007
train gradient:  0.29640478253131763
iteration : 5005
train acc:  0.828125
train loss:  0.3598596751689911
train gradient:  0.29727131883666674
iteration : 5006
train acc:  0.8671875
train loss:  0.3347814977169037
train gradient:  0.37848784940657304
iteration : 5007
train acc:  0.796875
train loss:  0.4657837748527527
train gradient:  0.35550343867693845
iteration : 5008
train acc:  0.890625
train loss:  0.34077462553977966
train gradient:  0.25799535957023995
iteration : 5009
train acc:  0.828125
train loss:  0.4398299753665924
train gradient:  0.31716432769072933
iteration : 5010
train acc:  0.8359375
train loss:  0.35144224762916565
train gradient:  0.20955530704054703
iteration : 5011
train acc:  0.8125
train loss:  0.4595939517021179
train gradient:  0.3344265082339715
iteration : 5012
train acc:  0.8828125
train loss:  0.3105544149875641
train gradient:  0.22025660380829562
iteration : 5013
train acc:  0.8515625
train loss:  0.3138387203216553
train gradient:  0.20555015690307776
iteration : 5014
train acc:  0.875
train loss:  0.29542848467826843
train gradient:  0.14940339228766944
iteration : 5015
train acc:  0.796875
train loss:  0.41873493790626526
train gradient:  0.31769652574747104
iteration : 5016
train acc:  0.8828125
train loss:  0.32198548316955566
train gradient:  0.18980908795065682
iteration : 5017
train acc:  0.84375
train loss:  0.3465367555618286
train gradient:  0.22303470529431135
iteration : 5018
train acc:  0.84375
train loss:  0.31391263008117676
train gradient:  0.22062005670817297
iteration : 5019
train acc:  0.9140625
train loss:  0.27927249670028687
train gradient:  0.16842073181628825
iteration : 5020
train acc:  0.8515625
train loss:  0.3249381184577942
train gradient:  0.3200327308555682
iteration : 5021
train acc:  0.8515625
train loss:  0.3470209836959839
train gradient:  0.3722685222937459
iteration : 5022
train acc:  0.8671875
train loss:  0.2947644293308258
train gradient:  0.18880294392891045
iteration : 5023
train acc:  0.8203125
train loss:  0.38444983959198
train gradient:  0.1990454329339675
iteration : 5024
train acc:  0.8828125
train loss:  0.32483968138694763
train gradient:  0.2630552891864735
iteration : 5025
train acc:  0.84375
train loss:  0.37706512212753296
train gradient:  0.371965785016269
iteration : 5026
train acc:  0.8125
train loss:  0.4119093418121338
train gradient:  0.35246143984062056
iteration : 5027
train acc:  0.84375
train loss:  0.37569543719291687
train gradient:  0.33008489262840046
iteration : 5028
train acc:  0.8515625
train loss:  0.3641200065612793
train gradient:  0.23407715093642703
iteration : 5029
train acc:  0.8359375
train loss:  0.3831407129764557
train gradient:  0.313122509646859
iteration : 5030
train acc:  0.8046875
train loss:  0.42570093274116516
train gradient:  0.42991971054092815
iteration : 5031
train acc:  0.8046875
train loss:  0.4047195613384247
train gradient:  0.3068956270664061
iteration : 5032
train acc:  0.8515625
train loss:  0.3268359303474426
train gradient:  0.1974838909590886
iteration : 5033
train acc:  0.828125
train loss:  0.3779004216194153
train gradient:  0.30192594365441733
iteration : 5034
train acc:  0.859375
train loss:  0.35391154885292053
train gradient:  0.2312800947264825
iteration : 5035
train acc:  0.8125
train loss:  0.4304889440536499
train gradient:  0.3332220184284044
iteration : 5036
train acc:  0.765625
train loss:  0.40533536672592163
train gradient:  0.29907765398128017
iteration : 5037
train acc:  0.875
train loss:  0.288467675447464
train gradient:  0.17596079054418057
iteration : 5038
train acc:  0.84375
train loss:  0.3798573315143585
train gradient:  0.21660907100077373
iteration : 5039
train acc:  0.828125
train loss:  0.3441178798675537
train gradient:  0.27553847096234
iteration : 5040
train acc:  0.8671875
train loss:  0.3351122736930847
train gradient:  0.18568011255772496
iteration : 5041
train acc:  0.8515625
train loss:  0.3506132662296295
train gradient:  0.2546403373534282
iteration : 5042
train acc:  0.8671875
train loss:  0.3316032290458679
train gradient:  0.2223160205289412
iteration : 5043
train acc:  0.8203125
train loss:  0.3671199083328247
train gradient:  0.2688624974584215
iteration : 5044
train acc:  0.859375
train loss:  0.3482641577720642
train gradient:  0.2637527138015338
iteration : 5045
train acc:  0.84375
train loss:  0.3200685977935791
train gradient:  0.24539083723791777
iteration : 5046
train acc:  0.84375
train loss:  0.40322473645210266
train gradient:  0.3445516285729156
iteration : 5047
train acc:  0.8125
train loss:  0.3475166857242584
train gradient:  0.19174421762389499
iteration : 5048
train acc:  0.859375
train loss:  0.3445618748664856
train gradient:  0.17927840566452408
iteration : 5049
train acc:  0.8203125
train loss:  0.3650752305984497
train gradient:  0.24575602927974405
iteration : 5050
train acc:  0.8515625
train loss:  0.3239362835884094
train gradient:  0.23366867153325416
iteration : 5051
train acc:  0.84375
train loss:  0.365235298871994
train gradient:  0.20132334875287644
iteration : 5052
train acc:  0.8359375
train loss:  0.3514074683189392
train gradient:  0.22948537469245575
iteration : 5053
train acc:  0.8671875
train loss:  0.3211485743522644
train gradient:  0.20110832466122835
iteration : 5054
train acc:  0.859375
train loss:  0.3276256024837494
train gradient:  0.2295073198093782
iteration : 5055
train acc:  0.7890625
train loss:  0.463849276304245
train gradient:  0.37659268917775374
iteration : 5056
train acc:  0.78125
train loss:  0.3922085762023926
train gradient:  0.2784381225882986
iteration : 5057
train acc:  0.8515625
train loss:  0.3457225263118744
train gradient:  0.2235385626325152
iteration : 5058
train acc:  0.8671875
train loss:  0.308000385761261
train gradient:  0.21741294502442776
iteration : 5059
train acc:  0.859375
train loss:  0.3605315089225769
train gradient:  0.22746584095030792
iteration : 5060
train acc:  0.796875
train loss:  0.4173097312450409
train gradient:  0.2727753508184758
iteration : 5061
train acc:  0.8125
train loss:  0.3608971834182739
train gradient:  0.31706846958923995
iteration : 5062
train acc:  0.78125
train loss:  0.39844903349876404
train gradient:  0.2889486943251298
iteration : 5063
train acc:  0.875
train loss:  0.27291589975357056
train gradient:  0.22302042536662287
iteration : 5064
train acc:  0.8125
train loss:  0.3912354111671448
train gradient:  0.3938573380107549
iteration : 5065
train acc:  0.84375
train loss:  0.3548847436904907
train gradient:  0.19584070902722847
iteration : 5066
train acc:  0.8515625
train loss:  0.3693090081214905
train gradient:  0.30199427465413575
iteration : 5067
train acc:  0.8203125
train loss:  0.3700031042098999
train gradient:  0.28390704240117776
iteration : 5068
train acc:  0.890625
train loss:  0.2741333842277527
train gradient:  0.1583482484577592
iteration : 5069
train acc:  0.8515625
train loss:  0.33344197273254395
train gradient:  0.2546969261345159
iteration : 5070
train acc:  0.859375
train loss:  0.3533875644207001
train gradient:  0.19146512370269814
iteration : 5071
train acc:  0.8828125
train loss:  0.2621443271636963
train gradient:  0.1798358531088952
iteration : 5072
train acc:  0.8125
train loss:  0.41087108850479126
train gradient:  0.32072951160519536
iteration : 5073
train acc:  0.921875
train loss:  0.24120932817459106
train gradient:  0.1288614560654422
iteration : 5074
train acc:  0.796875
train loss:  0.4386913776397705
train gradient:  0.22472487646725614
iteration : 5075
train acc:  0.8046875
train loss:  0.38510698080062866
train gradient:  0.33210158885345287
iteration : 5076
train acc:  0.890625
train loss:  0.30312827229499817
train gradient:  0.21759407299584616
iteration : 5077
train acc:  0.8515625
train loss:  0.2957514524459839
train gradient:  0.16753439263656306
iteration : 5078
train acc:  0.8203125
train loss:  0.4224059283733368
train gradient:  0.3984296127573373
iteration : 5079
train acc:  0.8671875
train loss:  0.30368533730506897
train gradient:  0.15691642807770964
iteration : 5080
train acc:  0.8203125
train loss:  0.3600281774997711
train gradient:  0.2064316428776713
iteration : 5081
train acc:  0.9375
train loss:  0.22550833225250244
train gradient:  0.15550106800836994
iteration : 5082
train acc:  0.8203125
train loss:  0.4044520854949951
train gradient:  0.35100441001981786
iteration : 5083
train acc:  0.859375
train loss:  0.3474580645561218
train gradient:  0.20780342632859283
iteration : 5084
train acc:  0.84375
train loss:  0.33936795592308044
train gradient:  0.22685264375458147
iteration : 5085
train acc:  0.828125
train loss:  0.33222633600234985
train gradient:  0.2460658113808275
iteration : 5086
train acc:  0.8203125
train loss:  0.45227599143981934
train gradient:  0.2750084164015647
iteration : 5087
train acc:  0.8125
train loss:  0.3661831021308899
train gradient:  0.19949035033221985
iteration : 5088
train acc:  0.8671875
train loss:  0.38595104217529297
train gradient:  0.3036785889980668
iteration : 5089
train acc:  0.859375
train loss:  0.37373000383377075
train gradient:  0.23607298975991986
iteration : 5090
train acc:  0.8203125
train loss:  0.35387372970581055
train gradient:  0.26350287766704417
iteration : 5091
train acc:  0.8359375
train loss:  0.3979349732398987
train gradient:  0.27012958671905435
iteration : 5092
train acc:  0.828125
train loss:  0.3422830104827881
train gradient:  0.314650867105028
iteration : 5093
train acc:  0.8515625
train loss:  0.32495200634002686
train gradient:  0.207566917193318
iteration : 5094
train acc:  0.859375
train loss:  0.35907915234565735
train gradient:  0.2647524442604874
iteration : 5095
train acc:  0.84375
train loss:  0.3444369435310364
train gradient:  0.21483220913663786
iteration : 5096
train acc:  0.875
train loss:  0.30284690856933594
train gradient:  0.22338797838625826
iteration : 5097
train acc:  0.8671875
train loss:  0.331837922334671
train gradient:  0.2199815802275109
iteration : 5098
train acc:  0.859375
train loss:  0.3447081744670868
train gradient:  0.17967739078447306
iteration : 5099
train acc:  0.8359375
train loss:  0.3711715340614319
train gradient:  0.32594236666159315
iteration : 5100
train acc:  0.875
train loss:  0.30790215730667114
train gradient:  0.21428037878603473
iteration : 5101
train acc:  0.8515625
train loss:  0.3998194932937622
train gradient:  0.2922705057720342
iteration : 5102
train acc:  0.8828125
train loss:  0.33991244435310364
train gradient:  0.2772284174896944
iteration : 5103
train acc:  0.859375
train loss:  0.2990719676017761
train gradient:  0.2030374195071784
iteration : 5104
train acc:  0.8125
train loss:  0.412939190864563
train gradient:  0.286961147796469
iteration : 5105
train acc:  0.8359375
train loss:  0.29723984003067017
train gradient:  0.18716740878012805
iteration : 5106
train acc:  0.8359375
train loss:  0.34568721055984497
train gradient:  0.1916012115751597
iteration : 5107
train acc:  0.7578125
train loss:  0.39074215292930603
train gradient:  0.3886445362487264
iteration : 5108
train acc:  0.8515625
train loss:  0.3130706548690796
train gradient:  0.20249871156096153
iteration : 5109
train acc:  0.8671875
train loss:  0.35853371024131775
train gradient:  0.23907159460070518
iteration : 5110
train acc:  0.8203125
train loss:  0.3740561008453369
train gradient:  0.2728369452706779
iteration : 5111
train acc:  0.84375
train loss:  0.4206635057926178
train gradient:  0.3236360249929686
iteration : 5112
train acc:  0.875
train loss:  0.3366355299949646
train gradient:  0.226859876209188
iteration : 5113
train acc:  0.8203125
train loss:  0.3531965911388397
train gradient:  0.22731981349676045
iteration : 5114
train acc:  0.7890625
train loss:  0.4640304744243622
train gradient:  0.32887756047369404
iteration : 5115
train acc:  0.828125
train loss:  0.42952588200569153
train gradient:  0.3260962785340417
iteration : 5116
train acc:  0.8671875
train loss:  0.3257370591163635
train gradient:  0.21236367566965403
iteration : 5117
train acc:  0.8671875
train loss:  0.3287210464477539
train gradient:  0.36596217464176783
iteration : 5118
train acc:  0.859375
train loss:  0.37416011095046997
train gradient:  0.275526416135704
iteration : 5119
train acc:  0.8203125
train loss:  0.39191585779190063
train gradient:  0.2124872482555151
iteration : 5120
train acc:  0.796875
train loss:  0.4427969753742218
train gradient:  0.38903059036155924
iteration : 5121
train acc:  0.859375
train loss:  0.34937337040901184
train gradient:  0.2094778685131206
iteration : 5122
train acc:  0.8125
train loss:  0.36303821206092834
train gradient:  0.2084812467148008
iteration : 5123
train acc:  0.890625
train loss:  0.2775062024593353
train gradient:  0.1909598884703229
iteration : 5124
train acc:  0.875
train loss:  0.3463861644268036
train gradient:  0.31822487894035756
iteration : 5125
train acc:  0.84375
train loss:  0.38169237971305847
train gradient:  0.3708565724832108
iteration : 5126
train acc:  0.890625
train loss:  0.2734941244125366
train gradient:  0.1871980966671017
iteration : 5127
train acc:  0.8359375
train loss:  0.3849143385887146
train gradient:  0.2656217086135911
iteration : 5128
train acc:  0.890625
train loss:  0.295451819896698
train gradient:  0.1535670944851722
iteration : 5129
train acc:  0.8515625
train loss:  0.3925739824771881
train gradient:  0.28804208267995507
iteration : 5130
train acc:  0.875
train loss:  0.32096850872039795
train gradient:  0.16860063462335395
iteration : 5131
train acc:  0.8671875
train loss:  0.34129130840301514
train gradient:  0.20167911666381222
iteration : 5132
train acc:  0.7578125
train loss:  0.4551514983177185
train gradient:  0.46378137539626807
iteration : 5133
train acc:  0.8515625
train loss:  0.3690894544124603
train gradient:  0.2976455784800357
iteration : 5134
train acc:  0.8203125
train loss:  0.3702624440193176
train gradient:  0.25582015431595506
iteration : 5135
train acc:  0.8671875
train loss:  0.3747790455818176
train gradient:  0.31580234679992614
iteration : 5136
train acc:  0.8203125
train loss:  0.3693197965621948
train gradient:  0.25010207761843095
iteration : 5137
train acc:  0.84375
train loss:  0.3566114902496338
train gradient:  0.23761692496839812
iteration : 5138
train acc:  0.828125
train loss:  0.44418036937713623
train gradient:  0.28368000998586657
iteration : 5139
train acc:  0.8203125
train loss:  0.3967954218387604
train gradient:  0.26332150041585245
iteration : 5140
train acc:  0.890625
train loss:  0.28151607513427734
train gradient:  0.1365929304001644
iteration : 5141
train acc:  0.8671875
train loss:  0.2916712164878845
train gradient:  0.16437912129488774
iteration : 5142
train acc:  0.8671875
train loss:  0.28744372725486755
train gradient:  0.275346680907764
iteration : 5143
train acc:  0.828125
train loss:  0.37290018796920776
train gradient:  0.2712992762397593
iteration : 5144
train acc:  0.90625
train loss:  0.25592929124832153
train gradient:  0.14011659864301246
iteration : 5145
train acc:  0.8125
train loss:  0.3753347396850586
train gradient:  0.24030995461228694
iteration : 5146
train acc:  0.84375
train loss:  0.3774421513080597
train gradient:  0.2500342370157128
iteration : 5147
train acc:  0.8203125
train loss:  0.3866051435470581
train gradient:  0.2771687676441749
iteration : 5148
train acc:  0.8984375
train loss:  0.29926401376724243
train gradient:  0.1980609196147553
iteration : 5149
train acc:  0.8828125
train loss:  0.3005325198173523
train gradient:  0.1806929113407276
iteration : 5150
train acc:  0.8359375
train loss:  0.379217267036438
train gradient:  0.3100399068020357
iteration : 5151
train acc:  0.84375
train loss:  0.33452242612838745
train gradient:  0.20054001642230326
iteration : 5152
train acc:  0.875
train loss:  0.29338863492012024
train gradient:  0.16474207063075139
iteration : 5153
train acc:  0.859375
train loss:  0.31729215383529663
train gradient:  0.2078652286673766
iteration : 5154
train acc:  0.8125
train loss:  0.40431469678878784
train gradient:  0.26204682818092107
iteration : 5155
train acc:  0.875
train loss:  0.3091162443161011
train gradient:  0.19826036274243264
iteration : 5156
train acc:  0.921875
train loss:  0.2577848732471466
train gradient:  0.1974326736382516
iteration : 5157
train acc:  0.8671875
train loss:  0.32429587841033936
train gradient:  0.23325132682117752
iteration : 5158
train acc:  0.8828125
train loss:  0.3142337203025818
train gradient:  0.25284236636583324
iteration : 5159
train acc:  0.890625
train loss:  0.24048122763633728
train gradient:  0.12262336217664528
iteration : 5160
train acc:  0.828125
train loss:  0.38575032353401184
train gradient:  0.30115970635839934
iteration : 5161
train acc:  0.8359375
train loss:  0.33146369457244873
train gradient:  0.2670552996440614
iteration : 5162
train acc:  0.8046875
train loss:  0.4409388303756714
train gradient:  0.3198080319844125
iteration : 5163
train acc:  0.8671875
train loss:  0.2681735157966614
train gradient:  0.16834790708547348
iteration : 5164
train acc:  0.765625
train loss:  0.4792235493659973
train gradient:  0.44561133126682173
iteration : 5165
train acc:  0.8125
train loss:  0.3891039490699768
train gradient:  0.26085855045814765
iteration : 5166
train acc:  0.84375
train loss:  0.3565080165863037
train gradient:  0.2592607233580369
iteration : 5167
train acc:  0.8515625
train loss:  0.3597208261489868
train gradient:  0.2738392719973594
iteration : 5168
train acc:  0.8125
train loss:  0.40627193450927734
train gradient:  0.26848356935177703
iteration : 5169
train acc:  0.7890625
train loss:  0.39530855417251587
train gradient:  0.3050028292388141
iteration : 5170
train acc:  0.8046875
train loss:  0.3683163523674011
train gradient:  0.2987969340126029
iteration : 5171
train acc:  0.8359375
train loss:  0.34170249104499817
train gradient:  0.19585132109517375
iteration : 5172
train acc:  0.828125
train loss:  0.4236285090446472
train gradient:  0.2618918309730714
iteration : 5173
train acc:  0.7890625
train loss:  0.420209139585495
train gradient:  0.45843723200295755
iteration : 5174
train acc:  0.890625
train loss:  0.30623966455459595
train gradient:  0.24305772489072952
iteration : 5175
train acc:  0.890625
train loss:  0.3056989908218384
train gradient:  0.1792697006993427
iteration : 5176
train acc:  0.8359375
train loss:  0.3577629625797272
train gradient:  0.2879511658518056
iteration : 5177
train acc:  0.859375
train loss:  0.29321253299713135
train gradient:  0.21697144586319753
iteration : 5178
train acc:  0.7578125
train loss:  0.4425393342971802
train gradient:  0.2835064008747947
iteration : 5179
train acc:  0.84375
train loss:  0.3953297734260559
train gradient:  0.4203033637036485
iteration : 5180
train acc:  0.8671875
train loss:  0.30877846479415894
train gradient:  0.21436272309578724
iteration : 5181
train acc:  0.8828125
train loss:  0.273966908454895
train gradient:  0.1569962756133988
iteration : 5182
train acc:  0.8125
train loss:  0.31484726071357727
train gradient:  0.2535306158313271
iteration : 5183
train acc:  0.78125
train loss:  0.501665472984314
train gradient:  0.5564156626366417
iteration : 5184
train acc:  0.8125
train loss:  0.4149497449398041
train gradient:  0.2819430503893613
iteration : 5185
train acc:  0.8671875
train loss:  0.2875487804412842
train gradient:  0.16737986500573065
iteration : 5186
train acc:  0.796875
train loss:  0.3499142527580261
train gradient:  0.29663268438188345
iteration : 5187
train acc:  0.859375
train loss:  0.3207028806209564
train gradient:  0.2002733030208336
iteration : 5188
train acc:  0.8125
train loss:  0.40520036220550537
train gradient:  0.31171552981095707
iteration : 5189
train acc:  0.8828125
train loss:  0.32948631048202515
train gradient:  0.2022858072067048
iteration : 5190
train acc:  0.796875
train loss:  0.4229629635810852
train gradient:  0.37948455405173864
iteration : 5191
train acc:  0.8046875
train loss:  0.431918203830719
train gradient:  0.23124624967266166
iteration : 5192
train acc:  0.8125
train loss:  0.3459759056568146
train gradient:  0.2906779277912444
iteration : 5193
train acc:  0.859375
train loss:  0.31227776408195496
train gradient:  0.23576159466038527
iteration : 5194
train acc:  0.8046875
train loss:  0.3984662890434265
train gradient:  0.21497143490181858
iteration : 5195
train acc:  0.828125
train loss:  0.34299859404563904
train gradient:  0.3137303903979277
iteration : 5196
train acc:  0.828125
train loss:  0.4140731692314148
train gradient:  0.26169276453741347
iteration : 5197
train acc:  0.84375
train loss:  0.322052925825119
train gradient:  0.2718638999144288
iteration : 5198
train acc:  0.8515625
train loss:  0.3641242980957031
train gradient:  0.1939722244301163
iteration : 5199
train acc:  0.875
train loss:  0.31330645084381104
train gradient:  0.1896481249516182
iteration : 5200
train acc:  0.84375
train loss:  0.39148086309432983
train gradient:  0.236807297475233
iteration : 5201
train acc:  0.8828125
train loss:  0.3203994035720825
train gradient:  0.19252295789129195
iteration : 5202
train acc:  0.8125
train loss:  0.3578904867172241
train gradient:  0.20343518921454112
iteration : 5203
train acc:  0.84375
train loss:  0.3747626841068268
train gradient:  0.3169844434348928
iteration : 5204
train acc:  0.8359375
train loss:  0.3533650040626526
train gradient:  0.17910497060816027
iteration : 5205
train acc:  0.8359375
train loss:  0.3783324062824249
train gradient:  0.2338688123528763
iteration : 5206
train acc:  0.8359375
train loss:  0.3364962339401245
train gradient:  0.2027194950082807
iteration : 5207
train acc:  0.859375
train loss:  0.32413673400878906
train gradient:  0.25783343652826785
iteration : 5208
train acc:  0.890625
train loss:  0.32056501507759094
train gradient:  0.16970398327305458
iteration : 5209
train acc:  0.8125
train loss:  0.38067761063575745
train gradient:  0.2047201304113777
iteration : 5210
train acc:  0.8125
train loss:  0.3817916214466095
train gradient:  0.33121532039544377
iteration : 5211
train acc:  0.828125
train loss:  0.35202085971832275
train gradient:  0.20490789300063444
iteration : 5212
train acc:  0.828125
train loss:  0.38679763674736023
train gradient:  0.23437720496373796
iteration : 5213
train acc:  0.8828125
train loss:  0.26520752906799316
train gradient:  0.17391902668748413
iteration : 5214
train acc:  0.78125
train loss:  0.4243896007537842
train gradient:  0.3765372599950948
iteration : 5215
train acc:  0.8515625
train loss:  0.2842335104942322
train gradient:  0.18925147188622565
iteration : 5216
train acc:  0.84375
train loss:  0.34544557332992554
train gradient:  0.2274705866507747
iteration : 5217
train acc:  0.8515625
train loss:  0.33577078580856323
train gradient:  0.2160666023107102
iteration : 5218
train acc:  0.8359375
train loss:  0.37355777621269226
train gradient:  0.2629698220475925
iteration : 5219
train acc:  0.8046875
train loss:  0.3929319977760315
train gradient:  0.29399020545316495
iteration : 5220
train acc:  0.84375
train loss:  0.35789060592651367
train gradient:  0.2606175596503103
iteration : 5221
train acc:  0.8046875
train loss:  0.3660236597061157
train gradient:  0.27661752393987243
iteration : 5222
train acc:  0.84375
train loss:  0.3561853766441345
train gradient:  0.26318749930542246
iteration : 5223
train acc:  0.796875
train loss:  0.3705422282218933
train gradient:  0.26882478872245397
iteration : 5224
train acc:  0.8359375
train loss:  0.35163140296936035
train gradient:  0.23324667211125896
iteration : 5225
train acc:  0.7890625
train loss:  0.4659196138381958
train gradient:  0.4397415733342669
iteration : 5226
train acc:  0.8203125
train loss:  0.4178057610988617
train gradient:  0.34556665624764193
iteration : 5227
train acc:  0.828125
train loss:  0.3751939535140991
train gradient:  0.25464840148724793
iteration : 5228
train acc:  0.84375
train loss:  0.3498131036758423
train gradient:  0.2853103140050184
iteration : 5229
train acc:  0.84375
train loss:  0.3366262912750244
train gradient:  0.22976949172057076
iteration : 5230
train acc:  0.828125
train loss:  0.35214555263519287
train gradient:  0.23425469074602004
iteration : 5231
train acc:  0.8125
train loss:  0.3750145435333252
train gradient:  0.19772705854236303
iteration : 5232
train acc:  0.8359375
train loss:  0.37819358706474304
train gradient:  0.40180101156069287
iteration : 5233
train acc:  0.8515625
train loss:  0.32012617588043213
train gradient:  0.26052007422477375
iteration : 5234
train acc:  0.8125
train loss:  0.47510063648223877
train gradient:  0.37866665674609823
iteration : 5235
train acc:  0.8125
train loss:  0.3675738275051117
train gradient:  0.4020685680396118
iteration : 5236
train acc:  0.8515625
train loss:  0.3271842300891876
train gradient:  0.17637947338749102
iteration : 5237
train acc:  0.859375
train loss:  0.31703561544418335
train gradient:  0.26045345330305875
iteration : 5238
train acc:  0.78125
train loss:  0.4497646689414978
train gradient:  0.3178043870503146
iteration : 5239
train acc:  0.875
train loss:  0.30866557359695435
train gradient:  0.21988037872582283
iteration : 5240
train acc:  0.890625
train loss:  0.3521921634674072
train gradient:  0.20221389557916636
iteration : 5241
train acc:  0.8515625
train loss:  0.32876265048980713
train gradient:  0.24971869045751094
iteration : 5242
train acc:  0.8359375
train loss:  0.3814721703529358
train gradient:  0.35420228607515697
iteration : 5243
train acc:  0.90625
train loss:  0.32877975702285767
train gradient:  0.214053247898286
iteration : 5244
train acc:  0.859375
train loss:  0.3579043447971344
train gradient:  0.3550977534438468
iteration : 5245
train acc:  0.859375
train loss:  0.34931278228759766
train gradient:  0.19177151084243604
iteration : 5246
train acc:  0.828125
train loss:  0.35199064016342163
train gradient:  0.4080280010518514
iteration : 5247
train acc:  0.8046875
train loss:  0.42521557211875916
train gradient:  0.3717837908102285
iteration : 5248
train acc:  0.859375
train loss:  0.36300045251846313
train gradient:  0.21351189406918145
iteration : 5249
train acc:  0.8984375
train loss:  0.2674218416213989
train gradient:  0.18142453496113856
iteration : 5250
train acc:  0.8671875
train loss:  0.3311176896095276
train gradient:  0.21547938770233788
iteration : 5251
train acc:  0.875
train loss:  0.2880769968032837
train gradient:  0.17191119850364037
iteration : 5252
train acc:  0.8203125
train loss:  0.41669878363609314
train gradient:  0.32892511504552696
iteration : 5253
train acc:  0.796875
train loss:  0.40711575746536255
train gradient:  0.290928571216461
iteration : 5254
train acc:  0.8671875
train loss:  0.35301199555397034
train gradient:  0.2391851328348737
iteration : 5255
train acc:  0.8359375
train loss:  0.3492920398712158
train gradient:  0.225156070030876
iteration : 5256
train acc:  0.828125
train loss:  0.36703115701675415
train gradient:  0.3029739978324382
iteration : 5257
train acc:  0.921875
train loss:  0.24413278698921204
train gradient:  0.16645934645052318
iteration : 5258
train acc:  0.8046875
train loss:  0.41207897663116455
train gradient:  0.29351394364441413
iteration : 5259
train acc:  0.8359375
train loss:  0.3442000150680542
train gradient:  0.26836864939116223
iteration : 5260
train acc:  0.8046875
train loss:  0.3695875406265259
train gradient:  0.22641452137398338
iteration : 5261
train acc:  0.8203125
train loss:  0.38718029856681824
train gradient:  0.5252936216631554
iteration : 5262
train acc:  0.859375
train loss:  0.3307512402534485
train gradient:  0.14425026142418068
iteration : 5263
train acc:  0.828125
train loss:  0.3388988971710205
train gradient:  0.21755551533113243
iteration : 5264
train acc:  0.875
train loss:  0.3697967529296875
train gradient:  0.2545242230996315
iteration : 5265
train acc:  0.828125
train loss:  0.35439354181289673
train gradient:  0.2685137255001732
iteration : 5266
train acc:  0.84375
train loss:  0.3466213345527649
train gradient:  0.22849733416051604
iteration : 5267
train acc:  0.8359375
train loss:  0.37099552154541016
train gradient:  0.2805623307009954
iteration : 5268
train acc:  0.8203125
train loss:  0.45196664333343506
train gradient:  0.34028145563697054
iteration : 5269
train acc:  0.8125
train loss:  0.3638322949409485
train gradient:  0.3336522506025802
iteration : 5270
train acc:  0.8359375
train loss:  0.3964819312095642
train gradient:  0.2541567055776521
iteration : 5271
train acc:  0.8125
train loss:  0.37674614787101746
train gradient:  0.28737113555671373
iteration : 5272
train acc:  0.890625
train loss:  0.26232028007507324
train gradient:  0.1504797761254393
iteration : 5273
train acc:  0.8671875
train loss:  0.3269452154636383
train gradient:  0.19276130299125274
iteration : 5274
train acc:  0.8046875
train loss:  0.42093393206596375
train gradient:  0.34862459667290463
iteration : 5275
train acc:  0.8515625
train loss:  0.3775067925453186
train gradient:  0.2458505835465801
iteration : 5276
train acc:  0.921875
train loss:  0.27901166677474976
train gradient:  0.18017619334379573
iteration : 5277
train acc:  0.8671875
train loss:  0.2831065058708191
train gradient:  0.16517544413842028
iteration : 5278
train acc:  0.7734375
train loss:  0.3899209499359131
train gradient:  0.3804301069766174
iteration : 5279
train acc:  0.875
train loss:  0.27889305353164673
train gradient:  0.17517293073882864
iteration : 5280
train acc:  0.859375
train loss:  0.3047693371772766
train gradient:  0.3203731753478519
iteration : 5281
train acc:  0.8671875
train loss:  0.3175528049468994
train gradient:  0.22363467676874288
iteration : 5282
train acc:  0.78125
train loss:  0.4770338237285614
train gradient:  0.46957047359596116
iteration : 5283
train acc:  0.8515625
train loss:  0.3813832700252533
train gradient:  0.22627577854206726
iteration : 5284
train acc:  0.859375
train loss:  0.32586371898651123
train gradient:  0.1854891956699977
iteration : 5285
train acc:  0.859375
train loss:  0.27050474286079407
train gradient:  0.1833208200254035
iteration : 5286
train acc:  0.8046875
train loss:  0.40139567852020264
train gradient:  0.31751954265558335
iteration : 5287
train acc:  0.859375
train loss:  0.3043566942214966
train gradient:  0.16470033553221566
iteration : 5288
train acc:  0.8359375
train loss:  0.3792732059955597
train gradient:  0.2529755546970272
iteration : 5289
train acc:  0.8203125
train loss:  0.4026415944099426
train gradient:  0.31116210254822535
iteration : 5290
train acc:  0.859375
train loss:  0.2711969017982483
train gradient:  0.19862923810337257
iteration : 5291
train acc:  0.859375
train loss:  0.3012021481990814
train gradient:  0.152367810839504
iteration : 5292
train acc:  0.8203125
train loss:  0.3697189688682556
train gradient:  0.22468530006729814
iteration : 5293
train acc:  0.8359375
train loss:  0.34592893719673157
train gradient:  0.31398195795829925
iteration : 5294
train acc:  0.890625
train loss:  0.274018257856369
train gradient:  0.18403795239897278
iteration : 5295
train acc:  0.7734375
train loss:  0.47578907012939453
train gradient:  0.4205894750390057
iteration : 5296
train acc:  0.8203125
train loss:  0.3780900537967682
train gradient:  0.26613821105078744
iteration : 5297
train acc:  0.8515625
train loss:  0.33746057748794556
train gradient:  0.17828664593770924
iteration : 5298
train acc:  0.8515625
train loss:  0.31989729404449463
train gradient:  0.1876483918422459
iteration : 5299
train acc:  0.8046875
train loss:  0.4208167791366577
train gradient:  0.29051216767660043
iteration : 5300
train acc:  0.8671875
train loss:  0.3588307201862335
train gradient:  0.18130525914344459
iteration : 5301
train acc:  0.875
train loss:  0.3018007278442383
train gradient:  0.2957682745165495
iteration : 5302
train acc:  0.84375
train loss:  0.3678824305534363
train gradient:  0.2364886541361329
iteration : 5303
train acc:  0.8515625
train loss:  0.40639951825141907
train gradient:  0.3227799686055297
iteration : 5304
train acc:  0.8203125
train loss:  0.3744046092033386
train gradient:  0.2753823296296555
iteration : 5305
train acc:  0.8515625
train loss:  0.3137251138687134
train gradient:  0.18386565328311563
iteration : 5306
train acc:  0.875
train loss:  0.35588210821151733
train gradient:  0.1569360358107498
iteration : 5307
train acc:  0.8125
train loss:  0.3558967709541321
train gradient:  0.30532072425207024
iteration : 5308
train acc:  0.828125
train loss:  0.43126872181892395
train gradient:  0.34521810560997623
iteration : 5309
train acc:  0.8203125
train loss:  0.40919721126556396
train gradient:  0.3196994974314034
iteration : 5310
train acc:  0.8671875
train loss:  0.35798925161361694
train gradient:  0.4020334772344345
iteration : 5311
train acc:  0.7890625
train loss:  0.4294697046279907
train gradient:  0.5036955966694049
iteration : 5312
train acc:  0.921875
train loss:  0.259012371301651
train gradient:  0.13041898396186158
iteration : 5313
train acc:  0.7734375
train loss:  0.44099414348602295
train gradient:  0.33850875734209507
iteration : 5314
train acc:  0.8125
train loss:  0.3776772618293762
train gradient:  0.3166697255409943
iteration : 5315
train acc:  0.8203125
train loss:  0.38796138763427734
train gradient:  0.36983157899726116
iteration : 5316
train acc:  0.8125
train loss:  0.42045217752456665
train gradient:  0.359322347555345
iteration : 5317
train acc:  0.78125
train loss:  0.38679713010787964
train gradient:  0.2751021642318947
iteration : 5318
train acc:  0.8125
train loss:  0.37328583002090454
train gradient:  0.28509122552237465
iteration : 5319
train acc:  0.890625
train loss:  0.3557623326778412
train gradient:  0.2513546978603531
iteration : 5320
train acc:  0.8359375
train loss:  0.3742362856864929
train gradient:  0.2738259935418157
iteration : 5321
train acc:  0.890625
train loss:  0.2866714596748352
train gradient:  0.1991230971617241
iteration : 5322
train acc:  0.8125
train loss:  0.40024909377098083
train gradient:  0.38091489475206136
iteration : 5323
train acc:  0.84375
train loss:  0.37374475598335266
train gradient:  0.2562614385080007
iteration : 5324
train acc:  0.8671875
train loss:  0.34197330474853516
train gradient:  0.27606840179244047
iteration : 5325
train acc:  0.8359375
train loss:  0.39454907178878784
train gradient:  0.23566728117716848
iteration : 5326
train acc:  0.828125
train loss:  0.40653297305107117
train gradient:  0.2800709658636369
iteration : 5327
train acc:  0.8515625
train loss:  0.32457995414733887
train gradient:  0.263943786367738
iteration : 5328
train acc:  0.8125
train loss:  0.3972812592983246
train gradient:  0.2182792361152514
iteration : 5329
train acc:  0.8515625
train loss:  0.3391404449939728
train gradient:  0.25646268608294537
iteration : 5330
train acc:  0.828125
train loss:  0.4201061725616455
train gradient:  0.3222012331161069
iteration : 5331
train acc:  0.8359375
train loss:  0.32769808173179626
train gradient:  0.1791072371183619
iteration : 5332
train acc:  0.875
train loss:  0.2878376245498657
train gradient:  0.18009288620559094
iteration : 5333
train acc:  0.8828125
train loss:  0.32036158442497253
train gradient:  0.3944094893406877
iteration : 5334
train acc:  0.8671875
train loss:  0.32332056760787964
train gradient:  0.156623090827749
iteration : 5335
train acc:  0.890625
train loss:  0.29498374462127686
train gradient:  0.15529322442252091
iteration : 5336
train acc:  0.8046875
train loss:  0.4345360994338989
train gradient:  0.32814785117190065
iteration : 5337
train acc:  0.8125
train loss:  0.40360602736473083
train gradient:  0.2979734058609121
iteration : 5338
train acc:  0.8359375
train loss:  0.40979403257369995
train gradient:  0.25629782346282043
iteration : 5339
train acc:  0.84375
train loss:  0.33498042821884155
train gradient:  0.24689825765592419
iteration : 5340
train acc:  0.78125
train loss:  0.42970266938209534
train gradient:  0.3786118308605232
iteration : 5341
train acc:  0.859375
train loss:  0.36937639117240906
train gradient:  0.20952429315501384
iteration : 5342
train acc:  0.875
train loss:  0.3565562069416046
train gradient:  0.1942300569950508
iteration : 5343
train acc:  0.8515625
train loss:  0.3692553639411926
train gradient:  0.21331872964069146
iteration : 5344
train acc:  0.8359375
train loss:  0.41269510984420776
train gradient:  0.32150035951881983
iteration : 5345
train acc:  0.8359375
train loss:  0.31586992740631104
train gradient:  0.19009112292286628
iteration : 5346
train acc:  0.8671875
train loss:  0.3140549957752228
train gradient:  0.22826760096367427
iteration : 5347
train acc:  0.8203125
train loss:  0.3583184480667114
train gradient:  0.2680079606935659
iteration : 5348
train acc:  0.8984375
train loss:  0.2852635085582733
train gradient:  0.16146748752660478
iteration : 5349
train acc:  0.8828125
train loss:  0.29833897948265076
train gradient:  0.2020705563356131
iteration : 5350
train acc:  0.8203125
train loss:  0.34538400173187256
train gradient:  0.22833804537127306
iteration : 5351
train acc:  0.7890625
train loss:  0.43682897090911865
train gradient:  0.294778451482089
iteration : 5352
train acc:  0.8828125
train loss:  0.3098461627960205
train gradient:  0.2418930923742817
iteration : 5353
train acc:  0.8671875
train loss:  0.2691101133823395
train gradient:  0.1626569189113384
iteration : 5354
train acc:  0.7734375
train loss:  0.43643414974212646
train gradient:  0.39000621766489935
iteration : 5355
train acc:  0.78125
train loss:  0.420510470867157
train gradient:  0.2625048298301722
iteration : 5356
train acc:  0.8203125
train loss:  0.4150376617908478
train gradient:  0.36233814143518855
iteration : 5357
train acc:  0.828125
train loss:  0.3704691529273987
train gradient:  0.2835876482200146
iteration : 5358
train acc:  0.890625
train loss:  0.31405526399612427
train gradient:  0.15494402022217751
iteration : 5359
train acc:  0.859375
train loss:  0.3430500030517578
train gradient:  0.1633483991883427
iteration : 5360
train acc:  0.8671875
train loss:  0.26924562454223633
train gradient:  0.17929630400183127
iteration : 5361
train acc:  0.875
train loss:  0.3336195647716522
train gradient:  0.17130814261001082
iteration : 5362
train acc:  0.859375
train loss:  0.3135562241077423
train gradient:  0.17440603494351875
iteration : 5363
train acc:  0.8984375
train loss:  0.2967366576194763
train gradient:  0.21633764067233924
iteration : 5364
train acc:  0.828125
train loss:  0.3805840015411377
train gradient:  0.28520387715554224
iteration : 5365
train acc:  0.875
train loss:  0.32436707615852356
train gradient:  0.23419683207350306
iteration : 5366
train acc:  0.8671875
train loss:  0.2860317826271057
train gradient:  0.2709020493723444
iteration : 5367
train acc:  0.828125
train loss:  0.4042942523956299
train gradient:  0.2799535460465588
iteration : 5368
train acc:  0.84375
train loss:  0.358910471200943
train gradient:  0.2278198796030235
iteration : 5369
train acc:  0.890625
train loss:  0.2757743000984192
train gradient:  0.13219160334581004
iteration : 5370
train acc:  0.84375
train loss:  0.35328495502471924
train gradient:  0.2263246651753425
iteration : 5371
train acc:  0.8046875
train loss:  0.41887399554252625
train gradient:  0.37945198010726394
iteration : 5372
train acc:  0.890625
train loss:  0.3132086396217346
train gradient:  0.1868584993529271
iteration : 5373
train acc:  0.890625
train loss:  0.3267415761947632
train gradient:  0.16138763853968457
iteration : 5374
train acc:  0.8203125
train loss:  0.362978458404541
train gradient:  0.21087872839569433
iteration : 5375
train acc:  0.90625
train loss:  0.3230462074279785
train gradient:  0.2142044919812885
iteration : 5376
train acc:  0.8828125
train loss:  0.3135001063346863
train gradient:  0.29478315002580974
iteration : 5377
train acc:  0.7890625
train loss:  0.4363245964050293
train gradient:  0.34704251674634223
iteration : 5378
train acc:  0.8046875
train loss:  0.38393211364746094
train gradient:  0.3820753757402811
iteration : 5379
train acc:  0.90625
train loss:  0.240218847990036
train gradient:  0.20423703147335898
iteration : 5380
train acc:  0.828125
train loss:  0.36020755767822266
train gradient:  0.2310404806449498
iteration : 5381
train acc:  0.8984375
train loss:  0.2748527526855469
train gradient:  0.1339140751728356
iteration : 5382
train acc:  0.875
train loss:  0.2760944366455078
train gradient:  0.2110638334471054
iteration : 5383
train acc:  0.859375
train loss:  0.3121758699417114
train gradient:  0.263417154836125
iteration : 5384
train acc:  0.8046875
train loss:  0.4889256954193115
train gradient:  0.44920480084770187
iteration : 5385
train acc:  0.890625
train loss:  0.34630632400512695
train gradient:  0.3091140118670194
iteration : 5386
train acc:  0.828125
train loss:  0.36523354053497314
train gradient:  0.2988474388482744
iteration : 5387
train acc:  0.8359375
train loss:  0.39335882663726807
train gradient:  0.35216138873152414
iteration : 5388
train acc:  0.875
train loss:  0.3513531982898712
train gradient:  0.19030647050636734
iteration : 5389
train acc:  0.8359375
train loss:  0.3522617518901825
train gradient:  0.2802188652028719
iteration : 5390
train acc:  0.890625
train loss:  0.30241602659225464
train gradient:  0.19023872168917708
iteration : 5391
train acc:  0.8046875
train loss:  0.3940633535385132
train gradient:  0.32235618067828953
iteration : 5392
train acc:  0.828125
train loss:  0.3404710292816162
train gradient:  0.27331324243216776
iteration : 5393
train acc:  0.8671875
train loss:  0.32141435146331787
train gradient:  0.2979266116981936
iteration : 5394
train acc:  0.890625
train loss:  0.2719643712043762
train gradient:  0.17539848453872195
iteration : 5395
train acc:  0.84375
train loss:  0.3252388834953308
train gradient:  0.20231315468808364
iteration : 5396
train acc:  0.8828125
train loss:  0.32205820083618164
train gradient:  0.17934239862796741
iteration : 5397
train acc:  0.84375
train loss:  0.35984447598457336
train gradient:  0.20640753965403996
iteration : 5398
train acc:  0.8046875
train loss:  0.42320895195007324
train gradient:  0.7835816091722421
iteration : 5399
train acc:  0.8828125
train loss:  0.2895285487174988
train gradient:  0.18931362143466307
iteration : 5400
train acc:  0.859375
train loss:  0.34721988439559937
train gradient:  0.32621336135544804
iteration : 5401
train acc:  0.9375
train loss:  0.28362131118774414
train gradient:  0.2107809785326557
iteration : 5402
train acc:  0.8828125
train loss:  0.28197890520095825
train gradient:  0.11729620921984016
iteration : 5403
train acc:  0.8671875
train loss:  0.3406486511230469
train gradient:  0.27249087581224546
iteration : 5404
train acc:  0.8359375
train loss:  0.31667560338974
train gradient:  0.30385017482516247
iteration : 5405
train acc:  0.875
train loss:  0.3390609622001648
train gradient:  0.23707391629550081
iteration : 5406
train acc:  0.8046875
train loss:  0.4100872874259949
train gradient:  0.32333815229654905
iteration : 5407
train acc:  0.8359375
train loss:  0.3609982132911682
train gradient:  0.2308861960439145
iteration : 5408
train acc:  0.828125
train loss:  0.3434666395187378
train gradient:  0.2483465964422628
iteration : 5409
train acc:  0.8671875
train loss:  0.28232887387275696
train gradient:  0.1646066228232653
iteration : 5410
train acc:  0.8828125
train loss:  0.3015064001083374
train gradient:  0.20724241735585372
iteration : 5411
train acc:  0.7734375
train loss:  0.3818357288837433
train gradient:  0.28856439344683166
iteration : 5412
train acc:  0.8203125
train loss:  0.39321452379226685
train gradient:  0.3011837665634472
iteration : 5413
train acc:  0.78125
train loss:  0.39362460374832153
train gradient:  0.26932152546992677
iteration : 5414
train acc:  0.8203125
train loss:  0.3907071650028229
train gradient:  0.2902569177143991
iteration : 5415
train acc:  0.828125
train loss:  0.41421133279800415
train gradient:  0.24527747130475752
iteration : 5416
train acc:  0.890625
train loss:  0.3375861942768097
train gradient:  0.24979943519327052
iteration : 5417
train acc:  0.8828125
train loss:  0.2609323263168335
train gradient:  0.16851327382903747
iteration : 5418
train acc:  0.8671875
train loss:  0.3608589470386505
train gradient:  0.27999419435227074
iteration : 5419
train acc:  0.8671875
train loss:  0.3592422604560852
train gradient:  0.25148123302506703
iteration : 5420
train acc:  0.828125
train loss:  0.3882944881916046
train gradient:  0.2662575510801046
iteration : 5421
train acc:  0.890625
train loss:  0.2896040976047516
train gradient:  0.2159882426016117
iteration : 5422
train acc:  0.859375
train loss:  0.3034171462059021
train gradient:  0.20462708687475784
iteration : 5423
train acc:  0.84375
train loss:  0.34945935010910034
train gradient:  0.25566470190686924
iteration : 5424
train acc:  0.8125
train loss:  0.44344252347946167
train gradient:  0.6121827345971442
iteration : 5425
train acc:  0.8125
train loss:  0.3633478581905365
train gradient:  0.3174935968891818
iteration : 5426
train acc:  0.859375
train loss:  0.3915426731109619
train gradient:  0.22008170765714521
iteration : 5427
train acc:  0.890625
train loss:  0.345344215631485
train gradient:  0.2478439377534415
iteration : 5428
train acc:  0.8984375
train loss:  0.26244890689849854
train gradient:  0.1028810729814896
iteration : 5429
train acc:  0.9140625
train loss:  0.26587045192718506
train gradient:  0.1947905716200536
iteration : 5430
train acc:  0.875
train loss:  0.27036798000335693
train gradient:  0.14235774456869882
iteration : 5431
train acc:  0.890625
train loss:  0.32302385568618774
train gradient:  0.37823586835179585
iteration : 5432
train acc:  0.84375
train loss:  0.36526113748550415
train gradient:  0.3009178146149211
iteration : 5433
train acc:  0.8515625
train loss:  0.32411989569664
train gradient:  0.20030566935867675
iteration : 5434
train acc:  0.796875
train loss:  0.36984509229660034
train gradient:  0.24190363066947995
iteration : 5435
train acc:  0.7890625
train loss:  0.42051661014556885
train gradient:  0.2854288664192961
iteration : 5436
train acc:  0.8515625
train loss:  0.3239256739616394
train gradient:  0.2846969403171091
iteration : 5437
train acc:  0.8203125
train loss:  0.35405850410461426
train gradient:  0.20383085250363941
iteration : 5438
train acc:  0.78125
train loss:  0.4705604314804077
train gradient:  0.33136906709054337
iteration : 5439
train acc:  0.828125
train loss:  0.36480581760406494
train gradient:  0.3674732846576262
iteration : 5440
train acc:  0.8203125
train loss:  0.4088097810745239
train gradient:  0.3944956884697755
iteration : 5441
train acc:  0.859375
train loss:  0.3097139894962311
train gradient:  0.2199332066286792
iteration : 5442
train acc:  0.859375
train loss:  0.3025723695755005
train gradient:  0.19681713524723773
iteration : 5443
train acc:  0.8671875
train loss:  0.3034295439720154
train gradient:  0.16368248756904563
iteration : 5444
train acc:  0.875
train loss:  0.3344517648220062
train gradient:  0.16578635727423582
iteration : 5445
train acc:  0.875
train loss:  0.29750877618789673
train gradient:  0.18473289267068027
iteration : 5446
train acc:  0.8359375
train loss:  0.3642112612724304
train gradient:  0.2807728172181871
iteration : 5447
train acc:  0.828125
train loss:  0.3970750570297241
train gradient:  0.25972489242752794
iteration : 5448
train acc:  0.84375
train loss:  0.341469407081604
train gradient:  0.19347319443357647
iteration : 5449
train acc:  0.8515625
train loss:  0.30035021901130676
train gradient:  0.1661687933843205
iteration : 5450
train acc:  0.84375
train loss:  0.4093690514564514
train gradient:  0.2892985967092117
iteration : 5451
train acc:  0.8203125
train loss:  0.38132017850875854
train gradient:  0.2030007060725294
iteration : 5452
train acc:  0.859375
train loss:  0.3112158179283142
train gradient:  0.20861423182824632
iteration : 5453
train acc:  0.90625
train loss:  0.29545295238494873
train gradient:  0.19975998705451134
iteration : 5454
train acc:  0.828125
train loss:  0.36966538429260254
train gradient:  0.2874906399080082
iteration : 5455
train acc:  0.84375
train loss:  0.31345486640930176
train gradient:  0.1817523209891524
iteration : 5456
train acc:  0.8671875
train loss:  0.3527134954929352
train gradient:  0.22305758246434557
iteration : 5457
train acc:  0.8125
train loss:  0.4833342432975769
train gradient:  0.452514138161648
iteration : 5458
train acc:  0.875
train loss:  0.4046735167503357
train gradient:  0.23890401821720503
iteration : 5459
train acc:  0.875
train loss:  0.31027400493621826
train gradient:  0.21331905423515127
iteration : 5460
train acc:  0.875
train loss:  0.3140878677368164
train gradient:  0.16746236837695863
iteration : 5461
train acc:  0.8359375
train loss:  0.3928965628147125
train gradient:  0.26968339875691716
iteration : 5462
train acc:  0.8125
train loss:  0.32468801736831665
train gradient:  0.23864364217675016
iteration : 5463
train acc:  0.8515625
train loss:  0.3280108869075775
train gradient:  0.2429479073565426
iteration : 5464
train acc:  0.765625
train loss:  0.43995988368988037
train gradient:  0.30353156575214835
iteration : 5465
train acc:  0.84375
train loss:  0.32721322774887085
train gradient:  0.2617847524412713
iteration : 5466
train acc:  0.90625
train loss:  0.2939581871032715
train gradient:  0.16871770831421512
iteration : 5467
train acc:  0.8515625
train loss:  0.2709803581237793
train gradient:  0.19144699955214345
iteration : 5468
train acc:  0.8984375
train loss:  0.27562999725341797
train gradient:  0.12137969337256643
iteration : 5469
train acc:  0.7890625
train loss:  0.41018447279930115
train gradient:  0.3053915311069328
iteration : 5470
train acc:  0.84375
train loss:  0.33072277903556824
train gradient:  0.20153118287735022
iteration : 5471
train acc:  0.8671875
train loss:  0.3079102039337158
train gradient:  0.20357558359168715
iteration : 5472
train acc:  0.8203125
train loss:  0.3835962116718292
train gradient:  0.27871296477139895
iteration : 5473
train acc:  0.84375
train loss:  0.3812728226184845
train gradient:  0.315323447966357
iteration : 5474
train acc:  0.796875
train loss:  0.4336833357810974
train gradient:  0.3155561773302364
iteration : 5475
train acc:  0.8671875
train loss:  0.31802308559417725
train gradient:  0.25519465359002813
iteration : 5476
train acc:  0.8203125
train loss:  0.3421567976474762
train gradient:  0.31897312077214884
iteration : 5477
train acc:  0.8828125
train loss:  0.34459686279296875
train gradient:  0.21571729896917305
iteration : 5478
train acc:  0.8515625
train loss:  0.3651186227798462
train gradient:  0.3248056036563824
iteration : 5479
train acc:  0.859375
train loss:  0.3156576156616211
train gradient:  0.18432835770486547
iteration : 5480
train acc:  0.8671875
train loss:  0.32316139340400696
train gradient:  0.2473115476487443
iteration : 5481
train acc:  0.8828125
train loss:  0.38494813442230225
train gradient:  0.23300078670410365
iteration : 5482
train acc:  0.8515625
train loss:  0.3340977430343628
train gradient:  0.19391443577470938
iteration : 5483
train acc:  0.828125
train loss:  0.37755846977233887
train gradient:  0.3540986570507673
iteration : 5484
train acc:  0.828125
train loss:  0.3720542788505554
train gradient:  0.19509805771370042
iteration : 5485
train acc:  0.796875
train loss:  0.42214709520339966
train gradient:  0.443141180058057
iteration : 5486
train acc:  0.8515625
train loss:  0.3022119104862213
train gradient:  0.2069711206565763
iteration : 5487
train acc:  0.875
train loss:  0.3059227168560028
train gradient:  0.27487023384852305
iteration : 5488
train acc:  0.8828125
train loss:  0.2812221646308899
train gradient:  0.2170965612887532
iteration : 5489
train acc:  0.828125
train loss:  0.3691542148590088
train gradient:  0.2358484853219479
iteration : 5490
train acc:  0.84375
train loss:  0.3311844766139984
train gradient:  0.2102235517586919
iteration : 5491
train acc:  0.84375
train loss:  0.39197906851768494
train gradient:  0.27655674690419363
iteration : 5492
train acc:  0.875
train loss:  0.3008353114128113
train gradient:  0.15121304600600666
iteration : 5493
train acc:  0.875
train loss:  0.32138368487358093
train gradient:  0.2318238369409376
iteration : 5494
train acc:  0.890625
train loss:  0.26649540662765503
train gradient:  0.18689733020466043
iteration : 5495
train acc:  0.875
train loss:  0.323075532913208
train gradient:  0.22707947383735316
iteration : 5496
train acc:  0.78125
train loss:  0.49417200684547424
train gradient:  0.38214534180187015
iteration : 5497
train acc:  0.7734375
train loss:  0.5053400993347168
train gradient:  0.45402400296513906
iteration : 5498
train acc:  0.859375
train loss:  0.33309993147850037
train gradient:  0.22803544324007347
iteration : 5499
train acc:  0.8203125
train loss:  0.4119527339935303
train gradient:  0.2897559170059096
iteration : 5500
train acc:  0.7890625
train loss:  0.3828009366989136
train gradient:  0.2829256184400081
iteration : 5501
train acc:  0.8671875
train loss:  0.2949724495410919
train gradient:  0.30602638209267596
iteration : 5502
train acc:  0.828125
train loss:  0.35370880365371704
train gradient:  0.2359529552796863
iteration : 5503
train acc:  0.859375
train loss:  0.29932135343551636
train gradient:  0.1684475343994696
iteration : 5504
train acc:  0.8046875
train loss:  0.3376575708389282
train gradient:  0.2909413031576381
iteration : 5505
train acc:  0.859375
train loss:  0.3151501715183258
train gradient:  0.26289915658753127
iteration : 5506
train acc:  0.8828125
train loss:  0.3511091470718384
train gradient:  0.41815720658333183
iteration : 5507
train acc:  0.8515625
train loss:  0.3306308090686798
train gradient:  0.2441485001751141
iteration : 5508
train acc:  0.8671875
train loss:  0.33685582876205444
train gradient:  0.2833588595336333
iteration : 5509
train acc:  0.8125
train loss:  0.3757169842720032
train gradient:  0.33784864264655556
iteration : 5510
train acc:  0.8203125
train loss:  0.38321834802627563
train gradient:  0.3226109790204177
iteration : 5511
train acc:  0.890625
train loss:  0.28567570447921753
train gradient:  0.19341736137987278
iteration : 5512
train acc:  0.8828125
train loss:  0.2806088328361511
train gradient:  0.18170649431948266
iteration : 5513
train acc:  0.828125
train loss:  0.436832070350647
train gradient:  0.28749822782467893
iteration : 5514
train acc:  0.8046875
train loss:  0.3918842375278473
train gradient:  0.29589664759034184
iteration : 5515
train acc:  0.8671875
train loss:  0.35700342059135437
train gradient:  0.20644545179366852
iteration : 5516
train acc:  0.84375
train loss:  0.3333561420440674
train gradient:  0.18204578948836775
iteration : 5517
train acc:  0.828125
train loss:  0.35173070430755615
train gradient:  0.22396139045321306
iteration : 5518
train acc:  0.765625
train loss:  0.39606690406799316
train gradient:  0.3460355139504036
iteration : 5519
train acc:  0.8125
train loss:  0.4061681926250458
train gradient:  0.295969736729203
iteration : 5520
train acc:  0.8828125
train loss:  0.30421897768974304
train gradient:  0.2250118250685728
iteration : 5521
train acc:  0.859375
train loss:  0.3464081585407257
train gradient:  0.2132954798671374
iteration : 5522
train acc:  0.828125
train loss:  0.4224475026130676
train gradient:  0.43752810628960764
iteration : 5523
train acc:  0.796875
train loss:  0.42653337121009827
train gradient:  0.22450240910213115
iteration : 5524
train acc:  0.8203125
train loss:  0.40314149856567383
train gradient:  0.27777926605053277
iteration : 5525
train acc:  0.8828125
train loss:  0.31367313861846924
train gradient:  0.18028338664352678
iteration : 5526
train acc:  0.8671875
train loss:  0.36400505900382996
train gradient:  0.19810179583750753
iteration : 5527
train acc:  0.8671875
train loss:  0.30842652916908264
train gradient:  0.23203941454671562
iteration : 5528
train acc:  0.8359375
train loss:  0.32332780957221985
train gradient:  0.16481925019546734
iteration : 5529
train acc:  0.8671875
train loss:  0.26626548171043396
train gradient:  0.14534367193352005
iteration : 5530
train acc:  0.875
train loss:  0.2990323305130005
train gradient:  0.18249164645870264
iteration : 5531
train acc:  0.8515625
train loss:  0.36306363344192505
train gradient:  0.18765814602340544
iteration : 5532
train acc:  0.84375
train loss:  0.37705475091934204
train gradient:  0.23497354072435653
iteration : 5533
train acc:  0.8671875
train loss:  0.350009948015213
train gradient:  0.19590245147157448
iteration : 5534
train acc:  0.8515625
train loss:  0.37107133865356445
train gradient:  0.18825688720324388
iteration : 5535
train acc:  0.84375
train loss:  0.36791282892227173
train gradient:  0.19068982910843754
iteration : 5536
train acc:  0.8671875
train loss:  0.2937160134315491
train gradient:  0.2342992238255804
iteration : 5537
train acc:  0.8671875
train loss:  0.3272083103656769
train gradient:  0.17616212206062623
iteration : 5538
train acc:  0.875
train loss:  0.33086147904396057
train gradient:  0.16883509713569356
iteration : 5539
train acc:  0.8203125
train loss:  0.36559802293777466
train gradient:  0.2279406874321801
iteration : 5540
train acc:  0.8125
train loss:  0.372922420501709
train gradient:  0.2134279923027712
iteration : 5541
train acc:  0.875
train loss:  0.36534884572029114
train gradient:  0.283011701599218
iteration : 5542
train acc:  0.890625
train loss:  0.3084396719932556
train gradient:  0.20225518569115497
iteration : 5543
train acc:  0.84375
train loss:  0.3760526180267334
train gradient:  0.20325818317773603
iteration : 5544
train acc:  0.8046875
train loss:  0.5090337991714478
train gradient:  0.3304877320989629
iteration : 5545
train acc:  0.875
train loss:  0.2720842659473419
train gradient:  0.15717713803046476
iteration : 5546
train acc:  0.8671875
train loss:  0.3390485942363739
train gradient:  0.20485309430865456
iteration : 5547
train acc:  0.8515625
train loss:  0.3577924966812134
train gradient:  0.23520753518266016
iteration : 5548
train acc:  0.84375
train loss:  0.3757096529006958
train gradient:  0.259908410979318
iteration : 5549
train acc:  0.9140625
train loss:  0.25082385540008545
train gradient:  0.161099140461073
iteration : 5550
train acc:  0.7890625
train loss:  0.38651013374328613
train gradient:  0.23911251449730003
iteration : 5551
train acc:  0.8359375
train loss:  0.3591765761375427
train gradient:  0.22183724026422985
iteration : 5552
train acc:  0.859375
train loss:  0.28977084159851074
train gradient:  0.1824457668683314
iteration : 5553
train acc:  0.8359375
train loss:  0.37006062269210815
train gradient:  0.31817917605019047
iteration : 5554
train acc:  0.8125
train loss:  0.39434561133384705
train gradient:  0.2355776076337195
iteration : 5555
train acc:  0.8515625
train loss:  0.3624690771102905
train gradient:  0.24550978463940504
iteration : 5556
train acc:  0.796875
train loss:  0.4659273028373718
train gradient:  0.3275002130659851
iteration : 5557
train acc:  0.8515625
train loss:  0.3087676167488098
train gradient:  0.2505126058613256
iteration : 5558
train acc:  0.828125
train loss:  0.44816797971725464
train gradient:  0.3100219707279738
iteration : 5559
train acc:  0.8203125
train loss:  0.38591355085372925
train gradient:  0.30676962458974016
iteration : 5560
train acc:  0.8515625
train loss:  0.3522081673145294
train gradient:  0.19720426126619325
iteration : 5561
train acc:  0.84375
train loss:  0.34466540813446045
train gradient:  0.17933031667122673
iteration : 5562
train acc:  0.875
train loss:  0.31061413884162903
train gradient:  0.19688183273289034
iteration : 5563
train acc:  0.8359375
train loss:  0.3318820595741272
train gradient:  0.19447831149171615
iteration : 5564
train acc:  0.84375
train loss:  0.3442319929599762
train gradient:  0.22031753749248345
iteration : 5565
train acc:  0.8984375
train loss:  0.30671635270118713
train gradient:  0.1878413022611085
iteration : 5566
train acc:  0.875
train loss:  0.3279014825820923
train gradient:  0.26824172337824176
iteration : 5567
train acc:  0.8046875
train loss:  0.38869208097457886
train gradient:  0.2877604641597634
iteration : 5568
train acc:  0.828125
train loss:  0.37385910749435425
train gradient:  0.2566016381072068
iteration : 5569
train acc:  0.875
train loss:  0.34017205238342285
train gradient:  0.23062790522966814
iteration : 5570
train acc:  0.8515625
train loss:  0.2832600772380829
train gradient:  0.16394155765145052
iteration : 5571
train acc:  0.84375
train loss:  0.3235915005207062
train gradient:  0.16248177391945284
iteration : 5572
train acc:  0.9140625
train loss:  0.2564009726047516
train gradient:  0.1112565572992875
iteration : 5573
train acc:  0.84375
train loss:  0.3289884030818939
train gradient:  0.26068889350035773
iteration : 5574
train acc:  0.859375
train loss:  0.3115549087524414
train gradient:  0.25687329275847426
iteration : 5575
train acc:  0.90625
train loss:  0.26190194487571716
train gradient:  0.1877609253187531
iteration : 5576
train acc:  0.828125
train loss:  0.36299681663513184
train gradient:  0.20689308360745967
iteration : 5577
train acc:  0.859375
train loss:  0.3353111743927002
train gradient:  0.20758197588504337
iteration : 5578
train acc:  0.8125
train loss:  0.39116477966308594
train gradient:  0.5850019683847545
iteration : 5579
train acc:  0.859375
train loss:  0.336164653301239
train gradient:  0.4293635478618435
iteration : 5580
train acc:  0.859375
train loss:  0.36443573236465454
train gradient:  0.21680878017732205
iteration : 5581
train acc:  0.8515625
train loss:  0.37733832001686096
train gradient:  0.2550157132949653
iteration : 5582
train acc:  0.796875
train loss:  0.37460553646087646
train gradient:  0.2999860043392137
iteration : 5583
train acc:  0.8828125
train loss:  0.29436853528022766
train gradient:  0.20675093563397365
iteration : 5584
train acc:  0.8828125
train loss:  0.3018169403076172
train gradient:  0.21966042889083032
iteration : 5585
train acc:  0.875
train loss:  0.37468209862709045
train gradient:  0.19296572470763465
iteration : 5586
train acc:  0.8515625
train loss:  0.3181290626525879
train gradient:  0.20656617219554324
iteration : 5587
train acc:  0.828125
train loss:  0.3969085216522217
train gradient:  0.2345529464825587
iteration : 5588
train acc:  0.8359375
train loss:  0.4039982557296753
train gradient:  0.3290724297306331
iteration : 5589
train acc:  0.859375
train loss:  0.3039310574531555
train gradient:  0.18623155353154944
iteration : 5590
train acc:  0.8203125
train loss:  0.4257529675960541
train gradient:  0.33080998974416426
iteration : 5591
train acc:  0.8359375
train loss:  0.326602578163147
train gradient:  0.24708999880634874
iteration : 5592
train acc:  0.8828125
train loss:  0.3215509355068207
train gradient:  0.20337430812368812
iteration : 5593
train acc:  0.8359375
train loss:  0.36275383830070496
train gradient:  0.3680504592469283
iteration : 5594
train acc:  0.8125
train loss:  0.389040470123291
train gradient:  0.3521879781060823
iteration : 5595
train acc:  0.8359375
train loss:  0.35903218388557434
train gradient:  0.26158036436392257
iteration : 5596
train acc:  0.875
train loss:  0.2813810706138611
train gradient:  0.14824276210166754
iteration : 5597
train acc:  0.8515625
train loss:  0.37055081129074097
train gradient:  0.4237473250138807
iteration : 5598
train acc:  0.8359375
train loss:  0.33468955755233765
train gradient:  0.2733100941235213
iteration : 5599
train acc:  0.84375
train loss:  0.3346060812473297
train gradient:  0.40021344684771004
iteration : 5600
train acc:  0.7890625
train loss:  0.4329645037651062
train gradient:  0.3063079499905842
iteration : 5601
train acc:  0.796875
train loss:  0.41744035482406616
train gradient:  0.3755173295361341
iteration : 5602
train acc:  0.859375
train loss:  0.3145180940628052
train gradient:  0.2568611790758294
iteration : 5603
train acc:  0.8203125
train loss:  0.40606364607810974
train gradient:  0.36046091780778267
iteration : 5604
train acc:  0.8046875
train loss:  0.37112218141555786
train gradient:  0.2363284998134697
iteration : 5605
train acc:  0.828125
train loss:  0.38867515325546265
train gradient:  0.26978755398445453
iteration : 5606
train acc:  0.875
train loss:  0.3052400052547455
train gradient:  0.16365220110609643
iteration : 5607
train acc:  0.78125
train loss:  0.45426103472709656
train gradient:  0.40135507921975105
iteration : 5608
train acc:  0.8984375
train loss:  0.3092164397239685
train gradient:  0.20249568444666088
iteration : 5609
train acc:  0.8359375
train loss:  0.30561837553977966
train gradient:  0.22273854987367883
iteration : 5610
train acc:  0.890625
train loss:  0.2705037295818329
train gradient:  0.1341819015844608
iteration : 5611
train acc:  0.90625
train loss:  0.3117704689502716
train gradient:  0.26503710362521016
iteration : 5612
train acc:  0.8828125
train loss:  0.31158214807510376
train gradient:  0.14601032244936343
iteration : 5613
train acc:  0.8984375
train loss:  0.28012901544570923
train gradient:  0.19315360539478182
iteration : 5614
train acc:  0.7890625
train loss:  0.3946149945259094
train gradient:  0.3323099209403745
iteration : 5615
train acc:  0.7890625
train loss:  0.4357227683067322
train gradient:  0.26436926282848217
iteration : 5616
train acc:  0.828125
train loss:  0.33386144042015076
train gradient:  0.2749534117337765
iteration : 5617
train acc:  0.859375
train loss:  0.3080325722694397
train gradient:  0.2801860271123314
iteration : 5618
train acc:  0.859375
train loss:  0.33673474192619324
train gradient:  0.24894944119165605
iteration : 5619
train acc:  0.859375
train loss:  0.3274862468242645
train gradient:  0.2802472649964021
iteration : 5620
train acc:  0.859375
train loss:  0.34204551577568054
train gradient:  0.23297873351872905
iteration : 5621
train acc:  0.84375
train loss:  0.4292573928833008
train gradient:  0.2918528401491399
iteration : 5622
train acc:  0.8671875
train loss:  0.2807483971118927
train gradient:  0.18130658303530472
iteration : 5623
train acc:  0.859375
train loss:  0.39326727390289307
train gradient:  0.19496976124538612
iteration : 5624
train acc:  0.8203125
train loss:  0.3665192723274231
train gradient:  0.22051691741547902
iteration : 5625
train acc:  0.8828125
train loss:  0.3247722387313843
train gradient:  0.21075972181806268
iteration : 5626
train acc:  0.78125
train loss:  0.43871009349823
train gradient:  0.3411028016695087
iteration : 5627
train acc:  0.8671875
train loss:  0.3198156952857971
train gradient:  0.20071327955467727
iteration : 5628
train acc:  0.859375
train loss:  0.3536199927330017
train gradient:  0.211226575384032
iteration : 5629
train acc:  0.859375
train loss:  0.3300229609012604
train gradient:  0.22611550368558525
iteration : 5630
train acc:  0.7578125
train loss:  0.46166449785232544
train gradient:  0.36904493813960837
iteration : 5631
train acc:  0.796875
train loss:  0.40181344747543335
train gradient:  0.3038313641317136
iteration : 5632
train acc:  0.9140625
train loss:  0.28087615966796875
train gradient:  0.18654535994986962
iteration : 5633
train acc:  0.84375
train loss:  0.29471272230148315
train gradient:  0.1611068297762262
iteration : 5634
train acc:  0.859375
train loss:  0.394487202167511
train gradient:  0.401956964676853
iteration : 5635
train acc:  0.8359375
train loss:  0.3468533456325531
train gradient:  0.23087212570291127
iteration : 5636
train acc:  0.8046875
train loss:  0.435677707195282
train gradient:  0.3784648470649286
iteration : 5637
train acc:  0.8515625
train loss:  0.3315986394882202
train gradient:  0.28870231183617623
iteration : 5638
train acc:  0.84375
train loss:  0.345244824886322
train gradient:  0.24166331887435305
iteration : 5639
train acc:  0.8203125
train loss:  0.38254112005233765
train gradient:  0.19583691871380263
iteration : 5640
train acc:  0.796875
train loss:  0.4412239193916321
train gradient:  0.31955068482875826
iteration : 5641
train acc:  0.8671875
train loss:  0.36020147800445557
train gradient:  0.202843541640698
iteration : 5642
train acc:  0.8515625
train loss:  0.3583489656448364
train gradient:  0.4578238513974264
iteration : 5643
train acc:  0.8515625
train loss:  0.36499351263046265
train gradient:  0.2535664400842001
iteration : 5644
train acc:  0.875
train loss:  0.3868217170238495
train gradient:  0.294418789397055
iteration : 5645
train acc:  0.8984375
train loss:  0.2696487009525299
train gradient:  0.2986795557437014
iteration : 5646
train acc:  0.859375
train loss:  0.3384987711906433
train gradient:  0.19573791500214538
iteration : 5647
train acc:  0.8125
train loss:  0.4135953187942505
train gradient:  0.22307448389086207
iteration : 5648
train acc:  0.875
train loss:  0.3446372449398041
train gradient:  0.2907217236173988
iteration : 5649
train acc:  0.859375
train loss:  0.3251594007015228
train gradient:  0.1823013539026556
iteration : 5650
train acc:  0.8359375
train loss:  0.3515983521938324
train gradient:  0.25400548200817874
iteration : 5651
train acc:  0.8515625
train loss:  0.36711591482162476
train gradient:  0.2158161043243158
iteration : 5652
train acc:  0.8359375
train loss:  0.368327796459198
train gradient:  0.21813895251137658
iteration : 5653
train acc:  0.8125
train loss:  0.3586631417274475
train gradient:  0.19076366619328441
iteration : 5654
train acc:  0.8203125
train loss:  0.3749126195907593
train gradient:  0.2011621623345008
iteration : 5655
train acc:  0.796875
train loss:  0.4103182852268219
train gradient:  0.32383477209307815
iteration : 5656
train acc:  0.828125
train loss:  0.3577653169631958
train gradient:  0.23826928012273524
iteration : 5657
train acc:  0.90625
train loss:  0.3267131447792053
train gradient:  0.30368263589185623
iteration : 5658
train acc:  0.8515625
train loss:  0.37574270367622375
train gradient:  0.3075132116470588
iteration : 5659
train acc:  0.828125
train loss:  0.42347216606140137
train gradient:  0.26956149957452336
iteration : 5660
train acc:  0.8125
train loss:  0.347265362739563
train gradient:  0.22418865240348684
iteration : 5661
train acc:  0.8671875
train loss:  0.3242991864681244
train gradient:  0.3706968243975892
iteration : 5662
train acc:  0.84375
train loss:  0.38745149970054626
train gradient:  0.25320063642023377
iteration : 5663
train acc:  0.8671875
train loss:  0.3110390901565552
train gradient:  0.3167375393394164
iteration : 5664
train acc:  0.921875
train loss:  0.28028416633605957
train gradient:  0.15148265685806656
iteration : 5665
train acc:  0.8125
train loss:  0.3843361437320709
train gradient:  0.2362132072893086
iteration : 5666
train acc:  0.875
train loss:  0.3262784481048584
train gradient:  0.1765206373503684
iteration : 5667
train acc:  0.78125
train loss:  0.6053065061569214
train gradient:  0.5344198929905823
iteration : 5668
train acc:  0.84375
train loss:  0.42755115032196045
train gradient:  0.3657572310352728
iteration : 5669
train acc:  0.90625
train loss:  0.29523754119873047
train gradient:  0.15770041504509977
iteration : 5670
train acc:  0.8203125
train loss:  0.4091857671737671
train gradient:  0.24017459616407166
iteration : 5671
train acc:  0.8515625
train loss:  0.3605543375015259
train gradient:  0.17344231691796574
iteration : 5672
train acc:  0.8515625
train loss:  0.32145488262176514
train gradient:  0.13530204351192002
iteration : 5673
train acc:  0.7734375
train loss:  0.45472872257232666
train gradient:  0.3057608807137377
iteration : 5674
train acc:  0.8984375
train loss:  0.2621735632419586
train gradient:  0.1581444301152231
iteration : 5675
train acc:  0.8046875
train loss:  0.3937183916568756
train gradient:  0.2525212112338463
iteration : 5676
train acc:  0.8984375
train loss:  0.31291669607162476
train gradient:  0.1459693118921715
iteration : 5677
train acc:  0.8359375
train loss:  0.41102689504623413
train gradient:  0.2130415814667763
iteration : 5678
train acc:  0.8515625
train loss:  0.35751569271087646
train gradient:  0.24648136709452695
iteration : 5679
train acc:  0.8671875
train loss:  0.3048091530799866
train gradient:  0.15175192293281162
iteration : 5680
train acc:  0.8515625
train loss:  0.322907030582428
train gradient:  0.1761647241080781
iteration : 5681
train acc:  0.8671875
train loss:  0.36443766951560974
train gradient:  0.22106505202392396
iteration : 5682
train acc:  0.859375
train loss:  0.34635794162750244
train gradient:  0.20629129908242527
iteration : 5683
train acc:  0.8359375
train loss:  0.4186311364173889
train gradient:  0.3189059095977082
iteration : 5684
train acc:  0.859375
train loss:  0.32486820220947266
train gradient:  0.2244741537191295
iteration : 5685
train acc:  0.84375
train loss:  0.3797381818294525
train gradient:  0.2824812982754231
iteration : 5686
train acc:  0.828125
train loss:  0.3841392993927002
train gradient:  0.36644017105841076
iteration : 5687
train acc:  0.8046875
train loss:  0.3881942331790924
train gradient:  0.33586435393230113
iteration : 5688
train acc:  0.8046875
train loss:  0.3707394003868103
train gradient:  0.36403801518714796
iteration : 5689
train acc:  0.796875
train loss:  0.3933698534965515
train gradient:  0.33126971676720185
iteration : 5690
train acc:  0.8515625
train loss:  0.3283534049987793
train gradient:  0.2338996400822009
iteration : 5691
train acc:  0.828125
train loss:  0.36584651470184326
train gradient:  0.29853832971509653
iteration : 5692
train acc:  0.828125
train loss:  0.3598432242870331
train gradient:  0.26761751448639665
iteration : 5693
train acc:  0.796875
train loss:  0.4396686851978302
train gradient:  0.30712281724993723
iteration : 5694
train acc:  0.8203125
train loss:  0.3874390125274658
train gradient:  0.26643006989322965
iteration : 5695
train acc:  0.84375
train loss:  0.31794771552085876
train gradient:  0.18296774352470396
iteration : 5696
train acc:  0.8359375
train loss:  0.32185691595077515
train gradient:  0.26230772603381125
iteration : 5697
train acc:  0.8046875
train loss:  0.3735487163066864
train gradient:  0.2022189655325073
iteration : 5698
train acc:  0.859375
train loss:  0.3008890748023987
train gradient:  0.2032247990359485
iteration : 5699
train acc:  0.859375
train loss:  0.326047420501709
train gradient:  0.23781807091904486
iteration : 5700
train acc:  0.75
train loss:  0.4305216670036316
train gradient:  0.32405437946116045
iteration : 5701
train acc:  0.875
train loss:  0.364940881729126
train gradient:  0.2857129915268167
iteration : 5702
train acc:  0.828125
train loss:  0.35538339614868164
train gradient:  0.2417895443594777
iteration : 5703
train acc:  0.8671875
train loss:  0.3134045898914337
train gradient:  0.21352499512255124
iteration : 5704
train acc:  0.8203125
train loss:  0.4303514361381531
train gradient:  0.2904933312963347
iteration : 5705
train acc:  0.8046875
train loss:  0.3541818857192993
train gradient:  0.28264629820629833
iteration : 5706
train acc:  0.8359375
train loss:  0.407005250453949
train gradient:  0.25233380911743164
iteration : 5707
train acc:  0.859375
train loss:  0.31313779950141907
train gradient:  0.17161843450563702
iteration : 5708
train acc:  0.84375
train loss:  0.3577294945716858
train gradient:  0.20013258328563668
iteration : 5709
train acc:  0.8125
train loss:  0.4519989788532257
train gradient:  0.4270858258650452
iteration : 5710
train acc:  0.8828125
train loss:  0.2644844055175781
train gradient:  0.14926837594566686
iteration : 5711
train acc:  0.8984375
train loss:  0.3130117654800415
train gradient:  0.18111665698055712
iteration : 5712
train acc:  0.859375
train loss:  0.3240200877189636
train gradient:  0.16717987423804231
iteration : 5713
train acc:  0.8828125
train loss:  0.355762243270874
train gradient:  0.1783488813379832
iteration : 5714
train acc:  0.8828125
train loss:  0.3094862699508667
train gradient:  0.19057752236717673
iteration : 5715
train acc:  0.84375
train loss:  0.38887763023376465
train gradient:  0.26045891717726494
iteration : 5716
train acc:  0.8203125
train loss:  0.38980332016944885
train gradient:  0.4604619063758744
iteration : 5717
train acc:  0.8515625
train loss:  0.34329524636268616
train gradient:  0.2025260457786228
iteration : 5718
train acc:  0.875
train loss:  0.27022993564605713
train gradient:  0.18292948771616566
iteration : 5719
train acc:  0.828125
train loss:  0.3811165988445282
train gradient:  0.21392279000894754
iteration : 5720
train acc:  0.8671875
train loss:  0.3495742678642273
train gradient:  0.23740963093283274
iteration : 5721
train acc:  0.8203125
train loss:  0.37343254685401917
train gradient:  0.2326867372730112
iteration : 5722
train acc:  0.828125
train loss:  0.375663161277771
train gradient:  0.2782235078074733
iteration : 5723
train acc:  0.875
train loss:  0.30179840326309204
train gradient:  0.33759814528141796
iteration : 5724
train acc:  0.8203125
train loss:  0.34786558151245117
train gradient:  0.21881344701126404
iteration : 5725
train acc:  0.828125
train loss:  0.4127998650074005
train gradient:  0.3341522187319553
iteration : 5726
train acc:  0.875
train loss:  0.2822147607803345
train gradient:  0.15852702918186373
iteration : 5727
train acc:  0.8515625
train loss:  0.40450066328048706
train gradient:  0.24736848036652953
iteration : 5728
train acc:  0.890625
train loss:  0.28513601422309875
train gradient:  0.20912198906932716
iteration : 5729
train acc:  0.8515625
train loss:  0.4141560196876526
train gradient:  0.35598479962811536
iteration : 5730
train acc:  0.828125
train loss:  0.3479062616825104
train gradient:  0.3603707233311198
iteration : 5731
train acc:  0.84375
train loss:  0.367031455039978
train gradient:  0.27046201765383204
iteration : 5732
train acc:  0.8359375
train loss:  0.3739506006240845
train gradient:  0.1914756224876326
iteration : 5733
train acc:  0.9140625
train loss:  0.2436588704586029
train gradient:  0.1374865167578262
iteration : 5734
train acc:  0.84375
train loss:  0.31272101402282715
train gradient:  0.15904171260250582
iteration : 5735
train acc:  0.84375
train loss:  0.3883447051048279
train gradient:  0.2271441968707092
iteration : 5736
train acc:  0.828125
train loss:  0.4018636643886566
train gradient:  0.29616322412583374
iteration : 5737
train acc:  0.828125
train loss:  0.36762142181396484
train gradient:  0.26976079266653274
iteration : 5738
train acc:  0.828125
train loss:  0.37994253635406494
train gradient:  0.26874696249194296
iteration : 5739
train acc:  0.84375
train loss:  0.3181740641593933
train gradient:  0.18803354812048742
iteration : 5740
train acc:  0.875
train loss:  0.33095890283584595
train gradient:  0.25188549101500024
iteration : 5741
train acc:  0.921875
train loss:  0.2959458827972412
train gradient:  0.16126603590032543
iteration : 5742
train acc:  0.84375
train loss:  0.3474987745285034
train gradient:  0.18022443491907947
iteration : 5743
train acc:  0.890625
train loss:  0.3303473889827728
train gradient:  0.2343774672845009
iteration : 5744
train acc:  0.8125
train loss:  0.43856218457221985
train gradient:  0.297031963907759
iteration : 5745
train acc:  0.8046875
train loss:  0.4096357822418213
train gradient:  0.25373009373857197
iteration : 5746
train acc:  0.8671875
train loss:  0.4075966775417328
train gradient:  0.21616465140382307
iteration : 5747
train acc:  0.84375
train loss:  0.33415454626083374
train gradient:  0.18249819058234826
iteration : 5748
train acc:  0.7890625
train loss:  0.38259589672088623
train gradient:  0.5067616273865443
iteration : 5749
train acc:  0.8984375
train loss:  0.31029170751571655
train gradient:  0.19639466277663498
iteration : 5750
train acc:  0.8984375
train loss:  0.2626197338104248
train gradient:  0.16940763420163266
iteration : 5751
train acc:  0.8671875
train loss:  0.31799983978271484
train gradient:  0.29739582722119795
iteration : 5752
train acc:  0.859375
train loss:  0.3337307274341583
train gradient:  0.29453783207104517
iteration : 5753
train acc:  0.828125
train loss:  0.3571307063102722
train gradient:  0.28938770494040356
iteration : 5754
train acc:  0.828125
train loss:  0.40636947751045227
train gradient:  0.3045566663253697
iteration : 5755
train acc:  0.8515625
train loss:  0.32620519399642944
train gradient:  0.2159908397551231
iteration : 5756
train acc:  0.796875
train loss:  0.45735448598861694
train gradient:  0.3731192157294684
iteration : 5757
train acc:  0.7734375
train loss:  0.4633067846298218
train gradient:  0.4306888673719119
iteration : 5758
train acc:  0.8828125
train loss:  0.3858950436115265
train gradient:  0.23166675718023017
iteration : 5759
train acc:  0.828125
train loss:  0.3666560649871826
train gradient:  0.5324955840256137
iteration : 5760
train acc:  0.859375
train loss:  0.31841862201690674
train gradient:  0.24802402149770564
iteration : 5761
train acc:  0.8359375
train loss:  0.4022560119628906
train gradient:  0.2518477896649695
iteration : 5762
train acc:  0.8359375
train loss:  0.3806644082069397
train gradient:  0.2672726825672607
iteration : 5763
train acc:  0.8203125
train loss:  0.4132006764411926
train gradient:  0.26062862677903015
iteration : 5764
train acc:  0.796875
train loss:  0.37863901257514954
train gradient:  0.21315156716598696
iteration : 5765
train acc:  0.84375
train loss:  0.31188708543777466
train gradient:  0.20962763082802116
iteration : 5766
train acc:  0.875
train loss:  0.3066158890724182
train gradient:  0.1435939991435271
iteration : 5767
train acc:  0.8828125
train loss:  0.3149246573448181
train gradient:  0.19513557232572573
iteration : 5768
train acc:  0.8203125
train loss:  0.35689109563827515
train gradient:  0.18928413974528152
iteration : 5769
train acc:  0.875
train loss:  0.3193417191505432
train gradient:  0.14382053240606235
iteration : 5770
train acc:  0.8515625
train loss:  0.35729795694351196
train gradient:  0.1961392422289719
iteration : 5771
train acc:  0.84375
train loss:  0.348787784576416
train gradient:  0.37998599312725445
iteration : 5772
train acc:  0.8359375
train loss:  0.33565351366996765
train gradient:  0.1755481400533005
iteration : 5773
train acc:  0.859375
train loss:  0.31971779465675354
train gradient:  0.1768620628693066
iteration : 5774
train acc:  0.8515625
train loss:  0.343608021736145
train gradient:  0.22866399034593698
iteration : 5775
train acc:  0.765625
train loss:  0.45548269152641296
train gradient:  0.29502566926676804
iteration : 5776
train acc:  0.8125
train loss:  0.3794238567352295
train gradient:  0.2881160550303965
iteration : 5777
train acc:  0.8828125
train loss:  0.29307061433792114
train gradient:  0.17635320894227646
iteration : 5778
train acc:  0.8828125
train loss:  0.26883912086486816
train gradient:  0.12876761278635834
iteration : 5779
train acc:  0.8671875
train loss:  0.3668470084667206
train gradient:  0.2575294538180633
iteration : 5780
train acc:  0.84375
train loss:  0.28689590096473694
train gradient:  0.2556302245115178
iteration : 5781
train acc:  0.8515625
train loss:  0.3901541829109192
train gradient:  0.2799072336197358
iteration : 5782
train acc:  0.8203125
train loss:  0.40791937708854675
train gradient:  0.30683632545283784
iteration : 5783
train acc:  0.8515625
train loss:  0.3433767259120941
train gradient:  0.24082908414154736
iteration : 5784
train acc:  0.8203125
train loss:  0.3295820951461792
train gradient:  0.2216158134203955
iteration : 5785
train acc:  0.90625
train loss:  0.2598254680633545
train gradient:  0.1841259108918602
iteration : 5786
train acc:  0.8984375
train loss:  0.25796931982040405
train gradient:  0.166921070175778
iteration : 5787
train acc:  0.8359375
train loss:  0.35713186860084534
train gradient:  0.2614454929802481
iteration : 5788
train acc:  0.890625
train loss:  0.27330148220062256
train gradient:  0.19569654046729634
iteration : 5789
train acc:  0.8203125
train loss:  0.43859338760375977
train gradient:  0.40115182987099995
iteration : 5790
train acc:  0.859375
train loss:  0.3498203754425049
train gradient:  0.17188378614458788
iteration : 5791
train acc:  0.8515625
train loss:  0.3312436640262604
train gradient:  0.1745686668951379
iteration : 5792
train acc:  0.84375
train loss:  0.35535353422164917
train gradient:  0.21615591602305662
iteration : 5793
train acc:  0.875
train loss:  0.3011607825756073
train gradient:  0.13746735501925766
iteration : 5794
train acc:  0.8515625
train loss:  0.33702653646469116
train gradient:  0.16800800645972624
iteration : 5795
train acc:  0.875
train loss:  0.2984941899776459
train gradient:  0.20343056367576917
iteration : 5796
train acc:  0.859375
train loss:  0.34884706139564514
train gradient:  0.27039990784969464
iteration : 5797
train acc:  0.859375
train loss:  0.3971254825592041
train gradient:  0.266123076328689
iteration : 5798
train acc:  0.84375
train loss:  0.3218211531639099
train gradient:  0.21884347378840055
iteration : 5799
train acc:  0.84375
train loss:  0.3613668978214264
train gradient:  0.29039039184418514
iteration : 5800
train acc:  0.7890625
train loss:  0.4473194181919098
train gradient:  0.42098004102369485
iteration : 5801
train acc:  0.8046875
train loss:  0.3458474278450012
train gradient:  0.2576177072116395
iteration : 5802
train acc:  0.890625
train loss:  0.3388024568557739
train gradient:  0.2028264215573574
iteration : 5803
train acc:  0.84375
train loss:  0.31159350275993347
train gradient:  0.19476000987692663
iteration : 5804
train acc:  0.828125
train loss:  0.351453959941864
train gradient:  0.15322946846356306
iteration : 5805
train acc:  0.9140625
train loss:  0.25019371509552
train gradient:  0.14777932038264185
iteration : 5806
train acc:  0.8515625
train loss:  0.3442884087562561
train gradient:  0.1701638330180228
iteration : 5807
train acc:  0.8515625
train loss:  0.3148302733898163
train gradient:  0.23992889211433277
iteration : 5808
train acc:  0.890625
train loss:  0.2661844789981842
train gradient:  0.16533956985553744
iteration : 5809
train acc:  0.796875
train loss:  0.39439263939857483
train gradient:  0.22347264082586835
iteration : 5810
train acc:  0.8359375
train loss:  0.34218356013298035
train gradient:  0.17831436395183892
iteration : 5811
train acc:  0.8125
train loss:  0.39630722999572754
train gradient:  0.33816894456663543
iteration : 5812
train acc:  0.8046875
train loss:  0.3692108690738678
train gradient:  0.2605463629650341
iteration : 5813
train acc:  0.8671875
train loss:  0.2974916398525238
train gradient:  0.144923626423318
iteration : 5814
train acc:  0.875
train loss:  0.31914663314819336
train gradient:  0.3265578804990845
iteration : 5815
train acc:  0.84375
train loss:  0.3645681142807007
train gradient:  0.1818842728377067
iteration : 5816
train acc:  0.9140625
train loss:  0.24677878618240356
train gradient:  0.1934205574246115
iteration : 5817
train acc:  0.8984375
train loss:  0.2798367142677307
train gradient:  0.19755555322586424
iteration : 5818
train acc:  0.8203125
train loss:  0.3706977963447571
train gradient:  0.25513218097750107
iteration : 5819
train acc:  0.859375
train loss:  0.3044079840183258
train gradient:  0.15289276572073962
iteration : 5820
train acc:  0.828125
train loss:  0.3731643855571747
train gradient:  0.24965194260945228
iteration : 5821
train acc:  0.8515625
train loss:  0.336929053068161
train gradient:  0.19088915763913308
iteration : 5822
train acc:  0.953125
train loss:  0.21245068311691284
train gradient:  0.1325109596423933
iteration : 5823
train acc:  0.84375
train loss:  0.3684602379798889
train gradient:  0.22283604105568597
iteration : 5824
train acc:  0.8046875
train loss:  0.40226367115974426
train gradient:  0.2806961826069989
iteration : 5825
train acc:  0.84375
train loss:  0.3216950595378876
train gradient:  0.2620068879844948
iteration : 5826
train acc:  0.84375
train loss:  0.3786660432815552
train gradient:  0.3483311595978082
iteration : 5827
train acc:  0.84375
train loss:  0.372650146484375
train gradient:  0.24665719180540857
iteration : 5828
train acc:  0.8359375
train loss:  0.40003103017807007
train gradient:  0.24060653929739412
iteration : 5829
train acc:  0.828125
train loss:  0.3802661597728729
train gradient:  0.32814927306893904
iteration : 5830
train acc:  0.8203125
train loss:  0.36210963129997253
train gradient:  0.24881317936168587
iteration : 5831
train acc:  0.8515625
train loss:  0.28603827953338623
train gradient:  0.21023168353560445
iteration : 5832
train acc:  0.8515625
train loss:  0.3526364862918854
train gradient:  0.35908399102896926
iteration : 5833
train acc:  0.875
train loss:  0.2689492404460907
train gradient:  0.16161342694788128
iteration : 5834
train acc:  0.8359375
train loss:  0.36291438341140747
train gradient:  0.29827393866693647
iteration : 5835
train acc:  0.8828125
train loss:  0.3455849885940552
train gradient:  0.20507951073274783
iteration : 5836
train acc:  0.890625
train loss:  0.3701450526714325
train gradient:  0.22247755940723946
iteration : 5837
train acc:  0.8359375
train loss:  0.3382902145385742
train gradient:  0.22000470572505326
iteration : 5838
train acc:  0.828125
train loss:  0.3631684184074402
train gradient:  0.27451125367059065
iteration : 5839
train acc:  0.859375
train loss:  0.28212955594062805
train gradient:  0.16615652176931744
iteration : 5840
train acc:  0.890625
train loss:  0.25360795855522156
train gradient:  0.1257225138902957
iteration : 5841
train acc:  0.8671875
train loss:  0.3189811706542969
train gradient:  0.2714944760943406
iteration : 5842
train acc:  0.8046875
train loss:  0.4741402268409729
train gradient:  0.3775537970292749
iteration : 5843
train acc:  0.828125
train loss:  0.3960503935813904
train gradient:  0.312299092140181
iteration : 5844
train acc:  0.8515625
train loss:  0.40231847763061523
train gradient:  0.20895802004437064
iteration : 5845
train acc:  0.8671875
train loss:  0.33378472924232483
train gradient:  0.29496158170174736
iteration : 5846
train acc:  0.8203125
train loss:  0.43029558658599854
train gradient:  0.3271373567539861
iteration : 5847
train acc:  0.8046875
train loss:  0.4133231043815613
train gradient:  0.3712752777709916
iteration : 5848
train acc:  0.8828125
train loss:  0.2851739227771759
train gradient:  0.23453179152774895
iteration : 5849
train acc:  0.890625
train loss:  0.27448827028274536
train gradient:  0.18837665628801303
iteration : 5850
train acc:  0.796875
train loss:  0.3583279252052307
train gradient:  0.3380259429064053
iteration : 5851
train acc:  0.890625
train loss:  0.26185062527656555
train gradient:  0.17281353138024816
iteration : 5852
train acc:  0.765625
train loss:  0.4825190007686615
train gradient:  0.36212917142295137
iteration : 5853
train acc:  0.84375
train loss:  0.34729403257369995
train gradient:  0.16466799519351516
iteration : 5854
train acc:  0.84375
train loss:  0.3789304196834564
train gradient:  0.28636178979828614
iteration : 5855
train acc:  0.7734375
train loss:  0.43606722354888916
train gradient:  0.3434290375183918
iteration : 5856
train acc:  0.9140625
train loss:  0.2535175681114197
train gradient:  0.1374511566280018
iteration : 5857
train acc:  0.8515625
train loss:  0.3402206599712372
train gradient:  0.18449926344932238
iteration : 5858
train acc:  0.8671875
train loss:  0.33075034618377686
train gradient:  0.2244987921573745
iteration : 5859
train acc:  0.8671875
train loss:  0.32677438855171204
train gradient:  0.19639996463590137
iteration : 5860
train acc:  0.8359375
train loss:  0.35303404927253723
train gradient:  0.33306785836870295
iteration : 5861
train acc:  0.8359375
train loss:  0.40863603353500366
train gradient:  0.48057339949407457
iteration : 5862
train acc:  0.8515625
train loss:  0.31325778365135193
train gradient:  0.19816680845385032
iteration : 5863
train acc:  0.8125
train loss:  0.4233247935771942
train gradient:  0.5674174469919994
iteration : 5864
train acc:  0.8359375
train loss:  0.37538740038871765
train gradient:  0.6408474616442743
iteration : 5865
train acc:  0.875
train loss:  0.30793580412864685
train gradient:  0.2860441956503693
iteration : 5866
train acc:  0.875
train loss:  0.30801019072532654
train gradient:  0.16722011148308896
iteration : 5867
train acc:  0.8515625
train loss:  0.38389191031455994
train gradient:  0.21808285270563066
iteration : 5868
train acc:  0.84375
train loss:  0.34694811701774597
train gradient:  0.2251350277312091
iteration : 5869
train acc:  0.8515625
train loss:  0.3928028643131256
train gradient:  0.2677691983639875
iteration : 5870
train acc:  0.8125
train loss:  0.3783459961414337
train gradient:  0.24565274862234326
iteration : 5871
train acc:  0.828125
train loss:  0.3791050910949707
train gradient:  0.2861500007658873
iteration : 5872
train acc:  0.8359375
train loss:  0.36242857575416565
train gradient:  0.22946218224381254
iteration : 5873
train acc:  0.8203125
train loss:  0.37201324105262756
train gradient:  0.22123588584844806
iteration : 5874
train acc:  0.8203125
train loss:  0.3832109868526459
train gradient:  0.34873442338215715
iteration : 5875
train acc:  0.8046875
train loss:  0.3649017810821533
train gradient:  0.213816274697463
iteration : 5876
train acc:  0.8359375
train loss:  0.3855758607387543
train gradient:  0.24561956701310883
iteration : 5877
train acc:  0.8515625
train loss:  0.36644524335861206
train gradient:  0.2561543458013873
iteration : 5878
train acc:  0.7890625
train loss:  0.37948745489120483
train gradient:  0.25274032651161643
iteration : 5879
train acc:  0.8125
train loss:  0.4005851745605469
train gradient:  0.28182433490656983
iteration : 5880
train acc:  0.859375
train loss:  0.32483038306236267
train gradient:  0.163824830453039
iteration : 5881
train acc:  0.8359375
train loss:  0.38309478759765625
train gradient:  0.2637031091412161
iteration : 5882
train acc:  0.8125
train loss:  0.3926171064376831
train gradient:  0.2720710381257353
iteration : 5883
train acc:  0.8203125
train loss:  0.3354507088661194
train gradient:  0.22421048030450424
iteration : 5884
train acc:  0.875
train loss:  0.28499987721443176
train gradient:  0.2117956043277946
iteration : 5885
train acc:  0.8671875
train loss:  0.32095637917518616
train gradient:  0.16539680255323708
iteration : 5886
train acc:  0.8203125
train loss:  0.37939542531967163
train gradient:  0.24162030475709217
iteration : 5887
train acc:  0.8125
train loss:  0.3822726607322693
train gradient:  0.20360181110531805
iteration : 5888
train acc:  0.859375
train loss:  0.3297193646430969
train gradient:  0.23341261687043327
iteration : 5889
train acc:  0.84375
train loss:  0.34027791023254395
train gradient:  0.17195409561126634
iteration : 5890
train acc:  0.859375
train loss:  0.297248899936676
train gradient:  0.20745196615061956
iteration : 5891
train acc:  0.8203125
train loss:  0.37230437994003296
train gradient:  0.27426203382622016
iteration : 5892
train acc:  0.84375
train loss:  0.37573501467704773
train gradient:  0.17668412916605503
iteration : 5893
train acc:  0.859375
train loss:  0.36092764139175415
train gradient:  0.2532622100350123
iteration : 5894
train acc:  0.875
train loss:  0.30662205815315247
train gradient:  0.19263337912394154
iteration : 5895
train acc:  0.8515625
train loss:  0.3904104232788086
train gradient:  0.2674114374606093
iteration : 5896
train acc:  0.8203125
train loss:  0.3938939869403839
train gradient:  0.24936287903447182
iteration : 5897
train acc:  0.78125
train loss:  0.4290549159049988
train gradient:  0.29762642762860253
iteration : 5898
train acc:  0.828125
train loss:  0.39738011360168457
train gradient:  0.3207906565939581
iteration : 5899
train acc:  0.875
train loss:  0.3243984878063202
train gradient:  0.23793856227204055
iteration : 5900
train acc:  0.8515625
train loss:  0.37985092401504517
train gradient:  0.17197891346285338
iteration : 5901
train acc:  0.890625
train loss:  0.32266145944595337
train gradient:  0.1926442130442338
iteration : 5902
train acc:  0.828125
train loss:  0.34016454219818115
train gradient:  0.31148143811359646
iteration : 5903
train acc:  0.8515625
train loss:  0.3019108772277832
train gradient:  0.26618609854873326
iteration : 5904
train acc:  0.828125
train loss:  0.3681701421737671
train gradient:  0.34313139729601944
iteration : 5905
train acc:  0.8828125
train loss:  0.3137654662132263
train gradient:  0.22892865221377776
iteration : 5906
train acc:  0.8359375
train loss:  0.33424097299575806
train gradient:  0.22146302852693506
iteration : 5907
train acc:  0.8125
train loss:  0.39876770973205566
train gradient:  0.2100352183780385
iteration : 5908
train acc:  0.84375
train loss:  0.316986620426178
train gradient:  0.15772340676075725
iteration : 5909
train acc:  0.8359375
train loss:  0.35878899693489075
train gradient:  0.1829440381149911
iteration : 5910
train acc:  0.796875
train loss:  0.42066842317581177
train gradient:  0.2836102914776228
iteration : 5911
train acc:  0.796875
train loss:  0.42644715309143066
train gradient:  0.23620490350485696
iteration : 5912
train acc:  0.8359375
train loss:  0.37638044357299805
train gradient:  0.23919435867855898
iteration : 5913
train acc:  0.8828125
train loss:  0.2964385151863098
train gradient:  0.19470647118331957
iteration : 5914
train acc:  0.875
train loss:  0.34007835388183594
train gradient:  0.1673231682471225
iteration : 5915
train acc:  0.84375
train loss:  0.32620954513549805
train gradient:  0.1708845298870733
iteration : 5916
train acc:  0.828125
train loss:  0.4249947667121887
train gradient:  0.29665438953319573
iteration : 5917
train acc:  0.8671875
train loss:  0.3300292491912842
train gradient:  0.15295201114903975
iteration : 5918
train acc:  0.8828125
train loss:  0.27756908535957336
train gradient:  0.18402399469177255
iteration : 5919
train acc:  0.8515625
train loss:  0.3115832805633545
train gradient:  0.2030460773463065
iteration : 5920
train acc:  0.8515625
train loss:  0.36954283714294434
train gradient:  0.2031709637840305
iteration : 5921
train acc:  0.859375
train loss:  0.3154468238353729
train gradient:  0.16728682289616903
iteration : 5922
train acc:  0.84375
train loss:  0.32196271419525146
train gradient:  0.17047422253533567
iteration : 5923
train acc:  0.7890625
train loss:  0.3998417258262634
train gradient:  0.2313013470418182
iteration : 5924
train acc:  0.859375
train loss:  0.29083192348480225
train gradient:  0.1739212924340029
iteration : 5925
train acc:  0.84375
train loss:  0.41587740182876587
train gradient:  0.35197562946126476
iteration : 5926
train acc:  0.875
train loss:  0.35593181848526
train gradient:  0.22282843827998094
iteration : 5927
train acc:  0.859375
train loss:  0.35138872265815735
train gradient:  0.25653118196637265
iteration : 5928
train acc:  0.84375
train loss:  0.3215581476688385
train gradient:  0.14752804858453328
iteration : 5929
train acc:  0.796875
train loss:  0.3461829721927643
train gradient:  0.2836578118952272
iteration : 5930
train acc:  0.8671875
train loss:  0.3299837112426758
train gradient:  0.27724003550167425
iteration : 5931
train acc:  0.8828125
train loss:  0.3255525529384613
train gradient:  0.15349794529763888
iteration : 5932
train acc:  0.859375
train loss:  0.376869261264801
train gradient:  0.23091261338767458
iteration : 5933
train acc:  0.8125
train loss:  0.41451388597488403
train gradient:  0.2646468348249667
iteration : 5934
train acc:  0.8125
train loss:  0.4058547914028168
train gradient:  0.33121623111635556
iteration : 5935
train acc:  0.90625
train loss:  0.2703832685947418
train gradient:  0.1674145075794415
iteration : 5936
train acc:  0.8828125
train loss:  0.3175544738769531
train gradient:  0.16622784513412128
iteration : 5937
train acc:  0.8671875
train loss:  0.30141395330429077
train gradient:  0.3301985498882337
iteration : 5938
train acc:  0.859375
train loss:  0.3568387031555176
train gradient:  0.21898830393635124
iteration : 5939
train acc:  0.8671875
train loss:  0.3184874653816223
train gradient:  0.2545983481706579
iteration : 5940
train acc:  0.84375
train loss:  0.3420790433883667
train gradient:  0.1541763366789516
iteration : 5941
train acc:  0.875
train loss:  0.31104859709739685
train gradient:  0.26896840758752044
iteration : 5942
train acc:  0.84375
train loss:  0.34736645221710205
train gradient:  0.20644695181143807
iteration : 5943
train acc:  0.84375
train loss:  0.3947826623916626
train gradient:  0.3237977535395346
iteration : 5944
train acc:  0.8984375
train loss:  0.27101773023605347
train gradient:  0.14586725605394027
iteration : 5945
train acc:  0.8359375
train loss:  0.3465386629104614
train gradient:  0.2957058936541769
iteration : 5946
train acc:  0.8125
train loss:  0.3857390880584717
train gradient:  0.2553542841626689
iteration : 5947
train acc:  0.828125
train loss:  0.3342961370944977
train gradient:  0.3085445119330518
iteration : 5948
train acc:  0.8984375
train loss:  0.2582184076309204
train gradient:  0.1473943666395639
iteration : 5949
train acc:  0.7890625
train loss:  0.44920194149017334
train gradient:  0.3158196407507005
iteration : 5950
train acc:  0.828125
train loss:  0.36126771569252014
train gradient:  0.27538216584924097
iteration : 5951
train acc:  0.828125
train loss:  0.32661378383636475
train gradient:  0.22343170086786301
iteration : 5952
train acc:  0.859375
train loss:  0.2809418737888336
train gradient:  0.15355987438191518
iteration : 5953
train acc:  0.828125
train loss:  0.37274104356765747
train gradient:  0.30840967490641874
iteration : 5954
train acc:  0.8046875
train loss:  0.35636651515960693
train gradient:  0.21139697826379875
iteration : 5955
train acc:  0.890625
train loss:  0.3375423550605774
train gradient:  0.14598692759218165
iteration : 5956
train acc:  0.8671875
train loss:  0.31735944747924805
train gradient:  0.20416125627664405
iteration : 5957
train acc:  0.8671875
train loss:  0.3097692131996155
train gradient:  0.16750301513995117
iteration : 5958
train acc:  0.828125
train loss:  0.3808557987213135
train gradient:  0.21858284300454212
iteration : 5959
train acc:  0.84375
train loss:  0.3038969337940216
train gradient:  0.21151838472622192
iteration : 5960
train acc:  0.859375
train loss:  0.3294579088687897
train gradient:  0.2163617162203198
iteration : 5961
train acc:  0.8046875
train loss:  0.40522050857543945
train gradient:  0.2577186732594654
iteration : 5962
train acc:  0.8515625
train loss:  0.37190714478492737
train gradient:  0.2893516505917512
iteration : 5963
train acc:  0.796875
train loss:  0.41356217861175537
train gradient:  0.4478928486718552
iteration : 5964
train acc:  0.8359375
train loss:  0.3881685137748718
train gradient:  0.2959155924325638
iteration : 5965
train acc:  0.8671875
train loss:  0.3181002736091614
train gradient:  0.2452805643552592
iteration : 5966
train acc:  0.828125
train loss:  0.3884265422821045
train gradient:  0.23943545302271696
iteration : 5967
train acc:  0.8359375
train loss:  0.37939324975013733
train gradient:  0.3356054004369523
iteration : 5968
train acc:  0.8359375
train loss:  0.3864673972129822
train gradient:  0.2784861928531073
iteration : 5969
train acc:  0.8359375
train loss:  0.37883713841438293
train gradient:  0.3004433603616656
iteration : 5970
train acc:  0.7890625
train loss:  0.47076931595802307
train gradient:  0.25687146300014096
iteration : 5971
train acc:  0.8828125
train loss:  0.3099667727947235
train gradient:  0.18186461504402807
iteration : 5972
train acc:  0.7734375
train loss:  0.42812642455101013
train gradient:  0.33147528494720935
iteration : 5973
train acc:  0.84375
train loss:  0.34094059467315674
train gradient:  0.24132457641887875
iteration : 5974
train acc:  0.859375
train loss:  0.32458391785621643
train gradient:  0.19135939847025718
iteration : 5975
train acc:  0.8984375
train loss:  0.31018030643463135
train gradient:  0.15869301414190395
iteration : 5976
train acc:  0.859375
train loss:  0.30911219120025635
train gradient:  0.18562657756864534
iteration : 5977
train acc:  0.8125
train loss:  0.3380316495895386
train gradient:  0.18715312702789028
iteration : 5978
train acc:  0.84375
train loss:  0.30981767177581787
train gradient:  0.23060857157962772
iteration : 5979
train acc:  0.8984375
train loss:  0.26339319348335266
train gradient:  0.16804246681473028
iteration : 5980
train acc:  0.8359375
train loss:  0.3787384033203125
train gradient:  0.20278782987690186
iteration : 5981
train acc:  0.8671875
train loss:  0.3061811923980713
train gradient:  0.4477786593834267
iteration : 5982
train acc:  0.7890625
train loss:  0.41276368498802185
train gradient:  0.37909346145606937
iteration : 5983
train acc:  0.796875
train loss:  0.44397276639938354
train gradient:  0.3894992529733614
iteration : 5984
train acc:  0.8828125
train loss:  0.2871543765068054
train gradient:  0.17334457227058808
iteration : 5985
train acc:  0.8359375
train loss:  0.39196598529815674
train gradient:  0.24446252543463465
iteration : 5986
train acc:  0.84375
train loss:  0.32464587688446045
train gradient:  0.265852206459885
iteration : 5987
train acc:  0.8671875
train loss:  0.33679473400115967
train gradient:  0.18726895719918077
iteration : 5988
train acc:  0.84375
train loss:  0.30089664459228516
train gradient:  0.15919084898399674
iteration : 5989
train acc:  0.796875
train loss:  0.41810381412506104
train gradient:  0.3069051231091345
iteration : 5990
train acc:  0.796875
train loss:  0.405869722366333
train gradient:  0.27732912603493376
iteration : 5991
train acc:  0.8125
train loss:  0.3976568579673767
train gradient:  0.24020840703858773
iteration : 5992
train acc:  0.8515625
train loss:  0.31580954790115356
train gradient:  0.16364032813471333
iteration : 5993
train acc:  0.890625
train loss:  0.304654061794281
train gradient:  0.15264112830348306
iteration : 5994
train acc:  0.890625
train loss:  0.2912175953388214
train gradient:  0.16991549292079888
iteration : 5995
train acc:  0.828125
train loss:  0.35658252239227295
train gradient:  0.2595455994346531
iteration : 5996
train acc:  0.8515625
train loss:  0.3097687363624573
train gradient:  0.2002623346061729
iteration : 5997
train acc:  0.8828125
train loss:  0.28459903597831726
train gradient:  0.1941604570533263
iteration : 5998
train acc:  0.8046875
train loss:  0.44824743270874023
train gradient:  0.266250261705864
iteration : 5999
train acc:  0.78125
train loss:  0.43118607997894287
train gradient:  0.2483530299708953
iteration : 6000
train acc:  0.7890625
train loss:  0.44134217500686646
train gradient:  0.31996558698675626
iteration : 6001
train acc:  0.7734375
train loss:  0.4801136255264282
train gradient:  0.36147269340882027
iteration : 6002
train acc:  0.84375
train loss:  0.338436484336853
train gradient:  0.18364646983376776
iteration : 6003
train acc:  0.8984375
train loss:  0.27475064992904663
train gradient:  0.17336962591632918
iteration : 6004
train acc:  0.8828125
train loss:  0.2892449200153351
train gradient:  0.1591186410992897
iteration : 6005
train acc:  0.7890625
train loss:  0.46548008918762207
train gradient:  0.24124141888137643
iteration : 6006
train acc:  0.8828125
train loss:  0.3120766878128052
train gradient:  0.1676322258268825
iteration : 6007
train acc:  0.8671875
train loss:  0.3450471758842468
train gradient:  0.1959512465889508
iteration : 6008
train acc:  0.859375
train loss:  0.3211299479007721
train gradient:  0.17245010605054967
iteration : 6009
train acc:  0.8359375
train loss:  0.3112857937812805
train gradient:  0.16677802465937214
iteration : 6010
train acc:  0.875
train loss:  0.31451693177223206
train gradient:  0.1612157869740579
iteration : 6011
train acc:  0.84375
train loss:  0.33506011962890625
train gradient:  0.1737027152711798
iteration : 6012
train acc:  0.890625
train loss:  0.3296763300895691
train gradient:  0.2360905848199579
iteration : 6013
train acc:  0.8671875
train loss:  0.3230365216732025
train gradient:  0.2348442735849478
iteration : 6014
train acc:  0.8125
train loss:  0.33917635679244995
train gradient:  0.26745348875321395
iteration : 6015
train acc:  0.828125
train loss:  0.337813138961792
train gradient:  0.19064353857069807
iteration : 6016
train acc:  0.828125
train loss:  0.3303540050983429
train gradient:  0.2551812508839873
iteration : 6017
train acc:  0.8359375
train loss:  0.35304373502731323
train gradient:  0.17647854311154548
iteration : 6018
train acc:  0.890625
train loss:  0.2922576665878296
train gradient:  0.17161454530608902
iteration : 6019
train acc:  0.8984375
train loss:  0.2734261751174927
train gradient:  0.18355441909894507
iteration : 6020
train acc:  0.8125
train loss:  0.37912777066230774
train gradient:  0.4176799964045004
iteration : 6021
train acc:  0.8359375
train loss:  0.3755095303058624
train gradient:  0.24521909527138602
iteration : 6022
train acc:  0.765625
train loss:  0.43714749813079834
train gradient:  0.3323567176622031
iteration : 6023
train acc:  0.8671875
train loss:  0.35162127017974854
train gradient:  0.1735016352006506
iteration : 6024
train acc:  0.84375
train loss:  0.3616023063659668
train gradient:  0.26481336960411805
iteration : 6025
train acc:  0.859375
train loss:  0.2888273000717163
train gradient:  0.18508072190678368
iteration : 6026
train acc:  0.8125
train loss:  0.4056902825832367
train gradient:  0.2906325152289715
iteration : 6027
train acc:  0.875
train loss:  0.3130664825439453
train gradient:  0.19043098023998528
iteration : 6028
train acc:  0.8671875
train loss:  0.32119816541671753
train gradient:  0.21306221024465222
iteration : 6029
train acc:  0.8125
train loss:  0.4634261727333069
train gradient:  0.37970546063209554
iteration : 6030
train acc:  0.8671875
train loss:  0.28507861495018005
train gradient:  0.21365159552174906
iteration : 6031
train acc:  0.828125
train loss:  0.3525770306587219
train gradient:  0.2301810929509324
iteration : 6032
train acc:  0.8671875
train loss:  0.37048959732055664
train gradient:  0.3203078717432443
iteration : 6033
train acc:  0.8828125
train loss:  0.27747485041618347
train gradient:  0.17482119345826155
iteration : 6034
train acc:  0.84375
train loss:  0.3168949484825134
train gradient:  0.13290689780226084
iteration : 6035
train acc:  0.875
train loss:  0.29387134313583374
train gradient:  0.24223036495007505
iteration : 6036
train acc:  0.8671875
train loss:  0.30575495958328247
train gradient:  0.1764273965016408
iteration : 6037
train acc:  0.7734375
train loss:  0.4192209839820862
train gradient:  0.32006342759789125
iteration : 6038
train acc:  0.8828125
train loss:  0.3101268410682678
train gradient:  0.16597163705270673
iteration : 6039
train acc:  0.84375
train loss:  0.35345178842544556
train gradient:  0.1870939754204396
iteration : 6040
train acc:  0.8203125
train loss:  0.3984346389770508
train gradient:  0.3072907548532116
iteration : 6041
train acc:  0.8515625
train loss:  0.3645879626274109
train gradient:  0.1959097030508804
iteration : 6042
train acc:  0.8125
train loss:  0.4337690770626068
train gradient:  0.31228962375514924
iteration : 6043
train acc:  0.890625
train loss:  0.30576881766319275
train gradient:  0.19974872012772565
iteration : 6044
train acc:  0.828125
train loss:  0.31730228662490845
train gradient:  0.28203380395249167
iteration : 6045
train acc:  0.84375
train loss:  0.3725775182247162
train gradient:  0.22768694039417758
iteration : 6046
train acc:  0.84375
train loss:  0.3499666154384613
train gradient:  0.19585611613094164
iteration : 6047
train acc:  0.828125
train loss:  0.32034939527511597
train gradient:  0.21320521910062806
iteration : 6048
train acc:  0.8671875
train loss:  0.34524011611938477
train gradient:  0.18001898491781218
iteration : 6049
train acc:  0.8046875
train loss:  0.4011814296245575
train gradient:  0.444803042965092
iteration : 6050
train acc:  0.890625
train loss:  0.30101218819618225
train gradient:  0.2145304936755768
iteration : 6051
train acc:  0.859375
train loss:  0.3522389531135559
train gradient:  0.22810483428825085
iteration : 6052
train acc:  0.828125
train loss:  0.4390783905982971
train gradient:  0.36873311812143383
iteration : 6053
train acc:  0.8984375
train loss:  0.2706998586654663
train gradient:  0.19703033534109007
iteration : 6054
train acc:  0.8125
train loss:  0.41194164752960205
train gradient:  0.4186585171001801
iteration : 6055
train acc:  0.859375
train loss:  0.35975977778434753
train gradient:  0.22969546099531757
iteration : 6056
train acc:  0.875
train loss:  0.2881617844104767
train gradient:  0.16736205668495333
iteration : 6057
train acc:  0.8828125
train loss:  0.3446546196937561
train gradient:  0.18248478188443146
iteration : 6058
train acc:  0.8359375
train loss:  0.3375176787376404
train gradient:  0.3275550064636312
iteration : 6059
train acc:  0.8515625
train loss:  0.35680562257766724
train gradient:  0.17785465269855197
iteration : 6060
train acc:  0.8828125
train loss:  0.3419126868247986
train gradient:  0.2702812481010782
iteration : 6061
train acc:  0.875
train loss:  0.34173691272735596
train gradient:  0.20348562679251925
iteration : 6062
train acc:  0.8515625
train loss:  0.3951224088668823
train gradient:  0.31429999549864246
iteration : 6063
train acc:  0.8984375
train loss:  0.2586452066898346
train gradient:  0.16763111652129414
iteration : 6064
train acc:  0.8828125
train loss:  0.28187912702560425
train gradient:  0.21463974909824127
iteration : 6065
train acc:  0.875
train loss:  0.3091735243797302
train gradient:  0.1696515843938291
iteration : 6066
train acc:  0.875
train loss:  0.31337791681289673
train gradient:  0.23813399312500244
iteration : 6067
train acc:  0.828125
train loss:  0.3644385039806366
train gradient:  0.24356520181327002
iteration : 6068
train acc:  0.890625
train loss:  0.28461208939552307
train gradient:  0.28960872698442497
iteration : 6069
train acc:  0.8125
train loss:  0.4213539958000183
train gradient:  0.36489948988131077
iteration : 6070
train acc:  0.8671875
train loss:  0.2854849398136139
train gradient:  0.15982299429339758
iteration : 6071
train acc:  0.859375
train loss:  0.3529421091079712
train gradient:  0.22007681385037972
iteration : 6072
train acc:  0.890625
train loss:  0.2634645700454712
train gradient:  0.18324346426493426
iteration : 6073
train acc:  0.828125
train loss:  0.35800573229789734
train gradient:  0.26111956474649933
iteration : 6074
train acc:  0.8125
train loss:  0.43234479427337646
train gradient:  0.3536761432816317
iteration : 6075
train acc:  0.8671875
train loss:  0.34986749291419983
train gradient:  0.2307890953566005
iteration : 6076
train acc:  0.8515625
train loss:  0.312549352645874
train gradient:  0.2466301597115115
iteration : 6077
train acc:  0.8046875
train loss:  0.4152347445487976
train gradient:  0.2878867859199334
iteration : 6078
train acc:  0.828125
train loss:  0.31840842962265015
train gradient:  0.2508434363397038
iteration : 6079
train acc:  0.84375
train loss:  0.423900842666626
train gradient:  0.2374137722737338
iteration : 6080
train acc:  0.8515625
train loss:  0.3407583236694336
train gradient:  0.21030973067736952
iteration : 6081
train acc:  0.875
train loss:  0.35489651560783386
train gradient:  0.3085105036408591
iteration : 6082
train acc:  0.8359375
train loss:  0.39247187972068787
train gradient:  0.3619818878097418
iteration : 6083
train acc:  0.8828125
train loss:  0.306117981672287
train gradient:  0.21806432306963824
iteration : 6084
train acc:  0.8671875
train loss:  0.31250128149986267
train gradient:  0.17276016592654114
iteration : 6085
train acc:  0.84375
train loss:  0.3401985764503479
train gradient:  0.25952646455166223
iteration : 6086
train acc:  0.8515625
train loss:  0.29889410734176636
train gradient:  0.14226564886479068
iteration : 6087
train acc:  0.859375
train loss:  0.35987383127212524
train gradient:  0.21920421474865326
iteration : 6088
train acc:  0.8125
train loss:  0.3930649161338806
train gradient:  0.22728542413190558
iteration : 6089
train acc:  0.8671875
train loss:  0.3317762017250061
train gradient:  0.22519454409678485
iteration : 6090
train acc:  0.890625
train loss:  0.27437156438827515
train gradient:  0.25490528991352357
iteration : 6091
train acc:  0.84375
train loss:  0.3478495478630066
train gradient:  0.3928112524149656
iteration : 6092
train acc:  0.875
train loss:  0.2802930474281311
train gradient:  0.14937982875854378
iteration : 6093
train acc:  0.8359375
train loss:  0.35795044898986816
train gradient:  0.3232367111057152
iteration : 6094
train acc:  0.8203125
train loss:  0.40155065059661865
train gradient:  0.24805246102079842
iteration : 6095
train acc:  0.8671875
train loss:  0.3056776523590088
train gradient:  0.18809729166641706
iteration : 6096
train acc:  0.8125
train loss:  0.3566541075706482
train gradient:  0.23514080232829562
iteration : 6097
train acc:  0.8515625
train loss:  0.35755038261413574
train gradient:  0.20555509259202226
iteration : 6098
train acc:  0.8515625
train loss:  0.32128533720970154
train gradient:  0.2322896936274129
iteration : 6099
train acc:  0.875
train loss:  0.3403003513813019
train gradient:  0.21304511945396215
iteration : 6100
train acc:  0.8671875
train loss:  0.3142249286174774
train gradient:  0.15383795645284382
iteration : 6101
train acc:  0.84375
train loss:  0.3742019534111023
train gradient:  0.2894087625509122
iteration : 6102
train acc:  0.875
train loss:  0.31958627700805664
train gradient:  0.17852779512127132
iteration : 6103
train acc:  0.828125
train loss:  0.44304513931274414
train gradient:  0.29018629817447045
iteration : 6104
train acc:  0.84375
train loss:  0.34984493255615234
train gradient:  0.21232452897934412
iteration : 6105
train acc:  0.828125
train loss:  0.3766089081764221
train gradient:  0.4024274960637898
iteration : 6106
train acc:  0.8203125
train loss:  0.37887609004974365
train gradient:  0.2627628801553531
iteration : 6107
train acc:  0.8515625
train loss:  0.33784234523773193
train gradient:  0.1634810624768403
iteration : 6108
train acc:  0.8359375
train loss:  0.3982957601547241
train gradient:  0.2589701589108526
iteration : 6109
train acc:  0.828125
train loss:  0.3628067970275879
train gradient:  0.18763291627466577
iteration : 6110
train acc:  0.890625
train loss:  0.2880512773990631
train gradient:  0.16334219686693988
iteration : 6111
train acc:  0.8203125
train loss:  0.36674508452415466
train gradient:  0.26192474343769795
iteration : 6112
train acc:  0.875
train loss:  0.30880969762802124
train gradient:  0.2586686222788226
iteration : 6113
train acc:  0.8203125
train loss:  0.3416728973388672
train gradient:  0.22449791183711287
iteration : 6114
train acc:  0.8515625
train loss:  0.35467687249183655
train gradient:  0.19299377614223356
iteration : 6115
train acc:  0.859375
train loss:  0.31399285793304443
train gradient:  0.19522210469890525
iteration : 6116
train acc:  0.828125
train loss:  0.37305182218551636
train gradient:  0.3097637207681875
iteration : 6117
train acc:  0.890625
train loss:  0.2958891987800598
train gradient:  0.15013562486994794
iteration : 6118
train acc:  0.9140625
train loss:  0.2180166393518448
train gradient:  0.14517644263883628
iteration : 6119
train acc:  0.8515625
train loss:  0.30267196893692017
train gradient:  0.1686605366274224
iteration : 6120
train acc:  0.8359375
train loss:  0.36900314688682556
train gradient:  0.1991241802018695
iteration : 6121
train acc:  0.8046875
train loss:  0.38218289613723755
train gradient:  0.24280594314169507
iteration : 6122
train acc:  0.875
train loss:  0.3262810707092285
train gradient:  0.22158485028649993
iteration : 6123
train acc:  0.8828125
train loss:  0.3080805838108063
train gradient:  0.1436286535836789
iteration : 6124
train acc:  0.859375
train loss:  0.3328915536403656
train gradient:  0.21308035001937134
iteration : 6125
train acc:  0.8515625
train loss:  0.3313456177711487
train gradient:  0.1775275521777248
iteration : 6126
train acc:  0.796875
train loss:  0.39480626583099365
train gradient:  0.2848732955291551
iteration : 6127
train acc:  0.90625
train loss:  0.24413946270942688
train gradient:  0.13347659382852656
iteration : 6128
train acc:  0.828125
train loss:  0.3939339518547058
train gradient:  0.27829941489316534
iteration : 6129
train acc:  0.8359375
train loss:  0.36476004123687744
train gradient:  0.25960635161436924
iteration : 6130
train acc:  0.8515625
train loss:  0.3355785012245178
train gradient:  0.19829968810341386
iteration : 6131
train acc:  0.9375
train loss:  0.2298038899898529
train gradient:  0.11920800473845285
iteration : 6132
train acc:  0.875
train loss:  0.3195168077945709
train gradient:  0.22553030536725954
iteration : 6133
train acc:  0.8984375
train loss:  0.3083112835884094
train gradient:  0.15856129221720797
iteration : 6134
train acc:  0.828125
train loss:  0.38568443059921265
train gradient:  0.2934543015257965
iteration : 6135
train acc:  0.875
train loss:  0.3152667284011841
train gradient:  0.23809614695575734
iteration : 6136
train acc:  0.8359375
train loss:  0.37732014060020447
train gradient:  0.21646742567650312
iteration : 6137
train acc:  0.890625
train loss:  0.2891722321510315
train gradient:  0.15496557010800183
iteration : 6138
train acc:  0.8671875
train loss:  0.2805710732936859
train gradient:  0.19620017470655637
iteration : 6139
train acc:  0.859375
train loss:  0.3323677182197571
train gradient:  0.20157285456144264
iteration : 6140
train acc:  0.78125
train loss:  0.47795602679252625
train gradient:  0.38780165784153453
iteration : 6141
train acc:  0.8671875
train loss:  0.33252400159835815
train gradient:  0.2127871477998775
iteration : 6142
train acc:  0.828125
train loss:  0.3388054072856903
train gradient:  0.2354035925128563
iteration : 6143
train acc:  0.8359375
train loss:  0.36028122901916504
train gradient:  0.3318055700485653
iteration : 6144
train acc:  0.8671875
train loss:  0.333530068397522
train gradient:  0.26618955352539486
iteration : 6145
train acc:  0.859375
train loss:  0.3865135610103607
train gradient:  0.2998690070305363
iteration : 6146
train acc:  0.84375
train loss:  0.39355796575546265
train gradient:  0.24545507118282434
iteration : 6147
train acc:  0.890625
train loss:  0.29932671785354614
train gradient:  0.1439998234659601
iteration : 6148
train acc:  0.8203125
train loss:  0.4071274995803833
train gradient:  0.28274491289428183
iteration : 6149
train acc:  0.8203125
train loss:  0.3952822685241699
train gradient:  0.2823733091816427
iteration : 6150
train acc:  0.7734375
train loss:  0.44453734159469604
train gradient:  0.42087696816757914
iteration : 6151
train acc:  0.8671875
train loss:  0.37515494227409363
train gradient:  0.20283233766869477
iteration : 6152
train acc:  0.8515625
train loss:  0.34077802300453186
train gradient:  0.22753244864751057
iteration : 6153
train acc:  0.890625
train loss:  0.3381456732749939
train gradient:  0.22537587266501036
iteration : 6154
train acc:  0.8984375
train loss:  0.31311798095703125
train gradient:  0.16459279611172076
iteration : 6155
train acc:  0.8671875
train loss:  0.3527548909187317
train gradient:  0.6574033893463884
iteration : 6156
train acc:  0.7890625
train loss:  0.4454349875450134
train gradient:  0.31438862014912666
iteration : 6157
train acc:  0.8359375
train loss:  0.3530932068824768
train gradient:  0.22786854151875136
iteration : 6158
train acc:  0.8828125
train loss:  0.3065178096294403
train gradient:  0.1413492104304093
iteration : 6159
train acc:  0.875
train loss:  0.2638050615787506
train gradient:  0.14106045314003748
iteration : 6160
train acc:  0.8359375
train loss:  0.4508379101753235
train gradient:  0.40959225288483797
iteration : 6161
train acc:  0.8046875
train loss:  0.42409321665763855
train gradient:  0.2725371359073198
iteration : 6162
train acc:  0.828125
train loss:  0.3456692099571228
train gradient:  0.21055602649417293
iteration : 6163
train acc:  0.84375
train loss:  0.3127802610397339
train gradient:  0.16550692968279127
iteration : 6164
train acc:  0.828125
train loss:  0.311473548412323
train gradient:  0.21945140028410268
iteration : 6165
train acc:  0.8125
train loss:  0.3603009581565857
train gradient:  0.22970569819506756
iteration : 6166
train acc:  0.84375
train loss:  0.4214434027671814
train gradient:  0.2639743849846111
iteration : 6167
train acc:  0.890625
train loss:  0.30577731132507324
train gradient:  0.1735466043574741
iteration : 6168
train acc:  0.84375
train loss:  0.374172180891037
train gradient:  0.2235823064710758
iteration : 6169
train acc:  0.828125
train loss:  0.35448843240737915
train gradient:  0.2158568747443957
iteration : 6170
train acc:  0.84375
train loss:  0.3097534775733948
train gradient:  0.20703232310623854
iteration : 6171
train acc:  0.8046875
train loss:  0.441992849111557
train gradient:  0.30343657676846214
iteration : 6172
train acc:  0.828125
train loss:  0.3958548307418823
train gradient:  0.2859348053237163
iteration : 6173
train acc:  0.828125
train loss:  0.3478642702102661
train gradient:  0.26253282690954105
iteration : 6174
train acc:  0.859375
train loss:  0.34271103143692017
train gradient:  0.15038522611470473
iteration : 6175
train acc:  0.875
train loss:  0.3680797219276428
train gradient:  0.19000751892912754
iteration : 6176
train acc:  0.84375
train loss:  0.4432525634765625
train gradient:  0.29006457032308713
iteration : 6177
train acc:  0.859375
train loss:  0.2926304340362549
train gradient:  0.2723638173346119
iteration : 6178
train acc:  0.875
train loss:  0.3390159010887146
train gradient:  0.1888160593760399
iteration : 6179
train acc:  0.8203125
train loss:  0.38264191150665283
train gradient:  0.32007866336323737
iteration : 6180
train acc:  0.8828125
train loss:  0.2953723967075348
train gradient:  0.23782347017646854
iteration : 6181
train acc:  0.8359375
train loss:  0.41423553228378296
train gradient:  0.3158805360718832
iteration : 6182
train acc:  0.90625
train loss:  0.2830493748188019
train gradient:  0.15827779846957868
iteration : 6183
train acc:  0.828125
train loss:  0.3549862802028656
train gradient:  0.1744112960048785
iteration : 6184
train acc:  0.8359375
train loss:  0.33102089166641235
train gradient:  0.17411065099277137
iteration : 6185
train acc:  0.84375
train loss:  0.36324626207351685
train gradient:  0.18361500174261203
iteration : 6186
train acc:  0.8671875
train loss:  0.33613792061805725
train gradient:  0.16718996004657183
iteration : 6187
train acc:  0.84375
train loss:  0.3533136248588562
train gradient:  0.1888044466612037
iteration : 6188
train acc:  0.84375
train loss:  0.3430215120315552
train gradient:  0.21660071377255158
iteration : 6189
train acc:  0.8671875
train loss:  0.33454629778862
train gradient:  0.23577125786918382
iteration : 6190
train acc:  0.8203125
train loss:  0.36711353063583374
train gradient:  0.364781869039388
iteration : 6191
train acc:  0.9140625
train loss:  0.2503144145011902
train gradient:  0.11954490455836708
iteration : 6192
train acc:  0.8984375
train loss:  0.29128387570381165
train gradient:  0.1691390654414902
iteration : 6193
train acc:  0.84375
train loss:  0.32587534189224243
train gradient:  0.16774963242835522
iteration : 6194
train acc:  0.8515625
train loss:  0.3746020793914795
train gradient:  0.3692278589549599
iteration : 6195
train acc:  0.828125
train loss:  0.35162627696990967
train gradient:  0.21916509517361
iteration : 6196
train acc:  0.8359375
train loss:  0.354214072227478
train gradient:  0.24820283963588066
iteration : 6197
train acc:  0.8125
train loss:  0.41326904296875
train gradient:  0.2678755621453351
iteration : 6198
train acc:  0.8671875
train loss:  0.32197993993759155
train gradient:  0.13401479019410478
iteration : 6199
train acc:  0.859375
train loss:  0.371823787689209
train gradient:  0.19452336209070226
iteration : 6200
train acc:  0.8515625
train loss:  0.31581181287765503
train gradient:  0.13193525971818942
iteration : 6201
train acc:  0.84375
train loss:  0.39534610509872437
train gradient:  0.2563920219412379
iteration : 6202
train acc:  0.890625
train loss:  0.2787829041481018
train gradient:  0.1804380764169051
iteration : 6203
train acc:  0.859375
train loss:  0.3102739155292511
train gradient:  0.23305899898552418
iteration : 6204
train acc:  0.8359375
train loss:  0.3488221764564514
train gradient:  0.30747328302870913
iteration : 6205
train acc:  0.8359375
train loss:  0.35822200775146484
train gradient:  0.24618775561729062
iteration : 6206
train acc:  0.8203125
train loss:  0.3559612035751343
train gradient:  0.20311747485019643
iteration : 6207
train acc:  0.8046875
train loss:  0.39619481563568115
train gradient:  0.27241758437545516
iteration : 6208
train acc:  0.859375
train loss:  0.3202689290046692
train gradient:  0.247645508635348
iteration : 6209
train acc:  0.75
train loss:  0.5514678359031677
train gradient:  0.4568935141112057
iteration : 6210
train acc:  0.796875
train loss:  0.39693620800971985
train gradient:  0.2813221477088879
iteration : 6211
train acc:  0.8359375
train loss:  0.3596438765525818
train gradient:  0.1678357511164163
iteration : 6212
train acc:  0.875
train loss:  0.30937957763671875
train gradient:  0.2630357603856387
iteration : 6213
train acc:  0.8359375
train loss:  0.35881921648979187
train gradient:  0.20035201699349414
iteration : 6214
train acc:  0.8828125
train loss:  0.32114893198013306
train gradient:  0.19122383664587872
iteration : 6215
train acc:  0.8515625
train loss:  0.31848016381263733
train gradient:  0.3841714049297414
iteration : 6216
train acc:  0.859375
train loss:  0.3323554992675781
train gradient:  0.2035133028639946
iteration : 6217
train acc:  0.828125
train loss:  0.4072335660457611
train gradient:  0.2856127488675284
iteration : 6218
train acc:  0.8359375
train loss:  0.3742935359477997
train gradient:  0.1640992573063531
iteration : 6219
train acc:  0.8125
train loss:  0.3995456099510193
train gradient:  0.24103721847467768
iteration : 6220
train acc:  0.875
train loss:  0.28440576791763306
train gradient:  0.22612870698532087
iteration : 6221
train acc:  0.859375
train loss:  0.30831050872802734
train gradient:  0.1890772635220507
iteration : 6222
train acc:  0.8359375
train loss:  0.371645450592041
train gradient:  0.23123103463144268
iteration : 6223
train acc:  0.8671875
train loss:  0.31022387742996216
train gradient:  0.1742526252676525
iteration : 6224
train acc:  0.8671875
train loss:  0.34101539850234985
train gradient:  0.21488404990272134
iteration : 6225
train acc:  0.8515625
train loss:  0.3535386919975281
train gradient:  0.220241807138459
iteration : 6226
train acc:  0.7734375
train loss:  0.44754141569137573
train gradient:  0.40809580131851514
iteration : 6227
train acc:  0.875
train loss:  0.2765367031097412
train gradient:  0.18534879786180797
iteration : 6228
train acc:  0.8984375
train loss:  0.2527029812335968
train gradient:  0.16992194759145984
iteration : 6229
train acc:  0.828125
train loss:  0.36489996314048767
train gradient:  0.25897904264008637
iteration : 6230
train acc:  0.8828125
train loss:  0.3122708201408386
train gradient:  0.16133491375325232
iteration : 6231
train acc:  0.84375
train loss:  0.3359149992465973
train gradient:  0.2007747492358165
iteration : 6232
train acc:  0.875
train loss:  0.3170815706253052
train gradient:  0.14018256992954098
iteration : 6233
train acc:  0.859375
train loss:  0.3255547881126404
train gradient:  0.40299324401817666
iteration : 6234
train acc:  0.828125
train loss:  0.36585426330566406
train gradient:  0.1961143999024661
iteration : 6235
train acc:  0.84375
train loss:  0.37639227509498596
train gradient:  0.23269613486994306
iteration : 6236
train acc:  0.8125
train loss:  0.3676721453666687
train gradient:  0.19386619878507896
iteration : 6237
train acc:  0.8828125
train loss:  0.3086945116519928
train gradient:  0.19135356421579597
iteration : 6238
train acc:  0.84375
train loss:  0.36792823672294617
train gradient:  0.24926071735994654
iteration : 6239
train acc:  0.8359375
train loss:  0.3553028702735901
train gradient:  0.24084289704132594
iteration : 6240
train acc:  0.8671875
train loss:  0.32548457384109497
train gradient:  0.17080458256193193
iteration : 6241
train acc:  0.8515625
train loss:  0.3362657129764557
train gradient:  0.24143970302994816
iteration : 6242
train acc:  0.84375
train loss:  0.3184322714805603
train gradient:  0.17653692337757174
iteration : 6243
train acc:  0.875
train loss:  0.32183098793029785
train gradient:  0.2160871390233079
iteration : 6244
train acc:  0.859375
train loss:  0.36695224046707153
train gradient:  0.2953640819642407
iteration : 6245
train acc:  0.8125
train loss:  0.3410370647907257
train gradient:  0.15150508277460414
iteration : 6246
train acc:  0.8203125
train loss:  0.34018808603286743
train gradient:  0.3330810458390457
iteration : 6247
train acc:  0.796875
train loss:  0.3919623792171478
train gradient:  0.6955943992946121
iteration : 6248
train acc:  0.890625
train loss:  0.26332563161849976
train gradient:  0.20191096501641453
iteration : 6249
train acc:  0.7578125
train loss:  0.4545701742172241
train gradient:  0.2509872975291376
iteration : 6250
train acc:  0.796875
train loss:  0.39652615785598755
train gradient:  0.2167102008412567
iteration : 6251
train acc:  0.9140625
train loss:  0.20841780304908752
train gradient:  0.08816717982386933
iteration : 6252
train acc:  0.84375
train loss:  0.3055996596813202
train gradient:  0.2175415901665539
iteration : 6253
train acc:  0.8515625
train loss:  0.33614581823349
train gradient:  0.1775512467260646
iteration : 6254
train acc:  0.8671875
train loss:  0.32169127464294434
train gradient:  0.25495245853448917
iteration : 6255
train acc:  0.859375
train loss:  0.35483840107917786
train gradient:  0.17369508323056315
iteration : 6256
train acc:  0.8828125
train loss:  0.296661376953125
train gradient:  0.12500219616574787
iteration : 6257
train acc:  0.890625
train loss:  0.33170250058174133
train gradient:  0.23234418120581454
iteration : 6258
train acc:  0.859375
train loss:  0.2919757664203644
train gradient:  0.2026745192207638
iteration : 6259
train acc:  0.84375
train loss:  0.38483524322509766
train gradient:  0.2907291128378041
iteration : 6260
train acc:  0.8203125
train loss:  0.33424773812294006
train gradient:  0.16908259121376787
iteration : 6261
train acc:  0.9140625
train loss:  0.27400416135787964
train gradient:  0.16963145007406275
iteration : 6262
train acc:  0.8515625
train loss:  0.27849018573760986
train gradient:  0.12484339338246389
iteration : 6263
train acc:  0.8515625
train loss:  0.3202219009399414
train gradient:  0.17993856773824318
iteration : 6264
train acc:  0.8203125
train loss:  0.3363969326019287
train gradient:  0.25133650785971645
iteration : 6265
train acc:  0.8203125
train loss:  0.34793680906295776
train gradient:  0.26467571039058085
iteration : 6266
train acc:  0.9140625
train loss:  0.29678887128829956
train gradient:  0.2011651457356602
iteration : 6267
train acc:  0.8671875
train loss:  0.3145298957824707
train gradient:  0.1710545796775846
iteration : 6268
train acc:  0.7890625
train loss:  0.4187135696411133
train gradient:  0.2746948196289153
iteration : 6269
train acc:  0.875
train loss:  0.308854877948761
train gradient:  0.20648361793714456
iteration : 6270
train acc:  0.8125
train loss:  0.3827167749404907
train gradient:  0.18055579516917636
iteration : 6271
train acc:  0.8515625
train loss:  0.3588259816169739
train gradient:  0.23249282305380328
iteration : 6272
train acc:  0.84375
train loss:  0.34283512830734253
train gradient:  0.2799019524227599
iteration : 6273
train acc:  0.8125
train loss:  0.4221024513244629
train gradient:  0.1911094756978957
iteration : 6274
train acc:  0.8359375
train loss:  0.4032592177391052
train gradient:  0.22692725397129676
iteration : 6275
train acc:  0.859375
train loss:  0.2989349067211151
train gradient:  0.1827449621654371
iteration : 6276
train acc:  0.875
train loss:  0.3279901146888733
train gradient:  0.22497335944244037
iteration : 6277
train acc:  0.9453125
train loss:  0.22009921073913574
train gradient:  0.12722083708663098
iteration : 6278
train acc:  0.8984375
train loss:  0.22712716460227966
train gradient:  0.12350061639527768
iteration : 6279
train acc:  0.828125
train loss:  0.3381030559539795
train gradient:  0.18462788177762185
iteration : 6280
train acc:  0.875
train loss:  0.28116950392723083
train gradient:  0.17853324463627165
iteration : 6281
train acc:  0.8203125
train loss:  0.36420637369155884
train gradient:  0.24688180682007987
iteration : 6282
train acc:  0.8203125
train loss:  0.4328906536102295
train gradient:  0.2898698127106961
iteration : 6283
train acc:  0.8046875
train loss:  0.3948041498661041
train gradient:  0.20387269182636875
iteration : 6284
train acc:  0.8828125
train loss:  0.33407163619995117
train gradient:  0.19542344192340488
iteration : 6285
train acc:  0.796875
train loss:  0.44509464502334595
train gradient:  0.3262349715383962
iteration : 6286
train acc:  0.84375
train loss:  0.34025007486343384
train gradient:  0.23396085858527324
iteration : 6287
train acc:  0.859375
train loss:  0.30172884464263916
train gradient:  0.1643119502597186
iteration : 6288
train acc:  0.8203125
train loss:  0.3902292847633362
train gradient:  0.28499520565089925
iteration : 6289
train acc:  0.8203125
train loss:  0.3477898836135864
train gradient:  0.2094147334449774
iteration : 6290
train acc:  0.8515625
train loss:  0.38872194290161133
train gradient:  0.26121216848922685
iteration : 6291
train acc:  0.875
train loss:  0.3552216589450836
train gradient:  0.24504147838175627
iteration : 6292
train acc:  0.8828125
train loss:  0.25917139649391174
train gradient:  0.1554889830346859
iteration : 6293
train acc:  0.8203125
train loss:  0.3897923529148102
train gradient:  0.29141611766302006
iteration : 6294
train acc:  0.84375
train loss:  0.35839855670928955
train gradient:  0.21228255025239473
iteration : 6295
train acc:  0.875
train loss:  0.3624976873397827
train gradient:  0.21301034140592903
iteration : 6296
train acc:  0.859375
train loss:  0.2853974401950836
train gradient:  0.2015357037465112
iteration : 6297
train acc:  0.8515625
train loss:  0.3568626344203949
train gradient:  0.21645545568834196
iteration : 6298
train acc:  0.8125
train loss:  0.3723488748073578
train gradient:  0.297381842815438
iteration : 6299
train acc:  0.875
train loss:  0.3039456009864807
train gradient:  0.1485167030163175
iteration : 6300
train acc:  0.8671875
train loss:  0.3376145362854004
train gradient:  0.22414051535078242
iteration : 6301
train acc:  0.8515625
train loss:  0.4783647954463959
train gradient:  0.5526558925725551
iteration : 6302
train acc:  0.875
train loss:  0.3469529151916504
train gradient:  0.2320018853146983
iteration : 6303
train acc:  0.890625
train loss:  0.3045402467250824
train gradient:  0.1867547363895913
iteration : 6304
train acc:  0.8828125
train loss:  0.30587685108184814
train gradient:  0.21086098598898007
iteration : 6305
train acc:  0.828125
train loss:  0.3690977096557617
train gradient:  0.23780350561023184
iteration : 6306
train acc:  0.796875
train loss:  0.428792268037796
train gradient:  0.32917489771112185
iteration : 6307
train acc:  0.8359375
train loss:  0.32320958375930786
train gradient:  0.189293079264853
iteration : 6308
train acc:  0.84375
train loss:  0.3382728397846222
train gradient:  0.20972166016541605
iteration : 6309
train acc:  0.8515625
train loss:  0.3358587324619293
train gradient:  0.2800537467316965
iteration : 6310
train acc:  0.84375
train loss:  0.36346930265426636
train gradient:  0.32966279252408853
iteration : 6311
train acc:  0.8515625
train loss:  0.3377162218093872
train gradient:  0.23261013949149842
iteration : 6312
train acc:  0.78125
train loss:  0.3894309997558594
train gradient:  0.2609879574908898
iteration : 6313
train acc:  0.828125
train loss:  0.3369961977005005
train gradient:  0.296361542090679
iteration : 6314
train acc:  0.8203125
train loss:  0.3774091899394989
train gradient:  0.2235776646125054
iteration : 6315
train acc:  0.8515625
train loss:  0.3513072729110718
train gradient:  0.23693307663409635
iteration : 6316
train acc:  0.828125
train loss:  0.404985249042511
train gradient:  0.33474248538521
iteration : 6317
train acc:  0.8828125
train loss:  0.297951877117157
train gradient:  0.14770981669940245
iteration : 6318
train acc:  0.859375
train loss:  0.3146265149116516
train gradient:  0.260489742502817
iteration : 6319
train acc:  0.78125
train loss:  0.4461649954319
train gradient:  0.4093235777481419
iteration : 6320
train acc:  0.8046875
train loss:  0.3773816227912903
train gradient:  0.2875420463977079
iteration : 6321
train acc:  0.859375
train loss:  0.32564985752105713
train gradient:  0.18618538899629272
iteration : 6322
train acc:  0.8359375
train loss:  0.4043847322463989
train gradient:  0.32047764689699504
iteration : 6323
train acc:  0.8203125
train loss:  0.40612393617630005
train gradient:  0.3288129479558771
iteration : 6324
train acc:  0.8984375
train loss:  0.2567766606807709
train gradient:  0.1239642203191668
iteration : 6325
train acc:  0.890625
train loss:  0.370025634765625
train gradient:  0.2516462007252057
iteration : 6326
train acc:  0.8515625
train loss:  0.3492280840873718
train gradient:  0.2381918794925485
iteration : 6327
train acc:  0.8515625
train loss:  0.314547598361969
train gradient:  0.34794970003007814
iteration : 6328
train acc:  0.875
train loss:  0.25854697823524475
train gradient:  0.13724516420798602
iteration : 6329
train acc:  0.8515625
train loss:  0.37945324182510376
train gradient:  0.25244537866811445
iteration : 6330
train acc:  0.875
train loss:  0.30081573128700256
train gradient:  0.2521804869481124
iteration : 6331
train acc:  0.9140625
train loss:  0.2973339557647705
train gradient:  0.2197138065041811
iteration : 6332
train acc:  0.8203125
train loss:  0.36576133966445923
train gradient:  0.3105626142029218
iteration : 6333
train acc:  0.8203125
train loss:  0.3997483253479004
train gradient:  0.3003573340021545
iteration : 6334
train acc:  0.8671875
train loss:  0.3110739588737488
train gradient:  0.18301653551011374
iteration : 6335
train acc:  0.859375
train loss:  0.3048601448535919
train gradient:  0.21924491296348386
iteration : 6336
train acc:  0.8515625
train loss:  0.3578561544418335
train gradient:  0.17292500491254273
iteration : 6337
train acc:  0.828125
train loss:  0.3896472454071045
train gradient:  0.35948296021263576
iteration : 6338
train acc:  0.84375
train loss:  0.3328196704387665
train gradient:  0.2365107984471011
iteration : 6339
train acc:  0.8359375
train loss:  0.3485390245914459
train gradient:  0.17608030850591344
iteration : 6340
train acc:  0.859375
train loss:  0.29600146412849426
train gradient:  0.13804815525867223
iteration : 6341
train acc:  0.8515625
train loss:  0.30776405334472656
train gradient:  0.14621978471115088
iteration : 6342
train acc:  0.84375
train loss:  0.37892788648605347
train gradient:  0.29941948390297196
iteration : 6343
train acc:  0.84375
train loss:  0.38378724455833435
train gradient:  0.14793909935280952
iteration : 6344
train acc:  0.828125
train loss:  0.3795265257358551
train gradient:  0.21915597119841151
iteration : 6345
train acc:  0.8359375
train loss:  0.33040255308151245
train gradient:  0.15671066522422977
iteration : 6346
train acc:  0.8125
train loss:  0.4130670428276062
train gradient:  0.2807366851563213
iteration : 6347
train acc:  0.875
train loss:  0.43321889638900757
train gradient:  0.28149723595493165
iteration : 6348
train acc:  0.875
train loss:  0.30041536688804626
train gradient:  0.28126826987482595
iteration : 6349
train acc:  0.8359375
train loss:  0.39409905672073364
train gradient:  0.18832743338846178
iteration : 6350
train acc:  0.8671875
train loss:  0.2896350622177124
train gradient:  0.13487531836135175
iteration : 6351
train acc:  0.828125
train loss:  0.3866601586341858
train gradient:  0.22886221751860633
iteration : 6352
train acc:  0.8515625
train loss:  0.34104782342910767
train gradient:  0.3433639778079057
iteration : 6353
train acc:  0.8984375
train loss:  0.2934824228286743
train gradient:  0.18991774703189307
iteration : 6354
train acc:  0.8671875
train loss:  0.34347906708717346
train gradient:  0.2359734161277441
iteration : 6355
train acc:  0.8359375
train loss:  0.3911338746547699
train gradient:  0.2888718747127799
iteration : 6356
train acc:  0.828125
train loss:  0.38343775272369385
train gradient:  0.22246243809270552
iteration : 6357
train acc:  0.875
train loss:  0.31495821475982666
train gradient:  0.1705838032079603
iteration : 6358
train acc:  0.8203125
train loss:  0.3844929337501526
train gradient:  0.2090469587742413
iteration : 6359
train acc:  0.875
train loss:  0.30580100417137146
train gradient:  0.25049118533156123
iteration : 6360
train acc:  0.890625
train loss:  0.2893357276916504
train gradient:  0.14029103046860428
iteration : 6361
train acc:  0.8046875
train loss:  0.4463143050670624
train gradient:  0.30095861267805313
iteration : 6362
train acc:  0.8515625
train loss:  0.32192039489746094
train gradient:  0.20483656431248692
iteration : 6363
train acc:  0.8671875
train loss:  0.3361019492149353
train gradient:  0.2287488197302487
iteration : 6364
train acc:  0.9140625
train loss:  0.28361406922340393
train gradient:  0.18621518099216577
iteration : 6365
train acc:  0.8984375
train loss:  0.289174348115921
train gradient:  0.1267394507167852
iteration : 6366
train acc:  0.8203125
train loss:  0.365689754486084
train gradient:  0.20536140949712217
iteration : 6367
train acc:  0.859375
train loss:  0.34502676129341125
train gradient:  0.22436855589030624
iteration : 6368
train acc:  0.8671875
train loss:  0.30415087938308716
train gradient:  0.1733516473821164
iteration : 6369
train acc:  0.859375
train loss:  0.34560883045196533
train gradient:  0.21359220971505433
iteration : 6370
train acc:  0.78125
train loss:  0.3951280117034912
train gradient:  0.2549079329370658
iteration : 6371
train acc:  0.8203125
train loss:  0.4206690490245819
train gradient:  0.4565040569542977
iteration : 6372
train acc:  0.8046875
train loss:  0.41377800703048706
train gradient:  0.2142822466110546
iteration : 6373
train acc:  0.84375
train loss:  0.3401097357273102
train gradient:  0.18631664557341476
iteration : 6374
train acc:  0.828125
train loss:  0.3544612228870392
train gradient:  0.19104376235760034
iteration : 6375
train acc:  0.8515625
train loss:  0.3559718132019043
train gradient:  0.1906153663488956
iteration : 6376
train acc:  0.84375
train loss:  0.3515818417072296
train gradient:  0.19664853370591087
iteration : 6377
train acc:  0.8828125
train loss:  0.28045302629470825
train gradient:  0.14673155446993685
iteration : 6378
train acc:  0.8125
train loss:  0.36625412106513977
train gradient:  0.22504476284529606
iteration : 6379
train acc:  0.828125
train loss:  0.39872050285339355
train gradient:  0.2638054489296421
iteration : 6380
train acc:  0.8046875
train loss:  0.35496050119400024
train gradient:  0.2623766614273167
iteration : 6381
train acc:  0.84375
train loss:  0.3452075719833374
train gradient:  0.25524982597318135
iteration : 6382
train acc:  0.8671875
train loss:  0.31368178129196167
train gradient:  0.17235938061769557
iteration : 6383
train acc:  0.8515625
train loss:  0.38686516880989075
train gradient:  0.26362676216357006
iteration : 6384
train acc:  0.875
train loss:  0.2960304617881775
train gradient:  0.1432006167683821
iteration : 6385
train acc:  0.921875
train loss:  0.2821803689002991
train gradient:  0.13880033619987683
iteration : 6386
train acc:  0.890625
train loss:  0.28057044744491577
train gradient:  0.23402620006751151
iteration : 6387
train acc:  0.8671875
train loss:  0.3381498157978058
train gradient:  0.1889544319793385
iteration : 6388
train acc:  0.8515625
train loss:  0.3642808794975281
train gradient:  0.19992986338842048
iteration : 6389
train acc:  0.8515625
train loss:  0.3676275312900543
train gradient:  0.20593248754829657
iteration : 6390
train acc:  0.84375
train loss:  0.32529503107070923
train gradient:  0.16296738410335843
iteration : 6391
train acc:  0.8046875
train loss:  0.373653382062912
train gradient:  0.28983676003482706
iteration : 6392
train acc:  0.828125
train loss:  0.37715303897857666
train gradient:  0.29704450051777986
iteration : 6393
train acc:  0.8046875
train loss:  0.4341791868209839
train gradient:  0.2267423522591434
iteration : 6394
train acc:  0.859375
train loss:  0.3404252827167511
train gradient:  0.23178233738134396
iteration : 6395
train acc:  0.8671875
train loss:  0.33274656534194946
train gradient:  0.13358956410668693
iteration : 6396
train acc:  0.8125
train loss:  0.3822411000728607
train gradient:  0.3626222596430775
iteration : 6397
train acc:  0.8359375
train loss:  0.3386881649494171
train gradient:  0.14096067134141188
iteration : 6398
train acc:  0.8203125
train loss:  0.3358467221260071
train gradient:  0.25200491841969597
iteration : 6399
train acc:  0.875
train loss:  0.32311075925827026
train gradient:  0.146185865385732
iteration : 6400
train acc:  0.8203125
train loss:  0.3672441244125366
train gradient:  0.2087029294023278
iteration : 6401
train acc:  0.8515625
train loss:  0.2961812913417816
train gradient:  0.1655418734110342
iteration : 6402
train acc:  0.8359375
train loss:  0.32994186878204346
train gradient:  0.1912692783729027
iteration : 6403
train acc:  0.8046875
train loss:  0.4606642723083496
train gradient:  0.35140491002100077
iteration : 6404
train acc:  0.8515625
train loss:  0.32931309938430786
train gradient:  0.14622301299113855
iteration : 6405
train acc:  0.8671875
train loss:  0.3279687464237213
train gradient:  0.27709676338673905
iteration : 6406
train acc:  0.84375
train loss:  0.31523919105529785
train gradient:  0.1765094360046888
iteration : 6407
train acc:  0.828125
train loss:  0.396073579788208
train gradient:  0.21247595254737817
iteration : 6408
train acc:  0.8125
train loss:  0.3643454909324646
train gradient:  0.2317913698818035
iteration : 6409
train acc:  0.8515625
train loss:  0.30181434750556946
train gradient:  0.21484234122605797
iteration : 6410
train acc:  0.8515625
train loss:  0.3292897045612335
train gradient:  0.18515928735561793
iteration : 6411
train acc:  0.8671875
train loss:  0.35906267166137695
train gradient:  0.1821962692232721
iteration : 6412
train acc:  0.8203125
train loss:  0.3433321714401245
train gradient:  0.2637549677621162
iteration : 6413
train acc:  0.84375
train loss:  0.33423686027526855
train gradient:  0.2105828083973345
iteration : 6414
train acc:  0.859375
train loss:  0.31927451491355896
train gradient:  0.2158052852077733
iteration : 6415
train acc:  0.890625
train loss:  0.2705153822898865
train gradient:  0.15191244109935254
iteration : 6416
train acc:  0.84375
train loss:  0.3124300241470337
train gradient:  0.19342211750175523
iteration : 6417
train acc:  0.8125
train loss:  0.3040003776550293
train gradient:  0.15343352090286122
iteration : 6418
train acc:  0.8828125
train loss:  0.29028642177581787
train gradient:  0.1514857566407396
iteration : 6419
train acc:  0.90625
train loss:  0.2750980257987976
train gradient:  0.1577282624191518
iteration : 6420
train acc:  0.8125
train loss:  0.3511718511581421
train gradient:  0.20129700273901133
iteration : 6421
train acc:  0.859375
train loss:  0.3560643196105957
train gradient:  0.19805526243757596
iteration : 6422
train acc:  0.8515625
train loss:  0.32612359523773193
train gradient:  0.18762083990363537
iteration : 6423
train acc:  0.890625
train loss:  0.32753393054008484
train gradient:  0.2633756875677293
iteration : 6424
train acc:  0.796875
train loss:  0.38218384981155396
train gradient:  0.1992672893007759
iteration : 6425
train acc:  0.828125
train loss:  0.3032815158367157
train gradient:  0.21003985870324926
iteration : 6426
train acc:  0.8046875
train loss:  0.41123801469802856
train gradient:  0.27092099839084366
iteration : 6427
train acc:  0.8828125
train loss:  0.2881140410900116
train gradient:  0.19291136387844154
iteration : 6428
train acc:  0.8671875
train loss:  0.3893168270587921
train gradient:  0.2992573098016217
iteration : 6429
train acc:  0.8359375
train loss:  0.3859572410583496
train gradient:  0.3406564478814796
iteration : 6430
train acc:  0.828125
train loss:  0.3985145092010498
train gradient:  0.2987831602513276
iteration : 6431
train acc:  0.8515625
train loss:  0.36020249128341675
train gradient:  0.27660696865401185
iteration : 6432
train acc:  0.84375
train loss:  0.3649362325668335
train gradient:  0.29009947035089356
iteration : 6433
train acc:  0.8984375
train loss:  0.27362385392189026
train gradient:  0.16919685067623746
iteration : 6434
train acc:  0.78125
train loss:  0.40738970041275024
train gradient:  0.27705370794474926
iteration : 6435
train acc:  0.8359375
train loss:  0.300838828086853
train gradient:  0.23234415486486423
iteration : 6436
train acc:  0.8203125
train loss:  0.3815993368625641
train gradient:  0.31562391844783605
iteration : 6437
train acc:  0.8359375
train loss:  0.34638041257858276
train gradient:  0.2329971960167143
iteration : 6438
train acc:  0.8828125
train loss:  0.30143487453460693
train gradient:  0.25936983043644657
iteration : 6439
train acc:  0.875
train loss:  0.29236677289009094
train gradient:  0.13968858666098238
iteration : 6440
train acc:  0.890625
train loss:  0.3485192060470581
train gradient:  0.2266710699466862
iteration : 6441
train acc:  0.859375
train loss:  0.3159429430961609
train gradient:  0.34720083163752863
iteration : 6442
train acc:  0.921875
train loss:  0.24714669585227966
train gradient:  0.17358959671672902
iteration : 6443
train acc:  0.8515625
train loss:  0.36009135842323303
train gradient:  0.17434837056484426
iteration : 6444
train acc:  0.8828125
train loss:  0.29338976740837097
train gradient:  0.13283787912123649
iteration : 6445
train acc:  0.8359375
train loss:  0.36769720911979675
train gradient:  0.2807918059660681
iteration : 6446
train acc:  0.859375
train loss:  0.3380504846572876
train gradient:  0.18681136027482273
iteration : 6447
train acc:  0.8046875
train loss:  0.40182462334632874
train gradient:  0.3292167871589686
iteration : 6448
train acc:  0.8671875
train loss:  0.2666003108024597
train gradient:  0.14100999897048502
iteration : 6449
train acc:  0.84375
train loss:  0.38781803846359253
train gradient:  0.192266600593192
iteration : 6450
train acc:  0.8125
train loss:  0.36760473251342773
train gradient:  0.24616643530249196
iteration : 6451
train acc:  0.8671875
train loss:  0.3635118901729584
train gradient:  0.29878223523107716
iteration : 6452
train acc:  0.875
train loss:  0.35041630268096924
train gradient:  0.25015237754234726
iteration : 6453
train acc:  0.84375
train loss:  0.4422749876976013
train gradient:  0.2836793172041657
iteration : 6454
train acc:  0.8359375
train loss:  0.31403055787086487
train gradient:  0.24363111162278678
iteration : 6455
train acc:  0.8125
train loss:  0.39538830518722534
train gradient:  0.22739681216994737
iteration : 6456
train acc:  0.84375
train loss:  0.3500177562236786
train gradient:  0.20295958686588644
iteration : 6457
train acc:  0.78125
train loss:  0.4437927007675171
train gradient:  0.2686505238028577
iteration : 6458
train acc:  0.84375
train loss:  0.322675496339798
train gradient:  0.17882658051859895
iteration : 6459
train acc:  0.828125
train loss:  0.38591691851615906
train gradient:  0.2719062120651722
iteration : 6460
train acc:  0.859375
train loss:  0.29993540048599243
train gradient:  0.1652024994544884
iteration : 6461
train acc:  0.8671875
train loss:  0.3657797574996948
train gradient:  0.22433736926634096
iteration : 6462
train acc:  0.8984375
train loss:  0.31067776679992676
train gradient:  0.17039579173136435
iteration : 6463
train acc:  0.859375
train loss:  0.3317485451698303
train gradient:  0.27782695571698923
iteration : 6464
train acc:  0.84375
train loss:  0.3428475260734558
train gradient:  0.2761974850896272
iteration : 6465
train acc:  0.8515625
train loss:  0.3072355389595032
train gradient:  0.17820200865635438
iteration : 6466
train acc:  0.8984375
train loss:  0.284637987613678
train gradient:  0.14004688016764683
iteration : 6467
train acc:  0.8515625
train loss:  0.31262117624282837
train gradient:  0.2355475932329577
iteration : 6468
train acc:  0.8359375
train loss:  0.34994369745254517
train gradient:  0.1780129329738106
iteration : 6469
train acc:  0.8359375
train loss:  0.3378387987613678
train gradient:  0.23225144603212614
iteration : 6470
train acc:  0.8515625
train loss:  0.34699466824531555
train gradient:  0.2141644789249318
iteration : 6471
train acc:  0.8359375
train loss:  0.329860121011734
train gradient:  0.2694399448756864
iteration : 6472
train acc:  0.8984375
train loss:  0.2690984904766083
train gradient:  0.30570709487387415
iteration : 6473
train acc:  0.796875
train loss:  0.36072516441345215
train gradient:  0.35015746691034966
iteration : 6474
train acc:  0.84375
train loss:  0.3255971074104309
train gradient:  0.1658520026256849
iteration : 6475
train acc:  0.8671875
train loss:  0.33223825693130493
train gradient:  0.22195592900900626
iteration : 6476
train acc:  0.859375
train loss:  0.34107351303100586
train gradient:  0.2626423727550596
iteration : 6477
train acc:  0.8359375
train loss:  0.3765401244163513
train gradient:  0.2518373963880548
iteration : 6478
train acc:  0.8046875
train loss:  0.4247850477695465
train gradient:  0.2550124098102843
iteration : 6479
train acc:  0.828125
train loss:  0.4104940593242645
train gradient:  0.3500681390331817
iteration : 6480
train acc:  0.8203125
train loss:  0.3592495918273926
train gradient:  0.30919620031328376
iteration : 6481
train acc:  0.8671875
train loss:  0.32715362310409546
train gradient:  0.20403395878162645
iteration : 6482
train acc:  0.78125
train loss:  0.4670811891555786
train gradient:  0.46751909613439896
iteration : 6483
train acc:  0.84375
train loss:  0.3780776262283325
train gradient:  0.2391243089542595
iteration : 6484
train acc:  0.8125
train loss:  0.41384777426719666
train gradient:  0.2907119874486273
iteration : 6485
train acc:  0.84375
train loss:  0.30701327323913574
train gradient:  0.2846965142369374
iteration : 6486
train acc:  0.859375
train loss:  0.33313268423080444
train gradient:  0.14242603782293284
iteration : 6487
train acc:  0.796875
train loss:  0.3506189286708832
train gradient:  0.18489564564218755
iteration : 6488
train acc:  0.8515625
train loss:  0.3752557635307312
train gradient:  0.2887392745082514
iteration : 6489
train acc:  0.859375
train loss:  0.27683714032173157
train gradient:  0.23145567016194557
iteration : 6490
train acc:  0.8359375
train loss:  0.40326571464538574
train gradient:  0.27022209690989085
iteration : 6491
train acc:  0.8671875
train loss:  0.3234337270259857
train gradient:  0.17508487288947097
iteration : 6492
train acc:  0.828125
train loss:  0.3456437289714813
train gradient:  0.346017432191633
iteration : 6493
train acc:  0.8359375
train loss:  0.3237811326980591
train gradient:  0.2644497861756721
iteration : 6494
train acc:  0.84375
train loss:  0.32134073972702026
train gradient:  0.17593242501266199
iteration : 6495
train acc:  0.828125
train loss:  0.39544475078582764
train gradient:  0.26920854560223717
iteration : 6496
train acc:  0.859375
train loss:  0.31023454666137695
train gradient:  0.1767165547522135
iteration : 6497
train acc:  0.828125
train loss:  0.34672272205352783
train gradient:  0.33032078917988594
iteration : 6498
train acc:  0.84375
train loss:  0.3373197615146637
train gradient:  0.17610583063423296
iteration : 6499
train acc:  0.859375
train loss:  0.3186371326446533
train gradient:  0.17483832099799396
iteration : 6500
train acc:  0.8125
train loss:  0.3913586735725403
train gradient:  0.3385478629875229
iteration : 6501
train acc:  0.8828125
train loss:  0.3175428509712219
train gradient:  0.16081732698867235
iteration : 6502
train acc:  0.8828125
train loss:  0.30393338203430176
train gradient:  0.21387563737665308
iteration : 6503
train acc:  0.828125
train loss:  0.37774550914764404
train gradient:  0.1821988283664951
iteration : 6504
train acc:  0.8125
train loss:  0.4290085732936859
train gradient:  0.3750994368624659
iteration : 6505
train acc:  0.8046875
train loss:  0.35032251477241516
train gradient:  0.304830184193577
iteration : 6506
train acc:  0.84375
train loss:  0.3218759000301361
train gradient:  0.17579826083951286
iteration : 6507
train acc:  0.828125
train loss:  0.3509722948074341
train gradient:  0.4873432168949716
iteration : 6508
train acc:  0.8515625
train loss:  0.3191179633140564
train gradient:  0.2515411508444774
iteration : 6509
train acc:  0.8203125
train loss:  0.37289959192276
train gradient:  0.25428079617804916
iteration : 6510
train acc:  0.7890625
train loss:  0.45583829283714294
train gradient:  0.2882080324069107
iteration : 6511
train acc:  0.859375
train loss:  0.3833562135696411
train gradient:  0.2744101139418756
iteration : 6512
train acc:  0.875
train loss:  0.3145196735858917
train gradient:  0.22492752680896166
iteration : 6513
train acc:  0.8671875
train loss:  0.32658910751342773
train gradient:  0.20631721334252318
iteration : 6514
train acc:  0.8515625
train loss:  0.3358421325683594
train gradient:  0.3253987549243377
iteration : 6515
train acc:  0.859375
train loss:  0.3477814793586731
train gradient:  0.14664337867039595
iteration : 6516
train acc:  0.859375
train loss:  0.3299488425254822
train gradient:  0.1729303664511189
iteration : 6517
train acc:  0.875
train loss:  0.29161790013313293
train gradient:  0.22031474212993835
iteration : 6518
train acc:  0.8359375
train loss:  0.3144708275794983
train gradient:  0.2154494968302242
iteration : 6519
train acc:  0.8515625
train loss:  0.2760861814022064
train gradient:  0.16364550126156313
iteration : 6520
train acc:  0.8671875
train loss:  0.2989055812358856
train gradient:  0.159788389297301
iteration : 6521
train acc:  0.875
train loss:  0.3219119608402252
train gradient:  0.17120492587330643
iteration : 6522
train acc:  0.8515625
train loss:  0.3198980689048767
train gradient:  0.23149580362752947
iteration : 6523
train acc:  0.828125
train loss:  0.36420345306396484
train gradient:  0.27702048684954483
iteration : 6524
train acc:  0.796875
train loss:  0.41234898567199707
train gradient:  0.24313400664672863
iteration : 6525
train acc:  0.875
train loss:  0.31616345047950745
train gradient:  0.20579687591896495
iteration : 6526
train acc:  0.8671875
train loss:  0.34866005182266235
train gradient:  0.17205777706914316
iteration : 6527
train acc:  0.84375
train loss:  0.36614030599594116
train gradient:  0.21782630031549352
iteration : 6528
train acc:  0.859375
train loss:  0.34529539942741394
train gradient:  0.29690803940761235
iteration : 6529
train acc:  0.8359375
train loss:  0.3719707429409027
train gradient:  0.28896996440725387
iteration : 6530
train acc:  0.8125
train loss:  0.4123256206512451
train gradient:  0.35598992200882257
iteration : 6531
train acc:  0.8828125
train loss:  0.26901817321777344
train gradient:  0.11053907732317057
iteration : 6532
train acc:  0.875
train loss:  0.32451480627059937
train gradient:  0.19110164902385698
iteration : 6533
train acc:  0.8359375
train loss:  0.3744608163833618
train gradient:  0.28463822436415315
iteration : 6534
train acc:  0.9140625
train loss:  0.2898751497268677
train gradient:  0.1386572216942269
iteration : 6535
train acc:  0.890625
train loss:  0.2743171155452728
train gradient:  0.18704624043541185
iteration : 6536
train acc:  0.875
train loss:  0.31460076570510864
train gradient:  0.2047578247069525
iteration : 6537
train acc:  0.8515625
train loss:  0.37564241886138916
train gradient:  0.40404226466927606
iteration : 6538
train acc:  0.8203125
train loss:  0.46719086170196533
train gradient:  0.8023298007881072
iteration : 6539
train acc:  0.8671875
train loss:  0.2994638979434967
train gradient:  0.24501307750444667
iteration : 6540
train acc:  0.859375
train loss:  0.3267047703266144
train gradient:  0.23766732134963642
iteration : 6541
train acc:  0.84375
train loss:  0.3315199017524719
train gradient:  0.17977567188928467
iteration : 6542
train acc:  0.84375
train loss:  0.3711584806442261
train gradient:  0.27130455275918686
iteration : 6543
train acc:  0.8984375
train loss:  0.29937461018562317
train gradient:  0.15905371356048245
iteration : 6544
train acc:  0.875
train loss:  0.3318488597869873
train gradient:  0.2636856397149435
iteration : 6545
train acc:  0.828125
train loss:  0.40639495849609375
train gradient:  0.24393035860329568
iteration : 6546
train acc:  0.859375
train loss:  0.3205762505531311
train gradient:  0.1703083180875299
iteration : 6547
train acc:  0.8203125
train loss:  0.4000440239906311
train gradient:  0.23108190367142198
iteration : 6548
train acc:  0.84375
train loss:  0.3124498426914215
train gradient:  0.21019005230507026
iteration : 6549
train acc:  0.84375
train loss:  0.395912230014801
train gradient:  0.2008752197889335
iteration : 6550
train acc:  0.828125
train loss:  0.32653623819351196
train gradient:  0.16619271260402796
iteration : 6551
train acc:  0.8515625
train loss:  0.340832382440567
train gradient:  0.2508787118702426
iteration : 6552
train acc:  0.8828125
train loss:  0.3518425226211548
train gradient:  0.1890014272857699
iteration : 6553
train acc:  0.890625
train loss:  0.30749568343162537
train gradient:  0.20720394154845945
iteration : 6554
train acc:  0.8671875
train loss:  0.29355448484420776
train gradient:  0.16340178331370742
iteration : 6555
train acc:  0.8125
train loss:  0.3654780089855194
train gradient:  0.20947958794017396
iteration : 6556
train acc:  0.8515625
train loss:  0.2860807776451111
train gradient:  0.13162441737625202
iteration : 6557
train acc:  0.8828125
train loss:  0.3053549528121948
train gradient:  0.23127036503690646
iteration : 6558
train acc:  0.8359375
train loss:  0.3632538616657257
train gradient:  0.30987679928911516
iteration : 6559
train acc:  0.84375
train loss:  0.39630016684532166
train gradient:  0.284844663563086
iteration : 6560
train acc:  0.8125
train loss:  0.3509719967842102
train gradient:  0.27332119415340383
iteration : 6561
train acc:  0.8359375
train loss:  0.3470340967178345
train gradient:  0.23725707425666862
iteration : 6562
train acc:  0.84375
train loss:  0.3824961185455322
train gradient:  0.30465534248763426
iteration : 6563
train acc:  0.890625
train loss:  0.2846384048461914
train gradient:  0.18411997100228072
iteration : 6564
train acc:  0.8203125
train loss:  0.39803752303123474
train gradient:  0.2575453700622813
iteration : 6565
train acc:  0.828125
train loss:  0.34852683544158936
train gradient:  0.1881601171865502
iteration : 6566
train acc:  0.8359375
train loss:  0.45124489068984985
train gradient:  0.24728174888150575
iteration : 6567
train acc:  0.8984375
train loss:  0.3048701882362366
train gradient:  0.19183696641678794
iteration : 6568
train acc:  0.890625
train loss:  0.28142768144607544
train gradient:  0.1191892161223954
iteration : 6569
train acc:  0.859375
train loss:  0.36009639501571655
train gradient:  0.19640912750719391
iteration : 6570
train acc:  0.859375
train loss:  0.29494625329971313
train gradient:  0.38643503460126805
iteration : 6571
train acc:  0.8671875
train loss:  0.2821688652038574
train gradient:  0.18017048575448436
iteration : 6572
train acc:  0.828125
train loss:  0.40561676025390625
train gradient:  0.3189103398366098
iteration : 6573
train acc:  0.84375
train loss:  0.32403305172920227
train gradient:  0.3794847964265044
iteration : 6574
train acc:  0.84375
train loss:  0.32697826623916626
train gradient:  0.176761322600616
iteration : 6575
train acc:  0.8359375
train loss:  0.32917970418930054
train gradient:  0.2203431778334688
iteration : 6576
train acc:  0.890625
train loss:  0.28483763337135315
train gradient:  0.16930055955481293
iteration : 6577
train acc:  0.8359375
train loss:  0.35249873995780945
train gradient:  0.19727345044565198
iteration : 6578
train acc:  0.8515625
train loss:  0.29168039560317993
train gradient:  0.1636477240668736
iteration : 6579
train acc:  0.828125
train loss:  0.3297653794288635
train gradient:  0.1925335460783985
iteration : 6580
train acc:  0.8828125
train loss:  0.30808478593826294
train gradient:  0.16898539576553212
iteration : 6581
train acc:  0.875
train loss:  0.3455745577812195
train gradient:  0.19961441995304985
iteration : 6582
train acc:  0.90625
train loss:  0.2796761095523834
train gradient:  0.15055204979365466
iteration : 6583
train acc:  0.8515625
train loss:  0.3271697461605072
train gradient:  0.22388663410922155
iteration : 6584
train acc:  0.8828125
train loss:  0.2811124324798584
train gradient:  0.16440605720448878
iteration : 6585
train acc:  0.875
train loss:  0.31474655866622925
train gradient:  0.16743760705223337
iteration : 6586
train acc:  0.8046875
train loss:  0.3886275291442871
train gradient:  0.3062491121742512
iteration : 6587
train acc:  0.9140625
train loss:  0.2631913125514984
train gradient:  0.13768901817727228
iteration : 6588
train acc:  0.859375
train loss:  0.35326695442199707
train gradient:  0.1683293708018626
iteration : 6589
train acc:  0.875
train loss:  0.30674538016319275
train gradient:  0.1692377157342333
iteration : 6590
train acc:  0.84375
train loss:  0.34804385900497437
train gradient:  0.269670986000341
iteration : 6591
train acc:  0.8515625
train loss:  0.407509446144104
train gradient:  0.26498809772314125
iteration : 6592
train acc:  0.875
train loss:  0.28202855587005615
train gradient:  0.18367385200192748
iteration : 6593
train acc:  0.890625
train loss:  0.2941662669181824
train gradient:  0.16375569121639477
iteration : 6594
train acc:  0.828125
train loss:  0.43392080068588257
train gradient:  0.26348883227884
iteration : 6595
train acc:  0.875
train loss:  0.2890625
train gradient:  0.16919872054144913
iteration : 6596
train acc:  0.796875
train loss:  0.4060487449169159
train gradient:  0.274941104172688
iteration : 6597
train acc:  0.84375
train loss:  0.34147384762763977
train gradient:  0.25705274666233396
iteration : 6598
train acc:  0.8828125
train loss:  0.3099955916404724
train gradient:  0.16756310313393608
iteration : 6599
train acc:  0.7421875
train loss:  0.5122299790382385
train gradient:  0.42104910960274305
iteration : 6600
train acc:  0.8671875
train loss:  0.2985762357711792
train gradient:  0.14797556262306713
iteration : 6601
train acc:  0.8203125
train loss:  0.4202668070793152
train gradient:  0.3490901455921838
iteration : 6602
train acc:  0.796875
train loss:  0.4227161109447479
train gradient:  0.34274120255984736
iteration : 6603
train acc:  0.828125
train loss:  0.345603346824646
train gradient:  0.23208073153952039
iteration : 6604
train acc:  0.875
train loss:  0.3122379779815674
train gradient:  0.18536677400144846
iteration : 6605
train acc:  0.8515625
train loss:  0.34911006689071655
train gradient:  0.16563852104348045
iteration : 6606
train acc:  0.8359375
train loss:  0.4026392698287964
train gradient:  0.23732771664848465
iteration : 6607
train acc:  0.84375
train loss:  0.3286231458187103
train gradient:  0.19794245714864456
iteration : 6608
train acc:  0.8125
train loss:  0.37343382835388184
train gradient:  0.43086314074593496
iteration : 6609
train acc:  0.796875
train loss:  0.3873330354690552
train gradient:  0.21928658113124913
iteration : 6610
train acc:  0.796875
train loss:  0.4387272894382477
train gradient:  0.32157273002450737
iteration : 6611
train acc:  0.875
train loss:  0.3236657679080963
train gradient:  0.2976934581920348
iteration : 6612
train acc:  0.828125
train loss:  0.3258601129055023
train gradient:  0.21766582358581688
iteration : 6613
train acc:  0.8203125
train loss:  0.36150234937667847
train gradient:  0.3126785000551242
iteration : 6614
train acc:  0.921875
train loss:  0.23564419150352478
train gradient:  0.15181687184707493
iteration : 6615
train acc:  0.828125
train loss:  0.33989980816841125
train gradient:  0.17796399161665724
iteration : 6616
train acc:  0.8671875
train loss:  0.3115497827529907
train gradient:  0.15768807724341183
iteration : 6617
train acc:  0.8203125
train loss:  0.3864021897315979
train gradient:  0.25809814944859516
iteration : 6618
train acc:  0.890625
train loss:  0.3020004630088806
train gradient:  0.18733974528090694
iteration : 6619
train acc:  0.8828125
train loss:  0.3447972536087036
train gradient:  0.16748138672948487
iteration : 6620
train acc:  0.8359375
train loss:  0.42501136660575867
train gradient:  0.24621145808880815
iteration : 6621
train acc:  0.8671875
train loss:  0.37511005997657776
train gradient:  0.2982463218322747
iteration : 6622
train acc:  0.875
train loss:  0.3136233687400818
train gradient:  0.1591542835159485
iteration : 6623
train acc:  0.8125
train loss:  0.41633427143096924
train gradient:  0.21736728657543497
iteration : 6624
train acc:  0.875
train loss:  0.32302409410476685
train gradient:  0.1636166429743402
iteration : 6625
train acc:  0.890625
train loss:  0.2843714654445648
train gradient:  0.13576220955454305
iteration : 6626
train acc:  0.828125
train loss:  0.3741661310195923
train gradient:  0.2524046036182696
iteration : 6627
train acc:  0.8671875
train loss:  0.36525577306747437
train gradient:  0.2238869942606998
iteration : 6628
train acc:  0.8125
train loss:  0.41850364208221436
train gradient:  0.35646259775394556
iteration : 6629
train acc:  0.8828125
train loss:  0.28340741991996765
train gradient:  0.16909773866003425
iteration : 6630
train acc:  0.796875
train loss:  0.4509839415550232
train gradient:  0.2364169854796777
iteration : 6631
train acc:  0.828125
train loss:  0.3926714062690735
train gradient:  0.3444541587758962
iteration : 6632
train acc:  0.84375
train loss:  0.361364483833313
train gradient:  0.23415808206556377
iteration : 6633
train acc:  0.8203125
train loss:  0.3826701045036316
train gradient:  0.2273068187683021
iteration : 6634
train acc:  0.84375
train loss:  0.3236697316169739
train gradient:  0.20507049299102814
iteration : 6635
train acc:  0.8515625
train loss:  0.36536699533462524
train gradient:  0.31562336466994095
iteration : 6636
train acc:  0.8828125
train loss:  0.3591974973678589
train gradient:  0.20123762331346273
iteration : 6637
train acc:  0.8515625
train loss:  0.34201133251190186
train gradient:  0.14801457669074075
iteration : 6638
train acc:  0.875
train loss:  0.32992321252822876
train gradient:  0.22238412503649121
iteration : 6639
train acc:  0.859375
train loss:  0.3264681100845337
train gradient:  0.15271823914601285
iteration : 6640
train acc:  0.8125
train loss:  0.40417784452438354
train gradient:  0.28391690963123245
iteration : 6641
train acc:  0.890625
train loss:  0.27323204278945923
train gradient:  0.15642473467701992
iteration : 6642
train acc:  0.859375
train loss:  0.33781611919403076
train gradient:  0.20207157734188724
iteration : 6643
train acc:  0.8515625
train loss:  0.33009254932403564
train gradient:  0.17472604435223243
iteration : 6644
train acc:  0.828125
train loss:  0.3539334535598755
train gradient:  0.2621564485546508
iteration : 6645
train acc:  0.8125
train loss:  0.39207351207733154
train gradient:  0.22236527276354118
iteration : 6646
train acc:  0.8046875
train loss:  0.3594350218772888
train gradient:  0.19430749296000555
iteration : 6647
train acc:  0.8671875
train loss:  0.3125746250152588
train gradient:  0.16084358262274162
iteration : 6648
train acc:  0.8046875
train loss:  0.3902062773704529
train gradient:  0.20759624161030432
iteration : 6649
train acc:  0.8671875
train loss:  0.4139975905418396
train gradient:  0.26421581692069585
iteration : 6650
train acc:  0.8515625
train loss:  0.3313668370246887
train gradient:  0.2888349382010656
iteration : 6651
train acc:  0.828125
train loss:  0.44205498695373535
train gradient:  0.3076527863894938
iteration : 6652
train acc:  0.8359375
train loss:  0.3254040479660034
train gradient:  0.21287628309229437
iteration : 6653
train acc:  0.828125
train loss:  0.32243648171424866
train gradient:  0.125200176742215
iteration : 6654
train acc:  0.8828125
train loss:  0.28530019521713257
train gradient:  0.12427659521108278
iteration : 6655
train acc:  0.84375
train loss:  0.3376004695892334
train gradient:  0.145279904350607
iteration : 6656
train acc:  0.8359375
train loss:  0.40477025508880615
train gradient:  0.37466687829875783
iteration : 6657
train acc:  0.84375
train loss:  0.34065040946006775
train gradient:  0.21834797533803635
iteration : 6658
train acc:  0.8359375
train loss:  0.34057265520095825
train gradient:  0.23177691501438707
iteration : 6659
train acc:  0.890625
train loss:  0.2688657343387604
train gradient:  0.1080721875042256
iteration : 6660
train acc:  0.8359375
train loss:  0.3630797863006592
train gradient:  0.23307026776996764
iteration : 6661
train acc:  0.7890625
train loss:  0.41911473870277405
train gradient:  0.2600975590379845
iteration : 6662
train acc:  0.8203125
train loss:  0.3825019299983978
train gradient:  0.2597097253451176
iteration : 6663
train acc:  0.84375
train loss:  0.31963831186294556
train gradient:  0.2380069321988042
iteration : 6664
train acc:  0.84375
train loss:  0.3454728126525879
train gradient:  0.23063312975979908
iteration : 6665
train acc:  0.8359375
train loss:  0.4211437702178955
train gradient:  0.25169276017457853
iteration : 6666
train acc:  0.8515625
train loss:  0.30578601360321045
train gradient:  0.3043206131043983
iteration : 6667
train acc:  0.90625
train loss:  0.26407647132873535
train gradient:  0.1952529148016219
iteration : 6668
train acc:  0.8515625
train loss:  0.3259187936782837
train gradient:  0.18742436204949459
iteration : 6669
train acc:  0.8828125
train loss:  0.25504231452941895
train gradient:  0.10077561605971547
iteration : 6670
train acc:  0.828125
train loss:  0.3620598316192627
train gradient:  0.14786806338360647
iteration : 6671
train acc:  0.8046875
train loss:  0.37400197982788086
train gradient:  0.3284738373833394
iteration : 6672
train acc:  0.8359375
train loss:  0.3406422734260559
train gradient:  0.22975707352340702
iteration : 6673
train acc:  0.875
train loss:  0.30824488401412964
train gradient:  0.1756973672799243
iteration : 6674
train acc:  0.8515625
train loss:  0.3061722218990326
train gradient:  0.15659923278603743
iteration : 6675
train acc:  0.8203125
train loss:  0.4588203430175781
train gradient:  0.2669816295619663
iteration : 6676
train acc:  0.8203125
train loss:  0.38887089490890503
train gradient:  0.2583969007796033
iteration : 6677
train acc:  0.8515625
train loss:  0.29815393686294556
train gradient:  0.14930762251174132
iteration : 6678
train acc:  0.8671875
train loss:  0.2732324004173279
train gradient:  0.16741692115385465
iteration : 6679
train acc:  0.828125
train loss:  0.393812358379364
train gradient:  0.21793230986348314
iteration : 6680
train acc:  0.828125
train loss:  0.35733985900878906
train gradient:  0.25561674915284716
iteration : 6681
train acc:  0.859375
train loss:  0.32479164004325867
train gradient:  0.20923870821061258
iteration : 6682
train acc:  0.8671875
train loss:  0.31890565156936646
train gradient:  0.17881241920062999
iteration : 6683
train acc:  0.8359375
train loss:  0.3664032220840454
train gradient:  0.3423370619252279
iteration : 6684
train acc:  0.8515625
train loss:  0.3260299265384674
train gradient:  0.16808394293711526
iteration : 6685
train acc:  0.8671875
train loss:  0.2556552588939667
train gradient:  0.12920484052938375
iteration : 6686
train acc:  0.828125
train loss:  0.3411569893360138
train gradient:  0.2673093972182444
iteration : 6687
train acc:  0.765625
train loss:  0.4027165174484253
train gradient:  0.2613594887567276
iteration : 6688
train acc:  0.875
train loss:  0.2991359829902649
train gradient:  0.1679401790852558
iteration : 6689
train acc:  0.859375
train loss:  0.32238972187042236
train gradient:  0.19720149314266716
iteration : 6690
train acc:  0.8984375
train loss:  0.27662307024002075
train gradient:  0.21140215295323012
iteration : 6691
train acc:  0.859375
train loss:  0.35428163409233093
train gradient:  0.20085559394322497
iteration : 6692
train acc:  0.7734375
train loss:  0.35599061846733093
train gradient:  0.35073465860449793
iteration : 6693
train acc:  0.8359375
train loss:  0.41495996713638306
train gradient:  0.3195215142639178
iteration : 6694
train acc:  0.8203125
train loss:  0.3768792748451233
train gradient:  0.2441746510221283
iteration : 6695
train acc:  0.875
train loss:  0.3891998529434204
train gradient:  0.42214105975232874
iteration : 6696
train acc:  0.828125
train loss:  0.3720307946205139
train gradient:  0.21885688091328007
iteration : 6697
train acc:  0.828125
train loss:  0.33918339014053345
train gradient:  0.273341216665522
iteration : 6698
train acc:  0.84375
train loss:  0.3600732982158661
train gradient:  0.20937178706548898
iteration : 6699
train acc:  0.84375
train loss:  0.3490905165672302
train gradient:  0.34712779536119887
iteration : 6700
train acc:  0.890625
train loss:  0.268750935792923
train gradient:  0.3607386931701271
iteration : 6701
train acc:  0.8671875
train loss:  0.3044417202472687
train gradient:  0.18564056783871988
iteration : 6702
train acc:  0.796875
train loss:  0.37097111344337463
train gradient:  0.33942517073654155
iteration : 6703
train acc:  0.828125
train loss:  0.3675486147403717
train gradient:  0.19555558479269775
iteration : 6704
train acc:  0.8046875
train loss:  0.40204501152038574
train gradient:  0.258260993595823
iteration : 6705
train acc:  0.796875
train loss:  0.4143398404121399
train gradient:  0.3524058014266839
iteration : 6706
train acc:  0.875
train loss:  0.31784695386886597
train gradient:  0.2130134643219739
iteration : 6707
train acc:  0.8203125
train loss:  0.3902158737182617
train gradient:  0.27324559422187294
iteration : 6708
train acc:  0.8359375
train loss:  0.3281468152999878
train gradient:  0.22398172717664355
iteration : 6709
train acc:  0.84375
train loss:  0.36519578099250793
train gradient:  0.25765665064340704
iteration : 6710
train acc:  0.859375
train loss:  0.31146085262298584
train gradient:  0.14580281745253762
iteration : 6711
train acc:  0.8359375
train loss:  0.3442794978618622
train gradient:  0.44603582682175025
iteration : 6712
train acc:  0.84375
train loss:  0.3364313542842865
train gradient:  0.21614010860660268
iteration : 6713
train acc:  0.875
train loss:  0.30800265073776245
train gradient:  0.17320052270472713
iteration : 6714
train acc:  0.8515625
train loss:  0.2950989603996277
train gradient:  0.1381962186150652
iteration : 6715
train acc:  0.828125
train loss:  0.3837329149246216
train gradient:  0.2924140493720882
iteration : 6716
train acc:  0.90625
train loss:  0.26999014616012573
train gradient:  0.14993447022677517
iteration : 6717
train acc:  0.8125
train loss:  0.40993374586105347
train gradient:  0.24689632821023025
iteration : 6718
train acc:  0.8046875
train loss:  0.4296262860298157
train gradient:  0.44336920597830265
iteration : 6719
train acc:  0.9140625
train loss:  0.2559241056442261
train gradient:  0.2584843997727706
iteration : 6720
train acc:  0.890625
train loss:  0.29531946778297424
train gradient:  0.2093216105817871
iteration : 6721
train acc:  0.8203125
train loss:  0.3813623785972595
train gradient:  0.2412129096879441
iteration : 6722
train acc:  0.8515625
train loss:  0.3430560827255249
train gradient:  0.2924818423838396
iteration : 6723
train acc:  0.84375
train loss:  0.35428500175476074
train gradient:  0.3097602773708366
iteration : 6724
train acc:  0.8984375
train loss:  0.2956905663013458
train gradient:  0.13211927247113825
iteration : 6725
train acc:  0.8671875
train loss:  0.33924856781959534
train gradient:  0.23019880296296275
iteration : 6726
train acc:  0.8515625
train loss:  0.3193017244338989
train gradient:  0.16936348910516066
iteration : 6727
train acc:  0.8515625
train loss:  0.40404707193374634
train gradient:  0.29339930189275265
iteration : 6728
train acc:  0.859375
train loss:  0.3140985369682312
train gradient:  0.1643204402937254
iteration : 6729
train acc:  0.8046875
train loss:  0.387958288192749
train gradient:  0.27766948963101207
iteration : 6730
train acc:  0.8515625
train loss:  0.35413801670074463
train gradient:  0.24407810430761429
iteration : 6731
train acc:  0.875
train loss:  0.28791457414627075
train gradient:  0.20738171897023006
iteration : 6732
train acc:  0.890625
train loss:  0.31224560737609863
train gradient:  0.21023392322994283
iteration : 6733
train acc:  0.8359375
train loss:  0.3808467388153076
train gradient:  0.30565419313348485
iteration : 6734
train acc:  0.828125
train loss:  0.37437212467193604
train gradient:  0.25517497751961526
iteration : 6735
train acc:  0.859375
train loss:  0.38368263840675354
train gradient:  0.2468406218867542
iteration : 6736
train acc:  0.8671875
train loss:  0.3492245674133301
train gradient:  0.34788962560133874
iteration : 6737
train acc:  0.8125
train loss:  0.41353365778923035
train gradient:  0.34883176535564125
iteration : 6738
train acc:  0.90625
train loss:  0.2519741952419281
train gradient:  0.20988155870424854
iteration : 6739
train acc:  0.8359375
train loss:  0.377956748008728
train gradient:  0.24981440037343394
iteration : 6740
train acc:  0.7890625
train loss:  0.48003116250038147
train gradient:  0.34063925200429335
iteration : 6741
train acc:  0.8203125
train loss:  0.39334478974342346
train gradient:  0.24075224836072562
iteration : 6742
train acc:  0.8828125
train loss:  0.35342198610305786
train gradient:  0.1885612955217178
iteration : 6743
train acc:  0.8671875
train loss:  0.321450412273407
train gradient:  0.18995948792599127
iteration : 6744
train acc:  0.828125
train loss:  0.3310883045196533
train gradient:  0.23431317927020806
iteration : 6745
train acc:  0.890625
train loss:  0.29738181829452515
train gradient:  0.22285454661788
iteration : 6746
train acc:  0.875
train loss:  0.35157209634780884
train gradient:  0.1739761725200003
iteration : 6747
train acc:  0.8515625
train loss:  0.4065830409526825
train gradient:  0.2557565623130404
iteration : 6748
train acc:  0.859375
train loss:  0.34897381067276
train gradient:  0.23700756166506376
iteration : 6749
train acc:  0.8359375
train loss:  0.3726436197757721
train gradient:  0.29137398820889465
iteration : 6750
train acc:  0.8671875
train loss:  0.3339441120624542
train gradient:  0.17284017222593875
iteration : 6751
train acc:  0.828125
train loss:  0.34130585193634033
train gradient:  0.21387809271882563
iteration : 6752
train acc:  0.8046875
train loss:  0.4289715588092804
train gradient:  0.3031855773358613
iteration : 6753
train acc:  0.828125
train loss:  0.3394559323787689
train gradient:  0.2638196152929308
iteration : 6754
train acc:  0.8359375
train loss:  0.3809819221496582
train gradient:  0.3605224827480483
iteration : 6755
train acc:  0.859375
train loss:  0.3398982882499695
train gradient:  0.23416313860554405
iteration : 6756
train acc:  0.84375
train loss:  0.34468376636505127
train gradient:  0.1489469749164845
iteration : 6757
train acc:  0.84375
train loss:  0.3192530870437622
train gradient:  0.1729079041655281
iteration : 6758
train acc:  0.875
train loss:  0.30479565262794495
train gradient:  0.2333262230431391
iteration : 6759
train acc:  0.875
train loss:  0.30344247817993164
train gradient:  0.16713079594128355
iteration : 6760
train acc:  0.8515625
train loss:  0.36996325850486755
train gradient:  0.19914958990266043
iteration : 6761
train acc:  0.8515625
train loss:  0.3127734065055847
train gradient:  0.13699905602293816
iteration : 6762
train acc:  0.7890625
train loss:  0.45801806449890137
train gradient:  0.28749968552290733
iteration : 6763
train acc:  0.8203125
train loss:  0.3503686785697937
train gradient:  0.2577944148242851
iteration : 6764
train acc:  0.828125
train loss:  0.34240150451660156
train gradient:  0.23236274142902105
iteration : 6765
train acc:  0.8671875
train loss:  0.2924003601074219
train gradient:  0.14112334444105543
iteration : 6766
train acc:  0.8671875
train loss:  0.28032419085502625
train gradient:  0.15280263470069672
iteration : 6767
train acc:  0.8359375
train loss:  0.3645153045654297
train gradient:  0.24193544348427035
iteration : 6768
train acc:  0.84375
train loss:  0.3256978988647461
train gradient:  0.17280044080125098
iteration : 6769
train acc:  0.8046875
train loss:  0.418132483959198
train gradient:  0.3022458235676915
iteration : 6770
train acc:  0.8828125
train loss:  0.2831805348396301
train gradient:  0.15571217258374076
iteration : 6771
train acc:  0.8359375
train loss:  0.33239877223968506
train gradient:  0.16012326501456298
iteration : 6772
train acc:  0.8515625
train loss:  0.3114636540412903
train gradient:  0.2611311137062292
iteration : 6773
train acc:  0.875
train loss:  0.3082098364830017
train gradient:  0.17998474370697864
iteration : 6774
train acc:  0.8828125
train loss:  0.2957216501235962
train gradient:  0.15123757486294587
iteration : 6775
train acc:  0.8359375
train loss:  0.33435142040252686
train gradient:  0.18396375516046082
iteration : 6776
train acc:  0.875
train loss:  0.32917338609695435
train gradient:  0.2929681429620441
iteration : 6777
train acc:  0.84375
train loss:  0.3071349263191223
train gradient:  0.17587438296159866
iteration : 6778
train acc:  0.8203125
train loss:  0.3456384539604187
train gradient:  0.17206515189306798
iteration : 6779
train acc:  0.84375
train loss:  0.32269057631492615
train gradient:  0.311284286863115
iteration : 6780
train acc:  0.8671875
train loss:  0.3168267011642456
train gradient:  0.20325590728787396
iteration : 6781
train acc:  0.8828125
train loss:  0.25103628635406494
train gradient:  0.11702293775630107
iteration : 6782
train acc:  0.875
train loss:  0.2897022068500519
train gradient:  0.13661858310565625
iteration : 6783
train acc:  0.8359375
train loss:  0.414950430393219
train gradient:  0.41556172319865936
iteration : 6784
train acc:  0.84375
train loss:  0.3676961362361908
train gradient:  0.2612895417194585
iteration : 6785
train acc:  0.7890625
train loss:  0.3628612756729126
train gradient:  0.23724452959141218
iteration : 6786
train acc:  0.8359375
train loss:  0.4200131893157959
train gradient:  0.26522202482051904
iteration : 6787
train acc:  0.84375
train loss:  0.3335224688053131
train gradient:  0.2214163671134094
iteration : 6788
train acc:  0.859375
train loss:  0.3586064577102661
train gradient:  0.22488984973323506
iteration : 6789
train acc:  0.8828125
train loss:  0.2985419034957886
train gradient:  0.17520355053996256
iteration : 6790
train acc:  0.859375
train loss:  0.3165650963783264
train gradient:  0.1895234774374383
iteration : 6791
train acc:  0.8671875
train loss:  0.3182123303413391
train gradient:  0.26791803473581727
iteration : 6792
train acc:  0.7890625
train loss:  0.44883832335472107
train gradient:  0.3207029675874975
iteration : 6793
train acc:  0.890625
train loss:  0.2631797194480896
train gradient:  0.12813855574641833
iteration : 6794
train acc:  0.8671875
train loss:  0.3110952377319336
train gradient:  0.1627175827704895
iteration : 6795
train acc:  0.9140625
train loss:  0.2318812906742096
train gradient:  0.1415598150578915
iteration : 6796
train acc:  0.796875
train loss:  0.3621036410331726
train gradient:  0.24373049018633006
iteration : 6797
train acc:  0.828125
train loss:  0.3788810074329376
train gradient:  0.3302063925046646
iteration : 6798
train acc:  0.875
train loss:  0.3045060336589813
train gradient:  0.17218978054324177
iteration : 6799
train acc:  0.8359375
train loss:  0.34400075674057007
train gradient:  0.2508775825376597
iteration : 6800
train acc:  0.8046875
train loss:  0.3766838312149048
train gradient:  0.21548717633763353
iteration : 6801
train acc:  0.8515625
train loss:  0.31757116317749023
train gradient:  0.2754977664556634
iteration : 6802
train acc:  0.859375
train loss:  0.3931242525577545
train gradient:  0.2698917179611607
iteration : 6803
train acc:  0.828125
train loss:  0.3358909487724304
train gradient:  0.24703181095487484
iteration : 6804
train acc:  0.9140625
train loss:  0.28521326184272766
train gradient:  0.15414319527915643
iteration : 6805
train acc:  0.78125
train loss:  0.4523780941963196
train gradient:  0.33190692267923155
iteration : 6806
train acc:  0.8984375
train loss:  0.30767154693603516
train gradient:  0.20481652956346963
iteration : 6807
train acc:  0.859375
train loss:  0.31763583421707153
train gradient:  0.18254685904053747
iteration : 6808
train acc:  0.859375
train loss:  0.3163658380508423
train gradient:  0.16143439878963656
iteration : 6809
train acc:  0.8203125
train loss:  0.34018629789352417
train gradient:  0.2629347856255083
iteration : 6810
train acc:  0.8359375
train loss:  0.36636316776275635
train gradient:  0.2459171693940132
iteration : 6811
train acc:  0.890625
train loss:  0.28522175550460815
train gradient:  0.147425331315085
iteration : 6812
train acc:  0.875
train loss:  0.29083195328712463
train gradient:  0.14864423090350634
iteration : 6813
train acc:  0.8828125
train loss:  0.2779347896575928
train gradient:  0.1848145047796319
iteration : 6814
train acc:  0.875
train loss:  0.30189526081085205
train gradient:  0.1769929210850002
iteration : 6815
train acc:  0.8359375
train loss:  0.37429100275039673
train gradient:  0.2803017806215077
iteration : 6816
train acc:  0.8515625
train loss:  0.3243516683578491
train gradient:  0.18911300406691686
iteration : 6817
train acc:  0.8359375
train loss:  0.32624155282974243
train gradient:  0.22991864350645103
iteration : 6818
train acc:  0.875
train loss:  0.3314141631126404
train gradient:  0.19866303649315142
iteration : 6819
train acc:  0.8046875
train loss:  0.3709143400192261
train gradient:  0.23164212361968878
iteration : 6820
train acc:  0.8203125
train loss:  0.36632010340690613
train gradient:  0.211528256803378
iteration : 6821
train acc:  0.8515625
train loss:  0.3415759801864624
train gradient:  0.21722118815770325
iteration : 6822
train acc:  0.78125
train loss:  0.42136141657829285
train gradient:  0.4344816539613782
iteration : 6823
train acc:  0.8515625
train loss:  0.30035245418548584
train gradient:  0.14551771463991403
iteration : 6824
train acc:  0.859375
train loss:  0.30750471353530884
train gradient:  0.19351059212916452
iteration : 6825
train acc:  0.828125
train loss:  0.34096992015838623
train gradient:  0.18652648279815903
iteration : 6826
train acc:  0.875
train loss:  0.2950533628463745
train gradient:  0.16340671918339772
iteration : 6827
train acc:  0.7890625
train loss:  0.3323141634464264
train gradient:  0.4993902196299091
iteration : 6828
train acc:  0.890625
train loss:  0.28921014070510864
train gradient:  0.17508976421365846
iteration : 6829
train acc:  0.828125
train loss:  0.3691027760505676
train gradient:  0.22606661366240982
iteration : 6830
train acc:  0.890625
train loss:  0.30255115032196045
train gradient:  0.16410040689003946
iteration : 6831
train acc:  0.8203125
train loss:  0.4375475347042084
train gradient:  0.3382022479745305
iteration : 6832
train acc:  0.84375
train loss:  0.37474697828292847
train gradient:  0.2264442924625628
iteration : 6833
train acc:  0.859375
train loss:  0.36015784740448
train gradient:  0.2744505631174696
iteration : 6834
train acc:  0.8828125
train loss:  0.27196481823921204
train gradient:  0.16789663662778673
iteration : 6835
train acc:  0.84375
train loss:  0.3612076938152313
train gradient:  0.1984420722060391
iteration : 6836
train acc:  0.8515625
train loss:  0.31083041429519653
train gradient:  0.18243056110266664
iteration : 6837
train acc:  0.8203125
train loss:  0.34268718957901
train gradient:  0.21655451579721496
iteration : 6838
train acc:  0.796875
train loss:  0.445466548204422
train gradient:  0.4122164412781856
iteration : 6839
train acc:  0.875
train loss:  0.31473103165626526
train gradient:  0.2625466246351979
iteration : 6840
train acc:  0.84375
train loss:  0.39262157678604126
train gradient:  0.34681180538643774
iteration : 6841
train acc:  0.890625
train loss:  0.2542823851108551
train gradient:  0.19743695085666987
iteration : 6842
train acc:  0.859375
train loss:  0.35966426134109497
train gradient:  0.23955769651688655
iteration : 6843
train acc:  0.8359375
train loss:  0.3422676920890808
train gradient:  0.19989521904325355
iteration : 6844
train acc:  0.84375
train loss:  0.40754878520965576
train gradient:  0.3640559424472258
iteration : 6845
train acc:  0.8515625
train loss:  0.397273987531662
train gradient:  0.24390201073255185
iteration : 6846
train acc:  0.9296875
train loss:  0.2815905809402466
train gradient:  0.1814459277986178
iteration : 6847
train acc:  0.8515625
train loss:  0.3406296968460083
train gradient:  0.1816444762861839
iteration : 6848
train acc:  0.875
train loss:  0.3784761130809784
train gradient:  0.2606582430834849
iteration : 6849
train acc:  0.8203125
train loss:  0.38468262553215027
train gradient:  0.29118646808314347
iteration : 6850
train acc:  0.796875
train loss:  0.4451843798160553
train gradient:  0.35860990790950603
iteration : 6851
train acc:  0.875
train loss:  0.35830724239349365
train gradient:  0.2624587434117519
iteration : 6852
train acc:  0.8828125
train loss:  0.29629409313201904
train gradient:  0.18064297842485483
iteration : 6853
train acc:  0.7734375
train loss:  0.45677196979522705
train gradient:  0.3583763395686689
iteration : 6854
train acc:  0.890625
train loss:  0.2984965741634369
train gradient:  0.21163858794808343
iteration : 6855
train acc:  0.828125
train loss:  0.4237268567085266
train gradient:  0.2846873240320664
iteration : 6856
train acc:  0.921875
train loss:  0.26021042466163635
train gradient:  0.18233029511269122
iteration : 6857
train acc:  0.84375
train loss:  0.3488600254058838
train gradient:  0.22743798519618208
iteration : 6858
train acc:  0.8515625
train loss:  0.3656728267669678
train gradient:  0.30016131579238925
iteration : 6859
train acc:  0.8515625
train loss:  0.3853965997695923
train gradient:  0.2530852404540729
iteration : 6860
train acc:  0.8671875
train loss:  0.29976943135261536
train gradient:  0.22753794843159808
iteration : 6861
train acc:  0.8203125
train loss:  0.36178088188171387
train gradient:  0.24306513386405154
iteration : 6862
train acc:  0.8828125
train loss:  0.3140574097633362
train gradient:  0.15844979550194596
iteration : 6863
train acc:  0.765625
train loss:  0.4772297739982605
train gradient:  0.3608792746510636
iteration : 6864
train acc:  0.8046875
train loss:  0.3998588025569916
train gradient:  0.2956165682307037
iteration : 6865
train acc:  0.8828125
train loss:  0.2719990015029907
train gradient:  0.1590574284377923
iteration : 6866
train acc:  0.8203125
train loss:  0.4124565124511719
train gradient:  0.3465165966245462
iteration : 6867
train acc:  0.828125
train loss:  0.3469560444355011
train gradient:  0.19238009328316444
iteration : 6868
train acc:  0.8125
train loss:  0.38342511653900146
train gradient:  0.19375975568970616
iteration : 6869
train acc:  0.8359375
train loss:  0.35635900497436523
train gradient:  0.24047909786826938
iteration : 6870
train acc:  0.8828125
train loss:  0.32970136404037476
train gradient:  0.19660526938418305
iteration : 6871
train acc:  0.8125
train loss:  0.3714403510093689
train gradient:  0.224369645673581
iteration : 6872
train acc:  0.84375
train loss:  0.34625867009162903
train gradient:  0.19967561343548382
iteration : 6873
train acc:  0.8046875
train loss:  0.40280967950820923
train gradient:  0.235281586365005
iteration : 6874
train acc:  0.828125
train loss:  0.36524853110313416
train gradient:  0.2491870155458918
iteration : 6875
train acc:  0.859375
train loss:  0.4087884724140167
train gradient:  0.3030630538475908
iteration : 6876
train acc:  0.8515625
train loss:  0.3385012149810791
train gradient:  0.17506703580921876
iteration : 6877
train acc:  0.8046875
train loss:  0.40167438983917236
train gradient:  0.2974148729877936
iteration : 6878
train acc:  0.828125
train loss:  0.39148205518722534
train gradient:  0.246641399115484
iteration : 6879
train acc:  0.8515625
train loss:  0.34114474058151245
train gradient:  0.19575118357276725
iteration : 6880
train acc:  0.828125
train loss:  0.3294490575790405
train gradient:  0.2730940044414114
iteration : 6881
train acc:  0.84375
train loss:  0.362155556678772
train gradient:  0.1962167221211469
iteration : 6882
train acc:  0.8359375
train loss:  0.323158323764801
train gradient:  0.17983277979013282
iteration : 6883
train acc:  0.8125
train loss:  0.42733919620513916
train gradient:  0.27336542151495186
iteration : 6884
train acc:  0.828125
train loss:  0.4018339514732361
train gradient:  0.28702278273670695
iteration : 6885
train acc:  0.90625
train loss:  0.2677857279777527
train gradient:  0.24158865310684263
iteration : 6886
train acc:  0.7890625
train loss:  0.4699839949607849
train gradient:  0.4410434197397033
iteration : 6887
train acc:  0.8828125
train loss:  0.2827618420124054
train gradient:  0.1700584713391663
iteration : 6888
train acc:  0.796875
train loss:  0.41336768865585327
train gradient:  0.269732943751446
iteration : 6889
train acc:  0.7890625
train loss:  0.42343616485595703
train gradient:  0.25195518926190874
iteration : 6890
train acc:  0.8515625
train loss:  0.32461830973625183
train gradient:  0.14879263749984403
iteration : 6891
train acc:  0.828125
train loss:  0.4181327223777771
train gradient:  0.27974630319023486
iteration : 6892
train acc:  0.8828125
train loss:  0.3005605936050415
train gradient:  0.11857693386174195
iteration : 6893
train acc:  0.8984375
train loss:  0.26781517267227173
train gradient:  0.14115897284642348
iteration : 6894
train acc:  0.84375
train loss:  0.31403905153274536
train gradient:  0.24070704331516446
iteration : 6895
train acc:  0.9296875
train loss:  0.2942325472831726
train gradient:  0.27341442835976987
iteration : 6896
train acc:  0.8671875
train loss:  0.32869938015937805
train gradient:  0.19919555855976892
iteration : 6897
train acc:  0.859375
train loss:  0.2950265407562256
train gradient:  0.16975307475266196
iteration : 6898
train acc:  0.8203125
train loss:  0.33969002962112427
train gradient:  0.19933590965681053
iteration : 6899
train acc:  0.890625
train loss:  0.266426146030426
train gradient:  0.1689891495550896
iteration : 6900
train acc:  0.8671875
train loss:  0.301220178604126
train gradient:  0.19358799154375247
iteration : 6901
train acc:  0.8828125
train loss:  0.3157793879508972
train gradient:  0.13423433747868369
iteration : 6902
train acc:  0.8671875
train loss:  0.3104984164237976
train gradient:  0.21071893416460175
iteration : 6903
train acc:  0.78125
train loss:  0.4604724049568176
train gradient:  0.4164340257517518
iteration : 6904
train acc:  0.859375
train loss:  0.3051769435405731
train gradient:  0.2095151459795243
iteration : 6905
train acc:  0.7890625
train loss:  0.4437617361545563
train gradient:  0.2622355448535594
iteration : 6906
train acc:  0.875
train loss:  0.2939993739128113
train gradient:  0.14532159345686413
iteration : 6907
train acc:  0.8828125
train loss:  0.3441178500652313
train gradient:  0.19401658847587192
iteration : 6908
train acc:  0.921875
train loss:  0.24002812802791595
train gradient:  0.14741866295527034
iteration : 6909
train acc:  0.8359375
train loss:  0.3525390625
train gradient:  0.19820052120249526
iteration : 6910
train acc:  0.875
train loss:  0.28735291957855225
train gradient:  0.1258663695056993
iteration : 6911
train acc:  0.8671875
train loss:  0.3280757665634155
train gradient:  0.19236528321243232
iteration : 6912
train acc:  0.859375
train loss:  0.35994574427604675
train gradient:  0.3098228494235738
iteration : 6913
train acc:  0.890625
train loss:  0.2768977880477905
train gradient:  0.1372496970695575
iteration : 6914
train acc:  0.8515625
train loss:  0.37323129177093506
train gradient:  0.17456452245718762
iteration : 6915
train acc:  0.8359375
train loss:  0.38719964027404785
train gradient:  0.33921581698394104
iteration : 6916
train acc:  0.8671875
train loss:  0.2913958430290222
train gradient:  0.21047597595369705
iteration : 6917
train acc:  0.875
train loss:  0.2666124701499939
train gradient:  0.16924908959470453
iteration : 6918
train acc:  0.7890625
train loss:  0.48803430795669556
train gradient:  0.30303183885449997
iteration : 6919
train acc:  0.875
train loss:  0.28766992688179016
train gradient:  0.13990214179348776
iteration : 6920
train acc:  0.8359375
train loss:  0.42174232006073
train gradient:  0.3008692849198677
iteration : 6921
train acc:  0.8359375
train loss:  0.38497424125671387
train gradient:  0.24793391938197995
iteration : 6922
train acc:  0.859375
train loss:  0.3970434367656708
train gradient:  0.2701775489481883
iteration : 6923
train acc:  0.828125
train loss:  0.3377843499183655
train gradient:  0.17964801310210726
iteration : 6924
train acc:  0.890625
train loss:  0.34955525398254395
train gradient:  0.22993328678966643
iteration : 6925
train acc:  0.8515625
train loss:  0.3407939076423645
train gradient:  0.48118801132611383
iteration : 6926
train acc:  0.8359375
train loss:  0.4584900140762329
train gradient:  0.3303670792939292
iteration : 6927
train acc:  0.8359375
train loss:  0.36530983448028564
train gradient:  0.23753259760988904
iteration : 6928
train acc:  0.8515625
train loss:  0.4048409163951874
train gradient:  0.17684865069746114
iteration : 6929
train acc:  0.859375
train loss:  0.35578882694244385
train gradient:  0.1683195407647719
iteration : 6930
train acc:  0.8125
train loss:  0.4427078068256378
train gradient:  0.24897255647364344
iteration : 6931
train acc:  0.8515625
train loss:  0.31496766209602356
train gradient:  0.1995661391113308
iteration : 6932
train acc:  0.8984375
train loss:  0.2770388126373291
train gradient:  0.1425133587584333
iteration : 6933
train acc:  0.828125
train loss:  0.4204111397266388
train gradient:  0.25604930180208424
iteration : 6934
train acc:  0.8515625
train loss:  0.3865545094013214
train gradient:  0.21254827153419492
iteration : 6935
train acc:  0.84375
train loss:  0.34657734632492065
train gradient:  0.1822161294190256
iteration : 6936
train acc:  0.828125
train loss:  0.33291518688201904
train gradient:  0.15494929148341646
iteration : 6937
train acc:  0.859375
train loss:  0.39566394686698914
train gradient:  0.23772287951142235
iteration : 6938
train acc:  0.8828125
train loss:  0.31126868724823
train gradient:  0.10965875424076789
iteration : 6939
train acc:  0.859375
train loss:  0.3204769790172577
train gradient:  0.14328115999779487
iteration : 6940
train acc:  0.9140625
train loss:  0.2222541868686676
train gradient:  0.12827119995109368
iteration : 6941
train acc:  0.8515625
train loss:  0.31142857670783997
train gradient:  0.1443486747521912
iteration : 6942
train acc:  0.84375
train loss:  0.35384154319763184
train gradient:  0.17003688283932739
iteration : 6943
train acc:  0.8515625
train loss:  0.3569033741950989
train gradient:  0.27586226383930806
iteration : 6944
train acc:  0.8828125
train loss:  0.338748574256897
train gradient:  0.30078738354912876
iteration : 6945
train acc:  0.8203125
train loss:  0.3209688365459442
train gradient:  0.1926206805437063
iteration : 6946
train acc:  0.828125
train loss:  0.34436845779418945
train gradient:  0.1723551938814854
iteration : 6947
train acc:  0.90625
train loss:  0.26529020071029663
train gradient:  0.1596844753476882
iteration : 6948
train acc:  0.8515625
train loss:  0.3801151514053345
train gradient:  0.2660817232007403
iteration : 6949
train acc:  0.7734375
train loss:  0.4570601284503937
train gradient:  0.4731238318490889
iteration : 6950
train acc:  0.8515625
train loss:  0.28492605686187744
train gradient:  0.29485693532155843
iteration : 6951
train acc:  0.828125
train loss:  0.3867781162261963
train gradient:  0.2684301240197533
iteration : 6952
train acc:  0.8203125
train loss:  0.4143410325050354
train gradient:  0.26025810289788853
iteration : 6953
train acc:  0.796875
train loss:  0.4676307439804077
train gradient:  0.42989133981654554
iteration : 6954
train acc:  0.859375
train loss:  0.29947030544281006
train gradient:  0.12814318458676005
iteration : 6955
train acc:  0.890625
train loss:  0.2646215558052063
train gradient:  0.13989415868177493
iteration : 6956
train acc:  0.8515625
train loss:  0.4183765649795532
train gradient:  0.22446388856137064
iteration : 6957
train acc:  0.796875
train loss:  0.38284939527511597
train gradient:  0.22472299680939278
iteration : 6958
train acc:  0.859375
train loss:  0.29664164781570435
train gradient:  0.17717655815156724
iteration : 6959
train acc:  0.859375
train loss:  0.2969140112400055
train gradient:  0.14650748807258152
iteration : 6960
train acc:  0.8671875
train loss:  0.29873061180114746
train gradient:  0.16482394352178364
iteration : 6961
train acc:  0.8671875
train loss:  0.3811708688735962
train gradient:  0.2442060194907134
iteration : 6962
train acc:  0.875
train loss:  0.3367025554180145
train gradient:  0.19182742498928773
iteration : 6963
train acc:  0.8515625
train loss:  0.33544352650642395
train gradient:  0.20301471767643503
iteration : 6964
train acc:  0.875
train loss:  0.35144680738449097
train gradient:  0.16984013121537486
iteration : 6965
train acc:  0.875
train loss:  0.33746811747550964
train gradient:  0.17690382615274314
iteration : 6966
train acc:  0.859375
train loss:  0.34659966826438904
train gradient:  0.1808872421383948
iteration : 6967
train acc:  0.875
train loss:  0.2790835499763489
train gradient:  0.1754846732673664
iteration : 6968
train acc:  0.8828125
train loss:  0.28693169355392456
train gradient:  0.18321230584306475
iteration : 6969
train acc:  0.8828125
train loss:  0.3577677309513092
train gradient:  0.13365091482704006
iteration : 6970
train acc:  0.8046875
train loss:  0.4185051918029785
train gradient:  0.2713396729005341
iteration : 6971
train acc:  0.8828125
train loss:  0.2612169086933136
train gradient:  0.15782189344751005
iteration : 6972
train acc:  0.84375
train loss:  0.32200127840042114
train gradient:  0.16420839953016325
iteration : 6973
train acc:  0.8125
train loss:  0.4043563902378082
train gradient:  0.20007199749732263
iteration : 6974
train acc:  0.8671875
train loss:  0.3467486500740051
train gradient:  0.2439410874911146
iteration : 6975
train acc:  0.8046875
train loss:  0.41241541504859924
train gradient:  0.3443558349579521
iteration : 6976
train acc:  0.8203125
train loss:  0.3665928542613983
train gradient:  0.21905451975021328
iteration : 6977
train acc:  0.90625
train loss:  0.25429782271385193
train gradient:  0.13380786042011475
iteration : 6978
train acc:  0.7734375
train loss:  0.4387451708316803
train gradient:  0.25048577015682605
iteration : 6979
train acc:  0.84375
train loss:  0.32680147886276245
train gradient:  0.15414248702571595
iteration : 6980
train acc:  0.8828125
train loss:  0.29242265224456787
train gradient:  0.13864067139309233
iteration : 6981
train acc:  0.8203125
train loss:  0.39817070960998535
train gradient:  0.26619941913789014
iteration : 6982
train acc:  0.828125
train loss:  0.4042011797428131
train gradient:  0.22864447940630558
iteration : 6983
train acc:  0.8203125
train loss:  0.36191582679748535
train gradient:  0.22565431916076215
iteration : 6984
train acc:  0.8125
train loss:  0.4073655605316162
train gradient:  0.3066658111589399
iteration : 6985
train acc:  0.875
train loss:  0.298450767993927
train gradient:  0.1554268536379978
iteration : 6986
train acc:  0.8515625
train loss:  0.34199589490890503
train gradient:  0.16937976617617645
iteration : 6987
train acc:  0.9140625
train loss:  0.23745855689048767
train gradient:  0.12181720777801877
iteration : 6988
train acc:  0.8203125
train loss:  0.3341405391693115
train gradient:  0.18643336925601287
iteration : 6989
train acc:  0.8046875
train loss:  0.4181857705116272
train gradient:  0.2177910029091243
iteration : 6990
train acc:  0.859375
train loss:  0.34766852855682373
train gradient:  0.1939369069286666
iteration : 6991
train acc:  0.9140625
train loss:  0.26795849204063416
train gradient:  0.11292905608903155
iteration : 6992
train acc:  0.84375
train loss:  0.3234994411468506
train gradient:  0.1533305236783668
iteration : 6993
train acc:  0.8359375
train loss:  0.3480455279350281
train gradient:  0.22671077645992374
iteration : 6994
train acc:  0.8515625
train loss:  0.3130108118057251
train gradient:  0.1557962028788789
iteration : 6995
train acc:  0.8515625
train loss:  0.3036707639694214
train gradient:  0.17645969791538713
iteration : 6996
train acc:  0.8515625
train loss:  0.3776688575744629
train gradient:  0.17646220977602695
iteration : 6997
train acc:  0.8125
train loss:  0.412090003490448
train gradient:  0.21054119608237792
iteration : 6998
train acc:  0.8203125
train loss:  0.3787161111831665
train gradient:  0.2872576517911711
iteration : 6999
train acc:  0.8203125
train loss:  0.34144890308380127
train gradient:  0.17009541836436431
iteration : 7000
train acc:  0.875
train loss:  0.2940378189086914
train gradient:  0.17007165623461706
iteration : 7001
train acc:  0.890625
train loss:  0.3260064125061035
train gradient:  0.16277546538888527
iteration : 7002
train acc:  0.8515625
train loss:  0.34895896911621094
train gradient:  0.2957143428923336
iteration : 7003
train acc:  0.8515625
train loss:  0.28989291191101074
train gradient:  0.1576400465982477
iteration : 7004
train acc:  0.8984375
train loss:  0.2565361261367798
train gradient:  0.14231802613445946
iteration : 7005
train acc:  0.8984375
train loss:  0.2655380964279175
train gradient:  0.18210446322681012
iteration : 7006
train acc:  0.8359375
train loss:  0.3813633322715759
train gradient:  0.2485528228315255
iteration : 7007
train acc:  0.84375
train loss:  0.35132500529289246
train gradient:  0.22256082865182014
iteration : 7008
train acc:  0.859375
train loss:  0.31419575214385986
train gradient:  0.18402908324926284
iteration : 7009
train acc:  0.765625
train loss:  0.5051624774932861
train gradient:  0.32915337804228406
iteration : 7010
train acc:  0.890625
train loss:  0.28420594334602356
train gradient:  0.15109923305008988
iteration : 7011
train acc:  0.8828125
train loss:  0.29303181171417236
train gradient:  0.12688157089930824
iteration : 7012
train acc:  0.875
train loss:  0.2754288613796234
train gradient:  0.13119232262537595
iteration : 7013
train acc:  0.84375
train loss:  0.3589996099472046
train gradient:  0.24704747794560067
iteration : 7014
train acc:  0.828125
train loss:  0.3585023283958435
train gradient:  0.23929108359797244
iteration : 7015
train acc:  0.9140625
train loss:  0.21801696717739105
train gradient:  0.1196667043024646
iteration : 7016
train acc:  0.875
train loss:  0.3091076910495758
train gradient:  0.22748089164922738
iteration : 7017
train acc:  0.7890625
train loss:  0.4422377943992615
train gradient:  0.5005912854186054
iteration : 7018
train acc:  0.8828125
train loss:  0.27224257588386536
train gradient:  0.12162572353243337
iteration : 7019
train acc:  0.859375
train loss:  0.35620003938674927
train gradient:  0.1442774187599558
iteration : 7020
train acc:  0.859375
train loss:  0.3202759027481079
train gradient:  0.209718252118765
iteration : 7021
train acc:  0.8515625
train loss:  0.2676464915275574
train gradient:  0.16581988194656613
iteration : 7022
train acc:  0.828125
train loss:  0.3974164128303528
train gradient:  0.3117212413699437
iteration : 7023
train acc:  0.9140625
train loss:  0.25860071182250977
train gradient:  0.12590942577957961
iteration : 7024
train acc:  0.84375
train loss:  0.333518385887146
train gradient:  0.2187878239906084
iteration : 7025
train acc:  0.8359375
train loss:  0.3494868874549866
train gradient:  0.265444947851305
iteration : 7026
train acc:  0.875
train loss:  0.289372980594635
train gradient:  0.1425077663668351
iteration : 7027
train acc:  0.875
train loss:  0.2657744288444519
train gradient:  0.19196341367923447
iteration : 7028
train acc:  0.8203125
train loss:  0.3859681487083435
train gradient:  0.2912266101675365
iteration : 7029
train acc:  0.828125
train loss:  0.35859107971191406
train gradient:  0.18102577914928114
iteration : 7030
train acc:  0.8671875
train loss:  0.29927974939346313
train gradient:  0.20684452832807548
iteration : 7031
train acc:  0.828125
train loss:  0.3073650002479553
train gradient:  0.3524793760900984
iteration : 7032
train acc:  0.8515625
train loss:  0.34416285157203674
train gradient:  0.2494899253306887
iteration : 7033
train acc:  0.8828125
train loss:  0.2810051441192627
train gradient:  0.2869651789169861
iteration : 7034
train acc:  0.890625
train loss:  0.3042331337928772
train gradient:  0.30263108413289896
iteration : 7035
train acc:  0.859375
train loss:  0.3951977789402008
train gradient:  0.37997830084090056
iteration : 7036
train acc:  0.859375
train loss:  0.3144744634628296
train gradient:  0.322975570486261
iteration : 7037
train acc:  0.8515625
train loss:  0.33845365047454834
train gradient:  0.17587460169018643
iteration : 7038
train acc:  0.8671875
train loss:  0.29482942819595337
train gradient:  0.18096775037872298
iteration : 7039
train acc:  0.859375
train loss:  0.3444618284702301
train gradient:  0.23553207154401729
iteration : 7040
train acc:  0.8671875
train loss:  0.3108839988708496
train gradient:  0.17059042423531054
iteration : 7041
train acc:  0.8828125
train loss:  0.29399633407592773
train gradient:  0.18423822420211755
iteration : 7042
train acc:  0.8359375
train loss:  0.39047500491142273
train gradient:  0.2010511770560926
iteration : 7043
train acc:  0.90625
train loss:  0.26034536957740784
train gradient:  0.17366692917240406
iteration : 7044
train acc:  0.859375
train loss:  0.30923521518707275
train gradient:  0.21630464072296826
iteration : 7045
train acc:  0.8359375
train loss:  0.34174278378486633
train gradient:  0.23259015830424232
iteration : 7046
train acc:  0.9140625
train loss:  0.2675705552101135
train gradient:  0.1238494521626081
iteration : 7047
train acc:  0.84375
train loss:  0.36908772587776184
train gradient:  0.3334609710174418
iteration : 7048
train acc:  0.84375
train loss:  0.3459087014198303
train gradient:  0.19375727349405775
iteration : 7049
train acc:  0.8515625
train loss:  0.35276055335998535
train gradient:  0.2670210277927559
iteration : 7050
train acc:  0.859375
train loss:  0.369073748588562
train gradient:  0.27648493763524545
iteration : 7051
train acc:  0.8671875
train loss:  0.2848265469074249
train gradient:  0.19396131285979162
iteration : 7052
train acc:  0.8671875
train loss:  0.33100542426109314
train gradient:  0.21248677855870574
iteration : 7053
train acc:  0.8359375
train loss:  0.3878207206726074
train gradient:  0.23731461347033878
iteration : 7054
train acc:  0.8359375
train loss:  0.3785042464733124
train gradient:  0.26400056392529525
iteration : 7055
train acc:  0.8515625
train loss:  0.3230785131454468
train gradient:  0.22499407028045487
iteration : 7056
train acc:  0.84375
train loss:  0.3245601952075958
train gradient:  0.1482495998744289
iteration : 7057
train acc:  0.8828125
train loss:  0.29793912172317505
train gradient:  0.14462903387382103
iteration : 7058
train acc:  0.8046875
train loss:  0.38509461283683777
train gradient:  0.27838946948579935
iteration : 7059
train acc:  0.765625
train loss:  0.4573083519935608
train gradient:  0.35658634517379484
iteration : 7060
train acc:  0.8515625
train loss:  0.35669663548469543
train gradient:  0.2902241540747602
iteration : 7061
train acc:  0.8671875
train loss:  0.31648027896881104
train gradient:  0.17107945012064435
iteration : 7062
train acc:  0.8515625
train loss:  0.3328930139541626
train gradient:  0.24676201945611087
iteration : 7063
train acc:  0.8828125
train loss:  0.31768640875816345
train gradient:  0.25366700127003133
iteration : 7064
train acc:  0.9140625
train loss:  0.25294986367225647
train gradient:  0.14296329576934863
iteration : 7065
train acc:  0.84375
train loss:  0.34410494565963745
train gradient:  0.2048283898341624
iteration : 7066
train acc:  0.890625
train loss:  0.26466891169548035
train gradient:  0.12695392570663536
iteration : 7067
train acc:  0.890625
train loss:  0.2670819163322449
train gradient:  0.2007864989087741
iteration : 7068
train acc:  0.859375
train loss:  0.3199279308319092
train gradient:  0.16951319226680933
iteration : 7069
train acc:  0.875
train loss:  0.34163326025009155
train gradient:  0.17949916934390667
iteration : 7070
train acc:  0.828125
train loss:  0.3832288980484009
train gradient:  0.19685977674967267
iteration : 7071
train acc:  0.8828125
train loss:  0.31000587344169617
train gradient:  0.20481463796091115
iteration : 7072
train acc:  0.8125
train loss:  0.34548890590667725
train gradient:  0.1561233603827521
iteration : 7073
train acc:  0.8203125
train loss:  0.35777547955513
train gradient:  0.2436552353414741
iteration : 7074
train acc:  0.8046875
train loss:  0.40544044971466064
train gradient:  0.2830118887352983
iteration : 7075
train acc:  0.875
train loss:  0.2705601453781128
train gradient:  0.19240844012537045
iteration : 7076
train acc:  0.8359375
train loss:  0.3527502417564392
train gradient:  0.2983737245786873
iteration : 7077
train acc:  0.890625
train loss:  0.3310806155204773
train gradient:  0.16282816716834417
iteration : 7078
train acc:  0.890625
train loss:  0.2721611261367798
train gradient:  0.15181449806070993
iteration : 7079
train acc:  0.8828125
train loss:  0.3151625394821167
train gradient:  0.24398854792961483
iteration : 7080
train acc:  0.84375
train loss:  0.41609832644462585
train gradient:  0.29071452997273417
iteration : 7081
train acc:  0.8828125
train loss:  0.29439014196395874
train gradient:  0.17387429354586018
iteration : 7082
train acc:  0.8515625
train loss:  0.2904086709022522
train gradient:  0.17096022741104788
iteration : 7083
train acc:  0.890625
train loss:  0.282002329826355
train gradient:  0.18049392434800515
iteration : 7084
train acc:  0.796875
train loss:  0.4530889093875885
train gradient:  0.30835320475883354
iteration : 7085
train acc:  0.875
train loss:  0.28423643112182617
train gradient:  0.1494659294742947
iteration : 7086
train acc:  0.8515625
train loss:  0.31352734565734863
train gradient:  0.1930425679097309
iteration : 7087
train acc:  0.765625
train loss:  0.506763219833374
train gradient:  0.4083640422508401
iteration : 7088
train acc:  0.875
train loss:  0.31379133462905884
train gradient:  0.16268615608760092
iteration : 7089
train acc:  0.890625
train loss:  0.2637287378311157
train gradient:  0.18567284022757524
iteration : 7090
train acc:  0.8515625
train loss:  0.3423815667629242
train gradient:  0.24359451765110832
iteration : 7091
train acc:  0.8984375
train loss:  0.2659171223640442
train gradient:  0.17296164085444532
iteration : 7092
train acc:  0.828125
train loss:  0.4263460040092468
train gradient:  0.40701557089410206
iteration : 7093
train acc:  0.8359375
train loss:  0.35425668954849243
train gradient:  0.220708073512812
iteration : 7094
train acc:  0.84375
train loss:  0.32180386781692505
train gradient:  0.19210004168916034
iteration : 7095
train acc:  0.8515625
train loss:  0.33016031980514526
train gradient:  0.1623911394966428
iteration : 7096
train acc:  0.8671875
train loss:  0.3370339274406433
train gradient:  0.200996342091761
iteration : 7097
train acc:  0.84375
train loss:  0.3785794973373413
train gradient:  0.2678802259405065
iteration : 7098
train acc:  0.828125
train loss:  0.3480667769908905
train gradient:  0.153892231817021
iteration : 7099
train acc:  0.8671875
train loss:  0.283774733543396
train gradient:  0.150834130607727
iteration : 7100
train acc:  0.8046875
train loss:  0.35460221767425537
train gradient:  0.23552575471099751
iteration : 7101
train acc:  0.8359375
train loss:  0.35475772619247437
train gradient:  0.21853517144248408
iteration : 7102
train acc:  0.8828125
train loss:  0.2995992600917816
train gradient:  0.14350493996359592
iteration : 7103
train acc:  0.8203125
train loss:  0.3627171516418457
train gradient:  0.20739673176418627
iteration : 7104
train acc:  0.8359375
train loss:  0.3406051993370056
train gradient:  0.2037507037780137
iteration : 7105
train acc:  0.828125
train loss:  0.37170499563217163
train gradient:  0.26441429563509317
iteration : 7106
train acc:  0.796875
train loss:  0.38178133964538574
train gradient:  0.2243912096150953
iteration : 7107
train acc:  0.828125
train loss:  0.3371911942958832
train gradient:  0.3120998254981425
iteration : 7108
train acc:  0.828125
train loss:  0.36894670128822327
train gradient:  0.32132476072449057
iteration : 7109
train acc:  0.875
train loss:  0.30319756269454956
train gradient:  0.15195718365307664
iteration : 7110
train acc:  0.8828125
train loss:  0.27411752939224243
train gradient:  0.14390231603101727
iteration : 7111
train acc:  0.8125
train loss:  0.3805404305458069
train gradient:  0.21956142925199112
iteration : 7112
train acc:  0.859375
train loss:  0.4104415774345398
train gradient:  0.32901248449656
iteration : 7113
train acc:  0.8671875
train loss:  0.3503098785877228
train gradient:  0.18630566692475073
iteration : 7114
train acc:  0.875
train loss:  0.28421294689178467
train gradient:  0.15304148033025733
iteration : 7115
train acc:  0.875
train loss:  0.2807413935661316
train gradient:  0.15158551173645946
iteration : 7116
train acc:  0.8984375
train loss:  0.27113160490989685
train gradient:  0.15921434943863472
iteration : 7117
train acc:  0.84375
train loss:  0.37594038248062134
train gradient:  0.23549070686642257
iteration : 7118
train acc:  0.8828125
train loss:  0.31157493591308594
train gradient:  0.1996682724163054
iteration : 7119
train acc:  0.8671875
train loss:  0.27387094497680664
train gradient:  0.19849523736475688
iteration : 7120
train acc:  0.8203125
train loss:  0.427423894405365
train gradient:  0.33932491237668044
iteration : 7121
train acc:  0.8828125
train loss:  0.2987907826900482
train gradient:  0.16001414479171566
iteration : 7122
train acc:  0.8515625
train loss:  0.3194042146205902
train gradient:  0.1495293218769122
iteration : 7123
train acc:  0.8046875
train loss:  0.3863089084625244
train gradient:  0.20862486371804173
iteration : 7124
train acc:  0.8515625
train loss:  0.354627788066864
train gradient:  0.1916781478087097
iteration : 7125
train acc:  0.8046875
train loss:  0.37994614243507385
train gradient:  0.29214718777201737
iteration : 7126
train acc:  0.859375
train loss:  0.2986026108264923
train gradient:  0.3152213031049279
iteration : 7127
train acc:  0.8515625
train loss:  0.3246774673461914
train gradient:  0.2577830929573639
iteration : 7128
train acc:  0.8125
train loss:  0.3799033463001251
train gradient:  0.1986426565461858
iteration : 7129
train acc:  0.90625
train loss:  0.2590944170951843
train gradient:  0.10449597074250545
iteration : 7130
train acc:  0.8984375
train loss:  0.33339324593544006
train gradient:  0.20054373354810523
iteration : 7131
train acc:  0.8203125
train loss:  0.3970046639442444
train gradient:  0.2198786549573032
iteration : 7132
train acc:  0.875
train loss:  0.3130912184715271
train gradient:  0.23251457452878235
iteration : 7133
train acc:  0.8359375
train loss:  0.3834828734397888
train gradient:  0.25546737432227884
iteration : 7134
train acc:  0.84375
train loss:  0.3305753469467163
train gradient:  0.16912779215690993
iteration : 7135
train acc:  0.8515625
train loss:  0.2851749360561371
train gradient:  0.12515653653153297
iteration : 7136
train acc:  0.8671875
train loss:  0.32028937339782715
train gradient:  0.17342801957831397
iteration : 7137
train acc:  0.8515625
train loss:  0.36113274097442627
train gradient:  0.25386706426002903
iteration : 7138
train acc:  0.90625
train loss:  0.270352840423584
train gradient:  0.10413945572032526
iteration : 7139
train acc:  0.875
train loss:  0.28495973348617554
train gradient:  0.27436937759294844
iteration : 7140
train acc:  0.8125
train loss:  0.4624423682689667
train gradient:  0.38304520743297515
iteration : 7141
train acc:  0.84375
train loss:  0.32173338532447815
train gradient:  0.21155737032202004
iteration : 7142
train acc:  0.9140625
train loss:  0.23501598834991455
train gradient:  0.20488734995955638
iteration : 7143
train acc:  0.8671875
train loss:  0.34777218103408813
train gradient:  0.18950867993617398
iteration : 7144
train acc:  0.8984375
train loss:  0.287069708108902
train gradient:  0.15151996275242274
iteration : 7145
train acc:  0.875
train loss:  0.32739728689193726
train gradient:  0.17736428789016842
iteration : 7146
train acc:  0.8828125
train loss:  0.30636629462242126
train gradient:  0.2070294502946955
iteration : 7147
train acc:  0.9140625
train loss:  0.23825609683990479
train gradient:  0.15408058423346147
iteration : 7148
train acc:  0.859375
train loss:  0.34450143575668335
train gradient:  0.2724191840753227
iteration : 7149
train acc:  0.8125
train loss:  0.4002087712287903
train gradient:  0.29293851128474657
iteration : 7150
train acc:  0.84375
train loss:  0.3447320759296417
train gradient:  0.23138378776204027
iteration : 7151
train acc:  0.890625
train loss:  0.29123324155807495
train gradient:  0.14834082889821512
iteration : 7152
train acc:  0.8515625
train loss:  0.31632566452026367
train gradient:  0.22089488591571368
iteration : 7153
train acc:  0.859375
train loss:  0.3636147975921631
train gradient:  0.22120870374849996
iteration : 7154
train acc:  0.859375
train loss:  0.2810027003288269
train gradient:  0.14081431558405716
iteration : 7155
train acc:  0.8515625
train loss:  0.2968701720237732
train gradient:  0.16938225876992688
iteration : 7156
train acc:  0.8671875
train loss:  0.3952946960926056
train gradient:  0.2117334246716053
iteration : 7157
train acc:  0.8359375
train loss:  0.35361334681510925
train gradient:  0.2042360528545173
iteration : 7158
train acc:  0.8515625
train loss:  0.28764694929122925
train gradient:  0.2297308806618013
iteration : 7159
train acc:  0.8203125
train loss:  0.38630175590515137
train gradient:  0.263615095266374
iteration : 7160
train acc:  0.8359375
train loss:  0.321554571390152
train gradient:  0.24294528212254002
iteration : 7161
train acc:  0.8359375
train loss:  0.3474195897579193
train gradient:  0.2894084788341242
iteration : 7162
train acc:  0.875
train loss:  0.3218517601490021
train gradient:  0.1535133721330362
iteration : 7163
train acc:  0.8359375
train loss:  0.3543638586997986
train gradient:  0.22557981027906762
iteration : 7164
train acc:  0.84375
train loss:  0.2875145673751831
train gradient:  0.19156445750398965
iteration : 7165
train acc:  0.84375
train loss:  0.29689544439315796
train gradient:  0.12796831451278295
iteration : 7166
train acc:  0.8125
train loss:  0.37495625019073486
train gradient:  0.27869875265316574
iteration : 7167
train acc:  0.84375
train loss:  0.26345688104629517
train gradient:  0.11158624238552584
iteration : 7168
train acc:  0.828125
train loss:  0.39781081676483154
train gradient:  0.3082556681413208
iteration : 7169
train acc:  0.8828125
train loss:  0.272006630897522
train gradient:  0.16028137380838225
iteration : 7170
train acc:  0.8359375
train loss:  0.3332439661026001
train gradient:  0.22851054648294145
iteration : 7171
train acc:  0.7734375
train loss:  0.46549612283706665
train gradient:  0.32338451866318435
iteration : 7172
train acc:  0.8125
train loss:  0.40654170513153076
train gradient:  0.2749650253091126
iteration : 7173
train acc:  0.8671875
train loss:  0.35428422689437866
train gradient:  0.23992497062703277
iteration : 7174
train acc:  0.875
train loss:  0.35400819778442383
train gradient:  0.2168317238576407
iteration : 7175
train acc:  0.84375
train loss:  0.3007250428199768
train gradient:  0.18162997220148513
iteration : 7176
train acc:  0.8671875
train loss:  0.33856427669525146
train gradient:  0.2688474932728839
iteration : 7177
train acc:  0.859375
train loss:  0.31595322489738464
train gradient:  0.21299532428217288
iteration : 7178
train acc:  0.84375
train loss:  0.3278846740722656
train gradient:  0.20705521488826154
iteration : 7179
train acc:  0.8203125
train loss:  0.45998507738113403
train gradient:  0.28958186286498727
iteration : 7180
train acc:  0.8515625
train loss:  0.2890201508998871
train gradient:  0.15484950514611395
iteration : 7181
train acc:  0.8828125
train loss:  0.2828914523124695
train gradient:  0.225483978212968
iteration : 7182
train acc:  0.8671875
train loss:  0.296556293964386
train gradient:  0.11963726278541752
iteration : 7183
train acc:  0.8984375
train loss:  0.28471624851226807
train gradient:  0.16743762395256695
iteration : 7184
train acc:  0.859375
train loss:  0.3380737900733948
train gradient:  0.22692262673740154
iteration : 7185
train acc:  0.90625
train loss:  0.2565377354621887
train gradient:  0.09923017958891756
iteration : 7186
train acc:  0.8203125
train loss:  0.36195439100265503
train gradient:  0.21035722488792524
iteration : 7187
train acc:  0.84375
train loss:  0.36843010783195496
train gradient:  0.2743825797240742
iteration : 7188
train acc:  0.890625
train loss:  0.3056231141090393
train gradient:  0.17285491353656185
iteration : 7189
train acc:  0.765625
train loss:  0.41211947798728943
train gradient:  0.28722837234205467
iteration : 7190
train acc:  0.859375
train loss:  0.3367929756641388
train gradient:  0.24599563273047387
iteration : 7191
train acc:  0.859375
train loss:  0.2927233874797821
train gradient:  0.21005994585782867
iteration : 7192
train acc:  0.859375
train loss:  0.296880841255188
train gradient:  0.2563904878842052
iteration : 7193
train acc:  0.859375
train loss:  0.3283016085624695
train gradient:  0.3364871142284503
iteration : 7194
train acc:  0.8359375
train loss:  0.31193792819976807
train gradient:  0.24991525995458305
iteration : 7195
train acc:  0.828125
train loss:  0.3681926131248474
train gradient:  0.2639165428875273
iteration : 7196
train acc:  0.8828125
train loss:  0.2828332185745239
train gradient:  0.18204642026146658
iteration : 7197
train acc:  0.8984375
train loss:  0.30219483375549316
train gradient:  0.11465538371927592
iteration : 7198
train acc:  0.875
train loss:  0.34527161717414856
train gradient:  0.1886412438765627
iteration : 7199
train acc:  0.796875
train loss:  0.34213167428970337
train gradient:  0.20661766843879126
iteration : 7200
train acc:  0.890625
train loss:  0.35023748874664307
train gradient:  0.21757547560502205
iteration : 7201
train acc:  0.8203125
train loss:  0.4224913716316223
train gradient:  0.3144935298172927
iteration : 7202
train acc:  0.8828125
train loss:  0.3104318380355835
train gradient:  0.24797838607429823
iteration : 7203
train acc:  0.8203125
train loss:  0.38943618535995483
train gradient:  0.25364480215111984
iteration : 7204
train acc:  0.8671875
train loss:  0.31757280230522156
train gradient:  0.19472150354029363
iteration : 7205
train acc:  0.84375
train loss:  0.31773340702056885
train gradient:  0.165981837713004
iteration : 7206
train acc:  0.828125
train loss:  0.4270319938659668
train gradient:  0.2405581373201569
iteration : 7207
train acc:  0.8671875
train loss:  0.31418392062187195
train gradient:  0.2577895410891911
iteration : 7208
train acc:  0.8984375
train loss:  0.2824818193912506
train gradient:  0.1776824383713911
iteration : 7209
train acc:  0.8515625
train loss:  0.36793580651283264
train gradient:  0.2162805922824136
iteration : 7210
train acc:  0.875
train loss:  0.32290083169937134
train gradient:  0.1671562655799156
iteration : 7211
train acc:  0.875
train loss:  0.2741711139678955
train gradient:  0.127336850758456
iteration : 7212
train acc:  0.8359375
train loss:  0.3431049585342407
train gradient:  0.19137648659381395
iteration : 7213
train acc:  0.8515625
train loss:  0.3431505858898163
train gradient:  0.2726765300683031
iteration : 7214
train acc:  0.84375
train loss:  0.361217737197876
train gradient:  0.2568670828080551
iteration : 7215
train acc:  0.78125
train loss:  0.43050965666770935
train gradient:  0.3596088946758643
iteration : 7216
train acc:  0.8515625
train loss:  0.2993745803833008
train gradient:  0.18637863619668557
iteration : 7217
train acc:  0.859375
train loss:  0.3153914213180542
train gradient:  0.18088257078581088
iteration : 7218
train acc:  0.8203125
train loss:  0.4067647159099579
train gradient:  0.2733826764472275
iteration : 7219
train acc:  0.859375
train loss:  0.3537694811820984
train gradient:  0.1976792685340732
iteration : 7220
train acc:  0.8359375
train loss:  0.40079987049102783
train gradient:  0.3289666241266985
iteration : 7221
train acc:  0.8359375
train loss:  0.3577471375465393
train gradient:  0.2210453860785631
iteration : 7222
train acc:  0.8671875
train loss:  0.2977321147918701
train gradient:  0.21049537712444377
iteration : 7223
train acc:  0.8828125
train loss:  0.3044493794441223
train gradient:  0.15832831173923836
iteration : 7224
train acc:  0.84375
train loss:  0.3267931342124939
train gradient:  0.1701860523653848
iteration : 7225
train acc:  0.8203125
train loss:  0.402696818113327
train gradient:  0.23641993693958732
iteration : 7226
train acc:  0.8203125
train loss:  0.4132581949234009
train gradient:  0.29853161221732466
iteration : 7227
train acc:  0.8515625
train loss:  0.36729124188423157
train gradient:  0.1563648956200605
iteration : 7228
train acc:  0.84375
train loss:  0.3251778781414032
train gradient:  0.1284227794165163
iteration : 7229
train acc:  0.84375
train loss:  0.34074345231056213
train gradient:  0.12619198018144195
iteration : 7230
train acc:  0.859375
train loss:  0.3631969690322876
train gradient:  0.22515506023371049
iteration : 7231
train acc:  0.84375
train loss:  0.375679612159729
train gradient:  0.20203825564941216
iteration : 7232
train acc:  0.9140625
train loss:  0.2793617844581604
train gradient:  0.1374475954341794
iteration : 7233
train acc:  0.8671875
train loss:  0.33598506450653076
train gradient:  0.21972331382967988
iteration : 7234
train acc:  0.875
train loss:  0.29799970984458923
train gradient:  0.14003376658700817
iteration : 7235
train acc:  0.828125
train loss:  0.3634777367115021
train gradient:  0.21172861673601512
iteration : 7236
train acc:  0.8046875
train loss:  0.38835087418556213
train gradient:  0.2861636298116081
iteration : 7237
train acc:  0.8828125
train loss:  0.3345302939414978
train gradient:  0.3376188589024552
iteration : 7238
train acc:  0.8671875
train loss:  0.3286895155906677
train gradient:  0.13059847744737246
iteration : 7239
train acc:  0.859375
train loss:  0.3192709684371948
train gradient:  0.23151530863332434
iteration : 7240
train acc:  0.890625
train loss:  0.360628604888916
train gradient:  0.41117258289643943
iteration : 7241
train acc:  0.84375
train loss:  0.41909557580947876
train gradient:  0.24992989638556176
iteration : 7242
train acc:  0.8515625
train loss:  0.34069404006004333
train gradient:  0.16395740531479205
iteration : 7243
train acc:  0.8203125
train loss:  0.37453189492225647
train gradient:  0.27923266684836334
iteration : 7244
train acc:  0.9296875
train loss:  0.23800843954086304
train gradient:  0.11832144048219914
iteration : 7245
train acc:  0.84375
train loss:  0.4439677596092224
train gradient:  0.40318220268003824
iteration : 7246
train acc:  0.875
train loss:  0.3048716187477112
train gradient:  0.17273291024598386
iteration : 7247
train acc:  0.875
train loss:  0.33960556983947754
train gradient:  0.25121899748132565
iteration : 7248
train acc:  0.84375
train loss:  0.3291720151901245
train gradient:  0.26783424092723757
iteration : 7249
train acc:  0.7890625
train loss:  0.40763595700263977
train gradient:  0.25403285355192184
iteration : 7250
train acc:  0.8515625
train loss:  0.29241645336151123
train gradient:  0.17507782107507958
iteration : 7251
train acc:  0.875
train loss:  0.28082871437072754
train gradient:  0.12868474230355384
iteration : 7252
train acc:  0.890625
train loss:  0.2648298442363739
train gradient:  0.1175429758085828
iteration : 7253
train acc:  0.84375
train loss:  0.33326780796051025
train gradient:  0.21909393198612068
iteration : 7254
train acc:  0.8359375
train loss:  0.3337276875972748
train gradient:  0.21444075372321883
iteration : 7255
train acc:  0.859375
train loss:  0.3452356457710266
train gradient:  0.20139630619014204
iteration : 7256
train acc:  0.8984375
train loss:  0.23803846538066864
train gradient:  0.14204802085017737
iteration : 7257
train acc:  0.8125
train loss:  0.38694310188293457
train gradient:  0.26466264867795997
iteration : 7258
train acc:  0.859375
train loss:  0.325158953666687
train gradient:  0.18373960388170457
iteration : 7259
train acc:  0.84375
train loss:  0.36511391401290894
train gradient:  0.2476824235363861
iteration : 7260
train acc:  0.875
train loss:  0.2718926668167114
train gradient:  0.12101338083093632
iteration : 7261
train acc:  0.8671875
train loss:  0.31817924976348877
train gradient:  0.18404767513262713
iteration : 7262
train acc:  0.890625
train loss:  0.2780313193798065
train gradient:  0.14558600382083603
iteration : 7263
train acc:  0.8984375
train loss:  0.31326621770858765
train gradient:  0.18702048761760942
iteration : 7264
train acc:  0.8125
train loss:  0.4407314360141754
train gradient:  0.2817085972136746
iteration : 7265
train acc:  0.8671875
train loss:  0.2967086732387543
train gradient:  0.19776719119667552
iteration : 7266
train acc:  0.859375
train loss:  0.3416369557380676
train gradient:  0.23498956170962235
iteration : 7267
train acc:  0.859375
train loss:  0.31733471155166626
train gradient:  0.22461563493260336
iteration : 7268
train acc:  0.8203125
train loss:  0.4296795129776001
train gradient:  0.36634893726230516
iteration : 7269
train acc:  0.859375
train loss:  0.29504087567329407
train gradient:  0.2660955565666596
iteration : 7270
train acc:  0.8359375
train loss:  0.3580854833126068
train gradient:  0.19402455141949979
iteration : 7271
train acc:  0.8359375
train loss:  0.3853239119052887
train gradient:  0.29575476479789586
iteration : 7272
train acc:  0.8671875
train loss:  0.30818474292755127
train gradient:  0.16062710992525633
iteration : 7273
train acc:  0.8671875
train loss:  0.36035144329071045
train gradient:  0.19991148088394106
iteration : 7274
train acc:  0.8515625
train loss:  0.3038409650325775
train gradient:  0.1936068896776602
iteration : 7275
train acc:  0.828125
train loss:  0.4251575767993927
train gradient:  0.2488230520677689
iteration : 7276
train acc:  0.828125
train loss:  0.3551950454711914
train gradient:  0.20970434849684977
iteration : 7277
train acc:  0.828125
train loss:  0.42956843972206116
train gradient:  0.2860993575905158
iteration : 7278
train acc:  0.8671875
train loss:  0.32891958951950073
train gradient:  0.18349322642396304
iteration : 7279
train acc:  0.8515625
train loss:  0.2941579222679138
train gradient:  0.1679077319156666
iteration : 7280
train acc:  0.84375
train loss:  0.3961285352706909
train gradient:  0.2951342971773365
iteration : 7281
train acc:  0.8671875
train loss:  0.3222559690475464
train gradient:  0.17346687682612036
iteration : 7282
train acc:  0.859375
train loss:  0.33192214369773865
train gradient:  0.1619752255970556
iteration : 7283
train acc:  0.8828125
train loss:  0.29485756158828735
train gradient:  0.1772041782622159
iteration : 7284
train acc:  0.875
train loss:  0.30086812376976013
train gradient:  0.17522286915777602
iteration : 7285
train acc:  0.8671875
train loss:  0.32889848947525024
train gradient:  0.16318335345012294
iteration : 7286
train acc:  0.875
train loss:  0.3439948856830597
train gradient:  0.15853933486197397
iteration : 7287
train acc:  0.859375
train loss:  0.28144359588623047
train gradient:  0.1339338538569092
iteration : 7288
train acc:  0.859375
train loss:  0.3310723602771759
train gradient:  0.19084042464947848
iteration : 7289
train acc:  0.8515625
train loss:  0.3426951766014099
train gradient:  0.14165225967918565
iteration : 7290
train acc:  0.859375
train loss:  0.3141327202320099
train gradient:  0.21526326024382583
iteration : 7291
train acc:  0.765625
train loss:  0.46619275212287903
train gradient:  0.4063148746765146
iteration : 7292
train acc:  0.84375
train loss:  0.3389226198196411
train gradient:  0.26329201508985883
iteration : 7293
train acc:  0.8828125
train loss:  0.30857303738594055
train gradient:  0.20549087496305118
iteration : 7294
train acc:  0.7890625
train loss:  0.4687080979347229
train gradient:  0.41084305679136557
iteration : 7295
train acc:  0.8046875
train loss:  0.37028375267982483
train gradient:  0.28628640408946726
iteration : 7296
train acc:  0.8203125
train loss:  0.34007835388183594
train gradient:  0.24536707916920347
iteration : 7297
train acc:  0.84375
train loss:  0.3642868995666504
train gradient:  0.2011861534545037
iteration : 7298
train acc:  0.8203125
train loss:  0.38591599464416504
train gradient:  0.2938841322464375
iteration : 7299
train acc:  0.8359375
train loss:  0.37582141160964966
train gradient:  0.29507086801077914
iteration : 7300
train acc:  0.90625
train loss:  0.25255241990089417
train gradient:  0.13156655563500197
iteration : 7301
train acc:  0.8515625
train loss:  0.33978384733200073
train gradient:  0.2571647957887068
iteration : 7302
train acc:  0.8828125
train loss:  0.2795996367931366
train gradient:  0.1992944817185735
iteration : 7303
train acc:  0.9140625
train loss:  0.2545318007469177
train gradient:  0.12631943689427338
iteration : 7304
train acc:  0.8671875
train loss:  0.2756745517253876
train gradient:  0.11881771123144295
iteration : 7305
train acc:  0.859375
train loss:  0.30041688680648804
train gradient:  0.31776786592034373
iteration : 7306
train acc:  0.8828125
train loss:  0.3004743754863739
train gradient:  0.15114345153697195
iteration : 7307
train acc:  0.75
train loss:  0.4716675877571106
train gradient:  0.3827728908217876
iteration : 7308
train acc:  0.828125
train loss:  0.37539273500442505
train gradient:  0.21274947696985347
iteration : 7309
train acc:  0.875
train loss:  0.25236114859580994
train gradient:  0.17862429105345948
iteration : 7310
train acc:  0.875
train loss:  0.32726162672042847
train gradient:  0.21133655248914354
iteration : 7311
train acc:  0.8828125
train loss:  0.25704988837242126
train gradient:  0.16278150793309587
iteration : 7312
train acc:  0.8515625
train loss:  0.3509815037250519
train gradient:  0.24034724532475218
iteration : 7313
train acc:  0.8359375
train loss:  0.42760851979255676
train gradient:  0.25172165747959285
iteration : 7314
train acc:  0.84375
train loss:  0.2937542796134949
train gradient:  0.17951296114989504
iteration : 7315
train acc:  0.8515625
train loss:  0.37119826674461365
train gradient:  0.3211247906122899
iteration : 7316
train acc:  0.84375
train loss:  0.29667896032333374
train gradient:  0.18913717951777975
iteration : 7317
train acc:  0.875
train loss:  0.28337764739990234
train gradient:  0.21910265614169422
iteration : 7318
train acc:  0.828125
train loss:  0.33013075590133667
train gradient:  0.15521036768697122
iteration : 7319
train acc:  0.828125
train loss:  0.370212197303772
train gradient:  0.27157957958379364
iteration : 7320
train acc:  0.828125
train loss:  0.30812013149261475
train gradient:  0.1322699887816605
iteration : 7321
train acc:  0.890625
train loss:  0.2784423828125
train gradient:  0.1707582866334495
iteration : 7322
train acc:  0.859375
train loss:  0.36675775051116943
train gradient:  0.2707901821777547
iteration : 7323
train acc:  0.890625
train loss:  0.2812887728214264
train gradient:  0.24725017871307484
iteration : 7324
train acc:  0.8359375
train loss:  0.4014805853366852
train gradient:  0.22612862375144266
iteration : 7325
train acc:  0.828125
train loss:  0.4446881413459778
train gradient:  0.40142146884565144
iteration : 7326
train acc:  0.8515625
train loss:  0.33015885949134827
train gradient:  0.22396624260735729
iteration : 7327
train acc:  0.8828125
train loss:  0.28165125846862793
train gradient:  0.18757002230163408
iteration : 7328
train acc:  0.875
train loss:  0.3379828631877899
train gradient:  0.22939129834294786
iteration : 7329
train acc:  0.8828125
train loss:  0.2815343141555786
train gradient:  0.24877026454634113
iteration : 7330
train acc:  0.8203125
train loss:  0.41562268137931824
train gradient:  0.26760486174263015
iteration : 7331
train acc:  0.8515625
train loss:  0.37365370988845825
train gradient:  0.3592941513195477
iteration : 7332
train acc:  0.8203125
train loss:  0.38351622223854065
train gradient:  0.319385136483706
iteration : 7333
train acc:  0.84375
train loss:  0.31453263759613037
train gradient:  0.22086244457269047
iteration : 7334
train acc:  0.7890625
train loss:  0.4443775713443756
train gradient:  0.337097749062727
iteration : 7335
train acc:  0.8984375
train loss:  0.29607394337654114
train gradient:  0.17100202011143212
iteration : 7336
train acc:  0.84375
train loss:  0.3576236963272095
train gradient:  0.29427153024464703
iteration : 7337
train acc:  0.8671875
train loss:  0.3258656859397888
train gradient:  0.225966117554301
iteration : 7338
train acc:  0.8515625
train loss:  0.3155573308467865
train gradient:  0.1822522628061935
iteration : 7339
train acc:  0.8046875
train loss:  0.410356342792511
train gradient:  0.29203255574253256
iteration : 7340
train acc:  0.8671875
train loss:  0.26983028650283813
train gradient:  0.15932252734972546
iteration : 7341
train acc:  0.8515625
train loss:  0.35779327154159546
train gradient:  0.2017064611606637
iteration : 7342
train acc:  0.84375
train loss:  0.34320467710494995
train gradient:  0.19742186157966873
iteration : 7343
train acc:  0.859375
train loss:  0.30264604091644287
train gradient:  0.18686607415095907
iteration : 7344
train acc:  0.859375
train loss:  0.35367685556411743
train gradient:  0.18990734532716402
iteration : 7345
train acc:  0.84375
train loss:  0.36359381675720215
train gradient:  0.21504246554247683
iteration : 7346
train acc:  0.8984375
train loss:  0.3227335810661316
train gradient:  0.19746955274864375
iteration : 7347
train acc:  0.8828125
train loss:  0.30605363845825195
train gradient:  0.2586118826235744
iteration : 7348
train acc:  0.890625
train loss:  0.2776734232902527
train gradient:  0.17914407881722458
iteration : 7349
train acc:  0.859375
train loss:  0.34364640712738037
train gradient:  0.29372234200685815
iteration : 7350
train acc:  0.8828125
train loss:  0.29463064670562744
train gradient:  0.29353520358036705
iteration : 7351
train acc:  0.90625
train loss:  0.28653159737586975
train gradient:  0.16642452366315208
iteration : 7352
train acc:  0.8515625
train loss:  0.3583469092845917
train gradient:  0.3490367315423789
iteration : 7353
train acc:  0.859375
train loss:  0.3062301278114319
train gradient:  0.16735707784226148
iteration : 7354
train acc:  0.859375
train loss:  0.323089599609375
train gradient:  0.13459410604967736
iteration : 7355
train acc:  0.8203125
train loss:  0.3704977333545685
train gradient:  0.2531127300614354
iteration : 7356
train acc:  0.8125
train loss:  0.4179283678531647
train gradient:  0.29941991742128565
iteration : 7357
train acc:  0.796875
train loss:  0.39587122201919556
train gradient:  0.25788469724943214
iteration : 7358
train acc:  0.8828125
train loss:  0.2868248224258423
train gradient:  0.171515055682933
iteration : 7359
train acc:  0.8828125
train loss:  0.2815760374069214
train gradient:  0.3061285789104081
iteration : 7360
train acc:  0.875
train loss:  0.31301581859588623
train gradient:  0.2620221731076845
iteration : 7361
train acc:  0.8359375
train loss:  0.41229814291000366
train gradient:  0.25359503088541224
iteration : 7362
train acc:  0.8515625
train loss:  0.3875885009765625
train gradient:  0.2782861877561714
iteration : 7363
train acc:  0.859375
train loss:  0.3342049717903137
train gradient:  0.23331104698447405
iteration : 7364
train acc:  0.859375
train loss:  0.3104891777038574
train gradient:  0.1559255552480563
iteration : 7365
train acc:  0.890625
train loss:  0.2845063805580139
train gradient:  0.15967541681067
iteration : 7366
train acc:  0.8515625
train loss:  0.28264713287353516
train gradient:  0.16240966518249073
iteration : 7367
train acc:  0.8125
train loss:  0.42895394563674927
train gradient:  0.28129915908971975
iteration : 7368
train acc:  0.8984375
train loss:  0.25842106342315674
train gradient:  0.16300560759332294
iteration : 7369
train acc:  0.84375
train loss:  0.39319971203804016
train gradient:  0.31011671810440666
iteration : 7370
train acc:  0.8671875
train loss:  0.2979587912559509
train gradient:  0.2006472304711486
iteration : 7371
train acc:  0.8203125
train loss:  0.3626920282840729
train gradient:  0.2569859124101504
iteration : 7372
train acc:  0.890625
train loss:  0.3134959042072296
train gradient:  0.2702997262895431
iteration : 7373
train acc:  0.875
train loss:  0.33064043521881104
train gradient:  0.1794936083944595
iteration : 7374
train acc:  0.8671875
train loss:  0.3272756338119507
train gradient:  0.14827195669103072
iteration : 7375
train acc:  0.859375
train loss:  0.3121216595172882
train gradient:  0.16550550612001302
iteration : 7376
train acc:  0.8984375
train loss:  0.297133207321167
train gradient:  0.1325937078248837
iteration : 7377
train acc:  0.8359375
train loss:  0.33607834577560425
train gradient:  0.42707708889166124
iteration : 7378
train acc:  0.8125
train loss:  0.3458563983440399
train gradient:  0.16116570688604823
iteration : 7379
train acc:  0.875
train loss:  0.26221728324890137
train gradient:  0.16897174665019613
iteration : 7380
train acc:  0.859375
train loss:  0.37172219157218933
train gradient:  0.20824621731080836
iteration : 7381
train acc:  0.8515625
train loss:  0.32151269912719727
train gradient:  0.1488975392842992
iteration : 7382
train acc:  0.8984375
train loss:  0.30104342103004456
train gradient:  0.2187233582267824
iteration : 7383
train acc:  0.8671875
train loss:  0.323188841342926
train gradient:  0.23098887732109952
iteration : 7384
train acc:  0.8359375
train loss:  0.33251678943634033
train gradient:  0.23828953755302074
iteration : 7385
train acc:  0.90625
train loss:  0.2922552227973938
train gradient:  0.16107668753242219
iteration : 7386
train acc:  0.8515625
train loss:  0.3301897644996643
train gradient:  0.22010623152290504
iteration : 7387
train acc:  0.8671875
train loss:  0.2865292429924011
train gradient:  0.16633514334743926
iteration : 7388
train acc:  0.90625
train loss:  0.228745698928833
train gradient:  0.11088656555304112
iteration : 7389
train acc:  0.7734375
train loss:  0.4533557891845703
train gradient:  0.3205808642590033
iteration : 7390
train acc:  0.875
train loss:  0.36356306076049805
train gradient:  0.42378819191602596
iteration : 7391
train acc:  0.84375
train loss:  0.3701411187648773
train gradient:  0.1705585709371438
iteration : 7392
train acc:  0.875
train loss:  0.29328644275665283
train gradient:  0.15083297732129058
iteration : 7393
train acc:  0.8828125
train loss:  0.34686553478240967
train gradient:  0.22884710360239452
iteration : 7394
train acc:  0.859375
train loss:  0.29529622197151184
train gradient:  0.18142453290446775
iteration : 7395
train acc:  0.859375
train loss:  0.3460255265235901
train gradient:  0.24605450633154313
iteration : 7396
train acc:  0.890625
train loss:  0.25114893913269043
train gradient:  0.13054167040377163
iteration : 7397
train acc:  0.8671875
train loss:  0.3657459020614624
train gradient:  0.2696691521164881
iteration : 7398
train acc:  0.8046875
train loss:  0.4108813405036926
train gradient:  0.27249017261808756
iteration : 7399
train acc:  0.828125
train loss:  0.4105261266231537
train gradient:  0.3207174657941314
iteration : 7400
train acc:  0.8515625
train loss:  0.35008227825164795
train gradient:  0.19315654727340476
iteration : 7401
train acc:  0.859375
train loss:  0.3415340781211853
train gradient:  0.20854958605827922
iteration : 7402
train acc:  0.8203125
train loss:  0.4114258587360382
train gradient:  0.24116982555851152
iteration : 7403
train acc:  0.859375
train loss:  0.2937316298484802
train gradient:  0.18742969296235693
iteration : 7404
train acc:  0.875
train loss:  0.3009447753429413
train gradient:  0.19286027045977686
iteration : 7405
train acc:  0.8515625
train loss:  0.30810993909835815
train gradient:  0.21831468814193566
iteration : 7406
train acc:  0.7734375
train loss:  0.4357073903083801
train gradient:  0.29954614519901956
iteration : 7407
train acc:  0.8515625
train loss:  0.3370826244354248
train gradient:  0.23895889189136088
iteration : 7408
train acc:  0.890625
train loss:  0.30325186252593994
train gradient:  0.17339039405708717
iteration : 7409
train acc:  0.8359375
train loss:  0.3467567563056946
train gradient:  0.22340364711157587
iteration : 7410
train acc:  0.8359375
train loss:  0.3523561954498291
train gradient:  0.2205835100879219
iteration : 7411
train acc:  0.8046875
train loss:  0.42162591218948364
train gradient:  0.2478641827792185
iteration : 7412
train acc:  0.8359375
train loss:  0.3724498748779297
train gradient:  0.28305937622508326
iteration : 7413
train acc:  0.84375
train loss:  0.2813134789466858
train gradient:  0.1462705032817011
iteration : 7414
train acc:  0.828125
train loss:  0.3628835678100586
train gradient:  0.22307420303668224
iteration : 7415
train acc:  0.8671875
train loss:  0.29651153087615967
train gradient:  0.17836697736493706
iteration : 7416
train acc:  0.828125
train loss:  0.44594717025756836
train gradient:  0.35580709346525563
iteration : 7417
train acc:  0.8671875
train loss:  0.3236413598060608
train gradient:  0.19421417432244453
iteration : 7418
train acc:  0.859375
train loss:  0.3528713583946228
train gradient:  0.19489028837985722
iteration : 7419
train acc:  0.8203125
train loss:  0.3316952884197235
train gradient:  0.18684084748425328
iteration : 7420
train acc:  0.8125
train loss:  0.40596839785575867
train gradient:  0.2493263354507338
iteration : 7421
train acc:  0.8359375
train loss:  0.3730867803096771
train gradient:  0.20211330084257256
iteration : 7422
train acc:  0.8203125
train loss:  0.3946484923362732
train gradient:  0.18005930656492258
iteration : 7423
train acc:  0.828125
train loss:  0.3918350040912628
train gradient:  0.2015350979630673
iteration : 7424
train acc:  0.796875
train loss:  0.40667518973350525
train gradient:  0.30041140335335065
iteration : 7425
train acc:  0.8359375
train loss:  0.3967529535293579
train gradient:  0.19894318133484779
iteration : 7426
train acc:  0.90625
train loss:  0.2652118504047394
train gradient:  0.13606196933702913
iteration : 7427
train acc:  0.8984375
train loss:  0.27757251262664795
train gradient:  0.11328514479545466
iteration : 7428
train acc:  0.8984375
train loss:  0.33626171946525574
train gradient:  0.16820722042291733
iteration : 7429
train acc:  0.8515625
train loss:  0.31974244117736816
train gradient:  0.18139601739699185
iteration : 7430
train acc:  0.8671875
train loss:  0.2622532546520233
train gradient:  0.19921789670439707
iteration : 7431
train acc:  0.890625
train loss:  0.24766455590724945
train gradient:  0.12645581545957485
iteration : 7432
train acc:  0.8046875
train loss:  0.4245704412460327
train gradient:  0.27433778306400625
iteration : 7433
train acc:  0.8828125
train loss:  0.27169227600097656
train gradient:  0.17767813561939133
iteration : 7434
train acc:  0.875
train loss:  0.30555665493011475
train gradient:  0.11357382695674585
iteration : 7435
train acc:  0.8984375
train loss:  0.3214438557624817
train gradient:  0.22487003596268476
iteration : 7436
train acc:  0.8671875
train loss:  0.3827182352542877
train gradient:  0.2475441357358784
iteration : 7437
train acc:  0.859375
train loss:  0.3330337405204773
train gradient:  0.14275750909272766
iteration : 7438
train acc:  0.8359375
train loss:  0.3441198468208313
train gradient:  0.16339147835054418
iteration : 7439
train acc:  0.8515625
train loss:  0.3976860046386719
train gradient:  0.20864193683864612
iteration : 7440
train acc:  0.875
train loss:  0.35599982738494873
train gradient:  0.18115586177343945
iteration : 7441
train acc:  0.828125
train loss:  0.4066029489040375
train gradient:  0.20729806447369847
iteration : 7442
train acc:  0.8125
train loss:  0.44222521781921387
train gradient:  0.2822594669440783
iteration : 7443
train acc:  0.875
train loss:  0.37815535068511963
train gradient:  0.2434066966207154
iteration : 7444
train acc:  0.890625
train loss:  0.2775542736053467
train gradient:  0.12135484841704237
iteration : 7445
train acc:  0.890625
train loss:  0.33681339025497437
train gradient:  0.18790516143350225
iteration : 7446
train acc:  0.8515625
train loss:  0.37197989225387573
train gradient:  0.22785860566826485
iteration : 7447
train acc:  0.828125
train loss:  0.4103316366672516
train gradient:  0.3141818666004481
iteration : 7448
train acc:  0.796875
train loss:  0.4364343285560608
train gradient:  0.27335773864941654
iteration : 7449
train acc:  0.90625
train loss:  0.2732878625392914
train gradient:  0.11687886579219385
iteration : 7450
train acc:  0.8515625
train loss:  0.3526119887828827
train gradient:  0.26113787557393475
iteration : 7451
train acc:  0.8515625
train loss:  0.33833998441696167
train gradient:  0.19603092067854685
iteration : 7452
train acc:  0.8515625
train loss:  0.34103330969810486
train gradient:  0.1867803578593149
iteration : 7453
train acc:  0.875
train loss:  0.33744680881500244
train gradient:  0.22937819394051656
iteration : 7454
train acc:  0.859375
train loss:  0.34631767868995667
train gradient:  0.22865952993123118
iteration : 7455
train acc:  0.8515625
train loss:  0.3320387005805969
train gradient:  0.17880019350059864
iteration : 7456
train acc:  0.8515625
train loss:  0.2919093370437622
train gradient:  0.15652200500263405
iteration : 7457
train acc:  0.890625
train loss:  0.23518231511116028
train gradient:  0.11628045341663487
iteration : 7458
train acc:  0.84375
train loss:  0.3307872712612152
train gradient:  0.22884764795607077
iteration : 7459
train acc:  0.859375
train loss:  0.3625384271144867
train gradient:  0.22258356557967068
iteration : 7460
train acc:  0.828125
train loss:  0.37249237298965454
train gradient:  0.24048597569216734
iteration : 7461
train acc:  0.7890625
train loss:  0.40168559551239014
train gradient:  0.20906995952241564
iteration : 7462
train acc:  0.828125
train loss:  0.35416436195373535
train gradient:  0.20032914150129488
iteration : 7463
train acc:  0.859375
train loss:  0.3475114107131958
train gradient:  0.2263027556712553
iteration : 7464
train acc:  0.875
train loss:  0.3412288725376129
train gradient:  0.1299855459285012
iteration : 7465
train acc:  0.8515625
train loss:  0.320473849773407
train gradient:  0.20136438961281944
iteration : 7466
train acc:  0.828125
train loss:  0.3336912989616394
train gradient:  0.1582762044868482
iteration : 7467
train acc:  0.8125
train loss:  0.4360089600086212
train gradient:  0.3625244198265178
iteration : 7468
train acc:  0.875
train loss:  0.2615131735801697
train gradient:  0.1612821709150397
iteration : 7469
train acc:  0.78125
train loss:  0.42867928743362427
train gradient:  0.25896677782690813
iteration : 7470
train acc:  0.8671875
train loss:  0.34014415740966797
train gradient:  0.22584231151423928
iteration : 7471
train acc:  0.84375
train loss:  0.31212982535362244
train gradient:  0.15324936727090827
iteration : 7472
train acc:  0.84375
train loss:  0.32573580741882324
train gradient:  0.17249215862451728
iteration : 7473
train acc:  0.8515625
train loss:  0.3892991244792938
train gradient:  0.3190138459173152
iteration : 7474
train acc:  0.9375
train loss:  0.1994594782590866
train gradient:  0.10325352055791076
iteration : 7475
train acc:  0.890625
train loss:  0.27477970719337463
train gradient:  0.14011036145263153
iteration : 7476
train acc:  0.859375
train loss:  0.36722660064697266
train gradient:  0.273489106115472
iteration : 7477
train acc:  0.8125
train loss:  0.3932363986968994
train gradient:  0.195693982005014
iteration : 7478
train acc:  0.90625
train loss:  0.2474404275417328
train gradient:  0.125975770963882
iteration : 7479
train acc:  0.8984375
train loss:  0.2616550326347351
train gradient:  0.14851931053275966
iteration : 7480
train acc:  0.84375
train loss:  0.37468159198760986
train gradient:  0.1722369422227672
iteration : 7481
train acc:  0.890625
train loss:  0.35022467374801636
train gradient:  0.19335973539408388
iteration : 7482
train acc:  0.828125
train loss:  0.3641170263290405
train gradient:  0.22500318487711268
iteration : 7483
train acc:  0.828125
train loss:  0.3221197724342346
train gradient:  0.16510297861852768
iteration : 7484
train acc:  0.8125
train loss:  0.40895897150039673
train gradient:  0.26973270774378477
iteration : 7485
train acc:  0.8359375
train loss:  0.39509356021881104
train gradient:  0.19958804429337434
iteration : 7486
train acc:  0.8828125
train loss:  0.24229733645915985
train gradient:  0.3366071654912615
iteration : 7487
train acc:  0.8671875
train loss:  0.29860422015190125
train gradient:  0.22387173184768117
iteration : 7488
train acc:  0.921875
train loss:  0.20487457513809204
train gradient:  0.12154522481185766
iteration : 7489
train acc:  0.875
train loss:  0.3291904330253601
train gradient:  0.22420553694046286
iteration : 7490
train acc:  0.9296875
train loss:  0.2199423611164093
train gradient:  0.08500635934934826
iteration : 7491
train acc:  0.8984375
train loss:  0.24993743002414703
train gradient:  0.13539557664297677
iteration : 7492
train acc:  0.8828125
train loss:  0.311359167098999
train gradient:  0.17546466288374818
iteration : 7493
train acc:  0.859375
train loss:  0.36123237013816833
train gradient:  0.22890622776039005
iteration : 7494
train acc:  0.7890625
train loss:  0.47385385632514954
train gradient:  0.3355214551457991
iteration : 7495
train acc:  0.828125
train loss:  0.33928751945495605
train gradient:  0.2551482777726178
iteration : 7496
train acc:  0.8828125
train loss:  0.26223716139793396
train gradient:  0.14637461552358216
iteration : 7497
train acc:  0.8359375
train loss:  0.4004550576210022
train gradient:  0.2656750779618653
iteration : 7498
train acc:  0.8515625
train loss:  0.3392806649208069
train gradient:  0.23520047634362598
iteration : 7499
train acc:  0.859375
train loss:  0.3043033480644226
train gradient:  0.17618379675984203
iteration : 7500
train acc:  0.890625
train loss:  0.2658310830593109
train gradient:  0.1339710947963441
iteration : 7501
train acc:  0.8515625
train loss:  0.31602782011032104
train gradient:  0.1751895415219139
iteration : 7502
train acc:  0.84375
train loss:  0.42553943395614624
train gradient:  0.37234637367725815
iteration : 7503
train acc:  0.8984375
train loss:  0.2323252260684967
train gradient:  0.11546027365874266
iteration : 7504
train acc:  0.84375
train loss:  0.39621150493621826
train gradient:  0.17168741939926846
iteration : 7505
train acc:  0.8828125
train loss:  0.25458982586860657
train gradient:  0.18156597545948389
iteration : 7506
train acc:  0.84375
train loss:  0.36882156133651733
train gradient:  0.19317456543676897
iteration : 7507
train acc:  0.8203125
train loss:  0.3648085594177246
train gradient:  0.2539725002569603
iteration : 7508
train acc:  0.859375
train loss:  0.2738955020904541
train gradient:  0.15932582288577365
iteration : 7509
train acc:  0.8515625
train loss:  0.38896477222442627
train gradient:  0.2679002036583953
iteration : 7510
train acc:  0.8515625
train loss:  0.38463589549064636
train gradient:  0.3184122808787301
iteration : 7511
train acc:  0.84375
train loss:  0.37039658427238464
train gradient:  0.4895565076811691
iteration : 7512
train acc:  0.859375
train loss:  0.3571907579898834
train gradient:  0.24054238182068322
iteration : 7513
train acc:  0.8203125
train loss:  0.39874887466430664
train gradient:  1.1142146821829384
iteration : 7514
train acc:  0.875
train loss:  0.324435830116272
train gradient:  0.17971577929827184
iteration : 7515
train acc:  0.859375
train loss:  0.3643677532672882
train gradient:  0.2915158719521884
iteration : 7516
train acc:  0.84375
train loss:  0.37269437313079834
train gradient:  0.2953803705854264
iteration : 7517
train acc:  0.84375
train loss:  0.3583826422691345
train gradient:  0.20338495532591572
iteration : 7518
train acc:  0.859375
train loss:  0.29770225286483765
train gradient:  0.1706402475188824
iteration : 7519
train acc:  0.8671875
train loss:  0.30947715044021606
train gradient:  0.1232301695005248
iteration : 7520
train acc:  0.8515625
train loss:  0.3418709635734558
train gradient:  0.19200326595027495
iteration : 7521
train acc:  0.875
train loss:  0.2775724530220032
train gradient:  0.14297054828196254
iteration : 7522
train acc:  0.8515625
train loss:  0.3679925203323364
train gradient:  0.16609913301832496
iteration : 7523
train acc:  0.8203125
train loss:  0.34227728843688965
train gradient:  0.16306185150351166
iteration : 7524
train acc:  0.8828125
train loss:  0.3224502205848694
train gradient:  0.1981789221625247
iteration : 7525
train acc:  0.84375
train loss:  0.29937589168548584
train gradient:  0.20745506739603692
iteration : 7526
train acc:  0.90625
train loss:  0.24592538177967072
train gradient:  0.14348193024788863
iteration : 7527
train acc:  0.84375
train loss:  0.3042301535606384
train gradient:  0.26671602124651017
iteration : 7528
train acc:  0.859375
train loss:  0.360949844121933
train gradient:  0.24565492176370865
iteration : 7529
train acc:  0.796875
train loss:  0.42353636026382446
train gradient:  0.31278593478508043
iteration : 7530
train acc:  0.8359375
train loss:  0.31434568762779236
train gradient:  0.13536764067789603
iteration : 7531
train acc:  0.84375
train loss:  0.3177679777145386
train gradient:  0.19307504162025851
iteration : 7532
train acc:  0.8515625
train loss:  0.37445878982543945
train gradient:  0.3341478672584603
iteration : 7533
train acc:  0.8671875
train loss:  0.321537584066391
train gradient:  0.1780038383868023
iteration : 7534
train acc:  0.8671875
train loss:  0.31242436170578003
train gradient:  0.20596203742748276
iteration : 7535
train acc:  0.84375
train loss:  0.3195830285549164
train gradient:  0.2037108861320331
iteration : 7536
train acc:  0.8125
train loss:  0.30884721875190735
train gradient:  0.220421173154202
iteration : 7537
train acc:  0.890625
train loss:  0.2608141005039215
train gradient:  0.13653500641673239
iteration : 7538
train acc:  0.84375
train loss:  0.32698529958724976
train gradient:  0.20516418247768098
iteration : 7539
train acc:  0.8671875
train loss:  0.37296581268310547
train gradient:  0.20888122423090869
iteration : 7540
train acc:  0.859375
train loss:  0.3796628415584564
train gradient:  0.19832066288013955
iteration : 7541
train acc:  0.7890625
train loss:  0.41646894812583923
train gradient:  0.2124477662600051
iteration : 7542
train acc:  0.875
train loss:  0.30460241436958313
train gradient:  0.23391183170812027
iteration : 7543
train acc:  0.859375
train loss:  0.3311138451099396
train gradient:  0.1782552373271581
iteration : 7544
train acc:  0.84375
train loss:  0.3604544997215271
train gradient:  0.22514428933524072
iteration : 7545
train acc:  0.9140625
train loss:  0.2712618112564087
train gradient:  0.1771065898218741
iteration : 7546
train acc:  0.8515625
train loss:  0.3120051920413971
train gradient:  0.18358046658146387
iteration : 7547
train acc:  0.84375
train loss:  0.35026079416275024
train gradient:  0.2149667307346231
iteration : 7548
train acc:  0.828125
train loss:  0.3242570161819458
train gradient:  0.17443015420674918
iteration : 7549
train acc:  0.859375
train loss:  0.3145887851715088
train gradient:  0.2250673561535515
iteration : 7550
train acc:  0.8515625
train loss:  0.35758787393569946
train gradient:  0.18309968717714564
iteration : 7551
train acc:  0.875
train loss:  0.2975471615791321
train gradient:  0.17320202390820366
iteration : 7552
train acc:  0.8828125
train loss:  0.28232765197753906
train gradient:  0.20055758762119605
iteration : 7553
train acc:  0.8828125
train loss:  0.3328680098056793
train gradient:  0.2477969968320511
iteration : 7554
train acc:  0.8828125
train loss:  0.2707090377807617
train gradient:  0.13067036548903016
iteration : 7555
train acc:  0.8671875
train loss:  0.30434510111808777
train gradient:  0.13773933797742557
iteration : 7556
train acc:  0.859375
train loss:  0.34120237827301025
train gradient:  0.16851502262918097
iteration : 7557
train acc:  0.875
train loss:  0.29490113258361816
train gradient:  0.17502693565172936
iteration : 7558
train acc:  0.8203125
train loss:  0.40210890769958496
train gradient:  0.257307909949099
iteration : 7559
train acc:  0.8203125
train loss:  0.3299863040447235
train gradient:  0.18224429208915666
iteration : 7560
train acc:  0.9375
train loss:  0.2528710663318634
train gradient:  0.14950464206082248
iteration : 7561
train acc:  0.84375
train loss:  0.38589727878570557
train gradient:  0.2547077216590387
iteration : 7562
train acc:  0.8203125
train loss:  0.3894902765750885
train gradient:  0.3736035576222463
iteration : 7563
train acc:  0.8125
train loss:  0.44990140199661255
train gradient:  0.38300784532581716
iteration : 7564
train acc:  0.890625
train loss:  0.2752552628517151
train gradient:  0.12957429811137167
iteration : 7565
train acc:  0.859375
train loss:  0.29760438203811646
train gradient:  0.24134686930186236
iteration : 7566
train acc:  0.828125
train loss:  0.4255540370941162
train gradient:  0.3775959056205779
iteration : 7567
train acc:  0.8203125
train loss:  0.40128111839294434
train gradient:  0.44730393460762713
iteration : 7568
train acc:  0.84375
train loss:  0.2970358729362488
train gradient:  0.2630485852536438
iteration : 7569
train acc:  0.8125
train loss:  0.4064531624317169
train gradient:  0.30092381120959527
iteration : 7570
train acc:  0.8203125
train loss:  0.3955013155937195
train gradient:  0.2833775451596167
iteration : 7571
train acc:  0.8671875
train loss:  0.31467723846435547
train gradient:  0.2390498596490521
iteration : 7572
train acc:  0.859375
train loss:  0.30323541164398193
train gradient:  0.16956492584557886
iteration : 7573
train acc:  0.8046875
train loss:  0.43604186177253723
train gradient:  0.3129936329564426
iteration : 7574
train acc:  0.8125
train loss:  0.3590543270111084
train gradient:  0.30715147265138587
iteration : 7575
train acc:  0.8515625
train loss:  0.3254384398460388
train gradient:  0.1940365240987377
iteration : 7576
train acc:  0.8828125
train loss:  0.28598716855049133
train gradient:  0.15875913846699533
iteration : 7577
train acc:  0.890625
train loss:  0.2714260220527649
train gradient:  0.12223075531711546
iteration : 7578
train acc:  0.8515625
train loss:  0.2968134582042694
train gradient:  0.24710243432881904
iteration : 7579
train acc:  0.8671875
train loss:  0.32609522342681885
train gradient:  0.1852797360147109
iteration : 7580
train acc:  0.8515625
train loss:  0.32863619923591614
train gradient:  0.2273530069851365
iteration : 7581
train acc:  0.890625
train loss:  0.29006487131118774
train gradient:  0.13730747947902217
iteration : 7582
train acc:  0.8125
train loss:  0.4633415937423706
train gradient:  0.4706867576188949
iteration : 7583
train acc:  0.8515625
train loss:  0.32934504747390747
train gradient:  0.15117143380662973
iteration : 7584
train acc:  0.859375
train loss:  0.33867573738098145
train gradient:  0.24910167544289782
iteration : 7585
train acc:  0.84375
train loss:  0.2797144055366516
train gradient:  0.1839422545794539
iteration : 7586
train acc:  0.8828125
train loss:  0.31807446479797363
train gradient:  0.1649911206945172
iteration : 7587
train acc:  0.875
train loss:  0.32525619864463806
train gradient:  0.17165080604872116
iteration : 7588
train acc:  0.859375
train loss:  0.32875555753707886
train gradient:  0.18454371767226985
iteration : 7589
train acc:  0.8671875
train loss:  0.35131922364234924
train gradient:  0.2359660968083096
iteration : 7590
train acc:  0.8671875
train loss:  0.33294209837913513
train gradient:  0.15330118245931462
iteration : 7591
train acc:  0.90625
train loss:  0.27601122856140137
train gradient:  0.13689420979612493
iteration : 7592
train acc:  0.8671875
train loss:  0.33122551441192627
train gradient:  0.24346815907498817
iteration : 7593
train acc:  0.7890625
train loss:  0.4531782269477844
train gradient:  0.2856753128154243
iteration : 7594
train acc:  0.8359375
train loss:  0.3980697989463806
train gradient:  0.27887571876308986
iteration : 7595
train acc:  0.8984375
train loss:  0.31833839416503906
train gradient:  0.1908236894040066
iteration : 7596
train acc:  0.8671875
train loss:  0.32719364762306213
train gradient:  0.13634188944234243
iteration : 7597
train acc:  0.8125
train loss:  0.3984032869338989
train gradient:  0.17672800500023733
iteration : 7598
train acc:  0.9140625
train loss:  0.22694389522075653
train gradient:  0.12929651707443432
iteration : 7599
train acc:  0.8359375
train loss:  0.35981616377830505
train gradient:  0.2025340170787256
iteration : 7600
train acc:  0.84375
train loss:  0.36539241671562195
train gradient:  0.19387291897949488
iteration : 7601
train acc:  0.8203125
train loss:  0.388417512178421
train gradient:  0.24886819184919415
iteration : 7602
train acc:  0.8359375
train loss:  0.33158987760543823
train gradient:  0.20469923279672345
iteration : 7603
train acc:  0.8046875
train loss:  0.392273485660553
train gradient:  0.41522519020425797
iteration : 7604
train acc:  0.84375
train loss:  0.34608036279678345
train gradient:  0.1582629509723854
iteration : 7605
train acc:  0.890625
train loss:  0.2539057731628418
train gradient:  0.13359627256965484
iteration : 7606
train acc:  0.8359375
train loss:  0.32111966609954834
train gradient:  0.17421818898808472
iteration : 7607
train acc:  0.828125
train loss:  0.3754153847694397
train gradient:  0.21564012045431952
iteration : 7608
train acc:  0.8515625
train loss:  0.3108559250831604
train gradient:  0.21677159513476185
iteration : 7609
train acc:  0.8671875
train loss:  0.34028375148773193
train gradient:  0.20743981460936967
iteration : 7610
train acc:  0.84375
train loss:  0.353169709444046
train gradient:  0.22966622262670316
iteration : 7611
train acc:  0.875
train loss:  0.2893112897872925
train gradient:  0.1148479702474585
iteration : 7612
train acc:  0.8125
train loss:  0.3561176657676697
train gradient:  0.24266121930849013
iteration : 7613
train acc:  0.90625
train loss:  0.23422300815582275
train gradient:  0.1535302249892082
iteration : 7614
train acc:  0.8828125
train loss:  0.3242185711860657
train gradient:  0.19798500934144858
iteration : 7615
train acc:  0.8203125
train loss:  0.3543551564216614
train gradient:  0.21457635331072822
iteration : 7616
train acc:  0.84375
train loss:  0.3614189624786377
train gradient:  0.293940048877692
iteration : 7617
train acc:  0.78125
train loss:  0.401585191488266
train gradient:  0.3238209156691577
iteration : 7618
train acc:  0.890625
train loss:  0.2899635136127472
train gradient:  0.16628801038557095
iteration : 7619
train acc:  0.859375
train loss:  0.31821203231811523
train gradient:  0.17100916001983368
iteration : 7620
train acc:  0.875
train loss:  0.3188115656375885
train gradient:  0.1998493252427705
iteration : 7621
train acc:  0.828125
train loss:  0.4062856435775757
train gradient:  0.3370610162978997
iteration : 7622
train acc:  0.84375
train loss:  0.35448265075683594
train gradient:  0.23210389645205598
iteration : 7623
train acc:  0.875
train loss:  0.3248969316482544
train gradient:  0.19242918202848996
iteration : 7624
train acc:  0.8515625
train loss:  0.3551473617553711
train gradient:  0.30794112388484174
iteration : 7625
train acc:  0.8359375
train loss:  0.4130229353904724
train gradient:  0.32076721546040665
iteration : 7626
train acc:  0.859375
train loss:  0.3022359609603882
train gradient:  0.18630482213403515
iteration : 7627
train acc:  0.8671875
train loss:  0.3405364751815796
train gradient:  0.2710880798654902
iteration : 7628
train acc:  0.8359375
train loss:  0.349571168422699
train gradient:  0.1782603831380658
iteration : 7629
train acc:  0.8359375
train loss:  0.33299487829208374
train gradient:  0.21577857328832162
iteration : 7630
train acc:  0.8359375
train loss:  0.34603631496429443
train gradient:  0.22852692548465126
iteration : 7631
train acc:  0.828125
train loss:  0.42971348762512207
train gradient:  0.32327982661844823
iteration : 7632
train acc:  0.8828125
train loss:  0.3344569206237793
train gradient:  0.20557047589530214
iteration : 7633
train acc:  0.8671875
train loss:  0.30761635303497314
train gradient:  0.18966100658956203
iteration : 7634
train acc:  0.90625
train loss:  0.2626953125
train gradient:  0.19937406106425123
iteration : 7635
train acc:  0.859375
train loss:  0.35833418369293213
train gradient:  0.23984242156819074
iteration : 7636
train acc:  0.8515625
train loss:  0.32296818494796753
train gradient:  0.19908934319844956
iteration : 7637
train acc:  0.859375
train loss:  0.29051148891448975
train gradient:  0.1713058948824731
iteration : 7638
train acc:  0.8125
train loss:  0.3427746891975403
train gradient:  0.2501502949769792
iteration : 7639
train acc:  0.828125
train loss:  0.3654389977455139
train gradient:  0.21351293838020874
iteration : 7640
train acc:  0.875
train loss:  0.2666855454444885
train gradient:  0.16871920258274806
iteration : 7641
train acc:  0.859375
train loss:  0.3120933175086975
train gradient:  0.23214077711403527
iteration : 7642
train acc:  0.8359375
train loss:  0.3689354658126831
train gradient:  0.1890708504331975
iteration : 7643
train acc:  0.9296875
train loss:  0.24156010150909424
train gradient:  0.17562072047503988
iteration : 7644
train acc:  0.8359375
train loss:  0.3337286114692688
train gradient:  0.23684344926940962
iteration : 7645
train acc:  0.78125
train loss:  0.43191081285476685
train gradient:  0.2808686870966978
iteration : 7646
train acc:  0.8671875
train loss:  0.2550230622291565
train gradient:  0.11973712014150958
iteration : 7647
train acc:  0.796875
train loss:  0.4459182918071747
train gradient:  0.5730026639903212
iteration : 7648
train acc:  0.875
train loss:  0.27708515524864197
train gradient:  0.1457808034687214
iteration : 7649
train acc:  0.828125
train loss:  0.41642245650291443
train gradient:  0.27128565224737955
iteration : 7650
train acc:  0.8515625
train loss:  0.3763221800327301
train gradient:  0.2608265385517423
iteration : 7651
train acc:  0.890625
train loss:  0.30811363458633423
train gradient:  0.16214090964169148
iteration : 7652
train acc:  0.8984375
train loss:  0.2987966239452362
train gradient:  0.144650279024274
iteration : 7653
train acc:  0.875
train loss:  0.34687063097953796
train gradient:  0.1646834673938288
iteration : 7654
train acc:  0.859375
train loss:  0.3289797902107239
train gradient:  0.1961503921300149
iteration : 7655
train acc:  0.828125
train loss:  0.34402576088905334
train gradient:  0.1624882627984897
iteration : 7656
train acc:  0.8671875
train loss:  0.33735960721969604
train gradient:  0.22531603674627665
iteration : 7657
train acc:  0.8671875
train loss:  0.3489495515823364
train gradient:  0.1581086976487171
iteration : 7658
train acc:  0.8671875
train loss:  0.29222267866134644
train gradient:  0.2524633494323457
iteration : 7659
train acc:  0.8515625
train loss:  0.31594306230545044
train gradient:  0.16383083061429343
iteration : 7660
train acc:  0.84375
train loss:  0.30493485927581787
train gradient:  0.13710125817530627
iteration : 7661
train acc:  0.875
train loss:  0.28312960267066956
train gradient:  0.17047432853480218
iteration : 7662
train acc:  0.8203125
train loss:  0.444428950548172
train gradient:  0.32295191873658763
iteration : 7663
train acc:  0.875
train loss:  0.3192795515060425
train gradient:  0.1501588709179676
iteration : 7664
train acc:  0.890625
train loss:  0.3378424644470215
train gradient:  0.2058909845760348
iteration : 7665
train acc:  0.90625
train loss:  0.2490333914756775
train gradient:  0.22097514647397976
iteration : 7666
train acc:  0.84375
train loss:  0.29794764518737793
train gradient:  0.15000676016688694
iteration : 7667
train acc:  0.875
train loss:  0.3066708743572235
train gradient:  0.18880173325476568
iteration : 7668
train acc:  0.796875
train loss:  0.4398868978023529
train gradient:  0.3186177542408862
iteration : 7669
train acc:  0.8671875
train loss:  0.30878502130508423
train gradient:  0.1990216768844847
iteration : 7670
train acc:  0.8203125
train loss:  0.40576279163360596
train gradient:  0.28088426719303755
iteration : 7671
train acc:  0.8671875
train loss:  0.30230966210365295
train gradient:  0.2070695371118409
iteration : 7672
train acc:  0.828125
train loss:  0.3565686047077179
train gradient:  0.2288043339545442
iteration : 7673
train acc:  0.8203125
train loss:  0.3264404535293579
train gradient:  0.2671734780870928
iteration : 7674
train acc:  0.8671875
train loss:  0.3094906508922577
train gradient:  0.14085189466946324
iteration : 7675
train acc:  0.859375
train loss:  0.31372517347335815
train gradient:  0.23807794014996791
iteration : 7676
train acc:  0.9140625
train loss:  0.2465159296989441
train gradient:  0.11324338007164679
iteration : 7677
train acc:  0.859375
train loss:  0.33034801483154297
train gradient:  0.17607255097557997
iteration : 7678
train acc:  0.8828125
train loss:  0.3274153470993042
train gradient:  0.194724125128522
iteration : 7679
train acc:  0.875
train loss:  0.3100496530532837
train gradient:  0.21483651328803788
iteration : 7680
train acc:  0.8671875
train loss:  0.2959296703338623
train gradient:  0.22334669625264153
iteration : 7681
train acc:  0.8046875
train loss:  0.459525465965271
train gradient:  0.47258435536520915
iteration : 7682
train acc:  0.828125
train loss:  0.37361621856689453
train gradient:  0.1933747399181584
iteration : 7683
train acc:  0.84375
train loss:  0.3750079274177551
train gradient:  0.217925323584787
iteration : 7684
train acc:  0.859375
train loss:  0.3665848970413208
train gradient:  0.23656926356477423
iteration : 7685
train acc:  0.8984375
train loss:  0.2942102253437042
train gradient:  0.19305983283763645
iteration : 7686
train acc:  0.828125
train loss:  0.3907511234283447
train gradient:  0.2643050370190524
iteration : 7687
train acc:  0.8515625
train loss:  0.35367169976234436
train gradient:  0.24465101396261757
iteration : 7688
train acc:  0.859375
train loss:  0.39145249128341675
train gradient:  0.2413691964851129
iteration : 7689
train acc:  0.8359375
train loss:  0.39934951066970825
train gradient:  0.2925462869853618
iteration : 7690
train acc:  0.8671875
train loss:  0.3310964107513428
train gradient:  0.17382632507437185
iteration : 7691
train acc:  0.859375
train loss:  0.43235108256340027
train gradient:  0.4493241098500976
iteration : 7692
train acc:  0.828125
train loss:  0.3428359925746918
train gradient:  0.2685071474046564
iteration : 7693
train acc:  0.8671875
train loss:  0.27354928851127625
train gradient:  0.19968530244932634
iteration : 7694
train acc:  0.8046875
train loss:  0.3761897087097168
train gradient:  0.2856220070661223
iteration : 7695
train acc:  0.8046875
train loss:  0.34786003828048706
train gradient:  0.2549894029905154
iteration : 7696
train acc:  0.859375
train loss:  0.28766635060310364
train gradient:  0.13209931462563324
iteration : 7697
train acc:  0.875
train loss:  0.28835734724998474
train gradient:  0.1453684810323394
iteration : 7698
train acc:  0.84375
train loss:  0.3565593659877777
train gradient:  0.2913202571894274
iteration : 7699
train acc:  0.8359375
train loss:  0.3739878535270691
train gradient:  0.31043730475407805
iteration : 7700
train acc:  0.796875
train loss:  0.39014995098114014
train gradient:  0.26457508726618556
iteration : 7701
train acc:  0.8828125
train loss:  0.2802746593952179
train gradient:  0.16889628762381834
iteration : 7702
train acc:  0.84375
train loss:  0.30649030208587646
train gradient:  0.1610215393067906
iteration : 7703
train acc:  0.8828125
train loss:  0.2643114924430847
train gradient:  0.1451171005756887
iteration : 7704
train acc:  0.859375
train loss:  0.3526455760002136
train gradient:  0.1810075939526412
iteration : 7705
train acc:  0.8359375
train loss:  0.3588319718837738
train gradient:  0.2579151742895781
iteration : 7706
train acc:  0.8984375
train loss:  0.24925073981285095
train gradient:  0.14301839848194497
iteration : 7707
train acc:  0.8671875
train loss:  0.30503806471824646
train gradient:  0.1903907879789461
iteration : 7708
train acc:  0.8359375
train loss:  0.34036630392074585
train gradient:  0.1800407238076726
iteration : 7709
train acc:  0.859375
train loss:  0.37300774455070496
train gradient:  0.20995240287035555
iteration : 7710
train acc:  0.890625
train loss:  0.33290761709213257
train gradient:  0.23646503914056077
iteration : 7711
train acc:  0.8515625
train loss:  0.3672184348106384
train gradient:  0.24329942801478582
iteration : 7712
train acc:  0.859375
train loss:  0.29702699184417725
train gradient:  0.17126333023251636
iteration : 7713
train acc:  0.828125
train loss:  0.38064926862716675
train gradient:  0.2732751211346412
iteration : 7714
train acc:  0.859375
train loss:  0.32436928153038025
train gradient:  0.18550209821721939
iteration : 7715
train acc:  0.90625
train loss:  0.2878863215446472
train gradient:  0.18651866103246034
iteration : 7716
train acc:  0.8828125
train loss:  0.3097688555717468
train gradient:  0.24512056609704774
iteration : 7717
train acc:  0.8828125
train loss:  0.2939109802246094
train gradient:  0.18343087010386075
iteration : 7718
train acc:  0.8828125
train loss:  0.29362422227859497
train gradient:  0.15735966455870573
iteration : 7719
train acc:  0.84375
train loss:  0.3999885320663452
train gradient:  0.30609016679087026
iteration : 7720
train acc:  0.90625
train loss:  0.2938794195652008
train gradient:  0.1358727694926106
iteration : 7721
train acc:  0.8515625
train loss:  0.29401665925979614
train gradient:  0.18510251254394927
iteration : 7722
train acc:  0.828125
train loss:  0.39171236753463745
train gradient:  0.23223479539064118
iteration : 7723
train acc:  0.8671875
train loss:  0.3130337595939636
train gradient:  0.229493062468842
iteration : 7724
train acc:  0.859375
train loss:  0.3176267147064209
train gradient:  0.22641217845764494
iteration : 7725
train acc:  0.9296875
train loss:  0.23254816234111786
train gradient:  0.11867900514097285
iteration : 7726
train acc:  0.828125
train loss:  0.37297964096069336
train gradient:  0.1844049313730674
iteration : 7727
train acc:  0.84375
train loss:  0.38244301080703735
train gradient:  0.3617722337992685
iteration : 7728
train acc:  0.8125
train loss:  0.3756917119026184
train gradient:  0.23708661040714132
iteration : 7729
train acc:  0.859375
train loss:  0.32641762495040894
train gradient:  0.2833367713417356
iteration : 7730
train acc:  0.8515625
train loss:  0.33035826683044434
train gradient:  0.20937213792181142
iteration : 7731
train acc:  0.84375
train loss:  0.31639590859413147
train gradient:  0.19297017124469468
iteration : 7732
train acc:  0.8515625
train loss:  0.3915698528289795
train gradient:  0.21178574338151343
iteration : 7733
train acc:  0.8828125
train loss:  0.2741437554359436
train gradient:  0.14131916449625812
iteration : 7734
train acc:  0.8671875
train loss:  0.3188285827636719
train gradient:  0.21556659293420208
iteration : 7735
train acc:  0.8671875
train loss:  0.36195096373558044
train gradient:  0.2202210065413865
iteration : 7736
train acc:  0.9296875
train loss:  0.19840505719184875
train gradient:  0.10800093545955253
iteration : 7737
train acc:  0.8515625
train loss:  0.30786916613578796
train gradient:  0.30219844518341304
iteration : 7738
train acc:  0.828125
train loss:  0.30526769161224365
train gradient:  0.14679867168996
iteration : 7739
train acc:  0.8671875
train loss:  0.32018905878067017
train gradient:  0.20429933954749196
iteration : 7740
train acc:  0.8671875
train loss:  0.34391725063323975
train gradient:  0.23797322565784107
iteration : 7741
train acc:  0.8515625
train loss:  0.356659471988678
train gradient:  0.19753454504353699
iteration : 7742
train acc:  0.8125
train loss:  0.39195647835731506
train gradient:  0.20608431448689676
iteration : 7743
train acc:  0.84375
train loss:  0.32999032735824585
train gradient:  0.18077997672391105
iteration : 7744
train acc:  0.8125
train loss:  0.3492636978626251
train gradient:  0.2425490312675316
iteration : 7745
train acc:  0.859375
train loss:  0.29018962383270264
train gradient:  0.1806534697676822
iteration : 7746
train acc:  0.796875
train loss:  0.39075976610183716
train gradient:  0.3201217951693759
iteration : 7747
train acc:  0.90625
train loss:  0.265634149312973
train gradient:  0.1391650625174323
iteration : 7748
train acc:  0.8359375
train loss:  0.36899393796920776
train gradient:  0.2588018016455032
iteration : 7749
train acc:  0.84375
train loss:  0.34340962767601013
train gradient:  0.2639474769590691
iteration : 7750
train acc:  0.90625
train loss:  0.26348328590393066
train gradient:  0.2404303854181276
iteration : 7751
train acc:  0.828125
train loss:  0.32190343737602234
train gradient:  0.19066644147358502
iteration : 7752
train acc:  0.8515625
train loss:  0.34501147270202637
train gradient:  0.2299066104275293
iteration : 7753
train acc:  0.84375
train loss:  0.3824966549873352
train gradient:  0.2783537057588125
iteration : 7754
train acc:  0.796875
train loss:  0.4429014325141907
train gradient:  0.2570088633835675
iteration : 7755
train acc:  0.890625
train loss:  0.2786417305469513
train gradient:  0.1610981949924598
iteration : 7756
train acc:  0.8828125
train loss:  0.26987192034721375
train gradient:  0.1673385041580362
iteration : 7757
train acc:  0.8125
train loss:  0.3768400251865387
train gradient:  0.2224433067994676
iteration : 7758
train acc:  0.8515625
train loss:  0.38846397399902344
train gradient:  0.3271165409952657
iteration : 7759
train acc:  0.859375
train loss:  0.2984331548213959
train gradient:  0.14784488241225435
iteration : 7760
train acc:  0.8828125
train loss:  0.30512934923171997
train gradient:  0.1457995184568954
iteration : 7761
train acc:  0.8984375
train loss:  0.2783515453338623
train gradient:  0.15109738474009227
iteration : 7762
train acc:  0.890625
train loss:  0.2882111072540283
train gradient:  0.1836315304242926
iteration : 7763
train acc:  0.890625
train loss:  0.25427573919296265
train gradient:  0.15825634199222102
iteration : 7764
train acc:  0.859375
train loss:  0.36951619386672974
train gradient:  0.23865774722121477
iteration : 7765
train acc:  0.7890625
train loss:  0.36392438411712646
train gradient:  0.29958565679102
iteration : 7766
train acc:  0.8359375
train loss:  0.34837907552719116
train gradient:  0.1878809046691519
iteration : 7767
train acc:  0.8125
train loss:  0.3292190432548523
train gradient:  0.2599580717089357
iteration : 7768
train acc:  0.859375
train loss:  0.3909560441970825
train gradient:  0.2631336778068758
iteration : 7769
train acc:  0.8125
train loss:  0.3994792103767395
train gradient:  0.3062068477055996
iteration : 7770
train acc:  0.8671875
train loss:  0.2612496018409729
train gradient:  0.13966196373224232
iteration : 7771
train acc:  0.8359375
train loss:  0.3685797452926636
train gradient:  0.20788680211917804
iteration : 7772
train acc:  0.8984375
train loss:  0.29345089197158813
train gradient:  0.18391412742297197
iteration : 7773
train acc:  0.8359375
train loss:  0.34490904211997986
train gradient:  0.2550108079113666
iteration : 7774
train acc:  0.875
train loss:  0.27187472581863403
train gradient:  0.15572161374754812
iteration : 7775
train acc:  0.8671875
train loss:  0.29011619091033936
train gradient:  0.14456101648426123
iteration : 7776
train acc:  0.875
train loss:  0.2988893389701843
train gradient:  0.21636834011781794
iteration : 7777
train acc:  0.875
train loss:  0.3729831874370575
train gradient:  0.19514522205594603
iteration : 7778
train acc:  0.8671875
train loss:  0.2771171033382416
train gradient:  0.15367061604737303
iteration : 7779
train acc:  0.8515625
train loss:  0.34931397438049316
train gradient:  0.24663333027536688
iteration : 7780
train acc:  0.8671875
train loss:  0.28406646847724915
train gradient:  0.1626354269788359
iteration : 7781
train acc:  0.859375
train loss:  0.3671506941318512
train gradient:  0.2308700096137623
iteration : 7782
train acc:  0.8359375
train loss:  0.37336060404777527
train gradient:  0.40077174678224425
iteration : 7783
train acc:  0.8515625
train loss:  0.3047941327095032
train gradient:  0.1791671342595955
iteration : 7784
train acc:  0.8046875
train loss:  0.47095850110054016
train gradient:  0.33382071981759015
iteration : 7785
train acc:  0.8671875
train loss:  0.32292333245277405
train gradient:  0.4374896854983925
iteration : 7786
train acc:  0.8046875
train loss:  0.3434239625930786
train gradient:  0.3418283080183966
iteration : 7787
train acc:  0.8203125
train loss:  0.40832412242889404
train gradient:  0.2565727496190735
iteration : 7788
train acc:  0.890625
train loss:  0.27385109663009644
train gradient:  0.18301246300667362
iteration : 7789
train acc:  0.84375
train loss:  0.3700818717479706
train gradient:  0.3249813440043401
iteration : 7790
train acc:  0.8359375
train loss:  0.37304767966270447
train gradient:  0.2428771249021941
iteration : 7791
train acc:  0.84375
train loss:  0.3552056550979614
train gradient:  0.28514705744227503
iteration : 7792
train acc:  0.8671875
train loss:  0.3170865774154663
train gradient:  0.2111092397250413
iteration : 7793
train acc:  0.8828125
train loss:  0.268767774105072
train gradient:  0.157674110428398
iteration : 7794
train acc:  0.8671875
train loss:  0.3106241822242737
train gradient:  0.14596616211178615
iteration : 7795
train acc:  0.796875
train loss:  0.3753219246864319
train gradient:  0.4040880889398049
iteration : 7796
train acc:  0.8203125
train loss:  0.3931731581687927
train gradient:  0.32727644985359566
iteration : 7797
train acc:  0.8359375
train loss:  0.4066832661628723
train gradient:  0.278289692965845
iteration : 7798
train acc:  0.8203125
train loss:  0.3308408856391907
train gradient:  0.3095136072063639
iteration : 7799
train acc:  0.875
train loss:  0.3563929498195648
train gradient:  0.16147680944760312
iteration : 7800
train acc:  0.890625
train loss:  0.28121763467788696
train gradient:  0.13448111684256553
iteration : 7801
train acc:  0.8671875
train loss:  0.2972242534160614
train gradient:  0.15013891087965206
iteration : 7802
train acc:  0.8203125
train loss:  0.3710283935070038
train gradient:  0.21793510972467578
iteration : 7803
train acc:  0.8828125
train loss:  0.3008331060409546
train gradient:  0.1921013590521356
iteration : 7804
train acc:  0.8515625
train loss:  0.2676272392272949
train gradient:  0.14464017896553516
iteration : 7805
train acc:  0.8671875
train loss:  0.2896542549133301
train gradient:  0.11724424816686814
iteration : 7806
train acc:  0.8515625
train loss:  0.3196040987968445
train gradient:  0.2269369741358834
iteration : 7807
train acc:  0.84375
train loss:  0.29820311069488525
train gradient:  0.20945524226458434
iteration : 7808
train acc:  0.8828125
train loss:  0.28205588459968567
train gradient:  0.16721196622270895
iteration : 7809
train acc:  0.8359375
train loss:  0.4121810495853424
train gradient:  0.265292387044596
iteration : 7810
train acc:  0.859375
train loss:  0.32495421171188354
train gradient:  0.16216407047584497
iteration : 7811
train acc:  0.8046875
train loss:  0.3638397753238678
train gradient:  0.21542013545934088
iteration : 7812
train acc:  0.875
train loss:  0.30580592155456543
train gradient:  0.15714877467141963
iteration : 7813
train acc:  0.84375
train loss:  0.34293460845947266
train gradient:  0.21994158606904488
iteration : 7814
train acc:  0.828125
train loss:  0.3612927198410034
train gradient:  0.2733478481131698
iteration : 7815
train acc:  0.84375
train loss:  0.31661415100097656
train gradient:  0.15280442852021542
iteration : 7816
train acc:  0.875
train loss:  0.31901121139526367
train gradient:  0.13347156448332437
iteration : 7817
train acc:  0.890625
train loss:  0.3193037509918213
train gradient:  0.1597178405732167
iteration : 7818
train acc:  0.8125
train loss:  0.3617466688156128
train gradient:  0.21420987041066708
iteration : 7819
train acc:  0.859375
train loss:  0.3864603340625763
train gradient:  0.4238439229223664
iteration : 7820
train acc:  0.859375
train loss:  0.29959338903427124
train gradient:  0.18366004516473533
iteration : 7821
train acc:  0.828125
train loss:  0.3344416320323944
train gradient:  0.31734118491971447
iteration : 7822
train acc:  0.8671875
train loss:  0.25273293256759644
train gradient:  0.16713863073495333
iteration : 7823
train acc:  0.8125
train loss:  0.4036308526992798
train gradient:  0.26622085560640124
iteration : 7824
train acc:  0.859375
train loss:  0.33219093084335327
train gradient:  0.15745749131077286
iteration : 7825
train acc:  0.8125
train loss:  0.35496556758880615
train gradient:  0.2342427974508419
iteration : 7826
train acc:  0.8359375
train loss:  0.34111326932907104
train gradient:  0.21664328823818085
iteration : 7827
train acc:  0.828125
train loss:  0.4437282085418701
train gradient:  0.2995674903650479
iteration : 7828
train acc:  0.84375
train loss:  0.34440335631370544
train gradient:  0.24957620645970952
iteration : 7829
train acc:  0.859375
train loss:  0.33850616216659546
train gradient:  0.17124587276634956
iteration : 7830
train acc:  0.84375
train loss:  0.3968512713909149
train gradient:  0.2672977455775081
iteration : 7831
train acc:  0.8984375
train loss:  0.2657467722892761
train gradient:  0.22018670877228871
iteration : 7832
train acc:  0.84375
train loss:  0.38890236616134644
train gradient:  0.25967511221365475
iteration : 7833
train acc:  0.7890625
train loss:  0.49946171045303345
train gradient:  0.4715039006622475
iteration : 7834
train acc:  0.875
train loss:  0.31318527460098267
train gradient:  0.17308082756470977
iteration : 7835
train acc:  0.7734375
train loss:  0.44053396582603455
train gradient:  0.3177106365133526
iteration : 7836
train acc:  0.828125
train loss:  0.3413184583187103
train gradient:  0.1831622322949298
iteration : 7837
train acc:  0.8671875
train loss:  0.3299362063407898
train gradient:  0.1914010760472712
iteration : 7838
train acc:  0.8046875
train loss:  0.3827444314956665
train gradient:  0.22959732641546537
iteration : 7839
train acc:  0.8359375
train loss:  0.3465927243232727
train gradient:  0.24525141402163247
iteration : 7840
train acc:  0.8125
train loss:  0.35629600286483765
train gradient:  0.20381844746706182
iteration : 7841
train acc:  0.8671875
train loss:  0.3041696548461914
train gradient:  0.16639688843148887
iteration : 7842
train acc:  0.875
train loss:  0.3035953640937805
train gradient:  0.19987105170749286
iteration : 7843
train acc:  0.859375
train loss:  0.30861708521842957
train gradient:  0.15957217570873466
iteration : 7844
train acc:  0.828125
train loss:  0.306552529335022
train gradient:  0.1591403222136985
iteration : 7845
train acc:  0.8203125
train loss:  0.37895768880844116
train gradient:  0.3107478022692019
iteration : 7846
train acc:  0.8359375
train loss:  0.34504204988479614
train gradient:  0.1517302482053793
iteration : 7847
train acc:  0.828125
train loss:  0.35999631881713867
train gradient:  0.22200059711772055
iteration : 7848
train acc:  0.8359375
train loss:  0.4205074608325958
train gradient:  0.33502973997030505
iteration : 7849
train acc:  0.875
train loss:  0.32989269495010376
train gradient:  0.3155343237582717
iteration : 7850
train acc:  0.84375
train loss:  0.33739688992500305
train gradient:  0.24535980687642142
iteration : 7851
train acc:  0.8671875
train loss:  0.35875463485717773
train gradient:  0.30830592882430463
iteration : 7852
train acc:  0.84375
train loss:  0.3943319320678711
train gradient:  0.20059231415345463
iteration : 7853
train acc:  0.8515625
train loss:  0.3815600872039795
train gradient:  0.21513839613797747
iteration : 7854
train acc:  0.78125
train loss:  0.4663291573524475
train gradient:  0.3071287886946386
iteration : 7855
train acc:  0.84375
train loss:  0.34985852241516113
train gradient:  0.18285328154104946
iteration : 7856
train acc:  0.8671875
train loss:  0.2819138467311859
train gradient:  0.15015594742869684
iteration : 7857
train acc:  0.890625
train loss:  0.2972770035266876
train gradient:  0.1468759857761951
iteration : 7858
train acc:  0.890625
train loss:  0.34156617522239685
train gradient:  0.31208059580091774
iteration : 7859
train acc:  0.875
train loss:  0.3661179542541504
train gradient:  0.1810669396175657
iteration : 7860
train acc:  0.875
train loss:  0.2972046732902527
train gradient:  0.21171157510576413
iteration : 7861
train acc:  0.921875
train loss:  0.23324400186538696
train gradient:  0.12989404121060327
iteration : 7862
train acc:  0.8046875
train loss:  0.3659289479255676
train gradient:  0.21336213581501667
iteration : 7863
train acc:  0.7734375
train loss:  0.44176173210144043
train gradient:  0.265950196430372
iteration : 7864
train acc:  0.8203125
train loss:  0.3601953983306885
train gradient:  0.2116777252917846
iteration : 7865
train acc:  0.890625
train loss:  0.2854890823364258
train gradient:  0.17620950682680864
iteration : 7866
train acc:  0.84375
train loss:  0.32359278202056885
train gradient:  0.16867976383121955
iteration : 7867
train acc:  0.8671875
train loss:  0.3067980706691742
train gradient:  0.15278131332772327
iteration : 7868
train acc:  0.859375
train loss:  0.3215588331222534
train gradient:  0.17364978836064685
iteration : 7869
train acc:  0.8046875
train loss:  0.4358108937740326
train gradient:  0.24680188727292426
iteration : 7870
train acc:  0.875
train loss:  0.2879590690135956
train gradient:  0.09371814386821131
iteration : 7871
train acc:  0.828125
train loss:  0.3732394874095917
train gradient:  0.15362985205899107
iteration : 7872
train acc:  0.828125
train loss:  0.3803803324699402
train gradient:  0.2173112508166353
iteration : 7873
train acc:  0.859375
train loss:  0.33788228034973145
train gradient:  0.19318999816185395
iteration : 7874
train acc:  0.8671875
train loss:  0.26766687631607056
train gradient:  0.1416029970824931
iteration : 7875
train acc:  0.8671875
train loss:  0.3080296218395233
train gradient:  0.21874322239159494
iteration : 7876
train acc:  0.8359375
train loss:  0.39022037386894226
train gradient:  0.33601153478512275
iteration : 7877
train acc:  0.8515625
train loss:  0.33364585041999817
train gradient:  0.15314517070479242
iteration : 7878
train acc:  0.9140625
train loss:  0.22928735613822937
train gradient:  0.09721718389010156
iteration : 7879
train acc:  0.7890625
train loss:  0.457338809967041
train gradient:  0.35519790358290565
iteration : 7880
train acc:  0.8828125
train loss:  0.2742958068847656
train gradient:  0.14430412832482845
iteration : 7881
train acc:  0.8203125
train loss:  0.3389335870742798
train gradient:  0.21128953023829278
iteration : 7882
train acc:  0.8828125
train loss:  0.30241498351097107
train gradient:  0.13526399972268044
iteration : 7883
train acc:  0.90625
train loss:  0.27530986070632935
train gradient:  0.18401624067283112
iteration : 7884
train acc:  0.796875
train loss:  0.46963316202163696
train gradient:  0.350357362115681
iteration : 7885
train acc:  0.84375
train loss:  0.33943426609039307
train gradient:  0.2322463035444104
iteration : 7886
train acc:  0.828125
train loss:  0.3520885705947876
train gradient:  0.19253783996627796
iteration : 7887
train acc:  0.875
train loss:  0.3025364875793457
train gradient:  0.21197206912099115
iteration : 7888
train acc:  0.8359375
train loss:  0.3503389060497284
train gradient:  0.23279304585251387
iteration : 7889
train acc:  0.828125
train loss:  0.3881211280822754
train gradient:  0.2722025353798284
iteration : 7890
train acc:  0.78125
train loss:  0.38046714663505554
train gradient:  0.23957703263029143
iteration : 7891
train acc:  0.8515625
train loss:  0.32830721139907837
train gradient:  0.17086406210957616
iteration : 7892
train acc:  0.84375
train loss:  0.3247981369495392
train gradient:  0.18922110036836853
iteration : 7893
train acc:  0.8671875
train loss:  0.2976263761520386
train gradient:  0.10985590974749052
iteration : 7894
train acc:  0.859375
train loss:  0.3088005781173706
train gradient:  0.17285130431763757
iteration : 7895
train acc:  0.890625
train loss:  0.31044501066207886
train gradient:  0.25412171849469317
iteration : 7896
train acc:  0.8515625
train loss:  0.34535151720046997
train gradient:  0.1958840471509266
iteration : 7897
train acc:  0.7578125
train loss:  0.49585556983947754
train gradient:  0.38361329015185613
iteration : 7898
train acc:  0.8515625
train loss:  0.34652525186538696
train gradient:  0.17741634692787123
iteration : 7899
train acc:  0.8515625
train loss:  0.3290891647338867
train gradient:  0.18933249253848555
iteration : 7900
train acc:  0.921875
train loss:  0.25331342220306396
train gradient:  0.18287881489787897
iteration : 7901
train acc:  0.890625
train loss:  0.3365461826324463
train gradient:  0.18148733225441877
iteration : 7902
train acc:  0.8125
train loss:  0.37884360551834106
train gradient:  0.2257628377490206
iteration : 7903
train acc:  0.8359375
train loss:  0.33426719903945923
train gradient:  0.18124402791891103
iteration : 7904
train acc:  0.859375
train loss:  0.31964677572250366
train gradient:  0.1888739085345193
iteration : 7905
train acc:  0.8515625
train loss:  0.3256404995918274
train gradient:  0.14926177675219726
iteration : 7906
train acc:  0.890625
train loss:  0.2562182545661926
train gradient:  0.11882815271081464
iteration : 7907
train acc:  0.8046875
train loss:  0.42189890146255493
train gradient:  0.26008620349969186
iteration : 7908
train acc:  0.84375
train loss:  0.35017669200897217
train gradient:  0.2052419324853958
iteration : 7909
train acc:  0.8984375
train loss:  0.2580856680870056
train gradient:  0.12946419897290204
iteration : 7910
train acc:  0.8359375
train loss:  0.33149123191833496
train gradient:  0.18382134499590516
iteration : 7911
train acc:  0.84375
train loss:  0.37441471219062805
train gradient:  0.20292393417533822
iteration : 7912
train acc:  0.875
train loss:  0.3262600302696228
train gradient:  0.14852791442450722
iteration : 7913
train acc:  0.8828125
train loss:  0.28505539894104004
train gradient:  0.17627175172085752
iteration : 7914
train acc:  0.78125
train loss:  0.40875282883644104
train gradient:  0.2389274977251285
iteration : 7915
train acc:  0.859375
train loss:  0.32588881254196167
train gradient:  0.2092812015992122
iteration : 7916
train acc:  0.9140625
train loss:  0.24246490001678467
train gradient:  0.11134972501156988
iteration : 7917
train acc:  0.8203125
train loss:  0.43635302782058716
train gradient:  0.2519672067894414
iteration : 7918
train acc:  0.765625
train loss:  0.4254756569862366
train gradient:  0.4145793234887599
iteration : 7919
train acc:  0.796875
train loss:  0.4865167737007141
train gradient:  0.44034734186406216
iteration : 7920
train acc:  0.84375
train loss:  0.35116860270500183
train gradient:  0.20855187917085186
iteration : 7921
train acc:  0.8828125
train loss:  0.2790767550468445
train gradient:  0.1669888592192756
iteration : 7922
train acc:  0.8828125
train loss:  0.28172028064727783
train gradient:  0.14232773857426936
iteration : 7923
train acc:  0.8828125
train loss:  0.32706841826438904
train gradient:  0.2076831638141409
iteration : 7924
train acc:  0.828125
train loss:  0.3838500380516052
train gradient:  0.22576855247634492
iteration : 7925
train acc:  0.8671875
train loss:  0.3324744999408722
train gradient:  0.24679834180427784
iteration : 7926
train acc:  0.8359375
train loss:  0.361225962638855
train gradient:  0.22619357070472873
iteration : 7927
train acc:  0.8359375
train loss:  0.30867987871170044
train gradient:  0.1828022665976759
iteration : 7928
train acc:  0.859375
train loss:  0.3353430926799774
train gradient:  0.22570602397449535
iteration : 7929
train acc:  0.875
train loss:  0.3386862576007843
train gradient:  0.157980837046965
iteration : 7930
train acc:  0.84375
train loss:  0.3431227207183838
train gradient:  0.2197402966439346
iteration : 7931
train acc:  0.828125
train loss:  0.319448322057724
train gradient:  0.17422552154793577
iteration : 7932
train acc:  0.890625
train loss:  0.2512063980102539
train gradient:  0.17571244418203985
iteration : 7933
train acc:  0.875
train loss:  0.3129575252532959
train gradient:  0.1855797339180042
iteration : 7934
train acc:  0.8125
train loss:  0.40289193391799927
train gradient:  0.20537739751127781
iteration : 7935
train acc:  0.84375
train loss:  0.3696161210536957
train gradient:  0.2573388607453559
iteration : 7936
train acc:  0.875
train loss:  0.30566972494125366
train gradient:  0.22031742247765024
iteration : 7937
train acc:  0.8671875
train loss:  0.2797918915748596
train gradient:  0.18372036601108147
iteration : 7938
train acc:  0.8203125
train loss:  0.4595668315887451
train gradient:  0.3265277212314337
iteration : 7939
train acc:  0.78125
train loss:  0.4404798150062561
train gradient:  0.26823117591328294
iteration : 7940
train acc:  0.828125
train loss:  0.43372952938079834
train gradient:  0.23248431843062156
iteration : 7941
train acc:  0.90625
train loss:  0.24640950560569763
train gradient:  0.12743386602377635
iteration : 7942
train acc:  0.890625
train loss:  0.25342658162117004
train gradient:  0.11647526794670314
iteration : 7943
train acc:  0.84375
train loss:  0.35781413316726685
train gradient:  0.1655318417428197
iteration : 7944
train acc:  0.890625
train loss:  0.28167420625686646
train gradient:  0.15434023490744408
iteration : 7945
train acc:  0.859375
train loss:  0.32318735122680664
train gradient:  0.1841224350894395
iteration : 7946
train acc:  0.8046875
train loss:  0.41329294443130493
train gradient:  0.3738489845002568
iteration : 7947
train acc:  0.8515625
train loss:  0.4001457691192627
train gradient:  0.2700067175315648
iteration : 7948
train acc:  0.828125
train loss:  0.40129369497299194
train gradient:  0.19511917994280945
iteration : 7949
train acc:  0.8515625
train loss:  0.3326960802078247
train gradient:  0.14031093784420529
iteration : 7950
train acc:  0.78125
train loss:  0.4314771592617035
train gradient:  0.32782705464777395
iteration : 7951
train acc:  0.90625
train loss:  0.2799208462238312
train gradient:  0.15697236463643285
iteration : 7952
train acc:  0.8203125
train loss:  0.3545365035533905
train gradient:  0.21356161831189335
iteration : 7953
train acc:  0.890625
train loss:  0.32640230655670166
train gradient:  0.14809743873668205
iteration : 7954
train acc:  0.8359375
train loss:  0.37350714206695557
train gradient:  0.22138321861249943
iteration : 7955
train acc:  0.828125
train loss:  0.3743917942047119
train gradient:  0.2949882692146335
iteration : 7956
train acc:  0.8359375
train loss:  0.34802430868148804
train gradient:  0.14566251344358871
iteration : 7957
train acc:  0.8125
train loss:  0.4785417318344116
train gradient:  0.2895895533966262
iteration : 7958
train acc:  0.8984375
train loss:  0.28891152143478394
train gradient:  0.14403397629385023
iteration : 7959
train acc:  0.84375
train loss:  0.3408787250518799
train gradient:  0.1721467974091518
iteration : 7960
train acc:  0.90625
train loss:  0.2572671175003052
train gradient:  0.10977099834311502
iteration : 7961
train acc:  0.8515625
train loss:  0.35550570487976074
train gradient:  0.16397702417013252
iteration : 7962
train acc:  0.7890625
train loss:  0.44565391540527344
train gradient:  0.3363271904303092
iteration : 7963
train acc:  0.859375
train loss:  0.31329697370529175
train gradient:  0.16457283479373566
iteration : 7964
train acc:  0.859375
train loss:  0.3954302966594696
train gradient:  0.25957668781007626
iteration : 7965
train acc:  0.8046875
train loss:  0.37470877170562744
train gradient:  0.19551514447828036
iteration : 7966
train acc:  0.8828125
train loss:  0.3310820460319519
train gradient:  0.1451851870408323
iteration : 7967
train acc:  0.828125
train loss:  0.38039928674697876
train gradient:  0.20612287190703377
iteration : 7968
train acc:  0.8828125
train loss:  0.28512802720069885
train gradient:  0.11842622882159004
iteration : 7969
train acc:  0.859375
train loss:  0.32322946190834045
train gradient:  0.15369452898137997
iteration : 7970
train acc:  0.8671875
train loss:  0.3024240732192993
train gradient:  0.11842314166991597
iteration : 7971
train acc:  0.890625
train loss:  0.3366544246673584
train gradient:  0.14877096016755828
iteration : 7972
train acc:  0.84375
train loss:  0.3329547345638275
train gradient:  0.19445923047330863
iteration : 7973
train acc:  0.8359375
train loss:  0.3538707494735718
train gradient:  0.17097830344037035
iteration : 7974
train acc:  0.8984375
train loss:  0.2594788670539856
train gradient:  0.1644992880666174
iteration : 7975
train acc:  0.8671875
train loss:  0.3181363344192505
train gradient:  0.1404403899416043
iteration : 7976
train acc:  0.8203125
train loss:  0.3442261219024658
train gradient:  0.2415164640715533
iteration : 7977
train acc:  0.8671875
train loss:  0.34387850761413574
train gradient:  0.22417917100012133
iteration : 7978
train acc:  0.828125
train loss:  0.3518480658531189
train gradient:  0.21666288366784872
iteration : 7979
train acc:  0.8359375
train loss:  0.3457791209220886
train gradient:  0.2597654294535835
iteration : 7980
train acc:  0.8359375
train loss:  0.35646843910217285
train gradient:  0.220063029227833
iteration : 7981
train acc:  0.8515625
train loss:  0.3326803743839264
train gradient:  0.22670290664932902
iteration : 7982
train acc:  0.921875
train loss:  0.23747995495796204
train gradient:  0.09213468507700348
iteration : 7983
train acc:  0.8984375
train loss:  0.26104167103767395
train gradient:  0.13687587435775228
iteration : 7984
train acc:  0.8984375
train loss:  0.2573036849498749
train gradient:  0.10994441829284873
iteration : 7985
train acc:  0.875
train loss:  0.2949036955833435
train gradient:  0.14356313484970318
iteration : 7986
train acc:  0.8828125
train loss:  0.2857964336872101
train gradient:  0.18358779896914887
iteration : 7987
train acc:  0.796875
train loss:  0.426496684551239
train gradient:  0.4293524491229449
iteration : 7988
train acc:  0.90625
train loss:  0.250672310590744
train gradient:  0.1269621015391118
iteration : 7989
train acc:  0.859375
train loss:  0.33732956647872925
train gradient:  0.13997777053593513
iteration : 7990
train acc:  0.828125
train loss:  0.41065534949302673
train gradient:  0.2873651413412823
iteration : 7991
train acc:  0.8515625
train loss:  0.3619769811630249
train gradient:  0.2292803245104574
iteration : 7992
train acc:  0.8359375
train loss:  0.3949021100997925
train gradient:  0.22546904824339603
iteration : 7993
train acc:  0.84375
train loss:  0.33948132395744324
train gradient:  0.17726760864765728
iteration : 7994
train acc:  0.84375
train loss:  0.3354074954986572
train gradient:  0.21206911004363085
iteration : 7995
train acc:  0.8828125
train loss:  0.29325246810913086
train gradient:  0.18534283912891866
iteration : 7996
train acc:  0.84375
train loss:  0.30619126558303833
train gradient:  0.20582998417957993
iteration : 7997
train acc:  0.84375
train loss:  0.32020318508148193
train gradient:  0.15172738523757912
iteration : 7998
train acc:  0.875
train loss:  0.3326336741447449
train gradient:  0.18208066401734108
iteration : 7999
train acc:  0.8359375
train loss:  0.3425810635089874
train gradient:  0.18852036241377318
iteration : 8000
train acc:  0.7734375
train loss:  0.4913052022457123
train gradient:  0.32583976341445126
iteration : 8001
train acc:  0.890625
train loss:  0.3212573230266571
train gradient:  0.13526304371320896
iteration : 8002
train acc:  0.875
train loss:  0.298450767993927
train gradient:  0.1511860035590366
iteration : 8003
train acc:  0.8359375
train loss:  0.3409793972969055
train gradient:  0.20858857414145085
iteration : 8004
train acc:  0.8125
train loss:  0.3303481936454773
train gradient:  0.2534575051025095
iteration : 8005
train acc:  0.8359375
train loss:  0.40656691789627075
train gradient:  0.240363423294899
iteration : 8006
train acc:  0.90625
train loss:  0.25559377670288086
train gradient:  0.07675165605875146
iteration : 8007
train acc:  0.875
train loss:  0.2956429123878479
train gradient:  0.1541789839328761
iteration : 8008
train acc:  0.859375
train loss:  0.3402577042579651
train gradient:  0.1884925802823281
iteration : 8009
train acc:  0.796875
train loss:  0.43400514125823975
train gradient:  0.24036240783345642
iteration : 8010
train acc:  0.859375
train loss:  0.3098703920841217
train gradient:  0.14979961641262501
iteration : 8011
train acc:  0.890625
train loss:  0.2598002552986145
train gradient:  0.12535048100875823
iteration : 8012
train acc:  0.8203125
train loss:  0.4061589539051056
train gradient:  0.23997061741597625
iteration : 8013
train acc:  0.859375
train loss:  0.333074688911438
train gradient:  0.19526096839354168
iteration : 8014
train acc:  0.8203125
train loss:  0.35530856251716614
train gradient:  0.18206107166919938
iteration : 8015
train acc:  0.875
train loss:  0.27859318256378174
train gradient:  0.11810364255161339
iteration : 8016
train acc:  0.890625
train loss:  0.323117196559906
train gradient:  0.15407692061231237
iteration : 8017
train acc:  0.8125
train loss:  0.36847570538520813
train gradient:  0.30316520916656
iteration : 8018
train acc:  0.8671875
train loss:  0.3197024464607239
train gradient:  0.12562051315453768
iteration : 8019
train acc:  0.8828125
train loss:  0.26733818650245667
train gradient:  0.11260389239760041
iteration : 8020
train acc:  0.8359375
train loss:  0.37835556268692017
train gradient:  0.23207978719545397
iteration : 8021
train acc:  0.828125
train loss:  0.35005366802215576
train gradient:  0.2002933247771555
iteration : 8022
train acc:  0.8515625
train loss:  0.3627375364303589
train gradient:  0.27525427984394557
iteration : 8023
train acc:  0.8671875
train loss:  0.3240794837474823
train gradient:  0.21195458919008353
iteration : 8024
train acc:  0.8359375
train loss:  0.368513822555542
train gradient:  0.1925651828587917
iteration : 8025
train acc:  0.9140625
train loss:  0.2522153854370117
train gradient:  0.11693288772052263
iteration : 8026
train acc:  0.8359375
train loss:  0.33617866039276123
train gradient:  0.23726524328885262
iteration : 8027
train acc:  0.8671875
train loss:  0.2957724928855896
train gradient:  0.13149157690973695
iteration : 8028
train acc:  0.859375
train loss:  0.3652024269104004
train gradient:  0.19516331679304966
iteration : 8029
train acc:  0.84375
train loss:  0.3663918077945709
train gradient:  0.16258244875226255
iteration : 8030
train acc:  0.9140625
train loss:  0.23260557651519775
train gradient:  0.10619185011470447
iteration : 8031
train acc:  0.8671875
train loss:  0.31982892751693726
train gradient:  0.1398612179531323
iteration : 8032
train acc:  0.8046875
train loss:  0.4200374484062195
train gradient:  0.31365095887907274
iteration : 8033
train acc:  0.828125
train loss:  0.4010260999202728
train gradient:  0.3160423122342997
iteration : 8034
train acc:  0.828125
train loss:  0.3497813045978546
train gradient:  0.16907186960738774
iteration : 8035
train acc:  0.7890625
train loss:  0.44226759672164917
train gradient:  0.3439731103183951
iteration : 8036
train acc:  0.8671875
train loss:  0.3582271635532379
train gradient:  0.23898152394680128
iteration : 8037
train acc:  0.875
train loss:  0.3597185015678406
train gradient:  0.24467978689544856
iteration : 8038
train acc:  0.84375
train loss:  0.34608548879623413
train gradient:  0.22332382751010516
iteration : 8039
train acc:  0.84375
train loss:  0.4014158546924591
train gradient:  0.29728023079568694
iteration : 8040
train acc:  0.84375
train loss:  0.29189354181289673
train gradient:  0.13432443320044538
iteration : 8041
train acc:  0.875
train loss:  0.3293931484222412
train gradient:  0.15331876218803414
iteration : 8042
train acc:  0.875
train loss:  0.27635133266448975
train gradient:  0.14274489359333162
iteration : 8043
train acc:  0.84375
train loss:  0.314161479473114
train gradient:  0.19653081246603293
iteration : 8044
train acc:  0.8203125
train loss:  0.3649597465991974
train gradient:  0.21659877483628637
iteration : 8045
train acc:  0.8359375
train loss:  0.40063121914863586
train gradient:  0.23072507812406573
iteration : 8046
train acc:  0.890625
train loss:  0.28743404150009155
train gradient:  0.1824301303889892
iteration : 8047
train acc:  0.8359375
train loss:  0.35933104157447815
train gradient:  0.21550974265910855
iteration : 8048
train acc:  0.8984375
train loss:  0.2513166666030884
train gradient:  0.13818151114495883
iteration : 8049
train acc:  0.8671875
train loss:  0.30820411443710327
train gradient:  0.17836964416608264
iteration : 8050
train acc:  0.8984375
train loss:  0.25828737020492554
train gradient:  0.12536600682645155
iteration : 8051
train acc:  0.8828125
train loss:  0.28629568219184875
train gradient:  0.21780531258013638
iteration : 8052
train acc:  0.890625
train loss:  0.31149375438690186
train gradient:  0.20448181108943725
iteration : 8053
train acc:  0.8671875
train loss:  0.35391247272491455
train gradient:  0.22326360189747388
iteration : 8054
train acc:  0.859375
train loss:  0.35824787616729736
train gradient:  0.22599542939073125
iteration : 8055
train acc:  0.8359375
train loss:  0.3344709277153015
train gradient:  0.1700898234848286
iteration : 8056
train acc:  0.84375
train loss:  0.3591587543487549
train gradient:  0.13449326688984725
iteration : 8057
train acc:  0.8671875
train loss:  0.32328519225120544
train gradient:  0.17935153668507076
iteration : 8058
train acc:  0.8515625
train loss:  0.3248194456100464
train gradient:  0.13954707170048453
iteration : 8059
train acc:  0.84375
train loss:  0.40178102254867554
train gradient:  0.23169119693989637
iteration : 8060
train acc:  0.8671875
train loss:  0.33896249532699585
train gradient:  0.2446016069592697
iteration : 8061
train acc:  0.828125
train loss:  0.34131544828414917
train gradient:  0.17735312350385954
iteration : 8062
train acc:  0.8828125
train loss:  0.3399084806442261
train gradient:  0.1934560774007448
iteration : 8063
train acc:  0.859375
train loss:  0.3481196165084839
train gradient:  0.20177303966941518
iteration : 8064
train acc:  0.8515625
train loss:  0.3184923231601715
train gradient:  0.15279165125973992
iteration : 8065
train acc:  0.8828125
train loss:  0.27382391691207886
train gradient:  0.1228214969468657
iteration : 8066
train acc:  0.8125
train loss:  0.3839122951030731
train gradient:  0.273097836444114
iteration : 8067
train acc:  0.8125
train loss:  0.3913388252258301
train gradient:  0.28052518961998
iteration : 8068
train acc:  0.8984375
train loss:  0.32231393456459045
train gradient:  0.22153998358228705
iteration : 8069
train acc:  0.8515625
train loss:  0.3347173035144806
train gradient:  0.2698374306229796
iteration : 8070
train acc:  0.9296875
train loss:  0.23785746097564697
train gradient:  0.09874496038618971
iteration : 8071
train acc:  0.90625
train loss:  0.2606881260871887
train gradient:  0.1369101447447366
iteration : 8072
train acc:  0.859375
train loss:  0.37660378217697144
train gradient:  0.22869873032255134
iteration : 8073
train acc:  0.8359375
train loss:  0.317291796207428
train gradient:  0.2006541131385059
iteration : 8074
train acc:  0.9140625
train loss:  0.2584429979324341
train gradient:  0.09380895432057729
iteration : 8075
train acc:  0.84375
train loss:  0.33980169892311096
train gradient:  0.17974630579961728
iteration : 8076
train acc:  0.90625
train loss:  0.24987094104290009
train gradient:  0.11296256076386868
iteration : 8077
train acc:  0.8515625
train loss:  0.3675648272037506
train gradient:  0.239472494089306
iteration : 8078
train acc:  0.8515625
train loss:  0.3072914481163025
train gradient:  0.19399928150466395
iteration : 8079
train acc:  0.8359375
train loss:  0.3411857485771179
train gradient:  0.19144756018284587
iteration : 8080
train acc:  0.875
train loss:  0.3225927948951721
train gradient:  0.1594441837206893
iteration : 8081
train acc:  0.859375
train loss:  0.3193936347961426
train gradient:  0.3804264096859549
iteration : 8082
train acc:  0.8359375
train loss:  0.34451282024383545
train gradient:  0.24586523873112567
iteration : 8083
train acc:  0.8828125
train loss:  0.3497787117958069
train gradient:  0.271090700386444
iteration : 8084
train acc:  0.8828125
train loss:  0.3082449436187744
train gradient:  0.22795215893223755
iteration : 8085
train acc:  0.8125
train loss:  0.3647879362106323
train gradient:  0.23407092121160994
iteration : 8086
train acc:  0.859375
train loss:  0.3109036684036255
train gradient:  0.24481799054209485
iteration : 8087
train acc:  0.8125
train loss:  0.4179738163948059
train gradient:  0.26354467623682526
iteration : 8088
train acc:  0.8203125
train loss:  0.33072420954704285
train gradient:  0.1486326763213483
iteration : 8089
train acc:  0.84375
train loss:  0.3551815152168274
train gradient:  0.252494236481588
iteration : 8090
train acc:  0.890625
train loss:  0.3202015161514282
train gradient:  0.15464638089525393
iteration : 8091
train acc:  0.8828125
train loss:  0.2933036684989929
train gradient:  0.1723325201833809
iteration : 8092
train acc:  0.828125
train loss:  0.33560049533843994
train gradient:  0.1741260548643939
iteration : 8093
train acc:  0.8125
train loss:  0.3871142268180847
train gradient:  0.21299210199350604
iteration : 8094
train acc:  0.8359375
train loss:  0.35151129961013794
train gradient:  0.27908710566024525
iteration : 8095
train acc:  0.828125
train loss:  0.3865562081336975
train gradient:  0.23635496275147394
iteration : 8096
train acc:  0.8359375
train loss:  0.3649479150772095
train gradient:  0.24870946014179934
iteration : 8097
train acc:  0.8515625
train loss:  0.3132719397544861
train gradient:  0.37778355247110146
iteration : 8098
train acc:  0.890625
train loss:  0.31981080770492554
train gradient:  0.1663911835202897
iteration : 8099
train acc:  0.8984375
train loss:  0.3229084014892578
train gradient:  0.13571909054743161
iteration : 8100
train acc:  0.8671875
train loss:  0.2841114401817322
train gradient:  0.23286104161881516
iteration : 8101
train acc:  0.9140625
train loss:  0.21976007521152496
train gradient:  0.11129079496909863
iteration : 8102
train acc:  0.8359375
train loss:  0.4186016917228699
train gradient:  0.26543638494550553
iteration : 8103
train acc:  0.8203125
train loss:  0.3252713680267334
train gradient:  0.21970803994880467
iteration : 8104
train acc:  0.8359375
train loss:  0.36372995376586914
train gradient:  0.2142176946480111
iteration : 8105
train acc:  0.828125
train loss:  0.3664975166320801
train gradient:  0.2729435191792942
iteration : 8106
train acc:  0.8515625
train loss:  0.2853553891181946
train gradient:  0.13560304523512695
iteration : 8107
train acc:  0.796875
train loss:  0.35180386900901794
train gradient:  0.20736449672936685
iteration : 8108
train acc:  0.890625
train loss:  0.31581348180770874
train gradient:  0.16687150844000165
iteration : 8109
train acc:  0.859375
train loss:  0.3611181974411011
train gradient:  0.19977026595938358
iteration : 8110
train acc:  0.8828125
train loss:  0.33911705017089844
train gradient:  0.18130967644281512
iteration : 8111
train acc:  0.8515625
train loss:  0.35847336053848267
train gradient:  0.21478767231328327
iteration : 8112
train acc:  0.796875
train loss:  0.36607903242111206
train gradient:  0.21334248363587488
iteration : 8113
train acc:  0.84375
train loss:  0.3702002167701721
train gradient:  0.1874134105673631
iteration : 8114
train acc:  0.890625
train loss:  0.308066189289093
train gradient:  0.15465885651147893
iteration : 8115
train acc:  0.90625
train loss:  0.2777813673019409
train gradient:  0.16594201210937468
iteration : 8116
train acc:  0.8671875
train loss:  0.35019123554229736
train gradient:  0.19168613219863323
iteration : 8117
train acc:  0.875
train loss:  0.30404117703437805
train gradient:  0.19346083289859572
iteration : 8118
train acc:  0.8203125
train loss:  0.44502848386764526
train gradient:  0.39782926543435526
iteration : 8119
train acc:  0.8515625
train loss:  0.34727901220321655
train gradient:  0.21444433717881783
iteration : 8120
train acc:  0.84375
train loss:  0.3279328942298889
train gradient:  0.16630713427690536
iteration : 8121
train acc:  0.7734375
train loss:  0.44294068217277527
train gradient:  0.34938519330965084
iteration : 8122
train acc:  0.875
train loss:  0.2865917980670929
train gradient:  0.12509230332232424
iteration : 8123
train acc:  0.84375
train loss:  0.33256012201309204
train gradient:  0.14230163278253016
iteration : 8124
train acc:  0.890625
train loss:  0.3420039117336273
train gradient:  0.20248851636023496
iteration : 8125
train acc:  0.8359375
train loss:  0.3626813292503357
train gradient:  0.2379196513163172
iteration : 8126
train acc:  0.8125
train loss:  0.3540383577346802
train gradient:  0.1698510640411882
iteration : 8127
train acc:  0.8671875
train loss:  0.3293938636779785
train gradient:  0.15108649344365574
iteration : 8128
train acc:  0.8671875
train loss:  0.3687896132469177
train gradient:  0.1905488886991768
iteration : 8129
train acc:  0.8125
train loss:  0.38682571053504944
train gradient:  0.24566346016726553
iteration : 8130
train acc:  0.8515625
train loss:  0.34404271841049194
train gradient:  0.23378822620740858
iteration : 8131
train acc:  0.8203125
train loss:  0.41948410868644714
train gradient:  0.22589207235601239
iteration : 8132
train acc:  0.921875
train loss:  0.22096945345401764
train gradient:  0.09208681978706885
iteration : 8133
train acc:  0.7890625
train loss:  0.38487300276756287
train gradient:  0.19995129401648315
iteration : 8134
train acc:  0.859375
train loss:  0.29477840662002563
train gradient:  0.17651642510946927
iteration : 8135
train acc:  0.796875
train loss:  0.381820410490036
train gradient:  0.1671733895911488
iteration : 8136
train acc:  0.875
train loss:  0.3183026909828186
train gradient:  0.15656980278015462
iteration : 8137
train acc:  0.84375
train loss:  0.3637404441833496
train gradient:  0.26135823796868185
iteration : 8138
train acc:  0.8359375
train loss:  0.351282000541687
train gradient:  0.17613175914664042
iteration : 8139
train acc:  0.859375
train loss:  0.29322439432144165
train gradient:  0.13951399580115703
iteration : 8140
train acc:  0.8671875
train loss:  0.31033796072006226
train gradient:  0.14860619017013227
iteration : 8141
train acc:  0.8828125
train loss:  0.2247636914253235
train gradient:  0.11678783455067762
iteration : 8142
train acc:  0.8515625
train loss:  0.4030911326408386
train gradient:  0.20320699414911392
iteration : 8143
train acc:  0.828125
train loss:  0.36892449855804443
train gradient:  0.24142939300576405
iteration : 8144
train acc:  0.859375
train loss:  0.26872718334198
train gradient:  0.1702472108035209
iteration : 8145
train acc:  0.859375
train loss:  0.3535475730895996
train gradient:  0.15593007801854888
iteration : 8146
train acc:  0.890625
train loss:  0.31142890453338623
train gradient:  0.18477238550378278
iteration : 8147
train acc:  0.90625
train loss:  0.24130386114120483
train gradient:  0.124450574409828
iteration : 8148
train acc:  0.8359375
train loss:  0.3758164048194885
train gradient:  0.22420057998404727
iteration : 8149
train acc:  0.8359375
train loss:  0.36733701825141907
train gradient:  0.1932333323097657
iteration : 8150
train acc:  0.859375
train loss:  0.39656341075897217
train gradient:  0.15942863253740241
iteration : 8151
train acc:  0.8828125
train loss:  0.26638662815093994
train gradient:  0.11194477110125435
iteration : 8152
train acc:  0.828125
train loss:  0.35031843185424805
train gradient:  0.18674827099813843
iteration : 8153
train acc:  0.8671875
train loss:  0.2916733920574188
train gradient:  0.1806429643983822
iteration : 8154
train acc:  0.796875
train loss:  0.3889543414115906
train gradient:  0.2696554280533338
iteration : 8155
train acc:  0.828125
train loss:  0.3700253367424011
train gradient:  0.1893443961964176
iteration : 8156
train acc:  0.8984375
train loss:  0.25194764137268066
train gradient:  0.10540935224887953
iteration : 8157
train acc:  0.765625
train loss:  0.4218822121620178
train gradient:  0.27724289991889317
iteration : 8158
train acc:  0.8359375
train loss:  0.4445990324020386
train gradient:  0.306068144977494
iteration : 8159
train acc:  0.8125
train loss:  0.38560956716537476
train gradient:  0.2908284681523464
iteration : 8160
train acc:  0.875
train loss:  0.32571130990982056
train gradient:  0.21486579193194458
iteration : 8161
train acc:  0.8671875
train loss:  0.29781460762023926
train gradient:  0.19175823284857874
iteration : 8162
train acc:  0.875
train loss:  0.31283849477767944
train gradient:  0.10573073042680807
iteration : 8163
train acc:  0.84375
train loss:  0.3243202865123749
train gradient:  0.1981521288214172
iteration : 8164
train acc:  0.9140625
train loss:  0.27121588587760925
train gradient:  0.12858827634290826
iteration : 8165
train acc:  0.8125
train loss:  0.4131903648376465
train gradient:  0.24595392722154483
iteration : 8166
train acc:  0.875
train loss:  0.3093869686126709
train gradient:  0.27384047383444166
iteration : 8167
train acc:  0.8515625
train loss:  0.3249633014202118
train gradient:  0.15041695251578374
iteration : 8168
train acc:  0.8828125
train loss:  0.29250332713127136
train gradient:  0.1433775370913441
iteration : 8169
train acc:  0.890625
train loss:  0.30147385597229004
train gradient:  0.20580055358859567
iteration : 8170
train acc:  0.8671875
train loss:  0.2978730797767639
train gradient:  0.21565631499982235
iteration : 8171
train acc:  0.890625
train loss:  0.33274683356285095
train gradient:  0.16510989277832933
iteration : 8172
train acc:  0.84375
train loss:  0.326237291097641
train gradient:  0.18759476807393022
iteration : 8173
train acc:  0.84375
train loss:  0.3774466812610626
train gradient:  0.2940267798671505
iteration : 8174
train acc:  0.8515625
train loss:  0.3880532383918762
train gradient:  0.22041444537583388
iteration : 8175
train acc:  0.84375
train loss:  0.37273338437080383
train gradient:  0.2008085367631415
iteration : 8176
train acc:  0.8828125
train loss:  0.30722734332084656
train gradient:  0.20718403052605078
iteration : 8177
train acc:  0.8515625
train loss:  0.3258286714553833
train gradient:  0.21258518564575038
iteration : 8178
train acc:  0.875
train loss:  0.31176403164863586
train gradient:  0.17836551483070434
iteration : 8179
train acc:  0.8125
train loss:  0.3569295406341553
train gradient:  0.5692265019850755
iteration : 8180
train acc:  0.828125
train loss:  0.3521389365196228
train gradient:  0.21057024397140994
iteration : 8181
train acc:  0.890625
train loss:  0.31103265285491943
train gradient:  0.1589973929081558
iteration : 8182
train acc:  0.796875
train loss:  0.396975576877594
train gradient:  0.23938375203622814
iteration : 8183
train acc:  0.8125
train loss:  0.38197195529937744
train gradient:  0.20929809863989768
iteration : 8184
train acc:  0.8828125
train loss:  0.2746657431125641
train gradient:  0.11905527938423083
iteration : 8185
train acc:  0.859375
train loss:  0.3281887173652649
train gradient:  0.250089711770021
iteration : 8186
train acc:  0.859375
train loss:  0.28784364461898804
train gradient:  0.13774443690710736
iteration : 8187
train acc:  0.890625
train loss:  0.27733877301216125
train gradient:  0.14659702951210474
iteration : 8188
train acc:  0.8671875
train loss:  0.3294799029827118
train gradient:  0.17538056245574413
iteration : 8189
train acc:  0.8828125
train loss:  0.3474242091178894
train gradient:  0.1705569856977031
iteration : 8190
train acc:  0.875
train loss:  0.3004325330257416
train gradient:  0.1905245894976277
iteration : 8191
train acc:  0.8515625
train loss:  0.2959192097187042
train gradient:  0.18935837096136124
iteration : 8192
train acc:  0.9140625
train loss:  0.2562301754951477
train gradient:  0.19627624816760664
iteration : 8193
train acc:  0.8671875
train loss:  0.34631961584091187
train gradient:  0.15611956493540377
iteration : 8194
train acc:  0.8671875
train loss:  0.36309513449668884
train gradient:  0.17955219128280436
iteration : 8195
train acc:  0.8671875
train loss:  0.351574182510376
train gradient:  0.1480721210539978
iteration : 8196
train acc:  0.8671875
train loss:  0.30907315015792847
train gradient:  0.18182152659529058
iteration : 8197
train acc:  0.8359375
train loss:  0.33058685064315796
train gradient:  0.18951811535622048
iteration : 8198
train acc:  0.84375
train loss:  0.29900017380714417
train gradient:  0.15975124137833807
iteration : 8199
train acc:  0.8125
train loss:  0.4043484628200531
train gradient:  0.3467904150278659
iteration : 8200
train acc:  0.78125
train loss:  0.41424405574798584
train gradient:  0.22806746571846587
iteration : 8201
train acc:  0.8359375
train loss:  0.38698825240135193
train gradient:  0.28595884873083666
iteration : 8202
train acc:  0.8671875
train loss:  0.3643949031829834
train gradient:  0.16478090350818012
iteration : 8203
train acc:  0.84375
train loss:  0.3246704041957855
train gradient:  0.187926504558498
iteration : 8204
train acc:  0.8359375
train loss:  0.3558732867240906
train gradient:  0.21948669439203655
iteration : 8205
train acc:  0.8125
train loss:  0.4205670654773712
train gradient:  0.28006156116747283
iteration : 8206
train acc:  0.828125
train loss:  0.36365973949432373
train gradient:  0.2245352398474429
iteration : 8207
train acc:  0.859375
train loss:  0.40291646122932434
train gradient:  0.2154496462737818
iteration : 8208
train acc:  0.859375
train loss:  0.3179526925086975
train gradient:  0.22285840406099452
iteration : 8209
train acc:  0.890625
train loss:  0.2743428349494934
train gradient:  0.13327159576737166
iteration : 8210
train acc:  0.828125
train loss:  0.3658519387245178
train gradient:  0.2968074262870851
iteration : 8211
train acc:  0.890625
train loss:  0.27756690979003906
train gradient:  0.13079531967853447
iteration : 8212
train acc:  0.796875
train loss:  0.35375356674194336
train gradient:  0.1611893850626508
iteration : 8213
train acc:  0.8828125
train loss:  0.29466408491134644
train gradient:  0.12892654312062624
iteration : 8214
train acc:  0.8671875
train loss:  0.3420788645744324
train gradient:  0.17617960608983163
iteration : 8215
train acc:  0.9296875
train loss:  0.21830089390277863
train gradient:  0.12806076796584712
iteration : 8216
train acc:  0.828125
train loss:  0.34579479694366455
train gradient:  0.19879959494074154
iteration : 8217
train acc:  0.8984375
train loss:  0.2847003936767578
train gradient:  0.20908635144157312
iteration : 8218
train acc:  0.8359375
train loss:  0.36989694833755493
train gradient:  0.22832874649236445
iteration : 8219
train acc:  0.8828125
train loss:  0.314912348985672
train gradient:  0.20961038021331313
iteration : 8220
train acc:  0.8515625
train loss:  0.2892664670944214
train gradient:  0.17539621753320483
iteration : 8221
train acc:  0.859375
train loss:  0.30845776200294495
train gradient:  0.22007331155915605
iteration : 8222
train acc:  0.859375
train loss:  0.33086901903152466
train gradient:  0.1869298765722846
iteration : 8223
train acc:  0.8203125
train loss:  0.4396616816520691
train gradient:  0.3144021481431032
iteration : 8224
train acc:  0.90625
train loss:  0.25994783639907837
train gradient:  0.13719032324773914
iteration : 8225
train acc:  0.84375
train loss:  0.32052624225616455
train gradient:  0.1649116346045215
iteration : 8226
train acc:  0.8203125
train loss:  0.4024916887283325
train gradient:  0.32432689095814077
iteration : 8227
train acc:  0.875
train loss:  0.3636840581893921
train gradient:  0.2581117533900452
iteration : 8228
train acc:  0.859375
train loss:  0.3639279007911682
train gradient:  0.20393322712641843
iteration : 8229
train acc:  0.828125
train loss:  0.3463436961174011
train gradient:  0.26067682769654477
iteration : 8230
train acc:  0.859375
train loss:  0.3527466654777527
train gradient:  0.2318882338726434
iteration : 8231
train acc:  0.828125
train loss:  0.38416945934295654
train gradient:  0.20285728834785766
iteration : 8232
train acc:  0.7890625
train loss:  0.4299312233924866
train gradient:  0.23320147951462603
iteration : 8233
train acc:  0.8828125
train loss:  0.306222528219223
train gradient:  0.1850865265791814
iteration : 8234
train acc:  0.921875
train loss:  0.25277242064476013
train gradient:  0.11362315621342897
iteration : 8235
train acc:  0.8515625
train loss:  0.3483206629753113
train gradient:  0.19166307622573323
iteration : 8236
train acc:  0.875
train loss:  0.28841742873191833
train gradient:  0.15182093396218183
iteration : 8237
train acc:  0.859375
train loss:  0.3323200047016144
train gradient:  0.33575752017402866
iteration : 8238
train acc:  0.8671875
train loss:  0.3937684893608093
train gradient:  0.22496093688660274
iteration : 8239
train acc:  0.8671875
train loss:  0.3141457438468933
train gradient:  0.14417927306913847
iteration : 8240
train acc:  0.8984375
train loss:  0.24641205370426178
train gradient:  0.15354499375137495
iteration : 8241
train acc:  0.7734375
train loss:  0.4597204327583313
train gradient:  0.28829568327938565
iteration : 8242
train acc:  0.8671875
train loss:  0.3090061545372009
train gradient:  0.17463629929994523
iteration : 8243
train acc:  0.84375
train loss:  0.34765589237213135
train gradient:  0.15388996305025887
iteration : 8244
train acc:  0.8828125
train loss:  0.27642276883125305
train gradient:  0.13606518158420108
iteration : 8245
train acc:  0.828125
train loss:  0.3634219467639923
train gradient:  0.21968041681320127
iteration : 8246
train acc:  0.8359375
train loss:  0.30566662549972534
train gradient:  0.12591505897923694
iteration : 8247
train acc:  0.828125
train loss:  0.3371254503726959
train gradient:  0.16191004048793528
iteration : 8248
train acc:  0.8515625
train loss:  0.2923377752304077
train gradient:  0.23301982618471467
iteration : 8249
train acc:  0.8828125
train loss:  0.3044678866863251
train gradient:  0.16496344758840037
iteration : 8250
train acc:  0.875
train loss:  0.3944319784641266
train gradient:  0.25142140687876846
iteration : 8251
train acc:  0.8359375
train loss:  0.4013538360595703
train gradient:  0.2270485483618147
iteration : 8252
train acc:  0.8984375
train loss:  0.29077380895614624
train gradient:  0.16944519950439105
iteration : 8253
train acc:  0.875
train loss:  0.2967146039009094
train gradient:  0.1662356355385578
iteration : 8254
train acc:  0.84375
train loss:  0.3514310121536255
train gradient:  0.15521619100842596
iteration : 8255
train acc:  0.8671875
train loss:  0.28782933950424194
train gradient:  0.14791683658474897
iteration : 8256
train acc:  0.8359375
train loss:  0.3621588349342346
train gradient:  0.2216814691010372
iteration : 8257
train acc:  0.828125
train loss:  0.3579397201538086
train gradient:  0.228639151776216
iteration : 8258
train acc:  0.8125
train loss:  0.3444502353668213
train gradient:  0.15648142939136003
iteration : 8259
train acc:  0.859375
train loss:  0.2905040979385376
train gradient:  0.27333937682444653
iteration : 8260
train acc:  0.90625
train loss:  0.2992146611213684
train gradient:  0.1540165293313862
iteration : 8261
train acc:  0.875
train loss:  0.24677345156669617
train gradient:  0.1182412314597956
iteration : 8262
train acc:  0.8515625
train loss:  0.3205692172050476
train gradient:  0.1672255919842701
iteration : 8263
train acc:  0.84375
train loss:  0.31959885358810425
train gradient:  0.14319928080975353
iteration : 8264
train acc:  0.8671875
train loss:  0.3207456171512604
train gradient:  0.22864761882317802
iteration : 8265
train acc:  0.9140625
train loss:  0.29598551988601685
train gradient:  0.14928309246182125
iteration : 8266
train acc:  0.8515625
train loss:  0.3544709086418152
train gradient:  0.1678728759268115
iteration : 8267
train acc:  0.875
train loss:  0.35719695687294006
train gradient:  0.2726727572235502
iteration : 8268
train acc:  0.859375
train loss:  0.31924793124198914
train gradient:  0.20120673038443232
iteration : 8269
train acc:  0.875
train loss:  0.3618050813674927
train gradient:  0.2594531328570358
iteration : 8270
train acc:  0.8046875
train loss:  0.38020384311676025
train gradient:  0.2918541001942812
iteration : 8271
train acc:  0.8359375
train loss:  0.34298980236053467
train gradient:  0.27310291842926937
iteration : 8272
train acc:  0.796875
train loss:  0.4071091413497925
train gradient:  0.2606224404527038
iteration : 8273
train acc:  0.921875
train loss:  0.28087741136550903
train gradient:  0.15682761440306514
iteration : 8274
train acc:  0.90625
train loss:  0.2780059576034546
train gradient:  0.15221517896044068
iteration : 8275
train acc:  0.859375
train loss:  0.30281028151512146
train gradient:  0.2986104536870472
iteration : 8276
train acc:  0.8359375
train loss:  0.3403499126434326
train gradient:  0.3999548736662526
iteration : 8277
train acc:  0.8671875
train loss:  0.2845626771450043
train gradient:  0.1741902185674667
iteration : 8278
train acc:  0.84375
train loss:  0.3272745609283447
train gradient:  0.2584498771069429
iteration : 8279
train acc:  0.875
train loss:  0.2991076111793518
train gradient:  0.18784000275591503
iteration : 8280
train acc:  0.796875
train loss:  0.45459839701652527
train gradient:  0.3585874023117963
iteration : 8281
train acc:  0.859375
train loss:  0.40311914682388306
train gradient:  0.26119116293890227
iteration : 8282
train acc:  0.875
train loss:  0.36978578567504883
train gradient:  0.3062601168669784
iteration : 8283
train acc:  0.90625
train loss:  0.22596150636672974
train gradient:  0.09738536439051308
iteration : 8284
train acc:  0.8984375
train loss:  0.2869822382926941
train gradient:  0.15593889843609599
iteration : 8285
train acc:  0.875
train loss:  0.27117443084716797
train gradient:  0.13233106901574332
iteration : 8286
train acc:  0.78125
train loss:  0.39753252267837524
train gradient:  0.2755882094407143
iteration : 8287
train acc:  0.8984375
train loss:  0.28207141160964966
train gradient:  0.1995675810063546
iteration : 8288
train acc:  0.8984375
train loss:  0.2522619962692261
train gradient:  0.11899913152888836
iteration : 8289
train acc:  0.8515625
train loss:  0.30489081144332886
train gradient:  0.20911691445641356
iteration : 8290
train acc:  0.84375
train loss:  0.34150803089141846
train gradient:  0.22437649388813227
iteration : 8291
train acc:  0.84375
train loss:  0.302151620388031
train gradient:  0.10839736350650833
iteration : 8292
train acc:  0.8671875
train loss:  0.2881699800491333
train gradient:  0.13272678328726634
iteration : 8293
train acc:  0.84375
train loss:  0.3769282102584839
train gradient:  0.23518546040261828
iteration : 8294
train acc:  0.8359375
train loss:  0.3092202842235565
train gradient:  0.13583221148901276
iteration : 8295
train acc:  0.859375
train loss:  0.331031858921051
train gradient:  0.2175144510845848
iteration : 8296
train acc:  0.828125
train loss:  0.350387841463089
train gradient:  0.1687941793773237
iteration : 8297
train acc:  0.8828125
train loss:  0.2889031767845154
train gradient:  0.12517571471761973
iteration : 8298
train acc:  0.875
train loss:  0.30847179889678955
train gradient:  0.27750246594822525
iteration : 8299
train acc:  0.875
train loss:  0.32636910676956177
train gradient:  0.20741556019325547
iteration : 8300
train acc:  0.75
train loss:  0.5175968408584595
train gradient:  0.37400444984166525
iteration : 8301
train acc:  0.859375
train loss:  0.3550269305706024
train gradient:  0.2035922843991475
iteration : 8302
train acc:  0.8671875
train loss:  0.32397353649139404
train gradient:  0.18870270706056297
iteration : 8303
train acc:  0.84375
train loss:  0.37794432044029236
train gradient:  0.20954194219184274
iteration : 8304
train acc:  0.84375
train loss:  0.31356281042099
train gradient:  0.16909915968826103
iteration : 8305
train acc:  0.8203125
train loss:  0.35687947273254395
train gradient:  0.2965146149581822
iteration : 8306
train acc:  0.8671875
train loss:  0.28945863246917725
train gradient:  0.14510558753202682
iteration : 8307
train acc:  0.8046875
train loss:  0.4048541784286499
train gradient:  0.2532728809481367
iteration : 8308
train acc:  0.8359375
train loss:  0.3768487572669983
train gradient:  0.18511495109578913
iteration : 8309
train acc:  0.8515625
train loss:  0.31999650597572327
train gradient:  0.20653682248341043
iteration : 8310
train acc:  0.8671875
train loss:  0.35242030024528503
train gradient:  0.24104438884129975
iteration : 8311
train acc:  0.8359375
train loss:  0.46035295724868774
train gradient:  0.3178643614183171
iteration : 8312
train acc:  0.8828125
train loss:  0.2556743621826172
train gradient:  0.13509984470178477
iteration : 8313
train acc:  0.890625
train loss:  0.2738078832626343
train gradient:  0.15953798416585607
iteration : 8314
train acc:  0.875
train loss:  0.29864078760147095
train gradient:  0.1549084167686278
iteration : 8315
train acc:  0.859375
train loss:  0.33784011006355286
train gradient:  0.1332236362743337
iteration : 8316
train acc:  0.8359375
train loss:  0.346861332654953
train gradient:  0.23478521689392473
iteration : 8317
train acc:  0.8671875
train loss:  0.3155835270881653
train gradient:  0.1888958728829911
iteration : 8318
train acc:  0.8515625
train loss:  0.3985922932624817
train gradient:  0.2056530757595763
iteration : 8319
train acc:  0.828125
train loss:  0.36539632081985474
train gradient:  0.23762228541634006
iteration : 8320
train acc:  0.828125
train loss:  0.35393112897872925
train gradient:  0.15956502633495617
iteration : 8321
train acc:  0.9140625
train loss:  0.2811335325241089
train gradient:  0.14942614955077338
iteration : 8322
train acc:  0.875
train loss:  0.29792430996894836
train gradient:  0.1578118333096157
iteration : 8323
train acc:  0.8125
train loss:  0.36570388078689575
train gradient:  0.16821395709806297
iteration : 8324
train acc:  0.8828125
train loss:  0.27375465631484985
train gradient:  0.13256800065264077
iteration : 8325
train acc:  0.8359375
train loss:  0.2976146936416626
train gradient:  0.25048237171645
iteration : 8326
train acc:  0.859375
train loss:  0.31902414560317993
train gradient:  0.15857919770718923
iteration : 8327
train acc:  0.8671875
train loss:  0.3620408773422241
train gradient:  0.2238743444245312
iteration : 8328
train acc:  0.84375
train loss:  0.35496270656585693
train gradient:  0.18650136372198398
iteration : 8329
train acc:  0.890625
train loss:  0.2844032049179077
train gradient:  0.12538504975736092
iteration : 8330
train acc:  0.859375
train loss:  0.32016313076019287
train gradient:  0.14558605059163335
iteration : 8331
train acc:  0.8515625
train loss:  0.3486226797103882
train gradient:  0.28045764241294935
iteration : 8332
train acc:  0.8125
train loss:  0.3490043878555298
train gradient:  0.15387703980509826
iteration : 8333
train acc:  0.859375
train loss:  0.2823650538921356
train gradient:  0.1501039999516442
iteration : 8334
train acc:  0.84375
train loss:  0.353393018245697
train gradient:  0.1582363403551526
iteration : 8335
train acc:  0.8515625
train loss:  0.32678425312042236
train gradient:  0.15904207792086653
iteration : 8336
train acc:  0.859375
train loss:  0.3184576630592346
train gradient:  0.4139630324214151
iteration : 8337
train acc:  0.8359375
train loss:  0.34802156686782837
train gradient:  0.17940947325322287
iteration : 8338
train acc:  0.8515625
train loss:  0.35283440351486206
train gradient:  0.22769610487382788
iteration : 8339
train acc:  0.8359375
train loss:  0.39479732513427734
train gradient:  0.2085866674272574
iteration : 8340
train acc:  0.875
train loss:  0.2673654556274414
train gradient:  0.16667648582119587
iteration : 8341
train acc:  0.875
train loss:  0.2933332324028015
train gradient:  0.15246851574681297
iteration : 8342
train acc:  0.8828125
train loss:  0.2726357579231262
train gradient:  0.1623040484314569
iteration : 8343
train acc:  0.8359375
train loss:  0.3106439709663391
train gradient:  0.1267917819957435
iteration : 8344
train acc:  0.875
train loss:  0.28812530636787415
train gradient:  0.15142980519092047
iteration : 8345
train acc:  0.84375
train loss:  0.3163681626319885
train gradient:  0.1917243771593039
iteration : 8346
train acc:  0.859375
train loss:  0.3319850265979767
train gradient:  0.22676877376849675
iteration : 8347
train acc:  0.84375
train loss:  0.3605566620826721
train gradient:  0.19228491947984933
iteration : 8348
train acc:  0.875
train loss:  0.3166690468788147
train gradient:  0.3258320227305017
iteration : 8349
train acc:  0.890625
train loss:  0.3153041899204254
train gradient:  0.19220944526321956
iteration : 8350
train acc:  0.8203125
train loss:  0.4271801710128784
train gradient:  0.25543765361635823
iteration : 8351
train acc:  0.8671875
train loss:  0.24466398358345032
train gradient:  0.12749523441871286
iteration : 8352
train acc:  0.859375
train loss:  0.3845115900039673
train gradient:  0.18506628531971436
iteration : 8353
train acc:  0.890625
train loss:  0.31375885009765625
train gradient:  0.1388901575828943
iteration : 8354
train acc:  0.890625
train loss:  0.3171226382255554
train gradient:  0.16136623345357648
iteration : 8355
train acc:  0.84375
train loss:  0.33030349016189575
train gradient:  0.1888457452318595
iteration : 8356
train acc:  0.859375
train loss:  0.35076195001602173
train gradient:  0.20786520359030686
iteration : 8357
train acc:  0.8359375
train loss:  0.35907620191574097
train gradient:  0.24931306406239379
iteration : 8358
train acc:  0.84375
train loss:  0.38011568784713745
train gradient:  0.21188123846890566
iteration : 8359
train acc:  0.8828125
train loss:  0.3065861463546753
train gradient:  0.284755480918965
iteration : 8360
train acc:  0.890625
train loss:  0.28382474184036255
train gradient:  0.19532560144958006
iteration : 8361
train acc:  0.859375
train loss:  0.30117911100387573
train gradient:  0.1825202704573466
iteration : 8362
train acc:  0.84375
train loss:  0.36147353053092957
train gradient:  0.3178002193413971
iteration : 8363
train acc:  0.875
train loss:  0.2957518696784973
train gradient:  0.1461541026074323
iteration : 8364
train acc:  0.8515625
train loss:  0.30543607473373413
train gradient:  0.14001134095518208
iteration : 8365
train acc:  0.8515625
train loss:  0.34441912174224854
train gradient:  0.21193723157451458
iteration : 8366
train acc:  0.8828125
train loss:  0.38543540239334106
train gradient:  0.2447980324429628
iteration : 8367
train acc:  0.9453125
train loss:  0.21679651737213135
train gradient:  0.16482447804612727
iteration : 8368
train acc:  0.8671875
train loss:  0.36303621530532837
train gradient:  0.26545405265642996
iteration : 8369
train acc:  0.8359375
train loss:  0.348782479763031
train gradient:  0.16490356772574055
iteration : 8370
train acc:  0.8359375
train loss:  0.3114912807941437
train gradient:  0.18617115068124154
iteration : 8371
train acc:  0.8828125
train loss:  0.29173755645751953
train gradient:  0.16732915051277306
iteration : 8372
train acc:  0.8203125
train loss:  0.4321499466896057
train gradient:  0.31381688738575075
iteration : 8373
train acc:  0.8515625
train loss:  0.37363654375076294
train gradient:  0.2854000883716999
iteration : 8374
train acc:  0.84375
train loss:  0.3373021185398102
train gradient:  0.16361291065005767
iteration : 8375
train acc:  0.8984375
train loss:  0.28386247158050537
train gradient:  0.1881492210671299
iteration : 8376
train acc:  0.8359375
train loss:  0.32128480076789856
train gradient:  0.17066675539262374
iteration : 8377
train acc:  0.8515625
train loss:  0.30964934825897217
train gradient:  0.22454554196797405
iteration : 8378
train acc:  0.875
train loss:  0.32858720421791077
train gradient:  0.2396952210770013
iteration : 8379
train acc:  0.859375
train loss:  0.3044703006744385
train gradient:  0.21731319958850864
iteration : 8380
train acc:  0.8515625
train loss:  0.36298879981040955
train gradient:  0.1718349325269109
iteration : 8381
train acc:  0.8515625
train loss:  0.3782975673675537
train gradient:  0.2358607102149063
iteration : 8382
train acc:  0.859375
train loss:  0.3290657699108124
train gradient:  0.20188570250042062
iteration : 8383
train acc:  0.859375
train loss:  0.33116674423217773
train gradient:  0.502631470100573
iteration : 8384
train acc:  0.890625
train loss:  0.2908058762550354
train gradient:  0.1623334955504051
iteration : 8385
train acc:  0.84375
train loss:  0.3430589437484741
train gradient:  0.20352550613242332
iteration : 8386
train acc:  0.875
train loss:  0.3943730890750885
train gradient:  0.1842244365774972
iteration : 8387
train acc:  0.84375
train loss:  0.31876879930496216
train gradient:  0.20415587304521027
iteration : 8388
train acc:  0.8203125
train loss:  0.3536490797996521
train gradient:  0.22076267982484465
iteration : 8389
train acc:  0.8125
train loss:  0.4460110366344452
train gradient:  0.376721349963367
iteration : 8390
train acc:  0.8125
train loss:  0.3577460050582886
train gradient:  0.17718791133707384
iteration : 8391
train acc:  0.78125
train loss:  0.47381114959716797
train gradient:  0.3164866898321032
iteration : 8392
train acc:  0.859375
train loss:  0.3102829158306122
train gradient:  0.1447392009145453
iteration : 8393
train acc:  0.8046875
train loss:  0.40776827931404114
train gradient:  0.29159845907152826
iteration : 8394
train acc:  0.8359375
train loss:  0.37439680099487305
train gradient:  0.18455602771694984
iteration : 8395
train acc:  0.8671875
train loss:  0.2818755507469177
train gradient:  0.11708776481973882
iteration : 8396
train acc:  0.8828125
train loss:  0.30019235610961914
train gradient:  0.13849759896923802
iteration : 8397
train acc:  0.859375
train loss:  0.34157663583755493
train gradient:  0.1926499905790332
iteration : 8398
train acc:  0.8671875
train loss:  0.26536399126052856
train gradient:  0.1192209635595198
iteration : 8399
train acc:  0.8515625
train loss:  0.35135525465011597
train gradient:  0.17311474180555844
iteration : 8400
train acc:  0.8203125
train loss:  0.4052770733833313
train gradient:  0.2022035407917841
iteration : 8401
train acc:  0.796875
train loss:  0.36695587635040283
train gradient:  0.23811738755908946
iteration : 8402
train acc:  0.84375
train loss:  0.32717975974082947
train gradient:  0.18896723229348925
iteration : 8403
train acc:  0.8515625
train loss:  0.36215388774871826
train gradient:  0.20725857103757475
iteration : 8404
train acc:  0.8671875
train loss:  0.2971605658531189
train gradient:  0.1489756939974513
iteration : 8405
train acc:  0.8359375
train loss:  0.3840559720993042
train gradient:  0.24257444263163722
iteration : 8406
train acc:  0.84375
train loss:  0.370517373085022
train gradient:  0.21936317074967981
iteration : 8407
train acc:  0.90625
train loss:  0.2238290160894394
train gradient:  0.12041790919186196
iteration : 8408
train acc:  0.84375
train loss:  0.4624403715133667
train gradient:  0.3601594961433788
iteration : 8409
train acc:  0.890625
train loss:  0.2687065005302429
train gradient:  0.15054638170276455
iteration : 8410
train acc:  0.8828125
train loss:  0.24308493733406067
train gradient:  0.10353897913306999
iteration : 8411
train acc:  0.84375
train loss:  0.3649700880050659
train gradient:  0.18948859540514285
iteration : 8412
train acc:  0.8515625
train loss:  0.31189894676208496
train gradient:  0.1612630680544532
iteration : 8413
train acc:  0.8203125
train loss:  0.3663269281387329
train gradient:  0.19936036832615694
iteration : 8414
train acc:  0.8046875
train loss:  0.44867995381355286
train gradient:  0.3243913978620791
iteration : 8415
train acc:  0.859375
train loss:  0.32469379901885986
train gradient:  0.1783408272505031
iteration : 8416
train acc:  0.859375
train loss:  0.3347279727458954
train gradient:  0.23471091896123006
iteration : 8417
train acc:  0.78125
train loss:  0.37969112396240234
train gradient:  0.212757077700485
iteration : 8418
train acc:  0.90625
train loss:  0.2459331899881363
train gradient:  0.11956612051556002
iteration : 8419
train acc:  0.828125
train loss:  0.3774067163467407
train gradient:  0.21535033076214438
iteration : 8420
train acc:  0.859375
train loss:  0.3049677312374115
train gradient:  0.16308697235501268
iteration : 8421
train acc:  0.7890625
train loss:  0.44190680980682373
train gradient:  0.283084315240025
iteration : 8422
train acc:  0.7890625
train loss:  0.44835102558135986
train gradient:  0.22388689951329255
iteration : 8423
train acc:  0.859375
train loss:  0.281597375869751
train gradient:  0.11920088956925173
iteration : 8424
train acc:  0.8515625
train loss:  0.3406275808811188
train gradient:  0.29253194171635766
iteration : 8425
train acc:  0.875
train loss:  0.35196125507354736
train gradient:  0.14640374838301842
iteration : 8426
train acc:  0.8828125
train loss:  0.28531986474990845
train gradient:  0.12459881650304173
iteration : 8427
train acc:  0.859375
train loss:  0.35578060150146484
train gradient:  0.16152442744264128
iteration : 8428
train acc:  0.828125
train loss:  0.34169188141822815
train gradient:  0.1750116885777382
iteration : 8429
train acc:  0.859375
train loss:  0.297460675239563
train gradient:  0.1348238361893518
iteration : 8430
train acc:  0.8671875
train loss:  0.32236191630363464
train gradient:  0.22643458521326099
iteration : 8431
train acc:  0.859375
train loss:  0.3829803466796875
train gradient:  0.2876557550541105
iteration : 8432
train acc:  0.875
train loss:  0.2791784405708313
train gradient:  0.17057749635977643
iteration : 8433
train acc:  0.8828125
train loss:  0.2830509841442108
train gradient:  0.1685310651605244
iteration : 8434
train acc:  0.921875
train loss:  0.26518821716308594
train gradient:  0.09314986350770893
iteration : 8435
train acc:  0.859375
train loss:  0.34112489223480225
train gradient:  0.18593787800317108
iteration : 8436
train acc:  0.7578125
train loss:  0.47435271739959717
train gradient:  0.3072621917992333
iteration : 8437
train acc:  0.8515625
train loss:  0.3507849872112274
train gradient:  0.2697600583814774
iteration : 8438
train acc:  0.875
train loss:  0.3257941007614136
train gradient:  0.2013987904188681
iteration : 8439
train acc:  0.90625
train loss:  0.24990946054458618
train gradient:  0.10353376693453421
iteration : 8440
train acc:  0.8125
train loss:  0.4112178683280945
train gradient:  0.316353172693056
iteration : 8441
train acc:  0.8515625
train loss:  0.30108535289764404
train gradient:  0.13810162494022454
iteration : 8442
train acc:  0.8515625
train loss:  0.3040657341480255
train gradient:  0.13448071545029078
iteration : 8443
train acc:  0.8359375
train loss:  0.38979533314704895
train gradient:  0.2855941424335272
iteration : 8444
train acc:  0.890625
train loss:  0.29292985796928406
train gradient:  0.13182600117951154
iteration : 8445
train acc:  0.828125
train loss:  0.3801532983779907
train gradient:  0.19379552195833707
iteration : 8446
train acc:  0.8046875
train loss:  0.45198875665664673
train gradient:  0.37068334409185677
iteration : 8447
train acc:  0.84375
train loss:  0.34940600395202637
train gradient:  0.14922693084037614
iteration : 8448
train acc:  0.875
train loss:  0.33196935057640076
train gradient:  0.2720284160692652
iteration : 8449
train acc:  0.8203125
train loss:  0.3968603014945984
train gradient:  0.3169508031765719
iteration : 8450
train acc:  0.8046875
train loss:  0.3372736871242523
train gradient:  0.2245199954860557
iteration : 8451
train acc:  0.8828125
train loss:  0.3730658292770386
train gradient:  0.19458258133437134
iteration : 8452
train acc:  0.8828125
train loss:  0.34748321771621704
train gradient:  0.24956687699342708
iteration : 8453
train acc:  0.8671875
train loss:  0.34778445959091187
train gradient:  0.16115379032328603
iteration : 8454
train acc:  0.8828125
train loss:  0.2975892722606659
train gradient:  0.12501512351970717
iteration : 8455
train acc:  0.8828125
train loss:  0.310610294342041
train gradient:  0.15174607387834962
iteration : 8456
train acc:  0.859375
train loss:  0.34210970997810364
train gradient:  0.14100399678882292
iteration : 8457
train acc:  0.7890625
train loss:  0.4353267550468445
train gradient:  0.26683340177464143
iteration : 8458
train acc:  0.8359375
train loss:  0.41996121406555176
train gradient:  0.23286640676906945
iteration : 8459
train acc:  0.84375
train loss:  0.32378774881362915
train gradient:  0.12860728429832285
iteration : 8460
train acc:  0.8515625
train loss:  0.33253031969070435
train gradient:  0.20203170297587353
iteration : 8461
train acc:  0.859375
train loss:  0.32471808791160583
train gradient:  0.20559663076332013
iteration : 8462
train acc:  0.828125
train loss:  0.35846078395843506
train gradient:  0.22690744566221743
iteration : 8463
train acc:  0.84375
train loss:  0.3022061586380005
train gradient:  0.14466135357821736
iteration : 8464
train acc:  0.8125
train loss:  0.39895468950271606
train gradient:  0.19285940795000708
iteration : 8465
train acc:  0.8359375
train loss:  0.36885523796081543
train gradient:  0.2323500963152525
iteration : 8466
train acc:  0.8359375
train loss:  0.38438040018081665
train gradient:  0.2096990626653789
iteration : 8467
train acc:  0.890625
train loss:  0.3289234936237335
train gradient:  0.22831312818675129
iteration : 8468
train acc:  0.828125
train loss:  0.3586636185646057
train gradient:  0.15661604047476366
iteration : 8469
train acc:  0.859375
train loss:  0.2999231219291687
train gradient:  0.12843464793045362
iteration : 8470
train acc:  0.8203125
train loss:  0.4032183289527893
train gradient:  0.18782554898695542
iteration : 8471
train acc:  0.8046875
train loss:  0.4259393811225891
train gradient:  0.2829194014002004
iteration : 8472
train acc:  0.875
train loss:  0.29280102252960205
train gradient:  0.15592932510764437
iteration : 8473
train acc:  0.84375
train loss:  0.37210994958877563
train gradient:  0.16193781941579277
iteration : 8474
train acc:  0.8359375
train loss:  0.31165611743927
train gradient:  0.1684939172327549
iteration : 8475
train acc:  0.8671875
train loss:  0.3206315338611603
train gradient:  0.1533516703631791
iteration : 8476
train acc:  0.84375
train loss:  0.4401649236679077
train gradient:  0.2409112172228114
iteration : 8477
train acc:  0.796875
train loss:  0.48398202657699585
train gradient:  0.34895333960672714
iteration : 8478
train acc:  0.78125
train loss:  0.4626515507698059
train gradient:  0.2116697881106656
iteration : 8479
train acc:  0.8828125
train loss:  0.29231229424476624
train gradient:  0.1529738048598019
iteration : 8480
train acc:  0.828125
train loss:  0.3361794650554657
train gradient:  0.18713341349119003
iteration : 8481
train acc:  0.8046875
train loss:  0.408083438873291
train gradient:  0.17270635039469556
iteration : 8482
train acc:  0.84375
train loss:  0.3332049250602722
train gradient:  0.1454683005904028
iteration : 8483
train acc:  0.8359375
train loss:  0.3266029357910156
train gradient:  0.17939312775459482
iteration : 8484
train acc:  0.859375
train loss:  0.31605738401412964
train gradient:  0.19067832673758356
iteration : 8485
train acc:  0.890625
train loss:  0.31139400601387024
train gradient:  0.10371819511412866
iteration : 8486
train acc:  0.890625
train loss:  0.2746390700340271
train gradient:  0.11067018400355404
iteration : 8487
train acc:  0.875
train loss:  0.32561179995536804
train gradient:  0.15837869246037145
iteration : 8488
train acc:  0.796875
train loss:  0.348433256149292
train gradient:  0.19292238361865638
iteration : 8489
train acc:  0.875
train loss:  0.2980663478374481
train gradient:  0.13817292953144134
iteration : 8490
train acc:  0.859375
train loss:  0.3417235016822815
train gradient:  0.17129457562465955
iteration : 8491
train acc:  0.8515625
train loss:  0.33710718154907227
train gradient:  0.20428336139173114
iteration : 8492
train acc:  0.8203125
train loss:  0.3950154185295105
train gradient:  0.211178129537615
iteration : 8493
train acc:  0.84375
train loss:  0.3748835325241089
train gradient:  0.19486391368832146
iteration : 8494
train acc:  0.8359375
train loss:  0.3965291976928711
train gradient:  0.24310142016211256
iteration : 8495
train acc:  0.8203125
train loss:  0.37501901388168335
train gradient:  0.16786029028024402
iteration : 8496
train acc:  0.8671875
train loss:  0.32813209295272827
train gradient:  0.14096063657855012
iteration : 8497
train acc:  0.8125
train loss:  0.4780280888080597
train gradient:  0.49062861531777485
iteration : 8498
train acc:  0.828125
train loss:  0.39116328954696655
train gradient:  0.26128187491519456
iteration : 8499
train acc:  0.84375
train loss:  0.35007381439208984
train gradient:  0.18042490722589663
iteration : 8500
train acc:  0.84375
train loss:  0.34744536876678467
train gradient:  0.16077918486736362
iteration : 8501
train acc:  0.859375
train loss:  0.2781358063220978
train gradient:  0.18808365928160664
iteration : 8502
train acc:  0.8671875
train loss:  0.3413831293582916
train gradient:  0.27046265854512175
iteration : 8503
train acc:  0.8671875
train loss:  0.27554112672805786
train gradient:  0.1450890436769815
iteration : 8504
train acc:  0.8515625
train loss:  0.28970807790756226
train gradient:  0.1664285679595737
iteration : 8505
train acc:  0.8203125
train loss:  0.37862277030944824
train gradient:  0.1586180370654813
iteration : 8506
train acc:  0.84375
train loss:  0.3632635474205017
train gradient:  0.2768624990052392
iteration : 8507
train acc:  0.84375
train loss:  0.319987416267395
train gradient:  0.16616557027988954
iteration : 8508
train acc:  0.8046875
train loss:  0.383125364780426
train gradient:  0.24113823147779506
iteration : 8509
train acc:  0.8671875
train loss:  0.28329914808273315
train gradient:  0.10176920490208889
iteration : 8510
train acc:  0.8671875
train loss:  0.32143184542655945
train gradient:  0.1666637761862497
iteration : 8511
train acc:  0.921875
train loss:  0.2933298945426941
train gradient:  0.12876096101844353
iteration : 8512
train acc:  0.9296875
train loss:  0.24624326825141907
train gradient:  0.1260301197976543
iteration : 8513
train acc:  0.8359375
train loss:  0.3286498486995697
train gradient:  0.13593370849782838
iteration : 8514
train acc:  0.8671875
train loss:  0.31411319971084595
train gradient:  0.11688609680428418
iteration : 8515
train acc:  0.796875
train loss:  0.4045138955116272
train gradient:  0.2203650675710347
iteration : 8516
train acc:  0.859375
train loss:  0.30325156450271606
train gradient:  0.14384963104114973
iteration : 8517
train acc:  0.828125
train loss:  0.3430473804473877
train gradient:  0.2109530792004291
iteration : 8518
train acc:  0.890625
train loss:  0.29403913021087646
train gradient:  0.12562198563197804
iteration : 8519
train acc:  0.78125
train loss:  0.40649232268333435
train gradient:  0.26074903010791034
iteration : 8520
train acc:  0.875
train loss:  0.3065364360809326
train gradient:  0.18008397981772498
iteration : 8521
train acc:  0.8515625
train loss:  0.35302096605300903
train gradient:  0.18834099474989385
iteration : 8522
train acc:  0.84375
train loss:  0.3221885561943054
train gradient:  0.14688551009201561
iteration : 8523
train acc:  0.875
train loss:  0.29590481519699097
train gradient:  0.21459942404201043
iteration : 8524
train acc:  0.8515625
train loss:  0.32463669776916504
train gradient:  0.17486967146442678
iteration : 8525
train acc:  0.7890625
train loss:  0.4610753059387207
train gradient:  0.3180549120567363
iteration : 8526
train acc:  0.875
train loss:  0.27984195947647095
train gradient:  0.1132816133425395
iteration : 8527
train acc:  0.8828125
train loss:  0.27076470851898193
train gradient:  0.15439501208620754
iteration : 8528
train acc:  0.890625
train loss:  0.2980878949165344
train gradient:  0.1410971816662581
iteration : 8529
train acc:  0.8359375
train loss:  0.3755902349948883
train gradient:  0.15569761972122026
iteration : 8530
train acc:  0.828125
train loss:  0.38730600476264954
train gradient:  0.23948130712347782
iteration : 8531
train acc:  0.890625
train loss:  0.28557318449020386
train gradient:  0.1204997213392404
iteration : 8532
train acc:  0.8515625
train loss:  0.3667560815811157
train gradient:  0.24476362739918822
iteration : 8533
train acc:  0.8046875
train loss:  0.3487813472747803
train gradient:  0.16875928122061873
iteration : 8534
train acc:  0.8828125
train loss:  0.2814137935638428
train gradient:  0.11560497990784879
iteration : 8535
train acc:  0.8515625
train loss:  0.39157092571258545
train gradient:  0.19293419021694455
iteration : 8536
train acc:  0.8515625
train loss:  0.304381787776947
train gradient:  0.14689416521633586
iteration : 8537
train acc:  0.859375
train loss:  0.35372787714004517
train gradient:  0.14519471249005345
iteration : 8538
train acc:  0.90625
train loss:  0.27661070227622986
train gradient:  0.13364014083753933
iteration : 8539
train acc:  0.84375
train loss:  0.35650235414505005
train gradient:  0.25416595188567825
iteration : 8540
train acc:  0.8515625
train loss:  0.34111660718917847
train gradient:  0.19990123181820307
iteration : 8541
train acc:  0.890625
train loss:  0.2565619945526123
train gradient:  0.10990547623605496
iteration : 8542
train acc:  0.796875
train loss:  0.3995118737220764
train gradient:  0.37623390760782804
iteration : 8543
train acc:  0.875
train loss:  0.2913879454135895
train gradient:  0.12864377941117305
iteration : 8544
train acc:  0.8984375
train loss:  0.28785189986228943
train gradient:  0.13654246305134005
iteration : 8545
train acc:  0.8359375
train loss:  0.3494568467140198
train gradient:  0.21115579129219997
iteration : 8546
train acc:  0.8359375
train loss:  0.350222110748291
train gradient:  0.19466697978277384
iteration : 8547
train acc:  0.90625
train loss:  0.24469339847564697
train gradient:  0.09893595798610134
iteration : 8548
train acc:  0.8828125
train loss:  0.2983439564704895
train gradient:  0.1371004659349604
iteration : 8549
train acc:  0.8515625
train loss:  0.31891632080078125
train gradient:  0.24617651323901535
iteration : 8550
train acc:  0.8671875
train loss:  0.27492648363113403
train gradient:  0.14957738043206367
iteration : 8551
train acc:  0.8671875
train loss:  0.34324967861175537
train gradient:  0.15873275289912553
iteration : 8552
train acc:  0.859375
train loss:  0.30308622121810913
train gradient:  0.18362240059273968
iteration : 8553
train acc:  0.875
train loss:  0.31753483414649963
train gradient:  0.1572723800935571
iteration : 8554
train acc:  0.875
train loss:  0.2669346332550049
train gradient:  0.12191161920817842
iteration : 8555
train acc:  0.8359375
train loss:  0.314837247133255
train gradient:  0.20668386215817897
iteration : 8556
train acc:  0.90625
train loss:  0.23464840650558472
train gradient:  0.07568669409646005
iteration : 8557
train acc:  0.8828125
train loss:  0.28431275486946106
train gradient:  0.18995289840481588
iteration : 8558
train acc:  0.890625
train loss:  0.31386396288871765
train gradient:  0.13511772026538996
iteration : 8559
train acc:  0.8984375
train loss:  0.25585317611694336
train gradient:  0.10900999123029197
iteration : 8560
train acc:  0.84375
train loss:  0.33976417779922485
train gradient:  0.29994841977255376
iteration : 8561
train acc:  0.8828125
train loss:  0.28207504749298096
train gradient:  0.14284669606500044
iteration : 8562
train acc:  0.859375
train loss:  0.36991971731185913
train gradient:  0.25180788919452096
iteration : 8563
train acc:  0.859375
train loss:  0.3647998571395874
train gradient:  0.17408922668788457
iteration : 8564
train acc:  0.90625
train loss:  0.2721131145954132
train gradient:  0.18039509301396253
iteration : 8565
train acc:  0.8671875
train loss:  0.3012954592704773
train gradient:  0.1534309458679957
iteration : 8566
train acc:  0.8046875
train loss:  0.38749638199806213
train gradient:  0.2661396743189821
iteration : 8567
train acc:  0.875
train loss:  0.36939406394958496
train gradient:  0.2324852978803644
iteration : 8568
train acc:  0.859375
train loss:  0.3457053303718567
train gradient:  0.22823693789621277
iteration : 8569
train acc:  0.859375
train loss:  0.30879053473472595
train gradient:  0.17387110658798371
iteration : 8570
train acc:  0.8828125
train loss:  0.30146265029907227
train gradient:  0.19602275384917733
iteration : 8571
train acc:  0.859375
train loss:  0.3817335367202759
train gradient:  0.2687257984916793
iteration : 8572
train acc:  0.8828125
train loss:  0.3364384174346924
train gradient:  0.22676485048712616
iteration : 8573
train acc:  0.8125
train loss:  0.3524557054042816
train gradient:  0.20098837684329532
iteration : 8574
train acc:  0.796875
train loss:  0.3701794445514679
train gradient:  0.20458416561750498
iteration : 8575
train acc:  0.859375
train loss:  0.37774235010147095
train gradient:  0.30312114636561727
iteration : 8576
train acc:  0.8359375
train loss:  0.35010862350463867
train gradient:  0.2205601161311163
iteration : 8577
train acc:  0.8515625
train loss:  0.28418880701065063
train gradient:  0.14393776234626005
iteration : 8578
train acc:  0.859375
train loss:  0.3247219920158386
train gradient:  0.18890063074410385
iteration : 8579
train acc:  0.8359375
train loss:  0.35245662927627563
train gradient:  0.2477204437610426
iteration : 8580
train acc:  0.875
train loss:  0.3358396291732788
train gradient:  0.1526705305816444
iteration : 8581
train acc:  0.8671875
train loss:  0.3185962438583374
train gradient:  0.19791618357214602
iteration : 8582
train acc:  0.796875
train loss:  0.36248576641082764
train gradient:  0.26148361271170106
iteration : 8583
train acc:  0.8828125
train loss:  0.3098987936973572
train gradient:  0.1883820289042807
iteration : 8584
train acc:  0.8671875
train loss:  0.26977989077568054
train gradient:  0.1609577345844319
iteration : 8585
train acc:  0.8125
train loss:  0.4111117124557495
train gradient:  0.2442972454156214
iteration : 8586
train acc:  0.8203125
train loss:  0.3770855665206909
train gradient:  0.19785337507314887
iteration : 8587
train acc:  0.828125
train loss:  0.31031614542007446
train gradient:  0.20196253206647147
iteration : 8588
train acc:  0.8984375
train loss:  0.2799571454524994
train gradient:  0.14683821289618065
iteration : 8589
train acc:  0.8125
train loss:  0.40390729904174805
train gradient:  0.2547065543671341
iteration : 8590
train acc:  0.84375
train loss:  0.3303995132446289
train gradient:  0.19962788166473866
iteration : 8591
train acc:  0.890625
train loss:  0.3588107228279114
train gradient:  0.20476487927661935
iteration : 8592
train acc:  0.84375
train loss:  0.38852760195732117
train gradient:  0.27136316465533494
iteration : 8593
train acc:  0.875
train loss:  0.3319229483604431
train gradient:  0.16119087330340398
iteration : 8594
train acc:  0.8359375
train loss:  0.33878108859062195
train gradient:  0.1581546264813425
iteration : 8595
train acc:  0.875
train loss:  0.27752596139907837
train gradient:  0.13447726363997425
iteration : 8596
train acc:  0.7890625
train loss:  0.4358687996864319
train gradient:  0.22419494646595586
iteration : 8597
train acc:  0.8515625
train loss:  0.3602394461631775
train gradient:  0.1805461677840869
iteration : 8598
train acc:  0.84375
train loss:  0.3357224464416504
train gradient:  0.14416462187574136
iteration : 8599
train acc:  0.90625
train loss:  0.29290130734443665
train gradient:  0.13067946640231154
iteration : 8600
train acc:  0.8125
train loss:  0.39061060547828674
train gradient:  0.3086636723611186
iteration : 8601
train acc:  0.9140625
train loss:  0.319600909948349
train gradient:  0.11236392024555378
iteration : 8602
train acc:  0.8515625
train loss:  0.32850009202957153
train gradient:  0.25445835215577645
iteration : 8603
train acc:  0.90625
train loss:  0.299243688583374
train gradient:  0.11482073646969847
iteration : 8604
train acc:  0.84375
train loss:  0.2959369719028473
train gradient:  0.27018719640934136
iteration : 8605
train acc:  0.8515625
train loss:  0.3859751522541046
train gradient:  0.251912582634513
iteration : 8606
train acc:  0.765625
train loss:  0.4976057708263397
train gradient:  0.3678716394541224
iteration : 8607
train acc:  0.8984375
train loss:  0.24742235243320465
train gradient:  0.10744881237738023
iteration : 8608
train acc:  0.8671875
train loss:  0.342168390750885
train gradient:  0.18348054114746196
iteration : 8609
train acc:  0.8984375
train loss:  0.2943747341632843
train gradient:  0.15486611423666582
iteration : 8610
train acc:  0.890625
train loss:  0.31188085675239563
train gradient:  0.19628197590424779
iteration : 8611
train acc:  0.9140625
train loss:  0.26716864109039307
train gradient:  0.1306857440723343
iteration : 8612
train acc:  0.90625
train loss:  0.3111591339111328
train gradient:  0.1171503629393831
iteration : 8613
train acc:  0.875
train loss:  0.30425527691841125
train gradient:  0.18957250236284473
iteration : 8614
train acc:  0.8828125
train loss:  0.3076917231082916
train gradient:  0.13790815246002616
iteration : 8615
train acc:  0.875
train loss:  0.32988467812538147
train gradient:  0.23147822049461225
iteration : 8616
train acc:  0.8984375
train loss:  0.25698816776275635
train gradient:  0.1357679967168035
iteration : 8617
train acc:  0.890625
train loss:  0.3248807489871979
train gradient:  0.16280345905848562
iteration : 8618
train acc:  0.8671875
train loss:  0.27564898133277893
train gradient:  0.2011052286622313
iteration : 8619
train acc:  0.859375
train loss:  0.3357841372489929
train gradient:  0.18029542391469083
iteration : 8620
train acc:  0.8515625
train loss:  0.2966771125793457
train gradient:  0.18220406684492166
iteration : 8621
train acc:  0.8984375
train loss:  0.25262489914894104
train gradient:  0.12301968517913399
iteration : 8622
train acc:  0.8359375
train loss:  0.368429571390152
train gradient:  0.22812773217618532
iteration : 8623
train acc:  0.828125
train loss:  0.36741966009140015
train gradient:  0.20676630140103688
iteration : 8624
train acc:  0.8828125
train loss:  0.2542981803417206
train gradient:  0.1798720306225763
iteration : 8625
train acc:  0.875
train loss:  0.27776288986206055
train gradient:  0.13306345005541467
iteration : 8626
train acc:  0.8359375
train loss:  0.3238721489906311
train gradient:  0.1941416844742023
iteration : 8627
train acc:  0.796875
train loss:  0.40813398361206055
train gradient:  0.20500751509821205
iteration : 8628
train acc:  0.8984375
train loss:  0.272733211517334
train gradient:  0.141212822968175
iteration : 8629
train acc:  0.90625
train loss:  0.27214738726615906
train gradient:  0.13186547166715384
iteration : 8630
train acc:  0.8515625
train loss:  0.3376975357532501
train gradient:  0.20260250167359534
iteration : 8631
train acc:  0.875
train loss:  0.2840951979160309
train gradient:  0.16369887720557214
iteration : 8632
train acc:  0.8671875
train loss:  0.3028048872947693
train gradient:  0.22373872693400748
iteration : 8633
train acc:  0.8515625
train loss:  0.3015553951263428
train gradient:  0.23179980305727668
iteration : 8634
train acc:  0.8828125
train loss:  0.29774710536003113
train gradient:  0.16377915841424623
iteration : 8635
train acc:  0.875
train loss:  0.3004087805747986
train gradient:  0.13375026704850954
iteration : 8636
train acc:  0.84375
train loss:  0.3565731644630432
train gradient:  0.2383283305682003
iteration : 8637
train acc:  0.828125
train loss:  0.3328709602355957
train gradient:  0.14819700775184608
iteration : 8638
train acc:  0.84375
train loss:  0.3487042188644409
train gradient:  0.17383899558104327
iteration : 8639
train acc:  0.8046875
train loss:  0.3818596303462982
train gradient:  0.22580257258450842
iteration : 8640
train acc:  0.8515625
train loss:  0.3874465823173523
train gradient:  0.26288533770014383
iteration : 8641
train acc:  0.859375
train loss:  0.3520084023475647
train gradient:  0.24757070126410943
iteration : 8642
train acc:  0.875
train loss:  0.31551241874694824
train gradient:  0.13712175586387215
iteration : 8643
train acc:  0.8515625
train loss:  0.3018650412559509
train gradient:  0.1247353880245163
iteration : 8644
train acc:  0.8671875
train loss:  0.3614312410354614
train gradient:  0.20063469680354123
iteration : 8645
train acc:  0.8359375
train loss:  0.335046648979187
train gradient:  0.2615527501683312
iteration : 8646
train acc:  0.8828125
train loss:  0.2929019331932068
train gradient:  0.2065056471166546
iteration : 8647
train acc:  0.8203125
train loss:  0.3099839985370636
train gradient:  0.22820636913845097
iteration : 8648
train acc:  0.8671875
train loss:  0.2853741943836212
train gradient:  0.1444563601575134
iteration : 8649
train acc:  0.8671875
train loss:  0.30467909574508667
train gradient:  0.16442308602442995
iteration : 8650
train acc:  0.84375
train loss:  0.33701711893081665
train gradient:  0.1870354965284327
iteration : 8651
train acc:  0.875
train loss:  0.26548832654953003
train gradient:  0.19484844019955153
iteration : 8652
train acc:  0.84375
train loss:  0.31573575735092163
train gradient:  0.16775889767287466
iteration : 8653
train acc:  0.890625
train loss:  0.29457587003707886
train gradient:  0.17236227154977182
iteration : 8654
train acc:  0.8828125
train loss:  0.277567982673645
train gradient:  0.15025965173812195
iteration : 8655
train acc:  0.890625
train loss:  0.2817036509513855
train gradient:  0.19901492536650964
iteration : 8656
train acc:  0.8828125
train loss:  0.29496341943740845
train gradient:  0.1266684740355015
iteration : 8657
train acc:  0.8671875
train loss:  0.3239021599292755
train gradient:  0.23750435545343265
iteration : 8658
train acc:  0.8671875
train loss:  0.2914566993713379
train gradient:  0.17179621725253524
iteration : 8659
train acc:  0.84375
train loss:  0.37714529037475586
train gradient:  0.20992122649392417
iteration : 8660
train acc:  0.9140625
train loss:  0.2828536629676819
train gradient:  0.1327598503396593
iteration : 8661
train acc:  0.8828125
train loss:  0.32036012411117554
train gradient:  0.2152972579204292
iteration : 8662
train acc:  0.8125
train loss:  0.32421520352363586
train gradient:  0.2535134003421531
iteration : 8663
train acc:  0.859375
train loss:  0.3221013844013214
train gradient:  0.1545960848049323
iteration : 8664
train acc:  0.875
train loss:  0.32984834909439087
train gradient:  0.20633643995058454
iteration : 8665
train acc:  0.8671875
train loss:  0.30528298020362854
train gradient:  0.2500265087671847
iteration : 8666
train acc:  0.8984375
train loss:  0.27540069818496704
train gradient:  0.1518407640898496
iteration : 8667
train acc:  0.8671875
train loss:  0.2873084247112274
train gradient:  0.1582223139611792
iteration : 8668
train acc:  0.890625
train loss:  0.3120892345905304
train gradient:  0.1765101935731086
iteration : 8669
train acc:  0.8515625
train loss:  0.30326491594314575
train gradient:  0.14134091760603762
iteration : 8670
train acc:  0.8359375
train loss:  0.37012410163879395
train gradient:  0.17895877781050576
iteration : 8671
train acc:  0.8828125
train loss:  0.26325923204421997
train gradient:  0.1302484342700191
iteration : 8672
train acc:  0.8515625
train loss:  0.26053839921951294
train gradient:  0.14491143280145405
iteration : 8673
train acc:  0.859375
train loss:  0.3581644296646118
train gradient:  0.18102546606209877
iteration : 8674
train acc:  0.8515625
train loss:  0.3673744201660156
train gradient:  0.20072389164774646
iteration : 8675
train acc:  0.859375
train loss:  0.3908758759498596
train gradient:  0.241324100559686
iteration : 8676
train acc:  0.8515625
train loss:  0.33881258964538574
train gradient:  0.24873249955947221
iteration : 8677
train acc:  0.796875
train loss:  0.40540409088134766
train gradient:  0.30180857610851525
iteration : 8678
train acc:  0.828125
train loss:  0.45608147978782654
train gradient:  0.33394291053386826
iteration : 8679
train acc:  0.8515625
train loss:  0.34273767471313477
train gradient:  0.1872169563444415
iteration : 8680
train acc:  0.84375
train loss:  0.38355445861816406
train gradient:  0.1920958861104612
iteration : 8681
train acc:  0.9140625
train loss:  0.26433831453323364
train gradient:  0.21140570248160007
iteration : 8682
train acc:  0.8359375
train loss:  0.36128419637680054
train gradient:  0.31473273246576927
iteration : 8683
train acc:  0.8515625
train loss:  0.3365437090396881
train gradient:  0.23100704872879838
iteration : 8684
train acc:  0.84375
train loss:  0.3421805500984192
train gradient:  0.15341320339734935
iteration : 8685
train acc:  0.8203125
train loss:  0.3418522775173187
train gradient:  0.27976942861627657
iteration : 8686
train acc:  0.859375
train loss:  0.3370893895626068
train gradient:  0.19157846776961857
iteration : 8687
train acc:  0.921875
train loss:  0.2593345642089844
train gradient:  0.16218887776576205
iteration : 8688
train acc:  0.8046875
train loss:  0.40631553530693054
train gradient:  0.224129251437505
iteration : 8689
train acc:  0.875
train loss:  0.31799131631851196
train gradient:  0.16880919804195993
iteration : 8690
train acc:  0.875
train loss:  0.29127323627471924
train gradient:  0.1638308080383004
iteration : 8691
train acc:  0.8671875
train loss:  0.3179027736186981
train gradient:  0.20310900175439783
iteration : 8692
train acc:  0.796875
train loss:  0.395395427942276
train gradient:  0.25988212158554325
iteration : 8693
train acc:  0.8515625
train loss:  0.29771846532821655
train gradient:  0.1134492048326842
iteration : 8694
train acc:  0.859375
train loss:  0.3283002972602844
train gradient:  0.21664582945029967
iteration : 8695
train acc:  0.9375
train loss:  0.20406457781791687
train gradient:  0.11061722686764418
iteration : 8696
train acc:  0.8359375
train loss:  0.37876084446907043
train gradient:  0.24147787411698207
iteration : 8697
train acc:  0.8515625
train loss:  0.3581731915473938
train gradient:  0.21887284570704335
iteration : 8698
train acc:  0.8828125
train loss:  0.27622997760772705
train gradient:  0.14469433130903706
iteration : 8699
train acc:  0.9375
train loss:  0.2532944083213806
train gradient:  0.1541503840038578
iteration : 8700
train acc:  0.8984375
train loss:  0.28782904148101807
train gradient:  0.1452506242163405
iteration : 8701
train acc:  0.8515625
train loss:  0.30963605642318726
train gradient:  0.17370376385123223
iteration : 8702
train acc:  0.8203125
train loss:  0.3999430239200592
train gradient:  0.21385114746817344
iteration : 8703
train acc:  0.8359375
train loss:  0.33318549394607544
train gradient:  0.18072182425824895
iteration : 8704
train acc:  0.8515625
train loss:  0.3097738027572632
train gradient:  0.20163652542479396
iteration : 8705
train acc:  0.8671875
train loss:  0.34511175751686096
train gradient:  0.25752425506180016
iteration : 8706
train acc:  0.859375
train loss:  0.34384387731552124
train gradient:  0.1452303186524716
iteration : 8707
train acc:  0.8359375
train loss:  0.360446572303772
train gradient:  0.21230431255001556
iteration : 8708
train acc:  0.8828125
train loss:  0.2814346253871918
train gradient:  0.1407667884490192
iteration : 8709
train acc:  0.8828125
train loss:  0.29123958945274353
train gradient:  0.23637204262208644
iteration : 8710
train acc:  0.890625
train loss:  0.27254343032836914
train gradient:  0.13476021944560346
iteration : 8711
train acc:  0.8671875
train loss:  0.33463266491889954
train gradient:  0.23069556371036848
iteration : 8712
train acc:  0.8515625
train loss:  0.36992761492729187
train gradient:  0.2840662166197259
iteration : 8713
train acc:  0.8125
train loss:  0.4344780147075653
train gradient:  0.2854912283450488
iteration : 8714
train acc:  0.84375
train loss:  0.37120622396469116
train gradient:  0.19484333366595372
iteration : 8715
train acc:  0.859375
train loss:  0.40323737263679504
train gradient:  0.25396533500048296
iteration : 8716
train acc:  0.859375
train loss:  0.29412347078323364
train gradient:  0.12918023891387737
iteration : 8717
train acc:  0.890625
train loss:  0.24250590801239014
train gradient:  0.22729835458842956
iteration : 8718
train acc:  0.8828125
train loss:  0.2978113293647766
train gradient:  0.1639324871757053
iteration : 8719
train acc:  0.8359375
train loss:  0.33504143357276917
train gradient:  0.16085424656792158
iteration : 8720
train acc:  0.90625
train loss:  0.26059094071388245
train gradient:  0.16238907671441566
iteration : 8721
train acc:  0.8671875
train loss:  0.28964963555336
train gradient:  0.16145725135229722
iteration : 8722
train acc:  0.8671875
train loss:  0.31231391429901123
train gradient:  0.16899651997578535
iteration : 8723
train acc:  0.890625
train loss:  0.24627162516117096
train gradient:  0.13411175278215517
iteration : 8724
train acc:  0.8515625
train loss:  0.3764015734195709
train gradient:  0.15453088837402612
iteration : 8725
train acc:  0.859375
train loss:  0.29296818375587463
train gradient:  0.13898697532328486
iteration : 8726
train acc:  0.8046875
train loss:  0.3727560341358185
train gradient:  0.28373926524999477
iteration : 8727
train acc:  0.8515625
train loss:  0.31782642006874084
train gradient:  0.2000707137830473
iteration : 8728
train acc:  0.84375
train loss:  0.3310834765434265
train gradient:  0.2442529781947078
iteration : 8729
train acc:  0.921875
train loss:  0.2785021662712097
train gradient:  0.12297064253137384
iteration : 8730
train acc:  0.8125
train loss:  0.4391178488731384
train gradient:  0.37038258139666946
iteration : 8731
train acc:  0.84375
train loss:  0.35254794359207153
train gradient:  0.19896631206633628
iteration : 8732
train acc:  0.8671875
train loss:  0.2946650981903076
train gradient:  0.14700137132736799
iteration : 8733
train acc:  0.8828125
train loss:  0.3127071261405945
train gradient:  0.169515709064036
iteration : 8734
train acc:  0.8671875
train loss:  0.31191518902778625
train gradient:  0.20315777427690151
iteration : 8735
train acc:  0.8125
train loss:  0.4521067142486572
train gradient:  0.28139807750383
iteration : 8736
train acc:  0.7734375
train loss:  0.4043784737586975
train gradient:  0.3262349975338604
iteration : 8737
train acc:  0.8359375
train loss:  0.3317863345146179
train gradient:  0.1417916349576452
iteration : 8738
train acc:  0.9296875
train loss:  0.23082400858402252
train gradient:  0.10168803703729706
iteration : 8739
train acc:  0.890625
train loss:  0.29424381256103516
train gradient:  0.14327586332478892
iteration : 8740
train acc:  0.875
train loss:  0.29988211393356323
train gradient:  0.14706395289528595
iteration : 8741
train acc:  0.8515625
train loss:  0.3799721896648407
train gradient:  0.21376626946694088
iteration : 8742
train acc:  0.8203125
train loss:  0.3708970546722412
train gradient:  0.26530961445189727
iteration : 8743
train acc:  0.8125
train loss:  0.4263538122177124
train gradient:  0.25802361590319195
iteration : 8744
train acc:  0.8515625
train loss:  0.37034159898757935
train gradient:  0.22448631842696504
iteration : 8745
train acc:  0.859375
train loss:  0.36716073751449585
train gradient:  0.16630348629409666
iteration : 8746
train acc:  0.8359375
train loss:  0.3363078236579895
train gradient:  0.14926748619488486
iteration : 8747
train acc:  0.890625
train loss:  0.2376573085784912
train gradient:  0.09634496047366009
iteration : 8748
train acc:  0.8515625
train loss:  0.3386295437812805
train gradient:  0.19362198929485286
iteration : 8749
train acc:  0.8671875
train loss:  0.31624138355255127
train gradient:  0.15772329959881723
iteration : 8750
train acc:  0.8671875
train loss:  0.2640426456928253
train gradient:  0.11397738084172372
iteration : 8751
train acc:  0.84375
train loss:  0.3593631386756897
train gradient:  0.20776144127981572
iteration : 8752
train acc:  0.8671875
train loss:  0.3097357749938965
train gradient:  0.2078332443737415
iteration : 8753
train acc:  0.8515625
train loss:  0.34103792905807495
train gradient:  0.16571757266147824
iteration : 8754
train acc:  0.8203125
train loss:  0.35400670766830444
train gradient:  0.23823336883411267
iteration : 8755
train acc:  0.921875
train loss:  0.26051798462867737
train gradient:  0.23454748013533272
iteration : 8756
train acc:  0.828125
train loss:  0.3491654694080353
train gradient:  0.1881259053362977
iteration : 8757
train acc:  0.8671875
train loss:  0.2840924859046936
train gradient:  0.1401926731584245
iteration : 8758
train acc:  0.875
train loss:  0.3134151101112366
train gradient:  0.13680087550491504
iteration : 8759
train acc:  0.828125
train loss:  0.35453420877456665
train gradient:  0.21563325827999635
iteration : 8760
train acc:  0.84375
train loss:  0.3049265742301941
train gradient:  0.1529009678515496
iteration : 8761
train acc:  0.8203125
train loss:  0.43091168999671936
train gradient:  0.276017252348224
iteration : 8762
train acc:  0.875
train loss:  0.29142460227012634
train gradient:  0.19937799488601232
iteration : 8763
train acc:  0.859375
train loss:  0.3146178126335144
train gradient:  0.15697086245650183
iteration : 8764
train acc:  0.859375
train loss:  0.2757590115070343
train gradient:  0.14438337130099704
iteration : 8765
train acc:  0.90625
train loss:  0.2557048499584198
train gradient:  0.15791202816212505
iteration : 8766
train acc:  0.9140625
train loss:  0.28389573097229004
train gradient:  0.13989229840112744
iteration : 8767
train acc:  0.875
train loss:  0.3574189841747284
train gradient:  0.20342700081813642
iteration : 8768
train acc:  0.8671875
train loss:  0.3794143795967102
train gradient:  0.25277696413341977
iteration : 8769
train acc:  0.8515625
train loss:  0.34217968583106995
train gradient:  0.13558395764309253
iteration : 8770
train acc:  0.84375
train loss:  0.327830046415329
train gradient:  0.1568713963135575
iteration : 8771
train acc:  0.875
train loss:  0.28877589106559753
train gradient:  0.12917338960665103
iteration : 8772
train acc:  0.859375
train loss:  0.32328981161117554
train gradient:  0.13334707777361726
iteration : 8773
train acc:  0.859375
train loss:  0.3428032398223877
train gradient:  0.16771150502730112
iteration : 8774
train acc:  0.875
train loss:  0.3014340400695801
train gradient:  0.14803444565299168
iteration : 8775
train acc:  0.84375
train loss:  0.36675095558166504
train gradient:  0.1722599909505992
iteration : 8776
train acc:  0.84375
train loss:  0.298878014087677
train gradient:  0.16138060338603916
iteration : 8777
train acc:  0.859375
train loss:  0.31182733178138733
train gradient:  0.1400164785746631
iteration : 8778
train acc:  0.8203125
train loss:  0.33672472834587097
train gradient:  0.1666166009271291
iteration : 8779
train acc:  0.8515625
train loss:  0.3396620750427246
train gradient:  0.13323383731752664
iteration : 8780
train acc:  0.875
train loss:  0.2877013385295868
train gradient:  0.13142129814868292
iteration : 8781
train acc:  0.9140625
train loss:  0.2607700228691101
train gradient:  0.1814149830051458
iteration : 8782
train acc:  0.8359375
train loss:  0.417461633682251
train gradient:  0.39717789124437813
iteration : 8783
train acc:  0.8828125
train loss:  0.26694488525390625
train gradient:  0.1086993312563544
iteration : 8784
train acc:  0.8359375
train loss:  0.3509584069252014
train gradient:  0.16705757419419814
iteration : 8785
train acc:  0.859375
train loss:  0.27125051617622375
train gradient:  0.11491455453694795
iteration : 8786
train acc:  0.828125
train loss:  0.3504432141780853
train gradient:  0.42241306517222327
iteration : 8787
train acc:  0.8828125
train loss:  0.24595940113067627
train gradient:  0.19759683910809717
iteration : 8788
train acc:  0.8359375
train loss:  0.31483885645866394
train gradient:  0.1525951050544118
iteration : 8789
train acc:  0.8828125
train loss:  0.25632792711257935
train gradient:  0.1408943692723254
iteration : 8790
train acc:  0.8046875
train loss:  0.36765509843826294
train gradient:  0.25247036658037886
iteration : 8791
train acc:  0.8671875
train loss:  0.32846009731292725
train gradient:  0.15655226969596997
iteration : 8792
train acc:  0.859375
train loss:  0.346771240234375
train gradient:  0.17365213440153
iteration : 8793
train acc:  0.84375
train loss:  0.3463764786720276
train gradient:  0.2029888110743634
iteration : 8794
train acc:  0.8515625
train loss:  0.3298766613006592
train gradient:  0.18537452540784827
iteration : 8795
train acc:  0.828125
train loss:  0.3874151110649109
train gradient:  0.2376066028115293
iteration : 8796
train acc:  0.84375
train loss:  0.39140790700912476
train gradient:  0.21850415115257465
iteration : 8797
train acc:  0.84375
train loss:  0.3122616410255432
train gradient:  0.25653248769223447
iteration : 8798
train acc:  0.8828125
train loss:  0.296734094619751
train gradient:  0.15235642643384717
iteration : 8799
train acc:  0.859375
train loss:  0.28896427154541016
train gradient:  0.14271161140652655
iteration : 8800
train acc:  0.8671875
train loss:  0.3609806299209595
train gradient:  0.26404927913267473
iteration : 8801
train acc:  0.84375
train loss:  0.3843678832054138
train gradient:  0.2402999332479294
iteration : 8802
train acc:  0.8203125
train loss:  0.36573630571365356
train gradient:  0.17583533753112654
iteration : 8803
train acc:  0.8671875
train loss:  0.36588677763938904
train gradient:  0.18830576461201037
iteration : 8804
train acc:  0.8984375
train loss:  0.27405864000320435
train gradient:  0.1107005893375988
iteration : 8805
train acc:  0.8984375
train loss:  0.2537734806537628
train gradient:  0.11793089499379146
iteration : 8806
train acc:  0.765625
train loss:  0.4713992476463318
train gradient:  0.316806718886018
iteration : 8807
train acc:  0.875
train loss:  0.31109097599983215
train gradient:  0.16013181820751182
iteration : 8808
train acc:  0.890625
train loss:  0.28226906061172485
train gradient:  0.1276435972765463
iteration : 8809
train acc:  0.8515625
train loss:  0.3895774483680725
train gradient:  0.2590090323786052
iteration : 8810
train acc:  0.8359375
train loss:  0.40517592430114746
train gradient:  0.22533070559428925
iteration : 8811
train acc:  0.8515625
train loss:  0.31637561321258545
train gradient:  0.154817743803419
iteration : 8812
train acc:  0.890625
train loss:  0.2738826870918274
train gradient:  0.14681339994578327
iteration : 8813
train acc:  0.828125
train loss:  0.4510918855667114
train gradient:  0.3253011757536065
iteration : 8814
train acc:  0.890625
train loss:  0.30126509070396423
train gradient:  0.27395487922048367
iteration : 8815
train acc:  0.8671875
train loss:  0.31020432710647583
train gradient:  0.17138187032897462
iteration : 8816
train acc:  0.8671875
train loss:  0.2860826253890991
train gradient:  0.11840502766522625
iteration : 8817
train acc:  0.8515625
train loss:  0.3614746630191803
train gradient:  0.15973419542457504
iteration : 8818
train acc:  0.859375
train loss:  0.2840563654899597
train gradient:  0.1481219634139712
iteration : 8819
train acc:  0.8515625
train loss:  0.34299641847610474
train gradient:  0.21111417578324063
iteration : 8820
train acc:  0.828125
train loss:  0.32636377215385437
train gradient:  0.13673078757858606
iteration : 8821
train acc:  0.8828125
train loss:  0.3064116835594177
train gradient:  0.15097851137053048
iteration : 8822
train acc:  0.8359375
train loss:  0.37881362438201904
train gradient:  0.23201142887708448
iteration : 8823
train acc:  0.8515625
train loss:  0.3223193287849426
train gradient:  0.17906947053265046
iteration : 8824
train acc:  0.8359375
train loss:  0.36501213908195496
train gradient:  0.17008260099188266
iteration : 8825
train acc:  0.890625
train loss:  0.27024880051612854
train gradient:  0.14848870018864513
iteration : 8826
train acc:  0.8828125
train loss:  0.30133506655693054
train gradient:  0.1267809355111876
iteration : 8827
train acc:  0.8671875
train loss:  0.3147341012954712
train gradient:  0.2111118990269722
iteration : 8828
train acc:  0.8046875
train loss:  0.46286845207214355
train gradient:  0.4334223008399732
iteration : 8829
train acc:  0.8125
train loss:  0.38817813992500305
train gradient:  0.270024541284918
iteration : 8830
train acc:  0.8359375
train loss:  0.3782214820384979
train gradient:  0.14803225970538034
iteration : 8831
train acc:  0.8828125
train loss:  0.3083014488220215
train gradient:  0.21361876048597267
iteration : 8832
train acc:  0.84375
train loss:  0.31448009610176086
train gradient:  0.1521571414786721
iteration : 8833
train acc:  0.875
train loss:  0.33799871802330017
train gradient:  0.14358846110182755
iteration : 8834
train acc:  0.796875
train loss:  0.36045750975608826
train gradient:  0.18443327545142718
iteration : 8835
train acc:  0.84375
train loss:  0.3133429288864136
train gradient:  0.2629426263848721
iteration : 8836
train acc:  0.859375
train loss:  0.3247520923614502
train gradient:  0.166323933310777
iteration : 8837
train acc:  0.828125
train loss:  0.3380507826805115
train gradient:  0.22185121705147723
iteration : 8838
train acc:  0.859375
train loss:  0.31691989302635193
train gradient:  0.18475968210616334
iteration : 8839
train acc:  0.8828125
train loss:  0.24501579999923706
train gradient:  0.1159653457918185
iteration : 8840
train acc:  0.890625
train loss:  0.26022347807884216
train gradient:  0.13841403933810728
iteration : 8841
train acc:  0.8359375
train loss:  0.3205852508544922
train gradient:  0.1413186032525129
iteration : 8842
train acc:  0.859375
train loss:  0.3594215512275696
train gradient:  0.15966072586921035
iteration : 8843
train acc:  0.8359375
train loss:  0.3700897991657257
train gradient:  0.2822933001912351
iteration : 8844
train acc:  0.8203125
train loss:  0.35350024700164795
train gradient:  0.15864359173330392
iteration : 8845
train acc:  0.8359375
train loss:  0.308474600315094
train gradient:  0.22699759065772784
iteration : 8846
train acc:  0.859375
train loss:  0.32610246539115906
train gradient:  0.15749784748689682
iteration : 8847
train acc:  0.9140625
train loss:  0.2308325469493866
train gradient:  0.14159940724890033
iteration : 8848
train acc:  0.828125
train loss:  0.36054539680480957
train gradient:  0.2568417735399907
iteration : 8849
train acc:  0.8515625
train loss:  0.3511677384376526
train gradient:  0.1738018952850419
iteration : 8850
train acc:  0.8828125
train loss:  0.29420387744903564
train gradient:  0.17231151252907884
iteration : 8851
train acc:  0.828125
train loss:  0.3999786376953125
train gradient:  0.19187334449849852
iteration : 8852
train acc:  0.90625
train loss:  0.3208291828632355
train gradient:  0.17574037272711315
iteration : 8853
train acc:  0.8515625
train loss:  0.3085903227329254
train gradient:  0.1657041350191411
iteration : 8854
train acc:  0.875
train loss:  0.3304775357246399
train gradient:  0.21935126354114676
iteration : 8855
train acc:  0.8515625
train loss:  0.360478013753891
train gradient:  0.19019839594949228
iteration : 8856
train acc:  0.828125
train loss:  0.3476637005805969
train gradient:  0.284909981200257
iteration : 8857
train acc:  0.859375
train loss:  0.29443931579589844
train gradient:  0.17154953298318903
iteration : 8858
train acc:  0.828125
train loss:  0.36574023962020874
train gradient:  0.18747776228216131
iteration : 8859
train acc:  0.875
train loss:  0.2770638167858124
train gradient:  0.1716285519994244
iteration : 8860
train acc:  0.859375
train loss:  0.3425045311450958
train gradient:  0.1847014050789358
iteration : 8861
train acc:  0.8515625
train loss:  0.3427388370037079
train gradient:  0.21485590303874683
iteration : 8862
train acc:  0.8515625
train loss:  0.3296647071838379
train gradient:  0.15321246321126464
iteration : 8863
train acc:  0.8828125
train loss:  0.3121175169944763
train gradient:  0.26728692185237085
iteration : 8864
train acc:  0.8671875
train loss:  0.30561763048171997
train gradient:  0.16792351171819014
iteration : 8865
train acc:  0.84375
train loss:  0.3472888469696045
train gradient:  0.21041515511645317
iteration : 8866
train acc:  0.8671875
train loss:  0.29030007123947144
train gradient:  0.1774065236145828
iteration : 8867
train acc:  0.859375
train loss:  0.33842557668685913
train gradient:  0.16786675199014323
iteration : 8868
train acc:  0.84375
train loss:  0.32626116275787354
train gradient:  0.15485746659430055
iteration : 8869
train acc:  0.8515625
train loss:  0.30368882417678833
train gradient:  0.11512931287546803
iteration : 8870
train acc:  0.84375
train loss:  0.3578905165195465
train gradient:  0.2593965124763986
iteration : 8871
train acc:  0.875
train loss:  0.36364859342575073
train gradient:  0.2877750076965626
iteration : 8872
train acc:  0.9140625
train loss:  0.26517346501350403
train gradient:  0.13652986748535897
iteration : 8873
train acc:  0.890625
train loss:  0.31673526763916016
train gradient:  0.18692941358421322
iteration : 8874
train acc:  0.875
train loss:  0.3028565049171448
train gradient:  0.18925601104235215
iteration : 8875
train acc:  0.7890625
train loss:  0.4015779495239258
train gradient:  0.21235157124998888
iteration : 8876
train acc:  0.84375
train loss:  0.339616596698761
train gradient:  0.22571455653809205
iteration : 8877
train acc:  0.8359375
train loss:  0.333257257938385
train gradient:  0.25056649601528114
iteration : 8878
train acc:  0.84375
train loss:  0.3252483010292053
train gradient:  0.16755751621007178
iteration : 8879
train acc:  0.859375
train loss:  0.3253445327281952
train gradient:  0.20461590257399473
iteration : 8880
train acc:  0.859375
train loss:  0.351106196641922
train gradient:  0.22954145144172072
iteration : 8881
train acc:  0.84375
train loss:  0.323829710483551
train gradient:  0.2444854486762291
iteration : 8882
train acc:  0.8828125
train loss:  0.29802343249320984
train gradient:  0.19020233967306457
iteration : 8883
train acc:  0.8828125
train loss:  0.2759997248649597
train gradient:  0.11271743873708161
iteration : 8884
train acc:  0.859375
train loss:  0.34727105498313904
train gradient:  0.20601715802291926
iteration : 8885
train acc:  0.84375
train loss:  0.31368350982666016
train gradient:  0.1751522176982901
iteration : 8886
train acc:  0.84375
train loss:  0.33521217107772827
train gradient:  0.16278095074823393
iteration : 8887
train acc:  0.8359375
train loss:  0.3214367926120758
train gradient:  0.1708657316365939
iteration : 8888
train acc:  0.9140625
train loss:  0.2659424841403961
train gradient:  0.22460898959538078
iteration : 8889
train acc:  0.828125
train loss:  0.36961936950683594
train gradient:  0.7063498332785275
iteration : 8890
train acc:  0.8828125
train loss:  0.3475619852542877
train gradient:  0.1398092836070275
iteration : 8891
train acc:  0.9140625
train loss:  0.22404710948467255
train gradient:  0.09561246410513036
iteration : 8892
train acc:  0.90625
train loss:  0.24605606496334076
train gradient:  0.16862522157179796
iteration : 8893
train acc:  0.875
train loss:  0.3263401389122009
train gradient:  0.18105907182606298
iteration : 8894
train acc:  0.890625
train loss:  0.25541216135025024
train gradient:  0.1240405169823077
iteration : 8895
train acc:  0.859375
train loss:  0.3162698745727539
train gradient:  0.18296791440789806
iteration : 8896
train acc:  0.828125
train loss:  0.3846816122531891
train gradient:  0.1859786597249977
iteration : 8897
train acc:  0.8359375
train loss:  0.39794492721557617
train gradient:  0.22600484988475933
iteration : 8898
train acc:  0.8515625
train loss:  0.3196575939655304
train gradient:  0.1699903729419909
iteration : 8899
train acc:  0.8515625
train loss:  0.3004103899002075
train gradient:  0.2137875055515323
iteration : 8900
train acc:  0.8671875
train loss:  0.3623400628566742
train gradient:  0.19707743397400884
iteration : 8901
train acc:  0.8515625
train loss:  0.30823594331741333
train gradient:  0.21649662084373889
iteration : 8902
train acc:  0.8828125
train loss:  0.2754707336425781
train gradient:  0.25429804953562996
iteration : 8903
train acc:  0.859375
train loss:  0.35496625304222107
train gradient:  0.20824728176958074
iteration : 8904
train acc:  0.8515625
train loss:  0.3415312170982361
train gradient:  0.3597727778524269
iteration : 8905
train acc:  0.796875
train loss:  0.4199047088623047
train gradient:  0.2801774316051216
iteration : 8906
train acc:  0.8984375
train loss:  0.23633450269699097
train gradient:  0.12580500670678293
iteration : 8907
train acc:  0.8515625
train loss:  0.33828091621398926
train gradient:  0.16749839253072843
iteration : 8908
train acc:  0.8046875
train loss:  0.4341643452644348
train gradient:  0.2888154698457934
iteration : 8909
train acc:  0.8515625
train loss:  0.3339744210243225
train gradient:  0.16006702143263363
iteration : 8910
train acc:  0.90625
train loss:  0.2552427053451538
train gradient:  0.14063473293776146
iteration : 8911
train acc:  0.890625
train loss:  0.2699702978134155
train gradient:  0.17104130377771898
iteration : 8912
train acc:  0.875
train loss:  0.26994356513023376
train gradient:  0.22230898732732185
iteration : 8913
train acc:  0.84375
train loss:  0.3532707989215851
train gradient:  0.2277159963543985
iteration : 8914
train acc:  0.921875
train loss:  0.21115395426750183
train gradient:  0.11104337273201313
iteration : 8915
train acc:  0.8046875
train loss:  0.3866581320762634
train gradient:  0.23421915579098196
iteration : 8916
train acc:  0.875
train loss:  0.2968468964099884
train gradient:  0.15839998247569365
iteration : 8917
train acc:  0.8046875
train loss:  0.3792318105697632
train gradient:  0.3261269559018554
iteration : 8918
train acc:  0.859375
train loss:  0.3394428491592407
train gradient:  0.20138986427627215
iteration : 8919
train acc:  0.8828125
train loss:  0.2640356123447418
train gradient:  0.13322020359530157
iteration : 8920
train acc:  0.8046875
train loss:  0.33171626925468445
train gradient:  0.22210494228570082
iteration : 8921
train acc:  0.8359375
train loss:  0.40240252017974854
train gradient:  0.242594831909737
iteration : 8922
train acc:  0.84375
train loss:  0.33913475275039673
train gradient:  0.23601904505370036
iteration : 8923
train acc:  0.84375
train loss:  0.31487876176834106
train gradient:  0.21543983831695626
iteration : 8924
train acc:  0.84375
train loss:  0.2939716577529907
train gradient:  0.19549894732692394
iteration : 8925
train acc:  0.84375
train loss:  0.39218905568122864
train gradient:  0.23887331300433096
iteration : 8926
train acc:  0.8359375
train loss:  0.33493804931640625
train gradient:  0.23373555545663863
iteration : 8927
train acc:  0.859375
train loss:  0.3772319555282593
train gradient:  0.2896569282025393
iteration : 8928
train acc:  0.828125
train loss:  0.4461526572704315
train gradient:  0.32157722357862334
iteration : 8929
train acc:  0.8515625
train loss:  0.38787373900413513
train gradient:  0.18454839547892846
iteration : 8930
train acc:  0.875
train loss:  0.3191348612308502
train gradient:  0.1756512512478896
iteration : 8931
train acc:  0.8203125
train loss:  0.36817479133605957
train gradient:  0.19122985975079573
iteration : 8932
train acc:  0.8359375
train loss:  0.3203224539756775
train gradient:  0.16910572028381077
iteration : 8933
train acc:  0.875
train loss:  0.31567174196243286
train gradient:  0.16639759643479116
iteration : 8934
train acc:  0.84375
train loss:  0.4010733366012573
train gradient:  0.2646472511272783
iteration : 8935
train acc:  0.859375
train loss:  0.3324308395385742
train gradient:  0.172549949510991
iteration : 8936
train acc:  0.8125
train loss:  0.3546806573867798
train gradient:  0.13029110720609885
iteration : 8937
train acc:  0.8671875
train loss:  0.3377978503704071
train gradient:  0.1912655098148653
iteration : 8938
train acc:  0.8828125
train loss:  0.2937970459461212
train gradient:  0.17206057291033874
iteration : 8939
train acc:  0.859375
train loss:  0.34504860639572144
train gradient:  0.19990093787786753
iteration : 8940
train acc:  0.890625
train loss:  0.2692182660102844
train gradient:  0.1406383690904809
iteration : 8941
train acc:  0.890625
train loss:  0.31567177176475525
train gradient:  0.18042238811731104
iteration : 8942
train acc:  0.78125
train loss:  0.3753187954425812
train gradient:  0.23129157544607104
iteration : 8943
train acc:  0.8828125
train loss:  0.3419201672077179
train gradient:  0.23045559045739816
iteration : 8944
train acc:  0.859375
train loss:  0.3132088780403137
train gradient:  0.18008277540189216
iteration : 8945
train acc:  0.8515625
train loss:  0.33575665950775146
train gradient:  0.14900869709537345
iteration : 8946
train acc:  0.84375
train loss:  0.310596764087677
train gradient:  0.17436821359435498
iteration : 8947
train acc:  0.8671875
train loss:  0.3551352620124817
train gradient:  0.25684993217507424
iteration : 8948
train acc:  0.8671875
train loss:  0.33337342739105225
train gradient:  0.2891989487066243
iteration : 8949
train acc:  0.8828125
train loss:  0.28679439425468445
train gradient:  0.1386854902445444
iteration : 8950
train acc:  0.8125
train loss:  0.32402220368385315
train gradient:  0.14399819081904774
iteration : 8951
train acc:  0.8828125
train loss:  0.2837090790271759
train gradient:  0.1701114262230045
iteration : 8952
train acc:  0.8984375
train loss:  0.2523137331008911
train gradient:  0.21246723749263002
iteration : 8953
train acc:  0.828125
train loss:  0.42920705676078796
train gradient:  0.24113555344955812
iteration : 8954
train acc:  0.8984375
train loss:  0.2709057033061981
train gradient:  0.19800209726246387
iteration : 8955
train acc:  0.859375
train loss:  0.374153733253479
train gradient:  0.1681297927389046
iteration : 8956
train acc:  0.8515625
train loss:  0.33518534898757935
train gradient:  0.16325964854198582
iteration : 8957
train acc:  0.875
train loss:  0.2710808515548706
train gradient:  0.1591489717855935
iteration : 8958
train acc:  0.890625
train loss:  0.3417539596557617
train gradient:  0.22756030743274605
iteration : 8959
train acc:  0.90625
train loss:  0.26995930075645447
train gradient:  0.11870119243425699
iteration : 8960
train acc:  0.859375
train loss:  0.348916232585907
train gradient:  0.1878255053623452
iteration : 8961
train acc:  0.9140625
train loss:  0.24927249550819397
train gradient:  0.12389070452495315
iteration : 8962
train acc:  0.890625
train loss:  0.2963784635066986
train gradient:  0.1619424706386346
iteration : 8963
train acc:  0.8515625
train loss:  0.3753722906112671
train gradient:  0.21475466086482514
iteration : 8964
train acc:  0.8828125
train loss:  0.33280807733535767
train gradient:  0.18256062284824037
iteration : 8965
train acc:  0.84375
train loss:  0.3971707820892334
train gradient:  0.2671987465880993
iteration : 8966
train acc:  0.8359375
train loss:  0.37200427055358887
train gradient:  0.15034940617056908
iteration : 8967
train acc:  0.8515625
train loss:  0.3513713479042053
train gradient:  0.18396712036792412
iteration : 8968
train acc:  0.8125
train loss:  0.4305744171142578
train gradient:  0.3916633768356779
iteration : 8969
train acc:  0.859375
train loss:  0.296219140291214
train gradient:  0.10436064728520987
iteration : 8970
train acc:  0.8359375
train loss:  0.4104515314102173
train gradient:  0.22733704839758365
iteration : 8971
train acc:  0.8671875
train loss:  0.30191513895988464
train gradient:  0.16722382715604583
iteration : 8972
train acc:  0.8828125
train loss:  0.2914469242095947
train gradient:  0.20607631622250738
iteration : 8973
train acc:  0.890625
train loss:  0.25482216477394104
train gradient:  0.12118153033183289
iteration : 8974
train acc:  0.859375
train loss:  0.37795490026474
train gradient:  0.2247997515227947
iteration : 8975
train acc:  0.8828125
train loss:  0.2997838258743286
train gradient:  0.1598106033660274
iteration : 8976
train acc:  0.859375
train loss:  0.32412776350975037
train gradient:  0.16941927474641563
iteration : 8977
train acc:  0.8671875
train loss:  0.3109269142150879
train gradient:  0.11552674178181091
iteration : 8978
train acc:  0.8671875
train loss:  0.2980404496192932
train gradient:  0.19750539127020084
iteration : 8979
train acc:  0.890625
train loss:  0.28991904854774475
train gradient:  0.12338931012543686
iteration : 8980
train acc:  0.8359375
train loss:  0.4204883575439453
train gradient:  0.3079726768180537
iteration : 8981
train acc:  0.8359375
train loss:  0.3630860447883606
train gradient:  0.2055725303332783
iteration : 8982
train acc:  0.890625
train loss:  0.26594579219818115
train gradient:  0.13206134944873404
iteration : 8983
train acc:  0.8203125
train loss:  0.469160258769989
train gradient:  0.2352403273737404
iteration : 8984
train acc:  0.8203125
train loss:  0.37307965755462646
train gradient:  0.16880200615806368
iteration : 8985
train acc:  0.8984375
train loss:  0.24202735722064972
train gradient:  0.08079152608867425
iteration : 8986
train acc:  0.859375
train loss:  0.3070799708366394
train gradient:  0.2027500178035538
iteration : 8987
train acc:  0.828125
train loss:  0.37667784094810486
train gradient:  0.29767950816608474
iteration : 8988
train acc:  0.828125
train loss:  0.3284757733345032
train gradient:  0.11861944409345483
iteration : 8989
train acc:  0.8984375
train loss:  0.25864917039871216
train gradient:  0.1092251530205237
iteration : 8990
train acc:  0.8046875
train loss:  0.3777836561203003
train gradient:  0.31032476606649795
iteration : 8991
train acc:  0.8359375
train loss:  0.34347355365753174
train gradient:  0.21709286778847886
iteration : 8992
train acc:  0.828125
train loss:  0.39422670006752014
train gradient:  0.23011316869462678
iteration : 8993
train acc:  0.8203125
train loss:  0.3400813341140747
train gradient:  0.18600038407207942
iteration : 8994
train acc:  0.8671875
train loss:  0.330744206905365
train gradient:  0.2978066084200199
iteration : 8995
train acc:  0.859375
train loss:  0.3491614758968353
train gradient:  0.21468523459160532
iteration : 8996
train acc:  0.8828125
train loss:  0.31043291091918945
train gradient:  0.20836464848789754
iteration : 8997
train acc:  0.890625
train loss:  0.3017539978027344
train gradient:  0.17053424983131146
iteration : 8998
train acc:  0.875
train loss:  0.32262948155403137
train gradient:  0.1843914752093865
iteration : 8999
train acc:  0.875
train loss:  0.3343034088611603
train gradient:  0.19032149427505962
iteration : 9000
train acc:  0.828125
train loss:  0.32561951875686646
train gradient:  0.15961368340094512
iteration : 9001
train acc:  0.8671875
train loss:  0.32529544830322266
train gradient:  0.13183397038866107
iteration : 9002
train acc:  0.90625
train loss:  0.26634684205055237
train gradient:  0.16567632478712596
iteration : 9003
train acc:  0.8125
train loss:  0.44151103496551514
train gradient:  0.3162241525634964
iteration : 9004
train acc:  0.78125
train loss:  0.4026356339454651
train gradient:  0.23438284893403707
iteration : 9005
train acc:  0.8828125
train loss:  0.3112949728965759
train gradient:  0.1293316890117922
iteration : 9006
train acc:  0.78125
train loss:  0.4080125093460083
train gradient:  0.3035805090140146
iteration : 9007
train acc:  0.828125
train loss:  0.31342363357543945
train gradient:  0.16488881591354645
iteration : 9008
train acc:  0.828125
train loss:  0.4209558665752411
train gradient:  0.16691202904242575
iteration : 9009
train acc:  0.875
train loss:  0.297420859336853
train gradient:  0.11236336561891763
iteration : 9010
train acc:  0.890625
train loss:  0.25208917260169983
train gradient:  0.09698353954354177
iteration : 9011
train acc:  0.859375
train loss:  0.30670005083084106
train gradient:  0.27676162170622765
iteration : 9012
train acc:  0.84375
train loss:  0.35755234956741333
train gradient:  0.18557746593429383
iteration : 9013
train acc:  0.828125
train loss:  0.3804035186767578
train gradient:  0.15525063958138385
iteration : 9014
train acc:  0.8828125
train loss:  0.26801154017448425
train gradient:  0.12110049563355556
iteration : 9015
train acc:  0.890625
train loss:  0.29444074630737305
train gradient:  0.08652760366225885
iteration : 9016
train acc:  0.84375
train loss:  0.3564501404762268
train gradient:  0.3147653971805392
iteration : 9017
train acc:  0.84375
train loss:  0.3608394265174866
train gradient:  0.29605890649736427
iteration : 9018
train acc:  0.8828125
train loss:  0.3666159510612488
train gradient:  0.1791401825855635
iteration : 9019
train acc:  0.859375
train loss:  0.2978224754333496
train gradient:  0.13935937426984796
iteration : 9020
train acc:  0.8671875
train loss:  0.32381105422973633
train gradient:  0.12509684150874162
iteration : 9021
train acc:  0.8515625
train loss:  0.32272231578826904
train gradient:  0.18289807020135831
iteration : 9022
train acc:  0.8671875
train loss:  0.3136807382106781
train gradient:  0.1984183360352625
iteration : 9023
train acc:  0.8984375
train loss:  0.33323341608047485
train gradient:  0.156349228730421
iteration : 9024
train acc:  0.859375
train loss:  0.37727344036102295
train gradient:  0.2208020845666977
iteration : 9025
train acc:  0.84375
train loss:  0.2915634214878082
train gradient:  0.16357793380101437
iteration : 9026
train acc:  0.8515625
train loss:  0.34233957529067993
train gradient:  0.17379152809574094
iteration : 9027
train acc:  0.8984375
train loss:  0.2839726209640503
train gradient:  0.19525487944324782
iteration : 9028
train acc:  0.8984375
train loss:  0.26044559478759766
train gradient:  0.19499841739368212
iteration : 9029
train acc:  0.84375
train loss:  0.3377130627632141
train gradient:  0.1136646445271786
iteration : 9030
train acc:  0.8203125
train loss:  0.44074949622154236
train gradient:  0.4633875753697336
iteration : 9031
train acc:  0.859375
train loss:  0.32309502363204956
train gradient:  0.13818526722412128
iteration : 9032
train acc:  0.84375
train loss:  0.3828819990158081
train gradient:  0.18111970231862942
iteration : 9033
train acc:  0.859375
train loss:  0.3393348455429077
train gradient:  0.13963075067206768
iteration : 9034
train acc:  0.8359375
train loss:  0.40470951795578003
train gradient:  0.26511407606308396
iteration : 9035
train acc:  0.8828125
train loss:  0.2958521544933319
train gradient:  0.1853928112130645
iteration : 9036
train acc:  0.859375
train loss:  0.3533504605293274
train gradient:  0.18568696789883396
iteration : 9037
train acc:  0.875
train loss:  0.2805243134498596
train gradient:  0.11245619817472848
iteration : 9038
train acc:  0.8046875
train loss:  0.43384575843811035
train gradient:  0.24605014704434813
iteration : 9039
train acc:  0.796875
train loss:  0.3555423617362976
train gradient:  0.2912631751579849
iteration : 9040
train acc:  0.84375
train loss:  0.38243168592453003
train gradient:  0.36783294428652813
iteration : 9041
train acc:  0.84375
train loss:  0.3662223517894745
train gradient:  0.2582162905346897
iteration : 9042
train acc:  0.875
train loss:  0.30365216732025146
train gradient:  0.11560179706795817
iteration : 9043
train acc:  0.875
train loss:  0.3091141879558563
train gradient:  0.228907799852292
iteration : 9044
train acc:  0.890625
train loss:  0.3018851578235626
train gradient:  0.16649025345481053
iteration : 9045
train acc:  0.8671875
train loss:  0.3213855028152466
train gradient:  0.12810508849031993
iteration : 9046
train acc:  0.828125
train loss:  0.33418008685112
train gradient:  0.16153867749041584
iteration : 9047
train acc:  0.84375
train loss:  0.33378589153289795
train gradient:  0.21125676057551335
iteration : 9048
train acc:  0.9140625
train loss:  0.23447661101818085
train gradient:  0.0826011808046983
iteration : 9049
train acc:  0.859375
train loss:  0.30811911821365356
train gradient:  0.15583602112995382
iteration : 9050
train acc:  0.859375
train loss:  0.3508042097091675
train gradient:  0.13593213837371448
iteration : 9051
train acc:  0.890625
train loss:  0.29853421449661255
train gradient:  0.12646131521201068
iteration : 9052
train acc:  0.875
train loss:  0.32158365845680237
train gradient:  0.1334393799731153
iteration : 9053
train acc:  0.890625
train loss:  0.2816765308380127
train gradient:  0.12207863235782457
iteration : 9054
train acc:  0.8515625
train loss:  0.32695358991622925
train gradient:  0.1271689784873107
iteration : 9055
train acc:  0.859375
train loss:  0.35030296444892883
train gradient:  0.15984071991085866
iteration : 9056
train acc:  0.8671875
train loss:  0.2969719469547272
train gradient:  0.11801862651425549
iteration : 9057
train acc:  0.84375
train loss:  0.32089436054229736
train gradient:  0.1325059657517636
iteration : 9058
train acc:  0.890625
train loss:  0.2633780241012573
train gradient:  0.14620957284306593
iteration : 9059
train acc:  0.890625
train loss:  0.31003862619400024
train gradient:  0.23390770979348754
iteration : 9060
train acc:  0.8828125
train loss:  0.3438686728477478
train gradient:  0.17565999524001735
iteration : 9061
train acc:  0.8984375
train loss:  0.2465999871492386
train gradient:  0.2288767436972864
iteration : 9062
train acc:  0.8671875
train loss:  0.3546885550022125
train gradient:  0.18389052263609884
iteration : 9063
train acc:  0.8828125
train loss:  0.26475685834884644
train gradient:  0.12302075766057022
iteration : 9064
train acc:  0.8671875
train loss:  0.2871307134628296
train gradient:  0.1328393804988729
iteration : 9065
train acc:  0.8125
train loss:  0.42440181970596313
train gradient:  0.3089667386499884
iteration : 9066
train acc:  0.8515625
train loss:  0.3296133577823639
train gradient:  0.171431812142394
iteration : 9067
train acc:  0.890625
train loss:  0.28372031450271606
train gradient:  0.13709088560221805
iteration : 9068
train acc:  0.8359375
train loss:  0.29650288820266724
train gradient:  0.1436260584263881
iteration : 9069
train acc:  0.8515625
train loss:  0.29443836212158203
train gradient:  0.18490184055810538
iteration : 9070
train acc:  0.8671875
train loss:  0.3024366497993469
train gradient:  0.1289942755509107
iteration : 9071
train acc:  0.90625
train loss:  0.21915605664253235
train gradient:  0.0991062037861522
iteration : 9072
train acc:  0.828125
train loss:  0.3344220519065857
train gradient:  0.2070903856497121
iteration : 9073
train acc:  0.8671875
train loss:  0.3257352411746979
train gradient:  0.17598658703736303
iteration : 9074
train acc:  0.8125
train loss:  0.39005911350250244
train gradient:  0.20022567675494884
iteration : 9075
train acc:  0.8125
train loss:  0.36060094833374023
train gradient:  0.19679674183326845
iteration : 9076
train acc:  0.859375
train loss:  0.28574618697166443
train gradient:  0.2481136529782043
iteration : 9077
train acc:  0.875
train loss:  0.32972458004951477
train gradient:  0.2618216311564383
iteration : 9078
train acc:  0.859375
train loss:  0.27688950300216675
train gradient:  0.13034494154028237
iteration : 9079
train acc:  0.8203125
train loss:  0.37793225049972534
train gradient:  0.4982776384876046
iteration : 9080
train acc:  0.8984375
train loss:  0.24656032025814056
train gradient:  0.1668841108464575
iteration : 9081
train acc:  0.8515625
train loss:  0.30294471979141235
train gradient:  0.1579859327784882
iteration : 9082
train acc:  0.875
train loss:  0.35617780685424805
train gradient:  0.18955492546306751
iteration : 9083
train acc:  0.875
train loss:  0.3719952404499054
train gradient:  0.16636660059028696
iteration : 9084
train acc:  0.84375
train loss:  0.3193513751029968
train gradient:  0.18679487607857898
iteration : 9085
train acc:  0.875
train loss:  0.33208751678466797
train gradient:  0.254633383069599
iteration : 9086
train acc:  0.8671875
train loss:  0.3281536400318146
train gradient:  0.26478646649253607
iteration : 9087
train acc:  0.8671875
train loss:  0.30872952938079834
train gradient:  0.15562622639689325
iteration : 9088
train acc:  0.8125
train loss:  0.46747201681137085
train gradient:  0.3655201602874661
iteration : 9089
train acc:  0.859375
train loss:  0.3558436632156372
train gradient:  0.20361737536699864
iteration : 9090
train acc:  0.875
train loss:  0.26399993896484375
train gradient:  0.2451071765963225
iteration : 9091
train acc:  0.859375
train loss:  0.3289716839790344
train gradient:  0.17328755361684486
iteration : 9092
train acc:  0.828125
train loss:  0.3266173005104065
train gradient:  0.16818870597375565
iteration : 9093
train acc:  0.9140625
train loss:  0.2606908977031708
train gradient:  0.14436914384137234
iteration : 9094
train acc:  0.8671875
train loss:  0.35121434926986694
train gradient:  0.2214384514013022
iteration : 9095
train acc:  0.828125
train loss:  0.448531836271286
train gradient:  0.2762319070366014
iteration : 9096
train acc:  0.8359375
train loss:  0.3617403209209442
train gradient:  0.27444668613781803
iteration : 9097
train acc:  0.8984375
train loss:  0.25973454117774963
train gradient:  0.12218454363082598
iteration : 9098
train acc:  0.828125
train loss:  0.4131195545196533
train gradient:  0.2763861875128705
iteration : 9099
train acc:  0.875
train loss:  0.302677720785141
train gradient:  0.19159655054748265
iteration : 9100
train acc:  0.84375
train loss:  0.32057076692581177
train gradient:  0.17062993080781866
iteration : 9101
train acc:  0.859375
train loss:  0.3531172275543213
train gradient:  0.17083475308476725
iteration : 9102
train acc:  0.8828125
train loss:  0.26088741421699524
train gradient:  0.1415132103478326
iteration : 9103
train acc:  0.84375
train loss:  0.3364912271499634
train gradient:  0.20224991621015903
iteration : 9104
train acc:  0.9140625
train loss:  0.24904152750968933
train gradient:  0.12207042814925012
iteration : 9105
train acc:  0.8515625
train loss:  0.27981966733932495
train gradient:  0.11041061652022212
iteration : 9106
train acc:  0.8515625
train loss:  0.29037022590637207
train gradient:  0.15837417984308094
iteration : 9107
train acc:  0.9296875
train loss:  0.19434858858585358
train gradient:  0.08769002555250818
iteration : 9108
train acc:  0.828125
train loss:  0.30606716871261597
train gradient:  0.15642614117462486
iteration : 9109
train acc:  0.8671875
train loss:  0.3075765073299408
train gradient:  0.17027358679230514
iteration : 9110
train acc:  0.8125
train loss:  0.39811229705810547
train gradient:  0.2253057427589879
iteration : 9111
train acc:  0.9296875
train loss:  0.2666841447353363
train gradient:  0.13100432337845025
iteration : 9112
train acc:  0.8515625
train loss:  0.3410646319389343
train gradient:  0.24054257766142167
iteration : 9113
train acc:  0.8828125
train loss:  0.27474838495254517
train gradient:  0.11873750978110857
iteration : 9114
train acc:  0.84375
train loss:  0.3736216425895691
train gradient:  0.1887400964432967
iteration : 9115
train acc:  0.8125
train loss:  0.3886352777481079
train gradient:  0.3245860770940525
iteration : 9116
train acc:  0.84375
train loss:  0.3060398995876312
train gradient:  0.15280543558853288
iteration : 9117
train acc:  0.8671875
train loss:  0.33398741483688354
train gradient:  0.16288743696610242
iteration : 9118
train acc:  0.890625
train loss:  0.29603150486946106
train gradient:  0.11926171533843852
iteration : 9119
train acc:  0.8671875
train loss:  0.33397620916366577
train gradient:  0.22018090504730856
iteration : 9120
train acc:  0.890625
train loss:  0.30886024236679077
train gradient:  0.1971275836682696
iteration : 9121
train acc:  0.8515625
train loss:  0.3463558256626129
train gradient:  0.1847158519245941
iteration : 9122
train acc:  0.84375
train loss:  0.35134798288345337
train gradient:  0.19374473233136674
iteration : 9123
train acc:  0.8203125
train loss:  0.33543264865875244
train gradient:  0.17847675763982096
iteration : 9124
train acc:  0.8359375
train loss:  0.30921703577041626
train gradient:  0.17471922535975215
iteration : 9125
train acc:  0.8828125
train loss:  0.2960304319858551
train gradient:  0.15646880829099008
iteration : 9126
train acc:  0.875
train loss:  0.38384073972702026
train gradient:  0.26028057190675724
iteration : 9127
train acc:  0.84375
train loss:  0.3807627260684967
train gradient:  0.23086667944946018
iteration : 9128
train acc:  0.796875
train loss:  0.41128987073898315
train gradient:  0.26448628935992496
iteration : 9129
train acc:  0.8203125
train loss:  0.33852556347846985
train gradient:  0.2417198761800649
iteration : 9130
train acc:  0.84375
train loss:  0.325215220451355
train gradient:  0.19868725433082488
iteration : 9131
train acc:  0.8984375
train loss:  0.2513148784637451
train gradient:  0.12801428068067977
iteration : 9132
train acc:  0.8203125
train loss:  0.453931599855423
train gradient:  0.2836022642471448
iteration : 9133
train acc:  0.875
train loss:  0.33410823345184326
train gradient:  0.2371999432774931
iteration : 9134
train acc:  0.8515625
train loss:  0.3145151734352112
train gradient:  0.20188749258683314
iteration : 9135
train acc:  0.796875
train loss:  0.46054476499557495
train gradient:  0.38176418670746615
iteration : 9136
train acc:  0.8515625
train loss:  0.3318423330783844
train gradient:  0.1682199572371269
iteration : 9137
train acc:  0.84375
train loss:  0.3117067813873291
train gradient:  0.2020072030408695
iteration : 9138
train acc:  0.859375
train loss:  0.37435245513916016
train gradient:  0.20783085027368137
iteration : 9139
train acc:  0.859375
train loss:  0.30557286739349365
train gradient:  0.1703088683120868
iteration : 9140
train acc:  0.890625
train loss:  0.2700127065181732
train gradient:  0.09139773125613344
iteration : 9141
train acc:  0.859375
train loss:  0.32168328762054443
train gradient:  0.19779777288163597
iteration : 9142
train acc:  0.8515625
train loss:  0.29085028171539307
train gradient:  0.11258169848235854
iteration : 9143
train acc:  0.8828125
train loss:  0.3034972548484802
train gradient:  0.19567474598930967
iteration : 9144
train acc:  0.84375
train loss:  0.37534642219543457
train gradient:  0.19399862145117297
iteration : 9145
train acc:  0.8828125
train loss:  0.30514559149742126
train gradient:  0.1633111910215016
iteration : 9146
train acc:  0.875
train loss:  0.3894636332988739
train gradient:  0.22250356438384858
iteration : 9147
train acc:  0.8359375
train loss:  0.3502359390258789
train gradient:  0.18707109250754794
iteration : 9148
train acc:  0.8515625
train loss:  0.3405270576477051
train gradient:  0.2395543867504516
iteration : 9149
train acc:  0.859375
train loss:  0.33021780848503113
train gradient:  0.17566359375376606
iteration : 9150
train acc:  0.8828125
train loss:  0.31466785073280334
train gradient:  0.19603297938002145
iteration : 9151
train acc:  0.7734375
train loss:  0.4487379193305969
train gradient:  0.26096173380477705
iteration : 9152
train acc:  0.8984375
train loss:  0.246412456035614
train gradient:  0.15836097690309328
iteration : 9153
train acc:  0.8125
train loss:  0.3713495433330536
train gradient:  0.17931394939395548
iteration : 9154
train acc:  0.796875
train loss:  0.4562796652317047
train gradient:  0.28732414564563236
iteration : 9155
train acc:  0.8515625
train loss:  0.34006983041763306
train gradient:  0.17143254109507053
iteration : 9156
train acc:  0.8203125
train loss:  0.38946467638015747
train gradient:  0.2641460452929335
iteration : 9157
train acc:  0.8671875
train loss:  0.36416035890579224
train gradient:  0.2526866085414492
iteration : 9158
train acc:  0.84375
train loss:  0.3876534700393677
train gradient:  0.19801925693460268
iteration : 9159
train acc:  0.8515625
train loss:  0.32106173038482666
train gradient:  0.1736821217578053
iteration : 9160
train acc:  0.875
train loss:  0.3031671643257141
train gradient:  0.14200142479140693
iteration : 9161
train acc:  0.859375
train loss:  0.32598650455474854
train gradient:  0.14415326621279545
iteration : 9162
train acc:  0.84375
train loss:  0.4225084185600281
train gradient:  0.2527003312546673
iteration : 9163
train acc:  0.890625
train loss:  0.28477734327316284
train gradient:  0.1628806387466849
iteration : 9164
train acc:  0.8203125
train loss:  0.43283766508102417
train gradient:  0.2745049471984799
iteration : 9165
train acc:  0.84375
train loss:  0.341541051864624
train gradient:  0.1569026398593999
iteration : 9166
train acc:  0.8359375
train loss:  0.395990252494812
train gradient:  0.24847765772111255
iteration : 9167
train acc:  0.8359375
train loss:  0.32481032609939575
train gradient:  0.12352510397167889
iteration : 9168
train acc:  0.8125
train loss:  0.3905777335166931
train gradient:  0.2112543656283803
iteration : 9169
train acc:  0.8828125
train loss:  0.3247845768928528
train gradient:  0.13728814558998903
iteration : 9170
train acc:  0.84375
train loss:  0.3106665313243866
train gradient:  0.11712435713562615
iteration : 9171
train acc:  0.8203125
train loss:  0.28524503111839294
train gradient:  0.14176753842789652
iteration : 9172
train acc:  0.828125
train loss:  0.38211995363235474
train gradient:  0.15838459565432547
iteration : 9173
train acc:  0.8125
train loss:  0.41136857867240906
train gradient:  0.20026753956046187
iteration : 9174
train acc:  0.84375
train loss:  0.32841378450393677
train gradient:  0.20266755761172564
iteration : 9175
train acc:  0.8828125
train loss:  0.32158708572387695
train gradient:  0.12234339353887831
iteration : 9176
train acc:  0.8046875
train loss:  0.34272652864456177
train gradient:  0.19086360000935088
iteration : 9177
train acc:  0.8046875
train loss:  0.3931644558906555
train gradient:  0.18270132893259186
iteration : 9178
train acc:  0.8671875
train loss:  0.29568374156951904
train gradient:  0.12370623765666129
iteration : 9179
train acc:  0.8984375
train loss:  0.35392820835113525
train gradient:  0.21883119201675846
iteration : 9180
train acc:  0.8046875
train loss:  0.3456171751022339
train gradient:  0.16428736546760292
iteration : 9181
train acc:  0.828125
train loss:  0.32038992643356323
train gradient:  0.16943570601590685
iteration : 9182
train acc:  0.84375
train loss:  0.33553725481033325
train gradient:  0.1483591082516544
iteration : 9183
train acc:  0.8359375
train loss:  0.2916933298110962
train gradient:  0.1375168106474809
iteration : 9184
train acc:  0.84375
train loss:  0.37140926718711853
train gradient:  0.17524690016938327
iteration : 9185
train acc:  0.8984375
train loss:  0.3091695010662079
train gradient:  0.13151410774939834
iteration : 9186
train acc:  0.8515625
train loss:  0.33634719252586365
train gradient:  0.19342138602978753
iteration : 9187
train acc:  0.8359375
train loss:  0.32286036014556885
train gradient:  0.1631454699018846
iteration : 9188
train acc:  0.859375
train loss:  0.32728415727615356
train gradient:  0.14460252209267552
iteration : 9189
train acc:  0.8828125
train loss:  0.2932698726654053
train gradient:  0.19550172743098207
iteration : 9190
train acc:  0.84375
train loss:  0.4194033741950989
train gradient:  0.2030529301943268
iteration : 9191
train acc:  0.84375
train loss:  0.3965412974357605
train gradient:  0.2877198760185864
iteration : 9192
train acc:  0.8984375
train loss:  0.35865986347198486
train gradient:  0.22522452666705461
iteration : 9193
train acc:  0.8046875
train loss:  0.362693190574646
train gradient:  0.160667052837348
iteration : 9194
train acc:  0.84375
train loss:  0.40132951736450195
train gradient:  0.23035851238757388
iteration : 9195
train acc:  0.84375
train loss:  0.2818165421485901
train gradient:  0.10568089403942343
iteration : 9196
train acc:  0.8671875
train loss:  0.3309287428855896
train gradient:  0.2645419768690966
iteration : 9197
train acc:  0.84375
train loss:  0.32664620876312256
train gradient:  0.12722770514625031
iteration : 9198
train acc:  0.875
train loss:  0.27993321418762207
train gradient:  0.11570001767465421
iteration : 9199
train acc:  0.8671875
train loss:  0.33160001039505005
train gradient:  0.12148614886718799
iteration : 9200
train acc:  0.875
train loss:  0.3055160641670227
train gradient:  0.12864449359076757
iteration : 9201
train acc:  0.8515625
train loss:  0.3163329064846039
train gradient:  0.12645772478302403
iteration : 9202
train acc:  0.8359375
train loss:  0.35995879769325256
train gradient:  0.18853352468802972
iteration : 9203
train acc:  0.828125
train loss:  0.37171265482902527
train gradient:  0.20990728480005286
iteration : 9204
train acc:  0.8984375
train loss:  0.25829529762268066
train gradient:  0.10442035236394888
iteration : 9205
train acc:  0.84375
train loss:  0.3172411620616913
train gradient:  0.15719451400125972
iteration : 9206
train acc:  0.90625
train loss:  0.24787402153015137
train gradient:  0.09376308738729834
iteration : 9207
train acc:  0.8671875
train loss:  0.33137351274490356
train gradient:  0.21649913856777786
iteration : 9208
train acc:  0.828125
train loss:  0.3583027124404907
train gradient:  0.17821285800635372
iteration : 9209
train acc:  0.90625
train loss:  0.2559245824813843
train gradient:  0.12351589915777404
iteration : 9210
train acc:  0.8515625
train loss:  0.34626805782318115
train gradient:  0.23580109343423977
iteration : 9211
train acc:  0.9140625
train loss:  0.24795572459697723
train gradient:  0.09085010383758144
iteration : 9212
train acc:  0.8515625
train loss:  0.37772735953330994
train gradient:  0.19885270108621264
iteration : 9213
train acc:  0.8671875
train loss:  0.31273722648620605
train gradient:  0.12423749386736527
iteration : 9214
train acc:  0.8984375
train loss:  0.24705395102500916
train gradient:  0.13247555661016527
iteration : 9215
train acc:  0.8515625
train loss:  0.34872132539749146
train gradient:  0.21840975849272687
iteration : 9216
train acc:  0.8359375
train loss:  0.38542306423187256
train gradient:  0.23523915078426294
iteration : 9217
train acc:  0.890625
train loss:  0.25308653712272644
train gradient:  0.11124532975830641
iteration : 9218
train acc:  0.8515625
train loss:  0.32902318239212036
train gradient:  0.16334008317386903
iteration : 9219
train acc:  0.828125
train loss:  0.37690162658691406
train gradient:  0.18385744582487099
iteration : 9220
train acc:  0.8515625
train loss:  0.3230832815170288
train gradient:  0.16582669186907645
iteration : 9221
train acc:  0.8359375
train loss:  0.34243515133857727
train gradient:  0.148778694930559
iteration : 9222
train acc:  0.8515625
train loss:  0.3028866648674011
train gradient:  0.1431233052596337
iteration : 9223
train acc:  0.890625
train loss:  0.28514713048934937
train gradient:  0.14147701667070406
iteration : 9224
train acc:  0.8359375
train loss:  0.37903404235839844
train gradient:  0.34219824657966086
iteration : 9225
train acc:  0.8046875
train loss:  0.37739959359169006
train gradient:  0.20957263546898813
iteration : 9226
train acc:  0.8359375
train loss:  0.4040488600730896
train gradient:  0.25506729609659107
iteration : 9227
train acc:  0.8671875
train loss:  0.40121158957481384
train gradient:  0.2155505509036817
iteration : 9228
train acc:  0.8828125
train loss:  0.2723846435546875
train gradient:  0.19053110834068898
iteration : 9229
train acc:  0.8671875
train loss:  0.37357479333877563
train gradient:  0.17691146582231576
iteration : 9230
train acc:  0.8515625
train loss:  0.33148056268692017
train gradient:  0.1399278547390705
iteration : 9231
train acc:  0.8203125
train loss:  0.37332800030708313
train gradient:  0.20035089084961416
iteration : 9232
train acc:  0.8984375
train loss:  0.30842769145965576
train gradient:  0.1339429714537192
iteration : 9233
train acc:  0.8203125
train loss:  0.31806570291519165
train gradient:  0.1747512027069198
iteration : 9234
train acc:  0.84375
train loss:  0.3771628141403198
train gradient:  0.20040151659218527
iteration : 9235
train acc:  0.859375
train loss:  0.3594568073749542
train gradient:  0.21980234136472004
iteration : 9236
train acc:  0.84375
train loss:  0.2922152876853943
train gradient:  0.1471457759543369
iteration : 9237
train acc:  0.859375
train loss:  0.3211612105369568
train gradient:  0.1969890304622811
iteration : 9238
train acc:  0.859375
train loss:  0.3969413936138153
train gradient:  0.20583227465489348
iteration : 9239
train acc:  0.7890625
train loss:  0.40128225088119507
train gradient:  0.21919299136922407
iteration : 9240
train acc:  0.84375
train loss:  0.3138471245765686
train gradient:  0.15654070224672217
iteration : 9241
train acc:  0.8984375
train loss:  0.27638036012649536
train gradient:  0.17513185649821753
iteration : 9242
train acc:  0.828125
train loss:  0.41274282336235046
train gradient:  0.28185457724821805
iteration : 9243
train acc:  0.859375
train loss:  0.29131919145584106
train gradient:  0.1660314848969023
iteration : 9244
train acc:  0.84375
train loss:  0.35242974758148193
train gradient:  0.23006600883283929
iteration : 9245
train acc:  0.8515625
train loss:  0.31706345081329346
train gradient:  0.19144059095045482
iteration : 9246
train acc:  0.8671875
train loss:  0.32462185621261597
train gradient:  0.1629289675005607
iteration : 9247
train acc:  0.8359375
train loss:  0.32751479744911194
train gradient:  0.2089671704453794
iteration : 9248
train acc:  0.8359375
train loss:  0.44585829973220825
train gradient:  0.37836056041633925
iteration : 9249
train acc:  0.9296875
train loss:  0.2230600118637085
train gradient:  0.10099549689902997
iteration : 9250
train acc:  0.875
train loss:  0.3132001757621765
train gradient:  0.16584757918561815
iteration : 9251
train acc:  0.7890625
train loss:  0.3610358238220215
train gradient:  0.22581786228299294
iteration : 9252
train acc:  0.8203125
train loss:  0.38929522037506104
train gradient:  0.20903134664105144
iteration : 9253
train acc:  0.796875
train loss:  0.4328879714012146
train gradient:  0.28708713677845527
iteration : 9254
train acc:  0.875
train loss:  0.29315686225891113
train gradient:  0.1225472089555361
iteration : 9255
train acc:  0.84375
train loss:  0.3464781939983368
train gradient:  0.1405614569891763
iteration : 9256
train acc:  0.8515625
train loss:  0.3165886402130127
train gradient:  0.16126392173210613
iteration : 9257
train acc:  0.875
train loss:  0.3207350969314575
train gradient:  0.09651233089031579
iteration : 9258
train acc:  0.8671875
train loss:  0.37919679284095764
train gradient:  0.2590886956456875
iteration : 9259
train acc:  0.859375
train loss:  0.290492445230484
train gradient:  0.1947333132255668
iteration : 9260
train acc:  0.8515625
train loss:  0.3165522813796997
train gradient:  0.16958383872720761
iteration : 9261
train acc:  0.828125
train loss:  0.3492696285247803
train gradient:  0.219068101208447
iteration : 9262
train acc:  0.8671875
train loss:  0.3322402238845825
train gradient:  0.1753038552338765
iteration : 9263
train acc:  0.8828125
train loss:  0.30276453495025635
train gradient:  0.13088847609733711
iteration : 9264
train acc:  0.8125
train loss:  0.4189021587371826
train gradient:  0.22838332043711054
iteration : 9265
train acc:  0.8671875
train loss:  0.3214045464992523
train gradient:  0.14471660462338762
iteration : 9266
train acc:  0.90625
train loss:  0.2519352436065674
train gradient:  0.13182041199757527
iteration : 9267
train acc:  0.9140625
train loss:  0.24093881249427795
train gradient:  0.1327285233454899
iteration : 9268
train acc:  0.7734375
train loss:  0.47956836223602295
train gradient:  0.3387041378499057
iteration : 9269
train acc:  0.78125
train loss:  0.4029942750930786
train gradient:  0.21844404224068048
iteration : 9270
train acc:  0.9140625
train loss:  0.2351606786251068
train gradient:  0.09248643058593498
iteration : 9271
train acc:  0.8515625
train loss:  0.3438275456428528
train gradient:  0.17586708603186546
iteration : 9272
train acc:  0.859375
train loss:  0.33928924798965454
train gradient:  0.1967733647268966
iteration : 9273
train acc:  0.859375
train loss:  0.36894679069519043
train gradient:  0.4348664005824958
iteration : 9274
train acc:  0.859375
train loss:  0.39002758264541626
train gradient:  0.28789461397817445
iteration : 9275
train acc:  0.859375
train loss:  0.31354525685310364
train gradient:  0.18296553791529893
iteration : 9276
train acc:  0.8359375
train loss:  0.3575587868690491
train gradient:  0.18901425765758567
iteration : 9277
train acc:  0.84375
train loss:  0.36927056312561035
train gradient:  0.19070737300948137
iteration : 9278
train acc:  0.8671875
train loss:  0.27375495433807373
train gradient:  0.12860941876897175
iteration : 9279
train acc:  0.859375
train loss:  0.2958192229270935
train gradient:  0.18446703043308377
iteration : 9280
train acc:  0.8671875
train loss:  0.3009493052959442
train gradient:  0.11496684410751011
iteration : 9281
train acc:  0.8203125
train loss:  0.32977724075317383
train gradient:  0.20547177261751917
iteration : 9282
train acc:  0.8671875
train loss:  0.2962232232093811
train gradient:  0.16325787058414154
iteration : 9283
train acc:  0.890625
train loss:  0.2972591519355774
train gradient:  0.1433310539414341
iteration : 9284
train acc:  0.875
train loss:  0.2847915291786194
train gradient:  0.14410457847524039
iteration : 9285
train acc:  0.84375
train loss:  0.4346311092376709
train gradient:  0.22967384869466495
iteration : 9286
train acc:  0.8046875
train loss:  0.39059847593307495
train gradient:  0.19443000726948156
iteration : 9287
train acc:  0.875
train loss:  0.25503072142601013
train gradient:  0.1275225954409017
iteration : 9288
train acc:  0.8203125
train loss:  0.3849974274635315
train gradient:  0.17203637844450764
iteration : 9289
train acc:  0.875
train loss:  0.29422351717948914
train gradient:  0.13024193164797157
iteration : 9290
train acc:  0.890625
train loss:  0.2565855383872986
train gradient:  0.13402256633221907
iteration : 9291
train acc:  0.8359375
train loss:  0.34851229190826416
train gradient:  0.17047227331743647
iteration : 9292
train acc:  0.8515625
train loss:  0.2912009358406067
train gradient:  0.1177201460490689
iteration : 9293
train acc:  0.8984375
train loss:  0.28032439947128296
train gradient:  0.11218616735578069
iteration : 9294
train acc:  0.90625
train loss:  0.2530907094478607
train gradient:  0.13050529614526118
iteration : 9295
train acc:  0.84375
train loss:  0.3092651665210724
train gradient:  0.257439582721851
iteration : 9296
train acc:  0.8203125
train loss:  0.37953290343284607
train gradient:  0.19712735873639342
iteration : 9297
train acc:  0.890625
train loss:  0.2794053852558136
train gradient:  0.13944052006715663
iteration : 9298
train acc:  0.8671875
train loss:  0.3225356936454773
train gradient:  0.14619459690918019
iteration : 9299
train acc:  0.8515625
train loss:  0.3985704183578491
train gradient:  0.25296595749324235
iteration : 9300
train acc:  0.8359375
train loss:  0.3610765337944031
train gradient:  0.22471819397898027
iteration : 9301
train acc:  0.84375
train loss:  0.31964433193206787
train gradient:  0.21766073395898877
iteration : 9302
train acc:  0.8203125
train loss:  0.348036527633667
train gradient:  0.19165383288472415
iteration : 9303
train acc:  0.8125
train loss:  0.41328322887420654
train gradient:  0.31677802227192325
iteration : 9304
train acc:  0.828125
train loss:  0.3924329876899719
train gradient:  0.21111629219293523
iteration : 9305
train acc:  0.84375
train loss:  0.32387563586235046
train gradient:  0.25038402781200747
iteration : 9306
train acc:  0.78125
train loss:  0.43068474531173706
train gradient:  0.23386412829415712
iteration : 9307
train acc:  0.828125
train loss:  0.3481045663356781
train gradient:  0.18820015210356028
iteration : 9308
train acc:  0.8125
train loss:  0.3999735414981842
train gradient:  0.27938038810336546
iteration : 9309
train acc:  0.875
train loss:  0.3002956509590149
train gradient:  0.1655758114606085
iteration : 9310
train acc:  0.8515625
train loss:  0.3569888472557068
train gradient:  0.2475404290464383
iteration : 9311
train acc:  0.859375
train loss:  0.28988543152809143
train gradient:  0.11814716331590505
iteration : 9312
train acc:  0.84375
train loss:  0.3059912919998169
train gradient:  0.10430731840594332
iteration : 9313
train acc:  0.828125
train loss:  0.42303267121315
train gradient:  0.19998682260100895
iteration : 9314
train acc:  0.875
train loss:  0.33642005920410156
train gradient:  0.13914920083883325
iteration : 9315
train acc:  0.859375
train loss:  0.31924575567245483
train gradient:  0.13198061364331565
iteration : 9316
train acc:  0.8828125
train loss:  0.2875654101371765
train gradient:  0.13142439683093
iteration : 9317
train acc:  0.890625
train loss:  0.27868717908859253
train gradient:  0.14163949381752022
iteration : 9318
train acc:  0.8828125
train loss:  0.30835485458374023
train gradient:  0.14835331933829782
iteration : 9319
train acc:  0.7890625
train loss:  0.4690943956375122
train gradient:  0.3349812818523257
iteration : 9320
train acc:  0.78125
train loss:  0.40222451090812683
train gradient:  0.2372070027119756
iteration : 9321
train acc:  0.8984375
train loss:  0.3026915192604065
train gradient:  0.12100132309185302
iteration : 9322
train acc:  0.8828125
train loss:  0.36364156007766724
train gradient:  0.2017822050997786
iteration : 9323
train acc:  0.875
train loss:  0.2727341651916504
train gradient:  0.18285746803195635
iteration : 9324
train acc:  0.8984375
train loss:  0.2478673756122589
train gradient:  0.08908810520427243
iteration : 9325
train acc:  0.8984375
train loss:  0.2514936923980713
train gradient:  0.12819259293922897
iteration : 9326
train acc:  0.828125
train loss:  0.35362568497657776
train gradient:  0.2137345709479482
iteration : 9327
train acc:  0.828125
train loss:  0.3673632740974426
train gradient:  0.2298504490191703
iteration : 9328
train acc:  0.8828125
train loss:  0.26796257495880127
train gradient:  0.12542316222736627
iteration : 9329
train acc:  0.8203125
train loss:  0.38766753673553467
train gradient:  0.2782341138359375
iteration : 9330
train acc:  0.8828125
train loss:  0.2811076045036316
train gradient:  0.17640610419190692
iteration : 9331
train acc:  0.84375
train loss:  0.38157808780670166
train gradient:  0.18728652392342832
iteration : 9332
train acc:  0.828125
train loss:  0.374457985162735
train gradient:  0.1972186696385796
iteration : 9333
train acc:  0.859375
train loss:  0.35234320163726807
train gradient:  0.256954294574214
iteration : 9334
train acc:  0.875
train loss:  0.3221479654312134
train gradient:  0.21948968963500726
iteration : 9335
train acc:  0.8828125
train loss:  0.29072561860084534
train gradient:  0.26397809815064277
iteration : 9336
train acc:  0.8359375
train loss:  0.3210826516151428
train gradient:  0.17808587104728285
iteration : 9337
train acc:  0.8125
train loss:  0.43691664934158325
train gradient:  0.1840739141830617
iteration : 9338
train acc:  0.8515625
train loss:  0.31981414556503296
train gradient:  0.18496463135793784
iteration : 9339
train acc:  0.9140625
train loss:  0.2378236949443817
train gradient:  0.13033515459395867
iteration : 9340
train acc:  0.8203125
train loss:  0.408163458108902
train gradient:  0.20018165215185046
iteration : 9341
train acc:  0.8359375
train loss:  0.46479976177215576
train gradient:  0.41986780716665817
iteration : 9342
train acc:  0.796875
train loss:  0.41109833121299744
train gradient:  0.2930671711570326
iteration : 9343
train acc:  0.84375
train loss:  0.32867223024368286
train gradient:  0.17357520552172453
iteration : 9344
train acc:  0.8828125
train loss:  0.2942315340042114
train gradient:  0.11402959966838079
iteration : 9345
train acc:  0.8515625
train loss:  0.3223036527633667
train gradient:  0.11028877257542039
iteration : 9346
train acc:  0.875
train loss:  0.3235737085342407
train gradient:  0.1542946432521332
iteration : 9347
train acc:  0.8125
train loss:  0.41118550300598145
train gradient:  0.25699158739606703
iteration : 9348
train acc:  0.8828125
train loss:  0.2865328788757324
train gradient:  0.12634460997341374
iteration : 9349
train acc:  0.859375
train loss:  0.307983934879303
train gradient:  0.152381908081286
iteration : 9350
train acc:  0.8515625
train loss:  0.31507956981658936
train gradient:  0.18048729407566355
iteration : 9351
train acc:  0.90625
train loss:  0.29932528734207153
train gradient:  0.11465076235015147
iteration : 9352
train acc:  0.84375
train loss:  0.3174194097518921
train gradient:  0.1223617775171371
iteration : 9353
train acc:  0.890625
train loss:  0.3266516923904419
train gradient:  0.13427790693284453
iteration : 9354
train acc:  0.8515625
train loss:  0.32264813780784607
train gradient:  0.16107776304319465
iteration : 9355
train acc:  0.828125
train loss:  0.3369435667991638
train gradient:  0.2695358505362119
iteration : 9356
train acc:  0.8828125
train loss:  0.2678823471069336
train gradient:  0.12396084171796848
iteration : 9357
train acc:  0.8515625
train loss:  0.38334327936172485
train gradient:  0.2432674370248824
iteration : 9358
train acc:  0.8984375
train loss:  0.2770693004131317
train gradient:  0.12471549680397918
iteration : 9359
train acc:  0.828125
train loss:  0.33908671140670776
train gradient:  0.16775491254805389
iteration : 9360
train acc:  0.875
train loss:  0.3107250928878784
train gradient:  0.13817365536553847
iteration : 9361
train acc:  0.84375
train loss:  0.3448757231235504
train gradient:  0.2057389361559263
iteration : 9362
train acc:  0.8046875
train loss:  0.43516460061073303
train gradient:  0.25031074986235885
iteration : 9363
train acc:  0.828125
train loss:  0.36058902740478516
train gradient:  0.19371804629184328
iteration : 9364
train acc:  0.8828125
train loss:  0.2989494800567627
train gradient:  0.12054914514512334
iteration : 9365
train acc:  0.84375
train loss:  0.3393433094024658
train gradient:  0.18944794967801054
iteration : 9366
train acc:  0.8046875
train loss:  0.3739916980266571
train gradient:  0.18630423292147313
iteration : 9367
train acc:  0.8671875
train loss:  0.29028794169425964
train gradient:  0.1166733341884301
iteration : 9368
train acc:  0.8984375
train loss:  0.268282949924469
train gradient:  0.16910242640874432
iteration : 9369
train acc:  0.875
train loss:  0.39161646366119385
train gradient:  0.2457231622633549
iteration : 9370
train acc:  0.890625
train loss:  0.26612192392349243
train gradient:  0.11132825563226402
iteration : 9371
train acc:  0.8671875
train loss:  0.33614856004714966
train gradient:  0.24221278716091146
iteration : 9372
train acc:  0.8671875
train loss:  0.3247203230857849
train gradient:  0.13330024022048395
iteration : 9373
train acc:  0.8359375
train loss:  0.3655751347541809
train gradient:  0.14192816225918
iteration : 9374
train acc:  0.8125
train loss:  0.40862980484962463
train gradient:  0.2462506431618419
iteration : 9375
train acc:  0.859375
train loss:  0.3034658432006836
train gradient:  0.17876783485728898
iteration : 9376
train acc:  0.8671875
train loss:  0.3712756633758545
train gradient:  0.25685884662475944
iteration : 9377
train acc:  0.8359375
train loss:  0.4432143568992615
train gradient:  0.23644353102272406
iteration : 9378
train acc:  0.9296875
train loss:  0.26748061180114746
train gradient:  0.08602918269058932
iteration : 9379
train acc:  0.8671875
train loss:  0.2812853455543518
train gradient:  0.1679248175841075
iteration : 9380
train acc:  0.8515625
train loss:  0.33084923028945923
train gradient:  0.1581032706064792
iteration : 9381
train acc:  0.8671875
train loss:  0.3301903009414673
train gradient:  0.2343471267791667
iteration : 9382
train acc:  0.8828125
train loss:  0.2720315456390381
train gradient:  0.16654332829871957
iteration : 9383
train acc:  0.84375
train loss:  0.3423864245414734
train gradient:  0.19582607141026193
iteration : 9384
train acc:  0.8828125
train loss:  0.27109724283218384
train gradient:  0.15038832192175966
iteration : 9385
train acc:  0.875
train loss:  0.28683412075042725
train gradient:  0.11403007744828095
iteration : 9386
train acc:  0.8359375
train loss:  0.3246656656265259
train gradient:  0.16380030140689167
iteration : 9387
train acc:  0.859375
train loss:  0.30323171615600586
train gradient:  0.17997453343081465
iteration : 9388
train acc:  0.859375
train loss:  0.3463221490383148
train gradient:  0.17706381705850094
iteration : 9389
train acc:  0.8046875
train loss:  0.3727988004684448
train gradient:  0.15805523512745928
iteration : 9390
train acc:  0.8671875
train loss:  0.3576478362083435
train gradient:  0.2826685176835528
iteration : 9391
train acc:  0.8515625
train loss:  0.3200759291648865
train gradient:  0.2512768364868978
iteration : 9392
train acc:  0.828125
train loss:  0.4045206904411316
train gradient:  0.27432153540386206
iteration : 9393
train acc:  0.8984375
train loss:  0.3007678985595703
train gradient:  0.17326526283162408
iteration : 9394
train acc:  0.8046875
train loss:  0.4070330560207367
train gradient:  0.22559671453477284
iteration : 9395
train acc:  0.8828125
train loss:  0.2578624486923218
train gradient:  0.09947968511471844
iteration : 9396
train acc:  0.890625
train loss:  0.26524728536605835
train gradient:  0.17643465932589922
iteration : 9397
train acc:  0.78125
train loss:  0.440738320350647
train gradient:  0.2581711118283807
iteration : 9398
train acc:  0.828125
train loss:  0.431349515914917
train gradient:  0.2951926023966707
iteration : 9399
train acc:  0.875
train loss:  0.28946584463119507
train gradient:  0.13548043302745932
iteration : 9400
train acc:  0.828125
train loss:  0.3478440046310425
train gradient:  0.17150623036528878
iteration : 9401
train acc:  0.859375
train loss:  0.34059953689575195
train gradient:  0.18491207790592004
iteration : 9402
train acc:  0.859375
train loss:  0.34456807374954224
train gradient:  0.14112759416664536
iteration : 9403
train acc:  0.859375
train loss:  0.3027082681655884
train gradient:  0.17772255555754368
iteration : 9404
train acc:  0.8671875
train loss:  0.3130074739456177
train gradient:  0.18968769740306818
iteration : 9405
train acc:  0.8671875
train loss:  0.30151399970054626
train gradient:  0.13569428221918176
iteration : 9406
train acc:  0.8203125
train loss:  0.4257100224494934
train gradient:  0.1908822095653171
iteration : 9407
train acc:  0.8671875
train loss:  0.3188241422176361
train gradient:  0.15816675770062752
iteration : 9408
train acc:  0.875
train loss:  0.27288368344306946
train gradient:  0.11820958488802
iteration : 9409
train acc:  0.8828125
train loss:  0.31211286783218384
train gradient:  0.15105788568700962
iteration : 9410
train acc:  0.8671875
train loss:  0.29478758573532104
train gradient:  0.12308346004428973
iteration : 9411
train acc:  0.90625
train loss:  0.30426251888275146
train gradient:  0.161969466404858
iteration : 9412
train acc:  0.796875
train loss:  0.3988974690437317
train gradient:  0.20708419263421407
iteration : 9413
train acc:  0.875
train loss:  0.2886818051338196
train gradient:  0.10579458543416068
iteration : 9414
train acc:  0.8671875
train loss:  0.3363983631134033
train gradient:  0.13856473656234497
iteration : 9415
train acc:  0.8828125
train loss:  0.3685375452041626
train gradient:  0.16777962010425934
iteration : 9416
train acc:  0.8359375
train loss:  0.3942154347896576
train gradient:  0.18810454489382178
iteration : 9417
train acc:  0.8671875
train loss:  0.3088458776473999
train gradient:  0.2378210427072499
iteration : 9418
train acc:  0.859375
train loss:  0.3212520480155945
train gradient:  0.14909531862016379
iteration : 9419
train acc:  0.8515625
train loss:  0.36936384439468384
train gradient:  0.18380493406736098
iteration : 9420
train acc:  0.8828125
train loss:  0.28895121812820435
train gradient:  0.17391557871603625
iteration : 9421
train acc:  0.890625
train loss:  0.28992342948913574
train gradient:  0.1607602281585128
iteration : 9422
train acc:  0.859375
train loss:  0.30793270468711853
train gradient:  0.16029740639251566
iteration : 9423
train acc:  0.8359375
train loss:  0.37441760301589966
train gradient:  0.17926716260333728
iteration : 9424
train acc:  0.828125
train loss:  0.38120949268341064
train gradient:  0.2776320671228214
iteration : 9425
train acc:  0.84375
train loss:  0.32793763279914856
train gradient:  0.19937106684118108
iteration : 9426
train acc:  0.859375
train loss:  0.2756311297416687
train gradient:  0.1715356724261241
iteration : 9427
train acc:  0.7890625
train loss:  0.40574562549591064
train gradient:  0.24930402369769855
iteration : 9428
train acc:  0.859375
train loss:  0.34416985511779785
train gradient:  0.20307359684753426
iteration : 9429
train acc:  0.8828125
train loss:  0.28263843059539795
train gradient:  0.12466762230149872
iteration : 9430
train acc:  0.8046875
train loss:  0.409857839345932
train gradient:  0.21784405433499734
iteration : 9431
train acc:  0.8671875
train loss:  0.28516775369644165
train gradient:  0.1798717136864379
iteration : 9432
train acc:  0.9140625
train loss:  0.264174222946167
train gradient:  0.17360538782593296
iteration : 9433
train acc:  0.84375
train loss:  0.3478561043739319
train gradient:  0.21530533287978137
iteration : 9434
train acc:  0.8515625
train loss:  0.351494699716568
train gradient:  0.1616512659321512
iteration : 9435
train acc:  0.84375
train loss:  0.34665054082870483
train gradient:  0.15454258945213922
iteration : 9436
train acc:  0.84375
train loss:  0.33737993240356445
train gradient:  0.20759038075625288
iteration : 9437
train acc:  0.8515625
train loss:  0.36773020029067993
train gradient:  0.1745787507985406
iteration : 9438
train acc:  0.8515625
train loss:  0.2953757345676422
train gradient:  0.166327904437524
iteration : 9439
train acc:  0.8046875
train loss:  0.3839147388935089
train gradient:  0.27152606744513363
iteration : 9440
train acc:  0.890625
train loss:  0.2793537974357605
train gradient:  0.1584561761828524
iteration : 9441
train acc:  0.8515625
train loss:  0.3209732174873352
train gradient:  0.190193919804601
iteration : 9442
train acc:  0.859375
train loss:  0.31587499380111694
train gradient:  0.1754834767537241
iteration : 9443
train acc:  0.8125
train loss:  0.37556034326553345
train gradient:  0.20398918443625402
iteration : 9444
train acc:  0.828125
train loss:  0.3839661777019501
train gradient:  0.2692451396569913
iteration : 9445
train acc:  0.8515625
train loss:  0.3909413814544678
train gradient:  0.24302915343416892
iteration : 9446
train acc:  0.859375
train loss:  0.3740425705909729
train gradient:  0.17793497173422712
iteration : 9447
train acc:  0.8203125
train loss:  0.33786651492118835
train gradient:  0.16094241143550087
iteration : 9448
train acc:  0.9140625
train loss:  0.23533137142658234
train gradient:  0.09781613860936704
iteration : 9449
train acc:  0.84375
train loss:  0.316349059343338
train gradient:  0.10318702583074729
iteration : 9450
train acc:  0.8828125
train loss:  0.27707064151763916
train gradient:  0.14673313900836274
iteration : 9451
train acc:  0.8671875
train loss:  0.31658992171287537
train gradient:  0.1462536874143836
iteration : 9452
train acc:  0.84375
train loss:  0.33426639437675476
train gradient:  0.11302642572117315
iteration : 9453
train acc:  0.84375
train loss:  0.3130912184715271
train gradient:  0.15319898682942354
iteration : 9454
train acc:  0.8125
train loss:  0.35975417494773865
train gradient:  0.16549194838669623
iteration : 9455
train acc:  0.84375
train loss:  0.302249550819397
train gradient:  0.119442602767682
iteration : 9456
train acc:  0.8671875
train loss:  0.3323543071746826
train gradient:  0.12137304825923952
iteration : 9457
train acc:  0.9140625
train loss:  0.2743205428123474
train gradient:  0.141208041732559
iteration : 9458
train acc:  0.8359375
train loss:  0.3485753834247589
train gradient:  0.18397465409704944
iteration : 9459
train acc:  0.8359375
train loss:  0.3195369243621826
train gradient:  0.15364513894322096
iteration : 9460
train acc:  0.8671875
train loss:  0.3469828963279724
train gradient:  0.16709776855412634
iteration : 9461
train acc:  0.8828125
train loss:  0.29066869616508484
train gradient:  0.14254015427374328
iteration : 9462
train acc:  0.8515625
train loss:  0.3311462998390198
train gradient:  0.17501503798646378
iteration : 9463
train acc:  0.890625
train loss:  0.26381444931030273
train gradient:  0.13189284189618833
iteration : 9464
train acc:  0.8828125
train loss:  0.2797037661075592
train gradient:  0.08551617761254122
iteration : 9465
train acc:  0.890625
train loss:  0.2705830931663513
train gradient:  0.2174424681911049
iteration : 9466
train acc:  0.8828125
train loss:  0.2723477780818939
train gradient:  0.12960799208348772
iteration : 9467
train acc:  0.859375
train loss:  0.34455573558807373
train gradient:  0.22171907587991768
iteration : 9468
train acc:  0.8046875
train loss:  0.38884979486465454
train gradient:  0.20609494840400114
iteration : 9469
train acc:  0.8359375
train loss:  0.36055949330329895
train gradient:  0.19756245007718026
iteration : 9470
train acc:  0.8515625
train loss:  0.3530169725418091
train gradient:  0.19560098431407674
iteration : 9471
train acc:  0.8671875
train loss:  0.30698758363723755
train gradient:  0.18719259714076225
iteration : 9472
train acc:  0.8671875
train loss:  0.31059500575065613
train gradient:  0.12564620052005815
iteration : 9473
train acc:  0.875
train loss:  0.33173978328704834
train gradient:  0.15655547359346095
iteration : 9474
train acc:  0.8125
train loss:  0.39250099658966064
train gradient:  0.34824965309183353
iteration : 9475
train acc:  0.8671875
train loss:  0.32757341861724854
train gradient:  0.18787927000533386
iteration : 9476
train acc:  0.90625
train loss:  0.27256089448928833
train gradient:  0.10854159133420513
iteration : 9477
train acc:  0.875
train loss:  0.32748377323150635
train gradient:  0.1390130418186159
iteration : 9478
train acc:  0.8515625
train loss:  0.33401721715927124
train gradient:  0.15620147249442995
iteration : 9479
train acc:  0.7890625
train loss:  0.4175841510295868
train gradient:  0.20213142098240425
iteration : 9480
train acc:  0.84375
train loss:  0.32867133617401123
train gradient:  0.2632562296512826
iteration : 9481
train acc:  0.8203125
train loss:  0.42540764808654785
train gradient:  0.5795725009532734
iteration : 9482
train acc:  0.859375
train loss:  0.3048752546310425
train gradient:  0.22596577963805445
iteration : 9483
train acc:  0.9375
train loss:  0.23202559351921082
train gradient:  0.11775169742895966
iteration : 9484
train acc:  0.8984375
train loss:  0.25840383768081665
train gradient:  0.1389062104158315
iteration : 9485
train acc:  0.84375
train loss:  0.3396565616130829
train gradient:  0.12449452126852534
iteration : 9486
train acc:  0.8671875
train loss:  0.28569382429122925
train gradient:  0.101055123361577
iteration : 9487
train acc:  0.890625
train loss:  0.27237194776535034
train gradient:  0.11154861145539229
iteration : 9488
train acc:  0.8515625
train loss:  0.32230430841445923
train gradient:  0.1542027110118704
iteration : 9489
train acc:  0.90625
train loss:  0.2763877213001251
train gradient:  0.12386006745417227
iteration : 9490
train acc:  0.859375
train loss:  0.38582509756088257
train gradient:  0.23837120452531013
iteration : 9491
train acc:  0.875
train loss:  0.31173884868621826
train gradient:  0.17716641891496204
iteration : 9492
train acc:  0.84375
train loss:  0.35528743267059326
train gradient:  0.16402530416787742
iteration : 9493
train acc:  0.8984375
train loss:  0.26604485511779785
train gradient:  0.13818777137518595
iteration : 9494
train acc:  0.8515625
train loss:  0.31680622696876526
train gradient:  0.20876752211821842
iteration : 9495
train acc:  0.828125
train loss:  0.3713584542274475
train gradient:  0.20246196000780908
iteration : 9496
train acc:  0.875
train loss:  0.3038126528263092
train gradient:  0.13494741776709765
iteration : 9497
train acc:  0.7890625
train loss:  0.4533471465110779
train gradient:  0.37556951157480767
iteration : 9498
train acc:  0.828125
train loss:  0.398555189371109
train gradient:  0.2562155167214153
iteration : 9499
train acc:  0.8125
train loss:  0.41122132539749146
train gradient:  0.2624718027761516
iteration : 9500
train acc:  0.9453125
train loss:  0.27937763929367065
train gradient:  0.234585685477371
iteration : 9501
train acc:  0.890625
train loss:  0.2689669728279114
train gradient:  0.16041268865420605
iteration : 9502
train acc:  0.890625
train loss:  0.31887561082839966
train gradient:  0.1443107574879821
iteration : 9503
train acc:  0.9140625
train loss:  0.27190834283828735
train gradient:  0.1070465669714245
iteration : 9504
train acc:  0.8125
train loss:  0.4415346384048462
train gradient:  0.3725780609647214
iteration : 9505
train acc:  0.8203125
train loss:  0.3576779067516327
train gradient:  0.17879113859646567
iteration : 9506
train acc:  0.859375
train loss:  0.2939998507499695
train gradient:  0.13419741377381722
iteration : 9507
train acc:  0.84375
train loss:  0.3509126901626587
train gradient:  0.16635507474935718
iteration : 9508
train acc:  0.875
train loss:  0.32299721240997314
train gradient:  0.15150047893550264
iteration : 9509
train acc:  0.8984375
train loss:  0.26579371094703674
train gradient:  0.16821084321365914
iteration : 9510
train acc:  0.90625
train loss:  0.26157402992248535
train gradient:  0.1509232064166264
iteration : 9511
train acc:  0.8515625
train loss:  0.3607650101184845
train gradient:  0.18209953361932832
iteration : 9512
train acc:  0.84375
train loss:  0.32733380794525146
train gradient:  0.17837764350112806
iteration : 9513
train acc:  0.890625
train loss:  0.254310667514801
train gradient:  0.11365328323599841
iteration : 9514
train acc:  0.84375
train loss:  0.34480834007263184
train gradient:  0.18836971453059992
iteration : 9515
train acc:  0.90625
train loss:  0.2701593041419983
train gradient:  0.13045086785872181
iteration : 9516
train acc:  0.828125
train loss:  0.37509697675704956
train gradient:  0.1820011754193251
iteration : 9517
train acc:  0.8359375
train loss:  0.3272739350795746
train gradient:  0.17583932718711953
iteration : 9518
train acc:  0.9140625
train loss:  0.24985502660274506
train gradient:  0.146055450736787
iteration : 9519
train acc:  0.8828125
train loss:  0.286175936460495
train gradient:  0.1482750424715756
iteration : 9520
train acc:  0.78125
train loss:  0.40124237537384033
train gradient:  0.20593224068427124
iteration : 9521
train acc:  0.8671875
train loss:  0.32277870178222656
train gradient:  0.12281586177457933
iteration : 9522
train acc:  0.90625
train loss:  0.3442210853099823
train gradient:  0.13222223596353422
iteration : 9523
train acc:  0.828125
train loss:  0.3956247568130493
train gradient:  0.23394474002508417
iteration : 9524
train acc:  0.890625
train loss:  0.28279849886894226
train gradient:  0.1480714007944567
iteration : 9525
train acc:  0.859375
train loss:  0.3602023124694824
train gradient:  0.23697341666012522
iteration : 9526
train acc:  0.8359375
train loss:  0.41005271673202515
train gradient:  0.2506767652778778
iteration : 9527
train acc:  0.875
train loss:  0.31618136167526245
train gradient:  0.16660494047842123
iteration : 9528
train acc:  0.875
train loss:  0.30014318227767944
train gradient:  0.11199854127882451
iteration : 9529
train acc:  0.8515625
train loss:  0.28788381814956665
train gradient:  0.15111055978494908
iteration : 9530
train acc:  0.859375
train loss:  0.33506590127944946
train gradient:  0.18220135211276314
iteration : 9531
train acc:  0.8203125
train loss:  0.31858712434768677
train gradient:  0.13552595848680205
iteration : 9532
train acc:  0.859375
train loss:  0.3389434218406677
train gradient:  0.15103756151472453
iteration : 9533
train acc:  0.859375
train loss:  0.3804476857185364
train gradient:  0.172907120605714
iteration : 9534
train acc:  0.8671875
train loss:  0.35077163577079773
train gradient:  0.2047449928550412
iteration : 9535
train acc:  0.875
train loss:  0.3370339274406433
train gradient:  0.15158445578672186
iteration : 9536
train acc:  0.890625
train loss:  0.24782249331474304
train gradient:  0.10263320053185145
iteration : 9537
train acc:  0.84375
train loss:  0.36989957094192505
train gradient:  0.22408391380540038
iteration : 9538
train acc:  0.875
train loss:  0.27860361337661743
train gradient:  0.13360386871576396
iteration : 9539
train acc:  0.8515625
train loss:  0.33966290950775146
train gradient:  0.20091411269714418
iteration : 9540
train acc:  0.8046875
train loss:  0.3829266428947449
train gradient:  0.17909008133623674
iteration : 9541
train acc:  0.8828125
train loss:  0.2960321307182312
train gradient:  0.13673287566444517
iteration : 9542
train acc:  0.8359375
train loss:  0.4213731288909912
train gradient:  0.29278422443161656
iteration : 9543
train acc:  0.8515625
train loss:  0.3300432562828064
train gradient:  0.303228506008268
iteration : 9544
train acc:  0.7578125
train loss:  0.443156361579895
train gradient:  0.2801526839363951
iteration : 9545
train acc:  0.8671875
train loss:  0.3619149625301361
train gradient:  0.22398442108031746
iteration : 9546
train acc:  0.8359375
train loss:  0.3194003701210022
train gradient:  0.1569513574175198
iteration : 9547
train acc:  0.875
train loss:  0.35680800676345825
train gradient:  0.2710007867266191
iteration : 9548
train acc:  0.8828125
train loss:  0.28180059790611267
train gradient:  0.11807293289616376
iteration : 9549
train acc:  0.890625
train loss:  0.3150118887424469
train gradient:  0.12023184450870601
iteration : 9550
train acc:  0.8671875
train loss:  0.31846532225608826
train gradient:  0.12385030370347405
iteration : 9551
train acc:  0.8203125
train loss:  0.38433384895324707
train gradient:  0.3241868485763714
iteration : 9552
train acc:  0.828125
train loss:  0.303862601518631
train gradient:  0.12337942201070907
iteration : 9553
train acc:  0.890625
train loss:  0.27178698778152466
train gradient:  0.09877897788726439
iteration : 9554
train acc:  0.8359375
train loss:  0.35057079792022705
train gradient:  0.19924521887525198
iteration : 9555
train acc:  0.8515625
train loss:  0.35082879662513733
train gradient:  0.12959963501433314
iteration : 9556
train acc:  0.859375
train loss:  0.3309088349342346
train gradient:  0.11403798263633935
iteration : 9557
train acc:  0.8671875
train loss:  0.3034560978412628
train gradient:  0.1505458951696802
iteration : 9558
train acc:  0.890625
train loss:  0.2897258996963501
train gradient:  0.13215821918187426
iteration : 9559
train acc:  0.859375
train loss:  0.28585129976272583
train gradient:  0.12288272975101174
iteration : 9560
train acc:  0.8828125
train loss:  0.2925727963447571
train gradient:  0.1547528047058755
iteration : 9561
train acc:  0.7734375
train loss:  0.4151626527309418
train gradient:  0.206265887668668
iteration : 9562
train acc:  0.8515625
train loss:  0.31409528851509094
train gradient:  0.12166121801385776
iteration : 9563
train acc:  0.828125
train loss:  0.38048291206359863
train gradient:  0.14967988566333512
iteration : 9564
train acc:  0.890625
train loss:  0.29355183243751526
train gradient:  0.10371355988137551
iteration : 9565
train acc:  0.8828125
train loss:  0.2875499129295349
train gradient:  0.14001165118886186
iteration : 9566
train acc:  0.828125
train loss:  0.3770901560783386
train gradient:  0.2100675237193404
iteration : 9567
train acc:  0.8359375
train loss:  0.4316331744194031
train gradient:  0.2356133892422109
iteration : 9568
train acc:  0.8046875
train loss:  0.41512930393218994
train gradient:  0.25231833593960856
iteration : 9569
train acc:  0.8515625
train loss:  0.3135603368282318
train gradient:  0.14622870023432558
iteration : 9570
train acc:  0.8671875
train loss:  0.27734896540641785
train gradient:  0.12807990380041567
iteration : 9571
train acc:  0.890625
train loss:  0.2830272614955902
train gradient:  0.12226157720459947
iteration : 9572
train acc:  0.8671875
train loss:  0.4091578722000122
train gradient:  0.3002468013187084
iteration : 9573
train acc:  0.859375
train loss:  0.32941240072250366
train gradient:  0.1987867747700597
iteration : 9574
train acc:  0.8359375
train loss:  0.3240012526512146
train gradient:  0.1664523995274227
iteration : 9575
train acc:  0.8515625
train loss:  0.3359556198120117
train gradient:  0.19583630582290118
iteration : 9576
train acc:  0.875
train loss:  0.30782657861709595
train gradient:  0.11851731159910224
iteration : 9577
train acc:  0.8671875
train loss:  0.2858659327030182
train gradient:  0.11577797328303903
iteration : 9578
train acc:  0.8359375
train loss:  0.31535273790359497
train gradient:  0.15100840915768424
iteration : 9579
train acc:  0.8515625
train loss:  0.32002559304237366
train gradient:  0.22082035876238781
iteration : 9580
train acc:  0.8828125
train loss:  0.2871341109275818
train gradient:  0.2017939624620343
iteration : 9581
train acc:  0.7890625
train loss:  0.39031925797462463
train gradient:  0.2631152256228427
iteration : 9582
train acc:  0.8125
train loss:  0.40327900648117065
train gradient:  0.28689276204838077
iteration : 9583
train acc:  0.8515625
train loss:  0.34647244215011597
train gradient:  0.17960175908255283
iteration : 9584
train acc:  0.84375
train loss:  0.3432045578956604
train gradient:  0.17843689204766633
iteration : 9585
train acc:  0.828125
train loss:  0.3562079071998596
train gradient:  0.1391808404986629
iteration : 9586
train acc:  0.875
train loss:  0.28947317600250244
train gradient:  0.08948944526598973
iteration : 9587
train acc:  0.8046875
train loss:  0.42638665437698364
train gradient:  0.2007531651486424
iteration : 9588
train acc:  0.8828125
train loss:  0.3176003396511078
train gradient:  0.19597669509411364
iteration : 9589
train acc:  0.8671875
train loss:  0.2929422855377197
train gradient:  0.36836507651533607
iteration : 9590
train acc:  0.8828125
train loss:  0.31906038522720337
train gradient:  0.19455112010082823
iteration : 9591
train acc:  0.8515625
train loss:  0.40805113315582275
train gradient:  0.24757291798348288
iteration : 9592
train acc:  0.84375
train loss:  0.3302789330482483
train gradient:  0.16436908220663682
iteration : 9593
train acc:  0.7734375
train loss:  0.44742345809936523
train gradient:  0.2422468612667405
iteration : 9594
train acc:  0.90625
train loss:  0.28206586837768555
train gradient:  0.1109382826032851
iteration : 9595
train acc:  0.796875
train loss:  0.4133830666542053
train gradient:  0.21193612515449417
iteration : 9596
train acc:  0.828125
train loss:  0.42510950565338135
train gradient:  0.2827307527478258
iteration : 9597
train acc:  0.875
train loss:  0.26110267639160156
train gradient:  0.12345294621338568
iteration : 9598
train acc:  0.8359375
train loss:  0.3861849904060364
train gradient:  0.1890444348451004
iteration : 9599
train acc:  0.8828125
train loss:  0.3150530457496643
train gradient:  0.13212483410310275
iteration : 9600
train acc:  0.8671875
train loss:  0.3069300055503845
train gradient:  0.11071228331737894
iteration : 9601
train acc:  0.875
train loss:  0.29744917154312134
train gradient:  0.12169448818320165
iteration : 9602
train acc:  0.8359375
train loss:  0.3171033263206482
train gradient:  0.14661208618970292
iteration : 9603
train acc:  0.84375
train loss:  0.293874055147171
train gradient:  0.1759752129089064
iteration : 9604
train acc:  0.8984375
train loss:  0.30904367566108704
train gradient:  0.1169649356696331
iteration : 9605
train acc:  0.859375
train loss:  0.32044315338134766
train gradient:  0.20955640876994597
iteration : 9606
train acc:  0.828125
train loss:  0.3332226872444153
train gradient:  0.15785044277620636
iteration : 9607
train acc:  0.8671875
train loss:  0.33657652139663696
train gradient:  0.17733921118067847
iteration : 9608
train acc:  0.890625
train loss:  0.2815636992454529
train gradient:  0.11148760894313436
iteration : 9609
train acc:  0.8359375
train loss:  0.32344624400138855
train gradient:  0.20263470866973665
iteration : 9610
train acc:  0.890625
train loss:  0.30065494775772095
train gradient:  0.15545898267020286
iteration : 9611
train acc:  0.8515625
train loss:  0.3283076286315918
train gradient:  0.32983666755685875
iteration : 9612
train acc:  0.8046875
train loss:  0.35124021768569946
train gradient:  0.1373560828646158
iteration : 9613
train acc:  0.8125
train loss:  0.3896291255950928
train gradient:  0.26471475015607465
iteration : 9614
train acc:  0.84375
train loss:  0.3341471552848816
train gradient:  0.13745318656704214
iteration : 9615
train acc:  0.859375
train loss:  0.32356488704681396
train gradient:  0.15087967834887409
iteration : 9616
train acc:  0.8359375
train loss:  0.31950899958610535
train gradient:  0.22850949666610482
iteration : 9617
train acc:  0.8671875
train loss:  0.29891932010650635
train gradient:  0.11997964340779371
iteration : 9618
train acc:  0.84375
train loss:  0.32747238874435425
train gradient:  0.14568591324778496
iteration : 9619
train acc:  0.859375
train loss:  0.3355541527271271
train gradient:  0.13933488098583935
iteration : 9620
train acc:  0.84375
train loss:  0.36216601729393005
train gradient:  0.16737641770167616
iteration : 9621
train acc:  0.8671875
train loss:  0.3302997350692749
train gradient:  0.1056569360640801
iteration : 9622
train acc:  0.8671875
train loss:  0.31141549348831177
train gradient:  0.09960835063282561
iteration : 9623
train acc:  0.828125
train loss:  0.3611396253108978
train gradient:  0.18501554559324823
iteration : 9624
train acc:  0.8359375
train loss:  0.37240031361579895
train gradient:  0.23412316776711134
iteration : 9625
train acc:  0.859375
train loss:  0.2927524745464325
train gradient:  0.11221096122453707
iteration : 9626
train acc:  0.828125
train loss:  0.3154025971889496
train gradient:  0.29098868054049803
iteration : 9627
train acc:  0.8515625
train loss:  0.375419944524765
train gradient:  0.16640993668919102
iteration : 9628
train acc:  0.8515625
train loss:  0.3912544548511505
train gradient:  0.17232240627902676
iteration : 9629
train acc:  0.828125
train loss:  0.35062888264656067
train gradient:  0.18125946043453467
iteration : 9630
train acc:  0.859375
train loss:  0.2817554771900177
train gradient:  0.12315812821427248
iteration : 9631
train acc:  0.8203125
train loss:  0.3521527051925659
train gradient:  0.23060076006780178
iteration : 9632
train acc:  0.8828125
train loss:  0.28368228673934937
train gradient:  0.12207240922955641
iteration : 9633
train acc:  0.8828125
train loss:  0.27238574624061584
train gradient:  0.17098035259144573
iteration : 9634
train acc:  0.8515625
train loss:  0.336222767829895
train gradient:  0.23124718845045025
iteration : 9635
train acc:  0.84375
train loss:  0.40427178144454956
train gradient:  0.2450372363698144
iteration : 9636
train acc:  0.8671875
train loss:  0.29111769795417786
train gradient:  0.16299550550676084
iteration : 9637
train acc:  0.84375
train loss:  0.32064831256866455
train gradient:  0.18470563673576867
iteration : 9638
train acc:  0.84375
train loss:  0.36482682824134827
train gradient:  0.19581425438047137
iteration : 9639
train acc:  0.875
train loss:  0.2992648482322693
train gradient:  0.11187872191630495
iteration : 9640
train acc:  0.90625
train loss:  0.3118841350078583
train gradient:  0.1356393019963856
iteration : 9641
train acc:  0.8359375
train loss:  0.36784690618515015
train gradient:  0.20869514293766211
iteration : 9642
train acc:  0.828125
train loss:  0.3393603563308716
train gradient:  0.24887862514535528
iteration : 9643
train acc:  0.859375
train loss:  0.3229842483997345
train gradient:  0.13232096359372847
iteration : 9644
train acc:  0.859375
train loss:  0.32253772020339966
train gradient:  0.17400857535036374
iteration : 9645
train acc:  0.875
train loss:  0.32992494106292725
train gradient:  0.13316789630926534
iteration : 9646
train acc:  0.8515625
train loss:  0.3118760883808136
train gradient:  0.1775023813601917
iteration : 9647
train acc:  0.84375
train loss:  0.3577613830566406
train gradient:  0.19961485654597122
iteration : 9648
train acc:  0.8515625
train loss:  0.28451859951019287
train gradient:  0.11241661664506047
iteration : 9649
train acc:  0.8203125
train loss:  0.39147940278053284
train gradient:  0.3406430483335585
iteration : 9650
train acc:  0.890625
train loss:  0.3304937183856964
train gradient:  0.11923714560093929
iteration : 9651
train acc:  0.859375
train loss:  0.2798536419868469
train gradient:  0.11481354557963164
iteration : 9652
train acc:  0.8515625
train loss:  0.3405402898788452
train gradient:  0.16964256438519154
iteration : 9653
train acc:  0.796875
train loss:  0.41989460587501526
train gradient:  0.2921925403366663
iteration : 9654
train acc:  0.84375
train loss:  0.35155776143074036
train gradient:  0.1808319222354235
iteration : 9655
train acc:  0.796875
train loss:  0.41899943351745605
train gradient:  0.19885694688107852
iteration : 9656
train acc:  0.828125
train loss:  0.3373493254184723
train gradient:  0.2230642107909619
iteration : 9657
train acc:  0.84375
train loss:  0.3750672936439514
train gradient:  0.18335985054969353
iteration : 9658
train acc:  0.84375
train loss:  0.4025987982749939
train gradient:  0.2141975662989697
iteration : 9659
train acc:  0.828125
train loss:  0.346535861492157
train gradient:  0.21549106372566618
iteration : 9660
train acc:  0.828125
train loss:  0.3544539213180542
train gradient:  0.18872608345323466
iteration : 9661
train acc:  0.84375
train loss:  0.33956485986709595
train gradient:  0.19101877601650277
iteration : 9662
train acc:  0.890625
train loss:  0.300752729177475
train gradient:  0.15626616769356574
iteration : 9663
train acc:  0.8515625
train loss:  0.3799813985824585
train gradient:  0.1992983094635139
iteration : 9664
train acc:  0.8671875
train loss:  0.3687950670719147
train gradient:  0.20665973761580708
iteration : 9665
train acc:  0.9296875
train loss:  0.22615154087543488
train gradient:  0.12186533708649684
iteration : 9666
train acc:  0.8984375
train loss:  0.3072360157966614
train gradient:  0.14059931689377325
iteration : 9667
train acc:  0.859375
train loss:  0.3325766623020172
train gradient:  0.23255067372596644
iteration : 9668
train acc:  0.90625
train loss:  0.28314900398254395
train gradient:  0.11519784996620562
iteration : 9669
train acc:  0.9140625
train loss:  0.28843504190444946
train gradient:  0.17265525001813875
iteration : 9670
train acc:  0.8515625
train loss:  0.3038511276245117
train gradient:  0.11370485298152257
iteration : 9671
train acc:  0.90625
train loss:  0.2895050048828125
train gradient:  0.1977280431964601
iteration : 9672
train acc:  0.8828125
train loss:  0.28154876828193665
train gradient:  0.08973019293374998
iteration : 9673
train acc:  0.8828125
train loss:  0.25987040996551514
train gradient:  0.08985350039594811
iteration : 9674
train acc:  0.875
train loss:  0.3344799280166626
train gradient:  0.26326031473790906
iteration : 9675
train acc:  0.890625
train loss:  0.28529566526412964
train gradient:  0.08551081251227959
iteration : 9676
train acc:  0.890625
train loss:  0.30176159739494324
train gradient:  0.11834032735270071
iteration : 9677
train acc:  0.875
train loss:  0.28705132007598877
train gradient:  0.16832536230742626
iteration : 9678
train acc:  0.8984375
train loss:  0.28152820467948914
train gradient:  0.21515419787170276
iteration : 9679
train acc:  0.890625
train loss:  0.2587742805480957
train gradient:  0.09837395565885293
iteration : 9680
train acc:  0.828125
train loss:  0.3602631092071533
train gradient:  0.15367759358773403
iteration : 9681
train acc:  0.828125
train loss:  0.3955429792404175
train gradient:  0.22769959387578934
iteration : 9682
train acc:  0.828125
train loss:  0.35548195242881775
train gradient:  0.17931223904651128
iteration : 9683
train acc:  0.859375
train loss:  0.3124352693557739
train gradient:  0.15100108252657857
iteration : 9684
train acc:  0.8359375
train loss:  0.3650156855583191
train gradient:  0.21435596192062947
iteration : 9685
train acc:  0.875
train loss:  0.30496490001678467
train gradient:  0.18242125726081038
iteration : 9686
train acc:  0.8828125
train loss:  0.290769100189209
train gradient:  0.15462547047394354
iteration : 9687
train acc:  0.859375
train loss:  0.33875930309295654
train gradient:  0.16233826475436072
iteration : 9688
train acc:  0.8671875
train loss:  0.383157342672348
train gradient:  0.24281451139269122
iteration : 9689
train acc:  0.890625
train loss:  0.28333523869514465
train gradient:  0.17966575504387994
iteration : 9690
train acc:  0.8671875
train loss:  0.292257159948349
train gradient:  0.12787291544053894
iteration : 9691
train acc:  0.8515625
train loss:  0.4123521149158478
train gradient:  0.24636516959035937
iteration : 9692
train acc:  0.9140625
train loss:  0.2643355131149292
train gradient:  0.11268702044546931
iteration : 9693
train acc:  0.8671875
train loss:  0.2825472354888916
train gradient:  0.25741942406700186
iteration : 9694
train acc:  0.859375
train loss:  0.3005822002887726
train gradient:  0.18339486649003184
iteration : 9695
train acc:  0.8671875
train loss:  0.33983612060546875
train gradient:  0.24516297096901465
iteration : 9696
train acc:  0.8515625
train loss:  0.3348667621612549
train gradient:  0.23468768795972447
iteration : 9697
train acc:  0.84375
train loss:  0.3489641547203064
train gradient:  0.1750931240726088
iteration : 9698
train acc:  0.8359375
train loss:  0.34743401408195496
train gradient:  0.15742570704988904
iteration : 9699
train acc:  0.8671875
train loss:  0.3074781894683838
train gradient:  0.13000267210327365
iteration : 9700
train acc:  0.859375
train loss:  0.33342739939689636
train gradient:  0.19077556157669698
iteration : 9701
train acc:  0.859375
train loss:  0.30259907245635986
train gradient:  0.12566337420897034
iteration : 9702
train acc:  0.8984375
train loss:  0.25591719150543213
train gradient:  0.1170214106752292
iteration : 9703
train acc:  0.8359375
train loss:  0.4107714891433716
train gradient:  0.2129871158398503
iteration : 9704
train acc:  0.84375
train loss:  0.3341684937477112
train gradient:  0.2183375795952385
iteration : 9705
train acc:  0.8359375
train loss:  0.33716410398483276
train gradient:  0.17385916035150722
iteration : 9706
train acc:  0.875
train loss:  0.3441963195800781
train gradient:  0.20362209805326043
iteration : 9707
train acc:  0.828125
train loss:  0.4156467318534851
train gradient:  0.2772507973245175
iteration : 9708
train acc:  0.8359375
train loss:  0.3391328752040863
train gradient:  0.17111716040383282
iteration : 9709
train acc:  0.8515625
train loss:  0.42591190338134766
train gradient:  0.3306185106859239
iteration : 9710
train acc:  0.8828125
train loss:  0.3357100486755371
train gradient:  0.19969180042251683
iteration : 9711
train acc:  0.859375
train loss:  0.2606731653213501
train gradient:  0.10702818164524766
iteration : 9712
train acc:  0.8984375
train loss:  0.28536686301231384
train gradient:  0.16625052717705807
iteration : 9713
train acc:  0.8671875
train loss:  0.3555546998977661
train gradient:  0.2051477868019147
iteration : 9714
train acc:  0.8359375
train loss:  0.35766100883483887
train gradient:  0.17800099937896205
iteration : 9715
train acc:  0.8125
train loss:  0.35219183564186096
train gradient:  0.17042864925979404
iteration : 9716
train acc:  0.8125
train loss:  0.40078383684158325
train gradient:  0.22873443686063463
iteration : 9717
train acc:  0.859375
train loss:  0.30925801396369934
train gradient:  0.1557750100837144
iteration : 9718
train acc:  0.8359375
train loss:  0.30039092898368835
train gradient:  0.19020560410386816
iteration : 9719
train acc:  0.8359375
train loss:  0.3711533546447754
train gradient:  0.23950824933633535
iteration : 9720
train acc:  0.859375
train loss:  0.34998637437820435
train gradient:  0.20252989293585502
iteration : 9721
train acc:  0.8828125
train loss:  0.3941282629966736
train gradient:  0.22873613512730132
iteration : 9722
train acc:  0.8359375
train loss:  0.37024444341659546
train gradient:  0.27204879989717046
iteration : 9723
train acc:  0.90625
train loss:  0.2681910991668701
train gradient:  0.14005312428752498
iteration : 9724
train acc:  0.9140625
train loss:  0.2948739528656006
train gradient:  0.11140341578397925
iteration : 9725
train acc:  0.7890625
train loss:  0.5090938806533813
train gradient:  0.35409363970060803
iteration : 9726
train acc:  0.8984375
train loss:  0.2866573929786682
train gradient:  0.15470941472213223
iteration : 9727
train acc:  0.84375
train loss:  0.31187617778778076
train gradient:  0.18247456049528674
iteration : 9728
train acc:  0.8984375
train loss:  0.23665596544742584
train gradient:  0.19820250900022507
iteration : 9729
train acc:  0.84375
train loss:  0.33686771988868713
train gradient:  0.20919098827051796
iteration : 9730
train acc:  0.890625
train loss:  0.29197871685028076
train gradient:  0.11795651514333631
iteration : 9731
train acc:  0.8828125
train loss:  0.27404946088790894
train gradient:  0.13682807774748526
iteration : 9732
train acc:  0.8671875
train loss:  0.31072258949279785
train gradient:  0.14506056174687837
iteration : 9733
train acc:  0.828125
train loss:  0.3268461227416992
train gradient:  0.13570952666760433
iteration : 9734
train acc:  0.8515625
train loss:  0.3388676643371582
train gradient:  0.1619113335692684
iteration : 9735
train acc:  0.8828125
train loss:  0.3047616481781006
train gradient:  0.18452832953281184
iteration : 9736
train acc:  0.8046875
train loss:  0.456400603055954
train gradient:  0.30555287729898717
iteration : 9737
train acc:  0.875
train loss:  0.3141227662563324
train gradient:  0.18718217045431684
iteration : 9738
train acc:  0.8515625
train loss:  0.3636704385280609
train gradient:  0.17239024196928868
iteration : 9739
train acc:  0.8671875
train loss:  0.35260236263275146
train gradient:  0.19224911066544903
iteration : 9740
train acc:  0.8828125
train loss:  0.3260854482650757
train gradient:  0.23369880013286384
iteration : 9741
train acc:  0.8203125
train loss:  0.30667543411254883
train gradient:  0.17438572509226322
iteration : 9742
train acc:  0.84375
train loss:  0.3243396282196045
train gradient:  0.16974737894968625
iteration : 9743
train acc:  0.7734375
train loss:  0.4026564657688141
train gradient:  0.21556956166928667
iteration : 9744
train acc:  0.859375
train loss:  0.3949480652809143
train gradient:  0.2602046490277145
iteration : 9745
train acc:  0.875
train loss:  0.26792168617248535
train gradient:  0.1285780834568513
iteration : 9746
train acc:  0.890625
train loss:  0.2357901632785797
train gradient:  0.12167669390017231
iteration : 9747
train acc:  0.796875
train loss:  0.3994455337524414
train gradient:  0.22813518868435825
iteration : 9748
train acc:  0.8359375
train loss:  0.3733750879764557
train gradient:  0.16334984958347618
iteration : 9749
train acc:  0.8515625
train loss:  0.34866058826446533
train gradient:  0.14422255180400192
iteration : 9750
train acc:  0.828125
train loss:  0.3607923686504364
train gradient:  0.17326760170473904
iteration : 9751
train acc:  0.921875
train loss:  0.23749566078186035
train gradient:  0.09445993180426361
iteration : 9752
train acc:  0.8984375
train loss:  0.2525455355644226
train gradient:  0.11096654909312717
iteration : 9753
train acc:  0.828125
train loss:  0.44735032320022583
train gradient:  0.24659916869311627
iteration : 9754
train acc:  0.828125
train loss:  0.400586873292923
train gradient:  0.23344424931520474
iteration : 9755
train acc:  0.828125
train loss:  0.4063582718372345
train gradient:  0.21149224142668774
iteration : 9756
train acc:  0.8046875
train loss:  0.37768417596817017
train gradient:  0.21009361632744103
iteration : 9757
train acc:  0.8515625
train loss:  0.30927616357803345
train gradient:  0.11312990653257941
iteration : 9758
train acc:  0.8828125
train loss:  0.2683693766593933
train gradient:  0.12356759385922263
iteration : 9759
train acc:  0.8203125
train loss:  0.43854770064353943
train gradient:  0.24319815240126608
iteration : 9760
train acc:  0.8359375
train loss:  0.3610643148422241
train gradient:  0.17211252351004297
iteration : 9761
train acc:  0.8359375
train loss:  0.3443453907966614
train gradient:  0.1274624188560871
iteration : 9762
train acc:  0.859375
train loss:  0.29628026485443115
train gradient:  0.12554132789421169
iteration : 9763
train acc:  0.8203125
train loss:  0.37113094329833984
train gradient:  0.14724304869593727
iteration : 9764
train acc:  0.7890625
train loss:  0.38461410999298096
train gradient:  0.25629691823384754
iteration : 9765
train acc:  0.8828125
train loss:  0.32580605149269104
train gradient:  0.2426097628909713
iteration : 9766
train acc:  0.796875
train loss:  0.445452481508255
train gradient:  0.30940163370555296
iteration : 9767
train acc:  0.90625
train loss:  0.293327659368515
train gradient:  0.0889264398759312
iteration : 9768
train acc:  0.875
train loss:  0.32776087522506714
train gradient:  0.1325612743098583
iteration : 9769
train acc:  0.8515625
train loss:  0.3047812879085541
train gradient:  0.12455272731843127
iteration : 9770
train acc:  0.8359375
train loss:  0.3280932307243347
train gradient:  0.13223879786514237
iteration : 9771
train acc:  0.8359375
train loss:  0.3346974849700928
train gradient:  0.11109538120771731
iteration : 9772
train acc:  0.8359375
train loss:  0.32802051305770874
train gradient:  0.14631437468538336
iteration : 9773
train acc:  0.796875
train loss:  0.38263291120529175
train gradient:  0.1899968070361491
iteration : 9774
train acc:  0.84375
train loss:  0.33521655201911926
train gradient:  0.2692514492139322
iteration : 9775
train acc:  0.84375
train loss:  0.3670497536659241
train gradient:  0.16073431082641668
iteration : 9776
train acc:  0.90625
train loss:  0.2680075168609619
train gradient:  0.12217193806122773
iteration : 9777
train acc:  0.890625
train loss:  0.2926296591758728
train gradient:  0.12517430146584224
iteration : 9778
train acc:  0.8984375
train loss:  0.28941553831100464
train gradient:  0.15765704719705192
iteration : 9779
train acc:  0.8203125
train loss:  0.35941082239151
train gradient:  0.13465445678909355
iteration : 9780
train acc:  0.875
train loss:  0.3053765892982483
train gradient:  0.13560100764434319
iteration : 9781
train acc:  0.8515625
train loss:  0.35972070693969727
train gradient:  0.14923098659128303
iteration : 9782
train acc:  0.8046875
train loss:  0.36496809124946594
train gradient:  0.17022344005441195
iteration : 9783
train acc:  0.90625
train loss:  0.2624228596687317
train gradient:  0.10174469400507083
iteration : 9784
train acc:  0.859375
train loss:  0.3415679633617401
train gradient:  0.18947680465178976
iteration : 9785
train acc:  0.828125
train loss:  0.3665928840637207
train gradient:  0.20427501344013535
iteration : 9786
train acc:  0.8984375
train loss:  0.2920866310596466
train gradient:  0.13116049640295346
iteration : 9787
train acc:  0.8671875
train loss:  0.2676154375076294
train gradient:  0.13052913881271028
iteration : 9788
train acc:  0.8203125
train loss:  0.3770684003829956
train gradient:  0.14728361408740562
iteration : 9789
train acc:  0.875
train loss:  0.34478646516799927
train gradient:  0.16237874430596522
iteration : 9790
train acc:  0.90625
train loss:  0.31371843814849854
train gradient:  0.23536398708899478
iteration : 9791
train acc:  0.859375
train loss:  0.29997414350509644
train gradient:  0.1367371237044238
iteration : 9792
train acc:  0.859375
train loss:  0.30632054805755615
train gradient:  0.1500377279683474
iteration : 9793
train acc:  0.8125
train loss:  0.4061937928199768
train gradient:  0.21027733454828285
iteration : 9794
train acc:  0.8359375
train loss:  0.3701382577419281
train gradient:  0.175216932371063
iteration : 9795
train acc:  0.8515625
train loss:  0.3199072480201721
train gradient:  0.15204731801213722
iteration : 9796
train acc:  0.8671875
train loss:  0.32329583168029785
train gradient:  0.21605580472931435
iteration : 9797
train acc:  0.8125
train loss:  0.3523004651069641
train gradient:  0.20967509999960898
iteration : 9798
train acc:  0.796875
train loss:  0.37486961483955383
train gradient:  0.234026820013456
iteration : 9799
train acc:  0.84375
train loss:  0.3124404549598694
train gradient:  0.11752987985846568
iteration : 9800
train acc:  0.8671875
train loss:  0.3098886013031006
train gradient:  0.10530622587095703
iteration : 9801
train acc:  0.828125
train loss:  0.3478645086288452
train gradient:  0.16138399266903428
iteration : 9802
train acc:  0.828125
train loss:  0.4175194501876831
train gradient:  0.19829002653665445
iteration : 9803
train acc:  0.796875
train loss:  0.41407471895217896
train gradient:  0.184497530515344
iteration : 9804
train acc:  0.8359375
train loss:  0.31133878231048584
train gradient:  0.13196323536977084
iteration : 9805
train acc:  0.8828125
train loss:  0.27821964025497437
train gradient:  0.13287895168114272
iteration : 9806
train acc:  0.828125
train loss:  0.33551788330078125
train gradient:  0.16971298636893134
iteration : 9807
train acc:  0.875
train loss:  0.28258293867111206
train gradient:  0.09194787152477242
iteration : 9808
train acc:  0.828125
train loss:  0.35556158423423767
train gradient:  0.17779309989094222
iteration : 9809
train acc:  0.8984375
train loss:  0.2930780053138733
train gradient:  0.18194799054894017
iteration : 9810
train acc:  0.84375
train loss:  0.3659784197807312
train gradient:  0.2331868859943716
iteration : 9811
train acc:  0.8203125
train loss:  0.39890027046203613
train gradient:  0.1899998648054278
iteration : 9812
train acc:  0.8828125
train loss:  0.3078157901763916
train gradient:  0.14086963896504512
iteration : 9813
train acc:  0.859375
train loss:  0.3244897127151489
train gradient:  0.14738286768053566
iteration : 9814
train acc:  0.8828125
train loss:  0.30034884810447693
train gradient:  0.1799498794970002
iteration : 9815
train acc:  0.890625
train loss:  0.27880027890205383
train gradient:  0.11154462343846193
iteration : 9816
train acc:  0.84375
train loss:  0.3547587990760803
train gradient:  0.23106023081222696
iteration : 9817
train acc:  0.8671875
train loss:  0.2797764539718628
train gradient:  0.12401367928850628
iteration : 9818
train acc:  0.8359375
train loss:  0.33736133575439453
train gradient:  0.1789795206340074
iteration : 9819
train acc:  0.8671875
train loss:  0.31403693556785583
train gradient:  0.15363877246754679
iteration : 9820
train acc:  0.84375
train loss:  0.4392435550689697
train gradient:  0.2239079810273939
iteration : 9821
train acc:  0.875
train loss:  0.30408042669296265
train gradient:  0.11937959043906196
iteration : 9822
train acc:  0.8828125
train loss:  0.27247682213783264
train gradient:  0.114183917871386
iteration : 9823
train acc:  0.90625
train loss:  0.2675628662109375
train gradient:  0.15213713983058832
iteration : 9824
train acc:  0.78125
train loss:  0.3915363848209381
train gradient:  0.21260930326330832
iteration : 9825
train acc:  0.875
train loss:  0.3232741057872772
train gradient:  0.13751457390316402
iteration : 9826
train acc:  0.84375
train loss:  0.3108147978782654
train gradient:  0.1311303695545794
iteration : 9827
train acc:  0.875
train loss:  0.32101649045944214
train gradient:  0.12993270290843859
iteration : 9828
train acc:  0.8984375
train loss:  0.2706894278526306
train gradient:  0.12254927073512943
iteration : 9829
train acc:  0.828125
train loss:  0.3175920844078064
train gradient:  0.18571783006094533
iteration : 9830
train acc:  0.8515625
train loss:  0.3552062511444092
train gradient:  0.28860440073431565
iteration : 9831
train acc:  0.859375
train loss:  0.32282131910324097
train gradient:  0.15124446262140948
iteration : 9832
train acc:  0.859375
train loss:  0.30808183550834656
train gradient:  0.15814102848800354
iteration : 9833
train acc:  0.8359375
train loss:  0.36764466762542725
train gradient:  0.22605788530097665
iteration : 9834
train acc:  0.8671875
train loss:  0.3266373872756958
train gradient:  0.1535061444104037
iteration : 9835
train acc:  0.8515625
train loss:  0.3424539268016815
train gradient:  0.16837205936107985
iteration : 9836
train acc:  0.796875
train loss:  0.4319009482860565
train gradient:  0.29328228601548473
iteration : 9837
train acc:  0.8671875
train loss:  0.3462616801261902
train gradient:  0.1418288858832613
iteration : 9838
train acc:  0.875
train loss:  0.3514861464500427
train gradient:  0.17079656602307652
iteration : 9839
train acc:  0.8984375
train loss:  0.24906650185585022
train gradient:  0.1456003532001211
iteration : 9840
train acc:  0.859375
train loss:  0.3298266530036926
train gradient:  0.16444238827161434
iteration : 9841
train acc:  0.859375
train loss:  0.3250967264175415
train gradient:  0.1483024672820366
iteration : 9842
train acc:  0.859375
train loss:  0.35700809955596924
train gradient:  0.19867883749191234
iteration : 9843
train acc:  0.859375
train loss:  0.3374190330505371
train gradient:  0.20322996314803904
iteration : 9844
train acc:  0.8203125
train loss:  0.3884212076663971
train gradient:  0.17128417046157052
iteration : 9845
train acc:  0.8359375
train loss:  0.3833606243133545
train gradient:  0.2843058737236237
iteration : 9846
train acc:  0.8984375
train loss:  0.2694111466407776
train gradient:  0.09962068652333207
iteration : 9847
train acc:  0.828125
train loss:  0.3765832781791687
train gradient:  0.1711373057529323
iteration : 9848
train acc:  0.875
train loss:  0.32890820503234863
train gradient:  0.1339661792329227
iteration : 9849
train acc:  0.890625
train loss:  0.29897648096084595
train gradient:  0.16609778620721685
iteration : 9850
train acc:  0.8828125
train loss:  0.27499863505363464
train gradient:  0.15417109195052772
iteration : 9851
train acc:  0.8671875
train loss:  0.2975570559501648
train gradient:  0.18951490837740254
iteration : 9852
train acc:  0.8515625
train loss:  0.3718187212944031
train gradient:  0.24091813319362215
iteration : 9853
train acc:  0.8515625
train loss:  0.3754228353500366
train gradient:  0.2683066614727315
iteration : 9854
train acc:  0.8515625
train loss:  0.3054901361465454
train gradient:  0.1498266756669846
iteration : 9855
train acc:  0.875
train loss:  0.29144132137298584
train gradient:  0.16090833211675637
iteration : 9856
train acc:  0.8671875
train loss:  0.278883159160614
train gradient:  0.1202085483831785
iteration : 9857
train acc:  0.859375
train loss:  0.3452804982662201
train gradient:  0.20509676641440366
iteration : 9858
train acc:  0.8671875
train loss:  0.3507697880268097
train gradient:  0.20123036295008562
iteration : 9859
train acc:  0.8671875
train loss:  0.3026997745037079
train gradient:  0.2176279188161749
iteration : 9860
train acc:  0.8828125
train loss:  0.28573858737945557
train gradient:  0.13149743596273913
iteration : 9861
train acc:  0.90625
train loss:  0.2663644850254059
train gradient:  0.14601634322058113
iteration : 9862
train acc:  0.8515625
train loss:  0.2962013781070709
train gradient:  0.11458906179603824
iteration : 9863
train acc:  0.8515625
train loss:  0.33594316244125366
train gradient:  0.226825491701441
iteration : 9864
train acc:  0.7734375
train loss:  0.45845866203308105
train gradient:  0.3131165581906963
iteration : 9865
train acc:  0.8203125
train loss:  0.39432528614997864
train gradient:  0.19433114698531662
iteration : 9866
train acc:  0.8515625
train loss:  0.3108898401260376
train gradient:  0.19162660527426342
iteration : 9867
train acc:  0.828125
train loss:  0.3011561930179596
train gradient:  0.11243558563176638
iteration : 9868
train acc:  0.90625
train loss:  0.25360921025276184
train gradient:  0.1330992255807164
iteration : 9869
train acc:  0.828125
train loss:  0.35859835147857666
train gradient:  0.17505176143585663
iteration : 9870
train acc:  0.828125
train loss:  0.3536606431007385
train gradient:  0.23185454153740714
iteration : 9871
train acc:  0.8828125
train loss:  0.24461032450199127
train gradient:  0.1273838287130146
iteration : 9872
train acc:  0.84375
train loss:  0.3338344693183899
train gradient:  0.20375199072172756
iteration : 9873
train acc:  0.8828125
train loss:  0.3071654438972473
train gradient:  0.18156548643849463
iteration : 9874
train acc:  0.8515625
train loss:  0.34764111042022705
train gradient:  0.22992256200424782
iteration : 9875
train acc:  0.8203125
train loss:  0.43898099660873413
train gradient:  0.23728628562983944
iteration : 9876
train acc:  0.8359375
train loss:  0.3182704448699951
train gradient:  0.1212313885687481
iteration : 9877
train acc:  0.921875
train loss:  0.3411651849746704
train gradient:  0.1927641188205385
iteration : 9878
train acc:  0.875
train loss:  0.3194977343082428
train gradient:  0.18971300169704516
iteration : 9879
train acc:  0.875
train loss:  0.30040454864501953
train gradient:  0.16106211022389266
iteration : 9880
train acc:  0.828125
train loss:  0.31731724739074707
train gradient:  0.12313021394408173
iteration : 9881
train acc:  0.8359375
train loss:  0.31900256872177124
train gradient:  0.15444250936606518
iteration : 9882
train acc:  0.8203125
train loss:  0.41861623525619507
train gradient:  0.1753707308176771
iteration : 9883
train acc:  0.8671875
train loss:  0.34856802225112915
train gradient:  0.13061241985055158
iteration : 9884
train acc:  0.8671875
train loss:  0.33976417779922485
train gradient:  0.2205979453850015
iteration : 9885
train acc:  0.8828125
train loss:  0.27022796869277954
train gradient:  0.15452222740219324
iteration : 9886
train acc:  0.8515625
train loss:  0.3374711871147156
train gradient:  0.26272799693326876
iteration : 9887
train acc:  0.8984375
train loss:  0.2517889738082886
train gradient:  0.09920906464429788
iteration : 9888
train acc:  0.8203125
train loss:  0.3661058843135834
train gradient:  0.22372453815606783
iteration : 9889
train acc:  0.859375
train loss:  0.3162943124771118
train gradient:  0.21429525375359193
iteration : 9890
train acc:  0.8125
train loss:  0.40269017219543457
train gradient:  0.23834881247440481
iteration : 9891
train acc:  0.8828125
train loss:  0.26050591468811035
train gradient:  0.10998026512872719
iteration : 9892
train acc:  0.8203125
train loss:  0.42558497190475464
train gradient:  0.2200054347446641
iteration : 9893
train acc:  0.8359375
train loss:  0.3434036076068878
train gradient:  0.18874725343723064
iteration : 9894
train acc:  0.8671875
train loss:  0.31710368394851685
train gradient:  0.16633811099372264
iteration : 9895
train acc:  0.890625
train loss:  0.3110661804676056
train gradient:  0.12837167338277744
iteration : 9896
train acc:  0.8828125
train loss:  0.32493412494659424
train gradient:  0.16817749487903716
iteration : 9897
train acc:  0.859375
train loss:  0.3457678556442261
train gradient:  0.20223944765258664
iteration : 9898
train acc:  0.8359375
train loss:  0.4029586911201477
train gradient:  0.2238652056938768
iteration : 9899
train acc:  0.828125
train loss:  0.34872564673423767
train gradient:  0.141753935587641
iteration : 9900
train acc:  0.890625
train loss:  0.2962077856063843
train gradient:  0.08848022843274643
iteration : 9901
train acc:  0.8671875
train loss:  0.29532182216644287
train gradient:  0.11815881318267202
iteration : 9902
train acc:  0.8671875
train loss:  0.33266395330429077
train gradient:  0.17033518208744386
iteration : 9903
train acc:  0.875
train loss:  0.35905689001083374
train gradient:  0.2687684123738469
iteration : 9904
train acc:  0.8515625
train loss:  0.32845133543014526
train gradient:  0.14567342705319014
iteration : 9905
train acc:  0.8515625
train loss:  0.3806150257587433
train gradient:  0.1498801850453881
iteration : 9906
train acc:  0.84375
train loss:  0.36800748109817505
train gradient:  0.18995776507853912
iteration : 9907
train acc:  0.8671875
train loss:  0.3102070689201355
train gradient:  0.17583816227230137
iteration : 9908
train acc:  0.8671875
train loss:  0.32516878843307495
train gradient:  0.16891921159630224
iteration : 9909
train acc:  0.8359375
train loss:  0.35250788927078247
train gradient:  0.11778349329813353
iteration : 9910
train acc:  0.875
train loss:  0.3478965163230896
train gradient:  0.1726975081640053
iteration : 9911
train acc:  0.8203125
train loss:  0.3939361870288849
train gradient:  0.23836139445352045
iteration : 9912
train acc:  0.8671875
train loss:  0.2973473072052002
train gradient:  0.09161468387164187
iteration : 9913
train acc:  0.8515625
train loss:  0.3660942316055298
train gradient:  0.19707449302423344
iteration : 9914
train acc:  0.8203125
train loss:  0.4160245358943939
train gradient:  0.276457280066293
iteration : 9915
train acc:  0.875
train loss:  0.320454478263855
train gradient:  0.22005880364168873
iteration : 9916
train acc:  0.828125
train loss:  0.41394901275634766
train gradient:  0.1825000377306475
iteration : 9917
train acc:  0.8125
train loss:  0.3867100477218628
train gradient:  0.21858353788522572
iteration : 9918
train acc:  0.875
train loss:  0.3306404948234558
train gradient:  0.10932487404447841
iteration : 9919
train acc:  0.8828125
train loss:  0.3401433229446411
train gradient:  0.12279658321920003
iteration : 9920
train acc:  0.8671875
train loss:  0.2921779155731201
train gradient:  0.11646823092290863
iteration : 9921
train acc:  0.875
train loss:  0.32105380296707153
train gradient:  0.16279892031864013
iteration : 9922
train acc:  0.859375
train loss:  0.27138078212738037
train gradient:  0.14540344492826673
iteration : 9923
train acc:  0.84375
train loss:  0.3656163513660431
train gradient:  0.1824431395662443
iteration : 9924
train acc:  0.84375
train loss:  0.3316764235496521
train gradient:  0.1429563528287373
iteration : 9925
train acc:  0.9140625
train loss:  0.24852365255355835
train gradient:  0.11532204097391349
iteration : 9926
train acc:  0.90625
train loss:  0.2503150701522827
train gradient:  0.08071881057937991
iteration : 9927
train acc:  0.8828125
train loss:  0.3001447916030884
train gradient:  0.11497207535306561
iteration : 9928
train acc:  0.796875
train loss:  0.38787075877189636
train gradient:  0.20713708479843526
iteration : 9929
train acc:  0.875
train loss:  0.24697345495224
train gradient:  0.09175440806067221
iteration : 9930
train acc:  0.84375
train loss:  0.3412030339241028
train gradient:  0.15723681202794026
iteration : 9931
train acc:  0.84375
train loss:  0.32876360416412354
train gradient:  0.14263009340159333
iteration : 9932
train acc:  0.8828125
train loss:  0.3207411766052246
train gradient:  0.16408738708478166
iteration : 9933
train acc:  0.8671875
train loss:  0.2820335924625397
train gradient:  0.13854630181071517
iteration : 9934
train acc:  0.8671875
train loss:  0.3841623365879059
train gradient:  0.2073706081231683
iteration : 9935
train acc:  0.84375
train loss:  0.33112290501594543
train gradient:  0.17321806199906808
iteration : 9936
train acc:  0.859375
train loss:  0.3571166694164276
train gradient:  0.20316928438773396
iteration : 9937
train acc:  0.8515625
train loss:  0.3630000650882721
train gradient:  0.193278370531534
iteration : 9938
train acc:  0.8984375
train loss:  0.25196534395217896
train gradient:  0.12675579141272986
iteration : 9939
train acc:  0.875
train loss:  0.3446447253227234
train gradient:  0.16137403046364002
iteration : 9940
train acc:  0.84375
train loss:  0.3974691331386566
train gradient:  0.23311526898159274
iteration : 9941
train acc:  0.796875
train loss:  0.38891926407814026
train gradient:  0.19024706820588963
iteration : 9942
train acc:  0.8515625
train loss:  0.3613455891609192
train gradient:  0.17662080928714008
iteration : 9943
train acc:  0.8515625
train loss:  0.3374072313308716
train gradient:  0.18473308594575885
iteration : 9944
train acc:  0.859375
train loss:  0.3438851535320282
train gradient:  0.27494729115036964
iteration : 9945
train acc:  0.890625
train loss:  0.3386881351470947
train gradient:  0.18002181780343157
iteration : 9946
train acc:  0.8203125
train loss:  0.39162957668304443
train gradient:  0.2444627205261115
iteration : 9947
train acc:  0.890625
train loss:  0.3048802614212036
train gradient:  0.1492489731886307
iteration : 9948
train acc:  0.84375
train loss:  0.3288208842277527
train gradient:  0.1198653594031264
iteration : 9949
train acc:  0.875
train loss:  0.2854028344154358
train gradient:  0.12736745729146928
iteration : 9950
train acc:  0.8359375
train loss:  0.39197611808776855
train gradient:  0.2242372587851173
iteration : 9951
train acc:  0.890625
train loss:  0.2659889757633209
train gradient:  0.15895825441419087
iteration : 9952
train acc:  0.890625
train loss:  0.302079439163208
train gradient:  0.14067835342713125
iteration : 9953
train acc:  0.8984375
train loss:  0.26078981161117554
train gradient:  0.19187805679029052
iteration : 9954
train acc:  0.859375
train loss:  0.2745976448059082
train gradient:  0.10646628127285086
iteration : 9955
train acc:  0.859375
train loss:  0.30624860525131226
train gradient:  0.14242110042490982
iteration : 9956
train acc:  0.8125
train loss:  0.4483155310153961
train gradient:  0.3463814613396531
iteration : 9957
train acc:  0.84375
train loss:  0.3576478958129883
train gradient:  0.1913055373743682
iteration : 9958
train acc:  0.8828125
train loss:  0.293838232755661
train gradient:  0.1728464332476645
iteration : 9959
train acc:  0.8359375
train loss:  0.39422768354415894
train gradient:  0.2447829625112244
iteration : 9960
train acc:  0.828125
train loss:  0.36503854393959045
train gradient:  0.20203858947300435
iteration : 9961
train acc:  0.828125
train loss:  0.39471450448036194
train gradient:  0.31431195560954484
iteration : 9962
train acc:  0.84375
train loss:  0.37198078632354736
train gradient:  0.14758034334732512
iteration : 9963
train acc:  0.8984375
train loss:  0.29434874653816223
train gradient:  0.204219288637795
iteration : 9964
train acc:  0.828125
train loss:  0.38205528259277344
train gradient:  0.22589325603914512
iteration : 9965
train acc:  0.8359375
train loss:  0.35696619749069214
train gradient:  0.1649616263255802
iteration : 9966
train acc:  0.90625
train loss:  0.2613281309604645
train gradient:  0.11263007461291089
iteration : 9967
train acc:  0.7890625
train loss:  0.4200434684753418
train gradient:  0.20788792764755493
iteration : 9968
train acc:  0.8984375
train loss:  0.3032916784286499
train gradient:  0.11021740894045702
iteration : 9969
train acc:  0.890625
train loss:  0.3065289855003357
train gradient:  0.19083348268770772
iteration : 9970
train acc:  0.8359375
train loss:  0.3951568603515625
train gradient:  0.2146108178349168
iteration : 9971
train acc:  0.8359375
train loss:  0.3525793254375458
train gradient:  0.16544994542357166
iteration : 9972
train acc:  0.859375
train loss:  0.33845871686935425
train gradient:  0.1942457285582695
iteration : 9973
train acc:  0.890625
train loss:  0.2855350375175476
train gradient:  0.1043193854412451
iteration : 9974
train acc:  0.8671875
train loss:  0.26350072026252747
train gradient:  0.10840645063281087
iteration : 9975
train acc:  0.8125
train loss:  0.3366679549217224
train gradient:  0.140342022902527
iteration : 9976
train acc:  0.90625
train loss:  0.240922212600708
train gradient:  0.08915848649993358
iteration : 9977
train acc:  0.8828125
train loss:  0.3503707945346832
train gradient:  0.17431657784500657
iteration : 9978
train acc:  0.796875
train loss:  0.4193708300590515
train gradient:  0.3365925155828064
iteration : 9979
train acc:  0.8671875
train loss:  0.3245525360107422
train gradient:  0.17512352553252272
iteration : 9980
train acc:  0.8828125
train loss:  0.3351328670978546
train gradient:  0.15492478542870497
iteration : 9981
train acc:  0.8515625
train loss:  0.41681158542633057
train gradient:  0.32437332069378416
iteration : 9982
train acc:  0.828125
train loss:  0.321355938911438
train gradient:  0.13206998025831648
iteration : 9983
train acc:  0.8359375
train loss:  0.3610548675060272
train gradient:  0.1710048784214333
iteration : 9984
train acc:  0.7890625
train loss:  0.3904935419559479
train gradient:  0.20590158039309683
iteration : 9985
train acc:  0.84375
train loss:  0.37893036007881165
train gradient:  0.15443322191883918
iteration : 9986
train acc:  0.84375
train loss:  0.30555635690689087
train gradient:  0.15627489064770145
iteration : 9987
train acc:  0.84375
train loss:  0.3220685124397278
train gradient:  0.1325972406356175
iteration : 9988
train acc:  0.8828125
train loss:  0.31294286251068115
train gradient:  0.14472391421496592
iteration : 9989
train acc:  0.875
train loss:  0.34656327962875366
train gradient:  0.20020759809440059
iteration : 9990
train acc:  0.859375
train loss:  0.39476215839385986
train gradient:  0.15861647649562
iteration : 9991
train acc:  0.828125
train loss:  0.36602094769477844
train gradient:  0.13888689895461578
iteration : 9992
train acc:  0.8203125
train loss:  0.3303588330745697
train gradient:  0.11674195137086708
iteration : 9993
train acc:  0.890625
train loss:  0.2535090148448944
train gradient:  0.1516460915909657
iteration : 9994
train acc:  0.8671875
train loss:  0.312309205532074
train gradient:  0.16232894439635487
iteration : 9995
train acc:  0.8046875
train loss:  0.4026198983192444
train gradient:  0.29845026649906325
iteration : 9996
train acc:  0.8125
train loss:  0.3950296938419342
train gradient:  0.29374315592918404
iteration : 9997
train acc:  0.8828125
train loss:  0.26831555366516113
train gradient:  0.11925321220818133
iteration : 9998
train acc:  0.890625
train loss:  0.2607061266899109
train gradient:  0.13750188600802518
iteration : 9999
train acc:  0.859375
train loss:  0.32260918617248535
train gradient:  0.1266434841723731
iteration : 10000
train acc:  0.8515625
train loss:  0.30467623472213745
train gradient:  0.15322905479072843
iteration : 10001
train acc:  0.84375
train loss:  0.3755931854248047
train gradient:  0.23361842179011566
iteration : 10002
train acc:  0.8828125
train loss:  0.31023168563842773
train gradient:  0.1025613107960045
iteration : 10003
train acc:  0.8125
train loss:  0.3328987956047058
train gradient:  0.20097763474304492
iteration : 10004
train acc:  0.7734375
train loss:  0.44066402316093445
train gradient:  0.26083208880424885
iteration : 10005
train acc:  0.8515625
train loss:  0.31087979674339294
train gradient:  0.1274584971814863
iteration : 10006
train acc:  0.90625
train loss:  0.3344195783138275
train gradient:  0.13876490770663796
iteration : 10007
train acc:  0.890625
train loss:  0.26806843280792236
train gradient:  0.14891106979981306
iteration : 10008
train acc:  0.875
train loss:  0.2903977632522583
train gradient:  0.15873928763873996
iteration : 10009
train acc:  0.859375
train loss:  0.2565171718597412
train gradient:  0.10363874618027849
iteration : 10010
train acc:  0.859375
train loss:  0.32574164867401123
train gradient:  0.17616405693676251
iteration : 10011
train acc:  0.828125
train loss:  0.3154558837413788
train gradient:  0.1976186803654788
iteration : 10012
train acc:  0.890625
train loss:  0.29442375898361206
train gradient:  0.17088937012520178
iteration : 10013
train acc:  0.8671875
train loss:  0.28988218307495117
train gradient:  0.18474364117808706
iteration : 10014
train acc:  0.890625
train loss:  0.27154773473739624
train gradient:  0.11513520549665854
iteration : 10015
train acc:  0.8515625
train loss:  0.31315821409225464
train gradient:  0.19211671247570639
iteration : 10016
train acc:  0.8203125
train loss:  0.36873602867126465
train gradient:  0.15994762944894286
iteration : 10017
train acc:  0.890625
train loss:  0.3153901696205139
train gradient:  0.15418769180233854
iteration : 10018
train acc:  0.8515625
train loss:  0.315456748008728
train gradient:  0.10050160735723108
iteration : 10019
train acc:  0.8671875
train loss:  0.28608018159866333
train gradient:  0.15013626169914265
iteration : 10020
train acc:  0.828125
train loss:  0.360761821269989
train gradient:  0.3349815818507077
iteration : 10021
train acc:  0.8125
train loss:  0.3521368205547333
train gradient:  0.20174904586732367
iteration : 10022
train acc:  0.8125
train loss:  0.3252811133861542
train gradient:  0.1658719538803952
iteration : 10023
train acc:  0.875
train loss:  0.24557964503765106
train gradient:  0.13081363729992265
iteration : 10024
train acc:  0.90625
train loss:  0.27893495559692383
train gradient:  0.1235127974701664
iteration : 10025
train acc:  0.8125
train loss:  0.38903534412384033
train gradient:  0.18732693563152836
iteration : 10026
train acc:  0.859375
train loss:  0.3215373754501343
train gradient:  0.13378453961154982
iteration : 10027
train acc:  0.796875
train loss:  0.44241276383399963
train gradient:  0.2445122700745529
iteration : 10028
train acc:  0.8515625
train loss:  0.3445398211479187
train gradient:  0.16029799537117392
iteration : 10029
train acc:  0.84375
train loss:  0.3806104063987732
train gradient:  0.21118252354574324
iteration : 10030
train acc:  0.875
train loss:  0.2678879499435425
train gradient:  0.10666280956133456
iteration : 10031
train acc:  0.84375
train loss:  0.3582819700241089
train gradient:  0.17917663918076301
iteration : 10032
train acc:  0.8671875
train loss:  0.3051248788833618
train gradient:  0.12851081549447319
iteration : 10033
train acc:  0.8203125
train loss:  0.38138118386268616
train gradient:  0.20205064559500327
iteration : 10034
train acc:  0.859375
train loss:  0.31829091906547546
train gradient:  0.1339541287816589
iteration : 10035
train acc:  0.875
train loss:  0.2781918942928314
train gradient:  0.10143372029721932
iteration : 10036
train acc:  0.84375
train loss:  0.3829908072948456
train gradient:  0.21843230790630172
iteration : 10037
train acc:  0.890625
train loss:  0.2600303888320923
train gradient:  0.10249516407493574
iteration : 10038
train acc:  0.84375
train loss:  0.30899035930633545
train gradient:  0.11677123376798651
iteration : 10039
train acc:  0.8671875
train loss:  0.3227226436138153
train gradient:  0.11276797784932302
iteration : 10040
train acc:  0.8515625
train loss:  0.3099355399608612
train gradient:  0.11327610096880314
iteration : 10041
train acc:  0.8828125
train loss:  0.27551114559173584
train gradient:  0.134788561922167
iteration : 10042
train acc:  0.8203125
train loss:  0.36979854106903076
train gradient:  0.19470934010441746
iteration : 10043
train acc:  0.796875
train loss:  0.4431201219558716
train gradient:  0.29566205165316045
iteration : 10044
train acc:  0.875
train loss:  0.3550592064857483
train gradient:  0.3166784778500495
iteration : 10045
train acc:  0.875
train loss:  0.3080083727836609
train gradient:  0.2886294566153098
iteration : 10046
train acc:  0.859375
train loss:  0.2829551696777344
train gradient:  0.10572910985523133
iteration : 10047
train acc:  0.890625
train loss:  0.28771597146987915
train gradient:  0.10030583807924673
iteration : 10048
train acc:  0.8671875
train loss:  0.31600332260131836
train gradient:  0.1709293632418656
iteration : 10049
train acc:  0.8671875
train loss:  0.3030663728713989
train gradient:  0.2016367974103662
iteration : 10050
train acc:  0.8359375
train loss:  0.3488100469112396
train gradient:  0.18689046199608433
iteration : 10051
train acc:  0.84375
train loss:  0.38585004210472107
train gradient:  0.2582175532797175
iteration : 10052
train acc:  0.859375
train loss:  0.33222419023513794
train gradient:  0.1870815633243609
iteration : 10053
train acc:  0.8828125
train loss:  0.2939736843109131
train gradient:  0.11418479759164515
iteration : 10054
train acc:  0.828125
train loss:  0.3354948163032532
train gradient:  0.24050498618650362
iteration : 10055
train acc:  0.859375
train loss:  0.32734405994415283
train gradient:  0.14906714703159732
iteration : 10056
train acc:  0.8359375
train loss:  0.3332681357860565
train gradient:  0.17275536381358442
iteration : 10057
train acc:  0.84375
train loss:  0.3693079948425293
train gradient:  0.1768592848578141
iteration : 10058
train acc:  0.8671875
train loss:  0.25402989983558655
train gradient:  0.09373877494481288
iteration : 10059
train acc:  0.8671875
train loss:  0.3297767639160156
train gradient:  0.18573329417035506
iteration : 10060
train acc:  0.8515625
train loss:  0.33751386404037476
train gradient:  0.15215783619883458
iteration : 10061
train acc:  0.8671875
train loss:  0.29031363129615784
train gradient:  0.11979182445051256
iteration : 10062
train acc:  0.890625
train loss:  0.26974883675575256
train gradient:  0.11451759513050323
iteration : 10063
train acc:  0.8671875
train loss:  0.2836572527885437
train gradient:  0.18157819779111015
iteration : 10064
train acc:  0.921875
train loss:  0.2248830795288086
train gradient:  0.13764003871823388
iteration : 10065
train acc:  0.90625
train loss:  0.2831859886646271
train gradient:  0.13782143339010144
iteration : 10066
train acc:  0.8671875
train loss:  0.32807737588882446
train gradient:  0.23080918436082443
iteration : 10067
train acc:  0.8515625
train loss:  0.31366804242134094
train gradient:  0.18751629849809948
iteration : 10068
train acc:  0.9140625
train loss:  0.2318219542503357
train gradient:  0.08360106566033658
iteration : 10069
train acc:  0.90625
train loss:  0.264139324426651
train gradient:  0.12473522399614567
iteration : 10070
train acc:  0.8671875
train loss:  0.3301956355571747
train gradient:  0.13062191948822977
iteration : 10071
train acc:  0.8671875
train loss:  0.29527372121810913
train gradient:  0.17713674982538571
iteration : 10072
train acc:  0.8359375
train loss:  0.3465553820133209
train gradient:  0.29221199501314415
iteration : 10073
train acc:  0.8515625
train loss:  0.36027050018310547
train gradient:  0.13974649751280363
iteration : 10074
train acc:  0.8515625
train loss:  0.32512354850769043
train gradient:  0.1890617313981538
iteration : 10075
train acc:  0.8203125
train loss:  0.3739660680294037
train gradient:  0.18897520482170227
iteration : 10076
train acc:  0.8828125
train loss:  0.26270923018455505
train gradient:  0.2352291242722691
iteration : 10077
train acc:  0.8671875
train loss:  0.2657310664653778
train gradient:  0.17750291801130522
iteration : 10078
train acc:  0.859375
train loss:  0.2974946200847626
train gradient:  0.14151910869148582
iteration : 10079
train acc:  0.8984375
train loss:  0.25440505146980286
train gradient:  0.19376276867881292
iteration : 10080
train acc:  0.8671875
train loss:  0.2875582277774811
train gradient:  0.12905187554239617
iteration : 10081
train acc:  0.9140625
train loss:  0.24756655097007751
train gradient:  0.14799766391439612
iteration : 10082
train acc:  0.859375
train loss:  0.2587796151638031
train gradient:  0.1322399364599212
iteration : 10083
train acc:  0.8359375
train loss:  0.3766186833381653
train gradient:  0.22890586651005154
iteration : 10084
train acc:  0.90625
train loss:  0.22748211026191711
train gradient:  0.09149178220466166
iteration : 10085
train acc:  0.8125
train loss:  0.4246155023574829
train gradient:  0.24562251199113022
iteration : 10086
train acc:  0.8359375
train loss:  0.37864950299263
train gradient:  0.2376444554844293
iteration : 10087
train acc:  0.828125
train loss:  0.3489975333213806
train gradient:  0.38489819006489934
iteration : 10088
train acc:  0.9375
train loss:  0.2051018476486206
train gradient:  0.13405873496081372
iteration : 10089
train acc:  0.859375
train loss:  0.3753136992454529
train gradient:  0.18932243828263112
iteration : 10090
train acc:  0.8671875
train loss:  0.2789996266365051
train gradient:  0.14206283668454225
iteration : 10091
train acc:  0.828125
train loss:  0.3760739266872406
train gradient:  0.31829448179998515
iteration : 10092
train acc:  0.8515625
train loss:  0.30864661931991577
train gradient:  0.15131167325068137
iteration : 10093
train acc:  0.8359375
train loss:  0.41766831278800964
train gradient:  0.24678310372878554
iteration : 10094
train acc:  0.875
train loss:  0.2869794964790344
train gradient:  0.1831674986754438
iteration : 10095
train acc:  0.8515625
train loss:  0.31198379397392273
train gradient:  0.18812289440546062
iteration : 10096
train acc:  0.875
train loss:  0.3087318241596222
train gradient:  0.19115129791054386
iteration : 10097
train acc:  0.890625
train loss:  0.26101154088974
train gradient:  0.12648501621577193
iteration : 10098
train acc:  0.8515625
train loss:  0.33797186613082886
train gradient:  0.24847875563202682
iteration : 10099
train acc:  0.8515625
train loss:  0.3087095320224762
train gradient:  0.13856646518449198
iteration : 10100
train acc:  0.84375
train loss:  0.3691267967224121
train gradient:  0.3005815201282459
iteration : 10101
train acc:  0.875
train loss:  0.31399083137512207
train gradient:  0.16908170932846447
iteration : 10102
train acc:  0.890625
train loss:  0.3511382043361664
train gradient:  0.1638134656152644
iteration : 10103
train acc:  0.859375
train loss:  0.28754952549934387
train gradient:  0.18831705870274526
iteration : 10104
train acc:  0.84375
train loss:  0.38103559613227844
train gradient:  0.194313578535721
iteration : 10105
train acc:  0.7890625
train loss:  0.40996885299682617
train gradient:  0.3460089111381879
iteration : 10106
train acc:  0.8671875
train loss:  0.3055459260940552
train gradient:  0.158046641405974
iteration : 10107
train acc:  0.8203125
train loss:  0.3544115424156189
train gradient:  0.2448963169581517
iteration : 10108
train acc:  0.890625
train loss:  0.2729923725128174
train gradient:  0.11828040359155019
iteration : 10109
train acc:  0.796875
train loss:  0.46520566940307617
train gradient:  0.3367233139445055
iteration : 10110
train acc:  0.8046875
train loss:  0.4541758894920349
train gradient:  0.26497711078249414
iteration : 10111
train acc:  0.921875
train loss:  0.2007032334804535
train gradient:  0.08745521954863884
iteration : 10112
train acc:  0.875
train loss:  0.28888460993766785
train gradient:  0.1576975718358904
iteration : 10113
train acc:  0.8828125
train loss:  0.2956310510635376
train gradient:  0.17018488373957116
iteration : 10114
train acc:  0.8046875
train loss:  0.39749813079833984
train gradient:  0.2507930643216296
iteration : 10115
train acc:  0.8828125
train loss:  0.3249780535697937
train gradient:  0.1750535582243674
iteration : 10116
train acc:  0.8359375
train loss:  0.3782230019569397
train gradient:  0.1931569822733843
iteration : 10117
train acc:  0.8515625
train loss:  0.3177188038825989
train gradient:  0.20190689948637142
iteration : 10118
train acc:  0.8828125
train loss:  0.297612726688385
train gradient:  0.12123529827781063
iteration : 10119
train acc:  0.8046875
train loss:  0.37593895196914673
train gradient:  0.22485652575579973
iteration : 10120
train acc:  0.8125
train loss:  0.4114164710044861
train gradient:  0.21571797981373014
iteration : 10121
train acc:  0.828125
train loss:  0.32753705978393555
train gradient:  0.16123119486991722
iteration : 10122
train acc:  0.8125
train loss:  0.3944498896598816
train gradient:  0.1864289812973661
iteration : 10123
train acc:  0.8515625
train loss:  0.3117043375968933
train gradient:  0.15436282102763607
iteration : 10124
train acc:  0.8515625
train loss:  0.3119983971118927
train gradient:  0.1970665439411587
iteration : 10125
train acc:  0.875
train loss:  0.2904554605484009
train gradient:  0.17697952391001245
iteration : 10126
train acc:  0.90625
train loss:  0.28628021478652954
train gradient:  0.12558471836940352
iteration : 10127
train acc:  0.8125
train loss:  0.36615753173828125
train gradient:  0.1698873625220797
iteration : 10128
train acc:  0.8359375
train loss:  0.3484708070755005
train gradient:  0.15362030780821545
iteration : 10129
train acc:  0.8984375
train loss:  0.2936462461948395
train gradient:  0.11235962663754925
iteration : 10130
train acc:  0.8203125
train loss:  0.36839163303375244
train gradient:  0.1902556243522689
iteration : 10131
train acc:  0.84375
train loss:  0.36990290880203247
train gradient:  0.20388617331055287
iteration : 10132
train acc:  0.8828125
train loss:  0.2638359069824219
train gradient:  0.12320504484990666
iteration : 10133
train acc:  0.8515625
train loss:  0.33138906955718994
train gradient:  0.1391976154175835
iteration : 10134
train acc:  0.8828125
train loss:  0.2685481905937195
train gradient:  0.15254809830470734
iteration : 10135
train acc:  0.8984375
train loss:  0.23001259565353394
train gradient:  0.07775137412228829
iteration : 10136
train acc:  0.890625
train loss:  0.27491867542266846
train gradient:  0.09513032930734737
iteration : 10137
train acc:  0.8046875
train loss:  0.4320979118347168
train gradient:  0.21098965902829347
iteration : 10138
train acc:  0.890625
train loss:  0.33406758308410645
train gradient:  0.20959652069444937
iteration : 10139
train acc:  0.8515625
train loss:  0.34667161107063293
train gradient:  0.1497856591044255
iteration : 10140
train acc:  0.828125
train loss:  0.3180670440196991
train gradient:  0.12305184052660018
iteration : 10141
train acc:  0.828125
train loss:  0.44480425119400024
train gradient:  0.24198571640926636
iteration : 10142
train acc:  0.8359375
train loss:  0.3771916329860687
train gradient:  0.22147409982962563
iteration : 10143
train acc:  0.8359375
train loss:  0.3862603008747101
train gradient:  0.24904401295634676
iteration : 10144
train acc:  0.859375
train loss:  0.33107736706733704
train gradient:  0.23424101803320505
iteration : 10145
train acc:  0.8515625
train loss:  0.2969830632209778
train gradient:  0.13163524853426029
iteration : 10146
train acc:  0.8671875
train loss:  0.32087063789367676
train gradient:  0.23279296559407614
iteration : 10147
train acc:  0.8828125
train loss:  0.2921640872955322
train gradient:  0.1646307158456784
iteration : 10148
train acc:  0.828125
train loss:  0.3611050248146057
train gradient:  0.3369247332055601
iteration : 10149
train acc:  0.8515625
train loss:  0.3478839099407196
train gradient:  0.1936796380933469
iteration : 10150
train acc:  0.90625
train loss:  0.26570969820022583
train gradient:  0.10084912231433592
iteration : 10151
train acc:  0.8359375
train loss:  0.31981712579727173
train gradient:  0.15208251839685233
iteration : 10152
train acc:  0.859375
train loss:  0.37767913937568665
train gradient:  0.22020020988992867
iteration : 10153
train acc:  0.8203125
train loss:  0.3583534061908722
train gradient:  0.22266866984463768
iteration : 10154
train acc:  0.859375
train loss:  0.3280276656150818
train gradient:  0.20587056651804825
iteration : 10155
train acc:  0.890625
train loss:  0.26690053939819336
train gradient:  0.1501684983518083
iteration : 10156
train acc:  0.859375
train loss:  0.3703833222389221
train gradient:  0.30968146131810975
iteration : 10157
train acc:  0.859375
train loss:  0.29877254366874695
train gradient:  0.18051601331834888
iteration : 10158
train acc:  0.875
train loss:  0.35192376375198364
train gradient:  0.20726285479658685
iteration : 10159
train acc:  0.8359375
train loss:  0.34601011872291565
train gradient:  0.23952059812359294
iteration : 10160
train acc:  0.8515625
train loss:  0.3664036989212036
train gradient:  0.15868166281958865
iteration : 10161
train acc:  0.8671875
train loss:  0.2602767050266266
train gradient:  0.11447893552886378
iteration : 10162
train acc:  0.796875
train loss:  0.41904550790786743
train gradient:  0.24570709312853392
iteration : 10163
train acc:  0.890625
train loss:  0.28270596265792847
train gradient:  0.12862835753948215
iteration : 10164
train acc:  0.84375
train loss:  0.3389793634414673
train gradient:  0.20395605420494417
iteration : 10165
train acc:  0.8203125
train loss:  0.4162713885307312
train gradient:  0.27420926577003735
iteration : 10166
train acc:  0.875
train loss:  0.28001436591148376
train gradient:  0.1319221796667024
iteration : 10167
train acc:  0.828125
train loss:  0.4342804253101349
train gradient:  0.28246785758993237
iteration : 10168
train acc:  0.8359375
train loss:  0.38506338000297546
train gradient:  0.28290857029553695
iteration : 10169
train acc:  0.796875
train loss:  0.4572076201438904
train gradient:  0.318447360951181
iteration : 10170
train acc:  0.890625
train loss:  0.2743273079395294
train gradient:  0.07878838729049113
iteration : 10171
train acc:  0.7890625
train loss:  0.42705821990966797
train gradient:  0.23822759822402245
iteration : 10172
train acc:  0.8515625
train loss:  0.3651409149169922
train gradient:  0.17210510718003166
iteration : 10173
train acc:  0.890625
train loss:  0.31431129574775696
train gradient:  0.11703612832947402
iteration : 10174
train acc:  0.84375
train loss:  0.352219820022583
train gradient:  0.12916724104339605
iteration : 10175
train acc:  0.8671875
train loss:  0.3172074556350708
train gradient:  0.12780673936720993
iteration : 10176
train acc:  0.8203125
train loss:  0.38493266701698303
train gradient:  0.21571419083537163
iteration : 10177
train acc:  0.8515625
train loss:  0.3042546808719635
train gradient:  0.1966061822904428
iteration : 10178
train acc:  0.7734375
train loss:  0.4196062684059143
train gradient:  0.19487454707069599
iteration : 10179
train acc:  0.890625
train loss:  0.2549515962600708
train gradient:  0.11906515665670636
iteration : 10180
train acc:  0.8671875
train loss:  0.289236843585968
train gradient:  0.13506958081623022
iteration : 10181
train acc:  0.8046875
train loss:  0.46385470032691956
train gradient:  0.23001259700948323
iteration : 10182
train acc:  0.8359375
train loss:  0.3204113245010376
train gradient:  0.14948929915546727
iteration : 10183
train acc:  0.8359375
train loss:  0.3493451476097107
train gradient:  0.13473388018337817
iteration : 10184
train acc:  0.8828125
train loss:  0.2652140259742737
train gradient:  0.09404834646184913
iteration : 10185
train acc:  0.890625
train loss:  0.2968595623970032
train gradient:  0.13680943785764993
iteration : 10186
train acc:  0.90625
train loss:  0.288026362657547
train gradient:  0.11903588525140446
iteration : 10187
train acc:  0.8046875
train loss:  0.41480129957199097
train gradient:  0.1935074820021904
iteration : 10188
train acc:  0.859375
train loss:  0.3262554407119751
train gradient:  0.1464245018240271
iteration : 10189
train acc:  0.8515625
train loss:  0.30472204089164734
train gradient:  0.11647849428400281
iteration : 10190
train acc:  0.8125
train loss:  0.4199273884296417
train gradient:  0.284364360686607
iteration : 10191
train acc:  0.859375
train loss:  0.2954603433609009
train gradient:  0.15897077642667498
iteration : 10192
train acc:  0.875
train loss:  0.3150455355644226
train gradient:  0.1372927860463835
iteration : 10193
train acc:  0.859375
train loss:  0.30353355407714844
train gradient:  0.13425337882303318
iteration : 10194
train acc:  0.796875
train loss:  0.4238370656967163
train gradient:  0.23355673200194912
iteration : 10195
train acc:  0.875
train loss:  0.30655959248542786
train gradient:  0.11454541908841788
iteration : 10196
train acc:  0.9296875
train loss:  0.23283593356609344
train gradient:  0.07756494289935992
iteration : 10197
train acc:  0.8359375
train loss:  0.3601066768169403
train gradient:  0.13901003535737427
iteration : 10198
train acc:  0.8203125
train loss:  0.39368873834609985
train gradient:  0.20011716269874308
iteration : 10199
train acc:  0.8515625
train loss:  0.3145010471343994
train gradient:  0.18359151937994495
iteration : 10200
train acc:  0.8671875
train loss:  0.31673622131347656
train gradient:  0.18214129359504197
iteration : 10201
train acc:  0.8671875
train loss:  0.3111112713813782
train gradient:  0.15130009217655963
iteration : 10202
train acc:  0.8828125
train loss:  0.28595995903015137
train gradient:  0.14809170827525697
iteration : 10203
train acc:  0.8359375
train loss:  0.37732475996017456
train gradient:  0.20279709337823265
iteration : 10204
train acc:  0.875
train loss:  0.30831921100616455
train gradient:  0.10801071022267206
iteration : 10205
train acc:  0.8828125
train loss:  0.3110273778438568
train gradient:  0.10928763902790496
iteration : 10206
train acc:  0.8359375
train loss:  0.4026888906955719
train gradient:  0.21447726611615095
iteration : 10207
train acc:  0.8359375
train loss:  0.3700996935367584
train gradient:  0.1855108981466127
iteration : 10208
train acc:  0.859375
train loss:  0.3208598792552948
train gradient:  0.18019502167389603
iteration : 10209
train acc:  0.8046875
train loss:  0.41247376799583435
train gradient:  0.20847412734034854
iteration : 10210
train acc:  0.8828125
train loss:  0.3103623390197754
train gradient:  0.12404670854832084
iteration : 10211
train acc:  0.8984375
train loss:  0.28416043519973755
train gradient:  0.10437663590272563
iteration : 10212
train acc:  0.8515625
train loss:  0.30798569321632385
train gradient:  0.16641392815116515
iteration : 10213
train acc:  0.8828125
train loss:  0.29759979248046875
train gradient:  0.1094926222777951
iteration : 10214
train acc:  0.8515625
train loss:  0.3467027544975281
train gradient:  0.17769147249651945
iteration : 10215
train acc:  0.875
train loss:  0.30568087100982666
train gradient:  0.11272911557370109
iteration : 10216
train acc:  0.875
train loss:  0.3179028630256653
train gradient:  0.14617416220622875
iteration : 10217
train acc:  0.875
train loss:  0.33463138341903687
train gradient:  0.16122208944896743
iteration : 10218
train acc:  0.859375
train loss:  0.35042548179626465
train gradient:  0.16578087037760664
iteration : 10219
train acc:  0.8046875
train loss:  0.38124948740005493
train gradient:  0.24049507367918027
iteration : 10220
train acc:  0.8359375
train loss:  0.33553746342658997
train gradient:  0.1242083806830561
iteration : 10221
train acc:  0.8828125
train loss:  0.3320518732070923
train gradient:  0.1754308453144697
iteration : 10222
train acc:  0.9140625
train loss:  0.21766814589500427
train gradient:  0.09277333800281716
iteration : 10223
train acc:  0.859375
train loss:  0.28064101934432983
train gradient:  0.20983027074917932
iteration : 10224
train acc:  0.8671875
train loss:  0.27704837918281555
train gradient:  0.15700255392119766
iteration : 10225
train acc:  0.859375
train loss:  0.34670013189315796
train gradient:  0.1458162612242655
iteration : 10226
train acc:  0.8828125
train loss:  0.34742972254753113
train gradient:  0.19159881874910775
iteration : 10227
train acc:  0.8515625
train loss:  0.42440325021743774
train gradient:  0.29128770547495714
iteration : 10228
train acc:  0.890625
train loss:  0.26900216937065125
train gradient:  0.12159613943794982
iteration : 10229
train acc:  0.890625
train loss:  0.3136500120162964
train gradient:  0.11130758306946899
iteration : 10230
train acc:  0.84375
train loss:  0.37614792585372925
train gradient:  0.19711880497849313
iteration : 10231
train acc:  0.84375
train loss:  0.29672032594680786
train gradient:  0.12361166515805469
iteration : 10232
train acc:  0.90625
train loss:  0.30440640449523926
train gradient:  0.17394172742935177
iteration : 10233
train acc:  0.8671875
train loss:  0.276252418756485
train gradient:  0.13371424371554685
iteration : 10234
train acc:  0.8984375
train loss:  0.22712944447994232
train gradient:  0.08624192755981963
iteration : 10235
train acc:  0.859375
train loss:  0.29344671964645386
train gradient:  0.15275838182285245
iteration : 10236
train acc:  0.8828125
train loss:  0.3038170039653778
train gradient:  0.18096564641076196
iteration : 10237
train acc:  0.84375
train loss:  0.3277886211872101
train gradient:  0.23607738582728682
iteration : 10238
train acc:  0.8515625
train loss:  0.31205299496650696
train gradient:  0.12909834360290207
iteration : 10239
train acc:  0.84375
train loss:  0.35999321937561035
train gradient:  0.21338879949379694
iteration : 10240
train acc:  0.90625
train loss:  0.2696468234062195
train gradient:  0.10448166378994515
iteration : 10241
train acc:  0.859375
train loss:  0.3138645589351654
train gradient:  0.1775472740340768
iteration : 10242
train acc:  0.8203125
train loss:  0.3345627188682556
train gradient:  0.16647344300185782
iteration : 10243
train acc:  0.8828125
train loss:  0.3173837959766388
train gradient:  0.1816017866772479
iteration : 10244
train acc:  0.828125
train loss:  0.35559242963790894
train gradient:  0.21888952909801662
iteration : 10245
train acc:  0.8984375
train loss:  0.23020893335342407
train gradient:  0.12131970983178691
iteration : 10246
train acc:  0.8359375
train loss:  0.367318332195282
train gradient:  0.2010934716471382
iteration : 10247
train acc:  0.796875
train loss:  0.4167448878288269
train gradient:  0.27615322716181445
iteration : 10248
train acc:  0.9140625
train loss:  0.21736015379428864
train gradient:  0.09866671293227028
iteration : 10249
train acc:  0.8515625
train loss:  0.3137036859989166
train gradient:  0.16667729218081678
iteration : 10250
train acc:  0.8125
train loss:  0.3748607635498047
train gradient:  0.2181029639009462
iteration : 10251
train acc:  0.8828125
train loss:  0.30242881178855896
train gradient:  0.13936299271699057
iteration : 10252
train acc:  0.8828125
train loss:  0.27635663747787476
train gradient:  0.11931926826924473
iteration : 10253
train acc:  0.8828125
train loss:  0.30690860748291016
train gradient:  0.17173340599229026
iteration : 10254
train acc:  0.859375
train loss:  0.3569950461387634
train gradient:  0.17841676507900753
iteration : 10255
train acc:  0.8125
train loss:  0.4148545265197754
train gradient:  0.31039106801309585
iteration : 10256
train acc:  0.8671875
train loss:  0.3131692707538605
train gradient:  0.20834278556176522
iteration : 10257
train acc:  0.8828125
train loss:  0.2641944885253906
train gradient:  0.167864795047508
iteration : 10258
train acc:  0.8671875
train loss:  0.3535759449005127
train gradient:  0.5754867912077075
iteration : 10259
train acc:  0.859375
train loss:  0.3392196297645569
train gradient:  0.13731746621887903
iteration : 10260
train acc:  0.8828125
train loss:  0.24787504971027374
train gradient:  0.09960679177809954
iteration : 10261
train acc:  0.8671875
train loss:  0.28494489192962646
train gradient:  0.15119954992722914
iteration : 10262
train acc:  0.8671875
train loss:  0.2839139699935913
train gradient:  0.15994189103238293
iteration : 10263
train acc:  0.7890625
train loss:  0.4672775864601135
train gradient:  0.24852002931429934
iteration : 10264
train acc:  0.84375
train loss:  0.34345346689224243
train gradient:  0.19696572715358193
iteration : 10265
train acc:  0.8515625
train loss:  0.3167341947555542
train gradient:  0.16736760489105673
iteration : 10266
train acc:  0.8359375
train loss:  0.4145987331867218
train gradient:  0.40500678631734566
iteration : 10267
train acc:  0.859375
train loss:  0.31488659977912903
train gradient:  0.1894686097283546
iteration : 10268
train acc:  0.8359375
train loss:  0.37226298451423645
train gradient:  0.2683677018075059
iteration : 10269
train acc:  0.8515625
train loss:  0.29038798809051514
train gradient:  0.10353203029909842
iteration : 10270
train acc:  0.84375
train loss:  0.3376660943031311
train gradient:  0.15231608055714269
iteration : 10271
train acc:  0.828125
train loss:  0.3222445249557495
train gradient:  0.15764578409125807
iteration : 10272
train acc:  0.921875
train loss:  0.22498303651809692
train gradient:  0.10998025551549298
iteration : 10273
train acc:  0.8828125
train loss:  0.31366580724716187
train gradient:  0.1431787512249953
iteration : 10274
train acc:  0.9140625
train loss:  0.22447362542152405
train gradient:  0.22089637425962466
iteration : 10275
train acc:  0.8125
train loss:  0.4037075638771057
train gradient:  0.2877491376116947
iteration : 10276
train acc:  0.8515625
train loss:  0.31040871143341064
train gradient:  0.26003366547175744
iteration : 10277
train acc:  0.8984375
train loss:  0.32152843475341797
train gradient:  0.1435083919888482
iteration : 10278
train acc:  0.875
train loss:  0.30369246006011963
train gradient:  0.167876886556701
iteration : 10279
train acc:  0.859375
train loss:  0.31314560770988464
train gradient:  0.217117622066837
iteration : 10280
train acc:  0.828125
train loss:  0.4095875024795532
train gradient:  0.24908855052923645
iteration : 10281
train acc:  0.8671875
train loss:  0.2746613919734955
train gradient:  0.1407837638997852
iteration : 10282
train acc:  0.8828125
train loss:  0.327838659286499
train gradient:  0.20190469531774607
iteration : 10283
train acc:  0.8671875
train loss:  0.39091068506240845
train gradient:  0.20345377309124008
iteration : 10284
train acc:  0.8203125
train loss:  0.4226458668708801
train gradient:  0.20478573999598776
iteration : 10285
train acc:  0.9296875
train loss:  0.2947435975074768
train gradient:  0.1852582278480674
iteration : 10286
train acc:  0.8828125
train loss:  0.26825258135795593
train gradient:  0.1503813214802182
iteration : 10287
train acc:  0.8828125
train loss:  0.34529608488082886
train gradient:  0.17453490012144635
iteration : 10288
train acc:  0.828125
train loss:  0.34188124537467957
train gradient:  0.16254871441283142
iteration : 10289
train acc:  0.8046875
train loss:  0.354404091835022
train gradient:  0.1931838196908366
iteration : 10290
train acc:  0.90625
train loss:  0.3083588778972626
train gradient:  0.29010205384165877
iteration : 10291
train acc:  0.859375
train loss:  0.33475229144096375
train gradient:  0.1880272685015724
iteration : 10292
train acc:  0.8828125
train loss:  0.26582983136177063
train gradient:  0.12891006228040636
iteration : 10293
train acc:  0.90625
train loss:  0.24687260389328003
train gradient:  0.08915189056793268
iteration : 10294
train acc:  0.875
train loss:  0.35100531578063965
train gradient:  0.23725242961128634
iteration : 10295
train acc:  0.875
train loss:  0.2619529962539673
train gradient:  0.11333908201151009
iteration : 10296
train acc:  0.84375
train loss:  0.34257665276527405
train gradient:  0.14427863870148205
iteration : 10297
train acc:  0.8203125
train loss:  0.4309699833393097
train gradient:  0.20332683318848513
iteration : 10298
train acc:  0.8828125
train loss:  0.2768233120441437
train gradient:  0.13157487835585308
iteration : 10299
train acc:  0.859375
train loss:  0.4132256507873535
train gradient:  0.22253626765021173
iteration : 10300
train acc:  0.84375
train loss:  0.34372827410697937
train gradient:  0.24329999312863818
iteration : 10301
train acc:  0.859375
train loss:  0.29329347610473633
train gradient:  0.12508616374838902
iteration : 10302
train acc:  0.8203125
train loss:  0.3745298385620117
train gradient:  0.22066739229953808
iteration : 10303
train acc:  0.8515625
train loss:  0.2999829947948456
train gradient:  0.1276806675058339
iteration : 10304
train acc:  0.9140625
train loss:  0.228697270154953
train gradient:  0.11576134614917018
iteration : 10305
train acc:  0.8828125
train loss:  0.26588737964630127
train gradient:  0.1392949632046046
iteration : 10306
train acc:  0.8125
train loss:  0.4465637505054474
train gradient:  0.2043047256653195
iteration : 10307
train acc:  0.875
train loss:  0.3107810914516449
train gradient:  0.12785933939672028
iteration : 10308
train acc:  0.84375
train loss:  0.35443902015686035
train gradient:  0.2493723420105028
iteration : 10309
train acc:  0.8671875
train loss:  0.291926771402359
train gradient:  0.15750378037339618
iteration : 10310
train acc:  0.84375
train loss:  0.3512479364871979
train gradient:  0.260178166007197
iteration : 10311
train acc:  0.875
train loss:  0.2878834307193756
train gradient:  0.1278877783144623
iteration : 10312
train acc:  0.8828125
train loss:  0.3207242488861084
train gradient:  0.1639479562790329
iteration : 10313
train acc:  0.8515625
train loss:  0.35935157537460327
train gradient:  0.18328366125794937
iteration : 10314
train acc:  0.8515625
train loss:  0.3455567955970764
train gradient:  0.18894441837830717
iteration : 10315
train acc:  0.90625
train loss:  0.25272274017333984
train gradient:  0.09842967535019552
iteration : 10316
train acc:  0.84375
train loss:  0.3477035462856293
train gradient:  0.2063003256172834
iteration : 10317
train acc:  0.84375
train loss:  0.3427557945251465
train gradient:  0.21915648054226566
iteration : 10318
train acc:  0.8515625
train loss:  0.3249399662017822
train gradient:  0.16487237456448073
iteration : 10319
train acc:  0.84375
train loss:  0.42476940155029297
train gradient:  0.3343871432076745
iteration : 10320
train acc:  0.8359375
train loss:  0.35234057903289795
train gradient:  0.18556397432496624
iteration : 10321
train acc:  0.9140625
train loss:  0.28209298849105835
train gradient:  0.16335007021721196
iteration : 10322
train acc:  0.8671875
train loss:  0.31799593567848206
train gradient:  0.26813642899302165
iteration : 10323
train acc:  0.8359375
train loss:  0.340262234210968
train gradient:  0.2408075344706971
iteration : 10324
train acc:  0.859375
train loss:  0.3092948794364929
train gradient:  0.1562571708237078
iteration : 10325
train acc:  0.8359375
train loss:  0.35031765699386597
train gradient:  0.19082858690631976
iteration : 10326
train acc:  0.8984375
train loss:  0.23217494785785675
train gradient:  0.0903640425530802
iteration : 10327
train acc:  0.90625
train loss:  0.277165025472641
train gradient:  0.1589481035008217
iteration : 10328
train acc:  0.8125
train loss:  0.45478495955467224
train gradient:  0.3092876472665473
iteration : 10329
train acc:  0.8671875
train loss:  0.32253003120422363
train gradient:  0.11384052415871146
iteration : 10330
train acc:  0.8671875
train loss:  0.3092629909515381
train gradient:  0.1659073553138195
iteration : 10331
train acc:  0.8125
train loss:  0.37263578176498413
train gradient:  0.1814785960933317
iteration : 10332
train acc:  0.8203125
train loss:  0.4033791422843933
train gradient:  0.200461318193894
iteration : 10333
train acc:  0.8515625
train loss:  0.3781720995903015
train gradient:  0.23990622558602642
iteration : 10334
train acc:  0.890625
train loss:  0.26119285821914673
train gradient:  0.12141507749174739
iteration : 10335
train acc:  0.875
train loss:  0.2726088762283325
train gradient:  0.11797879706437964
iteration : 10336
train acc:  0.875
train loss:  0.33337271213531494
train gradient:  0.14695926520021693
iteration : 10337
train acc:  0.84375
train loss:  0.3325236439704895
train gradient:  0.1912126326670913
iteration : 10338
train acc:  0.8671875
train loss:  0.25695621967315674
train gradient:  0.11428786434519866
iteration : 10339
train acc:  0.8046875
train loss:  0.4049099087715149
train gradient:  0.2057216751881879
iteration : 10340
train acc:  0.890625
train loss:  0.2864724099636078
train gradient:  0.1607962844894028
iteration : 10341
train acc:  0.8125
train loss:  0.35750317573547363
train gradient:  0.14859636843322285
iteration : 10342
train acc:  0.78125
train loss:  0.4870476722717285
train gradient:  0.29477606066194384
iteration : 10343
train acc:  0.8984375
train loss:  0.2655375599861145
train gradient:  0.14025687844256324
iteration : 10344
train acc:  0.859375
train loss:  0.337014377117157
train gradient:  0.14393740734094385
iteration : 10345
train acc:  0.8671875
train loss:  0.2905493974685669
train gradient:  0.1559280106571697
iteration : 10346
train acc:  0.8515625
train loss:  0.36601680517196655
train gradient:  0.17554099579190266
iteration : 10347
train acc:  0.890625
train loss:  0.3181490898132324
train gradient:  0.20141611719898558
iteration : 10348
train acc:  0.8203125
train loss:  0.4336831569671631
train gradient:  0.25212531294316576
iteration : 10349
train acc:  0.859375
train loss:  0.32300180196762085
train gradient:  0.21558502716796138
iteration : 10350
train acc:  0.875
train loss:  0.32829511165618896
train gradient:  0.19707433836507757
iteration : 10351
train acc:  0.890625
train loss:  0.302693247795105
train gradient:  0.13404027261359652
iteration : 10352
train acc:  0.8359375
train loss:  0.3954010307788849
train gradient:  0.3007089632310382
iteration : 10353
train acc:  0.921875
train loss:  0.24682532250881195
train gradient:  0.1749250426902983
iteration : 10354
train acc:  0.859375
train loss:  0.3323974013328552
train gradient:  0.17252816261666498
iteration : 10355
train acc:  0.8828125
train loss:  0.34277862310409546
train gradient:  0.18550567135190268
iteration : 10356
train acc:  0.8125
train loss:  0.45144516229629517
train gradient:  0.3214455601313291
iteration : 10357
train acc:  0.859375
train loss:  0.30770546197891235
train gradient:  0.11094284364662337
iteration : 10358
train acc:  0.859375
train loss:  0.33712437748908997
train gradient:  0.17886018738394313
iteration : 10359
train acc:  0.875
train loss:  0.3027498126029968
train gradient:  0.13318900967837588
iteration : 10360
train acc:  0.8828125
train loss:  0.3218798041343689
train gradient:  0.14977746633672492
iteration : 10361
train acc:  0.8359375
train loss:  0.3407033085823059
train gradient:  0.13511765274009604
iteration : 10362
train acc:  0.890625
train loss:  0.30291545391082764
train gradient:  0.1612596298513884
iteration : 10363
train acc:  0.875
train loss:  0.320395290851593
train gradient:  0.13478264441068644
iteration : 10364
train acc:  0.8125
train loss:  0.36672520637512207
train gradient:  0.22896003977954854
iteration : 10365
train acc:  0.875
train loss:  0.2878555655479431
train gradient:  0.17382849999046857
iteration : 10366
train acc:  0.8828125
train loss:  0.2742975354194641
train gradient:  0.1420190186944453
iteration : 10367
train acc:  0.78125
train loss:  0.5125948786735535
train gradient:  0.33411147528438034
iteration : 10368
train acc:  0.8203125
train loss:  0.3799480199813843
train gradient:  0.2904112640932112
iteration : 10369
train acc:  0.8125
train loss:  0.34843647480010986
train gradient:  0.16637455697901013
iteration : 10370
train acc:  0.890625
train loss:  0.2892967164516449
train gradient:  0.3311369478764231
iteration : 10371
train acc:  0.8828125
train loss:  0.36590543389320374
train gradient:  0.1644902185136416
iteration : 10372
train acc:  0.875
train loss:  0.32572734355926514
train gradient:  0.175596832564609
iteration : 10373
train acc:  0.8671875
train loss:  0.28776878118515015
train gradient:  0.1530767337594372
iteration : 10374
train acc:  0.84375
train loss:  0.33222299814224243
train gradient:  0.15340977989768534
iteration : 10375
train acc:  0.8359375
train loss:  0.3269513249397278
train gradient:  0.19422380032116893
iteration : 10376
train acc:  0.8828125
train loss:  0.34860923886299133
train gradient:  0.13537270440368032
iteration : 10377
train acc:  0.890625
train loss:  0.2969764173030853
train gradient:  0.128876323085338
iteration : 10378
train acc:  0.828125
train loss:  0.36525383591651917
train gradient:  0.1727075283085323
iteration : 10379
train acc:  0.8125
train loss:  0.37888914346694946
train gradient:  0.22310082558193028
iteration : 10380
train acc:  0.796875
train loss:  0.4441455602645874
train gradient:  0.28278794798158896
iteration : 10381
train acc:  0.8125
train loss:  0.3439836800098419
train gradient:  0.1826575219767651
iteration : 10382
train acc:  0.90625
train loss:  0.24173881113529205
train gradient:  0.10672027261463359
iteration : 10383
train acc:  0.84375
train loss:  0.31247952580451965
train gradient:  0.1377143054383724
iteration : 10384
train acc:  0.859375
train loss:  0.3301364779472351
train gradient:  0.13296579459603636
iteration : 10385
train acc:  0.8046875
train loss:  0.3907250761985779
train gradient:  0.17212720228212042
iteration : 10386
train acc:  0.84375
train loss:  0.3507351875305176
train gradient:  0.1375401678234443
iteration : 10387
train acc:  0.875
train loss:  0.30142176151275635
train gradient:  0.1200874918897486
iteration : 10388
train acc:  0.90625
train loss:  0.2719574570655823
train gradient:  0.16219121435724973
iteration : 10389
train acc:  0.859375
train loss:  0.3440353274345398
train gradient:  0.21110403149242596
iteration : 10390
train acc:  0.859375
train loss:  0.26334479451179504
train gradient:  0.13892180713505342
iteration : 10391
train acc:  0.8828125
train loss:  0.27916085720062256
train gradient:  0.1551066786506345
iteration : 10392
train acc:  0.8359375
train loss:  0.3712325692176819
train gradient:  0.21820403966640278
iteration : 10393
train acc:  0.8203125
train loss:  0.3419516682624817
train gradient:  0.2109854500767859
iteration : 10394
train acc:  0.84375
train loss:  0.3325226902961731
train gradient:  0.17383832145184064
iteration : 10395
train acc:  0.90625
train loss:  0.2644277811050415
train gradient:  0.1038013584603672
iteration : 10396
train acc:  0.921875
train loss:  0.2335711121559143
train gradient:  0.08405762290381219
iteration : 10397
train acc:  0.84375
train loss:  0.3745013475418091
train gradient:  0.19790502109297592
iteration : 10398
train acc:  0.8671875
train loss:  0.29760318994522095
train gradient:  0.14622635049345892
iteration : 10399
train acc:  0.8671875
train loss:  0.2607010006904602
train gradient:  0.1024257943983089
iteration : 10400
train acc:  0.875
train loss:  0.36285850405693054
train gradient:  0.12722553897858374
iteration : 10401
train acc:  0.875
train loss:  0.29516080021858215
train gradient:  0.15134080752836215
iteration : 10402
train acc:  0.8359375
train loss:  0.42755797505378723
train gradient:  0.277412225691285
iteration : 10403
train acc:  0.859375
train loss:  0.3144243061542511
train gradient:  0.13485004981111182
iteration : 10404
train acc:  0.8984375
train loss:  0.31279683113098145
train gradient:  0.12108282791569078
iteration : 10405
train acc:  0.8203125
train loss:  0.3853188455104828
train gradient:  0.19247626890113823
iteration : 10406
train acc:  0.84375
train loss:  0.3445424735546112
train gradient:  0.43918150378884563
iteration : 10407
train acc:  0.828125
train loss:  0.33748859167099
train gradient:  0.1491431290937452
iteration : 10408
train acc:  0.921875
train loss:  0.265126496553421
train gradient:  0.10942201517855299
iteration : 10409
train acc:  0.890625
train loss:  0.2845785915851593
train gradient:  0.17533175488666514
iteration : 10410
train acc:  0.90625
train loss:  0.22974252700805664
train gradient:  0.09553638783657922
iteration : 10411
train acc:  0.859375
train loss:  0.3287615180015564
train gradient:  0.13072936638538835
iteration : 10412
train acc:  0.8046875
train loss:  0.4216524064540863
train gradient:  0.3521073145526403
iteration : 10413
train acc:  0.8515625
train loss:  0.3218492865562439
train gradient:  0.15241320105911563
iteration : 10414
train acc:  0.859375
train loss:  0.29605138301849365
train gradient:  0.1107973541123539
iteration : 10415
train acc:  0.8671875
train loss:  0.29378199577331543
train gradient:  0.10090197438418128
iteration : 10416
train acc:  0.828125
train loss:  0.4423075318336487
train gradient:  0.22569191828712393
iteration : 10417
train acc:  0.84375
train loss:  0.34700801968574524
train gradient:  0.18896420107479328
iteration : 10418
train acc:  0.84375
train loss:  0.33490216732025146
train gradient:  0.15947355637264637
iteration : 10419
train acc:  0.8671875
train loss:  0.2984621524810791
train gradient:  0.12675343336577868
iteration : 10420
train acc:  0.8828125
train loss:  0.3193436861038208
train gradient:  0.15366035545600804
iteration : 10421
train acc:  0.7734375
train loss:  0.45892637968063354
train gradient:  0.3373054780808324
iteration : 10422
train acc:  0.90625
train loss:  0.23476719856262207
train gradient:  0.08702260620699938
iteration : 10423
train acc:  0.8515625
train loss:  0.3862661123275757
train gradient:  0.1678911155479232
iteration : 10424
train acc:  0.8515625
train loss:  0.33451047539711
train gradient:  0.2040358760203149
iteration : 10425
train acc:  0.90625
train loss:  0.2922283709049225
train gradient:  0.15938363575956255
iteration : 10426
train acc:  0.875
train loss:  0.2599990963935852
train gradient:  0.10901693770115319
iteration : 10427
train acc:  0.859375
train loss:  0.252366304397583
train gradient:  0.10085233749208745
iteration : 10428
train acc:  0.859375
train loss:  0.35453712940216064
train gradient:  0.20718589861314357
iteration : 10429
train acc:  0.8203125
train loss:  0.41392257809638977
train gradient:  0.3267372881922193
iteration : 10430
train acc:  0.8671875
train loss:  0.30628257989883423
train gradient:  0.16400989222028406
iteration : 10431
train acc:  0.90625
train loss:  0.2373116910457611
train gradient:  0.10831318641622953
iteration : 10432
train acc:  0.875
train loss:  0.24955759942531586
train gradient:  0.09623966867211342
iteration : 10433
train acc:  0.84375
train loss:  0.3736547827720642
train gradient:  0.1720499149891158
iteration : 10434
train acc:  0.8359375
train loss:  0.37332573533058167
train gradient:  0.22631249950791044
iteration : 10435
train acc:  0.859375
train loss:  0.37376299500465393
train gradient:  0.20471966050597395
iteration : 10436
train acc:  0.859375
train loss:  0.35973435640335083
train gradient:  0.18330565454608647
iteration : 10437
train acc:  0.8203125
train loss:  0.3723430931568146
train gradient:  0.225795339456442
iteration : 10438
train acc:  0.796875
train loss:  0.39246419072151184
train gradient:  0.1857964434300861
iteration : 10439
train acc:  0.890625
train loss:  0.30001190304756165
train gradient:  0.12305234813020438
iteration : 10440
train acc:  0.8359375
train loss:  0.3494228720664978
train gradient:  0.1898304301516958
iteration : 10441
train acc:  0.8671875
train loss:  0.3054078221321106
train gradient:  0.1408239640184825
iteration : 10442
train acc:  0.8359375
train loss:  0.40958303213119507
train gradient:  0.2764134169760624
iteration : 10443
train acc:  0.8515625
train loss:  0.326864629983902
train gradient:  0.1703129766814952
iteration : 10444
train acc:  0.8828125
train loss:  0.29743558168411255
train gradient:  0.14745489614345592
iteration : 10445
train acc:  0.8984375
train loss:  0.27365273237228394
train gradient:  0.14770990318276017
iteration : 10446
train acc:  0.8828125
train loss:  0.28117334842681885
train gradient:  0.12316285138916311
iteration : 10447
train acc:  0.8828125
train loss:  0.2703532576560974
train gradient:  0.11800705031251799
iteration : 10448
train acc:  0.8125
train loss:  0.3587905168533325
train gradient:  0.2557209883202005
iteration : 10449
train acc:  0.8125
train loss:  0.3644563853740692
train gradient:  0.1832170487061892
iteration : 10450
train acc:  0.8984375
train loss:  0.2816956341266632
train gradient:  0.12399844484376718
iteration : 10451
train acc:  0.890625
train loss:  0.3180410861968994
train gradient:  0.19326996464409157
iteration : 10452
train acc:  0.8984375
train loss:  0.2584354281425476
train gradient:  0.2513066402030911
iteration : 10453
train acc:  0.8984375
train loss:  0.28286048769950867
train gradient:  0.12960597823680164
iteration : 10454
train acc:  0.7890625
train loss:  0.40660959482192993
train gradient:  0.2373422784688638
iteration : 10455
train acc:  0.8515625
train loss:  0.2943093478679657
train gradient:  0.13833135866404522
iteration : 10456
train acc:  0.875
train loss:  0.31609827280044556
train gradient:  0.12914974786542743
iteration : 10457
train acc:  0.90625
train loss:  0.29079484939575195
train gradient:  0.19243948002684105
iteration : 10458
train acc:  0.8671875
train loss:  0.3253600299358368
train gradient:  0.15701852713701658
iteration : 10459
train acc:  0.8984375
train loss:  0.2533147931098938
train gradient:  0.10383577187214081
iteration : 10460
train acc:  0.90625
train loss:  0.2543075978755951
train gradient:  0.16624279466572012
iteration : 10461
train acc:  0.859375
train loss:  0.29904741048812866
train gradient:  0.12412560646903084
iteration : 10462
train acc:  0.828125
train loss:  0.3031728267669678
train gradient:  0.149474593825781
iteration : 10463
train acc:  0.875
train loss:  0.3119572699069977
train gradient:  0.17483666811886514
iteration : 10464
train acc:  0.8828125
train loss:  0.28234022855758667
train gradient:  0.14992154697715032
iteration : 10465
train acc:  0.84375
train loss:  0.3386164903640747
train gradient:  0.16264119213471323
iteration : 10466
train acc:  0.828125
train loss:  0.3900459408760071
train gradient:  0.19076224276199238
iteration : 10467
train acc:  0.8359375
train loss:  0.33668503165245056
train gradient:  0.16453042126646467
iteration : 10468
train acc:  0.8671875
train loss:  0.327075719833374
train gradient:  0.16708848704275908
iteration : 10469
train acc:  0.8984375
train loss:  0.3148108422756195
train gradient:  0.11520665010193488
iteration : 10470
train acc:  0.78125
train loss:  0.40850508213043213
train gradient:  0.2698012804719805
iteration : 10471
train acc:  0.921875
train loss:  0.24157050251960754
train gradient:  0.12457025783607695
iteration : 10472
train acc:  0.875
train loss:  0.35222816467285156
train gradient:  0.23158819299394387
iteration : 10473
train acc:  0.859375
train loss:  0.37866735458374023
train gradient:  0.22608971774422104
iteration : 10474
train acc:  0.8515625
train loss:  0.3876345157623291
train gradient:  0.20177729903200475
iteration : 10475
train acc:  0.8203125
train loss:  0.3424749970436096
train gradient:  0.15653722460278646
iteration : 10476
train acc:  0.84375
train loss:  0.3673412799835205
train gradient:  0.2899749069959692
iteration : 10477
train acc:  0.8671875
train loss:  0.3193098306655884
train gradient:  0.1012140750346835
iteration : 10478
train acc:  0.8984375
train loss:  0.28598684072494507
train gradient:  0.19957873788673014
iteration : 10479
train acc:  0.8828125
train loss:  0.319366991519928
train gradient:  0.16980569238054707
iteration : 10480
train acc:  0.8046875
train loss:  0.43118467926979065
train gradient:  0.2833594986077752
iteration : 10481
train acc:  0.84375
train loss:  0.337918758392334
train gradient:  0.17472826395787333
iteration : 10482
train acc:  0.8359375
train loss:  0.33697348833084106
train gradient:  0.14030926666242033
iteration : 10483
train acc:  0.8515625
train loss:  0.34855663776397705
train gradient:  0.1662345168227449
iteration : 10484
train acc:  0.8984375
train loss:  0.28491878509521484
train gradient:  0.11984033908547448
iteration : 10485
train acc:  0.84375
train loss:  0.34364354610443115
train gradient:  0.2390693121375599
iteration : 10486
train acc:  0.8828125
train loss:  0.30224859714508057
train gradient:  0.14529904260147541
iteration : 10487
train acc:  0.7734375
train loss:  0.4215923547744751
train gradient:  0.1668902711357027
iteration : 10488
train acc:  0.84375
train loss:  0.4252423048019409
train gradient:  0.22681570549502034
iteration : 10489
train acc:  0.8125
train loss:  0.35061752796173096
train gradient:  0.16587600947754452
iteration : 10490
train acc:  0.8203125
train loss:  0.3771895468235016
train gradient:  0.21055421345912725
iteration : 10491
train acc:  0.7734375
train loss:  0.4757401943206787
train gradient:  0.2969389244934254
iteration : 10492
train acc:  0.8828125
train loss:  0.27271661162376404
train gradient:  0.12032549257884866
iteration : 10493
train acc:  0.8125
train loss:  0.5127017498016357
train gradient:  0.28112354180613314
iteration : 10494
train acc:  0.890625
train loss:  0.2732200026512146
train gradient:  0.12417256182533867
iteration : 10495
train acc:  0.8984375
train loss:  0.27222368121147156
train gradient:  0.09257112400193467
iteration : 10496
train acc:  0.859375
train loss:  0.3042922019958496
train gradient:  0.1629636702886834
iteration : 10497
train acc:  0.8984375
train loss:  0.34293484687805176
train gradient:  0.10872974060954801
iteration : 10498
train acc:  0.84375
train loss:  0.3617938756942749
train gradient:  0.12858303144489114
iteration : 10499
train acc:  0.8828125
train loss:  0.2898404002189636
train gradient:  0.12879865294552786
iteration : 10500
train acc:  0.8828125
train loss:  0.28532081842422485
train gradient:  0.11745507757611258
iteration : 10501
train acc:  0.8203125
train loss:  0.32988619804382324
train gradient:  0.16948694402041625
iteration : 10502
train acc:  0.890625
train loss:  0.28845298290252686
train gradient:  0.0945149092379348
iteration : 10503
train acc:  0.8359375
train loss:  0.4275488555431366
train gradient:  0.22064588492058357
iteration : 10504
train acc:  0.875
train loss:  0.2557893693447113
train gradient:  0.09088674548375429
iteration : 10505
train acc:  0.8203125
train loss:  0.4164614677429199
train gradient:  0.20305575419581767
iteration : 10506
train acc:  0.828125
train loss:  0.4075990319252014
train gradient:  0.2162290481930228
iteration : 10507
train acc:  0.875
train loss:  0.3184078633785248
train gradient:  0.11161704887957105
iteration : 10508
train acc:  0.828125
train loss:  0.37986573576927185
train gradient:  0.18311138688466158
iteration : 10509
train acc:  0.8515625
train loss:  0.36387696862220764
train gradient:  0.197786827708559
iteration : 10510
train acc:  0.8828125
train loss:  0.2967778444290161
train gradient:  0.13999335217131648
iteration : 10511
train acc:  0.828125
train loss:  0.37665653228759766
train gradient:  0.205854844245581
iteration : 10512
train acc:  0.7890625
train loss:  0.4029155373573303
train gradient:  0.20495307708596414
iteration : 10513
train acc:  0.8203125
train loss:  0.3957699239253998
train gradient:  0.2844213048475902
iteration : 10514
train acc:  0.8515625
train loss:  0.3383926749229431
train gradient:  0.15512668385562337
iteration : 10515
train acc:  0.890625
train loss:  0.2849671244621277
train gradient:  0.13906316659758167
iteration : 10516
train acc:  0.890625
train loss:  0.26789844036102295
train gradient:  0.15027490232067667
iteration : 10517
train acc:  0.84375
train loss:  0.3578629791736603
train gradient:  0.11135733240594455
iteration : 10518
train acc:  0.875
train loss:  0.30147093534469604
train gradient:  0.11870356235850751
iteration : 10519
train acc:  0.8359375
train loss:  0.3887518644332886
train gradient:  0.23852867221568302
iteration : 10520
train acc:  0.859375
train loss:  0.3011561334133148
train gradient:  0.15705557848212726
iteration : 10521
train acc:  0.84375
train loss:  0.34295618534088135
train gradient:  0.17276848386691152
iteration : 10522
train acc:  0.875
train loss:  0.37770822644233704
train gradient:  0.11755617066570911
iteration : 10523
train acc:  0.828125
train loss:  0.3117046058177948
train gradient:  0.148736546717203
iteration : 10524
train acc:  0.859375
train loss:  0.2779594361782074
train gradient:  0.11487632434647344
iteration : 10525
train acc:  0.828125
train loss:  0.43006664514541626
train gradient:  0.1937621014589559
iteration : 10526
train acc:  0.875
train loss:  0.3110131621360779
train gradient:  0.1516442704258586
iteration : 10527
train acc:  0.8515625
train loss:  0.32810693979263306
train gradient:  0.12762627581714045
iteration : 10528
train acc:  0.8828125
train loss:  0.25945794582366943
train gradient:  0.41809467669178585
iteration : 10529
train acc:  0.8828125
train loss:  0.2971227169036865
train gradient:  0.13529723786827774
iteration : 10530
train acc:  0.875
train loss:  0.3601236343383789
train gradient:  0.20971124747389625
iteration : 10531
train acc:  0.8125
train loss:  0.3596442639827728
train gradient:  0.1585061550330278
iteration : 10532
train acc:  0.90625
train loss:  0.28531414270401
train gradient:  0.11387478973176819
iteration : 10533
train acc:  0.828125
train loss:  0.47936972975730896
train gradient:  0.2922222364839563
iteration : 10534
train acc:  0.8203125
train loss:  0.3881244659423828
train gradient:  0.16936572649828235
iteration : 10535
train acc:  0.8125
train loss:  0.41672033071517944
train gradient:  0.19535603797754733
iteration : 10536
train acc:  0.8515625
train loss:  0.39002203941345215
train gradient:  0.19438397795114457
iteration : 10537
train acc:  0.8359375
train loss:  0.33498525619506836
train gradient:  0.11851439954647965
iteration : 10538
train acc:  0.8203125
train loss:  0.3810654580593109
train gradient:  0.1434764643917782
iteration : 10539
train acc:  0.8671875
train loss:  0.33813226222991943
train gradient:  0.13626195731155227
iteration : 10540
train acc:  0.859375
train loss:  0.3455696702003479
train gradient:  0.14443775099130796
iteration : 10541
train acc:  0.859375
train loss:  0.3270584046840668
train gradient:  0.17989095130669353
iteration : 10542
train acc:  0.875
train loss:  0.30569911003112793
train gradient:  0.1263582695469003
iteration : 10543
train acc:  0.8671875
train loss:  0.28957781195640564
train gradient:  0.15928125616552122
iteration : 10544
train acc:  0.9140625
train loss:  0.2727122902870178
train gradient:  0.11099035215251837
iteration : 10545
train acc:  0.8203125
train loss:  0.32392486929893494
train gradient:  0.17221819714748887
iteration : 10546
train acc:  0.875
train loss:  0.31793850660324097
train gradient:  0.2259414816576411
iteration : 10547
train acc:  0.890625
train loss:  0.3135680556297302
train gradient:  0.22063963125752217
iteration : 10548
train acc:  0.859375
train loss:  0.3653818964958191
train gradient:  0.1308412146489474
iteration : 10549
train acc:  0.8203125
train loss:  0.4280396103858948
train gradient:  0.19458480653723548
iteration : 10550
train acc:  0.8046875
train loss:  0.4052830636501312
train gradient:  0.2258813252432072
iteration : 10551
train acc:  0.828125
train loss:  0.3472349941730499
train gradient:  0.12247009790256116
iteration : 10552
train acc:  0.8984375
train loss:  0.25314927101135254
train gradient:  0.09044759252447859
iteration : 10553
train acc:  0.8515625
train loss:  0.3590330481529236
train gradient:  0.139432907890164
iteration : 10554
train acc:  0.8515625
train loss:  0.3672318458557129
train gradient:  0.13810346690623426
iteration : 10555
train acc:  0.84375
train loss:  0.37417006492614746
train gradient:  0.1679140551814197
iteration : 10556
train acc:  0.8515625
train loss:  0.3040321171283722
train gradient:  0.10831143585558489
iteration : 10557
train acc:  0.859375
train loss:  0.3445286750793457
train gradient:  0.20847132107957267
iteration : 10558
train acc:  0.8125
train loss:  0.4124947488307953
train gradient:  0.3517334339341348
iteration : 10559
train acc:  0.890625
train loss:  0.2915361821651459
train gradient:  0.15768262717031445
iteration : 10560
train acc:  0.8359375
train loss:  0.3962278366088867
train gradient:  0.23305095981007468
iteration : 10561
train acc:  0.8203125
train loss:  0.3538243770599365
train gradient:  0.2028464782620758
iteration : 10562
train acc:  0.8515625
train loss:  0.2777450680732727
train gradient:  0.12127644135922865
iteration : 10563
train acc:  0.8203125
train loss:  0.3898693323135376
train gradient:  0.19099550836068188
iteration : 10564
train acc:  0.890625
train loss:  0.2536908686161041
train gradient:  0.10436525871076437
iteration : 10565
train acc:  0.9140625
train loss:  0.24845357239246368
train gradient:  0.08296194956330291
iteration : 10566
train acc:  0.84375
train loss:  0.3194442689418793
train gradient:  0.13475596272844875
iteration : 10567
train acc:  0.890625
train loss:  0.27047330141067505
train gradient:  0.18317707955921997
iteration : 10568
train acc:  0.875
train loss:  0.32205551862716675
train gradient:  0.14037414758290764
iteration : 10569
train acc:  0.875
train loss:  0.2951575517654419
train gradient:  0.11882426057363277
iteration : 10570
train acc:  0.90625
train loss:  0.29072368144989014
train gradient:  0.12004568236930499
iteration : 10571
train acc:  0.8515625
train loss:  0.332063227891922
train gradient:  0.1591696715949964
iteration : 10572
train acc:  0.859375
train loss:  0.3585495948791504
train gradient:  0.17045252619145917
iteration : 10573
train acc:  0.8828125
train loss:  0.28994083404541016
train gradient:  0.09411623304585905
iteration : 10574
train acc:  0.859375
train loss:  0.3066785931587219
train gradient:  0.11961219830277874
iteration : 10575
train acc:  0.875
train loss:  0.32091668248176575
train gradient:  0.14474998308763046
iteration : 10576
train acc:  0.796875
train loss:  0.3988197445869446
train gradient:  0.19825672383438678
iteration : 10577
train acc:  0.875
train loss:  0.30625131726264954
train gradient:  0.12354343550425277
iteration : 10578
train acc:  0.8515625
train loss:  0.33068034052848816
train gradient:  0.21575243083031118
iteration : 10579
train acc:  0.8515625
train loss:  0.3962141275405884
train gradient:  0.1995494241051024
iteration : 10580
train acc:  0.8515625
train loss:  0.30877363681793213
train gradient:  0.133096339835727
iteration : 10581
train acc:  0.828125
train loss:  0.3187098205089569
train gradient:  0.14657962618026787
iteration : 10582
train acc:  0.90625
train loss:  0.2634546160697937
train gradient:  0.09412531459551642
iteration : 10583
train acc:  0.8671875
train loss:  0.3210296630859375
train gradient:  0.09505793502281087
iteration : 10584
train acc:  0.859375
train loss:  0.32466626167297363
train gradient:  0.09874773602887224
iteration : 10585
train acc:  0.84375
train loss:  0.3101646304130554
train gradient:  0.13717641517598989
iteration : 10586
train acc:  0.8828125
train loss:  0.33978283405303955
train gradient:  0.22494649682597528
iteration : 10587
train acc:  0.8515625
train loss:  0.2897239923477173
train gradient:  0.14313350960024385
iteration : 10588
train acc:  0.8671875
train loss:  0.3359842896461487
train gradient:  0.1318212261290162
iteration : 10589
train acc:  0.8984375
train loss:  0.28811758756637573
train gradient:  0.1465805585490567
iteration : 10590
train acc:  0.8515625
train loss:  0.35963839292526245
train gradient:  0.17680834860182612
iteration : 10591
train acc:  0.8125
train loss:  0.3922582268714905
train gradient:  0.2141443479555343
iteration : 10592
train acc:  0.90625
train loss:  0.2596036195755005
train gradient:  0.08138142401115699
iteration : 10593
train acc:  0.875
train loss:  0.27544015645980835
train gradient:  0.12230388823195798
iteration : 10594
train acc:  0.8515625
train loss:  0.32311564683914185
train gradient:  0.15381932608820753
iteration : 10595
train acc:  0.796875
train loss:  0.4467577040195465
train gradient:  0.26249275844604714
iteration : 10596
train acc:  0.875
train loss:  0.30731314420700073
train gradient:  0.1480687973736678
iteration : 10597
train acc:  0.8359375
train loss:  0.32640963792800903
train gradient:  0.18156861559498
iteration : 10598
train acc:  0.9296875
train loss:  0.2226320505142212
train gradient:  0.09317630160841223
iteration : 10599
train acc:  0.890625
train loss:  0.27949967980384827
train gradient:  0.13007824225511316
iteration : 10600
train acc:  0.859375
train loss:  0.3663118779659271
train gradient:  0.15095390206609124
iteration : 10601
train acc:  0.84375
train loss:  0.3418646454811096
train gradient:  0.15441297357857273
iteration : 10602
train acc:  0.8515625
train loss:  0.3584132790565491
train gradient:  0.19054773806262026
iteration : 10603
train acc:  0.8515625
train loss:  0.3372752070426941
train gradient:  0.1700739211766523
iteration : 10604
train acc:  0.8203125
train loss:  0.34259849786758423
train gradient:  0.2198792088269049
iteration : 10605
train acc:  0.8046875
train loss:  0.44240429997444153
train gradient:  0.25833784849990965
iteration : 10606
train acc:  0.875
train loss:  0.34722182154655457
train gradient:  0.17634321841809597
iteration : 10607
train acc:  0.8359375
train loss:  0.3452141284942627
train gradient:  0.16519201071087525
iteration : 10608
train acc:  0.8671875
train loss:  0.31375110149383545
train gradient:  0.1045236690422621
iteration : 10609
train acc:  0.8125
train loss:  0.3398997485637665
train gradient:  0.27133679658585785
iteration : 10610
train acc:  0.875
train loss:  0.3175187408924103
train gradient:  0.1490177941691737
iteration : 10611
train acc:  0.859375
train loss:  0.34808534383773804
train gradient:  0.1998166448765593
iteration : 10612
train acc:  0.859375
train loss:  0.35417354106903076
train gradient:  0.1438344858017053
iteration : 10613
train acc:  0.8359375
train loss:  0.386141836643219
train gradient:  0.224461670767095
iteration : 10614
train acc:  0.859375
train loss:  0.29913103580474854
train gradient:  0.1239670251965532
iteration : 10615
train acc:  0.84375
train loss:  0.291187047958374
train gradient:  0.17973908848602982
iteration : 10616
train acc:  0.8671875
train loss:  0.31267374753952026
train gradient:  0.16628581721991495
iteration : 10617
train acc:  0.890625
train loss:  0.2549329400062561
train gradient:  0.0957732111772926
iteration : 10618
train acc:  0.8671875
train loss:  0.3762322962284088
train gradient:  0.18838322595232682
iteration : 10619
train acc:  0.8515625
train loss:  0.3583851456642151
train gradient:  0.13572223549808665
iteration : 10620
train acc:  0.8671875
train loss:  0.32764604687690735
train gradient:  0.1457524163480186
iteration : 10621
train acc:  0.875
train loss:  0.27564021944999695
train gradient:  0.11655679006059472
iteration : 10622
train acc:  0.8359375
train loss:  0.34259921312332153
train gradient:  0.15271262446827277
iteration : 10623
train acc:  0.8671875
train loss:  0.36982429027557373
train gradient:  0.15237349106918507
iteration : 10624
train acc:  0.8203125
train loss:  0.43753325939178467
train gradient:  0.22385896371474442
iteration : 10625
train acc:  0.8515625
train loss:  0.3492693305015564
train gradient:  0.14148593483619004
iteration : 10626
train acc:  0.8515625
train loss:  0.35222405195236206
train gradient:  0.2582203081192196
iteration : 10627
train acc:  0.828125
train loss:  0.4666408896446228
train gradient:  0.19120493983523795
iteration : 10628
train acc:  0.828125
train loss:  0.3558414876461029
train gradient:  0.21569474269582622
iteration : 10629
train acc:  0.8515625
train loss:  0.33405229449272156
train gradient:  0.1659006894881151
iteration : 10630
train acc:  0.8671875
train loss:  0.2954986095428467
train gradient:  0.12093858804461992
iteration : 10631
train acc:  0.7890625
train loss:  0.3983369469642639
train gradient:  0.24808129787232097
iteration : 10632
train acc:  0.8828125
train loss:  0.2913340926170349
train gradient:  0.14003309222633242
iteration : 10633
train acc:  0.8515625
train loss:  0.2925347685813904
train gradient:  0.12149697237176652
iteration : 10634
train acc:  0.875
train loss:  0.31756752729415894
train gradient:  0.09962160612049953
iteration : 10635
train acc:  0.8671875
train loss:  0.28895819187164307
train gradient:  0.12079779473219156
iteration : 10636
train acc:  0.828125
train loss:  0.4145274758338928
train gradient:  0.2389148829545315
iteration : 10637
train acc:  0.8515625
train loss:  0.351237952709198
train gradient:  0.16445936439424833
iteration : 10638
train acc:  0.7890625
train loss:  0.3937377333641052
train gradient:  0.1645314076346776
iteration : 10639
train acc:  0.8515625
train loss:  0.3224816918373108
train gradient:  0.1328917085373115
iteration : 10640
train acc:  0.875
train loss:  0.3205723166465759
train gradient:  0.1279827354397362
iteration : 10641
train acc:  0.828125
train loss:  0.3471504747867584
train gradient:  0.2468204119630037
iteration : 10642
train acc:  0.828125
train loss:  0.4111044704914093
train gradient:  0.21312183556683173
iteration : 10643
train acc:  0.9140625
train loss:  0.2770498991012573
train gradient:  0.10819840929878573
iteration : 10644
train acc:  0.8515625
train loss:  0.32239484786987305
train gradient:  0.12353424320440391
iteration : 10645
train acc:  0.859375
train loss:  0.32176050543785095
train gradient:  0.09694052162316824
iteration : 10646
train acc:  0.875
train loss:  0.3675467073917389
train gradient:  0.18757097105963372
iteration : 10647
train acc:  0.8828125
train loss:  0.31484711170196533
train gradient:  0.13184343190299647
iteration : 10648
train acc:  0.8125
train loss:  0.36949843168258667
train gradient:  0.20698164078500922
iteration : 10649
train acc:  0.84375
train loss:  0.33877861499786377
train gradient:  0.1604073637645594
iteration : 10650
train acc:  0.890625
train loss:  0.29932451248168945
train gradient:  0.11949491197107766
iteration : 10651
train acc:  0.8515625
train loss:  0.3388594388961792
train gradient:  0.2921644871055134
iteration : 10652
train acc:  0.796875
train loss:  0.38459622859954834
train gradient:  0.1394814258486966
iteration : 10653
train acc:  0.875
train loss:  0.27113014459609985
train gradient:  0.12834741221415882
iteration : 10654
train acc:  0.8359375
train loss:  0.31231361627578735
train gradient:  0.15011322937035212
iteration : 10655
train acc:  0.8671875
train loss:  0.30228304862976074
train gradient:  0.12476403651941247
iteration : 10656
train acc:  0.8515625
train loss:  0.33768224716186523
train gradient:  0.187423379697242
iteration : 10657
train acc:  0.921875
train loss:  0.23615038394927979
train gradient:  0.13790027446759798
iteration : 10658
train acc:  0.8828125
train loss:  0.2925499677658081
train gradient:  0.1333352445034881
iteration : 10659
train acc:  0.84375
train loss:  0.30050262808799744
train gradient:  0.16674666003568017
iteration : 10660
train acc:  0.8125
train loss:  0.4305233955383301
train gradient:  0.37176001691714117
iteration : 10661
train acc:  0.875
train loss:  0.28020989894866943
train gradient:  0.14996206063719597
iteration : 10662
train acc:  0.8359375
train loss:  0.3584110736846924
train gradient:  0.17201770266441924
iteration : 10663
train acc:  0.875
train loss:  0.3301597833633423
train gradient:  0.1236503994073498
iteration : 10664
train acc:  0.8671875
train loss:  0.319915235042572
train gradient:  0.189232883216121
iteration : 10665
train acc:  0.8359375
train loss:  0.36542201042175293
train gradient:  0.1676814254764342
iteration : 10666
train acc:  0.84375
train loss:  0.37117040157318115
train gradient:  0.1929036795535038
iteration : 10667
train acc:  0.875
train loss:  0.34421563148498535
train gradient:  0.14860457439593627
iteration : 10668
train acc:  0.8125
train loss:  0.34077584743499756
train gradient:  0.21081817011896659
iteration : 10669
train acc:  0.8515625
train loss:  0.38683581352233887
train gradient:  0.16794298005076838
iteration : 10670
train acc:  0.828125
train loss:  0.3771629333496094
train gradient:  0.1636647705894163
iteration : 10671
train acc:  0.875
train loss:  0.29348236322402954
train gradient:  0.10127114745192171
iteration : 10672
train acc:  0.8203125
train loss:  0.4003279209136963
train gradient:  0.2200107305908181
iteration : 10673
train acc:  0.8359375
train loss:  0.3542080521583557
train gradient:  0.1627625021490679
iteration : 10674
train acc:  0.9296875
train loss:  0.23528632521629333
train gradient:  0.1419787207820748
iteration : 10675
train acc:  0.84375
train loss:  0.3191947042942047
train gradient:  0.19319109476126056
iteration : 10676
train acc:  0.859375
train loss:  0.3250313997268677
train gradient:  0.11847116666327792
iteration : 10677
train acc:  0.828125
train loss:  0.39138689637184143
train gradient:  0.20759711388166624
iteration : 10678
train acc:  0.8203125
train loss:  0.4206198453903198
train gradient:  0.27319605935475866
iteration : 10679
train acc:  0.8125
train loss:  0.4309549927711487
train gradient:  0.28869991050771393
iteration : 10680
train acc:  0.875
train loss:  0.2481219470500946
train gradient:  0.11701653535679739
iteration : 10681
train acc:  0.859375
train loss:  0.33636337518692017
train gradient:  0.2000897460010007
iteration : 10682
train acc:  0.859375
train loss:  0.31556573510169983
train gradient:  0.13534360139882906
iteration : 10683
train acc:  0.859375
train loss:  0.3141515254974365
train gradient:  0.12716261727913214
iteration : 10684
train acc:  0.8671875
train loss:  0.26325398683547974
train gradient:  0.17474484499896717
iteration : 10685
train acc:  0.84375
train loss:  0.31715357303619385
train gradient:  0.18878763510177374
iteration : 10686
train acc:  0.921875
train loss:  0.23101192712783813
train gradient:  0.10822021947059383
iteration : 10687
train acc:  0.7734375
train loss:  0.370097815990448
train gradient:  0.18458132791002693
iteration : 10688
train acc:  0.8515625
train loss:  0.35234302282333374
train gradient:  0.180432288811902
iteration : 10689
train acc:  0.8828125
train loss:  0.28815096616744995
train gradient:  0.1737354770699158
iteration : 10690
train acc:  0.8515625
train loss:  0.30037274956703186
train gradient:  0.129996488149302
iteration : 10691
train acc:  0.828125
train loss:  0.41779395937919617
train gradient:  0.5484428421342464
iteration : 10692
train acc:  0.828125
train loss:  0.3868558704853058
train gradient:  0.2163300743857849
iteration : 10693
train acc:  0.890625
train loss:  0.2750009298324585
train gradient:  0.20877794415313655
iteration : 10694
train acc:  0.890625
train loss:  0.2866820693016052
train gradient:  0.13706273827352086
iteration : 10695
train acc:  0.8828125
train loss:  0.2964504063129425
train gradient:  0.11679340901552768
iteration : 10696
train acc:  0.8828125
train loss:  0.2663520574569702
train gradient:  0.14328805321869859
iteration : 10697
train acc:  0.828125
train loss:  0.39146363735198975
train gradient:  0.34280647685736604
iteration : 10698
train acc:  0.875
train loss:  0.34078821539878845
train gradient:  0.1450606934190236
iteration : 10699
train acc:  0.9453125
train loss:  0.21719105541706085
train gradient:  0.10176702936205807
iteration : 10700
train acc:  0.8671875
train loss:  0.2738416790962219
train gradient:  0.11510924367831088
iteration : 10701
train acc:  0.8671875
train loss:  0.2865505814552307
train gradient:  0.1448672357577207
iteration : 10702
train acc:  0.8125
train loss:  0.37232762575149536
train gradient:  0.30153145581062946
iteration : 10703
train acc:  0.796875
train loss:  0.3856451213359833
train gradient:  0.21192391169297295
iteration : 10704
train acc:  0.859375
train loss:  0.3457355201244354
train gradient:  0.22219375005710296
iteration : 10705
train acc:  0.875
train loss:  0.26946622133255005
train gradient:  0.16937660501213586
iteration : 10706
train acc:  0.84375
train loss:  0.3594508171081543
train gradient:  0.17962639794104385
iteration : 10707
train acc:  0.8203125
train loss:  0.34856560826301575
train gradient:  0.16942763640826775
iteration : 10708
train acc:  0.8515625
train loss:  0.3664470314979553
train gradient:  0.3811525786089389
iteration : 10709
train acc:  0.9140625
train loss:  0.2576181888580322
train gradient:  0.14497846375435186
iteration : 10710
train acc:  0.8359375
train loss:  0.3063749074935913
train gradient:  0.13792901975497665
iteration : 10711
train acc:  0.84375
train loss:  0.36069971323013306
train gradient:  0.22975641031971603
iteration : 10712
train acc:  0.8203125
train loss:  0.36548861861228943
train gradient:  0.18435825507910478
iteration : 10713
train acc:  0.8046875
train loss:  0.42853668332099915
train gradient:  0.22490620483134643
iteration : 10714
train acc:  0.8671875
train loss:  0.32851335406303406
train gradient:  0.16099365393521115
iteration : 10715
train acc:  0.8828125
train loss:  0.3202643394470215
train gradient:  0.13261960646898052
iteration : 10716
train acc:  0.84375
train loss:  0.3325055241584778
train gradient:  0.2157316715845896
iteration : 10717
train acc:  0.8515625
train loss:  0.3076133728027344
train gradient:  0.14429284996043526
iteration : 10718
train acc:  0.8671875
train loss:  0.34207630157470703
train gradient:  0.15954519202004303
iteration : 10719
train acc:  0.875
train loss:  0.2967150807380676
train gradient:  0.15126348201789505
iteration : 10720
train acc:  0.875
train loss:  0.3174724578857422
train gradient:  0.15466756872921433
iteration : 10721
train acc:  0.890625
train loss:  0.2968751788139343
train gradient:  0.15440983501263278
iteration : 10722
train acc:  0.8671875
train loss:  0.3173331022262573
train gradient:  0.20937653746218476
iteration : 10723
train acc:  0.84375
train loss:  0.3288801610469818
train gradient:  0.15427464900768972
iteration : 10724
train acc:  0.890625
train loss:  0.2553744316101074
train gradient:  0.11412363410891284
iteration : 10725
train acc:  0.8671875
train loss:  0.3098382353782654
train gradient:  0.20216031974081383
iteration : 10726
train acc:  0.828125
train loss:  0.3863890767097473
train gradient:  0.16016827163709396
iteration : 10727
train acc:  0.828125
train loss:  0.3244096636772156
train gradient:  0.12924007732663734
iteration : 10728
train acc:  0.8984375
train loss:  0.28410857915878296
train gradient:  0.11982095223967015
iteration : 10729
train acc:  0.84375
train loss:  0.3765835165977478
train gradient:  0.18468934302554918
iteration : 10730
train acc:  0.875
train loss:  0.2979752719402313
train gradient:  0.17236699928109203
iteration : 10731
train acc:  0.859375
train loss:  0.31025275588035583
train gradient:  0.10949785755233225
iteration : 10732
train acc:  0.8671875
train loss:  0.3662967383861542
train gradient:  0.13187487669775486
iteration : 10733
train acc:  0.8828125
train loss:  0.3006952404975891
train gradient:  0.13055107127329335
iteration : 10734
train acc:  0.84375
train loss:  0.35329142212867737
train gradient:  0.25061245504898777
iteration : 10735
train acc:  0.8515625
train loss:  0.3818150758743286
train gradient:  0.198654423051903
iteration : 10736
train acc:  0.84375
train loss:  0.2748183012008667
train gradient:  0.12771146509769937
iteration : 10737
train acc:  0.8671875
train loss:  0.3075495660305023
train gradient:  0.15927118604340143
iteration : 10738
train acc:  0.8828125
train loss:  0.28496843576431274
train gradient:  0.12225853821655847
iteration : 10739
train acc:  0.8828125
train loss:  0.27014797925949097
train gradient:  0.11658520419522007
iteration : 10740
train acc:  0.828125
train loss:  0.3855246901512146
train gradient:  0.17395961505450425
iteration : 10741
train acc:  0.9296875
train loss:  0.21731382608413696
train gradient:  0.10500358863512889
iteration : 10742
train acc:  0.828125
train loss:  0.3492228090763092
train gradient:  0.22639282568842276
iteration : 10743
train acc:  0.890625
train loss:  0.3144402503967285
train gradient:  0.10614690008171886
iteration : 10744
train acc:  0.859375
train loss:  0.3398958444595337
train gradient:  0.15585202223692546
iteration : 10745
train acc:  0.84375
train loss:  0.38905447721481323
train gradient:  0.19951520801864625
iteration : 10746
train acc:  0.8203125
train loss:  0.36356866359710693
train gradient:  0.19522544055455876
iteration : 10747
train acc:  0.828125
train loss:  0.36950981616973877
train gradient:  0.15246144895528368
iteration : 10748
train acc:  0.8828125
train loss:  0.3155663311481476
train gradient:  0.18103117488299114
iteration : 10749
train acc:  0.8828125
train loss:  0.34030818939208984
train gradient:  0.19914444107564144
iteration : 10750
train acc:  0.890625
train loss:  0.27438101172447205
train gradient:  0.09024237228425692
iteration : 10751
train acc:  0.8515625
train loss:  0.33720213174819946
train gradient:  0.20153068959423748
iteration : 10752
train acc:  0.90625
train loss:  0.27289390563964844
train gradient:  0.5093831820633848
iteration : 10753
train acc:  0.8671875
train loss:  0.33196642994880676
train gradient:  0.20519220536695865
iteration : 10754
train acc:  0.8515625
train loss:  0.3356631100177765
train gradient:  0.14694348015455705
iteration : 10755
train acc:  0.8125
train loss:  0.3888751268386841
train gradient:  0.265057118612261
iteration : 10756
train acc:  0.8125
train loss:  0.41263478994369507
train gradient:  0.2187097208951258
iteration : 10757
train acc:  0.8828125
train loss:  0.25342634320259094
train gradient:  0.1384582248275649
iteration : 10758
train acc:  0.875
train loss:  0.30118072032928467
train gradient:  0.1649694191155459
iteration : 10759
train acc:  0.8359375
train loss:  0.32521575689315796
train gradient:  0.1484527284968684
iteration : 10760
train acc:  0.828125
train loss:  0.37063974142074585
train gradient:  0.2453235783879535
iteration : 10761
train acc:  0.9375
train loss:  0.20832279324531555
train gradient:  0.07373765120806072
iteration : 10762
train acc:  0.890625
train loss:  0.2902839779853821
train gradient:  0.2145072108025449
iteration : 10763
train acc:  0.890625
train loss:  0.280322790145874
train gradient:  0.15138252335372532
iteration : 10764
train acc:  0.859375
train loss:  0.32671594619750977
train gradient:  0.17459886612877196
iteration : 10765
train acc:  0.84375
train loss:  0.37910979986190796
train gradient:  0.18501324342503866
iteration : 10766
train acc:  0.828125
train loss:  0.3488142490386963
train gradient:  0.23043432456784108
iteration : 10767
train acc:  0.8515625
train loss:  0.3766474723815918
train gradient:  0.16390915294843267
iteration : 10768
train acc:  0.921875
train loss:  0.22193938493728638
train gradient:  0.07141176959802197
iteration : 10769
train acc:  0.8359375
train loss:  0.36916837096214294
train gradient:  0.16240316214199235
iteration : 10770
train acc:  0.8671875
train loss:  0.3306398391723633
train gradient:  0.10458678713544836
iteration : 10771
train acc:  0.828125
train loss:  0.37434324622154236
train gradient:  0.191883894585541
iteration : 10772
train acc:  0.8203125
train loss:  0.3635590374469757
train gradient:  0.18045502388331716
iteration : 10773
train acc:  0.8828125
train loss:  0.32142043113708496
train gradient:  0.32422093132916696
iteration : 10774
train acc:  0.8828125
train loss:  0.315041720867157
train gradient:  0.12014162717065263
iteration : 10775
train acc:  0.8671875
train loss:  0.2989339232444763
train gradient:  0.12154196968907621
iteration : 10776
train acc:  0.890625
train loss:  0.2899249196052551
train gradient:  0.14081230324918564
iteration : 10777
train acc:  0.859375
train loss:  0.26503270864486694
train gradient:  0.12133375580975779
iteration : 10778
train acc:  0.84375
train loss:  0.3663357198238373
train gradient:  0.14949758273488525
iteration : 10779
train acc:  0.859375
train loss:  0.30298206210136414
train gradient:  0.143766110684025
iteration : 10780
train acc:  0.84375
train loss:  0.3862670958042145
train gradient:  0.14569924055549718
iteration : 10781
train acc:  0.9375
train loss:  0.21386301517486572
train gradient:  0.13982566465660706
iteration : 10782
train acc:  0.8359375
train loss:  0.31080925464630127
train gradient:  0.11860960689547285
iteration : 10783
train acc:  0.8828125
train loss:  0.2639123201370239
train gradient:  0.19347610393458053
iteration : 10784
train acc:  0.9375
train loss:  0.20919296145439148
train gradient:  0.14819428623415476
iteration : 10785
train acc:  0.890625
train loss:  0.25770193338394165
train gradient:  0.0999972644762748
iteration : 10786
train acc:  0.875
train loss:  0.3264710307121277
train gradient:  0.1486285180305601
iteration : 10787
train acc:  0.859375
train loss:  0.3397430181503296
train gradient:  0.14170430047661375
iteration : 10788
train acc:  0.8828125
train loss:  0.2587359547615051
train gradient:  0.1022415733422482
iteration : 10789
train acc:  0.8671875
train loss:  0.28995954990386963
train gradient:  0.1349721509039215
iteration : 10790
train acc:  0.8203125
train loss:  0.33719730377197266
train gradient:  0.15083840546229443
iteration : 10791
train acc:  0.8515625
train loss:  0.3081502914428711
train gradient:  0.17388100625225847
iteration : 10792
train acc:  0.859375
train loss:  0.32140469551086426
train gradient:  0.1671250338718281
iteration : 10793
train acc:  0.78125
train loss:  0.5103088617324829
train gradient:  0.34830890601583736
iteration : 10794
train acc:  0.875
train loss:  0.35733917355537415
train gradient:  0.1495400864841106
iteration : 10795
train acc:  0.84375
train loss:  0.30287960171699524
train gradient:  0.11724591070509452
iteration : 10796
train acc:  0.921875
train loss:  0.2097909152507782
train gradient:  0.08089530436951632
iteration : 10797
train acc:  0.90625
train loss:  0.28087514638900757
train gradient:  0.13182499309722906
iteration : 10798
train acc:  0.8515625
train loss:  0.3240511119365692
train gradient:  0.26054319947671195
iteration : 10799
train acc:  0.875
train loss:  0.2821294665336609
train gradient:  0.17543593682396846
iteration : 10800
train acc:  0.8671875
train loss:  0.30884405970573425
train gradient:  0.10699407669179291
iteration : 10801
train acc:  0.828125
train loss:  0.34788113832473755
train gradient:  0.12762079019849282
iteration : 10802
train acc:  0.90625
train loss:  0.2475888431072235
train gradient:  0.16125218815915127
iteration : 10803
train acc:  0.8671875
train loss:  0.3183157742023468
train gradient:  0.16360765714960504
iteration : 10804
train acc:  0.8828125
train loss:  0.24666950106620789
train gradient:  0.1082944474307816
iteration : 10805
train acc:  0.8203125
train loss:  0.3380557596683502
train gradient:  0.198329587123629
iteration : 10806
train acc:  0.8671875
train loss:  0.34399691224098206
train gradient:  0.28349679026136854
iteration : 10807
train acc:  0.84375
train loss:  0.33117735385894775
train gradient:  0.14537789771776027
iteration : 10808
train acc:  0.90625
train loss:  0.2778964340686798
train gradient:  0.1920450434662994
iteration : 10809
train acc:  0.8828125
train loss:  0.29252487421035767
train gradient:  0.1456931117604801
iteration : 10810
train acc:  0.875
train loss:  0.3607097268104553
train gradient:  0.15665775080751093
iteration : 10811
train acc:  0.8046875
train loss:  0.389865905046463
train gradient:  0.2421458431098017
iteration : 10812
train acc:  0.8828125
train loss:  0.32732105255126953
train gradient:  0.22996284384554255
iteration : 10813
train acc:  0.828125
train loss:  0.40563952922821045
train gradient:  0.19964572647633438
iteration : 10814
train acc:  0.921875
train loss:  0.2239818572998047
train gradient:  0.1061691800386219
iteration : 10815
train acc:  0.8359375
train loss:  0.36617207527160645
train gradient:  0.1892015304662212
iteration : 10816
train acc:  0.921875
train loss:  0.27807530760765076
train gradient:  0.09747401503465918
iteration : 10817
train acc:  0.828125
train loss:  0.32465216517448425
train gradient:  0.18425971840004368
iteration : 10818
train acc:  0.9296875
train loss:  0.29374998807907104
train gradient:  0.16636788634608343
iteration : 10819
train acc:  0.8984375
train loss:  0.23882806301116943
train gradient:  0.08542631309175737
iteration : 10820
train acc:  0.84375
train loss:  0.37967193126678467
train gradient:  0.24934813163700856
iteration : 10821
train acc:  0.8515625
train loss:  0.35191822052001953
train gradient:  0.15514691017033694
iteration : 10822
train acc:  0.8203125
train loss:  0.3333289921283722
train gradient:  0.17993167582770553
iteration : 10823
train acc:  0.8671875
train loss:  0.3325120210647583
train gradient:  0.1449008601228431
iteration : 10824
train acc:  0.8515625
train loss:  0.3866989016532898
train gradient:  0.23360881810424997
iteration : 10825
train acc:  0.890625
train loss:  0.3636431396007538
train gradient:  0.15942082461720578
iteration : 10826
train acc:  0.84375
train loss:  0.32619190216064453
train gradient:  0.13300894641348182
iteration : 10827
train acc:  0.8359375
train loss:  0.32137370109558105
train gradient:  0.2108066859666986
iteration : 10828
train acc:  0.8359375
train loss:  0.35732540488243103
train gradient:  0.17383007234198072
iteration : 10829
train acc:  0.859375
train loss:  0.29854393005371094
train gradient:  0.16666441074528432
iteration : 10830
train acc:  0.859375
train loss:  0.32588374614715576
train gradient:  0.1613790177918909
iteration : 10831
train acc:  0.8828125
train loss:  0.26401540637016296
train gradient:  0.14253324007665208
iteration : 10832
train acc:  0.859375
train loss:  0.3645590543746948
train gradient:  0.1525146344131863
iteration : 10833
train acc:  0.8203125
train loss:  0.4114918112754822
train gradient:  0.2866175460687056
iteration : 10834
train acc:  0.8515625
train loss:  0.31145671010017395
train gradient:  0.10023720266642036
iteration : 10835
train acc:  0.890625
train loss:  0.3222982883453369
train gradient:  0.12763326113763718
iteration : 10836
train acc:  0.8828125
train loss:  0.2907654643058777
train gradient:  0.12812584654124265
iteration : 10837
train acc:  0.8984375
train loss:  0.29790160059928894
train gradient:  0.11966422072733982
iteration : 10838
train acc:  0.859375
train loss:  0.30022409558296204
train gradient:  0.14534698547643607
iteration : 10839
train acc:  0.8828125
train loss:  0.26975786685943604
train gradient:  0.09493844852512534
iteration : 10840
train acc:  0.84375
train loss:  0.3108275830745697
train gradient:  0.219602834242016
iteration : 10841
train acc:  0.8515625
train loss:  0.317205011844635
train gradient:  0.19551204025511953
iteration : 10842
train acc:  0.875
train loss:  0.28835710883140564
train gradient:  0.11942000935857448
iteration : 10843
train acc:  0.84375
train loss:  0.30251967906951904
train gradient:  0.11936284597375345
iteration : 10844
train acc:  0.8671875
train loss:  0.2882891595363617
train gradient:  0.10917666515503333
iteration : 10845
train acc:  0.9140625
train loss:  0.2997117042541504
train gradient:  0.09132792312259494
iteration : 10846
train acc:  0.8046875
train loss:  0.3485659956932068
train gradient:  0.15628445671960509
iteration : 10847
train acc:  0.921875
train loss:  0.2843007445335388
train gradient:  0.14268647084894978
iteration : 10848
train acc:  0.8203125
train loss:  0.36731773614883423
train gradient:  0.20931326675046324
iteration : 10849
train acc:  0.8515625
train loss:  0.2986767888069153
train gradient:  0.12188470294542891
iteration : 10850
train acc:  0.796875
train loss:  0.3706803321838379
train gradient:  0.17107908489947737
iteration : 10851
train acc:  0.8828125
train loss:  0.27946239709854126
train gradient:  0.13553915292051852
iteration : 10852
train acc:  0.859375
train loss:  0.275551974773407
train gradient:  0.1105203768767989
iteration : 10853
train acc:  0.8515625
train loss:  0.32428839802742004
train gradient:  0.15936063656580113
iteration : 10854
train acc:  0.8359375
train loss:  0.37140411138534546
train gradient:  0.20860526160951187
iteration : 10855
train acc:  0.8828125
train loss:  0.3289602994918823
train gradient:  0.295075473210186
iteration : 10856
train acc:  0.8359375
train loss:  0.3970904052257538
train gradient:  0.18091617492157247
iteration : 10857
train acc:  0.859375
train loss:  0.36744576692581177
train gradient:  0.26037984641808487
iteration : 10858
train acc:  0.8203125
train loss:  0.33391284942626953
train gradient:  0.23524920873544874
iteration : 10859
train acc:  0.8828125
train loss:  0.2455817013978958
train gradient:  0.09344468417837669
iteration : 10860
train acc:  0.828125
train loss:  0.4733850955963135
train gradient:  0.23169136965988762
iteration : 10861
train acc:  0.8984375
train loss:  0.2778845429420471
train gradient:  0.11273463942160102
iteration : 10862
train acc:  0.8671875
train loss:  0.3257015347480774
train gradient:  0.3477875977585201
iteration : 10863
train acc:  0.828125
train loss:  0.3125159740447998
train gradient:  0.14704735448571005
iteration : 10864
train acc:  0.8203125
train loss:  0.3636557459831238
train gradient:  0.1463997105602004
iteration : 10865
train acc:  0.8828125
train loss:  0.2837156653404236
train gradient:  0.13716080056334318
iteration : 10866
train acc:  0.8359375
train loss:  0.3465302586555481
train gradient:  0.17349039169445774
iteration : 10867
train acc:  0.8671875
train loss:  0.28128308057785034
train gradient:  0.10248507996439352
iteration : 10868
train acc:  0.8359375
train loss:  0.4310010075569153
train gradient:  0.21940596877139257
iteration : 10869
train acc:  0.859375
train loss:  0.33137398958206177
train gradient:  0.1643587167853474
iteration : 10870
train acc:  0.8125
train loss:  0.3196449875831604
train gradient:  0.2325705080586845
iteration : 10871
train acc:  0.84375
train loss:  0.38989973068237305
train gradient:  0.3090736903229204
iteration : 10872
train acc:  0.8046875
train loss:  0.370850533246994
train gradient:  0.20847560810565338
iteration : 10873
train acc:  0.8671875
train loss:  0.3358832597732544
train gradient:  0.15383142154362528
iteration : 10874
train acc:  0.859375
train loss:  0.37116754055023193
train gradient:  0.12611478532129258
iteration : 10875
train acc:  0.8828125
train loss:  0.2988986372947693
train gradient:  0.11790671863756833
iteration : 10876
train acc:  0.8359375
train loss:  0.32647472620010376
train gradient:  0.18047646066948952
iteration : 10877
train acc:  0.859375
train loss:  0.301906943321228
train gradient:  0.11113057645273815
iteration : 10878
train acc:  0.8671875
train loss:  0.3144116997718811
train gradient:  0.10701265783083919
iteration : 10879
train acc:  0.859375
train loss:  0.31837373971939087
train gradient:  0.1558482356028909
iteration : 10880
train acc:  0.8203125
train loss:  0.38866716623306274
train gradient:  0.17677569636279422
iteration : 10881
train acc:  0.890625
train loss:  0.3231703042984009
train gradient:  0.14019961787543062
iteration : 10882
train acc:  0.8125
train loss:  0.38237059116363525
train gradient:  0.17028980330149512
iteration : 10883
train acc:  0.7578125
train loss:  0.46832266449928284
train gradient:  0.31092348386602925
iteration : 10884
train acc:  0.8359375
train loss:  0.33642008900642395
train gradient:  0.15794074521031398
iteration : 10885
train acc:  0.875
train loss:  0.3089311718940735
train gradient:  0.14439517646617445
iteration : 10886
train acc:  0.828125
train loss:  0.3722071647644043
train gradient:  0.21449889436614525
iteration : 10887
train acc:  0.8359375
train loss:  0.3462564945220947
train gradient:  0.13975034240230064
iteration : 10888
train acc:  0.875
train loss:  0.2867317199707031
train gradient:  0.12298289985837584
iteration : 10889
train acc:  0.8515625
train loss:  0.34461310505867004
train gradient:  0.15349567577469142
iteration : 10890
train acc:  0.8828125
train loss:  0.30753836035728455
train gradient:  0.18238328281317562
iteration : 10891
train acc:  0.8828125
train loss:  0.3198024034500122
train gradient:  0.15876631383711037
iteration : 10892
train acc:  0.84375
train loss:  0.34551727771759033
train gradient:  0.16799281503212082
iteration : 10893
train acc:  0.8515625
train loss:  0.3447113335132599
train gradient:  0.1515757463516435
iteration : 10894
train acc:  0.8828125
train loss:  0.3060470223426819
train gradient:  0.12120433640423542
iteration : 10895
train acc:  0.828125
train loss:  0.38222187757492065
train gradient:  0.287683163609804
iteration : 10896
train acc:  0.859375
train loss:  0.3812640607357025
train gradient:  0.16574159106562303
iteration : 10897
train acc:  0.8046875
train loss:  0.39551210403442383
train gradient:  0.16104047861803175
iteration : 10898
train acc:  0.8359375
train loss:  0.37336060404777527
train gradient:  0.15477281436265872
iteration : 10899
train acc:  0.8671875
train loss:  0.32595157623291016
train gradient:  0.11464705721321615
iteration : 10900
train acc:  0.8515625
train loss:  0.3274926245212555
train gradient:  0.1782119648787492
iteration : 10901
train acc:  0.8515625
train loss:  0.31491410732269287
train gradient:  0.14799901803914026
iteration : 10902
train acc:  0.921875
train loss:  0.23201268911361694
train gradient:  0.08979294930786219
iteration : 10903
train acc:  0.8671875
train loss:  0.36452072858810425
train gradient:  0.24734118883035347
iteration : 10904
train acc:  0.8671875
train loss:  0.3514331877231598
train gradient:  0.16796286793943224
iteration : 10905
train acc:  0.8671875
train loss:  0.33254164457321167
train gradient:  0.1480947275247501
iteration : 10906
train acc:  0.8515625
train loss:  0.31645283102989197
train gradient:  0.2408736141476353
iteration : 10907
train acc:  0.8359375
train loss:  0.36264270544052124
train gradient:  0.2140123987242793
iteration : 10908
train acc:  0.7890625
train loss:  0.42583030462265015
train gradient:  0.24624469057685425
iteration : 10909
train acc:  0.8828125
train loss:  0.30077657103538513
train gradient:  0.09787607470955635
iteration : 10910
train acc:  0.8828125
train loss:  0.2731468677520752
train gradient:  0.07823733594606813
iteration : 10911
train acc:  0.8359375
train loss:  0.32664039731025696
train gradient:  0.13573903786517832
iteration : 10912
train acc:  0.8203125
train loss:  0.4002489149570465
train gradient:  0.20527512203279052
iteration : 10913
train acc:  0.828125
train loss:  0.34783488512039185
train gradient:  0.2268223462467467
iteration : 10914
train acc:  0.90625
train loss:  0.2443419098854065
train gradient:  0.09413421282319173
iteration : 10915
train acc:  0.890625
train loss:  0.33618178963661194
train gradient:  0.18381639901624225
iteration : 10916
train acc:  0.875
train loss:  0.3367263674736023
train gradient:  0.17707277441882294
iteration : 10917
train acc:  0.8984375
train loss:  0.2901848554611206
train gradient:  0.11976800021519501
iteration : 10918
train acc:  0.8828125
train loss:  0.3094468116760254
train gradient:  0.11870001673478883
iteration : 10919
train acc:  0.8671875
train loss:  0.2928263545036316
train gradient:  0.1742780356527771
iteration : 10920
train acc:  0.890625
train loss:  0.2674610912799835
train gradient:  0.08715979352956163
iteration : 10921
train acc:  0.8515625
train loss:  0.3295421600341797
train gradient:  0.33141232667474485
iteration : 10922
train acc:  0.84375
train loss:  0.38098418712615967
train gradient:  0.243869708493915
iteration : 10923
train acc:  0.890625
train loss:  0.3162984848022461
train gradient:  0.15233101058639958
iteration : 10924
train acc:  0.8125
train loss:  0.3694044351577759
train gradient:  0.2523114871292301
iteration : 10925
train acc:  0.84375
train loss:  0.322501003742218
train gradient:  0.12217851812947296
iteration : 10926
train acc:  0.84375
train loss:  0.34548863768577576
train gradient:  0.13510344128843244
iteration : 10927
train acc:  0.8359375
train loss:  0.3887483477592468
train gradient:  0.22081466561165852
iteration : 10928
train acc:  0.8671875
train loss:  0.2735669016838074
train gradient:  0.14885209599049445
iteration : 10929
train acc:  0.90625
train loss:  0.25075510144233704
train gradient:  0.11784524871611583
iteration : 10930
train acc:  0.859375
train loss:  0.26853716373443604
train gradient:  0.12000782884042861
iteration : 10931
train acc:  0.8828125
train loss:  0.2929224371910095
train gradient:  0.11539333297352114
iteration : 10932
train acc:  0.8125
train loss:  0.46322089433670044
train gradient:  0.2509985093030496
iteration : 10933
train acc:  0.828125
train loss:  0.3665775656700134
train gradient:  0.24867445737242694
iteration : 10934
train acc:  0.8671875
train loss:  0.31126049160957336
train gradient:  0.10727965308819563
iteration : 10935
train acc:  0.8671875
train loss:  0.305696576833725
train gradient:  0.15492755211844397
iteration : 10936
train acc:  0.8359375
train loss:  0.35787004232406616
train gradient:  0.14770525742497492
iteration : 10937
train acc:  0.8671875
train loss:  0.3094351291656494
train gradient:  0.12883092304198573
iteration : 10938
train acc:  0.890625
train loss:  0.28192293643951416
train gradient:  0.0968763426273194
iteration : 10939
train acc:  0.8984375
train loss:  0.2505578398704529
train gradient:  0.126222588754734
iteration : 10940
train acc:  0.8515625
train loss:  0.2969188988208771
train gradient:  0.12986231705501472
iteration : 10941
train acc:  0.8671875
train loss:  0.3512817323207855
train gradient:  0.14500942707741415
iteration : 10942
train acc:  0.8828125
train loss:  0.2382258176803589
train gradient:  0.083221051764302
iteration : 10943
train acc:  0.8359375
train loss:  0.36520108580589294
train gradient:  0.16266902455637575
iteration : 10944
train acc:  0.8515625
train loss:  0.3290243446826935
train gradient:  0.19673762688860608
iteration : 10945
train acc:  0.859375
train loss:  0.37300097942352295
train gradient:  0.20554060104846653
iteration : 10946
train acc:  0.8203125
train loss:  0.3414462208747864
train gradient:  0.18244846416252486
iteration : 10947
train acc:  0.8515625
train loss:  0.3758719563484192
train gradient:  0.1593124895148823
iteration : 10948
train acc:  0.890625
train loss:  0.30933964252471924
train gradient:  0.15956690342320098
iteration : 10949
train acc:  0.875
train loss:  0.2919341027736664
train gradient:  0.13085263002989334
iteration : 10950
train acc:  0.875
train loss:  0.347647100687027
train gradient:  0.1498684660847296
iteration : 10951
train acc:  0.875
train loss:  0.27379751205444336
train gradient:  0.15185704858249333
iteration : 10952
train acc:  0.8828125
train loss:  0.30017930269241333
train gradient:  0.11475757787470041
iteration : 10953
train acc:  0.828125
train loss:  0.35301339626312256
train gradient:  0.2530878107658448
iteration : 10954
train acc:  0.859375
train loss:  0.3215607702732086
train gradient:  0.15893918857022982
iteration : 10955
train acc:  0.8984375
train loss:  0.3063938021659851
train gradient:  0.1475714495589942
iteration : 10956
train acc:  0.828125
train loss:  0.397830069065094
train gradient:  0.2778902458857235
iteration : 10957
train acc:  0.8828125
train loss:  0.30386650562286377
train gradient:  0.1314297697587501
iteration : 10958
train acc:  0.8828125
train loss:  0.3179727792739868
train gradient:  0.13779379638307176
iteration : 10959
train acc:  0.8671875
train loss:  0.31566327810287476
train gradient:  0.14869562330321254
iteration : 10960
train acc:  0.8046875
train loss:  0.3830931484699249
train gradient:  0.2189773999290457
iteration : 10961
train acc:  0.8515625
train loss:  0.36436694860458374
train gradient:  0.12916251209127744
iteration : 10962
train acc:  0.875
train loss:  0.3577113151550293
train gradient:  0.1322677477602634
iteration : 10963
train acc:  0.8359375
train loss:  0.29836714267730713
train gradient:  0.15964198118334408
iteration : 10964
train acc:  0.8359375
train loss:  0.38996046781539917
train gradient:  0.18478309627378312
iteration : 10965
train acc:  0.8046875
train loss:  0.35586538910865784
train gradient:  0.23979229116863787
iteration : 10966
train acc:  0.8515625
train loss:  0.3519798815250397
train gradient:  0.14659816490671448
iteration : 10967
train acc:  0.8984375
train loss:  0.3125454783439636
train gradient:  0.17441815293505858
iteration : 10968
train acc:  0.8828125
train loss:  0.3158312439918518
train gradient:  0.15124497952937982
iteration : 10969
train acc:  0.859375
train loss:  0.28379786014556885
train gradient:  0.1321693963140126
iteration : 10970
train acc:  0.84375
train loss:  0.314872145652771
train gradient:  0.16796858096612205
iteration : 10971
train acc:  0.8359375
train loss:  0.4270645081996918
train gradient:  0.2895633997973519
iteration : 10972
train acc:  0.8359375
train loss:  0.4117298126220703
train gradient:  0.2242916476417697
iteration : 10973
train acc:  0.84375
train loss:  0.3996272683143616
train gradient:  0.13662688838011272
iteration : 10974
train acc:  0.8984375
train loss:  0.2641575038433075
train gradient:  0.09603684119943741
iteration : 10975
train acc:  0.8828125
train loss:  0.2438041865825653
train gradient:  0.08414831071509883
iteration : 10976
train acc:  0.890625
train loss:  0.2592812478542328
train gradient:  0.17382515907247448
iteration : 10977
train acc:  0.875
train loss:  0.37211692333221436
train gradient:  0.1484376633283493
iteration : 10978
train acc:  0.84375
train loss:  0.3340916633605957
train gradient:  0.17466996651845249
iteration : 10979
train acc:  0.8359375
train loss:  0.364515483379364
train gradient:  0.15178270258666804
iteration : 10980
train acc:  0.8515625
train loss:  0.32892680168151855
train gradient:  0.11426461018387692
iteration : 10981
train acc:  0.8828125
train loss:  0.2954423129558563
train gradient:  0.10889103755042358
iteration : 10982
train acc:  0.8359375
train loss:  0.34029099345207214
train gradient:  0.1399052142587101
iteration : 10983
train acc:  0.8203125
train loss:  0.3833911418914795
train gradient:  0.17002616897727268
iteration : 10984
train acc:  0.8515625
train loss:  0.32247817516326904
train gradient:  0.19974674013723032
iteration : 10985
train acc:  0.890625
train loss:  0.2944830358028412
train gradient:  0.12617204854786482
iteration : 10986
train acc:  0.796875
train loss:  0.44150447845458984
train gradient:  0.19753948468310556
iteration : 10987
train acc:  0.875
train loss:  0.34806740283966064
train gradient:  0.15843020342382347
iteration : 10988
train acc:  0.8515625
train loss:  0.34541594982147217
train gradient:  0.16352205527297206
iteration : 10989
train acc:  0.8515625
train loss:  0.3156932592391968
train gradient:  0.12323787267270611
iteration : 10990
train acc:  0.890625
train loss:  0.29168009757995605
train gradient:  0.10350020108833641
iteration : 10991
train acc:  0.8671875
train loss:  0.34302276372909546
train gradient:  0.17795163740197883
iteration : 10992
train acc:  0.84375
train loss:  0.3168768882751465
train gradient:  0.14711198913760778
iteration : 10993
train acc:  0.8828125
train loss:  0.30621200799942017
train gradient:  0.10703956291359808
iteration : 10994
train acc:  0.890625
train loss:  0.27190297842025757
train gradient:  0.11644680197613018
iteration : 10995
train acc:  0.8828125
train loss:  0.28498417139053345
train gradient:  0.10488222843483386
iteration : 10996
train acc:  0.84375
train loss:  0.37641623616218567
train gradient:  0.18654929250219834
iteration : 10997
train acc:  0.8046875
train loss:  0.38677358627319336
train gradient:  0.21363947475040612
iteration : 10998
train acc:  0.84375
train loss:  0.40051692724227905
train gradient:  0.22114190687243318
iteration : 10999
train acc:  0.859375
train loss:  0.33594372868537903
train gradient:  0.19744027888234877
iteration : 11000
train acc:  0.84375
train loss:  0.3338223695755005
train gradient:  0.1360068194232465
iteration : 11001
train acc:  0.8515625
train loss:  0.31472986936569214
train gradient:  0.19871979328309133
iteration : 11002
train acc:  0.8671875
train loss:  0.29738131165504456
train gradient:  0.16145375276085155
iteration : 11003
train acc:  0.8671875
train loss:  0.3088149428367615
train gradient:  0.16264326090078368
iteration : 11004
train acc:  0.7578125
train loss:  0.40035706758499146
train gradient:  0.17767759025644977
iteration : 11005
train acc:  0.859375
train loss:  0.34418320655822754
train gradient:  0.14881501568236868
iteration : 11006
train acc:  0.8359375
train loss:  0.32419925928115845
train gradient:  0.14397821393549054
iteration : 11007
train acc:  0.890625
train loss:  0.23867088556289673
train gradient:  0.1321122127621317
iteration : 11008
train acc:  0.8515625
train loss:  0.3312256932258606
train gradient:  0.14034592727761552
iteration : 11009
train acc:  0.875
train loss:  0.28402379155158997
train gradient:  0.14149609283261777
iteration : 11010
train acc:  0.90625
train loss:  0.2422851026058197
train gradient:  0.10278690569708458
iteration : 11011
train acc:  0.875
train loss:  0.30402809381484985
train gradient:  0.12027904272636598
iteration : 11012
train acc:  0.90625
train loss:  0.30743715167045593
train gradient:  0.13134222905679793
iteration : 11013
train acc:  0.8828125
train loss:  0.2877994775772095
train gradient:  0.1251659673699148
iteration : 11014
train acc:  0.8515625
train loss:  0.33239686489105225
train gradient:  0.19900043998301198
iteration : 11015
train acc:  0.8125
train loss:  0.3892386555671692
train gradient:  0.1786683317601993
iteration : 11016
train acc:  0.8359375
train loss:  0.3351607918739319
train gradient:  0.21690698064978065
iteration : 11017
train acc:  0.8671875
train loss:  0.31915560364723206
train gradient:  0.14625641620718347
iteration : 11018
train acc:  0.9140625
train loss:  0.2696545720100403
train gradient:  0.11660452748437433
iteration : 11019
train acc:  0.875
train loss:  0.28474509716033936
train gradient:  0.12082813499221394
iteration : 11020
train acc:  0.78125
train loss:  0.3808797001838684
train gradient:  0.20029600938445874
iteration : 11021
train acc:  0.8828125
train loss:  0.2956593632698059
train gradient:  0.13708492249330956
iteration : 11022
train acc:  0.875
train loss:  0.2851165533065796
train gradient:  0.11801808255678148
iteration : 11023
train acc:  0.8125
train loss:  0.3792754113674164
train gradient:  0.23708557045608877
iteration : 11024
train acc:  0.84375
train loss:  0.29528671503067017
train gradient:  0.13286743665301187
iteration : 11025
train acc:  0.8515625
train loss:  0.3121544122695923
train gradient:  0.12426768334178603
iteration : 11026
train acc:  0.84375
train loss:  0.34192368388175964
train gradient:  0.1836205979389578
iteration : 11027
train acc:  0.8828125
train loss:  0.2791503667831421
train gradient:  0.14903754673831385
iteration : 11028
train acc:  0.8984375
train loss:  0.2704522907733917
train gradient:  0.09629671323229122
iteration : 11029
train acc:  0.8125
train loss:  0.4737502634525299
train gradient:  0.2991694328842296
iteration : 11030
train acc:  0.859375
train loss:  0.3657988905906677
train gradient:  0.22230175475247338
iteration : 11031
train acc:  0.84375
train loss:  0.36555561423301697
train gradient:  0.13840096801692553
iteration : 11032
train acc:  0.90625
train loss:  0.26493555307388306
train gradient:  0.13934401262672116
iteration : 11033
train acc:  0.8359375
train loss:  0.3275790214538574
train gradient:  0.1818214869209306
iteration : 11034
train acc:  0.8515625
train loss:  0.29801255464553833
train gradient:  0.12762133146893728
iteration : 11035
train acc:  0.8515625
train loss:  0.3194652497768402
train gradient:  0.12547376611106575
iteration : 11036
train acc:  0.84375
train loss:  0.3160669803619385
train gradient:  0.13720861406228993
iteration : 11037
train acc:  0.875
train loss:  0.2775636613368988
train gradient:  0.12072581312846965
iteration : 11038
train acc:  0.84375
train loss:  0.349457323551178
train gradient:  0.22113815106956994
iteration : 11039
train acc:  0.8828125
train loss:  0.2657376825809479
train gradient:  0.08539288228912385
iteration : 11040
train acc:  0.875
train loss:  0.25532156229019165
train gradient:  0.0875183002321475
iteration : 11041
train acc:  0.8515625
train loss:  0.31983545422554016
train gradient:  0.12110662217626417
iteration : 11042
train acc:  0.859375
train loss:  0.2991304397583008
train gradient:  0.11178632619531438
iteration : 11043
train acc:  0.921875
train loss:  0.22236301004886627
train gradient:  0.12298040605055233
iteration : 11044
train acc:  0.8671875
train loss:  0.2966027557849884
train gradient:  0.1386337414706686
iteration : 11045
train acc:  0.890625
train loss:  0.3073270618915558
train gradient:  0.1398887569323171
iteration : 11046
train acc:  0.859375
train loss:  0.402604877948761
train gradient:  0.23927582690523938
iteration : 11047
train acc:  0.8828125
train loss:  0.35072049498558044
train gradient:  0.13335888319623784
iteration : 11048
train acc:  0.859375
train loss:  0.32410430908203125
train gradient:  0.18509420481323044
iteration : 11049
train acc:  0.9453125
train loss:  0.21782851219177246
train gradient:  0.07011315199536763
iteration : 11050
train acc:  0.890625
train loss:  0.2711000144481659
train gradient:  0.12938301410086844
iteration : 11051
train acc:  0.8125
train loss:  0.40199682116508484
train gradient:  0.21275573952850282
iteration : 11052
train acc:  0.90625
train loss:  0.24695879220962524
train gradient:  0.08271406939921154
iteration : 11053
train acc:  0.890625
train loss:  0.2238607555627823
train gradient:  0.09581696923500166
iteration : 11054
train acc:  0.8828125
train loss:  0.29826366901397705
train gradient:  0.10257669290700273
iteration : 11055
train acc:  0.90625
train loss:  0.3519517779350281
train gradient:  0.1721202288502553
iteration : 11056
train acc:  0.8203125
train loss:  0.3672695755958557
train gradient:  0.20838888363286207
iteration : 11057
train acc:  0.859375
train loss:  0.29562121629714966
train gradient:  0.11953927273195751
iteration : 11058
train acc:  0.8515625
train loss:  0.3357751965522766
train gradient:  0.20937103807719015
iteration : 11059
train acc:  0.90625
train loss:  0.3389339745044708
train gradient:  0.18255644369369836
iteration : 11060
train acc:  0.8671875
train loss:  0.3305342197418213
train gradient:  0.16715809635013226
iteration : 11061
train acc:  0.8359375
train loss:  0.4301000237464905
train gradient:  0.17516842066314667
iteration : 11062
train acc:  0.859375
train loss:  0.3210715055465698
train gradient:  0.1305352219329235
iteration : 11063
train acc:  0.8671875
train loss:  0.33963197469711304
train gradient:  0.16699344003464733
iteration : 11064
train acc:  0.875
train loss:  0.26324111223220825
train gradient:  0.08244085826260839
iteration : 11065
train acc:  0.8359375
train loss:  0.37842756509780884
train gradient:  0.2451145759591125
iteration : 11066
train acc:  0.890625
train loss:  0.2968900799751282
train gradient:  0.13945741004243492
iteration : 11067
train acc:  0.8828125
train loss:  0.24047167599201202
train gradient:  0.1215476802838406
iteration : 11068
train acc:  0.8828125
train loss:  0.28161436319351196
train gradient:  0.11509251705991365
iteration : 11069
train acc:  0.8515625
train loss:  0.3051401376724243
train gradient:  0.08741249345848577
iteration : 11070
train acc:  0.8984375
train loss:  0.2549779415130615
train gradient:  0.11007616953511888
iteration : 11071
train acc:  0.8203125
train loss:  0.3461840748786926
train gradient:  0.14881895406465107
iteration : 11072
train acc:  0.90625
train loss:  0.2261403501033783
train gradient:  0.11241472112390014
iteration : 11073
train acc:  0.8671875
train loss:  0.360736608505249
train gradient:  0.19785589932599246
iteration : 11074
train acc:  0.8515625
train loss:  0.32847487926483154
train gradient:  0.16669558016948316
iteration : 11075
train acc:  0.84375
train loss:  0.3462352752685547
train gradient:  0.15575560851116582
iteration : 11076
train acc:  0.890625
train loss:  0.3204181492328644
train gradient:  0.16049777190057224
iteration : 11077
train acc:  0.8515625
train loss:  0.35697707533836365
train gradient:  0.23898403938235774
iteration : 11078
train acc:  0.828125
train loss:  0.38159453868865967
train gradient:  0.2406326952113476
iteration : 11079
train acc:  0.8515625
train loss:  0.3679506480693817
train gradient:  0.21571645418842572
iteration : 11080
train acc:  0.8203125
train loss:  0.3974703550338745
train gradient:  0.307421462777072
iteration : 11081
train acc:  0.890625
train loss:  0.27589595317840576
train gradient:  0.13569358287587163
iteration : 11082
train acc:  0.84375
train loss:  0.3330617845058441
train gradient:  0.12505327262487637
iteration : 11083
train acc:  0.8671875
train loss:  0.2877962291240692
train gradient:  0.13937139520096906
iteration : 11084
train acc:  0.859375
train loss:  0.3434205651283264
train gradient:  0.21919923925196816
iteration : 11085
train acc:  0.828125
train loss:  0.368149995803833
train gradient:  0.2066097929616345
iteration : 11086
train acc:  0.8671875
train loss:  0.32503944635391235
train gradient:  0.13565775093319424
iteration : 11087
train acc:  0.8984375
train loss:  0.28992411494255066
train gradient:  0.1896249610876839
iteration : 11088
train acc:  0.921875
train loss:  0.24935516715049744
train gradient:  0.09188194416056925
iteration : 11089
train acc:  0.8359375
train loss:  0.4016580283641815
train gradient:  0.38946334237835845
iteration : 11090
train acc:  0.8046875
train loss:  0.42506811022758484
train gradient:  0.24350187544287014
iteration : 11091
train acc:  0.8828125
train loss:  0.27912700176239014
train gradient:  0.11886035010247302
iteration : 11092
train acc:  0.8671875
train loss:  0.3489971160888672
train gradient:  0.17460124207494732
iteration : 11093
train acc:  0.875
train loss:  0.3323338031768799
train gradient:  0.17182670556561144
iteration : 11094
train acc:  0.8515625
train loss:  0.32536613941192627
train gradient:  0.15110172194295712
iteration : 11095
train acc:  0.8984375
train loss:  0.2918827533721924
train gradient:  0.16632706708579692
iteration : 11096
train acc:  0.890625
train loss:  0.28981855511665344
train gradient:  0.12295202979594784
iteration : 11097
train acc:  0.828125
train loss:  0.3746209144592285
train gradient:  0.2100131782368715
iteration : 11098
train acc:  0.875
train loss:  0.3083055019378662
train gradient:  0.13437701427434207
iteration : 11099
train acc:  0.7890625
train loss:  0.41796979308128357
train gradient:  0.23746652721820533
iteration : 11100
train acc:  0.90625
train loss:  0.27440205216407776
train gradient:  0.1598345642783867
iteration : 11101
train acc:  0.8515625
train loss:  0.3480389714241028
train gradient:  0.18213421987096112
iteration : 11102
train acc:  0.8359375
train loss:  0.41465866565704346
train gradient:  0.2287005628310353
iteration : 11103
train acc:  0.8828125
train loss:  0.33969205617904663
train gradient:  0.25249369907148567
iteration : 11104
train acc:  0.9140625
train loss:  0.24154409766197205
train gradient:  0.11046679281445902
iteration : 11105
train acc:  0.875
train loss:  0.3272305727005005
train gradient:  0.1263781047113619
iteration : 11106
train acc:  0.8515625
train loss:  0.35133567452430725
train gradient:  0.14211924006504684
iteration : 11107
train acc:  0.828125
train loss:  0.37526148557662964
train gradient:  0.23768714480742203
iteration : 11108
train acc:  0.84375
train loss:  0.30116793513298035
train gradient:  0.1649735467065742
iteration : 11109
train acc:  0.921875
train loss:  0.20871621370315552
train gradient:  0.12181906849112048
iteration : 11110
train acc:  0.859375
train loss:  0.34689074754714966
train gradient:  0.15386071161754408
iteration : 11111
train acc:  0.859375
train loss:  0.2943706512451172
train gradient:  0.11631138993272601
iteration : 11112
train acc:  0.8828125
train loss:  0.28783002495765686
train gradient:  0.10689683955907738
iteration : 11113
train acc:  0.84375
train loss:  0.32654422521591187
train gradient:  0.16577206245337622
iteration : 11114
train acc:  0.8125
train loss:  0.3773655593395233
train gradient:  0.2274004147861081
iteration : 11115
train acc:  0.84375
train loss:  0.4023616313934326
train gradient:  0.22714044952038098
iteration : 11116
train acc:  0.8671875
train loss:  0.27756455540657043
train gradient:  0.12155795159099898
iteration : 11117
train acc:  0.8515625
train loss:  0.35910937190055847
train gradient:  0.21190501423746877
iteration : 11118
train acc:  0.875
train loss:  0.3337247371673584
train gradient:  0.18239783368518356
iteration : 11119
train acc:  0.8828125
train loss:  0.2973930239677429
train gradient:  0.14706339836956053
iteration : 11120
train acc:  0.84375
train loss:  0.3649526834487915
train gradient:  0.18327318331222758
iteration : 11121
train acc:  0.8671875
train loss:  0.31108343601226807
train gradient:  0.19690328851609168
iteration : 11122
train acc:  0.859375
train loss:  0.3337443768978119
train gradient:  0.19744970476004836
iteration : 11123
train acc:  0.875
train loss:  0.2839880883693695
train gradient:  0.1302011433665071
iteration : 11124
train acc:  0.890625
train loss:  0.27536025643348694
train gradient:  0.17288264394958888
iteration : 11125
train acc:  0.8359375
train loss:  0.3887539505958557
train gradient:  0.14698708429927637
iteration : 11126
train acc:  0.9140625
train loss:  0.24195227026939392
train gradient:  0.12986181285783238
iteration : 11127
train acc:  0.84375
train loss:  0.32191646099090576
train gradient:  0.18024340838918507
iteration : 11128
train acc:  0.9140625
train loss:  0.21170446276664734
train gradient:  0.07378400703067778
iteration : 11129
train acc:  0.890625
train loss:  0.3108983039855957
train gradient:  0.1527184070877019
iteration : 11130
train acc:  0.8125
train loss:  0.3659409284591675
train gradient:  0.21432478222767581
iteration : 11131
train acc:  0.890625
train loss:  0.29303497076034546
train gradient:  0.21310338884254962
iteration : 11132
train acc:  0.890625
train loss:  0.2598905563354492
train gradient:  0.11072412545078467
iteration : 11133
train acc:  0.90625
train loss:  0.24016769230365753
train gradient:  0.11450339332571591
iteration : 11134
train acc:  0.921875
train loss:  0.2648523449897766
train gradient:  0.1419362751595027
iteration : 11135
train acc:  0.859375
train loss:  0.30205485224723816
train gradient:  0.20330800160300505
iteration : 11136
train acc:  0.8828125
train loss:  0.24477584660053253
train gradient:  0.09616421439475838
iteration : 11137
train acc:  0.8515625
train loss:  0.34486186504364014
train gradient:  0.19684110011204198
iteration : 11138
train acc:  0.875
train loss:  0.3060302734375
train gradient:  0.10603005468976029
iteration : 11139
train acc:  0.8671875
train loss:  0.301389217376709
train gradient:  0.15411604372411927
iteration : 11140
train acc:  0.8515625
train loss:  0.35010844469070435
train gradient:  0.1706894185276822
iteration : 11141
train acc:  0.8359375
train loss:  0.36716270446777344
train gradient:  0.24112134483867295
iteration : 11142
train acc:  0.84375
train loss:  0.33859533071517944
train gradient:  0.16410987530890833
iteration : 11143
train acc:  0.9375
train loss:  0.28645700216293335
train gradient:  0.16419238630017663
iteration : 11144
train acc:  0.8515625
train loss:  0.33468008041381836
train gradient:  0.18170389435615766
iteration : 11145
train acc:  0.8671875
train loss:  0.3314393162727356
train gradient:  0.13575567266774394
iteration : 11146
train acc:  0.8828125
train loss:  0.26207268238067627
train gradient:  0.15616052714358497
iteration : 11147
train acc:  0.8359375
train loss:  0.36702513694763184
train gradient:  0.2282179707879625
iteration : 11148
train acc:  0.8671875
train loss:  0.35990649461746216
train gradient:  0.20557846470788294
iteration : 11149
train acc:  0.859375
train loss:  0.27107787132263184
train gradient:  0.09482129897892538
iteration : 11150
train acc:  0.8828125
train loss:  0.28067758679389954
train gradient:  0.10601634623113464
iteration : 11151
train acc:  0.8515625
train loss:  0.35683566331863403
train gradient:  0.21824255856517458
iteration : 11152
train acc:  0.8359375
train loss:  0.3266880512237549
train gradient:  0.15869577492808173
iteration : 11153
train acc:  0.890625
train loss:  0.3151566684246063
train gradient:  0.15355961169723936
iteration : 11154
train acc:  0.8359375
train loss:  0.299136221408844
train gradient:  0.1400654371471845
iteration : 11155
train acc:  0.8671875
train loss:  0.2962920069694519
train gradient:  0.1129462014223756
iteration : 11156
train acc:  0.8984375
train loss:  0.23445089161396027
train gradient:  0.09010957937155711
iteration : 11157
train acc:  0.9140625
train loss:  0.2331894040107727
train gradient:  0.12987501976415106
iteration : 11158
train acc:  0.8359375
train loss:  0.37250784039497375
train gradient:  0.20570787001962432
iteration : 11159
train acc:  0.890625
train loss:  0.2135816514492035
train gradient:  0.08430396595563155
iteration : 11160
train acc:  0.84375
train loss:  0.3504490554332733
train gradient:  0.17384178911532383
iteration : 11161
train acc:  0.8515625
train loss:  0.32215821743011475
train gradient:  0.15502898346417046
iteration : 11162
train acc:  0.828125
train loss:  0.304254412651062
train gradient:  0.16348436272404415
iteration : 11163
train acc:  0.875
train loss:  0.32665523886680603
train gradient:  0.2088457951688634
iteration : 11164
train acc:  0.9453125
train loss:  0.20211100578308105
train gradient:  0.13111321237444623
iteration : 11165
train acc:  0.84375
train loss:  0.3877125084400177
train gradient:  0.15450215772621767
iteration : 11166
train acc:  0.8359375
train loss:  0.4351012706756592
train gradient:  0.2424457359280584
iteration : 11167
train acc:  0.84375
train loss:  0.3697258532047272
train gradient:  0.21527414278812634
iteration : 11168
train acc:  0.7890625
train loss:  0.5093992948532104
train gradient:  0.39150857748135004
iteration : 11169
train acc:  0.8515625
train loss:  0.4769142270088196
train gradient:  0.3451381003484667
iteration : 11170
train acc:  0.84375
train loss:  0.3523435592651367
train gradient:  0.22077934866584437
iteration : 11171
train acc:  0.8515625
train loss:  0.3398677110671997
train gradient:  0.18204651591822396
iteration : 11172
train acc:  0.84375
train loss:  0.3227405250072479
train gradient:  0.12903734540693307
iteration : 11173
train acc:  0.8671875
train loss:  0.3119186758995056
train gradient:  0.13153120714101246
iteration : 11174
train acc:  0.8125
train loss:  0.3761084973812103
train gradient:  0.18568797190832365
iteration : 11175
train acc:  0.9140625
train loss:  0.3090980648994446
train gradient:  0.1575919987025598
iteration : 11176
train acc:  0.8359375
train loss:  0.37247568368911743
train gradient:  0.21052981641070403
iteration : 11177
train acc:  0.859375
train loss:  0.3418678045272827
train gradient:  0.1567801640801103
iteration : 11178
train acc:  0.828125
train loss:  0.3400838375091553
train gradient:  0.18474917232108914
iteration : 11179
train acc:  0.8359375
train loss:  0.34868478775024414
train gradient:  0.13121477080678673
iteration : 11180
train acc:  0.8671875
train loss:  0.3187066316604614
train gradient:  0.13967534013394045
iteration : 11181
train acc:  0.84375
train loss:  0.36654648184776306
train gradient:  0.2887834111798375
iteration : 11182
train acc:  0.8828125
train loss:  0.3461553454399109
train gradient:  0.1532617820022494
iteration : 11183
train acc:  0.8828125
train loss:  0.2823335528373718
train gradient:  0.10716285900806807
iteration : 11184
train acc:  0.796875
train loss:  0.3720785975456238
train gradient:  0.16690933138054265
iteration : 11185
train acc:  0.8671875
train loss:  0.39233291149139404
train gradient:  0.2194806552867085
iteration : 11186
train acc:  0.8046875
train loss:  0.37753066420555115
train gradient:  0.20433056763616608
iteration : 11187
train acc:  0.8515625
train loss:  0.3130556344985962
train gradient:  0.15868014196234168
iteration : 11188
train acc:  0.953125
train loss:  0.20160654187202454
train gradient:  0.08257349474303624
iteration : 11189
train acc:  0.828125
train loss:  0.37215083837509155
train gradient:  0.16641583916938874
iteration : 11190
train acc:  0.84375
train loss:  0.36777418851852417
train gradient:  0.1675266776313649
iteration : 11191
train acc:  0.8359375
train loss:  0.4059980809688568
train gradient:  0.20240308773469096
iteration : 11192
train acc:  0.84375
train loss:  0.30046331882476807
train gradient:  0.11644039977166568
iteration : 11193
train acc:  0.828125
train loss:  0.355431467294693
train gradient:  0.1834561989230774
iteration : 11194
train acc:  0.859375
train loss:  0.301993191242218
train gradient:  0.13909222366740592
iteration : 11195
train acc:  0.859375
train loss:  0.3740268051624298
train gradient:  0.15873781800131403
iteration : 11196
train acc:  0.828125
train loss:  0.3502700626850128
train gradient:  0.2006880854799723
iteration : 11197
train acc:  0.890625
train loss:  0.28606733679771423
train gradient:  0.10001114276867006
iteration : 11198
train acc:  0.8515625
train loss:  0.315914511680603
train gradient:  0.2254087359043503
iteration : 11199
train acc:  0.890625
train loss:  0.30070924758911133
train gradient:  0.12934493967188723
iteration : 11200
train acc:  0.8515625
train loss:  0.3159053325653076
train gradient:  0.0929667417937064
iteration : 11201
train acc:  0.8515625
train loss:  0.3178340792655945
train gradient:  0.13532446724224156
iteration : 11202
train acc:  0.84375
train loss:  0.3296907842159271
train gradient:  0.19471194970681788
iteration : 11203
train acc:  0.8359375
train loss:  0.36346885561943054
train gradient:  0.12963694287927013
iteration : 11204
train acc:  0.8359375
train loss:  0.3134753704071045
train gradient:  0.09172997542049863
iteration : 11205
train acc:  0.8671875
train loss:  0.3095100522041321
train gradient:  0.09854050079465655
iteration : 11206
train acc:  0.8984375
train loss:  0.26285040378570557
train gradient:  0.09427094639143353
iteration : 11207
train acc:  0.9296875
train loss:  0.2934926152229309
train gradient:  0.12134931969928174
iteration : 11208
train acc:  0.8203125
train loss:  0.3813421428203583
train gradient:  0.35358815763673573
iteration : 11209
train acc:  0.875
train loss:  0.3099168539047241
train gradient:  0.174681573655656
iteration : 11210
train acc:  0.890625
train loss:  0.2892315983772278
train gradient:  0.11015584419067759
iteration : 11211
train acc:  0.8671875
train loss:  0.3817512094974518
train gradient:  0.1318896880546426
iteration : 11212
train acc:  0.8984375
train loss:  0.2627562880516052
train gradient:  0.08027226990932815
iteration : 11213
train acc:  0.8671875
train loss:  0.32157257199287415
train gradient:  0.19586962672776503
iteration : 11214
train acc:  0.8671875
train loss:  0.29365330934524536
train gradient:  0.11991792978458135
iteration : 11215
train acc:  0.8359375
train loss:  0.34048283100128174
train gradient:  0.171022926659778
iteration : 11216
train acc:  0.859375
train loss:  0.34756889939308167
train gradient:  0.12134096011561019
iteration : 11217
train acc:  0.875
train loss:  0.34523144364356995
train gradient:  0.15690267726387647
iteration : 11218
train acc:  0.875
train loss:  0.2568332850933075
train gradient:  0.08344538352908186
iteration : 11219
train acc:  0.890625
train loss:  0.26068776845932007
train gradient:  0.12944037423541244
iteration : 11220
train acc:  0.890625
train loss:  0.24587970972061157
train gradient:  0.10994513734279646
iteration : 11221
train acc:  0.8515625
train loss:  0.29672008752822876
train gradient:  0.11871173079076351
iteration : 11222
train acc:  0.859375
train loss:  0.4102727770805359
train gradient:  0.2638297887339198
iteration : 11223
train acc:  0.8671875
train loss:  0.34915387630462646
train gradient:  0.14195658643862694
iteration : 11224
train acc:  0.859375
train loss:  0.33043429255485535
train gradient:  0.12414439052309968
iteration : 11225
train acc:  0.859375
train loss:  0.28904056549072266
train gradient:  0.12524302614612204
iteration : 11226
train acc:  0.90625
train loss:  0.2188788205385208
train gradient:  0.11438714166522791
iteration : 11227
train acc:  0.890625
train loss:  0.2975706160068512
train gradient:  0.15981204643245833
iteration : 11228
train acc:  0.8515625
train loss:  0.3401690721511841
train gradient:  0.14375970520635828
iteration : 11229
train acc:  0.8828125
train loss:  0.2818215489387512
train gradient:  0.10240379636877328
iteration : 11230
train acc:  0.9140625
train loss:  0.25767388939857483
train gradient:  0.10462602421161427
iteration : 11231
train acc:  0.828125
train loss:  0.3449031710624695
train gradient:  0.15402274313382142
iteration : 11232
train acc:  0.890625
train loss:  0.25943613052368164
train gradient:  0.12915897262649254
iteration : 11233
train acc:  0.8515625
train loss:  0.34357789158821106
train gradient:  0.22815030929518618
iteration : 11234
train acc:  0.8671875
train loss:  0.30245208740234375
train gradient:  0.1125636108496656
iteration : 11235
train acc:  0.859375
train loss:  0.35333874821662903
train gradient:  0.17855782251569252
iteration : 11236
train acc:  0.84375
train loss:  0.4156852066516876
train gradient:  0.174293449687471
iteration : 11237
train acc:  0.8515625
train loss:  0.3227289915084839
train gradient:  0.13870584850883813
iteration : 11238
train acc:  0.8359375
train loss:  0.34022170305252075
train gradient:  0.1191416753446729
iteration : 11239
train acc:  0.828125
train loss:  0.32922202348709106
train gradient:  0.19686942427816131
iteration : 11240
train acc:  0.859375
train loss:  0.29490694403648376
train gradient:  0.15331437361970804
iteration : 11241
train acc:  0.890625
train loss:  0.2727196216583252
train gradient:  0.17217262395742683
iteration : 11242
train acc:  0.890625
train loss:  0.2544034719467163
train gradient:  0.17256279689864792
iteration : 11243
train acc:  0.8515625
train loss:  0.3572671413421631
train gradient:  0.18422002163062354
iteration : 11244
train acc:  0.84375
train loss:  0.3551158905029297
train gradient:  0.23479351690463388
iteration : 11245
train acc:  0.859375
train loss:  0.3864462077617645
train gradient:  0.38078293335480046
iteration : 11246
train acc:  0.8125
train loss:  0.36661139130592346
train gradient:  0.2316913174664424
iteration : 11247
train acc:  0.9140625
train loss:  0.26200979948043823
train gradient:  0.12241892589639962
iteration : 11248
train acc:  0.875
train loss:  0.31967902183532715
train gradient:  0.178342297708471
iteration : 11249
train acc:  0.8515625
train loss:  0.3582950830459595
train gradient:  0.2339402523295155
iteration : 11250
train acc:  0.8828125
train loss:  0.30692699551582336
train gradient:  0.1130510438869189
iteration : 11251
train acc:  0.84375
train loss:  0.3451154828071594
train gradient:  0.15860903505550408
iteration : 11252
train acc:  0.8828125
train loss:  0.29839038848876953
train gradient:  0.15590252494964385
iteration : 11253
train acc:  0.84375
train loss:  0.3184363842010498
train gradient:  0.1842085827607897
iteration : 11254
train acc:  0.875
train loss:  0.28907647728919983
train gradient:  0.10415081014028926
iteration : 11255
train acc:  0.90625
train loss:  0.26907575130462646
train gradient:  0.12137242062903598
iteration : 11256
train acc:  0.84375
train loss:  0.3684563636779785
train gradient:  0.11886650221869481
iteration : 11257
train acc:  0.859375
train loss:  0.3639298677444458
train gradient:  0.13641128195208152
iteration : 11258
train acc:  0.8359375
train loss:  0.3046320378780365
train gradient:  0.14581315795373534
iteration : 11259
train acc:  0.8671875
train loss:  0.30449894070625305
train gradient:  0.12169514542250238
iteration : 11260
train acc:  0.875
train loss:  0.284217894077301
train gradient:  0.13688828271882092
iteration : 11261
train acc:  0.890625
train loss:  0.25683504343032837
train gradient:  0.11147684275689314
iteration : 11262
train acc:  0.84375
train loss:  0.30058562755584717
train gradient:  0.11378498734587926
iteration : 11263
train acc:  0.84375
train loss:  0.346815824508667
train gradient:  0.15520017485765464
iteration : 11264
train acc:  0.890625
train loss:  0.3016873896121979
train gradient:  0.14119821411640351
iteration : 11265
train acc:  0.859375
train loss:  0.3222355842590332
train gradient:  0.13965086404381105
iteration : 11266
train acc:  0.859375
train loss:  0.3284262418746948
train gradient:  0.15248306674795425
iteration : 11267
train acc:  0.84375
train loss:  0.3291493058204651
train gradient:  0.14305920678518447
iteration : 11268
train acc:  0.828125
train loss:  0.37987735867500305
train gradient:  0.16384456575260975
iteration : 11269
train acc:  0.9296875
train loss:  0.2555427849292755
train gradient:  0.09769830293088712
iteration : 11270
train acc:  0.8515625
train loss:  0.3098963499069214
train gradient:  0.2261924889562571
iteration : 11271
train acc:  0.875
train loss:  0.3270743489265442
train gradient:  0.13064298685342163
iteration : 11272
train acc:  0.84375
train loss:  0.3115842938423157
train gradient:  0.16144229825493353
iteration : 11273
train acc:  0.8203125
train loss:  0.3485203981399536
train gradient:  0.17356727850881143
iteration : 11274
train acc:  0.84375
train loss:  0.2994405925273895
train gradient:  0.09547071424471942
iteration : 11275
train acc:  0.8671875
train loss:  0.3385808765888214
train gradient:  0.15783234531690804
iteration : 11276
train acc:  0.859375
train loss:  0.3922913670539856
train gradient:  0.17427571287626406
iteration : 11277
train acc:  0.8671875
train loss:  0.2922431230545044
train gradient:  0.13853682992775476
iteration : 11278
train acc:  0.875
train loss:  0.26262539625167847
train gradient:  0.11441099164983871
iteration : 11279
train acc:  0.8203125
train loss:  0.35277843475341797
train gradient:  0.16872591644627472
iteration : 11280
train acc:  0.875
train loss:  0.29101336002349854
train gradient:  0.11496242082328605
iteration : 11281
train acc:  0.8828125
train loss:  0.29893702268600464
train gradient:  0.1869381589059303
iteration : 11282
train acc:  0.875
train loss:  0.3306608498096466
train gradient:  0.1789530303127367
iteration : 11283
train acc:  0.8515625
train loss:  0.3209305703639984
train gradient:  0.10101607770252978
iteration : 11284
train acc:  0.8828125
train loss:  0.2956720292568207
train gradient:  0.12822745024534532
iteration : 11285
train acc:  0.84375
train loss:  0.3336939215660095
train gradient:  0.12298380637657778
iteration : 11286
train acc:  0.8828125
train loss:  0.3284754157066345
train gradient:  0.22236343952476678
iteration : 11287
train acc:  0.875
train loss:  0.2780553698539734
train gradient:  0.12304765262564733
iteration : 11288
train acc:  0.8359375
train loss:  0.3259769678115845
train gradient:  0.20888728473880613
iteration : 11289
train acc:  0.8828125
train loss:  0.2660403847694397
train gradient:  0.10159485127197547
iteration : 11290
train acc:  0.859375
train loss:  0.3494255840778351
train gradient:  0.21036637005370057
iteration : 11291
train acc:  0.84375
train loss:  0.35497358441352844
train gradient:  0.15547962348345337
iteration : 11292
train acc:  0.8203125
train loss:  0.38829779624938965
train gradient:  0.19384144869305428
iteration : 11293
train acc:  0.8359375
train loss:  0.34554827213287354
train gradient:  0.16482497490996248
iteration : 11294
train acc:  0.8984375
train loss:  0.26447808742523193
train gradient:  0.11798531327613565
iteration : 11295
train acc:  0.8359375
train loss:  0.3740810453891754
train gradient:  0.24764225467394702
iteration : 11296
train acc:  0.84375
train loss:  0.37964481115341187
train gradient:  0.1599823163334893
iteration : 11297
train acc:  0.8828125
train loss:  0.30826234817504883
train gradient:  0.12133663122205583
iteration : 11298
train acc:  0.875
train loss:  0.34573623538017273
train gradient:  0.3595481817690951
iteration : 11299
train acc:  0.90625
train loss:  0.25081828236579895
train gradient:  0.11593937828275012
iteration : 11300
train acc:  0.859375
train loss:  0.25954410433769226
train gradient:  0.1051678264832367
iteration : 11301
train acc:  0.90625
train loss:  0.25804880261421204
train gradient:  0.08509865911365325
iteration : 11302
train acc:  0.7734375
train loss:  0.4346698820590973
train gradient:  0.25722422440763804
iteration : 11303
train acc:  0.7734375
train loss:  0.43901336193084717
train gradient:  0.3263913498149064
iteration : 11304
train acc:  0.921875
train loss:  0.23449334502220154
train gradient:  0.08005559688204679
iteration : 11305
train acc:  0.8984375
train loss:  0.2866079807281494
train gradient:  0.159355092268246
iteration : 11306
train acc:  0.8203125
train loss:  0.36839938163757324
train gradient:  0.22382411733010218
iteration : 11307
train acc:  0.890625
train loss:  0.24672459065914154
train gradient:  0.10309183753240848
iteration : 11308
train acc:  0.8984375
train loss:  0.241609588265419
train gradient:  0.08567196384869728
iteration : 11309
train acc:  0.8125
train loss:  0.3865888714790344
train gradient:  0.1801764356351265
iteration : 11310
train acc:  0.8046875
train loss:  0.39116454124450684
train gradient:  0.23374871106480186
iteration : 11311
train acc:  0.890625
train loss:  0.2884978652000427
train gradient:  0.12301530730765282
iteration : 11312
train acc:  0.8671875
train loss:  0.2615680396556854
train gradient:  0.19596991597005653
iteration : 11313
train acc:  0.9140625
train loss:  0.2850424647331238
train gradient:  0.1256873813123696
iteration : 11314
train acc:  0.859375
train loss:  0.3493902087211609
train gradient:  0.16469406392864955
iteration : 11315
train acc:  0.8515625
train loss:  0.3573906421661377
train gradient:  0.1953324014192852
iteration : 11316
train acc:  0.8671875
train loss:  0.2945212125778198
train gradient:  0.15554849670440246
iteration : 11317
train acc:  0.8828125
train loss:  0.30810999870300293
train gradient:  0.14117450993071096
iteration : 11318
train acc:  0.8125
train loss:  0.41612598299980164
train gradient:  0.21012699807130092
iteration : 11319
train acc:  0.7890625
train loss:  0.3593560755252838
train gradient:  0.15393403861627153
iteration : 11320
train acc:  0.828125
train loss:  0.3587854504585266
train gradient:  0.19848710638357214
iteration : 11321
train acc:  0.8828125
train loss:  0.2901878356933594
train gradient:  0.12684483910021538
iteration : 11322
train acc:  0.8515625
train loss:  0.3759783208370209
train gradient:  0.2780906490193766
iteration : 11323
train acc:  0.8671875
train loss:  0.3228491544723511
train gradient:  0.1942325372838965
iteration : 11324
train acc:  0.890625
train loss:  0.28342169523239136
train gradient:  0.14503064509596927
iteration : 11325
train acc:  0.875
train loss:  0.37154269218444824
train gradient:  0.4479210072437561
iteration : 11326
train acc:  0.875
train loss:  0.28504645824432373
train gradient:  0.20853327541995675
iteration : 11327
train acc:  0.8515625
train loss:  0.3048008680343628
train gradient:  0.16842299625598817
iteration : 11328
train acc:  0.8828125
train loss:  0.31275153160095215
train gradient:  0.13208859079254537
iteration : 11329
train acc:  0.9140625
train loss:  0.20299485325813293
train gradient:  0.08214882784342609
iteration : 11330
train acc:  0.890625
train loss:  0.28904736042022705
train gradient:  0.15249877448936933
iteration : 11331
train acc:  0.8671875
train loss:  0.3170112073421478
train gradient:  0.1695661922371965
iteration : 11332
train acc:  0.828125
train loss:  0.3385072350502014
train gradient:  0.15778301051724505
iteration : 11333
train acc:  0.90625
train loss:  0.26606249809265137
train gradient:  0.13000222560508948
iteration : 11334
train acc:  0.8828125
train loss:  0.3309841752052307
train gradient:  0.26625635352837423
iteration : 11335
train acc:  0.8984375
train loss:  0.2852267622947693
train gradient:  0.13764376602822248
iteration : 11336
train acc:  0.890625
train loss:  0.2436942607164383
train gradient:  0.09719343419913593
iteration : 11337
train acc:  0.7890625
train loss:  0.4817642271518707
train gradient:  0.2684365842315288
iteration : 11338
train acc:  0.8515625
train loss:  0.3090561330318451
train gradient:  0.13571137037807507
iteration : 11339
train acc:  0.890625
train loss:  0.27139419317245483
train gradient:  0.09315018592370096
iteration : 11340
train acc:  0.8515625
train loss:  0.31960296630859375
train gradient:  0.14644146122459678
iteration : 11341
train acc:  0.8046875
train loss:  0.418058842420578
train gradient:  0.31704242862760607
iteration : 11342
train acc:  0.875
train loss:  0.3578869104385376
train gradient:  0.22969088149013445
iteration : 11343
train acc:  0.859375
train loss:  0.33551251888275146
train gradient:  0.12189258284313392
iteration : 11344
train acc:  0.875
train loss:  0.26418206095695496
train gradient:  0.13009458720181075
iteration : 11345
train acc:  0.8671875
train loss:  0.30523836612701416
train gradient:  0.15831076077862768
iteration : 11346
train acc:  0.8671875
train loss:  0.34315937757492065
train gradient:  0.18014478895056774
iteration : 11347
train acc:  0.8984375
train loss:  0.24328145384788513
train gradient:  0.10312821027396464
iteration : 11348
train acc:  0.875
train loss:  0.3310316801071167
train gradient:  0.16320215917499503
iteration : 11349
train acc:  0.890625
train loss:  0.28822949528694153
train gradient:  0.2550440285061642
iteration : 11350
train acc:  0.84375
train loss:  0.33352750539779663
train gradient:  0.19618679629039984
iteration : 11351
train acc:  0.828125
train loss:  0.37711405754089355
train gradient:  0.2382381407756149
iteration : 11352
train acc:  0.8515625
train loss:  0.3426774740219116
train gradient:  0.17577787216855367
iteration : 11353
train acc:  0.8359375
train loss:  0.3447818160057068
train gradient:  0.25525106475331716
iteration : 11354
train acc:  0.84375
train loss:  0.35139793157577515
train gradient:  0.24899913575254523
iteration : 11355
train acc:  0.8359375
train loss:  0.3528738021850586
train gradient:  0.1977104514845373
iteration : 11356
train acc:  0.859375
train loss:  0.29106491804122925
train gradient:  0.18943998207549081
iteration : 11357
train acc:  0.8828125
train loss:  0.2685984969139099
train gradient:  0.12303105129553844
iteration : 11358
train acc:  0.8515625
train loss:  0.3776499032974243
train gradient:  0.16652092502794763
iteration : 11359
train acc:  0.90625
train loss:  0.23945870995521545
train gradient:  0.07672835579609966
iteration : 11360
train acc:  0.8359375
train loss:  0.30274224281311035
train gradient:  0.152138899694449
iteration : 11361
train acc:  0.859375
train loss:  0.30846893787384033
train gradient:  0.1343219504241106
iteration : 11362
train acc:  0.8671875
train loss:  0.29668164253234863
train gradient:  0.143713657687698
iteration : 11363
train acc:  0.90625
train loss:  0.26570677757263184
train gradient:  0.1046719935101992
iteration : 11364
train acc:  0.859375
train loss:  0.30894359946250916
train gradient:  0.19935529383842981
iteration : 11365
train acc:  0.8515625
train loss:  0.35102224349975586
train gradient:  0.18410268101339528
iteration : 11366
train acc:  0.859375
train loss:  0.2790702283382416
train gradient:  0.1328812935732172
iteration : 11367
train acc:  0.8125
train loss:  0.3947107493877411
train gradient:  0.20031647094982383
iteration : 11368
train acc:  0.8125
train loss:  0.42671531438827515
train gradient:  0.28405557767204337
iteration : 11369
train acc:  0.890625
train loss:  0.2636464238166809
train gradient:  0.10027564178604123
iteration : 11370
train acc:  0.9140625
train loss:  0.2643948793411255
train gradient:  0.10151029746346178
iteration : 11371
train acc:  0.8359375
train loss:  0.315213680267334
train gradient:  0.15446943380468575
iteration : 11372
train acc:  0.859375
train loss:  0.33964425325393677
train gradient:  0.23526811978298448
iteration : 11373
train acc:  0.875
train loss:  0.3054003119468689
train gradient:  0.11154891808106278
iteration : 11374
train acc:  0.7890625
train loss:  0.44524461030960083
train gradient:  0.27757624915052503
iteration : 11375
train acc:  0.875
train loss:  0.2514532208442688
train gradient:  0.14738649796631798
iteration : 11376
train acc:  0.859375
train loss:  0.3401446044445038
train gradient:  0.12489812638448783
iteration : 11377
train acc:  0.8515625
train loss:  0.334053635597229
train gradient:  0.11930401065623081
iteration : 11378
train acc:  0.8671875
train loss:  0.32653582096099854
train gradient:  0.16648392739219942
iteration : 11379
train acc:  0.875
train loss:  0.34178832173347473
train gradient:  0.1504126653870303
iteration : 11380
train acc:  0.84375
train loss:  0.37385135889053345
train gradient:  0.2109867630997404
iteration : 11381
train acc:  0.9140625
train loss:  0.25742587447166443
train gradient:  0.08437852619951204
iteration : 11382
train acc:  0.84375
train loss:  0.34696313738822937
train gradient:  0.19509334360026992
iteration : 11383
train acc:  0.8671875
train loss:  0.2668493390083313
train gradient:  0.14561460439850438
iteration : 11384
train acc:  0.8984375
train loss:  0.22657866775989532
train gradient:  0.09590460251255385
iteration : 11385
train acc:  0.90625
train loss:  0.2641250491142273
train gradient:  0.13599085276000766
iteration : 11386
train acc:  0.8359375
train loss:  0.32869014143943787
train gradient:  0.14078131887802367
iteration : 11387
train acc:  0.84375
train loss:  0.3490692973136902
train gradient:  0.2344886745567591
iteration : 11388
train acc:  0.8515625
train loss:  0.32431694865226746
train gradient:  0.12930326317543034
iteration : 11389
train acc:  0.7734375
train loss:  0.463800311088562
train gradient:  0.2737861855688455
iteration : 11390
train acc:  0.859375
train loss:  0.32198524475097656
train gradient:  0.21283363907426722
iteration : 11391
train acc:  0.8828125
train loss:  0.28650736808776855
train gradient:  0.1125447379500154
iteration : 11392
train acc:  0.84375
train loss:  0.3158041834831238
train gradient:  0.18092686871727207
iteration : 11393
train acc:  0.921875
train loss:  0.3097197115421295
train gradient:  0.24495901248944568
iteration : 11394
train acc:  0.84375
train loss:  0.31435614824295044
train gradient:  0.17972257279619552
iteration : 11395
train acc:  0.828125
train loss:  0.3713197410106659
train gradient:  0.3167831354396514
iteration : 11396
train acc:  0.828125
train loss:  0.30594921112060547
train gradient:  0.34202148625981366
iteration : 11397
train acc:  0.8515625
train loss:  0.34304821491241455
train gradient:  0.14355858542151706
iteration : 11398
train acc:  0.8125
train loss:  0.42020007967948914
train gradient:  0.30914491242245545
iteration : 11399
train acc:  0.8671875
train loss:  0.3361453711986542
train gradient:  0.2813702041857228
iteration : 11400
train acc:  0.8828125
train loss:  0.3002949357032776
train gradient:  0.12091674129210205
iteration : 11401
train acc:  0.8359375
train loss:  0.38349586725234985
train gradient:  0.1949327385269896
iteration : 11402
train acc:  0.859375
train loss:  0.3245486617088318
train gradient:  0.1882182095640623
iteration : 11403
train acc:  0.8125
train loss:  0.4003898501396179
train gradient:  0.25927097521341813
iteration : 11404
train acc:  0.8671875
train loss:  0.32848912477493286
train gradient:  0.16279074331209942
iteration : 11405
train acc:  0.890625
train loss:  0.305686354637146
train gradient:  0.13745600459200027
iteration : 11406
train acc:  0.859375
train loss:  0.36105597019195557
train gradient:  0.2559543107185461
iteration : 11407
train acc:  0.859375
train loss:  0.3170931339263916
train gradient:  0.14889599600916031
iteration : 11408
train acc:  0.875
train loss:  0.3218437433242798
train gradient:  0.15435381428858275
iteration : 11409
train acc:  0.875
train loss:  0.3031105399131775
train gradient:  0.13162828369131918
iteration : 11410
train acc:  0.90625
train loss:  0.3045642077922821
train gradient:  0.09482161692188508
iteration : 11411
train acc:  0.8203125
train loss:  0.3450223207473755
train gradient:  0.21602972454523434
iteration : 11412
train acc:  0.84375
train loss:  0.377472460269928
train gradient:  0.16762744213847047
iteration : 11413
train acc:  0.8515625
train loss:  0.35802051424980164
train gradient:  0.13229815124878103
iteration : 11414
train acc:  0.8359375
train loss:  0.3388865888118744
train gradient:  0.21474099259778928
iteration : 11415
train acc:  0.890625
train loss:  0.25779277086257935
train gradient:  0.07293616720278144
iteration : 11416
train acc:  0.8828125
train loss:  0.2985854148864746
train gradient:  0.1232065380350419
iteration : 11417
train acc:  0.8515625
train loss:  0.29311707615852356
train gradient:  0.15337507299282782
iteration : 11418
train acc:  0.875
train loss:  0.28467240929603577
train gradient:  0.10249989042267194
iteration : 11419
train acc:  0.859375
train loss:  0.36266809701919556
train gradient:  0.1828275955221604
iteration : 11420
train acc:  0.8359375
train loss:  0.3783565163612366
train gradient:  0.14919024043834073
iteration : 11421
train acc:  0.921875
train loss:  0.23318567872047424
train gradient:  0.13264602517027357
iteration : 11422
train acc:  0.84375
train loss:  0.30863332748413086
train gradient:  0.09744330731301
iteration : 11423
train acc:  0.828125
train loss:  0.2933698892593384
train gradient:  0.14014246128608132
iteration : 11424
train acc:  0.8359375
train loss:  0.3286953568458557
train gradient:  0.12052386447426382
iteration : 11425
train acc:  0.890625
train loss:  0.294315367937088
train gradient:  0.15080850052579312
iteration : 11426
train acc:  0.9296875
train loss:  0.22344163060188293
train gradient:  0.0912780871898963
iteration : 11427
train acc:  0.90625
train loss:  0.31211212277412415
train gradient:  0.1526360371139234
iteration : 11428
train acc:  0.8828125
train loss:  0.2898132801055908
train gradient:  0.09623679508509825
iteration : 11429
train acc:  0.8828125
train loss:  0.3219602704048157
train gradient:  0.17665329848556016
iteration : 11430
train acc:  0.875
train loss:  0.34043458104133606
train gradient:  0.1370408940378662
iteration : 11431
train acc:  0.8515625
train loss:  0.3789517879486084
train gradient:  0.24824647038808967
iteration : 11432
train acc:  0.8515625
train loss:  0.28726649284362793
train gradient:  0.10486459764646185
iteration : 11433
train acc:  0.84375
train loss:  0.32536613941192627
train gradient:  0.11641488592221892
iteration : 11434
train acc:  0.84375
train loss:  0.3442355692386627
train gradient:  0.15628419538569102
iteration : 11435
train acc:  0.890625
train loss:  0.2936371862888336
train gradient:  0.18272890667238356
iteration : 11436
train acc:  0.84375
train loss:  0.3270900249481201
train gradient:  0.17035089839503842
iteration : 11437
train acc:  0.84375
train loss:  0.3597967028617859
train gradient:  0.18455973069734327
iteration : 11438
train acc:  0.8359375
train loss:  0.41119638085365295
train gradient:  0.1802566596750892
iteration : 11439
train acc:  0.8203125
train loss:  0.36937832832336426
train gradient:  0.1334736096204373
iteration : 11440
train acc:  0.796875
train loss:  0.44641074538230896
train gradient:  0.29755265679239185
iteration : 11441
train acc:  0.875
train loss:  0.3047950267791748
train gradient:  0.1320369719993645
iteration : 11442
train acc:  0.796875
train loss:  0.3689920902252197
train gradient:  0.1636007510960402
iteration : 11443
train acc:  0.8125
train loss:  0.454224169254303
train gradient:  0.21113533502817605
iteration : 11444
train acc:  0.859375
train loss:  0.29783186316490173
train gradient:  0.10515679362206783
iteration : 11445
train acc:  0.9140625
train loss:  0.2676273584365845
train gradient:  0.13462496507331267
iteration : 11446
train acc:  0.8359375
train loss:  0.37604403495788574
train gradient:  0.18010078388428746
iteration : 11447
train acc:  0.8359375
train loss:  0.3364885449409485
train gradient:  0.1362664980830413
iteration : 11448
train acc:  0.8203125
train loss:  0.363038033246994
train gradient:  0.13904063105945016
iteration : 11449
train acc:  0.8515625
train loss:  0.3241730034351349
train gradient:  0.15424112368900117
iteration : 11450
train acc:  0.84375
train loss:  0.32121503353118896
train gradient:  0.12479023109257871
iteration : 11451
train acc:  0.8359375
train loss:  0.3263765573501587
train gradient:  0.13606075905830584
iteration : 11452
train acc:  0.875
train loss:  0.27690964937210083
train gradient:  0.1385182193609019
iteration : 11453
train acc:  0.8671875
train loss:  0.315094530582428
train gradient:  0.13453894146023213
iteration : 11454
train acc:  0.9140625
train loss:  0.24963077902793884
train gradient:  0.08096396434647186
iteration : 11455
train acc:  0.8828125
train loss:  0.31968170404434204
train gradient:  0.1401904037109536
iteration : 11456
train acc:  0.875
train loss:  0.28170573711395264
train gradient:  0.11785618076409167
iteration : 11457
train acc:  0.8671875
train loss:  0.3020991086959839
train gradient:  0.11597097642509255
iteration : 11458
train acc:  0.8984375
train loss:  0.26646867394447327
train gradient:  0.08574123958386498
iteration : 11459
train acc:  0.828125
train loss:  0.31456226110458374
train gradient:  0.11428776807579118
iteration : 11460
train acc:  0.859375
train loss:  0.3123296797275543
train gradient:  0.12187490511053577
iteration : 11461
train acc:  0.8984375
train loss:  0.26292961835861206
train gradient:  0.14069034633034927
iteration : 11462
train acc:  0.8359375
train loss:  0.35069602727890015
train gradient:  0.13796614682649525
iteration : 11463
train acc:  0.859375
train loss:  0.28255945444107056
train gradient:  0.1271157629226562
iteration : 11464
train acc:  0.8671875
train loss:  0.3404068350791931
train gradient:  0.13940425507284984
iteration : 11465
train acc:  0.8828125
train loss:  0.2999410927295685
train gradient:  0.11542989081194696
iteration : 11466
train acc:  0.890625
train loss:  0.33437082171440125
train gradient:  0.26336444166097844
iteration : 11467
train acc:  0.8359375
train loss:  0.32993483543395996
train gradient:  0.1586608933125268
iteration : 11468
train acc:  0.875
train loss:  0.35550835728645325
train gradient:  0.15575795174898804
iteration : 11469
train acc:  0.90625
train loss:  0.2788379490375519
train gradient:  0.12719083612019239
iteration : 11470
train acc:  0.8515625
train loss:  0.32323336601257324
train gradient:  0.14320822278777812
iteration : 11471
train acc:  0.8828125
train loss:  0.2745775580406189
train gradient:  0.11383349535508441
iteration : 11472
train acc:  0.859375
train loss:  0.31326600909233093
train gradient:  0.12563578576701973
iteration : 11473
train acc:  0.859375
train loss:  0.3649313151836395
train gradient:  0.21030500024736265
iteration : 11474
train acc:  0.9375
train loss:  0.2121497243642807
train gradient:  0.10748626407488251
iteration : 11475
train acc:  0.8203125
train loss:  0.3553581237792969
train gradient:  0.21764166465362506
iteration : 11476
train acc:  0.8671875
train loss:  0.3176000118255615
train gradient:  0.1299405333369772
iteration : 11477
train acc:  0.8671875
train loss:  0.3087511658668518
train gradient:  0.2678878731451224
iteration : 11478
train acc:  0.8359375
train loss:  0.3463195264339447
train gradient:  0.14339916884977844
iteration : 11479
train acc:  0.8359375
train loss:  0.4571731686592102
train gradient:  0.2601502257519296
iteration : 11480
train acc:  0.828125
train loss:  0.42846107482910156
train gradient:  0.20892489487449212
iteration : 11481
train acc:  0.84375
train loss:  0.4295918047428131
train gradient:  0.2941872003996992
iteration : 11482
train acc:  0.8671875
train loss:  0.29739654064178467
train gradient:  0.13377752733233594
iteration : 11483
train acc:  0.8203125
train loss:  0.37170132994651794
train gradient:  0.17497399747567438
iteration : 11484
train acc:  0.921875
train loss:  0.23647204041481018
train gradient:  0.09610409400883856
iteration : 11485
train acc:  0.84375
train loss:  0.3657678961753845
train gradient:  0.16494913541161638
iteration : 11486
train acc:  0.8515625
train loss:  0.2968096435070038
train gradient:  0.13961194831200097
iteration : 11487
train acc:  0.8671875
train loss:  0.31253135204315186
train gradient:  0.12308272111979252
iteration : 11488
train acc:  0.9140625
train loss:  0.23915402591228485
train gradient:  0.11614852767298232
iteration : 11489
train acc:  0.8515625
train loss:  0.3182898163795471
train gradient:  0.22845338805740945
iteration : 11490
train acc:  0.84375
train loss:  0.3276848793029785
train gradient:  0.15032754974108622
iteration : 11491
train acc:  0.8671875
train loss:  0.3212120234966278
train gradient:  0.14097077927245721
iteration : 11492
train acc:  0.859375
train loss:  0.308319091796875
train gradient:  0.15522073330936892
iteration : 11493
train acc:  0.8515625
train loss:  0.33636972308158875
train gradient:  0.19756775684892788
iteration : 11494
train acc:  0.8359375
train loss:  0.2855597138404846
train gradient:  0.1489572535744425
iteration : 11495
train acc:  0.8671875
train loss:  0.3143412470817566
train gradient:  0.14609182960388034
iteration : 11496
train acc:  0.84375
train loss:  0.29318344593048096
train gradient:  0.19939127682880042
iteration : 11497
train acc:  0.8671875
train loss:  0.29532861709594727
train gradient:  0.12557460510896934
iteration : 11498
train acc:  0.8984375
train loss:  0.28859007358551025
train gradient:  0.11639408907584885
iteration : 11499
train acc:  0.90625
train loss:  0.26466408371925354
train gradient:  0.11881508564517988
iteration : 11500
train acc:  0.78125
train loss:  0.4514789283275604
train gradient:  0.2941439759412479
iteration : 11501
train acc:  0.8359375
train loss:  0.30928754806518555
train gradient:  0.14079070417404785
iteration : 11502
train acc:  0.8828125
train loss:  0.266512393951416
train gradient:  0.14018046547390997
iteration : 11503
train acc:  0.828125
train loss:  0.40770065784454346
train gradient:  0.18237548726956376
iteration : 11504
train acc:  0.859375
train loss:  0.30623137950897217
train gradient:  0.20823312709485603
iteration : 11505
train acc:  0.90625
train loss:  0.2546214461326599
train gradient:  0.11801136357981924
iteration : 11506
train acc:  0.8125
train loss:  0.34960508346557617
train gradient:  0.15700373646508115
iteration : 11507
train acc:  0.859375
train loss:  0.40701019763946533
train gradient:  0.16401439688289807
iteration : 11508
train acc:  0.8828125
train loss:  0.3112490773200989
train gradient:  0.19033065051357845
iteration : 11509
train acc:  0.8671875
train loss:  0.33726227283477783
train gradient:  0.1559902231829912
iteration : 11510
train acc:  0.8125
train loss:  0.40029656887054443
train gradient:  0.2075448992552453
iteration : 11511
train acc:  0.8515625
train loss:  0.33114850521087646
train gradient:  0.1714485422135049
iteration : 11512
train acc:  0.84375
train loss:  0.33056220412254333
train gradient:  0.13174934169043614
iteration : 11513
train acc:  0.875
train loss:  0.3104608654975891
train gradient:  0.16576923115913833
iteration : 11514
train acc:  0.84375
train loss:  0.32241642475128174
train gradient:  0.14489924892815362
iteration : 11515
train acc:  0.875
train loss:  0.2965731620788574
train gradient:  0.14301826977272014
iteration : 11516
train acc:  0.890625
train loss:  0.2506336271762848
train gradient:  0.10876208220577936
iteration : 11517
train acc:  0.8515625
train loss:  0.33942773938179016
train gradient:  0.1964899233851597
iteration : 11518
train acc:  0.890625
train loss:  0.24698573350906372
train gradient:  0.14947409297918832
iteration : 11519
train acc:  0.8828125
train loss:  0.31061840057373047
train gradient:  0.125876614052368
iteration : 11520
train acc:  0.84375
train loss:  0.28565847873687744
train gradient:  0.10490834479079869
iteration : 11521
train acc:  0.8671875
train loss:  0.32075923681259155
train gradient:  0.181005064347611
iteration : 11522
train acc:  0.8671875
train loss:  0.2891114354133606
train gradient:  0.1348010489819514
iteration : 11523
train acc:  0.890625
train loss:  0.2737981081008911
train gradient:  0.09603011622527372
iteration : 11524
train acc:  0.859375
train loss:  0.3415658473968506
train gradient:  0.18070274741667103
iteration : 11525
train acc:  0.890625
train loss:  0.3895784616470337
train gradient:  0.1999422115063697
iteration : 11526
train acc:  0.8203125
train loss:  0.3972870707511902
train gradient:  0.1881059495864914
iteration : 11527
train acc:  0.90625
train loss:  0.2863408327102661
train gradient:  0.1304410562148789
iteration : 11528
train acc:  0.84375
train loss:  0.3356728255748749
train gradient:  0.15425589696428382
iteration : 11529
train acc:  0.8515625
train loss:  0.3044469654560089
train gradient:  0.11410101043858403
iteration : 11530
train acc:  0.7890625
train loss:  0.32160359621047974
train gradient:  0.1186170149142648
iteration : 11531
train acc:  0.8984375
train loss:  0.266710102558136
train gradient:  0.09775243699750732
iteration : 11532
train acc:  0.84375
train loss:  0.3530166745185852
train gradient:  0.1408529844262632
iteration : 11533
train acc:  0.8515625
train loss:  0.28956833481788635
train gradient:  0.12993323378892185
iteration : 11534
train acc:  0.828125
train loss:  0.3949381113052368
train gradient:  0.19030578250498503
iteration : 11535
train acc:  0.890625
train loss:  0.31767338514328003
train gradient:  0.23865509752228348
iteration : 11536
train acc:  0.859375
train loss:  0.32851099967956543
train gradient:  0.1844311120332826
iteration : 11537
train acc:  0.875
train loss:  0.2913461923599243
train gradient:  0.11760648452916532
iteration : 11538
train acc:  0.8828125
train loss:  0.34542471170425415
train gradient:  0.1511815347965387
iteration : 11539
train acc:  0.875
train loss:  0.27761924266815186
train gradient:  0.119543668768136
iteration : 11540
train acc:  0.875
train loss:  0.27809250354766846
train gradient:  0.1281895683024168
iteration : 11541
train acc:  0.890625
train loss:  0.31100884079933167
train gradient:  0.13642091917641247
iteration : 11542
train acc:  0.90625
train loss:  0.24677333235740662
train gradient:  0.12173248755363406
iteration : 11543
train acc:  0.8125
train loss:  0.34764552116394043
train gradient:  0.1588593554148545
iteration : 11544
train acc:  0.8671875
train loss:  0.2949230670928955
train gradient:  0.1166601044964098
iteration : 11545
train acc:  0.828125
train loss:  0.35266411304473877
train gradient:  0.1326964031568274
iteration : 11546
train acc:  0.8828125
train loss:  0.29548558592796326
train gradient:  0.11411602208641432
iteration : 11547
train acc:  0.8359375
train loss:  0.32766473293304443
train gradient:  0.19147776798826732
iteration : 11548
train acc:  0.796875
train loss:  0.40204790234565735
train gradient:  0.2887763948618979
iteration : 11549
train acc:  0.859375
train loss:  0.3617834448814392
train gradient:  0.20656993317628705
iteration : 11550
train acc:  0.875
train loss:  0.3002808094024658
train gradient:  0.12769085404633995
iteration : 11551
train acc:  0.8359375
train loss:  0.2978188395500183
train gradient:  0.14500181193448436
iteration : 11552
train acc:  0.8984375
train loss:  0.2758595049381256
train gradient:  0.09414748360318449
iteration : 11553
train acc:  0.8359375
train loss:  0.35120993852615356
train gradient:  0.1973471454407587
iteration : 11554
train acc:  0.859375
train loss:  0.32582810521125793
train gradient:  0.23560533654354143
iteration : 11555
train acc:  0.828125
train loss:  0.3888198435306549
train gradient:  0.20201866424450543
iteration : 11556
train acc:  0.8671875
train loss:  0.2756239175796509
train gradient:  0.14298466845187163
iteration : 11557
train acc:  0.890625
train loss:  0.24723869562149048
train gradient:  0.10339108916588581
iteration : 11558
train acc:  0.859375
train loss:  0.37291282415390015
train gradient:  0.18795253088916816
iteration : 11559
train acc:  0.8359375
train loss:  0.3903889060020447
train gradient:  0.25149849883146874
iteration : 11560
train acc:  0.8359375
train loss:  0.3451116979122162
train gradient:  0.14706091578361405
iteration : 11561
train acc:  0.859375
train loss:  0.36286455392837524
train gradient:  0.20976362743685356
iteration : 11562
train acc:  0.890625
train loss:  0.3004736304283142
train gradient:  0.117041822646813
iteration : 11563
train acc:  0.8515625
train loss:  0.2954990863800049
train gradient:  0.10990412064703685
iteration : 11564
train acc:  0.8203125
train loss:  0.3691883683204651
train gradient:  0.19025524286491663
iteration : 11565
train acc:  0.84375
train loss:  0.31873399019241333
train gradient:  0.15186462446923044
iteration : 11566
train acc:  0.859375
train loss:  0.36837238073349
train gradient:  0.20521230882574273
iteration : 11567
train acc:  0.859375
train loss:  0.3033602237701416
train gradient:  0.12392472445587367
iteration : 11568
train acc:  0.8828125
train loss:  0.29440364241600037
train gradient:  0.2580210878202971
iteration : 11569
train acc:  0.8828125
train loss:  0.28615283966064453
train gradient:  0.10862004971663668
iteration : 11570
train acc:  0.8671875
train loss:  0.31019362807273865
train gradient:  0.11927001027290185
iteration : 11571
train acc:  0.859375
train loss:  0.3391892910003662
train gradient:  0.17627426629186455
iteration : 11572
train acc:  0.859375
train loss:  0.29668253660202026
train gradient:  0.14968645648402135
iteration : 11573
train acc:  0.8125
train loss:  0.3639602065086365
train gradient:  0.20402353554234343
iteration : 11574
train acc:  0.8828125
train loss:  0.32844778895378113
train gradient:  0.13263339252624096
iteration : 11575
train acc:  0.828125
train loss:  0.4240915775299072
train gradient:  0.23993667136832383
iteration : 11576
train acc:  0.875
train loss:  0.34027159214019775
train gradient:  0.21359028438166086
iteration : 11577
train acc:  0.875
train loss:  0.3242390751838684
train gradient:  0.17927832232511953
iteration : 11578
train acc:  0.875
train loss:  0.29620692133903503
train gradient:  0.17011456499380764
iteration : 11579
train acc:  0.875
train loss:  0.2940114438533783
train gradient:  0.09295041710868031
iteration : 11580
train acc:  0.8359375
train loss:  0.3576117753982544
train gradient:  0.16683353788549113
iteration : 11581
train acc:  0.8828125
train loss:  0.2804609537124634
train gradient:  0.13059854903775492
iteration : 11582
train acc:  0.796875
train loss:  0.38627874851226807
train gradient:  0.24430178017066062
iteration : 11583
train acc:  0.8828125
train loss:  0.29626864194869995
train gradient:  0.2240378453305261
iteration : 11584
train acc:  0.8828125
train loss:  0.27616018056869507
train gradient:  0.22478542826674724
iteration : 11585
train acc:  0.828125
train loss:  0.36069178581237793
train gradient:  0.12880622503987343
iteration : 11586
train acc:  0.84375
train loss:  0.30658847093582153
train gradient:  0.12244550555310414
iteration : 11587
train acc:  0.8125
train loss:  0.34227779507637024
train gradient:  0.18850533360665783
iteration : 11588
train acc:  0.8359375
train loss:  0.34771978855133057
train gradient:  0.25774997997106014
iteration : 11589
train acc:  0.8515625
train loss:  0.295830100774765
train gradient:  0.13442857760465418
iteration : 11590
train acc:  0.8671875
train loss:  0.30923697352409363
train gradient:  0.10942170499900933
iteration : 11591
train acc:  0.8359375
train loss:  0.3618742525577545
train gradient:  0.12442690482408739
iteration : 11592
train acc:  0.890625
train loss:  0.2685360908508301
train gradient:  0.15617323569008495
iteration : 11593
train acc:  0.8671875
train loss:  0.30943241715431213
train gradient:  0.12001356593394856
iteration : 11594
train acc:  0.8046875
train loss:  0.3305496573448181
train gradient:  0.16637738865841248
iteration : 11595
train acc:  0.828125
train loss:  0.3520662188529968
train gradient:  0.15852442341028228
iteration : 11596
train acc:  0.828125
train loss:  0.3791147768497467
train gradient:  0.17451766865185586
iteration : 11597
train acc:  0.8125
train loss:  0.3460768163204193
train gradient:  0.15784094500273035
iteration : 11598
train acc:  0.8203125
train loss:  0.3892885148525238
train gradient:  0.25338144308664684
iteration : 11599
train acc:  0.8671875
train loss:  0.26762428879737854
train gradient:  0.09813448798130055
iteration : 11600
train acc:  0.8359375
train loss:  0.34393495321273804
train gradient:  0.14713421272062904
iteration : 11601
train acc:  0.8671875
train loss:  0.28401628136634827
train gradient:  0.1283253962943594
iteration : 11602
train acc:  0.8515625
train loss:  0.3257918357849121
train gradient:  0.15806550637794015
iteration : 11603
train acc:  0.828125
train loss:  0.36612749099731445
train gradient:  0.15635356697514696
iteration : 11604
train acc:  0.8359375
train loss:  0.360873818397522
train gradient:  0.1896470832952225
iteration : 11605
train acc:  0.84375
train loss:  0.34429290890693665
train gradient:  0.16991758544904173
iteration : 11606
train acc:  0.890625
train loss:  0.28155967593193054
train gradient:  0.12378833044231152
iteration : 11607
train acc:  0.9140625
train loss:  0.25944358110427856
train gradient:  0.09014650788542708
iteration : 11608
train acc:  0.8984375
train loss:  0.26443564891815186
train gradient:  0.10820848856646538
iteration : 11609
train acc:  0.890625
train loss:  0.31045129895210266
train gradient:  0.11249933662026373
iteration : 11610
train acc:  0.78125
train loss:  0.35157322883605957
train gradient:  0.174121764716792
iteration : 11611
train acc:  0.875
train loss:  0.3427666425704956
train gradient:  0.21519376827786252
iteration : 11612
train acc:  0.8828125
train loss:  0.26876887679100037
train gradient:  0.08820730128532454
iteration : 11613
train acc:  0.890625
train loss:  0.27510127425193787
train gradient:  0.15067221884403315
iteration : 11614
train acc:  0.8515625
train loss:  0.37442952394485474
train gradient:  0.25765577896392794
iteration : 11615
train acc:  0.8671875
train loss:  0.3150658905506134
train gradient:  0.12403041402080407
iteration : 11616
train acc:  0.8515625
train loss:  0.33591145277023315
train gradient:  0.12630327043148126
iteration : 11617
train acc:  0.8828125
train loss:  0.2641620635986328
train gradient:  0.11333804325724893
iteration : 11618
train acc:  0.859375
train loss:  0.3521871864795685
train gradient:  0.1523977526736651
iteration : 11619
train acc:  0.8359375
train loss:  0.33206477761268616
train gradient:  0.1608743378850826
iteration : 11620
train acc:  0.8515625
train loss:  0.3640446364879608
train gradient:  0.13774824085776344
iteration : 11621
train acc:  0.875
train loss:  0.2989736795425415
train gradient:  0.1197413859890577
iteration : 11622
train acc:  0.875
train loss:  0.30440422892570496
train gradient:  0.13959633972575447
iteration : 11623
train acc:  0.8359375
train loss:  0.3445686101913452
train gradient:  0.146191396667843
iteration : 11624
train acc:  0.8359375
train loss:  0.3652091324329376
train gradient:  0.1355251489368965
iteration : 11625
train acc:  0.890625
train loss:  0.31076765060424805
train gradient:  0.1900207870068859
iteration : 11626
train acc:  0.859375
train loss:  0.2878870666027069
train gradient:  0.19651715532256853
iteration : 11627
train acc:  0.765625
train loss:  0.4724297523498535
train gradient:  0.2659680573529541
iteration : 11628
train acc:  0.8671875
train loss:  0.28012657165527344
train gradient:  0.1114944943923509
iteration : 11629
train acc:  0.875
train loss:  0.3136926293373108
train gradient:  0.11803297421511494
iteration : 11630
train acc:  0.90625
train loss:  0.23245175182819366
train gradient:  0.0878168630908755
iteration : 11631
train acc:  0.875
train loss:  0.29495593905448914
train gradient:  0.12602034703389586
iteration : 11632
train acc:  0.8515625
train loss:  0.3440222144126892
train gradient:  0.14264275420549294
iteration : 11633
train acc:  0.84375
train loss:  0.29054391384124756
train gradient:  0.15555556726039838
iteration : 11634
train acc:  0.859375
train loss:  0.37471020221710205
train gradient:  0.15577868394378652
iteration : 11635
train acc:  0.875
train loss:  0.34111523628234863
train gradient:  0.15316561217766406
iteration : 11636
train acc:  0.8515625
train loss:  0.3582741916179657
train gradient:  0.1315493545345495
iteration : 11637
train acc:  0.8046875
train loss:  0.35062941908836365
train gradient:  0.18051965691827637
iteration : 11638
train acc:  0.8671875
train loss:  0.2660834491252899
train gradient:  0.08242225222599076
iteration : 11639
train acc:  0.90625
train loss:  0.2399313747882843
train gradient:  0.07753747295743424
iteration : 11640
train acc:  0.8828125
train loss:  0.31783878803253174
train gradient:  0.14794863323703897
iteration : 11641
train acc:  0.8359375
train loss:  0.3242049813270569
train gradient:  0.12638879620977211
iteration : 11642
train acc:  0.828125
train loss:  0.34694942831993103
train gradient:  0.13777260049766987
iteration : 11643
train acc:  0.8671875
train loss:  0.2949709892272949
train gradient:  0.13895415887290982
iteration : 11644
train acc:  0.8984375
train loss:  0.3303256630897522
train gradient:  0.24979426480564781
iteration : 11645
train acc:  0.8671875
train loss:  0.30782824754714966
train gradient:  0.15461736404763676
iteration : 11646
train acc:  0.9140625
train loss:  0.28071463108062744
train gradient:  0.13325696574515622
iteration : 11647
train acc:  0.796875
train loss:  0.4404597282409668
train gradient:  0.26787171880677735
iteration : 11648
train acc:  0.8203125
train loss:  0.39028894901275635
train gradient:  0.20885868062069035
iteration : 11649
train acc:  0.8359375
train loss:  0.30529525876045227
train gradient:  0.12252682748595137
iteration : 11650
train acc:  0.890625
train loss:  0.2560963034629822
train gradient:  0.07847208968704052
iteration : 11651
train acc:  0.84375
train loss:  0.3407791256904602
train gradient:  0.12602580589653575
iteration : 11652
train acc:  0.8203125
train loss:  0.3751756250858307
train gradient:  0.18953474476316368
iteration : 11653
train acc:  0.859375
train loss:  0.28992629051208496
train gradient:  0.12171038589409576
iteration : 11654
train acc:  0.8203125
train loss:  0.34430938959121704
train gradient:  0.18051193476697497
iteration : 11655
train acc:  0.828125
train loss:  0.3340453505516052
train gradient:  0.24438024669736896
iteration : 11656
train acc:  0.8671875
train loss:  0.283926784992218
train gradient:  0.11609350285486075
iteration : 11657
train acc:  0.8828125
train loss:  0.3726802468299866
train gradient:  0.22176362985125542
iteration : 11658
train acc:  0.8515625
train loss:  0.3338870108127594
train gradient:  0.12821044690155614
iteration : 11659
train acc:  0.8359375
train loss:  0.36239904165267944
train gradient:  0.16371139819662794
iteration : 11660
train acc:  0.828125
train loss:  0.3260382115840912
train gradient:  0.15388504480642573
iteration : 11661
train acc:  0.875
train loss:  0.27872616052627563
train gradient:  0.11374743733161234
iteration : 11662
train acc:  0.796875
train loss:  0.3787505626678467
train gradient:  0.20407011145693862
iteration : 11663
train acc:  0.90625
train loss:  0.2668359577655792
train gradient:  0.14303899450439295
iteration : 11664
train acc:  0.8828125
train loss:  0.28650516271591187
train gradient:  0.12702123159740694
iteration : 11665
train acc:  0.890625
train loss:  0.2849101424217224
train gradient:  0.13086425487461745
iteration : 11666
train acc:  0.859375
train loss:  0.3210808038711548
train gradient:  0.14860865979958385
iteration : 11667
train acc:  0.8046875
train loss:  0.41103747487068176
train gradient:  0.20715707835043004
iteration : 11668
train acc:  0.8359375
train loss:  0.33775943517684937
train gradient:  0.2709632620946439
iteration : 11669
train acc:  0.8671875
train loss:  0.311581015586853
train gradient:  0.12391141190439968
iteration : 11670
train acc:  0.8125
train loss:  0.41591036319732666
train gradient:  0.19774515304785367
iteration : 11671
train acc:  0.859375
train loss:  0.3502059280872345
train gradient:  0.16833027813723592
iteration : 11672
train acc:  0.84375
train loss:  0.3831503689289093
train gradient:  0.22793180805287777
iteration : 11673
train acc:  0.84375
train loss:  0.38451582193374634
train gradient:  0.2583765346714888
iteration : 11674
train acc:  0.859375
train loss:  0.3134979009628296
train gradient:  0.15633404832742415
iteration : 11675
train acc:  0.8984375
train loss:  0.24532616138458252
train gradient:  0.07656675144620283
iteration : 11676
train acc:  0.8671875
train loss:  0.3002749979496002
train gradient:  0.13807111237549097
iteration : 11677
train acc:  0.921875
train loss:  0.2598344087600708
train gradient:  0.09175216524531417
iteration : 11678
train acc:  0.859375
train loss:  0.27785342931747437
train gradient:  0.1102258313695956
iteration : 11679
train acc:  0.796875
train loss:  0.38683021068573
train gradient:  0.17633891339419916
iteration : 11680
train acc:  0.90625
train loss:  0.26081037521362305
train gradient:  0.10395495931479702
iteration : 11681
train acc:  0.90625
train loss:  0.25591251254081726
train gradient:  0.09924995234958589
iteration : 11682
train acc:  0.859375
train loss:  0.3803774118423462
train gradient:  0.20576555592139037
iteration : 11683
train acc:  0.8828125
train loss:  0.33249107003211975
train gradient:  0.2325460946487656
iteration : 11684
train acc:  0.84375
train loss:  0.35865914821624756
train gradient:  0.13316664750569157
iteration : 11685
train acc:  0.8515625
train loss:  0.371157705783844
train gradient:  0.13137618662342254
iteration : 11686
train acc:  0.890625
train loss:  0.2631300687789917
train gradient:  0.12411844362855347
iteration : 11687
train acc:  0.8984375
train loss:  0.29268649220466614
train gradient:  0.1616368140540217
iteration : 11688
train acc:  0.859375
train loss:  0.4004136323928833
train gradient:  0.21344373902690084
iteration : 11689
train acc:  0.8515625
train loss:  0.3663029074668884
train gradient:  0.18715356174230918
iteration : 11690
train acc:  0.84375
train loss:  0.3002510070800781
train gradient:  0.11261394957246883
iteration : 11691
train acc:  0.828125
train loss:  0.34048688411712646
train gradient:  0.14149321345914095
iteration : 11692
train acc:  0.8828125
train loss:  0.2846361994743347
train gradient:  0.1738994634907185
iteration : 11693
train acc:  0.890625
train loss:  0.28734540939331055
train gradient:  0.15872547037577622
iteration : 11694
train acc:  0.890625
train loss:  0.28917455673217773
train gradient:  0.1057444978734437
iteration : 11695
train acc:  0.875
train loss:  0.3948005139827728
train gradient:  0.20670747663343397
iteration : 11696
train acc:  0.8515625
train loss:  0.311866819858551
train gradient:  0.1610656336591158
iteration : 11697
train acc:  0.890625
train loss:  0.24382126331329346
train gradient:  0.09296643616296212
iteration : 11698
train acc:  0.8359375
train loss:  0.338653564453125
train gradient:  0.14913545860939403
iteration : 11699
train acc:  0.8671875
train loss:  0.32970303297042847
train gradient:  0.13393570648762654
iteration : 11700
train acc:  0.8671875
train loss:  0.32187992334365845
train gradient:  0.1312922875937923
iteration : 11701
train acc:  0.8125
train loss:  0.3314501941204071
train gradient:  0.1825934384622124
iteration : 11702
train acc:  0.8125
train loss:  0.3223458230495453
train gradient:  0.2469330514045036
iteration : 11703
train acc:  0.8828125
train loss:  0.26592469215393066
train gradient:  0.08320507377314813
iteration : 11704
train acc:  0.8671875
train loss:  0.40443408489227295
train gradient:  0.2216040959699178
iteration : 11705
train acc:  0.8515625
train loss:  0.29731255769729614
train gradient:  0.1749867433460776
iteration : 11706
train acc:  0.875
train loss:  0.28656306862831116
train gradient:  0.17228697988869146
iteration : 11707
train acc:  0.875
train loss:  0.307226300239563
train gradient:  0.1347966137220994
iteration : 11708
train acc:  0.8671875
train loss:  0.36363083124160767
train gradient:  0.18903621942470006
iteration : 11709
train acc:  0.8828125
train loss:  0.26764875650405884
train gradient:  0.1003471331863386
iteration : 11710
train acc:  0.84375
train loss:  0.297541081905365
train gradient:  0.08541232127268715
iteration : 11711
train acc:  0.8984375
train loss:  0.2671465277671814
train gradient:  0.08200032652645405
iteration : 11712
train acc:  0.890625
train loss:  0.30676892399787903
train gradient:  0.14515006629068533
iteration : 11713
train acc:  0.8203125
train loss:  0.38267260789871216
train gradient:  0.28030590772490643
iteration : 11714
train acc:  0.84375
train loss:  0.34418416023254395
train gradient:  0.1712205720463007
iteration : 11715
train acc:  0.8515625
train loss:  0.3397085964679718
train gradient:  0.13644200432611278
iteration : 11716
train acc:  0.8359375
train loss:  0.3892360329627991
train gradient:  0.18635490336822658
iteration : 11717
train acc:  0.8984375
train loss:  0.22916992008686066
train gradient:  0.08671874555621381
iteration : 11718
train acc:  0.8359375
train loss:  0.32270723581314087
train gradient:  0.14500371681715585
iteration : 11719
train acc:  0.875
train loss:  0.2645496129989624
train gradient:  0.14535464871122328
iteration : 11720
train acc:  0.8359375
train loss:  0.32242250442504883
train gradient:  0.12334680674620803
iteration : 11721
train acc:  0.8046875
train loss:  0.4032819867134094
train gradient:  0.22174083403579706
iteration : 11722
train acc:  0.8125
train loss:  0.4530743956565857
train gradient:  0.2204155051244971
iteration : 11723
train acc:  0.828125
train loss:  0.35124537348747253
train gradient:  0.16412074123706621
iteration : 11724
train acc:  0.8671875
train loss:  0.3184264302253723
train gradient:  0.22502290394609492
iteration : 11725
train acc:  0.84375
train loss:  0.40795958042144775
train gradient:  0.22197265965919294
iteration : 11726
train acc:  0.8203125
train loss:  0.3536541759967804
train gradient:  0.18196061565388744
iteration : 11727
train acc:  0.8515625
train loss:  0.28098899126052856
train gradient:  0.1026262638478149
iteration : 11728
train acc:  0.859375
train loss:  0.3165861964225769
train gradient:  0.2508942173065523
iteration : 11729
train acc:  0.8359375
train loss:  0.35947185754776
train gradient:  0.1976576101479464
iteration : 11730
train acc:  0.875
train loss:  0.3503081202507019
train gradient:  0.22577092554295908
iteration : 11731
train acc:  0.8125
train loss:  0.34875616431236267
train gradient:  0.22371561677977445
iteration : 11732
train acc:  0.8125
train loss:  0.37161701917648315
train gradient:  0.16901294979485287
iteration : 11733
train acc:  0.859375
train loss:  0.29352518916130066
train gradient:  0.12405516769805473
iteration : 11734
train acc:  0.8984375
train loss:  0.26310333609580994
train gradient:  0.16286067789274616
iteration : 11735
train acc:  0.84375
train loss:  0.3970567584037781
train gradient:  0.33216755556371547
iteration : 11736
train acc:  0.8359375
train loss:  0.3396538496017456
train gradient:  0.12682559803556487
iteration : 11737
train acc:  0.828125
train loss:  0.3755379021167755
train gradient:  0.19065508839523404
iteration : 11738
train acc:  0.8671875
train loss:  0.30270257592201233
train gradient:  0.15778480219759003
iteration : 11739
train acc:  0.859375
train loss:  0.2821829915046692
train gradient:  0.10783843463073477
iteration : 11740
train acc:  0.890625
train loss:  0.32715487480163574
train gradient:  0.13046134281858596
iteration : 11741
train acc:  0.8515625
train loss:  0.32597947120666504
train gradient:  0.1594279229862471
iteration : 11742
train acc:  0.84375
train loss:  0.3555568754673004
train gradient:  0.149333497811974
iteration : 11743
train acc:  0.890625
train loss:  0.2720372676849365
train gradient:  0.0907034664525591
iteration : 11744
train acc:  0.9140625
train loss:  0.2709769606590271
train gradient:  0.1149975713322846
iteration : 11745
train acc:  0.84375
train loss:  0.31561195850372314
train gradient:  0.1461281876102325
iteration : 11746
train acc:  0.84375
train loss:  0.3566626310348511
train gradient:  0.1426220955601981
iteration : 11747
train acc:  0.859375
train loss:  0.3023700714111328
train gradient:  0.1088453916127453
iteration : 11748
train acc:  0.859375
train loss:  0.30053576827049255
train gradient:  0.1260879504868721
iteration : 11749
train acc:  0.9296875
train loss:  0.22876740992069244
train gradient:  0.06497880267393327
iteration : 11750
train acc:  0.859375
train loss:  0.26054733991622925
train gradient:  0.10249180592773284
iteration : 11751
train acc:  0.84375
train loss:  0.36278021335601807
train gradient:  0.20307678505773394
iteration : 11752
train acc:  0.9140625
train loss:  0.23463699221611023
train gradient:  0.08974469412268919
iteration : 11753
train acc:  0.828125
train loss:  0.35113799571990967
train gradient:  0.14739113208451052
iteration : 11754
train acc:  0.890625
train loss:  0.326931357383728
train gradient:  0.1421219035926394
iteration : 11755
train acc:  0.8125
train loss:  0.34680700302124023
train gradient:  0.1332260647352999
iteration : 11756
train acc:  0.8203125
train loss:  0.4200563132762909
train gradient:  0.18484552788707237
iteration : 11757
train acc:  0.8203125
train loss:  0.3829609155654907
train gradient:  0.24707587018831695
iteration : 11758
train acc:  0.84375
train loss:  0.3812705874443054
train gradient:  0.15547675686192566
iteration : 11759
train acc:  0.890625
train loss:  0.2917584180831909
train gradient:  0.09355591355815206
iteration : 11760
train acc:  0.796875
train loss:  0.357850581407547
train gradient:  0.141909145482392
iteration : 11761
train acc:  0.8515625
train loss:  0.34491580724716187
train gradient:  0.12325964473210044
iteration : 11762
train acc:  0.84375
train loss:  0.37220877408981323
train gradient:  0.21286260513230992
iteration : 11763
train acc:  0.90625
train loss:  0.2894442677497864
train gradient:  0.1768536645134513
iteration : 11764
train acc:  0.8671875
train loss:  0.2899531424045563
train gradient:  0.2065224503079715
iteration : 11765
train acc:  0.8203125
train loss:  0.3664470911026001
train gradient:  0.17556126190586024
iteration : 11766
train acc:  0.859375
train loss:  0.3281645178794861
train gradient:  0.22135661954137692
iteration : 11767
train acc:  0.875
train loss:  0.3277548551559448
train gradient:  0.1581143376914244
iteration : 11768
train acc:  0.8671875
train loss:  0.33892709016799927
train gradient:  0.18120112315017858
iteration : 11769
train acc:  0.84375
train loss:  0.3334159851074219
train gradient:  0.18090162995285097
iteration : 11770
train acc:  0.8984375
train loss:  0.2460464984178543
train gradient:  0.10311939621238708
iteration : 11771
train acc:  0.921875
train loss:  0.21554818749427795
train gradient:  0.0830314075030857
iteration : 11772
train acc:  0.9140625
train loss:  0.2383674681186676
train gradient:  0.09396806799586858
iteration : 11773
train acc:  0.8984375
train loss:  0.29960572719573975
train gradient:  0.20302574840041948
iteration : 11774
train acc:  0.8515625
train loss:  0.42741847038269043
train gradient:  0.20360446310256888
iteration : 11775
train acc:  0.8203125
train loss:  0.4006407856941223
train gradient:  0.2878861829154234
iteration : 11776
train acc:  0.8359375
train loss:  0.31654059886932373
train gradient:  0.13904570414027867
iteration : 11777
train acc:  0.8828125
train loss:  0.28534573316574097
train gradient:  0.13733421080366182
iteration : 11778
train acc:  0.8671875
train loss:  0.344496488571167
train gradient:  0.14633279269915908
iteration : 11779
train acc:  0.8515625
train loss:  0.32821956276893616
train gradient:  0.14517229672057763
iteration : 11780
train acc:  0.8984375
train loss:  0.2838650047779083
train gradient:  0.12896454380427141
iteration : 11781
train acc:  0.8828125
train loss:  0.256903737783432
train gradient:  0.13804462967212672
iteration : 11782
train acc:  0.8125
train loss:  0.3950250744819641
train gradient:  0.20049647743327692
iteration : 11783
train acc:  0.875
train loss:  0.33989232778549194
train gradient:  0.20446549317979837
iteration : 11784
train acc:  0.8359375
train loss:  0.3750370740890503
train gradient:  0.14827648634366397
iteration : 11785
train acc:  0.8671875
train loss:  0.3173179626464844
train gradient:  0.12340362934664326
iteration : 11786
train acc:  0.8515625
train loss:  0.36088114976882935
train gradient:  0.16697424086668003
iteration : 11787
train acc:  0.90625
train loss:  0.28658393025398254
train gradient:  0.1378697115680786
iteration : 11788
train acc:  0.859375
train loss:  0.33170050382614136
train gradient:  0.3325022235353506
iteration : 11789
train acc:  0.8671875
train loss:  0.28479331731796265
train gradient:  0.20344666661334676
iteration : 11790
train acc:  0.9375
train loss:  0.21188753843307495
train gradient:  0.09061467961486326
iteration : 11791
train acc:  0.875
train loss:  0.2899082899093628
train gradient:  0.1296936315185378
iteration : 11792
train acc:  0.8359375
train loss:  0.359638512134552
train gradient:  0.15535457473580228
iteration : 11793
train acc:  0.8515625
train loss:  0.30664414167404175
train gradient:  0.16484335452800192
iteration : 11794
train acc:  0.84375
train loss:  0.3022984266281128
train gradient:  0.18236356956641805
iteration : 11795
train acc:  0.8203125
train loss:  0.3273899555206299
train gradient:  0.1643252270886161
iteration : 11796
train acc:  0.8125
train loss:  0.36349421739578247
train gradient:  0.154402032036063
iteration : 11797
train acc:  0.8828125
train loss:  0.253617525100708
train gradient:  0.12023124890338859
iteration : 11798
train acc:  0.890625
train loss:  0.25674116611480713
train gradient:  0.14977788699295475
iteration : 11799
train acc:  0.9140625
train loss:  0.2808392345905304
train gradient:  0.11666694761778965
iteration : 11800
train acc:  0.84375
train loss:  0.318702757358551
train gradient:  0.16334105313326577
iteration : 11801
train acc:  0.8515625
train loss:  0.32911133766174316
train gradient:  0.16781649017923722
iteration : 11802
train acc:  0.875
train loss:  0.33195802569389343
train gradient:  0.1968524785878536
iteration : 11803
train acc:  0.90625
train loss:  0.25472646951675415
train gradient:  0.12953579387061892
iteration : 11804
train acc:  0.90625
train loss:  0.2931002676486969
train gradient:  0.1312041941331677
iteration : 11805
train acc:  0.8515625
train loss:  0.29599642753601074
train gradient:  0.11908084011318333
iteration : 11806
train acc:  0.90625
train loss:  0.2750934958457947
train gradient:  0.1375801442409924
iteration : 11807
train acc:  0.8671875
train loss:  0.28709685802459717
train gradient:  0.1819803889322883
iteration : 11808
train acc:  0.890625
train loss:  0.30156582593917847
train gradient:  0.10985774640394289
iteration : 11809
train acc:  0.8828125
train loss:  0.29506218433380127
train gradient:  0.09390602476678604
iteration : 11810
train acc:  0.8828125
train loss:  0.2892560362815857
train gradient:  0.13007620221120766
iteration : 11811
train acc:  0.8203125
train loss:  0.42927590012550354
train gradient:  0.2947053384517856
iteration : 11812
train acc:  0.9375
train loss:  0.20680418610572815
train gradient:  0.12092965692269861
iteration : 11813
train acc:  0.84375
train loss:  0.35566943883895874
train gradient:  0.147814054561132
iteration : 11814
train acc:  0.8203125
train loss:  0.3734477758407593
train gradient:  0.186810174620895
iteration : 11815
train acc:  0.921875
train loss:  0.2278628796339035
train gradient:  0.1087888337577911
iteration : 11816
train acc:  0.8203125
train loss:  0.38936111330986023
train gradient:  0.21789620207008012
iteration : 11817
train acc:  0.84375
train loss:  0.3563805818557739
train gradient:  0.21008091438507823
iteration : 11818
train acc:  0.8046875
train loss:  0.37259018421173096
train gradient:  0.2434537765905721
iteration : 11819
train acc:  0.8046875
train loss:  0.37278199195861816
train gradient:  0.19724766861607157
iteration : 11820
train acc:  0.8828125
train loss:  0.2983242869377136
train gradient:  0.15039407555043935
iteration : 11821
train acc:  0.9375
train loss:  0.18899108469486237
train gradient:  0.08550942461102351
iteration : 11822
train acc:  0.859375
train loss:  0.2582482099533081
train gradient:  0.11254219984968154
iteration : 11823
train acc:  0.8203125
train loss:  0.3493236303329468
train gradient:  0.1534408548064528
iteration : 11824
train acc:  0.890625
train loss:  0.283639520406723
train gradient:  0.11572794477270856
iteration : 11825
train acc:  0.8671875
train loss:  0.34421128034591675
train gradient:  0.14889644778259603
iteration : 11826
train acc:  0.890625
train loss:  0.2907288372516632
train gradient:  0.11763610453743925
iteration : 11827
train acc:  0.8359375
train loss:  0.3550098240375519
train gradient:  0.1455255173051016
iteration : 11828
train acc:  0.8828125
train loss:  0.2825486361980438
train gradient:  0.1583226806421562
iteration : 11829
train acc:  0.7890625
train loss:  0.37846624851226807
train gradient:  0.19678618294424505
iteration : 11830
train acc:  0.8828125
train loss:  0.32448574900627136
train gradient:  0.18794581811828637
iteration : 11831
train acc:  0.8203125
train loss:  0.3415941298007965
train gradient:  0.13567174667971185
iteration : 11832
train acc:  0.84375
train loss:  0.3239142894744873
train gradient:  0.1650673388710383
iteration : 11833
train acc:  0.90625
train loss:  0.2608617842197418
train gradient:  0.11870704416872262
iteration : 11834
train acc:  0.859375
train loss:  0.3664554953575134
train gradient:  0.15958809768046855
iteration : 11835
train acc:  0.859375
train loss:  0.30582597851753235
train gradient:  0.15308342948878823
iteration : 11836
train acc:  0.8828125
train loss:  0.2822677493095398
train gradient:  0.13449460735103883
iteration : 11837
train acc:  0.8125
train loss:  0.35572928190231323
train gradient:  0.1689544746576214
iteration : 11838
train acc:  0.875
train loss:  0.3184564411640167
train gradient:  0.17555046673530014
iteration : 11839
train acc:  0.875
train loss:  0.32338374853134155
train gradient:  0.18008463277230335
iteration : 11840
train acc:  0.78125
train loss:  0.4175593852996826
train gradient:  0.258970820461221
iteration : 11841
train acc:  0.84375
train loss:  0.3228127360343933
train gradient:  0.1372703686181677
iteration : 11842
train acc:  0.8984375
train loss:  0.24846875667572021
train gradient:  0.10428870639247743
iteration : 11843
train acc:  0.8671875
train loss:  0.2847006916999817
train gradient:  0.179504379784656
iteration : 11844
train acc:  0.8671875
train loss:  0.26761868596076965
train gradient:  0.1791404460415469
iteration : 11845
train acc:  0.875
train loss:  0.2843295931816101
train gradient:  0.25887402315471697
iteration : 11846
train acc:  0.8359375
train loss:  0.35553330183029175
train gradient:  0.19661157363709109
iteration : 11847
train acc:  0.828125
train loss:  0.3498826026916504
train gradient:  0.17718314535786422
iteration : 11848
train acc:  0.8671875
train loss:  0.3014768958091736
train gradient:  0.17907115992901018
iteration : 11849
train acc:  0.8203125
train loss:  0.3799148201942444
train gradient:  0.2173209018311733
iteration : 11850
train acc:  0.859375
train loss:  0.3325785994529724
train gradient:  0.13806759128138912
iteration : 11851
train acc:  0.8671875
train loss:  0.36172550916671753
train gradient:  0.17207505846991467
iteration : 11852
train acc:  0.7890625
train loss:  0.4765195846557617
train gradient:  0.37276044519063406
iteration : 11853
train acc:  0.9140625
train loss:  0.2609065771102905
train gradient:  0.1386969747570185
iteration : 11854
train acc:  0.828125
train loss:  0.38999679684638977
train gradient:  0.1685099817802182
iteration : 11855
train acc:  0.859375
train loss:  0.31714773178100586
train gradient:  0.14879096880396867
iteration : 11856
train acc:  0.8828125
train loss:  0.276919960975647
train gradient:  0.17931091427839718
iteration : 11857
train acc:  0.8046875
train loss:  0.40543603897094727
train gradient:  0.19368746705052853
iteration : 11858
train acc:  0.921875
train loss:  0.23053579032421112
train gradient:  0.11240421669484552
iteration : 11859
train acc:  0.8828125
train loss:  0.283824622631073
train gradient:  0.10202473242932131
iteration : 11860
train acc:  0.8984375
train loss:  0.3007400631904602
train gradient:  0.128645373845228
iteration : 11861
train acc:  0.890625
train loss:  0.2766537666320801
train gradient:  0.11290909681827756
iteration : 11862
train acc:  0.8671875
train loss:  0.30290502309799194
train gradient:  0.13484582036443882
iteration : 11863
train acc:  0.90625
train loss:  0.25179606676101685
train gradient:  0.10266403325862684
iteration : 11864
train acc:  0.8359375
train loss:  0.42512190341949463
train gradient:  0.22382692601738077
iteration : 11865
train acc:  0.859375
train loss:  0.27591103315353394
train gradient:  0.12562253354330383
iteration : 11866
train acc:  0.890625
train loss:  0.2994292378425598
train gradient:  0.11470151046291217
iteration : 11867
train acc:  0.859375
train loss:  0.2988766133785248
train gradient:  0.11924404109494911
iteration : 11868
train acc:  0.8984375
train loss:  0.32134050130844116
train gradient:  0.2297135410383383
iteration : 11869
train acc:  0.8828125
train loss:  0.2764531970024109
train gradient:  0.10811905891797437
iteration : 11870
train acc:  0.8515625
train loss:  0.3352489471435547
train gradient:  0.1765392784390512
iteration : 11871
train acc:  0.8203125
train loss:  0.3545645475387573
train gradient:  0.15261459999260304
iteration : 11872
train acc:  0.84375
train loss:  0.34484314918518066
train gradient:  0.2143422782945015
iteration : 11873
train acc:  0.8046875
train loss:  0.36532264947891235
train gradient:  0.17953456873062573
iteration : 11874
train acc:  0.8359375
train loss:  0.36025577783584595
train gradient:  0.20155410920031125
iteration : 11875
train acc:  0.84375
train loss:  0.34287992119789124
train gradient:  0.15059892474615405
iteration : 11876
train acc:  0.8828125
train loss:  0.34301841259002686
train gradient:  0.19565765898656406
iteration : 11877
train acc:  0.9140625
train loss:  0.25298047065734863
train gradient:  0.10357129595944035
iteration : 11878
train acc:  0.8359375
train loss:  0.31493133306503296
train gradient:  0.1425530759553295
iteration : 11879
train acc:  0.90625
train loss:  0.30587995052337646
train gradient:  0.1554682629440926
iteration : 11880
train acc:  0.84375
train loss:  0.3519383668899536
train gradient:  0.14753119056266645
iteration : 11881
train acc:  0.828125
train loss:  0.39110052585601807
train gradient:  0.19188525239445742
iteration : 11882
train acc:  0.859375
train loss:  0.3019202947616577
train gradient:  0.11973734378201129
iteration : 11883
train acc:  0.859375
train loss:  0.3172905445098877
train gradient:  0.1404350360951968
iteration : 11884
train acc:  0.8515625
train loss:  0.3910101652145386
train gradient:  0.227714864712184
iteration : 11885
train acc:  0.8984375
train loss:  0.24192145466804504
train gradient:  0.14964438413299547
iteration : 11886
train acc:  0.8125
train loss:  0.4835418164730072
train gradient:  0.37059024033816806
iteration : 11887
train acc:  0.875
train loss:  0.3348594009876251
train gradient:  0.38557024528169453
iteration : 11888
train acc:  0.8203125
train loss:  0.41096919775009155
train gradient:  0.2471504955065876
iteration : 11889
train acc:  0.875
train loss:  0.3346782922744751
train gradient:  0.1294803768332256
iteration : 11890
train acc:  0.84375
train loss:  0.34088385105133057
train gradient:  0.12828471161151161
iteration : 11891
train acc:  0.875
train loss:  0.27564775943756104
train gradient:  0.0910011808558552
iteration : 11892
train acc:  0.8515625
train loss:  0.33761706948280334
train gradient:  0.16182001274610558
iteration : 11893
train acc:  0.8515625
train loss:  0.3514842092990875
train gradient:  0.1364544056427521
iteration : 11894
train acc:  0.8671875
train loss:  0.31428974866867065
train gradient:  0.14311588046582
iteration : 11895
train acc:  0.8515625
train loss:  0.4222487807273865
train gradient:  0.2547669797421282
iteration : 11896
train acc:  0.9375
train loss:  0.23133555054664612
train gradient:  0.0921036605422015
iteration : 11897
train acc:  0.859375
train loss:  0.33844923973083496
train gradient:  0.18664368738272005
iteration : 11898
train acc:  0.859375
train loss:  0.3079926669597626
train gradient:  0.11912767507587324
iteration : 11899
train acc:  0.828125
train loss:  0.3385179042816162
train gradient:  0.1992856268084226
iteration : 11900
train acc:  0.859375
train loss:  0.3174307644367218
train gradient:  0.1782580260808121
iteration : 11901
train acc:  0.8515625
train loss:  0.3691336214542389
train gradient:  0.21421457498970742
iteration : 11902
train acc:  0.8359375
train loss:  0.3201739490032196
train gradient:  0.13146406802408392
iteration : 11903
train acc:  0.7734375
train loss:  0.48038870096206665
train gradient:  0.24472172952450594
iteration : 11904
train acc:  0.8984375
train loss:  0.2726950943470001
train gradient:  0.09425858141387677
iteration : 11905
train acc:  0.8984375
train loss:  0.26518213748931885
train gradient:  0.10929768062634558
iteration : 11906
train acc:  0.84375
train loss:  0.3798488676548004
train gradient:  0.19798928623080375
iteration : 11907
train acc:  0.8515625
train loss:  0.32201433181762695
train gradient:  0.14102236478009744
iteration : 11908
train acc:  0.8671875
train loss:  0.30138957500457764
train gradient:  0.11968681452547528
iteration : 11909
train acc:  0.8359375
train loss:  0.33770057559013367
train gradient:  0.14338775432096548
iteration : 11910
train acc:  0.8125
train loss:  0.399924099445343
train gradient:  0.1797077496660869
iteration : 11911
train acc:  0.8046875
train loss:  0.37349385023117065
train gradient:  0.1755779783030571
iteration : 11912
train acc:  0.921875
train loss:  0.2514241933822632
train gradient:  0.10667507368266667
iteration : 11913
train acc:  0.8515625
train loss:  0.355857253074646
train gradient:  0.16772610412493827
iteration : 11914
train acc:  0.84375
train loss:  0.3482537269592285
train gradient:  0.23183073480472388
iteration : 11915
train acc:  0.890625
train loss:  0.29890432953834534
train gradient:  0.08318116320569664
iteration : 11916
train acc:  0.8515625
train loss:  0.29850971698760986
train gradient:  0.11737127779530754
iteration : 11917
train acc:  0.8671875
train loss:  0.30873823165893555
train gradient:  0.12529566960815333
iteration : 11918
train acc:  0.8359375
train loss:  0.3592405319213867
train gradient:  0.1537169856270088
iteration : 11919
train acc:  0.859375
train loss:  0.3110085129737854
train gradient:  0.13766299471428983
iteration : 11920
train acc:  0.859375
train loss:  0.29079312086105347
train gradient:  0.2322055760031767
iteration : 11921
train acc:  0.859375
train loss:  0.3675343692302704
train gradient:  0.18408287123135786
iteration : 11922
train acc:  0.8828125
train loss:  0.26964545249938965
train gradient:  0.11548848426280395
iteration : 11923
train acc:  0.8515625
train loss:  0.3390197455883026
train gradient:  0.1982250436051266
iteration : 11924
train acc:  0.8828125
train loss:  0.3325546979904175
train gradient:  0.19229831422697394
iteration : 11925
train acc:  0.8671875
train loss:  0.3138554096221924
train gradient:  0.15075535274337526
iteration : 11926
train acc:  0.875
train loss:  0.2983814775943756
train gradient:  0.09459705801129027
iteration : 11927
train acc:  0.8984375
train loss:  0.2798738181591034
train gradient:  0.13027305052196725
iteration : 11928
train acc:  0.8359375
train loss:  0.3553260862827301
train gradient:  0.14567595325219862
iteration : 11929
train acc:  0.7890625
train loss:  0.3793979287147522
train gradient:  0.18199433120555583
iteration : 11930
train acc:  0.8671875
train loss:  0.2674955129623413
train gradient:  0.10412391129631611
iteration : 11931
train acc:  0.8828125
train loss:  0.2609027624130249
train gradient:  0.09165012356986116
iteration : 11932
train acc:  0.8828125
train loss:  0.34222158789634705
train gradient:  0.15639097225652857
iteration : 11933
train acc:  0.890625
train loss:  0.28011947870254517
train gradient:  0.11185910451014952
iteration : 11934
train acc:  0.890625
train loss:  0.2473331093788147
train gradient:  0.09762987776443112
iteration : 11935
train acc:  0.9375
train loss:  0.22014552354812622
train gradient:  0.07913059829810673
iteration : 11936
train acc:  0.84375
train loss:  0.36374518275260925
train gradient:  0.1487923877461395
iteration : 11937
train acc:  0.8984375
train loss:  0.2512913644313812
train gradient:  0.08418030540376845
iteration : 11938
train acc:  0.890625
train loss:  0.2946876585483551
train gradient:  0.11634742269394208
iteration : 11939
train acc:  0.8515625
train loss:  0.3556220829486847
train gradient:  0.1519222851928995
iteration : 11940
train acc:  0.875
train loss:  0.3036809265613556
train gradient:  0.13423500634522958
iteration : 11941
train acc:  0.875
train loss:  0.2812557518482208
train gradient:  0.1455511909211286
iteration : 11942
train acc:  0.875
train loss:  0.29080963134765625
train gradient:  0.13217572570948039
iteration : 11943
train acc:  0.8125
train loss:  0.438189297914505
train gradient:  0.22565185376357155
iteration : 11944
train acc:  0.8203125
train loss:  0.33324557542800903
train gradient:  0.1649436684714124
iteration : 11945
train acc:  0.828125
train loss:  0.35517704486846924
train gradient:  0.22394732594674427
iteration : 11946
train acc:  0.84375
train loss:  0.35525375604629517
train gradient:  0.17452843746093136
iteration : 11947
train acc:  0.8828125
train loss:  0.29728567600250244
train gradient:  0.17848887374599937
iteration : 11948
train acc:  0.8359375
train loss:  0.372662752866745
train gradient:  0.2814012647984705
iteration : 11949
train acc:  0.875
train loss:  0.29859447479248047
train gradient:  0.21542398865128165
iteration : 11950
train acc:  0.8671875
train loss:  0.35290825366973877
train gradient:  0.17961302591632666
iteration : 11951
train acc:  0.8828125
train loss:  0.27622631192207336
train gradient:  0.11923035733615196
iteration : 11952
train acc:  0.8515625
train loss:  0.34675759077072144
train gradient:  0.20430650700606
iteration : 11953
train acc:  0.8828125
train loss:  0.3345772325992584
train gradient:  0.20167707621873437
iteration : 11954
train acc:  0.84375
train loss:  0.3250199258327484
train gradient:  0.18980982439149657
iteration : 11955
train acc:  0.875
train loss:  0.29878875613212585
train gradient:  0.1401163302629652
iteration : 11956
train acc:  0.8515625
train loss:  0.341792494058609
train gradient:  0.13394149366612806
iteration : 11957
train acc:  0.84375
train loss:  0.30718642473220825
train gradient:  0.13083935380858752
iteration : 11958
train acc:  0.8203125
train loss:  0.3566572070121765
train gradient:  0.19487982249941704
iteration : 11959
train acc:  0.8828125
train loss:  0.30996716022491455
train gradient:  0.18962135214213233
iteration : 11960
train acc:  0.8359375
train loss:  0.3440015912055969
train gradient:  0.23687278319292346
iteration : 11961
train acc:  0.8828125
train loss:  0.30425652861595154
train gradient:  0.1775962029092278
iteration : 11962
train acc:  0.8671875
train loss:  0.2865375280380249
train gradient:  0.15741485865578714
iteration : 11963
train acc:  0.890625
train loss:  0.25266531109809875
train gradient:  0.09843410690481896
iteration : 11964
train acc:  0.859375
train loss:  0.3545382618904114
train gradient:  0.15300885213813267
iteration : 11965
train acc:  0.890625
train loss:  0.2821213901042938
train gradient:  0.1298983777916855
iteration : 11966
train acc:  0.8515625
train loss:  0.3339279592037201
train gradient:  0.1385037225650022
iteration : 11967
train acc:  0.8125
train loss:  0.3610592782497406
train gradient:  0.14196950133334174
iteration : 11968
train acc:  0.8515625
train loss:  0.3191833794116974
train gradient:  0.13395044994169597
iteration : 11969
train acc:  0.8203125
train loss:  0.37653499841690063
train gradient:  0.1871176513349375
iteration : 11970
train acc:  0.859375
train loss:  0.26614007353782654
train gradient:  0.22418588407718498
iteration : 11971
train acc:  0.8671875
train loss:  0.36289554834365845
train gradient:  0.16849769867322878
iteration : 11972
train acc:  0.84375
train loss:  0.31702786684036255
train gradient:  0.34941319650999464
iteration : 11973
train acc:  0.921875
train loss:  0.21299707889556885
train gradient:  0.08974302346868765
iteration : 11974
train acc:  0.921875
train loss:  0.2297390252351761
train gradient:  0.09607099852903243
iteration : 11975
train acc:  0.8046875
train loss:  0.36154142022132874
train gradient:  0.144001128346227
iteration : 11976
train acc:  0.8515625
train loss:  0.3242481052875519
train gradient:  0.19231696102623674
iteration : 11977
train acc:  0.9296875
train loss:  0.21632404625415802
train gradient:  0.08884619776071587
iteration : 11978
train acc:  0.8671875
train loss:  0.29476866126060486
train gradient:  0.15051502964191033
iteration : 11979
train acc:  0.8359375
train loss:  0.3945876955986023
train gradient:  0.22417198309379877
iteration : 11980
train acc:  0.8515625
train loss:  0.34995222091674805
train gradient:  0.18289871048997774
iteration : 11981
train acc:  0.859375
train loss:  0.2773665189743042
train gradient:  0.17172368360399123
iteration : 11982
train acc:  0.8359375
train loss:  0.3417673110961914
train gradient:  0.1652458963083285
iteration : 11983
train acc:  0.8828125
train loss:  0.2925305962562561
train gradient:  0.16983864420153935
iteration : 11984
train acc:  0.921875
train loss:  0.20324844121932983
train gradient:  0.09731096328635189
iteration : 11985
train acc:  0.8203125
train loss:  0.38015037775039673
train gradient:  0.1893422893420723
iteration : 11986
train acc:  0.859375
train loss:  0.3313242495059967
train gradient:  0.164341111579896
iteration : 11987
train acc:  0.8125
train loss:  0.4167128801345825
train gradient:  0.27532189744513097
iteration : 11988
train acc:  0.859375
train loss:  0.3767513334751129
train gradient:  0.23233613410996773
iteration : 11989
train acc:  0.8046875
train loss:  0.4739982485771179
train gradient:  0.34576079927112313
iteration : 11990
train acc:  0.8515625
train loss:  0.342243492603302
train gradient:  0.1330933708726138
iteration : 11991
train acc:  0.8828125
train loss:  0.2972452640533447
train gradient:  0.11173546624790899
iteration : 11992
train acc:  0.859375
train loss:  0.3278968930244446
train gradient:  0.13254754605186672
iteration : 11993
train acc:  0.8515625
train loss:  0.3558368682861328
train gradient:  0.17034703800748785
iteration : 11994
train acc:  0.8671875
train loss:  0.2525445222854614
train gradient:  0.12495856210255828
iteration : 11995
train acc:  0.828125
train loss:  0.3413711190223694
train gradient:  0.15951317721575906
iteration : 11996
train acc:  0.875
train loss:  0.25357362627983093
train gradient:  0.1107212466847982
iteration : 11997
train acc:  0.9140625
train loss:  0.23190611600875854
train gradient:  0.1549375023210624
iteration : 11998
train acc:  0.859375
train loss:  0.37016820907592773
train gradient:  0.30445963352486416
iteration : 11999
train acc:  0.8828125
train loss:  0.30936574935913086
train gradient:  0.1998137004486899
iteration : 12000
train acc:  0.828125
train loss:  0.3839154839515686
train gradient:  0.17269806900947415
iteration : 12001
train acc:  0.8671875
train loss:  0.26878535747528076
train gradient:  0.11788021445645396
iteration : 12002
train acc:  0.8671875
train loss:  0.27166107296943665
train gradient:  0.141775176273576
iteration : 12003
train acc:  0.8515625
train loss:  0.32149162888526917
train gradient:  0.11601933540425721
iteration : 12004
train acc:  0.890625
train loss:  0.2577507197856903
train gradient:  0.10617928787173642
iteration : 12005
train acc:  0.8984375
train loss:  0.28791940212249756
train gradient:  0.13676764535941938
iteration : 12006
train acc:  0.875
train loss:  0.2967098355293274
train gradient:  0.18818777091474131
iteration : 12007
train acc:  0.8671875
train loss:  0.34203147888183594
train gradient:  0.21430835272270982
iteration : 12008
train acc:  0.8046875
train loss:  0.31221598386764526
train gradient:  0.20045360550815675
iteration : 12009
train acc:  0.859375
train loss:  0.3108406066894531
train gradient:  0.1494656718190535
iteration : 12010
train acc:  0.84375
train loss:  0.38597896695137024
train gradient:  0.20760894031812832
iteration : 12011
train acc:  0.8671875
train loss:  0.3041292428970337
train gradient:  0.18561031283046686
iteration : 12012
train acc:  0.84375
train loss:  0.3443624973297119
train gradient:  0.2111758595569741
iteration : 12013
train acc:  0.875
train loss:  0.3342614769935608
train gradient:  0.1770218096217872
iteration : 12014
train acc:  0.8984375
train loss:  0.27342885732650757
train gradient:  0.1601153615116358
iteration : 12015
train acc:  0.859375
train loss:  0.33011260628700256
train gradient:  0.14577554638234946
iteration : 12016
train acc:  0.859375
train loss:  0.38472816348075867
train gradient:  0.17108724538578443
iteration : 12017
train acc:  0.9375
train loss:  0.20971077680587769
train gradient:  0.07179013805612411
iteration : 12018
train acc:  0.859375
train loss:  0.30479198694229126
train gradient:  0.14238163063754372
iteration : 12019
train acc:  0.84375
train loss:  0.38094472885131836
train gradient:  0.22222010426659905
iteration : 12020
train acc:  0.8203125
train loss:  0.3475969433784485
train gradient:  0.20537381063815272
iteration : 12021
train acc:  0.8203125
train loss:  0.35655781626701355
train gradient:  0.21731218835357552
iteration : 12022
train acc:  0.84375
train loss:  0.3541524112224579
train gradient:  0.1795715676598323
iteration : 12023
train acc:  0.859375
train loss:  0.29791074991226196
train gradient:  0.15116868802684486
iteration : 12024
train acc:  0.8828125
train loss:  0.3447118401527405
train gradient:  0.12647563588040364
iteration : 12025
train acc:  0.890625
train loss:  0.31126531958580017
train gradient:  0.18414394943594248
iteration : 12026
train acc:  0.8984375
train loss:  0.25350984930992126
train gradient:  0.13004294999883342
iteration : 12027
train acc:  0.859375
train loss:  0.3206350803375244
train gradient:  0.1781123802275102
iteration : 12028
train acc:  0.8125
train loss:  0.3622787594795227
train gradient:  0.17707387870170696
iteration : 12029
train acc:  0.8515625
train loss:  0.3318657875061035
train gradient:  0.16619317469421335
iteration : 12030
train acc:  0.8671875
train loss:  0.31478220224380493
train gradient:  0.18803783191092777
iteration : 12031
train acc:  0.8828125
train loss:  0.3157767355442047
train gradient:  0.17110572312588895
iteration : 12032
train acc:  0.890625
train loss:  0.3281708359718323
train gradient:  0.20818759005059428
iteration : 12033
train acc:  0.8515625
train loss:  0.3771626055240631
train gradient:  0.26654227388659857
iteration : 12034
train acc:  0.8203125
train loss:  0.32053181529045105
train gradient:  0.1385710238714506
iteration : 12035
train acc:  0.890625
train loss:  0.29178017377853394
train gradient:  0.09812149493876755
iteration : 12036
train acc:  0.8671875
train loss:  0.2992074489593506
train gradient:  0.11837971898822398
iteration : 12037
train acc:  0.828125
train loss:  0.40408504009246826
train gradient:  0.20433768019551124
iteration : 12038
train acc:  0.84375
train loss:  0.3063098192214966
train gradient:  0.17144820830375485
iteration : 12039
train acc:  0.8828125
train loss:  0.34192976355552673
train gradient:  0.1589935784883334
iteration : 12040
train acc:  0.8359375
train loss:  0.34537190198898315
train gradient:  0.20351601944075914
iteration : 12041
train acc:  0.8671875
train loss:  0.3195011615753174
train gradient:  0.1515647818630555
iteration : 12042
train acc:  0.875
train loss:  0.34152501821517944
train gradient:  0.15855545342914523
iteration : 12043
train acc:  0.9140625
train loss:  0.3055560290813446
train gradient:  0.18373525168436255
iteration : 12044
train acc:  0.890625
train loss:  0.29741719365119934
train gradient:  0.15386127858749427
iteration : 12045
train acc:  0.890625
train loss:  0.29569756984710693
train gradient:  0.14219305514479547
iteration : 12046
train acc:  0.8515625
train loss:  0.32931241393089294
train gradient:  0.2002487977018114
iteration : 12047
train acc:  0.84375
train loss:  0.28425270318984985
train gradient:  0.13353919326759828
iteration : 12048
train acc:  0.890625
train loss:  0.27222543954849243
train gradient:  0.1375434512285542
iteration : 12049
train acc:  0.8359375
train loss:  0.3472685217857361
train gradient:  0.11495022960152242
iteration : 12050
train acc:  0.8359375
train loss:  0.3986120820045471
train gradient:  0.172361122526964
iteration : 12051
train acc:  0.859375
train loss:  0.35150736570358276
train gradient:  0.15736265087307316
iteration : 12052
train acc:  0.8046875
train loss:  0.3768245577812195
train gradient:  0.18500408623987166
iteration : 12053
train acc:  0.8203125
train loss:  0.32127612829208374
train gradient:  0.14011556380329337
iteration : 12054
train acc:  0.8515625
train loss:  0.36555206775665283
train gradient:  0.17957328030426956
iteration : 12055
train acc:  0.8671875
train loss:  0.3638116121292114
train gradient:  0.20485446576206368
iteration : 12056
train acc:  0.84375
train loss:  0.34910351037979126
train gradient:  0.1377736160985008
iteration : 12057
train acc:  0.859375
train loss:  0.33027708530426025
train gradient:  0.10296960229482092
iteration : 12058
train acc:  0.8671875
train loss:  0.25460126996040344
train gradient:  0.13990064464952007
iteration : 12059
train acc:  0.875
train loss:  0.308139443397522
train gradient:  0.22526098403757966
iteration : 12060
train acc:  0.8515625
train loss:  0.3418770134449005
train gradient:  0.15028879098456888
iteration : 12061
train acc:  0.875
train loss:  0.3168126940727234
train gradient:  0.11080525563460877
iteration : 12062
train acc:  0.8671875
train loss:  0.3482685685157776
train gradient:  0.251849059272505
iteration : 12063
train acc:  0.8359375
train loss:  0.3165706396102905
train gradient:  0.16368308533363302
iteration : 12064
train acc:  0.859375
train loss:  0.3329150080680847
train gradient:  0.13894849982132984
iteration : 12065
train acc:  0.890625
train loss:  0.36770689487457275
train gradient:  0.19962808697835532
iteration : 12066
train acc:  0.859375
train loss:  0.33401334285736084
train gradient:  0.14798010572810924
iteration : 12067
train acc:  0.8125
train loss:  0.41343364119529724
train gradient:  0.27598176825440174
iteration : 12068
train acc:  0.8203125
train loss:  0.3533581495285034
train gradient:  0.14674286176089418
iteration : 12069
train acc:  0.8671875
train loss:  0.3245593309402466
train gradient:  0.13761007536130376
iteration : 12070
train acc:  0.8671875
train loss:  0.26176679134368896
train gradient:  0.10172433015557791
iteration : 12071
train acc:  0.8359375
train loss:  0.4398139715194702
train gradient:  0.2614990437878786
iteration : 12072
train acc:  0.859375
train loss:  0.35675811767578125
train gradient:  0.1645169163853903
iteration : 12073
train acc:  0.890625
train loss:  0.31723344326019287
train gradient:  0.13177586478811248
iteration : 12074
train acc:  0.828125
train loss:  0.4077719449996948
train gradient:  0.2373423647395812
iteration : 12075
train acc:  0.8203125
train loss:  0.385873019695282
train gradient:  0.22001271694881486
iteration : 12076
train acc:  0.890625
train loss:  0.24090561270713806
train gradient:  0.14211126642253974
iteration : 12077
train acc:  0.8671875
train loss:  0.335636705160141
train gradient:  0.14110020806471696
iteration : 12078
train acc:  0.875
train loss:  0.33975139260292053
train gradient:  0.1560415173773239
iteration : 12079
train acc:  0.90625
train loss:  0.2545836567878723
train gradient:  0.09430335921420352
iteration : 12080
train acc:  0.84375
train loss:  0.33602628111839294
train gradient:  0.18241944008717673
iteration : 12081
train acc:  0.8359375
train loss:  0.3877459168434143
train gradient:  0.2086726513801646
iteration : 12082
train acc:  0.859375
train loss:  0.3266104459762573
train gradient:  0.1371587515800149
iteration : 12083
train acc:  0.9296875
train loss:  0.28639304637908936
train gradient:  0.10812670067204719
iteration : 12084
train acc:  0.90625
train loss:  0.2753041088581085
train gradient:  0.10707722438987441
iteration : 12085
train acc:  0.84375
train loss:  0.2930561304092407
train gradient:  0.19088080332285573
iteration : 12086
train acc:  0.8671875
train loss:  0.3729705512523651
train gradient:  0.3112987384577267
iteration : 12087
train acc:  0.890625
train loss:  0.28182291984558105
train gradient:  0.11776435755373657
iteration : 12088
train acc:  0.890625
train loss:  0.27451372146606445
train gradient:  0.13179968735968628
iteration : 12089
train acc:  0.890625
train loss:  0.28818219900131226
train gradient:  0.12460235620388208
iteration : 12090
train acc:  0.828125
train loss:  0.3413691520690918
train gradient:  0.1487836432170319
iteration : 12091
train acc:  0.875
train loss:  0.3042703866958618
train gradient:  0.13587754364720628
iteration : 12092
train acc:  0.78125
train loss:  0.38708168268203735
train gradient:  0.2227925691100915
iteration : 12093
train acc:  0.875
train loss:  0.330461323261261
train gradient:  0.12006503842951731
iteration : 12094
train acc:  0.8671875
train loss:  0.2939639687538147
train gradient:  0.10010978922685082
iteration : 12095
train acc:  0.8515625
train loss:  0.349079966545105
train gradient:  0.2519903963210268
iteration : 12096
train acc:  0.828125
train loss:  0.3686977028846741
train gradient:  0.16482360684951658
iteration : 12097
train acc:  0.8984375
train loss:  0.2287362515926361
train gradient:  0.14398158867807637
iteration : 12098
train acc:  0.8203125
train loss:  0.37306487560272217
train gradient:  0.20762842388484587
iteration : 12099
train acc:  0.859375
train loss:  0.3130483627319336
train gradient:  0.12454872344716393
iteration : 12100
train acc:  0.8671875
train loss:  0.3128959834575653
train gradient:  0.14335639173455522
iteration : 12101
train acc:  0.828125
train loss:  0.4331580102443695
train gradient:  0.2806575590564538
iteration : 12102
train acc:  0.8671875
train loss:  0.3063550293445587
train gradient:  0.11139662626258462
iteration : 12103
train acc:  0.875
train loss:  0.3287235498428345
train gradient:  0.10793683086352988
iteration : 12104
train acc:  0.859375
train loss:  0.353171706199646
train gradient:  0.20838963943266575
iteration : 12105
train acc:  0.921875
train loss:  0.21415135264396667
train gradient:  0.15095253285734667
iteration : 12106
train acc:  0.84375
train loss:  0.32338401675224304
train gradient:  0.1149969145259553
iteration : 12107
train acc:  0.796875
train loss:  0.4338337779045105
train gradient:  0.21594117261561774
iteration : 12108
train acc:  0.7890625
train loss:  0.5047631859779358
train gradient:  0.3341836355127469
iteration : 12109
train acc:  0.875
train loss:  0.3647254407405853
train gradient:  0.16707082546147112
iteration : 12110
train acc:  0.84375
train loss:  0.3436518609523773
train gradient:  0.20620127017111223
iteration : 12111
train acc:  0.7890625
train loss:  0.4277808666229248
train gradient:  0.1856601231059323
iteration : 12112
train acc:  0.8828125
train loss:  0.3245205879211426
train gradient:  0.15459063414955598
iteration : 12113
train acc:  0.8359375
train loss:  0.37089669704437256
train gradient:  0.12966115463044414
iteration : 12114
train acc:  0.8671875
train loss:  0.26937350630760193
train gradient:  0.142384181380806
iteration : 12115
train acc:  0.8828125
train loss:  0.27514901757240295
train gradient:  0.13385451415560323
iteration : 12116
train acc:  0.8984375
train loss:  0.28014570474624634
train gradient:  0.09921577638816757
iteration : 12117
train acc:  0.9140625
train loss:  0.2610016465187073
train gradient:  0.12713055943213591
iteration : 12118
train acc:  0.875
train loss:  0.2830769121646881
train gradient:  0.15322635633300985
iteration : 12119
train acc:  0.859375
train loss:  0.2811582088470459
train gradient:  0.13431723170301008
iteration : 12120
train acc:  0.8203125
train loss:  0.34958070516586304
train gradient:  0.15704477805537798
iteration : 12121
train acc:  0.78125
train loss:  0.4652140438556671
train gradient:  0.23118908023492307
iteration : 12122
train acc:  0.828125
train loss:  0.32165300846099854
train gradient:  0.0954530606824392
iteration : 12123
train acc:  0.8828125
train loss:  0.26888173818588257
train gradient:  0.08922977354728494
iteration : 12124
train acc:  0.8984375
train loss:  0.24288256466388702
train gradient:  0.09535924973492226
iteration : 12125
train acc:  0.90625
train loss:  0.24903689324855804
train gradient:  0.07922873376569495
iteration : 12126
train acc:  0.8515625
train loss:  0.31331658363342285
train gradient:  0.14471715597263848
iteration : 12127
train acc:  0.8828125
train loss:  0.29884374141693115
train gradient:  0.09772211097877412
iteration : 12128
train acc:  0.875
train loss:  0.33557623624801636
train gradient:  0.15969794551565103
iteration : 12129
train acc:  0.875
train loss:  0.3430884778499603
train gradient:  0.11764587174961777
iteration : 12130
train acc:  0.828125
train loss:  0.3497197926044464
train gradient:  0.18809111386277105
iteration : 12131
train acc:  0.828125
train loss:  0.39266687631607056
train gradient:  0.2564858868109461
iteration : 12132
train acc:  0.8515625
train loss:  0.33883434534072876
train gradient:  0.1696678261485617
iteration : 12133
train acc:  0.8671875
train loss:  0.28560197353363037
train gradient:  0.13547135097847468
iteration : 12134
train acc:  0.859375
train loss:  0.31671494245529175
train gradient:  0.11768886252589957
iteration : 12135
train acc:  0.8828125
train loss:  0.3000008463859558
train gradient:  0.13312456792409127
iteration : 12136
train acc:  0.9375
train loss:  0.2471516728401184
train gradient:  0.14412354436268632
iteration : 12137
train acc:  0.8125
train loss:  0.35544872283935547
train gradient:  0.19179835253785094
iteration : 12138
train acc:  0.8359375
train loss:  0.3727869987487793
train gradient:  0.12379080624111109
iteration : 12139
train acc:  0.875
train loss:  0.29637235403060913
train gradient:  0.1849369425555885
iteration : 12140
train acc:  0.859375
train loss:  0.30556032061576843
train gradient:  0.1277557803645871
iteration : 12141
train acc:  0.90625
train loss:  0.32451313734054565
train gradient:  0.11549013355904472
iteration : 12142
train acc:  0.890625
train loss:  0.2601237893104553
train gradient:  0.126250180549283
iteration : 12143
train acc:  0.8359375
train loss:  0.39665594696998596
train gradient:  0.23537142703243036
iteration : 12144
train acc:  0.890625
train loss:  0.2903292775154114
train gradient:  0.12022257405869907
iteration : 12145
train acc:  0.90625
train loss:  0.2523361146450043
train gradient:  0.1263361416661989
iteration : 12146
train acc:  0.8046875
train loss:  0.37660205364227295
train gradient:  0.1786173069982929
iteration : 12147
train acc:  0.8671875
train loss:  0.3074013590812683
train gradient:  0.09277385218721429
iteration : 12148
train acc:  0.890625
train loss:  0.2596180737018585
train gradient:  0.0979362325515347
iteration : 12149
train acc:  0.859375
train loss:  0.31026870012283325
train gradient:  0.1440137432963217
iteration : 12150
train acc:  0.8828125
train loss:  0.2605275809764862
train gradient:  0.12262610027848972
iteration : 12151
train acc:  0.875
train loss:  0.28946125507354736
train gradient:  0.12764481866022023
iteration : 12152
train acc:  0.8359375
train loss:  0.3829994201660156
train gradient:  0.3255636195897298
iteration : 12153
train acc:  0.8203125
train loss:  0.33176183700561523
train gradient:  0.1567451636998706
iteration : 12154
train acc:  0.828125
train loss:  0.36901864409446716
train gradient:  0.16763799163865623
iteration : 12155
train acc:  0.875
train loss:  0.29408568143844604
train gradient:  0.1526920383621238
iteration : 12156
train acc:  0.8515625
train loss:  0.3200633227825165
train gradient:  0.11715827479288905
iteration : 12157
train acc:  0.921875
train loss:  0.27883073687553406
train gradient:  0.2103302425897369
iteration : 12158
train acc:  0.9296875
train loss:  0.20806457102298737
train gradient:  0.07926889379721662
iteration : 12159
train acc:  0.9375
train loss:  0.1919579952955246
train gradient:  0.08329421131479167
iteration : 12160
train acc:  0.8671875
train loss:  0.35361313819885254
train gradient:  0.22816296347176107
iteration : 12161
train acc:  0.84375
train loss:  0.31705358624458313
train gradient:  0.12287856548633017
iteration : 12162
train acc:  0.8359375
train loss:  0.3598982095718384
train gradient:  0.20153552828749127
iteration : 12163
train acc:  0.8515625
train loss:  0.3070409893989563
train gradient:  0.10964523701054102
iteration : 12164
train acc:  0.8203125
train loss:  0.35310155153274536
train gradient:  0.2484532967120252
iteration : 12165
train acc:  0.828125
train loss:  0.39978837966918945
train gradient:  0.16443096964893306
iteration : 12166
train acc:  0.8671875
train loss:  0.3301452100276947
train gradient:  0.1397651922138981
iteration : 12167
train acc:  0.8671875
train loss:  0.2630288600921631
train gradient:  0.15105950635794793
iteration : 12168
train acc:  0.84375
train loss:  0.37029382586479187
train gradient:  0.1795591693698282
iteration : 12169
train acc:  0.890625
train loss:  0.29229098558425903
train gradient:  0.147190853696767
iteration : 12170
train acc:  0.859375
train loss:  0.2924692928791046
train gradient:  0.14604149909705996
iteration : 12171
train acc:  0.8984375
train loss:  0.2774324119091034
train gradient:  0.10583375855119738
iteration : 12172
train acc:  0.828125
train loss:  0.4027140140533447
train gradient:  0.20383477513020398
iteration : 12173
train acc:  0.859375
train loss:  0.33759284019470215
train gradient:  0.13777718015955742
iteration : 12174
train acc:  0.8828125
train loss:  0.31174740195274353
train gradient:  0.09834168326613409
iteration : 12175
train acc:  0.828125
train loss:  0.36467188596725464
train gradient:  0.13359144714010884
iteration : 12176
train acc:  0.859375
train loss:  0.3203362226486206
train gradient:  0.1645609511082145
iteration : 12177
train acc:  0.90625
train loss:  0.2676198482513428
train gradient:  0.15384962234598215
iteration : 12178
train acc:  0.921875
train loss:  0.2568241357803345
train gradient:  0.12037650850203824
iteration : 12179
train acc:  0.875
train loss:  0.3669273257255554
train gradient:  0.16697685168291604
iteration : 12180
train acc:  0.8359375
train loss:  0.32625383138656616
train gradient:  0.1650846266977589
iteration : 12181
train acc:  0.890625
train loss:  0.29929089546203613
train gradient:  0.15339773345123256
iteration : 12182
train acc:  0.8828125
train loss:  0.3121578097343445
train gradient:  0.14370746086139446
iteration : 12183
train acc:  0.859375
train loss:  0.3725486397743225
train gradient:  0.18314307460971954
iteration : 12184
train acc:  0.84375
train loss:  0.30821526050567627
train gradient:  0.11733993193251285
iteration : 12185
train acc:  0.859375
train loss:  0.29465705156326294
train gradient:  0.10423190297668788
iteration : 12186
train acc:  0.8671875
train loss:  0.27166444063186646
train gradient:  0.1117857239826887
iteration : 12187
train acc:  0.859375
train loss:  0.3283078670501709
train gradient:  0.24834106611788986
iteration : 12188
train acc:  0.828125
train loss:  0.32776057720184326
train gradient:  0.19534306525667045
iteration : 12189
train acc:  0.90625
train loss:  0.24143852293491364
train gradient:  0.10868225748263978
iteration : 12190
train acc:  0.8359375
train loss:  0.3376726806163788
train gradient:  0.1281216808368994
iteration : 12191
train acc:  0.9140625
train loss:  0.2789463698863983
train gradient:  0.1057669269532232
iteration : 12192
train acc:  0.8671875
train loss:  0.3172736167907715
train gradient:  0.13575198553174264
iteration : 12193
train acc:  0.890625
train loss:  0.2574453055858612
train gradient:  0.1315624608616761
iteration : 12194
train acc:  0.859375
train loss:  0.32062703371047974
train gradient:  0.1895205412581386
iteration : 12195
train acc:  0.8359375
train loss:  0.3662445843219757
train gradient:  0.16610273829878452
iteration : 12196
train acc:  0.8671875
train loss:  0.29122602939605713
train gradient:  0.13150370863445382
iteration : 12197
train acc:  0.8203125
train loss:  0.3879515528678894
train gradient:  0.17194201944356063
iteration : 12198
train acc:  0.859375
train loss:  0.27626365423202515
train gradient:  0.1364153546862737
iteration : 12199
train acc:  0.84375
train loss:  0.35034680366516113
train gradient:  0.18414251700901046
iteration : 12200
train acc:  0.8515625
train loss:  0.3104989528656006
train gradient:  0.19908197969464086
iteration : 12201
train acc:  0.84375
train loss:  0.3211769163608551
train gradient:  0.13421421543879442
iteration : 12202
train acc:  0.859375
train loss:  0.33263659477233887
train gradient:  0.1703883825675448
iteration : 12203
train acc:  0.8984375
train loss:  0.2812640070915222
train gradient:  0.15431578770409818
iteration : 12204
train acc:  0.859375
train loss:  0.3009340763092041
train gradient:  0.18504974591346618
iteration : 12205
train acc:  0.9296875
train loss:  0.2440725862979889
train gradient:  0.0790819758364099
iteration : 12206
train acc:  0.8359375
train loss:  0.37056612968444824
train gradient:  0.19564174220808492
iteration : 12207
train acc:  0.8828125
train loss:  0.2729663550853729
train gradient:  0.09917194029863313
iteration : 12208
train acc:  0.8515625
train loss:  0.3520929515361786
train gradient:  0.15664214890637132
iteration : 12209
train acc:  0.8828125
train loss:  0.3347451686859131
train gradient:  0.14118564966615987
iteration : 12210
train acc:  0.8671875
train loss:  0.3089708089828491
train gradient:  0.1316293285147579
iteration : 12211
train acc:  0.890625
train loss:  0.274569034576416
train gradient:  0.14718577859631135
iteration : 12212
train acc:  0.8359375
train loss:  0.3074357211589813
train gradient:  0.13434323905779355
iteration : 12213
train acc:  0.828125
train loss:  0.3641034960746765
train gradient:  0.20130725343495962
iteration : 12214
train acc:  0.90625
train loss:  0.25745949149131775
train gradient:  0.15492058348284432
iteration : 12215
train acc:  0.8359375
train loss:  0.3054342269897461
train gradient:  0.14023729374659388
iteration : 12216
train acc:  0.859375
train loss:  0.3215142786502838
train gradient:  0.2790936590102801
iteration : 12217
train acc:  0.890625
train loss:  0.3273088335990906
train gradient:  0.12428725573191766
iteration : 12218
train acc:  0.8828125
train loss:  0.23597776889801025
train gradient:  0.126255670205121
iteration : 12219
train acc:  0.90625
train loss:  0.28164270520210266
train gradient:  0.12499461620592198
iteration : 12220
train acc:  0.890625
train loss:  0.26886069774627686
train gradient:  0.13081187847644082
iteration : 12221
train acc:  0.8828125
train loss:  0.30171462893486023
train gradient:  0.12240247614651732
iteration : 12222
train acc:  0.875
train loss:  0.3162379264831543
train gradient:  0.16922951185135074
iteration : 12223
train acc:  0.875
train loss:  0.35268473625183105
train gradient:  0.32803011112918007
iteration : 12224
train acc:  0.8828125
train loss:  0.25541552901268005
train gradient:  0.11488924222968247
iteration : 12225
train acc:  0.890625
train loss:  0.28516656160354614
train gradient:  0.14296325046136
iteration : 12226
train acc:  0.8515625
train loss:  0.3283067047595978
train gradient:  0.13636705455162984
iteration : 12227
train acc:  0.8515625
train loss:  0.3426439166069031
train gradient:  0.13949473195160544
iteration : 12228
train acc:  0.875
train loss:  0.3028450310230255
train gradient:  0.16638796183066779
iteration : 12229
train acc:  0.84375
train loss:  0.31892549991607666
train gradient:  0.15393420087229726
iteration : 12230
train acc:  0.8515625
train loss:  0.3118959963321686
train gradient:  0.15160299749601153
iteration : 12231
train acc:  0.8515625
train loss:  0.39351940155029297
train gradient:  0.1922832077209521
iteration : 12232
train acc:  0.8125
train loss:  0.4212970733642578
train gradient:  0.2109542724364854
iteration : 12233
train acc:  0.8125
train loss:  0.32910075783729553
train gradient:  0.14296350403859803
iteration : 12234
train acc:  0.859375
train loss:  0.3394162654876709
train gradient:  0.13237992072016203
iteration : 12235
train acc:  0.828125
train loss:  0.30569788813591003
train gradient:  0.1266362489086184
iteration : 12236
train acc:  0.8671875
train loss:  0.3213531970977783
train gradient:  0.16452645154677437
iteration : 12237
train acc:  0.8671875
train loss:  0.30486059188842773
train gradient:  0.13594630942711755
iteration : 12238
train acc:  0.8203125
train loss:  0.40068572759628296
train gradient:  0.32380936086718093
iteration : 12239
train acc:  0.859375
train loss:  0.33683648705482483
train gradient:  0.16570541135889225
iteration : 12240
train acc:  0.859375
train loss:  0.3261435627937317
train gradient:  0.15955232450523077
iteration : 12241
train acc:  0.828125
train loss:  0.3848506212234497
train gradient:  0.13584226074287087
iteration : 12242
train acc:  0.8125
train loss:  0.4198889136314392
train gradient:  0.2641753709674885
iteration : 12243
train acc:  0.921875
train loss:  0.23963813483715057
train gradient:  0.12394765496248429
iteration : 12244
train acc:  0.921875
train loss:  0.2384280562400818
train gradient:  0.06634181816440718
iteration : 12245
train acc:  0.8359375
train loss:  0.40206968784332275
train gradient:  0.1996193790654031
iteration : 12246
train acc:  0.8984375
train loss:  0.28998756408691406
train gradient:  0.12605885910838316
iteration : 12247
train acc:  0.828125
train loss:  0.383877158164978
train gradient:  0.19115769566053112
iteration : 12248
train acc:  0.859375
train loss:  0.35533761978149414
train gradient:  0.17853489174309614
iteration : 12249
train acc:  0.8828125
train loss:  0.32963043451309204
train gradient:  0.13638214128695447
iteration : 12250
train acc:  0.890625
train loss:  0.31055423617362976
train gradient:  0.09222168149879087
iteration : 12251
train acc:  0.8671875
train loss:  0.2570510506629944
train gradient:  0.09278073726931574
iteration : 12252
train acc:  0.875
train loss:  0.3550984263420105
train gradient:  0.17972566982804583
iteration : 12253
train acc:  0.8203125
train loss:  0.3594520092010498
train gradient:  0.19327753357401015
iteration : 12254
train acc:  0.8828125
train loss:  0.2916821241378784
train gradient:  0.1475857749697705
iteration : 12255
train acc:  0.8671875
train loss:  0.29425424337387085
train gradient:  0.11894705445957787
iteration : 12256
train acc:  0.8515625
train loss:  0.31283724308013916
train gradient:  0.13274561262572124
iteration : 12257
train acc:  0.875
train loss:  0.29110437631607056
train gradient:  0.10953714985609257
iteration : 12258
train acc:  0.9140625
train loss:  0.24937134981155396
train gradient:  0.10942183022171181
iteration : 12259
train acc:  0.8359375
train loss:  0.3228740096092224
train gradient:  0.22967963771805305
iteration : 12260
train acc:  0.875
train loss:  0.28325504064559937
train gradient:  0.09879802137185245
iteration : 12261
train acc:  0.859375
train loss:  0.32521265745162964
train gradient:  0.18494760321940387
iteration : 12262
train acc:  0.828125
train loss:  0.39966118335723877
train gradient:  0.21866597257002102
iteration : 12263
train acc:  0.8515625
train loss:  0.3342726528644562
train gradient:  0.1391554034329266
iteration : 12264
train acc:  0.875
train loss:  0.27450209856033325
train gradient:  0.10041432843345507
iteration : 12265
train acc:  0.9375
train loss:  0.21000638604164124
train gradient:  0.11028292673655091
iteration : 12266
train acc:  0.8828125
train loss:  0.2837831377983093
train gradient:  0.4160152806491062
iteration : 12267
train acc:  0.8671875
train loss:  0.38019102811813354
train gradient:  0.18315596711828558
iteration : 12268
train acc:  0.8828125
train loss:  0.2844615578651428
train gradient:  0.16882528550192077
iteration : 12269
train acc:  0.8671875
train loss:  0.415314644575119
train gradient:  0.327633634223741
iteration : 12270
train acc:  0.875
train loss:  0.30162835121154785
train gradient:  0.17431763774656972
iteration : 12271
train acc:  0.8515625
train loss:  0.341408908367157
train gradient:  0.16488590341642778
iteration : 12272
train acc:  0.8515625
train loss:  0.3282477855682373
train gradient:  0.12747322613813852
iteration : 12273
train acc:  0.796875
train loss:  0.3709854483604431
train gradient:  0.21744176274702987
iteration : 12274
train acc:  0.875
train loss:  0.35316625237464905
train gradient:  0.14749214731050858
iteration : 12275
train acc:  0.84375
train loss:  0.3992048501968384
train gradient:  0.22387958216347764
iteration : 12276
train acc:  0.8046875
train loss:  0.384915828704834
train gradient:  0.20275359625686695
iteration : 12277
train acc:  0.8203125
train loss:  0.4059765934944153
train gradient:  0.2102030880874421
iteration : 12278
train acc:  0.8203125
train loss:  0.4637148380279541
train gradient:  0.31018548950302777
iteration : 12279
train acc:  0.8203125
train loss:  0.37245869636535645
train gradient:  0.29719890655012376
iteration : 12280
train acc:  0.890625
train loss:  0.3050945997238159
train gradient:  0.17261951905371106
iteration : 12281
train acc:  0.90625
train loss:  0.2831002175807953
train gradient:  0.11410294980047771
iteration : 12282
train acc:  0.875
train loss:  0.27678197622299194
train gradient:  0.10028839889863553
iteration : 12283
train acc:  0.8671875
train loss:  0.39827895164489746
train gradient:  0.18926901950158137
iteration : 12284
train acc:  0.796875
train loss:  0.3827481269836426
train gradient:  0.1546075302936072
iteration : 12285
train acc:  0.8515625
train loss:  0.2799797058105469
train gradient:  0.10437706942848471
iteration : 12286
train acc:  0.8671875
train loss:  0.2893759608268738
train gradient:  0.1640216546138451
iteration : 12287
train acc:  0.8828125
train loss:  0.3201327621936798
train gradient:  0.13939559417005365
iteration : 12288
train acc:  0.8515625
train loss:  0.3152373433113098
train gradient:  0.15482410192152019
iteration : 12289
train acc:  0.875
train loss:  0.36293768882751465
train gradient:  0.1610050498738617
iteration : 12290
train acc:  0.90625
train loss:  0.2850274443626404
train gradient:  0.13352773684080352
iteration : 12291
train acc:  0.8671875
train loss:  0.2960250675678253
train gradient:  0.16975300069811827
iteration : 12292
train acc:  0.890625
train loss:  0.26799267530441284
train gradient:  0.1339643791117064
iteration : 12293
train acc:  0.84375
train loss:  0.35647520422935486
train gradient:  0.16398417646789412
iteration : 12294
train acc:  0.8515625
train loss:  0.29503190517425537
train gradient:  0.12536125738918785
iteration : 12295
train acc:  0.90625
train loss:  0.2760773003101349
train gradient:  0.13836063183113778
iteration : 12296
train acc:  0.859375
train loss:  0.32565200328826904
train gradient:  0.15855244637762414
iteration : 12297
train acc:  0.8125
train loss:  0.3324047923088074
train gradient:  0.16374329846260322
iteration : 12298
train acc:  0.8671875
train loss:  0.3098404109477997
train gradient:  0.16941053278938334
iteration : 12299
train acc:  0.875
train loss:  0.31931060552597046
train gradient:  0.2404816966956278
iteration : 12300
train acc:  0.8984375
train loss:  0.2903645932674408
train gradient:  0.09404027158046169
iteration : 12301
train acc:  0.859375
train loss:  0.3010011911392212
train gradient:  0.14953631469307904
iteration : 12302
train acc:  0.8671875
train loss:  0.33908310532569885
train gradient:  0.1413688809319683
iteration : 12303
train acc:  0.8203125
train loss:  0.4615432620048523
train gradient:  0.3792616981662424
iteration : 12304
train acc:  0.7890625
train loss:  0.4009166955947876
train gradient:  0.2306101528394635
iteration : 12305
train acc:  0.8125
train loss:  0.38392317295074463
train gradient:  0.15996312022525186
iteration : 12306
train acc:  0.9140625
train loss:  0.2430656999349594
train gradient:  0.09981781447615849
iteration : 12307
train acc:  0.890625
train loss:  0.2426583617925644
train gradient:  0.08178161804013614
iteration : 12308
train acc:  0.859375
train loss:  0.2643214762210846
train gradient:  0.1692822743879655
iteration : 12309
train acc:  0.8359375
train loss:  0.36074012517929077
train gradient:  0.15012481628777116
iteration : 12310
train acc:  0.875
train loss:  0.29377883672714233
train gradient:  0.10781276930993908
iteration : 12311
train acc:  0.875
train loss:  0.2898086905479431
train gradient:  0.11299026056278554
iteration : 12312
train acc:  0.84375
train loss:  0.3101179599761963
train gradient:  0.18300409777162824
iteration : 12313
train acc:  0.875
train loss:  0.3332250714302063
train gradient:  0.15511368710792994
iteration : 12314
train acc:  0.84375
train loss:  0.31509941816329956
train gradient:  0.1345203132977238
iteration : 12315
train acc:  0.8046875
train loss:  0.3984912633895874
train gradient:  0.20877591074767005
iteration : 12316
train acc:  0.84375
train loss:  0.3457292914390564
train gradient:  0.1737010804289538
iteration : 12317
train acc:  0.875
train loss:  0.26116806268692017
train gradient:  0.07972860134448168
iteration : 12318
train acc:  0.890625
train loss:  0.3396683633327484
train gradient:  0.15046987569385994
iteration : 12319
train acc:  0.890625
train loss:  0.280101478099823
train gradient:  0.11352592242503123
iteration : 12320
train acc:  0.90625
train loss:  0.24726150929927826
train gradient:  0.13139457260191736
iteration : 12321
train acc:  0.8671875
train loss:  0.39669519662857056
train gradient:  0.2164221320885778
iteration : 12322
train acc:  0.84375
train loss:  0.2885558009147644
train gradient:  0.12172871042528441
iteration : 12323
train acc:  0.8515625
train loss:  0.3551352620124817
train gradient:  0.2172848118513864
iteration : 12324
train acc:  0.828125
train loss:  0.4219757914543152
train gradient:  0.20273292507725904
iteration : 12325
train acc:  0.828125
train loss:  0.3988957703113556
train gradient:  0.19183157809843474
iteration : 12326
train acc:  0.84375
train loss:  0.3028908967971802
train gradient:  0.13664431186027087
iteration : 12327
train acc:  0.890625
train loss:  0.31502723693847656
train gradient:  0.12044398590855442
iteration : 12328
train acc:  0.890625
train loss:  0.2644706964492798
train gradient:  0.08270641678822076
iteration : 12329
train acc:  0.875
train loss:  0.301636278629303
train gradient:  0.15178907998612817
iteration : 12330
train acc:  0.875
train loss:  0.2802720069885254
train gradient:  0.16655889120923711
iteration : 12331
train acc:  0.84375
train loss:  0.3458983898162842
train gradient:  0.1564745216471149
iteration : 12332
train acc:  0.84375
train loss:  0.30638766288757324
train gradient:  0.12597077907240442
iteration : 12333
train acc:  0.875
train loss:  0.2769869863986969
train gradient:  0.1515388171028427
iteration : 12334
train acc:  0.90625
train loss:  0.24879609048366547
train gradient:  0.1495574396225779
iteration : 12335
train acc:  0.859375
train loss:  0.30823951959609985
train gradient:  0.10434105685262814
iteration : 12336
train acc:  0.875
train loss:  0.29255419969558716
train gradient:  0.11245463276887809
iteration : 12337
train acc:  0.8359375
train loss:  0.3885311782360077
train gradient:  0.22696370979471392
iteration : 12338
train acc:  0.875
train loss:  0.3041112720966339
train gradient:  0.15420339547695586
iteration : 12339
train acc:  0.84375
train loss:  0.25335267186164856
train gradient:  0.11258619548947278
iteration : 12340
train acc:  0.90625
train loss:  0.2815857529640198
train gradient:  0.156920633230285
iteration : 12341
train acc:  0.8671875
train loss:  0.29708558320999146
train gradient:  0.1080251489720071
iteration : 12342
train acc:  0.890625
train loss:  0.28049853444099426
train gradient:  0.13558924204119627
iteration : 12343
train acc:  0.8828125
train loss:  0.2768869698047638
train gradient:  0.1520945252056872
iteration : 12344
train acc:  0.84375
train loss:  0.35901376605033875
train gradient:  0.12466042783415564
iteration : 12345
train acc:  0.8984375
train loss:  0.2659197449684143
train gradient:  0.11990183911534512
iteration : 12346
train acc:  0.7734375
train loss:  0.4633779525756836
train gradient:  0.3496437035798855
iteration : 12347
train acc:  0.8125
train loss:  0.4028099477291107
train gradient:  0.18348004057271025
iteration : 12348
train acc:  0.8125
train loss:  0.39612871408462524
train gradient:  0.18118983138135447
iteration : 12349
train acc:  0.859375
train loss:  0.33264416456222534
train gradient:  0.15357892995293218
iteration : 12350
train acc:  0.75
train loss:  0.42582958936691284
train gradient:  0.2790099427836827
iteration : 12351
train acc:  0.78125
train loss:  0.43348371982574463
train gradient:  0.2371710941266209
iteration : 12352
train acc:  0.859375
train loss:  0.31641048192977905
train gradient:  0.11959551255257891
iteration : 12353
train acc:  0.921875
train loss:  0.2185058891773224
train gradient:  0.08370696507215425
iteration : 12354
train acc:  0.8828125
train loss:  0.29958564043045044
train gradient:  0.09438603667624892
iteration : 12355
train acc:  0.8125
train loss:  0.31796908378601074
train gradient:  0.13483958621692427
iteration : 12356
train acc:  0.890625
train loss:  0.2780751883983612
train gradient:  0.09765747274166081
iteration : 12357
train acc:  0.8828125
train loss:  0.3054961562156677
train gradient:  0.1152717020068306
iteration : 12358
train acc:  0.796875
train loss:  0.44150716066360474
train gradient:  0.2541995669358321
iteration : 12359
train acc:  0.8515625
train loss:  0.32291412353515625
train gradient:  0.12955441638396448
iteration : 12360
train acc:  0.875
train loss:  0.3090555667877197
train gradient:  0.11971722749890347
iteration : 12361
train acc:  0.84375
train loss:  0.380364328622818
train gradient:  0.1631225477401947
iteration : 12362
train acc:  0.875
train loss:  0.2601284980773926
train gradient:  0.1417973678723991
iteration : 12363
train acc:  0.828125
train loss:  0.37124305963516235
train gradient:  0.14536768339753953
iteration : 12364
train acc:  0.859375
train loss:  0.26628488302230835
train gradient:  0.1253929505711754
iteration : 12365
train acc:  0.921875
train loss:  0.2977888584136963
train gradient:  0.09985706940337225
iteration : 12366
train acc:  0.8515625
train loss:  0.3242432773113251
train gradient:  0.11003975659658749
iteration : 12367
train acc:  0.8828125
train loss:  0.2921295464038849
train gradient:  0.12527872913769925
iteration : 12368
train acc:  0.84375
train loss:  0.2813764214515686
train gradient:  0.11595981547017939
iteration : 12369
train acc:  0.8515625
train loss:  0.32932549715042114
train gradient:  0.15701770496642967
iteration : 12370
train acc:  0.875
train loss:  0.31113559007644653
train gradient:  0.14246034067909147
iteration : 12371
train acc:  0.9140625
train loss:  0.2653970420360565
train gradient:  0.08080633185205961
iteration : 12372
train acc:  0.9296875
train loss:  0.20793810486793518
train gradient:  0.08018816589855278
iteration : 12373
train acc:  0.8515625
train loss:  0.35973459482192993
train gradient:  0.14316154867254277
iteration : 12374
train acc:  0.9296875
train loss:  0.2565193176269531
train gradient:  0.1016280896742155
iteration : 12375
train acc:  0.90625
train loss:  0.3113274872303009
train gradient:  0.10666155579115501
iteration : 12376
train acc:  0.8203125
train loss:  0.37575769424438477
train gradient:  0.1489084882558926
iteration : 12377
train acc:  0.8515625
train loss:  0.29858893156051636
train gradient:  0.11039246733074651
iteration : 12378
train acc:  0.8515625
train loss:  0.33402007818222046
train gradient:  0.13114369793430647
iteration : 12379
train acc:  0.8515625
train loss:  0.3533509373664856
train gradient:  0.15092374029990158
iteration : 12380
train acc:  0.859375
train loss:  0.3243054449558258
train gradient:  0.12481592560691224
iteration : 12381
train acc:  0.8359375
train loss:  0.3886723518371582
train gradient:  0.17070377804204964
iteration : 12382
train acc:  0.8359375
train loss:  0.3337535262107849
train gradient:  0.13294758637067322
iteration : 12383
train acc:  0.8671875
train loss:  0.31950265169143677
train gradient:  0.12300493325669581
iteration : 12384
train acc:  0.8828125
train loss:  0.28780627250671387
train gradient:  0.1046103418209179
iteration : 12385
train acc:  0.84375
train loss:  0.30530738830566406
train gradient:  0.15899082804397296
iteration : 12386
train acc:  0.828125
train loss:  0.40799441933631897
train gradient:  0.1843514288967706
iteration : 12387
train acc:  0.8671875
train loss:  0.29729312658309937
train gradient:  0.1509559784411088
iteration : 12388
train acc:  0.9296875
train loss:  0.25497499108314514
train gradient:  0.18251687785902543
iteration : 12389
train acc:  0.8046875
train loss:  0.3520745635032654
train gradient:  0.1799444428869977
iteration : 12390
train acc:  0.875
train loss:  0.2974711060523987
train gradient:  0.18853229794922355
iteration : 12391
train acc:  0.828125
train loss:  0.30690285563468933
train gradient:  0.11674822006464283
iteration : 12392
train acc:  0.8203125
train loss:  0.3634739816188812
train gradient:  0.1842442740734222
iteration : 12393
train acc:  0.8515625
train loss:  0.3194641172885895
train gradient:  0.10261568995053141
iteration : 12394
train acc:  0.875
train loss:  0.32425403594970703
train gradient:  0.17673753832436212
iteration : 12395
train acc:  0.8515625
train loss:  0.3454958200454712
train gradient:  0.1677606420806601
iteration : 12396
train acc:  0.8203125
train loss:  0.43032366037368774
train gradient:  0.24051520414396454
iteration : 12397
train acc:  0.84375
train loss:  0.32780882716178894
train gradient:  0.1346984300745939
iteration : 12398
train acc:  0.90625
train loss:  0.25864261388778687
train gradient:  0.1290711459604171
iteration : 12399
train acc:  0.8515625
train loss:  0.33517399430274963
train gradient:  0.1286295969915342
iteration : 12400
train acc:  0.8984375
train loss:  0.32004454731941223
train gradient:  0.15678376294472135
iteration : 12401
train acc:  0.875
train loss:  0.31365054845809937
train gradient:  0.13688257463414655
iteration : 12402
train acc:  0.8359375
train loss:  0.392578661441803
train gradient:  0.17482435703568927
iteration : 12403
train acc:  0.8203125
train loss:  0.3391307592391968
train gradient:  0.15752527586310078
iteration : 12404
train acc:  0.90625
train loss:  0.3027883768081665
train gradient:  0.12153113928332361
iteration : 12405
train acc:  0.8984375
train loss:  0.27075886726379395
train gradient:  0.15861748371430034
iteration : 12406
train acc:  0.8984375
train loss:  0.27211466431617737
train gradient:  0.09690085296509383
iteration : 12407
train acc:  0.796875
train loss:  0.4393397271633148
train gradient:  0.2683113512388147
iteration : 12408
train acc:  0.90625
train loss:  0.2546970844268799
train gradient:  0.12831931508132685
iteration : 12409
train acc:  0.8203125
train loss:  0.37951022386550903
train gradient:  0.16727408199713062
iteration : 12410
train acc:  0.8203125
train loss:  0.42406851053237915
train gradient:  0.2282269597840681
iteration : 12411
train acc:  0.8359375
train loss:  0.3227267861366272
train gradient:  0.18804187860234306
iteration : 12412
train acc:  0.90625
train loss:  0.27982667088508606
train gradient:  0.1269830535047221
iteration : 12413
train acc:  0.8359375
train loss:  0.34017276763916016
train gradient:  0.15106817109658388
iteration : 12414
train acc:  0.8203125
train loss:  0.3393910229206085
train gradient:  0.15591489983027512
iteration : 12415
train acc:  0.84375
train loss:  0.332690954208374
train gradient:  0.14165735086588538
iteration : 12416
train acc:  0.8046875
train loss:  0.37642884254455566
train gradient:  0.22332166744356374
iteration : 12417
train acc:  0.8359375
train loss:  0.36822542548179626
train gradient:  0.15893156741925585
iteration : 12418
train acc:  0.8828125
train loss:  0.3357439637184143
train gradient:  0.2502930827764763
iteration : 12419
train acc:  0.84375
train loss:  0.36680328845977783
train gradient:  0.18904778410288417
iteration : 12420
train acc:  0.78125
train loss:  0.37316450476646423
train gradient:  0.18662649198917913
iteration : 12421
train acc:  0.859375
train loss:  0.28662702441215515
train gradient:  0.10721364505662441
iteration : 12422
train acc:  0.8359375
train loss:  0.3431750237941742
train gradient:  0.18081273337879838
iteration : 12423
train acc:  0.875
train loss:  0.2842315435409546
train gradient:  0.11072569066664402
iteration : 12424
train acc:  0.90625
train loss:  0.24573343992233276
train gradient:  0.1605056370653668
iteration : 12425
train acc:  0.90625
train loss:  0.2872198820114136
train gradient:  0.11602989996376187
iteration : 12426
train acc:  0.8203125
train loss:  0.3495020270347595
train gradient:  0.1530104581597565
iteration : 12427
train acc:  0.8359375
train loss:  0.3546907901763916
train gradient:  0.14825699797853242
iteration : 12428
train acc:  0.8671875
train loss:  0.32000380754470825
train gradient:  0.1613595553399582
iteration : 12429
train acc:  0.90625
train loss:  0.2767316699028015
train gradient:  0.12221328022141789
iteration : 12430
train acc:  0.8984375
train loss:  0.32798826694488525
train gradient:  0.23779209945843133
iteration : 12431
train acc:  0.8671875
train loss:  0.3471166789531708
train gradient:  0.16938824964237426
iteration : 12432
train acc:  0.84375
train loss:  0.3078007996082306
train gradient:  0.11786991278123024
iteration : 12433
train acc:  0.875
train loss:  0.3018369674682617
train gradient:  0.12289464451284939
iteration : 12434
train acc:  0.8125
train loss:  0.3721782863140106
train gradient:  0.17094087815500952
iteration : 12435
train acc:  0.890625
train loss:  0.28330692648887634
train gradient:  0.1380793001983635
iteration : 12436
train acc:  0.9140625
train loss:  0.2592773735523224
train gradient:  0.11824850057671137
iteration : 12437
train acc:  0.890625
train loss:  0.26110926270484924
train gradient:  0.12499064190973243
iteration : 12438
train acc:  0.859375
train loss:  0.38812577724456787
train gradient:  0.20053061640920436
iteration : 12439
train acc:  0.890625
train loss:  0.2883780002593994
train gradient:  0.1173760147701652
iteration : 12440
train acc:  0.8515625
train loss:  0.33027854561805725
train gradient:  0.13656504833098637
iteration : 12441
train acc:  0.890625
train loss:  0.27366968989372253
train gradient:  0.14743665431478692
iteration : 12442
train acc:  0.9140625
train loss:  0.25027546286582947
train gradient:  0.10985999661677712
iteration : 12443
train acc:  0.875
train loss:  0.3039393126964569
train gradient:  0.1161922731703078
iteration : 12444
train acc:  0.8671875
train loss:  0.3119969964027405
train gradient:  0.1356983236260162
iteration : 12445
train acc:  0.8828125
train loss:  0.30565357208251953
train gradient:  0.11991131011617201
iteration : 12446
train acc:  0.8984375
train loss:  0.24000594019889832
train gradient:  0.11355071587226759
iteration : 12447
train acc:  0.828125
train loss:  0.4240725040435791
train gradient:  0.23650431159271784
iteration : 12448
train acc:  0.9140625
train loss:  0.30158841609954834
train gradient:  0.1822489400543542
iteration : 12449
train acc:  0.828125
train loss:  0.3554396629333496
train gradient:  0.1899765168417588
iteration : 12450
train acc:  0.8515625
train loss:  0.33981359004974365
train gradient:  0.1669886245141188
iteration : 12451
train acc:  0.8671875
train loss:  0.3078762888908386
train gradient:  0.1559556316823796
iteration : 12452
train acc:  0.890625
train loss:  0.2776584327220917
train gradient:  0.09300652722709178
iteration : 12453
train acc:  0.8984375
train loss:  0.26437169313430786
train gradient:  0.12893038030417886
iteration : 12454
train acc:  0.8671875
train loss:  0.3056142330169678
train gradient:  0.16495682517427057
iteration : 12455
train acc:  0.8828125
train loss:  0.27832311391830444
train gradient:  0.10435856651874591
iteration : 12456
train acc:  0.8671875
train loss:  0.3215654790401459
train gradient:  0.12238243934299706
iteration : 12457
train acc:  0.8984375
train loss:  0.2439192235469818
train gradient:  0.11919359590831798
iteration : 12458
train acc:  0.859375
train loss:  0.3576103448867798
train gradient:  0.17708985721531736
iteration : 12459
train acc:  0.8828125
train loss:  0.2759675979614258
train gradient:  0.11086834396082343
iteration : 12460
train acc:  0.8671875
train loss:  0.36824560165405273
train gradient:  0.17721279717496802
iteration : 12461
train acc:  0.8984375
train loss:  0.2718544900417328
train gradient:  0.11866613848043543
iteration : 12462
train acc:  0.9140625
train loss:  0.23352700471878052
train gradient:  0.08150047214673412
iteration : 12463
train acc:  0.8515625
train loss:  0.30976372957229614
train gradient:  0.16229781882481964
iteration : 12464
train acc:  0.875
train loss:  0.3285409212112427
train gradient:  0.20263528590004898
iteration : 12465
train acc:  0.8515625
train loss:  0.3385263979434967
train gradient:  0.19056909832183072
iteration : 12466
train acc:  0.875
train loss:  0.288213849067688
train gradient:  0.13756743810486294
iteration : 12467
train acc:  0.8984375
train loss:  0.2819517254829407
train gradient:  0.1442501687300784
iteration : 12468
train acc:  0.8828125
train loss:  0.2656315267086029
train gradient:  0.12346916937056254
iteration : 12469
train acc:  0.8359375
train loss:  0.3217087388038635
train gradient:  0.18514413787423378
iteration : 12470
train acc:  0.8515625
train loss:  0.3463873863220215
train gradient:  0.17083467779159492
iteration : 12471
train acc:  0.8515625
train loss:  0.34955281019210815
train gradient:  0.16133270117389334
iteration : 12472
train acc:  0.8671875
train loss:  0.41355055570602417
train gradient:  0.2151900919487144
iteration : 12473
train acc:  0.890625
train loss:  0.25353217124938965
train gradient:  0.07429725901151898
iteration : 12474
train acc:  0.8984375
train loss:  0.25254592299461365
train gradient:  0.10129477653472369
iteration : 12475
train acc:  0.84375
train loss:  0.35815656185150146
train gradient:  0.14189684013163664
iteration : 12476
train acc:  0.8671875
train loss:  0.3224531412124634
train gradient:  0.14535538095319633
iteration : 12477
train acc:  0.8984375
train loss:  0.25631847977638245
train gradient:  0.10880813759312853
iteration : 12478
train acc:  0.84375
train loss:  0.32344695925712585
train gradient:  0.17920480445795667
iteration : 12479
train acc:  0.8515625
train loss:  0.29416823387145996
train gradient:  0.1340958224803534
iteration : 12480
train acc:  0.8515625
train loss:  0.2915622591972351
train gradient:  0.14097549794378875
iteration : 12481
train acc:  0.875
train loss:  0.2829047441482544
train gradient:  0.17215811532010544
iteration : 12482
train acc:  0.8203125
train loss:  0.3476751446723938
train gradient:  0.1773909256225361
iteration : 12483
train acc:  0.890625
train loss:  0.2399788796901703
train gradient:  0.08853597091003591
iteration : 12484
train acc:  0.8515625
train loss:  0.31973111629486084
train gradient:  0.15652934970103022
iteration : 12485
train acc:  0.875
train loss:  0.3431059718132019
train gradient:  0.2099720100420393
iteration : 12486
train acc:  0.8359375
train loss:  0.36098605394363403
train gradient:  0.18064070061125304
iteration : 12487
train acc:  0.8046875
train loss:  0.41202080249786377
train gradient:  0.21844560456422674
iteration : 12488
train acc:  0.859375
train loss:  0.35329669713974
train gradient:  0.17475914305828455
iteration : 12489
train acc:  0.8359375
train loss:  0.3126949667930603
train gradient:  0.2005773089806637
iteration : 12490
train acc:  0.8359375
train loss:  0.3530282974243164
train gradient:  0.3327769955404574
iteration : 12491
train acc:  0.8359375
train loss:  0.3079580068588257
train gradient:  0.11139644490638766
iteration : 12492
train acc:  0.8359375
train loss:  0.35801804065704346
train gradient:  0.245114423157749
iteration : 12493
train acc:  0.8203125
train loss:  0.38827717304229736
train gradient:  0.21314565351396736
iteration : 12494
train acc:  0.8828125
train loss:  0.2575559616088867
train gradient:  0.10891896401439193
iteration : 12495
train acc:  0.8515625
train loss:  0.3366321325302124
train gradient:  0.15512145791254062
iteration : 12496
train acc:  0.8203125
train loss:  0.35630571842193604
train gradient:  0.23325128775901155
iteration : 12497
train acc:  0.890625
train loss:  0.25668269395828247
train gradient:  0.1128912226441303
iteration : 12498
train acc:  0.8203125
train loss:  0.320698082447052
train gradient:  0.1748697345244195
iteration : 12499
train acc:  0.796875
train loss:  0.40433692932128906
train gradient:  0.2315794936672887
iteration : 12500
train acc:  0.8359375
train loss:  0.35973674058914185
train gradient:  0.16052403038384996
iteration : 12501
train acc:  0.8046875
train loss:  0.4220794439315796
train gradient:  0.23768664472202056
iteration : 12502
train acc:  0.859375
train loss:  0.3250690996646881
train gradient:  0.1838016549598254
iteration : 12503
train acc:  0.8984375
train loss:  0.29992640018463135
train gradient:  0.13048708657676697
iteration : 12504
train acc:  0.890625
train loss:  0.3083615005016327
train gradient:  0.15850613988665838
iteration : 12505
train acc:  0.8671875
train loss:  0.3631882071495056
train gradient:  0.19722627707803836
iteration : 12506
train acc:  0.9296875
train loss:  0.22527356445789337
train gradient:  0.1910019160053882
iteration : 12507
train acc:  0.8984375
train loss:  0.24797385931015015
train gradient:  0.08774765748818758
iteration : 12508
train acc:  0.859375
train loss:  0.3259198069572449
train gradient:  0.12368038655011517
iteration : 12509
train acc:  0.8515625
train loss:  0.37683284282684326
train gradient:  0.2837653106703298
iteration : 12510
train acc:  0.890625
train loss:  0.27821409702301025
train gradient:  0.10229029536858625
iteration : 12511
train acc:  0.8828125
train loss:  0.24578413367271423
train gradient:  0.12250177829162182
iteration : 12512
train acc:  0.8515625
train loss:  0.4093564748764038
train gradient:  0.19531971933041392
iteration : 12513
train acc:  0.90625
train loss:  0.24111418426036835
train gradient:  0.0830177690341412
iteration : 12514
train acc:  0.875
train loss:  0.2974439263343811
train gradient:  0.1678843944557094
iteration : 12515
train acc:  0.8671875
train loss:  0.2821062505245209
train gradient:  0.1882031544057151
iteration : 12516
train acc:  0.875
train loss:  0.2527967691421509
train gradient:  0.09122643961250293
iteration : 12517
train acc:  0.859375
train loss:  0.3519584536552429
train gradient:  0.18254784367654953
iteration : 12518
train acc:  0.828125
train loss:  0.3382551074028015
train gradient:  0.2695876089252932
iteration : 12519
train acc:  0.890625
train loss:  0.2707260251045227
train gradient:  0.11082189744973522
iteration : 12520
train acc:  0.8671875
train loss:  0.23982632160186768
train gradient:  0.156276076274938
iteration : 12521
train acc:  0.875
train loss:  0.3132654130458832
train gradient:  0.14023625223529407
iteration : 12522
train acc:  0.8515625
train loss:  0.32726722955703735
train gradient:  0.129551070442402
iteration : 12523
train acc:  0.8203125
train loss:  0.3816964626312256
train gradient:  0.21771519807163364
iteration : 12524
train acc:  0.859375
train loss:  0.30054718255996704
train gradient:  0.1816003800830327
iteration : 12525
train acc:  0.8359375
train loss:  0.3614142835140228
train gradient:  0.1689648830705377
iteration : 12526
train acc:  0.859375
train loss:  0.3607977330684662
train gradient:  0.13596997660797616
iteration : 12527
train acc:  0.875
train loss:  0.27424755692481995
train gradient:  0.16689262550156808
iteration : 12528
train acc:  0.828125
train loss:  0.3741564154624939
train gradient:  0.2213831972649488
iteration : 12529
train acc:  0.8671875
train loss:  0.3353193402290344
train gradient:  0.13857434695624066
iteration : 12530
train acc:  0.8515625
train loss:  0.34623926877975464
train gradient:  0.18708063388480262
iteration : 12531
train acc:  0.8515625
train loss:  0.3598541021347046
train gradient:  0.16771734367216484
iteration : 12532
train acc:  0.8515625
train loss:  0.2424323558807373
train gradient:  0.10345305195291221
iteration : 12533
train acc:  0.8515625
train loss:  0.32420140504837036
train gradient:  0.18667513649951367
iteration : 12534
train acc:  0.890625
train loss:  0.32626140117645264
train gradient:  0.1576733286114383
iteration : 12535
train acc:  0.8828125
train loss:  0.21779462695121765
train gradient:  0.08509635904069593
iteration : 12536
train acc:  0.859375
train loss:  0.340694785118103
train gradient:  0.1840973766613349
iteration : 12537
train acc:  0.890625
train loss:  0.2779237627983093
train gradient:  0.12734525266846783
iteration : 12538
train acc:  0.90625
train loss:  0.24440032243728638
train gradient:  0.11565477386296441
iteration : 12539
train acc:  0.875
train loss:  0.2972036898136139
train gradient:  0.13767801667617255
iteration : 12540
train acc:  0.859375
train loss:  0.3004376292228699
train gradient:  0.15391796186035864
iteration : 12541
train acc:  0.890625
train loss:  0.28906023502349854
train gradient:  0.11875393451286985
iteration : 12542
train acc:  0.890625
train loss:  0.33297765254974365
train gradient:  0.16258020031756124
iteration : 12543
train acc:  0.859375
train loss:  0.3777256906032562
train gradient:  0.22389164760619412
iteration : 12544
train acc:  0.8671875
train loss:  0.2967728078365326
train gradient:  0.10403387984506599
iteration : 12545
train acc:  0.8359375
train loss:  0.3538416922092438
train gradient:  0.19372070729542973
iteration : 12546
train acc:  0.8828125
train loss:  0.29901647567749023
train gradient:  0.08675291240760837
iteration : 12547
train acc:  0.8359375
train loss:  0.45063427090644836
train gradient:  0.2582554653830926
iteration : 12548
train acc:  0.859375
train loss:  0.35527849197387695
train gradient:  0.14859523221968718
iteration : 12549
train acc:  0.8515625
train loss:  0.3488842844963074
train gradient:  0.20429162114084062
iteration : 12550
train acc:  0.8828125
train loss:  0.3089548349380493
train gradient:  0.15884390072750082
iteration : 12551
train acc:  0.8359375
train loss:  0.3531843423843384
train gradient:  0.1344816510207434
iteration : 12552
train acc:  0.8671875
train loss:  0.2727659046649933
train gradient:  0.09807123849395702
iteration : 12553
train acc:  0.7890625
train loss:  0.4611371159553528
train gradient:  0.20257505082480143
iteration : 12554
train acc:  0.8125
train loss:  0.33010271191596985
train gradient:  0.16845620066526018
iteration : 12555
train acc:  0.84375
train loss:  0.34690403938293457
train gradient:  0.16839440258144991
iteration : 12556
train acc:  0.8828125
train loss:  0.29603973031044006
train gradient:  0.11837127766980116
iteration : 12557
train acc:  0.828125
train loss:  0.3555363714694977
train gradient:  0.18672931328534995
iteration : 12558
train acc:  0.8828125
train loss:  0.2753034830093384
train gradient:  0.20917897015130443
iteration : 12559
train acc:  0.875
train loss:  0.27975213527679443
train gradient:  0.15381015734894488
iteration : 12560
train acc:  0.8515625
train loss:  0.32667821645736694
train gradient:  0.13764049771909095
iteration : 12561
train acc:  0.8671875
train loss:  0.28812718391418457
train gradient:  0.14442402664019027
iteration : 12562
train acc:  0.828125
train loss:  0.34129804372787476
train gradient:  0.19146504905238687
iteration : 12563
train acc:  0.8203125
train loss:  0.3568238615989685
train gradient:  0.18899673358961278
iteration : 12564
train acc:  0.828125
train loss:  0.34910479187965393
train gradient:  0.14540563103210974
iteration : 12565
train acc:  0.8515625
train loss:  0.3257262110710144
train gradient:  0.11587336986471894
iteration : 12566
train acc:  0.8828125
train loss:  0.3129487633705139
train gradient:  0.12895125719333012
iteration : 12567
train acc:  0.890625
train loss:  0.2738637924194336
train gradient:  0.14477401250609617
iteration : 12568
train acc:  0.921875
train loss:  0.23014235496520996
train gradient:  0.06923790226002015
iteration : 12569
train acc:  0.8671875
train loss:  0.33593982458114624
train gradient:  0.11865598847791878
iteration : 12570
train acc:  0.8125
train loss:  0.4167308807373047
train gradient:  0.1777154186916017
iteration : 12571
train acc:  0.8828125
train loss:  0.3355620801448822
train gradient:  0.1592546486084134
iteration : 12572
train acc:  0.84375
train loss:  0.39935746788978577
train gradient:  0.18068116751365715
iteration : 12573
train acc:  0.90625
train loss:  0.23704668879508972
train gradient:  0.07655210229848508
iteration : 12574
train acc:  0.84375
train loss:  0.3624395430088043
train gradient:  0.15483874207332168
iteration : 12575
train acc:  0.8359375
train loss:  0.321350634098053
train gradient:  0.18137003680828928
iteration : 12576
train acc:  0.7734375
train loss:  0.45475757122039795
train gradient:  0.224871634078655
iteration : 12577
train acc:  0.890625
train loss:  0.2717846930027008
train gradient:  0.07879825783998805
iteration : 12578
train acc:  0.8828125
train loss:  0.30612656474113464
train gradient:  0.12924503945184734
iteration : 12579
train acc:  0.8515625
train loss:  0.33151960372924805
train gradient:  0.19854968310324742
iteration : 12580
train acc:  0.921875
train loss:  0.23358207941055298
train gradient:  0.10816794286710937
iteration : 12581
train acc:  0.9140625
train loss:  0.25772470235824585
train gradient:  0.10190992205318743
iteration : 12582
train acc:  0.8828125
train loss:  0.33383238315582275
train gradient:  0.23995980679595352
iteration : 12583
train acc:  0.828125
train loss:  0.39085161685943604
train gradient:  0.16112497512423957
iteration : 12584
train acc:  0.8984375
train loss:  0.24749015271663666
train gradient:  0.07935258218300897
iteration : 12585
train acc:  0.84375
train loss:  0.32932090759277344
train gradient:  0.1601018376535402
iteration : 12586
train acc:  0.890625
train loss:  0.26150402426719666
train gradient:  0.1134963726880179
iteration : 12587
train acc:  0.90625
train loss:  0.24482791125774384
train gradient:  0.09317479102357994
iteration : 12588
train acc:  0.890625
train loss:  0.2977806031703949
train gradient:  0.16150533686317547
iteration : 12589
train acc:  0.875
train loss:  0.3381159007549286
train gradient:  0.1895500603761302
iteration : 12590
train acc:  0.8671875
train loss:  0.3022305369377136
train gradient:  0.12788394177839868
iteration : 12591
train acc:  0.84375
train loss:  0.3361704349517822
train gradient:  0.15027465444654647
iteration : 12592
train acc:  0.8359375
train loss:  0.40489205718040466
train gradient:  0.19611110536711102
iteration : 12593
train acc:  0.875
train loss:  0.3064991235733032
train gradient:  0.15485375123418277
iteration : 12594
train acc:  0.859375
train loss:  0.3110268712043762
train gradient:  0.14394259626448658
iteration : 12595
train acc:  0.859375
train loss:  0.32318180799484253
train gradient:  0.15320881219661586
iteration : 12596
train acc:  0.78125
train loss:  0.3689447045326233
train gradient:  0.18865586186138564
iteration : 12597
train acc:  0.875
train loss:  0.2850923240184784
train gradient:  0.14112803289536713
iteration : 12598
train acc:  0.9140625
train loss:  0.26066896319389343
train gradient:  0.08452805460401128
iteration : 12599
train acc:  0.84375
train loss:  0.2977316379547119
train gradient:  0.1262276629001871
iteration : 12600
train acc:  0.859375
train loss:  0.3436930179595947
train gradient:  0.19743763651952517
iteration : 12601
train acc:  0.8515625
train loss:  0.3029857873916626
train gradient:  0.21752572033977915
iteration : 12602
train acc:  0.859375
train loss:  0.3478141129016876
train gradient:  0.13507704480383356
iteration : 12603
train acc:  0.8515625
train loss:  0.32531189918518066
train gradient:  0.11465722565032833
iteration : 12604
train acc:  0.8359375
train loss:  0.30343863368034363
train gradient:  0.11164122518249156
iteration : 12605
train acc:  0.8046875
train loss:  0.4471156597137451
train gradient:  0.40706621047858843
iteration : 12606
train acc:  0.8359375
train loss:  0.3499714136123657
train gradient:  0.13121781860165407
iteration : 12607
train acc:  0.859375
train loss:  0.2956078052520752
train gradient:  0.13928871891812802
iteration : 12608
train acc:  0.828125
train loss:  0.37140369415283203
train gradient:  0.1860738296565917
iteration : 12609
train acc:  0.8828125
train loss:  0.27558571100234985
train gradient:  0.15071677188276245
iteration : 12610
train acc:  0.9140625
train loss:  0.24042288959026337
train gradient:  0.14086810936383198
iteration : 12611
train acc:  0.8203125
train loss:  0.36396995186805725
train gradient:  0.18249249692771524
iteration : 12612
train acc:  0.8984375
train loss:  0.26049673557281494
train gradient:  0.10169735153292037
iteration : 12613
train acc:  0.8671875
train loss:  0.28542420268058777
train gradient:  0.11867947772943352
iteration : 12614
train acc:  0.8359375
train loss:  0.31451982259750366
train gradient:  0.15323301022914582
iteration : 12615
train acc:  0.84375
train loss:  0.3735506236553192
train gradient:  0.1826183992878328
iteration : 12616
train acc:  0.8828125
train loss:  0.3056598901748657
train gradient:  0.16046808846927912
iteration : 12617
train acc:  0.8359375
train loss:  0.37568143010139465
train gradient:  0.19082443943379868
iteration : 12618
train acc:  0.8671875
train loss:  0.33813387155532837
train gradient:  0.15578975306985618
iteration : 12619
train acc:  0.8046875
train loss:  0.35180342197418213
train gradient:  0.17809722251092092
iteration : 12620
train acc:  0.9140625
train loss:  0.2435983270406723
train gradient:  0.12053839912071372
iteration : 12621
train acc:  0.8671875
train loss:  0.27357226610183716
train gradient:  0.08507719485527734
iteration : 12622
train acc:  0.8125
train loss:  0.3769453465938568
train gradient:  0.24507141072121305
iteration : 12623
train acc:  0.8359375
train loss:  0.34779131412506104
train gradient:  0.18513135778318524
iteration : 12624
train acc:  0.875
train loss:  0.3114551305770874
train gradient:  0.138587392343485
iteration : 12625
train acc:  0.84375
train loss:  0.33086442947387695
train gradient:  0.14669806725036247
iteration : 12626
train acc:  0.84375
train loss:  0.3578999638557434
train gradient:  0.13358889519494116
iteration : 12627
train acc:  0.8671875
train loss:  0.30008840560913086
train gradient:  0.11783193617329815
iteration : 12628
train acc:  0.828125
train loss:  0.35179436206817627
train gradient:  0.16768338812269074
iteration : 12629
train acc:  0.890625
train loss:  0.2690927982330322
train gradient:  0.1279420860014373
iteration : 12630
train acc:  0.8515625
train loss:  0.30423229932785034
train gradient:  0.15857386072819368
iteration : 12631
train acc:  0.8203125
train loss:  0.36078476905822754
train gradient:  0.2808577565811086
iteration : 12632
train acc:  0.8359375
train loss:  0.31857407093048096
train gradient:  0.22340470563991066
iteration : 12633
train acc:  0.8515625
train loss:  0.29664796590805054
train gradient:  0.17410157758115946
iteration : 12634
train acc:  0.8671875
train loss:  0.26231491565704346
train gradient:  0.111739680420347
iteration : 12635
train acc:  0.8203125
train loss:  0.39501190185546875
train gradient:  0.21062776506764763
iteration : 12636
train acc:  0.890625
train loss:  0.28920960426330566
train gradient:  0.14054951647962388
iteration : 12637
train acc:  0.921875
train loss:  0.25769734382629395
train gradient:  0.10647249188153692
iteration : 12638
train acc:  0.8671875
train loss:  0.27014270424842834
train gradient:  0.1383188052850211
iteration : 12639
train acc:  0.8515625
train loss:  0.3403347432613373
train gradient:  0.17445451508916815
iteration : 12640
train acc:  0.8359375
train loss:  0.39166802167892456
train gradient:  0.19518582748573965
iteration : 12641
train acc:  0.8203125
train loss:  0.35023272037506104
train gradient:  0.15009210708804888
iteration : 12642
train acc:  0.8984375
train loss:  0.23551145195960999
train gradient:  0.09943795368662917
iteration : 12643
train acc:  0.8515625
train loss:  0.36187392473220825
train gradient:  0.16402863556023478
iteration : 12644
train acc:  0.859375
train loss:  0.3059103488922119
train gradient:  0.12502857010226542
iteration : 12645
train acc:  0.8359375
train loss:  0.38515934348106384
train gradient:  0.17457356922202227
iteration : 12646
train acc:  0.8515625
train loss:  0.37739962339401245
train gradient:  0.1387204141903114
iteration : 12647
train acc:  0.8203125
train loss:  0.35615795850753784
train gradient:  0.20347709052238402
iteration : 12648
train acc:  0.875
train loss:  0.2579362988471985
train gradient:  0.0815240616736147
iteration : 12649
train acc:  0.859375
train loss:  0.3309825658798218
train gradient:  0.17267127589643672
iteration : 12650
train acc:  0.859375
train loss:  0.3408659100532532
train gradient:  0.163311824752356
iteration : 12651
train acc:  0.84375
train loss:  0.35662585496902466
train gradient:  0.19500605759426087
iteration : 12652
train acc:  0.8515625
train loss:  0.35148802399635315
train gradient:  0.18616196800044088
iteration : 12653
train acc:  0.9140625
train loss:  0.27173542976379395
train gradient:  0.3241118616309072
iteration : 12654
train acc:  0.8125
train loss:  0.47287479043006897
train gradient:  0.2569587312193068
iteration : 12655
train acc:  0.828125
train loss:  0.3367944657802582
train gradient:  0.14374005521015837
iteration : 12656
train acc:  0.8984375
train loss:  0.2596191167831421
train gradient:  0.12697810521556702
iteration : 12657
train acc:  0.8671875
train loss:  0.3147364854812622
train gradient:  0.14892544764411153
iteration : 12658
train acc:  0.84375
train loss:  0.3018178343772888
train gradient:  0.1261141854347002
iteration : 12659
train acc:  0.9140625
train loss:  0.2432967722415924
train gradient:  0.08936613768533005
iteration : 12660
train acc:  0.8515625
train loss:  0.3261567950248718
train gradient:  0.13639739515071175
iteration : 12661
train acc:  0.8828125
train loss:  0.32039058208465576
train gradient:  0.1399277261431462
iteration : 12662
train acc:  0.8828125
train loss:  0.2754700779914856
train gradient:  0.11815144701515215
iteration : 12663
train acc:  0.8828125
train loss:  0.27177804708480835
train gradient:  0.13597467422522838
iteration : 12664
train acc:  0.84375
train loss:  0.31210142374038696
train gradient:  0.17817799479843802
iteration : 12665
train acc:  0.8984375
train loss:  0.23770146071910858
train gradient:  0.0947995112707178
iteration : 12666
train acc:  0.859375
train loss:  0.32957923412323
train gradient:  0.1708128732920765
iteration : 12667
train acc:  0.8828125
train loss:  0.3339228928089142
train gradient:  0.10487423043045231
iteration : 12668
train acc:  0.890625
train loss:  0.314683735370636
train gradient:  0.13901216514858583
iteration : 12669
train acc:  0.8359375
train loss:  0.37617194652557373
train gradient:  0.2100065025885728
iteration : 12670
train acc:  0.8203125
train loss:  0.4113205075263977
train gradient:  0.18163321076941635
iteration : 12671
train acc:  0.9296875
train loss:  0.24114933609962463
train gradient:  0.11435405012691624
iteration : 12672
train acc:  0.890625
train loss:  0.27329280972480774
train gradient:  0.08742655250553492
iteration : 12673
train acc:  0.828125
train loss:  0.35070669651031494
train gradient:  0.15275946553704925
iteration : 12674
train acc:  0.875
train loss:  0.3352658152580261
train gradient:  0.1918369979953981
iteration : 12675
train acc:  0.8828125
train loss:  0.2961050271987915
train gradient:  0.14640097603435012
iteration : 12676
train acc:  0.828125
train loss:  0.3512738347053528
train gradient:  0.16494057244428617
iteration : 12677
train acc:  0.8828125
train loss:  0.2949175238609314
train gradient:  0.15054201494507363
iteration : 12678
train acc:  0.921875
train loss:  0.26631850004196167
train gradient:  0.1474657322104134
iteration : 12679
train acc:  0.8359375
train loss:  0.35992223024368286
train gradient:  0.20063593907193544
iteration : 12680
train acc:  0.8359375
train loss:  0.4266658425331116
train gradient:  0.24718961294269082
iteration : 12681
train acc:  0.828125
train loss:  0.44814369082450867
train gradient:  0.2541586178527932
iteration : 12682
train acc:  0.84375
train loss:  0.36286431550979614
train gradient:  0.20327973024628443
iteration : 12683
train acc:  0.8984375
train loss:  0.24963878095149994
train gradient:  0.1297377364052769
iteration : 12684
train acc:  0.8125
train loss:  0.4237668216228485
train gradient:  0.19943214358389294
iteration : 12685
train acc:  0.8359375
train loss:  0.37525901198387146
train gradient:  0.2270479615636275
iteration : 12686
train acc:  0.8828125
train loss:  0.3202955722808838
train gradient:  0.17313560512314816
iteration : 12687
train acc:  0.90625
train loss:  0.254660964012146
train gradient:  0.12586935702487367
iteration : 12688
train acc:  0.8671875
train loss:  0.2534286677837372
train gradient:  0.11908710620090043
iteration : 12689
train acc:  0.8359375
train loss:  0.3383771479129791
train gradient:  0.13939713725825878
iteration : 12690
train acc:  0.8515625
train loss:  0.33004963397979736
train gradient:  0.17632023300551397
iteration : 12691
train acc:  0.8125
train loss:  0.3387525975704193
train gradient:  0.12152057672559144
iteration : 12692
train acc:  0.890625
train loss:  0.3047053813934326
train gradient:  0.1314275061981216
iteration : 12693
train acc:  0.9140625
train loss:  0.22261129319667816
train gradient:  0.1127462055320566
iteration : 12694
train acc:  0.890625
train loss:  0.3071483373641968
train gradient:  0.16027811480722784
iteration : 12695
train acc:  0.8359375
train loss:  0.3456093370914459
train gradient:  0.19652673777006607
iteration : 12696
train acc:  0.8828125
train loss:  0.2811717391014099
train gradient:  0.10639506709288848
iteration : 12697
train acc:  0.8515625
train loss:  0.36023589968681335
train gradient:  0.17659742919985544
iteration : 12698
train acc:  0.8515625
train loss:  0.3572971522808075
train gradient:  0.2113455441159559
iteration : 12699
train acc:  0.8359375
train loss:  0.30429717898368835
train gradient:  0.14001524648462876
iteration : 12700
train acc:  0.90625
train loss:  0.24637135863304138
train gradient:  0.0932849877447647
iteration : 12701
train acc:  0.8046875
train loss:  0.4356422424316406
train gradient:  0.28332842156709076
iteration : 12702
train acc:  0.8359375
train loss:  0.31540337204933167
train gradient:  0.15908296344065134
iteration : 12703
train acc:  0.859375
train loss:  0.3015929162502289
train gradient:  0.13598846560340733
iteration : 12704
train acc:  0.9296875
train loss:  0.23035183548927307
train gradient:  0.15748978106071665
iteration : 12705
train acc:  0.859375
train loss:  0.31997621059417725
train gradient:  0.13164530556611903
iteration : 12706
train acc:  0.875
train loss:  0.3200482130050659
train gradient:  0.13634120269707622
iteration : 12707
train acc:  0.859375
train loss:  0.3068852424621582
train gradient:  0.18285389633297133
iteration : 12708
train acc:  0.796875
train loss:  0.40498507022857666
train gradient:  0.23830046155365298
iteration : 12709
train acc:  0.8359375
train loss:  0.3925483226776123
train gradient:  0.2615306058893755
iteration : 12710
train acc:  0.8515625
train loss:  0.3290189504623413
train gradient:  0.16269212414853568
iteration : 12711
train acc:  0.8671875
train loss:  0.3308812975883484
train gradient:  0.1626852019758536
iteration : 12712
train acc:  0.8828125
train loss:  0.3105018734931946
train gradient:  0.1404108433370681
iteration : 12713
train acc:  0.8515625
train loss:  0.3640519380569458
train gradient:  0.19863352343700857
iteration : 12714
train acc:  0.8984375
train loss:  0.2804074287414551
train gradient:  0.11544269979756762
iteration : 12715
train acc:  0.890625
train loss:  0.23223504424095154
train gradient:  0.13869612918943303
iteration : 12716
train acc:  0.8515625
train loss:  0.3285782039165497
train gradient:  0.13363641110908198
iteration : 12717
train acc:  0.84375
train loss:  0.3925921320915222
train gradient:  0.2317600003241473
iteration : 12718
train acc:  0.8359375
train loss:  0.3156281113624573
train gradient:  0.23005176432772298
iteration : 12719
train acc:  0.859375
train loss:  0.3196353614330292
train gradient:  0.1714431474166196
iteration : 12720
train acc:  0.8359375
train loss:  0.3821602165699005
train gradient:  0.21818248404106533
iteration : 12721
train acc:  0.828125
train loss:  0.36795204877853394
train gradient:  0.175959792416361
iteration : 12722
train acc:  0.890625
train loss:  0.2707064151763916
train gradient:  0.1345673299838691
iteration : 12723
train acc:  0.8828125
train loss:  0.2605660855770111
train gradient:  0.10661097651885458
iteration : 12724
train acc:  0.8671875
train loss:  0.29394960403442383
train gradient:  0.14154339204544963
iteration : 12725
train acc:  0.859375
train loss:  0.33893531560897827
train gradient:  0.15333504809726062
iteration : 12726
train acc:  0.890625
train loss:  0.29164397716522217
train gradient:  0.11771198572373702
iteration : 12727
train acc:  0.8203125
train loss:  0.3823297619819641
train gradient:  0.15635406972369253
iteration : 12728
train acc:  0.8671875
train loss:  0.27942150831222534
train gradient:  0.10956044616023937
iteration : 12729
train acc:  0.890625
train loss:  0.27450239658355713
train gradient:  0.1300810556465135
iteration : 12730
train acc:  0.84375
train loss:  0.33476269245147705
train gradient:  0.16291294118090294
iteration : 12731
train acc:  0.8671875
train loss:  0.3417350649833679
train gradient:  0.20946373603950105
iteration : 12732
train acc:  0.8515625
train loss:  0.3410488963127136
train gradient:  0.16200932875861207
iteration : 12733
train acc:  0.859375
train loss:  0.31560325622558594
train gradient:  0.13701021154686427
iteration : 12734
train acc:  0.8984375
train loss:  0.26505810022354126
train gradient:  0.11746044535487775
iteration : 12735
train acc:  0.921875
train loss:  0.26710087060928345
train gradient:  0.09319239852688543
iteration : 12736
train acc:  0.8671875
train loss:  0.26179760694503784
train gradient:  0.11172511871890742
iteration : 12737
train acc:  0.8515625
train loss:  0.3758332133293152
train gradient:  0.18621928032841864
iteration : 12738
train acc:  0.859375
train loss:  0.29339852929115295
train gradient:  0.11290851419623653
iteration : 12739
train acc:  0.9140625
train loss:  0.22573710978031158
train gradient:  0.11423543361596879
iteration : 12740
train acc:  0.875
train loss:  0.25373244285583496
train gradient:  0.1046661906701489
iteration : 12741
train acc:  0.8671875
train loss:  0.33798885345458984
train gradient:  0.1640082852056585
iteration : 12742
train acc:  0.84375
train loss:  0.3588915467262268
train gradient:  0.15881869948738392
iteration : 12743
train acc:  0.875
train loss:  0.31365981698036194
train gradient:  0.1665514897120768
iteration : 12744
train acc:  0.8984375
train loss:  0.263834148645401
train gradient:  0.16979626907073453
iteration : 12745
train acc:  0.875
train loss:  0.26879262924194336
train gradient:  0.12336822309459348
iteration : 12746
train acc:  0.8984375
train loss:  0.2828017473220825
train gradient:  0.1220996299647401
iteration : 12747
train acc:  0.8203125
train loss:  0.3710196614265442
train gradient:  0.16122169380445667
iteration : 12748
train acc:  0.8046875
train loss:  0.39132779836654663
train gradient:  0.22957749993641566
iteration : 12749
train acc:  0.8671875
train loss:  0.3256322145462036
train gradient:  0.16336968990425213
iteration : 12750
train acc:  0.796875
train loss:  0.4221568703651428
train gradient:  0.22861162725659445
iteration : 12751
train acc:  0.921875
train loss:  0.2736918330192566
train gradient:  0.0774915264939502
iteration : 12752
train acc:  0.875
train loss:  0.3086735010147095
train gradient:  0.1440874187571057
iteration : 12753
train acc:  0.828125
train loss:  0.4302347004413605
train gradient:  0.20525618222906425
iteration : 12754
train acc:  0.875
train loss:  0.27974891662597656
train gradient:  0.11133077140562969
iteration : 12755
train acc:  0.90625
train loss:  0.25312674045562744
train gradient:  0.1402093712655737
iteration : 12756
train acc:  0.8671875
train loss:  0.3447001278400421
train gradient:  0.17651361044328945
iteration : 12757
train acc:  0.8515625
train loss:  0.32114094495773315
train gradient:  0.1396646670090279
iteration : 12758
train acc:  0.8828125
train loss:  0.27994632720947266
train gradient:  0.09333472856936505
iteration : 12759
train acc:  0.84375
train loss:  0.33331719040870667
train gradient:  0.10331522248127499
iteration : 12760
train acc:  0.8671875
train loss:  0.3537600636482239
train gradient:  0.16124726140631518
iteration : 12761
train acc:  0.8359375
train loss:  0.32678407430648804
train gradient:  0.2022774071744122
iteration : 12762
train acc:  0.8671875
train loss:  0.288882851600647
train gradient:  0.1549506992690609
iteration : 12763
train acc:  0.828125
train loss:  0.3808787167072296
train gradient:  0.24703684723514913
iteration : 12764
train acc:  0.859375
train loss:  0.30636101961135864
train gradient:  0.15709694567635732
iteration : 12765
train acc:  0.828125
train loss:  0.3472822308540344
train gradient:  0.17602819848812698
iteration : 12766
train acc:  0.890625
train loss:  0.2688426077365875
train gradient:  0.09140985015470465
iteration : 12767
train acc:  0.890625
train loss:  0.3187865614891052
train gradient:  0.10428458740723727
iteration : 12768
train acc:  0.90625
train loss:  0.21493248641490936
train gradient:  0.0759296605734237
iteration : 12769
train acc:  0.84375
train loss:  0.32763975858688354
train gradient:  0.1155480087657535
iteration : 12770
train acc:  0.828125
train loss:  0.42644089460372925
train gradient:  0.2649244413101078
iteration : 12771
train acc:  0.8984375
train loss:  0.2901531159877777
train gradient:  0.11655550137510486
iteration : 12772
train acc:  0.796875
train loss:  0.35551971197128296
train gradient:  0.16233425544090813
iteration : 12773
train acc:  0.875
train loss:  0.29807543754577637
train gradient:  0.11058620174491376
iteration : 12774
train acc:  0.875
train loss:  0.3212769031524658
train gradient:  0.14326868876522814
iteration : 12775
train acc:  0.921875
train loss:  0.23160526156425476
train gradient:  0.10134087049875237
iteration : 12776
train acc:  0.8984375
train loss:  0.2578008770942688
train gradient:  0.11528167878906706
iteration : 12777
train acc:  0.9296875
train loss:  0.2485165148973465
train gradient:  0.12137603585424092
iteration : 12778
train acc:  0.90625
train loss:  0.21956667304039001
train gradient:  0.18013803682806123
iteration : 12779
train acc:  0.8203125
train loss:  0.36949753761291504
train gradient:  0.2208151510919827
iteration : 12780
train acc:  0.8671875
train loss:  0.34653446078300476
train gradient:  0.1439643635128191
iteration : 12781
train acc:  0.8515625
train loss:  0.31789979338645935
train gradient:  0.151426425607107
iteration : 12782
train acc:  0.859375
train loss:  0.2673014998435974
train gradient:  0.1816197351387237
iteration : 12783
train acc:  0.875
train loss:  0.30532729625701904
train gradient:  0.13229062978953451
iteration : 12784
train acc:  0.84375
train loss:  0.33820587396621704
train gradient:  0.1375927758452618
iteration : 12785
train acc:  0.859375
train loss:  0.3365638256072998
train gradient:  0.15269802877034078
iteration : 12786
train acc:  0.8203125
train loss:  0.37594035267829895
train gradient:  0.18102655836993625
iteration : 12787
train acc:  0.8515625
train loss:  0.3472650945186615
train gradient:  0.1470499944148705
iteration : 12788
train acc:  0.859375
train loss:  0.3039534091949463
train gradient:  0.12184676175171241
iteration : 12789
train acc:  0.90625
train loss:  0.31189483404159546
train gradient:  0.11595382600044234
iteration : 12790
train acc:  0.859375
train loss:  0.3574714958667755
train gradient:  0.15921021977369243
iteration : 12791
train acc:  0.8125
train loss:  0.44882330298423767
train gradient:  0.24415305157879175
iteration : 12792
train acc:  0.859375
train loss:  0.33114683628082275
train gradient:  0.1427441207451582
iteration : 12793
train acc:  0.8828125
train loss:  0.28944921493530273
train gradient:  0.1233465977311626
iteration : 12794
train acc:  0.859375
train loss:  0.37183040380477905
train gradient:  0.1853805028132226
iteration : 12795
train acc:  0.8671875
train loss:  0.3028176426887512
train gradient:  0.10668218005528517
iteration : 12796
train acc:  0.8203125
train loss:  0.4576091468334198
train gradient:  0.2320860576188502
iteration : 12797
train acc:  0.8359375
train loss:  0.29749631881713867
train gradient:  0.1279021833018838
iteration : 12798
train acc:  0.8125
train loss:  0.3545280396938324
train gradient:  0.18968893273991264
iteration : 12799
train acc:  0.8828125
train loss:  0.26719164848327637
train gradient:  0.11620807863376743
iteration : 12800
train acc:  0.84375
train loss:  0.3148266077041626
train gradient:  0.13144454647559667
iteration : 12801
train acc:  0.828125
train loss:  0.40661734342575073
train gradient:  0.23992758695785543
iteration : 12802
train acc:  0.828125
train loss:  0.37045204639434814
train gradient:  0.16941210437940887
iteration : 12803
train acc:  0.859375
train loss:  0.33730190992355347
train gradient:  0.13192087241947742
iteration : 12804
train acc:  0.8125
train loss:  0.35858654975891113
train gradient:  0.1892004776912486
iteration : 12805
train acc:  0.8359375
train loss:  0.33411508798599243
train gradient:  0.13448102655401184
iteration : 12806
train acc:  0.8203125
train loss:  0.41232600808143616
train gradient:  0.1642232453241661
iteration : 12807
train acc:  0.875
train loss:  0.27248093485832214
train gradient:  0.10827322590583832
iteration : 12808
train acc:  0.921875
train loss:  0.23726323246955872
train gradient:  0.09387634626601042
iteration : 12809
train acc:  0.8515625
train loss:  0.32630905508995056
train gradient:  0.16528674757521272
iteration : 12810
train acc:  0.9140625
train loss:  0.24208733439445496
train gradient:  0.10099957322270715
iteration : 12811
train acc:  0.8515625
train loss:  0.3140145242214203
train gradient:  0.11944894005169464
iteration : 12812
train acc:  0.84375
train loss:  0.3387247920036316
train gradient:  0.18097607600783067
iteration : 12813
train acc:  0.875
train loss:  0.273229718208313
train gradient:  0.12031624875261719
iteration : 12814
train acc:  0.8984375
train loss:  0.2818438410758972
train gradient:  0.10424316676618295
iteration : 12815
train acc:  0.890625
train loss:  0.2864786386489868
train gradient:  0.11123350409441955
iteration : 12816
train acc:  0.890625
train loss:  0.26136863231658936
train gradient:  0.12763787404876303
iteration : 12817
train acc:  0.8671875
train loss:  0.271686851978302
train gradient:  0.10209047456433942
iteration : 12818
train acc:  0.875
train loss:  0.29208675026893616
train gradient:  0.08761538807351137
iteration : 12819
train acc:  0.9140625
train loss:  0.2769860029220581
train gradient:  0.09988827898315071
iteration : 12820
train acc:  0.84375
train loss:  0.3843920826911926
train gradient:  0.20172325496240862
iteration : 12821
train acc:  0.84375
train loss:  0.2892487645149231
train gradient:  0.1068282587918011
iteration : 12822
train acc:  0.9140625
train loss:  0.23248857259750366
train gradient:  0.08524095962349053
iteration : 12823
train acc:  0.859375
train loss:  0.3249252438545227
train gradient:  0.14851323648254533
iteration : 12824
train acc:  0.9296875
train loss:  0.24492183327674866
train gradient:  0.11163802595806402
iteration : 12825
train acc:  0.859375
train loss:  0.32140809297561646
train gradient:  0.15372015738769604
iteration : 12826
train acc:  0.875
train loss:  0.2953476011753082
train gradient:  0.18779500573732372
iteration : 12827
train acc:  0.8046875
train loss:  0.3755134046077728
train gradient:  0.21350177296803016
iteration : 12828
train acc:  0.8671875
train loss:  0.34690988063812256
train gradient:  0.15406653107557433
iteration : 12829
train acc:  0.8828125
train loss:  0.30408692359924316
train gradient:  0.11619571569234091
iteration : 12830
train acc:  0.84375
train loss:  0.32518941164016724
train gradient:  0.19096567745549617
iteration : 12831
train acc:  0.8671875
train loss:  0.27594703435897827
train gradient:  0.10318162063997774
iteration : 12832
train acc:  0.84375
train loss:  0.33570927381515503
train gradient:  0.1457733113158865
iteration : 12833
train acc:  0.8125
train loss:  0.4605478346347809
train gradient:  0.2520824293376297
iteration : 12834
train acc:  0.8671875
train loss:  0.3099375367164612
train gradient:  0.16451263263104327
iteration : 12835
train acc:  0.875
train loss:  0.3244049549102783
train gradient:  0.10314721957576514
iteration : 12836
train acc:  0.8671875
train loss:  0.33152252435684204
train gradient:  0.14548972399740412
iteration : 12837
train acc:  0.875
train loss:  0.3010549247264862
train gradient:  0.1696128113196021
iteration : 12838
train acc:  0.8828125
train loss:  0.29355311393737793
train gradient:  0.10421421054406128
iteration : 12839
train acc:  0.859375
train loss:  0.32997480034828186
train gradient:  0.10060895520117853
iteration : 12840
train acc:  0.8359375
train loss:  0.3461650013923645
train gradient:  0.15410114703295083
iteration : 12841
train acc:  0.875
train loss:  0.34920018911361694
train gradient:  0.15454110067591081
iteration : 12842
train acc:  0.8515625
train loss:  0.311205118894577
train gradient:  0.13164716011104483
iteration : 12843
train acc:  0.8359375
train loss:  0.38721227645874023
train gradient:  0.10912663908866725
iteration : 12844
train acc:  0.859375
train loss:  0.35445570945739746
train gradient:  0.13044545537811789
iteration : 12845
train acc:  0.859375
train loss:  0.35602137446403503
train gradient:  0.1125767470566933
iteration : 12846
train acc:  0.875
train loss:  0.3541903495788574
train gradient:  0.12204369069955326
iteration : 12847
train acc:  0.8828125
train loss:  0.2932540476322174
train gradient:  0.08307124991511111
iteration : 12848
train acc:  0.859375
train loss:  0.2913427948951721
train gradient:  0.11475535433852914
iteration : 12849
train acc:  0.8203125
train loss:  0.35358455777168274
train gradient:  0.1813680994910974
iteration : 12850
train acc:  0.90625
train loss:  0.2926834225654602
train gradient:  0.12043460058074966
iteration : 12851
train acc:  0.9453125
train loss:  0.2098141312599182
train gradient:  0.09447034024091831
iteration : 12852
train acc:  0.8515625
train loss:  0.3320373296737671
train gradient:  0.12585868053371868
iteration : 12853
train acc:  0.8671875
train loss:  0.28554537892341614
train gradient:  0.15337037253496533
iteration : 12854
train acc:  0.90625
train loss:  0.262856662273407
train gradient:  0.08783968871579571
iteration : 12855
train acc:  0.8671875
train loss:  0.2756189703941345
train gradient:  0.11583814957875424
iteration : 12856
train acc:  0.8203125
train loss:  0.34832763671875
train gradient:  0.23173660782778335
iteration : 12857
train acc:  0.9140625
train loss:  0.24436117708683014
train gradient:  0.1080762195708585
iteration : 12858
train acc:  0.84375
train loss:  0.3721402883529663
train gradient:  0.13211358351378577
iteration : 12859
train acc:  0.8359375
train loss:  0.3419637084007263
train gradient:  0.13847710491125031
iteration : 12860
train acc:  0.8984375
train loss:  0.24426445364952087
train gradient:  0.08504892398861871
iteration : 12861
train acc:  0.890625
train loss:  0.3323066234588623
train gradient:  0.1280349815897864
iteration : 12862
train acc:  0.828125
train loss:  0.415476530790329
train gradient:  0.28167285363223926
iteration : 12863
train acc:  0.875
train loss:  0.32485735416412354
train gradient:  0.16333298859342738
iteration : 12864
train acc:  0.8515625
train loss:  0.3021090030670166
train gradient:  0.10050291259132356
iteration : 12865
train acc:  0.8828125
train loss:  0.3308388888835907
train gradient:  0.12868369389676437
iteration : 12866
train acc:  0.8515625
train loss:  0.3316033184528351
train gradient:  0.1486456463041064
iteration : 12867
train acc:  0.921875
train loss:  0.23186475038528442
train gradient:  0.09780572224580115
iteration : 12868
train acc:  0.9140625
train loss:  0.27525991201400757
train gradient:  0.0923960103369764
iteration : 12869
train acc:  0.8359375
train loss:  0.3683158755302429
train gradient:  0.18051455416676232
iteration : 12870
train acc:  0.84375
train loss:  0.35452115535736084
train gradient:  0.2067026071889131
iteration : 12871
train acc:  0.8359375
train loss:  0.3045136630535126
train gradient:  0.1473226685113334
iteration : 12872
train acc:  0.8515625
train loss:  0.3776118755340576
train gradient:  0.20221283176268567
iteration : 12873
train acc:  0.859375
train loss:  0.2876328229904175
train gradient:  0.10591207176002797
iteration : 12874
train acc:  0.8984375
train loss:  0.25951001048088074
train gradient:  0.07774660373175739
iteration : 12875
train acc:  0.875
train loss:  0.26934191584587097
train gradient:  0.10724021042460015
iteration : 12876
train acc:  0.875
train loss:  0.3429502546787262
train gradient:  0.12403895128114505
iteration : 12877
train acc:  0.828125
train loss:  0.34037038683891296
train gradient:  0.1281235816418202
iteration : 12878
train acc:  0.78125
train loss:  0.5344522595405579
train gradient:  0.31669997895895013
iteration : 12879
train acc:  0.875
train loss:  0.264664888381958
train gradient:  0.15251460198745478
iteration : 12880
train acc:  0.8359375
train loss:  0.36752942204475403
train gradient:  0.15120761769700178
iteration : 12881
train acc:  0.90625
train loss:  0.26395413279533386
train gradient:  0.1513437986547334
iteration : 12882
train acc:  0.8671875
train loss:  0.29947933554649353
train gradient:  0.1830389049874634
iteration : 12883
train acc:  0.8671875
train loss:  0.3749086260795593
train gradient:  0.16739608687110374
iteration : 12884
train acc:  0.8515625
train loss:  0.4310868978500366
train gradient:  0.2341627770508922
iteration : 12885
train acc:  0.875
train loss:  0.3049391508102417
train gradient:  0.160293688561022
iteration : 12886
train acc:  0.859375
train loss:  0.2997056841850281
train gradient:  0.16071795514451823
iteration : 12887
train acc:  0.84375
train loss:  0.2984023094177246
train gradient:  0.09457473295075843
iteration : 12888
train acc:  0.8828125
train loss:  0.29241639375686646
train gradient:  0.13540370149695846
iteration : 12889
train acc:  0.875
train loss:  0.307526171207428
train gradient:  0.14144561666476607
iteration : 12890
train acc:  0.890625
train loss:  0.2425759732723236
train gradient:  0.09807389783487724
iteration : 12891
train acc:  0.875
train loss:  0.31445443630218506
train gradient:  0.11942416531643793
iteration : 12892
train acc:  0.8359375
train loss:  0.368333101272583
train gradient:  0.22434017009158957
iteration : 12893
train acc:  0.90625
train loss:  0.24310949444770813
train gradient:  0.0835417132400223
iteration : 12894
train acc:  0.8125
train loss:  0.4630391001701355
train gradient:  0.26180572318570455
iteration : 12895
train acc:  0.84375
train loss:  0.33350881934165955
train gradient:  0.16048096631172087
iteration : 12896
train acc:  0.890625
train loss:  0.24421149492263794
train gradient:  0.14047461101604938
iteration : 12897
train acc:  0.8828125
train loss:  0.2906407117843628
train gradient:  0.09096707536324583
iteration : 12898
train acc:  0.8984375
train loss:  0.27723896503448486
train gradient:  0.10151349400006143
iteration : 12899
train acc:  0.8515625
train loss:  0.35659259557724
train gradient:  0.1949916632262706
iteration : 12900
train acc:  0.8515625
train loss:  0.34165701270103455
train gradient:  0.14418545278416994
iteration : 12901
train acc:  0.84375
train loss:  0.3472069501876831
train gradient:  0.15544262170085466
iteration : 12902
train acc:  0.8671875
train loss:  0.32531535625457764
train gradient:  0.18625314712717006
iteration : 12903
train acc:  0.8359375
train loss:  0.3993280529975891
train gradient:  0.16398897162179932
iteration : 12904
train acc:  0.8671875
train loss:  0.307139128446579
train gradient:  0.18254572250216936
iteration : 12905
train acc:  0.84375
train loss:  0.36579227447509766
train gradient:  0.16265509416780394
iteration : 12906
train acc:  0.8359375
train loss:  0.3290482461452484
train gradient:  0.15324591726074416
iteration : 12907
train acc:  0.90625
train loss:  0.2712956666946411
train gradient:  0.1162586001608921
iteration : 12908
train acc:  0.8515625
train loss:  0.35204845666885376
train gradient:  0.21264418675301616
iteration : 12909
train acc:  0.890625
train loss:  0.2655470371246338
train gradient:  0.07249835314587658
iteration : 12910
train acc:  0.8515625
train loss:  0.3202100098133087
train gradient:  0.13131050792450105
iteration : 12911
train acc:  0.875
train loss:  0.37341785430908203
train gradient:  0.1483029269267131
iteration : 12912
train acc:  0.8359375
train loss:  0.34630486369132996
train gradient:  0.2150360474413756
iteration : 12913
train acc:  0.859375
train loss:  0.26873451471328735
train gradient:  0.06743262098456276
iteration : 12914
train acc:  0.875
train loss:  0.2962702512741089
train gradient:  0.15740834131543782
iteration : 12915
train acc:  0.875
train loss:  0.2668215036392212
train gradient:  0.11941784144426142
iteration : 12916
train acc:  0.8828125
train loss:  0.26491671800613403
train gradient:  0.15051533546284998
iteration : 12917
train acc:  0.875
train loss:  0.30403411388397217
train gradient:  0.11668865558067025
iteration : 12918
train acc:  0.875
train loss:  0.31497329473495483
train gradient:  0.15768169419193728
iteration : 12919
train acc:  0.828125
train loss:  0.38264259696006775
train gradient:  0.25507752148465
iteration : 12920
train acc:  0.828125
train loss:  0.3718496263027191
train gradient:  0.21878308936326438
iteration : 12921
train acc:  0.90625
train loss:  0.3055174648761749
train gradient:  0.1142520073848712
iteration : 12922
train acc:  0.8984375
train loss:  0.25510454177856445
train gradient:  0.09252541800598425
iteration : 12923
train acc:  0.921875
train loss:  0.24960960447788239
train gradient:  0.12413857556730395
iteration : 12924
train acc:  0.8671875
train loss:  0.33266282081604004
train gradient:  0.11886816876137153
iteration : 12925
train acc:  0.84375
train loss:  0.3650161623954773
train gradient:  0.22084259892494337
iteration : 12926
train acc:  0.8828125
train loss:  0.25162285566329956
train gradient:  0.10745972246070441
iteration : 12927
train acc:  0.875
train loss:  0.3574090600013733
train gradient:  0.17254347571212886
iteration : 12928
train acc:  0.890625
train loss:  0.3287535011768341
train gradient:  0.14732531285140057
iteration : 12929
train acc:  0.90625
train loss:  0.23525656759738922
train gradient:  0.07875784448309812
iteration : 12930
train acc:  0.90625
train loss:  0.2550559937953949
train gradient:  0.10741314163314625
iteration : 12931
train acc:  0.796875
train loss:  0.41650792956352234
train gradient:  0.23310219146152608
iteration : 12932
train acc:  0.828125
train loss:  0.3948866128921509
train gradient:  0.2981361895149826
iteration : 12933
train acc:  0.875
train loss:  0.26730895042419434
train gradient:  0.12719010312023024
iteration : 12934
train acc:  0.8359375
train loss:  0.498263418674469
train gradient:  0.3209564992410789
iteration : 12935
train acc:  0.8828125
train loss:  0.291361004114151
train gradient:  0.13175887177621232
iteration : 12936
train acc:  0.9140625
train loss:  0.2553364634513855
train gradient:  0.10104632832666498
iteration : 12937
train acc:  0.796875
train loss:  0.3554910719394684
train gradient:  0.21638728159455423
iteration : 12938
train acc:  0.8671875
train loss:  0.33670616149902344
train gradient:  0.24117927017516272
iteration : 12939
train acc:  0.8828125
train loss:  0.25748980045318604
train gradient:  0.10200483985848874
iteration : 12940
train acc:  0.8671875
train loss:  0.24839401245117188
train gradient:  0.09553763483879768
iteration : 12941
train acc:  0.8828125
train loss:  0.28121694922447205
train gradient:  0.09581414605356452
iteration : 12942
train acc:  0.875
train loss:  0.259152889251709
train gradient:  0.12541608996442144
iteration : 12943
train acc:  0.8359375
train loss:  0.3849329352378845
train gradient:  0.176134418088292
iteration : 12944
train acc:  0.8515625
train loss:  0.3145020604133606
train gradient:  0.19317880940280763
iteration : 12945
train acc:  0.890625
train loss:  0.32002440094947815
train gradient:  0.11479157290032692
iteration : 12946
train acc:  0.875
train loss:  0.28425002098083496
train gradient:  0.16275190999485303
iteration : 12947
train acc:  0.9140625
train loss:  0.2234421670436859
train gradient:  0.07218098511770055
iteration : 12948
train acc:  0.8359375
train loss:  0.3766336441040039
train gradient:  0.18756863671217755
iteration : 12949
train acc:  0.9140625
train loss:  0.2701573371887207
train gradient:  0.13394485105476564
iteration : 12950
train acc:  0.8125
train loss:  0.4561377465724945
train gradient:  0.37307739735654166
iteration : 12951
train acc:  0.8828125
train loss:  0.2850717306137085
train gradient:  0.15816116857702908
iteration : 12952
train acc:  0.8359375
train loss:  0.3489336371421814
train gradient:  0.1808014971722921
iteration : 12953
train acc:  0.7890625
train loss:  0.4060787558555603
train gradient:  0.17619452389338894
iteration : 12954
train acc:  0.828125
train loss:  0.37105792760849
train gradient:  0.1476000256280101
iteration : 12955
train acc:  0.8828125
train loss:  0.29087141156196594
train gradient:  0.15080628639293991
iteration : 12956
train acc:  0.875
train loss:  0.30780962109565735
train gradient:  0.1526574089719634
iteration : 12957
train acc:  0.90625
train loss:  0.2838028371334076
train gradient:  0.09834087538131335
iteration : 12958
train acc:  0.9296875
train loss:  0.22570039331912994
train gradient:  0.11693736713838704
iteration : 12959
train acc:  0.8828125
train loss:  0.27786222100257874
train gradient:  0.11651403897864271
iteration : 12960
train acc:  0.9375
train loss:  0.23756226897239685
train gradient:  0.06533359476849479
iteration : 12961
train acc:  0.8515625
train loss:  0.3340367078781128
train gradient:  0.13845640600774908
iteration : 12962
train acc:  0.8359375
train loss:  0.47202056646347046
train gradient:  0.40980443119658977
iteration : 12963
train acc:  0.875
train loss:  0.29447266459465027
train gradient:  0.08723978820671997
iteration : 12964
train acc:  0.8671875
train loss:  0.39453476667404175
train gradient:  0.19328820716583028
iteration : 12965
train acc:  0.875
train loss:  0.28805291652679443
train gradient:  0.11096289110503038
iteration : 12966
train acc:  0.8828125
train loss:  0.3006797730922699
train gradient:  0.10195173032202884
iteration : 12967
train acc:  0.796875
train loss:  0.4280607998371124
train gradient:  0.2121299410199132
iteration : 12968
train acc:  0.859375
train loss:  0.3419678807258606
train gradient:  0.16023028192429006
iteration : 12969
train acc:  0.8359375
train loss:  0.3370351791381836
train gradient:  0.1402363800594455
iteration : 12970
train acc:  0.859375
train loss:  0.3312731385231018
train gradient:  0.17206423146256744
iteration : 12971
train acc:  0.828125
train loss:  0.3594334125518799
train gradient:  0.2668090015968252
iteration : 12972
train acc:  0.890625
train loss:  0.25308042764663696
train gradient:  0.14477112733204198
iteration : 12973
train acc:  0.84375
train loss:  0.37045082449913025
train gradient:  0.16628002033826456
iteration : 12974
train acc:  0.859375
train loss:  0.3172537684440613
train gradient:  0.23358243772517567
iteration : 12975
train acc:  0.84375
train loss:  0.3786157965660095
train gradient:  0.22180082329366718
iteration : 12976
train acc:  0.84375
train loss:  0.2914000153541565
train gradient:  0.14046706412871512
iteration : 12977
train acc:  0.8046875
train loss:  0.3904045820236206
train gradient:  0.23325796217110442
iteration : 12978
train acc:  0.84375
train loss:  0.34895479679107666
train gradient:  0.1523904600580946
iteration : 12979
train acc:  0.875
train loss:  0.3067512512207031
train gradient:  0.19430840259356824
iteration : 12980
train acc:  0.8203125
train loss:  0.36054617166519165
train gradient:  0.17587130582767818
iteration : 12981
train acc:  0.859375
train loss:  0.2960352599620819
train gradient:  0.1321048790504346
iteration : 12982
train acc:  0.8515625
train loss:  0.35922664403915405
train gradient:  0.135787944622637
iteration : 12983
train acc:  0.9296875
train loss:  0.22330176830291748
train gradient:  0.09338178032939351
iteration : 12984
train acc:  0.8046875
train loss:  0.32914772629737854
train gradient:  0.11199608477528224
iteration : 12985
train acc:  0.84375
train loss:  0.3005852699279785
train gradient:  0.11748318550194879
iteration : 12986
train acc:  0.8359375
train loss:  0.31948667764663696
train gradient:  0.20720582429831844
iteration : 12987
train acc:  0.8671875
train loss:  0.3208385109901428
train gradient:  0.15671361004563047
iteration : 12988
train acc:  0.828125
train loss:  0.3011248707771301
train gradient:  0.1305242378312741
iteration : 12989
train acc:  0.8671875
train loss:  0.27726051211357117
train gradient:  0.10003291808614458
iteration : 12990
train acc:  0.90625
train loss:  0.27850109338760376
train gradient:  0.1364570285896472
iteration : 12991
train acc:  0.8671875
train loss:  0.290404349565506
train gradient:  0.09322476763283022
iteration : 12992
train acc:  0.8984375
train loss:  0.2536352276802063
train gradient:  0.09661627971485018
iteration : 12993
train acc:  0.859375
train loss:  0.30521225929260254
train gradient:  0.19918793700655762
iteration : 12994
train acc:  0.921875
train loss:  0.1956036537885666
train gradient:  0.06902383740879864
iteration : 12995
train acc:  0.8515625
train loss:  0.35113829374313354
train gradient:  0.14775953778098344
iteration : 12996
train acc:  0.84375
train loss:  0.45071789622306824
train gradient:  0.2941898329656694
iteration : 12997
train acc:  0.828125
train loss:  0.4036303162574768
train gradient:  0.23245659603621788
iteration : 12998
train acc:  0.828125
train loss:  0.3675987124443054
train gradient:  0.22263851218174868
iteration : 12999
train acc:  0.8671875
train loss:  0.3133431077003479
train gradient:  0.12260034140267621
iteration : 13000
train acc:  0.84375
train loss:  0.3378939926624298
train gradient:  0.12522191654747653
iteration : 13001
train acc:  0.875
train loss:  0.3072531819343567
train gradient:  0.12284038897384474
iteration : 13002
train acc:  0.875
train loss:  0.24585099518299103
train gradient:  0.08168169533323198
iteration : 13003
train acc:  0.8984375
train loss:  0.3232996463775635
train gradient:  0.13349637406880704
iteration : 13004
train acc:  0.8828125
train loss:  0.25165459513664246
train gradient:  0.10571218718259343
iteration : 13005
train acc:  0.90625
train loss:  0.2674809396266937
train gradient:  0.13314896554430505
iteration : 13006
train acc:  0.8828125
train loss:  0.2679130434989929
train gradient:  0.12434718841635929
iteration : 13007
train acc:  0.8828125
train loss:  0.2731980085372925
train gradient:  0.11705101546733705
iteration : 13008
train acc:  0.890625
train loss:  0.22516871988773346
train gradient:  0.10748243887202642
iteration : 13009
train acc:  0.796875
train loss:  0.4685150980949402
train gradient:  0.26309925202786716
iteration : 13010
train acc:  0.921875
train loss:  0.26775285601615906
train gradient:  0.09375673094852953
iteration : 13011
train acc:  0.859375
train loss:  0.33250904083251953
train gradient:  0.15058883715346827
iteration : 13012
train acc:  0.828125
train loss:  0.3707212209701538
train gradient:  0.19764135078230155
iteration : 13013
train acc:  0.8515625
train loss:  0.36670854687690735
train gradient:  0.12211408199191369
iteration : 13014
train acc:  0.890625
train loss:  0.2904530465602875
train gradient:  0.14610674552971725
iteration : 13015
train acc:  0.8828125
train loss:  0.24496813118457794
train gradient:  0.18333646967030154
iteration : 13016
train acc:  0.8828125
train loss:  0.269596129655838
train gradient:  0.13418009211221113
iteration : 13017
train acc:  0.8828125
train loss:  0.27086156606674194
train gradient:  0.10949461308812829
iteration : 13018
train acc:  0.8984375
train loss:  0.2650502026081085
train gradient:  0.082964496011381
iteration : 13019
train acc:  0.9140625
train loss:  0.26496750116348267
train gradient:  0.10227496293866456
iteration : 13020
train acc:  0.8515625
train loss:  0.3421919047832489
train gradient:  0.18145809164839827
iteration : 13021
train acc:  0.8203125
train loss:  0.36111971735954285
train gradient:  0.14144823852052654
iteration : 13022
train acc:  0.875
train loss:  0.3464961349964142
train gradient:  0.16971490293785527
iteration : 13023
train acc:  0.8671875
train loss:  0.2957603335380554
train gradient:  0.16208444137411857
iteration : 13024
train acc:  0.859375
train loss:  0.3425056040287018
train gradient:  0.17539775677959907
iteration : 13025
train acc:  0.8203125
train loss:  0.3474723696708679
train gradient:  0.11248852161558967
iteration : 13026
train acc:  0.875
train loss:  0.2869817912578583
train gradient:  0.12544058288476762
iteration : 13027
train acc:  0.7890625
train loss:  0.48620861768722534
train gradient:  0.34055902159129703
iteration : 13028
train acc:  0.78125
train loss:  0.41936492919921875
train gradient:  0.17954624650893602
iteration : 13029
train acc:  0.8515625
train loss:  0.29777881503105164
train gradient:  0.12983774089566652
iteration : 13030
train acc:  0.8046875
train loss:  0.3766367435455322
train gradient:  0.2464917203599301
iteration : 13031
train acc:  0.8515625
train loss:  0.2935679256916046
train gradient:  0.14578430183031968
iteration : 13032
train acc:  0.8828125
train loss:  0.3212276101112366
train gradient:  0.21380783046010435
iteration : 13033
train acc:  0.8515625
train loss:  0.31371641159057617
train gradient:  0.11678381392941561
iteration : 13034
train acc:  0.8515625
train loss:  0.2815209627151489
train gradient:  0.10297631318271146
iteration : 13035
train acc:  0.8515625
train loss:  0.3782361149787903
train gradient:  0.23063006443018447
iteration : 13036
train acc:  0.8203125
train loss:  0.3958163857460022
train gradient:  0.17091648236740983
iteration : 13037
train acc:  0.875
train loss:  0.29413536190986633
train gradient:  0.13359193646414194
iteration : 13038
train acc:  0.859375
train loss:  0.3339594006538391
train gradient:  0.12026367285222113
iteration : 13039
train acc:  0.90625
train loss:  0.2654254138469696
train gradient:  0.08996788504912212
iteration : 13040
train acc:  0.875
train loss:  0.34624728560447693
train gradient:  0.2515050210707953
iteration : 13041
train acc:  0.875
train loss:  0.3167966604232788
train gradient:  0.16217913222011077
iteration : 13042
train acc:  0.875
train loss:  0.30514469742774963
train gradient:  0.08858569289469878
iteration : 13043
train acc:  0.8359375
train loss:  0.31333938241004944
train gradient:  0.14854351773537505
iteration : 13044
train acc:  0.875
train loss:  0.3639609217643738
train gradient:  0.18156226275124582
iteration : 13045
train acc:  0.8828125
train loss:  0.28792858123779297
train gradient:  0.10121261586453832
iteration : 13046
train acc:  0.8828125
train loss:  0.29227375984191895
train gradient:  0.11284865053312834
iteration : 13047
train acc:  0.78125
train loss:  0.40204688906669617
train gradient:  0.15024467414596537
iteration : 13048
train acc:  0.859375
train loss:  0.3071605861186981
train gradient:  0.13510706330366679
iteration : 13049
train acc:  0.875
train loss:  0.352933406829834
train gradient:  0.1700506134327583
iteration : 13050
train acc:  0.8515625
train loss:  0.3211156725883484
train gradient:  0.15787852410729886
iteration : 13051
train acc:  0.859375
train loss:  0.286883145570755
train gradient:  0.08328933308801027
iteration : 13052
train acc:  0.8984375
train loss:  0.2880609631538391
train gradient:  0.10026824127519891
iteration : 13053
train acc:  0.8828125
train loss:  0.30935871601104736
train gradient:  0.1110117472057957
iteration : 13054
train acc:  0.8671875
train loss:  0.2775682806968689
train gradient:  0.11851767027755895
iteration : 13055
train acc:  0.859375
train loss:  0.28410565853118896
train gradient:  0.13615025094675598
iteration : 13056
train acc:  0.890625
train loss:  0.2500978112220764
train gradient:  0.08103424892034555
iteration : 13057
train acc:  0.890625
train loss:  0.31301188468933105
train gradient:  0.12497323437650464
iteration : 13058
train acc:  0.8125
train loss:  0.4147465229034424
train gradient:  0.1368021374568901
iteration : 13059
train acc:  0.8671875
train loss:  0.33252155780792236
train gradient:  0.16286253498592185
iteration : 13060
train acc:  0.890625
train loss:  0.3458934426307678
train gradient:  0.13800863677133848
iteration : 13061
train acc:  0.859375
train loss:  0.32614636421203613
train gradient:  0.13133312831285426
iteration : 13062
train acc:  0.8359375
train loss:  0.34290042519569397
train gradient:  0.17568734345948356
iteration : 13063
train acc:  0.78125
train loss:  0.46892961859703064
train gradient:  0.20563053522677813
iteration : 13064
train acc:  0.84375
train loss:  0.3961961567401886
train gradient:  0.1925598233069914
iteration : 13065
train acc:  0.8046875
train loss:  0.45451727509498596
train gradient:  0.23764460136859605
iteration : 13066
train acc:  0.8125
train loss:  0.4394974708557129
train gradient:  0.229967056636544
iteration : 13067
train acc:  0.8359375
train loss:  0.3592330813407898
train gradient:  0.20153632844667785
iteration : 13068
train acc:  0.8984375
train loss:  0.3018389344215393
train gradient:  0.12110900393555911
iteration : 13069
train acc:  0.8828125
train loss:  0.2869974374771118
train gradient:  0.1337485845289773
iteration : 13070
train acc:  0.84375
train loss:  0.33161622285842896
train gradient:  0.13546067302232967
iteration : 13071
train acc:  0.9296875
train loss:  0.23747612535953522
train gradient:  0.0728055003205062
iteration : 13072
train acc:  0.8828125
train loss:  0.30780744552612305
train gradient:  0.11012160198414422
iteration : 13073
train acc:  0.921875
train loss:  0.2102769911289215
train gradient:  0.07904090496761938
iteration : 13074
train acc:  0.84375
train loss:  0.3091897666454315
train gradient:  0.1595241975711526
iteration : 13075
train acc:  0.84375
train loss:  0.3073927164077759
train gradient:  0.09759544463568823
iteration : 13076
train acc:  0.7734375
train loss:  0.4820946455001831
train gradient:  0.30376846623055537
iteration : 13077
train acc:  0.9296875
train loss:  0.22604167461395264
train gradient:  0.09013844361189269
iteration : 13078
train acc:  0.8203125
train loss:  0.4108167886734009
train gradient:  0.18638024729473837
iteration : 13079
train acc:  0.84375
train loss:  0.3436063528060913
train gradient:  0.293601722821207
iteration : 13080
train acc:  0.8671875
train loss:  0.4009414613246918
train gradient:  0.22629821022794433
iteration : 13081
train acc:  0.828125
train loss:  0.37855491042137146
train gradient:  0.14131098377002443
iteration : 13082
train acc:  0.859375
train loss:  0.30284252762794495
train gradient:  0.11647943625537895
iteration : 13083
train acc:  0.890625
train loss:  0.3117467761039734
train gradient:  0.10042972085140267
iteration : 13084
train acc:  0.8125
train loss:  0.4157550036907196
train gradient:  0.215985760639392
iteration : 13085
train acc:  0.8671875
train loss:  0.3515093922615051
train gradient:  0.1458767157594904
iteration : 13086
train acc:  0.8671875
train loss:  0.28091928362846375
train gradient:  0.1543531840780164
iteration : 13087
train acc:  0.9140625
train loss:  0.23947153985500336
train gradient:  0.12232902580464709
iteration : 13088
train acc:  0.8125
train loss:  0.32998818159103394
train gradient:  0.24142099026161823
iteration : 13089
train acc:  0.875
train loss:  0.25650742650032043
train gradient:  0.10453169885469096
iteration : 13090
train acc:  0.859375
train loss:  0.3648422956466675
train gradient:  0.2049442129143813
iteration : 13091
train acc:  0.8125
train loss:  0.4061892330646515
train gradient:  0.1546415933759661
iteration : 13092
train acc:  0.890625
train loss:  0.3218282461166382
train gradient:  0.13743636683701488
iteration : 13093
train acc:  0.8359375
train loss:  0.31246086955070496
train gradient:  0.0988505860996377
iteration : 13094
train acc:  0.8828125
train loss:  0.277526319026947
train gradient:  0.09515009563489316
iteration : 13095
train acc:  0.828125
train loss:  0.38556796312332153
train gradient:  0.1525579284798482
iteration : 13096
train acc:  0.8515625
train loss:  0.307273268699646
train gradient:  0.10196197960653879
iteration : 13097
train acc:  0.8984375
train loss:  0.2563127279281616
train gradient:  0.13779186007053712
iteration : 13098
train acc:  0.890625
train loss:  0.27445024251937866
train gradient:  0.11603097966743794
iteration : 13099
train acc:  0.8359375
train loss:  0.3172948658466339
train gradient:  0.10270191104676699
iteration : 13100
train acc:  0.90625
train loss:  0.2710511088371277
train gradient:  0.10030920950621917
iteration : 13101
train acc:  0.921875
train loss:  0.24944330751895905
train gradient:  0.10235258174086544
iteration : 13102
train acc:  0.890625
train loss:  0.23871418833732605
train gradient:  0.06556477632607849
iteration : 13103
train acc:  0.890625
train loss:  0.2621254622936249
train gradient:  0.08435655272207514
iteration : 13104
train acc:  0.8359375
train loss:  0.40900152921676636
train gradient:  0.20220438183376846
iteration : 13105
train acc:  0.859375
train loss:  0.3774169087409973
train gradient:  0.37958058471940415
iteration : 13106
train acc:  0.828125
train loss:  0.26364195346832275
train gradient:  0.13452655365326172
iteration : 13107
train acc:  0.8671875
train loss:  0.2877931594848633
train gradient:  0.09759472265990952
iteration : 13108
train acc:  0.8359375
train loss:  0.4400927424430847
train gradient:  0.2466469257252946
iteration : 13109
train acc:  0.890625
train loss:  0.2747530937194824
train gradient:  0.15065866161294247
iteration : 13110
train acc:  0.9375
train loss:  0.23588088154792786
train gradient:  0.12069800139964038
iteration : 13111
train acc:  0.8671875
train loss:  0.2986919581890106
train gradient:  0.11108086934330896
iteration : 13112
train acc:  0.7734375
train loss:  0.43441903591156006
train gradient:  0.22179141871106633
iteration : 13113
train acc:  0.8828125
train loss:  0.31933319568634033
train gradient:  0.14066256983766925
iteration : 13114
train acc:  0.859375
train loss:  0.37068110704421997
train gradient:  0.212301544959581
iteration : 13115
train acc:  0.8828125
train loss:  0.2850627303123474
train gradient:  0.17556682024442533
iteration : 13116
train acc:  0.875
train loss:  0.305320680141449
train gradient:  0.10463583916174639
iteration : 13117
train acc:  0.859375
train loss:  0.3241311311721802
train gradient:  0.17028531338915975
iteration : 13118
train acc:  0.8515625
train loss:  0.3592609167098999
train gradient:  0.1748516341949872
iteration : 13119
train acc:  0.90625
train loss:  0.2783607840538025
train gradient:  0.11170923849133761
iteration : 13120
train acc:  0.828125
train loss:  0.3563220202922821
train gradient:  0.23022653146865513
iteration : 13121
train acc:  0.7890625
train loss:  0.4661123752593994
train gradient:  0.23568651380791933
iteration : 13122
train acc:  0.8828125
train loss:  0.3321133255958557
train gradient:  0.12618644096190917
iteration : 13123
train acc:  0.8671875
train loss:  0.2904031276702881
train gradient:  0.08315123647554527
iteration : 13124
train acc:  0.84375
train loss:  0.410549521446228
train gradient:  0.2716627018802192
iteration : 13125
train acc:  0.828125
train loss:  0.3809242248535156
train gradient:  0.16624581360093516
iteration : 13126
train acc:  0.8515625
train loss:  0.3368251323699951
train gradient:  0.14821850447125123
iteration : 13127
train acc:  0.875
train loss:  0.3481617569923401
train gradient:  0.11965407025891434
iteration : 13128
train acc:  0.828125
train loss:  0.3581618666648865
train gradient:  0.1657029373581046
iteration : 13129
train acc:  0.8984375
train loss:  0.2490374594926834
train gradient:  0.08676027851022297
iteration : 13130
train acc:  0.875
train loss:  0.3193075656890869
train gradient:  0.14342961387026892
iteration : 13131
train acc:  0.9140625
train loss:  0.2569686770439148
train gradient:  0.07192666457500288
iteration : 13132
train acc:  0.90625
train loss:  0.245733842253685
train gradient:  0.1247545603956361
iteration : 13133
train acc:  0.8515625
train loss:  0.38425910472869873
train gradient:  0.17562227190484542
iteration : 13134
train acc:  0.84375
train loss:  0.3241536021232605
train gradient:  0.17947809426774436
iteration : 13135
train acc:  0.84375
train loss:  0.33518126606941223
train gradient:  0.1462891899231158
iteration : 13136
train acc:  0.8515625
train loss:  0.31953495740890503
train gradient:  0.27552295432209184
iteration : 13137
train acc:  0.9375
train loss:  0.22576375305652618
train gradient:  0.09852293530223927
iteration : 13138
train acc:  0.859375
train loss:  0.29410648345947266
train gradient:  0.15698983483284093
iteration : 13139
train acc:  0.8984375
train loss:  0.2994938790798187
train gradient:  0.09221795988108727
iteration : 13140
train acc:  0.890625
train loss:  0.26932820677757263
train gradient:  0.09198989856154001
iteration : 13141
train acc:  0.875
train loss:  0.3092593252658844
train gradient:  0.1408077197322357
iteration : 13142
train acc:  0.828125
train loss:  0.3649577796459198
train gradient:  0.20987458910068912
iteration : 13143
train acc:  0.828125
train loss:  0.3066215217113495
train gradient:  0.13451365150441458
iteration : 13144
train acc:  0.8828125
train loss:  0.28150975704193115
train gradient:  0.08573693369180839
iteration : 13145
train acc:  0.8984375
train loss:  0.3196949064731598
train gradient:  0.12325225919419473
iteration : 13146
train acc:  0.8203125
train loss:  0.3477088510990143
train gradient:  0.19003607706759607
iteration : 13147
train acc:  0.8515625
train loss:  0.3812893033027649
train gradient:  0.20997967258437605
iteration : 13148
train acc:  0.8515625
train loss:  0.3201730251312256
train gradient:  0.12342950915006394
iteration : 13149
train acc:  0.8671875
train loss:  0.30907613039016724
train gradient:  0.10477468458243873
iteration : 13150
train acc:  0.8671875
train loss:  0.30782264471054077
train gradient:  0.13084076415611887
iteration : 13151
train acc:  0.8671875
train loss:  0.29505300521850586
train gradient:  0.09266341263996526
iteration : 13152
train acc:  0.890625
train loss:  0.2280811071395874
train gradient:  0.11879091948502594
iteration : 13153
train acc:  0.8828125
train loss:  0.305719792842865
train gradient:  0.10657864218243186
iteration : 13154
train acc:  0.84375
train loss:  0.34319043159484863
train gradient:  0.11110655576708615
iteration : 13155
train acc:  0.8671875
train loss:  0.32405978441238403
train gradient:  0.09930155413033727
iteration : 13156
train acc:  0.859375
train loss:  0.41046077013015747
train gradient:  0.2027768834644464
iteration : 13157
train acc:  0.8828125
train loss:  0.28947913646698
train gradient:  0.21061400793110857
iteration : 13158
train acc:  0.8203125
train loss:  0.4171444773674011
train gradient:  0.2695282509808274
iteration : 13159
train acc:  0.8671875
train loss:  0.3400484323501587
train gradient:  0.15583309998951064
iteration : 13160
train acc:  0.796875
train loss:  0.41198986768722534
train gradient:  0.20845521214986967
iteration : 13161
train acc:  0.8828125
train loss:  0.2631727457046509
train gradient:  0.13934880288363385
iteration : 13162
train acc:  0.90625
train loss:  0.23486396670341492
train gradient:  0.09995571273090229
iteration : 13163
train acc:  0.8515625
train loss:  0.3537023663520813
train gradient:  0.19437503136939313
iteration : 13164
train acc:  0.875
train loss:  0.31408175826072693
train gradient:  0.29300306808662885
iteration : 13165
train acc:  0.90625
train loss:  0.25829118490219116
train gradient:  0.09809645649452142
iteration : 13166
train acc:  0.8515625
train loss:  0.3271457552909851
train gradient:  0.13863392357817023
iteration : 13167
train acc:  0.859375
train loss:  0.29609549045562744
train gradient:  0.11715362562935305
iteration : 13168
train acc:  0.8515625
train loss:  0.3539600372314453
train gradient:  0.16009839533322112
iteration : 13169
train acc:  0.8515625
train loss:  0.2957194447517395
train gradient:  0.13110945050631168
iteration : 13170
train acc:  0.828125
train loss:  0.29779839515686035
train gradient:  0.1162843992133334
iteration : 13171
train acc:  0.8359375
train loss:  0.4007894694805145
train gradient:  0.19200161923053446
iteration : 13172
train acc:  0.8984375
train loss:  0.24075669050216675
train gradient:  0.08337027448804817
iteration : 13173
train acc:  0.8515625
train loss:  0.325918972492218
train gradient:  0.15505521187941113
iteration : 13174
train acc:  0.8359375
train loss:  0.3748213052749634
train gradient:  0.22576355105083457
iteration : 13175
train acc:  0.875
train loss:  0.2675655484199524
train gradient:  0.09444916478707761
iteration : 13176
train acc:  0.8984375
train loss:  0.2997419536113739
train gradient:  0.18573333058650948
iteration : 13177
train acc:  0.875
train loss:  0.36416542530059814
train gradient:  0.17429910960935968
iteration : 13178
train acc:  0.8671875
train loss:  0.34382718801498413
train gradient:  0.11628005029731771
iteration : 13179
train acc:  0.8828125
train loss:  0.28830963373184204
train gradient:  0.11369631050938565
iteration : 13180
train acc:  0.859375
train loss:  0.3021968603134155
train gradient:  0.10035530221245044
iteration : 13181
train acc:  0.890625
train loss:  0.2851329743862152
train gradient:  0.150695233311249
iteration : 13182
train acc:  0.8046875
train loss:  0.401999294757843
train gradient:  0.25881842592788296
iteration : 13183
train acc:  0.84375
train loss:  0.3679465353488922
train gradient:  0.2165136446598755
iteration : 13184
train acc:  0.8359375
train loss:  0.32174474000930786
train gradient:  0.14395305178816903
iteration : 13185
train acc:  0.875
train loss:  0.3009054660797119
train gradient:  0.12593236889577292
iteration : 13186
train acc:  0.859375
train loss:  0.31994080543518066
train gradient:  0.1994419232129669
iteration : 13187
train acc:  0.875
train loss:  0.26136329770088196
train gradient:  0.0885118471721313
iteration : 13188
train acc:  0.859375
train loss:  0.2974663972854614
train gradient:  0.13465781777298308
iteration : 13189
train acc:  0.9140625
train loss:  0.21102076768875122
train gradient:  0.08439983847411657
iteration : 13190
train acc:  0.84375
train loss:  0.35314708948135376
train gradient:  0.17757226577490381
iteration : 13191
train acc:  0.859375
train loss:  0.31244105100631714
train gradient:  0.1242955794131593
iteration : 13192
train acc:  0.828125
train loss:  0.3177452087402344
train gradient:  0.10481405179710557
iteration : 13193
train acc:  0.859375
train loss:  0.30312228202819824
train gradient:  0.13445357376910222
iteration : 13194
train acc:  0.8125
train loss:  0.3598698377609253
train gradient:  0.17329370889108967
iteration : 13195
train acc:  0.90625
train loss:  0.2862232029438019
train gradient:  0.09945011944598664
iteration : 13196
train acc:  0.859375
train loss:  0.3119090795516968
train gradient:  0.16146438858906303
iteration : 13197
train acc:  0.828125
train loss:  0.3985217809677124
train gradient:  0.19866847282885436
iteration : 13198
train acc:  0.9140625
train loss:  0.25611811876296997
train gradient:  0.08171891867652815
iteration : 13199
train acc:  0.8515625
train loss:  0.3369804322719574
train gradient:  0.11222456100701278
iteration : 13200
train acc:  0.9296875
train loss:  0.20061135292053223
train gradient:  0.06156326028873753
iteration : 13201
train acc:  0.8515625
train loss:  0.2951366603374481
train gradient:  0.11437482882906773
iteration : 13202
train acc:  0.84375
train loss:  0.3160121440887451
train gradient:  0.1145658038788672
iteration : 13203
train acc:  0.859375
train loss:  0.38580387830734253
train gradient:  0.22807518568682922
iteration : 13204
train acc:  0.84375
train loss:  0.33442798256874084
train gradient:  0.14163202910565093
iteration : 13205
train acc:  0.796875
train loss:  0.3999781608581543
train gradient:  0.25874328873135977
iteration : 13206
train acc:  0.875
train loss:  0.2846740782260895
train gradient:  0.11104631151893375
iteration : 13207
train acc:  0.8203125
train loss:  0.37904298305511475
train gradient:  0.19841439765534546
iteration : 13208
train acc:  0.8515625
train loss:  0.34255173802375793
train gradient:  0.16798213985090013
iteration : 13209
train acc:  0.859375
train loss:  0.31573766469955444
train gradient:  0.10664656864194519
iteration : 13210
train acc:  0.8515625
train loss:  0.35812538862228394
train gradient:  0.13611942881258066
iteration : 13211
train acc:  0.8671875
train loss:  0.3199269473552704
train gradient:  0.10962113543381084
iteration : 13212
train acc:  0.8828125
train loss:  0.28846457600593567
train gradient:  0.10057230628516413
iteration : 13213
train acc:  0.84375
train loss:  0.29606592655181885
train gradient:  0.10987319903726743
iteration : 13214
train acc:  0.84375
train loss:  0.37913039326667786
train gradient:  0.15683267029471198
iteration : 13215
train acc:  0.8515625
train loss:  0.3223904073238373
train gradient:  0.20257819378973857
iteration : 13216
train acc:  0.859375
train loss:  0.2873283624649048
train gradient:  0.10853167679329026
iteration : 13217
train acc:  0.8515625
train loss:  0.3200001120567322
train gradient:  0.14958937235944267
iteration : 13218
train acc:  0.8515625
train loss:  0.30560940504074097
train gradient:  0.11675277643351674
iteration : 13219
train acc:  0.8515625
train loss:  0.3611464202404022
train gradient:  0.13650878317357312
iteration : 13220
train acc:  0.8828125
train loss:  0.28838154673576355
train gradient:  0.11474270790527949
iteration : 13221
train acc:  0.78125
train loss:  0.4092770218849182
train gradient:  0.1900343211652653
iteration : 13222
train acc:  0.8984375
train loss:  0.2723463177680969
train gradient:  0.119748437519544
iteration : 13223
train acc:  0.8515625
train loss:  0.3318791687488556
train gradient:  0.1326423319146929
iteration : 13224
train acc:  0.875
train loss:  0.3093469738960266
train gradient:  0.11820003114533775
iteration : 13225
train acc:  0.8671875
train loss:  0.3686905801296234
train gradient:  0.18571840697233274
iteration : 13226
train acc:  0.765625
train loss:  0.4825781285762787
train gradient:  0.2950025174815238
iteration : 13227
train acc:  0.859375
train loss:  0.3376804292201996
train gradient:  0.2164795568952653
iteration : 13228
train acc:  0.8203125
train loss:  0.35071513056755066
train gradient:  0.24936482643599725
iteration : 13229
train acc:  0.890625
train loss:  0.2647484540939331
train gradient:  0.08410280866031287
iteration : 13230
train acc:  0.8203125
train loss:  0.3561685085296631
train gradient:  0.17194010797967932
iteration : 13231
train acc:  0.875
train loss:  0.3037077784538269
train gradient:  0.11664460449718347
iteration : 13232
train acc:  0.890625
train loss:  0.3379015028476715
train gradient:  0.12642340489484755
iteration : 13233
train acc:  0.875
train loss:  0.24708464741706848
train gradient:  0.08271994115973773
iteration : 13234
train acc:  0.84375
train loss:  0.358235627412796
train gradient:  0.21239029381086327
iteration : 13235
train acc:  0.8046875
train loss:  0.407993882894516
train gradient:  0.20198757040112672
iteration : 13236
train acc:  0.8984375
train loss:  0.31446200609207153
train gradient:  0.10353294356601235
iteration : 13237
train acc:  0.8828125
train loss:  0.2760806083679199
train gradient:  0.09016931592321996
iteration : 13238
train acc:  0.8671875
train loss:  0.2995772957801819
train gradient:  0.13650633830472508
iteration : 13239
train acc:  0.8828125
train loss:  0.2911071181297302
train gradient:  0.10752720338366087
iteration : 13240
train acc:  0.8125
train loss:  0.3687712550163269
train gradient:  0.20249362209517613
iteration : 13241
train acc:  0.84375
train loss:  0.39225125312805176
train gradient:  0.14331403316189645
iteration : 13242
train acc:  0.8515625
train loss:  0.29900598526000977
train gradient:  0.11250574792268007
iteration : 13243
train acc:  0.9140625
train loss:  0.26146596670150757
train gradient:  0.11066654961115775
iteration : 13244
train acc:  0.859375
train loss:  0.3015226125717163
train gradient:  0.12114248581374326
iteration : 13245
train acc:  0.8515625
train loss:  0.4223098158836365
train gradient:  0.1832765228437373
iteration : 13246
train acc:  0.828125
train loss:  0.4685254395008087
train gradient:  0.2695312919150614
iteration : 13247
train acc:  0.8984375
train loss:  0.3161126971244812
train gradient:  0.13856513429263495
iteration : 13248
train acc:  0.8125
train loss:  0.37776437401771545
train gradient:  0.1686344843148331
iteration : 13249
train acc:  0.8828125
train loss:  0.3320176303386688
train gradient:  0.11453013626274346
iteration : 13250
train acc:  0.9140625
train loss:  0.24413257837295532
train gradient:  0.09435662951526394
iteration : 13251
train acc:  0.859375
train loss:  0.33686041831970215
train gradient:  0.13391748472519815
iteration : 13252
train acc:  0.921875
train loss:  0.2361137568950653
train gradient:  0.125678367467015
iteration : 13253
train acc:  0.8671875
train loss:  0.3243199586868286
train gradient:  0.1422504134849642
iteration : 13254
train acc:  0.8828125
train loss:  0.2969561219215393
train gradient:  0.1245229876627564
iteration : 13255
train acc:  0.828125
train loss:  0.3602343797683716
train gradient:  0.14156931857390795
iteration : 13256
train acc:  0.8828125
train loss:  0.3084808588027954
train gradient:  0.13055114465317333
iteration : 13257
train acc:  0.8125
train loss:  0.429196298122406
train gradient:  0.20168934757356297
iteration : 13258
train acc:  0.84375
train loss:  0.36534634232521057
train gradient:  0.1470417897736791
iteration : 13259
train acc:  0.8203125
train loss:  0.401094913482666
train gradient:  0.23386154967594341
iteration : 13260
train acc:  0.875
train loss:  0.30322328209877014
train gradient:  0.09473468597248265
iteration : 13261
train acc:  0.8828125
train loss:  0.32267338037490845
train gradient:  0.14200961554421415
iteration : 13262
train acc:  0.84375
train loss:  0.3332797884941101
train gradient:  0.13011071628337978
iteration : 13263
train acc:  0.8515625
train loss:  0.34945422410964966
train gradient:  0.15237211636165715
iteration : 13264
train acc:  0.84375
train loss:  0.3211718797683716
train gradient:  0.1445870223023759
iteration : 13265
train acc:  0.8671875
train loss:  0.34788352251052856
train gradient:  0.14763696418998407
iteration : 13266
train acc:  0.8515625
train loss:  0.36602750420570374
train gradient:  0.22810250957518158
iteration : 13267
train acc:  0.8359375
train loss:  0.33967316150665283
train gradient:  0.1428385827756789
iteration : 13268
train acc:  0.8515625
train loss:  0.31879693269729614
train gradient:  0.15059569409948165
iteration : 13269
train acc:  0.8359375
train loss:  0.3628409504890442
train gradient:  0.13866761244821937
iteration : 13270
train acc:  0.859375
train loss:  0.32691460847854614
train gradient:  0.11755885034448949
iteration : 13271
train acc:  0.8046875
train loss:  0.42476797103881836
train gradient:  0.26404668231044764
iteration : 13272
train acc:  0.8984375
train loss:  0.263599157333374
train gradient:  0.12792839534226286
iteration : 13273
train acc:  0.8828125
train loss:  0.2986210584640503
train gradient:  0.13921072764418205
iteration : 13274
train acc:  0.84375
train loss:  0.3453376889228821
train gradient:  0.13464199086711576
iteration : 13275
train acc:  0.8671875
train loss:  0.30264613032341003
train gradient:  0.13226834349378397
iteration : 13276
train acc:  0.859375
train loss:  0.3197541832923889
train gradient:  0.1252197080748349
iteration : 13277
train acc:  0.859375
train loss:  0.3669098913669586
train gradient:  0.18902907782314277
iteration : 13278
train acc:  0.875
train loss:  0.2866758704185486
train gradient:  0.15691514729524747
iteration : 13279
train acc:  0.8515625
train loss:  0.38433384895324707
train gradient:  0.15192627952256482
iteration : 13280
train acc:  0.859375
train loss:  0.29505953192710876
train gradient:  0.18448012621717375
iteration : 13281
train acc:  0.8515625
train loss:  0.36952394247055054
train gradient:  0.21186667062822168
iteration : 13282
train acc:  0.875
train loss:  0.305042028427124
train gradient:  0.16139634045775308
iteration : 13283
train acc:  0.8515625
train loss:  0.29397332668304443
train gradient:  0.13030106768243457
iteration : 13284
train acc:  0.90625
train loss:  0.27202311158180237
train gradient:  0.11515537681284266
iteration : 13285
train acc:  0.8671875
train loss:  0.2950296401977539
train gradient:  0.12267435729906076
iteration : 13286
train acc:  0.9140625
train loss:  0.24009239673614502
train gradient:  0.07070524949027807
iteration : 13287
train acc:  0.8359375
train loss:  0.3859201669692993
train gradient:  0.1497313741986318
iteration : 13288
train acc:  0.8359375
train loss:  0.35533738136291504
train gradient:  0.19318658554233492
iteration : 13289
train acc:  0.921875
train loss:  0.2847557067871094
train gradient:  0.08450398568729937
iteration : 13290
train acc:  0.859375
train loss:  0.3531978130340576
train gradient:  0.15643221040620703
iteration : 13291
train acc:  0.8671875
train loss:  0.29814696311950684
train gradient:  0.10456496497902035
iteration : 13292
train acc:  0.828125
train loss:  0.34488025307655334
train gradient:  0.14976118826756923
iteration : 13293
train acc:  0.8828125
train loss:  0.33024609088897705
train gradient:  0.14552043896502426
iteration : 13294
train acc:  0.8984375
train loss:  0.2773975729942322
train gradient:  0.11075353343996243
iteration : 13295
train acc:  0.8515625
train loss:  0.3476402163505554
train gradient:  0.21093289115896433
iteration : 13296
train acc:  0.859375
train loss:  0.37925660610198975
train gradient:  0.14899236683987133
iteration : 13297
train acc:  0.8515625
train loss:  0.3514378070831299
train gradient:  0.16157055020318856
iteration : 13298
train acc:  0.8828125
train loss:  0.3301038146018982
train gradient:  0.13452229043093186
iteration : 13299
train acc:  0.921875
train loss:  0.21225225925445557
train gradient:  0.08674205233341356
iteration : 13300
train acc:  0.8828125
train loss:  0.29669904708862305
train gradient:  0.11899164273629237
iteration : 13301
train acc:  0.84375
train loss:  0.34414762258529663
train gradient:  0.12920537944354965
iteration : 13302
train acc:  0.84375
train loss:  0.3903200626373291
train gradient:  0.2066733781547822
iteration : 13303
train acc:  0.8828125
train loss:  0.287307471036911
train gradient:  0.08984850239565416
iteration : 13304
train acc:  0.8671875
train loss:  0.3151238262653351
train gradient:  0.12055128221316429
iteration : 13305
train acc:  0.8359375
train loss:  0.39794349670410156
train gradient:  0.42775994083227503
iteration : 13306
train acc:  0.8359375
train loss:  0.4002279043197632
train gradient:  0.21563019310259357
iteration : 13307
train acc:  0.859375
train loss:  0.2852311432361603
train gradient:  0.12992244161145294
iteration : 13308
train acc:  0.8515625
train loss:  0.37465617060661316
train gradient:  0.19200158354345825
iteration : 13309
train acc:  0.84375
train loss:  0.396156907081604
train gradient:  0.1920468052283456
iteration : 13310
train acc:  0.890625
train loss:  0.2678034007549286
train gradient:  0.13010849827254056
iteration : 13311
train acc:  0.8671875
train loss:  0.31432831287384033
train gradient:  0.1391903909680865
iteration : 13312
train acc:  0.8828125
train loss:  0.3559305667877197
train gradient:  0.16472008945645483
iteration : 13313
train acc:  0.859375
train loss:  0.30530428886413574
train gradient:  0.15114265327079907
iteration : 13314
train acc:  0.875
train loss:  0.33787456154823303
train gradient:  0.1558626396673048
iteration : 13315
train acc:  0.8984375
train loss:  0.29317808151245117
train gradient:  0.12615679013090558
iteration : 13316
train acc:  0.7734375
train loss:  0.4309145212173462
train gradient:  0.23690602112056786
iteration : 13317
train acc:  0.8984375
train loss:  0.28167539834976196
train gradient:  0.1033371814416619
iteration : 13318
train acc:  0.8515625
train loss:  0.3353766202926636
train gradient:  0.1696482613647884
iteration : 13319
train acc:  0.8828125
train loss:  0.2674703299999237
train gradient:  0.09224649426945507
iteration : 13320
train acc:  0.8359375
train loss:  0.3537795841693878
train gradient:  0.18068409107090677
iteration : 13321
train acc:  0.8984375
train loss:  0.27184879779815674
train gradient:  0.09680030759003493
iteration : 13322
train acc:  0.84375
train loss:  0.29321345686912537
train gradient:  0.13326815624800878
iteration : 13323
train acc:  0.875
train loss:  0.3317769765853882
train gradient:  0.11547941653250017
iteration : 13324
train acc:  0.8671875
train loss:  0.31181517243385315
train gradient:  0.11118294554122754
iteration : 13325
train acc:  0.8515625
train loss:  0.35256898403167725
train gradient:  0.13965389039253898
iteration : 13326
train acc:  0.8125
train loss:  0.3590579628944397
train gradient:  0.11836583873146314
iteration : 13327
train acc:  0.8359375
train loss:  0.32039517164230347
train gradient:  0.1455871593174964
iteration : 13328
train acc:  0.8828125
train loss:  0.2987123131752014
train gradient:  0.11002535380413787
iteration : 13329
train acc:  0.8203125
train loss:  0.31185227632522583
train gradient:  0.12624292991651226
iteration : 13330
train acc:  0.875
train loss:  0.248539999127388
train gradient:  0.09116701028460877
iteration : 13331
train acc:  0.890625
train loss:  0.29930534958839417
train gradient:  0.09592954867113801
iteration : 13332
train acc:  0.9375
train loss:  0.22032371163368225
train gradient:  0.11766086365868732
iteration : 13333
train acc:  0.8671875
train loss:  0.30091243982315063
train gradient:  0.1681836542970231
iteration : 13334
train acc:  0.8125
train loss:  0.36676424741744995
train gradient:  0.15010253324991096
iteration : 13335
train acc:  0.8515625
train loss:  0.3536330461502075
train gradient:  0.19610836943522636
iteration : 13336
train acc:  0.890625
train loss:  0.31051111221313477
train gradient:  0.15321627234012253
iteration : 13337
train acc:  0.8828125
train loss:  0.31972748041152954
train gradient:  0.13471357439715695
iteration : 13338
train acc:  0.828125
train loss:  0.32718583941459656
train gradient:  0.11742756844503002
iteration : 13339
train acc:  0.84375
train loss:  0.30811506509780884
train gradient:  0.11530282177749329
iteration : 13340
train acc:  0.8515625
train loss:  0.363860547542572
train gradient:  0.22856193148737458
iteration : 13341
train acc:  0.921875
train loss:  0.29550135135650635
train gradient:  0.11088126244027013
iteration : 13342
train acc:  0.8359375
train loss:  0.36271965503692627
train gradient:  0.3229546330605859
iteration : 13343
train acc:  0.8671875
train loss:  0.3199721872806549
train gradient:  0.1805664618342373
iteration : 13344
train acc:  0.859375
train loss:  0.35390186309814453
train gradient:  0.14427000842609045
iteration : 13345
train acc:  0.90625
train loss:  0.2476324439048767
train gradient:  0.0957048193535926
iteration : 13346
train acc:  0.8828125
train loss:  0.31206196546554565
train gradient:  0.19568644015849213
iteration : 13347
train acc:  0.84375
train loss:  0.39279013872146606
train gradient:  0.2941300773011549
iteration : 13348
train acc:  0.875
train loss:  0.2975442409515381
train gradient:  0.08873840539028424
iteration : 13349
train acc:  0.859375
train loss:  0.2886316180229187
train gradient:  0.13215935276845675
iteration : 13350
train acc:  0.9140625
train loss:  0.2701054513454437
train gradient:  0.11596734766365788
iteration : 13351
train acc:  0.8515625
train loss:  0.3877016007900238
train gradient:  0.22391559264106442
iteration : 13352
train acc:  0.90625
train loss:  0.21100594103336334
train gradient:  0.0779398334806518
iteration : 13353
train acc:  0.90625
train loss:  0.25720861554145813
train gradient:  0.08032478108981723
iteration : 13354
train acc:  0.9140625
train loss:  0.23515336215496063
train gradient:  0.07750846889890385
iteration : 13355
train acc:  0.859375
train loss:  0.31982123851776123
train gradient:  0.14746664789206304
iteration : 13356
train acc:  0.8671875
train loss:  0.3525915741920471
train gradient:  0.16004413045885868
iteration : 13357
train acc:  0.9453125
train loss:  0.2125139981508255
train gradient:  0.11448771778984325
iteration : 13358
train acc:  0.875
train loss:  0.3573281466960907
train gradient:  0.1184693952867168
iteration : 13359
train acc:  0.8984375
train loss:  0.26721563935279846
train gradient:  0.10682643130945807
iteration : 13360
train acc:  0.890625
train loss:  0.3069632351398468
train gradient:  0.10205940343258771
iteration : 13361
train acc:  0.859375
train loss:  0.3635229170322418
train gradient:  0.1308985017504311
iteration : 13362
train acc:  0.84375
train loss:  0.31807559728622437
train gradient:  0.1334976188430238
iteration : 13363
train acc:  0.8515625
train loss:  0.30370181798934937
train gradient:  0.10187419316103796
iteration : 13364
train acc:  0.828125
train loss:  0.35645297169685364
train gradient:  0.1708316679318207
iteration : 13365
train acc:  0.8203125
train loss:  0.3172540068626404
train gradient:  0.1518388060906082
iteration : 13366
train acc:  0.9140625
train loss:  0.26765939593315125
train gradient:  0.10407989410586085
iteration : 13367
train acc:  0.8828125
train loss:  0.3000740706920624
train gradient:  0.10549612731284506
iteration : 13368
train acc:  0.796875
train loss:  0.3693227767944336
train gradient:  0.19079709352409788
iteration : 13369
train acc:  0.7890625
train loss:  0.4335157573223114
train gradient:  0.18063677874395645
iteration : 13370
train acc:  0.8125
train loss:  0.3861469626426697
train gradient:  0.19577613779417183
iteration : 13371
train acc:  0.875
train loss:  0.3302164673805237
train gradient:  0.12048026612307836
iteration : 13372
train acc:  0.8046875
train loss:  0.4165985584259033
train gradient:  0.17802714361893782
iteration : 13373
train acc:  0.8984375
train loss:  0.2577294409275055
train gradient:  0.0678711320325807
iteration : 13374
train acc:  0.875
train loss:  0.27782997488975525
train gradient:  0.12555486399832236
iteration : 13375
train acc:  0.8828125
train loss:  0.24982571601867676
train gradient:  0.0633553863879502
iteration : 13376
train acc:  0.859375
train loss:  0.3248463571071625
train gradient:  0.12640930652769078
iteration : 13377
train acc:  0.90625
train loss:  0.22285044193267822
train gradient:  0.07824665757627242
iteration : 13378
train acc:  0.875
train loss:  0.2912670373916626
train gradient:  0.13113565341155858
iteration : 13379
train acc:  0.890625
train loss:  0.268014132976532
train gradient:  0.12738218929929418
iteration : 13380
train acc:  0.8515625
train loss:  0.31654512882232666
train gradient:  0.11624368356947128
iteration : 13381
train acc:  0.8984375
train loss:  0.2705698013305664
train gradient:  0.08619081234573567
iteration : 13382
train acc:  0.8671875
train loss:  0.26146501302719116
train gradient:  0.07237054995889025
iteration : 13383
train acc:  0.859375
train loss:  0.3630462884902954
train gradient:  0.19188871931139276
iteration : 13384
train acc:  0.828125
train loss:  0.35727155208587646
train gradient:  0.1797705016300575
iteration : 13385
train acc:  0.8203125
train loss:  0.34340807795524597
train gradient:  0.17178618226329667
iteration : 13386
train acc:  0.890625
train loss:  0.2778324484825134
train gradient:  0.09265534662290602
iteration : 13387
train acc:  0.921875
train loss:  0.24753791093826294
train gradient:  0.10472846646172777
iteration : 13388
train acc:  0.875
train loss:  0.30785417556762695
train gradient:  0.15669813981847697
iteration : 13389
train acc:  0.8359375
train loss:  0.29321128129959106
train gradient:  0.0867603997698514
iteration : 13390
train acc:  0.8125
train loss:  0.3857797384262085
train gradient:  0.18720077282937755
iteration : 13391
train acc:  0.90625
train loss:  0.24923069775104523
train gradient:  0.07921127341060274
iteration : 13392
train acc:  0.828125
train loss:  0.38373568654060364
train gradient:  0.2118582722247151
iteration : 13393
train acc:  0.8203125
train loss:  0.3899959921836853
train gradient:  0.22365614295791575
iteration : 13394
train acc:  0.875
train loss:  0.3566180467605591
train gradient:  0.13079034278306345
iteration : 13395
train acc:  0.859375
train loss:  0.2667725682258606
train gradient:  0.09851167512972372
iteration : 13396
train acc:  0.84375
train loss:  0.3490658402442932
train gradient:  0.17491943642593827
iteration : 13397
train acc:  0.875
train loss:  0.29054802656173706
train gradient:  0.1214193320392035
iteration : 13398
train acc:  0.8828125
train loss:  0.30489253997802734
train gradient:  0.1885727549996864
iteration : 13399
train acc:  0.8359375
train loss:  0.31924834847450256
train gradient:  0.18200202137172197
iteration : 13400
train acc:  0.875
train loss:  0.25954264402389526
train gradient:  0.1300667761988346
iteration : 13401
train acc:  0.8203125
train loss:  0.4016205072402954
train gradient:  0.23107393404966653
iteration : 13402
train acc:  0.890625
train loss:  0.30103152990341187
train gradient:  0.11909949608975486
iteration : 13403
train acc:  0.859375
train loss:  0.3116207420825958
train gradient:  0.1554942946445674
iteration : 13404
train acc:  0.8359375
train loss:  0.3563697934150696
train gradient:  0.18623845623799126
iteration : 13405
train acc:  0.8125
train loss:  0.40344762802124023
train gradient:  0.20052157974956658
iteration : 13406
train acc:  0.859375
train loss:  0.30008798837661743
train gradient:  0.1331008037394413
iteration : 13407
train acc:  0.8359375
train loss:  0.32180655002593994
train gradient:  0.14147558007116265
iteration : 13408
train acc:  0.859375
train loss:  0.3289908766746521
train gradient:  0.12466600397309774
iteration : 13409
train acc:  0.8828125
train loss:  0.26881998777389526
train gradient:  0.0916986762769395
iteration : 13410
train acc:  0.8515625
train loss:  0.3770067095756531
train gradient:  0.1642120454000679
iteration : 13411
train acc:  0.8828125
train loss:  0.27969908714294434
train gradient:  0.11986716515492228
iteration : 13412
train acc:  0.8125
train loss:  0.4119005501270294
train gradient:  0.24431013836291854
iteration : 13413
train acc:  0.8671875
train loss:  0.2602097690105438
train gradient:  0.10111089777922147
iteration : 13414
train acc:  0.8828125
train loss:  0.2934045195579529
train gradient:  0.08954912068810043
iteration : 13415
train acc:  0.890625
train loss:  0.2951282858848572
train gradient:  0.10395605130822315
iteration : 13416
train acc:  0.859375
train loss:  0.28063273429870605
train gradient:  0.143180153479813
iteration : 13417
train acc:  0.8125
train loss:  0.42837971448898315
train gradient:  0.14271855364304134
iteration : 13418
train acc:  0.8515625
train loss:  0.33698469400405884
train gradient:  0.15920735273062805
iteration : 13419
train acc:  0.875
train loss:  0.2697067856788635
train gradient:  0.11028066556999232
iteration : 13420
train acc:  0.9140625
train loss:  0.2706924080848694
train gradient:  0.1555320177452659
iteration : 13421
train acc:  0.8359375
train loss:  0.3698412775993347
train gradient:  0.15349283263390176
iteration : 13422
train acc:  0.859375
train loss:  0.3742386996746063
train gradient:  0.1291400044666267
iteration : 13423
train acc:  0.8359375
train loss:  0.3220163583755493
train gradient:  0.13201232312232092
iteration : 13424
train acc:  0.8515625
train loss:  0.299638569355011
train gradient:  0.1254326957891859
iteration : 13425
train acc:  0.890625
train loss:  0.28020018339157104
train gradient:  0.10146077162183083
iteration : 13426
train acc:  0.8203125
train loss:  0.3655683994293213
train gradient:  0.1413801425278739
iteration : 13427
train acc:  0.8828125
train loss:  0.31026148796081543
train gradient:  0.12113033112980358
iteration : 13428
train acc:  0.828125
train loss:  0.3553062677383423
train gradient:  0.1665005115455172
iteration : 13429
train acc:  0.8671875
train loss:  0.28000906109809875
train gradient:  0.1345923376594105
iteration : 13430
train acc:  0.8515625
train loss:  0.36616265773773193
train gradient:  0.140478938348379
iteration : 13431
train acc:  0.9296875
train loss:  0.22338593006134033
train gradient:  0.06351705626941134
iteration : 13432
train acc:  0.8984375
train loss:  0.32803329825401306
train gradient:  0.1354200676689249
iteration : 13433
train acc:  0.8671875
train loss:  0.288933128118515
train gradient:  0.10023165025864618
iteration : 13434
train acc:  0.8515625
train loss:  0.35809773206710815
train gradient:  0.214072062232539
iteration : 13435
train acc:  0.8671875
train loss:  0.2818412184715271
train gradient:  0.08582837702533726
iteration : 13436
train acc:  0.7890625
train loss:  0.405803918838501
train gradient:  0.2494232219125559
iteration : 13437
train acc:  0.8828125
train loss:  0.29046717286109924
train gradient:  0.12122353457848162
iteration : 13438
train acc:  0.8203125
train loss:  0.37250688672065735
train gradient:  0.21937256019723766
iteration : 13439
train acc:  0.8984375
train loss:  0.24098265171051025
train gradient:  0.10384085941830548
iteration : 13440
train acc:  0.8359375
train loss:  0.29393959045410156
train gradient:  0.09543286134597946
iteration : 13441
train acc:  0.8125
train loss:  0.43711405992507935
train gradient:  0.24218431878466268
iteration : 13442
train acc:  0.875
train loss:  0.3072945177555084
train gradient:  0.14849758208394215
iteration : 13443
train acc:  0.90625
train loss:  0.2684899866580963
train gradient:  0.07935536746948416
iteration : 13444
train acc:  0.890625
train loss:  0.25898855924606323
train gradient:  0.10727747759782041
iteration : 13445
train acc:  0.84375
train loss:  0.34984642267227173
train gradient:  0.14206200179020245
iteration : 13446
train acc:  0.90625
train loss:  0.2544453740119934
train gradient:  0.09127483501521022
iteration : 13447
train acc:  0.90625
train loss:  0.2967773675918579
train gradient:  0.11585216579156818
iteration : 13448
train acc:  0.8828125
train loss:  0.24283328652381897
train gradient:  0.11740992778913459
iteration : 13449
train acc:  0.84375
train loss:  0.3385162353515625
train gradient:  0.16462520817925425
iteration : 13450
train acc:  0.8671875
train loss:  0.2800062298774719
train gradient:  0.09363767315786364
iteration : 13451
train acc:  0.859375
train loss:  0.34407299757003784
train gradient:  0.11673827974294448
iteration : 13452
train acc:  0.8671875
train loss:  0.3184523582458496
train gradient:  0.18528367929176037
iteration : 13453
train acc:  0.8046875
train loss:  0.39746803045272827
train gradient:  0.23320809724612107
iteration : 13454
train acc:  0.8984375
train loss:  0.2835697531700134
train gradient:  0.10840873649721763
iteration : 13455
train acc:  0.890625
train loss:  0.2937140464782715
train gradient:  0.2338298208506096
iteration : 13456
train acc:  0.828125
train loss:  0.3387875258922577
train gradient:  0.21289735910611074
iteration : 13457
train acc:  0.8828125
train loss:  0.2621707320213318
train gradient:  0.11653495587722197
iteration : 13458
train acc:  0.8359375
train loss:  0.34102863073349
train gradient:  0.15011775172590142
iteration : 13459
train acc:  0.84375
train loss:  0.30895519256591797
train gradient:  0.10385612158252486
iteration : 13460
train acc:  0.828125
train loss:  0.3501114249229431
train gradient:  0.1576837561918427
iteration : 13461
train acc:  0.859375
train loss:  0.3908735513687134
train gradient:  0.18465241420945133
iteration : 13462
train acc:  0.84375
train loss:  0.36297279596328735
train gradient:  0.12989852398372234
iteration : 13463
train acc:  0.9375
train loss:  0.22952760756015778
train gradient:  0.09666563337468977
iteration : 13464
train acc:  0.828125
train loss:  0.3308420181274414
train gradient:  0.1018422087636453
iteration : 13465
train acc:  0.8515625
train loss:  0.34277117252349854
train gradient:  0.1625510042366201
iteration : 13466
train acc:  0.84375
train loss:  0.374041348695755
train gradient:  0.13806239473586007
iteration : 13467
train acc:  0.875
train loss:  0.34353107213974
train gradient:  0.13736580550316196
iteration : 13468
train acc:  0.875
train loss:  0.30841928720474243
train gradient:  0.13815310317033738
iteration : 13469
train acc:  0.859375
train loss:  0.3048253059387207
train gradient:  0.17756634605819055
iteration : 13470
train acc:  0.859375
train loss:  0.29192978143692017
train gradient:  0.10476194704947907
iteration : 13471
train acc:  0.828125
train loss:  0.39531171321868896
train gradient:  0.14161803226017355
iteration : 13472
train acc:  0.875
train loss:  0.28519779443740845
train gradient:  0.12935750724311534
iteration : 13473
train acc:  0.8359375
train loss:  0.3973131477832794
train gradient:  0.2610484395303666
iteration : 13474
train acc:  0.828125
train loss:  0.32989299297332764
train gradient:  0.17671108320833162
iteration : 13475
train acc:  0.875
train loss:  0.28984686732292175
train gradient:  0.1320691428881134
iteration : 13476
train acc:  0.84375
train loss:  0.33731114864349365
train gradient:  0.14460185332114464
iteration : 13477
train acc:  0.859375
train loss:  0.38040658831596375
train gradient:  0.21168459277837906
iteration : 13478
train acc:  0.828125
train loss:  0.3383032977581024
train gradient:  0.14928384597394292
iteration : 13479
train acc:  0.8671875
train loss:  0.31441348791122437
train gradient:  0.1678439791041521
iteration : 13480
train acc:  0.8828125
train loss:  0.33678731322288513
train gradient:  0.13783808907789452
iteration : 13481
train acc:  0.8671875
train loss:  0.2862823009490967
train gradient:  0.15408871204594626
iteration : 13482
train acc:  0.8203125
train loss:  0.40723761916160583
train gradient:  0.23288200018042754
iteration : 13483
train acc:  0.8515625
train loss:  0.3487631380558014
train gradient:  0.21013101061354283
iteration : 13484
train acc:  0.859375
train loss:  0.3066072165966034
train gradient:  0.148111644321881
iteration : 13485
train acc:  0.8828125
train loss:  0.27955690026283264
train gradient:  0.11842746757913339
iteration : 13486
train acc:  0.890625
train loss:  0.26918232440948486
train gradient:  0.11129494576595442
iteration : 13487
train acc:  0.8515625
train loss:  0.4117477536201477
train gradient:  0.19595351311314196
iteration : 13488
train acc:  0.8359375
train loss:  0.31702184677124023
train gradient:  0.1062539930026367
iteration : 13489
train acc:  0.8515625
train loss:  0.29521769285202026
train gradient:  0.10299569300969255
iteration : 13490
train acc:  0.8515625
train loss:  0.36315295100212097
train gradient:  0.24724714915340829
iteration : 13491
train acc:  0.9296875
train loss:  0.2024017721414566
train gradient:  0.0818495340874303
iteration : 13492
train acc:  0.8515625
train loss:  0.31824153661727905
train gradient:  0.12970593296981475
iteration : 13493
train acc:  0.90625
train loss:  0.32636183500289917
train gradient:  0.12319137057222533
iteration : 13494
train acc:  0.8671875
train loss:  0.3221556544303894
train gradient:  0.10090536260630267
iteration : 13495
train acc:  0.828125
train loss:  0.41006937623023987
train gradient:  0.21047019339688933
iteration : 13496
train acc:  0.859375
train loss:  0.313792884349823
train gradient:  0.1095814334046133
iteration : 13497
train acc:  0.84375
train loss:  0.3457196354866028
train gradient:  0.15768232496060672
iteration : 13498
train acc:  0.8828125
train loss:  0.26863184571266174
train gradient:  0.10761407653313519
iteration : 13499
train acc:  0.8203125
train loss:  0.39252370595932007
train gradient:  0.17641196412383459
iteration : 13500
train acc:  0.84375
train loss:  0.3118428587913513
train gradient:  0.09234595684977784
iteration : 13501
train acc:  0.8828125
train loss:  0.3169487416744232
train gradient:  0.11847193918792681
iteration : 13502
train acc:  0.828125
train loss:  0.3136019706726074
train gradient:  0.13024823819269432
iteration : 13503
train acc:  0.8828125
train loss:  0.25213268399238586
train gradient:  0.11340806972158601
iteration : 13504
train acc:  0.84375
train loss:  0.3735853135585785
train gradient:  0.1535106697793197
iteration : 13505
train acc:  0.875
train loss:  0.3034610152244568
train gradient:  0.12678648002288245
iteration : 13506
train acc:  0.84375
train loss:  0.44356292486190796
train gradient:  0.25158645886159103
iteration : 13507
train acc:  0.8359375
train loss:  0.35357311367988586
train gradient:  0.13266039206338096
iteration : 13508
train acc:  0.7890625
train loss:  0.4501534700393677
train gradient:  0.2714952087599711
iteration : 13509
train acc:  0.8828125
train loss:  0.30073708295822144
train gradient:  0.1098116597190883
iteration : 13510
train acc:  0.8125
train loss:  0.33756983280181885
train gradient:  0.13831933094038465
iteration : 13511
train acc:  0.8359375
train loss:  0.30914562940597534
train gradient:  0.10917019282715189
iteration : 13512
train acc:  0.8671875
train loss:  0.3620256781578064
train gradient:  0.11587550842211507
iteration : 13513
train acc:  0.8515625
train loss:  0.35325053334236145
train gradient:  0.1289885893195799
iteration : 13514
train acc:  0.828125
train loss:  0.37041693925857544
train gradient:  0.13911185107342333
iteration : 13515
train acc:  0.84375
train loss:  0.33847302198410034
train gradient:  0.18023223360626695
iteration : 13516
train acc:  0.84375
train loss:  0.31783825159072876
train gradient:  0.10575772014362597
iteration : 13517
train acc:  0.890625
train loss:  0.2554735839366913
train gradient:  0.0901208713390322
iteration : 13518
train acc:  0.890625
train loss:  0.3342301845550537
train gradient:  0.13182655763551648
iteration : 13519
train acc:  0.8359375
train loss:  0.3598785698413849
train gradient:  0.11750688023285719
iteration : 13520
train acc:  0.859375
train loss:  0.3076149821281433
train gradient:  0.094756996983773
iteration : 13521
train acc:  0.859375
train loss:  0.2923268973827362
train gradient:  0.13684918776982197
iteration : 13522
train acc:  0.8828125
train loss:  0.30757784843444824
train gradient:  0.10679533905060093
iteration : 13523
train acc:  0.8828125
train loss:  0.302431583404541
train gradient:  0.11163516683047306
iteration : 13524
train acc:  0.8671875
train loss:  0.29922908544540405
train gradient:  0.07827884659187494
iteration : 13525
train acc:  0.8359375
train loss:  0.346197247505188
train gradient:  0.1279981135936893
iteration : 13526
train acc:  0.8125
train loss:  0.39792436361312866
train gradient:  0.1436771650968694
iteration : 13527
train acc:  0.8828125
train loss:  0.28230080008506775
train gradient:  0.07967617428721045
iteration : 13528
train acc:  0.921875
train loss:  0.20933836698532104
train gradient:  0.07804918621543998
iteration : 13529
train acc:  0.8828125
train loss:  0.30570489168167114
train gradient:  0.08509615884131493
iteration : 13530
train acc:  0.828125
train loss:  0.32795873284339905
train gradient:  0.11514259746850052
iteration : 13531
train acc:  0.84375
train loss:  0.3281843662261963
train gradient:  0.13829038045406195
iteration : 13532
train acc:  0.84375
train loss:  0.3647388815879822
train gradient:  0.1349588048541167
iteration : 13533
train acc:  0.890625
train loss:  0.2786647379398346
train gradient:  0.09564039564177786
iteration : 13534
train acc:  0.8671875
train loss:  0.30558374524116516
train gradient:  0.11190703472337032
iteration : 13535
train acc:  0.8046875
train loss:  0.3458172380924225
train gradient:  0.1252087321268226
iteration : 13536
train acc:  0.890625
train loss:  0.2643146514892578
train gradient:  0.08540139156823018
iteration : 13537
train acc:  0.859375
train loss:  0.2858891189098358
train gradient:  0.14749380192762196
iteration : 13538
train acc:  0.828125
train loss:  0.3620587885379791
train gradient:  0.11606464022489989
iteration : 13539
train acc:  0.9140625
train loss:  0.27532291412353516
train gradient:  0.09473327330878105
iteration : 13540
train acc:  0.8203125
train loss:  0.4038231670856476
train gradient:  0.13750239782468873
iteration : 13541
train acc:  0.9140625
train loss:  0.24363809823989868
train gradient:  0.0845097303832905
iteration : 13542
train acc:  0.890625
train loss:  0.3154580593109131
train gradient:  0.13326887142943636
iteration : 13543
train acc:  0.84375
train loss:  0.3797926902770996
train gradient:  0.13777571706680686
iteration : 13544
train acc:  0.78125
train loss:  0.4324454963207245
train gradient:  0.21777536601317565
iteration : 13545
train acc:  0.8203125
train loss:  0.34378552436828613
train gradient:  0.14756663923794044
iteration : 13546
train acc:  0.8671875
train loss:  0.2937818169593811
train gradient:  0.11760635401919575
iteration : 13547
train acc:  0.875
train loss:  0.3187258243560791
train gradient:  0.1099289283895394
iteration : 13548
train acc:  0.828125
train loss:  0.40341687202453613
train gradient:  0.17124456694985896
iteration : 13549
train acc:  0.8125
train loss:  0.4576932191848755
train gradient:  0.21024596394172188
iteration : 13550
train acc:  0.90625
train loss:  0.25563064217567444
train gradient:  0.13484229075736978
iteration : 13551
train acc:  0.8515625
train loss:  0.37080979347229004
train gradient:  0.1745037924708555
iteration : 13552
train acc:  0.8984375
train loss:  0.26424896717071533
train gradient:  0.12495821996687849
iteration : 13553
train acc:  0.90625
train loss:  0.302484929561615
train gradient:  0.09521818404529829
iteration : 13554
train acc:  0.8125
train loss:  0.3303239941596985
train gradient:  0.10165504261835655
iteration : 13555
train acc:  0.875
train loss:  0.29152271151542664
train gradient:  0.13079690286660986
iteration : 13556
train acc:  0.7734375
train loss:  0.4908981919288635
train gradient:  0.32536940171696005
iteration : 13557
train acc:  0.8671875
train loss:  0.26010823249816895
train gradient:  0.08326176780428002
iteration : 13558
train acc:  0.8359375
train loss:  0.4025723338127136
train gradient:  0.21370752710773377
iteration : 13559
train acc:  0.8671875
train loss:  0.29607290029525757
train gradient:  0.12907938486234738
iteration : 13560
train acc:  0.859375
train loss:  0.3107908368110657
train gradient:  0.1348384586937555
iteration : 13561
train acc:  0.875
train loss:  0.31677865982055664
train gradient:  0.11416676722781727
iteration : 13562
train acc:  0.8828125
train loss:  0.29415401816368103
train gradient:  0.13809449420366215
iteration : 13563
train acc:  0.890625
train loss:  0.3345763087272644
train gradient:  0.13950035297767271
iteration : 13564
train acc:  0.921875
train loss:  0.28691861033439636
train gradient:  0.14826229720362438
iteration : 13565
train acc:  0.875
train loss:  0.3091960549354553
train gradient:  0.12695030427877058
iteration : 13566
train acc:  0.8203125
train loss:  0.3121103048324585
train gradient:  0.11667198508927894
iteration : 13567
train acc:  0.859375
train loss:  0.33467814326286316
train gradient:  0.14541418279594837
iteration : 13568
train acc:  0.75
train loss:  0.44451743364334106
train gradient:  0.298235255616369
iteration : 13569
train acc:  0.9375
train loss:  0.2503405213356018
train gradient:  0.1052677128006498
iteration : 13570
train acc:  0.828125
train loss:  0.38042154908180237
train gradient:  0.15003595019966207
iteration : 13571
train acc:  0.828125
train loss:  0.3667846918106079
train gradient:  0.13178613306980325
iteration : 13572
train acc:  0.8671875
train loss:  0.31654876470565796
train gradient:  0.1614798443503011
iteration : 13573
train acc:  0.921875
train loss:  0.26500236988067627
train gradient:  0.3641108862456336
iteration : 13574
train acc:  0.859375
train loss:  0.3437826633453369
train gradient:  0.14831319738036067
iteration : 13575
train acc:  0.8828125
train loss:  0.2700057327747345
train gradient:  0.09916716791437295
iteration : 13576
train acc:  0.8359375
train loss:  0.3055749535560608
train gradient:  0.11015361827173889
iteration : 13577
train acc:  0.875
train loss:  0.26881229877471924
train gradient:  0.07619644663024037
iteration : 13578
train acc:  0.8515625
train loss:  0.3584979474544525
train gradient:  0.1278085207957962
iteration : 13579
train acc:  0.9140625
train loss:  0.25011008977890015
train gradient:  0.11995036677445531
iteration : 13580
train acc:  0.859375
train loss:  0.3078099191188812
train gradient:  0.12393794480310769
iteration : 13581
train acc:  0.8828125
train loss:  0.31107789278030396
train gradient:  0.10622171105677676
iteration : 13582
train acc:  0.8671875
train loss:  0.29323810338974
train gradient:  0.10128836671794868
iteration : 13583
train acc:  0.8515625
train loss:  0.3008478283882141
train gradient:  0.13677053055876937
iteration : 13584
train acc:  0.875
train loss:  0.3626113831996918
train gradient:  0.17174571974014277
iteration : 13585
train acc:  0.8671875
train loss:  0.29530203342437744
train gradient:  0.15271566669363867
iteration : 13586
train acc:  0.8515625
train loss:  0.2994367778301239
train gradient:  0.07553633326734004
iteration : 13587
train acc:  0.90625
train loss:  0.2719017267227173
train gradient:  0.11045682893466294
iteration : 13588
train acc:  0.8515625
train loss:  0.29075461626052856
train gradient:  0.13632497035546431
iteration : 13589
train acc:  0.84375
train loss:  0.3619518280029297
train gradient:  0.26205563423192585
iteration : 13590
train acc:  0.890625
train loss:  0.2744576930999756
train gradient:  0.12111092511450228
iteration : 13591
train acc:  0.859375
train loss:  0.35010942816734314
train gradient:  0.1319462705164331
iteration : 13592
train acc:  0.859375
train loss:  0.3174557685852051
train gradient:  0.14890180860836083
iteration : 13593
train acc:  0.875
train loss:  0.33236175775527954
train gradient:  0.11325402146915918
iteration : 13594
train acc:  0.890625
train loss:  0.30650416016578674
train gradient:  0.11226410216087798
iteration : 13595
train acc:  0.8125
train loss:  0.32948315143585205
train gradient:  0.1260867646090586
iteration : 13596
train acc:  0.8671875
train loss:  0.28955066204071045
train gradient:  0.12558280682435136
iteration : 13597
train acc:  0.859375
train loss:  0.32828062772750854
train gradient:  0.25870241124094406
iteration : 13598
train acc:  0.8984375
train loss:  0.26697248220443726
train gradient:  0.07210173361510841
iteration : 13599
train acc:  0.8828125
train loss:  0.29056867957115173
train gradient:  0.06622272416372854
iteration : 13600
train acc:  0.8203125
train loss:  0.4329553246498108
train gradient:  0.21050660998712506
iteration : 13601
train acc:  0.8984375
train loss:  0.26642000675201416
train gradient:  0.1415096163009974
iteration : 13602
train acc:  0.8984375
train loss:  0.2726435959339142
train gradient:  0.09063134823697051
iteration : 13603
train acc:  0.84375
train loss:  0.3742799162864685
train gradient:  0.21235381851558371
iteration : 13604
train acc:  0.859375
train loss:  0.2851974368095398
train gradient:  0.09826461179537994
iteration : 13605
train acc:  0.90625
train loss:  0.2534814774990082
train gradient:  0.0718631005281573
iteration : 13606
train acc:  0.8828125
train loss:  0.27942395210266113
train gradient:  0.10446660092960622
iteration : 13607
train acc:  0.859375
train loss:  0.2878575325012207
train gradient:  0.13240057488346146
iteration : 13608
train acc:  0.8828125
train loss:  0.2726641893386841
train gradient:  0.11973690860058252
iteration : 13609
train acc:  0.828125
train loss:  0.32949119806289673
train gradient:  0.14460571083167273
iteration : 13610
train acc:  0.84375
train loss:  0.31021344661712646
train gradient:  0.16159593889949986
iteration : 13611
train acc:  0.890625
train loss:  0.25092631578445435
train gradient:  0.10768131319890294
iteration : 13612
train acc:  0.828125
train loss:  0.340506911277771
train gradient:  0.14345564040272502
iteration : 13613
train acc:  0.890625
train loss:  0.3261125683784485
train gradient:  0.19895032263082618
iteration : 13614
train acc:  0.921875
train loss:  0.22380714118480682
train gradient:  0.07608562573008387
iteration : 13615
train acc:  0.875
train loss:  0.2598397731781006
train gradient:  0.09632153157987619
iteration : 13616
train acc:  0.90625
train loss:  0.24155578017234802
train gradient:  0.11100823257467851
iteration : 13617
train acc:  0.8671875
train loss:  0.3031429052352905
train gradient:  0.14737260231321686
iteration : 13618
train acc:  0.859375
train loss:  0.31059926748275757
train gradient:  0.16685707187634147
iteration : 13619
train acc:  0.8671875
train loss:  0.3472679555416107
train gradient:  0.19055166887304015
iteration : 13620
train acc:  0.859375
train loss:  0.32593998312950134
train gradient:  0.12480769323936218
iteration : 13621
train acc:  0.8828125
train loss:  0.31354954838752747
train gradient:  0.1118958041437376
iteration : 13622
train acc:  0.8984375
train loss:  0.2594189941883087
train gradient:  0.12526469743339846
iteration : 13623
train acc:  0.8515625
train loss:  0.3304001986980438
train gradient:  0.17682509947792502
iteration : 13624
train acc:  0.8359375
train loss:  0.3338615298271179
train gradient:  0.18523141414216787
iteration : 13625
train acc:  0.859375
train loss:  0.2929207980632782
train gradient:  0.14414922033041394
iteration : 13626
train acc:  0.8984375
train loss:  0.25126856565475464
train gradient:  0.08705735401593717
iteration : 13627
train acc:  0.828125
train loss:  0.32895803451538086
train gradient:  0.13257763130827815
iteration : 13628
train acc:  0.796875
train loss:  0.40785014629364014
train gradient:  0.21011258618817513
iteration : 13629
train acc:  0.84375
train loss:  0.36884358525276184
train gradient:  0.22030930436863488
iteration : 13630
train acc:  0.8359375
train loss:  0.38171204924583435
train gradient:  0.16980260565580368
iteration : 13631
train acc:  0.875
train loss:  0.2531094551086426
train gradient:  0.131532924986047
iteration : 13632
train acc:  0.828125
train loss:  0.3461771011352539
train gradient:  0.15265122120675273
iteration : 13633
train acc:  0.8515625
train loss:  0.34947896003723145
train gradient:  0.14792619914175748
iteration : 13634
train acc:  0.859375
train loss:  0.3980860412120819
train gradient:  0.20918261684001643
iteration : 13635
train acc:  0.859375
train loss:  0.37068411707878113
train gradient:  0.16918705898987174
iteration : 13636
train acc:  0.875
train loss:  0.3520594835281372
train gradient:  0.15882271667713363
iteration : 13637
train acc:  0.8203125
train loss:  0.3547859191894531
train gradient:  0.17356808325521153
iteration : 13638
train acc:  0.8359375
train loss:  0.3816770613193512
train gradient:  0.18037846246446498
iteration : 13639
train acc:  0.8125
train loss:  0.40251997113227844
train gradient:  0.1876531615060732
iteration : 13640
train acc:  0.875
train loss:  0.3175226151943207
train gradient:  0.1140616518969582
iteration : 13641
train acc:  0.8671875
train loss:  0.3328395485877991
train gradient:  0.15865699959106044
iteration : 13642
train acc:  0.84375
train loss:  0.34200429916381836
train gradient:  0.16068326129082844
iteration : 13643
train acc:  0.84375
train loss:  0.31005969643592834
train gradient:  0.12338905934539084
iteration : 13644
train acc:  0.875
train loss:  0.3157830536365509
train gradient:  0.1062635673311426
iteration : 13645
train acc:  0.8984375
train loss:  0.32801297307014465
train gradient:  0.11029126719507863
iteration : 13646
train acc:  0.828125
train loss:  0.372156023979187
train gradient:  0.21199648420161515
iteration : 13647
train acc:  0.8828125
train loss:  0.3052394986152649
train gradient:  0.10112360659151819
iteration : 13648
train acc:  0.8515625
train loss:  0.37752771377563477
train gradient:  0.2657553923833434
iteration : 13649
train acc:  0.8671875
train loss:  0.39028576016426086
train gradient:  0.16107255229418052
iteration : 13650
train acc:  0.8828125
train loss:  0.27001991868019104
train gradient:  0.09791005449829575
iteration : 13651
train acc:  0.9140625
train loss:  0.22349949181079865
train gradient:  0.10071602521116704
iteration : 13652
train acc:  0.875
train loss:  0.28781500458717346
train gradient:  0.10875265849603816
iteration : 13653
train acc:  0.8828125
train loss:  0.2717468738555908
train gradient:  0.09774102316728694
iteration : 13654
train acc:  0.8671875
train loss:  0.27862608432769775
train gradient:  0.10351038241094702
iteration : 13655
train acc:  0.890625
train loss:  0.24514630436897278
train gradient:  0.07624316070219263
iteration : 13656
train acc:  0.8515625
train loss:  0.2732580900192261
train gradient:  0.14802321913507954
iteration : 13657
train acc:  0.8359375
train loss:  0.38479694724082947
train gradient:  0.14666955223175837
iteration : 13658
train acc:  0.890625
train loss:  0.28520071506500244
train gradient:  0.1713143821071596
iteration : 13659
train acc:  0.8359375
train loss:  0.42917221784591675
train gradient:  0.26920304882797597
iteration : 13660
train acc:  0.859375
train loss:  0.3188156187534332
train gradient:  0.1287755264500859
iteration : 13661
train acc:  0.8125
train loss:  0.3225395083427429
train gradient:  0.16005240641779273
iteration : 13662
train acc:  0.90625
train loss:  0.27639538049697876
train gradient:  0.09620736842306064
iteration : 13663
train acc:  0.8671875
train loss:  0.36648789048194885
train gradient:  0.18433784771633277
iteration : 13664
train acc:  0.890625
train loss:  0.3199062943458557
train gradient:  0.12903521435328055
iteration : 13665
train acc:  0.828125
train loss:  0.3538106679916382
train gradient:  0.15700963774254037
iteration : 13666
train acc:  0.828125
train loss:  0.3626607060432434
train gradient:  0.1461895678169239
iteration : 13667
train acc:  0.828125
train loss:  0.35716351866722107
train gradient:  0.14476646696327075
iteration : 13668
train acc:  0.859375
train loss:  0.3460538685321808
train gradient:  0.14564263816093198
iteration : 13669
train acc:  0.828125
train loss:  0.3185185194015503
train gradient:  0.21295755473710498
iteration : 13670
train acc:  0.828125
train loss:  0.3129885196685791
train gradient:  0.12653798041497927
iteration : 13671
train acc:  0.90625
train loss:  0.24822333455085754
train gradient:  0.07507510246261714
iteration : 13672
train acc:  0.8515625
train loss:  0.3414270281791687
train gradient:  0.16955423268433306
iteration : 13673
train acc:  0.84375
train loss:  0.3140488266944885
train gradient:  0.178069676782839
iteration : 13674
train acc:  0.8828125
train loss:  0.26087215542793274
train gradient:  0.11310519047602802
iteration : 13675
train acc:  0.859375
train loss:  0.2780310809612274
train gradient:  0.13685204526719275
iteration : 13676
train acc:  0.859375
train loss:  0.3036959767341614
train gradient:  0.14844220067587305
iteration : 13677
train acc:  0.859375
train loss:  0.3175261914730072
train gradient:  0.11180608140248599
iteration : 13678
train acc:  0.90625
train loss:  0.27915626764297485
train gradient:  0.09317666963601419
iteration : 13679
train acc:  0.8828125
train loss:  0.2767139673233032
train gradient:  0.14417278577439463
iteration : 13680
train acc:  0.828125
train loss:  0.31848952174186707
train gradient:  0.12401891414502626
iteration : 13681
train acc:  0.8515625
train loss:  0.32694900035858154
train gradient:  0.12083070949214528
iteration : 13682
train acc:  0.84375
train loss:  0.3528060019016266
train gradient:  0.1644207625455087
iteration : 13683
train acc:  0.8828125
train loss:  0.2787380814552307
train gradient:  0.14008207694790026
iteration : 13684
train acc:  0.8671875
train loss:  0.33066070079803467
train gradient:  0.20848222081700718
iteration : 13685
train acc:  0.890625
train loss:  0.2789999842643738
train gradient:  0.07749864356336948
iteration : 13686
train acc:  0.8046875
train loss:  0.4143669009208679
train gradient:  0.17738819951518248
iteration : 13687
train acc:  0.84375
train loss:  0.3288722336292267
train gradient:  0.1550265957431541
iteration : 13688
train acc:  0.875
train loss:  0.27072107791900635
train gradient:  0.10965116626574174
iteration : 13689
train acc:  0.875
train loss:  0.35445088148117065
train gradient:  0.1253323663247899
iteration : 13690
train acc:  0.8515625
train loss:  0.32307004928588867
train gradient:  0.1530532845229369
iteration : 13691
train acc:  0.875
train loss:  0.2654307782649994
train gradient:  0.08869752659859167
iteration : 13692
train acc:  0.9375
train loss:  0.21555328369140625
train gradient:  0.08755196077426408
iteration : 13693
train acc:  0.9140625
train loss:  0.2491803765296936
train gradient:  0.0763581928035327
iteration : 13694
train acc:  0.8671875
train loss:  0.3285359740257263
train gradient:  0.13898143965541537
iteration : 13695
train acc:  0.8515625
train loss:  0.3328062891960144
train gradient:  0.13351209204678344
iteration : 13696
train acc:  0.8359375
train loss:  0.3187892436981201
train gradient:  0.14666687452852256
iteration : 13697
train acc:  0.8359375
train loss:  0.35421282052993774
train gradient:  0.14098859580618706
iteration : 13698
train acc:  0.859375
train loss:  0.2752733826637268
train gradient:  0.08029472997093112
iteration : 13699
train acc:  0.84375
train loss:  0.29913148283958435
train gradient:  0.10643343387474515
iteration : 13700
train acc:  0.8125
train loss:  0.3527268171310425
train gradient:  0.14158639194056485
iteration : 13701
train acc:  0.921875
train loss:  0.24327975511550903
train gradient:  0.07147953150160556
iteration : 13702
train acc:  0.8828125
train loss:  0.2711948752403259
train gradient:  0.11275527357866365
iteration : 13703
train acc:  0.8671875
train loss:  0.3226662576198578
train gradient:  0.11802931241815802
iteration : 13704
train acc:  0.875
train loss:  0.27593037486076355
train gradient:  0.09748743572220224
iteration : 13705
train acc:  0.8515625
train loss:  0.30745571851730347
train gradient:  0.10716127979033735
iteration : 13706
train acc:  0.8828125
train loss:  0.28874513506889343
train gradient:  0.14945348966945754
iteration : 13707
train acc:  0.8515625
train loss:  0.3162214756011963
train gradient:  0.12115770477245784
iteration : 13708
train acc:  0.875
train loss:  0.29319486021995544
train gradient:  0.15545974909124027
iteration : 13709
train acc:  0.7890625
train loss:  0.3818252682685852
train gradient:  0.14198185449972675
iteration : 13710
train acc:  0.859375
train loss:  0.29118913412094116
train gradient:  0.10055073188854409
iteration : 13711
train acc:  0.8984375
train loss:  0.32823675870895386
train gradient:  0.11857756241506871
iteration : 13712
train acc:  0.9140625
train loss:  0.2656767964363098
train gradient:  0.1127153961717334
iteration : 13713
train acc:  0.8515625
train loss:  0.3207254409790039
train gradient:  0.11442665437837185
iteration : 13714
train acc:  0.875
train loss:  0.3234240710735321
train gradient:  0.12253530431917728
iteration : 13715
train acc:  0.90625
train loss:  0.28485965728759766
train gradient:  0.10578589932190553
iteration : 13716
train acc:  0.796875
train loss:  0.352974534034729
train gradient:  0.24658442185647975
iteration : 13717
train acc:  0.8125
train loss:  0.3398449420928955
train gradient:  0.16631565585945315
iteration : 13718
train acc:  0.890625
train loss:  0.25167572498321533
train gradient:  0.09845980723935688
iteration : 13719
train acc:  0.859375
train loss:  0.31660932302474976
train gradient:  0.09446127941809714
iteration : 13720
train acc:  0.8828125
train loss:  0.2587408721446991
train gradient:  0.12194783065165588
iteration : 13721
train acc:  0.8984375
train loss:  0.27722033858299255
train gradient:  0.09339126312792795
iteration : 13722
train acc:  0.8125
train loss:  0.38108134269714355
train gradient:  0.1266436727929735
iteration : 13723
train acc:  0.859375
train loss:  0.3522500991821289
train gradient:  0.1495206356188612
iteration : 13724
train acc:  0.8359375
train loss:  0.3299350440502167
train gradient:  0.12995339908146689
iteration : 13725
train acc:  0.8828125
train loss:  0.2703425884246826
train gradient:  0.08853957639567506
iteration : 13726
train acc:  0.90625
train loss:  0.2643485367298126
train gradient:  0.09676937955729879
iteration : 13727
train acc:  0.921875
train loss:  0.2545037567615509
train gradient:  0.09453804934181184
iteration : 13728
train acc:  0.859375
train loss:  0.29311689734458923
train gradient:  0.13602631453011138
iteration : 13729
train acc:  0.859375
train loss:  0.2827157974243164
train gradient:  0.12247778493057686
iteration : 13730
train acc:  0.8828125
train loss:  0.2688398063182831
train gradient:  0.0847433423330848
iteration : 13731
train acc:  0.8359375
train loss:  0.38270384073257446
train gradient:  0.19018180629914463
iteration : 13732
train acc:  0.890625
train loss:  0.29554787278175354
train gradient:  0.09911149565616617
iteration : 13733
train acc:  0.8203125
train loss:  0.33741044998168945
train gradient:  0.1567398262257657
iteration : 13734
train acc:  0.859375
train loss:  0.2913911044597626
train gradient:  0.1393427646997515
iteration : 13735
train acc:  0.90625
train loss:  0.2839357554912567
train gradient:  0.12138907271827183
iteration : 13736
train acc:  0.8671875
train loss:  0.27924007177352905
train gradient:  0.145653218033023
iteration : 13737
train acc:  0.8671875
train loss:  0.3003997206687927
train gradient:  0.13472797960624094
iteration : 13738
train acc:  0.859375
train loss:  0.34573376178741455
train gradient:  0.13012334971835082
iteration : 13739
train acc:  0.875
train loss:  0.23019279539585114
train gradient:  0.10415181954218906
iteration : 13740
train acc:  0.8203125
train loss:  0.36656156182289124
train gradient:  0.14623489770080134
iteration : 13741
train acc:  0.8671875
train loss:  0.3306436538696289
train gradient:  0.25487157446418773
iteration : 13742
train acc:  0.875
train loss:  0.27055996656417847
train gradient:  0.1182992074024093
iteration : 13743
train acc:  0.828125
train loss:  0.33764058351516724
train gradient:  0.14087905681019747
iteration : 13744
train acc:  0.8203125
train loss:  0.37002724409103394
train gradient:  0.19676852221681512
iteration : 13745
train acc:  0.890625
train loss:  0.2769588232040405
train gradient:  0.11810422357471696
iteration : 13746
train acc:  0.859375
train loss:  0.30456268787384033
train gradient:  0.1550270614401444
iteration : 13747
train acc:  0.8515625
train loss:  0.31740137934684753
train gradient:  0.13212215418901718
iteration : 13748
train acc:  0.8828125
train loss:  0.2811630368232727
train gradient:  0.20550084778533562
iteration : 13749
train acc:  0.84375
train loss:  0.3750719428062439
train gradient:  0.1623596610643186
iteration : 13750
train acc:  0.828125
train loss:  0.3726128935813904
train gradient:  0.2001329948065413
iteration : 13751
train acc:  0.8125
train loss:  0.43024104833602905
train gradient:  0.27986015896004063
iteration : 13752
train acc:  0.8359375
train loss:  0.31647244095802307
train gradient:  0.10225902501667727
iteration : 13753
train acc:  0.859375
train loss:  0.36965781450271606
train gradient:  0.19198904221866836
iteration : 13754
train acc:  0.859375
train loss:  0.339321494102478
train gradient:  0.14015857279496596
iteration : 13755
train acc:  0.890625
train loss:  0.34669262170791626
train gradient:  0.2184246149812844
iteration : 13756
train acc:  0.8515625
train loss:  0.3192402720451355
train gradient:  0.19133109200725568
iteration : 13757
train acc:  0.8125
train loss:  0.4000628888607025
train gradient:  0.23498266346011376
iteration : 13758
train acc:  0.8671875
train loss:  0.30003049969673157
train gradient:  0.17121421446762874
iteration : 13759
train acc:  0.8515625
train loss:  0.3284579813480377
train gradient:  0.20761063008154224
iteration : 13760
train acc:  0.859375
train loss:  0.319316029548645
train gradient:  0.1380781072187866
iteration : 13761
train acc:  0.890625
train loss:  0.2408982515335083
train gradient:  0.11379914776090194
iteration : 13762
train acc:  0.8515625
train loss:  0.38046008348464966
train gradient:  0.17234722783983625
iteration : 13763
train acc:  0.90625
train loss:  0.246209055185318
train gradient:  0.07747442656886222
iteration : 13764
train acc:  0.921875
train loss:  0.2573056221008301
train gradient:  0.0952316155410217
iteration : 13765
train acc:  0.8125
train loss:  0.390567421913147
train gradient:  0.22755652676166277
iteration : 13766
train acc:  0.8828125
train loss:  0.25824835896492004
train gradient:  0.09913273333537938
iteration : 13767
train acc:  0.8359375
train loss:  0.3396884799003601
train gradient:  0.15690518894260513
iteration : 13768
train acc:  0.90625
train loss:  0.26062119007110596
train gradient:  0.10387273774948867
iteration : 13769
train acc:  0.8203125
train loss:  0.35168981552124023
train gradient:  0.16100403963162746
iteration : 13770
train acc:  0.875
train loss:  0.3237549662590027
train gradient:  0.14123414276855273
iteration : 13771
train acc:  0.8984375
train loss:  0.28916969895362854
train gradient:  0.14773912231420566
iteration : 13772
train acc:  0.8828125
train loss:  0.2677529752254486
train gradient:  0.08872813308093988
iteration : 13773
train acc:  0.8671875
train loss:  0.31530851125717163
train gradient:  0.12661425807027415
iteration : 13774
train acc:  0.8671875
train loss:  0.32378777861595154
train gradient:  0.16387170781568833
iteration : 13775
train acc:  0.8359375
train loss:  0.3019169569015503
train gradient:  0.21054203791299644
iteration : 13776
train acc:  0.8359375
train loss:  0.39787617325782776
train gradient:  0.21323935443884823
iteration : 13777
train acc:  0.8515625
train loss:  0.31219902634620667
train gradient:  0.16873541466473718
iteration : 13778
train acc:  0.9140625
train loss:  0.2313464879989624
train gradient:  0.091763821509858
iteration : 13779
train acc:  0.8125
train loss:  0.3663238286972046
train gradient:  0.22382685990710155
iteration : 13780
train acc:  0.828125
train loss:  0.3683708608150482
train gradient:  0.12276461716103965
iteration : 13781
train acc:  0.8125
train loss:  0.3750588297843933
train gradient:  0.22883104952605782
iteration : 13782
train acc:  0.859375
train loss:  0.35341328382492065
train gradient:  0.18676325572671842
iteration : 13783
train acc:  0.828125
train loss:  0.3274388313293457
train gradient:  0.14764988593713246
iteration : 13784
train acc:  0.84375
train loss:  0.3685762882232666
train gradient:  0.18090157700355308
iteration : 13785
train acc:  0.8515625
train loss:  0.3143894672393799
train gradient:  0.1416821605984777
iteration : 13786
train acc:  0.84375
train loss:  0.347085565328598
train gradient:  0.14813562209232273
iteration : 13787
train acc:  0.859375
train loss:  0.2881236970424652
train gradient:  0.101257693860282
iteration : 13788
train acc:  0.890625
train loss:  0.270422101020813
train gradient:  0.13195723572186427
iteration : 13789
train acc:  0.84375
train loss:  0.33538150787353516
train gradient:  0.16312742953722026
iteration : 13790
train acc:  0.90625
train loss:  0.22195810079574585
train gradient:  0.08932660564766703
iteration : 13791
train acc:  0.9140625
train loss:  0.25825390219688416
train gradient:  0.1406924062951442
iteration : 13792
train acc:  0.875
train loss:  0.2999241054058075
train gradient:  0.1261102857018707
iteration : 13793
train acc:  0.8828125
train loss:  0.29151690006256104
train gradient:  0.11265230180152183
iteration : 13794
train acc:  0.8984375
train loss:  0.27022290229797363
train gradient:  0.20161739632253622
iteration : 13795
train acc:  0.875
train loss:  0.2522401809692383
train gradient:  0.09853471676454038
iteration : 13796
train acc:  0.8515625
train loss:  0.35170236229896545
train gradient:  0.14433046164470234
iteration : 13797
train acc:  0.84375
train loss:  0.37295418977737427
train gradient:  0.31697977206791644
iteration : 13798
train acc:  0.859375
train loss:  0.3083740472793579
train gradient:  0.11794793558584175
iteration : 13799
train acc:  0.90625
train loss:  0.25468260049819946
train gradient:  0.0952293674784015
iteration : 13800
train acc:  0.8828125
train loss:  0.3388303518295288
train gradient:  0.12215164889081105
iteration : 13801
train acc:  0.8828125
train loss:  0.2750428020954132
train gradient:  0.12636778699770768
iteration : 13802
train acc:  0.8984375
train loss:  0.28972530364990234
train gradient:  0.1488411155784392
iteration : 13803
train acc:  0.8671875
train loss:  0.3019130825996399
train gradient:  0.0981486143436589
iteration : 13804
train acc:  0.8828125
train loss:  0.27843379974365234
train gradient:  0.15258032502259927
iteration : 13805
train acc:  0.8984375
train loss:  0.2721550464630127
train gradient:  0.08758588191786008
iteration : 13806
train acc:  0.859375
train loss:  0.37773022055625916
train gradient:  0.22529459930827234
iteration : 13807
train acc:  0.8828125
train loss:  0.2513670027256012
train gradient:  0.07524831279293033
iteration : 13808
train acc:  0.7890625
train loss:  0.35820066928863525
train gradient:  0.11473981106563362
iteration : 13809
train acc:  0.84375
train loss:  0.3581831157207489
train gradient:  0.1667793025790924
iteration : 13810
train acc:  0.859375
train loss:  0.372632771730423
train gradient:  0.21502682395811085
iteration : 13811
train acc:  0.890625
train loss:  0.2719551622867584
train gradient:  0.1403179282826643
iteration : 13812
train acc:  0.84375
train loss:  0.3774639368057251
train gradient:  0.1856580041452416
iteration : 13813
train acc:  0.8984375
train loss:  0.26730334758758545
train gradient:  0.10133942602060154
iteration : 13814
train acc:  0.8671875
train loss:  0.2781802713871002
train gradient:  0.11118600765156153
iteration : 13815
train acc:  0.8359375
train loss:  0.3947874903678894
train gradient:  0.28852505627165614
iteration : 13816
train acc:  0.84375
train loss:  0.28649604320526123
train gradient:  0.10864219241981336
iteration : 13817
train acc:  0.8671875
train loss:  0.32669568061828613
train gradient:  0.12090797141452725
iteration : 13818
train acc:  0.8515625
train loss:  0.35547009110450745
train gradient:  0.17986864255826018
iteration : 13819
train acc:  0.859375
train loss:  0.32241010665893555
train gradient:  0.09692139840827195
iteration : 13820
train acc:  0.859375
train loss:  0.30206140875816345
train gradient:  0.08644348592582822
iteration : 13821
train acc:  0.875
train loss:  0.26198986172676086
train gradient:  0.1092460570639915
iteration : 13822
train acc:  0.859375
train loss:  0.32637691497802734
train gradient:  0.133278437528134
iteration : 13823
train acc:  0.7890625
train loss:  0.3803708851337433
train gradient:  0.1378823786477608
iteration : 13824
train acc:  0.8671875
train loss:  0.3386824131011963
train gradient:  0.13362271349292365
iteration : 13825
train acc:  0.875
train loss:  0.306121289730072
train gradient:  0.0988431780578403
iteration : 13826
train acc:  0.921875
train loss:  0.251380980014801
train gradient:  0.09685840848368973
iteration : 13827
train acc:  0.8359375
train loss:  0.365081250667572
train gradient:  0.13185581031978194
iteration : 13828
train acc:  0.8515625
train loss:  0.2800673246383667
train gradient:  0.0984258668923858
iteration : 13829
train acc:  0.8203125
train loss:  0.34689104557037354
train gradient:  0.14778657169764134
iteration : 13830
train acc:  0.8515625
train loss:  0.3215375244617462
train gradient:  0.1409695128956485
iteration : 13831
train acc:  0.8828125
train loss:  0.3015856146812439
train gradient:  0.14961862453300656
iteration : 13832
train acc:  0.8671875
train loss:  0.3005434274673462
train gradient:  0.08785558710652051
iteration : 13833
train acc:  0.8671875
train loss:  0.32983875274658203
train gradient:  0.1250210033070268
iteration : 13834
train acc:  0.9375
train loss:  0.2103199064731598
train gradient:  0.057589079191497375
iteration : 13835
train acc:  0.890625
train loss:  0.2751297354698181
train gradient:  0.11696565557016594
iteration : 13836
train acc:  0.875
train loss:  0.32087743282318115
train gradient:  0.13695137462326534
iteration : 13837
train acc:  0.8984375
train loss:  0.23140257596969604
train gradient:  0.10869965152050766
iteration : 13838
train acc:  0.8125
train loss:  0.39563190937042236
train gradient:  0.18396674418562292
iteration : 13839
train acc:  0.828125
train loss:  0.32257944345474243
train gradient:  0.2584877814330113
iteration : 13840
train acc:  0.875
train loss:  0.2955731749534607
train gradient:  0.11386246726278348
iteration : 13841
train acc:  0.90625
train loss:  0.2560901641845703
train gradient:  0.10035011130836562
iteration : 13842
train acc:  0.90625
train loss:  0.24558895826339722
train gradient:  0.09838588101965162
iteration : 13843
train acc:  0.796875
train loss:  0.397752583026886
train gradient:  0.13810004357850209
iteration : 13844
train acc:  0.84375
train loss:  0.304785817861557
train gradient:  0.0980189817038691
iteration : 13845
train acc:  0.890625
train loss:  0.26707300543785095
train gradient:  0.0639518539779175
iteration : 13846
train acc:  0.8515625
train loss:  0.3791445791721344
train gradient:  0.24444656489265926
iteration : 13847
train acc:  0.8828125
train loss:  0.3023206889629364
train gradient:  0.10346671892695614
iteration : 13848
train acc:  0.875
train loss:  0.28069043159484863
train gradient:  0.1221964060894032
iteration : 13849
train acc:  0.7734375
train loss:  0.4621518850326538
train gradient:  0.35834749349209705
iteration : 13850
train acc:  0.859375
train loss:  0.3158751428127289
train gradient:  0.11712281965447995
iteration : 13851
train acc:  0.8203125
train loss:  0.39146748185157776
train gradient:  0.19959923228899792
iteration : 13852
train acc:  0.8515625
train loss:  0.36377018690109253
train gradient:  0.1623165688594674
iteration : 13853
train acc:  0.8125
train loss:  0.40364736318588257
train gradient:  0.25379877434086057
iteration : 13854
train acc:  0.8671875
train loss:  0.34357112646102905
train gradient:  0.11769720341091397
iteration : 13855
train acc:  0.8515625
train loss:  0.29476937651634216
train gradient:  0.13274011797530694
iteration : 13856
train acc:  0.875
train loss:  0.28204798698425293
train gradient:  0.08129772551395505
iteration : 13857
train acc:  0.8046875
train loss:  0.4411763548851013
train gradient:  0.2562800758788814
iteration : 13858
train acc:  0.8515625
train loss:  0.2803254723548889
train gradient:  0.1265981716615631
iteration : 13859
train acc:  0.90625
train loss:  0.3386112451553345
train gradient:  0.16385063283534715
iteration : 13860
train acc:  0.8984375
train loss:  0.255524218082428
train gradient:  0.062471149167760906
iteration : 13861
train acc:  0.8046875
train loss:  0.4007027745246887
train gradient:  0.18716330240642237
iteration : 13862
train acc:  0.84375
train loss:  0.3427164554595947
train gradient:  0.1421987029238168
iteration : 13863
train acc:  0.8515625
train loss:  0.33487963676452637
train gradient:  0.12271523948358579
iteration : 13864
train acc:  0.9296875
train loss:  0.2099185585975647
train gradient:  0.11404906129826312
iteration : 13865
train acc:  0.84375
train loss:  0.376417875289917
train gradient:  0.15450777414351768
iteration : 13866
train acc:  0.9140625
train loss:  0.26184791326522827
train gradient:  0.09605384509517259
iteration : 13867
train acc:  0.875
train loss:  0.26675692200660706
train gradient:  0.09740964686714189
iteration : 13868
train acc:  0.8984375
train loss:  0.2657870054244995
train gradient:  0.09735487181394599
iteration : 13869
train acc:  0.90625
train loss:  0.2508934736251831
train gradient:  0.09673290397052839
iteration : 13870
train acc:  0.8828125
train loss:  0.2553667724132538
train gradient:  0.12537604811230552
iteration : 13871
train acc:  0.8671875
train loss:  0.25564438104629517
train gradient:  0.11748841027498225
iteration : 13872
train acc:  0.8515625
train loss:  0.3767831027507782
train gradient:  0.17853583390290373
iteration : 13873
train acc:  0.84375
train loss:  0.3514356017112732
train gradient:  0.15068985654032752
iteration : 13874
train acc:  0.859375
train loss:  0.28196489810943604
train gradient:  0.10265219342489297
iteration : 13875
train acc:  0.84375
train loss:  0.34987673163414
train gradient:  0.23791125153999737
iteration : 13876
train acc:  0.8515625
train loss:  0.3357754349708557
train gradient:  0.24369960847733774
iteration : 13877
train acc:  0.8515625
train loss:  0.34147024154663086
train gradient:  0.14580697723769248
iteration : 13878
train acc:  0.8671875
train loss:  0.3057577311992645
train gradient:  0.10345928405945676
iteration : 13879
train acc:  0.8984375
train loss:  0.2663067579269409
train gradient:  0.12133032846391921
iteration : 13880
train acc:  0.8984375
train loss:  0.2639140784740448
train gradient:  0.12618929003563503
iteration : 13881
train acc:  0.8828125
train loss:  0.2890591621398926
train gradient:  0.1674643153696525
iteration : 13882
train acc:  0.8828125
train loss:  0.25760534405708313
train gradient:  0.14285323744156064
iteration : 13883
train acc:  0.8203125
train loss:  0.36492103338241577
train gradient:  0.12902270468314783
iteration : 13884
train acc:  0.8984375
train loss:  0.28286415338516235
train gradient:  0.1320065946160104
iteration : 13885
train acc:  0.8671875
train loss:  0.2949691414833069
train gradient:  0.08914482898105366
iteration : 13886
train acc:  0.859375
train loss:  0.32251739501953125
train gradient:  0.14839062088431348
iteration : 13887
train acc:  0.859375
train loss:  0.31270837783813477
train gradient:  0.13333167583143635
iteration : 13888
train acc:  0.8828125
train loss:  0.28513088822364807
train gradient:  0.14673158810134523
iteration : 13889
train acc:  0.8203125
train loss:  0.3123313784599304
train gradient:  0.12692702897536925
iteration : 13890
train acc:  0.8046875
train loss:  0.4449975788593292
train gradient:  0.2980011026089218
iteration : 13891
train acc:  0.8125
train loss:  0.36125966906547546
train gradient:  0.15209989118991543
iteration : 13892
train acc:  0.859375
train loss:  0.3680501878261566
train gradient:  0.16497531928741177
iteration : 13893
train acc:  0.921875
train loss:  0.23164668679237366
train gradient:  0.09479736605997055
iteration : 13894
train acc:  0.9140625
train loss:  0.2794649302959442
train gradient:  0.2888066887935321
iteration : 13895
train acc:  0.8984375
train loss:  0.256852388381958
train gradient:  0.10584109947501143
iteration : 13896
train acc:  0.875
train loss:  0.26526081562042236
train gradient:  0.10472739335833164
iteration : 13897
train acc:  0.875
train loss:  0.293042927980423
train gradient:  0.13602970782551577
iteration : 13898
train acc:  0.8515625
train loss:  0.33587658405303955
train gradient:  0.0897021718570319
iteration : 13899
train acc:  0.8828125
train loss:  0.28673940896987915
train gradient:  0.14468321317427207
iteration : 13900
train acc:  0.859375
train loss:  0.3214106559753418
train gradient:  0.09789953710432099
iteration : 13901
train acc:  0.859375
train loss:  0.3164370656013489
train gradient:  0.11565252819558015
iteration : 13902
train acc:  0.8828125
train loss:  0.2952605187892914
train gradient:  0.10280365332774717
iteration : 13903
train acc:  0.859375
train loss:  0.30622345209121704
train gradient:  0.12698635432778543
iteration : 13904
train acc:  0.828125
train loss:  0.3450026512145996
train gradient:  0.1357424518027786
iteration : 13905
train acc:  0.875
train loss:  0.3637596368789673
train gradient:  0.19363495726760613
iteration : 13906
train acc:  0.8828125
train loss:  0.35142701864242554
train gradient:  0.13290286924376882
iteration : 13907
train acc:  0.890625
train loss:  0.2779659032821655
train gradient:  0.23967394530778957
iteration : 13908
train acc:  0.8828125
train loss:  0.2714979648590088
train gradient:  0.10120830790901332
iteration : 13909
train acc:  0.8828125
train loss:  0.33348318934440613
train gradient:  0.10707183972786861
iteration : 13910
train acc:  0.8828125
train loss:  0.30673569440841675
train gradient:  0.1336265101264441
iteration : 13911
train acc:  0.84375
train loss:  0.3303121328353882
train gradient:  0.1420779231711743
iteration : 13912
train acc:  0.9140625
train loss:  0.2333458662033081
train gradient:  0.08839304688365733
iteration : 13913
train acc:  0.859375
train loss:  0.27043986320495605
train gradient:  0.08131010329887879
iteration : 13914
train acc:  0.8203125
train loss:  0.3717115521430969
train gradient:  0.1451517232076056
iteration : 13915
train acc:  0.8359375
train loss:  0.2989209294319153
train gradient:  0.1264432812623601
iteration : 13916
train acc:  0.8671875
train loss:  0.32771822810173035
train gradient:  0.20626570182819653
iteration : 13917
train acc:  0.8984375
train loss:  0.2939980626106262
train gradient:  0.11589989133514148
iteration : 13918
train acc:  0.8671875
train loss:  0.30940210819244385
train gradient:  0.12151636645501372
iteration : 13919
train acc:  0.890625
train loss:  0.2617432177066803
train gradient:  0.07760477751107424
iteration : 13920
train acc:  0.875
train loss:  0.26953160762786865
train gradient:  0.09963278787209331
iteration : 13921
train acc:  0.8828125
train loss:  0.27645808458328247
train gradient:  0.09592309012027689
iteration : 13922
train acc:  0.8828125
train loss:  0.2862219214439392
train gradient:  0.08517211017732651
iteration : 13923
train acc:  0.890625
train loss:  0.27318161725997925
train gradient:  0.08694793352219535
iteration : 13924
train acc:  0.8828125
train loss:  0.2817501425743103
train gradient:  0.09700398452231886
iteration : 13925
train acc:  0.8515625
train loss:  0.38493812084198
train gradient:  0.19419342334468054
iteration : 13926
train acc:  0.828125
train loss:  0.41623520851135254
train gradient:  0.22838462035986143
iteration : 13927
train acc:  0.8359375
train loss:  0.3437751233577728
train gradient:  0.17358230557116233
iteration : 13928
train acc:  0.8671875
train loss:  0.2975197434425354
train gradient:  0.14794336829142665
iteration : 13929
train acc:  0.859375
train loss:  0.3292863070964813
train gradient:  0.25398546714030457
iteration : 13930
train acc:  0.8203125
train loss:  0.3597596287727356
train gradient:  0.25491820016057015
iteration : 13931
train acc:  0.875
train loss:  0.3120063245296478
train gradient:  0.11826505155992151
iteration : 13932
train acc:  0.90625
train loss:  0.24543771147727966
train gradient:  0.15901998977189152
iteration : 13933
train acc:  0.859375
train loss:  0.3047184348106384
train gradient:  0.11940732130487411
iteration : 13934
train acc:  0.875
train loss:  0.3057928681373596
train gradient:  0.1184332131732339
iteration : 13935
train acc:  0.8515625
train loss:  0.286946177482605
train gradient:  0.109663755335309
iteration : 13936
train acc:  0.859375
train loss:  0.3435221314430237
train gradient:  0.14809767888326486
iteration : 13937
train acc:  0.890625
train loss:  0.2518399953842163
train gradient:  0.10155375577809785
iteration : 13938
train acc:  0.859375
train loss:  0.334283709526062
train gradient:  0.1419836526864958
iteration : 13939
train acc:  0.859375
train loss:  0.390301913022995
train gradient:  0.18499557409386708
iteration : 13940
train acc:  0.8359375
train loss:  0.37999725341796875
train gradient:  0.2030058324793229
iteration : 13941
train acc:  0.8671875
train loss:  0.3118221163749695
train gradient:  0.1243005477421361
iteration : 13942
train acc:  0.8203125
train loss:  0.4061925709247589
train gradient:  0.23511014282527892
iteration : 13943
train acc:  0.859375
train loss:  0.3302188515663147
train gradient:  0.3145798063637943
iteration : 13944
train acc:  0.890625
train loss:  0.30513930320739746
train gradient:  0.2689553139140825
iteration : 13945
train acc:  0.890625
train loss:  0.2763040363788605
train gradient:  0.11560554548303166
iteration : 13946
train acc:  0.9140625
train loss:  0.26850998401641846
train gradient:  0.09921504713283126
iteration : 13947
train acc:  0.8671875
train loss:  0.36090075969696045
train gradient:  0.16785314633421647
iteration : 13948
train acc:  0.859375
train loss:  0.33162617683410645
train gradient:  0.17922933573175914
iteration : 13949
train acc:  0.8203125
train loss:  0.3897809684276581
train gradient:  0.17585828110434654
iteration : 13950
train acc:  0.9140625
train loss:  0.27870190143585205
train gradient:  0.10371364337430142
iteration : 13951
train acc:  0.8359375
train loss:  0.3524758219718933
train gradient:  0.14421058847140233
iteration : 13952
train acc:  0.9140625
train loss:  0.25215283036231995
train gradient:  0.07389312671133082
iteration : 13953
train acc:  0.796875
train loss:  0.37895193696022034
train gradient:  0.2831094852898674
iteration : 13954
train acc:  0.8828125
train loss:  0.24472390115261078
train gradient:  0.10225449800371297
iteration : 13955
train acc:  0.90625
train loss:  0.25198328495025635
train gradient:  0.07283976578251254
iteration : 13956
train acc:  0.8828125
train loss:  0.3046805262565613
train gradient:  0.12609157950588412
iteration : 13957
train acc:  0.8828125
train loss:  0.2993046045303345
train gradient:  0.1439892095934943
iteration : 13958
train acc:  0.8125
train loss:  0.3752821683883667
train gradient:  0.15920286195466546
iteration : 13959
train acc:  0.8515625
train loss:  0.32046741247177124
train gradient:  0.14842109072364726
iteration : 13960
train acc:  0.8828125
train loss:  0.26880118250846863
train gradient:  0.18415158523861253
iteration : 13961
train acc:  0.8828125
train loss:  0.2616274058818817
train gradient:  0.13878515148474524
iteration : 13962
train acc:  0.8671875
train loss:  0.2886093556880951
train gradient:  0.1273964305038914
iteration : 13963
train acc:  0.8984375
train loss:  0.2824482023715973
train gradient:  0.11605560296844883
iteration : 13964
train acc:  0.7890625
train loss:  0.4438772201538086
train gradient:  0.235279773655394
iteration : 13965
train acc:  0.8671875
train loss:  0.3033020496368408
train gradient:  0.11462330487480041
iteration : 13966
train acc:  0.84375
train loss:  0.31997576355934143
train gradient:  0.20448185683044737
iteration : 13967
train acc:  0.859375
train loss:  0.3213713765144348
train gradient:  0.15290207774799375
iteration : 13968
train acc:  0.8671875
train loss:  0.3213719129562378
train gradient:  0.1349620258255091
iteration : 13969
train acc:  0.8671875
train loss:  0.32598432898521423
train gradient:  0.11540143603637547
iteration : 13970
train acc:  0.875
train loss:  0.3173876702785492
train gradient:  0.1332812952859354
iteration : 13971
train acc:  0.8671875
train loss:  0.35167163610458374
train gradient:  0.17978890765437905
iteration : 13972
train acc:  0.859375
train loss:  0.3254968523979187
train gradient:  0.13950764957398584
iteration : 13973
train acc:  0.8671875
train loss:  0.32039502263069153
train gradient:  0.1614135716827192
iteration : 13974
train acc:  0.8515625
train loss:  0.34234538674354553
train gradient:  0.14922028909863086
iteration : 13975
train acc:  0.8515625
train loss:  0.3414020538330078
train gradient:  0.11048781275306907
iteration : 13976
train acc:  0.828125
train loss:  0.41317418217658997
train gradient:  0.20475855922034664
iteration : 13977
train acc:  0.8203125
train loss:  0.3447701930999756
train gradient:  0.15195587805376198
iteration : 13978
train acc:  0.84375
train loss:  0.29947715997695923
train gradient:  0.0920317912780721
iteration : 13979
train acc:  0.921875
train loss:  0.22710251808166504
train gradient:  0.08393653012629546
iteration : 13980
train acc:  0.90625
train loss:  0.29341182112693787
train gradient:  0.160830369140922
iteration : 13981
train acc:  0.8515625
train loss:  0.33450591564178467
train gradient:  0.15806058365071535
iteration : 13982
train acc:  0.8203125
train loss:  0.32280975580215454
train gradient:  0.1198627228810883
iteration : 13983
train acc:  0.875
train loss:  0.257222980260849
train gradient:  0.09323121956610637
iteration : 13984
train acc:  0.8359375
train loss:  0.34194591641426086
train gradient:  0.1412089661148012
iteration : 13985
train acc:  0.890625
train loss:  0.2756193280220032
train gradient:  0.10931199652334059
iteration : 13986
train acc:  0.8515625
train loss:  0.3098387122154236
train gradient:  0.20741592045449025
iteration : 13987
train acc:  0.84375
train loss:  0.33340057730674744
train gradient:  0.13386046950903077
iteration : 13988
train acc:  0.890625
train loss:  0.3117011487483978
train gradient:  0.140194858406623
iteration : 13989
train acc:  0.859375
train loss:  0.3652770519256592
train gradient:  0.17115892332926763
iteration : 13990
train acc:  0.90625
train loss:  0.24104297161102295
train gradient:  0.07312685922379797
iteration : 13991
train acc:  0.8828125
train loss:  0.24778957664966583
train gradient:  0.07604573714429728
iteration : 13992
train acc:  0.859375
train loss:  0.3363257646560669
train gradient:  0.11325453118478217
iteration : 13993
train acc:  0.8671875
train loss:  0.3113466799259186
train gradient:  0.1319058766011753
iteration : 13994
train acc:  0.90625
train loss:  0.29103517532348633
train gradient:  0.08724759272644733
iteration : 13995
train acc:  0.84375
train loss:  0.3640856444835663
train gradient:  0.14452460863402689
iteration : 13996
train acc:  0.859375
train loss:  0.3112873136997223
train gradient:  0.12030939281130303
iteration : 13997
train acc:  0.8515625
train loss:  0.2748010456562042
train gradient:  0.09195849270271204
iteration : 13998
train acc:  0.90625
train loss:  0.33353516459465027
train gradient:  0.1460756070772663
iteration : 13999
train acc:  0.8515625
train loss:  0.3266077935695648
train gradient:  0.1252366534014917
iteration : 14000
train acc:  0.9140625
train loss:  0.2505243420600891
train gradient:  0.0743898390748205
iteration : 14001
train acc:  0.8671875
train loss:  0.3074975609779358
train gradient:  0.08668157586711256
iteration : 14002
train acc:  0.90625
train loss:  0.21893244981765747
train gradient:  0.1078936913327485
iteration : 14003
train acc:  0.8515625
train loss:  0.3309507369995117
train gradient:  0.15827614178570004
iteration : 14004
train acc:  0.8359375
train loss:  0.3775108754634857
train gradient:  0.20130641348976805
iteration : 14005
train acc:  0.828125
train loss:  0.35649925470352173
train gradient:  0.19010189774798789
iteration : 14006
train acc:  0.859375
train loss:  0.34009724855422974
train gradient:  0.23526012564462623
iteration : 14007
train acc:  0.90625
train loss:  0.2558833360671997
train gradient:  0.08234713744826262
iteration : 14008
train acc:  0.890625
train loss:  0.2954520285129547
train gradient:  0.11976708433540398
iteration : 14009
train acc:  0.875
train loss:  0.2791494131088257
train gradient:  0.10111144591727866
iteration : 14010
train acc:  0.8984375
train loss:  0.23564457893371582
train gradient:  0.12109439192149189
iteration : 14011
train acc:  0.90625
train loss:  0.22841212153434753
train gradient:  0.07726885040617106
iteration : 14012
train acc:  0.859375
train loss:  0.3187040686607361
train gradient:  0.14329978477688907
iteration : 14013
train acc:  0.875
train loss:  0.24015918374061584
train gradient:  0.08810300938624276
iteration : 14014
train acc:  0.8828125
train loss:  0.28593435883522034
train gradient:  0.11727880371123776
iteration : 14015
train acc:  0.828125
train loss:  0.46975788474082947
train gradient:  0.2943974683099683
iteration : 14016
train acc:  0.8046875
train loss:  0.48097118735313416
train gradient:  0.2699364012430265
iteration : 14017
train acc:  0.859375
train loss:  0.35304325819015503
train gradient:  0.16753241966352644
iteration : 14018
train acc:  0.875
train loss:  0.3103511929512024
train gradient:  0.1486460952669568
iteration : 14019
train acc:  0.859375
train loss:  0.29200252890586853
train gradient:  0.1631213302788542
iteration : 14020
train acc:  0.875
train loss:  0.3492268919944763
train gradient:  0.15077558299309646
iteration : 14021
train acc:  0.890625
train loss:  0.2643669545650482
train gradient:  0.11185973051136182
iteration : 14022
train acc:  0.8671875
train loss:  0.2703762650489807
train gradient:  0.12376502758698062
iteration : 14023
train acc:  0.8203125
train loss:  0.42385101318359375
train gradient:  0.19633143194993039
iteration : 14024
train acc:  0.8984375
train loss:  0.24794651567935944
train gradient:  0.08069167787176935
iteration : 14025
train acc:  0.8984375
train loss:  0.29828137159347534
train gradient:  0.11732542245933204
iteration : 14026
train acc:  0.8515625
train loss:  0.28881677985191345
train gradient:  0.09909644627206207
iteration : 14027
train acc:  0.9296875
train loss:  0.23850947618484497
train gradient:  0.08945708295567732
iteration : 14028
train acc:  0.8359375
train loss:  0.30990707874298096
train gradient:  0.13157336948383208
iteration : 14029
train acc:  0.8828125
train loss:  0.28098881244659424
train gradient:  0.09038825978068192
iteration : 14030
train acc:  0.8515625
train loss:  0.3832864761352539
train gradient:  0.14888802244497362
iteration : 14031
train acc:  0.84375
train loss:  0.39637571573257446
train gradient:  0.17513026348409674
iteration : 14032
train acc:  0.9140625
train loss:  0.27412354946136475
train gradient:  0.09558598243144713
iteration : 14033
train acc:  0.8203125
train loss:  0.37565264105796814
train gradient:  0.1589079202630982
iteration : 14034
train acc:  0.890625
train loss:  0.2914450466632843
train gradient:  0.139019312390527
iteration : 14035
train acc:  0.8359375
train loss:  0.37285637855529785
train gradient:  0.16727413718443518
iteration : 14036
train acc:  0.8515625
train loss:  0.2966649532318115
train gradient:  0.0916814628218945
iteration : 14037
train acc:  0.8984375
train loss:  0.2645667493343353
train gradient:  0.09066493126975451
iteration : 14038
train acc:  0.8359375
train loss:  0.32223695516586304
train gradient:  0.15584136847502328
iteration : 14039
train acc:  0.8515625
train loss:  0.3196653425693512
train gradient:  0.1353875979422214
iteration : 14040
train acc:  0.8671875
train loss:  0.2991331219673157
train gradient:  0.08624183700888671
iteration : 14041
train acc:  0.8671875
train loss:  0.28638869524002075
train gradient:  0.09096801946197543
iteration : 14042
train acc:  0.8515625
train loss:  0.3220144510269165
train gradient:  0.17749101574540493
iteration : 14043
train acc:  0.8359375
train loss:  0.35548368096351624
train gradient:  0.11610573948315983
iteration : 14044
train acc:  0.8828125
train loss:  0.28451207280158997
train gradient:  0.0952185479591314
iteration : 14045
train acc:  0.890625
train loss:  0.26496851444244385
train gradient:  0.11827428382205388
iteration : 14046
train acc:  0.8203125
train loss:  0.3750801384449005
train gradient:  0.30059810277959365
iteration : 14047
train acc:  0.84375
train loss:  0.4165869951248169
train gradient:  0.360703973991134
iteration : 14048
train acc:  0.875
train loss:  0.35982218384742737
train gradient:  0.1387268841433672
iteration : 14049
train acc:  0.8984375
train loss:  0.25366508960723877
train gradient:  0.11104142649169677
iteration : 14050
train acc:  0.8125
train loss:  0.39000216126441956
train gradient:  0.1841096611082716
iteration : 14051
train acc:  0.8046875
train loss:  0.36142057180404663
train gradient:  0.2048297604427896
iteration : 14052
train acc:  0.84375
train loss:  0.31434258818626404
train gradient:  0.17341875186949574
iteration : 14053
train acc:  0.8359375
train loss:  0.4259301424026489
train gradient:  0.2917665478879008
iteration : 14054
train acc:  0.8828125
train loss:  0.2696366608142853
train gradient:  0.09198139781018147
iteration : 14055
train acc:  0.890625
train loss:  0.22325178980827332
train gradient:  0.10976312265525563
iteration : 14056
train acc:  0.8359375
train loss:  0.373342901468277
train gradient:  0.18997761570041843
iteration : 14057
train acc:  0.84375
train loss:  0.40040719509124756
train gradient:  0.2000252847422338
iteration : 14058
train acc:  0.8984375
train loss:  0.28673893213272095
train gradient:  0.09582022539565524
iteration : 14059
train acc:  0.8828125
train loss:  0.26817724108695984
train gradient:  0.1087999423065798
iteration : 14060
train acc:  0.84375
train loss:  0.2989928126335144
train gradient:  0.1232437373283759
iteration : 14061
train acc:  0.8515625
train loss:  0.3237963616847992
train gradient:  0.1421731301655283
iteration : 14062
train acc:  0.890625
train loss:  0.30711495876312256
train gradient:  0.11528471180966814
iteration : 14063
train acc:  0.828125
train loss:  0.3821500837802887
train gradient:  0.2190340799375003
iteration : 14064
train acc:  0.8828125
train loss:  0.2902602553367615
train gradient:  0.08721353990360982
iteration : 14065
train acc:  0.8515625
train loss:  0.3699449896812439
train gradient:  0.1579809329481064
iteration : 14066
train acc:  0.859375
train loss:  0.32931217551231384
train gradient:  0.11547315003355861
iteration : 14067
train acc:  0.8671875
train loss:  0.28965115547180176
train gradient:  0.10434025716737756
iteration : 14068
train acc:  0.875
train loss:  0.3073093295097351
train gradient:  0.10472482593117934
iteration : 14069
train acc:  0.8125
train loss:  0.34833985567092896
train gradient:  0.14492213621758637
iteration : 14070
train acc:  0.8515625
train loss:  0.3285830616950989
train gradient:  0.13708166984213593
iteration : 14071
train acc:  0.921875
train loss:  0.25083109736442566
train gradient:  0.08966193417788411
iteration : 14072
train acc:  0.859375
train loss:  0.3164936900138855
train gradient:  0.27958437882627063
iteration : 14073
train acc:  0.890625
train loss:  0.26774048805236816
train gradient:  0.10937515998576137
iteration : 14074
train acc:  0.828125
train loss:  0.4021420180797577
train gradient:  0.21535051859187387
iteration : 14075
train acc:  0.90625
train loss:  0.2508516311645508
train gradient:  0.0674048675330609
iteration : 14076
train acc:  0.9140625
train loss:  0.223402738571167
train gradient:  0.06850692053356444
iteration : 14077
train acc:  0.859375
train loss:  0.35934656858444214
train gradient:  0.17651319453581765
iteration : 14078
train acc:  0.828125
train loss:  0.32687729597091675
train gradient:  0.15511933560418473
iteration : 14079
train acc:  0.8671875
train loss:  0.3434141278266907
train gradient:  0.14963752581520107
iteration : 14080
train acc:  0.8984375
train loss:  0.2582669258117676
train gradient:  0.10049960908947196
iteration : 14081
train acc:  0.8515625
train loss:  0.3685224652290344
train gradient:  0.19990285403746266
iteration : 14082
train acc:  0.8984375
train loss:  0.2810630798339844
train gradient:  0.12352923266386924
iteration : 14083
train acc:  0.8515625
train loss:  0.38419169187545776
train gradient:  0.17715130319807404
iteration : 14084
train acc:  0.875
train loss:  0.28320634365081787
train gradient:  0.08453421494372684
iteration : 14085
train acc:  0.90625
train loss:  0.22836878895759583
train gradient:  0.0962487935976253
iteration : 14086
train acc:  0.8671875
train loss:  0.32036957144737244
train gradient:  0.1855251299030508
iteration : 14087
train acc:  0.8984375
train loss:  0.24754151701927185
train gradient:  0.09330867998142085
iteration : 14088
train acc:  0.8671875
train loss:  0.27562785148620605
train gradient:  0.08829222576949884
iteration : 14089
train acc:  0.8671875
train loss:  0.3106679320335388
train gradient:  0.11018172599591473
iteration : 14090
train acc:  0.8828125
train loss:  0.3153560161590576
train gradient:  0.1404496960297123
iteration : 14091
train acc:  0.796875
train loss:  0.4690512418746948
train gradient:  0.24225343539459132
iteration : 14092
train acc:  0.890625
train loss:  0.3128400444984436
train gradient:  0.1359928016184329
iteration : 14093
train acc:  0.859375
train loss:  0.30509862303733826
train gradient:  0.14818639925526775
iteration : 14094
train acc:  0.875
train loss:  0.25578606128692627
train gradient:  0.13488575113827972
iteration : 14095
train acc:  0.8515625
train loss:  0.3358858823776245
train gradient:  0.1479467681890427
iteration : 14096
train acc:  0.8828125
train loss:  0.2626636028289795
train gradient:  0.08916679261360819
iteration : 14097
train acc:  0.859375
train loss:  0.28992730379104614
train gradient:  0.12043722125308007
iteration : 14098
train acc:  0.8515625
train loss:  0.3348681926727295
train gradient:  0.14692546369682447
iteration : 14099
train acc:  0.8203125
train loss:  0.44809845089912415
train gradient:  0.3234726113650213
iteration : 14100
train acc:  0.84375
train loss:  0.3221784234046936
train gradient:  0.12393394660350229
iteration : 14101
train acc:  0.875
train loss:  0.2900604009628296
train gradient:  0.11676511226761141
iteration : 14102
train acc:  0.890625
train loss:  0.281229704618454
train gradient:  0.1743654203991461
iteration : 14103
train acc:  0.78125
train loss:  0.49174654483795166
train gradient:  0.27870775085948524
iteration : 14104
train acc:  0.84375
train loss:  0.39103490114212036
train gradient:  0.15008627788934964
iteration : 14105
train acc:  0.8828125
train loss:  0.35740184783935547
train gradient:  0.14419080149825125
iteration : 14106
train acc:  0.8828125
train loss:  0.2933523952960968
train gradient:  0.09827707454742951
iteration : 14107
train acc:  0.875
train loss:  0.2999407649040222
train gradient:  0.12928628344435494
iteration : 14108
train acc:  0.8828125
train loss:  0.2701040804386139
train gradient:  0.10512400142904646
iteration : 14109
train acc:  0.859375
train loss:  0.33369266986846924
train gradient:  0.14617703068377091
iteration : 14110
train acc:  0.8515625
train loss:  0.3047512173652649
train gradient:  0.1309538120855164
iteration : 14111
train acc:  0.859375
train loss:  0.30693280696868896
train gradient:  0.11382986930852416
iteration : 14112
train acc:  0.90625
train loss:  0.28391605615615845
train gradient:  0.08052237015657057
iteration : 14113
train acc:  0.90625
train loss:  0.22532227635383606
train gradient:  0.0829685582304107
iteration : 14114
train acc:  0.8671875
train loss:  0.2692156434059143
train gradient:  0.08134944688273683
iteration : 14115
train acc:  0.890625
train loss:  0.3425333499908447
train gradient:  0.14461559425740914
iteration : 14116
train acc:  0.8359375
train loss:  0.3582918345928192
train gradient:  0.1440200805850353
iteration : 14117
train acc:  0.859375
train loss:  0.3163227140903473
train gradient:  0.17607347116926342
iteration : 14118
train acc:  0.859375
train loss:  0.2866622805595398
train gradient:  0.12796637267123462
iteration : 14119
train acc:  0.84375
train loss:  0.3472170829772949
train gradient:  0.14737010501873385
iteration : 14120
train acc:  0.8515625
train loss:  0.3037554621696472
train gradient:  0.1381634101594248
iteration : 14121
train acc:  0.8671875
train loss:  0.29726681113243103
train gradient:  0.17142443970088306
iteration : 14122
train acc:  0.8359375
train loss:  0.43505004048347473
train gradient:  0.23577961572356293
iteration : 14123
train acc:  0.875
train loss:  0.29574400186538696
train gradient:  0.13422057254850414
iteration : 14124
train acc:  0.8984375
train loss:  0.2813190817832947
train gradient:  0.08858422274832559
iteration : 14125
train acc:  0.8515625
train loss:  0.31575506925582886
train gradient:  0.15851421415707775
iteration : 14126
train acc:  0.8828125
train loss:  0.2618510127067566
train gradient:  0.10889858487064367
iteration : 14127
train acc:  0.8828125
train loss:  0.2657034993171692
train gradient:  0.08523880498408096
iteration : 14128
train acc:  0.8828125
train loss:  0.327237606048584
train gradient:  0.12245146685665834
iteration : 14129
train acc:  0.8359375
train loss:  0.318085640668869
train gradient:  0.21561690608769446
iteration : 14130
train acc:  0.78125
train loss:  0.38886988162994385
train gradient:  0.178235231956726
iteration : 14131
train acc:  0.859375
train loss:  0.33830228447914124
train gradient:  0.16283881865807748
iteration : 14132
train acc:  0.8984375
train loss:  0.24949698150157928
train gradient:  0.08958016977632462
iteration : 14133
train acc:  0.8828125
train loss:  0.32410135865211487
train gradient:  0.14452506933876486
iteration : 14134
train acc:  0.8359375
train loss:  0.3345496654510498
train gradient:  0.1504222366372131
iteration : 14135
train acc:  0.859375
train loss:  0.2797594666481018
train gradient:  0.12756914279822057
iteration : 14136
train acc:  0.8515625
train loss:  0.2829112410545349
train gradient:  0.10088506923334378
iteration : 14137
train acc:  0.875
train loss:  0.30728545784950256
train gradient:  0.1388819048016241
iteration : 14138
train acc:  0.84375
train loss:  0.2927308678627014
train gradient:  0.11163875717808494
iteration : 14139
train acc:  0.828125
train loss:  0.346908837556839
train gradient:  0.1354030970498426
iteration : 14140
train acc:  0.8671875
train loss:  0.30843132734298706
train gradient:  0.12748942960733273
iteration : 14141
train acc:  0.8515625
train loss:  0.2854992747306824
train gradient:  0.09638596968355115
iteration : 14142
train acc:  0.8671875
train loss:  0.3928036093711853
train gradient:  0.18594238737639185
iteration : 14143
train acc:  0.875
train loss:  0.31774193048477173
train gradient:  0.22669843532162676
iteration : 14144
train acc:  0.8515625
train loss:  0.33387017250061035
train gradient:  0.12749004315713403
iteration : 14145
train acc:  0.90625
train loss:  0.3482395112514496
train gradient:  0.18806359257473865
iteration : 14146
train acc:  0.9140625
train loss:  0.2992381751537323
train gradient:  0.2066581637262183
iteration : 14147
train acc:  0.890625
train loss:  0.2570457458496094
train gradient:  0.14827451861607233
iteration : 14148
train acc:  0.8359375
train loss:  0.34497272968292236
train gradient:  0.1621510315891161
iteration : 14149
train acc:  0.84375
train loss:  0.3434261083602905
train gradient:  0.1672033636088958
iteration : 14150
train acc:  0.8671875
train loss:  0.3452838659286499
train gradient:  0.32126221544749656
iteration : 14151
train acc:  0.8828125
train loss:  0.2946714162826538
train gradient:  0.2028170379787939
iteration : 14152
train acc:  0.84375
train loss:  0.32266783714294434
train gradient:  0.0931540669223585
iteration : 14153
train acc:  0.8203125
train loss:  0.34883061051368713
train gradient:  0.15348382791666682
iteration : 14154
train acc:  0.8671875
train loss:  0.36743199825286865
train gradient:  0.18129480009608934
iteration : 14155
train acc:  0.875
train loss:  0.3547005355358124
train gradient:  0.15562660062264322
iteration : 14156
train acc:  0.84375
train loss:  0.326203852891922
train gradient:  0.1542850604330332
iteration : 14157
train acc:  0.8359375
train loss:  0.34084397554397583
train gradient:  0.1430499035678593
iteration : 14158
train acc:  0.8671875
train loss:  0.29466137290000916
train gradient:  0.13592984345781947
iteration : 14159
train acc:  0.828125
train loss:  0.3803083300590515
train gradient:  0.15478885721333338
iteration : 14160
train acc:  0.875
train loss:  0.2990797162055969
train gradient:  0.14942488340777818
iteration : 14161
train acc:  0.828125
train loss:  0.3999538719654083
train gradient:  0.2086136182763953
iteration : 14162
train acc:  0.8671875
train loss:  0.2937301993370056
train gradient:  0.124237221173793
iteration : 14163
train acc:  0.953125
train loss:  0.2011490911245346
train gradient:  0.09424367901513006
iteration : 14164
train acc:  0.859375
train loss:  0.33800578117370605
train gradient:  0.1331476156876945
iteration : 14165
train acc:  0.8359375
train loss:  0.3756018877029419
train gradient:  0.21918116365429913
iteration : 14166
train acc:  0.859375
train loss:  0.25431108474731445
train gradient:  0.08357858249465268
iteration : 14167
train acc:  0.8671875
train loss:  0.3238558769226074
train gradient:  0.13426874420516327
iteration : 14168
train acc:  0.859375
train loss:  0.380669504404068
train gradient:  0.17749717381773858
iteration : 14169
train acc:  0.890625
train loss:  0.3047337532043457
train gradient:  0.11577429521858996
iteration : 14170
train acc:  0.890625
train loss:  0.3054482340812683
train gradient:  0.15697179581352985
iteration : 14171
train acc:  0.875
train loss:  0.3218638002872467
train gradient:  0.148072882579181
iteration : 14172
train acc:  0.875
train loss:  0.28414565324783325
train gradient:  0.11411560479838495
iteration : 14173
train acc:  0.8828125
train loss:  0.2970016300678253
train gradient:  0.17339319024434607
iteration : 14174
train acc:  0.84375
train loss:  0.39168521761894226
train gradient:  0.23959779664127842
iteration : 14175
train acc:  0.890625
train loss:  0.2763293981552124
train gradient:  0.09656330980792893
iteration : 14176
train acc:  0.8984375
train loss:  0.2688910961151123
train gradient:  0.10966025813185563
iteration : 14177
train acc:  0.84375
train loss:  0.3066484332084656
train gradient:  0.07932018654819877
iteration : 14178
train acc:  0.875
train loss:  0.3405267000198364
train gradient:  0.14409691193309304
iteration : 14179
train acc:  0.8359375
train loss:  0.32028135657310486
train gradient:  0.11284213128719771
iteration : 14180
train acc:  0.8984375
train loss:  0.24471306800842285
train gradient:  0.11539981932967804
iteration : 14181
train acc:  0.9375
train loss:  0.23625361919403076
train gradient:  0.09714682612619062
iteration : 14182
train acc:  0.890625
train loss:  0.29829660058021545
train gradient:  0.14583825825553282
iteration : 14183
train acc:  0.890625
train loss:  0.2958066463470459
train gradient:  0.11065640162255407
iteration : 14184
train acc:  0.8671875
train loss:  0.31327471137046814
train gradient:  0.13006707006537277
iteration : 14185
train acc:  0.765625
train loss:  0.41859737038612366
train gradient:  0.2845268437225887
iteration : 14186
train acc:  0.7222222222222222
train loss:  0.48436206579208374
train gradient:  1.2640261301424356
val acc:  0.8669911773653788
val f1:  0.8648949320148331
val confusion matrix:  [[87024 11586]
 [14646 83964]]

----------------------------------------new_epoch--------------------------------------

epoch:  1
iteration : 0
train acc:  0.859375
train loss:  0.3310078978538513
train gradient:  0.1286245573380197
iteration : 1
train acc:  0.8828125
train loss:  0.2803311049938202
train gradient:  0.136590287090872
iteration : 2
train acc:  0.875
train loss:  0.28900161385536194
train gradient:  0.09151264465477729
iteration : 3
train acc:  0.8359375
train loss:  0.36315441131591797
train gradient:  0.14219478141892852
iteration : 4
train acc:  0.890625
train loss:  0.27829283475875854
train gradient:  0.1306703698493716
iteration : 5
train acc:  0.8515625
train loss:  0.3490592837333679
train gradient:  0.31027159025361867
iteration : 6
train acc:  0.828125
train loss:  0.3666006028652191
train gradient:  0.2059251798165841
iteration : 7
train acc:  0.8984375
train loss:  0.30896222591400146
train gradient:  0.11487559075443442
iteration : 8
train acc:  0.7890625
train loss:  0.4381636381149292
train gradient:  0.20237718644641955
iteration : 9
train acc:  0.8515625
train loss:  0.2758444547653198
train gradient:  0.10054962130046724
iteration : 10
train acc:  0.8671875
train loss:  0.34467363357543945
train gradient:  0.16180958880203952
iteration : 11
train acc:  0.859375
train loss:  0.30203914642333984
train gradient:  0.11488508222671551
iteration : 12
train acc:  0.8671875
train loss:  0.33299610018730164
train gradient:  0.15106304227240505
iteration : 13
train acc:  0.8828125
train loss:  0.285578191280365
train gradient:  0.11808844862962928
iteration : 14
train acc:  0.8359375
train loss:  0.3304409384727478
train gradient:  0.273932977705865
iteration : 15
train acc:  0.8359375
train loss:  0.3378983438014984
train gradient:  0.12881107868085945
iteration : 16
train acc:  0.8203125
train loss:  0.46552950143814087
train gradient:  0.21625135009561489
iteration : 17
train acc:  0.8671875
train loss:  0.340140163898468
train gradient:  0.10821881797685151
iteration : 18
train acc:  0.8671875
train loss:  0.3280595541000366
train gradient:  0.1990301362796767
iteration : 19
train acc:  0.84375
train loss:  0.34748488664627075
train gradient:  0.1301235887836618
iteration : 20
train acc:  0.8984375
train loss:  0.2512322664260864
train gradient:  0.1026491730190035
iteration : 21
train acc:  0.8828125
train loss:  0.2982761859893799
train gradient:  0.12747331363058007
iteration : 22
train acc:  0.890625
train loss:  0.2816644310951233
train gradient:  0.11942246877361078
iteration : 23
train acc:  0.8359375
train loss:  0.3405326008796692
train gradient:  0.12733020761979774
iteration : 24
train acc:  0.875
train loss:  0.27942290902137756
train gradient:  0.13538836687363165
iteration : 25
train acc:  0.8984375
train loss:  0.2715185880661011
train gradient:  0.11645439719527212
iteration : 26
train acc:  0.84375
train loss:  0.33801162242889404
train gradient:  0.19445279799092088
iteration : 27
train acc:  0.8828125
train loss:  0.295055091381073
train gradient:  0.0993606595503953
iteration : 28
train acc:  0.859375
train loss:  0.3346884846687317
train gradient:  0.1178030628251904
iteration : 29
train acc:  0.8984375
train loss:  0.2414705753326416
train gradient:  0.07441951915707659
iteration : 30
train acc:  0.859375
train loss:  0.3207847476005554
train gradient:  0.11029740373166853
iteration : 31
train acc:  0.8671875
train loss:  0.3777298331260681
train gradient:  0.1703859687789656
iteration : 32
train acc:  0.859375
train loss:  0.39189696311950684
train gradient:  0.14043903815719688
iteration : 33
train acc:  0.859375
train loss:  0.2816949784755707
train gradient:  0.12249178544589091
iteration : 34
train acc:  0.84375
train loss:  0.4121449291706085
train gradient:  0.2005583352300888
iteration : 35
train acc:  0.890625
train loss:  0.3343450427055359
train gradient:  0.22971890416546065
iteration : 36
train acc:  0.859375
train loss:  0.3311068117618561
train gradient:  0.14727999906481842
iteration : 37
train acc:  0.859375
train loss:  0.3035711646080017
train gradient:  0.12577552442904502
iteration : 38
train acc:  0.8203125
train loss:  0.3896316587924957
train gradient:  0.1564182057713095
iteration : 39
train acc:  0.828125
train loss:  0.31778639554977417
train gradient:  0.13056218376831896
iteration : 40
train acc:  0.8984375
train loss:  0.2799925208091736
train gradient:  0.09775074852543779
iteration : 41
train acc:  0.8515625
train loss:  0.3425738215446472
train gradient:  0.1275270721928121
iteration : 42
train acc:  0.8359375
train loss:  0.3009644150733948
train gradient:  0.09821772729375557
iteration : 43
train acc:  0.875
train loss:  0.2915192246437073
train gradient:  0.10583955373610711
iteration : 44
train acc:  0.8359375
train loss:  0.29746386408805847
train gradient:  0.11909870873300593
iteration : 45
train acc:  0.8515625
train loss:  0.36681243777275085
train gradient:  0.13438032373971237
iteration : 46
train acc:  0.8515625
train loss:  0.3434217572212219
train gradient:  0.15219239607085316
iteration : 47
train acc:  0.8828125
train loss:  0.30966639518737793
train gradient:  0.10346113783101271
iteration : 48
train acc:  0.8671875
train loss:  0.32793980836868286
train gradient:  0.1447307387479721
iteration : 49
train acc:  0.890625
train loss:  0.2649605870246887
train gradient:  0.09107610717454993
iteration : 50
train acc:  0.8671875
train loss:  0.30126672983169556
train gradient:  0.10156546476640062
iteration : 51
train acc:  0.90625
train loss:  0.2372443825006485
train gradient:  0.07505343276312275
iteration : 52
train acc:  0.828125
train loss:  0.40977221727371216
train gradient:  0.15818830709561993
iteration : 53
train acc:  0.8359375
train loss:  0.342865526676178
train gradient:  0.11891317191714525
iteration : 54
train acc:  0.8984375
train loss:  0.2888939380645752
train gradient:  0.09006991488804124
iteration : 55
train acc:  0.8671875
train loss:  0.3147432804107666
train gradient:  0.12984315862764265
iteration : 56
train acc:  0.890625
train loss:  0.31119343638420105
train gradient:  0.10045258824946834
iteration : 57
train acc:  0.8515625
train loss:  0.32930120825767517
train gradient:  0.17072982127882227
iteration : 58
train acc:  0.84375
train loss:  0.38749372959136963
train gradient:  0.12744654043350223
iteration : 59
train acc:  0.859375
train loss:  0.3014729619026184
train gradient:  0.10916106388761541
iteration : 60
train acc:  0.84375
train loss:  0.40422552824020386
train gradient:  0.20788101045919366
iteration : 61
train acc:  0.8515625
train loss:  0.36051541566848755
train gradient:  0.16156013909065942
iteration : 62
train acc:  0.8515625
train loss:  0.35843977332115173
train gradient:  0.14354197390711507
iteration : 63
train acc:  0.8828125
train loss:  0.2616056203842163
train gradient:  0.09074840260563152
iteration : 64
train acc:  0.8671875
train loss:  0.28523486852645874
train gradient:  0.13070667999422902
iteration : 65
train acc:  0.890625
train loss:  0.24899545311927795
train gradient:  0.07302343970261731
iteration : 66
train acc:  0.8515625
train loss:  0.3659858703613281
train gradient:  0.24075444622352327
iteration : 67
train acc:  0.78125
train loss:  0.401554673910141
train gradient:  0.23806944565185134
iteration : 68
train acc:  0.90625
train loss:  0.25558793544769287
train gradient:  0.12240204272639192
iteration : 69
train acc:  0.8359375
train loss:  0.38229164481163025
train gradient:  0.15715097356994903
iteration : 70
train acc:  0.8359375
train loss:  0.38733378052711487
train gradient:  0.20748706235334885
iteration : 71
train acc:  0.8515625
train loss:  0.32956981658935547
train gradient:  0.12568474129492205
iteration : 72
train acc:  0.890625
train loss:  0.275043785572052
train gradient:  0.11471118691156165
iteration : 73
train acc:  0.875
train loss:  0.322822630405426
train gradient:  0.09685375959260287
iteration : 74
train acc:  0.8359375
train loss:  0.32071611285209656
train gradient:  0.10372583788138744
iteration : 75
train acc:  0.8828125
train loss:  0.2934165596961975
train gradient:  0.11789089188278377
iteration : 76
train acc:  0.8828125
train loss:  0.280853271484375
train gradient:  0.09796495677418855
iteration : 77
train acc:  0.875
train loss:  0.3323093056678772
train gradient:  0.1289608041878859
iteration : 78
train acc:  0.8515625
train loss:  0.32949888706207275
train gradient:  0.10843219422161691
iteration : 79
train acc:  0.875
train loss:  0.24346783757209778
train gradient:  0.0829679035872154
iteration : 80
train acc:  0.8515625
train loss:  0.29215189814567566
train gradient:  0.09840783569466713
iteration : 81
train acc:  0.8359375
train loss:  0.3940785825252533
train gradient:  0.1548621204206429
iteration : 82
train acc:  0.8671875
train loss:  0.2992285490036011
train gradient:  0.11643189647493854
iteration : 83
train acc:  0.8125
train loss:  0.4044584333896637
train gradient:  0.19986416019568765
iteration : 84
train acc:  0.859375
train loss:  0.3351372480392456
train gradient:  0.12809849781368376
iteration : 85
train acc:  0.875
train loss:  0.27350467443466187
train gradient:  0.13272135537122376
iteration : 86
train acc:  0.7890625
train loss:  0.38562673330307007
train gradient:  0.18829541325349283
iteration : 87
train acc:  0.875
train loss:  0.2989220917224884
train gradient:  0.10487608533748222
iteration : 88
train acc:  0.8515625
train loss:  0.3652620017528534
train gradient:  0.21710702424299802
iteration : 89
train acc:  0.9140625
train loss:  0.25980859994888306
train gradient:  0.19363591208611214
iteration : 90
train acc:  0.890625
train loss:  0.26714789867401123
train gradient:  0.07229448663745054
iteration : 91
train acc:  0.8828125
train loss:  0.29103362560272217
train gradient:  0.11890535463095425
iteration : 92
train acc:  0.8984375
train loss:  0.29932674765586853
train gradient:  0.1383855713300524
iteration : 93
train acc:  0.9140625
train loss:  0.2486071139574051
train gradient:  0.09824754544627609
iteration : 94
train acc:  0.8984375
train loss:  0.2847595512866974
train gradient:  0.094481620184278
iteration : 95
train acc:  0.875
train loss:  0.2787996828556061
train gradient:  0.1158848573214858
iteration : 96
train acc:  0.8828125
train loss:  0.2570403814315796
train gradient:  0.07719049949642069
iteration : 97
train acc:  0.8828125
train loss:  0.28785252571105957
train gradient:  0.1504515802309188
iteration : 98
train acc:  0.828125
train loss:  0.3687670826911926
train gradient:  0.19811800038488891
iteration : 99
train acc:  0.8671875
train loss:  0.3048257827758789
train gradient:  0.1618101391274346
iteration : 100
train acc:  0.8984375
train loss:  0.28001999855041504
train gradient:  0.0647701839454019
iteration : 101
train acc:  0.8359375
train loss:  0.3222188353538513
train gradient:  0.1249096602037232
iteration : 102
train acc:  0.8515625
train loss:  0.32504719495773315
train gradient:  0.16098197618850532
iteration : 103
train acc:  0.859375
train loss:  0.30534839630126953
train gradient:  0.10484399372035916
iteration : 104
train acc:  0.8515625
train loss:  0.316633939743042
train gradient:  0.13859262268267886
iteration : 105
train acc:  0.84375
train loss:  0.32295742630958557
train gradient:  0.12914828390981803
iteration : 106
train acc:  0.8984375
train loss:  0.27911269664764404
train gradient:  0.0956460239367112
iteration : 107
train acc:  0.859375
train loss:  0.29951298236846924
train gradient:  0.1149953152608338
iteration : 108
train acc:  0.859375
train loss:  0.3158712685108185
train gradient:  0.09542907350684426
iteration : 109
train acc:  0.9140625
train loss:  0.2588561475276947
train gradient:  0.08507426050786543
iteration : 110
train acc:  0.8046875
train loss:  0.35807958245277405
train gradient:  0.14429483585650404
iteration : 111
train acc:  0.8984375
train loss:  0.26103532314300537
train gradient:  0.11118871759457769
iteration : 112
train acc:  0.8828125
train loss:  0.3079290986061096
train gradient:  0.12171044555673821
iteration : 113
train acc:  0.8984375
train loss:  0.24626001715660095
train gradient:  0.08959965553316933
iteration : 114
train acc:  0.859375
train loss:  0.2863572835922241
train gradient:  0.09075647741765246
iteration : 115
train acc:  0.8671875
train loss:  0.23421737551689148
train gradient:  0.07684394629246048
iteration : 116
train acc:  0.8984375
train loss:  0.2491026073694229
train gradient:  0.07299731492709538
iteration : 117
train acc:  0.8359375
train loss:  0.27492862939834595
train gradient:  0.12388411818203711
iteration : 118
train acc:  0.921875
train loss:  0.22379058599472046
train gradient:  0.07014673098514658
iteration : 119
train acc:  0.8671875
train loss:  0.31630900502204895
train gradient:  0.1419202355982258
iteration : 120
train acc:  0.84375
train loss:  0.3153298795223236
train gradient:  0.1446761398121603
iteration : 121
train acc:  0.8828125
train loss:  0.2653721570968628
train gradient:  0.10980208143332193
iteration : 122
train acc:  0.8125
train loss:  0.4059673547744751
train gradient:  0.14595824845909308
iteration : 123
train acc:  0.9140625
train loss:  0.2505031228065491
train gradient:  0.12053618289144241
iteration : 124
train acc:  0.8515625
train loss:  0.32971763610839844
train gradient:  0.11539954612887131
iteration : 125
train acc:  0.8515625
train loss:  0.3191385865211487
train gradient:  0.12634707471126733
iteration : 126
train acc:  0.890625
train loss:  0.31713545322418213
train gradient:  0.14145485867092086
iteration : 127
train acc:  0.90625
train loss:  0.240607351064682
train gradient:  0.07524962577800595
iteration : 128
train acc:  0.84375
train loss:  0.33041808009147644
train gradient:  0.12732817583342723
iteration : 129
train acc:  0.8984375
train loss:  0.25031912326812744
train gradient:  0.0856628922935331
iteration : 130
train acc:  0.8359375
train loss:  0.31829339265823364
train gradient:  0.09857307170467082
iteration : 131
train acc:  0.8359375
train loss:  0.3748151659965515
train gradient:  0.1572772526567218
iteration : 132
train acc:  0.8515625
train loss:  0.31582894921302795
train gradient:  0.1081648955997251
iteration : 133
train acc:  0.8671875
train loss:  0.32064926624298096
train gradient:  0.12211978773971428
iteration : 134
train acc:  0.8828125
train loss:  0.25430482625961304
train gradient:  0.11561810948733345
iteration : 135
train acc:  0.78125
train loss:  0.4242076873779297
train gradient:  0.21144534260335535
iteration : 136
train acc:  0.921875
train loss:  0.27695390582084656
train gradient:  0.15674760032486418
iteration : 137
train acc:  0.8359375
train loss:  0.38523292541503906
train gradient:  0.15412222526503772
iteration : 138
train acc:  0.8359375
train loss:  0.3678028881549835
train gradient:  0.1649990695530414
iteration : 139
train acc:  0.875
train loss:  0.24930568039417267
train gradient:  0.13755562947025057
iteration : 140
train acc:  0.8203125
train loss:  0.3728976845741272
train gradient:  0.19149971711939767
iteration : 141
train acc:  0.8359375
train loss:  0.353532999753952
train gradient:  0.12344294606557238
iteration : 142
train acc:  0.8828125
train loss:  0.2697400748729706
train gradient:  0.12233674636720927
iteration : 143
train acc:  0.8515625
train loss:  0.3156229853630066
train gradient:  0.11526323466656725
iteration : 144
train acc:  0.8671875
train loss:  0.2878261208534241
train gradient:  0.10339815188903966
iteration : 145
train acc:  0.828125
train loss:  0.34588003158569336
train gradient:  0.13482218107029587
iteration : 146
train acc:  0.84375
train loss:  0.34584999084472656
train gradient:  0.13854991345694156
iteration : 147
train acc:  0.84375
train loss:  0.3864661753177643
train gradient:  0.26650568234101935
iteration : 148
train acc:  0.8359375
train loss:  0.3395250141620636
train gradient:  0.11697445054597458
iteration : 149
train acc:  0.90625
train loss:  0.24534758925437927
train gradient:  0.0746135205512074
iteration : 150
train acc:  0.8671875
train loss:  0.3185737133026123
train gradient:  0.11671704974009633
iteration : 151
train acc:  0.890625
train loss:  0.2640247642993927
train gradient:  0.14605652583158843
iteration : 152
train acc:  0.8515625
train loss:  0.3293393552303314
train gradient:  0.17613037246126176
iteration : 153
train acc:  0.890625
train loss:  0.28121858835220337
train gradient:  0.1051377463583782
iteration : 154
train acc:  0.8671875
train loss:  0.32105332612991333
train gradient:  0.14007990063338457
iteration : 155
train acc:  0.8515625
train loss:  0.37094101309776306
train gradient:  0.13338790161682493
iteration : 156
train acc:  0.8359375
train loss:  0.36641883850097656
train gradient:  0.1629938403569251
iteration : 157
train acc:  0.890625
train loss:  0.2866881191730499
train gradient:  0.14892474821994475
iteration : 158
train acc:  0.890625
train loss:  0.24501165747642517
train gradient:  0.10483624556797352
iteration : 159
train acc:  0.8984375
train loss:  0.24719706177711487
train gradient:  0.09176115815106409
iteration : 160
train acc:  0.8828125
train loss:  0.33482661843299866
train gradient:  0.1910808680385449
iteration : 161
train acc:  0.8828125
train loss:  0.29890161752700806
train gradient:  0.1258998834852057
iteration : 162
train acc:  0.8671875
train loss:  0.3497743010520935
train gradient:  0.15794264912805317
iteration : 163
train acc:  0.890625
train loss:  0.29876211285591125
train gradient:  0.11325275618274809
iteration : 164
train acc:  0.859375
train loss:  0.33464381098747253
train gradient:  0.17809479125473954
iteration : 165
train acc:  0.84375
train loss:  0.34376323223114014
train gradient:  0.17460905155639606
iteration : 166
train acc:  0.9140625
train loss:  0.25124555826187134
train gradient:  0.1531753095638746
iteration : 167
train acc:  0.859375
train loss:  0.33551257848739624
train gradient:  0.14216031598711149
iteration : 168
train acc:  0.8828125
train loss:  0.31012991070747375
train gradient:  0.16208962698539156
iteration : 169
train acc:  0.890625
train loss:  0.2888867259025574
train gradient:  0.08206173990850918
iteration : 170
train acc:  0.8671875
train loss:  0.30578887462615967
train gradient:  0.1663881898565555
iteration : 171
train acc:  0.90625
train loss:  0.2992357611656189
train gradient:  0.14830951223507122
iteration : 172
train acc:  0.875
train loss:  0.4126513600349426
train gradient:  0.1644417645323928
iteration : 173
train acc:  0.8671875
train loss:  0.3052622973918915
train gradient:  0.1757180300134095
iteration : 174
train acc:  0.859375
train loss:  0.3274686634540558
train gradient:  0.12293265160769075
iteration : 175
train acc:  0.859375
train loss:  0.37090831995010376
train gradient:  0.1529445822873564
iteration : 176
train acc:  0.8828125
train loss:  0.27258580923080444
train gradient:  0.10733815952824975
iteration : 177
train acc:  0.84375
train loss:  0.3475806713104248
train gradient:  0.1629988646101957
iteration : 178
train acc:  0.890625
train loss:  0.3126813769340515
train gradient:  0.1570937375756632
iteration : 179
train acc:  0.828125
train loss:  0.326094388961792
train gradient:  0.12981778425532833
iteration : 180
train acc:  0.890625
train loss:  0.33484721183776855
train gradient:  0.11934371687918216
iteration : 181
train acc:  0.8984375
train loss:  0.2912555932998657
train gradient:  0.10513423702156814
iteration : 182
train acc:  0.9140625
train loss:  0.2866016626358032
train gradient:  0.11137266189381785
iteration : 183
train acc:  0.8671875
train loss:  0.27729734778404236
train gradient:  0.1550217475304146
iteration : 184
train acc:  0.828125
train loss:  0.39647769927978516
train gradient:  0.17558588655943813
iteration : 185
train acc:  0.8203125
train loss:  0.38986900448799133
train gradient:  0.22724707320979087
iteration : 186
train acc:  0.8984375
train loss:  0.22853721678256989
train gradient:  0.1052353264056222
iteration : 187
train acc:  0.8359375
train loss:  0.3675680160522461
train gradient:  0.16674238778161637
iteration : 188
train acc:  0.828125
train loss:  0.36859169602394104
train gradient:  0.15581714395494956
iteration : 189
train acc:  0.828125
train loss:  0.3936348557472229
train gradient:  0.21879073658621367
iteration : 190
train acc:  0.875
train loss:  0.3785247206687927
train gradient:  0.33021940437569874
iteration : 191
train acc:  0.90625
train loss:  0.2740180790424347
train gradient:  0.0832595197843333
iteration : 192
train acc:  0.8828125
train loss:  0.27589550614356995
train gradient:  0.11850445440377923
iteration : 193
train acc:  0.8828125
train loss:  0.2632880210876465
train gradient:  0.08162156494400112
iteration : 194
train acc:  0.828125
train loss:  0.3728969693183899
train gradient:  0.21286522224351306
iteration : 195
train acc:  0.875
train loss:  0.315524160861969
train gradient:  0.1356994047348457
iteration : 196
train acc:  0.8046875
train loss:  0.47152402997016907
train gradient:  0.2657840091104207
iteration : 197
train acc:  0.90625
train loss:  0.24509680271148682
train gradient:  0.10833810345224969
iteration : 198
train acc:  0.8828125
train loss:  0.27858108282089233
train gradient:  0.16931246846435913
iteration : 199
train acc:  0.8671875
train loss:  0.27019792795181274
train gradient:  0.1176502828091371
iteration : 200
train acc:  0.859375
train loss:  0.2900553345680237
train gradient:  0.11292877094492229
iteration : 201
train acc:  0.90625
train loss:  0.2450810968875885
train gradient:  0.09602143351810401
iteration : 202
train acc:  0.859375
train loss:  0.3161999583244324
train gradient:  0.1617893458406056
iteration : 203
train acc:  0.875
train loss:  0.3891681432723999
train gradient:  0.15003339319604064
iteration : 204
train acc:  0.890625
train loss:  0.2579396367073059
train gradient:  0.15036416238504438
iteration : 205
train acc:  0.8359375
train loss:  0.31684568524360657
train gradient:  0.1440187671423391
iteration : 206
train acc:  0.84375
train loss:  0.33949315547943115
train gradient:  0.1287161045740141
iteration : 207
train acc:  0.7578125
train loss:  0.4043804407119751
train gradient:  0.1739241677541532
iteration : 208
train acc:  0.9140625
train loss:  0.22950425744056702
train gradient:  0.07507278703570466
iteration : 209
train acc:  0.8671875
train loss:  0.29209741950035095
train gradient:  0.13325852294049895
iteration : 210
train acc:  0.8671875
train loss:  0.2756735682487488
train gradient:  0.22893152320768434
iteration : 211
train acc:  0.875
train loss:  0.2955436706542969
train gradient:  0.08702809681666604
iteration : 212
train acc:  0.859375
train loss:  0.3269069790840149
train gradient:  0.1232363225807247
iteration : 213
train acc:  0.8515625
train loss:  0.30361151695251465
train gradient:  0.1332121463563305
iteration : 214
train acc:  0.8828125
train loss:  0.3427119255065918
train gradient:  0.09580943129390868
iteration : 215
train acc:  0.84375
train loss:  0.3174457252025604
train gradient:  0.10051502514051745
iteration : 216
train acc:  0.828125
train loss:  0.35415413975715637
train gradient:  0.1601525911193662
iteration : 217
train acc:  0.890625
train loss:  0.263599693775177
train gradient:  0.09191913101280087
iteration : 218
train acc:  0.84375
train loss:  0.3248690068721771
train gradient:  0.1578543208905603
iteration : 219
train acc:  0.8671875
train loss:  0.2831566333770752
train gradient:  0.1180010228630477
iteration : 220
train acc:  0.875
train loss:  0.28368768095970154
train gradient:  0.13294515412831875
iteration : 221
train acc:  0.90625
train loss:  0.26918867230415344
train gradient:  0.10473449696778264
iteration : 222
train acc:  0.84375
train loss:  0.32778698205947876
train gradient:  0.1031564622736378
iteration : 223
train acc:  0.796875
train loss:  0.39185625314712524
train gradient:  0.21649988753404037
iteration : 224
train acc:  0.828125
train loss:  0.32779937982559204
train gradient:  0.1630820144033222
iteration : 225
train acc:  0.8515625
train loss:  0.3207980990409851
train gradient:  0.13497609449950668
iteration : 226
train acc:  0.8359375
train loss:  0.3144312798976898
train gradient:  0.1304072758512595
iteration : 227
train acc:  0.84375
train loss:  0.3341544270515442
train gradient:  0.11383676618831931
iteration : 228
train acc:  0.828125
train loss:  0.389767050743103
train gradient:  0.22478712403271392
iteration : 229
train acc:  0.8359375
train loss:  0.3296380043029785
train gradient:  0.168314946114618
iteration : 230
train acc:  0.859375
train loss:  0.30825507640838623
train gradient:  0.12187167863566455
iteration : 231
train acc:  0.8671875
train loss:  0.2883915305137634
train gradient:  0.12117573641320105
iteration : 232
train acc:  0.8671875
train loss:  0.2757306396961212
train gradient:  0.10927058077602174
iteration : 233
train acc:  0.8515625
train loss:  0.330349326133728
train gradient:  0.11341686595175
iteration : 234
train acc:  0.875
train loss:  0.3071349263191223
train gradient:  0.12994802781273732
iteration : 235
train acc:  0.875
train loss:  0.254669725894928
train gradient:  0.11089469802124233
iteration : 236
train acc:  0.921875
train loss:  0.20621660351753235
train gradient:  0.09100097766968558
iteration : 237
train acc:  0.875
train loss:  0.30219563841819763
train gradient:  0.09882579690242124
iteration : 238
train acc:  0.828125
train loss:  0.3128296136856079
train gradient:  0.09761907251970932
iteration : 239
train acc:  0.8515625
train loss:  0.3319131135940552
train gradient:  0.1605239878401748
iteration : 240
train acc:  0.828125
train loss:  0.33517295122146606
train gradient:  0.1404141147887336
iteration : 241
train acc:  0.84375
train loss:  0.38847094774246216
train gradient:  0.16920540871486173
iteration : 242
train acc:  0.8828125
train loss:  0.2782781720161438
train gradient:  0.09829424624091927
iteration : 243
train acc:  0.8671875
train loss:  0.27591872215270996
train gradient:  0.08542141360120765
iteration : 244
train acc:  0.875
train loss:  0.30806106328964233
train gradient:  0.15704934403385218
iteration : 245
train acc:  0.875
train loss:  0.2731132507324219
train gradient:  0.13428344555936486
iteration : 246
train acc:  0.8671875
train loss:  0.2691228687763214
train gradient:  0.08516750199837149
iteration : 247
train acc:  0.921875
train loss:  0.2963656187057495
train gradient:  0.0953493499396248
iteration : 248
train acc:  0.875
train loss:  0.3322824239730835
train gradient:  0.1630332258329809
iteration : 249
train acc:  0.8359375
train loss:  0.31096577644348145
train gradient:  0.16736763677655064
iteration : 250
train acc:  0.875
train loss:  0.288826584815979
train gradient:  0.09599953727824527
iteration : 251
train acc:  0.828125
train loss:  0.33881044387817383
train gradient:  0.11932856488395542
iteration : 252
train acc:  0.890625
train loss:  0.25178349018096924
train gradient:  0.12068977703525377
iteration : 253
train acc:  0.90625
train loss:  0.25870493054389954
train gradient:  0.09580868004237027
iteration : 254
train acc:  0.8828125
train loss:  0.2926698625087738
train gradient:  0.11207233695049942
iteration : 255
train acc:  0.8984375
train loss:  0.24225759506225586
train gradient:  0.11136372558997891
iteration : 256
train acc:  0.859375
train loss:  0.32709264755249023
train gradient:  0.107691479644861
iteration : 257
train acc:  0.875
train loss:  0.28510162234306335
train gradient:  0.1045835378962487
iteration : 258
train acc:  0.8359375
train loss:  0.327556848526001
train gradient:  0.0899410525286495
iteration : 259
train acc:  0.875
train loss:  0.3249545693397522
train gradient:  0.1255337853571
iteration : 260
train acc:  0.875
train loss:  0.32872873544692993
train gradient:  0.1059265782922581
iteration : 261
train acc:  0.8046875
train loss:  0.38802075386047363
train gradient:  0.22105498669790183
iteration : 262
train acc:  0.8828125
train loss:  0.27460768818855286
train gradient:  0.14324088760558656
iteration : 263
train acc:  0.84375
train loss:  0.28128358721733093
train gradient:  0.10145879760189912
iteration : 264
train acc:  0.8125
train loss:  0.38849517703056335
train gradient:  0.174723409117777
iteration : 265
train acc:  0.84375
train loss:  0.3349218964576721
train gradient:  0.13642677192382274
iteration : 266
train acc:  0.890625
train loss:  0.291001558303833
train gradient:  0.0733109330330826
iteration : 267
train acc:  0.8828125
train loss:  0.25914767384529114
train gradient:  0.11906413813076677
iteration : 268
train acc:  0.84375
train loss:  0.3275705575942993
train gradient:  0.1768053482890803
iteration : 269
train acc:  0.890625
train loss:  0.3191082179546356
train gradient:  0.13024056625755448
iteration : 270
train acc:  0.828125
train loss:  0.33008629083633423
train gradient:  0.1541564505106237
iteration : 271
train acc:  0.8203125
train loss:  0.4989112317562103
train gradient:  0.22017182729905876
iteration : 272
train acc:  0.859375
train loss:  0.3212030529975891
train gradient:  0.17972606476858627
iteration : 273
train acc:  0.7734375
train loss:  0.48420536518096924
train gradient:  0.2772352639914236
iteration : 274
train acc:  0.8515625
train loss:  0.4314521849155426
train gradient:  0.1867043798784151
iteration : 275
train acc:  0.84375
train loss:  0.3117517828941345
train gradient:  0.18301944333020387
iteration : 276
train acc:  0.8984375
train loss:  0.33522289991378784
train gradient:  0.16274785088683827
iteration : 277
train acc:  0.90625
train loss:  0.24591779708862305
train gradient:  0.08564901243493125
iteration : 278
train acc:  0.890625
train loss:  0.22839269042015076
train gradient:  0.12639945860267315
iteration : 279
train acc:  0.8828125
train loss:  0.3365931808948517
train gradient:  0.1506509707664024
iteration : 280
train acc:  0.859375
train loss:  0.3067631125450134
train gradient:  0.15239166672314125
iteration : 281
train acc:  0.8671875
train loss:  0.34012314677238464
train gradient:  0.14611173165678806
iteration : 282
train acc:  0.859375
train loss:  0.349944531917572
train gradient:  0.16498937350645182
iteration : 283
train acc:  0.859375
train loss:  0.3354220390319824
train gradient:  0.1484646576723455
iteration : 284
train acc:  0.875
train loss:  0.2745007872581482
train gradient:  0.12150812260312607
iteration : 285
train acc:  0.8359375
train loss:  0.3394681215286255
train gradient:  0.13127010396885702
iteration : 286
train acc:  0.859375
train loss:  0.2983303666114807
train gradient:  0.09722194218735503
iteration : 287
train acc:  0.953125
train loss:  0.2093583047389984
train gradient:  0.07814804924407318
iteration : 288
train acc:  0.859375
train loss:  0.30114486813545227
train gradient:  0.11548095861438165
iteration : 289
train acc:  0.8828125
train loss:  0.32220661640167236
train gradient:  0.16978928370319554
iteration : 290
train acc:  0.8984375
train loss:  0.2928847372531891
train gradient:  0.1404892492761511
iteration : 291
train acc:  0.8125
train loss:  0.3335687220096588
train gradient:  0.1215736059604491
iteration : 292
train acc:  0.8984375
train loss:  0.26991596817970276
train gradient:  0.11013684312487926
iteration : 293
train acc:  0.875
train loss:  0.28202730417251587
train gradient:  0.0956662929645392
iteration : 294
train acc:  0.8515625
train loss:  0.3274419903755188
train gradient:  0.14755028193809364
iteration : 295
train acc:  0.859375
train loss:  0.3253691792488098
train gradient:  0.16282145780538806
iteration : 296
train acc:  0.828125
train loss:  0.44689619541168213
train gradient:  0.2007313172050711
iteration : 297
train acc:  0.890625
train loss:  0.2753902077674866
train gradient:  0.12958671025249668
iteration : 298
train acc:  0.8984375
train loss:  0.25906968116760254
train gradient:  0.08640557357016271
iteration : 299
train acc:  0.8125
train loss:  0.34408265352249146
train gradient:  0.11507614310584414
iteration : 300
train acc:  0.875
train loss:  0.3028811812400818
train gradient:  0.09587828567181503
iteration : 301
train acc:  0.8125
train loss:  0.33756545186042786
train gradient:  0.14795789738502965
iteration : 302
train acc:  0.8359375
train loss:  0.3434421718120575
train gradient:  0.16849840927263693
iteration : 303
train acc:  0.890625
train loss:  0.3291248679161072
train gradient:  0.13949692156909257
iteration : 304
train acc:  0.859375
train loss:  0.3460996747016907
train gradient:  0.20088270372239123
iteration : 305
train acc:  0.890625
train loss:  0.27568763494491577
train gradient:  0.1735554191223832
iteration : 306
train acc:  0.9296875
train loss:  0.2539658546447754
train gradient:  0.07536020891717872
iteration : 307
train acc:  0.8359375
train loss:  0.42842984199523926
train gradient:  0.17285231315875962
iteration : 308
train acc:  0.875
train loss:  0.27900686860084534
train gradient:  0.12000192724098194
iteration : 309
train acc:  0.8046875
train loss:  0.39959365129470825
train gradient:  0.2818584058893388
iteration : 310
train acc:  0.84375
train loss:  0.3569742739200592
train gradient:  0.15759093260345253
iteration : 311
train acc:  0.8984375
train loss:  0.3232095539569855
train gradient:  0.12088593777262754
iteration : 312
train acc:  0.8671875
train loss:  0.31645339727401733
train gradient:  0.08845994245985271
iteration : 313
train acc:  0.859375
train loss:  0.3054988384246826
train gradient:  0.15892065469600763
iteration : 314
train acc:  0.921875
train loss:  0.24426451325416565
train gradient:  0.09329135872068399
iteration : 315
train acc:  0.84375
train loss:  0.34927234053611755
train gradient:  0.13742695486611173
iteration : 316
train acc:  0.875
train loss:  0.2792835831642151
train gradient:  0.09071384956270909
iteration : 317
train acc:  0.8203125
train loss:  0.3576502501964569
train gradient:  0.21477900818863846
iteration : 318
train acc:  0.8125
train loss:  0.34922000765800476
train gradient:  0.2827869685310061
iteration : 319
train acc:  0.8203125
train loss:  0.3376457095146179
train gradient:  0.12786760646028117
iteration : 320
train acc:  0.8828125
train loss:  0.26502060890197754
train gradient:  0.11883640234312676
iteration : 321
train acc:  0.890625
train loss:  0.2563265860080719
train gradient:  0.09707932008893799
iteration : 322
train acc:  0.8984375
train loss:  0.2699750065803528
train gradient:  0.07852027452091155
iteration : 323
train acc:  0.8671875
train loss:  0.2937169373035431
train gradient:  0.13848583228633762
iteration : 324
train acc:  0.859375
train loss:  0.3357406258583069
train gradient:  0.10320558300927427
iteration : 325
train acc:  0.8828125
train loss:  0.2841375470161438
train gradient:  0.19998628118646286
iteration : 326
train acc:  0.8515625
train loss:  0.33179041743278503
train gradient:  0.1801739228835063
iteration : 327
train acc:  0.875
train loss:  0.2976440191268921
train gradient:  0.12393657454814026
iteration : 328
train acc:  0.8671875
train loss:  0.32503706216812134
train gradient:  0.09430589576105179
iteration : 329
train acc:  0.859375
train loss:  0.26941144466400146
train gradient:  0.09060355843517169
iteration : 330
train acc:  0.859375
train loss:  0.3515086770057678
train gradient:  0.16889808548473328
iteration : 331
train acc:  0.8828125
train loss:  0.27095258235931396
train gradient:  0.1220439247324538
iteration : 332
train acc:  0.828125
train loss:  0.3862614333629608
train gradient:  0.14990683977271702
iteration : 333
train acc:  0.7890625
train loss:  0.41407912969589233
train gradient:  0.14929154150008142
iteration : 334
train acc:  0.875
train loss:  0.2528553605079651
train gradient:  0.09593798006174233
iteration : 335
train acc:  0.8671875
train loss:  0.3336122930049896
train gradient:  0.13243525713596754
iteration : 336
train acc:  0.890625
train loss:  0.29936614632606506
train gradient:  0.10682446076032022
iteration : 337
train acc:  0.828125
train loss:  0.3944394588470459
train gradient:  0.17079252121890243
iteration : 338
train acc:  0.828125
train loss:  0.44372278451919556
train gradient:  0.23243104605866816
iteration : 339
train acc:  0.796875
train loss:  0.400129109621048
train gradient:  0.42534586649238076
iteration : 340
train acc:  0.859375
train loss:  0.43924060463905334
train gradient:  0.2389987247959977
iteration : 341
train acc:  0.859375
train loss:  0.3742337226867676
train gradient:  0.19108645340284175
iteration : 342
train acc:  0.890625
train loss:  0.27042412757873535
train gradient:  0.07662231761723098
iteration : 343
train acc:  0.890625
train loss:  0.27809444069862366
train gradient:  0.08327610703666963
iteration : 344
train acc:  0.796875
train loss:  0.41804221272468567
train gradient:  0.1585014427732951
iteration : 345
train acc:  0.8828125
train loss:  0.26033076643943787
train gradient:  0.10909743733549372
iteration : 346
train acc:  0.8515625
train loss:  0.3187314569950104
train gradient:  0.1176891301397133
iteration : 347
train acc:  0.90625
train loss:  0.23998376727104187
train gradient:  0.15543846645227727
iteration : 348
train acc:  0.8984375
train loss:  0.29219111800193787
train gradient:  0.08677718207390599
iteration : 349
train acc:  0.8671875
train loss:  0.32701170444488525
train gradient:  0.15506548929029768
iteration : 350
train acc:  0.859375
train loss:  0.335370808839798
train gradient:  0.1121057234884897
iteration : 351
train acc:  0.9140625
train loss:  0.3036002814769745
train gradient:  0.11222753312015062
iteration : 352
train acc:  0.8515625
train loss:  0.35779428482055664
train gradient:  0.15412413309263404
iteration : 353
train acc:  0.84375
train loss:  0.336505264043808
train gradient:  0.10358848134889931
iteration : 354
train acc:  0.8828125
train loss:  0.3173367977142334
train gradient:  0.1439142480125678
iteration : 355
train acc:  0.8671875
train loss:  0.32954663038253784
train gradient:  0.14387316207017578
iteration : 356
train acc:  0.8515625
train loss:  0.34235239028930664
train gradient:  0.12440762093393003
iteration : 357
train acc:  0.875
train loss:  0.31047189235687256
train gradient:  0.14943178480021543
iteration : 358
train acc:  0.84375
train loss:  0.3110693395137787
train gradient:  0.11322608597816844
iteration : 359
train acc:  0.8359375
train loss:  0.32147693634033203
train gradient:  0.10390966430213872
iteration : 360
train acc:  0.875
train loss:  0.2643367648124695
train gradient:  0.1156088077530283
iteration : 361
train acc:  0.828125
train loss:  0.3619728684425354
train gradient:  0.15542041634185
iteration : 362
train acc:  0.8359375
train loss:  0.3693754971027374
train gradient:  0.271163352265348
iteration : 363
train acc:  0.828125
train loss:  0.39743801951408386
train gradient:  0.23701837952645616
iteration : 364
train acc:  0.7890625
train loss:  0.4270779490470886
train gradient:  0.2266390768060893
iteration : 365
train acc:  0.9140625
train loss:  0.2573203444480896
train gradient:  0.09140355034380036
iteration : 366
train acc:  0.8984375
train loss:  0.26873522996902466
train gradient:  0.10440375401733701
iteration : 367
train acc:  0.9296875
train loss:  0.25119009613990784
train gradient:  0.10042308678475358
iteration : 368
train acc:  0.8671875
train loss:  0.2836793065071106
train gradient:  0.11421429984674493
iteration : 369
train acc:  0.84375
train loss:  0.3359031081199646
train gradient:  0.14407252706218324
iteration : 370
train acc:  0.8984375
train loss:  0.2593564987182617
train gradient:  0.08476799411718003
iteration : 371
train acc:  0.8828125
train loss:  0.25912073254585266
train gradient:  0.08597183433326124
iteration : 372
train acc:  0.84375
train loss:  0.34655797481536865
train gradient:  0.14929752683497555
iteration : 373
train acc:  0.9140625
train loss:  0.2600596249103546
train gradient:  0.09417030669753076
iteration : 374
train acc:  0.890625
train loss:  0.29488620162010193
train gradient:  0.10012306611274753
iteration : 375
train acc:  0.8046875
train loss:  0.3636077046394348
train gradient:  0.15753828008962634
iteration : 376
train acc:  0.875
train loss:  0.2673951983451843
train gradient:  0.15567353740835999
iteration : 377
train acc:  0.8671875
train loss:  0.29168909788131714
train gradient:  0.1509667669969284
iteration : 378
train acc:  0.875
train loss:  0.32002902030944824
train gradient:  0.11178673762552725
iteration : 379
train acc:  0.8359375
train loss:  0.3697640299797058
train gradient:  0.18976787874826842
iteration : 380
train acc:  0.90625
train loss:  0.29756295680999756
train gradient:  0.0938878229947846
iteration : 381
train acc:  0.875
train loss:  0.36607539653778076
train gradient:  0.1535571382542235
iteration : 382
train acc:  0.8515625
train loss:  0.32887688279151917
train gradient:  0.12060740166475899
iteration : 383
train acc:  0.8671875
train loss:  0.2765389382839203
train gradient:  0.14522107413494184
iteration : 384
train acc:  0.859375
train loss:  0.3150460124015808
train gradient:  0.12424523681623113
iteration : 385
train acc:  0.8671875
train loss:  0.33324265480041504
train gradient:  0.19028719121330434
iteration : 386
train acc:  0.8359375
train loss:  0.30193349719047546
train gradient:  0.1173970400702222
iteration : 387
train acc:  0.890625
train loss:  0.2415468394756317
train gradient:  0.07181275482135435
iteration : 388
train acc:  0.8671875
train loss:  0.2611342966556549
train gradient:  0.1331931474812021
iteration : 389
train acc:  0.8359375
train loss:  0.3605504333972931
train gradient:  0.13483611323216935
iteration : 390
train acc:  0.8671875
train loss:  0.32756441831588745
train gradient:  0.14768505534881943
iteration : 391
train acc:  0.8359375
train loss:  0.3210905194282532
train gradient:  0.44929585199904226
iteration : 392
train acc:  0.828125
train loss:  0.34499692916870117
train gradient:  0.158444138411659
iteration : 393
train acc:  0.8828125
train loss:  0.3365101218223572
train gradient:  0.12618870748326405
iteration : 394
train acc:  0.84375
train loss:  0.31815746426582336
train gradient:  0.11682810056641464
iteration : 395
train acc:  0.84375
train loss:  0.3098182678222656
train gradient:  0.09386942263300212
iteration : 396
train acc:  0.8671875
train loss:  0.3424777686595917
train gradient:  0.19151928899485793
iteration : 397
train acc:  0.8515625
train loss:  0.40081310272216797
train gradient:  0.19708210692949613
iteration : 398
train acc:  0.8984375
train loss:  0.2942503094673157
train gradient:  0.09366276264285125
iteration : 399
train acc:  0.8984375
train loss:  0.2530362010002136
train gradient:  0.12342347265236454
iteration : 400
train acc:  0.8515625
train loss:  0.3321880102157593
train gradient:  0.13414216639482612
iteration : 401
train acc:  0.8359375
train loss:  0.3726946711540222
train gradient:  0.16106779285731182
iteration : 402
train acc:  0.8984375
train loss:  0.28458547592163086
train gradient:  0.11516259432125965
iteration : 403
train acc:  0.890625
train loss:  0.29150673747062683
train gradient:  0.09891487068728373
iteration : 404
train acc:  0.890625
train loss:  0.2936912178993225
train gradient:  0.11210540063592894
iteration : 405
train acc:  0.890625
train loss:  0.284000426530838
train gradient:  0.12048833008010955
iteration : 406
train acc:  0.8125
train loss:  0.38602733612060547
train gradient:  0.1572738083304967
iteration : 407
train acc:  0.8671875
train loss:  0.29776808619499207
train gradient:  0.10714692484816576
iteration : 408
train acc:  0.796875
train loss:  0.3913494050502777
train gradient:  0.14432596061037822
iteration : 409
train acc:  0.84375
train loss:  0.31338006258010864
train gradient:  0.17885149995878286
iteration : 410
train acc:  0.8359375
train loss:  0.3625886142253876
train gradient:  0.15468650767860603
iteration : 411
train acc:  0.859375
train loss:  0.34274056553840637
train gradient:  0.15914743151767555
iteration : 412
train acc:  0.921875
train loss:  0.21493232250213623
train gradient:  0.0804078038252372
iteration : 413
train acc:  0.859375
train loss:  0.3331860303878784
train gradient:  0.10690574387673923
iteration : 414
train acc:  0.859375
train loss:  0.2818149924278259
train gradient:  0.10027876192459803
iteration : 415
train acc:  0.8515625
train loss:  0.31355637311935425
train gradient:  0.12868125012966958
iteration : 416
train acc:  0.8828125
train loss:  0.32018595933914185
train gradient:  0.13993457968065984
iteration : 417
train acc:  0.890625
train loss:  0.29729005694389343
train gradient:  0.10625851532665523
iteration : 418
train acc:  0.890625
train loss:  0.28530192375183105
train gradient:  0.11989586604032804
iteration : 419
train acc:  0.8359375
train loss:  0.31227052211761475
train gradient:  0.14592577325351716
iteration : 420
train acc:  0.8125
train loss:  0.41261011362075806
train gradient:  0.22137499341684191
iteration : 421
train acc:  0.8671875
train loss:  0.34377503395080566
train gradient:  0.1180228349487709
iteration : 422
train acc:  0.90625
train loss:  0.20458582043647766
train gradient:  0.09979609233300386
iteration : 423
train acc:  0.875
train loss:  0.2782154977321625
train gradient:  0.15798011788336225
iteration : 424
train acc:  0.890625
train loss:  0.2741853594779968
train gradient:  0.09645537675034434
iteration : 425
train acc:  0.8515625
train loss:  0.354677677154541
train gradient:  0.15285455447374616
iteration : 426
train acc:  0.875
train loss:  0.3176884055137634
train gradient:  0.16391951345488548
iteration : 427
train acc:  0.84375
train loss:  0.3823752999305725
train gradient:  0.1743556513183615
iteration : 428
train acc:  0.8515625
train loss:  0.37623170018196106
train gradient:  0.1408767891026682
iteration : 429
train acc:  0.890625
train loss:  0.24548111855983734
train gradient:  0.07252491999031332
iteration : 430
train acc:  0.8125
train loss:  0.3602025508880615
train gradient:  0.22461847980843264
iteration : 431
train acc:  0.875
train loss:  0.2696816325187683
train gradient:  0.12456650775077739
iteration : 432
train acc:  0.890625
train loss:  0.2631092667579651
train gradient:  0.20814150606949344
iteration : 433
train acc:  0.8984375
train loss:  0.2770366668701172
train gradient:  0.06543546201752934
iteration : 434
train acc:  0.8671875
train loss:  0.30809980630874634
train gradient:  0.12465867387615336
iteration : 435
train acc:  0.859375
train loss:  0.34996503591537476
train gradient:  0.13339445308896436
iteration : 436
train acc:  0.828125
train loss:  0.38538745045661926
train gradient:  0.20019683923758458
iteration : 437
train acc:  0.8203125
train loss:  0.4022114872932434
train gradient:  0.19142615990579034
iteration : 438
train acc:  0.8671875
train loss:  0.30327069759368896
train gradient:  0.12703786440920217
iteration : 439
train acc:  0.8515625
train loss:  0.3366299271583557
train gradient:  0.14561155982671337
iteration : 440
train acc:  0.8828125
train loss:  0.31803029775619507
train gradient:  0.11107309478936295
iteration : 441
train acc:  0.84375
train loss:  0.299472451210022
train gradient:  0.1233262276400716
iteration : 442
train acc:  0.8359375
train loss:  0.3690148591995239
train gradient:  0.17444257409357022
iteration : 443
train acc:  0.8828125
train loss:  0.28429174423217773
train gradient:  0.30024995536127197
iteration : 444
train acc:  0.859375
train loss:  0.29803603887557983
train gradient:  0.09578909166059348
iteration : 445
train acc:  0.90625
train loss:  0.2747848629951477
train gradient:  0.10801277891029662
iteration : 446
train acc:  0.890625
train loss:  0.29002517461776733
train gradient:  0.10348961609826907
iteration : 447
train acc:  0.828125
train loss:  0.36235225200653076
train gradient:  0.14681215649163756
iteration : 448
train acc:  0.890625
train loss:  0.28194868564605713
train gradient:  0.10400816223013273
iteration : 449
train acc:  0.8984375
train loss:  0.2878347933292389
train gradient:  0.2059392887389997
iteration : 450
train acc:  0.8828125
train loss:  0.2659469246864319
train gradient:  0.11027063429609649
iteration : 451
train acc:  0.84375
train loss:  0.3079822063446045
train gradient:  0.14717007087134654
iteration : 452
train acc:  0.859375
train loss:  0.2959315776824951
train gradient:  0.148567864079661
iteration : 453
train acc:  0.8359375
train loss:  0.2932635545730591
train gradient:  0.11592418155243434
iteration : 454
train acc:  0.8828125
train loss:  0.2214014083147049
train gradient:  0.08545240013938281
iteration : 455
train acc:  0.890625
train loss:  0.2752988934516907
train gradient:  0.08610948701924916
iteration : 456
train acc:  0.8828125
train loss:  0.2928003966808319
train gradient:  0.16778431054666962
iteration : 457
train acc:  0.828125
train loss:  0.32509005069732666
train gradient:  0.10495962745965744
iteration : 458
train acc:  0.890625
train loss:  0.27613383531570435
train gradient:  0.10366319938805786
iteration : 459
train acc:  0.875
train loss:  0.33968180418014526
train gradient:  0.15772625842943533
iteration : 460
train acc:  0.890625
train loss:  0.2726346254348755
train gradient:  0.13582097287117617
iteration : 461
train acc:  0.90625
train loss:  0.24090000987052917
train gradient:  0.22041234177888647
iteration : 462
train acc:  0.8984375
train loss:  0.28913402557373047
train gradient:  0.11840030195756227
iteration : 463
train acc:  0.84375
train loss:  0.3347608745098114
train gradient:  0.13706645411256402
iteration : 464
train acc:  0.8125
train loss:  0.39020079374313354
train gradient:  0.1630528920866468
iteration : 465
train acc:  0.8828125
train loss:  0.2771190106868744
train gradient:  0.18522449677460406
iteration : 466
train acc:  0.8671875
train loss:  0.3490900993347168
train gradient:  0.15840490260746476
iteration : 467
train acc:  0.8046875
train loss:  0.37060290575027466
train gradient:  0.1996584314791786
iteration : 468
train acc:  0.859375
train loss:  0.33487918972969055
train gradient:  0.15522424019252595
iteration : 469
train acc:  0.875
train loss:  0.3207668960094452
train gradient:  0.14162352258405236
iteration : 470
train acc:  0.8046875
train loss:  0.32729077339172363
train gradient:  0.14822290265711077
iteration : 471
train acc:  0.859375
train loss:  0.33396023511886597
train gradient:  0.10317641782761641
iteration : 472
train acc:  0.890625
train loss:  0.276244580745697
train gradient:  0.10136745712650769
iteration : 473
train acc:  0.84375
train loss:  0.2982163727283478
train gradient:  0.12273405277890137
iteration : 474
train acc:  0.78125
train loss:  0.45211049914360046
train gradient:  0.2033296304844054
iteration : 475
train acc:  0.84375
train loss:  0.364590048789978
train gradient:  0.1807932488276499
iteration : 476
train acc:  0.828125
train loss:  0.3522897958755493
train gradient:  0.15340120747421196
iteration : 477
train acc:  0.8515625
train loss:  0.31078970432281494
train gradient:  0.09610930393670834
iteration : 478
train acc:  0.828125
train loss:  0.33120793104171753
train gradient:  0.14043338424564022
iteration : 479
train acc:  0.8828125
train loss:  0.2894009053707123
train gradient:  0.16157582627117228
iteration : 480
train acc:  0.8828125
train loss:  0.31786927580833435
train gradient:  0.15702608339497443
iteration : 481
train acc:  0.859375
train loss:  0.28959065675735474
train gradient:  0.0918414781769932
iteration : 482
train acc:  0.9140625
train loss:  0.2547663450241089
train gradient:  0.09750003940670714
iteration : 483
train acc:  0.828125
train loss:  0.3266984224319458
train gradient:  0.13446897381539952
iteration : 484
train acc:  0.8515625
train loss:  0.2869769334793091
train gradient:  0.10655299661990099
iteration : 485
train acc:  0.875
train loss:  0.30188536643981934
train gradient:  0.10987766551362886
iteration : 486
train acc:  0.8515625
train loss:  0.3397408723831177
train gradient:  0.12321335999595612
iteration : 487
train acc:  0.859375
train loss:  0.3231070339679718
train gradient:  0.1250702990717934
iteration : 488
train acc:  0.890625
train loss:  0.2888890504837036
train gradient:  0.1051185697418142
iteration : 489
train acc:  0.859375
train loss:  0.330220490694046
train gradient:  0.1251638499083284
iteration : 490
train acc:  0.8203125
train loss:  0.3523252606391907
train gradient:  0.2091366249503817
iteration : 491
train acc:  0.84375
train loss:  0.38202589750289917
train gradient:  0.1430804651849809
iteration : 492
train acc:  0.8359375
train loss:  0.3309299945831299
train gradient:  0.17825922153849932
iteration : 493
train acc:  0.8828125
train loss:  0.25906792283058167
train gradient:  0.06094491560100106
iteration : 494
train acc:  0.890625
train loss:  0.2836761772632599
train gradient:  0.10057495457530458
iteration : 495
train acc:  0.8984375
train loss:  0.3149030804634094
train gradient:  0.33827288986325776
iteration : 496
train acc:  0.8984375
train loss:  0.2431616485118866
train gradient:  0.07500588784064484
iteration : 497
train acc:  0.7734375
train loss:  0.49353858828544617
train gradient:  0.28458014527042
iteration : 498
train acc:  0.8828125
train loss:  0.30274486541748047
train gradient:  0.15684339679469256
iteration : 499
train acc:  0.875
train loss:  0.36544081568717957
train gradient:  0.17372630071159548
iteration : 500
train acc:  0.90625
train loss:  0.2642507553100586
train gradient:  0.0944796779821059
iteration : 501
train acc:  0.9140625
train loss:  0.24226878583431244
train gradient:  0.11936887682226648
iteration : 502
train acc:  0.8984375
train loss:  0.2748470604419708
train gradient:  0.10271615411433667
iteration : 503
train acc:  0.8359375
train loss:  0.29866823554039
train gradient:  0.12241719977539671
iteration : 504
train acc:  0.8125
train loss:  0.4123392701148987
train gradient:  0.17820434286998557
iteration : 505
train acc:  0.8515625
train loss:  0.31461912393569946
train gradient:  0.11550656071340829
iteration : 506
train acc:  0.859375
train loss:  0.29749929904937744
train gradient:  0.09442750162468341
iteration : 507
train acc:  0.875
train loss:  0.27040907740592957
train gradient:  0.11333821543200474
iteration : 508
train acc:  0.90625
train loss:  0.2539590299129486
train gradient:  0.09659990409099524
iteration : 509
train acc:  0.90625
train loss:  0.24912960827350616
train gradient:  0.12042389034043789
iteration : 510
train acc:  0.8359375
train loss:  0.4382380545139313
train gradient:  0.24936815011078972
iteration : 511
train acc:  0.859375
train loss:  0.3565661907196045
train gradient:  0.15393063355549086
iteration : 512
train acc:  0.875
train loss:  0.2705707550048828
train gradient:  0.10149905999570853
iteration : 513
train acc:  0.8828125
train loss:  0.2596592307090759
train gradient:  0.1199337839457671
iteration : 514
train acc:  0.84375
train loss:  0.3037155866622925
train gradient:  0.15086790290518232
iteration : 515
train acc:  0.890625
train loss:  0.27622371912002563
train gradient:  0.13906970530765428
iteration : 516
train acc:  0.8203125
train loss:  0.33887365460395813
train gradient:  0.19139395440056167
iteration : 517
train acc:  0.8515625
train loss:  0.3508679270744324
train gradient:  0.18625537017972996
iteration : 518
train acc:  0.8671875
train loss:  0.28303399682044983
train gradient:  0.125960153387011
iteration : 519
train acc:  0.8671875
train loss:  0.32103994488716125
train gradient:  0.14562022584850326
iteration : 520
train acc:  0.8828125
train loss:  0.328951895236969
train gradient:  0.14857206541902906
iteration : 521
train acc:  0.8984375
train loss:  0.24496592581272125
train gradient:  0.09945960040878898
iteration : 522
train acc:  0.8515625
train loss:  0.34727129340171814
train gradient:  0.1490897476836056
iteration : 523
train acc:  0.8671875
train loss:  0.28988438844680786
train gradient:  0.11687045875233575
iteration : 524
train acc:  0.84375
train loss:  0.3865818977355957
train gradient:  0.21542855833756572
iteration : 525
train acc:  0.890625
train loss:  0.27030330896377563
train gradient:  0.11317265094871949
iteration : 526
train acc:  0.8828125
train loss:  0.2682074308395386
train gradient:  0.13926883331541723
iteration : 527
train acc:  0.8359375
train loss:  0.3005010485649109
train gradient:  0.1279670708142554
iteration : 528
train acc:  0.90625
train loss:  0.2635149359703064
train gradient:  0.08449647495029784
iteration : 529
train acc:  0.9296875
train loss:  0.25920194387435913
train gradient:  0.090233439295041
iteration : 530
train acc:  0.875
train loss:  0.3509989380836487
train gradient:  0.13477866471176686
iteration : 531
train acc:  0.8125
train loss:  0.3771267235279083
train gradient:  0.22820728607203555
iteration : 532
train acc:  0.9140625
train loss:  0.2777845859527588
train gradient:  0.1192933633958784
iteration : 533
train acc:  0.875
train loss:  0.2946431040763855
train gradient:  0.26513854399345077
iteration : 534
train acc:  0.875
train loss:  0.29977673292160034
train gradient:  0.10560280159783159
iteration : 535
train acc:  0.921875
train loss:  0.2062593251466751
train gradient:  0.09998093981815827
iteration : 536
train acc:  0.796875
train loss:  0.43068987131118774
train gradient:  0.20928894632458472
iteration : 537
train acc:  0.796875
train loss:  0.40976235270500183
train gradient:  0.22144828570624234
iteration : 538
train acc:  0.84375
train loss:  0.3182787597179413
train gradient:  0.12422302660153162
iteration : 539
train acc:  0.8515625
train loss:  0.35222864151000977
train gradient:  0.18450812303002342
iteration : 540
train acc:  0.9140625
train loss:  0.25582069158554077
train gradient:  0.09633082789278341
iteration : 541
train acc:  0.8984375
train loss:  0.2549835443496704
train gradient:  0.08130947106865688
iteration : 542
train acc:  0.875
train loss:  0.27859291434288025
train gradient:  0.11136061282676933
iteration : 543
train acc:  0.9140625
train loss:  0.25916001200675964
train gradient:  0.11278625603635858
iteration : 544
train acc:  0.8515625
train loss:  0.3687857687473297
train gradient:  0.16119303048987027
iteration : 545
train acc:  0.859375
train loss:  0.3778313398361206
train gradient:  0.19784563646324738
iteration : 546
train acc:  0.90625
train loss:  0.2991218864917755
train gradient:  0.12564359002685793
iteration : 547
train acc:  0.859375
train loss:  0.3361218571662903
train gradient:  0.1412283598842104
iteration : 548
train acc:  0.859375
train loss:  0.29922181367874146
train gradient:  0.2163023264714502
iteration : 549
train acc:  0.8515625
train loss:  0.3667433559894562
train gradient:  0.21519595382155624
iteration : 550
train acc:  0.890625
train loss:  0.2540990114212036
train gradient:  0.13673694561490538
iteration : 551
train acc:  0.875
train loss:  0.3174106478691101
train gradient:  0.11128646603700376
iteration : 552
train acc:  0.8515625
train loss:  0.33609333634376526
train gradient:  0.13925294871842653
iteration : 553
train acc:  0.8515625
train loss:  0.31943807005882263
train gradient:  0.2604891757190492
iteration : 554
train acc:  0.8828125
train loss:  0.26400187611579895
train gradient:  0.1109741109592284
iteration : 555
train acc:  0.84375
train loss:  0.39016246795654297
train gradient:  0.19125628219144022
iteration : 556
train acc:  0.8671875
train loss:  0.3885859251022339
train gradient:  0.2013985157445047
iteration : 557
train acc:  0.90625
train loss:  0.26929420232772827
train gradient:  0.07255792725544227
iteration : 558
train acc:  0.9140625
train loss:  0.22602790594100952
train gradient:  0.09527671170173013
iteration : 559
train acc:  0.9140625
train loss:  0.26276999711990356
train gradient:  0.0934127017643231
iteration : 560
train acc:  0.859375
train loss:  0.3189026713371277
train gradient:  0.12395110834567022
iteration : 561
train acc:  0.828125
train loss:  0.37815719842910767
train gradient:  0.1838836494338263
iteration : 562
train acc:  0.9375
train loss:  0.27005234360694885
train gradient:  0.10093064918527032
iteration : 563
train acc:  0.875
train loss:  0.3015921115875244
train gradient:  0.1478550414361476
iteration : 564
train acc:  0.84375
train loss:  0.3698306679725647
train gradient:  0.14883372823552016
iteration : 565
train acc:  0.8671875
train loss:  0.30630096793174744
train gradient:  0.11435263867521081
iteration : 566
train acc:  0.8984375
train loss:  0.2604137361049652
train gradient:  0.07918598324909952
iteration : 567
train acc:  0.8671875
train loss:  0.28166234493255615
train gradient:  0.1259117316108901
iteration : 568
train acc:  0.8515625
train loss:  0.4223291873931885
train gradient:  0.18272091828520298
iteration : 569
train acc:  0.859375
train loss:  0.34559136629104614
train gradient:  0.1335707219493648
iteration : 570
train acc:  0.890625
train loss:  0.28663140535354614
train gradient:  0.09938988969933368
iteration : 571
train acc:  0.7734375
train loss:  0.4679468870162964
train gradient:  0.2829481119660584
iteration : 572
train acc:  0.8203125
train loss:  0.44686758518218994
train gradient:  0.28204275221142855
iteration : 573
train acc:  0.8359375
train loss:  0.4438316822052002
train gradient:  0.22720030286052306
iteration : 574
train acc:  0.859375
train loss:  0.2979658842086792
train gradient:  0.13773624306505575
iteration : 575
train acc:  0.90625
train loss:  0.23563888669013977
train gradient:  0.10055931432533917
iteration : 576
train acc:  0.8046875
train loss:  0.37226539850234985
train gradient:  0.18565688810453518
iteration : 577
train acc:  0.859375
train loss:  0.32364463806152344
train gradient:  0.1636312752602643
iteration : 578
train acc:  0.890625
train loss:  0.26000356674194336
train gradient:  0.09949893799974074
iteration : 579
train acc:  0.875
train loss:  0.3787422776222229
train gradient:  0.18089530937421108
iteration : 580
train acc:  0.84375
train loss:  0.34379124641418457
train gradient:  0.1289482803171967
iteration : 581
train acc:  0.84375
train loss:  0.3454880118370056
train gradient:  0.12224188990942886
iteration : 582
train acc:  0.8828125
train loss:  0.28949427604675293
train gradient:  0.23660450739368113
iteration : 583
train acc:  0.8203125
train loss:  0.3778740167617798
train gradient:  0.23173767630272735
iteration : 584
train acc:  0.8828125
train loss:  0.28357943892478943
train gradient:  0.13460404381110302
iteration : 585
train acc:  0.8828125
train loss:  0.24869531393051147
train gradient:  0.08472649001662858
iteration : 586
train acc:  0.8671875
train loss:  0.30861949920654297
train gradient:  0.12901674652181583
iteration : 587
train acc:  0.8359375
train loss:  0.3496044874191284
train gradient:  0.12062431235634966
iteration : 588
train acc:  0.859375
train loss:  0.31963810324668884
train gradient:  0.1229865800042818
iteration : 589
train acc:  0.875
train loss:  0.30974647402763367
train gradient:  0.14519094830317486
iteration : 590
train acc:  0.8359375
train loss:  0.3761955201625824
train gradient:  0.4486796306538704
iteration : 591
train acc:  0.8359375
train loss:  0.3789234161376953
train gradient:  0.175205091624686
iteration : 592
train acc:  0.8515625
train loss:  0.319227010011673
train gradient:  0.12063173429793014
iteration : 593
train acc:  0.828125
train loss:  0.3792996406555176
train gradient:  0.15663012052414363
iteration : 594
train acc:  0.8671875
train loss:  0.3330041170120239
train gradient:  0.16298226401812677
iteration : 595
train acc:  0.828125
train loss:  0.43368929624557495
train gradient:  0.15598336473743646
iteration : 596
train acc:  0.859375
train loss:  0.34870728850364685
train gradient:  0.13508370819361554
iteration : 597
train acc:  0.8203125
train loss:  0.42519211769104004
train gradient:  0.20186261392557933
iteration : 598
train acc:  0.859375
train loss:  0.32549595832824707
train gradient:  0.12023549331575684
iteration : 599
train acc:  0.890625
train loss:  0.2669776678085327
train gradient:  0.0853662340647025
iteration : 600
train acc:  0.875
train loss:  0.2796562612056732
train gradient:  0.13328782780949894
iteration : 601
train acc:  0.9453125
train loss:  0.20976755023002625
train gradient:  0.095333928032828
iteration : 602
train acc:  0.8515625
train loss:  0.34190377593040466
train gradient:  0.15805834562933618
iteration : 603
train acc:  0.84375
train loss:  0.31849318742752075
train gradient:  0.12374785398409967
iteration : 604
train acc:  0.84375
train loss:  0.3205353617668152
train gradient:  0.13550705488750792
iteration : 605
train acc:  0.859375
train loss:  0.3187541663646698
train gradient:  0.12486501181667804
iteration : 606
train acc:  0.8203125
train loss:  0.37345364689826965
train gradient:  0.13628741156674345
iteration : 607
train acc:  0.8828125
train loss:  0.2926079332828522
train gradient:  0.11074021872201698
iteration : 608
train acc:  0.8359375
train loss:  0.3299749195575714
train gradient:  0.1407239838617637
iteration : 609
train acc:  0.8671875
train loss:  0.3350507616996765
train gradient:  0.10638576842908175
iteration : 610
train acc:  0.921875
train loss:  0.2688872218132019
train gradient:  0.13348666376447166
iteration : 611
train acc:  0.859375
train loss:  0.31873178482055664
train gradient:  0.10428211491408669
iteration : 612
train acc:  0.9140625
train loss:  0.23575496673583984
train gradient:  0.08710453758532387
iteration : 613
train acc:  0.8203125
train loss:  0.36218416690826416
train gradient:  0.16487194087813148
iteration : 614
train acc:  0.890625
train loss:  0.27214205265045166
train gradient:  0.1659925375789765
iteration : 615
train acc:  0.7890625
train loss:  0.37933045625686646
train gradient:  0.21647639107230432
iteration : 616
train acc:  0.8828125
train loss:  0.26002365350723267
train gradient:  0.11304220509964565
iteration : 617
train acc:  0.859375
train loss:  0.30991142988204956
train gradient:  0.10390692042767166
iteration : 618
train acc:  0.8828125
train loss:  0.2519570291042328
train gradient:  0.09378046621110592
iteration : 619
train acc:  0.8828125
train loss:  0.2932531237602234
train gradient:  0.09258766065038476
iteration : 620
train acc:  0.90625
train loss:  0.22295540571212769
train gradient:  0.08864634194907518
iteration : 621
train acc:  0.84375
train loss:  0.41805270314216614
train gradient:  0.17312672739190088
iteration : 622
train acc:  0.8125
train loss:  0.39592137932777405
train gradient:  0.2038200589156381
iteration : 623
train acc:  0.9140625
train loss:  0.25046688318252563
train gradient:  0.07036717754222822
iteration : 624
train acc:  0.84375
train loss:  0.3328774571418762
train gradient:  0.1317889134726169
iteration : 625
train acc:  0.9140625
train loss:  0.3140638470649719
train gradient:  0.1314765140863145
iteration : 626
train acc:  0.875
train loss:  0.35789239406585693
train gradient:  0.1508600260737492
iteration : 627
train acc:  0.8671875
train loss:  0.3237958252429962
train gradient:  0.1256694331681664
iteration : 628
train acc:  0.875
train loss:  0.37335360050201416
train gradient:  0.15376015620140593
iteration : 629
train acc:  0.8671875
train loss:  0.3076631426811218
train gradient:  0.16068724660612904
iteration : 630
train acc:  0.8828125
train loss:  0.27451208233833313
train gradient:  0.10376115559962014
iteration : 631
train acc:  0.859375
train loss:  0.30284208059310913
train gradient:  0.23449179956779115
iteration : 632
train acc:  0.8515625
train loss:  0.3467336893081665
train gradient:  0.139137809077277
iteration : 633
train acc:  0.8515625
train loss:  0.3214626908302307
train gradient:  0.12911832920376637
iteration : 634
train acc:  0.859375
train loss:  0.2786833345890045
train gradient:  0.090818191687928
iteration : 635
train acc:  0.828125
train loss:  0.39045780897140503
train gradient:  0.20553373270622843
iteration : 636
train acc:  0.9140625
train loss:  0.22394701838493347
train gradient:  0.06307862921955652
iteration : 637
train acc:  0.890625
train loss:  0.26428377628326416
train gradient:  0.11433211759981218
iteration : 638
train acc:  0.8125
train loss:  0.3774363100528717
train gradient:  0.1470261163194726
iteration : 639
train acc:  0.8203125
train loss:  0.3959250748157501
train gradient:  0.15784156816303713
iteration : 640
train acc:  0.890625
train loss:  0.27080732583999634
train gradient:  0.07624228554309323
iteration : 641
train acc:  0.828125
train loss:  0.3640021085739136
train gradient:  0.15331642594056677
iteration : 642
train acc:  0.8125
train loss:  0.3601418733596802
train gradient:  0.15905564356647256
iteration : 643
train acc:  0.8671875
train loss:  0.3219422996044159
train gradient:  0.10282033176728056
iteration : 644
train acc:  0.8359375
train loss:  0.32292115688323975
train gradient:  0.10783806449066083
iteration : 645
train acc:  0.875
train loss:  0.2626541256904602
train gradient:  0.07977859675305872
iteration : 646
train acc:  0.875
train loss:  0.307441771030426
train gradient:  0.12389811147559736
iteration : 647
train acc:  0.875
train loss:  0.35878193378448486
train gradient:  0.12791138987525136
iteration : 648
train acc:  0.890625
train loss:  0.36630913615226746
train gradient:  0.1865030728125705
iteration : 649
train acc:  0.859375
train loss:  0.28262394666671753
train gradient:  0.09454743089930939
iteration : 650
train acc:  0.859375
train loss:  0.3011736273765564
train gradient:  0.13464643446564323
iteration : 651
train acc:  0.9140625
train loss:  0.20726272463798523
train gradient:  0.09176518319194248
iteration : 652
train acc:  0.8203125
train loss:  0.32940948009490967
train gradient:  0.12197159463599
iteration : 653
train acc:  0.875
train loss:  0.29908517003059387
train gradient:  0.12247198552626928
iteration : 654
train acc:  0.921875
train loss:  0.23699529469013214
train gradient:  0.09745859099918085
iteration : 655
train acc:  0.8515625
train loss:  0.35509932041168213
train gradient:  0.15043246956645123
iteration : 656
train acc:  0.8671875
train loss:  0.2725490629673004
train gradient:  0.1764873606765881
iteration : 657
train acc:  0.890625
train loss:  0.25519654154777527
train gradient:  0.12104217497175919
iteration : 658
train acc:  0.8671875
train loss:  0.28247392177581787
train gradient:  0.13319136474886392
iteration : 659
train acc:  0.875
train loss:  0.309884250164032
train gradient:  0.11546132453656124
iteration : 660
train acc:  0.875
train loss:  0.29415130615234375
train gradient:  0.1349108340839974
iteration : 661
train acc:  0.875
train loss:  0.328620582818985
train gradient:  0.17270716176388404
iteration : 662
train acc:  0.8515625
train loss:  0.31708207726478577
train gradient:  0.12183937569823458
iteration : 663
train acc:  0.890625
train loss:  0.26132386922836304
train gradient:  0.09548095679250286
iteration : 664
train acc:  0.84375
train loss:  0.38299357891082764
train gradient:  0.13737853108334364
iteration : 665
train acc:  0.8828125
train loss:  0.26182350516319275
train gradient:  0.09728675451100109
iteration : 666
train acc:  0.8359375
train loss:  0.287861704826355
train gradient:  0.08899875640209935
iteration : 667
train acc:  0.9140625
train loss:  0.23708844184875488
train gradient:  0.0981572503756773
iteration : 668
train acc:  0.9453125
train loss:  0.1956792175769806
train gradient:  0.08922917796450745
iteration : 669
train acc:  0.890625
train loss:  0.2603208124637604
train gradient:  0.0840306345031596
iteration : 670
train acc:  0.890625
train loss:  0.27857664227485657
train gradient:  0.09535973552502225
iteration : 671
train acc:  0.84375
train loss:  0.3546489477157593
train gradient:  0.18029142926726327
iteration : 672
train acc:  0.8984375
train loss:  0.26342883706092834
train gradient:  0.10329558109835446
iteration : 673
train acc:  0.8828125
train loss:  0.28154876828193665
train gradient:  0.13519404104203542
iteration : 674
train acc:  0.8671875
train loss:  0.351300984621048
train gradient:  0.12991393469046492
iteration : 675
train acc:  0.828125
train loss:  0.32599470019340515
train gradient:  0.12772412406854458
iteration : 676
train acc:  0.8828125
train loss:  0.3465142548084259
train gradient:  0.123359902089858
iteration : 677
train acc:  0.8515625
train loss:  0.36247506737709045
train gradient:  0.1383103011955304
iteration : 678
train acc:  0.84375
train loss:  0.34211164712905884
train gradient:  0.15866177084244615
iteration : 679
train acc:  0.890625
train loss:  0.29219478368759155
train gradient:  0.12138410512572734
iteration : 680
train acc:  0.8828125
train loss:  0.3256078362464905
train gradient:  0.22741564664090796
iteration : 681
train acc:  0.8828125
train loss:  0.27607935667037964
train gradient:  0.07954087839457591
iteration : 682
train acc:  0.859375
train loss:  0.28875866532325745
train gradient:  0.09157362297593014
iteration : 683
train acc:  0.9140625
train loss:  0.18823473155498505
train gradient:  0.06676210332368243
iteration : 684
train acc:  0.8984375
train loss:  0.26820895075798035
train gradient:  0.11055560456714823
iteration : 685
train acc:  0.875
train loss:  0.3745400607585907
train gradient:  0.18769959384306828
iteration : 686
train acc:  0.84375
train loss:  0.3169021010398865
train gradient:  0.1276774298179143
iteration : 687
train acc:  0.8671875
train loss:  0.2974171042442322
train gradient:  0.11149044702536863
iteration : 688
train acc:  0.9375
train loss:  0.22439715266227722
train gradient:  0.09316123264890763
iteration : 689
train acc:  0.9140625
train loss:  0.24455535411834717
train gradient:  0.07046449703905175
iteration : 690
train acc:  0.859375
train loss:  0.347711980342865
train gradient:  0.14482370792424343
iteration : 691
train acc:  0.8671875
train loss:  0.29110008478164673
train gradient:  0.09923557777288823
iteration : 692
train acc:  0.8671875
train loss:  0.34400200843811035
train gradient:  0.13438006316875578
iteration : 693
train acc:  0.8984375
train loss:  0.2555067241191864
train gradient:  0.08993557250947516
iteration : 694
train acc:  0.8203125
train loss:  0.3469237685203552
train gradient:  0.12119102665921938
iteration : 695
train acc:  0.8671875
train loss:  0.31345680356025696
train gradient:  0.1278456548078739
iteration : 696
train acc:  0.9140625
train loss:  0.2942652702331543
train gradient:  0.1371455410039257
iteration : 697
train acc:  0.84375
train loss:  0.37802743911743164
train gradient:  0.18474712179535535
iteration : 698
train acc:  0.8515625
train loss:  0.3714699149131775
train gradient:  0.14181235051495444
iteration : 699
train acc:  0.859375
train loss:  0.3589209020137787
train gradient:  0.12222640720018131
iteration : 700
train acc:  0.9453125
train loss:  0.25630563497543335
train gradient:  0.10615953076318761
iteration : 701
train acc:  0.8359375
train loss:  0.33995291590690613
train gradient:  0.15677866582460162
iteration : 702
train acc:  0.8984375
train loss:  0.28115206956863403
train gradient:  0.14042762373183304
iteration : 703
train acc:  0.8125
train loss:  0.4096413254737854
train gradient:  0.22273562589990614
iteration : 704
train acc:  0.828125
train loss:  0.3506816625595093
train gradient:  0.19868002461087553
iteration : 705
train acc:  0.8359375
train loss:  0.34911710023880005
train gradient:  0.13639510634925633
iteration : 706
train acc:  0.9140625
train loss:  0.2417314350605011
train gradient:  0.08725746933577397
iteration : 707
train acc:  0.8828125
train loss:  0.24566462635993958
train gradient:  0.07976788296780699
iteration : 708
train acc:  0.859375
train loss:  0.30980759859085083
train gradient:  0.10732507244961938
iteration : 709
train acc:  0.7890625
train loss:  0.44648921489715576
train gradient:  0.24875606732691768
iteration : 710
train acc:  0.84375
train loss:  0.4053633511066437
train gradient:  0.20294067338775545
iteration : 711
train acc:  0.8203125
train loss:  0.3779633045196533
train gradient:  0.17048546921209273
iteration : 712
train acc:  0.875
train loss:  0.30953362584114075
train gradient:  0.1483492900777683
iteration : 713
train acc:  0.875
train loss:  0.2772688865661621
train gradient:  0.0977690450851999
iteration : 714
train acc:  0.859375
train loss:  0.33310967683792114
train gradient:  0.16836226179765923
iteration : 715
train acc:  0.859375
train loss:  0.25596341490745544
train gradient:  0.0772987395077618
iteration : 716
train acc:  0.8671875
train loss:  0.297052264213562
train gradient:  0.11943190023714456
iteration : 717
train acc:  0.8203125
train loss:  0.43789565563201904
train gradient:  0.2552598298001984
iteration : 718
train acc:  0.8515625
train loss:  0.3100624084472656
train gradient:  0.15025684098926942
iteration : 719
train acc:  0.84375
train loss:  0.312367707490921
train gradient:  0.0959046589544089
iteration : 720
train acc:  0.8671875
train loss:  0.3542380928993225
train gradient:  0.14317115134626174
iteration : 721
train acc:  0.890625
train loss:  0.3338771462440491
train gradient:  0.12712373683619252
iteration : 722
train acc:  0.8984375
train loss:  0.2348492443561554
train gradient:  0.0830478735303836
iteration : 723
train acc:  0.8828125
train loss:  0.3093871474266052
train gradient:  0.11867491651578238
iteration : 724
train acc:  0.84375
train loss:  0.3581593334674835
train gradient:  0.1255905176492742
iteration : 725
train acc:  0.8125
train loss:  0.3384358286857605
train gradient:  0.10478818997998299
iteration : 726
train acc:  0.859375
train loss:  0.3470235764980316
train gradient:  0.1335100405517221
iteration : 727
train acc:  0.8515625
train loss:  0.3445074260234833
train gradient:  0.16640426554912874
iteration : 728
train acc:  0.8671875
train loss:  0.31300151348114014
train gradient:  0.11238124701399411
iteration : 729
train acc:  0.8515625
train loss:  0.2956734597682953
train gradient:  0.11966882817586946
iteration : 730
train acc:  0.859375
train loss:  0.3070392608642578
train gradient:  0.14975843275482909
iteration : 731
train acc:  0.8046875
train loss:  0.39124178886413574
train gradient:  0.1531373508468034
iteration : 732
train acc:  0.828125
train loss:  0.34487009048461914
train gradient:  0.2557590375849083
iteration : 733
train acc:  0.8125
train loss:  0.4294315278530121
train gradient:  0.22067878607075567
iteration : 734
train acc:  0.9140625
train loss:  0.2755814790725708
train gradient:  0.12713054448859804
iteration : 735
train acc:  0.859375
train loss:  0.3257076144218445
train gradient:  0.22298517955627178
iteration : 736
train acc:  0.8359375
train loss:  0.38215774297714233
train gradient:  0.1986666776052407
iteration : 737
train acc:  0.8828125
train loss:  0.2624594569206238
train gradient:  0.11159558886989115
iteration : 738
train acc:  0.8984375
train loss:  0.2445831596851349
train gradient:  0.10620741383652497
iteration : 739
train acc:  0.8984375
train loss:  0.21800561249256134
train gradient:  0.0802670082193654
iteration : 740
train acc:  0.8203125
train loss:  0.3812156319618225
train gradient:  0.24236084789826967
iteration : 741
train acc:  0.8515625
train loss:  0.3478569984436035
train gradient:  0.15483070062288784
iteration : 742
train acc:  0.890625
train loss:  0.2393229603767395
train gradient:  0.07642210329985555
iteration : 743
train acc:  0.9296875
train loss:  0.2455868273973465
train gradient:  0.09624622315163782
iteration : 744
train acc:  0.8515625
train loss:  0.3088555335998535
train gradient:  0.14456544833183266
iteration : 745
train acc:  0.859375
train loss:  0.289118230342865
train gradient:  0.0855553164007863
iteration : 746
train acc:  0.859375
train loss:  0.32754814624786377
train gradient:  0.11925977721460161
iteration : 747
train acc:  0.8515625
train loss:  0.32742661237716675
train gradient:  0.09831520298857936
iteration : 748
train acc:  0.8984375
train loss:  0.25652366876602173
train gradient:  0.07867675523466282
iteration : 749
train acc:  0.890625
train loss:  0.28299641609191895
train gradient:  0.1143262115199223
iteration : 750
train acc:  0.8828125
train loss:  0.2655119001865387
train gradient:  0.11481143399691766
iteration : 751
train acc:  0.78125
train loss:  0.4412965178489685
train gradient:  0.26586843239890323
iteration : 752
train acc:  0.8671875
train loss:  0.34224963188171387
train gradient:  0.21143740370480385
iteration : 753
train acc:  0.8515625
train loss:  0.36984485387802124
train gradient:  0.13467762185897053
iteration : 754
train acc:  0.8671875
train loss:  0.2636229991912842
train gradient:  0.12647687673374433
iteration : 755
train acc:  0.8515625
train loss:  0.35842353105545044
train gradient:  0.15264644488558038
iteration : 756
train acc:  0.8671875
train loss:  0.3343387842178345
train gradient:  0.16989518330292885
iteration : 757
train acc:  0.8984375
train loss:  0.2604873776435852
train gradient:  0.09131380059752152
iteration : 758
train acc:  0.796875
train loss:  0.39975807070732117
train gradient:  0.22325217679026133
iteration : 759
train acc:  0.8984375
train loss:  0.24463698267936707
train gradient:  0.09589487946940999
iteration : 760
train acc:  0.8671875
train loss:  0.2898072600364685
train gradient:  0.1431957788734377
iteration : 761
train acc:  0.8671875
train loss:  0.30717530846595764
train gradient:  0.11237211194826545
iteration : 762
train acc:  0.890625
train loss:  0.2845614552497864
train gradient:  0.09940084461110175
iteration : 763
train acc:  0.875
train loss:  0.292495995759964
train gradient:  0.13925084033270835
iteration : 764
train acc:  0.8828125
train loss:  0.2865479290485382
train gradient:  0.14679401818554777
iteration : 765
train acc:  0.828125
train loss:  0.34533363580703735
train gradient:  0.12328070982660788
iteration : 766
train acc:  0.875
train loss:  0.34971848130226135
train gradient:  0.33754893925423357
iteration : 767
train acc:  0.90625
train loss:  0.2718111276626587
train gradient:  0.12268668106526648
iteration : 768
train acc:  0.828125
train loss:  0.37512049078941345
train gradient:  0.16953161088805985
iteration : 769
train acc:  0.828125
train loss:  0.3551858067512512
train gradient:  0.1450018446400083
iteration : 770
train acc:  0.8984375
train loss:  0.2824658155441284
train gradient:  0.1120063667565936
iteration : 771
train acc:  0.890625
train loss:  0.29786181449890137
train gradient:  0.10766934125927233
iteration : 772
train acc:  0.8515625
train loss:  0.33707231283187866
train gradient:  0.13856587651512875
iteration : 773
train acc:  0.875
train loss:  0.27757787704467773
train gradient:  0.0846900387150831
iteration : 774
train acc:  0.8125
train loss:  0.4078809916973114
train gradient:  0.19515492543011492
iteration : 775
train acc:  0.8359375
train loss:  0.33849993348121643
train gradient:  0.16216824069172087
iteration : 776
train acc:  0.9453125
train loss:  0.2410135567188263
train gradient:  0.09511649032987297
iteration : 777
train acc:  0.84375
train loss:  0.3474045395851135
train gradient:  0.12726605451967343
iteration : 778
train acc:  0.84375
train loss:  0.2851099371910095
train gradient:  0.08888626526588561
iteration : 779
train acc:  0.890625
train loss:  0.29607143998146057
train gradient:  0.07674769309216457
iteration : 780
train acc:  0.9140625
train loss:  0.2501039505004883
train gradient:  0.10776909504366146
iteration : 781
train acc:  0.875
train loss:  0.24862687289714813
train gradient:  0.10097773159707846
iteration : 782
train acc:  0.8671875
train loss:  0.3289787173271179
train gradient:  0.14762374128582492
iteration : 783
train acc:  0.84375
train loss:  0.326812744140625
train gradient:  0.11942747386935201
iteration : 784
train acc:  0.859375
train loss:  0.3565632104873657
train gradient:  0.23269369792120664
iteration : 785
train acc:  0.75
train loss:  0.47657787799835205
train gradient:  0.21361277139288457
iteration : 786
train acc:  0.859375
train loss:  0.3330167531967163
train gradient:  0.1965907014495784
iteration : 787
train acc:  0.875
train loss:  0.29455798864364624
train gradient:  0.11548959421650784
iteration : 788
train acc:  0.8125
train loss:  0.3361284136772156
train gradient:  0.11726931694169565
iteration : 789
train acc:  0.890625
train loss:  0.2976219356060028
train gradient:  0.08460080416965095
iteration : 790
train acc:  0.859375
train loss:  0.25307005643844604
train gradient:  0.12743600581911224
iteration : 791
train acc:  0.8671875
train loss:  0.3037729263305664
train gradient:  0.10695496534570352
iteration : 792
train acc:  0.90625
train loss:  0.27786684036254883
train gradient:  0.09366607925473697
iteration : 793
train acc:  0.890625
train loss:  0.2766208052635193
train gradient:  0.10716304597245466
iteration : 794
train acc:  0.8671875
train loss:  0.28086748719215393
train gradient:  0.11516525372972655
iteration : 795
train acc:  0.890625
train loss:  0.30811652541160583
train gradient:  0.1054671966570334
iteration : 796
train acc:  0.921875
train loss:  0.26262909173965454
train gradient:  0.08641315695740871
iteration : 797
train acc:  0.8515625
train loss:  0.30948910117149353
train gradient:  0.13662504635720718
iteration : 798
train acc:  0.84375
train loss:  0.35756543278694153
train gradient:  0.1887138160240509
iteration : 799
train acc:  0.84375
train loss:  0.36794549226760864
train gradient:  0.18225382990596978
iteration : 800
train acc:  0.8203125
train loss:  0.37020671367645264
train gradient:  0.170979544906287
iteration : 801
train acc:  0.8671875
train loss:  0.28905898332595825
train gradient:  0.1376554689229069
iteration : 802
train acc:  0.8671875
train loss:  0.33477193117141724
train gradient:  0.1394513881387548
iteration : 803
train acc:  0.9140625
train loss:  0.25564005970954895
train gradient:  0.08594528645892159
iteration : 804
train acc:  0.8671875
train loss:  0.260097473859787
train gradient:  0.1022111905603893
iteration : 805
train acc:  0.8671875
train loss:  0.4062999188899994
train gradient:  0.1641637576948456
iteration : 806
train acc:  0.9140625
train loss:  0.2069045454263687
train gradient:  0.08662845318633647
iteration : 807
train acc:  0.828125
train loss:  0.3252789378166199
train gradient:  0.18962044620892593
iteration : 808
train acc:  0.890625
train loss:  0.33814167976379395
train gradient:  0.12722790311155185
iteration : 809
train acc:  0.859375
train loss:  0.36085283756256104
train gradient:  0.22102589664039912
iteration : 810
train acc:  0.8828125
train loss:  0.29946017265319824
train gradient:  0.14741608904367623
iteration : 811
train acc:  0.8125
train loss:  0.3797834813594818
train gradient:  0.243804923599885
iteration : 812
train acc:  0.890625
train loss:  0.34537172317504883
train gradient:  0.13217173198853088
iteration : 813
train acc:  0.8515625
train loss:  0.3561820685863495
train gradient:  0.17919461908793366
iteration : 814
train acc:  0.859375
train loss:  0.2773095965385437
train gradient:  0.0911777905646994
iteration : 815
train acc:  0.8515625
train loss:  0.41679495573043823
train gradient:  0.2340600410088296
iteration : 816
train acc:  0.890625
train loss:  0.315117210149765
train gradient:  0.10581068815731241
iteration : 817
train acc:  0.8515625
train loss:  0.3798868656158447
train gradient:  0.2856780822805153
iteration : 818
train acc:  0.875
train loss:  0.2924707233905792
train gradient:  0.12855663458050642
iteration : 819
train acc:  0.8359375
train loss:  0.34825533628463745
train gradient:  0.1467896122397892
iteration : 820
train acc:  0.8515625
train loss:  0.3446716368198395
train gradient:  0.12482180204362756
iteration : 821
train acc:  0.8671875
train loss:  0.3119550347328186
train gradient:  0.1064711351674818
iteration : 822
train acc:  0.8515625
train loss:  0.37359341979026794
train gradient:  0.1693020174273171
iteration : 823
train acc:  0.828125
train loss:  0.3429088294506073
train gradient:  0.19069227491498314
iteration : 824
train acc:  0.8125
train loss:  0.3635438084602356
train gradient:  0.250875131166161
iteration : 825
train acc:  0.7890625
train loss:  0.3697671890258789
train gradient:  0.1750599659597842
iteration : 826
train acc:  0.859375
train loss:  0.31511789560317993
train gradient:  0.1378618564849629
iteration : 827
train acc:  0.8125
train loss:  0.4019157290458679
train gradient:  0.18960817199324914
iteration : 828
train acc:  0.8671875
train loss:  0.3412613868713379
train gradient:  0.1296978815063789
iteration : 829
train acc:  0.8828125
train loss:  0.28192445635795593
train gradient:  0.11175708047888963
iteration : 830
train acc:  0.8671875
train loss:  0.2941863536834717
train gradient:  0.11051447500175374
iteration : 831
train acc:  0.8515625
train loss:  0.352607399225235
train gradient:  0.17563949257176623
iteration : 832
train acc:  0.8984375
train loss:  0.2285957932472229
train gradient:  0.12885429506780688
iteration : 833
train acc:  0.8046875
train loss:  0.36909934878349304
train gradient:  0.1402277209046645
iteration : 834
train acc:  0.8046875
train loss:  0.4216764569282532
train gradient:  0.17229658240236098
iteration : 835
train acc:  0.828125
train loss:  0.33666396141052246
train gradient:  0.1757643345911952
iteration : 836
train acc:  0.890625
train loss:  0.2762991189956665
train gradient:  0.10798121302129761
iteration : 837
train acc:  0.859375
train loss:  0.32970476150512695
train gradient:  0.137631617056248
iteration : 838
train acc:  0.8828125
train loss:  0.2466251403093338
train gradient:  0.07683136787905231
iteration : 839
train acc:  0.9140625
train loss:  0.25912490487098694
train gradient:  0.10725245334011604
iteration : 840
train acc:  0.90625
train loss:  0.25341159105300903
train gradient:  0.08479410463105642
iteration : 841
train acc:  0.8671875
train loss:  0.285724014043808
train gradient:  0.0936640519278096
iteration : 842
train acc:  0.8671875
train loss:  0.3618108034133911
train gradient:  0.14197806497143628
iteration : 843
train acc:  0.921875
train loss:  0.2026481181383133
train gradient:  0.07182665224879617
iteration : 844
train acc:  0.8359375
train loss:  0.3944435119628906
train gradient:  0.2353376503632748
iteration : 845
train acc:  0.8828125
train loss:  0.3013783097267151
train gradient:  0.09233876357637114
iteration : 846
train acc:  0.875
train loss:  0.30652379989624023
train gradient:  0.11575712217380815
iteration : 847
train acc:  0.84375
train loss:  0.3616272211074829
train gradient:  0.1505063164193966
iteration : 848
train acc:  0.8515625
train loss:  0.3725387454032898
train gradient:  0.12369154822195966
iteration : 849
train acc:  0.8828125
train loss:  0.25018107891082764
train gradient:  0.08629568968202207
iteration : 850
train acc:  0.875
train loss:  0.2903876304626465
train gradient:  0.0878697727529946
iteration : 851
train acc:  0.875
train loss:  0.3311782479286194
train gradient:  0.12992691158599842
iteration : 852
train acc:  0.9375
train loss:  0.22652176022529602
train gradient:  0.047077565703775304
iteration : 853
train acc:  0.828125
train loss:  0.4427521526813507
train gradient:  0.21817081100335534
iteration : 854
train acc:  0.8515625
train loss:  0.27622127532958984
train gradient:  0.08184517619009897
iteration : 855
train acc:  0.859375
train loss:  0.3186030983924866
train gradient:  0.1462066934089661
iteration : 856
train acc:  0.8984375
train loss:  0.26716384291648865
train gradient:  0.11707520037278753
iteration : 857
train acc:  0.859375
train loss:  0.32476869225502014
train gradient:  0.11137178101160998
iteration : 858
train acc:  0.875
train loss:  0.2636049687862396
train gradient:  0.0950807316270024
iteration : 859
train acc:  0.875
train loss:  0.3152310848236084
train gradient:  0.09435893206123729
iteration : 860
train acc:  0.8359375
train loss:  0.3064216673374176
train gradient:  0.12523699721714093
iteration : 861
train acc:  0.90625
train loss:  0.2759319543838501
train gradient:  0.13370365594064493
iteration : 862
train acc:  0.84375
train loss:  0.3853142261505127
train gradient:  0.1773851490524297
iteration : 863
train acc:  0.859375
train loss:  0.34253454208374023
train gradient:  0.14463642850882535
iteration : 864
train acc:  0.859375
train loss:  0.28685832023620605
train gradient:  0.13550284236388482
iteration : 865
train acc:  0.8828125
train loss:  0.2612517178058624
train gradient:  0.0767569828666604
iteration : 866
train acc:  0.8515625
train loss:  0.29870712757110596
train gradient:  0.09953132894727612
iteration : 867
train acc:  0.8984375
train loss:  0.26976513862609863
train gradient:  0.11003142173152312
iteration : 868
train acc:  0.84375
train loss:  0.3155069351196289
train gradient:  0.09595158630124298
iteration : 869
train acc:  0.8671875
train loss:  0.273860901594162
train gradient:  0.09480718227932476
iteration : 870
train acc:  0.84375
train loss:  0.32146400213241577
train gradient:  0.13551623761310178
iteration : 871
train acc:  0.90625
train loss:  0.26453936100006104
train gradient:  0.10103859327165686
iteration : 872
train acc:  0.8359375
train loss:  0.3444063067436218
train gradient:  0.12948495486274217
iteration : 873
train acc:  0.875
train loss:  0.3638209402561188
train gradient:  0.18746343931897813
iteration : 874
train acc:  0.8828125
train loss:  0.2671123147010803
train gradient:  0.1128702359310019
iteration : 875
train acc:  0.90625
train loss:  0.23041118681430817
train gradient:  0.10102723715477263
iteration : 876
train acc:  0.8671875
train loss:  0.31702449917793274
train gradient:  0.13496577581263294
iteration : 877
train acc:  0.859375
train loss:  0.2779120206832886
train gradient:  0.10021588278877543
iteration : 878
train acc:  0.8515625
train loss:  0.4198038578033447
train gradient:  0.357479139520136
iteration : 879
train acc:  0.90625
train loss:  0.22640757262706757
train gradient:  0.10705580122725124
iteration : 880
train acc:  0.8671875
train loss:  0.3184112310409546
train gradient:  0.20290262267016435
iteration : 881
train acc:  0.8359375
train loss:  0.36992311477661133
train gradient:  0.340899605782874
iteration : 882
train acc:  0.796875
train loss:  0.3717178702354431
train gradient:  0.15569028521852668
iteration : 883
train acc:  0.859375
train loss:  0.2835029661655426
train gradient:  0.18466499822903298
iteration : 884
train acc:  0.8125
train loss:  0.3278958201408386
train gradient:  0.13569175078565146
iteration : 885
train acc:  0.875
train loss:  0.3085590600967407
train gradient:  0.10642951607241655
iteration : 886
train acc:  0.8671875
train loss:  0.3039793372154236
train gradient:  0.15186351499819156
iteration : 887
train acc:  0.8828125
train loss:  0.23497948050498962
train gradient:  0.0947765466463795
iteration : 888
train acc:  0.828125
train loss:  0.3633211851119995
train gradient:  0.14539081250082253
iteration : 889
train acc:  0.875
train loss:  0.3283507525920868
train gradient:  0.14199182521143494
iteration : 890
train acc:  0.8671875
train loss:  0.3311610221862793
train gradient:  0.2006601469931894
iteration : 891
train acc:  0.9140625
train loss:  0.22827382385730743
train gradient:  0.11565527978917163
iteration : 892
train acc:  0.8359375
train loss:  0.3733116388320923
train gradient:  0.17740471927884072
iteration : 893
train acc:  0.890625
train loss:  0.2707805931568146
train gradient:  0.09596150277737506
iteration : 894
train acc:  0.875
train loss:  0.3048897087574005
train gradient:  0.15819870963881447
iteration : 895
train acc:  0.8046875
train loss:  0.3596349358558655
train gradient:  0.2542546245096064
iteration : 896
train acc:  0.84375
train loss:  0.37638911604881287
train gradient:  0.17292560599457263
iteration : 897
train acc:  0.8203125
train loss:  0.38349342346191406
train gradient:  0.17376469190032623
iteration : 898
train acc:  0.8671875
train loss:  0.3392234444618225
train gradient:  0.1369072204753638
iteration : 899
train acc:  0.8828125
train loss:  0.2797037363052368
train gradient:  0.12759046075956906
iteration : 900
train acc:  0.8828125
train loss:  0.29663264751434326
train gradient:  0.13676180118988854
iteration : 901
train acc:  0.8671875
train loss:  0.3319511115550995
train gradient:  0.10973543645812606
iteration : 902
train acc:  0.890625
train loss:  0.26624780893325806
train gradient:  0.09715498190099946
iteration : 903
train acc:  0.8828125
train loss:  0.2790392339229584
train gradient:  0.12102685319294569
iteration : 904
train acc:  0.8046875
train loss:  0.3143264651298523
train gradient:  0.14785644320595615
iteration : 905
train acc:  0.9375
train loss:  0.26619037985801697
train gradient:  0.12131308976281761
iteration : 906
train acc:  0.84375
train loss:  0.3110450506210327
train gradient:  0.13605713224536697
iteration : 907
train acc:  0.8515625
train loss:  0.33620670437812805
train gradient:  0.1777056848998907
iteration : 908
train acc:  0.8671875
train loss:  0.31275463104248047
train gradient:  0.11746698635412783
iteration : 909
train acc:  0.875
train loss:  0.2621760964393616
train gradient:  0.10442303470411654
iteration : 910
train acc:  0.90625
train loss:  0.24603819847106934
train gradient:  0.06980476395469659
iteration : 911
train acc:  0.859375
train loss:  0.31219106912612915
train gradient:  0.10293832329177278
iteration : 912
train acc:  0.859375
train loss:  0.327915757894516
train gradient:  0.1619644808095385
iteration : 913
train acc:  0.8671875
train loss:  0.32295525074005127
train gradient:  0.11397098781163965
iteration : 914
train acc:  0.875
train loss:  0.212381511926651
train gradient:  0.07066328733956645
iteration : 915
train acc:  0.828125
train loss:  0.366670697927475
train gradient:  0.18644494436185932
iteration : 916
train acc:  0.8984375
train loss:  0.2658291459083557
train gradient:  0.08192092623971069
iteration : 917
train acc:  0.8828125
train loss:  0.2770121097564697
train gradient:  0.15762959163760137
iteration : 918
train acc:  0.890625
train loss:  0.26185983419418335
train gradient:  0.09203773995737682
iteration : 919
train acc:  0.8515625
train loss:  0.3479355573654175
train gradient:  0.15660694032185016
iteration : 920
train acc:  0.8515625
train loss:  0.29761552810668945
train gradient:  0.09967364624520442
iteration : 921
train acc:  0.890625
train loss:  0.367240846157074
train gradient:  0.1622621052016669
iteration : 922
train acc:  0.8984375
train loss:  0.28941214084625244
train gradient:  0.10217140064798283
iteration : 923
train acc:  0.9375
train loss:  0.19638170301914215
train gradient:  0.08843382956759789
iteration : 924
train acc:  0.8203125
train loss:  0.39299121499061584
train gradient:  0.2499759154756433
iteration : 925
train acc:  0.859375
train loss:  0.27813032269477844
train gradient:  0.11645833544727195
iteration : 926
train acc:  0.875
train loss:  0.3046773076057434
train gradient:  0.2062279436413177
iteration : 927
train acc:  0.890625
train loss:  0.27283650636672974
train gradient:  0.08870721129366103
iteration : 928
train acc:  0.8515625
train loss:  0.32039350271224976
train gradient:  0.2016005683883598
iteration : 929
train acc:  0.859375
train loss:  0.32490643858909607
train gradient:  0.1036035305654827
iteration : 930
train acc:  0.8984375
train loss:  0.2905285954475403
train gradient:  0.10416460793406732
iteration : 931
train acc:  0.84375
train loss:  0.38456815481185913
train gradient:  0.1797181998844889
iteration : 932
train acc:  0.8671875
train loss:  0.28078120946884155
train gradient:  0.10932369164449258
iteration : 933
train acc:  0.8671875
train loss:  0.3112821578979492
train gradient:  0.14365736267830148
iteration : 934
train acc:  0.8671875
train loss:  0.3197115957736969
train gradient:  0.12986366493433946
iteration : 935
train acc:  0.875
train loss:  0.2773546576499939
train gradient:  0.1140641644001795
iteration : 936
train acc:  0.90625
train loss:  0.2087521255016327
train gradient:  0.07193902931325871
iteration : 937
train acc:  0.84375
train loss:  0.3276107609272003
train gradient:  0.1305556998757246
iteration : 938
train acc:  0.875
train loss:  0.310625821352005
train gradient:  0.10539253970172002
iteration : 939
train acc:  0.8359375
train loss:  0.3302210867404938
train gradient:  0.15966154372796246
iteration : 940
train acc:  0.890625
train loss:  0.2589874863624573
train gradient:  0.11909964789922002
iteration : 941
train acc:  0.8984375
train loss:  0.34949806332588196
train gradient:  0.1994628623579408
iteration : 942
train acc:  0.8359375
train loss:  0.35215699672698975
train gradient:  0.21346257961252324
iteration : 943
train acc:  0.828125
train loss:  0.35138973593711853
train gradient:  0.20414700978141576
iteration : 944
train acc:  0.8828125
train loss:  0.2445799708366394
train gradient:  0.13619695801234172
iteration : 945
train acc:  0.828125
train loss:  0.4200543165206909
train gradient:  0.19083260123769835
iteration : 946
train acc:  0.875
train loss:  0.30002617835998535
train gradient:  0.12102722166749393
iteration : 947
train acc:  0.859375
train loss:  0.3013730049133301
train gradient:  0.1767630238915648
iteration : 948
train acc:  0.8671875
train loss:  0.31908756494522095
train gradient:  0.10870247504649576
iteration : 949
train acc:  0.8125
train loss:  0.39753854274749756
train gradient:  0.18089470165177984
iteration : 950
train acc:  0.8515625
train loss:  0.3421849310398102
train gradient:  0.1546033339673399
iteration : 951
train acc:  0.8671875
train loss:  0.299704909324646
train gradient:  0.1077751016881204
iteration : 952
train acc:  0.859375
train loss:  0.28552910685539246
train gradient:  0.14066734565695752
iteration : 953
train acc:  0.859375
train loss:  0.3389683961868286
train gradient:  0.14442764406871747
iteration : 954
train acc:  0.90625
train loss:  0.24239642918109894
train gradient:  0.2741430519257423
iteration : 955
train acc:  0.8046875
train loss:  0.3428247570991516
train gradient:  0.17555817354456826
iteration : 956
train acc:  0.8828125
train loss:  0.3213253915309906
train gradient:  0.15193552922569126
iteration : 957
train acc:  0.8671875
train loss:  0.2488551139831543
train gradient:  0.09360418447566957
iteration : 958
train acc:  0.84375
train loss:  0.2999153733253479
train gradient:  0.1217929234093621
iteration : 959
train acc:  0.8359375
train loss:  0.398456871509552
train gradient:  0.2156102374718172
iteration : 960
train acc:  0.8984375
train loss:  0.24771137535572052
train gradient:  0.07837193764338303
iteration : 961
train acc:  0.8515625
train loss:  0.26602983474731445
train gradient:  0.10410795640317441
iteration : 962
train acc:  0.8671875
train loss:  0.28197169303894043
train gradient:  0.10379996180944744
iteration : 963
train acc:  0.84375
train loss:  0.32348716259002686
train gradient:  0.1349314799772484
iteration : 964
train acc:  0.859375
train loss:  0.39449596405029297
train gradient:  0.1637801645591621
iteration : 965
train acc:  0.9140625
train loss:  0.25979846715927124
train gradient:  0.10619004615604684
iteration : 966
train acc:  0.8515625
train loss:  0.25890177488327026
train gradient:  0.12959797717203664
iteration : 967
train acc:  0.8203125
train loss:  0.37580013275146484
train gradient:  0.1999705044329964
iteration : 968
train acc:  0.90625
train loss:  0.28927573561668396
train gradient:  0.11576445328602149
iteration : 969
train acc:  0.828125
train loss:  0.3344411551952362
train gradient:  0.16503998923095464
iteration : 970
train acc:  0.8828125
train loss:  0.3488847017288208
train gradient:  0.13784655273328933
iteration : 971
train acc:  0.8671875
train loss:  0.26839855313301086
train gradient:  0.08932408969992876
iteration : 972
train acc:  0.859375
train loss:  0.35581523180007935
train gradient:  0.20468114419888195
iteration : 973
train acc:  0.796875
train loss:  0.4547184407711029
train gradient:  0.2214608848214788
iteration : 974
train acc:  0.859375
train loss:  0.3855619430541992
train gradient:  0.19065844269158874
iteration : 975
train acc:  0.90625
train loss:  0.27101901173591614
train gradient:  0.1729024826384002
iteration : 976
train acc:  0.875
train loss:  0.2911059856414795
train gradient:  0.08912407800841349
iteration : 977
train acc:  0.875
train loss:  0.3089386820793152
train gradient:  0.1956485667152006
iteration : 978
train acc:  0.921875
train loss:  0.26704204082489014
train gradient:  0.1028676890010575
iteration : 979
train acc:  0.8671875
train loss:  0.30925917625427246
train gradient:  0.1754919788115577
iteration : 980
train acc:  0.8828125
train loss:  0.24732805788516998
train gradient:  0.10329013210226444
iteration : 981
train acc:  0.8671875
train loss:  0.31731608510017395
train gradient:  0.13318076289704453
iteration : 982
train acc:  0.828125
train loss:  0.3660275638103485
train gradient:  0.19679575211473904
iteration : 983
train acc:  0.875
train loss:  0.3745933771133423
train gradient:  0.23648313540240512
iteration : 984
train acc:  0.859375
train loss:  0.28229087591171265
train gradient:  0.1553810947635308
iteration : 985
train acc:  0.8828125
train loss:  0.2827112078666687
train gradient:  0.11877557780430667
iteration : 986
train acc:  0.8515625
train loss:  0.2587091326713562
train gradient:  0.16498793402483347
iteration : 987
train acc:  0.8671875
train loss:  0.332689106464386
train gradient:  0.18918178958552767
iteration : 988
train acc:  0.8671875
train loss:  0.28546473383903503
train gradient:  0.10806295067437803
iteration : 989
train acc:  0.8671875
train loss:  0.3455076217651367
train gradient:  0.10737241175444144
iteration : 990
train acc:  0.8515625
train loss:  0.27784985303878784
train gradient:  0.08278313246844367
iteration : 991
train acc:  0.859375
train loss:  0.2905948758125305
train gradient:  0.11439111233973481
iteration : 992
train acc:  0.828125
train loss:  0.36151599884033203
train gradient:  0.2686073551022165
iteration : 993
train acc:  0.828125
train loss:  0.321114182472229
train gradient:  0.11364299210735469
iteration : 994
train acc:  0.90625
train loss:  0.2574988007545471
train gradient:  0.1183716803713389
iteration : 995
train acc:  0.859375
train loss:  0.31144022941589355
train gradient:  0.1128805775188526
iteration : 996
train acc:  0.8359375
train loss:  0.4005325734615326
train gradient:  0.20181592432464712
iteration : 997
train acc:  0.90625
train loss:  0.2653086483478546
train gradient:  0.10539558294551055
iteration : 998
train acc:  0.9453125
train loss:  0.2089838683605194
train gradient:  0.07910452510780663
iteration : 999
train acc:  0.8203125
train loss:  0.36281639337539673
train gradient:  0.1529961828763424
iteration : 1000
train acc:  0.828125
train loss:  0.30357909202575684
train gradient:  0.11850706260053226
iteration : 1001
train acc:  0.8671875
train loss:  0.2535412013530731
train gradient:  0.0885473450197881
iteration : 1002
train acc:  0.890625
train loss:  0.2853872776031494
train gradient:  0.10905916380372276
iteration : 1003
train acc:  0.8515625
train loss:  0.31067776679992676
train gradient:  0.16148540812337908
iteration : 1004
train acc:  0.7890625
train loss:  0.4430552124977112
train gradient:  0.260358726773707
iteration : 1005
train acc:  0.84375
train loss:  0.38529014587402344
train gradient:  0.20312510048679328
iteration : 1006
train acc:  0.8828125
train loss:  0.2900089919567108
train gradient:  0.13310581136547545
iteration : 1007
train acc:  0.8671875
train loss:  0.2932754158973694
train gradient:  0.10706011231980114
iteration : 1008
train acc:  0.8359375
train loss:  0.35775548219680786
train gradient:  0.13040204291793825
iteration : 1009
train acc:  0.875
train loss:  0.27786386013031006
train gradient:  0.12123233529288546
iteration : 1010
train acc:  0.8671875
train loss:  0.28256550431251526
train gradient:  0.0948367858590719
iteration : 1011
train acc:  0.828125
train loss:  0.3807353973388672
train gradient:  0.246884839124648
iteration : 1012
train acc:  0.84375
train loss:  0.3681735396385193
train gradient:  0.1410108396982382
iteration : 1013
train acc:  0.8828125
train loss:  0.24036388099193573
train gradient:  0.07851239565386232
iteration : 1014
train acc:  0.8671875
train loss:  0.28295576572418213
train gradient:  0.17316278231118132
iteration : 1015
train acc:  0.84375
train loss:  0.33230942487716675
train gradient:  0.1017552344583801
iteration : 1016
train acc:  0.8984375
train loss:  0.24978671967983246
train gradient:  0.10062219258912275
iteration : 1017
train acc:  0.8046875
train loss:  0.37734395265579224
train gradient:  0.1538315810464292
iteration : 1018
train acc:  0.8359375
train loss:  0.3266322910785675
train gradient:  0.12691014294896352
iteration : 1019
train acc:  0.8359375
train loss:  0.3491991460323334
train gradient:  0.2145317594956112
iteration : 1020
train acc:  0.84375
train loss:  0.35971421003341675
train gradient:  0.19038905096349495
iteration : 1021
train acc:  0.84375
train loss:  0.295789897441864
train gradient:  0.11451924577069421
iteration : 1022
train acc:  0.8046875
train loss:  0.44625353813171387
train gradient:  0.22094040824854083
iteration : 1023
train acc:  0.8671875
train loss:  0.3407234251499176
train gradient:  0.1919184287931356
iteration : 1024
train acc:  0.8984375
train loss:  0.28270530700683594
train gradient:  0.08134737219840245
iteration : 1025
train acc:  0.9140625
train loss:  0.2801021337509155
train gradient:  0.1270884068126501
iteration : 1026
train acc:  0.8671875
train loss:  0.24999359250068665
train gradient:  0.09203231913332227
iteration : 1027
train acc:  0.8671875
train loss:  0.35988837480545044
train gradient:  0.14418415087511707
iteration : 1028
train acc:  0.84375
train loss:  0.3384784460067749
train gradient:  0.1594882630894149
iteration : 1029
train acc:  0.875
train loss:  0.33688250184059143
train gradient:  0.11318514855530445
iteration : 1030
train acc:  0.90625
train loss:  0.2546992301940918
train gradient:  0.09733392211888497
iteration : 1031
train acc:  0.921875
train loss:  0.22074481844902039
train gradient:  0.07387276059694463
iteration : 1032
train acc:  0.8359375
train loss:  0.3658592700958252
train gradient:  0.1646018009177324
iteration : 1033
train acc:  0.90625
train loss:  0.2962563633918762
train gradient:  0.1152750621331939
iteration : 1034
train acc:  0.8984375
train loss:  0.35331204533576965
train gradient:  0.15090882838665803
iteration : 1035
train acc:  0.859375
train loss:  0.39373674988746643
train gradient:  0.15734906332125367
iteration : 1036
train acc:  0.84375
train loss:  0.29031357169151306
train gradient:  0.10244276921727831
iteration : 1037
train acc:  0.8671875
train loss:  0.3358003497123718
train gradient:  0.1311815271222479
iteration : 1038
train acc:  0.8203125
train loss:  0.4165968596935272
train gradient:  0.16192273401077573
iteration : 1039
train acc:  0.796875
train loss:  0.4607706367969513
train gradient:  0.1679347900984996
iteration : 1040
train acc:  0.8828125
train loss:  0.3085673749446869
train gradient:  0.13176960771389074
iteration : 1041
train acc:  0.8984375
train loss:  0.27247917652130127
train gradient:  0.10003756572920237
iteration : 1042
train acc:  0.8828125
train loss:  0.3131130039691925
train gradient:  0.15705338284917963
iteration : 1043
train acc:  0.875
train loss:  0.31096625328063965
train gradient:  0.10780866764511242
iteration : 1044
train acc:  0.84375
train loss:  0.36334890127182007
train gradient:  0.15937221180558853
iteration : 1045
train acc:  0.9296875
train loss:  0.2289418876171112
train gradient:  0.08274978313165006
iteration : 1046
train acc:  0.8671875
train loss:  0.2766745388507843
train gradient:  0.0914959153432148
iteration : 1047
train acc:  0.8984375
train loss:  0.2955501675605774
train gradient:  0.1013079090511608
iteration : 1048
train acc:  0.8203125
train loss:  0.32774364948272705
train gradient:  0.11950648645622937
iteration : 1049
train acc:  0.84375
train loss:  0.3225523829460144
train gradient:  0.11724163446627199
iteration : 1050
train acc:  0.859375
train loss:  0.30626487731933594
train gradient:  0.10833872959196758
iteration : 1051
train acc:  0.859375
train loss:  0.35252875089645386
train gradient:  0.17219413539613032
iteration : 1052
train acc:  0.828125
train loss:  0.36815789341926575
train gradient:  0.12550187665162377
iteration : 1053
train acc:  0.828125
train loss:  0.3661595284938812
train gradient:  0.14882743410615704
iteration : 1054
train acc:  0.8984375
train loss:  0.261646568775177
train gradient:  0.0904799191391732
iteration : 1055
train acc:  0.875
train loss:  0.32398921251296997
train gradient:  0.19017706630165038
iteration : 1056
train acc:  0.875
train loss:  0.3264103829860687
train gradient:  0.10471786721181205
iteration : 1057
train acc:  0.8515625
train loss:  0.3555424213409424
train gradient:  0.15305806740942338
iteration : 1058
train acc:  0.875
train loss:  0.2642463445663452
train gradient:  0.08078265872530425
iteration : 1059
train acc:  0.8125
train loss:  0.3978584110736847
train gradient:  0.21299900416558326
iteration : 1060
train acc:  0.8671875
train loss:  0.29656657576560974
train gradient:  0.1145062180298635
iteration : 1061
train acc:  0.828125
train loss:  0.3776390254497528
train gradient:  0.21417064658687412
iteration : 1062
train acc:  0.859375
train loss:  0.3206036686897278
train gradient:  0.11460201397740981
iteration : 1063
train acc:  0.890625
train loss:  0.2635117173194885
train gradient:  0.09727048074717495
iteration : 1064
train acc:  0.8203125
train loss:  0.3557285666465759
train gradient:  0.12975090513848203
iteration : 1065
train acc:  0.8984375
train loss:  0.2639648914337158
train gradient:  0.10492075174532896
iteration : 1066
train acc:  0.859375
train loss:  0.29170486330986023
train gradient:  0.09153804668897204
iteration : 1067
train acc:  0.890625
train loss:  0.26826393604278564
train gradient:  0.11771263884154588
iteration : 1068
train acc:  0.8359375
train loss:  0.39687007665634155
train gradient:  0.19290702183066533
iteration : 1069
train acc:  0.8828125
train loss:  0.28444111347198486
train gradient:  0.1371934180355452
iteration : 1070
train acc:  0.859375
train loss:  0.32674747705459595
train gradient:  0.12005847056649689
iteration : 1071
train acc:  0.875
train loss:  0.3027515113353729
train gradient:  0.12041214512135251
iteration : 1072
train acc:  0.8359375
train loss:  0.3704892694950104
train gradient:  0.15589961049861972
iteration : 1073
train acc:  0.8671875
train loss:  0.3143123686313629
train gradient:  0.07432195589248716
iteration : 1074
train acc:  0.8671875
train loss:  0.32085680961608887
train gradient:  0.1456232308649245
iteration : 1075
train acc:  0.8359375
train loss:  0.32542848587036133
train gradient:  0.12580487159805234
iteration : 1076
train acc:  0.8828125
train loss:  0.26487985253334045
train gradient:  0.10158546781757156
iteration : 1077
train acc:  0.859375
train loss:  0.31522950530052185
train gradient:  0.12921370245856456
iteration : 1078
train acc:  0.875
train loss:  0.27938079833984375
train gradient:  0.09907625305052851
iteration : 1079
train acc:  0.8984375
train loss:  0.23409219086170197
train gradient:  0.08248719456091641
iteration : 1080
train acc:  0.8515625
train loss:  0.2757617235183716
train gradient:  0.10157382774115714
iteration : 1081
train acc:  0.9140625
train loss:  0.210139200091362
train gradient:  0.07606449950925526
iteration : 1082
train acc:  0.8828125
train loss:  0.2731723189353943
train gradient:  0.10189374619683607
iteration : 1083
train acc:  0.875
train loss:  0.31670528650283813
train gradient:  0.1362417666230859
iteration : 1084
train acc:  0.875
train loss:  0.31590497493743896
train gradient:  0.1281331411337229
iteration : 1085
train acc:  0.875
train loss:  0.2990787625312805
train gradient:  0.16784009631097221
iteration : 1086
train acc:  0.875
train loss:  0.2874375581741333
train gradient:  0.12651005813035351
iteration : 1087
train acc:  0.859375
train loss:  0.40279126167297363
train gradient:  0.21062274257969116
iteration : 1088
train acc:  0.8125
train loss:  0.324862539768219
train gradient:  0.13583090584618268
iteration : 1089
train acc:  0.8515625
train loss:  0.3316453993320465
train gradient:  0.10567984532989454
iteration : 1090
train acc:  0.890625
train loss:  0.28716176748275757
train gradient:  0.13420835103849355
iteration : 1091
train acc:  0.859375
train loss:  0.2916911542415619
train gradient:  0.08789807451854015
iteration : 1092
train acc:  0.8125
train loss:  0.35151249170303345
train gradient:  0.14522373115102397
iteration : 1093
train acc:  0.84375
train loss:  0.3396894931793213
train gradient:  0.17145581426653217
iteration : 1094
train acc:  0.7734375
train loss:  0.5301142930984497
train gradient:  0.30675610089810784
iteration : 1095
train acc:  0.828125
train loss:  0.3392564058303833
train gradient:  0.15736266382002578
iteration : 1096
train acc:  0.8515625
train loss:  0.3630320131778717
train gradient:  0.14087678106583376
iteration : 1097
train acc:  0.90625
train loss:  0.1982542723417282
train gradient:  0.06141881206696099
iteration : 1098
train acc:  0.875
train loss:  0.2700090706348419
train gradient:  0.11068171528781498
iteration : 1099
train acc:  0.8515625
train loss:  0.30231529474258423
train gradient:  0.12235534653808763
iteration : 1100
train acc:  0.84375
train loss:  0.3859608471393585
train gradient:  0.17569556773127082
iteration : 1101
train acc:  0.9140625
train loss:  0.2723139524459839
train gradient:  0.12645505769508072
iteration : 1102
train acc:  0.8984375
train loss:  0.2863985002040863
train gradient:  0.13753850617589136
iteration : 1103
train acc:  0.9140625
train loss:  0.24852576851844788
train gradient:  0.08491959030333282
iteration : 1104
train acc:  0.84375
train loss:  0.3666146993637085
train gradient:  0.20673477859599554
iteration : 1105
train acc:  0.875
train loss:  0.30646616220474243
train gradient:  0.12381258260913976
iteration : 1106
train acc:  0.796875
train loss:  0.46713346242904663
train gradient:  0.4498112463679357
iteration : 1107
train acc:  0.859375
train loss:  0.39320850372314453
train gradient:  0.14663927812876834
iteration : 1108
train acc:  0.90625
train loss:  0.29379573464393616
train gradient:  0.16547616688689792
iteration : 1109
train acc:  0.8671875
train loss:  0.27829110622406006
train gradient:  0.10956505736228765
iteration : 1110
train acc:  0.8984375
train loss:  0.23884031176567078
train gradient:  0.09505461346676013
iteration : 1111
train acc:  0.8671875
train loss:  0.289922833442688
train gradient:  0.09623411480209486
iteration : 1112
train acc:  0.8515625
train loss:  0.344244122505188
train gradient:  0.13914052307698477
iteration : 1113
train acc:  0.8671875
train loss:  0.2810204029083252
train gradient:  0.10557916949913355
iteration : 1114
train acc:  0.875
train loss:  0.3308652639389038
train gradient:  0.10349624865334374
iteration : 1115
train acc:  0.8828125
train loss:  0.32382017374038696
train gradient:  0.1223141970091476
iteration : 1116
train acc:  0.890625
train loss:  0.3082568049430847
train gradient:  0.14384212216717868
iteration : 1117
train acc:  0.84375
train loss:  0.30458223819732666
train gradient:  0.13249459201207658
iteration : 1118
train acc:  0.8984375
train loss:  0.2721363306045532
train gradient:  0.07172046184526999
iteration : 1119
train acc:  0.890625
train loss:  0.24532434344291687
train gradient:  0.08706622741987192
iteration : 1120
train acc:  0.828125
train loss:  0.3496752381324768
train gradient:  0.1665732926711505
iteration : 1121
train acc:  0.84375
train loss:  0.33930569887161255
train gradient:  0.16013847825426092
iteration : 1122
train acc:  0.8828125
train loss:  0.2581103444099426
train gradient:  0.09315227639139201
iteration : 1123
train acc:  0.890625
train loss:  0.3036818206310272
train gradient:  0.09158097126398856
iteration : 1124
train acc:  0.859375
train loss:  0.327741414308548
train gradient:  0.15187781168634512
iteration : 1125
train acc:  0.8359375
train loss:  0.33472520112991333
train gradient:  0.20218365208517614
iteration : 1126
train acc:  0.90625
train loss:  0.28337135910987854
train gradient:  0.07022741595225686
iteration : 1127
train acc:  0.890625
train loss:  0.2593107521533966
train gradient:  0.09269124073844805
iteration : 1128
train acc:  0.8203125
train loss:  0.37364301085472107
train gradient:  0.16312065648308954
iteration : 1129
train acc:  0.8828125
train loss:  0.2758397161960602
train gradient:  0.102062006103472
iteration : 1130
train acc:  0.8671875
train loss:  0.3092520833015442
train gradient:  0.10333524925105958
iteration : 1131
train acc:  0.890625
train loss:  0.24619142711162567
train gradient:  0.11200932069290341
iteration : 1132
train acc:  0.8671875
train loss:  0.30383211374282837
train gradient:  0.0670518643985646
iteration : 1133
train acc:  0.921875
train loss:  0.25136128067970276
train gradient:  0.08883830335198277
iteration : 1134
train acc:  0.9296875
train loss:  0.2383524477481842
train gradient:  0.0875447422338698
iteration : 1135
train acc:  0.8359375
train loss:  0.4092663824558258
train gradient:  0.27633208479514376
iteration : 1136
train acc:  0.90625
train loss:  0.22793330252170563
train gradient:  0.09737424082635433
iteration : 1137
train acc:  0.8515625
train loss:  0.337408185005188
train gradient:  0.17084116837103186
iteration : 1138
train acc:  0.8828125
train loss:  0.29022106528282166
train gradient:  0.10276964211513519
iteration : 1139
train acc:  0.8828125
train loss:  0.2634105086326599
train gradient:  0.11067475747859569
iteration : 1140
train acc:  0.8828125
train loss:  0.28231382369995117
train gradient:  0.13612795932064847
iteration : 1141
train acc:  0.8984375
train loss:  0.26024535298347473
train gradient:  0.11001169926681659
iteration : 1142
train acc:  0.84375
train loss:  0.3460681438446045
train gradient:  0.1547815454717572
iteration : 1143
train acc:  0.8359375
train loss:  0.3080037534236908
train gradient:  0.12321533297395107
iteration : 1144
train acc:  0.890625
train loss:  0.30787190794944763
train gradient:  0.09568200012315012
iteration : 1145
train acc:  0.890625
train loss:  0.2266187220811844
train gradient:  0.11474661318223925
iteration : 1146
train acc:  0.859375
train loss:  0.30032944679260254
train gradient:  0.1452001667892197
iteration : 1147
train acc:  0.890625
train loss:  0.3034493029117584
train gradient:  0.10881203449660869
iteration : 1148
train acc:  0.84375
train loss:  0.321079820394516
train gradient:  0.1318647386061626
iteration : 1149
train acc:  0.875
train loss:  0.32169604301452637
train gradient:  0.17171863281889604
iteration : 1150
train acc:  0.90625
train loss:  0.25275158882141113
train gradient:  0.11031957192822245
iteration : 1151
train acc:  0.8203125
train loss:  0.37126675248146057
train gradient:  0.21336786188964332
iteration : 1152
train acc:  0.8828125
train loss:  0.24511171877384186
train gradient:  0.08104622946573534
iteration : 1153
train acc:  0.8515625
train loss:  0.35191768407821655
train gradient:  0.13793570771977043
iteration : 1154
train acc:  0.90625
train loss:  0.23027358949184418
train gradient:  0.0813512052409145
iteration : 1155
train acc:  0.7890625
train loss:  0.40922436118125916
train gradient:  0.20008893007612477
iteration : 1156
train acc:  0.859375
train loss:  0.3138793706893921
train gradient:  0.1448016753452492
iteration : 1157
train acc:  0.8515625
train loss:  0.3071955144405365
train gradient:  0.12926138063368384
iteration : 1158
train acc:  0.8984375
train loss:  0.29676514863967896
train gradient:  0.17728140627993538
iteration : 1159
train acc:  0.84375
train loss:  0.338632732629776
train gradient:  0.145508342135983
iteration : 1160
train acc:  0.875
train loss:  0.27831608057022095
train gradient:  0.19458069145985696
iteration : 1161
train acc:  0.8515625
train loss:  0.3119005560874939
train gradient:  0.13828453359827048
iteration : 1162
train acc:  0.8828125
train loss:  0.27656516432762146
train gradient:  0.13045335792265222
iteration : 1163
train acc:  0.8359375
train loss:  0.3742154836654663
train gradient:  0.19903135185335413
iteration : 1164
train acc:  0.8828125
train loss:  0.2629649043083191
train gradient:  0.14132227399580263
iteration : 1165
train acc:  0.8671875
train loss:  0.29056334495544434
train gradient:  0.207516182180565
iteration : 1166
train acc:  0.84375
train loss:  0.3186405897140503
train gradient:  0.14712098163727058
iteration : 1167
train acc:  0.875
train loss:  0.2642078697681427
train gradient:  0.10718152349855399
iteration : 1168
train acc:  0.8828125
train loss:  0.3032688498497009
train gradient:  0.11678164009748933
iteration : 1169
train acc:  0.875
train loss:  0.30468082427978516
train gradient:  0.14607553912178767
iteration : 1170
train acc:  0.8671875
train loss:  0.32883763313293457
train gradient:  0.22781509862864316
iteration : 1171
train acc:  0.8984375
train loss:  0.26895827054977417
train gradient:  0.08715633634912291
iteration : 1172
train acc:  0.859375
train loss:  0.2788397967815399
train gradient:  0.09847467200614479
iteration : 1173
train acc:  0.8828125
train loss:  0.2582903504371643
train gradient:  0.10961846426296458
iteration : 1174
train acc:  0.859375
train loss:  0.3117743730545044
train gradient:  0.1305597452921488
iteration : 1175
train acc:  0.859375
train loss:  0.3098798394203186
train gradient:  0.12705696252682402
iteration : 1176
train acc:  0.859375
train loss:  0.3093182444572449
train gradient:  0.14432042309845045
iteration : 1177
train acc:  0.8984375
train loss:  0.30923184752464294
train gradient:  0.1329118779474991
iteration : 1178
train acc:  0.875
train loss:  0.3229990601539612
train gradient:  0.12277857238874224
iteration : 1179
train acc:  0.8125
train loss:  0.3983071446418762
train gradient:  0.1511698300376578
iteration : 1180
train acc:  0.8203125
train loss:  0.4373435974121094
train gradient:  0.21653642911533227
iteration : 1181
train acc:  0.859375
train loss:  0.2960417866706848
train gradient:  0.1545019206637088
iteration : 1182
train acc:  0.8984375
train loss:  0.2606744170188904
train gradient:  0.06922749084658325
iteration : 1183
train acc:  0.8671875
train loss:  0.3219076991081238
train gradient:  0.15934145035139125
iteration : 1184
train acc:  0.8671875
train loss:  0.3079811632633209
train gradient:  0.12503870009184684
iteration : 1185
train acc:  0.859375
train loss:  0.29354792833328247
train gradient:  0.1466872402919132
iteration : 1186
train acc:  0.8984375
train loss:  0.25924354791641235
train gradient:  0.07199894384349495
iteration : 1187
train acc:  0.8515625
train loss:  0.2976372241973877
train gradient:  0.45128319634758873
iteration : 1188
train acc:  0.875
train loss:  0.3597782850265503
train gradient:  0.1297927470311223
iteration : 1189
train acc:  0.8828125
train loss:  0.3196605145931244
train gradient:  0.14266545475333223
iteration : 1190
train acc:  0.8203125
train loss:  0.35952359437942505
train gradient:  0.1397513636111124
iteration : 1191
train acc:  0.9140625
train loss:  0.2296943962574005
train gradient:  0.07274370824880731
iteration : 1192
train acc:  0.828125
train loss:  0.43057525157928467
train gradient:  0.2192484838261106
iteration : 1193
train acc:  0.875
train loss:  0.31551095843315125
train gradient:  0.12889181079461287
iteration : 1194
train acc:  0.796875
train loss:  0.3611759841442108
train gradient:  0.17555764759086284
iteration : 1195
train acc:  0.84375
train loss:  0.3619937300682068
train gradient:  0.15753672420199272
iteration : 1196
train acc:  0.890625
train loss:  0.32570895552635193
train gradient:  0.14812403459184187
iteration : 1197
train acc:  0.8984375
train loss:  0.306086003780365
train gradient:  0.1809255380947314
iteration : 1198
train acc:  0.90625
train loss:  0.2610149383544922
train gradient:  0.13314935316265158
iteration : 1199
train acc:  0.8125
train loss:  0.38590189814567566
train gradient:  0.1457072422669317
iteration : 1200
train acc:  0.8828125
train loss:  0.30427464842796326
train gradient:  0.09934967387154907
iteration : 1201
train acc:  0.8671875
train loss:  0.2994444966316223
train gradient:  0.10683683570828166
iteration : 1202
train acc:  0.8984375
train loss:  0.3893689215183258
train gradient:  0.3870455954167487
iteration : 1203
train acc:  0.8828125
train loss:  0.28627023100852966
train gradient:  0.1060104331956119
iteration : 1204
train acc:  0.8671875
train loss:  0.3166394829750061
train gradient:  0.11393899380671776
iteration : 1205
train acc:  0.90625
train loss:  0.21933571994304657
train gradient:  0.053818850892382405
iteration : 1206
train acc:  0.8828125
train loss:  0.291254460811615
train gradient:  0.12261772178515337
iteration : 1207
train acc:  0.875
train loss:  0.2712595462799072
train gradient:  0.09343085116203778
iteration : 1208
train acc:  0.8515625
train loss:  0.3049556314945221
train gradient:  0.09416100668435132
iteration : 1209
train acc:  0.828125
train loss:  0.3678883910179138
train gradient:  0.15039541042732402
iteration : 1210
train acc:  0.875
train loss:  0.3218156099319458
train gradient:  0.098450435306484
iteration : 1211
train acc:  0.875
train loss:  0.2424352616071701
train gradient:  0.12704336103860464
iteration : 1212
train acc:  0.859375
train loss:  0.3313882052898407
train gradient:  0.12130121431721867
iteration : 1213
train acc:  0.8359375
train loss:  0.31307291984558105
train gradient:  0.12665645051009455
iteration : 1214
train acc:  0.8359375
train loss:  0.3541272282600403
train gradient:  0.16778521321890938
iteration : 1215
train acc:  0.890625
train loss:  0.31908729672431946
train gradient:  0.13561483034235278
iteration : 1216
train acc:  0.8828125
train loss:  0.29614588618278503
train gradient:  0.1079223327710326
iteration : 1217
train acc:  0.875
train loss:  0.278117299079895
train gradient:  0.13008934068109512
iteration : 1218
train acc:  0.8828125
train loss:  0.2869403660297394
train gradient:  0.1251560402340329
iteration : 1219
train acc:  0.875
train loss:  0.3199045658111572
train gradient:  0.11424904289505444
iteration : 1220
train acc:  0.8203125
train loss:  0.3800870180130005
train gradient:  0.17448100229646538
iteration : 1221
train acc:  0.9296875
train loss:  0.2087838053703308
train gradient:  0.1470085411178295
iteration : 1222
train acc:  0.921875
train loss:  0.29418492317199707
train gradient:  0.09745162660007686
iteration : 1223
train acc:  0.8984375
train loss:  0.2510048747062683
train gradient:  0.10401880992820842
iteration : 1224
train acc:  0.828125
train loss:  0.2988907992839813
train gradient:  0.1186733954425446
iteration : 1225
train acc:  0.859375
train loss:  0.3259471654891968
train gradient:  0.13542254678306923
iteration : 1226
train acc:  0.921875
train loss:  0.20312218368053436
train gradient:  0.06490865657013561
iteration : 1227
train acc:  0.9140625
train loss:  0.2442384958267212
train gradient:  0.11150143721399347
iteration : 1228
train acc:  0.8046875
train loss:  0.3691698908805847
train gradient:  0.1724887844590844
iteration : 1229
train acc:  0.8671875
train loss:  0.30945783853530884
train gradient:  0.13545947489517976
iteration : 1230
train acc:  0.84375
train loss:  0.2964254319667816
train gradient:  0.15382283870574875
iteration : 1231
train acc:  0.8984375
train loss:  0.24322044849395752
train gradient:  0.08303763563248051
iteration : 1232
train acc:  0.84375
train loss:  0.3031412363052368
train gradient:  0.17691759512252714
iteration : 1233
train acc:  0.890625
train loss:  0.2392648309469223
train gradient:  0.06740821248513106
iteration : 1234
train acc:  0.8359375
train loss:  0.3560032546520233
train gradient:  0.1558128721676866
iteration : 1235
train acc:  0.875
train loss:  0.2656525671482086
train gradient:  0.10977382782231318
iteration : 1236
train acc:  0.8671875
train loss:  0.2516857087612152
train gradient:  0.1075479981162168
iteration : 1237
train acc:  0.8984375
train loss:  0.250771701335907
train gradient:  0.1114959393435231
iteration : 1238
train acc:  0.84375
train loss:  0.32927316427230835
train gradient:  0.19629775107714847
iteration : 1239
train acc:  0.90625
train loss:  0.29597195982933044
train gradient:  0.10918220271461251
iteration : 1240
train acc:  0.921875
train loss:  0.27186399698257446
train gradient:  0.08675953204339971
iteration : 1241
train acc:  0.8828125
train loss:  0.2648874521255493
train gradient:  0.10655074090275439
iteration : 1242
train acc:  0.8984375
train loss:  0.29780590534210205
train gradient:  0.17678443156121193
iteration : 1243
train acc:  0.90625
train loss:  0.2432575225830078
train gradient:  0.0918959772004944
iteration : 1244
train acc:  0.84375
train loss:  0.36660730838775635
train gradient:  0.1338608053702775
iteration : 1245
train acc:  0.8984375
train loss:  0.25802546739578247
train gradient:  0.08713844439710357
iteration : 1246
train acc:  0.8515625
train loss:  0.3270784318447113
train gradient:  0.0893543867674597
iteration : 1247
train acc:  0.859375
train loss:  0.30936890840530396
train gradient:  0.14781790108208004
iteration : 1248
train acc:  0.828125
train loss:  0.2770884037017822
train gradient:  0.12082440156249079
iteration : 1249
train acc:  0.8828125
train loss:  0.28489771485328674
train gradient:  0.12270686514772723
iteration : 1250
train acc:  0.890625
train loss:  0.3155229091644287
train gradient:  0.17398515262657227
iteration : 1251
train acc:  0.890625
train loss:  0.2731889486312866
train gradient:  0.11452314203857046
iteration : 1252
train acc:  0.90625
train loss:  0.2885347008705139
train gradient:  0.10093518898563604
iteration : 1253
train acc:  0.8515625
train loss:  0.30369436740875244
train gradient:  0.11506296505815473
iteration : 1254
train acc:  0.859375
train loss:  0.3493254780769348
train gradient:  0.1318522798091703
iteration : 1255
train acc:  0.84375
train loss:  0.26908746361732483
train gradient:  0.16961335589836765
iteration : 1256
train acc:  0.875
train loss:  0.2922394573688507
train gradient:  0.10895002250601843
iteration : 1257
train acc:  0.7890625
train loss:  0.408764123916626
train gradient:  0.24396482576760917
iteration : 1258
train acc:  0.890625
train loss:  0.2604955732822418
train gradient:  0.11724006645122494
iteration : 1259
train acc:  0.8984375
train loss:  0.3050306439399719
train gradient:  0.11163548528825308
iteration : 1260
train acc:  0.859375
train loss:  0.372458815574646
train gradient:  0.16800047039128185
iteration : 1261
train acc:  0.8671875
train loss:  0.2985696792602539
train gradient:  0.17889053827150725
iteration : 1262
train acc:  0.875
train loss:  0.2510666251182556
train gradient:  0.07610405557178836
iteration : 1263
train acc:  0.8515625
train loss:  0.3254995048046112
train gradient:  0.1664627574221958
iteration : 1264
train acc:  0.875
train loss:  0.2538381516933441
train gradient:  0.06925760122574225
iteration : 1265
train acc:  0.875
train loss:  0.27011460065841675
train gradient:  0.10110153957887764
iteration : 1266
train acc:  0.8671875
train loss:  0.3241503834724426
train gradient:  0.1908097219724083
iteration : 1267
train acc:  0.8046875
train loss:  0.41774916648864746
train gradient:  0.1981170985325829
iteration : 1268
train acc:  0.8359375
train loss:  0.3066530227661133
train gradient:  0.17328680134237087
iteration : 1269
train acc:  0.8984375
train loss:  0.2571459412574768
train gradient:  0.0999548551230842
iteration : 1270
train acc:  0.90625
train loss:  0.2210533320903778
train gradient:  0.0714032295399409
iteration : 1271
train acc:  0.8828125
train loss:  0.2951599359512329
train gradient:  0.1297924220038381
iteration : 1272
train acc:  0.8515625
train loss:  0.3555578589439392
train gradient:  0.19607140609929052
iteration : 1273
train acc:  0.875
train loss:  0.27805063128471375
train gradient:  0.10202084937629628
iteration : 1274
train acc:  0.8203125
train loss:  0.40145134925842285
train gradient:  0.27133968409388926
iteration : 1275
train acc:  0.921875
train loss:  0.2430272102355957
train gradient:  0.09413978904466785
iteration : 1276
train acc:  0.890625
train loss:  0.31292349100112915
train gradient:  0.10595901109387298
iteration : 1277
train acc:  0.875
train loss:  0.34047621488571167
train gradient:  0.1671295562287738
iteration : 1278
train acc:  0.84375
train loss:  0.3808148503303528
train gradient:  0.21089459933853077
iteration : 1279
train acc:  0.9140625
train loss:  0.2250119298696518
train gradient:  0.08558947994816314
iteration : 1280
train acc:  0.8828125
train loss:  0.30618947744369507
train gradient:  0.11693410100327903
iteration : 1281
train acc:  0.8828125
train loss:  0.29946261644363403
train gradient:  0.15516114992652147
iteration : 1282
train acc:  0.8515625
train loss:  0.34768691658973694
train gradient:  0.1232603243038936
iteration : 1283
train acc:  0.875
train loss:  0.30755072832107544
train gradient:  0.10517658271217203
iteration : 1284
train acc:  0.8515625
train loss:  0.3501994013786316
train gradient:  0.13032350174539692
iteration : 1285
train acc:  0.9140625
train loss:  0.2711058557033539
train gradient:  0.09432559852883275
iteration : 1286
train acc:  0.8671875
train loss:  0.28601107001304626
train gradient:  0.10828360994775867
iteration : 1287
train acc:  0.875
train loss:  0.2887561619281769
train gradient:  0.10363386289985048
iteration : 1288
train acc:  0.8828125
train loss:  0.3095470070838928
train gradient:  0.12406772670394287
iteration : 1289
train acc:  0.8515625
train loss:  0.3447583317756653
train gradient:  0.19555014927299735
iteration : 1290
train acc:  0.890625
train loss:  0.2876003682613373
train gradient:  0.0930804972223434
iteration : 1291
train acc:  0.8984375
train loss:  0.3119630813598633
train gradient:  0.2042867888180184
iteration : 1292
train acc:  0.84375
train loss:  0.2971373200416565
train gradient:  0.1449468586503445
iteration : 1293
train acc:  0.8125
train loss:  0.38168367743492126
train gradient:  0.2009798014811261
iteration : 1294
train acc:  0.859375
train loss:  0.3361736536026001
train gradient:  0.18602595412338976
iteration : 1295
train acc:  0.8828125
train loss:  0.25390082597732544
train gradient:  0.12113183867292453
iteration : 1296
train acc:  0.8359375
train loss:  0.42755022644996643
train gradient:  0.16391121570703468
iteration : 1297
train acc:  0.859375
train loss:  0.3019638657569885
train gradient:  0.13549645784521175
iteration : 1298
train acc:  0.90625
train loss:  0.29560765624046326
train gradient:  0.08749815006986511
iteration : 1299
train acc:  0.8671875
train loss:  0.285161554813385
train gradient:  0.11477667468103638
iteration : 1300
train acc:  0.8359375
train loss:  0.3762417137622833
train gradient:  0.3012647950431595
iteration : 1301
train acc:  0.84375
train loss:  0.35006362199783325
train gradient:  0.2017656677403071
iteration : 1302
train acc:  0.78125
train loss:  0.4234897792339325
train gradient:  0.198569265157238
iteration : 1303
train acc:  0.84375
train loss:  0.373832106590271
train gradient:  0.2329547099171571
iteration : 1304
train acc:  0.8984375
train loss:  0.31360918283462524
train gradient:  0.11847585899623987
iteration : 1305
train acc:  0.875
train loss:  0.3236052393913269
train gradient:  0.14481706786081663
iteration : 1306
train acc:  0.8515625
train loss:  0.3261639177799225
train gradient:  0.1204001100721953
iteration : 1307
train acc:  0.8515625
train loss:  0.33975154161453247
train gradient:  0.20545865850892828
iteration : 1308
train acc:  0.8671875
train loss:  0.293876051902771
train gradient:  0.15973460330601452
iteration : 1309
train acc:  0.8359375
train loss:  0.3483720123767853
train gradient:  0.14532739271639095
iteration : 1310
train acc:  0.890625
train loss:  0.2482929676771164
train gradient:  0.13840678349027774
iteration : 1311
train acc:  0.8203125
train loss:  0.3620079457759857
train gradient:  0.16736270435588446
iteration : 1312
train acc:  0.875
train loss:  0.2864837944507599
train gradient:  0.10531114859058244
iteration : 1313
train acc:  0.875
train loss:  0.2554525136947632
train gradient:  0.11623086682027492
iteration : 1314
train acc:  0.890625
train loss:  0.26759642362594604
train gradient:  0.09547425516429353
iteration : 1315
train acc:  0.8359375
train loss:  0.3432321548461914
train gradient:  0.13339271070331826
iteration : 1316
train acc:  0.8828125
train loss:  0.2931380271911621
train gradient:  0.08989173470792684
iteration : 1317
train acc:  0.921875
train loss:  0.20410162210464478
train gradient:  0.10650117900923141
iteration : 1318
train acc:  0.8359375
train loss:  0.33787763118743896
train gradient:  0.1783429012040871
iteration : 1319
train acc:  0.8828125
train loss:  0.25906988978385925
train gradient:  0.10738814820607072
iteration : 1320
train acc:  0.8984375
train loss:  0.24454687535762787
train gradient:  0.1381059273543386
iteration : 1321
train acc:  0.8828125
train loss:  0.2525736391544342
train gradient:  0.12273904842841038
iteration : 1322
train acc:  0.8125
train loss:  0.4140661060810089
train gradient:  0.22906232177199093
iteration : 1323
train acc:  0.90625
train loss:  0.2047906517982483
train gradient:  0.08356034963773318
iteration : 1324
train acc:  0.8671875
train loss:  0.3141533136367798
train gradient:  0.13714564746774077
iteration : 1325
train acc:  0.8125
train loss:  0.3744736611843109
train gradient:  0.19351523230152326
iteration : 1326
train acc:  0.84375
train loss:  0.34847939014434814
train gradient:  0.16857313939700289
iteration : 1327
train acc:  0.84375
train loss:  0.312980979681015
train gradient:  0.1458998950503333
iteration : 1328
train acc:  0.8828125
train loss:  0.2863687574863434
train gradient:  0.13165302336443466
iteration : 1329
train acc:  0.8984375
train loss:  0.2833865284919739
train gradient:  0.1268454558562126
iteration : 1330
train acc:  0.890625
train loss:  0.2665064334869385
train gradient:  0.15654634814136542
iteration : 1331
train acc:  0.875
train loss:  0.2494708001613617
train gradient:  0.09470252090027012
iteration : 1332
train acc:  0.8984375
train loss:  0.253182053565979
train gradient:  0.10196820034831978
iteration : 1333
train acc:  0.8671875
train loss:  0.322229266166687
train gradient:  0.2418652622866278
iteration : 1334
train acc:  0.8671875
train loss:  0.32826468348503113
train gradient:  0.15475196210605868
iteration : 1335
train acc:  0.921875
train loss:  0.2399221658706665
train gradient:  0.09113105049188626
iteration : 1336
train acc:  0.875
train loss:  0.37843629717826843
train gradient:  0.1667318695353308
iteration : 1337
train acc:  0.8203125
train loss:  0.41700077056884766
train gradient:  0.19554363573796352
iteration : 1338
train acc:  0.8203125
train loss:  0.3571131229400635
train gradient:  0.1901110538458654
iteration : 1339
train acc:  0.8359375
train loss:  0.4071325659751892
train gradient:  0.168893108474689
iteration : 1340
train acc:  0.8203125
train loss:  0.34931644797325134
train gradient:  0.1976490287374374
iteration : 1341
train acc:  0.875
train loss:  0.31969910860061646
train gradient:  0.0989796527597738
iteration : 1342
train acc:  0.859375
train loss:  0.266814261674881
train gradient:  0.08737776755517299
iteration : 1343
train acc:  0.875
train loss:  0.2674238979816437
train gradient:  0.10821524538221115
iteration : 1344
train acc:  0.828125
train loss:  0.34851476550102234
train gradient:  0.16233939877475062
iteration : 1345
train acc:  0.8671875
train loss:  0.3193075656890869
train gradient:  0.15772753278512
iteration : 1346
train acc:  0.8515625
train loss:  0.34636300802230835
train gradient:  0.19412361712580928
iteration : 1347
train acc:  0.90625
train loss:  0.2800126075744629
train gradient:  0.131024312831329
iteration : 1348
train acc:  0.8125
train loss:  0.3840928077697754
train gradient:  0.15943670493766146
iteration : 1349
train acc:  0.890625
train loss:  0.23825247585773468
train gradient:  0.08772528097336865
iteration : 1350
train acc:  0.8515625
train loss:  0.30263882875442505
train gradient:  0.13502619881390532
iteration : 1351
train acc:  0.9296875
train loss:  0.24234908819198608
train gradient:  0.06408143657228342
iteration : 1352
train acc:  0.859375
train loss:  0.3363863229751587
train gradient:  0.17351046260822495
iteration : 1353
train acc:  0.796875
train loss:  0.4215964078903198
train gradient:  0.19559771260491093
iteration : 1354
train acc:  0.875
train loss:  0.30716121196746826
train gradient:  0.12073178738519592
iteration : 1355
train acc:  0.90625
train loss:  0.2592657208442688
train gradient:  0.09658889003826794
iteration : 1356
train acc:  0.890625
train loss:  0.3586139976978302
train gradient:  0.13247372153815246
iteration : 1357
train acc:  0.875
train loss:  0.2805372476577759
train gradient:  0.08705257117332962
iteration : 1358
train acc:  0.875
train loss:  0.29208746552467346
train gradient:  0.09848883847427564
iteration : 1359
train acc:  0.84375
train loss:  0.2955920696258545
train gradient:  0.1331279520135861
iteration : 1360
train acc:  0.9140625
train loss:  0.23717959225177765
train gradient:  0.08730385623230186
iteration : 1361
train acc:  0.921875
train loss:  0.21746939420700073
train gradient:  0.07855357339611059
iteration : 1362
train acc:  0.890625
train loss:  0.3206317126750946
train gradient:  0.15424358827297824
iteration : 1363
train acc:  0.84375
train loss:  0.31208914518356323
train gradient:  0.09320915018064792
iteration : 1364
train acc:  0.890625
train loss:  0.26196208596229553
train gradient:  0.09886604425637684
iteration : 1365
train acc:  0.8125
train loss:  0.39358678460121155
train gradient:  0.2035381696919908
iteration : 1366
train acc:  0.8203125
train loss:  0.3907303810119629
train gradient:  0.1597600308218951
iteration : 1367
train acc:  0.8984375
train loss:  0.2712821364402771
train gradient:  0.09300083498585264
iteration : 1368
train acc:  0.8671875
train loss:  0.28946956992149353
train gradient:  0.11478652047359678
iteration : 1369
train acc:  0.8828125
train loss:  0.34244588017463684
train gradient:  0.15573516735384946
iteration : 1370
train acc:  0.8359375
train loss:  0.3830610513687134
train gradient:  0.203076634641855
iteration : 1371
train acc:  0.8984375
train loss:  0.23027029633522034
train gradient:  0.08352708277511343
iteration : 1372
train acc:  0.8828125
train loss:  0.32007670402526855
train gradient:  0.14105438889573169
iteration : 1373
train acc:  0.8671875
train loss:  0.2758881747722626
train gradient:  0.09557299035298931
iteration : 1374
train acc:  0.8359375
train loss:  0.39210736751556396
train gradient:  0.18124199581138822
iteration : 1375
train acc:  0.8515625
train loss:  0.37470030784606934
train gradient:  0.1492932510268769
iteration : 1376
train acc:  0.890625
train loss:  0.2708122730255127
train gradient:  0.0929019243177385
iteration : 1377
train acc:  0.8515625
train loss:  0.3754355013370514
train gradient:  0.20983616864841126
iteration : 1378
train acc:  0.828125
train loss:  0.3533698320388794
train gradient:  0.22354618107595725
iteration : 1379
train acc:  0.84375
train loss:  0.3204246759414673
train gradient:  0.13275590651781216
iteration : 1380
train acc:  0.8515625
train loss:  0.3265807330608368
train gradient:  0.13476385173376204
iteration : 1381
train acc:  0.859375
train loss:  0.34065788984298706
train gradient:  0.12064371780895973
iteration : 1382
train acc:  0.8515625
train loss:  0.3011133372783661
train gradient:  0.11803123230836994
iteration : 1383
train acc:  0.890625
train loss:  0.2692326009273529
train gradient:  0.11034740630181691
iteration : 1384
train acc:  0.859375
train loss:  0.34090426564216614
train gradient:  0.11709700864130657
iteration : 1385
train acc:  0.90625
train loss:  0.3020751476287842
train gradient:  0.11940940685245983
iteration : 1386
train acc:  0.90625
train loss:  0.2710896134376526
train gradient:  0.11618279946550712
iteration : 1387
train acc:  0.921875
train loss:  0.24209561944007874
train gradient:  0.08714393563189429
iteration : 1388
train acc:  0.8515625
train loss:  0.316763699054718
train gradient:  0.11842272297077468
iteration : 1389
train acc:  0.859375
train loss:  0.28660207986831665
train gradient:  0.10274550742329573
iteration : 1390
train acc:  0.875
train loss:  0.2589692771434784
train gradient:  0.07906860756818461
iteration : 1391
train acc:  0.90625
train loss:  0.31429535150527954
train gradient:  0.14280668887188458
iteration : 1392
train acc:  0.8515625
train loss:  0.3086273670196533
train gradient:  0.11256108622322604
iteration : 1393
train acc:  0.875
train loss:  0.3648431897163391
train gradient:  0.13613926340289856
iteration : 1394
train acc:  0.8828125
train loss:  0.2483171969652176
train gradient:  0.0856845308333111
iteration : 1395
train acc:  0.8515625
train loss:  0.27080827951431274
train gradient:  0.11220944388492443
iteration : 1396
train acc:  0.7890625
train loss:  0.32971513271331787
train gradient:  0.12652597732691848
iteration : 1397
train acc:  0.84375
train loss:  0.3360668122768402
train gradient:  0.20571039748147987
iteration : 1398
train acc:  0.8515625
train loss:  0.34993040561676025
train gradient:  0.1636786971434249
iteration : 1399
train acc:  0.8671875
train loss:  0.29420506954193115
train gradient:  0.19133254341394124
iteration : 1400
train acc:  0.90625
train loss:  0.23932260274887085
train gradient:  0.06306106782012053
iteration : 1401
train acc:  0.8203125
train loss:  0.40735161304473877
train gradient:  0.15854741471299444
iteration : 1402
train acc:  0.8671875
train loss:  0.3569056987762451
train gradient:  0.1462083694164487
iteration : 1403
train acc:  0.90625
train loss:  0.29223373532295227
train gradient:  0.12770089331834572
iteration : 1404
train acc:  0.859375
train loss:  0.328781396150589
train gradient:  0.10363977197883349
iteration : 1405
train acc:  0.84375
train loss:  0.3749206066131592
train gradient:  0.20140227660743537
iteration : 1406
train acc:  0.890625
train loss:  0.2644593119621277
train gradient:  0.10183748240935615
iteration : 1407
train acc:  0.890625
train loss:  0.27072659134864807
train gradient:  0.09435403369173785
iteration : 1408
train acc:  0.8359375
train loss:  0.3413349986076355
train gradient:  0.13868713679864603
iteration : 1409
train acc:  0.875
train loss:  0.24089762568473816
train gradient:  0.11372897083558377
iteration : 1410
train acc:  0.9375
train loss:  0.21730628609657288
train gradient:  0.09668962935329917
iteration : 1411
train acc:  0.8984375
train loss:  0.23650309443473816
train gradient:  0.09698122145388388
iteration : 1412
train acc:  0.9140625
train loss:  0.28925982117652893
train gradient:  0.1140797319183852
iteration : 1413
train acc:  0.8671875
train loss:  0.29146844148635864
train gradient:  0.12273422825938343
iteration : 1414
train acc:  0.8359375
train loss:  0.3879847526550293
train gradient:  0.136882038463816
iteration : 1415
train acc:  0.8671875
train loss:  0.2581937313079834
train gradient:  0.0737840093512171
iteration : 1416
train acc:  0.8828125
train loss:  0.2648336589336395
train gradient:  0.10155114902503966
iteration : 1417
train acc:  0.8515625
train loss:  0.38754045963287354
train gradient:  0.23399879509245075
iteration : 1418
train acc:  0.890625
train loss:  0.2265268713235855
train gradient:  0.10998429837420783
iteration : 1419
train acc:  0.8671875
train loss:  0.2889498770236969
train gradient:  0.12384443185326141
iteration : 1420
train acc:  0.8984375
train loss:  0.2717311978340149
train gradient:  0.09896544857271013
iteration : 1421
train acc:  0.8828125
train loss:  0.3578077554702759
train gradient:  0.15512504174322148
iteration : 1422
train acc:  0.890625
train loss:  0.25179997086524963
train gradient:  0.10226882963863249
iteration : 1423
train acc:  0.90625
train loss:  0.22459883987903595
train gradient:  0.0722296654527121
iteration : 1424
train acc:  0.8515625
train loss:  0.3138587772846222
train gradient:  0.1434315776769854
iteration : 1425
train acc:  0.84375
train loss:  0.3390491008758545
train gradient:  0.16746741097015863
iteration : 1426
train acc:  0.828125
train loss:  0.3764292597770691
train gradient:  0.18514305777386114
iteration : 1427
train acc:  0.859375
train loss:  0.34627842903137207
train gradient:  0.2404888385509688
iteration : 1428
train acc:  0.8359375
train loss:  0.38978642225265503
train gradient:  0.20314630780493528
iteration : 1429
train acc:  0.796875
train loss:  0.45778554677963257
train gradient:  0.19136683283822795
iteration : 1430
train acc:  0.8671875
train loss:  0.2702888548374176
train gradient:  0.12538213879455262
iteration : 1431
train acc:  0.8125
train loss:  0.3945101499557495
train gradient:  0.15611857299590015
iteration : 1432
train acc:  0.8515625
train loss:  0.3226959705352783
train gradient:  0.13850574885540012
iteration : 1433
train acc:  0.8828125
train loss:  0.24614271521568298
train gradient:  0.09336451640616393
iteration : 1434
train acc:  0.7890625
train loss:  0.4746443033218384
train gradient:  0.284834654618116
iteration : 1435
train acc:  0.90625
train loss:  0.19931864738464355
train gradient:  0.08916677184255498
iteration : 1436
train acc:  0.8671875
train loss:  0.33872756361961365
train gradient:  0.15527900709379078
iteration : 1437
train acc:  0.8984375
train loss:  0.27750277519226074
train gradient:  0.12031988812076752
iteration : 1438
train acc:  0.875
train loss:  0.29741281270980835
train gradient:  0.11127050753918667
iteration : 1439
train acc:  0.921875
train loss:  0.2663934826850891
train gradient:  0.09199532380950683
iteration : 1440
train acc:  0.859375
train loss:  0.35054725408554077
train gradient:  0.14211487252183586
iteration : 1441
train acc:  0.859375
train loss:  0.3229650855064392
train gradient:  0.1061539213529839
iteration : 1442
train acc:  0.8359375
train loss:  0.35672566294670105
train gradient:  0.12826772949988322
iteration : 1443
train acc:  0.859375
train loss:  0.2971700429916382
train gradient:  0.15855031557258825
iteration : 1444
train acc:  0.7890625
train loss:  0.4079563617706299
train gradient:  0.14983185415255135
iteration : 1445
train acc:  0.8671875
train loss:  0.30439722537994385
train gradient:  0.16038081249345482
iteration : 1446
train acc:  0.8984375
train loss:  0.2821834683418274
train gradient:  0.11763739233361199
iteration : 1447
train acc:  0.8984375
train loss:  0.2860042452812195
train gradient:  0.12493294774983611
iteration : 1448
train acc:  0.8515625
train loss:  0.27948492765426636
train gradient:  0.11539368401152084
iteration : 1449
train acc:  0.8515625
train loss:  0.31255948543548584
train gradient:  0.10264303739284686
iteration : 1450
train acc:  0.875
train loss:  0.29990458488464355
train gradient:  0.1442835322914243
iteration : 1451
train acc:  0.8359375
train loss:  0.3272843658924103
train gradient:  0.1752892839343337
iteration : 1452
train acc:  0.8515625
train loss:  0.3029549717903137
train gradient:  0.2278532604217844
iteration : 1453
train acc:  0.828125
train loss:  0.3452760577201843
train gradient:  0.16893296740041058
iteration : 1454
train acc:  0.8828125
train loss:  0.30603355169296265
train gradient:  0.11211060738960675
iteration : 1455
train acc:  0.859375
train loss:  0.271949827671051
train gradient:  0.10346862680702501
iteration : 1456
train acc:  0.8203125
train loss:  0.44865381717681885
train gradient:  0.3038116232096821
iteration : 1457
train acc:  0.8515625
train loss:  0.32914459705352783
train gradient:  0.15988544074568567
iteration : 1458
train acc:  0.84375
train loss:  0.30056336522102356
train gradient:  0.12606425954176959
iteration : 1459
train acc:  0.890625
train loss:  0.24038460850715637
train gradient:  0.08348976066464253
iteration : 1460
train acc:  0.8515625
train loss:  0.3240478038787842
train gradient:  0.12867353178629642
iteration : 1461
train acc:  0.90625
train loss:  0.2375987023115158
train gradient:  0.08490540136730891
iteration : 1462
train acc:  0.8984375
train loss:  0.2731781601905823
train gradient:  0.16115881503218737
iteration : 1463
train acc:  0.8359375
train loss:  0.3607502579689026
train gradient:  0.17459179888757825
iteration : 1464
train acc:  0.8828125
train loss:  0.27323058247566223
train gradient:  0.08935645134427382
iteration : 1465
train acc:  0.890625
train loss:  0.22747230529785156
train gradient:  0.07598807792947389
iteration : 1466
train acc:  0.859375
train loss:  0.4040084481239319
train gradient:  0.19021190696529092
iteration : 1467
train acc:  0.8984375
train loss:  0.3096247911453247
train gradient:  0.09174723326940441
iteration : 1468
train acc:  0.859375
train loss:  0.3590317964553833
train gradient:  0.17555187638263886
iteration : 1469
train acc:  0.859375
train loss:  0.31021642684936523
train gradient:  0.10951045819506877
iteration : 1470
train acc:  0.859375
train loss:  0.3896477222442627
train gradient:  0.16687786875368804
iteration : 1471
train acc:  0.8515625
train loss:  0.3412057161331177
train gradient:  0.190978778940608
iteration : 1472
train acc:  0.890625
train loss:  0.23443040251731873
train gradient:  0.08746115752238474
iteration : 1473
train acc:  0.90625
train loss:  0.27452749013900757
train gradient:  0.09645925459719654
iteration : 1474
train acc:  0.875
train loss:  0.3114957809448242
train gradient:  0.17263713090927776
iteration : 1475
train acc:  0.8828125
train loss:  0.31619590520858765
train gradient:  0.10065869586933254
iteration : 1476
train acc:  0.8671875
train loss:  0.25049513578414917
train gradient:  0.11385344883762459
iteration : 1477
train acc:  0.8828125
train loss:  0.2946312725543976
train gradient:  0.12428110979833307
iteration : 1478
train acc:  0.921875
train loss:  0.23683802783489227
train gradient:  0.1251415974461903
iteration : 1479
train acc:  0.8671875
train loss:  0.3066490590572357
train gradient:  0.09896111150304791
iteration : 1480
train acc:  0.8984375
train loss:  0.24121010303497314
train gradient:  0.0630223973659748
iteration : 1481
train acc:  0.875
train loss:  0.2825901508331299
train gradient:  0.10425796532373796
iteration : 1482
train acc:  0.8515625
train loss:  0.3377390503883362
train gradient:  0.11314945239479433
iteration : 1483
train acc:  0.875
train loss:  0.29029592871665955
train gradient:  0.1206118345179704
iteration : 1484
train acc:  0.859375
train loss:  0.28930002450942993
train gradient:  0.13140030657592572
iteration : 1485
train acc:  0.796875
train loss:  0.4137671887874603
train gradient:  0.20266188159777776
iteration : 1486
train acc:  0.8984375
train loss:  0.2599495053291321
train gradient:  0.09951591448896568
iteration : 1487
train acc:  0.8515625
train loss:  0.40444809198379517
train gradient:  0.20389919023239944
iteration : 1488
train acc:  0.8359375
train loss:  0.3422965705394745
train gradient:  0.134615434940282
iteration : 1489
train acc:  0.84375
train loss:  0.3061564564704895
train gradient:  0.11407110334332479
iteration : 1490
train acc:  0.8671875
train loss:  0.34146857261657715
train gradient:  0.13062722359466114
iteration : 1491
train acc:  0.875
train loss:  0.30741962790489197
train gradient:  0.13485841765126294
iteration : 1492
train acc:  0.8671875
train loss:  0.3048627972602844
train gradient:  0.14887675722207883
iteration : 1493
train acc:  0.8984375
train loss:  0.22415143251419067
train gradient:  0.09867691841323975
iteration : 1494
train acc:  0.859375
train loss:  0.29562103748321533
train gradient:  0.1275132334217569
iteration : 1495
train acc:  0.8828125
train loss:  0.271672785282135
train gradient:  0.12481170586929996
iteration : 1496
train acc:  0.8359375
train loss:  0.32926610112190247
train gradient:  0.16252338145038642
iteration : 1497
train acc:  0.859375
train loss:  0.28853940963745117
train gradient:  0.13532984711819424
iteration : 1498
train acc:  0.875
train loss:  0.2834996283054352
train gradient:  0.26102831410169824
iteration : 1499
train acc:  0.8671875
train loss:  0.29225027561187744
train gradient:  0.09764059618000356
iteration : 1500
train acc:  0.921875
train loss:  0.22711291909217834
train gradient:  0.08082709404777265
iteration : 1501
train acc:  0.8828125
train loss:  0.27884620428085327
train gradient:  0.13748314764858852
iteration : 1502
train acc:  0.8359375
train loss:  0.33218157291412354
train gradient:  0.15931393193001425
iteration : 1503
train acc:  0.9140625
train loss:  0.25307583808898926
train gradient:  0.15692635385603865
iteration : 1504
train acc:  0.8828125
train loss:  0.2926836907863617
train gradient:  0.10838044526806953
iteration : 1505
train acc:  0.90625
train loss:  0.24830366671085358
train gradient:  0.07839073838817326
iteration : 1506
train acc:  0.8203125
train loss:  0.3687184453010559
train gradient:  0.1650221922756663
iteration : 1507
train acc:  0.8515625
train loss:  0.34830090403556824
train gradient:  0.1423935946379271
iteration : 1508
train acc:  0.8671875
train loss:  0.3584679961204529
train gradient:  0.15559002977649916
iteration : 1509
train acc:  0.84375
train loss:  0.33607611060142517
train gradient:  0.18722807927435892
iteration : 1510
train acc:  0.9140625
train loss:  0.2877650260925293
train gradient:  0.13397218218078286
iteration : 1511
train acc:  0.8984375
train loss:  0.24252784252166748
train gradient:  0.07298834041627039
iteration : 1512
train acc:  0.90625
train loss:  0.3103969097137451
train gradient:  0.11202463870124874
iteration : 1513
train acc:  0.90625
train loss:  0.2489280104637146
train gradient:  0.06741381473244676
iteration : 1514
train acc:  0.828125
train loss:  0.38504207134246826
train gradient:  0.19197011295414523
iteration : 1515
train acc:  0.8671875
train loss:  0.2955649495124817
train gradient:  0.1288451739084962
iteration : 1516
train acc:  0.875
train loss:  0.2625756859779358
train gradient:  0.09735524749892219
iteration : 1517
train acc:  0.796875
train loss:  0.38605600595474243
train gradient:  0.16964499692691049
iteration : 1518
train acc:  0.8984375
train loss:  0.26264688372612
train gradient:  0.09821812553690483
iteration : 1519
train acc:  0.875
train loss:  0.33625099062919617
train gradient:  0.14247357830175383
iteration : 1520
train acc:  0.8359375
train loss:  0.35658788681030273
train gradient:  0.17322880294778087
iteration : 1521
train acc:  0.9140625
train loss:  0.2231784164905548
train gradient:  0.09511013315550262
iteration : 1522
train acc:  0.890625
train loss:  0.31602808833122253
train gradient:  0.1485419447423007
iteration : 1523
train acc:  0.890625
train loss:  0.2901840806007385
train gradient:  0.09053367700923733
iteration : 1524
train acc:  0.8671875
train loss:  0.3101663589477539
train gradient:  0.11121004320311005
iteration : 1525
train acc:  0.859375
train loss:  0.32137197256088257
train gradient:  0.10397299434204205
iteration : 1526
train acc:  0.8515625
train loss:  0.31513383984565735
train gradient:  0.1466662414280525
iteration : 1527
train acc:  0.890625
train loss:  0.2400997281074524
train gradient:  0.11159577901823235
iteration : 1528
train acc:  0.84375
train loss:  0.31201115250587463
train gradient:  0.10783635163010392
iteration : 1529
train acc:  0.8671875
train loss:  0.328605055809021
train gradient:  0.14680432945778643
iteration : 1530
train acc:  0.8984375
train loss:  0.27437111735343933
train gradient:  0.07110251032518053
iteration : 1531
train acc:  0.8515625
train loss:  0.3625423312187195
train gradient:  0.12112296475392313
iteration : 1532
train acc:  0.8203125
train loss:  0.42795702815055847
train gradient:  0.26192130524343654
iteration : 1533
train acc:  0.8515625
train loss:  0.3385109305381775
train gradient:  0.1270072195155577
iteration : 1534
train acc:  0.8671875
train loss:  0.2811242341995239
train gradient:  0.10639975508653467
iteration : 1535
train acc:  0.8203125
train loss:  0.3683508634567261
train gradient:  0.18247198424289468
iteration : 1536
train acc:  0.8671875
train loss:  0.2779616117477417
train gradient:  0.12618551293448915
iteration : 1537
train acc:  0.890625
train loss:  0.2671932876110077
train gradient:  0.10127444646708525
iteration : 1538
train acc:  0.8203125
train loss:  0.43096625804901123
train gradient:  0.1825701945290812
iteration : 1539
train acc:  0.8515625
train loss:  0.29837819933891296
train gradient:  0.1752137575182519
iteration : 1540
train acc:  0.8828125
train loss:  0.2761632204055786
train gradient:  0.14289033722652517
iteration : 1541
train acc:  0.890625
train loss:  0.3029991388320923
train gradient:  0.11718546887181386
iteration : 1542
train acc:  0.8515625
train loss:  0.3756749927997589
train gradient:  0.17947005926349394
iteration : 1543
train acc:  0.9453125
train loss:  0.20135551691055298
train gradient:  0.07969636507442275
iteration : 1544
train acc:  0.875
train loss:  0.3153289556503296
train gradient:  0.1469464022820261
iteration : 1545
train acc:  0.8984375
train loss:  0.3067689538002014
train gradient:  0.11477137142581888
iteration : 1546
train acc:  0.796875
train loss:  0.3743836581707001
train gradient:  0.14824070945521894
iteration : 1547
train acc:  0.8515625
train loss:  0.3221716582775116
train gradient:  0.10635394065730255
iteration : 1548
train acc:  0.828125
train loss:  0.4094109535217285
train gradient:  0.14994484623307686
iteration : 1549
train acc:  0.9140625
train loss:  0.2190140187740326
train gradient:  0.08951381802215393
iteration : 1550
train acc:  0.84375
train loss:  0.3437469005584717
train gradient:  0.14096840711576175
iteration : 1551
train acc:  0.9140625
train loss:  0.29544776678085327
train gradient:  0.10025769007967228
iteration : 1552
train acc:  0.921875
train loss:  0.20558777451515198
train gradient:  0.06908309194503948
iteration : 1553
train acc:  0.8984375
train loss:  0.30637890100479126
train gradient:  0.08866509404378142
iteration : 1554
train acc:  0.8515625
train loss:  0.27920806407928467
train gradient:  0.1142615250514085
iteration : 1555
train acc:  0.890625
train loss:  0.2960524559020996
train gradient:  0.10427374675079001
iteration : 1556
train acc:  0.875
train loss:  0.3568403720855713
train gradient:  0.13167607332022307
iteration : 1557
train acc:  0.8828125
train loss:  0.3376935124397278
train gradient:  0.13365458238636224
iteration : 1558
train acc:  0.859375
train loss:  0.3882853090763092
train gradient:  0.3727364728183424
iteration : 1559
train acc:  0.90625
train loss:  0.27549248933792114
train gradient:  0.09810399893722938
iteration : 1560
train acc:  0.828125
train loss:  0.3882710933685303
train gradient:  0.14707893348599382
iteration : 1561
train acc:  0.8984375
train loss:  0.2731223702430725
train gradient:  0.11261052627907496
iteration : 1562
train acc:  0.8671875
train loss:  0.300657719373703
train gradient:  0.12796007951235816
iteration : 1563
train acc:  0.921875
train loss:  0.20156338810920715
train gradient:  0.06970198862544125
iteration : 1564
train acc:  0.8828125
train loss:  0.2837963104248047
train gradient:  0.10061243075751598
iteration : 1565
train acc:  0.8671875
train loss:  0.28533267974853516
train gradient:  0.16261056900679285
iteration : 1566
train acc:  0.828125
train loss:  0.34883761405944824
train gradient:  0.17105986926398853
iteration : 1567
train acc:  0.8359375
train loss:  0.42390403151512146
train gradient:  0.20544884053979073
iteration : 1568
train acc:  0.84375
train loss:  0.3065260946750641
train gradient:  0.10957756709236986
iteration : 1569
train acc:  0.8828125
train loss:  0.31428301334381104
train gradient:  0.11840914194213409
iteration : 1570
train acc:  0.828125
train loss:  0.39354148507118225
train gradient:  0.20928522382856218
iteration : 1571
train acc:  0.8984375
train loss:  0.2993866205215454
train gradient:  0.10050267598772616
iteration : 1572
train acc:  0.90625
train loss:  0.2692725658416748
train gradient:  0.0982667497756571
iteration : 1573
train acc:  0.859375
train loss:  0.380624383687973
train gradient:  0.1393509017767444
iteration : 1574
train acc:  0.84375
train loss:  0.37781721353530884
train gradient:  0.11902265612565727
iteration : 1575
train acc:  0.8984375
train loss:  0.26378244161605835
train gradient:  0.09415656491041274
iteration : 1576
train acc:  0.8984375
train loss:  0.2570358216762543
train gradient:  0.09210051623942356
iteration : 1577
train acc:  0.8828125
train loss:  0.2591763138771057
train gradient:  0.0989201390908905
iteration : 1578
train acc:  0.8515625
train loss:  0.30462366342544556
train gradient:  0.10145139525645462
iteration : 1579
train acc:  0.828125
train loss:  0.3702411651611328
train gradient:  0.12571995902677313
iteration : 1580
train acc:  0.890625
train loss:  0.28115546703338623
train gradient:  0.10257440515333094
iteration : 1581
train acc:  0.875
train loss:  0.31223127245903015
train gradient:  0.08455750965406748
iteration : 1582
train acc:  0.8359375
train loss:  0.30841830372810364
train gradient:  0.15583725837535067
iteration : 1583
train acc:  0.9453125
train loss:  0.19266466796398163
train gradient:  0.07674048197447157
iteration : 1584
train acc:  0.8828125
train loss:  0.2879416048526764
train gradient:  0.13439385236479356
iteration : 1585
train acc:  0.8515625
train loss:  0.33223819732666016
train gradient:  0.12330689356966354
iteration : 1586
train acc:  0.8671875
train loss:  0.3530401289463043
train gradient:  0.12237481015449508
iteration : 1587
train acc:  0.9140625
train loss:  0.3120778799057007
train gradient:  0.11495474143704823
iteration : 1588
train acc:  0.90625
train loss:  0.2991328537464142
train gradient:  0.11364287317854106
iteration : 1589
train acc:  0.8984375
train loss:  0.24739429354667664
train gradient:  0.06894811799053067
iteration : 1590
train acc:  0.8984375
train loss:  0.2511367201805115
train gradient:  0.08045298145844403
iteration : 1591
train acc:  0.8828125
train loss:  0.26143503189086914
train gradient:  0.10277612524753645
iteration : 1592
train acc:  0.84375
train loss:  0.3550351560115814
train gradient:  0.11535678664399392
iteration : 1593
train acc:  0.8828125
train loss:  0.2770085334777832
train gradient:  0.1031867686986475
iteration : 1594
train acc:  0.890625
train loss:  0.25643283128738403
train gradient:  0.1315294979411209
iteration : 1595
train acc:  0.875
train loss:  0.3589451313018799
train gradient:  0.12608290156838703
iteration : 1596
train acc:  0.9140625
train loss:  0.2583796977996826
train gradient:  0.09795055176917293
iteration : 1597
train acc:  0.859375
train loss:  0.3283948600292206
train gradient:  0.12093970766267975
iteration : 1598
train acc:  0.875
train loss:  0.2830200791358948
train gradient:  0.09375320351896077
iteration : 1599
train acc:  0.8671875
train loss:  0.3146669864654541
train gradient:  0.12108165217082852
iteration : 1600
train acc:  0.875
train loss:  0.2976916432380676
train gradient:  0.11275705657231506
iteration : 1601
train acc:  0.890625
train loss:  0.2826465964317322
train gradient:  0.09292313627221324
iteration : 1602
train acc:  0.84375
train loss:  0.34660202264785767
train gradient:  0.11704615814362682
iteration : 1603
train acc:  0.8984375
train loss:  0.2464977353811264
train gradient:  0.08972442087616056
iteration : 1604
train acc:  0.859375
train loss:  0.31171712279319763
train gradient:  0.11161533347490694
iteration : 1605
train acc:  0.8515625
train loss:  0.28396299481391907
train gradient:  0.08708704058316892
iteration : 1606
train acc:  0.8671875
train loss:  0.278009295463562
train gradient:  0.108332330074896
iteration : 1607
train acc:  0.90625
train loss:  0.26457729935646057
train gradient:  0.17847615845682963
iteration : 1608
train acc:  0.8671875
train loss:  0.3171185255050659
train gradient:  0.11640721458456771
iteration : 1609
train acc:  0.8828125
train loss:  0.3138543665409088
train gradient:  0.08557089542564206
iteration : 1610
train acc:  0.8671875
train loss:  0.3047456741333008
train gradient:  0.11923011223388733
iteration : 1611
train acc:  0.8515625
train loss:  0.33832430839538574
train gradient:  0.114635668491919
iteration : 1612
train acc:  0.8515625
train loss:  0.3528677225112915
train gradient:  0.1329497522450685
iteration : 1613
train acc:  0.828125
train loss:  0.4486999809741974
train gradient:  0.2703788099459657
iteration : 1614
train acc:  0.859375
train loss:  0.29661819338798523
train gradient:  0.11118249186421106
iteration : 1615
train acc:  0.8515625
train loss:  0.3094577193260193
train gradient:  0.12246189026016881
iteration : 1616
train acc:  0.8671875
train loss:  0.2819508910179138
train gradient:  0.0770987982126637
iteration : 1617
train acc:  0.8671875
train loss:  0.3087575137615204
train gradient:  0.11651993237786348
iteration : 1618
train acc:  0.875
train loss:  0.3633730411529541
train gradient:  0.11671555217941817
iteration : 1619
train acc:  0.875
train loss:  0.33253809809684753
train gradient:  0.14954478948175998
iteration : 1620
train acc:  0.828125
train loss:  0.3002108931541443
train gradient:  0.11832253187879128
iteration : 1621
train acc:  0.8828125
train loss:  0.27428150177001953
train gradient:  0.10559598718095303
iteration : 1622
train acc:  0.875
train loss:  0.283247172832489
train gradient:  0.1282674690366621
iteration : 1623
train acc:  0.875
train loss:  0.30015891790390015
train gradient:  0.1321786293290276
iteration : 1624
train acc:  0.8828125
train loss:  0.24653932452201843
train gradient:  0.11073884933050464
iteration : 1625
train acc:  0.8828125
train loss:  0.293215811252594
train gradient:  0.0893841926096148
iteration : 1626
train acc:  0.8671875
train loss:  0.3230283558368683
train gradient:  0.12402682746336188
iteration : 1627
train acc:  0.8671875
train loss:  0.3088420629501343
train gradient:  0.11097150134268698
iteration : 1628
train acc:  0.8671875
train loss:  0.30070000886917114
train gradient:  0.11057692904895512
iteration : 1629
train acc:  0.8359375
train loss:  0.34201133251190186
train gradient:  0.11539259351126574
iteration : 1630
train acc:  0.8515625
train loss:  0.3564351201057434
train gradient:  0.12377329963039731
iteration : 1631
train acc:  0.8515625
train loss:  0.2828451693058014
train gradient:  0.10623122231871018
iteration : 1632
train acc:  0.890625
train loss:  0.28902918100357056
train gradient:  0.10486695788359039
iteration : 1633
train acc:  0.8203125
train loss:  0.3703324794769287
train gradient:  0.1357209191419314
iteration : 1634
train acc:  0.890625
train loss:  0.2884378731250763
train gradient:  0.1118196089745753
iteration : 1635
train acc:  0.8828125
train loss:  0.34231001138687134
train gradient:  0.11823249986647029
iteration : 1636
train acc:  0.8515625
train loss:  0.33523279428482056
train gradient:  0.19624768889584968
iteration : 1637
train acc:  0.84375
train loss:  0.3356158137321472
train gradient:  0.15431752099925475
iteration : 1638
train acc:  0.828125
train loss:  0.3599807024002075
train gradient:  0.15284037368845033
iteration : 1639
train acc:  0.8515625
train loss:  0.2810778021812439
train gradient:  0.11911000995344143
iteration : 1640
train acc:  0.8203125
train loss:  0.36882704496383667
train gradient:  0.16364471158469074
iteration : 1641
train acc:  0.890625
train loss:  0.27502185106277466
train gradient:  0.10146916311026458
iteration : 1642
train acc:  0.8203125
train loss:  0.3655780553817749
train gradient:  0.20201433754218728
iteration : 1643
train acc:  0.84375
train loss:  0.29884523153305054
train gradient:  0.12650978776854133
iteration : 1644
train acc:  0.8671875
train loss:  0.2837710380554199
train gradient:  0.16919303538717959
iteration : 1645
train acc:  0.8203125
train loss:  0.34712421894073486
train gradient:  0.14444250792609056
iteration : 1646
train acc:  0.8515625
train loss:  0.2757452130317688
train gradient:  0.11088031072458936
iteration : 1647
train acc:  0.8671875
train loss:  0.3390527367591858
train gradient:  0.15568028812111567
iteration : 1648
train acc:  0.875
train loss:  0.3008478879928589
train gradient:  0.1504524341025446
iteration : 1649
train acc:  0.8515625
train loss:  0.30484235286712646
train gradient:  0.10255075887159439
iteration : 1650
train acc:  0.8359375
train loss:  0.33596035838127136
train gradient:  0.14247748082359624
iteration : 1651
train acc:  0.8828125
train loss:  0.3067627549171448
train gradient:  0.13874972172916772
iteration : 1652
train acc:  0.8515625
train loss:  0.2936800718307495
train gradient:  0.11560996232228758
iteration : 1653
train acc:  0.875
train loss:  0.3208218514919281
train gradient:  0.091624663520013
iteration : 1654
train acc:  0.84375
train loss:  0.3778038024902344
train gradient:  0.18542074638582598
iteration : 1655
train acc:  0.796875
train loss:  0.35016125440597534
train gradient:  0.2748314177986932
iteration : 1656
train acc:  0.890625
train loss:  0.2701517939567566
train gradient:  0.10151391968407432
iteration : 1657
train acc:  0.859375
train loss:  0.3092663884162903
train gradient:  0.11156463840822198
iteration : 1658
train acc:  0.8828125
train loss:  0.288430780172348
train gradient:  0.22462700614142303
iteration : 1659
train acc:  0.8515625
train loss:  0.32728445529937744
train gradient:  0.159085367769843
iteration : 1660
train acc:  0.8984375
train loss:  0.2588998079299927
train gradient:  0.10009670825405144
iteration : 1661
train acc:  0.859375
train loss:  0.25288742780685425
train gradient:  0.102656499294547
iteration : 1662
train acc:  0.90625
train loss:  0.27245649695396423
train gradient:  0.1131481720048722
iteration : 1663
train acc:  0.875
train loss:  0.3041350245475769
train gradient:  0.11619877257688152
iteration : 1664
train acc:  0.8515625
train loss:  0.32927918434143066
train gradient:  0.20351932364133707
iteration : 1665
train acc:  0.828125
train loss:  0.31903281807899475
train gradient:  0.11103190665351621
iteration : 1666
train acc:  0.8671875
train loss:  0.3562828600406647
train gradient:  0.15992366368649788
iteration : 1667
train acc:  0.875
train loss:  0.23315489292144775
train gradient:  0.0797663318336215
iteration : 1668
train acc:  0.875
train loss:  0.28783756494522095
train gradient:  0.10074878887112496
iteration : 1669
train acc:  0.8671875
train loss:  0.38101503252983093
train gradient:  0.1689780329008643
iteration : 1670
train acc:  0.765625
train loss:  0.41790634393692017
train gradient:  0.1802270804028816
iteration : 1671
train acc:  0.8828125
train loss:  0.33382317423820496
train gradient:  0.1373190608510507
iteration : 1672
train acc:  0.8671875
train loss:  0.30019620060920715
train gradient:  0.10928833352567571
iteration : 1673
train acc:  0.8515625
train loss:  0.32376736402511597
train gradient:  0.10989637598181179
iteration : 1674
train acc:  0.8515625
train loss:  0.3567054867744446
train gradient:  0.18144001458549447
iteration : 1675
train acc:  0.8046875
train loss:  0.38192808628082275
train gradient:  0.13540926641011206
iteration : 1676
train acc:  0.84375
train loss:  0.35129404067993164
train gradient:  0.16108625754218295
iteration : 1677
train acc:  0.8515625
train loss:  0.31597399711608887
train gradient:  0.14703269968687754
iteration : 1678
train acc:  0.875
train loss:  0.30163419246673584
train gradient:  0.15968604625084237
iteration : 1679
train acc:  0.8359375
train loss:  0.3331013023853302
train gradient:  0.10890971999693962
iteration : 1680
train acc:  0.8671875
train loss:  0.29175853729248047
train gradient:  0.22242488302717112
iteration : 1681
train acc:  0.8984375
train loss:  0.22222188115119934
train gradient:  0.06534532310050518
iteration : 1682
train acc:  0.890625
train loss:  0.30757564306259155
train gradient:  0.09418780310008149
iteration : 1683
train acc:  0.9140625
train loss:  0.24090257287025452
train gradient:  0.10053579765407206
iteration : 1684
train acc:  0.8515625
train loss:  0.31274452805519104
train gradient:  0.14147423898042738
iteration : 1685
train acc:  0.84375
train loss:  0.3050403296947479
train gradient:  0.14511638409850153
iteration : 1686
train acc:  0.84375
train loss:  0.3244647979736328
train gradient:  0.1395796010250842
iteration : 1687
train acc:  0.8671875
train loss:  0.28717041015625
train gradient:  0.11028596997898196
iteration : 1688
train acc:  0.90625
train loss:  0.2716967463493347
train gradient:  0.11654216592278858
iteration : 1689
train acc:  0.84375
train loss:  0.28603383898735046
train gradient:  0.10676434293042011
iteration : 1690
train acc:  0.9140625
train loss:  0.22751322388648987
train gradient:  0.11282540092638586
iteration : 1691
train acc:  0.859375
train loss:  0.3074342608451843
train gradient:  0.12442714856977283
iteration : 1692
train acc:  0.84375
train loss:  0.39019399881362915
train gradient:  0.16050006266237485
iteration : 1693
train acc:  0.90625
train loss:  0.2838048040866852
train gradient:  0.09479010812021153
iteration : 1694
train acc:  0.8828125
train loss:  0.3076908588409424
train gradient:  0.1688498135744267
iteration : 1695
train acc:  0.875
train loss:  0.34516751766204834
train gradient:  0.13871212350358542
iteration : 1696
train acc:  0.7890625
train loss:  0.34650298953056335
train gradient:  0.17656574455990498
iteration : 1697
train acc:  0.875
train loss:  0.32560622692108154
train gradient:  0.1156311057265944
iteration : 1698
train acc:  0.8203125
train loss:  0.4047359824180603
train gradient:  0.17342920895476047
iteration : 1699
train acc:  0.875
train loss:  0.3229817748069763
train gradient:  0.10905074254824516
iteration : 1700
train acc:  0.8828125
train loss:  0.22555682063102722
train gradient:  0.07916544390550075
iteration : 1701
train acc:  0.8828125
train loss:  0.24212411046028137
train gradient:  0.08219958398775946
iteration : 1702
train acc:  0.9140625
train loss:  0.2504369020462036
train gradient:  0.07243949906603818
iteration : 1703
train acc:  0.8828125
train loss:  0.26622921228408813
train gradient:  0.10253447659107844
iteration : 1704
train acc:  0.8359375
train loss:  0.2961018681526184
train gradient:  0.08457792292923996
iteration : 1705
train acc:  0.890625
train loss:  0.23481833934783936
train gradient:  0.08523114158241933
iteration : 1706
train acc:  0.859375
train loss:  0.30107468366622925
train gradient:  0.08667750261688571
iteration : 1707
train acc:  0.8671875
train loss:  0.34201037883758545
train gradient:  0.14838226846998698
iteration : 1708
train acc:  0.8046875
train loss:  0.45057836174964905
train gradient:  0.2324857282567815
iteration : 1709
train acc:  0.890625
train loss:  0.3117998242378235
train gradient:  0.19488841756069522
iteration : 1710
train acc:  0.8359375
train loss:  0.35743510723114014
train gradient:  0.15575291852283632
iteration : 1711
train acc:  0.859375
train loss:  0.3042491674423218
train gradient:  0.09918148671471358
iteration : 1712
train acc:  0.8203125
train loss:  0.35847923159599304
train gradient:  0.15008754478596392
iteration : 1713
train acc:  0.8671875
train loss:  0.3222931921482086
train gradient:  0.13753180487388217
iteration : 1714
train acc:  0.8359375
train loss:  0.32886064052581787
train gradient:  0.14960270794892844
iteration : 1715
train acc:  0.8203125
train loss:  0.3711138367652893
train gradient:  0.1784287951523934
iteration : 1716
train acc:  0.84375
train loss:  0.33388230204582214
train gradient:  0.1590526275500141
iteration : 1717
train acc:  0.84375
train loss:  0.3763279914855957
train gradient:  0.18068800277749086
iteration : 1718
train acc:  0.828125
train loss:  0.35125023126602173
train gradient:  0.12135822130609683
iteration : 1719
train acc:  0.8515625
train loss:  0.3066178858280182
train gradient:  0.14697161096520003
iteration : 1720
train acc:  0.8828125
train loss:  0.2640596628189087
