program start:
num_rounds= 0
node_emb_dim= 16

----------------------------------------new_epoch--------------------------------------

epoch:  0
iteration : 0
train acc:  0.546875
train loss:  0.7134292721748352
train gradient:  0.3486649318699063
iteration : 1
train acc:  0.53125
train loss:  0.7145648002624512
train gradient:  0.43626944713807986
iteration : 2
train acc:  0.484375
train loss:  0.7657283544540405
train gradient:  0.609273825760789
iteration : 3
train acc:  0.4921875
train loss:  0.7579123973846436
train gradient:  0.628924789113988
iteration : 4
train acc:  0.546875
train loss:  0.6782066822052002
train gradient:  0.35066021956594473
iteration : 5
train acc:  0.5390625
train loss:  0.7069507241249084
train gradient:  0.43230750650313055
iteration : 6
train acc:  0.5625
train loss:  0.7024819850921631
train gradient:  0.555178249088911
iteration : 7
train acc:  0.5859375
train loss:  0.6634088158607483
train gradient:  0.2905524404405863
iteration : 8
train acc:  0.59375
train loss:  0.6960372924804688
train gradient:  0.35805068669321727
iteration : 9
train acc:  0.5625
train loss:  0.6997689008712769
train gradient:  0.28449651542156
iteration : 10
train acc:  0.5703125
train loss:  0.6682376861572266
train gradient:  0.3998744144922801
iteration : 11
train acc:  0.6328125
train loss:  0.640900731086731
train gradient:  0.20192253021285322
iteration : 12
train acc:  0.5625
train loss:  0.6991695761680603
train gradient:  0.22799902714456688
iteration : 13
train acc:  0.5859375
train loss:  0.6836869120597839
train gradient:  0.2313130874989631
iteration : 14
train acc:  0.546875
train loss:  0.6866698265075684
train gradient:  0.20565678824638503
iteration : 15
train acc:  0.6328125
train loss:  0.6379454135894775
train gradient:  0.22020148055235783
iteration : 16
train acc:  0.6953125
train loss:  0.6389605402946472
train gradient:  0.14962200549885793
iteration : 17
train acc:  0.5546875
train loss:  0.6831125020980835
train gradient:  0.2256707214513885
iteration : 18
train acc:  0.7109375
train loss:  0.6281977891921997
train gradient:  0.19314865625227118
iteration : 19
train acc:  0.609375
train loss:  0.6985565423965454
train gradient:  0.2564111033485229
iteration : 20
train acc:  0.5625
train loss:  0.6718462705612183
train gradient:  0.160585617223637
iteration : 21
train acc:  0.640625
train loss:  0.651134729385376
train gradient:  0.17585419968731425
iteration : 22
train acc:  0.6484375
train loss:  0.641242265701294
train gradient:  0.20446202953491688
iteration : 23
train acc:  0.6640625
train loss:  0.6497772932052612
train gradient:  0.14701243430360428
iteration : 24
train acc:  0.7421875
train loss:  0.5763304829597473
train gradient:  0.18589762491230027
iteration : 25
train acc:  0.5234375
train loss:  0.6727532148361206
train gradient:  0.21277457333627925
iteration : 26
train acc:  0.65625
train loss:  0.6184425354003906
train gradient:  0.19037160385772123
iteration : 27
train acc:  0.6171875
train loss:  0.6313927173614502
train gradient:  0.17933131379219738
iteration : 28
train acc:  0.6484375
train loss:  0.6470500230789185
train gradient:  0.19759878939244963
iteration : 29
train acc:  0.625
train loss:  0.6309308409690857
train gradient:  0.21394376320835204
iteration : 30
train acc:  0.6484375
train loss:  0.6556113958358765
train gradient:  0.18119789236171657
iteration : 31
train acc:  0.640625
train loss:  0.6202402114868164
train gradient:  0.18120314614333977
iteration : 32
train acc:  0.6171875
train loss:  0.6537674069404602
train gradient:  0.23089149034324352
iteration : 33
train acc:  0.578125
train loss:  0.6706863641738892
train gradient:  0.19982396498882954
iteration : 34
train acc:  0.6015625
train loss:  0.6747908592224121
train gradient:  0.18045587941518443
iteration : 35
train acc:  0.703125
train loss:  0.5947387218475342
train gradient:  0.2449707371679864
iteration : 36
train acc:  0.7109375
train loss:  0.5794395804405212
train gradient:  0.1415166418234684
iteration : 37
train acc:  0.6328125
train loss:  0.6173983812332153
train gradient:  0.2511689188429267
iteration : 38
train acc:  0.6953125
train loss:  0.617232620716095
train gradient:  0.21504975106577928
iteration : 39
train acc:  0.59375
train loss:  0.6759378910064697
train gradient:  0.15302445986592533
iteration : 40
train acc:  0.6328125
train loss:  0.6372803449630737
train gradient:  0.17658619640142562
iteration : 41
train acc:  0.59375
train loss:  0.6732140183448792
train gradient:  0.2093525734999951
iteration : 42
train acc:  0.640625
train loss:  0.621128499507904
train gradient:  0.17807902451242397
iteration : 43
train acc:  0.7265625
train loss:  0.597051203250885
train gradient:  0.1714394383775647
iteration : 44
train acc:  0.6640625
train loss:  0.6370000839233398
train gradient:  0.156704219646268
iteration : 45
train acc:  0.6875
train loss:  0.5729814767837524
train gradient:  0.15829207747731555
iteration : 46
train acc:  0.6015625
train loss:  0.6598978638648987
train gradient:  0.15589355770810578
iteration : 47
train acc:  0.6484375
train loss:  0.6525184512138367
train gradient:  0.25849468061597797
iteration : 48
train acc:  0.65625
train loss:  0.6086796522140503
train gradient:  0.17977483935796407
iteration : 49
train acc:  0.6796875
train loss:  0.6014226675033569
train gradient:  0.17228568655743298
iteration : 50
train acc:  0.671875
train loss:  0.6183046102523804
train gradient:  0.1606148338713033
iteration : 51
train acc:  0.6171875
train loss:  0.6738720536231995
train gradient:  0.1944513499497884
iteration : 52
train acc:  0.640625
train loss:  0.6410858631134033
train gradient:  0.1706658032539411
iteration : 53
train acc:  0.6953125
train loss:  0.6191459894180298
train gradient:  0.24334663798813266
iteration : 54
train acc:  0.5546875
train loss:  0.6857374906539917
train gradient:  0.20361154162363498
iteration : 55
train acc:  0.6640625
train loss:  0.5902683138847351
train gradient:  0.17229705762548803
iteration : 56
train acc:  0.6171875
train loss:  0.6126758456230164
train gradient:  0.18885807177377095
iteration : 57
train acc:  0.6015625
train loss:  0.6441448330879211
train gradient:  0.16802293620423575
iteration : 58
train acc:  0.6640625
train loss:  0.5852669477462769
train gradient:  0.1836787214350673
iteration : 59
train acc:  0.6171875
train loss:  0.6459363698959351
train gradient:  0.18803567311145397
iteration : 60
train acc:  0.5859375
train loss:  0.7148312330245972
train gradient:  0.23142941175080783
iteration : 61
train acc:  0.6328125
train loss:  0.6221693158149719
train gradient:  0.2132546451608324
iteration : 62
train acc:  0.640625
train loss:  0.6303431987762451
train gradient:  0.14448578012256288
iteration : 63
train acc:  0.6015625
train loss:  0.624809741973877
train gradient:  0.1720851791333685
iteration : 64
train acc:  0.6484375
train loss:  0.6624648571014404
train gradient:  0.20279694842735924
iteration : 65
train acc:  0.6484375
train loss:  0.6351252794265747
train gradient:  0.20158987349818822
iteration : 66
train acc:  0.6484375
train loss:  0.6419552564620972
train gradient:  0.2131565649282703
iteration : 67
train acc:  0.6953125
train loss:  0.596866250038147
train gradient:  0.1586855397205254
iteration : 68
train acc:  0.671875
train loss:  0.6228110790252686
train gradient:  0.15773897586053875
iteration : 69
train acc:  0.6796875
train loss:  0.6259421110153198
train gradient:  0.17955866361293177
iteration : 70
train acc:  0.6328125
train loss:  0.672688901424408
train gradient:  0.22655371185526563
iteration : 71
train acc:  0.6484375
train loss:  0.6717187166213989
train gradient:  0.16431694341933697
iteration : 72
train acc:  0.5859375
train loss:  0.6509977579116821
train gradient:  0.27598206742370396
iteration : 73
train acc:  0.6171875
train loss:  0.6294094920158386
train gradient:  0.1398727544271326
iteration : 74
train acc:  0.6875
train loss:  0.5826250910758972
train gradient:  0.1421726664466989
iteration : 75
train acc:  0.6171875
train loss:  0.6548904180526733
train gradient:  0.21418086597686226
iteration : 76
train acc:  0.6171875
train loss:  0.6416479349136353
train gradient:  0.14514322857278167
iteration : 77
train acc:  0.640625
train loss:  0.6365844011306763
train gradient:  0.188479099707781
iteration : 78
train acc:  0.609375
train loss:  0.6422151923179626
train gradient:  0.15933091984334757
iteration : 79
train acc:  0.6171875
train loss:  0.6384499669075012
train gradient:  0.21844019068083814
iteration : 80
train acc:  0.671875
train loss:  0.6022801399230957
train gradient:  0.16962264463649626
iteration : 81
train acc:  0.5546875
train loss:  0.6712527275085449
train gradient:  0.25228846578093994
iteration : 82
train acc:  0.65625
train loss:  0.614290714263916
train gradient:  0.13308903154217958
iteration : 83
train acc:  0.5625
train loss:  0.6561436057090759
train gradient:  0.17189345804232747
iteration : 84
train acc:  0.6484375
train loss:  0.6667734980583191
train gradient:  0.16308651499720037
iteration : 85
train acc:  0.65625
train loss:  0.6164120435714722
train gradient:  0.17966269965571835
iteration : 86
train acc:  0.6015625
train loss:  0.6574961543083191
train gradient:  0.1641628780877391
iteration : 87
train acc:  0.6640625
train loss:  0.6058039665222168
train gradient:  0.14422861923197958
iteration : 88
train acc:  0.671875
train loss:  0.6156835556030273
train gradient:  0.178718963449843
iteration : 89
train acc:  0.6171875
train loss:  0.6428127288818359
train gradient:  0.17172148100901238
iteration : 90
train acc:  0.6796875
train loss:  0.594440758228302
train gradient:  0.16803958157767507
iteration : 91
train acc:  0.65625
train loss:  0.6035009026527405
train gradient:  0.17903020285237975
iteration : 92
train acc:  0.703125
train loss:  0.602442741394043
train gradient:  0.279187184215644
iteration : 93
train acc:  0.671875
train loss:  0.6080394387245178
train gradient:  0.15954507334487256
iteration : 94
train acc:  0.6796875
train loss:  0.6126019358634949
train gradient:  0.2446467667449004
iteration : 95
train acc:  0.6796875
train loss:  0.6081384420394897
train gradient:  0.18683148557466905
iteration : 96
train acc:  0.640625
train loss:  0.6283931732177734
train gradient:  0.11947329803698911
iteration : 97
train acc:  0.5859375
train loss:  0.6695683598518372
train gradient:  0.18357674284862707
iteration : 98
train acc:  0.671875
train loss:  0.5853012800216675
train gradient:  0.19476532460558185
iteration : 99
train acc:  0.625
train loss:  0.6475075483322144
train gradient:  0.18829135336703548
iteration : 100
train acc:  0.6328125
train loss:  0.666146993637085
train gradient:  0.2716048750897274
iteration : 101
train acc:  0.6796875
train loss:  0.601974368095398
train gradient:  0.15214451234862092
iteration : 102
train acc:  0.671875
train loss:  0.6115782856941223
train gradient:  0.2122851199499739
iteration : 103
train acc:  0.6328125
train loss:  0.6188842058181763
train gradient:  0.15793967999381553
iteration : 104
train acc:  0.6484375
train loss:  0.6233170628547668
train gradient:  0.18590359700614256
iteration : 105
train acc:  0.6015625
train loss:  0.6321152448654175
train gradient:  0.13286582320432413
iteration : 106
train acc:  0.59375
train loss:  0.6044138669967651
train gradient:  0.19944501456091754
iteration : 107
train acc:  0.671875
train loss:  0.6113380789756775
train gradient:  0.17495532994172242
iteration : 108
train acc:  0.609375
train loss:  0.63240647315979
train gradient:  0.17628625738722783
iteration : 109
train acc:  0.6171875
train loss:  0.6674290299415588
train gradient:  0.18016061827239058
iteration : 110
train acc:  0.609375
train loss:  0.6498223543167114
train gradient:  0.17694038190426756
iteration : 111
train acc:  0.5859375
train loss:  0.6323330402374268
train gradient:  0.13670297748929652
iteration : 112
train acc:  0.59375
train loss:  0.6863927841186523
train gradient:  0.21911517176944767
iteration : 113
train acc:  0.640625
train loss:  0.6035770773887634
train gradient:  0.1382465436587339
iteration : 114
train acc:  0.6484375
train loss:  0.6186525821685791
train gradient:  0.16365092278560223
iteration : 115
train acc:  0.6796875
train loss:  0.6354261636734009
train gradient:  0.17533023375388104
iteration : 116
train acc:  0.6640625
train loss:  0.5993673205375671
train gradient:  0.16876353652077158
iteration : 117
train acc:  0.6015625
train loss:  0.6701350212097168
train gradient:  0.22439590345766647
iteration : 118
train acc:  0.6484375
train loss:  0.5958197116851807
train gradient:  0.14732264614683413
iteration : 119
train acc:  0.6015625
train loss:  0.625694990158081
train gradient:  0.1785098636996776
iteration : 120
train acc:  0.578125
train loss:  0.6568319201469421
train gradient:  0.14579245412194608
iteration : 121
train acc:  0.6328125
train loss:  0.6473774909973145
train gradient:  0.1728335038825216
iteration : 122
train acc:  0.59375
train loss:  0.6347200870513916
train gradient:  0.2999321906454889
iteration : 123
train acc:  0.65625
train loss:  0.616212010383606
train gradient:  0.1327790924795692
iteration : 124
train acc:  0.671875
train loss:  0.6365991234779358
train gradient:  0.1911067539770413
iteration : 125
train acc:  0.515625
train loss:  0.6918553113937378
train gradient:  0.2209182450643044
iteration : 126
train acc:  0.6484375
train loss:  0.5647189021110535
train gradient:  0.1469485470446971
iteration : 127
train acc:  0.6171875
train loss:  0.6499431133270264
train gradient:  0.15666532564669822
iteration : 128
train acc:  0.7578125
train loss:  0.5476976633071899
train gradient:  0.17426264808652847
iteration : 129
train acc:  0.609375
train loss:  0.6328991055488586
train gradient:  0.17633756411165943
iteration : 130
train acc:  0.6015625
train loss:  0.6673300266265869
train gradient:  0.18999580714441727
iteration : 131
train acc:  0.5859375
train loss:  0.6621685028076172
train gradient:  0.24643493343052914
iteration : 132
train acc:  0.6171875
train loss:  0.659250020980835
train gradient:  0.2357672821060049
iteration : 133
train acc:  0.65625
train loss:  0.6041407585144043
train gradient:  0.21855144764673212
iteration : 134
train acc:  0.6953125
train loss:  0.5989924073219299
train gradient:  0.29350705536224336
iteration : 135
train acc:  0.625
train loss:  0.6181886792182922
train gradient:  0.1293953119500696
iteration : 136
train acc:  0.6953125
train loss:  0.5878180861473083
train gradient:  0.1600650200185479
iteration : 137
train acc:  0.7109375
train loss:  0.5899748206138611
train gradient:  0.203343978478144
iteration : 138
train acc:  0.609375
train loss:  0.6508262157440186
train gradient:  0.18634365223261645
iteration : 139
train acc:  0.703125
train loss:  0.5949872732162476
train gradient:  0.17184148619112127
iteration : 140
train acc:  0.640625
train loss:  0.6327470541000366
train gradient:  0.16988657817036684
iteration : 141
train acc:  0.640625
train loss:  0.6059267520904541
train gradient:  0.15735628474643348
iteration : 142
train acc:  0.703125
train loss:  0.5915002822875977
train gradient:  0.18350282679842927
iteration : 143
train acc:  0.6484375
train loss:  0.625522255897522
train gradient:  0.1480343649126966
iteration : 144
train acc:  0.6953125
train loss:  0.6107184886932373
train gradient:  0.18680268385618337
iteration : 145
train acc:  0.6171875
train loss:  0.6339218616485596
train gradient:  0.2274331114964817
iteration : 146
train acc:  0.6875
train loss:  0.6115177869796753
train gradient:  0.15602095248383163
iteration : 147
train acc:  0.6640625
train loss:  0.6065899729728699
train gradient:  0.19276135843240905
iteration : 148
train acc:  0.609375
train loss:  0.6338843107223511
train gradient:  0.23615518927166623
iteration : 149
train acc:  0.6640625
train loss:  0.5930289626121521
train gradient:  0.17408126854995565
iteration : 150
train acc:  0.59375
train loss:  0.666132390499115
train gradient:  0.19773602839151438
iteration : 151
train acc:  0.71875
train loss:  0.559544563293457
train gradient:  0.15592355461793256
iteration : 152
train acc:  0.625
train loss:  0.6178932189941406
train gradient:  0.1352698668827198
iteration : 153
train acc:  0.734375
train loss:  0.5595436096191406
train gradient:  0.18987201243225366
iteration : 154
train acc:  0.5859375
train loss:  0.616804838180542
train gradient:  0.13906889378600276
iteration : 155
train acc:  0.6484375
train loss:  0.6477354764938354
train gradient:  0.24086450659366998
iteration : 156
train acc:  0.640625
train loss:  0.6242160797119141
train gradient:  0.21355671924872507
iteration : 157
train acc:  0.671875
train loss:  0.5812388062477112
train gradient:  0.1932124577327401
iteration : 158
train acc:  0.671875
train loss:  0.6132538914680481
train gradient:  0.12975734627338137
iteration : 159
train acc:  0.6484375
train loss:  0.5938500165939331
train gradient:  0.13600431913711197
iteration : 160
train acc:  0.6640625
train loss:  0.5798920392990112
train gradient:  0.15539537070311227
iteration : 161
train acc:  0.6796875
train loss:  0.5952613353729248
train gradient:  0.15204612095541897
iteration : 162
train acc:  0.5859375
train loss:  0.6389507055282593
train gradient:  0.35250087791308193
iteration : 163
train acc:  0.59375
train loss:  0.6650792360305786
train gradient:  0.21930194822418683
iteration : 164
train acc:  0.640625
train loss:  0.6274222731590271
train gradient:  0.18492124506966556
iteration : 165
train acc:  0.6484375
train loss:  0.6124775409698486
train gradient:  0.19026811773803115
iteration : 166
train acc:  0.65625
train loss:  0.6260724067687988
train gradient:  0.16467142761838932
iteration : 167
train acc:  0.6171875
train loss:  0.6302580237388611
train gradient:  0.18889245914276165
iteration : 168
train acc:  0.6328125
train loss:  0.6370564699172974
train gradient:  0.18119658301034583
iteration : 169
train acc:  0.6640625
train loss:  0.5938358306884766
train gradient:  0.15599835331503567
iteration : 170
train acc:  0.6796875
train loss:  0.6047465205192566
train gradient:  0.12389228672510526
iteration : 171
train acc:  0.703125
train loss:  0.5995302200317383
train gradient:  0.1888294781029343
iteration : 172
train acc:  0.6484375
train loss:  0.6046200394630432
train gradient:  0.16382978329995074
iteration : 173
train acc:  0.65625
train loss:  0.5862683057785034
train gradient:  0.13900136105064131
iteration : 174
train acc:  0.6484375
train loss:  0.6085748672485352
train gradient:  0.16740288921552832
iteration : 175
train acc:  0.625
train loss:  0.6214721202850342
train gradient:  0.20073258463697763
iteration : 176
train acc:  0.640625
train loss:  0.6162166595458984
train gradient:  0.23285527584856353
iteration : 177
train acc:  0.6640625
train loss:  0.6130605936050415
train gradient:  0.16205905002102425
iteration : 178
train acc:  0.578125
train loss:  0.6573824286460876
train gradient:  0.1790525260169175
iteration : 179
train acc:  0.59375
train loss:  0.6604467034339905
train gradient:  0.16993008352811423
iteration : 180
train acc:  0.6953125
train loss:  0.5656508207321167
train gradient:  0.16384147649117708
iteration : 181
train acc:  0.6640625
train loss:  0.5825937986373901
train gradient:  0.17083706453822278
iteration : 182
train acc:  0.640625
train loss:  0.6259255409240723
train gradient:  0.1792569870621836
iteration : 183
train acc:  0.546875
train loss:  0.6542500257492065
train gradient:  0.14573075532901086
iteration : 184
train acc:  0.6328125
train loss:  0.6539326906204224
train gradient:  0.14653627666810254
iteration : 185
train acc:  0.640625
train loss:  0.5896629691123962
train gradient:  0.21314236954956428
iteration : 186
train acc:  0.6796875
train loss:  0.5727270841598511
train gradient:  0.1413830441737848
iteration : 187
train acc:  0.6328125
train loss:  0.6413344144821167
train gradient:  0.20229228733789362
iteration : 188
train acc:  0.6328125
train loss:  0.5958475470542908
train gradient:  0.19353811967196916
iteration : 189
train acc:  0.65625
train loss:  0.608966052532196
train gradient:  0.18744295441148545
iteration : 190
train acc:  0.6640625
train loss:  0.6013005971908569
train gradient:  0.1891990076376101
iteration : 191
train acc:  0.671875
train loss:  0.5719982385635376
train gradient:  0.14081872490593167
iteration : 192
train acc:  0.6796875
train loss:  0.6052790284156799
train gradient:  0.19515504126643649
iteration : 193
train acc:  0.671875
train loss:  0.5897833108901978
train gradient:  0.15807780760026235
iteration : 194
train acc:  0.625
train loss:  0.6678298711776733
train gradient:  0.24262079157095512
iteration : 195
train acc:  0.6484375
train loss:  0.6519185304641724
train gradient:  0.21615584995625847
iteration : 196
train acc:  0.65625
train loss:  0.5898566842079163
train gradient:  0.2026250218801655
iteration : 197
train acc:  0.5859375
train loss:  0.6760958433151245
train gradient:  0.2154790106631922
iteration : 198
train acc:  0.6484375
train loss:  0.6268342733383179
train gradient:  0.2846115906008402
iteration : 199
train acc:  0.7265625
train loss:  0.5618735551834106
train gradient:  0.1336227042517264
iteration : 200
train acc:  0.6328125
train loss:  0.6248226761817932
train gradient:  0.14639567113671742
iteration : 201
train acc:  0.6328125
train loss:  0.6014126539230347
train gradient:  0.17633763025925675
iteration : 202
train acc:  0.609375
train loss:  0.6281341910362244
train gradient:  0.2819119757684258
iteration : 203
train acc:  0.6484375
train loss:  0.6177127361297607
train gradient:  0.18039959825753965
iteration : 204
train acc:  0.6953125
train loss:  0.5685769319534302
train gradient:  0.14375875167138585
iteration : 205
train acc:  0.6796875
train loss:  0.5994324684143066
train gradient:  0.1689977277868004
iteration : 206
train acc:  0.6875
train loss:  0.5914222002029419
train gradient:  0.2184689909300571
iteration : 207
train acc:  0.71875
train loss:  0.5397017002105713
train gradient:  0.1502472779413112
iteration : 208
train acc:  0.6484375
train loss:  0.5919843912124634
train gradient:  0.1256502707094168
iteration : 209
train acc:  0.671875
train loss:  0.626616358757019
train gradient:  0.14211847715622145
iteration : 210
train acc:  0.734375
train loss:  0.6080222129821777
train gradient:  0.23786319719107218
iteration : 211
train acc:  0.671875
train loss:  0.6436742544174194
train gradient:  0.19313177476599006
iteration : 212
train acc:  0.6875
train loss:  0.5831664800643921
train gradient:  0.16777736536888188
iteration : 213
train acc:  0.6171875
train loss:  0.600864052772522
train gradient:  0.16288166439797122
iteration : 214
train acc:  0.6875
train loss:  0.5808775424957275
train gradient:  0.2505047151028765
iteration : 215
train acc:  0.640625
train loss:  0.6445550918579102
train gradient:  0.1661848831019853
iteration : 216
train acc:  0.625
train loss:  0.6327919363975525
train gradient:  0.2186346567094365
iteration : 217
train acc:  0.671875
train loss:  0.5882904529571533
train gradient:  0.18610503439025072
iteration : 218
train acc:  0.640625
train loss:  0.6126232743263245
train gradient:  0.22698754243400704
iteration : 219
train acc:  0.671875
train loss:  0.5733709931373596
train gradient:  0.11775051272347838
iteration : 220
train acc:  0.578125
train loss:  0.6369186639785767
train gradient:  0.21978342852415278
iteration : 221
train acc:  0.625
train loss:  0.651976466178894
train gradient:  0.19557414316055366
iteration : 222
train acc:  0.6796875
train loss:  0.5721521973609924
train gradient:  0.1263869431251787
iteration : 223
train acc:  0.6328125
train loss:  0.6240514516830444
train gradient:  0.16118616872882582
iteration : 224
train acc:  0.703125
train loss:  0.5584301352500916
train gradient:  0.13069090068331188
iteration : 225
train acc:  0.625
train loss:  0.6105536222457886
train gradient:  0.12090558177497739
iteration : 226
train acc:  0.71875
train loss:  0.5671205520629883
train gradient:  0.13718163211151588
iteration : 227
train acc:  0.765625
train loss:  0.5622828602790833
train gradient:  0.13611749635715997
iteration : 228
train acc:  0.640625
train loss:  0.6044673919677734
train gradient:  0.13855833331195905
iteration : 229
train acc:  0.65625
train loss:  0.6330317258834839
train gradient:  0.1907130502042373
iteration : 230
train acc:  0.6015625
train loss:  0.6974190473556519
train gradient:  0.25262733682497945
iteration : 231
train acc:  0.7265625
train loss:  0.5766811966896057
train gradient:  0.14213119416954845
iteration : 232
train acc:  0.6796875
train loss:  0.5799740552902222
train gradient:  0.29115686046931466
iteration : 233
train acc:  0.6796875
train loss:  0.5825951099395752
train gradient:  0.14954528559238478
iteration : 234
train acc:  0.7265625
train loss:  0.5681227445602417
train gradient:  0.2188606703828243
iteration : 235
train acc:  0.6796875
train loss:  0.5769810676574707
train gradient:  0.14541177320399742
iteration : 236
train acc:  0.6875
train loss:  0.6081123352050781
train gradient:  0.17604819820772305
iteration : 237
train acc:  0.7421875
train loss:  0.5441181659698486
train gradient:  0.1889153024905127
iteration : 238
train acc:  0.65625
train loss:  0.585448145866394
train gradient:  0.1332181520055594
iteration : 239
train acc:  0.625
train loss:  0.6308717727661133
train gradient:  0.19790150054220917
iteration : 240
train acc:  0.7109375
train loss:  0.5849366784095764
train gradient:  0.14487534169009625
iteration : 241
train acc:  0.5546875
train loss:  0.6855276823043823
train gradient:  0.21390256886514675
iteration : 242
train acc:  0.671875
train loss:  0.6328672170639038
train gradient:  0.17019168548154529
iteration : 243
train acc:  0.6328125
train loss:  0.6110823154449463
train gradient:  0.151022936583392
iteration : 244
train acc:  0.6875
train loss:  0.5819143056869507
train gradient:  0.22051900396746466
iteration : 245
train acc:  0.703125
train loss:  0.583088219165802
train gradient:  0.1562555119667079
iteration : 246
train acc:  0.6640625
train loss:  0.6122382879257202
train gradient:  0.16883559376826968
iteration : 247
train acc:  0.6484375
train loss:  0.6211612224578857
train gradient:  0.1865093042515583
iteration : 248
train acc:  0.6171875
train loss:  0.6170843243598938
train gradient:  0.14867446439312865
iteration : 249
train acc:  0.5703125
train loss:  0.6646782755851746
train gradient:  0.17737222039403788
iteration : 250
train acc:  0.6796875
train loss:  0.6065101623535156
train gradient:  0.19804590681832207
iteration : 251
train acc:  0.609375
train loss:  0.642396092414856
train gradient:  0.15918261422251073
iteration : 252
train acc:  0.6640625
train loss:  0.6096119284629822
train gradient:  0.14696705329766227
iteration : 253
train acc:  0.6328125
train loss:  0.608352541923523
train gradient:  0.19776387263967923
iteration : 254
train acc:  0.6953125
train loss:  0.5920016765594482
train gradient:  0.15499198669898576
iteration : 255
train acc:  0.6640625
train loss:  0.6362994909286499
train gradient:  0.19310665698665877
iteration : 256
train acc:  0.703125
train loss:  0.579749584197998
train gradient:  0.14388132152682076
iteration : 257
train acc:  0.703125
train loss:  0.5801874995231628
train gradient:  0.1527892062265618
iteration : 258
train acc:  0.6796875
train loss:  0.6102479696273804
train gradient:  0.1650689248179314
iteration : 259
train acc:  0.703125
train loss:  0.61977219581604
train gradient:  0.14146385018221924
iteration : 260
train acc:  0.640625
train loss:  0.6127941608428955
train gradient:  0.20808055830308966
iteration : 261
train acc:  0.6328125
train loss:  0.6202020049095154
train gradient:  0.20924556422962176
iteration : 262
train acc:  0.6875
train loss:  0.6115378141403198
train gradient:  0.1508186190327133
iteration : 263
train acc:  0.6171875
train loss:  0.6104451417922974
train gradient:  0.16720621963888083
iteration : 264
train acc:  0.6953125
train loss:  0.6177031397819519
train gradient:  0.15862938772602314
iteration : 265
train acc:  0.6484375
train loss:  0.5967201590538025
train gradient:  0.20652629818660728
iteration : 266
train acc:  0.6796875
train loss:  0.6893613338470459
train gradient:  0.3255653265682926
iteration : 267
train acc:  0.6171875
train loss:  0.6323294043540955
train gradient:  0.168922275920735
iteration : 268
train acc:  0.71875
train loss:  0.5602516531944275
train gradient:  0.1632741515192564
iteration : 269
train acc:  0.65625
train loss:  0.6180444359779358
train gradient:  0.26469900942870583
iteration : 270
train acc:  0.6796875
train loss:  0.5891535878181458
train gradient:  0.18345340007957617
iteration : 271
train acc:  0.5546875
train loss:  0.6732253432273865
train gradient:  0.23372150061891078
iteration : 272
train acc:  0.6328125
train loss:  0.637736976146698
train gradient:  0.163793910256201
iteration : 273
train acc:  0.6875
train loss:  0.5831915140151978
train gradient:  0.172887596022812
iteration : 274
train acc:  0.609375
train loss:  0.6132380366325378
train gradient:  0.1341785830389448
iteration : 275
train acc:  0.6875
train loss:  0.5813359022140503
train gradient:  0.15056087357553533
iteration : 276
train acc:  0.765625
train loss:  0.5355665683746338
train gradient:  0.11727504386091518
iteration : 277
train acc:  0.640625
train loss:  0.6065346002578735
train gradient:  0.17774201141887477
iteration : 278
train acc:  0.6171875
train loss:  0.627611517906189
train gradient:  0.17913849725548148
iteration : 279
train acc:  0.6328125
train loss:  0.6165642738342285
train gradient:  0.20861604575493126
iteration : 280
train acc:  0.5703125
train loss:  0.6771430373191833
train gradient:  0.17885184706488863
iteration : 281
train acc:  0.671875
train loss:  0.6086311340332031
train gradient:  0.20310375036523443
iteration : 282
train acc:  0.640625
train loss:  0.5949624180793762
train gradient:  0.18623356846566158
iteration : 283
train acc:  0.640625
train loss:  0.5853675603866577
train gradient:  0.20014922927209827
iteration : 284
train acc:  0.6875
train loss:  0.578336238861084
train gradient:  0.17125581881139684
iteration : 285
train acc:  0.75
train loss:  0.5492219924926758
train gradient:  0.1719851629357695
iteration : 286
train acc:  0.6484375
train loss:  0.6578508615493774
train gradient:  0.19119173611550133
iteration : 287
train acc:  0.6328125
train loss:  0.6165330410003662
train gradient:  0.1875115392196528
iteration : 288
train acc:  0.7109375
train loss:  0.5712579488754272
train gradient:  0.17562277966283965
iteration : 289
train acc:  0.6484375
train loss:  0.602441668510437
train gradient:  0.14713044655582552
iteration : 290
train acc:  0.7109375
train loss:  0.5575798749923706
train gradient:  0.14904047531278247
iteration : 291
train acc:  0.609375
train loss:  0.6248334646224976
train gradient:  0.36439988480980134
iteration : 292
train acc:  0.640625
train loss:  0.6046171188354492
train gradient:  0.13721427256801422
iteration : 293
train acc:  0.6171875
train loss:  0.6316715478897095
train gradient:  0.2653213370313531
iteration : 294
train acc:  0.6796875
train loss:  0.5948081016540527
train gradient:  0.15135743226087708
iteration : 295
train acc:  0.6015625
train loss:  0.6221013069152832
train gradient:  0.20121528372254271
iteration : 296
train acc:  0.6328125
train loss:  0.5755219459533691
train gradient:  0.1253539046979049
iteration : 297
train acc:  0.703125
train loss:  0.5612930655479431
train gradient:  0.16343994015555385
iteration : 298
train acc:  0.640625
train loss:  0.5935866832733154
train gradient:  0.14111550446995996
iteration : 299
train acc:  0.6328125
train loss:  0.6447144150733948
train gradient:  0.2189273974364757
iteration : 300
train acc:  0.6484375
train loss:  0.5748764872550964
train gradient:  0.16831117759742603
iteration : 301
train acc:  0.609375
train loss:  0.6223382949829102
train gradient:  0.13624377994972714
iteration : 302
train acc:  0.7109375
train loss:  0.551939845085144
train gradient:  0.17209533158354162
iteration : 303
train acc:  0.6484375
train loss:  0.619679868221283
train gradient:  0.14575838715439735
iteration : 304
train acc:  0.609375
train loss:  0.6152931451797485
train gradient:  0.13809512531147183
iteration : 305
train acc:  0.7265625
train loss:  0.5324291586875916
train gradient:  0.15310999078154622
iteration : 306
train acc:  0.609375
train loss:  0.6198617815971375
train gradient:  0.13833598314025086
iteration : 307
train acc:  0.71875
train loss:  0.5643019676208496
train gradient:  0.14305919056582445
iteration : 308
train acc:  0.6796875
train loss:  0.5866457223892212
train gradient:  0.13912600399113068
iteration : 309
train acc:  0.6640625
train loss:  0.5895121097564697
train gradient:  0.15281377593817153
iteration : 310
train acc:  0.703125
train loss:  0.6234344244003296
train gradient:  0.19494952481453715
iteration : 311
train acc:  0.6015625
train loss:  0.6401661038398743
train gradient:  0.20605163241983027
iteration : 312
train acc:  0.609375
train loss:  0.6213213205337524
train gradient:  0.14452388341801775
iteration : 313
train acc:  0.6171875
train loss:  0.6354504823684692
train gradient:  0.18978971814772205
iteration : 314
train acc:  0.625
train loss:  0.6355481147766113
train gradient:  0.2184818205351441
iteration : 315
train acc:  0.71875
train loss:  0.548039436340332
train gradient:  0.15167944560740265
iteration : 316
train acc:  0.71875
train loss:  0.5952661037445068
train gradient:  0.14675117216555397
iteration : 317
train acc:  0.6875
train loss:  0.6174257397651672
train gradient:  0.19189339034835218
iteration : 318
train acc:  0.671875
train loss:  0.6052418947219849
train gradient:  0.25731871445674115
iteration : 319
train acc:  0.6640625
train loss:  0.6159892082214355
train gradient:  0.14454424678541516
iteration : 320
train acc:  0.59375
train loss:  0.6286420822143555
train gradient:  0.19172611657477145
iteration : 321
train acc:  0.640625
train loss:  0.6172597408294678
train gradient:  0.20367650572861237
iteration : 322
train acc:  0.71875
train loss:  0.5463912487030029
train gradient:  0.13206066688737556
iteration : 323
train acc:  0.7109375
train loss:  0.5685238242149353
train gradient:  0.22194454886047588
iteration : 324
train acc:  0.625
train loss:  0.6488631963729858
train gradient:  0.16694189595579145
iteration : 325
train acc:  0.65625
train loss:  0.6102703213691711
train gradient:  0.22935378093636585
iteration : 326
train acc:  0.6875
train loss:  0.6321542859077454
train gradient:  0.13123496295925297
iteration : 327
train acc:  0.6796875
train loss:  0.6212946772575378
train gradient:  0.14968595981623062
iteration : 328
train acc:  0.640625
train loss:  0.6246873736381531
train gradient:  0.1432800249376841
iteration : 329
train acc:  0.765625
train loss:  0.53803551197052
train gradient:  0.1508139924050762
iteration : 330
train acc:  0.6171875
train loss:  0.6610902547836304
train gradient:  0.19833291793696078
iteration : 331
train acc:  0.6328125
train loss:  0.6078616380691528
train gradient:  0.14289498543549345
iteration : 332
train acc:  0.6171875
train loss:  0.6619237661361694
train gradient:  0.15392489328799186
iteration : 333
train acc:  0.640625
train loss:  0.5953006148338318
train gradient:  0.17008402309039938
iteration : 334
train acc:  0.6953125
train loss:  0.5694972276687622
train gradient:  0.19043347185136827
iteration : 335
train acc:  0.6640625
train loss:  0.5633995532989502
train gradient:  0.18874262156218652
iteration : 336
train acc:  0.671875
train loss:  0.6039689779281616
train gradient:  0.1498306770163878
iteration : 337
train acc:  0.546875
train loss:  0.6605523824691772
train gradient:  0.19991393060561058
iteration : 338
train acc:  0.6875
train loss:  0.584463894367218
train gradient:  0.18623904277539843
iteration : 339
train acc:  0.6328125
train loss:  0.6256140470504761
train gradient:  0.16856655447269608
iteration : 340
train acc:  0.6328125
train loss:  0.6088943481445312
train gradient:  0.1790893876513699
iteration : 341
train acc:  0.6640625
train loss:  0.5952548980712891
train gradient:  0.20329468871063133
iteration : 342
train acc:  0.6328125
train loss:  0.6267032027244568
train gradient:  0.15369529351971492
iteration : 343
train acc:  0.671875
train loss:  0.5934233069419861
train gradient:  0.12997736553006006
iteration : 344
train acc:  0.625
train loss:  0.6242664456367493
train gradient:  0.19666811198643208
iteration : 345
train acc:  0.640625
train loss:  0.6443911790847778
train gradient:  0.1773518039215357
iteration : 346
train acc:  0.609375
train loss:  0.6248111128807068
train gradient:  0.22514614698002142
iteration : 347
train acc:  0.7265625
train loss:  0.5345375537872314
train gradient:  0.13604228812840363
iteration : 348
train acc:  0.6640625
train loss:  0.5851312875747681
train gradient:  0.15012975064284478
iteration : 349
train acc:  0.6796875
train loss:  0.587155282497406
train gradient:  0.15965080237102874
iteration : 350
train acc:  0.5859375
train loss:  0.6109951138496399
train gradient:  0.1365081886661596
iteration : 351
train acc:  0.6953125
train loss:  0.5686569213867188
train gradient:  0.14672999391185848
iteration : 352
train acc:  0.6640625
train loss:  0.6035420894622803
train gradient:  0.17224242877051532
iteration : 353
train acc:  0.640625
train loss:  0.6426734924316406
train gradient:  0.18512282687974485
iteration : 354
train acc:  0.6953125
train loss:  0.5840734839439392
train gradient:  0.1481225844664077
iteration : 355
train acc:  0.7578125
train loss:  0.5482519865036011
train gradient:  0.22228574288266606
iteration : 356
train acc:  0.6875
train loss:  0.6086843609809875
train gradient:  0.19445756324817995
iteration : 357
train acc:  0.6796875
train loss:  0.5941271781921387
train gradient:  0.2112115083178781
iteration : 358
train acc:  0.65625
train loss:  0.6369508504867554
train gradient:  0.36223994650969366
iteration : 359
train acc:  0.6953125
train loss:  0.5860134363174438
train gradient:  0.16797263976341262
iteration : 360
train acc:  0.6171875
train loss:  0.5968186855316162
train gradient:  0.2078176326735548
iteration : 361
train acc:  0.640625
train loss:  0.6061107516288757
train gradient:  0.1579927874645001
iteration : 362
train acc:  0.703125
train loss:  0.5905146598815918
train gradient:  0.13651739929423995
iteration : 363
train acc:  0.65625
train loss:  0.5979371666908264
train gradient:  0.1529472686599193
iteration : 364
train acc:  0.734375
train loss:  0.5870804786682129
train gradient:  0.1901569832204259
iteration : 365
train acc:  0.6796875
train loss:  0.5848373174667358
train gradient:  0.13637576263555584
iteration : 366
train acc:  0.609375
train loss:  0.6458249092102051
train gradient:  0.15568707825564126
iteration : 367
train acc:  0.59375
train loss:  0.6526428461074829
train gradient:  0.2030266522321345
iteration : 368
train acc:  0.578125
train loss:  0.656874418258667
train gradient:  0.1812992145419621
iteration : 369
train acc:  0.671875
train loss:  0.623458981513977
train gradient:  0.20645163247869164
iteration : 370
train acc:  0.6640625
train loss:  0.5814314484596252
train gradient:  0.15965533947753988
iteration : 371
train acc:  0.6484375
train loss:  0.5749071836471558
train gradient:  0.21777652730174468
iteration : 372
train acc:  0.71875
train loss:  0.5586649179458618
train gradient:  0.14210005144712248
iteration : 373
train acc:  0.6875
train loss:  0.5930579900741577
train gradient:  0.14219838223310388
iteration : 374
train acc:  0.6796875
train loss:  0.5718779563903809
train gradient:  0.15864716642377824
iteration : 375
train acc:  0.671875
train loss:  0.5894593596458435
train gradient:  0.1545514386192116
iteration : 376
train acc:  0.625
train loss:  0.5985864400863647
train gradient:  0.1429130252147326
iteration : 377
train acc:  0.6875
train loss:  0.5830435156822205
train gradient:  0.12623085091768405
iteration : 378
train acc:  0.6171875
train loss:  0.5998305082321167
train gradient:  0.15214791732570382
iteration : 379
train acc:  0.71875
train loss:  0.5510599613189697
train gradient:  0.16669080949938458
iteration : 380
train acc:  0.6640625
train loss:  0.5691146850585938
train gradient:  0.1414322122929001
iteration : 381
train acc:  0.59375
train loss:  0.6187013387680054
train gradient:  0.17402174131382953
iteration : 382
train acc:  0.7109375
train loss:  0.5846256017684937
train gradient:  0.14306201438290173
iteration : 383
train acc:  0.6328125
train loss:  0.6042213439941406
train gradient:  0.16044881485974688
iteration : 384
train acc:  0.6328125
train loss:  0.5840101838111877
train gradient:  0.1388981926600162
iteration : 385
train acc:  0.65625
train loss:  0.5801072120666504
train gradient:  0.21802148174368677
iteration : 386
train acc:  0.6328125
train loss:  0.6327051520347595
train gradient:  0.17193348608474546
iteration : 387
train acc:  0.65625
train loss:  0.570278525352478
train gradient:  0.1283519429563055
iteration : 388
train acc:  0.671875
train loss:  0.5876001715660095
train gradient:  0.20023205635395275
iteration : 389
train acc:  0.703125
train loss:  0.5453600883483887
train gradient:  0.14474634357999872
iteration : 390
train acc:  0.609375
train loss:  0.5838725566864014
train gradient:  0.13068077524640942
iteration : 391
train acc:  0.6875
train loss:  0.5619586706161499
train gradient:  0.11426772982206213
iteration : 392
train acc:  0.6953125
train loss:  0.573221743106842
train gradient:  0.15590502766029146
iteration : 393
train acc:  0.7265625
train loss:  0.5468019843101501
train gradient:  0.1191264930811884
iteration : 394
train acc:  0.6015625
train loss:  0.6677060127258301
train gradient:  0.2935891238771795
iteration : 395
train acc:  0.6640625
train loss:  0.594657301902771
train gradient:  0.16122008612708802
iteration : 396
train acc:  0.671875
train loss:  0.6193670034408569
train gradient:  0.1997295457438734
iteration : 397
train acc:  0.59375
train loss:  0.6286094188690186
train gradient:  0.17100138349308333
iteration : 398
train acc:  0.6953125
train loss:  0.5563770532608032
train gradient:  0.14418915969432336
iteration : 399
train acc:  0.7578125
train loss:  0.5278319120407104
train gradient:  0.1581154710616469
iteration : 400
train acc:  0.6484375
train loss:  0.6156598329544067
train gradient:  0.15615191708412762
iteration : 401
train acc:  0.6640625
train loss:  0.5842735767364502
train gradient:  0.1633174374061231
iteration : 402
train acc:  0.6328125
train loss:  0.609028697013855
train gradient:  0.14291986506928422
iteration : 403
train acc:  0.703125
train loss:  0.6056596040725708
train gradient:  0.1694866951595773
iteration : 404
train acc:  0.640625
train loss:  0.6105884313583374
train gradient:  0.14347557228789715
iteration : 405
train acc:  0.6328125
train loss:  0.6108661890029907
train gradient:  0.1575761746848717
iteration : 406
train acc:  0.6796875
train loss:  0.5732940435409546
train gradient:  0.14699713382160368
iteration : 407
train acc:  0.5859375
train loss:  0.6583236455917358
train gradient:  0.1821391642649403
iteration : 408
train acc:  0.671875
train loss:  0.5816429853439331
train gradient:  0.22400160685019566
iteration : 409
train acc:  0.6640625
train loss:  0.5745390057563782
train gradient:  0.13601879901825248
iteration : 410
train acc:  0.6484375
train loss:  0.6366283297538757
train gradient:  0.2541729838320622
iteration : 411
train acc:  0.671875
train loss:  0.5738543272018433
train gradient:  0.1495090210076907
iteration : 412
train acc:  0.6484375
train loss:  0.5965445041656494
train gradient:  0.19176072891912016
iteration : 413
train acc:  0.6640625
train loss:  0.6034216284751892
train gradient:  0.1438169620109026
iteration : 414
train acc:  0.59375
train loss:  0.6377902030944824
train gradient:  0.18458130784297752
iteration : 415
train acc:  0.6484375
train loss:  0.6107439994812012
train gradient:  0.19628207142939869
iteration : 416
train acc:  0.6875
train loss:  0.6025168299674988
train gradient:  0.14296551787124284
iteration : 417
train acc:  0.7578125
train loss:  0.5204527378082275
train gradient:  0.20660646108715602
iteration : 418
train acc:  0.609375
train loss:  0.6338114738464355
train gradient:  0.19374877228095194
iteration : 419
train acc:  0.6328125
train loss:  0.683154821395874
train gradient:  0.2806281008947399
iteration : 420
train acc:  0.71875
train loss:  0.556523323059082
train gradient:  0.18102617701051996
iteration : 421
train acc:  0.671875
train loss:  0.6163776516914368
train gradient:  0.1820457458025883
iteration : 422
train acc:  0.703125
train loss:  0.6091587543487549
train gradient:  0.16062550011043542
iteration : 423
train acc:  0.6015625
train loss:  0.6398195028305054
train gradient:  0.2573217283640304
iteration : 424
train acc:  0.6015625
train loss:  0.6471288204193115
train gradient:  0.18644610440640147
iteration : 425
train acc:  0.65625
train loss:  0.6164111495018005
train gradient:  0.15567309634353022
iteration : 426
train acc:  0.6328125
train loss:  0.5892678499221802
train gradient:  0.14890154590499954
iteration : 427
train acc:  0.5546875
train loss:  0.6554055213928223
train gradient:  0.169311648341064
iteration : 428
train acc:  0.7109375
train loss:  0.5505083203315735
train gradient:  0.14108147122047618
iteration : 429
train acc:  0.6796875
train loss:  0.5614021420478821
train gradient:  0.15168788785373183
iteration : 430
train acc:  0.65625
train loss:  0.5955332517623901
train gradient:  0.16171321102790437
iteration : 431
train acc:  0.65625
train loss:  0.58896803855896
train gradient:  0.4056413525823491
iteration : 432
train acc:  0.671875
train loss:  0.5745150446891785
train gradient:  0.20605952748632309
iteration : 433
train acc:  0.6171875
train loss:  0.6163524985313416
train gradient:  0.22703526988714334
iteration : 434
train acc:  0.703125
train loss:  0.5762583613395691
train gradient:  0.16284612749235133
iteration : 435
train acc:  0.6796875
train loss:  0.5919500589370728
train gradient:  0.21002646321288812
iteration : 436
train acc:  0.6328125
train loss:  0.6139802932739258
train gradient:  0.17635067438249954
iteration : 437
train acc:  0.671875
train loss:  0.5805754661560059
train gradient:  0.14925781606890257
iteration : 438
train acc:  0.71875
train loss:  0.5554750561714172
train gradient:  0.13795163063317706
iteration : 439
train acc:  0.6484375
train loss:  0.6334500312805176
train gradient:  0.22826606075882627
iteration : 440
train acc:  0.6484375
train loss:  0.6020183563232422
train gradient:  0.14349131816150804
iteration : 441
train acc:  0.6328125
train loss:  0.601589560508728
train gradient:  0.1679321307583745
iteration : 442
train acc:  0.703125
train loss:  0.562575101852417
train gradient:  0.18559706488239935
iteration : 443
train acc:  0.7109375
train loss:  0.5697119235992432
train gradient:  0.1514139973070664
iteration : 444
train acc:  0.703125
train loss:  0.5800605416297913
train gradient:  0.1324820901523347
iteration : 445
train acc:  0.6171875
train loss:  0.6549748182296753
train gradient:  0.22295676403533213
iteration : 446
train acc:  0.6640625
train loss:  0.5934469699859619
train gradient:  0.16126347446987868
iteration : 447
train acc:  0.6796875
train loss:  0.6216858625411987
train gradient:  0.19098959353549588
iteration : 448
train acc:  0.6875
train loss:  0.5691336393356323
train gradient:  0.1345970834069426
iteration : 449
train acc:  0.75
train loss:  0.5154199600219727
train gradient:  0.1959906293918284
iteration : 450
train acc:  0.6796875
train loss:  0.5777169466018677
train gradient:  0.16229055080762173
iteration : 451
train acc:  0.7578125
train loss:  0.534156084060669
train gradient:  0.13346190533074107
iteration : 452
train acc:  0.6796875
train loss:  0.6079988479614258
train gradient:  0.1716791585537213
iteration : 453
train acc:  0.6953125
train loss:  0.5708006620407104
train gradient:  0.12183987020613712
iteration : 454
train acc:  0.5546875
train loss:  0.6629599928855896
train gradient:  0.19364450046246082
iteration : 455
train acc:  0.6640625
train loss:  0.6443992853164673
train gradient:  0.18723242140775037
iteration : 456
train acc:  0.703125
train loss:  0.5750482082366943
train gradient:  0.18997403924742362
iteration : 457
train acc:  0.6640625
train loss:  0.6179725527763367
train gradient:  0.13914258451201372
iteration : 458
train acc:  0.6484375
train loss:  0.6384060382843018
train gradient:  0.16307716151613977
iteration : 459
train acc:  0.71875
train loss:  0.5683391094207764
train gradient:  0.14000472628948127
iteration : 460
train acc:  0.71875
train loss:  0.5992861986160278
train gradient:  0.14727601795582512
iteration : 461
train acc:  0.65625
train loss:  0.593739926815033
train gradient:  0.12337547416634674
iteration : 462
train acc:  0.6640625
train loss:  0.595949113368988
train gradient:  0.14039656651547128
iteration : 463
train acc:  0.671875
train loss:  0.5952016115188599
train gradient:  0.17337332136541014
iteration : 464
train acc:  0.640625
train loss:  0.6130952835083008
train gradient:  0.11917224499504123
iteration : 465
train acc:  0.7734375
train loss:  0.49127376079559326
train gradient:  0.160925760191528
iteration : 466
train acc:  0.671875
train loss:  0.600116491317749
train gradient:  0.19146717180761982
iteration : 467
train acc:  0.6875
train loss:  0.5916149616241455
train gradient:  0.18232054967190256
iteration : 468
train acc:  0.6796875
train loss:  0.5773411989212036
train gradient:  0.1351774097823944
iteration : 469
train acc:  0.6953125
train loss:  0.5605840086936951
train gradient:  0.19357881593004728
iteration : 470
train acc:  0.6875
train loss:  0.5786319971084595
train gradient:  0.13186982561612037
iteration : 471
train acc:  0.6328125
train loss:  0.6391704082489014
train gradient:  0.18435496450975508
iteration : 472
train acc:  0.6640625
train loss:  0.5863672494888306
train gradient:  0.1371460444607907
iteration : 473
train acc:  0.6796875
train loss:  0.582719087600708
train gradient:  0.13419015035802961
iteration : 474
train acc:  0.625
train loss:  0.5809148550033569
train gradient:  0.12948917574711835
iteration : 475
train acc:  0.734375
train loss:  0.5075713992118835
train gradient:  0.12711393440972513
iteration : 476
train acc:  0.6171875
train loss:  0.6356050372123718
train gradient:  0.2166643896309295
iteration : 477
train acc:  0.6875
train loss:  0.5771024227142334
train gradient:  0.1573142943711654
iteration : 478
train acc:  0.703125
train loss:  0.5921852588653564
train gradient:  0.13821053011292694
iteration : 479
train acc:  0.6640625
train loss:  0.6296870708465576
train gradient:  0.19146190261354212
iteration : 480
train acc:  0.703125
train loss:  0.5481646060943604
train gradient:  0.14413875126252645
iteration : 481
train acc:  0.671875
train loss:  0.5869412422180176
train gradient:  0.15898588532094077
iteration : 482
train acc:  0.6796875
train loss:  0.5773669481277466
train gradient:  0.1463231138614331
iteration : 483
train acc:  0.6640625
train loss:  0.5853227972984314
train gradient:  0.18002393894761654
iteration : 484
train acc:  0.6640625
train loss:  0.5734290480613708
train gradient:  0.1221129858725673
iteration : 485
train acc:  0.6328125
train loss:  0.6290792226791382
train gradient:  0.2249170728096766
iteration : 486
train acc:  0.6875
train loss:  0.5731337666511536
train gradient:  0.16004070009855997
iteration : 487
train acc:  0.671875
train loss:  0.5992563366889954
train gradient:  0.17285797116304297
iteration : 488
train acc:  0.65625
train loss:  0.6056395769119263
train gradient:  0.22700848169458065
iteration : 489
train acc:  0.6640625
train loss:  0.6016525626182556
train gradient:  0.17559140475926302
iteration : 490
train acc:  0.75
train loss:  0.5128687620162964
train gradient:  0.154370480353147
iteration : 491
train acc:  0.625
train loss:  0.6304227113723755
train gradient:  0.14334685011030118
iteration : 492
train acc:  0.6640625
train loss:  0.5628422498703003
train gradient:  0.1796460347663777
iteration : 493
train acc:  0.6953125
train loss:  0.570737361907959
train gradient:  0.18120058631518113
iteration : 494
train acc:  0.6875
train loss:  0.6219614744186401
train gradient:  0.18044986750323122
iteration : 495
train acc:  0.6875
train loss:  0.5746634602546692
train gradient:  0.1677943915071664
iteration : 496
train acc:  0.6015625
train loss:  0.6131170988082886
train gradient:  0.17782716577003932
iteration : 497
train acc:  0.7421875
train loss:  0.5349317789077759
train gradient:  0.1523347893422446
iteration : 498
train acc:  0.6171875
train loss:  0.6420238018035889
train gradient:  0.15877013629382208
iteration : 499
train acc:  0.7265625
train loss:  0.5524821281433105
train gradient:  0.13178836609133632
iteration : 500
train acc:  0.640625
train loss:  0.6319217085838318
train gradient:  0.17310848917192734
iteration : 501
train acc:  0.7109375
train loss:  0.5817694664001465
train gradient:  0.1799129915573918
iteration : 502
train acc:  0.6953125
train loss:  0.5894678235054016
train gradient:  0.1695401351406125
iteration : 503
train acc:  0.6328125
train loss:  0.60233473777771
train gradient:  0.1591180753979394
iteration : 504
train acc:  0.6796875
train loss:  0.5550397038459778
train gradient:  0.16252406440238648
iteration : 505
train acc:  0.6640625
train loss:  0.5960306525230408
train gradient:  0.1463922297015544
iteration : 506
train acc:  0.6171875
train loss:  0.6077333688735962
train gradient:  0.17894903921074057
iteration : 507
train acc:  0.6328125
train loss:  0.5772855877876282
train gradient:  0.1754457096739474
iteration : 508
train acc:  0.7421875
train loss:  0.4944692552089691
train gradient:  0.13705046584671204
iteration : 509
train acc:  0.625
train loss:  0.6094309687614441
train gradient:  0.16668765325828921
iteration : 510
train acc:  0.71875
train loss:  0.5717833042144775
train gradient:  0.13313338353569448
iteration : 511
train acc:  0.609375
train loss:  0.6185506582260132
train gradient:  0.16784237761085496
iteration : 512
train acc:  0.6796875
train loss:  0.5610800981521606
train gradient:  0.11456498668380732
iteration : 513
train acc:  0.6875
train loss:  0.5866663455963135
train gradient:  0.12847932593313105
iteration : 514
train acc:  0.6953125
train loss:  0.6008656024932861
train gradient:  0.2254919990328878
iteration : 515
train acc:  0.6796875
train loss:  0.5830960273742676
train gradient:  0.15466305025825344
iteration : 516
train acc:  0.65625
train loss:  0.5968071222305298
train gradient:  0.1933134426660845
iteration : 517
train acc:  0.5859375
train loss:  0.6187888383865356
train gradient:  0.24015828876984813
iteration : 518
train acc:  0.6875
train loss:  0.5654538869857788
train gradient:  0.19714845986280632
iteration : 519
train acc:  0.6875
train loss:  0.5735867619514465
train gradient:  0.19563631759699185
iteration : 520
train acc:  0.609375
train loss:  0.6318819522857666
train gradient:  0.18823199705472285
iteration : 521
train acc:  0.6953125
train loss:  0.5623413324356079
train gradient:  0.14916117419618444
iteration : 522
train acc:  0.734375
train loss:  0.5546130537986755
train gradient:  0.14079399883917917
iteration : 523
train acc:  0.6796875
train loss:  0.5798598527908325
train gradient:  0.14702489867563948
iteration : 524
train acc:  0.75
train loss:  0.5499907732009888
train gradient:  0.17635190318811095
iteration : 525
train acc:  0.7421875
train loss:  0.5324366092681885
train gradient:  0.11621863687058326
iteration : 526
train acc:  0.6640625
train loss:  0.5605699419975281
train gradient:  0.12113091899042691
iteration : 527
train acc:  0.6875
train loss:  0.5639095306396484
train gradient:  0.16627358949907223
iteration : 528
train acc:  0.65625
train loss:  0.5845451951026917
train gradient:  0.19223815343658113
iteration : 529
train acc:  0.6484375
train loss:  0.6228744387626648
train gradient:  0.1734475582431876
iteration : 530
train acc:  0.6640625
train loss:  0.5891077518463135
train gradient:  0.20791028742108536
iteration : 531
train acc:  0.7265625
train loss:  0.5672211647033691
train gradient:  0.12560961639746887
iteration : 532
train acc:  0.6796875
train loss:  0.5670748353004456
train gradient:  0.2063581181303208
iteration : 533
train acc:  0.703125
train loss:  0.5600963830947876
train gradient:  0.15052769054810544
iteration : 534
train acc:  0.703125
train loss:  0.593625545501709
train gradient:  0.13778192452588636
iteration : 535
train acc:  0.703125
train loss:  0.5842349529266357
train gradient:  0.1446045341431789
iteration : 536
train acc:  0.7421875
train loss:  0.5511475801467896
train gradient:  0.20285885955644703
iteration : 537
train acc:  0.578125
train loss:  0.6805108189582825
train gradient:  0.2683453441622943
iteration : 538
train acc:  0.6953125
train loss:  0.5636060237884521
train gradient:  0.15306960376413842
iteration : 539
train acc:  0.6875
train loss:  0.6282998919487
train gradient:  0.15769432018228174
iteration : 540
train acc:  0.703125
train loss:  0.5830400586128235
train gradient:  0.18434217667140718
iteration : 541
train acc:  0.6953125
train loss:  0.5782535672187805
train gradient:  0.12123871936864525
iteration : 542
train acc:  0.6328125
train loss:  0.6243776082992554
train gradient:  0.22169401360142577
iteration : 543
train acc:  0.703125
train loss:  0.564816951751709
train gradient:  0.14935624125051256
iteration : 544
train acc:  0.6640625
train loss:  0.6037412881851196
train gradient:  0.15560894904466965
iteration : 545
train acc:  0.7265625
train loss:  0.5456700325012207
train gradient:  0.10987581609695383
iteration : 546
train acc:  0.65625
train loss:  0.5894582867622375
train gradient:  0.16955720351314948
iteration : 547
train acc:  0.6328125
train loss:  0.6002324223518372
train gradient:  0.15650505464056533
iteration : 548
train acc:  0.640625
train loss:  0.5954144597053528
train gradient:  0.19562630044226598
iteration : 549
train acc:  0.7421875
train loss:  0.522831380367279
train gradient:  0.12191641169252866
iteration : 550
train acc:  0.7109375
train loss:  0.5290878415107727
train gradient:  0.13352002601998286
iteration : 551
train acc:  0.7265625
train loss:  0.5755276679992676
train gradient:  0.1643419811883961
iteration : 552
train acc:  0.640625
train loss:  0.5927991271018982
train gradient:  0.16643050979954588
iteration : 553
train acc:  0.6171875
train loss:  0.598442018032074
train gradient:  0.17030408960250726
iteration : 554
train acc:  0.6328125
train loss:  0.6326076984405518
train gradient:  0.16720787444105628
iteration : 555
train acc:  0.6640625
train loss:  0.5912496447563171
train gradient:  0.13794357974701313
iteration : 556
train acc:  0.703125
train loss:  0.5632849931716919
train gradient:  0.2729282551980897
iteration : 557
train acc:  0.625
train loss:  0.6256729364395142
train gradient:  0.15093201475488938
iteration : 558
train acc:  0.671875
train loss:  0.6089451909065247
train gradient:  0.16709623509074933
iteration : 559
train acc:  0.6875
train loss:  0.5647851824760437
train gradient:  0.16875351396066784
iteration : 560
train acc:  0.6875
train loss:  0.5981885194778442
train gradient:  0.15071228337593795
iteration : 561
train acc:  0.703125
train loss:  0.5705816745758057
train gradient:  0.14853071016612823
iteration : 562
train acc:  0.6875
train loss:  0.6204184293746948
train gradient:  0.16833936467710725
iteration : 563
train acc:  0.640625
train loss:  0.5884536504745483
train gradient:  0.16511777818630216
iteration : 564
train acc:  0.6796875
train loss:  0.6191306114196777
train gradient:  0.18499618808060564
iteration : 565
train acc:  0.578125
train loss:  0.6606580018997192
train gradient:  0.22938070663813703
iteration : 566
train acc:  0.7109375
train loss:  0.5951392650604248
train gradient:  0.1374058429443756
iteration : 567
train acc:  0.7265625
train loss:  0.5713533163070679
train gradient:  0.20552013184970208
iteration : 568
train acc:  0.7421875
train loss:  0.5301642417907715
train gradient:  0.14548482224070935
iteration : 569
train acc:  0.71875
train loss:  0.5681639909744263
train gradient:  0.12435901360136618
iteration : 570
train acc:  0.640625
train loss:  0.6110098361968994
train gradient:  0.17243030792491476
iteration : 571
train acc:  0.6953125
train loss:  0.5816863775253296
train gradient:  0.13764227387130487
iteration : 572
train acc:  0.6328125
train loss:  0.6322619915008545
train gradient:  0.20923056888925834
iteration : 573
train acc:  0.6953125
train loss:  0.5886337161064148
train gradient:  0.16383717341171836
iteration : 574
train acc:  0.703125
train loss:  0.559707760810852
train gradient:  0.14810015354069583
iteration : 575
train acc:  0.640625
train loss:  0.6164924502372742
train gradient:  0.2731064555675414
iteration : 576
train acc:  0.59375
train loss:  0.665230393409729
train gradient:  0.30615136804574183
iteration : 577
train acc:  0.625
train loss:  0.6421658992767334
train gradient:  0.19597470916343634
iteration : 578
train acc:  0.6796875
train loss:  0.5634486079216003
train gradient:  0.15367253143963897
iteration : 579
train acc:  0.640625
train loss:  0.597020149230957
train gradient:  0.1388311276454735
iteration : 580
train acc:  0.640625
train loss:  0.5788923501968384
train gradient:  0.15652707407461208
iteration : 581
train acc:  0.609375
train loss:  0.6494250297546387
train gradient:  0.1774908403446648
iteration : 582
train acc:  0.625
train loss:  0.6063141822814941
train gradient:  0.191636126139976
iteration : 583
train acc:  0.625
train loss:  0.6636412143707275
train gradient:  0.2262646522168428
iteration : 584
train acc:  0.671875
train loss:  0.5762490034103394
train gradient:  0.15510067866107213
iteration : 585
train acc:  0.703125
train loss:  0.585802435874939
train gradient:  0.1425673079147348
iteration : 586
train acc:  0.6953125
train loss:  0.5897915363311768
train gradient:  0.1534812874612273
iteration : 587
train acc:  0.6875
train loss:  0.5799842476844788
train gradient:  0.14120956869396875
iteration : 588
train acc:  0.7734375
train loss:  0.5147257447242737
train gradient:  0.19728096166154946
iteration : 589
train acc:  0.6796875
train loss:  0.5815800428390503
train gradient:  0.16780802572288056
iteration : 590
train acc:  0.6875
train loss:  0.5516510009765625
train gradient:  0.12391555106923784
iteration : 591
train acc:  0.6796875
train loss:  0.5832904577255249
train gradient:  0.18683428417122902
iteration : 592
train acc:  0.625
train loss:  0.6228674650192261
train gradient:  0.17528876110120944
iteration : 593
train acc:  0.6640625
train loss:  0.6252124309539795
train gradient:  0.14970193664700487
iteration : 594
train acc:  0.734375
train loss:  0.5572696924209595
train gradient:  0.15044188116482274
iteration : 595
train acc:  0.703125
train loss:  0.5803958773612976
train gradient:  0.19916036333516154
iteration : 596
train acc:  0.6640625
train loss:  0.619072437286377
train gradient:  0.19564159714935897
iteration : 597
train acc:  0.7421875
train loss:  0.5501818656921387
train gradient:  0.16253872202055974
iteration : 598
train acc:  0.6640625
train loss:  0.6058523654937744
train gradient:  0.15674975988499734
iteration : 599
train acc:  0.7265625
train loss:  0.5457061529159546
train gradient:  0.19443674184587492
iteration : 600
train acc:  0.7109375
train loss:  0.5956231355667114
train gradient:  0.2270619506787575
iteration : 601
train acc:  0.6875
train loss:  0.5712382793426514
train gradient:  0.16507912872418107
iteration : 602
train acc:  0.6640625
train loss:  0.6192562580108643
train gradient:  0.1496864688627057
iteration : 603
train acc:  0.625
train loss:  0.6292394399642944
train gradient:  0.1438412978099331
iteration : 604
train acc:  0.6015625
train loss:  0.6224767565727234
train gradient:  0.19205304249934074
iteration : 605
train acc:  0.625
train loss:  0.612065315246582
train gradient:  0.19115247825646836
iteration : 606
train acc:  0.625
train loss:  0.6057864427566528
train gradient:  0.23459799832480982
iteration : 607
train acc:  0.6875
train loss:  0.5878419280052185
train gradient:  0.1853634788233849
iteration : 608
train acc:  0.6796875
train loss:  0.5878840088844299
train gradient:  0.17821827153542907
iteration : 609
train acc:  0.6953125
train loss:  0.5735510587692261
train gradient:  0.11445712061768007
iteration : 610
train acc:  0.6953125
train loss:  0.5617491006851196
train gradient:  0.17722329703157869
iteration : 611
train acc:  0.6484375
train loss:  0.594874918460846
train gradient:  0.17744000553589429
iteration : 612
train acc:  0.6875
train loss:  0.6080942153930664
train gradient:  0.1485969271521106
iteration : 613
train acc:  0.6875
train loss:  0.6143282651901245
train gradient:  0.13115737229043367
iteration : 614
train acc:  0.65625
train loss:  0.5884580612182617
train gradient:  0.17343894134825405
iteration : 615
train acc:  0.703125
train loss:  0.5866278409957886
train gradient:  0.1633306316522479
iteration : 616
train acc:  0.640625
train loss:  0.6176565885543823
train gradient:  0.180161633524499
iteration : 617
train acc:  0.6484375
train loss:  0.5959749221801758
train gradient:  0.141451180515669
iteration : 618
train acc:  0.625
train loss:  0.5814568996429443
train gradient:  0.2620608358628237
iteration : 619
train acc:  0.671875
train loss:  0.5836737751960754
train gradient:  0.1483584802071633
iteration : 620
train acc:  0.703125
train loss:  0.5667520761489868
train gradient:  0.1353713740012438
iteration : 621
train acc:  0.640625
train loss:  0.6169201731681824
train gradient:  0.20024555901469798
iteration : 622
train acc:  0.6875
train loss:  0.6047980785369873
train gradient:  0.17790578596459317
iteration : 623
train acc:  0.6953125
train loss:  0.580047070980072
train gradient:  0.22280692668472807
iteration : 624
train acc:  0.703125
train loss:  0.5519444942474365
train gradient:  0.151725056509381
iteration : 625
train acc:  0.625
train loss:  0.6268430948257446
train gradient:  0.17879610448221306
iteration : 626
train acc:  0.6875
train loss:  0.5796629190444946
train gradient:  0.16871096669674884
iteration : 627
train acc:  0.6640625
train loss:  0.5992283225059509
train gradient:  0.20538916533584528
iteration : 628
train acc:  0.578125
train loss:  0.6241810321807861
train gradient:  0.14989468291419963
iteration : 629
train acc:  0.625
train loss:  0.6347958445549011
train gradient:  0.1937692048747774
iteration : 630
train acc:  0.765625
train loss:  0.5302087664604187
train gradient:  0.12650508996885573
iteration : 631
train acc:  0.703125
train loss:  0.5694922208786011
train gradient:  0.13687349929640785
iteration : 632
train acc:  0.71875
train loss:  0.5627057552337646
train gradient:  0.16811186016099874
iteration : 633
train acc:  0.7265625
train loss:  0.5790926218032837
train gradient:  0.15372718030600288
iteration : 634
train acc:  0.75
train loss:  0.5304479598999023
train gradient:  0.133550813369392
iteration : 635
train acc:  0.7109375
train loss:  0.5836921334266663
train gradient:  0.11717358282261263
iteration : 636
train acc:  0.671875
train loss:  0.5878158211708069
train gradient:  0.19086870041579274
iteration : 637
train acc:  0.6484375
train loss:  0.58484947681427
train gradient:  0.15252866042991528
iteration : 638
train acc:  0.65625
train loss:  0.5718231797218323
train gradient:  0.18759807217864538
iteration : 639
train acc:  0.6953125
train loss:  0.6147502064704895
train gradient:  0.20039791116722178
iteration : 640
train acc:  0.6171875
train loss:  0.5833698511123657
train gradient:  0.17072335115710296
iteration : 641
train acc:  0.703125
train loss:  0.5605559945106506
train gradient:  0.1485388709511678
iteration : 642
train acc:  0.671875
train loss:  0.6173683404922485
train gradient:  0.15941298487901134
iteration : 643
train acc:  0.703125
train loss:  0.5920462608337402
train gradient:  0.13414042902283252
iteration : 644
train acc:  0.6328125
train loss:  0.6531461477279663
train gradient:  0.24183772169778855
iteration : 645
train acc:  0.6328125
train loss:  0.6186990141868591
train gradient:  0.20282457772186596
iteration : 646
train acc:  0.6875
train loss:  0.575645387172699
train gradient:  0.14322776198572423
iteration : 647
train acc:  0.6875
train loss:  0.5884653925895691
train gradient:  0.17064404740810202
iteration : 648
train acc:  0.703125
train loss:  0.5604680776596069
train gradient:  0.17271457830006423
iteration : 649
train acc:  0.7578125
train loss:  0.5557554364204407
train gradient:  0.13110464022379179
iteration : 650
train acc:  0.6484375
train loss:  0.6363871693611145
train gradient:  0.19733831707180238
iteration : 651
train acc:  0.671875
train loss:  0.6286808252334595
train gradient:  0.16250570349511267
iteration : 652
train acc:  0.6796875
train loss:  0.5719391107559204
train gradient:  0.13615990707496667
iteration : 653
train acc:  0.6875
train loss:  0.6123583316802979
train gradient:  0.18768031871515645
iteration : 654
train acc:  0.703125
train loss:  0.5550457835197449
train gradient:  0.15645202494901622
iteration : 655
train acc:  0.625
train loss:  0.6299575567245483
train gradient:  0.205112847933247
iteration : 656
train acc:  0.734375
train loss:  0.5408280491828918
train gradient:  0.1511356663779095
iteration : 657
train acc:  0.7265625
train loss:  0.6132375001907349
train gradient:  0.15413688031000916
iteration : 658
train acc:  0.6484375
train loss:  0.6299055814743042
train gradient:  0.16529337253631238
iteration : 659
train acc:  0.703125
train loss:  0.5872106552124023
train gradient:  0.11686060936656403
iteration : 660
train acc:  0.7265625
train loss:  0.5599136352539062
train gradient:  0.17220160819058047
iteration : 661
train acc:  0.6484375
train loss:  0.5657421946525574
train gradient:  0.12579795887381076
iteration : 662
train acc:  0.625
train loss:  0.6094294786453247
train gradient:  0.13709203473123885
iteration : 663
train acc:  0.6875
train loss:  0.6228445768356323
train gradient:  0.22806222131396953
iteration : 664
train acc:  0.6875
train loss:  0.5607451796531677
train gradient:  0.12240738900320293
iteration : 665
train acc:  0.6875
train loss:  0.5709600448608398
train gradient:  0.12860349371420532
iteration : 666
train acc:  0.65625
train loss:  0.576305627822876
train gradient:  0.21169420420959334
iteration : 667
train acc:  0.7578125
train loss:  0.5249613523483276
train gradient:  0.13678226493066903
iteration : 668
train acc:  0.703125
train loss:  0.5563642978668213
train gradient:  0.13636664752061067
iteration : 669
train acc:  0.703125
train loss:  0.5555551052093506
train gradient:  0.12735333781400407
iteration : 670
train acc:  0.75
train loss:  0.5168715119361877
train gradient:  0.21914762218189565
iteration : 671
train acc:  0.671875
train loss:  0.5748404860496521
train gradient:  0.15260627510061192
iteration : 672
train acc:  0.703125
train loss:  0.561909556388855
train gradient:  0.12405165442148958
iteration : 673
train acc:  0.7109375
train loss:  0.5530294179916382
train gradient:  0.29825694858902674
iteration : 674
train acc:  0.6875
train loss:  0.6401671171188354
train gradient:  0.21754606973878662
iteration : 675
train acc:  0.7109375
train loss:  0.5894850492477417
train gradient:  0.2322999447532363
iteration : 676
train acc:  0.609375
train loss:  0.6501495242118835
train gradient:  0.15581486759562335
iteration : 677
train acc:  0.71875
train loss:  0.5534972548484802
train gradient:  0.16424246559634975
iteration : 678
train acc:  0.703125
train loss:  0.5712536573410034
train gradient:  0.15112764624539077
iteration : 679
train acc:  0.71875
train loss:  0.5377074480056763
train gradient:  0.3209876865987572
iteration : 680
train acc:  0.6328125
train loss:  0.6280295848846436
train gradient:  0.16387114085843318
iteration : 681
train acc:  0.625
train loss:  0.6408438682556152
train gradient:  0.21945653919602787
iteration : 682
train acc:  0.671875
train loss:  0.633094310760498
train gradient:  0.1480055635734553
iteration : 683
train acc:  0.6796875
train loss:  0.6118549108505249
train gradient:  0.16169710402858
iteration : 684
train acc:  0.609375
train loss:  0.5794746279716492
train gradient:  0.14445046506521209
iteration : 685
train acc:  0.703125
train loss:  0.576721727848053
train gradient:  0.14081384240882389
iteration : 686
train acc:  0.734375
train loss:  0.5452485084533691
train gradient:  0.13628324489647153
iteration : 687
train acc:  0.671875
train loss:  0.5649311542510986
train gradient:  0.2123333977889845
iteration : 688
train acc:  0.71875
train loss:  0.5463417768478394
train gradient:  0.15174434541188125
iteration : 689
train acc:  0.6640625
train loss:  0.5703005790710449
train gradient:  0.15451962480824127
iteration : 690
train acc:  0.7109375
train loss:  0.5926535725593567
train gradient:  0.2218505125935893
iteration : 691
train acc:  0.6796875
train loss:  0.5436207056045532
train gradient:  0.11557614840560902
iteration : 692
train acc:  0.6484375
train loss:  0.6145403385162354
train gradient:  0.16674302010317466
iteration : 693
train acc:  0.6796875
train loss:  0.5856684446334839
train gradient:  0.16121056478101256
iteration : 694
train acc:  0.703125
train loss:  0.5577559471130371
train gradient:  0.13153119927223222
iteration : 695
train acc:  0.7578125
train loss:  0.5143327116966248
train gradient:  0.14923817931851097
iteration : 696
train acc:  0.703125
train loss:  0.5661971569061279
train gradient:  0.13501807351350328
iteration : 697
train acc:  0.703125
train loss:  0.5544478893280029
train gradient:  0.1821618354168802
iteration : 698
train acc:  0.671875
train loss:  0.5711017847061157
train gradient:  0.15945485037732432
iteration : 699
train acc:  0.6875
train loss:  0.5764360427856445
train gradient:  0.1585340223203844
iteration : 700
train acc:  0.625
train loss:  0.6014790534973145
train gradient:  0.19525783449642553
iteration : 701
train acc:  0.7109375
train loss:  0.5730027556419373
train gradient:  0.14206728849788372
iteration : 702
train acc:  0.6796875
train loss:  0.6338930726051331
train gradient:  0.16661134966270263
iteration : 703
train acc:  0.6484375
train loss:  0.6288135647773743
train gradient:  0.18354338520997154
iteration : 704
train acc:  0.703125
train loss:  0.5741927623748779
train gradient:  0.17021368050875796
iteration : 705
train acc:  0.6171875
train loss:  0.6304119825363159
train gradient:  0.1746394573283865
iteration : 706
train acc:  0.65625
train loss:  0.6432839632034302
train gradient:  0.21429016450536692
iteration : 707
train acc:  0.625
train loss:  0.6288731098175049
train gradient:  0.15948698766366692
iteration : 708
train acc:  0.6640625
train loss:  0.5746791958808899
train gradient:  0.15006419551879788
iteration : 709
train acc:  0.6875
train loss:  0.5717986226081848
train gradient:  0.13353338487957064
iteration : 710
train acc:  0.71875
train loss:  0.5641714930534363
train gradient:  0.11535326941050866
iteration : 711
train acc:  0.671875
train loss:  0.6018522381782532
train gradient:  0.1588307032257224
iteration : 712
train acc:  0.625
train loss:  0.6656851768493652
train gradient:  0.21171130368890215
iteration : 713
train acc:  0.7421875
train loss:  0.5341107845306396
train gradient:  0.19965900989024865
iteration : 714
train acc:  0.7265625
train loss:  0.5601547360420227
train gradient:  0.1418047490606434
iteration : 715
train acc:  0.6015625
train loss:  0.6389533281326294
train gradient:  0.20937609787774175
iteration : 716
train acc:  0.671875
train loss:  0.5845245122909546
train gradient:  0.1528015847451517
iteration : 717
train acc:  0.7421875
train loss:  0.5284990072250366
train gradient:  0.12724783744693408
iteration : 718
train acc:  0.671875
train loss:  0.5637185573577881
train gradient:  0.20947762014244048
iteration : 719
train acc:  0.703125
train loss:  0.5990192294120789
train gradient:  0.17055385708319282
iteration : 720
train acc:  0.6640625
train loss:  0.6088050603866577
train gradient:  0.13289079388386255
iteration : 721
train acc:  0.609375
train loss:  0.6082029938697815
train gradient:  0.1708798856198748
iteration : 722
train acc:  0.671875
train loss:  0.5823696851730347
train gradient:  0.18687099980949012
iteration : 723
train acc:  0.640625
train loss:  0.6403933763504028
train gradient:  0.16329460744767207
iteration : 724
train acc:  0.6953125
train loss:  0.5671404600143433
train gradient:  0.17037168425463833
iteration : 725
train acc:  0.6640625
train loss:  0.5729537606239319
train gradient:  0.16778247188741652
iteration : 726
train acc:  0.6171875
train loss:  0.6276187300682068
train gradient:  0.1594273893809544
iteration : 727
train acc:  0.734375
train loss:  0.5418907403945923
train gradient:  0.14111501213315236
iteration : 728
train acc:  0.7265625
train loss:  0.5309191346168518
train gradient:  0.1524140226509879
iteration : 729
train acc:  0.671875
train loss:  0.5817667841911316
train gradient:  0.19131983358745175
iteration : 730
train acc:  0.7109375
train loss:  0.5894471406936646
train gradient:  0.1693466590452771
iteration : 731
train acc:  0.7578125
train loss:  0.533650279045105
train gradient:  0.12108306024917873
iteration : 732
train acc:  0.578125
train loss:  0.6674187183380127
train gradient:  0.22455658331024242
iteration : 733
train acc:  0.671875
train loss:  0.6034032106399536
train gradient:  0.14555723692624706
iteration : 734
train acc:  0.7421875
train loss:  0.5168560743331909
train gradient:  0.184273945610223
iteration : 735
train acc:  0.65625
train loss:  0.6141563653945923
train gradient:  0.1982944482200079
iteration : 736
train acc:  0.703125
train loss:  0.5972824096679688
train gradient:  0.21271190514874583
iteration : 737
train acc:  0.7109375
train loss:  0.5671609044075012
train gradient:  0.17097697738020545
iteration : 738
train acc:  0.6015625
train loss:  0.6150286197662354
train gradient:  0.21042846834286427
iteration : 739
train acc:  0.65625
train loss:  0.6061215400695801
train gradient:  0.13970716695411226
iteration : 740
train acc:  0.6796875
train loss:  0.5662322044372559
train gradient:  0.14703094132367237
iteration : 741
train acc:  0.6875
train loss:  0.5623128414154053
train gradient:  0.13175747571245855
iteration : 742
train acc:  0.71875
train loss:  0.5318435430526733
train gradient:  0.1708953581701932
iteration : 743
train acc:  0.6328125
train loss:  0.6502194404602051
train gradient:  0.20894798647296745
iteration : 744
train acc:  0.65625
train loss:  0.6245265007019043
train gradient:  0.18235008083023066
iteration : 745
train acc:  0.703125
train loss:  0.5827289819717407
train gradient:  0.1859268585181364
iteration : 746
train acc:  0.6640625
train loss:  0.5777795314788818
train gradient:  0.12882489860831764
iteration : 747
train acc:  0.6640625
train loss:  0.5966764688491821
train gradient:  0.18868151914912445
iteration : 748
train acc:  0.703125
train loss:  0.575382649898529
train gradient:  0.1359899682192465
iteration : 749
train acc:  0.6640625
train loss:  0.6065410375595093
train gradient:  0.15746977127642106
iteration : 750
train acc:  0.6796875
train loss:  0.579859733581543
train gradient:  0.13253646595588606
iteration : 751
train acc:  0.7421875
train loss:  0.5743801593780518
train gradient:  0.12545309062331816
iteration : 752
train acc:  0.625
train loss:  0.6230243444442749
train gradient:  0.17162916367223208
iteration : 753
train acc:  0.671875
train loss:  0.5688759684562683
train gradient:  0.13166954912088774
iteration : 754
train acc:  0.703125
train loss:  0.5713753700256348
train gradient:  0.14035155188830567
iteration : 755
train acc:  0.7890625
train loss:  0.5120716094970703
train gradient:  0.15186254711699354
iteration : 756
train acc:  0.6875
train loss:  0.5563338994979858
train gradient:  0.13923814223120617
iteration : 757
train acc:  0.6640625
train loss:  0.5982604026794434
train gradient:  0.1635719670359272
iteration : 758
train acc:  0.671875
train loss:  0.6047117710113525
train gradient:  0.16120713037344267
iteration : 759
train acc:  0.6484375
train loss:  0.5921400785446167
train gradient:  0.14585179446162455
iteration : 760
train acc:  0.71875
train loss:  0.5845589637756348
train gradient:  0.1915812948368997
iteration : 761
train acc:  0.7578125
train loss:  0.5428003072738647
train gradient:  0.1581408101579397
iteration : 762
train acc:  0.6640625
train loss:  0.5742825269699097
train gradient:  0.21252195220275155
iteration : 763
train acc:  0.7109375
train loss:  0.5660692453384399
train gradient:  0.19347318911220276
iteration : 764
train acc:  0.65625
train loss:  0.5952075123786926
train gradient:  0.13949849463758413
iteration : 765
train acc:  0.65625
train loss:  0.5948169231414795
train gradient:  0.13917363909790625
iteration : 766
train acc:  0.6796875
train loss:  0.5754785537719727
train gradient:  0.13145176072476553
iteration : 767
train acc:  0.7265625
train loss:  0.5481363534927368
train gradient:  0.155386593066289
iteration : 768
train acc:  0.7421875
train loss:  0.5272229909896851
train gradient:  0.1644453254041569
iteration : 769
train acc:  0.6640625
train loss:  0.6067006587982178
train gradient:  0.17426237881070547
iteration : 770
train acc:  0.734375
train loss:  0.5561419129371643
train gradient:  0.13928920834623637
iteration : 771
train acc:  0.640625
train loss:  0.6057468056678772
train gradient:  0.19379925827059913
iteration : 772
train acc:  0.6484375
train loss:  0.6004701256752014
train gradient:  0.1540187749137399
iteration : 773
train acc:  0.6875
train loss:  0.5594806671142578
train gradient:  0.17367018029550413
iteration : 774
train acc:  0.671875
train loss:  0.5624998807907104
train gradient:  0.18480038937912102
iteration : 775
train acc:  0.671875
train loss:  0.6133190989494324
train gradient:  0.16903140454508198
iteration : 776
train acc:  0.703125
train loss:  0.5215510129928589
train gradient:  0.14713700861713752
iteration : 777
train acc:  0.6328125
train loss:  0.6104468107223511
train gradient:  0.15596222608322147
iteration : 778
train acc:  0.6875
train loss:  0.5734919905662537
train gradient:  0.09958707246120997
iteration : 779
train acc:  0.6640625
train loss:  0.5673129558563232
train gradient:  0.1696223732143015
iteration : 780
train acc:  0.71875
train loss:  0.5570372343063354
train gradient:  0.11095549733766344
iteration : 781
train acc:  0.6953125
train loss:  0.5997713208198547
train gradient:  0.2130550166342416
iteration : 782
train acc:  0.6953125
train loss:  0.5690288543701172
train gradient:  0.1490981065851783
iteration : 783
train acc:  0.703125
train loss:  0.563213586807251
train gradient:  0.15408700145804438
iteration : 784
train acc:  0.6640625
train loss:  0.5731017589569092
train gradient:  0.1726036709002658
iteration : 785
train acc:  0.734375
train loss:  0.5452454090118408
train gradient:  0.139570410371389
iteration : 786
train acc:  0.75
train loss:  0.5290700197219849
train gradient:  0.20061251866949376
iteration : 787
train acc:  0.625
train loss:  0.5985146760940552
train gradient:  0.1995792697811225
iteration : 788
train acc:  0.6875
train loss:  0.589815080165863
train gradient:  0.15898762766549301
iteration : 789
train acc:  0.6484375
train loss:  0.5571608543395996
train gradient:  0.18187637061977507
iteration : 790
train acc:  0.75
train loss:  0.5164562463760376
train gradient:  0.11482525928265003
iteration : 791
train acc:  0.671875
train loss:  0.5838218927383423
train gradient:  0.13339155282147153
iteration : 792
train acc:  0.65625
train loss:  0.6136373281478882
train gradient:  0.20383859364135903
iteration : 793
train acc:  0.5859375
train loss:  0.6605222225189209
train gradient:  0.18031894317279532
iteration : 794
train acc:  0.65625
train loss:  0.6065471172332764
train gradient:  0.20460579829937997
iteration : 795
train acc:  0.734375
train loss:  0.5219202041625977
train gradient:  0.126032124148619
iteration : 796
train acc:  0.703125
train loss:  0.554272472858429
train gradient:  0.15733956864208024
iteration : 797
train acc:  0.671875
train loss:  0.5750772953033447
train gradient:  0.23326401952792933
iteration : 798
train acc:  0.734375
train loss:  0.5233131647109985
train gradient:  0.15431289222848923
iteration : 799
train acc:  0.6171875
train loss:  0.5886619091033936
train gradient:  0.1627486586493337
iteration : 800
train acc:  0.671875
train loss:  0.5968692302703857
train gradient:  0.1510743081747438
iteration : 801
train acc:  0.71875
train loss:  0.5254740715026855
train gradient:  0.15946525017856458
iteration : 802
train acc:  0.640625
train loss:  0.6353097558021545
train gradient:  0.21137500856194835
iteration : 803
train acc:  0.7109375
train loss:  0.5770899057388306
train gradient:  0.1286820547222901
iteration : 804
train acc:  0.71875
train loss:  0.5173411965370178
train gradient:  0.13408918092576932
iteration : 805
train acc:  0.71875
train loss:  0.5761134624481201
train gradient:  0.15506282753123857
iteration : 806
train acc:  0.6796875
train loss:  0.5815958976745605
train gradient:  0.18179068574781854
iteration : 807
train acc:  0.7421875
train loss:  0.5590487718582153
train gradient:  0.15423309070649358
iteration : 808
train acc:  0.734375
train loss:  0.557453453540802
train gradient:  0.19292250653277787
iteration : 809
train acc:  0.7265625
train loss:  0.5600285530090332
train gradient:  0.2642640919992589
iteration : 810
train acc:  0.65625
train loss:  0.5848187208175659
train gradient:  0.2439045730048659
iteration : 811
train acc:  0.6953125
train loss:  0.5913023948669434
train gradient:  0.12894452612276217
iteration : 812
train acc:  0.6953125
train loss:  0.5689582228660583
train gradient:  0.20267647616265672
iteration : 813
train acc:  0.6953125
train loss:  0.5534712672233582
train gradient:  0.13901334966754966
iteration : 814
train acc:  0.6875
train loss:  0.6020588874816895
train gradient:  0.17098455814589952
iteration : 815
train acc:  0.6328125
train loss:  0.6179839968681335
train gradient:  0.16264728073093948
iteration : 816
train acc:  0.6171875
train loss:  0.6390243768692017
train gradient:  0.1722665719625162
iteration : 817
train acc:  0.71875
train loss:  0.571714460849762
train gradient:  0.17106256012504206
iteration : 818
train acc:  0.6796875
train loss:  0.5687464475631714
train gradient:  0.2083460139310571
iteration : 819
train acc:  0.65625
train loss:  0.5856493711471558
train gradient:  0.14296743672531478
iteration : 820
train acc:  0.640625
train loss:  0.58458012342453
train gradient:  0.19879243600889668
iteration : 821
train acc:  0.625
train loss:  0.632013201713562
train gradient:  0.23957781253253058
iteration : 822
train acc:  0.6875
train loss:  0.6090236902236938
train gradient:  0.2075576100269328
iteration : 823
train acc:  0.671875
train loss:  0.5862420797348022
train gradient:  0.18452946431628114
iteration : 824
train acc:  0.640625
train loss:  0.5888253450393677
train gradient:  0.19269236571558512
iteration : 825
train acc:  0.703125
train loss:  0.5651684999465942
train gradient:  0.1342161968063818
iteration : 826
train acc:  0.7265625
train loss:  0.5531934499740601
train gradient:  0.13983973110286613
iteration : 827
train acc:  0.609375
train loss:  0.6130052804946899
train gradient:  0.1934395705978216
iteration : 828
train acc:  0.703125
train loss:  0.5871378183364868
train gradient:  0.15105527365362914
iteration : 829
train acc:  0.7109375
train loss:  0.5566489100456238
train gradient:  0.16934365222339176
iteration : 830
train acc:  0.6875
train loss:  0.541344404220581
train gradient:  0.17043528009511402
iteration : 831
train acc:  0.75
train loss:  0.5473608374595642
train gradient:  0.17512939830789193
iteration : 832
train acc:  0.703125
train loss:  0.577057957649231
train gradient:  0.15050862925857705
iteration : 833
train acc:  0.6875
train loss:  0.6214054822921753
train gradient:  0.2331907213152371
iteration : 834
train acc:  0.703125
train loss:  0.540771484375
train gradient:  0.14673139391522894
iteration : 835
train acc:  0.6796875
train loss:  0.5744840502738953
train gradient:  0.17335865029241998
iteration : 836
train acc:  0.7265625
train loss:  0.6322320699691772
train gradient:  0.1802041743118034
iteration : 837
train acc:  0.7578125
train loss:  0.5141369104385376
train gradient:  0.13071438992728387
iteration : 838
train acc:  0.6875
train loss:  0.5992305278778076
train gradient:  0.1788692915672735
iteration : 839
train acc:  0.5859375
train loss:  0.6930792331695557
train gradient:  0.22810378858675293
iteration : 840
train acc:  0.6640625
train loss:  0.6041638255119324
train gradient:  0.1895757544252112
iteration : 841
train acc:  0.7265625
train loss:  0.5387351512908936
train gradient:  0.15857586415918767
iteration : 842
train acc:  0.6796875
train loss:  0.556538462638855
train gradient:  0.13709370945282098
iteration : 843
train acc:  0.7421875
train loss:  0.5149492025375366
train gradient:  0.16615939269893132
iteration : 844
train acc:  0.7109375
train loss:  0.5666970610618591
train gradient:  0.17403550582518132
iteration : 845
train acc:  0.6328125
train loss:  0.65442955493927
train gradient:  0.19863621627980754
iteration : 846
train acc:  0.6953125
train loss:  0.5748188495635986
train gradient:  0.12640656013657658
iteration : 847
train acc:  0.6953125
train loss:  0.5339571237564087
train gradient:  0.1290812581493042
iteration : 848
train acc:  0.703125
train loss:  0.5149194002151489
train gradient:  0.13520407143353125
iteration : 849
train acc:  0.6953125
train loss:  0.5524284839630127
train gradient:  0.1520222868855456
iteration : 850
train acc:  0.75
train loss:  0.5008187294006348
train gradient:  0.22576369422595505
iteration : 851
train acc:  0.6875
train loss:  0.5732943415641785
train gradient:  0.1564954660405274
iteration : 852
train acc:  0.6875
train loss:  0.5827568173408508
train gradient:  0.16887485276438685
iteration : 853
train acc:  0.671875
train loss:  0.6013267040252686
train gradient:  0.1489349811430748
iteration : 854
train acc:  0.75
train loss:  0.5630436539649963
train gradient:  0.2421722047668869
iteration : 855
train acc:  0.6484375
train loss:  0.6022133827209473
train gradient:  0.17947867886325686
iteration : 856
train acc:  0.6796875
train loss:  0.5533866882324219
train gradient:  0.1536731837797755
iteration : 857
train acc:  0.6875
train loss:  0.5598272085189819
train gradient:  0.15244087558494873
iteration : 858
train acc:  0.71875
train loss:  0.564244270324707
train gradient:  0.17326789737852763
iteration : 859
train acc:  0.78125
train loss:  0.47321340441703796
train gradient:  0.1578323133978624
iteration : 860
train acc:  0.6796875
train loss:  0.5655503869056702
train gradient:  0.1348631986698416
iteration : 861
train acc:  0.71875
train loss:  0.5646429061889648
train gradient:  0.13341234108039862
iteration : 862
train acc:  0.71875
train loss:  0.5436097383499146
train gradient:  0.1665109218128411
iteration : 863
train acc:  0.65625
train loss:  0.6309889554977417
train gradient:  0.17049680604465814
iteration : 864
train acc:  0.6484375
train loss:  0.6112765073776245
train gradient:  0.2297192676074183
iteration : 865
train acc:  0.703125
train loss:  0.5206205248832703
train gradient:  0.11294537417405706
iteration : 866
train acc:  0.6796875
train loss:  0.5744893550872803
train gradient:  0.12602383565503852
iteration : 867
train acc:  0.71875
train loss:  0.5577375292778015
train gradient:  0.17643013242294006
iteration : 868
train acc:  0.703125
train loss:  0.5790143609046936
train gradient:  0.1802730738365032
iteration : 869
train acc:  0.65625
train loss:  0.5503057241439819
train gradient:  0.12722745181838097
iteration : 870
train acc:  0.703125
train loss:  0.542353093624115
train gradient:  0.12228144438578545
iteration : 871
train acc:  0.6484375
train loss:  0.6282052397727966
train gradient:  0.18631664118242697
iteration : 872
train acc:  0.71875
train loss:  0.5316821336746216
train gradient:  0.15909782925704738
iteration : 873
train acc:  0.65625
train loss:  0.590155839920044
train gradient:  0.178184980174248
iteration : 874
train acc:  0.640625
train loss:  0.626460075378418
train gradient:  0.19127024697197745
iteration : 875
train acc:  0.6640625
train loss:  0.6113097667694092
train gradient:  0.2129004577941265
iteration : 876
train acc:  0.6484375
train loss:  0.6257632970809937
train gradient:  0.20911907782142625
iteration : 877
train acc:  0.703125
train loss:  0.5635238885879517
train gradient:  0.1587289774126347
iteration : 878
train acc:  0.7265625
train loss:  0.5622133016586304
train gradient:  0.13169580805783257
iteration : 879
train acc:  0.6875
train loss:  0.5681897401809692
train gradient:  0.16368752355515967
iteration : 880
train acc:  0.6484375
train loss:  0.606531023979187
train gradient:  0.2609136248236064
iteration : 881
train acc:  0.6015625
train loss:  0.604765772819519
train gradient:  0.1768395018738842
iteration : 882
train acc:  0.671875
train loss:  0.586280882358551
train gradient:  0.143671244727915
iteration : 883
train acc:  0.6640625
train loss:  0.5616301894187927
train gradient:  0.13148375849577273
iteration : 884
train acc:  0.6640625
train loss:  0.6155313849449158
train gradient:  0.24917424676054672
iteration : 885
train acc:  0.6640625
train loss:  0.5570074319839478
train gradient:  0.16679569396535873
iteration : 886
train acc:  0.6796875
train loss:  0.6077069044113159
train gradient:  0.16365698714878754
iteration : 887
train acc:  0.6015625
train loss:  0.6734637022018433
train gradient:  0.2040411871213455
iteration : 888
train acc:  0.734375
train loss:  0.5127472877502441
train gradient:  0.12802080514529918
iteration : 889
train acc:  0.6875
train loss:  0.5998156666755676
train gradient:  0.1989000643222436
iteration : 890
train acc:  0.703125
train loss:  0.5860493183135986
train gradient:  0.19227408079313532
iteration : 891
train acc:  0.6015625
train loss:  0.6303369998931885
train gradient:  0.18016928362531603
iteration : 892
train acc:  0.6796875
train loss:  0.5306856632232666
train gradient:  0.12715431600400837
iteration : 893
train acc:  0.734375
train loss:  0.554674506187439
train gradient:  0.16181993889939528
iteration : 894
train acc:  0.6796875
train loss:  0.6140503883361816
train gradient:  0.23584256104853393
iteration : 895
train acc:  0.7421875
train loss:  0.5454128384590149
train gradient:  0.15697790832448108
iteration : 896
train acc:  0.6640625
train loss:  0.5634355545043945
train gradient:  0.12491821127216551
iteration : 897
train acc:  0.734375
train loss:  0.5391983985900879
train gradient:  0.10263426932212917
iteration : 898
train acc:  0.765625
train loss:  0.5120984315872192
train gradient:  0.13544948372832186
iteration : 899
train acc:  0.671875
train loss:  0.5821517705917358
train gradient:  0.15281060768750565
iteration : 900
train acc:  0.65625
train loss:  0.5767840147018433
train gradient:  0.14767932390714839
iteration : 901
train acc:  0.6953125
train loss:  0.5664389729499817
train gradient:  0.16090299432003707
iteration : 902
train acc:  0.625
train loss:  0.6258653998374939
train gradient:  0.17353839091023848
iteration : 903
train acc:  0.6171875
train loss:  0.6207839250564575
train gradient:  0.16948593967173095
iteration : 904
train acc:  0.65625
train loss:  0.6001664996147156
train gradient:  0.1355335241645229
iteration : 905
train acc:  0.640625
train loss:  0.5905299186706543
train gradient:  0.21714316225073665
iteration : 906
train acc:  0.734375
train loss:  0.5346847772598267
train gradient:  0.1542915369988363
iteration : 907
train acc:  0.6484375
train loss:  0.5967801809310913
train gradient:  0.13459541555292803
iteration : 908
train acc:  0.671875
train loss:  0.6137795448303223
train gradient:  0.16184339241344792
iteration : 909
train acc:  0.71875
train loss:  0.5582740902900696
train gradient:  0.1428933964451473
iteration : 910
train acc:  0.6484375
train loss:  0.6184476613998413
train gradient:  0.2321240968425407
iteration : 911
train acc:  0.7265625
train loss:  0.5605344176292419
train gradient:  0.13579991271789627
iteration : 912
train acc:  0.6875
train loss:  0.5970568060874939
train gradient:  0.17253502525762524
iteration : 913
train acc:  0.703125
train loss:  0.5340515375137329
train gradient:  0.12815606640402913
iteration : 914
train acc:  0.671875
train loss:  0.6018316149711609
train gradient:  0.13609525126005473
iteration : 915
train acc:  0.671875
train loss:  0.5972230434417725
train gradient:  0.16918679186935953
iteration : 916
train acc:  0.6796875
train loss:  0.5727066993713379
train gradient:  0.16140672337303316
iteration : 917
train acc:  0.703125
train loss:  0.5405725240707397
train gradient:  0.1194474782853637
iteration : 918
train acc:  0.6171875
train loss:  0.6292698979377747
train gradient:  0.2343884289594215
iteration : 919
train acc:  0.734375
train loss:  0.5299680233001709
train gradient:  0.17225016970765678
iteration : 920
train acc:  0.6796875
train loss:  0.6205278635025024
train gradient:  0.1900843768837767
iteration : 921
train acc:  0.7265625
train loss:  0.570915162563324
train gradient:  0.18430466617855545
iteration : 922
train acc:  0.6796875
train loss:  0.54686439037323
train gradient:  0.18009952683262304
iteration : 923
train acc:  0.65625
train loss:  0.5977224707603455
train gradient:  0.19784862286642613
iteration : 924
train acc:  0.7109375
train loss:  0.5485687851905823
train gradient:  0.36658661555505534
iteration : 925
train acc:  0.6640625
train loss:  0.6135152578353882
train gradient:  0.16769102659076623
iteration : 926
train acc:  0.7578125
train loss:  0.5324337482452393
train gradient:  0.1307143187356251
iteration : 927
train acc:  0.625
train loss:  0.5747277736663818
train gradient:  0.13719960986934446
iteration : 928
train acc:  0.734375
train loss:  0.5495054125785828
train gradient:  0.14506498764298187
iteration : 929
train acc:  0.75
train loss:  0.5220978260040283
train gradient:  0.21741438315968253
iteration : 930
train acc:  0.6328125
train loss:  0.5895172357559204
train gradient:  0.18084933030542913
iteration : 931
train acc:  0.6796875
train loss:  0.5851286053657532
train gradient:  0.18620141826588035
iteration : 932
train acc:  0.703125
train loss:  0.5528689026832581
train gradient:  0.15575409236310428
iteration : 933
train acc:  0.7265625
train loss:  0.5689414739608765
train gradient:  0.14283653368279586
iteration : 934
train acc:  0.71875
train loss:  0.5510606169700623
train gradient:  0.15291915392126226
iteration : 935
train acc:  0.6953125
train loss:  0.5970529317855835
train gradient:  0.22981924698208528
iteration : 936
train acc:  0.6796875
train loss:  0.5684390068054199
train gradient:  0.18160385098490162
iteration : 937
train acc:  0.71875
train loss:  0.5452579259872437
train gradient:  0.17719625709227715
iteration : 938
train acc:  0.640625
train loss:  0.6171735525131226
train gradient:  0.2903742496454708
iteration : 939
train acc:  0.671875
train loss:  0.5907384157180786
train gradient:  0.16907121137138625
iteration : 940
train acc:  0.6640625
train loss:  0.5689176321029663
train gradient:  0.18146856798982025
iteration : 941
train acc:  0.734375
train loss:  0.5409665107727051
train gradient:  0.1704240109596496
iteration : 942
train acc:  0.703125
train loss:  0.5794010758399963
train gradient:  0.23786150088489383
iteration : 943
train acc:  0.5859375
train loss:  0.6127631068229675
train gradient:  0.2127945816064737
iteration : 944
train acc:  0.671875
train loss:  0.6357035636901855
train gradient:  0.1883490370842296
iteration : 945
train acc:  0.65625
train loss:  0.6202297210693359
train gradient:  0.18340735053037657
iteration : 946
train acc:  0.6875
train loss:  0.6033674478530884
train gradient:  0.14799658554599282
iteration : 947
train acc:  0.6796875
train loss:  0.5959482192993164
train gradient:  0.1399073721294135
iteration : 948
train acc:  0.6640625
train loss:  0.5780993103981018
train gradient:  0.19315381531146023
iteration : 949
train acc:  0.609375
train loss:  0.6469864845275879
train gradient:  0.28524441723803307
iteration : 950
train acc:  0.75
train loss:  0.5159826278686523
train gradient:  0.151172369225278
iteration : 951
train acc:  0.640625
train loss:  0.6175785064697266
train gradient:  0.1483209155620285
iteration : 952
train acc:  0.65625
train loss:  0.6185860633850098
train gradient:  0.21942245401827573
iteration : 953
train acc:  0.6484375
train loss:  0.6517060995101929
train gradient:  0.18781524514038317
iteration : 954
train acc:  0.6953125
train loss:  0.5686208009719849
train gradient:  0.2311112251814535
iteration : 955
train acc:  0.6484375
train loss:  0.6007375121116638
train gradient:  0.1683674361126181
iteration : 956
train acc:  0.6796875
train loss:  0.6421247720718384
train gradient:  0.2096214208310201
iteration : 957
train acc:  0.6328125
train loss:  0.6217846274375916
train gradient:  0.15007133562815783
iteration : 958
train acc:  0.6640625
train loss:  0.5811723470687866
train gradient:  0.13194822544705764
iteration : 959
train acc:  0.75
train loss:  0.5408947467803955
train gradient:  0.16523053860636522
iteration : 960
train acc:  0.6953125
train loss:  0.5556010603904724
train gradient:  0.13088421176565682
iteration : 961
train acc:  0.65625
train loss:  0.5720173120498657
train gradient:  0.1543496880181976
iteration : 962
train acc:  0.734375
train loss:  0.5233055949211121
train gradient:  0.17903357169352244
iteration : 963
train acc:  0.703125
train loss:  0.5654791593551636
train gradient:  0.14643777675551753
iteration : 964
train acc:  0.7578125
train loss:  0.5588383674621582
train gradient:  0.19669466279345277
iteration : 965
train acc:  0.6953125
train loss:  0.570602297782898
train gradient:  0.25773882894515
iteration : 966
train acc:  0.7109375
train loss:  0.5323179364204407
train gradient:  0.15821663173852482
iteration : 967
train acc:  0.6875
train loss:  0.5697863101959229
train gradient:  0.13062865108684474
iteration : 968
train acc:  0.6640625
train loss:  0.6128363609313965
train gradient:  0.20415033557124249
iteration : 969
train acc:  0.65625
train loss:  0.5693446397781372
train gradient:  0.16608225112977776
iteration : 970
train acc:  0.7109375
train loss:  0.5374855995178223
train gradient:  0.14274824620002263
iteration : 971
train acc:  0.671875
train loss:  0.5944428443908691
train gradient:  0.15719617353528503
iteration : 972
train acc:  0.7109375
train loss:  0.5419278144836426
train gradient:  0.1331340063633906
iteration : 973
train acc:  0.6875
train loss:  0.5464385747909546
train gradient:  0.1580520294486133
iteration : 974
train acc:  0.6171875
train loss:  0.6072825789451599
train gradient:  0.20885277371908956
iteration : 975
train acc:  0.6953125
train loss:  0.5478786826133728
train gradient:  0.1284456606696821
iteration : 976
train acc:  0.734375
train loss:  0.5510488748550415
train gradient:  0.13381231217824197
iteration : 977
train acc:  0.6875
train loss:  0.5468012690544128
train gradient:  0.11957010025011768
iteration : 978
train acc:  0.6640625
train loss:  0.549957275390625
train gradient:  0.1420591030809995
iteration : 979
train acc:  0.6953125
train loss:  0.5474896430969238
train gradient:  0.13263564619419727
iteration : 980
train acc:  0.734375
train loss:  0.5129973888397217
train gradient:  0.1367444509752908
iteration : 981
train acc:  0.671875
train loss:  0.5798554420471191
train gradient:  0.21389641208646226
iteration : 982
train acc:  0.765625
train loss:  0.5125828981399536
train gradient:  0.12970316160811346
iteration : 983
train acc:  0.6796875
train loss:  0.5805085301399231
train gradient:  0.12666391266315544
iteration : 984
train acc:  0.734375
train loss:  0.5546692609786987
train gradient:  0.2635869456412234
iteration : 985
train acc:  0.71875
train loss:  0.5765777826309204
train gradient:  0.1434762245500843
iteration : 986
train acc:  0.734375
train loss:  0.5300574898719788
train gradient:  0.13595924021967998
iteration : 987
train acc:  0.7265625
train loss:  0.5347614288330078
train gradient:  0.1871426345939905
iteration : 988
train acc:  0.625
train loss:  0.6080732345581055
train gradient:  0.1271024950612963
iteration : 989
train acc:  0.6796875
train loss:  0.6169620752334595
train gradient:  0.17548992831955937
iteration : 990
train acc:  0.6484375
train loss:  0.6151015758514404
train gradient:  0.20277917136733484
iteration : 991
train acc:  0.671875
train loss:  0.5606005191802979
train gradient:  0.20179839425657922
iteration : 992
train acc:  0.640625
train loss:  0.648495614528656
train gradient:  0.20963673226644616
iteration : 993
train acc:  0.703125
train loss:  0.5961813926696777
train gradient:  0.13801885343462286
iteration : 994
train acc:  0.6796875
train loss:  0.5317182540893555
train gradient:  0.16198063560407405
iteration : 995
train acc:  0.703125
train loss:  0.5799033641815186
train gradient:  0.24061999525958228
iteration : 996
train acc:  0.7109375
train loss:  0.5833426713943481
train gradient:  0.19959112639276422
iteration : 997
train acc:  0.75
train loss:  0.5490808486938477
train gradient:  0.18499652372723482
iteration : 998
train acc:  0.734375
train loss:  0.5321762561798096
train gradient:  0.13238208278091232
iteration : 999
train acc:  0.640625
train loss:  0.6290364861488342
train gradient:  0.13869899110629624
iteration : 1000
train acc:  0.6875
train loss:  0.5911921262741089
train gradient:  0.16738995676659202
iteration : 1001
train acc:  0.75
train loss:  0.5113431215286255
train gradient:  0.10979692101874025
iteration : 1002
train acc:  0.703125
train loss:  0.5670438408851624
train gradient:  0.1806155307588354
iteration : 1003
train acc:  0.6875
train loss:  0.5964484214782715
train gradient:  0.1325515560167949
iteration : 1004
train acc:  0.625
train loss:  0.6574641466140747
train gradient:  0.21073335504512503
iteration : 1005
train acc:  0.6484375
train loss:  0.5946028232574463
train gradient:  0.14958022345949468
iteration : 1006
train acc:  0.6796875
train loss:  0.5376622676849365
train gradient:  0.11156218086547803
iteration : 1007
train acc:  0.6796875
train loss:  0.5870934128761292
train gradient:  0.15618179699725004
iteration : 1008
train acc:  0.6796875
train loss:  0.5596842169761658
train gradient:  0.13282655421318026
iteration : 1009
train acc:  0.59375
train loss:  0.6594641208648682
train gradient:  0.19477954685547402
iteration : 1010
train acc:  0.6484375
train loss:  0.6064491271972656
train gradient:  0.17597270092028844
iteration : 1011
train acc:  0.6796875
train loss:  0.608277440071106
train gradient:  0.17061201783014343
iteration : 1012
train acc:  0.7734375
train loss:  0.5046476125717163
train gradient:  0.152251374536487
iteration : 1013
train acc:  0.7109375
train loss:  0.5361031293869019
train gradient:  0.1688835078460736
iteration : 1014
train acc:  0.6484375
train loss:  0.5843566656112671
train gradient:  0.388467940799704
iteration : 1015
train acc:  0.7265625
train loss:  0.5422707796096802
train gradient:  0.13204077490523114
iteration : 1016
train acc:  0.7109375
train loss:  0.5883898138999939
train gradient:  0.17904645412968098
iteration : 1017
train acc:  0.734375
train loss:  0.5725955963134766
train gradient:  0.16250774203171064
iteration : 1018
train acc:  0.7421875
train loss:  0.50174480676651
train gradient:  0.13428385109780952
iteration : 1019
train acc:  0.6953125
train loss:  0.5806958675384521
train gradient:  0.18523441545688746
iteration : 1020
train acc:  0.71875
train loss:  0.543566882610321
train gradient:  0.1515454960753529
iteration : 1021
train acc:  0.7109375
train loss:  0.5391280651092529
train gradient:  0.2856428529814632
iteration : 1022
train acc:  0.7890625
train loss:  0.5124292373657227
train gradient:  0.10584809436489868
iteration : 1023
train acc:  0.7109375
train loss:  0.5438368320465088
train gradient:  0.14383422766384696
iteration : 1024
train acc:  0.6875
train loss:  0.5655554533004761
train gradient:  0.17756412004691818
iteration : 1025
train acc:  0.6640625
train loss:  0.6017690896987915
train gradient:  0.18205850129941098
iteration : 1026
train acc:  0.734375
train loss:  0.5331844091415405
train gradient:  0.15095608234869845
iteration : 1027
train acc:  0.6875
train loss:  0.5713660717010498
train gradient:  0.1642621372374629
iteration : 1028
train acc:  0.640625
train loss:  0.5982182621955872
train gradient:  0.17777837802217014
iteration : 1029
train acc:  0.765625
train loss:  0.5052412748336792
train gradient:  0.1721200566021986
iteration : 1030
train acc:  0.65625
train loss:  0.6058800220489502
train gradient:  0.18066435552551421
iteration : 1031
train acc:  0.6875
train loss:  0.591362714767456
train gradient:  0.1729648231423507
iteration : 1032
train acc:  0.703125
train loss:  0.5591509342193604
train gradient:  0.13252025869225872
iteration : 1033
train acc:  0.7421875
train loss:  0.5158055424690247
train gradient:  0.1304362277559234
iteration : 1034
train acc:  0.6953125
train loss:  0.5722885131835938
train gradient:  0.1622995851403195
iteration : 1035
train acc:  0.65625
train loss:  0.6043622493743896
train gradient:  0.1495368340499042
iteration : 1036
train acc:  0.671875
train loss:  0.580056369304657
train gradient:  0.13270553538135377
iteration : 1037
train acc:  0.703125
train loss:  0.5598692893981934
train gradient:  0.2110749468753784
iteration : 1038
train acc:  0.640625
train loss:  0.606174111366272
train gradient:  0.21708224724356814
iteration : 1039
train acc:  0.71875
train loss:  0.5550887584686279
train gradient:  0.14046147373981835
iteration : 1040
train acc:  0.671875
train loss:  0.572950005531311
train gradient:  0.15583663884513269
iteration : 1041
train acc:  0.6640625
train loss:  0.570083498954773
train gradient:  0.16631716224645204
iteration : 1042
train acc:  0.7265625
train loss:  0.514263391494751
train gradient:  0.13223393510122075
iteration : 1043
train acc:  0.734375
train loss:  0.5309631824493408
train gradient:  0.13670788958317215
iteration : 1044
train acc:  0.7578125
train loss:  0.49751508235931396
train gradient:  0.13497257421889877
iteration : 1045
train acc:  0.6328125
train loss:  0.6039792895317078
train gradient:  0.15237812708374618
iteration : 1046
train acc:  0.6640625
train loss:  0.567254364490509
train gradient:  0.23089528566235873
iteration : 1047
train acc:  0.6953125
train loss:  0.5581146478652954
train gradient:  0.1739567181138294
iteration : 1048
train acc:  0.703125
train loss:  0.5659180283546448
train gradient:  0.13998153349386605
iteration : 1049
train acc:  0.8125
train loss:  0.49617069959640503
train gradient:  0.22043449511904006
iteration : 1050
train acc:  0.703125
train loss:  0.5620375871658325
train gradient:  0.1795180044550573
iteration : 1051
train acc:  0.703125
train loss:  0.5661463737487793
train gradient:  0.15659145049850653
iteration : 1052
train acc:  0.71875
train loss:  0.5633753538131714
train gradient:  0.2034792148548671
iteration : 1053
train acc:  0.6796875
train loss:  0.5784590840339661
train gradient:  0.16581819756610974
iteration : 1054
train acc:  0.65625
train loss:  0.5687340497970581
train gradient:  0.21918480375047442
iteration : 1055
train acc:  0.6796875
train loss:  0.5954332947731018
train gradient:  0.14860597732157174
iteration : 1056
train acc:  0.734375
train loss:  0.5625481605529785
train gradient:  0.14654683991324574
iteration : 1057
train acc:  0.6796875
train loss:  0.6553119421005249
train gradient:  0.2496879552732995
iteration : 1058
train acc:  0.6484375
train loss:  0.5610325336456299
train gradient:  0.19639657451461928
iteration : 1059
train acc:  0.734375
train loss:  0.5601058602333069
train gradient:  0.19390033779013977
iteration : 1060
train acc:  0.6796875
train loss:  0.5808074474334717
train gradient:  0.17254646324476242
iteration : 1061
train acc:  0.7109375
train loss:  0.5226355791091919
train gradient:  0.13312465901382942
iteration : 1062
train acc:  0.625
train loss:  0.6153720617294312
train gradient:  0.16990693615811742
iteration : 1063
train acc:  0.71875
train loss:  0.527127206325531
train gradient:  0.2234842330554604
iteration : 1064
train acc:  0.75
train loss:  0.5058746337890625
train gradient:  0.1462985733810393
iteration : 1065
train acc:  0.6640625
train loss:  0.5898196697235107
train gradient:  0.19137646143773035
iteration : 1066
train acc:  0.703125
train loss:  0.5490342378616333
train gradient:  0.15890555733417058
iteration : 1067
train acc:  0.6875
train loss:  0.5883673429489136
train gradient:  0.1769529201152924
iteration : 1068
train acc:  0.671875
train loss:  0.5509356260299683
train gradient:  0.12021510490145497
iteration : 1069
train acc:  0.640625
train loss:  0.6480732560157776
train gradient:  0.22921888862257198
iteration : 1070
train acc:  0.671875
train loss:  0.6243876218795776
train gradient:  0.17614022485517777
iteration : 1071
train acc:  0.78125
train loss:  0.5124863386154175
train gradient:  0.14619009845366027
iteration : 1072
train acc:  0.7109375
train loss:  0.5486729145050049
train gradient:  0.17040732411594495
iteration : 1073
train acc:  0.7734375
train loss:  0.5060452818870544
train gradient:  0.10905563683752757
iteration : 1074
train acc:  0.6875
train loss:  0.5577968955039978
train gradient:  0.13499670156930277
iteration : 1075
train acc:  0.6953125
train loss:  0.5575017929077148
train gradient:  0.17142333624466383
iteration : 1076
train acc:  0.6484375
train loss:  0.6016793251037598
train gradient:  0.19606711165862525
iteration : 1077
train acc:  0.75
train loss:  0.5671499967575073
train gradient:  0.2365389862080537
iteration : 1078
train acc:  0.6953125
train loss:  0.5701034069061279
train gradient:  0.20625673199771313
iteration : 1079
train acc:  0.6953125
train loss:  0.6109166145324707
train gradient:  0.2355138974339814
iteration : 1080
train acc:  0.7265625
train loss:  0.5101718902587891
train gradient:  0.16749015209498647
iteration : 1081
train acc:  0.671875
train loss:  0.5879842042922974
train gradient:  0.19853510396323454
iteration : 1082
train acc:  0.671875
train loss:  0.5825197100639343
train gradient:  0.1456400986070332
iteration : 1083
train acc:  0.6796875
train loss:  0.5843698382377625
train gradient:  0.16118989344290152
iteration : 1084
train acc:  0.6796875
train loss:  0.5736404061317444
train gradient:  0.16674885871012216
iteration : 1085
train acc:  0.703125
train loss:  0.5825753808021545
train gradient:  0.21127990494306986
iteration : 1086
train acc:  0.6953125
train loss:  0.5466958284378052
train gradient:  0.1273007917721101
iteration : 1087
train acc:  0.75
train loss:  0.5087532997131348
train gradient:  0.1707409253207095
iteration : 1088
train acc:  0.6875
train loss:  0.5918852686882019
train gradient:  0.15048390523361818
iteration : 1089
train acc:  0.6640625
train loss:  0.5886096954345703
train gradient:  0.14780543888121195
iteration : 1090
train acc:  0.7265625
train loss:  0.5497803092002869
train gradient:  0.19239453978647325
iteration : 1091
train acc:  0.609375
train loss:  0.6042187213897705
train gradient:  0.16585458470222827
iteration : 1092
train acc:  0.6796875
train loss:  0.552842378616333
train gradient:  0.16181616808143323
iteration : 1093
train acc:  0.65625
train loss:  0.5876901149749756
train gradient:  0.1724950706984223
iteration : 1094
train acc:  0.7109375
train loss:  0.573868989944458
train gradient:  0.1432502596941394
iteration : 1095
train acc:  0.6875
train loss:  0.5690600872039795
train gradient:  0.18629680167517837
iteration : 1096
train acc:  0.6640625
train loss:  0.5811512470245361
train gradient:  0.15068564367408988
iteration : 1097
train acc:  0.71875
train loss:  0.5422670841217041
train gradient:  0.18579221074249674
iteration : 1098
train acc:  0.6796875
train loss:  0.5625505447387695
train gradient:  0.13818825261822598
iteration : 1099
train acc:  0.5859375
train loss:  0.6739633083343506
train gradient:  0.19929989235770645
iteration : 1100
train acc:  0.7578125
train loss:  0.5428420901298523
train gradient:  0.2069799513065459
iteration : 1101
train acc:  0.7109375
train loss:  0.5548427104949951
train gradient:  0.17998279051326868
iteration : 1102
train acc:  0.6796875
train loss:  0.563679575920105
train gradient:  0.12846211602146496
iteration : 1103
train acc:  0.6484375
train loss:  0.5916121602058411
train gradient:  0.22113262276391146
iteration : 1104
train acc:  0.6640625
train loss:  0.6014362573623657
train gradient:  0.17626873683099314
iteration : 1105
train acc:  0.71875
train loss:  0.5369927287101746
train gradient:  0.12826680846854183
iteration : 1106
train acc:  0.7578125
train loss:  0.5107123851776123
train gradient:  0.22465273794196805
iteration : 1107
train acc:  0.6796875
train loss:  0.5942702293395996
train gradient:  0.1363634119467565
iteration : 1108
train acc:  0.671875
train loss:  0.5667825937271118
train gradient:  0.17018376768394594
iteration : 1109
train acc:  0.765625
train loss:  0.5104029178619385
train gradient:  0.10637894683553899
iteration : 1110
train acc:  0.6328125
train loss:  0.600506067276001
train gradient:  0.2246252423347877
iteration : 1111
train acc:  0.78125
train loss:  0.474205881357193
train gradient:  0.17080479913264657
iteration : 1112
train acc:  0.6796875
train loss:  0.585922122001648
train gradient:  0.15879319830032482
iteration : 1113
train acc:  0.6328125
train loss:  0.6057367324829102
train gradient:  0.169878363920611
iteration : 1114
train acc:  0.625
train loss:  0.594586968421936
train gradient:  0.1750232611166498
iteration : 1115
train acc:  0.6953125
train loss:  0.6002594232559204
train gradient:  0.20948999168844729
iteration : 1116
train acc:  0.6796875
train loss:  0.5663508176803589
train gradient:  0.15887042953623182
iteration : 1117
train acc:  0.734375
train loss:  0.5531594753265381
train gradient:  0.17612407977817188
iteration : 1118
train acc:  0.6796875
train loss:  0.5274255275726318
train gradient:  0.1603362236508315
iteration : 1119
train acc:  0.6875
train loss:  0.578217625617981
train gradient:  0.11473429302774645
iteration : 1120
train acc:  0.7421875
train loss:  0.5127529501914978
train gradient:  0.1950783706394806
iteration : 1121
train acc:  0.765625
train loss:  0.4893312454223633
train gradient:  0.18102875727804482
iteration : 1122
train acc:  0.7578125
train loss:  0.5397094488143921
train gradient:  0.15874951571823626
iteration : 1123
train acc:  0.7109375
train loss:  0.5350510478019714
train gradient:  0.13265882089680597
iteration : 1124
train acc:  0.7578125
train loss:  0.5011271834373474
train gradient:  0.15817822626589106
iteration : 1125
train acc:  0.71875
train loss:  0.5342754125595093
train gradient:  0.1734216149377189
iteration : 1126
train acc:  0.6640625
train loss:  0.6162959933280945
train gradient:  0.2075662907403708
iteration : 1127
train acc:  0.6484375
train loss:  0.6247603893280029
train gradient:  0.24545167736896897
iteration : 1128
train acc:  0.6796875
train loss:  0.5590807199478149
train gradient:  0.15995021439708115
iteration : 1129
train acc:  0.6953125
train loss:  0.5820480585098267
train gradient:  0.20488601279111388
iteration : 1130
train acc:  0.6875
train loss:  0.5826910734176636
train gradient:  0.18892963980711
iteration : 1131
train acc:  0.7421875
train loss:  0.5125232934951782
train gradient:  0.15582780871337293
iteration : 1132
train acc:  0.6953125
train loss:  0.5950509905815125
train gradient:  0.19561652941977312
iteration : 1133
train acc:  0.640625
train loss:  0.616905927658081
train gradient:  0.1664213045865338
iteration : 1134
train acc:  0.625
train loss:  0.6432645320892334
train gradient:  0.21660784051878168
iteration : 1135
train acc:  0.7265625
train loss:  0.5352555513381958
train gradient:  0.19663917987216487
iteration : 1136
train acc:  0.65625
train loss:  0.636345386505127
train gradient:  0.24834910493176432
iteration : 1137
train acc:  0.625
train loss:  0.6092983484268188
train gradient:  0.20950133261239873
iteration : 1138
train acc:  0.6796875
train loss:  0.5863449573516846
train gradient:  0.21326689149503236
iteration : 1139
train acc:  0.7109375
train loss:  0.5240386724472046
train gradient:  0.16610211103503275
iteration : 1140
train acc:  0.6796875
train loss:  0.5717278718948364
train gradient:  0.16727837065684587
iteration : 1141
train acc:  0.7109375
train loss:  0.5964231491088867
train gradient:  0.17102333359454366
iteration : 1142
train acc:  0.71875
train loss:  0.5561498403549194
train gradient:  0.21785079492294318
iteration : 1143
train acc:  0.6640625
train loss:  0.5865917801856995
train gradient:  0.19994652785353403
iteration : 1144
train acc:  0.703125
train loss:  0.5431878566741943
train gradient:  0.1983350865290463
iteration : 1145
train acc:  0.65625
train loss:  0.5797672271728516
train gradient:  0.24870286583984633
iteration : 1146
train acc:  0.6484375
train loss:  0.5856611132621765
train gradient:  0.2487270284025236
iteration : 1147
train acc:  0.6953125
train loss:  0.5604532361030579
train gradient:  0.2136715081333977
iteration : 1148
train acc:  0.6875
train loss:  0.5613703727722168
train gradient:  0.17674798583146906
iteration : 1149
train acc:  0.6796875
train loss:  0.5635042190551758
train gradient:  0.29465072879942633
iteration : 1150
train acc:  0.6640625
train loss:  0.6156887412071228
train gradient:  0.2899602073574487
iteration : 1151
train acc:  0.7421875
train loss:  0.5348376631736755
train gradient:  0.15238927843478617
iteration : 1152
train acc:  0.7578125
train loss:  0.5310674905776978
train gradient:  0.16559253898784898
iteration : 1153
train acc:  0.6953125
train loss:  0.558491587638855
train gradient:  0.14212024964742703
iteration : 1154
train acc:  0.765625
train loss:  0.5156410932540894
train gradient:  0.19033032261624855
iteration : 1155
train acc:  0.734375
train loss:  0.5543227195739746
train gradient:  0.14016126782729504
iteration : 1156
train acc:  0.6484375
train loss:  0.5977191925048828
train gradient:  0.1652414021307581
iteration : 1157
train acc:  0.65625
train loss:  0.5610643625259399
train gradient:  0.15778028595123292
iteration : 1158
train acc:  0.7265625
train loss:  0.5423813462257385
train gradient:  0.16334268113096395
iteration : 1159
train acc:  0.6953125
train loss:  0.5970923900604248
train gradient:  0.2274870610193072
iteration : 1160
train acc:  0.6953125
train loss:  0.5530589818954468
train gradient:  0.13030986728581678
iteration : 1161
train acc:  0.6875
train loss:  0.597248375415802
train gradient:  0.24815892524550842
iteration : 1162
train acc:  0.703125
train loss:  0.5603684782981873
train gradient:  0.18362156275620084
iteration : 1163
train acc:  0.6953125
train loss:  0.576564610004425
train gradient:  0.1965118270279419
iteration : 1164
train acc:  0.6015625
train loss:  0.6184391379356384
train gradient:  0.1510136211564676
iteration : 1165
train acc:  0.703125
train loss:  0.5476202964782715
train gradient:  0.21859273566905602
iteration : 1166
train acc:  0.734375
train loss:  0.555626392364502
train gradient:  0.14736962944358295
iteration : 1167
train acc:  0.6484375
train loss:  0.5991548299789429
train gradient:  0.1751366595563708
iteration : 1168
train acc:  0.7109375
train loss:  0.5462953448295593
train gradient:  0.19776008287443325
iteration : 1169
train acc:  0.625
train loss:  0.6475211381912231
train gradient:  0.22682432317392778
iteration : 1170
train acc:  0.703125
train loss:  0.5379598736763
train gradient:  0.17343721877199017
iteration : 1171
train acc:  0.59375
train loss:  0.5880717039108276
train gradient:  0.17209975608789885
iteration : 1172
train acc:  0.6953125
train loss:  0.5407755374908447
train gradient:  0.13717279618444542
iteration : 1173
train acc:  0.75
train loss:  0.5111376643180847
train gradient:  0.1622644384781115
iteration : 1174
train acc:  0.7421875
train loss:  0.5312728881835938
train gradient:  0.19868737138171477
iteration : 1175
train acc:  0.7265625
train loss:  0.5755580067634583
train gradient:  0.17560205459872075
iteration : 1176
train acc:  0.6328125
train loss:  0.6363571882247925
train gradient:  0.2542687909113128
iteration : 1177
train acc:  0.6953125
train loss:  0.5658094882965088
train gradient:  0.21756818471236483
iteration : 1178
train acc:  0.7734375
train loss:  0.5075281858444214
train gradient:  0.15429116530042328
iteration : 1179
train acc:  0.7109375
train loss:  0.5564420223236084
train gradient:  0.17299929073209103
iteration : 1180
train acc:  0.671875
train loss:  0.5926830768585205
train gradient:  0.22820242543042368
iteration : 1181
train acc:  0.6640625
train loss:  0.6124597787857056
train gradient:  0.17597582009371224
iteration : 1182
train acc:  0.7109375
train loss:  0.5246022939682007
train gradient:  0.12396405523393865
iteration : 1183
train acc:  0.71875
train loss:  0.532869815826416
train gradient:  0.13306375875470544
iteration : 1184
train acc:  0.625
train loss:  0.6502432227134705
train gradient:  0.17963073853571904
iteration : 1185
train acc:  0.671875
train loss:  0.5699320435523987
train gradient:  0.2374894141136784
iteration : 1186
train acc:  0.7578125
train loss:  0.5125375986099243
train gradient:  0.1479257528598375
iteration : 1187
train acc:  0.7265625
train loss:  0.5099826455116272
train gradient:  0.14973179992750496
iteration : 1188
train acc:  0.640625
train loss:  0.5619677901268005
train gradient:  0.14878073769038536
iteration : 1189
train acc:  0.703125
train loss:  0.6011512279510498
train gradient:  0.17849016225126257
iteration : 1190
train acc:  0.65625
train loss:  0.6142306327819824
train gradient:  0.19095127818937124
iteration : 1191
train acc:  0.6328125
train loss:  0.6411094069480896
train gradient:  0.2694543728023501
iteration : 1192
train acc:  0.703125
train loss:  0.5720081925392151
train gradient:  0.15705941418899783
iteration : 1193
train acc:  0.71875
train loss:  0.545312225818634
train gradient:  0.1438808238205376
iteration : 1194
train acc:  0.71875
train loss:  0.5331209897994995
train gradient:  0.1670203267620543
iteration : 1195
train acc:  0.65625
train loss:  0.6250159740447998
train gradient:  0.21739870669642258
iteration : 1196
train acc:  0.6328125
train loss:  0.6556916236877441
train gradient:  0.21150055789824546
iteration : 1197
train acc:  0.6875
train loss:  0.5817046165466309
train gradient:  0.16538548884661253
iteration : 1198
train acc:  0.71875
train loss:  0.6263126134872437
train gradient:  0.23592362821957374
iteration : 1199
train acc:  0.71875
train loss:  0.5279890298843384
train gradient:  0.1489221833478143
iteration : 1200
train acc:  0.671875
train loss:  0.6150928139686584
train gradient:  0.2024837026621392
iteration : 1201
train acc:  0.765625
train loss:  0.5304640531539917
train gradient:  0.15477817975116995
iteration : 1202
train acc:  0.671875
train loss:  0.5339800119400024
train gradient:  0.17852292410678255
iteration : 1203
train acc:  0.6484375
train loss:  0.5678689479827881
train gradient:  0.19570477379718876
iteration : 1204
train acc:  0.6796875
train loss:  0.5313820838928223
train gradient:  0.12945736374148578
iteration : 1205
train acc:  0.671875
train loss:  0.5706081390380859
train gradient:  0.14467936522062066
iteration : 1206
train acc:  0.75
train loss:  0.5094324350357056
train gradient:  0.14787753721000607
iteration : 1207
train acc:  0.6640625
train loss:  0.5770843029022217
train gradient:  0.20847969375894118
iteration : 1208
train acc:  0.6953125
train loss:  0.5394243001937866
train gradient:  0.17897676458402084
iteration : 1209
train acc:  0.6875
train loss:  0.5821955800056458
train gradient:  0.2010945232278274
iteration : 1210
train acc:  0.6640625
train loss:  0.5732525587081909
train gradient:  0.20066515535872403
iteration : 1211
train acc:  0.65625
train loss:  0.6059527397155762
train gradient:  0.18528650261230534
iteration : 1212
train acc:  0.6171875
train loss:  0.645575761795044
train gradient:  0.21467586950734316
iteration : 1213
train acc:  0.7265625
train loss:  0.5296755433082581
train gradient:  0.17436917047743836
iteration : 1214
train acc:  0.7578125
train loss:  0.5221605896949768
train gradient:  0.15270925215840886
iteration : 1215
train acc:  0.7109375
train loss:  0.548080325126648
train gradient:  0.1570437561736562
iteration : 1216
train acc:  0.71875
train loss:  0.5385802388191223
train gradient:  0.15121914009551818
iteration : 1217
train acc:  0.6796875
train loss:  0.5804515480995178
train gradient:  0.16693883846088087
iteration : 1218
train acc:  0.6171875
train loss:  0.6296337842941284
train gradient:  0.1873413721550054
iteration : 1219
train acc:  0.625
train loss:  0.6039237976074219
train gradient:  0.16499115536327832
iteration : 1220
train acc:  0.71875
train loss:  0.5494757294654846
train gradient:  0.16660934317250847
iteration : 1221
train acc:  0.75
train loss:  0.5254709124565125
train gradient:  0.17502490952275215
iteration : 1222
train acc:  0.609375
train loss:  0.6293632984161377
train gradient:  0.26298449349062214
iteration : 1223
train acc:  0.6328125
train loss:  0.5968286991119385
train gradient:  0.19897806837092713
iteration : 1224
train acc:  0.671875
train loss:  0.5905206203460693
train gradient:  0.19647448586433428
iteration : 1225
train acc:  0.6953125
train loss:  0.5745062828063965
train gradient:  0.16200300347723517
iteration : 1226
train acc:  0.6796875
train loss:  0.5747570991516113
train gradient:  0.15761089129745298
iteration : 1227
train acc:  0.7109375
train loss:  0.5352433323860168
train gradient:  0.19452769788733865
iteration : 1228
train acc:  0.671875
train loss:  0.5609211325645447
train gradient:  0.17371422557974495
iteration : 1229
train acc:  0.734375
train loss:  0.5096882581710815
train gradient:  0.11525441214183098
iteration : 1230
train acc:  0.7109375
train loss:  0.5703459978103638
train gradient:  0.1432279933182988
iteration : 1231
train acc:  0.671875
train loss:  0.6092917919158936
train gradient:  0.1522042170019003
iteration : 1232
train acc:  0.671875
train loss:  0.591599702835083
train gradient:  0.1403567966698302
iteration : 1233
train acc:  0.6171875
train loss:  0.6311572790145874
train gradient:  0.2072019916090857
iteration : 1234
train acc:  0.75
train loss:  0.5611268877983093
train gradient:  0.19170686982799967
iteration : 1235
train acc:  0.703125
train loss:  0.5475742220878601
train gradient:  0.15282381438964382
iteration : 1236
train acc:  0.765625
train loss:  0.506423830986023
train gradient:  0.13476002435831408
iteration : 1237
train acc:  0.7734375
train loss:  0.5025089979171753
train gradient:  0.16357876251117587
iteration : 1238
train acc:  0.640625
train loss:  0.5647677183151245
train gradient:  0.15131556518586498
iteration : 1239
train acc:  0.6171875
train loss:  0.6411406397819519
train gradient:  0.2591765574895128
iteration : 1240
train acc:  0.6484375
train loss:  0.6434817910194397
train gradient:  0.18963403694885314
iteration : 1241
train acc:  0.7265625
train loss:  0.5805383920669556
train gradient:  0.16024952698846592
iteration : 1242
train acc:  0.7109375
train loss:  0.5975114107131958
train gradient:  0.19947792318573795
iteration : 1243
train acc:  0.7109375
train loss:  0.5457943677902222
train gradient:  0.13812779255371352
iteration : 1244
train acc:  0.7265625
train loss:  0.5156092643737793
train gradient:  0.15854749231389403
iteration : 1245
train acc:  0.671875
train loss:  0.5898376107215881
train gradient:  0.21651975640848148
iteration : 1246
train acc:  0.6796875
train loss:  0.5836104154586792
train gradient:  0.15637418335159042
iteration : 1247
train acc:  0.65625
train loss:  0.6101810336112976
train gradient:  0.1869101298557253
iteration : 1248
train acc:  0.7265625
train loss:  0.5316143035888672
train gradient:  0.1753454362453007
iteration : 1249
train acc:  0.7734375
train loss:  0.5473192930221558
train gradient:  0.19513419328449866
iteration : 1250
train acc:  0.671875
train loss:  0.5546689629554749
train gradient:  0.1452229174544915
iteration : 1251
train acc:  0.7734375
train loss:  0.5186434388160706
train gradient:  0.10150920161854007
iteration : 1252
train acc:  0.71875
train loss:  0.5475395917892456
train gradient:  0.14329403884305683
iteration : 1253
train acc:  0.6875
train loss:  0.5374174118041992
train gradient:  0.12859936124161767
iteration : 1254
train acc:  0.6796875
train loss:  0.614226222038269
train gradient:  0.36930254246035954
iteration : 1255
train acc:  0.7109375
train loss:  0.5527975559234619
train gradient:  0.14161217327458397
iteration : 1256
train acc:  0.65625
train loss:  0.5933635234832764
train gradient:  0.18264986927115284
iteration : 1257
train acc:  0.7265625
train loss:  0.5477775931358337
train gradient:  0.1633620672141928
iteration : 1258
train acc:  0.7421875
train loss:  0.5661048293113708
train gradient:  0.21517568874858362
iteration : 1259
train acc:  0.6484375
train loss:  0.582496166229248
train gradient:  0.15855373364618341
iteration : 1260
train acc:  0.75
train loss:  0.4751426577568054
train gradient:  0.24819777359556167
iteration : 1261
train acc:  0.7109375
train loss:  0.5561398863792419
train gradient:  0.20585715503915086
iteration : 1262
train acc:  0.6328125
train loss:  0.6307413578033447
train gradient:  0.1637419392998979
iteration : 1263
train acc:  0.7109375
train loss:  0.5346556901931763
train gradient:  0.19510048775185906
iteration : 1264
train acc:  0.7578125
train loss:  0.5315648317337036
train gradient:  0.14911240764309566
iteration : 1265
train acc:  0.7578125
train loss:  0.5162601470947266
train gradient:  0.18848386190123628
iteration : 1266
train acc:  0.671875
train loss:  0.5774060487747192
train gradient:  0.18318722467789098
iteration : 1267
train acc:  0.7109375
train loss:  0.5642123222351074
train gradient:  0.1486911706792421
iteration : 1268
train acc:  0.71875
train loss:  0.5670361518859863
train gradient:  0.16485562852253408
iteration : 1269
train acc:  0.6796875
train loss:  0.5396252870559692
train gradient:  0.1833729824924837
iteration : 1270
train acc:  0.71875
train loss:  0.6147769093513489
train gradient:  0.178786805762825
iteration : 1271
train acc:  0.71875
train loss:  0.5410791635513306
train gradient:  0.19244169434272201
iteration : 1272
train acc:  0.6484375
train loss:  0.6124516725540161
train gradient:  0.2115446887022719
iteration : 1273
train acc:  0.71875
train loss:  0.5662548542022705
train gradient:  0.19629305647833908
iteration : 1274
train acc:  0.703125
train loss:  0.5300683379173279
train gradient:  0.12724253728437623
iteration : 1275
train acc:  0.6875
train loss:  0.5577301979064941
train gradient:  0.17079251852012078
iteration : 1276
train acc:  0.6875
train loss:  0.6041522026062012
train gradient:  0.16430008865774354
iteration : 1277
train acc:  0.6484375
train loss:  0.6148138046264648
train gradient:  0.20438499080804307
iteration : 1278
train acc:  0.6953125
train loss:  0.5971488356590271
train gradient:  0.1844222807281073
iteration : 1279
train acc:  0.6875
train loss:  0.526719331741333
train gradient:  0.1838170082157669
iteration : 1280
train acc:  0.7578125
train loss:  0.4861254096031189
train gradient:  0.20124998610978567
iteration : 1281
train acc:  0.6953125
train loss:  0.580869197845459
train gradient:  0.14408784193729207
iteration : 1282
train acc:  0.71875
train loss:  0.5397859811782837
train gradient:  0.1510201252320657
iteration : 1283
train acc:  0.6328125
train loss:  0.6303415298461914
train gradient:  0.2183894078021516
iteration : 1284
train acc:  0.6875
train loss:  0.5491304397583008
train gradient:  0.13498890166419614
iteration : 1285
train acc:  0.6953125
train loss:  0.536032497882843
train gradient:  0.15095024836360796
iteration : 1286
train acc:  0.6640625
train loss:  0.6071664690971375
train gradient:  0.19153558392465253
iteration : 1287
train acc:  0.703125
train loss:  0.5449492335319519
train gradient:  0.15882477074781873
iteration : 1288
train acc:  0.7890625
train loss:  0.4930303394794464
train gradient:  0.13823905017578048
iteration : 1289
train acc:  0.7265625
train loss:  0.49320852756500244
train gradient:  0.20271906415824695
iteration : 1290
train acc:  0.734375
train loss:  0.5117212533950806
train gradient:  0.15063624722742247
iteration : 1291
train acc:  0.6875
train loss:  0.5671112537384033
train gradient:  0.1529708941688551
iteration : 1292
train acc:  0.7109375
train loss:  0.5722934007644653
train gradient:  0.19262842047724058
iteration : 1293
train acc:  0.734375
train loss:  0.5236537456512451
train gradient:  0.1925140493232914
iteration : 1294
train acc:  0.71875
train loss:  0.5572860836982727
train gradient:  0.14104931176463176
iteration : 1295
train acc:  0.703125
train loss:  0.551677942276001
train gradient:  0.18919086493332737
iteration : 1296
train acc:  0.7421875
train loss:  0.5414035320281982
train gradient:  0.13981063088775994
iteration : 1297
train acc:  0.6953125
train loss:  0.5731481313705444
train gradient:  0.18311224359438605
iteration : 1298
train acc:  0.71875
train loss:  0.547615110874176
train gradient:  0.1921872072179429
iteration : 1299
train acc:  0.6875
train loss:  0.5445367097854614
train gradient:  0.1904682096404968
iteration : 1300
train acc:  0.6640625
train loss:  0.5829007625579834
train gradient:  0.28326601187937744
iteration : 1301
train acc:  0.78125
train loss:  0.4998694658279419
train gradient:  0.16800418381693896
iteration : 1302
train acc:  0.6796875
train loss:  0.5816564559936523
train gradient:  0.18979255013212115
iteration : 1303
train acc:  0.6796875
train loss:  0.5383787155151367
train gradient:  0.15448506974300763
iteration : 1304
train acc:  0.6875
train loss:  0.5595399141311646
train gradient:  0.17669706516831607
iteration : 1305
train acc:  0.6953125
train loss:  0.5703345537185669
train gradient:  0.17825932595566604
iteration : 1306
train acc:  0.734375
train loss:  0.5376603007316589
train gradient:  0.18266954992988244
iteration : 1307
train acc:  0.71875
train loss:  0.5223070979118347
train gradient:  0.21335099629037976
iteration : 1308
train acc:  0.7109375
train loss:  0.5326821208000183
train gradient:  0.16652987646833983
iteration : 1309
train acc:  0.6171875
train loss:  0.6313363313674927
train gradient:  0.18227706879288977
iteration : 1310
train acc:  0.6875
train loss:  0.5656278729438782
train gradient:  0.1769398778963196
iteration : 1311
train acc:  0.640625
train loss:  0.5853340029716492
train gradient:  0.1838372778933376
iteration : 1312
train acc:  0.765625
train loss:  0.49372342228889465
train gradient:  0.11565974484110476
iteration : 1313
train acc:  0.7265625
train loss:  0.5280129909515381
train gradient:  0.1847344511376435
iteration : 1314
train acc:  0.7109375
train loss:  0.5677064657211304
train gradient:  0.22854865385031323
iteration : 1315
train acc:  0.7421875
train loss:  0.5021637678146362
train gradient:  0.14069561699967353
iteration : 1316
train acc:  0.6953125
train loss:  0.570748507976532
train gradient:  0.17282457713201288
iteration : 1317
train acc:  0.6484375
train loss:  0.5994051694869995
train gradient:  0.14951395079559765
iteration : 1318
train acc:  0.7109375
train loss:  0.5394271612167358
train gradient:  0.15092405170408557
iteration : 1319
train acc:  0.671875
train loss:  0.5529725551605225
train gradient:  0.2766621965889564
iteration : 1320
train acc:  0.7265625
train loss:  0.5640982985496521
train gradient:  0.20510577383855805
iteration : 1321
train acc:  0.7421875
train loss:  0.5158408880233765
train gradient:  0.12779882184960653
iteration : 1322
train acc:  0.6484375
train loss:  0.5930836200714111
train gradient:  0.2935020044421777
iteration : 1323
train acc:  0.703125
train loss:  0.6135812997817993
train gradient:  0.21881886945488627
iteration : 1324
train acc:  0.6328125
train loss:  0.58060622215271
train gradient:  0.17699218908827863
iteration : 1325
train acc:  0.6640625
train loss:  0.6107845306396484
train gradient:  0.22249309741729179
iteration : 1326
train acc:  0.6015625
train loss:  0.643441915512085
train gradient:  0.22767496160923661
iteration : 1327
train acc:  0.6875
train loss:  0.6139536499977112
train gradient:  0.20900355141886479
iteration : 1328
train acc:  0.6484375
train loss:  0.5975650548934937
train gradient:  0.21713443428568718
iteration : 1329
train acc:  0.6796875
train loss:  0.5490565896034241
train gradient:  0.14480869626816595
iteration : 1330
train acc:  0.6953125
train loss:  0.5330438613891602
train gradient:  0.1518055179730124
iteration : 1331
train acc:  0.65625
train loss:  0.6151483654975891
train gradient:  0.18900735426957727
iteration : 1332
train acc:  0.7109375
train loss:  0.5476045608520508
train gradient:  0.16332324010774213
iteration : 1333
train acc:  0.65625
train loss:  0.5791077613830566
train gradient:  0.19521980327190439
iteration : 1334
train acc:  0.7578125
train loss:  0.5342538356781006
train gradient:  0.16336126867248213
iteration : 1335
train acc:  0.734375
train loss:  0.5269038677215576
train gradient:  0.21451165154680257
iteration : 1336
train acc:  0.7578125
train loss:  0.5200090408325195
train gradient:  0.21787713211701493
iteration : 1337
train acc:  0.7109375
train loss:  0.5415852069854736
train gradient:  0.18545553471861337
iteration : 1338
train acc:  0.734375
train loss:  0.5262538194656372
train gradient:  0.1272804236740445
iteration : 1339
train acc:  0.6640625
train loss:  0.5517716407775879
train gradient:  0.14045637509093373
iteration : 1340
train acc:  0.765625
train loss:  0.5081058740615845
train gradient:  0.14989147578937972
iteration : 1341
train acc:  0.7578125
train loss:  0.48945218324661255
train gradient:  0.15788993916126423
iteration : 1342
train acc:  0.6875
train loss:  0.5662646889686584
train gradient:  0.1875363133810933
iteration : 1343
train acc:  0.7109375
train loss:  0.5617085695266724
train gradient:  0.1888722014051933
iteration : 1344
train acc:  0.6171875
train loss:  0.6561644673347473
train gradient:  0.3741727230809563
iteration : 1345
train acc:  0.75
train loss:  0.5444478988647461
train gradient:  0.14524534873101097
iteration : 1346
train acc:  0.671875
train loss:  0.5851659774780273
train gradient:  0.16232797691429493
iteration : 1347
train acc:  0.65625
train loss:  0.5714306831359863
train gradient:  0.17063838095654552
iteration : 1348
train acc:  0.75
train loss:  0.5560891628265381
train gradient:  0.16051478812505074
iteration : 1349
train acc:  0.6796875
train loss:  0.5633389949798584
train gradient:  0.17956584984556728
iteration : 1350
train acc:  0.6953125
train loss:  0.5885958671569824
train gradient:  0.1805750626507654
iteration : 1351
train acc:  0.7109375
train loss:  0.571549117565155
train gradient:  0.1567490689101957
iteration : 1352
train acc:  0.703125
train loss:  0.5758847594261169
train gradient:  0.24170998063443178
iteration : 1353
train acc:  0.7265625
train loss:  0.562764048576355
train gradient:  0.14103248155462772
iteration : 1354
train acc:  0.6640625
train loss:  0.5960561037063599
train gradient:  0.16842459026895276
iteration : 1355
train acc:  0.71875
train loss:  0.520539402961731
train gradient:  0.15540190984622304
iteration : 1356
train acc:  0.6796875
train loss:  0.595993161201477
train gradient:  0.25148699738226044
iteration : 1357
train acc:  0.75
train loss:  0.49329450726509094
train gradient:  0.13181657149329468
iteration : 1358
train acc:  0.703125
train loss:  0.5419692993164062
train gradient:  0.1826445883101911
iteration : 1359
train acc:  0.7109375
train loss:  0.5172861814498901
train gradient:  0.11685582372508055
iteration : 1360
train acc:  0.7109375
train loss:  0.5648379325866699
train gradient:  0.17906132385863302
iteration : 1361
train acc:  0.7578125
train loss:  0.5321565866470337
train gradient:  0.1625676216186001
iteration : 1362
train acc:  0.6953125
train loss:  0.592723548412323
train gradient:  0.22330523250326584
iteration : 1363
train acc:  0.703125
train loss:  0.5711233615875244
train gradient:  0.1977733876050069
iteration : 1364
train acc:  0.703125
train loss:  0.5537164211273193
train gradient:  0.2657852983785283
iteration : 1365
train acc:  0.640625
train loss:  0.6557794809341431
train gradient:  0.39702522887100844
iteration : 1366
train acc:  0.703125
train loss:  0.5615708231925964
train gradient:  0.19213158510320594
iteration : 1367
train acc:  0.7421875
train loss:  0.5339782238006592
train gradient:  0.231787963310972
iteration : 1368
train acc:  0.6640625
train loss:  0.5895163416862488
train gradient:  0.18229466406732672
iteration : 1369
train acc:  0.7578125
train loss:  0.5636298656463623
train gradient:  0.1826008075025871
iteration : 1370
train acc:  0.6171875
train loss:  0.598482608795166
train gradient:  0.18767097665657795
iteration : 1371
train acc:  0.6953125
train loss:  0.5921372771263123
train gradient:  0.1719366289627125
iteration : 1372
train acc:  0.6640625
train loss:  0.5834989547729492
train gradient:  0.18163965553696537
iteration : 1373
train acc:  0.796875
train loss:  0.4742373824119568
train gradient:  0.14106684432383063
iteration : 1374
train acc:  0.7109375
train loss:  0.5724805593490601
train gradient:  0.20334474946919684
iteration : 1375
train acc:  0.7109375
train loss:  0.5428692698478699
train gradient:  0.14595591464368857
iteration : 1376
train acc:  0.6640625
train loss:  0.573015034198761
train gradient:  0.17743017256245752
iteration : 1377
train acc:  0.6953125
train loss:  0.5714544653892517
train gradient:  0.14395961273608143
iteration : 1378
train acc:  0.7421875
train loss:  0.549301028251648
train gradient:  0.23165381118034023
iteration : 1379
train acc:  0.7265625
train loss:  0.5356082916259766
train gradient:  0.17691879192482635
iteration : 1380
train acc:  0.71875
train loss:  0.5397607088088989
train gradient:  0.19044675404632536
iteration : 1381
train acc:  0.7734375
train loss:  0.501817524433136
train gradient:  0.13781687751829522
iteration : 1382
train acc:  0.7109375
train loss:  0.5430099964141846
train gradient:  0.141884210204007
iteration : 1383
train acc:  0.65625
train loss:  0.632759690284729
train gradient:  0.2504182202529406
iteration : 1384
train acc:  0.703125
train loss:  0.5620089173316956
train gradient:  0.22393840704120488
iteration : 1385
train acc:  0.71875
train loss:  0.5294036269187927
train gradient:  0.14889469802266897
iteration : 1386
train acc:  0.6484375
train loss:  0.5567414164543152
train gradient:  0.16820534721856234
iteration : 1387
train acc:  0.6875
train loss:  0.5632513165473938
train gradient:  0.14728723707842448
iteration : 1388
train acc:  0.7265625
train loss:  0.5457269549369812
train gradient:  0.1339242031243169
iteration : 1389
train acc:  0.6953125
train loss:  0.5808294415473938
train gradient:  0.18611635231183163
iteration : 1390
train acc:  0.65625
train loss:  0.5630475282669067
train gradient:  0.1547535694689674
iteration : 1391
train acc:  0.71875
train loss:  0.5596872568130493
train gradient:  0.21802766775268445
iteration : 1392
train acc:  0.671875
train loss:  0.569391667842865
train gradient:  0.15501564846934834
iteration : 1393
train acc:  0.7265625
train loss:  0.5583780407905579
train gradient:  0.23732628944039585
iteration : 1394
train acc:  0.75
train loss:  0.5164597034454346
train gradient:  0.14001157644830026
iteration : 1395
train acc:  0.7421875
train loss:  0.5758860111236572
train gradient:  0.1660483076769963
iteration : 1396
train acc:  0.640625
train loss:  0.6290194988250732
train gradient:  0.19148038959381597
iteration : 1397
train acc:  0.7265625
train loss:  0.5443799495697021
train gradient:  0.19633594517253367
iteration : 1398
train acc:  0.6796875
train loss:  0.5682612061500549
train gradient:  0.18194772883701305
iteration : 1399
train acc:  0.7265625
train loss:  0.5245994329452515
train gradient:  0.16465515005853917
iteration : 1400
train acc:  0.6875
train loss:  0.5921790599822998
train gradient:  0.17995019746548518
iteration : 1401
train acc:  0.71875
train loss:  0.5689380168914795
train gradient:  0.2052363485705515
iteration : 1402
train acc:  0.640625
train loss:  0.6482330560684204
train gradient:  0.18107560876381862
iteration : 1403
train acc:  0.625
train loss:  0.6284441947937012
train gradient:  0.17476178299718637
iteration : 1404
train acc:  0.671875
train loss:  0.5786159634590149
train gradient:  0.2029879133066903
iteration : 1405
train acc:  0.6640625
train loss:  0.5375787019729614
train gradient:  0.15130274025996573
iteration : 1406
train acc:  0.6796875
train loss:  0.5484650135040283
train gradient:  0.1284923467554562
iteration : 1407
train acc:  0.734375
train loss:  0.5347490310668945
train gradient:  0.18306289531597308
iteration : 1408
train acc:  0.734375
train loss:  0.5271316170692444
train gradient:  0.20319050592966098
iteration : 1409
train acc:  0.7265625
train loss:  0.5410021543502808
train gradient:  0.2061941646153168
iteration : 1410
train acc:  0.71875
train loss:  0.4944254159927368
train gradient:  0.1305740798674476
iteration : 1411
train acc:  0.671875
train loss:  0.5719926953315735
train gradient:  0.21736903102976496
iteration : 1412
train acc:  0.6328125
train loss:  0.5952348113059998
train gradient:  0.16617127407820664
iteration : 1413
train acc:  0.6875
train loss:  0.5694282054901123
train gradient:  0.16338292084014056
iteration : 1414
train acc:  0.7421875
train loss:  0.5352953672409058
train gradient:  0.1552106971141361
iteration : 1415
train acc:  0.6875
train loss:  0.5841265916824341
train gradient:  0.16000350334603683
iteration : 1416
train acc:  0.6796875
train loss:  0.5465079545974731
train gradient:  0.16312155409541074
iteration : 1417
train acc:  0.703125
train loss:  0.5411365628242493
train gradient:  0.12898882033649822
iteration : 1418
train acc:  0.671875
train loss:  0.593915581703186
train gradient:  0.17592874508311304
iteration : 1419
train acc:  0.6953125
train loss:  0.5468752980232239
train gradient:  0.21350124602646342
iteration : 1420
train acc:  0.703125
train loss:  0.5300784111022949
train gradient:  0.18255992427240397
iteration : 1421
train acc:  0.6796875
train loss:  0.5961878299713135
train gradient:  0.19420685295220835
iteration : 1422
train acc:  0.71875
train loss:  0.5618536472320557
train gradient:  0.18050122893450432
iteration : 1423
train acc:  0.6875
train loss:  0.5843213796615601
train gradient:  0.25935226637712594
iteration : 1424
train acc:  0.71875
train loss:  0.549249529838562
train gradient:  0.13946366268819252
iteration : 1425
train acc:  0.671875
train loss:  0.5439887046813965
train gradient:  0.1336834195323552
iteration : 1426
train acc:  0.6640625
train loss:  0.6081966161727905
train gradient:  0.1730056936508959
iteration : 1427
train acc:  0.7109375
train loss:  0.5277693271636963
train gradient:  0.11878912640709138
iteration : 1428
train acc:  0.671875
train loss:  0.6248373985290527
train gradient:  0.184823616041145
iteration : 1429
train acc:  0.6875
train loss:  0.556847095489502
train gradient:  0.12864040558528267
iteration : 1430
train acc:  0.6953125
train loss:  0.5794738531112671
train gradient:  0.16999920685774167
iteration : 1431
train acc:  0.6484375
train loss:  0.612070620059967
train gradient:  0.28704929546794056
iteration : 1432
train acc:  0.7265625
train loss:  0.5128138065338135
train gradient:  0.16925782192442618
iteration : 1433
train acc:  0.65625
train loss:  0.5586428642272949
train gradient:  0.17724504166082328
iteration : 1434
train acc:  0.71875
train loss:  0.5311190485954285
train gradient:  0.13502710108478805
iteration : 1435
train acc:  0.7265625
train loss:  0.5245267152786255
train gradient:  0.1659491922665343
iteration : 1436
train acc:  0.6796875
train loss:  0.5666309595108032
train gradient:  0.16514599017727355
iteration : 1437
train acc:  0.7265625
train loss:  0.5321468114852905
train gradient:  0.13786644454992347
iteration : 1438
train acc:  0.6875
train loss:  0.5832304954528809
train gradient:  0.18648231541616367
iteration : 1439
train acc:  0.75
train loss:  0.5488475561141968
train gradient:  0.20095129709635917
iteration : 1440
train acc:  0.65625
train loss:  0.5990726947784424
train gradient:  0.2155774840273825
iteration : 1441
train acc:  0.6640625
train loss:  0.5820536613464355
train gradient:  0.153486738043631
iteration : 1442
train acc:  0.6484375
train loss:  0.6093116998672485
train gradient:  0.1797556591700698
iteration : 1443
train acc:  0.6953125
train loss:  0.56929612159729
train gradient:  0.1748551086058596
iteration : 1444
train acc:  0.6484375
train loss:  0.607820987701416
train gradient:  0.1869227753394842
iteration : 1445
train acc:  0.7109375
train loss:  0.6533457040786743
train gradient:  0.23923356501004553
iteration : 1446
train acc:  0.6875
train loss:  0.5483399629592896
train gradient:  0.1720389527540399
iteration : 1447
train acc:  0.6484375
train loss:  0.5657652616500854
train gradient:  0.1599261630336887
iteration : 1448
train acc:  0.7421875
train loss:  0.5301171541213989
train gradient:  0.20229407922743559
iteration : 1449
train acc:  0.7421875
train loss:  0.5716489553451538
train gradient:  0.13555382504167313
iteration : 1450
train acc:  0.7265625
train loss:  0.5930764079093933
train gradient:  0.17946055231568997
iteration : 1451
train acc:  0.703125
train loss:  0.5534383058547974
train gradient:  0.1442334578508716
iteration : 1452
train acc:  0.71875
train loss:  0.5323776006698608
train gradient:  0.13530487792945162
iteration : 1453
train acc:  0.65625
train loss:  0.5633915662765503
train gradient:  0.19178596134045758
iteration : 1454
train acc:  0.6796875
train loss:  0.5841565132141113
train gradient:  0.2013649219056508
iteration : 1455
train acc:  0.6796875
train loss:  0.5771891474723816
train gradient:  0.20608260122227584
iteration : 1456
train acc:  0.71875
train loss:  0.5255410671234131
train gradient:  0.13762482093092399
iteration : 1457
train acc:  0.671875
train loss:  0.6057572364807129
train gradient:  0.18277300660503049
iteration : 1458
train acc:  0.6875
train loss:  0.5507705807685852
train gradient:  0.21772291695398394
iteration : 1459
train acc:  0.7265625
train loss:  0.538927435874939
train gradient:  0.16862678689703686
iteration : 1460
train acc:  0.71875
train loss:  0.5277602672576904
train gradient:  0.14013299987832328
iteration : 1461
train acc:  0.7578125
train loss:  0.5058748722076416
train gradient:  0.14759444612560874
iteration : 1462
train acc:  0.65625
train loss:  0.5693053007125854
train gradient:  0.16045733540132806
iteration : 1463
train acc:  0.6640625
train loss:  0.559507429599762
train gradient:  0.16359099859241544
iteration : 1464
train acc:  0.6953125
train loss:  0.5811868906021118
train gradient:  0.19315487078006194
iteration : 1465
train acc:  0.703125
train loss:  0.5456469655036926
train gradient:  0.1540833920704533
iteration : 1466
train acc:  0.7734375
train loss:  0.5361026525497437
train gradient:  0.1239061480800235
iteration : 1467
train acc:  0.6796875
train loss:  0.61285799741745
train gradient:  0.1930199465098065
iteration : 1468
train acc:  0.7421875
train loss:  0.5395992994308472
train gradient:  0.17623746146489772
iteration : 1469
train acc:  0.7265625
train loss:  0.5227642059326172
train gradient:  0.1301743037389266
iteration : 1470
train acc:  0.7890625
train loss:  0.49102237820625305
train gradient:  0.14487203696398857
iteration : 1471
train acc:  0.7109375
train loss:  0.5690522193908691
train gradient:  0.17165571635088292
iteration : 1472
train acc:  0.765625
train loss:  0.5433762073516846
train gradient:  0.12595149328357252
iteration : 1473
train acc:  0.6328125
train loss:  0.5848442316055298
train gradient:  0.16086570440164727
iteration : 1474
train acc:  0.6953125
train loss:  0.5574454069137573
train gradient:  0.16543924506180785
iteration : 1475
train acc:  0.7109375
train loss:  0.4956037998199463
train gradient:  0.13549184430634237
iteration : 1476
train acc:  0.640625
train loss:  0.5854213237762451
train gradient:  0.14646416490820288
iteration : 1477
train acc:  0.703125
train loss:  0.5488625168800354
train gradient:  0.1554898833654293
iteration : 1478
train acc:  0.7890625
train loss:  0.4969828724861145
train gradient:  0.12399103257563336
iteration : 1479
train acc:  0.625
train loss:  0.6028672456741333
train gradient:  0.15289544258763133
iteration : 1480
train acc:  0.7421875
train loss:  0.5153369903564453
train gradient:  0.16205474363300604
iteration : 1481
train acc:  0.7421875
train loss:  0.5056793093681335
train gradient:  0.14779097793309381
iteration : 1482
train acc:  0.671875
train loss:  0.5668133497238159
train gradient:  0.1418777287683173
iteration : 1483
train acc:  0.703125
train loss:  0.5586138367652893
train gradient:  0.1789278767239239
iteration : 1484
train acc:  0.7109375
train loss:  0.5314267873764038
train gradient:  0.13046173391441512
iteration : 1485
train acc:  0.765625
train loss:  0.5293552875518799
train gradient:  0.18251656784081977
iteration : 1486
train acc:  0.7578125
train loss:  0.4769522547721863
train gradient:  0.16849622684446147
iteration : 1487
train acc:  0.6484375
train loss:  0.5880603790283203
train gradient:  0.20531505930537602
iteration : 1488
train acc:  0.71875
train loss:  0.5816202163696289
train gradient:  0.21901304122501028
iteration : 1489
train acc:  0.6953125
train loss:  0.5248180031776428
train gradient:  0.11333894005940715
iteration : 1490
train acc:  0.6953125
train loss:  0.5766432285308838
train gradient:  0.17571472941760533
iteration : 1491
train acc:  0.6640625
train loss:  0.5886284112930298
train gradient:  0.17898661126009296
iteration : 1492
train acc:  0.65625
train loss:  0.6044337153434753
train gradient:  0.17396505071264545
iteration : 1493
train acc:  0.7421875
train loss:  0.5034342408180237
train gradient:  0.2286521567888206
iteration : 1494
train acc:  0.65625
train loss:  0.6007204651832581
train gradient:  0.16192422137640475
iteration : 1495
train acc:  0.6796875
train loss:  0.575282096862793
train gradient:  0.16302693104873162
iteration : 1496
train acc:  0.7109375
train loss:  0.5535417199134827
train gradient:  0.16246557946410123
iteration : 1497
train acc:  0.71875
train loss:  0.5540889501571655
train gradient:  0.16791378414813504
iteration : 1498
train acc:  0.78125
train loss:  0.5040085911750793
train gradient:  0.14153065630716408
iteration : 1499
train acc:  0.7109375
train loss:  0.5637452006340027
train gradient:  0.16681116031198195
iteration : 1500
train acc:  0.7421875
train loss:  0.5277599692344666
train gradient:  0.1702020111232122
iteration : 1501
train acc:  0.6484375
train loss:  0.5912443399429321
train gradient:  0.20384366764990852
iteration : 1502
train acc:  0.7265625
train loss:  0.5581298470497131
train gradient:  0.18751293188599702
iteration : 1503
train acc:  0.6875
train loss:  0.6050177812576294
train gradient:  0.16323218059930722
iteration : 1504
train acc:  0.6328125
train loss:  0.630535364151001
train gradient:  0.29638682178750697
iteration : 1505
train acc:  0.6953125
train loss:  0.6039795279502869
train gradient:  0.16597196681430423
iteration : 1506
train acc:  0.7265625
train loss:  0.5428982377052307
train gradient:  0.20113362192107764
iteration : 1507
train acc:  0.75
train loss:  0.5166524648666382
train gradient:  0.1258466891958554
iteration : 1508
train acc:  0.6875
train loss:  0.5866173505783081
train gradient:  0.20326419979946125
iteration : 1509
train acc:  0.65625
train loss:  0.5977633595466614
train gradient:  0.1846661393100536
iteration : 1510
train acc:  0.640625
train loss:  0.6203204393386841
train gradient:  0.2042311776864738
iteration : 1511
train acc:  0.6875
train loss:  0.6095824241638184
train gradient:  0.16159444136951015
iteration : 1512
train acc:  0.65625
train loss:  0.607958197593689
train gradient:  0.21726715748880032
iteration : 1513
train acc:  0.75
train loss:  0.5443980097770691
train gradient:  0.14964845139995203
iteration : 1514
train acc:  0.7265625
train loss:  0.5513112545013428
train gradient:  0.1264768686948758
iteration : 1515
train acc:  0.625
train loss:  0.5846033096313477
train gradient:  0.17081093583136467
iteration : 1516
train acc:  0.6875
train loss:  0.6122772693634033
train gradient:  0.19996268652187144
iteration : 1517
train acc:  0.75
train loss:  0.5227407217025757
train gradient:  0.16508938803150852
iteration : 1518
train acc:  0.71875
train loss:  0.503993034362793
train gradient:  0.13641295420685964
iteration : 1519
train acc:  0.71875
train loss:  0.5142842531204224
train gradient:  0.14455025483996328
iteration : 1520
train acc:  0.6875
train loss:  0.5637701749801636
train gradient:  0.13460031398646966
iteration : 1521
train acc:  0.65625
train loss:  0.6065924763679504
train gradient:  0.19452398800634485
iteration : 1522
train acc:  0.6875
train loss:  0.5519185662269592
train gradient:  0.13193090820147507
iteration : 1523
train acc:  0.7421875
train loss:  0.5539090633392334
train gradient:  0.13403349728069097
iteration : 1524
train acc:  0.7109375
train loss:  0.5332601070404053
train gradient:  0.1587414123337897
iteration : 1525
train acc:  0.75
train loss:  0.5437607765197754
train gradient:  0.1470745722898807
iteration : 1526
train acc:  0.6953125
train loss:  0.5470129251480103
train gradient:  0.12804461290796404
iteration : 1527
train acc:  0.7109375
train loss:  0.5244197845458984
train gradient:  0.1367759230102995
iteration : 1528
train acc:  0.7109375
train loss:  0.5163711309432983
train gradient:  0.14448897495627125
iteration : 1529
train acc:  0.625
train loss:  0.6127942800521851
train gradient:  0.19859215772350092
iteration : 1530
train acc:  0.7421875
train loss:  0.5462199449539185
train gradient:  0.16610166626041806
iteration : 1531
train acc:  0.7421875
train loss:  0.5669794082641602
train gradient:  0.1765213097853286
iteration : 1532
train acc:  0.7578125
train loss:  0.5168693661689758
train gradient:  0.15502622156781218
iteration : 1533
train acc:  0.7578125
train loss:  0.5029592514038086
train gradient:  0.15474442793352572
iteration : 1534
train acc:  0.6171875
train loss:  0.6061054468154907
train gradient:  0.1789905669077423
iteration : 1535
train acc:  0.7265625
train loss:  0.5488722920417786
train gradient:  0.13911422405067175
iteration : 1536
train acc:  0.6953125
train loss:  0.5411267280578613
train gradient:  0.14519230015092255
iteration : 1537
train acc:  0.7265625
train loss:  0.5307996273040771
train gradient:  0.17581413039016458
iteration : 1538
train acc:  0.734375
train loss:  0.5175560116767883
train gradient:  0.1472250057349506
iteration : 1539
train acc:  0.7421875
train loss:  0.5326875448226929
train gradient:  0.16742079568529578
iteration : 1540
train acc:  0.6640625
train loss:  0.5588961839675903
train gradient:  0.21148734766242505
iteration : 1541
train acc:  0.6875
train loss:  0.5827922821044922
train gradient:  0.225873449185638
iteration : 1542
train acc:  0.703125
train loss:  0.6074086427688599
train gradient:  0.23401595096212618
iteration : 1543
train acc:  0.625
train loss:  0.6423362493515015
train gradient:  0.2599961588674666
iteration : 1544
train acc:  0.75
train loss:  0.5072468519210815
train gradient:  0.1382622229220313
iteration : 1545
train acc:  0.71875
train loss:  0.565691351890564
train gradient:  0.1663561655624886
iteration : 1546
train acc:  0.7578125
train loss:  0.48873114585876465
train gradient:  0.13511871239667889
iteration : 1547
train acc:  0.7265625
train loss:  0.538073718547821
train gradient:  0.1602331193057049
iteration : 1548
train acc:  0.7578125
train loss:  0.49686044454574585
train gradient:  0.20168017371142097
iteration : 1549
train acc:  0.65625
train loss:  0.5835733413696289
train gradient:  0.15021478099869612
iteration : 1550
train acc:  0.65625
train loss:  0.6158308982849121
train gradient:  0.2222336323845599
iteration : 1551
train acc:  0.75
train loss:  0.5222705602645874
train gradient:  0.14327327614669477
iteration : 1552
train acc:  0.65625
train loss:  0.6142077445983887
train gradient:  0.2659327166212419
iteration : 1553
train acc:  0.7109375
train loss:  0.5677460432052612
train gradient:  0.17430173946406932
iteration : 1554
train acc:  0.609375
train loss:  0.627747654914856
train gradient:  0.21248647745981936
iteration : 1555
train acc:  0.7421875
train loss:  0.5165005922317505
train gradient:  0.168772680070187
iteration : 1556
train acc:  0.7109375
train loss:  0.5401245355606079
train gradient:  0.16417818079264035
iteration : 1557
train acc:  0.6328125
train loss:  0.6219251155853271
train gradient:  0.17659913413111294
iteration : 1558
train acc:  0.6875
train loss:  0.5956986546516418
train gradient:  0.20711618509483842
iteration : 1559
train acc:  0.7265625
train loss:  0.5855699777603149
train gradient:  0.254978802160787
iteration : 1560
train acc:  0.6796875
train loss:  0.5985549092292786
train gradient:  0.23058122534087866
iteration : 1561
train acc:  0.6796875
train loss:  0.6102476119995117
train gradient:  0.2395749967764575
iteration : 1562
train acc:  0.7578125
train loss:  0.5110883116722107
train gradient:  0.12695589327898876
iteration : 1563
train acc:  0.6953125
train loss:  0.565605878829956
train gradient:  0.1824933395938409
iteration : 1564
train acc:  0.609375
train loss:  0.6529840230941772
train gradient:  0.24236395080434667
iteration : 1565
train acc:  0.6953125
train loss:  0.5538210868835449
train gradient:  0.25189009794219536
iteration : 1566
train acc:  0.7109375
train loss:  0.5766230821609497
train gradient:  0.18418637791659043
iteration : 1567
train acc:  0.7421875
train loss:  0.5032448768615723
train gradient:  0.11395459834623407
iteration : 1568
train acc:  0.734375
train loss:  0.5089978575706482
train gradient:  0.15024751074422005
iteration : 1569
train acc:  0.671875
train loss:  0.6172275543212891
train gradient:  0.21381767242393973
iteration : 1570
train acc:  0.6796875
train loss:  0.5457158088684082
train gradient:  0.18290293704635138
iteration : 1571
train acc:  0.5625
train loss:  0.6868613958358765
train gradient:  0.2979615982676099
iteration : 1572
train acc:  0.7578125
train loss:  0.5224924087524414
train gradient:  0.2020320727136085
iteration : 1573
train acc:  0.7578125
train loss:  0.5297342538833618
train gradient:  0.17006512040264457
iteration : 1574
train acc:  0.671875
train loss:  0.565895140171051
train gradient:  0.1743496421154168
iteration : 1575
train acc:  0.6484375
train loss:  0.5818303823471069
train gradient:  0.17961049036257454
iteration : 1576
train acc:  0.71875
train loss:  0.5101382732391357
train gradient:  0.17162293662895173
iteration : 1577
train acc:  0.671875
train loss:  0.5775741934776306
train gradient:  0.2818116133593606
iteration : 1578
train acc:  0.671875
train loss:  0.5475866794586182
train gradient:  0.14537006761719906
iteration : 1579
train acc:  0.6328125
train loss:  0.610674262046814
train gradient:  0.2190025695799775
iteration : 1580
train acc:  0.734375
train loss:  0.537247896194458
train gradient:  0.14261794890119936
iteration : 1581
train acc:  0.65625
train loss:  0.5866555571556091
train gradient:  0.1679414946915464
iteration : 1582
train acc:  0.7578125
train loss:  0.5351434946060181
train gradient:  0.14370155915644645
iteration : 1583
train acc:  0.7109375
train loss:  0.5420148372650146
train gradient:  0.16296323801357643
iteration : 1584
train acc:  0.703125
train loss:  0.5767426490783691
train gradient:  0.15834844020831584
iteration : 1585
train acc:  0.6640625
train loss:  0.5944855213165283
train gradient:  0.17728818353330972
iteration : 1586
train acc:  0.7265625
train loss:  0.5677233338356018
train gradient:  0.1352727626288449
iteration : 1587
train acc:  0.71875
train loss:  0.5419977903366089
train gradient:  0.1582623607557646
iteration : 1588
train acc:  0.6640625
train loss:  0.6179652810096741
train gradient:  0.22015626922066706
iteration : 1589
train acc:  0.7109375
train loss:  0.550818920135498
train gradient:  0.1511058816948765
iteration : 1590
train acc:  0.6953125
train loss:  0.5867416858673096
train gradient:  0.13583619820810003
iteration : 1591
train acc:  0.59375
train loss:  0.6506989598274231
train gradient:  0.20492146695960153
iteration : 1592
train acc:  0.7109375
train loss:  0.5881385207176208
train gradient:  0.17926736680036903
iteration : 1593
train acc:  0.78125
train loss:  0.48961910605430603
train gradient:  0.18087341894188522
iteration : 1594
train acc:  0.65625
train loss:  0.6025213003158569
train gradient:  0.20166483663424295
iteration : 1595
train acc:  0.6484375
train loss:  0.5938708782196045
train gradient:  0.15832127497907406
iteration : 1596
train acc:  0.6640625
train loss:  0.5770519971847534
train gradient:  0.1367203682138638
iteration : 1597
train acc:  0.7265625
train loss:  0.5252712368965149
train gradient:  0.27175179378328784
iteration : 1598
train acc:  0.703125
train loss:  0.5115761160850525
train gradient:  0.15624078515455558
iteration : 1599
train acc:  0.6875
train loss:  0.5503475069999695
train gradient:  0.16908107345465506
iteration : 1600
train acc:  0.6875
train loss:  0.5733109712600708
train gradient:  0.14616063883703498
iteration : 1601
train acc:  0.6796875
train loss:  0.5559741854667664
train gradient:  0.16968654694718532
iteration : 1602
train acc:  0.6640625
train loss:  0.5766837000846863
train gradient:  0.1374390790188614
iteration : 1603
train acc:  0.6953125
train loss:  0.5587713718414307
train gradient:  0.135434563387689
iteration : 1604
train acc:  0.734375
train loss:  0.551721453666687
train gradient:  0.1351332424538894
iteration : 1605
train acc:  0.6875
train loss:  0.5584725737571716
train gradient:  0.17408369534222398
iteration : 1606
train acc:  0.7109375
train loss:  0.5884681344032288
train gradient:  0.14672526968762423
iteration : 1607
train acc:  0.71875
train loss:  0.5285451412200928
train gradient:  0.15165079156695827
iteration : 1608
train acc:  0.71875
train loss:  0.5451654195785522
train gradient:  0.11372052612863302
iteration : 1609
train acc:  0.6171875
train loss:  0.6602318286895752
train gradient:  0.2746037670630675
iteration : 1610
train acc:  0.6484375
train loss:  0.6340674161911011
train gradient:  0.16990031702415603
iteration : 1611
train acc:  0.7109375
train loss:  0.5541520714759827
train gradient:  0.13487002179145774
iteration : 1612
train acc:  0.6875
train loss:  0.563372790813446
train gradient:  0.1574593395804345
iteration : 1613
train acc:  0.640625
train loss:  0.5807346105575562
train gradient:  0.1798495228901708
iteration : 1614
train acc:  0.7265625
train loss:  0.5384563207626343
train gradient:  0.1745488166978823
iteration : 1615
train acc:  0.7109375
train loss:  0.5355348587036133
train gradient:  0.13778001509742216
iteration : 1616
train acc:  0.7265625
train loss:  0.5408735275268555
train gradient:  0.1677982061061826
iteration : 1617
train acc:  0.71875
train loss:  0.5387952327728271
train gradient:  0.13049898361539652
iteration : 1618
train acc:  0.71875
train loss:  0.5504685044288635
train gradient:  0.14581262042101967
iteration : 1619
train acc:  0.7109375
train loss:  0.537528395652771
train gradient:  0.15950726697106157
iteration : 1620
train acc:  0.7578125
train loss:  0.5295511484146118
train gradient:  0.14332658400119697
iteration : 1621
train acc:  0.78125
train loss:  0.5245460867881775
train gradient:  0.13055790815914226
iteration : 1622
train acc:  0.703125
train loss:  0.545182466506958
train gradient:  0.1354788971071991
iteration : 1623
train acc:  0.7421875
train loss:  0.4992629289627075
train gradient:  0.17590457803205412
iteration : 1624
train acc:  0.71875
train loss:  0.5817097425460815
train gradient:  0.16862057968042204
iteration : 1625
train acc:  0.7890625
train loss:  0.4700016677379608
train gradient:  0.13439237743878318
iteration : 1626
train acc:  0.609375
train loss:  0.5888556838035583
train gradient:  0.16248893100120715
iteration : 1627
train acc:  0.703125
train loss:  0.5718780159950256
train gradient:  0.19063164628714144
iteration : 1628
train acc:  0.6875
train loss:  0.6147059202194214
train gradient:  0.17653094845280393
iteration : 1629
train acc:  0.7109375
train loss:  0.5859326124191284
train gradient:  0.22322533364388547
iteration : 1630
train acc:  0.7109375
train loss:  0.5277294516563416
train gradient:  0.17828460678855435
iteration : 1631
train acc:  0.703125
train loss:  0.5633081197738647
train gradient:  0.1747056230947137
iteration : 1632
train acc:  0.7578125
train loss:  0.5022282600402832
train gradient:  0.14449339919846405
iteration : 1633
train acc:  0.7109375
train loss:  0.5832985043525696
train gradient:  0.15622649853221948
iteration : 1634
train acc:  0.75
train loss:  0.49849629402160645
train gradient:  0.14216057759790685
iteration : 1635
train acc:  0.7109375
train loss:  0.5107008814811707
train gradient:  0.11980048062043096
iteration : 1636
train acc:  0.7109375
train loss:  0.5726891160011292
train gradient:  0.16237077955225665
iteration : 1637
train acc:  0.6875
train loss:  0.5665004849433899
train gradient:  0.19238318657936768
iteration : 1638
train acc:  0.7734375
train loss:  0.5306389331817627
train gradient:  0.14769154323503048
iteration : 1639
train acc:  0.6953125
train loss:  0.552353024482727
train gradient:  0.15122528085682557
iteration : 1640
train acc:  0.7109375
train loss:  0.5643017888069153
train gradient:  0.17937582991941461
iteration : 1641
train acc:  0.671875
train loss:  0.5418891906738281
train gradient:  0.1206381967447614
iteration : 1642
train acc:  0.8046875
train loss:  0.4736582636833191
train gradient:  0.17682550410156106
iteration : 1643
train acc:  0.6875
train loss:  0.5569208860397339
train gradient:  0.13622820152585444
iteration : 1644
train acc:  0.7265625
train loss:  0.5930473804473877
train gradient:  0.1683086643688092
iteration : 1645
train acc:  0.7421875
train loss:  0.51689612865448
train gradient:  0.17947318818579278
iteration : 1646
train acc:  0.671875
train loss:  0.5490378141403198
train gradient:  0.12070412351485779
iteration : 1647
train acc:  0.6875
train loss:  0.5303542017936707
train gradient:  0.17376434961203008
iteration : 1648
train acc:  0.71875
train loss:  0.5399751663208008
train gradient:  0.17048202512782845
iteration : 1649
train acc:  0.75
train loss:  0.5114935040473938
train gradient:  0.11240378950644375
iteration : 1650
train acc:  0.6875
train loss:  0.573521077632904
train gradient:  0.19191669059881783
iteration : 1651
train acc:  0.6953125
train loss:  0.5409599542617798
train gradient:  0.15683424679292196
iteration : 1652
train acc:  0.71875
train loss:  0.5496559143066406
train gradient:  0.19527385521777693
iteration : 1653
train acc:  0.7109375
train loss:  0.5649146437644958
train gradient:  0.1636732566816842
iteration : 1654
train acc:  0.6484375
train loss:  0.5708036422729492
train gradient:  0.1611723669154774
iteration : 1655
train acc:  0.7109375
train loss:  0.51552414894104
train gradient:  0.12786007941896854
iteration : 1656
train acc:  0.703125
train loss:  0.5381470322608948
train gradient:  0.15415289046213063
iteration : 1657
train acc:  0.6875
train loss:  0.56007981300354
train gradient:  0.17649336312998448
iteration : 1658
train acc:  0.7265625
train loss:  0.5588886141777039
train gradient:  0.2745270462774331
iteration : 1659
train acc:  0.7421875
train loss:  0.5265161991119385
train gradient:  0.14158963876022584
iteration : 1660
train acc:  0.6796875
train loss:  0.5525773763656616
train gradient:  0.14634810856749653
iteration : 1661
train acc:  0.625
train loss:  0.6016901135444641
train gradient:  0.2443390409627791
iteration : 1662
train acc:  0.703125
train loss:  0.5653998255729675
train gradient:  0.2020332135580144
iteration : 1663
train acc:  0.65625
train loss:  0.5929484367370605
train gradient:  0.3772197497086019
iteration : 1664
train acc:  0.6484375
train loss:  0.5870180130004883
train gradient:  0.17574773226324597
iteration : 1665
train acc:  0.7109375
train loss:  0.5238494873046875
train gradient:  0.11212257404894432
iteration : 1666
train acc:  0.65625
train loss:  0.6028736233711243
train gradient:  0.26081990354614587
iteration : 1667
train acc:  0.640625
train loss:  0.5933190584182739
train gradient:  0.1950844984550485
iteration : 1668
train acc:  0.8046875
train loss:  0.4634336531162262
train gradient:  0.14234469029293068
iteration : 1669
train acc:  0.65625
train loss:  0.5480716824531555
train gradient:  0.14572248275519767
iteration : 1670
train acc:  0.65625
train loss:  0.5785660743713379
train gradient:  0.2053760338732297
iteration : 1671
train acc:  0.7109375
train loss:  0.5097187757492065
train gradient:  0.11871167017427463
iteration : 1672
train acc:  0.671875
train loss:  0.5918186902999878
train gradient:  0.250936857491784
iteration : 1673
train acc:  0.703125
train loss:  0.5585008263587952
train gradient:  0.23227077477557567
iteration : 1674
train acc:  0.625
train loss:  0.6580151319503784
train gradient:  0.25312545062261316
iteration : 1675
train acc:  0.7265625
train loss:  0.5232645273208618
train gradient:  0.16262155134902495
iteration : 1676
train acc:  0.734375
train loss:  0.518322229385376
train gradient:  0.1789408534412965
iteration : 1677
train acc:  0.7265625
train loss:  0.5524803400039673
train gradient:  0.14049117145633405
iteration : 1678
train acc:  0.640625
train loss:  0.6080378293991089
train gradient:  0.19746562875446744
iteration : 1679
train acc:  0.703125
train loss:  0.5380421876907349
train gradient:  0.1730547873600652
iteration : 1680
train acc:  0.6328125
train loss:  0.6024705171585083
train gradient:  0.2217957866610633
iteration : 1681
train acc:  0.71875
train loss:  0.5341954827308655
train gradient:  0.1218045954177575
iteration : 1682
train acc:  0.71875
train loss:  0.573781430721283
train gradient:  0.21663807248631586
iteration : 1683
train acc:  0.6953125
train loss:  0.5683038234710693
train gradient:  0.17987313666570037
iteration : 1684
train acc:  0.6875
train loss:  0.5542424321174622
train gradient:  0.16586100602881376
iteration : 1685
train acc:  0.7890625
train loss:  0.5112531185150146
train gradient:  0.1355025651129406
iteration : 1686
train acc:  0.6328125
train loss:  0.5778601169586182
train gradient:  0.16298094962109538
iteration : 1687
train acc:  0.703125
train loss:  0.5269278883934021
train gradient:  0.16725406676167198
iteration : 1688
train acc:  0.734375
train loss:  0.511357307434082
train gradient:  0.16454628734682858
iteration : 1689
train acc:  0.734375
train loss:  0.5394484996795654
train gradient:  0.1588295820035552
iteration : 1690
train acc:  0.71875
train loss:  0.5696109533309937
train gradient:  0.2167263145501353
iteration : 1691
train acc:  0.734375
train loss:  0.5165627002716064
train gradient:  0.1780299214459597
iteration : 1692
train acc:  0.7265625
train loss:  0.5681097507476807
train gradient:  0.17419901637506674
iteration : 1693
train acc:  0.671875
train loss:  0.6119987964630127
train gradient:  0.2090524787894784
iteration : 1694
train acc:  0.6484375
train loss:  0.5699667930603027
train gradient:  0.15828209301430848
iteration : 1695
train acc:  0.7109375
train loss:  0.5461090803146362
train gradient:  0.1457381531402845
iteration : 1696
train acc:  0.6875
train loss:  0.5334193706512451
train gradient:  0.14526826991242583
iteration : 1697
train acc:  0.703125
train loss:  0.5480204820632935
train gradient:  0.1513786000910732
iteration : 1698
train acc:  0.71875
train loss:  0.5502619743347168
train gradient:  0.14412343013470713
iteration : 1699
train acc:  0.765625
train loss:  0.5173653364181519
train gradient:  0.1491359293551765
iteration : 1700
train acc:  0.7734375
train loss:  0.5242711901664734
train gradient:  0.14077032963571495
iteration : 1701
train acc:  0.71875
train loss:  0.5493671894073486
train gradient:  0.14634000217920473
iteration : 1702
train acc:  0.6953125
train loss:  0.5707124471664429
train gradient:  0.17310423615702203
iteration : 1703
train acc:  0.6796875
train loss:  0.5744202136993408
train gradient:  0.19048569850654157
iteration : 1704
train acc:  0.7421875
train loss:  0.5444628000259399
train gradient:  0.1389150138350846
iteration : 1705
train acc:  0.7265625
train loss:  0.5229623317718506
train gradient:  0.18599566622544753
iteration : 1706
train acc:  0.7421875
train loss:  0.553126871585846
train gradient:  0.1362699907325417
iteration : 1707
train acc:  0.78125
train loss:  0.5033644437789917
train gradient:  0.14677359906930232
iteration : 1708
train acc:  0.6796875
train loss:  0.5700844526290894
train gradient:  0.2170431202910864
iteration : 1709
train acc:  0.734375
train loss:  0.4819616675376892
train gradient:  0.13122160329683302
iteration : 1710
train acc:  0.7109375
train loss:  0.578465461730957
train gradient:  0.310836156060374
iteration : 1711
train acc:  0.765625
train loss:  0.5119448900222778
train gradient:  0.1625166117105518
iteration : 1712
train acc:  0.7265625
train loss:  0.5693575143814087
train gradient:  0.15937244206947748
iteration : 1713
train acc:  0.765625
train loss:  0.48675811290740967
train gradient:  0.22987343458405401
iteration : 1714
train acc:  0.71875
train loss:  0.568524956703186
train gradient:  0.21352434103105583
iteration : 1715
train acc:  0.7421875
train loss:  0.48510563373565674
train gradient:  0.15370657233716128
iteration : 1716
train acc:  0.765625
train loss:  0.5352118611335754
train gradient:  0.18736737363021172
iteration : 1717
train acc:  0.6953125
train loss:  0.5416576862335205
train gradient:  0.14115294423655556
iteration : 1718
train acc:  0.640625
train loss:  0.5659142732620239
train gradient:  0.1548015934667605
iteration : 1719
train acc:  0.671875
train loss:  0.5728355050086975
train gradient:  0.14644820394265262
iteration : 1720
train acc:  0.6796875
train loss:  0.5620980262756348
train gradient:  0.136162488577908
iteration : 1721
train acc:  0.734375
train loss:  0.5475109815597534
train gradient:  0.2171139476937784
iteration : 1722
train acc:  0.734375
train loss:  0.5205315351486206
train gradient:  0.18304100749002988
iteration : 1723
train acc:  0.7265625
train loss:  0.536815881729126
train gradient:  0.16408541032845783
iteration : 1724
train acc:  0.609375
train loss:  0.5862031579017639
train gradient:  0.20273773029342523
iteration : 1725
train acc:  0.7421875
train loss:  0.5375251173973083
train gradient:  0.24862126326356354
iteration : 1726
train acc:  0.640625
train loss:  0.5759941339492798
train gradient:  0.15920449777551943
iteration : 1727
train acc:  0.7109375
train loss:  0.5315769910812378
train gradient:  0.16627788809813315
iteration : 1728
train acc:  0.6640625
train loss:  0.5905219316482544
train gradient:  0.27817920490990533
iteration : 1729
train acc:  0.6953125
train loss:  0.5450257062911987
train gradient:  0.17145314709338821
iteration : 1730
train acc:  0.6640625
train loss:  0.5604033470153809
train gradient:  0.17450454843873253
iteration : 1731
train acc:  0.71875
train loss:  0.5250822305679321
train gradient:  0.18864562658630873
iteration : 1732
train acc:  0.6640625
train loss:  0.6091899871826172
train gradient:  0.1708478455851845
iteration : 1733
train acc:  0.6796875
train loss:  0.5829784870147705
train gradient:  0.16634553233093646
iteration : 1734
train acc:  0.7265625
train loss:  0.5725876092910767
train gradient:  0.13540107601813725
iteration : 1735
train acc:  0.8046875
train loss:  0.4644339680671692
train gradient:  0.18977166299131348
iteration : 1736
train acc:  0.7421875
train loss:  0.5305655598640442
train gradient:  0.15880837754514157
iteration : 1737
train acc:  0.671875
train loss:  0.6203386783599854
train gradient:  0.1603748306577477
iteration : 1738
train acc:  0.6875
train loss:  0.5588902235031128
train gradient:  0.18463709635593403
iteration : 1739
train acc:  0.6875
train loss:  0.5722209811210632
train gradient:  0.2549306991396658
iteration : 1740
train acc:  0.7265625
train loss:  0.5454136729240417
train gradient:  0.23451182426472178
iteration : 1741
train acc:  0.6875
train loss:  0.5195357203483582
train gradient:  0.1759967425429777
iteration : 1742
train acc:  0.65625
train loss:  0.5616198182106018
train gradient:  0.1451937216015079
iteration : 1743
train acc:  0.671875
train loss:  0.5704153180122375
train gradient:  0.16936847574874903
iteration : 1744
train acc:  0.6796875
train loss:  0.6000992655754089
train gradient:  0.21355250061751374
iteration : 1745
train acc:  0.71875
train loss:  0.5065865516662598
train gradient:  0.14300906331931806
iteration : 1746
train acc:  0.7109375
train loss:  0.5353150963783264
train gradient:  0.15317275625049537
iteration : 1747
train acc:  0.734375
train loss:  0.5229188203811646
train gradient:  0.18278311681307957
iteration : 1748
train acc:  0.71875
train loss:  0.6015884280204773
train gradient:  0.27854475019524444
iteration : 1749
train acc:  0.75
train loss:  0.48839664459228516
train gradient:  0.18563249479435256
iteration : 1750
train acc:  0.65625
train loss:  0.6019681692123413
train gradient:  0.1901308904538791
iteration : 1751
train acc:  0.671875
train loss:  0.5548954010009766
train gradient:  0.1842292821513269
iteration : 1752
train acc:  0.6796875
train loss:  0.5677297711372375
train gradient:  0.19668865499015897
iteration : 1753
train acc:  0.671875
train loss:  0.5810585021972656
train gradient:  0.21645288991941866
iteration : 1754
train acc:  0.6328125
train loss:  0.642653226852417
train gradient:  0.27682189430012527
iteration : 1755
train acc:  0.734375
train loss:  0.5120726227760315
train gradient:  0.14870453050168803
iteration : 1756
train acc:  0.6875
train loss:  0.5500403046607971
train gradient:  0.18079579317369912
iteration : 1757
train acc:  0.7421875
train loss:  0.5130059719085693
train gradient:  0.16074243839774768
iteration : 1758
train acc:  0.6640625
train loss:  0.619476854801178
train gradient:  0.24289985744864695
iteration : 1759
train acc:  0.6953125
train loss:  0.5441980361938477
train gradient:  0.1399913029957151
iteration : 1760
train acc:  0.7265625
train loss:  0.543282151222229
train gradient:  0.16975103383592421
iteration : 1761
train acc:  0.6953125
train loss:  0.5960699319839478
train gradient:  0.1808405662495038
iteration : 1762
train acc:  0.7109375
train loss:  0.5226656794548035
train gradient:  0.1465342349003475
iteration : 1763
train acc:  0.7265625
train loss:  0.5689327716827393
train gradient:  0.17661189220720042
iteration : 1764
train acc:  0.7890625
train loss:  0.4946921765804291
train gradient:  0.1423108094390767
iteration : 1765
train acc:  0.734375
train loss:  0.4902811646461487
train gradient:  0.1448139962011798
iteration : 1766
train acc:  0.7109375
train loss:  0.5735068321228027
train gradient:  0.16475278399844212
iteration : 1767
train acc:  0.7578125
train loss:  0.48355308175086975
train gradient:  0.14505779406215052
iteration : 1768
train acc:  0.703125
train loss:  0.5519418120384216
train gradient:  0.1654544689146007
iteration : 1769
train acc:  0.71875
train loss:  0.5207456946372986
train gradient:  0.14636942007014309
iteration : 1770
train acc:  0.640625
train loss:  0.5611081123352051
train gradient:  0.15992488259573695
iteration : 1771
train acc:  0.65625
train loss:  0.6301020383834839
train gradient:  0.18806083403646875
iteration : 1772
train acc:  0.6875
train loss:  0.5798041820526123
train gradient:  0.1837486381259409
iteration : 1773
train acc:  0.71875
train loss:  0.5339962840080261
train gradient:  0.13195086673383266
iteration : 1774
train acc:  0.609375
train loss:  0.6208028793334961
train gradient:  0.209182061100759
iteration : 1775
train acc:  0.625
train loss:  0.6342349052429199
train gradient:  0.22141860112670134
iteration : 1776
train acc:  0.7109375
train loss:  0.5193581581115723
train gradient:  0.14239531244384884
iteration : 1777
train acc:  0.75
train loss:  0.556399405002594
train gradient:  0.3384750458040326
iteration : 1778
train acc:  0.671875
train loss:  0.5390355587005615
train gradient:  0.16343123030061546
iteration : 1779
train acc:  0.75
train loss:  0.5363193154335022
train gradient:  0.15699120005542655
iteration : 1780
train acc:  0.71875
train loss:  0.5546249151229858
train gradient:  0.13792416860321433
iteration : 1781
train acc:  0.6484375
train loss:  0.6064529418945312
train gradient:  0.19540608654649644
iteration : 1782
train acc:  0.6953125
train loss:  0.5396533608436584
train gradient:  0.14987021323820215
iteration : 1783
train acc:  0.6484375
train loss:  0.6175894737243652
train gradient:  0.1645364191508078
iteration : 1784
train acc:  0.6015625
train loss:  0.6299513578414917
train gradient:  0.21443022201595008
iteration : 1785
train acc:  0.6875
train loss:  0.567393958568573
train gradient:  0.18920287218833787
iteration : 1786
train acc:  0.71875
train loss:  0.5651133060455322
train gradient:  0.23476884726986902
iteration : 1787
train acc:  0.7109375
train loss:  0.5312319993972778
train gradient:  0.13173349277082436
iteration : 1788
train acc:  0.6875
train loss:  0.5775395631790161
train gradient:  0.18633359889012238
iteration : 1789
train acc:  0.75
train loss:  0.5445706248283386
train gradient:  0.1808097438960236
iteration : 1790
train acc:  0.6484375
train loss:  0.5734930038452148
train gradient:  0.20646147064203993
iteration : 1791
train acc:  0.6875
train loss:  0.5659723877906799
train gradient:  0.17670721251628102
iteration : 1792
train acc:  0.71875
train loss:  0.5168870091438293
train gradient:  0.14162473636847778
iteration : 1793
train acc:  0.6640625
train loss:  0.608725905418396
train gradient:  0.22471690336964262
iteration : 1794
train acc:  0.703125
train loss:  0.6008298993110657
train gradient:  0.3157590016426778
iteration : 1795
train acc:  0.6953125
train loss:  0.5472874045372009
train gradient:  0.10665170196712138
iteration : 1796
train acc:  0.6875
train loss:  0.5264472961425781
train gradient:  0.13776996996575172
iteration : 1797
train acc:  0.6875
train loss:  0.5318245887756348
train gradient:  0.15210043591999417
iteration : 1798
train acc:  0.7734375
train loss:  0.5418492555618286
train gradient:  0.21272590588924958
iteration : 1799
train acc:  0.625
train loss:  0.6053256988525391
train gradient:  0.17281216160772228
iteration : 1800
train acc:  0.7734375
train loss:  0.4886922538280487
train gradient:  0.13271113455762135
iteration : 1801
train acc:  0.765625
train loss:  0.5157358646392822
train gradient:  0.1424773724982839
iteration : 1802
train acc:  0.6796875
train loss:  0.5642146468162537
train gradient:  0.1901547909475318
iteration : 1803
train acc:  0.6640625
train loss:  0.640906572341919
train gradient:  0.4128629604610094
iteration : 1804
train acc:  0.7109375
train loss:  0.5771968364715576
train gradient:  0.20810901735356385
iteration : 1805
train acc:  0.7421875
train loss:  0.5193322896957397
train gradient:  0.15059263888790017
iteration : 1806
train acc:  0.6953125
train loss:  0.5738311409950256
train gradient:  0.14454362720180036
iteration : 1807
train acc:  0.765625
train loss:  0.5161340236663818
train gradient:  0.13459872277350182
iteration : 1808
train acc:  0.71875
train loss:  0.5630650520324707
train gradient:  0.15820971020350277
iteration : 1809
train acc:  0.78125
train loss:  0.5071088075637817
train gradient:  0.1480628314010266
iteration : 1810
train acc:  0.671875
train loss:  0.5718753337860107
train gradient:  0.15097431206439832
iteration : 1811
train acc:  0.796875
train loss:  0.4488315284252167
train gradient:  0.14275088624640153
iteration : 1812
train acc:  0.7734375
train loss:  0.5049303770065308
train gradient:  0.1515457136957289
iteration : 1813
train acc:  0.703125
train loss:  0.5826051235198975
train gradient:  0.2220616843544761
iteration : 1814
train acc:  0.734375
train loss:  0.5113004446029663
train gradient:  0.15877922462723754
iteration : 1815
train acc:  0.6640625
train loss:  0.6027406454086304
train gradient:  0.18354621936802715
iteration : 1816
train acc:  0.765625
train loss:  0.49812209606170654
train gradient:  0.14365922188149755
iteration : 1817
train acc:  0.703125
train loss:  0.5635653138160706
train gradient:  0.17803063730562776
iteration : 1818
train acc:  0.5859375
train loss:  0.6366517543792725
train gradient:  0.35262726307852893
iteration : 1819
train acc:  0.7109375
train loss:  0.5388991832733154
train gradient:  0.1607561749905133
iteration : 1820
train acc:  0.6875
train loss:  0.533607006072998
train gradient:  0.13982572504629054
iteration : 1821
train acc:  0.7421875
train loss:  0.49613118171691895
train gradient:  0.1798029418513198
iteration : 1822
train acc:  0.71875
train loss:  0.5087319016456604
train gradient:  0.1483406356902786
iteration : 1823
train acc:  0.765625
train loss:  0.5048391222953796
train gradient:  0.1761514613786796
iteration : 1824
train acc:  0.65625
train loss:  0.64936763048172
train gradient:  0.20106407265054416
iteration : 1825
train acc:  0.6875
train loss:  0.5896055698394775
train gradient:  0.17609207173805042
iteration : 1826
train acc:  0.71875
train loss:  0.5564016699790955
train gradient:  0.2356760991426402
iteration : 1827
train acc:  0.71875
train loss:  0.5163748264312744
train gradient:  0.14410660992494317
iteration : 1828
train acc:  0.765625
train loss:  0.5127400755882263
train gradient:  0.1531084074338727
iteration : 1829
train acc:  0.734375
train loss:  0.5113130807876587
train gradient:  0.16790178626087704
iteration : 1830
train acc:  0.6640625
train loss:  0.5762754082679749
train gradient:  0.18084526679402646
iteration : 1831
train acc:  0.625
train loss:  0.5997069478034973
train gradient:  0.21363167417787932
iteration : 1832
train acc:  0.71875
train loss:  0.5325010418891907
train gradient:  0.13034095848942384
iteration : 1833
train acc:  0.6875
train loss:  0.5396939516067505
train gradient:  0.27315168277704405
iteration : 1834
train acc:  0.765625
train loss:  0.5035013556480408
train gradient:  0.20797847643538686
iteration : 1835
train acc:  0.78125
train loss:  0.49321022629737854
train gradient:  0.2049587416762987
iteration : 1836
train acc:  0.7265625
train loss:  0.5302320718765259
train gradient:  0.1454412780182276
iteration : 1837
train acc:  0.734375
train loss:  0.5006365776062012
train gradient:  0.14517301624184953
iteration : 1838
train acc:  0.734375
train loss:  0.565392255783081
train gradient:  0.23836287864103878
iteration : 1839
train acc:  0.671875
train loss:  0.5711684226989746
train gradient:  0.17779257914366164
iteration : 1840
train acc:  0.6953125
train loss:  0.5437990427017212
train gradient:  0.1987392960712671
iteration : 1841
train acc:  0.6875
train loss:  0.5342484712600708
train gradient:  0.13560619970504692
iteration : 1842
train acc:  0.7421875
train loss:  0.5189955234527588
train gradient:  0.15048905464591086
iteration : 1843
train acc:  0.6875
train loss:  0.5603466033935547
train gradient:  0.17914866002232915
iteration : 1844
train acc:  0.703125
train loss:  0.6039679050445557
train gradient:  0.251487582055946
iteration : 1845
train acc:  0.7265625
train loss:  0.5508877038955688
train gradient:  0.14610570192888794
iteration : 1846
train acc:  0.6953125
train loss:  0.5064362287521362
train gradient:  0.20658261813359835
iteration : 1847
train acc:  0.6484375
train loss:  0.5836985111236572
train gradient:  0.15474965755977474
iteration : 1848
train acc:  0.65625
train loss:  0.6009560823440552
train gradient:  0.18142104212391455
iteration : 1849
train acc:  0.703125
train loss:  0.5721099376678467
train gradient:  0.14758712492335407
iteration : 1850
train acc:  0.6484375
train loss:  0.6384752988815308
train gradient:  0.20061609825942583
iteration : 1851
train acc:  0.6953125
train loss:  0.5766186714172363
train gradient:  0.15392662233036453
iteration : 1852
train acc:  0.703125
train loss:  0.5741854906082153
train gradient:  0.2492342286339675
iteration : 1853
train acc:  0.6328125
train loss:  0.6196140050888062
train gradient:  0.2530207209495071
iteration : 1854
train acc:  0.6875
train loss:  0.5709583759307861
train gradient:  0.20972242998611557
iteration : 1855
train acc:  0.7578125
train loss:  0.5102884769439697
train gradient:  0.14386199655054788
iteration : 1856
train acc:  0.6796875
train loss:  0.5486922264099121
train gradient:  0.19529516043252776
iteration : 1857
train acc:  0.765625
train loss:  0.5298066735267639
train gradient:  0.19748913076524788
iteration : 1858
train acc:  0.6328125
train loss:  0.6356315612792969
train gradient:  0.24486028113576858
iteration : 1859
train acc:  0.671875
train loss:  0.5815075635910034
train gradient:  0.2100326631212735
iteration : 1860
train acc:  0.7421875
train loss:  0.5215063095092773
train gradient:  0.2124416068726513
iteration : 1861
train acc:  0.6328125
train loss:  0.5754671096801758
train gradient:  0.193324871795458
iteration : 1862
train acc:  0.7421875
train loss:  0.5179545879364014
train gradient:  0.16872717356382108
iteration : 1863
train acc:  0.703125
train loss:  0.5417221188545227
train gradient:  0.14035201102444034
iteration : 1864
train acc:  0.75
train loss:  0.5154424905776978
train gradient:  0.13491107233417565
iteration : 1865
train acc:  0.6953125
train loss:  0.5656304359436035
train gradient:  0.1526306404805929
iteration : 1866
train acc:  0.7265625
train loss:  0.5450261235237122
train gradient:  0.14034068878962985
iteration : 1867
train acc:  0.7265625
train loss:  0.5495567917823792
train gradient:  0.24992426375413612
iteration : 1868
train acc:  0.7265625
train loss:  0.5245570540428162
train gradient:  0.16845442719413034
iteration : 1869
train acc:  0.71875
train loss:  0.5268505811691284
train gradient:  0.143034254932962
iteration : 1870
train acc:  0.734375
train loss:  0.5463559627532959
train gradient:  0.16340558873184302
iteration : 1871
train acc:  0.671875
train loss:  0.5853344798088074
train gradient:  0.17774011655378447
iteration : 1872
train acc:  0.734375
train loss:  0.557701587677002
train gradient:  0.18467994638667085
iteration : 1873
train acc:  0.7265625
train loss:  0.5157299041748047
train gradient:  0.2090330713714697
iteration : 1874
train acc:  0.6953125
train loss:  0.5558488368988037
train gradient:  0.17861198237920134
iteration : 1875
train acc:  0.625
train loss:  0.6045690774917603
train gradient:  0.24087700624773317
iteration : 1876
train acc:  0.7109375
train loss:  0.5787886381149292
train gradient:  0.16818480359320728
iteration : 1877
train acc:  0.6171875
train loss:  0.599697470664978
train gradient:  0.17418015681638818
iteration : 1878
train acc:  0.671875
train loss:  0.5388653874397278
train gradient:  0.2601062436412741
iteration : 1879
train acc:  0.6796875
train loss:  0.5949006080627441
train gradient:  0.18359786139471962
iteration : 1880
train acc:  0.7421875
train loss:  0.5363577604293823
train gradient:  0.17513628252487623
iteration : 1881
train acc:  0.6953125
train loss:  0.602626621723175
train gradient:  0.15879366651104235
iteration : 1882
train acc:  0.7109375
train loss:  0.5530068874359131
train gradient:  0.133878300727838
iteration : 1883
train acc:  0.796875
train loss:  0.49493780732154846
train gradient:  0.14959044351121306
iteration : 1884
train acc:  0.765625
train loss:  0.5148187875747681
train gradient:  0.16027646523539357
iteration : 1885
train acc:  0.765625
train loss:  0.5408058166503906
train gradient:  0.19750439707646955
iteration : 1886
train acc:  0.65625
train loss:  0.547832190990448
train gradient:  0.1904434680707488
iteration : 1887
train acc:  0.75
train loss:  0.5173300504684448
train gradient:  0.12639820453813744
iteration : 1888
train acc:  0.71875
train loss:  0.5714797377586365
train gradient:  0.27407180138277026
iteration : 1889
train acc:  0.65625
train loss:  0.5673342943191528
train gradient:  0.1576724176678948
iteration : 1890
train acc:  0.7734375
train loss:  0.47512897849082947
train gradient:  0.17795718146309675
iteration : 1891
train acc:  0.671875
train loss:  0.5638201832771301
train gradient:  0.14593671408755737
iteration : 1892
train acc:  0.7265625
train loss:  0.515519917011261
train gradient:  0.17508441262546784
iteration : 1893
train acc:  0.671875
train loss:  0.5889806747436523
train gradient:  0.17983590280488815
iteration : 1894
train acc:  0.71875
train loss:  0.5451714992523193
train gradient:  0.15690894986530007
iteration : 1895
train acc:  0.7109375
train loss:  0.6057648658752441
train gradient:  0.20219204938979918
iteration : 1896
train acc:  0.7265625
train loss:  0.5593617558479309
train gradient:  0.1754752841573039
iteration : 1897
train acc:  0.734375
train loss:  0.4720810055732727
train gradient:  0.17055350280672643
iteration : 1898
train acc:  0.7265625
train loss:  0.5764594078063965
train gradient:  0.16253355924037427
iteration : 1899
train acc:  0.71875
train loss:  0.5446908473968506
train gradient:  0.1272731522208838
iteration : 1900
train acc:  0.6796875
train loss:  0.6139895915985107
train gradient:  0.19228491929280045
iteration : 1901
train acc:  0.6484375
train loss:  0.6252905130386353
train gradient:  0.22242887170792797
iteration : 1902
train acc:  0.671875
train loss:  0.5832725763320923
train gradient:  0.22753054493315178
iteration : 1903
train acc:  0.6953125
train loss:  0.5745159387588501
train gradient:  0.2731674296435319
iteration : 1904
train acc:  0.78125
train loss:  0.4737984836101532
train gradient:  0.1385393002174697
iteration : 1905
train acc:  0.734375
train loss:  0.5451782941818237
train gradient:  0.13231379373636998
iteration : 1906
train acc:  0.7421875
train loss:  0.5292381048202515
train gradient:  0.1584922374710257
iteration : 1907
train acc:  0.7890625
train loss:  0.530547559261322
train gradient:  0.14849265148789378
iteration : 1908
train acc:  0.625
train loss:  0.5951411724090576
train gradient:  0.1674534900466324
iteration : 1909
train acc:  0.7265625
train loss:  0.5434770584106445
train gradient:  0.13391053990666876
iteration : 1910
train acc:  0.734375
train loss:  0.5251672267913818
train gradient:  0.17034532056573481
iteration : 1911
train acc:  0.71875
train loss:  0.5236803293228149
train gradient:  0.12483639407132864
iteration : 1912
train acc:  0.7109375
train loss:  0.5370152592658997
train gradient:  0.18628110803209885
iteration : 1913
train acc:  0.6953125
train loss:  0.6016741991043091
train gradient:  0.22174839670157623
iteration : 1914
train acc:  0.703125
train loss:  0.4889015257358551
train gradient:  0.14512663724662617
iteration : 1915
train acc:  0.7421875
train loss:  0.5155730247497559
train gradient:  0.31304666478086507
iteration : 1916
train acc:  0.6484375
train loss:  0.592922568321228
train gradient:  0.18314124345744903
iteration : 1917
train acc:  0.703125
train loss:  0.5648603439331055
train gradient:  0.1430745989815551
iteration : 1918
train acc:  0.6875
train loss:  0.5908152461051941
train gradient:  0.2734385723759223
iteration : 1919
train acc:  0.6875
train loss:  0.5998933911323547
train gradient:  0.18038271831011718
iteration : 1920
train acc:  0.6953125
train loss:  0.567412257194519
train gradient:  0.15808966981947623
iteration : 1921
train acc:  0.71875
train loss:  0.5344458818435669
train gradient:  0.14611346732173966
iteration : 1922
train acc:  0.703125
train loss:  0.5629070997238159
train gradient:  0.14822006755352096
iteration : 1923
train acc:  0.6953125
train loss:  0.551990270614624
train gradient:  0.1694899786317512
iteration : 1924
train acc:  0.7578125
train loss:  0.5012643337249756
train gradient:  0.13206668064281626
iteration : 1925
train acc:  0.609375
train loss:  0.6201332211494446
train gradient:  0.15836984092620032
iteration : 1926
train acc:  0.7109375
train loss:  0.5445493459701538
train gradient:  0.23751066696699813
iteration : 1927
train acc:  0.7421875
train loss:  0.4906159043312073
train gradient:  0.16277390466913522
iteration : 1928
train acc:  0.7265625
train loss:  0.5557839870452881
train gradient:  0.24014473206286838
iteration : 1929
train acc:  0.7578125
train loss:  0.4941968321800232
train gradient:  0.15802172834730746
iteration : 1930
train acc:  0.71875
train loss:  0.5461833477020264
train gradient:  0.17321217620990925
iteration : 1931
train acc:  0.734375
train loss:  0.5263819694519043
train gradient:  0.15813060456850647
iteration : 1932
train acc:  0.7265625
train loss:  0.5384132862091064
train gradient:  0.15815115885044562
iteration : 1933
train acc:  0.6796875
train loss:  0.6086865663528442
train gradient:  0.17416258723045297
iteration : 1934
train acc:  0.7890625
train loss:  0.475650429725647
train gradient:  0.1542332161353666
iteration : 1935
train acc:  0.765625
train loss:  0.5043296813964844
train gradient:  0.13220255837763978
iteration : 1936
train acc:  0.6796875
train loss:  0.5748888850212097
train gradient:  0.1726759365580899
iteration : 1937
train acc:  0.7109375
train loss:  0.4782832860946655
train gradient:  0.15272701820894669
iteration : 1938
train acc:  0.734375
train loss:  0.5084655284881592
train gradient:  0.138939039709765
iteration : 1939
train acc:  0.7421875
train loss:  0.4866583049297333
train gradient:  0.14640654414173873
iteration : 1940
train acc:  0.734375
train loss:  0.5284483432769775
train gradient:  0.15499905229544847
iteration : 1941
train acc:  0.6953125
train loss:  0.5485937595367432
train gradient:  0.1416653170832923
iteration : 1942
train acc:  0.640625
train loss:  0.6461893320083618
train gradient:  0.19855747460191772
iteration : 1943
train acc:  0.7265625
train loss:  0.49965614080429077
train gradient:  0.14295015049967102
iteration : 1944
train acc:  0.671875
train loss:  0.5793375372886658
train gradient:  0.17218713672983282
iteration : 1945
train acc:  0.6484375
train loss:  0.611018180847168
train gradient:  0.1991361307551618
iteration : 1946
train acc:  0.6953125
train loss:  0.5771914720535278
train gradient:  0.2637408948962739
iteration : 1947
train acc:  0.7265625
train loss:  0.5595499873161316
train gradient:  0.16405980704973921
iteration : 1948
train acc:  0.7265625
train loss:  0.521075427532196
train gradient:  0.12859262881462077
iteration : 1949
train acc:  0.703125
train loss:  0.5651576519012451
train gradient:  0.15761984849424748
iteration : 1950
train acc:  0.6640625
train loss:  0.5770042538642883
train gradient:  0.19116341659790764
iteration : 1951
train acc:  0.734375
train loss:  0.5765240788459778
train gradient:  0.18174463015748135
iteration : 1952
train acc:  0.6796875
train loss:  0.5403616428375244
train gradient:  0.2664831874578999
iteration : 1953
train acc:  0.6484375
train loss:  0.595393180847168
train gradient:  0.17409377181315736
iteration : 1954
train acc:  0.7421875
train loss:  0.5879783034324646
train gradient:  0.2572670417910217
iteration : 1955
train acc:  0.71875
train loss:  0.4997139573097229
train gradient:  0.12434298493613881
iteration : 1956
train acc:  0.71875
train loss:  0.5236399173736572
train gradient:  0.12627497543298002
iteration : 1957
train acc:  0.703125
train loss:  0.5576560497283936
train gradient:  0.19996335713537253
iteration : 1958
train acc:  0.734375
train loss:  0.5095300078392029
train gradient:  0.13694649709429374
iteration : 1959
train acc:  0.7265625
train loss:  0.5525848865509033
train gradient:  0.14320484983177334
iteration : 1960
train acc:  0.7421875
train loss:  0.5504879951477051
train gradient:  0.17754346432623413
iteration : 1961
train acc:  0.703125
train loss:  0.4966484606266022
train gradient:  0.15800642335917306
iteration : 1962
train acc:  0.7421875
train loss:  0.507733941078186
train gradient:  0.11325890043063157
iteration : 1963
train acc:  0.6484375
train loss:  0.6034572720527649
train gradient:  0.17804175375848907
iteration : 1964
train acc:  0.6875
train loss:  0.5975711345672607
train gradient:  0.16367514404652053
iteration : 1965
train acc:  0.6796875
train loss:  0.554949164390564
train gradient:  0.15840383252786622
iteration : 1966
train acc:  0.703125
train loss:  0.5680884122848511
train gradient:  0.19690957274283225
iteration : 1967
train acc:  0.7265625
train loss:  0.5777549743652344
train gradient:  0.1745347867639065
iteration : 1968
train acc:  0.6796875
train loss:  0.5395193099975586
train gradient:  0.14737914353392986
iteration : 1969
train acc:  0.7734375
train loss:  0.5211842060089111
train gradient:  0.13062157583621434
iteration : 1970
train acc:  0.6640625
train loss:  0.5839648246765137
train gradient:  0.17220559075395972
iteration : 1971
train acc:  0.71875
train loss:  0.5649601817131042
train gradient:  0.1924632790887171
iteration : 1972
train acc:  0.640625
train loss:  0.5942313075065613
train gradient:  0.18959480861101957
iteration : 1973
train acc:  0.7421875
train loss:  0.5554108023643494
train gradient:  0.1665910048797317
iteration : 1974
train acc:  0.7421875
train loss:  0.519622802734375
train gradient:  0.23185709005379052
iteration : 1975
train acc:  0.7109375
train loss:  0.5451856851577759
train gradient:  0.1418161461488783
iteration : 1976
train acc:  0.7265625
train loss:  0.5535345077514648
train gradient:  0.18263570865334472
iteration : 1977
train acc:  0.7109375
train loss:  0.586311936378479
train gradient:  0.16146713159779283
iteration : 1978
train acc:  0.734375
train loss:  0.494950532913208
train gradient:  0.2595890565526818
iteration : 1979
train acc:  0.6953125
train loss:  0.5144488215446472
train gradient:  0.18522195029470157
iteration : 1980
train acc:  0.671875
train loss:  0.5671267509460449
train gradient:  0.19713384844787601
iteration : 1981
train acc:  0.6875
train loss:  0.5872540473937988
train gradient:  0.1575897737388407
iteration : 1982
train acc:  0.7109375
train loss:  0.5869890451431274
train gradient:  0.2277888887371899
iteration : 1983
train acc:  0.7109375
train loss:  0.5099523067474365
train gradient:  0.1366174644404312
iteration : 1984
train acc:  0.703125
train loss:  0.5118806958198547
train gradient:  0.13764385683753017
iteration : 1985
train acc:  0.6640625
train loss:  0.6256160736083984
train gradient:  0.1838865670915374
iteration : 1986
train acc:  0.7578125
train loss:  0.5350401997566223
train gradient:  0.1369691174691506
iteration : 1987
train acc:  0.75
train loss:  0.573265552520752
train gradient:  0.17311811529245516
iteration : 1988
train acc:  0.6875
train loss:  0.5544522404670715
train gradient:  0.19149809766096798
iteration : 1989
train acc:  0.7265625
train loss:  0.5297627449035645
train gradient:  0.1596652968669547
iteration : 1990
train acc:  0.6875
train loss:  0.559399425983429
train gradient:  0.1352998610910866
iteration : 1991
train acc:  0.703125
train loss:  0.5566719770431519
train gradient:  0.15041662278418322
iteration : 1992
train acc:  0.6171875
train loss:  0.6141230463981628
train gradient:  0.2033543837381649
iteration : 1993
train acc:  0.7421875
train loss:  0.5702872276306152
train gradient:  0.18751148351622565
iteration : 1994
train acc:  0.7265625
train loss:  0.5262027978897095
train gradient:  0.1654514618952147
iteration : 1995
train acc:  0.65625
train loss:  0.6230590343475342
train gradient:  0.18766041401315625
iteration : 1996
train acc:  0.7421875
train loss:  0.5691118240356445
train gradient:  0.20248978684916286
iteration : 1997
train acc:  0.6953125
train loss:  0.5224959850311279
train gradient:  0.16585834333399885
iteration : 1998
train acc:  0.71875
train loss:  0.5524311065673828
train gradient:  0.1949055043141482
iteration : 1999
train acc:  0.7265625
train loss:  0.5482597947120667
train gradient:  0.15669625748671873
iteration : 2000
train acc:  0.734375
train loss:  0.5264626741409302
train gradient:  0.15616403584905536
iteration : 2001
train acc:  0.75
train loss:  0.5168737173080444
train gradient:  0.1829223133803005
iteration : 2002
train acc:  0.765625
train loss:  0.491397500038147
train gradient:  0.14207154916570813
iteration : 2003
train acc:  0.671875
train loss:  0.5495023131370544
train gradient:  0.16097815458938391
iteration : 2004
train acc:  0.7109375
train loss:  0.5401755571365356
train gradient:  0.13521068225702004
iteration : 2005
train acc:  0.625
train loss:  0.6012977361679077
train gradient:  0.156874803135847
iteration : 2006
train acc:  0.7109375
train loss:  0.5464794039726257
train gradient:  0.14891661775361892
iteration : 2007
train acc:  0.6953125
train loss:  0.5656107664108276
train gradient:  0.18135367590323653
iteration : 2008
train acc:  0.734375
train loss:  0.5682864189147949
train gradient:  0.21230220892951562
iteration : 2009
train acc:  0.75
train loss:  0.5445356965065002
train gradient:  0.17014148164783255
iteration : 2010
train acc:  0.7421875
train loss:  0.5385032892227173
train gradient:  0.1352361554798448
iteration : 2011
train acc:  0.625
train loss:  0.6096169948577881
train gradient:  0.224411297595704
iteration : 2012
train acc:  0.6484375
train loss:  0.5860412120819092
train gradient:  0.1809424551253418
iteration : 2013
train acc:  0.7109375
train loss:  0.5710594058036804
train gradient:  0.23551734139842412
iteration : 2014
train acc:  0.7265625
train loss:  0.5155938863754272
train gradient:  0.2261744022027918
iteration : 2015
train acc:  0.734375
train loss:  0.5171812176704407
train gradient:  0.18932188690656077
iteration : 2016
train acc:  0.65625
train loss:  0.5458797812461853
train gradient:  0.14320438519751244
iteration : 2017
train acc:  0.6953125
train loss:  0.5365883111953735
train gradient:  0.15254520363624197
iteration : 2018
train acc:  0.6953125
train loss:  0.5547708868980408
train gradient:  0.14087249757895556
iteration : 2019
train acc:  0.75
train loss:  0.5069886445999146
train gradient:  0.14928885270037734
iteration : 2020
train acc:  0.671875
train loss:  0.5681843757629395
train gradient:  0.15717324316045833
iteration : 2021
train acc:  0.6640625
train loss:  0.5508889555931091
train gradient:  0.1749127054159132
iteration : 2022
train acc:  0.6953125
train loss:  0.5311390161514282
train gradient:  0.15062696665756625
iteration : 2023
train acc:  0.7109375
train loss:  0.5438928604125977
train gradient:  0.15516894189280755
iteration : 2024
train acc:  0.6875
train loss:  0.5275992155075073
train gradient:  0.1148136449027007
iteration : 2025
train acc:  0.71875
train loss:  0.5450451970100403
train gradient:  0.23096041702351405
iteration : 2026
train acc:  0.6484375
train loss:  0.6227959990501404
train gradient:  0.18587768851242076
iteration : 2027
train acc:  0.65625
train loss:  0.5791999101638794
train gradient:  0.1666866674013369
iteration : 2028
train acc:  0.75
train loss:  0.5460634827613831
train gradient:  0.15503758763597447
iteration : 2029
train acc:  0.6953125
train loss:  0.6026291847229004
train gradient:  0.1653980786535139
iteration : 2030
train acc:  0.75
train loss:  0.5142554640769958
train gradient:  0.1569209622046982
iteration : 2031
train acc:  0.71875
train loss:  0.5226811766624451
train gradient:  0.17949358820148498
iteration : 2032
train acc:  0.7265625
train loss:  0.5366660356521606
train gradient:  0.15625574658994035
iteration : 2033
train acc:  0.765625
train loss:  0.4975357949733734
train gradient:  0.16715405511323878
iteration : 2034
train acc:  0.6953125
train loss:  0.6168568134307861
train gradient:  0.2087349807072678
iteration : 2035
train acc:  0.671875
train loss:  0.5396357774734497
train gradient:  0.14061646074351924
iteration : 2036
train acc:  0.7265625
train loss:  0.46257805824279785
train gradient:  0.14094530520860407
iteration : 2037
train acc:  0.703125
train loss:  0.5427846908569336
train gradient:  0.16075762397512186
iteration : 2038
train acc:  0.6796875
train loss:  0.5836163759231567
train gradient:  0.1923412436354869
iteration : 2039
train acc:  0.6796875
train loss:  0.5730767250061035
train gradient:  0.12359667805465385
iteration : 2040
train acc:  0.75
train loss:  0.5063268542289734
train gradient:  0.11498111415776029
iteration : 2041
train acc:  0.671875
train loss:  0.5712442398071289
train gradient:  0.20911880903247532
iteration : 2042
train acc:  0.6640625
train loss:  0.5568145513534546
train gradient:  0.19957535543932803
iteration : 2043
train acc:  0.796875
train loss:  0.5012288093566895
train gradient:  0.11581995240139045
iteration : 2044
train acc:  0.6875
train loss:  0.5380076169967651
train gradient:  0.14236788484432084
iteration : 2045
train acc:  0.7421875
train loss:  0.5259938836097717
train gradient:  0.1581926730937681
iteration : 2046
train acc:  0.734375
train loss:  0.5613913536071777
train gradient:  0.15343358038354676
iteration : 2047
train acc:  0.6796875
train loss:  0.5653166770935059
train gradient:  0.16198283052910303
iteration : 2048
train acc:  0.671875
train loss:  0.6216405034065247
train gradient:  0.21577321840589753
iteration : 2049
train acc:  0.734375
train loss:  0.5543625354766846
train gradient:  0.1480332828775492
iteration : 2050
train acc:  0.703125
train loss:  0.5697529315948486
train gradient:  0.20155553870816156
iteration : 2051
train acc:  0.6953125
train loss:  0.5495384335517883
train gradient:  0.16128678838871954
iteration : 2052
train acc:  0.7265625
train loss:  0.5780469179153442
train gradient:  0.21390450075261308
iteration : 2053
train acc:  0.625
train loss:  0.5915423035621643
train gradient:  0.15696061813935383
iteration : 2054
train acc:  0.734375
train loss:  0.5208252668380737
train gradient:  0.1608537043307478
iteration : 2055
train acc:  0.7421875
train loss:  0.5177498459815979
train gradient:  0.17309254035538357
iteration : 2056
train acc:  0.6640625
train loss:  0.5453166365623474
train gradient:  0.16567111667939133
iteration : 2057
train acc:  0.6875
train loss:  0.565940797328949
train gradient:  0.2003377757713049
iteration : 2058
train acc:  0.7109375
train loss:  0.5524026155471802
train gradient:  0.13104670648222955
iteration : 2059
train acc:  0.6953125
train loss:  0.5850666761398315
train gradient:  0.16910588950369868
iteration : 2060
train acc:  0.6640625
train loss:  0.5844300389289856
train gradient:  0.1661917736862003
iteration : 2061
train acc:  0.6875
train loss:  0.5535447597503662
train gradient:  0.13860799462610218
iteration : 2062
train acc:  0.78125
train loss:  0.473949134349823
train gradient:  0.13549151795926678
iteration : 2063
train acc:  0.7578125
train loss:  0.5345808267593384
train gradient:  0.1644920329703356
iteration : 2064
train acc:  0.703125
train loss:  0.5783060789108276
train gradient:  0.16888025083712832
iteration : 2065
train acc:  0.6796875
train loss:  0.5389025211334229
train gradient:  0.12938296426410287
iteration : 2066
train acc:  0.6640625
train loss:  0.5778070688247681
train gradient:  0.16737556882260857
iteration : 2067
train acc:  0.7109375
train loss:  0.5112357139587402
train gradient:  0.15805674700130973
iteration : 2068
train acc:  0.7265625
train loss:  0.5463874340057373
train gradient:  0.13433771443609688
iteration : 2069
train acc:  0.703125
train loss:  0.5570918321609497
train gradient:  0.16644210700972123
iteration : 2070
train acc:  0.765625
train loss:  0.5516467094421387
train gradient:  0.18353673308717658
iteration : 2071
train acc:  0.765625
train loss:  0.5541117191314697
train gradient:  0.17512391810546185
iteration : 2072
train acc:  0.78125
train loss:  0.4853971004486084
train gradient:  0.12496972665035216
iteration : 2073
train acc:  0.6796875
train loss:  0.5510579347610474
train gradient:  0.16990236225533123
iteration : 2074
train acc:  0.6328125
train loss:  0.6078305840492249
train gradient:  0.27413977195160516
iteration : 2075
train acc:  0.7109375
train loss:  0.5031198263168335
train gradient:  0.14203217919175576
iteration : 2076
train acc:  0.7265625
train loss:  0.48000115156173706
train gradient:  0.16423673755160717
iteration : 2077
train acc:  0.6796875
train loss:  0.5708937644958496
train gradient:  0.22997844813191082
iteration : 2078
train acc:  0.7578125
train loss:  0.4925485849380493
train gradient:  0.13744640022675952
iteration : 2079
train acc:  0.7421875
train loss:  0.47450119256973267
train gradient:  0.14693108186443787
iteration : 2080
train acc:  0.765625
train loss:  0.5259621143341064
train gradient:  0.1753194293186498
iteration : 2081
train acc:  0.6484375
train loss:  0.6209098100662231
train gradient:  0.19996030221396252
iteration : 2082
train acc:  0.7421875
train loss:  0.4938150644302368
train gradient:  0.16029295620715528
iteration : 2083
train acc:  0.7578125
train loss:  0.5339747667312622
train gradient:  0.14581334808780289
iteration : 2084
train acc:  0.7578125
train loss:  0.5099451541900635
train gradient:  0.12291811957387415
iteration : 2085
train acc:  0.6171875
train loss:  0.6283630132675171
train gradient:  0.27375108135886095
iteration : 2086
train acc:  0.6953125
train loss:  0.5727012157440186
train gradient:  0.18073535082311748
iteration : 2087
train acc:  0.6953125
train loss:  0.5563450455665588
train gradient:  0.20283026603360294
iteration : 2088
train acc:  0.6796875
train loss:  0.5582749843597412
train gradient:  0.18928857017495454
iteration : 2089
train acc:  0.7265625
train loss:  0.5748529434204102
train gradient:  0.18163556361107028
iteration : 2090
train acc:  0.71875
train loss:  0.5842864513397217
train gradient:  0.15506227283961607
iteration : 2091
train acc:  0.625
train loss:  0.6271407604217529
train gradient:  0.2142443414601614
iteration : 2092
train acc:  0.6484375
train loss:  0.554344654083252
train gradient:  0.2004615659007418
iteration : 2093
train acc:  0.71875
train loss:  0.539624810218811
train gradient:  0.1746207627959393
iteration : 2094
train acc:  0.734375
train loss:  0.5348615646362305
train gradient:  0.16517307091310018
iteration : 2095
train acc:  0.671875
train loss:  0.5526750683784485
train gradient:  0.14264328900771378
iteration : 2096
train acc:  0.6875
train loss:  0.5766527652740479
train gradient:  0.19313955165826963
iteration : 2097
train acc:  0.6875
train loss:  0.5577313303947449
train gradient:  0.15985297828739461
iteration : 2098
train acc:  0.6796875
train loss:  0.5560746192932129
train gradient:  0.15797352275149196
iteration : 2099
train acc:  0.71875
train loss:  0.5292775630950928
train gradient:  0.11807817096619287
iteration : 2100
train acc:  0.703125
train loss:  0.5365754961967468
train gradient:  0.19912387071178905
iteration : 2101
train acc:  0.6796875
train loss:  0.5791234374046326
train gradient:  0.24809706686265026
iteration : 2102
train acc:  0.7109375
train loss:  0.5535851716995239
train gradient:  0.14710375076737395
iteration : 2103
train acc:  0.7421875
train loss:  0.4758424460887909
train gradient:  0.21143567202772695
iteration : 2104
train acc:  0.78125
train loss:  0.5074145793914795
train gradient:  0.1403300129711078
iteration : 2105
train acc:  0.703125
train loss:  0.5727976560592651
train gradient:  0.21085703204680334
iteration : 2106
train acc:  0.734375
train loss:  0.5434621572494507
train gradient:  0.13509010513178574
iteration : 2107
train acc:  0.71875
train loss:  0.5478631258010864
train gradient:  0.13227550182469094
iteration : 2108
train acc:  0.6796875
train loss:  0.5727946758270264
train gradient:  0.19942233221244987
iteration : 2109
train acc:  0.734375
train loss:  0.5345607995986938
train gradient:  0.15753319393938559
iteration : 2110
train acc:  0.6796875
train loss:  0.57695472240448
train gradient:  0.16393275196575763
iteration : 2111
train acc:  0.703125
train loss:  0.5308760404586792
train gradient:  0.16426278085976903
iteration : 2112
train acc:  0.7265625
train loss:  0.5271139144897461
train gradient:  0.15078085305260686
iteration : 2113
train acc:  0.7421875
train loss:  0.5044728517532349
train gradient:  0.143504753411092
iteration : 2114
train acc:  0.7109375
train loss:  0.5607401728630066
train gradient:  0.17588155733657623
iteration : 2115
train acc:  0.7109375
train loss:  0.5047856569290161
train gradient:  0.13722864438508214
iteration : 2116
train acc:  0.703125
train loss:  0.5432515740394592
train gradient:  0.1473968109141779
iteration : 2117
train acc:  0.71875
train loss:  0.525638222694397
train gradient:  0.14225921179215412
iteration : 2118
train acc:  0.6640625
train loss:  0.5924962162971497
train gradient:  0.18341363302325123
iteration : 2119
train acc:  0.6640625
train loss:  0.6015897989273071
train gradient:  0.16811007408267709
iteration : 2120
train acc:  0.6953125
train loss:  0.611716091632843
train gradient:  0.15949479381483903
iteration : 2121
train acc:  0.7421875
train loss:  0.47751787304878235
train gradient:  0.1113383529494481
iteration : 2122
train acc:  0.6640625
train loss:  0.573352575302124
train gradient:  0.1536860785286439
iteration : 2123
train acc:  0.671875
train loss:  0.5743012428283691
train gradient:  0.14908917126725335
iteration : 2124
train acc:  0.7109375
train loss:  0.5378511548042297
train gradient:  0.141393300817371
iteration : 2125
train acc:  0.734375
train loss:  0.5072895288467407
train gradient:  0.12260253879419
iteration : 2126
train acc:  0.671875
train loss:  0.5289086103439331
train gradient:  0.1606073735068801
iteration : 2127
train acc:  0.71875
train loss:  0.5369683504104614
train gradient:  0.2302272142784464
iteration : 2128
train acc:  0.703125
train loss:  0.5411453247070312
train gradient:  0.1262427164504037
iteration : 2129
train acc:  0.734375
train loss:  0.536394476890564
train gradient:  0.12782057738738298
iteration : 2130
train acc:  0.734375
train loss:  0.5370290279388428
train gradient:  0.16632675937949426
iteration : 2131
train acc:  0.7734375
train loss:  0.4944366216659546
train gradient:  0.12592495202533602
iteration : 2132
train acc:  0.7109375
train loss:  0.534702718257904
train gradient:  0.16951664454964685
iteration : 2133
train acc:  0.703125
train loss:  0.5200064778327942
train gradient:  0.14743078948106136
iteration : 2134
train acc:  0.6953125
train loss:  0.5589344501495361
train gradient:  0.14515195481658943
iteration : 2135
train acc:  0.7890625
train loss:  0.4938194751739502
train gradient:  0.13948850431380425
iteration : 2136
train acc:  0.7421875
train loss:  0.5298810601234436
train gradient:  0.16425291051015256
iteration : 2137
train acc:  0.65625
train loss:  0.6450637578964233
train gradient:  0.3579068995131665
iteration : 2138
train acc:  0.7734375
train loss:  0.5011980533599854
train gradient:  0.11772433865755291
iteration : 2139
train acc:  0.6796875
train loss:  0.5559283494949341
train gradient:  0.16126120484237552
iteration : 2140
train acc:  0.65625
train loss:  0.5816639065742493
train gradient:  0.18742698153768533
iteration : 2141
train acc:  0.7578125
train loss:  0.51634681224823
train gradient:  0.13376039819222663
iteration : 2142
train acc:  0.671875
train loss:  0.5797613859176636
train gradient:  0.18640986774846452
iteration : 2143
train acc:  0.6640625
train loss:  0.6000625491142273
train gradient:  0.20211235316513648
iteration : 2144
train acc:  0.6953125
train loss:  0.5753628015518188
train gradient:  0.15975922710012713
iteration : 2145
train acc:  0.7265625
train loss:  0.5865179896354675
train gradient:  0.17763122879124266
iteration : 2146
train acc:  0.71875
train loss:  0.509192943572998
train gradient:  0.1756389351113638
iteration : 2147
train acc:  0.71875
train loss:  0.5164003372192383
train gradient:  0.14113119205612712
iteration : 2148
train acc:  0.71875
train loss:  0.5329316854476929
train gradient:  0.11878170220721726
iteration : 2149
train acc:  0.7421875
train loss:  0.5340346097946167
train gradient:  0.13462758318085466
iteration : 2150
train acc:  0.6875
train loss:  0.5193355679512024
train gradient:  0.1279544305667564
iteration : 2151
train acc:  0.734375
train loss:  0.5037347674369812
train gradient:  0.15941483460493563
iteration : 2152
train acc:  0.6484375
train loss:  0.5777102708816528
train gradient:  0.15513242397000715
iteration : 2153
train acc:  0.765625
train loss:  0.4962284564971924
train gradient:  0.12358983213865099
iteration : 2154
train acc:  0.703125
train loss:  0.5852097272872925
train gradient:  0.20445893489410077
iteration : 2155
train acc:  0.65625
train loss:  0.5977931618690491
train gradient:  0.1838254677819482
iteration : 2156
train acc:  0.625
train loss:  0.6155608892440796
train gradient:  0.23236721730443985
iteration : 2157
train acc:  0.6875
train loss:  0.5755566358566284
train gradient:  0.1765894026901949
iteration : 2158
train acc:  0.7109375
train loss:  0.5852038860321045
train gradient:  0.21829792546487364
iteration : 2159
train acc:  0.7265625
train loss:  0.536150336265564
train gradient:  0.13465220388608934
iteration : 2160
train acc:  0.7421875
train loss:  0.5184102058410645
train gradient:  0.1631260819649049
iteration : 2161
train acc:  0.734375
train loss:  0.5561583042144775
train gradient:  0.14912511266286088
iteration : 2162
train acc:  0.71875
train loss:  0.5296430587768555
train gradient:  0.15492932398183323
iteration : 2163
train acc:  0.6953125
train loss:  0.5324987173080444
train gradient:  0.14878865192591834
iteration : 2164
train acc:  0.71875
train loss:  0.571613073348999
train gradient:  0.14135827676434293
iteration : 2165
train acc:  0.6953125
train loss:  0.5356525182723999
train gradient:  0.18885165314749303
iteration : 2166
train acc:  0.6640625
train loss:  0.5516330003738403
train gradient:  0.16837134696762518
iteration : 2167
train acc:  0.7265625
train loss:  0.5349216461181641
train gradient:  0.14315613737712493
iteration : 2168
train acc:  0.71875
train loss:  0.5868198871612549
train gradient:  0.2209790308152894
iteration : 2169
train acc:  0.7265625
train loss:  0.535611093044281
train gradient:  0.13739258640078295
iteration : 2170
train acc:  0.734375
train loss:  0.5048058032989502
train gradient:  0.17592884296167852
iteration : 2171
train acc:  0.7890625
train loss:  0.48042795062065125
train gradient:  0.13398230747110842
iteration : 2172
train acc:  0.6875
train loss:  0.5772452354431152
train gradient:  0.15857652246137954
iteration : 2173
train acc:  0.6953125
train loss:  0.5612289309501648
train gradient:  0.28785606915128414
iteration : 2174
train acc:  0.7421875
train loss:  0.5155851244926453
train gradient:  0.15745558399029183
iteration : 2175
train acc:  0.6796875
train loss:  0.5450281500816345
train gradient:  0.17853898723681272
iteration : 2176
train acc:  0.7109375
train loss:  0.5632734298706055
train gradient:  0.16440775014417675
iteration : 2177
train acc:  0.7421875
train loss:  0.5361967086791992
train gradient:  0.14473233162770505
iteration : 2178
train acc:  0.703125
train loss:  0.5753078460693359
train gradient:  0.1568865271447731
iteration : 2179
train acc:  0.703125
train loss:  0.5682511329650879
train gradient:  0.16198908566763975
iteration : 2180
train acc:  0.6796875
train loss:  0.5320739150047302
train gradient:  0.13248401643673557
iteration : 2181
train acc:  0.6640625
train loss:  0.56609708070755
train gradient:  0.1405114370023148
iteration : 2182
train acc:  0.765625
train loss:  0.5121260285377502
train gradient:  0.12856795017318728
iteration : 2183
train acc:  0.6484375
train loss:  0.6233678460121155
train gradient:  0.18360360951177845
iteration : 2184
train acc:  0.7109375
train loss:  0.5556715726852417
train gradient:  0.15311545436720864
iteration : 2185
train acc:  0.703125
train loss:  0.5159666538238525
train gradient:  0.162842736065171
iteration : 2186
train acc:  0.75
train loss:  0.5426173210144043
train gradient:  0.16278733475367818
iteration : 2187
train acc:  0.7734375
train loss:  0.4797845184803009
train gradient:  0.11549860143739785
iteration : 2188
train acc:  0.71875
train loss:  0.5479167699813843
train gradient:  0.15338446922075305
iteration : 2189
train acc:  0.7578125
train loss:  0.5334997177124023
train gradient:  0.13399403707720592
iteration : 2190
train acc:  0.703125
train loss:  0.5509613752365112
train gradient:  0.1380232508639177
iteration : 2191
train acc:  0.6484375
train loss:  0.5992650985717773
train gradient:  0.20892908795880616
iteration : 2192
train acc:  0.7109375
train loss:  0.5860274434089661
train gradient:  0.12497672449542996
iteration : 2193
train acc:  0.7265625
train loss:  0.49796926975250244
train gradient:  0.1365086479368196
iteration : 2194
train acc:  0.75
train loss:  0.5263929963111877
train gradient:  0.13795297135103865
iteration : 2195
train acc:  0.6796875
train loss:  0.5378553867340088
train gradient:  0.15180488365631545
iteration : 2196
train acc:  0.7265625
train loss:  0.5253883004188538
train gradient:  0.20326303812604787
iteration : 2197
train acc:  0.765625
train loss:  0.4809073209762573
train gradient:  0.11893715336221339
iteration : 2198
train acc:  0.6875
train loss:  0.569744348526001
train gradient:  0.13150661720886397
iteration : 2199
train acc:  0.671875
train loss:  0.5376714468002319
train gradient:  0.15199118806830023
iteration : 2200
train acc:  0.7265625
train loss:  0.5098823308944702
train gradient:  0.16817913794087644
iteration : 2201
train acc:  0.7265625
train loss:  0.5071059465408325
train gradient:  0.1862948104659017
iteration : 2202
train acc:  0.6875
train loss:  0.5593541264533997
train gradient:  0.1662186745423333
iteration : 2203
train acc:  0.65625
train loss:  0.6016607284545898
train gradient:  0.15489803141061678
iteration : 2204
train acc:  0.703125
train loss:  0.5673396587371826
train gradient:  0.1713209151450042
iteration : 2205
train acc:  0.7109375
train loss:  0.5505220890045166
train gradient:  0.15356451686371486
iteration : 2206
train acc:  0.7109375
train loss:  0.5579335689544678
train gradient:  0.11623654991317633
iteration : 2207
train acc:  0.734375
train loss:  0.5244371891021729
train gradient:  0.15325346863755981
iteration : 2208
train acc:  0.6953125
train loss:  0.6207334399223328
train gradient:  0.19581788442244571
iteration : 2209
train acc:  0.734375
train loss:  0.49781155586242676
train gradient:  0.09994815778572218
iteration : 2210
train acc:  0.6484375
train loss:  0.5711114406585693
train gradient:  0.17691617246315514
iteration : 2211
train acc:  0.703125
train loss:  0.5332727432250977
train gradient:  0.16069510203166293
iteration : 2212
train acc:  0.6796875
train loss:  0.5779708027839661
train gradient:  0.19063997967844204
iteration : 2213
train acc:  0.671875
train loss:  0.5831484198570251
train gradient:  0.23181467847732456
iteration : 2214
train acc:  0.7578125
train loss:  0.5191654562950134
train gradient:  0.15458520736317258
iteration : 2215
train acc:  0.6640625
train loss:  0.5489261150360107
train gradient:  0.14211635185863863
iteration : 2216
train acc:  0.703125
train loss:  0.5468160510063171
train gradient:  0.151616318166844
iteration : 2217
train acc:  0.6796875
train loss:  0.5963588356971741
train gradient:  0.20189495350644063
iteration : 2218
train acc:  0.7421875
train loss:  0.5089461803436279
train gradient:  0.1516177218807913
iteration : 2219
train acc:  0.6875
train loss:  0.6268329620361328
train gradient:  0.17984599998808715
iteration : 2220
train acc:  0.7421875
train loss:  0.5256842374801636
train gradient:  0.15106197096514956
iteration : 2221
train acc:  0.75
train loss:  0.5362206697463989
train gradient:  0.14157696703466993
iteration : 2222
train acc:  0.6640625
train loss:  0.58949214220047
train gradient:  0.1658077590863654
iteration : 2223
train acc:  0.7265625
train loss:  0.5184342861175537
train gradient:  0.1464672431415486
iteration : 2224
train acc:  0.75
train loss:  0.48179125785827637
train gradient:  0.19004764095044122
iteration : 2225
train acc:  0.765625
train loss:  0.47427070140838623
train gradient:  0.13653991912536212
iteration : 2226
train acc:  0.6484375
train loss:  0.6004959344863892
train gradient:  0.16499885252589835
iteration : 2227
train acc:  0.7421875
train loss:  0.4930902123451233
train gradient:  0.17627439735096384
iteration : 2228
train acc:  0.765625
train loss:  0.489053338766098
train gradient:  0.09805568243000548
iteration : 2229
train acc:  0.6796875
train loss:  0.5585108995437622
train gradient:  0.1469814292862406
iteration : 2230
train acc:  0.7109375
train loss:  0.5490871667861938
train gradient:  0.15896622271148103
iteration : 2231
train acc:  0.71875
train loss:  0.534065306186676
train gradient:  0.20201238036320002
iteration : 2232
train acc:  0.7265625
train loss:  0.5397248268127441
train gradient:  0.1682540471166159
iteration : 2233
train acc:  0.6796875
train loss:  0.5924060940742493
train gradient:  0.2046589982785123
iteration : 2234
train acc:  0.71875
train loss:  0.5359897017478943
train gradient:  0.17789550831256373
iteration : 2235
train acc:  0.7109375
train loss:  0.5688422322273254
train gradient:  0.18135205634369594
iteration : 2236
train acc:  0.75
train loss:  0.5410146117210388
train gradient:  0.1691788646917232
iteration : 2237
train acc:  0.6328125
train loss:  0.5753461122512817
train gradient:  0.1460291192555933
iteration : 2238
train acc:  0.7734375
train loss:  0.4968912601470947
train gradient:  0.14860372020825166
iteration : 2239
train acc:  0.703125
train loss:  0.5440459251403809
train gradient:  0.1410966477378472
iteration : 2240
train acc:  0.6640625
train loss:  0.5893884301185608
train gradient:  0.1681821241111825
iteration : 2241
train acc:  0.7421875
train loss:  0.5178520679473877
train gradient:  0.17000017672572793
iteration : 2242
train acc:  0.6328125
train loss:  0.5816770195960999
train gradient:  0.17578579404108197
iteration : 2243
train acc:  0.6875
train loss:  0.5951968431472778
train gradient:  0.2196303516711053
iteration : 2244
train acc:  0.75
train loss:  0.4754452705383301
train gradient:  0.1055028068865939
iteration : 2245
train acc:  0.734375
train loss:  0.5275506973266602
train gradient:  0.13415103545427592
iteration : 2246
train acc:  0.6875
train loss:  0.5865663886070251
train gradient:  0.23670177995799305
iteration : 2247
train acc:  0.7578125
train loss:  0.4970606565475464
train gradient:  0.12820214368395472
iteration : 2248
train acc:  0.6015625
train loss:  0.6841108798980713
train gradient:  0.19034137646926397
iteration : 2249
train acc:  0.7265625
train loss:  0.5171164274215698
train gradient:  0.18072899584943364
iteration : 2250
train acc:  0.7421875
train loss:  0.4979898929595947
train gradient:  0.12564350767275545
iteration : 2251
train acc:  0.7109375
train loss:  0.553876519203186
train gradient:  0.171761187422239
iteration : 2252
train acc:  0.6796875
train loss:  0.5397423505783081
train gradient:  0.14088169415345875
iteration : 2253
train acc:  0.6640625
train loss:  0.6264495849609375
train gradient:  0.1928493667709051
iteration : 2254
train acc:  0.7578125
train loss:  0.4833676815032959
train gradient:  0.12631522927237343
iteration : 2255
train acc:  0.7265625
train loss:  0.5054497718811035
train gradient:  0.15708648917787177
iteration : 2256
train acc:  0.75
train loss:  0.519182562828064
train gradient:  0.13500067289573792
iteration : 2257
train acc:  0.6953125
train loss:  0.5305677652359009
train gradient:  0.18598749017060512
iteration : 2258
train acc:  0.7265625
train loss:  0.5273891091346741
train gradient:  0.16739388206095568
iteration : 2259
train acc:  0.7734375
train loss:  0.46992844343185425
train gradient:  0.1392117310790526
iteration : 2260
train acc:  0.6875
train loss:  0.5060184001922607
train gradient:  0.1445573404308604
iteration : 2261
train acc:  0.671875
train loss:  0.596192479133606
train gradient:  0.27007529130861396
iteration : 2262
train acc:  0.6640625
train loss:  0.6004583835601807
train gradient:  0.20657227577824955
iteration : 2263
train acc:  0.6171875
train loss:  0.6446391344070435
train gradient:  0.1817015546656334
iteration : 2264
train acc:  0.6953125
train loss:  0.560027539730072
train gradient:  0.16682737637536493
iteration : 2265
train acc:  0.6875
train loss:  0.542851984500885
train gradient:  0.1733167810768994
iteration : 2266
train acc:  0.6796875
train loss:  0.6004883050918579
train gradient:  0.299102635598345
iteration : 2267
train acc:  0.6640625
train loss:  0.5909464359283447
train gradient:  0.19236488655425366
iteration : 2268
train acc:  0.703125
train loss:  0.4982503056526184
train gradient:  0.2729356054867828
iteration : 2269
train acc:  0.7421875
train loss:  0.5589584112167358
train gradient:  0.17689971545132455
iteration : 2270
train acc:  0.7265625
train loss:  0.565766453742981
train gradient:  0.16053863722796485
iteration : 2271
train acc:  0.7578125
train loss:  0.4945020079612732
train gradient:  0.1433347021568847
iteration : 2272
train acc:  0.7421875
train loss:  0.5173070430755615
train gradient:  0.11990730570551572
iteration : 2273
train acc:  0.734375
train loss:  0.5203990936279297
train gradient:  0.15419805839419926
iteration : 2274
train acc:  0.7109375
train loss:  0.5527914762496948
train gradient:  0.18764753043011456
iteration : 2275
train acc:  0.6796875
train loss:  0.5318001508712769
train gradient:  0.1858941302813796
iteration : 2276
train acc:  0.6171875
train loss:  0.6543068885803223
train gradient:  0.1908996992702594
iteration : 2277
train acc:  0.71875
train loss:  0.5582157373428345
train gradient:  0.15001977237816888
iteration : 2278
train acc:  0.7265625
train loss:  0.5320259928703308
train gradient:  0.14709971385668635
iteration : 2279
train acc:  0.6640625
train loss:  0.5578006505966187
train gradient:  0.169869070401305
iteration : 2280
train acc:  0.75
train loss:  0.503495991230011
train gradient:  0.14474816878533614
iteration : 2281
train acc:  0.671875
train loss:  0.5363722443580627
train gradient:  0.1240950164972697
iteration : 2282
train acc:  0.6953125
train loss:  0.5631884336471558
train gradient:  0.15854321107311486
iteration : 2283
train acc:  0.65625
train loss:  0.5680313110351562
train gradient:  0.15608395258065333
iteration : 2284
train acc:  0.6953125
train loss:  0.576824963092804
train gradient:  0.18886887424712878
iteration : 2285
train acc:  0.734375
train loss:  0.5514746904373169
train gradient:  0.1408935800402455
iteration : 2286
train acc:  0.71875
train loss:  0.5364176034927368
train gradient:  0.1390813724350421
iteration : 2287
train acc:  0.6875
train loss:  0.5288575887680054
train gradient:  0.13420624144176674
iteration : 2288
train acc:  0.734375
train loss:  0.5627812147140503
train gradient:  0.21318728634098488
iteration : 2289
train acc:  0.7109375
train loss:  0.5290473699569702
train gradient:  0.17186992377756088
iteration : 2290
train acc:  0.640625
train loss:  0.5994102954864502
train gradient:  0.19037074874094356
iteration : 2291
train acc:  0.75
train loss:  0.5043601393699646
train gradient:  0.1368258306390412
iteration : 2292
train acc:  0.7109375
train loss:  0.5067952871322632
train gradient:  0.164316880661846
iteration : 2293
train acc:  0.703125
train loss:  0.58939528465271
train gradient:  0.19409798014260643
iteration : 2294
train acc:  0.6875
train loss:  0.5332515239715576
train gradient:  0.1396943761956979
iteration : 2295
train acc:  0.78125
train loss:  0.4705013632774353
train gradient:  0.13862324452944358
iteration : 2296
train acc:  0.734375
train loss:  0.5200237035751343
train gradient:  0.14665358596871889
iteration : 2297
train acc:  0.734375
train loss:  0.5721779465675354
train gradient:  0.1790921864132682
iteration : 2298
train acc:  0.71875
train loss:  0.5484864115715027
train gradient:  0.14338147089329834
iteration : 2299
train acc:  0.6640625
train loss:  0.5655275583267212
train gradient:  0.22537944101248925
iteration : 2300
train acc:  0.6875
train loss:  0.5364667177200317
train gradient:  0.13472899414280087
iteration : 2301
train acc:  0.6875
train loss:  0.5788755416870117
train gradient:  0.18338028832957987
iteration : 2302
train acc:  0.796875
train loss:  0.4532715976238251
train gradient:  0.18124241154677967
iteration : 2303
train acc:  0.6796875
train loss:  0.6046783924102783
train gradient:  0.16085926273338136
iteration : 2304
train acc:  0.6796875
train loss:  0.5280441045761108
train gradient:  0.17924024315177442
iteration : 2305
train acc:  0.6640625
train loss:  0.6173794269561768
train gradient:  0.18116170244710977
iteration : 2306
train acc:  0.6640625
train loss:  0.5913777947425842
train gradient:  0.1997778252083844
iteration : 2307
train acc:  0.765625
train loss:  0.4704698324203491
train gradient:  0.12643287918066531
iteration : 2308
train acc:  0.7109375
train loss:  0.5314814448356628
train gradient:  0.1639230977441497
iteration : 2309
train acc:  0.7578125
train loss:  0.5133485198020935
train gradient:  0.20241785784903146
iteration : 2310
train acc:  0.75
train loss:  0.5164896249771118
train gradient:  0.1485225207955519
iteration : 2311
train acc:  0.6171875
train loss:  0.6013216376304626
train gradient:  0.22029214921685147
iteration : 2312
train acc:  0.703125
train loss:  0.5679915547370911
train gradient:  0.1667032455561896
iteration : 2313
train acc:  0.6640625
train loss:  0.5855201482772827
train gradient:  0.1683835342024843
iteration : 2314
train acc:  0.6796875
train loss:  0.5579637289047241
train gradient:  0.15780873306269338
iteration : 2315
train acc:  0.6953125
train loss:  0.5368671417236328
train gradient:  0.1857584419266446
iteration : 2316
train acc:  0.640625
train loss:  0.6103808879852295
train gradient:  0.3062878651840237
iteration : 2317
train acc:  0.7265625
train loss:  0.540431559085846
train gradient:  0.15881740794442684
iteration : 2318
train acc:  0.7265625
train loss:  0.48812204599380493
train gradient:  0.13499707449578666
iteration : 2319
train acc:  0.71875
train loss:  0.5382677912712097
train gradient:  0.1685134854756023
iteration : 2320
train acc:  0.6875
train loss:  0.6112309694290161
train gradient:  0.18387609301152394
iteration : 2321
train acc:  0.7265625
train loss:  0.533450186252594
train gradient:  0.15788235118737964
iteration : 2322
train acc:  0.609375
train loss:  0.6141688823699951
train gradient:  0.1875639328669998
iteration : 2323
train acc:  0.65625
train loss:  0.5440705418586731
train gradient:  0.13778364862248316
iteration : 2324
train acc:  0.6953125
train loss:  0.5807814598083496
train gradient:  0.28435923747408454
iteration : 2325
train acc:  0.7109375
train loss:  0.5816776752471924
train gradient:  0.20722161512253356
iteration : 2326
train acc:  0.6796875
train loss:  0.572634756565094
train gradient:  0.19101790427849913
iteration : 2327
train acc:  0.65625
train loss:  0.5965361595153809
train gradient:  0.2272060379500278
iteration : 2328
train acc:  0.6171875
train loss:  0.586510956287384
train gradient:  0.14976630980409422
iteration : 2329
train acc:  0.6640625
train loss:  0.6088330745697021
train gradient:  0.2102965392578952
iteration : 2330
train acc:  0.6640625
train loss:  0.5697792768478394
train gradient:  0.2475746521455171
iteration : 2331
train acc:  0.78125
train loss:  0.5106720328330994
train gradient:  0.1785213275579658
iteration : 2332
train acc:  0.703125
train loss:  0.5347902178764343
train gradient:  0.16164449209153187
iteration : 2333
train acc:  0.7890625
train loss:  0.4804671108722687
train gradient:  0.13720009962000296
iteration : 2334
train acc:  0.71875
train loss:  0.5376952290534973
train gradient:  0.13046575324519513
iteration : 2335
train acc:  0.7109375
train loss:  0.5263076424598694
train gradient:  0.14266861572741277
iteration : 2336
train acc:  0.6640625
train loss:  0.5531514883041382
train gradient:  0.1486368657818497
iteration : 2337
train acc:  0.6640625
train loss:  0.5984565019607544
train gradient:  0.2018493051822135
iteration : 2338
train acc:  0.6640625
train loss:  0.5525102019309998
train gradient:  0.2580977792460956
iteration : 2339
train acc:  0.6484375
train loss:  0.5931239128112793
train gradient:  0.16425500516465186
iteration : 2340
train acc:  0.65625
train loss:  0.6031244993209839
train gradient:  0.17534392245070257
iteration : 2341
train acc:  0.6953125
train loss:  0.5185767412185669
train gradient:  0.16622305299228807
iteration : 2342
train acc:  0.703125
train loss:  0.5022450685501099
train gradient:  0.1113963892238237
iteration : 2343
train acc:  0.734375
train loss:  0.49658864736557007
train gradient:  0.16110647562694053
iteration : 2344
train acc:  0.734375
train loss:  0.5321435332298279
train gradient:  0.1777491599490253
iteration : 2345
train acc:  0.71875
train loss:  0.556707501411438
train gradient:  0.13487240238220358
iteration : 2346
train acc:  0.6875
train loss:  0.5167091488838196
train gradient:  0.1458533109712014
iteration : 2347
train acc:  0.7578125
train loss:  0.501188337802887
train gradient:  0.17799183406721453
iteration : 2348
train acc:  0.6953125
train loss:  0.6040117144584656
train gradient:  0.2251864296780246
iteration : 2349
train acc:  0.671875
train loss:  0.5697544813156128
train gradient:  0.20160169557717592
iteration : 2350
train acc:  0.703125
train loss:  0.5641793012619019
train gradient:  0.16991110367898726
iteration : 2351
train acc:  0.7578125
train loss:  0.496437132358551
train gradient:  0.11577379759322402
iteration : 2352
train acc:  0.7265625
train loss:  0.5821519494056702
train gradient:  0.1525633815707545
iteration : 2353
train acc:  0.6796875
train loss:  0.563649594783783
train gradient:  0.18120334620314124
iteration : 2354
train acc:  0.6796875
train loss:  0.6063172817230225
train gradient:  0.16241029936939316
iteration : 2355
train acc:  0.7265625
train loss:  0.5334015488624573
train gradient:  0.14984753080520202
iteration : 2356
train acc:  0.6953125
train loss:  0.5521935224533081
train gradient:  0.18973969057900153
iteration : 2357
train acc:  0.6328125
train loss:  0.6127753853797913
train gradient:  0.1979371002810256
iteration : 2358
train acc:  0.65625
train loss:  0.6077139973640442
train gradient:  0.18788511746411868
iteration : 2359
train acc:  0.71875
train loss:  0.5390481948852539
train gradient:  0.1523385942901986
iteration : 2360
train acc:  0.703125
train loss:  0.5163111686706543
train gradient:  0.1453643327891534
iteration : 2361
train acc:  0.6640625
train loss:  0.5747031569480896
train gradient:  0.2194374931138961
iteration : 2362
train acc:  0.6640625
train loss:  0.568329930305481
train gradient:  0.19999963534556126
iteration : 2363
train acc:  0.71875
train loss:  0.5286508202552795
train gradient:  0.15690406244841298
iteration : 2364
train acc:  0.7421875
train loss:  0.521388828754425
train gradient:  0.15076123711757067
iteration : 2365
train acc:  0.6875
train loss:  0.5595380067825317
train gradient:  0.18059902708131603
iteration : 2366
train acc:  0.734375
train loss:  0.5152724981307983
train gradient:  0.1451126381111174
iteration : 2367
train acc:  0.7421875
train loss:  0.5363291501998901
train gradient:  0.22768488733875714
iteration : 2368
train acc:  0.671875
train loss:  0.6235350370407104
train gradient:  0.31530614654171335
iteration : 2369
train acc:  0.6875
train loss:  0.5686047673225403
train gradient:  0.15576171214958173
iteration : 2370
train acc:  0.65625
train loss:  0.6130803823471069
train gradient:  0.20967704176250984
iteration : 2371
train acc:  0.6875
train loss:  0.6054231524467468
train gradient:  0.2431953522945794
iteration : 2372
train acc:  0.7265625
train loss:  0.4978536367416382
train gradient:  0.15146171787631182
iteration : 2373
train acc:  0.6875
train loss:  0.5454660654067993
train gradient:  0.15167932402956516
iteration : 2374
train acc:  0.703125
train loss:  0.5723772644996643
train gradient:  0.18068831450661693
iteration : 2375
train acc:  0.609375
train loss:  0.6317379474639893
train gradient:  0.18362625065155092
iteration : 2376
train acc:  0.7421875
train loss:  0.5535448789596558
train gradient:  0.19877075637087244
iteration : 2377
train acc:  0.7578125
train loss:  0.5541884899139404
train gradient:  0.161109996538908
iteration : 2378
train acc:  0.75
train loss:  0.49183565378189087
train gradient:  0.2671077799021048
iteration : 2379
train acc:  0.6796875
train loss:  0.6168707609176636
train gradient:  0.17141248624384853
iteration : 2380
train acc:  0.671875
train loss:  0.5609018206596375
train gradient:  0.1855335333866275
iteration : 2381
train acc:  0.75
train loss:  0.5036537647247314
train gradient:  0.1158319646833411
iteration : 2382
train acc:  0.6953125
train loss:  0.5594469904899597
train gradient:  0.15531214717383102
iteration : 2383
train acc:  0.71875
train loss:  0.5251660346984863
train gradient:  0.12959844628210554
iteration : 2384
train acc:  0.8203125
train loss:  0.43255186080932617
train gradient:  0.13903647316304332
iteration : 2385
train acc:  0.734375
train loss:  0.5469114780426025
train gradient:  0.20133483258216656
iteration : 2386
train acc:  0.7265625
train loss:  0.5171962976455688
train gradient:  0.1319255779747684
iteration : 2387
train acc:  0.7265625
train loss:  0.5278348922729492
train gradient:  0.1561136147351499
iteration : 2388
train acc:  0.6796875
train loss:  0.5547430515289307
train gradient:  0.13075540793363177
iteration : 2389
train acc:  0.671875
train loss:  0.5953335762023926
train gradient:  0.1809410069575873
iteration : 2390
train acc:  0.609375
train loss:  0.6102584600448608
train gradient:  0.21456319182125244
iteration : 2391
train acc:  0.7265625
train loss:  0.5535011291503906
train gradient:  0.18891676747747177
iteration : 2392
train acc:  0.7421875
train loss:  0.5290287733078003
train gradient:  0.19558208321478454
iteration : 2393
train acc:  0.765625
train loss:  0.4963759779930115
train gradient:  0.12600917616668617
iteration : 2394
train acc:  0.7734375
train loss:  0.5070254802703857
train gradient:  0.1993552735311656
iteration : 2395
train acc:  0.6796875
train loss:  0.6090661883354187
train gradient:  0.23395633552723594
iteration : 2396
train acc:  0.6484375
train loss:  0.5965790748596191
train gradient:  0.26825101038359733
iteration : 2397
train acc:  0.6640625
train loss:  0.5791770219802856
train gradient:  0.18246018447805876
iteration : 2398
train acc:  0.796875
train loss:  0.45805785059928894
train gradient:  0.12070955036262185
iteration : 2399
train acc:  0.6796875
train loss:  0.5606347918510437
train gradient:  0.1589366906779906
iteration : 2400
train acc:  0.6796875
train loss:  0.5770795345306396
train gradient:  0.18630010660680724
iteration : 2401
train acc:  0.6875
train loss:  0.5474240779876709
train gradient:  0.15057263464029358
iteration : 2402
train acc:  0.6953125
train loss:  0.553561806678772
train gradient:  0.13589292686047316
iteration : 2403
train acc:  0.7890625
train loss:  0.4667660593986511
train gradient:  0.11784848110787215
iteration : 2404
train acc:  0.7265625
train loss:  0.5365440249443054
train gradient:  0.17077123760500473
iteration : 2405
train acc:  0.7109375
train loss:  0.5253804922103882
train gradient:  0.14194711011900094
iteration : 2406
train acc:  0.7109375
train loss:  0.5647665858268738
train gradient:  0.13511338404885992
iteration : 2407
train acc:  0.703125
train loss:  0.5129462480545044
train gradient:  0.11013479253699841
iteration : 2408
train acc:  0.75
train loss:  0.5071790218353271
train gradient:  0.13154107412624144
iteration : 2409
train acc:  0.625
train loss:  0.6033785343170166
train gradient:  0.2075484889537989
iteration : 2410
train acc:  0.6484375
train loss:  0.6152417063713074
train gradient:  0.18113346681709117
iteration : 2411
train acc:  0.71875
train loss:  0.521793007850647
train gradient:  0.15189225531971484
iteration : 2412
train acc:  0.6796875
train loss:  0.55912184715271
train gradient:  0.17473827598190647
iteration : 2413
train acc:  0.6796875
train loss:  0.562629222869873
train gradient:  0.20602076614268378
iteration : 2414
train acc:  0.7421875
train loss:  0.48360660672187805
train gradient:  0.11929115324385607
iteration : 2415
train acc:  0.671875
train loss:  0.5493456125259399
train gradient:  0.17201119369244638
iteration : 2416
train acc:  0.75
train loss:  0.5063648819923401
train gradient:  0.141631567990335
iteration : 2417
train acc:  0.7421875
train loss:  0.5503395795822144
train gradient:  0.22807930204592258
iteration : 2418
train acc:  0.7421875
train loss:  0.5256661176681519
train gradient:  0.16798775175614136
iteration : 2419
train acc:  0.71875
train loss:  0.5310290455818176
train gradient:  0.12099053540940882
iteration : 2420
train acc:  0.7421875
train loss:  0.5045928359031677
train gradient:  0.15519738370696856
iteration : 2421
train acc:  0.6875
train loss:  0.5406877994537354
train gradient:  0.1796872599078566
iteration : 2422
train acc:  0.6796875
train loss:  0.5815541744232178
train gradient:  0.22330359075635722
iteration : 2423
train acc:  0.6875
train loss:  0.592309296131134
train gradient:  0.17022321626554734
iteration : 2424
train acc:  0.6796875
train loss:  0.5461820960044861
train gradient:  0.1688008873236382
iteration : 2425
train acc:  0.71875
train loss:  0.5452467799186707
train gradient:  0.14675807824748596
iteration : 2426
train acc:  0.734375
train loss:  0.486433744430542
train gradient:  0.20177127267675932
iteration : 2427
train acc:  0.734375
train loss:  0.5152804255485535
train gradient:  0.15276024150972972
iteration : 2428
train acc:  0.703125
train loss:  0.5670861005783081
train gradient:  0.16390775000847352
iteration : 2429
train acc:  0.7265625
train loss:  0.5122867822647095
train gradient:  0.12724513988155603
iteration : 2430
train acc:  0.6796875
train loss:  0.5646024346351624
train gradient:  0.1941372209287161
iteration : 2431
train acc:  0.6796875
train loss:  0.6029941439628601
train gradient:  0.22668254988344677
iteration : 2432
train acc:  0.6953125
train loss:  0.5535299777984619
train gradient:  0.18307194561685602
iteration : 2433
train acc:  0.71875
train loss:  0.5548843145370483
train gradient:  0.1586565186429051
iteration : 2434
train acc:  0.703125
train loss:  0.5805478096008301
train gradient:  0.18141415010338224
iteration : 2435
train acc:  0.6953125
train loss:  0.576073408126831
train gradient:  0.2120547146485545
iteration : 2436
train acc:  0.734375
train loss:  0.5538489818572998
train gradient:  0.14735014323365792
iteration : 2437
train acc:  0.6640625
train loss:  0.5938729643821716
train gradient:  0.18529451967357286
iteration : 2438
train acc:  0.625
train loss:  0.6059823036193848
train gradient:  0.23013308744522443
iteration : 2439
train acc:  0.671875
train loss:  0.5304796695709229
train gradient:  0.1565991241662378
iteration : 2440
train acc:  0.6953125
train loss:  0.5384700298309326
train gradient:  0.13382868761934852
iteration : 2441
train acc:  0.703125
train loss:  0.5199360251426697
train gradient:  0.14315072611997
iteration : 2442
train acc:  0.65625
train loss:  0.6055306196212769
train gradient:  0.17289973406618256
iteration : 2443
train acc:  0.7421875
train loss:  0.5318883061408997
train gradient:  0.13940005972606068
iteration : 2444
train acc:  0.7421875
train loss:  0.5317178964614868
train gradient:  0.15213393217920843
iteration : 2445
train acc:  0.7265625
train loss:  0.5145807266235352
train gradient:  0.17497621462500276
iteration : 2446
train acc:  0.7109375
train loss:  0.5605772733688354
train gradient:  0.14743271964623642
iteration : 2447
train acc:  0.71875
train loss:  0.5570525527000427
train gradient:  0.1501294963407545
iteration : 2448
train acc:  0.65625
train loss:  0.5693471431732178
train gradient:  0.16502982720841666
iteration : 2449
train acc:  0.6796875
train loss:  0.522435188293457
train gradient:  0.1486740959894538
iteration : 2450
train acc:  0.7421875
train loss:  0.5143824815750122
train gradient:  0.14156639170462798
iteration : 2451
train acc:  0.71875
train loss:  0.527226448059082
train gradient:  0.15787977337849954
iteration : 2452
train acc:  0.703125
train loss:  0.512168824672699
train gradient:  0.13575641893240611
iteration : 2453
train acc:  0.6484375
train loss:  0.633995771408081
train gradient:  0.2613842239660709
iteration : 2454
train acc:  0.671875
train loss:  0.5739575624465942
train gradient:  0.17195976006262162
iteration : 2455
train acc:  0.6875
train loss:  0.5181097388267517
train gradient:  0.13457968059850162
iteration : 2456
train acc:  0.765625
train loss:  0.4753683805465698
train gradient:  0.1398684426487109
iteration : 2457
train acc:  0.6953125
train loss:  0.5323286056518555
train gradient:  0.12823451294443827
iteration : 2458
train acc:  0.7578125
train loss:  0.514808177947998
train gradient:  0.10701039772374796
iteration : 2459
train acc:  0.6796875
train loss:  0.5635389089584351
train gradient:  0.2513777732721839
iteration : 2460
train acc:  0.6953125
train loss:  0.5336949229240417
train gradient:  0.14299959121157285
iteration : 2461
train acc:  0.671875
train loss:  0.5509817600250244
train gradient:  0.13374664042112167
iteration : 2462
train acc:  0.6875
train loss:  0.5538938641548157
train gradient:  0.13903788350046645
iteration : 2463
train acc:  0.734375
train loss:  0.5286393761634827
train gradient:  0.20546059988695956
iteration : 2464
train acc:  0.78125
train loss:  0.47217267751693726
train gradient:  0.14663120989488193
iteration : 2465
train acc:  0.6953125
train loss:  0.5303026437759399
train gradient:  0.14291311742248464
iteration : 2466
train acc:  0.6953125
train loss:  0.5753909349441528
train gradient:  0.20597401062460388
iteration : 2467
train acc:  0.6875
train loss:  0.6119299530982971
train gradient:  0.195148605259938
iteration : 2468
train acc:  0.71875
train loss:  0.5285579562187195
train gradient:  0.15120893743520125
iteration : 2469
train acc:  0.7265625
train loss:  0.5483223795890808
train gradient:  0.2221103651379403
iteration : 2470
train acc:  0.6796875
train loss:  0.5555905103683472
train gradient:  0.17330437237775886
iteration : 2471
train acc:  0.6796875
train loss:  0.5998110771179199
train gradient:  0.2413111454486198
iteration : 2472
train acc:  0.7109375
train loss:  0.524702787399292
train gradient:  0.12559044085004945
iteration : 2473
train acc:  0.7890625
train loss:  0.47751471400260925
train gradient:  0.12812001732662362
iteration : 2474
train acc:  0.75
train loss:  0.48795366287231445
train gradient:  0.1046757080237236
iteration : 2475
train acc:  0.765625
train loss:  0.47656118869781494
train gradient:  0.11616551535215959
iteration : 2476
train acc:  0.7109375
train loss:  0.5610920190811157
train gradient:  0.16761313382091048
iteration : 2477
train acc:  0.671875
train loss:  0.5779051780700684
train gradient:  0.195748322243965
iteration : 2478
train acc:  0.7265625
train loss:  0.5202337503433228
train gradient:  0.1331507345899495
iteration : 2479
train acc:  0.7265625
train loss:  0.5258991718292236
train gradient:  0.16610220878370632
iteration : 2480
train acc:  0.71875
train loss:  0.5451951026916504
train gradient:  0.18287973613784775
iteration : 2481
train acc:  0.75
train loss:  0.524093747138977
train gradient:  0.141976194816129
iteration : 2482
train acc:  0.7890625
train loss:  0.49405646324157715
train gradient:  0.15170855595927596
iteration : 2483
train acc:  0.6796875
train loss:  0.5807449817657471
train gradient:  0.1783741777709577
iteration : 2484
train acc:  0.7109375
train loss:  0.5292869806289673
train gradient:  0.1862799017814578
iteration : 2485
train acc:  0.7734375
train loss:  0.5159620046615601
train gradient:  0.12196617800220536
iteration : 2486
train acc:  0.640625
train loss:  0.5803850889205933
train gradient:  0.15492473242543675
iteration : 2487
train acc:  0.71875
train loss:  0.4932076334953308
train gradient:  0.13332909365058634
iteration : 2488
train acc:  0.703125
train loss:  0.5370808839797974
train gradient:  0.1506304284036177
iteration : 2489
train acc:  0.75
train loss:  0.5066683292388916
train gradient:  0.1963335095312877
iteration : 2490
train acc:  0.7109375
train loss:  0.5566006898880005
train gradient:  0.1670692414122732
iteration : 2491
train acc:  0.7109375
train loss:  0.48800429701805115
train gradient:  0.13063707637202576
iteration : 2492
train acc:  0.7265625
train loss:  0.5278329849243164
train gradient:  0.14547109802737104
iteration : 2493
train acc:  0.75
train loss:  0.5222529172897339
train gradient:  0.12402577991555326
iteration : 2494
train acc:  0.6953125
train loss:  0.5199497938156128
train gradient:  0.13368382385642363
iteration : 2495
train acc:  0.71875
train loss:  0.5306887030601501
train gradient:  0.12054632392496259
iteration : 2496
train acc:  0.703125
train loss:  0.5147815942764282
train gradient:  0.1430316382690403
iteration : 2497
train acc:  0.71875
train loss:  0.5251029133796692
train gradient:  0.19093815741607026
iteration : 2498
train acc:  0.6953125
train loss:  0.5470227003097534
train gradient:  0.1599513532550917
iteration : 2499
train acc:  0.7578125
train loss:  0.4951993227005005
train gradient:  0.147506143202197
iteration : 2500
train acc:  0.703125
train loss:  0.5516170859336853
train gradient:  0.2048590929615089
iteration : 2501
train acc:  0.6875
train loss:  0.5465199947357178
train gradient:  0.14424006218755656
iteration : 2502
train acc:  0.7421875
train loss:  0.5169273614883423
train gradient:  0.11927786034554537
iteration : 2503
train acc:  0.65625
train loss:  0.590347409248352
train gradient:  0.16607605873696107
iteration : 2504
train acc:  0.6796875
train loss:  0.5516800880432129
train gradient:  0.163063336043837
iteration : 2505
train acc:  0.6875
train loss:  0.576784610748291
train gradient:  0.2215513461183306
iteration : 2506
train acc:  0.734375
train loss:  0.5125414133071899
train gradient:  0.15225891455637078
iteration : 2507
train acc:  0.7734375
train loss:  0.4855870008468628
train gradient:  0.16211242283633148
iteration : 2508
train acc:  0.6875
train loss:  0.6310182213783264
train gradient:  0.2782933450358255
iteration : 2509
train acc:  0.6875
train loss:  0.5746110081672668
train gradient:  0.1965444462669354
iteration : 2510
train acc:  0.7890625
train loss:  0.47721028327941895
train gradient:  0.1322207178826727
iteration : 2511
train acc:  0.6796875
train loss:  0.5484535694122314
train gradient:  0.1671821461007304
iteration : 2512
train acc:  0.671875
train loss:  0.5654891133308411
train gradient:  0.1840510516959147
iteration : 2513
train acc:  0.6484375
train loss:  0.5895625352859497
train gradient:  0.12160784145296422
iteration : 2514
train acc:  0.6953125
train loss:  0.5549107789993286
train gradient:  0.13013951001187268
iteration : 2515
train acc:  0.75
train loss:  0.46970072388648987
train gradient:  0.14110869007297933
iteration : 2516
train acc:  0.6796875
train loss:  0.5452063083648682
train gradient:  0.16662850683241942
iteration : 2517
train acc:  0.6640625
train loss:  0.6360021233558655
train gradient:  0.1830668150022787
iteration : 2518
train acc:  0.71875
train loss:  0.5371131896972656
train gradient:  0.2029317582918974
iteration : 2519
train acc:  0.734375
train loss:  0.5329078435897827
train gradient:  0.1682677021614526
iteration : 2520
train acc:  0.71875
train loss:  0.5849802494049072
train gradient:  0.1633903209710098
iteration : 2521
train acc:  0.7421875
train loss:  0.48224109411239624
train gradient:  0.13599128773059974
iteration : 2522
train acc:  0.6875
train loss:  0.5630521178245544
train gradient:  0.16054386819389738
iteration : 2523
train acc:  0.78125
train loss:  0.46230924129486084
train gradient:  0.14099708503777786
iteration : 2524
train acc:  0.703125
train loss:  0.5619637966156006
train gradient:  0.1620593824422227
iteration : 2525
train acc:  0.6796875
train loss:  0.5909369587898254
train gradient:  0.16111631012836186
iteration : 2526
train acc:  0.671875
train loss:  0.5668686628341675
train gradient:  0.1987427730853702
iteration : 2527
train acc:  0.734375
train loss:  0.541801929473877
train gradient:  0.18692883210479486
iteration : 2528
train acc:  0.6875
train loss:  0.5855534076690674
train gradient:  0.181457974400565
iteration : 2529
train acc:  0.6640625
train loss:  0.5729299783706665
train gradient:  0.18722584299640688
iteration : 2530
train acc:  0.7421875
train loss:  0.5003254413604736
train gradient:  0.1317730171003521
iteration : 2531
train acc:  0.7265625
train loss:  0.507457971572876
train gradient:  0.14795218316312606
iteration : 2532
train acc:  0.6953125
train loss:  0.5654075145721436
train gradient:  0.21376248655058405
iteration : 2533
train acc:  0.71875
train loss:  0.5374066829681396
train gradient:  0.15592373982342217
iteration : 2534
train acc:  0.7578125
train loss:  0.5391353368759155
train gradient:  0.1550574035317085
iteration : 2535
train acc:  0.7265625
train loss:  0.5274568796157837
train gradient:  0.12889860986826
iteration : 2536
train acc:  0.7421875
train loss:  0.523411750793457
train gradient:  0.14128146997274937
iteration : 2537
train acc:  0.6875
train loss:  0.5641840696334839
train gradient:  0.19629847118247717
iteration : 2538
train acc:  0.75
train loss:  0.5100042819976807
train gradient:  0.12523679185231693
iteration : 2539
train acc:  0.734375
train loss:  0.5341389179229736
train gradient:  0.12786806892866232
iteration : 2540
train acc:  0.734375
train loss:  0.5752532482147217
train gradient:  0.16650715235846392
iteration : 2541
train acc:  0.71875
train loss:  0.5310447812080383
train gradient:  0.15327340029345518
iteration : 2542
train acc:  0.734375
train loss:  0.5319147109985352
train gradient:  0.175958571685967
iteration : 2543
train acc:  0.703125
train loss:  0.5818093419075012
train gradient:  0.1595691526272468
iteration : 2544
train acc:  0.7265625
train loss:  0.5264947414398193
train gradient:  0.15700738870044548
iteration : 2545
train acc:  0.734375
train loss:  0.5038696527481079
train gradient:  0.14146174421433638
iteration : 2546
train acc:  0.6875
train loss:  0.5145903825759888
train gradient:  0.1499989300173027
iteration : 2547
train acc:  0.6171875
train loss:  0.6508097648620605
train gradient:  0.20482195728529828
iteration : 2548
train acc:  0.6484375
train loss:  0.594202995300293
train gradient:  0.20568252320132172
iteration : 2549
train acc:  0.734375
train loss:  0.5044533014297485
train gradient:  0.14357147272229054
iteration : 2550
train acc:  0.65625
train loss:  0.569384753704071
train gradient:  0.17616355661072358
iteration : 2551
train acc:  0.7265625
train loss:  0.4937397241592407
train gradient:  0.15686895017459285
iteration : 2552
train acc:  0.734375
train loss:  0.5246576070785522
train gradient:  0.1902488416957167
iteration : 2553
train acc:  0.65625
train loss:  0.6499196290969849
train gradient:  0.21508524829215192
iteration : 2554
train acc:  0.671875
train loss:  0.5597895383834839
train gradient:  0.13488834473770028
iteration : 2555
train acc:  0.7265625
train loss:  0.5288709402084351
train gradient:  0.16397670077876358
iteration : 2556
train acc:  0.7421875
train loss:  0.5185530185699463
train gradient:  0.17007912017112042
iteration : 2557
train acc:  0.6875
train loss:  0.5231478810310364
train gradient:  0.1649126850024486
iteration : 2558
train acc:  0.75
train loss:  0.5371019840240479
train gradient:  0.16956509731111064
iteration : 2559
train acc:  0.671875
train loss:  0.5945919156074524
train gradient:  0.15354510305804114
iteration : 2560
train acc:  0.7109375
train loss:  0.5620694160461426
train gradient:  0.15299801683637437
iteration : 2561
train acc:  0.734375
train loss:  0.555838406085968
train gradient:  0.17945086574584213
iteration : 2562
train acc:  0.765625
train loss:  0.4844730794429779
train gradient:  0.15784071103439956
iteration : 2563
train acc:  0.59375
train loss:  0.6185227632522583
train gradient:  0.18023802801307992
iteration : 2564
train acc:  0.71875
train loss:  0.5386210083961487
train gradient:  0.2598219089092381
iteration : 2565
train acc:  0.703125
train loss:  0.558266282081604
train gradient:  0.21602157059826
iteration : 2566
train acc:  0.71875
train loss:  0.5293076634407043
train gradient:  0.1192666078692344
iteration : 2567
train acc:  0.7578125
train loss:  0.48944464325904846
train gradient:  0.14320899525662817
iteration : 2568
train acc:  0.703125
train loss:  0.574750542640686
train gradient:  0.14715961564327107
iteration : 2569
train acc:  0.703125
train loss:  0.5201630592346191
train gradient:  0.14201674109287832
iteration : 2570
train acc:  0.75
train loss:  0.5423921346664429
train gradient:  0.17736884049092072
iteration : 2571
train acc:  0.75
train loss:  0.5437984466552734
train gradient:  0.14727895769978727
iteration : 2572
train acc:  0.7265625
train loss:  0.48652708530426025
train gradient:  0.14470423653468428
iteration : 2573
train acc:  0.671875
train loss:  0.5780510902404785
train gradient:  0.16294571091321944
iteration : 2574
train acc:  0.703125
train loss:  0.5177306532859802
train gradient:  0.19051786118405317
iteration : 2575
train acc:  0.6875
train loss:  0.5287958979606628
train gradient:  0.17986718146477068
iteration : 2576
train acc:  0.703125
train loss:  0.5737776756286621
train gradient:  0.17727266034500333
iteration : 2577
train acc:  0.7265625
train loss:  0.5342695116996765
train gradient:  0.19747474272081691
iteration : 2578
train acc:  0.6640625
train loss:  0.5632085204124451
train gradient:  0.28299462533079656
iteration : 2579
train acc:  0.7734375
train loss:  0.5119895935058594
train gradient:  0.17087608213283678
iteration : 2580
train acc:  0.734375
train loss:  0.5313376188278198
train gradient:  0.17436645967546238
iteration : 2581
train acc:  0.671875
train loss:  0.5585148930549622
train gradient:  0.16894215395340229
iteration : 2582
train acc:  0.8359375
train loss:  0.435455322265625
train gradient:  0.12757937495141022
iteration : 2583
train acc:  0.6953125
train loss:  0.5314370393753052
train gradient:  0.18894623694750845
iteration : 2584
train acc:  0.6640625
train loss:  0.5831093788146973
train gradient:  0.1543338980723384
iteration : 2585
train acc:  0.71875
train loss:  0.5635470151901245
train gradient:  0.2685048370901921
iteration : 2586
train acc:  0.734375
train loss:  0.5770657062530518
train gradient:  0.19384902071624596
iteration : 2587
train acc:  0.65625
train loss:  0.5873342156410217
train gradient:  0.16798604765365327
iteration : 2588
train acc:  0.7265625
train loss:  0.5178269743919373
train gradient:  0.15206224881546754
iteration : 2589
train acc:  0.6484375
train loss:  0.6121360063552856
train gradient:  0.22781224893394933
iteration : 2590
train acc:  0.7578125
train loss:  0.4852569103240967
train gradient:  0.1546977273178869
iteration : 2591
train acc:  0.765625
train loss:  0.47516199946403503
train gradient:  0.14226149611181388
iteration : 2592
train acc:  0.65625
train loss:  0.5709940195083618
train gradient:  0.29519668578908353
iteration : 2593
train acc:  0.7109375
train loss:  0.5194571018218994
train gradient:  0.14266157768745374
iteration : 2594
train acc:  0.7265625
train loss:  0.563567578792572
train gradient:  0.1709707128736465
iteration : 2595
train acc:  0.71875
train loss:  0.5313276648521423
train gradient:  0.15023770791895352
iteration : 2596
train acc:  0.6796875
train loss:  0.5752027034759521
train gradient:  0.15631183336827165
iteration : 2597
train acc:  0.703125
train loss:  0.5344746708869934
train gradient:  0.1465818933409977
iteration : 2598
train acc:  0.6875
train loss:  0.5730782747268677
train gradient:  0.2088022072653224
iteration : 2599
train acc:  0.71875
train loss:  0.5342977046966553
train gradient:  0.15204255708981107
iteration : 2600
train acc:  0.7421875
train loss:  0.5221593379974365
train gradient:  0.15781980721125619
iteration : 2601
train acc:  0.7578125
train loss:  0.5333136320114136
train gradient:  0.2356735875680338
iteration : 2602
train acc:  0.7734375
train loss:  0.4768527150154114
train gradient:  0.13614082272911932
iteration : 2603
train acc:  0.7265625
train loss:  0.5382527112960815
train gradient:  0.1673080925633358
iteration : 2604
train acc:  0.6953125
train loss:  0.5369845032691956
train gradient:  0.13561649558977273
iteration : 2605
train acc:  0.671875
train loss:  0.5972587466239929
train gradient:  0.23521410441448015
iteration : 2606
train acc:  0.6796875
train loss:  0.5279648900032043
train gradient:  0.15797550141202554
iteration : 2607
train acc:  0.71875
train loss:  0.5603476762771606
train gradient:  0.19062433119765848
iteration : 2608
train acc:  0.765625
train loss:  0.5116795301437378
train gradient:  0.15651826864855836
iteration : 2609
train acc:  0.671875
train loss:  0.5714863538742065
train gradient:  0.17800963848235024
iteration : 2610
train acc:  0.75
train loss:  0.5020866990089417
train gradient:  0.16492470136840004
iteration : 2611
train acc:  0.7734375
train loss:  0.46666038036346436
train gradient:  0.1267350055969817
iteration : 2612
train acc:  0.6484375
train loss:  0.6796461343765259
train gradient:  0.2625726479065738
iteration : 2613
train acc:  0.734375
train loss:  0.5278587341308594
train gradient:  0.15547576182853978
iteration : 2614
train acc:  0.6875
train loss:  0.5948690176010132
train gradient:  0.17094221134727094
iteration : 2615
train acc:  0.7109375
train loss:  0.5315413475036621
train gradient:  0.1749554716293994
iteration : 2616
train acc:  0.734375
train loss:  0.5136976838111877
train gradient:  0.15687347173327126
iteration : 2617
train acc:  0.6796875
train loss:  0.5833219885826111
train gradient:  0.20000461646387913
iteration : 2618
train acc:  0.7421875
train loss:  0.4826129972934723
train gradient:  0.12497431469817363
iteration : 2619
train acc:  0.6875
train loss:  0.5717970132827759
train gradient:  0.1716865423920521
iteration : 2620
train acc:  0.75
train loss:  0.4765009880065918
train gradient:  0.11213480142468615
iteration : 2621
train acc:  0.734375
train loss:  0.5258408784866333
train gradient:  0.1838510528084965
iteration : 2622
train acc:  0.7265625
train loss:  0.5198924541473389
train gradient:  0.15839872428448004
iteration : 2623
train acc:  0.7578125
train loss:  0.5084162354469299
train gradient:  0.16287918411775126
iteration : 2624
train acc:  0.71875
train loss:  0.5432571172714233
train gradient:  0.15209325895645576
iteration : 2625
train acc:  0.765625
train loss:  0.4762333631515503
train gradient:  0.13783465359771754
iteration : 2626
train acc:  0.7578125
train loss:  0.5110108256340027
train gradient:  0.14829862695017315
iteration : 2627
train acc:  0.6953125
train loss:  0.5652329921722412
train gradient:  0.17323556265890072
iteration : 2628
train acc:  0.75
train loss:  0.4917430877685547
train gradient:  0.13039318859372234
iteration : 2629
train acc:  0.6015625
train loss:  0.6440551280975342
train gradient:  0.19430908262040242
iteration : 2630
train acc:  0.765625
train loss:  0.5138636827468872
train gradient:  0.13567095158845055
iteration : 2631
train acc:  0.6875
train loss:  0.6054813265800476
train gradient:  0.18789129030770765
iteration : 2632
train acc:  0.6875
train loss:  0.5710482001304626
train gradient:  0.2179063706505756
iteration : 2633
train acc:  0.7421875
train loss:  0.5132712125778198
train gradient:  0.14660759226356385
iteration : 2634
train acc:  0.7421875
train loss:  0.5206685066223145
train gradient:  0.18539474829886196
iteration : 2635
train acc:  0.734375
train loss:  0.507496178150177
train gradient:  0.1834037056092173
iteration : 2636
train acc:  0.71875
train loss:  0.5388884544372559
train gradient:  0.15790250999080668
iteration : 2637
train acc:  0.7578125
train loss:  0.5312753319740295
train gradient:  0.2012095259558382
iteration : 2638
train acc:  0.734375
train loss:  0.5299215316772461
train gradient:  0.16067364923087962
iteration : 2639
train acc:  0.703125
train loss:  0.5831359624862671
train gradient:  0.2075961778204179
iteration : 2640
train acc:  0.671875
train loss:  0.5801578760147095
train gradient:  0.20938520709494726
iteration : 2641
train acc:  0.78125
train loss:  0.46574750542640686
train gradient:  0.1384114851683798
iteration : 2642
train acc:  0.71875
train loss:  0.531326413154602
train gradient:  0.13680387368646332
iteration : 2643
train acc:  0.765625
train loss:  0.507044792175293
train gradient:  0.1580045708252505
iteration : 2644
train acc:  0.671875
train loss:  0.5420055389404297
train gradient:  0.22642654449609878
iteration : 2645
train acc:  0.6328125
train loss:  0.6072050333023071
train gradient:  0.22332119231967695
iteration : 2646
train acc:  0.7578125
train loss:  0.5231640338897705
train gradient:  0.1772423298137657
iteration : 2647
train acc:  0.671875
train loss:  0.6115109920501709
train gradient:  0.21362500060826328
iteration : 2648
train acc:  0.671875
train loss:  0.5580183267593384
train gradient:  0.15155809852719201
iteration : 2649
train acc:  0.6875
train loss:  0.5506172180175781
train gradient:  0.14385458212368246
iteration : 2650
train acc:  0.625
train loss:  0.6468528509140015
train gradient:  0.18696187212563212
iteration : 2651
train acc:  0.7578125
train loss:  0.5101834535598755
train gradient:  0.1204236522169018
iteration : 2652
train acc:  0.8359375
train loss:  0.44924214482307434
train gradient:  0.1431162460974078
iteration : 2653
train acc:  0.6796875
train loss:  0.6081387996673584
train gradient:  0.21544576669531484
iteration : 2654
train acc:  0.7578125
train loss:  0.4820779263973236
train gradient:  0.14293933774490514
iteration : 2655
train acc:  0.6875
train loss:  0.5303584933280945
train gradient:  0.17428557456952742
iteration : 2656
train acc:  0.6875
train loss:  0.5704336166381836
train gradient:  0.21227065325622207
iteration : 2657
train acc:  0.640625
train loss:  0.5843762159347534
train gradient:  0.15302260211465113
iteration : 2658
train acc:  0.703125
train loss:  0.5514304637908936
train gradient:  0.23740836011284833
iteration : 2659
train acc:  0.75
train loss:  0.5183017253875732
train gradient:  0.164972814531665
iteration : 2660
train acc:  0.703125
train loss:  0.5212863683700562
train gradient:  0.12154322560185744
iteration : 2661
train acc:  0.6875
train loss:  0.5904744267463684
train gradient:  0.21260763079911865
iteration : 2662
train acc:  0.71875
train loss:  0.49939343333244324
train gradient:  0.17656796526836915
iteration : 2663
train acc:  0.703125
train loss:  0.5536925792694092
train gradient:  0.193215265103531
iteration : 2664
train acc:  0.6171875
train loss:  0.5974165201187134
train gradient:  0.21642497332592664
iteration : 2665
train acc:  0.6796875
train loss:  0.6040376424789429
train gradient:  0.24197012401099544
iteration : 2666
train acc:  0.796875
train loss:  0.42819246649742126
train gradient:  0.12564218341877703
iteration : 2667
train acc:  0.703125
train loss:  0.5658971667289734
train gradient:  0.21331272243924884
iteration : 2668
train acc:  0.6875
train loss:  0.5386257767677307
train gradient:  0.1691690898840321
iteration : 2669
train acc:  0.7578125
train loss:  0.5281500816345215
train gradient:  0.16479452522235805
iteration : 2670
train acc:  0.7578125
train loss:  0.4973302185535431
train gradient:  0.14676549298405617
iteration : 2671
train acc:  0.71875
train loss:  0.5717708468437195
train gradient:  0.22980486618986573
iteration : 2672
train acc:  0.703125
train loss:  0.5952543020248413
train gradient:  0.1888417858990668
iteration : 2673
train acc:  0.6953125
train loss:  0.5620423555374146
train gradient:  0.15981058111568613
iteration : 2674
train acc:  0.7890625
train loss:  0.46258458495140076
train gradient:  0.10518573369854008
iteration : 2675
train acc:  0.7421875
train loss:  0.5120007395744324
train gradient:  0.17028385904587273
iteration : 2676
train acc:  0.6953125
train loss:  0.5598508715629578
train gradient:  0.1837479383096754
iteration : 2677
train acc:  0.7734375
train loss:  0.48482245206832886
train gradient:  0.15338893861369818
iteration : 2678
train acc:  0.7421875
train loss:  0.5180560350418091
train gradient:  0.1794710432379335
iteration : 2679
train acc:  0.71875
train loss:  0.5244295001029968
train gradient:  0.1618030051347344
iteration : 2680
train acc:  0.7421875
train loss:  0.5321584343910217
train gradient:  0.15984585972747772
iteration : 2681
train acc:  0.7109375
train loss:  0.5147588849067688
train gradient:  0.17996713997172398
iteration : 2682
train acc:  0.734375
train loss:  0.48650890588760376
train gradient:  0.14368168571445822
iteration : 2683
train acc:  0.7265625
train loss:  0.5008764863014221
train gradient:  0.15291535358378255
iteration : 2684
train acc:  0.7890625
train loss:  0.4700281023979187
train gradient:  0.1398661992805041
iteration : 2685
train acc:  0.7421875
train loss:  0.5062539577484131
train gradient:  0.17584976589095663
iteration : 2686
train acc:  0.71875
train loss:  0.5674500465393066
train gradient:  0.16722602842466422
iteration : 2687
train acc:  0.671875
train loss:  0.5285221338272095
train gradient:  0.20398060180611888
iteration : 2688
train acc:  0.6953125
train loss:  0.5718761682510376
train gradient:  0.18110368018155854
iteration : 2689
train acc:  0.6015625
train loss:  0.6187511086463928
train gradient:  0.251305782583333
iteration : 2690
train acc:  0.6953125
train loss:  0.5561317205429077
train gradient:  0.17168850060326696
iteration : 2691
train acc:  0.7109375
train loss:  0.5281299948692322
train gradient:  0.1888964149897303
iteration : 2692
train acc:  0.703125
train loss:  0.5389906167984009
train gradient:  0.19332649750883213
iteration : 2693
train acc:  0.7890625
train loss:  0.4365094304084778
train gradient:  0.1515220942496348
iteration : 2694
train acc:  0.7109375
train loss:  0.5883169174194336
train gradient:  0.1863730186718081
iteration : 2695
train acc:  0.6953125
train loss:  0.5571457743644714
train gradient:  0.17256062207523035
iteration : 2696
train acc:  0.6953125
train loss:  0.5536935925483704
train gradient:  0.1521009108383799
iteration : 2697
train acc:  0.734375
train loss:  0.5029093623161316
train gradient:  0.22195931426669768
iteration : 2698
train acc:  0.7734375
train loss:  0.4827471077442169
train gradient:  0.1510932876337978
iteration : 2699
train acc:  0.6640625
train loss:  0.5728599429130554
train gradient:  0.24665050678527228
iteration : 2700
train acc:  0.6953125
train loss:  0.530017614364624
train gradient:  0.1759012391962349
iteration : 2701
train acc:  0.703125
train loss:  0.5662609934806824
train gradient:  0.19793889778645735
iteration : 2702
train acc:  0.734375
train loss:  0.509506344795227
train gradient:  0.13619615326150994
iteration : 2703
train acc:  0.6953125
train loss:  0.5828139781951904
train gradient:  0.19086410380580976
iteration : 2704
train acc:  0.7109375
train loss:  0.5290491580963135
train gradient:  0.1351395926493853
iteration : 2705
train acc:  0.765625
train loss:  0.49923500418663025
train gradient:  0.18267476638512314
iteration : 2706
train acc:  0.65625
train loss:  0.5527855157852173
train gradient:  0.16637761107591353
iteration : 2707
train acc:  0.78125
train loss:  0.5125481486320496
train gradient:  0.1075301232552001
iteration : 2708
train acc:  0.6640625
train loss:  0.5202654600143433
train gradient:  0.1538353497998577
iteration : 2709
train acc:  0.6875
train loss:  0.5526950359344482
train gradient:  0.15704035532421068
iteration : 2710
train acc:  0.7265625
train loss:  0.567020058631897
train gradient:  0.17898660603068314
iteration : 2711
train acc:  0.6875
train loss:  0.5840853452682495
train gradient:  0.15107088069417088
iteration : 2712
train acc:  0.7421875
train loss:  0.5467624068260193
train gradient:  0.2044916594622363
iteration : 2713
train acc:  0.671875
train loss:  0.5840462446212769
train gradient:  0.21838205394968568
iteration : 2714
train acc:  0.7578125
train loss:  0.5039560198783875
train gradient:  0.1292074602697096
iteration : 2715
train acc:  0.671875
train loss:  0.6272302865982056
train gradient:  0.24475765322555626
iteration : 2716
train acc:  0.6875
train loss:  0.5869580507278442
train gradient:  0.1831128557715678
iteration : 2717
train acc:  0.6328125
train loss:  0.6481714844703674
train gradient:  0.1864840223577678
iteration : 2718
train acc:  0.7109375
train loss:  0.573939323425293
train gradient:  0.18527987987580197
iteration : 2719
train acc:  0.6328125
train loss:  0.6247183084487915
train gradient:  0.2111330162175462
iteration : 2720
train acc:  0.78125
train loss:  0.4592829942703247
train gradient:  0.10202602065624343
iteration : 2721
train acc:  0.7265625
train loss:  0.5143828988075256
train gradient:  0.1504875553755508
iteration : 2722
train acc:  0.7421875
train loss:  0.535896897315979
train gradient:  0.18248829424616647
iteration : 2723
train acc:  0.7890625
train loss:  0.45231810212135315
train gradient:  0.11367182886091089
iteration : 2724
train acc:  0.8046875
train loss:  0.46101605892181396
train gradient:  0.1489699900028432
iteration : 2725
train acc:  0.6484375
train loss:  0.6250284314155579
train gradient:  0.24615573202906715
iteration : 2726
train acc:  0.734375
train loss:  0.5094918012619019
train gradient:  0.1260994956268705
iteration : 2727
train acc:  0.734375
train loss:  0.5163372755050659
train gradient:  0.19747964481064909
iteration : 2728
train acc:  0.6640625
train loss:  0.6001809239387512
train gradient:  0.23779794267306503
iteration : 2729
train acc:  0.71875
train loss:  0.5298470258712769
train gradient:  0.1798480430826936
iteration : 2730
train acc:  0.7421875
train loss:  0.5029605627059937
train gradient:  0.199991635163771
iteration : 2731
train acc:  0.6640625
train loss:  0.5798444747924805
train gradient:  0.20137349265852814
iteration : 2732
train acc:  0.6875
train loss:  0.5895990133285522
train gradient:  0.228895179702307
iteration : 2733
train acc:  0.6875
train loss:  0.5279431343078613
train gradient:  0.15920952564432622
iteration : 2734
train acc:  0.7109375
train loss:  0.5393952131271362
train gradient:  0.12914044805837155
iteration : 2735
train acc:  0.6796875
train loss:  0.55875164270401
train gradient:  0.1527279398234176
iteration : 2736
train acc:  0.6953125
train loss:  0.6020757555961609
train gradient:  0.17516771682588045
iteration : 2737
train acc:  0.734375
train loss:  0.5319890975952148
train gradient:  0.14896178111035513
iteration : 2738
train acc:  0.6875
train loss:  0.5719276666641235
train gradient:  0.18500144572933397
iteration : 2739
train acc:  0.765625
train loss:  0.4684255123138428
train gradient:  0.13262627730880877
iteration : 2740
train acc:  0.7421875
train loss:  0.4813114106655121
train gradient:  0.13221152230034267
iteration : 2741
train acc:  0.75
train loss:  0.5178855061531067
train gradient:  0.138079582648096
iteration : 2742
train acc:  0.7421875
train loss:  0.5039807558059692
train gradient:  0.1313344811767958
iteration : 2743
train acc:  0.7578125
train loss:  0.5024615526199341
train gradient:  0.11667768425600149
iteration : 2744
train acc:  0.703125
train loss:  0.5537919998168945
train gradient:  0.1613288491660605
iteration : 2745
train acc:  0.7265625
train loss:  0.5030484199523926
train gradient:  0.12590024719934037
iteration : 2746
train acc:  0.7265625
train loss:  0.5199110507965088
train gradient:  0.14548746401917573
iteration : 2747
train acc:  0.6953125
train loss:  0.5713440179824829
train gradient:  0.17261053900734785
iteration : 2748
train acc:  0.734375
train loss:  0.5146383047103882
train gradient:  0.17173622148911394
iteration : 2749
train acc:  0.75
train loss:  0.4770858883857727
train gradient:  0.13982997360175653
iteration : 2750
train acc:  0.734375
train loss:  0.4937456250190735
train gradient:  0.13081852934761654
iteration : 2751
train acc:  0.703125
train loss:  0.5696781873703003
train gradient:  0.2112323215484884
iteration : 2752
train acc:  0.7578125
train loss:  0.47057804465293884
train gradient:  0.10929603806534745
iteration : 2753
train acc:  0.6875
train loss:  0.5596742630004883
train gradient:  0.2335678228039917
iteration : 2754
train acc:  0.703125
train loss:  0.5365124940872192
train gradient:  0.20131677106099613
iteration : 2755
train acc:  0.703125
train loss:  0.569219708442688
train gradient:  0.1383553354699291
iteration : 2756
train acc:  0.7265625
train loss:  0.529784619808197
train gradient:  0.15997337994784971
iteration : 2757
train acc:  0.6953125
train loss:  0.5526224374771118
train gradient:  0.1599400626459065
iteration : 2758
train acc:  0.7109375
train loss:  0.5503862500190735
train gradient:  0.16056297565037875
iteration : 2759
train acc:  0.703125
train loss:  0.5518143177032471
train gradient:  0.18226999632179913
iteration : 2760
train acc:  0.71875
train loss:  0.570889949798584
train gradient:  0.15781548882316512
iteration : 2761
train acc:  0.6953125
train loss:  0.5503020286560059
train gradient:  0.183057124504284
iteration : 2762
train acc:  0.765625
train loss:  0.5093425512313843
train gradient:  0.23863155566266508
iteration : 2763
train acc:  0.6015625
train loss:  0.6613624691963196
train gradient:  0.23050069839461368
iteration : 2764
train acc:  0.7265625
train loss:  0.5215341448783875
train gradient:  0.169802938229537
iteration : 2765
train acc:  0.671875
train loss:  0.5658689141273499
train gradient:  0.200732933164299
iteration : 2766
train acc:  0.71875
train loss:  0.5533353686332703
train gradient:  0.15750887196779334
iteration : 2767
train acc:  0.8046875
train loss:  0.45789095759391785
train gradient:  0.17100161465026864
iteration : 2768
train acc:  0.703125
train loss:  0.5142990946769714
train gradient:  0.16429768551561352
iteration : 2769
train acc:  0.734375
train loss:  0.5330384969711304
train gradient:  0.15497386251951634
iteration : 2770
train acc:  0.7109375
train loss:  0.562527060508728
train gradient:  0.18385251056938146
iteration : 2771
train acc:  0.7421875
train loss:  0.5489957928657532
train gradient:  0.15821603491566216
iteration : 2772
train acc:  0.7734375
train loss:  0.5106193423271179
train gradient:  0.16256047527027984
iteration : 2773
train acc:  0.71875
train loss:  0.5575622320175171
train gradient:  0.19185980473968367
iteration : 2774
train acc:  0.6953125
train loss:  0.5547357797622681
train gradient:  0.1700930529021773
iteration : 2775
train acc:  0.6875
train loss:  0.5443268418312073
train gradient:  0.15083126644093314
iteration : 2776
train acc:  0.8203125
train loss:  0.4623297154903412
train gradient:  0.19294182068012644
iteration : 2777
train acc:  0.7578125
train loss:  0.4734695553779602
train gradient:  0.15184175389299587
iteration : 2778
train acc:  0.7421875
train loss:  0.49939805269241333
train gradient:  0.13353989522855542
iteration : 2779
train acc:  0.71875
train loss:  0.5479768514633179
train gradient:  0.17457981040611323
iteration : 2780
train acc:  0.6953125
train loss:  0.5253261923789978
train gradient:  0.2718888317011896
iteration : 2781
train acc:  0.734375
train loss:  0.5534282922744751
train gradient:  0.18303753627320934
iteration : 2782
train acc:  0.65625
train loss:  0.6068277359008789
train gradient:  0.16475562739859834
iteration : 2783
train acc:  0.7265625
train loss:  0.5132150053977966
train gradient:  0.16291073703520403
iteration : 2784
train acc:  0.6328125
train loss:  0.6025619506835938
train gradient:  0.24068340674575023
iteration : 2785
train acc:  0.65625
train loss:  0.6229429244995117
train gradient:  0.24190742370774593
iteration : 2786
train acc:  0.75
train loss:  0.5274535417556763
train gradient:  0.16294062007167
iteration : 2787
train acc:  0.7265625
train loss:  0.5548439621925354
train gradient:  0.16616280120344348
iteration : 2788
train acc:  0.7578125
train loss:  0.5081573128700256
train gradient:  0.1368287330124811
iteration : 2789
train acc:  0.6640625
train loss:  0.5687837600708008
train gradient:  0.17494891830092968
iteration : 2790
train acc:  0.6875
train loss:  0.5788849592208862
train gradient:  0.239019788637731
iteration : 2791
train acc:  0.7265625
train loss:  0.5363357067108154
train gradient:  0.16669393803692842
iteration : 2792
train acc:  0.7109375
train loss:  0.5487586259841919
train gradient:  0.2251414528333731
iteration : 2793
train acc:  0.640625
train loss:  0.6193218231201172
train gradient:  0.25722222883807333
iteration : 2794
train acc:  0.71875
train loss:  0.546027660369873
train gradient:  0.173042960060202
iteration : 2795
train acc:  0.7109375
train loss:  0.5687536001205444
train gradient:  0.1538684467177186
iteration : 2796
train acc:  0.75
train loss:  0.4825759530067444
train gradient:  0.11977246273467165
iteration : 2797
train acc:  0.7109375
train loss:  0.5830914378166199
train gradient:  0.16543780316279813
iteration : 2798
train acc:  0.7421875
train loss:  0.4885890483856201
train gradient:  0.11934203712518238
iteration : 2799
train acc:  0.7734375
train loss:  0.5065491199493408
train gradient:  0.1299253930678827
iteration : 2800
train acc:  0.7109375
train loss:  0.49897536635398865
train gradient:  0.14093842528623032
iteration : 2801
train acc:  0.703125
train loss:  0.5486767292022705
train gradient:  0.1690367728003856
iteration : 2802
train acc:  0.65625
train loss:  0.5834108591079712
train gradient:  0.20588358185507516
iteration : 2803
train acc:  0.6953125
train loss:  0.5081204175949097
train gradient:  0.1788322164315566
iteration : 2804
train acc:  0.7109375
train loss:  0.5354604125022888
train gradient:  0.1339954945505064
iteration : 2805
train acc:  0.6796875
train loss:  0.5963442921638489
train gradient:  0.19441072120089972
iteration : 2806
train acc:  0.7265625
train loss:  0.49066096544265747
train gradient:  0.1279719125757217
iteration : 2807
train acc:  0.71875
train loss:  0.4893045425415039
train gradient:  0.12336210037876803
iteration : 2808
train acc:  0.71875
train loss:  0.5375529527664185
train gradient:  0.1530754578199568
iteration : 2809
train acc:  0.7265625
train loss:  0.546256422996521
train gradient:  0.1884152077604187
iteration : 2810
train acc:  0.71875
train loss:  0.5535085797309875
train gradient:  0.19086097920753003
iteration : 2811
train acc:  0.703125
train loss:  0.5846594572067261
train gradient:  0.20475930069613504
iteration : 2812
train acc:  0.7265625
train loss:  0.5308461785316467
train gradient:  0.16184920305409595
iteration : 2813
train acc:  0.7109375
train loss:  0.5401166081428528
train gradient:  0.1585192297575791
iteration : 2814
train acc:  0.6640625
train loss:  0.6544449329376221
train gradient:  0.2594709283013927
iteration : 2815
train acc:  0.7421875
train loss:  0.4818577170372009
train gradient:  0.14991316722789044
iteration : 2816
train acc:  0.65625
train loss:  0.6232882738113403
train gradient:  0.21521023462237432
iteration : 2817
train acc:  0.765625
train loss:  0.5111124515533447
train gradient:  0.22971189094208666
iteration : 2818
train acc:  0.703125
train loss:  0.5662059783935547
train gradient:  0.20210913178708723
iteration : 2819
train acc:  0.7421875
train loss:  0.5135188102722168
train gradient:  0.17897339486362956
iteration : 2820
train acc:  0.734375
train loss:  0.5144107341766357
train gradient:  0.1511940350800114
iteration : 2821
train acc:  0.75
train loss:  0.5237434506416321
train gradient:  0.1670577642975652
iteration : 2822
train acc:  0.796875
train loss:  0.48902568221092224
train gradient:  0.20540135794325826
iteration : 2823
train acc:  0.6796875
train loss:  0.5761401653289795
train gradient:  0.17336538600975
iteration : 2824
train acc:  0.734375
train loss:  0.5511765480041504
train gradient:  0.14714105876974556
iteration : 2825
train acc:  0.8203125
train loss:  0.49339163303375244
train gradient:  0.15757454961682604
iteration : 2826
train acc:  0.7109375
train loss:  0.5217728614807129
train gradient:  0.16179908674011323
iteration : 2827
train acc:  0.7109375
train loss:  0.4897252917289734
train gradient:  0.1265272433269301
iteration : 2828
train acc:  0.703125
train loss:  0.5398377776145935
train gradient:  0.1748305109893083
iteration : 2829
train acc:  0.7578125
train loss:  0.5040960907936096
train gradient:  0.13770862989009114
iteration : 2830
train acc:  0.734375
train loss:  0.5178591012954712
train gradient:  0.23124683185522235
iteration : 2831
train acc:  0.7578125
train loss:  0.4777822494506836
train gradient:  0.12052486061258282
iteration : 2832
train acc:  0.7109375
train loss:  0.5548995733261108
train gradient:  0.15630771305325042
iteration : 2833
train acc:  0.7421875
train loss:  0.4798967242240906
train gradient:  0.13882248963345314
iteration : 2834
train acc:  0.6640625
train loss:  0.5990787744522095
train gradient:  0.2574774713079965
iteration : 2835
train acc:  0.703125
train loss:  0.5742064118385315
train gradient:  0.14220393404483805
iteration : 2836
train acc:  0.765625
train loss:  0.47057682275772095
train gradient:  0.12241529366489508
iteration : 2837
train acc:  0.640625
train loss:  0.6118772029876709
train gradient:  0.2152994547857752
iteration : 2838
train acc:  0.78125
train loss:  0.471903920173645
train gradient:  0.12987740168052767
iteration : 2839
train acc:  0.734375
train loss:  0.5442557334899902
train gradient:  0.2878258188130185
iteration : 2840
train acc:  0.734375
train loss:  0.531126081943512
train gradient:  0.15610852257130647
iteration : 2841
train acc:  0.6484375
train loss:  0.6237242817878723
train gradient:  0.21948088789821305
iteration : 2842
train acc:  0.671875
train loss:  0.5426430106163025
train gradient:  0.15919347988537652
iteration : 2843
train acc:  0.75
train loss:  0.560374915599823
train gradient:  0.21290099076516011
iteration : 2844
train acc:  0.734375
train loss:  0.5474468469619751
train gradient:  0.1687606265055604
iteration : 2845
train acc:  0.671875
train loss:  0.6255173087120056
train gradient:  0.19034858541660915
iteration : 2846
train acc:  0.71875
train loss:  0.5238045454025269
train gradient:  0.15246939158736067
iteration : 2847
train acc:  0.7265625
train loss:  0.5411055088043213
train gradient:  0.1609842644503657
iteration : 2848
train acc:  0.7265625
train loss:  0.5034317374229431
train gradient:  0.15095572135905214
iteration : 2849
train acc:  0.78125
train loss:  0.4640132188796997
train gradient:  0.14579457263613532
iteration : 2850
train acc:  0.71875
train loss:  0.5302294492721558
train gradient:  0.20865740053226076
iteration : 2851
train acc:  0.78125
train loss:  0.4969121813774109
train gradient:  0.14431214840663242
iteration : 2852
train acc:  0.7109375
train loss:  0.5165704488754272
train gradient:  0.15144099886067433
iteration : 2853
train acc:  0.7421875
train loss:  0.5447137355804443
train gradient:  0.14773886516942725
iteration : 2854
train acc:  0.7421875
train loss:  0.5027843713760376
train gradient:  0.14945491424835752
iteration : 2855
train acc:  0.671875
train loss:  0.589849591255188
train gradient:  0.19092884524429982
iteration : 2856
train acc:  0.71875
train loss:  0.577583372592926
train gradient:  0.25120723828897806
iteration : 2857
train acc:  0.671875
train loss:  0.5719473361968994
train gradient:  0.217000428552924
iteration : 2858
train acc:  0.796875
train loss:  0.4836079180240631
train gradient:  0.13408530253024398
iteration : 2859
train acc:  0.7109375
train loss:  0.5284160375595093
train gradient:  0.15412839001819537
iteration : 2860
train acc:  0.7421875
train loss:  0.5131266117095947
train gradient:  0.17215573803400347
iteration : 2861
train acc:  0.6796875
train loss:  0.5369285345077515
train gradient:  0.13397359038511122
iteration : 2862
train acc:  0.78125
train loss:  0.48522448539733887
train gradient:  0.12729551867601774
iteration : 2863
train acc:  0.765625
train loss:  0.4877322018146515
train gradient:  0.12885630407436943
iteration : 2864
train acc:  0.71875
train loss:  0.5294348001480103
train gradient:  0.1486541537543826
iteration : 2865
train acc:  0.71875
train loss:  0.5496103763580322
train gradient:  0.1458329102424193
iteration : 2866
train acc:  0.734375
train loss:  0.5340928435325623
train gradient:  0.13660383482990665
iteration : 2867
train acc:  0.703125
train loss:  0.5346195101737976
train gradient:  0.16428796421405298
iteration : 2868
train acc:  0.6640625
train loss:  0.587017297744751
train gradient:  0.16537028019673103
iteration : 2869
train acc:  0.7265625
train loss:  0.5290427803993225
train gradient:  0.1710820544282435
iteration : 2870
train acc:  0.671875
train loss:  0.6040791869163513
train gradient:  0.19306283909890187
iteration : 2871
train acc:  0.734375
train loss:  0.5269773006439209
train gradient:  0.21060015916319802
iteration : 2872
train acc:  0.703125
train loss:  0.5508441925048828
train gradient:  0.1851226550135466
iteration : 2873
train acc:  0.71875
train loss:  0.5164541006088257
train gradient:  0.14614593598333986
iteration : 2874
train acc:  0.703125
train loss:  0.5281321406364441
train gradient:  0.17243066027179335
iteration : 2875
train acc:  0.796875
train loss:  0.4899556040763855
train gradient:  0.11596719759314683
iteration : 2876
train acc:  0.75
train loss:  0.4948716163635254
train gradient:  0.15987268871592902
iteration : 2877
train acc:  0.78125
train loss:  0.43029534816741943
train gradient:  0.12074510324151491
iteration : 2878
train acc:  0.6640625
train loss:  0.5739560127258301
train gradient:  0.16760647601643758
iteration : 2879
train acc:  0.7421875
train loss:  0.5115594267845154
train gradient:  0.20388321420394873
iteration : 2880
train acc:  0.7578125
train loss:  0.48225921392440796
train gradient:  0.12558491162421306
iteration : 2881
train acc:  0.7421875
train loss:  0.5137961506843567
train gradient:  0.1683210580196014
iteration : 2882
train acc:  0.71875
train loss:  0.5238288640975952
train gradient:  0.13497772651390616
iteration : 2883
train acc:  0.6796875
train loss:  0.5975155234336853
train gradient:  0.19318165791141995
iteration : 2884
train acc:  0.640625
train loss:  0.631277322769165
train gradient:  0.16765972886535466
iteration : 2885
train acc:  0.796875
train loss:  0.4743689000606537
train gradient:  0.10590995884354996
iteration : 2886
train acc:  0.6875
train loss:  0.5798749923706055
train gradient:  0.16426744966539183
iteration : 2887
train acc:  0.7890625
train loss:  0.5333168506622314
train gradient:  0.18027243745159954
iteration : 2888
train acc:  0.7421875
train loss:  0.48698878288269043
train gradient:  0.12071619960500758
iteration : 2889
train acc:  0.6796875
train loss:  0.5449755787849426
train gradient:  0.15814737205170207
iteration : 2890
train acc:  0.6953125
train loss:  0.5712260007858276
train gradient:  0.19452155712046076
iteration : 2891
train acc:  0.7734375
train loss:  0.48134297132492065
train gradient:  0.18772762569202628
iteration : 2892
train acc:  0.7421875
train loss:  0.5440028309822083
train gradient:  0.15193847087575835
iteration : 2893
train acc:  0.7109375
train loss:  0.5530217885971069
train gradient:  0.14501653635199965
iteration : 2894
train acc:  0.6953125
train loss:  0.5692768096923828
train gradient:  0.196073737821174
iteration : 2895
train acc:  0.78125
train loss:  0.5239944458007812
train gradient:  0.1673853596586297
iteration : 2896
train acc:  0.71875
train loss:  0.5452228784561157
train gradient:  0.19683678031084917
iteration : 2897
train acc:  0.75
train loss:  0.5010440349578857
train gradient:  0.1464023976501984
iteration : 2898
train acc:  0.71875
train loss:  0.5148731470108032
train gradient:  0.1471340581681288
iteration : 2899
train acc:  0.7265625
train loss:  0.5004934072494507
train gradient:  0.1665727685804121
iteration : 2900
train acc:  0.7109375
train loss:  0.5117834806442261
train gradient:  0.15108924202036866
iteration : 2901
train acc:  0.8125
train loss:  0.42208248376846313
train gradient:  0.13871555015586307
iteration : 2902
train acc:  0.6875
train loss:  0.5306496024131775
train gradient:  0.1740844679981229
iteration : 2903
train acc:  0.765625
train loss:  0.4926910400390625
train gradient:  0.14218905250517358
iteration : 2904
train acc:  0.7421875
train loss:  0.5110183954238892
train gradient:  0.15697330478962082
iteration : 2905
train acc:  0.7109375
train loss:  0.5753137469291687
train gradient:  0.2521973160852509
iteration : 2906
train acc:  0.6640625
train loss:  0.6645165681838989
train gradient:  0.19995199516122783
iteration : 2907
train acc:  0.6875
train loss:  0.5397334098815918
train gradient:  0.1578056824931965
iteration : 2908
train acc:  0.6953125
train loss:  0.5196083188056946
train gradient:  0.15470771226001886
iteration : 2909
train acc:  0.7421875
train loss:  0.504552960395813
train gradient:  0.19332966162039225
iteration : 2910
train acc:  0.7265625
train loss:  0.5142133235931396
train gradient:  0.1698637052759548
iteration : 2911
train acc:  0.75
train loss:  0.4880797863006592
train gradient:  0.12536186089319207
iteration : 2912
train acc:  0.6484375
train loss:  0.5522982478141785
train gradient:  0.21103761705960677
iteration : 2913
train acc:  0.75
train loss:  0.48848360776901245
train gradient:  0.147647117076214
iteration : 2914
train acc:  0.7421875
train loss:  0.5276923179626465
train gradient:  0.15158010787954057
iteration : 2915
train acc:  0.703125
train loss:  0.5720240473747253
train gradient:  0.16732062583110777
iteration : 2916
train acc:  0.6953125
train loss:  0.559351921081543
train gradient:  0.15262023240037714
iteration : 2917
train acc:  0.71875
train loss:  0.5490606427192688
train gradient:  0.17330703389433721
iteration : 2918
train acc:  0.75
train loss:  0.47491535544395447
train gradient:  0.15977789109653656
iteration : 2919
train acc:  0.734375
train loss:  0.5658444166183472
train gradient:  0.1890614378892131
iteration : 2920
train acc:  0.6875
train loss:  0.5583851337432861
train gradient:  0.17998310248199775
iteration : 2921
train acc:  0.7265625
train loss:  0.4821678102016449
train gradient:  0.13434618654349403
iteration : 2922
train acc:  0.75
train loss:  0.5263044834136963
train gradient:  0.14498078038304837
iteration : 2923
train acc:  0.7265625
train loss:  0.5150027871131897
train gradient:  0.13887445840061313
iteration : 2924
train acc:  0.7578125
train loss:  0.5346254110336304
train gradient:  0.1833781176958254
iteration : 2925
train acc:  0.71875
train loss:  0.5317566394805908
train gradient:  0.16030554380392112
iteration : 2926
train acc:  0.7421875
train loss:  0.5052645206451416
train gradient:  0.17108042671881651
iteration : 2927
train acc:  0.6796875
train loss:  0.5810489058494568
train gradient:  0.1848466534083913
iteration : 2928
train acc:  0.734375
train loss:  0.50568026304245
train gradient:  0.14987074677800694
iteration : 2929
train acc:  0.6796875
train loss:  0.528075098991394
train gradient:  0.18159236567832004
iteration : 2930
train acc:  0.7265625
train loss:  0.5335085988044739
train gradient:  0.2215481425878718
iteration : 2931
train acc:  0.703125
train loss:  0.5537578463554382
train gradient:  0.2294070198948843
iteration : 2932
train acc:  0.75
train loss:  0.4588351547718048
train gradient:  0.14078916855211962
iteration : 2933
train acc:  0.7578125
train loss:  0.4759313464164734
train gradient:  0.1260259307675211
iteration : 2934
train acc:  0.71875
train loss:  0.5357338786125183
train gradient:  0.13729469251885074
iteration : 2935
train acc:  0.75
train loss:  0.5099587440490723
train gradient:  0.12392690172209873
iteration : 2936
train acc:  0.7265625
train loss:  0.5209351778030396
train gradient:  0.19880033626077487
iteration : 2937
train acc:  0.625
train loss:  0.6103531718254089
train gradient:  0.17339832906088015
iteration : 2938
train acc:  0.78125
train loss:  0.452584445476532
train gradient:  0.14725733717097172
iteration : 2939
train acc:  0.6640625
train loss:  0.5537711381912231
train gradient:  0.23975062109080347
iteration : 2940
train acc:  0.640625
train loss:  0.6119817495346069
train gradient:  0.2086140057248827
iteration : 2941
train acc:  0.6875
train loss:  0.5510556697845459
train gradient:  0.1505554171926513
iteration : 2942
train acc:  0.6796875
train loss:  0.518851637840271
train gradient:  0.13805841312675007
iteration : 2943
train acc:  0.7734375
train loss:  0.48931893706321716
train gradient:  0.15760572630451464
iteration : 2944
train acc:  0.734375
train loss:  0.5363795161247253
train gradient:  0.15909085238028442
iteration : 2945
train acc:  0.7109375
train loss:  0.5768336057662964
train gradient:  0.15016346470128178
iteration : 2946
train acc:  0.7265625
train loss:  0.5590282082557678
train gradient:  0.18122466419789485
iteration : 2947
train acc:  0.7421875
train loss:  0.49661338329315186
train gradient:  0.15239101970169655
iteration : 2948
train acc:  0.7109375
train loss:  0.5103942155838013
train gradient:  0.13059635918834356
iteration : 2949
train acc:  0.6640625
train loss:  0.5647301077842712
train gradient:  0.16418640685645308
iteration : 2950
train acc:  0.6953125
train loss:  0.5535925030708313
train gradient:  0.15308345215285105
iteration : 2951
train acc:  0.671875
train loss:  0.5561635494232178
train gradient:  0.21531137027039887
iteration : 2952
train acc:  0.734375
train loss:  0.5527522563934326
train gradient:  0.17055385247222912
iteration : 2953
train acc:  0.7421875
train loss:  0.5386298894882202
train gradient:  0.15040891682167815
iteration : 2954
train acc:  0.7734375
train loss:  0.4839552044868469
train gradient:  0.1371719021816999
iteration : 2955
train acc:  0.7265625
train loss:  0.5895477533340454
train gradient:  0.191600285001812
iteration : 2956
train acc:  0.734375
train loss:  0.5051044225692749
train gradient:  0.1697296397553289
iteration : 2957
train acc:  0.7578125
train loss:  0.4936775863170624
train gradient:  0.14573391326380078
iteration : 2958
train acc:  0.6640625
train loss:  0.6088995933532715
train gradient:  0.21728322970319308
iteration : 2959
train acc:  0.6328125
train loss:  0.5980351567268372
train gradient:  0.24857749320634737
iteration : 2960
train acc:  0.765625
train loss:  0.524465799331665
train gradient:  0.17914116624282542
iteration : 2961
train acc:  0.6796875
train loss:  0.6143537759780884
train gradient:  0.2535452237499631
iteration : 2962
train acc:  0.7109375
train loss:  0.5385953187942505
train gradient:  0.1568534755921817
iteration : 2963
train acc:  0.7265625
train loss:  0.47624582052230835
train gradient:  0.15914615135137064
iteration : 2964
train acc:  0.75
train loss:  0.5177039504051208
train gradient:  0.14429735639649094
iteration : 2965
train acc:  0.640625
train loss:  0.5673002004623413
train gradient:  0.15482238309135643
iteration : 2966
train acc:  0.6171875
train loss:  0.5655061602592468
train gradient:  0.16961694501861974
iteration : 2967
train acc:  0.75
train loss:  0.5314000248908997
train gradient:  0.20472606064863283
iteration : 2968
train acc:  0.6796875
train loss:  0.5323819518089294
train gradient:  0.16973003065324432
iteration : 2969
train acc:  0.734375
train loss:  0.523817777633667
train gradient:  0.2197879231402075
iteration : 2970
train acc:  0.7734375
train loss:  0.495189368724823
train gradient:  0.23386794247738213
iteration : 2971
train acc:  0.7578125
train loss:  0.4908769428730011
train gradient:  0.1660449426535197
iteration : 2972
train acc:  0.6953125
train loss:  0.5266634225845337
train gradient:  0.14577874408150648
iteration : 2973
train acc:  0.640625
train loss:  0.632279634475708
train gradient:  0.2239074193339589
iteration : 2974
train acc:  0.6953125
train loss:  0.552638828754425
train gradient:  0.15981678517587225
iteration : 2975
train acc:  0.7421875
train loss:  0.5468294620513916
train gradient:  0.1634350768088622
iteration : 2976
train acc:  0.7421875
train loss:  0.5018278360366821
train gradient:  0.13814302513235016
iteration : 2977
train acc:  0.7421875
train loss:  0.5192539095878601
train gradient:  0.1899315177223208
iteration : 2978
train acc:  0.671875
train loss:  0.6108121871948242
train gradient:  0.241196371978266
iteration : 2979
train acc:  0.7578125
train loss:  0.49296462535858154
train gradient:  0.11945783767945563
iteration : 2980
train acc:  0.6953125
train loss:  0.5424944162368774
train gradient:  0.17622778471231126
iteration : 2981
train acc:  0.7578125
train loss:  0.5062189698219299
train gradient:  0.1731310134862061
iteration : 2982
train acc:  0.65625
train loss:  0.6148935556411743
train gradient:  0.20058898221337806
iteration : 2983
train acc:  0.765625
train loss:  0.47811630368232727
train gradient:  0.12505759315640583
iteration : 2984
train acc:  0.65625
train loss:  0.6018002033233643
train gradient:  0.19933919574753037
iteration : 2985
train acc:  0.71875
train loss:  0.5324850082397461
train gradient:  0.15691355656008182
iteration : 2986
train acc:  0.703125
train loss:  0.5233052968978882
train gradient:  0.14277020174153704
iteration : 2987
train acc:  0.78125
train loss:  0.4522785246372223
train gradient:  0.13863202690292173
iteration : 2988
train acc:  0.71875
train loss:  0.5036798715591431
train gradient:  0.14424331649178507
iteration : 2989
train acc:  0.734375
train loss:  0.5421620607376099
train gradient:  0.16831806191195164
iteration : 2990
train acc:  0.6796875
train loss:  0.5344711542129517
train gradient:  0.17478335983602833
iteration : 2991
train acc:  0.65625
train loss:  0.5677686929702759
train gradient:  0.18128295712759152
iteration : 2992
train acc:  0.75
train loss:  0.518588662147522
train gradient:  0.1516791897962319
iteration : 2993
train acc:  0.7578125
train loss:  0.4811812937259674
train gradient:  0.1506222376535576
iteration : 2994
train acc:  0.7421875
train loss:  0.4616070091724396
train gradient:  0.13738184442363566
iteration : 2995
train acc:  0.71875
train loss:  0.5546915531158447
train gradient:  0.1653179740924491
iteration : 2996
train acc:  0.65625
train loss:  0.5665370225906372
train gradient:  0.17953429547788385
iteration : 2997
train acc:  0.7890625
train loss:  0.4686991274356842
train gradient:  0.16507291689274853
iteration : 2998
train acc:  0.71875
train loss:  0.5275247097015381
train gradient:  0.16402725189767958
iteration : 2999
train acc:  0.6484375
train loss:  0.5591828227043152
train gradient:  0.2628787850697632
iteration : 3000
train acc:  0.6640625
train loss:  0.6012069582939148
train gradient:  0.19798722606960467
iteration : 3001
train acc:  0.65625
train loss:  0.6243064403533936
train gradient:  0.15503380824498644
iteration : 3002
train acc:  0.734375
train loss:  0.5531477928161621
train gradient:  0.19041816559970118
iteration : 3003
train acc:  0.6328125
train loss:  0.6002434492111206
train gradient:  0.18569370750977213
iteration : 3004
train acc:  0.703125
train loss:  0.5967423915863037
train gradient:  0.22215054319203625
iteration : 3005
train acc:  0.671875
train loss:  0.5566363334655762
train gradient:  0.1683638123803654
iteration : 3006
train acc:  0.7265625
train loss:  0.5339174270629883
train gradient:  0.2435366500364488
iteration : 3007
train acc:  0.6953125
train loss:  0.5384594202041626
train gradient:  0.17778468281875864
iteration : 3008
train acc:  0.734375
train loss:  0.5178036689758301
train gradient:  0.2825952283399824
iteration : 3009
train acc:  0.6875
train loss:  0.5502524375915527
train gradient:  0.1564249043329254
iteration : 3010
train acc:  0.7421875
train loss:  0.5073734521865845
train gradient:  0.20363251500211263
iteration : 3011
train acc:  0.6796875
train loss:  0.5338770747184753
train gradient:  0.1715672436905239
iteration : 3012
train acc:  0.671875
train loss:  0.5648788213729858
train gradient:  0.13937734275111435
iteration : 3013
train acc:  0.7421875
train loss:  0.5149757266044617
train gradient:  0.15488926816640056
iteration : 3014
train acc:  0.734375
train loss:  0.5297369956970215
train gradient:  0.18033267078786025
iteration : 3015
train acc:  0.7109375
train loss:  0.5097590684890747
train gradient:  0.18859846770867694
iteration : 3016
train acc:  0.6875
train loss:  0.6240720152854919
train gradient:  0.22255675697159438
iteration : 3017
train acc:  0.7265625
train loss:  0.5214648246765137
train gradient:  0.15359392987351636
iteration : 3018
train acc:  0.78125
train loss:  0.5067769289016724
train gradient:  0.11478757598633169
iteration : 3019
train acc:  0.7109375
train loss:  0.5670539140701294
train gradient:  0.16268784125180522
iteration : 3020
train acc:  0.6875
train loss:  0.5254663228988647
train gradient:  0.2137125364026925
iteration : 3021
train acc:  0.7734375
train loss:  0.4937795102596283
train gradient:  0.1564746272716906
iteration : 3022
train acc:  0.71875
train loss:  0.49998238682746887
train gradient:  0.17427359228163242
iteration : 3023
train acc:  0.7109375
train loss:  0.5654416084289551
train gradient:  0.1657893261580687
iteration : 3024
train acc:  0.6875
train loss:  0.5361810922622681
train gradient:  0.17688050432135416
iteration : 3025
train acc:  0.7734375
train loss:  0.47204500436782837
train gradient:  0.129114455513673
iteration : 3026
train acc:  0.7109375
train loss:  0.5593152642250061
train gradient:  0.24487731077131125
iteration : 3027
train acc:  0.703125
train loss:  0.5520791411399841
train gradient:  0.148564265963816
iteration : 3028
train acc:  0.65625
train loss:  0.5794715881347656
train gradient:  0.20301464089981758
iteration : 3029
train acc:  0.71875
train loss:  0.5562621355056763
train gradient:  0.1901672473763209
iteration : 3030
train acc:  0.7421875
train loss:  0.49353528022766113
train gradient:  0.1194271968132016
iteration : 3031
train acc:  0.7578125
train loss:  0.48858821392059326
train gradient:  0.13819079618715746
iteration : 3032
train acc:  0.6953125
train loss:  0.531309187412262
train gradient:  0.11565293859487212
iteration : 3033
train acc:  0.71875
train loss:  0.5845785140991211
train gradient:  0.18629276457140856
iteration : 3034
train acc:  0.7734375
train loss:  0.4990087151527405
train gradient:  0.11564222041735527
iteration : 3035
train acc:  0.8046875
train loss:  0.5036050081253052
train gradient:  0.14902297521430843
iteration : 3036
train acc:  0.6796875
train loss:  0.541752815246582
train gradient:  0.1563214666884912
iteration : 3037
train acc:  0.7578125
train loss:  0.48803192377090454
train gradient:  0.17384164556787285
iteration : 3038
train acc:  0.703125
train loss:  0.5412228107452393
train gradient:  0.1633478698905011
iteration : 3039
train acc:  0.7890625
train loss:  0.4847499430179596
train gradient:  0.14878343004650085
iteration : 3040
train acc:  0.7265625
train loss:  0.5504382848739624
train gradient:  0.1507543243502668
iteration : 3041
train acc:  0.6953125
train loss:  0.5396844148635864
train gradient:  0.1816509715718987
iteration : 3042
train acc:  0.71875
train loss:  0.5332881212234497
train gradient:  0.1972958299539812
iteration : 3043
train acc:  0.7265625
train loss:  0.509387195110321
train gradient:  0.1396869906930196
iteration : 3044
train acc:  0.7109375
train loss:  0.5120298862457275
train gradient:  0.15478207607370187
iteration : 3045
train acc:  0.7265625
train loss:  0.5366498827934265
train gradient:  0.1321481993146239
iteration : 3046
train acc:  0.6640625
train loss:  0.5982717275619507
train gradient:  0.19916430515765554
iteration : 3047
train acc:  0.7109375
train loss:  0.5309520363807678
train gradient:  0.15862291854209637
iteration : 3048
train acc:  0.7265625
train loss:  0.5463056564331055
train gradient:  0.19132870048240025
iteration : 3049
train acc:  0.7734375
train loss:  0.4510922431945801
train gradient:  0.16535330308717733
iteration : 3050
train acc:  0.671875
train loss:  0.5643892288208008
train gradient:  0.16334111776412363
iteration : 3051
train acc:  0.6953125
train loss:  0.5544573068618774
train gradient:  0.18104953409959124
iteration : 3052
train acc:  0.6875
train loss:  0.514633297920227
train gradient:  0.15924404604094075
iteration : 3053
train acc:  0.640625
train loss:  0.6666547060012817
train gradient:  0.33233442783330397
iteration : 3054
train acc:  0.75
train loss:  0.5426287055015564
train gradient:  0.2968852230456359
iteration : 3055
train acc:  0.6640625
train loss:  0.5975967645645142
train gradient:  0.23331700214559387
iteration : 3056
train acc:  0.7734375
train loss:  0.4798929989337921
train gradient:  0.12555451741627466
iteration : 3057
train acc:  0.6640625
train loss:  0.6045331954956055
train gradient:  0.19935635522622036
iteration : 3058
train acc:  0.7109375
train loss:  0.5420618057250977
train gradient:  0.1809780455382789
iteration : 3059
train acc:  0.765625
train loss:  0.5409915447235107
train gradient:  0.15894449888382456
iteration : 3060
train acc:  0.671875
train loss:  0.5769737362861633
train gradient:  0.19715906229150482
iteration : 3061
train acc:  0.7734375
train loss:  0.5048248767852783
train gradient:  0.10411594592062982
iteration : 3062
train acc:  0.7578125
train loss:  0.4666473865509033
train gradient:  0.15513135081737062
iteration : 3063
train acc:  0.6640625
train loss:  0.5857352018356323
train gradient:  0.17248939432068305
iteration : 3064
train acc:  0.7265625
train loss:  0.5270125269889832
train gradient:  0.1729938385712358
iteration : 3065
train acc:  0.7265625
train loss:  0.5590915679931641
train gradient:  0.16422241186904823
iteration : 3066
train acc:  0.71875
train loss:  0.48434188961982727
train gradient:  0.13888774591899095
iteration : 3067
train acc:  0.7421875
train loss:  0.4960183799266815
train gradient:  0.12475567957934712
iteration : 3068
train acc:  0.6171875
train loss:  0.6257122755050659
train gradient:  0.1993437783900676
iteration : 3069
train acc:  0.6953125
train loss:  0.5502407550811768
train gradient:  0.16933907208961335
iteration : 3070
train acc:  0.796875
train loss:  0.4676215946674347
train gradient:  0.13273218648385904
iteration : 3071
train acc:  0.7578125
train loss:  0.5535586476325989
train gradient:  0.21965308976954595
iteration : 3072
train acc:  0.765625
train loss:  0.508273720741272
train gradient:  0.16500323938528383
iteration : 3073
train acc:  0.7890625
train loss:  0.5021384358406067
train gradient:  0.15981798762843263
iteration : 3074
train acc:  0.71875
train loss:  0.5521023869514465
train gradient:  0.15714808403185518
iteration : 3075
train acc:  0.6953125
train loss:  0.5586084127426147
train gradient:  0.14548608235303956
iteration : 3076
train acc:  0.734375
train loss:  0.5644143223762512
train gradient:  0.1513734009244957
iteration : 3077
train acc:  0.6875
train loss:  0.5933659672737122
train gradient:  0.2159231713126863
iteration : 3078
train acc:  0.734375
train loss:  0.5028064250946045
train gradient:  0.13039133488707785
iteration : 3079
train acc:  0.6953125
train loss:  0.5670607686042786
train gradient:  0.20101764988354925
iteration : 3080
train acc:  0.71875
train loss:  0.5583794116973877
train gradient:  0.2052531175197656
iteration : 3081
train acc:  0.7578125
train loss:  0.5074681043624878
train gradient:  0.1932174820774457
iteration : 3082
train acc:  0.6953125
train loss:  0.5565568208694458
train gradient:  0.17478047608209643
iteration : 3083
train acc:  0.75
train loss:  0.5049675107002258
train gradient:  0.15203760457710597
iteration : 3084
train acc:  0.7109375
train loss:  0.5057909488677979
train gradient:  0.15968019256589916
iteration : 3085
train acc:  0.734375
train loss:  0.5229737162590027
train gradient:  0.2174038119343713
iteration : 3086
train acc:  0.65625
train loss:  0.5879188179969788
train gradient:  0.20334841550011273
iteration : 3087
train acc:  0.6953125
train loss:  0.5374655723571777
train gradient:  0.16624443716170714
iteration : 3088
train acc:  0.71875
train loss:  0.5667202472686768
train gradient:  0.1960132540442413
iteration : 3089
train acc:  0.75
train loss:  0.5410201549530029
train gradient:  0.13410130366231876
iteration : 3090
train acc:  0.703125
train loss:  0.5336736440658569
train gradient:  0.16004751297898973
iteration : 3091
train acc:  0.7421875
train loss:  0.481600821018219
train gradient:  0.13963542554175762
iteration : 3092
train acc:  0.75
train loss:  0.5347579717636108
train gradient:  0.15518724439529805
iteration : 3093
train acc:  0.703125
train loss:  0.5450390577316284
train gradient:  0.14852313415438256
iteration : 3094
train acc:  0.7265625
train loss:  0.5250070691108704
train gradient:  0.2421626890644838
iteration : 3095
train acc:  0.640625
train loss:  0.589401125907898
train gradient:  0.17765688492495085
iteration : 3096
train acc:  0.7578125
train loss:  0.4625698924064636
train gradient:  0.12097102861005767
iteration : 3097
train acc:  0.7265625
train loss:  0.5181324481964111
train gradient:  0.12598303960998072
iteration : 3098
train acc:  0.6875
train loss:  0.5980713963508606
train gradient:  0.19241342586501897
iteration : 3099
train acc:  0.703125
train loss:  0.5451606512069702
train gradient:  0.16392612759895853
iteration : 3100
train acc:  0.7578125
train loss:  0.46667468547821045
train gradient:  0.12967221014487623
iteration : 3101
train acc:  0.7265625
train loss:  0.5657575130462646
train gradient:  0.1720138876001369
iteration : 3102
train acc:  0.7265625
train loss:  0.5357163548469543
train gradient:  0.15400114411943797
iteration : 3103
train acc:  0.765625
train loss:  0.4687458574771881
train gradient:  0.17800662751510482
iteration : 3104
train acc:  0.6640625
train loss:  0.57244873046875
train gradient:  0.2081480677399687
iteration : 3105
train acc:  0.703125
train loss:  0.5350439548492432
train gradient:  0.1680895957509549
iteration : 3106
train acc:  0.671875
train loss:  0.5649409294128418
train gradient:  0.19612119666729766
iteration : 3107
train acc:  0.703125
train loss:  0.5583130121231079
train gradient:  0.17855995340155006
iteration : 3108
train acc:  0.7578125
train loss:  0.4878293573856354
train gradient:  0.14508533599117499
iteration : 3109
train acc:  0.7265625
train loss:  0.5431028008460999
train gradient:  0.16318070071943203
iteration : 3110
train acc:  0.7421875
train loss:  0.49371394515037537
train gradient:  0.1739159669276795
iteration : 3111
train acc:  0.71875
train loss:  0.5447471141815186
train gradient:  0.20626308298689877
iteration : 3112
train acc:  0.6953125
train loss:  0.5867011547088623
train gradient:  0.24993585733439333
iteration : 3113
train acc:  0.7265625
train loss:  0.5194900035858154
train gradient:  0.18383334711934612
iteration : 3114
train acc:  0.671875
train loss:  0.5371311902999878
train gradient:  0.17057310016364802
iteration : 3115
train acc:  0.71875
train loss:  0.5516310930252075
train gradient:  0.16200283437770618
iteration : 3116
train acc:  0.7109375
train loss:  0.4989478588104248
train gradient:  0.14764131153234877
iteration : 3117
train acc:  0.671875
train loss:  0.5416763424873352
train gradient:  0.1684913662836824
iteration : 3118
train acc:  0.78125
train loss:  0.4638548493385315
train gradient:  0.11971051038353922
iteration : 3119
train acc:  0.7109375
train loss:  0.5541638731956482
train gradient:  0.15318700662180695
iteration : 3120
train acc:  0.75
train loss:  0.5363749265670776
train gradient:  0.17843696291804884
iteration : 3121
train acc:  0.765625
train loss:  0.4616178274154663
train gradient:  0.11947721885522405
iteration : 3122
train acc:  0.6875
train loss:  0.5241031646728516
train gradient:  0.16791786362293176
iteration : 3123
train acc:  0.75
train loss:  0.5576579570770264
train gradient:  0.22069712709225053
iteration : 3124
train acc:  0.6171875
train loss:  0.6399848461151123
train gradient:  0.21235038013467702
iteration : 3125
train acc:  0.7734375
train loss:  0.476040244102478
train gradient:  0.156768645963216
iteration : 3126
train acc:  0.7109375
train loss:  0.5503945350646973
train gradient:  0.15707365050498467
iteration : 3127
train acc:  0.6875
train loss:  0.5598709583282471
train gradient:  0.16523905331295297
iteration : 3128
train acc:  0.65625
train loss:  0.6068190336227417
train gradient:  0.23484948284259066
iteration : 3129
train acc:  0.7421875
train loss:  0.4948764741420746
train gradient:  0.17358211150039127
iteration : 3130
train acc:  0.7734375
train loss:  0.5096372365951538
train gradient:  0.16795321438119593
iteration : 3131
train acc:  0.71875
train loss:  0.5364004969596863
train gradient:  0.15088911973757013
iteration : 3132
train acc:  0.7109375
train loss:  0.5559278726577759
train gradient:  0.17640550370163766
iteration : 3133
train acc:  0.7578125
train loss:  0.5214921236038208
train gradient:  0.20038830252699164
iteration : 3134
train acc:  0.703125
train loss:  0.5049839019775391
train gradient:  0.14161749818368718
iteration : 3135
train acc:  0.65625
train loss:  0.5595041513442993
train gradient:  0.18203518685485576
iteration : 3136
train acc:  0.6640625
train loss:  0.599058210849762
train gradient:  0.22667531645417965
iteration : 3137
train acc:  0.703125
train loss:  0.4958871006965637
train gradient:  0.14287204965389458
iteration : 3138
train acc:  0.6015625
train loss:  0.6314202547073364
train gradient:  0.2516094234987891
iteration : 3139
train acc:  0.71875
train loss:  0.5895043611526489
train gradient:  0.22666939805155403
iteration : 3140
train acc:  0.703125
train loss:  0.5434374809265137
train gradient:  0.20212583564597153
iteration : 3141
train acc:  0.6875
train loss:  0.5823264718055725
train gradient:  0.21997776951585185
iteration : 3142
train acc:  0.7578125
train loss:  0.49855098128318787
train gradient:  0.1628822878582496
iteration : 3143
train acc:  0.703125
train loss:  0.5659474730491638
train gradient:  0.1847344097334213
iteration : 3144
train acc:  0.6796875
train loss:  0.5687969923019409
train gradient:  0.2057575438032903
iteration : 3145
train acc:  0.7109375
train loss:  0.5615063905715942
train gradient:  0.19385993013750052
iteration : 3146
train acc:  0.671875
train loss:  0.5478321313858032
train gradient:  0.15921088634299807
iteration : 3147
train acc:  0.765625
train loss:  0.5035030245780945
train gradient:  0.11702163038423467
iteration : 3148
train acc:  0.6796875
train loss:  0.5918631553649902
train gradient:  0.15540837726681336
iteration : 3149
train acc:  0.71875
train loss:  0.47335806488990784
train gradient:  0.13824836022933124
iteration : 3150
train acc:  0.7265625
train loss:  0.5246007442474365
train gradient:  0.15759293069057256
iteration : 3151
train acc:  0.7421875
train loss:  0.49107667803764343
train gradient:  0.17055018444746695
iteration : 3152
train acc:  0.59375
train loss:  0.6329060792922974
train gradient:  0.2590721374546353
iteration : 3153
train acc:  0.7109375
train loss:  0.5669387578964233
train gradient:  0.1530519328128071
iteration : 3154
train acc:  0.6796875
train loss:  0.603227436542511
train gradient:  0.25423878679838774
iteration : 3155
train acc:  0.75
train loss:  0.49877041578292847
train gradient:  0.1623281705661816
iteration : 3156
train acc:  0.703125
train loss:  0.5544775724411011
train gradient:  0.16792291939618428
iteration : 3157
train acc:  0.6328125
train loss:  0.6171437501907349
train gradient:  0.3086161365150954
iteration : 3158
train acc:  0.6796875
train loss:  0.5363704562187195
train gradient:  0.14527702493900255
iteration : 3159
train acc:  0.6953125
train loss:  0.5257818698883057
train gradient:  0.1540282325459662
iteration : 3160
train acc:  0.7265625
train loss:  0.5244691371917725
train gradient:  0.1462369618857257
iteration : 3161
train acc:  0.7265625
train loss:  0.5673130750656128
train gradient:  0.17075358949705843
iteration : 3162
train acc:  0.7734375
train loss:  0.5067830681800842
train gradient:  0.17361830462715055
iteration : 3163
train acc:  0.671875
train loss:  0.561600923538208
train gradient:  0.1513831577164044
iteration : 3164
train acc:  0.75
train loss:  0.5156664848327637
train gradient:  0.14709815000637402
iteration : 3165
train acc:  0.6484375
train loss:  0.5760125517845154
train gradient:  0.1707470759821211
iteration : 3166
train acc:  0.75
train loss:  0.50456702709198
train gradient:  0.16584205634160287
iteration : 3167
train acc:  0.7265625
train loss:  0.5715737342834473
train gradient:  0.1770870711711966
iteration : 3168
train acc:  0.75
train loss:  0.5194406509399414
train gradient:  0.1769256150455613
iteration : 3169
train acc:  0.640625
train loss:  0.6083661913871765
train gradient:  0.18116570125097448
iteration : 3170
train acc:  0.6953125
train loss:  0.5431785583496094
train gradient:  0.18646792263226045
iteration : 3171
train acc:  0.6640625
train loss:  0.5786442756652832
train gradient:  0.1400792519699008
iteration : 3172
train acc:  0.734375
train loss:  0.5253994464874268
train gradient:  0.16551977875683996
iteration : 3173
train acc:  0.671875
train loss:  0.5290715098381042
train gradient:  0.1914623069209419
iteration : 3174
train acc:  0.7265625
train loss:  0.5495002269744873
train gradient:  0.16526668590938154
iteration : 3175
train acc:  0.7265625
train loss:  0.5896779298782349
train gradient:  0.2224643604728737
iteration : 3176
train acc:  0.734375
train loss:  0.5267634987831116
train gradient:  0.13320299516511966
iteration : 3177
train acc:  0.734375
train loss:  0.4968113899230957
train gradient:  0.15181826841425528
iteration : 3178
train acc:  0.640625
train loss:  0.5866469144821167
train gradient:  0.2300729950355085
iteration : 3179
train acc:  0.7265625
train loss:  0.5108981728553772
train gradient:  0.1644206546850967
iteration : 3180
train acc:  0.7578125
train loss:  0.516933262348175
train gradient:  0.13880557895660856
iteration : 3181
train acc:  0.7734375
train loss:  0.48611944913864136
train gradient:  0.1509042227236907
iteration : 3182
train acc:  0.671875
train loss:  0.5924006700515747
train gradient:  0.14150302455323116
iteration : 3183
train acc:  0.7890625
train loss:  0.4581506848335266
train gradient:  0.11927036123506833
iteration : 3184
train acc:  0.7421875
train loss:  0.5461196899414062
train gradient:  0.1707948930846782
iteration : 3185
train acc:  0.734375
train loss:  0.48753082752227783
train gradient:  0.11095944099494787
iteration : 3186
train acc:  0.7109375
train loss:  0.5608724355697632
train gradient:  0.19893868577454227
iteration : 3187
train acc:  0.7265625
train loss:  0.49943315982818604
train gradient:  0.13051491297420137
iteration : 3188
train acc:  0.8046875
train loss:  0.45365071296691895
train gradient:  0.12510698312356228
iteration : 3189
train acc:  0.6953125
train loss:  0.568632960319519
train gradient:  0.17127253044860447
iteration : 3190
train acc:  0.71875
train loss:  0.5646679997444153
train gradient:  0.1956100257369664
iteration : 3191
train acc:  0.7421875
train loss:  0.4883135259151459
train gradient:  0.12722299522193897
iteration : 3192
train acc:  0.7421875
train loss:  0.5182157158851624
train gradient:  0.1590689633331656
iteration : 3193
train acc:  0.703125
train loss:  0.5072019696235657
train gradient:  0.1728250102270702
iteration : 3194
train acc:  0.6953125
train loss:  0.5631425380706787
train gradient:  0.17541711774302027
iteration : 3195
train acc:  0.6796875
train loss:  0.5461680889129639
train gradient:  0.16642017395477526
iteration : 3196
train acc:  0.7734375
train loss:  0.46766191720962524
train gradient:  0.17205345466319716
iteration : 3197
train acc:  0.7109375
train loss:  0.5427663922309875
train gradient:  0.2000557068980551
iteration : 3198
train acc:  0.7109375
train loss:  0.5067135691642761
train gradient:  0.18451674352830627
iteration : 3199
train acc:  0.7578125
train loss:  0.4624418616294861
train gradient:  0.12687711161684906
iteration : 3200
train acc:  0.6640625
train loss:  0.5743628740310669
train gradient:  0.17082889231641613
iteration : 3201
train acc:  0.71875
train loss:  0.4951270818710327
train gradient:  0.117774612096276
iteration : 3202
train acc:  0.75
train loss:  0.5306300520896912
train gradient:  0.14883940795279976
iteration : 3203
train acc:  0.6875
train loss:  0.5086794495582581
train gradient:  0.14561561536320633
iteration : 3204
train acc:  0.6171875
train loss:  0.6099618077278137
train gradient:  0.22222451361572296
iteration : 3205
train acc:  0.734375
train loss:  0.5057693719863892
train gradient:  0.17161921892534054
iteration : 3206
train acc:  0.6953125
train loss:  0.5752531290054321
train gradient:  0.18281936899403933
iteration : 3207
train acc:  0.7578125
train loss:  0.46502649784088135
train gradient:  0.13757783738736523
iteration : 3208
train acc:  0.75
train loss:  0.5200635194778442
train gradient:  0.1200564201322849
iteration : 3209
train acc:  0.734375
train loss:  0.500891923904419
train gradient:  0.15883289990081195
iteration : 3210
train acc:  0.7109375
train loss:  0.5446456074714661
train gradient:  0.15918760757347694
iteration : 3211
train acc:  0.7109375
train loss:  0.5534195303916931
train gradient:  0.1615940668595245
iteration : 3212
train acc:  0.78125
train loss:  0.4971970021724701
train gradient:  0.1948579118652306
iteration : 3213
train acc:  0.765625
train loss:  0.5519469976425171
train gradient:  0.16181543958319583
iteration : 3214
train acc:  0.7578125
train loss:  0.5391408801078796
train gradient:  0.20105037552665306
iteration : 3215
train acc:  0.7109375
train loss:  0.5510249137878418
train gradient:  0.19551183240672426
iteration : 3216
train acc:  0.65625
train loss:  0.569938600063324
train gradient:  0.22681043324419758
iteration : 3217
train acc:  0.7890625
train loss:  0.48546528816223145
train gradient:  0.13106904184046625
iteration : 3218
train acc:  0.75
train loss:  0.5177262425422668
train gradient:  0.16875213807979872
iteration : 3219
train acc:  0.8125
train loss:  0.45357388257980347
train gradient:  0.14996958350509282
iteration : 3220
train acc:  0.6796875
train loss:  0.5390275716781616
train gradient:  0.1854348586462221
iteration : 3221
train acc:  0.703125
train loss:  0.5189421772956848
train gradient:  0.18881734027066643
iteration : 3222
train acc:  0.65625
train loss:  0.594333291053772
train gradient:  0.18841092022663708
iteration : 3223
train acc:  0.7734375
train loss:  0.4602283239364624
train gradient:  0.18949238832733822
iteration : 3224
train acc:  0.734375
train loss:  0.5269732475280762
train gradient:  0.15119238244919503
iteration : 3225
train acc:  0.71875
train loss:  0.5312830209732056
train gradient:  0.15136926399499795
iteration : 3226
train acc:  0.7109375
train loss:  0.5368846654891968
train gradient:  0.1737282461837008
iteration : 3227
train acc:  0.7734375
train loss:  0.5062958002090454
train gradient:  0.12695567267590807
iteration : 3228
train acc:  0.6796875
train loss:  0.5386455655097961
train gradient:  0.18432723278344837
iteration : 3229
train acc:  0.65625
train loss:  0.6534677743911743
train gradient:  0.22118145182277643
iteration : 3230
train acc:  0.6640625
train loss:  0.5933009386062622
train gradient:  0.20768938467955306
iteration : 3231
train acc:  0.75
train loss:  0.46669459342956543
train gradient:  0.11597125887727017
iteration : 3232
train acc:  0.7421875
train loss:  0.5283374786376953
train gradient:  0.15535213197399675
iteration : 3233
train acc:  0.7265625
train loss:  0.5270812511444092
train gradient:  0.1847009466693737
iteration : 3234
train acc:  0.7578125
train loss:  0.49705183506011963
train gradient:  0.10320913828222367
iteration : 3235
train acc:  0.703125
train loss:  0.5197494029998779
train gradient:  0.1607324701995377
iteration : 3236
train acc:  0.6484375
train loss:  0.5864388942718506
train gradient:  0.15949364319518572
iteration : 3237
train acc:  0.6640625
train loss:  0.6109849214553833
train gradient:  0.1651394398797913
iteration : 3238
train acc:  0.7421875
train loss:  0.5471927523612976
train gradient:  0.1498535796476182
iteration : 3239
train acc:  0.6796875
train loss:  0.5457032918930054
train gradient:  0.1566925523063186
iteration : 3240
train acc:  0.6953125
train loss:  0.5462808609008789
train gradient:  0.16180522514615792
iteration : 3241
train acc:  0.71875
train loss:  0.5143919587135315
train gradient:  0.15878113565520158
iteration : 3242
train acc:  0.7265625
train loss:  0.512528657913208
train gradient:  0.17564019160243563
iteration : 3243
train acc:  0.7109375
train loss:  0.5163263082504272
train gradient:  0.20003101903241666
iteration : 3244
train acc:  0.6796875
train loss:  0.533936619758606
train gradient:  0.17591901881291677
iteration : 3245
train acc:  0.7578125
train loss:  0.516255259513855
train gradient:  0.21000529109253785
iteration : 3246
train acc:  0.734375
train loss:  0.48970356583595276
train gradient:  0.1575281540265922
iteration : 3247
train acc:  0.7265625
train loss:  0.5435127019882202
train gradient:  0.12438256324557748
iteration : 3248
train acc:  0.6640625
train loss:  0.5617132186889648
train gradient:  0.1794132518695208
iteration : 3249
train acc:  0.703125
train loss:  0.5380983948707581
train gradient:  0.155003099660659
iteration : 3250
train acc:  0.7734375
train loss:  0.48707544803619385
train gradient:  0.12774186203440693
iteration : 3251
train acc:  0.8359375
train loss:  0.4418339729309082
train gradient:  0.10248957883599356
iteration : 3252
train acc:  0.75
train loss:  0.5126855373382568
train gradient:  0.13178555053500549
iteration : 3253
train acc:  0.671875
train loss:  0.5437904000282288
train gradient:  0.17344707916909952
iteration : 3254
train acc:  0.6796875
train loss:  0.5546820163726807
train gradient:  0.16873263037913638
iteration : 3255
train acc:  0.6875
train loss:  0.5635297894477844
train gradient:  0.18580135796400848
iteration : 3256
train acc:  0.625
train loss:  0.6132051944732666
train gradient:  0.20849966180070723
iteration : 3257
train acc:  0.734375
train loss:  0.5402764081954956
train gradient:  0.18615839732547457
iteration : 3258
train acc:  0.7109375
train loss:  0.5575679540634155
train gradient:  0.2042714906800936
iteration : 3259
train acc:  0.765625
train loss:  0.47376471757888794
train gradient:  0.12294682464363417
iteration : 3260
train acc:  0.671875
train loss:  0.6131085157394409
train gradient:  0.21494077897873784
iteration : 3261
train acc:  0.734375
train loss:  0.5430700778961182
train gradient:  0.12995496812730736
iteration : 3262
train acc:  0.7578125
train loss:  0.5056072473526001
train gradient:  0.15783378161683725
iteration : 3263
train acc:  0.6796875
train loss:  0.5417448282241821
train gradient:  0.14330846431064082
iteration : 3264
train acc:  0.796875
train loss:  0.46598926186561584
train gradient:  0.1335793648959738
iteration : 3265
train acc:  0.765625
train loss:  0.5625783205032349
train gradient:  0.15404079228764547
iteration : 3266
train acc:  0.734375
train loss:  0.5045535564422607
train gradient:  0.16241918011703738
iteration : 3267
train acc:  0.734375
train loss:  0.5226696729660034
train gradient:  0.15998978579811177
iteration : 3268
train acc:  0.71875
train loss:  0.6155906915664673
train gradient:  0.3035276819586638
iteration : 3269
train acc:  0.7734375
train loss:  0.4665873944759369
train gradient:  0.14983985435930203
iteration : 3270
train acc:  0.765625
train loss:  0.5050206780433655
train gradient:  0.2372215071333249
iteration : 3271
train acc:  0.703125
train loss:  0.5128878951072693
train gradient:  0.14839848912503495
iteration : 3272
train acc:  0.6640625
train loss:  0.5841364860534668
train gradient:  0.19053546850443553
iteration : 3273
train acc:  0.7265625
train loss:  0.5321659445762634
train gradient:  0.13811435554391635
iteration : 3274
train acc:  0.6484375
train loss:  0.5686988234519958
train gradient:  0.26481832084830265
iteration : 3275
train acc:  0.75
train loss:  0.5041775107383728
train gradient:  0.12539265428022975
iteration : 3276
train acc:  0.6796875
train loss:  0.6097730398178101
train gradient:  0.19281391436204826
iteration : 3277
train acc:  0.71875
train loss:  0.5238068699836731
train gradient:  0.1351375814529599
iteration : 3278
train acc:  0.7421875
train loss:  0.5319002866744995
train gradient:  0.14727581210196022
iteration : 3279
train acc:  0.7421875
train loss:  0.49796709418296814
train gradient:  0.1336086089306095
iteration : 3280
train acc:  0.7265625
train loss:  0.5171807408332825
train gradient:  0.16073550017144955
iteration : 3281
train acc:  0.6796875
train loss:  0.549956738948822
train gradient:  0.16001049590449046
iteration : 3282
train acc:  0.71875
train loss:  0.5172381401062012
train gradient:  0.13280840556875578
iteration : 3283
train acc:  0.6171875
train loss:  0.5833876729011536
train gradient:  0.17546789552252906
iteration : 3284
train acc:  0.6875
train loss:  0.5407639741897583
train gradient:  0.25824952298930903
iteration : 3285
train acc:  0.7265625
train loss:  0.49900946021080017
train gradient:  0.14138389784887656
iteration : 3286
train acc:  0.75
train loss:  0.5371057987213135
train gradient:  0.15484092122182008
iteration : 3287
train acc:  0.7265625
train loss:  0.5513820648193359
train gradient:  0.14220234577530144
iteration : 3288
train acc:  0.7578125
train loss:  0.5045187473297119
train gradient:  0.12280272630226542
iteration : 3289
train acc:  0.7265625
train loss:  0.525204062461853
train gradient:  0.19826661018320466
iteration : 3290
train acc:  0.640625
train loss:  0.6016056537628174
train gradient:  0.21342339775796487
iteration : 3291
train acc:  0.6953125
train loss:  0.544089138507843
train gradient:  0.14600718438143714
iteration : 3292
train acc:  0.734375
train loss:  0.5208554267883301
train gradient:  0.12812566585115914
iteration : 3293
train acc:  0.6953125
train loss:  0.5957403182983398
train gradient:  0.1690788440288327
iteration : 3294
train acc:  0.671875
train loss:  0.5894829034805298
train gradient:  0.21662156266699065
iteration : 3295
train acc:  0.7734375
train loss:  0.5133683681488037
train gradient:  0.15375956927573114
iteration : 3296
train acc:  0.6953125
train loss:  0.5726084113121033
train gradient:  0.14771045101276947
iteration : 3297
train acc:  0.703125
train loss:  0.5122578144073486
train gradient:  0.16347976280335566
iteration : 3298
train acc:  0.765625
train loss:  0.5128961801528931
train gradient:  0.12702240487830882
iteration : 3299
train acc:  0.765625
train loss:  0.5054807662963867
train gradient:  0.15465701940410886
iteration : 3300
train acc:  0.78125
train loss:  0.46569743752479553
train gradient:  0.1762565513845571
iteration : 3301
train acc:  0.7265625
train loss:  0.5177345275878906
train gradient:  0.14632158217755104
iteration : 3302
train acc:  0.6875
train loss:  0.5660855174064636
train gradient:  0.21634866593694335
iteration : 3303
train acc:  0.6640625
train loss:  0.5750322937965393
train gradient:  0.1549706251091388
iteration : 3304
train acc:  0.65625
train loss:  0.5775449275970459
train gradient:  0.19877785524943115
iteration : 3305
train acc:  0.75
train loss:  0.48328518867492676
train gradient:  0.13460151693243522
iteration : 3306
train acc:  0.6796875
train loss:  0.5637428760528564
train gradient:  0.1966233706005423
iteration : 3307
train acc:  0.734375
train loss:  0.5413693189620972
train gradient:  0.17718965272170392
iteration : 3308
train acc:  0.703125
train loss:  0.5470831990242004
train gradient:  0.15356024579613503
iteration : 3309
train acc:  0.671875
train loss:  0.5550389885902405
train gradient:  0.17595011359812002
iteration : 3310
train acc:  0.6953125
train loss:  0.5164074897766113
train gradient:  0.16883794270149427
iteration : 3311
train acc:  0.6953125
train loss:  0.5508136749267578
train gradient:  0.2119146922465285
iteration : 3312
train acc:  0.7421875
train loss:  0.505832314491272
train gradient:  0.14097615777556372
iteration : 3313
train acc:  0.75
train loss:  0.5300670862197876
train gradient:  0.20257756221882423
iteration : 3314
train acc:  0.75
train loss:  0.4988904595375061
train gradient:  0.14646791853922292
iteration : 3315
train acc:  0.6640625
train loss:  0.54917973279953
train gradient:  0.17060865355108146
iteration : 3316
train acc:  0.65625
train loss:  0.6084216833114624
train gradient:  0.257323143666357
iteration : 3317
train acc:  0.703125
train loss:  0.5590204000473022
train gradient:  0.14538895828621284
iteration : 3318
train acc:  0.75
train loss:  0.4818214178085327
train gradient:  0.13664786832016435
iteration : 3319
train acc:  0.7421875
train loss:  0.49671295285224915
train gradient:  0.15116377379921797
iteration : 3320
train acc:  0.6953125
train loss:  0.5838344097137451
train gradient:  0.18761241619083177
iteration : 3321
train acc:  0.75
train loss:  0.5083746910095215
train gradient:  0.14284439863963289
iteration : 3322
train acc:  0.71875
train loss:  0.5115946531295776
train gradient:  0.1595498416429212
iteration : 3323
train acc:  0.7265625
train loss:  0.5120168924331665
train gradient:  0.15330347277665263
iteration : 3324
train acc:  0.71875
train loss:  0.5227388143539429
train gradient:  0.15992167743863933
iteration : 3325
train acc:  0.765625
train loss:  0.481737345457077
train gradient:  0.1778097505806383
iteration : 3326
train acc:  0.703125
train loss:  0.5586445927619934
train gradient:  0.2188588683524826
iteration : 3327
train acc:  0.671875
train loss:  0.5739269256591797
train gradient:  0.1509420653856538
iteration : 3328
train acc:  0.78125
train loss:  0.47821664810180664
train gradient:  0.12843579551226722
iteration : 3329
train acc:  0.7421875
train loss:  0.5193902254104614
train gradient:  0.14074744737656494
iteration : 3330
train acc:  0.7578125
train loss:  0.46944335103034973
train gradient:  0.1288709138626195
iteration : 3331
train acc:  0.7265625
train loss:  0.5073164701461792
train gradient:  0.12181704774157708
iteration : 3332
train acc:  0.78125
train loss:  0.49585095047950745
train gradient:  0.16067648444576205
iteration : 3333
train acc:  0.71875
train loss:  0.5341541767120361
train gradient:  0.14077669888686017
iteration : 3334
train acc:  0.7578125
train loss:  0.5281606316566467
train gradient:  0.12900506920628874
iteration : 3335
train acc:  0.6875
train loss:  0.5502607822418213
train gradient:  0.2100245454184651
iteration : 3336
train acc:  0.6796875
train loss:  0.5593976974487305
train gradient:  0.18544766486610653
iteration : 3337
train acc:  0.7890625
train loss:  0.4833142161369324
train gradient:  0.1221723926044599
iteration : 3338
train acc:  0.7421875
train loss:  0.5063319206237793
train gradient:  0.15521863049164242
iteration : 3339
train acc:  0.6640625
train loss:  0.5747040510177612
train gradient:  0.21529760236568665
iteration : 3340
train acc:  0.6953125
train loss:  0.510844349861145
train gradient:  0.15376572109535502
iteration : 3341
train acc:  0.7734375
train loss:  0.4929509460926056
train gradient:  0.1737025104794359
iteration : 3342
train acc:  0.734375
train loss:  0.5225584506988525
train gradient:  0.15210250702482994
iteration : 3343
train acc:  0.7578125
train loss:  0.4453326463699341
train gradient:  0.13571668506976176
iteration : 3344
train acc:  0.7109375
train loss:  0.49154454469680786
train gradient:  0.12349810407262941
iteration : 3345
train acc:  0.6953125
train loss:  0.5533503293991089
train gradient:  0.19712709734866618
iteration : 3346
train acc:  0.7109375
train loss:  0.5543210506439209
train gradient:  0.21516696567655563
iteration : 3347
train acc:  0.703125
train loss:  0.5470550060272217
train gradient:  0.1840621608509498
iteration : 3348
train acc:  0.65625
train loss:  0.5767590403556824
train gradient:  0.18973100656049868
iteration : 3349
train acc:  0.703125
train loss:  0.5492498874664307
train gradient:  0.1641916680407003
iteration : 3350
train acc:  0.6640625
train loss:  0.5803130865097046
train gradient:  0.1571941452990398
iteration : 3351
train acc:  0.703125
train loss:  0.5507189035415649
train gradient:  0.17214966924191716
iteration : 3352
train acc:  0.765625
train loss:  0.49712711572647095
train gradient:  0.17347994706106437
iteration : 3353
train acc:  0.71875
train loss:  0.5369521379470825
train gradient:  0.15626601572752136
iteration : 3354
train acc:  0.671875
train loss:  0.5445276498794556
train gradient:  0.1806097867802377
iteration : 3355
train acc:  0.734375
train loss:  0.5369524359703064
train gradient:  0.15426178059146434
iteration : 3356
train acc:  0.71875
train loss:  0.5510561466217041
train gradient:  0.15209964563981948
iteration : 3357
train acc:  0.75
train loss:  0.4819421172142029
train gradient:  0.1274960346159827
iteration : 3358
train acc:  0.7421875
train loss:  0.4870087504386902
train gradient:  0.1366952810597903
iteration : 3359
train acc:  0.7421875
train loss:  0.5183871984481812
train gradient:  0.17267662208361872
iteration : 3360
train acc:  0.640625
train loss:  0.6154597401618958
train gradient:  0.16934751834688827
iteration : 3361
train acc:  0.75
train loss:  0.4954523742198944
train gradient:  0.15111191620116313
iteration : 3362
train acc:  0.7421875
train loss:  0.49744144082069397
train gradient:  0.14469115723946616
iteration : 3363
train acc:  0.7421875
train loss:  0.526415228843689
train gradient:  0.18969674150345303
iteration : 3364
train acc:  0.71875
train loss:  0.5108307003974915
train gradient:  0.17661444095498754
iteration : 3365
train acc:  0.7421875
train loss:  0.5008431673049927
train gradient:  0.1478213906617031
iteration : 3366
train acc:  0.703125
train loss:  0.5184751749038696
train gradient:  0.15553619472150887
iteration : 3367
train acc:  0.6484375
train loss:  0.5930083394050598
train gradient:  0.2083702985674739
iteration : 3368
train acc:  0.6640625
train loss:  0.5867699384689331
train gradient:  0.22204987748060878
iteration : 3369
train acc:  0.6640625
train loss:  0.5659536719322205
train gradient:  0.19381107748762988
iteration : 3370
train acc:  0.734375
train loss:  0.5367833375930786
train gradient:  0.15943092297746245
iteration : 3371
train acc:  0.6953125
train loss:  0.5716491937637329
train gradient:  0.15430098518576552
iteration : 3372
train acc:  0.7265625
train loss:  0.4675293564796448
train gradient:  0.11990380570492726
iteration : 3373
train acc:  0.6796875
train loss:  0.5375666618347168
train gradient:  0.15129795472652063
iteration : 3374
train acc:  0.6875
train loss:  0.5028674006462097
train gradient:  0.18787244423168686
iteration : 3375
train acc:  0.671875
train loss:  0.606967568397522
train gradient:  0.19330317642076117
iteration : 3376
train acc:  0.6640625
train loss:  0.6113362312316895
train gradient:  0.250022754456552
iteration : 3377
train acc:  0.78125
train loss:  0.49271324276924133
train gradient:  0.16881850218114616
iteration : 3378
train acc:  0.6796875
train loss:  0.5599879026412964
train gradient:  0.18295328605025313
iteration : 3379
train acc:  0.6953125
train loss:  0.5461035370826721
train gradient:  0.1666622263651959
iteration : 3380
train acc:  0.6796875
train loss:  0.5766878128051758
train gradient:  0.20646696907814094
iteration : 3381
train acc:  0.6640625
train loss:  0.5883503556251526
train gradient:  0.24634219961938125
iteration : 3382
train acc:  0.703125
train loss:  0.5279319286346436
train gradient:  0.1900666196486681
iteration : 3383
train acc:  0.765625
train loss:  0.4979701042175293
train gradient:  0.15825246694290512
iteration : 3384
train acc:  0.734375
train loss:  0.5056264996528625
train gradient:  0.17489946484382
iteration : 3385
train acc:  0.7578125
train loss:  0.5163171887397766
train gradient:  0.15518380610128882
iteration : 3386
train acc:  0.703125
train loss:  0.5934031009674072
train gradient:  0.17711649440070462
iteration : 3387
train acc:  0.7265625
train loss:  0.5409122705459595
train gradient:  0.16666618045801357
iteration : 3388
train acc:  0.6640625
train loss:  0.5904805660247803
train gradient:  0.19562680066661792
iteration : 3389
train acc:  0.6875
train loss:  0.526208221912384
train gradient:  0.14060038849075995
iteration : 3390
train acc:  0.65625
train loss:  0.58915114402771
train gradient:  0.20851161394246187
iteration : 3391
train acc:  0.7421875
train loss:  0.5078930854797363
train gradient:  0.12875488799300439
iteration : 3392
train acc:  0.7421875
train loss:  0.48596370220184326
train gradient:  0.16677549221175092
iteration : 3393
train acc:  0.765625
train loss:  0.5632173418998718
train gradient:  0.18133135058372085
iteration : 3394
train acc:  0.734375
train loss:  0.5374941825866699
train gradient:  0.14922809563499972
iteration : 3395
train acc:  0.7734375
train loss:  0.5247839689254761
train gradient:  0.1629208634291413
iteration : 3396
train acc:  0.765625
train loss:  0.4912091791629791
train gradient:  0.13851612242366743
iteration : 3397
train acc:  0.703125
train loss:  0.5495560169219971
train gradient:  0.18029971234852615
iteration : 3398
train acc:  0.6875
train loss:  0.5994787812232971
train gradient:  0.16632931787502664
iteration : 3399
train acc:  0.765625
train loss:  0.45603951811790466
train gradient:  0.13082498321202762
iteration : 3400
train acc:  0.6953125
train loss:  0.5506619811058044
train gradient:  0.2016344123161892
iteration : 3401
train acc:  0.7578125
train loss:  0.47575563192367554
train gradient:  0.15180316448294273
iteration : 3402
train acc:  0.75
train loss:  0.5037885308265686
train gradient:  0.1591583469218475
iteration : 3403
train acc:  0.765625
train loss:  0.505963921546936
train gradient:  0.1560362510348463
iteration : 3404
train acc:  0.734375
train loss:  0.4932260513305664
train gradient:  0.11892047324932696
iteration : 3405
train acc:  0.6953125
train loss:  0.5289766788482666
train gradient:  0.16147058649266222
iteration : 3406
train acc:  0.7109375
train loss:  0.5751195549964905
train gradient:  0.1780709603797867
iteration : 3407
train acc:  0.78125
train loss:  0.5291303396224976
train gradient:  0.1382936614935438
iteration : 3408
train acc:  0.71875
train loss:  0.5175009965896606
train gradient:  0.13764118676868936
iteration : 3409
train acc:  0.6875
train loss:  0.535459041595459
train gradient:  0.1472178787375263
iteration : 3410
train acc:  0.7109375
train loss:  0.5066690444946289
train gradient:  0.13568517273544323
iteration : 3411
train acc:  0.734375
train loss:  0.5030491352081299
train gradient:  0.14211480432134665
iteration : 3412
train acc:  0.6875
train loss:  0.536063551902771
train gradient:  0.19933433416303958
iteration : 3413
train acc:  0.6953125
train loss:  0.5627594590187073
train gradient:  0.1694897147787018
iteration : 3414
train acc:  0.734375
train loss:  0.5376068353652954
train gradient:  0.14549833263220885
iteration : 3415
train acc:  0.7421875
train loss:  0.5193434953689575
train gradient:  0.2439901099920559
iteration : 3416
train acc:  0.703125
train loss:  0.5319072008132935
train gradient:  0.12727245711362833
iteration : 3417
train acc:  0.734375
train loss:  0.5094598531723022
train gradient:  0.20658708790981084
iteration : 3418
train acc:  0.734375
train loss:  0.5201039910316467
train gradient:  0.15107240888049242
iteration : 3419
train acc:  0.7109375
train loss:  0.5352230668067932
train gradient:  0.1585643505788083
iteration : 3420
train acc:  0.71875
train loss:  0.5001614689826965
train gradient:  0.12745789153492038
iteration : 3421
train acc:  0.734375
train loss:  0.5586200952529907
train gradient:  0.1544963241866475
iteration : 3422
train acc:  0.671875
train loss:  0.5583946108818054
train gradient:  0.18384662104754668
iteration : 3423
train acc:  0.7265625
train loss:  0.4782907962799072
train gradient:  0.1788568136809584
iteration : 3424
train acc:  0.7265625
train loss:  0.5368583798408508
train gradient:  0.21550692452323716
iteration : 3425
train acc:  0.6953125
train loss:  0.5518633127212524
train gradient:  0.14576121290222147
iteration : 3426
train acc:  0.6953125
train loss:  0.5553433895111084
train gradient:  0.1596323997973746
iteration : 3427
train acc:  0.6953125
train loss:  0.5434412956237793
train gradient:  0.1823447632582867
iteration : 3428
train acc:  0.6875
train loss:  0.5409705638885498
train gradient:  0.18718612858445122
iteration : 3429
train acc:  0.65625
train loss:  0.5359211564064026
train gradient:  0.1821152973408029
iteration : 3430
train acc:  0.7109375
train loss:  0.5310385227203369
train gradient:  0.12257773273088399
iteration : 3431
train acc:  0.7109375
train loss:  0.5288163423538208
train gradient:  0.16281981839858023
iteration : 3432
train acc:  0.703125
train loss:  0.5412862300872803
train gradient:  0.19323966134681675
iteration : 3433
train acc:  0.6640625
train loss:  0.6040436029434204
train gradient:  0.2194100602588499
iteration : 3434
train acc:  0.734375
train loss:  0.5004492998123169
train gradient:  0.14202421444042962
iteration : 3435
train acc:  0.7265625
train loss:  0.5469108819961548
train gradient:  0.13208823573318665
iteration : 3436
train acc:  0.7109375
train loss:  0.5572835206985474
train gradient:  0.1786732142014807
iteration : 3437
train acc:  0.734375
train loss:  0.4486503601074219
train gradient:  0.14288909167606317
iteration : 3438
train acc:  0.8046875
train loss:  0.4793381690979004
train gradient:  0.1781324683908997
iteration : 3439
train acc:  0.6484375
train loss:  0.5479742288589478
train gradient:  0.1576453345444777
iteration : 3440
train acc:  0.765625
train loss:  0.5089777708053589
train gradient:  0.15437826840393376
iteration : 3441
train acc:  0.7265625
train loss:  0.5631294250488281
train gradient:  0.1406420107233406
iteration : 3442
train acc:  0.65625
train loss:  0.5635204315185547
train gradient:  0.14501616864500694
iteration : 3443
train acc:  0.640625
train loss:  0.6293315887451172
train gradient:  0.23043637918761112
iteration : 3444
train acc:  0.6796875
train loss:  0.551059365272522
train gradient:  0.1415885364144015
iteration : 3445
train acc:  0.6875
train loss:  0.5363947749137878
train gradient:  0.18608287969064075
iteration : 3446
train acc:  0.7578125
train loss:  0.5009210705757141
train gradient:  0.1637921350187403
iteration : 3447
train acc:  0.7890625
train loss:  0.5051157474517822
train gradient:  0.13798729074682808
iteration : 3448
train acc:  0.7421875
train loss:  0.547493577003479
train gradient:  0.1487968000516569
iteration : 3449
train acc:  0.640625
train loss:  0.5949679613113403
train gradient:  0.19060490364046728
iteration : 3450
train acc:  0.75
train loss:  0.560126781463623
train gradient:  0.2044918929825731
iteration : 3451
train acc:  0.7421875
train loss:  0.5672077536582947
train gradient:  0.17462224694379586
iteration : 3452
train acc:  0.7109375
train loss:  0.5605353116989136
train gradient:  0.18882804854978022
iteration : 3453
train acc:  0.671875
train loss:  0.5859091281890869
train gradient:  0.16591545939716165
iteration : 3454
train acc:  0.7734375
train loss:  0.45609763264656067
train gradient:  0.14182695092873512
iteration : 3455
train acc:  0.7265625
train loss:  0.4730913043022156
train gradient:  0.13629867180350846
iteration : 3456
train acc:  0.65625
train loss:  0.5768065452575684
train gradient:  0.1931382366210712
iteration : 3457
train acc:  0.71875
train loss:  0.5471975207328796
train gradient:  0.1877409288992846
iteration : 3458
train acc:  0.7265625
train loss:  0.5251650810241699
train gradient:  0.16197501872288306
iteration : 3459
train acc:  0.6796875
train loss:  0.591210663318634
train gradient:  0.16288052625245797
iteration : 3460
train acc:  0.734375
train loss:  0.543007493019104
train gradient:  0.16046672834408487
iteration : 3461
train acc:  0.7109375
train loss:  0.5097984075546265
train gradient:  0.1684964376022423
iteration : 3462
train acc:  0.7265625
train loss:  0.5488206744194031
train gradient:  0.15839562351692374
iteration : 3463
train acc:  0.6796875
train loss:  0.5371866226196289
train gradient:  0.1886553121843343
iteration : 3464
train acc:  0.734375
train loss:  0.4996131658554077
train gradient:  0.186979388142248
iteration : 3465
train acc:  0.765625
train loss:  0.4812466502189636
train gradient:  0.13699178988697044
iteration : 3466
train acc:  0.7890625
train loss:  0.4894636869430542
train gradient:  0.13612451639266537
iteration : 3467
train acc:  0.7109375
train loss:  0.5371633768081665
train gradient:  0.15335891742968039
iteration : 3468
train acc:  0.7890625
train loss:  0.46458643674850464
train gradient:  0.13281559676300816
iteration : 3469
train acc:  0.7421875
train loss:  0.5167759656906128
train gradient:  0.14921069616164429
iteration : 3470
train acc:  0.71875
train loss:  0.4842005968093872
train gradient:  0.11710834856894763
iteration : 3471
train acc:  0.7265625
train loss:  0.5261759161949158
train gradient:  0.17108348398238077
iteration : 3472
train acc:  0.703125
train loss:  0.6023627519607544
train gradient:  0.18827917382521697
iteration : 3473
train acc:  0.75
train loss:  0.5198469758033752
train gradient:  0.1921319557931771
iteration : 3474
train acc:  0.671875
train loss:  0.5626113414764404
train gradient:  0.22145766063926187
iteration : 3475
train acc:  0.7421875
train loss:  0.4929855465888977
train gradient:  0.11811124605932871
iteration : 3476
train acc:  0.703125
train loss:  0.5935895442962646
train gradient:  0.29759142974191816
iteration : 3477
train acc:  0.734375
train loss:  0.5154873132705688
train gradient:  0.15732761788146127
iteration : 3478
train acc:  0.703125
train loss:  0.5118398070335388
train gradient:  0.12407647033453877
iteration : 3479
train acc:  0.7109375
train loss:  0.553450345993042
train gradient:  0.13272202423008012
iteration : 3480
train acc:  0.734375
train loss:  0.5230106711387634
train gradient:  0.13263742302646297
iteration : 3481
train acc:  0.71875
train loss:  0.513096809387207
train gradient:  0.16738632864515918
iteration : 3482
train acc:  0.6796875
train loss:  0.5834928750991821
train gradient:  0.19476378929169796
iteration : 3483
train acc:  0.7578125
train loss:  0.5150166153907776
train gradient:  0.1580368758863986
iteration : 3484
train acc:  0.7578125
train loss:  0.517772376537323
train gradient:  0.13271893818196845
iteration : 3485
train acc:  0.6953125
train loss:  0.5441335439682007
train gradient:  0.19833230203236288
iteration : 3486
train acc:  0.6640625
train loss:  0.5622490644454956
train gradient:  0.1703202747173646
iteration : 3487
train acc:  0.671875
train loss:  0.5203641653060913
train gradient:  0.13942902995780576
iteration : 3488
train acc:  0.734375
train loss:  0.5366333723068237
train gradient:  0.17619979668924043
iteration : 3489
train acc:  0.6875
train loss:  0.6307045221328735
train gradient:  0.21067717205750425
iteration : 3490
train acc:  0.6875
train loss:  0.5870068073272705
train gradient:  0.19297427577929982
iteration : 3491
train acc:  0.7109375
train loss:  0.5469245910644531
train gradient:  0.1691643734243684
iteration : 3492
train acc:  0.7578125
train loss:  0.5411518812179565
train gradient:  0.20362648866919186
iteration : 3493
train acc:  0.7109375
train loss:  0.5348641872406006
train gradient:  0.14837687516692422
iteration : 3494
train acc:  0.671875
train loss:  0.5829747319221497
train gradient:  0.15191490542496394
iteration : 3495
train acc:  0.7109375
train loss:  0.5332043170928955
train gradient:  0.18287054059588376
iteration : 3496
train acc:  0.7109375
train loss:  0.5532892942428589
train gradient:  0.16215268667319316
iteration : 3497
train acc:  0.7734375
train loss:  0.47035640478134155
train gradient:  0.12275910900936225
iteration : 3498
train acc:  0.6328125
train loss:  0.5608621835708618
train gradient:  0.13699424507255398
iteration : 3499
train acc:  0.75
train loss:  0.5128808617591858
train gradient:  0.1406186134871152
iteration : 3500
train acc:  0.703125
train loss:  0.5963167548179626
train gradient:  0.16301096131331289
iteration : 3501
train acc:  0.703125
train loss:  0.5467379093170166
train gradient:  0.19057993822553249
iteration : 3502
train acc:  0.703125
train loss:  0.5907611846923828
train gradient:  0.3591235935427479
iteration : 3503
train acc:  0.75
train loss:  0.49955999851226807
train gradient:  0.15443196729211892
iteration : 3504
train acc:  0.7265625
train loss:  0.5633853077888489
train gradient:  0.16342572500284652
iteration : 3505
train acc:  0.7265625
train loss:  0.48366034030914307
train gradient:  0.1484331366676866
iteration : 3506
train acc:  0.703125
train loss:  0.5471243858337402
train gradient:  0.14033071777908915
iteration : 3507
train acc:  0.7109375
train loss:  0.5687441825866699
train gradient:  0.31885365224379725
iteration : 3508
train acc:  0.7890625
train loss:  0.47508615255355835
train gradient:  0.1203576504309161
iteration : 3509
train acc:  0.703125
train loss:  0.5261074304580688
train gradient:  0.15069741087673727
iteration : 3510
train acc:  0.765625
train loss:  0.49784374237060547
train gradient:  0.16949302997329907
iteration : 3511
train acc:  0.671875
train loss:  0.573565661907196
train gradient:  0.157015087668825
iteration : 3512
train acc:  0.734375
train loss:  0.5248641967773438
train gradient:  0.1424556314828278
iteration : 3513
train acc:  0.765625
train loss:  0.45788541436195374
train gradient:  0.11056570629405085
iteration : 3514
train acc:  0.6171875
train loss:  0.5976965427398682
train gradient:  0.22350065204399389
iteration : 3515
train acc:  0.6953125
train loss:  0.5596837997436523
train gradient:  0.16048009559708293
iteration : 3516
train acc:  0.7265625
train loss:  0.5172115564346313
train gradient:  0.1580948195026572
iteration : 3517
train acc:  0.75
train loss:  0.5057616233825684
train gradient:  0.15136158400548982
iteration : 3518
train acc:  0.703125
train loss:  0.5586398243904114
train gradient:  0.14697137463935336
iteration : 3519
train acc:  0.7734375
train loss:  0.48720377683639526
train gradient:  0.15250687601729507
iteration : 3520
train acc:  0.6953125
train loss:  0.5878528356552124
train gradient:  0.1832232147169982
iteration : 3521
train acc:  0.765625
train loss:  0.4772658348083496
train gradient:  0.11490147614152021
iteration : 3522
train acc:  0.6796875
train loss:  0.541807234287262
train gradient:  0.16254719427158648
iteration : 3523
train acc:  0.671875
train loss:  0.5778089761734009
train gradient:  0.17035841708977426
iteration : 3524
train acc:  0.7265625
train loss:  0.5403800010681152
train gradient:  0.2256981411944412
iteration : 3525
train acc:  0.7421875
train loss:  0.4941556453704834
train gradient:  0.12557710279552786
iteration : 3526
train acc:  0.7578125
train loss:  0.5118374228477478
train gradient:  0.13812972091333414
iteration : 3527
train acc:  0.6953125
train loss:  0.5543174743652344
train gradient:  0.16109603687470841
iteration : 3528
train acc:  0.6328125
train loss:  0.6238139867782593
train gradient:  0.1943844988439355
iteration : 3529
train acc:  0.75
train loss:  0.4869571328163147
train gradient:  0.15023288775777688
iteration : 3530
train acc:  0.7265625
train loss:  0.5018178224563599
train gradient:  0.1467749865226406
iteration : 3531
train acc:  0.6953125
train loss:  0.5997785329818726
train gradient:  0.24152031965323628
iteration : 3532
train acc:  0.7421875
train loss:  0.5145513415336609
train gradient:  0.15621005988656092
iteration : 3533
train acc:  0.6953125
train loss:  0.5644310712814331
train gradient:  0.16230831239636628
iteration : 3534
train acc:  0.65625
train loss:  0.5899785161018372
train gradient:  0.16950539772715856
iteration : 3535
train acc:  0.609375
train loss:  0.6257920265197754
train gradient:  0.22752502507940353
iteration : 3536
train acc:  0.6953125
train loss:  0.5649510622024536
train gradient:  0.2676281491883231
iteration : 3537
train acc:  0.7578125
train loss:  0.5073227882385254
train gradient:  0.10686526732413135
iteration : 3538
train acc:  0.7578125
train loss:  0.5049107670783997
train gradient:  0.1658495996594094
iteration : 3539
train acc:  0.703125
train loss:  0.5726907253265381
train gradient:  0.19031585739616177
iteration : 3540
train acc:  0.6953125
train loss:  0.5332329869270325
train gradient:  0.1417294041224234
iteration : 3541
train acc:  0.734375
train loss:  0.5253307819366455
train gradient:  0.15932929647711763
iteration : 3542
train acc:  0.7578125
train loss:  0.5017710328102112
train gradient:  0.15452938488044793
iteration : 3543
train acc:  0.7578125
train loss:  0.5143130421638489
train gradient:  0.24098546927618192
iteration : 3544
train acc:  0.765625
train loss:  0.46631038188934326
train gradient:  0.15067963509606425
iteration : 3545
train acc:  0.7890625
train loss:  0.4473609924316406
train gradient:  0.1597742875847671
iteration : 3546
train acc:  0.7109375
train loss:  0.48912638425827026
train gradient:  0.11751703551658262
iteration : 3547
train acc:  0.8046875
train loss:  0.47568559646606445
train gradient:  0.11478594080760512
iteration : 3548
train acc:  0.75
train loss:  0.501380980014801
train gradient:  0.1896885705575725
iteration : 3549
train acc:  0.6796875
train loss:  0.5725618600845337
train gradient:  0.15798921298795024
iteration : 3550
train acc:  0.7109375
train loss:  0.5373340249061584
train gradient:  0.15618066009587545
iteration : 3551
train acc:  0.6640625
train loss:  0.5680819749832153
train gradient:  0.22339322980193915
iteration : 3552
train acc:  0.7578125
train loss:  0.4851003587245941
train gradient:  0.1401438751077949
iteration : 3553
train acc:  0.65625
train loss:  0.5785385966300964
train gradient:  0.16107108047364752
iteration : 3554
train acc:  0.7734375
train loss:  0.5005351901054382
train gradient:  0.1515071955024156
iteration : 3555
train acc:  0.75
train loss:  0.5114550590515137
train gradient:  0.1562359875295433
iteration : 3556
train acc:  0.703125
train loss:  0.5248361229896545
train gradient:  0.13697664923183805
iteration : 3557
train acc:  0.75
train loss:  0.45288050174713135
train gradient:  0.11557893827496891
iteration : 3558
train acc:  0.6796875
train loss:  0.5427913069725037
train gradient:  0.138147431040264
iteration : 3559
train acc:  0.765625
train loss:  0.5039804577827454
train gradient:  0.12697058873181905
iteration : 3560
train acc:  0.71875
train loss:  0.48144015669822693
train gradient:  0.11826050333984794
iteration : 3561
train acc:  0.765625
train loss:  0.49292418360710144
train gradient:  0.12414989719715208
iteration : 3562
train acc:  0.6796875
train loss:  0.5228829383850098
train gradient:  0.19264671717062887
iteration : 3563
train acc:  0.765625
train loss:  0.5398131608963013
train gradient:  0.1400302537609578
iteration : 3564
train acc:  0.71875
train loss:  0.554500937461853
train gradient:  0.18527634074536514
iteration : 3565
train acc:  0.7578125
train loss:  0.5165148973464966
train gradient:  0.15671575419850448
iteration : 3566
train acc:  0.65625
train loss:  0.5905117988586426
train gradient:  0.15457512891520647
iteration : 3567
train acc:  0.703125
train loss:  0.5304427146911621
train gradient:  0.14746436803679233
iteration : 3568
train acc:  0.6875
train loss:  0.5619772672653198
train gradient:  0.17417983683729665
iteration : 3569
train acc:  0.7734375
train loss:  0.47117263078689575
train gradient:  0.10700128182728218
iteration : 3570
train acc:  0.7734375
train loss:  0.5443581938743591
train gradient:  0.17109306165867771
iteration : 3571
train acc:  0.7265625
train loss:  0.5292303562164307
train gradient:  0.13212923872290408
iteration : 3572
train acc:  0.71875
train loss:  0.510511040687561
train gradient:  0.13096283810865733
iteration : 3573
train acc:  0.734375
train loss:  0.4855453073978424
train gradient:  0.15776774999950408
iteration : 3574
train acc:  0.7578125
train loss:  0.4795769155025482
train gradient:  0.10957131713813914
iteration : 3575
train acc:  0.734375
train loss:  0.5384648442268372
train gradient:  0.16137242236008595
iteration : 3576
train acc:  0.6875
train loss:  0.5625850558280945
train gradient:  0.15842067942664123
iteration : 3577
train acc:  0.6875
train loss:  0.530505895614624
train gradient:  0.14561787407452592
iteration : 3578
train acc:  0.703125
train loss:  0.5851600170135498
train gradient:  0.1749254221582744
iteration : 3579
train acc:  0.6640625
train loss:  0.6349314451217651
train gradient:  0.16869047647126514
iteration : 3580
train acc:  0.7578125
train loss:  0.47471731901168823
train gradient:  0.15353120648062063
iteration : 3581
train acc:  0.671875
train loss:  0.5683625936508179
train gradient:  0.1977933310258405
iteration : 3582
train acc:  0.7578125
train loss:  0.5072469711303711
train gradient:  0.19540333945273192
iteration : 3583
train acc:  0.7265625
train loss:  0.49830740690231323
train gradient:  0.23472928379742586
iteration : 3584
train acc:  0.7265625
train loss:  0.5233374834060669
train gradient:  0.18585215696236057
iteration : 3585
train acc:  0.7265625
train loss:  0.48123231530189514
train gradient:  0.13879855288948717
iteration : 3586
train acc:  0.7265625
train loss:  0.5295335054397583
train gradient:  0.13969347113226765
iteration : 3587
train acc:  0.6796875
train loss:  0.5336463451385498
train gradient:  0.23133372273636532
iteration : 3588
train acc:  0.6953125
train loss:  0.5683403611183167
train gradient:  0.15862435350722898
iteration : 3589
train acc:  0.7734375
train loss:  0.4940466284751892
train gradient:  0.1457021804810521
iteration : 3590
train acc:  0.75
train loss:  0.4646352529525757
train gradient:  0.14745564810138745
iteration : 3591
train acc:  0.7890625
train loss:  0.4607427716255188
train gradient:  0.11725189884499947
iteration : 3592
train acc:  0.703125
train loss:  0.5129498243331909
train gradient:  0.1475897609527954
iteration : 3593
train acc:  0.7734375
train loss:  0.4653083384037018
train gradient:  0.18763372568163997
iteration : 3594
train acc:  0.6953125
train loss:  0.5364247560501099
train gradient:  0.13445007820980504
iteration : 3595
train acc:  0.7265625
train loss:  0.534608781337738
train gradient:  0.14751982634027427
iteration : 3596
train acc:  0.671875
train loss:  0.5478423833847046
train gradient:  0.17391757338352073
iteration : 3597
train acc:  0.765625
train loss:  0.494651734828949
train gradient:  0.16554150674995127
iteration : 3598
train acc:  0.6953125
train loss:  0.5551806092262268
train gradient:  0.18355982375166227
iteration : 3599
train acc:  0.7109375
train loss:  0.5521146059036255
train gradient:  0.1560543059550397
iteration : 3600
train acc:  0.7265625
train loss:  0.5794146060943604
train gradient:  0.14333721736170874
iteration : 3601
train acc:  0.6640625
train loss:  0.5932278633117676
train gradient:  0.2205415379382981
iteration : 3602
train acc:  0.796875
train loss:  0.41671043634414673
train gradient:  0.11353659279493751
iteration : 3603
train acc:  0.75
train loss:  0.4830242991447449
train gradient:  0.1308545003241032
iteration : 3604
train acc:  0.6328125
train loss:  0.6545729637145996
train gradient:  0.2687308400647953
iteration : 3605
train acc:  0.7734375
train loss:  0.46882501244544983
train gradient:  0.16451409516723003
iteration : 3606
train acc:  0.765625
train loss:  0.5142167806625366
train gradient:  0.17238574228409187
iteration : 3607
train acc:  0.7421875
train loss:  0.492873877286911
train gradient:  0.16531108766666397
iteration : 3608
train acc:  0.7109375
train loss:  0.5678684711456299
train gradient:  0.19851200509298572
iteration : 3609
train acc:  0.6875
train loss:  0.5469952821731567
train gradient:  0.2004314195046012
iteration : 3610
train acc:  0.703125
train loss:  0.563444197177887
train gradient:  0.16065387821960328
iteration : 3611
train acc:  0.703125
train loss:  0.5695213079452515
train gradient:  0.19912766489740152
iteration : 3612
train acc:  0.6640625
train loss:  0.5555863380432129
train gradient:  0.18279526077664288
iteration : 3613
train acc:  0.7109375
train loss:  0.5403591394424438
train gradient:  0.17609732711335058
iteration : 3614
train acc:  0.734375
train loss:  0.5170767903327942
train gradient:  0.24110171752189874
iteration : 3615
train acc:  0.765625
train loss:  0.49694639444351196
train gradient:  0.17321042962697153
iteration : 3616
train acc:  0.703125
train loss:  0.5599071383476257
train gradient:  0.21960056652289933
iteration : 3617
train acc:  0.78125
train loss:  0.461973637342453
train gradient:  0.13548426725072774
iteration : 3618
train acc:  0.7109375
train loss:  0.5075594782829285
train gradient:  0.16369672803572752
iteration : 3619
train acc:  0.734375
train loss:  0.5435044765472412
train gradient:  0.1628446255571151
iteration : 3620
train acc:  0.6875
train loss:  0.5413289070129395
train gradient:  0.1951212757108094
iteration : 3621
train acc:  0.703125
train loss:  0.5432653427124023
train gradient:  0.193059260958198
iteration : 3622
train acc:  0.71875
train loss:  0.48527276515960693
train gradient:  0.1927056837237922
iteration : 3623
train acc:  0.75
train loss:  0.49039849638938904
train gradient:  0.16152614654688918
iteration : 3624
train acc:  0.765625
train loss:  0.508465051651001
train gradient:  0.18742605360767658
iteration : 3625
train acc:  0.78125
train loss:  0.49205079674720764
train gradient:  0.17067995399652036
iteration : 3626
train acc:  0.7109375
train loss:  0.5358584523200989
train gradient:  0.18701314398271868
iteration : 3627
train acc:  0.671875
train loss:  0.5221724510192871
train gradient:  0.1371794317041114
iteration : 3628
train acc:  0.65625
train loss:  0.5481557846069336
train gradient:  0.1909686006353709
iteration : 3629
train acc:  0.75
train loss:  0.4672524333000183
train gradient:  0.1512851354753891
iteration : 3630
train acc:  0.703125
train loss:  0.5184454917907715
train gradient:  0.13081095538140597
iteration : 3631
train acc:  0.7421875
train loss:  0.5353310704231262
train gradient:  0.174952228960142
iteration : 3632
train acc:  0.671875
train loss:  0.5588699579238892
train gradient:  0.17831428401855104
iteration : 3633
train acc:  0.78125
train loss:  0.4692171514034271
train gradient:  0.1530249441131729
iteration : 3634
train acc:  0.734375
train loss:  0.47539371252059937
train gradient:  0.12984875258520406
iteration : 3635
train acc:  0.734375
train loss:  0.5099511742591858
train gradient:  0.1395689080173626
iteration : 3636
train acc:  0.7265625
train loss:  0.5294322967529297
train gradient:  0.14884249752531523
iteration : 3637
train acc:  0.6953125
train loss:  0.5598351955413818
train gradient:  0.19903973275853393
iteration : 3638
train acc:  0.75
train loss:  0.5365004539489746
train gradient:  0.20691065836890377
iteration : 3639
train acc:  0.7890625
train loss:  0.4592062830924988
train gradient:  0.16021712335928834
iteration : 3640
train acc:  0.75
train loss:  0.5238169431686401
train gradient:  0.1420887253793307
iteration : 3641
train acc:  0.6953125
train loss:  0.5709697008132935
train gradient:  0.1647379711167607
iteration : 3642
train acc:  0.7578125
train loss:  0.4975864589214325
train gradient:  0.15186152034500572
iteration : 3643
train acc:  0.7578125
train loss:  0.4473612606525421
train gradient:  0.14498369941453082
iteration : 3644
train acc:  0.578125
train loss:  0.6430378556251526
train gradient:  0.1910701886019467
iteration : 3645
train acc:  0.75
train loss:  0.4797937870025635
train gradient:  0.15164973620615985
iteration : 3646
train acc:  0.71875
train loss:  0.5642648935317993
train gradient:  0.17349068510064802
iteration : 3647
train acc:  0.640625
train loss:  0.6120826005935669
train gradient:  0.22520702471180712
iteration : 3648
train acc:  0.734375
train loss:  0.4741293787956238
train gradient:  0.11098526502259949
iteration : 3649
train acc:  0.7734375
train loss:  0.4903831481933594
train gradient:  0.18556035543315846
iteration : 3650
train acc:  0.6875
train loss:  0.541297972202301
train gradient:  0.1325251821720635
iteration : 3651
train acc:  0.6796875
train loss:  0.5900213718414307
train gradient:  0.20201702819831097
iteration : 3652
train acc:  0.7109375
train loss:  0.5002937316894531
train gradient:  0.14669865950054972
iteration : 3653
train acc:  0.640625
train loss:  0.6086033582687378
train gradient:  0.24083070965825643
iteration : 3654
train acc:  0.75
train loss:  0.4980039596557617
train gradient:  0.17128069620672767
iteration : 3655
train acc:  0.7109375
train loss:  0.5347660779953003
train gradient:  0.1739095708166004
iteration : 3656
train acc:  0.6875
train loss:  0.5974581837654114
train gradient:  0.27124629798371847
iteration : 3657
train acc:  0.6640625
train loss:  0.6301156282424927
train gradient:  0.2050196191813955
iteration : 3658
train acc:  0.7265625
train loss:  0.5177950859069824
train gradient:  0.16425700633181878
iteration : 3659
train acc:  0.703125
train loss:  0.5761244297027588
train gradient:  0.218874064887685
iteration : 3660
train acc:  0.71875
train loss:  0.4997299313545227
train gradient:  0.15054356657001106
iteration : 3661
train acc:  0.75
train loss:  0.47601693868637085
train gradient:  0.12102783670801492
iteration : 3662
train acc:  0.7421875
train loss:  0.46966248750686646
train gradient:  0.17395031544186662
iteration : 3663
train acc:  0.7265625
train loss:  0.5152661204338074
train gradient:  0.12837148784495017
iteration : 3664
train acc:  0.7421875
train loss:  0.5094428062438965
train gradient:  0.1377587548041817
iteration : 3665
train acc:  0.703125
train loss:  0.5299190282821655
train gradient:  0.1455649433925758
iteration : 3666
train acc:  0.7734375
train loss:  0.511690616607666
train gradient:  0.15620465950527923
iteration : 3667
train acc:  0.765625
train loss:  0.489824116230011
train gradient:  0.1447202352307649
iteration : 3668
train acc:  0.640625
train loss:  0.561352550983429
train gradient:  0.14208792515205498
iteration : 3669
train acc:  0.671875
train loss:  0.581161379814148
train gradient:  0.18394023487675737
iteration : 3670
train acc:  0.7578125
train loss:  0.47492095828056335
train gradient:  0.1426309456771605
iteration : 3671
train acc:  0.75
train loss:  0.48342835903167725
train gradient:  0.12537620249286935
iteration : 3672
train acc:  0.7578125
train loss:  0.5452345609664917
train gradient:  0.1949527596672856
iteration : 3673
train acc:  0.7109375
train loss:  0.5450510382652283
train gradient:  0.17219699630600888
iteration : 3674
train acc:  0.71875
train loss:  0.4986146092414856
train gradient:  0.1427308931760272
iteration : 3675
train acc:  0.6953125
train loss:  0.5992505550384521
train gradient:  0.17527366063256095
iteration : 3676
train acc:  0.6640625
train loss:  0.5226583480834961
train gradient:  0.14129359157462545
iteration : 3677
train acc:  0.7421875
train loss:  0.5046151280403137
train gradient:  0.18477251757338128
iteration : 3678
train acc:  0.703125
train loss:  0.529690146446228
train gradient:  0.1735456554575167
iteration : 3679
train acc:  0.7421875
train loss:  0.5367102026939392
train gradient:  0.16388924604175809
iteration : 3680
train acc:  0.734375
train loss:  0.516459584236145
train gradient:  0.14626541414573863
iteration : 3681
train acc:  0.640625
train loss:  0.6262766122817993
train gradient:  0.28315811561642257
iteration : 3682
train acc:  0.671875
train loss:  0.5853672027587891
train gradient:  0.15960872490879086
iteration : 3683
train acc:  0.7578125
train loss:  0.4709722399711609
train gradient:  0.1309251590057331
iteration : 3684
train acc:  0.8046875
train loss:  0.42450371384620667
train gradient:  0.11428005433465444
iteration : 3685
train acc:  0.6640625
train loss:  0.6474885940551758
train gradient:  0.2963277250748221
iteration : 3686
train acc:  0.7265625
train loss:  0.5366488099098206
train gradient:  0.16192343144205043
iteration : 3687
train acc:  0.6953125
train loss:  0.5354017615318298
train gradient:  0.15147996089650867
iteration : 3688
train acc:  0.7265625
train loss:  0.4905012845993042
train gradient:  0.15973359486622113
iteration : 3689
train acc:  0.6796875
train loss:  0.5840981006622314
train gradient:  0.1641954171231169
iteration : 3690
train acc:  0.78125
train loss:  0.498822957277298
train gradient:  0.1715835023001166
iteration : 3691
train acc:  0.7265625
train loss:  0.5224183797836304
train gradient:  0.15712888198557007
iteration : 3692
train acc:  0.6875
train loss:  0.6637994647026062
train gradient:  0.29518165658051
iteration : 3693
train acc:  0.640625
train loss:  0.5694228410720825
train gradient:  0.17430715267549518
iteration : 3694
train acc:  0.765625
train loss:  0.5138116478919983
train gradient:  0.1985345236529597
iteration : 3695
train acc:  0.7265625
train loss:  0.5379313230514526
train gradient:  0.1380203831409913
iteration : 3696
train acc:  0.7421875
train loss:  0.5153018236160278
train gradient:  0.13398591165693619
iteration : 3697
train acc:  0.734375
train loss:  0.4764871299266815
train gradient:  0.12142897301832675
iteration : 3698
train acc:  0.6796875
train loss:  0.571576714515686
train gradient:  0.19090945329013786
iteration : 3699
train acc:  0.71875
train loss:  0.5164694786071777
train gradient:  0.1523540049078305
iteration : 3700
train acc:  0.765625
train loss:  0.5625467896461487
train gradient:  0.16205561763965087
iteration : 3701
train acc:  0.7734375
train loss:  0.5044615864753723
train gradient:  0.13138882419161574
iteration : 3702
train acc:  0.7265625
train loss:  0.5322233438491821
train gradient:  0.1323997930113529
iteration : 3703
train acc:  0.640625
train loss:  0.5659778118133545
train gradient:  0.16980239960591686
iteration : 3704
train acc:  0.6875
train loss:  0.562757670879364
train gradient:  0.1484198752001209
iteration : 3705
train acc:  0.7421875
train loss:  0.5063730478286743
train gradient:  0.1486729380411214
iteration : 3706
train acc:  0.703125
train loss:  0.599743664264679
train gradient:  0.21273004824629882
iteration : 3707
train acc:  0.7734375
train loss:  0.4768106937408447
train gradient:  0.19284575708822527
iteration : 3708
train acc:  0.765625
train loss:  0.4855119585990906
train gradient:  0.15427326832141508
iteration : 3709
train acc:  0.6953125
train loss:  0.5667791366577148
train gradient:  0.18078655551019457
iteration : 3710
train acc:  0.796875
train loss:  0.46401071548461914
train gradient:  0.16565992526797912
iteration : 3711
train acc:  0.7109375
train loss:  0.5158721804618835
train gradient:  0.14440976147811574
iteration : 3712
train acc:  0.71875
train loss:  0.512847900390625
train gradient:  0.1380945533414333
iteration : 3713
train acc:  0.6953125
train loss:  0.5192112922668457
train gradient:  0.15915629153437783
iteration : 3714
train acc:  0.734375
train loss:  0.5093590021133423
train gradient:  0.1816242074923084
iteration : 3715
train acc:  0.671875
train loss:  0.636964738368988
train gradient:  0.2002148004712069
iteration : 3716
train acc:  0.734375
train loss:  0.5007627010345459
train gradient:  0.12837832286002865
iteration : 3717
train acc:  0.703125
train loss:  0.5652719736099243
train gradient:  0.2377312968337756
iteration : 3718
train acc:  0.7421875
train loss:  0.5343390107154846
train gradient:  0.17129274662037278
iteration : 3719
train acc:  0.7578125
train loss:  0.520097017288208
train gradient:  0.15738284746156006
iteration : 3720
train acc:  0.7421875
train loss:  0.5410290956497192
train gradient:  0.20498802319745973
iteration : 3721
train acc:  0.7578125
train loss:  0.5190150141716003
train gradient:  0.19131755627694708
iteration : 3722
train acc:  0.7265625
train loss:  0.5016821622848511
train gradient:  0.14184555737733828
iteration : 3723
train acc:  0.734375
train loss:  0.5170348882675171
train gradient:  0.14302573419758507
iteration : 3724
train acc:  0.7421875
train loss:  0.5827059745788574
train gradient:  0.20305053176584037
iteration : 3725
train acc:  0.765625
train loss:  0.5119547843933105
train gradient:  0.1369179614449298
iteration : 3726
train acc:  0.703125
train loss:  0.5767583250999451
train gradient:  0.19087637093781057
iteration : 3727
train acc:  0.6640625
train loss:  0.5906921625137329
train gradient:  0.20799610765225052
iteration : 3728
train acc:  0.7109375
train loss:  0.5867263078689575
train gradient:  0.16534297200218653
iteration : 3729
train acc:  0.734375
train loss:  0.5295352935791016
train gradient:  0.12026167172418865
iteration : 3730
train acc:  0.6875
train loss:  0.568520188331604
train gradient:  0.20665844906150155
iteration : 3731
train acc:  0.7578125
train loss:  0.553682267665863
train gradient:  0.19727046007550264
iteration : 3732
train acc:  0.7734375
train loss:  0.46752041578292847
train gradient:  0.15828982440727157
iteration : 3733
train acc:  0.7265625
train loss:  0.5051573514938354
train gradient:  0.15206025805231135
iteration : 3734
train acc:  0.7265625
train loss:  0.545047402381897
train gradient:  0.1917617843506323
iteration : 3735
train acc:  0.7421875
train loss:  0.5105209350585938
train gradient:  0.13611725260264285
iteration : 3736
train acc:  0.6875
train loss:  0.6051710844039917
train gradient:  0.20127432371648257
iteration : 3737
train acc:  0.6796875
train loss:  0.5741101503372192
train gradient:  0.166648847906458
iteration : 3738
train acc:  0.7578125
train loss:  0.49453145265579224
train gradient:  0.12461773751766021
iteration : 3739
train acc:  0.734375
train loss:  0.5041723847389221
train gradient:  0.1368306760004906
iteration : 3740
train acc:  0.7109375
train loss:  0.5177896618843079
train gradient:  0.1984937057087072
iteration : 3741
train acc:  0.8046875
train loss:  0.46005457639694214
train gradient:  0.1388250405029146
iteration : 3742
train acc:  0.765625
train loss:  0.509373664855957
train gradient:  0.19421937038690762
iteration : 3743
train acc:  0.7265625
train loss:  0.4865962862968445
train gradient:  0.1753609076852043
iteration : 3744
train acc:  0.7421875
train loss:  0.48796263337135315
train gradient:  0.1455869682272234
iteration : 3745
train acc:  0.703125
train loss:  0.5156533718109131
train gradient:  0.13252826790992717
iteration : 3746
train acc:  0.7890625
train loss:  0.4508771300315857
train gradient:  0.11727506240189971
iteration : 3747
train acc:  0.703125
train loss:  0.5851658582687378
train gradient:  0.1595271529049866
iteration : 3748
train acc:  0.7734375
train loss:  0.44583413004875183
train gradient:  0.12049087320664342
iteration : 3749
train acc:  0.7421875
train loss:  0.5604935884475708
train gradient:  0.16319595310058946
iteration : 3750
train acc:  0.703125
train loss:  0.5489810705184937
train gradient:  0.14730330255317312
iteration : 3751
train acc:  0.6953125
train loss:  0.5457208752632141
train gradient:  0.2317567645491755
iteration : 3752
train acc:  0.765625
train loss:  0.550098180770874
train gradient:  0.17159666446419158
iteration : 3753
train acc:  0.6953125
train loss:  0.5213408470153809
train gradient:  0.1816343356782156
iteration : 3754
train acc:  0.6796875
train loss:  0.612012505531311
train gradient:  0.18943203510409612
iteration : 3755
train acc:  0.7421875
train loss:  0.5099872350692749
train gradient:  0.1508973090089708
iteration : 3756
train acc:  0.6953125
train loss:  0.5750490427017212
train gradient:  0.14300776324886574
iteration : 3757
train acc:  0.6484375
train loss:  0.6172953844070435
train gradient:  0.20467430259090091
iteration : 3758
train acc:  0.6796875
train loss:  0.6106151938438416
train gradient:  0.19772271353498883
iteration : 3759
train acc:  0.671875
train loss:  0.573453426361084
train gradient:  0.25928633779146554
iteration : 3760
train acc:  0.703125
train loss:  0.542681097984314
train gradient:  0.19781661203276185
iteration : 3761
train acc:  0.734375
train loss:  0.5161262154579163
train gradient:  0.14597750106525287
iteration : 3762
train acc:  0.8125
train loss:  0.4669438600540161
train gradient:  0.14698038136713543
iteration : 3763
train acc:  0.7421875
train loss:  0.5158016085624695
train gradient:  0.13782659408869158
iteration : 3764
train acc:  0.7109375
train loss:  0.5354024767875671
train gradient:  0.13836866821440752
iteration : 3765
train acc:  0.7578125
train loss:  0.5004394054412842
train gradient:  0.13573380786433178
iteration : 3766
train acc:  0.671875
train loss:  0.6083517074584961
train gradient:  0.2518311555297011
iteration : 3767
train acc:  0.6796875
train loss:  0.5488601922988892
train gradient:  0.14396914917277315
iteration : 3768
train acc:  0.765625
train loss:  0.4812811613082886
train gradient:  0.12481035588160777
iteration : 3769
train acc:  0.7421875
train loss:  0.5536038875579834
train gradient:  0.1781018385173151
iteration : 3770
train acc:  0.75
train loss:  0.4990302324295044
train gradient:  0.12996780533740915
iteration : 3771
train acc:  0.75
train loss:  0.5163743495941162
train gradient:  0.21142092295169157
iteration : 3772
train acc:  0.7265625
train loss:  0.5173079967498779
train gradient:  0.12870053915295518
iteration : 3773
train acc:  0.6953125
train loss:  0.5001280903816223
train gradient:  0.18120628389917967
iteration : 3774
train acc:  0.765625
train loss:  0.49656379222869873
train gradient:  0.1772899265746865
iteration : 3775
train acc:  0.71875
train loss:  0.47551149129867554
train gradient:  0.11302780086980835
iteration : 3776
train acc:  0.7109375
train loss:  0.5325450897216797
train gradient:  0.14954164097867753
iteration : 3777
train acc:  0.6875
train loss:  0.5485706329345703
train gradient:  0.17386279429673185
iteration : 3778
train acc:  0.75
train loss:  0.49713966250419617
train gradient:  0.1619330902385358
iteration : 3779
train acc:  0.765625
train loss:  0.49727416038513184
train gradient:  0.12357893659762474
iteration : 3780
train acc:  0.71875
train loss:  0.4935906231403351
train gradient:  0.1614625768108674
iteration : 3781
train acc:  0.6796875
train loss:  0.6016548871994019
train gradient:  0.18692009392390696
iteration : 3782
train acc:  0.6484375
train loss:  0.5832937955856323
train gradient:  0.22538455667517243
iteration : 3783
train acc:  0.7421875
train loss:  0.5127940773963928
train gradient:  0.15203755890553583
iteration : 3784
train acc:  0.71875
train loss:  0.5625196695327759
train gradient:  0.2119590137459995
iteration : 3785
train acc:  0.703125
train loss:  0.5171689391136169
train gradient:  0.1288809591205016
iteration : 3786
train acc:  0.703125
train loss:  0.5164453387260437
train gradient:  0.16005220565061792
iteration : 3787
train acc:  0.703125
train loss:  0.5318527221679688
train gradient:  0.14261178350924214
iteration : 3788
train acc:  0.765625
train loss:  0.5172320008277893
train gradient:  0.12433253072270763
iteration : 3789
train acc:  0.7265625
train loss:  0.5200766324996948
train gradient:  0.21207535635323127
iteration : 3790
train acc:  0.71875
train loss:  0.5339693427085876
train gradient:  0.20542980249616932
iteration : 3791
train acc:  0.7265625
train loss:  0.5399345755577087
train gradient:  0.14897420881145923
iteration : 3792
train acc:  0.6796875
train loss:  0.5561555624008179
train gradient:  0.17285771230694957
iteration : 3793
train acc:  0.7578125
train loss:  0.5183897018432617
train gradient:  0.19318375589911907
iteration : 3794
train acc:  0.6953125
train loss:  0.5222514867782593
train gradient:  0.15259466486631035
iteration : 3795
train acc:  0.7421875
train loss:  0.4939291477203369
train gradient:  0.15208811563244656
iteration : 3796
train acc:  0.7578125
train loss:  0.5295129418373108
train gradient:  0.17405732305695282
iteration : 3797
train acc:  0.65625
train loss:  0.5797744989395142
train gradient:  0.17913310114109515
iteration : 3798
train acc:  0.71875
train loss:  0.5225829482078552
train gradient:  0.2527920210684517
iteration : 3799
train acc:  0.7109375
train loss:  0.5573558211326599
train gradient:  0.15237200124694067
iteration : 3800
train acc:  0.671875
train loss:  0.5715845823287964
train gradient:  0.19312119170271716
iteration : 3801
train acc:  0.703125
train loss:  0.519426703453064
train gradient:  0.13046385186387
iteration : 3802
train acc:  0.78125
train loss:  0.43110036849975586
train gradient:  0.11384041277641833
iteration : 3803
train acc:  0.6875
train loss:  0.5626299977302551
train gradient:  0.17951527396866868
iteration : 3804
train acc:  0.78125
train loss:  0.48561638593673706
train gradient:  0.144571154796462
iteration : 3805
train acc:  0.6953125
train loss:  0.5395128726959229
train gradient:  0.15096310552520642
iteration : 3806
train acc:  0.7421875
train loss:  0.4937719404697418
train gradient:  0.11903850987726126
iteration : 3807
train acc:  0.75
train loss:  0.5294475555419922
train gradient:  0.1727613256547881
iteration : 3808
train acc:  0.6640625
train loss:  0.5593928098678589
train gradient:  0.17112411500570135
iteration : 3809
train acc:  0.734375
train loss:  0.5323896408081055
train gradient:  0.1824722096166236
iteration : 3810
train acc:  0.6796875
train loss:  0.5558046698570251
train gradient:  0.21690244378082763
iteration : 3811
train acc:  0.796875
train loss:  0.4331422448158264
train gradient:  0.14891237609051133
iteration : 3812
train acc:  0.6875
train loss:  0.6239285469055176
train gradient:  0.2572174529753733
iteration : 3813
train acc:  0.796875
train loss:  0.4839877486228943
train gradient:  0.1265207056928251
iteration : 3814
train acc:  0.703125
train loss:  0.5652555823326111
train gradient:  0.16933519762783228
iteration : 3815
train acc:  0.6796875
train loss:  0.5897454023361206
train gradient:  0.21783089529459293
iteration : 3816
train acc:  0.734375
train loss:  0.5432043075561523
train gradient:  0.20735281623000934
iteration : 3817
train acc:  0.7578125
train loss:  0.5013765096664429
train gradient:  0.14572538520467618
iteration : 3818
train acc:  0.71875
train loss:  0.502875804901123
train gradient:  0.1436402844098211
iteration : 3819
train acc:  0.75
train loss:  0.49971431493759155
train gradient:  0.13434972656764152
iteration : 3820
train acc:  0.6796875
train loss:  0.5311640501022339
train gradient:  0.15176558339234594
iteration : 3821
train acc:  0.7109375
train loss:  0.5243250131607056
train gradient:  0.14973598135648084
iteration : 3822
train acc:  0.6796875
train loss:  0.5249074697494507
train gradient:  0.149430490112541
iteration : 3823
train acc:  0.6953125
train loss:  0.5398304462432861
train gradient:  0.1873817365449189
iteration : 3824
train acc:  0.75
train loss:  0.48076683282852173
train gradient:  0.09987124650812178
iteration : 3825
train acc:  0.6953125
train loss:  0.5974048376083374
train gradient:  0.16144543882780663
iteration : 3826
train acc:  0.7421875
train loss:  0.5159670114517212
train gradient:  0.14591201801537107
iteration : 3827
train acc:  0.6875
train loss:  0.5361971855163574
train gradient:  0.15632172862469096
iteration : 3828
train acc:  0.6640625
train loss:  0.5284427404403687
train gradient:  0.14877924786195457
iteration : 3829
train acc:  0.71875
train loss:  0.4985928237438202
train gradient:  0.12896334466378595
iteration : 3830
train acc:  0.6640625
train loss:  0.5570788383483887
train gradient:  0.149336089666117
iteration : 3831
train acc:  0.7265625
train loss:  0.5114138126373291
train gradient:  0.1411814861481166
iteration : 3832
train acc:  0.765625
train loss:  0.5271649360656738
train gradient:  0.15189493854494157
iteration : 3833
train acc:  0.7109375
train loss:  0.5703584551811218
train gradient:  0.2015113433386471
iteration : 3834
train acc:  0.7109375
train loss:  0.5296117067337036
train gradient:  0.13361674386009903
iteration : 3835
train acc:  0.71875
train loss:  0.5489640235900879
train gradient:  0.18837839996011568
iteration : 3836
train acc:  0.71875
train loss:  0.5316133499145508
train gradient:  0.1786253901637344
iteration : 3837
train acc:  0.7734375
train loss:  0.5531198978424072
train gradient:  0.15562449824499391
iteration : 3838
train acc:  0.6875
train loss:  0.5280987620353699
train gradient:  0.130297066357454
iteration : 3839
train acc:  0.6484375
train loss:  0.5935715436935425
train gradient:  0.2207015837074116
iteration : 3840
train acc:  0.7421875
train loss:  0.5192599296569824
train gradient:  0.17099453753439425
iteration : 3841
train acc:  0.6953125
train loss:  0.5640345811843872
train gradient:  0.1587433496742937
iteration : 3842
train acc:  0.640625
train loss:  0.557391345500946
train gradient:  0.2046576952389783
iteration : 3843
train acc:  0.7265625
train loss:  0.5313453078269958
train gradient:  0.19755022038035153
iteration : 3844
train acc:  0.71875
train loss:  0.5404883623123169
train gradient:  0.16189697412224868
iteration : 3845
train acc:  0.7265625
train loss:  0.5396034717559814
train gradient:  0.15312862323751986
iteration : 3846
train acc:  0.7265625
train loss:  0.48114654421806335
train gradient:  0.11919097639142821
iteration : 3847
train acc:  0.6796875
train loss:  0.6152358651161194
train gradient:  0.184850764866698
iteration : 3848
train acc:  0.71875
train loss:  0.5433903932571411
train gradient:  0.13955583025392185
iteration : 3849
train acc:  0.6875
train loss:  0.5023127198219299
train gradient:  0.1296111855210904
iteration : 3850
train acc:  0.734375
train loss:  0.4628704786300659
train gradient:  0.12398807457585359
iteration : 3851
train acc:  0.703125
train loss:  0.5708043575286865
train gradient:  0.19090623742504795
iteration : 3852
train acc:  0.703125
train loss:  0.5557036399841309
train gradient:  0.1816513687629298
iteration : 3853
train acc:  0.7265625
train loss:  0.5197680592536926
train gradient:  0.1368176933032936
iteration : 3854
train acc:  0.7421875
train loss:  0.4929976165294647
train gradient:  0.15124159091848483
iteration : 3855
train acc:  0.71875
train loss:  0.5392723679542542
train gradient:  0.17094675734827297
iteration : 3856
train acc:  0.7890625
train loss:  0.4814615249633789
train gradient:  0.16761160096671118
iteration : 3857
train acc:  0.7265625
train loss:  0.5259894132614136
train gradient:  0.13299634921130923
iteration : 3858
train acc:  0.7578125
train loss:  0.5166348814964294
train gradient:  0.13977748304524712
iteration : 3859
train acc:  0.765625
train loss:  0.4908497929573059
train gradient:  0.14739067672923198
iteration : 3860
train acc:  0.671875
train loss:  0.5434567332267761
train gradient:  0.16203623653799418
iteration : 3861
train acc:  0.78125
train loss:  0.4481421411037445
train gradient:  0.11930530024106714
iteration : 3862
train acc:  0.71875
train loss:  0.5107592344284058
train gradient:  0.11533584694278688
iteration : 3863
train acc:  0.734375
train loss:  0.4619952440261841
train gradient:  0.1596861582883644
iteration : 3864
train acc:  0.71875
train loss:  0.5595136880874634
train gradient:  0.151231834587893
iteration : 3865
train acc:  0.7109375
train loss:  0.5007580518722534
train gradient:  0.12734990251932576
iteration : 3866
train acc:  0.734375
train loss:  0.5986559391021729
train gradient:  0.26182646696525896
iteration : 3867
train acc:  0.7109375
train loss:  0.5366071462631226
train gradient:  0.15025826763294114
iteration : 3868
train acc:  0.71875
train loss:  0.5045892000198364
train gradient:  0.13706772812330403
iteration : 3869
train acc:  0.6875
train loss:  0.576085090637207
train gradient:  0.20122959215196148
iteration : 3870
train acc:  0.6875
train loss:  0.5410612225532532
train gradient:  0.15317459662574534
iteration : 3871
train acc:  0.703125
train loss:  0.5333609580993652
train gradient:  0.14732765881101656
iteration : 3872
train acc:  0.6796875
train loss:  0.6156954169273376
train gradient:  0.23825088286747415
iteration : 3873
train acc:  0.7734375
train loss:  0.47329968214035034
train gradient:  0.13677788503720523
iteration : 3874
train acc:  0.8125
train loss:  0.4697882831096649
train gradient:  0.14725063303871105
iteration : 3875
train acc:  0.6953125
train loss:  0.5367350578308105
train gradient:  0.19953416895569076
iteration : 3876
train acc:  0.8046875
train loss:  0.4719451665878296
train gradient:  0.16213584713952178
iteration : 3877
train acc:  0.75
train loss:  0.5115346312522888
train gradient:  0.1785507493451544
iteration : 3878
train acc:  0.734375
train loss:  0.5135424137115479
train gradient:  0.1391166668385763
iteration : 3879
train acc:  0.7421875
train loss:  0.49340495467185974
train gradient:  0.16279585709371247
iteration : 3880
train acc:  0.7265625
train loss:  0.5583730936050415
train gradient:  0.12960301097327237
iteration : 3881
train acc:  0.6953125
train loss:  0.5544912815093994
train gradient:  0.17611510004112535
iteration : 3882
train acc:  0.765625
train loss:  0.5012943744659424
train gradient:  0.11607970925560684
iteration : 3883
train acc:  0.7421875
train loss:  0.48812565207481384
train gradient:  0.13957082603785004
iteration : 3884
train acc:  0.7734375
train loss:  0.46595990657806396
train gradient:  0.09939357273511297
iteration : 3885
train acc:  0.7265625
train loss:  0.5405992865562439
train gradient:  0.1351048733180949
iteration : 3886
train acc:  0.6328125
train loss:  0.5929836630821228
train gradient:  0.22626388902575761
iteration : 3887
train acc:  0.7578125
train loss:  0.5490362048149109
train gradient:  0.1839551164802197
iteration : 3888
train acc:  0.7109375
train loss:  0.5455564260482788
train gradient:  0.1628789063455463
iteration : 3889
train acc:  0.7734375
train loss:  0.5118373036384583
train gradient:  0.12815548294816453
iteration : 3890
train acc:  0.71875
train loss:  0.4952780306339264
train gradient:  0.14544152678012368
iteration : 3891
train acc:  0.71875
train loss:  0.5168914794921875
train gradient:  0.1322154214458124
iteration : 3892
train acc:  0.65625
train loss:  0.6307145357131958
train gradient:  0.2159976972532558
iteration : 3893
train acc:  0.703125
train loss:  0.5389412641525269
train gradient:  0.23855259962212955
iteration : 3894
train acc:  0.71875
train loss:  0.48833999037742615
train gradient:  0.1693354442229265
iteration : 3895
train acc:  0.7109375
train loss:  0.5387307405471802
train gradient:  0.13748310326615754
iteration : 3896
train acc:  0.765625
train loss:  0.48676759004592896
train gradient:  0.13173887975502696
iteration : 3897
train acc:  0.8046875
train loss:  0.46815237402915955
train gradient:  0.13339769935965368
iteration : 3898
train acc:  0.6484375
train loss:  0.5758354663848877
train gradient:  0.17850649354066095
iteration : 3899
train acc:  0.671875
train loss:  0.593221127986908
train gradient:  0.18412079206492354
iteration : 3900
train acc:  0.7109375
train loss:  0.5535190105438232
train gradient:  0.15873848832855358
iteration : 3901
train acc:  0.71875
train loss:  0.49337324500083923
train gradient:  0.1499956769092366
iteration : 3902
train acc:  0.796875
train loss:  0.45249152183532715
train gradient:  0.1884529762211545
iteration : 3903
train acc:  0.734375
train loss:  0.5049127340316772
train gradient:  0.2131273231858044
iteration : 3904
train acc:  0.7421875
train loss:  0.5401535034179688
train gradient:  0.14151970186832444
iteration : 3905
train acc:  0.71875
train loss:  0.5416423678398132
train gradient:  0.15904442815714076
iteration : 3906
train acc:  0.6953125
train loss:  0.5400118827819824
train gradient:  0.1529587381673712
iteration : 3907
train acc:  0.796875
train loss:  0.4928189218044281
train gradient:  0.20058704697471913
iteration : 3908
train acc:  0.7578125
train loss:  0.518064022064209
train gradient:  0.14551812535858355
iteration : 3909
train acc:  0.796875
train loss:  0.5078265070915222
train gradient:  0.13333340080904216
iteration : 3910
train acc:  0.7265625
train loss:  0.5135976076126099
train gradient:  0.17676258658683694
iteration : 3911
train acc:  0.6875
train loss:  0.5076315999031067
train gradient:  0.18772877050816916
iteration : 3912
train acc:  0.703125
train loss:  0.5763964056968689
train gradient:  0.2141185081138387
iteration : 3913
train acc:  0.7265625
train loss:  0.5442545413970947
train gradient:  0.1251498282767912
iteration : 3914
train acc:  0.75
train loss:  0.4424331784248352
train gradient:  0.12549035595536592
iteration : 3915
train acc:  0.78125
train loss:  0.49052754044532776
train gradient:  0.14471920132682184
iteration : 3916
train acc:  0.71875
train loss:  0.49260765314102173
train gradient:  0.1318459393746642
iteration : 3917
train acc:  0.8046875
train loss:  0.45901548862457275
train gradient:  0.1084421976035383
iteration : 3918
train acc:  0.7890625
train loss:  0.4909743666648865
train gradient:  0.128845071990548
iteration : 3919
train acc:  0.6875
train loss:  0.5491687059402466
train gradient:  0.13044446526184042
iteration : 3920
train acc:  0.703125
train loss:  0.5663715600967407
train gradient:  0.17538290265265774
iteration : 3921
train acc:  0.7265625
train loss:  0.4923005700111389
train gradient:  0.1746871391011151
iteration : 3922
train acc:  0.75
train loss:  0.5174909830093384
train gradient:  0.13817860533181633
iteration : 3923
train acc:  0.7421875
train loss:  0.5123978853225708
train gradient:  0.23537850439011843
iteration : 3924
train acc:  0.7421875
train loss:  0.5264672040939331
train gradient:  0.19646190011928272
iteration : 3925
train acc:  0.71875
train loss:  0.5327551364898682
train gradient:  0.1587782884061945
iteration : 3926
train acc:  0.7265625
train loss:  0.4999205470085144
train gradient:  0.13916000374208554
iteration : 3927
train acc:  0.640625
train loss:  0.5364973545074463
train gradient:  0.1658323079189338
iteration : 3928
train acc:  0.734375
train loss:  0.4888518452644348
train gradient:  0.1567589142511554
iteration : 3929
train acc:  0.8203125
train loss:  0.41873669624328613
train gradient:  0.12568562487498874
iteration : 3930
train acc:  0.6796875
train loss:  0.5601277947425842
train gradient:  0.18883349621525813
iteration : 3931
train acc:  0.7265625
train loss:  0.5538760423660278
train gradient:  0.18132302453225774
iteration : 3932
train acc:  0.7109375
train loss:  0.5291690230369568
train gradient:  0.14484441280574212
iteration : 3933
train acc:  0.6953125
train loss:  0.5175504684448242
train gradient:  0.16952894676077182
iteration : 3934
train acc:  0.7578125
train loss:  0.4719341993331909
train gradient:  0.1250390003537487
iteration : 3935
train acc:  0.6796875
train loss:  0.5035772323608398
train gradient:  0.1332908369029159
iteration : 3936
train acc:  0.71875
train loss:  0.511674165725708
train gradient:  0.1502317894562796
iteration : 3937
train acc:  0.765625
train loss:  0.4937055706977844
train gradient:  0.14224171561885862
iteration : 3938
train acc:  0.734375
train loss:  0.49465298652648926
train gradient:  0.13876543984262152
iteration : 3939
train acc:  0.78125
train loss:  0.5497985482215881
train gradient:  0.23588491895260363
iteration : 3940
train acc:  0.734375
train loss:  0.5165209770202637
train gradient:  0.13306155093614708
iteration : 3941
train acc:  0.7578125
train loss:  0.4989715814590454
train gradient:  0.11405620024722231
iteration : 3942
train acc:  0.7109375
train loss:  0.5118761658668518
train gradient:  0.155601244493578
iteration : 3943
train acc:  0.7578125
train loss:  0.5063257217407227
train gradient:  0.17763558980622904
iteration : 3944
train acc:  0.71875
train loss:  0.5116229057312012
train gradient:  0.14781860724926388
iteration : 3945
train acc:  0.671875
train loss:  0.5632687211036682
train gradient:  0.18031444655084472
iteration : 3946
train acc:  0.71875
train loss:  0.557705283164978
train gradient:  0.1760006844054561
iteration : 3947
train acc:  0.7578125
train loss:  0.5038450360298157
train gradient:  0.15954884324848587
iteration : 3948
train acc:  0.71875
train loss:  0.5476270318031311
train gradient:  0.22316654149153342
iteration : 3949
train acc:  0.7890625
train loss:  0.5001023411750793
train gradient:  0.1520374841324704
iteration : 3950
train acc:  0.6796875
train loss:  0.5409311056137085
train gradient:  0.17028699009973525
iteration : 3951
train acc:  0.7265625
train loss:  0.5140928030014038
train gradient:  0.14740327553945248
iteration : 3952
train acc:  0.7421875
train loss:  0.49551576375961304
train gradient:  0.14302686652966115
iteration : 3953
train acc:  0.71875
train loss:  0.534930408000946
train gradient:  0.16144508824599457
iteration : 3954
train acc:  0.7578125
train loss:  0.49527889490127563
train gradient:  0.15807991782747188
iteration : 3955
train acc:  0.7890625
train loss:  0.5124279856681824
train gradient:  0.1349158804949811
iteration : 3956
train acc:  0.7734375
train loss:  0.4754163324832916
train gradient:  0.1740743782117205
iteration : 3957
train acc:  0.7421875
train loss:  0.5068425536155701
train gradient:  0.20010699546550528
iteration : 3958
train acc:  0.7421875
train loss:  0.48444774746894836
train gradient:  0.16488125081950106
iteration : 3959
train acc:  0.734375
train loss:  0.5127286911010742
train gradient:  0.16045649902428374
iteration : 3960
train acc:  0.765625
train loss:  0.47801804542541504
train gradient:  0.13591416565552988
iteration : 3961
train acc:  0.6875
train loss:  0.5564571619033813
train gradient:  0.17585255801926025
iteration : 3962
train acc:  0.8046875
train loss:  0.49805164337158203
train gradient:  0.16430291106096995
iteration : 3963
train acc:  0.6953125
train loss:  0.566048264503479
train gradient:  0.1559022530682179
iteration : 3964
train acc:  0.671875
train loss:  0.5298744440078735
train gradient:  0.220667104724988
iteration : 3965
train acc:  0.8203125
train loss:  0.42889219522476196
train gradient:  0.13533511602239626
iteration : 3966
train acc:  0.7421875
train loss:  0.4810757637023926
train gradient:  0.12729610932998947
iteration : 3967
train acc:  0.7421875
train loss:  0.5260378122329712
train gradient:  0.14307697786086016
iteration : 3968
train acc:  0.7265625
train loss:  0.5390124320983887
train gradient:  0.1874816527342255
iteration : 3969
train acc:  0.7578125
train loss:  0.5086747407913208
train gradient:  0.1560847090003724
iteration : 3970
train acc:  0.703125
train loss:  0.5287100076675415
train gradient:  0.1790188117780614
iteration : 3971
train acc:  0.7421875
train loss:  0.4918920695781708
train gradient:  0.17726394175730703
iteration : 3972
train acc:  0.7890625
train loss:  0.47770965099334717
train gradient:  0.1383175789352627
iteration : 3973
train acc:  0.7109375
train loss:  0.581032395362854
train gradient:  0.22604077251576804
iteration : 3974
train acc:  0.6796875
train loss:  0.5633368492126465
train gradient:  0.18343828937812712
iteration : 3975
train acc:  0.7734375
train loss:  0.4704750180244446
train gradient:  0.12282767742206661
iteration : 3976
train acc:  0.671875
train loss:  0.554792046546936
train gradient:  0.17723525070493698
iteration : 3977
train acc:  0.6953125
train loss:  0.4906502664089203
train gradient:  0.12961194691847044
iteration : 3978
train acc:  0.6796875
train loss:  0.569710373878479
train gradient:  0.1577105429684758
iteration : 3979
train acc:  0.671875
train loss:  0.5638945698738098
train gradient:  0.18478925696670448
iteration : 3980
train acc:  0.75
train loss:  0.4974202811717987
train gradient:  0.14277138735900308
iteration : 3981
train acc:  0.734375
train loss:  0.5266937017440796
train gradient:  0.15177258541367034
iteration : 3982
train acc:  0.7109375
train loss:  0.525382399559021
train gradient:  0.16162366999219657
iteration : 3983
train acc:  0.734375
train loss:  0.4967450797557831
train gradient:  0.11816258493773327
iteration : 3984
train acc:  0.6953125
train loss:  0.5460861921310425
train gradient:  0.23769182443212344
iteration : 3985
train acc:  0.703125
train loss:  0.5224897861480713
train gradient:  0.15552086591699626
iteration : 3986
train acc:  0.6484375
train loss:  0.5775557160377502
train gradient:  0.17340378282969304
iteration : 3987
train acc:  0.6953125
train loss:  0.5163143873214722
train gradient:  0.14504810761885195
iteration : 3988
train acc:  0.703125
train loss:  0.5337207913398743
train gradient:  0.13572868110030545
iteration : 3989
train acc:  0.703125
train loss:  0.5505129098892212
train gradient:  0.23372471852507193
iteration : 3990
train acc:  0.703125
train loss:  0.5715304017066956
train gradient:  0.17517657072739717
iteration : 3991
train acc:  0.71875
train loss:  0.5606068968772888
train gradient:  0.24237851955475453
iteration : 3992
train acc:  0.71875
train loss:  0.539054274559021
train gradient:  0.17094941357699595
iteration : 3993
train acc:  0.75
train loss:  0.5199613571166992
train gradient:  0.14859938597935923
iteration : 3994
train acc:  0.78125
train loss:  0.4736180305480957
train gradient:  0.11645917355864999
iteration : 3995
train acc:  0.703125
train loss:  0.5562281012535095
train gradient:  0.16749534912390285
iteration : 3996
train acc:  0.7265625
train loss:  0.5442007780075073
train gradient:  0.15620449003247666
iteration : 3997
train acc:  0.7421875
train loss:  0.5548697710037231
train gradient:  0.17042068477600622
iteration : 3998
train acc:  0.6796875
train loss:  0.5937122106552124
train gradient:  0.18638677947320986
iteration : 3999
train acc:  0.671875
train loss:  0.600582480430603
train gradient:  0.2621181093456699
iteration : 4000
train acc:  0.7265625
train loss:  0.5116902589797974
train gradient:  0.1466137885054169
iteration : 4001
train acc:  0.6875
train loss:  0.5991412401199341
train gradient:  0.2066489840283543
iteration : 4002
train acc:  0.765625
train loss:  0.517426609992981
train gradient:  0.1771416932455875
iteration : 4003
train acc:  0.7734375
train loss:  0.4634607434272766
train gradient:  0.09952368177587956
iteration : 4004
train acc:  0.7421875
train loss:  0.5194632411003113
train gradient:  0.20446599935650434
iteration : 4005
train acc:  0.6875
train loss:  0.5776426196098328
train gradient:  0.32515715485664953
iteration : 4006
train acc:  0.6796875
train loss:  0.5137661695480347
train gradient:  0.1789185628463692
iteration : 4007
train acc:  0.671875
train loss:  0.5839256048202515
train gradient:  0.17684132276827555
iteration : 4008
train acc:  0.71875
train loss:  0.5133405923843384
train gradient:  0.16447929105042142
iteration : 4009
train acc:  0.6484375
train loss:  0.5852567553520203
train gradient:  0.23368063539424766
iteration : 4010
train acc:  0.7421875
train loss:  0.4681282639503479
train gradient:  0.13979736176308188
iteration : 4011
train acc:  0.6875
train loss:  0.5667044520378113
train gradient:  0.24007556672308436
iteration : 4012
train acc:  0.7109375
train loss:  0.5937228798866272
train gradient:  0.23927880001215812
iteration : 4013
train acc:  0.71875
train loss:  0.5241863131523132
train gradient:  0.1581883146931597
iteration : 4014
train acc:  0.78125
train loss:  0.42455440759658813
train gradient:  0.11028483335904539
iteration : 4015
train acc:  0.7109375
train loss:  0.5420861840248108
train gradient:  0.13115346775493203
iteration : 4016
train acc:  0.6875
train loss:  0.5591312646865845
train gradient:  0.1548505735872437
iteration : 4017
train acc:  0.6796875
train loss:  0.5923910140991211
train gradient:  0.1919543797121238
iteration : 4018
train acc:  0.734375
train loss:  0.47346174716949463
train gradient:  0.10201301593260778
iteration : 4019
train acc:  0.671875
train loss:  0.5913001298904419
train gradient:  0.19200363518861197
iteration : 4020
train acc:  0.734375
train loss:  0.47100630402565
train gradient:  0.14509119135189907
iteration : 4021
train acc:  0.7421875
train loss:  0.49777984619140625
train gradient:  0.12763942225298447
iteration : 4022
train acc:  0.671875
train loss:  0.5107309818267822
train gradient:  0.1388422222509207
iteration : 4023
train acc:  0.765625
train loss:  0.5017127394676208
train gradient:  0.14158815227275773
iteration : 4024
train acc:  0.7890625
train loss:  0.5178052186965942
train gradient:  0.14543701496779934
iteration : 4025
train acc:  0.6953125
train loss:  0.5404068231582642
train gradient:  0.1642516679678933
iteration : 4026
train acc:  0.7109375
train loss:  0.5050472617149353
train gradient:  0.1345883209053183
iteration : 4027
train acc:  0.6796875
train loss:  0.6025260090827942
train gradient:  0.1890680878211856
iteration : 4028
train acc:  0.7734375
train loss:  0.48207902908325195
train gradient:  0.127427041526571
iteration : 4029
train acc:  0.7578125
train loss:  0.4834689199924469
train gradient:  0.15376412863654854
iteration : 4030
train acc:  0.78125
train loss:  0.48844173550605774
train gradient:  0.12506386994786023
iteration : 4031
train acc:  0.6328125
train loss:  0.5394778847694397
train gradient:  0.1601203787885606
iteration : 4032
train acc:  0.765625
train loss:  0.4793817102909088
train gradient:  0.1456809069120628
iteration : 4033
train acc:  0.7109375
train loss:  0.5398035645484924
train gradient:  0.19147988808218402
iteration : 4034
train acc:  0.78125
train loss:  0.4742574989795685
train gradient:  0.11491528026467335
iteration : 4035
train acc:  0.734375
train loss:  0.5028449296951294
train gradient:  0.13049748537858397
iteration : 4036
train acc:  0.75
train loss:  0.5099289417266846
train gradient:  0.1141339024849267
iteration : 4037
train acc:  0.7578125
train loss:  0.4636058807373047
train gradient:  0.17372530924121904
iteration : 4038
train acc:  0.71875
train loss:  0.5256866216659546
train gradient:  0.1694952308505493
iteration : 4039
train acc:  0.6875
train loss:  0.5308475494384766
train gradient:  0.16496627603299813
iteration : 4040
train acc:  0.7578125
train loss:  0.517652153968811
train gradient:  0.15491966984908323
iteration : 4041
train acc:  0.6484375
train loss:  0.5969828963279724
train gradient:  0.2010145079999166
iteration : 4042
train acc:  0.7109375
train loss:  0.5745493173599243
train gradient:  0.20095967764754238
iteration : 4043
train acc:  0.71875
train loss:  0.49130141735076904
train gradient:  0.14080912600090523
iteration : 4044
train acc:  0.78125
train loss:  0.4786248505115509
train gradient:  0.15002970821670378
iteration : 4045
train acc:  0.703125
train loss:  0.557628333568573
train gradient:  0.1713705710568819
iteration : 4046
train acc:  0.6953125
train loss:  0.566925048828125
train gradient:  0.1756120974637009
iteration : 4047
train acc:  0.7421875
train loss:  0.49954283237457275
train gradient:  0.15290708235281247
iteration : 4048
train acc:  0.78125
train loss:  0.46225234866142273
train gradient:  0.11916948733998786
iteration : 4049
train acc:  0.71875
train loss:  0.5306436419487
train gradient:  0.1536719285900648
iteration : 4050
train acc:  0.7734375
train loss:  0.483831524848938
train gradient:  0.15143006436644663
iteration : 4051
train acc:  0.703125
train loss:  0.5044147372245789
train gradient:  0.1413433236242832
iteration : 4052
train acc:  0.7578125
train loss:  0.5073758363723755
train gradient:  0.13052651860094466
iteration : 4053
train acc:  0.6796875
train loss:  0.5555769205093384
train gradient:  0.1664262621328919
iteration : 4054
train acc:  0.6875
train loss:  0.5387555360794067
train gradient:  0.13213704666473988
iteration : 4055
train acc:  0.6953125
train loss:  0.554616391658783
train gradient:  0.18709585335850099
iteration : 4056
train acc:  0.734375
train loss:  0.5049940943717957
train gradient:  0.115334534193237
iteration : 4057
train acc:  0.71875
train loss:  0.5410162210464478
train gradient:  0.15317355170041666
iteration : 4058
train acc:  0.7265625
train loss:  0.501606285572052
train gradient:  0.18620510058003126
iteration : 4059
train acc:  0.75
train loss:  0.4666261672973633
train gradient:  0.1770612748518051
iteration : 4060
train acc:  0.640625
train loss:  0.5642396211624146
train gradient:  0.17974219604065425
iteration : 4061
train acc:  0.71875
train loss:  0.5292717814445496
train gradient:  0.18414413677988706
iteration : 4062
train acc:  0.671875
train loss:  0.5289454460144043
train gradient:  0.1482279267611682
iteration : 4063
train acc:  0.765625
train loss:  0.4635269343852997
train gradient:  0.12207669777705651
iteration : 4064
train acc:  0.71875
train loss:  0.5248775482177734
train gradient:  0.16547561717836295
iteration : 4065
train acc:  0.65625
train loss:  0.5115653276443481
train gradient:  0.11614267937768745
iteration : 4066
train acc:  0.671875
train loss:  0.5463575124740601
train gradient:  0.129057255347408
iteration : 4067
train acc:  0.6875
train loss:  0.5104644298553467
train gradient:  0.14074832322245479
iteration : 4068
train acc:  0.75
train loss:  0.48492705821990967
train gradient:  0.13179063898808513
iteration : 4069
train acc:  0.7109375
train loss:  0.526555061340332
train gradient:  0.14971711469907154
iteration : 4070
train acc:  0.7109375
train loss:  0.5227128267288208
train gradient:  0.15999472599681158
iteration : 4071
train acc:  0.7890625
train loss:  0.4493672549724579
train gradient:  0.12976525813488973
iteration : 4072
train acc:  0.75
train loss:  0.5668380260467529
train gradient:  0.16565821931845304
iteration : 4073
train acc:  0.7421875
train loss:  0.5493322610855103
train gradient:  0.12699026406413283
iteration : 4074
train acc:  0.8125
train loss:  0.45485156774520874
train gradient:  0.1116064475993835
iteration : 4075
train acc:  0.75
train loss:  0.5316170454025269
train gradient:  0.15731887667074243
iteration : 4076
train acc:  0.71875
train loss:  0.4892830550670624
train gradient:  0.11373657097812846
iteration : 4077
train acc:  0.6953125
train loss:  0.522516131401062
train gradient:  0.14387887083576711
iteration : 4078
train acc:  0.7734375
train loss:  0.5156791806221008
train gradient:  0.1690575758217316
iteration : 4079
train acc:  0.6796875
train loss:  0.5741485953330994
train gradient:  0.22115561933855515
iteration : 4080
train acc:  0.734375
train loss:  0.517533540725708
train gradient:  0.2088041122781167
iteration : 4081
train acc:  0.6875
train loss:  0.5889955759048462
train gradient:  0.2365132288071366
iteration : 4082
train acc:  0.7734375
train loss:  0.47878679633140564
train gradient:  0.1370290305820535
iteration : 4083
train acc:  0.7890625
train loss:  0.4769560992717743
train gradient:  0.1833330394444355
iteration : 4084
train acc:  0.7109375
train loss:  0.5444027185440063
train gradient:  0.18932836865889058
iteration : 4085
train acc:  0.7578125
train loss:  0.5271822214126587
train gradient:  0.1596585042333677
iteration : 4086
train acc:  0.6796875
train loss:  0.5441468954086304
train gradient:  0.12978899232961566
iteration : 4087
train acc:  0.703125
train loss:  0.58293616771698
train gradient:  0.16737409129909325
iteration : 4088
train acc:  0.78125
train loss:  0.4506075978279114
train gradient:  0.10367615656216173
iteration : 4089
train acc:  0.7890625
train loss:  0.4608118236064911
train gradient:  0.15125619369343019
iteration : 4090
train acc:  0.75
train loss:  0.5024338960647583
train gradient:  0.12072971379442134
iteration : 4091
train acc:  0.7890625
train loss:  0.4688414931297302
train gradient:  0.13489037006948063
iteration : 4092
train acc:  0.71875
train loss:  0.5271643400192261
train gradient:  0.13202735459202236
iteration : 4093
train acc:  0.7265625
train loss:  0.5278860330581665
train gradient:  0.15431261161280213
iteration : 4094
train acc:  0.734375
train loss:  0.4990398585796356
train gradient:  0.11135624612067783
iteration : 4095
train acc:  0.71875
train loss:  0.4873153269290924
train gradient:  0.1297363957077242
iteration : 4096
train acc:  0.734375
train loss:  0.48538529872894287
train gradient:  0.14853953721778312
iteration : 4097
train acc:  0.765625
train loss:  0.47212839126586914
train gradient:  0.138320117986453
iteration : 4098
train acc:  0.75
train loss:  0.5005673170089722
train gradient:  0.11491903493125268
iteration : 4099
train acc:  0.6875
train loss:  0.627830982208252
train gradient:  0.19471070621124614
iteration : 4100
train acc:  0.6953125
train loss:  0.5509924292564392
train gradient:  0.13973034536515255
iteration : 4101
train acc:  0.6953125
train loss:  0.5285310745239258
train gradient:  0.19421591758566176
iteration : 4102
train acc:  0.671875
train loss:  0.5734965801239014
train gradient:  0.18613233151254088
iteration : 4103
train acc:  0.7265625
train loss:  0.5167418718338013
train gradient:  0.1355022408227349
iteration : 4104
train acc:  0.7265625
train loss:  0.5501075983047485
train gradient:  0.17816967557099136
iteration : 4105
train acc:  0.7109375
train loss:  0.5307192206382751
train gradient:  0.16799017969952018
iteration : 4106
train acc:  0.765625
train loss:  0.5090839862823486
train gradient:  0.14425309863298225
iteration : 4107
train acc:  0.7734375
train loss:  0.4209163188934326
train gradient:  0.10676314765672314
iteration : 4108
train acc:  0.71875
train loss:  0.49876609444618225
train gradient:  0.14003250348608112
iteration : 4109
train acc:  0.734375
train loss:  0.5149466395378113
train gradient:  0.15848824862005195
iteration : 4110
train acc:  0.765625
train loss:  0.48879849910736084
train gradient:  0.14125623340092294
iteration : 4111
train acc:  0.7421875
train loss:  0.5769157409667969
train gradient:  0.22147159534297983
iteration : 4112
train acc:  0.7109375
train loss:  0.5027221441268921
train gradient:  0.15587987972979886
iteration : 4113
train acc:  0.7734375
train loss:  0.48122429847717285
train gradient:  0.1365449564048285
iteration : 4114
train acc:  0.671875
train loss:  0.5992957353591919
train gradient:  0.2173978873335643
iteration : 4115
train acc:  0.7890625
train loss:  0.5168223977088928
train gradient:  0.20439855301323706
iteration : 4116
train acc:  0.6640625
train loss:  0.569017231464386
train gradient:  0.21175006942031951
iteration : 4117
train acc:  0.6796875
train loss:  0.552922248840332
train gradient:  0.17030158785198019
iteration : 4118
train acc:  0.7109375
train loss:  0.5458112955093384
train gradient:  0.17332981532640296
iteration : 4119
train acc:  0.734375
train loss:  0.49238044023513794
train gradient:  0.16878747412833622
iteration : 4120
train acc:  0.7265625
train loss:  0.524217963218689
train gradient:  0.1751974722724277
iteration : 4121
train acc:  0.703125
train loss:  0.5238181948661804
train gradient:  0.1574790375825827
iteration : 4122
train acc:  0.734375
train loss:  0.5373656153678894
train gradient:  0.14625416412152475
iteration : 4123
train acc:  0.7578125
train loss:  0.4987156093120575
train gradient:  0.16310786643082753
iteration : 4124
train acc:  0.7578125
train loss:  0.5128105878829956
train gradient:  0.14333601178228772
iteration : 4125
train acc:  0.734375
train loss:  0.48464107513427734
train gradient:  0.14088272751652337
iteration : 4126
train acc:  0.7265625
train loss:  0.5342264771461487
train gradient:  0.1752903835582859
iteration : 4127
train acc:  0.703125
train loss:  0.5571284294128418
train gradient:  0.14918524095094987
iteration : 4128
train acc:  0.7578125
train loss:  0.567477285861969
train gradient:  0.1935151151471735
iteration : 4129
train acc:  0.765625
train loss:  0.44939059019088745
train gradient:  0.12820176556374838
iteration : 4130
train acc:  0.6640625
train loss:  0.5738154649734497
train gradient:  0.2144495760276275
iteration : 4131
train acc:  0.6640625
train loss:  0.5373827219009399
train gradient:  0.1581552582458417
iteration : 4132
train acc:  0.6796875
train loss:  0.5540240406990051
train gradient:  0.1746930628517307
iteration : 4133
train acc:  0.6953125
train loss:  0.5564708113670349
train gradient:  0.15333879106657208
iteration : 4134
train acc:  0.6875
train loss:  0.5974549651145935
train gradient:  0.18925680368100706
iteration : 4135
train acc:  0.734375
train loss:  0.49466246366500854
train gradient:  0.14635850292928973
iteration : 4136
train acc:  0.6875
train loss:  0.53834068775177
train gradient:  0.13351583497611652
iteration : 4137
train acc:  0.7109375
train loss:  0.4830062985420227
train gradient:  0.13497024638856883
iteration : 4138
train acc:  0.7109375
train loss:  0.5428391098976135
train gradient:  0.1260885148911299
iteration : 4139
train acc:  0.8125
train loss:  0.4658086597919464
train gradient:  0.1172501554414543
iteration : 4140
train acc:  0.7109375
train loss:  0.5333855152130127
train gradient:  0.20154466450850272
iteration : 4141
train acc:  0.65625
train loss:  0.5986111164093018
train gradient:  0.17616002381731585
iteration : 4142
train acc:  0.75
train loss:  0.4784352481365204
train gradient:  0.15569378662027278
iteration : 4143
train acc:  0.7734375
train loss:  0.4762614369392395
train gradient:  0.1282156346506531
iteration : 4144
train acc:  0.7265625
train loss:  0.5575588941574097
train gradient:  0.15293499568281685
iteration : 4145
train acc:  0.6796875
train loss:  0.5901246070861816
train gradient:  0.16429356774640633
iteration : 4146
train acc:  0.703125
train loss:  0.5715858936309814
train gradient:  0.18206719872700733
iteration : 4147
train acc:  0.71875
train loss:  0.5287593603134155
train gradient:  0.15150979478386756
iteration : 4148
train acc:  0.7578125
train loss:  0.505201518535614
train gradient:  0.1755495342104088
iteration : 4149
train acc:  0.671875
train loss:  0.5941795110702515
train gradient:  0.25397424957525866
iteration : 4150
train acc:  0.6953125
train loss:  0.5559202432632446
train gradient:  0.22601478712760836
iteration : 4151
train acc:  0.765625
train loss:  0.5116148591041565
train gradient:  0.1259616017364328
iteration : 4152
train acc:  0.65625
train loss:  0.6062713265419006
train gradient:  0.2069399973212791
iteration : 4153
train acc:  0.7265625
train loss:  0.5233153104782104
train gradient:  0.159885510693812
iteration : 4154
train acc:  0.7265625
train loss:  0.521312952041626
train gradient:  0.14225559363185677
iteration : 4155
train acc:  0.6640625
train loss:  0.5542525053024292
train gradient:  0.18656495687380936
iteration : 4156
train acc:  0.65625
train loss:  0.5732938051223755
train gradient:  0.1783316472069943
iteration : 4157
train acc:  0.6953125
train loss:  0.576500654220581
train gradient:  0.16110254088923648
iteration : 4158
train acc:  0.71875
train loss:  0.4808034300804138
train gradient:  0.14651732973640264
iteration : 4159
train acc:  0.6953125
train loss:  0.5061935186386108
train gradient:  0.14214936851377447
iteration : 4160
train acc:  0.71875
train loss:  0.515245258808136
train gradient:  0.16489918585843644
iteration : 4161
train acc:  0.71875
train loss:  0.5045617818832397
train gradient:  0.15982696154993617
iteration : 4162
train acc:  0.6953125
train loss:  0.5714720487594604
train gradient:  0.19394036311735824
iteration : 4163
train acc:  0.7421875
train loss:  0.4981941282749176
train gradient:  0.22259591599838122
iteration : 4164
train acc:  0.7109375
train loss:  0.555745005607605
train gradient:  0.17708413849771965
iteration : 4165
train acc:  0.7265625
train loss:  0.5366930365562439
train gradient:  0.18015401303979217
iteration : 4166
train acc:  0.734375
train loss:  0.5038576722145081
train gradient:  0.1836592567286749
iteration : 4167
train acc:  0.7109375
train loss:  0.5037577152252197
train gradient:  0.13984608054932982
iteration : 4168
train acc:  0.71875
train loss:  0.5530322790145874
train gradient:  0.12364180905475326
iteration : 4169
train acc:  0.65625
train loss:  0.5649858713150024
train gradient:  0.168863789512312
iteration : 4170
train acc:  0.7265625
train loss:  0.5400886535644531
train gradient:  0.1559432573873548
iteration : 4171
train acc:  0.6796875
train loss:  0.5826611518859863
train gradient:  0.19027323358328635
iteration : 4172
train acc:  0.7578125
train loss:  0.47997814416885376
train gradient:  0.13341166758993328
iteration : 4173
train acc:  0.703125
train loss:  0.5551921129226685
train gradient:  0.15411698978697974
iteration : 4174
train acc:  0.6796875
train loss:  0.5811632871627808
train gradient:  0.20138262673028734
iteration : 4175
train acc:  0.7109375
train loss:  0.5226218700408936
train gradient:  0.13575448290390832
iteration : 4176
train acc:  0.75
train loss:  0.49013209342956543
train gradient:  0.1883396422186701
iteration : 4177
train acc:  0.7265625
train loss:  0.5033096671104431
train gradient:  0.1284439746504068
iteration : 4178
train acc:  0.734375
train loss:  0.4930395483970642
train gradient:  0.132617479818593
iteration : 4179
train acc:  0.71875
train loss:  0.5550406575202942
train gradient:  0.16055802314311712
iteration : 4180
train acc:  0.71875
train loss:  0.4947151839733124
train gradient:  0.17946113863675955
iteration : 4181
train acc:  0.6953125
train loss:  0.5221699476242065
train gradient:  0.15215678171554003
iteration : 4182
train acc:  0.78125
train loss:  0.45398661494255066
train gradient:  0.1335350413541314
iteration : 4183
train acc:  0.7734375
train loss:  0.4601876437664032
train gradient:  0.11878556967791978
iteration : 4184
train acc:  0.7109375
train loss:  0.5423627495765686
train gradient:  0.13477053008555598
iteration : 4185
train acc:  0.703125
train loss:  0.5217548608779907
train gradient:  0.15160875364664145
iteration : 4186
train acc:  0.78125
train loss:  0.46155625581741333
train gradient:  0.145632131977655
iteration : 4187
train acc:  0.703125
train loss:  0.5044962167739868
train gradient:  0.1506637800562285
iteration : 4188
train acc:  0.734375
train loss:  0.5273271799087524
train gradient:  0.17629044022077553
iteration : 4189
train acc:  0.6953125
train loss:  0.5621064305305481
train gradient:  0.1565107591024556
iteration : 4190
train acc:  0.6484375
train loss:  0.6239391565322876
train gradient:  0.20334070828707623
iteration : 4191
train acc:  0.7578125
train loss:  0.465673565864563
train gradient:  0.10831717970895122
iteration : 4192
train acc:  0.8046875
train loss:  0.4603137969970703
train gradient:  0.11680127795927306
iteration : 4193
train acc:  0.71875
train loss:  0.530474841594696
train gradient:  0.1447101116902887
iteration : 4194
train acc:  0.6953125
train loss:  0.5316158533096313
train gradient:  0.18919790984875068
iteration : 4195
train acc:  0.75
train loss:  0.474526047706604
train gradient:  0.12452242652554607
iteration : 4196
train acc:  0.765625
train loss:  0.5172954201698303
train gradient:  0.12253507268089453
iteration : 4197
train acc:  0.6953125
train loss:  0.5339648723602295
train gradient:  0.16112720481965842
iteration : 4198
train acc:  0.75
train loss:  0.4864514470100403
train gradient:  0.13039028985555323
iteration : 4199
train acc:  0.7421875
train loss:  0.47760921716690063
train gradient:  0.12083316801866992
iteration : 4200
train acc:  0.7109375
train loss:  0.5342906713485718
train gradient:  0.16469370068360437
iteration : 4201
train acc:  0.7265625
train loss:  0.46729934215545654
train gradient:  0.13536986154011071
iteration : 4202
train acc:  0.6875
train loss:  0.5240848064422607
train gradient:  0.25638865895627444
iteration : 4203
train acc:  0.7109375
train loss:  0.5216740369796753
train gradient:  0.18653735467059096
iteration : 4204
train acc:  0.6953125
train loss:  0.6016467213630676
train gradient:  0.17151533581276823
iteration : 4205
train acc:  0.7265625
train loss:  0.5073539614677429
train gradient:  0.16016896482527865
iteration : 4206
train acc:  0.734375
train loss:  0.5266327857971191
train gradient:  0.18314034036057608
iteration : 4207
train acc:  0.703125
train loss:  0.5317590236663818
train gradient:  0.20002806336599394
iteration : 4208
train acc:  0.734375
train loss:  0.5374545454978943
train gradient:  0.16720249099817952
iteration : 4209
train acc:  0.734375
train loss:  0.5076804757118225
train gradient:  0.11777723748981624
iteration : 4210
train acc:  0.6484375
train loss:  0.6072455644607544
train gradient:  0.29726195065405225
iteration : 4211
train acc:  0.7734375
train loss:  0.4912870526313782
train gradient:  0.1284564042150224
iteration : 4212
train acc:  0.7265625
train loss:  0.5078060626983643
train gradient:  0.15058639880227387
iteration : 4213
train acc:  0.7421875
train loss:  0.518738865852356
train gradient:  0.1357057312827672
iteration : 4214
train acc:  0.7421875
train loss:  0.5342763662338257
train gradient:  0.14244592908635273
iteration : 4215
train acc:  0.765625
train loss:  0.4543013870716095
train gradient:  0.11882956576865648
iteration : 4216
train acc:  0.71875
train loss:  0.5069479942321777
train gradient:  0.2566080459106557
iteration : 4217
train acc:  0.7578125
train loss:  0.47697997093200684
train gradient:  0.11288094354039493
iteration : 4218
train acc:  0.78125
train loss:  0.52097088098526
train gradient:  0.18014916825919092
iteration : 4219
train acc:  0.7578125
train loss:  0.4886270761489868
train gradient:  0.18050842517189167
iteration : 4220
train acc:  0.7890625
train loss:  0.4291292130947113
train gradient:  0.09735874954421943
iteration : 4221
train acc:  0.7421875
train loss:  0.4911450147628784
train gradient:  0.1592105536462991
iteration : 4222
train acc:  0.7109375
train loss:  0.5285255908966064
train gradient:  0.14811579239524475
iteration : 4223
train acc:  0.6953125
train loss:  0.559709906578064
train gradient:  0.1853843260702014
iteration : 4224
train acc:  0.765625
train loss:  0.49153932929039
train gradient:  0.18515166621685916
iteration : 4225
train acc:  0.71875
train loss:  0.5340630412101746
train gradient:  0.1793164306399711
iteration : 4226
train acc:  0.7109375
train loss:  0.5170350074768066
train gradient:  0.17057776429804355
iteration : 4227
train acc:  0.75
train loss:  0.5394390225410461
train gradient:  0.15909628356265465
iteration : 4228
train acc:  0.6953125
train loss:  0.5026388168334961
train gradient:  0.1516627966083625
iteration : 4229
train acc:  0.7421875
train loss:  0.539840579032898
train gradient:  0.21806911607298046
iteration : 4230
train acc:  0.6953125
train loss:  0.5381710529327393
train gradient:  0.14759094162593692
iteration : 4231
train acc:  0.6640625
train loss:  0.5543684959411621
train gradient:  0.16086083001975948
iteration : 4232
train acc:  0.71875
train loss:  0.5441362857818604
train gradient:  0.26227601316381516
iteration : 4233
train acc:  0.7734375
train loss:  0.503902018070221
train gradient:  0.13993308315494946
iteration : 4234
train acc:  0.75
train loss:  0.5151557922363281
train gradient:  0.13314300460486025
iteration : 4235
train acc:  0.6953125
train loss:  0.5522984862327576
train gradient:  0.21112159611809583
iteration : 4236
train acc:  0.7734375
train loss:  0.48589977622032166
train gradient:  0.11919229638711408
iteration : 4237
train acc:  0.765625
train loss:  0.5063348412513733
train gradient:  0.1366624640900705
iteration : 4238
train acc:  0.7734375
train loss:  0.47076767683029175
train gradient:  0.13937498719603675
iteration : 4239
train acc:  0.71875
train loss:  0.5292885303497314
train gradient:  0.2191091798105851
iteration : 4240
train acc:  0.65625
train loss:  0.6024503707885742
train gradient:  0.2998338114493282
iteration : 4241
train acc:  0.6953125
train loss:  0.5290402173995972
train gradient:  0.1413171682154601
iteration : 4242
train acc:  0.625
train loss:  0.648225724697113
train gradient:  0.21837694066690133
iteration : 4243
train acc:  0.6953125
train loss:  0.5200266242027283
train gradient:  0.16116222874802666
iteration : 4244
train acc:  0.734375
train loss:  0.5180946588516235
train gradient:  0.15273581782560505
iteration : 4245
train acc:  0.7578125
train loss:  0.4813934564590454
train gradient:  0.14937594470967303
iteration : 4246
train acc:  0.71875
train loss:  0.5407826900482178
train gradient:  0.14430786830913106
iteration : 4247
train acc:  0.796875
train loss:  0.4577871561050415
train gradient:  0.11298433463246343
iteration : 4248
train acc:  0.703125
train loss:  0.5432444214820862
train gradient:  0.1355783659276255
iteration : 4249
train acc:  0.6875
train loss:  0.5449676513671875
train gradient:  0.17472997270815954
iteration : 4250
train acc:  0.6875
train loss:  0.5702219009399414
train gradient:  0.1622802762214211
iteration : 4251
train acc:  0.7578125
train loss:  0.47969573736190796
train gradient:  0.16989770588730124
iteration : 4252
train acc:  0.71875
train loss:  0.536491334438324
train gradient:  0.24155044790820657
iteration : 4253
train acc:  0.7109375
train loss:  0.5338687896728516
train gradient:  0.13414572261740199
iteration : 4254
train acc:  0.734375
train loss:  0.5125852823257446
train gradient:  0.14181946022164096
iteration : 4255
train acc:  0.734375
train loss:  0.49894633889198303
train gradient:  0.1683943558666941
iteration : 4256
train acc:  0.7265625
train loss:  0.5510989427566528
train gradient:  0.15926230459293633
iteration : 4257
train acc:  0.6875
train loss:  0.5589636564254761
train gradient:  0.15493408615230128
iteration : 4258
train acc:  0.7421875
train loss:  0.5213168859481812
train gradient:  0.15558359251060933
iteration : 4259
train acc:  0.671875
train loss:  0.5783358812332153
train gradient:  0.16564674017256187
iteration : 4260
train acc:  0.828125
train loss:  0.43297648429870605
train gradient:  0.1555525268744563
iteration : 4261
train acc:  0.671875
train loss:  0.5803185701370239
train gradient:  0.19312568804474922
iteration : 4262
train acc:  0.734375
train loss:  0.5559031963348389
train gradient:  0.18590449300580703
iteration : 4263
train acc:  0.7109375
train loss:  0.518700361251831
train gradient:  0.12288412803634838
iteration : 4264
train acc:  0.6796875
train loss:  0.5178611278533936
train gradient:  0.16750456319577448
iteration : 4265
train acc:  0.8125
train loss:  0.41268396377563477
train gradient:  0.1266535098778243
iteration : 4266
train acc:  0.75
train loss:  0.5147596597671509
train gradient:  0.15287111023870364
iteration : 4267
train acc:  0.703125
train loss:  0.5558427572250366
train gradient:  0.21315767096454247
iteration : 4268
train acc:  0.7578125
train loss:  0.519568681716919
train gradient:  0.1386424766908829
iteration : 4269
train acc:  0.6875
train loss:  0.5394124388694763
train gradient:  0.15757382423273406
iteration : 4270
train acc:  0.71875
train loss:  0.5817661881446838
train gradient:  0.22327433275258907
iteration : 4271
train acc:  0.6484375
train loss:  0.5744360685348511
train gradient:  0.16261055984828
iteration : 4272
train acc:  0.703125
train loss:  0.5577120780944824
train gradient:  0.1912442443953108
iteration : 4273
train acc:  0.7265625
train loss:  0.5432491302490234
train gradient:  0.18301179204834261
iteration : 4274
train acc:  0.6875
train loss:  0.5363422632217407
train gradient:  0.16981520225635077
iteration : 4275
train acc:  0.7421875
train loss:  0.4475287199020386
train gradient:  0.10284180130308769
iteration : 4276
train acc:  0.765625
train loss:  0.4874506890773773
train gradient:  0.15611512207779413
iteration : 4277
train acc:  0.6484375
train loss:  0.5585722327232361
train gradient:  0.1923932721500684
iteration : 4278
train acc:  0.7421875
train loss:  0.5015097856521606
train gradient:  0.17831934099213861
iteration : 4279
train acc:  0.71875
train loss:  0.5102377533912659
train gradient:  0.16260247855953802
iteration : 4280
train acc:  0.734375
train loss:  0.5003175735473633
train gradient:  0.12422733922819451
iteration : 4281
train acc:  0.765625
train loss:  0.5177517533302307
train gradient:  0.15130160606486676
iteration : 4282
train acc:  0.671875
train loss:  0.534454345703125
train gradient:  0.12817214814286307
iteration : 4283
train acc:  0.7109375
train loss:  0.5399343371391296
train gradient:  0.14559341224014122
iteration : 4284
train acc:  0.75
train loss:  0.5039800405502319
train gradient:  0.13057712198539967
iteration : 4285
train acc:  0.703125
train loss:  0.5390209555625916
train gradient:  0.13067093302158583
iteration : 4286
train acc:  0.703125
train loss:  0.5726670026779175
train gradient:  0.2071749067230822
iteration : 4287
train acc:  0.7109375
train loss:  0.5257269740104675
train gradient:  0.19072091795358864
iteration : 4288
train acc:  0.75
train loss:  0.5254759788513184
train gradient:  0.16660747538210363
iteration : 4289
train acc:  0.6953125
train loss:  0.5517993569374084
train gradient:  0.16076826258942484
iteration : 4290
train acc:  0.6953125
train loss:  0.5375837087631226
train gradient:  0.1757310263085519
iteration : 4291
train acc:  0.65625
train loss:  0.5981426239013672
train gradient:  0.1938073313612953
iteration : 4292
train acc:  0.75
train loss:  0.48803067207336426
train gradient:  0.14583579480544212
iteration : 4293
train acc:  0.6875
train loss:  0.5063759088516235
train gradient:  0.14364263808714317
iteration : 4294
train acc:  0.6640625
train loss:  0.6177581548690796
train gradient:  0.22082667008417156
iteration : 4295
train acc:  0.7109375
train loss:  0.548646092414856
train gradient:  0.16687991295855897
iteration : 4296
train acc:  0.703125
train loss:  0.5514034628868103
train gradient:  0.14268364499392827
iteration : 4297
train acc:  0.71875
train loss:  0.5545514225959778
train gradient:  0.17936240892435168
iteration : 4298
train acc:  0.7578125
train loss:  0.48685941100120544
train gradient:  0.11104608838630006
iteration : 4299
train acc:  0.7578125
train loss:  0.4759117662906647
train gradient:  0.1337131050616609
iteration : 4300
train acc:  0.6875
train loss:  0.5645244121551514
train gradient:  0.17308974726528398
iteration : 4301
train acc:  0.734375
train loss:  0.47855228185653687
train gradient:  0.1140633395113398
iteration : 4302
train acc:  0.78125
train loss:  0.5034527778625488
train gradient:  0.18738869117988177
iteration : 4303
train acc:  0.7265625
train loss:  0.5337594747543335
train gradient:  0.1590760195171111
iteration : 4304
train acc:  0.7578125
train loss:  0.462304025888443
train gradient:  0.11199425969737141
iteration : 4305
train acc:  0.71875
train loss:  0.5396362543106079
train gradient:  0.19574660192713592
iteration : 4306
train acc:  0.703125
train loss:  0.5365227460861206
train gradient:  0.17362368837861675
iteration : 4307
train acc:  0.7578125
train loss:  0.4922199249267578
train gradient:  0.14121572105161098
iteration : 4308
train acc:  0.6796875
train loss:  0.5509035587310791
train gradient:  0.1679917649582902
iteration : 4309
train acc:  0.78125
train loss:  0.48615407943725586
train gradient:  0.15029417360849157
iteration : 4310
train acc:  0.6875
train loss:  0.5650094747543335
train gradient:  0.18398686976379974
iteration : 4311
train acc:  0.75
train loss:  0.5002042055130005
train gradient:  0.16479807208473884
iteration : 4312
train acc:  0.6796875
train loss:  0.5820987820625305
train gradient:  0.1568409013083758
iteration : 4313
train acc:  0.796875
train loss:  0.44841206073760986
train gradient:  0.17267919559514522
iteration : 4314
train acc:  0.7421875
train loss:  0.4980236887931824
train gradient:  0.14570817630704958
iteration : 4315
train acc:  0.7421875
train loss:  0.5207305550575256
train gradient:  0.1740583722268167
iteration : 4316
train acc:  0.7421875
train loss:  0.48373115062713623
train gradient:  0.12340933047207547
iteration : 4317
train acc:  0.71875
train loss:  0.5174881219863892
train gradient:  0.1442090760840984
iteration : 4318
train acc:  0.734375
train loss:  0.5229734182357788
train gradient:  0.1342038468315551
iteration : 4319
train acc:  0.71875
train loss:  0.5271852612495422
train gradient:  0.20553220055981786
iteration : 4320
train acc:  0.7265625
train loss:  0.5733864307403564
train gradient:  0.14699828951570457
iteration : 4321
train acc:  0.7734375
train loss:  0.5275058150291443
train gradient:  0.15557462651686044
iteration : 4322
train acc:  0.6953125
train loss:  0.5484988689422607
train gradient:  0.15631595251074587
iteration : 4323
train acc:  0.71875
train loss:  0.5299874544143677
train gradient:  0.18926529018775579
iteration : 4324
train acc:  0.7578125
train loss:  0.49767959117889404
train gradient:  0.11861781737236711
iteration : 4325
train acc:  0.7421875
train loss:  0.5334742069244385
train gradient:  0.20274192590759693
iteration : 4326
train acc:  0.6953125
train loss:  0.5253636240959167
train gradient:  0.1735441636277258
iteration : 4327
train acc:  0.71875
train loss:  0.533682107925415
train gradient:  0.1735305172698624
iteration : 4328
train acc:  0.7890625
train loss:  0.4513985514640808
train gradient:  0.11944398968770129
iteration : 4329
train acc:  0.75
train loss:  0.47466856241226196
train gradient:  0.16773868422468247
iteration : 4330
train acc:  0.7421875
train loss:  0.5113016366958618
train gradient:  0.16507503921875952
iteration : 4331
train acc:  0.765625
train loss:  0.4951816201210022
train gradient:  0.17074462999003992
iteration : 4332
train acc:  0.7109375
train loss:  0.4996044635772705
train gradient:  0.15533058404144204
iteration : 4333
train acc:  0.6796875
train loss:  0.5699158906936646
train gradient:  0.20503387192159978
iteration : 4334
train acc:  0.703125
train loss:  0.5284572243690491
train gradient:  0.13249965689049145
iteration : 4335
train acc:  0.7578125
train loss:  0.4713670015335083
train gradient:  0.14286790928050863
iteration : 4336
train acc:  0.7734375
train loss:  0.49432745575904846
train gradient:  0.1452271739136042
iteration : 4337
train acc:  0.75
train loss:  0.49647873640060425
train gradient:  0.12611554443490175
iteration : 4338
train acc:  0.7421875
train loss:  0.5249135494232178
train gradient:  0.14487711543487403
iteration : 4339
train acc:  0.796875
train loss:  0.46366292238235474
train gradient:  0.13362294781349437
iteration : 4340
train acc:  0.7421875
train loss:  0.5412557721138
train gradient:  0.12537682766312405
iteration : 4341
train acc:  0.703125
train loss:  0.5497796535491943
train gradient:  0.22210898063568615
iteration : 4342
train acc:  0.7421875
train loss:  0.519990086555481
train gradient:  0.1853753881155939
iteration : 4343
train acc:  0.6796875
train loss:  0.5511298179626465
train gradient:  0.1876189282039864
iteration : 4344
train acc:  0.59375
train loss:  0.593876838684082
train gradient:  0.22358554907644196
iteration : 4345
train acc:  0.65625
train loss:  0.644768476486206
train gradient:  0.23203789461979973
iteration : 4346
train acc:  0.6640625
train loss:  0.572394609451294
train gradient:  0.28281137796414174
iteration : 4347
train acc:  0.7421875
train loss:  0.4874413311481476
train gradient:  0.1561508807430752
iteration : 4348
train acc:  0.75
train loss:  0.5146074295043945
train gradient:  0.13465496115458622
iteration : 4349
train acc:  0.734375
train loss:  0.5279699563980103
train gradient:  0.15213750293855338
iteration : 4350
train acc:  0.7578125
train loss:  0.5019109845161438
train gradient:  0.1351225290252679
iteration : 4351
train acc:  0.71875
train loss:  0.5093048810958862
train gradient:  0.15573124638133373
iteration : 4352
train acc:  0.6953125
train loss:  0.5354172587394714
train gradient:  0.14384549342587183
iteration : 4353
train acc:  0.671875
train loss:  0.5301679372787476
train gradient:  0.1274296139861843
iteration : 4354
train acc:  0.7734375
train loss:  0.5179958343505859
train gradient:  0.19257449298389612
iteration : 4355
train acc:  0.75
train loss:  0.5461657047271729
train gradient:  0.16575817090024744
iteration : 4356
train acc:  0.7265625
train loss:  0.5578351020812988
train gradient:  0.16525621655864828
iteration : 4357
train acc:  0.765625
train loss:  0.4804922342300415
train gradient:  0.11815487693721982
iteration : 4358
train acc:  0.734375
train loss:  0.5163824558258057
train gradient:  0.12002088493612373
iteration : 4359
train acc:  0.6953125
train loss:  0.5532869100570679
train gradient:  0.13455000211771634
iteration : 4360
train acc:  0.75
train loss:  0.537600576877594
train gradient:  0.16315043449423117
iteration : 4361
train acc:  0.703125
train loss:  0.6002106070518494
train gradient:  0.21905693986903318
iteration : 4362
train acc:  0.71875
train loss:  0.5534712076187134
train gradient:  0.17092261997660224
iteration : 4363
train acc:  0.734375
train loss:  0.5089944005012512
train gradient:  0.17489265356861028
iteration : 4364
train acc:  0.671875
train loss:  0.5512233972549438
train gradient:  0.21855318181774963
iteration : 4365
train acc:  0.7890625
train loss:  0.4393270015716553
train gradient:  0.13460846556869477
iteration : 4366
train acc:  0.734375
train loss:  0.4785667657852173
train gradient:  0.13339755016898358
iteration : 4367
train acc:  0.703125
train loss:  0.568270742893219
train gradient:  0.17351026198327052
iteration : 4368
train acc:  0.671875
train loss:  0.5489405393600464
train gradient:  0.15732944626799805
iteration : 4369
train acc:  0.7421875
train loss:  0.5131164193153381
train gradient:  0.11353811621050892
iteration : 4370
train acc:  0.78125
train loss:  0.4952887296676636
train gradient:  0.14279007367065122
iteration : 4371
train acc:  0.71875
train loss:  0.5478281378746033
train gradient:  0.1978538171549744
iteration : 4372
train acc:  0.828125
train loss:  0.49231022596359253
train gradient:  0.1466032836969528
iteration : 4373
train acc:  0.6875
train loss:  0.5369729399681091
train gradient:  0.1682453169395528
iteration : 4374
train acc:  0.671875
train loss:  0.6037096977233887
train gradient:  0.291092288490366
iteration : 4375
train acc:  0.671875
train loss:  0.6036155819892883
train gradient:  0.178734419100129
iteration : 4376
train acc:  0.7578125
train loss:  0.5342488288879395
train gradient:  0.20286990579571734
iteration : 4377
train acc:  0.7109375
train loss:  0.5343458652496338
train gradient:  0.2096772487966624
iteration : 4378
train acc:  0.6953125
train loss:  0.5070898532867432
train gradient:  0.12709994463062985
iteration : 4379
train acc:  0.7421875
train loss:  0.5225328207015991
train gradient:  0.14805470957475697
iteration : 4380
train acc:  0.7890625
train loss:  0.4554610252380371
train gradient:  0.1036409410544905
iteration : 4381
train acc:  0.7578125
train loss:  0.5050736665725708
train gradient:  0.1508601006474938
iteration : 4382
train acc:  0.71875
train loss:  0.5381237268447876
train gradient:  0.17078614682822135
iteration : 4383
train acc:  0.765625
train loss:  0.4811195135116577
train gradient:  0.19511059841406903
iteration : 4384
train acc:  0.6640625
train loss:  0.5627359747886658
train gradient:  0.16870889497786665
iteration : 4385
train acc:  0.671875
train loss:  0.5686556100845337
train gradient:  0.1844742902617039
iteration : 4386
train acc:  0.7109375
train loss:  0.533142626285553
train gradient:  0.16934118436100631
iteration : 4387
train acc:  0.765625
train loss:  0.5139012932777405
train gradient:  0.141957096968228
iteration : 4388
train acc:  0.75
train loss:  0.5141623616218567
train gradient:  0.1839715916522925
iteration : 4389
train acc:  0.765625
train loss:  0.4927194118499756
train gradient:  0.14041209729088366
iteration : 4390
train acc:  0.7109375
train loss:  0.5637081861495972
train gradient:  0.15186375142930436
iteration : 4391
train acc:  0.6875
train loss:  0.5134035348892212
train gradient:  0.15837855442236162
iteration : 4392
train acc:  0.75
train loss:  0.4791055917739868
train gradient:  0.1509488320982884
iteration : 4393
train acc:  0.7890625
train loss:  0.4508158564567566
train gradient:  0.1254048085555467
iteration : 4394
train acc:  0.71875
train loss:  0.578387975692749
train gradient:  0.18129330791985054
iteration : 4395
train acc:  0.6796875
train loss:  0.5535390973091125
train gradient:  0.16233769651323024
iteration : 4396
train acc:  0.75
train loss:  0.502781093120575
train gradient:  0.15238856144176216
iteration : 4397
train acc:  0.7265625
train loss:  0.5126208066940308
train gradient:  0.18648890077189653
iteration : 4398
train acc:  0.7578125
train loss:  0.5170217752456665
train gradient:  0.15838748812630365
iteration : 4399
train acc:  0.6875
train loss:  0.5660088062286377
train gradient:  0.18281317746310927
iteration : 4400
train acc:  0.71875
train loss:  0.5439774990081787
train gradient:  0.20548024629055597
iteration : 4401
train acc:  0.7421875
train loss:  0.4858938157558441
train gradient:  0.12368818515848949
iteration : 4402
train acc:  0.75
train loss:  0.48081234097480774
train gradient:  0.11915942823551709
iteration : 4403
train acc:  0.75
train loss:  0.48871129751205444
train gradient:  0.15606812492967398
iteration : 4404
train acc:  0.703125
train loss:  0.5377919673919678
train gradient:  0.16817673553157533
iteration : 4405
train acc:  0.7421875
train loss:  0.4745640456676483
train gradient:  0.14784037443829312
iteration : 4406
train acc:  0.734375
train loss:  0.5015309453010559
train gradient:  0.14369787332511658
iteration : 4407
train acc:  0.7109375
train loss:  0.558358907699585
train gradient:  0.1670264998853231
iteration : 4408
train acc:  0.75
train loss:  0.5026744604110718
train gradient:  0.1155605078277677
iteration : 4409
train acc:  0.7421875
train loss:  0.5124577879905701
train gradient:  0.14355634280397464
iteration : 4410
train acc:  0.7421875
train loss:  0.530968189239502
train gradient:  0.1685349306471202
iteration : 4411
train acc:  0.7421875
train loss:  0.5037379860877991
train gradient:  0.11981364265263841
iteration : 4412
train acc:  0.7734375
train loss:  0.46285757422447205
train gradient:  0.11250796236635725
iteration : 4413
train acc:  0.7734375
train loss:  0.48826825618743896
train gradient:  0.13799796503990464
iteration : 4414
train acc:  0.6796875
train loss:  0.5933753848075867
train gradient:  0.1807295292538433
iteration : 4415
train acc:  0.734375
train loss:  0.5232272148132324
train gradient:  0.1676859105840537
iteration : 4416
train acc:  0.734375
train loss:  0.5395764112472534
train gradient:  0.13955935346970283
iteration : 4417
train acc:  0.71875
train loss:  0.5167208909988403
train gradient:  0.13808008039651407
iteration : 4418
train acc:  0.7421875
train loss:  0.48166054487228394
train gradient:  0.17884141631404255
iteration : 4419
train acc:  0.6484375
train loss:  0.5602151155471802
train gradient:  0.1839052729084867
iteration : 4420
train acc:  0.6796875
train loss:  0.5450013875961304
train gradient:  0.18942292719854337
iteration : 4421
train acc:  0.7890625
train loss:  0.4912463426589966
train gradient:  0.13831274435271285
iteration : 4422
train acc:  0.734375
train loss:  0.478930801153183
train gradient:  0.13530791586925495
iteration : 4423
train acc:  0.7421875
train loss:  0.500839352607727
train gradient:  0.12351098776155717
iteration : 4424
train acc:  0.703125
train loss:  0.5727030038833618
train gradient:  0.16497276886701157
iteration : 4425
train acc:  0.734375
train loss:  0.5109518766403198
train gradient:  0.12687175058969183
iteration : 4426
train acc:  0.6953125
train loss:  0.5347534418106079
train gradient:  0.17482855530335917
iteration : 4427
train acc:  0.75
train loss:  0.5017096996307373
train gradient:  0.14252623179137047
iteration : 4428
train acc:  0.671875
train loss:  0.5812508463859558
train gradient:  0.20364678100791156
iteration : 4429
train acc:  0.7578125
train loss:  0.49842143058776855
train gradient:  0.17397570733160841
iteration : 4430
train acc:  0.765625
train loss:  0.48153966665267944
train gradient:  0.11439261812848053
iteration : 4431
train acc:  0.7265625
train loss:  0.48011648654937744
train gradient:  0.14586719149605148
iteration : 4432
train acc:  0.71875
train loss:  0.5170488357543945
train gradient:  0.19568019305747314
iteration : 4433
train acc:  0.625
train loss:  0.6045351028442383
train gradient:  0.19088470188660214
iteration : 4434
train acc:  0.7421875
train loss:  0.5146340131759644
train gradient:  0.15561354837626468
iteration : 4435
train acc:  0.703125
train loss:  0.4956877827644348
train gradient:  0.14016920524169174
iteration : 4436
train acc:  0.765625
train loss:  0.45716947317123413
train gradient:  0.17418465876026507
iteration : 4437
train acc:  0.734375
train loss:  0.5154486894607544
train gradient:  0.11536783490094625
iteration : 4438
train acc:  0.78125
train loss:  0.48312613368034363
train gradient:  0.11099985000525676
iteration : 4439
train acc:  0.71875
train loss:  0.5292283296585083
train gradient:  0.14297088214957904
iteration : 4440
train acc:  0.7421875
train loss:  0.4888894557952881
train gradient:  0.1299105016031823
iteration : 4441
train acc:  0.7734375
train loss:  0.4823824167251587
train gradient:  0.16139231515457197
iteration : 4442
train acc:  0.6875
train loss:  0.5444158315658569
train gradient:  0.1751506602648606
iteration : 4443
train acc:  0.6953125
train loss:  0.5370295643806458
train gradient:  0.18058309836094633
iteration : 4444
train acc:  0.7421875
train loss:  0.488439679145813
train gradient:  0.14901468241500643
iteration : 4445
train acc:  0.7578125
train loss:  0.5163456201553345
train gradient:  0.15679052188687392
iteration : 4446
train acc:  0.7421875
train loss:  0.5073549747467041
train gradient:  0.1463303207809511
iteration : 4447
train acc:  0.71875
train loss:  0.525383472442627
train gradient:  0.1305960238408128
iteration : 4448
train acc:  0.7421875
train loss:  0.4621502161026001
train gradient:  0.15297103251654298
iteration : 4449
train acc:  0.7578125
train loss:  0.49817004799842834
train gradient:  0.17075988632731903
iteration : 4450
train acc:  0.6953125
train loss:  0.5194058418273926
train gradient:  0.18457143587902436
iteration : 4451
train acc:  0.703125
train loss:  0.5463712215423584
train gradient:  0.1876879199535658
iteration : 4452
train acc:  0.6953125
train loss:  0.5144253969192505
train gradient:  0.15484114158549994
iteration : 4453
train acc:  0.765625
train loss:  0.5303910970687866
train gradient:  0.1636151088185185
iteration : 4454
train acc:  0.6875
train loss:  0.520318865776062
train gradient:  0.1731624797138986
iteration : 4455
train acc:  0.7578125
train loss:  0.47437265515327454
train gradient:  0.1354325795773057
iteration : 4456
train acc:  0.6875
train loss:  0.5158874988555908
train gradient:  0.16321730777088805
iteration : 4457
train acc:  0.6796875
train loss:  0.5807185173034668
train gradient:  0.20231309182961174
iteration : 4458
train acc:  0.6640625
train loss:  0.583296537399292
train gradient:  0.2563032533200915
iteration : 4459
train acc:  0.71875
train loss:  0.5233802795410156
train gradient:  0.16505771484494336
iteration : 4460
train acc:  0.75
train loss:  0.5229009389877319
train gradient:  0.14090121063617794
iteration : 4461
train acc:  0.7421875
train loss:  0.4964084327220917
train gradient:  0.12964766491710406
iteration : 4462
train acc:  0.75
train loss:  0.5218296647071838
train gradient:  0.17398020524948438
iteration : 4463
train acc:  0.65625
train loss:  0.5808776617050171
train gradient:  0.16196483833759356
iteration : 4464
train acc:  0.7421875
train loss:  0.49620333313941956
train gradient:  0.133028390703704
iteration : 4465
train acc:  0.65625
train loss:  0.5546634793281555
train gradient:  0.17029555649264416
iteration : 4466
train acc:  0.7890625
train loss:  0.4463309645652771
train gradient:  0.13445430191704472
iteration : 4467
train acc:  0.796875
train loss:  0.46048444509506226
train gradient:  0.1621549069980195
iteration : 4468
train acc:  0.7265625
train loss:  0.5632404088973999
train gradient:  0.175201369186516
iteration : 4469
train acc:  0.6953125
train loss:  0.546378493309021
train gradient:  0.14881080987283316
iteration : 4470
train acc:  0.734375
train loss:  0.4771093726158142
train gradient:  0.12121076557061625
iteration : 4471
train acc:  0.71875
train loss:  0.5220918655395508
train gradient:  0.13104927928181417
iteration : 4472
train acc:  0.6953125
train loss:  0.5596622228622437
train gradient:  0.1806986218650692
iteration : 4473
train acc:  0.703125
train loss:  0.4807857275009155
train gradient:  0.13217467494467056
iteration : 4474
train acc:  0.703125
train loss:  0.5256100296974182
train gradient:  0.26805826937990695
iteration : 4475
train acc:  0.75
train loss:  0.4899425506591797
train gradient:  0.14701387483855605
iteration : 4476
train acc:  0.7890625
train loss:  0.46091753244400024
train gradient:  0.10761645546396356
iteration : 4477
train acc:  0.7734375
train loss:  0.5611415505409241
train gradient:  0.16828333950385765
iteration : 4478
train acc:  0.7578125
train loss:  0.49881234765052795
train gradient:  0.13461732000947613
iteration : 4479
train acc:  0.765625
train loss:  0.4433807134628296
train gradient:  0.12176278082414721
iteration : 4480
train acc:  0.734375
train loss:  0.5566694140434265
train gradient:  0.18006732072391635
iteration : 4481
train acc:  0.6875
train loss:  0.5722736120223999
train gradient:  0.1961879175360038
iteration : 4482
train acc:  0.765625
train loss:  0.5092326998710632
train gradient:  0.16044143882559336
iteration : 4483
train acc:  0.6953125
train loss:  0.5890657901763916
train gradient:  0.21886463184429372
iteration : 4484
train acc:  0.6875
train loss:  0.5637645125389099
train gradient:  0.1657358428767523
iteration : 4485
train acc:  0.7578125
train loss:  0.4722449779510498
train gradient:  0.1654003084393198
iteration : 4486
train acc:  0.7109375
train loss:  0.5469261407852173
train gradient:  0.18412549352344748
iteration : 4487
train acc:  0.8125
train loss:  0.4396149516105652
train gradient:  0.13087750579041801
iteration : 4488
train acc:  0.7578125
train loss:  0.46250706911087036
train gradient:  0.12909244291876054
iteration : 4489
train acc:  0.625
train loss:  0.6170769929885864
train gradient:  0.17366681142491555
iteration : 4490
train acc:  0.734375
train loss:  0.515217661857605
train gradient:  0.18150624419474154
iteration : 4491
train acc:  0.7421875
train loss:  0.5571324825286865
train gradient:  0.17476527757954244
iteration : 4492
train acc:  0.7421875
train loss:  0.5002667307853699
train gradient:  0.20245552647247173
iteration : 4493
train acc:  0.7578125
train loss:  0.49831077456474304
train gradient:  0.14147307470170198
iteration : 4494
train acc:  0.6953125
train loss:  0.5617445111274719
train gradient:  0.20782727074111917
iteration : 4495
train acc:  0.7890625
train loss:  0.44384509325027466
train gradient:  0.13917207425660766
iteration : 4496
train acc:  0.734375
train loss:  0.5439297556877136
train gradient:  0.19275999460980137
iteration : 4497
train acc:  0.6875
train loss:  0.5770410299301147
train gradient:  0.2025542453068192
iteration : 4498
train acc:  0.734375
train loss:  0.5071806311607361
train gradient:  0.18078449145174663
iteration : 4499
train acc:  0.78125
train loss:  0.4843063950538635
train gradient:  0.13153430136407163
iteration : 4500
train acc:  0.71875
train loss:  0.4842326045036316
train gradient:  0.11493482758122404
iteration : 4501
train acc:  0.7734375
train loss:  0.4800463318824768
train gradient:  0.12177971041534669
iteration : 4502
train acc:  0.7109375
train loss:  0.5129475593566895
train gradient:  0.15694995669105183
iteration : 4503
train acc:  0.8125
train loss:  0.45098876953125
train gradient:  0.10602578486832673
iteration : 4504
train acc:  0.75
train loss:  0.5248152613639832
train gradient:  0.18154334690843588
iteration : 4505
train acc:  0.796875
train loss:  0.5042480826377869
train gradient:  0.15001644740408865
iteration : 4506
train acc:  0.7109375
train loss:  0.49057114124298096
train gradient:  0.1278002459601251
iteration : 4507
train acc:  0.765625
train loss:  0.5336266756057739
train gradient:  0.17597727841086916
iteration : 4508
train acc:  0.71875
train loss:  0.5149176716804504
train gradient:  0.1235779358646188
iteration : 4509
train acc:  0.75
train loss:  0.4947376549243927
train gradient:  0.15374215913742084
iteration : 4510
train acc:  0.71875
train loss:  0.5253115296363831
train gradient:  0.15760073973135164
iteration : 4511
train acc:  0.6953125
train loss:  0.5452812910079956
train gradient:  0.16879035785335128
iteration : 4512
train acc:  0.8203125
train loss:  0.47076401114463806
train gradient:  0.19786653170300733
iteration : 4513
train acc:  0.7265625
train loss:  0.5713177919387817
train gradient:  0.15544579353188673
iteration : 4514
train acc:  0.734375
train loss:  0.4942137598991394
train gradient:  0.1329695206530705
iteration : 4515
train acc:  0.734375
train loss:  0.4858456254005432
train gradient:  0.15728997686746488
iteration : 4516
train acc:  0.6640625
train loss:  0.5783382654190063
train gradient:  0.15796939705871482
iteration : 4517
train acc:  0.734375
train loss:  0.5136605501174927
train gradient:  0.1481360023986176
iteration : 4518
train acc:  0.6640625
train loss:  0.5610924959182739
train gradient:  0.16320993731367522
iteration : 4519
train acc:  0.7109375
train loss:  0.5260546803474426
train gradient:  0.15226424591406462
iteration : 4520
train acc:  0.65625
train loss:  0.560620129108429
train gradient:  0.2561473810120633
iteration : 4521
train acc:  0.6875
train loss:  0.5418410897254944
train gradient:  0.16030324843152935
iteration : 4522
train acc:  0.734375
train loss:  0.4818578362464905
train gradient:  0.15854942473235467
iteration : 4523
train acc:  0.71875
train loss:  0.5417498350143433
train gradient:  0.20467815767809283
iteration : 4524
train acc:  0.7265625
train loss:  0.49936240911483765
train gradient:  0.11884101749349305
iteration : 4525
train acc:  0.7109375
train loss:  0.537263810634613
train gradient:  0.2179038995270014
iteration : 4526
train acc:  0.7109375
train loss:  0.4923473298549652
train gradient:  0.17275001017147945
iteration : 4527
train acc:  0.8125
train loss:  0.4716436266899109
train gradient:  0.12975758421018388
iteration : 4528
train acc:  0.7421875
train loss:  0.5178054571151733
train gradient:  0.15789087120790932
iteration : 4529
train acc:  0.6484375
train loss:  0.5629196166992188
train gradient:  0.22076647528429016
iteration : 4530
train acc:  0.75
train loss:  0.5316262245178223
train gradient:  0.17677567317174694
iteration : 4531
train acc:  0.7734375
train loss:  0.487632691860199
train gradient:  0.1500871961483406
iteration : 4532
train acc:  0.7578125
train loss:  0.48802435398101807
train gradient:  0.1365099557169201
iteration : 4533
train acc:  0.765625
train loss:  0.5264240503311157
train gradient:  0.1618776872112574
iteration : 4534
train acc:  0.78125
train loss:  0.46400657296180725
train gradient:  0.1314197062402332
iteration : 4535
train acc:  0.7109375
train loss:  0.5505304336547852
train gradient:  0.21258464882767772
iteration : 4536
train acc:  0.75
train loss:  0.510542094707489
train gradient:  0.1852808915796873
iteration : 4537
train acc:  0.71875
train loss:  0.567683219909668
train gradient:  0.17049537284568134
iteration : 4538
train acc:  0.734375
train loss:  0.4900813400745392
train gradient:  0.22528380205410842
iteration : 4539
train acc:  0.734375
train loss:  0.562333881855011
train gradient:  0.22719691205388287
iteration : 4540
train acc:  0.78125
train loss:  0.49796974658966064
train gradient:  0.12046292807598301
iteration : 4541
train acc:  0.765625
train loss:  0.47264987230300903
train gradient:  0.1487571352042778
iteration : 4542
train acc:  0.6875
train loss:  0.5380833745002747
train gradient:  0.1532071771246382
iteration : 4543
train acc:  0.7578125
train loss:  0.470172643661499
train gradient:  0.1263533654781826
iteration : 4544
train acc:  0.78125
train loss:  0.44544756412506104
train gradient:  0.12086669313715874
iteration : 4545
train acc:  0.734375
train loss:  0.49653375148773193
train gradient:  0.14321386712053064
iteration : 4546
train acc:  0.7578125
train loss:  0.5449632406234741
train gradient:  0.17500773482575435
iteration : 4547
train acc:  0.765625
train loss:  0.4560963213443756
train gradient:  0.15693632830092769
iteration : 4548
train acc:  0.7890625
train loss:  0.40623414516448975
train gradient:  0.11425750462870417
iteration : 4549
train acc:  0.75
train loss:  0.5204697847366333
train gradient:  0.13577660453024104
iteration : 4550
train acc:  0.703125
train loss:  0.5396559834480286
train gradient:  0.17630333402469728
iteration : 4551
train acc:  0.6640625
train loss:  0.6085931658744812
train gradient:  0.1656294959020248
iteration : 4552
train acc:  0.8125
train loss:  0.42258110642433167
train gradient:  0.1459385144007519
iteration : 4553
train acc:  0.75
train loss:  0.5136722922325134
train gradient:  0.1853397226913039
iteration : 4554
train acc:  0.734375
train loss:  0.5322907567024231
train gradient:  0.1926320909146813
iteration : 4555
train acc:  0.7734375
train loss:  0.4806136190891266
train gradient:  0.13684853672677172
iteration : 4556
train acc:  0.7421875
train loss:  0.5152257680892944
train gradient:  0.1596778891438168
iteration : 4557
train acc:  0.6328125
train loss:  0.5939590334892273
train gradient:  0.2011713467948395
iteration : 4558
train acc:  0.7109375
train loss:  0.5441850423812866
train gradient:  0.18592395811955098
iteration : 4559
train acc:  0.6875
train loss:  0.5525414943695068
train gradient:  0.20805078315544062
iteration : 4560
train acc:  0.7734375
train loss:  0.4640897810459137
train gradient:  0.12611392924001502
iteration : 4561
train acc:  0.734375
train loss:  0.4595409631729126
train gradient:  0.12450830752493408
iteration : 4562
train acc:  0.7109375
train loss:  0.5427281856536865
train gradient:  0.1978969425864417
iteration : 4563
train acc:  0.78125
train loss:  0.455650269985199
train gradient:  0.12358794297350405
iteration : 4564
train acc:  0.75
train loss:  0.5198522806167603
train gradient:  0.17468992036389164
iteration : 4565
train acc:  0.75
train loss:  0.5008996725082397
train gradient:  0.15136051469069126
iteration : 4566
train acc:  0.7734375
train loss:  0.4940151572227478
train gradient:  0.12820334758295587
iteration : 4567
train acc:  0.703125
train loss:  0.5002532005310059
train gradient:  0.13671356236332433
iteration : 4568
train acc:  0.71875
train loss:  0.544100284576416
train gradient:  0.1463855528491642
iteration : 4569
train acc:  0.71875
train loss:  0.5533921718597412
train gradient:  0.2138586024111695
iteration : 4570
train acc:  0.7421875
train loss:  0.5389244556427002
train gradient:  0.30473799621080533
iteration : 4571
train acc:  0.7734375
train loss:  0.49726057052612305
train gradient:  0.1511178026280674
iteration : 4572
train acc:  0.734375
train loss:  0.4835875928401947
train gradient:  0.1287313025611888
iteration : 4573
train acc:  0.71875
train loss:  0.5220308303833008
train gradient:  0.14667937027179195
iteration : 4574
train acc:  0.8046875
train loss:  0.44107896089553833
train gradient:  0.13769285316711408
iteration : 4575
train acc:  0.7421875
train loss:  0.528864860534668
train gradient:  0.1440279231821074
iteration : 4576
train acc:  0.765625
train loss:  0.5257205367088318
train gradient:  0.17192083926650809
iteration : 4577
train acc:  0.7578125
train loss:  0.5194554328918457
train gradient:  0.16319063076817852
iteration : 4578
train acc:  0.7421875
train loss:  0.5095987319946289
train gradient:  0.1571686513671226
iteration : 4579
train acc:  0.703125
train loss:  0.5438126921653748
train gradient:  0.18299107942420623
iteration : 4580
train acc:  0.734375
train loss:  0.5166780948638916
train gradient:  0.18272496711325104
iteration : 4581
train acc:  0.6796875
train loss:  0.6166506409645081
train gradient:  0.23487067834680692
iteration : 4582
train acc:  0.703125
train loss:  0.4849860668182373
train gradient:  0.14127882591322022
iteration : 4583
train acc:  0.75
train loss:  0.463478684425354
train gradient:  0.10221457277537878
iteration : 4584
train acc:  0.71875
train loss:  0.5730394124984741
train gradient:  0.1535780426570672
iteration : 4585
train acc:  0.703125
train loss:  0.5683720111846924
train gradient:  0.20680751408747822
iteration : 4586
train acc:  0.734375
train loss:  0.516507625579834
train gradient:  0.1650589362059568
iteration : 4587
train acc:  0.71875
train loss:  0.4956092834472656
train gradient:  0.1817466380260328
iteration : 4588
train acc:  0.71875
train loss:  0.5234050154685974
train gradient:  0.14986142416572032
iteration : 4589
train acc:  0.6875
train loss:  0.5577317476272583
train gradient:  0.17685201843341014
iteration : 4590
train acc:  0.6875
train loss:  0.5978416800498962
train gradient:  0.16338785227123473
iteration : 4591
train acc:  0.6875
train loss:  0.5305463075637817
train gradient:  0.175494275238748
iteration : 4592
train acc:  0.75
train loss:  0.5501360297203064
train gradient:  0.2230589962668263
iteration : 4593
train acc:  0.7578125
train loss:  0.5329097509384155
train gradient:  0.18243950251451801
iteration : 4594
train acc:  0.6875
train loss:  0.5826521515846252
train gradient:  0.14721232342770246
iteration : 4595
train acc:  0.7734375
train loss:  0.5207743644714355
train gradient:  0.13751652498696526
iteration : 4596
train acc:  0.6640625
train loss:  0.5816994309425354
train gradient:  0.19447484528547693
iteration : 4597
train acc:  0.6796875
train loss:  0.5045874118804932
train gradient:  0.12094795726233236
iteration : 4598
train acc:  0.7109375
train loss:  0.5049707889556885
train gradient:  0.15885148797888216
iteration : 4599
train acc:  0.7265625
train loss:  0.5327274203300476
train gradient:  0.13912387488971828
iteration : 4600
train acc:  0.7265625
train loss:  0.5218327045440674
train gradient:  0.15209880072994938
iteration : 4601
train acc:  0.671875
train loss:  0.5604210495948792
train gradient:  0.17512329613851832
iteration : 4602
train acc:  0.6953125
train loss:  0.549953281879425
train gradient:  0.1446108027910145
iteration : 4603
train acc:  0.765625
train loss:  0.5234229564666748
train gradient:  0.18905156860371286
iteration : 4604
train acc:  0.6640625
train loss:  0.568338930606842
train gradient:  0.19071520267922842
iteration : 4605
train acc:  0.6953125
train loss:  0.5129315257072449
train gradient:  0.14789285892187123
iteration : 4606
train acc:  0.7890625
train loss:  0.4364698529243469
train gradient:  0.11954111630847994
iteration : 4607
train acc:  0.6953125
train loss:  0.5491125583648682
train gradient:  0.15719261340309937
iteration : 4608
train acc:  0.734375
train loss:  0.5348571538925171
train gradient:  0.16525300754027628
iteration : 4609
train acc:  0.7578125
train loss:  0.4733782112598419
train gradient:  0.14725339550253752
iteration : 4610
train acc:  0.71875
train loss:  0.5179046392440796
train gradient:  0.16343234659416125
iteration : 4611
train acc:  0.7265625
train loss:  0.5192060470581055
train gradient:  0.1726143329863783
iteration : 4612
train acc:  0.6953125
train loss:  0.5743544101715088
train gradient:  0.21070394073227744
iteration : 4613
train acc:  0.78125
train loss:  0.45800524950027466
train gradient:  0.18818698586688926
iteration : 4614
train acc:  0.6796875
train loss:  0.5302026271820068
train gradient:  0.16062307778574358
iteration : 4615
train acc:  0.703125
train loss:  0.5339891910552979
train gradient:  0.17210949969301004
iteration : 4616
train acc:  0.671875
train loss:  0.6011768579483032
train gradient:  0.21175610111347154
iteration : 4617
train acc:  0.703125
train loss:  0.5265904664993286
train gradient:  0.1354053040523791
iteration : 4618
train acc:  0.71875
train loss:  0.5069354772567749
train gradient:  0.1476544819690488
iteration : 4619
train acc:  0.734375
train loss:  0.5160380005836487
train gradient:  0.17924185217386973
iteration : 4620
train acc:  0.734375
train loss:  0.5058414936065674
train gradient:  0.1490038205095699
iteration : 4621
train acc:  0.71875
train loss:  0.5016468167304993
train gradient:  0.14737642021512215
iteration : 4622
train acc:  0.65625
train loss:  0.5474053025245667
train gradient:  0.1385271276685177
iteration : 4623
train acc:  0.734375
train loss:  0.5340255498886108
train gradient:  0.1481658618423574
iteration : 4624
train acc:  0.7578125
train loss:  0.4795982837677002
train gradient:  0.12704759625416134
iteration : 4625
train acc:  0.7734375
train loss:  0.5296228528022766
train gradient:  0.19900015185827602
iteration : 4626
train acc:  0.6953125
train loss:  0.5764214992523193
train gradient:  0.16929190593259402
iteration : 4627
train acc:  0.7421875
train loss:  0.5491523742675781
train gradient:  0.14717821278041016
iteration : 4628
train acc:  0.8203125
train loss:  0.49084097146987915
train gradient:  0.14564682723896577
iteration : 4629
train acc:  0.8125
train loss:  0.46484220027923584
train gradient:  0.1346406714321819
iteration : 4630
train acc:  0.65625
train loss:  0.6061173677444458
train gradient:  0.1601732692458634
iteration : 4631
train acc:  0.6796875
train loss:  0.5320473313331604
train gradient:  0.17803785727944096
iteration : 4632
train acc:  0.6796875
train loss:  0.5257989764213562
train gradient:  0.13375191419695326
iteration : 4633
train acc:  0.75
train loss:  0.5061935186386108
train gradient:  0.15401931184736184
iteration : 4634
train acc:  0.7421875
train loss:  0.5020542144775391
train gradient:  0.17914546242359752
iteration : 4635
train acc:  0.734375
train loss:  0.4958280026912689
train gradient:  0.141054336706402
iteration : 4636
train acc:  0.7109375
train loss:  0.5841174721717834
train gradient:  0.1992457681033113
iteration : 4637
train acc:  0.703125
train loss:  0.5133600234985352
train gradient:  0.1863967205210173
iteration : 4638
train acc:  0.703125
train loss:  0.5448634028434753
train gradient:  0.15138418909499693
iteration : 4639
train acc:  0.71875
train loss:  0.5276541709899902
train gradient:  0.17607516342244708
iteration : 4640
train acc:  0.6796875
train loss:  0.5717079639434814
train gradient:  0.19724881296393862
iteration : 4641
train acc:  0.71875
train loss:  0.5291173458099365
train gradient:  0.1576945741489463
iteration : 4642
train acc:  0.7734375
train loss:  0.4520748257637024
train gradient:  0.1307873049315179
iteration : 4643
train acc:  0.734375
train loss:  0.5323636531829834
train gradient:  0.14627271128381988
iteration : 4644
train acc:  0.7578125
train loss:  0.5012519359588623
train gradient:  0.15251687930818147
iteration : 4645
train acc:  0.7734375
train loss:  0.47545379400253296
train gradient:  0.1384263168138144
iteration : 4646
train acc:  0.7265625
train loss:  0.48565688729286194
train gradient:  0.13490200074890735
iteration : 4647
train acc:  0.6640625
train loss:  0.602968692779541
train gradient:  0.1827535109578848
iteration : 4648
train acc:  0.7421875
train loss:  0.4764372706413269
train gradient:  0.1550106461496446
iteration : 4649
train acc:  0.671875
train loss:  0.5381366014480591
train gradient:  0.1584869465033688
iteration : 4650
train acc:  0.7109375
train loss:  0.5508387088775635
train gradient:  0.14705241413547343
iteration : 4651
train acc:  0.75
train loss:  0.49350449442863464
train gradient:  0.15464418434884267
iteration : 4652
train acc:  0.8046875
train loss:  0.40385085344314575
train gradient:  0.12880638124486776
iteration : 4653
train acc:  0.75
train loss:  0.5145955085754395
train gradient:  0.20116037450687366
iteration : 4654
train acc:  0.734375
train loss:  0.5145323872566223
train gradient:  0.17936061036185805
iteration : 4655
train acc:  0.7734375
train loss:  0.4894423186779022
train gradient:  0.1267464818948136
iteration : 4656
train acc:  0.703125
train loss:  0.5121786594390869
train gradient:  0.14407243641627832
iteration : 4657
train acc:  0.71875
train loss:  0.5644317865371704
train gradient:  0.17155303032181485
iteration : 4658
train acc:  0.703125
train loss:  0.5637611746788025
train gradient:  0.24455441943104267
iteration : 4659
train acc:  0.6875
train loss:  0.5412741899490356
train gradient:  0.22716076497625387
iteration : 4660
train acc:  0.7734375
train loss:  0.45465534925460815
train gradient:  0.12141551876281713
iteration : 4661
train acc:  0.703125
train loss:  0.5436410307884216
train gradient:  0.17541653353752562
iteration : 4662
train acc:  0.6796875
train loss:  0.6211830377578735
train gradient:  0.27813615062629776
iteration : 4663
train acc:  0.7890625
train loss:  0.4815645217895508
train gradient:  0.1500109686287322
iteration : 4664
train acc:  0.7109375
train loss:  0.5364355444908142
train gradient:  0.19480239404708322
iteration : 4665
train acc:  0.71875
train loss:  0.5402847528457642
train gradient:  0.17998413351233297
iteration : 4666
train acc:  0.7578125
train loss:  0.5240819454193115
train gradient:  0.1635199206742291
iteration : 4667
train acc:  0.6328125
train loss:  0.6165534257888794
train gradient:  0.2279278233491432
iteration : 4668
train acc:  0.75
train loss:  0.514756441116333
train gradient:  0.16665431093034672
iteration : 4669
train acc:  0.7109375
train loss:  0.48767784237861633
train gradient:  0.13929027871273236
iteration : 4670
train acc:  0.6484375
train loss:  0.5776594877243042
train gradient:  0.20575738605486948
iteration : 4671
train acc:  0.78125
train loss:  0.4898603558540344
train gradient:  0.13872345045873868
iteration : 4672
train acc:  0.7421875
train loss:  0.49160146713256836
train gradient:  0.12993379947199388
iteration : 4673
train acc:  0.6875
train loss:  0.6059841513633728
train gradient:  0.1819259253729793
iteration : 4674
train acc:  0.7421875
train loss:  0.4947165250778198
train gradient:  0.14699101737814424
iteration : 4675
train acc:  0.6796875
train loss:  0.5677657127380371
train gradient:  0.18589203620862288
iteration : 4676
train acc:  0.734375
train loss:  0.5387073755264282
train gradient:  0.18687390593257558
iteration : 4677
train acc:  0.6796875
train loss:  0.5947640538215637
train gradient:  0.18029700445158792
iteration : 4678
train acc:  0.78125
train loss:  0.47069019079208374
train gradient:  0.16568925587865863
iteration : 4679
train acc:  0.78125
train loss:  0.5001505017280579
train gradient:  0.14696376672928824
iteration : 4680
train acc:  0.734375
train loss:  0.5265049338340759
train gradient:  0.1255403368575062
iteration : 4681
train acc:  0.78125
train loss:  0.4701562523841858
train gradient:  0.15251988889737195
iteration : 4682
train acc:  0.734375
train loss:  0.5361115336418152
train gradient:  0.1615834409424431
iteration : 4683
train acc:  0.7734375
train loss:  0.48226457834243774
train gradient:  0.15709966460381217
iteration : 4684
train acc:  0.6640625
train loss:  0.5368911027908325
train gradient:  0.19697273220312933
iteration : 4685
train acc:  0.6796875
train loss:  0.5654728412628174
train gradient:  0.223363133915554
iteration : 4686
train acc:  0.7734375
train loss:  0.48249906301498413
train gradient:  0.13295874887985804
iteration : 4687
train acc:  0.71875
train loss:  0.5278031826019287
train gradient:  0.17292222881969588
iteration : 4688
train acc:  0.7421875
train loss:  0.5085256695747375
train gradient:  0.14753797292202714
iteration : 4689
train acc:  0.6875
train loss:  0.5346195697784424
train gradient:  0.14375168384782386
iteration : 4690
train acc:  0.71875
train loss:  0.49470627307891846
train gradient:  0.1756790883936557
iteration : 4691
train acc:  0.71875
train loss:  0.5306714773178101
train gradient:  0.17640479349688487
iteration : 4692
train acc:  0.7109375
train loss:  0.554192066192627
train gradient:  0.18892261525674092
iteration : 4693
train acc:  0.6796875
train loss:  0.5937647819519043
train gradient:  0.1509250792125313
iteration : 4694
train acc:  0.7109375
train loss:  0.522322952747345
train gradient:  0.22501500015723297
iteration : 4695
train acc:  0.71875
train loss:  0.519436776638031
train gradient:  0.1398674022553611
iteration : 4696
train acc:  0.765625
train loss:  0.4814522862434387
train gradient:  0.15812512009291832
iteration : 4697
train acc:  0.703125
train loss:  0.5546097755432129
train gradient:  0.15135354216170696
iteration : 4698
train acc:  0.75
train loss:  0.43303826451301575
train gradient:  0.10787753332752878
iteration : 4699
train acc:  0.7578125
train loss:  0.5256729125976562
train gradient:  0.17847482904655054
iteration : 4700
train acc:  0.6953125
train loss:  0.5347459316253662
train gradient:  0.15155706905245622
iteration : 4701
train acc:  0.71875
train loss:  0.5297999382019043
train gradient:  0.1705584937059027
iteration : 4702
train acc:  0.7734375
train loss:  0.47595465183258057
train gradient:  0.1298593327364124
iteration : 4703
train acc:  0.765625
train loss:  0.4929524064064026
train gradient:  0.13794727731362133
iteration : 4704
train acc:  0.734375
train loss:  0.5258716344833374
train gradient:  0.1663399359462713
iteration : 4705
train acc:  0.703125
train loss:  0.5700804591178894
train gradient:  0.18352362580481552
iteration : 4706
train acc:  0.71875
train loss:  0.49797046184539795
train gradient:  0.12233023340861592
iteration : 4707
train acc:  0.6953125
train loss:  0.5684707760810852
train gradient:  0.15201627030279333
iteration : 4708
train acc:  0.6796875
train loss:  0.5848268270492554
train gradient:  0.1906462189676642
iteration : 4709
train acc:  0.7109375
train loss:  0.5153394341468811
train gradient:  0.132437470431642
iteration : 4710
train acc:  0.7578125
train loss:  0.5259654521942139
train gradient:  0.22466117888039683
iteration : 4711
train acc:  0.65625
train loss:  0.6143642067909241
train gradient:  0.21046836783689377
iteration : 4712
train acc:  0.734375
train loss:  0.48711246252059937
train gradient:  0.1475004596940952
iteration : 4713
train acc:  0.7734375
train loss:  0.4787202775478363
train gradient:  0.11979780665011322
iteration : 4714
train acc:  0.7265625
train loss:  0.5355903506278992
train gradient:  0.157087553019779
iteration : 4715
train acc:  0.7265625
train loss:  0.4894706606864929
train gradient:  0.11734275740641369
iteration : 4716
train acc:  0.6640625
train loss:  0.598659336566925
train gradient:  0.19283516621526287
iteration : 4717
train acc:  0.6875
train loss:  0.5095911622047424
train gradient:  0.12661839876795689
iteration : 4718
train acc:  0.7734375
train loss:  0.43198680877685547
train gradient:  0.10205922870165254
iteration : 4719
train acc:  0.7578125
train loss:  0.4747399091720581
train gradient:  0.14707901591848638
iteration : 4720
train acc:  0.75
train loss:  0.4923335313796997
train gradient:  0.16964540943192088
iteration : 4721
train acc:  0.6953125
train loss:  0.5603954792022705
train gradient:  0.2231518841443187
iteration : 4722
train acc:  0.7265625
train loss:  0.5300366878509521
train gradient:  0.14984157320821842
iteration : 4723
train acc:  0.734375
train loss:  0.5238772034645081
train gradient:  0.16465751791368768
iteration : 4724
train acc:  0.75
train loss:  0.47830069065093994
train gradient:  0.11513586703282533
iteration : 4725
train acc:  0.7265625
train loss:  0.5518506169319153
train gradient:  0.12745444658537886
iteration : 4726
train acc:  0.7265625
train loss:  0.5263526439666748
train gradient:  0.2043200463879891
iteration : 4727
train acc:  0.7578125
train loss:  0.47789186239242554
train gradient:  0.12459104849097051
iteration : 4728
train acc:  0.734375
train loss:  0.4965367019176483
train gradient:  0.19359618136348183
iteration : 4729
train acc:  0.7578125
train loss:  0.47594332695007324
train gradient:  0.11876657355092965
iteration : 4730
train acc:  0.7421875
train loss:  0.4739728569984436
train gradient:  0.14356114715986112
iteration : 4731
train acc:  0.6796875
train loss:  0.6228866577148438
train gradient:  0.2530766363803477
iteration : 4732
train acc:  0.7265625
train loss:  0.49034562706947327
train gradient:  0.11143374122737694
iteration : 4733
train acc:  0.7109375
train loss:  0.5271461606025696
train gradient:  0.13249031705345718
iteration : 4734
train acc:  0.7109375
train loss:  0.5363573431968689
train gradient:  0.15299281072971313
iteration : 4735
train acc:  0.6875
train loss:  0.5378900766372681
train gradient:  0.16381102391931612
iteration : 4736
train acc:  0.7265625
train loss:  0.5205308198928833
train gradient:  0.15783767875400484
iteration : 4737
train acc:  0.7578125
train loss:  0.5115890502929688
train gradient:  0.15285528970984835
iteration : 4738
train acc:  0.65625
train loss:  0.5545992851257324
train gradient:  0.1292919004513826
iteration : 4739
train acc:  0.71875
train loss:  0.5334534645080566
train gradient:  0.20449288830598472
iteration : 4740
train acc:  0.7734375
train loss:  0.4528490900993347
train gradient:  0.11533968910484313
iteration : 4741
train acc:  0.6953125
train loss:  0.558887779712677
train gradient:  0.15610759659412946
iteration : 4742
train acc:  0.71875
train loss:  0.5708776712417603
train gradient:  0.16671113864552428
iteration : 4743
train acc:  0.7421875
train loss:  0.5197166204452515
train gradient:  0.14020099668062408
iteration : 4744
train acc:  0.71875
train loss:  0.5662493705749512
train gradient:  0.20853945724547812
iteration : 4745
train acc:  0.6953125
train loss:  0.5551325082778931
train gradient:  0.20405736389254187
iteration : 4746
train acc:  0.765625
train loss:  0.4806669056415558
train gradient:  0.13379898030667606
iteration : 4747
train acc:  0.6796875
train loss:  0.5667093396186829
train gradient:  0.1443181207095236
iteration : 4748
train acc:  0.7578125
train loss:  0.4896338880062103
train gradient:  0.13454452295656028
iteration : 4749
train acc:  0.7265625
train loss:  0.4838082194328308
train gradient:  0.1387617527184833
iteration : 4750
train acc:  0.765625
train loss:  0.4415409564971924
train gradient:  0.12447236929968686
iteration : 4751
train acc:  0.71875
train loss:  0.5833665132522583
train gradient:  0.17036232938130225
iteration : 4752
train acc:  0.7578125
train loss:  0.4735329747200012
train gradient:  0.14604626315943392
iteration : 4753
train acc:  0.734375
train loss:  0.5285985469818115
train gradient:  0.20696237867895848
iteration : 4754
train acc:  0.6640625
train loss:  0.5527839660644531
train gradient:  0.21497658108574608
iteration : 4755
train acc:  0.734375
train loss:  0.4680840075016022
train gradient:  0.1397000391794295
iteration : 4756
train acc:  0.7578125
train loss:  0.496421217918396
train gradient:  0.20017741833777147
iteration : 4757
train acc:  0.75
train loss:  0.49132752418518066
train gradient:  0.15475775709786432
iteration : 4758
train acc:  0.7265625
train loss:  0.4977613091468811
train gradient:  0.16016713184071224
iteration : 4759
train acc:  0.78125
train loss:  0.4853329360485077
train gradient:  0.10970595190742682
iteration : 4760
train acc:  0.78125
train loss:  0.45961233973503113
train gradient:  0.119828112916316
iteration : 4761
train acc:  0.7421875
train loss:  0.5069679021835327
train gradient:  0.13975840808126383
iteration : 4762
train acc:  0.71875
train loss:  0.5864651203155518
train gradient:  0.2103571777073291
iteration : 4763
train acc:  0.7421875
train loss:  0.5139816999435425
train gradient:  0.14723836428213022
iteration : 4764
train acc:  0.7578125
train loss:  0.4507109224796295
train gradient:  0.11271213438813614
iteration : 4765
train acc:  0.734375
train loss:  0.5156985521316528
train gradient:  0.16732999135242818
iteration : 4766
train acc:  0.75
train loss:  0.5405392050743103
train gradient:  0.19031945933662436
iteration : 4767
train acc:  0.7265625
train loss:  0.5278668403625488
train gradient:  0.17312697835044338
iteration : 4768
train acc:  0.7265625
train loss:  0.5145516395568848
train gradient:  0.15812327568637086
iteration : 4769
train acc:  0.8046875
train loss:  0.43354135751724243
train gradient:  0.12850037263060138
iteration : 4770
train acc:  0.71875
train loss:  0.5652424097061157
train gradient:  0.15897919055713425
iteration : 4771
train acc:  0.6796875
train loss:  0.5549106597900391
train gradient:  0.16425131402333631
iteration : 4772
train acc:  0.609375
train loss:  0.6615991592407227
train gradient:  0.24907215024390006
iteration : 4773
train acc:  0.7421875
train loss:  0.490846186876297
train gradient:  0.14354171779315014
iteration : 4774
train acc:  0.75
train loss:  0.4734252095222473
train gradient:  0.12362839455601778
iteration : 4775
train acc:  0.765625
train loss:  0.5090727806091309
train gradient:  0.20351498864216797
iteration : 4776
train acc:  0.703125
train loss:  0.5170280933380127
train gradient:  0.15270218994599047
iteration : 4777
train acc:  0.765625
train loss:  0.4626501202583313
train gradient:  0.14053730588795768
iteration : 4778
train acc:  0.671875
train loss:  0.5752296447753906
train gradient:  0.21985716867942845
iteration : 4779
train acc:  0.703125
train loss:  0.5401691198348999
train gradient:  0.14930131409899408
iteration : 4780
train acc:  0.6953125
train loss:  0.5034946203231812
train gradient:  0.11891702757856092
iteration : 4781
train acc:  0.7265625
train loss:  0.5216395258903503
train gradient:  0.1646487964544221
iteration : 4782
train acc:  0.765625
train loss:  0.48239666223526
train gradient:  0.1592141112487184
iteration : 4783
train acc:  0.7421875
train loss:  0.4916371703147888
train gradient:  0.11029887114662137
iteration : 4784
train acc:  0.7265625
train loss:  0.5087676048278809
train gradient:  0.16442014817271872
iteration : 4785
train acc:  0.671875
train loss:  0.5385795831680298
train gradient:  0.14991981128302367
iteration : 4786
train acc:  0.7421875
train loss:  0.5287439823150635
train gradient:  0.1536266583014445
iteration : 4787
train acc:  0.765625
train loss:  0.46345600485801697
train gradient:  0.12009075028212401
iteration : 4788
train acc:  0.703125
train loss:  0.5921534895896912
train gradient:  0.15898380445692928
iteration : 4789
train acc:  0.734375
train loss:  0.5257357358932495
train gradient:  0.1319491691968639
iteration : 4790
train acc:  0.703125
train loss:  0.5494836568832397
train gradient:  0.1716220992344886
iteration : 4791
train acc:  0.6875
train loss:  0.5229302644729614
train gradient:  0.18107544985434396
iteration : 4792
train acc:  0.6796875
train loss:  0.5741608142852783
train gradient:  0.17111304773893132
iteration : 4793
train acc:  0.7421875
train loss:  0.5119065046310425
train gradient:  0.16277307750149783
iteration : 4794
train acc:  0.7421875
train loss:  0.47953081130981445
train gradient:  0.12203920544669929
iteration : 4795
train acc:  0.703125
train loss:  0.5848134160041809
train gradient:  0.18475716230652706
iteration : 4796
train acc:  0.6875
train loss:  0.5773619413375854
train gradient:  0.21745831235795643
iteration : 4797
train acc:  0.7421875
train loss:  0.5328774452209473
train gradient:  0.16679333101190277
iteration : 4798
train acc:  0.796875
train loss:  0.43172189593315125
train gradient:  0.1527071805017618
iteration : 4799
train acc:  0.8125
train loss:  0.4459368586540222
train gradient:  0.16776993911979615
iteration : 4800
train acc:  0.6953125
train loss:  0.5925139784812927
train gradient:  0.22501227212621583
iteration : 4801
train acc:  0.703125
train loss:  0.5064769387245178
train gradient:  0.15593730996201063
iteration : 4802
train acc:  0.8046875
train loss:  0.46093931794166565
train gradient:  0.11836435184479409
iteration : 4803
train acc:  0.7265625
train loss:  0.5397342443466187
train gradient:  0.1489064845556307
iteration : 4804
train acc:  0.6953125
train loss:  0.5554304122924805
train gradient:  0.1366280193339987
iteration : 4805
train acc:  0.71875
train loss:  0.5892438888549805
train gradient:  0.2193771997070469
iteration : 4806
train acc:  0.75
train loss:  0.49291834235191345
train gradient:  0.16475402800943983
iteration : 4807
train acc:  0.7578125
train loss:  0.4529023766517639
train gradient:  0.13472277308879582
iteration : 4808
train acc:  0.6875
train loss:  0.519761860370636
train gradient:  0.1470377247807172
iteration : 4809
train acc:  0.6953125
train loss:  0.5327935814857483
train gradient:  0.19949391614813775
iteration : 4810
train acc:  0.7265625
train loss:  0.47963541746139526
train gradient:  0.1153200930532487
iteration : 4811
train acc:  0.734375
train loss:  0.5474361777305603
train gradient:  0.15876539024513886
iteration : 4812
train acc:  0.75
train loss:  0.5084182620048523
train gradient:  0.1176713517592538
iteration : 4813
train acc:  0.8046875
train loss:  0.46549534797668457
train gradient:  0.1376561885589425
iteration : 4814
train acc:  0.6875
train loss:  0.5510386228561401
train gradient:  0.164653575203345
iteration : 4815
train acc:  0.78125
train loss:  0.48630213737487793
train gradient:  0.1601550950673461
iteration : 4816
train acc:  0.71875
train loss:  0.5029932260513306
train gradient:  0.13303253585196673
iteration : 4817
train acc:  0.6953125
train loss:  0.548941433429718
train gradient:  0.1640384956265152
iteration : 4818
train acc:  0.734375
train loss:  0.4927399158477783
train gradient:  0.1515266208622842
iteration : 4819
train acc:  0.6953125
train loss:  0.5374996066093445
train gradient:  0.12764979303060664
iteration : 4820
train acc:  0.7265625
train loss:  0.49674129486083984
train gradient:  0.14292912383601514
iteration : 4821
train acc:  0.7109375
train loss:  0.5435039401054382
train gradient:  0.19881792982937455
iteration : 4822
train acc:  0.7109375
train loss:  0.5672932863235474
train gradient:  0.16717258426868115
iteration : 4823
train acc:  0.78125
train loss:  0.4697945713996887
train gradient:  0.11237686622604694
iteration : 4824
train acc:  0.6953125
train loss:  0.5609269142150879
train gradient:  0.1884624199726586
iteration : 4825
train acc:  0.7578125
train loss:  0.513286292552948
train gradient:  0.17102830340004316
iteration : 4826
train acc:  0.7265625
train loss:  0.5095990300178528
train gradient:  0.1296244118730632
iteration : 4827
train acc:  0.765625
train loss:  0.4886772632598877
train gradient:  0.17708120607548222
iteration : 4828
train acc:  0.7265625
train loss:  0.5115022659301758
train gradient:  0.15163633748224337
iteration : 4829
train acc:  0.7890625
train loss:  0.4922899603843689
train gradient:  0.13355794990265124
iteration : 4830
train acc:  0.625
train loss:  0.6231526136398315
train gradient:  0.20102106481344445
iteration : 4831
train acc:  0.7109375
train loss:  0.5469893217086792
train gradient:  0.15751560539881396
iteration : 4832
train acc:  0.78125
train loss:  0.45884498953819275
train gradient:  0.12378439392095898
iteration : 4833
train acc:  0.6875
train loss:  0.5272337198257446
train gradient:  0.15676312183469177
iteration : 4834
train acc:  0.6484375
train loss:  0.5883147716522217
train gradient:  0.25920376720710975
iteration : 4835
train acc:  0.7421875
train loss:  0.49421072006225586
train gradient:  0.1735378031955675
iteration : 4836
train acc:  0.734375
train loss:  0.49254748225212097
train gradient:  0.13833181021918312
iteration : 4837
train acc:  0.71875
train loss:  0.5185137987136841
train gradient:  0.14388871182361168
iteration : 4838
train acc:  0.734375
train loss:  0.5249749422073364
train gradient:  0.17057358616687035
iteration : 4839
train acc:  0.71875
train loss:  0.5144165754318237
train gradient:  0.14710074397244302
iteration : 4840
train acc:  0.6796875
train loss:  0.5139533877372742
train gradient:  0.14690465025345947
iteration : 4841
train acc:  0.6796875
train loss:  0.5389470458030701
train gradient:  0.15227092099400102
iteration : 4842
train acc:  0.734375
train loss:  0.44704246520996094
train gradient:  0.10938842001742331
iteration : 4843
train acc:  0.7265625
train loss:  0.5444289445877075
train gradient:  0.21863086430413045
iteration : 4844
train acc:  0.765625
train loss:  0.4441232681274414
train gradient:  0.13694095949460422
iteration : 4845
train acc:  0.71875
train loss:  0.5739798545837402
train gradient:  0.1368825468301527
iteration : 4846
train acc:  0.734375
train loss:  0.5036247372627258
train gradient:  0.14993241751932898
iteration : 4847
train acc:  0.78125
train loss:  0.4574633240699768
train gradient:  0.14823305341447243
iteration : 4848
train acc:  0.7109375
train loss:  0.5545188188552856
train gradient:  0.14233386212538313
iteration : 4849
train acc:  0.734375
train loss:  0.5004249811172485
train gradient:  0.1597411118271193
iteration : 4850
train acc:  0.734375
train loss:  0.5019811391830444
train gradient:  0.1297909382807519
iteration : 4851
train acc:  0.765625
train loss:  0.5053184032440186
train gradient:  0.15088181213731144
iteration : 4852
train acc:  0.6875
train loss:  0.5641887784004211
train gradient:  0.1795973111649527
iteration : 4853
train acc:  0.6875
train loss:  0.5253682136535645
train gradient:  0.14000718986654848
iteration : 4854
train acc:  0.7890625
train loss:  0.47073379158973694
train gradient:  0.12628303930132376
iteration : 4855
train acc:  0.75
train loss:  0.5031123757362366
train gradient:  0.1386247107953204
iteration : 4856
train acc:  0.65625
train loss:  0.638791561126709
train gradient:  0.2434172904431383
iteration : 4857
train acc:  0.6328125
train loss:  0.6121689081192017
train gradient:  0.20825394918900214
iteration : 4858
train acc:  0.671875
train loss:  0.5365834832191467
train gradient:  0.15364687400566918
iteration : 4859
train acc:  0.765625
train loss:  0.5128758549690247
train gradient:  0.13652931112713068
iteration : 4860
train acc:  0.75
train loss:  0.4771007299423218
train gradient:  0.13899132542679168
iteration : 4861
train acc:  0.75
train loss:  0.5291016697883606
train gradient:  0.18318307882091334
iteration : 4862
train acc:  0.6796875
train loss:  0.5360341668128967
train gradient:  0.17833565780934668
iteration : 4863
train acc:  0.7109375
train loss:  0.5020444393157959
train gradient:  0.15973555978442794
iteration : 4864
train acc:  0.765625
train loss:  0.4470081329345703
train gradient:  0.12424717525686296
iteration : 4865
train acc:  0.75
train loss:  0.499828040599823
train gradient:  0.12167822534993752
iteration : 4866
train acc:  0.703125
train loss:  0.5277523994445801
train gradient:  0.1741934971335135
iteration : 4867
train acc:  0.703125
train loss:  0.5306714773178101
train gradient:  0.21179880132694046
iteration : 4868
train acc:  0.796875
train loss:  0.4571654796600342
train gradient:  0.13620973878857823
iteration : 4869
train acc:  0.75
train loss:  0.4362841844558716
train gradient:  0.11024474887977809
iteration : 4870
train acc:  0.7265625
train loss:  0.5036215782165527
train gradient:  0.13550418232851108
iteration : 4871
train acc:  0.7734375
train loss:  0.4653705954551697
train gradient:  0.12395837951018947
iteration : 4872
train acc:  0.7265625
train loss:  0.4910825192928314
train gradient:  0.13244335731121454
iteration : 4873
train acc:  0.71875
train loss:  0.4947766363620758
train gradient:  0.13616249129071
iteration : 4874
train acc:  0.65625
train loss:  0.5858646035194397
train gradient:  0.20788968708918265
iteration : 4875
train acc:  0.734375
train loss:  0.4893946647644043
train gradient:  0.1329910252819279
iteration : 4876
train acc:  0.6953125
train loss:  0.511542797088623
train gradient:  0.1686444126421866
iteration : 4877
train acc:  0.7578125
train loss:  0.5052103996276855
train gradient:  0.127942345241284
iteration : 4878
train acc:  0.7421875
train loss:  0.5399205684661865
train gradient:  0.1692155666422549
iteration : 4879
train acc:  0.78125
train loss:  0.44735392928123474
train gradient:  0.11388486254256151
iteration : 4880
train acc:  0.71875
train loss:  0.5159034132957458
train gradient:  0.17854040092201226
iteration : 4881
train acc:  0.765625
train loss:  0.4559468626976013
train gradient:  0.10768501288119231
iteration : 4882
train acc:  0.703125
train loss:  0.6138101816177368
train gradient:  0.275781952921142
iteration : 4883
train acc:  0.71875
train loss:  0.5646014213562012
train gradient:  0.1544108060643949
iteration : 4884
train acc:  0.7421875
train loss:  0.5016697645187378
train gradient:  0.14584339747624703
iteration : 4885
train acc:  0.734375
train loss:  0.4921608865261078
train gradient:  0.12461153746429543
iteration : 4886
train acc:  0.75
train loss:  0.5088419914245605
train gradient:  0.1474463309575182
iteration : 4887
train acc:  0.734375
train loss:  0.48870208859443665
train gradient:  0.10805229838547899
iteration : 4888
train acc:  0.7265625
train loss:  0.513168454170227
train gradient:  0.18448641471647642
iteration : 4889
train acc:  0.7421875
train loss:  0.4889841079711914
train gradient:  0.1587923391659659
iteration : 4890
train acc:  0.6875
train loss:  0.5667349100112915
train gradient:  0.19799016962276106
iteration : 4891
train acc:  0.7109375
train loss:  0.5275765657424927
train gradient:  0.1485089269697933
iteration : 4892
train acc:  0.7734375
train loss:  0.5316450595855713
train gradient:  0.13742081577118698
iteration : 4893
train acc:  0.765625
train loss:  0.4823574721813202
train gradient:  0.16510384918742904
iteration : 4894
train acc:  0.6875
train loss:  0.5479440093040466
train gradient:  0.16589745356098945
iteration : 4895
train acc:  0.7421875
train loss:  0.5131528377532959
train gradient:  0.13368902168378247
iteration : 4896
train acc:  0.7734375
train loss:  0.44172975420951843
train gradient:  0.11715318448830282
iteration : 4897
train acc:  0.7578125
train loss:  0.48637306690216064
train gradient:  0.1533909010285776
iteration : 4898
train acc:  0.765625
train loss:  0.44878485798835754
train gradient:  0.10957526773827883
iteration : 4899
train acc:  0.7421875
train loss:  0.5024554133415222
train gradient:  0.12607009512742073
iteration : 4900
train acc:  0.734375
train loss:  0.48244112730026245
train gradient:  0.1312610574422607
iteration : 4901
train acc:  0.7109375
train loss:  0.5163118839263916
train gradient:  0.17178845966406942
iteration : 4902
train acc:  0.75
train loss:  0.5278661251068115
train gradient:  0.13842608401623638
iteration : 4903
train acc:  0.71875
train loss:  0.5730396509170532
train gradient:  0.16982206806677888
iteration : 4904
train acc:  0.671875
train loss:  0.5377322435379028
train gradient:  0.16686467343252198
iteration : 4905
train acc:  0.7109375
train loss:  0.5243085622787476
train gradient:  0.16014508142259876
iteration : 4906
train acc:  0.6640625
train loss:  0.565496563911438
train gradient:  0.15524547591916038
iteration : 4907
train acc:  0.734375
train loss:  0.534783124923706
train gradient:  0.17605052834845866
iteration : 4908
train acc:  0.71875
train loss:  0.5018609166145325
train gradient:  0.13582777163623844
iteration : 4909
train acc:  0.7578125
train loss:  0.5393474102020264
train gradient:  0.17093004178511995
iteration : 4910
train acc:  0.796875
train loss:  0.42240116000175476
train gradient:  0.10817964225475224
iteration : 4911
train acc:  0.703125
train loss:  0.5479100942611694
train gradient:  0.19364026550112157
iteration : 4912
train acc:  0.71875
train loss:  0.5242783427238464
train gradient:  0.16717994774363737
iteration : 4913
train acc:  0.7734375
train loss:  0.4883820116519928
train gradient:  0.11584702791243506
iteration : 4914
train acc:  0.7109375
train loss:  0.5135213136672974
train gradient:  0.15184585317381583
iteration : 4915
train acc:  0.75
train loss:  0.5322401523590088
train gradient:  0.16743016998880106
iteration : 4916
train acc:  0.703125
train loss:  0.5509061217308044
train gradient:  0.18503417242589731
iteration : 4917
train acc:  0.765625
train loss:  0.5102214217185974
train gradient:  0.18367929494125376
iteration : 4918
train acc:  0.75
train loss:  0.5452300310134888
train gradient:  0.1764030903232854
iteration : 4919
train acc:  0.734375
train loss:  0.544840931892395
train gradient:  0.18445170062719735
iteration : 4920
train acc:  0.75
train loss:  0.550868034362793
train gradient:  0.17537037083153537
iteration : 4921
train acc:  0.8046875
train loss:  0.46793752908706665
train gradient:  0.13448719640886198
iteration : 4922
train acc:  0.8125
train loss:  0.4851606786251068
train gradient:  0.12391099653974527
iteration : 4923
train acc:  0.7578125
train loss:  0.48840537667274475
train gradient:  0.11205128486384251
iteration : 4924
train acc:  0.7265625
train loss:  0.5189855098724365
train gradient:  0.14818046639111448
iteration : 4925
train acc:  0.7578125
train loss:  0.4449540972709656
train gradient:  0.13645084456492462
iteration : 4926
train acc:  0.796875
train loss:  0.4342266619205475
train gradient:  0.11218708505606999
iteration : 4927
train acc:  0.75
train loss:  0.5018009543418884
train gradient:  0.13989257296415958
iteration : 4928
train acc:  0.671875
train loss:  0.5746279954910278
train gradient:  0.19795601710093252
iteration : 4929
train acc:  0.6953125
train loss:  0.5682495832443237
train gradient:  0.17342619867071557
iteration : 4930
train acc:  0.8359375
train loss:  0.4382128119468689
train gradient:  0.14114383267735842
iteration : 4931
train acc:  0.703125
train loss:  0.5606034994125366
train gradient:  0.18638618409521654
iteration : 4932
train acc:  0.7578125
train loss:  0.4703940749168396
train gradient:  0.1269361629730289
iteration : 4933
train acc:  0.6953125
train loss:  0.5426411032676697
train gradient:  0.12335405187809387
iteration : 4934
train acc:  0.671875
train loss:  0.5711480379104614
train gradient:  0.2326129084247308
iteration : 4935
train acc:  0.78125
train loss:  0.4383366107940674
train gradient:  0.11710335912234618
iteration : 4936
train acc:  0.734375
train loss:  0.517057478427887
train gradient:  0.1553649396393245
iteration : 4937
train acc:  0.6875
train loss:  0.5367099046707153
train gradient:  0.16702158130801556
iteration : 4938
train acc:  0.71875
train loss:  0.5822432041168213
train gradient:  0.30387596010611795
iteration : 4939
train acc:  0.7421875
train loss:  0.508418619632721
train gradient:  0.13084201749917734
iteration : 4940
train acc:  0.6796875
train loss:  0.5745620131492615
train gradient:  0.17424277094805748
iteration : 4941
train acc:  0.7265625
train loss:  0.511886715888977
train gradient:  0.17460802684054377
iteration : 4942
train acc:  0.7578125
train loss:  0.507851243019104
train gradient:  0.13731592264830764
iteration : 4943
train acc:  0.6796875
train loss:  0.6135190725326538
train gradient:  0.2677694116677243
iteration : 4944
train acc:  0.7265625
train loss:  0.5438069105148315
train gradient:  0.2086987762589495
iteration : 4945
train acc:  0.734375
train loss:  0.5014135837554932
train gradient:  0.12861658431127737
iteration : 4946
train acc:  0.7578125
train loss:  0.5174928307533264
train gradient:  0.13881245571747802
iteration : 4947
train acc:  0.6875
train loss:  0.6163855195045471
train gradient:  0.18544094481951842
iteration : 4948
train acc:  0.765625
train loss:  0.48807597160339355
train gradient:  0.15202981201887517
iteration : 4949
train acc:  0.6875
train loss:  0.5873557925224304
train gradient:  0.2219179324740232
iteration : 4950
train acc:  0.796875
train loss:  0.4432713985443115
train gradient:  0.10436938153681903
iteration : 4951
train acc:  0.703125
train loss:  0.5417745113372803
train gradient:  0.16015296006062407
iteration : 4952
train acc:  0.734375
train loss:  0.5136526823043823
train gradient:  0.11621467471733354
iteration : 4953
train acc:  0.7734375
train loss:  0.48814207315444946
train gradient:  0.13154601590837411
iteration : 4954
train acc:  0.7890625
train loss:  0.5020633339881897
train gradient:  0.1404388196728483
iteration : 4955
train acc:  0.6875
train loss:  0.49186205863952637
train gradient:  0.16940685425843052
iteration : 4956
train acc:  0.75
train loss:  0.4985368549823761
train gradient:  0.13772196276159077
iteration : 4957
train acc:  0.7421875
train loss:  0.4923601448535919
train gradient:  0.1491922442916461
iteration : 4958
train acc:  0.703125
train loss:  0.5213477611541748
train gradient:  0.12755181437263707
iteration : 4959
train acc:  0.734375
train loss:  0.4790780544281006
train gradient:  0.11614064875361758
iteration : 4960
train acc:  0.78125
train loss:  0.4644116759300232
train gradient:  0.12503248272467077
iteration : 4961
train acc:  0.75
train loss:  0.4805225431919098
train gradient:  0.1448082900022657
iteration : 4962
train acc:  0.78125
train loss:  0.4351622462272644
train gradient:  0.11934525556152412
iteration : 4963
train acc:  0.765625
train loss:  0.45008569955825806
train gradient:  0.14941964951962639
iteration : 4964
train acc:  0.7421875
train loss:  0.5358912348747253
train gradient:  0.1740499583311034
iteration : 4965
train acc:  0.7109375
train loss:  0.5570446848869324
train gradient:  0.19122111617635762
iteration : 4966
train acc:  0.6796875
train loss:  0.5697681903839111
train gradient:  0.2227657681991339
iteration : 4967
train acc:  0.7578125
train loss:  0.48012739419937134
train gradient:  0.1193386144461327
iteration : 4968
train acc:  0.75
train loss:  0.48272889852523804
train gradient:  0.15499748820013068
iteration : 4969
train acc:  0.78125
train loss:  0.5206983089447021
train gradient:  0.1376785961193175
iteration : 4970
train acc:  0.7265625
train loss:  0.5312944650650024
train gradient:  0.1585374068536783
iteration : 4971
train acc:  0.8125
train loss:  0.44698116183280945
train gradient:  0.1321867072079718
iteration : 4972
train acc:  0.78125
train loss:  0.4765363335609436
train gradient:  0.16658656847676498
iteration : 4973
train acc:  0.6328125
train loss:  0.5980009436607361
train gradient:  0.256386301785575
iteration : 4974
train acc:  0.671875
train loss:  0.5850473642349243
train gradient:  0.23809871829252305
iteration : 4975
train acc:  0.71875
train loss:  0.5267618894577026
train gradient:  0.18646617542364718
iteration : 4976
train acc:  0.671875
train loss:  0.5864460468292236
train gradient:  0.1938357034224642
iteration : 4977
train acc:  0.7109375
train loss:  0.5314626693725586
train gradient:  0.1603482221549314
iteration : 4978
train acc:  0.7109375
train loss:  0.5679739713668823
train gradient:  0.1755368825078219
iteration : 4979
train acc:  0.7578125
train loss:  0.478803426027298
train gradient:  0.15046556161535563
iteration : 4980
train acc:  0.6953125
train loss:  0.5365890264511108
train gradient:  0.18031794328281642
iteration : 4981
train acc:  0.6953125
train loss:  0.5447278022766113
train gradient:  0.14531458136667863
iteration : 4982
train acc:  0.7421875
train loss:  0.48600518703460693
train gradient:  0.20345106501724972
iteration : 4983
train acc:  0.7578125
train loss:  0.4816020727157593
train gradient:  0.11448544828630336
iteration : 4984
train acc:  0.7578125
train loss:  0.5038058161735535
train gradient:  0.19363654143990494
iteration : 4985
train acc:  0.734375
train loss:  0.48527243733406067
train gradient:  0.1436810419313419
iteration : 4986
train acc:  0.7265625
train loss:  0.4728127419948578
train gradient:  0.13200860765877748
iteration : 4987
train acc:  0.703125
train loss:  0.5346120595932007
train gradient:  0.14707783794880144
iteration : 4988
train acc:  0.71875
train loss:  0.5014286041259766
train gradient:  0.15495391247167795
iteration : 4989
train acc:  0.7109375
train loss:  0.5215562582015991
train gradient:  0.17295124540029677
iteration : 4990
train acc:  0.6953125
train loss:  0.5557478070259094
train gradient:  0.18787235275972575
iteration : 4991
train acc:  0.7265625
train loss:  0.46338310837745667
train gradient:  0.15116622977435595
iteration : 4992
train acc:  0.71875
train loss:  0.5164650678634644
train gradient:  0.20061818580869714
iteration : 4993
train acc:  0.7421875
train loss:  0.4957939386367798
train gradient:  0.13709162798229443
iteration : 4994
train acc:  0.71875
train loss:  0.5437886118888855
train gradient:  0.17605065217857907
iteration : 4995
train acc:  0.6875
train loss:  0.5616912245750427
train gradient:  0.1605420260200784
iteration : 4996
train acc:  0.6640625
train loss:  0.5794246196746826
train gradient:  0.21135144300390082
iteration : 4997
train acc:  0.796875
train loss:  0.46033549308776855
train gradient:  0.14289987141580024
iteration : 4998
train acc:  0.71875
train loss:  0.5621994733810425
train gradient:  0.15319906225971655
iteration : 4999
train acc:  0.734375
train loss:  0.5185155868530273
train gradient:  0.11948089209060216
iteration : 5000
train acc:  0.734375
train loss:  0.5523250102996826
train gradient:  0.1506703211216312
iteration : 5001
train acc:  0.765625
train loss:  0.50019371509552
train gradient:  0.15011314937662928
iteration : 5002
train acc:  0.7421875
train loss:  0.5641412138938904
train gradient:  0.1803347283062512
iteration : 5003
train acc:  0.734375
train loss:  0.5427343249320984
train gradient:  0.13704247740822115
iteration : 5004
train acc:  0.6875
train loss:  0.5856116414070129
train gradient:  0.1973642588279543
iteration : 5005
train acc:  0.7421875
train loss:  0.5602821707725525
train gradient:  0.20891063927299963
iteration : 5006
train acc:  0.7265625
train loss:  0.5443049669265747
train gradient:  0.18837083238787594
iteration : 5007
train acc:  0.6796875
train loss:  0.584297776222229
train gradient:  0.1713916036727515
iteration : 5008
train acc:  0.703125
train loss:  0.4838774502277374
train gradient:  0.1384644736745184
iteration : 5009
train acc:  0.7578125
train loss:  0.4880650043487549
train gradient:  0.1580043813172351
iteration : 5010
train acc:  0.78125
train loss:  0.4984574317932129
train gradient:  0.1525381932939526
iteration : 5011
train acc:  0.6953125
train loss:  0.5703986883163452
train gradient:  0.19757170168900332
iteration : 5012
train acc:  0.6953125
train loss:  0.5835792422294617
train gradient:  0.15314821439439213
iteration : 5013
train acc:  0.7421875
train loss:  0.5239484906196594
train gradient:  0.2001132527704807
iteration : 5014
train acc:  0.671875
train loss:  0.5796580910682678
train gradient:  0.20473209589380215
iteration : 5015
train acc:  0.75
train loss:  0.5216184854507446
train gradient:  0.16897435335044894
iteration : 5016
train acc:  0.703125
train loss:  0.5482597947120667
train gradient:  0.20376719747636265
iteration : 5017
train acc:  0.75
train loss:  0.5607712268829346
train gradient:  0.1697611670147879
iteration : 5018
train acc:  0.703125
train loss:  0.5371378660202026
train gradient:  0.12942031025466272
iteration : 5019
train acc:  0.765625
train loss:  0.4450799822807312
train gradient:  0.10733451068813717
iteration : 5020
train acc:  0.8359375
train loss:  0.4418439269065857
train gradient:  0.12881023142014214
iteration : 5021
train acc:  0.734375
train loss:  0.5137189626693726
train gradient:  0.131473091221322
iteration : 5022
train acc:  0.6875
train loss:  0.5319749116897583
train gradient:  0.16871495552478102
iteration : 5023
train acc:  0.7265625
train loss:  0.5239999890327454
train gradient:  0.13342403252334875
iteration : 5024
train acc:  0.71875
train loss:  0.5275212526321411
train gradient:  0.18530041405784575
iteration : 5025
train acc:  0.7890625
train loss:  0.45587772130966187
train gradient:  0.13559711288160387
iteration : 5026
train acc:  0.7109375
train loss:  0.5673598051071167
train gradient:  0.17260723710042145
iteration : 5027
train acc:  0.7265625
train loss:  0.5183249711990356
train gradient:  0.15157341107838146
iteration : 5028
train acc:  0.671875
train loss:  0.5568071603775024
train gradient:  0.17020355723157823
iteration : 5029
train acc:  0.7421875
train loss:  0.45609211921691895
train gradient:  0.09758999349928035
iteration : 5030
train acc:  0.7734375
train loss:  0.46280205249786377
train gradient:  0.1114522586381546
iteration : 5031
train acc:  0.765625
train loss:  0.5299437046051025
train gradient:  0.17278092860343922
iteration : 5032
train acc:  0.7109375
train loss:  0.539574384689331
train gradient:  0.1746553493547967
iteration : 5033
train acc:  0.75
train loss:  0.5462137460708618
train gradient:  0.15335562398137345
iteration : 5034
train acc:  0.7734375
train loss:  0.47007226943969727
train gradient:  0.12456500415521313
iteration : 5035
train acc:  0.7890625
train loss:  0.48921939730644226
train gradient:  0.155129862300315
iteration : 5036
train acc:  0.6640625
train loss:  0.5667328834533691
train gradient:  0.20356938375264128
iteration : 5037
train acc:  0.7109375
train loss:  0.5161893367767334
train gradient:  0.15317219995220768
iteration : 5038
train acc:  0.6953125
train loss:  0.5424299836158752
train gradient:  0.15790297803895234
iteration : 5039
train acc:  0.7421875
train loss:  0.5138600468635559
train gradient:  0.14608622602768495
iteration : 5040
train acc:  0.78125
train loss:  0.4880814254283905
train gradient:  0.12907326304189537
iteration : 5041
train acc:  0.6796875
train loss:  0.5627189874649048
train gradient:  0.1531956366855562
iteration : 5042
train acc:  0.734375
train loss:  0.47708606719970703
train gradient:  0.12625865075875678
iteration : 5043
train acc:  0.7421875
train loss:  0.4946531653404236
train gradient:  0.1150883528990952
iteration : 5044
train acc:  0.796875
train loss:  0.4653395116329193
train gradient:  0.16540480573960245
iteration : 5045
train acc:  0.7734375
train loss:  0.47702643275260925
train gradient:  0.16360737080729365
iteration : 5046
train acc:  0.6875
train loss:  0.5356109142303467
train gradient:  0.1592238321854404
iteration : 5047
train acc:  0.7578125
train loss:  0.5139003992080688
train gradient:  0.18450896660691674
iteration : 5048
train acc:  0.75
train loss:  0.4625232517719269
train gradient:  0.12979286963182812
iteration : 5049
train acc:  0.7109375
train loss:  0.507735013961792
train gradient:  0.10975627556766701
iteration : 5050
train acc:  0.7109375
train loss:  0.6208740472793579
train gradient:  0.24274245901005215
iteration : 5051
train acc:  0.6796875
train loss:  0.5227781534194946
train gradient:  0.14765939563301483
iteration : 5052
train acc:  0.7265625
train loss:  0.5037671327590942
train gradient:  0.1296653044075585
iteration : 5053
train acc:  0.703125
train loss:  0.5208227038383484
train gradient:  0.13857630610893013
iteration : 5054
train acc:  0.765625
train loss:  0.5062305927276611
train gradient:  0.15734907593004815
iteration : 5055
train acc:  0.703125
train loss:  0.5607466101646423
train gradient:  0.17395636808853737
iteration : 5056
train acc:  0.7109375
train loss:  0.5173460245132446
train gradient:  0.11622827650724803
iteration : 5057
train acc:  0.71875
train loss:  0.5144457221031189
train gradient:  0.16345825368976602
iteration : 5058
train acc:  0.71875
train loss:  0.5337741374969482
train gradient:  0.15198515394330958
iteration : 5059
train acc:  0.6953125
train loss:  0.541084885597229
train gradient:  0.15146768777011657
iteration : 5060
train acc:  0.65625
train loss:  0.5406492948532104
train gradient:  0.20251880477935436
iteration : 5061
train acc:  0.75
train loss:  0.4829063415527344
train gradient:  0.12779369145453878
iteration : 5062
train acc:  0.6796875
train loss:  0.5074881911277771
train gradient:  0.13568535860212746
iteration : 5063
train acc:  0.7890625
train loss:  0.4667215943336487
train gradient:  0.13073961631142342
iteration : 5064
train acc:  0.734375
train loss:  0.5339101552963257
train gradient:  0.12085823970813925
iteration : 5065
train acc:  0.75
train loss:  0.5106765031814575
train gradient:  0.16261393058462414
iteration : 5066
train acc:  0.78125
train loss:  0.4213981628417969
train gradient:  0.10567931066013639
iteration : 5067
train acc:  0.734375
train loss:  0.5286022424697876
train gradient:  0.15497666738318255
iteration : 5068
train acc:  0.734375
train loss:  0.5155411958694458
train gradient:  0.15789319517203676
iteration : 5069
train acc:  0.6953125
train loss:  0.5230225920677185
train gradient:  0.14379452198701376
iteration : 5070
train acc:  0.7421875
train loss:  0.491949200630188
train gradient:  0.16046913738777568
iteration : 5071
train acc:  0.7265625
train loss:  0.5414618253707886
train gradient:  0.25332010101119107
iteration : 5072
train acc:  0.7421875
train loss:  0.5334658622741699
train gradient:  0.14854707788103078
iteration : 5073
train acc:  0.734375
train loss:  0.5033904314041138
train gradient:  0.16924272929119225
iteration : 5074
train acc:  0.75
train loss:  0.4890235960483551
train gradient:  0.1648868786859176
iteration : 5075
train acc:  0.734375
train loss:  0.5038506984710693
train gradient:  0.21227120446006997
iteration : 5076
train acc:  0.765625
train loss:  0.4871036112308502
train gradient:  0.1525943299481099
iteration : 5077
train acc:  0.7109375
train loss:  0.5208555459976196
train gradient:  0.12498801555145633
iteration : 5078
train acc:  0.6796875
train loss:  0.5467471480369568
train gradient:  0.21411419817665073
iteration : 5079
train acc:  0.6484375
train loss:  0.6386591196060181
train gradient:  0.22634785312438102
iteration : 5080
train acc:  0.75
train loss:  0.4889300763607025
train gradient:  0.14009708610460056
iteration : 5081
train acc:  0.6875
train loss:  0.5529676079750061
train gradient:  0.1713886813132567
iteration : 5082
train acc:  0.7890625
train loss:  0.506810188293457
train gradient:  0.12298980214580008
iteration : 5083
train acc:  0.7421875
train loss:  0.5029206275939941
train gradient:  0.20862059713692704
iteration : 5084
train acc:  0.765625
train loss:  0.4708836078643799
train gradient:  0.1536070376793196
iteration : 5085
train acc:  0.625
train loss:  0.6153335571289062
train gradient:  0.18599588079984686
iteration : 5086
train acc:  0.7421875
train loss:  0.49523651599884033
train gradient:  0.13088069450242446
iteration : 5087
train acc:  0.703125
train loss:  0.5036035776138306
train gradient:  0.13646889891577202
iteration : 5088
train acc:  0.7265625
train loss:  0.5274332761764526
train gradient:  0.19308899933936996
iteration : 5089
train acc:  0.734375
train loss:  0.4834493100643158
train gradient:  0.12232318157545152
iteration : 5090
train acc:  0.7109375
train loss:  0.5189833641052246
train gradient:  0.11154994717091772
iteration : 5091
train acc:  0.78125
train loss:  0.5153656601905823
train gradient:  0.13929655913301925
iteration : 5092
train acc:  0.6484375
train loss:  0.5738356709480286
train gradient:  0.16954454745483044
iteration : 5093
train acc:  0.734375
train loss:  0.5322858095169067
train gradient:  0.14630427766824555
iteration : 5094
train acc:  0.703125
train loss:  0.520817756652832
train gradient:  0.1925653196192379
iteration : 5095
train acc:  0.71875
train loss:  0.5174618363380432
train gradient:  0.1332059863364496
iteration : 5096
train acc:  0.7421875
train loss:  0.5362962484359741
train gradient:  0.15524463250573878
iteration : 5097
train acc:  0.703125
train loss:  0.4902927279472351
train gradient:  0.13890958421339542
iteration : 5098
train acc:  0.703125
train loss:  0.522083044052124
train gradient:  0.15893451340149028
iteration : 5099
train acc:  0.671875
train loss:  0.5808631181716919
train gradient:  0.17169260461788088
iteration : 5100
train acc:  0.7109375
train loss:  0.525862991809845
train gradient:  0.17444775345692054
iteration : 5101
train acc:  0.7734375
train loss:  0.49734532833099365
train gradient:  0.11735927150005278
iteration : 5102
train acc:  0.765625
train loss:  0.47837206721305847
train gradient:  0.13426223276579574
iteration : 5103
train acc:  0.7421875
train loss:  0.5228762626647949
train gradient:  0.16126069303676077
iteration : 5104
train acc:  0.765625
train loss:  0.5520874261856079
train gradient:  0.1763417241714232
iteration : 5105
train acc:  0.7421875
train loss:  0.4935756325721741
train gradient:  0.12357823272459359
iteration : 5106
train acc:  0.7421875
train loss:  0.5458284020423889
train gradient:  0.1399186277608993
iteration : 5107
train acc:  0.796875
train loss:  0.48967403173446655
train gradient:  0.1432268796402683
iteration : 5108
train acc:  0.671875
train loss:  0.6384268999099731
train gradient:  0.21884641192866128
iteration : 5109
train acc:  0.75
train loss:  0.47586485743522644
train gradient:  0.14916706314595896
iteration : 5110
train acc:  0.7734375
train loss:  0.4476294219493866
train gradient:  0.10281359461255485
iteration : 5111
train acc:  0.75
train loss:  0.5164220333099365
train gradient:  0.13028716714156707
iteration : 5112
train acc:  0.6796875
train loss:  0.5408271551132202
train gradient:  0.13870266598510433
iteration : 5113
train acc:  0.6796875
train loss:  0.5593262314796448
train gradient:  0.20594386624511518
iteration : 5114
train acc:  0.6796875
train loss:  0.5623643398284912
train gradient:  0.19724617194735883
iteration : 5115
train acc:  0.765625
train loss:  0.49913251399993896
train gradient:  0.13380889090464315
iteration : 5116
train acc:  0.71875
train loss:  0.5053720474243164
train gradient:  0.1171114871688414
iteration : 5117
train acc:  0.7421875
train loss:  0.4948941767215729
train gradient:  0.11108454682878431
iteration : 5118
train acc:  0.640625
train loss:  0.5808401107788086
train gradient:  0.2000577623457444
iteration : 5119
train acc:  0.75
train loss:  0.5347673296928406
train gradient:  0.14782212368484887
iteration : 5120
train acc:  0.7265625
train loss:  0.5011247992515564
train gradient:  0.20850234638319876
iteration : 5121
train acc:  0.734375
train loss:  0.4931143522262573
train gradient:  0.13521868581967733
iteration : 5122
train acc:  0.7421875
train loss:  0.5149688124656677
train gradient:  0.13569525861151602
iteration : 5123
train acc:  0.7421875
train loss:  0.5251309871673584
train gradient:  0.1462221237432956
iteration : 5124
train acc:  0.765625
train loss:  0.47941654920578003
train gradient:  0.13535899980947003
iteration : 5125
train acc:  0.6484375
train loss:  0.5443812012672424
train gradient:  0.13484156426205224
iteration : 5126
train acc:  0.65625
train loss:  0.5684258937835693
train gradient:  0.18569438924343767
iteration : 5127
train acc:  0.7109375
train loss:  0.5301247239112854
train gradient:  0.1899118206463164
iteration : 5128
train acc:  0.6640625
train loss:  0.5740735530853271
train gradient:  0.208364641850677
iteration : 5129
train acc:  0.734375
train loss:  0.5371335744857788
train gradient:  0.19759892281564923
iteration : 5130
train acc:  0.7109375
train loss:  0.5276216268539429
train gradient:  0.18728718530439165
iteration : 5131
train acc:  0.6953125
train loss:  0.5161235928535461
train gradient:  0.13294965667364325
iteration : 5132
train acc:  0.703125
train loss:  0.5435526371002197
train gradient:  0.2061078016985831
iteration : 5133
train acc:  0.734375
train loss:  0.48471423983573914
train gradient:  0.14691474750450773
iteration : 5134
train acc:  0.734375
train loss:  0.4657057225704193
train gradient:  0.14380532273972052
iteration : 5135
train acc:  0.765625
train loss:  0.4877850115299225
train gradient:  0.09626013835310888
iteration : 5136
train acc:  0.828125
train loss:  0.4320404827594757
train gradient:  0.11535010084060385
iteration : 5137
train acc:  0.7265625
train loss:  0.5665446519851685
train gradient:  0.15523689501892896
iteration : 5138
train acc:  0.796875
train loss:  0.453672856092453
train gradient:  0.11250129182070599
iteration : 5139
train acc:  0.6953125
train loss:  0.50688636302948
train gradient:  0.18695651176347056
iteration : 5140
train acc:  0.7109375
train loss:  0.5491827726364136
train gradient:  0.14490928668525704
iteration : 5141
train acc:  0.71875
train loss:  0.5012054443359375
train gradient:  0.14299709865567242
iteration : 5142
train acc:  0.71875
train loss:  0.49397140741348267
train gradient:  0.1212810167579185
iteration : 5143
train acc:  0.734375
train loss:  0.5163319110870361
train gradient:  0.14867698067405452
iteration : 5144
train acc:  0.734375
train loss:  0.5318576693534851
train gradient:  0.16575330279452188
iteration : 5145
train acc:  0.7421875
train loss:  0.5082908868789673
train gradient:  0.16540610688212562
iteration : 5146
train acc:  0.7421875
train loss:  0.47502270340919495
train gradient:  0.14827915419210508
iteration : 5147
train acc:  0.7578125
train loss:  0.47487539052963257
train gradient:  0.14827589042930828
iteration : 5148
train acc:  0.6796875
train loss:  0.5850220918655396
train gradient:  0.2247618047829541
iteration : 5149
train acc:  0.671875
train loss:  0.5696690082550049
train gradient:  0.22489671220828406
iteration : 5150
train acc:  0.78125
train loss:  0.438025563955307
train gradient:  0.15869007946342434
iteration : 5151
train acc:  0.7265625
train loss:  0.5834558606147766
train gradient:  0.18987360338461032
iteration : 5152
train acc:  0.75
train loss:  0.4639774262905121
train gradient:  0.12739889318774478
iteration : 5153
train acc:  0.71875
train loss:  0.524216890335083
train gradient:  0.16169968766619364
iteration : 5154
train acc:  0.703125
train loss:  0.5568454265594482
train gradient:  0.14877919371761433
iteration : 5155
train acc:  0.7265625
train loss:  0.5065450668334961
train gradient:  0.1606057321049269
iteration : 5156
train acc:  0.7421875
train loss:  0.48464256525039673
train gradient:  0.16607209806603274
iteration : 5157
train acc:  0.71875
train loss:  0.555850625038147
train gradient:  0.17562215975600615
iteration : 5158
train acc:  0.7734375
train loss:  0.48709791898727417
train gradient:  0.16892628637811757
iteration : 5159
train acc:  0.7265625
train loss:  0.47668468952178955
train gradient:  0.1464772400467912
iteration : 5160
train acc:  0.734375
train loss:  0.5155365467071533
train gradient:  0.1417215921967141
iteration : 5161
train acc:  0.6796875
train loss:  0.5897759199142456
train gradient:  0.1876919204671085
iteration : 5162
train acc:  0.7578125
train loss:  0.4758005142211914
train gradient:  0.16297777726958984
iteration : 5163
train acc:  0.703125
train loss:  0.6070044040679932
train gradient:  0.2836342411305877
iteration : 5164
train acc:  0.7578125
train loss:  0.5208823680877686
train gradient:  0.1684058187717334
iteration : 5165
train acc:  0.765625
train loss:  0.4872601628303528
train gradient:  0.16213536251679755
iteration : 5166
train acc:  0.7421875
train loss:  0.4856763482093811
train gradient:  0.14100097909795373
iteration : 5167
train acc:  0.75
train loss:  0.4942442774772644
train gradient:  0.14674803686136711
iteration : 5168
train acc:  0.7734375
train loss:  0.4362144470214844
train gradient:  0.10596009913715024
iteration : 5169
train acc:  0.7890625
train loss:  0.4904363751411438
train gradient:  0.12153690055503807
iteration : 5170
train acc:  0.8125
train loss:  0.44748473167419434
train gradient:  0.12126576853885279
iteration : 5171
train acc:  0.7578125
train loss:  0.5243120193481445
train gradient:  0.14639354141220728
iteration : 5172
train acc:  0.765625
train loss:  0.46739983558654785
train gradient:  0.18699815147272858
iteration : 5173
train acc:  0.765625
train loss:  0.48607122898101807
train gradient:  0.15375702823489298
iteration : 5174
train acc:  0.734375
train loss:  0.518412709236145
train gradient:  0.15695165409797518
iteration : 5175
train acc:  0.703125
train loss:  0.5375091433525085
train gradient:  0.14157716479136462
iteration : 5176
train acc:  0.7421875
train loss:  0.4728233814239502
train gradient:  0.14679238020300212
iteration : 5177
train acc:  0.7734375
train loss:  0.467024564743042
train gradient:  0.15332064958732644
iteration : 5178
train acc:  0.671875
train loss:  0.5893247127532959
train gradient:  0.19836247888002645
iteration : 5179
train acc:  0.7109375
train loss:  0.527222752571106
train gradient:  0.23544474713024477
iteration : 5180
train acc:  0.7109375
train loss:  0.5206251740455627
train gradient:  0.14836693678482063
iteration : 5181
train acc:  0.7578125
train loss:  0.49509114027023315
train gradient:  0.1715081682970604
iteration : 5182
train acc:  0.7578125
train loss:  0.4666672945022583
train gradient:  0.1796343804208133
iteration : 5183
train acc:  0.78125
train loss:  0.500677227973938
train gradient:  0.14809401913487025
iteration : 5184
train acc:  0.7578125
train loss:  0.4924793839454651
train gradient:  0.14426874167745374
iteration : 5185
train acc:  0.6875
train loss:  0.5850171446800232
train gradient:  0.1873885215919311
iteration : 5186
train acc:  0.7109375
train loss:  0.5259293913841248
train gradient:  0.15856183764047682
iteration : 5187
train acc:  0.7734375
train loss:  0.5214669704437256
train gradient:  0.12451636172047134
iteration : 5188
train acc:  0.703125
train loss:  0.562383770942688
train gradient:  0.1591815501510926
iteration : 5189
train acc:  0.78125
train loss:  0.47377654910087585
train gradient:  0.17477816321743217
iteration : 5190
train acc:  0.7265625
train loss:  0.47065043449401855
train gradient:  0.12380904539635326
iteration : 5191
train acc:  0.7421875
train loss:  0.48211508989334106
train gradient:  0.12178640925911315
iteration : 5192
train acc:  0.671875
train loss:  0.5437582731246948
train gradient:  0.1657128771537974
iteration : 5193
train acc:  0.7734375
train loss:  0.44541192054748535
train gradient:  0.11093539034826569
iteration : 5194
train acc:  0.765625
train loss:  0.48688969016075134
train gradient:  0.14267823139603614
iteration : 5195
train acc:  0.7734375
train loss:  0.4921402335166931
train gradient:  0.144643106526393
iteration : 5196
train acc:  0.6640625
train loss:  0.6314631700515747
train gradient:  0.17453249208878796
iteration : 5197
train acc:  0.71875
train loss:  0.49769192934036255
train gradient:  0.23146539598846777
iteration : 5198
train acc:  0.765625
train loss:  0.502552330493927
train gradient:  0.13031060045673273
iteration : 5199
train acc:  0.75
train loss:  0.5493965148925781
train gradient:  0.2006723475699547
iteration : 5200
train acc:  0.734375
train loss:  0.5360313057899475
train gradient:  0.16620620520955842
iteration : 5201
train acc:  0.7578125
train loss:  0.41287392377853394
train gradient:  0.09738026293401437
iteration : 5202
train acc:  0.71875
train loss:  0.4702088236808777
train gradient:  0.1218451547283164
iteration : 5203
train acc:  0.734375
train loss:  0.5365613102912903
train gradient:  0.1990256342714425
iteration : 5204
train acc:  0.6875
train loss:  0.5149713158607483
train gradient:  0.16840606972251887
iteration : 5205
train acc:  0.6953125
train loss:  0.5462835431098938
train gradient:  0.1916457515098311
iteration : 5206
train acc:  0.7421875
train loss:  0.49320483207702637
train gradient:  0.18095322483040285
iteration : 5207
train acc:  0.7265625
train loss:  0.5480248332023621
train gradient:  0.1719495075320583
iteration : 5208
train acc:  0.7109375
train loss:  0.5273702144622803
train gradient:  0.14677094947736924
iteration : 5209
train acc:  0.7890625
train loss:  0.4803122580051422
train gradient:  0.1323765128319831
iteration : 5210
train acc:  0.6484375
train loss:  0.6001909971237183
train gradient:  0.26262648134636873
iteration : 5211
train acc:  0.7421875
train loss:  0.5400546789169312
train gradient:  0.15175202730114218
iteration : 5212
train acc:  0.6796875
train loss:  0.5107648372650146
train gradient:  0.18401035102911614
iteration : 5213
train acc:  0.75
train loss:  0.48750391602516174
train gradient:  0.16806077315780876
iteration : 5214
train acc:  0.71875
train loss:  0.5145728588104248
train gradient:  0.14693492815107848
iteration : 5215
train acc:  0.71875
train loss:  0.5279571413993835
train gradient:  0.14262640335571242
iteration : 5216
train acc:  0.75
train loss:  0.47343897819519043
train gradient:  0.14658629460682632
iteration : 5217
train acc:  0.6484375
train loss:  0.5864797234535217
train gradient:  0.1505250064200332
iteration : 5218
train acc:  0.6640625
train loss:  0.5402261018753052
train gradient:  0.19228277990218232
iteration : 5219
train acc:  0.7265625
train loss:  0.5221571922302246
train gradient:  0.1812919067582931
iteration : 5220
train acc:  0.734375
train loss:  0.5366474390029907
train gradient:  0.1441585013230267
iteration : 5221
train acc:  0.7421875
train loss:  0.5442233085632324
train gradient:  0.1793846921373809
iteration : 5222
train acc:  0.734375
train loss:  0.5073429942131042
train gradient:  0.1585354351435153
iteration : 5223
train acc:  0.796875
train loss:  0.47554245591163635
train gradient:  0.149366329999975
iteration : 5224
train acc:  0.71875
train loss:  0.5125245451927185
train gradient:  0.16616309323125394
iteration : 5225
train acc:  0.71875
train loss:  0.4886648952960968
train gradient:  0.14295522265084293
iteration : 5226
train acc:  0.75
train loss:  0.49321189522743225
train gradient:  0.14590263880947119
iteration : 5227
train acc:  0.734375
train loss:  0.5093302726745605
train gradient:  0.15339179022687005
iteration : 5228
train acc:  0.71875
train loss:  0.48752373456954956
train gradient:  0.13321190873853947
iteration : 5229
train acc:  0.7265625
train loss:  0.5215573310852051
train gradient:  0.1415223421325842
iteration : 5230
train acc:  0.6875
train loss:  0.5554274916648865
train gradient:  0.16882722457696356
iteration : 5231
train acc:  0.7890625
train loss:  0.42111659049987793
train gradient:  0.121402301095746
iteration : 5232
train acc:  0.6640625
train loss:  0.6025195121765137
train gradient:  0.18180007276327403
iteration : 5233
train acc:  0.75
train loss:  0.47912168502807617
train gradient:  0.12337253323418736
iteration : 5234
train acc:  0.765625
train loss:  0.48208409547805786
train gradient:  0.1416805545291694
iteration : 5235
train acc:  0.78125
train loss:  0.483228862285614
train gradient:  0.137558148447826
iteration : 5236
train acc:  0.8125
train loss:  0.43330252170562744
train gradient:  0.12239909925682473
iteration : 5237
train acc:  0.7421875
train loss:  0.46301913261413574
train gradient:  0.13750855484185215
iteration : 5238
train acc:  0.71875
train loss:  0.5518258810043335
train gradient:  0.1761712692919335
iteration : 5239
train acc:  0.6484375
train loss:  0.5876368284225464
train gradient:  0.2108816285951317
iteration : 5240
train acc:  0.71875
train loss:  0.5123064517974854
train gradient:  0.20667888830210557
iteration : 5241
train acc:  0.7421875
train loss:  0.49909090995788574
train gradient:  0.1473106564253006
iteration : 5242
train acc:  0.6796875
train loss:  0.5229732394218445
train gradient:  0.14893123888089732
iteration : 5243
train acc:  0.703125
train loss:  0.560038149356842
train gradient:  0.18899457908806383
iteration : 5244
train acc:  0.71875
train loss:  0.5249137282371521
train gradient:  0.16878384500442922
iteration : 5245
train acc:  0.703125
train loss:  0.5134512782096863
train gradient:  0.1655229265050319
iteration : 5246
train acc:  0.765625
train loss:  0.46815165877342224
train gradient:  0.10566153451477368
iteration : 5247
train acc:  0.7265625
train loss:  0.5003820657730103
train gradient:  0.14031798768577156
iteration : 5248
train acc:  0.7578125
train loss:  0.4965989589691162
train gradient:  0.1327931695509624
iteration : 5249
train acc:  0.7734375
train loss:  0.5078732967376709
train gradient:  0.1285196121747559
iteration : 5250
train acc:  0.7265625
train loss:  0.5110510587692261
train gradient:  0.14859063209033374
iteration : 5251
train acc:  0.625
train loss:  0.6336795687675476
train gradient:  0.24282603491099822
iteration : 5252
train acc:  0.7578125
train loss:  0.4718674421310425
train gradient:  0.1513306627456396
iteration : 5253
train acc:  0.7421875
train loss:  0.5084123611450195
train gradient:  0.1681556085123187
iteration : 5254
train acc:  0.78125
train loss:  0.4764137864112854
train gradient:  0.1405137293127366
iteration : 5255
train acc:  0.796875
train loss:  0.4456543028354645
train gradient:  0.1531635687420172
iteration : 5256
train acc:  0.671875
train loss:  0.5671122074127197
train gradient:  0.17391848473399518
iteration : 5257
train acc:  0.734375
train loss:  0.5645418763160706
train gradient:  0.17244043709583962
iteration : 5258
train acc:  0.734375
train loss:  0.5253538489341736
train gradient:  0.145950166013531
iteration : 5259
train acc:  0.7578125
train loss:  0.4668509066104889
train gradient:  0.15837026965957096
iteration : 5260
train acc:  0.7890625
train loss:  0.4678319990634918
train gradient:  0.1293308761358941
iteration : 5261
train acc:  0.828125
train loss:  0.44834935665130615
train gradient:  0.14695448325800936
iteration : 5262
train acc:  0.75
train loss:  0.4592156410217285
train gradient:  0.15599301255512843
iteration : 5263
train acc:  0.6953125
train loss:  0.5162842273712158
train gradient:  0.1319444436880421
iteration : 5264
train acc:  0.6875
train loss:  0.5504837036132812
train gradient:  0.23815179754534804
iteration : 5265
train acc:  0.703125
train loss:  0.5388308763504028
train gradient:  0.18275264835901944
iteration : 5266
train acc:  0.71875
train loss:  0.516973078250885
train gradient:  0.15727724686609837
iteration : 5267
train acc:  0.7421875
train loss:  0.48701906204223633
train gradient:  0.13418284079131546
iteration : 5268
train acc:  0.75
train loss:  0.4896164536476135
train gradient:  0.13372982518759643
iteration : 5269
train acc:  0.6796875
train loss:  0.5740527510643005
train gradient:  0.24243390883325533
iteration : 5270
train acc:  0.8125
train loss:  0.42663639783859253
train gradient:  0.11900664044171036
iteration : 5271
train acc:  0.734375
train loss:  0.5231289863586426
train gradient:  0.1399155016760789
iteration : 5272
train acc:  0.7734375
train loss:  0.44899123907089233
train gradient:  0.14935265182922136
iteration : 5273
train acc:  0.6796875
train loss:  0.5202105045318604
train gradient:  0.1378186849921948
iteration : 5274
train acc:  0.71875
train loss:  0.4879304766654968
train gradient:  0.13776718728495754
iteration : 5275
train acc:  0.75
train loss:  0.5177143216133118
train gradient:  0.14663375773633786
iteration : 5276
train acc:  0.7265625
train loss:  0.48571059107780457
train gradient:  0.1466191802994497
iteration : 5277
train acc:  0.71875
train loss:  0.5107817053794861
train gradient:  0.16419586537973913
iteration : 5278
train acc:  0.75
train loss:  0.4778735637664795
train gradient:  0.1318858045723018
iteration : 5279
train acc:  0.78125
train loss:  0.4552702307701111
train gradient:  0.138892975757986
iteration : 5280
train acc:  0.6640625
train loss:  0.5654209852218628
train gradient:  0.20413437570756582
iteration : 5281
train acc:  0.7890625
train loss:  0.4329834282398224
train gradient:  0.13092417380955634
iteration : 5282
train acc:  0.75
train loss:  0.47275853157043457
train gradient:  0.1709201090588548
iteration : 5283
train acc:  0.75
train loss:  0.49599164724349976
train gradient:  0.14478291537247145
iteration : 5284
train acc:  0.7578125
train loss:  0.5053364038467407
train gradient:  0.18692540080074277
iteration : 5285
train acc:  0.7265625
train loss:  0.4905436635017395
train gradient:  0.134302854625524
iteration : 5286
train acc:  0.765625
train loss:  0.4541378617286682
train gradient:  0.12758387112185904
iteration : 5287
train acc:  0.78125
train loss:  0.4721859395503998
train gradient:  0.1515980902498299
iteration : 5288
train acc:  0.7265625
train loss:  0.5114990472793579
train gradient:  0.14281595222661683
iteration : 5289
train acc:  0.734375
train loss:  0.46123695373535156
train gradient:  0.13410227805122885
iteration : 5290
train acc:  0.703125
train loss:  0.5406748652458191
train gradient:  0.19352339633904375
iteration : 5291
train acc:  0.765625
train loss:  0.48813268542289734
train gradient:  0.1541213032568512
iteration : 5292
train acc:  0.6875
train loss:  0.5648559331893921
train gradient:  0.1713773933341921
iteration : 5293
train acc:  0.796875
train loss:  0.4454290568828583
train gradient:  0.10618995294896336
iteration : 5294
train acc:  0.6796875
train loss:  0.573570966720581
train gradient:  0.2327099153644298
iteration : 5295
train acc:  0.7578125
train loss:  0.508307695388794
train gradient:  0.1923717188448666
iteration : 5296
train acc:  0.7421875
train loss:  0.47405296564102173
train gradient:  0.1446030589791186
iteration : 5297
train acc:  0.75
train loss:  0.5154881477355957
train gradient:  0.13592894170148762
iteration : 5298
train acc:  0.7109375
train loss:  0.551154375076294
train gradient:  0.2057518060205615
iteration : 5299
train acc:  0.7265625
train loss:  0.48711466789245605
train gradient:  0.12215916000909502
iteration : 5300
train acc:  0.78125
train loss:  0.4749510884284973
train gradient:  0.12300517681323074
iteration : 5301
train acc:  0.7265625
train loss:  0.500095784664154
train gradient:  0.21401581654335416
iteration : 5302
train acc:  0.7734375
train loss:  0.49434947967529297
train gradient:  0.1765057942597408
iteration : 5303
train acc:  0.75
train loss:  0.5376831293106079
train gradient:  0.14861799609093038
iteration : 5304
train acc:  0.6640625
train loss:  0.5373197197914124
train gradient:  0.21818267263090468
iteration : 5305
train acc:  0.703125
train loss:  0.5907878875732422
train gradient:  0.2314202622738078
iteration : 5306
train acc:  0.7734375
train loss:  0.4750209152698517
train gradient:  0.15744650786310915
iteration : 5307
train acc:  0.7421875
train loss:  0.5149275660514832
train gradient:  0.1659018112890846
iteration : 5308
train acc:  0.6640625
train loss:  0.5288934707641602
train gradient:  0.19876506858337334
iteration : 5309
train acc:  0.7578125
train loss:  0.5380321741104126
train gradient:  0.158243789284172
iteration : 5310
train acc:  0.7265625
train loss:  0.5494107007980347
train gradient:  0.21007977404482092
iteration : 5311
train acc:  0.765625
train loss:  0.511074423789978
train gradient:  0.15446714987578664
iteration : 5312
train acc:  0.78125
train loss:  0.4774523973464966
train gradient:  0.18196314562557847
iteration : 5313
train acc:  0.765625
train loss:  0.46981197595596313
train gradient:  0.13448232734046428
iteration : 5314
train acc:  0.75
train loss:  0.5174825191497803
train gradient:  0.17372207178665683
iteration : 5315
train acc:  0.7265625
train loss:  0.5301417112350464
train gradient:  0.15352906707970873
iteration : 5316
train acc:  0.828125
train loss:  0.43886131048202515
train gradient:  0.1259856433165364
iteration : 5317
train acc:  0.75
train loss:  0.4898766577243805
train gradient:  0.12762504410553993
iteration : 5318
train acc:  0.6953125
train loss:  0.600907027721405
train gradient:  0.2125740058977506
iteration : 5319
train acc:  0.75
train loss:  0.478137731552124
train gradient:  0.12133278692332419
iteration : 5320
train acc:  0.734375
train loss:  0.4746338129043579
train gradient:  0.15381878060281373
iteration : 5321
train acc:  0.703125
train loss:  0.5444339513778687
train gradient:  0.20308339779071088
iteration : 5322
train acc:  0.8046875
train loss:  0.4446020722389221
train gradient:  0.142455669899351
iteration : 5323
train acc:  0.7734375
train loss:  0.4798957109451294
train gradient:  0.13759194735780236
iteration : 5324
train acc:  0.796875
train loss:  0.46651309728622437
train gradient:  0.12563193328774092
iteration : 5325
train acc:  0.734375
train loss:  0.5143617391586304
train gradient:  0.21505660886656391
iteration : 5326
train acc:  0.6796875
train loss:  0.5526320338249207
train gradient:  0.17721890759945086
iteration : 5327
train acc:  0.703125
train loss:  0.5855878591537476
train gradient:  0.20577725736332234
iteration : 5328
train acc:  0.765625
train loss:  0.5089851021766663
train gradient:  0.19919623563403244
iteration : 5329
train acc:  0.6953125
train loss:  0.5407975316047668
train gradient:  0.19852999760284856
iteration : 5330
train acc:  0.6875
train loss:  0.5761412382125854
train gradient:  0.2077295869479372
iteration : 5331
train acc:  0.6953125
train loss:  0.5198436975479126
train gradient:  0.156222901323659
iteration : 5332
train acc:  0.78125
train loss:  0.44936972856521606
train gradient:  0.1341268178645596
iteration : 5333
train acc:  0.6796875
train loss:  0.6036990880966187
train gradient:  0.19716727506177698
iteration : 5334
train acc:  0.71875
train loss:  0.5571823120117188
train gradient:  0.16967011975856178
iteration : 5335
train acc:  0.6015625
train loss:  0.590624988079071
train gradient:  0.1769783901795873
iteration : 5336
train acc:  0.765625
train loss:  0.4879915714263916
train gradient:  0.14894393115254545
iteration : 5337
train acc:  0.7890625
train loss:  0.476820707321167
train gradient:  0.12734716259649825
iteration : 5338
train acc:  0.796875
train loss:  0.49131137132644653
train gradient:  0.1451841536690352
iteration : 5339
train acc:  0.7265625
train loss:  0.5316144227981567
train gradient:  0.18786341370141219
iteration : 5340
train acc:  0.7109375
train loss:  0.518520712852478
train gradient:  0.15809615790081394
iteration : 5341
train acc:  0.78125
train loss:  0.500464677810669
train gradient:  0.13848073188651955
iteration : 5342
train acc:  0.71875
train loss:  0.5068206191062927
train gradient:  0.19090343821923167
iteration : 5343
train acc:  0.7109375
train loss:  0.5472049117088318
train gradient:  0.13211775141116
iteration : 5344
train acc:  0.78125
train loss:  0.49403446912765503
train gradient:  0.15230893333620213
iteration : 5345
train acc:  0.765625
train loss:  0.48229044675827026
train gradient:  0.1459160141987953
iteration : 5346
train acc:  0.703125
train loss:  0.527389407157898
train gradient:  0.1902101600880841
iteration : 5347
train acc:  0.734375
train loss:  0.5206229090690613
train gradient:  0.16122181319779083
iteration : 5348
train acc:  0.7421875
train loss:  0.4882221817970276
train gradient:  0.1296633236895347
iteration : 5349
train acc:  0.8046875
train loss:  0.4633636176586151
train gradient:  0.12234468630419254
iteration : 5350
train acc:  0.71875
train loss:  0.5245691537857056
train gradient:  0.17101736214954574
iteration : 5351
train acc:  0.7890625
train loss:  0.4924538731575012
train gradient:  0.1699500299877867
iteration : 5352
train acc:  0.6640625
train loss:  0.5282341837882996
train gradient:  0.14454118720699052
iteration : 5353
train acc:  0.6796875
train loss:  0.5275534391403198
train gradient:  0.19618792284415487
iteration : 5354
train acc:  0.7265625
train loss:  0.5298941135406494
train gradient:  0.17471955322327207
iteration : 5355
train acc:  0.7421875
train loss:  0.49817606806755066
train gradient:  0.1351763037533863
iteration : 5356
train acc:  0.6796875
train loss:  0.5969380140304565
train gradient:  0.2519265792406379
iteration : 5357
train acc:  0.671875
train loss:  0.5213636159896851
train gradient:  0.15958714519624057
iteration : 5358
train acc:  0.71875
train loss:  0.5053314566612244
train gradient:  0.12410399317589131
iteration : 5359
train acc:  0.703125
train loss:  0.5065269470214844
train gradient:  0.15173087083597017
iteration : 5360
train acc:  0.7109375
train loss:  0.5632603764533997
train gradient:  0.1596403880560255
iteration : 5361
train acc:  0.6796875
train loss:  0.5927923321723938
train gradient:  0.19159156098525898
iteration : 5362
train acc:  0.7265625
train loss:  0.5062737464904785
train gradient:  0.16899876730724672
iteration : 5363
train acc:  0.7265625
train loss:  0.541083812713623
train gradient:  0.1617311010492728
iteration : 5364
train acc:  0.6875
train loss:  0.5296460390090942
train gradient:  0.18804031452496467
iteration : 5365
train acc:  0.7109375
train loss:  0.49756497144699097
train gradient:  0.14436918759462736
iteration : 5366
train acc:  0.703125
train loss:  0.5519231557846069
train gradient:  0.1554871819484438
iteration : 5367
train acc:  0.765625
train loss:  0.5608147978782654
train gradient:  0.18449319405173997
iteration : 5368
train acc:  0.6875
train loss:  0.5462331771850586
train gradient:  0.13681737692564155
iteration : 5369
train acc:  0.7421875
train loss:  0.46183544397354126
train gradient:  0.1459129852887049
iteration : 5370
train acc:  0.71875
train loss:  0.5152627229690552
train gradient:  0.1822855282419685
iteration : 5371
train acc:  0.78125
train loss:  0.47924983501434326
train gradient:  0.11123333222546261
iteration : 5372
train acc:  0.78125
train loss:  0.46405696868896484
train gradient:  0.13296847814737478
iteration : 5373
train acc:  0.78125
train loss:  0.426547110080719
train gradient:  0.12021940251740709
iteration : 5374
train acc:  0.7734375
train loss:  0.5020688772201538
train gradient:  0.14854415788781664
iteration : 5375
train acc:  0.71875
train loss:  0.5044217109680176
train gradient:  0.2073860450499289
iteration : 5376
train acc:  0.7578125
train loss:  0.5262420177459717
train gradient:  0.1271721087930479
iteration : 5377
train acc:  0.6796875
train loss:  0.5286422371864319
train gradient:  0.1778781820330131
iteration : 5378
train acc:  0.71875
train loss:  0.5324524641036987
train gradient:  0.1496326643606642
iteration : 5379
train acc:  0.71875
train loss:  0.515931248664856
train gradient:  0.16289414239159475
iteration : 5380
train acc:  0.7421875
train loss:  0.5359335541725159
train gradient:  0.1895287863177797
iteration : 5381
train acc:  0.6953125
train loss:  0.5047988295555115
train gradient:  0.150800734179603
iteration : 5382
train acc:  0.7265625
train loss:  0.5974106192588806
train gradient:  0.16894284438899437
iteration : 5383
train acc:  0.734375
train loss:  0.5283083319664001
train gradient:  0.14695493566939807
iteration : 5384
train acc:  0.765625
train loss:  0.47183167934417725
train gradient:  0.12980858492357533
iteration : 5385
train acc:  0.71875
train loss:  0.5150485038757324
train gradient:  0.26255862250133105
iteration : 5386
train acc:  0.7421875
train loss:  0.47341272234916687
train gradient:  0.12788809778449678
iteration : 5387
train acc:  0.78125
train loss:  0.42027097940444946
train gradient:  0.09973963160843136
iteration : 5388
train acc:  0.671875
train loss:  0.563156008720398
train gradient:  0.18537790071696858
iteration : 5389
train acc:  0.6875
train loss:  0.49581241607666016
train gradient:  0.1412180994904493
iteration : 5390
train acc:  0.75
train loss:  0.48615723848342896
train gradient:  0.164300886032704
iteration : 5391
train acc:  0.6953125
train loss:  0.4858447313308716
train gradient:  0.149738564177841
iteration : 5392
train acc:  0.7109375
train loss:  0.5312501192092896
train gradient:  0.14335490717759336
iteration : 5393
train acc:  0.7265625
train loss:  0.5276835560798645
train gradient:  0.17583128649393037
iteration : 5394
train acc:  0.7578125
train loss:  0.42618268728256226
train gradient:  0.10858427692390359
iteration : 5395
train acc:  0.8046875
train loss:  0.41472065448760986
train gradient:  0.11094823615642731
iteration : 5396
train acc:  0.703125
train loss:  0.5630455017089844
train gradient:  0.2201310531936836
iteration : 5397
train acc:  0.6875
train loss:  0.5526819229125977
train gradient:  0.13803239203577553
iteration : 5398
train acc:  0.7421875
train loss:  0.4719395935535431
train gradient:  0.16306375381869095
iteration : 5399
train acc:  0.734375
train loss:  0.493602991104126
train gradient:  0.16104960024142523
iteration : 5400
train acc:  0.7421875
train loss:  0.5077452063560486
train gradient:  0.14259871482847153
iteration : 5401
train acc:  0.765625
train loss:  0.46862420439720154
train gradient:  0.12374733091940367
iteration : 5402
train acc:  0.7734375
train loss:  0.47361186146736145
train gradient:  0.15043488088626084
iteration : 5403
train acc:  0.703125
train loss:  0.5366697311401367
train gradient:  0.18210764967953819
iteration : 5404
train acc:  0.7265625
train loss:  0.5707753300666809
train gradient:  0.15046413774081285
iteration : 5405
train acc:  0.703125
train loss:  0.5503708124160767
train gradient:  0.2144601253060224
iteration : 5406
train acc:  0.6953125
train loss:  0.5635470151901245
train gradient:  0.1773944423308159
iteration : 5407
train acc:  0.765625
train loss:  0.48159724473953247
train gradient:  0.11461369261314687
iteration : 5408
train acc:  0.703125
train loss:  0.4812982678413391
train gradient:  0.12042162182467364
iteration : 5409
train acc:  0.71875
train loss:  0.56722092628479
train gradient:  0.3173074629877115
iteration : 5410
train acc:  0.7109375
train loss:  0.4893021583557129
train gradient:  0.15418440245868847
iteration : 5411
train acc:  0.6796875
train loss:  0.5510055422782898
train gradient:  0.20322179343674174
iteration : 5412
train acc:  0.7578125
train loss:  0.479712575674057
train gradient:  0.1217047416899614
iteration : 5413
train acc:  0.7265625
train loss:  0.5107048153877258
train gradient:  0.19535974867205702
iteration : 5414
train acc:  0.765625
train loss:  0.5306591391563416
train gradient:  0.16109033945054846
iteration : 5415
train acc:  0.7734375
train loss:  0.46370309591293335
train gradient:  0.15853029627649762
iteration : 5416
train acc:  0.7265625
train loss:  0.540818452835083
train gradient:  0.1958899847548321
iteration : 5417
train acc:  0.7421875
train loss:  0.545132040977478
train gradient:  0.18912590844115482
iteration : 5418
train acc:  0.7890625
train loss:  0.4479030966758728
train gradient:  0.16668577234947027
iteration : 5419
train acc:  0.7421875
train loss:  0.5112079381942749
train gradient:  0.15349672317301918
iteration : 5420
train acc:  0.75
train loss:  0.48164844512939453
train gradient:  0.1390447307014585
iteration : 5421
train acc:  0.7265625
train loss:  0.5126861929893494
train gradient:  0.14035899978245314
iteration : 5422
train acc:  0.6953125
train loss:  0.5087646842002869
train gradient:  0.14095734525749748
iteration : 5423
train acc:  0.75
train loss:  0.49330881237983704
train gradient:  0.1498439230805133
iteration : 5424
train acc:  0.7890625
train loss:  0.47110477089881897
train gradient:  0.12984325221877166
iteration : 5425
train acc:  0.640625
train loss:  0.554574728012085
train gradient:  0.1814369755960205
iteration : 5426
train acc:  0.71875
train loss:  0.5370028018951416
train gradient:  0.1812326303618323
iteration : 5427
train acc:  0.75
train loss:  0.47777506709098816
train gradient:  0.1595057179216116
iteration : 5428
train acc:  0.7109375
train loss:  0.5479888916015625
train gradient:  0.15306669744609652
iteration : 5429
train acc:  0.671875
train loss:  0.5943074226379395
train gradient:  0.3385732508578971
iteration : 5430
train acc:  0.71875
train loss:  0.5274810791015625
train gradient:  0.16434287922369428
iteration : 5431
train acc:  0.75
train loss:  0.5379385352134705
train gradient:  0.1831331477228304
iteration : 5432
train acc:  0.734375
train loss:  0.48030033707618713
train gradient:  0.14772091182796093
iteration : 5433
train acc:  0.6875
train loss:  0.5556663274765015
train gradient:  0.19025335811350735
iteration : 5434
train acc:  0.7109375
train loss:  0.5533531904220581
train gradient:  0.20125791602678905
iteration : 5435
train acc:  0.71875
train loss:  0.517134964466095
train gradient:  0.1644540118452249
iteration : 5436
train acc:  0.7421875
train loss:  0.5271558165550232
train gradient:  0.16233471892432724
iteration : 5437
train acc:  0.71875
train loss:  0.5134822130203247
train gradient:  0.13657843008401693
iteration : 5438
train acc:  0.671875
train loss:  0.5446789264678955
train gradient:  0.15473978987944145
iteration : 5439
train acc:  0.78125
train loss:  0.4492204785346985
train gradient:  0.13516025775059362
iteration : 5440
train acc:  0.71875
train loss:  0.529392421245575
train gradient:  0.1443297860782896
iteration : 5441
train acc:  0.7109375
train loss:  0.5536076426506042
train gradient:  0.11958887238574283
iteration : 5442
train acc:  0.7265625
train loss:  0.5145378112792969
train gradient:  0.14626275096589542
iteration : 5443
train acc:  0.7421875
train loss:  0.5085859894752502
train gradient:  0.12593902656693517
iteration : 5444
train acc:  0.6953125
train loss:  0.5762929320335388
train gradient:  0.18366012356557207
iteration : 5445
train acc:  0.7109375
train loss:  0.4974477291107178
train gradient:  0.15023159686549759
iteration : 5446
train acc:  0.71875
train loss:  0.5413311719894409
train gradient:  0.1847420729740677
iteration : 5447
train acc:  0.640625
train loss:  0.5547777414321899
train gradient:  0.14233993154215452
iteration : 5448
train acc:  0.703125
train loss:  0.5880287885665894
train gradient:  0.16598399817901252
iteration : 5449
train acc:  0.6796875
train loss:  0.5568346977233887
train gradient:  0.1689846305981444
iteration : 5450
train acc:  0.7421875
train loss:  0.544037938117981
train gradient:  0.12920266506852354
iteration : 5451
train acc:  0.7578125
train loss:  0.4841611683368683
train gradient:  0.16769874719949057
iteration : 5452
train acc:  0.7578125
train loss:  0.4705849885940552
train gradient:  0.14703951519110525
iteration : 5453
train acc:  0.71875
train loss:  0.518586277961731
train gradient:  0.2040447901436749
iteration : 5454
train acc:  0.75
train loss:  0.48425450921058655
train gradient:  0.17375605414046555
iteration : 5455
train acc:  0.7421875
train loss:  0.5160095691680908
train gradient:  0.16286286291524688
iteration : 5456
train acc:  0.7265625
train loss:  0.5479878187179565
train gradient:  0.1659125924304759
iteration : 5457
train acc:  0.7109375
train loss:  0.49617743492126465
train gradient:  0.1568887076839961
iteration : 5458
train acc:  0.7890625
train loss:  0.5369331240653992
train gradient:  0.2070506717752864
iteration : 5459
train acc:  0.65625
train loss:  0.5883190631866455
train gradient:  0.3302021992678722
iteration : 5460
train acc:  0.7265625
train loss:  0.47165924310684204
train gradient:  0.11611824119155899
iteration : 5461
train acc:  0.6640625
train loss:  0.5966898202896118
train gradient:  0.20903134109878846
iteration : 5462
train acc:  0.703125
train loss:  0.5959434509277344
train gradient:  0.23997924813434085
iteration : 5463
train acc:  0.7890625
train loss:  0.48976951837539673
train gradient:  0.14162359967107158
iteration : 5464
train acc:  0.7265625
train loss:  0.521020770072937
train gradient:  0.17024034322411874
iteration : 5465
train acc:  0.7265625
train loss:  0.5168344974517822
train gradient:  0.1622625885390671
iteration : 5466
train acc:  0.7265625
train loss:  0.5280574560165405
train gradient:  0.1321808397726415
iteration : 5467
train acc:  0.7421875
train loss:  0.4983685314655304
train gradient:  0.13355319812028904
iteration : 5468
train acc:  0.6953125
train loss:  0.5977339744567871
train gradient:  0.1715685045964118
iteration : 5469
train acc:  0.6875
train loss:  0.5528811812400818
train gradient:  0.24467172203672802
iteration : 5470
train acc:  0.734375
train loss:  0.4841284453868866
train gradient:  0.11928095019769804
iteration : 5471
train acc:  0.765625
train loss:  0.4706920385360718
train gradient:  0.14303398411167034
iteration : 5472
train acc:  0.7109375
train loss:  0.5169442892074585
train gradient:  0.14092208097868236
iteration : 5473
train acc:  0.7109375
train loss:  0.5103912949562073
train gradient:  0.15088556496776664
iteration : 5474
train acc:  0.6328125
train loss:  0.5844829082489014
train gradient:  0.18515897030023415
iteration : 5475
train acc:  0.6796875
train loss:  0.5693457126617432
train gradient:  0.16900671851244115
iteration : 5476
train acc:  0.75
train loss:  0.5054539442062378
train gradient:  0.16271646274722013
iteration : 5477
train acc:  0.7109375
train loss:  0.5758814215660095
train gradient:  0.19266555768624755
iteration : 5478
train acc:  0.71875
train loss:  0.5355516672134399
train gradient:  0.1856962920844663
iteration : 5479
train acc:  0.75
train loss:  0.47886085510253906
train gradient:  0.1338279106707233
iteration : 5480
train acc:  0.8046875
train loss:  0.4458078145980835
train gradient:  0.13667035753099185
iteration : 5481
train acc:  0.796875
train loss:  0.48659753799438477
train gradient:  0.1579762321510086
iteration : 5482
train acc:  0.7421875
train loss:  0.5200273990631104
train gradient:  0.12555888879812985
iteration : 5483
train acc:  0.75
train loss:  0.4539179801940918
train gradient:  0.11812401632947829
iteration : 5484
train acc:  0.7109375
train loss:  0.5421074628829956
train gradient:  0.16267831772728508
iteration : 5485
train acc:  0.6171875
train loss:  0.6325600147247314
train gradient:  0.236753245854305
iteration : 5486
train acc:  0.7734375
train loss:  0.4677373468875885
train gradient:  0.11506980405107627
iteration : 5487
train acc:  0.71875
train loss:  0.5181733965873718
train gradient:  0.14849656402898517
iteration : 5488
train acc:  0.734375
train loss:  0.5082675218582153
train gradient:  0.14358668756878945
iteration : 5489
train acc:  0.71875
train loss:  0.5489266514778137
train gradient:  0.1916637541415654
iteration : 5490
train acc:  0.7890625
train loss:  0.47654712200164795
train gradient:  0.14456625437342202
iteration : 5491
train acc:  0.6875
train loss:  0.5362465381622314
train gradient:  0.13802630635562352
iteration : 5492
train acc:  0.796875
train loss:  0.42386606335639954
train gradient:  0.1232473920625473
iteration : 5493
train acc:  0.734375
train loss:  0.528687596321106
train gradient:  0.2112992044127372
iteration : 5494
train acc:  0.734375
train loss:  0.5127228498458862
train gradient:  0.16234297525140584
iteration : 5495
train acc:  0.6953125
train loss:  0.5050190687179565
train gradient:  0.18090867483533357
iteration : 5496
train acc:  0.6875
train loss:  0.5542198419570923
train gradient:  0.14824196742097584
iteration : 5497
train acc:  0.75
train loss:  0.49370861053466797
train gradient:  0.13948690107304074
iteration : 5498
train acc:  0.6953125
train loss:  0.5620530843734741
train gradient:  0.1769544679574575
iteration : 5499
train acc:  0.75
train loss:  0.5158334970474243
train gradient:  0.1563754286242608
iteration : 5500
train acc:  0.8203125
train loss:  0.44048571586608887
train gradient:  0.12350366073814548
iteration : 5501
train acc:  0.703125
train loss:  0.5256914496421814
train gradient:  0.17543468959997438
iteration : 5502
train acc:  0.7109375
train loss:  0.5062230825424194
train gradient:  0.12755665587473825
iteration : 5503
train acc:  0.765625
train loss:  0.5174795389175415
train gradient:  0.15648624389862714
iteration : 5504
train acc:  0.8203125
train loss:  0.42878299951553345
train gradient:  0.14151278594324757
iteration : 5505
train acc:  0.7421875
train loss:  0.5099966526031494
train gradient:  0.1467095211723849
iteration : 5506
train acc:  0.703125
train loss:  0.5015010237693787
train gradient:  0.12553348731600045
iteration : 5507
train acc:  0.734375
train loss:  0.4817819595336914
train gradient:  0.16184061816763456
iteration : 5508
train acc:  0.7578125
train loss:  0.5406460762023926
train gradient:  0.18653764368674736
iteration : 5509
train acc:  0.7890625
train loss:  0.5040286779403687
train gradient:  0.15381714220247364
iteration : 5510
train acc:  0.71875
train loss:  0.513933539390564
train gradient:  0.1364991042777009
iteration : 5511
train acc:  0.765625
train loss:  0.47338443994522095
train gradient:  0.16877849330656972
iteration : 5512
train acc:  0.765625
train loss:  0.4836394488811493
train gradient:  0.1212819309653489
iteration : 5513
train acc:  0.7578125
train loss:  0.4480205178260803
train gradient:  0.10243562519160444
iteration : 5514
train acc:  0.7109375
train loss:  0.5227574110031128
train gradient:  0.14595350039239197
iteration : 5515
train acc:  0.734375
train loss:  0.5006133317947388
train gradient:  0.1526128015540683
iteration : 5516
train acc:  0.6875
train loss:  0.5153295993804932
train gradient:  0.1785076150788748
iteration : 5517
train acc:  0.703125
train loss:  0.559974193572998
train gradient:  0.19516347422497907
iteration : 5518
train acc:  0.703125
train loss:  0.5276033282279968
train gradient:  0.15241885859860102
iteration : 5519
train acc:  0.765625
train loss:  0.5207934975624084
train gradient:  0.1327809671537334
iteration : 5520
train acc:  0.75
train loss:  0.4827355146408081
train gradient:  0.10913087952029843
iteration : 5521
train acc:  0.65625
train loss:  0.6196063756942749
train gradient:  0.21550517715990114
iteration : 5522
train acc:  0.7578125
train loss:  0.5356051921844482
train gradient:  0.14733951851797644
iteration : 5523
train acc:  0.7734375
train loss:  0.5011061429977417
train gradient:  0.1346423558725805
iteration : 5524
train acc:  0.7109375
train loss:  0.5072542428970337
train gradient:  0.1705480186795728
iteration : 5525
train acc:  0.671875
train loss:  0.5238125324249268
train gradient:  0.13877719671129263
iteration : 5526
train acc:  0.7265625
train loss:  0.5131709575653076
train gradient:  0.12237763845719178
iteration : 5527
train acc:  0.671875
train loss:  0.542311429977417
train gradient:  0.16810449816405565
iteration : 5528
train acc:  0.75
train loss:  0.5543512105941772
train gradient:  0.16644409078375136
iteration : 5529
train acc:  0.765625
train loss:  0.48397406935691833
train gradient:  0.16065308711101547
iteration : 5530
train acc:  0.7421875
train loss:  0.4822576940059662
train gradient:  0.12547080162144153
iteration : 5531
train acc:  0.765625
train loss:  0.485271155834198
train gradient:  0.1110113259565893
iteration : 5532
train acc:  0.765625
train loss:  0.45667532086372375
train gradient:  0.11006905318625394
iteration : 5533
train acc:  0.75
train loss:  0.49377191066741943
train gradient:  0.14569308853816715
iteration : 5534
train acc:  0.7421875
train loss:  0.4701869785785675
train gradient:  0.13882885921362562
iteration : 5535
train acc:  0.8046875
train loss:  0.43611159920692444
train gradient:  0.12811400524292643
iteration : 5536
train acc:  0.71875
train loss:  0.5413128137588501
train gradient:  0.1287355523090422
iteration : 5537
train acc:  0.6953125
train loss:  0.5532402396202087
train gradient:  0.1757211394044868
iteration : 5538
train acc:  0.6953125
train loss:  0.5109876394271851
train gradient:  0.17495219693357283
iteration : 5539
train acc:  0.6953125
train loss:  0.4896736145019531
train gradient:  0.19298844831805667
iteration : 5540
train acc:  0.71875
train loss:  0.49111247062683105
train gradient:  0.1366247769382081
iteration : 5541
train acc:  0.765625
train loss:  0.5369687080383301
train gradient:  0.19751232994870926
iteration : 5542
train acc:  0.75
train loss:  0.4803697466850281
train gradient:  0.12757383439422923
iteration : 5543
train acc:  0.703125
train loss:  0.55628901720047
train gradient:  0.1641950223035999
iteration : 5544
train acc:  0.78125
train loss:  0.49634885787963867
train gradient:  0.1442252082082839
iteration : 5545
train acc:  0.78125
train loss:  0.45263639092445374
train gradient:  0.11768575526653126
iteration : 5546
train acc:  0.6875
train loss:  0.5796708464622498
train gradient:  0.1763010334830909
iteration : 5547
train acc:  0.7265625
train loss:  0.500269889831543
train gradient:  0.12653695423665573
iteration : 5548
train acc:  0.7890625
train loss:  0.4729384183883667
train gradient:  0.18393593006755055
iteration : 5549
train acc:  0.671875
train loss:  0.5498384237289429
train gradient:  0.15440911343939173
iteration : 5550
train acc:  0.7421875
train loss:  0.5137158632278442
train gradient:  0.15690104935810795
iteration : 5551
train acc:  0.7578125
train loss:  0.5100193619728088
train gradient:  0.13578195922075967
iteration : 5552
train acc:  0.640625
train loss:  0.5976641178131104
train gradient:  0.19372836344702898
iteration : 5553
train acc:  0.7578125
train loss:  0.446718692779541
train gradient:  0.12530598907072757
iteration : 5554
train acc:  0.734375
train loss:  0.5315138697624207
train gradient:  0.14430133913439525
iteration : 5555
train acc:  0.734375
train loss:  0.5112982988357544
train gradient:  0.1612576494025178
iteration : 5556
train acc:  0.6796875
train loss:  0.5593452453613281
train gradient:  0.14275750201516973
iteration : 5557
train acc:  0.7578125
train loss:  0.454090416431427
train gradient:  0.11695088702629622
iteration : 5558
train acc:  0.7265625
train loss:  0.5542834401130676
train gradient:  0.1509454164479829
iteration : 5559
train acc:  0.7734375
train loss:  0.4772831201553345
train gradient:  0.155729261187358
iteration : 5560
train acc:  0.6953125
train loss:  0.5613226890563965
train gradient:  0.18068379099536536
iteration : 5561
train acc:  0.7578125
train loss:  0.4838542640209198
train gradient:  0.12647860666379557
iteration : 5562
train acc:  0.7890625
train loss:  0.46696004271507263
train gradient:  0.12869393519490893
iteration : 5563
train acc:  0.703125
train loss:  0.5339958667755127
train gradient:  0.21552702290928613
iteration : 5564
train acc:  0.734375
train loss:  0.5438868999481201
train gradient:  0.1635829756988478
iteration : 5565
train acc:  0.7421875
train loss:  0.5116031765937805
train gradient:  0.15006852850875352
iteration : 5566
train acc:  0.6875
train loss:  0.5691883563995361
train gradient:  0.16868783009226476
iteration : 5567
train acc:  0.7265625
train loss:  0.619907796382904
train gradient:  0.2579079254571295
iteration : 5568
train acc:  0.734375
train loss:  0.5220362544059753
train gradient:  0.16225438218698712
iteration : 5569
train acc:  0.6953125
train loss:  0.5574465990066528
train gradient:  0.18746474927454373
iteration : 5570
train acc:  0.7734375
train loss:  0.5022676587104797
train gradient:  0.14460544734431227
iteration : 5571
train acc:  0.734375
train loss:  0.4929143786430359
train gradient:  0.15249849282209027
iteration : 5572
train acc:  0.734375
train loss:  0.4556905925273895
train gradient:  0.12164317997883779
iteration : 5573
train acc:  0.7734375
train loss:  0.47608867287635803
train gradient:  0.1130198153070779
iteration : 5574
train acc:  0.8125
train loss:  0.4510558247566223
train gradient:  0.12871023017476613
iteration : 5575
train acc:  0.7109375
train loss:  0.5193904638290405
train gradient:  0.17200000349605954
iteration : 5576
train acc:  0.6875
train loss:  0.5732929706573486
train gradient:  0.2029491346025705
iteration : 5577
train acc:  0.7578125
train loss:  0.4886629581451416
train gradient:  0.14028029778852114
iteration : 5578
train acc:  0.7109375
train loss:  0.5404452085494995
train gradient:  0.1604799210075833
iteration : 5579
train acc:  0.734375
train loss:  0.5321241617202759
train gradient:  0.15144635123072991
iteration : 5580
train acc:  0.671875
train loss:  0.5803070068359375
train gradient:  0.14390182724752923
iteration : 5581
train acc:  0.7265625
train loss:  0.5130907297134399
train gradient:  0.15503147014881186
iteration : 5582
train acc:  0.7109375
train loss:  0.4900973439216614
train gradient:  0.1638522423243506
iteration : 5583
train acc:  0.75
train loss:  0.5110659003257751
train gradient:  0.15408144947243518
iteration : 5584
train acc:  0.7734375
train loss:  0.5558125376701355
train gradient:  0.171779684877107
iteration : 5585
train acc:  0.7421875
train loss:  0.5228562355041504
train gradient:  0.16413866838138674
iteration : 5586
train acc:  0.734375
train loss:  0.5360924601554871
train gradient:  0.17714520068439685
iteration : 5587
train acc:  0.703125
train loss:  0.5428659915924072
train gradient:  0.1519312930813333
iteration : 5588
train acc:  0.671875
train loss:  0.5600979924201965
train gradient:  0.15506611092960543
iteration : 5589
train acc:  0.65625
train loss:  0.550346851348877
train gradient:  0.1594908475162935
iteration : 5590
train acc:  0.7578125
train loss:  0.5003541707992554
train gradient:  0.14226269066588704
iteration : 5591
train acc:  0.6796875
train loss:  0.50225430727005
train gradient:  0.171163350875441
iteration : 5592
train acc:  0.765625
train loss:  0.4731883406639099
train gradient:  0.15723011863903918
iteration : 5593
train acc:  0.703125
train loss:  0.5197895169258118
train gradient:  0.1608491805692846
iteration : 5594
train acc:  0.6875
train loss:  0.5377805233001709
train gradient:  0.164630621571494
iteration : 5595
train acc:  0.75
train loss:  0.5198864936828613
train gradient:  0.15534450087519547
iteration : 5596
train acc:  0.671875
train loss:  0.5700134634971619
train gradient:  0.16994081572623632
iteration : 5597
train acc:  0.7421875
train loss:  0.5394244194030762
train gradient:  0.1789819662499795
iteration : 5598
train acc:  0.7578125
train loss:  0.43986690044403076
train gradient:  0.1106816400043816
iteration : 5599
train acc:  0.7109375
train loss:  0.4901602566242218
train gradient:  0.12036457140362766
iteration : 5600
train acc:  0.703125
train loss:  0.4876551032066345
train gradient:  0.17852334151457178
iteration : 5601
train acc:  0.7578125
train loss:  0.516913652420044
train gradient:  0.14703042744049
iteration : 5602
train acc:  0.703125
train loss:  0.5388191938400269
train gradient:  0.16045522943940083
iteration : 5603
train acc:  0.7109375
train loss:  0.5496797561645508
train gradient:  0.16382719054246592
iteration : 5604
train acc:  0.703125
train loss:  0.5715961456298828
train gradient:  0.1727229396240164
iteration : 5605
train acc:  0.7578125
train loss:  0.48541849851608276
train gradient:  0.11515042284390797
iteration : 5606
train acc:  0.7109375
train loss:  0.5201070308685303
train gradient:  0.1613133252607869
iteration : 5607
train acc:  0.7734375
train loss:  0.4510892629623413
train gradient:  0.10928268269958084
iteration : 5608
train acc:  0.7265625
train loss:  0.5386702418327332
train gradient:  0.18471047469239465
iteration : 5609
train acc:  0.765625
train loss:  0.4696395695209503
train gradient:  0.12523196029872818
iteration : 5610
train acc:  0.6796875
train loss:  0.5179599523544312
train gradient:  0.138951332486294
iteration : 5611
train acc:  0.734375
train loss:  0.508600115776062
train gradient:  0.13804852043125265
iteration : 5612
train acc:  0.8125
train loss:  0.4358079731464386
train gradient:  0.14058234615421267
iteration : 5613
train acc:  0.8125
train loss:  0.43821975588798523
train gradient:  0.11163188328438188
iteration : 5614
train acc:  0.75
train loss:  0.5212421417236328
train gradient:  0.16710162157859293
iteration : 5615
train acc:  0.7109375
train loss:  0.47806382179260254
train gradient:  0.13299189543709056
iteration : 5616
train acc:  0.7265625
train loss:  0.5011805295944214
train gradient:  0.16398291318120556
iteration : 5617
train acc:  0.7421875
train loss:  0.4725359082221985
train gradient:  0.14092874086048615
iteration : 5618
train acc:  0.71875
train loss:  0.5210663080215454
train gradient:  0.14128605908842048
iteration : 5619
train acc:  0.734375
train loss:  0.5436427593231201
train gradient:  0.20348869483815857
iteration : 5620
train acc:  0.7578125
train loss:  0.46992769837379456
train gradient:  0.12728159889629642
iteration : 5621
train acc:  0.6875
train loss:  0.5588569641113281
train gradient:  0.18908965143712692
iteration : 5622
train acc:  0.7578125
train loss:  0.47542262077331543
train gradient:  0.14537783025306372
iteration : 5623
train acc:  0.765625
train loss:  0.4502427279949188
train gradient:  0.12734333555095356
iteration : 5624
train acc:  0.7734375
train loss:  0.4647369682788849
train gradient:  0.11592126965820951
iteration : 5625
train acc:  0.6953125
train loss:  0.5398558378219604
train gradient:  0.20460484789695488
iteration : 5626
train acc:  0.71875
train loss:  0.5669816136360168
train gradient:  0.18516282990883545
iteration : 5627
train acc:  0.7578125
train loss:  0.5442802906036377
train gradient:  0.23402306043783205
iteration : 5628
train acc:  0.6875
train loss:  0.543235182762146
train gradient:  0.1703968156088088
iteration : 5629
train acc:  0.7578125
train loss:  0.5014793872833252
train gradient:  0.1439793518056037
iteration : 5630
train acc:  0.8125
train loss:  0.4399833679199219
train gradient:  0.13450081477015136
iteration : 5631
train acc:  0.765625
train loss:  0.5018768310546875
train gradient:  0.16228945066013634
iteration : 5632
train acc:  0.75
train loss:  0.5252220034599304
train gradient:  0.15223209249870623
iteration : 5633
train acc:  0.71875
train loss:  0.49885043501853943
train gradient:  0.14378316884082307
iteration : 5634
train acc:  0.8046875
train loss:  0.44132646918296814
train gradient:  0.10895077423795903
iteration : 5635
train acc:  0.7265625
train loss:  0.5361183881759644
train gradient:  0.15086695723781895
iteration : 5636
train acc:  0.8125
train loss:  0.4256284236907959
train gradient:  0.10885417423016079
iteration : 5637
train acc:  0.6484375
train loss:  0.5754754543304443
train gradient:  0.1586486751133006
iteration : 5638
train acc:  0.703125
train loss:  0.5364879369735718
train gradient:  0.20532091051961165
iteration : 5639
train acc:  0.7109375
train loss:  0.48237550258636475
train gradient:  0.14884247613438095
iteration : 5640
train acc:  0.75
train loss:  0.49795910716056824
train gradient:  0.15336304739892084
iteration : 5641
train acc:  0.75
train loss:  0.47906607389450073
train gradient:  0.12166105091546628
iteration : 5642
train acc:  0.765625
train loss:  0.4837683439254761
train gradient:  0.1670621388493072
iteration : 5643
train acc:  0.734375
train loss:  0.5586842894554138
train gradient:  0.18752404639334214
iteration : 5644
train acc:  0.7421875
train loss:  0.4740617573261261
train gradient:  0.12180248960712566
iteration : 5645
train acc:  0.6640625
train loss:  0.5886102914810181
train gradient:  0.30583579832594365
iteration : 5646
train acc:  0.6953125
train loss:  0.49411994218826294
train gradient:  0.16134297944132103
iteration : 5647
train acc:  0.65625
train loss:  0.6243184208869934
train gradient:  0.2532280951582906
iteration : 5648
train acc:  0.78125
train loss:  0.43527621030807495
train gradient:  0.1355874601693463
iteration : 5649
train acc:  0.765625
train loss:  0.4843749701976776
train gradient:  0.10735516200034938
iteration : 5650
train acc:  0.75
train loss:  0.4883841872215271
train gradient:  0.1377163164437402
iteration : 5651
train acc:  0.6796875
train loss:  0.5451592206954956
train gradient:  0.19028718497708985
iteration : 5652
train acc:  0.765625
train loss:  0.5034225583076477
train gradient:  0.12816854941430528
iteration : 5653
train acc:  0.734375
train loss:  0.531994640827179
train gradient:  0.15758280979996847
iteration : 5654
train acc:  0.6875
train loss:  0.5682257413864136
train gradient:  0.1456421149167683
iteration : 5655
train acc:  0.7109375
train loss:  0.5475105047225952
train gradient:  0.1961990906781247
iteration : 5656
train acc:  0.8046875
train loss:  0.40628570318222046
train gradient:  0.11308342706923152
iteration : 5657
train acc:  0.765625
train loss:  0.47421467304229736
train gradient:  0.12618525329954952
iteration : 5658
train acc:  0.671875
train loss:  0.5776324272155762
train gradient:  0.15942961529268446
iteration : 5659
train acc:  0.765625
train loss:  0.5280945897102356
train gradient:  0.1721329673787671
iteration : 5660
train acc:  0.75
train loss:  0.49909693002700806
train gradient:  0.1625482569871286
iteration : 5661
train acc:  0.7578125
train loss:  0.5107681155204773
train gradient:  0.18241259701652002
iteration : 5662
train acc:  0.71875
train loss:  0.5534883737564087
train gradient:  0.16568056701323836
iteration : 5663
train acc:  0.765625
train loss:  0.5364108681678772
train gradient:  0.15483723340419875
iteration : 5664
train acc:  0.7578125
train loss:  0.49954214692115784
train gradient:  0.14946986674267493
iteration : 5665
train acc:  0.7265625
train loss:  0.4930911958217621
train gradient:  0.16435195739596165
iteration : 5666
train acc:  0.796875
train loss:  0.47676920890808105
train gradient:  0.1577979283411671
iteration : 5667
train acc:  0.7265625
train loss:  0.5187090635299683
train gradient:  0.14456207357555612
iteration : 5668
train acc:  0.6796875
train loss:  0.5611361265182495
train gradient:  0.2007780235103045
iteration : 5669
train acc:  0.7265625
train loss:  0.5719327926635742
train gradient:  0.16934087179219298
iteration : 5670
train acc:  0.671875
train loss:  0.6436997652053833
train gradient:  0.27548150137570027
iteration : 5671
train acc:  0.6640625
train loss:  0.5532116293907166
train gradient:  0.21463057108835887
iteration : 5672
train acc:  0.6953125
train loss:  0.5249993801116943
train gradient:  0.157349580009827
iteration : 5673
train acc:  0.765625
train loss:  0.47785717248916626
train gradient:  0.10467571096861016
iteration : 5674
train acc:  0.75
train loss:  0.4789358377456665
train gradient:  0.14764368265937983
iteration : 5675
train acc:  0.7421875
train loss:  0.4805466830730438
train gradient:  0.15610590841377503
iteration : 5676
train acc:  0.6875
train loss:  0.5382198095321655
train gradient:  0.18717875490048208
iteration : 5677
train acc:  0.7265625
train loss:  0.5602965354919434
train gradient:  0.1665717018636929
iteration : 5678
train acc:  0.640625
train loss:  0.635606050491333
train gradient:  0.1658138349002381
iteration : 5679
train acc:  0.78125
train loss:  0.46737906336784363
train gradient:  0.15962432839442484
iteration : 5680
train acc:  0.703125
train loss:  0.5639064908027649
train gradient:  0.1668355652987831
iteration : 5681
train acc:  0.6875
train loss:  0.5765945911407471
train gradient:  0.1662533447967876
iteration : 5682
train acc:  0.7265625
train loss:  0.5109511613845825
train gradient:  0.19735252326068375
iteration : 5683
train acc:  0.71875
train loss:  0.5212404727935791
train gradient:  0.11739822932201587
iteration : 5684
train acc:  0.7265625
train loss:  0.5121973752975464
train gradient:  0.12583813612564448
iteration : 5685
train acc:  0.703125
train loss:  0.5714730620384216
train gradient:  0.21804001492889258
iteration : 5686
train acc:  0.6953125
train loss:  0.5413803458213806
train gradient:  0.19142814796949095
iteration : 5687
train acc:  0.765625
train loss:  0.448133647441864
train gradient:  0.12198920330525309
iteration : 5688
train acc:  0.71875
train loss:  0.5308664441108704
train gradient:  0.15477884212753407
iteration : 5689
train acc:  0.7265625
train loss:  0.5320830345153809
train gradient:  0.13357362482800456
iteration : 5690
train acc:  0.703125
train loss:  0.5041646361351013
train gradient:  0.14503237561582744
iteration : 5691
train acc:  0.703125
train loss:  0.5530383586883545
train gradient:  0.13864903704666026
iteration : 5692
train acc:  0.7265625
train loss:  0.5219858884811401
train gradient:  0.11731895662950319
iteration : 5693
train acc:  0.75
train loss:  0.5381430983543396
train gradient:  0.16673480318578748
iteration : 5694
train acc:  0.78125
train loss:  0.44748666882514954
train gradient:  0.13962200082720688
iteration : 5695
train acc:  0.7890625
train loss:  0.4979178011417389
train gradient:  0.12336756506861322
iteration : 5696
train acc:  0.7578125
train loss:  0.45762529969215393
train gradient:  0.09692928657161703
iteration : 5697
train acc:  0.7734375
train loss:  0.49770358204841614
train gradient:  0.12456406120951367
iteration : 5698
train acc:  0.7109375
train loss:  0.5020977258682251
train gradient:  0.1378344299742002
iteration : 5699
train acc:  0.734375
train loss:  0.49059656262397766
train gradient:  0.1270288854265782
iteration : 5700
train acc:  0.796875
train loss:  0.4729805588722229
train gradient:  0.12759468427199813
iteration : 5701
train acc:  0.8515625
train loss:  0.4578002691268921
train gradient:  0.13664349441989038
iteration : 5702
train acc:  0.78125
train loss:  0.4580875039100647
train gradient:  0.1204728835732299
iteration : 5703
train acc:  0.734375
train loss:  0.514295756816864
train gradient:  0.13633641110971384
iteration : 5704
train acc:  0.78125
train loss:  0.47105544805526733
train gradient:  0.1781444785522283
iteration : 5705
train acc:  0.78125
train loss:  0.4465271830558777
train gradient:  0.14638902303113793
iteration : 5706
train acc:  0.7890625
train loss:  0.4488082826137543
train gradient:  0.12828826803923124
iteration : 5707
train acc:  0.765625
train loss:  0.4864494800567627
train gradient:  0.10920476949208625
iteration : 5708
train acc:  0.7265625
train loss:  0.5304884910583496
train gradient:  0.1707762255245931
iteration : 5709
train acc:  0.703125
train loss:  0.5237734317779541
train gradient:  0.1622864428461417
iteration : 5710
train acc:  0.7421875
train loss:  0.5048307776451111
train gradient:  0.12795748081370298
iteration : 5711
train acc:  0.640625
train loss:  0.5920583009719849
train gradient:  0.30226793693504905
iteration : 5712
train acc:  0.7421875
train loss:  0.507597029209137
train gradient:  0.15576134390847937
iteration : 5713
train acc:  0.7578125
train loss:  0.4923539459705353
train gradient:  0.16684426330563784
iteration : 5714
train acc:  0.75
train loss:  0.45716220140457153
train gradient:  0.1282146269474641
iteration : 5715
train acc:  0.671875
train loss:  0.5504226088523865
train gradient:  0.12884294412985484
iteration : 5716
train acc:  0.71875
train loss:  0.555536150932312
train gradient:  0.1638719331044162
iteration : 5717
train acc:  0.7421875
train loss:  0.5311185717582703
train gradient:  0.15451669356124
iteration : 5718
train acc:  0.625
train loss:  0.5996648669242859
train gradient:  0.1758545644272349
iteration : 5719
train acc:  0.6640625
train loss:  0.5852371454238892
train gradient:  0.23047657376412195
iteration : 5720
train acc:  0.6875
train loss:  0.5420413017272949
train gradient:  0.16283911831702175
iteration : 5721
train acc:  0.71875
train loss:  0.4828506112098694
train gradient:  0.12385932832994258
iteration : 5722
train acc:  0.7578125
train loss:  0.464097797870636
train gradient:  0.10986955013841716
iteration : 5723
train acc:  0.75
train loss:  0.5184080600738525
train gradient:  0.17419055222014695
iteration : 5724
train acc:  0.65625
train loss:  0.5809139609336853
train gradient:  0.19584970399589913
iteration : 5725
train acc:  0.6953125
train loss:  0.5180448293685913
train gradient:  0.15641688330587122
iteration : 5726
train acc:  0.6875
train loss:  0.5259044170379639
train gradient:  0.12759471589575733
iteration : 5727
train acc:  0.75
train loss:  0.485815167427063
train gradient:  0.12779412418593686
iteration : 5728
train acc:  0.8203125
train loss:  0.4201459288597107
train gradient:  0.13331659749728103
iteration : 5729
train acc:  0.75
train loss:  0.5020405650138855
train gradient:  0.14069156535463218
iteration : 5730
train acc:  0.7265625
train loss:  0.47967737913131714
train gradient:  0.1383779792279118
iteration : 5731
train acc:  0.7578125
train loss:  0.4999235272407532
train gradient:  0.13836175758965102
iteration : 5732
train acc:  0.6484375
train loss:  0.5848028659820557
train gradient:  0.18398388487694511
iteration : 5733
train acc:  0.734375
train loss:  0.512482762336731
train gradient:  0.1861025687842095
iteration : 5734
train acc:  0.75
train loss:  0.4684801995754242
train gradient:  0.13015375831009218
iteration : 5735
train acc:  0.765625
train loss:  0.4814010560512543
train gradient:  0.13450535823309423
iteration : 5736
train acc:  0.765625
train loss:  0.5730348229408264
train gradient:  0.22313801331008426
iteration : 5737
train acc:  0.7734375
train loss:  0.44434913992881775
train gradient:  0.12676897761988035
iteration : 5738
train acc:  0.734375
train loss:  0.4900755286216736
train gradient:  0.1977680250104124
iteration : 5739
train acc:  0.65625
train loss:  0.5719486474990845
train gradient:  0.17596935913249995
iteration : 5740
train acc:  0.640625
train loss:  0.6451505422592163
train gradient:  0.2046547378378264
iteration : 5741
train acc:  0.6953125
train loss:  0.5626965761184692
train gradient:  0.1884658147927996
iteration : 5742
train acc:  0.65625
train loss:  0.5822632312774658
train gradient:  0.16460592387121128
iteration : 5743
train acc:  0.7578125
train loss:  0.4763195812702179
train gradient:  0.14025462847629416
iteration : 5744
train acc:  0.7421875
train loss:  0.49999725818634033
train gradient:  0.178721141207416
iteration : 5745
train acc:  0.75
train loss:  0.5144782066345215
train gradient:  0.16448227481945055
iteration : 5746
train acc:  0.7109375
train loss:  0.5201313495635986
train gradient:  0.13802858962412468
iteration : 5747
train acc:  0.7734375
train loss:  0.4639425277709961
train gradient:  0.12643484752050163
iteration : 5748
train acc:  0.8125
train loss:  0.4569324553012848
train gradient:  0.12223370102911568
iteration : 5749
train acc:  0.734375
train loss:  0.49860015511512756
train gradient:  0.1786876005347573
iteration : 5750
train acc:  0.75
train loss:  0.477893590927124
train gradient:  0.11411534375395307
iteration : 5751
train acc:  0.7265625
train loss:  0.48582780361175537
train gradient:  0.11708188198811718
iteration : 5752
train acc:  0.78125
train loss:  0.46183478832244873
train gradient:  0.13673307960134917
iteration : 5753
train acc:  0.6953125
train loss:  0.5574667453765869
train gradient:  0.1586629966059857
iteration : 5754
train acc:  0.7578125
train loss:  0.5571579337120056
train gradient:  0.14971123547538173
iteration : 5755
train acc:  0.8203125
train loss:  0.3994835615158081
train gradient:  0.11350403580312284
iteration : 5756
train acc:  0.7734375
train loss:  0.5340659618377686
train gradient:  0.13986152275292102
iteration : 5757
train acc:  0.6953125
train loss:  0.52705979347229
train gradient:  0.14892864396909616
iteration : 5758
train acc:  0.734375
train loss:  0.5506668090820312
train gradient:  0.1591923829771968
iteration : 5759
train acc:  0.7109375
train loss:  0.49743765592575073
train gradient:  0.12753892633758937
iteration : 5760
train acc:  0.7734375
train loss:  0.4253477454185486
train gradient:  0.12022130895981456
iteration : 5761
train acc:  0.765625
train loss:  0.49240100383758545
train gradient:  0.12975663402510493
iteration : 5762
train acc:  0.6953125
train loss:  0.6072152853012085
train gradient:  0.20036003686681103
iteration : 5763
train acc:  0.703125
train loss:  0.5471855401992798
train gradient:  0.12955734762265927
iteration : 5764
train acc:  0.8046875
train loss:  0.42581045627593994
train gradient:  0.11450283378865787
iteration : 5765
train acc:  0.7109375
train loss:  0.5669814348220825
train gradient:  0.20349812548426047
iteration : 5766
train acc:  0.796875
train loss:  0.463350385427475
train gradient:  0.11621367464197659
iteration : 5767
train acc:  0.703125
train loss:  0.5101597309112549
train gradient:  0.253942976925876
iteration : 5768
train acc:  0.7578125
train loss:  0.4697917699813843
train gradient:  0.1377841318211329
iteration : 5769
train acc:  0.71875
train loss:  0.5015389323234558
train gradient:  0.13682905168618487
iteration : 5770
train acc:  0.671875
train loss:  0.5769736766815186
train gradient:  0.16160006840984026
iteration : 5771
train acc:  0.7421875
train loss:  0.49290138483047485
train gradient:  0.10019534108549183
iteration : 5772
train acc:  0.671875
train loss:  0.5620789527893066
train gradient:  0.17459964485586194
iteration : 5773
train acc:  0.7109375
train loss:  0.536184549331665
train gradient:  0.14667219138310597
iteration : 5774
train acc:  0.7109375
train loss:  0.5568486452102661
train gradient:  0.14936785627764984
iteration : 5775
train acc:  0.7265625
train loss:  0.5170085430145264
train gradient:  0.13206416652897207
iteration : 5776
train acc:  0.7109375
train loss:  0.5608694553375244
train gradient:  0.16104696666127272
iteration : 5777
train acc:  0.8046875
train loss:  0.4765109121799469
train gradient:  0.14019744311247873
iteration : 5778
train acc:  0.8046875
train loss:  0.4666750729084015
train gradient:  0.12953265362971628
iteration : 5779
train acc:  0.6640625
train loss:  0.5546412467956543
train gradient:  0.14313887001720166
iteration : 5780
train acc:  0.734375
train loss:  0.5438461303710938
train gradient:  0.1438097683591752
iteration : 5781
train acc:  0.7109375
train loss:  0.5104360580444336
train gradient:  0.11847474598488276
iteration : 5782
train acc:  0.765625
train loss:  0.5128971338272095
train gradient:  0.18341793537671386
iteration : 5783
train acc:  0.6875
train loss:  0.5805309414863586
train gradient:  0.16573866933388057
iteration : 5784
train acc:  0.6953125
train loss:  0.5569019317626953
train gradient:  0.16099918542693276
iteration : 5785
train acc:  0.7890625
train loss:  0.4774724841117859
train gradient:  0.12944161121527345
iteration : 5786
train acc:  0.7109375
train loss:  0.5176941752433777
train gradient:  0.142421208540709
iteration : 5787
train acc:  0.7265625
train loss:  0.5052470564842224
train gradient:  0.12072521491755546
iteration : 5788
train acc:  0.7265625
train loss:  0.5605230331420898
train gradient:  0.1763450491309954
iteration : 5789
train acc:  0.78125
train loss:  0.4662119746208191
train gradient:  0.13570628261345183
iteration : 5790
train acc:  0.71875
train loss:  0.5389241576194763
train gradient:  0.1868838300930375
iteration : 5791
train acc:  0.6875
train loss:  0.5601729154586792
train gradient:  0.1473959557882864
iteration : 5792
train acc:  0.765625
train loss:  0.46741342544555664
train gradient:  0.12195046626216806
iteration : 5793
train acc:  0.71875
train loss:  0.5261228084564209
train gradient:  0.1752957558734603
iteration : 5794
train acc:  0.765625
train loss:  0.4582920968532562
train gradient:  0.1138997736971698
iteration : 5795
train acc:  0.6875
train loss:  0.526752769947052
train gradient:  0.207611084430808
iteration : 5796
train acc:  0.8046875
train loss:  0.460196316242218
train gradient:  0.11608997292800244
iteration : 5797
train acc:  0.7421875
train loss:  0.5213401317596436
train gradient:  0.14181107137491306
iteration : 5798
train acc:  0.7734375
train loss:  0.4864475429058075
train gradient:  0.1404138399316159
iteration : 5799
train acc:  0.7421875
train loss:  0.4791509509086609
train gradient:  0.12472976081152322
iteration : 5800
train acc:  0.7578125
train loss:  0.487107515335083
train gradient:  0.1373060024119948
iteration : 5801
train acc:  0.671875
train loss:  0.5751346349716187
train gradient:  0.15303120171024712
iteration : 5802
train acc:  0.71875
train loss:  0.5440574884414673
train gradient:  0.1852462059362741
iteration : 5803
train acc:  0.7421875
train loss:  0.51537024974823
train gradient:  0.13384198616513598
iteration : 5804
train acc:  0.8125
train loss:  0.4203541874885559
train gradient:  0.11399899857545251
iteration : 5805
train acc:  0.6953125
train loss:  0.5110692977905273
train gradient:  0.1850796615329071
iteration : 5806
train acc:  0.6953125
train loss:  0.5578888654708862
train gradient:  0.14859950193076116
iteration : 5807
train acc:  0.6875
train loss:  0.5479929447174072
train gradient:  0.1445613132740064
iteration : 5808
train acc:  0.6953125
train loss:  0.5295085906982422
train gradient:  0.15049577833252592
iteration : 5809
train acc:  0.7421875
train loss:  0.4718489646911621
train gradient:  0.13899457651743466
iteration : 5810
train acc:  0.7265625
train loss:  0.5095593929290771
train gradient:  0.1293878287919793
iteration : 5811
train acc:  0.78125
train loss:  0.44622570276260376
train gradient:  0.13248424692308797
iteration : 5812
train acc:  0.6953125
train loss:  0.5616205930709839
train gradient:  0.18833432251415577
iteration : 5813
train acc:  0.7578125
train loss:  0.5038392543792725
train gradient:  0.11554645705793626
iteration : 5814
train acc:  0.6953125
train loss:  0.5456839203834534
train gradient:  0.23778003601879086
iteration : 5815
train acc:  0.7578125
train loss:  0.5035945177078247
train gradient:  0.14937465272622147
iteration : 5816
train acc:  0.7734375
train loss:  0.48876696825027466
train gradient:  0.13867083837924699
iteration : 5817
train acc:  0.7421875
train loss:  0.47725629806518555
train gradient:  0.12747036855052749
iteration : 5818
train acc:  0.7421875
train loss:  0.4869803488254547
train gradient:  0.11696567928030635
iteration : 5819
train acc:  0.75
train loss:  0.48372864723205566
train gradient:  0.13450290010324062
iteration : 5820
train acc:  0.7734375
train loss:  0.4494856297969818
train gradient:  0.09576973127990437
iteration : 5821
train acc:  0.75
train loss:  0.5128136873245239
train gradient:  0.13683187192694415
iteration : 5822
train acc:  0.7734375
train loss:  0.47829821705818176
train gradient:  0.14438913395246356
iteration : 5823
train acc:  0.75
train loss:  0.4314820170402527
train gradient:  0.11295739831846799
iteration : 5824
train acc:  0.8125
train loss:  0.48047876358032227
train gradient:  0.11198643089371013
iteration : 5825
train acc:  0.765625
train loss:  0.47125717997550964
train gradient:  0.1410160642717075
iteration : 5826
train acc:  0.7578125
train loss:  0.4812639653682709
train gradient:  0.16013779250410803
iteration : 5827
train acc:  0.71875
train loss:  0.5226359367370605
train gradient:  0.17394544870675072
iteration : 5828
train acc:  0.734375
train loss:  0.5550425052642822
train gradient:  0.22707267723372748
iteration : 5829
train acc:  0.65625
train loss:  0.5686091184616089
train gradient:  0.2223541803770681
iteration : 5830
train acc:  0.7265625
train loss:  0.4935886859893799
train gradient:  0.13159021990461192
iteration : 5831
train acc:  0.6953125
train loss:  0.550614058971405
train gradient:  0.17842050874334853
iteration : 5832
train acc:  0.734375
train loss:  0.4968032240867615
train gradient:  0.1451231573250381
iteration : 5833
train acc:  0.6796875
train loss:  0.6142792105674744
train gradient:  0.20030105554322866
iteration : 5834
train acc:  0.7578125
train loss:  0.47660958766937256
train gradient:  0.106782032229965
iteration : 5835
train acc:  0.6953125
train loss:  0.5552910566329956
train gradient:  0.16500380557560868
iteration : 5836
train acc:  0.65625
train loss:  0.5837994813919067
train gradient:  0.19629747132175762
iteration : 5837
train acc:  0.75
train loss:  0.4945108890533447
train gradient:  0.14428423679369118
iteration : 5838
train acc:  0.765625
train loss:  0.4649413526058197
train gradient:  0.10351033413836781
iteration : 5839
train acc:  0.7578125
train loss:  0.524645209312439
train gradient:  0.13991401442372953
iteration : 5840
train acc:  0.7578125
train loss:  0.5019568204879761
train gradient:  0.16910945857805637
iteration : 5841
train acc:  0.734375
train loss:  0.5221065878868103
train gradient:  0.1523860637751085
iteration : 5842
train acc:  0.71875
train loss:  0.5254296064376831
train gradient:  0.13546805862252922
iteration : 5843
train acc:  0.7421875
train loss:  0.5124693512916565
train gradient:  0.12232115090883072
iteration : 5844
train acc:  0.7578125
train loss:  0.46815070509910583
train gradient:  0.14007376458713772
iteration : 5845
train acc:  0.7578125
train loss:  0.5130820870399475
train gradient:  0.15524944555952933
iteration : 5846
train acc:  0.75
train loss:  0.4954211711883545
train gradient:  0.12627881736031182
iteration : 5847
train acc:  0.75
train loss:  0.5007270574569702
train gradient:  0.2248505968408354
iteration : 5848
train acc:  0.78125
train loss:  0.4708865284919739
train gradient:  0.11082026231446655
iteration : 5849
train acc:  0.7890625
train loss:  0.43276864290237427
train gradient:  0.11160525754404081
iteration : 5850
train acc:  0.7578125
train loss:  0.5110606551170349
train gradient:  0.13848471830887615
iteration : 5851
train acc:  0.6640625
train loss:  0.5959330797195435
train gradient:  0.24341242646388683
iteration : 5852
train acc:  0.7578125
train loss:  0.4919634163379669
train gradient:  0.13541243773185957
iteration : 5853
train acc:  0.828125
train loss:  0.4408789873123169
train gradient:  0.10657886014695298
iteration : 5854
train acc:  0.7265625
train loss:  0.48165735602378845
train gradient:  0.11402982896037576
iteration : 5855
train acc:  0.734375
train loss:  0.5341908931732178
train gradient:  0.17947775123142806
iteration : 5856
train acc:  0.6953125
train loss:  0.5307811498641968
train gradient:  0.15284853143067817
iteration : 5857
train acc:  0.7578125
train loss:  0.459800660610199
train gradient:  0.1155607595613748
iteration : 5858
train acc:  0.6875
train loss:  0.5428078174591064
train gradient:  0.19377475887605258
iteration : 5859
train acc:  0.7109375
train loss:  0.5041816830635071
train gradient:  0.15437414162242383
iteration : 5860
train acc:  0.7890625
train loss:  0.44333553314208984
train gradient:  0.11897637139196804
iteration : 5861
train acc:  0.71875
train loss:  0.48439037799835205
train gradient:  0.11549821880805737
iteration : 5862
train acc:  0.6484375
train loss:  0.5871497988700867
train gradient:  0.16247173295943557
iteration : 5863
train acc:  0.765625
train loss:  0.46800464391708374
train gradient:  0.1120119521041759
iteration : 5864
train acc:  0.71875
train loss:  0.5250660181045532
train gradient:  0.1520270164821911
iteration : 5865
train acc:  0.7109375
train loss:  0.5227294564247131
train gradient:  0.19602708156120957
iteration : 5866
train acc:  0.6953125
train loss:  0.5235884785652161
train gradient:  0.1587000334487988
iteration : 5867
train acc:  0.765625
train loss:  0.4766443073749542
train gradient:  0.13041517251604182
iteration : 5868
train acc:  0.7421875
train loss:  0.4798625707626343
train gradient:  0.1389969207016341
iteration : 5869
train acc:  0.6875
train loss:  0.5132483839988708
train gradient:  0.1545068418020983
iteration : 5870
train acc:  0.75
train loss:  0.48752906918525696
train gradient:  0.1669589125535581
iteration : 5871
train acc:  0.6640625
train loss:  0.5596585273742676
train gradient:  0.23633360075751292
iteration : 5872
train acc:  0.7578125
train loss:  0.47390037775039673
train gradient:  0.16722557413501843
iteration : 5873
train acc:  0.7265625
train loss:  0.5063840746879578
train gradient:  0.11805697595540826
iteration : 5874
train acc:  0.71875
train loss:  0.5081846117973328
train gradient:  0.13665720428011027
iteration : 5875
train acc:  0.78125
train loss:  0.4463898539543152
train gradient:  0.10687086974224946
iteration : 5876
train acc:  0.8125
train loss:  0.45554986596107483
train gradient:  0.10166379496189555
iteration : 5877
train acc:  0.7421875
train loss:  0.5161053538322449
train gradient:  0.17345875568429975
iteration : 5878
train acc:  0.6875
train loss:  0.5374656915664673
train gradient:  0.14844994780385917
iteration : 5879
train acc:  0.703125
train loss:  0.5410585403442383
train gradient:  0.15743254632961978
iteration : 5880
train acc:  0.7109375
train loss:  0.5314438343048096
train gradient:  0.20505084339227408
iteration : 5881
train acc:  0.7578125
train loss:  0.4769544005393982
train gradient:  0.13341802952132023
iteration : 5882
train acc:  0.75
train loss:  0.4814552068710327
train gradient:  0.14791538415540242
iteration : 5883
train acc:  0.6953125
train loss:  0.5278825759887695
train gradient:  0.15924800133786204
iteration : 5884
train acc:  0.7578125
train loss:  0.4806520342826843
train gradient:  0.10131295814281017
iteration : 5885
train acc:  0.7578125
train loss:  0.4973411560058594
train gradient:  0.12892370116776247
iteration : 5886
train acc:  0.6953125
train loss:  0.5130491256713867
train gradient:  0.1780846822190098
iteration : 5887
train acc:  0.7109375
train loss:  0.49092304706573486
train gradient:  0.1470789644690023
iteration : 5888
train acc:  0.7578125
train loss:  0.4683663547039032
train gradient:  0.14048871223725412
iteration : 5889
train acc:  0.734375
train loss:  0.49544757604599
train gradient:  0.12796471675534662
iteration : 5890
train acc:  0.703125
train loss:  0.5554728507995605
train gradient:  0.185264331331832
iteration : 5891
train acc:  0.6875
train loss:  0.5263634920120239
train gradient:  0.14458936448941126
iteration : 5892
train acc:  0.7421875
train loss:  0.4732062816619873
train gradient:  0.13747890506892846
iteration : 5893
train acc:  0.6953125
train loss:  0.5461969375610352
train gradient:  0.20241744143502044
iteration : 5894
train acc:  0.71875
train loss:  0.5503749251365662
train gradient:  0.16761125432383278
iteration : 5895
train acc:  0.75
train loss:  0.4749671220779419
train gradient:  0.1350060631711374
iteration : 5896
train acc:  0.7421875
train loss:  0.49101513624191284
train gradient:  0.12754705259485874
iteration : 5897
train acc:  0.7734375
train loss:  0.456706702709198
train gradient:  0.11498554749459756
iteration : 5898
train acc:  0.6953125
train loss:  0.575110912322998
train gradient:  0.19005778661402362
iteration : 5899
train acc:  0.703125
train loss:  0.5059295296669006
train gradient:  0.1519805068917172
iteration : 5900
train acc:  0.7265625
train loss:  0.502508282661438
train gradient:  0.1226679557891826
iteration : 5901
train acc:  0.6875
train loss:  0.5748429298400879
train gradient:  0.16614543271817353
iteration : 5902
train acc:  0.7578125
train loss:  0.47005340456962585
train gradient:  0.1498918594255748
iteration : 5903
train acc:  0.8125
train loss:  0.47721922397613525
train gradient:  0.13520007091682532
iteration : 5904
train acc:  0.765625
train loss:  0.4828304648399353
train gradient:  0.15084542009648155
iteration : 5905
train acc:  0.71875
train loss:  0.5024325847625732
train gradient:  0.15435338549321645
iteration : 5906
train acc:  0.71875
train loss:  0.5433580279350281
train gradient:  0.16384901999334306
iteration : 5907
train acc:  0.71875
train loss:  0.5373914241790771
train gradient:  0.22137723576081517
iteration : 5908
train acc:  0.71875
train loss:  0.5106871128082275
train gradient:  0.18537640982104137
iteration : 5909
train acc:  0.765625
train loss:  0.47054463624954224
train gradient:  0.14411035579783782
iteration : 5910
train acc:  0.765625
train loss:  0.4946276545524597
train gradient:  0.1487502777967178
iteration : 5911
train acc:  0.734375
train loss:  0.5777530670166016
train gradient:  0.21132186204783854
iteration : 5912
train acc:  0.78125
train loss:  0.4925333857536316
train gradient:  0.1564290835978418
iteration : 5913
train acc:  0.7421875
train loss:  0.48078858852386475
train gradient:  0.14758336217504905
iteration : 5914
train acc:  0.8046875
train loss:  0.4507703185081482
train gradient:  0.1275724477176492
iteration : 5915
train acc:  0.7421875
train loss:  0.5048255324363708
train gradient:  0.15743948597209306
iteration : 5916
train acc:  0.734375
train loss:  0.5426731109619141
train gradient:  0.17938728089296802
iteration : 5917
train acc:  0.7109375
train loss:  0.5375402569770813
train gradient:  0.15818452955861467
iteration : 5918
train acc:  0.75
train loss:  0.44500088691711426
train gradient:  0.1558114728455567
iteration : 5919
train acc:  0.7578125
train loss:  0.4840225279331207
train gradient:  0.13905305875788085
iteration : 5920
train acc:  0.75
train loss:  0.5627027750015259
train gradient:  0.17621482422059312
iteration : 5921
train acc:  0.7109375
train loss:  0.4913557767868042
train gradient:  0.15604187678681886
iteration : 5922
train acc:  0.6640625
train loss:  0.652715802192688
train gradient:  0.20770353854277945
iteration : 5923
train acc:  0.765625
train loss:  0.4871350824832916
train gradient:  0.12791311138521383
iteration : 5924
train acc:  0.6875
train loss:  0.5671117901802063
train gradient:  0.1835642154460162
iteration : 5925
train acc:  0.703125
train loss:  0.5516233444213867
train gradient:  0.17218487871371785
iteration : 5926
train acc:  0.75
train loss:  0.5046694278717041
train gradient:  0.1506155702135533
iteration : 5927
train acc:  0.734375
train loss:  0.5132867097854614
train gradient:  0.17431165563731943
iteration : 5928
train acc:  0.6796875
train loss:  0.5694267153739929
train gradient:  0.16339745563509
iteration : 5929
train acc:  0.6796875
train loss:  0.5468828678131104
train gradient:  0.1235443101133621
iteration : 5930
train acc:  0.7578125
train loss:  0.5163107514381409
train gradient:  0.16072795216545005
iteration : 5931
train acc:  0.71875
train loss:  0.5024872422218323
train gradient:  0.1159718030713603
iteration : 5932
train acc:  0.734375
train loss:  0.5302261114120483
train gradient:  0.15887110935237647
iteration : 5933
train acc:  0.75
train loss:  0.519655704498291
train gradient:  0.1286687317076512
iteration : 5934
train acc:  0.7421875
train loss:  0.500796377658844
train gradient:  0.13168542549497103
iteration : 5935
train acc:  0.78125
train loss:  0.4968864321708679
train gradient:  0.16345010338866714
iteration : 5936
train acc:  0.734375
train loss:  0.5112953186035156
train gradient:  0.14908558907526204
iteration : 5937
train acc:  0.71875
train loss:  0.4881628751754761
train gradient:  0.12456082393138054
iteration : 5938
train acc:  0.7421875
train loss:  0.5093035101890564
train gradient:  0.13139593443708864
iteration : 5939
train acc:  0.796875
train loss:  0.44954928755760193
train gradient:  0.11383650735711814
iteration : 5940
train acc:  0.75
train loss:  0.49256837368011475
train gradient:  0.16517579091526197
iteration : 5941
train acc:  0.75
train loss:  0.506490170955658
train gradient:  0.13499775988994314
iteration : 5942
train acc:  0.7734375
train loss:  0.4469124972820282
train gradient:  0.1009597107521648
iteration : 5943
train acc:  0.6875
train loss:  0.5423746109008789
train gradient:  0.14484047265336747
iteration : 5944
train acc:  0.6796875
train loss:  0.5695109367370605
train gradient:  0.1682439415391152
iteration : 5945
train acc:  0.7109375
train loss:  0.5013074278831482
train gradient:  0.10612481566919964
iteration : 5946
train acc:  0.7265625
train loss:  0.5121126174926758
train gradient:  0.17159054555992825
iteration : 5947
train acc:  0.703125
train loss:  0.4874018430709839
train gradient:  0.13140481459093784
iteration : 5948
train acc:  0.640625
train loss:  0.5502666234970093
train gradient:  0.20393168256949423
iteration : 5949
train acc:  0.7734375
train loss:  0.4469662308692932
train gradient:  0.10390907567826314
iteration : 5950
train acc:  0.8046875
train loss:  0.4791328012943268
train gradient:  0.14426741798949938
iteration : 5951
train acc:  0.6953125
train loss:  0.5393192172050476
train gradient:  0.12354619262392706
iteration : 5952
train acc:  0.703125
train loss:  0.5403339862823486
train gradient:  0.15234856112565553
iteration : 5953
train acc:  0.734375
train loss:  0.5318976044654846
train gradient:  0.15058545256196787
iteration : 5954
train acc:  0.6484375
train loss:  0.577617883682251
train gradient:  0.16101589539007727
iteration : 5955
train acc:  0.7109375
train loss:  0.5098601579666138
train gradient:  0.14710655777145965
iteration : 5956
train acc:  0.7109375
train loss:  0.5246092081069946
train gradient:  0.17506448012110631
iteration : 5957
train acc:  0.734375
train loss:  0.4916045665740967
train gradient:  0.17430761818192314
iteration : 5958
train acc:  0.7109375
train loss:  0.48090797662734985
train gradient:  0.12813871733241805
iteration : 5959
train acc:  0.671875
train loss:  0.5802415013313293
train gradient:  0.1997755465145581
iteration : 5960
train acc:  0.7890625
train loss:  0.5069990754127502
train gradient:  0.13595734191701556
iteration : 5961
train acc:  0.703125
train loss:  0.49562138319015503
train gradient:  0.18006494445486304
iteration : 5962
train acc:  0.71875
train loss:  0.5357187986373901
train gradient:  0.17363685784013633
iteration : 5963
train acc:  0.71875
train loss:  0.5598229169845581
train gradient:  0.15920708550705126
iteration : 5964
train acc:  0.75
train loss:  0.5220803618431091
train gradient:  0.15418205338757132
iteration : 5965
train acc:  0.75
train loss:  0.5135126113891602
train gradient:  0.10959838634386898
iteration : 5966
train acc:  0.796875
train loss:  0.4136861562728882
train gradient:  0.11450148497284864
iteration : 5967
train acc:  0.7421875
train loss:  0.5031198859214783
train gradient:  0.13828605092330154
iteration : 5968
train acc:  0.7890625
train loss:  0.45993131399154663
train gradient:  0.13367313187734564
iteration : 5969
train acc:  0.7890625
train loss:  0.43136489391326904
train gradient:  0.12536648870441183
iteration : 5970
train acc:  0.7265625
train loss:  0.49038246273994446
train gradient:  0.10514652146298485
iteration : 5971
train acc:  0.7265625
train loss:  0.5073591470718384
train gradient:  0.14311976613938482
iteration : 5972
train acc:  0.703125
train loss:  0.5299134254455566
train gradient:  0.1272371585979113
iteration : 5973
train acc:  0.7421875
train loss:  0.4779443144798279
train gradient:  0.12586170356526333
iteration : 5974
train acc:  0.71875
train loss:  0.5024676322937012
train gradient:  0.16614157020440418
iteration : 5975
train acc:  0.71875
train loss:  0.567846417427063
train gradient:  0.14857482898587562
iteration : 5976
train acc:  0.671875
train loss:  0.6020985841751099
train gradient:  0.2065895779052685
iteration : 5977
train acc:  0.7109375
train loss:  0.5079426765441895
train gradient:  0.16270261977409228
iteration : 5978
train acc:  0.7109375
train loss:  0.534390389919281
train gradient:  0.1853254554738719
iteration : 5979
train acc:  0.75
train loss:  0.4648091495037079
train gradient:  0.1095197829375785
iteration : 5980
train acc:  0.6953125
train loss:  0.5056412220001221
train gradient:  0.13019960011096274
iteration : 5981
train acc:  0.75
train loss:  0.5175952315330505
train gradient:  0.13883939648782562
iteration : 5982
train acc:  0.7421875
train loss:  0.5301319360733032
train gradient:  0.17860896578590926
iteration : 5983
train acc:  0.6953125
train loss:  0.59633868932724
train gradient:  0.19852635642508282
iteration : 5984
train acc:  0.7265625
train loss:  0.49824780225753784
train gradient:  0.12208218099283973
iteration : 5985
train acc:  0.7578125
train loss:  0.47164851427078247
train gradient:  0.11437464773423664
iteration : 5986
train acc:  0.7890625
train loss:  0.5344728231430054
train gradient:  0.14465563421274502
iteration : 5987
train acc:  0.765625
train loss:  0.5064998865127563
train gradient:  0.126704520416036
iteration : 5988
train acc:  0.71875
train loss:  0.5418009161949158
train gradient:  0.17622647448138276
iteration : 5989
train acc:  0.703125
train loss:  0.5358038544654846
train gradient:  0.15504576157056543
iteration : 5990
train acc:  0.71875
train loss:  0.5242450833320618
train gradient:  0.15845680358186148
iteration : 5991
train acc:  0.703125
train loss:  0.5911278128623962
train gradient:  0.18024146075047753
iteration : 5992
train acc:  0.7265625
train loss:  0.4886283278465271
train gradient:  0.14373141080150195
iteration : 5993
train acc:  0.6796875
train loss:  0.5722590684890747
train gradient:  0.14544956310053933
iteration : 5994
train acc:  0.7421875
train loss:  0.5290894508361816
train gradient:  0.1624116027845316
iteration : 5995
train acc:  0.703125
train loss:  0.5169346332550049
train gradient:  0.12228236129146926
iteration : 5996
train acc:  0.7578125
train loss:  0.5198206901550293
train gradient:  0.12998654288516076
iteration : 5997
train acc:  0.7421875
train loss:  0.45195525884628296
train gradient:  0.10649577013750441
iteration : 5998
train acc:  0.75
train loss:  0.4831446409225464
train gradient:  0.16268513899180934
iteration : 5999
train acc:  0.7421875
train loss:  0.5442415475845337
train gradient:  0.2066090459420457
iteration : 6000
train acc:  0.7890625
train loss:  0.46001794934272766
train gradient:  0.14639523418927114
iteration : 6001
train acc:  0.7890625
train loss:  0.47623103857040405
train gradient:  0.12408664975910293
iteration : 6002
train acc:  0.75
train loss:  0.5428833365440369
train gradient:  0.15247219402665255
iteration : 6003
train acc:  0.7890625
train loss:  0.4756596088409424
train gradient:  0.13085741953898933
iteration : 6004
train acc:  0.7734375
train loss:  0.4599684774875641
train gradient:  0.13375521005448157
iteration : 6005
train acc:  0.7734375
train loss:  0.4800828695297241
train gradient:  0.1495893115802993
iteration : 6006
train acc:  0.78125
train loss:  0.48473966121673584
train gradient:  0.14942291294218518
iteration : 6007
train acc:  0.71875
train loss:  0.5031142234802246
train gradient:  0.11753380015399287
iteration : 6008
train acc:  0.7578125
train loss:  0.4867989122867584
train gradient:  0.11912695435397895
iteration : 6009
train acc:  0.7421875
train loss:  0.5204765200614929
train gradient:  0.16205195462702995
iteration : 6010
train acc:  0.7421875
train loss:  0.5003093481063843
train gradient:  0.16925794464660526
iteration : 6011
train acc:  0.75
train loss:  0.5034608840942383
train gradient:  0.16348931305141978
iteration : 6012
train acc:  0.6015625
train loss:  0.6377391219139099
train gradient:  0.24202419783333576
iteration : 6013
train acc:  0.7734375
train loss:  0.4979247450828552
train gradient:  0.14695470610776812
iteration : 6014
train acc:  0.7890625
train loss:  0.451774001121521
train gradient:  0.12914940382298862
iteration : 6015
train acc:  0.6796875
train loss:  0.5384253263473511
train gradient:  0.18323306404096562
iteration : 6016
train acc:  0.7109375
train loss:  0.5698797702789307
train gradient:  0.2312994254916403
iteration : 6017
train acc:  0.8046875
train loss:  0.4418882727622986
train gradient:  0.09497974724110778
iteration : 6018
train acc:  0.7421875
train loss:  0.5196496844291687
train gradient:  0.14759212903602176
iteration : 6019
train acc:  0.7265625
train loss:  0.5165701508522034
train gradient:  0.13906795165546362
iteration : 6020
train acc:  0.734375
train loss:  0.49248209595680237
train gradient:  0.13669760816443105
iteration : 6021
train acc:  0.75
train loss:  0.49723726511001587
train gradient:  0.13280913874515177
iteration : 6022
train acc:  0.75
train loss:  0.47470223903656006
train gradient:  0.12932162199455471
iteration : 6023
train acc:  0.734375
train loss:  0.5800193548202515
train gradient:  0.21674570123318995
iteration : 6024
train acc:  0.734375
train loss:  0.4705618619918823
train gradient:  0.14057047938144976
iteration : 6025
train acc:  0.7265625
train loss:  0.6013575792312622
train gradient:  0.169685026468509
iteration : 6026
train acc:  0.7421875
train loss:  0.5045035481452942
train gradient:  0.20124338634737704
iteration : 6027
train acc:  0.7265625
train loss:  0.5488805174827576
train gradient:  0.17625230094209304
iteration : 6028
train acc:  0.6953125
train loss:  0.6130783557891846
train gradient:  0.35419574776862617
iteration : 6029
train acc:  0.796875
train loss:  0.4604322612285614
train gradient:  0.15922917799748149
iteration : 6030
train acc:  0.71875
train loss:  0.5450083017349243
train gradient:  0.16514378599814414
iteration : 6031
train acc:  0.765625
train loss:  0.4818521738052368
train gradient:  0.13229311571856037
iteration : 6032
train acc:  0.8046875
train loss:  0.451118141412735
train gradient:  0.1324086764029981
iteration : 6033
train acc:  0.7578125
train loss:  0.5082526206970215
train gradient:  0.21759147126768252
iteration : 6034
train acc:  0.6171875
train loss:  0.6168815493583679
train gradient:  0.23652008913175226
iteration : 6035
train acc:  0.7421875
train loss:  0.5253198146820068
train gradient:  0.17132461763801327
iteration : 6036
train acc:  0.7578125
train loss:  0.5153781175613403
train gradient:  0.12468140002533401
iteration : 6037
train acc:  0.765625
train loss:  0.50447016954422
train gradient:  0.1243266409315994
iteration : 6038
train acc:  0.765625
train loss:  0.47833967208862305
train gradient:  0.13287151005217995
iteration : 6039
train acc:  0.703125
train loss:  0.6071563959121704
train gradient:  0.2429866140818998
iteration : 6040
train acc:  0.71875
train loss:  0.5375689268112183
train gradient:  0.14374560593138128
iteration : 6041
train acc:  0.7734375
train loss:  0.48344579339027405
train gradient:  0.10589683729909613
iteration : 6042
train acc:  0.8359375
train loss:  0.43730875849723816
train gradient:  0.1120615162409574
iteration : 6043
train acc:  0.71875
train loss:  0.49293485283851624
train gradient:  0.15565492182187274
iteration : 6044
train acc:  0.7265625
train loss:  0.5017566680908203
train gradient:  0.14451170425789064
iteration : 6045
train acc:  0.71875
train loss:  0.483637273311615
train gradient:  0.15422848591251362
iteration : 6046
train acc:  0.7578125
train loss:  0.4509987235069275
train gradient:  0.13729807899417035
iteration : 6047
train acc:  0.765625
train loss:  0.49124574661254883
train gradient:  0.13706420441574432
iteration : 6048
train acc:  0.7421875
train loss:  0.5104108452796936
train gradient:  0.1310564096899668
iteration : 6049
train acc:  0.7265625
train loss:  0.5576974153518677
train gradient:  0.15555422769146549
iteration : 6050
train acc:  0.765625
train loss:  0.47699251770973206
train gradient:  0.158902578885403
iteration : 6051
train acc:  0.765625
train loss:  0.46399205923080444
train gradient:  0.13647321402392587
iteration : 6052
train acc:  0.6875
train loss:  0.5029839873313904
train gradient:  0.12819923437902622
iteration : 6053
train acc:  0.75
train loss:  0.5159716606140137
train gradient:  0.12803023914566658
iteration : 6054
train acc:  0.7265625
train loss:  0.47672736644744873
train gradient:  0.11628813941568605
iteration : 6055
train acc:  0.734375
train loss:  0.5305981040000916
train gradient:  0.13951429071963367
iteration : 6056
train acc:  0.6796875
train loss:  0.5599321126937866
train gradient:  0.19444600713637233
iteration : 6057
train acc:  0.671875
train loss:  0.5718188285827637
train gradient:  0.18112254635757236
iteration : 6058
train acc:  0.7265625
train loss:  0.4822196364402771
train gradient:  0.12454202129695414
iteration : 6059
train acc:  0.7421875
train loss:  0.4526033401489258
train gradient:  0.11792383563079806
iteration : 6060
train acc:  0.6796875
train loss:  0.5410894155502319
train gradient:  0.23476577831792972
iteration : 6061
train acc:  0.671875
train loss:  0.5901668071746826
train gradient:  0.20159849005051128
iteration : 6062
train acc:  0.78125
train loss:  0.4658014476299286
train gradient:  0.11911859877000348
iteration : 6063
train acc:  0.75
train loss:  0.4946511387825012
train gradient:  0.13687922863762503
iteration : 6064
train acc:  0.734375
train loss:  0.5675824880599976
train gradient:  0.162186727317073
iteration : 6065
train acc:  0.8203125
train loss:  0.446691632270813
train gradient:  0.1183651124489307
iteration : 6066
train acc:  0.6953125
train loss:  0.5280467867851257
train gradient:  0.1393951226234608
iteration : 6067
train acc:  0.8046875
train loss:  0.45731207728385925
train gradient:  0.1363712796904769
iteration : 6068
train acc:  0.7734375
train loss:  0.47779184579849243
train gradient:  0.12837787383303462
iteration : 6069
train acc:  0.7734375
train loss:  0.45167720317840576
train gradient:  0.11639647454598338
iteration : 6070
train acc:  0.734375
train loss:  0.5154739022254944
train gradient:  0.16653461160015548
iteration : 6071
train acc:  0.7734375
train loss:  0.4576776623725891
train gradient:  0.15519829000004587
iteration : 6072
train acc:  0.734375
train loss:  0.5047684907913208
train gradient:  0.18889448153472516
iteration : 6073
train acc:  0.7265625
train loss:  0.5082353353500366
train gradient:  0.17928488636104217
iteration : 6074
train acc:  0.7578125
train loss:  0.4679933190345764
train gradient:  0.16651343427106347
iteration : 6075
train acc:  0.8203125
train loss:  0.41757631301879883
train gradient:  0.13036743274647689
iteration : 6076
train acc:  0.734375
train loss:  0.5005600452423096
train gradient:  0.11020633167138735
iteration : 6077
train acc:  0.7421875
train loss:  0.5231819152832031
train gradient:  0.12035726864041676
iteration : 6078
train acc:  0.8515625
train loss:  0.4216305613517761
train gradient:  0.11773192673505932
iteration : 6079
train acc:  0.765625
train loss:  0.4365948438644409
train gradient:  0.09751433004765372
iteration : 6080
train acc:  0.71875
train loss:  0.5248560905456543
train gradient:  0.16092539325636973
iteration : 6081
train acc:  0.78125
train loss:  0.4489634335041046
train gradient:  0.15177722382638387
iteration : 6082
train acc:  0.7890625
train loss:  0.5277294516563416
train gradient:  0.1904752560311247
iteration : 6083
train acc:  0.75
train loss:  0.4911394715309143
train gradient:  0.14275361188003877
iteration : 6084
train acc:  0.703125
train loss:  0.5006498098373413
train gradient:  0.17396830580803666
iteration : 6085
train acc:  0.7265625
train loss:  0.521054744720459
train gradient:  0.15429301239567644
iteration : 6086
train acc:  0.71875
train loss:  0.49032363295555115
train gradient:  0.12478273464821606
iteration : 6087
train acc:  0.703125
train loss:  0.515300989151001
train gradient:  0.13763662013802952
iteration : 6088
train acc:  0.7578125
train loss:  0.5119017362594604
train gradient:  0.15101496494896172
iteration : 6089
train acc:  0.7421875
train loss:  0.48178184032440186
train gradient:  0.15565516744222818
iteration : 6090
train acc:  0.6875
train loss:  0.5236461758613586
train gradient:  0.13176837599256688
iteration : 6091
train acc:  0.65625
train loss:  0.5763217210769653
train gradient:  0.15922023896985726
iteration : 6092
train acc:  0.71875
train loss:  0.4921150803565979
train gradient:  0.16156900225859905
iteration : 6093
train acc:  0.6640625
train loss:  0.5767167210578918
train gradient:  0.18390972330602295
iteration : 6094
train acc:  0.8046875
train loss:  0.45030441880226135
train gradient:  0.21672881346235068
iteration : 6095
train acc:  0.7578125
train loss:  0.48184430599212646
train gradient:  0.12265276076748122
iteration : 6096
train acc:  0.7265625
train loss:  0.4694230556488037
train gradient:  0.10450304830997928
iteration : 6097
train acc:  0.734375
train loss:  0.5290975570678711
train gradient:  0.14475014081317897
iteration : 6098
train acc:  0.671875
train loss:  0.5660269260406494
train gradient:  0.2002719878666857
iteration : 6099
train acc:  0.7265625
train loss:  0.5101264119148254
train gradient:  0.16411238789839677
iteration : 6100
train acc:  0.734375
train loss:  0.5097516179084778
train gradient:  0.17055181120531
iteration : 6101
train acc:  0.71875
train loss:  0.507420539855957
train gradient:  0.12806782414299986
iteration : 6102
train acc:  0.71875
train loss:  0.4956289827823639
train gradient:  0.13567173560491325
iteration : 6103
train acc:  0.78125
train loss:  0.457712322473526
train gradient:  0.12848666265250056
iteration : 6104
train acc:  0.703125
train loss:  0.5390058755874634
train gradient:  0.1547952072405654
iteration : 6105
train acc:  0.6953125
train loss:  0.5619524717330933
train gradient:  0.1844240651565719
iteration : 6106
train acc:  0.7734375
train loss:  0.481077641248703
train gradient:  0.1259463934997327
iteration : 6107
train acc:  0.640625
train loss:  0.593294620513916
train gradient:  0.22007889677671005
iteration : 6108
train acc:  0.7421875
train loss:  0.4820822477340698
train gradient:  0.1264946965362297
iteration : 6109
train acc:  0.671875
train loss:  0.5408875942230225
train gradient:  0.1702562855616136
iteration : 6110
train acc:  0.75
train loss:  0.5215921998023987
train gradient:  0.15380081978841834
iteration : 6111
train acc:  0.765625
train loss:  0.5201033353805542
train gradient:  0.17484603511695518
iteration : 6112
train acc:  0.6640625
train loss:  0.5189971327781677
train gradient:  0.11765680188388283
iteration : 6113
train acc:  0.75
train loss:  0.5155664682388306
train gradient:  0.14383958621959334
iteration : 6114
train acc:  0.6796875
train loss:  0.5819011926651001
train gradient:  0.20893097758230794
iteration : 6115
train acc:  0.671875
train loss:  0.5719667673110962
train gradient:  0.16340347160330204
iteration : 6116
train acc:  0.6796875
train loss:  0.5514428615570068
train gradient:  0.17052313966501847
iteration : 6117
train acc:  0.7578125
train loss:  0.5176311731338501
train gradient:  0.14469734809210277
iteration : 6118
train acc:  0.7421875
train loss:  0.4973156750202179
train gradient:  0.13182721872701877
iteration : 6119
train acc:  0.734375
train loss:  0.4902321696281433
train gradient:  0.13581819967279213
iteration : 6120
train acc:  0.7578125
train loss:  0.5215494632720947
train gradient:  0.1326262706543979
iteration : 6121
train acc:  0.734375
train loss:  0.5058885812759399
train gradient:  0.1522806537087391
iteration : 6122
train acc:  0.6875
train loss:  0.553552508354187
train gradient:  0.16766021283777888
iteration : 6123
train acc:  0.734375
train loss:  0.48482486605644226
train gradient:  0.14640503271179178
iteration : 6124
train acc:  0.8046875
train loss:  0.4948301315307617
train gradient:  0.11682929343313331
iteration : 6125
train acc:  0.7265625
train loss:  0.5268614888191223
train gradient:  0.14521505542388818
iteration : 6126
train acc:  0.71875
train loss:  0.5887815356254578
train gradient:  0.17873770648936332
iteration : 6127
train acc:  0.75
train loss:  0.47353753447532654
train gradient:  0.12190458452636596
iteration : 6128
train acc:  0.7421875
train loss:  0.4790666401386261
train gradient:  0.12716402507873426
iteration : 6129
train acc:  0.7109375
train loss:  0.5197582244873047
train gradient:  0.12744546990654537
iteration : 6130
train acc:  0.796875
train loss:  0.4802831709384918
train gradient:  0.10896674042983026
iteration : 6131
train acc:  0.6640625
train loss:  0.5296303629875183
train gradient:  0.18055610426898822
iteration : 6132
train acc:  0.7265625
train loss:  0.5247317552566528
train gradient:  0.17324737768867185
iteration : 6133
train acc:  0.765625
train loss:  0.5185624361038208
train gradient:  0.16938126347935734
iteration : 6134
train acc:  0.6953125
train loss:  0.5408070683479309
train gradient:  0.14354477123729653
iteration : 6135
train acc:  0.7578125
train loss:  0.5170470476150513
train gradient:  0.12747844830447397
iteration : 6136
train acc:  0.7265625
train loss:  0.5275686979293823
train gradient:  0.13202454229007926
iteration : 6137
train acc:  0.7890625
train loss:  0.48652487993240356
train gradient:  0.1398633226031888
iteration : 6138
train acc:  0.71875
train loss:  0.5307546854019165
train gradient:  0.15108145078983415
iteration : 6139
train acc:  0.765625
train loss:  0.5399646162986755
train gradient:  0.15008326339709904
iteration : 6140
train acc:  0.8125
train loss:  0.4257151484489441
train gradient:  0.0985209402595633
iteration : 6141
train acc:  0.7109375
train loss:  0.5197421312332153
train gradient:  0.13059820150810908
iteration : 6142
train acc:  0.7109375
train loss:  0.49101924896240234
train gradient:  0.13776124292475928
iteration : 6143
train acc:  0.8046875
train loss:  0.5065307021141052
train gradient:  0.1467226260412386
iteration : 6144
train acc:  0.78125
train loss:  0.48401951789855957
train gradient:  0.15767304678266258
iteration : 6145
train acc:  0.703125
train loss:  0.5265512466430664
train gradient:  0.13796082477344063
iteration : 6146
train acc:  0.765625
train loss:  0.501009464263916
train gradient:  0.16394035015554453
iteration : 6147
train acc:  0.734375
train loss:  0.4923291802406311
train gradient:  0.1324680260818234
iteration : 6148
train acc:  0.65625
train loss:  0.5231742858886719
train gradient:  0.18016371770130657
iteration : 6149
train acc:  0.75
train loss:  0.45813509821891785
train gradient:  0.12421938213212587
iteration : 6150
train acc:  0.75
train loss:  0.4650033712387085
train gradient:  0.1555352385559372
iteration : 6151
train acc:  0.7265625
train loss:  0.5528049468994141
train gradient:  0.20650330894675767
iteration : 6152
train acc:  0.7109375
train loss:  0.5647625923156738
train gradient:  0.1587912978399786
iteration : 6153
train acc:  0.7578125
train loss:  0.4638407230377197
train gradient:  0.10915487764017145
iteration : 6154
train acc:  0.734375
train loss:  0.4694579541683197
train gradient:  0.1319083097979009
iteration : 6155
train acc:  0.796875
train loss:  0.4578382670879364
train gradient:  0.1353880488969358
iteration : 6156
train acc:  0.7421875
train loss:  0.5053069591522217
train gradient:  0.18153405048281906
iteration : 6157
train acc:  0.6953125
train loss:  0.5748254060745239
train gradient:  0.21766731179813614
iteration : 6158
train acc:  0.796875
train loss:  0.46671709418296814
train gradient:  0.1358414248133556
iteration : 6159
train acc:  0.6953125
train loss:  0.5592325925827026
train gradient:  0.16424841489419467
iteration : 6160
train acc:  0.7421875
train loss:  0.5137723088264465
train gradient:  0.17122477423551077
iteration : 6161
train acc:  0.75
train loss:  0.5068698525428772
train gradient:  0.13042606061308237
iteration : 6162
train acc:  0.734375
train loss:  0.488023042678833
train gradient:  0.12597921379292065
iteration : 6163
train acc:  0.7421875
train loss:  0.5172175168991089
train gradient:  0.1778230790621311
iteration : 6164
train acc:  0.7109375
train loss:  0.4957473874092102
train gradient:  0.1321762780168903
iteration : 6165
train acc:  0.6640625
train loss:  0.5774440765380859
train gradient:  0.22792308993187768
iteration : 6166
train acc:  0.7734375
train loss:  0.4857668876647949
train gradient:  0.14725967680982122
iteration : 6167
train acc:  0.71875
train loss:  0.5120589733123779
train gradient:  0.15385052989433134
iteration : 6168
train acc:  0.6796875
train loss:  0.5819322466850281
train gradient:  0.15644239242057661
iteration : 6169
train acc:  0.703125
train loss:  0.5002493262290955
train gradient:  0.13280153145814882
iteration : 6170
train acc:  0.7109375
train loss:  0.507183313369751
train gradient:  0.18506901189590547
iteration : 6171
train acc:  0.7421875
train loss:  0.49931538105010986
train gradient:  0.14038846501553548
iteration : 6172
train acc:  0.78125
train loss:  0.467620313167572
train gradient:  0.11951579156393911
iteration : 6173
train acc:  0.78125
train loss:  0.4488329589366913
train gradient:  0.11614801472116483
iteration : 6174
train acc:  0.7421875
train loss:  0.5808495283126831
train gradient:  0.2052596079339729
iteration : 6175
train acc:  0.671875
train loss:  0.5459638833999634
train gradient:  0.17988376404088463
iteration : 6176
train acc:  0.75
train loss:  0.46995416283607483
train gradient:  0.15517545353458728
iteration : 6177
train acc:  0.7578125
train loss:  0.4872198700904846
train gradient:  0.12178741229193574
iteration : 6178
train acc:  0.6953125
train loss:  0.5075104236602783
train gradient:  0.14153531306795883
iteration : 6179
train acc:  0.6796875
train loss:  0.5044327974319458
train gradient:  0.14657358895122266
iteration : 6180
train acc:  0.7109375
train loss:  0.5050959587097168
train gradient:  0.12362703750300395
iteration : 6181
train acc:  0.71875
train loss:  0.5150731801986694
train gradient:  0.14017638951866707
iteration : 6182
train acc:  0.7734375
train loss:  0.47573745250701904
train gradient:  0.16469921156828107
iteration : 6183
train acc:  0.7421875
train loss:  0.4825060963630676
train gradient:  0.18936052785697943
iteration : 6184
train acc:  0.734375
train loss:  0.5028429627418518
train gradient:  0.13998040920012436
iteration : 6185
train acc:  0.7109375
train loss:  0.5304437875747681
train gradient:  0.17945660629544213
iteration : 6186
train acc:  0.765625
train loss:  0.46077990531921387
train gradient:  0.17193639768922175
iteration : 6187
train acc:  0.7578125
train loss:  0.4761117398738861
train gradient:  0.15579242467279153
iteration : 6188
train acc:  0.71875
train loss:  0.5164229869842529
train gradient:  0.1504100748005021
iteration : 6189
train acc:  0.6875
train loss:  0.5604651570320129
train gradient:  0.155957746519633
iteration : 6190
train acc:  0.8046875
train loss:  0.4401935040950775
train gradient:  0.1177207756041088
iteration : 6191
train acc:  0.7578125
train loss:  0.4605986475944519
train gradient:  0.1041028150390912
iteration : 6192
train acc:  0.7109375
train loss:  0.49375802278518677
train gradient:  0.13385432659128443
iteration : 6193
train acc:  0.7265625
train loss:  0.49315518140792847
train gradient:  0.1285568459765209
iteration : 6194
train acc:  0.796875
train loss:  0.49228304624557495
train gradient:  0.1678857590817927
iteration : 6195
train acc:  0.7421875
train loss:  0.5023118257522583
train gradient:  0.12846620977356843
iteration : 6196
train acc:  0.828125
train loss:  0.4547232389450073
train gradient:  0.14920267338566784
iteration : 6197
train acc:  0.6796875
train loss:  0.5438679456710815
train gradient:  0.1628877845366612
iteration : 6198
train acc:  0.7421875
train loss:  0.5064758062362671
train gradient:  0.1493426091529253
iteration : 6199
train acc:  0.7421875
train loss:  0.5221042633056641
train gradient:  0.17563164686173816
iteration : 6200
train acc:  0.6796875
train loss:  0.5731140971183777
train gradient:  0.1871692062554237
iteration : 6201
train acc:  0.6875
train loss:  0.5024201273918152
train gradient:  0.17638866451916427
iteration : 6202
train acc:  0.796875
train loss:  0.45280420780181885
train gradient:  0.1327613997185518
iteration : 6203
train acc:  0.7890625
train loss:  0.47418102622032166
train gradient:  0.11631105595346701
iteration : 6204
train acc:  0.78125
train loss:  0.458435982465744
train gradient:  0.14373435238451784
iteration : 6205
train acc:  0.7265625
train loss:  0.49899065494537354
train gradient:  0.12915726051605056
iteration : 6206
train acc:  0.7265625
train loss:  0.47464048862457275
train gradient:  0.1512101754961695
iteration : 6207
train acc:  0.875
train loss:  0.38493937253952026
train gradient:  0.09250212420155837
iteration : 6208
train acc:  0.7890625
train loss:  0.46418729424476624
train gradient:  0.11782670560953162
iteration : 6209
train acc:  0.71875
train loss:  0.5426359176635742
train gradient:  0.16060985164588576
iteration : 6210
train acc:  0.734375
train loss:  0.4481486678123474
train gradient:  0.11849944082851487
iteration : 6211
train acc:  0.7734375
train loss:  0.530208170413971
train gradient:  0.2012409107921533
iteration : 6212
train acc:  0.7421875
train loss:  0.5431928634643555
train gradient:  0.17339602859983005
iteration : 6213
train acc:  0.734375
train loss:  0.5027880668640137
train gradient:  0.16476046278567097
iteration : 6214
train acc:  0.75
train loss:  0.5591200590133667
train gradient:  0.240067834396687
iteration : 6215
train acc:  0.703125
train loss:  0.5353506803512573
train gradient:  0.203135449069624
iteration : 6216
train acc:  0.6953125
train loss:  0.6035853624343872
train gradient:  0.17933195314680517
iteration : 6217
train acc:  0.7890625
train loss:  0.446723073720932
train gradient:  0.1451726173254863
iteration : 6218
train acc:  0.765625
train loss:  0.5067812204360962
train gradient:  0.16750343712692503
iteration : 6219
train acc:  0.7734375
train loss:  0.4865720570087433
train gradient:  0.13862782281671993
iteration : 6220
train acc:  0.6484375
train loss:  0.6376737356185913
train gradient:  0.27213474049487246
iteration : 6221
train acc:  0.703125
train loss:  0.5069741010665894
train gradient:  0.14332774048483615
iteration : 6222
train acc:  0.7890625
train loss:  0.4695574641227722
train gradient:  0.12970675942554916
iteration : 6223
train acc:  0.765625
train loss:  0.4688355326652527
train gradient:  0.1393818038572014
iteration : 6224
train acc:  0.7109375
train loss:  0.5076265931129456
train gradient:  0.13854553790703
iteration : 6225
train acc:  0.765625
train loss:  0.4624093770980835
train gradient:  0.1082136300886984
iteration : 6226
train acc:  0.71875
train loss:  0.5605730414390564
train gradient:  0.21296692989602412
iteration : 6227
train acc:  0.7734375
train loss:  0.4672892391681671
train gradient:  0.13537497031548068
iteration : 6228
train acc:  0.75
train loss:  0.46052882075309753
train gradient:  0.10679837741656091
iteration : 6229
train acc:  0.703125
train loss:  0.517402708530426
train gradient:  0.17538433368939116
iteration : 6230
train acc:  0.6796875
train loss:  0.600581705570221
train gradient:  0.1742283286924413
iteration : 6231
train acc:  0.75
train loss:  0.5484122037887573
train gradient:  0.1544362388803735
iteration : 6232
train acc:  0.71875
train loss:  0.5107076168060303
train gradient:  0.14241174896994463
iteration : 6233
train acc:  0.71875
train loss:  0.5445525646209717
train gradient:  0.1443825803571763
iteration : 6234
train acc:  0.734375
train loss:  0.49659818410873413
train gradient:  0.15439025169517379
iteration : 6235
train acc:  0.7734375
train loss:  0.4541712999343872
train gradient:  0.14222612859982942
iteration : 6236
train acc:  0.78125
train loss:  0.4981898069381714
train gradient:  0.14767302424212073
iteration : 6237
train acc:  0.671875
train loss:  0.5606405735015869
train gradient:  0.17037820588240898
iteration : 6238
train acc:  0.734375
train loss:  0.5088417530059814
train gradient:  0.17515322553681092
iteration : 6239
train acc:  0.6875
train loss:  0.5685295462608337
train gradient:  0.1571012066930927
iteration : 6240
train acc:  0.7265625
train loss:  0.5522791147232056
train gradient:  0.1658669869097183
iteration : 6241
train acc:  0.7265625
train loss:  0.44963380694389343
train gradient:  0.11305720390458718
iteration : 6242
train acc:  0.7734375
train loss:  0.46275994181632996
train gradient:  0.11141169867852599
iteration : 6243
train acc:  0.734375
train loss:  0.505190372467041
train gradient:  0.17309797076963918
iteration : 6244
train acc:  0.6953125
train loss:  0.5532565116882324
train gradient:  0.15892993535437366
iteration : 6245
train acc:  0.7265625
train loss:  0.5139564275741577
train gradient:  0.17392287378726276
iteration : 6246
train acc:  0.7734375
train loss:  0.4445877969264984
train gradient:  0.12328691827204152
iteration : 6247
train acc:  0.734375
train loss:  0.5459132194519043
train gradient:  0.17858159999747056
iteration : 6248
train acc:  0.7265625
train loss:  0.5280036330223083
train gradient:  0.17174315755521538
iteration : 6249
train acc:  0.703125
train loss:  0.5449748039245605
train gradient:  0.20643806175837415
iteration : 6250
train acc:  0.765625
train loss:  0.4671577513217926
train gradient:  0.11244900843869965
iteration : 6251
train acc:  0.6875
train loss:  0.5948395729064941
train gradient:  0.20303902587136174
iteration : 6252
train acc:  0.75
train loss:  0.4720397889614105
train gradient:  0.11045775413047265
iteration : 6253
train acc:  0.6875
train loss:  0.5439133048057556
train gradient:  0.1473248661565272
iteration : 6254
train acc:  0.78125
train loss:  0.47053444385528564
train gradient:  0.15031723685381476
iteration : 6255
train acc:  0.7578125
train loss:  0.48284369707107544
train gradient:  0.1277200686845057
iteration : 6256
train acc:  0.7265625
train loss:  0.5130027532577515
train gradient:  0.14574329607602876
iteration : 6257
train acc:  0.75
train loss:  0.501804530620575
train gradient:  0.14239664848792793
iteration : 6258
train acc:  0.7578125
train loss:  0.4738646149635315
train gradient:  0.12286163346785613
iteration : 6259
train acc:  0.75
train loss:  0.47533726692199707
train gradient:  0.13332069080611703
iteration : 6260
train acc:  0.7890625
train loss:  0.4606770873069763
train gradient:  0.12526350155433655
iteration : 6261
train acc:  0.765625
train loss:  0.49205273389816284
train gradient:  0.12336054430371844
iteration : 6262
train acc:  0.765625
train loss:  0.5151682496070862
train gradient:  0.13468297905343257
iteration : 6263
train acc:  0.703125
train loss:  0.5586336255073547
train gradient:  0.18413225910379022
iteration : 6264
train acc:  0.765625
train loss:  0.5015335083007812
train gradient:  0.14770597920854148
iteration : 6265
train acc:  0.7109375
train loss:  0.5055832862854004
train gradient:  0.18686917245068976
iteration : 6266
train acc:  0.7109375
train loss:  0.5280554294586182
train gradient:  0.12499332589863689
iteration : 6267
train acc:  0.734375
train loss:  0.5495021343231201
train gradient:  0.14268529269007155
iteration : 6268
train acc:  0.671875
train loss:  0.545920193195343
train gradient:  0.21630558093683605
iteration : 6269
train acc:  0.6875
train loss:  0.5115454196929932
train gradient:  0.17287489020627134
iteration : 6270
train acc:  0.7421875
train loss:  0.5217667818069458
train gradient:  0.14627577613854353
iteration : 6271
train acc:  0.7890625
train loss:  0.4579099416732788
train gradient:  0.12160022529497713
iteration : 6272
train acc:  0.703125
train loss:  0.5286499261856079
train gradient:  0.16535966288137022
iteration : 6273
train acc:  0.734375
train loss:  0.5353857278823853
train gradient:  0.16683863177495029
iteration : 6274
train acc:  0.7890625
train loss:  0.45010504126548767
train gradient:  0.09924463337813685
iteration : 6275
train acc:  0.703125
train loss:  0.5592333674430847
train gradient:  0.16664115276694014
iteration : 6276
train acc:  0.7578125
train loss:  0.4667545557022095
train gradient:  0.11427918252268401
iteration : 6277
train acc:  0.71875
train loss:  0.5643540620803833
train gradient:  0.15669833771829725
iteration : 6278
train acc:  0.6875
train loss:  0.5540183782577515
train gradient:  0.214751464733803
iteration : 6279
train acc:  0.7578125
train loss:  0.5130559206008911
train gradient:  0.15749010955195927
iteration : 6280
train acc:  0.7109375
train loss:  0.5241162180900574
train gradient:  0.14383115278929653
iteration : 6281
train acc:  0.703125
train loss:  0.49567732214927673
train gradient:  0.14209606521251195
iteration : 6282
train acc:  0.796875
train loss:  0.4342399835586548
train gradient:  0.10504365193910775
iteration : 6283
train acc:  0.7578125
train loss:  0.444698691368103
train gradient:  0.14566896944459012
iteration : 6284
train acc:  0.7265625
train loss:  0.505662202835083
train gradient:  0.1393239623948014
iteration : 6285
train acc:  0.7578125
train loss:  0.5273389220237732
train gradient:  0.1394888464418794
iteration : 6286
train acc:  0.7578125
train loss:  0.4834821820259094
train gradient:  0.1288055570832457
iteration : 6287
train acc:  0.7890625
train loss:  0.4419553279876709
train gradient:  0.09751614233431004
iteration : 6288
train acc:  0.6953125
train loss:  0.5151228308677673
train gradient:  0.1576302830306075
iteration : 6289
train acc:  0.7109375
train loss:  0.5174310207366943
train gradient:  0.1642423265649942
iteration : 6290
train acc:  0.75
train loss:  0.4722328782081604
train gradient:  0.137336431938824
iteration : 6291
train acc:  0.734375
train loss:  0.5562301874160767
train gradient:  0.17855196232726755
iteration : 6292
train acc:  0.765625
train loss:  0.5101498961448669
train gradient:  0.12889745573609052
iteration : 6293
train acc:  0.640625
train loss:  0.57774418592453
train gradient:  0.18176745166944916
iteration : 6294
train acc:  0.75
train loss:  0.49537527561187744
train gradient:  0.1453596993030824
iteration : 6295
train acc:  0.671875
train loss:  0.5814616680145264
train gradient:  0.17949034256005353
iteration : 6296
train acc:  0.734375
train loss:  0.5168730020523071
train gradient:  0.13426257314098594
iteration : 6297
train acc:  0.734375
train loss:  0.5016292929649353
train gradient:  0.1680122060341741
iteration : 6298
train acc:  0.6953125
train loss:  0.5826197862625122
train gradient:  0.14422544465455406
iteration : 6299
train acc:  0.6796875
train loss:  0.5486599206924438
train gradient:  0.14938253947891592
iteration : 6300
train acc:  0.71875
train loss:  0.5298108458518982
train gradient:  0.16302282234190257
iteration : 6301
train acc:  0.7421875
train loss:  0.539313554763794
train gradient:  0.1397455148881496
iteration : 6302
train acc:  0.703125
train loss:  0.5183249711990356
train gradient:  0.14297054357180916
iteration : 6303
train acc:  0.75
train loss:  0.527631938457489
train gradient:  0.1628570124609207
iteration : 6304
train acc:  0.78125
train loss:  0.49242734909057617
train gradient:  0.14481477052938713
iteration : 6305
train acc:  0.75
train loss:  0.5134653449058533
train gradient:  0.14655146243713052
iteration : 6306
train acc:  0.6875
train loss:  0.5077043771743774
train gradient:  0.13243942165496247
iteration : 6307
train acc:  0.7109375
train loss:  0.5278949737548828
train gradient:  0.15499157031177174
iteration : 6308
train acc:  0.7265625
train loss:  0.5169354677200317
train gradient:  0.1481812228470326
iteration : 6309
train acc:  0.6640625
train loss:  0.596848726272583
train gradient:  0.19044633665375083
iteration : 6310
train acc:  0.7265625
train loss:  0.5092499852180481
train gradient:  0.15244849119953696
iteration : 6311
train acc:  0.7109375
train loss:  0.5362487435340881
train gradient:  0.15721047402373056
iteration : 6312
train acc:  0.7421875
train loss:  0.5492744445800781
train gradient:  0.1852819291527949
iteration : 6313
train acc:  0.7890625
train loss:  0.4289087653160095
train gradient:  0.10325264922117863
iteration : 6314
train acc:  0.7109375
train loss:  0.5647949576377869
train gradient:  0.20392285549600914
iteration : 6315
train acc:  0.6953125
train loss:  0.48875418305397034
train gradient:  0.1135097777018181
iteration : 6316
train acc:  0.71875
train loss:  0.5596885681152344
train gradient:  0.16602164779328277
iteration : 6317
train acc:  0.7109375
train loss:  0.5297032594680786
train gradient:  0.1989357272076689
iteration : 6318
train acc:  0.765625
train loss:  0.48863065242767334
train gradient:  0.12900102480705544
iteration : 6319
train acc:  0.7578125
train loss:  0.5316404104232788
train gradient:  0.15036365914341793
iteration : 6320
train acc:  0.7734375
train loss:  0.4628118574619293
train gradient:  0.1408127296094725
iteration : 6321
train acc:  0.6796875
train loss:  0.5561952590942383
train gradient:  0.17302430830938015
iteration : 6322
train acc:  0.8125
train loss:  0.44671157002449036
train gradient:  0.11515000745083179
iteration : 6323
train acc:  0.7109375
train loss:  0.5290593504905701
train gradient:  0.14109143056056378
iteration : 6324
train acc:  0.8046875
train loss:  0.44256019592285156
train gradient:  0.1316741512828178
iteration : 6325
train acc:  0.765625
train loss:  0.473802387714386
train gradient:  0.14176012424650963
iteration : 6326
train acc:  0.71875
train loss:  0.48092156648635864
train gradient:  0.14427812887651711
iteration : 6327
train acc:  0.765625
train loss:  0.48828449845314026
train gradient:  0.12060966359557952
iteration : 6328
train acc:  0.7578125
train loss:  0.48554572463035583
train gradient:  0.14308775921342845
iteration : 6329
train acc:  0.7734375
train loss:  0.46654459834098816
train gradient:  0.12901355257234212
iteration : 6330
train acc:  0.7265625
train loss:  0.4797789454460144
train gradient:  0.10319608998710786
iteration : 6331
train acc:  0.8046875
train loss:  0.4503878951072693
train gradient:  0.11435449778809391
iteration : 6332
train acc:  0.78125
train loss:  0.45218420028686523
train gradient:  0.11716719848348763
iteration : 6333
train acc:  0.8046875
train loss:  0.4378765821456909
train gradient:  0.11483615832530882
iteration : 6334
train acc:  0.7109375
train loss:  0.5583735704421997
train gradient:  0.1575805262626098
iteration : 6335
train acc:  0.703125
train loss:  0.5543960332870483
train gradient:  0.18279505512281874
iteration : 6336
train acc:  0.7421875
train loss:  0.49889305233955383
train gradient:  0.180747256503578
iteration : 6337
train acc:  0.765625
train loss:  0.4698920249938965
train gradient:  0.1283542989977342
iteration : 6338
train acc:  0.765625
train loss:  0.48651015758514404
train gradient:  0.14096291240606904
iteration : 6339
train acc:  0.6875
train loss:  0.5607099533081055
train gradient:  0.17059771797304
iteration : 6340
train acc:  0.78125
train loss:  0.4552686810493469
train gradient:  0.11799223370300514
iteration : 6341
train acc:  0.7265625
train loss:  0.487176775932312
train gradient:  0.13215310354088922
iteration : 6342
train acc:  0.75
train loss:  0.5047319531440735
train gradient:  0.1484836457916594
iteration : 6343
train acc:  0.734375
train loss:  0.5204091668128967
train gradient:  0.14926377746884345
iteration : 6344
train acc:  0.796875
train loss:  0.4376007914543152
train gradient:  0.1067720366984033
iteration : 6345
train acc:  0.71875
train loss:  0.502489447593689
train gradient:  0.13143598948689972
iteration : 6346
train acc:  0.7734375
train loss:  0.48723864555358887
train gradient:  0.1317416845629793
iteration : 6347
train acc:  0.7265625
train loss:  0.5272257328033447
train gradient:  0.1579052463110593
iteration : 6348
train acc:  0.6953125
train loss:  0.6056879758834839
train gradient:  0.26797882244094096
iteration : 6349
train acc:  0.7734375
train loss:  0.4334656298160553
train gradient:  0.16587656292573505
iteration : 6350
train acc:  0.734375
train loss:  0.46904563903808594
train gradient:  0.12499111750492911
iteration : 6351
train acc:  0.7578125
train loss:  0.46238845586776733
train gradient:  0.11579761946931404
iteration : 6352
train acc:  0.7734375
train loss:  0.4737224578857422
train gradient:  0.14854476216688486
iteration : 6353
train acc:  0.6640625
train loss:  0.5664031505584717
train gradient:  0.20917333416331382
iteration : 6354
train acc:  0.71875
train loss:  0.514840841293335
train gradient:  0.15072776759070983
iteration : 6355
train acc:  0.7265625
train loss:  0.5116279125213623
train gradient:  0.14238247497079498
iteration : 6356
train acc:  0.7265625
train loss:  0.5369688272476196
train gradient:  0.177154694427395
iteration : 6357
train acc:  0.7109375
train loss:  0.56333327293396
train gradient:  0.17294776237070114
iteration : 6358
train acc:  0.6953125
train loss:  0.6071210503578186
train gradient:  0.18596033515984112
iteration : 6359
train acc:  0.7734375
train loss:  0.45715954899787903
train gradient:  0.1476651678719061
iteration : 6360
train acc:  0.75
train loss:  0.4947732090950012
train gradient:  0.2152943281921989
iteration : 6361
train acc:  0.71875
train loss:  0.518926203250885
train gradient:  0.15449259629705198
iteration : 6362
train acc:  0.75
train loss:  0.4962618350982666
train gradient:  0.13332121555343734
iteration : 6363
train acc:  0.6796875
train loss:  0.5692266225814819
train gradient:  0.16085702377122357
iteration : 6364
train acc:  0.7578125
train loss:  0.525923490524292
train gradient:  0.15284183655531794
iteration : 6365
train acc:  0.7421875
train loss:  0.5162777900695801
train gradient:  0.1521658047260036
iteration : 6366
train acc:  0.6953125
train loss:  0.5208108425140381
train gradient:  0.12343503965333188
iteration : 6367
train acc:  0.71875
train loss:  0.5666581392288208
train gradient:  0.14362199995790065
iteration : 6368
train acc:  0.7734375
train loss:  0.4474869966506958
train gradient:  0.11725049835163397
iteration : 6369
train acc:  0.765625
train loss:  0.5207124948501587
train gradient:  0.13564518743344095
iteration : 6370
train acc:  0.7734375
train loss:  0.48587971925735474
train gradient:  0.14091826487666254
iteration : 6371
train acc:  0.78125
train loss:  0.42951273918151855
train gradient:  0.12682212107909824
iteration : 6372
train acc:  0.7265625
train loss:  0.5193352103233337
train gradient:  0.14363549797547037
iteration : 6373
train acc:  0.6484375
train loss:  0.6023609638214111
train gradient:  0.21746056700406602
iteration : 6374
train acc:  0.734375
train loss:  0.518531858921051
train gradient:  0.15713566437282778
iteration : 6375
train acc:  0.703125
train loss:  0.5437420606613159
train gradient:  0.1606504446489705
iteration : 6376
train acc:  0.7109375
train loss:  0.5714577436447144
train gradient:  0.17486817243357763
iteration : 6377
train acc:  0.71875
train loss:  0.48166972398757935
train gradient:  0.12804850513663885
iteration : 6378
train acc:  0.6953125
train loss:  0.5871344804763794
train gradient:  0.2060760812297106
iteration : 6379
train acc:  0.8125
train loss:  0.4063491225242615
train gradient:  0.15113948267017052
iteration : 6380
train acc:  0.703125
train loss:  0.5776996612548828
train gradient:  0.16051234493139743
iteration : 6381
train acc:  0.7578125
train loss:  0.48053407669067383
train gradient:  0.12118960448118068
iteration : 6382
train acc:  0.7421875
train loss:  0.5001462697982788
train gradient:  0.11022508991185213
iteration : 6383
train acc:  0.7578125
train loss:  0.48062288761138916
train gradient:  0.12064663566611124
iteration : 6384
train acc:  0.765625
train loss:  0.5815186500549316
train gradient:  0.2525636863468959
iteration : 6385
train acc:  0.75
train loss:  0.47529110312461853
train gradient:  0.11105687650410012
iteration : 6386
train acc:  0.765625
train loss:  0.43937093019485474
train gradient:  0.13556906031047888
iteration : 6387
train acc:  0.71875
train loss:  0.47971582412719727
train gradient:  0.14051477607965468
iteration : 6388
train acc:  0.75
train loss:  0.5341204404830933
train gradient:  0.16278467509554503
iteration : 6389
train acc:  0.7578125
train loss:  0.4575052857398987
train gradient:  0.12568243753873964
iteration : 6390
train acc:  0.703125
train loss:  0.5315819978713989
train gradient:  0.16291956505510166
iteration : 6391
train acc:  0.765625
train loss:  0.4956464171409607
train gradient:  0.14787859105830348
iteration : 6392
train acc:  0.7578125
train loss:  0.46490129828453064
train gradient:  0.10998874150827441
iteration : 6393
train acc:  0.8125
train loss:  0.42731353640556335
train gradient:  0.12208334690727572
iteration : 6394
train acc:  0.734375
train loss:  0.4980086088180542
train gradient:  0.205026213836218
iteration : 6395
train acc:  0.7890625
train loss:  0.44812270998954773
train gradient:  0.14202064035362583
iteration : 6396
train acc:  0.75
train loss:  0.47350263595581055
train gradient:  0.11898156779546652
iteration : 6397
train acc:  0.6875
train loss:  0.5651357173919678
train gradient:  0.19969473356648187
iteration : 6398
train acc:  0.7109375
train loss:  0.5463336110115051
train gradient:  0.14884048209701845
iteration : 6399
train acc:  0.7421875
train loss:  0.5402572751045227
train gradient:  0.18009704799560255
iteration : 6400
train acc:  0.8046875
train loss:  0.4639242887496948
train gradient:  0.10794373785154952
iteration : 6401
train acc:  0.71875
train loss:  0.503548264503479
train gradient:  0.1417379335390313
iteration : 6402
train acc:  0.734375
train loss:  0.5217103958129883
train gradient:  0.12828123941688538
iteration : 6403
train acc:  0.6640625
train loss:  0.6194670796394348
train gradient:  0.16987202913570074
iteration : 6404
train acc:  0.6875
train loss:  0.5229800939559937
train gradient:  0.14446239840210323
iteration : 6405
train acc:  0.734375
train loss:  0.5135226845741272
train gradient:  0.13916096794050062
iteration : 6406
train acc:  0.671875
train loss:  0.544160008430481
train gradient:  0.15803208426702006
iteration : 6407
train acc:  0.6953125
train loss:  0.5244051218032837
train gradient:  0.14883686205951246
iteration : 6408
train acc:  0.703125
train loss:  0.5034239292144775
train gradient:  0.1248099181629201
iteration : 6409
train acc:  0.7421875
train loss:  0.48400232195854187
train gradient:  0.15534529556562193
iteration : 6410
train acc:  0.7734375
train loss:  0.5139836072921753
train gradient:  0.1613456388493956
iteration : 6411
train acc:  0.734375
train loss:  0.5210666060447693
train gradient:  0.1479305989367316
iteration : 6412
train acc:  0.703125
train loss:  0.5107182264328003
train gradient:  0.13618899755087321
iteration : 6413
train acc:  0.7578125
train loss:  0.4807490110397339
train gradient:  0.1286872917593685
iteration : 6414
train acc:  0.75
train loss:  0.5412832498550415
train gradient:  0.17677021176642055
iteration : 6415
train acc:  0.78125
train loss:  0.4566442370414734
train gradient:  0.1518018592399664
iteration : 6416
train acc:  0.7421875
train loss:  0.4542286992073059
train gradient:  0.11138744792374093
iteration : 6417
train acc:  0.6953125
train loss:  0.5145834684371948
train gradient:  0.17446456033796154
iteration : 6418
train acc:  0.7421875
train loss:  0.5470337867736816
train gradient:  0.1741114922940833
iteration : 6419
train acc:  0.7421875
train loss:  0.4920581579208374
train gradient:  0.1603339319896249
iteration : 6420
train acc:  0.7734375
train loss:  0.4435592591762543
train gradient:  0.10806264735739832
iteration : 6421
train acc:  0.765625
train loss:  0.5129486322402954
train gradient:  0.15543934224885625
iteration : 6422
train acc:  0.8203125
train loss:  0.46954792737960815
train gradient:  0.10604797734538274
iteration : 6423
train acc:  0.7421875
train loss:  0.44914495944976807
train gradient:  0.13081121037547927
iteration : 6424
train acc:  0.640625
train loss:  0.5572888255119324
train gradient:  0.215209529541536
iteration : 6425
train acc:  0.765625
train loss:  0.46183252334594727
train gradient:  0.13954108555307226
iteration : 6426
train acc:  0.7265625
train loss:  0.518834114074707
train gradient:  0.1475201447821491
iteration : 6427
train acc:  0.71875
train loss:  0.5228254795074463
train gradient:  0.15920218485310786
iteration : 6428
train acc:  0.7890625
train loss:  0.5123435258865356
train gradient:  0.15406767361917323
iteration : 6429
train acc:  0.7734375
train loss:  0.45267611742019653
train gradient:  0.11355110216147696
iteration : 6430
train acc:  0.78125
train loss:  0.416830837726593
train gradient:  0.1494401721853842
iteration : 6431
train acc:  0.7265625
train loss:  0.4998018145561218
train gradient:  0.127162740820754
iteration : 6432
train acc:  0.6953125
train loss:  0.5090605020523071
train gradient:  0.17795041254572486
iteration : 6433
train acc:  0.7734375
train loss:  0.4530433416366577
train gradient:  0.13474372792136732
iteration : 6434
train acc:  0.6796875
train loss:  0.5391759276390076
train gradient:  0.12490203039608912
iteration : 6435
train acc:  0.7734375
train loss:  0.5179499387741089
train gradient:  0.15686357720554733
iteration : 6436
train acc:  0.6796875
train loss:  0.5177589654922485
train gradient:  0.19103808434326175
iteration : 6437
train acc:  0.71875
train loss:  0.479764461517334
train gradient:  0.14199720742450433
iteration : 6438
train acc:  0.7421875
train loss:  0.5313180685043335
train gradient:  0.1782106270763651
iteration : 6439
train acc:  0.75
train loss:  0.5380860567092896
train gradient:  0.19076014112065415
iteration : 6440
train acc:  0.78125
train loss:  0.4840748906135559
train gradient:  0.1367641318391667
iteration : 6441
train acc:  0.6953125
train loss:  0.532785177230835
train gradient:  0.15189969945452592
iteration : 6442
train acc:  0.71875
train loss:  0.5241570472717285
train gradient:  0.12754942740554093
iteration : 6443
train acc:  0.7578125
train loss:  0.5006564855575562
train gradient:  0.1287642448271157
iteration : 6444
train acc:  0.734375
train loss:  0.4873862564563751
train gradient:  0.1419468015973317
iteration : 6445
train acc:  0.703125
train loss:  0.5364384651184082
train gradient:  0.2169666795022016
iteration : 6446
train acc:  0.8046875
train loss:  0.42707785964012146
train gradient:  0.09994818380219421
iteration : 6447
train acc:  0.6953125
train loss:  0.5563850402832031
train gradient:  0.14834091293463408
iteration : 6448
train acc:  0.7890625
train loss:  0.4607061743736267
train gradient:  0.1483992399303899
iteration : 6449
train acc:  0.703125
train loss:  0.5435804128646851
train gradient:  0.1727519342422795
iteration : 6450
train acc:  0.7265625
train loss:  0.4824029207229614
train gradient:  0.11112579936542075
iteration : 6451
train acc:  0.6875
train loss:  0.5337375402450562
train gradient:  0.17672714503265113
iteration : 6452
train acc:  0.8203125
train loss:  0.4080723226070404
train gradient:  0.15115829027268496
iteration : 6453
train acc:  0.78125
train loss:  0.4671913981437683
train gradient:  0.13540668898964764
iteration : 6454
train acc:  0.78125
train loss:  0.4247162938117981
train gradient:  0.1468263861587496
iteration : 6455
train acc:  0.734375
train loss:  0.5091482400894165
train gradient:  0.15085918056694064
iteration : 6456
train acc:  0.6484375
train loss:  0.590552568435669
train gradient:  0.252008499943607
iteration : 6457
train acc:  0.7109375
train loss:  0.48533403873443604
train gradient:  0.14278783208973467
iteration : 6458
train acc:  0.6953125
train loss:  0.5262460708618164
train gradient:  0.18613198190665609
iteration : 6459
train acc:  0.75
train loss:  0.5030867457389832
train gradient:  0.16529547834336095
iteration : 6460
train acc:  0.71875
train loss:  0.5096309781074524
train gradient:  0.13510614880277538
iteration : 6461
train acc:  0.796875
train loss:  0.47045502066612244
train gradient:  0.12666372868003561
iteration : 6462
train acc:  0.7578125
train loss:  0.49841010570526123
train gradient:  0.18386318485152947
iteration : 6463
train acc:  0.796875
train loss:  0.4748079180717468
train gradient:  0.12392445834960845
iteration : 6464
train acc:  0.7421875
train loss:  0.483039915561676
train gradient:  0.11270253632360742
iteration : 6465
train acc:  0.7265625
train loss:  0.4991753101348877
train gradient:  0.18809819027351304
iteration : 6466
train acc:  0.734375
train loss:  0.499016135931015
train gradient:  0.1373383601470859
iteration : 6467
train acc:  0.734375
train loss:  0.5105874538421631
train gradient:  0.1514565806852743
iteration : 6468
train acc:  0.75
train loss:  0.5125035047531128
train gradient:  0.1503606456664468
iteration : 6469
train acc:  0.6796875
train loss:  0.6078720092773438
train gradient:  0.2089478388204447
iteration : 6470
train acc:  0.734375
train loss:  0.4827800393104553
train gradient:  0.12768634777100699
iteration : 6471
train acc:  0.7421875
train loss:  0.4970848262310028
train gradient:  0.14485769288455758
iteration : 6472
train acc:  0.7265625
train loss:  0.5181944966316223
train gradient:  0.1287240841369252
iteration : 6473
train acc:  0.7265625
train loss:  0.505096435546875
train gradient:  0.13829201339136515
iteration : 6474
train acc:  0.765625
train loss:  0.4732043743133545
train gradient:  0.12124434502387049
iteration : 6475
train acc:  0.78125
train loss:  0.49606287479400635
train gradient:  0.17950565534824986
iteration : 6476
train acc:  0.7890625
train loss:  0.49635446071624756
train gradient:  0.14661044709039778
iteration : 6477
train acc:  0.7421875
train loss:  0.5041656494140625
train gradient:  0.14966875438112076
iteration : 6478
train acc:  0.71875
train loss:  0.5899131298065186
train gradient:  0.16827579757228253
iteration : 6479
train acc:  0.78125
train loss:  0.4505109190940857
train gradient:  0.12144567159597197
iteration : 6480
train acc:  0.78125
train loss:  0.4654654264450073
train gradient:  0.13256327188537123
iteration : 6481
train acc:  0.7109375
train loss:  0.49195486307144165
train gradient:  0.15574106113985345
iteration : 6482
train acc:  0.6796875
train loss:  0.5745421051979065
train gradient:  0.15380690368332164
iteration : 6483
train acc:  0.703125
train loss:  0.5393773317337036
train gradient:  0.14909326673630846
iteration : 6484
train acc:  0.7890625
train loss:  0.45178788900375366
train gradient:  0.11227459145293484
iteration : 6485
train acc:  0.7265625
train loss:  0.5170868635177612
train gradient:  0.18601471898382033
iteration : 6486
train acc:  0.671875
train loss:  0.5343271493911743
train gradient:  0.1575277046796904
iteration : 6487
train acc:  0.7734375
train loss:  0.43070554733276367
train gradient:  0.11351080099677487
iteration : 6488
train acc:  0.796875
train loss:  0.43255192041397095
train gradient:  0.12304025747417022
iteration : 6489
train acc:  0.65625
train loss:  0.5620529651641846
train gradient:  0.189278771285392
iteration : 6490
train acc:  0.7109375
train loss:  0.5541712641716003
train gradient:  0.14141991710021073
iteration : 6491
train acc:  0.75
train loss:  0.5068570971488953
train gradient:  0.14207467557756037
iteration : 6492
train acc:  0.7265625
train loss:  0.5072277188301086
train gradient:  0.14434403591579553
iteration : 6493
train acc:  0.671875
train loss:  0.5714786052703857
train gradient:  0.14110200731243683
iteration : 6494
train acc:  0.71875
train loss:  0.5441404581069946
train gradient:  0.16963360333336097
iteration : 6495
train acc:  0.765625
train loss:  0.45299670100212097
train gradient:  0.14116808907256528
iteration : 6496
train acc:  0.734375
train loss:  0.48822352290153503
train gradient:  0.17255687164173833
iteration : 6497
train acc:  0.734375
train loss:  0.517848551273346
train gradient:  0.12686805565077533
iteration : 6498
train acc:  0.796875
train loss:  0.4466586112976074
train gradient:  0.1457590488322471
iteration : 6499
train acc:  0.7265625
train loss:  0.5468778610229492
train gradient:  0.17582744712046056
iteration : 6500
train acc:  0.765625
train loss:  0.48626142740249634
train gradient:  0.13313512552068812
iteration : 6501
train acc:  0.828125
train loss:  0.43130388855934143
train gradient:  0.1280173761286397
iteration : 6502
train acc:  0.7421875
train loss:  0.49367302656173706
train gradient:  0.19142986562939623
iteration : 6503
train acc:  0.703125
train loss:  0.5535931587219238
train gradient:  0.22945522084461661
iteration : 6504
train acc:  0.7265625
train loss:  0.5763518810272217
train gradient:  0.17636148997757406
iteration : 6505
train acc:  0.703125
train loss:  0.5399172306060791
train gradient:  0.14193830703207339
iteration : 6506
train acc:  0.75
train loss:  0.4424936771392822
train gradient:  0.12462589526108364
iteration : 6507
train acc:  0.765625
train loss:  0.4629009962081909
train gradient:  0.12704135086037804
iteration : 6508
train acc:  0.734375
train loss:  0.4901886582374573
train gradient:  0.11757703976279084
iteration : 6509
train acc:  0.75
train loss:  0.47594520449638367
train gradient:  0.11514886128092727
iteration : 6510
train acc:  0.8125
train loss:  0.4649140238761902
train gradient:  0.17517693260015516
iteration : 6511
train acc:  0.7265625
train loss:  0.547143816947937
train gradient:  0.15393597440152923
iteration : 6512
train acc:  0.7109375
train loss:  0.5039058327674866
train gradient:  0.16705973665324703
iteration : 6513
train acc:  0.734375
train loss:  0.4735807180404663
train gradient:  0.1413858987660278
iteration : 6514
train acc:  0.6875
train loss:  0.5262901186943054
train gradient:  0.16740557623358704
iteration : 6515
train acc:  0.78125
train loss:  0.48763376474380493
train gradient:  0.15953494303950488
iteration : 6516
train acc:  0.796875
train loss:  0.5043196082115173
train gradient:  0.1530079703751345
iteration : 6517
train acc:  0.71875
train loss:  0.5347130298614502
train gradient:  0.17094396323221916
iteration : 6518
train acc:  0.703125
train loss:  0.5123312473297119
train gradient:  0.12278454504355109
iteration : 6519
train acc:  0.75
train loss:  0.4454934597015381
train gradient:  0.12475420263352462
iteration : 6520
train acc:  0.8046875
train loss:  0.4606926739215851
train gradient:  0.1269983318692135
iteration : 6521
train acc:  0.7734375
train loss:  0.4593782126903534
train gradient:  0.1129372980640322
iteration : 6522
train acc:  0.7734375
train loss:  0.5284277200698853
train gradient:  0.19587310816949516
iteration : 6523
train acc:  0.7265625
train loss:  0.5789129734039307
train gradient:  0.18618229436490774
iteration : 6524
train acc:  0.78125
train loss:  0.4454512596130371
train gradient:  0.1490652802108266
iteration : 6525
train acc:  0.765625
train loss:  0.475131630897522
train gradient:  0.12582361031705674
iteration : 6526
train acc:  0.703125
train loss:  0.526945948600769
train gradient:  0.17400590267394916
iteration : 6527
train acc:  0.7578125
train loss:  0.5253239870071411
train gradient:  0.13652718012893783
iteration : 6528
train acc:  0.7109375
train loss:  0.5944038033485413
train gradient:  0.178449641626448
iteration : 6529
train acc:  0.7109375
train loss:  0.547389805316925
train gradient:  0.15719687902798374
iteration : 6530
train acc:  0.703125
train loss:  0.5572865605354309
train gradient:  0.17471484031053208
iteration : 6531
train acc:  0.734375
train loss:  0.5249595642089844
train gradient:  0.16033162093874553
iteration : 6532
train acc:  0.78125
train loss:  0.44539275765419006
train gradient:  0.11381602223941796
iteration : 6533
train acc:  0.7421875
train loss:  0.5198708772659302
train gradient:  0.13230397889399995
iteration : 6534
train acc:  0.71875
train loss:  0.5138782262802124
train gradient:  0.13451401986661188
iteration : 6535
train acc:  0.75
train loss:  0.5106917023658752
train gradient:  0.15068773499525684
iteration : 6536
train acc:  0.734375
train loss:  0.5682247877120972
train gradient:  0.18730095657357843
iteration : 6537
train acc:  0.8046875
train loss:  0.4279274642467499
train gradient:  0.1253953001825636
iteration : 6538
train acc:  0.7109375
train loss:  0.538763165473938
train gradient:  0.1469248066870639
iteration : 6539
train acc:  0.703125
train loss:  0.5254528522491455
train gradient:  0.14395833381134726
iteration : 6540
train acc:  0.7109375
train loss:  0.5526964664459229
train gradient:  0.15643774207000916
iteration : 6541
train acc:  0.765625
train loss:  0.4753333330154419
train gradient:  0.11161032054528632
iteration : 6542
train acc:  0.734375
train loss:  0.49001914262771606
train gradient:  0.14013566096501204
iteration : 6543
train acc:  0.7265625
train loss:  0.5116779804229736
train gradient:  0.14791456296964497
iteration : 6544
train acc:  0.7421875
train loss:  0.5244714021682739
train gradient:  0.16114975312253993
iteration : 6545
train acc:  0.7734375
train loss:  0.47483330965042114
train gradient:  0.1183914929919522
iteration : 6546
train acc:  0.75
train loss:  0.48218101263046265
train gradient:  0.12490093480286119
iteration : 6547
train acc:  0.7265625
train loss:  0.5326846241950989
train gradient:  0.15981277953688566
iteration : 6548
train acc:  0.71875
train loss:  0.5053914785385132
train gradient:  0.13261262087307324
iteration : 6549
train acc:  0.71875
train loss:  0.536449670791626
train gradient:  0.14559114080592395
iteration : 6550
train acc:  0.7890625
train loss:  0.508664608001709
train gradient:  0.1984869441603536
iteration : 6551
train acc:  0.78125
train loss:  0.46088820695877075
train gradient:  0.1476508922690326
iteration : 6552
train acc:  0.703125
train loss:  0.5534075498580933
train gradient:  0.14043232334964018
iteration : 6553
train acc:  0.8125
train loss:  0.4630863666534424
train gradient:  0.1160206337200204
iteration : 6554
train acc:  0.6875
train loss:  0.548983097076416
train gradient:  0.1465889327598396
iteration : 6555
train acc:  0.734375
train loss:  0.590671181678772
train gradient:  0.19453724955180896
iteration : 6556
train acc:  0.78125
train loss:  0.4613904356956482
train gradient:  0.11200737207059577
iteration : 6557
train acc:  0.71875
train loss:  0.5125445127487183
train gradient:  0.1403977071534665
iteration : 6558
train acc:  0.671875
train loss:  0.6222747564315796
train gradient:  0.22817920361148036
iteration : 6559
train acc:  0.75
train loss:  0.5340265035629272
train gradient:  0.19676204697229077
iteration : 6560
train acc:  0.765625
train loss:  0.5196835994720459
train gradient:  0.15263825796815947
iteration : 6561
train acc:  0.7109375
train loss:  0.5373307466506958
train gradient:  0.12660102828809736
iteration : 6562
train acc:  0.7421875
train loss:  0.5496355295181274
train gradient:  0.15053240576874632
iteration : 6563
train acc:  0.7109375
train loss:  0.5096680521965027
train gradient:  0.12795435357559654
iteration : 6564
train acc:  0.734375
train loss:  0.5350502133369446
train gradient:  0.15215764494933576
iteration : 6565
train acc:  0.765625
train loss:  0.48462846875190735
train gradient:  0.13915408339448526
iteration : 6566
train acc:  0.703125
train loss:  0.589057207107544
train gradient:  0.23692293638770268
iteration : 6567
train acc:  0.7890625
train loss:  0.47668448090553284
train gradient:  0.13536054389486843
iteration : 6568
train acc:  0.7734375
train loss:  0.4854898452758789
train gradient:  0.11987485822282704
iteration : 6569
train acc:  0.7421875
train loss:  0.5073901414871216
train gradient:  0.1273581908781568
iteration : 6570
train acc:  0.71875
train loss:  0.5335730910301208
train gradient:  0.17547646903166064
iteration : 6571
train acc:  0.6796875
train loss:  0.5993779897689819
train gradient:  0.16157205282862974
iteration : 6572
train acc:  0.6484375
train loss:  0.6310595870018005
train gradient:  0.23279743773775846
iteration : 6573
train acc:  0.7265625
train loss:  0.5061403512954712
train gradient:  0.1278060408066447
iteration : 6574
train acc:  0.7578125
train loss:  0.49532926082611084
train gradient:  0.15293412635468281
iteration : 6575
train acc:  0.75
train loss:  0.5106139779090881
train gradient:  0.17328356297415207
iteration : 6576
train acc:  0.734375
train loss:  0.504973292350769
train gradient:  0.1823059476768568
iteration : 6577
train acc:  0.734375
train loss:  0.5244255065917969
train gradient:  0.13383696893299557
iteration : 6578
train acc:  0.7421875
train loss:  0.4768930971622467
train gradient:  0.13089213863709254
iteration : 6579
train acc:  0.734375
train loss:  0.4427376687526703
train gradient:  0.12234869955486918
iteration : 6580
train acc:  0.71875
train loss:  0.5081164240837097
train gradient:  0.13006420917268258
iteration : 6581
train acc:  0.734375
train loss:  0.5397152900695801
train gradient:  0.13653886350638975
iteration : 6582
train acc:  0.7421875
train loss:  0.4757247567176819
train gradient:  0.13121038342042723
iteration : 6583
train acc:  0.7421875
train loss:  0.5014567971229553
train gradient:  0.15563792061512768
iteration : 6584
train acc:  0.7578125
train loss:  0.5012381076812744
train gradient:  0.15691868361326433
iteration : 6585
train acc:  0.7421875
train loss:  0.5269350409507751
train gradient:  0.11751887671889841
iteration : 6586
train acc:  0.703125
train loss:  0.5252820253372192
train gradient:  0.21327070821920374
iteration : 6587
train acc:  0.734375
train loss:  0.5085301995277405
train gradient:  0.12576645728414348
iteration : 6588
train acc:  0.71875
train loss:  0.508064329624176
train gradient:  0.13252757559224915
iteration : 6589
train acc:  0.7109375
train loss:  0.5144269466400146
train gradient:  0.1331893870283371
iteration : 6590
train acc:  0.7421875
train loss:  0.5318846106529236
train gradient:  0.15089455469816995
iteration : 6591
train acc:  0.703125
train loss:  0.49005967378616333
train gradient:  0.14383913494187048
iteration : 6592
train acc:  0.7578125
train loss:  0.4806073307991028
train gradient:  0.14297986393572792
iteration : 6593
train acc:  0.734375
train loss:  0.5057764053344727
train gradient:  0.11855156628474277
iteration : 6594
train acc:  0.7578125
train loss:  0.5284590721130371
train gradient:  0.1299904899053816
iteration : 6595
train acc:  0.71875
train loss:  0.5338680744171143
train gradient:  0.21131385891326265
iteration : 6596
train acc:  0.75
train loss:  0.48327210545539856
train gradient:  0.12394596644350547
iteration : 6597
train acc:  0.78125
train loss:  0.4753830134868622
train gradient:  0.13368021345530814
iteration : 6598
train acc:  0.765625
train loss:  0.476680725812912
train gradient:  0.1324408647566865
iteration : 6599
train acc:  0.65625
train loss:  0.598380982875824
train gradient:  0.1522016641746337
iteration : 6600
train acc:  0.703125
train loss:  0.5025817155838013
train gradient:  0.13508426173278404
iteration : 6601
train acc:  0.765625
train loss:  0.43869876861572266
train gradient:  0.11818345059461478
iteration : 6602
train acc:  0.6953125
train loss:  0.5485659837722778
train gradient:  0.12881760970049963
iteration : 6603
train acc:  0.75
train loss:  0.5000419616699219
train gradient:  0.1326510574211669
iteration : 6604
train acc:  0.765625
train loss:  0.47833308577537537
train gradient:  0.1875834231916276
iteration : 6605
train acc:  0.765625
train loss:  0.5349588394165039
train gradient:  0.159350795919026
iteration : 6606
train acc:  0.7890625
train loss:  0.4573938250541687
train gradient:  0.11428117956738276
iteration : 6607
train acc:  0.6796875
train loss:  0.5688493251800537
train gradient:  0.17940737553196556
iteration : 6608
train acc:  0.7109375
train loss:  0.5082570314407349
train gradient:  0.15694056785122895
iteration : 6609
train acc:  0.765625
train loss:  0.45459017157554626
train gradient:  0.09740706546751776
iteration : 6610
train acc:  0.7421875
train loss:  0.5047154426574707
train gradient:  0.1453511915134551
iteration : 6611
train acc:  0.7421875
train loss:  0.48027077317237854
train gradient:  0.16528673976439484
iteration : 6612
train acc:  0.703125
train loss:  0.5657117366790771
train gradient:  0.16026707234286602
iteration : 6613
train acc:  0.7265625
train loss:  0.46689459681510925
train gradient:  0.1359050798911717
iteration : 6614
train acc:  0.6875
train loss:  0.542046070098877
train gradient:  0.21528338984086498
iteration : 6615
train acc:  0.7265625
train loss:  0.5426851511001587
train gradient:  0.23290606408088654
iteration : 6616
train acc:  0.765625
train loss:  0.46626001596450806
train gradient:  0.1416805410775936
iteration : 6617
train acc:  0.6953125
train loss:  0.554480791091919
train gradient:  0.18709944202937334
iteration : 6618
train acc:  0.78125
train loss:  0.45682746171951294
train gradient:  0.10516016594168213
iteration : 6619
train acc:  0.7734375
train loss:  0.5201551914215088
train gradient:  0.15324478629908458
iteration : 6620
train acc:  0.765625
train loss:  0.4697636067867279
train gradient:  0.10784369826005465
iteration : 6621
train acc:  0.8046875
train loss:  0.4757470488548279
train gradient:  0.12399786252114599
iteration : 6622
train acc:  0.7109375
train loss:  0.5568902492523193
train gradient:  0.14582230757128858
iteration : 6623
train acc:  0.6796875
train loss:  0.5231517553329468
train gradient:  0.15385633999805484
iteration : 6624
train acc:  0.7890625
train loss:  0.44129878282546997
train gradient:  0.1308519175755486
iteration : 6625
train acc:  0.7578125
train loss:  0.4539094567298889
train gradient:  0.12019758691012083
iteration : 6626
train acc:  0.6796875
train loss:  0.5614032745361328
train gradient:  0.22039354554768076
iteration : 6627
train acc:  0.71875
train loss:  0.5107731819152832
train gradient:  0.11294824783208784
iteration : 6628
train acc:  0.7578125
train loss:  0.47658222913742065
train gradient:  0.12110773846239146
iteration : 6629
train acc:  0.7421875
train loss:  0.5253766775131226
train gradient:  0.14065826259904976
iteration : 6630
train acc:  0.703125
train loss:  0.5445674061775208
train gradient:  0.1686092484889478
iteration : 6631
train acc:  0.6796875
train loss:  0.5835697650909424
train gradient:  0.16705589500875795
iteration : 6632
train acc:  0.71875
train loss:  0.5179343223571777
train gradient:  0.17176725935474996
iteration : 6633
train acc:  0.78125
train loss:  0.4464823007583618
train gradient:  0.10856422159249195
iteration : 6634
train acc:  0.7109375
train loss:  0.5317877531051636
train gradient:  0.15901581906903495
iteration : 6635
train acc:  0.75
train loss:  0.5213093161582947
train gradient:  0.1924110721966099
iteration : 6636
train acc:  0.6640625
train loss:  0.5603897571563721
train gradient:  0.15437232354366334
iteration : 6637
train acc:  0.7109375
train loss:  0.5302954912185669
train gradient:  0.15260861121284092
iteration : 6638
train acc:  0.796875
train loss:  0.4115469455718994
train gradient:  0.09934204670558033
iteration : 6639
train acc:  0.6640625
train loss:  0.5355266332626343
train gradient:  0.15431464237917614
iteration : 6640
train acc:  0.78125
train loss:  0.49272623658180237
train gradient:  0.13187290644826352
iteration : 6641
train acc:  0.75
train loss:  0.46901798248291016
train gradient:  0.11570903641649766
iteration : 6642
train acc:  0.734375
train loss:  0.5097357034683228
train gradient:  0.13253264577100052
iteration : 6643
train acc:  0.7421875
train loss:  0.46594589948654175
train gradient:  0.16014293233982585
iteration : 6644
train acc:  0.765625
train loss:  0.4586673676967621
train gradient:  0.15615879224292423
iteration : 6645
train acc:  0.7421875
train loss:  0.5144308805465698
train gradient:  0.1543195546004386
iteration : 6646
train acc:  0.7109375
train loss:  0.5026600360870361
train gradient:  0.1407863363252076
iteration : 6647
train acc:  0.7734375
train loss:  0.4646191895008087
train gradient:  0.09722864844730601
iteration : 6648
train acc:  0.71875
train loss:  0.5117928981781006
train gradient:  0.1372074896274334
iteration : 6649
train acc:  0.6875
train loss:  0.5359131693840027
train gradient:  0.15485569539627736
iteration : 6650
train acc:  0.6640625
train loss:  0.5694241523742676
train gradient:  0.1870049956391967
iteration : 6651
train acc:  0.796875
train loss:  0.482948362827301
train gradient:  0.13770109490350535
iteration : 6652
train acc:  0.7578125
train loss:  0.5107886791229248
train gradient:  0.1315292050599703
iteration : 6653
train acc:  0.7421875
train loss:  0.4958426058292389
train gradient:  0.11720023018103275
iteration : 6654
train acc:  0.734375
train loss:  0.4737190008163452
train gradient:  0.13856632092478988
iteration : 6655
train acc:  0.7265625
train loss:  0.5352786779403687
train gradient:  0.15642643987007304
iteration : 6656
train acc:  0.625
train loss:  0.5846642255783081
train gradient:  0.2215011907309657
iteration : 6657
train acc:  0.734375
train loss:  0.47203704714775085
train gradient:  0.1483657031087549
iteration : 6658
train acc:  0.7421875
train loss:  0.5114330649375916
train gradient:  0.12734263672936202
iteration : 6659
train acc:  0.71875
train loss:  0.5006275177001953
train gradient:  0.16293576333657378
iteration : 6660
train acc:  0.7421875
train loss:  0.5294737219810486
train gradient:  0.16489327998601183
iteration : 6661
train acc:  0.75
train loss:  0.4932739734649658
train gradient:  0.11142723809337887
iteration : 6662
train acc:  0.78125
train loss:  0.43745067715644836
train gradient:  0.1391886103103817
iteration : 6663
train acc:  0.828125
train loss:  0.44345155358314514
train gradient:  0.0991443059918918
iteration : 6664
train acc:  0.7421875
train loss:  0.49809369444847107
train gradient:  0.12931789420556594
iteration : 6665
train acc:  0.7734375
train loss:  0.48610249161720276
train gradient:  0.15855590128637498
iteration : 6666
train acc:  0.671875
train loss:  0.555496096611023
train gradient:  0.19381713320942917
iteration : 6667
train acc:  0.734375
train loss:  0.5413311719894409
train gradient:  0.13425317138053405
iteration : 6668
train acc:  0.71875
train loss:  0.5232688784599304
train gradient:  0.13133182877468375
iteration : 6669
train acc:  0.7265625
train loss:  0.5109484791755676
train gradient:  0.17064886480600905
iteration : 6670
train acc:  0.65625
train loss:  0.5374536514282227
train gradient:  0.14240995225121145
iteration : 6671
train acc:  0.6796875
train loss:  0.5582481026649475
train gradient:  0.18454664740274318
iteration : 6672
train acc:  0.71875
train loss:  0.5369852185249329
train gradient:  0.15956868783878536
iteration : 6673
train acc:  0.6953125
train loss:  0.5275017023086548
train gradient:  0.1382824044659831
iteration : 6674
train acc:  0.75
train loss:  0.49935969710350037
train gradient:  0.17443673690953304
iteration : 6675
train acc:  0.7578125
train loss:  0.4713731110095978
train gradient:  0.1349300593836078
iteration : 6676
train acc:  0.734375
train loss:  0.5160846710205078
train gradient:  0.1690096459001965
iteration : 6677
train acc:  0.734375
train loss:  0.5295052528381348
train gradient:  0.14906436580041366
iteration : 6678
train acc:  0.703125
train loss:  0.5397026538848877
train gradient:  0.13593283755153532
iteration : 6679
train acc:  0.7421875
train loss:  0.5290778279304504
train gradient:  0.13569241040468605
iteration : 6680
train acc:  0.8359375
train loss:  0.439506471157074
train gradient:  0.13097488732616785
iteration : 6681
train acc:  0.796875
train loss:  0.45522552728652954
train gradient:  0.12807115274100891
iteration : 6682
train acc:  0.7421875
train loss:  0.48460328578948975
train gradient:  0.13253023231294492
iteration : 6683
train acc:  0.765625
train loss:  0.49044981598854065
train gradient:  0.13857488160140957
iteration : 6684
train acc:  0.75
train loss:  0.48747706413269043
train gradient:  0.13991252889934308
iteration : 6685
train acc:  0.71875
train loss:  0.5159506797790527
train gradient:  0.13737040330921813
iteration : 6686
train acc:  0.734375
train loss:  0.5358443260192871
train gradient:  0.15842459131741476
iteration : 6687
train acc:  0.734375
train loss:  0.5250250101089478
train gradient:  0.15742878149550377
iteration : 6688
train acc:  0.7734375
train loss:  0.5181249380111694
train gradient:  0.13304866716557368
iteration : 6689
train acc:  0.671875
train loss:  0.5794879198074341
train gradient:  0.23129115766998798
iteration : 6690
train acc:  0.7578125
train loss:  0.4912620186805725
train gradient:  0.14799629144497742
iteration : 6691
train acc:  0.75
train loss:  0.46804502606391907
train gradient:  0.1050233384120018
iteration : 6692
train acc:  0.734375
train loss:  0.5096278786659241
train gradient:  0.1458860179138861
iteration : 6693
train acc:  0.734375
train loss:  0.4722975492477417
train gradient:  0.11557541361388014
iteration : 6694
train acc:  0.703125
train loss:  0.5474045276641846
train gradient:  0.13071482802319048
iteration : 6695
train acc:  0.7265625
train loss:  0.49294403195381165
train gradient:  0.13666815835137056
iteration : 6696
train acc:  0.640625
train loss:  0.6539914011955261
train gradient:  0.1970309346081583
iteration : 6697
train acc:  0.8125
train loss:  0.4831176996231079
train gradient:  0.13194731762662748
iteration : 6698
train acc:  0.7890625
train loss:  0.49637073278427124
train gradient:  0.13495450979052218
iteration : 6699
train acc:  0.703125
train loss:  0.5539768934249878
train gradient:  0.15733079470882133
iteration : 6700
train acc:  0.75
train loss:  0.5245471000671387
train gradient:  0.16345112214079643
iteration : 6701
train acc:  0.71875
train loss:  0.5529030561447144
train gradient:  0.16025067936615578
iteration : 6702
train acc:  0.7109375
train loss:  0.5336640477180481
train gradient:  0.12332681433695832
iteration : 6703
train acc:  0.7109375
train loss:  0.5093052387237549
train gradient:  0.14751147675515597
iteration : 6704
train acc:  0.765625
train loss:  0.4544992446899414
train gradient:  0.12125501526132841
iteration : 6705
train acc:  0.75
train loss:  0.4852701425552368
train gradient:  0.13097985197227574
iteration : 6706
train acc:  0.7734375
train loss:  0.5083343386650085
train gradient:  0.14598381178198927
iteration : 6707
train acc:  0.765625
train loss:  0.5290541052818298
train gradient:  0.17715676199279368
iteration : 6708
train acc:  0.7265625
train loss:  0.5352568626403809
train gradient:  0.16637906963595606
iteration : 6709
train acc:  0.7109375
train loss:  0.5372096300125122
train gradient:  0.166423284175381
iteration : 6710
train acc:  0.734375
train loss:  0.5191904306411743
train gradient:  0.14947481722946537
iteration : 6711
train acc:  0.6875
train loss:  0.5384045839309692
train gradient:  0.1460296900128868
iteration : 6712
train acc:  0.765625
train loss:  0.4793519377708435
train gradient:  0.11893355809322403
iteration : 6713
train acc:  0.7421875
train loss:  0.4747481644153595
train gradient:  0.13755386539167752
iteration : 6714
train acc:  0.796875
train loss:  0.4565722942352295
train gradient:  0.10059848841698404
iteration : 6715
train acc:  0.578125
train loss:  0.6417087912559509
train gradient:  0.25014164173139264
iteration : 6716
train acc:  0.7109375
train loss:  0.4954726994037628
train gradient:  0.13070194880211294
iteration : 6717
train acc:  0.7265625
train loss:  0.541641354560852
train gradient:  0.1250398451111786
iteration : 6718
train acc:  0.796875
train loss:  0.42525532841682434
train gradient:  0.1276573757045013
iteration : 6719
train acc:  0.796875
train loss:  0.427894651889801
train gradient:  0.09448152783827703
iteration : 6720
train acc:  0.78125
train loss:  0.4549283981323242
train gradient:  0.11818378730801368
iteration : 6721
train acc:  0.7265625
train loss:  0.49742522835731506
train gradient:  0.17466626005555974
iteration : 6722
train acc:  0.7109375
train loss:  0.5351254343986511
train gradient:  0.14137339043312114
iteration : 6723
train acc:  0.7578125
train loss:  0.4885904788970947
train gradient:  0.12535038704866885
iteration : 6724
train acc:  0.71875
train loss:  0.5142737030982971
train gradient:  0.13170330733112237
iteration : 6725
train acc:  0.7265625
train loss:  0.4781113862991333
train gradient:  0.11520497650844899
iteration : 6726
train acc:  0.75
train loss:  0.46947765350341797
train gradient:  0.11854442151080481
iteration : 6727
train acc:  0.7421875
train loss:  0.5080312490463257
train gradient:  0.1303058379920783
iteration : 6728
train acc:  0.765625
train loss:  0.4674522578716278
train gradient:  0.12595778722042322
iteration : 6729
train acc:  0.78125
train loss:  0.5102236270904541
train gradient:  0.21143810199999027
iteration : 6730
train acc:  0.71875
train loss:  0.5726301670074463
train gradient:  0.14541725792793386
iteration : 6731
train acc:  0.75
train loss:  0.5041438937187195
train gradient:  0.1388909344473005
iteration : 6732
train acc:  0.7109375
train loss:  0.5489113330841064
train gradient:  0.17214566816418486
iteration : 6733
train acc:  0.734375
train loss:  0.5097333788871765
train gradient:  0.12333153621854283
iteration : 6734
train acc:  0.7109375
train loss:  0.5391837954521179
train gradient:  0.17741172106467304
iteration : 6735
train acc:  0.71875
train loss:  0.5430721044540405
train gradient:  0.22084457887605846
iteration : 6736
train acc:  0.7734375
train loss:  0.5279492139816284
train gradient:  0.15660566658228978
iteration : 6737
train acc:  0.7421875
train loss:  0.4711959660053253
train gradient:  0.11436662354659116
iteration : 6738
train acc:  0.7734375
train loss:  0.46544843912124634
train gradient:  0.10106827718522614
iteration : 6739
train acc:  0.71875
train loss:  0.5140621066093445
train gradient:  0.16641340117215142
iteration : 6740
train acc:  0.7890625
train loss:  0.5018428564071655
train gradient:  0.15788596005694105
iteration : 6741
train acc:  0.71875
train loss:  0.4614381492137909
train gradient:  0.13458211902024825
iteration : 6742
train acc:  0.765625
train loss:  0.4913766086101532
train gradient:  0.12394300309036707
iteration : 6743
train acc:  0.6953125
train loss:  0.540469765663147
train gradient:  0.1834467238592104
iteration : 6744
train acc:  0.703125
train loss:  0.5627658367156982
train gradient:  0.17855656442408752
iteration : 6745
train acc:  0.78125
train loss:  0.46583399176597595
train gradient:  0.0950580655305566
iteration : 6746
train acc:  0.78125
train loss:  0.5099228620529175
train gradient:  0.15982862426355482
iteration : 6747
train acc:  0.703125
train loss:  0.5165280103683472
train gradient:  0.1500578271972322
iteration : 6748
train acc:  0.703125
train loss:  0.5225183963775635
train gradient:  0.1774825404190002
iteration : 6749
train acc:  0.8125
train loss:  0.41337406635284424
train gradient:  0.10378704489550158
iteration : 6750
train acc:  0.671875
train loss:  0.5594514608383179
train gradient:  0.17548426771170167
iteration : 6751
train acc:  0.65625
train loss:  0.5988627076148987
train gradient:  0.16161654815904114
iteration : 6752
train acc:  0.71875
train loss:  0.5083876848220825
train gradient:  0.1357064836840175
iteration : 6753
train acc:  0.6953125
train loss:  0.5729353427886963
train gradient:  0.1536669713828907
iteration : 6754
train acc:  0.7265625
train loss:  0.5073773860931396
train gradient:  0.1407129123493496
iteration : 6755
train acc:  0.734375
train loss:  0.5085357427597046
train gradient:  0.14082250059591161
iteration : 6756
train acc:  0.703125
train loss:  0.546400785446167
train gradient:  0.20085646112122277
iteration : 6757
train acc:  0.6796875
train loss:  0.5442290306091309
train gradient:  0.19846648125945948
iteration : 6758
train acc:  0.71875
train loss:  0.5267888307571411
train gradient:  0.12664169070236286
iteration : 6759
train acc:  0.7421875
train loss:  0.47821831703186035
train gradient:  0.12246590921428614
iteration : 6760
train acc:  0.828125
train loss:  0.43870222568511963
train gradient:  0.12140248927810755
iteration : 6761
train acc:  0.703125
train loss:  0.5255716443061829
train gradient:  0.14183245745681464
iteration : 6762
train acc:  0.765625
train loss:  0.5068087577819824
train gradient:  0.13979541636934398
iteration : 6763
train acc:  0.75
train loss:  0.49987584352493286
train gradient:  0.10667154894478847
iteration : 6764
train acc:  0.75
train loss:  0.49701711535453796
train gradient:  0.12053220755716851
iteration : 6765
train acc:  0.7265625
train loss:  0.5254648923873901
train gradient:  0.12572869755942917
iteration : 6766
train acc:  0.7421875
train loss:  0.5081049203872681
train gradient:  0.13922579907522847
iteration : 6767
train acc:  0.7890625
train loss:  0.4242776334285736
train gradient:  0.11989546423045253
iteration : 6768
train acc:  0.6953125
train loss:  0.5136507749557495
train gradient:  0.16186437703372566
iteration : 6769
train acc:  0.7578125
train loss:  0.4507940709590912
train gradient:  0.10987851342689255
iteration : 6770
train acc:  0.734375
train loss:  0.5192058086395264
train gradient:  0.12499168459925841
iteration : 6771
train acc:  0.71875
train loss:  0.5196013450622559
train gradient:  0.16769722628888606
iteration : 6772
train acc:  0.734375
train loss:  0.4554503262042999
train gradient:  0.12045227801675623
iteration : 6773
train acc:  0.71875
train loss:  0.5144078731536865
train gradient:  0.2033943921916485
iteration : 6774
train acc:  0.7421875
train loss:  0.5105058550834656
train gradient:  0.13440904385334182
iteration : 6775
train acc:  0.7421875
train loss:  0.5009199380874634
train gradient:  0.1486139181383032
iteration : 6776
train acc:  0.6953125
train loss:  0.5436246395111084
train gradient:  0.16085958011155188
iteration : 6777
train acc:  0.7890625
train loss:  0.4687160849571228
train gradient:  0.11752871370762125
iteration : 6778
train acc:  0.6875
train loss:  0.5686875581741333
train gradient:  0.21734544183829638
iteration : 6779
train acc:  0.7578125
train loss:  0.5845670700073242
train gradient:  0.16593100502437735
iteration : 6780
train acc:  0.6875
train loss:  0.5505322217941284
train gradient:  0.14136662430080615
iteration : 6781
train acc:  0.6953125
train loss:  0.5740607976913452
train gradient:  0.17206221155331947
iteration : 6782
train acc:  0.7578125
train loss:  0.5162928104400635
train gradient:  0.10573561648811114
iteration : 6783
train acc:  0.75
train loss:  0.584694504737854
train gradient:  0.16541188690299108
iteration : 6784
train acc:  0.6875
train loss:  0.5241234302520752
train gradient:  0.1458897856133498
iteration : 6785
train acc:  0.703125
train loss:  0.5374591946601868
train gradient:  0.16577567512843622
iteration : 6786
train acc:  0.7265625
train loss:  0.5271164774894714
train gradient:  0.1436236106187903
iteration : 6787
train acc:  0.7578125
train loss:  0.4705718755722046
train gradient:  0.12465860205532993
iteration : 6788
train acc:  0.671875
train loss:  0.5439125299453735
train gradient:  0.11708004774842047
iteration : 6789
train acc:  0.75
train loss:  0.4902152419090271
train gradient:  0.13478028566314393
iteration : 6790
train acc:  0.6953125
train loss:  0.5600594282150269
train gradient:  0.14797007224451736
iteration : 6791
train acc:  0.75
train loss:  0.4917840361595154
train gradient:  0.15232604403823125
iteration : 6792
train acc:  0.7265625
train loss:  0.5258525013923645
train gradient:  0.13260439747204114
iteration : 6793
train acc:  0.7734375
train loss:  0.4755125939846039
train gradient:  0.15418698680013365
iteration : 6794
train acc:  0.7265625
train loss:  0.5420957803726196
train gradient:  0.14528198734736955
iteration : 6795
train acc:  0.8046875
train loss:  0.45585116744041443
train gradient:  0.11205001785510345
iteration : 6796
train acc:  0.734375
train loss:  0.5354398488998413
train gradient:  0.15473277568496338
iteration : 6797
train acc:  0.734375
train loss:  0.532167911529541
train gradient:  0.15911170111580697
iteration : 6798
train acc:  0.734375
train loss:  0.5182312726974487
train gradient:  0.15158244932242798
iteration : 6799
train acc:  0.6640625
train loss:  0.5598365068435669
train gradient:  0.20842459055751733
iteration : 6800
train acc:  0.7265625
train loss:  0.5353338122367859
train gradient:  0.1409814523128321
iteration : 6801
train acc:  0.8203125
train loss:  0.4273180663585663
train gradient:  0.10545594548087027
iteration : 6802
train acc:  0.6953125
train loss:  0.5276108980178833
train gradient:  0.12015394014752377
iteration : 6803
train acc:  0.734375
train loss:  0.4930453598499298
train gradient:  0.13290059001286567
iteration : 6804
train acc:  0.7265625
train loss:  0.48457515239715576
train gradient:  0.1157863917721935
iteration : 6805
train acc:  0.6875
train loss:  0.5476036071777344
train gradient:  0.1820725608789056
iteration : 6806
train acc:  0.7578125
train loss:  0.4379866123199463
train gradient:  0.11505235640708796
iteration : 6807
train acc:  0.7578125
train loss:  0.48075318336486816
train gradient:  0.12072221598886058
iteration : 6808
train acc:  0.703125
train loss:  0.5227311253547668
train gradient:  0.11214266766896674
iteration : 6809
train acc:  0.734375
train loss:  0.5287133455276489
train gradient:  0.15408518368925592
iteration : 6810
train acc:  0.6953125
train loss:  0.5241235494613647
train gradient:  0.15606309079106984
iteration : 6811
train acc:  0.859375
train loss:  0.4010537266731262
train gradient:  0.08839542955592404
iteration : 6812
train acc:  0.6875
train loss:  0.5902746915817261
train gradient:  0.2042774723781608
iteration : 6813
train acc:  0.75
train loss:  0.48260021209716797
train gradient:  0.1231475379664978
iteration : 6814
train acc:  0.7109375
train loss:  0.5195597410202026
train gradient:  0.14959136222002223
iteration : 6815
train acc:  0.7109375
train loss:  0.5148704051971436
train gradient:  0.16646673654234098
iteration : 6816
train acc:  0.734375
train loss:  0.47568202018737793
train gradient:  0.13181837118673972
iteration : 6817
train acc:  0.7421875
train loss:  0.4872668981552124
train gradient:  0.11184718954269579
iteration : 6818
train acc:  0.78125
train loss:  0.5022941827774048
train gradient:  0.12239191778494954
iteration : 6819
train acc:  0.625
train loss:  0.6024470329284668
train gradient:  0.18186681475183752
iteration : 6820
train acc:  0.71875
train loss:  0.5321965217590332
train gradient:  0.1406330606742293
iteration : 6821
train acc:  0.734375
train loss:  0.5237267017364502
train gradient:  0.16036491809189157
iteration : 6822
train acc:  0.6640625
train loss:  0.5362597703933716
train gradient:  0.1506567708543552
iteration : 6823
train acc:  0.6796875
train loss:  0.5592272281646729
train gradient:  0.17140811088472302
iteration : 6824
train acc:  0.734375
train loss:  0.4664757251739502
train gradient:  0.10266680704775528
iteration : 6825
train acc:  0.7265625
train loss:  0.5548127293586731
train gradient:  0.1688696758036761
iteration : 6826
train acc:  0.7109375
train loss:  0.5027195811271667
train gradient:  0.14305329485425142
iteration : 6827
train acc:  0.7421875
train loss:  0.4828992486000061
train gradient:  0.11487811540098686
iteration : 6828
train acc:  0.703125
train loss:  0.5433611869812012
train gradient:  0.16131936324125734
iteration : 6829
train acc:  0.765625
train loss:  0.49868977069854736
train gradient:  0.13082530092681455
iteration : 6830
train acc:  0.7890625
train loss:  0.4523632228374481
train gradient:  0.11888467322107905
iteration : 6831
train acc:  0.7578125
train loss:  0.5420028567314148
train gradient:  0.18534467533980747
iteration : 6832
train acc:  0.71875
train loss:  0.5154817700386047
train gradient:  0.13253434948155263
iteration : 6833
train acc:  0.78125
train loss:  0.4874463677406311
train gradient:  0.16253171232662275
iteration : 6834
train acc:  0.6953125
train loss:  0.5164693593978882
train gradient:  0.13344422293341235
iteration : 6835
train acc:  0.671875
train loss:  0.5711091160774231
train gradient:  0.14555308480124035
iteration : 6836
train acc:  0.7421875
train loss:  0.4860577881336212
train gradient:  0.1476622936769364
iteration : 6837
train acc:  0.6953125
train loss:  0.5723366737365723
train gradient:  0.1549815927610561
iteration : 6838
train acc:  0.734375
train loss:  0.48400431871414185
train gradient:  0.12017218129641517
iteration : 6839
train acc:  0.84375
train loss:  0.4046993553638458
train gradient:  0.09987428371276241
iteration : 6840
train acc:  0.75
train loss:  0.5097717046737671
train gradient:  0.13819453368162116
iteration : 6841
train acc:  0.7578125
train loss:  0.5064839124679565
train gradient:  0.129494223862817
iteration : 6842
train acc:  0.671875
train loss:  0.5562247633934021
train gradient:  0.16019968417322164
iteration : 6843
train acc:  0.7578125
train loss:  0.4896327555179596
train gradient:  0.1419064356468569
iteration : 6844
train acc:  0.75
train loss:  0.4773399233818054
train gradient:  0.12013525983123031
iteration : 6845
train acc:  0.765625
train loss:  0.4932518005371094
train gradient:  0.14874019809527145
iteration : 6846
train acc:  0.7421875
train loss:  0.5100539326667786
train gradient:  0.20993895496494047
iteration : 6847
train acc:  0.7109375
train loss:  0.5325477123260498
train gradient:  0.19783103580008826
iteration : 6848
train acc:  0.71875
train loss:  0.5435464382171631
train gradient:  0.1571224034510717
iteration : 6849
train acc:  0.7109375
train loss:  0.5464187860488892
train gradient:  0.11813712862437083
iteration : 6850
train acc:  0.765625
train loss:  0.5087350606918335
train gradient:  0.1302487493740882
iteration : 6851
train acc:  0.6171875
train loss:  0.6202869415283203
train gradient:  0.2228649202025832
iteration : 6852
train acc:  0.7265625
train loss:  0.5351431369781494
train gradient:  0.15819286548698747
iteration : 6853
train acc:  0.859375
train loss:  0.4069931209087372
train gradient:  0.08525480479751604
iteration : 6854
train acc:  0.78125
train loss:  0.45624881982803345
train gradient:  0.1341850480118087
iteration : 6855
train acc:  0.7734375
train loss:  0.4842936396598816
train gradient:  0.12176032532683115
iteration : 6856
train acc:  0.734375
train loss:  0.5436539649963379
train gradient:  0.13276039230202968
iteration : 6857
train acc:  0.7890625
train loss:  0.46859830617904663
train gradient:  0.08913464222678853
iteration : 6858
train acc:  0.734375
train loss:  0.49312227964401245
train gradient:  0.1244557618645835
iteration : 6859
train acc:  0.7734375
train loss:  0.4508620500564575
train gradient:  0.1374853018499784
iteration : 6860
train acc:  0.78125
train loss:  0.44044560194015503
train gradient:  0.16967846754093674
iteration : 6861
train acc:  0.7421875
train loss:  0.5005526542663574
train gradient:  0.13544740420176726
iteration : 6862
train acc:  0.7578125
train loss:  0.5249000787734985
train gradient:  0.14248472363992798
iteration : 6863
train acc:  0.796875
train loss:  0.4262987971305847
train gradient:  0.10494930917820615
iteration : 6864
train acc:  0.734375
train loss:  0.5320916771888733
train gradient:  0.15602181539918997
iteration : 6865
train acc:  0.7734375
train loss:  0.5203151702880859
train gradient:  0.14585443214601207
iteration : 6866
train acc:  0.6875
train loss:  0.5590755939483643
train gradient:  0.13754220162010825
iteration : 6867
train acc:  0.7421875
train loss:  0.48809996247291565
train gradient:  0.12892626838543586
iteration : 6868
train acc:  0.7109375
train loss:  0.5135841369628906
train gradient:  0.1699173081543952
iteration : 6869
train acc:  0.7890625
train loss:  0.47297102212905884
train gradient:  0.14450902333483068
iteration : 6870
train acc:  0.75
train loss:  0.5063151121139526
train gradient:  0.14365193041489377
iteration : 6871
train acc:  0.640625
train loss:  0.5974945425987244
train gradient:  0.1669283922081805
iteration : 6872
train acc:  0.6796875
train loss:  0.5463029742240906
train gradient:  0.14281226120618007
iteration : 6873
train acc:  0.7109375
train loss:  0.49832749366760254
train gradient:  0.1547382213413172
iteration : 6874
train acc:  0.8046875
train loss:  0.45214053988456726
train gradient:  0.12005210628865635
iteration : 6875
train acc:  0.6953125
train loss:  0.4786040782928467
train gradient:  0.12597149806902985
iteration : 6876
train acc:  0.7421875
train loss:  0.521325945854187
train gradient:  0.12109643004234362
iteration : 6877
train acc:  0.703125
train loss:  0.515571117401123
train gradient:  0.14865935793058963
iteration : 6878
train acc:  0.7734375
train loss:  0.47480615973472595
train gradient:  0.1335913163392452
iteration : 6879
train acc:  0.7109375
train loss:  0.5056670904159546
train gradient:  0.1157909773722161
iteration : 6880
train acc:  0.6875
train loss:  0.5944533348083496
train gradient:  0.14104834025584517
iteration : 6881
train acc:  0.7265625
train loss:  0.5006606578826904
train gradient:  0.1542048853255868
iteration : 6882
train acc:  0.6875
train loss:  0.5609001517295837
train gradient:  0.14869336596046195
iteration : 6883
train acc:  0.765625
train loss:  0.4708743691444397
train gradient:  0.12018052849675427
iteration : 6884
train acc:  0.75
train loss:  0.45125812292099
train gradient:  0.11845138352566238
iteration : 6885
train acc:  0.828125
train loss:  0.4651470482349396
train gradient:  0.14094882581750384
iteration : 6886
train acc:  0.765625
train loss:  0.4906545877456665
train gradient:  0.14456992194267132
iteration : 6887
train acc:  0.734375
train loss:  0.503777027130127
train gradient:  0.12235800040626184
iteration : 6888
train acc:  0.71875
train loss:  0.5021530389785767
train gradient:  0.11384910550169663
iteration : 6889
train acc:  0.75
train loss:  0.5028454065322876
train gradient:  0.14652756427877328
iteration : 6890
train acc:  0.6953125
train loss:  0.5343468189239502
train gradient:  0.19275293593999143
iteration : 6891
train acc:  0.640625
train loss:  0.6059184074401855
train gradient:  0.14374097537078087
iteration : 6892
train acc:  0.71875
train loss:  0.5411137342453003
train gradient:  0.18637793383925988
iteration : 6893
train acc:  0.7421875
train loss:  0.4825863838195801
train gradient:  0.11576236379248801
iteration : 6894
train acc:  0.765625
train loss:  0.5136668682098389
train gradient:  0.1302534759660673
iteration : 6895
train acc:  0.7109375
train loss:  0.5167072415351868
train gradient:  0.12795896185076483
iteration : 6896
train acc:  0.703125
train loss:  0.5240419507026672
train gradient:  0.13169258817659996
iteration : 6897
train acc:  0.65625
train loss:  0.5994536876678467
train gradient:  0.18123149249050455
iteration : 6898
train acc:  0.7109375
train loss:  0.5122462511062622
train gradient:  0.1315961207518322
iteration : 6899
train acc:  0.7421875
train loss:  0.5072607398033142
train gradient:  0.1428535969147211
iteration : 6900
train acc:  0.734375
train loss:  0.5063748359680176
train gradient:  0.12394225532495745
iteration : 6901
train acc:  0.7109375
train loss:  0.5903955698013306
train gradient:  0.19246994797030872
iteration : 6902
train acc:  0.7578125
train loss:  0.48763424158096313
train gradient:  0.14837684876651658
iteration : 6903
train acc:  0.6953125
train loss:  0.5788472294807434
train gradient:  0.16625421164862497
iteration : 6904
train acc:  0.7265625
train loss:  0.5337774753570557
train gradient:  0.12935363097790753
iteration : 6905
train acc:  0.7421875
train loss:  0.5740447640419006
train gradient:  0.1884476782160423
iteration : 6906
train acc:  0.671875
train loss:  0.5405985116958618
train gradient:  0.19074099243727288
iteration : 6907
train acc:  0.7578125
train loss:  0.49598854780197144
train gradient:  0.1292714867852375
iteration : 6908
train acc:  0.7421875
train loss:  0.5013838410377502
train gradient:  0.17255544182074406
iteration : 6909
train acc:  0.703125
train loss:  0.5438364148139954
train gradient:  0.12961653019582065
iteration : 6910
train acc:  0.734375
train loss:  0.5535572171211243
train gradient:  0.14673461704781238
iteration : 6911
train acc:  0.7421875
train loss:  0.5113581418991089
train gradient:  0.13551485203394248
iteration : 6912
train acc:  0.7734375
train loss:  0.4835832715034485
train gradient:  0.12770526740453186
iteration : 6913
train acc:  0.7265625
train loss:  0.5166215896606445
train gradient:  0.14774975621977865
iteration : 6914
train acc:  0.65625
train loss:  0.5755559802055359
train gradient:  0.15264964443282455
iteration : 6915
train acc:  0.703125
train loss:  0.5249710083007812
train gradient:  0.16110807584782866
iteration : 6916
train acc:  0.734375
train loss:  0.5354187488555908
train gradient:  0.14488182008017308
iteration : 6917
train acc:  0.7421875
train loss:  0.48224425315856934
train gradient:  0.17921498924846696
iteration : 6918
train acc:  0.734375
train loss:  0.5051349997520447
train gradient:  0.1761077031196982
iteration : 6919
train acc:  0.734375
train loss:  0.5187710523605347
train gradient:  0.13567099017617873
iteration : 6920
train acc:  0.75
train loss:  0.5012203454971313
train gradient:  0.14119523507233872
iteration : 6921
train acc:  0.7265625
train loss:  0.5439059734344482
train gradient:  0.13028628240147966
iteration : 6922
train acc:  0.6875
train loss:  0.502076268196106
train gradient:  0.13296791792994583
iteration : 6923
train acc:  0.7265625
train loss:  0.5068128108978271
train gradient:  0.151417838519638
iteration : 6924
train acc:  0.7109375
train loss:  0.5329355597496033
train gradient:  0.12849084553515971
iteration : 6925
train acc:  0.7578125
train loss:  0.4736228287220001
train gradient:  0.13695223907984072
iteration : 6926
train acc:  0.671875
train loss:  0.5490608215332031
train gradient:  0.17249018483171233
iteration : 6927
train acc:  0.75
train loss:  0.4990639090538025
train gradient:  0.16473753435737898
iteration : 6928
train acc:  0.765625
train loss:  0.5555307865142822
train gradient:  0.19792220645189268
iteration : 6929
train acc:  0.6640625
train loss:  0.609052836894989
train gradient:  0.1866691485432183
iteration : 6930
train acc:  0.7421875
train loss:  0.47678378224372864
train gradient:  0.11017343913192863
iteration : 6931
train acc:  0.6875
train loss:  0.5210156440734863
train gradient:  0.19300789855410783
iteration : 6932
train acc:  0.796875
train loss:  0.45149505138397217
train gradient:  0.13310033584867254
iteration : 6933
train acc:  0.703125
train loss:  0.4853047728538513
train gradient:  0.13063645080738054
iteration : 6934
train acc:  0.6953125
train loss:  0.5360360741615295
train gradient:  0.17115355707848362
iteration : 6935
train acc:  0.6875
train loss:  0.575188398361206
train gradient:  0.19932838950186327
iteration : 6936
train acc:  0.7734375
train loss:  0.4670431613922119
train gradient:  0.11080953229697726
iteration : 6937
train acc:  0.6484375
train loss:  0.5874537825584412
train gradient:  0.1497213590011118
iteration : 6938
train acc:  0.7421875
train loss:  0.4608522653579712
train gradient:  0.09432697028905723
iteration : 6939
train acc:  0.71875
train loss:  0.5090692639350891
train gradient:  0.144083717157314
iteration : 6940
train acc:  0.7421875
train loss:  0.4929673969745636
train gradient:  0.11843429579926504
iteration : 6941
train acc:  0.7578125
train loss:  0.447432279586792
train gradient:  0.15306601140045428
iteration : 6942
train acc:  0.7421875
train loss:  0.5419696569442749
train gradient:  0.15312967879125147
iteration : 6943
train acc:  0.6953125
train loss:  0.4904048442840576
train gradient:  0.11816176309233008
iteration : 6944
train acc:  0.7421875
train loss:  0.5321729183197021
train gradient:  0.16874631136143664
iteration : 6945
train acc:  0.71875
train loss:  0.5510941743850708
train gradient:  0.16810587856838005
iteration : 6946
train acc:  0.734375
train loss:  0.4766395092010498
train gradient:  0.11079965033879596
iteration : 6947
train acc:  0.7734375
train loss:  0.5169017314910889
train gradient:  0.14349100512780685
iteration : 6948
train acc:  0.8125
train loss:  0.43067702651023865
train gradient:  0.10760849821322911
iteration : 6949
train acc:  0.7421875
train loss:  0.5128648281097412
train gradient:  0.13725334218202234
iteration : 6950
train acc:  0.765625
train loss:  0.49492529034614563
train gradient:  0.14694906155832435
iteration : 6951
train acc:  0.7421875
train loss:  0.4991000294685364
train gradient:  0.13925365074292864
iteration : 6952
train acc:  0.7421875
train loss:  0.5194239616394043
train gradient:  0.13946951780724226
iteration : 6953
train acc:  0.7421875
train loss:  0.5609310865402222
train gradient:  0.18674151357979588
iteration : 6954
train acc:  0.7265625
train loss:  0.5319837927818298
train gradient:  0.1568604541011338
iteration : 6955
train acc:  0.7265625
train loss:  0.5270612239837646
train gradient:  0.203388074909708
iteration : 6956
train acc:  0.7578125
train loss:  0.4958153963088989
train gradient:  0.17483450362521963
iteration : 6957
train acc:  0.734375
train loss:  0.45727214217185974
train gradient:  0.11643467744634486
iteration : 6958
train acc:  0.7421875
train loss:  0.4646143317222595
train gradient:  0.127284488082069
iteration : 6959
train acc:  0.75
train loss:  0.49408337473869324
train gradient:  0.14058447464041401
iteration : 6960
train acc:  0.765625
train loss:  0.47697752714157104
train gradient:  0.12165808975881895
iteration : 6961
train acc:  0.7265625
train loss:  0.557051420211792
train gradient:  0.15734383181747935
iteration : 6962
train acc:  0.703125
train loss:  0.5365893840789795
train gradient:  0.16359297623257213
iteration : 6963
train acc:  0.734375
train loss:  0.4818354845046997
train gradient:  0.1358028706493149
iteration : 6964
train acc:  0.7578125
train loss:  0.4690708518028259
train gradient:  0.10615694364643033
iteration : 6965
train acc:  0.71875
train loss:  0.5463629364967346
train gradient:  0.19597443345864013
iteration : 6966
train acc:  0.78125
train loss:  0.48099634051322937
train gradient:  0.1372264231973477
iteration : 6967
train acc:  0.796875
train loss:  0.44223982095718384
train gradient:  0.10765811700717169
iteration : 6968
train acc:  0.7421875
train loss:  0.4757689833641052
train gradient:  0.11934192082706196
iteration : 6969
train acc:  0.7109375
train loss:  0.5735660791397095
train gradient:  0.18139061745710694
iteration : 6970
train acc:  0.765625
train loss:  0.4784323573112488
train gradient:  0.13881330863021318
iteration : 6971
train acc:  0.71875
train loss:  0.5247403383255005
train gradient:  0.14475959610338854
iteration : 6972
train acc:  0.765625
train loss:  0.47580331563949585
train gradient:  0.11767330460750768
iteration : 6973
train acc:  0.703125
train loss:  0.5315770506858826
train gradient:  0.13215053182510708
iteration : 6974
train acc:  0.7734375
train loss:  0.47823837399482727
train gradient:  0.12409567046872712
iteration : 6975
train acc:  0.7578125
train loss:  0.46404463052749634
train gradient:  0.11585848507258607
iteration : 6976
train acc:  0.75
train loss:  0.5098017454147339
train gradient:  0.1461138082153946
iteration : 6977
train acc:  0.734375
train loss:  0.505200207233429
train gradient:  0.1608001710148914
iteration : 6978
train acc:  0.703125
train loss:  0.5762567520141602
train gradient:  0.1648701624446388
iteration : 6979
train acc:  0.71875
train loss:  0.4573832154273987
train gradient:  0.12606079897637756
iteration : 6980
train acc:  0.7734375
train loss:  0.47508925199508667
train gradient:  0.1431523976692597
iteration : 6981
train acc:  0.6953125
train loss:  0.5641955137252808
train gradient:  0.14781275386313908
iteration : 6982
train acc:  0.7265625
train loss:  0.5055755376815796
train gradient:  0.13202762997635709
iteration : 6983
train acc:  0.6953125
train loss:  0.525351881980896
train gradient:  0.1927509510328762
iteration : 6984
train acc:  0.78125
train loss:  0.4835108518600464
train gradient:  0.11571412476477416
iteration : 6985
train acc:  0.6953125
train loss:  0.5018565654754639
train gradient:  0.1568777919732286
iteration : 6986
train acc:  0.78125
train loss:  0.46467161178588867
train gradient:  0.1260582767213464
iteration : 6987
train acc:  0.703125
train loss:  0.5518710613250732
train gradient:  0.19618132070607874
iteration : 6988
train acc:  0.6875
train loss:  0.5652027130126953
train gradient:  0.20327512882529158
iteration : 6989
train acc:  0.7265625
train loss:  0.5229635834693909
train gradient:  0.16380933537906908
iteration : 6990
train acc:  0.71875
train loss:  0.5148672461509705
train gradient:  0.14312288662039885
iteration : 6991
train acc:  0.71875
train loss:  0.5379563570022583
train gradient:  0.15372820238891832
iteration : 6992
train acc:  0.6640625
train loss:  0.60725337266922
train gradient:  0.1854009368291583
iteration : 6993
train acc:  0.75
train loss:  0.49325039982795715
train gradient:  0.12491423652172423
iteration : 6994
train acc:  0.734375
train loss:  0.5091613531112671
train gradient:  0.14885579417226003
iteration : 6995
train acc:  0.6875
train loss:  0.5482400059700012
train gradient:  0.15607565439833482
iteration : 6996
train acc:  0.7734375
train loss:  0.4662540555000305
train gradient:  0.11365176186698452
iteration : 6997
train acc:  0.6875
train loss:  0.5350973010063171
train gradient:  0.1291002348631554
iteration : 6998
train acc:  0.765625
train loss:  0.4606998562812805
train gradient:  0.11778681515084694
iteration : 6999
train acc:  0.8125
train loss:  0.45275020599365234
train gradient:  0.12986354560609312
iteration : 7000
train acc:  0.6484375
train loss:  0.5457162261009216
train gradient:  0.16907164653281856
iteration : 7001
train acc:  0.671875
train loss:  0.5962914228439331
train gradient:  0.20258516728623843
iteration : 7002
train acc:  0.6640625
train loss:  0.5612207055091858
train gradient:  0.19359958365056157
iteration : 7003
train acc:  0.703125
train loss:  0.5155566930770874
train gradient:  0.12153399071766897
iteration : 7004
train acc:  0.78125
train loss:  0.4765796363353729
train gradient:  0.12451102621269251
iteration : 7005
train acc:  0.71875
train loss:  0.5516543388366699
train gradient:  0.14607143630286884
iteration : 7006
train acc:  0.734375
train loss:  0.4577493965625763
train gradient:  0.1342828542227001
iteration : 7007
train acc:  0.71875
train loss:  0.545322597026825
train gradient:  0.16917864266482602
iteration : 7008
train acc:  0.7734375
train loss:  0.4550562798976898
train gradient:  0.10514163284173778
iteration : 7009
train acc:  0.6796875
train loss:  0.5699618458747864
train gradient:  0.18232282449310416
iteration : 7010
train acc:  0.796875
train loss:  0.47400200366973877
train gradient:  0.1457430422160442
iteration : 7011
train acc:  0.7265625
train loss:  0.46165961027145386
train gradient:  0.10167152693315795
iteration : 7012
train acc:  0.7890625
train loss:  0.43933022022247314
train gradient:  0.10115377644438746
iteration : 7013
train acc:  0.75
train loss:  0.43840736150741577
train gradient:  0.11481117566950086
iteration : 7014
train acc:  0.7578125
train loss:  0.46063506603240967
train gradient:  0.11732647358610476
iteration : 7015
train acc:  0.6796875
train loss:  0.5544442534446716
train gradient:  0.1477169385198116
iteration : 7016
train acc:  0.765625
train loss:  0.4740300178527832
train gradient:  0.11968536342275528
iteration : 7017
train acc:  0.796875
train loss:  0.5348412394523621
train gradient:  0.15222684815860354
iteration : 7018
train acc:  0.734375
train loss:  0.49129387736320496
train gradient:  0.17892895968865472
iteration : 7019
train acc:  0.703125
train loss:  0.5541475415229797
train gradient:  0.15068206136042056
iteration : 7020
train acc:  0.6953125
train loss:  0.5766555070877075
train gradient:  0.19828923920766733
iteration : 7021
train acc:  0.71875
train loss:  0.5623762011528015
train gradient:  0.1526234571652722
iteration : 7022
train acc:  0.7421875
train loss:  0.48287275433540344
train gradient:  0.10357844820069469
iteration : 7023
train acc:  0.7265625
train loss:  0.5342161655426025
train gradient:  0.1390274674989776
iteration : 7024
train acc:  0.7265625
train loss:  0.5283632278442383
train gradient:  0.16354822763639676
iteration : 7025
train acc:  0.671875
train loss:  0.5495326519012451
train gradient:  0.1761231157195729
iteration : 7026
train acc:  0.734375
train loss:  0.4956623315811157
train gradient:  0.1450272541241539
iteration : 7027
train acc:  0.8046875
train loss:  0.4307909607887268
train gradient:  0.12153224465046825
iteration : 7028
train acc:  0.734375
train loss:  0.4977495074272156
train gradient:  0.13197144833438246
iteration : 7029
train acc:  0.734375
train loss:  0.5031121373176575
train gradient:  0.1542991172546459
iteration : 7030
train acc:  0.6328125
train loss:  0.6489619016647339
train gradient:  0.16062679756891493
iteration : 7031
train acc:  0.78125
train loss:  0.4716449975967407
train gradient:  0.12524819210378849
iteration : 7032
train acc:  0.8125
train loss:  0.43287527561187744
train gradient:  0.12926100554787742
iteration : 7033
train acc:  0.7421875
train loss:  0.5058026909828186
train gradient:  0.11061714474041973
iteration : 7034
train acc:  0.7421875
train loss:  0.5059680938720703
train gradient:  0.11330628985434632
iteration : 7035
train acc:  0.703125
train loss:  0.4868531823158264
train gradient:  0.13955969678631547
iteration : 7036
train acc:  0.7578125
train loss:  0.469045490026474
train gradient:  0.10442681949320906
iteration : 7037
train acc:  0.71875
train loss:  0.5141744613647461
train gradient:  0.1613997801174557
iteration : 7038
train acc:  0.7109375
train loss:  0.5016645193099976
train gradient:  0.118624547226748
iteration : 7039
train acc:  0.6953125
train loss:  0.5525118112564087
train gradient:  0.19342262324602738
iteration : 7040
train acc:  0.7890625
train loss:  0.5003566741943359
train gradient:  0.128066281202037
iteration : 7041
train acc:  0.7421875
train loss:  0.49659550189971924
train gradient:  0.12392530142614651
iteration : 7042
train acc:  0.7265625
train loss:  0.46927082538604736
train gradient:  0.1560863037184371
iteration : 7043
train acc:  0.734375
train loss:  0.511044979095459
train gradient:  0.13224764981795228
iteration : 7044
train acc:  0.6484375
train loss:  0.5790109634399414
train gradient:  0.16380277988459724
iteration : 7045
train acc:  0.7734375
train loss:  0.52040034532547
train gradient:  0.13931360052975972
iteration : 7046
train acc:  0.7890625
train loss:  0.49420085549354553
train gradient:  0.12438481928105202
iteration : 7047
train acc:  0.734375
train loss:  0.4865233898162842
train gradient:  0.10501218987234798
iteration : 7048
train acc:  0.765625
train loss:  0.48205241560935974
train gradient:  0.12756252626854003
iteration : 7049
train acc:  0.8203125
train loss:  0.4207041263580322
train gradient:  0.09816038569624223
iteration : 7050
train acc:  0.7109375
train loss:  0.5201904773712158
train gradient:  0.16435148436288716
iteration : 7051
train acc:  0.703125
train loss:  0.5573577284812927
train gradient:  0.16944419016275425
iteration : 7052
train acc:  0.65625
train loss:  0.5401525497436523
train gradient:  0.1836216698989918
iteration : 7053
train acc:  0.78125
train loss:  0.48939236998558044
train gradient:  0.13779863064309242
iteration : 7054
train acc:  0.6640625
train loss:  0.5877736806869507
train gradient:  0.17117056782929452
iteration : 7055
train acc:  0.7734375
train loss:  0.5045039653778076
train gradient:  0.13496160373135205
iteration : 7056
train acc:  0.6953125
train loss:  0.5663437843322754
train gradient:  0.1855487615011668
iteration : 7057
train acc:  0.71875
train loss:  0.49874642491340637
train gradient:  0.1159430155464043
iteration : 7058
train acc:  0.6640625
train loss:  0.5316699147224426
train gradient:  0.1368707032337803
iteration : 7059
train acc:  0.671875
train loss:  0.542069673538208
train gradient:  0.14757408747428652
iteration : 7060
train acc:  0.734375
train loss:  0.478259414434433
train gradient:  0.10230006509289975
iteration : 7061
train acc:  0.7890625
train loss:  0.433976411819458
train gradient:  0.12779210027401014
iteration : 7062
train acc:  0.7421875
train loss:  0.4886057376861572
train gradient:  0.12847038326628613
iteration : 7063
train acc:  0.765625
train loss:  0.46440115571022034
train gradient:  0.15534643113981372
iteration : 7064
train acc:  0.7265625
train loss:  0.4854341745376587
train gradient:  0.1532890553505765
iteration : 7065
train acc:  0.7109375
train loss:  0.5031349658966064
train gradient:  0.1235919255953353
iteration : 7066
train acc:  0.7578125
train loss:  0.4679281711578369
train gradient:  0.12252965046534094
iteration : 7067
train acc:  0.765625
train loss:  0.4808436930179596
train gradient:  0.1303251913740878
iteration : 7068
train acc:  0.6796875
train loss:  0.5820427536964417
train gradient:  0.21298951284872408
iteration : 7069
train acc:  0.765625
train loss:  0.49147462844848633
train gradient:  0.12314676621641991
iteration : 7070
train acc:  0.734375
train loss:  0.5270252227783203
train gradient:  0.17110669026080888
iteration : 7071
train acc:  0.6796875
train loss:  0.5639775395393372
train gradient:  0.168937376971517
iteration : 7072
train acc:  0.7578125
train loss:  0.4373263716697693
train gradient:  0.11844589297911866
iteration : 7073
train acc:  0.6953125
train loss:  0.5588486194610596
train gradient:  0.1838001707168096
iteration : 7074
train acc:  0.7578125
train loss:  0.4599003791809082
train gradient:  0.11861615231515704
iteration : 7075
train acc:  0.7578125
train loss:  0.5036783814430237
train gradient:  0.13263465004626224
iteration : 7076
train acc:  0.8125
train loss:  0.4841727614402771
train gradient:  0.14057480230544125
iteration : 7077
train acc:  0.6953125
train loss:  0.5093101859092712
train gradient:  0.13323988899818345
iteration : 7078
train acc:  0.703125
train loss:  0.5566932559013367
train gradient:  0.13528770580840588
iteration : 7079
train acc:  0.734375
train loss:  0.49048471450805664
train gradient:  0.13228888419621454
iteration : 7080
train acc:  0.7265625
train loss:  0.4951493740081787
train gradient:  0.15840366959366597
iteration : 7081
train acc:  0.7421875
train loss:  0.6006553173065186
train gradient:  0.19233364820263793
iteration : 7082
train acc:  0.8125
train loss:  0.4864974915981293
train gradient:  0.11505119012995499
iteration : 7083
train acc:  0.7265625
train loss:  0.5309674739837646
train gradient:  0.12311115529764516
iteration : 7084
train acc:  0.78125
train loss:  0.4931241273880005
train gradient:  0.14230023048454496
iteration : 7085
train acc:  0.7109375
train loss:  0.5326234698295593
train gradient:  0.16255903629943355
iteration : 7086
train acc:  0.7265625
train loss:  0.5238655805587769
train gradient:  0.14020093652936474
iteration : 7087
train acc:  0.703125
train loss:  0.5143564343452454
train gradient:  0.1525006430636749
iteration : 7088
train acc:  0.7421875
train loss:  0.45880910754203796
train gradient:  0.10931057176946268
iteration : 7089
train acc:  0.7109375
train loss:  0.5186538100242615
train gradient:  0.18441530294046482
iteration : 7090
train acc:  0.7265625
train loss:  0.5365058779716492
train gradient:  0.14968385010813323
iteration : 7091
train acc:  0.7265625
train loss:  0.4899615943431854
train gradient:  0.11825957973665835
iteration : 7092
train acc:  0.78125
train loss:  0.431647926568985
train gradient:  0.10613753501010216
iteration : 7093
train acc:  0.7734375
train loss:  0.4657558500766754
train gradient:  0.12606688990228523
iteration : 7094
train acc:  0.7265625
train loss:  0.4960339367389679
train gradient:  0.12335613723092194
iteration : 7095
train acc:  0.7421875
train loss:  0.5031642913818359
train gradient:  0.11995572122171645
iteration : 7096
train acc:  0.7734375
train loss:  0.4606377184391022
train gradient:  0.09254140737488786
iteration : 7097
train acc:  0.8125
train loss:  0.44580820202827454
train gradient:  0.17848523644575742
iteration : 7098
train acc:  0.6875
train loss:  0.525273323059082
train gradient:  0.16856240990281363
iteration : 7099
train acc:  0.7421875
train loss:  0.5143570899963379
train gradient:  0.1563431178761155
iteration : 7100
train acc:  0.7578125
train loss:  0.5021011829376221
train gradient:  0.11331659834893425
iteration : 7101
train acc:  0.734375
train loss:  0.510196328163147
train gradient:  0.13520739705860232
iteration : 7102
train acc:  0.6953125
train loss:  0.5283253788948059
train gradient:  0.13392289134632857
iteration : 7103
train acc:  0.6640625
train loss:  0.5608993172645569
train gradient:  0.18127944051731903
iteration : 7104
train acc:  0.6953125
train loss:  0.5553756952285767
train gradient:  0.20022633248670318
iteration : 7105
train acc:  0.71875
train loss:  0.5523495078086853
train gradient:  0.1360485050680755
iteration : 7106
train acc:  0.796875
train loss:  0.4607761800289154
train gradient:  0.10784601928450098
iteration : 7107
train acc:  0.7578125
train loss:  0.49084189534187317
train gradient:  0.14200430390297197
iteration : 7108
train acc:  0.7890625
train loss:  0.4567253589630127
train gradient:  0.12593756565331835
iteration : 7109
train acc:  0.71875
train loss:  0.4980103671550751
train gradient:  0.15559000265902573
iteration : 7110
train acc:  0.734375
train loss:  0.5221604108810425
train gradient:  0.15715132115180286
iteration : 7111
train acc:  0.6953125
train loss:  0.5808945894241333
train gradient:  0.16520149564832454
iteration : 7112
train acc:  0.7421875
train loss:  0.47880473732948303
train gradient:  0.15507634517351027
iteration : 7113
train acc:  0.703125
train loss:  0.5220078229904175
train gradient:  0.14100070578892188
iteration : 7114
train acc:  0.71875
train loss:  0.4918650984764099
train gradient:  0.1201398043221779
iteration : 7115
train acc:  0.6875
train loss:  0.5591223835945129
train gradient:  0.14328132502807633
iteration : 7116
train acc:  0.71875
train loss:  0.5320868492126465
train gradient:  0.1497801968143881
iteration : 7117
train acc:  0.7890625
train loss:  0.4864295721054077
train gradient:  0.10966311928938077
iteration : 7118
train acc:  0.7578125
train loss:  0.4882541596889496
train gradient:  0.11913927093098507
iteration : 7119
train acc:  0.7578125
train loss:  0.4652852714061737
train gradient:  0.12203641731530773
iteration : 7120
train acc:  0.75
train loss:  0.5126531720161438
train gradient:  0.20492761790577685
iteration : 7121
train acc:  0.734375
train loss:  0.48841798305511475
train gradient:  0.12721124594902003
iteration : 7122
train acc:  0.75
train loss:  0.49271756410598755
train gradient:  0.12921360904718016
iteration : 7123
train acc:  0.703125
train loss:  0.5695521235466003
train gradient:  0.1736748167571844
iteration : 7124
train acc:  0.7109375
train loss:  0.5041241645812988
train gradient:  0.11858586237845704
iteration : 7125
train acc:  0.75
train loss:  0.49540507793426514
train gradient:  0.14982369485157182
iteration : 7126
train acc:  0.7265625
train loss:  0.4845942556858063
train gradient:  0.10798202395755392
iteration : 7127
train acc:  0.7890625
train loss:  0.46039673686027527
train gradient:  0.14923733146541407
iteration : 7128
train acc:  0.75
train loss:  0.570702314376831
train gradient:  0.17275109101420166
iteration : 7129
train acc:  0.7578125
train loss:  0.4963679313659668
train gradient:  0.16526425722563565
iteration : 7130
train acc:  0.65625
train loss:  0.5689519643783569
train gradient:  0.17456947965505787
iteration : 7131
train acc:  0.7734375
train loss:  0.5014543533325195
train gradient:  0.16583258578959234
iteration : 7132
train acc:  0.734375
train loss:  0.5197343826293945
train gradient:  0.12325028901672942
iteration : 7133
train acc:  0.6953125
train loss:  0.5063180923461914
train gradient:  0.13152991201141745
iteration : 7134
train acc:  0.7109375
train loss:  0.5688409209251404
train gradient:  0.18779146283086018
iteration : 7135
train acc:  0.71875
train loss:  0.5305162072181702
train gradient:  0.15742555202470834
iteration : 7136
train acc:  0.7265625
train loss:  0.5224097967147827
train gradient:  0.1390458554041587
iteration : 7137
train acc:  0.765625
train loss:  0.49683600664138794
train gradient:  0.09938092151908096
iteration : 7138
train acc:  0.7734375
train loss:  0.44678840041160583
train gradient:  0.16112717143049352
iteration : 7139
train acc:  0.78125
train loss:  0.46459144353866577
train gradient:  0.13702572891696757
iteration : 7140
train acc:  0.7578125
train loss:  0.4468011260032654
train gradient:  0.10597936278659706
iteration : 7141
train acc:  0.71875
train loss:  0.5606987476348877
train gradient:  0.16414947772499935
iteration : 7142
train acc:  0.7578125
train loss:  0.49124327301979065
train gradient:  0.1250251912755821
iteration : 7143
train acc:  0.703125
train loss:  0.5311376452445984
train gradient:  0.15155448354875878
iteration : 7144
train acc:  0.7265625
train loss:  0.5400946140289307
train gradient:  0.19749485840766584
iteration : 7145
train acc:  0.7578125
train loss:  0.4420304298400879
train gradient:  0.12228047768903418
iteration : 7146
train acc:  0.78125
train loss:  0.4229077100753784
train gradient:  0.09518832491749253
iteration : 7147
train acc:  0.6640625
train loss:  0.5411825180053711
train gradient:  0.1420137927053009
iteration : 7148
train acc:  0.7734375
train loss:  0.49298301339149475
train gradient:  0.1262991159877508
iteration : 7149
train acc:  0.7421875
train loss:  0.5063971877098083
train gradient:  0.2090603455265494
iteration : 7150
train acc:  0.6171875
train loss:  0.6386431455612183
train gradient:  0.21382569355451775
iteration : 7151
train acc:  0.796875
train loss:  0.43488815426826477
train gradient:  0.09117815513159047
iteration : 7152
train acc:  0.7421875
train loss:  0.5164421796798706
train gradient:  0.12351955155685355
iteration : 7153
train acc:  0.765625
train loss:  0.479117751121521
train gradient:  0.11870936102764935
iteration : 7154
train acc:  0.65625
train loss:  0.6094211339950562
train gradient:  0.18272441216881347
iteration : 7155
train acc:  0.7578125
train loss:  0.48996835947036743
train gradient:  0.13652119427018053
iteration : 7156
train acc:  0.75
train loss:  0.5271022915840149
train gradient:  0.14873066154271047
iteration : 7157
train acc:  0.6328125
train loss:  0.6076239347457886
train gradient:  0.204610628677885
iteration : 7158
train acc:  0.6953125
train loss:  0.5275387763977051
train gradient:  0.15351252393137527
iteration : 7159
train acc:  0.796875
train loss:  0.5020617842674255
train gradient:  0.16723866298721388
iteration : 7160
train acc:  0.6875
train loss:  0.5316758751869202
train gradient:  0.16636303665778285
iteration : 7161
train acc:  0.671875
train loss:  0.6147651076316833
train gradient:  0.22735623675150768
iteration : 7162
train acc:  0.75
train loss:  0.49295809864997864
train gradient:  0.1376273746096094
iteration : 7163
train acc:  0.71875
train loss:  0.5062556266784668
train gradient:  0.15269400516944717
iteration : 7164
train acc:  0.7421875
train loss:  0.47638893127441406
train gradient:  0.10552961341349955
iteration : 7165
train acc:  0.7734375
train loss:  0.5149292945861816
train gradient:  0.14482987252760313
iteration : 7166
train acc:  0.71875
train loss:  0.5134769082069397
train gradient:  0.14737110911128942
iteration : 7167
train acc:  0.7421875
train loss:  0.504697322845459
train gradient:  0.1147741893983504
iteration : 7168
train acc:  0.7265625
train loss:  0.510356068611145
train gradient:  0.1329069381451537
iteration : 7169
train acc:  0.71875
train loss:  0.4653371572494507
train gradient:  0.10965832413625821
iteration : 7170
train acc:  0.671875
train loss:  0.5778164863586426
train gradient:  0.20673656363329188
iteration : 7171
train acc:  0.7421875
train loss:  0.5013853311538696
train gradient:  0.14837520507515398
iteration : 7172
train acc:  0.7265625
train loss:  0.5081719160079956
train gradient:  0.1534124052587979
iteration : 7173
train acc:  0.734375
train loss:  0.4901025891304016
train gradient:  0.12848536408753858
iteration : 7174
train acc:  0.7578125
train loss:  0.49355608224868774
train gradient:  0.1301472559804452
iteration : 7175
train acc:  0.75
train loss:  0.46736985445022583
train gradient:  0.11853188854177256
iteration : 7176
train acc:  0.7578125
train loss:  0.4788515567779541
train gradient:  0.10738367431639333
iteration : 7177
train acc:  0.71875
train loss:  0.5315316915512085
train gradient:  0.1208522379147851
iteration : 7178
train acc:  0.734375
train loss:  0.5028135776519775
train gradient:  0.1495869812619216
iteration : 7179
train acc:  0.8046875
train loss:  0.4150606393814087
train gradient:  0.1032282217834806
iteration : 7180
train acc:  0.765625
train loss:  0.49899202585220337
train gradient:  0.18717514888191217
iteration : 7181
train acc:  0.7265625
train loss:  0.5115581750869751
train gradient:  0.11846367363084034
iteration : 7182
train acc:  0.765625
train loss:  0.47321271896362305
train gradient:  0.1077006177582291
iteration : 7183
train acc:  0.8359375
train loss:  0.4104410409927368
train gradient:  0.09825586975951757
iteration : 7184
train acc:  0.734375
train loss:  0.4991338551044464
train gradient:  0.13286419055024512
iteration : 7185
train acc:  0.703125
train loss:  0.5213872194290161
train gradient:  0.1420798876924788
iteration : 7186
train acc:  0.7109375
train loss:  0.4960708022117615
train gradient:  0.13550490810850838
iteration : 7187
train acc:  0.7734375
train loss:  0.4503456950187683
train gradient:  0.10033936502955985
iteration : 7188
train acc:  0.8046875
train loss:  0.4427570104598999
train gradient:  0.1029574487433488
iteration : 7189
train acc:  0.6953125
train loss:  0.5377331376075745
train gradient:  0.17400677914256396
iteration : 7190
train acc:  0.7421875
train loss:  0.5506362915039062
train gradient:  0.15452067104393669
iteration : 7191
train acc:  0.7734375
train loss:  0.48930782079696655
train gradient:  0.13179808919301744
iteration : 7192
train acc:  0.609375
train loss:  0.6132296323776245
train gradient:  0.1824850150984138
iteration : 7193
train acc:  0.71875
train loss:  0.5292588472366333
train gradient:  0.15865740360903124
iteration : 7194
train acc:  0.734375
train loss:  0.5330222845077515
train gradient:  0.13451375541143015
iteration : 7195
train acc:  0.6796875
train loss:  0.5282577276229858
train gradient:  0.14503329233145526
iteration : 7196
train acc:  0.7421875
train loss:  0.4920055866241455
train gradient:  0.11472284785830218
iteration : 7197
train acc:  0.7265625
train loss:  0.5382558107376099
train gradient:  0.1674978316699155
iteration : 7198
train acc:  0.7734375
train loss:  0.5067264437675476
train gradient:  0.14418159498046876
iteration : 7199
train acc:  0.7421875
train loss:  0.5713273286819458
train gradient:  0.14143355413328737
iteration : 7200
train acc:  0.734375
train loss:  0.53651362657547
train gradient:  0.13684611717278072
iteration : 7201
train acc:  0.7265625
train loss:  0.5158692598342896
train gradient:  0.16338144756251935
iteration : 7202
train acc:  0.7421875
train loss:  0.475318968296051
train gradient:  0.11596124449980924
iteration : 7203
train acc:  0.734375
train loss:  0.5308446288108826
train gradient:  0.12806875906775833
iteration : 7204
train acc:  0.78125
train loss:  0.4572218060493469
train gradient:  0.12590479726633313
iteration : 7205
train acc:  0.7265625
train loss:  0.5034016370773315
train gradient:  0.13043321421704596
iteration : 7206
train acc:  0.7578125
train loss:  0.4414854645729065
train gradient:  0.10554451311044943
iteration : 7207
train acc:  0.703125
train loss:  0.5251729488372803
train gradient:  0.1550598827719164
iteration : 7208
train acc:  0.7421875
train loss:  0.48141396045684814
train gradient:  0.13646132233918523
iteration : 7209
train acc:  0.734375
train loss:  0.4946284890174866
train gradient:  0.13746201028694313
iteration : 7210
train acc:  0.71875
train loss:  0.5301986932754517
train gradient:  0.1332531257198618
iteration : 7211
train acc:  0.6796875
train loss:  0.5615108013153076
train gradient:  0.17079278886422286
iteration : 7212
train acc:  0.7109375
train loss:  0.501270055770874
train gradient:  0.18361713513918276
iteration : 7213
train acc:  0.6796875
train loss:  0.577621340751648
train gradient:  0.1841726558763903
iteration : 7214
train acc:  0.75
train loss:  0.5129497051239014
train gradient:  0.11839700050187787
iteration : 7215
train acc:  0.7265625
train loss:  0.48776593804359436
train gradient:  0.15794319874637353
iteration : 7216
train acc:  0.6953125
train loss:  0.5499072670936584
train gradient:  0.1416911649953759
iteration : 7217
train acc:  0.78125
train loss:  0.49267858266830444
train gradient:  0.14087342932107494
iteration : 7218
train acc:  0.7265625
train loss:  0.46603840589523315
train gradient:  0.10578438515238377
iteration : 7219
train acc:  0.7734375
train loss:  0.4647049903869629
train gradient:  0.10388060211764795
iteration : 7220
train acc:  0.8125
train loss:  0.438325434923172
train gradient:  0.10300830279726739
iteration : 7221
train acc:  0.765625
train loss:  0.5059571266174316
train gradient:  0.14222609583331927
iteration : 7222
train acc:  0.7421875
train loss:  0.4921848773956299
train gradient:  0.1514156617148597
iteration : 7223
train acc:  0.765625
train loss:  0.43827828764915466
train gradient:  0.1348579336472222
iteration : 7224
train acc:  0.7578125
train loss:  0.44837817549705505
train gradient:  0.10420486942142908
iteration : 7225
train acc:  0.765625
train loss:  0.47869163751602173
train gradient:  0.11343624030359402
iteration : 7226
train acc:  0.7890625
train loss:  0.4390087127685547
train gradient:  0.11975241006414322
iteration : 7227
train acc:  0.7578125
train loss:  0.4661800265312195
train gradient:  0.14388316610787336
iteration : 7228
train acc:  0.7265625
train loss:  0.5039429664611816
train gradient:  0.11849192107295353
iteration : 7229
train acc:  0.765625
train loss:  0.4830628037452698
train gradient:  0.1386226757486754
iteration : 7230
train acc:  0.71875
train loss:  0.5037798881530762
train gradient:  0.174213037976432
iteration : 7231
train acc:  0.75
train loss:  0.5207915306091309
train gradient:  0.22836690467264742
iteration : 7232
train acc:  0.78125
train loss:  0.46515053510665894
train gradient:  0.11392664181363209
iteration : 7233
train acc:  0.7421875
train loss:  0.46688637137413025
train gradient:  0.1023365549560301
iteration : 7234
train acc:  0.734375
train loss:  0.576345682144165
train gradient:  0.17334822143915013
iteration : 7235
train acc:  0.8046875
train loss:  0.4481772780418396
train gradient:  0.10113446638877702
iteration : 7236
train acc:  0.7734375
train loss:  0.4565172493457794
train gradient:  0.11102480501896315
iteration : 7237
train acc:  0.7578125
train loss:  0.46644291281700134
train gradient:  0.11361858618106233
iteration : 7238
train acc:  0.703125
train loss:  0.5327697992324829
train gradient:  0.1566477088641931
iteration : 7239
train acc:  0.75
train loss:  0.48955032229423523
train gradient:  0.13366501243138224
iteration : 7240
train acc:  0.7734375
train loss:  0.48492103815078735
train gradient:  0.12431822502503052
iteration : 7241
train acc:  0.7890625
train loss:  0.4884164333343506
train gradient:  0.12259836766820568
iteration : 7242
train acc:  0.71875
train loss:  0.5214954018592834
train gradient:  0.1371609650166894
iteration : 7243
train acc:  0.7265625
train loss:  0.5216131210327148
train gradient:  0.11640691923848928
iteration : 7244
train acc:  0.734375
train loss:  0.5655465126037598
train gradient:  0.1603674056688726
iteration : 7245
train acc:  0.640625
train loss:  0.6159704923629761
train gradient:  0.21494513439610352
iteration : 7246
train acc:  0.703125
train loss:  0.5059982538223267
train gradient:  0.13114092768197338
iteration : 7247
train acc:  0.765625
train loss:  0.48882362246513367
train gradient:  0.14504923022048738
iteration : 7248
train acc:  0.71875
train loss:  0.5014262795448303
train gradient:  0.1277511653552043
iteration : 7249
train acc:  0.796875
train loss:  0.4403202533721924
train gradient:  0.12214456281180752
iteration : 7250
train acc:  0.75
train loss:  0.479582816362381
train gradient:  0.14599832273321078
iteration : 7251
train acc:  0.75
train loss:  0.49478763341903687
train gradient:  0.12491320565042138
iteration : 7252
train acc:  0.65625
train loss:  0.6059964895248413
train gradient:  0.23313636497656345
iteration : 7253
train acc:  0.7421875
train loss:  0.4995102286338806
train gradient:  0.1439020057317524
iteration : 7254
train acc:  0.7421875
train loss:  0.5188395977020264
train gradient:  0.15680232005373623
iteration : 7255
train acc:  0.7578125
train loss:  0.46910950541496277
train gradient:  0.11271517063729543
iteration : 7256
train acc:  0.6796875
train loss:  0.5693478584289551
train gradient:  0.17872643048833323
iteration : 7257
train acc:  0.765625
train loss:  0.5008193850517273
train gradient:  0.16234901855328288
iteration : 7258
train acc:  0.71875
train loss:  0.5033504962921143
train gradient:  0.10269559926764796
iteration : 7259
train acc:  0.671875
train loss:  0.5737593770027161
train gradient:  0.21128015772389785
iteration : 7260
train acc:  0.734375
train loss:  0.4946989417076111
train gradient:  0.12343554056525188
iteration : 7261
train acc:  0.703125
train loss:  0.5275965929031372
train gradient:  0.11712086354255634
iteration : 7262
train acc:  0.75
train loss:  0.5427087545394897
train gradient:  0.15699402997379264
iteration : 7263
train acc:  0.734375
train loss:  0.5243517756462097
train gradient:  0.16891817193066572
iteration : 7264
train acc:  0.796875
train loss:  0.44260507822036743
train gradient:  0.13510318213692396
iteration : 7265
train acc:  0.734375
train loss:  0.5014532804489136
train gradient:  0.1830133366525059
iteration : 7266
train acc:  0.7265625
train loss:  0.49363547563552856
train gradient:  0.13090591537732024
iteration : 7267
train acc:  0.6796875
train loss:  0.5375716686248779
train gradient:  0.17355877028066513
iteration : 7268
train acc:  0.703125
train loss:  0.5459626913070679
train gradient:  0.149606050885433
iteration : 7269
train acc:  0.6796875
train loss:  0.5421173572540283
train gradient:  0.16367550242660797
iteration : 7270
train acc:  0.7734375
train loss:  0.49747204780578613
train gradient:  0.14758827850164835
iteration : 7271
train acc:  0.75
train loss:  0.46497640013694763
train gradient:  0.11690614946132147
iteration : 7272
train acc:  0.7578125
train loss:  0.46785181760787964
train gradient:  0.12881731647824013
iteration : 7273
train acc:  0.6875
train loss:  0.5091819763183594
train gradient:  0.14697781746704897
iteration : 7274
train acc:  0.7890625
train loss:  0.515825092792511
train gradient:  0.17769483535464253
iteration : 7275
train acc:  0.78125
train loss:  0.5056840181350708
train gradient:  0.11598056940801237
iteration : 7276
train acc:  0.8046875
train loss:  0.49773499369621277
train gradient:  0.14209546854586086
iteration : 7277
train acc:  0.7578125
train loss:  0.48287659883499146
train gradient:  0.15122886804089858
iteration : 7278
train acc:  0.75
train loss:  0.4649813175201416
train gradient:  0.10799764614694221
iteration : 7279
train acc:  0.71875
train loss:  0.4850741922855377
train gradient:  0.12930853631001243
iteration : 7280
train acc:  0.8046875
train loss:  0.441501259803772
train gradient:  0.13208111474962833
iteration : 7281
train acc:  0.78125
train loss:  0.45992225408554077
train gradient:  0.11652989099713369
iteration : 7282
train acc:  0.6875
train loss:  0.5024930238723755
train gradient:  0.13353996028178175
iteration : 7283
train acc:  0.6796875
train loss:  0.5657559633255005
train gradient:  0.15064153956691562
iteration : 7284
train acc:  0.7265625
train loss:  0.5188286304473877
train gradient:  0.1714433999619969
iteration : 7285
train acc:  0.7578125
train loss:  0.5371717810630798
train gradient:  0.172243430656216
iteration : 7286
train acc:  0.7734375
train loss:  0.5105066299438477
train gradient:  0.13113746839083953
iteration : 7287
train acc:  0.75
train loss:  0.49250635504722595
train gradient:  0.12033668729937184
iteration : 7288
train acc:  0.75
train loss:  0.4768890142440796
train gradient:  0.1317732514184204
iteration : 7289
train acc:  0.78125
train loss:  0.4578635096549988
train gradient:  0.1167532445263798
iteration : 7290
train acc:  0.703125
train loss:  0.525772213935852
train gradient:  0.1340515022766125
iteration : 7291
train acc:  0.765625
train loss:  0.45092523097991943
train gradient:  0.0942392421238605
iteration : 7292
train acc:  0.7109375
train loss:  0.5046330690383911
train gradient:  0.17263951810814365
iteration : 7293
train acc:  0.7578125
train loss:  0.4708751440048218
train gradient:  0.13038243959827228
iteration : 7294
train acc:  0.8046875
train loss:  0.416326642036438
train gradient:  0.10077315618524203
iteration : 7295
train acc:  0.78125
train loss:  0.44648876786231995
train gradient:  0.13325339099224687
iteration : 7296
train acc:  0.7734375
train loss:  0.47072434425354004
train gradient:  0.14411263596926405
iteration : 7297
train acc:  0.796875
train loss:  0.4322802424430847
train gradient:  0.12368566606163837
iteration : 7298
train acc:  0.703125
train loss:  0.6184077262878418
train gradient:  0.20123854705824673
iteration : 7299
train acc:  0.734375
train loss:  0.4696488082408905
train gradient:  0.1730957755580943
iteration : 7300
train acc:  0.640625
train loss:  0.5483286380767822
train gradient:  0.1409299997129136
iteration : 7301
train acc:  0.75
train loss:  0.5136119723320007
train gradient:  0.13085123518602912
iteration : 7302
train acc:  0.71875
train loss:  0.5558028221130371
train gradient:  0.17073576066457063
iteration : 7303
train acc:  0.71875
train loss:  0.5119760036468506
train gradient:  0.11390823364443756
iteration : 7304
train acc:  0.6875
train loss:  0.5123907327651978
train gradient:  0.1333247861711601
iteration : 7305
train acc:  0.765625
train loss:  0.4988030195236206
train gradient:  0.15609172587951686
iteration : 7306
train acc:  0.7109375
train loss:  0.5626204013824463
train gradient:  0.18767131686610117
iteration : 7307
train acc:  0.78125
train loss:  0.451460063457489
train gradient:  0.1360756910947546
iteration : 7308
train acc:  0.6796875
train loss:  0.5573891401290894
train gradient:  0.2261183478170004
iteration : 7309
train acc:  0.765625
train loss:  0.4827175438404083
train gradient:  0.13587147263972188
iteration : 7310
train acc:  0.734375
train loss:  0.4823754131793976
train gradient:  0.11637048840275513
iteration : 7311
train acc:  0.71875
train loss:  0.5460976362228394
train gradient:  0.17384525057029226
iteration : 7312
train acc:  0.7578125
train loss:  0.485027551651001
train gradient:  0.1400553880073009
iteration : 7313
train acc:  0.6796875
train loss:  0.5531373023986816
train gradient:  0.12704780352958328
iteration : 7314
train acc:  0.7734375
train loss:  0.4753414988517761
train gradient:  0.12539680977970555
iteration : 7315
train acc:  0.6953125
train loss:  0.5568830370903015
train gradient:  0.14749609373816952
iteration : 7316
train acc:  0.7578125
train loss:  0.4371223747730255
train gradient:  0.09095447139641986
iteration : 7317
train acc:  0.765625
train loss:  0.517308235168457
train gradient:  0.13627169658379323
iteration : 7318
train acc:  0.7421875
train loss:  0.5096931457519531
train gradient:  0.1673306565022476
iteration : 7319
train acc:  0.796875
train loss:  0.4697726368904114
train gradient:  0.16501798584618574
iteration : 7320
train acc:  0.796875
train loss:  0.4290582537651062
train gradient:  0.0979003845004911
iteration : 7321
train acc:  0.7421875
train loss:  0.4747074246406555
train gradient:  0.12687296234845408
iteration : 7322
train acc:  0.6796875
train loss:  0.5625529289245605
train gradient:  0.1318335393650927
iteration : 7323
train acc:  0.6953125
train loss:  0.5408585071563721
train gradient:  0.16714246880173578
iteration : 7324
train acc:  0.7578125
train loss:  0.5014176368713379
train gradient:  0.1599311073978137
iteration : 7325
train acc:  0.7109375
train loss:  0.546809732913971
train gradient:  0.15762473795862397
iteration : 7326
train acc:  0.734375
train loss:  0.4960736632347107
train gradient:  0.1398738197253785
iteration : 7327
train acc:  0.7578125
train loss:  0.4799758195877075
train gradient:  0.11446114505727505
iteration : 7328
train acc:  0.75
train loss:  0.5049949884414673
train gradient:  0.1643986585269681
iteration : 7329
train acc:  0.7421875
train loss:  0.5144856572151184
train gradient:  0.12337283587308079
iteration : 7330
train acc:  0.796875
train loss:  0.42378467321395874
train gradient:  0.09802296107877766
iteration : 7331
train acc:  0.734375
train loss:  0.5376338958740234
train gradient:  0.16787642734815605
iteration : 7332
train acc:  0.7578125
train loss:  0.501324474811554
train gradient:  0.14187613398209672
iteration : 7333
train acc:  0.734375
train loss:  0.5064679384231567
train gradient:  0.12138035767801797
iteration : 7334
train acc:  0.7265625
train loss:  0.5004681944847107
train gradient:  0.20455346003010594
iteration : 7335
train acc:  0.8046875
train loss:  0.45136529207229614
train gradient:  0.12156930760311434
iteration : 7336
train acc:  0.7578125
train loss:  0.4626915752887726
train gradient:  0.12661240778294947
iteration : 7337
train acc:  0.671875
train loss:  0.562583863735199
train gradient:  0.14262358310718837
iteration : 7338
train acc:  0.7109375
train loss:  0.5474781394004822
train gradient:  0.15444995981544485
iteration : 7339
train acc:  0.734375
train loss:  0.5202292799949646
train gradient:  0.16000109997054923
iteration : 7340
train acc:  0.703125
train loss:  0.5603629350662231
train gradient:  0.1615936694232067
iteration : 7341
train acc:  0.7734375
train loss:  0.4537200927734375
train gradient:  0.12446287286692402
iteration : 7342
train acc:  0.671875
train loss:  0.5466590523719788
train gradient:  0.15334389240963425
iteration : 7343
train acc:  0.7265625
train loss:  0.5749955177307129
train gradient:  0.17667346879743948
iteration : 7344
train acc:  0.6875
train loss:  0.5021342635154724
train gradient:  0.12010941562304597
iteration : 7345
train acc:  0.71875
train loss:  0.5039986371994019
train gradient:  0.11864179794557864
iteration : 7346
train acc:  0.703125
train loss:  0.5136952996253967
train gradient:  0.14373512217604978
iteration : 7347
train acc:  0.7265625
train loss:  0.5301482677459717
train gradient:  0.1460649896103961
iteration : 7348
train acc:  0.7734375
train loss:  0.47257301211357117
train gradient:  0.12371654711240623
iteration : 7349
train acc:  0.6953125
train loss:  0.5459673404693604
train gradient:  0.15457652125527283
iteration : 7350
train acc:  0.765625
train loss:  0.5178225040435791
train gradient:  0.11911742081334432
iteration : 7351
train acc:  0.6796875
train loss:  0.6098454594612122
train gradient:  0.22797511391973013
iteration : 7352
train acc:  0.6640625
train loss:  0.5658018589019775
train gradient:  0.16656404854287063
iteration : 7353
train acc:  0.7578125
train loss:  0.4673062562942505
train gradient:  0.11271745084164457
iteration : 7354
train acc:  0.7265625
train loss:  0.5438215136528015
train gradient:  0.13503382796224905
iteration : 7355
train acc:  0.8046875
train loss:  0.4724377989768982
train gradient:  0.1367363091311342
iteration : 7356
train acc:  0.7265625
train loss:  0.5388997793197632
train gradient:  0.15827983699000248
iteration : 7357
train acc:  0.796875
train loss:  0.4147980511188507
train gradient:  0.10232220571518279
iteration : 7358
train acc:  0.640625
train loss:  0.5821916460990906
train gradient:  0.17056991740499627
iteration : 7359
train acc:  0.703125
train loss:  0.5608757734298706
train gradient:  0.1596772317426638
iteration : 7360
train acc:  0.7421875
train loss:  0.46776825189590454
train gradient:  0.14027896857817523
iteration : 7361
train acc:  0.75
train loss:  0.5219807624816895
train gradient:  0.20978475104542837
iteration : 7362
train acc:  0.6953125
train loss:  0.5502915382385254
train gradient:  0.15804023063032283
iteration : 7363
train acc:  0.8203125
train loss:  0.39925968647003174
train gradient:  0.105502654637687
iteration : 7364
train acc:  0.7421875
train loss:  0.4907105565071106
train gradient:  0.16912476616461092
iteration : 7365
train acc:  0.6796875
train loss:  0.5397576689720154
train gradient:  0.1437201948277368
iteration : 7366
train acc:  0.7421875
train loss:  0.5003932118415833
train gradient:  0.14063643171805743
iteration : 7367
train acc:  0.7578125
train loss:  0.5494757294654846
train gradient:  0.14933086741876953
iteration : 7368
train acc:  0.796875
train loss:  0.5297378301620483
train gradient:  0.16535738121623794
iteration : 7369
train acc:  0.765625
train loss:  0.527998685836792
train gradient:  0.16697125444180078
iteration : 7370
train acc:  0.6953125
train loss:  0.5280538201332092
train gradient:  0.16140810846761683
iteration : 7371
train acc:  0.7109375
train loss:  0.5323594808578491
train gradient:  0.1324933689555751
iteration : 7372
train acc:  0.7109375
train loss:  0.5315956473350525
train gradient:  0.17192517252644107
iteration : 7373
train acc:  0.7890625
train loss:  0.4249652028083801
train gradient:  0.10121955425600437
iteration : 7374
train acc:  0.71875
train loss:  0.5003311634063721
train gradient:  0.14250992700773785
iteration : 7375
train acc:  0.765625
train loss:  0.5178617238998413
train gradient:  0.1279093186785984
iteration : 7376
train acc:  0.703125
train loss:  0.535502552986145
train gradient:  0.14505801608412128
iteration : 7377
train acc:  0.71875
train loss:  0.49103251099586487
train gradient:  0.1263173632673383
iteration : 7378
train acc:  0.78125
train loss:  0.4134834110736847
train gradient:  0.11714090365850507
iteration : 7379
train acc:  0.75
train loss:  0.5010573863983154
train gradient:  0.16286829491971672
iteration : 7380
train acc:  0.703125
train loss:  0.5333871841430664
train gradient:  0.1933327644805058
iteration : 7381
train acc:  0.7265625
train loss:  0.48565682768821716
train gradient:  0.12732664425556334
iteration : 7382
train acc:  0.703125
train loss:  0.5071301460266113
train gradient:  0.13177031361863484
iteration : 7383
train acc:  0.71875
train loss:  0.5305202007293701
train gradient:  0.1426956994641051
iteration : 7384
train acc:  0.7421875
train loss:  0.5526762008666992
train gradient:  0.181077068672954
iteration : 7385
train acc:  0.75
train loss:  0.537888765335083
train gradient:  0.15796595933922408
iteration : 7386
train acc:  0.7421875
train loss:  0.5068410634994507
train gradient:  0.11393195178391177
iteration : 7387
train acc:  0.765625
train loss:  0.49493172764778137
train gradient:  0.14546156965307933
iteration : 7388
train acc:  0.6484375
train loss:  0.5876328349113464
train gradient:  0.15121780171388982
iteration : 7389
train acc:  0.734375
train loss:  0.48524153232574463
train gradient:  0.1313072401126673
iteration : 7390
train acc:  0.765625
train loss:  0.47892728447914124
train gradient:  0.1163997847081489
iteration : 7391
train acc:  0.7421875
train loss:  0.5585430264472961
train gradient:  0.15503133149463266
iteration : 7392
train acc:  0.7578125
train loss:  0.4928811192512512
train gradient:  0.15864732341343563
iteration : 7393
train acc:  0.6875
train loss:  0.55964195728302
train gradient:  0.1628036388717311
iteration : 7394
train acc:  0.6953125
train loss:  0.5650992393493652
train gradient:  0.20174489341369994
iteration : 7395
train acc:  0.7265625
train loss:  0.5515607595443726
train gradient:  0.1789544513201633
iteration : 7396
train acc:  0.7109375
train loss:  0.5339077115058899
train gradient:  0.1312748437534174
iteration : 7397
train acc:  0.7265625
train loss:  0.5170408487319946
train gradient:  0.16872135816450298
iteration : 7398
train acc:  0.7578125
train loss:  0.5090446472167969
train gradient:  0.15114118612479616
iteration : 7399
train acc:  0.796875
train loss:  0.46222081780433655
train gradient:  0.14236787478079987
iteration : 7400
train acc:  0.6875
train loss:  0.5236876606941223
train gradient:  0.12326228074491817
iteration : 7401
train acc:  0.7109375
train loss:  0.5068819522857666
train gradient:  0.1374873913774844
iteration : 7402
train acc:  0.75
train loss:  0.4621307849884033
train gradient:  0.09680818514439012
iteration : 7403
train acc:  0.8359375
train loss:  0.40058475732803345
train gradient:  0.10162043428604679
iteration : 7404
train acc:  0.7578125
train loss:  0.5022816061973572
train gradient:  0.11788774896999571
iteration : 7405
train acc:  0.734375
train loss:  0.5197705030441284
train gradient:  0.13303506755346223
iteration : 7406
train acc:  0.75
train loss:  0.5058113932609558
train gradient:  0.1525079651199119
iteration : 7407
train acc:  0.75
train loss:  0.4721525311470032
train gradient:  0.13048876211376426
iteration : 7408
train acc:  0.828125
train loss:  0.45081305503845215
train gradient:  0.11549916398315567
iteration : 7409
train acc:  0.609375
train loss:  0.6008145809173584
train gradient:  0.18050040059728106
iteration : 7410
train acc:  0.734375
train loss:  0.5336812734603882
train gradient:  0.1584719735503714
iteration : 7411
train acc:  0.6484375
train loss:  0.5811409950256348
train gradient:  0.18075849065324195
iteration : 7412
train acc:  0.734375
train loss:  0.493813157081604
train gradient:  0.16177111096425595
iteration : 7413
train acc:  0.7109375
train loss:  0.5286891460418701
train gradient:  0.15271577821879653
iteration : 7414
train acc:  0.7109375
train loss:  0.4873465895652771
train gradient:  0.12146059630183727
iteration : 7415
train acc:  0.75
train loss:  0.46779435873031616
train gradient:  0.1354043317502615
iteration : 7416
train acc:  0.7265625
train loss:  0.5652403831481934
train gradient:  0.16539502987848137
iteration : 7417
train acc:  0.796875
train loss:  0.4832630753517151
train gradient:  0.1028045232721242
iteration : 7418
train acc:  0.765625
train loss:  0.504132091999054
train gradient:  0.19530812064467545
iteration : 7419
train acc:  0.7421875
train loss:  0.5248531699180603
train gradient:  0.14263659765979847
iteration : 7420
train acc:  0.7578125
train loss:  0.48039090633392334
train gradient:  0.11504602493905404
iteration : 7421
train acc:  0.7734375
train loss:  0.4506821632385254
train gradient:  0.11555987066725595
iteration : 7422
train acc:  0.609375
train loss:  0.658421516418457
train gradient:  0.24963959151505738
iteration : 7423
train acc:  0.765625
train loss:  0.45138540863990784
train gradient:  0.09593728009020894
iteration : 7424
train acc:  0.703125
train loss:  0.5271066427230835
train gradient:  0.14868502510866377
iteration : 7425
train acc:  0.7890625
train loss:  0.4746067225933075
train gradient:  0.10536719705662455
iteration : 7426
train acc:  0.7578125
train loss:  0.5489281415939331
train gradient:  0.1632249906558131
iteration : 7427
train acc:  0.6875
train loss:  0.5606895685195923
train gradient:  0.17175958766735686
iteration : 7428
train acc:  0.671875
train loss:  0.5486708879470825
train gradient:  0.12894967008021802
iteration : 7429
train acc:  0.7109375
train loss:  0.5394128561019897
train gradient:  0.176130630479361
iteration : 7430
train acc:  0.7265625
train loss:  0.49684926867485046
train gradient:  0.11794067000393776
iteration : 7431
train acc:  0.78125
train loss:  0.4673369228839874
train gradient:  0.10429031101893733
iteration : 7432
train acc:  0.7734375
train loss:  0.4786044955253601
train gradient:  0.12678664224591057
iteration : 7433
train acc:  0.703125
train loss:  0.5627204179763794
train gradient:  0.1480714200007868
iteration : 7434
train acc:  0.7109375
train loss:  0.5421740412712097
train gradient:  0.15813726140304546
iteration : 7435
train acc:  0.7265625
train loss:  0.4954773783683777
train gradient:  0.13316358960335323
iteration : 7436
train acc:  0.7421875
train loss:  0.46133676171302795
train gradient:  0.11543271088216943
iteration : 7437
train acc:  0.75
train loss:  0.48294365406036377
train gradient:  0.12851771133779313
iteration : 7438
train acc:  0.71875
train loss:  0.5207338333129883
train gradient:  0.14028505988853102
iteration : 7439
train acc:  0.734375
train loss:  0.5138055086135864
train gradient:  0.10887514432569209
iteration : 7440
train acc:  0.7421875
train loss:  0.5087857246398926
train gradient:  0.1360843251233289
iteration : 7441
train acc:  0.6796875
train loss:  0.5329131484031677
train gradient:  0.13423292026296116
iteration : 7442
train acc:  0.7421875
train loss:  0.5656321048736572
train gradient:  0.16457160472721144
iteration : 7443
train acc:  0.8203125
train loss:  0.46221834421157837
train gradient:  0.12170320745953112
iteration : 7444
train acc:  0.75
train loss:  0.47112831473350525
train gradient:  0.12941145879769933
iteration : 7445
train acc:  0.7578125
train loss:  0.4653577208518982
train gradient:  0.0994246466876899
iteration : 7446
train acc:  0.7265625
train loss:  0.5114157199859619
train gradient:  0.15836876390778987
iteration : 7447
train acc:  0.71875
train loss:  0.5105170607566833
train gradient:  0.1416028487225463
iteration : 7448
train acc:  0.7890625
train loss:  0.4412621259689331
train gradient:  0.10783056212615451
iteration : 7449
train acc:  0.703125
train loss:  0.524953305721283
train gradient:  0.15163537051243672
iteration : 7450
train acc:  0.78125
train loss:  0.4408244490623474
train gradient:  0.10921182578316681
iteration : 7451
train acc:  0.734375
train loss:  0.5330352783203125
train gradient:  0.14268721831237716
iteration : 7452
train acc:  0.796875
train loss:  0.44921135902404785
train gradient:  0.09728006161333301
iteration : 7453
train acc:  0.765625
train loss:  0.48446616530418396
train gradient:  0.13834387619141256
iteration : 7454
train acc:  0.7734375
train loss:  0.48120495676994324
train gradient:  0.1127250119266782
iteration : 7455
train acc:  0.7421875
train loss:  0.5339534282684326
train gradient:  0.15690758324302206
iteration : 7456
train acc:  0.7109375
train loss:  0.5484923124313354
train gradient:  0.17929175708813377
iteration : 7457
train acc:  0.6953125
train loss:  0.5665819644927979
train gradient:  0.21802226101243258
iteration : 7458
train acc:  0.6875
train loss:  0.5433419942855835
train gradient:  0.15222563154643665
iteration : 7459
train acc:  0.71875
train loss:  0.5431525707244873
train gradient:  0.16107463284043494
iteration : 7460
train acc:  0.7421875
train loss:  0.5326946973800659
train gradient:  0.1435035091051337
iteration : 7461
train acc:  0.6953125
train loss:  0.5659399032592773
train gradient:  0.20451175070903366
iteration : 7462
train acc:  0.8046875
train loss:  0.423554003238678
train gradient:  0.1120275510022692
iteration : 7463
train acc:  0.7109375
train loss:  0.5003090500831604
train gradient:  0.14018813704922994
iteration : 7464
train acc:  0.75
train loss:  0.5797274708747864
train gradient:  0.22863924182273743
iteration : 7465
train acc:  0.765625
train loss:  0.5198891758918762
train gradient:  0.17130288613936984
iteration : 7466
train acc:  0.7109375
train loss:  0.540579080581665
train gradient:  0.1775447658307029
iteration : 7467
train acc:  0.7265625
train loss:  0.5489678382873535
train gradient:  0.1842193667156362
iteration : 7468
train acc:  0.6875
train loss:  0.6046162843704224
train gradient:  0.19194951471591343
iteration : 7469
train acc:  0.7578125
train loss:  0.5178227424621582
train gradient:  0.14763331152999226
iteration : 7470
train acc:  0.7265625
train loss:  0.5390601754188538
train gradient:  0.1722296718228939
iteration : 7471
train acc:  0.7734375
train loss:  0.45249736309051514
train gradient:  0.13562832551344253
iteration : 7472
train acc:  0.7421875
train loss:  0.492343932390213
train gradient:  0.12579014746647857
iteration : 7473
train acc:  0.7265625
train loss:  0.5411171317100525
train gradient:  0.16552074327962285
iteration : 7474
train acc:  0.7421875
train loss:  0.45724257826805115
train gradient:  0.1158093112671517
iteration : 7475
train acc:  0.6953125
train loss:  0.5494197607040405
train gradient:  0.126222903559414
iteration : 7476
train acc:  0.734375
train loss:  0.5793708562850952
train gradient:  0.254106870869223
iteration : 7477
train acc:  0.7421875
train loss:  0.5244521498680115
train gradient:  0.16196257274411469
iteration : 7478
train acc:  0.7890625
train loss:  0.453839510679245
train gradient:  0.12782815510659326
iteration : 7479
train acc:  0.6640625
train loss:  0.5945411324501038
train gradient:  0.21343442044047062
iteration : 7480
train acc:  0.7421875
train loss:  0.4705390930175781
train gradient:  0.132132266606533
iteration : 7481
train acc:  0.703125
train loss:  0.5344598293304443
train gradient:  0.17217383324123492
iteration : 7482
train acc:  0.6953125
train loss:  0.5913883447647095
train gradient:  0.23015337990734475
iteration : 7483
train acc:  0.734375
train loss:  0.5079781413078308
train gradient:  0.1460188269979906
iteration : 7484
train acc:  0.75
train loss:  0.48394936323165894
train gradient:  0.10506234409532979
iteration : 7485
train acc:  0.7109375
train loss:  0.5060298442840576
train gradient:  0.11997937691489738
iteration : 7486
train acc:  0.7421875
train loss:  0.4989752471446991
train gradient:  0.1247053624873141
iteration : 7487
train acc:  0.7265625
train loss:  0.5093109607696533
train gradient:  0.12780858033775266
iteration : 7488
train acc:  0.796875
train loss:  0.46418678760528564
train gradient:  0.12456671052273662
iteration : 7489
train acc:  0.7109375
train loss:  0.48504579067230225
train gradient:  0.10658933840239852
iteration : 7490
train acc:  0.7421875
train loss:  0.4874938130378723
train gradient:  0.1652885213470099
iteration : 7491
train acc:  0.75
train loss:  0.46968573331832886
train gradient:  0.14676162370699206
iteration : 7492
train acc:  0.703125
train loss:  0.5418925285339355
train gradient:  0.17513048922309882
iteration : 7493
train acc:  0.7734375
train loss:  0.5136263966560364
train gradient:  0.18616715354376973
iteration : 7494
train acc:  0.6953125
train loss:  0.5297268629074097
train gradient:  0.1441379867094898
iteration : 7495
train acc:  0.7578125
train loss:  0.526614248752594
train gradient:  0.1236814634189295
iteration : 7496
train acc:  0.71875
train loss:  0.5706044435501099
train gradient:  0.14502691233183218
iteration : 7497
train acc:  0.7265625
train loss:  0.5447044372558594
train gradient:  0.16220531894577134
iteration : 7498
train acc:  0.7421875
train loss:  0.5596206784248352
train gradient:  0.16604754859175164
iteration : 7499
train acc:  0.796875
train loss:  0.47097283601760864
train gradient:  0.1266417874188842
iteration : 7500
train acc:  0.7890625
train loss:  0.4116254448890686
train gradient:  0.0934423599312386
iteration : 7501
train acc:  0.75
train loss:  0.5185017585754395
train gradient:  0.140519176869959
iteration : 7502
train acc:  0.734375
train loss:  0.45868173241615295
train gradient:  0.13803973259984037
iteration : 7503
train acc:  0.7578125
train loss:  0.4859495759010315
train gradient:  0.17490587510307132
iteration : 7504
train acc:  0.75
train loss:  0.48470744490623474
train gradient:  0.13245534797269198
iteration : 7505
train acc:  0.6953125
train loss:  0.5358834266662598
train gradient:  0.18102040022363347
iteration : 7506
train acc:  0.7734375
train loss:  0.49200040102005005
train gradient:  0.12242211323691225
iteration : 7507
train acc:  0.71875
train loss:  0.4842630624771118
train gradient:  0.13950293245415266
iteration : 7508
train acc:  0.7734375
train loss:  0.4608387351036072
train gradient:  0.11303284775705791
iteration : 7509
train acc:  0.7578125
train loss:  0.4512900710105896
train gradient:  0.1286680330631214
iteration : 7510
train acc:  0.7265625
train loss:  0.5185864567756653
train gradient:  0.13990183621733254
iteration : 7511
train acc:  0.7734375
train loss:  0.4659452438354492
train gradient:  0.09989630593630174
iteration : 7512
train acc:  0.7421875
train loss:  0.5021483302116394
train gradient:  0.13288636572136037
iteration : 7513
train acc:  0.75
train loss:  0.48313993215560913
train gradient:  0.16210709898646286
iteration : 7514
train acc:  0.7421875
train loss:  0.4624895453453064
train gradient:  0.13980411868430404
iteration : 7515
train acc:  0.765625
train loss:  0.48089122772216797
train gradient:  0.12682197740602474
iteration : 7516
train acc:  0.7265625
train loss:  0.5118943452835083
train gradient:  0.17663177383837014
iteration : 7517
train acc:  0.6640625
train loss:  0.5977736711502075
train gradient:  0.2188382780168427
iteration : 7518
train acc:  0.71875
train loss:  0.5018333196640015
train gradient:  0.17489980956088327
iteration : 7519
train acc:  0.71875
train loss:  0.5446881055831909
train gradient:  0.1574690583962533
iteration : 7520
train acc:  0.7421875
train loss:  0.49089473485946655
train gradient:  0.12475128359099527
iteration : 7521
train acc:  0.703125
train loss:  0.5577905774116516
train gradient:  0.14749078914516714
iteration : 7522
train acc:  0.7109375
train loss:  0.5388028025627136
train gradient:  0.18538800373149283
iteration : 7523
train acc:  0.75
train loss:  0.45856136083602905
train gradient:  0.14601157823681327
iteration : 7524
train acc:  0.7734375
train loss:  0.4927515983581543
train gradient:  0.14679781227243072
iteration : 7525
train acc:  0.7109375
train loss:  0.5234144926071167
train gradient:  0.11464343823037475
iteration : 7526
train acc:  0.7421875
train loss:  0.46922609210014343
train gradient:  0.1525105456554246
iteration : 7527
train acc:  0.7578125
train loss:  0.456638902425766
train gradient:  0.10692219497976281
iteration : 7528
train acc:  0.7265625
train loss:  0.4982243478298187
train gradient:  0.12973260043895313
iteration : 7529
train acc:  0.75
train loss:  0.5231019258499146
train gradient:  0.14545686467012597
iteration : 7530
train acc:  0.6640625
train loss:  0.6198092699050903
train gradient:  0.16754976690391776
iteration : 7531
train acc:  0.7734375
train loss:  0.4742448031902313
train gradient:  0.12773393991639104
iteration : 7532
train acc:  0.7734375
train loss:  0.4377124011516571
train gradient:  0.1239253986449803
iteration : 7533
train acc:  0.75
train loss:  0.5098435282707214
train gradient:  0.1326778430578951
iteration : 7534
train acc:  0.7578125
train loss:  0.5236002206802368
train gradient:  0.14889256857575528
iteration : 7535
train acc:  0.71875
train loss:  0.543556809425354
train gradient:  0.22684370558503342
iteration : 7536
train acc:  0.6875
train loss:  0.5671647787094116
train gradient:  0.17596786609798787
iteration : 7537
train acc:  0.71875
train loss:  0.494313508272171
train gradient:  0.12800900594062362
iteration : 7538
train acc:  0.6953125
train loss:  0.524678111076355
train gradient:  0.11086924473785596
iteration : 7539
train acc:  0.7578125
train loss:  0.5099796056747437
train gradient:  0.13205668060029607
iteration : 7540
train acc:  0.7421875
train loss:  0.46962568163871765
train gradient:  0.1353431595682406
iteration : 7541
train acc:  0.7578125
train loss:  0.49416935443878174
train gradient:  0.14369057249809392
iteration : 7542
train acc:  0.6796875
train loss:  0.5126659870147705
train gradient:  0.11367252488228831
iteration : 7543
train acc:  0.765625
train loss:  0.48078715801239014
train gradient:  0.1656455322691896
iteration : 7544
train acc:  0.6953125
train loss:  0.5261926054954529
train gradient:  0.13511950004668694
iteration : 7545
train acc:  0.6953125
train loss:  0.556559681892395
train gradient:  0.1598329668902329
iteration : 7546
train acc:  0.7421875
train loss:  0.48642075061798096
train gradient:  0.11214175668233822
iteration : 7547
train acc:  0.6875
train loss:  0.6158803701400757
train gradient:  0.1916504527464118
iteration : 7548
train acc:  0.6953125
train loss:  0.5328648090362549
train gradient:  0.16530487349536988
iteration : 7549
train acc:  0.765625
train loss:  0.47767606377601624
train gradient:  0.12229346864986433
iteration : 7550
train acc:  0.75
train loss:  0.46484071016311646
train gradient:  0.13353977552286422
iteration : 7551
train acc:  0.7421875
train loss:  0.510214626789093
train gradient:  0.13928247904382673
iteration : 7552
train acc:  0.7421875
train loss:  0.5009125471115112
train gradient:  0.11869748989196811
iteration : 7553
train acc:  0.7421875
train loss:  0.5011996626853943
train gradient:  0.13602843837648015
iteration : 7554
train acc:  0.71875
train loss:  0.5507032871246338
train gradient:  0.16051133291608516
iteration : 7555
train acc:  0.6796875
train loss:  0.5855623483657837
train gradient:  0.19740990549776039
iteration : 7556
train acc:  0.7578125
train loss:  0.5269953012466431
train gradient:  0.17533039787061894
iteration : 7557
train acc:  0.796875
train loss:  0.39894023537635803
train gradient:  0.09034796493750952
iteration : 7558
train acc:  0.6953125
train loss:  0.5736132860183716
train gradient:  0.1995006566736493
iteration : 7559
train acc:  0.65625
train loss:  0.61092209815979
train gradient:  0.1629195300867578
iteration : 7560
train acc:  0.6640625
train loss:  0.5670589208602905
train gradient:  0.13573365992203043
iteration : 7561
train acc:  0.703125
train loss:  0.5183864235877991
train gradient:  0.14142808977484256
iteration : 7562
train acc:  0.6640625
train loss:  0.541797935962677
train gradient:  0.1329435447350194
iteration : 7563
train acc:  0.7421875
train loss:  0.4751446843147278
train gradient:  0.10993021381720322
iteration : 7564
train acc:  0.6953125
train loss:  0.537695050239563
train gradient:  0.1721701403060863
iteration : 7565
train acc:  0.703125
train loss:  0.5688936114311218
train gradient:  0.18051623307329207
iteration : 7566
train acc:  0.8125
train loss:  0.4431244730949402
train gradient:  0.1075046244829198
iteration : 7567
train acc:  0.65625
train loss:  0.5455572605133057
train gradient:  0.18122543226577414
iteration : 7568
train acc:  0.7265625
train loss:  0.5336349010467529
train gradient:  0.15325934239766015
iteration : 7569
train acc:  0.6796875
train loss:  0.5461339950561523
train gradient:  0.1252776449018424
iteration : 7570
train acc:  0.6640625
train loss:  0.5503531694412231
train gradient:  0.12562259366454445
iteration : 7571
train acc:  0.734375
train loss:  0.5334824323654175
train gradient:  0.16944659849474786
iteration : 7572
train acc:  0.7421875
train loss:  0.48969805240631104
train gradient:  0.1329035727298583
iteration : 7573
train acc:  0.765625
train loss:  0.449712872505188
train gradient:  0.12034198030099731
iteration : 7574
train acc:  0.734375
train loss:  0.48890894651412964
train gradient:  0.15830489444575213
iteration : 7575
train acc:  0.71875
train loss:  0.5347623229026794
train gradient:  0.16611595567961707
iteration : 7576
train acc:  0.765625
train loss:  0.48976558446884155
train gradient:  0.12659712357905334
iteration : 7577
train acc:  0.75
train loss:  0.5152324438095093
train gradient:  0.13041532346628493
iteration : 7578
train acc:  0.78125
train loss:  0.4565603733062744
train gradient:  0.12694088653301094
iteration : 7579
train acc:  0.7578125
train loss:  0.5225048065185547
train gradient:  0.14076115642497117
iteration : 7580
train acc:  0.7578125
train loss:  0.4813357889652252
train gradient:  0.12794225948505433
iteration : 7581
train acc:  0.75
train loss:  0.5113198757171631
train gradient:  0.1660837934692604
iteration : 7582
train acc:  0.8125
train loss:  0.4689340591430664
train gradient:  0.10909627657523822
iteration : 7583
train acc:  0.75
train loss:  0.5477451086044312
train gradient:  0.1520112785596432
iteration : 7584
train acc:  0.6875
train loss:  0.5557870864868164
train gradient:  0.15371768765079893
iteration : 7585
train acc:  0.65625
train loss:  0.5660218000411987
train gradient:  0.15649972078813207
iteration : 7586
train acc:  0.75
train loss:  0.5228265523910522
train gradient:  0.17158834576642346
iteration : 7587
train acc:  0.7109375
train loss:  0.5760945677757263
train gradient:  0.164082738236706
iteration : 7588
train acc:  0.7109375
train loss:  0.5334500074386597
train gradient:  0.135693644453299
iteration : 7589
train acc:  0.796875
train loss:  0.4914410710334778
train gradient:  0.08267402363874957
iteration : 7590
train acc:  0.796875
train loss:  0.452544629573822
train gradient:  0.11712568971022207
iteration : 7591
train acc:  0.7578125
train loss:  0.48743653297424316
train gradient:  0.1290414276844853
iteration : 7592
train acc:  0.8125
train loss:  0.45738816261291504
train gradient:  0.10475412151989165
iteration : 7593
train acc:  0.6796875
train loss:  0.583149790763855
train gradient:  0.21360959775667054
iteration : 7594
train acc:  0.7265625
train loss:  0.4945332705974579
train gradient:  0.10328093670942494
iteration : 7595
train acc:  0.7734375
train loss:  0.5116580724716187
train gradient:  0.1346157401211336
iteration : 7596
train acc:  0.671875
train loss:  0.5691950917243958
train gradient:  0.14091647488186235
iteration : 7597
train acc:  0.7578125
train loss:  0.46768903732299805
train gradient:  0.16832939183958606
iteration : 7598
train acc:  0.703125
train loss:  0.5297585725784302
train gradient:  0.12768821212104325
iteration : 7599
train acc:  0.6953125
train loss:  0.5243993997573853
train gradient:  0.1453499390809342
iteration : 7600
train acc:  0.71875
train loss:  0.5445114374160767
train gradient:  0.15577404520919136
iteration : 7601
train acc:  0.765625
train loss:  0.4747428596019745
train gradient:  0.10107036637666056
iteration : 7602
train acc:  0.71875
train loss:  0.5387970209121704
train gradient:  0.20481809400117856
iteration : 7603
train acc:  0.7265625
train loss:  0.5262652635574341
train gradient:  0.13971849029738415
iteration : 7604
train acc:  0.6484375
train loss:  0.5929779410362244
train gradient:  0.17551607299407657
iteration : 7605
train acc:  0.734375
train loss:  0.49988386034965515
train gradient:  0.1661086027518492
iteration : 7606
train acc:  0.75
train loss:  0.4945550262928009
train gradient:  0.12252193188852821
iteration : 7607
train acc:  0.6796875
train loss:  0.544823408126831
train gradient:  0.13424351522676575
iteration : 7608
train acc:  0.75
train loss:  0.5135005712509155
train gradient:  0.21404751563635555
iteration : 7609
train acc:  0.7109375
train loss:  0.48662567138671875
train gradient:  0.13280358402039075
iteration : 7610
train acc:  0.796875
train loss:  0.4756629168987274
train gradient:  0.11449377629461233
iteration : 7611
train acc:  0.671875
train loss:  0.5644838809967041
train gradient:  0.1462292109457922
iteration : 7612
train acc:  0.765625
train loss:  0.4883544445037842
train gradient:  0.10635437768189417
iteration : 7613
train acc:  0.796875
train loss:  0.4535655379295349
train gradient:  0.12103235521656701
iteration : 7614
train acc:  0.7421875
train loss:  0.4959062337875366
train gradient:  0.14131847292073202
iteration : 7615
train acc:  0.78125
train loss:  0.4891210198402405
train gradient:  0.1016526918779947
iteration : 7616
train acc:  0.7109375
train loss:  0.5699993371963501
train gradient:  0.15877508029844548
iteration : 7617
train acc:  0.625
train loss:  0.6979433298110962
train gradient:  0.19944599571513189
iteration : 7618
train acc:  0.765625
train loss:  0.45034801959991455
train gradient:  0.09153133664774556
iteration : 7619
train acc:  0.6484375
train loss:  0.5359756946563721
train gradient:  0.1231151519342519
iteration : 7620
train acc:  0.765625
train loss:  0.4547934830188751
train gradient:  0.09727899577868754
iteration : 7621
train acc:  0.703125
train loss:  0.5565990805625916
train gradient:  0.16267257895537118
iteration : 7622
train acc:  0.65625
train loss:  0.5386322736740112
train gradient:  0.14738975384503084
iteration : 7623
train acc:  0.7109375
train loss:  0.5166317224502563
train gradient:  0.12386014765442639
iteration : 7624
train acc:  0.734375
train loss:  0.49315571784973145
train gradient:  0.11172061953110718
iteration : 7625
train acc:  0.7421875
train loss:  0.502723753452301
train gradient:  0.13781855334635776
iteration : 7626
train acc:  0.75
train loss:  0.528131365776062
train gradient:  0.14761964535499747
iteration : 7627
train acc:  0.765625
train loss:  0.48135921359062195
train gradient:  0.14219744942750387
iteration : 7628
train acc:  0.671875
train loss:  0.5764997005462646
train gradient:  0.15989803273223901
iteration : 7629
train acc:  0.734375
train loss:  0.5311405658721924
train gradient:  0.153256894570039
iteration : 7630
train acc:  0.6875
train loss:  0.5299267768859863
train gradient:  0.13112102144165153
iteration : 7631
train acc:  0.7421875
train loss:  0.4943176805973053
train gradient:  0.11954367486581115
iteration : 7632
train acc:  0.7734375
train loss:  0.47837918996810913
train gradient:  0.1146491776625314
iteration : 7633
train acc:  0.7890625
train loss:  0.4492837190628052
train gradient:  0.11782206179414687
iteration : 7634
train acc:  0.7265625
train loss:  0.5637733936309814
train gradient:  0.21731379982040047
iteration : 7635
train acc:  0.7421875
train loss:  0.49930596351623535
train gradient:  0.12062449690662394
iteration : 7636
train acc:  0.78125
train loss:  0.43533602356910706
train gradient:  0.10328036937416495
iteration : 7637
train acc:  0.7265625
train loss:  0.5306366682052612
train gradient:  0.13051500186011034
iteration : 7638
train acc:  0.7421875
train loss:  0.5246962308883667
train gradient:  0.17807280169655243
iteration : 7639
train acc:  0.7734375
train loss:  0.4674258530139923
train gradient:  0.12134251503735284
iteration : 7640
train acc:  0.7265625
train loss:  0.4986404478549957
train gradient:  0.12928267166641763
iteration : 7641
train acc:  0.765625
train loss:  0.4645485281944275
train gradient:  0.13925393005022957
iteration : 7642
train acc:  0.765625
train loss:  0.5146180987358093
train gradient:  0.12266975745324761
iteration : 7643
train acc:  0.671875
train loss:  0.5940409898757935
train gradient:  0.18383020804628006
iteration : 7644
train acc:  0.7890625
train loss:  0.44632428884506226
train gradient:  0.13708290658151745
iteration : 7645
train acc:  0.640625
train loss:  0.543160617351532
train gradient:  0.14350565072913746
iteration : 7646
train acc:  0.765625
train loss:  0.5076888203620911
train gradient:  0.1257366116228655
iteration : 7647
train acc:  0.7734375
train loss:  0.47892701625823975
train gradient:  0.1260101668974903
iteration : 7648
train acc:  0.75
train loss:  0.487251877784729
train gradient:  0.1131245118241913
iteration : 7649
train acc:  0.7421875
train loss:  0.48297348618507385
train gradient:  0.10466567300991772
iteration : 7650
train acc:  0.7421875
train loss:  0.4806210398674011
train gradient:  0.1151549771215092
iteration : 7651
train acc:  0.7734375
train loss:  0.43018966913223267
train gradient:  0.09405286685130364
iteration : 7652
train acc:  0.765625
train loss:  0.4509633779525757
train gradient:  0.11384985674993091
iteration : 7653
train acc:  0.8125
train loss:  0.46557140350341797
train gradient:  0.09715695134664434
iteration : 7654
train acc:  0.7421875
train loss:  0.48113730549812317
train gradient:  0.1304852313838627
iteration : 7655
train acc:  0.71875
train loss:  0.5625309944152832
train gradient:  0.13615053864414195
iteration : 7656
train acc:  0.78125
train loss:  0.4741958975791931
train gradient:  0.12773407361443612
iteration : 7657
train acc:  0.734375
train loss:  0.5022767782211304
train gradient:  0.1449134071330401
iteration : 7658
train acc:  0.75
train loss:  0.47155946493148804
train gradient:  0.09547416841114621
iteration : 7659
train acc:  0.7109375
train loss:  0.541583776473999
train gradient:  0.13395797480369753
iteration : 7660
train acc:  0.7265625
train loss:  0.5266484618186951
train gradient:  0.13984517973032942
iteration : 7661
train acc:  0.7421875
train loss:  0.5508307814598083
train gradient:  0.14519036117544495
iteration : 7662
train acc:  0.7421875
train loss:  0.474889874458313
train gradient:  0.1748509972419655
iteration : 7663
train acc:  0.6796875
train loss:  0.6326888203620911
train gradient:  0.33198640326686935
iteration : 7664
train acc:  0.7734375
train loss:  0.5042088031768799
train gradient:  0.10963059276170153
iteration : 7665
train acc:  0.75
train loss:  0.5296216011047363
train gradient:  0.12836250939022695
iteration : 7666
train acc:  0.828125
train loss:  0.42595064640045166
train gradient:  0.10262047364689732
iteration : 7667
train acc:  0.7890625
train loss:  0.470851868391037
train gradient:  0.13209398023712499
iteration : 7668
train acc:  0.8046875
train loss:  0.4335929751396179
train gradient:  0.09581680511413095
iteration : 7669
train acc:  0.7578125
train loss:  0.502832293510437
train gradient:  0.16668778413181884
iteration : 7670
train acc:  0.6328125
train loss:  0.5827463865280151
train gradient:  0.19673288752024687
iteration : 7671
train acc:  0.8125
train loss:  0.45059096813201904
train gradient:  0.1041532315994239
iteration : 7672
train acc:  0.734375
train loss:  0.5057488083839417
train gradient:  0.13295642737025753
iteration : 7673
train acc:  0.7421875
train loss:  0.48857051134109497
train gradient:  0.1120753644710803
iteration : 7674
train acc:  0.71875
train loss:  0.5222731828689575
train gradient:  0.12066190234589284
iteration : 7675
train acc:  0.6796875
train loss:  0.4924716353416443
train gradient:  0.12829112118122143
iteration : 7676
train acc:  0.7109375
train loss:  0.5080992579460144
train gradient:  0.12141980593688828
iteration : 7677
train acc:  0.7578125
train loss:  0.4437507390975952
train gradient:  0.11623056225676523
iteration : 7678
train acc:  0.7421875
train loss:  0.5180681347846985
train gradient:  0.14612336097498796
iteration : 7679
train acc:  0.71875
train loss:  0.5107429027557373
train gradient:  0.14705448371766466
iteration : 7680
train acc:  0.6875
train loss:  0.4792262613773346
train gradient:  0.1322061986030113
iteration : 7681
train acc:  0.734375
train loss:  0.4693511724472046
train gradient:  0.14057236552918134
iteration : 7682
train acc:  0.71875
train loss:  0.5251820683479309
train gradient:  0.13762342490782098
iteration : 7683
train acc:  0.75
train loss:  0.4820634722709656
train gradient:  0.12280893185049087
iteration : 7684
train acc:  0.7421875
train loss:  0.48953694105148315
train gradient:  0.1267546564821458
iteration : 7685
train acc:  0.7421875
train loss:  0.4734496474266052
train gradient:  0.11908892973935532
iteration : 7686
train acc:  0.765625
train loss:  0.4735812246799469
train gradient:  0.10963093354386048
iteration : 7687
train acc:  0.75
train loss:  0.5029836893081665
train gradient:  0.1456774820472339
iteration : 7688
train acc:  0.6953125
train loss:  0.5676443576812744
train gradient:  0.162433490469179
iteration : 7689
train acc:  0.8046875
train loss:  0.43682318925857544
train gradient:  0.11001567265285853
iteration : 7690
train acc:  0.703125
train loss:  0.5385939478874207
train gradient:  0.12206456503319693
iteration : 7691
train acc:  0.796875
train loss:  0.4707626402378082
train gradient:  0.1536618896743489
iteration : 7692
train acc:  0.765625
train loss:  0.47676029801368713
train gradient:  0.11652972453149155
iteration : 7693
train acc:  0.8046875
train loss:  0.4341265857219696
train gradient:  0.12859467410138142
iteration : 7694
train acc:  0.7109375
train loss:  0.5408004522323608
train gradient:  0.11547335897015683
iteration : 7695
train acc:  0.796875
train loss:  0.43400049209594727
train gradient:  0.10892682532714006
iteration : 7696
train acc:  0.765625
train loss:  0.44615525007247925
train gradient:  0.0986088509105558
iteration : 7697
train acc:  0.765625
train loss:  0.5116385221481323
train gradient:  0.14771559893947145
iteration : 7698
train acc:  0.7109375
train loss:  0.5125988125801086
train gradient:  0.14866669204887611
iteration : 7699
train acc:  0.6875
train loss:  0.520675539970398
train gradient:  0.14572058087955728
iteration : 7700
train acc:  0.7421875
train loss:  0.5203976035118103
train gradient:  0.18027988288727043
iteration : 7701
train acc:  0.71875
train loss:  0.5007365942001343
train gradient:  0.14683336204244749
iteration : 7702
train acc:  0.703125
train loss:  0.5577539205551147
train gradient:  0.14758746753717192
iteration : 7703
train acc:  0.78125
train loss:  0.47504985332489014
train gradient:  0.1797020381618524
iteration : 7704
train acc:  0.7890625
train loss:  0.4258660674095154
train gradient:  0.1046573507168033
iteration : 7705
train acc:  0.765625
train loss:  0.4655900001525879
train gradient:  0.12622337781344178
iteration : 7706
train acc:  0.6953125
train loss:  0.49259406328201294
train gradient:  0.13715475712208156
iteration : 7707
train acc:  0.734375
train loss:  0.5637622475624084
train gradient:  0.17012959762130248
iteration : 7708
train acc:  0.6953125
train loss:  0.5815424919128418
train gradient:  0.20128563257819587
iteration : 7709
train acc:  0.7109375
train loss:  0.47383013367652893
train gradient:  0.12036873651870866
iteration : 7710
train acc:  0.703125
train loss:  0.5411620140075684
train gradient:  0.21446652201144795
iteration : 7711
train acc:  0.765625
train loss:  0.48797982931137085
train gradient:  0.11033370804347861
iteration : 7712
train acc:  0.8046875
train loss:  0.40155038237571716
train gradient:  0.10596838953959803
iteration : 7713
train acc:  0.7109375
train loss:  0.5317773818969727
train gradient:  0.1374752811580552
iteration : 7714
train acc:  0.7734375
train loss:  0.493839293718338
train gradient:  0.13613684499914375
iteration : 7715
train acc:  0.765625
train loss:  0.5410641431808472
train gradient:  0.15302474252906467
iteration : 7716
train acc:  0.7734375
train loss:  0.47522205114364624
train gradient:  0.1193745876269931
iteration : 7717
train acc:  0.71875
train loss:  0.5158696174621582
train gradient:  0.17679795291028905
iteration : 7718
train acc:  0.796875
train loss:  0.4606688618659973
train gradient:  0.10956558155377288
iteration : 7719
train acc:  0.6953125
train loss:  0.5297027826309204
train gradient:  0.12694610907292544
iteration : 7720
train acc:  0.7421875
train loss:  0.5478947162628174
train gradient:  0.19975755046729604
iteration : 7721
train acc:  0.75
train loss:  0.48090195655822754
train gradient:  0.11275824481764875
iteration : 7722
train acc:  0.6796875
train loss:  0.5264757871627808
train gradient:  0.14672564450137032
iteration : 7723
train acc:  0.6640625
train loss:  0.5435725450515747
train gradient:  0.16889710018214735
iteration : 7724
train acc:  0.765625
train loss:  0.5663979053497314
train gradient:  0.15818188429463498
iteration : 7725
train acc:  0.734375
train loss:  0.4882969856262207
train gradient:  0.11746954807502694
iteration : 7726
train acc:  0.640625
train loss:  0.5849713683128357
train gradient:  0.17913751196600458
iteration : 7727
train acc:  0.734375
train loss:  0.5151875019073486
train gradient:  0.129133134395839
iteration : 7728
train acc:  0.7734375
train loss:  0.4864794909954071
train gradient:  0.1688933221984386
iteration : 7729
train acc:  0.6875
train loss:  0.5899141430854797
train gradient:  0.2571344580083811
iteration : 7730
train acc:  0.703125
train loss:  0.507111668586731
train gradient:  0.14086167041228098
iteration : 7731
train acc:  0.7109375
train loss:  0.49257367849349976
train gradient:  0.17529665041438242
iteration : 7732
train acc:  0.7734375
train loss:  0.4506857395172119
train gradient:  0.11000332492987348
iteration : 7733
train acc:  0.7578125
train loss:  0.49891385436058044
train gradient:  0.11506030890729424
iteration : 7734
train acc:  0.7265625
train loss:  0.542396068572998
train gradient:  0.1590111086141731
iteration : 7735
train acc:  0.6953125
train loss:  0.5924016237258911
train gradient:  0.15273885310582563
iteration : 7736
train acc:  0.796875
train loss:  0.46607446670532227
train gradient:  0.10584082728731546
iteration : 7737
train acc:  0.765625
train loss:  0.49513447284698486
train gradient:  0.16991227968135272
iteration : 7738
train acc:  0.7421875
train loss:  0.5234717130661011
train gradient:  0.1429761222528547
iteration : 7739
train acc:  0.7734375
train loss:  0.47107911109924316
train gradient:  0.10664925461418974
iteration : 7740
train acc:  0.703125
train loss:  0.4967982769012451
train gradient:  0.11421532279185427
iteration : 7741
train acc:  0.7421875
train loss:  0.5003653168678284
train gradient:  0.1714248382298233
iteration : 7742
train acc:  0.71875
train loss:  0.5183922648429871
train gradient:  0.14093690303380124
iteration : 7743
train acc:  0.75
train loss:  0.49638912081718445
train gradient:  0.13858819819469276
iteration : 7744
train acc:  0.7109375
train loss:  0.5483170747756958
train gradient:  0.16577166264416654
iteration : 7745
train acc:  0.6640625
train loss:  0.5212106704711914
train gradient:  0.1420664101598587
iteration : 7746
train acc:  0.796875
train loss:  0.4359731078147888
train gradient:  0.15987066652382587
iteration : 7747
train acc:  0.7109375
train loss:  0.5002982020378113
train gradient:  0.13315831239689585
iteration : 7748
train acc:  0.7265625
train loss:  0.505913257598877
train gradient:  0.1628550000395986
iteration : 7749
train acc:  0.7421875
train loss:  0.5097711086273193
train gradient:  0.12742938113278288
iteration : 7750
train acc:  0.75
train loss:  0.5030790567398071
train gradient:  0.1422155431993716
iteration : 7751
train acc:  0.78125
train loss:  0.4921414256095886
train gradient:  0.12413575554320538
iteration : 7752
train acc:  0.703125
train loss:  0.5559670329093933
train gradient:  0.16480747670880097
iteration : 7753
train acc:  0.75
train loss:  0.469847172498703
train gradient:  0.14821706698500392
iteration : 7754
train acc:  0.734375
train loss:  0.5065368413925171
train gradient:  0.13656709146093807
iteration : 7755
train acc:  0.671875
train loss:  0.5476728677749634
train gradient:  0.1702517778742288
iteration : 7756
train acc:  0.7734375
train loss:  0.5443432331085205
train gradient:  0.1441034481983207
iteration : 7757
train acc:  0.703125
train loss:  0.5406694412231445
train gradient:  0.13868252109944793
iteration : 7758
train acc:  0.7421875
train loss:  0.5174944400787354
train gradient:  0.1646013598805678
iteration : 7759
train acc:  0.7421875
train loss:  0.4915037453174591
train gradient:  0.1831594454284966
iteration : 7760
train acc:  0.71875
train loss:  0.5131093263626099
train gradient:  0.14789068870522726
iteration : 7761
train acc:  0.75
train loss:  0.45074379444122314
train gradient:  0.09828346807165497
iteration : 7762
train acc:  0.796875
train loss:  0.4331906735897064
train gradient:  0.10691236113254769
iteration : 7763
train acc:  0.796875
train loss:  0.46072104573249817
train gradient:  0.11993966644101274
iteration : 7764
train acc:  0.71875
train loss:  0.5611816644668579
train gradient:  0.15426526161427706
iteration : 7765
train acc:  0.7265625
train loss:  0.500374436378479
train gradient:  0.12042323842124913
iteration : 7766
train acc:  0.765625
train loss:  0.4513222575187683
train gradient:  0.11186156718530091
iteration : 7767
train acc:  0.7265625
train loss:  0.5136734247207642
train gradient:  0.14105107360357372
iteration : 7768
train acc:  0.734375
train loss:  0.5219825506210327
train gradient:  0.1560589296170144
iteration : 7769
train acc:  0.75
train loss:  0.4963393807411194
train gradient:  0.17637659846378828
iteration : 7770
train acc:  0.7578125
train loss:  0.49178069829940796
train gradient:  0.1285023092426903
iteration : 7771
train acc:  0.6875
train loss:  0.5455150008201599
train gradient:  0.17005121254499778
iteration : 7772
train acc:  0.7109375
train loss:  0.514721155166626
train gradient:  0.15960692723158948
iteration : 7773
train acc:  0.8046875
train loss:  0.4459332227706909
train gradient:  0.12502687221085407
iteration : 7774
train acc:  0.7265625
train loss:  0.5630860924720764
train gradient:  0.17137315684046606
iteration : 7775
train acc:  0.78125
train loss:  0.4548220634460449
train gradient:  0.14109690162396865
iteration : 7776
train acc:  0.7265625
train loss:  0.5004333257675171
train gradient:  0.12004211622568445
iteration : 7777
train acc:  0.7265625
train loss:  0.4773900806903839
train gradient:  0.09638523777452337
iteration : 7778
train acc:  0.7890625
train loss:  0.4477197527885437
train gradient:  0.12576171273712086
iteration : 7779
train acc:  0.6953125
train loss:  0.5308685302734375
train gradient:  0.15465611029412163
iteration : 7780
train acc:  0.703125
train loss:  0.4769746661186218
train gradient:  0.11293087079019724
iteration : 7781
train acc:  0.703125
train loss:  0.5563728213310242
train gradient:  0.18933802119133272
iteration : 7782
train acc:  0.71875
train loss:  0.47896623611450195
train gradient:  0.12181357542668497
iteration : 7783
train acc:  0.75
train loss:  0.4675701856613159
train gradient:  0.12393109840076566
iteration : 7784
train acc:  0.7421875
train loss:  0.5473231077194214
train gradient:  0.13708488649865364
iteration : 7785
train acc:  0.7421875
train loss:  0.5072132349014282
train gradient:  0.18373584248754282
iteration : 7786
train acc:  0.6953125
train loss:  0.519816517829895
train gradient:  0.25614125054541254
iteration : 7787
train acc:  0.78125
train loss:  0.4902660846710205
train gradient:  0.13167434191996558
iteration : 7788
train acc:  0.78125
train loss:  0.482286661863327
train gradient:  0.11189754105972176
iteration : 7789
train acc:  0.640625
train loss:  0.5687437057495117
train gradient:  0.2122745641484557
iteration : 7790
train acc:  0.6875
train loss:  0.5388779640197754
train gradient:  0.18838643442552988
iteration : 7791
train acc:  0.671875
train loss:  0.5524702072143555
train gradient:  0.2536444830839107
iteration : 7792
train acc:  0.703125
train loss:  0.5404640436172485
train gradient:  0.14282473796771838
iteration : 7793
train acc:  0.7890625
train loss:  0.460965633392334
train gradient:  0.12360788293449362
iteration : 7794
train acc:  0.7109375
train loss:  0.5676833391189575
train gradient:  0.14913664506219113
iteration : 7795
train acc:  0.703125
train loss:  0.5038450360298157
train gradient:  0.13646774871566636
iteration : 7796
train acc:  0.78125
train loss:  0.5345995426177979
train gradient:  0.1451877058290612
iteration : 7797
train acc:  0.6328125
train loss:  0.6167131662368774
train gradient:  0.26232736872827844
iteration : 7798
train acc:  0.75
train loss:  0.48175710439682007
train gradient:  0.10982468285122145
iteration : 7799
train acc:  0.7109375
train loss:  0.6091614961624146
train gradient:  0.27670016493002864
iteration : 7800
train acc:  0.671875
train loss:  0.558849573135376
train gradient:  0.18251601435179948
iteration : 7801
train acc:  0.7265625
train loss:  0.4734794497489929
train gradient:  0.09999915731028138
iteration : 7802
train acc:  0.7265625
train loss:  0.5186468362808228
train gradient:  0.12853125197420456
iteration : 7803
train acc:  0.7578125
train loss:  0.49613475799560547
train gradient:  0.12821449884162434
iteration : 7804
train acc:  0.703125
train loss:  0.5110516548156738
train gradient:  0.12706580216542776
iteration : 7805
train acc:  0.7734375
train loss:  0.47114020586013794
train gradient:  0.12865354838353604
iteration : 7806
train acc:  0.7109375
train loss:  0.5115014314651489
train gradient:  0.16768105822648194
iteration : 7807
train acc:  0.7265625
train loss:  0.4968048632144928
train gradient:  0.13931132974404892
iteration : 7808
train acc:  0.75
train loss:  0.48038923740386963
train gradient:  0.12469015123862671
iteration : 7809
train acc:  0.8046875
train loss:  0.4472157955169678
train gradient:  0.10949749587229511
iteration : 7810
train acc:  0.71875
train loss:  0.511484682559967
train gradient:  0.1524106127516085
iteration : 7811
train acc:  0.7734375
train loss:  0.4479578137397766
train gradient:  0.1529368695005676
iteration : 7812
train acc:  0.7578125
train loss:  0.5034940242767334
train gradient:  0.1337497164276949
iteration : 7813
train acc:  0.765625
train loss:  0.4315663278102875
train gradient:  0.12173713128920775
iteration : 7814
train acc:  0.7265625
train loss:  0.5534521341323853
train gradient:  0.16066538323420998
iteration : 7815
train acc:  0.7265625
train loss:  0.5504969358444214
train gradient:  0.15560793693490832
iteration : 7816
train acc:  0.7890625
train loss:  0.4372180998325348
train gradient:  0.10845548318194799
iteration : 7817
train acc:  0.796875
train loss:  0.4811457693576813
train gradient:  0.12658234595828866
iteration : 7818
train acc:  0.7734375
train loss:  0.4611784517765045
train gradient:  0.10552766704487179
iteration : 7819
train acc:  0.765625
train loss:  0.4844837784767151
train gradient:  0.10804650087646184
iteration : 7820
train acc:  0.765625
train loss:  0.45208996534347534
train gradient:  0.12956915825921766
iteration : 7821
train acc:  0.7734375
train loss:  0.46896934509277344
train gradient:  0.1672201882427537
iteration : 7822
train acc:  0.796875
train loss:  0.5033625960350037
train gradient:  0.13431474683898584
iteration : 7823
train acc:  0.765625
train loss:  0.5101008415222168
train gradient:  0.12986060307676278
iteration : 7824
train acc:  0.734375
train loss:  0.4686748683452606
train gradient:  0.11725994841490912
iteration : 7825
train acc:  0.7109375
train loss:  0.5600978136062622
train gradient:  0.16296662527969766
iteration : 7826
train acc:  0.7265625
train loss:  0.5230393409729004
train gradient:  0.1434177407384065
iteration : 7827
train acc:  0.7890625
train loss:  0.4398908019065857
train gradient:  0.12861858671556492
iteration : 7828
train acc:  0.7265625
train loss:  0.5415331721305847
train gradient:  0.21854754739916415
iteration : 7829
train acc:  0.7265625
train loss:  0.47404447197914124
train gradient:  0.10159698497696282
iteration : 7830
train acc:  0.796875
train loss:  0.44962167739868164
train gradient:  0.11789227880180166
iteration : 7831
train acc:  0.75
train loss:  0.4838443994522095
train gradient:  0.1358180639726732
iteration : 7832
train acc:  0.7421875
train loss:  0.5311837792396545
train gradient:  0.16159033190957717
iteration : 7833
train acc:  0.796875
train loss:  0.48384737968444824
train gradient:  0.1473898281463982
iteration : 7834
train acc:  0.78125
train loss:  0.5054436922073364
train gradient:  0.13347768799690626
iteration : 7835
train acc:  0.7109375
train loss:  0.5237470865249634
train gradient:  0.13159495616184083
iteration : 7836
train acc:  0.8125
train loss:  0.4550119936466217
train gradient:  0.11291592464086317
iteration : 7837
train acc:  0.796875
train loss:  0.48400935530662537
train gradient:  0.14520683893478653
iteration : 7838
train acc:  0.71875
train loss:  0.520222544670105
train gradient:  0.12236301565287143
iteration : 7839
train acc:  0.6875
train loss:  0.5559396743774414
train gradient:  0.16282954288493906
iteration : 7840
train acc:  0.765625
train loss:  0.4590456485748291
train gradient:  0.12346737266058343
iteration : 7841
train acc:  0.734375
train loss:  0.5313664674758911
train gradient:  0.1955672001041201
iteration : 7842
train acc:  0.796875
train loss:  0.47162672877311707
train gradient:  0.10971976107530607
iteration : 7843
train acc:  0.671875
train loss:  0.5142844915390015
train gradient:  0.1567319555074544
iteration : 7844
train acc:  0.671875
train loss:  0.5761619806289673
train gradient:  0.18886716455123795
iteration : 7845
train acc:  0.6328125
train loss:  0.61124587059021
train gradient:  0.18989985305832038
iteration : 7846
train acc:  0.796875
train loss:  0.4454483687877655
train gradient:  0.10976399537332492
iteration : 7847
train acc:  0.71875
train loss:  0.5505828261375427
train gradient:  0.16830057024254097
iteration : 7848
train acc:  0.78125
train loss:  0.45735129714012146
train gradient:  0.10595731962168686
iteration : 7849
train acc:  0.7265625
train loss:  0.4975665211677551
train gradient:  0.12202986835234712
iteration : 7850
train acc:  0.71875
train loss:  0.5380380153656006
train gradient:  0.14319017123212746
iteration : 7851
train acc:  0.734375
train loss:  0.5374579429626465
train gradient:  0.18607908208537338
iteration : 7852
train acc:  0.765625
train loss:  0.5120358467102051
train gradient:  0.13419874468062212
iteration : 7853
train acc:  0.71875
train loss:  0.5512309074401855
train gradient:  0.134129943493149
iteration : 7854
train acc:  0.7734375
train loss:  0.47537848353385925
train gradient:  0.13713006687480342
iteration : 7855
train acc:  0.7578125
train loss:  0.46516844630241394
train gradient:  0.12868597228553996
iteration : 7856
train acc:  0.7890625
train loss:  0.470811665058136
train gradient:  0.16477557847336216
iteration : 7857
train acc:  0.7265625
train loss:  0.49429747462272644
train gradient:  0.10951323947460104
iteration : 7858
train acc:  0.65625
train loss:  0.5928614735603333
train gradient:  0.23232134430105944
iteration : 7859
train acc:  0.7578125
train loss:  0.4509967565536499
train gradient:  0.10139604995799313
iteration : 7860
train acc:  0.75
train loss:  0.46866822242736816
train gradient:  0.10132848459805165
iteration : 7861
train acc:  0.7265625
train loss:  0.5312720537185669
train gradient:  0.1528924555689627
iteration : 7862
train acc:  0.7421875
train loss:  0.5258361101150513
train gradient:  0.16355169715536488
iteration : 7863
train acc:  0.7421875
train loss:  0.49466022849082947
train gradient:  0.12411769616749635
iteration : 7864
train acc:  0.6875
train loss:  0.5476213097572327
train gradient:  0.1758482759849714
iteration : 7865
train acc:  0.7734375
train loss:  0.45504847168922424
train gradient:  0.11560021243573203
iteration : 7866
train acc:  0.6171875
train loss:  0.6200566291809082
train gradient:  0.22841479049870478
iteration : 7867
train acc:  0.7421875
train loss:  0.5285623073577881
train gradient:  0.17224921117896508
iteration : 7868
train acc:  0.7734375
train loss:  0.45779773592948914
train gradient:  0.11797265231210517
iteration : 7869
train acc:  0.71875
train loss:  0.5745770931243896
train gradient:  0.1700123574742825
iteration : 7870
train acc:  0.7265625
train loss:  0.4892583191394806
train gradient:  0.14228816844097317
iteration : 7871
train acc:  0.71875
train loss:  0.507939338684082
train gradient:  0.14188430569648827
iteration : 7872
train acc:  0.765625
train loss:  0.4942213296890259
train gradient:  0.14156811736103417
iteration : 7873
train acc:  0.71875
train loss:  0.5557528138160706
train gradient:  0.16630668922084346
iteration : 7874
train acc:  0.796875
train loss:  0.43910667300224304
train gradient:  0.11160675816120777
iteration : 7875
train acc:  0.7890625
train loss:  0.4635452628135681
train gradient:  0.12137885965502684
iteration : 7876
train acc:  0.6875
train loss:  0.5888090133666992
train gradient:  0.1707785795973809
iteration : 7877
train acc:  0.7109375
train loss:  0.5590646266937256
train gradient:  0.2080156780589061
iteration : 7878
train acc:  0.7578125
train loss:  0.4337844252586365
train gradient:  0.1146383941785247
iteration : 7879
train acc:  0.7421875
train loss:  0.5162359476089478
train gradient:  0.16053433579680915
iteration : 7880
train acc:  0.7265625
train loss:  0.5325334072113037
train gradient:  0.15058967368596649
iteration : 7881
train acc:  0.734375
train loss:  0.523367166519165
train gradient:  0.13780044246532927
iteration : 7882
train acc:  0.8203125
train loss:  0.4571133553981781
train gradient:  0.140760242822288
iteration : 7883
train acc:  0.7265625
train loss:  0.5239707231521606
train gradient:  0.19769957469248287
iteration : 7884
train acc:  0.7421875
train loss:  0.4969596266746521
train gradient:  0.13833555483862697
iteration : 7885
train acc:  0.7734375
train loss:  0.5073341727256775
train gradient:  0.1512766877271004
iteration : 7886
train acc:  0.765625
train loss:  0.5260767340660095
train gradient:  0.13527556371610994
iteration : 7887
train acc:  0.703125
train loss:  0.568705677986145
train gradient:  0.16753241409522746
iteration : 7888
train acc:  0.7109375
train loss:  0.5238981246948242
train gradient:  0.1271864196229615
iteration : 7889
train acc:  0.7890625
train loss:  0.3994120955467224
train gradient:  0.12257062652416356
iteration : 7890
train acc:  0.71875
train loss:  0.5327129364013672
train gradient:  0.14033876937630563
iteration : 7891
train acc:  0.6875
train loss:  0.5685531497001648
train gradient:  0.19827776073169306
iteration : 7892
train acc:  0.6953125
train loss:  0.5258185863494873
train gradient:  0.20073497268753543
iteration : 7893
train acc:  0.6796875
train loss:  0.5500149130821228
train gradient:  0.17795822655674837
iteration : 7894
train acc:  0.7421875
train loss:  0.5102327466011047
train gradient:  0.13718469331556174
iteration : 7895
train acc:  0.6640625
train loss:  0.5979011058807373
train gradient:  0.1993031227569766
iteration : 7896
train acc:  0.8203125
train loss:  0.46450719237327576
train gradient:  0.13126690110516565
iteration : 7897
train acc:  0.7421875
train loss:  0.4784730076789856
train gradient:  0.11962285006904136
iteration : 7898
train acc:  0.78125
train loss:  0.46981239318847656
train gradient:  0.11761738010633742
iteration : 7899
train acc:  0.7578125
train loss:  0.5115559101104736
train gradient:  0.12707620134245923
iteration : 7900
train acc:  0.7578125
train loss:  0.48304712772369385
train gradient:  0.1901626952795463
iteration : 7901
train acc:  0.78125
train loss:  0.45162320137023926
train gradient:  0.13061513203569997
iteration : 7902
train acc:  0.7578125
train loss:  0.45315805077552795
train gradient:  0.11932275112157237
iteration : 7903
train acc:  0.75
train loss:  0.4670718312263489
train gradient:  0.12459229474783326
iteration : 7904
train acc:  0.734375
train loss:  0.5096657276153564
train gradient:  0.11287402218046914
iteration : 7905
train acc:  0.734375
train loss:  0.5323073863983154
train gradient:  0.15878260250847093
iteration : 7906
train acc:  0.71875
train loss:  0.49714019894599915
train gradient:  0.1243639003931024
iteration : 7907
train acc:  0.7265625
train loss:  0.521668016910553
train gradient:  0.11976192961323731
iteration : 7908
train acc:  0.6640625
train loss:  0.5868732333183289
train gradient:  0.15409744995049057
iteration : 7909
train acc:  0.7421875
train loss:  0.5020462870597839
train gradient:  0.12374201668065891
iteration : 7910
train acc:  0.78125
train loss:  0.4541628062725067
train gradient:  0.11848485267805942
iteration : 7911
train acc:  0.7890625
train loss:  0.48025548458099365
train gradient:  0.15750461244027705
iteration : 7912
train acc:  0.7578125
train loss:  0.5314891338348389
train gradient:  0.15295423073247857
iteration : 7913
train acc:  0.7578125
train loss:  0.5063892602920532
train gradient:  0.12765721023773585
iteration : 7914
train acc:  0.7265625
train loss:  0.5697158575057983
train gradient:  0.20409479184852014
iteration : 7915
train acc:  0.6640625
train loss:  0.5648171901702881
train gradient:  0.19403266177085476
iteration : 7916
train acc:  0.765625
train loss:  0.512130618095398
train gradient:  0.12937126852869946
iteration : 7917
train acc:  0.7265625
train loss:  0.5380125045776367
train gradient:  0.14039166226397137
iteration : 7918
train acc:  0.7265625
train loss:  0.5075411796569824
train gradient:  0.14942603404041793
iteration : 7919
train acc:  0.734375
train loss:  0.4985661804676056
train gradient:  0.1476722779810794
iteration : 7920
train acc:  0.7734375
train loss:  0.46911323070526123
train gradient:  0.13337208201159922
iteration : 7921
train acc:  0.7421875
train loss:  0.4588751196861267
train gradient:  0.1237423521798082
iteration : 7922
train acc:  0.75
train loss:  0.4594153165817261
train gradient:  0.13596476181306597
iteration : 7923
train acc:  0.71875
train loss:  0.4847589135169983
train gradient:  0.10708692241466775
iteration : 7924
train acc:  0.7421875
train loss:  0.5225845575332642
train gradient:  0.16502239274259267
iteration : 7925
train acc:  0.75
train loss:  0.49553993344306946
train gradient:  0.11060626014596632
iteration : 7926
train acc:  0.734375
train loss:  0.5194357633590698
train gradient:  0.12234356044342105
iteration : 7927
train acc:  0.7421875
train loss:  0.4395033121109009
train gradient:  0.10560608192680206
iteration : 7928
train acc:  0.765625
train loss:  0.45553627610206604
train gradient:  0.10866296523589482
iteration : 7929
train acc:  0.7578125
train loss:  0.4783117175102234
train gradient:  0.1487109483350859
iteration : 7930
train acc:  0.765625
train loss:  0.5186191201210022
train gradient:  0.12741396713002962
iteration : 7931
train acc:  0.703125
train loss:  0.5020917654037476
train gradient:  0.13905369975834259
iteration : 7932
train acc:  0.7890625
train loss:  0.4462457001209259
train gradient:  0.1740613389645363
iteration : 7933
train acc:  0.6875
train loss:  0.5845047235488892
train gradient:  0.17096028711724393
iteration : 7934
train acc:  0.7109375
train loss:  0.5120722055435181
train gradient:  0.13414362779203595
iteration : 7935
train acc:  0.7578125
train loss:  0.478549599647522
train gradient:  0.1293167334448742
iteration : 7936
train acc:  0.7421875
train loss:  0.5606077909469604
train gradient:  0.1288859050690047
iteration : 7937
train acc:  0.7421875
train loss:  0.4952608644962311
train gradient:  0.14387143884959547
iteration : 7938
train acc:  0.71875
train loss:  0.49138739705085754
train gradient:  0.12754419634781017
iteration : 7939
train acc:  0.71875
train loss:  0.49687495827674866
train gradient:  0.10890559552872599
iteration : 7940
train acc:  0.734375
train loss:  0.535526692867279
train gradient:  0.22900444513864443
iteration : 7941
train acc:  0.75
train loss:  0.49871009588241577
train gradient:  0.14676217093447963
iteration : 7942
train acc:  0.7421875
train loss:  0.5267777442932129
train gradient:  0.1766474209499978
iteration : 7943
train acc:  0.8125
train loss:  0.4565854072570801
train gradient:  0.13288059937330265
iteration : 7944
train acc:  0.734375
train loss:  0.5328949689865112
train gradient:  0.1190257864617883
iteration : 7945
train acc:  0.6875
train loss:  0.5233348608016968
train gradient:  0.1323954801829611
iteration : 7946
train acc:  0.734375
train loss:  0.5250462889671326
train gradient:  0.15073315863612602
iteration : 7947
train acc:  0.7421875
train loss:  0.5082465410232544
train gradient:  0.13088995159910782
iteration : 7948
train acc:  0.7578125
train loss:  0.4813084602355957
train gradient:  0.10491785453307625
iteration : 7949
train acc:  0.71875
train loss:  0.5131756067276001
train gradient:  0.10894506430100703
iteration : 7950
train acc:  0.703125
train loss:  0.5285422205924988
train gradient:  0.1768833902031029
iteration : 7951
train acc:  0.7109375
train loss:  0.5857005715370178
train gradient:  0.18760113557739094
iteration : 7952
train acc:  0.7265625
train loss:  0.5141644477844238
train gradient:  0.1447394489243281
iteration : 7953
train acc:  0.734375
train loss:  0.4941651523113251
train gradient:  0.1614567871685439
iteration : 7954
train acc:  0.640625
train loss:  0.5628296136856079
train gradient:  0.1595069597276127
iteration : 7955
train acc:  0.734375
train loss:  0.49176424741744995
train gradient:  0.11147280298634951
iteration : 7956
train acc:  0.7734375
train loss:  0.5003395080566406
train gradient:  0.11760821596863025
iteration : 7957
train acc:  0.765625
train loss:  0.48287588357925415
train gradient:  0.13913814274221872
iteration : 7958
train acc:  0.6953125
train loss:  0.5091092586517334
train gradient:  0.13805577041239142
iteration : 7959
train acc:  0.734375
train loss:  0.4828301966190338
train gradient:  0.11470030384298403
iteration : 7960
train acc:  0.734375
train loss:  0.5034599900245667
train gradient:  0.13752009202938714
iteration : 7961
train acc:  0.7578125
train loss:  0.4830835461616516
train gradient:  0.13470809358214342
iteration : 7962
train acc:  0.8046875
train loss:  0.44953155517578125
train gradient:  0.11727762791267911
iteration : 7963
train acc:  0.734375
train loss:  0.5393282175064087
train gradient:  0.13044685620665336
iteration : 7964
train acc:  0.6953125
train loss:  0.5388017296791077
train gradient:  0.17492993760652636
iteration : 7965
train acc:  0.671875
train loss:  0.5437230467796326
train gradient:  0.1474637900715275
iteration : 7966
train acc:  0.71875
train loss:  0.5207118988037109
train gradient:  0.13478085329836587
iteration : 7967
train acc:  0.6875
train loss:  0.5397029519081116
train gradient:  0.1728793161808912
iteration : 7968
train acc:  0.7421875
train loss:  0.5025650262832642
train gradient:  0.14484659670709168
iteration : 7969
train acc:  0.765625
train loss:  0.4970027804374695
train gradient:  0.12265608688067754
iteration : 7970
train acc:  0.71875
train loss:  0.4902326464653015
train gradient:  0.13790733801616187
iteration : 7971
train acc:  0.6796875
train loss:  0.5871514081954956
train gradient:  0.17522467060369462
iteration : 7972
train acc:  0.7421875
train loss:  0.45955947041511536
train gradient:  0.10887523655632378
iteration : 7973
train acc:  0.71875
train loss:  0.492061972618103
train gradient:  0.13855117661356636
iteration : 7974
train acc:  0.7734375
train loss:  0.4408271908760071
train gradient:  0.10918558095723391
iteration : 7975
train acc:  0.7109375
train loss:  0.48179101943969727
train gradient:  0.11597429187320123
iteration : 7976
train acc:  0.6875
train loss:  0.5434168577194214
train gradient:  0.18513919226690626
iteration : 7977
train acc:  0.78125
train loss:  0.4425888955593109
train gradient:  0.10698892265646187
iteration : 7978
train acc:  0.7109375
train loss:  0.5821303129196167
train gradient:  0.19472250115068823
iteration : 7979
train acc:  0.7421875
train loss:  0.4700581431388855
train gradient:  0.10867072434223035
iteration : 7980
train acc:  0.7265625
train loss:  0.47753435373306274
train gradient:  0.13398755268790624
iteration : 7981
train acc:  0.765625
train loss:  0.4648019075393677
train gradient:  0.10473054020008613
iteration : 7982
train acc:  0.7734375
train loss:  0.445771187543869
train gradient:  0.1392609414091262
iteration : 7983
train acc:  0.6875
train loss:  0.5666955709457397
train gradient:  0.19574523946027206
iteration : 7984
train acc:  0.7578125
train loss:  0.47712162137031555
train gradient:  0.11024685999105854
iteration : 7985
train acc:  0.7265625
train loss:  0.5352058410644531
train gradient:  0.15494508132517215
iteration : 7986
train acc:  0.7265625
train loss:  0.5089811086654663
train gradient:  0.12876139099286144
iteration : 7987
train acc:  0.796875
train loss:  0.4864323139190674
train gradient:  0.16258869513363747
iteration : 7988
train acc:  0.765625
train loss:  0.49253207445144653
train gradient:  0.1144467195641373
iteration : 7989
train acc:  0.7734375
train loss:  0.46501827239990234
train gradient:  0.11237795249759423
iteration : 7990
train acc:  0.75
train loss:  0.4806858003139496
train gradient:  0.16953627075175648
iteration : 7991
train acc:  0.7421875
train loss:  0.45427626371383667
train gradient:  0.14034813150250083
iteration : 7992
train acc:  0.78125
train loss:  0.4187353253364563
train gradient:  0.10089722794530338
iteration : 7993
train acc:  0.7109375
train loss:  0.5427468419075012
train gradient:  0.15981628911686202
iteration : 7994
train acc:  0.78125
train loss:  0.46287041902542114
train gradient:  0.10812528625648439
iteration : 7995
train acc:  0.671875
train loss:  0.5641783475875854
train gradient:  0.1985578196185145
iteration : 7996
train acc:  0.734375
train loss:  0.4790746569633484
train gradient:  0.1107354796688559
iteration : 7997
train acc:  0.6953125
train loss:  0.5530738234519958
train gradient:  0.1753349422289704
iteration : 7998
train acc:  0.7578125
train loss:  0.47922617197036743
train gradient:  0.1225732872961264
iteration : 7999
train acc:  0.6875
train loss:  0.550693154335022
train gradient:  0.1637872098597415
iteration : 8000
train acc:  0.7890625
train loss:  0.4721168875694275
train gradient:  0.12413601746121901
iteration : 8001
train acc:  0.734375
train loss:  0.5148347616195679
train gradient:  0.19370079558926184
iteration : 8002
train acc:  0.765625
train loss:  0.5222890377044678
train gradient:  0.16364847203355318
iteration : 8003
train acc:  0.7578125
train loss:  0.47224754095077515
train gradient:  0.10014927717729821
iteration : 8004
train acc:  0.7578125
train loss:  0.46937471628189087
train gradient:  0.13735381940055283
iteration : 8005
train acc:  0.7421875
train loss:  0.49635738134384155
train gradient:  0.15069117411972743
iteration : 8006
train acc:  0.75
train loss:  0.4917184114456177
train gradient:  0.12542416355686636
iteration : 8007
train acc:  0.7421875
train loss:  0.5254937410354614
train gradient:  0.13360592743132155
iteration : 8008
train acc:  0.734375
train loss:  0.4980163276195526
train gradient:  0.13118206051170989
iteration : 8009
train acc:  0.734375
train loss:  0.5058267116546631
train gradient:  0.1307629119426193
iteration : 8010
train acc:  0.796875
train loss:  0.44508546590805054
train gradient:  0.16110470489315193
iteration : 8011
train acc:  0.7265625
train loss:  0.5565623044967651
train gradient:  0.15989791135298476
iteration : 8012
train acc:  0.7421875
train loss:  0.4560490846633911
train gradient:  0.09085737442093265
iteration : 8013
train acc:  0.7265625
train loss:  0.5426475405693054
train gradient:  0.14857948402545837
iteration : 8014
train acc:  0.7265625
train loss:  0.48170626163482666
train gradient:  0.12691023903202572
iteration : 8015
train acc:  0.7890625
train loss:  0.4628167450428009
train gradient:  0.13582009229023273
iteration : 8016
train acc:  0.7890625
train loss:  0.4965630769729614
train gradient:  0.12788810987945795
iteration : 8017
train acc:  0.7578125
train loss:  0.4984663426876068
train gradient:  0.14305822823759518
iteration : 8018
train acc:  0.7578125
train loss:  0.49729180335998535
train gradient:  0.15423911293151193
iteration : 8019
train acc:  0.703125
train loss:  0.5377204418182373
train gradient:  0.15680151427613365
iteration : 8020
train acc:  0.7421875
train loss:  0.45880916714668274
train gradient:  0.13710482943127927
iteration : 8021
train acc:  0.7265625
train loss:  0.5351747274398804
train gradient:  0.1492538890812062
iteration : 8022
train acc:  0.7578125
train loss:  0.46178990602493286
train gradient:  0.1232413979314265
iteration : 8023
train acc:  0.7421875
train loss:  0.5109614133834839
train gradient:  0.13591012036598243
iteration : 8024
train acc:  0.75
train loss:  0.5212319493293762
train gradient:  0.14443881159244545
iteration : 8025
train acc:  0.734375
train loss:  0.4948333501815796
train gradient:  0.1404905799511508
iteration : 8026
train acc:  0.703125
train loss:  0.6002016067504883
train gradient:  0.2103458517929374
iteration : 8027
train acc:  0.7421875
train loss:  0.5104000568389893
train gradient:  0.1480980973317849
iteration : 8028
train acc:  0.703125
train loss:  0.5453211069107056
train gradient:  0.1885540199491892
iteration : 8029
train acc:  0.796875
train loss:  0.45418936014175415
train gradient:  0.1375870951243884
iteration : 8030
train acc:  0.7265625
train loss:  0.5140553712844849
train gradient:  0.15870513747949272
iteration : 8031
train acc:  0.78125
train loss:  0.5331737399101257
train gradient:  0.15345284709705026
iteration : 8032
train acc:  0.765625
train loss:  0.4788028299808502
train gradient:  0.14196291066599298
iteration : 8033
train acc:  0.765625
train loss:  0.471413254737854
train gradient:  0.11079099532152331
iteration : 8034
train acc:  0.6953125
train loss:  0.5444394946098328
train gradient:  0.1687766486880824
iteration : 8035
train acc:  0.75
train loss:  0.5822806358337402
train gradient:  0.2133137863204888
iteration : 8036
train acc:  0.734375
train loss:  0.5099232792854309
train gradient:  0.15670983912485426
iteration : 8037
train acc:  0.765625
train loss:  0.4808063507080078
train gradient:  0.1900005977528769
iteration : 8038
train acc:  0.7890625
train loss:  0.5073878765106201
train gradient:  0.17367456794160294
iteration : 8039
train acc:  0.75
train loss:  0.46588796377182007
train gradient:  0.12583301558589738
iteration : 8040
train acc:  0.7734375
train loss:  0.5059035420417786
train gradient:  0.1343724261399754
iteration : 8041
train acc:  0.765625
train loss:  0.49101361632347107
train gradient:  0.13166518162692115
iteration : 8042
train acc:  0.7421875
train loss:  0.4900788962841034
train gradient:  0.13005133387839485
iteration : 8043
train acc:  0.71875
train loss:  0.4529457092285156
train gradient:  0.11312817116549652
iteration : 8044
train acc:  0.75
train loss:  0.5358680486679077
train gradient:  0.14978712248204923
iteration : 8045
train acc:  0.796875
train loss:  0.4835943877696991
train gradient:  0.13464154414435525
iteration : 8046
train acc:  0.8359375
train loss:  0.40945616364479065
train gradient:  0.10562024363974735
iteration : 8047
train acc:  0.734375
train loss:  0.5115511417388916
train gradient:  0.15207798619616097
iteration : 8048
train acc:  0.8046875
train loss:  0.479970782995224
train gradient:  0.10681223309175059
iteration : 8049
train acc:  0.7265625
train loss:  0.4860142171382904
train gradient:  0.11382631370983488
iteration : 8050
train acc:  0.7265625
train loss:  0.5317738056182861
train gradient:  0.13097944432123476
iteration : 8051
train acc:  0.6875
train loss:  0.4844508767127991
train gradient:  0.15931727921903205
iteration : 8052
train acc:  0.7421875
train loss:  0.5217576026916504
train gradient:  0.1490647991714788
iteration : 8053
train acc:  0.765625
train loss:  0.4861908555030823
train gradient:  0.1155749473166564
iteration : 8054
train acc:  0.71875
train loss:  0.4918381869792938
train gradient:  0.1360497054883548
iteration : 8055
train acc:  0.7421875
train loss:  0.5780158042907715
train gradient:  0.16001383445256434
iteration : 8056
train acc:  0.6640625
train loss:  0.5431596040725708
train gradient:  0.13376469885117032
iteration : 8057
train acc:  0.6875
train loss:  0.5480723977088928
train gradient:  0.15035009368026872
iteration : 8058
train acc:  0.6875
train loss:  0.5344561338424683
train gradient:  0.15959477137640837
iteration : 8059
train acc:  0.7421875
train loss:  0.47786831855773926
train gradient:  0.12654083423104623
iteration : 8060
train acc:  0.796875
train loss:  0.4406251311302185
train gradient:  0.12969719232341226
iteration : 8061
train acc:  0.75
train loss:  0.5046937465667725
train gradient:  0.13816449372527134
iteration : 8062
train acc:  0.7265625
train loss:  0.5375730991363525
train gradient:  0.14532375191472774
iteration : 8063
train acc:  0.7265625
train loss:  0.5420633554458618
train gradient:  0.13861613370035014
iteration : 8064
train acc:  0.703125
train loss:  0.4853637218475342
train gradient:  0.13008228836831404
iteration : 8065
train acc:  0.75
train loss:  0.4977549910545349
train gradient:  0.1229678058184126
iteration : 8066
train acc:  0.703125
train loss:  0.5234793424606323
train gradient:  0.14781409397351603
iteration : 8067
train acc:  0.703125
train loss:  0.5513423681259155
train gradient:  0.17468082872326357
iteration : 8068
train acc:  0.703125
train loss:  0.5359206199645996
train gradient:  0.15914629600587665
iteration : 8069
train acc:  0.6953125
train loss:  0.5139894485473633
train gradient:  0.12934309907966324
iteration : 8070
train acc:  0.71875
train loss:  0.5406129360198975
train gradient:  0.14292222855523243
iteration : 8071
train acc:  0.7265625
train loss:  0.5011411905288696
train gradient:  0.2151547822343864
iteration : 8072
train acc:  0.8046875
train loss:  0.4354618787765503
train gradient:  0.09114036014381163
iteration : 8073
train acc:  0.7890625
train loss:  0.44357866048812866
train gradient:  0.12825467707224336
iteration : 8074
train acc:  0.6875
train loss:  0.5398352742195129
train gradient:  0.1714470132186814
iteration : 8075
train acc:  0.7421875
train loss:  0.48199138045310974
train gradient:  0.151422898052182
iteration : 8076
train acc:  0.828125
train loss:  0.42743629217147827
train gradient:  0.10321428637961315
iteration : 8077
train acc:  0.7265625
train loss:  0.5194588303565979
train gradient:  0.15537923945629561
iteration : 8078
train acc:  0.6875
train loss:  0.5431605577468872
train gradient:  0.15125604311089227
iteration : 8079
train acc:  0.8359375
train loss:  0.4272562265396118
train gradient:  0.12621671693127381
iteration : 8080
train acc:  0.6953125
train loss:  0.5149566531181335
train gradient:  0.1642676071309186
iteration : 8081
train acc:  0.7265625
train loss:  0.5239391326904297
train gradient:  0.1877370722602976
iteration : 8082
train acc:  0.734375
train loss:  0.5328287482261658
train gradient:  0.169870498984243
iteration : 8083
train acc:  0.75
train loss:  0.4515319764614105
train gradient:  0.13714204684414133
iteration : 8084
train acc:  0.6953125
train loss:  0.5669281482696533
train gradient:  0.20851044269523378
iteration : 8085
train acc:  0.7109375
train loss:  0.555059552192688
train gradient:  0.21818893180350551
iteration : 8086
train acc:  0.765625
train loss:  0.5057165622711182
train gradient:  0.15198818784326074
iteration : 8087
train acc:  0.7265625
train loss:  0.5134263038635254
train gradient:  0.1376964446395486
iteration : 8088
train acc:  0.734375
train loss:  0.5045287013053894
train gradient:  0.1350762941263326
iteration : 8089
train acc:  0.75
train loss:  0.49097228050231934
train gradient:  0.12320544893015059
iteration : 8090
train acc:  0.7578125
train loss:  0.5274539589881897
train gradient:  0.14338949597330763
iteration : 8091
train acc:  0.7421875
train loss:  0.4777067005634308
train gradient:  0.11894371854510981
iteration : 8092
train acc:  0.7265625
train loss:  0.5485266447067261
train gradient:  0.14863225681859427
iteration : 8093
train acc:  0.671875
train loss:  0.5296188592910767
train gradient:  0.16417429739921213
iteration : 8094
train acc:  0.734375
train loss:  0.49770021438598633
train gradient:  0.13255661874493013
iteration : 8095
train acc:  0.671875
train loss:  0.5517120361328125
train gradient:  0.154879461936536
iteration : 8096
train acc:  0.671875
train loss:  0.520799994468689
train gradient:  0.13334117781914628
iteration : 8097
train acc:  0.765625
train loss:  0.4537566304206848
train gradient:  0.1167683993315909
iteration : 8098
train acc:  0.7265625
train loss:  0.5384756326675415
train gradient:  0.15714833688612
iteration : 8099
train acc:  0.7109375
train loss:  0.5288138389587402
train gradient:  0.14275031371490693
iteration : 8100
train acc:  0.7421875
train loss:  0.5153538584709167
train gradient:  0.15597837067530412
iteration : 8101
train acc:  0.828125
train loss:  0.42862415313720703
train gradient:  0.1353908069057331
iteration : 8102
train acc:  0.765625
train loss:  0.45423316955566406
train gradient:  0.12107924788311646
iteration : 8103
train acc:  0.7421875
train loss:  0.5161892175674438
train gradient:  0.14233857828904029
iteration : 8104
train acc:  0.7734375
train loss:  0.48580706119537354
train gradient:  0.1742107310461291
iteration : 8105
train acc:  0.734375
train loss:  0.5277435779571533
train gradient:  0.14480068089416046
iteration : 8106
train acc:  0.703125
train loss:  0.5240358114242554
train gradient:  0.1960298816837854
iteration : 8107
train acc:  0.7265625
train loss:  0.5158488154411316
train gradient:  0.1454823635704067
iteration : 8108
train acc:  0.7734375
train loss:  0.4309563934803009
train gradient:  0.10482175239727244
iteration : 8109
train acc:  0.75
train loss:  0.4933636784553528
train gradient:  0.17594114313535664
iteration : 8110
train acc:  0.7265625
train loss:  0.520088255405426
train gradient:  0.14349437155785677
iteration : 8111
train acc:  0.7265625
train loss:  0.5020174384117126
train gradient:  0.14421547531095963
iteration : 8112
train acc:  0.7265625
train loss:  0.5298042297363281
train gradient:  0.13590786372768882
iteration : 8113
train acc:  0.7890625
train loss:  0.45210587978363037
train gradient:  0.14119988243538434
iteration : 8114
train acc:  0.71875
train loss:  0.49704763293266296
train gradient:  0.14238348706975792
iteration : 8115
train acc:  0.734375
train loss:  0.5148999691009521
train gradient:  0.13277185671046993
iteration : 8116
train acc:  0.7578125
train loss:  0.4425138831138611
train gradient:  0.09359007142494583
iteration : 8117
train acc:  0.7890625
train loss:  0.4635782241821289
train gradient:  0.1176936941089491
iteration : 8118
train acc:  0.7265625
train loss:  0.5294419527053833
train gradient:  0.1599537653372789
iteration : 8119
train acc:  0.765625
train loss:  0.4674803912639618
train gradient:  0.12820429289823126
iteration : 8120
train acc:  0.75
train loss:  0.49728745222091675
train gradient:  0.14228562637154496
iteration : 8121
train acc:  0.7578125
train loss:  0.46067655086517334
train gradient:  0.13336121916601998
iteration : 8122
train acc:  0.84375
train loss:  0.3975350260734558
train gradient:  0.14357708542832498
iteration : 8123
train acc:  0.78125
train loss:  0.44398045539855957
train gradient:  0.09656902089989015
iteration : 8124
train acc:  0.7578125
train loss:  0.49766024947166443
train gradient:  0.12622820670372037
iteration : 8125
train acc:  0.71875
train loss:  0.5134631395339966
train gradient:  0.12211720495469469
iteration : 8126
train acc:  0.7890625
train loss:  0.48685523867607117
train gradient:  0.1480844045533459
iteration : 8127
train acc:  0.703125
train loss:  0.5501171350479126
train gradient:  0.1432873746131214
iteration : 8128
train acc:  0.796875
train loss:  0.4771747589111328
train gradient:  0.13740971604846303
iteration : 8129
train acc:  0.78125
train loss:  0.47137323021888733
train gradient:  0.15275679042545248
iteration : 8130
train acc:  0.75
train loss:  0.45205020904541016
train gradient:  0.11700267867263361
iteration : 8131
train acc:  0.828125
train loss:  0.40819811820983887
train gradient:  0.10392124324447222
iteration : 8132
train acc:  0.7890625
train loss:  0.46415087580680847
train gradient:  0.12996970059188706
iteration : 8133
train acc:  0.71875
train loss:  0.5327249765396118
train gradient:  0.17201018719385278
iteration : 8134
train acc:  0.7421875
train loss:  0.5152275562286377
train gradient:  0.19577115927920086
iteration : 8135
train acc:  0.7734375
train loss:  0.47789719700813293
train gradient:  0.13674265249288137
iteration : 8136
train acc:  0.75
train loss:  0.4834141433238983
train gradient:  0.13331830738611475
iteration : 8137
train acc:  0.7421875
train loss:  0.5154352188110352
train gradient:  0.14579279727125694
iteration : 8138
train acc:  0.6953125
train loss:  0.6346261501312256
train gradient:  0.2172175984259609
iteration : 8139
train acc:  0.6953125
train loss:  0.5705142617225647
train gradient:  0.17154209293475192
iteration : 8140
train acc:  0.71875
train loss:  0.5292778611183167
train gradient:  0.14744441290940247
iteration : 8141
train acc:  0.7109375
train loss:  0.5016904473304749
train gradient:  0.1273500710329351
iteration : 8142
train acc:  0.8046875
train loss:  0.49010199308395386
train gradient:  0.12120877617210783
iteration : 8143
train acc:  0.8046875
train loss:  0.45667600631713867
train gradient:  0.13746677770175877
iteration : 8144
train acc:  0.7578125
train loss:  0.5113288164138794
train gradient:  0.14379549336487957
iteration : 8145
train acc:  0.6640625
train loss:  0.5592395067214966
train gradient:  0.13555858349069694
iteration : 8146
train acc:  0.703125
train loss:  0.5400253534317017
train gradient:  0.1585876898276431
iteration : 8147
train acc:  0.765625
train loss:  0.49841347336769104
train gradient:  0.1286672009893633
iteration : 8148
train acc:  0.7421875
train loss:  0.5257594585418701
train gradient:  0.2600812273395685
iteration : 8149
train acc:  0.765625
train loss:  0.472952276468277
train gradient:  0.1295876587480676
iteration : 8150
train acc:  0.7578125
train loss:  0.5235706567764282
train gradient:  0.17470307714810532
iteration : 8151
train acc:  0.765625
train loss:  0.45442479848861694
train gradient:  0.11548858681222646
iteration : 8152
train acc:  0.7265625
train loss:  0.49808168411254883
train gradient:  0.13173796654576816
iteration : 8153
train acc:  0.8125
train loss:  0.43562936782836914
train gradient:  0.0878150209227715
iteration : 8154
train acc:  0.7265625
train loss:  0.5581426620483398
train gradient:  0.17220066379607185
iteration : 8155
train acc:  0.7109375
train loss:  0.5038372278213501
train gradient:  0.1227520997499854
iteration : 8156
train acc:  0.7890625
train loss:  0.4698030352592468
train gradient:  0.12662548942253266
iteration : 8157
train acc:  0.75
train loss:  0.4717206358909607
train gradient:  0.13032470230026666
iteration : 8158
train acc:  0.7421875
train loss:  0.4653502404689789
train gradient:  0.11847645774905265
iteration : 8159
train acc:  0.78125
train loss:  0.4671201705932617
train gradient:  0.11365555140065417
iteration : 8160
train acc:  0.7109375
train loss:  0.5104351043701172
train gradient:  0.11627846561412288
iteration : 8161
train acc:  0.78125
train loss:  0.47168463468551636
train gradient:  0.1098504292781123
iteration : 8162
train acc:  0.765625
train loss:  0.47149360179901123
train gradient:  0.14254508939754407
iteration : 8163
train acc:  0.734375
train loss:  0.5137717723846436
train gradient:  0.16182146201600406
iteration : 8164
train acc:  0.7421875
train loss:  0.5077195167541504
train gradient:  0.15052659305762894
iteration : 8165
train acc:  0.828125
train loss:  0.41972365975379944
train gradient:  0.12729644694989428
iteration : 8166
train acc:  0.7421875
train loss:  0.4947887361049652
train gradient:  0.13027652233642067
iteration : 8167
train acc:  0.7109375
train loss:  0.5488196015357971
train gradient:  0.16860297358191095
iteration : 8168
train acc:  0.6875
train loss:  0.5581149458885193
train gradient:  0.15563002216066615
iteration : 8169
train acc:  0.6953125
train loss:  0.5434237718582153
train gradient:  0.1727643203116136
iteration : 8170
train acc:  0.71875
train loss:  0.5129395723342896
train gradient:  0.16141080717332873
iteration : 8171
train acc:  0.6796875
train loss:  0.5683056116104126
train gradient:  0.21044341020493723
iteration : 8172
train acc:  0.796875
train loss:  0.4006321430206299
train gradient:  0.09929162137654272
iteration : 8173
train acc:  0.6796875
train loss:  0.5442779660224915
train gradient:  0.1849182870009174
iteration : 8174
train acc:  0.7578125
train loss:  0.47929561138153076
train gradient:  0.16317776885721397
iteration : 8175
train acc:  0.7578125
train loss:  0.5222006440162659
train gradient:  0.1958515408808068
iteration : 8176
train acc:  0.7578125
train loss:  0.47544732689857483
train gradient:  0.1920427177801391
iteration : 8177
train acc:  0.7109375
train loss:  0.4670655131340027
train gradient:  0.12734999763048907
iteration : 8178
train acc:  0.7421875
train loss:  0.5401332974433899
train gradient:  0.2023418580992405
iteration : 8179
train acc:  0.765625
train loss:  0.44761940836906433
train gradient:  0.11225268319405501
iteration : 8180
train acc:  0.78125
train loss:  0.4845128655433655
train gradient:  0.14681130170493076
iteration : 8181
train acc:  0.765625
train loss:  0.49194812774658203
train gradient:  0.15000010520486673
iteration : 8182
train acc:  0.734375
train loss:  0.4854518175125122
train gradient:  0.1729491238117759
iteration : 8183
train acc:  0.7421875
train loss:  0.48657259345054626
train gradient:  0.1306944674046998
iteration : 8184
train acc:  0.765625
train loss:  0.5061745047569275
train gradient:  0.1267361570829023
iteration : 8185
train acc:  0.765625
train loss:  0.5157777070999146
train gradient:  0.15779898765388323
iteration : 8186
train acc:  0.71875
train loss:  0.5208388566970825
train gradient:  0.14273306872405198
iteration : 8187
train acc:  0.6953125
train loss:  0.549881100654602
train gradient:  0.14791510144330378
iteration : 8188
train acc:  0.75
train loss:  0.513329029083252
train gradient:  0.15850887109950207
iteration : 8189
train acc:  0.671875
train loss:  0.5841357111930847
train gradient:  0.17024181143218636
iteration : 8190
train acc:  0.8046875
train loss:  0.45689162611961365
train gradient:  0.22046032717394543
iteration : 8191
train acc:  0.734375
train loss:  0.514035701751709
train gradient:  0.1916496141890327
iteration : 8192
train acc:  0.65625
train loss:  0.6197648644447327
train gradient:  0.1910183559495233
iteration : 8193
train acc:  0.7109375
train loss:  0.5407599210739136
train gradient:  0.1217656603123046
iteration : 8194
train acc:  0.8203125
train loss:  0.391414999961853
train gradient:  0.10505946524934481
iteration : 8195
train acc:  0.796875
train loss:  0.4717566668987274
train gradient:  0.12451384222234659
iteration : 8196
train acc:  0.7109375
train loss:  0.593892514705658
train gradient:  0.20085554174236467
iteration : 8197
train acc:  0.71875
train loss:  0.4946887791156769
train gradient:  0.12938726531069583
iteration : 8198
train acc:  0.7734375
train loss:  0.4601749777793884
train gradient:  0.12766946893863215
iteration : 8199
train acc:  0.7109375
train loss:  0.5263490676879883
train gradient:  0.15331669129619602
iteration : 8200
train acc:  0.7421875
train loss:  0.5055230259895325
train gradient:  0.1385150396611105
iteration : 8201
train acc:  0.734375
train loss:  0.5380208492279053
train gradient:  0.18211088444591556
iteration : 8202
train acc:  0.703125
train loss:  0.5319230556488037
train gradient:  0.14117519145227947
iteration : 8203
train acc:  0.8203125
train loss:  0.43238532543182373
train gradient:  0.12119267867326305
iteration : 8204
train acc:  0.7421875
train loss:  0.5053091049194336
train gradient:  0.16557619924423345
iteration : 8205
train acc:  0.796875
train loss:  0.4436226189136505
train gradient:  0.12855497739021732
iteration : 8206
train acc:  0.71875
train loss:  0.5325418710708618
train gradient:  0.1680455394041207
iteration : 8207
train acc:  0.703125
train loss:  0.5879775285720825
train gradient:  0.17783910393279384
iteration : 8208
train acc:  0.703125
train loss:  0.5545878410339355
train gradient:  0.18869243324031798
iteration : 8209
train acc:  0.7734375
train loss:  0.4887098968029022
train gradient:  0.11861096985576204
iteration : 8210
train acc:  0.7265625
train loss:  0.5709208846092224
train gradient:  0.16389363037039772
iteration : 8211
train acc:  0.796875
train loss:  0.45828777551651
train gradient:  0.12098852278282232
iteration : 8212
train acc:  0.78125
train loss:  0.48458242416381836
train gradient:  0.1297235327085349
iteration : 8213
train acc:  0.75
train loss:  0.4849323034286499
train gradient:  0.12917324268889785
iteration : 8214
train acc:  0.7265625
train loss:  0.5285940170288086
train gradient:  0.1362066986974517
iteration : 8215
train acc:  0.703125
train loss:  0.5461119413375854
train gradient:  0.1868162249837868
iteration : 8216
train acc:  0.765625
train loss:  0.5022857189178467
train gradient:  0.14631212193872875
iteration : 8217
train acc:  0.7578125
train loss:  0.4850880801677704
train gradient:  0.15502007181602825
iteration : 8218
train acc:  0.7265625
train loss:  0.5520137548446655
train gradient:  0.14562768306733948
iteration : 8219
train acc:  0.71875
train loss:  0.5132205486297607
train gradient:  0.14118141356008979
iteration : 8220
train acc:  0.7578125
train loss:  0.5155267715454102
train gradient:  0.1927356006251445
iteration : 8221
train acc:  0.765625
train loss:  0.5314093828201294
train gradient:  0.1887064556774496
iteration : 8222
train acc:  0.6875
train loss:  0.547534704208374
train gradient:  0.1751034867423601
iteration : 8223
train acc:  0.75
train loss:  0.5123695731163025
train gradient:  0.1213483497676839
iteration : 8224
train acc:  0.78125
train loss:  0.42613500356674194
train gradient:  0.10635253173174826
iteration : 8225
train acc:  0.71875
train loss:  0.4808952510356903
train gradient:  0.14989037577801728
iteration : 8226
train acc:  0.8515625
train loss:  0.4224068224430084
train gradient:  0.0862499441934554
iteration : 8227
train acc:  0.7265625
train loss:  0.5674444437026978
train gradient:  0.15229897327324965
iteration : 8228
train acc:  0.6875
train loss:  0.5196675062179565
train gradient:  0.14552986568943388
iteration : 8229
train acc:  0.7265625
train loss:  0.5760728120803833
train gradient:  0.19567162590633896
iteration : 8230
train acc:  0.75
train loss:  0.47948163747787476
train gradient:  0.11479680777724871
iteration : 8231
train acc:  0.7734375
train loss:  0.5022606253623962
train gradient:  0.15508017740887425
iteration : 8232
train acc:  0.6953125
train loss:  0.5050581693649292
train gradient:  0.18776323707053433
iteration : 8233
train acc:  0.71875
train loss:  0.5123196244239807
train gradient:  0.12983546012452174
iteration : 8234
train acc:  0.7265625
train loss:  0.48638397455215454
train gradient:  0.10293255176059686
iteration : 8235
train acc:  0.8046875
train loss:  0.4521391987800598
train gradient:  0.10760218764002986
iteration : 8236
train acc:  0.71875
train loss:  0.5552973747253418
train gradient:  0.18325585343659823
iteration : 8237
train acc:  0.7421875
train loss:  0.4689978361129761
train gradient:  0.09628579759595013
iteration : 8238
train acc:  0.7578125
train loss:  0.4708353579044342
train gradient:  0.10496367823131565
iteration : 8239
train acc:  0.7734375
train loss:  0.4437970519065857
train gradient:  0.0994563446459936
iteration : 8240
train acc:  0.734375
train loss:  0.49218887090682983
train gradient:  0.11427587992144204
iteration : 8241
train acc:  0.6640625
train loss:  0.5625840425491333
train gradient:  0.18075837002495102
iteration : 8242
train acc:  0.7265625
train loss:  0.553718090057373
train gradient:  0.13784030158467242
iteration : 8243
train acc:  0.7734375
train loss:  0.46868133544921875
train gradient:  0.11687695572857706
iteration : 8244
train acc:  0.7109375
train loss:  0.5311769843101501
train gradient:  0.13350896559426562
iteration : 8245
train acc:  0.671875
train loss:  0.5304082036018372
train gradient:  0.16144614923783385
iteration : 8246
train acc:  0.75
train loss:  0.4876813590526581
train gradient:  0.13311285206067083
iteration : 8247
train acc:  0.734375
train loss:  0.48692139983177185
train gradient:  0.15090463506784824
iteration : 8248
train acc:  0.734375
train loss:  0.5187541842460632
train gradient:  0.157338345495959
iteration : 8249
train acc:  0.8125
train loss:  0.3990541100502014
train gradient:  0.10478593401570321
iteration : 8250
train acc:  0.765625
train loss:  0.4899875521659851
train gradient:  0.12808732871756956
iteration : 8251
train acc:  0.75
train loss:  0.4844491481781006
train gradient:  0.14232538999364402
iteration : 8252
train acc:  0.6953125
train loss:  0.4760082960128784
train gradient:  0.11937192957511122
iteration : 8253
train acc:  0.7890625
train loss:  0.4488576650619507
train gradient:  0.11359943628866663
iteration : 8254
train acc:  0.71875
train loss:  0.5191195011138916
train gradient:  0.13791137257917144
iteration : 8255
train acc:  0.75
train loss:  0.4617133140563965
train gradient:  0.13366609857222306
iteration : 8256
train acc:  0.7890625
train loss:  0.4893345534801483
train gradient:  0.148028964877025
iteration : 8257
train acc:  0.7890625
train loss:  0.4466080665588379
train gradient:  0.10585374445442607
iteration : 8258
train acc:  0.6796875
train loss:  0.5830754041671753
train gradient:  0.19392447325860762
iteration : 8259
train acc:  0.7578125
train loss:  0.46104496717453003
train gradient:  0.12716376739029445
iteration : 8260
train acc:  0.7265625
train loss:  0.5472201108932495
train gradient:  0.17476488581140132
iteration : 8261
train acc:  0.7421875
train loss:  0.46791332960128784
train gradient:  0.13565378385741517
iteration : 8262
train acc:  0.7265625
train loss:  0.5021914839744568
train gradient:  0.1647173390687289
iteration : 8263
train acc:  0.734375
train loss:  0.5249745845794678
train gradient:  0.14433622521839845
iteration : 8264
train acc:  0.6953125
train loss:  0.5296260714530945
train gradient:  0.11810030859137426
iteration : 8265
train acc:  0.734375
train loss:  0.5435031652450562
train gradient:  0.1666584929204124
iteration : 8266
train acc:  0.7421875
train loss:  0.4719037413597107
train gradient:  0.12462675056343986
iteration : 8267
train acc:  0.71875
train loss:  0.4734039306640625
train gradient:  0.11498356205757283
iteration : 8268
train acc:  0.734375
train loss:  0.495997816324234
train gradient:  0.14477338895738928
iteration : 8269
train acc:  0.7109375
train loss:  0.5231956243515015
train gradient:  0.1382340914017127
iteration : 8270
train acc:  0.7578125
train loss:  0.5014010667800903
train gradient:  0.1356140645446448
iteration : 8271
train acc:  0.78125
train loss:  0.4757009744644165
train gradient:  0.10971953089408426
iteration : 8272
train acc:  0.734375
train loss:  0.5160437822341919
train gradient:  0.15146653224885792
iteration : 8273
train acc:  0.7578125
train loss:  0.4388309717178345
train gradient:  0.11615551771682003
iteration : 8274
train acc:  0.7734375
train loss:  0.49658092856407166
train gradient:  0.1438915271157911
iteration : 8275
train acc:  0.6796875
train loss:  0.5261131525039673
train gradient:  0.1594687682043388
iteration : 8276
train acc:  0.75
train loss:  0.5171589255332947
train gradient:  0.16517373650168407
iteration : 8277
train acc:  0.7890625
train loss:  0.4251219630241394
train gradient:  0.09479034328840177
iteration : 8278
train acc:  0.7421875
train loss:  0.49582967162132263
train gradient:  0.15699121615270906
iteration : 8279
train acc:  0.6875
train loss:  0.5663433074951172
train gradient:  0.1624279058503441
iteration : 8280
train acc:  0.75
train loss:  0.49562764167785645
train gradient:  0.12833858673426293
iteration : 8281
train acc:  0.6953125
train loss:  0.5178936719894409
train gradient:  0.13842386463730405
iteration : 8282
train acc:  0.8046875
train loss:  0.4720616638660431
train gradient:  0.1221567735699696
iteration : 8283
train acc:  0.7734375
train loss:  0.4729946255683899
train gradient:  0.16182352987356458
iteration : 8284
train acc:  0.7109375
train loss:  0.5663954019546509
train gradient:  0.14654565631877675
iteration : 8285
train acc:  0.71875
train loss:  0.5497862696647644
train gradient:  0.17743696275313067
iteration : 8286
train acc:  0.71875
train loss:  0.5227304100990295
train gradient:  0.14770317275411415
iteration : 8287
train acc:  0.75
train loss:  0.5000706911087036
train gradient:  0.13271033913322217
iteration : 8288
train acc:  0.6484375
train loss:  0.5996606349945068
train gradient:  0.21421464257678574
iteration : 8289
train acc:  0.71875
train loss:  0.5114070773124695
train gradient:  0.17924572067460792
iteration : 8290
train acc:  0.7421875
train loss:  0.5002071857452393
train gradient:  0.12290821834131574
iteration : 8291
train acc:  0.734375
train loss:  0.48138192296028137
train gradient:  0.18632946028622505
iteration : 8292
train acc:  0.7421875
train loss:  0.5403945446014404
train gradient:  0.15394924154941125
iteration : 8293
train acc:  0.7734375
train loss:  0.4822864234447479
train gradient:  0.10239861375709872
iteration : 8294
train acc:  0.6875
train loss:  0.5585589408874512
train gradient:  0.17244802663293535
iteration : 8295
train acc:  0.703125
train loss:  0.5169851779937744
train gradient:  0.13612790594508378
iteration : 8296
train acc:  0.71875
train loss:  0.5059747099876404
train gradient:  0.16483582513291486
iteration : 8297
train acc:  0.765625
train loss:  0.4487477242946625
train gradient:  0.11396727840217385
iteration : 8298
train acc:  0.703125
train loss:  0.4850921630859375
train gradient:  0.1276407071556022
iteration : 8299
train acc:  0.78125
train loss:  0.4832812547683716
train gradient:  0.11913945795679134
iteration : 8300
train acc:  0.7578125
train loss:  0.543320894241333
train gradient:  0.19727633894496013
iteration : 8301
train acc:  0.7109375
train loss:  0.5276315212249756
train gradient:  0.1751464942501762
iteration : 8302
train acc:  0.7578125
train loss:  0.5139735341072083
train gradient:  0.15478355479126865
iteration : 8303
train acc:  0.859375
train loss:  0.4145808219909668
train gradient:  0.09110530319475943
iteration : 8304
train acc:  0.8046875
train loss:  0.4679366946220398
train gradient:  0.1454946995448915
iteration : 8305
train acc:  0.8046875
train loss:  0.4458433985710144
train gradient:  0.134838952240835
iteration : 8306
train acc:  0.7734375
train loss:  0.43898046016693115
train gradient:  0.16072695429778375
iteration : 8307
train acc:  0.703125
train loss:  0.4906843900680542
train gradient:  0.16455090423951244
iteration : 8308
train acc:  0.71875
train loss:  0.5522530674934387
train gradient:  0.15112884044582173
iteration : 8309
train acc:  0.7265625
train loss:  0.5417960286140442
train gradient:  0.15389728772009065
iteration : 8310
train acc:  0.6796875
train loss:  0.5674850940704346
train gradient:  0.17535816426301726
iteration : 8311
train acc:  0.6796875
train loss:  0.5956557989120483
train gradient:  0.21041695880603478
iteration : 8312
train acc:  0.7265625
train loss:  0.5187860131263733
train gradient:  0.1731342623514252
iteration : 8313
train acc:  0.7578125
train loss:  0.4542800486087799
train gradient:  0.10112087813163224
iteration : 8314
train acc:  0.8203125
train loss:  0.4300268590450287
train gradient:  0.12447273214940538
iteration : 8315
train acc:  0.7265625
train loss:  0.5523741245269775
train gradient:  0.14541002864298427
iteration : 8316
train acc:  0.7265625
train loss:  0.5184890031814575
train gradient:  0.18283323950922353
iteration : 8317
train acc:  0.8046875
train loss:  0.4440540075302124
train gradient:  0.08788023105174292
iteration : 8318
train acc:  0.734375
train loss:  0.4997721016407013
train gradient:  0.14189033824403724
iteration : 8319
train acc:  0.8046875
train loss:  0.43749526143074036
train gradient:  0.13471737543548873
iteration : 8320
train acc:  0.7265625
train loss:  0.5065430998802185
train gradient:  0.12820255455712515
iteration : 8321
train acc:  0.765625
train loss:  0.4681505560874939
train gradient:  0.13204219282146068
iteration : 8322
train acc:  0.734375
train loss:  0.5187544822692871
train gradient:  0.12890219431415836
iteration : 8323
train acc:  0.78125
train loss:  0.449673593044281
train gradient:  0.11409618774691145
iteration : 8324
train acc:  0.7890625
train loss:  0.4587087333202362
train gradient:  0.12042273034623607
iteration : 8325
train acc:  0.734375
train loss:  0.4898749887943268
train gradient:  0.13306083927293338
iteration : 8326
train acc:  0.703125
train loss:  0.5679306983947754
train gradient:  0.1582172601612093
iteration : 8327
train acc:  0.6875
train loss:  0.5429511070251465
train gradient:  0.13786730700579025
iteration : 8328
train acc:  0.78125
train loss:  0.4749830961227417
train gradient:  0.16094779197582032
iteration : 8329
train acc:  0.75
train loss:  0.5242308378219604
train gradient:  0.12107900521282192
iteration : 8330
train acc:  0.7890625
train loss:  0.43709176778793335
train gradient:  0.11474398844542516
iteration : 8331
train acc:  0.6015625
train loss:  0.5870346426963806
train gradient:  0.2161021525282577
iteration : 8332
train acc:  0.71875
train loss:  0.5350606441497803
train gradient:  0.15277042070657262
iteration : 8333
train acc:  0.7265625
train loss:  0.502951979637146
train gradient:  0.13886663704288357
iteration : 8334
train acc:  0.796875
train loss:  0.4224364459514618
train gradient:  0.10758543262871983
iteration : 8335
train acc:  0.734375
train loss:  0.5220706462860107
train gradient:  0.1444792822903086
iteration : 8336
train acc:  0.796875
train loss:  0.40522247552871704
train gradient:  0.11313383614378066
iteration : 8337
train acc:  0.6796875
train loss:  0.618315577507019
train gradient:  0.16284934656753774
iteration : 8338
train acc:  0.65625
train loss:  0.6173028945922852
train gradient:  0.18872956972301796
iteration : 8339
train acc:  0.71875
train loss:  0.48641979694366455
train gradient:  0.12380471922959645
iteration : 8340
train acc:  0.7578125
train loss:  0.4997328519821167
train gradient:  0.12566051036597095
iteration : 8341
train acc:  0.7578125
train loss:  0.4920256733894348
train gradient:  0.14337166348100877
iteration : 8342
train acc:  0.765625
train loss:  0.4982815682888031
train gradient:  0.09884349679822095
iteration : 8343
train acc:  0.765625
train loss:  0.4757588803768158
train gradient:  0.1582455616447538
iteration : 8344
train acc:  0.8046875
train loss:  0.45912495255470276
train gradient:  0.13029584650737788
iteration : 8345
train acc:  0.71875
train loss:  0.49373605847358704
train gradient:  0.13140071915441176
iteration : 8346
train acc:  0.7421875
train loss:  0.5146371126174927
train gradient:  0.14781807458955531
iteration : 8347
train acc:  0.765625
train loss:  0.5211509466171265
train gradient:  0.14531703064256504
iteration : 8348
train acc:  0.734375
train loss:  0.48761269450187683
train gradient:  0.1043399704265645
iteration : 8349
train acc:  0.78125
train loss:  0.47319990396499634
train gradient:  0.14125671447000882
iteration : 8350
train acc:  0.765625
train loss:  0.4656716287136078
train gradient:  0.09420339427434384
iteration : 8351
train acc:  0.765625
train loss:  0.5375916957855225
train gradient:  0.13055644738094135
iteration : 8352
train acc:  0.78125
train loss:  0.4405544400215149
train gradient:  0.10115184342263071
iteration : 8353
train acc:  0.6875
train loss:  0.49408644437789917
train gradient:  0.1334849579310391
iteration : 8354
train acc:  0.796875
train loss:  0.4483823776245117
train gradient:  0.10440126348636958
iteration : 8355
train acc:  0.6640625
train loss:  0.5936286449432373
train gradient:  0.18503467184305067
iteration : 8356
train acc:  0.7890625
train loss:  0.45752158761024475
train gradient:  0.12528316204334078
iteration : 8357
train acc:  0.7421875
train loss:  0.5527222156524658
train gradient:  0.18737138750410506
iteration : 8358
train acc:  0.6875
train loss:  0.5777682065963745
train gradient:  0.16727472276799119
iteration : 8359
train acc:  0.7109375
train loss:  0.517188310623169
train gradient:  0.1562386264569282
iteration : 8360
train acc:  0.75
train loss:  0.49059948325157166
train gradient:  0.10489080950020009
iteration : 8361
train acc:  0.7421875
train loss:  0.5505030155181885
train gradient:  0.1589003593554987
iteration : 8362
train acc:  0.71875
train loss:  0.4968976378440857
train gradient:  0.1342456255760987
iteration : 8363
train acc:  0.796875
train loss:  0.44641536474227905
train gradient:  0.13210302806661894
iteration : 8364
train acc:  0.671875
train loss:  0.5409303307533264
train gradient:  0.1719537874103258
iteration : 8365
train acc:  0.734375
train loss:  0.4980306625366211
train gradient:  0.1203568665188238
iteration : 8366
train acc:  0.6953125
train loss:  0.5242729187011719
train gradient:  0.1337389019352455
iteration : 8367
train acc:  0.7421875
train loss:  0.5425369143486023
train gradient:  0.16695739564230005
iteration : 8368
train acc:  0.765625
train loss:  0.4555399417877197
train gradient:  0.11715656665204971
iteration : 8369
train acc:  0.671875
train loss:  0.5868894457817078
train gradient:  0.20113524739575286
iteration : 8370
train acc:  0.7109375
train loss:  0.5196921229362488
train gradient:  0.1484291442074195
iteration : 8371
train acc:  0.7109375
train loss:  0.516063392162323
train gradient:  0.1377015721445871
iteration : 8372
train acc:  0.7734375
train loss:  0.5007379651069641
train gradient:  0.15944190597399127
iteration : 8373
train acc:  0.7265625
train loss:  0.5522032976150513
train gradient:  0.1480026925664937
iteration : 8374
train acc:  0.7890625
train loss:  0.4461238980293274
train gradient:  0.10900318789000721
iteration : 8375
train acc:  0.7265625
train loss:  0.5050004124641418
train gradient:  0.13371150305164042
iteration : 8376
train acc:  0.734375
train loss:  0.4826016426086426
train gradient:  0.12950136044437963
iteration : 8377
train acc:  0.7578125
train loss:  0.47049227356910706
train gradient:  0.1350226867203392
iteration : 8378
train acc:  0.7109375
train loss:  0.4975511133670807
train gradient:  0.13590637161491306
iteration : 8379
train acc:  0.734375
train loss:  0.49915561079978943
train gradient:  0.13785824387618748
iteration : 8380
train acc:  0.640625
train loss:  0.569838285446167
train gradient:  0.18268593977535946
iteration : 8381
train acc:  0.8046875
train loss:  0.4133336544036865
train gradient:  0.11221075459543894
iteration : 8382
train acc:  0.7265625
train loss:  0.4720066785812378
train gradient:  0.13588784514147595
iteration : 8383
train acc:  0.7890625
train loss:  0.41301488876342773
train gradient:  0.08632842828421308
iteration : 8384
train acc:  0.7109375
train loss:  0.5513427257537842
train gradient:  0.16505080062698263
iteration : 8385
train acc:  0.671875
train loss:  0.5244041085243225
train gradient:  0.14622781730963275
iteration : 8386
train acc:  0.796875
train loss:  0.43805772066116333
train gradient:  0.1294149755101402
iteration : 8387
train acc:  0.8125
train loss:  0.49058395624160767
train gradient:  0.1410369023919018
iteration : 8388
train acc:  0.7109375
train loss:  0.5038986206054688
train gradient:  0.14865080221555466
iteration : 8389
train acc:  0.703125
train loss:  0.5275000929832458
train gradient:  0.16767747473327066
iteration : 8390
train acc:  0.8046875
train loss:  0.43362316489219666
train gradient:  0.10189255597208363
iteration : 8391
train acc:  0.734375
train loss:  0.4690520167350769
train gradient:  0.12701811964867776
iteration : 8392
train acc:  0.7578125
train loss:  0.47606873512268066
train gradient:  0.12490558691564638
iteration : 8393
train acc:  0.71875
train loss:  0.5142725706100464
train gradient:  0.13921875978978943
iteration : 8394
train acc:  0.7265625
train loss:  0.4531073570251465
train gradient:  0.11583581598656525
iteration : 8395
train acc:  0.75
train loss:  0.4954375922679901
train gradient:  0.12708506739857145
iteration : 8396
train acc:  0.734375
train loss:  0.5090515613555908
train gradient:  0.16513245672451893
iteration : 8397
train acc:  0.78125
train loss:  0.46170586347579956
train gradient:  0.10027019072814566
iteration : 8398
train acc:  0.78125
train loss:  0.42976874113082886
train gradient:  0.09583175945496435
iteration : 8399
train acc:  0.7421875
train loss:  0.4464430809020996
train gradient:  0.1462684410654338
iteration : 8400
train acc:  0.7109375
train loss:  0.5268949270248413
train gradient:  0.17808763175868708
iteration : 8401
train acc:  0.6640625
train loss:  0.5743221044540405
train gradient:  0.15513812182047965
iteration : 8402
train acc:  0.7578125
train loss:  0.45954418182373047
train gradient:  0.13932987626948778
iteration : 8403
train acc:  0.734375
train loss:  0.5066810250282288
train gradient:  0.12020390795128279
iteration : 8404
train acc:  0.828125
train loss:  0.45092669129371643
train gradient:  0.13621467486158267
iteration : 8405
train acc:  0.6953125
train loss:  0.5320355892181396
train gradient:  0.15356039467806037
iteration : 8406
train acc:  0.7265625
train loss:  0.5440140962600708
train gradient:  0.1864041605607063
iteration : 8407
train acc:  0.734375
train loss:  0.5474661588668823
train gradient:  0.1629895985884326
iteration : 8408
train acc:  0.7109375
train loss:  0.5094471573829651
train gradient:  0.14091857981758477
iteration : 8409
train acc:  0.859375
train loss:  0.37758737802505493
train gradient:  0.09487687353526379
iteration : 8410
train acc:  0.7265625
train loss:  0.5281962752342224
train gradient:  0.14069109334387037
iteration : 8411
train acc:  0.71875
train loss:  0.5302473306655884
train gradient:  0.14906365568021662
iteration : 8412
train acc:  0.734375
train loss:  0.4858803153038025
train gradient:  0.1316943280169316
iteration : 8413
train acc:  0.703125
train loss:  0.5286855101585388
train gradient:  0.14125499129857225
iteration : 8414
train acc:  0.7421875
train loss:  0.4973127543926239
train gradient:  0.13222701231930978
iteration : 8415
train acc:  0.765625
train loss:  0.4665328860282898
train gradient:  0.15083449023393367
iteration : 8416
train acc:  0.734375
train loss:  0.49643170833587646
train gradient:  0.13518516787275137
iteration : 8417
train acc:  0.78125
train loss:  0.42309635877609253
train gradient:  0.10304099021738271
iteration : 8418
train acc:  0.7421875
train loss:  0.5268549919128418
train gradient:  0.1293964183331357
iteration : 8419
train acc:  0.6640625
train loss:  0.5830174684524536
train gradient:  0.15529871949837493
iteration : 8420
train acc:  0.7734375
train loss:  0.46921658515930176
train gradient:  0.13320948681091382
iteration : 8421
train acc:  0.765625
train loss:  0.4933372437953949
train gradient:  0.11291512925019985
iteration : 8422
train acc:  0.78125
train loss:  0.4445992410182953
train gradient:  0.1328059140653451
iteration : 8423
train acc:  0.78125
train loss:  0.4290195107460022
train gradient:  0.1068415952370502
iteration : 8424
train acc:  0.7109375
train loss:  0.48592400550842285
train gradient:  0.1349213047537819
iteration : 8425
train acc:  0.6796875
train loss:  0.5042216777801514
train gradient:  0.1528450623230728
iteration : 8426
train acc:  0.7421875
train loss:  0.5119351148605347
train gradient:  0.13369228998170388
iteration : 8427
train acc:  0.6484375
train loss:  0.5535701513290405
train gradient:  0.1329415545574955
iteration : 8428
train acc:  0.765625
train loss:  0.4672417640686035
train gradient:  0.11913066944896056
iteration : 8429
train acc:  0.75
train loss:  0.5052250027656555
train gradient:  0.12404637775582253
iteration : 8430
train acc:  0.65625
train loss:  0.5373333692550659
train gradient:  0.15710196860036446
iteration : 8431
train acc:  0.765625
train loss:  0.470284640789032
train gradient:  0.1335101280939438
iteration : 8432
train acc:  0.78125
train loss:  0.4585111737251282
train gradient:  0.1117720809245345
iteration : 8433
train acc:  0.703125
train loss:  0.5950441360473633
train gradient:  0.23405684569487462
iteration : 8434
train acc:  0.6484375
train loss:  0.6026577949523926
train gradient:  0.17876180903273833
iteration : 8435
train acc:  0.7734375
train loss:  0.4610089063644409
train gradient:  0.12193193804162135
iteration : 8436
train acc:  0.671875
train loss:  0.6025475263595581
train gradient:  0.18567801619082125
iteration : 8437
train acc:  0.71875
train loss:  0.5708198547363281
train gradient:  0.23545497953324074
iteration : 8438
train acc:  0.7109375
train loss:  0.5167371034622192
train gradient:  0.16907897693215146
iteration : 8439
train acc:  0.7265625
train loss:  0.4664367437362671
train gradient:  0.12872352316655233
iteration : 8440
train acc:  0.734375
train loss:  0.504987359046936
train gradient:  0.15314132811770975
iteration : 8441
train acc:  0.7890625
train loss:  0.44545814394950867
train gradient:  0.11524626225161802
iteration : 8442
train acc:  0.671875
train loss:  0.6245137453079224
train gradient:  0.20011139527791294
iteration : 8443
train acc:  0.734375
train loss:  0.49207818508148193
train gradient:  0.13297912499019554
iteration : 8444
train acc:  0.71875
train loss:  0.5030113458633423
train gradient:  0.1224059467580579
iteration : 8445
train acc:  0.671875
train loss:  0.6273791790008545
train gradient:  0.24540972438388692
iteration : 8446
train acc:  0.7890625
train loss:  0.44704848527908325
train gradient:  0.1149335734255216
iteration : 8447
train acc:  0.703125
train loss:  0.5157448649406433
train gradient:  0.15867794685935188
iteration : 8448
train acc:  0.734375
train loss:  0.5048624277114868
train gradient:  0.1306702503626715
iteration : 8449
train acc:  0.734375
train loss:  0.5036390423774719
train gradient:  0.11047284767009319
iteration : 8450
train acc:  0.75
train loss:  0.4919232130050659
train gradient:  0.15311888772560717
iteration : 8451
train acc:  0.8046875
train loss:  0.4544883370399475
train gradient:  0.12389188717465459
iteration : 8452
train acc:  0.6796875
train loss:  0.5768263339996338
train gradient:  0.18572523472809704
iteration : 8453
train acc:  0.7578125
train loss:  0.47604846954345703
train gradient:  0.13817858762722587
iteration : 8454
train acc:  0.7734375
train loss:  0.5034095048904419
train gradient:  0.12958227241426634
iteration : 8455
train acc:  0.703125
train loss:  0.548054575920105
train gradient:  0.14621309599754673
iteration : 8456
train acc:  0.734375
train loss:  0.46195894479751587
train gradient:  0.10858181407544788
iteration : 8457
train acc:  0.734375
train loss:  0.4993849992752075
train gradient:  0.17546185335598458
iteration : 8458
train acc:  0.7734375
train loss:  0.5317072868347168
train gradient:  0.16900062446053835
iteration : 8459
train acc:  0.7578125
train loss:  0.4609137177467346
train gradient:  0.1332052230912748
iteration : 8460
train acc:  0.71875
train loss:  0.5434277057647705
train gradient:  0.16672813510518303
iteration : 8461
train acc:  0.6796875
train loss:  0.5418346524238586
train gradient:  0.1561288850605983
iteration : 8462
train acc:  0.7421875
train loss:  0.4817233979701996
train gradient:  0.12520244740483774
iteration : 8463
train acc:  0.765625
train loss:  0.4751879572868347
train gradient:  0.12158854655250949
iteration : 8464
train acc:  0.6953125
train loss:  0.552270770072937
train gradient:  0.17357401760671798
iteration : 8465
train acc:  0.8125
train loss:  0.40109190344810486
train gradient:  0.08912176711100694
iteration : 8466
train acc:  0.7734375
train loss:  0.46318018436431885
train gradient:  0.13371137051475357
iteration : 8467
train acc:  0.796875
train loss:  0.4574856758117676
train gradient:  0.1285793806653946
iteration : 8468
train acc:  0.703125
train loss:  0.514112651348114
train gradient:  0.1334854174442861
iteration : 8469
train acc:  0.71875
train loss:  0.5152027606964111
train gradient:  0.1447101222531723
iteration : 8470
train acc:  0.7421875
train loss:  0.4767915606498718
train gradient:  0.1085660839799401
iteration : 8471
train acc:  0.828125
train loss:  0.41515684127807617
train gradient:  0.1273265985728813
iteration : 8472
train acc:  0.703125
train loss:  0.5480170249938965
train gradient:  0.15968903736187573
iteration : 8473
train acc:  0.7578125
train loss:  0.454509437084198
train gradient:  0.12340752887964923
iteration : 8474
train acc:  0.7109375
train loss:  0.48438599705696106
train gradient:  0.11690069385652992
iteration : 8475
train acc:  0.7734375
train loss:  0.5029653906822205
train gradient:  0.15071139272405282
iteration : 8476
train acc:  0.75
train loss:  0.5075349807739258
train gradient:  0.12960672302596532
iteration : 8477
train acc:  0.75
train loss:  0.5764536261558533
train gradient:  0.18955300231730116
iteration : 8478
train acc:  0.8203125
train loss:  0.4483931362628937
train gradient:  0.10077069790919665
iteration : 8479
train acc:  0.7890625
train loss:  0.4463541805744171
train gradient:  0.11045390836193701
iteration : 8480
train acc:  0.75
train loss:  0.4717647433280945
train gradient:  0.13959999517768784
iteration : 8481
train acc:  0.7578125
train loss:  0.436816930770874
train gradient:  0.10206400770226505
iteration : 8482
train acc:  0.8046875
train loss:  0.4380217492580414
train gradient:  0.10384225299915975
iteration : 8483
train acc:  0.734375
train loss:  0.4836958348751068
train gradient:  0.13695631427507235
iteration : 8484
train acc:  0.6953125
train loss:  0.5086623430252075
train gradient:  0.1207779921398535
iteration : 8485
train acc:  0.75
train loss:  0.5039643049240112
train gradient:  0.12883748889355748
iteration : 8486
train acc:  0.7265625
train loss:  0.5580704212188721
train gradient:  0.13031049750760068
iteration : 8487
train acc:  0.78125
train loss:  0.47603318095207214
train gradient:  0.13291082438642354
iteration : 8488
train acc:  0.796875
train loss:  0.4769290089607239
train gradient:  0.12827959323468008
iteration : 8489
train acc:  0.7265625
train loss:  0.5040913820266724
train gradient:  0.13060490845260805
iteration : 8490
train acc:  0.78125
train loss:  0.48583680391311646
train gradient:  0.12509334283195458
iteration : 8491
train acc:  0.765625
train loss:  0.5265665054321289
train gradient:  0.14397683796126165
iteration : 8492
train acc:  0.7265625
train loss:  0.49592551589012146
train gradient:  0.11044325601533833
iteration : 8493
train acc:  0.8046875
train loss:  0.43488141894340515
train gradient:  0.10692525616378985
iteration : 8494
train acc:  0.7421875
train loss:  0.4933898448944092
train gradient:  0.13022248418450336
iteration : 8495
train acc:  0.78125
train loss:  0.4360300302505493
train gradient:  0.1069724589623
iteration : 8496
train acc:  0.71875
train loss:  0.48373284935951233
train gradient:  0.11989835522488212
iteration : 8497
train acc:  0.7421875
train loss:  0.5249481201171875
train gradient:  0.12979532050772155
iteration : 8498
train acc:  0.7578125
train loss:  0.46864980459213257
train gradient:  0.12828852700895416
iteration : 8499
train acc:  0.7890625
train loss:  0.48085692524909973
train gradient:  0.12526347372136842
iteration : 8500
train acc:  0.75
train loss:  0.5083049535751343
train gradient:  0.1220779468041336
iteration : 8501
train acc:  0.75
train loss:  0.4984854757785797
train gradient:  0.10174171522595807
iteration : 8502
train acc:  0.6875
train loss:  0.5775415301322937
train gradient:  0.17068937987716098
iteration : 8503
train acc:  0.7109375
train loss:  0.4941902160644531
train gradient:  0.13058887097843425
iteration : 8504
train acc:  0.734375
train loss:  0.5010430812835693
train gradient:  0.1534443565032518
iteration : 8505
train acc:  0.6796875
train loss:  0.5254786014556885
train gradient:  0.161621939876044
iteration : 8506
train acc:  0.75
train loss:  0.5026960372924805
train gradient:  0.11697777719046569
iteration : 8507
train acc:  0.7578125
train loss:  0.4842294752597809
train gradient:  0.12077188444046127
iteration : 8508
train acc:  0.7734375
train loss:  0.48791247606277466
train gradient:  0.10227653993480874
iteration : 8509
train acc:  0.7109375
train loss:  0.545029878616333
train gradient:  0.1349476202943618
iteration : 8510
train acc:  0.765625
train loss:  0.45155853033065796
train gradient:  0.113308699727296
iteration : 8511
train acc:  0.765625
train loss:  0.49580782651901245
train gradient:  0.15881076612864709
iteration : 8512
train acc:  0.71875
train loss:  0.5025600790977478
train gradient:  0.1475626556510165
iteration : 8513
train acc:  0.7734375
train loss:  0.4867211580276489
train gradient:  0.1827752953358583
iteration : 8514
train acc:  0.8046875
train loss:  0.44308409094810486
train gradient:  0.10231637688893541
iteration : 8515
train acc:  0.7734375
train loss:  0.45346418023109436
train gradient:  0.13789584333264215
iteration : 8516
train acc:  0.7734375
train loss:  0.4725368022918701
train gradient:  0.15173494644444832
iteration : 8517
train acc:  0.6953125
train loss:  0.5057966709136963
train gradient:  0.1470170817617366
iteration : 8518
train acc:  0.828125
train loss:  0.4171983003616333
train gradient:  0.09010625075008453
iteration : 8519
train acc:  0.8046875
train loss:  0.42701107263565063
train gradient:  0.08737000257593568
iteration : 8520
train acc:  0.6640625
train loss:  0.5383039116859436
train gradient:  0.12931187708611952
iteration : 8521
train acc:  0.765625
train loss:  0.480888694524765
train gradient:  0.11304606284460861
iteration : 8522
train acc:  0.71875
train loss:  0.4888647794723511
train gradient:  0.11775457232385682
iteration : 8523
train acc:  0.6796875
train loss:  0.5755547285079956
train gradient:  0.1556443066501774
iteration : 8524
train acc:  0.796875
train loss:  0.4635113477706909
train gradient:  0.11454391849298014
iteration : 8525
train acc:  0.6875
train loss:  0.5261553525924683
train gradient:  0.12663817068242253
iteration : 8526
train acc:  0.765625
train loss:  0.45386162400245667
train gradient:  0.11977982999404584
iteration : 8527
train acc:  0.65625
train loss:  0.5354835391044617
train gradient:  0.169035564194149
iteration : 8528
train acc:  0.8046875
train loss:  0.4296899735927582
train gradient:  0.12581282218024492
iteration : 8529
train acc:  0.75
train loss:  0.4625198543071747
train gradient:  0.09871360029471525
iteration : 8530
train acc:  0.71875
train loss:  0.5941148996353149
train gradient:  0.232442639016312
iteration : 8531
train acc:  0.8046875
train loss:  0.41573238372802734
train gradient:  0.08303487605889735
iteration : 8532
train acc:  0.734375
train loss:  0.5223338603973389
train gradient:  0.15979220927616825
iteration : 8533
train acc:  0.65625
train loss:  0.564239501953125
train gradient:  0.14858180935325027
iteration : 8534
train acc:  0.6953125
train loss:  0.5344253778457642
train gradient:  0.16620022344420357
iteration : 8535
train acc:  0.7421875
train loss:  0.5355135798454285
train gradient:  0.17368459643871326
iteration : 8536
train acc:  0.703125
train loss:  0.5735145807266235
train gradient:  0.1868216111238863
iteration : 8537
train acc:  0.765625
train loss:  0.48927566409111023
train gradient:  0.13416787347279896
iteration : 8538
train acc:  0.71875
train loss:  0.5570328235626221
train gradient:  0.15049063987942632
iteration : 8539
train acc:  0.78125
train loss:  0.4708440899848938
train gradient:  0.11532124617760894
iteration : 8540
train acc:  0.75
train loss:  0.49760037660598755
train gradient:  0.10833179351950857
iteration : 8541
train acc:  0.7265625
train loss:  0.4612278938293457
train gradient:  0.10685418373214572
iteration : 8542
train acc:  0.71875
train loss:  0.5401353240013123
train gradient:  0.1789841863460152
iteration : 8543
train acc:  0.7265625
train loss:  0.4914330542087555
train gradient:  0.13780039485757997
iteration : 8544
train acc:  0.734375
train loss:  0.538344144821167
train gradient:  0.20250253781887134
iteration : 8545
train acc:  0.7578125
train loss:  0.47153449058532715
train gradient:  0.1323513055346593
iteration : 8546
train acc:  0.7578125
train loss:  0.5099384784698486
train gradient:  0.14148907518050413
iteration : 8547
train acc:  0.7109375
train loss:  0.5234757661819458
train gradient:  0.12881427566351578
iteration : 8548
train acc:  0.7421875
train loss:  0.46545737981796265
train gradient:  0.11424717556942213
iteration : 8549
train acc:  0.7109375
train loss:  0.4941885769367218
train gradient:  0.12649220273747255
iteration : 8550
train acc:  0.7109375
train loss:  0.5661245584487915
train gradient:  0.15706418069387781
iteration : 8551
train acc:  0.7890625
train loss:  0.4879421591758728
train gradient:  0.11217747522968098
iteration : 8552
train acc:  0.7890625
train loss:  0.46682196855545044
train gradient:  0.10968394595217691
iteration : 8553
train acc:  0.7734375
train loss:  0.483204185962677
train gradient:  0.14562768564541784
iteration : 8554
train acc:  0.734375
train loss:  0.541081428527832
train gradient:  0.16884384216676926
iteration : 8555
train acc:  0.7109375
train loss:  0.5407308340072632
train gradient:  0.14270744959162804
iteration : 8556
train acc:  0.734375
train loss:  0.4995797872543335
train gradient:  0.12596778342993628
iteration : 8557
train acc:  0.734375
train loss:  0.460715115070343
train gradient:  0.12987268695416349
iteration : 8558
train acc:  0.734375
train loss:  0.47706353664398193
train gradient:  0.12513166235822623
iteration : 8559
train acc:  0.71875
train loss:  0.48683053255081177
train gradient:  0.13767291253227804
iteration : 8560
train acc:  0.7265625
train loss:  0.49117499589920044
train gradient:  0.1322401234807561
iteration : 8561
train acc:  0.7578125
train loss:  0.5556097030639648
train gradient:  0.1753396252515772
iteration : 8562
train acc:  0.78125
train loss:  0.4617938995361328
train gradient:  0.13520571278882057
iteration : 8563
train acc:  0.7421875
train loss:  0.48146510124206543
train gradient:  0.1267398745736525
iteration : 8564
train acc:  0.7578125
train loss:  0.48161399364471436
train gradient:  0.11338795241785378
iteration : 8565
train acc:  0.703125
train loss:  0.5491717457771301
train gradient:  0.1391346340209253
iteration : 8566
train acc:  0.7734375
train loss:  0.4307110905647278
train gradient:  0.10882251551927614
iteration : 8567
train acc:  0.734375
train loss:  0.45838311314582825
train gradient:  0.1219214970250129
iteration : 8568
train acc:  0.6796875
train loss:  0.5869633555412292
train gradient:  0.17576274915614648
iteration : 8569
train acc:  0.6875
train loss:  0.6082512140274048
train gradient:  0.18518588623280866
iteration : 8570
train acc:  0.703125
train loss:  0.5329383611679077
train gradient:  0.1767366906951185
iteration : 8571
train acc:  0.6796875
train loss:  0.5615770220756531
train gradient:  0.18627992953361666
iteration : 8572
train acc:  0.8046875
train loss:  0.4341397285461426
train gradient:  0.0971353440508419
iteration : 8573
train acc:  0.75
train loss:  0.49635371565818787
train gradient:  0.13691469582116544
iteration : 8574
train acc:  0.8203125
train loss:  0.4233558177947998
train gradient:  0.10052497215482788
iteration : 8575
train acc:  0.71875
train loss:  0.5209195613861084
train gradient:  0.1528683380686668
iteration : 8576
train acc:  0.7578125
train loss:  0.4640182852745056
train gradient:  0.11685791521352107
iteration : 8577
train acc:  0.7109375
train loss:  0.5132259130477905
train gradient:  0.2228265186980798
iteration : 8578
train acc:  0.78125
train loss:  0.4156668782234192
train gradient:  0.10199619745897465
iteration : 8579
train acc:  0.7734375
train loss:  0.4456477165222168
train gradient:  0.11254762118524297
iteration : 8580
train acc:  0.8046875
train loss:  0.45035848021507263
train gradient:  0.10952812670951419
iteration : 8581
train acc:  0.7109375
train loss:  0.5357295870780945
train gradient:  0.1296007215126366
iteration : 8582
train acc:  0.75
train loss:  0.4812779426574707
train gradient:  0.1515567650552001
iteration : 8583
train acc:  0.75
train loss:  0.4996507465839386
train gradient:  0.11969172036427846
iteration : 8584
train acc:  0.7421875
train loss:  0.5581612586975098
train gradient:  0.16059161669656424
iteration : 8585
train acc:  0.6953125
train loss:  0.5705596208572388
train gradient:  0.16987272245764998
iteration : 8586
train acc:  0.734375
train loss:  0.5520586371421814
train gradient:  0.14524774192213163
iteration : 8587
train acc:  0.796875
train loss:  0.4570561349391937
train gradient:  0.10047247011148538
iteration : 8588
train acc:  0.765625
train loss:  0.5432695150375366
train gradient:  0.15566660309391323
iteration : 8589
train acc:  0.6875
train loss:  0.5793810486793518
train gradient:  0.20172292621582916
iteration : 8590
train acc:  0.75
train loss:  0.49933552742004395
train gradient:  0.12510859982630884
iteration : 8591
train acc:  0.71875
train loss:  0.501717209815979
train gradient:  0.11346428600085331
iteration : 8592
train acc:  0.7578125
train loss:  0.45038482546806335
train gradient:  0.11046285356758437
iteration : 8593
train acc:  0.7734375
train loss:  0.45699623227119446
train gradient:  0.12325351309707276
iteration : 8594
train acc:  0.765625
train loss:  0.4608539342880249
train gradient:  0.10983167315948217
iteration : 8595
train acc:  0.7890625
train loss:  0.4414876103401184
train gradient:  0.11374528197196013
iteration : 8596
train acc:  0.7265625
train loss:  0.4930371642112732
train gradient:  0.12367706437131935
iteration : 8597
train acc:  0.7421875
train loss:  0.5316684246063232
train gradient:  0.1463392841144621
iteration : 8598
train acc:  0.7421875
train loss:  0.48204171657562256
train gradient:  0.133761508892202
iteration : 8599
train acc:  0.6796875
train loss:  0.5758732557296753
train gradient:  0.16173090401687068
iteration : 8600
train acc:  0.734375
train loss:  0.5163999795913696
train gradient:  0.1235900232194301
iteration : 8601
train acc:  0.7109375
train loss:  0.4882776141166687
train gradient:  0.15998538222770442
iteration : 8602
train acc:  0.703125
train loss:  0.5305361747741699
train gradient:  0.19519932080313004
iteration : 8603
train acc:  0.7890625
train loss:  0.46321120858192444
train gradient:  0.1230898530858713
iteration : 8604
train acc:  0.78125
train loss:  0.42593804001808167
train gradient:  0.1292697289942077
iteration : 8605
train acc:  0.71875
train loss:  0.5645256638526917
train gradient:  0.1625888844591763
iteration : 8606
train acc:  0.71875
train loss:  0.524448037147522
train gradient:  0.13387817770178653
iteration : 8607
train acc:  0.75
train loss:  0.5050730109214783
train gradient:  0.1440527040814795
iteration : 8608
train acc:  0.703125
train loss:  0.5262264013290405
train gradient:  0.1271286095806042
iteration : 8609
train acc:  0.734375
train loss:  0.5190978050231934
train gradient:  0.13176098336997746
iteration : 8610
train acc:  0.78125
train loss:  0.45250165462493896
train gradient:  0.11642004635673613
iteration : 8611
train acc:  0.7890625
train loss:  0.47467267513275146
train gradient:  0.1335167897226416
iteration : 8612
train acc:  0.7578125
train loss:  0.491379976272583
train gradient:  0.12099877197287191
iteration : 8613
train acc:  0.765625
train loss:  0.4620188772678375
train gradient:  0.15720247099202317
iteration : 8614
train acc:  0.75
train loss:  0.47211337089538574
train gradient:  0.13841482481869766
iteration : 8615
train acc:  0.71875
train loss:  0.5377203226089478
train gradient:  0.12257289255890975
iteration : 8616
train acc:  0.796875
train loss:  0.4522818326950073
train gradient:  0.13572199979358257
iteration : 8617
train acc:  0.7421875
train loss:  0.5203964114189148
train gradient:  0.16289566000183558
iteration : 8618
train acc:  0.671875
train loss:  0.6106917858123779
train gradient:  0.19746413632697374
iteration : 8619
train acc:  0.703125
train loss:  0.5455252528190613
train gradient:  0.17394963826466625
iteration : 8620
train acc:  0.71875
train loss:  0.5424543619155884
train gradient:  0.12587899498387609
iteration : 8621
train acc:  0.7578125
train loss:  0.49584832787513733
train gradient:  0.13679792509727606
iteration : 8622
train acc:  0.7578125
train loss:  0.5100048780441284
train gradient:  0.15653577022107462
iteration : 8623
train acc:  0.7421875
train loss:  0.5014159083366394
train gradient:  0.11190101326052083
iteration : 8624
train acc:  0.6953125
train loss:  0.5104147791862488
train gradient:  0.13432012155118958
iteration : 8625
train acc:  0.7265625
train loss:  0.47003135085105896
train gradient:  0.11246990142570568
iteration : 8626
train acc:  0.71875
train loss:  0.5200839042663574
train gradient:  0.1497364641813355
iteration : 8627
train acc:  0.7578125
train loss:  0.4574981927871704
train gradient:  0.1358251389583222
iteration : 8628
train acc:  0.7421875
train loss:  0.5710848569869995
train gradient:  0.18944959593920035
iteration : 8629
train acc:  0.703125
train loss:  0.5167877078056335
train gradient:  0.15461339562228174
iteration : 8630
train acc:  0.671875
train loss:  0.6210054159164429
train gradient:  0.22957313514196123
iteration : 8631
train acc:  0.7734375
train loss:  0.5056163668632507
train gradient:  0.11033794270945702
iteration : 8632
train acc:  0.8125
train loss:  0.44789478182792664
train gradient:  0.19179624540380108
iteration : 8633
train acc:  0.703125
train loss:  0.5470237135887146
train gradient:  0.15181446989393904
iteration : 8634
train acc:  0.7265625
train loss:  0.5096324682235718
train gradient:  0.1783148045052045
iteration : 8635
train acc:  0.796875
train loss:  0.4349769949913025
train gradient:  0.1159406988990027
iteration : 8636
train acc:  0.75
train loss:  0.4796638488769531
train gradient:  0.14041741335579533
iteration : 8637
train acc:  0.6640625
train loss:  0.5352797508239746
train gradient:  0.14379257784212848
iteration : 8638
train acc:  0.75
train loss:  0.5121822357177734
train gradient:  0.11399825400568278
iteration : 8639
train acc:  0.7265625
train loss:  0.4458857774734497
train gradient:  0.1091530061542212
iteration : 8640
train acc:  0.7265625
train loss:  0.47447454929351807
train gradient:  0.11524820538284447
iteration : 8641
train acc:  0.7265625
train loss:  0.49896863102912903
train gradient:  0.13153905698356946
iteration : 8642
train acc:  0.703125
train loss:  0.5203936100006104
train gradient:  0.14415598919293354
iteration : 8643
train acc:  0.734375
train loss:  0.5197100639343262
train gradient:  0.15380385207973063
iteration : 8644
train acc:  0.734375
train loss:  0.5504710674285889
train gradient:  0.16213174006126
iteration : 8645
train acc:  0.8046875
train loss:  0.4590801000595093
train gradient:  0.1093755851706128
iteration : 8646
train acc:  0.7265625
train loss:  0.5200216174125671
train gradient:  0.12103600943716362
iteration : 8647
train acc:  0.7265625
train loss:  0.5057243704795837
train gradient:  0.12582965035172605
iteration : 8648
train acc:  0.7109375
train loss:  0.5070200562477112
train gradient:  0.12178946136673409
iteration : 8649
train acc:  0.71875
train loss:  0.53287672996521
train gradient:  0.16151280806185697
iteration : 8650
train acc:  0.765625
train loss:  0.512019157409668
train gradient:  0.1687328809733583
iteration : 8651
train acc:  0.6796875
train loss:  0.541598916053772
train gradient:  0.22114965288993998
iteration : 8652
train acc:  0.75
train loss:  0.49386507272720337
train gradient:  0.13570236980104355
iteration : 8653
train acc:  0.796875
train loss:  0.41908884048461914
train gradient:  0.10478624905780953
iteration : 8654
train acc:  0.703125
train loss:  0.5654013156890869
train gradient:  0.15064740724692788
iteration : 8655
train acc:  0.765625
train loss:  0.46859580278396606
train gradient:  0.11796836358982142
iteration : 8656
train acc:  0.75
train loss:  0.4683014154434204
train gradient:  0.11812733619278086
iteration : 8657
train acc:  0.8203125
train loss:  0.43465685844421387
train gradient:  0.09392764323375538
iteration : 8658
train acc:  0.796875
train loss:  0.5023289918899536
train gradient:  0.13973390547870473
iteration : 8659
train acc:  0.7265625
train loss:  0.5389920473098755
train gradient:  0.18244661451335664
iteration : 8660
train acc:  0.7265625
train loss:  0.5096670389175415
train gradient:  0.15557772928633073
iteration : 8661
train acc:  0.75
train loss:  0.47319111227989197
train gradient:  0.11353605519376266
iteration : 8662
train acc:  0.7578125
train loss:  0.5337158441543579
train gradient:  0.19140287144216034
iteration : 8663
train acc:  0.7265625
train loss:  0.5243358612060547
train gradient:  0.1798679681724887
iteration : 8664
train acc:  0.71875
train loss:  0.5059126615524292
train gradient:  0.13105365959737675
iteration : 8665
train acc:  0.734375
train loss:  0.4871782660484314
train gradient:  0.11317010286096632
iteration : 8666
train acc:  0.765625
train loss:  0.5096318125724792
train gradient:  0.12792725041864106
iteration : 8667
train acc:  0.8046875
train loss:  0.42977267503738403
train gradient:  0.13034405510063302
iteration : 8668
train acc:  0.8203125
train loss:  0.4395103454589844
train gradient:  0.09712270020041724
iteration : 8669
train acc:  0.671875
train loss:  0.5701483488082886
train gradient:  0.14259997645798364
iteration : 8670
train acc:  0.7265625
train loss:  0.529636025428772
train gradient:  0.12898714160418276
iteration : 8671
train acc:  0.7734375
train loss:  0.5157959461212158
train gradient:  0.1420350372737076
iteration : 8672
train acc:  0.765625
train loss:  0.4832600951194763
train gradient:  0.1720727184191897
iteration : 8673
train acc:  0.796875
train loss:  0.46843498945236206
train gradient:  0.13119178352078134
iteration : 8674
train acc:  0.7421875
train loss:  0.48872166872024536
train gradient:  0.14979277752712988
iteration : 8675
train acc:  0.703125
train loss:  0.5606133341789246
train gradient:  0.196413927509001
iteration : 8676
train acc:  0.734375
train loss:  0.5104907751083374
train gradient:  0.11770770951691625
iteration : 8677
train acc:  0.765625
train loss:  0.49101540446281433
train gradient:  0.121001624394954
iteration : 8678
train acc:  0.7578125
train loss:  0.5028232336044312
train gradient:  0.12415647409553299
iteration : 8679
train acc:  0.7421875
train loss:  0.47918814420700073
train gradient:  0.12399354690519167
iteration : 8680
train acc:  0.765625
train loss:  0.49862751364707947
train gradient:  0.12872556381277578
iteration : 8681
train acc:  0.703125
train loss:  0.549357533454895
train gradient:  0.1615829841097788
iteration : 8682
train acc:  0.78125
train loss:  0.48765480518341064
train gradient:  0.11574561402271225
iteration : 8683
train acc:  0.796875
train loss:  0.4694897532463074
train gradient:  0.14098651240242335
iteration : 8684
train acc:  0.734375
train loss:  0.5383893251419067
train gradient:  0.15863207727312617
iteration : 8685
train acc:  0.7578125
train loss:  0.4768986105918884
train gradient:  0.13324100033862202
iteration : 8686
train acc:  0.7265625
train loss:  0.5419676899909973
train gradient:  0.1490716456720365
iteration : 8687
train acc:  0.75
train loss:  0.45207715034484863
train gradient:  0.11680053854402762
iteration : 8688
train acc:  0.71875
train loss:  0.5008749961853027
train gradient:  0.12695859178431002
iteration : 8689
train acc:  0.7109375
train loss:  0.5368648171424866
train gradient:  0.17166465168640554
iteration : 8690
train acc:  0.7109375
train loss:  0.5098121166229248
train gradient:  0.17728771559008172
iteration : 8691
train acc:  0.609375
train loss:  0.6343623399734497
train gradient:  0.20477548896010078
iteration : 8692
train acc:  0.7578125
train loss:  0.41731390357017517
train gradient:  0.09879076302379211
iteration : 8693
train acc:  0.703125
train loss:  0.5071398019790649
train gradient:  0.1604355119672471
iteration : 8694
train acc:  0.75
train loss:  0.47171255946159363
train gradient:  0.12461697449773562
iteration : 8695
train acc:  0.8046875
train loss:  0.4456727206707001
train gradient:  0.10424942212266179
iteration : 8696
train acc:  0.71875
train loss:  0.5156780481338501
train gradient:  0.12124788702223598
iteration : 8697
train acc:  0.7734375
train loss:  0.4524345397949219
train gradient:  0.14020945836081683
iteration : 8698
train acc:  0.765625
train loss:  0.46131467819213867
train gradient:  0.13085370018574696
iteration : 8699
train acc:  0.75
train loss:  0.457803875207901
train gradient:  0.12967390901743836
iteration : 8700
train acc:  0.7265625
train loss:  0.536001443862915
train gradient:  0.1332263629735536
iteration : 8701
train acc:  0.734375
train loss:  0.5361984968185425
train gradient:  0.15488491995139914
iteration : 8702
train acc:  0.6953125
train loss:  0.581994891166687
train gradient:  0.16838732951182553
iteration : 8703
train acc:  0.7109375
train loss:  0.5934749841690063
train gradient:  0.19214763888153547
iteration : 8704
train acc:  0.71875
train loss:  0.5177402496337891
train gradient:  0.18128247051302962
iteration : 8705
train acc:  0.7109375
train loss:  0.5046794414520264
train gradient:  0.11316992856640905
iteration : 8706
train acc:  0.7109375
train loss:  0.49812793731689453
train gradient:  0.14358712183906638
iteration : 8707
train acc:  0.75
train loss:  0.5318784713745117
train gradient:  0.139756909025248
iteration : 8708
train acc:  0.71875
train loss:  0.5046279430389404
train gradient:  0.1283652557289467
iteration : 8709
train acc:  0.78125
train loss:  0.4786940813064575
train gradient:  0.1345660809952182
iteration : 8710
train acc:  0.78125
train loss:  0.4544496238231659
train gradient:  0.1270143776025695
iteration : 8711
train acc:  0.734375
train loss:  0.5073747634887695
train gradient:  0.12339644591407797
iteration : 8712
train acc:  0.7421875
train loss:  0.5126045942306519
train gradient:  0.12519383319915334
iteration : 8713
train acc:  0.71875
train loss:  0.49868109822273254
train gradient:  0.13617692732132902
iteration : 8714
train acc:  0.7421875
train loss:  0.5157602429389954
train gradient:  0.11561315389249857
iteration : 8715
train acc:  0.6875
train loss:  0.5201576352119446
train gradient:  0.125469038979603
iteration : 8716
train acc:  0.7578125
train loss:  0.46985870599746704
train gradient:  0.10085055318022827
iteration : 8717
train acc:  0.8203125
train loss:  0.43856722116470337
train gradient:  0.10410107441527366
iteration : 8718
train acc:  0.734375
train loss:  0.4927307665348053
train gradient:  0.1450159895343133
iteration : 8719
train acc:  0.6875
train loss:  0.5360741019248962
train gradient:  0.16711760111374896
iteration : 8720
train acc:  0.765625
train loss:  0.5040429830551147
train gradient:  0.14620817052616636
iteration : 8721
train acc:  0.6640625
train loss:  0.5781522989273071
train gradient:  0.18508441565947184
iteration : 8722
train acc:  0.765625
train loss:  0.49496638774871826
train gradient:  0.1318329750970205
iteration : 8723
train acc:  0.7265625
train loss:  0.5394369959831238
train gradient:  0.15322599020891237
iteration : 8724
train acc:  0.75
train loss:  0.5071474313735962
train gradient:  0.16064142109469481
iteration : 8725
train acc:  0.765625
train loss:  0.4621695280075073
train gradient:  0.15297331724170488
iteration : 8726
train acc:  0.7421875
train loss:  0.5329585075378418
train gradient:  0.23948394804680825
iteration : 8727
train acc:  0.71875
train loss:  0.526964545249939
train gradient:  0.15597493664544387
iteration : 8728
train acc:  0.75
train loss:  0.4885541796684265
train gradient:  0.11813338931486744
iteration : 8729
train acc:  0.6953125
train loss:  0.5210461616516113
train gradient:  0.1574798296978091
iteration : 8730
train acc:  0.7734375
train loss:  0.47534728050231934
train gradient:  0.15141001448847125
iteration : 8731
train acc:  0.7421875
train loss:  0.48618239164352417
train gradient:  0.15170550396963198
iteration : 8732
train acc:  0.75
train loss:  0.48618459701538086
train gradient:  0.1541061214875504
iteration : 8733
train acc:  0.7265625
train loss:  0.4701176583766937
train gradient:  0.10195518416315019
iteration : 8734
train acc:  0.65625
train loss:  0.6270722150802612
train gradient:  0.16423584807729558
iteration : 8735
train acc:  0.7265625
train loss:  0.5330396890640259
train gradient:  0.14923383663660156
iteration : 8736
train acc:  0.765625
train loss:  0.4670959413051605
train gradient:  0.15192403339329222
iteration : 8737
train acc:  0.7578125
train loss:  0.475547730922699
train gradient:  0.11077843669548118
iteration : 8738
train acc:  0.8046875
train loss:  0.44207993149757385
train gradient:  0.09712679556804768
iteration : 8739
train acc:  0.8046875
train loss:  0.4433836042881012
train gradient:  0.09268520306237428
iteration : 8740
train acc:  0.703125
train loss:  0.5203768014907837
train gradient:  0.15303325217014666
iteration : 8741
train acc:  0.6875
train loss:  0.5144648551940918
train gradient:  0.16379460579275978
iteration : 8742
train acc:  0.7890625
train loss:  0.5057101249694824
train gradient:  0.12674111973414143
iteration : 8743
train acc:  0.71875
train loss:  0.48636388778686523
train gradient:  0.11656186862652851
iteration : 8744
train acc:  0.734375
train loss:  0.5266379117965698
train gradient:  0.13934191320527312
iteration : 8745
train acc:  0.6796875
train loss:  0.5708713531494141
train gradient:  0.15825844625790064
iteration : 8746
train acc:  0.7265625
train loss:  0.5447801351547241
train gradient:  0.14510081877713232
iteration : 8747
train acc:  0.7734375
train loss:  0.4760318994522095
train gradient:  0.15426674798473483
iteration : 8748
train acc:  0.7421875
train loss:  0.502926766872406
train gradient:  0.15315999590525908
iteration : 8749
train acc:  0.7578125
train loss:  0.4665455222129822
train gradient:  0.12118748171744015
iteration : 8750
train acc:  0.7421875
train loss:  0.49037063121795654
train gradient:  0.13360743801475003
iteration : 8751
train acc:  0.7265625
train loss:  0.5516672134399414
train gradient:  0.2095306969731082
iteration : 8752
train acc:  0.671875
train loss:  0.5301384329795837
train gradient:  0.12230256357192973
iteration : 8753
train acc:  0.734375
train loss:  0.5363049507141113
train gradient:  0.1576349741607379
iteration : 8754
train acc:  0.734375
train loss:  0.49588602781295776
train gradient:  0.147021641613797
iteration : 8755
train acc:  0.71875
train loss:  0.4925602972507477
train gradient:  0.1275168515301523
iteration : 8756
train acc:  0.7109375
train loss:  0.5452397465705872
train gradient:  0.15830678493807476
iteration : 8757
train acc:  0.7421875
train loss:  0.5135076642036438
train gradient:  0.1375271990936362
iteration : 8758
train acc:  0.75
train loss:  0.5094309449195862
train gradient:  0.14520138295108242
iteration : 8759
train acc:  0.7265625
train loss:  0.5553014874458313
train gradient:  0.12549184963629526
iteration : 8760
train acc:  0.6796875
train loss:  0.5850549936294556
train gradient:  0.1434850049056008
iteration : 8761
train acc:  0.7734375
train loss:  0.4302348494529724
train gradient:  0.10113266416531892
iteration : 8762
train acc:  0.828125
train loss:  0.39445000886917114
train gradient:  0.11342867895153605
iteration : 8763
train acc:  0.7421875
train loss:  0.5018414258956909
train gradient:  0.1720604422719499
iteration : 8764
train acc:  0.7578125
train loss:  0.470816969871521
train gradient:  0.09942094281414429
iteration : 8765
train acc:  0.734375
train loss:  0.5186932682991028
train gradient:  0.16245329071357745
iteration : 8766
train acc:  0.7265625
train loss:  0.5406099557876587
train gradient:  0.1256674207512266
iteration : 8767
train acc:  0.7734375
train loss:  0.4528767466545105
train gradient:  0.09596927078821996
iteration : 8768
train acc:  0.765625
train loss:  0.4451484978199005
train gradient:  0.1057387427977811
iteration : 8769
train acc:  0.734375
train loss:  0.5377539992332458
train gradient:  0.18821277072966092
iteration : 8770
train acc:  0.765625
train loss:  0.492247998714447
train gradient:  0.13458660320903146
iteration : 8771
train acc:  0.8046875
train loss:  0.4350041151046753
train gradient:  0.09013987407485982
iteration : 8772
train acc:  0.734375
train loss:  0.4958251118659973
train gradient:  0.1513398561525969
iteration : 8773
train acc:  0.7265625
train loss:  0.49199268221855164
train gradient:  0.12545295998555364
iteration : 8774
train acc:  0.703125
train loss:  0.5971124172210693
train gradient:  0.24332658782552719
iteration : 8775
train acc:  0.78125
train loss:  0.4827306270599365
train gradient:  0.13970295900157698
iteration : 8776
train acc:  0.796875
train loss:  0.4493257403373718
train gradient:  0.09916418089550637
iteration : 8777
train acc:  0.765625
train loss:  0.5297620296478271
train gradient:  0.17020686488173772
iteration : 8778
train acc:  0.7734375
train loss:  0.49794867634773254
train gradient:  0.14857820210327033
iteration : 8779
train acc:  0.75
train loss:  0.5006410479545593
train gradient:  0.12798073922379577
iteration : 8780
train acc:  0.71875
train loss:  0.5158966779708862
train gradient:  0.11862205507740362
iteration : 8781
train acc:  0.8125
train loss:  0.41226452589035034
train gradient:  0.08066818415533686
iteration : 8782
train acc:  0.75
train loss:  0.49655744433403015
train gradient:  0.11840359753211566
iteration : 8783
train acc:  0.703125
train loss:  0.5231576561927795
train gradient:  0.1570842938700419
iteration : 8784
train acc:  0.765625
train loss:  0.458814412355423
train gradient:  0.11315271119545263
iteration : 8785
train acc:  0.7421875
train loss:  0.5119394063949585
train gradient:  0.16419013667018717
iteration : 8786
train acc:  0.6875
train loss:  0.5664539337158203
train gradient:  0.20150033526292055
iteration : 8787
train acc:  0.75
train loss:  0.4858614504337311
train gradient:  0.11132942854046206
iteration : 8788
train acc:  0.7109375
train loss:  0.5647968053817749
train gradient:  0.1522993861266832
iteration : 8789
train acc:  0.7109375
train loss:  0.5293095707893372
train gradient:  0.1391395085187641
iteration : 8790
train acc:  0.6875
train loss:  0.5500143766403198
train gradient:  0.13549231205706633
iteration : 8791
train acc:  0.6953125
train loss:  0.5404959917068481
train gradient:  0.13507756242090807
iteration : 8792
train acc:  0.703125
train loss:  0.5265952348709106
train gradient:  0.12134857139774262
iteration : 8793
train acc:  0.765625
train loss:  0.4790000319480896
train gradient:  0.11311553976858696
iteration : 8794
train acc:  0.703125
train loss:  0.5097899436950684
train gradient:  0.12929585115941272
iteration : 8795
train acc:  0.765625
train loss:  0.505996584892273
train gradient:  0.12743614732615322
iteration : 8796
train acc:  0.734375
train loss:  0.515205979347229
train gradient:  0.14502537949093203
iteration : 8797
train acc:  0.7109375
train loss:  0.5601209998130798
train gradient:  0.16624844165026847
iteration : 8798
train acc:  0.78125
train loss:  0.476687490940094
train gradient:  0.15080776876265953
iteration : 8799
train acc:  0.6875
train loss:  0.49647554755210876
train gradient:  0.12845204819658476
iteration : 8800
train acc:  0.671875
train loss:  0.576231837272644
train gradient:  0.18778700435090617
iteration : 8801
train acc:  0.7734375
train loss:  0.43291717767715454
train gradient:  0.10483866650698989
iteration : 8802
train acc:  0.7578125
train loss:  0.4859863519668579
train gradient:  0.13611093733736368
iteration : 8803
train acc:  0.828125
train loss:  0.41651761531829834
train gradient:  0.11918329284131607
iteration : 8804
train acc:  0.7578125
train loss:  0.48770344257354736
train gradient:  0.131584898705129
iteration : 8805
train acc:  0.7421875
train loss:  0.4901444613933563
train gradient:  0.1547746252697519
iteration : 8806
train acc:  0.7578125
train loss:  0.47525110840797424
train gradient:  0.12635193196705646
iteration : 8807
train acc:  0.734375
train loss:  0.49918368458747864
train gradient:  0.12084417375006458
iteration : 8808
train acc:  0.796875
train loss:  0.4487334191799164
train gradient:  0.10684337487405056
iteration : 8809
train acc:  0.7578125
train loss:  0.48993155360221863
train gradient:  0.16482477516975544
iteration : 8810
train acc:  0.78125
train loss:  0.46756643056869507
train gradient:  0.15580895496164004
iteration : 8811
train acc:  0.6640625
train loss:  0.5638961791992188
train gradient:  0.22018162708545214
iteration : 8812
train acc:  0.7578125
train loss:  0.5004585385322571
train gradient:  0.12935897497503113
iteration : 8813
train acc:  0.8046875
train loss:  0.4185144305229187
train gradient:  0.12458206710895543
iteration : 8814
train acc:  0.7734375
train loss:  0.4473689794540405
train gradient:  0.12398765491483245
iteration : 8815
train acc:  0.6953125
train loss:  0.55042564868927
train gradient:  0.1682319737965082
iteration : 8816
train acc:  0.8125
train loss:  0.46461987495422363
train gradient:  0.10367995212666363
iteration : 8817
train acc:  0.734375
train loss:  0.4875262379646301
train gradient:  0.10964257917049854
iteration : 8818
train acc:  0.796875
train loss:  0.46257105469703674
train gradient:  0.10984368623338753
iteration : 8819
train acc:  0.71875
train loss:  0.5065751075744629
train gradient:  0.1283404575499752
iteration : 8820
train acc:  0.7421875
train loss:  0.4976363778114319
train gradient:  0.1461732791206129
iteration : 8821
train acc:  0.7109375
train loss:  0.5720334053039551
train gradient:  0.1373286401010576
iteration : 8822
train acc:  0.765625
train loss:  0.4968271851539612
train gradient:  0.1573133674393146
iteration : 8823
train acc:  0.7578125
train loss:  0.504631757736206
train gradient:  0.12093350079090258
iteration : 8824
train acc:  0.6953125
train loss:  0.5325642228126526
train gradient:  0.16433155534399618
iteration : 8825
train acc:  0.7578125
train loss:  0.47037559747695923
train gradient:  0.13594331425054526
iteration : 8826
train acc:  0.7734375
train loss:  0.4341062009334564
train gradient:  0.10871695271320551
iteration : 8827
train acc:  0.6640625
train loss:  0.5468618869781494
train gradient:  0.1310715137213719
iteration : 8828
train acc:  0.6953125
train loss:  0.5186798572540283
train gradient:  0.13914442393990534
iteration : 8829
train acc:  0.6875
train loss:  0.5934646129608154
train gradient:  0.21075389244571813
iteration : 8830
train acc:  0.7578125
train loss:  0.49495983123779297
train gradient:  0.13629749370528427
iteration : 8831
train acc:  0.7578125
train loss:  0.5419917106628418
train gradient:  0.12081197413333479
iteration : 8832
train acc:  0.7734375
train loss:  0.4609559178352356
train gradient:  0.10987085722402753
iteration : 8833
train acc:  0.7421875
train loss:  0.5055798888206482
train gradient:  0.13912208699736558
iteration : 8834
train acc:  0.75
train loss:  0.4700569212436676
train gradient:  0.11950933451537578
iteration : 8835
train acc:  0.765625
train loss:  0.45621636509895325
train gradient:  0.12086797980701844
iteration : 8836
train acc:  0.7578125
train loss:  0.4738684296607971
train gradient:  0.13979773175118962
iteration : 8837
train acc:  0.765625
train loss:  0.4669298231601715
train gradient:  0.11170232215147055
iteration : 8838
train acc:  0.7265625
train loss:  0.46976912021636963
train gradient:  0.14196136617166044
iteration : 8839
train acc:  0.765625
train loss:  0.4502134323120117
train gradient:  0.11212546452061133
iteration : 8840
train acc:  0.734375
train loss:  0.47585684061050415
train gradient:  0.11696376538697574
iteration : 8841
train acc:  0.7265625
train loss:  0.5215561985969543
train gradient:  0.12442535151094969
iteration : 8842
train acc:  0.703125
train loss:  0.4837386906147003
train gradient:  0.11491081028056783
iteration : 8843
train acc:  0.75
train loss:  0.4797709584236145
train gradient:  0.11534593833526717
iteration : 8844
train acc:  0.765625
train loss:  0.535709023475647
train gradient:  0.1746916546379691
iteration : 8845
train acc:  0.75
train loss:  0.49731963872909546
train gradient:  0.129444110266696
iteration : 8846
train acc:  0.6953125
train loss:  0.515602707862854
train gradient:  0.11714298067953331
iteration : 8847
train acc:  0.7109375
train loss:  0.5447293519973755
train gradient:  0.14099640411027087
iteration : 8848
train acc:  0.7890625
train loss:  0.43087542057037354
train gradient:  0.14202472011170214
iteration : 8849
train acc:  0.6640625
train loss:  0.5841768980026245
train gradient:  0.1781683946331356
iteration : 8850
train acc:  0.671875
train loss:  0.5172228813171387
train gradient:  0.1291868283209766
iteration : 8851
train acc:  0.6796875
train loss:  0.5476311445236206
train gradient:  0.13948244513232327
iteration : 8852
train acc:  0.7734375
train loss:  0.46999412775039673
train gradient:  0.12084873797395565
iteration : 8853
train acc:  0.7734375
train loss:  0.442776620388031
train gradient:  0.09759480966419753
iteration : 8854
train acc:  0.7421875
train loss:  0.5107051730155945
train gradient:  0.1342912970772788
iteration : 8855
train acc:  0.765625
train loss:  0.5157346129417419
train gradient:  0.1304150519934439
iteration : 8856
train acc:  0.7421875
train loss:  0.48559051752090454
train gradient:  0.12074070182398033
iteration : 8857
train acc:  0.7890625
train loss:  0.4861987829208374
train gradient:  0.1368071793990825
iteration : 8858
train acc:  0.734375
train loss:  0.505389928817749
train gradient:  0.14607690731628842
iteration : 8859
train acc:  0.734375
train loss:  0.5247564911842346
train gradient:  0.134152802173706
iteration : 8860
train acc:  0.7734375
train loss:  0.5297759771347046
train gradient:  0.1709092925953425
iteration : 8861
train acc:  0.7890625
train loss:  0.4476305842399597
train gradient:  0.09711337632895245
iteration : 8862
train acc:  0.71875
train loss:  0.564940333366394
train gradient:  0.1550879091955482
iteration : 8863
train acc:  0.734375
train loss:  0.4795202612876892
train gradient:  0.11523149143234464
iteration : 8864
train acc:  0.78125
train loss:  0.43625009059906006
train gradient:  0.0727130673613208
iteration : 8865
train acc:  0.7265625
train loss:  0.5470243096351624
train gradient:  0.14901655376648448
iteration : 8866
train acc:  0.6484375
train loss:  0.5969944596290588
train gradient:  0.153800166824259
iteration : 8867
train acc:  0.7734375
train loss:  0.45509108901023865
train gradient:  0.11649099170861044
iteration : 8868
train acc:  0.7109375
train loss:  0.5389266014099121
train gradient:  0.1700650074986982
iteration : 8869
train acc:  0.734375
train loss:  0.5191413164138794
train gradient:  0.19217134184756818
iteration : 8870
train acc:  0.65625
train loss:  0.5462822914123535
train gradient:  0.20044035647638564
iteration : 8871
train acc:  0.78125
train loss:  0.481527179479599
train gradient:  0.09768179919427943
iteration : 8872
train acc:  0.75
train loss:  0.5065652132034302
train gradient:  0.11458247441099949
iteration : 8873
train acc:  0.828125
train loss:  0.4226657748222351
train gradient:  0.11014625320921928
iteration : 8874
train acc:  0.71875
train loss:  0.53398597240448
train gradient:  0.15680650864731593
iteration : 8875
train acc:  0.796875
train loss:  0.3896413743495941
train gradient:  0.07864130916383165
iteration : 8876
train acc:  0.734375
train loss:  0.5060765743255615
train gradient:  0.15867763586669323
iteration : 8877
train acc:  0.7109375
train loss:  0.5071365237236023
train gradient:  0.1331627780385839
iteration : 8878
train acc:  0.71875
train loss:  0.5445036888122559
train gradient:  0.15033116746011987
iteration : 8879
train acc:  0.7109375
train loss:  0.5387248396873474
train gradient:  0.18954753935255525
iteration : 8880
train acc:  0.734375
train loss:  0.44510596990585327
train gradient:  0.11783713211093842
iteration : 8881
train acc:  0.6796875
train loss:  0.5083768963813782
train gradient:  0.12819867951889105
iteration : 8882
train acc:  0.703125
train loss:  0.5398778915405273
train gradient:  0.16551615222646232
iteration : 8883
train acc:  0.734375
train loss:  0.5309612154960632
train gradient:  0.15100402292429332
iteration : 8884
train acc:  0.71875
train loss:  0.5483587384223938
train gradient:  0.15625511989688462
iteration : 8885
train acc:  0.703125
train loss:  0.544358491897583
train gradient:  0.1557946406714117
iteration : 8886
train acc:  0.796875
train loss:  0.4392945468425751
train gradient:  0.09569008948118851
iteration : 8887
train acc:  0.734375
train loss:  0.5261963605880737
train gradient:  0.19977996330671505
iteration : 8888
train acc:  0.6484375
train loss:  0.538068413734436
train gradient:  0.1668017058973486
iteration : 8889
train acc:  0.7890625
train loss:  0.47128617763519287
train gradient:  0.09866610791977654
iteration : 8890
train acc:  0.734375
train loss:  0.4768974184989929
train gradient:  0.13348630353328728
iteration : 8891
train acc:  0.734375
train loss:  0.4699905216693878
train gradient:  0.1132034363979566
iteration : 8892
train acc:  0.671875
train loss:  0.5654596090316772
train gradient:  0.15170278527904973
iteration : 8893
train acc:  0.75
train loss:  0.5113518238067627
train gradient:  0.13975257441989464
iteration : 8894
train acc:  0.7421875
train loss:  0.49915337562561035
train gradient:  0.18314838048320747
iteration : 8895
train acc:  0.6875
train loss:  0.554111659526825
train gradient:  0.13575048685432645
iteration : 8896
train acc:  0.734375
train loss:  0.5171376466751099
train gradient:  0.13249389780561754
iteration : 8897
train acc:  0.7578125
train loss:  0.4804125428199768
train gradient:  0.15514737776042467
iteration : 8898
train acc:  0.78125
train loss:  0.4818260073661804
train gradient:  0.121421609238738
iteration : 8899
train acc:  0.75
train loss:  0.49331414699554443
train gradient:  0.12403314177459882
iteration : 8900
train acc:  0.7421875
train loss:  0.5181806087493896
train gradient:  0.1614663099797834
iteration : 8901
train acc:  0.6875
train loss:  0.5733574032783508
train gradient:  0.16660323334513266
iteration : 8902
train acc:  0.765625
train loss:  0.49315232038497925
train gradient:  0.11867720600998787
iteration : 8903
train acc:  0.7109375
train loss:  0.4963911473751068
train gradient:  0.17627741005409198
iteration : 8904
train acc:  0.7109375
train loss:  0.5183819532394409
train gradient:  0.16899282480766625
iteration : 8905
train acc:  0.75
train loss:  0.4346426725387573
train gradient:  0.08608943743277762
iteration : 8906
train acc:  0.75
train loss:  0.5038591623306274
train gradient:  0.12398994583736504
iteration : 8907
train acc:  0.7421875
train loss:  0.5424736738204956
train gradient:  0.15227965091505258
iteration : 8908
train acc:  0.671875
train loss:  0.5731003880500793
train gradient:  0.17962024233530421
iteration : 8909
train acc:  0.7734375
train loss:  0.4673624038696289
train gradient:  0.1317791801863482
iteration : 8910
train acc:  0.78125
train loss:  0.4238157868385315
train gradient:  0.08992000948307265
iteration : 8911
train acc:  0.7265625
train loss:  0.4940265417098999
train gradient:  0.17323900508990472
iteration : 8912
train acc:  0.765625
train loss:  0.5020174980163574
train gradient:  0.13821878016210937
iteration : 8913
train acc:  0.71875
train loss:  0.5572019219398499
train gradient:  0.15915627376764313
iteration : 8914
train acc:  0.7578125
train loss:  0.4738677442073822
train gradient:  0.1591259127966036
iteration : 8915
train acc:  0.7734375
train loss:  0.45427781343460083
train gradient:  0.11251810997561301
iteration : 8916
train acc:  0.7109375
train loss:  0.5056825876235962
train gradient:  0.12712116247879665
iteration : 8917
train acc:  0.75
train loss:  0.4685838222503662
train gradient:  0.18315401608977383
iteration : 8918
train acc:  0.6875
train loss:  0.5677858591079712
train gradient:  0.15478217855677667
iteration : 8919
train acc:  0.7421875
train loss:  0.4423571825027466
train gradient:  0.11643317057791744
iteration : 8920
train acc:  0.7578125
train loss:  0.5439140796661377
train gradient:  0.1727764469716664
iteration : 8921
train acc:  0.796875
train loss:  0.467022567987442
train gradient:  0.16243441526467856
iteration : 8922
train acc:  0.734375
train loss:  0.48548078536987305
train gradient:  0.1633440968345149
iteration : 8923
train acc:  0.6640625
train loss:  0.5416127443313599
train gradient:  0.13196542468955091
iteration : 8924
train acc:  0.6875
train loss:  0.5661766529083252
train gradient:  0.15187288323193962
iteration : 8925
train acc:  0.6875
train loss:  0.5942209362983704
train gradient:  0.1664328142065737
iteration : 8926
train acc:  0.703125
train loss:  0.5482792854309082
train gradient:  0.15164010195252675
iteration : 8927
train acc:  0.7890625
train loss:  0.4479908049106598
train gradient:  0.10376048708506684
iteration : 8928
train acc:  0.8046875
train loss:  0.479397177696228
train gradient:  0.12367707801159354
iteration : 8929
train acc:  0.7109375
train loss:  0.5003336071968079
train gradient:  0.13457585743870076
iteration : 8930
train acc:  0.7578125
train loss:  0.4746452569961548
train gradient:  0.10891466665396138
iteration : 8931
train acc:  0.6953125
train loss:  0.5332512855529785
train gradient:  0.14360014910433497
iteration : 8932
train acc:  0.765625
train loss:  0.4845636487007141
train gradient:  0.11835030240196649
iteration : 8933
train acc:  0.7578125
train loss:  0.4818898141384125
train gradient:  0.1361217010032731
iteration : 8934
train acc:  0.75
train loss:  0.4675963521003723
train gradient:  0.12829773455063626
iteration : 8935
train acc:  0.75
train loss:  0.5143245458602905
train gradient:  0.13829422937391825
iteration : 8936
train acc:  0.734375
train loss:  0.498289555311203
train gradient:  0.1552692786818643
iteration : 8937
train acc:  0.8125
train loss:  0.42467623949050903
train gradient:  0.09564411375205148
iteration : 8938
train acc:  0.796875
train loss:  0.4847978949546814
train gradient:  0.12980422232160518
iteration : 8939
train acc:  0.7578125
train loss:  0.5104180574417114
train gradient:  0.10276654628509624
iteration : 8940
train acc:  0.75
train loss:  0.46371889114379883
train gradient:  0.09735717434667157
iteration : 8941
train acc:  0.734375
train loss:  0.4931643307209015
train gradient:  0.14314787173024301
iteration : 8942
train acc:  0.7578125
train loss:  0.4915599524974823
train gradient:  0.13020490135713014
iteration : 8943
train acc:  0.734375
train loss:  0.4952716529369354
train gradient:  0.14496700884019048
iteration : 8944
train acc:  0.671875
train loss:  0.5967597961425781
train gradient:  0.1941568459330224
iteration : 8945
train acc:  0.7109375
train loss:  0.5082422494888306
train gradient:  0.15736741933758236
iteration : 8946
train acc:  0.8046875
train loss:  0.4620826244354248
train gradient:  0.13162928835601748
iteration : 8947
train acc:  0.796875
train loss:  0.4073675870895386
train gradient:  0.10441877627300533
iteration : 8948
train acc:  0.7578125
train loss:  0.4813510775566101
train gradient:  0.13470022246131474
iteration : 8949
train acc:  0.7734375
train loss:  0.4986502528190613
train gradient:  0.1479312522505239
iteration : 8950
train acc:  0.6796875
train loss:  0.5258337259292603
train gradient:  0.17597541739474296
iteration : 8951
train acc:  0.8046875
train loss:  0.41742509603500366
train gradient:  0.1218504083066356
iteration : 8952
train acc:  0.671875
train loss:  0.5181008577346802
train gradient:  0.13655560702702244
iteration : 8953
train acc:  0.7578125
train loss:  0.4771367013454437
train gradient:  0.09750078210385038
iteration : 8954
train acc:  0.7421875
train loss:  0.5292952060699463
train gradient:  0.14862359111746398
iteration : 8955
train acc:  0.7109375
train loss:  0.5118236541748047
train gradient:  0.15295276477460149
iteration : 8956
train acc:  0.734375
train loss:  0.5632971525192261
train gradient:  0.15918851800208705
iteration : 8957
train acc:  0.7265625
train loss:  0.49427059292793274
train gradient:  0.15711894802556647
iteration : 8958
train acc:  0.75
train loss:  0.5191490650177002
train gradient:  0.12553480173900228
iteration : 8959
train acc:  0.671875
train loss:  0.5629690289497375
train gradient:  0.15047245865480863
iteration : 8960
train acc:  0.7265625
train loss:  0.5031558275222778
train gradient:  0.11987473022496194
iteration : 8961
train acc:  0.796875
train loss:  0.48290884494781494
train gradient:  0.1880233954452925
iteration : 8962
train acc:  0.6875
train loss:  0.5609735250473022
train gradient:  0.17814830549351368
iteration : 8963
train acc:  0.7265625
train loss:  0.4983723759651184
train gradient:  0.1291936701849136
iteration : 8964
train acc:  0.7890625
train loss:  0.4463128447532654
train gradient:  0.14505444095430053
iteration : 8965
train acc:  0.734375
train loss:  0.4617253541946411
train gradient:  0.11094181616588175
iteration : 8966
train acc:  0.75
train loss:  0.48680827021598816
train gradient:  0.11861668315508642
iteration : 8967
train acc:  0.71875
train loss:  0.48720476031303406
train gradient:  0.12312518260021132
iteration : 8968
train acc:  0.71875
train loss:  0.5382083058357239
train gradient:  0.1498662005457756
iteration : 8969
train acc:  0.8046875
train loss:  0.4552013576030731
train gradient:  0.1133722234826356
iteration : 8970
train acc:  0.734375
train loss:  0.5580637454986572
train gradient:  0.17772549336965807
iteration : 8971
train acc:  0.75
train loss:  0.49686503410339355
train gradient:  0.12168415025607994
iteration : 8972
train acc:  0.703125
train loss:  0.49799466133117676
train gradient:  0.13506862279674808
iteration : 8973
train acc:  0.796875
train loss:  0.46170157194137573
train gradient:  0.12698468337471433
iteration : 8974
train acc:  0.765625
train loss:  0.4832896888256073
train gradient:  0.11188785633064674
iteration : 8975
train acc:  0.71875
train loss:  0.5173162221908569
train gradient:  0.1612394117984163
iteration : 8976
train acc:  0.8046875
train loss:  0.45110785961151123
train gradient:  0.12485955101367485
iteration : 8977
train acc:  0.640625
train loss:  0.5739231705665588
train gradient:  0.1848358006702551
iteration : 8978
train acc:  0.734375
train loss:  0.48791345953941345
train gradient:  0.14263839544234114
iteration : 8979
train acc:  0.7578125
train loss:  0.529464840888977
train gradient:  0.1503318760325814
iteration : 8980
train acc:  0.8046875
train loss:  0.4424375295639038
train gradient:  0.13056358272605
iteration : 8981
train acc:  0.75
train loss:  0.5241271257400513
train gradient:  0.14004360384416453
iteration : 8982
train acc:  0.7734375
train loss:  0.475452184677124
train gradient:  0.10779355834119654
iteration : 8983
train acc:  0.7578125
train loss:  0.4743478298187256
train gradient:  0.11921785504512483
iteration : 8984
train acc:  0.75
train loss:  0.46491628885269165
train gradient:  0.12163694764809811
iteration : 8985
train acc:  0.734375
train loss:  0.5105278491973877
train gradient:  0.1334256206109697
iteration : 8986
train acc:  0.796875
train loss:  0.4174720048904419
train gradient:  0.1160935663585906
iteration : 8987
train acc:  0.71875
train loss:  0.5268493890762329
train gradient:  0.15426876852657667
iteration : 8988
train acc:  0.71875
train loss:  0.5491811037063599
train gradient:  0.15064342740054004
iteration : 8989
train acc:  0.765625
train loss:  0.5120761394500732
train gradient:  0.19486076357885995
iteration : 8990
train acc:  0.765625
train loss:  0.47179654240608215
train gradient:  0.12127309401863352
iteration : 8991
train acc:  0.796875
train loss:  0.4745748043060303
train gradient:  0.11444934100808671
iteration : 8992
train acc:  0.7734375
train loss:  0.47350361943244934
train gradient:  0.11687312363448742
iteration : 8993
train acc:  0.7578125
train loss:  0.5159541368484497
train gradient:  0.18720479476631552
iteration : 8994
train acc:  0.796875
train loss:  0.45816245675086975
train gradient:  0.10437810039286097
iteration : 8995
train acc:  0.7890625
train loss:  0.42258918285369873
train gradient:  0.1054048257580185
iteration : 8996
train acc:  0.765625
train loss:  0.5010900497436523
train gradient:  0.16044233874460773
iteration : 8997
train acc:  0.765625
train loss:  0.45125237107276917
train gradient:  0.11938233762450112
iteration : 8998
train acc:  0.7421875
train loss:  0.5155640244483948
train gradient:  0.141379314030414
iteration : 8999
train acc:  0.7109375
train loss:  0.524549126625061
train gradient:  0.13110305343286302
iteration : 9000
train acc:  0.7109375
train loss:  0.48942887783050537
train gradient:  0.12032619745388094
iteration : 9001
train acc:  0.7421875
train loss:  0.4959685206413269
train gradient:  0.11388799049283552
iteration : 9002
train acc:  0.7109375
train loss:  0.5296214818954468
train gradient:  0.1701400600249791
iteration : 9003
train acc:  0.765625
train loss:  0.4620205760002136
train gradient:  0.11852376403528662
iteration : 9004
train acc:  0.7421875
train loss:  0.5020030736923218
train gradient:  0.17425805103267278
iteration : 9005
train acc:  0.7421875
train loss:  0.5053200721740723
train gradient:  0.13551266961744762
iteration : 9006
train acc:  0.7578125
train loss:  0.5170609951019287
train gradient:  0.14416386845808232
iteration : 9007
train acc:  0.6953125
train loss:  0.5437954664230347
train gradient:  0.14162775618034462
iteration : 9008
train acc:  0.6328125
train loss:  0.5870323181152344
train gradient:  0.1649023381844545
iteration : 9009
train acc:  0.71875
train loss:  0.4984310567378998
train gradient:  0.11670652819558551
iteration : 9010
train acc:  0.7578125
train loss:  0.4478685259819031
train gradient:  0.12273852408814306
iteration : 9011
train acc:  0.7109375
train loss:  0.5416940450668335
train gradient:  0.12425120471551382
iteration : 9012
train acc:  0.6953125
train loss:  0.5210435390472412
train gradient:  0.1399923891273192
iteration : 9013
train acc:  0.8046875
train loss:  0.43023166060447693
train gradient:  0.09656664769962617
iteration : 9014
train acc:  0.7890625
train loss:  0.46276426315307617
train gradient:  0.11338107281436818
iteration : 9015
train acc:  0.6875
train loss:  0.5923352241516113
train gradient:  0.18268375360665656
iteration : 9016
train acc:  0.7578125
train loss:  0.4949440062046051
train gradient:  0.1264890333348287
iteration : 9017
train acc:  0.671875
train loss:  0.5191283822059631
train gradient:  0.18313591396235795
iteration : 9018
train acc:  0.7578125
train loss:  0.4777631461620331
train gradient:  0.12038054888459747
iteration : 9019
train acc:  0.6875
train loss:  0.5284060835838318
train gradient:  0.11688194550846139
iteration : 9020
train acc:  0.71875
train loss:  0.4635242223739624
train gradient:  0.1132627485100859
iteration : 9021
train acc:  0.7890625
train loss:  0.4669002294540405
train gradient:  0.18909683121247167
iteration : 9022
train acc:  0.6875
train loss:  0.5170645117759705
train gradient:  0.13788013615330846
iteration : 9023
train acc:  0.734375
train loss:  0.5245708227157593
train gradient:  0.1337940663625379
iteration : 9024
train acc:  0.6796875
train loss:  0.5331078767776489
train gradient:  0.19006812889594882
iteration : 9025
train acc:  0.71875
train loss:  0.5510799288749695
train gradient:  0.17807969476614038
iteration : 9026
train acc:  0.625
train loss:  0.5769206285476685
train gradient:  0.19473219929961377
iteration : 9027
train acc:  0.8046875
train loss:  0.4384923279285431
train gradient:  0.10252885078049725
iteration : 9028
train acc:  0.7421875
train loss:  0.5270211696624756
train gradient:  0.20183135637919525
iteration : 9029
train acc:  0.7890625
train loss:  0.42993485927581787
train gradient:  0.10716492772279261
iteration : 9030
train acc:  0.6875
train loss:  0.5659884214401245
train gradient:  0.22223412978202794
iteration : 9031
train acc:  0.6796875
train loss:  0.5232157707214355
train gradient:  0.13781579329543583
iteration : 9032
train acc:  0.7265625
train loss:  0.4747101068496704
train gradient:  0.1309394519005328
iteration : 9033
train acc:  0.6484375
train loss:  0.5607889890670776
train gradient:  0.19108246863380068
iteration : 9034
train acc:  0.734375
train loss:  0.4868878126144409
train gradient:  0.1259806270799687
iteration : 9035
train acc:  0.7578125
train loss:  0.4547582268714905
train gradient:  0.10898024458544703
iteration : 9036
train acc:  0.765625
train loss:  0.44323939085006714
train gradient:  0.12434209515407363
iteration : 9037
train acc:  0.7265625
train loss:  0.5321468710899353
train gradient:  0.1581639933068434
iteration : 9038
train acc:  0.7578125
train loss:  0.45739060640335083
train gradient:  0.13409478791084384
iteration : 9039
train acc:  0.7734375
train loss:  0.47675222158432007
train gradient:  0.15698310816234978
iteration : 9040
train acc:  0.71875
train loss:  0.5164662003517151
train gradient:  0.12367913906464842
iteration : 9041
train acc:  0.7421875
train loss:  0.4435153603553772
train gradient:  0.11187532047251689
iteration : 9042
train acc:  0.703125
train loss:  0.528967559337616
train gradient:  0.16836686194465628
iteration : 9043
train acc:  0.71875
train loss:  0.48604321479797363
train gradient:  0.11431590960455808
iteration : 9044
train acc:  0.703125
train loss:  0.5425060391426086
train gradient:  0.14881676608584393
iteration : 9045
train acc:  0.6875
train loss:  0.5617926120758057
train gradient:  0.12938113654439737
iteration : 9046
train acc:  0.796875
train loss:  0.4958290755748749
train gradient:  0.13558551941167268
iteration : 9047
train acc:  0.703125
train loss:  0.5478335022926331
train gradient:  0.19776900580107337
iteration : 9048
train acc:  0.671875
train loss:  0.5545207262039185
train gradient:  0.20281119195307606
iteration : 9049
train acc:  0.6875
train loss:  0.5241214632987976
train gradient:  0.13354687554022288
iteration : 9050
train acc:  0.7578125
train loss:  0.4606187641620636
train gradient:  0.11976559312597002
iteration : 9051
train acc:  0.7734375
train loss:  0.45248061418533325
train gradient:  0.11180859602631642
iteration : 9052
train acc:  0.7109375
train loss:  0.5010486841201782
train gradient:  0.13045671259854658
iteration : 9053
train acc:  0.734375
train loss:  0.4963195323944092
train gradient:  0.114534739383907
iteration : 9054
train acc:  0.734375
train loss:  0.4598505198955536
train gradient:  0.1152054990696477
iteration : 9055
train acc:  0.703125
train loss:  0.5397054553031921
train gradient:  0.15888986346754755
iteration : 9056
train acc:  0.6640625
train loss:  0.5826937556266785
train gradient:  0.1615692091374925
iteration : 9057
train acc:  0.703125
train loss:  0.5467778444290161
train gradient:  0.21469317123929788
iteration : 9058
train acc:  0.75
train loss:  0.5042943358421326
train gradient:  0.1382760592703263
iteration : 9059
train acc:  0.78125
train loss:  0.44926735758781433
train gradient:  0.10544169727833624
iteration : 9060
train acc:  0.765625
train loss:  0.49901923537254333
train gradient:  0.16738289913702148
iteration : 9061
train acc:  0.8671875
train loss:  0.41998350620269775
train gradient:  0.13556929161062856
iteration : 9062
train acc:  0.78125
train loss:  0.47071942687034607
train gradient:  0.10342600482744574
iteration : 9063
train acc:  0.7734375
train loss:  0.44668325781822205
train gradient:  0.14172548139574065
iteration : 9064
train acc:  0.6875
train loss:  0.5101125836372375
train gradient:  0.11993756188102646
iteration : 9065
train acc:  0.7421875
train loss:  0.5376942753791809
train gradient:  0.20982304520820005
iteration : 9066
train acc:  0.703125
train loss:  0.5233098268508911
train gradient:  0.18455307612941652
iteration : 9067
train acc:  0.765625
train loss:  0.4944092035293579
train gradient:  0.13353215585069306
iteration : 9068
train acc:  0.7265625
train loss:  0.4875343441963196
train gradient:  0.14208609564931396
iteration : 9069
train acc:  0.75
train loss:  0.47094041109085083
train gradient:  0.10689241942712359
iteration : 9070
train acc:  0.765625
train loss:  0.4482755661010742
train gradient:  0.11865545589173795
iteration : 9071
train acc:  0.7109375
train loss:  0.49587321281433105
train gradient:  0.13811176063782554
iteration : 9072
train acc:  0.6875
train loss:  0.528678297996521
train gradient:  0.15680767839362003
iteration : 9073
train acc:  0.7265625
train loss:  0.5411347150802612
train gradient:  0.18217442558947602
iteration : 9074
train acc:  0.703125
train loss:  0.5227422714233398
train gradient:  0.14887737580396282
iteration : 9075
train acc:  0.78125
train loss:  0.49631112813949585
train gradient:  0.14171294441083893
iteration : 9076
train acc:  0.734375
train loss:  0.527783215045929
train gradient:  0.1542320253551034
iteration : 9077
train acc:  0.78125
train loss:  0.46116116642951965
train gradient:  0.13907726705431978
iteration : 9078
train acc:  0.7578125
train loss:  0.5413742065429688
train gradient:  0.220293754479643
iteration : 9079
train acc:  0.7265625
train loss:  0.5257248878479004
train gradient:  0.14095113840558338
iteration : 9080
train acc:  0.7890625
train loss:  0.46666038036346436
train gradient:  0.12322000102153984
iteration : 9081
train acc:  0.71875
train loss:  0.4819871783256531
train gradient:  0.15963700882755133
iteration : 9082
train acc:  0.75
train loss:  0.46422258019447327
train gradient:  0.11238037358359418
iteration : 9083
train acc:  0.7890625
train loss:  0.4166407585144043
train gradient:  0.1076170642882372
iteration : 9084
train acc:  0.71875
train loss:  0.5656204223632812
train gradient:  0.15221579657799858
iteration : 9085
train acc:  0.6875
train loss:  0.515091061592102
train gradient:  0.12194913036710003
iteration : 9086
train acc:  0.7578125
train loss:  0.48252424597740173
train gradient:  0.12870871555143434
iteration : 9087
train acc:  0.7578125
train loss:  0.5051752328872681
train gradient:  0.12309531682509389
iteration : 9088
train acc:  0.671875
train loss:  0.5269593000411987
train gradient:  0.1519829496628737
iteration : 9089
train acc:  0.6953125
train loss:  0.5973687171936035
train gradient:  0.16377628531455365
iteration : 9090
train acc:  0.7578125
train loss:  0.4789254069328308
train gradient:  0.11632957263258527
iteration : 9091
train acc:  0.7421875
train loss:  0.48293256759643555
train gradient:  0.13956641440819753
iteration : 9092
train acc:  0.78125
train loss:  0.4327318072319031
train gradient:  0.10244070518723694
iteration : 9093
train acc:  0.7421875
train loss:  0.46731337904930115
train gradient:  0.14980759340760674
iteration : 9094
train acc:  0.7109375
train loss:  0.5301099419593811
train gradient:  0.12434264651038723
iteration : 9095
train acc:  0.671875
train loss:  0.6303648948669434
train gradient:  0.19399752274721976
iteration : 9096
train acc:  0.7578125
train loss:  0.45338428020477295
train gradient:  0.11991829914995555
iteration : 9097
train acc:  0.7265625
train loss:  0.5419800281524658
train gradient:  0.16647550539830763
iteration : 9098
train acc:  0.7109375
train loss:  0.4962831437587738
train gradient:  0.11125998555910872
iteration : 9099
train acc:  0.734375
train loss:  0.45188236236572266
train gradient:  0.11120826487486293
iteration : 9100
train acc:  0.75
train loss:  0.49263298511505127
train gradient:  0.13914680899198167
iteration : 9101
train acc:  0.75
train loss:  0.4719431400299072
train gradient:  0.12341944990631507
iteration : 9102
train acc:  0.7890625
train loss:  0.433726966381073
train gradient:  0.10364022880784436
iteration : 9103
train acc:  0.7109375
train loss:  0.524808406829834
train gradient:  0.1326970531898814
iteration : 9104
train acc:  0.8125
train loss:  0.43215757608413696
train gradient:  0.10354047170904719
iteration : 9105
train acc:  0.734375
train loss:  0.5201330780982971
train gradient:  0.1791340712781022
iteration : 9106
train acc:  0.7421875
train loss:  0.5367767214775085
train gradient:  0.156156611819945
iteration : 9107
train acc:  0.7421875
train loss:  0.4238526225090027
train gradient:  0.11769074475075772
iteration : 9108
train acc:  0.6953125
train loss:  0.5323588252067566
train gradient:  0.1291147528668882
iteration : 9109
train acc:  0.734375
train loss:  0.5167009830474854
train gradient:  0.12097800803521601
iteration : 9110
train acc:  0.703125
train loss:  0.49682173132896423
train gradient:  0.1234847436391155
iteration : 9111
train acc:  0.734375
train loss:  0.5042224526405334
train gradient:  0.12268768385684707
iteration : 9112
train acc:  0.625
train loss:  0.6208318471908569
train gradient:  0.16959999125279124
iteration : 9113
train acc:  0.7734375
train loss:  0.4621520936489105
train gradient:  0.1411946051538832
iteration : 9114
train acc:  0.7421875
train loss:  0.46463674306869507
train gradient:  0.11997769921581329
iteration : 9115
train acc:  0.7421875
train loss:  0.5191015005111694
train gradient:  0.15436307758897072
iteration : 9116
train acc:  0.734375
train loss:  0.44781094789505005
train gradient:  0.09234534990662038
iteration : 9117
train acc:  0.71875
train loss:  0.5469604730606079
train gradient:  0.15051646185514314
iteration : 9118
train acc:  0.703125
train loss:  0.5137063264846802
train gradient:  0.16387997090017623
iteration : 9119
train acc:  0.765625
train loss:  0.47523027658462524
train gradient:  0.1496905459496427
iteration : 9120
train acc:  0.6953125
train loss:  0.5129926204681396
train gradient:  0.13043992659751408
iteration : 9121
train acc:  0.8046875
train loss:  0.408649742603302
train gradient:  0.1071909896474647
iteration : 9122
train acc:  0.6796875
train loss:  0.5645994544029236
train gradient:  0.1806213911582869
iteration : 9123
train acc:  0.640625
train loss:  0.5926786065101624
train gradient:  0.16244347365206155
iteration : 9124
train acc:  0.6875
train loss:  0.4741862416267395
train gradient:  0.1252570289637735
iteration : 9125
train acc:  0.78125
train loss:  0.5375794172286987
train gradient:  0.15000303342878674
iteration : 9126
train acc:  0.7734375
train loss:  0.5097837448120117
train gradient:  0.1334305896023065
iteration : 9127
train acc:  0.75
train loss:  0.46079346537590027
train gradient:  0.13382764610334436
iteration : 9128
train acc:  0.734375
train loss:  0.4969657361507416
train gradient:  0.13162428797850062
iteration : 9129
train acc:  0.7890625
train loss:  0.4127342700958252
train gradient:  0.1091272762280551
iteration : 9130
train acc:  0.7265625
train loss:  0.5036489367485046
train gradient:  0.15581792478994683
iteration : 9131
train acc:  0.765625
train loss:  0.46754178404808044
train gradient:  0.13126728373229984
iteration : 9132
train acc:  0.7421875
train loss:  0.541478157043457
train gradient:  0.14136157791807272
iteration : 9133
train acc:  0.6875
train loss:  0.6266614198684692
train gradient:  0.22733839744258988
iteration : 9134
train acc:  0.7421875
train loss:  0.5421831011772156
train gradient:  0.16116252718968083
iteration : 9135
train acc:  0.7109375
train loss:  0.44997596740722656
train gradient:  0.09988373885828568
iteration : 9136
train acc:  0.75
train loss:  0.49926796555519104
train gradient:  0.14367770290777104
iteration : 9137
train acc:  0.71875
train loss:  0.5245418548583984
train gradient:  0.14115127310367315
iteration : 9138
train acc:  0.6953125
train loss:  0.5234327912330627
train gradient:  0.14608462916369075
iteration : 9139
train acc:  0.796875
train loss:  0.47415053844451904
train gradient:  0.1300216669385233
iteration : 9140
train acc:  0.75
train loss:  0.47086673974990845
train gradient:  0.12234407424753198
iteration : 9141
train acc:  0.7578125
train loss:  0.4889235198497772
train gradient:  0.13607803254736017
iteration : 9142
train acc:  0.7734375
train loss:  0.4816340506076813
train gradient:  0.16999770904959502
iteration : 9143
train acc:  0.75
train loss:  0.4430607259273529
train gradient:  0.09283532768699226
iteration : 9144
train acc:  0.796875
train loss:  0.46009594202041626
train gradient:  0.10863032656479042
iteration : 9145
train acc:  0.796875
train loss:  0.5290048122406006
train gradient:  0.14309503118989234
iteration : 9146
train acc:  0.7734375
train loss:  0.46988239884376526
train gradient:  0.10952283459770652
iteration : 9147
train acc:  0.7265625
train loss:  0.5012317299842834
train gradient:  0.11260449771047783
iteration : 9148
train acc:  0.765625
train loss:  0.5035057067871094
train gradient:  0.13396352798285657
iteration : 9149
train acc:  0.703125
train loss:  0.5108508467674255
train gradient:  0.13144949041976778
iteration : 9150
train acc:  0.7109375
train loss:  0.5119506120681763
train gradient:  0.11715677681328476
iteration : 9151
train acc:  0.7265625
train loss:  0.5425538420677185
train gradient:  0.1264823474119596
iteration : 9152
train acc:  0.75
train loss:  0.45838111639022827
train gradient:  0.11064878234687127
iteration : 9153
train acc:  0.75
train loss:  0.5270570516586304
train gradient:  0.14562919668054902
iteration : 9154
train acc:  0.78125
train loss:  0.47197532653808594
train gradient:  0.12388506481118197
iteration : 9155
train acc:  0.7578125
train loss:  0.5242066383361816
train gradient:  0.12818865436705407
iteration : 9156
train acc:  0.7109375
train loss:  0.5004943609237671
train gradient:  0.1202543025103089
iteration : 9157
train acc:  0.765625
train loss:  0.46947774291038513
train gradient:  0.12652742025204766
iteration : 9158
train acc:  0.734375
train loss:  0.5161811709403992
train gradient:  0.13649587143768382
iteration : 9159
train acc:  0.7578125
train loss:  0.472787082195282
train gradient:  0.132148498964764
iteration : 9160
train acc:  0.765625
train loss:  0.4624404013156891
train gradient:  0.1009850238269094
iteration : 9161
train acc:  0.7265625
train loss:  0.5125305652618408
train gradient:  0.13732439418587483
iteration : 9162
train acc:  0.71875
train loss:  0.5167537927627563
train gradient:  0.12972270960412322
iteration : 9163
train acc:  0.703125
train loss:  0.5150492191314697
train gradient:  0.12783366960800457
iteration : 9164
train acc:  0.78125
train loss:  0.4997701644897461
train gradient:  0.19697249780205006
iteration : 9165
train acc:  0.6328125
train loss:  0.5483955144882202
train gradient:  0.1725588921824025
iteration : 9166
train acc:  0.7578125
train loss:  0.527432382106781
train gradient:  0.1796662331818967
iteration : 9167
train acc:  0.765625
train loss:  0.48605939745903015
train gradient:  0.12129793134978237
iteration : 9168
train acc:  0.75
train loss:  0.5220573544502258
train gradient:  0.11975311877544653
iteration : 9169
train acc:  0.6875
train loss:  0.558741569519043
train gradient:  0.16680150426069298
iteration : 9170
train acc:  0.7265625
train loss:  0.4904201328754425
train gradient:  0.10178675882552293
iteration : 9171
train acc:  0.8203125
train loss:  0.4415261149406433
train gradient:  0.11581889532408259
iteration : 9172
train acc:  0.7578125
train loss:  0.46287035942077637
train gradient:  0.11006319916053992
iteration : 9173
train acc:  0.78125
train loss:  0.490296870470047
train gradient:  0.13591397274886524
iteration : 9174
train acc:  0.734375
train loss:  0.4978310465812683
train gradient:  0.12017418787457759
iteration : 9175
train acc:  0.734375
train loss:  0.4982171654701233
train gradient:  0.1106479934845564
iteration : 9176
train acc:  0.7265625
train loss:  0.5126998424530029
train gradient:  0.14711127032383942
iteration : 9177
train acc:  0.65625
train loss:  0.6014564037322998
train gradient:  0.1849770471664028
iteration : 9178
train acc:  0.6953125
train loss:  0.5344778299331665
train gradient:  0.161358813573461
iteration : 9179
train acc:  0.71875
train loss:  0.4951457977294922
train gradient:  0.11537936984218199
iteration : 9180
train acc:  0.7890625
train loss:  0.47661614418029785
train gradient:  0.1071064111634741
iteration : 9181
train acc:  0.7265625
train loss:  0.47870421409606934
train gradient:  0.10273815884335782
iteration : 9182
train acc:  0.7109375
train loss:  0.495804101228714
train gradient:  0.14311777369095877
iteration : 9183
train acc:  0.796875
train loss:  0.44749951362609863
train gradient:  0.11594397121133519
iteration : 9184
train acc:  0.7421875
train loss:  0.5230040550231934
train gradient:  0.20168900825386593
iteration : 9185
train acc:  0.765625
train loss:  0.46834880113601685
train gradient:  0.12235431790738036
iteration : 9186
train acc:  0.6875
train loss:  0.4972372353076935
train gradient:  0.11909553694557445
iteration : 9187
train acc:  0.7890625
train loss:  0.43096303939819336
train gradient:  0.09174076012856955
iteration : 9188
train acc:  0.734375
train loss:  0.44785356521606445
train gradient:  0.11078964882491392
iteration : 9189
train acc:  0.796875
train loss:  0.426307737827301
train gradient:  0.1262018239168171
iteration : 9190
train acc:  0.78125
train loss:  0.4870535433292389
train gradient:  0.12271154715661989
iteration : 9191
train acc:  0.765625
train loss:  0.4749794602394104
train gradient:  0.10774020833157369
iteration : 9192
train acc:  0.6640625
train loss:  0.5207107663154602
train gradient:  0.13920828123522594
iteration : 9193
train acc:  0.734375
train loss:  0.4983196556568146
train gradient:  0.14208365423294023
iteration : 9194
train acc:  0.71875
train loss:  0.4937347173690796
train gradient:  0.11498296659296914
iteration : 9195
train acc:  0.7734375
train loss:  0.46327829360961914
train gradient:  0.1181418717435787
iteration : 9196
train acc:  0.7421875
train loss:  0.5108907222747803
train gradient:  0.15271851258359545
iteration : 9197
train acc:  0.7890625
train loss:  0.46325305104255676
train gradient:  0.13762501279949999
iteration : 9198
train acc:  0.6875
train loss:  0.5739105939865112
train gradient:  0.1762673521204565
iteration : 9199
train acc:  0.6953125
train loss:  0.5846344232559204
train gradient:  0.17506114603179584
iteration : 9200
train acc:  0.6953125
train loss:  0.5228476524353027
train gradient:  0.12539058569660022
iteration : 9201
train acc:  0.7109375
train loss:  0.5060120224952698
train gradient:  0.12981171986916312
iteration : 9202
train acc:  0.75
train loss:  0.5321031808853149
train gradient:  0.1971493449879448
iteration : 9203
train acc:  0.8125
train loss:  0.43234843015670776
train gradient:  0.09378609749876202
iteration : 9204
train acc:  0.8046875
train loss:  0.48218846321105957
train gradient:  0.13577723290026733
iteration : 9205
train acc:  0.703125
train loss:  0.5385711789131165
train gradient:  0.16700543832957176
iteration : 9206
train acc:  0.6484375
train loss:  0.543864369392395
train gradient:  0.15572290597958477
iteration : 9207
train acc:  0.734375
train loss:  0.5178738236427307
train gradient:  0.17718128225925905
iteration : 9208
train acc:  0.703125
train loss:  0.5811368227005005
train gradient:  0.1954424214471822
iteration : 9209
train acc:  0.71875
train loss:  0.5392681360244751
train gradient:  0.15474178199584307
iteration : 9210
train acc:  0.7109375
train loss:  0.5020540356636047
train gradient:  0.11774728874502484
iteration : 9211
train acc:  0.78125
train loss:  0.43519774079322815
train gradient:  0.10820992050123884
iteration : 9212
train acc:  0.6875
train loss:  0.5823351144790649
train gradient:  0.1387484059932766
iteration : 9213
train acc:  0.7265625
train loss:  0.4799269437789917
train gradient:  0.13412788990466318
iteration : 9214
train acc:  0.7265625
train loss:  0.5237808227539062
train gradient:  0.11558236571438431
iteration : 9215
train acc:  0.75
train loss:  0.5177239179611206
train gradient:  0.13542495222243595
iteration : 9216
train acc:  0.7734375
train loss:  0.4993852972984314
train gradient:  0.13551866857247535
iteration : 9217
train acc:  0.765625
train loss:  0.4501461386680603
train gradient:  0.14849406287858402
iteration : 9218
train acc:  0.7578125
train loss:  0.4935959279537201
train gradient:  0.1467760416958085
iteration : 9219
train acc:  0.65625
train loss:  0.5567824840545654
train gradient:  0.16814568920722558
iteration : 9220
train acc:  0.703125
train loss:  0.5168077349662781
train gradient:  0.13518801035446243
iteration : 9221
train acc:  0.765625
train loss:  0.48662084341049194
train gradient:  0.10864906109477382
iteration : 9222
train acc:  0.7578125
train loss:  0.44621217250823975
train gradient:  0.1131050936710798
iteration : 9223
train acc:  0.7109375
train loss:  0.47864866256713867
train gradient:  0.1206571724111651
iteration : 9224
train acc:  0.78125
train loss:  0.44248223304748535
train gradient:  0.09309696710066118
iteration : 9225
train acc:  0.71875
train loss:  0.49176672101020813
train gradient:  0.13404017720082778
iteration : 9226
train acc:  0.6953125
train loss:  0.550228476524353
train gradient:  0.1401387393043388
iteration : 9227
train acc:  0.7734375
train loss:  0.46883079409599304
train gradient:  0.13964771689945124
iteration : 9228
train acc:  0.7421875
train loss:  0.5070785284042358
train gradient:  0.12558550999140983
iteration : 9229
train acc:  0.7734375
train loss:  0.47972583770751953
train gradient:  0.11886881465101831
iteration : 9230
train acc:  0.703125
train loss:  0.5556623935699463
train gradient:  0.17017502822097147
iteration : 9231
train acc:  0.7578125
train loss:  0.4968903064727783
train gradient:  0.13918440935858128
iteration : 9232
train acc:  0.6875
train loss:  0.5330116748809814
train gradient:  0.12566196137242433
iteration : 9233
train acc:  0.640625
train loss:  0.5816895961761475
train gradient:  0.18807336529533245
iteration : 9234
train acc:  0.7265625
train loss:  0.5598276853561401
train gradient:  0.1653268660858931
iteration : 9235
train acc:  0.7734375
train loss:  0.42777878046035767
train gradient:  0.10812905187082825
iteration : 9236
train acc:  0.6015625
train loss:  0.6249318718910217
train gradient:  0.2846218175829411
iteration : 9237
train acc:  0.6796875
train loss:  0.47380051016807556
train gradient:  0.11793682271268718
iteration : 9238
train acc:  0.7890625
train loss:  0.4717733860015869
train gradient:  0.1335145035989586
iteration : 9239
train acc:  0.7734375
train loss:  0.4947521686553955
train gradient:  0.14220681063030377
iteration : 9240
train acc:  0.7734375
train loss:  0.4451029598712921
train gradient:  0.13670299640586137
iteration : 9241
train acc:  0.75
train loss:  0.4666352868080139
train gradient:  0.10790146045351383
iteration : 9242
train acc:  0.8046875
train loss:  0.4399458169937134
train gradient:  0.09999676777148941
iteration : 9243
train acc:  0.796875
train loss:  0.5202745199203491
train gradient:  0.16427812055164426
iteration : 9244
train acc:  0.71875
train loss:  0.5711592435836792
train gradient:  0.17391928809310503
iteration : 9245
train acc:  0.6875
train loss:  0.5434890985488892
train gradient:  0.1671465345195479
iteration : 9246
train acc:  0.671875
train loss:  0.5412786602973938
train gradient:  0.13146025948677376
iteration : 9247
train acc:  0.6484375
train loss:  0.5766620635986328
train gradient:  0.16965478511902618
iteration : 9248
train acc:  0.7109375
train loss:  0.4754267930984497
train gradient:  0.11230970048892434
iteration : 9249
train acc:  0.6953125
train loss:  0.5256574153900146
train gradient:  0.13076003959687696
iteration : 9250
train acc:  0.7734375
train loss:  0.4658910632133484
train gradient:  0.10255351692213201
iteration : 9251
train acc:  0.75
train loss:  0.4688856601715088
train gradient:  0.13233625737484328
iteration : 9252
train acc:  0.7578125
train loss:  0.4792114198207855
train gradient:  0.13327049685555914
iteration : 9253
train acc:  0.7734375
train loss:  0.46376875042915344
train gradient:  0.14877352704609215
iteration : 9254
train acc:  0.7421875
train loss:  0.4295980930328369
train gradient:  0.09840147156217063
iteration : 9255
train acc:  0.7578125
train loss:  0.4809507429599762
train gradient:  0.12882417803253365
iteration : 9256
train acc:  0.6484375
train loss:  0.5946371555328369
train gradient:  0.1950260636342984
iteration : 9257
train acc:  0.765625
train loss:  0.5167218446731567
train gradient:  0.15369732187860483
iteration : 9258
train acc:  0.7421875
train loss:  0.4745057225227356
train gradient:  0.1544925439593271
iteration : 9259
train acc:  0.8359375
train loss:  0.4394412040710449
train gradient:  0.12765283514899825
iteration : 9260
train acc:  0.71875
train loss:  0.5085386037826538
train gradient:  0.11233168568034081
iteration : 9261
train acc:  0.75
train loss:  0.49386587738990784
train gradient:  0.1812006323904743
iteration : 9262
train acc:  0.7265625
train loss:  0.4635049104690552
train gradient:  0.09981821629592155
iteration : 9263
train acc:  0.8125
train loss:  0.4291573762893677
train gradient:  0.11164756455672185
iteration : 9264
train acc:  0.7109375
train loss:  0.558052122592926
train gradient:  0.15796975550120357
iteration : 9265
train acc:  0.640625
train loss:  0.5715100765228271
train gradient:  0.1589317553691902
iteration : 9266
train acc:  0.734375
train loss:  0.4902821183204651
train gradient:  0.15007238755965088
iteration : 9267
train acc:  0.71875
train loss:  0.5147866010665894
train gradient:  0.17933427665382878
iteration : 9268
train acc:  0.6875
train loss:  0.5277408361434937
train gradient:  0.2062091970360896
iteration : 9269
train acc:  0.765625
train loss:  0.48009973764419556
train gradient:  0.120327648956086
iteration : 9270
train acc:  0.6796875
train loss:  0.59180748462677
train gradient:  0.17957435297709037
iteration : 9271
train acc:  0.7421875
train loss:  0.5352005362510681
train gradient:  0.15141321175299333
iteration : 9272
train acc:  0.7890625
train loss:  0.4386635422706604
train gradient:  0.1062526573646667
iteration : 9273
train acc:  0.7734375
train loss:  0.452555775642395
train gradient:  0.13504741423303168
iteration : 9274
train acc:  0.734375
train loss:  0.520919680595398
train gradient:  0.17539039508218096
iteration : 9275
train acc:  0.796875
train loss:  0.4646870791912079
train gradient:  0.1352363960682679
iteration : 9276
train acc:  0.734375
train loss:  0.5148810148239136
train gradient:  0.11529095130347354
iteration : 9277
train acc:  0.78125
train loss:  0.4460291862487793
train gradient:  0.13096473289320135
iteration : 9278
train acc:  0.8203125
train loss:  0.44955384731292725
train gradient:  0.1217083472821975
iteration : 9279
train acc:  0.78125
train loss:  0.4625580608844757
train gradient:  0.1338394977321846
iteration : 9280
train acc:  0.75
train loss:  0.4619574248790741
train gradient:  0.09346228534571427
iteration : 9281
train acc:  0.7578125
train loss:  0.49719440937042236
train gradient:  0.1800106524514255
iteration : 9282
train acc:  0.78125
train loss:  0.4588465094566345
train gradient:  0.13074331078239498
iteration : 9283
train acc:  0.703125
train loss:  0.5036637783050537
train gradient:  0.13519504962350926
iteration : 9284
train acc:  0.78125
train loss:  0.4961085319519043
train gradient:  0.14681200090016494
iteration : 9285
train acc:  0.796875
train loss:  0.4681808650493622
train gradient:  0.14371413294859217
iteration : 9286
train acc:  0.7265625
train loss:  0.49672994017601013
train gradient:  0.1677365521649135
iteration : 9287
train acc:  0.734375
train loss:  0.5673041343688965
train gradient:  0.1564604854128987
iteration : 9288
train acc:  0.734375
train loss:  0.5116735696792603
train gradient:  0.13928387345313586
iteration : 9289
train acc:  0.7890625
train loss:  0.4610458016395569
train gradient:  0.10256322676577859
iteration : 9290
train acc:  0.7421875
train loss:  0.4705352783203125
train gradient:  0.13191832221313957
iteration : 9291
train acc:  0.78125
train loss:  0.48652514815330505
train gradient:  0.16784248532529494
iteration : 9292
train acc:  0.78125
train loss:  0.5317957401275635
train gradient:  0.15124725941249853
iteration : 9293
train acc:  0.71875
train loss:  0.5190910696983337
train gradient:  0.1838609647889914
iteration : 9294
train acc:  0.6953125
train loss:  0.5529860854148865
train gradient:  0.1717468505877665
iteration : 9295
train acc:  0.734375
train loss:  0.44918450713157654
train gradient:  0.1143698669299616
iteration : 9296
train acc:  0.7578125
train loss:  0.5098596811294556
train gradient:  0.11965251619495884
iteration : 9297
train acc:  0.8125
train loss:  0.4455941617488861
train gradient:  0.11693977637057426
iteration : 9298
train acc:  0.6796875
train loss:  0.5758647918701172
train gradient:  0.15754822250595535
iteration : 9299
train acc:  0.734375
train loss:  0.4916289448738098
train gradient:  0.17955382964345773
iteration : 9300
train acc:  0.765625
train loss:  0.5100011825561523
train gradient:  0.175577865920681
iteration : 9301
train acc:  0.7734375
train loss:  0.4419439136981964
train gradient:  0.09811236074419764
iteration : 9302
train acc:  0.71875
train loss:  0.514415979385376
train gradient:  0.14553761209029736
iteration : 9303
train acc:  0.7890625
train loss:  0.45863330364227295
train gradient:  0.09391774240457046
iteration : 9304
train acc:  0.703125
train loss:  0.5096807479858398
train gradient:  0.12595644262684494
iteration : 9305
train acc:  0.7109375
train loss:  0.5723298192024231
train gradient:  0.1888015358667713
iteration : 9306
train acc:  0.75
train loss:  0.4493286609649658
train gradient:  0.12972803961040782
iteration : 9307
train acc:  0.734375
train loss:  0.46361780166625977
train gradient:  0.11387399673241998
iteration : 9308
train acc:  0.7421875
train loss:  0.5499081611633301
train gradient:  0.16516798040358016
iteration : 9309
train acc:  0.796875
train loss:  0.501776397228241
train gradient:  0.16692717884281666
iteration : 9310
train acc:  0.765625
train loss:  0.47289568185806274
train gradient:  0.12913054663541268
iteration : 9311
train acc:  0.7734375
train loss:  0.47408872842788696
train gradient:  0.1310242064797893
iteration : 9312
train acc:  0.71875
train loss:  0.5122905969619751
train gradient:  0.19272179439351242
iteration : 9313
train acc:  0.734375
train loss:  0.4956119954586029
train gradient:  0.15647549295148197
iteration : 9314
train acc:  0.7421875
train loss:  0.5161129236221313
train gradient:  0.14193226430214012
iteration : 9315
train acc:  0.7109375
train loss:  0.539710521697998
train gradient:  0.12842211234630097
iteration : 9316
train acc:  0.78125
train loss:  0.4615216851234436
train gradient:  0.16774162782393637
iteration : 9317
train acc:  0.7421875
train loss:  0.497738778591156
train gradient:  0.11445194159333952
iteration : 9318
train acc:  0.7734375
train loss:  0.4466286897659302
train gradient:  0.12820331769624738
iteration : 9319
train acc:  0.765625
train loss:  0.5405246019363403
train gradient:  0.14813483921406018
iteration : 9320
train acc:  0.7421875
train loss:  0.5249684453010559
train gradient:  0.17341570036812864
iteration : 9321
train acc:  0.78125
train loss:  0.47080305218696594
train gradient:  0.10345701169322619
iteration : 9322
train acc:  0.765625
train loss:  0.4640124440193176
train gradient:  0.12138678920838557
iteration : 9323
train acc:  0.7109375
train loss:  0.5383955240249634
train gradient:  0.14014549253270622
iteration : 9324
train acc:  0.7109375
train loss:  0.5021570920944214
train gradient:  0.125965785633898
iteration : 9325
train acc:  0.703125
train loss:  0.5167746543884277
train gradient:  0.1422904970212292
iteration : 9326
train acc:  0.78125
train loss:  0.4587925672531128
train gradient:  0.1432892642954532
iteration : 9327
train acc:  0.7421875
train loss:  0.5105091333389282
train gradient:  0.12856053649203025
iteration : 9328
train acc:  0.71875
train loss:  0.4917788803577423
train gradient:  0.13487155336832612
iteration : 9329
train acc:  0.7734375
train loss:  0.5198802947998047
train gradient:  0.14178359899791299
iteration : 9330
train acc:  0.6953125
train loss:  0.5281640291213989
train gradient:  0.16037712328486542
iteration : 9331
train acc:  0.6875
train loss:  0.5007462501525879
train gradient:  0.15512465185753793
iteration : 9332
train acc:  0.703125
train loss:  0.5360189080238342
train gradient:  0.15885355152565606
iteration : 9333
train acc:  0.734375
train loss:  0.47091156244277954
train gradient:  0.11046008278809986
iteration : 9334
train acc:  0.734375
train loss:  0.496515154838562
train gradient:  0.11801589462274233
iteration : 9335
train acc:  0.796875
train loss:  0.4244440197944641
train gradient:  0.12472365903551315
iteration : 9336
train acc:  0.7734375
train loss:  0.47539040446281433
train gradient:  0.1250543808100446
iteration : 9337
train acc:  0.765625
train loss:  0.480971097946167
train gradient:  0.15223401797764946
iteration : 9338
train acc:  0.8046875
train loss:  0.4953896403312683
train gradient:  0.16413059005124897
iteration : 9339
train acc:  0.671875
train loss:  0.5661958456039429
train gradient:  0.1770477109968217
iteration : 9340
train acc:  0.6875
train loss:  0.5190389156341553
train gradient:  0.16756910958717264
iteration : 9341
train acc:  0.765625
train loss:  0.48978757858276367
train gradient:  0.1499958390743456
iteration : 9342
train acc:  0.7265625
train loss:  0.46606165170669556
train gradient:  0.15124213543773068
iteration : 9343
train acc:  0.7109375
train loss:  0.585191011428833
train gradient:  0.1741888066572712
iteration : 9344
train acc:  0.703125
train loss:  0.5455756187438965
train gradient:  0.19005849907077743
iteration : 9345
train acc:  0.734375
train loss:  0.5181186199188232
train gradient:  0.14861091806356697
iteration : 9346
train acc:  0.7421875
train loss:  0.5365430116653442
train gradient:  0.13782616855601126
iteration : 9347
train acc:  0.78125
train loss:  0.4972590208053589
train gradient:  0.16975058414746147
iteration : 9348
train acc:  0.6953125
train loss:  0.5633402466773987
train gradient:  0.17239925704452697
iteration : 9349
train acc:  0.6875
train loss:  0.5683917999267578
train gradient:  0.15633919053556378
iteration : 9350
train acc:  0.71875
train loss:  0.49945321679115295
train gradient:  0.11958959052200292
iteration : 9351
train acc:  0.7265625
train loss:  0.5294526815414429
train gradient:  0.17412312214740905
iteration : 9352
train acc:  0.6875
train loss:  0.5201855897903442
train gradient:  0.13166512742476802
iteration : 9353
train acc:  0.734375
train loss:  0.5522782802581787
train gradient:  0.16159436865514198
iteration : 9354
train acc:  0.734375
train loss:  0.48646605014801025
train gradient:  0.1314572427288359
iteration : 9355
train acc:  0.75
train loss:  0.488503098487854
train gradient:  0.13188300390975904
iteration : 9356
train acc:  0.75
train loss:  0.45687389373779297
train gradient:  0.12779000254229145
iteration : 9357
train acc:  0.7890625
train loss:  0.4169614315032959
train gradient:  0.1171719904284983
iteration : 9358
train acc:  0.7421875
train loss:  0.5132602453231812
train gradient:  0.13113215825196428
iteration : 9359
train acc:  0.703125
train loss:  0.5106245279312134
train gradient:  0.13341508038237604
iteration : 9360
train acc:  0.78125
train loss:  0.4505997598171234
train gradient:  0.09578338009558206
iteration : 9361
train acc:  0.734375
train loss:  0.5108162760734558
train gradient:  0.12014676065882425
iteration : 9362
train acc:  0.703125
train loss:  0.5048014521598816
train gradient:  0.1290963112617332
iteration : 9363
train acc:  0.6953125
train loss:  0.5349582433700562
train gradient:  0.13439617121472075
iteration : 9364
train acc:  0.7734375
train loss:  0.4525097608566284
train gradient:  0.1126273786755587
iteration : 9365
train acc:  0.65625
train loss:  0.5766439437866211
train gradient:  0.188640594453507
iteration : 9366
train acc:  0.6875
train loss:  0.5427613258361816
train gradient:  0.15519043785502362
iteration : 9367
train acc:  0.8515625
train loss:  0.4361730217933655
train gradient:  0.10470889524805113
iteration : 9368
train acc:  0.7109375
train loss:  0.5408562421798706
train gradient:  0.12634896546632296
iteration : 9369
train acc:  0.796875
train loss:  0.4429318606853485
train gradient:  0.10923411470319125
iteration : 9370
train acc:  0.71875
train loss:  0.4671042263507843
train gradient:  0.10707856318544845
iteration : 9371
train acc:  0.75
train loss:  0.5012066960334778
train gradient:  0.1309500848510818
iteration : 9372
train acc:  0.765625
train loss:  0.4319112002849579
train gradient:  0.12451511762699567
iteration : 9373
train acc:  0.734375
train loss:  0.5048105716705322
train gradient:  0.1437412904373574
iteration : 9374
train acc:  0.8046875
train loss:  0.4273795187473297
train gradient:  0.09063798335534075
iteration : 9375
train acc:  0.7421875
train loss:  0.5075929164886475
train gradient:  0.15874424703297454
iteration : 9376
train acc:  0.6796875
train loss:  0.5329487323760986
train gradient:  0.1549667481851859
iteration : 9377
train acc:  0.7578125
train loss:  0.48794710636138916
train gradient:  0.12185501255358007
iteration : 9378
train acc:  0.7734375
train loss:  0.4356720447540283
train gradient:  0.14570771515242475
iteration : 9379
train acc:  0.8203125
train loss:  0.47752535343170166
train gradient:  0.12872954282755536
iteration : 9380
train acc:  0.7578125
train loss:  0.4555017948150635
train gradient:  0.12751419075108567
iteration : 9381
train acc:  0.75
train loss:  0.516892671585083
train gradient:  0.11732815060163578
iteration : 9382
train acc:  0.78125
train loss:  0.4593315124511719
train gradient:  0.09513492792158683
iteration : 9383
train acc:  0.7265625
train loss:  0.49099117517471313
train gradient:  0.10141131020353548
iteration : 9384
train acc:  0.71875
train loss:  0.511395275592804
train gradient:  0.14661368959399218
iteration : 9385
train acc:  0.84375
train loss:  0.38676825165748596
train gradient:  0.09248397530707372
iteration : 9386
train acc:  0.71875
train loss:  0.5022246241569519
train gradient:  0.1362789907241479
iteration : 9387
train acc:  0.78125
train loss:  0.40853390097618103
train gradient:  0.10030805235187698
iteration : 9388
train acc:  0.7890625
train loss:  0.44196897745132446
train gradient:  0.09707104205516717
iteration : 9389
train acc:  0.7265625
train loss:  0.504777193069458
train gradient:  0.11632982500044436
iteration : 9390
train acc:  0.796875
train loss:  0.42331182956695557
train gradient:  0.09740942280059707
iteration : 9391
train acc:  0.78125
train loss:  0.48335301876068115
train gradient:  0.11364032288278095
iteration : 9392
train acc:  0.7265625
train loss:  0.531723141670227
train gradient:  0.1520565794548901
iteration : 9393
train acc:  0.796875
train loss:  0.46804243326187134
train gradient:  0.15032597247619622
iteration : 9394
train acc:  0.75
train loss:  0.4611886739730835
train gradient:  0.11793651280093544
iteration : 9395
train acc:  0.6953125
train loss:  0.5202627182006836
train gradient:  0.17659478663214645
iteration : 9396
train acc:  0.765625
train loss:  0.48537370562553406
train gradient:  0.12881643742914053
iteration : 9397
train acc:  0.78125
train loss:  0.5199508666992188
train gradient:  0.1573641767588874
iteration : 9398
train acc:  0.7421875
train loss:  0.4943413734436035
train gradient:  0.11063578510551252
iteration : 9399
train acc:  0.6875
train loss:  0.555104672908783
train gradient:  0.16716786804079137
iteration : 9400
train acc:  0.765625
train loss:  0.42539849877357483
train gradient:  0.10074646531557042
iteration : 9401
train acc:  0.6953125
train loss:  0.5067043304443359
train gradient:  0.1269860904298031
iteration : 9402
train acc:  0.6953125
train loss:  0.6071509122848511
train gradient:  0.1819754402150779
iteration : 9403
train acc:  0.6953125
train loss:  0.5093871355056763
train gradient:  0.1383500513823957
iteration : 9404
train acc:  0.75
train loss:  0.5179957747459412
train gradient:  0.15553187005091074
iteration : 9405
train acc:  0.734375
train loss:  0.562411904335022
train gradient:  0.15043579596757747
iteration : 9406
train acc:  0.734375
train loss:  0.4872208535671234
train gradient:  0.1078920505789177
iteration : 9407
train acc:  0.6640625
train loss:  0.562640905380249
train gradient:  0.17577173178298855
iteration : 9408
train acc:  0.734375
train loss:  0.554384708404541
train gradient:  0.1373062449344547
iteration : 9409
train acc:  0.6953125
train loss:  0.524958610534668
train gradient:  0.11670771196610917
iteration : 9410
train acc:  0.7109375
train loss:  0.5317222476005554
train gradient:  0.18318655226567537
iteration : 9411
train acc:  0.8203125
train loss:  0.4450381398200989
train gradient:  0.09518350119307063
iteration : 9412
train acc:  0.75
train loss:  0.5130459070205688
train gradient:  0.12209414478199149
iteration : 9413
train acc:  0.75
train loss:  0.503381609916687
train gradient:  0.12555760888075598
iteration : 9414
train acc:  0.7734375
train loss:  0.48491090536117554
train gradient:  0.11325528190976794
iteration : 9415
train acc:  0.7890625
train loss:  0.49333837628364563
train gradient:  0.11973867527835437
iteration : 9416
train acc:  0.703125
train loss:  0.5158334970474243
train gradient:  0.13960041820719649
iteration : 9417
train acc:  0.7578125
train loss:  0.4858866333961487
train gradient:  0.12296065282773733
iteration : 9418
train acc:  0.6796875
train loss:  0.6023193597793579
train gradient:  0.18153493475237445
iteration : 9419
train acc:  0.7578125
train loss:  0.5064289569854736
train gradient:  0.12005307993314514
iteration : 9420
train acc:  0.765625
train loss:  0.4485523998737335
train gradient:  0.11500909081937753
iteration : 9421
train acc:  0.640625
train loss:  0.5973159670829773
train gradient:  0.16546685006364736
iteration : 9422
train acc:  0.6875
train loss:  0.5263558030128479
train gradient:  0.15331075839651115
iteration : 9423
train acc:  0.7890625
train loss:  0.4785505533218384
train gradient:  0.15497272330741146
iteration : 9424
train acc:  0.6796875
train loss:  0.5900389552116394
train gradient:  0.17013690674994214
iteration : 9425
train acc:  0.7265625
train loss:  0.5351992845535278
train gradient:  0.19031251564010143
iteration : 9426
train acc:  0.71875
train loss:  0.5083239078521729
train gradient:  0.12090378718487078
iteration : 9427
train acc:  0.8125
train loss:  0.4323047399520874
train gradient:  0.0934595583273562
iteration : 9428
train acc:  0.78125
train loss:  0.4611189365386963
train gradient:  0.10807123164385413
iteration : 9429
train acc:  0.7578125
train loss:  0.5126984119415283
train gradient:  0.15266068140634054
iteration : 9430
train acc:  0.734375
train loss:  0.5218318700790405
train gradient:  0.13083527889889535
iteration : 9431
train acc:  0.7109375
train loss:  0.46645587682724
train gradient:  0.13904863508703053
iteration : 9432
train acc:  0.8203125
train loss:  0.4134569764137268
train gradient:  0.1055160468532637
iteration : 9433
train acc:  0.7265625
train loss:  0.4932539463043213
train gradient:  0.11333570216211811
iteration : 9434
train acc:  0.6875
train loss:  0.5229349732398987
train gradient:  0.142773302521092
iteration : 9435
train acc:  0.7578125
train loss:  0.4661089777946472
train gradient:  0.14267322664376764
iteration : 9436
train acc:  0.734375
train loss:  0.5548577904701233
train gradient:  0.17991507715578176
iteration : 9437
train acc:  0.7109375
train loss:  0.5321028232574463
train gradient:  0.17941798615753846
iteration : 9438
train acc:  0.7578125
train loss:  0.4880496561527252
train gradient:  0.12201111726460956
iteration : 9439
train acc:  0.71875
train loss:  0.5165014266967773
train gradient:  0.14588873363084628
iteration : 9440
train acc:  0.6953125
train loss:  0.49609747529029846
train gradient:  0.1195175878085845
iteration : 9441
train acc:  0.71875
train loss:  0.4982190728187561
train gradient:  0.15344715129524722
iteration : 9442
train acc:  0.640625
train loss:  0.5601741671562195
train gradient:  0.13739660929984132
iteration : 9443
train acc:  0.828125
train loss:  0.4254215955734253
train gradient:  0.11494111692707469
iteration : 9444
train acc:  0.7109375
train loss:  0.546762228012085
train gradient:  0.1596136694814138
iteration : 9445
train acc:  0.734375
train loss:  0.47622188925743103
train gradient:  0.09542648613138986
iteration : 9446
train acc:  0.7890625
train loss:  0.4257344603538513
train gradient:  0.09928511038437544
iteration : 9447
train acc:  0.8046875
train loss:  0.44185876846313477
train gradient:  0.11523684633421719
iteration : 9448
train acc:  0.7734375
train loss:  0.4529603123664856
train gradient:  0.11431696361110044
iteration : 9449
train acc:  0.6953125
train loss:  0.4996475577354431
train gradient:  0.13530024934881268
iteration : 9450
train acc:  0.71875
train loss:  0.5455251336097717
train gradient:  0.16651036986113063
iteration : 9451
train acc:  0.7109375
train loss:  0.515912652015686
train gradient:  0.14074146195194126
iteration : 9452
train acc:  0.7421875
train loss:  0.5130548477172852
train gradient:  0.1198506179645383
iteration : 9453
train acc:  0.7578125
train loss:  0.46706944704055786
train gradient:  0.10120383031834507
iteration : 9454
train acc:  0.7421875
train loss:  0.4273901581764221
train gradient:  0.10692160191390583
iteration : 9455
train acc:  0.7734375
train loss:  0.468689888715744
train gradient:  0.14238940487284713
iteration : 9456
train acc:  0.765625
train loss:  0.4770525097846985
train gradient:  0.1210581563258573
iteration : 9457
train acc:  0.75
train loss:  0.4754713773727417
train gradient:  0.11764110275434873
iteration : 9458
train acc:  0.765625
train loss:  0.4094300866127014
train gradient:  0.0817248288392319
iteration : 9459
train acc:  0.796875
train loss:  0.4532625675201416
train gradient:  0.15410941276659026
iteration : 9460
train acc:  0.78125
train loss:  0.42895376682281494
train gradient:  0.11369310829425755
iteration : 9461
train acc:  0.703125
train loss:  0.5250937342643738
train gradient:  0.15362964476802538
iteration : 9462
train acc:  0.7109375
train loss:  0.5111747980117798
train gradient:  0.1652309974123109
iteration : 9463
train acc:  0.765625
train loss:  0.4689297378063202
train gradient:  0.10718293712540768
iteration : 9464
train acc:  0.734375
train loss:  0.49952834844589233
train gradient:  0.16916572463487922
iteration : 9465
train acc:  0.671875
train loss:  0.6370784044265747
train gradient:  0.17183594342323538
iteration : 9466
train acc:  0.71875
train loss:  0.5514634251594543
train gradient:  0.20350888467133277
iteration : 9467
train acc:  0.734375
train loss:  0.4932006299495697
train gradient:  0.1257332952718555
iteration : 9468
train acc:  0.7578125
train loss:  0.46893471479415894
train gradient:  0.11400970152072222
iteration : 9469
train acc:  0.7265625
train loss:  0.49921920895576477
train gradient:  0.15610119608694756
iteration : 9470
train acc:  0.78125
train loss:  0.45042678713798523
train gradient:  0.10658544297715764
iteration : 9471
train acc:  0.7734375
train loss:  0.4821390211582184
train gradient:  0.12561379243027382
iteration : 9472
train acc:  0.7890625
train loss:  0.4500230848789215
train gradient:  0.10896637161414685
iteration : 9473
train acc:  0.765625
train loss:  0.4907773435115814
train gradient:  0.14748690685121468
iteration : 9474
train acc:  0.75
train loss:  0.4913881719112396
train gradient:  0.11134000462554447
iteration : 9475
train acc:  0.7265625
train loss:  0.5043240189552307
train gradient:  0.1389505277934726
iteration : 9476
train acc:  0.71875
train loss:  0.5324311852455139
train gradient:  0.16926162343725293
iteration : 9477
train acc:  0.6953125
train loss:  0.5400397777557373
train gradient:  0.14849902773404483
iteration : 9478
train acc:  0.765625
train loss:  0.4866706132888794
train gradient:  0.13087503348836288
iteration : 9479
train acc:  0.7734375
train loss:  0.4633858799934387
train gradient:  0.13894838219457933
iteration : 9480
train acc:  0.7421875
train loss:  0.4695439338684082
train gradient:  0.11108652859387609
iteration : 9481
train acc:  0.6953125
train loss:  0.6245701909065247
train gradient:  0.21771217824095956
iteration : 9482
train acc:  0.703125
train loss:  0.5408831238746643
train gradient:  0.17226038804876026
iteration : 9483
train acc:  0.75
train loss:  0.4794568419456482
train gradient:  0.14519793111355916
iteration : 9484
train acc:  0.7890625
train loss:  0.4369163513183594
train gradient:  0.10639703407722076
iteration : 9485
train acc:  0.75
train loss:  0.4501267075538635
train gradient:  0.13743732901098415
iteration : 9486
train acc:  0.6796875
train loss:  0.5757665634155273
train gradient:  0.1875129613559693
iteration : 9487
train acc:  0.71875
train loss:  0.5400124788284302
train gradient:  0.1379064768852551
iteration : 9488
train acc:  0.734375
train loss:  0.4925304055213928
train gradient:  0.14578868081342444
iteration : 9489
train acc:  0.7265625
train loss:  0.4952429533004761
train gradient:  0.1535346584709828
iteration : 9490
train acc:  0.765625
train loss:  0.48285746574401855
train gradient:  0.1391277669835037
iteration : 9491
train acc:  0.765625
train loss:  0.49006980657577515
train gradient:  0.11725274871136765
iteration : 9492
train acc:  0.703125
train loss:  0.5240539312362671
train gradient:  0.12795466245205694
iteration : 9493
train acc:  0.6953125
train loss:  0.5386073589324951
train gradient:  0.17978558527977134
iteration : 9494
train acc:  0.75
train loss:  0.4843185544013977
train gradient:  0.10541861349594263
iteration : 9495
train acc:  0.765625
train loss:  0.4993460178375244
train gradient:  0.11879808233388564
iteration : 9496
train acc:  0.7109375
train loss:  0.5388484001159668
train gradient:  0.15019368497635907
iteration : 9497
train acc:  0.78125
train loss:  0.4387442171573639
train gradient:  0.0853496606108634
iteration : 9498
train acc:  0.7890625
train loss:  0.5036534667015076
train gradient:  0.16183606100622883
iteration : 9499
train acc:  0.78125
train loss:  0.46028628945350647
train gradient:  0.15681711832183692
iteration : 9500
train acc:  0.6953125
train loss:  0.5204198360443115
train gradient:  0.13366313318393147
iteration : 9501
train acc:  0.671875
train loss:  0.5747246742248535
train gradient:  0.17089565511097016
iteration : 9502
train acc:  0.75
train loss:  0.5198689699172974
train gradient:  0.13440483160289873
iteration : 9503
train acc:  0.7109375
train loss:  0.5425800085067749
train gradient:  0.17143642896676295
iteration : 9504
train acc:  0.7109375
train loss:  0.5603006482124329
train gradient:  0.14578256513134327
iteration : 9505
train acc:  0.7265625
train loss:  0.5309798717498779
train gradient:  0.1756094339165815
iteration : 9506
train acc:  0.7578125
train loss:  0.4994107782840729
train gradient:  0.11099022841079606
iteration : 9507
train acc:  0.765625
train loss:  0.4649350643157959
train gradient:  0.12353725395221954
iteration : 9508
train acc:  0.71875
train loss:  0.5373429656028748
train gradient:  0.1395918552310933
iteration : 9509
train acc:  0.75
train loss:  0.48723018169403076
train gradient:  0.13416713175975345
iteration : 9510
train acc:  0.75
train loss:  0.5145392417907715
train gradient:  0.10240916550246064
iteration : 9511
train acc:  0.7265625
train loss:  0.526808500289917
train gradient:  0.13271707934780075
iteration : 9512
train acc:  0.71875
train loss:  0.514205276966095
train gradient:  0.1599280490198103
iteration : 9513
train acc:  0.7421875
train loss:  0.48344212770462036
train gradient:  0.111763366449964
iteration : 9514
train acc:  0.7734375
train loss:  0.4429931044578552
train gradient:  0.09783319995627424
iteration : 9515
train acc:  0.7421875
train loss:  0.4992630183696747
train gradient:  0.16268673739856315
iteration : 9516
train acc:  0.7265625
train loss:  0.4596239924430847
train gradient:  0.13651097824799346
iteration : 9517
train acc:  0.734375
train loss:  0.47273439168930054
train gradient:  0.13915017586796408
iteration : 9518
train acc:  0.71875
train loss:  0.530936598777771
train gradient:  0.1655495366639395
iteration : 9519
train acc:  0.734375
train loss:  0.522813618183136
train gradient:  0.2648002279284499
iteration : 9520
train acc:  0.7578125
train loss:  0.4736098051071167
train gradient:  0.1181770239217074
iteration : 9521
train acc:  0.7734375
train loss:  0.4241492748260498
train gradient:  0.10466567943016125
iteration : 9522
train acc:  0.7109375
train loss:  0.5393208265304565
train gradient:  0.1320174156298544
iteration : 9523
train acc:  0.765625
train loss:  0.45304304361343384
train gradient:  0.12240302667117238
iteration : 9524
train acc:  0.734375
train loss:  0.5265166759490967
train gradient:  0.16997760162425896
iteration : 9525
train acc:  0.6953125
train loss:  0.5598196983337402
train gradient:  0.20121770296377356
iteration : 9526
train acc:  0.7265625
train loss:  0.5059192180633545
train gradient:  0.13029055574849469
iteration : 9527
train acc:  0.765625
train loss:  0.5256776809692383
train gradient:  0.16243996676166106
iteration : 9528
train acc:  0.7734375
train loss:  0.4415120482444763
train gradient:  0.11812607769086667
iteration : 9529
train acc:  0.7265625
train loss:  0.514732301235199
train gradient:  0.14489998394570908
iteration : 9530
train acc:  0.7734375
train loss:  0.44485902786254883
train gradient:  0.1282525650328511
iteration : 9531
train acc:  0.765625
train loss:  0.46004149317741394
train gradient:  0.12557256244668258
iteration : 9532
train acc:  0.7109375
train loss:  0.4942566752433777
train gradient:  0.12112175337346227
iteration : 9533
train acc:  0.671875
train loss:  0.5551707148551941
train gradient:  0.13235763442414053
iteration : 9534
train acc:  0.7421875
train loss:  0.482758104801178
train gradient:  0.1368575764849228
iteration : 9535
train acc:  0.8046875
train loss:  0.4478844404220581
train gradient:  0.1048560290356755
iteration : 9536
train acc:  0.7734375
train loss:  0.48194682598114014
train gradient:  0.13496811292396482
iteration : 9537
train acc:  0.703125
train loss:  0.49778762459754944
train gradient:  0.11087441839031008
iteration : 9538
train acc:  0.7265625
train loss:  0.5017547607421875
train gradient:  0.13608289234532867
iteration : 9539
train acc:  0.7421875
train loss:  0.47107234597206116
train gradient:  0.14589859267674127
iteration : 9540
train acc:  0.765625
train loss:  0.46597063541412354
train gradient:  0.12256642264061807
iteration : 9541
train acc:  0.765625
train loss:  0.4723834991455078
train gradient:  0.11951467913225686
iteration : 9542
train acc:  0.8203125
train loss:  0.4225178360939026
train gradient:  0.09393603435787655
iteration : 9543
train acc:  0.7265625
train loss:  0.4974558353424072
train gradient:  0.12490224950332061
iteration : 9544
train acc:  0.7265625
train loss:  0.5036764740943909
train gradient:  0.09797023032045142
iteration : 9545
train acc:  0.7734375
train loss:  0.4519440829753876
train gradient:  0.11630874076604522
iteration : 9546
train acc:  0.703125
train loss:  0.5246321558952332
train gradient:  0.1528529979363496
iteration : 9547
train acc:  0.6484375
train loss:  0.612621545791626
train gradient:  0.20196654775020545
iteration : 9548
train acc:  0.6875
train loss:  0.5416405200958252
train gradient:  0.12608627744192985
iteration : 9549
train acc:  0.828125
train loss:  0.4270527958869934
train gradient:  0.08203339553444654
iteration : 9550
train acc:  0.734375
train loss:  0.5023117065429688
train gradient:  0.13421013471989474
iteration : 9551
train acc:  0.78125
train loss:  0.47466981410980225
train gradient:  0.11296823248332404
iteration : 9552
train acc:  0.6484375
train loss:  0.5859098434448242
train gradient:  0.1554667233274107
iteration : 9553
train acc:  0.7734375
train loss:  0.5165019035339355
train gradient:  0.14704604515319916
iteration : 9554
train acc:  0.7890625
train loss:  0.4330042004585266
train gradient:  0.14102092292882237
iteration : 9555
train acc:  0.671875
train loss:  0.5930697917938232
train gradient:  0.1813363818546387
iteration : 9556
train acc:  0.6953125
train loss:  0.553559422492981
train gradient:  0.16732554739066452
iteration : 9557
train acc:  0.65625
train loss:  0.5695611834526062
train gradient:  0.17216258585395694
iteration : 9558
train acc:  0.7734375
train loss:  0.46359074115753174
train gradient:  0.09868548278451533
iteration : 9559
train acc:  0.6875
train loss:  0.5492666363716125
train gradient:  0.15225340114079242
iteration : 9560
train acc:  0.7578125
train loss:  0.48022881150245667
train gradient:  0.13467275006978846
iteration : 9561
train acc:  0.734375
train loss:  0.5095510482788086
train gradient:  0.11267565036065588
iteration : 9562
train acc:  0.7421875
train loss:  0.4721127450466156
train gradient:  0.14418467526860607
iteration : 9563
train acc:  0.78125
train loss:  0.454697847366333
train gradient:  0.14209829467076956
iteration : 9564
train acc:  0.765625
train loss:  0.4874036908149719
train gradient:  0.14283982660024808
iteration : 9565
train acc:  0.7265625
train loss:  0.5395504236221313
train gradient:  0.16715800178656048
iteration : 9566
train acc:  0.7734375
train loss:  0.49721431732177734
train gradient:  0.13218002351839503
iteration : 9567
train acc:  0.7265625
train loss:  0.519119143486023
train gradient:  0.11341911135948911
iteration : 9568
train acc:  0.7421875
train loss:  0.5194805860519409
train gradient:  0.13612779450710305
iteration : 9569
train acc:  0.71875
train loss:  0.4718417525291443
train gradient:  0.1214827268900045
iteration : 9570
train acc:  0.8125
train loss:  0.44047868251800537
train gradient:  0.10334587591213887
iteration : 9571
train acc:  0.7265625
train loss:  0.5015050768852234
train gradient:  0.16762941030782247
iteration : 9572
train acc:  0.765625
train loss:  0.4579421579837799
train gradient:  0.12349108563188001
iteration : 9573
train acc:  0.7109375
train loss:  0.554134726524353
train gradient:  0.14054139915920547
iteration : 9574
train acc:  0.703125
train loss:  0.5022894144058228
train gradient:  0.13430723674474004
iteration : 9575
train acc:  0.7109375
train loss:  0.5416188836097717
train gradient:  0.12043710780587831
iteration : 9576
train acc:  0.6953125
train loss:  0.506479799747467
train gradient:  0.13574638041966566
iteration : 9577
train acc:  0.71875
train loss:  0.5099945068359375
train gradient:  0.11741076440531391
iteration : 9578
train acc:  0.734375
train loss:  0.5429044961929321
train gradient:  0.13147996897351322
iteration : 9579
train acc:  0.765625
train loss:  0.4727632403373718
train gradient:  0.13078938659362246
iteration : 9580
train acc:  0.7890625
train loss:  0.5059454441070557
train gradient:  0.11657352677385951
iteration : 9581
train acc:  0.765625
train loss:  0.48065313696861267
train gradient:  0.1737210729835581
iteration : 9582
train acc:  0.71875
train loss:  0.48702722787857056
train gradient:  0.10716216770684577
iteration : 9583
train acc:  0.7421875
train loss:  0.4605772793292999
train gradient:  0.10675177219368746
iteration : 9584
train acc:  0.78125
train loss:  0.4564327001571655
train gradient:  0.12120895826580973
iteration : 9585
train acc:  0.7109375
train loss:  0.5278903245925903
train gradient:  0.11058154637829938
iteration : 9586
train acc:  0.65625
train loss:  0.5602850317955017
train gradient:  0.18752145943779008
iteration : 9587
train acc:  0.6640625
train loss:  0.5540809631347656
train gradient:  0.1800921561197743
iteration : 9588
train acc:  0.7578125
train loss:  0.49270138144493103
train gradient:  0.1172189889882582
iteration : 9589
train acc:  0.7734375
train loss:  0.45395636558532715
train gradient:  0.1286167132321182
iteration : 9590
train acc:  0.78125
train loss:  0.4886503219604492
train gradient:  0.12321613777139294
iteration : 9591
train acc:  0.734375
train loss:  0.4699033498764038
train gradient:  0.09225746585894487
iteration : 9592
train acc:  0.78125
train loss:  0.4405190646648407
train gradient:  0.11989235480979024
iteration : 9593
train acc:  0.765625
train loss:  0.4616517722606659
train gradient:  0.08527901978328781
iteration : 9594
train acc:  0.7109375
train loss:  0.5069659352302551
train gradient:  0.1461666873742749
iteration : 9595
train acc:  0.7734375
train loss:  0.4473501443862915
train gradient:  0.15618079427319764
iteration : 9596
train acc:  0.6953125
train loss:  0.5275682210922241
train gradient:  0.14862857050575637
iteration : 9597
train acc:  0.703125
train loss:  0.5001302361488342
train gradient:  0.12364136331141547
iteration : 9598
train acc:  0.7890625
train loss:  0.46329694986343384
train gradient:  0.10449706585622888
iteration : 9599
train acc:  0.6640625
train loss:  0.6193456649780273
train gradient:  0.17716672626552898
iteration : 9600
train acc:  0.75
train loss:  0.4889640212059021
train gradient:  0.13525611345885952
iteration : 9601
train acc:  0.765625
train loss:  0.4946255087852478
train gradient:  0.1359260670007775
iteration : 9602
train acc:  0.7734375
train loss:  0.4581001400947571
train gradient:  0.09964254399187257
iteration : 9603
train acc:  0.71875
train loss:  0.5128455758094788
train gradient:  0.13775779053064185
iteration : 9604
train acc:  0.78125
train loss:  0.4428951144218445
train gradient:  0.11447037709410307
iteration : 9605
train acc:  0.7265625
train loss:  0.5406168699264526
train gradient:  0.16228637054772993
iteration : 9606
train acc:  0.7734375
train loss:  0.44384628534317017
train gradient:  0.08759452213935562
iteration : 9607
train acc:  0.78125
train loss:  0.4464397132396698
train gradient:  0.11165911541509009
iteration : 9608
train acc:  0.703125
train loss:  0.4800525903701782
train gradient:  0.10241772474227966
iteration : 9609
train acc:  0.78125
train loss:  0.4842545688152313
train gradient:  0.10812003183165958
iteration : 9610
train acc:  0.78125
train loss:  0.4275496304035187
train gradient:  0.11413316701054536
iteration : 9611
train acc:  0.7421875
train loss:  0.4748760461807251
train gradient:  0.12542500161990572
iteration : 9612
train acc:  0.71875
train loss:  0.4934660792350769
train gradient:  0.13789677996552963
iteration : 9613
train acc:  0.703125
train loss:  0.5182692408561707
train gradient:  0.14428539462791384
iteration : 9614
train acc:  0.6953125
train loss:  0.5146986246109009
train gradient:  0.15883434086890386
iteration : 9615
train acc:  0.8125
train loss:  0.44499510526657104
train gradient:  0.10755013902351603
iteration : 9616
train acc:  0.7265625
train loss:  0.48183733224868774
train gradient:  0.10578739318311842
iteration : 9617
train acc:  0.7734375
train loss:  0.4536898732185364
train gradient:  0.12571339304410256
iteration : 9618
train acc:  0.7421875
train loss:  0.4733402132987976
train gradient:  0.0995182769193034
iteration : 9619
train acc:  0.7421875
train loss:  0.5303293466567993
train gradient:  0.1587522093053747
iteration : 9620
train acc:  0.6875
train loss:  0.5056442022323608
train gradient:  0.13456003413960482
iteration : 9621
train acc:  0.8046875
train loss:  0.3878108263015747
train gradient:  0.07798413697361763
iteration : 9622
train acc:  0.7265625
train loss:  0.5271570682525635
train gradient:  0.13448782109722496
iteration : 9623
train acc:  0.765625
train loss:  0.4672061800956726
train gradient:  0.11511482542543579
iteration : 9624
train acc:  0.75
train loss:  0.43272972106933594
train gradient:  0.08708324561528343
iteration : 9625
train acc:  0.78125
train loss:  0.4662550985813141
train gradient:  0.11706761277401073
iteration : 9626
train acc:  0.734375
train loss:  0.5098671913146973
train gradient:  0.13740296477660213
iteration : 9627
train acc:  0.796875
train loss:  0.4146146774291992
train gradient:  0.09025328704695972
iteration : 9628
train acc:  0.6796875
train loss:  0.5507407784461975
train gradient:  0.1430828262436562
iteration : 9629
train acc:  0.7265625
train loss:  0.5144484043121338
train gradient:  0.14844031688325934
iteration : 9630
train acc:  0.7109375
train loss:  0.5479676723480225
train gradient:  0.1928681613633466
iteration : 9631
train acc:  0.765625
train loss:  0.469138503074646
train gradient:  0.11117819483227392
iteration : 9632
train acc:  0.8359375
train loss:  0.41717785596847534
train gradient:  0.09770068010088329
iteration : 9633
train acc:  0.796875
train loss:  0.44938087463378906
train gradient:  0.09752673800897038
iteration : 9634
train acc:  0.7109375
train loss:  0.5672197341918945
train gradient:  0.15055003776679748
iteration : 9635
train acc:  0.765625
train loss:  0.4717749357223511
train gradient:  0.1407643490702632
iteration : 9636
train acc:  0.6875
train loss:  0.5490705966949463
train gradient:  0.13611165783298468
iteration : 9637
train acc:  0.7421875
train loss:  0.498751699924469
train gradient:  0.1153549518746738
iteration : 9638
train acc:  0.78125
train loss:  0.4459560513496399
train gradient:  0.1341769670106136
iteration : 9639
train acc:  0.6953125
train loss:  0.517213761806488
train gradient:  0.15197044316777703
iteration : 9640
train acc:  0.7578125
train loss:  0.4550250470638275
train gradient:  0.11426509054576295
iteration : 9641
train acc:  0.734375
train loss:  0.5274745225906372
train gradient:  0.1475009394924831
iteration : 9642
train acc:  0.7578125
train loss:  0.4832885265350342
train gradient:  0.13946220038132584
iteration : 9643
train acc:  0.7109375
train loss:  0.5566370487213135
train gradient:  0.19477454145260542
iteration : 9644
train acc:  0.7890625
train loss:  0.5135995149612427
train gradient:  0.13624988810371308
iteration : 9645
train acc:  0.8046875
train loss:  0.42518386244773865
train gradient:  0.11679049016193693
iteration : 9646
train acc:  0.7265625
train loss:  0.5297455191612244
train gradient:  0.1306137065477518
iteration : 9647
train acc:  0.7109375
train loss:  0.5479204058647156
train gradient:  0.13219348186633334
iteration : 9648
train acc:  0.75
train loss:  0.545520544052124
train gradient:  0.16309412988582922
iteration : 9649
train acc:  0.7265625
train loss:  0.5164631009101868
train gradient:  0.1390111782404521
iteration : 9650
train acc:  0.7421875
train loss:  0.4976194500923157
train gradient:  0.14276763858806465
iteration : 9651
train acc:  0.734375
train loss:  0.5088555812835693
train gradient:  0.15632709223546948
iteration : 9652
train acc:  0.78125
train loss:  0.4728938341140747
train gradient:  0.12035835742675084
iteration : 9653
train acc:  0.6484375
train loss:  0.5798799991607666
train gradient:  0.16771186634411006
iteration : 9654
train acc:  0.703125
train loss:  0.5523819923400879
train gradient:  0.17377183984100403
iteration : 9655
train acc:  0.7109375
train loss:  0.5332266688346863
train gradient:  0.14885466072762477
iteration : 9656
train acc:  0.828125
train loss:  0.4231248199939728
train gradient:  0.11634465698281206
iteration : 9657
train acc:  0.7578125
train loss:  0.45234811305999756
train gradient:  0.08974680694574806
iteration : 9658
train acc:  0.7265625
train loss:  0.5585874319076538
train gradient:  0.13293821866528993
iteration : 9659
train acc:  0.7421875
train loss:  0.4887363910675049
train gradient:  0.14842936350377267
iteration : 9660
train acc:  0.8125
train loss:  0.471272349357605
train gradient:  0.1072560896569892
iteration : 9661
train acc:  0.71875
train loss:  0.519963800907135
train gradient:  0.11890264083888445
iteration : 9662
train acc:  0.78125
train loss:  0.4468899965286255
train gradient:  0.09267951063997107
iteration : 9663
train acc:  0.7265625
train loss:  0.46916595101356506
train gradient:  0.12260894066500051
iteration : 9664
train acc:  0.78125
train loss:  0.47779902815818787
train gradient:  0.11455759532415027
iteration : 9665
train acc:  0.7890625
train loss:  0.46349936723709106
train gradient:  0.14666954004114413
iteration : 9666
train acc:  0.8671875
train loss:  0.37426257133483887
train gradient:  0.08596618827497884
iteration : 9667
train acc:  0.7578125
train loss:  0.491461843252182
train gradient:  0.13081829751176538
iteration : 9668
train acc:  0.7734375
train loss:  0.47814369201660156
train gradient:  0.10839151446158662
iteration : 9669
train acc:  0.6640625
train loss:  0.5474237203598022
train gradient:  0.16850108224536064
iteration : 9670
train acc:  0.7265625
train loss:  0.4895903766155243
train gradient:  0.13774024984240893
iteration : 9671
train acc:  0.734375
train loss:  0.5035248398780823
train gradient:  0.12860945368714613
iteration : 9672
train acc:  0.7578125
train loss:  0.5116277933120728
train gradient:  0.19964099618898543
iteration : 9673
train acc:  0.6484375
train loss:  0.5876184701919556
train gradient:  0.1634118525682759
iteration : 9674
train acc:  0.7734375
train loss:  0.47917428612709045
train gradient:  0.14754675239620474
iteration : 9675
train acc:  0.7265625
train loss:  0.5470937490463257
train gradient:  0.1462622329011477
iteration : 9676
train acc:  0.75
train loss:  0.5271344184875488
train gradient:  0.1671907087835957
iteration : 9677
train acc:  0.578125
train loss:  0.606866180896759
train gradient:  0.20136585167878493
iteration : 9678
train acc:  0.71875
train loss:  0.49005645513534546
train gradient:  0.12010091393739783
iteration : 9679
train acc:  0.7265625
train loss:  0.46955394744873047
train gradient:  0.14779741815867214
iteration : 9680
train acc:  0.703125
train loss:  0.5321718454360962
train gradient:  0.15449756002774057
iteration : 9681
train acc:  0.7734375
train loss:  0.5408310890197754
train gradient:  0.15845082963391122
iteration : 9682
train acc:  0.8046875
train loss:  0.43144455552101135
train gradient:  0.10198540436375322
iteration : 9683
train acc:  0.6953125
train loss:  0.5579653978347778
train gradient:  0.17492453412066722
iteration : 9684
train acc:  0.734375
train loss:  0.5171464681625366
train gradient:  0.13252194931870176
iteration : 9685
train acc:  0.7578125
train loss:  0.45022085309028625
train gradient:  0.09229128216160827
iteration : 9686
train acc:  0.7734375
train loss:  0.44915443658828735
train gradient:  0.12434704830114371
iteration : 9687
train acc:  0.671875
train loss:  0.5059643387794495
train gradient:  0.12325468157961691
iteration : 9688
train acc:  0.7578125
train loss:  0.532545268535614
train gradient:  0.1267909498222481
iteration : 9689
train acc:  0.7421875
train loss:  0.47112539410591125
train gradient:  0.11768494907248464
iteration : 9690
train acc:  0.7265625
train loss:  0.4940221309661865
train gradient:  0.1456996244217455
iteration : 9691
train acc:  0.6484375
train loss:  0.6841367483139038
train gradient:  0.23055471899812874
iteration : 9692
train acc:  0.796875
train loss:  0.44015729427337646
train gradient:  0.12392360999355505
iteration : 9693
train acc:  0.7421875
train loss:  0.4445164203643799
train gradient:  0.1105971824257374
iteration : 9694
train acc:  0.734375
train loss:  0.5009077787399292
train gradient:  0.11368005652321046
iteration : 9695
train acc:  0.765625
train loss:  0.4656815528869629
train gradient:  0.1573296132495115
iteration : 9696
train acc:  0.7734375
train loss:  0.4313957691192627
train gradient:  0.10932231269108544
iteration : 9697
train acc:  0.75
train loss:  0.46374571323394775
train gradient:  0.13076978358362576
iteration : 9698
train acc:  0.78125
train loss:  0.4461817741394043
train gradient:  0.10369885023663758
iteration : 9699
train acc:  0.765625
train loss:  0.4945647120475769
train gradient:  0.13931516565355845
iteration : 9700
train acc:  0.75
train loss:  0.4776129722595215
train gradient:  0.12791353536641636
iteration : 9701
train acc:  0.671875
train loss:  0.5747281312942505
train gradient:  0.19201476331348222
iteration : 9702
train acc:  0.671875
train loss:  0.5357769727706909
train gradient:  0.14758511131999652
iteration : 9703
train acc:  0.8125
train loss:  0.42417070269584656
train gradient:  0.12664354035663267
iteration : 9704
train acc:  0.75
train loss:  0.5258222818374634
train gradient:  0.15887542427441265
iteration : 9705
train acc:  0.78125
train loss:  0.47108209133148193
train gradient:  0.1300667922467239
iteration : 9706
train acc:  0.75
train loss:  0.5024215579032898
train gradient:  0.14636090212397224
iteration : 9707
train acc:  0.796875
train loss:  0.4594656229019165
train gradient:  0.11793801201579537
iteration : 9708
train acc:  0.8046875
train loss:  0.4064950942993164
train gradient:  0.10275779642684454
iteration : 9709
train acc:  0.7421875
train loss:  0.49260836839675903
train gradient:  0.11877937414956283
iteration : 9710
train acc:  0.703125
train loss:  0.5041143298149109
train gradient:  0.1264059317671774
iteration : 9711
train acc:  0.6796875
train loss:  0.5594456195831299
train gradient:  0.14345592573814134
iteration : 9712
train acc:  0.78125
train loss:  0.4343230724334717
train gradient:  0.10718340427228487
iteration : 9713
train acc:  0.7109375
train loss:  0.5321058034896851
train gradient:  0.13815350284937344
iteration : 9714
train acc:  0.7109375
train loss:  0.4901195168495178
train gradient:  0.117160316819229
iteration : 9715
train acc:  0.75
train loss:  0.46327751874923706
train gradient:  0.10187470389839873
iteration : 9716
train acc:  0.703125
train loss:  0.5524643659591675
train gradient:  0.19789242059171658
iteration : 9717
train acc:  0.75
train loss:  0.5010577440261841
train gradient:  0.140510773703999
iteration : 9718
train acc:  0.7109375
train loss:  0.4924743175506592
train gradient:  0.1444228519898943
iteration : 9719
train acc:  0.7109375
train loss:  0.48922333121299744
train gradient:  0.14586219095859165
iteration : 9720
train acc:  0.6484375
train loss:  0.6171637773513794
train gradient:  0.1733784285424148
iteration : 9721
train acc:  0.6953125
train loss:  0.559807300567627
train gradient:  0.13415607810710511
iteration : 9722
train acc:  0.75
train loss:  0.4817196726799011
train gradient:  0.1220939687146906
iteration : 9723
train acc:  0.765625
train loss:  0.4459405541419983
train gradient:  0.10846401424174046
iteration : 9724
train acc:  0.7578125
train loss:  0.4961774945259094
train gradient:  0.11909579629064138
iteration : 9725
train acc:  0.7265625
train loss:  0.514246940612793
train gradient:  0.16848650506577928
iteration : 9726
train acc:  0.7109375
train loss:  0.5557936429977417
train gradient:  0.1647796677068269
iteration : 9727
train acc:  0.6953125
train loss:  0.5502285957336426
train gradient:  0.13726390456771012
iteration : 9728
train acc:  0.7109375
train loss:  0.5544211864471436
train gradient:  0.12275751404320283
iteration : 9729
train acc:  0.6953125
train loss:  0.5218360424041748
train gradient:  0.16030570830512791
iteration : 9730
train acc:  0.7109375
train loss:  0.5175919532775879
train gradient:  0.1453022547508982
iteration : 9731
train acc:  0.7421875
train loss:  0.5122383832931519
train gradient:  0.1501204507198179
iteration : 9732
train acc:  0.734375
train loss:  0.48694100975990295
train gradient:  0.13367782273144607
iteration : 9733
train acc:  0.6796875
train loss:  0.5683866143226624
train gradient:  0.15570837093226722
iteration : 9734
train acc:  0.7578125
train loss:  0.46588265895843506
train gradient:  0.14619681335416904
iteration : 9735
train acc:  0.7421875
train loss:  0.5194353461265564
train gradient:  0.1844438963116441
iteration : 9736
train acc:  0.8125
train loss:  0.4268735647201538
train gradient:  0.10126104005904063
iteration : 9737
train acc:  0.7734375
train loss:  0.4727298617362976
train gradient:  0.1049996418238088
iteration : 9738
train acc:  0.796875
train loss:  0.47519421577453613
train gradient:  0.10644155120865342
iteration : 9739
train acc:  0.6875
train loss:  0.5215682983398438
train gradient:  0.25556341412678457
iteration : 9740
train acc:  0.7265625
train loss:  0.5405752062797546
train gradient:  0.13358485589021518
iteration : 9741
train acc:  0.78125
train loss:  0.455816388130188
train gradient:  0.12779381606894835
iteration : 9742
train acc:  0.7421875
train loss:  0.49537476897239685
train gradient:  0.12830434442821348
iteration : 9743
train acc:  0.75
train loss:  0.459320992231369
train gradient:  0.1027876852029432
iteration : 9744
train acc:  0.7421875
train loss:  0.4996088743209839
train gradient:  0.12943248037960894
iteration : 9745
train acc:  0.7109375
train loss:  0.49082502722740173
train gradient:  0.12870896250416164
iteration : 9746
train acc:  0.734375
train loss:  0.494895339012146
train gradient:  0.11510271001259693
iteration : 9747
train acc:  0.6796875
train loss:  0.5812511444091797
train gradient:  0.1756206627210285
iteration : 9748
train acc:  0.7265625
train loss:  0.48364338278770447
train gradient:  0.12003379103924997
iteration : 9749
train acc:  0.75
train loss:  0.5303522348403931
train gradient:  0.16102190717108922
iteration : 9750
train acc:  0.7109375
train loss:  0.5495685935020447
train gradient:  0.1819715513159182
iteration : 9751
train acc:  0.7421875
train loss:  0.4758182764053345
train gradient:  0.12319910154540907
iteration : 9752
train acc:  0.7734375
train loss:  0.5067954063415527
train gradient:  0.13210320175379578
iteration : 9753
train acc:  0.6875
train loss:  0.507713794708252
train gradient:  0.16626072167458306
iteration : 9754
train acc:  0.75
train loss:  0.4636874198913574
train gradient:  0.11000882661084734
iteration : 9755
train acc:  0.7578125
train loss:  0.4924679696559906
train gradient:  0.14446098135600338
iteration : 9756
train acc:  0.6953125
train loss:  0.5110979080200195
train gradient:  0.12011343362395618
iteration : 9757
train acc:  0.75
train loss:  0.5365986824035645
train gradient:  0.16518066493109537
iteration : 9758
train acc:  0.71875
train loss:  0.5086920261383057
train gradient:  0.14044262044519373
iteration : 9759
train acc:  0.6796875
train loss:  0.5400756001472473
train gradient:  0.16570314577583342
iteration : 9760
train acc:  0.7265625
train loss:  0.4521671533584595
train gradient:  0.0944440475405138
iteration : 9761
train acc:  0.7109375
train loss:  0.4902103841304779
train gradient:  0.12364869199052621
iteration : 9762
train acc:  0.7421875
train loss:  0.4654088616371155
train gradient:  0.11322941397206418
iteration : 9763
train acc:  0.7421875
train loss:  0.48338937759399414
train gradient:  0.10286252757385424
iteration : 9764
train acc:  0.7578125
train loss:  0.455143541097641
train gradient:  0.12357662698739667
iteration : 9765
train acc:  0.765625
train loss:  0.4615466892719269
train gradient:  0.10055631296931583
iteration : 9766
train acc:  0.75
train loss:  0.45954620838165283
train gradient:  0.11479831455348924
iteration : 9767
train acc:  0.7109375
train loss:  0.5149027109146118
train gradient:  0.1271757076612824
iteration : 9768
train acc:  0.765625
train loss:  0.4991194009780884
train gradient:  0.10199630636839686
iteration : 9769
train acc:  0.7109375
train loss:  0.545423686504364
train gradient:  0.13121300538026193
iteration : 9770
train acc:  0.671875
train loss:  0.5368073582649231
train gradient:  0.15220956531065433
iteration : 9771
train acc:  0.71875
train loss:  0.5163022875785828
train gradient:  0.16681919956431218
iteration : 9772
train acc:  0.734375
train loss:  0.4679687023162842
train gradient:  0.10396434211477514
iteration : 9773
train acc:  0.7578125
train loss:  0.49238795042037964
train gradient:  0.19083672138803315
iteration : 9774
train acc:  0.78125
train loss:  0.4571550190448761
train gradient:  0.1121910898827179
iteration : 9775
train acc:  0.8203125
train loss:  0.41189706325531006
train gradient:  0.10076112711170397
iteration : 9776
train acc:  0.703125
train loss:  0.5900719165802002
train gradient:  0.18473115282989516
iteration : 9777
train acc:  0.75
train loss:  0.46613287925720215
train gradient:  0.12616354000141675
iteration : 9778
train acc:  0.75
train loss:  0.4683719277381897
train gradient:  0.10579472882366159
iteration : 9779
train acc:  0.734375
train loss:  0.466921865940094
train gradient:  0.11536629632465634
iteration : 9780
train acc:  0.71875
train loss:  0.5360653400421143
train gradient:  0.15396775534544194
iteration : 9781
train acc:  0.703125
train loss:  0.4805454909801483
train gradient:  0.11406068028116052
iteration : 9782
train acc:  0.7421875
train loss:  0.5066770315170288
train gradient:  0.16107444929905357
iteration : 9783
train acc:  0.7109375
train loss:  0.4994066059589386
train gradient:  0.1657841685394446
iteration : 9784
train acc:  0.75
train loss:  0.4435674250125885
train gradient:  0.10647515566594386
iteration : 9785
train acc:  0.6796875
train loss:  0.47010523080825806
train gradient:  0.11177783691959482
iteration : 9786
train acc:  0.8203125
train loss:  0.43258458375930786
train gradient:  0.10052075489701492
iteration : 9787
train acc:  0.7578125
train loss:  0.4762282967567444
train gradient:  0.12207914186451362
iteration : 9788
train acc:  0.734375
train loss:  0.502675473690033
train gradient:  0.11782039665411524
iteration : 9789
train acc:  0.796875
train loss:  0.45743176341056824
train gradient:  0.14409237357368287
iteration : 9790
train acc:  0.6484375
train loss:  0.5831152200698853
train gradient:  0.1735895525455988
iteration : 9791
train acc:  0.703125
train loss:  0.5990093350410461
train gradient:  0.1464522231072852
iteration : 9792
train acc:  0.7421875
train loss:  0.46639275550842285
train gradient:  0.10674477479200137
iteration : 9793
train acc:  0.7421875
train loss:  0.4704533517360687
train gradient:  0.10792111045034142
iteration : 9794
train acc:  0.734375
train loss:  0.5483468174934387
train gradient:  0.12503680242796644
iteration : 9795
train acc:  0.75
train loss:  0.4710475206375122
train gradient:  0.12426458442717551
iteration : 9796
train acc:  0.7890625
train loss:  0.4640486240386963
train gradient:  0.1116027609857914
iteration : 9797
train acc:  0.6640625
train loss:  0.5895209908485413
train gradient:  0.19160840253142447
iteration : 9798
train acc:  0.7734375
train loss:  0.4610874652862549
train gradient:  0.11242369513103075
iteration : 9799
train acc:  0.796875
train loss:  0.4564172625541687
train gradient:  0.11018092924056716
iteration : 9800
train acc:  0.6328125
train loss:  0.5663818120956421
train gradient:  0.16539002385054452
iteration : 9801
train acc:  0.78125
train loss:  0.4194943904876709
train gradient:  0.12737075723431818
iteration : 9802
train acc:  0.75
train loss:  0.4441371560096741
train gradient:  0.10839300502565467
iteration : 9803
train acc:  0.75
train loss:  0.457960307598114
train gradient:  0.1154267367628995
iteration : 9804
train acc:  0.7421875
train loss:  0.472129762172699
train gradient:  0.12545674034750004
iteration : 9805
train acc:  0.71875
train loss:  0.5238165855407715
train gradient:  0.15172371445850122
iteration : 9806
train acc:  0.78125
train loss:  0.45181870460510254
train gradient:  0.1417393737541736
iteration : 9807
train acc:  0.71875
train loss:  0.5395814776420593
train gradient:  0.1390824179176369
iteration : 9808
train acc:  0.78125
train loss:  0.4972400665283203
train gradient:  0.11350462526177814
iteration : 9809
train acc:  0.75
train loss:  0.47483742237091064
train gradient:  0.11941392365870925
iteration : 9810
train acc:  0.671875
train loss:  0.5410795211791992
train gradient:  0.1909660100392761
iteration : 9811
train acc:  0.75
train loss:  0.45073187351226807
train gradient:  0.10824806708267327
iteration : 9812
train acc:  0.703125
train loss:  0.5240168571472168
train gradient:  0.10973014709355604
iteration : 9813
train acc:  0.625
train loss:  0.6575851440429688
train gradient:  0.2312147438471846
iteration : 9814
train acc:  0.6953125
train loss:  0.5231418609619141
train gradient:  0.16757253873679706
iteration : 9815
train acc:  0.6953125
train loss:  0.5419323444366455
train gradient:  0.18270942782948152
iteration : 9816
train acc:  0.78125
train loss:  0.44067466259002686
train gradient:  0.12334008495361962
iteration : 9817
train acc:  0.6875
train loss:  0.5949511528015137
train gradient:  0.1512107346854516
iteration : 9818
train acc:  0.8125
train loss:  0.4366455674171448
train gradient:  0.10136131810389992
iteration : 9819
train acc:  0.734375
train loss:  0.5609641075134277
train gradient:  0.17020323349911304
iteration : 9820
train acc:  0.7578125
train loss:  0.5007226467132568
train gradient:  0.13187642973154295
iteration : 9821
train acc:  0.7421875
train loss:  0.46471840143203735
train gradient:  0.1115828429373893
iteration : 9822
train acc:  0.765625
train loss:  0.4651030898094177
train gradient:  0.11871594830469884
iteration : 9823
train acc:  0.6640625
train loss:  0.5836743116378784
train gradient:  0.17528083819756035
iteration : 9824
train acc:  0.78125
train loss:  0.4511546492576599
train gradient:  0.12014498467626784
iteration : 9825
train acc:  0.78125
train loss:  0.4731389880180359
train gradient:  0.10302174236514242
iteration : 9826
train acc:  0.765625
train loss:  0.42006346583366394
train gradient:  0.0903424018858057
iteration : 9827
train acc:  0.7109375
train loss:  0.5008081197738647
train gradient:  0.16073337249607683
iteration : 9828
train acc:  0.8203125
train loss:  0.43928295373916626
train gradient:  0.09739411707176561
iteration : 9829
train acc:  0.7109375
train loss:  0.5199742317199707
train gradient:  0.1307453215979414
iteration : 9830
train acc:  0.703125
train loss:  0.5618060231208801
train gradient:  0.1737669355247971
iteration : 9831
train acc:  0.7421875
train loss:  0.4772990047931671
train gradient:  0.09607031883918446
iteration : 9832
train acc:  0.703125
train loss:  0.5429468154907227
train gradient:  0.15412232232164452
iteration : 9833
train acc:  0.7578125
train loss:  0.49437084794044495
train gradient:  0.1333294641393381
iteration : 9834
train acc:  0.734375
train loss:  0.4884989261627197
train gradient:  0.1409258701072571
iteration : 9835
train acc:  0.8046875
train loss:  0.46408259868621826
train gradient:  0.11398735007493292
iteration : 9836
train acc:  0.71875
train loss:  0.5348091125488281
train gradient:  0.15002877417439833
iteration : 9837
train acc:  0.7890625
train loss:  0.42934444546699524
train gradient:  0.12985656315579663
iteration : 9838
train acc:  0.78125
train loss:  0.48725107312202454
train gradient:  0.10192830126999027
iteration : 9839
train acc:  0.703125
train loss:  0.5209792256355286
train gradient:  0.16296479286633409
iteration : 9840
train acc:  0.78125
train loss:  0.5062564015388489
train gradient:  0.13996657169919186
iteration : 9841
train acc:  0.75
train loss:  0.5261164903640747
train gradient:  0.14389760236621557
iteration : 9842
train acc:  0.734375
train loss:  0.4850519001483917
train gradient:  0.10561893010147015
iteration : 9843
train acc:  0.765625
train loss:  0.45772242546081543
train gradient:  0.14510631043492525
iteration : 9844
train acc:  0.765625
train loss:  0.5015506744384766
train gradient:  0.15119426809811215
iteration : 9845
train acc:  0.6953125
train loss:  0.5655704736709595
train gradient:  0.16742536664299223
iteration : 9846
train acc:  0.765625
train loss:  0.4872801899909973
train gradient:  0.14083243324626327
iteration : 9847
train acc:  0.71875
train loss:  0.5250688195228577
train gradient:  0.14644012579057591
iteration : 9848
train acc:  0.7421875
train loss:  0.48575296998023987
train gradient:  0.15091486399147774
iteration : 9849
train acc:  0.78125
train loss:  0.4323326349258423
train gradient:  0.12036651055725518
iteration : 9850
train acc:  0.8125
train loss:  0.4388890564441681
train gradient:  0.11133735390198761
iteration : 9851
train acc:  0.8046875
train loss:  0.4442823529243469
train gradient:  0.13004387088356395
iteration : 9852
train acc:  0.75
train loss:  0.49482467770576477
train gradient:  0.12287900919633242
iteration : 9853
train acc:  0.7421875
train loss:  0.4961872696876526
train gradient:  0.1304208311394225
iteration : 9854
train acc:  0.796875
train loss:  0.42666369676589966
train gradient:  0.11154040691031605
iteration : 9855
train acc:  0.6875
train loss:  0.5756778717041016
train gradient:  0.18592268681923976
iteration : 9856
train acc:  0.765625
train loss:  0.49592557549476624
train gradient:  0.12165038580208302
iteration : 9857
train acc:  0.765625
train loss:  0.4437580108642578
train gradient:  0.09624186314092503
iteration : 9858
train acc:  0.71875
train loss:  0.49792975187301636
train gradient:  0.1622934104239874
iteration : 9859
train acc:  0.7578125
train loss:  0.530377984046936
train gradient:  0.15103273951887072
iteration : 9860
train acc:  0.7734375
train loss:  0.4518675208091736
train gradient:  0.12938538379974723
iteration : 9861
train acc:  0.7734375
train loss:  0.44286447763442993
train gradient:  0.1388368810513828
iteration : 9862
train acc:  0.828125
train loss:  0.4331969916820526
train gradient:  0.0944543117691637
iteration : 9863
train acc:  0.7890625
train loss:  0.47147613763809204
train gradient:  0.10914687053478639
iteration : 9864
train acc:  0.71875
train loss:  0.5704493522644043
train gradient:  0.19004813834392237
iteration : 9865
train acc:  0.71875
train loss:  0.5642960667610168
train gradient:  0.1987209691207402
iteration : 9866
train acc:  0.703125
train loss:  0.4854344129562378
train gradient:  0.13445550735229758
iteration : 9867
train acc:  0.7578125
train loss:  0.504043698310852
train gradient:  0.13386895029708729
iteration : 9868
train acc:  0.703125
train loss:  0.501705527305603
train gradient:  0.11755645635792787
iteration : 9869
train acc:  0.765625
train loss:  0.45853981375694275
train gradient:  0.11654824965489728
iteration : 9870
train acc:  0.7890625
train loss:  0.4742315113544464
train gradient:  0.1574971603760036
iteration : 9871
train acc:  0.6640625
train loss:  0.5818353891372681
train gradient:  0.17029797790088128
iteration : 9872
train acc:  0.7421875
train loss:  0.5187245607376099
train gradient:  0.1397055932076008
iteration : 9873
train acc:  0.8046875
train loss:  0.4365697503089905
train gradient:  0.11479942757953976
iteration : 9874
train acc:  0.703125
train loss:  0.5410001873970032
train gradient:  0.16121464031078214
iteration : 9875
train acc:  0.78125
train loss:  0.5025042295455933
train gradient:  0.11522990394704039
iteration : 9876
train acc:  0.7265625
train loss:  0.523532509803772
train gradient:  0.17879470848647927
iteration : 9877
train acc:  0.7734375
train loss:  0.46112874150276184
train gradient:  0.1367547388844758
iteration : 9878
train acc:  0.7265625
train loss:  0.5282568335533142
train gradient:  0.14514390698939644
iteration : 9879
train acc:  0.7421875
train loss:  0.5422488451004028
train gradient:  0.1512038973592027
iteration : 9880
train acc:  0.8203125
train loss:  0.4066945016384125
train gradient:  0.09758984327870267
iteration : 9881
train acc:  0.7734375
train loss:  0.46042656898498535
train gradient:  0.14565062889294
iteration : 9882
train acc:  0.71875
train loss:  0.4916270673274994
train gradient:  0.1245086096909208
iteration : 9883
train acc:  0.703125
train loss:  0.5493482351303101
train gradient:  0.1883249915894662
iteration : 9884
train acc:  0.7265625
train loss:  0.5004595518112183
train gradient:  0.17814993579988547
iteration : 9885
train acc:  0.78125
train loss:  0.4673963785171509
train gradient:  0.12266093646281263
iteration : 9886
train acc:  0.765625
train loss:  0.4900515675544739
train gradient:  0.12056686716766578
iteration : 9887
train acc:  0.7578125
train loss:  0.47169029712677
train gradient:  0.12273493402261566
iteration : 9888
train acc:  0.7890625
train loss:  0.440199077129364
train gradient:  0.11166239746986412
iteration : 9889
train acc:  0.75
train loss:  0.45446109771728516
train gradient:  0.11858148848257836
iteration : 9890
train acc:  0.78125
train loss:  0.5068461894989014
train gradient:  0.13318179030242006
iteration : 9891
train acc:  0.7265625
train loss:  0.5321990251541138
train gradient:  0.1509772302725561
iteration : 9892
train acc:  0.7578125
train loss:  0.5202026963233948
train gradient:  0.13543118816195274
iteration : 9893
train acc:  0.7734375
train loss:  0.5560792088508606
train gradient:  0.1395597486826069
iteration : 9894
train acc:  0.8203125
train loss:  0.3922218680381775
train gradient:  0.09099536295942946
iteration : 9895
train acc:  0.703125
train loss:  0.4707026183605194
train gradient:  0.1357376529652288
iteration : 9896
train acc:  0.7421875
train loss:  0.47741347551345825
train gradient:  0.11931695982641045
iteration : 9897
train acc:  0.75
train loss:  0.49161869287490845
train gradient:  0.10373262385967326
iteration : 9898
train acc:  0.734375
train loss:  0.5099719762802124
train gradient:  0.11172875729921869
iteration : 9899
train acc:  0.7421875
train loss:  0.43558305501937866
train gradient:  0.13488789779186736
iteration : 9900
train acc:  0.796875
train loss:  0.415591299533844
train gradient:  0.09254213913456472
iteration : 9901
train acc:  0.7265625
train loss:  0.5057036280632019
train gradient:  0.1413458320575785
iteration : 9902
train acc:  0.78125
train loss:  0.473339319229126
train gradient:  0.12334194098582574
iteration : 9903
train acc:  0.65625
train loss:  0.6039366126060486
train gradient:  0.19480163209828005
iteration : 9904
train acc:  0.7734375
train loss:  0.45723089575767517
train gradient:  0.10444299980542102
iteration : 9905
train acc:  0.7578125
train loss:  0.5238481760025024
train gradient:  0.15411914889825773
iteration : 9906
train acc:  0.75
train loss:  0.47553446888923645
train gradient:  0.13532393176847585
iteration : 9907
train acc:  0.7109375
train loss:  0.5486247539520264
train gradient:  0.16049980131287808
iteration : 9908
train acc:  0.7578125
train loss:  0.4737547039985657
train gradient:  0.1110816317459502
iteration : 9909
train acc:  0.703125
train loss:  0.531420111656189
train gradient:  0.13793331138748505
iteration : 9910
train acc:  0.7421875
train loss:  0.4766891598701477
train gradient:  0.08651493491545116
iteration : 9911
train acc:  0.7578125
train loss:  0.5333819389343262
train gradient:  0.1326731500200846
iteration : 9912
train acc:  0.703125
train loss:  0.580484926700592
train gradient:  0.2359556990787533
iteration : 9913
train acc:  0.71875
train loss:  0.49962878227233887
train gradient:  0.10335987434712653
iteration : 9914
train acc:  0.75
train loss:  0.526594877243042
train gradient:  0.14193865896899016
iteration : 9915
train acc:  0.71875
train loss:  0.5247006416320801
train gradient:  0.15517296914299666
iteration : 9916
train acc:  0.7265625
train loss:  0.5239529609680176
train gradient:  0.16993073400754544
iteration : 9917
train acc:  0.7265625
train loss:  0.5236482620239258
train gradient:  0.15526137409858132
iteration : 9918
train acc:  0.7734375
train loss:  0.46631044149398804
train gradient:  0.11600925541005194
iteration : 9919
train acc:  0.7265625
train loss:  0.49782904982566833
train gradient:  0.10768908476965867
iteration : 9920
train acc:  0.78125
train loss:  0.4587004780769348
train gradient:  0.10080660116066079
iteration : 9921
train acc:  0.7734375
train loss:  0.4814329147338867
train gradient:  0.12904570396036144
iteration : 9922
train acc:  0.7421875
train loss:  0.5114433765411377
train gradient:  0.1555248354591896
iteration : 9923
train acc:  0.65625
train loss:  0.554013192653656
train gradient:  0.1727484216613655
iteration : 9924
train acc:  0.734375
train loss:  0.5089313387870789
train gradient:  0.14344087420282026
iteration : 9925
train acc:  0.703125
train loss:  0.5124568343162537
train gradient:  0.1351054125972658
iteration : 9926
train acc:  0.765625
train loss:  0.45098626613616943
train gradient:  0.12045398523036417
iteration : 9927
train acc:  0.7578125
train loss:  0.4761107563972473
train gradient:  0.10495890589922713
iteration : 9928
train acc:  0.734375
train loss:  0.46984589099884033
train gradient:  0.1249350933520552
iteration : 9929
train acc:  0.7265625
train loss:  0.49217095971107483
train gradient:  0.1540515999101952
iteration : 9930
train acc:  0.7890625
train loss:  0.4824700951576233
train gradient:  0.13481250587425575
iteration : 9931
train acc:  0.75
train loss:  0.47809457778930664
train gradient:  0.12558855736355634
iteration : 9932
train acc:  0.703125
train loss:  0.5143542289733887
train gradient:  0.1294415950548951
iteration : 9933
train acc:  0.8359375
train loss:  0.4471327066421509
train gradient:  0.10517148496143496
iteration : 9934
train acc:  0.6796875
train loss:  0.5282691717147827
train gradient:  0.16377687925443873
iteration : 9935
train acc:  0.7890625
train loss:  0.4854828119277954
train gradient:  0.11464909220607984
iteration : 9936
train acc:  0.6875
train loss:  0.5321638584136963
train gradient:  0.15403009709081877
iteration : 9937
train acc:  0.734375
train loss:  0.5026941895484924
train gradient:  0.12538047988165657
iteration : 9938
train acc:  0.7421875
train loss:  0.5400772094726562
train gradient:  0.1602859970659778
iteration : 9939
train acc:  0.7578125
train loss:  0.43409109115600586
train gradient:  0.11649906969921711
iteration : 9940
train acc:  0.7421875
train loss:  0.5377720594406128
train gradient:  0.17994351972814884
iteration : 9941
train acc:  0.7734375
train loss:  0.47153809666633606
train gradient:  0.11244423407625094
iteration : 9942
train acc:  0.734375
train loss:  0.465746134519577
train gradient:  0.1088286134301598
iteration : 9943
train acc:  0.7734375
train loss:  0.44453203678131104
train gradient:  0.12930715608989796
iteration : 9944
train acc:  0.6796875
train loss:  0.5307436585426331
train gradient:  0.1554279217299175
iteration : 9945
train acc:  0.7890625
train loss:  0.47653210163116455
train gradient:  0.14160194502070242
iteration : 9946
train acc:  0.7578125
train loss:  0.4486323893070221
train gradient:  0.11955618879475237
iteration : 9947
train acc:  0.703125
train loss:  0.5217955708503723
train gradient:  0.12957607745186805
iteration : 9948
train acc:  0.75
train loss:  0.48437196016311646
train gradient:  0.16291838871868553
iteration : 9949
train acc:  0.7734375
train loss:  0.5038185119628906
train gradient:  0.15654516590877307
iteration : 9950
train acc:  0.7578125
train loss:  0.43601253628730774
train gradient:  0.0953455069860911
iteration : 9951
train acc:  0.7265625
train loss:  0.5162643790245056
train gradient:  0.19059198887218498
iteration : 9952
train acc:  0.765625
train loss:  0.45929551124572754
train gradient:  0.09528950040834046
iteration : 9953
train acc:  0.7734375
train loss:  0.4590694308280945
train gradient:  0.1475657251917372
iteration : 9954
train acc:  0.71875
train loss:  0.5322554111480713
train gradient:  0.1419188840941256
iteration : 9955
train acc:  0.7890625
train loss:  0.4307122230529785
train gradient:  0.08806564858063791
iteration : 9956
train acc:  0.796875
train loss:  0.430697500705719
train gradient:  0.12319617038282994
iteration : 9957
train acc:  0.8203125
train loss:  0.42636433243751526
train gradient:  0.11995325369710134
iteration : 9958
train acc:  0.71875
train loss:  0.6098405122756958
train gradient:  0.2891001603010455
iteration : 9959
train acc:  0.78125
train loss:  0.41044074296951294
train gradient:  0.10121107182154244
iteration : 9960
train acc:  0.734375
train loss:  0.5225719213485718
train gradient:  0.11618922646882021
iteration : 9961
train acc:  0.734375
train loss:  0.5111908316612244
train gradient:  0.15314584120259583
iteration : 9962
train acc:  0.71875
train loss:  0.4796997904777527
train gradient:  0.14571462786331824
iteration : 9963
train acc:  0.7109375
train loss:  0.4834386706352234
train gradient:  0.12467711984007823
iteration : 9964
train acc:  0.7265625
train loss:  0.49890992045402527
train gradient:  0.1386893058709094
iteration : 9965
train acc:  0.7421875
train loss:  0.5432361364364624
train gradient:  0.16435283345965546
iteration : 9966
train acc:  0.7890625
train loss:  0.4618774354457855
train gradient:  0.1034879345487749
iteration : 9967
train acc:  0.7109375
train loss:  0.5055825710296631
train gradient:  0.12596434949774082
iteration : 9968
train acc:  0.78125
train loss:  0.4460076093673706
train gradient:  0.1060426686165149
iteration : 9969
train acc:  0.765625
train loss:  0.4883325397968292
train gradient:  0.11778176729504661
iteration : 9970
train acc:  0.7578125
train loss:  0.4787008762359619
train gradient:  0.11082466767383906
iteration : 9971
train acc:  0.78125
train loss:  0.47923728823661804
train gradient:  0.13937661736399898
iteration : 9972
train acc:  0.671875
train loss:  0.5684659481048584
train gradient:  0.16783083999050907
iteration : 9973
train acc:  0.7109375
train loss:  0.49190694093704224
train gradient:  0.14312769455133645
iteration : 9974
train acc:  0.6875
train loss:  0.5003881454467773
train gradient:  0.09427589753020076
iteration : 9975
train acc:  0.71875
train loss:  0.5885127782821655
train gradient:  0.1920217559611733
iteration : 9976
train acc:  0.6875
train loss:  0.5551899671554565
train gradient:  0.17092632293118493
iteration : 9977
train acc:  0.7578125
train loss:  0.47095245122909546
train gradient:  0.1138960314082787
iteration : 9978
train acc:  0.734375
train loss:  0.5151581168174744
train gradient:  0.16441271255453926
iteration : 9979
train acc:  0.8203125
train loss:  0.43144893646240234
train gradient:  0.09539527836629993
iteration : 9980
train acc:  0.7109375
train loss:  0.5458177328109741
train gradient:  0.15740756766303426
iteration : 9981
train acc:  0.7734375
train loss:  0.48123234510421753
train gradient:  0.12879433506788512
iteration : 9982
train acc:  0.7734375
train loss:  0.44878268241882324
train gradient:  0.08501899132950029
iteration : 9983
train acc:  0.7421875
train loss:  0.48420166969299316
train gradient:  0.1151102100659774
iteration : 9984
train acc:  0.8125
train loss:  0.460124671459198
train gradient:  0.1574690756736932
iteration : 9985
train acc:  0.71875
train loss:  0.5262032747268677
train gradient:  0.1492185859048235
iteration : 9986
train acc:  0.7421875
train loss:  0.4998325705528259
train gradient:  0.14137835057317102
iteration : 9987
train acc:  0.7421875
train loss:  0.5079017877578735
train gradient:  0.1514417754668449
iteration : 9988
train acc:  0.7734375
train loss:  0.45064982771873474
train gradient:  0.10032452501810203
iteration : 9989
train acc:  0.7265625
train loss:  0.5431233644485474
train gradient:  0.18109839725283502
iteration : 9990
train acc:  0.7734375
train loss:  0.47355589270591736
train gradient:  0.10474902199625935
iteration : 9991
train acc:  0.6796875
train loss:  0.5539959669113159
train gradient:  0.15868266269064532
iteration : 9992
train acc:  0.7265625
train loss:  0.48556455969810486
train gradient:  0.1463165064544655
iteration : 9993
train acc:  0.75
train loss:  0.48286890983581543
train gradient:  0.1248581894504155
iteration : 9994
train acc:  0.7421875
train loss:  0.4498859941959381
train gradient:  0.11156221372469849
iteration : 9995
train acc:  0.7421875
train loss:  0.5010617971420288
train gradient:  0.14612057925960303
iteration : 9996
train acc:  0.71875
train loss:  0.5094444751739502
train gradient:  0.15304171066149153
iteration : 9997
train acc:  0.7578125
train loss:  0.4558058977127075
train gradient:  0.114546190531484
iteration : 9998
train acc:  0.7109375
train loss:  0.5288872718811035
train gradient:  0.16399172085031932
iteration : 9999
train acc:  0.75
train loss:  0.4769929051399231
train gradient:  0.11474594128894744
iteration : 10000
train acc:  0.796875
train loss:  0.4682997465133667
train gradient:  0.11392676032772425
iteration : 10001
train acc:  0.75
train loss:  0.4961674213409424
train gradient:  0.12575413278464637
iteration : 10002
train acc:  0.71875
train loss:  0.49561893939971924
train gradient:  0.12139340530100295
iteration : 10003
train acc:  0.7578125
train loss:  0.4543483257293701
train gradient:  0.10653730679084539
iteration : 10004
train acc:  0.78125
train loss:  0.4430125057697296
train gradient:  0.13077388032127685
iteration : 10005
train acc:  0.71875
train loss:  0.5511907339096069
train gradient:  0.16953347942505792
iteration : 10006
train acc:  0.7421875
train loss:  0.46943098306655884
train gradient:  0.1223232183064533
iteration : 10007
train acc:  0.7734375
train loss:  0.4759244918823242
train gradient:  0.13344209347696273
iteration : 10008
train acc:  0.7578125
train loss:  0.48088327050209045
train gradient:  0.11410041422273372
iteration : 10009
train acc:  0.7578125
train loss:  0.4867061972618103
train gradient:  0.10218264540213814
iteration : 10010
train acc:  0.7109375
train loss:  0.5066165924072266
train gradient:  0.11518716402025178
iteration : 10011
train acc:  0.7734375
train loss:  0.4790589511394501
train gradient:  0.12000901198549688
iteration : 10012
train acc:  0.7578125
train loss:  0.4425711929798126
train gradient:  0.11150905879931619
iteration : 10013
train acc:  0.734375
train loss:  0.5150116682052612
train gradient:  0.16930825114115783
iteration : 10014
train acc:  0.7734375
train loss:  0.4498952031135559
train gradient:  0.09062691191640672
iteration : 10015
train acc:  0.796875
train loss:  0.4240436553955078
train gradient:  0.08966980300475587
iteration : 10016
train acc:  0.671875
train loss:  0.5152144432067871
train gradient:  0.1430204708254819
iteration : 10017
train acc:  0.7421875
train loss:  0.48274970054626465
train gradient:  0.171795422636856
iteration : 10018
train acc:  0.78125
train loss:  0.4716748595237732
train gradient:  0.1254143419695709
iteration : 10019
train acc:  0.734375
train loss:  0.47418099641799927
train gradient:  0.1256261423812902
iteration : 10020
train acc:  0.7578125
train loss:  0.5645914077758789
train gradient:  0.2775440125844709
iteration : 10021
train acc:  0.7421875
train loss:  0.4870541989803314
train gradient:  0.13395463238244418
iteration : 10022
train acc:  0.7421875
train loss:  0.4910014271736145
train gradient:  0.13838674025730355
iteration : 10023
train acc:  0.7734375
train loss:  0.4750821888446808
train gradient:  0.1244002976355779
iteration : 10024
train acc:  0.734375
train loss:  0.5380977392196655
train gradient:  0.16971759976151685
iteration : 10025
train acc:  0.765625
train loss:  0.4638703167438507
train gradient:  0.13254417680740588
iteration : 10026
train acc:  0.7265625
train loss:  0.52992844581604
train gradient:  0.15925597451361284
iteration : 10027
train acc:  0.75
train loss:  0.4964076280593872
train gradient:  0.12914764296638276
iteration : 10028
train acc:  0.7421875
train loss:  0.5145115852355957
train gradient:  0.17461735337036965
iteration : 10029
train acc:  0.75
train loss:  0.5593809485435486
train gradient:  0.15713075967096293
iteration : 10030
train acc:  0.671875
train loss:  0.5606447458267212
train gradient:  0.17338575240659657
iteration : 10031
train acc:  0.78125
train loss:  0.4275130033493042
train gradient:  0.11774638661403693
iteration : 10032
train acc:  0.703125
train loss:  0.5446110963821411
train gradient:  0.14868176926608376
iteration : 10033
train acc:  0.6953125
train loss:  0.5621152520179749
train gradient:  0.16010177190579947
iteration : 10034
train acc:  0.7578125
train loss:  0.4788169860839844
train gradient:  0.14771418052830937
iteration : 10035
train acc:  0.671875
train loss:  0.5445830821990967
train gradient:  0.16332445887047098
iteration : 10036
train acc:  0.734375
train loss:  0.4908272325992584
train gradient:  0.14164808957780523
iteration : 10037
train acc:  0.7734375
train loss:  0.4614183306694031
train gradient:  0.11632493991430047
iteration : 10038
train acc:  0.7421875
train loss:  0.5171912908554077
train gradient:  0.12811489346557092
iteration : 10039
train acc:  0.7734375
train loss:  0.44996726512908936
train gradient:  0.14392770760561707
iteration : 10040
train acc:  0.7734375
train loss:  0.4526045024394989
train gradient:  0.1174518300725774
iteration : 10041
train acc:  0.7421875
train loss:  0.4820854067802429
train gradient:  0.11422301677290976
iteration : 10042
train acc:  0.7421875
train loss:  0.5082653760910034
train gradient:  0.167235178812627
iteration : 10043
train acc:  0.8203125
train loss:  0.4179399609565735
train gradient:  0.12192439451162548
iteration : 10044
train acc:  0.7890625
train loss:  0.45942458510398865
train gradient:  0.10528315595660519
iteration : 10045
train acc:  0.7421875
train loss:  0.5042794942855835
train gradient:  0.1827047425524688
iteration : 10046
train acc:  0.7421875
train loss:  0.48167288303375244
train gradient:  0.15252952758956295
iteration : 10047
train acc:  0.7890625
train loss:  0.44286349415779114
train gradient:  0.11021627075976671
iteration : 10048
train acc:  0.765625
train loss:  0.4477923512458801
train gradient:  0.11100648859837395
iteration : 10049
train acc:  0.7109375
train loss:  0.5325295329093933
train gradient:  0.16327516625213578
iteration : 10050
train acc:  0.765625
train loss:  0.45186883211135864
train gradient:  0.11555334003904696
iteration : 10051
train acc:  0.7890625
train loss:  0.5198536515235901
train gradient:  0.16639105636657592
iteration : 10052
train acc:  0.7265625
train loss:  0.4938516914844513
train gradient:  0.13554558742130862
iteration : 10053
train acc:  0.8203125
train loss:  0.4468105435371399
train gradient:  0.10349359433813775
iteration : 10054
train acc:  0.6484375
train loss:  0.5600372552871704
train gradient:  0.17214129073390236
iteration : 10055
train acc:  0.7265625
train loss:  0.5197435021400452
train gradient:  0.11914856565252537
iteration : 10056
train acc:  0.7421875
train loss:  0.5315704345703125
train gradient:  0.15488971011444297
iteration : 10057
train acc:  0.765625
train loss:  0.5210862755775452
train gradient:  0.14065946192324968
iteration : 10058
train acc:  0.75
train loss:  0.5163512229919434
train gradient:  0.18229984462604495
iteration : 10059
train acc:  0.78125
train loss:  0.5037685632705688
train gradient:  0.16395088145048348
iteration : 10060
train acc:  0.7265625
train loss:  0.5395060777664185
train gradient:  0.1419415413142401
iteration : 10061
train acc:  0.75
train loss:  0.5202123522758484
train gradient:  0.14547287113989438
iteration : 10062
train acc:  0.7421875
train loss:  0.523016095161438
train gradient:  0.15537652653534398
iteration : 10063
train acc:  0.734375
train loss:  0.5224385857582092
train gradient:  0.15256952646614738
iteration : 10064
train acc:  0.7578125
train loss:  0.48201674222946167
train gradient:  0.12385423129525652
iteration : 10065
train acc:  0.78125
train loss:  0.4621767997741699
train gradient:  0.1271910515969297
iteration : 10066
train acc:  0.7109375
train loss:  0.5295982360839844
train gradient:  0.17504649799749747
iteration : 10067
train acc:  0.7265625
train loss:  0.49976634979248047
train gradient:  0.1499906573539191
iteration : 10068
train acc:  0.75
train loss:  0.47404488921165466
train gradient:  0.12251924403324346
iteration : 10069
train acc:  0.6875
train loss:  0.48921099305152893
train gradient:  0.1235277181313463
iteration : 10070
train acc:  0.828125
train loss:  0.39138105511665344
train gradient:  0.14137558555177593
iteration : 10071
train acc:  0.796875
train loss:  0.44915926456451416
train gradient:  0.11375799497471341
iteration : 10072
train acc:  0.734375
train loss:  0.5014738440513611
train gradient:  0.15912909410136986
iteration : 10073
train acc:  0.65625
train loss:  0.5537960529327393
train gradient:  0.2366652932532836
iteration : 10074
train acc:  0.6953125
train loss:  0.524499773979187
train gradient:  0.11956044382367853
iteration : 10075
train acc:  0.7734375
train loss:  0.47486233711242676
train gradient:  0.11978756270636406
iteration : 10076
train acc:  0.7578125
train loss:  0.4844128489494324
train gradient:  0.12258930078810108
iteration : 10077
train acc:  0.7890625
train loss:  0.4633944630622864
train gradient:  0.10023947662206638
iteration : 10078
train acc:  0.765625
train loss:  0.4983430504798889
train gradient:  0.12578698592990056
iteration : 10079
train acc:  0.7890625
train loss:  0.4678160846233368
train gradient:  0.15398459733427466
iteration : 10080
train acc:  0.8203125
train loss:  0.4726080298423767
train gradient:  0.12609674015973088
iteration : 10081
train acc:  0.7734375
train loss:  0.5031837821006775
train gradient:  0.14007179240187662
iteration : 10082
train acc:  0.734375
train loss:  0.45579734444618225
train gradient:  0.1100493699908539
iteration : 10083
train acc:  0.7421875
train loss:  0.4721108675003052
train gradient:  0.09573983432532582
iteration : 10084
train acc:  0.7421875
train loss:  0.4672665596008301
train gradient:  0.13515190376034517
iteration : 10085
train acc:  0.7734375
train loss:  0.45400452613830566
train gradient:  0.12516785397252647
iteration : 10086
train acc:  0.71875
train loss:  0.509018063545227
train gradient:  0.14579178030051382
iteration : 10087
train acc:  0.6953125
train loss:  0.5531622171401978
train gradient:  0.14329981990852697
iteration : 10088
train acc:  0.7109375
train loss:  0.5033047795295715
train gradient:  0.13874775451310395
iteration : 10089
train acc:  0.7421875
train loss:  0.5258684158325195
train gradient:  0.14498285975451408
iteration : 10090
train acc:  0.703125
train loss:  0.5301687121391296
train gradient:  0.16956848781861777
iteration : 10091
train acc:  0.734375
train loss:  0.49319058656692505
train gradient:  0.12878576154339716
iteration : 10092
train acc:  0.6796875
train loss:  0.6058415174484253
train gradient:  0.192149428004454
iteration : 10093
train acc:  0.765625
train loss:  0.46329769492149353
train gradient:  0.1485468001713658
iteration : 10094
train acc:  0.6953125
train loss:  0.5677952766418457
train gradient:  0.20362146333797465
iteration : 10095
train acc:  0.703125
train loss:  0.529521644115448
train gradient:  0.1541302696029399
iteration : 10096
train acc:  0.6953125
train loss:  0.5663172602653503
train gradient:  0.18148341966356143
iteration : 10097
train acc:  0.7734375
train loss:  0.44735991954803467
train gradient:  0.1390963261168392
iteration : 10098
train acc:  0.7421875
train loss:  0.44734346866607666
train gradient:  0.09837628083737072
iteration : 10099
train acc:  0.703125
train loss:  0.5103650093078613
train gradient:  0.1294191628042332
iteration : 10100
train acc:  0.8046875
train loss:  0.4517498314380646
train gradient:  0.08877322216985183
iteration : 10101
train acc:  0.7109375
train loss:  0.48711127042770386
train gradient:  0.1290655359628513
iteration : 10102
train acc:  0.71875
train loss:  0.500810980796814
train gradient:  0.14589912703688962
iteration : 10103
train acc:  0.7578125
train loss:  0.4776436686515808
train gradient:  0.12597796656747176
iteration : 10104
train acc:  0.734375
train loss:  0.5471776127815247
train gradient:  0.15822583429668963
iteration : 10105
train acc:  0.703125
train loss:  0.5038074254989624
train gradient:  0.12235903649905303
iteration : 10106
train acc:  0.7578125
train loss:  0.4984692931175232
train gradient:  0.1422215723846387
iteration : 10107
train acc:  0.59375
train loss:  0.5818480253219604
train gradient:  0.1898586618685133
iteration : 10108
train acc:  0.7421875
train loss:  0.48289817571640015
train gradient:  0.13594627678338467
iteration : 10109
train acc:  0.7421875
train loss:  0.47165441513061523
train gradient:  0.15663257335811404
iteration : 10110
train acc:  0.7265625
train loss:  0.5536753535270691
train gradient:  0.17048363081285672
iteration : 10111
train acc:  0.8203125
train loss:  0.4457293152809143
train gradient:  0.11346434415525863
iteration : 10112
train acc:  0.8046875
train loss:  0.44909942150115967
train gradient:  0.12966324512565824
iteration : 10113
train acc:  0.75
train loss:  0.5287697911262512
train gradient:  0.14710120638393132
iteration : 10114
train acc:  0.7109375
train loss:  0.49159640073776245
train gradient:  0.11665774166643189
iteration : 10115
train acc:  0.765625
train loss:  0.4478479027748108
train gradient:  0.10732970057474563
iteration : 10116
train acc:  0.7734375
train loss:  0.4513240456581116
train gradient:  0.12539881099074185
iteration : 10117
train acc:  0.703125
train loss:  0.5758579969406128
train gradient:  0.18728790847402316
iteration : 10118
train acc:  0.7265625
train loss:  0.5674848556518555
train gradient:  0.16070519278308826
iteration : 10119
train acc:  0.7734375
train loss:  0.48512762784957886
train gradient:  0.12365411854997312
iteration : 10120
train acc:  0.734375
train loss:  0.5417062640190125
train gradient:  0.17344717440300772
iteration : 10121
train acc:  0.7265625
train loss:  0.49937376379966736
train gradient:  0.11917931418151952
iteration : 10122
train acc:  0.75
train loss:  0.49496641755104065
train gradient:  0.13921651653174888
iteration : 10123
train acc:  0.7578125
train loss:  0.4709857106208801
train gradient:  0.10589981472983678
iteration : 10124
train acc:  0.78125
train loss:  0.4093100428581238
train gradient:  0.1073752689376634
iteration : 10125
train acc:  0.8203125
train loss:  0.44214436411857605
train gradient:  0.11577147139430027
iteration : 10126
train acc:  0.71875
train loss:  0.5226843953132629
train gradient:  0.15963289542279907
iteration : 10127
train acc:  0.75
train loss:  0.5003056526184082
train gradient:  0.1324170444778532
iteration : 10128
train acc:  0.7578125
train loss:  0.5259965658187866
train gradient:  0.14301409508192503
iteration : 10129
train acc:  0.7578125
train loss:  0.4816286563873291
train gradient:  0.11181553866232065
iteration : 10130
train acc:  0.78125
train loss:  0.4528893828392029
train gradient:  0.14289922534713778
iteration : 10131
train acc:  0.6875
train loss:  0.5574194192886353
train gradient:  0.1713514066492363
iteration : 10132
train acc:  0.6640625
train loss:  0.5940141677856445
train gradient:  0.17259003789101085
iteration : 10133
train acc:  0.84375
train loss:  0.3829194903373718
train gradient:  0.08821424298781198
iteration : 10134
train acc:  0.765625
train loss:  0.49395883083343506
train gradient:  0.13356155528235475
iteration : 10135
train acc:  0.8671875
train loss:  0.4127630293369293
train gradient:  0.09746637587563929
iteration : 10136
train acc:  0.75
train loss:  0.47466903924942017
train gradient:  0.1318305727300706
iteration : 10137
train acc:  0.7109375
train loss:  0.49355852603912354
train gradient:  0.1505054007509406
iteration : 10138
train acc:  0.6796875
train loss:  0.5661789178848267
train gradient:  0.16300424196365765
iteration : 10139
train acc:  0.78125
train loss:  0.4550577402114868
train gradient:  0.11553200819527022
iteration : 10140
train acc:  0.75
train loss:  0.5280770063400269
train gradient:  0.15055488723993082
iteration : 10141
train acc:  0.765625
train loss:  0.4445686340332031
train gradient:  0.1220713983663145
iteration : 10142
train acc:  0.7578125
train loss:  0.48873308300971985
train gradient:  0.1427127191182937
iteration : 10143
train acc:  0.7734375
train loss:  0.46515750885009766
train gradient:  0.11157449132239068
iteration : 10144
train acc:  0.71875
train loss:  0.530762791633606
train gradient:  0.17398597280544878
iteration : 10145
train acc:  0.75
train loss:  0.4813162684440613
train gradient:  0.11457126595929071
iteration : 10146
train acc:  0.7734375
train loss:  0.47854387760162354
train gradient:  0.13625633843897295
iteration : 10147
train acc:  0.7578125
train loss:  0.5130338072776794
train gradient:  0.16687673179849322
iteration : 10148
train acc:  0.78125
train loss:  0.41763296723365784
train gradient:  0.12869510358771571
iteration : 10149
train acc:  0.765625
train loss:  0.4810194671154022
train gradient:  0.14441843805369012
iteration : 10150
train acc:  0.8046875
train loss:  0.4251495599746704
train gradient:  0.1295411075697209
iteration : 10151
train acc:  0.7109375
train loss:  0.5099596977233887
train gradient:  0.13159315494932836
iteration : 10152
train acc:  0.6875
train loss:  0.5954283475875854
train gradient:  0.20078479244170816
iteration : 10153
train acc:  0.765625
train loss:  0.45993053913116455
train gradient:  0.15153196249913872
iteration : 10154
train acc:  0.734375
train loss:  0.4957381784915924
train gradient:  0.1804129734153726
iteration : 10155
train acc:  0.6953125
train loss:  0.5083663463592529
train gradient:  0.12872391299516106
iteration : 10156
train acc:  0.6875
train loss:  0.5560836791992188
train gradient:  0.19782463568510641
iteration : 10157
train acc:  0.6796875
train loss:  0.5179430246353149
train gradient:  0.12981902507570248
iteration : 10158
train acc:  0.7578125
train loss:  0.5290795564651489
train gradient:  0.1526156447317531
iteration : 10159
train acc:  0.7265625
train loss:  0.5266880393028259
train gradient:  0.14732340818099549
iteration : 10160
train acc:  0.71875
train loss:  0.5522196888923645
train gradient:  0.13966443922980315
iteration : 10161
train acc:  0.75
train loss:  0.45313000679016113
train gradient:  0.12399808839055651
iteration : 10162
train acc:  0.734375
train loss:  0.5120814442634583
train gradient:  0.14912502308962006
iteration : 10163
train acc:  0.6796875
train loss:  0.548271656036377
train gradient:  0.14750561219852176
iteration : 10164
train acc:  0.7734375
train loss:  0.4706386625766754
train gradient:  0.1474876075341638
iteration : 10165
train acc:  0.796875
train loss:  0.44168493151664734
train gradient:  0.09569558096324421
iteration : 10166
train acc:  0.75
train loss:  0.4757121503353119
train gradient:  0.15106374329887517
iteration : 10167
train acc:  0.7109375
train loss:  0.579792857170105
train gradient:  0.16124363503929878
iteration : 10168
train acc:  0.78125
train loss:  0.43621179461479187
train gradient:  0.14387766241505323
iteration : 10169
train acc:  0.78125
train loss:  0.445565789937973
train gradient:  0.12077389632723239
iteration : 10170
train acc:  0.75
train loss:  0.5440172553062439
train gradient:  0.16773693015271207
iteration : 10171
train acc:  0.7578125
train loss:  0.46544453501701355
train gradient:  0.12175855684490623
iteration : 10172
train acc:  0.828125
train loss:  0.42792314291000366
train gradient:  0.10005562332152697
iteration : 10173
train acc:  0.8125
train loss:  0.4636118710041046
train gradient:  0.14960011603541323
iteration : 10174
train acc:  0.703125
train loss:  0.5852260589599609
train gradient:  0.15838448778288064
iteration : 10175
train acc:  0.796875
train loss:  0.4381442666053772
train gradient:  0.12504851613750356
iteration : 10176
train acc:  0.7109375
train loss:  0.5327022075653076
train gradient:  0.15858711527119895
iteration : 10177
train acc:  0.7265625
train loss:  0.5013017654418945
train gradient:  0.1323783581988952
iteration : 10178
train acc:  0.6953125
train loss:  0.546046257019043
train gradient:  0.13105117581376652
iteration : 10179
train acc:  0.78125
train loss:  0.47391414642333984
train gradient:  0.12368096936356002
iteration : 10180
train acc:  0.78125
train loss:  0.4518525004386902
train gradient:  0.13254745107077867
iteration : 10181
train acc:  0.7265625
train loss:  0.5023613572120667
train gradient:  0.16947501467127843
iteration : 10182
train acc:  0.7265625
train loss:  0.5007134675979614
train gradient:  0.13722523007372855
iteration : 10183
train acc:  0.7578125
train loss:  0.5212795734405518
train gradient:  0.1613669700947465
iteration : 10184
train acc:  0.71875
train loss:  0.5009749531745911
train gradient:  0.11004347904362156
iteration : 10185
train acc:  0.734375
train loss:  0.5119182467460632
train gradient:  0.14285777674502897
iteration : 10186
train acc:  0.78125
train loss:  0.47166162729263306
train gradient:  0.12850204892600575
iteration : 10187
train acc:  0.703125
train loss:  0.5258322954177856
train gradient:  0.15986575232447497
iteration : 10188
train acc:  0.71875
train loss:  0.4957714080810547
train gradient:  0.13387781879908345
iteration : 10189
train acc:  0.6875
train loss:  0.5745173692703247
train gradient:  0.14202093928394183
iteration : 10190
train acc:  0.734375
train loss:  0.4935298562049866
train gradient:  0.1956409829761402
iteration : 10191
train acc:  0.75
train loss:  0.48012885451316833
train gradient:  0.11950478698899246
iteration : 10192
train acc:  0.7265625
train loss:  0.46769028902053833
train gradient:  0.10033856927751532
iteration : 10193
train acc:  0.8046875
train loss:  0.4330490231513977
train gradient:  0.12042794062156345
iteration : 10194
train acc:  0.75
train loss:  0.5057899951934814
train gradient:  0.1327371598926752
iteration : 10195
train acc:  0.796875
train loss:  0.47642505168914795
train gradient:  0.12385059216812945
iteration : 10196
train acc:  0.71875
train loss:  0.5500719547271729
train gradient:  0.1510943232852474
iteration : 10197
train acc:  0.71875
train loss:  0.5235533714294434
train gradient:  0.14683049143606824
iteration : 10198
train acc:  0.78125
train loss:  0.45367610454559326
train gradient:  0.1398558692620654
iteration : 10199
train acc:  0.6953125
train loss:  0.5248172283172607
train gradient:  0.16604659422977963
iteration : 10200
train acc:  0.71875
train loss:  0.4847927987575531
train gradient:  0.1400944695780024
iteration : 10201
train acc:  0.6484375
train loss:  0.5792167782783508
train gradient:  0.23934037503144884
iteration : 10202
train acc:  0.6640625
train loss:  0.5719462633132935
train gradient:  0.19789187130801364
iteration : 10203
train acc:  0.78125
train loss:  0.4892294704914093
train gradient:  0.12194238947950199
iteration : 10204
train acc:  0.765625
train loss:  0.4739789366722107
train gradient:  0.11131567099057693
iteration : 10205
train acc:  0.7265625
train loss:  0.5272573232650757
train gradient:  0.16261421842748233
iteration : 10206
train acc:  0.7109375
train loss:  0.5383734107017517
train gradient:  0.13997452868939417
iteration : 10207
train acc:  0.75
train loss:  0.45721501111984253
train gradient:  0.11050722480015232
iteration : 10208
train acc:  0.7578125
train loss:  0.4796522557735443
train gradient:  0.144321853565543
iteration : 10209
train acc:  0.7578125
train loss:  0.5102367401123047
train gradient:  0.15355703030359996
iteration : 10210
train acc:  0.828125
train loss:  0.41817706823349
train gradient:  0.11379503540419444
iteration : 10211
train acc:  0.7421875
train loss:  0.5407932996749878
train gradient:  0.16705061326429432
iteration : 10212
train acc:  0.7421875
train loss:  0.49804824590682983
train gradient:  0.14417548938185348
iteration : 10213
train acc:  0.734375
train loss:  0.4514530301094055
train gradient:  0.14503123380408606
iteration : 10214
train acc:  0.734375
train loss:  0.5050597190856934
train gradient:  0.1684820871411935
iteration : 10215
train acc:  0.71875
train loss:  0.5418213605880737
train gradient:  0.1791252752739409
iteration : 10216
train acc:  0.7578125
train loss:  0.5056894421577454
train gradient:  0.13198393650841456
iteration : 10217
train acc:  0.6953125
train loss:  0.5350974202156067
train gradient:  0.15082213542095885
iteration : 10218
train acc:  0.671875
train loss:  0.5795164108276367
train gradient:  0.15855862673717447
iteration : 10219
train acc:  0.765625
train loss:  0.4792720675468445
train gradient:  0.10176711311279944
iteration : 10220
train acc:  0.75
train loss:  0.5059093236923218
train gradient:  0.181361250001822
iteration : 10221
train acc:  0.71875
train loss:  0.4494897425174713
train gradient:  0.117882486742615
iteration : 10222
train acc:  0.7421875
train loss:  0.5106388926506042
train gradient:  0.15899348186854245
iteration : 10223
train acc:  0.765625
train loss:  0.493678480386734
train gradient:  0.15991413813092664
iteration : 10224
train acc:  0.7578125
train loss:  0.4551699459552765
train gradient:  0.12006069243146726
iteration : 10225
train acc:  0.71875
train loss:  0.4648416340351105
train gradient:  0.11228361898375455
iteration : 10226
train acc:  0.7734375
train loss:  0.4614824056625366
train gradient:  0.1076230907592689
iteration : 10227
train acc:  0.703125
train loss:  0.494552344083786
train gradient:  0.1390589290299688
iteration : 10228
train acc:  0.7890625
train loss:  0.4327618479728699
train gradient:  0.11532218533190972
iteration : 10229
train acc:  0.7265625
train loss:  0.5219999551773071
train gradient:  0.16007014651501308
iteration : 10230
train acc:  0.7265625
train loss:  0.4711700975894928
train gradient:  0.12455177222824973
iteration : 10231
train acc:  0.7265625
train loss:  0.5167191028594971
train gradient:  0.14407121877703777
iteration : 10232
train acc:  0.7109375
train loss:  0.5080909729003906
train gradient:  0.1455104586401059
iteration : 10233
train acc:  0.6953125
train loss:  0.5248258113861084
train gradient:  0.13544624193206317
iteration : 10234
train acc:  0.6640625
train loss:  0.5727396607398987
train gradient:  0.12085919610437969
iteration : 10235
train acc:  0.6953125
train loss:  0.5161162614822388
train gradient:  0.13106891231610243
iteration : 10236
train acc:  0.8125
train loss:  0.4678186774253845
train gradient:  0.10848243537607641
iteration : 10237
train acc:  0.734375
train loss:  0.5031540393829346
train gradient:  0.13657511221491675
iteration : 10238
train acc:  0.734375
train loss:  0.4838075339794159
train gradient:  0.14908741770893733
iteration : 10239
train acc:  0.8125
train loss:  0.4535188674926758
train gradient:  0.11017899408803629
iteration : 10240
train acc:  0.7421875
train loss:  0.4379887580871582
train gradient:  0.1269400112423153
iteration : 10241
train acc:  0.6875
train loss:  0.5450907945632935
train gradient:  0.2089916175097101
iteration : 10242
train acc:  0.7734375
train loss:  0.429415225982666
train gradient:  0.07815785724916566
iteration : 10243
train acc:  0.6796875
train loss:  0.5530505180358887
train gradient:  0.1473261543956895
iteration : 10244
train acc:  0.7265625
train loss:  0.48042088747024536
train gradient:  0.12104163402582462
iteration : 10245
train acc:  0.6875
train loss:  0.5732606649398804
train gradient:  0.18165747707981467
iteration : 10246
train acc:  0.7421875
train loss:  0.5129941701889038
train gradient:  0.17612260319778988
iteration : 10247
train acc:  0.7109375
train loss:  0.5015503764152527
train gradient:  0.12332546155288827
iteration : 10248
train acc:  0.75
train loss:  0.46051210165023804
train gradient:  0.12633606409053919
iteration : 10249
train acc:  0.6875
train loss:  0.5705516934394836
train gradient:  0.12857199458020152
iteration : 10250
train acc:  0.7109375
train loss:  0.5235897898674011
train gradient:  0.13631606260911927
iteration : 10251
train acc:  0.7265625
train loss:  0.5381332039833069
train gradient:  0.1466346664625256
iteration : 10252
train acc:  0.6953125
train loss:  0.49574166536331177
train gradient:  0.1160770261329277
iteration : 10253
train acc:  0.71875
train loss:  0.5324944257736206
train gradient:  0.20090765342669123
iteration : 10254
train acc:  0.7109375
train loss:  0.46594011783599854
train gradient:  0.14177563433823165
iteration : 10255
train acc:  0.8125
train loss:  0.411702036857605
train gradient:  0.09777181719174959
iteration : 10256
train acc:  0.7734375
train loss:  0.5442386865615845
train gradient:  0.19572132781740476
iteration : 10257
train acc:  0.7109375
train loss:  0.5800451636314392
train gradient:  0.2181004025009899
iteration : 10258
train acc:  0.7734375
train loss:  0.4692161977291107
train gradient:  0.11334470082076097
iteration : 10259
train acc:  0.703125
train loss:  0.5095781087875366
train gradient:  0.1295877877214239
iteration : 10260
train acc:  0.7734375
train loss:  0.4405449628829956
train gradient:  0.10756243605840458
iteration : 10261
train acc:  0.7421875
train loss:  0.5064133405685425
train gradient:  0.14633191330758177
iteration : 10262
train acc:  0.6953125
train loss:  0.5447684526443481
train gradient:  0.145593836127628
iteration : 10263
train acc:  0.8203125
train loss:  0.44059979915618896
train gradient:  0.13124680274764688
iteration : 10264
train acc:  0.71875
train loss:  0.5014354586601257
train gradient:  0.13829068405630696
iteration : 10265
train acc:  0.734375
train loss:  0.5275808572769165
train gradient:  0.12999351097966522
iteration : 10266
train acc:  0.7578125
train loss:  0.5291339159011841
train gradient:  0.14208555245365964
iteration : 10267
train acc:  0.65625
train loss:  0.5487348437309265
train gradient:  0.15571932536392957
iteration : 10268
train acc:  0.7421875
train loss:  0.48024797439575195
train gradient:  0.13150822728582695
iteration : 10269
train acc:  0.7109375
train loss:  0.5300613045692444
train gradient:  0.15102549900382578
iteration : 10270
train acc:  0.734375
train loss:  0.5656599998474121
train gradient:  0.18664357235194062
iteration : 10271
train acc:  0.7578125
train loss:  0.48140856623649597
train gradient:  0.12199245178636522
iteration : 10272
train acc:  0.8046875
train loss:  0.45128166675567627
train gradient:  0.10825966283860446
iteration : 10273
train acc:  0.71875
train loss:  0.5284260511398315
train gradient:  0.16912518187171255
iteration : 10274
train acc:  0.71875
train loss:  0.5265237092971802
train gradient:  0.14836338697807305
iteration : 10275
train acc:  0.765625
train loss:  0.4706384241580963
train gradient:  0.11344519488287834
iteration : 10276
train acc:  0.71875
train loss:  0.4908158779144287
train gradient:  0.1209533455867141
iteration : 10277
train acc:  0.8046875
train loss:  0.44344455003738403
train gradient:  0.12074230078757991
iteration : 10278
train acc:  0.7421875
train loss:  0.4837038516998291
train gradient:  0.13378307940846487
iteration : 10279
train acc:  0.8046875
train loss:  0.45140647888183594
train gradient:  0.11431148431312226
iteration : 10280
train acc:  0.765625
train loss:  0.5088772773742676
train gradient:  0.1382264467390022
iteration : 10281
train acc:  0.7890625
train loss:  0.4320624768733978
train gradient:  0.09728268268371686
iteration : 10282
train acc:  0.765625
train loss:  0.45686835050582886
train gradient:  0.09632164252838425
iteration : 10283
train acc:  0.78125
train loss:  0.4695696532726288
train gradient:  0.10715734125475157
iteration : 10284
train acc:  0.7578125
train loss:  0.4875921607017517
train gradient:  0.15241989441706585
iteration : 10285
train acc:  0.734375
train loss:  0.529120922088623
train gradient:  0.15308231185750903
iteration : 10286
train acc:  0.7265625
train loss:  0.5176815986633301
train gradient:  0.1410427973686238
iteration : 10287
train acc:  0.65625
train loss:  0.5320948362350464
train gradient:  0.15141550812433363
iteration : 10288
train acc:  0.796875
train loss:  0.45090800523757935
train gradient:  0.11104816803174382
iteration : 10289
train acc:  0.78125
train loss:  0.4521959722042084
train gradient:  0.10708745171311651
iteration : 10290
train acc:  0.7578125
train loss:  0.48221829533576965
train gradient:  0.12112333847398403
iteration : 10291
train acc:  0.703125
train loss:  0.5423605442047119
train gradient:  0.14236756926495192
iteration : 10292
train acc:  0.6953125
train loss:  0.522609293460846
train gradient:  0.16383233665843133
iteration : 10293
train acc:  0.765625
train loss:  0.47727441787719727
train gradient:  0.1647940316604827
iteration : 10294
train acc:  0.734375
train loss:  0.49942082166671753
train gradient:  0.15266746469712342
iteration : 10295
train acc:  0.75
train loss:  0.47408953309059143
train gradient:  0.12643587454100647
iteration : 10296
train acc:  0.75
train loss:  0.4805220067501068
train gradient:  0.14414235689583083
iteration : 10297
train acc:  0.75
train loss:  0.47788751125335693
train gradient:  0.14140285253121643
iteration : 10298
train acc:  0.6875
train loss:  0.5231359004974365
train gradient:  0.14339471092269845
iteration : 10299
train acc:  0.84375
train loss:  0.38437169790267944
train gradient:  0.08121743041727401
iteration : 10300
train acc:  0.7890625
train loss:  0.474558562040329
train gradient:  0.16810106860103352
iteration : 10301
train acc:  0.7734375
train loss:  0.48235395550727844
train gradient:  0.14229532075053597
iteration : 10302
train acc:  0.7734375
train loss:  0.4937630891799927
train gradient:  0.1327519578612546
iteration : 10303
train acc:  0.6640625
train loss:  0.6275034546852112
train gradient:  0.19534397263351638
iteration : 10304
train acc:  0.6953125
train loss:  0.5122992992401123
train gradient:  0.13524454380768022
iteration : 10305
train acc:  0.734375
train loss:  0.483388215303421
train gradient:  0.11127514275467246
iteration : 10306
train acc:  0.765625
train loss:  0.46912774443626404
train gradient:  0.13763322528527672
iteration : 10307
train acc:  0.7265625
train loss:  0.49375027418136597
train gradient:  0.15124254979256924
iteration : 10308
train acc:  0.78125
train loss:  0.46431687474250793
train gradient:  0.14519693556319357
iteration : 10309
train acc:  0.7109375
train loss:  0.5205345153808594
train gradient:  0.13482580034171884
iteration : 10310
train acc:  0.6640625
train loss:  0.5899573564529419
train gradient:  0.21392320534328826
iteration : 10311
train acc:  0.8046875
train loss:  0.4714025855064392
train gradient:  0.13398116669126503
iteration : 10312
train acc:  0.7890625
train loss:  0.45687931776046753
train gradient:  0.1063580947485629
iteration : 10313
train acc:  0.71875
train loss:  0.47391006350517273
train gradient:  0.1300277870552628
iteration : 10314
train acc:  0.6953125
train loss:  0.5205317735671997
train gradient:  0.11557337019366046
iteration : 10315
train acc:  0.703125
train loss:  0.47346973419189453
train gradient:  0.14811544911773128
iteration : 10316
train acc:  0.7734375
train loss:  0.44399940967559814
train gradient:  0.14154894121363193
iteration : 10317
train acc:  0.7578125
train loss:  0.5091202855110168
train gradient:  0.11544448322759256
iteration : 10318
train acc:  0.78125
train loss:  0.45629626512527466
train gradient:  0.15479486937268638
iteration : 10319
train acc:  0.703125
train loss:  0.5456176400184631
train gradient:  0.1899089153453387
iteration : 10320
train acc:  0.765625
train loss:  0.4538417458534241
train gradient:  0.10714507485024975
iteration : 10321
train acc:  0.8125
train loss:  0.4060177803039551
train gradient:  0.1165759453682405
iteration : 10322
train acc:  0.7578125
train loss:  0.4685482382774353
train gradient:  0.12209870805598394
iteration : 10323
train acc:  0.7734375
train loss:  0.4873995780944824
train gradient:  0.1342161645561482
iteration : 10324
train acc:  0.734375
train loss:  0.5387898683547974
train gradient:  0.16550712476787466
iteration : 10325
train acc:  0.734375
train loss:  0.50232994556427
train gradient:  0.14035146351727773
iteration : 10326
train acc:  0.765625
train loss:  0.4783990979194641
train gradient:  0.14655961649578586
iteration : 10327
train acc:  0.6953125
train loss:  0.5522984266281128
train gradient:  0.166393874859775
iteration : 10328
train acc:  0.7109375
train loss:  0.4777568578720093
train gradient:  0.12758191306205108
iteration : 10329
train acc:  0.734375
train loss:  0.5022599697113037
train gradient:  0.13611565099171968
iteration : 10330
train acc:  0.7421875
train loss:  0.4835086166858673
train gradient:  0.12314484376839144
iteration : 10331
train acc:  0.71875
train loss:  0.5078088045120239
train gradient:  0.1949639293951414
iteration : 10332
train acc:  0.7265625
train loss:  0.49725598096847534
train gradient:  0.12891177969327863
iteration : 10333
train acc:  0.7421875
train loss:  0.46840041875839233
train gradient:  0.13251978455936841
iteration : 10334
train acc:  0.7578125
train loss:  0.541570782661438
train gradient:  0.19059700168251048
iteration : 10335
train acc:  0.8125
train loss:  0.40886443853378296
train gradient:  0.09860176737323056
iteration : 10336
train acc:  0.796875
train loss:  0.4304213225841522
train gradient:  0.10149168597976368
iteration : 10337
train acc:  0.765625
train loss:  0.4676414430141449
train gradient:  0.12260796934078448
iteration : 10338
train acc:  0.7421875
train loss:  0.497905433177948
train gradient:  0.11476059327247048
iteration : 10339
train acc:  0.6875
train loss:  0.550089955329895
train gradient:  0.17768148953909546
iteration : 10340
train acc:  0.78125
train loss:  0.4442802965641022
train gradient:  0.1446505881849529
iteration : 10341
train acc:  0.6953125
train loss:  0.49150967597961426
train gradient:  0.14103377266118505
iteration : 10342
train acc:  0.6796875
train loss:  0.5347919464111328
train gradient:  0.14578692489396783
iteration : 10343
train acc:  0.8046875
train loss:  0.4587705135345459
train gradient:  0.1230695143401945
iteration : 10344
train acc:  0.75
train loss:  0.4749830961227417
train gradient:  0.10644418164495208
iteration : 10345
train acc:  0.71875
train loss:  0.5370444059371948
train gradient:  0.1853234987397325
iteration : 10346
train acc:  0.7734375
train loss:  0.46168363094329834
train gradient:  0.13242065158729452
iteration : 10347
train acc:  0.78125
train loss:  0.4258570373058319
train gradient:  0.10425511584737798
iteration : 10348
train acc:  0.75
train loss:  0.4858805537223816
train gradient:  0.1483446190321454
iteration : 10349
train acc:  0.7265625
train loss:  0.5091888904571533
train gradient:  0.14665488442992924
iteration : 10350
train acc:  0.8203125
train loss:  0.41714805364608765
train gradient:  0.11486969009582744
iteration : 10351
train acc:  0.7578125
train loss:  0.4415985941886902
train gradient:  0.12116756660676764
iteration : 10352
train acc:  0.7421875
train loss:  0.5043751001358032
train gradient:  0.15835773566402223
iteration : 10353
train acc:  0.7890625
train loss:  0.43796807527542114
train gradient:  0.12493265971302107
iteration : 10354
train acc:  0.7734375
train loss:  0.5036951899528503
train gradient:  0.14383435386633114
iteration : 10355
train acc:  0.78125
train loss:  0.4414237141609192
train gradient:  0.12368745978755064
iteration : 10356
train acc:  0.7890625
train loss:  0.5053387880325317
train gradient:  0.14679655901128963
iteration : 10357
train acc:  0.7421875
train loss:  0.5496790409088135
train gradient:  0.13990928453418083
iteration : 10358
train acc:  0.734375
train loss:  0.4832357168197632
train gradient:  0.11835841243317764
iteration : 10359
train acc:  0.7265625
train loss:  0.5143941044807434
train gradient:  0.15328245404213572
iteration : 10360
train acc:  0.796875
train loss:  0.4292101562023163
train gradient:  0.11127552089079906
iteration : 10361
train acc:  0.75
train loss:  0.4873278737068176
train gradient:  0.14090755287996476
iteration : 10362
train acc:  0.765625
train loss:  0.4841355085372925
train gradient:  0.14050170364651088
iteration : 10363
train acc:  0.734375
train loss:  0.5438961982727051
train gradient:  0.1463672754367571
iteration : 10364
train acc:  0.7734375
train loss:  0.4704919755458832
train gradient:  0.11607054954670469
iteration : 10365
train acc:  0.75
train loss:  0.4862288236618042
train gradient:  0.15109614189166926
iteration : 10366
train acc:  0.7421875
train loss:  0.46631819009780884
train gradient:  0.13207691782156367
iteration : 10367
train acc:  0.6796875
train loss:  0.5344508290290833
train gradient:  0.16919995250921122
iteration : 10368
train acc:  0.8125
train loss:  0.415558785200119
train gradient:  0.11424583476047687
iteration : 10369
train acc:  0.78125
train loss:  0.45362916588783264
train gradient:  0.10989698717546033
iteration : 10370
train acc:  0.671875
train loss:  0.5267232060432434
train gradient:  0.1628149362699362
iteration : 10371
train acc:  0.8046875
train loss:  0.4356890618801117
train gradient:  0.10336115132684096
iteration : 10372
train acc:  0.7734375
train loss:  0.4694975018501282
train gradient:  0.1287482851262029
iteration : 10373
train acc:  0.7109375
train loss:  0.5048279762268066
train gradient:  0.17758974978693848
iteration : 10374
train acc:  0.7734375
train loss:  0.5024760961532593
train gradient:  0.17106088879384695
iteration : 10375
train acc:  0.765625
train loss:  0.46956372261047363
train gradient:  0.12041574208624924
iteration : 10376
train acc:  0.7265625
train loss:  0.5334681272506714
train gradient:  0.19738736374492635
iteration : 10377
train acc:  0.8125
train loss:  0.42439544200897217
train gradient:  0.09583690226023381
iteration : 10378
train acc:  0.7734375
train loss:  0.49498656392097473
train gradient:  0.10529117160914542
iteration : 10379
train acc:  0.703125
train loss:  0.5476889610290527
train gradient:  0.12797565496197727
iteration : 10380
train acc:  0.7421875
train loss:  0.5093262195587158
train gradient:  0.13343485681384337
iteration : 10381
train acc:  0.703125
train loss:  0.5689631104469299
train gradient:  0.24432602268259845
iteration : 10382
train acc:  0.7265625
train loss:  0.501057505607605
train gradient:  0.13347838111782864
iteration : 10383
train acc:  0.7578125
train loss:  0.47394782304763794
train gradient:  0.12145420481386712
iteration : 10384
train acc:  0.8125
train loss:  0.44799137115478516
train gradient:  0.09365609217938882
iteration : 10385
train acc:  0.7109375
train loss:  0.5231859087944031
train gradient:  0.13255562253588438
iteration : 10386
train acc:  0.7421875
train loss:  0.460273802280426
train gradient:  0.11956076310947333
iteration : 10387
train acc:  0.7265625
train loss:  0.5311672687530518
train gradient:  0.13917943660770724
iteration : 10388
train acc:  0.7890625
train loss:  0.47522974014282227
train gradient:  0.1103862395791415
iteration : 10389
train acc:  0.671875
train loss:  0.5388136506080627
train gradient:  0.17029085982567188
iteration : 10390
train acc:  0.7734375
train loss:  0.4630707800388336
train gradient:  0.1171343750201314
iteration : 10391
train acc:  0.7109375
train loss:  0.5115540027618408
train gradient:  0.16523179510744118
iteration : 10392
train acc:  0.75
train loss:  0.48220503330230713
train gradient:  0.14235052435562914
iteration : 10393
train acc:  0.71875
train loss:  0.5000182390213013
train gradient:  0.16103694567035814
iteration : 10394
train acc:  0.734375
train loss:  0.5610415935516357
train gradient:  0.15107282185703225
iteration : 10395
train acc:  0.7890625
train loss:  0.45223888754844666
train gradient:  0.1343353824554102
iteration : 10396
train acc:  0.7734375
train loss:  0.45246821641921997
train gradient:  0.13360774668332134
iteration : 10397
train acc:  0.6796875
train loss:  0.517503023147583
train gradient:  0.23247081665212455
iteration : 10398
train acc:  0.7734375
train loss:  0.46232306957244873
train gradient:  0.1250044881209751
iteration : 10399
train acc:  0.71875
train loss:  0.48882007598876953
train gradient:  0.16604609679637466
iteration : 10400
train acc:  0.6796875
train loss:  0.6150921583175659
train gradient:  0.20395799119672636
iteration : 10401
train acc:  0.7890625
train loss:  0.46949392557144165
train gradient:  0.10611194844252338
iteration : 10402
train acc:  0.7421875
train loss:  0.4566153287887573
train gradient:  0.11520998890709487
iteration : 10403
train acc:  0.71875
train loss:  0.5579546689987183
train gradient:  0.17011322901010384
iteration : 10404
train acc:  0.78125
train loss:  0.4191263020038605
train gradient:  0.1213192808707158
iteration : 10405
train acc:  0.6875
train loss:  0.5355076789855957
train gradient:  0.15051083345736715
iteration : 10406
train acc:  0.6796875
train loss:  0.5759280920028687
train gradient:  0.20589661802042492
iteration : 10407
train acc:  0.7578125
train loss:  0.43560442328453064
train gradient:  0.1141040299057585
iteration : 10408
train acc:  0.7734375
train loss:  0.49356332421302795
train gradient:  0.11865054396480142
iteration : 10409
train acc:  0.71875
train loss:  0.5072582960128784
train gradient:  0.12639564882647575
iteration : 10410
train acc:  0.71875
train loss:  0.48617637157440186
train gradient:  0.14776816461859021
iteration : 10411
train acc:  0.7265625
train loss:  0.4881485104560852
train gradient:  0.13543289385112783
iteration : 10412
train acc:  0.7890625
train loss:  0.45598334074020386
train gradient:  0.11949744965483798
iteration : 10413
train acc:  0.7578125
train loss:  0.532386302947998
train gradient:  0.1371818572780035
iteration : 10414
train acc:  0.75
train loss:  0.467082142829895
train gradient:  0.10729686593088801
iteration : 10415
train acc:  0.7890625
train loss:  0.4428291320800781
train gradient:  0.11172059281811363
iteration : 10416
train acc:  0.703125
train loss:  0.5282794237136841
train gradient:  0.20563962704500915
iteration : 10417
train acc:  0.8046875
train loss:  0.44126713275909424
train gradient:  0.1308803041375361
iteration : 10418
train acc:  0.6953125
train loss:  0.49544385075569153
train gradient:  0.13981706045634834
iteration : 10419
train acc:  0.6875
train loss:  0.5307340025901794
train gradient:  0.17351222976533637
iteration : 10420
train acc:  0.7421875
train loss:  0.48233968019485474
train gradient:  0.13093305691578733
iteration : 10421
train acc:  0.7890625
train loss:  0.48753222823143005
train gradient:  0.11571403185011556
iteration : 10422
train acc:  0.765625
train loss:  0.4985758066177368
train gradient:  0.15664580192530975
iteration : 10423
train acc:  0.7421875
train loss:  0.5157384872436523
train gradient:  0.13246451293613754
iteration : 10424
train acc:  0.7890625
train loss:  0.4259355068206787
train gradient:  0.10669342375872745
iteration : 10425
train acc:  0.7890625
train loss:  0.47396230697631836
train gradient:  0.12301649217079971
iteration : 10426
train acc:  0.7421875
train loss:  0.4647274911403656
train gradient:  0.12316857255820421
iteration : 10427
train acc:  0.78125
train loss:  0.4547629654407501
train gradient:  0.1256584475536209
iteration : 10428
train acc:  0.7578125
train loss:  0.4871901571750641
train gradient:  0.11317628494828956
iteration : 10429
train acc:  0.828125
train loss:  0.46288183331489563
train gradient:  0.11658335364452044
iteration : 10430
train acc:  0.75
train loss:  0.49833375215530396
train gradient:  0.15275788439617233
iteration : 10431
train acc:  0.6640625
train loss:  0.5427084565162659
train gradient:  0.19087116446896696
iteration : 10432
train acc:  0.7265625
train loss:  0.48336705565452576
train gradient:  0.1043880084471649
iteration : 10433
train acc:  0.75
train loss:  0.5059476494789124
train gradient:  0.1412212777209726
iteration : 10434
train acc:  0.6953125
train loss:  0.5049704909324646
train gradient:  0.1408272106881246
iteration : 10435
train acc:  0.7578125
train loss:  0.4656738340854645
train gradient:  0.10119097777431707
iteration : 10436
train acc:  0.7109375
train loss:  0.5662036538124084
train gradient:  0.13623141524163443
iteration : 10437
train acc:  0.7421875
train loss:  0.48718661069869995
train gradient:  0.11398199573646287
iteration : 10438
train acc:  0.78125
train loss:  0.4800546169281006
train gradient:  0.16849383143457447
iteration : 10439
train acc:  0.734375
train loss:  0.5563873648643494
train gradient:  0.16422344767810784
iteration : 10440
train acc:  0.7734375
train loss:  0.49523523449897766
train gradient:  0.1414518601425578
iteration : 10441
train acc:  0.7578125
train loss:  0.50093674659729
train gradient:  0.1409585666392973
iteration : 10442
train acc:  0.7109375
train loss:  0.533470869064331
train gradient:  0.15128150630275655
iteration : 10443
train acc:  0.7578125
train loss:  0.44098320603370667
train gradient:  0.10873225072327332
iteration : 10444
train acc:  0.671875
train loss:  0.5890858173370361
train gradient:  0.1912513479386091
iteration : 10445
train acc:  0.6953125
train loss:  0.49135130643844604
train gradient:  0.12968836273684775
iteration : 10446
train acc:  0.75
train loss:  0.4613422155380249
train gradient:  0.10503801562759565
iteration : 10447
train acc:  0.7421875
train loss:  0.5138767957687378
train gradient:  0.15045623945494496
iteration : 10448
train acc:  0.8125
train loss:  0.4382001459598541
train gradient:  0.10968251523289171
iteration : 10449
train acc:  0.71875
train loss:  0.5476112961769104
train gradient:  0.15804773131044753
iteration : 10450
train acc:  0.7734375
train loss:  0.4779198169708252
train gradient:  0.11876498853690495
iteration : 10451
train acc:  0.734375
train loss:  0.5184992551803589
train gradient:  0.14247460474521878
iteration : 10452
train acc:  0.6953125
train loss:  0.5792073607444763
train gradient:  0.13670556990172428
iteration : 10453
train acc:  0.7421875
train loss:  0.5104472041130066
train gradient:  0.12309804281576794
iteration : 10454
train acc:  0.7265625
train loss:  0.4987413287162781
train gradient:  0.14178574468983998
iteration : 10455
train acc:  0.6484375
train loss:  0.5932665467262268
train gradient:  0.1432196463014355
iteration : 10456
train acc:  0.75
train loss:  0.44875654578208923
train gradient:  0.11792048259903797
iteration : 10457
train acc:  0.78125
train loss:  0.4699161648750305
train gradient:  0.11980192440059914
iteration : 10458
train acc:  0.671875
train loss:  0.5699184536933899
train gradient:  0.1522010026418107
iteration : 10459
train acc:  0.7578125
train loss:  0.4501902461051941
train gradient:  0.15075598166239026
iteration : 10460
train acc:  0.765625
train loss:  0.41801977157592773
train gradient:  0.11450803988686137
iteration : 10461
train acc:  0.78125
train loss:  0.4398927092552185
train gradient:  0.1335592133216315
iteration : 10462
train acc:  0.703125
train loss:  0.4862362742424011
train gradient:  0.14296015816343144
iteration : 10463
train acc:  0.734375
train loss:  0.4603385031223297
train gradient:  0.11757305802476
iteration : 10464
train acc:  0.7265625
train loss:  0.4768596291542053
train gradient:  0.12846288824176744
iteration : 10465
train acc:  0.75
train loss:  0.5155037641525269
train gradient:  0.14366124833242436
iteration : 10466
train acc:  0.7578125
train loss:  0.5021482110023499
train gradient:  0.1741644151835857
iteration : 10467
train acc:  0.7421875
train loss:  0.42511025071144104
train gradient:  0.127575010866939
iteration : 10468
train acc:  0.7265625
train loss:  0.48241332173347473
train gradient:  0.12427761326106516
iteration : 10469
train acc:  0.671875
train loss:  0.5330467820167542
train gradient:  0.18151852743142627
iteration : 10470
train acc:  0.6328125
train loss:  0.5743878483772278
train gradient:  0.21855164844874264
iteration : 10471
train acc:  0.7578125
train loss:  0.46370479464530945
train gradient:  0.1420293362442357
iteration : 10472
train acc:  0.765625
train loss:  0.4646499752998352
train gradient:  0.127973920056142
iteration : 10473
train acc:  0.7578125
train loss:  0.45726755261421204
train gradient:  0.10497895756912401
iteration : 10474
train acc:  0.734375
train loss:  0.5286349058151245
train gradient:  0.15827850529206966
iteration : 10475
train acc:  0.75
train loss:  0.5069131851196289
train gradient:  0.12488985185769375
iteration : 10476
train acc:  0.8125
train loss:  0.39914143085479736
train gradient:  0.10710451942367072
iteration : 10477
train acc:  0.7265625
train loss:  0.4906299114227295
train gradient:  0.14181184906288888
iteration : 10478
train acc:  0.7265625
train loss:  0.4853859543800354
train gradient:  0.13264685346388538
iteration : 10479
train acc:  0.734375
train loss:  0.5708187222480774
train gradient:  0.17473109342100485
iteration : 10480
train acc:  0.7734375
train loss:  0.5330774784088135
train gradient:  0.14776854520311167
iteration : 10481
train acc:  0.7421875
train loss:  0.5207346677780151
train gradient:  0.14770928742917366
iteration : 10482
train acc:  0.734375
train loss:  0.5678953528404236
train gradient:  0.15247442243200582
iteration : 10483
train acc:  0.75
train loss:  0.49740585684776306
train gradient:  0.12472504503644165
iteration : 10484
train acc:  0.765625
train loss:  0.4737537205219269
train gradient:  0.1260635805807147
iteration : 10485
train acc:  0.7109375
train loss:  0.5450853109359741
train gradient:  0.13654399240834023
iteration : 10486
train acc:  0.7421875
train loss:  0.45802024006843567
train gradient:  0.12786594734255913
iteration : 10487
train acc:  0.7421875
train loss:  0.4860156178474426
train gradient:  0.115514426404925
iteration : 10488
train acc:  0.7421875
train loss:  0.4831095337867737
train gradient:  0.10926331338883892
iteration : 10489
train acc:  0.7109375
train loss:  0.488701730966568
train gradient:  0.1174392023272493
iteration : 10490
train acc:  0.8125
train loss:  0.45556798577308655
train gradient:  0.1271344382562171
iteration : 10491
train acc:  0.7421875
train loss:  0.4579765796661377
train gradient:  0.10310458444480475
iteration : 10492
train acc:  0.7578125
train loss:  0.4951772391796112
train gradient:  0.13584054169797394
iteration : 10493
train acc:  0.7109375
train loss:  0.5086415410041809
train gradient:  0.12985958819732984
iteration : 10494
train acc:  0.734375
train loss:  0.4980087876319885
train gradient:  0.1409508449871574
iteration : 10495
train acc:  0.75
train loss:  0.5078094005584717
train gradient:  0.19023137921397243
iteration : 10496
train acc:  0.75
train loss:  0.4818311631679535
train gradient:  0.13928067589323212
iteration : 10497
train acc:  0.7421875
train loss:  0.4916287958621979
train gradient:  0.1150458778906222
iteration : 10498
train acc:  0.71875
train loss:  0.5296024084091187
train gradient:  0.15108517036743355
iteration : 10499
train acc:  0.7265625
train loss:  0.5008705854415894
train gradient:  0.1567276744256314
iteration : 10500
train acc:  0.7109375
train loss:  0.48551884293556213
train gradient:  0.13264951490726823
iteration : 10501
train acc:  0.703125
train loss:  0.5106109380722046
train gradient:  0.13731447402831692
iteration : 10502
train acc:  0.7421875
train loss:  0.5038177371025085
train gradient:  0.1525919453393309
iteration : 10503
train acc:  0.7265625
train loss:  0.4895086884498596
train gradient:  0.14171952587783387
iteration : 10504
train acc:  0.6953125
train loss:  0.5049986839294434
train gradient:  0.1334120712178664
iteration : 10505
train acc:  0.796875
train loss:  0.45234066247940063
train gradient:  0.1351635139949579
iteration : 10506
train acc:  0.7578125
train loss:  0.4857636094093323
train gradient:  0.16613253582240042
iteration : 10507
train acc:  0.7265625
train loss:  0.476243793964386
train gradient:  0.14053820908904738
iteration : 10508
train acc:  0.7578125
train loss:  0.5142096281051636
train gradient:  0.15559902686943908
iteration : 10509
train acc:  0.796875
train loss:  0.4316747188568115
train gradient:  0.10185753678590355
iteration : 10510
train acc:  0.78125
train loss:  0.43241289258003235
train gradient:  0.10349209724813421
iteration : 10511
train acc:  0.6953125
train loss:  0.5358602404594421
train gradient:  0.14665152895938577
iteration : 10512
train acc:  0.765625
train loss:  0.46227172017097473
train gradient:  0.1420805696214094
iteration : 10513
train acc:  0.703125
train loss:  0.5286471843719482
train gradient:  0.13090865943527463
iteration : 10514
train acc:  0.8203125
train loss:  0.4527507424354553
train gradient:  0.14634215145364313
iteration : 10515
train acc:  0.703125
train loss:  0.5480443239212036
train gradient:  0.16398577866603162
iteration : 10516
train acc:  0.7265625
train loss:  0.5478084087371826
train gradient:  0.15814828844539103
iteration : 10517
train acc:  0.71875
train loss:  0.5581560730934143
train gradient:  0.1354614031940527
iteration : 10518
train acc:  0.734375
train loss:  0.5506154894828796
train gradient:  0.19126093982126877
iteration : 10519
train acc:  0.7421875
train loss:  0.4797865152359009
train gradient:  0.12227688608178237
iteration : 10520
train acc:  0.7890625
train loss:  0.5021191239356995
train gradient:  0.18464112207928102
iteration : 10521
train acc:  0.7578125
train loss:  0.4799754321575165
train gradient:  0.11704341184870658
iteration : 10522
train acc:  0.703125
train loss:  0.5118523836135864
train gradient:  0.13627315805711565
iteration : 10523
train acc:  0.7578125
train loss:  0.5297303795814514
train gradient:  0.1492734959395223
iteration : 10524
train acc:  0.7734375
train loss:  0.4395391345024109
train gradient:  0.102535169751244
iteration : 10525
train acc:  0.7578125
train loss:  0.45001062750816345
train gradient:  0.09245718571629673
iteration : 10526
train acc:  0.796875
train loss:  0.4478890597820282
train gradient:  0.10932292989807088
iteration : 10527
train acc:  0.703125
train loss:  0.5420628190040588
train gradient:  0.16240409653844942
iteration : 10528
train acc:  0.71875
train loss:  0.5223404765129089
train gradient:  0.14445268890389334
iteration : 10529
train acc:  0.7578125
train loss:  0.4752346873283386
train gradient:  0.11794024152675402
iteration : 10530
train acc:  0.7109375
train loss:  0.5063962340354919
train gradient:  0.1505499633051847
iteration : 10531
train acc:  0.8828125
train loss:  0.38602083921432495
train gradient:  0.09672422350687314
iteration : 10532
train acc:  0.734375
train loss:  0.4941783547401428
train gradient:  0.12244221967036807
iteration : 10533
train acc:  0.78125
train loss:  0.43664902448654175
train gradient:  0.09545449983775105
iteration : 10534
train acc:  0.65625
train loss:  0.5576148629188538
train gradient:  0.231989808166023
iteration : 10535
train acc:  0.7578125
train loss:  0.49695783853530884
train gradient:  0.14382352322093928
iteration : 10536
train acc:  0.71875
train loss:  0.4854186475276947
train gradient:  0.11029093743072874
iteration : 10537
train acc:  0.734375
train loss:  0.5158711671829224
train gradient:  0.14395847565635372
iteration : 10538
train acc:  0.6953125
train loss:  0.5500519871711731
train gradient:  0.14666114360604213
iteration : 10539
train acc:  0.71875
train loss:  0.5070129632949829
train gradient:  0.15847236441927073
iteration : 10540
train acc:  0.6875
train loss:  0.55615234375
train gradient:  0.16545218623465746
iteration : 10541
train acc:  0.7109375
train loss:  0.5344246029853821
train gradient:  0.15735551920080262
iteration : 10542
train acc:  0.7109375
train loss:  0.49664613604545593
train gradient:  0.1201274085874192
iteration : 10543
train acc:  0.7109375
train loss:  0.5216687321662903
train gradient:  0.19019881396818722
iteration : 10544
train acc:  0.7421875
train loss:  0.5101197361946106
train gradient:  0.1697802346910387
iteration : 10545
train acc:  0.6875
train loss:  0.5262245535850525
train gradient:  0.15962214537481473
iteration : 10546
train acc:  0.78125
train loss:  0.4665796756744385
train gradient:  0.13401059372110022
iteration : 10547
train acc:  0.78125
train loss:  0.4456537961959839
train gradient:  0.10502513263184865
iteration : 10548
train acc:  0.7265625
train loss:  0.5341407060623169
train gradient:  0.1562490595853818
iteration : 10549
train acc:  0.71875
train loss:  0.5240916013717651
train gradient:  0.14582618514903173
iteration : 10550
train acc:  0.8359375
train loss:  0.40295106172561646
train gradient:  0.08953452546780653
iteration : 10551
train acc:  0.7578125
train loss:  0.4744413197040558
train gradient:  0.17996799191250956
iteration : 10552
train acc:  0.671875
train loss:  0.6327934861183167
train gradient:  0.18813194589429405
iteration : 10553
train acc:  0.703125
train loss:  0.5285191535949707
train gradient:  0.13450984975382763
iteration : 10554
train acc:  0.734375
train loss:  0.4847858250141144
train gradient:  0.10977965228802285
iteration : 10555
train acc:  0.7421875
train loss:  0.5016515254974365
train gradient:  0.14030046391007422
iteration : 10556
train acc:  0.75
train loss:  0.5429539680480957
train gradient:  0.16354036550524054
iteration : 10557
train acc:  0.765625
train loss:  0.5058944225311279
train gradient:  0.16906203655143703
iteration : 10558
train acc:  0.7578125
train loss:  0.4658823311328888
train gradient:  0.12254281904451242
iteration : 10559
train acc:  0.75
train loss:  0.44395214319229126
train gradient:  0.10064202808230754
iteration : 10560
train acc:  0.8125
train loss:  0.4607275128364563
train gradient:  0.1221477303440601
iteration : 10561
train acc:  0.8046875
train loss:  0.43646112084388733
train gradient:  0.10959139365727087
iteration : 10562
train acc:  0.7421875
train loss:  0.4484928846359253
train gradient:  0.10051570006387194
iteration : 10563
train acc:  0.765625
train loss:  0.4730075001716614
train gradient:  0.12925678686928616
iteration : 10564
train acc:  0.7109375
train loss:  0.48866260051727295
train gradient:  0.13391317042241419
iteration : 10565
train acc:  0.703125
train loss:  0.524177074432373
train gradient:  0.12591225054576638
iteration : 10566
train acc:  0.7265625
train loss:  0.5324150323867798
train gradient:  0.2017092277849259
iteration : 10567
train acc:  0.6796875
train loss:  0.5617046356201172
train gradient:  0.12463502117811374
iteration : 10568
train acc:  0.7578125
train loss:  0.5052851438522339
train gradient:  0.12680523095631474
iteration : 10569
train acc:  0.7890625
train loss:  0.4615612030029297
train gradient:  0.10515909382331712
iteration : 10570
train acc:  0.7890625
train loss:  0.4469020366668701
train gradient:  0.10394839090771131
iteration : 10571
train acc:  0.7109375
train loss:  0.5605990886688232
train gradient:  0.1586342630059665
iteration : 10572
train acc:  0.734375
train loss:  0.4680593013763428
train gradient:  0.10476466761438218
iteration : 10573
train acc:  0.671875
train loss:  0.5487958192825317
train gradient:  0.14149567802124846
iteration : 10574
train acc:  0.8359375
train loss:  0.42885297536849976
train gradient:  0.13042397460691574
iteration : 10575
train acc:  0.671875
train loss:  0.5996779203414917
train gradient:  0.17325593117021953
iteration : 10576
train acc:  0.7578125
train loss:  0.4500507712364197
train gradient:  0.11165481450861935
iteration : 10577
train acc:  0.7265625
train loss:  0.4824333190917969
train gradient:  0.1359389369445359
iteration : 10578
train acc:  0.765625
train loss:  0.4441702961921692
train gradient:  0.12598288005754094
iteration : 10579
train acc:  0.7578125
train loss:  0.4972819685935974
train gradient:  0.11434890213061477
iteration : 10580
train acc:  0.6953125
train loss:  0.517004132270813
train gradient:  0.138259405280182
iteration : 10581
train acc:  0.6875
train loss:  0.5297582745552063
train gradient:  0.14721011072160375
iteration : 10582
train acc:  0.671875
train loss:  0.5223367214202881
train gradient:  0.12163789010234424
iteration : 10583
train acc:  0.7578125
train loss:  0.4883870482444763
train gradient:  0.1519919815538478
iteration : 10584
train acc:  0.765625
train loss:  0.48966333270072937
train gradient:  0.1364431150858404
iteration : 10585
train acc:  0.7578125
train loss:  0.44862788915634155
train gradient:  0.13381804912367684
iteration : 10586
train acc:  0.7265625
train loss:  0.4975454807281494
train gradient:  0.12063278147106156
iteration : 10587
train acc:  0.734375
train loss:  0.4609307050704956
train gradient:  0.104405759786775
iteration : 10588
train acc:  0.703125
train loss:  0.5405042171478271
train gradient:  0.1305299070039049
iteration : 10589
train acc:  0.71875
train loss:  0.5014870166778564
train gradient:  0.1438272385991644
iteration : 10590
train acc:  0.859375
train loss:  0.3828246593475342
train gradient:  0.10069788916969354
iteration : 10591
train acc:  0.75
train loss:  0.46891528367996216
train gradient:  0.1496068061045806
iteration : 10592
train acc:  0.71875
train loss:  0.5040322542190552
train gradient:  0.13846479939102213
iteration : 10593
train acc:  0.796875
train loss:  0.4339636564254761
train gradient:  0.11307501996728224
iteration : 10594
train acc:  0.7421875
train loss:  0.4999051094055176
train gradient:  0.15943286254887243
iteration : 10595
train acc:  0.75
train loss:  0.47850120067596436
train gradient:  0.11812470456673786
iteration : 10596
train acc:  0.7265625
train loss:  0.5635195970535278
train gradient:  0.1998046202633138
iteration : 10597
train acc:  0.7109375
train loss:  0.49839672446250916
train gradient:  0.12481192540234753
iteration : 10598
train acc:  0.7109375
train loss:  0.5018187761306763
train gradient:  0.14838720121342375
iteration : 10599
train acc:  0.78125
train loss:  0.4723935127258301
train gradient:  0.10909090141481469
iteration : 10600
train acc:  0.734375
train loss:  0.5107201933860779
train gradient:  0.15822680884116214
iteration : 10601
train acc:  0.703125
train loss:  0.4889994263648987
train gradient:  0.12153458174364568
iteration : 10602
train acc:  0.6796875
train loss:  0.5295119285583496
train gradient:  0.16120297908649164
iteration : 10603
train acc:  0.8125
train loss:  0.4351140856742859
train gradient:  0.13926237012550524
iteration : 10604
train acc:  0.8125
train loss:  0.45119163393974304
train gradient:  0.1221041428224415
iteration : 10605
train acc:  0.734375
train loss:  0.5450767278671265
train gradient:  0.1502437265757915
iteration : 10606
train acc:  0.734375
train loss:  0.5740379691123962
train gradient:  0.16971176437392604
iteration : 10607
train acc:  0.7578125
train loss:  0.46560513973236084
train gradient:  0.162133702147081
iteration : 10608
train acc:  0.7578125
train loss:  0.4889814853668213
train gradient:  0.12220371820556881
iteration : 10609
train acc:  0.7578125
train loss:  0.47217583656311035
train gradient:  0.1313870786821302
iteration : 10610
train acc:  0.6875
train loss:  0.5777740478515625
train gradient:  0.20327217283075116
iteration : 10611
train acc:  0.7265625
train loss:  0.5177348852157593
train gradient:  0.11787283860139178
iteration : 10612
train acc:  0.7109375
train loss:  0.5333093404769897
train gradient:  0.17606575893988624
iteration : 10613
train acc:  0.765625
train loss:  0.47202324867248535
train gradient:  0.14318683030828983
iteration : 10614
train acc:  0.7421875
train loss:  0.49856582283973694
train gradient:  0.14617525758106237
iteration : 10615
train acc:  0.703125
train loss:  0.5111914277076721
train gradient:  0.12038642600865283
iteration : 10616
train acc:  0.6953125
train loss:  0.5608795881271362
train gradient:  0.1752227507626446
iteration : 10617
train acc:  0.6640625
train loss:  0.544581949710846
train gradient:  0.1739564944856754
iteration : 10618
train acc:  0.78125
train loss:  0.47520726919174194
train gradient:  0.18423466642160152
iteration : 10619
train acc:  0.7265625
train loss:  0.5436704754829407
train gradient:  0.14986597150307956
iteration : 10620
train acc:  0.7734375
train loss:  0.4873911440372467
train gradient:  0.13523294700657912
iteration : 10621
train acc:  0.7421875
train loss:  0.47621428966522217
train gradient:  0.11047332463482776
iteration : 10622
train acc:  0.7265625
train loss:  0.48859530687332153
train gradient:  0.1377494465400546
iteration : 10623
train acc:  0.75
train loss:  0.49722591042518616
train gradient:  0.13527282864219398
iteration : 10624
train acc:  0.7578125
train loss:  0.5664425492286682
train gradient:  0.1780239727374511
iteration : 10625
train acc:  0.7421875
train loss:  0.4951433837413788
train gradient:  0.12400939359349848
iteration : 10626
train acc:  0.7578125
train loss:  0.4630354940891266
train gradient:  0.1380669343216777
iteration : 10627
train acc:  0.6484375
train loss:  0.6042073369026184
train gradient:  0.17393061944150806
iteration : 10628
train acc:  0.6640625
train loss:  0.5614577531814575
train gradient:  0.18559723591854624
iteration : 10629
train acc:  0.7421875
train loss:  0.47510042786598206
train gradient:  0.1117125607741035
iteration : 10630
train acc:  0.7578125
train loss:  0.4517272114753723
train gradient:  0.09240595701202314
iteration : 10631
train acc:  0.703125
train loss:  0.5142707824707031
train gradient:  0.13881246984630846
iteration : 10632
train acc:  0.7578125
train loss:  0.4824675917625427
train gradient:  0.14943366309290734
iteration : 10633
train acc:  0.7890625
train loss:  0.4428918957710266
train gradient:  0.12008593709725938
iteration : 10634
train acc:  0.671875
train loss:  0.5473339557647705
train gradient:  0.17306245040618812
iteration : 10635
train acc:  0.6171875
train loss:  0.6385725736618042
train gradient:  0.2555539581331273
iteration : 10636
train acc:  0.7109375
train loss:  0.562882661819458
train gradient:  0.17253787090264452
iteration : 10637
train acc:  0.78125
train loss:  0.4398645758628845
train gradient:  0.12438145608108059
iteration : 10638
train acc:  0.6875
train loss:  0.577593207359314
train gradient:  0.17933251425235008
iteration : 10639
train acc:  0.71875
train loss:  0.49898678064346313
train gradient:  0.19106426561580858
iteration : 10640
train acc:  0.78125
train loss:  0.4347885847091675
train gradient:  0.12693111468516208
iteration : 10641
train acc:  0.7734375
train loss:  0.5079247951507568
train gradient:  0.14976753231132334
iteration : 10642
train acc:  0.796875
train loss:  0.450271338224411
train gradient:  0.11801545896772597
iteration : 10643
train acc:  0.765625
train loss:  0.4705983102321625
train gradient:  0.09773801011239464
iteration : 10644
train acc:  0.78125
train loss:  0.4501884877681732
train gradient:  0.10966693234581479
iteration : 10645
train acc:  0.796875
train loss:  0.4841456413269043
train gradient:  0.1288680145468637
iteration : 10646
train acc:  0.71875
train loss:  0.5239834189414978
train gradient:  0.13884946770347828
iteration : 10647
train acc:  0.7109375
train loss:  0.5106837153434753
train gradient:  0.13080442836116427
iteration : 10648
train acc:  0.7265625
train loss:  0.5382872819900513
train gradient:  0.1936459739257168
iteration : 10649
train acc:  0.7109375
train loss:  0.4911862313747406
train gradient:  0.11914679264942833
iteration : 10650
train acc:  0.8125
train loss:  0.454196035861969
train gradient:  0.10179358024487996
iteration : 10651
train acc:  0.8046875
train loss:  0.4651796221733093
train gradient:  0.14276441347525531
iteration : 10652
train acc:  0.71875
train loss:  0.4948456883430481
train gradient:  0.12423725103475346
iteration : 10653
train acc:  0.71875
train loss:  0.4957089424133301
train gradient:  0.14152024767448923
iteration : 10654
train acc:  0.6484375
train loss:  0.5549733638763428
train gradient:  0.15786618185342416
iteration : 10655
train acc:  0.640625
train loss:  0.5382843017578125
train gradient:  0.1580277217879762
iteration : 10656
train acc:  0.796875
train loss:  0.4184804856777191
train gradient:  0.10987178826382281
iteration : 10657
train acc:  0.7265625
train loss:  0.5104506611824036
train gradient:  0.13174637587609628
iteration : 10658
train acc:  0.765625
train loss:  0.4286928176879883
train gradient:  0.09506631645874525
iteration : 10659
train acc:  0.78125
train loss:  0.47737544775009155
train gradient:  0.12490685808669404
iteration : 10660
train acc:  0.765625
train loss:  0.47031623125076294
train gradient:  0.11582286194945414
iteration : 10661
train acc:  0.7734375
train loss:  0.45919907093048096
train gradient:  0.10444369522727907
iteration : 10662
train acc:  0.7265625
train loss:  0.5254790782928467
train gradient:  0.15229225297934446
iteration : 10663
train acc:  0.765625
train loss:  0.4794052541255951
train gradient:  0.11711714305130044
iteration : 10664
train acc:  0.78125
train loss:  0.45931851863861084
train gradient:  0.10545191247510334
iteration : 10665
train acc:  0.7421875
train loss:  0.5062041282653809
train gradient:  0.1356986418032686
iteration : 10666
train acc:  0.71875
train loss:  0.48903346061706543
train gradient:  0.1160996467435466
iteration : 10667
train acc:  0.71875
train loss:  0.5034916400909424
train gradient:  0.12020390909948975
iteration : 10668
train acc:  0.71875
train loss:  0.5186432600021362
train gradient:  0.15674465547540106
iteration : 10669
train acc:  0.7578125
train loss:  0.48437759280204773
train gradient:  0.117568256195911
iteration : 10670
train acc:  0.75
train loss:  0.4440447986125946
train gradient:  0.10165596652996989
iteration : 10671
train acc:  0.71875
train loss:  0.514324963092804
train gradient:  0.12600025705308027
iteration : 10672
train acc:  0.6640625
train loss:  0.5423130989074707
train gradient:  0.15290730954783788
iteration : 10673
train acc:  0.7265625
train loss:  0.5414566397666931
train gradient:  0.13779950564180554
iteration : 10674
train acc:  0.796875
train loss:  0.428514689207077
train gradient:  0.10753893015473696
iteration : 10675
train acc:  0.7578125
train loss:  0.5069559216499329
train gradient:  0.12222143147235763
iteration : 10676
train acc:  0.7734375
train loss:  0.4654233455657959
train gradient:  0.14491871107123638
iteration : 10677
train acc:  0.8203125
train loss:  0.40046226978302
train gradient:  0.09116426467054188
iteration : 10678
train acc:  0.7421875
train loss:  0.49215421080589294
train gradient:  0.15636488423027786
iteration : 10679
train acc:  0.7578125
train loss:  0.5221887230873108
train gradient:  0.13995012774064083
iteration : 10680
train acc:  0.78125
train loss:  0.46885573863983154
train gradient:  0.12516489923307655
iteration : 10681
train acc:  0.78125
train loss:  0.4621013402938843
train gradient:  0.1077236271127433
iteration : 10682
train acc:  0.7734375
train loss:  0.47804856300354004
train gradient:  0.12037915778556149
iteration : 10683
train acc:  0.7109375
train loss:  0.501238226890564
train gradient:  0.11148796623008685
iteration : 10684
train acc:  0.71875
train loss:  0.4819674491882324
train gradient:  0.12049621912368781
iteration : 10685
train acc:  0.8203125
train loss:  0.4317944645881653
train gradient:  0.07383304423218644
iteration : 10686
train acc:  0.65625
train loss:  0.6119617819786072
train gradient:  0.18976519919501966
iteration : 10687
train acc:  0.7265625
train loss:  0.4810968041419983
train gradient:  0.13006495635179677
iteration : 10688
train acc:  0.8125
train loss:  0.40831708908081055
train gradient:  0.1101902616577565
iteration : 10689
train acc:  0.75
train loss:  0.47944962978363037
train gradient:  0.14664656517624808
iteration : 10690
train acc:  0.703125
train loss:  0.5576021671295166
train gradient:  0.16904968860533182
iteration : 10691
train acc:  0.6640625
train loss:  0.541039228439331
train gradient:  0.12521519540132747
iteration : 10692
train acc:  0.7109375
train loss:  0.5831945538520813
train gradient:  0.1703483636068956
iteration : 10693
train acc:  0.8125
train loss:  0.44278138875961304
train gradient:  0.10357112303288034
iteration : 10694
train acc:  0.7578125
train loss:  0.4718766212463379
train gradient:  0.12118934748036266
iteration : 10695
train acc:  0.7421875
train loss:  0.5201674103736877
train gradient:  0.20821205163008397
iteration : 10696
train acc:  0.765625
train loss:  0.4979248046875
train gradient:  0.11660143433005865
iteration : 10697
train acc:  0.734375
train loss:  0.5163782835006714
train gradient:  0.13490446810933704
iteration : 10698
train acc:  0.734375
train loss:  0.5110948085784912
train gradient:  0.1518190932198177
iteration : 10699
train acc:  0.6484375
train loss:  0.6012542247772217
train gradient:  0.2056834058506123
iteration : 10700
train acc:  0.7421875
train loss:  0.47230732440948486
train gradient:  0.11874807136532216
iteration : 10701
train acc:  0.7734375
train loss:  0.4827743470668793
train gradient:  0.09720730205378746
iteration : 10702
train acc:  0.8046875
train loss:  0.4143356382846832
train gradient:  0.07856600769106156
iteration : 10703
train acc:  0.75
train loss:  0.5088385343551636
train gradient:  0.16967091379019378
iteration : 10704
train acc:  0.7421875
train loss:  0.5302058458328247
train gradient:  0.15589847131049692
iteration : 10705
train acc:  0.7109375
train loss:  0.4748469591140747
train gradient:  0.11024134912990338
iteration : 10706
train acc:  0.7421875
train loss:  0.49598848819732666
train gradient:  0.12537947280001888
iteration : 10707
train acc:  0.78125
train loss:  0.5172351598739624
train gradient:  0.13789743315458514
iteration : 10708
train acc:  0.6796875
train loss:  0.5376274585723877
train gradient:  0.15589606222879504
iteration : 10709
train acc:  0.7265625
train loss:  0.5357835292816162
train gradient:  0.15358520167103956
iteration : 10710
train acc:  0.7265625
train loss:  0.5020180344581604
train gradient:  0.11579455994495058
iteration : 10711
train acc:  0.7890625
train loss:  0.4363389313220978
train gradient:  0.147716148466029
iteration : 10712
train acc:  0.8125
train loss:  0.43673571944236755
train gradient:  0.12206967546071534
iteration : 10713
train acc:  0.734375
train loss:  0.5168297290802002
train gradient:  0.13120136750149874
iteration : 10714
train acc:  0.703125
train loss:  0.5360683798789978
train gradient:  0.1559024708608494
iteration : 10715
train acc:  0.7578125
train loss:  0.5157651901245117
train gradient:  0.17808278803464653
iteration : 10716
train acc:  0.7265625
train loss:  0.5270322561264038
train gradient:  0.14122809993257063
iteration : 10717
train acc:  0.7109375
train loss:  0.477984756231308
train gradient:  0.12612652195894491
iteration : 10718
train acc:  0.703125
train loss:  0.5766395330429077
train gradient:  0.21576344961646843
iteration : 10719
train acc:  0.71875
train loss:  0.49188756942749023
train gradient:  0.15850890797997638
iteration : 10720
train acc:  0.7109375
train loss:  0.49695858359336853
train gradient:  0.10462725659687079
iteration : 10721
train acc:  0.7109375
train loss:  0.5270758867263794
train gradient:  0.15701489711130123
iteration : 10722
train acc:  0.765625
train loss:  0.5258382558822632
train gradient:  0.16056871214728602
iteration : 10723
train acc:  0.7265625
train loss:  0.4898649752140045
train gradient:  0.11397886388702316
iteration : 10724
train acc:  0.734375
train loss:  0.4755067825317383
train gradient:  0.13115535857564098
iteration : 10725
train acc:  0.75
train loss:  0.4798928499221802
train gradient:  0.10878096667693453
iteration : 10726
train acc:  0.7734375
train loss:  0.4696378707885742
train gradient:  0.11552842083243696
iteration : 10727
train acc:  0.703125
train loss:  0.4656869173049927
train gradient:  0.12808832173835746
iteration : 10728
train acc:  0.7265625
train loss:  0.4907289445400238
train gradient:  0.13393328535279503
iteration : 10729
train acc:  0.734375
train loss:  0.5207666158676147
train gradient:  0.14375838745068947
iteration : 10730
train acc:  0.7734375
train loss:  0.5207040309906006
train gradient:  0.1612164441607103
iteration : 10731
train acc:  0.7421875
train loss:  0.4812880754470825
train gradient:  0.12378630817279396
iteration : 10732
train acc:  0.7734375
train loss:  0.5073448419570923
train gradient:  0.14011384405553595
iteration : 10733
train acc:  0.671875
train loss:  0.5696375370025635
train gradient:  0.16242354084163668
iteration : 10734
train acc:  0.7578125
train loss:  0.4588015079498291
train gradient:  0.12206391571717018
iteration : 10735
train acc:  0.7265625
train loss:  0.4755570590496063
train gradient:  0.133103757647723
iteration : 10736
train acc:  0.7890625
train loss:  0.4203992486000061
train gradient:  0.08272261111056381
iteration : 10737
train acc:  0.7421875
train loss:  0.5106842517852783
train gradient:  0.181688105212705
iteration : 10738
train acc:  0.734375
train loss:  0.5272477865219116
train gradient:  0.15334838726057176
iteration : 10739
train acc:  0.7109375
train loss:  0.512935996055603
train gradient:  0.1458443096849429
iteration : 10740
train acc:  0.78125
train loss:  0.43742966651916504
train gradient:  0.09487145546729787
iteration : 10741
train acc:  0.7109375
train loss:  0.5156224370002747
train gradient:  0.11978633915816017
iteration : 10742
train acc:  0.6953125
train loss:  0.5255601406097412
train gradient:  0.16213981068325134
iteration : 10743
train acc:  0.765625
train loss:  0.5383884310722351
train gradient:  0.14201155227727072
iteration : 10744
train acc:  0.7734375
train loss:  0.49629801511764526
train gradient:  0.11798978377358546
iteration : 10745
train acc:  0.734375
train loss:  0.5393034815788269
train gradient:  0.12900046408408317
iteration : 10746
train acc:  0.7421875
train loss:  0.517805814743042
train gradient:  0.13559762839794953
iteration : 10747
train acc:  0.71875
train loss:  0.5491713881492615
train gradient:  0.15142002599622278
iteration : 10748
train acc:  0.7109375
train loss:  0.5052154660224915
train gradient:  0.13614442419027933
iteration : 10749
train acc:  0.7265625
train loss:  0.4920543432235718
train gradient:  0.11271389632958057
iteration : 10750
train acc:  0.765625
train loss:  0.47614097595214844
train gradient:  0.12499500118855719
iteration : 10751
train acc:  0.6875
train loss:  0.5500481128692627
train gradient:  0.17543579801173625
iteration : 10752
train acc:  0.6953125
train loss:  0.5622872114181519
train gradient:  0.16260799768216433
iteration : 10753
train acc:  0.703125
train loss:  0.5747511982917786
train gradient:  0.1782739471665548
iteration : 10754
train acc:  0.765625
train loss:  0.46631914377212524
train gradient:  0.11142375078502803
iteration : 10755
train acc:  0.71875
train loss:  0.5307902097702026
train gradient:  0.17026022192619902
iteration : 10756
train acc:  0.765625
train loss:  0.460704505443573
train gradient:  0.09722185067537348
iteration : 10757
train acc:  0.796875
train loss:  0.44998857378959656
train gradient:  0.10600466740785583
iteration : 10758
train acc:  0.6953125
train loss:  0.5238714814186096
train gradient:  0.14189284693892573
iteration : 10759
train acc:  0.765625
train loss:  0.48232582211494446
train gradient:  0.1423366965798027
iteration : 10760
train acc:  0.6875
train loss:  0.5382812023162842
train gradient:  0.1323409940474054
iteration : 10761
train acc:  0.78125
train loss:  0.42976945638656616
train gradient:  0.10980149971822835
iteration : 10762
train acc:  0.7265625
train loss:  0.5245780944824219
train gradient:  0.15287976791041819
iteration : 10763
train acc:  0.7421875
train loss:  0.4589838981628418
train gradient:  0.10618632091049038
iteration : 10764
train acc:  0.7109375
train loss:  0.5188565254211426
train gradient:  0.1532493992815171
iteration : 10765
train acc:  0.8125
train loss:  0.4209151268005371
train gradient:  0.09851042730107729
iteration : 10766
train acc:  0.71875
train loss:  0.564802885055542
train gradient:  0.14977870250466616
iteration : 10767
train acc:  0.7578125
train loss:  0.4740305244922638
train gradient:  0.11649958553176136
iteration : 10768
train acc:  0.7578125
train loss:  0.5033047795295715
train gradient:  0.1853982080293246
iteration : 10769
train acc:  0.75
train loss:  0.48854565620422363
train gradient:  0.11826254727682194
iteration : 10770
train acc:  0.796875
train loss:  0.461928129196167
train gradient:  0.1301364062455171
iteration : 10771
train acc:  0.7109375
train loss:  0.5632094144821167
train gradient:  0.17673560617916728
iteration : 10772
train acc:  0.7265625
train loss:  0.5263011455535889
train gradient:  0.157926378834344
iteration : 10773
train acc:  0.7109375
train loss:  0.5766998529434204
train gradient:  0.1961052080623193
iteration : 10774
train acc:  0.84375
train loss:  0.47244107723236084
train gradient:  0.12460751198033389
iteration : 10775
train acc:  0.765625
train loss:  0.4916985332965851
train gradient:  0.18566261177651475
iteration : 10776
train acc:  0.765625
train loss:  0.4723755717277527
train gradient:  0.11386557495085048
iteration : 10777
train acc:  0.8046875
train loss:  0.4444040060043335
train gradient:  0.1395015784357184
iteration : 10778
train acc:  0.7578125
train loss:  0.46973466873168945
train gradient:  0.12168333633972021
iteration : 10779
train acc:  0.7890625
train loss:  0.40790045261383057
train gradient:  0.10571777178299555
iteration : 10780
train acc:  0.703125
train loss:  0.5326269268989563
train gradient:  0.16558698300875502
iteration : 10781
train acc:  0.7578125
train loss:  0.4587133526802063
train gradient:  0.11075461213403526
iteration : 10782
train acc:  0.703125
train loss:  0.503849446773529
train gradient:  0.1528416402943243
iteration : 10783
train acc:  0.75
train loss:  0.503951907157898
train gradient:  0.16955264829279412
iteration : 10784
train acc:  0.7421875
train loss:  0.42760154604911804
train gradient:  0.10448887378989569
iteration : 10785
train acc:  0.734375
train loss:  0.5249273777008057
train gradient:  0.12953212753006826
iteration : 10786
train acc:  0.7578125
train loss:  0.5182040929794312
train gradient:  0.16475697848980325
iteration : 10787
train acc:  0.71875
train loss:  0.5239957571029663
train gradient:  0.13266303519318368
iteration : 10788
train acc:  0.765625
train loss:  0.46268340945243835
train gradient:  0.11720070785591909
iteration : 10789
train acc:  0.765625
train loss:  0.45053359866142273
train gradient:  0.10825405101638721
iteration : 10790
train acc:  0.7109375
train loss:  0.5500397682189941
train gradient:  0.14984337534185477
iteration : 10791
train acc:  0.7890625
train loss:  0.4944653809070587
train gradient:  0.15378011120127155
iteration : 10792
train acc:  0.765625
train loss:  0.4778812527656555
train gradient:  0.1531669388540587
iteration : 10793
train acc:  0.8203125
train loss:  0.4827158451080322
train gradient:  0.10735163986876771
iteration : 10794
train acc:  0.671875
train loss:  0.5916815400123596
train gradient:  0.1763729956682425
iteration : 10795
train acc:  0.7734375
train loss:  0.46190187335014343
train gradient:  0.11173844737259915
iteration : 10796
train acc:  0.75
train loss:  0.477553129196167
train gradient:  0.1442243297879858
iteration : 10797
train acc:  0.71875
train loss:  0.49208083748817444
train gradient:  0.16021133090317285
iteration : 10798
train acc:  0.7890625
train loss:  0.49757203459739685
train gradient:  0.20473775922492146
iteration : 10799
train acc:  0.7578125
train loss:  0.4964010715484619
train gradient:  0.1186647727885954
iteration : 10800
train acc:  0.7734375
train loss:  0.4330626130104065
train gradient:  0.11426362861679018
iteration : 10801
train acc:  0.765625
train loss:  0.49948176741600037
train gradient:  0.11720515777827248
iteration : 10802
train acc:  0.7265625
train loss:  0.46935343742370605
train gradient:  0.1242408586710357
iteration : 10803
train acc:  0.703125
train loss:  0.5435101389884949
train gradient:  0.1979980735869166
iteration : 10804
train acc:  0.7421875
train loss:  0.5393702387809753
train gradient:  0.13807229137909027
iteration : 10805
train acc:  0.7109375
train loss:  0.48965632915496826
train gradient:  0.13305890531782105
iteration : 10806
train acc:  0.7265625
train loss:  0.4908461272716522
train gradient:  0.17243849961640692
iteration : 10807
train acc:  0.7421875
train loss:  0.5125097036361694
train gradient:  0.12112460225441114
iteration : 10808
train acc:  0.7421875
train loss:  0.517650842666626
train gradient:  0.14334842874117884
iteration : 10809
train acc:  0.75
train loss:  0.4603247046470642
train gradient:  0.09692136761376734
iteration : 10810
train acc:  0.7109375
train loss:  0.5437372922897339
train gradient:  0.14242595907215905
iteration : 10811
train acc:  0.78125
train loss:  0.46003860235214233
train gradient:  0.11112367330590031
iteration : 10812
train acc:  0.7890625
train loss:  0.4751889407634735
train gradient:  0.11171279126774988
iteration : 10813
train acc:  0.75
train loss:  0.4894868731498718
train gradient:  0.12095680132361128
iteration : 10814
train acc:  0.7890625
train loss:  0.47316887974739075
train gradient:  0.15741168719149592
iteration : 10815
train acc:  0.7890625
train loss:  0.4888337254524231
train gradient:  0.19429313776870316
iteration : 10816
train acc:  0.7109375
train loss:  0.5048949718475342
train gradient:  0.14469885210047262
iteration : 10817
train acc:  0.703125
train loss:  0.5626020431518555
train gradient:  0.1653409886750039
iteration : 10818
train acc:  0.6875
train loss:  0.6105912923812866
train gradient:  0.2072303152161517
iteration : 10819
train acc:  0.765625
train loss:  0.48743224143981934
train gradient:  0.14862144543551437
iteration : 10820
train acc:  0.6875
train loss:  0.5622110366821289
train gradient:  0.1834984571243241
iteration : 10821
train acc:  0.7734375
train loss:  0.442341148853302
train gradient:  0.11353151411008523
iteration : 10822
train acc:  0.71875
train loss:  0.4675086736679077
train gradient:  0.10953769412760002
iteration : 10823
train acc:  0.7734375
train loss:  0.4866163432598114
train gradient:  0.14819806043361616
iteration : 10824
train acc:  0.765625
train loss:  0.4482974708080292
train gradient:  0.09203460531187228
iteration : 10825
train acc:  0.6796875
train loss:  0.5336454510688782
train gradient:  0.1488242621848782
iteration : 10826
train acc:  0.734375
train loss:  0.5083970427513123
train gradient:  0.1295662417598323
iteration : 10827
train acc:  0.6796875
train loss:  0.5711064338684082
train gradient:  0.21212002636551272
iteration : 10828
train acc:  0.75
train loss:  0.5133121609687805
train gradient:  0.17187863297732975
iteration : 10829
train acc:  0.734375
train loss:  0.5046571493148804
train gradient:  0.19023499574066569
iteration : 10830
train acc:  0.7734375
train loss:  0.4652862846851349
train gradient:  0.1351591786613464
iteration : 10831
train acc:  0.75
train loss:  0.5558038949966431
train gradient:  0.1683524753671521
iteration : 10832
train acc:  0.71875
train loss:  0.5232025384902954
train gradient:  0.17054861560625567
iteration : 10833
train acc:  0.7734375
train loss:  0.4161730408668518
train gradient:  0.10470676929873587
iteration : 10834
train acc:  0.7890625
train loss:  0.47774821519851685
train gradient:  0.12440332714792363
iteration : 10835
train acc:  0.796875
train loss:  0.42425304651260376
train gradient:  0.0998400700175057
iteration : 10836
train acc:  0.71875
train loss:  0.5391501188278198
train gradient:  0.16147542461580247
iteration : 10837
train acc:  0.7265625
train loss:  0.5463550686836243
train gradient:  0.17131135547735699
iteration : 10838
train acc:  0.7890625
train loss:  0.4669768512248993
train gradient:  0.14902650845669868
iteration : 10839
train acc:  0.765625
train loss:  0.491627037525177
train gradient:  0.1377498196331871
iteration : 10840
train acc:  0.7578125
train loss:  0.4772738814353943
train gradient:  0.12709552243428368
iteration : 10841
train acc:  0.78125
train loss:  0.5025352835655212
train gradient:  0.14699011787819394
iteration : 10842
train acc:  0.671875
train loss:  0.5020470023155212
train gradient:  0.12102434294334949
iteration : 10843
train acc:  0.6875
train loss:  0.5472258925437927
train gradient:  0.18176790019319689
iteration : 10844
train acc:  0.78125
train loss:  0.4639827609062195
train gradient:  0.10464523699006459
iteration : 10845
train acc:  0.7265625
train loss:  0.5322465896606445
train gradient:  0.15629917975604313
iteration : 10846
train acc:  0.75
train loss:  0.4962397515773773
train gradient:  0.17083187567741887
iteration : 10847
train acc:  0.7578125
train loss:  0.48970717191696167
train gradient:  0.16599827438956338
iteration : 10848
train acc:  0.75
train loss:  0.5108359456062317
train gradient:  0.11395443550103157
iteration : 10849
train acc:  0.765625
train loss:  0.44870054721832275
train gradient:  0.10401893535427961
iteration : 10850
train acc:  0.7890625
train loss:  0.4510855972766876
train gradient:  0.1018011794950038
iteration : 10851
train acc:  0.734375
train loss:  0.4972473680973053
train gradient:  0.1219574789258675
iteration : 10852
train acc:  0.7734375
train loss:  0.4239940941333771
train gradient:  0.09866216259919197
iteration : 10853
train acc:  0.6875
train loss:  0.5163180232048035
train gradient:  0.13628569397654994
iteration : 10854
train acc:  0.7421875
train loss:  0.4599025249481201
train gradient:  0.11737440618628522
iteration : 10855
train acc:  0.6875
train loss:  0.5610687732696533
train gradient:  0.18239643830282914
iteration : 10856
train acc:  0.7421875
train loss:  0.4806385040283203
train gradient:  0.12155434972601409
iteration : 10857
train acc:  0.71875
train loss:  0.45325738191604614
train gradient:  0.10974584152378537
iteration : 10858
train acc:  0.65625
train loss:  0.5982575416564941
train gradient:  0.1795697914714245
iteration : 10859
train acc:  0.7578125
train loss:  0.4624752998352051
train gradient:  0.1023595418439056
iteration : 10860
train acc:  0.7578125
train loss:  0.47731930017471313
train gradient:  0.11697060722079908
iteration : 10861
train acc:  0.71875
train loss:  0.5580269694328308
train gradient:  0.1489749159721393
iteration : 10862
train acc:  0.7734375
train loss:  0.4507439136505127
train gradient:  0.08565397777943186
iteration : 10863
train acc:  0.8203125
train loss:  0.44394224882125854
train gradient:  0.11485874204489463
iteration : 10864
train acc:  0.7734375
train loss:  0.4615701138973236
train gradient:  0.10824384050777064
iteration : 10865
train acc:  0.7578125
train loss:  0.5015393495559692
train gradient:  0.15371302398788694
iteration : 10866
train acc:  0.6953125
train loss:  0.5501919388771057
train gradient:  0.16883586906237474
iteration : 10867
train acc:  0.71875
train loss:  0.4977176785469055
train gradient:  0.1313634461904319
iteration : 10868
train acc:  0.7578125
train loss:  0.47914475202560425
train gradient:  0.10291548731505204
iteration : 10869
train acc:  0.765625
train loss:  0.4761947691440582
train gradient:  0.1287936165629835
iteration : 10870
train acc:  0.8125
train loss:  0.4184384047985077
train gradient:  0.09111232331582772
iteration : 10871
train acc:  0.734375
train loss:  0.5010765790939331
train gradient:  0.1479033943820108
iteration : 10872
train acc:  0.75
train loss:  0.5358495116233826
train gradient:  0.1661944406954794
iteration : 10873
train acc:  0.7109375
train loss:  0.51463383436203
train gradient:  0.1546273299482424
iteration : 10874
train acc:  0.734375
train loss:  0.5574240684509277
train gradient:  0.17723758044785537
iteration : 10875
train acc:  0.7421875
train loss:  0.49558311700820923
train gradient:  0.1269264355179145
iteration : 10876
train acc:  0.7421875
train loss:  0.48239514231681824
train gradient:  0.11445108012828126
iteration : 10877
train acc:  0.7421875
train loss:  0.5145146250724792
train gradient:  0.10316873031902941
iteration : 10878
train acc:  0.6796875
train loss:  0.5541422367095947
train gradient:  0.17537592030567728
iteration : 10879
train acc:  0.703125
train loss:  0.5548518896102905
train gradient:  0.14728204076768586
iteration : 10880
train acc:  0.703125
train loss:  0.5265673398971558
train gradient:  0.14619142595995938
iteration : 10881
train acc:  0.7890625
train loss:  0.43046432733535767
train gradient:  0.11242755966236859
iteration : 10882
train acc:  0.7265625
train loss:  0.5104051828384399
train gradient:  0.13854293619298275
iteration : 10883
train acc:  0.734375
train loss:  0.46931254863739014
train gradient:  0.12002011213410156
iteration : 10884
train acc:  0.7734375
train loss:  0.46340277791023254
train gradient:  0.11139037972399395
iteration : 10885
train acc:  0.78125
train loss:  0.4982393682003021
train gradient:  0.12131261562212979
iteration : 10886
train acc:  0.7578125
train loss:  0.47467005252838135
train gradient:  0.119977469233047
iteration : 10887
train acc:  0.734375
train loss:  0.49600881338119507
train gradient:  0.14401749622880328
iteration : 10888
train acc:  0.78125
train loss:  0.46408480405807495
train gradient:  0.14728202688655284
iteration : 10889
train acc:  0.6875
train loss:  0.5468413829803467
train gradient:  0.13908241287809553
iteration : 10890
train acc:  0.7265625
train loss:  0.4813639521598816
train gradient:  0.14855807253161374
iteration : 10891
train acc:  0.75
train loss:  0.46142420172691345
train gradient:  0.1064192468592281
iteration : 10892
train acc:  0.734375
train loss:  0.5027370452880859
train gradient:  0.12473698051943558
iteration : 10893
train acc:  0.78125
train loss:  0.4509367346763611
train gradient:  0.10977903187160062
iteration : 10894
train acc:  0.7421875
train loss:  0.47567498683929443
train gradient:  0.11115115714060764
iteration : 10895
train acc:  0.8359375
train loss:  0.4255741834640503
train gradient:  0.12311037448698589
iteration : 10896
train acc:  0.875
train loss:  0.37805479764938354
train gradient:  0.10440301001802126
iteration : 10897
train acc:  0.765625
train loss:  0.46370741724967957
train gradient:  0.19845782609781115
iteration : 10898
train acc:  0.703125
train loss:  0.47637656331062317
train gradient:  0.12055253067141157
iteration : 10899
train acc:  0.7578125
train loss:  0.5179992914199829
train gradient:  0.122398254911347
iteration : 10900
train acc:  0.7578125
train loss:  0.48653271794319153
train gradient:  0.11551040923353367
iteration : 10901
train acc:  0.765625
train loss:  0.48099973797798157
train gradient:  0.15471993023884223
iteration : 10902
train acc:  0.6484375
train loss:  0.629077672958374
train gradient:  0.2556023664911301
iteration : 10903
train acc:  0.765625
train loss:  0.5380403995513916
train gradient:  0.16063894336446732
iteration : 10904
train acc:  0.7734375
train loss:  0.4456753134727478
train gradient:  0.12139447046817593
iteration : 10905
train acc:  0.6875
train loss:  0.5429219007492065
train gradient:  0.13738498610920874
iteration : 10906
train acc:  0.7109375
train loss:  0.5598110556602478
train gradient:  0.1665764415278566
iteration : 10907
train acc:  0.71875
train loss:  0.5542923212051392
train gradient:  0.20745310791524685
iteration : 10908
train acc:  0.7890625
train loss:  0.45490625500679016
train gradient:  0.0987034935903918
iteration : 10909
train acc:  0.7421875
train loss:  0.4952906668186188
train gradient:  0.15786796119446822
iteration : 10910
train acc:  0.7890625
train loss:  0.501441240310669
train gradient:  0.14763287448008705
iteration : 10911
train acc:  0.75
train loss:  0.5350095629692078
train gradient:  0.14081430468505413
iteration : 10912
train acc:  0.75
train loss:  0.4666702151298523
train gradient:  0.13107986191174117
iteration : 10913
train acc:  0.796875
train loss:  0.4816961884498596
train gradient:  0.11388019245002284
iteration : 10914
train acc:  0.796875
train loss:  0.4396167993545532
train gradient:  0.1194487446163906
iteration : 10915
train acc:  0.703125
train loss:  0.5780496597290039
train gradient:  0.15561503669477372
iteration : 10916
train acc:  0.7578125
train loss:  0.4539875388145447
train gradient:  0.12290569186038357
iteration : 10917
train acc:  0.75
train loss:  0.49749118089675903
train gradient:  0.11991168776758848
iteration : 10918
train acc:  0.6875
train loss:  0.4939040541648865
train gradient:  0.14695521023807218
iteration : 10919
train acc:  0.71875
train loss:  0.5059298276901245
train gradient:  0.11502801030614646
iteration : 10920
train acc:  0.734375
train loss:  0.5012159943580627
train gradient:  0.13307706227552457
iteration : 10921
train acc:  0.7421875
train loss:  0.4986467957496643
train gradient:  0.12191140575187792
iteration : 10922
train acc:  0.671875
train loss:  0.6027898788452148
train gradient:  0.23098329981812854
iteration : 10923
train acc:  0.765625
train loss:  0.45367783308029175
train gradient:  0.14130388546007058
iteration : 10924
train acc:  0.7265625
train loss:  0.5529471039772034
train gradient:  0.15923060797271182
iteration : 10925
train acc:  0.7890625
train loss:  0.446399986743927
train gradient:  0.11353628731368659
iteration : 10926
train acc:  0.7578125
train loss:  0.46890532970428467
train gradient:  0.0948322994425003
iteration : 10927
train acc:  0.75
train loss:  0.4927361011505127
train gradient:  0.14321499199410148
iteration : 10928
train acc:  0.75
train loss:  0.47014638781547546
train gradient:  0.12750711223090933
iteration : 10929
train acc:  0.7421875
train loss:  0.5733362436294556
train gradient:  0.15289697169121985
iteration : 10930
train acc:  0.78125
train loss:  0.41590553522109985
train gradient:  0.09269922568157644
iteration : 10931
train acc:  0.8046875
train loss:  0.4535428285598755
train gradient:  0.10193064602261027
iteration : 10932
train acc:  0.7734375
train loss:  0.41695916652679443
train gradient:  0.09650477111128003
iteration : 10933
train acc:  0.6875
train loss:  0.5297020673751831
train gradient:  0.1472820162533256
iteration : 10934
train acc:  0.734375
train loss:  0.45634642243385315
train gradient:  0.10471703770180965
iteration : 10935
train acc:  0.78125
train loss:  0.4856145679950714
train gradient:  0.13255371130539811
iteration : 10936
train acc:  0.7578125
train loss:  0.4733258783817291
train gradient:  0.13886147708270963
iteration : 10937
train acc:  0.78125
train loss:  0.4574316143989563
train gradient:  0.11867563039979397
iteration : 10938
train acc:  0.7421875
train loss:  0.5479243993759155
train gradient:  0.17838724114778504
iteration : 10939
train acc:  0.7265625
train loss:  0.48374736309051514
train gradient:  0.1348399233373132
iteration : 10940
train acc:  0.6875
train loss:  0.513620138168335
train gradient:  0.15037224554126125
iteration : 10941
train acc:  0.7578125
train loss:  0.442996084690094
train gradient:  0.0943171835143326
iteration : 10942
train acc:  0.8359375
train loss:  0.39739200472831726
train gradient:  0.09750668553262974
iteration : 10943
train acc:  0.6875
train loss:  0.5567409992218018
train gradient:  0.16593842993910363
iteration : 10944
train acc:  0.71875
train loss:  0.48079830408096313
train gradient:  0.13317184405426236
iteration : 10945
train acc:  0.7421875
train loss:  0.49932628870010376
train gradient:  0.12905006959957116
iteration : 10946
train acc:  0.6953125
train loss:  0.5314359664916992
train gradient:  0.15579396019387337
iteration : 10947
train acc:  0.6953125
train loss:  0.5700109004974365
train gradient:  0.16087416095630286
iteration : 10948
train acc:  0.6953125
train loss:  0.52135169506073
train gradient:  0.157658551923881
iteration : 10949
train acc:  0.7109375
train loss:  0.5100835561752319
train gradient:  0.12042243529919683
iteration : 10950
train acc:  0.7265625
train loss:  0.5428639650344849
train gradient:  0.19469743837276152
iteration : 10951
train acc:  0.6875
train loss:  0.5262194871902466
train gradient:  0.20274167372200708
iteration : 10952
train acc:  0.6796875
train loss:  0.571665346622467
train gradient:  0.17078300815841388
iteration : 10953
train acc:  0.75
train loss:  0.4503137469291687
train gradient:  0.11661835703510898
iteration : 10954
train acc:  0.8046875
train loss:  0.433529257774353
train gradient:  0.08564469222996883
iteration : 10955
train acc:  0.7421875
train loss:  0.4884189963340759
train gradient:  0.1352011383853505
iteration : 10956
train acc:  0.7421875
train loss:  0.4736291766166687
train gradient:  0.12016055971803666
iteration : 10957
train acc:  0.7578125
train loss:  0.49210724234580994
train gradient:  0.139621955809881
iteration : 10958
train acc:  0.7265625
train loss:  0.5210079550743103
train gradient:  0.15293579957974407
iteration : 10959
train acc:  0.7734375
train loss:  0.4473903775215149
train gradient:  0.10266052264730215
iteration : 10960
train acc:  0.7109375
train loss:  0.46432051062583923
train gradient:  0.11865675944102098
iteration : 10961
train acc:  0.7109375
train loss:  0.4678497314453125
train gradient:  0.10260867317467778
iteration : 10962
train acc:  0.71875
train loss:  0.5092025995254517
train gradient:  0.15368023622551913
iteration : 10963
train acc:  0.734375
train loss:  0.5362513065338135
train gradient:  0.1798202502810038
iteration : 10964
train acc:  0.75
train loss:  0.44178640842437744
train gradient:  0.10827481997664927
iteration : 10965
train acc:  0.8125
train loss:  0.49317505955696106
train gradient:  0.11285308391001654
iteration : 10966
train acc:  0.6953125
train loss:  0.5514653325080872
train gradient:  0.1739229007373908
iteration : 10967
train acc:  0.6875
train loss:  0.49909886717796326
train gradient:  0.13095972955064322
iteration : 10968
train acc:  0.6953125
train loss:  0.5259608626365662
train gradient:  0.12514444102420774
iteration : 10969
train acc:  0.7421875
train loss:  0.4814710021018982
train gradient:  0.136109075988317
iteration : 10970
train acc:  0.7421875
train loss:  0.5325908660888672
train gradient:  0.15449718750072503
iteration : 10971
train acc:  0.6875
train loss:  0.5089085698127747
train gradient:  0.1151571019226836
iteration : 10972
train acc:  0.75
train loss:  0.4791707694530487
train gradient:  0.13005429554711734
iteration : 10973
train acc:  0.7578125
train loss:  0.4496793746948242
train gradient:  0.11895593931295605
iteration : 10974
train acc:  0.7890625
train loss:  0.47350895404815674
train gradient:  0.10887916528138801
iteration : 10975
train acc:  0.8125
train loss:  0.42346805334091187
train gradient:  0.10070974091718665
iteration : 10976
train acc:  0.765625
train loss:  0.48658841848373413
train gradient:  0.1329988193414341
iteration : 10977
train acc:  0.7265625
train loss:  0.5125008821487427
train gradient:  0.1390931364230636
iteration : 10978
train acc:  0.765625
train loss:  0.4712962210178375
train gradient:  0.10133838197663259
iteration : 10979
train acc:  0.78125
train loss:  0.44083142280578613
train gradient:  0.13054481929189993
iteration : 10980
train acc:  0.7734375
train loss:  0.4423418641090393
train gradient:  0.08870015893835183
iteration : 10981
train acc:  0.7578125
train loss:  0.49231284856796265
train gradient:  0.14033921282821327
iteration : 10982
train acc:  0.8046875
train loss:  0.4283422529697418
train gradient:  0.10546507701648825
iteration : 10983
train acc:  0.7265625
train loss:  0.523030698299408
train gradient:  0.1687380189148761
iteration : 10984
train acc:  0.75
train loss:  0.49031227827072144
train gradient:  0.13201238600743356
iteration : 10985
train acc:  0.6796875
train loss:  0.5566729307174683
train gradient:  0.18279071550302223
iteration : 10986
train acc:  0.7578125
train loss:  0.45816871523857117
train gradient:  0.11036769312310417
iteration : 10987
train acc:  0.828125
train loss:  0.4382730722427368
train gradient:  0.1161454977278163
iteration : 10988
train acc:  0.765625
train loss:  0.43131667375564575
train gradient:  0.10328020011206118
iteration : 10989
train acc:  0.78125
train loss:  0.4392026662826538
train gradient:  0.11237133588972446
iteration : 10990
train acc:  0.7578125
train loss:  0.5168423056602478
train gradient:  0.1403343223892549
iteration : 10991
train acc:  0.6796875
train loss:  0.5552199482917786
train gradient:  0.12412209292311246
iteration : 10992
train acc:  0.7734375
train loss:  0.45808690786361694
train gradient:  0.1158987190095728
iteration : 10993
train acc:  0.7734375
train loss:  0.478141188621521
train gradient:  0.11376084732777327
iteration : 10994
train acc:  0.734375
train loss:  0.48554202914237976
train gradient:  0.12996779177272125
iteration : 10995
train acc:  0.703125
train loss:  0.5274003744125366
train gradient:  0.13267362842680913
iteration : 10996
train acc:  0.7109375
train loss:  0.6463824510574341
train gradient:  0.19288223989649658
iteration : 10997
train acc:  0.71875
train loss:  0.5659976005554199
train gradient:  0.14636744217246453
iteration : 10998
train acc:  0.8046875
train loss:  0.4095202088356018
train gradient:  0.09064740734316597
iteration : 10999
train acc:  0.7890625
train loss:  0.4576268792152405
train gradient:  0.12486043210343606
iteration : 11000
train acc:  0.703125
train loss:  0.5017793774604797
train gradient:  0.16092665782280483
iteration : 11001
train acc:  0.765625
train loss:  0.5458950996398926
train gradient:  0.15935560711424346
iteration : 11002
train acc:  0.8125
train loss:  0.4082198143005371
train gradient:  0.09375615448407627
iteration : 11003
train acc:  0.7421875
train loss:  0.47235170006752014
train gradient:  0.12365135216837729
iteration : 11004
train acc:  0.71875
train loss:  0.4834676682949066
train gradient:  0.12951634666611694
iteration : 11005
train acc:  0.6953125
train loss:  0.5557506084442139
train gradient:  0.16295912576273375
iteration : 11006
train acc:  0.6875
train loss:  0.5700035691261292
train gradient:  0.17537982008992045
iteration : 11007
train acc:  0.75
train loss:  0.4740644693374634
train gradient:  0.14866270075393206
iteration : 11008
train acc:  0.7265625
train loss:  0.5088480710983276
train gradient:  0.17626343682963808
iteration : 11009
train acc:  0.7265625
train loss:  0.47280704975128174
train gradient:  0.12871138057963485
iteration : 11010
train acc:  0.65625
train loss:  0.5362423658370972
train gradient:  0.17670203793759753
iteration : 11011
train acc:  0.7578125
train loss:  0.499498188495636
train gradient:  0.1288156068655116
iteration : 11012
train acc:  0.765625
train loss:  0.4542366862297058
train gradient:  0.1251718410519845
iteration : 11013
train acc:  0.7578125
train loss:  0.4725583791732788
train gradient:  0.1194370206896196
iteration : 11014
train acc:  0.734375
train loss:  0.5110477805137634
train gradient:  0.13880424737903183
iteration : 11015
train acc:  0.7421875
train loss:  0.49071794748306274
train gradient:  0.13912608334904317
iteration : 11016
train acc:  0.7421875
train loss:  0.48577895760536194
train gradient:  0.12451718505755073
iteration : 11017
train acc:  0.7265625
train loss:  0.4812428057193756
train gradient:  0.13932302172402639
iteration : 11018
train acc:  0.78125
train loss:  0.4410310387611389
train gradient:  0.11729721961940055
iteration : 11019
train acc:  0.71875
train loss:  0.5032964944839478
train gradient:  0.13390089369448072
iteration : 11020
train acc:  0.75
train loss:  0.5205499529838562
train gradient:  0.12911738279745705
iteration : 11021
train acc:  0.796875
train loss:  0.4874376058578491
train gradient:  0.119331763023756
iteration : 11022
train acc:  0.7734375
train loss:  0.4480157196521759
train gradient:  0.1322110121968758
iteration : 11023
train acc:  0.765625
train loss:  0.49430593848228455
train gradient:  0.16806383996536217
iteration : 11024
train acc:  0.7109375
train loss:  0.5195485353469849
train gradient:  0.1304824986840768
iteration : 11025
train acc:  0.71875
train loss:  0.4478977918624878
train gradient:  0.09137971999812468
iteration : 11026
train acc:  0.7109375
train loss:  0.48955515027046204
train gradient:  0.11966977985070694
iteration : 11027
train acc:  0.8046875
train loss:  0.48585188388824463
train gradient:  0.1344480760765334
iteration : 11028
train acc:  0.6875
train loss:  0.5379270911216736
train gradient:  0.1349883564916748
iteration : 11029
train acc:  0.71875
train loss:  0.5680328011512756
train gradient:  0.16027604559511183
iteration : 11030
train acc:  0.7265625
train loss:  0.500220537185669
train gradient:  0.1438906061804484
iteration : 11031
train acc:  0.6953125
train loss:  0.49071991443634033
train gradient:  0.11056101533089328
iteration : 11032
train acc:  0.8125
train loss:  0.42665427923202515
train gradient:  0.10134232508211122
iteration : 11033
train acc:  0.7421875
train loss:  0.511471152305603
train gradient:  0.13960873708984528
iteration : 11034
train acc:  0.8046875
train loss:  0.4711624085903168
train gradient:  0.10924592985268501
iteration : 11035
train acc:  0.7421875
train loss:  0.5427526235580444
train gradient:  0.1266863099494541
iteration : 11036
train acc:  0.765625
train loss:  0.4562395215034485
train gradient:  0.10739784093200327
iteration : 11037
train acc:  0.7734375
train loss:  0.4581039547920227
train gradient:  0.08758659126226538
iteration : 11038
train acc:  0.71875
train loss:  0.5133088827133179
train gradient:  0.12654143625322037
iteration : 11039
train acc:  0.65625
train loss:  0.5508396029472351
train gradient:  0.1481063213452093
iteration : 11040
train acc:  0.78125
train loss:  0.457023561000824
train gradient:  0.13342101201404669
iteration : 11041
train acc:  0.7265625
train loss:  0.4823031425476074
train gradient:  0.1385840373124892
iteration : 11042
train acc:  0.7265625
train loss:  0.5221116542816162
train gradient:  0.13463937472073878
iteration : 11043
train acc:  0.734375
train loss:  0.48146966099739075
train gradient:  0.12251242618545406
iteration : 11044
train acc:  0.6953125
train loss:  0.5257012844085693
train gradient:  0.17903309099351977
iteration : 11045
train acc:  0.734375
train loss:  0.5305307507514954
train gradient:  0.19607584491588695
iteration : 11046
train acc:  0.6875
train loss:  0.5412311553955078
train gradient:  0.18580301057578752
iteration : 11047
train acc:  0.78125
train loss:  0.4321308135986328
train gradient:  0.09044109626314052
iteration : 11048
train acc:  0.75
train loss:  0.47087037563323975
train gradient:  0.11988977429740412
iteration : 11049
train acc:  0.6796875
train loss:  0.522871732711792
train gradient:  0.1629478637722364
iteration : 11050
train acc:  0.7265625
train loss:  0.5141479969024658
train gradient:  0.14223316403605046
iteration : 11051
train acc:  0.75
train loss:  0.4833785891532898
train gradient:  0.1339916541830628
iteration : 11052
train acc:  0.7421875
train loss:  0.5085583925247192
train gradient:  0.12316229638429585
iteration : 11053
train acc:  0.7578125
train loss:  0.5244982242584229
train gradient:  0.12121525474746504
iteration : 11054
train acc:  0.6953125
train loss:  0.5536579489707947
train gradient:  0.15798881691075245
iteration : 11055
train acc:  0.7578125
train loss:  0.4479275047779083
train gradient:  0.10045057249614313
iteration : 11056
train acc:  0.7890625
train loss:  0.4337309002876282
train gradient:  0.11244410745202828
iteration : 11057
train acc:  0.734375
train loss:  0.5278534889221191
train gradient:  0.13758534137370948
iteration : 11058
train acc:  0.7421875
train loss:  0.4828340411186218
train gradient:  0.11920414322916531
iteration : 11059
train acc:  0.703125
train loss:  0.5289613604545593
train gradient:  0.1276909843539994
iteration : 11060
train acc:  0.796875
train loss:  0.48423945903778076
train gradient:  0.1531772353472311
iteration : 11061
train acc:  0.7421875
train loss:  0.5188961029052734
train gradient:  0.1361535228500978
iteration : 11062
train acc:  0.765625
train loss:  0.5029511451721191
train gradient:  0.15243843682496622
iteration : 11063
train acc:  0.71875
train loss:  0.528802216053009
train gradient:  0.14389924833726023
iteration : 11064
train acc:  0.7578125
train loss:  0.45087847113609314
train gradient:  0.12078498507984059
iteration : 11065
train acc:  0.7734375
train loss:  0.46329981088638306
train gradient:  0.13047473669441706
iteration : 11066
train acc:  0.734375
train loss:  0.4882451891899109
train gradient:  0.14379853217069816
iteration : 11067
train acc:  0.796875
train loss:  0.4045959711074829
train gradient:  0.09507374063273684
iteration : 11068
train acc:  0.6953125
train loss:  0.5499707460403442
train gradient:  0.13700108677582828
iteration : 11069
train acc:  0.765625
train loss:  0.44945791363716125
train gradient:  0.08711348473068985
iteration : 11070
train acc:  0.71875
train loss:  0.4772924780845642
train gradient:  0.13399421621583166
iteration : 11071
train acc:  0.71875
train loss:  0.5137474536895752
train gradient:  0.14376563037365375
iteration : 11072
train acc:  0.7734375
train loss:  0.46005722880363464
train gradient:  0.10215916087823007
iteration : 11073
train acc:  0.765625
train loss:  0.4704269766807556
train gradient:  0.10077999761965556
iteration : 11074
train acc:  0.765625
train loss:  0.45287713408470154
train gradient:  0.1120946241131254
iteration : 11075
train acc:  0.703125
train loss:  0.547899603843689
train gradient:  0.2151869486478935
iteration : 11076
train acc:  0.65625
train loss:  0.6451903581619263
train gradient:  0.19341992656770451
iteration : 11077
train acc:  0.71875
train loss:  0.4778367877006531
train gradient:  0.11157127667079655
iteration : 11078
train acc:  0.7734375
train loss:  0.46307826042175293
train gradient:  0.1145551194999656
iteration : 11079
train acc:  0.7109375
train loss:  0.4974012076854706
train gradient:  0.12771124056800068
iteration : 11080
train acc:  0.7578125
train loss:  0.467146635055542
train gradient:  0.12509045558922557
iteration : 11081
train acc:  0.6796875
train loss:  0.5719347596168518
train gradient:  0.1835799289920837
iteration : 11082
train acc:  0.7109375
train loss:  0.49835100769996643
train gradient:  0.1405890825691285
iteration : 11083
train acc:  0.7265625
train loss:  0.5096060633659363
train gradient:  0.13185246106126025
iteration : 11084
train acc:  0.7109375
train loss:  0.45716530084609985
train gradient:  0.11539662364520795
iteration : 11085
train acc:  0.7265625
train loss:  0.48281601071357727
train gradient:  0.11440663629033512
iteration : 11086
train acc:  0.7109375
train loss:  0.5187374353408813
train gradient:  0.12397154620796513
iteration : 11087
train acc:  0.734375
train loss:  0.5008420944213867
train gradient:  0.11632384427750135
iteration : 11088
train acc:  0.7109375
train loss:  0.5337436199188232
train gradient:  0.13654444358925666
iteration : 11089
train acc:  0.765625
train loss:  0.46518486738204956
train gradient:  0.14128632109422268
iteration : 11090
train acc:  0.7578125
train loss:  0.44086799025535583
train gradient:  0.08846535950973536
iteration : 11091
train acc:  0.7421875
train loss:  0.5411000847816467
train gradient:  0.1589198054891956
iteration : 11092
train acc:  0.796875
train loss:  0.47073689103126526
train gradient:  0.17297307743951612
iteration : 11093
train acc:  0.6953125
train loss:  0.5197054147720337
train gradient:  0.11661172958132118
iteration : 11094
train acc:  0.734375
train loss:  0.49149948358535767
train gradient:  0.11606947117230497
iteration : 11095
train acc:  0.796875
train loss:  0.4743603467941284
train gradient:  0.11282478753460622
iteration : 11096
train acc:  0.7421875
train loss:  0.5292185544967651
train gradient:  0.19014425687371825
iteration : 11097
train acc:  0.734375
train loss:  0.5123106837272644
train gradient:  0.17048421624907517
iteration : 11098
train acc:  0.6953125
train loss:  0.5344506502151489
train gradient:  0.148405094995316
iteration : 11099
train acc:  0.734375
train loss:  0.5087016224861145
train gradient:  0.13558405567672738
iteration : 11100
train acc:  0.7421875
train loss:  0.4539443850517273
train gradient:  0.0891249447697296
iteration : 11101
train acc:  0.7109375
train loss:  0.4808412790298462
train gradient:  0.1151215952569203
iteration : 11102
train acc:  0.734375
train loss:  0.4737091064453125
train gradient:  0.1273334137434664
iteration : 11103
train acc:  0.71875
train loss:  0.5369139909744263
train gradient:  0.12966155657946196
iteration : 11104
train acc:  0.6875
train loss:  0.5546718239784241
train gradient:  0.21162960656225005
iteration : 11105
train acc:  0.75
train loss:  0.4636082649230957
train gradient:  0.13396563156225522
iteration : 11106
train acc:  0.7265625
train loss:  0.49749618768692017
train gradient:  0.1274500830730962
iteration : 11107
train acc:  0.703125
train loss:  0.5390297174453735
train gradient:  0.17083144575233122
iteration : 11108
train acc:  0.7265625
train loss:  0.5085586309432983
train gradient:  0.14850906938485275
iteration : 11109
train acc:  0.796875
train loss:  0.5089235305786133
train gradient:  0.1392296472754901
iteration : 11110
train acc:  0.75
train loss:  0.5356533527374268
train gradient:  0.16845290921812545
iteration : 11111
train acc:  0.71875
train loss:  0.504840612411499
train gradient:  0.12115657715026637
iteration : 11112
train acc:  0.7265625
train loss:  0.5061459541320801
train gradient:  0.1664371016854499
iteration : 11113
train acc:  0.75
train loss:  0.5336408615112305
train gradient:  0.13176432358018753
iteration : 11114
train acc:  0.8359375
train loss:  0.4445526599884033
train gradient:  0.13678706352471748
iteration : 11115
train acc:  0.7734375
train loss:  0.4204155504703522
train gradient:  0.07823108731854791
iteration : 11116
train acc:  0.8046875
train loss:  0.3941287100315094
train gradient:  0.08366957295171991
iteration : 11117
train acc:  0.71875
train loss:  0.5918595790863037
train gradient:  0.19013163060900007
iteration : 11118
train acc:  0.734375
train loss:  0.5034381151199341
train gradient:  0.14002339818219273
iteration : 11119
train acc:  0.7109375
train loss:  0.5218058228492737
train gradient:  0.1662913774671635
iteration : 11120
train acc:  0.71875
train loss:  0.5070065259933472
train gradient:  0.13823618251864211
iteration : 11121
train acc:  0.7265625
train loss:  0.5227512121200562
train gradient:  0.15792833202976253
iteration : 11122
train acc:  0.703125
train loss:  0.575281023979187
train gradient:  0.19014683081224631
iteration : 11123
train acc:  0.7734375
train loss:  0.4607124328613281
train gradient:  0.09996066670915246
iteration : 11124
train acc:  0.7734375
train loss:  0.4921233057975769
train gradient:  0.14627971859633415
iteration : 11125
train acc:  0.7265625
train loss:  0.47976887226104736
train gradient:  0.11313392257373932
iteration : 11126
train acc:  0.7734375
train loss:  0.5094627737998962
train gradient:  0.12355882792405518
iteration : 11127
train acc:  0.734375
train loss:  0.49245283007621765
train gradient:  0.12344013528920861
iteration : 11128
train acc:  0.765625
train loss:  0.4874376356601715
train gradient:  0.11497538836286711
iteration : 11129
train acc:  0.7109375
train loss:  0.5370866060256958
train gradient:  0.18096469396211678
iteration : 11130
train acc:  0.6640625
train loss:  0.5701557397842407
train gradient:  0.15361582857194456
iteration : 11131
train acc:  0.7265625
train loss:  0.5281791687011719
train gradient:  0.12125415508769055
iteration : 11132
train acc:  0.6875
train loss:  0.5239734053611755
train gradient:  0.14021956774240296
iteration : 11133
train acc:  0.734375
train loss:  0.4834308624267578
train gradient:  0.13346355900204634
iteration : 11134
train acc:  0.75
train loss:  0.47370481491088867
train gradient:  0.09884321418927121
iteration : 11135
train acc:  0.7890625
train loss:  0.47993940114974976
train gradient:  0.12530696438856662
iteration : 11136
train acc:  0.75
train loss:  0.500151515007019
train gradient:  0.14821956813692627
iteration : 11137
train acc:  0.734375
train loss:  0.5368185639381409
train gradient:  0.13732498994392628
iteration : 11138
train acc:  0.78125
train loss:  0.4310724139213562
train gradient:  0.12111704905496426
iteration : 11139
train acc:  0.7421875
train loss:  0.48297154903411865
train gradient:  0.11971550931510024
iteration : 11140
train acc:  0.7734375
train loss:  0.4765239655971527
train gradient:  0.10825473107009523
iteration : 11141
train acc:  0.71875
train loss:  0.5124111175537109
train gradient:  0.1404379458219449
iteration : 11142
train acc:  0.7734375
train loss:  0.48417627811431885
train gradient:  0.1290260137493247
iteration : 11143
train acc:  0.734375
train loss:  0.5503203868865967
train gradient:  0.13487098681300363
iteration : 11144
train acc:  0.765625
train loss:  0.49762797355651855
train gradient:  0.13695003135409334
iteration : 11145
train acc:  0.7109375
train loss:  0.5155515670776367
train gradient:  0.1046452002280827
iteration : 11146
train acc:  0.703125
train loss:  0.5422049760818481
train gradient:  0.16292584698590462
iteration : 11147
train acc:  0.7578125
train loss:  0.4716956317424774
train gradient:  0.12889586264367964
iteration : 11148
train acc:  0.8046875
train loss:  0.4413082003593445
train gradient:  0.10145114207282484
iteration : 11149
train acc:  0.7265625
train loss:  0.5000663995742798
train gradient:  0.11618874534277429
iteration : 11150
train acc:  0.71875
train loss:  0.47228094935417175
train gradient:  0.14189597644125163
iteration : 11151
train acc:  0.828125
train loss:  0.4146136939525604
train gradient:  0.10773700356046932
iteration : 11152
train acc:  0.71875
train loss:  0.518879771232605
train gradient:  0.1587934660640481
iteration : 11153
train acc:  0.7734375
train loss:  0.4863419830799103
train gradient:  0.13845763551202847
iteration : 11154
train acc:  0.765625
train loss:  0.4877499043941498
train gradient:  0.12981137998871722
iteration : 11155
train acc:  0.734375
train loss:  0.5063501000404358
train gradient:  0.1326788169334473
iteration : 11156
train acc:  0.7734375
train loss:  0.4508504271507263
train gradient:  0.13557394662999936
iteration : 11157
train acc:  0.7421875
train loss:  0.4835246503353119
train gradient:  0.11604928017374429
iteration : 11158
train acc:  0.75
train loss:  0.47477883100509644
train gradient:  0.1328277300992538
iteration : 11159
train acc:  0.734375
train loss:  0.4633933901786804
train gradient:  0.11892002078011105
iteration : 11160
train acc:  0.75
train loss:  0.5438017845153809
train gradient:  0.16500107857315743
iteration : 11161
train acc:  0.7578125
train loss:  0.5193866491317749
train gradient:  0.1298760167383227
iteration : 11162
train acc:  0.8203125
train loss:  0.42186716198921204
train gradient:  0.09011296919628384
iteration : 11163
train acc:  0.7109375
train loss:  0.46472394466400146
train gradient:  0.12407433371461286
iteration : 11164
train acc:  0.734375
train loss:  0.4867057800292969
train gradient:  0.12348357308666504
iteration : 11165
train acc:  0.75
train loss:  0.49515286087989807
train gradient:  0.1303829906454816
iteration : 11166
train acc:  0.6953125
train loss:  0.5336217880249023
train gradient:  0.16766525992534595
iteration : 11167
train acc:  0.75
train loss:  0.4938357472419739
train gradient:  0.16702091238408004
iteration : 11168
train acc:  0.7421875
train loss:  0.5053242444992065
train gradient:  0.15790823519152491
iteration : 11169
train acc:  0.734375
train loss:  0.49167102575302124
train gradient:  0.10678483393728876
iteration : 11170
train acc:  0.734375
train loss:  0.510055661201477
train gradient:  0.14320982629828938
iteration : 11171
train acc:  0.6875
train loss:  0.5605879426002502
train gradient:  0.1346947633020068
iteration : 11172
train acc:  0.71875
train loss:  0.48797330260276794
train gradient:  0.12176239353849765
iteration : 11173
train acc:  0.8671875
train loss:  0.38027554750442505
train gradient:  0.08152205304866664
iteration : 11174
train acc:  0.78125
train loss:  0.4400253891944885
train gradient:  0.10826892202820765
iteration : 11175
train acc:  0.7109375
train loss:  0.5144416093826294
train gradient:  0.15204440523660118
iteration : 11176
train acc:  0.75
train loss:  0.47170594334602356
train gradient:  0.12430510586843489
iteration : 11177
train acc:  0.7734375
train loss:  0.41932833194732666
train gradient:  0.12385175937840276
iteration : 11178
train acc:  0.796875
train loss:  0.477634459733963
train gradient:  0.11594158478879467
iteration : 11179
train acc:  0.78125
train loss:  0.43869563937187195
train gradient:  0.12585925508020956
iteration : 11180
train acc:  0.6875
train loss:  0.5461252331733704
train gradient:  0.15629743903540738
iteration : 11181
train acc:  0.734375
train loss:  0.47108298540115356
train gradient:  0.14643740145516493
iteration : 11182
train acc:  0.703125
train loss:  0.5989624261856079
train gradient:  0.16550915562668447
iteration : 11183
train acc:  0.6953125
train loss:  0.5689470767974854
train gradient:  0.1747163222273292
iteration : 11184
train acc:  0.734375
train loss:  0.5218650102615356
train gradient:  0.18007504660515616
iteration : 11185
train acc:  0.7734375
train loss:  0.5065229535102844
train gradient:  0.14748888156076873
iteration : 11186
train acc:  0.828125
train loss:  0.4252115488052368
train gradient:  0.09521412140486472
iteration : 11187
train acc:  0.7421875
train loss:  0.5009041428565979
train gradient:  0.1590789554264893
iteration : 11188
train acc:  0.8125
train loss:  0.425340473651886
train gradient:  0.08697066386810459
iteration : 11189
train acc:  0.7578125
train loss:  0.48531967401504517
train gradient:  0.141173329216923
iteration : 11190
train acc:  0.765625
train loss:  0.4318373501300812
train gradient:  0.09333376657400808
iteration : 11191
train acc:  0.8203125
train loss:  0.3941119313240051
train gradient:  0.12439293505071121
iteration : 11192
train acc:  0.6875
train loss:  0.5477581024169922
train gradient:  0.1428273015970643
iteration : 11193
train acc:  0.7890625
train loss:  0.4542784094810486
train gradient:  0.10443456869153
iteration : 11194
train acc:  0.734375
train loss:  0.49384379386901855
train gradient:  0.15089474129900288
iteration : 11195
train acc:  0.75
train loss:  0.528311014175415
train gradient:  0.15340865794544412
iteration : 11196
train acc:  0.671875
train loss:  0.5787416696548462
train gradient:  0.18759830654137444
iteration : 11197
train acc:  0.7890625
train loss:  0.4494016766548157
train gradient:  0.0940405139894655
iteration : 11198
train acc:  0.734375
train loss:  0.49400779604911804
train gradient:  0.11295154360289256
iteration : 11199
train acc:  0.7421875
train loss:  0.5555635690689087
train gradient:  0.15925408151975434
iteration : 11200
train acc:  0.71875
train loss:  0.47413116693496704
train gradient:  0.11337596291451744
iteration : 11201
train acc:  0.734375
train loss:  0.5134308338165283
train gradient:  0.12418042146956726
iteration : 11202
train acc:  0.7421875
train loss:  0.47459185123443604
train gradient:  0.15499124513753026
iteration : 11203
train acc:  0.7578125
train loss:  0.46103668212890625
train gradient:  0.11560032735613135
iteration : 11204
train acc:  0.7421875
train loss:  0.5293857455253601
train gradient:  0.19794535266556923
iteration : 11205
train acc:  0.7578125
train loss:  0.4659574627876282
train gradient:  0.10521581556676854
iteration : 11206
train acc:  0.7734375
train loss:  0.4982278347015381
train gradient:  0.11104494587082432
iteration : 11207
train acc:  0.78125
train loss:  0.47833016514778137
train gradient:  0.12518002939665557
iteration : 11208
train acc:  0.703125
train loss:  0.522191047668457
train gradient:  0.15658175221146314
iteration : 11209
train acc:  0.8203125
train loss:  0.409697949886322
train gradient:  0.12708068162963995
iteration : 11210
train acc:  0.703125
train loss:  0.5592647194862366
train gradient:  0.16488879125435435
iteration : 11211
train acc:  0.859375
train loss:  0.4219628870487213
train gradient:  0.11134558226962338
iteration : 11212
train acc:  0.7109375
train loss:  0.5534968376159668
train gradient:  0.13911787930290642
iteration : 11213
train acc:  0.78125
train loss:  0.4911558926105499
train gradient:  0.11267831072089533
iteration : 11214
train acc:  0.75
train loss:  0.4729125499725342
train gradient:  0.12050806467417814
iteration : 11215
train acc:  0.7109375
train loss:  0.5388909578323364
train gradient:  0.15113495273151156
iteration : 11216
train acc:  0.7109375
train loss:  0.5563331842422485
train gradient:  0.1474915980903918
iteration : 11217
train acc:  0.75
train loss:  0.4762706160545349
train gradient:  0.14072192775857678
iteration : 11218
train acc:  0.734375
train loss:  0.46814173460006714
train gradient:  0.13269027988445542
iteration : 11219
train acc:  0.71875
train loss:  0.5298469662666321
train gradient:  0.16646312274356467
iteration : 11220
train acc:  0.765625
train loss:  0.5029243230819702
train gradient:  0.1061710998719998
iteration : 11221
train acc:  0.7734375
train loss:  0.42307358980178833
train gradient:  0.10572600833828358
iteration : 11222
train acc:  0.7109375
train loss:  0.5566661357879639
train gradient:  0.14513397141695122
iteration : 11223
train acc:  0.8046875
train loss:  0.45878374576568604
train gradient:  0.11235874570777402
iteration : 11224
train acc:  0.765625
train loss:  0.5230015516281128
train gradient:  0.1184560447465542
iteration : 11225
train acc:  0.703125
train loss:  0.506883442401886
train gradient:  0.15592546291085466
iteration : 11226
train acc:  0.703125
train loss:  0.502768874168396
train gradient:  0.1144473022143296
iteration : 11227
train acc:  0.7734375
train loss:  0.42621955275535583
train gradient:  0.10863708380603854
iteration : 11228
train acc:  0.7109375
train loss:  0.5033442378044128
train gradient:  0.1282803033610596
iteration : 11229
train acc:  0.765625
train loss:  0.5011808276176453
train gradient:  0.1173318472550326
iteration : 11230
train acc:  0.71875
train loss:  0.5038183331489563
train gradient:  0.1255979552684211
iteration : 11231
train acc:  0.7265625
train loss:  0.49329105019569397
train gradient:  0.12218638361458395
iteration : 11232
train acc:  0.7109375
train loss:  0.47919127345085144
train gradient:  0.14233729298653847
iteration : 11233
train acc:  0.765625
train loss:  0.4226410984992981
train gradient:  0.10255566420170906
iteration : 11234
train acc:  0.8046875
train loss:  0.42841967940330505
train gradient:  0.09302492049502116
iteration : 11235
train acc:  0.7265625
train loss:  0.49016234278678894
train gradient:  0.13556373864056098
iteration : 11236
train acc:  0.703125
train loss:  0.5128931403160095
train gradient:  0.14594955862441727
iteration : 11237
train acc:  0.8125
train loss:  0.4645535349845886
train gradient:  0.19334156175954129
iteration : 11238
train acc:  0.7421875
train loss:  0.5332937240600586
train gradient:  0.1925333727190473
iteration : 11239
train acc:  0.75
train loss:  0.4805811643600464
train gradient:  0.17611406580968914
iteration : 11240
train acc:  0.734375
train loss:  0.5447993278503418
train gradient:  0.1539742335355277
iteration : 11241
train acc:  0.796875
train loss:  0.4774813652038574
train gradient:  0.11206170712796616
iteration : 11242
train acc:  0.75
train loss:  0.49535298347473145
train gradient:  0.13235720710775167
iteration : 11243
train acc:  0.7109375
train loss:  0.5038425922393799
train gradient:  0.15068341523187934
iteration : 11244
train acc:  0.7265625
train loss:  0.5472958087921143
train gradient:  0.17291637348185157
iteration : 11245
train acc:  0.796875
train loss:  0.4408973455429077
train gradient:  0.09566463517242979
iteration : 11246
train acc:  0.7265625
train loss:  0.5581990480422974
train gradient:  0.1525088530959018
iteration : 11247
train acc:  0.75
train loss:  0.47095006704330444
train gradient:  0.12330878789209747
iteration : 11248
train acc:  0.7109375
train loss:  0.48233845829963684
train gradient:  0.12984138635825196
iteration : 11249
train acc:  0.7109375
train loss:  0.5349739789962769
train gradient:  0.15718884250277776
iteration : 11250
train acc:  0.71875
train loss:  0.5357978940010071
train gradient:  0.155203585712067
iteration : 11251
train acc:  0.765625
train loss:  0.4445532262325287
train gradient:  0.10457500770313663
iteration : 11252
train acc:  0.75
train loss:  0.48973673582077026
train gradient:  0.11861259016747402
iteration : 11253
train acc:  0.78125
train loss:  0.45146822929382324
train gradient:  0.10559468581963474
iteration : 11254
train acc:  0.765625
train loss:  0.4594568610191345
train gradient:  0.10768335187084976
iteration : 11255
train acc:  0.703125
train loss:  0.5245924592018127
train gradient:  0.1366412577781983
iteration : 11256
train acc:  0.6953125
train loss:  0.5507129430770874
train gradient:  0.1630488857584469
iteration : 11257
train acc:  0.7734375
train loss:  0.4600820541381836
train gradient:  0.09996053379067156
iteration : 11258
train acc:  0.7890625
train loss:  0.48542529344558716
train gradient:  0.14063660771438208
iteration : 11259
train acc:  0.78125
train loss:  0.4632956385612488
train gradient:  0.10855453155486013
iteration : 11260
train acc:  0.734375
train loss:  0.5376922488212585
train gradient:  0.13467522125846917
iteration : 11261
train acc:  0.7109375
train loss:  0.4948711097240448
train gradient:  0.12571516530411994
iteration : 11262
train acc:  0.7578125
train loss:  0.4772952198982239
train gradient:  0.13658755500588854
iteration : 11263
train acc:  0.7578125
train loss:  0.5030101537704468
train gradient:  0.14451620499115964
iteration : 11264
train acc:  0.78125
train loss:  0.42523473501205444
train gradient:  0.08185230857679167
iteration : 11265
train acc:  0.75
train loss:  0.44634854793548584
train gradient:  0.11949224599854735
iteration : 11266
train acc:  0.7265625
train loss:  0.5271323323249817
train gradient:  0.13178940334395964
iteration : 11267
train acc:  0.7421875
train loss:  0.4607055187225342
train gradient:  0.10988052807943155
iteration : 11268
train acc:  0.765625
train loss:  0.45170092582702637
train gradient:  0.11107902227316595
iteration : 11269
train acc:  0.828125
train loss:  0.4340648651123047
train gradient:  0.12543432656499004
iteration : 11270
train acc:  0.8203125
train loss:  0.42611026763916016
train gradient:  0.10257553990024562
iteration : 11271
train acc:  0.7109375
train loss:  0.5351358652114868
train gradient:  0.2205170892737555
iteration : 11272
train acc:  0.828125
train loss:  0.4101446866989136
train gradient:  0.07598291580470878
iteration : 11273
train acc:  0.71875
train loss:  0.49809980392456055
train gradient:  0.15211510305894035
iteration : 11274
train acc:  0.75
train loss:  0.4726802408695221
train gradient:  0.11677350492447275
iteration : 11275
train acc:  0.7578125
train loss:  0.43866193294525146
train gradient:  0.12146211589738963
iteration : 11276
train acc:  0.7890625
train loss:  0.451006144285202
train gradient:  0.12462556812758316
iteration : 11277
train acc:  0.7109375
train loss:  0.5340720415115356
train gradient:  0.14518082018274012
iteration : 11278
train acc:  0.796875
train loss:  0.411909282207489
train gradient:  0.08336555337156124
iteration : 11279
train acc:  0.7734375
train loss:  0.48003071546554565
train gradient:  0.16959949670515218
iteration : 11280
train acc:  0.7890625
train loss:  0.48199212551116943
train gradient:  0.126766948782973
iteration : 11281
train acc:  0.75
train loss:  0.5145854353904724
train gradient:  0.1677504310369825
iteration : 11282
train acc:  0.7421875
train loss:  0.4751185178756714
train gradient:  0.11802854315791265
iteration : 11283
train acc:  0.8046875
train loss:  0.5036827921867371
train gradient:  0.2760335998834021
iteration : 11284
train acc:  0.734375
train loss:  0.5104393362998962
train gradient:  0.15461973676557028
iteration : 11285
train acc:  0.734375
train loss:  0.5270769596099854
train gradient:  0.1735485599199505
iteration : 11286
train acc:  0.7734375
train loss:  0.45507922768592834
train gradient:  0.13422297333199518
iteration : 11287
train acc:  0.734375
train loss:  0.4767225980758667
train gradient:  0.14615635629482943
iteration : 11288
train acc:  0.7265625
train loss:  0.4576287567615509
train gradient:  0.15821115068349623
iteration : 11289
train acc:  0.6171875
train loss:  0.6312295198440552
train gradient:  0.18098501179112786
iteration : 11290
train acc:  0.6484375
train loss:  0.5958913564682007
train gradient:  0.1542402013174562
iteration : 11291
train acc:  0.859375
train loss:  0.3805125951766968
train gradient:  0.09920454911448988
iteration : 11292
train acc:  0.703125
train loss:  0.5489007234573364
train gradient:  0.13645537852651052
iteration : 11293
train acc:  0.7265625
train loss:  0.5316284894943237
train gradient:  0.15940280510205407
iteration : 11294
train acc:  0.6640625
train loss:  0.563046395778656
train gradient:  0.18406372165834264
iteration : 11295
train acc:  0.75
train loss:  0.48215198516845703
train gradient:  0.11997100956351839
iteration : 11296
train acc:  0.7109375
train loss:  0.5193924307823181
train gradient:  0.14778901616040457
iteration : 11297
train acc:  0.828125
train loss:  0.44156867265701294
train gradient:  0.10635791624216853
iteration : 11298
train acc:  0.7734375
train loss:  0.47394564747810364
train gradient:  0.14639683114470553
iteration : 11299
train acc:  0.7265625
train loss:  0.5539681315422058
train gradient:  0.14622419540239767
iteration : 11300
train acc:  0.75
train loss:  0.4456290006637573
train gradient:  0.10152786414365274
iteration : 11301
train acc:  0.7265625
train loss:  0.5177204608917236
train gradient:  0.12585676074343732
iteration : 11302
train acc:  0.7109375
train loss:  0.531352162361145
train gradient:  0.13112515008313966
iteration : 11303
train acc:  0.6875
train loss:  0.5596320629119873
train gradient:  0.160971186365994
iteration : 11304
train acc:  0.7421875
train loss:  0.5085229873657227
train gradient:  0.1377434816055057
iteration : 11305
train acc:  0.75
train loss:  0.4930430054664612
train gradient:  0.11506094811015771
iteration : 11306
train acc:  0.71875
train loss:  0.507850170135498
train gradient:  0.13415559547004766
iteration : 11307
train acc:  0.75
train loss:  0.5096962451934814
train gradient:  0.15104987937789288
iteration : 11308
train acc:  0.78125
train loss:  0.456498920917511
train gradient:  0.11146358803027617
iteration : 11309
train acc:  0.75
train loss:  0.4772672951221466
train gradient:  0.09905419199923923
iteration : 11310
train acc:  0.734375
train loss:  0.4504052698612213
train gradient:  0.1277294186555414
iteration : 11311
train acc:  0.734375
train loss:  0.5467429161071777
train gradient:  0.19838443802742467
iteration : 11312
train acc:  0.7421875
train loss:  0.467861533164978
train gradient:  0.12704818058337147
iteration : 11313
train acc:  0.71875
train loss:  0.5430992245674133
train gradient:  0.16237464100081433
iteration : 11314
train acc:  0.7734375
train loss:  0.46975845098495483
train gradient:  0.1259219933804835
iteration : 11315
train acc:  0.75
train loss:  0.48884761333465576
train gradient:  0.1673394109195787
iteration : 11316
train acc:  0.8203125
train loss:  0.42693179845809937
train gradient:  0.10387821080240849
iteration : 11317
train acc:  0.671875
train loss:  0.5943185091018677
train gradient:  0.1442046448782437
iteration : 11318
train acc:  0.8046875
train loss:  0.4501430094242096
train gradient:  0.11223933367468994
iteration : 11319
train acc:  0.640625
train loss:  0.6370243430137634
train gradient:  0.2119705444751941
iteration : 11320
train acc:  0.7265625
train loss:  0.5085884928703308
train gradient:  0.14877588124366842
iteration : 11321
train acc:  0.71875
train loss:  0.5370795726776123
train gradient:  0.17605167209465505
iteration : 11322
train acc:  0.8046875
train loss:  0.4554910957813263
train gradient:  0.11597529985706807
iteration : 11323
train acc:  0.7421875
train loss:  0.5332757830619812
train gradient:  0.244315169599794
iteration : 11324
train acc:  0.78125
train loss:  0.5034459829330444
train gradient:  0.13357902361251056
iteration : 11325
train acc:  0.8203125
train loss:  0.4081445634365082
train gradient:  0.09897707430846256
iteration : 11326
train acc:  0.7578125
train loss:  0.5079956650733948
train gradient:  0.11827973050296473
iteration : 11327
train acc:  0.7421875
train loss:  0.4848480820655823
train gradient:  0.11909698420994995
iteration : 11328
train acc:  0.734375
train loss:  0.5598095059394836
train gradient:  0.19558969411419164
iteration : 11329
train acc:  0.71875
train loss:  0.4985594153404236
train gradient:  0.13614049210760498
iteration : 11330
train acc:  0.71875
train loss:  0.521283745765686
train gradient:  0.13749166996151607
iteration : 11331
train acc:  0.7109375
train loss:  0.5282121896743774
train gradient:  0.1280951302748441
iteration : 11332
train acc:  0.75
train loss:  0.5085595846176147
train gradient:  0.14298113845439353
iteration : 11333
train acc:  0.7890625
train loss:  0.46192261576652527
train gradient:  0.12629370273330967
iteration : 11334
train acc:  0.7109375
train loss:  0.4818035066127777
train gradient:  0.1397462163494559
iteration : 11335
train acc:  0.734375
train loss:  0.504407525062561
train gradient:  0.13702517847988718
iteration : 11336
train acc:  0.7578125
train loss:  0.4985524117946625
train gradient:  0.10806443837242811
iteration : 11337
train acc:  0.75
train loss:  0.4771089553833008
train gradient:  0.12081504514514597
iteration : 11338
train acc:  0.8046875
train loss:  0.4186158776283264
train gradient:  0.09507415978361732
iteration : 11339
train acc:  0.703125
train loss:  0.5072743892669678
train gradient:  0.124137160980447
iteration : 11340
train acc:  0.75
train loss:  0.45329296588897705
train gradient:  0.104382809495026
iteration : 11341
train acc:  0.734375
train loss:  0.5612739324569702
train gradient:  0.1842938938333729
iteration : 11342
train acc:  0.75
train loss:  0.5189911723136902
train gradient:  0.14907196434048475
iteration : 11343
train acc:  0.75
train loss:  0.5148348808288574
train gradient:  0.14183696401667054
iteration : 11344
train acc:  0.71875
train loss:  0.5070363283157349
train gradient:  0.1465044917323458
iteration : 11345
train acc:  0.734375
train loss:  0.5026358366012573
train gradient:  0.11384858610552863
iteration : 11346
train acc:  0.765625
train loss:  0.505272626876831
train gradient:  0.197373116950382
iteration : 11347
train acc:  0.7890625
train loss:  0.5034582018852234
train gradient:  0.1148859663494866
iteration : 11348
train acc:  0.7890625
train loss:  0.4310673475265503
train gradient:  0.11690740211922131
iteration : 11349
train acc:  0.7734375
train loss:  0.45240527391433716
train gradient:  0.14277982653029547
iteration : 11350
train acc:  0.796875
train loss:  0.47441771626472473
train gradient:  0.13981371387704727
iteration : 11351
train acc:  0.734375
train loss:  0.4796822965145111
train gradient:  0.1000710678236344
iteration : 11352
train acc:  0.75
train loss:  0.5137506723403931
train gradient:  0.12953364672483578
iteration : 11353
train acc:  0.734375
train loss:  0.48145800828933716
train gradient:  0.138825089931544
iteration : 11354
train acc:  0.7109375
train loss:  0.47058552503585815
train gradient:  0.10300943442415562
iteration : 11355
train acc:  0.75
train loss:  0.4824082553386688
train gradient:  0.13999872795076296
iteration : 11356
train acc:  0.7109375
train loss:  0.46261727809906006
train gradient:  0.10534301748337471
iteration : 11357
train acc:  0.78125
train loss:  0.47528383135795593
train gradient:  0.11901241237879297
iteration : 11358
train acc:  0.7890625
train loss:  0.46485087275505066
train gradient:  0.14386030177898348
iteration : 11359
train acc:  0.65625
train loss:  0.6317065954208374
train gradient:  0.1819416507959382
iteration : 11360
train acc:  0.7578125
train loss:  0.4997711777687073
train gradient:  0.11640228626594465
iteration : 11361
train acc:  0.7578125
train loss:  0.49820268154144287
train gradient:  0.14548078752888582
iteration : 11362
train acc:  0.734375
train loss:  0.535055935382843
train gradient:  0.2001137368689611
iteration : 11363
train acc:  0.7265625
train loss:  0.5066516399383545
train gradient:  0.1557853558615998
iteration : 11364
train acc:  0.7265625
train loss:  0.52776038646698
train gradient:  0.20712330851116367
iteration : 11365
train acc:  0.7890625
train loss:  0.46094876527786255
train gradient:  0.12007710564319753
iteration : 11366
train acc:  0.7421875
train loss:  0.5043644905090332
train gradient:  0.14000374588124587
iteration : 11367
train acc:  0.6953125
train loss:  0.5581498146057129
train gradient:  0.15521231039065325
iteration : 11368
train acc:  0.765625
train loss:  0.4447745084762573
train gradient:  0.09294518825997404
iteration : 11369
train acc:  0.765625
train loss:  0.4548894166946411
train gradient:  0.10208653960044044
iteration : 11370
train acc:  0.765625
train loss:  0.45054033398628235
train gradient:  0.1011111733026777
iteration : 11371
train acc:  0.796875
train loss:  0.45111143589019775
train gradient:  0.09903907250903786
iteration : 11372
train acc:  0.7734375
train loss:  0.5119799375534058
train gradient:  0.16242690802687015
iteration : 11373
train acc:  0.7578125
train loss:  0.5454202890396118
train gradient:  0.1260591896638415
iteration : 11374
train acc:  0.78125
train loss:  0.4379035234451294
train gradient:  0.10606266102239066
iteration : 11375
train acc:  0.765625
train loss:  0.46894586086273193
train gradient:  0.1152101444332178
iteration : 11376
train acc:  0.796875
train loss:  0.452226459980011
train gradient:  0.11143612050755655
iteration : 11377
train acc:  0.78125
train loss:  0.5140033960342407
train gradient:  0.162479159979532
iteration : 11378
train acc:  0.6875
train loss:  0.5451744794845581
train gradient:  0.15245576804147007
iteration : 11379
train acc:  0.75
train loss:  0.508206844329834
train gradient:  0.13405267970580945
iteration : 11380
train acc:  0.71875
train loss:  0.4713824391365051
train gradient:  0.14102710492053933
iteration : 11381
train acc:  0.703125
train loss:  0.48840856552124023
train gradient:  0.11255711555164044
iteration : 11382
train acc:  0.7734375
train loss:  0.48722556233406067
train gradient:  0.1386469964279305
iteration : 11383
train acc:  0.7421875
train loss:  0.48429834842681885
train gradient:  0.14071282778245808
iteration : 11384
train acc:  0.703125
train loss:  0.527290940284729
train gradient:  0.11471096234450205
iteration : 11385
train acc:  0.78125
train loss:  0.4691064655780792
train gradient:  0.16187988836347728
iteration : 11386
train acc:  0.6953125
train loss:  0.5301157832145691
train gradient:  0.1344210860406741
iteration : 11387
train acc:  0.796875
train loss:  0.48470252752304077
train gradient:  0.14839481753970918
iteration : 11388
train acc:  0.7265625
train loss:  0.49921661615371704
train gradient:  0.13609592382156152
iteration : 11389
train acc:  0.8359375
train loss:  0.3960336744785309
train gradient:  0.09656282755458308
iteration : 11390
train acc:  0.7265625
train loss:  0.5154224634170532
train gradient:  0.17140774346905532
iteration : 11391
train acc:  0.78125
train loss:  0.4808086156845093
train gradient:  0.14943349742257273
iteration : 11392
train acc:  0.734375
train loss:  0.519153356552124
train gradient:  0.13585727395638597
iteration : 11393
train acc:  0.703125
train loss:  0.5512179136276245
train gradient:  0.187396190143349
iteration : 11394
train acc:  0.7265625
train loss:  0.5782874822616577
train gradient:  0.27037642315909144
iteration : 11395
train acc:  0.7265625
train loss:  0.5520853400230408
train gradient:  0.16270641637841876
iteration : 11396
train acc:  0.75
train loss:  0.46391481161117554
train gradient:  0.11337489529446368
iteration : 11397
train acc:  0.7421875
train loss:  0.4942461848258972
train gradient:  0.11289181694804765
iteration : 11398
train acc:  0.7109375
train loss:  0.5483070015907288
train gradient:  0.15371926680771628
iteration : 11399
train acc:  0.734375
train loss:  0.4962533414363861
train gradient:  0.12352335700603562
iteration : 11400
train acc:  0.765625
train loss:  0.4934759736061096
train gradient:  0.14284725139137242
iteration : 11401
train acc:  0.7421875
train loss:  0.45711296796798706
train gradient:  0.11338778288124807
iteration : 11402
train acc:  0.734375
train loss:  0.5341121554374695
train gradient:  0.12489297452284497
iteration : 11403
train acc:  0.65625
train loss:  0.6134090423583984
train gradient:  0.16273651657834307
iteration : 11404
train acc:  0.7265625
train loss:  0.5026946067810059
train gradient:  0.09532451307303808
iteration : 11405
train acc:  0.7890625
train loss:  0.4347621202468872
train gradient:  0.1579517137176531
iteration : 11406
train acc:  0.7421875
train loss:  0.47470366954803467
train gradient:  0.14186639468394677
iteration : 11407
train acc:  0.7578125
train loss:  0.4751277565956116
train gradient:  0.10945328943362949
iteration : 11408
train acc:  0.640625
train loss:  0.6541613936424255
train gradient:  0.20351309951898533
iteration : 11409
train acc:  0.6640625
train loss:  0.5801915526390076
train gradient:  0.12882210643690872
iteration : 11410
train acc:  0.7421875
train loss:  0.4973140358924866
train gradient:  0.11740014673132945
iteration : 11411
train acc:  0.765625
train loss:  0.4304078221321106
train gradient:  0.10560421730047274
iteration : 11412
train acc:  0.7734375
train loss:  0.47524046897888184
train gradient:  0.11777398358393029
iteration : 11413
train acc:  0.796875
train loss:  0.45237231254577637
train gradient:  0.10836548921383618
iteration : 11414
train acc:  0.78125
train loss:  0.4531002342700958
train gradient:  0.12147245747018706
iteration : 11415
train acc:  0.7109375
train loss:  0.5182911157608032
train gradient:  0.14830269559170345
iteration : 11416
train acc:  0.7109375
train loss:  0.5164830684661865
train gradient:  0.1768696290596141
iteration : 11417
train acc:  0.75
train loss:  0.521579384803772
train gradient:  0.13549866590286638
iteration : 11418
train acc:  0.6953125
train loss:  0.5716130137443542
train gradient:  0.15981439344760218
iteration : 11419
train acc:  0.6953125
train loss:  0.5182018280029297
train gradient:  0.13032518523041894
iteration : 11420
train acc:  0.7265625
train loss:  0.4815506041049957
train gradient:  0.121064632522896
iteration : 11421
train acc:  0.7734375
train loss:  0.48349136114120483
train gradient:  0.1247515799835348
iteration : 11422
train acc:  0.75
train loss:  0.46631625294685364
train gradient:  0.16580851428172882
iteration : 11423
train acc:  0.703125
train loss:  0.5776616334915161
train gradient:  0.18096666355079766
iteration : 11424
train acc:  0.8046875
train loss:  0.4536990523338318
train gradient:  0.10421946985400228
iteration : 11425
train acc:  0.8125
train loss:  0.40576377511024475
train gradient:  0.11101401504664923
iteration : 11426
train acc:  0.7734375
train loss:  0.4411976635456085
train gradient:  0.1024821331867847
iteration : 11427
train acc:  0.7265625
train loss:  0.4707643389701843
train gradient:  0.14125384986384953
iteration : 11428
train acc:  0.78125
train loss:  0.4433809816837311
train gradient:  0.12739138392223226
iteration : 11429
train acc:  0.734375
train loss:  0.4930824041366577
train gradient:  0.10410361071254853
iteration : 11430
train acc:  0.734375
train loss:  0.5091400742530823
train gradient:  0.13885836649953115
iteration : 11431
train acc:  0.71875
train loss:  0.5573956370353699
train gradient:  0.15117482857376036
iteration : 11432
train acc:  0.75
train loss:  0.48462149500846863
train gradient:  0.12192130920969936
iteration : 11433
train acc:  0.7265625
train loss:  0.5289371013641357
train gradient:  0.11221563848397148
iteration : 11434
train acc:  0.734375
train loss:  0.5073968172073364
train gradient:  0.13362335105904738
iteration : 11435
train acc:  0.828125
train loss:  0.43421730399131775
train gradient:  0.0789737768800111
iteration : 11436
train acc:  0.6796875
train loss:  0.5482063293457031
train gradient:  0.14963589393548235
iteration : 11437
train acc:  0.8046875
train loss:  0.4534229338169098
train gradient:  0.1040741238385636
iteration : 11438
train acc:  0.671875
train loss:  0.5681288242340088
train gradient:  0.16539603940457764
iteration : 11439
train acc:  0.8046875
train loss:  0.4099445939064026
train gradient:  0.1147247326637141
iteration : 11440
train acc:  0.7578125
train loss:  0.5134786367416382
train gradient:  0.14151354495922958
iteration : 11441
train acc:  0.6875
train loss:  0.5506808757781982
train gradient:  0.13754862670234502
iteration : 11442
train acc:  0.75
train loss:  0.4897386133670807
train gradient:  0.1406117725535248
iteration : 11443
train acc:  0.671875
train loss:  0.5661947727203369
train gradient:  0.13816254046327536
iteration : 11444
train acc:  0.75
train loss:  0.5026098489761353
train gradient:  0.11721021361260554
iteration : 11445
train acc:  0.6875
train loss:  0.5006513595581055
train gradient:  0.11216046450140345
iteration : 11446
train acc:  0.65625
train loss:  0.5145512223243713
train gradient:  0.12342783540192846
iteration : 11447
train acc:  0.6875
train loss:  0.4732022285461426
train gradient:  0.10674547757411686
iteration : 11448
train acc:  0.765625
train loss:  0.48298293352127075
train gradient:  0.13477497677887446
iteration : 11449
train acc:  0.7734375
train loss:  0.4599013328552246
train gradient:  0.11265808654210822
iteration : 11450
train acc:  0.703125
train loss:  0.5272114276885986
train gradient:  0.13695566203517867
iteration : 11451
train acc:  0.765625
train loss:  0.4572742283344269
train gradient:  0.09126872331372149
iteration : 11452
train acc:  0.7109375
train loss:  0.5209962725639343
train gradient:  0.1305963396705963
iteration : 11453
train acc:  0.78125
train loss:  0.44198286533355713
train gradient:  0.10294186499578026
iteration : 11454
train acc:  0.78125
train loss:  0.4574858546257019
train gradient:  0.09059348311878276
iteration : 11455
train acc:  0.7578125
train loss:  0.5071742534637451
train gradient:  0.12735030063459923
iteration : 11456
train acc:  0.6875
train loss:  0.5615761280059814
train gradient:  0.1781902978561451
iteration : 11457
train acc:  0.7734375
train loss:  0.45012223720550537
train gradient:  0.134752016232783
iteration : 11458
train acc:  0.6875
train loss:  0.5281678438186646
train gradient:  0.16524095737921918
iteration : 11459
train acc:  0.765625
train loss:  0.4655115604400635
train gradient:  0.0946928046254559
iteration : 11460
train acc:  0.7265625
train loss:  0.5515204668045044
train gradient:  0.16871866380709039
iteration : 11461
train acc:  0.734375
train loss:  0.5185198783874512
train gradient:  0.14621641912506253
iteration : 11462
train acc:  0.75
train loss:  0.48942169547080994
train gradient:  0.11688772133706118
iteration : 11463
train acc:  0.7421875
train loss:  0.4773463308811188
train gradient:  0.13979286156186974
iteration : 11464
train acc:  0.8125
train loss:  0.4545108377933502
train gradient:  0.10271590365603253
iteration : 11465
train acc:  0.7109375
train loss:  0.5072482824325562
train gradient:  0.13596151754170321
iteration : 11466
train acc:  0.7734375
train loss:  0.43262290954589844
train gradient:  0.09597300545989372
iteration : 11467
train acc:  0.734375
train loss:  0.4821929633617401
train gradient:  0.12719369332701325
iteration : 11468
train acc:  0.7421875
train loss:  0.5056829452514648
train gradient:  0.1254363276793229
iteration : 11469
train acc:  0.7890625
train loss:  0.4668571650981903
train gradient:  0.13298549114099317
iteration : 11470
train acc:  0.734375
train loss:  0.5161116123199463
train gradient:  0.15711728563039123
iteration : 11471
train acc:  0.703125
train loss:  0.5606564283370972
train gradient:  0.16473119635973674
iteration : 11472
train acc:  0.734375
train loss:  0.48900550603866577
train gradient:  0.1387296651505574
iteration : 11473
train acc:  0.765625
train loss:  0.4419160485267639
train gradient:  0.1225844578417816
iteration : 11474
train acc:  0.78125
train loss:  0.46737101674079895
train gradient:  0.13225542991177025
iteration : 11475
train acc:  0.6875
train loss:  0.5548439621925354
train gradient:  0.1475282288277699
iteration : 11476
train acc:  0.796875
train loss:  0.42530012130737305
train gradient:  0.07706798134562169
iteration : 11477
train acc:  0.7109375
train loss:  0.5372135639190674
train gradient:  0.12492355813563265
iteration : 11478
train acc:  0.65625
train loss:  0.5480311512947083
train gradient:  0.14793063807576212
iteration : 11479
train acc:  0.78125
train loss:  0.4842452108860016
train gradient:  0.12663694339331638
iteration : 11480
train acc:  0.8125
train loss:  0.4506344497203827
train gradient:  0.12052765119322076
iteration : 11481
train acc:  0.7890625
train loss:  0.45617425441741943
train gradient:  0.10945342895763843
iteration : 11482
train acc:  0.7265625
train loss:  0.540319561958313
train gradient:  0.18020671182555553
iteration : 11483
train acc:  0.8125
train loss:  0.40102821588516235
train gradient:  0.09685765964619135
iteration : 11484
train acc:  0.7109375
train loss:  0.5121584534645081
train gradient:  0.11743472790263777
iteration : 11485
train acc:  0.7578125
train loss:  0.5256370902061462
train gradient:  0.14402791204390786
iteration : 11486
train acc:  0.7734375
train loss:  0.5030928254127502
train gradient:  0.1347676403545463
iteration : 11487
train acc:  0.7265625
train loss:  0.4816029667854309
train gradient:  0.09635669431163262
iteration : 11488
train acc:  0.7421875
train loss:  0.4818730354309082
train gradient:  0.1212529788159622
iteration : 11489
train acc:  0.71875
train loss:  0.4818188548088074
train gradient:  0.1286200650235978
iteration : 11490
train acc:  0.7578125
train loss:  0.5070169568061829
train gradient:  0.1431169332194102
iteration : 11491
train acc:  0.8125
train loss:  0.41662049293518066
train gradient:  0.09367998090713102
iteration : 11492
train acc:  0.71875
train loss:  0.5817853212356567
train gradient:  0.18017772245176397
iteration : 11493
train acc:  0.7109375
train loss:  0.5509721040725708
train gradient:  0.1826277152856991
iteration : 11494
train acc:  0.7734375
train loss:  0.4794861674308777
train gradient:  0.13323339526884206
iteration : 11495
train acc:  0.6796875
train loss:  0.5321882963180542
train gradient:  0.1275688636476733
iteration : 11496
train acc:  0.7421875
train loss:  0.4371396601200104
train gradient:  0.09856453112784455
iteration : 11497
train acc:  0.765625
train loss:  0.4334474802017212
train gradient:  0.12451508716970947
iteration : 11498
train acc:  0.7421875
train loss:  0.5161138772964478
train gradient:  0.11317540000222141
iteration : 11499
train acc:  0.78125
train loss:  0.4408138394355774
train gradient:  0.12782177348699852
iteration : 11500
train acc:  0.7109375
train loss:  0.529733419418335
train gradient:  0.163724281865525
iteration : 11501
train acc:  0.7265625
train loss:  0.4915792942047119
train gradient:  0.17965790085526123
iteration : 11502
train acc:  0.7421875
train loss:  0.4668019413948059
train gradient:  0.13188402532623028
iteration : 11503
train acc:  0.796875
train loss:  0.40865179896354675
train gradient:  0.11612124871741654
iteration : 11504
train acc:  0.78125
train loss:  0.45716923475265503
train gradient:  0.10252760684329451
iteration : 11505
train acc:  0.7578125
train loss:  0.46523821353912354
train gradient:  0.10970316614361879
iteration : 11506
train acc:  0.75
train loss:  0.5019023418426514
train gradient:  0.14548118979821192
iteration : 11507
train acc:  0.734375
train loss:  0.44563865661621094
train gradient:  0.12066010221137409
iteration : 11508
train acc:  0.7421875
train loss:  0.47274520993232727
train gradient:  0.13368322631328064
iteration : 11509
train acc:  0.765625
train loss:  0.4581131637096405
train gradient:  0.1228574525597115
iteration : 11510
train acc:  0.7890625
train loss:  0.4977489113807678
train gradient:  0.14110571593453403
iteration : 11511
train acc:  0.734375
train loss:  0.4543164372444153
train gradient:  0.0905511500657255
iteration : 11512
train acc:  0.796875
train loss:  0.43019479513168335
train gradient:  0.09831400355215762
iteration : 11513
train acc:  0.78125
train loss:  0.5162123441696167
train gradient:  0.1198432266481274
iteration : 11514
train acc:  0.765625
train loss:  0.45445752143859863
train gradient:  0.09960790186790867
iteration : 11515
train acc:  0.75
train loss:  0.5017969608306885
train gradient:  0.12142068725877185
iteration : 11516
train acc:  0.6640625
train loss:  0.5547701716423035
train gradient:  0.16591220191939432
iteration : 11517
train acc:  0.78125
train loss:  0.4266478419303894
train gradient:  0.08766968354755454
iteration : 11518
train acc:  0.7421875
train loss:  0.49309390783309937
train gradient:  0.15479734871068412
iteration : 11519
train acc:  0.6875
train loss:  0.5151735544204712
train gradient:  0.1360076630708506
iteration : 11520
train acc:  0.6875
train loss:  0.5500879883766174
train gradient:  0.19226267295910843
iteration : 11521
train acc:  0.7421875
train loss:  0.46596115827560425
train gradient:  0.11600997400862333
iteration : 11522
train acc:  0.78125
train loss:  0.45841145515441895
train gradient:  0.12520268131146312
iteration : 11523
train acc:  0.8359375
train loss:  0.43524760007858276
train gradient:  0.10221779258351998
iteration : 11524
train acc:  0.765625
train loss:  0.5131623148918152
train gradient:  0.12425402888283386
iteration : 11525
train acc:  0.7734375
train loss:  0.5187884569168091
train gradient:  0.18961724216432385
iteration : 11526
train acc:  0.75
train loss:  0.5128114223480225
train gradient:  0.14928478470300488
iteration : 11527
train acc:  0.7578125
train loss:  0.4307563900947571
train gradient:  0.09786127436788426
iteration : 11528
train acc:  0.7265625
train loss:  0.49907994270324707
train gradient:  0.14097374306822733
iteration : 11529
train acc:  0.7890625
train loss:  0.4234645962715149
train gradient:  0.08932646413790812
iteration : 11530
train acc:  0.8046875
train loss:  0.42115193605422974
train gradient:  0.11798182240332261
iteration : 11531
train acc:  0.7109375
train loss:  0.5357274413108826
train gradient:  0.17636427046493625
iteration : 11532
train acc:  0.7890625
train loss:  0.4451465904712677
train gradient:  0.11220819144309678
iteration : 11533
train acc:  0.7421875
train loss:  0.4808138608932495
train gradient:  0.18387382217941928
iteration : 11534
train acc:  0.71875
train loss:  0.5354496240615845
train gradient:  0.15207382341218062
iteration : 11535
train acc:  0.6953125
train loss:  0.5076766014099121
train gradient:  0.11712905171519158
iteration : 11536
train acc:  0.75
train loss:  0.4677162766456604
train gradient:  0.11648234134164644
iteration : 11537
train acc:  0.7734375
train loss:  0.4905751049518585
train gradient:  0.12256824024646215
iteration : 11538
train acc:  0.703125
train loss:  0.5544228553771973
train gradient:  0.17680136272417973
iteration : 11539
train acc:  0.7109375
train loss:  0.5224826335906982
train gradient:  0.14362645240684224
iteration : 11540
train acc:  0.71875
train loss:  0.5217435359954834
train gradient:  0.17476340664714846
iteration : 11541
train acc:  0.7265625
train loss:  0.5067000389099121
train gradient:  0.11617571868704502
iteration : 11542
train acc:  0.734375
train loss:  0.5051548480987549
train gradient:  0.13231518934012723
iteration : 11543
train acc:  0.7578125
train loss:  0.5174047946929932
train gradient:  0.16445169058360107
iteration : 11544
train acc:  0.6796875
train loss:  0.5557960867881775
train gradient:  0.159177331414056
iteration : 11545
train acc:  0.75
train loss:  0.4758467674255371
train gradient:  0.12076423943604798
iteration : 11546
train acc:  0.734375
train loss:  0.5395749807357788
train gradient:  0.15543836684194
iteration : 11547
train acc:  0.7890625
train loss:  0.42540109157562256
train gradient:  0.12536859061855388
iteration : 11548
train acc:  0.7109375
train loss:  0.5415449738502502
train gradient:  0.1342713947848112
iteration : 11549
train acc:  0.71875
train loss:  0.5453185439109802
train gradient:  0.14612654136981937
iteration : 11550
train acc:  0.6796875
train loss:  0.5279637575149536
train gradient:  0.13067826705505517
iteration : 11551
train acc:  0.6953125
train loss:  0.47687485814094543
train gradient:  0.1486732280730941
iteration : 11552
train acc:  0.734375
train loss:  0.4949510395526886
train gradient:  0.13543615351252553
iteration : 11553
train acc:  0.71875
train loss:  0.5253040790557861
train gradient:  0.13301972251183825
iteration : 11554
train acc:  0.78125
train loss:  0.4528844952583313
train gradient:  0.12242111674627039
iteration : 11555
train acc:  0.7109375
train loss:  0.48202136158943176
train gradient:  0.10631873642007011
iteration : 11556
train acc:  0.78125
train loss:  0.48638415336608887
train gradient:  0.12365733087576036
iteration : 11557
train acc:  0.7734375
train loss:  0.5152883529663086
train gradient:  0.15747935711879374
iteration : 11558
train acc:  0.71875
train loss:  0.5187884569168091
train gradient:  0.14061898682879798
iteration : 11559
train acc:  0.7265625
train loss:  0.49685215950012207
train gradient:  0.14470022115575704
iteration : 11560
train acc:  0.6953125
train loss:  0.5334261655807495
train gradient:  0.15962144386573215
iteration : 11561
train acc:  0.7421875
train loss:  0.47221705317497253
train gradient:  0.102074332225308
iteration : 11562
train acc:  0.7734375
train loss:  0.48430171608924866
train gradient:  0.10830234167097642
iteration : 11563
train acc:  0.78125
train loss:  0.46872609853744507
train gradient:  0.14124441873066557
iteration : 11564
train acc:  0.7109375
train loss:  0.58433997631073
train gradient:  0.20670390145971607
iteration : 11565
train acc:  0.71875
train loss:  0.5229157209396362
train gradient:  0.16091820390207046
iteration : 11566
train acc:  0.734375
train loss:  0.5319936275482178
train gradient:  0.12670583473539182
iteration : 11567
train acc:  0.734375
train loss:  0.5110983848571777
train gradient:  0.13676658951626933
iteration : 11568
train acc:  0.75
train loss:  0.5282297730445862
train gradient:  0.15352608580986626
iteration : 11569
train acc:  0.7109375
train loss:  0.5136048793792725
train gradient:  0.12332501619174334
iteration : 11570
train acc:  0.71875
train loss:  0.5574016571044922
train gradient:  0.1431024832217025
iteration : 11571
train acc:  0.8046875
train loss:  0.4496869444847107
train gradient:  0.12415078772672525
iteration : 11572
train acc:  0.7421875
train loss:  0.5094278454780579
train gradient:  0.11406507332375936
iteration : 11573
train acc:  0.765625
train loss:  0.48341894149780273
train gradient:  0.1570738098176665
iteration : 11574
train acc:  0.8125
train loss:  0.4080308675765991
train gradient:  0.1054366499004438
iteration : 11575
train acc:  0.7578125
train loss:  0.48338067531585693
train gradient:  0.11122098311356846
iteration : 11576
train acc:  0.796875
train loss:  0.43088632822036743
train gradient:  0.09250136495822543
iteration : 11577
train acc:  0.7109375
train loss:  0.5220486521720886
train gradient:  0.15702084509744296
iteration : 11578
train acc:  0.71875
train loss:  0.49896004796028137
train gradient:  0.1450394374689445
iteration : 11579
train acc:  0.7421875
train loss:  0.43519508838653564
train gradient:  0.0844540414248974
iteration : 11580
train acc:  0.5859375
train loss:  0.6192950010299683
train gradient:  0.17283378819513096
iteration : 11581
train acc:  0.78125
train loss:  0.45746368169784546
train gradient:  0.10713034483183724
iteration : 11582
train acc:  0.75
train loss:  0.4574738144874573
train gradient:  0.09637696335245739
iteration : 11583
train acc:  0.6875
train loss:  0.48988857865333557
train gradient:  0.1095518899405971
iteration : 11584
train acc:  0.7421875
train loss:  0.49305132031440735
train gradient:  0.13718302280587558
iteration : 11585
train acc:  0.7578125
train loss:  0.46809259057044983
train gradient:  0.16268053920433706
iteration : 11586
train acc:  0.7734375
train loss:  0.4736611247062683
train gradient:  0.13590473507996093
iteration : 11587
train acc:  0.8359375
train loss:  0.3716410994529724
train gradient:  0.08737285439245558
iteration : 11588
train acc:  0.703125
train loss:  0.5331919193267822
train gradient:  0.13536095196155504
iteration : 11589
train acc:  0.703125
train loss:  0.49490052461624146
train gradient:  0.1250616421747971
iteration : 11590
train acc:  0.7265625
train loss:  0.4923727810382843
train gradient:  0.12601147046293854
iteration : 11591
train acc:  0.7734375
train loss:  0.4540957510471344
train gradient:  0.10260061700252716
iteration : 11592
train acc:  0.75
train loss:  0.5043789148330688
train gradient:  0.13457245001041074
iteration : 11593
train acc:  0.8046875
train loss:  0.49558573961257935
train gradient:  0.1298938988780055
iteration : 11594
train acc:  0.6875
train loss:  0.5109056830406189
train gradient:  0.1665237711432304
iteration : 11595
train acc:  0.7734375
train loss:  0.40964287519454956
train gradient:  0.11892868297277137
iteration : 11596
train acc:  0.71875
train loss:  0.45191699266433716
train gradient:  0.08925844324576226
iteration : 11597
train acc:  0.640625
train loss:  0.5475565195083618
train gradient:  0.13531290916381727
iteration : 11598
train acc:  0.6796875
train loss:  0.5584861040115356
train gradient:  0.16799339711756878
iteration : 11599
train acc:  0.734375
train loss:  0.45722874999046326
train gradient:  0.11571569448598852
iteration : 11600
train acc:  0.7265625
train loss:  0.5028644800186157
train gradient:  0.11699147352714716
iteration : 11601
train acc:  0.7109375
train loss:  0.5925495624542236
train gradient:  0.18329898349405205
iteration : 11602
train acc:  0.7109375
train loss:  0.5129473805427551
train gradient:  0.12655187623891107
iteration : 11603
train acc:  0.734375
train loss:  0.5325270891189575
train gradient:  0.16982568795131425
iteration : 11604
train acc:  0.7109375
train loss:  0.5111498832702637
train gradient:  0.11172457895767338
iteration : 11605
train acc:  0.7890625
train loss:  0.4317879378795624
train gradient:  0.09982949829181509
iteration : 11606
train acc:  0.765625
train loss:  0.5087169408798218
train gradient:  0.14010201385118315
iteration : 11607
train acc:  0.7578125
train loss:  0.47282490134239197
train gradient:  0.12021821351012617
iteration : 11608
train acc:  0.7578125
train loss:  0.5004523396492004
train gradient:  0.16943081596548
iteration : 11609
train acc:  0.7421875
train loss:  0.48715707659721375
train gradient:  0.12717366101401642
iteration : 11610
train acc:  0.7265625
train loss:  0.5433114767074585
train gradient:  0.16593497898871534
iteration : 11611
train acc:  0.703125
train loss:  0.5665990710258484
train gradient:  0.17341529335852074
iteration : 11612
train acc:  0.71875
train loss:  0.47816258668899536
train gradient:  0.1524325434389423
iteration : 11613
train acc:  0.734375
train loss:  0.4710378646850586
train gradient:  0.12068701128472782
iteration : 11614
train acc:  0.7578125
train loss:  0.49055755138397217
train gradient:  0.12410718222661554
iteration : 11615
train acc:  0.78125
train loss:  0.48342016339302063
train gradient:  0.15952039159601814
iteration : 11616
train acc:  0.7578125
train loss:  0.4523656666278839
train gradient:  0.0969132587229435
iteration : 11617
train acc:  0.6875
train loss:  0.5564017295837402
train gradient:  0.1467880968549886
iteration : 11618
train acc:  0.75
train loss:  0.5337979793548584
train gradient:  0.19738544809537392
iteration : 11619
train acc:  0.8125
train loss:  0.46171441674232483
train gradient:  0.10325332389851963
iteration : 11620
train acc:  0.75
train loss:  0.4935179352760315
train gradient:  0.1090013692270323
iteration : 11621
train acc:  0.7734375
train loss:  0.4959685504436493
train gradient:  0.1511669688429791
iteration : 11622
train acc:  0.7109375
train loss:  0.5145835876464844
train gradient:  0.12023160638046082
iteration : 11623
train acc:  0.7421875
train loss:  0.4643520414829254
train gradient:  0.11794218780582069
iteration : 11624
train acc:  0.7265625
train loss:  0.5096217393875122
train gradient:  0.13582534141897468
iteration : 11625
train acc:  0.75
train loss:  0.4852004051208496
train gradient:  0.1254410047985839
iteration : 11626
train acc:  0.7578125
train loss:  0.4717505872249603
train gradient:  0.09971036284655407
iteration : 11627
train acc:  0.7421875
train loss:  0.5204738974571228
train gradient:  0.1372899643856278
iteration : 11628
train acc:  0.7890625
train loss:  0.46336105465888977
train gradient:  0.14216794585607012
iteration : 11629
train acc:  0.7265625
train loss:  0.4660889804363251
train gradient:  0.10669275477077289
iteration : 11630
train acc:  0.6796875
train loss:  0.5994083881378174
train gradient:  0.18651972465844968
iteration : 11631
train acc:  0.703125
train loss:  0.5234561562538147
train gradient:  0.16325827260017117
iteration : 11632
train acc:  0.65625
train loss:  0.5792128443717957
train gradient:  0.18183732103446096
iteration : 11633
train acc:  0.734375
train loss:  0.514423131942749
train gradient:  0.13265614887902843
iteration : 11634
train acc:  0.7421875
train loss:  0.49681249260902405
train gradient:  0.1790306799596629
iteration : 11635
train acc:  0.75
train loss:  0.4567033648490906
train gradient:  0.0925201008733836
iteration : 11636
train acc:  0.765625
train loss:  0.44848567247390747
train gradient:  0.11655536959268428
iteration : 11637
train acc:  0.7734375
train loss:  0.4370955228805542
train gradient:  0.08884921952795333
iteration : 11638
train acc:  0.75
train loss:  0.502043604850769
train gradient:  0.11392798244261286
iteration : 11639
train acc:  0.8203125
train loss:  0.42650967836380005
train gradient:  0.0888532900051932
iteration : 11640
train acc:  0.796875
train loss:  0.4953354597091675
train gradient:  0.128861480670713
iteration : 11641
train acc:  0.703125
train loss:  0.5544625520706177
train gradient:  0.14664279017827195
iteration : 11642
train acc:  0.765625
train loss:  0.49121320247650146
train gradient:  0.12203100685820295
iteration : 11643
train acc:  0.7734375
train loss:  0.46456146240234375
train gradient:  0.12513196223310455
iteration : 11644
train acc:  0.6328125
train loss:  0.637177050113678
train gradient:  0.20228567560476687
iteration : 11645
train acc:  0.7734375
train loss:  0.44813981652259827
train gradient:  0.12359288913260606
iteration : 11646
train acc:  0.7421875
train loss:  0.47574329376220703
train gradient:  0.16175081462782606
iteration : 11647
train acc:  0.796875
train loss:  0.4337749183177948
train gradient:  0.08959908881491466
iteration : 11648
train acc:  0.7109375
train loss:  0.5112596154212952
train gradient:  0.12654590807668983
iteration : 11649
train acc:  0.75
train loss:  0.5108257532119751
train gradient:  0.13650581648300236
iteration : 11650
train acc:  0.78125
train loss:  0.44175654649734497
train gradient:  0.10934139743540197
iteration : 11651
train acc:  0.734375
train loss:  0.48714083433151245
train gradient:  0.1437741801236221
iteration : 11652
train acc:  0.7578125
train loss:  0.5180389285087585
train gradient:  0.153345662261911
iteration : 11653
train acc:  0.7265625
train loss:  0.5201654434204102
train gradient:  0.11774147361452265
iteration : 11654
train acc:  0.8046875
train loss:  0.45385485887527466
train gradient:  0.12135609253804336
iteration : 11655
train acc:  0.6953125
train loss:  0.48379456996917725
train gradient:  0.11294777045143815
iteration : 11656
train acc:  0.7578125
train loss:  0.49215269088745117
train gradient:  0.1344469164736395
iteration : 11657
train acc:  0.78125
train loss:  0.4398456811904907
train gradient:  0.1058959054752479
iteration : 11658
train acc:  0.7265625
train loss:  0.5638251304626465
train gradient:  0.17960338886831734
iteration : 11659
train acc:  0.71875
train loss:  0.5382808446884155
train gradient:  0.14118052375510082
iteration : 11660
train acc:  0.8125
train loss:  0.4238513708114624
train gradient:  0.0985421892858812
iteration : 11661
train acc:  0.8515625
train loss:  0.3864484429359436
train gradient:  0.1118804707788244
iteration : 11662
train acc:  0.7421875
train loss:  0.4724959433078766
train gradient:  0.11158091983881809
iteration : 11663
train acc:  0.7578125
train loss:  0.46368861198425293
train gradient:  0.10170640511199888
iteration : 11664
train acc:  0.7265625
train loss:  0.5454425811767578
train gradient:  0.17573718611273953
iteration : 11665
train acc:  0.7265625
train loss:  0.5008875727653503
train gradient:  0.11563739561861862
iteration : 11666
train acc:  0.765625
train loss:  0.4854479134082794
train gradient:  0.1067942420592939
iteration : 11667
train acc:  0.78125
train loss:  0.46646755933761597
train gradient:  0.11218618622088032
iteration : 11668
train acc:  0.734375
train loss:  0.5062271356582642
train gradient:  0.12556528650643278
iteration : 11669
train acc:  0.734375
train loss:  0.5057012438774109
train gradient:  0.11834860010688934
iteration : 11670
train acc:  0.6953125
train loss:  0.5434337854385376
train gradient:  0.15400770867482128
iteration : 11671
train acc:  0.765625
train loss:  0.49640706181526184
train gradient:  0.11483301175522628
iteration : 11672
train acc:  0.71875
train loss:  0.4749835729598999
train gradient:  0.11117059197415692
iteration : 11673
train acc:  0.6796875
train loss:  0.5647010803222656
train gradient:  0.16192481973489187
iteration : 11674
train acc:  0.7578125
train loss:  0.4880814552307129
train gradient:  0.13409730883314694
iteration : 11675
train acc:  0.71875
train loss:  0.5524003505706787
train gradient:  0.23044627406513216
iteration : 11676
train acc:  0.671875
train loss:  0.5266075134277344
train gradient:  0.14203889194817115
iteration : 11677
train acc:  0.7890625
train loss:  0.4996209740638733
train gradient:  0.12067750612231018
iteration : 11678
train acc:  0.65625
train loss:  0.5532665848731995
train gradient:  0.16121557909044593
iteration : 11679
train acc:  0.796875
train loss:  0.4175824522972107
train gradient:  0.13095040890066434
iteration : 11680
train acc:  0.8125
train loss:  0.39613106846809387
train gradient:  0.08870685163122938
iteration : 11681
train acc:  0.6796875
train loss:  0.5241161584854126
train gradient:  0.12262311526303651
iteration : 11682
train acc:  0.7578125
train loss:  0.5059998035430908
train gradient:  0.12331010059387106
iteration : 11683
train acc:  0.7265625
train loss:  0.4743832051753998
train gradient:  0.13216313241184946
iteration : 11684
train acc:  0.7734375
train loss:  0.45551347732543945
train gradient:  0.12150888448600915
iteration : 11685
train acc:  0.75
train loss:  0.5093945264816284
train gradient:  0.1490913514793924
iteration : 11686
train acc:  0.734375
train loss:  0.5208025574684143
train gradient:  0.1616214832447569
iteration : 11687
train acc:  0.765625
train loss:  0.5360066890716553
train gradient:  0.13232082235581005
iteration : 11688
train acc:  0.6484375
train loss:  0.5739781856536865
train gradient:  0.1367617636400374
iteration : 11689
train acc:  0.8125
train loss:  0.4332519769668579
train gradient:  0.14245602222579437
iteration : 11690
train acc:  0.75
train loss:  0.48995423316955566
train gradient:  0.10896548252297011
iteration : 11691
train acc:  0.7578125
train loss:  0.4791715741157532
train gradient:  0.14795990120044336
iteration : 11692
train acc:  0.7578125
train loss:  0.4746854305267334
train gradient:  0.12570874976688878
iteration : 11693
train acc:  0.7109375
train loss:  0.5055535435676575
train gradient:  0.11285531753024747
iteration : 11694
train acc:  0.75
train loss:  0.47490301728248596
train gradient:  0.11715325946869143
iteration : 11695
train acc:  0.765625
train loss:  0.49606087803840637
train gradient:  0.13246044744995228
iteration : 11696
train acc:  0.7578125
train loss:  0.4481203556060791
train gradient:  0.10627695316322165
iteration : 11697
train acc:  0.7890625
train loss:  0.4348825216293335
train gradient:  0.1682613728787986
iteration : 11698
train acc:  0.765625
train loss:  0.43198370933532715
train gradient:  0.1185171897030337
iteration : 11699
train acc:  0.8203125
train loss:  0.4156545400619507
train gradient:  0.10925615360346841
iteration : 11700
train acc:  0.671875
train loss:  0.5429457426071167
train gradient:  0.14222015155472223
iteration : 11701
train acc:  0.7421875
train loss:  0.4998050034046173
train gradient:  0.12501130130392993
iteration : 11702
train acc:  0.7265625
train loss:  0.4923500418663025
train gradient:  0.16847165093664246
iteration : 11703
train acc:  0.75
train loss:  0.4633457064628601
train gradient:  0.10631924505545666
iteration : 11704
train acc:  0.8359375
train loss:  0.40611356496810913
train gradient:  0.11561585194594165
iteration : 11705
train acc:  0.734375
train loss:  0.4862392544746399
train gradient:  0.11077204544567716
iteration : 11706
train acc:  0.6953125
train loss:  0.5601905584335327
train gradient:  0.15267694648950694
iteration : 11707
train acc:  0.78125
train loss:  0.43476858735084534
train gradient:  0.10433230866306767
iteration : 11708
train acc:  0.703125
train loss:  0.5432519316673279
train gradient:  0.14408353768537974
iteration : 11709
train acc:  0.75
train loss:  0.4635910987854004
train gradient:  0.12380148802316374
iteration : 11710
train acc:  0.6875
train loss:  0.5485672950744629
train gradient:  0.15087221329910655
iteration : 11711
train acc:  0.7265625
train loss:  0.46402037143707275
train gradient:  0.10317628840725802
iteration : 11712
train acc:  0.765625
train loss:  0.4513709843158722
train gradient:  0.13448862087796293
iteration : 11713
train acc:  0.765625
train loss:  0.5432984232902527
train gradient:  0.1301850989642923
iteration : 11714
train acc:  0.7578125
train loss:  0.47050315141677856
train gradient:  0.15623753158176068
iteration : 11715
train acc:  0.78125
train loss:  0.4925273060798645
train gradient:  0.13911419130674763
iteration : 11716
train acc:  0.71875
train loss:  0.5271192789077759
train gradient:  0.12447663221448357
iteration : 11717
train acc:  0.7421875
train loss:  0.46720361709594727
train gradient:  0.10571779496319215
iteration : 11718
train acc:  0.7421875
train loss:  0.4877321720123291
train gradient:  0.1276535738253336
iteration : 11719
train acc:  0.7265625
train loss:  0.5269888639450073
train gradient:  0.13320034876421483
iteration : 11720
train acc:  0.7421875
train loss:  0.47920188307762146
train gradient:  0.11331974596901824
iteration : 11721
train acc:  0.78125
train loss:  0.4901294708251953
train gradient:  0.13180167185271116
iteration : 11722
train acc:  0.75
train loss:  0.4755952060222626
train gradient:  0.13387901208574227
iteration : 11723
train acc:  0.734375
train loss:  0.4720788598060608
train gradient:  0.1366875263984092
iteration : 11724
train acc:  0.71875
train loss:  0.4780674874782562
train gradient:  0.10802997936976104
iteration : 11725
train acc:  0.75
train loss:  0.530835747718811
train gradient:  0.17825824546980634
iteration : 11726
train acc:  0.7734375
train loss:  0.43841439485549927
train gradient:  0.10493991281620024
iteration : 11727
train acc:  0.7734375
train loss:  0.46103358268737793
train gradient:  0.11240710712353756
iteration : 11728
train acc:  0.828125
train loss:  0.38072869181632996
train gradient:  0.08992180135438928
iteration : 11729
train acc:  0.734375
train loss:  0.5039151906967163
train gradient:  0.1371867752838115
iteration : 11730
train acc:  0.7421875
train loss:  0.48820167779922485
train gradient:  0.10700554305040473
iteration : 11731
train acc:  0.765625
train loss:  0.460423082113266
train gradient:  0.10936644071028244
iteration : 11732
train acc:  0.7890625
train loss:  0.43905317783355713
train gradient:  0.08302686254277114
iteration : 11733
train acc:  0.7578125
train loss:  0.47435295581817627
train gradient:  0.1037063318613558
iteration : 11734
train acc:  0.71875
train loss:  0.5479766130447388
train gradient:  0.1644331405376458
iteration : 11735
train acc:  0.796875
train loss:  0.45132017135620117
train gradient:  0.13014577645914827
iteration : 11736
train acc:  0.75
train loss:  0.4693295955657959
train gradient:  0.09238857392825822
iteration : 11737
train acc:  0.71875
train loss:  0.5008732080459595
train gradient:  0.11912206787895459
iteration : 11738
train acc:  0.7421875
train loss:  0.4679284989833832
train gradient:  0.11999748609166251
iteration : 11739
train acc:  0.7890625
train loss:  0.4258401393890381
train gradient:  0.11867554683714414
iteration : 11740
train acc:  0.7578125
train loss:  0.48154568672180176
train gradient:  0.13252070228447393
iteration : 11741
train acc:  0.71875
train loss:  0.5162192583084106
train gradient:  0.14646613101837214
iteration : 11742
train acc:  0.75
train loss:  0.45974257588386536
train gradient:  0.10944042530301708
iteration : 11743
train acc:  0.7421875
train loss:  0.5012997388839722
train gradient:  0.15154610346905262
iteration : 11744
train acc:  0.71875
train loss:  0.524909257888794
train gradient:  0.17794460697654813
iteration : 11745
train acc:  0.7578125
train loss:  0.48691463470458984
train gradient:  0.14411455440357357
iteration : 11746
train acc:  0.7734375
train loss:  0.4522474408149719
train gradient:  0.0998770132466589
iteration : 11747
train acc:  0.734375
train loss:  0.4668574333190918
train gradient:  0.09922891562936757
iteration : 11748
train acc:  0.6484375
train loss:  0.5694510340690613
train gradient:  0.1515648474859346
iteration : 11749
train acc:  0.703125
train loss:  0.5132537484169006
train gradient:  0.1363935317339406
iteration : 11750
train acc:  0.75
train loss:  0.4834285378456116
train gradient:  0.12335460917079864
iteration : 11751
train acc:  0.7265625
train loss:  0.48921167850494385
train gradient:  0.1287313090486226
iteration : 11752
train acc:  0.7578125
train loss:  0.48733827471733093
train gradient:  0.10620137993487226
iteration : 11753
train acc:  0.8125
train loss:  0.4194222092628479
train gradient:  0.10175683197279205
iteration : 11754
train acc:  0.6953125
train loss:  0.5232274532318115
train gradient:  0.1201264773295454
iteration : 11755
train acc:  0.7265625
train loss:  0.4795711934566498
train gradient:  0.1798936001718904
iteration : 11756
train acc:  0.7421875
train loss:  0.5327414274215698
train gradient:  0.15021136465477547
iteration : 11757
train acc:  0.7890625
train loss:  0.43561118841171265
train gradient:  0.09997647855422993
iteration : 11758
train acc:  0.7734375
train loss:  0.46664613485336304
train gradient:  0.11381596140811434
iteration : 11759
train acc:  0.78125
train loss:  0.4476942718029022
train gradient:  0.10011726450692796
iteration : 11760
train acc:  0.671875
train loss:  0.5472208261489868
train gradient:  0.13981764864141055
iteration : 11761
train acc:  0.7265625
train loss:  0.5197885036468506
train gradient:  0.1740272251751077
iteration : 11762
train acc:  0.7890625
train loss:  0.46448951959609985
train gradient:  0.12627542346510245
iteration : 11763
train acc:  0.671875
train loss:  0.593072772026062
train gradient:  0.19908907682790103
iteration : 11764
train acc:  0.796875
train loss:  0.4087827801704407
train gradient:  0.10389542168288037
iteration : 11765
train acc:  0.78125
train loss:  0.44484397768974304
train gradient:  0.13139240514181816
iteration : 11766
train acc:  0.8125
train loss:  0.4208623170852661
train gradient:  0.09742184665853865
iteration : 11767
train acc:  0.7890625
train loss:  0.46188119053840637
train gradient:  0.11715476241075712
iteration : 11768
train acc:  0.7109375
train loss:  0.5731712579727173
train gradient:  0.18797742559947572
iteration : 11769
train acc:  0.8046875
train loss:  0.395404189825058
train gradient:  0.08849863393950234
iteration : 11770
train acc:  0.7578125
train loss:  0.4857647120952606
train gradient:  0.11787936440280664
iteration : 11771
train acc:  0.765625
train loss:  0.43082165718078613
train gradient:  0.09871997224737675
iteration : 11772
train acc:  0.7265625
train loss:  0.48497164249420166
train gradient:  0.11613546779085233
iteration : 11773
train acc:  0.78125
train loss:  0.4871366620063782
train gradient:  0.11027244975548663
iteration : 11774
train acc:  0.734375
train loss:  0.5238323211669922
train gradient:  0.12716595707027256
iteration : 11775
train acc:  0.7265625
train loss:  0.5106192827224731
train gradient:  0.14996968850268916
iteration : 11776
train acc:  0.7421875
train loss:  0.4905538558959961
train gradient:  0.11232235828164494
iteration : 11777
train acc:  0.7578125
train loss:  0.5448015928268433
train gradient:  0.15004155653773543
iteration : 11778
train acc:  0.765625
train loss:  0.4295099377632141
train gradient:  0.11423338797126156
iteration : 11779
train acc:  0.703125
train loss:  0.46259546279907227
train gradient:  0.120506300865312
iteration : 11780
train acc:  0.7265625
train loss:  0.5302892923355103
train gradient:  0.14732568608355265
iteration : 11781
train acc:  0.71875
train loss:  0.4663114845752716
train gradient:  0.1081039979897306
iteration : 11782
train acc:  0.703125
train loss:  0.5750054121017456
train gradient:  0.1966729039847646
iteration : 11783
train acc:  0.75
train loss:  0.4866068661212921
train gradient:  0.11836214156535865
iteration : 11784
train acc:  0.765625
train loss:  0.5091754198074341
train gradient:  0.13727895736866408
iteration : 11785
train acc:  0.7578125
train loss:  0.4517960548400879
train gradient:  0.08528566130701751
iteration : 11786
train acc:  0.7421875
train loss:  0.5026309490203857
train gradient:  0.13024047540912292
iteration : 11787
train acc:  0.7265625
train loss:  0.53961181640625
train gradient:  0.16186962482966977
iteration : 11788
train acc:  0.765625
train loss:  0.5463221073150635
train gradient:  0.18224326341255642
iteration : 11789
train acc:  0.6953125
train loss:  0.5397506952285767
train gradient:  0.19184907964517472
iteration : 11790
train acc:  0.7265625
train loss:  0.5435710549354553
train gradient:  0.16267150817701587
iteration : 11791
train acc:  0.7421875
train loss:  0.5063285827636719
train gradient:  0.19532525409984877
iteration : 11792
train acc:  0.671875
train loss:  0.5129666328430176
train gradient:  0.13169999488653888
iteration : 11793
train acc:  0.7265625
train loss:  0.4849514663219452
train gradient:  0.1285129225772738
iteration : 11794
train acc:  0.734375
train loss:  0.4967139959335327
train gradient:  0.12202441451978409
iteration : 11795
train acc:  0.703125
train loss:  0.5367580652236938
train gradient:  0.18798921013982045
iteration : 11796
train acc:  0.78125
train loss:  0.5131100416183472
train gradient:  0.12987106513413588
iteration : 11797
train acc:  0.7734375
train loss:  0.49858516454696655
train gradient:  0.11968617980400847
iteration : 11798
train acc:  0.7734375
train loss:  0.4554038345813751
train gradient:  0.11763473678610475
iteration : 11799
train acc:  0.75
train loss:  0.5045325756072998
train gradient:  0.20039107141284376
iteration : 11800
train acc:  0.7265625
train loss:  0.5632644891738892
train gradient:  0.19766726835902262
iteration : 11801
train acc:  0.75
train loss:  0.4442442059516907
train gradient:  0.11376675811953894
iteration : 11802
train acc:  0.7421875
train loss:  0.4885100722312927
train gradient:  0.15515497307269394
iteration : 11803
train acc:  0.734375
train loss:  0.5055140256881714
train gradient:  0.12316759218644271
iteration : 11804
train acc:  0.765625
train loss:  0.4818836450576782
train gradient:  0.14057752245419936
iteration : 11805
train acc:  0.78125
train loss:  0.4624679386615753
train gradient:  0.10667655935897036
iteration : 11806
train acc:  0.75
train loss:  0.4861901104450226
train gradient:  0.1340445089745046
iteration : 11807
train acc:  0.6796875
train loss:  0.5551264882087708
train gradient:  0.15811647823172054
iteration : 11808
train acc:  0.7109375
train loss:  0.49893254041671753
train gradient:  0.14426788325354087
iteration : 11809
train acc:  0.7734375
train loss:  0.5181467533111572
train gradient:  0.13473057963583346
iteration : 11810
train acc:  0.765625
train loss:  0.44731029868125916
train gradient:  0.07417952304459041
iteration : 11811
train acc:  0.7421875
train loss:  0.4818646311759949
train gradient:  0.127896903664789
iteration : 11812
train acc:  0.796875
train loss:  0.44342702627182007
train gradient:  0.11892662762807514
iteration : 11813
train acc:  0.7265625
train loss:  0.5133517980575562
train gradient:  0.16316544959827706
iteration : 11814
train acc:  0.8046875
train loss:  0.4646736979484558
train gradient:  0.13350053426595349
iteration : 11815
train acc:  0.71875
train loss:  0.5119081735610962
train gradient:  0.1530013614214682
iteration : 11816
train acc:  0.7421875
train loss:  0.4960664212703705
train gradient:  0.1309860854188668
iteration : 11817
train acc:  0.75
train loss:  0.5165939927101135
train gradient:  0.172845546917747
iteration : 11818
train acc:  0.7265625
train loss:  0.5186141729354858
train gradient:  0.15081415644849785
iteration : 11819
train acc:  0.75
train loss:  0.4773477017879486
train gradient:  0.12705100406770165
iteration : 11820
train acc:  0.7265625
train loss:  0.5470495223999023
train gradient:  0.1338623669735724
iteration : 11821
train acc:  0.734375
train loss:  0.5589526891708374
train gradient:  0.14963277125040067
iteration : 11822
train acc:  0.75
train loss:  0.4755209684371948
train gradient:  0.1279524034747
iteration : 11823
train acc:  0.75
train loss:  0.5224775075912476
train gradient:  0.13333475037340858
iteration : 11824
train acc:  0.7421875
train loss:  0.4543461203575134
train gradient:  0.11441870153762225
iteration : 11825
train acc:  0.7421875
train loss:  0.4780564308166504
train gradient:  0.10843238774021599
iteration : 11826
train acc:  0.78125
train loss:  0.4657907783985138
train gradient:  0.10959641032828443
iteration : 11827
train acc:  0.7578125
train loss:  0.5047292709350586
train gradient:  0.12963320357888194
iteration : 11828
train acc:  0.7109375
train loss:  0.5169947147369385
train gradient:  0.12576651528332028
iteration : 11829
train acc:  0.71875
train loss:  0.5603411197662354
train gradient:  0.20196217406339306
iteration : 11830
train acc:  0.8359375
train loss:  0.4158859848976135
train gradient:  0.0889244884054557
iteration : 11831
train acc:  0.8203125
train loss:  0.4232249855995178
train gradient:  0.07505950942151285
iteration : 11832
train acc:  0.6796875
train loss:  0.5460692644119263
train gradient:  0.14008633347759175
iteration : 11833
train acc:  0.7265625
train loss:  0.49449124932289124
train gradient:  0.12672080306227335
iteration : 11834
train acc:  0.6796875
train loss:  0.6227589845657349
train gradient:  0.20193915268064866
iteration : 11835
train acc:  0.734375
train loss:  0.4562496244907379
train gradient:  0.1064700136908621
iteration : 11836
train acc:  0.78125
train loss:  0.451408714056015
train gradient:  0.14315253521390975
iteration : 11837
train acc:  0.7265625
train loss:  0.5007534623146057
train gradient:  0.13153338154386132
iteration : 11838
train acc:  0.7890625
train loss:  0.4196477234363556
train gradient:  0.10246341830366841
iteration : 11839
train acc:  0.7421875
train loss:  0.4454120397567749
train gradient:  0.12842063303239168
iteration : 11840
train acc:  0.7890625
train loss:  0.4709259271621704
train gradient:  0.11162415174926424
iteration : 11841
train acc:  0.7578125
train loss:  0.545805811882019
train gradient:  0.14096822858600958
iteration : 11842
train acc:  0.765625
train loss:  0.4904523193836212
train gradient:  0.116039345283363
iteration : 11843
train acc:  0.71875
train loss:  0.5094027519226074
train gradient:  0.17092826502702985
iteration : 11844
train acc:  0.6484375
train loss:  0.5908894538879395
train gradient:  0.18895924694371316
iteration : 11845
train acc:  0.75
train loss:  0.5482686758041382
train gradient:  0.1506928154776111
iteration : 11846
train acc:  0.765625
train loss:  0.47686827182769775
train gradient:  0.15227628700874862
iteration : 11847
train acc:  0.7578125
train loss:  0.4513687491416931
train gradient:  0.11312144287978082
iteration : 11848
train acc:  0.7421875
train loss:  0.4994891881942749
train gradient:  0.15825214447867653
iteration : 11849
train acc:  0.7265625
train loss:  0.48764535784721375
train gradient:  0.12364619775011
iteration : 11850
train acc:  0.7265625
train loss:  0.5028738379478455
train gradient:  0.12939317230931296
iteration : 11851
train acc:  0.6796875
train loss:  0.5602672100067139
train gradient:  0.17619658412880818
iteration : 11852
train acc:  0.7109375
train loss:  0.49316224455833435
train gradient:  0.12529893316680127
iteration : 11853
train acc:  0.75
train loss:  0.4850810766220093
train gradient:  0.10853748727693971
iteration : 11854
train acc:  0.7421875
train loss:  0.5439797639846802
train gradient:  0.18994505186543428
iteration : 11855
train acc:  0.6953125
train loss:  0.5816525220870972
train gradient:  0.21434114962473175
iteration : 11856
train acc:  0.8515625
train loss:  0.3779388666152954
train gradient:  0.0748646832409599
iteration : 11857
train acc:  0.7109375
train loss:  0.571912407875061
train gradient:  0.16239663041808267
iteration : 11858
train acc:  0.703125
train loss:  0.5270043611526489
train gradient:  0.12248326229405943
iteration : 11859
train acc:  0.7890625
train loss:  0.45055145025253296
train gradient:  0.10705078918799221
iteration : 11860
train acc:  0.75
train loss:  0.479326993227005
train gradient:  0.12612594227813778
iteration : 11861
train acc:  0.7734375
train loss:  0.45220279693603516
train gradient:  0.13174277487074082
iteration : 11862
train acc:  0.7265625
train loss:  0.4780789613723755
train gradient:  0.10459992649518338
iteration : 11863
train acc:  0.765625
train loss:  0.4829299747943878
train gradient:  0.12359871933936013
iteration : 11864
train acc:  0.6953125
train loss:  0.5889015793800354
train gradient:  0.1811342573036392
iteration : 11865
train acc:  0.6796875
train loss:  0.6190112829208374
train gradient:  0.1818040971940434
iteration : 11866
train acc:  0.6953125
train loss:  0.5290488004684448
train gradient:  0.12560136408100547
iteration : 11867
train acc:  0.8046875
train loss:  0.43521612882614136
train gradient:  0.12661235453379682
iteration : 11868
train acc:  0.7265625
train loss:  0.5178564786911011
train gradient:  0.14325506723790166
iteration : 11869
train acc:  0.78125
train loss:  0.4644997715950012
train gradient:  0.10175487221505913
iteration : 11870
train acc:  0.7421875
train loss:  0.4719478487968445
train gradient:  0.11762148632439844
iteration : 11871
train acc:  0.7578125
train loss:  0.4558045566082001
train gradient:  0.09620548781662266
iteration : 11872
train acc:  0.7265625
train loss:  0.4953528642654419
train gradient:  0.10932585398569278
iteration : 11873
train acc:  0.734375
train loss:  0.49844908714294434
train gradient:  0.10482053311937142
iteration : 11874
train acc:  0.8203125
train loss:  0.41937732696533203
train gradient:  0.09405182475531063
iteration : 11875
train acc:  0.7421875
train loss:  0.4990794360637665
train gradient:  0.12429639624411969
iteration : 11876
train acc:  0.78125
train loss:  0.4505123794078827
train gradient:  0.08751059959847544
iteration : 11877
train acc:  0.7109375
train loss:  0.550471305847168
train gradient:  0.16028023259413032
iteration : 11878
train acc:  0.765625
train loss:  0.4528353214263916
train gradient:  0.10451930901674521
iteration : 11879
train acc:  0.75
train loss:  0.505922794342041
train gradient:  0.12808212043799921
iteration : 11880
train acc:  0.6953125
train loss:  0.5601444244384766
train gradient:  0.18379049283425727
iteration : 11881
train acc:  0.765625
train loss:  0.5002056360244751
train gradient:  0.13609226762238105
iteration : 11882
train acc:  0.75
train loss:  0.46044981479644775
train gradient:  0.12545668462044746
iteration : 11883
train acc:  0.7734375
train loss:  0.4543081521987915
train gradient:  0.1168666222741631
iteration : 11884
train acc:  0.6875
train loss:  0.5714055299758911
train gradient:  0.18816482565768147
iteration : 11885
train acc:  0.7265625
train loss:  0.4653266668319702
train gradient:  0.136149336586644
iteration : 11886
train acc:  0.7578125
train loss:  0.4978371560573578
train gradient:  0.12512759537374352
iteration : 11887
train acc:  0.7421875
train loss:  0.47963038086891174
train gradient:  0.11942972498804971
iteration : 11888
train acc:  0.8671875
train loss:  0.34560567140579224
train gradient:  0.0766057013179856
iteration : 11889
train acc:  0.7421875
train loss:  0.48444539308547974
train gradient:  0.11189387880198262
iteration : 11890
train acc:  0.78125
train loss:  0.4686852991580963
train gradient:  0.15312199669204846
iteration : 11891
train acc:  0.7109375
train loss:  0.48684263229370117
train gradient:  0.1277253566391171
iteration : 11892
train acc:  0.7734375
train loss:  0.4805150032043457
train gradient:  0.11859513637286517
iteration : 11893
train acc:  0.7421875
train loss:  0.517035961151123
train gradient:  0.14752291319137031
iteration : 11894
train acc:  0.734375
train loss:  0.5063191652297974
train gradient:  0.14477912917331015
iteration : 11895
train acc:  0.6875
train loss:  0.5447242259979248
train gradient:  0.18801821547871128
iteration : 11896
train acc:  0.8203125
train loss:  0.4248180091381073
train gradient:  0.11056045872272069
iteration : 11897
train acc:  0.75
train loss:  0.48824256658554077
train gradient:  0.18407756704909067
iteration : 11898
train acc:  0.7578125
train loss:  0.4647819697856903
train gradient:  0.12159933578979952
iteration : 11899
train acc:  0.7421875
train loss:  0.4723030924797058
train gradient:  0.10177633859260492
iteration : 11900
train acc:  0.7578125
train loss:  0.4938664436340332
train gradient:  0.11743060712690283
iteration : 11901
train acc:  0.734375
train loss:  0.446280837059021
train gradient:  0.10248353823878499
iteration : 11902
train acc:  0.71875
train loss:  0.5042741298675537
train gradient:  0.1457954556671649
iteration : 11903
train acc:  0.671875
train loss:  0.5670514106750488
train gradient:  0.1994623531355481
iteration : 11904
train acc:  0.7578125
train loss:  0.5012762546539307
train gradient:  0.14324177598095583
iteration : 11905
train acc:  0.7578125
train loss:  0.5133758783340454
train gradient:  0.1418869055740089
iteration : 11906
train acc:  0.734375
train loss:  0.48830366134643555
train gradient:  0.12560441762346858
iteration : 11907
train acc:  0.7109375
train loss:  0.5282025933265686
train gradient:  0.15592818048494134
iteration : 11908
train acc:  0.734375
train loss:  0.5184486508369446
train gradient:  0.16100947927050357
iteration : 11909
train acc:  0.78125
train loss:  0.402678906917572
train gradient:  0.10650977914989881
iteration : 11910
train acc:  0.78125
train loss:  0.48923394083976746
train gradient:  0.12472364593933166
iteration : 11911
train acc:  0.796875
train loss:  0.4664198160171509
train gradient:  0.11570274850196278
iteration : 11912
train acc:  0.828125
train loss:  0.4434893727302551
train gradient:  0.08336266092434555
iteration : 11913
train acc:  0.796875
train loss:  0.4222232401371002
train gradient:  0.0978630755130578
iteration : 11914
train acc:  0.734375
train loss:  0.5670676231384277
train gradient:  0.18637774841616914
iteration : 11915
train acc:  0.765625
train loss:  0.47618263959884644
train gradient:  0.16568280537050656
iteration : 11916
train acc:  0.7265625
train loss:  0.4860777258872986
train gradient:  0.12349226253169245
iteration : 11917
train acc:  0.7421875
train loss:  0.5017209053039551
train gradient:  0.08829225532186756
iteration : 11918
train acc:  0.734375
train loss:  0.48729923367500305
train gradient:  0.12095636109853251
iteration : 11919
train acc:  0.6796875
train loss:  0.5501694083213806
train gradient:  0.1447035767032499
iteration : 11920
train acc:  0.7109375
train loss:  0.515961766242981
train gradient:  0.13389053479888946
iteration : 11921
train acc:  0.7578125
train loss:  0.5120658874511719
train gradient:  0.13887281674956492
iteration : 11922
train acc:  0.734375
train loss:  0.519481897354126
train gradient:  0.13416052916876003
iteration : 11923
train acc:  0.7109375
train loss:  0.4954649806022644
train gradient:  0.12310743225599426
iteration : 11924
train acc:  0.765625
train loss:  0.4705173373222351
train gradient:  0.11344667002808058
iteration : 11925
train acc:  0.7265625
train loss:  0.5057098865509033
train gradient:  0.13890907463222651
iteration : 11926
train acc:  0.7265625
train loss:  0.4854721426963806
train gradient:  0.12484641962288076
iteration : 11927
train acc:  0.765625
train loss:  0.5458834171295166
train gradient:  0.14329196630537971
iteration : 11928
train acc:  0.7890625
train loss:  0.4479650855064392
train gradient:  0.12397364411014614
iteration : 11929
train acc:  0.734375
train loss:  0.5339963436126709
train gradient:  0.16070567752272963
iteration : 11930
train acc:  0.6953125
train loss:  0.5071519017219543
train gradient:  0.17940967152205575
iteration : 11931
train acc:  0.7578125
train loss:  0.5042535066604614
train gradient:  0.14519673661026564
iteration : 11932
train acc:  0.71875
train loss:  0.5322438478469849
train gradient:  0.13784879808356376
iteration : 11933
train acc:  0.671875
train loss:  0.5519225001335144
train gradient:  0.12680045415682387
iteration : 11934
train acc:  0.7421875
train loss:  0.5098921060562134
train gradient:  0.1374582520868391
iteration : 11935
train acc:  0.796875
train loss:  0.469634473323822
train gradient:  0.12155479014423567
iteration : 11936
train acc:  0.6875
train loss:  0.5483047962188721
train gradient:  0.14524363033919374
iteration : 11937
train acc:  0.8203125
train loss:  0.45210835337638855
train gradient:  0.11294105709154309
iteration : 11938
train acc:  0.6953125
train loss:  0.5529786944389343
train gradient:  0.12933098735143825
iteration : 11939
train acc:  0.7578125
train loss:  0.5009233355522156
train gradient:  0.1425041950380005
iteration : 11940
train acc:  0.7109375
train loss:  0.5041108727455139
train gradient:  0.1310120899072101
iteration : 11941
train acc:  0.71875
train loss:  0.5635701417922974
train gradient:  0.17209216488050294
iteration : 11942
train acc:  0.78125
train loss:  0.4579738974571228
train gradient:  0.13173042856504866
iteration : 11943
train acc:  0.7578125
train loss:  0.45324718952178955
train gradient:  0.11747469233529761
iteration : 11944
train acc:  0.765625
train loss:  0.48749279975891113
train gradient:  0.10169698310013153
iteration : 11945
train acc:  0.7421875
train loss:  0.4903484582901001
train gradient:  0.12274593545867661
iteration : 11946
train acc:  0.75
train loss:  0.46708381175994873
train gradient:  0.09730916798205622
iteration : 11947
train acc:  0.7109375
train loss:  0.5039492845535278
train gradient:  0.1188924648743625
iteration : 11948
train acc:  0.7578125
train loss:  0.4747209846973419
train gradient:  0.11833121072470638
iteration : 11949
train acc:  0.765625
train loss:  0.4601902961730957
train gradient:  0.1055783737159635
iteration : 11950
train acc:  0.671875
train loss:  0.5789409279823303
train gradient:  0.1432141464579716
iteration : 11951
train acc:  0.7421875
train loss:  0.472988486289978
train gradient:  0.10170288202342394
iteration : 11952
train acc:  0.71875
train loss:  0.46660566329956055
train gradient:  0.10674056683654753
iteration : 11953
train acc:  0.7578125
train loss:  0.48369133472442627
train gradient:  0.1207394795346193
iteration : 11954
train acc:  0.7421875
train loss:  0.48943644762039185
train gradient:  0.11280623249582651
iteration : 11955
train acc:  0.734375
train loss:  0.5203668475151062
train gradient:  0.12789775114899304
iteration : 11956
train acc:  0.71875
train loss:  0.5281682014465332
train gradient:  0.17707359405161435
iteration : 11957
train acc:  0.734375
train loss:  0.4845612645149231
train gradient:  0.14074668428489012
iteration : 11958
train acc:  0.765625
train loss:  0.4832044243812561
train gradient:  0.09681209430285598
iteration : 11959
train acc:  0.734375
train loss:  0.4869970679283142
train gradient:  0.1832336781742466
iteration : 11960
train acc:  0.8125
train loss:  0.44488123059272766
train gradient:  0.10544648074216945
iteration : 11961
train acc:  0.75
train loss:  0.4807460606098175
train gradient:  0.10987570325276619
iteration : 11962
train acc:  0.734375
train loss:  0.47493776679039
train gradient:  0.14214676529627696
iteration : 11963
train acc:  0.7734375
train loss:  0.4613068699836731
train gradient:  0.11355284139693282
iteration : 11964
train acc:  0.7109375
train loss:  0.5267466902732849
train gradient:  0.1559340640470307
iteration : 11965
train acc:  0.75
train loss:  0.5040457248687744
train gradient:  0.15675355590794376
iteration : 11966
train acc:  0.7109375
train loss:  0.5239033699035645
train gradient:  0.1372348921183676
iteration : 11967
train acc:  0.765625
train loss:  0.47649329900741577
train gradient:  0.12970912937094964
iteration : 11968
train acc:  0.7265625
train loss:  0.5352843999862671
train gradient:  0.16837593824447195
iteration : 11969
train acc:  0.734375
train loss:  0.47605639696121216
train gradient:  0.11240235212550043
iteration : 11970
train acc:  0.6953125
train loss:  0.5187629461288452
train gradient:  0.12413492376924654
iteration : 11971
train acc:  0.8203125
train loss:  0.4864356219768524
train gradient:  0.12903007772308261
iteration : 11972
train acc:  0.734375
train loss:  0.5236434936523438
train gradient:  0.14547572178590018
iteration : 11973
train acc:  0.765625
train loss:  0.43134447932243347
train gradient:  0.10616392847791511
iteration : 11974
train acc:  0.7734375
train loss:  0.489438533782959
train gradient:  0.14615397476849123
iteration : 11975
train acc:  0.7578125
train loss:  0.410216748714447
train gradient:  0.09235681412772476
iteration : 11976
train acc:  0.734375
train loss:  0.49756187200546265
train gradient:  0.11992719067846547
iteration : 11977
train acc:  0.734375
train loss:  0.5598461031913757
train gradient:  0.12912189810662694
iteration : 11978
train acc:  0.8125
train loss:  0.4144083857536316
train gradient:  0.10762072233028766
iteration : 11979
train acc:  0.671875
train loss:  0.5491913557052612
train gradient:  0.14409649205685818
iteration : 11980
train acc:  0.7734375
train loss:  0.43058642745018005
train gradient:  0.0978852821458622
iteration : 11981
train acc:  0.75
train loss:  0.4490339756011963
train gradient:  0.10413144425491604
iteration : 11982
train acc:  0.8046875
train loss:  0.4215865731239319
train gradient:  0.09036210614070947
iteration : 11983
train acc:  0.828125
train loss:  0.40458258986473083
train gradient:  0.09330989699022468
iteration : 11984
train acc:  0.7734375
train loss:  0.4586191177368164
train gradient:  0.0990985803887522
iteration : 11985
train acc:  0.75
train loss:  0.5002859830856323
train gradient:  0.13082961225199247
iteration : 11986
train acc:  0.7109375
train loss:  0.5308475494384766
train gradient:  0.1686394009325329
iteration : 11987
train acc:  0.7109375
train loss:  0.4919655919075012
train gradient:  0.10681607933491795
iteration : 11988
train acc:  0.7265625
train loss:  0.49188366532325745
train gradient:  0.12356232593407014
iteration : 11989
train acc:  0.734375
train loss:  0.4842131435871124
train gradient:  0.11667265998326805
iteration : 11990
train acc:  0.7578125
train loss:  0.4581524729728699
train gradient:  0.19833721497941878
iteration : 11991
train acc:  0.7890625
train loss:  0.4378812909126282
train gradient:  0.10904672776070726
iteration : 11992
train acc:  0.796875
train loss:  0.4253823161125183
train gradient:  0.13305713873564468
iteration : 11993
train acc:  0.71875
train loss:  0.5282789468765259
train gradient:  0.13871528277990525
iteration : 11994
train acc:  0.7265625
train loss:  0.4604038596153259
train gradient:  0.10665857673103987
iteration : 11995
train acc:  0.7890625
train loss:  0.4358685612678528
train gradient:  0.09693584457470919
iteration : 11996
train acc:  0.71875
train loss:  0.49882176518440247
train gradient:  0.10727998753016899
iteration : 11997
train acc:  0.7109375
train loss:  0.5078040361404419
train gradient:  0.14145918667910484
iteration : 11998
train acc:  0.7265625
train loss:  0.5272922515869141
train gradient:  0.15366223016912622
iteration : 11999
train acc:  0.75
train loss:  0.47968196868896484
train gradient:  0.12290571479617708
iteration : 12000
train acc:  0.71875
train loss:  0.5017920732498169
train gradient:  0.11974223897474771
iteration : 12001
train acc:  0.71875
train loss:  0.4981347918510437
train gradient:  0.10911328716472536
iteration : 12002
train acc:  0.765625
train loss:  0.5076653957366943
train gradient:  0.1544133351420469
iteration : 12003
train acc:  0.6640625
train loss:  0.4927563965320587
train gradient:  0.1098245824452307
iteration : 12004
train acc:  0.8046875
train loss:  0.4250637888908386
train gradient:  0.11231570504449138
iteration : 12005
train acc:  0.7890625
train loss:  0.4584959149360657
train gradient:  0.09404451443230116
iteration : 12006
train acc:  0.734375
train loss:  0.5301651954650879
train gradient:  0.1199513312894422
iteration : 12007
train acc:  0.7734375
train loss:  0.42261946201324463
train gradient:  0.10502104585755077
iteration : 12008
train acc:  0.78125
train loss:  0.45472872257232666
train gradient:  0.10933905419800893
iteration : 12009
train acc:  0.7890625
train loss:  0.4744687080383301
train gradient:  0.13590031910913872
iteration : 12010
train acc:  0.7734375
train loss:  0.49524107575416565
train gradient:  0.1576234229497231
iteration : 12011
train acc:  0.734375
train loss:  0.5261866450309753
train gradient:  0.14046011064934777
iteration : 12012
train acc:  0.6640625
train loss:  0.5230377316474915
train gradient:  0.18404950813242862
iteration : 12013
train acc:  0.78125
train loss:  0.44837749004364014
train gradient:  0.10397514959203895
iteration : 12014
train acc:  0.7578125
train loss:  0.5033167600631714
train gradient:  0.12316760473325358
iteration : 12015
train acc:  0.75
train loss:  0.44168931245803833
train gradient:  0.13033329606417648
iteration : 12016
train acc:  0.765625
train loss:  0.44974014163017273
train gradient:  0.11169054058999439
iteration : 12017
train acc:  0.6796875
train loss:  0.5609834790229797
train gradient:  0.1566167205631468
iteration : 12018
train acc:  0.7421875
train loss:  0.5187567472457886
train gradient:  0.14476723209958192
iteration : 12019
train acc:  0.7265625
train loss:  0.5014467239379883
train gradient:  0.16551700188168633
iteration : 12020
train acc:  0.7734375
train loss:  0.46216073632240295
train gradient:  0.13222938401552495
iteration : 12021
train acc:  0.7265625
train loss:  0.5140024423599243
train gradient:  0.15307709392627744
iteration : 12022
train acc:  0.78125
train loss:  0.42110902070999146
train gradient:  0.08247650514675185
iteration : 12023
train acc:  0.7578125
train loss:  0.5131819248199463
train gradient:  0.13726092175400378
iteration : 12024
train acc:  0.6875
train loss:  0.5753780603408813
train gradient:  0.18532858284866988
iteration : 12025
train acc:  0.71875
train loss:  0.5253586769104004
train gradient:  0.14907058546657598
iteration : 12026
train acc:  0.765625
train loss:  0.47794532775878906
train gradient:  0.11049150795328112
iteration : 12027
train acc:  0.765625
train loss:  0.4717215895652771
train gradient:  0.14537574755662236
iteration : 12028
train acc:  0.7890625
train loss:  0.4677242338657379
train gradient:  0.11611526779244387
iteration : 12029
train acc:  0.71875
train loss:  0.5135776996612549
train gradient:  0.10856889499914106
iteration : 12030
train acc:  0.7578125
train loss:  0.4532579779624939
train gradient:  0.11449237324142082
iteration : 12031
train acc:  0.7109375
train loss:  0.5266189575195312
train gradient:  0.1423558252564036
iteration : 12032
train acc:  0.71875
train loss:  0.5565507411956787
train gradient:  0.19086600362751022
iteration : 12033
train acc:  0.734375
train loss:  0.47229641675949097
train gradient:  0.14366385310730925
iteration : 12034
train acc:  0.7421875
train loss:  0.4683606028556824
train gradient:  0.11665432472919025
iteration : 12035
train acc:  0.75
train loss:  0.5263854265213013
train gradient:  0.12555205641055256
iteration : 12036
train acc:  0.71875
train loss:  0.48840874433517456
train gradient:  0.1092621472400749
iteration : 12037
train acc:  0.7734375
train loss:  0.493439257144928
train gradient:  0.12881219600707416
iteration : 12038
train acc:  0.7109375
train loss:  0.5334662795066833
train gradient:  0.13624267478951155
iteration : 12039
train acc:  0.71875
train loss:  0.4970591962337494
train gradient:  0.13182444131421495
iteration : 12040
train acc:  0.75
train loss:  0.5080556869506836
train gradient:  0.11153478489937788
iteration : 12041
train acc:  0.7734375
train loss:  0.4833296537399292
train gradient:  0.12643376076435872
iteration : 12042
train acc:  0.703125
train loss:  0.5276670455932617
train gradient:  0.1118828215800215
iteration : 12043
train acc:  0.71875
train loss:  0.5255900621414185
train gradient:  0.13678737044250316
iteration : 12044
train acc:  0.734375
train loss:  0.48625925183296204
train gradient:  0.12368541809115552
iteration : 12045
train acc:  0.765625
train loss:  0.4812813997268677
train gradient:  0.13956475142741354
iteration : 12046
train acc:  0.7109375
train loss:  0.45203858613967896
train gradient:  0.09320159571473763
iteration : 12047
train acc:  0.7265625
train loss:  0.5209642648696899
train gradient:  0.11496078868567121
iteration : 12048
train acc:  0.65625
train loss:  0.5747753977775574
train gradient:  0.1648000952972563
iteration : 12049
train acc:  0.7578125
train loss:  0.5057058334350586
train gradient:  0.10931816463063096
iteration : 12050
train acc:  0.734375
train loss:  0.49316486716270447
train gradient:  0.11352559347523243
iteration : 12051
train acc:  0.7734375
train loss:  0.44349515438079834
train gradient:  0.10587891507577005
iteration : 12052
train acc:  0.78125
train loss:  0.5294392108917236
train gradient:  0.13616487810628072
iteration : 12053
train acc:  0.7421875
train loss:  0.44044291973114014
train gradient:  0.08824943573496195
iteration : 12054
train acc:  0.75
train loss:  0.458888441324234
train gradient:  0.10868811770202996
iteration : 12055
train acc:  0.7578125
train loss:  0.5203505754470825
train gradient:  0.14254395180746748
iteration : 12056
train acc:  0.828125
train loss:  0.39889204502105713
train gradient:  0.09936338350387855
iteration : 12057
train acc:  0.65625
train loss:  0.5443633794784546
train gradient:  0.12919236090203573
iteration : 12058
train acc:  0.734375
train loss:  0.5104722380638123
train gradient:  0.14537532161324102
iteration : 12059
train acc:  0.8046875
train loss:  0.4288223087787628
train gradient:  0.10675947872775675
iteration : 12060
train acc:  0.7578125
train loss:  0.46047574281692505
train gradient:  0.11130480489530899
iteration : 12061
train acc:  0.765625
train loss:  0.4407810866832733
train gradient:  0.09684239041630355
iteration : 12062
train acc:  0.734375
train loss:  0.46972158551216125
train gradient:  0.11462011603301774
iteration : 12063
train acc:  0.7421875
train loss:  0.5138514041900635
train gradient:  0.15821589273777042
iteration : 12064
train acc:  0.734375
train loss:  0.47800999879837036
train gradient:  0.12509193085455156
iteration : 12065
train acc:  0.7265625
train loss:  0.46045082807540894
train gradient:  0.11839192192356816
iteration : 12066
train acc:  0.75
train loss:  0.4512042999267578
train gradient:  0.11400105679550995
iteration : 12067
train acc:  0.71875
train loss:  0.49605417251586914
train gradient:  0.09429663542470497
iteration : 12068
train acc:  0.671875
train loss:  0.5160771012306213
train gradient:  0.11907576636740218
iteration : 12069
train acc:  0.7890625
train loss:  0.44790536165237427
train gradient:  0.11171732338428314
iteration : 12070
train acc:  0.6953125
train loss:  0.5738261342048645
train gradient:  0.2354784495552876
iteration : 12071
train acc:  0.8046875
train loss:  0.4282911717891693
train gradient:  0.11892684958250986
iteration : 12072
train acc:  0.765625
train loss:  0.5354969501495361
train gradient:  0.1407370137025209
iteration : 12073
train acc:  0.703125
train loss:  0.5898292064666748
train gradient:  0.15720153780015328
iteration : 12074
train acc:  0.7890625
train loss:  0.42581886053085327
train gradient:  0.12364527533250351
iteration : 12075
train acc:  0.7421875
train loss:  0.4710104763507843
train gradient:  0.12417124831970361
iteration : 12076
train acc:  0.765625
train loss:  0.47324129939079285
train gradient:  0.11470848540405099
iteration : 12077
train acc:  0.7421875
train loss:  0.5026302933692932
train gradient:  0.15562513093446512
iteration : 12078
train acc:  0.6796875
train loss:  0.5668165683746338
train gradient:  0.20149141297446937
iteration : 12079
train acc:  0.71875
train loss:  0.5791206359863281
train gradient:  0.1804514444934301
iteration : 12080
train acc:  0.7578125
train loss:  0.475725382566452
train gradient:  0.12869190139763945
iteration : 12081
train acc:  0.828125
train loss:  0.4270707368850708
train gradient:  0.11362887227164595
iteration : 12082
train acc:  0.7109375
train loss:  0.5510058403015137
train gradient:  0.18257247614899072
iteration : 12083
train acc:  0.765625
train loss:  0.4996204972267151
train gradient:  0.12907178663149377
iteration : 12084
train acc:  0.703125
train loss:  0.5691759586334229
train gradient:  0.17872380374056046
iteration : 12085
train acc:  0.828125
train loss:  0.40253743529319763
train gradient:  0.08070986506825543
iteration : 12086
train acc:  0.7734375
train loss:  0.4498874545097351
train gradient:  0.09680225942669392
iteration : 12087
train acc:  0.78125
train loss:  0.43857330083847046
train gradient:  0.09350218960044324
iteration : 12088
train acc:  0.6953125
train loss:  0.5256198644638062
train gradient:  0.13950855804901333
iteration : 12089
train acc:  0.8125
train loss:  0.39745357632637024
train gradient:  0.09502711289811347
iteration : 12090
train acc:  0.7265625
train loss:  0.484249472618103
train gradient:  0.114753855675263
iteration : 12091
train acc:  0.7578125
train loss:  0.5029550194740295
train gradient:  0.15918006578108007
iteration : 12092
train acc:  0.828125
train loss:  0.41054531931877136
train gradient:  0.0854596526466299
iteration : 12093
train acc:  0.734375
train loss:  0.5113818645477295
train gradient:  0.11754430579991215
iteration : 12094
train acc:  0.6875
train loss:  0.5330628752708435
train gradient:  0.12641607480847675
iteration : 12095
train acc:  0.7578125
train loss:  0.4907061457633972
train gradient:  0.1761560408561265
iteration : 12096
train acc:  0.765625
train loss:  0.4650357663631439
train gradient:  0.10521987240687751
iteration : 12097
train acc:  0.7421875
train loss:  0.4979571998119354
train gradient:  0.1733297279226299
iteration : 12098
train acc:  0.7265625
train loss:  0.5514383912086487
train gradient:  0.15868305727014798
iteration : 12099
train acc:  0.75
train loss:  0.4772762656211853
train gradient:  0.1030149735359321
iteration : 12100
train acc:  0.734375
train loss:  0.5211615562438965
train gradient:  0.11772670300952033
iteration : 12101
train acc:  0.765625
train loss:  0.4839443862438202
train gradient:  0.11170116205908737
iteration : 12102
train acc:  0.7734375
train loss:  0.48526930809020996
train gradient:  0.13071910406567216
iteration : 12103
train acc:  0.7734375
train loss:  0.4514375925064087
train gradient:  0.11558667362933667
iteration : 12104
train acc:  0.7421875
train loss:  0.5090715885162354
train gradient:  0.13828740803115724
iteration : 12105
train acc:  0.71875
train loss:  0.4762774109840393
train gradient:  0.12006115288520477
iteration : 12106
train acc:  0.7109375
train loss:  0.4825018644332886
train gradient:  0.14225974918643167
iteration : 12107
train acc:  0.7890625
train loss:  0.4536413848400116
train gradient:  0.1011795120860861
iteration : 12108
train acc:  0.671875
train loss:  0.5301402807235718
train gradient:  0.2051354673869208
iteration : 12109
train acc:  0.6640625
train loss:  0.5478495955467224
train gradient:  0.1382926815884537
iteration : 12110
train acc:  0.765625
train loss:  0.47414666414260864
train gradient:  0.11634363903226998
iteration : 12111
train acc:  0.71875
train loss:  0.5008289813995361
train gradient:  0.13661718459003896
iteration : 12112
train acc:  0.7578125
train loss:  0.502831757068634
train gradient:  0.1197147599605049
iteration : 12113
train acc:  0.796875
train loss:  0.4246089458465576
train gradient:  0.09717666973374976
iteration : 12114
train acc:  0.8125
train loss:  0.41399672627449036
train gradient:  0.12609754517798533
iteration : 12115
train acc:  0.71875
train loss:  0.4849693775177002
train gradient:  0.14993246741760607
iteration : 12116
train acc:  0.7734375
train loss:  0.4934331178665161
train gradient:  0.12971925231160247
iteration : 12117
train acc:  0.6875
train loss:  0.5464496612548828
train gradient:  0.1801357850340064
iteration : 12118
train acc:  0.7265625
train loss:  0.480025976896286
train gradient:  0.11519955396855498
iteration : 12119
train acc:  0.7890625
train loss:  0.46529489755630493
train gradient:  0.10311567304038921
iteration : 12120
train acc:  0.765625
train loss:  0.4825936257839203
train gradient:  0.12549728506992983
iteration : 12121
train acc:  0.734375
train loss:  0.5185432434082031
train gradient:  0.11045009423546219
iteration : 12122
train acc:  0.8203125
train loss:  0.40972185134887695
train gradient:  0.08870844125552933
iteration : 12123
train acc:  0.796875
train loss:  0.4670850336551666
train gradient:  0.11648915354091831
iteration : 12124
train acc:  0.65625
train loss:  0.6160346269607544
train gradient:  0.18849320233862066
iteration : 12125
train acc:  0.71875
train loss:  0.5204852819442749
train gradient:  0.1703506734099624
iteration : 12126
train acc:  0.8046875
train loss:  0.44130969047546387
train gradient:  0.10952031526714605
iteration : 12127
train acc:  0.6875
train loss:  0.5191428065299988
train gradient:  0.12271671876316322
iteration : 12128
train acc:  0.6875
train loss:  0.5170073509216309
train gradient:  0.11422350846732433
iteration : 12129
train acc:  0.7421875
train loss:  0.47679761052131653
train gradient:  0.11855267870623294
iteration : 12130
train acc:  0.71875
train loss:  0.5665217638015747
train gradient:  0.15835641707796907
iteration : 12131
train acc:  0.7734375
train loss:  0.4628443717956543
train gradient:  0.11310498290064955
iteration : 12132
train acc:  0.7421875
train loss:  0.4549839496612549
train gradient:  0.11349545333439832
iteration : 12133
train acc:  0.7421875
train loss:  0.4893304109573364
train gradient:  0.13559321447756875
iteration : 12134
train acc:  0.7578125
train loss:  0.48135995864868164
train gradient:  0.1409485958801343
iteration : 12135
train acc:  0.7890625
train loss:  0.4471004605293274
train gradient:  0.10216377947586489
iteration : 12136
train acc:  0.71875
train loss:  0.5083702802658081
train gradient:  0.12859274890153136
iteration : 12137
train acc:  0.78125
train loss:  0.4726164638996124
train gradient:  0.12134873475981063
iteration : 12138
train acc:  0.71875
train loss:  0.487857460975647
train gradient:  0.13607724792357873
iteration : 12139
train acc:  0.7421875
train loss:  0.49165356159210205
train gradient:  0.11270426630106908
iteration : 12140
train acc:  0.796875
train loss:  0.41150963306427
train gradient:  0.10992156648859873
iteration : 12141
train acc:  0.71875
train loss:  0.5228456854820251
train gradient:  0.1524759558053294
iteration : 12142
train acc:  0.734375
train loss:  0.49268025159835815
train gradient:  0.15052394804776373
iteration : 12143
train acc:  0.7265625
train loss:  0.46800005435943604
train gradient:  0.12112712201994161
iteration : 12144
train acc:  0.78125
train loss:  0.4779045581817627
train gradient:  0.10995017501598241
iteration : 12145
train acc:  0.765625
train loss:  0.47839364409446716
train gradient:  0.1294421578792634
iteration : 12146
train acc:  0.65625
train loss:  0.6699672937393188
train gradient:  0.19525577170564912
iteration : 12147
train acc:  0.7578125
train loss:  0.4829610586166382
train gradient:  0.12980613707501024
iteration : 12148
train acc:  0.7734375
train loss:  0.4468592405319214
train gradient:  0.1160487707669273
iteration : 12149
train acc:  0.796875
train loss:  0.43500322103500366
train gradient:  0.09621337550636197
iteration : 12150
train acc:  0.65625
train loss:  0.6110197901725769
train gradient:  0.22997615971383495
iteration : 12151
train acc:  0.703125
train loss:  0.5457010269165039
train gradient:  0.15213880712797306
iteration : 12152
train acc:  0.7421875
train loss:  0.5102733373641968
train gradient:  0.11330587038325077
iteration : 12153
train acc:  0.765625
train loss:  0.4826960861682892
train gradient:  0.12513390021241386
iteration : 12154
train acc:  0.765625
train loss:  0.49671289324760437
train gradient:  0.13517011133417
iteration : 12155
train acc:  0.7109375
train loss:  0.5423552393913269
train gradient:  0.14561949159819867
iteration : 12156
train acc:  0.71875
train loss:  0.5164870619773865
train gradient:  0.14116264481031993
iteration : 12157
train acc:  0.7890625
train loss:  0.4587060809135437
train gradient:  0.10653879210069823
iteration : 12158
train acc:  0.71875
train loss:  0.5214972496032715
train gradient:  0.11534213311529717
iteration : 12159
train acc:  0.703125
train loss:  0.5116956233978271
train gradient:  0.12913839658123294
iteration : 12160
train acc:  0.7734375
train loss:  0.4286920428276062
train gradient:  0.13693944161194982
iteration : 12161
train acc:  0.734375
train loss:  0.4959408938884735
train gradient:  0.12486778490209324
iteration : 12162
train acc:  0.703125
train loss:  0.5315567255020142
train gradient:  0.12979202019384437
iteration : 12163
train acc:  0.7265625
train loss:  0.5568619966506958
train gradient:  0.1428764799593234
iteration : 12164
train acc:  0.7578125
train loss:  0.4707879424095154
train gradient:  0.11249714604515054
iteration : 12165
train acc:  0.7421875
train loss:  0.5206476449966431
train gradient:  0.1546130414312198
iteration : 12166
train acc:  0.7421875
train loss:  0.47600454092025757
train gradient:  0.10684730598343894
iteration : 12167
train acc:  0.734375
train loss:  0.4523191452026367
train gradient:  0.11698357488915734
iteration : 12168
train acc:  0.8046875
train loss:  0.4468994736671448
train gradient:  0.1051856602227346
iteration : 12169
train acc:  0.7421875
train loss:  0.5466998815536499
train gradient:  0.14903391083643414
iteration : 12170
train acc:  0.6875
train loss:  0.49716249108314514
train gradient:  0.11877030825441058
iteration : 12171
train acc:  0.6953125
train loss:  0.5050727128982544
train gradient:  0.16645923351146674
iteration : 12172
train acc:  0.7734375
train loss:  0.47956836223602295
train gradient:  0.10787826089263686
iteration : 12173
train acc:  0.7734375
train loss:  0.4502491354942322
train gradient:  0.0993023129058499
iteration : 12174
train acc:  0.796875
train loss:  0.46128183603286743
train gradient:  0.11110675954928223
iteration : 12175
train acc:  0.7109375
train loss:  0.5161780714988708
train gradient:  0.1666258387077177
iteration : 12176
train acc:  0.71875
train loss:  0.4944913387298584
train gradient:  0.1286357561159041
iteration : 12177
train acc:  0.7109375
train loss:  0.5072159767150879
train gradient:  0.12122009673226072
iteration : 12178
train acc:  0.75
train loss:  0.4867187738418579
train gradient:  0.2064809470211189
iteration : 12179
train acc:  0.7265625
train loss:  0.5131720304489136
train gradient:  0.15458685024393604
iteration : 12180
train acc:  0.78125
train loss:  0.42792168259620667
train gradient:  0.09503324285966429
iteration : 12181
train acc:  0.7265625
train loss:  0.5360082387924194
train gradient:  0.18464576063828014
iteration : 12182
train acc:  0.75
train loss:  0.42958498001098633
train gradient:  0.10212322428127386
iteration : 12183
train acc:  0.7578125
train loss:  0.47535616159439087
train gradient:  0.11238973990934585
iteration : 12184
train acc:  0.7421875
train loss:  0.4957417845726013
train gradient:  0.1258536153589792
iteration : 12185
train acc:  0.75
train loss:  0.4999468922615051
train gradient:  0.14194472572289596
iteration : 12186
train acc:  0.765625
train loss:  0.47312211990356445
train gradient:  0.12208135000278897
iteration : 12187
train acc:  0.7578125
train loss:  0.509052574634552
train gradient:  0.16541992217031443
iteration : 12188
train acc:  0.6953125
train loss:  0.5352564454078674
train gradient:  0.18508298110165705
iteration : 12189
train acc:  0.703125
train loss:  0.5680520534515381
train gradient:  0.1588856289026821
iteration : 12190
train acc:  0.71875
train loss:  0.5223343372344971
train gradient:  0.1736885377862486
iteration : 12191
train acc:  0.7421875
train loss:  0.4733508229255676
train gradient:  0.10375468994196428
iteration : 12192
train acc:  0.7578125
train loss:  0.45810413360595703
train gradient:  0.18849855424291467
iteration : 12193
train acc:  0.7734375
train loss:  0.45642009377479553
train gradient:  0.12795833905526993
iteration : 12194
train acc:  0.75
train loss:  0.49505770206451416
train gradient:  0.11428091269207057
iteration : 12195
train acc:  0.765625
train loss:  0.48409023880958557
train gradient:  0.12039397948414084
iteration : 12196
train acc:  0.7890625
train loss:  0.45925039052963257
train gradient:  0.139290778627019
iteration : 12197
train acc:  0.765625
train loss:  0.4836394786834717
train gradient:  0.11132092812689033
iteration : 12198
train acc:  0.6796875
train loss:  0.5170920491218567
train gradient:  0.12663720570273362
iteration : 12199
train acc:  0.625
train loss:  0.5276410579681396
train gradient:  0.12461679922882639
iteration : 12200
train acc:  0.75
train loss:  0.5048121809959412
train gradient:  0.11786736865755419
iteration : 12201
train acc:  0.734375
train loss:  0.473951131105423
train gradient:  0.12538774579196232
iteration : 12202
train acc:  0.796875
train loss:  0.43597835302352905
train gradient:  0.07913456490027176
iteration : 12203
train acc:  0.8359375
train loss:  0.41938406229019165
train gradient:  0.10429179461507616
iteration : 12204
train acc:  0.7734375
train loss:  0.4844939112663269
train gradient:  0.11575367750764959
iteration : 12205
train acc:  0.7265625
train loss:  0.5107658505439758
train gradient:  0.15883833944692027
iteration : 12206
train acc:  0.6796875
train loss:  0.531380295753479
train gradient:  0.1290246381726971
iteration : 12207
train acc:  0.703125
train loss:  0.5636414289474487
train gradient:  0.24718516995316508
iteration : 12208
train acc:  0.7421875
train loss:  0.5013058185577393
train gradient:  0.14689321215079792
iteration : 12209
train acc:  0.75
train loss:  0.5093018412590027
train gradient:  0.14346132548964752
iteration : 12210
train acc:  0.7578125
train loss:  0.5101677179336548
train gradient:  0.13336428126833427
iteration : 12211
train acc:  0.7265625
train loss:  0.5057680606842041
train gradient:  0.14515535789032838
iteration : 12212
train acc:  0.7265625
train loss:  0.5206373333930969
train gradient:  0.13822325492616894
iteration : 12213
train acc:  0.75
train loss:  0.5164703726768494
train gradient:  0.11635916502705347
iteration : 12214
train acc:  0.7734375
train loss:  0.4906509518623352
train gradient:  0.1263298875474978
iteration : 12215
train acc:  0.6796875
train loss:  0.5456181168556213
train gradient:  0.13289603766679192
iteration : 12216
train acc:  0.75
train loss:  0.5032995939254761
train gradient:  0.13965024615211652
iteration : 12217
train acc:  0.7890625
train loss:  0.4930058717727661
train gradient:  0.13017382476682302
iteration : 12218
train acc:  0.7578125
train loss:  0.4849526584148407
train gradient:  0.14091701427167735
iteration : 12219
train acc:  0.796875
train loss:  0.42007511854171753
train gradient:  0.08409742439210496
iteration : 12220
train acc:  0.6796875
train loss:  0.54852694272995
train gradient:  0.158259023088286
iteration : 12221
train acc:  0.75
train loss:  0.44494831562042236
train gradient:  0.12985079619731854
iteration : 12222
train acc:  0.7109375
train loss:  0.5491898655891418
train gradient:  0.14780309835378866
iteration : 12223
train acc:  0.7421875
train loss:  0.5222986340522766
train gradient:  0.14289500575519543
iteration : 12224
train acc:  0.75
train loss:  0.48158028721809387
train gradient:  0.11788110419493536
iteration : 12225
train acc:  0.7578125
train loss:  0.49077585339546204
train gradient:  0.16420273971701288
iteration : 12226
train acc:  0.7109375
train loss:  0.5304640531539917
train gradient:  0.13810939561190672
iteration : 12227
train acc:  0.7109375
train loss:  0.491926908493042
train gradient:  0.144810816159749
iteration : 12228
train acc:  0.765625
train loss:  0.47692424058914185
train gradient:  0.13850905443143754
iteration : 12229
train acc:  0.640625
train loss:  0.5582553148269653
train gradient:  0.21422813642019206
iteration : 12230
train acc:  0.734375
train loss:  0.47672373056411743
train gradient:  0.1317238103883257
iteration : 12231
train acc:  0.6640625
train loss:  0.5327539443969727
train gradient:  0.1217781435791564
iteration : 12232
train acc:  0.7421875
train loss:  0.44106346368789673
train gradient:  0.0855231323059296
iteration : 12233
train acc:  0.765625
train loss:  0.4817460775375366
train gradient:  0.17003180996435036
iteration : 12234
train acc:  0.71875
train loss:  0.52804034948349
train gradient:  0.14137577236107024
iteration : 12235
train acc:  0.65625
train loss:  0.5436751246452332
train gradient:  0.14820407236937183
iteration : 12236
train acc:  0.734375
train loss:  0.48778268694877625
train gradient:  0.11990385106854146
iteration : 12237
train acc:  0.75
train loss:  0.46729010343551636
train gradient:  0.10336324349506838
iteration : 12238
train acc:  0.6796875
train loss:  0.5230687856674194
train gradient:  0.13454128616419256
iteration : 12239
train acc:  0.7421875
train loss:  0.4838813245296478
train gradient:  0.15649999899697997
iteration : 12240
train acc:  0.71875
train loss:  0.5138508677482605
train gradient:  0.1532990520305772
iteration : 12241
train acc:  0.765625
train loss:  0.47932764887809753
train gradient:  0.1180551105244106
iteration : 12242
train acc:  0.8125
train loss:  0.43961235880851746
train gradient:  0.12373589316080123
iteration : 12243
train acc:  0.7578125
train loss:  0.44975778460502625
train gradient:  0.09929417686448667
iteration : 12244
train acc:  0.75
train loss:  0.4587561786174774
train gradient:  0.1155942777164863
iteration : 12245
train acc:  0.7734375
train loss:  0.48356515169143677
train gradient:  0.1417917931422374
iteration : 12246
train acc:  0.7578125
train loss:  0.5129075646400452
train gradient:  0.14058242441850538
iteration : 12247
train acc:  0.7109375
train loss:  0.49317336082458496
train gradient:  0.1375112618020179
iteration : 12248
train acc:  0.6875
train loss:  0.5626290440559387
train gradient:  0.142568252988067
iteration : 12249
train acc:  0.6640625
train loss:  0.5895960330963135
train gradient:  0.14295737454403723
iteration : 12250
train acc:  0.7734375
train loss:  0.48909324407577515
train gradient:  0.1274564072908314
iteration : 12251
train acc:  0.796875
train loss:  0.4602293074131012
train gradient:  0.11596986259953573
iteration : 12252
train acc:  0.7109375
train loss:  0.5426797866821289
train gradient:  0.1621141401425989
iteration : 12253
train acc:  0.8046875
train loss:  0.4688132405281067
train gradient:  0.10824002934125417
iteration : 12254
train acc:  0.75
train loss:  0.48977527022361755
train gradient:  0.14796053083360872
iteration : 12255
train acc:  0.7109375
train loss:  0.5184828042984009
train gradient:  0.1416204020190177
iteration : 12256
train acc:  0.78125
train loss:  0.46281635761260986
train gradient:  0.11056840745645588
iteration : 12257
train acc:  0.7421875
train loss:  0.4618772268295288
train gradient:  0.11027309928679377
iteration : 12258
train acc:  0.7578125
train loss:  0.4823184609413147
train gradient:  0.13433496958974783
iteration : 12259
train acc:  0.796875
train loss:  0.4575900435447693
train gradient:  0.10814499243514057
iteration : 12260
train acc:  0.765625
train loss:  0.5157032012939453
train gradient:  0.14645337534471964
iteration : 12261
train acc:  0.7578125
train loss:  0.4989088177680969
train gradient:  0.12010649972796296
iteration : 12262
train acc:  0.734375
train loss:  0.48662954568862915
train gradient:  0.13221160569166623
iteration : 12263
train acc:  0.7265625
train loss:  0.5022412538528442
train gradient:  0.15022797627713752
iteration : 12264
train acc:  0.734375
train loss:  0.5338839292526245
train gradient:  0.16105812533656116
iteration : 12265
train acc:  0.8359375
train loss:  0.4145813584327698
train gradient:  0.09129238367906503
iteration : 12266
train acc:  0.8046875
train loss:  0.4280184507369995
train gradient:  0.11197925752645106
iteration : 12267
train acc:  0.796875
train loss:  0.4264075756072998
train gradient:  0.087634425644664
iteration : 12268
train acc:  0.75
train loss:  0.5143988132476807
train gradient:  0.13978440433537143
iteration : 12269
train acc:  0.8359375
train loss:  0.384364515542984
train gradient:  0.1269153960909417
iteration : 12270
train acc:  0.71875
train loss:  0.5002716779708862
train gradient:  0.14426825632260842
iteration : 12271
train acc:  0.7734375
train loss:  0.43849462270736694
train gradient:  0.11265250788220531
iteration : 12272
train acc:  0.8125
train loss:  0.4117525517940521
train gradient:  0.09874139739541543
iteration : 12273
train acc:  0.75
train loss:  0.4382595121860504
train gradient:  0.12262140654008273
iteration : 12274
train acc:  0.78125
train loss:  0.4614712595939636
train gradient:  0.13028093258547777
iteration : 12275
train acc:  0.7890625
train loss:  0.4489157795906067
train gradient:  0.0934598610877899
iteration : 12276
train acc:  0.6953125
train loss:  0.5519359707832336
train gradient:  0.15582039401396663
iteration : 12277
train acc:  0.8046875
train loss:  0.3671451807022095
train gradient:  0.082334053817948
iteration : 12278
train acc:  0.75
train loss:  0.45346081256866455
train gradient:  0.1131460646486257
iteration : 12279
train acc:  0.71875
train loss:  0.5180835723876953
train gradient:  0.13249567599477602
iteration : 12280
train acc:  0.6953125
train loss:  0.5439953804016113
train gradient:  0.1422774197341096
iteration : 12281
train acc:  0.71875
train loss:  0.4904303550720215
train gradient:  0.11586272454294215
iteration : 12282
train acc:  0.765625
train loss:  0.46457090973854065
train gradient:  0.11923598372340391
iteration : 12283
train acc:  0.765625
train loss:  0.46711593866348267
train gradient:  0.12171811475666221
iteration : 12284
train acc:  0.75
train loss:  0.46109074354171753
train gradient:  0.1057978572963536
iteration : 12285
train acc:  0.6953125
train loss:  0.598084032535553
train gradient:  0.2012303979627778
iteration : 12286
train acc:  0.7578125
train loss:  0.5025283098220825
train gradient:  0.12154814785552734
iteration : 12287
train acc:  0.7578125
train loss:  0.45847398042678833
train gradient:  0.10923146564642967
iteration : 12288
train acc:  0.734375
train loss:  0.49723878502845764
train gradient:  0.12968570879380764
iteration : 12289
train acc:  0.8515625
train loss:  0.40967145562171936
train gradient:  0.10046251744860872
iteration : 12290
train acc:  0.75
train loss:  0.4877815246582031
train gradient:  0.12602486645341385
iteration : 12291
train acc:  0.7578125
train loss:  0.44796234369277954
train gradient:  0.11267441451247719
iteration : 12292
train acc:  0.7265625
train loss:  0.5360482931137085
train gradient:  0.15726324976950928
iteration : 12293
train acc:  0.78125
train loss:  0.4294598400592804
train gradient:  0.11777494734136632
iteration : 12294
train acc:  0.75
train loss:  0.46863940358161926
train gradient:  0.09866269844611542
iteration : 12295
train acc:  0.7109375
train loss:  0.550000786781311
train gradient:  0.20029482663985793
iteration : 12296
train acc:  0.7578125
train loss:  0.46745365858078003
train gradient:  0.11383975726977542
iteration : 12297
train acc:  0.7265625
train loss:  0.5231530666351318
train gradient:  0.14213892154416288
iteration : 12298
train acc:  0.71875
train loss:  0.4918563961982727
train gradient:  0.11377976654019155
iteration : 12299
train acc:  0.71875
train loss:  0.4658682644367218
train gradient:  0.09982429465372833
iteration : 12300
train acc:  0.7734375
train loss:  0.4611234664916992
train gradient:  0.12835102662063005
iteration : 12301
train acc:  0.6640625
train loss:  0.5342063307762146
train gradient:  0.15173347458444672
iteration : 12302
train acc:  0.765625
train loss:  0.4595400393009186
train gradient:  0.09790750066781351
iteration : 12303
train acc:  0.75
train loss:  0.48234719038009644
train gradient:  0.12584012193978467
iteration : 12304
train acc:  0.7421875
train loss:  0.4809364080429077
train gradient:  0.13743584030688916
iteration : 12305
train acc:  0.6640625
train loss:  0.5832394361495972
train gradient:  0.1598458022208491
iteration : 12306
train acc:  0.7265625
train loss:  0.5116969347000122
train gradient:  0.13217176736194436
iteration : 12307
train acc:  0.71875
train loss:  0.5198295712471008
train gradient:  0.15156024452645436
iteration : 12308
train acc:  0.7578125
train loss:  0.45413926243782043
train gradient:  0.11577842054532995
iteration : 12309
train acc:  0.703125
train loss:  0.5747579336166382
train gradient:  0.16700264528105915
iteration : 12310
train acc:  0.75
train loss:  0.4994036555290222
train gradient:  0.1335808341906462
iteration : 12311
train acc:  0.7578125
train loss:  0.4602764844894409
train gradient:  0.11097495424682888
iteration : 12312
train acc:  0.6796875
train loss:  0.5698957443237305
train gradient:  0.15244622714836115
iteration : 12313
train acc:  0.7265625
train loss:  0.4867088198661804
train gradient:  0.17247841895155983
iteration : 12314
train acc:  0.703125
train loss:  0.5270887017250061
train gradient:  0.1552899636322843
iteration : 12315
train acc:  0.71875
train loss:  0.506200909614563
train gradient:  0.1312520907631538
iteration : 12316
train acc:  0.71875
train loss:  0.4818083345890045
train gradient:  0.10851903508729206
iteration : 12317
train acc:  0.7265625
train loss:  0.47579050064086914
train gradient:  0.10323644643186891
iteration : 12318
train acc:  0.765625
train loss:  0.47617894411087036
train gradient:  0.12277662434728624
iteration : 12319
train acc:  0.625
train loss:  0.6137120723724365
train gradient:  0.17928640763594828
iteration : 12320
train acc:  0.6953125
train loss:  0.5292896628379822
train gradient:  0.13839938628209308
iteration : 12321
train acc:  0.71875
train loss:  0.5165270566940308
train gradient:  0.15216813358692557
iteration : 12322
train acc:  0.71875
train loss:  0.4818210303783417
train gradient:  0.13714044689034688
iteration : 12323
train acc:  0.7890625
train loss:  0.45154088735580444
train gradient:  0.20136165141204412
iteration : 12324
train acc:  0.8046875
train loss:  0.4243941009044647
train gradient:  0.1089797620434254
iteration : 12325
train acc:  0.7421875
train loss:  0.507492184638977
train gradient:  0.24203260592861234
iteration : 12326
train acc:  0.7421875
train loss:  0.49545013904571533
train gradient:  0.11530734655147198
iteration : 12327
train acc:  0.7421875
train loss:  0.523300051689148
train gradient:  0.13473504563917893
iteration : 12328
train acc:  0.71875
train loss:  0.5770468711853027
train gradient:  0.1579040933313619
iteration : 12329
train acc:  0.75
train loss:  0.45259809494018555
train gradient:  0.10275056090027318
iteration : 12330
train acc:  0.7265625
train loss:  0.5018791556358337
train gradient:  0.11712287987200096
iteration : 12331
train acc:  0.78125
train loss:  0.46957287192344666
train gradient:  0.13395905048948123
iteration : 12332
train acc:  0.8125
train loss:  0.4153097867965698
train gradient:  0.09532572597230952
iteration : 12333
train acc:  0.796875
train loss:  0.4275050759315491
train gradient:  0.13770178835725555
iteration : 12334
train acc:  0.7578125
train loss:  0.4732276499271393
train gradient:  0.10091836927515271
iteration : 12335
train acc:  0.7734375
train loss:  0.4989643096923828
train gradient:  0.11749893585774254
iteration : 12336
train acc:  0.796875
train loss:  0.45334577560424805
train gradient:  0.08019395607946381
iteration : 12337
train acc:  0.765625
train loss:  0.4772227108478546
train gradient:  0.12554118533293854
iteration : 12338
train acc:  0.7890625
train loss:  0.4699724018573761
train gradient:  0.09473577860439535
iteration : 12339
train acc:  0.703125
train loss:  0.5330083966255188
train gradient:  0.1346590274470132
iteration : 12340
train acc:  0.7109375
train loss:  0.5213799476623535
train gradient:  0.13065217981791855
iteration : 12341
train acc:  0.7421875
train loss:  0.6117236614227295
train gradient:  0.17415728691566623
iteration : 12342
train acc:  0.78125
train loss:  0.46036699414253235
train gradient:  0.11837171445437317
iteration : 12343
train acc:  0.7734375
train loss:  0.5041472315788269
train gradient:  0.1705700697511631
iteration : 12344
train acc:  0.7421875
train loss:  0.4757600426673889
train gradient:  0.15221239052406854
iteration : 12345
train acc:  0.7421875
train loss:  0.5019989013671875
train gradient:  0.10337615924789344
iteration : 12346
train acc:  0.6796875
train loss:  0.5893653631210327
train gradient:  0.16977320706881788
iteration : 12347
train acc:  0.7890625
train loss:  0.4447672963142395
train gradient:  0.09690533830999074
iteration : 12348
train acc:  0.6484375
train loss:  0.627945065498352
train gradient:  0.21450268127220373
iteration : 12349
train acc:  0.7265625
train loss:  0.5280259847640991
train gradient:  0.1417088888416253
iteration : 12350
train acc:  0.78125
train loss:  0.4266064763069153
train gradient:  0.10648335812836657
iteration : 12351
train acc:  0.765625
train loss:  0.4695730209350586
train gradient:  0.11683274767998277
iteration : 12352
train acc:  0.7734375
train loss:  0.42680060863494873
train gradient:  0.10479407492706812
iteration : 12353
train acc:  0.75
train loss:  0.510136604309082
train gradient:  0.14147454855249464
iteration : 12354
train acc:  0.8125
train loss:  0.5039377212524414
train gradient:  0.11419793351675658
iteration : 12355
train acc:  0.7734375
train loss:  0.46146780252456665
train gradient:  0.10373081231638857
iteration : 12356
train acc:  0.7265625
train loss:  0.49141725897789
train gradient:  0.1239678046046534
iteration : 12357
train acc:  0.7265625
train loss:  0.5068764686584473
train gradient:  0.14378475399681728
iteration : 12358
train acc:  0.71875
train loss:  0.5320219993591309
train gradient:  0.21138362250169557
iteration : 12359
train acc:  0.71875
train loss:  0.5131333470344543
train gradient:  0.11711667135035189
iteration : 12360
train acc:  0.671875
train loss:  0.5937676429748535
train gradient:  0.1623481169625023
iteration : 12361
train acc:  0.75
train loss:  0.545962393283844
train gradient:  0.14528323738856555
iteration : 12362
train acc:  0.7890625
train loss:  0.41510719060897827
train gradient:  0.10272086086255912
iteration : 12363
train acc:  0.7421875
train loss:  0.4599810242652893
train gradient:  0.11483956693254128
iteration : 12364
train acc:  0.7109375
train loss:  0.5599079132080078
train gradient:  0.13774563545988322
iteration : 12365
train acc:  0.6875
train loss:  0.5278241634368896
train gradient:  0.1430700427307646
iteration : 12366
train acc:  0.796875
train loss:  0.4322430491447449
train gradient:  0.10175388901076803
iteration : 12367
train acc:  0.6640625
train loss:  0.5680415034294128
train gradient:  0.16209529139422485
iteration : 12368
train acc:  0.7421875
train loss:  0.48865073919296265
train gradient:  0.10750623773847909
iteration : 12369
train acc:  0.7421875
train loss:  0.48239707946777344
train gradient:  0.12056011107001463
iteration : 12370
train acc:  0.734375
train loss:  0.5098982453346252
train gradient:  0.13547421928091136
iteration : 12371
train acc:  0.6640625
train loss:  0.5801424384117126
train gradient:  0.17683273221433166
iteration : 12372
train acc:  0.7265625
train loss:  0.5239039063453674
train gradient:  0.09587978532238554
iteration : 12373
train acc:  0.8515625
train loss:  0.3920106291770935
train gradient:  0.0930028518245245
iteration : 12374
train acc:  0.765625
train loss:  0.47819042205810547
train gradient:  0.09866866186541046
iteration : 12375
train acc:  0.734375
train loss:  0.49370017647743225
train gradient:  0.12844864442608328
iteration : 12376
train acc:  0.75
train loss:  0.47146278619766235
train gradient:  0.11952543045862495
iteration : 12377
train acc:  0.7421875
train loss:  0.4927400052547455
train gradient:  0.1163461232537391
iteration : 12378
train acc:  0.7265625
train loss:  0.4945850968360901
train gradient:  0.12206702996797787
iteration : 12379
train acc:  0.7578125
train loss:  0.5261037349700928
train gradient:  0.14786614585130564
iteration : 12380
train acc:  0.796875
train loss:  0.4161548316478729
train gradient:  0.09248154391319446
iteration : 12381
train acc:  0.7109375
train loss:  0.49328434467315674
train gradient:  0.13993269893757054
iteration : 12382
train acc:  0.765625
train loss:  0.49212104082107544
train gradient:  0.145845123936728
iteration : 12383
train acc:  0.6796875
train loss:  0.6035977005958557
train gradient:  0.2075661536699509
iteration : 12384
train acc:  0.7578125
train loss:  0.4976515769958496
train gradient:  0.1364197183344907
iteration : 12385
train acc:  0.78125
train loss:  0.4445710778236389
train gradient:  0.1074514867309802
iteration : 12386
train acc:  0.7578125
train loss:  0.4692723751068115
train gradient:  0.10548387484001522
iteration : 12387
train acc:  0.75
train loss:  0.5128575563430786
train gradient:  0.1503115995844995
iteration : 12388
train acc:  0.7578125
train loss:  0.4880700409412384
train gradient:  0.11821697145548128
iteration : 12389
train acc:  0.7265625
train loss:  0.4434713125228882
train gradient:  0.10594789595395587
iteration : 12390
train acc:  0.75
train loss:  0.4819255769252777
train gradient:  0.11263415080676946
iteration : 12391
train acc:  0.6953125
train loss:  0.5364786386489868
train gradient:  0.13580598131894817
iteration : 12392
train acc:  0.7578125
train loss:  0.46426093578338623
train gradient:  0.10802034193513449
iteration : 12393
train acc:  0.703125
train loss:  0.5944388508796692
train gradient:  0.1815182063689674
iteration : 12394
train acc:  0.6875
train loss:  0.5231356620788574
train gradient:  0.14395968095772946
iteration : 12395
train acc:  0.7421875
train loss:  0.5134161710739136
train gradient:  0.16540509210591842
iteration : 12396
train acc:  0.796875
train loss:  0.41607463359832764
train gradient:  0.09078143496996097
iteration : 12397
train acc:  0.7265625
train loss:  0.5013746619224548
train gradient:  0.13385773737910817
iteration : 12398
train acc:  0.765625
train loss:  0.5090948939323425
train gradient:  0.13311558961730985
iteration : 12399
train acc:  0.7890625
train loss:  0.4304584264755249
train gradient:  0.09230931916088324
iteration : 12400
train acc:  0.8359375
train loss:  0.4001544415950775
train gradient:  0.08262424602443114
iteration : 12401
train acc:  0.6796875
train loss:  0.559053897857666
train gradient:  0.1429802103724573
iteration : 12402
train acc:  0.71875
train loss:  0.49952447414398193
train gradient:  0.11622302278284107
iteration : 12403
train acc:  0.8046875
train loss:  0.4059256911277771
train gradient:  0.09173284058708946
iteration : 12404
train acc:  0.8203125
train loss:  0.42648494243621826
train gradient:  0.08827758300749819
iteration : 12405
train acc:  0.7890625
train loss:  0.44630834460258484
train gradient:  0.15423918546753626
iteration : 12406
train acc:  0.75
train loss:  0.48024100065231323
train gradient:  0.14143936178694624
iteration : 12407
train acc:  0.734375
train loss:  0.48344722390174866
train gradient:  0.1272684014582402
iteration : 12408
train acc:  0.7265625
train loss:  0.46871283650398254
train gradient:  0.1052036492792529
iteration : 12409
train acc:  0.671875
train loss:  0.5510753393173218
train gradient:  0.17380939973378184
iteration : 12410
train acc:  0.734375
train loss:  0.5079231262207031
train gradient:  0.11933346076907221
iteration : 12411
train acc:  0.6875
train loss:  0.5475276112556458
train gradient:  0.1544761612600219
iteration : 12412
train acc:  0.7421875
train loss:  0.5077670812606812
train gradient:  0.1279909947039508
iteration : 12413
train acc:  0.765625
train loss:  0.4893653392791748
train gradient:  0.10929547060116641
iteration : 12414
train acc:  0.7890625
train loss:  0.4689306318759918
train gradient:  0.09594677494976152
iteration : 12415
train acc:  0.7109375
train loss:  0.5219498872756958
train gradient:  0.11283634590600367
iteration : 12416
train acc:  0.7421875
train loss:  0.5105748176574707
train gradient:  0.15825018214365807
iteration : 12417
train acc:  0.6640625
train loss:  0.5756579041481018
train gradient:  0.15122068445056125
iteration : 12418
train acc:  0.71875
train loss:  0.548835813999176
train gradient:  0.14268128067081332
iteration : 12419
train acc:  0.7265625
train loss:  0.4993959963321686
train gradient:  0.13797527404289728
iteration : 12420
train acc:  0.7265625
train loss:  0.4841923117637634
train gradient:  0.10519054166340303
iteration : 12421
train acc:  0.6640625
train loss:  0.5784957408905029
train gradient:  0.19104236754441775
iteration : 12422
train acc:  0.734375
train loss:  0.5287944674491882
train gradient:  0.14137903453471928
iteration : 12423
train acc:  0.734375
train loss:  0.501582682132721
train gradient:  0.12190525435476122
iteration : 12424
train acc:  0.7890625
train loss:  0.46084678173065186
train gradient:  0.1052475909331994
iteration : 12425
train acc:  0.75
train loss:  0.5248726010322571
train gradient:  0.12815890489652357
iteration : 12426
train acc:  0.7578125
train loss:  0.47933080792427063
train gradient:  0.09773117001677784
iteration : 12427
train acc:  0.796875
train loss:  0.4770733714103699
train gradient:  0.11703466930009
iteration : 12428
train acc:  0.7578125
train loss:  0.5063539743423462
train gradient:  0.12372405001976954
iteration : 12429
train acc:  0.734375
train loss:  0.44167181849479675
train gradient:  0.11092837774057167
iteration : 12430
train acc:  0.7265625
train loss:  0.5080700516700745
train gradient:  0.11953641836556765
iteration : 12431
train acc:  0.7734375
train loss:  0.43507978320121765
train gradient:  0.0971951931798355
iteration : 12432
train acc:  0.71875
train loss:  0.5291523933410645
train gradient:  0.11384883704373197
iteration : 12433
train acc:  0.765625
train loss:  0.45244932174682617
train gradient:  0.11703700286802352
iteration : 12434
train acc:  0.703125
train loss:  0.4938426613807678
train gradient:  0.1323569261853728
iteration : 12435
train acc:  0.765625
train loss:  0.4515802562236786
train gradient:  0.08981796022421504
iteration : 12436
train acc:  0.7890625
train loss:  0.4449717402458191
train gradient:  0.09256226609597959
iteration : 12437
train acc:  0.7890625
train loss:  0.4384446144104004
train gradient:  0.09833544493738178
iteration : 12438
train acc:  0.6875
train loss:  0.5233948230743408
train gradient:  0.11468085600626955
iteration : 12439
train acc:  0.796875
train loss:  0.47982797026634216
train gradient:  0.11477496145005181
iteration : 12440
train acc:  0.7109375
train loss:  0.5605654716491699
train gradient:  0.21412560413607432
iteration : 12441
train acc:  0.7109375
train loss:  0.5238021612167358
train gradient:  0.1547960580186457
iteration : 12442
train acc:  0.6953125
train loss:  0.5640420913696289
train gradient:  0.19197872832306884
iteration : 12443
train acc:  0.7265625
train loss:  0.47597265243530273
train gradient:  0.13169609209904015
iteration : 12444
train acc:  0.6875
train loss:  0.5384091138839722
train gradient:  0.15549765439915447
iteration : 12445
train acc:  0.7578125
train loss:  0.5444544553756714
train gradient:  0.15575487664020446
iteration : 12446
train acc:  0.734375
train loss:  0.5013068914413452
train gradient:  0.11009103469092928
iteration : 12447
train acc:  0.6328125
train loss:  0.6646365523338318
train gradient:  0.2163131417983839
iteration : 12448
train acc:  0.734375
train loss:  0.4944896996021271
train gradient:  0.1376112778915528
iteration : 12449
train acc:  0.7734375
train loss:  0.49179190397262573
train gradient:  0.13112515472912284
iteration : 12450
train acc:  0.7421875
train loss:  0.4791557490825653
train gradient:  0.12524303827582217
iteration : 12451
train acc:  0.7265625
train loss:  0.5203437209129333
train gradient:  0.1531605371635869
iteration : 12452
train acc:  0.734375
train loss:  0.5327069759368896
train gradient:  0.13555889350043843
iteration : 12453
train acc:  0.7265625
train loss:  0.4895927906036377
train gradient:  0.1128316054709328
iteration : 12454
train acc:  0.7265625
train loss:  0.5189545154571533
train gradient:  0.15301308637815098
iteration : 12455
train acc:  0.6953125
train loss:  0.5307525396347046
train gradient:  0.1252742277849026
iteration : 12456
train acc:  0.796875
train loss:  0.5063040256500244
train gradient:  0.1349885561656971
iteration : 12457
train acc:  0.7265625
train loss:  0.5226311087608337
train gradient:  0.16725373696143775
iteration : 12458
train acc:  0.75
train loss:  0.4985886514186859
train gradient:  0.11120842476876021
iteration : 12459
train acc:  0.7421875
train loss:  0.49155014753341675
train gradient:  0.12093807764998668
iteration : 12460
train acc:  0.734375
train loss:  0.45215266942977905
train gradient:  0.10220708252597427
iteration : 12461
train acc:  0.734375
train loss:  0.47744354605674744
train gradient:  0.11193857311559396
iteration : 12462
train acc:  0.6953125
train loss:  0.5175539255142212
train gradient:  0.15427229574048765
iteration : 12463
train acc:  0.8203125
train loss:  0.4139898419380188
train gradient:  0.07802940782343025
iteration : 12464
train acc:  0.7421875
train loss:  0.4769344925880432
train gradient:  0.10264951897150543
iteration : 12465
train acc:  0.75
train loss:  0.4752315580844879
train gradient:  0.16235095067305438
iteration : 12466
train acc:  0.75
train loss:  0.5085597038269043
train gradient:  0.13883011337444764
iteration : 12467
train acc:  0.7265625
train loss:  0.4855235815048218
train gradient:  0.16799298143156388
iteration : 12468
train acc:  0.7109375
train loss:  0.49737924337387085
train gradient:  0.1329833968741292
iteration : 12469
train acc:  0.7265625
train loss:  0.5180319547653198
train gradient:  0.1333124382305212
iteration : 12470
train acc:  0.7578125
train loss:  0.5013741850852966
train gradient:  0.14061037799030635
iteration : 12471
train acc:  0.765625
train loss:  0.4588161110877991
train gradient:  0.12473721115899569
iteration : 12472
train acc:  0.765625
train loss:  0.4768250584602356
train gradient:  0.10875927094860703
iteration : 12473
train acc:  0.8203125
train loss:  0.458204448223114
train gradient:  0.11670550203602172
iteration : 12474
train acc:  0.75
train loss:  0.5319696068763733
train gradient:  0.12019291639489077
iteration : 12475
train acc:  0.734375
train loss:  0.47527843713760376
train gradient:  0.12913840390962886
iteration : 12476
train acc:  0.765625
train loss:  0.46194860339164734
train gradient:  0.12051265128683301
iteration : 12477
train acc:  0.7890625
train loss:  0.47330665588378906
train gradient:  0.09656680425162387
iteration : 12478
train acc:  0.75
train loss:  0.5027062892913818
train gradient:  0.14404516930973826
iteration : 12479
train acc:  0.7109375
train loss:  0.520460307598114
train gradient:  0.1323290661772255
iteration : 12480
train acc:  0.71875
train loss:  0.5405580997467041
train gradient:  0.1606573729937688
iteration : 12481
train acc:  0.7265625
train loss:  0.48269474506378174
train gradient:  0.1360001231274324
iteration : 12482
train acc:  0.703125
train loss:  0.5186225771903992
train gradient:  0.1407298271901757
iteration : 12483
train acc:  0.7265625
train loss:  0.49648937582969666
train gradient:  0.1440257882122668
iteration : 12484
train acc:  0.75
train loss:  0.47219765186309814
train gradient:  0.1030413964595998
iteration : 12485
train acc:  0.75
train loss:  0.5446665287017822
train gradient:  0.1511201383150121
iteration : 12486
train acc:  0.6796875
train loss:  0.6140982508659363
train gradient:  0.18935674808779523
iteration : 12487
train acc:  0.75
train loss:  0.5237540006637573
train gradient:  0.15095047525093797
iteration : 12488
train acc:  0.7890625
train loss:  0.4274539351463318
train gradient:  0.08671226674865924
iteration : 12489
train acc:  0.765625
train loss:  0.4315553903579712
train gradient:  0.09217978450913253
iteration : 12490
train acc:  0.671875
train loss:  0.5828111171722412
train gradient:  0.15135517702624324
iteration : 12491
train acc:  0.765625
train loss:  0.4559173583984375
train gradient:  0.09543375440333289
iteration : 12492
train acc:  0.7265625
train loss:  0.531023383140564
train gradient:  0.1831430065768265
iteration : 12493
train acc:  0.703125
train loss:  0.4980185031890869
train gradient:  0.13683470567145428
iteration : 12494
train acc:  0.7578125
train loss:  0.5200743675231934
train gradient:  0.15745734936318062
iteration : 12495
train acc:  0.8046875
train loss:  0.5012603998184204
train gradient:  0.1191945008529867
iteration : 12496
train acc:  0.8046875
train loss:  0.4268178343772888
train gradient:  0.08841480463926381
iteration : 12497
train acc:  0.7578125
train loss:  0.47723475098609924
train gradient:  0.13796991480461912
iteration : 12498
train acc:  0.7421875
train loss:  0.4783673882484436
train gradient:  0.11158092213576837
iteration : 12499
train acc:  0.75
train loss:  0.48740115761756897
train gradient:  0.10516145424765369
iteration : 12500
train acc:  0.765625
train loss:  0.4783618450164795
train gradient:  0.11049098344741556
iteration : 12501
train acc:  0.7109375
train loss:  0.5548123121261597
train gradient:  0.16666831806773216
iteration : 12502
train acc:  0.734375
train loss:  0.46257174015045166
train gradient:  0.14969297204512375
iteration : 12503
train acc:  0.7890625
train loss:  0.41863691806793213
train gradient:  0.10869054192086182
iteration : 12504
train acc:  0.7109375
train loss:  0.529263973236084
train gradient:  0.12785853957823134
iteration : 12505
train acc:  0.6953125
train loss:  0.5312423706054688
train gradient:  0.14825302568115778
iteration : 12506
train acc:  0.7109375
train loss:  0.48089301586151123
train gradient:  0.11991469314712695
iteration : 12507
train acc:  0.7109375
train loss:  0.5577622652053833
train gradient:  0.15488775317177222
iteration : 12508
train acc:  0.6875
train loss:  0.5170719623565674
train gradient:  0.13101028494559364
iteration : 12509
train acc:  0.734375
train loss:  0.493644654750824
train gradient:  0.10009724582446283
iteration : 12510
train acc:  0.7578125
train loss:  0.4761698246002197
train gradient:  0.1395749562877447
iteration : 12511
train acc:  0.71875
train loss:  0.5005269050598145
train gradient:  0.10345479101239063
iteration : 12512
train acc:  0.7109375
train loss:  0.5184723734855652
train gradient:  0.11170709689157714
iteration : 12513
train acc:  0.796875
train loss:  0.42024099826812744
train gradient:  0.09561889485002092
iteration : 12514
train acc:  0.703125
train loss:  0.48480552434921265
train gradient:  0.11546747296382096
iteration : 12515
train acc:  0.671875
train loss:  0.5685558319091797
train gradient:  0.1532048990788034
iteration : 12516
train acc:  0.6875
train loss:  0.5244842767715454
train gradient:  0.1417980980387658
iteration : 12517
train acc:  0.8046875
train loss:  0.4435698986053467
train gradient:  0.09378467802045011
iteration : 12518
train acc:  0.7578125
train loss:  0.4696492850780487
train gradient:  0.09987879008407251
iteration : 12519
train acc:  0.7734375
train loss:  0.4837513566017151
train gradient:  0.08964201249584274
iteration : 12520
train acc:  0.671875
train loss:  0.5626944899559021
train gradient:  0.15459347252564926
iteration : 12521
train acc:  0.734375
train loss:  0.47976934909820557
train gradient:  0.10569216076829657
iteration : 12522
train acc:  0.7734375
train loss:  0.4497368335723877
train gradient:  0.10332487745157529
iteration : 12523
train acc:  0.78125
train loss:  0.4339277446269989
train gradient:  0.09542991192117342
iteration : 12524
train acc:  0.7421875
train loss:  0.4983823895454407
train gradient:  0.11581859452040254
iteration : 12525
train acc:  0.7265625
train loss:  0.49643659591674805
train gradient:  0.12838446955803168
iteration : 12526
train acc:  0.7890625
train loss:  0.44391560554504395
train gradient:  0.09720341005652083
iteration : 12527
train acc:  0.734375
train loss:  0.5402118563652039
train gradient:  0.13890105698692623
iteration : 12528
train acc:  0.7578125
train loss:  0.5328247547149658
train gradient:  0.15778006663445054
iteration : 12529
train acc:  0.71875
train loss:  0.4914276599884033
train gradient:  0.11672799693822161
iteration : 12530
train acc:  0.7578125
train loss:  0.4836929440498352
train gradient:  0.1109028680260235
iteration : 12531
train acc:  0.734375
train loss:  0.46971943974494934
train gradient:  0.11758098056255016
iteration : 12532
train acc:  0.7578125
train loss:  0.46700745820999146
train gradient:  0.14389028417943672
iteration : 12533
train acc:  0.734375
train loss:  0.459123820066452
train gradient:  0.10513509983184635
iteration : 12534
train acc:  0.6953125
train loss:  0.548112154006958
train gradient:  0.14459518299963764
iteration : 12535
train acc:  0.75
train loss:  0.48731857538223267
train gradient:  0.13755140306613442
iteration : 12536
train acc:  0.734375
train loss:  0.5056120753288269
train gradient:  0.1555080543278754
iteration : 12537
train acc:  0.75
train loss:  0.5067188739776611
train gradient:  0.13449030524819672
iteration : 12538
train acc:  0.78125
train loss:  0.4816361367702484
train gradient:  0.1071017297507332
iteration : 12539
train acc:  0.7734375
train loss:  0.45482733845710754
train gradient:  0.08889072210800845
iteration : 12540
train acc:  0.75
train loss:  0.4863792657852173
train gradient:  0.12653844508859302
iteration : 12541
train acc:  0.71875
train loss:  0.5223815441131592
train gradient:  0.1369140267512851
iteration : 12542
train acc:  0.734375
train loss:  0.47035646438598633
train gradient:  0.10477720607480245
iteration : 12543
train acc:  0.8046875
train loss:  0.4592002034187317
train gradient:  0.11127725946591158
iteration : 12544
train acc:  0.7734375
train loss:  0.4597100615501404
train gradient:  0.10641432845909406
iteration : 12545
train acc:  0.6640625
train loss:  0.5357440710067749
train gradient:  0.15219694956779373
iteration : 12546
train acc:  0.734375
train loss:  0.5818434357643127
train gradient:  0.2298127475746346
iteration : 12547
train acc:  0.7890625
train loss:  0.440263032913208
train gradient:  0.10544488722962578
iteration : 12548
train acc:  0.796875
train loss:  0.43060824275016785
train gradient:  0.09219410090357875
iteration : 12549
train acc:  0.7109375
train loss:  0.5005235075950623
train gradient:  0.1232265240849137
iteration : 12550
train acc:  0.7578125
train loss:  0.4794742166996002
train gradient:  0.09796010223671871
iteration : 12551
train acc:  0.734375
train loss:  0.47770071029663086
train gradient:  0.12040683648393054
iteration : 12552
train acc:  0.7890625
train loss:  0.44314879179000854
train gradient:  0.1050657823752344
iteration : 12553
train acc:  0.7265625
train loss:  0.5201494693756104
train gradient:  0.13772744832400516
iteration : 12554
train acc:  0.7578125
train loss:  0.49209150671958923
train gradient:  0.13464595642149435
iteration : 12555
train acc:  0.7421875
train loss:  0.48413848876953125
train gradient:  0.12190760596472713
iteration : 12556
train acc:  0.734375
train loss:  0.5208624005317688
train gradient:  0.14226094604558326
iteration : 12557
train acc:  0.7734375
train loss:  0.46569252014160156
train gradient:  0.10146130458553218
iteration : 12558
train acc:  0.8125
train loss:  0.393862247467041
train gradient:  0.0956676812948621
iteration : 12559
train acc:  0.7109375
train loss:  0.5589684247970581
train gradient:  0.16325471110967665
iteration : 12560
train acc:  0.765625
train loss:  0.4978063404560089
train gradient:  0.15315545555833743
iteration : 12561
train acc:  0.7734375
train loss:  0.47927531599998474
train gradient:  0.10524489830777338
iteration : 12562
train acc:  0.7421875
train loss:  0.4719194173812866
train gradient:  0.10297480192125603
iteration : 12563
train acc:  0.78125
train loss:  0.4362229108810425
train gradient:  0.08375918417443635
iteration : 12564
train acc:  0.734375
train loss:  0.46393465995788574
train gradient:  0.09016857044353413
iteration : 12565
train acc:  0.7734375
train loss:  0.5018095970153809
train gradient:  0.11841341774762999
iteration : 12566
train acc:  0.7109375
train loss:  0.5131080746650696
train gradient:  0.1421155757387243
iteration : 12567
train acc:  0.796875
train loss:  0.4604318141937256
train gradient:  0.10434185071653196
iteration : 12568
train acc:  0.78125
train loss:  0.46156924962997437
train gradient:  0.10844486648285495
iteration : 12569
train acc:  0.7421875
train loss:  0.4959180951118469
train gradient:  0.11472167839153087
iteration : 12570
train acc:  0.78125
train loss:  0.4542155861854553
train gradient:  0.10042992277411975
iteration : 12571
train acc:  0.734375
train loss:  0.49675196409225464
train gradient:  0.14297102954911275
iteration : 12572
train acc:  0.6953125
train loss:  0.5313829183578491
train gradient:  0.1626136295357008
iteration : 12573
train acc:  0.71875
train loss:  0.4854399561882019
train gradient:  0.13655397904839073
iteration : 12574
train acc:  0.8203125
train loss:  0.4319875240325928
train gradient:  0.09800258765261745
iteration : 12575
train acc:  0.703125
train loss:  0.5378371477127075
train gradient:  0.12328166211628928
iteration : 12576
train acc:  0.6953125
train loss:  0.5139126181602478
train gradient:  0.1121909376280045
iteration : 12577
train acc:  0.7578125
train loss:  0.47405922412872314
train gradient:  0.14061965694673673
iteration : 12578
train acc:  0.8046875
train loss:  0.45787015557289124
train gradient:  0.10187448095197234
iteration : 12579
train acc:  0.7890625
train loss:  0.4975222647190094
train gradient:  0.11823223583787118
iteration : 12580
train acc:  0.7109375
train loss:  0.5471821427345276
train gradient:  0.1499853037927345
iteration : 12581
train acc:  0.7734375
train loss:  0.5160321593284607
train gradient:  0.11194128618612456
iteration : 12582
train acc:  0.7265625
train loss:  0.48021310567855835
train gradient:  0.14762784754289987
iteration : 12583
train acc:  0.7890625
train loss:  0.4393722414970398
train gradient:  0.1271068596512541
iteration : 12584
train acc:  0.7265625
train loss:  0.4894062876701355
train gradient:  0.1159179587159205
iteration : 12585
train acc:  0.7578125
train loss:  0.45567402243614197
train gradient:  0.10585181407081838
iteration : 12586
train acc:  0.75
train loss:  0.5103715062141418
train gradient:  0.163540599465587
iteration : 12587
train acc:  0.734375
train loss:  0.49165478348731995
train gradient:  0.12452640362577919
iteration : 12588
train acc:  0.734375
train loss:  0.5242335796356201
train gradient:  0.12142530062885772
iteration : 12589
train acc:  0.71875
train loss:  0.5068376064300537
train gradient:  0.14570896747122553
iteration : 12590
train acc:  0.7578125
train loss:  0.42946457862854004
train gradient:  0.08597544644033427
iteration : 12591
train acc:  0.7734375
train loss:  0.43887707591056824
train gradient:  0.0897625762893189
iteration : 12592
train acc:  0.734375
train loss:  0.44819867610931396
train gradient:  0.09078880085683137
iteration : 12593
train acc:  0.6953125
train loss:  0.5243595838546753
train gradient:  0.13422031811567398
iteration : 12594
train acc:  0.7890625
train loss:  0.4481719434261322
train gradient:  0.12842133354902485
iteration : 12595
train acc:  0.6953125
train loss:  0.5071088075637817
train gradient:  0.14443626937789894
iteration : 12596
train acc:  0.6640625
train loss:  0.651248037815094
train gradient:  0.2354657652016527
iteration : 12597
train acc:  0.6796875
train loss:  0.502532958984375
train gradient:  0.1346649649309335
iteration : 12598
train acc:  0.6875
train loss:  0.5388860702514648
train gradient:  0.14036725063547478
iteration : 12599
train acc:  0.703125
train loss:  0.5226551294326782
train gradient:  0.11392348052653632
iteration : 12600
train acc:  0.7734375
train loss:  0.4492708146572113
train gradient:  0.1219793572082079
iteration : 12601
train acc:  0.7421875
train loss:  0.4476976990699768
train gradient:  0.11618639988669878
iteration : 12602
train acc:  0.7265625
train loss:  0.517625093460083
train gradient:  0.13762745240189195
iteration : 12603
train acc:  0.7578125
train loss:  0.5391638278961182
train gradient:  0.12082679654599959
iteration : 12604
train acc:  0.7421875
train loss:  0.43688642978668213
train gradient:  0.09002856407672309
iteration : 12605
train acc:  0.6953125
train loss:  0.5392128229141235
train gradient:  0.15223086646939576
iteration : 12606
train acc:  0.7421875
train loss:  0.5111796855926514
train gradient:  0.11293710158345784
iteration : 12607
train acc:  0.7578125
train loss:  0.5037498474121094
train gradient:  0.14735186522447388
iteration : 12608
train acc:  0.765625
train loss:  0.4356953203678131
train gradient:  0.0880018457761177
iteration : 12609
train acc:  0.703125
train loss:  0.5402650833129883
train gradient:  0.14073485045474343
iteration : 12610
train acc:  0.8125
train loss:  0.4293692111968994
train gradient:  0.09156519025698039
iteration : 12611
train acc:  0.6875
train loss:  0.5343441963195801
train gradient:  0.14447599861764732
iteration : 12612
train acc:  0.78125
train loss:  0.46180444955825806
train gradient:  0.10920211989535016
iteration : 12613
train acc:  0.7265625
train loss:  0.47061848640441895
train gradient:  0.10276767434382142
iteration : 12614
train acc:  0.7578125
train loss:  0.5078768730163574
train gradient:  0.13472284904916465
iteration : 12615
train acc:  0.71875
train loss:  0.47838518023490906
train gradient:  0.10009715524436827
iteration : 12616
train acc:  0.828125
train loss:  0.3806314468383789
train gradient:  0.08088521269858456
iteration : 12617
train acc:  0.7578125
train loss:  0.4827911853790283
train gradient:  0.11509623542306933
iteration : 12618
train acc:  0.734375
train loss:  0.5122994184494019
train gradient:  0.13660735355112302
iteration : 12619
train acc:  0.8125
train loss:  0.44453394412994385
train gradient:  0.11253409683499202
iteration : 12620
train acc:  0.7109375
train loss:  0.5580081343650818
train gradient:  0.16701582977046328
iteration : 12621
train acc:  0.71875
train loss:  0.4681444764137268
train gradient:  0.11641841240215742
iteration : 12622
train acc:  0.7109375
train loss:  0.5163251161575317
train gradient:  0.1454461743648246
iteration : 12623
train acc:  0.6953125
train loss:  0.5301291942596436
train gradient:  0.13693671040004074
iteration : 12624
train acc:  0.703125
train loss:  0.5229589343070984
train gradient:  0.12910001385219272
iteration : 12625
train acc:  0.7421875
train loss:  0.4806567430496216
train gradient:  0.12228464841665325
iteration : 12626
train acc:  0.7421875
train loss:  0.5061351656913757
train gradient:  0.128105566572658
iteration : 12627
train acc:  0.7421875
train loss:  0.4927455186843872
train gradient:  0.11413838720600031
iteration : 12628
train acc:  0.7421875
train loss:  0.49900946021080017
train gradient:  0.1438098621529248
iteration : 12629
train acc:  0.75
train loss:  0.4733455181121826
train gradient:  0.12130737360861023
iteration : 12630
train acc:  0.71875
train loss:  0.46386224031448364
train gradient:  0.09362154906484968
iteration : 12631
train acc:  0.7109375
train loss:  0.5168101787567139
train gradient:  0.1879456007736378
iteration : 12632
train acc:  0.6875
train loss:  0.5357773303985596
train gradient:  0.14273260858803313
iteration : 12633
train acc:  0.7265625
train loss:  0.558864951133728
train gradient:  0.14623902180251297
iteration : 12634
train acc:  0.7734375
train loss:  0.4575113356113434
train gradient:  0.1106762299957445
iteration : 12635
train acc:  0.7734375
train loss:  0.4929605722427368
train gradient:  0.14266570772259551
iteration : 12636
train acc:  0.71875
train loss:  0.5148131847381592
train gradient:  0.12926546745701037
iteration : 12637
train acc:  0.78125
train loss:  0.43283361196517944
train gradient:  0.0955464279718814
iteration : 12638
train acc:  0.71875
train loss:  0.49621647596359253
train gradient:  0.12882355189880002
iteration : 12639
train acc:  0.75
train loss:  0.5182557106018066
train gradient:  0.16294938078938243
iteration : 12640
train acc:  0.734375
train loss:  0.47863873839378357
train gradient:  0.10496669427612575
iteration : 12641
train acc:  0.7578125
train loss:  0.530293345451355
train gradient:  0.19436078586969802
iteration : 12642
train acc:  0.75
train loss:  0.5522637367248535
train gradient:  0.151955215973411
iteration : 12643
train acc:  0.71875
train loss:  0.4839261472225189
train gradient:  0.13160563750347426
iteration : 12644
train acc:  0.7734375
train loss:  0.4750133156776428
train gradient:  0.09839172043392355
iteration : 12645
train acc:  0.7265625
train loss:  0.5368971228599548
train gradient:  0.15808137748526982
iteration : 12646
train acc:  0.7421875
train loss:  0.490569531917572
train gradient:  0.11314123564592485
iteration : 12647
train acc:  0.7265625
train loss:  0.47167614102363586
train gradient:  0.0957365648481369
iteration : 12648
train acc:  0.796875
train loss:  0.4756125807762146
train gradient:  0.11430067609467486
iteration : 12649
train acc:  0.75
train loss:  0.4277331829071045
train gradient:  0.11495163099082499
iteration : 12650
train acc:  0.75
train loss:  0.5379153490066528
train gradient:  0.12853132981237064
iteration : 12651
train acc:  0.7265625
train loss:  0.5278758406639099
train gradient:  0.12452533570138893
iteration : 12652
train acc:  0.71875
train loss:  0.49453306198120117
train gradient:  0.11664348406210434
iteration : 12653
train acc:  0.71875
train loss:  0.517428994178772
train gradient:  0.12918041857407328
iteration : 12654
train acc:  0.7890625
train loss:  0.48198285698890686
train gradient:  0.12556043147169652
iteration : 12655
train acc:  0.7890625
train loss:  0.44634658098220825
train gradient:  0.09473516662286816
iteration : 12656
train acc:  0.71875
train loss:  0.5114707946777344
train gradient:  0.11354098782486549
iteration : 12657
train acc:  0.7890625
train loss:  0.41674384474754333
train gradient:  0.10651424273721523
iteration : 12658
train acc:  0.703125
train loss:  0.5449316501617432
train gradient:  0.14777946071613512
iteration : 12659
train acc:  0.7734375
train loss:  0.45510920882225037
train gradient:  0.10084686809364725
iteration : 12660
train acc:  0.7421875
train loss:  0.49444299936294556
train gradient:  0.13104375369692928
iteration : 12661
train acc:  0.6953125
train loss:  0.5367486476898193
train gradient:  0.26862637762895397
iteration : 12662
train acc:  0.75
train loss:  0.4739840030670166
train gradient:  0.1226284136568074
iteration : 12663
train acc:  0.796875
train loss:  0.4622945785522461
train gradient:  0.14354705638829
iteration : 12664
train acc:  0.7578125
train loss:  0.48041224479675293
train gradient:  0.11332278367462498
iteration : 12665
train acc:  0.734375
train loss:  0.525627613067627
train gradient:  0.12336156568875982
iteration : 12666
train acc:  0.7890625
train loss:  0.4026249647140503
train gradient:  0.07515762741472058
iteration : 12667
train acc:  0.765625
train loss:  0.4486791789531708
train gradient:  0.09405247738774401
iteration : 12668
train acc:  0.7890625
train loss:  0.47650599479675293
train gradient:  0.10662867221982905
iteration : 12669
train acc:  0.7421875
train loss:  0.5467376708984375
train gradient:  0.15088683458216973
iteration : 12670
train acc:  0.734375
train loss:  0.484995573759079
train gradient:  0.1201374648287026
iteration : 12671
train acc:  0.7578125
train loss:  0.4513161778450012
train gradient:  0.09533693453438466
iteration : 12672
train acc:  0.71875
train loss:  0.5015314817428589
train gradient:  0.10084594617533628
iteration : 12673
train acc:  0.7421875
train loss:  0.4804072380065918
train gradient:  0.10241530281754062
iteration : 12674
train acc:  0.6640625
train loss:  0.5720874667167664
train gradient:  0.14147875607127572
iteration : 12675
train acc:  0.75
train loss:  0.5020151138305664
train gradient:  0.12222449552041052
iteration : 12676
train acc:  0.6796875
train loss:  0.5245096683502197
train gradient:  0.17282899234776794
iteration : 12677
train acc:  0.7734375
train loss:  0.4916052222251892
train gradient:  0.11964968019760851
iteration : 12678
train acc:  0.75
train loss:  0.46543562412261963
train gradient:  0.0937082280726879
iteration : 12679
train acc:  0.7265625
train loss:  0.5221800804138184
train gradient:  0.17456685366714175
iteration : 12680
train acc:  0.7890625
train loss:  0.483840674161911
train gradient:  0.18435516890782377
iteration : 12681
train acc:  0.71875
train loss:  0.4877575933933258
train gradient:  0.11440274385056833
iteration : 12682
train acc:  0.765625
train loss:  0.4364309012889862
train gradient:  0.08820872613316391
iteration : 12683
train acc:  0.796875
train loss:  0.4510641098022461
train gradient:  0.0959982384608673
iteration : 12684
train acc:  0.6796875
train loss:  0.5323385000228882
train gradient:  0.1402611665965948
iteration : 12685
train acc:  0.7890625
train loss:  0.44268763065338135
train gradient:  0.11689942284371947
iteration : 12686
train acc:  0.7421875
train loss:  0.5159878730773926
train gradient:  0.12198131597608625
iteration : 12687
train acc:  0.71875
train loss:  0.48596417903900146
train gradient:  0.1395928997430226
iteration : 12688
train acc:  0.8046875
train loss:  0.4221648871898651
train gradient:  0.14660232138386464
iteration : 12689
train acc:  0.734375
train loss:  0.49416524171829224
train gradient:  0.11978056214689817
iteration : 12690
train acc:  0.7421875
train loss:  0.49279630184173584
train gradient:  0.14578273346908893
iteration : 12691
train acc:  0.671875
train loss:  0.5649542808532715
train gradient:  0.1449656322922413
iteration : 12692
train acc:  0.71875
train loss:  0.5185621976852417
train gradient:  0.1632914440308768
iteration : 12693
train acc:  0.734375
train loss:  0.49247151613235474
train gradient:  0.13584219402198328
iteration : 12694
train acc:  0.7265625
train loss:  0.5156354904174805
train gradient:  0.13050063238712417
iteration : 12695
train acc:  0.6953125
train loss:  0.5690680742263794
train gradient:  0.17974848141063465
iteration : 12696
train acc:  0.7890625
train loss:  0.46222805976867676
train gradient:  0.10974157535074987
iteration : 12697
train acc:  0.765625
train loss:  0.5483092665672302
train gradient:  0.13982321312441032
iteration : 12698
train acc:  0.6875
train loss:  0.5078551173210144
train gradient:  0.14369311952450026
iteration : 12699
train acc:  0.6796875
train loss:  0.4951070249080658
train gradient:  0.1388656146280547
iteration : 12700
train acc:  0.7890625
train loss:  0.456326425075531
train gradient:  0.10446350020204064
iteration : 12701
train acc:  0.75
train loss:  0.4616553783416748
train gradient:  0.13270118071553988
iteration : 12702
train acc:  0.7421875
train loss:  0.5115979909896851
train gradient:  0.1337572796472553
iteration : 12703
train acc:  0.7734375
train loss:  0.4726904630661011
train gradient:  0.10178858066648598
iteration : 12704
train acc:  0.7421875
train loss:  0.4681175947189331
train gradient:  0.11333997113773799
iteration : 12705
train acc:  0.7734375
train loss:  0.5240176916122437
train gradient:  0.13018178360373928
iteration : 12706
train acc:  0.7734375
train loss:  0.4642874598503113
train gradient:  0.1074795627555864
iteration : 12707
train acc:  0.71875
train loss:  0.5413722991943359
train gradient:  0.12865047385698908
iteration : 12708
train acc:  0.7578125
train loss:  0.45901715755462646
train gradient:  0.10780710483703004
iteration : 12709
train acc:  0.765625
train loss:  0.46378952264785767
train gradient:  0.11495521656379996
iteration : 12710
train acc:  0.75
train loss:  0.4755827784538269
train gradient:  0.1262213163265487
iteration : 12711
train acc:  0.734375
train loss:  0.5086349248886108
train gradient:  0.11633930305349119
iteration : 12712
train acc:  0.71875
train loss:  0.5165529847145081
train gradient:  0.12353741113558453
iteration : 12713
train acc:  0.765625
train loss:  0.4885528087615967
train gradient:  0.1467074002913342
iteration : 12714
train acc:  0.7421875
train loss:  0.46826082468032837
train gradient:  0.10527470007478117
iteration : 12715
train acc:  0.75
train loss:  0.516128659248352
train gradient:  0.18575121956747817
iteration : 12716
train acc:  0.734375
train loss:  0.4663962423801422
train gradient:  0.09207544325300934
iteration : 12717
train acc:  0.75
train loss:  0.4816436767578125
train gradient:  0.13895380820173409
iteration : 12718
train acc:  0.6484375
train loss:  0.5880495309829712
train gradient:  0.18431496047387128
iteration : 12719
train acc:  0.7890625
train loss:  0.4432391822338104
train gradient:  0.1122634942659371
iteration : 12720
train acc:  0.75
train loss:  0.4871789813041687
train gradient:  0.12581616995950462
iteration : 12721
train acc:  0.6640625
train loss:  0.5546651482582092
train gradient:  0.13287920537149805
iteration : 12722
train acc:  0.7890625
train loss:  0.4267660975456238
train gradient:  0.10726834161002588
iteration : 12723
train acc:  0.7265625
train loss:  0.49531030654907227
train gradient:  0.1501925386518133
iteration : 12724
train acc:  0.6953125
train loss:  0.5965672135353088
train gradient:  0.17711378957038199
iteration : 12725
train acc:  0.7734375
train loss:  0.4414154291152954
train gradient:  0.09615387632369181
iteration : 12726
train acc:  0.765625
train loss:  0.5074825286865234
train gradient:  0.14378229373314538
iteration : 12727
train acc:  0.7734375
train loss:  0.43307116627693176
train gradient:  0.1293223223559879
iteration : 12728
train acc:  0.7578125
train loss:  0.4503597915172577
train gradient:  0.11336440028015152
iteration : 12729
train acc:  0.71875
train loss:  0.529067873954773
train gradient:  0.11581998336240883
iteration : 12730
train acc:  0.75
train loss:  0.4718254804611206
train gradient:  0.1377440814026623
iteration : 12731
train acc:  0.765625
train loss:  0.4506675601005554
train gradient:  0.09829368055106709
iteration : 12732
train acc:  0.8203125
train loss:  0.3828779458999634
train gradient:  0.07660340397886982
iteration : 12733
train acc:  0.734375
train loss:  0.5105372667312622
train gradient:  0.11673086353781978
iteration : 12734
train acc:  0.7734375
train loss:  0.5020326375961304
train gradient:  0.14673932130002035
iteration : 12735
train acc:  0.765625
train loss:  0.4569900929927826
train gradient:  0.10201477041142891
iteration : 12736
train acc:  0.703125
train loss:  0.5069774389266968
train gradient:  0.13377406399958625
iteration : 12737
train acc:  0.7421875
train loss:  0.5240364074707031
train gradient:  0.12067934180109399
iteration : 12738
train acc:  0.7890625
train loss:  0.42875146865844727
train gradient:  0.10381079269334614
iteration : 12739
train acc:  0.6875
train loss:  0.5345101356506348
train gradient:  0.1503724749804931
iteration : 12740
train acc:  0.796875
train loss:  0.43472516536712646
train gradient:  0.11178005842842847
iteration : 12741
train acc:  0.6875
train loss:  0.5367259979248047
train gradient:  0.195402535970754
iteration : 12742
train acc:  0.765625
train loss:  0.4389585256576538
train gradient:  0.09719671219380828
iteration : 12743
train acc:  0.7734375
train loss:  0.4107453525066376
train gradient:  0.0791703094380224
iteration : 12744
train acc:  0.7265625
train loss:  0.5004116296768188
train gradient:  0.13170069923534905
iteration : 12745
train acc:  0.8359375
train loss:  0.42824164032936096
train gradient:  0.098195527050617
iteration : 12746
train acc:  0.640625
train loss:  0.6154474020004272
train gradient:  0.17649055169360606
iteration : 12747
train acc:  0.7734375
train loss:  0.4669998288154602
train gradient:  0.11463482370640514
iteration : 12748
train acc:  0.7734375
train loss:  0.4738934338092804
train gradient:  0.11318607334247002
iteration : 12749
train acc:  0.78125
train loss:  0.4374086260795593
train gradient:  0.11561046294672317
iteration : 12750
train acc:  0.71875
train loss:  0.5140135884284973
train gradient:  0.13482806279901127
iteration : 12751
train acc:  0.71875
train loss:  0.4795524775981903
train gradient:  0.10742624280237661
iteration : 12752
train acc:  0.7578125
train loss:  0.4875430464744568
train gradient:  0.10637148797204467
iteration : 12753
train acc:  0.7890625
train loss:  0.4641672670841217
train gradient:  0.11229876195746542
iteration : 12754
train acc:  0.7109375
train loss:  0.5484300255775452
train gradient:  0.13537071841554937
iteration : 12755
train acc:  0.78125
train loss:  0.49464666843414307
train gradient:  0.1032024722758889
iteration : 12756
train acc:  0.7265625
train loss:  0.5236110091209412
train gradient:  0.12786776754227358
iteration : 12757
train acc:  0.7890625
train loss:  0.4455050230026245
train gradient:  0.10489303973437236
iteration : 12758
train acc:  0.78125
train loss:  0.5058382153511047
train gradient:  0.1355295140591165
iteration : 12759
train acc:  0.75
train loss:  0.49441906809806824
train gradient:  0.13790014225859157
iteration : 12760
train acc:  0.8046875
train loss:  0.44896095991134644
train gradient:  0.10964948056382438
iteration : 12761
train acc:  0.71875
train loss:  0.513154149055481
train gradient:  0.12848413254875501
iteration : 12762
train acc:  0.7890625
train loss:  0.41596710681915283
train gradient:  0.09836741367676026
iteration : 12763
train acc:  0.6796875
train loss:  0.5664019584655762
train gradient:  0.15137598394222657
iteration : 12764
train acc:  0.7109375
train loss:  0.5846391916275024
train gradient:  0.15658823921726578
iteration : 12765
train acc:  0.734375
train loss:  0.47637349367141724
train gradient:  0.11283578288712201
iteration : 12766
train acc:  0.7890625
train loss:  0.5089628100395203
train gradient:  0.16219200326586414
iteration : 12767
train acc:  0.75
train loss:  0.5602920055389404
train gradient:  0.20087565481536146
iteration : 12768
train acc:  0.734375
train loss:  0.5577225089073181
train gradient:  0.18602225643646483
iteration : 12769
train acc:  0.75
train loss:  0.4860000014305115
train gradient:  0.11069847966530351
iteration : 12770
train acc:  0.71875
train loss:  0.5078140497207642
train gradient:  0.13712769090868332
iteration : 12771
train acc:  0.7578125
train loss:  0.48071444034576416
train gradient:  0.10171015783410386
iteration : 12772
train acc:  0.7265625
train loss:  0.472564697265625
train gradient:  0.1325402667310304
iteration : 12773
train acc:  0.734375
train loss:  0.5005444288253784
train gradient:  0.14191567751579826
iteration : 12774
train acc:  0.75
train loss:  0.45364874601364136
train gradient:  0.10776304927988653
iteration : 12775
train acc:  0.7109375
train loss:  0.5295131206512451
train gradient:  0.1679548047544558
iteration : 12776
train acc:  0.765625
train loss:  0.4753303527832031
train gradient:  0.11842181718659513
iteration : 12777
train acc:  0.7265625
train loss:  0.531806468963623
train gradient:  0.14088347651165337
iteration : 12778
train acc:  0.71875
train loss:  0.5428191423416138
train gradient:  0.13403975306598992
iteration : 12779
train acc:  0.75
train loss:  0.4850311577320099
train gradient:  0.10943149333790876
iteration : 12780
train acc:  0.734375
train loss:  0.4643087387084961
train gradient:  0.08267604523640218
iteration : 12781
train acc:  0.765625
train loss:  0.4504792094230652
train gradient:  0.11014083251875477
iteration : 12782
train acc:  0.75
train loss:  0.5127791166305542
train gradient:  0.14024180288242055
iteration : 12783
train acc:  0.6953125
train loss:  0.6078735589981079
train gradient:  0.132929739115846
iteration : 12784
train acc:  0.7109375
train loss:  0.5967216491699219
train gradient:  0.18898117819885754
iteration : 12785
train acc:  0.78125
train loss:  0.4588727355003357
train gradient:  0.11334787158778173
iteration : 12786
train acc:  0.7265625
train loss:  0.4669259786605835
train gradient:  0.10174934086057172
iteration : 12787
train acc:  0.734375
train loss:  0.4722098708152771
train gradient:  0.12770289016259662
iteration : 12788
train acc:  0.7421875
train loss:  0.529495358467102
train gradient:  0.16366481112881948
iteration : 12789
train acc:  0.75
train loss:  0.452471524477005
train gradient:  0.14446151983404504
iteration : 12790
train acc:  0.6953125
train loss:  0.503454327583313
train gradient:  0.15453565100384004
iteration : 12791
train acc:  0.7109375
train loss:  0.5509575009346008
train gradient:  0.12385899791461882
iteration : 12792
train acc:  0.703125
train loss:  0.5259832739830017
train gradient:  0.13043758488155896
iteration : 12793
train acc:  0.7265625
train loss:  0.5431312918663025
train gradient:  0.13553109477629216
iteration : 12794
train acc:  0.7265625
train loss:  0.5062251091003418
train gradient:  0.11646323817325771
iteration : 12795
train acc:  0.703125
train loss:  0.5742213726043701
train gradient:  0.13833303519913107
iteration : 12796
train acc:  0.7421875
train loss:  0.45907026529312134
train gradient:  0.10693792261034525
iteration : 12797
train acc:  0.78125
train loss:  0.4652330279350281
train gradient:  0.11228124189702171
iteration : 12798
train acc:  0.71875
train loss:  0.5311646461486816
train gradient:  0.13689544816370938
iteration : 12799
train acc:  0.7734375
train loss:  0.4575727581977844
train gradient:  0.09361573449685315
iteration : 12800
train acc:  0.734375
train loss:  0.5582446455955505
train gradient:  0.14134420649226637
iteration : 12801
train acc:  0.765625
train loss:  0.528813362121582
train gradient:  0.1347931481087917
iteration : 12802
train acc:  0.75
train loss:  0.46007367968559265
train gradient:  0.0979309830035639
iteration : 12803
train acc:  0.765625
train loss:  0.43023067712783813
train gradient:  0.08272416063325978
iteration : 12804
train acc:  0.78125
train loss:  0.4939892590045929
train gradient:  0.11722281914333033
iteration : 12805
train acc:  0.78125
train loss:  0.5155960321426392
train gradient:  0.14162062679045245
iteration : 12806
train acc:  0.7265625
train loss:  0.45004135370254517
train gradient:  0.10830466738143611
iteration : 12807
train acc:  0.8203125
train loss:  0.43421483039855957
train gradient:  0.11344004817329885
iteration : 12808
train acc:  0.765625
train loss:  0.46660664677619934
train gradient:  0.1077240332898451
iteration : 12809
train acc:  0.78125
train loss:  0.451028436422348
train gradient:  0.12603631603448484
iteration : 12810
train acc:  0.7890625
train loss:  0.47811514139175415
train gradient:  0.12755468823660274
iteration : 12811
train acc:  0.828125
train loss:  0.5088196992874146
train gradient:  0.14665070932075272
iteration : 12812
train acc:  0.75
train loss:  0.4860756993293762
train gradient:  0.11106839509020022
iteration : 12813
train acc:  0.7578125
train loss:  0.4873661398887634
train gradient:  0.13491889935727572
iteration : 12814
train acc:  0.8203125
train loss:  0.3852255344390869
train gradient:  0.07101875638741041
iteration : 12815
train acc:  0.7734375
train loss:  0.49946749210357666
train gradient:  0.12673485174159518
iteration : 12816
train acc:  0.734375
train loss:  0.48647332191467285
train gradient:  0.12966753622097427
iteration : 12817
train acc:  0.734375
train loss:  0.5084527730941772
train gradient:  0.13343107598012202
iteration : 12818
train acc:  0.7421875
train loss:  0.5229290723800659
train gradient:  0.12883956613002706
iteration : 12819
train acc:  0.7734375
train loss:  0.48344582319259644
train gradient:  0.14073373414278856
iteration : 12820
train acc:  0.671875
train loss:  0.5580270290374756
train gradient:  0.1550543563120921
iteration : 12821
train acc:  0.6640625
train loss:  0.5713766813278198
train gradient:  0.20958491477347127
iteration : 12822
train acc:  0.6953125
train loss:  0.5158361196517944
train gradient:  0.13193906991842952
iteration : 12823
train acc:  0.78125
train loss:  0.4353790879249573
train gradient:  0.10920545266524775
iteration : 12824
train acc:  0.7421875
train loss:  0.5224273204803467
train gradient:  0.13795917275414193
iteration : 12825
train acc:  0.6953125
train loss:  0.6259194612503052
train gradient:  0.19519866122290447
iteration : 12826
train acc:  0.7734375
train loss:  0.4755873680114746
train gradient:  0.13138641821670116
iteration : 12827
train acc:  0.7578125
train loss:  0.43567728996276855
train gradient:  0.11098250278447581
iteration : 12828
train acc:  0.703125
train loss:  0.5216497182846069
train gradient:  0.13035496002133845
iteration : 12829
train acc:  0.765625
train loss:  0.4458175301551819
train gradient:  0.1059996166391347
iteration : 12830
train acc:  0.7109375
train loss:  0.5449227094650269
train gradient:  0.15946801695280277
iteration : 12831
train acc:  0.7265625
train loss:  0.4911072254180908
train gradient:  0.11877270851088542
iteration : 12832
train acc:  0.75
train loss:  0.4811306595802307
train gradient:  0.12713000730915744
iteration : 12833
train acc:  0.7578125
train loss:  0.44481343030929565
train gradient:  0.09773162917121371
iteration : 12834
train acc:  0.7578125
train loss:  0.47487056255340576
train gradient:  0.11977558042410658
iteration : 12835
train acc:  0.8203125
train loss:  0.43453162908554077
train gradient:  0.11626269901478208
iteration : 12836
train acc:  0.7734375
train loss:  0.5042703747749329
train gradient:  0.12104331334151668
iteration : 12837
train acc:  0.8046875
train loss:  0.43829914927482605
train gradient:  0.11786883461186232
iteration : 12838
train acc:  0.7890625
train loss:  0.4760622978210449
train gradient:  0.143075197705484
iteration : 12839
train acc:  0.6953125
train loss:  0.5767935514450073
train gradient:  0.13986024871891284
iteration : 12840
train acc:  0.7421875
train loss:  0.48091787099838257
train gradient:  0.14243543330224864
iteration : 12841
train acc:  0.7578125
train loss:  0.4843059480190277
train gradient:  0.14005923950343857
iteration : 12842
train acc:  0.734375
train loss:  0.5118613243103027
train gradient:  0.14538750078212853
iteration : 12843
train acc:  0.6640625
train loss:  0.5593540668487549
train gradient:  0.20154329234928442
iteration : 12844
train acc:  0.765625
train loss:  0.47032493352890015
train gradient:  0.10275756856259985
iteration : 12845
train acc:  0.78125
train loss:  0.4497978389263153
train gradient:  0.13249952529200748
iteration : 12846
train acc:  0.6953125
train loss:  0.5360057353973389
train gradient:  0.18297265706794297
iteration : 12847
train acc:  0.703125
train loss:  0.5220807790756226
train gradient:  0.11416565260682153
iteration : 12848
train acc:  0.828125
train loss:  0.41740673780441284
train gradient:  0.09886751281251704
iteration : 12849
train acc:  0.6953125
train loss:  0.5216380953788757
train gradient:  0.1734148034773455
iteration : 12850
train acc:  0.71875
train loss:  0.49766039848327637
train gradient:  0.11913036200825204
iteration : 12851
train acc:  0.75
train loss:  0.46260300278663635
train gradient:  0.13399783333846244
iteration : 12852
train acc:  0.765625
train loss:  0.4532754421234131
train gradient:  0.11029037259854513
iteration : 12853
train acc:  0.7265625
train loss:  0.4798066020011902
train gradient:  0.13563978725152415
iteration : 12854
train acc:  0.796875
train loss:  0.4748504161834717
train gradient:  0.09971839463762189
iteration : 12855
train acc:  0.75
train loss:  0.4744516611099243
train gradient:  0.1206619609732683
iteration : 12856
train acc:  0.7890625
train loss:  0.48479771614074707
train gradient:  0.1331710200795579
iteration : 12857
train acc:  0.7421875
train loss:  0.4899265766143799
train gradient:  0.1176206247770757
iteration : 12858
train acc:  0.7578125
train loss:  0.48829275369644165
train gradient:  0.11956196773287263
iteration : 12859
train acc:  0.78125
train loss:  0.4339410662651062
train gradient:  0.10228392829840496
iteration : 12860
train acc:  0.7109375
train loss:  0.5209370851516724
train gradient:  0.16214506854623661
iteration : 12861
train acc:  0.7890625
train loss:  0.45799094438552856
train gradient:  0.1163968023967215
iteration : 12862
train acc:  0.671875
train loss:  0.5787209272384644
train gradient:  0.16903813355839425
iteration : 12863
train acc:  0.6953125
train loss:  0.4933638274669647
train gradient:  0.12416960710873834
iteration : 12864
train acc:  0.7734375
train loss:  0.5126302242279053
train gradient:  0.17002254754070145
iteration : 12865
train acc:  0.8046875
train loss:  0.4411061406135559
train gradient:  0.09783859904044882
iteration : 12866
train acc:  0.765625
train loss:  0.48983490467071533
train gradient:  0.11404103481441938
iteration : 12867
train acc:  0.7734375
train loss:  0.44581514596939087
train gradient:  0.1268766063728235
iteration : 12868
train acc:  0.734375
train loss:  0.5183217525482178
train gradient:  0.13302598201049842
iteration : 12869
train acc:  0.765625
train loss:  0.5475922226905823
train gradient:  0.14311352638154162
iteration : 12870
train acc:  0.765625
train loss:  0.5117902159690857
train gradient:  0.17464090340852928
iteration : 12871
train acc:  0.7265625
train loss:  0.5031642317771912
train gradient:  0.10819135822689838
iteration : 12872
train acc:  0.671875
train loss:  0.5260448455810547
train gradient:  0.14554191353999574
iteration : 12873
train acc:  0.6875
train loss:  0.5215787291526794
train gradient:  0.1365854642973302
iteration : 12874
train acc:  0.7578125
train loss:  0.4414728879928589
train gradient:  0.11696163975111332
iteration : 12875
train acc:  0.78125
train loss:  0.502155065536499
train gradient:  0.13504152787348406
iteration : 12876
train acc:  0.71875
train loss:  0.4932350516319275
train gradient:  0.1252951952872959
iteration : 12877
train acc:  0.7421875
train loss:  0.5092731714248657
train gradient:  0.11145514849361433
iteration : 12878
train acc:  0.8125
train loss:  0.47919711470603943
train gradient:  0.09370753128434495
iteration : 12879
train acc:  0.796875
train loss:  0.4571250379085541
train gradient:  0.12837861714741677
iteration : 12880
train acc:  0.7109375
train loss:  0.5208133459091187
train gradient:  0.1460552717321318
iteration : 12881
train acc:  0.75
train loss:  0.5053420066833496
train gradient:  0.1709845943440032
iteration : 12882
train acc:  0.734375
train loss:  0.5294790267944336
train gradient:  0.13661569587440406
iteration : 12883
train acc:  0.703125
train loss:  0.510624885559082
train gradient:  0.13083194980535157
iteration : 12884
train acc:  0.734375
train loss:  0.4839109480381012
train gradient:  0.1451927437990011
iteration : 12885
train acc:  0.7578125
train loss:  0.49723899364471436
train gradient:  0.13354577325953276
iteration : 12886
train acc:  0.7578125
train loss:  0.45957615971565247
train gradient:  0.09590628240293703
iteration : 12887
train acc:  0.71875
train loss:  0.515217661857605
train gradient:  0.16998061456024616
iteration : 12888
train acc:  0.734375
train loss:  0.47567495703697205
train gradient:  0.12040121676051811
iteration : 12889
train acc:  0.7734375
train loss:  0.4760937988758087
train gradient:  0.13076853399384025
iteration : 12890
train acc:  0.7734375
train loss:  0.45495012402534485
train gradient:  0.09041387561477825
iteration : 12891
train acc:  0.703125
train loss:  0.5100404024124146
train gradient:  0.13634015776914313
iteration : 12892
train acc:  0.75
train loss:  0.5047681331634521
train gradient:  0.12405479779727471
iteration : 12893
train acc:  0.734375
train loss:  0.4483449161052704
train gradient:  0.10604035791092187
iteration : 12894
train acc:  0.7421875
train loss:  0.4717804789543152
train gradient:  0.12428076937339522
iteration : 12895
train acc:  0.84375
train loss:  0.39989355206489563
train gradient:  0.09181854577555229
iteration : 12896
train acc:  0.71875
train loss:  0.5187458395957947
train gradient:  0.16297740487353835
iteration : 12897
train acc:  0.734375
train loss:  0.549834668636322
train gradient:  0.14180847722444329
iteration : 12898
train acc:  0.828125
train loss:  0.42205893993377686
train gradient:  0.09007572973846864
iteration : 12899
train acc:  0.703125
train loss:  0.4985334575176239
train gradient:  0.14000220325280116
iteration : 12900
train acc:  0.734375
train loss:  0.5102912187576294
train gradient:  0.14077390279762153
iteration : 12901
train acc:  0.734375
train loss:  0.47345685958862305
train gradient:  0.11492418491935318
iteration : 12902
train acc:  0.734375
train loss:  0.5144662857055664
train gradient:  0.17035887441684466
iteration : 12903
train acc:  0.71875
train loss:  0.4901258945465088
train gradient:  0.1327116836778465
iteration : 12904
train acc:  0.7265625
train loss:  0.5191243886947632
train gradient:  0.15682622352521317
iteration : 12905
train acc:  0.671875
train loss:  0.5637128353118896
train gradient:  0.16784545380031396
iteration : 12906
train acc:  0.7265625
train loss:  0.4810941219329834
train gradient:  0.09606839201989187
iteration : 12907
train acc:  0.71875
train loss:  0.5199952721595764
train gradient:  0.16417541775492897
iteration : 12908
train acc:  0.6953125
train loss:  0.4644660949707031
train gradient:  0.1108362102220333
iteration : 12909
train acc:  0.765625
train loss:  0.4530678987503052
train gradient:  0.11005695831443792
iteration : 12910
train acc:  0.8046875
train loss:  0.47903984785079956
train gradient:  0.13368167956673532
iteration : 12911
train acc:  0.7578125
train loss:  0.4617612659931183
train gradient:  0.0953338241976753
iteration : 12912
train acc:  0.75
train loss:  0.5072615146636963
train gradient:  0.11052194060417968
iteration : 12913
train acc:  0.734375
train loss:  0.4752085506916046
train gradient:  0.10248742584896332
iteration : 12914
train acc:  0.7421875
train loss:  0.45836204290390015
train gradient:  0.10747977363627603
iteration : 12915
train acc:  0.7578125
train loss:  0.45915353298187256
train gradient:  0.10525548389851368
iteration : 12916
train acc:  0.71875
train loss:  0.5177180767059326
train gradient:  0.13642727718303194
iteration : 12917
train acc:  0.7578125
train loss:  0.5065776705741882
train gradient:  0.15755410882485266
iteration : 12918
train acc:  0.8359375
train loss:  0.42116862535476685
train gradient:  0.08745969757827007
iteration : 12919
train acc:  0.7265625
train loss:  0.4910929799079895
train gradient:  0.12076732885715313
iteration : 12920
train acc:  0.7109375
train loss:  0.5084452033042908
train gradient:  0.13809848055954654
iteration : 12921
train acc:  0.734375
train loss:  0.45806214213371277
train gradient:  0.10950744986135066
iteration : 12922
train acc:  0.75
train loss:  0.5019724369049072
train gradient:  0.1170855563976095
iteration : 12923
train acc:  0.7109375
train loss:  0.49685895442962646
train gradient:  0.11887357134995725
iteration : 12924
train acc:  0.71875
train loss:  0.4897724986076355
train gradient:  0.12831359463711806
iteration : 12925
train acc:  0.703125
train loss:  0.49880173802375793
train gradient:  0.13989386252356678
iteration : 12926
train acc:  0.7421875
train loss:  0.5260898470878601
train gradient:  0.1430325125376467
iteration : 12927
train acc:  0.75
train loss:  0.5079988241195679
train gradient:  0.12377337056992928
iteration : 12928
train acc:  0.765625
train loss:  0.4910380244255066
train gradient:  0.10420760569070074
iteration : 12929
train acc:  0.7734375
train loss:  0.43095314502716064
train gradient:  0.09251868838006544
iteration : 12930
train acc:  0.703125
train loss:  0.5074737668037415
train gradient:  0.1442297943916569
iteration : 12931
train acc:  0.765625
train loss:  0.4537201523780823
train gradient:  0.09851596560586981
iteration : 12932
train acc:  0.7265625
train loss:  0.5535978674888611
train gradient:  0.17090233505607155
iteration : 12933
train acc:  0.7109375
train loss:  0.522321343421936
train gradient:  0.15261681376957448
iteration : 12934
train acc:  0.78125
train loss:  0.45233961939811707
train gradient:  0.09712113528264685
iteration : 12935
train acc:  0.7421875
train loss:  0.5311392545700073
train gradient:  0.14538912073129473
iteration : 12936
train acc:  0.7421875
train loss:  0.5001258850097656
train gradient:  0.12538258814787662
iteration : 12937
train acc:  0.765625
train loss:  0.47664526104927063
train gradient:  0.11246569106881397
iteration : 12938
train acc:  0.71875
train loss:  0.513634204864502
train gradient:  0.13439206060474906
iteration : 12939
train acc:  0.8046875
train loss:  0.39626848697662354
train gradient:  0.09515945903007782
iteration : 12940
train acc:  0.6875
train loss:  0.5403774380683899
train gradient:  0.18598555225378877
iteration : 12941
train acc:  0.6875
train loss:  0.5244407653808594
train gradient:  0.13397992999762925
iteration : 12942
train acc:  0.7421875
train loss:  0.5148352980613708
train gradient:  0.2023114143553726
iteration : 12943
train acc:  0.734375
train loss:  0.4933353364467621
train gradient:  0.10753291601181866
iteration : 12944
train acc:  0.6640625
train loss:  0.5599510073661804
train gradient:  0.18031604381827843
iteration : 12945
train acc:  0.8203125
train loss:  0.4186219274997711
train gradient:  0.09628671220746435
iteration : 12946
train acc:  0.75
train loss:  0.47689151763916016
train gradient:  0.11213861047056695
iteration : 12947
train acc:  0.7578125
train loss:  0.4741362929344177
train gradient:  0.08487540751225056
iteration : 12948
train acc:  0.7421875
train loss:  0.47171682119369507
train gradient:  0.14206688166322445
iteration : 12949
train acc:  0.7265625
train loss:  0.4831780195236206
train gradient:  0.11973433632430776
iteration : 12950
train acc:  0.8359375
train loss:  0.4302317500114441
train gradient:  0.09363358839871753
iteration : 12951
train acc:  0.734375
train loss:  0.5065711140632629
train gradient:  0.13269889248410183
iteration : 12952
train acc:  0.78125
train loss:  0.41204050183296204
train gradient:  0.07243781945231603
iteration : 12953
train acc:  0.796875
train loss:  0.46372175216674805
train gradient:  0.1050827166994188
iteration : 12954
train acc:  0.8203125
train loss:  0.40121203660964966
train gradient:  0.09457488316359525
iteration : 12955
train acc:  0.765625
train loss:  0.4621346592903137
train gradient:  0.1291839522009653
iteration : 12956
train acc:  0.7734375
train loss:  0.4667324423789978
train gradient:  0.12868546760089677
iteration : 12957
train acc:  0.6875
train loss:  0.5069671869277954
train gradient:  0.16247581588350812
iteration : 12958
train acc:  0.796875
train loss:  0.44837796688079834
train gradient:  0.11661934432480797
iteration : 12959
train acc:  0.8046875
train loss:  0.4493297338485718
train gradient:  0.12552082922713537
iteration : 12960
train acc:  0.65625
train loss:  0.5447061061859131
train gradient:  0.16772432273051985
iteration : 12961
train acc:  0.734375
train loss:  0.4864075779914856
train gradient:  0.1318221869474253
iteration : 12962
train acc:  0.78125
train loss:  0.46277549862861633
train gradient:  0.09906738850327307
iteration : 12963
train acc:  0.734375
train loss:  0.46789082884788513
train gradient:  0.13393982339358204
iteration : 12964
train acc:  0.734375
train loss:  0.48656541109085083
train gradient:  0.13729165873854807
iteration : 12965
train acc:  0.7421875
train loss:  0.5133787989616394
train gradient:  0.14508752246607082
iteration : 12966
train acc:  0.7421875
train loss:  0.5594810247421265
train gradient:  0.16382406412427575
iteration : 12967
train acc:  0.7734375
train loss:  0.4727011024951935
train gradient:  0.1455244085902071
iteration : 12968
train acc:  0.7890625
train loss:  0.47858911752700806
train gradient:  0.144995582041711
iteration : 12969
train acc:  0.71875
train loss:  0.5365041494369507
train gradient:  0.1581308178308082
iteration : 12970
train acc:  0.734375
train loss:  0.5413950085639954
train gradient:  0.1945983583595835
iteration : 12971
train acc:  0.703125
train loss:  0.5051721334457397
train gradient:  0.1370957155683211
iteration : 12972
train acc:  0.6796875
train loss:  0.5149641036987305
train gradient:  0.1206862442764921
iteration : 12973
train acc:  0.640625
train loss:  0.6021878123283386
train gradient:  0.18133156582299947
iteration : 12974
train acc:  0.703125
train loss:  0.4915754795074463
train gradient:  0.11661659666036564
iteration : 12975
train acc:  0.765625
train loss:  0.47640079259872437
train gradient:  0.12522204797139683
iteration : 12976
train acc:  0.78125
train loss:  0.4365798532962799
train gradient:  0.0892655185422929
iteration : 12977
train acc:  0.796875
train loss:  0.47134506702423096
train gradient:  0.13511187409950715
iteration : 12978
train acc:  0.71875
train loss:  0.5441411137580872
train gradient:  0.14714763330009709
iteration : 12979
train acc:  0.7109375
train loss:  0.585584282875061
train gradient:  0.18224753038531882
iteration : 12980
train acc:  0.7578125
train loss:  0.48893237113952637
train gradient:  0.1385104212421198
iteration : 12981
train acc:  0.796875
train loss:  0.4207221567630768
train gradient:  0.09886954329137336
iteration : 12982
train acc:  0.7109375
train loss:  0.5111894607543945
train gradient:  0.15423414890159148
iteration : 12983
train acc:  0.734375
train loss:  0.5145989656448364
train gradient:  0.13226161430277616
iteration : 12984
train acc:  0.6953125
train loss:  0.4944988489151001
train gradient:  0.13019999488999612
iteration : 12985
train acc:  0.7421875
train loss:  0.48874223232269287
train gradient:  0.18690419730406227
iteration : 12986
train acc:  0.7265625
train loss:  0.4841859042644501
train gradient:  0.10973945354223316
iteration : 12987
train acc:  0.75
train loss:  0.44123077392578125
train gradient:  0.12130271238608345
iteration : 12988
train acc:  0.71875
train loss:  0.4773402810096741
train gradient:  0.10507785219327628
iteration : 12989
train acc:  0.7265625
train loss:  0.46837252378463745
train gradient:  0.10698438858627718
iteration : 12990
train acc:  0.7578125
train loss:  0.47739601135253906
train gradient:  0.10068402560726823
iteration : 12991
train acc:  0.7578125
train loss:  0.46193987131118774
train gradient:  0.10831116899424718
iteration : 12992
train acc:  0.8125
train loss:  0.47322970628738403
train gradient:  0.14662857542662355
iteration : 12993
train acc:  0.71875
train loss:  0.48214536905288696
train gradient:  0.10402328572710469
iteration : 12994
train acc:  0.7578125
train loss:  0.41057807207107544
train gradient:  0.10931737795629262
iteration : 12995
train acc:  0.71875
train loss:  0.5326355695724487
train gradient:  0.14782065827098617
iteration : 12996
train acc:  0.7109375
train loss:  0.5116715431213379
train gradient:  0.11799694886728626
iteration : 12997
train acc:  0.765625
train loss:  0.498599648475647
train gradient:  0.16822740970966477
iteration : 12998
train acc:  0.7109375
train loss:  0.5050426721572876
train gradient:  0.14767060232854795
iteration : 12999
train acc:  0.703125
train loss:  0.5114995837211609
train gradient:  0.14407196183110016
iteration : 13000
train acc:  0.6953125
train loss:  0.5631141662597656
train gradient:  0.15136536358304722
iteration : 13001
train acc:  0.734375
train loss:  0.5123397707939148
train gradient:  0.15260435445768933
iteration : 13002
train acc:  0.7890625
train loss:  0.4293195903301239
train gradient:  0.12500672841986712
iteration : 13003
train acc:  0.7734375
train loss:  0.5200196504592896
train gradient:  0.17617685042882375
iteration : 13004
train acc:  0.7578125
train loss:  0.5059646964073181
train gradient:  0.13299161236991944
iteration : 13005
train acc:  0.7734375
train loss:  0.4582810401916504
train gradient:  0.10802643705808969
iteration : 13006
train acc:  0.7421875
train loss:  0.5257214903831482
train gradient:  0.14426426025357164
iteration : 13007
train acc:  0.765625
train loss:  0.4780231714248657
train gradient:  0.10186641856297887
iteration : 13008
train acc:  0.7734375
train loss:  0.4827895164489746
train gradient:  0.1363897363513261
iteration : 13009
train acc:  0.7890625
train loss:  0.43263208866119385
train gradient:  0.09386160307670018
iteration : 13010
train acc:  0.6875
train loss:  0.5580325126647949
train gradient:  0.12329678219356656
iteration : 13011
train acc:  0.71875
train loss:  0.4557501971721649
train gradient:  0.09162377042216666
iteration : 13012
train acc:  0.671875
train loss:  0.6087799072265625
train gradient:  0.2519993914471323
iteration : 13013
train acc:  0.7578125
train loss:  0.47030824422836304
train gradient:  0.10358658740743269
iteration : 13014
train acc:  0.7421875
train loss:  0.5167179107666016
train gradient:  0.14870910693933648
iteration : 13015
train acc:  0.7265625
train loss:  0.4775260090827942
train gradient:  0.09661568808934304
iteration : 13016
train acc:  0.7265625
train loss:  0.47809159755706787
train gradient:  0.11928475018526982
iteration : 13017
train acc:  0.6796875
train loss:  0.5922598838806152
train gradient:  0.16010665919907308
iteration : 13018
train acc:  0.7578125
train loss:  0.45550572872161865
train gradient:  0.10901629060488228
iteration : 13019
train acc:  0.765625
train loss:  0.49832412600517273
train gradient:  0.12126302924198032
iteration : 13020
train acc:  0.6328125
train loss:  0.5606738328933716
train gradient:  0.18603024114273797
iteration : 13021
train acc:  0.7734375
train loss:  0.41806381940841675
train gradient:  0.09298159327910531
iteration : 13022
train acc:  0.7578125
train loss:  0.47100183367729187
train gradient:  0.1011648015659273
iteration : 13023
train acc:  0.6953125
train loss:  0.523542046546936
train gradient:  0.1272344833564683
iteration : 13024
train acc:  0.765625
train loss:  0.46220892667770386
train gradient:  0.1113185242020963
iteration : 13025
train acc:  0.75
train loss:  0.529464602470398
train gradient:  0.14683042427140247
iteration : 13026
train acc:  0.6953125
train loss:  0.4954710006713867
train gradient:  0.11203669469050878
iteration : 13027
train acc:  0.734375
train loss:  0.5154138207435608
train gradient:  0.11846409426353567
iteration : 13028
train acc:  0.765625
train loss:  0.45832502841949463
train gradient:  0.15972610788396624
iteration : 13029
train acc:  0.7421875
train loss:  0.5311230421066284
train gradient:  0.14338643625676917
iteration : 13030
train acc:  0.75
train loss:  0.4684511125087738
train gradient:  0.14268002880337377
iteration : 13031
train acc:  0.75
train loss:  0.46343809366226196
train gradient:  0.1070456174694028
iteration : 13032
train acc:  0.7890625
train loss:  0.4273909032344818
train gradient:  0.0994645802865409
iteration : 13033
train acc:  0.6953125
train loss:  0.5125819444656372
train gradient:  0.16073996026531912
iteration : 13034
train acc:  0.7734375
train loss:  0.4655440151691437
train gradient:  0.10514201604442154
iteration : 13035
train acc:  0.796875
train loss:  0.48106396198272705
train gradient:  0.136661191185779
iteration : 13036
train acc:  0.75
train loss:  0.4668152332305908
train gradient:  0.0990852486470703
iteration : 13037
train acc:  0.7734375
train loss:  0.46067002415657043
train gradient:  0.09334911030595763
iteration : 13038
train acc:  0.765625
train loss:  0.4808815121650696
train gradient:  0.09297905859553421
iteration : 13039
train acc:  0.7734375
train loss:  0.4838399887084961
train gradient:  0.11607335068696696
iteration : 13040
train acc:  0.7109375
train loss:  0.5196524262428284
train gradient:  0.13066871078456171
iteration : 13041
train acc:  0.7109375
train loss:  0.5604013204574585
train gradient:  0.13745359829356563
iteration : 13042
train acc:  0.8125
train loss:  0.47518616914749146
train gradient:  0.14352055986913353
iteration : 13043
train acc:  0.7890625
train loss:  0.4465710520744324
train gradient:  0.08573190305139482
iteration : 13044
train acc:  0.7421875
train loss:  0.5356374979019165
train gradient:  0.15287348952481195
iteration : 13045
train acc:  0.7265625
train loss:  0.4952026307582855
train gradient:  0.1360529369711897
iteration : 13046
train acc:  0.75
train loss:  0.47857028245925903
train gradient:  0.11159552197882629
iteration : 13047
train acc:  0.765625
train loss:  0.45281076431274414
train gradient:  0.12741623443305378
iteration : 13048
train acc:  0.7265625
train loss:  0.5336636304855347
train gradient:  0.17240116633486408
iteration : 13049
train acc:  0.75
train loss:  0.45986253023147583
train gradient:  0.12417952219417336
iteration : 13050
train acc:  0.75
train loss:  0.4770095646381378
train gradient:  0.10683817964417637
iteration : 13051
train acc:  0.734375
train loss:  0.49936580657958984
train gradient:  0.14153174389912287
iteration : 13052
train acc:  0.703125
train loss:  0.5696007013320923
train gradient:  0.14127616153392947
iteration : 13053
train acc:  0.7421875
train loss:  0.47425758838653564
train gradient:  0.11441949989304313
iteration : 13054
train acc:  0.7109375
train loss:  0.513039231300354
train gradient:  0.11830985837263062
iteration : 13055
train acc:  0.734375
train loss:  0.48938822746276855
train gradient:  0.1417573562745501
iteration : 13056
train acc:  0.703125
train loss:  0.5072317123413086
train gradient:  0.12978745386283064
iteration : 13057
train acc:  0.75
train loss:  0.4642249345779419
train gradient:  0.09749528585055556
iteration : 13058
train acc:  0.7578125
train loss:  0.4593876600265503
train gradient:  0.08584803575430065
iteration : 13059
train acc:  0.7421875
train loss:  0.48716163635253906
train gradient:  0.11855322449515115
iteration : 13060
train acc:  0.7109375
train loss:  0.5407638549804688
train gradient:  0.14798023080823725
iteration : 13061
train acc:  0.7109375
train loss:  0.5646750330924988
train gradient:  0.15706479877440283
iteration : 13062
train acc:  0.7265625
train loss:  0.5095301866531372
train gradient:  0.14533869895776974
iteration : 13063
train acc:  0.8203125
train loss:  0.416614830493927
train gradient:  0.09217834589039929
iteration : 13064
train acc:  0.796875
train loss:  0.44479209184646606
train gradient:  0.10892229985193906
iteration : 13065
train acc:  0.7578125
train loss:  0.4816281199455261
train gradient:  0.1344082372052569
iteration : 13066
train acc:  0.78125
train loss:  0.4539506435394287
train gradient:  0.09760986419618246
iteration : 13067
train acc:  0.8046875
train loss:  0.4311676621437073
train gradient:  0.09050180612938356
iteration : 13068
train acc:  0.765625
train loss:  0.48029857873916626
train gradient:  0.11420101138223725
iteration : 13069
train acc:  0.7265625
train loss:  0.5217281579971313
train gradient:  0.14327586602929965
iteration : 13070
train acc:  0.8046875
train loss:  0.45137637853622437
train gradient:  0.11079383558813721
iteration : 13071
train acc:  0.796875
train loss:  0.44663822650909424
train gradient:  0.13401155505018722
iteration : 13072
train acc:  0.7421875
train loss:  0.4737662374973297
train gradient:  0.10812541774959654
iteration : 13073
train acc:  0.7421875
train loss:  0.5281102061271667
train gradient:  0.14502457426143586
iteration : 13074
train acc:  0.734375
train loss:  0.5111382007598877
train gradient:  0.12728054750258289
iteration : 13075
train acc:  0.84375
train loss:  0.3605343699455261
train gradient:  0.06696310607167276
iteration : 13076
train acc:  0.7578125
train loss:  0.4860147535800934
train gradient:  0.13421957097978265
iteration : 13077
train acc:  0.734375
train loss:  0.4815835654735565
train gradient:  0.16051254842830553
iteration : 13078
train acc:  0.7421875
train loss:  0.5135220289230347
train gradient:  0.128163954159834
iteration : 13079
train acc:  0.7109375
train loss:  0.5526610612869263
train gradient:  0.1601599528685005
iteration : 13080
train acc:  0.796875
train loss:  0.4452167749404907
train gradient:  0.0900640010894551
iteration : 13081
train acc:  0.734375
train loss:  0.5354870557785034
train gradient:  0.15929299200714447
iteration : 13082
train acc:  0.8046875
train loss:  0.4772685766220093
train gradient:  0.11531236678169321
iteration : 13083
train acc:  0.671875
train loss:  0.521390438079834
train gradient:  0.15921115392401855
iteration : 13084
train acc:  0.7265625
train loss:  0.5026267766952515
train gradient:  0.1334426427430216
iteration : 13085
train acc:  0.734375
train loss:  0.4876123070716858
train gradient:  0.11048987980446831
iteration : 13086
train acc:  0.796875
train loss:  0.44195640087127686
train gradient:  0.09208603891567088
iteration : 13087
train acc:  0.7265625
train loss:  0.5318463444709778
train gradient:  0.14793218925899107
iteration : 13088
train acc:  0.65625
train loss:  0.577229380607605
train gradient:  0.14214920876081294
iteration : 13089
train acc:  0.75
train loss:  0.4464483857154846
train gradient:  0.10218636221856635
iteration : 13090
train acc:  0.7734375
train loss:  0.48013556003570557
train gradient:  0.12541439963129702
iteration : 13091
train acc:  0.828125
train loss:  0.40329045057296753
train gradient:  0.08686831956755547
iteration : 13092
train acc:  0.7421875
train loss:  0.48879897594451904
train gradient:  0.1250893080430679
iteration : 13093
train acc:  0.7734375
train loss:  0.4887215793132782
train gradient:  0.13634643767407367
iteration : 13094
train acc:  0.7734375
train loss:  0.4791622757911682
train gradient:  0.12819397193554205
iteration : 13095
train acc:  0.734375
train loss:  0.49512219429016113
train gradient:  0.1641823669611855
iteration : 13096
train acc:  0.7734375
train loss:  0.4592621922492981
train gradient:  0.10974002239605411
iteration : 13097
train acc:  0.7265625
train loss:  0.528846263885498
train gradient:  0.1733418241616161
iteration : 13098
train acc:  0.734375
train loss:  0.4951239228248596
train gradient:  0.1096057826720193
iteration : 13099
train acc:  0.7421875
train loss:  0.5279407501220703
train gradient:  0.15954818120174374
iteration : 13100
train acc:  0.7890625
train loss:  0.44844749569892883
train gradient:  0.10300135183387608
iteration : 13101
train acc:  0.765625
train loss:  0.4706866145133972
train gradient:  0.11356739865710364
iteration : 13102
train acc:  0.7109375
train loss:  0.5189592242240906
train gradient:  0.1399675020403221
iteration : 13103
train acc:  0.7421875
train loss:  0.49284249544143677
train gradient:  0.13721931274382082
iteration : 13104
train acc:  0.7890625
train loss:  0.4962765574455261
train gradient:  0.12689168069635015
iteration : 13105
train acc:  0.7578125
train loss:  0.4764542877674103
train gradient:  0.12257212895947243
iteration : 13106
train acc:  0.765625
train loss:  0.4825746417045593
train gradient:  0.13056162667673082
iteration : 13107
train acc:  0.796875
train loss:  0.46707883477211
train gradient:  0.11385982224814685
iteration : 13108
train acc:  0.75
train loss:  0.5024458169937134
train gradient:  0.11614565371350259
iteration : 13109
train acc:  0.7265625
train loss:  0.4864593744277954
train gradient:  0.09704123133491174
iteration : 13110
train acc:  0.796875
train loss:  0.42131197452545166
train gradient:  0.10927462834183937
iteration : 13111
train acc:  0.71875
train loss:  0.5403670072555542
train gradient:  0.13106329906229303
iteration : 13112
train acc:  0.796875
train loss:  0.45943954586982727
train gradient:  0.13239387924479584
iteration : 13113
train acc:  0.734375
train loss:  0.5046364068984985
train gradient:  0.13048032924884267
iteration : 13114
train acc:  0.765625
train loss:  0.4647214710712433
train gradient:  0.09747076624277023
iteration : 13115
train acc:  0.765625
train loss:  0.4470698833465576
train gradient:  0.09616310556785189
iteration : 13116
train acc:  0.78125
train loss:  0.45009779930114746
train gradient:  0.09783700639503107
iteration : 13117
train acc:  0.75
train loss:  0.46197760105133057
train gradient:  0.10948526719830826
iteration : 13118
train acc:  0.7578125
train loss:  0.4982753396034241
train gradient:  0.11586418454738108
iteration : 13119
train acc:  0.7734375
train loss:  0.4963911175727844
train gradient:  0.1286900546894093
iteration : 13120
train acc:  0.7734375
train loss:  0.46110475063323975
train gradient:  0.140873601224132
iteration : 13121
train acc:  0.734375
train loss:  0.5099829435348511
train gradient:  0.12346029491552789
iteration : 13122
train acc:  0.796875
train loss:  0.40860164165496826
train gradient:  0.11681686869517283
iteration : 13123
train acc:  0.765625
train loss:  0.5039288997650146
train gradient:  0.12165277958018109
iteration : 13124
train acc:  0.71875
train loss:  0.5006971955299377
train gradient:  0.12116604317165054
iteration : 13125
train acc:  0.75
train loss:  0.45772042870521545
train gradient:  0.12405933373255298
iteration : 13126
train acc:  0.734375
train loss:  0.5524144172668457
train gradient:  0.1271945956091815
iteration : 13127
train acc:  0.734375
train loss:  0.5114241242408752
train gradient:  0.1752110688163681
iteration : 13128
train acc:  0.75
train loss:  0.46922117471694946
train gradient:  0.13593649449932516
iteration : 13129
train acc:  0.7109375
train loss:  0.5169196724891663
train gradient:  0.14527570051318345
iteration : 13130
train acc:  0.796875
train loss:  0.40572935342788696
train gradient:  0.08740034607640303
iteration : 13131
train acc:  0.78125
train loss:  0.4455656409263611
train gradient:  0.12917016875569381
iteration : 13132
train acc:  0.7734375
train loss:  0.45261943340301514
train gradient:  0.11346995275517012
iteration : 13133
train acc:  0.671875
train loss:  0.5662077069282532
train gradient:  0.1641435391309432
iteration : 13134
train acc:  0.7421875
train loss:  0.503333568572998
train gradient:  0.10750470519091745
iteration : 13135
train acc:  0.703125
train loss:  0.5712196826934814
train gradient:  0.15157832225381745
iteration : 13136
train acc:  0.7109375
train loss:  0.525662899017334
train gradient:  0.15558031493018037
iteration : 13137
train acc:  0.75
train loss:  0.49587610363960266
train gradient:  0.0956669026207492
iteration : 13138
train acc:  0.75
train loss:  0.4704738259315491
train gradient:  0.11260842171179095
iteration : 13139
train acc:  0.75
train loss:  0.47526922821998596
train gradient:  0.10600558749902882
iteration : 13140
train acc:  0.7109375
train loss:  0.5095587968826294
train gradient:  0.1659599362951807
iteration : 13141
train acc:  0.7265625
train loss:  0.5340625047683716
train gradient:  0.14817191740135277
iteration : 13142
train acc:  0.734375
train loss:  0.500541090965271
train gradient:  0.12508906737938208
iteration : 13143
train acc:  0.7578125
train loss:  0.46181243658065796
train gradient:  0.12693247891559695
iteration : 13144
train acc:  0.734375
train loss:  0.5242152214050293
train gradient:  0.16570591941416993
iteration : 13145
train acc:  0.78125
train loss:  0.47413116693496704
train gradient:  0.11593657509300796
iteration : 13146
train acc:  0.7265625
train loss:  0.5286243557929993
train gradient:  0.172528804973196
iteration : 13147
train acc:  0.7109375
train loss:  0.494876503944397
train gradient:  0.13122830156343218
iteration : 13148
train acc:  0.7421875
train loss:  0.4764118790626526
train gradient:  0.12023430264466063
iteration : 13149
train acc:  0.6953125
train loss:  0.49332553148269653
train gradient:  0.13641373852585623
iteration : 13150
train acc:  0.7421875
train loss:  0.5119692087173462
train gradient:  0.1587312907291769
iteration : 13151
train acc:  0.71875
train loss:  0.5089057087898254
train gradient:  0.1295526392890308
iteration : 13152
train acc:  0.7890625
train loss:  0.4537375569343567
train gradient:  0.1076221409086034
iteration : 13153
train acc:  0.703125
train loss:  0.5127435922622681
train gradient:  0.12604665745168434
iteration : 13154
train acc:  0.734375
train loss:  0.43026864528656006
train gradient:  0.1086597880980142
iteration : 13155
train acc:  0.6875
train loss:  0.5843624472618103
train gradient:  0.1303724512553065
iteration : 13156
train acc:  0.75
train loss:  0.47248339653015137
train gradient:  0.11435870828869081
iteration : 13157
train acc:  0.703125
train loss:  0.499557226896286
train gradient:  0.11633705427931759
iteration : 13158
train acc:  0.7578125
train loss:  0.494508296251297
train gradient:  0.12053129256155325
iteration : 13159
train acc:  0.7109375
train loss:  0.5374578237533569
train gradient:  0.15288019844245954
iteration : 13160
train acc:  0.6875
train loss:  0.5451140403747559
train gradient:  0.14690929920947782
iteration : 13161
train acc:  0.7265625
train loss:  0.5294162034988403
train gradient:  0.14614442107058323
iteration : 13162
train acc:  0.8125
train loss:  0.4223405718803406
train gradient:  0.09255603303016796
iteration : 13163
train acc:  0.7734375
train loss:  0.4494205117225647
train gradient:  0.10723179839662694
iteration : 13164
train acc:  0.8046875
train loss:  0.4688805937767029
train gradient:  0.14012371898400167
iteration : 13165
train acc:  0.75
train loss:  0.5512606501579285
train gradient:  0.15427514716941843
iteration : 13166
train acc:  0.765625
train loss:  0.43776124715805054
train gradient:  0.10089239946042992
iteration : 13167
train acc:  0.6796875
train loss:  0.5387026071548462
train gradient:  0.16671465171849192
iteration : 13168
train acc:  0.78125
train loss:  0.43939101696014404
train gradient:  0.13775878558855467
iteration : 13169
train acc:  0.7734375
train loss:  0.5095142722129822
train gradient:  0.13460723021096985
iteration : 13170
train acc:  0.765625
train loss:  0.45852091908454895
train gradient:  0.11968537672537093
iteration : 13171
train acc:  0.734375
train loss:  0.4824320375919342
train gradient:  0.12587360134210196
iteration : 13172
train acc:  0.703125
train loss:  0.5148083567619324
train gradient:  0.13468061771168371
iteration : 13173
train acc:  0.7734375
train loss:  0.4708150029182434
train gradient:  0.12615072574253372
iteration : 13174
train acc:  0.71875
train loss:  0.540596604347229
train gradient:  0.11693150602838669
iteration : 13175
train acc:  0.71875
train loss:  0.4751436114311218
train gradient:  0.12979177353816776
iteration : 13176
train acc:  0.7421875
train loss:  0.510343611240387
train gradient:  0.11851194470903623
iteration : 13177
train acc:  0.75
train loss:  0.4819594621658325
train gradient:  0.11871515678441703
iteration : 13178
train acc:  0.671875
train loss:  0.5340563058853149
train gradient:  0.16602110733401193
iteration : 13179
train acc:  0.7421875
train loss:  0.5017293095588684
train gradient:  0.12915363235044697
iteration : 13180
train acc:  0.78125
train loss:  0.44307976961135864
train gradient:  0.11975280371038592
iteration : 13181
train acc:  0.734375
train loss:  0.4741082787513733
train gradient:  0.0867421788458037
iteration : 13182
train acc:  0.8203125
train loss:  0.40599507093429565
train gradient:  0.09904702106381529
iteration : 13183
train acc:  0.78125
train loss:  0.46795815229415894
train gradient:  0.09972295048499738
iteration : 13184
train acc:  0.7734375
train loss:  0.5069911479949951
train gradient:  0.16026733415634475
iteration : 13185
train acc:  0.796875
train loss:  0.4275381863117218
train gradient:  0.10661228618329888
iteration : 13186
train acc:  0.7734375
train loss:  0.476204514503479
train gradient:  0.10035145483862563
iteration : 13187
train acc:  0.765625
train loss:  0.5433496236801147
train gradient:  0.13868227390215082
iteration : 13188
train acc:  0.796875
train loss:  0.41094541549682617
train gradient:  0.08924445198393342
iteration : 13189
train acc:  0.765625
train loss:  0.4760622978210449
train gradient:  0.12092784069204862
iteration : 13190
train acc:  0.7578125
train loss:  0.43905630707740784
train gradient:  0.11122199971520677
iteration : 13191
train acc:  0.7421875
train loss:  0.4771275818347931
train gradient:  0.10566584939980095
iteration : 13192
train acc:  0.7265625
train loss:  0.5078984498977661
train gradient:  0.16473968713119155
iteration : 13193
train acc:  0.796875
train loss:  0.4584352374076843
train gradient:  0.12364782985747423
iteration : 13194
train acc:  0.8046875
train loss:  0.4255053997039795
train gradient:  0.08669061951346878
iteration : 13195
train acc:  0.7734375
train loss:  0.4555289149284363
train gradient:  0.10478876232307033
iteration : 13196
train acc:  0.796875
train loss:  0.4398720860481262
train gradient:  0.10804482074759658
iteration : 13197
train acc:  0.765625
train loss:  0.4666670262813568
train gradient:  0.10616071561840316
iteration : 13198
train acc:  0.7109375
train loss:  0.515127420425415
train gradient:  0.19036079991465255
iteration : 13199
train acc:  0.7265625
train loss:  0.5153253078460693
train gradient:  0.13401295021325021
iteration : 13200
train acc:  0.765625
train loss:  0.4635537266731262
train gradient:  0.11219098004970282
iteration : 13201
train acc:  0.7578125
train loss:  0.5016475319862366
train gradient:  0.1698122105821701
iteration : 13202
train acc:  0.71875
train loss:  0.5125658512115479
train gradient:  0.15858222421733556
iteration : 13203
train acc:  0.75
train loss:  0.465725839138031
train gradient:  0.11635383252336544
iteration : 13204
train acc:  0.765625
train loss:  0.4665084481239319
train gradient:  0.11520825647412261
iteration : 13205
train acc:  0.78125
train loss:  0.5481306910514832
train gradient:  0.16659413751547236
iteration : 13206
train acc:  0.796875
train loss:  0.48680809140205383
train gradient:  0.11577219004174105
iteration : 13207
train acc:  0.703125
train loss:  0.5402591228485107
train gradient:  0.12958715584105057
iteration : 13208
train acc:  0.6640625
train loss:  0.506283164024353
train gradient:  0.11393556258334783
iteration : 13209
train acc:  0.78125
train loss:  0.5029969811439514
train gradient:  0.18980619705295054
iteration : 13210
train acc:  0.75
train loss:  0.4358983635902405
train gradient:  0.11285719159224947
iteration : 13211
train acc:  0.7890625
train loss:  0.4477487802505493
train gradient:  0.10004355513406663
iteration : 13212
train acc:  0.7109375
train loss:  0.5626803040504456
train gradient:  0.17339580206084224
iteration : 13213
train acc:  0.71875
train loss:  0.5517432689666748
train gradient:  0.18744435338519422
iteration : 13214
train acc:  0.65625
train loss:  0.541778564453125
train gradient:  0.1524726437692337
iteration : 13215
train acc:  0.828125
train loss:  0.40499401092529297
train gradient:  0.09334211528682165
iteration : 13216
train acc:  0.7734375
train loss:  0.47471725940704346
train gradient:  0.11097772852590322
iteration : 13217
train acc:  0.75
train loss:  0.43100404739379883
train gradient:  0.090259758312932
iteration : 13218
train acc:  0.8046875
train loss:  0.3795015811920166
train gradient:  0.09592087751052857
iteration : 13219
train acc:  0.8046875
train loss:  0.4306948781013489
train gradient:  0.10017383467383618
iteration : 13220
train acc:  0.71875
train loss:  0.5834693908691406
train gradient:  0.15764779567769643
iteration : 13221
train acc:  0.796875
train loss:  0.45566970109939575
train gradient:  0.10638726896547028
iteration : 13222
train acc:  0.7265625
train loss:  0.5125700235366821
train gradient:  0.1289605486851723
iteration : 13223
train acc:  0.703125
train loss:  0.5014050602912903
train gradient:  0.1231366535443762
iteration : 13224
train acc:  0.671875
train loss:  0.5339572429656982
train gradient:  0.15663670694261783
iteration : 13225
train acc:  0.796875
train loss:  0.42118698358535767
train gradient:  0.11976258555107035
iteration : 13226
train acc:  0.640625
train loss:  0.5401210188865662
train gradient:  0.1628888044089848
iteration : 13227
train acc:  0.671875
train loss:  0.5954461693763733
train gradient:  0.1679607412645626
iteration : 13228
train acc:  0.7421875
train loss:  0.46709850430488586
train gradient:  0.11254126939747564
iteration : 13229
train acc:  0.7734375
train loss:  0.4495090842247009
train gradient:  0.10723899896772189
iteration : 13230
train acc:  0.7734375
train loss:  0.4784500002861023
train gradient:  0.1156954429474745
iteration : 13231
train acc:  0.75
train loss:  0.46838343143463135
train gradient:  0.10596031163560325
iteration : 13232
train acc:  0.765625
train loss:  0.4614057242870331
train gradient:  0.11513701245005718
iteration : 13233
train acc:  0.734375
train loss:  0.5188306570053101
train gradient:  0.17999716582351355
iteration : 13234
train acc:  0.8359375
train loss:  0.41685622930526733
train gradient:  0.07647966314897882
iteration : 13235
train acc:  0.7734375
train loss:  0.5157386064529419
train gradient:  0.18362300626598238
iteration : 13236
train acc:  0.734375
train loss:  0.5196404457092285
train gradient:  0.1671791673909206
iteration : 13237
train acc:  0.765625
train loss:  0.4572227895259857
train gradient:  0.10068353198425539
iteration : 13238
train acc:  0.6796875
train loss:  0.5589315891265869
train gradient:  0.20312687415000658
iteration : 13239
train acc:  0.703125
train loss:  0.5516766905784607
train gradient:  0.1575872179158882
iteration : 13240
train acc:  0.6953125
train loss:  0.531175971031189
train gradient:  0.13210304717605933
iteration : 13241
train acc:  0.734375
train loss:  0.49211978912353516
train gradient:  0.1164947079991009
iteration : 13242
train acc:  0.7578125
train loss:  0.46120965480804443
train gradient:  0.1201305956040475
iteration : 13243
train acc:  0.8125
train loss:  0.42880988121032715
train gradient:  0.11468259924700912
iteration : 13244
train acc:  0.7890625
train loss:  0.44673168659210205
train gradient:  0.0999998391719879
iteration : 13245
train acc:  0.6875
train loss:  0.515006422996521
train gradient:  0.1312755355919947
iteration : 13246
train acc:  0.71875
train loss:  0.49113449454307556
train gradient:  0.11743908515570625
iteration : 13247
train acc:  0.6953125
train loss:  0.47618404030799866
train gradient:  0.13062930217251179
iteration : 13248
train acc:  0.7421875
train loss:  0.4745869040489197
train gradient:  0.12139302441404261
iteration : 13249
train acc:  0.796875
train loss:  0.4603499174118042
train gradient:  0.13094472813347616
iteration : 13250
train acc:  0.7421875
train loss:  0.49308204650878906
train gradient:  0.11779015226605097
iteration : 13251
train acc:  0.7578125
train loss:  0.5235072374343872
train gradient:  0.14232224515824526
iteration : 13252
train acc:  0.7421875
train loss:  0.5005440711975098
train gradient:  0.13416275769953917
iteration : 13253
train acc:  0.796875
train loss:  0.44002264738082886
train gradient:  0.1450935721171212
iteration : 13254
train acc:  0.65625
train loss:  0.5606197118759155
train gradient:  0.1706304125831381
iteration : 13255
train acc:  0.6328125
train loss:  0.5683555603027344
train gradient:  0.14780139673933612
iteration : 13256
train acc:  0.65625
train loss:  0.548468828201294
train gradient:  0.1757435931062502
iteration : 13257
train acc:  0.734375
train loss:  0.49600863456726074
train gradient:  0.11870515546615475
iteration : 13258
train acc:  0.734375
train loss:  0.5158243179321289
train gradient:  0.16333189214683896
iteration : 13259
train acc:  0.703125
train loss:  0.49449315667152405
train gradient:  0.10738328723058845
iteration : 13260
train acc:  0.765625
train loss:  0.49140244722366333
train gradient:  0.127963759609714
iteration : 13261
train acc:  0.875
train loss:  0.3609378933906555
train gradient:  0.08340102987388615
iteration : 13262
train acc:  0.71875
train loss:  0.5508336424827576
train gradient:  0.16007034201694476
iteration : 13263
train acc:  0.8125
train loss:  0.4281138777732849
train gradient:  0.09034829590214052
iteration : 13264
train acc:  0.78125
train loss:  0.47656458616256714
train gradient:  0.14541050031677966
iteration : 13265
train acc:  0.6640625
train loss:  0.5754671692848206
train gradient:  0.1625751276975017
iteration : 13266
train acc:  0.75
train loss:  0.5136175155639648
train gradient:  0.1715650429040654
iteration : 13267
train acc:  0.7109375
train loss:  0.5718210935592651
train gradient:  0.15159075419694223
iteration : 13268
train acc:  0.8125
train loss:  0.4246041476726532
train gradient:  0.11427621678885796
iteration : 13269
train acc:  0.734375
train loss:  0.5037246942520142
train gradient:  0.15198865896712477
iteration : 13270
train acc:  0.7421875
train loss:  0.48194485902786255
train gradient:  0.11746164439802043
iteration : 13271
train acc:  0.734375
train loss:  0.5304713249206543
train gradient:  0.20580272332235755
iteration : 13272
train acc:  0.7890625
train loss:  0.5073091983795166
train gradient:  0.1182796664181563
iteration : 13273
train acc:  0.7109375
train loss:  0.5044951438903809
train gradient:  0.12664356496852708
iteration : 13274
train acc:  0.7578125
train loss:  0.460732638835907
train gradient:  0.09676856252796909
iteration : 13275
train acc:  0.7578125
train loss:  0.4465569257736206
train gradient:  0.10526516398983896
iteration : 13276
train acc:  0.703125
train loss:  0.5336945652961731
train gradient:  0.1380648170139225
iteration : 13277
train acc:  0.796875
train loss:  0.417544424533844
train gradient:  0.08796920252049324
iteration : 13278
train acc:  0.7734375
train loss:  0.4581165015697479
train gradient:  0.13952320448689998
iteration : 13279
train acc:  0.78125
train loss:  0.43086743354797363
train gradient:  0.10040024677627286
iteration : 13280
train acc:  0.7265625
train loss:  0.462083637714386
train gradient:  0.10361253063147066
iteration : 13281
train acc:  0.8046875
train loss:  0.4265699088573456
train gradient:  0.08688333072283351
iteration : 13282
train acc:  0.765625
train loss:  0.47802913188934326
train gradient:  0.1042007365058758
iteration : 13283
train acc:  0.734375
train loss:  0.5011582970619202
train gradient:  0.15263346402590428
iteration : 13284
train acc:  0.75
train loss:  0.5197759866714478
train gradient:  0.2057138835266638
iteration : 13285
train acc:  0.734375
train loss:  0.523973286151886
train gradient:  0.1206828208780544
iteration : 13286
train acc:  0.71875
train loss:  0.5510146021842957
train gradient:  0.15800445750901898
iteration : 13287
train acc:  0.7578125
train loss:  0.47993630170822144
train gradient:  0.10496978730350232
iteration : 13288
train acc:  0.7734375
train loss:  0.46946802735328674
train gradient:  0.12383492031540147
iteration : 13289
train acc:  0.6953125
train loss:  0.5102399587631226
train gradient:  0.14263063547789245
iteration : 13290
train acc:  0.7890625
train loss:  0.46856483817100525
train gradient:  0.10859623570828529
iteration : 13291
train acc:  0.7109375
train loss:  0.5122678875923157
train gradient:  0.15462156515657777
iteration : 13292
train acc:  0.8046875
train loss:  0.4176787734031677
train gradient:  0.14528641942249443
iteration : 13293
train acc:  0.7734375
train loss:  0.4724598228931427
train gradient:  0.13431138177905555
iteration : 13294
train acc:  0.65625
train loss:  0.5342110395431519
train gradient:  0.11975140794186442
iteration : 13295
train acc:  0.7578125
train loss:  0.461769700050354
train gradient:  0.11143424775986778
iteration : 13296
train acc:  0.7265625
train loss:  0.4868258833885193
train gradient:  0.12220104815565444
iteration : 13297
train acc:  0.71875
train loss:  0.5339905023574829
train gradient:  0.14973274923327273
iteration : 13298
train acc:  0.671875
train loss:  0.5597525835037231
train gradient:  0.21832302329606865
iteration : 13299
train acc:  0.71875
train loss:  0.48259347677230835
train gradient:  0.10941363716825511
iteration : 13300
train acc:  0.671875
train loss:  0.5664585828781128
train gradient:  0.18044130137226283
iteration : 13301
train acc:  0.7734375
train loss:  0.46108055114746094
train gradient:  0.11911983389233023
iteration : 13302
train acc:  0.7265625
train loss:  0.4995630085468292
train gradient:  0.12114532741298305
iteration : 13303
train acc:  0.7734375
train loss:  0.4580524265766144
train gradient:  0.13102311446671241
iteration : 13304
train acc:  0.7109375
train loss:  0.49000710248947144
train gradient:  0.12642573098904586
iteration : 13305
train acc:  0.7734375
train loss:  0.5249592065811157
train gradient:  0.12948869324111328
iteration : 13306
train acc:  0.8125
train loss:  0.44644296169281006
train gradient:  0.09645970801376055
iteration : 13307
train acc:  0.7109375
train loss:  0.4580175280570984
train gradient:  0.11063660198432522
iteration : 13308
train acc:  0.71875
train loss:  0.4699026942253113
train gradient:  0.10177896394519707
iteration : 13309
train acc:  0.7734375
train loss:  0.5104659199714661
train gradient:  0.09648138223074924
iteration : 13310
train acc:  0.7890625
train loss:  0.4438714385032654
train gradient:  0.1226198668427119
iteration : 13311
train acc:  0.8046875
train loss:  0.46776294708251953
train gradient:  0.1255989082236489
iteration : 13312
train acc:  0.8203125
train loss:  0.41246268153190613
train gradient:  0.08857111285293402
iteration : 13313
train acc:  0.78125
train loss:  0.4857635498046875
train gradient:  0.11600202271227121
iteration : 13314
train acc:  0.703125
train loss:  0.5577321648597717
train gradient:  0.1554277744125761
iteration : 13315
train acc:  0.734375
train loss:  0.58525550365448
train gradient:  0.1949760535273764
iteration : 13316
train acc:  0.7734375
train loss:  0.42089319229125977
train gradient:  0.09836838918129365
iteration : 13317
train acc:  0.765625
train loss:  0.5021617412567139
train gradient:  0.12196233090488122
iteration : 13318
train acc:  0.8046875
train loss:  0.45246729254722595
train gradient:  0.1204541322878733
iteration : 13319
train acc:  0.703125
train loss:  0.4835539162158966
train gradient:  0.09510657263551406
iteration : 13320
train acc:  0.7265625
train loss:  0.4937305450439453
train gradient:  0.1522603768370615
iteration : 13321
train acc:  0.7578125
train loss:  0.49988099932670593
train gradient:  0.11677674124340161
iteration : 13322
train acc:  0.7109375
train loss:  0.5188080072402954
train gradient:  0.13852992606835657
iteration : 13323
train acc:  0.7890625
train loss:  0.4267467260360718
train gradient:  0.1170353565923812
iteration : 13324
train acc:  0.71875
train loss:  0.5240899920463562
train gradient:  0.19024698271542861
iteration : 13325
train acc:  0.78125
train loss:  0.4124199151992798
train gradient:  0.08481687446812292
iteration : 13326
train acc:  0.7578125
train loss:  0.4730243682861328
train gradient:  0.0984982953371305
iteration : 13327
train acc:  0.6953125
train loss:  0.5259882807731628
train gradient:  0.125570513741753
iteration : 13328
train acc:  0.78125
train loss:  0.466693639755249
train gradient:  0.12452107379427434
iteration : 13329
train acc:  0.8203125
train loss:  0.4178357720375061
train gradient:  0.12366423277990755
iteration : 13330
train acc:  0.71875
train loss:  0.5130553841590881
train gradient:  0.1581276890329612
iteration : 13331
train acc:  0.8203125
train loss:  0.38614097237586975
train gradient:  0.10594839330709471
iteration : 13332
train acc:  0.78125
train loss:  0.44154873490333557
train gradient:  0.11616400474913029
iteration : 13333
train acc:  0.6953125
train loss:  0.5435330867767334
train gradient:  0.17169142058520118
iteration : 13334
train acc:  0.7265625
train loss:  0.49561333656311035
train gradient:  0.14227501234304613
iteration : 13335
train acc:  0.71875
train loss:  0.5084915161132812
train gradient:  0.16369153820250643
iteration : 13336
train acc:  0.734375
train loss:  0.4626190662384033
train gradient:  0.11048077437214104
iteration : 13337
train acc:  0.75
train loss:  0.49426165223121643
train gradient:  0.12760013399480336
iteration : 13338
train acc:  0.75
train loss:  0.5163788795471191
train gradient:  0.1297052361058626
iteration : 13339
train acc:  0.734375
train loss:  0.4902225136756897
train gradient:  0.137788735198345
iteration : 13340
train acc:  0.7578125
train loss:  0.45083242654800415
train gradient:  0.10683183380823918
iteration : 13341
train acc:  0.7265625
train loss:  0.5207334160804749
train gradient:  0.1366830777723519
iteration : 13342
train acc:  0.7578125
train loss:  0.5433896780014038
train gradient:  0.11777654167813986
iteration : 13343
train acc:  0.7578125
train loss:  0.42439004778862
train gradient:  0.12864471335545524
iteration : 13344
train acc:  0.7421875
train loss:  0.51462721824646
train gradient:  0.13277520631744893
iteration : 13345
train acc:  0.7578125
train loss:  0.4975131154060364
train gradient:  0.14396683391948828
iteration : 13346
train acc:  0.734375
train loss:  0.5004559755325317
train gradient:  0.1435556545857799
iteration : 13347
train acc:  0.8046875
train loss:  0.47508668899536133
train gradient:  0.12781407915969034
iteration : 13348
train acc:  0.7421875
train loss:  0.542037844657898
train gradient:  0.17491787804547307
iteration : 13349
train acc:  0.7109375
train loss:  0.5155909657478333
train gradient:  0.14198324343845678
iteration : 13350
train acc:  0.7578125
train loss:  0.4742458462715149
train gradient:  0.12462893006863077
iteration : 13351
train acc:  0.78125
train loss:  0.4261392652988434
train gradient:  0.08368313819782086
iteration : 13352
train acc:  0.7578125
train loss:  0.46000611782073975
train gradient:  0.12327851131091445
iteration : 13353
train acc:  0.7265625
train loss:  0.48139914870262146
train gradient:  0.10647652032078005
iteration : 13354
train acc:  0.765625
train loss:  0.47992265224456787
train gradient:  0.13645092603600734
iteration : 13355
train acc:  0.7734375
train loss:  0.46976327896118164
train gradient:  0.12283105081612167
iteration : 13356
train acc:  0.7578125
train loss:  0.4874202013015747
train gradient:  0.13078134469871686
iteration : 13357
train acc:  0.7734375
train loss:  0.44037991762161255
train gradient:  0.08524598480421425
iteration : 13358
train acc:  0.6875
train loss:  0.5342539548873901
train gradient:  0.15587729390739613
iteration : 13359
train acc:  0.7265625
train loss:  0.5544430613517761
train gradient:  0.16352130270423326
iteration : 13360
train acc:  0.8203125
train loss:  0.43920981884002686
train gradient:  0.09978015088177353
iteration : 13361
train acc:  0.75
train loss:  0.45793697237968445
train gradient:  0.1279768283231979
iteration : 13362
train acc:  0.7734375
train loss:  0.44173479080200195
train gradient:  0.09182375616923727
iteration : 13363
train acc:  0.703125
train loss:  0.5144960880279541
train gradient:  0.12417745273871446
iteration : 13364
train acc:  0.7109375
train loss:  0.4975346326828003
train gradient:  0.11907621066929043
iteration : 13365
train acc:  0.7578125
train loss:  0.46290451288223267
train gradient:  0.10754249169728317
iteration : 13366
train acc:  0.7578125
train loss:  0.47911590337753296
train gradient:  0.12949839134006197
iteration : 13367
train acc:  0.734375
train loss:  0.4710767865180969
train gradient:  0.10602735402132209
iteration : 13368
train acc:  0.765625
train loss:  0.47859111428260803
train gradient:  0.11258641237764744
iteration : 13369
train acc:  0.7265625
train loss:  0.5063438415527344
train gradient:  0.12357946930348011
iteration : 13370
train acc:  0.8359375
train loss:  0.4103909730911255
train gradient:  0.08902100722840851
iteration : 13371
train acc:  0.78125
train loss:  0.44478166103363037
train gradient:  0.10451333806408551
iteration : 13372
train acc:  0.703125
train loss:  0.5765884518623352
train gradient:  0.15717410759654388
iteration : 13373
train acc:  0.7265625
train loss:  0.5012184977531433
train gradient:  0.11916055330125318
iteration : 13374
train acc:  0.7578125
train loss:  0.45997852087020874
train gradient:  0.10835928960725054
iteration : 13375
train acc:  0.7421875
train loss:  0.4696471095085144
train gradient:  0.10570311084906525
iteration : 13376
train acc:  0.7578125
train loss:  0.46933749318122864
train gradient:  0.1441451809278389
iteration : 13377
train acc:  0.796875
train loss:  0.4522625803947449
train gradient:  0.12354015769231376
iteration : 13378
train acc:  0.703125
train loss:  0.5187113285064697
train gradient:  0.1430712989550623
iteration : 13379
train acc:  0.6875
train loss:  0.5728170871734619
train gradient:  0.17904078784088145
iteration : 13380
train acc:  0.8046875
train loss:  0.47439998388290405
train gradient:  0.13112140093947364
iteration : 13381
train acc:  0.734375
train loss:  0.4801207184791565
train gradient:  0.1314786046780402
iteration : 13382
train acc:  0.78125
train loss:  0.5014770030975342
train gradient:  0.12717680577972792
iteration : 13383
train acc:  0.71875
train loss:  0.5533037185668945
train gradient:  0.14750078559294044
iteration : 13384
train acc:  0.6875
train loss:  0.5524671673774719
train gradient:  0.13882213070472482
iteration : 13385
train acc:  0.765625
train loss:  0.4378839135169983
train gradient:  0.11434521202760989
iteration : 13386
train acc:  0.75
train loss:  0.45851296186447144
train gradient:  0.09859040978953124
iteration : 13387
train acc:  0.796875
train loss:  0.43576425313949585
train gradient:  0.11406192180304255
iteration : 13388
train acc:  0.7265625
train loss:  0.5542551279067993
train gradient:  0.15552122571372579
iteration : 13389
train acc:  0.8046875
train loss:  0.4403861463069916
train gradient:  0.10892595370383833
iteration : 13390
train acc:  0.78125
train loss:  0.5033437013626099
train gradient:  0.14146301827627633
iteration : 13391
train acc:  0.7421875
train loss:  0.4795844554901123
train gradient:  0.1242049223619595
iteration : 13392
train acc:  0.765625
train loss:  0.5230057835578918
train gradient:  0.15691154279619363
iteration : 13393
train acc:  0.765625
train loss:  0.49358731508255005
train gradient:  0.13503647365107807
iteration : 13394
train acc:  0.71875
train loss:  0.5547448396682739
train gradient:  0.16358737919475877
iteration : 13395
train acc:  0.765625
train loss:  0.4405924379825592
train gradient:  0.08808986847360949
iteration : 13396
train acc:  0.7890625
train loss:  0.4383615553379059
train gradient:  0.10982745646094647
iteration : 13397
train acc:  0.7265625
train loss:  0.5583593249320984
train gradient:  0.15189689585265853
iteration : 13398
train acc:  0.7734375
train loss:  0.5163770914077759
train gradient:  0.14063228723917237
iteration : 13399
train acc:  0.765625
train loss:  0.46918007731437683
train gradient:  0.12005623365356892
iteration : 13400
train acc:  0.75
train loss:  0.4313811659812927
train gradient:  0.10403223898832084
iteration : 13401
train acc:  0.7265625
train loss:  0.4836859703063965
train gradient:  0.11196895166002359
iteration : 13402
train acc:  0.734375
train loss:  0.5092203617095947
train gradient:  0.14192676622814915
iteration : 13403
train acc:  0.7265625
train loss:  0.523807168006897
train gradient:  0.13379200363164423
iteration : 13404
train acc:  0.6875
train loss:  0.5690790414810181
train gradient:  0.14676107227782378
iteration : 13405
train acc:  0.7890625
train loss:  0.43521660566329956
train gradient:  0.11969145318652216
iteration : 13406
train acc:  0.703125
train loss:  0.5283299684524536
train gradient:  0.14869324747554774
iteration : 13407
train acc:  0.7578125
train loss:  0.48852935433387756
train gradient:  0.1501194494018721
iteration : 13408
train acc:  0.7421875
train loss:  0.49061810970306396
train gradient:  0.11766124214946533
iteration : 13409
train acc:  0.7734375
train loss:  0.45710569620132446
train gradient:  0.11781337222239582
iteration : 13410
train acc:  0.7578125
train loss:  0.5023648142814636
train gradient:  0.15992900775394764
iteration : 13411
train acc:  0.6875
train loss:  0.5022006034851074
train gradient:  0.10808801804436806
iteration : 13412
train acc:  0.8046875
train loss:  0.4424784779548645
train gradient:  0.09950157802344885
iteration : 13413
train acc:  0.7734375
train loss:  0.40148812532424927
train gradient:  0.08898647800480441
iteration : 13414
train acc:  0.7890625
train loss:  0.4298718273639679
train gradient:  0.13640291405339655
iteration : 13415
train acc:  0.7890625
train loss:  0.44267433881759644
train gradient:  0.10627647647410551
iteration : 13416
train acc:  0.78125
train loss:  0.4454187750816345
train gradient:  0.12188765658254946
iteration : 13417
train acc:  0.78125
train loss:  0.501046359539032
train gradient:  0.11931071936000541
iteration : 13418
train acc:  0.828125
train loss:  0.4485044479370117
train gradient:  0.12758246215419877
iteration : 13419
train acc:  0.8125
train loss:  0.4136658012866974
train gradient:  0.09689719730081696
iteration : 13420
train acc:  0.7578125
train loss:  0.4772009253501892
train gradient:  0.11801651685208261
iteration : 13421
train acc:  0.765625
train loss:  0.502229630947113
train gradient:  0.1305177835287259
iteration : 13422
train acc:  0.7265625
train loss:  0.46354663372039795
train gradient:  0.12598333574319423
iteration : 13423
train acc:  0.7578125
train loss:  0.5338878631591797
train gradient:  0.12466177348479275
iteration : 13424
train acc:  0.78125
train loss:  0.4418145418167114
train gradient:  0.11334221546212671
iteration : 13425
train acc:  0.7890625
train loss:  0.4487884044647217
train gradient:  0.11418408544156414
iteration : 13426
train acc:  0.7734375
train loss:  0.4937703609466553
train gradient:  0.15375396076005735
iteration : 13427
train acc:  0.7109375
train loss:  0.5077290534973145
train gradient:  0.17175800620924936
iteration : 13428
train acc:  0.7578125
train loss:  0.5021714568138123
train gradient:  0.16580151469203336
iteration : 13429
train acc:  0.796875
train loss:  0.3862077593803406
train gradient:  0.10459287408568184
iteration : 13430
train acc:  0.765625
train loss:  0.49759477376937866
train gradient:  0.18153682661236203
iteration : 13431
train acc:  0.734375
train loss:  0.49437734484672546
train gradient:  0.11751545742106578
iteration : 13432
train acc:  0.7421875
train loss:  0.5049318075180054
train gradient:  0.1299373470237974
iteration : 13433
train acc:  0.6796875
train loss:  0.5553339123725891
train gradient:  0.18836406347317214
iteration : 13434
train acc:  0.7578125
train loss:  0.4574660062789917
train gradient:  0.15044721858430965
iteration : 13435
train acc:  0.6875
train loss:  0.5188122987747192
train gradient:  0.11970391978117016
iteration : 13436
train acc:  0.734375
train loss:  0.5021212100982666
train gradient:  0.1283948851776424
iteration : 13437
train acc:  0.7734375
train loss:  0.4428965747356415
train gradient:  0.14040037854802911
iteration : 13438
train acc:  0.765625
train loss:  0.4692703187465668
train gradient:  0.12740873711592376
iteration : 13439
train acc:  0.7421875
train loss:  0.5158025026321411
train gradient:  0.1444209738384674
iteration : 13440
train acc:  0.7578125
train loss:  0.525174081325531
train gradient:  0.14181937934304106
iteration : 13441
train acc:  0.7109375
train loss:  0.4875944256782532
train gradient:  0.12237468100187832
iteration : 13442
train acc:  0.671875
train loss:  0.5026592016220093
train gradient:  0.1204551600543967
iteration : 13443
train acc:  0.7578125
train loss:  0.5115634202957153
train gradient:  0.14103335488498714
iteration : 13444
train acc:  0.7890625
train loss:  0.4695866107940674
train gradient:  0.13751094831571034
iteration : 13445
train acc:  0.78125
train loss:  0.4609578251838684
train gradient:  0.10216062674714313
iteration : 13446
train acc:  0.78125
train loss:  0.45804789662361145
train gradient:  0.12068072696349501
iteration : 13447
train acc:  0.7734375
train loss:  0.42307186126708984
train gradient:  0.09720699057709367
iteration : 13448
train acc:  0.75
train loss:  0.45947834849357605
train gradient:  0.0915278342945127
iteration : 13449
train acc:  0.7109375
train loss:  0.48329755663871765
train gradient:  0.17293362227100048
iteration : 13450
train acc:  0.8046875
train loss:  0.4489561915397644
train gradient:  0.1238363855105466
iteration : 13451
train acc:  0.7890625
train loss:  0.40055689215660095
train gradient:  0.12011901905059602
iteration : 13452
train acc:  0.7734375
train loss:  0.46933943033218384
train gradient:  0.11841418069645028
iteration : 13453
train acc:  0.7265625
train loss:  0.51336669921875
train gradient:  0.12506165408507647
iteration : 13454
train acc:  0.765625
train loss:  0.46688947081565857
train gradient:  0.14283230289541632
iteration : 13455
train acc:  0.7109375
train loss:  0.5182230472564697
train gradient:  0.1220863015714217
iteration : 13456
train acc:  0.7734375
train loss:  0.47590371966362
train gradient:  0.1445140083418321
iteration : 13457
train acc:  0.7734375
train loss:  0.4199904799461365
train gradient:  0.10898588260559465
iteration : 13458
train acc:  0.6953125
train loss:  0.5408945083618164
train gradient:  0.15028977785589903
iteration : 13459
train acc:  0.7578125
train loss:  0.45910537242889404
train gradient:  0.11385105507714224
iteration : 13460
train acc:  0.7109375
train loss:  0.5812139511108398
train gradient:  0.17497021308290833
iteration : 13461
train acc:  0.71875
train loss:  0.49345654249191284
train gradient:  0.13066601540816142
iteration : 13462
train acc:  0.6875
train loss:  0.5222693681716919
train gradient:  0.1774790976610088
iteration : 13463
train acc:  0.71875
train loss:  0.493725061416626
train gradient:  0.13184617439937135
iteration : 13464
train acc:  0.7421875
train loss:  0.5264971256256104
train gradient:  0.15879961056183842
iteration : 13465
train acc:  0.734375
train loss:  0.4645232558250427
train gradient:  0.11264693221536845
iteration : 13466
train acc:  0.7578125
train loss:  0.48675137758255005
train gradient:  0.10970289768405993
iteration : 13467
train acc:  0.7109375
train loss:  0.46339020133018494
train gradient:  0.11987939336011268
iteration : 13468
train acc:  0.7578125
train loss:  0.5243110656738281
train gradient:  0.13354342450818874
iteration : 13469
train acc:  0.7578125
train loss:  0.4442463517189026
train gradient:  0.09344258997024835
iteration : 13470
train acc:  0.71875
train loss:  0.47794651985168457
train gradient:  0.12276475559576447
iteration : 13471
train acc:  0.75
train loss:  0.4668695032596588
train gradient:  0.12573061906933483
iteration : 13472
train acc:  0.765625
train loss:  0.4701078534126282
train gradient:  0.13558471827373175
iteration : 13473
train acc:  0.7265625
train loss:  0.5060939788818359
train gradient:  0.13149545447934088
iteration : 13474
train acc:  0.7421875
train loss:  0.43921440839767456
train gradient:  0.11385715164965966
iteration : 13475
train acc:  0.7265625
train loss:  0.4994525909423828
train gradient:  0.12503563314633576
iteration : 13476
train acc:  0.71875
train loss:  0.5006786584854126
train gradient:  0.1386209042819201
iteration : 13477
train acc:  0.765625
train loss:  0.48139283061027527
train gradient:  0.12632552044717954
iteration : 13478
train acc:  0.75
train loss:  0.52786785364151
train gradient:  0.13276510909527428
iteration : 13479
train acc:  0.7265625
train loss:  0.522553563117981
train gradient:  0.1339493920984291
iteration : 13480
train acc:  0.7578125
train loss:  0.47614189982414246
train gradient:  0.09509864604179685
iteration : 13481
train acc:  0.7109375
train loss:  0.48951059579849243
train gradient:  0.1695937734092835
iteration : 13482
train acc:  0.7734375
train loss:  0.4912845492362976
train gradient:  0.14852723366052667
iteration : 13483
train acc:  0.640625
train loss:  0.6059609651565552
train gradient:  0.1592716769680722
iteration : 13484
train acc:  0.765625
train loss:  0.4731123447418213
train gradient:  0.13640564786578122
iteration : 13485
train acc:  0.71875
train loss:  0.4803127646446228
train gradient:  0.1263078995930963
iteration : 13486
train acc:  0.765625
train loss:  0.4655214846134186
train gradient:  0.1271979669685514
iteration : 13487
train acc:  0.7109375
train loss:  0.4883619546890259
train gradient:  0.13034345422877477
iteration : 13488
train acc:  0.71875
train loss:  0.4957126975059509
train gradient:  0.12631225203352886
iteration : 13489
train acc:  0.765625
train loss:  0.4362059533596039
train gradient:  0.12687095966045348
iteration : 13490
train acc:  0.75
train loss:  0.5073085427284241
train gradient:  0.1509406039784532
iteration : 13491
train acc:  0.8203125
train loss:  0.4405178427696228
train gradient:  0.13639696852617245
iteration : 13492
train acc:  0.7265625
train loss:  0.5001711845397949
train gradient:  0.13085892821741935
iteration : 13493
train acc:  0.71875
train loss:  0.4847179055213928
train gradient:  0.13459946366494557
iteration : 13494
train acc:  0.7890625
train loss:  0.5019000768661499
train gradient:  0.1425866709908588
iteration : 13495
train acc:  0.6640625
train loss:  0.5342227220535278
train gradient:  0.13209279458473322
iteration : 13496
train acc:  0.7578125
train loss:  0.4590892791748047
train gradient:  0.1005292524123874
iteration : 13497
train acc:  0.75
train loss:  0.5400421619415283
train gradient:  0.16400782326124339
iteration : 13498
train acc:  0.7578125
train loss:  0.495635986328125
train gradient:  0.14678987544311017
iteration : 13499
train acc:  0.8125
train loss:  0.430786669254303
train gradient:  0.08802689005808667
iteration : 13500
train acc:  0.7109375
train loss:  0.5303971767425537
train gradient:  0.15945835895666055
iteration : 13501
train acc:  0.8125
train loss:  0.4429752230644226
train gradient:  0.11510281616618391
iteration : 13502
train acc:  0.7890625
train loss:  0.46615052223205566
train gradient:  0.15527231646415768
iteration : 13503
train acc:  0.7265625
train loss:  0.5142579078674316
train gradient:  0.10964021675938194
iteration : 13504
train acc:  0.71875
train loss:  0.47679460048675537
train gradient:  0.1020625652931823
iteration : 13505
train acc:  0.703125
train loss:  0.516553521156311
train gradient:  0.14119829017661584
iteration : 13506
train acc:  0.7421875
train loss:  0.46082112193107605
train gradient:  0.0955807013788725
iteration : 13507
train acc:  0.796875
train loss:  0.4624646306037903
train gradient:  0.12985369600568242
iteration : 13508
train acc:  0.703125
train loss:  0.49338221549987793
train gradient:  0.15855201184088152
iteration : 13509
train acc:  0.7421875
train loss:  0.48907914757728577
train gradient:  0.13392500088732212
iteration : 13510
train acc:  0.71875
train loss:  0.5136635899543762
train gradient:  0.11378771314331348
iteration : 13511
train acc:  0.71875
train loss:  0.4812467396259308
train gradient:  0.13087994087978003
iteration : 13512
train acc:  0.7734375
train loss:  0.5023354887962341
train gradient:  0.1206003379737944
iteration : 13513
train acc:  0.71875
train loss:  0.553533673286438
train gradient:  0.14448429227359155
iteration : 13514
train acc:  0.7421875
train loss:  0.48103204369544983
train gradient:  0.11200099978879935
iteration : 13515
train acc:  0.7578125
train loss:  0.4720920920372009
train gradient:  0.11881938258186103
iteration : 13516
train acc:  0.71875
train loss:  0.5684740543365479
train gradient:  0.2282318411683612
iteration : 13517
train acc:  0.6875
train loss:  0.5062538385391235
train gradient:  0.14227799301805555
iteration : 13518
train acc:  0.8125
train loss:  0.4308187663555145
train gradient:  0.12083803282980285
iteration : 13519
train acc:  0.7265625
train loss:  0.5092796087265015
train gradient:  0.11788170483890247
iteration : 13520
train acc:  0.78125
train loss:  0.4783790707588196
train gradient:  0.1743911059359723
iteration : 13521
train acc:  0.8359375
train loss:  0.42792245745658875
train gradient:  0.10471018822434662
iteration : 13522
train acc:  0.7734375
train loss:  0.4334648549556732
train gradient:  0.1317438419611331
iteration : 13523
train acc:  0.7265625
train loss:  0.5246802568435669
train gradient:  0.12457973069414029
iteration : 13524
train acc:  0.734375
train loss:  0.48584651947021484
train gradient:  0.11513026717642867
iteration : 13525
train acc:  0.75
train loss:  0.4535239636898041
train gradient:  0.09843356861216565
iteration : 13526
train acc:  0.734375
train loss:  0.5060625076293945
train gradient:  0.12999219442991589
iteration : 13527
train acc:  0.7265625
train loss:  0.5476037263870239
train gradient:  0.1499801884176245
iteration : 13528
train acc:  0.734375
train loss:  0.5040757656097412
train gradient:  0.14824931516400786
iteration : 13529
train acc:  0.7421875
train loss:  0.4969647228717804
train gradient:  0.09979602093102004
iteration : 13530
train acc:  0.6953125
train loss:  0.48078426718711853
train gradient:  0.13716041928654388
iteration : 13531
train acc:  0.7265625
train loss:  0.5073468685150146
train gradient:  0.14370933711549763
iteration : 13532
train acc:  0.71875
train loss:  0.4954807460308075
train gradient:  0.14056127441954136
iteration : 13533
train acc:  0.7890625
train loss:  0.4696420133113861
train gradient:  0.13507924641385394
iteration : 13534
train acc:  0.75
train loss:  0.4839969277381897
train gradient:  0.11649045377266967
iteration : 13535
train acc:  0.75
train loss:  0.4669189453125
train gradient:  0.11772803373928516
iteration : 13536
train acc:  0.7421875
train loss:  0.5241924524307251
train gradient:  0.114379337696911
iteration : 13537
train acc:  0.71875
train loss:  0.5204405784606934
train gradient:  0.15636075787073683
iteration : 13538
train acc:  0.796875
train loss:  0.4738813638687134
train gradient:  0.12500173143240786
iteration : 13539
train acc:  0.765625
train loss:  0.48294100165367126
train gradient:  0.1580551880252845
iteration : 13540
train acc:  0.828125
train loss:  0.41240665316581726
train gradient:  0.08995679482373418
iteration : 13541
train acc:  0.6953125
train loss:  0.5356099009513855
train gradient:  0.13513357015905353
iteration : 13542
train acc:  0.75
train loss:  0.47870728373527527
train gradient:  0.12317178510016004
iteration : 13543
train acc:  0.75
train loss:  0.49143850803375244
train gradient:  0.11509915668415029
iteration : 13544
train acc:  0.7890625
train loss:  0.42315590381622314
train gradient:  0.10802248121630138
iteration : 13545
train acc:  0.875
train loss:  0.3711043894290924
train gradient:  0.08297474649864253
iteration : 13546
train acc:  0.78125
train loss:  0.4265051484107971
train gradient:  0.08701286641316243
iteration : 13547
train acc:  0.7109375
train loss:  0.5002606511116028
train gradient:  0.12762175196948206
iteration : 13548
train acc:  0.78125
train loss:  0.4576377868652344
train gradient:  0.11653776758233028
iteration : 13549
train acc:  0.7734375
train loss:  0.4251491129398346
train gradient:  0.09235792633110523
iteration : 13550
train acc:  0.6484375
train loss:  0.623863697052002
train gradient:  0.1864291424191873
iteration : 13551
train acc:  0.75
train loss:  0.5036874413490295
train gradient:  0.1317891426873214
iteration : 13552
train acc:  0.75
train loss:  0.5297368764877319
train gradient:  0.13052535472691523
iteration : 13553
train acc:  0.78125
train loss:  0.44339966773986816
train gradient:  0.09599123782657604
iteration : 13554
train acc:  0.75
train loss:  0.4885557293891907
train gradient:  0.13124011267291394
iteration : 13555
train acc:  0.796875
train loss:  0.425973117351532
train gradient:  0.13110116566166408
iteration : 13556
train acc:  0.7265625
train loss:  0.49662482738494873
train gradient:  0.11245664491077831
iteration : 13557
train acc:  0.6875
train loss:  0.5757832527160645
train gradient:  0.14909733338267606
iteration : 13558
train acc:  0.8359375
train loss:  0.4436717629432678
train gradient:  0.10275283621542117
iteration : 13559
train acc:  0.7109375
train loss:  0.5010820627212524
train gradient:  0.1337387398477909
iteration : 13560
train acc:  0.6875
train loss:  0.5308186411857605
train gradient:  0.134717783738597
iteration : 13561
train acc:  0.7265625
train loss:  0.5048412084579468
train gradient:  0.18476475230748834
iteration : 13562
train acc:  0.6875
train loss:  0.5397669672966003
train gradient:  0.1573037315162128
iteration : 13563
train acc:  0.7734375
train loss:  0.44090282917022705
train gradient:  0.12072029017091218
iteration : 13564
train acc:  0.6875
train loss:  0.503149688243866
train gradient:  0.12658920707606497
iteration : 13565
train acc:  0.7734375
train loss:  0.4579513669013977
train gradient:  0.10233322482073869
iteration : 13566
train acc:  0.640625
train loss:  0.5702940225601196
train gradient:  0.19804163858947413
iteration : 13567
train acc:  0.7578125
train loss:  0.4677866995334625
train gradient:  0.11661100987885828
iteration : 13568
train acc:  0.7734375
train loss:  0.47155559062957764
train gradient:  0.10444713067423565
iteration : 13569
train acc:  0.71875
train loss:  0.49488577246665955
train gradient:  0.12716562885657373
iteration : 13570
train acc:  0.75
train loss:  0.523042619228363
train gradient:  0.14710962307884004
iteration : 13571
train acc:  0.7734375
train loss:  0.4217018187046051
train gradient:  0.11902582462706326
iteration : 13572
train acc:  0.765625
train loss:  0.4425255060195923
train gradient:  0.0951817016904634
iteration : 13573
train acc:  0.7421875
train loss:  0.525245726108551
train gradient:  0.13803918807193716
iteration : 13574
train acc:  0.8125
train loss:  0.4255479574203491
train gradient:  0.08770695792574362
iteration : 13575
train acc:  0.8125
train loss:  0.43514418601989746
train gradient:  0.10347596952099901
iteration : 13576
train acc:  0.7109375
train loss:  0.5074054598808289
train gradient:  0.12829618112531888
iteration : 13577
train acc:  0.71875
train loss:  0.5013384222984314
train gradient:  0.14320565499444496
iteration : 13578
train acc:  0.71875
train loss:  0.5661178231239319
train gradient:  0.19998034484241184
iteration : 13579
train acc:  0.765625
train loss:  0.46082234382629395
train gradient:  0.0981274619554521
iteration : 13580
train acc:  0.71875
train loss:  0.550289511680603
train gradient:  0.1504998632864589
iteration : 13581
train acc:  0.765625
train loss:  0.4312590956687927
train gradient:  0.09373203226910844
iteration : 13582
train acc:  0.7421875
train loss:  0.46883249282836914
train gradient:  0.12360910207192043
iteration : 13583
train acc:  0.6796875
train loss:  0.5758370757102966
train gradient:  0.1868966085436523
iteration : 13584
train acc:  0.71875
train loss:  0.47906509041786194
train gradient:  0.10866930010004165
iteration : 13585
train acc:  0.7421875
train loss:  0.48605403304100037
train gradient:  0.10595278088408445
iteration : 13586
train acc:  0.6953125
train loss:  0.5270141363143921
train gradient:  0.17900352588760066
iteration : 13587
train acc:  0.6953125
train loss:  0.548142671585083
train gradient:  0.16842901120939913
iteration : 13588
train acc:  0.796875
train loss:  0.42342042922973633
train gradient:  0.1519607189809028
iteration : 13589
train acc:  0.796875
train loss:  0.5074626207351685
train gradient:  0.18989679588921465
iteration : 13590
train acc:  0.7890625
train loss:  0.4780195355415344
train gradient:  0.17478994768663564
iteration : 13591
train acc:  0.765625
train loss:  0.4746263027191162
train gradient:  0.12460883634113867
iteration : 13592
train acc:  0.7421875
train loss:  0.45536988973617554
train gradient:  0.127154599108473
iteration : 13593
train acc:  0.765625
train loss:  0.45786046981811523
train gradient:  0.09960754797090587
iteration : 13594
train acc:  0.71875
train loss:  0.4917699992656708
train gradient:  0.13886915776398162
iteration : 13595
train acc:  0.7578125
train loss:  0.5104044675827026
train gradient:  0.158955877591173
iteration : 13596
train acc:  0.796875
train loss:  0.43128761649131775
train gradient:  0.08978033135451083
iteration : 13597
train acc:  0.7421875
train loss:  0.5127620697021484
train gradient:  0.16942461091157976
iteration : 13598
train acc:  0.7109375
train loss:  0.5209990739822388
train gradient:  0.1568350482539571
iteration : 13599
train acc:  0.7265625
train loss:  0.4748585522174835
train gradient:  0.15229593089995025
iteration : 13600
train acc:  0.8046875
train loss:  0.4458783268928528
train gradient:  0.1199872525734355
iteration : 13601
train acc:  0.6953125
train loss:  0.5891222357749939
train gradient:  0.2317243784968224
iteration : 13602
train acc:  0.6953125
train loss:  0.5315577387809753
train gradient:  0.12713639869760365
iteration : 13603
train acc:  0.734375
train loss:  0.4295251965522766
train gradient:  0.09796144476374363
iteration : 13604
train acc:  0.71875
train loss:  0.5127416849136353
train gradient:  0.15858928977787043
iteration : 13605
train acc:  0.75
train loss:  0.4867110848426819
train gradient:  0.12228461510777505
iteration : 13606
train acc:  0.6640625
train loss:  0.5706228017807007
train gradient:  0.1667004970275304
iteration : 13607
train acc:  0.7265625
train loss:  0.5074320435523987
train gradient:  0.11060771603674109
iteration : 13608
train acc:  0.7265625
train loss:  0.5076844692230225
train gradient:  0.16873682874296114
iteration : 13609
train acc:  0.6953125
train loss:  0.5498435497283936
train gradient:  0.14429691847304188
iteration : 13610
train acc:  0.7109375
train loss:  0.5301575660705566
train gradient:  0.16659133980547766
iteration : 13611
train acc:  0.7421875
train loss:  0.5114055871963501
train gradient:  0.12650655094765684
iteration : 13612
train acc:  0.765625
train loss:  0.44230538606643677
train gradient:  0.09572066066243519
iteration : 13613
train acc:  0.7265625
train loss:  0.5102893114089966
train gradient:  0.15303782442804126
iteration : 13614
train acc:  0.78125
train loss:  0.4368528723716736
train gradient:  0.11255633208402416
iteration : 13615
train acc:  0.7734375
train loss:  0.4788955748081207
train gradient:  0.11380965146005159
iteration : 13616
train acc:  0.734375
train loss:  0.5275603532791138
train gradient:  0.15934605665368634
iteration : 13617
train acc:  0.765625
train loss:  0.45307451486587524
train gradient:  0.18148057096778297
iteration : 13618
train acc:  0.7421875
train loss:  0.5066437721252441
train gradient:  0.12164443091815774
iteration : 13619
train acc:  0.7421875
train loss:  0.47331178188323975
train gradient:  0.1119569292461996
iteration : 13620
train acc:  0.78125
train loss:  0.46903085708618164
train gradient:  0.13955286813170606
iteration : 13621
train acc:  0.7578125
train loss:  0.45346835255622864
train gradient:  0.11997539883368234
iteration : 13622
train acc:  0.6875
train loss:  0.5304649472236633
train gradient:  0.16453109805877197
iteration : 13623
train acc:  0.734375
train loss:  0.47121140360832214
train gradient:  0.11217198283708202
iteration : 13624
train acc:  0.671875
train loss:  0.5706174373626709
train gradient:  0.14330326039511293
iteration : 13625
train acc:  0.7578125
train loss:  0.48447078466415405
train gradient:  0.10725923174946235
iteration : 13626
train acc:  0.703125
train loss:  0.5764496922492981
train gradient:  0.1692057132770029
iteration : 13627
train acc:  0.671875
train loss:  0.5530747175216675
train gradient:  0.16939282037184591
iteration : 13628
train acc:  0.7265625
train loss:  0.5250866413116455
train gradient:  0.15968325077261109
iteration : 13629
train acc:  0.78125
train loss:  0.4914456605911255
train gradient:  0.1276256725292289
iteration : 13630
train acc:  0.765625
train loss:  0.4667978286743164
train gradient:  0.09818638404965187
iteration : 13631
train acc:  0.7421875
train loss:  0.493293434381485
train gradient:  0.12130942265588303
iteration : 13632
train acc:  0.7578125
train loss:  0.501777708530426
train gradient:  0.11892537340515916
iteration : 13633
train acc:  0.71875
train loss:  0.5463424324989319
train gradient:  0.16926506779239767
iteration : 13634
train acc:  0.7578125
train loss:  0.5012871623039246
train gradient:  0.16118584104913064
iteration : 13635
train acc:  0.7421875
train loss:  0.47848665714263916
train gradient:  0.11953522836710201
iteration : 13636
train acc:  0.6953125
train loss:  0.5336324572563171
train gradient:  0.1733718497848078
iteration : 13637
train acc:  0.7421875
train loss:  0.4714653491973877
train gradient:  0.12380257968978652
iteration : 13638
train acc:  0.78125
train loss:  0.49106845259666443
train gradient:  0.10609082375746698
iteration : 13639
train acc:  0.7578125
train loss:  0.4872719645500183
train gradient:  0.11506193811748963
iteration : 13640
train acc:  0.734375
train loss:  0.4898040294647217
train gradient:  0.17080473860278939
iteration : 13641
train acc:  0.765625
train loss:  0.4699476957321167
train gradient:  0.11240793155830804
iteration : 13642
train acc:  0.8046875
train loss:  0.4157288670539856
train gradient:  0.10696813985614372
iteration : 13643
train acc:  0.65625
train loss:  0.5093421339988708
train gradient:  0.14881616299580952
iteration : 13644
train acc:  0.765625
train loss:  0.46876537799835205
train gradient:  0.13076852273229433
iteration : 13645
train acc:  0.7890625
train loss:  0.4663197696208954
train gradient:  0.13851060011932212
iteration : 13646
train acc:  0.765625
train loss:  0.4573577046394348
train gradient:  0.09368074474976958
iteration : 13647
train acc:  0.75
train loss:  0.4804859757423401
train gradient:  0.11223692170081635
iteration : 13648
train acc:  0.71875
train loss:  0.511456310749054
train gradient:  0.13280023693612897
iteration : 13649
train acc:  0.7578125
train loss:  0.49643582105636597
train gradient:  0.12010989723974994
iteration : 13650
train acc:  0.7109375
train loss:  0.5273988246917725
train gradient:  0.1247513295717775
iteration : 13651
train acc:  0.8125
train loss:  0.40091603994369507
train gradient:  0.10088005290384966
iteration : 13652
train acc:  0.8203125
train loss:  0.45024484395980835
train gradient:  0.12273730942619247
iteration : 13653
train acc:  0.6796875
train loss:  0.5217562317848206
train gradient:  0.12771013706194256
iteration : 13654
train acc:  0.7421875
train loss:  0.4550543427467346
train gradient:  0.10336235421613502
iteration : 13655
train acc:  0.7890625
train loss:  0.466708242893219
train gradient:  0.1132724105386846
iteration : 13656
train acc:  0.7421875
train loss:  0.500354528427124
train gradient:  0.1586581554345481
iteration : 13657
train acc:  0.7734375
train loss:  0.43878522515296936
train gradient:  0.09897391020779837
iteration : 13658
train acc:  0.7578125
train loss:  0.4700675308704376
train gradient:  0.12544209288862773
iteration : 13659
train acc:  0.796875
train loss:  0.46372777223587036
train gradient:  0.12547155891142478
iteration : 13660
train acc:  0.8125
train loss:  0.4069552421569824
train gradient:  0.09397817400014159
iteration : 13661
train acc:  0.6953125
train loss:  0.5923731923103333
train gradient:  0.19818260621210404
iteration : 13662
train acc:  0.75
train loss:  0.4609876275062561
train gradient:  0.09947181888050331
iteration : 13663
train acc:  0.7265625
train loss:  0.5053881406784058
train gradient:  0.1214285622582673
iteration : 13664
train acc:  0.7265625
train loss:  0.4768727421760559
train gradient:  0.12124081788772444
iteration : 13665
train acc:  0.7265625
train loss:  0.4855039715766907
train gradient:  0.1594974065076616
iteration : 13666
train acc:  0.734375
train loss:  0.4299221634864807
train gradient:  0.11207305761180487
iteration : 13667
train acc:  0.7421875
train loss:  0.526013970375061
train gradient:  0.13697423079556875
iteration : 13668
train acc:  0.71875
train loss:  0.49343639612197876
train gradient:  0.139093410224228
iteration : 13669
train acc:  0.7734375
train loss:  0.45336663722991943
train gradient:  0.1191124201270342
iteration : 13670
train acc:  0.6875
train loss:  0.5969215631484985
train gradient:  0.21252193182025403
iteration : 13671
train acc:  0.7109375
train loss:  0.5291397571563721
train gradient:  0.13536663322094694
iteration : 13672
train acc:  0.78125
train loss:  0.44285017251968384
train gradient:  0.10195936576562778
iteration : 13673
train acc:  0.796875
train loss:  0.4601881504058838
train gradient:  0.10942652075254174
iteration : 13674
train acc:  0.75
train loss:  0.49152350425720215
train gradient:  0.10619298722017641
iteration : 13675
train acc:  0.7890625
train loss:  0.4558425843715668
train gradient:  0.11462647464967536
iteration : 13676
train acc:  0.703125
train loss:  0.5100246667861938
train gradient:  0.13185262521787078
iteration : 13677
train acc:  0.8359375
train loss:  0.46345165371894836
train gradient:  0.11556656718041991
iteration : 13678
train acc:  0.734375
train loss:  0.49173566699028015
train gradient:  0.11765702665298748
iteration : 13679
train acc:  0.7109375
train loss:  0.5773124694824219
train gradient:  0.15602613283429817
iteration : 13680
train acc:  0.7109375
train loss:  0.5530064702033997
train gradient:  0.1281859925846942
iteration : 13681
train acc:  0.734375
train loss:  0.5276307463645935
train gradient:  0.15838456875110135
iteration : 13682
train acc:  0.71875
train loss:  0.5254095792770386
train gradient:  0.1141776100920364
iteration : 13683
train acc:  0.7890625
train loss:  0.45562154054641724
train gradient:  0.12846700134374076
iteration : 13684
train acc:  0.78125
train loss:  0.4903067350387573
train gradient:  0.14307908893225862
iteration : 13685
train acc:  0.7734375
train loss:  0.4813641905784607
train gradient:  0.1542271013364656
iteration : 13686
train acc:  0.765625
train loss:  0.4912984371185303
train gradient:  0.12237319037062122
iteration : 13687
train acc:  0.71875
train loss:  0.48833125829696655
train gradient:  0.15033315355546628
iteration : 13688
train acc:  0.734375
train loss:  0.47352972626686096
train gradient:  0.16193762880404355
iteration : 13689
train acc:  0.765625
train loss:  0.481281042098999
train gradient:  0.11095348949652153
iteration : 13690
train acc:  0.7109375
train loss:  0.4834306836128235
train gradient:  0.10282363146250215
iteration : 13691
train acc:  0.7734375
train loss:  0.45847421884536743
train gradient:  0.10396101830285018
iteration : 13692
train acc:  0.734375
train loss:  0.47789478302001953
train gradient:  0.13458442554412653
iteration : 13693
train acc:  0.8125
train loss:  0.4453744888305664
train gradient:  0.09516170372773564
iteration : 13694
train acc:  0.765625
train loss:  0.46164223551750183
train gradient:  0.10581422795918981
iteration : 13695
train acc:  0.71875
train loss:  0.4702255129814148
train gradient:  0.103856015047312
iteration : 13696
train acc:  0.734375
train loss:  0.49498066306114197
train gradient:  0.1211162553933176
iteration : 13697
train acc:  0.8359375
train loss:  0.392672061920166
train gradient:  0.07912757011138512
iteration : 13698
train acc:  0.765625
train loss:  0.4762760102748871
train gradient:  0.13158678349246777
iteration : 13699
train acc:  0.765625
train loss:  0.4443040192127228
train gradient:  0.10174502050243449
iteration : 13700
train acc:  0.8125
train loss:  0.5075242519378662
train gradient:  0.11718374074765442
iteration : 13701
train acc:  0.7109375
train loss:  0.5168517827987671
train gradient:  0.12957653797694052
iteration : 13702
train acc:  0.7578125
train loss:  0.4980587959289551
train gradient:  0.12934259685492389
iteration : 13703
train acc:  0.7109375
train loss:  0.5141816139221191
train gradient:  0.14342342631622904
iteration : 13704
train acc:  0.7421875
train loss:  0.4489530026912689
train gradient:  0.09027038182651523
iteration : 13705
train acc:  0.75
train loss:  0.5600153207778931
train gradient:  0.13610861014144868
iteration : 13706
train acc:  0.796875
train loss:  0.4908396005630493
train gradient:  0.16074462328462286
iteration : 13707
train acc:  0.734375
train loss:  0.4866657257080078
train gradient:  0.11121369767628393
iteration : 13708
train acc:  0.7578125
train loss:  0.48191094398498535
train gradient:  0.1296584081974211
iteration : 13709
train acc:  0.7109375
train loss:  0.5386592149734497
train gradient:  0.14324084769964607
iteration : 13710
train acc:  0.75
train loss:  0.5008876323699951
train gradient:  0.15095397162211102
iteration : 13711
train acc:  0.7421875
train loss:  0.47982317209243774
train gradient:  0.12872888576149244
iteration : 13712
train acc:  0.734375
train loss:  0.5235101580619812
train gradient:  0.12389113599898767
iteration : 13713
train acc:  0.765625
train loss:  0.4607717990875244
train gradient:  0.10082437666438197
iteration : 13714
train acc:  0.8046875
train loss:  0.39359837770462036
train gradient:  0.0952302462476813
iteration : 13715
train acc:  0.7109375
train loss:  0.5242998600006104
train gradient:  0.14406294245361348
iteration : 13716
train acc:  0.8046875
train loss:  0.39630061388015747
train gradient:  0.0804548035950127
iteration : 13717
train acc:  0.8125
train loss:  0.4403578042984009
train gradient:  0.10101654825144969
iteration : 13718
train acc:  0.8125
train loss:  0.4305238723754883
train gradient:  0.12237611370960093
iteration : 13719
train acc:  0.75
train loss:  0.4426764249801636
train gradient:  0.1131790045947053
iteration : 13720
train acc:  0.796875
train loss:  0.4176356792449951
train gradient:  0.07480867384371641
iteration : 13721
train acc:  0.7109375
train loss:  0.5338407158851624
train gradient:  0.1405875234995715
iteration : 13722
train acc:  0.7421875
train loss:  0.5238403677940369
train gradient:  0.13827900894749937
iteration : 13723
train acc:  0.6953125
train loss:  0.48173317313194275
train gradient:  0.11003490037980264
iteration : 13724
train acc:  0.7734375
train loss:  0.4249008893966675
train gradient:  0.09074898547644887
iteration : 13725
train acc:  0.75
train loss:  0.4703283905982971
train gradient:  0.10386230564930246
iteration : 13726
train acc:  0.78125
train loss:  0.42779409885406494
train gradient:  0.09804237985346168
iteration : 13727
train acc:  0.75
train loss:  0.48154765367507935
train gradient:  0.1229279467260747
iteration : 13728
train acc:  0.7109375
train loss:  0.5103023052215576
train gradient:  0.12278438829837585
iteration : 13729
train acc:  0.6796875
train loss:  0.5266274809837341
train gradient:  0.12019864210448206
iteration : 13730
train acc:  0.6953125
train loss:  0.5650644302368164
train gradient:  0.1639464262309182
iteration : 13731
train acc:  0.7109375
train loss:  0.5331708192825317
train gradient:  0.1402339719304389
iteration : 13732
train acc:  0.7265625
train loss:  0.540326714515686
train gradient:  0.1681429935139027
iteration : 13733
train acc:  0.75
train loss:  0.4778671860694885
train gradient:  0.10692679119198685
iteration : 13734
train acc:  0.75
train loss:  0.5117957592010498
train gradient:  0.13665970084101964
iteration : 13735
train acc:  0.734375
train loss:  0.48757404088974
train gradient:  0.11019486848757318
iteration : 13736
train acc:  0.7421875
train loss:  0.5078252553939819
train gradient:  0.1448727446187103
iteration : 13737
train acc:  0.7890625
train loss:  0.44450169801712036
train gradient:  0.0956535909645385
iteration : 13738
train acc:  0.734375
train loss:  0.5377507209777832
train gradient:  0.16446026296325195
iteration : 13739
train acc:  0.7890625
train loss:  0.4592571258544922
train gradient:  0.1285775488376182
iteration : 13740
train acc:  0.7578125
train loss:  0.4515029191970825
train gradient:  0.10278082658533415
iteration : 13741
train acc:  0.71875
train loss:  0.5003191232681274
train gradient:  0.12098602913879779
iteration : 13742
train acc:  0.7890625
train loss:  0.4639527499675751
train gradient:  0.1094094696867295
iteration : 13743
train acc:  0.78125
train loss:  0.47427958250045776
train gradient:  0.13600968023092985
iteration : 13744
train acc:  0.7421875
train loss:  0.4771426320075989
train gradient:  0.12258561456313646
iteration : 13745
train acc:  0.7109375
train loss:  0.5574838519096375
train gradient:  0.13157322082622655
iteration : 13746
train acc:  0.7890625
train loss:  0.45033442974090576
train gradient:  0.09970490171618694
iteration : 13747
train acc:  0.796875
train loss:  0.45891073346138
train gradient:  0.11067451493732587
iteration : 13748
train acc:  0.734375
train loss:  0.46325042843818665
train gradient:  0.11442089632942508
iteration : 13749
train acc:  0.75
train loss:  0.4727253019809723
train gradient:  0.12351743698506161
iteration : 13750
train acc:  0.71875
train loss:  0.5090680718421936
train gradient:  0.1389529640536975
iteration : 13751
train acc:  0.7578125
train loss:  0.4458657503128052
train gradient:  0.11480712272429056
iteration : 13752
train acc:  0.796875
train loss:  0.44714391231536865
train gradient:  0.11881775968304564
iteration : 13753
train acc:  0.765625
train loss:  0.4874042570590973
train gradient:  0.12054392840564444
iteration : 13754
train acc:  0.796875
train loss:  0.42911767959594727
train gradient:  0.0878052248702114
iteration : 13755
train acc:  0.765625
train loss:  0.4672158360481262
train gradient:  0.12654027145396912
iteration : 13756
train acc:  0.7265625
train loss:  0.5003383159637451
train gradient:  0.1649770241661531
iteration : 13757
train acc:  0.7578125
train loss:  0.45296311378479004
train gradient:  0.11527461542231486
iteration : 13758
train acc:  0.703125
train loss:  0.5960277915000916
train gradient:  0.19703049499898848
iteration : 13759
train acc:  0.7578125
train loss:  0.45430001616477966
train gradient:  0.0972656810239354
iteration : 13760
train acc:  0.7109375
train loss:  0.4985167682170868
train gradient:  0.11634223758477831
iteration : 13761
train acc:  0.78125
train loss:  0.46122875809669495
train gradient:  0.10756765037712361
iteration : 13762
train acc:  0.75
train loss:  0.46569687128067017
train gradient:  0.13937735809533752
iteration : 13763
train acc:  0.7890625
train loss:  0.42446744441986084
train gradient:  0.10207452795610181
iteration : 13764
train acc:  0.7578125
train loss:  0.4747775197029114
train gradient:  0.11828849897306591
iteration : 13765
train acc:  0.7578125
train loss:  0.46680182218551636
train gradient:  0.1277209861574352
iteration : 13766
train acc:  0.734375
train loss:  0.4707355499267578
train gradient:  0.1168891423749232
iteration : 13767
train acc:  0.7109375
train loss:  0.5501524209976196
train gradient:  0.1401635998181232
iteration : 13768
train acc:  0.6484375
train loss:  0.5704406499862671
train gradient:  0.16608305818486396
iteration : 13769
train acc:  0.7890625
train loss:  0.5102160573005676
train gradient:  0.137745397535405
iteration : 13770
train acc:  0.734375
train loss:  0.5224082469940186
train gradient:  0.1387095979918348
iteration : 13771
train acc:  0.71875
train loss:  0.5568835735321045
train gradient:  0.1491325962348184
iteration : 13772
train acc:  0.78125
train loss:  0.4519605040550232
train gradient:  0.1539903212717143
iteration : 13773
train acc:  0.7734375
train loss:  0.45778125524520874
train gradient:  0.10007349341851682
iteration : 13774
train acc:  0.75
train loss:  0.4895353317260742
train gradient:  0.1150652148555628
iteration : 13775
train acc:  0.7109375
train loss:  0.48592710494995117
train gradient:  0.13480866553121124
iteration : 13776
train acc:  0.75
train loss:  0.50554358959198
train gradient:  0.11248375279522171
iteration : 13777
train acc:  0.765625
train loss:  0.5369377136230469
train gradient:  0.15177293167443345
iteration : 13778
train acc:  0.7734375
train loss:  0.4810522198677063
train gradient:  0.12277961952846805
iteration : 13779
train acc:  0.6875
train loss:  0.558159589767456
train gradient:  0.17654220729882975
iteration : 13780
train acc:  0.75
train loss:  0.41690593957901
train gradient:  0.0774189810737448
iteration : 13781
train acc:  0.6796875
train loss:  0.5015260577201843
train gradient:  0.12018314610590229
iteration : 13782
train acc:  0.75
train loss:  0.4725927412509918
train gradient:  0.13228813013941576
iteration : 13783
train acc:  0.7265625
train loss:  0.5053008198738098
train gradient:  0.13730523027731073
iteration : 13784
train acc:  0.7421875
train loss:  0.485251784324646
train gradient:  0.1268412548105257
iteration : 13785
train acc:  0.7890625
train loss:  0.42882975935935974
train gradient:  0.09556568101570176
iteration : 13786
train acc:  0.75
train loss:  0.4850931763648987
train gradient:  0.12659231181328906
iteration : 13787
train acc:  0.75
train loss:  0.49351176619529724
train gradient:  0.12702680150296897
iteration : 13788
train acc:  0.7421875
train loss:  0.4536352753639221
train gradient:  0.11842910215462026
iteration : 13789
train acc:  0.8203125
train loss:  0.40159690380096436
train gradient:  0.0699599743124099
iteration : 13790
train acc:  0.78125
train loss:  0.4758744239807129
train gradient:  0.10270317056427909
iteration : 13791
train acc:  0.71875
train loss:  0.49513089656829834
train gradient:  0.12830209036730836
iteration : 13792
train acc:  0.703125
train loss:  0.5682079792022705
train gradient:  0.17167627720537204
iteration : 13793
train acc:  0.7421875
train loss:  0.47031858563423157
train gradient:  0.08742738553172359
iteration : 13794
train acc:  0.7421875
train loss:  0.5428303480148315
train gradient:  0.15413468660198654
iteration : 13795
train acc:  0.7265625
train loss:  0.5518699288368225
train gradient:  0.16102454966419527
iteration : 13796
train acc:  0.78125
train loss:  0.44345375895500183
train gradient:  0.10916211717482902
iteration : 13797
train acc:  0.828125
train loss:  0.4080469310283661
train gradient:  0.08830581907527017
iteration : 13798
train acc:  0.7734375
train loss:  0.5004849433898926
train gradient:  0.1290339649810599
iteration : 13799
train acc:  0.7734375
train loss:  0.40957629680633545
train gradient:  0.09222563531156705
iteration : 13800
train acc:  0.671875
train loss:  0.5340729355812073
train gradient:  0.13212432640261546
iteration : 13801
train acc:  0.78125
train loss:  0.4886624813079834
train gradient:  0.12744284433122716
iteration : 13802
train acc:  0.7421875
train loss:  0.44919419288635254
train gradient:  0.11605593333537825
iteration : 13803
train acc:  0.75
train loss:  0.5156571269035339
train gradient:  0.10365724162537783
iteration : 13804
train acc:  0.71875
train loss:  0.5479278564453125
train gradient:  0.15231855094720653
iteration : 13805
train acc:  0.765625
train loss:  0.47110193967819214
train gradient:  0.10377947643215661
iteration : 13806
train acc:  0.7578125
train loss:  0.48242709040641785
train gradient:  0.12389779287725985
iteration : 13807
train acc:  0.7265625
train loss:  0.5190234184265137
train gradient:  0.12123024110182755
iteration : 13808
train acc:  0.7734375
train loss:  0.49927717447280884
train gradient:  0.12409128193699494
iteration : 13809
train acc:  0.765625
train loss:  0.4707493782043457
train gradient:  0.13885255857895407
iteration : 13810
train acc:  0.7578125
train loss:  0.43560510873794556
train gradient:  0.10416008055180456
iteration : 13811
train acc:  0.7421875
train loss:  0.4787972569465637
train gradient:  0.10128272673521452
iteration : 13812
train acc:  0.765625
train loss:  0.43080902099609375
train gradient:  0.09939518018286407
iteration : 13813
train acc:  0.6875
train loss:  0.5399168729782104
train gradient:  0.15259281210818115
iteration : 13814
train acc:  0.75
train loss:  0.48347464203834534
train gradient:  0.13606733474396338
iteration : 13815
train acc:  0.7265625
train loss:  0.48350822925567627
train gradient:  0.11874275280281081
iteration : 13816
train acc:  0.7734375
train loss:  0.47194555401802063
train gradient:  0.11296630528021988
iteration : 13817
train acc:  0.75
train loss:  0.46886616945266724
train gradient:  0.15593539757749264
iteration : 13818
train acc:  0.7578125
train loss:  0.491075336933136
train gradient:  0.11398949734299266
iteration : 13819
train acc:  0.6875
train loss:  0.560645341873169
train gradient:  0.13704039548339678
iteration : 13820
train acc:  0.7265625
train loss:  0.48441189527511597
train gradient:  0.1149711131902616
iteration : 13821
train acc:  0.7265625
train loss:  0.47107118368148804
train gradient:  0.10550175036415602
iteration : 13822
train acc:  0.71875
train loss:  0.5803077220916748
train gradient:  0.14796235366259297
iteration : 13823
train acc:  0.78125
train loss:  0.4402230679988861
train gradient:  0.08412947960582685
iteration : 13824
train acc:  0.7578125
train loss:  0.4858233332633972
train gradient:  0.14481178638989417
iteration : 13825
train acc:  0.7109375
train loss:  0.5089256167411804
train gradient:  0.11618872758664489
iteration : 13826
train acc:  0.7421875
train loss:  0.5009916424751282
train gradient:  0.16407451951839713
iteration : 13827
train acc:  0.7890625
train loss:  0.4090896248817444
train gradient:  0.10824527300721803
iteration : 13828
train acc:  0.7421875
train loss:  0.5266693830490112
train gradient:  0.13795325450823193
iteration : 13829
train acc:  0.7421875
train loss:  0.5154297351837158
train gradient:  0.12427963666383512
iteration : 13830
train acc:  0.75
train loss:  0.4762147068977356
train gradient:  0.10635343132858963
iteration : 13831
train acc:  0.7734375
train loss:  0.4349237084388733
train gradient:  0.10449412992691122
iteration : 13832
train acc:  0.7421875
train loss:  0.43894755840301514
train gradient:  0.10693870830732491
iteration : 13833
train acc:  0.7578125
train loss:  0.5010550022125244
train gradient:  0.16137422319098926
iteration : 13834
train acc:  0.7265625
train loss:  0.516571044921875
train gradient:  0.1590585962522418
iteration : 13835
train acc:  0.765625
train loss:  0.46523553133010864
train gradient:  0.15903658808875204
iteration : 13836
train acc:  0.7578125
train loss:  0.48729634284973145
train gradient:  0.11883163298458904
iteration : 13837
train acc:  0.8046875
train loss:  0.4339637756347656
train gradient:  0.11166303251848488
iteration : 13838
train acc:  0.734375
train loss:  0.5190948247909546
train gradient:  0.159354969147107
iteration : 13839
train acc:  0.671875
train loss:  0.5636149644851685
train gradient:  0.19370656971640965
iteration : 13840
train acc:  0.7421875
train loss:  0.5451639890670776
train gradient:  0.16321266868476164
iteration : 13841
train acc:  0.796875
train loss:  0.4662306308746338
train gradient:  0.12226789938880256
iteration : 13842
train acc:  0.765625
train loss:  0.4466301500797272
train gradient:  0.09708405321932949
iteration : 13843
train acc:  0.8125
train loss:  0.41809016466140747
train gradient:  0.099338125447501
iteration : 13844
train acc:  0.71875
train loss:  0.5449520945549011
train gradient:  0.14006902204830868
iteration : 13845
train acc:  0.671875
train loss:  0.522649347782135
train gradient:  0.14107118143304906
iteration : 13846
train acc:  0.7890625
train loss:  0.46829015016555786
train gradient:  0.11475733203087406
iteration : 13847
train acc:  0.7890625
train loss:  0.4532391130924225
train gradient:  0.12521707242173752
iteration : 13848
train acc:  0.7421875
train loss:  0.5165624022483826
train gradient:  0.12612309666409352
iteration : 13849
train acc:  0.734375
train loss:  0.4937819838523865
train gradient:  0.1347422694769522
iteration : 13850
train acc:  0.7734375
train loss:  0.4663279950618744
train gradient:  0.11231025273094951
iteration : 13851
train acc:  0.796875
train loss:  0.423669695854187
train gradient:  0.09105802454128453
iteration : 13852
train acc:  0.75
train loss:  0.49529314041137695
train gradient:  0.1336216335833731
iteration : 13853
train acc:  0.71875
train loss:  0.4910666346549988
train gradient:  0.12658433555053833
iteration : 13854
train acc:  0.7421875
train loss:  0.49136415123939514
train gradient:  0.13037471309121063
iteration : 13855
train acc:  0.765625
train loss:  0.45754846930503845
train gradient:  0.15594151989111327
iteration : 13856
train acc:  0.7109375
train loss:  0.4786919057369232
train gradient:  0.10181676495120083
iteration : 13857
train acc:  0.6796875
train loss:  0.5501542687416077
train gradient:  0.14108956135253914
iteration : 13858
train acc:  0.828125
train loss:  0.4559289216995239
train gradient:  0.12834596708422807
iteration : 13859
train acc:  0.796875
train loss:  0.4485085904598236
train gradient:  0.10573340561618472
iteration : 13860
train acc:  0.765625
train loss:  0.46776890754699707
train gradient:  0.13081179352159877
iteration : 13861
train acc:  0.6796875
train loss:  0.5502730011940002
train gradient:  0.16840082749383092
iteration : 13862
train acc:  0.71875
train loss:  0.5032107830047607
train gradient:  0.1410973041281558
iteration : 13863
train acc:  0.6953125
train loss:  0.5235418081283569
train gradient:  0.12184274690290539
iteration : 13864
train acc:  0.7265625
train loss:  0.4472660422325134
train gradient:  0.10907345385230124
iteration : 13865
train acc:  0.7890625
train loss:  0.4509372115135193
train gradient:  0.08642572610497318
iteration : 13866
train acc:  0.7109375
train loss:  0.5893067717552185
train gradient:  0.19881156984348536
iteration : 13867
train acc:  0.8125
train loss:  0.452419638633728
train gradient:  0.11041455340722049
iteration : 13868
train acc:  0.7421875
train loss:  0.5195810794830322
train gradient:  0.22079492440536352
iteration : 13869
train acc:  0.7890625
train loss:  0.4445521831512451
train gradient:  0.15448652788096712
iteration : 13870
train acc:  0.7265625
train loss:  0.4870399236679077
train gradient:  0.11551812325070746
iteration : 13871
train acc:  0.78125
train loss:  0.45596206188201904
train gradient:  0.11724024987556132
iteration : 13872
train acc:  0.7265625
train loss:  0.4911423325538635
train gradient:  0.13779819252408826
iteration : 13873
train acc:  0.7421875
train loss:  0.5021888017654419
train gradient:  0.1405307405379969
iteration : 13874
train acc:  0.796875
train loss:  0.4695785641670227
train gradient:  0.13004832218850276
iteration : 13875
train acc:  0.7734375
train loss:  0.5262787342071533
train gradient:  0.1397080249442697
iteration : 13876
train acc:  0.734375
train loss:  0.5066415667533875
train gradient:  0.14413373216403336
iteration : 13877
train acc:  0.71875
train loss:  0.47941315174102783
train gradient:  0.11220181422080755
iteration : 13878
train acc:  0.828125
train loss:  0.426441490650177
train gradient:  0.11609599616645853
iteration : 13879
train acc:  0.7421875
train loss:  0.45561349391937256
train gradient:  0.11167868026264068
iteration : 13880
train acc:  0.703125
train loss:  0.49819016456604004
train gradient:  0.12184918209004943
iteration : 13881
train acc:  0.8203125
train loss:  0.4685097932815552
train gradient:  0.1252129041696995
iteration : 13882
train acc:  0.7578125
train loss:  0.47458770871162415
train gradient:  0.11139546095848596
iteration : 13883
train acc:  0.7109375
train loss:  0.526666522026062
train gradient:  0.1410615162703998
iteration : 13884
train acc:  0.6796875
train loss:  0.6009479761123657
train gradient:  0.16218934349798028
iteration : 13885
train acc:  0.671875
train loss:  0.5240134596824646
train gradient:  0.1456965200121252
iteration : 13886
train acc:  0.7578125
train loss:  0.4756973385810852
train gradient:  0.09888102477816949
iteration : 13887
train acc:  0.765625
train loss:  0.47874125838279724
train gradient:  0.12114466403442761
iteration : 13888
train acc:  0.828125
train loss:  0.4039291739463806
train gradient:  0.09304676090763782
iteration : 13889
train acc:  0.6796875
train loss:  0.5788415670394897
train gradient:  0.18060296983461388
iteration : 13890
train acc:  0.7265625
train loss:  0.5608250498771667
train gradient:  0.1520893984283114
iteration : 13891
train acc:  0.734375
train loss:  0.5642620325088501
train gradient:  0.17863812081670502
iteration : 13892
train acc:  0.734375
train loss:  0.4490945339202881
train gradient:  0.09074051878609474
iteration : 13893
train acc:  0.6875
train loss:  0.5818580389022827
train gradient:  0.21169469868044255
iteration : 13894
train acc:  0.765625
train loss:  0.49052995443344116
train gradient:  0.1771799885562858
iteration : 13895
train acc:  0.6640625
train loss:  0.5096291303634644
train gradient:  0.11972159933739303
iteration : 13896
train acc:  0.7890625
train loss:  0.4635891616344452
train gradient:  0.1331279656020466
iteration : 13897
train acc:  0.703125
train loss:  0.5264573693275452
train gradient:  0.11954728484994376
iteration : 13898
train acc:  0.7734375
train loss:  0.46938973665237427
train gradient:  0.10115560162838794
iteration : 13899
train acc:  0.734375
train loss:  0.464589387178421
train gradient:  0.11527783166224716
iteration : 13900
train acc:  0.625
train loss:  0.612865686416626
train gradient:  0.21405951022528052
iteration : 13901
train acc:  0.765625
train loss:  0.4407617747783661
train gradient:  0.12610360852016175
iteration : 13902
train acc:  0.7890625
train loss:  0.44535332918167114
train gradient:  0.10697155995992796
iteration : 13903
train acc:  0.7734375
train loss:  0.46944597363471985
train gradient:  0.13828051920056966
iteration : 13904
train acc:  0.703125
train loss:  0.560897707939148
train gradient:  0.1549968593005985
iteration : 13905
train acc:  0.7265625
train loss:  0.5180749893188477
train gradient:  0.1339247164254479
iteration : 13906
train acc:  0.828125
train loss:  0.44036275148391724
train gradient:  0.09229877633180775
iteration : 13907
train acc:  0.7890625
train loss:  0.4715800881385803
train gradient:  0.1350373011793488
iteration : 13908
train acc:  0.703125
train loss:  0.537733793258667
train gradient:  0.1410110013861085
iteration : 13909
train acc:  0.734375
train loss:  0.47937098145484924
train gradient:  0.11223805998930941
iteration : 13910
train acc:  0.7265625
train loss:  0.47893112897872925
train gradient:  0.12273076804464718
iteration : 13911
train acc:  0.671875
train loss:  0.5635186433792114
train gradient:  0.21366039765537062
iteration : 13912
train acc:  0.7109375
train loss:  0.5612819194793701
train gradient:  0.15839681793574767
iteration : 13913
train acc:  0.75
train loss:  0.4817296266555786
train gradient:  0.1340126909703557
iteration : 13914
train acc:  0.78125
train loss:  0.4089931845664978
train gradient:  0.08603734440325539
iteration : 13915
train acc:  0.796875
train loss:  0.4129224121570587
train gradient:  0.08615135311252221
iteration : 13916
train acc:  0.734375
train loss:  0.49231255054473877
train gradient:  0.1276317482388657
iteration : 13917
train acc:  0.75
train loss:  0.503636360168457
train gradient:  0.14421558347862168
iteration : 13918
train acc:  0.7578125
train loss:  0.4973553419113159
train gradient:  0.14536724033441384
iteration : 13919
train acc:  0.7578125
train loss:  0.46656274795532227
train gradient:  0.10473216606674728
iteration : 13920
train acc:  0.796875
train loss:  0.43991100788116455
train gradient:  0.0902783417571714
iteration : 13921
train acc:  0.7578125
train loss:  0.5268107056617737
train gradient:  0.16203681370526016
iteration : 13922
train acc:  0.7890625
train loss:  0.4531856179237366
train gradient:  0.13552133361040605
iteration : 13923
train acc:  0.7109375
train loss:  0.5033320784568787
train gradient:  0.14750183214330992
iteration : 13924
train acc:  0.7109375
train loss:  0.5288490056991577
train gradient:  0.11841553832819907
iteration : 13925
train acc:  0.7578125
train loss:  0.47012197971343994
train gradient:  0.10890015290704001
iteration : 13926
train acc:  0.7265625
train loss:  0.5523384809494019
train gradient:  0.15993394482266005
iteration : 13927
train acc:  0.7734375
train loss:  0.438981294631958
train gradient:  0.07511589930414203
iteration : 13928
train acc:  0.7421875
train loss:  0.4843403697013855
train gradient:  0.11676485914516918
iteration : 13929
train acc:  0.78125
train loss:  0.4669259786605835
train gradient:  0.14694721772650748
iteration : 13930
train acc:  0.703125
train loss:  0.4798242747783661
train gradient:  0.10190079236497473
iteration : 13931
train acc:  0.75
train loss:  0.4715683162212372
train gradient:  0.10953792959608491
iteration : 13932
train acc:  0.7421875
train loss:  0.48576945066452026
train gradient:  0.11663590833020505
iteration : 13933
train acc:  0.6484375
train loss:  0.5729210376739502
train gradient:  0.1671378073292455
iteration : 13934
train acc:  0.75
train loss:  0.500749945640564
train gradient:  0.11068381597285808
iteration : 13935
train acc:  0.71875
train loss:  0.4889477491378784
train gradient:  0.1158226101936231
iteration : 13936
train acc:  0.7421875
train loss:  0.499176561832428
train gradient:  0.11980887356652183
iteration : 13937
train acc:  0.75
train loss:  0.5168590545654297
train gradient:  0.1620019610400961
iteration : 13938
train acc:  0.7265625
train loss:  0.48539865016937256
train gradient:  0.1284753419372428
iteration : 13939
train acc:  0.765625
train loss:  0.4367856979370117
train gradient:  0.11459122771220782
iteration : 13940
train acc:  0.7421875
train loss:  0.5223554372787476
train gradient:  0.13252785089703384
iteration : 13941
train acc:  0.734375
train loss:  0.4934484362602234
train gradient:  0.10770377405132694
iteration : 13942
train acc:  0.7734375
train loss:  0.4577571153640747
train gradient:  0.14254840123237006
iteration : 13943
train acc:  0.734375
train loss:  0.4825870990753174
train gradient:  0.1250256878660235
iteration : 13944
train acc:  0.671875
train loss:  0.565073549747467
train gradient:  0.18381445472600183
iteration : 13945
train acc:  0.71875
train loss:  0.46990299224853516
train gradient:  0.10049865543191176
iteration : 13946
train acc:  0.71875
train loss:  0.5486498475074768
train gradient:  0.21142124884804656
iteration : 13947
train acc:  0.703125
train loss:  0.5330404043197632
train gradient:  0.13431942751555626
iteration : 13948
train acc:  0.84375
train loss:  0.41848090291023254
train gradient:  0.11684643964640967
iteration : 13949
train acc:  0.7890625
train loss:  0.40734976530075073
train gradient:  0.0863403429243296
iteration : 13950
train acc:  0.7265625
train loss:  0.501447319984436
train gradient:  0.12485203467141812
iteration : 13951
train acc:  0.7265625
train loss:  0.5394018292427063
train gradient:  0.12794980314140747
iteration : 13952
train acc:  0.7109375
train loss:  0.5443340539932251
train gradient:  0.1890893682074581
iteration : 13953
train acc:  0.703125
train loss:  0.5433762073516846
train gradient:  0.1466748801432361
iteration : 13954
train acc:  0.7109375
train loss:  0.5637146234512329
train gradient:  0.19067405159235545
iteration : 13955
train acc:  0.7421875
train loss:  0.5197065472602844
train gradient:  0.12930965741416672
iteration : 13956
train acc:  0.734375
train loss:  0.4687081575393677
train gradient:  0.10879120022457722
iteration : 13957
train acc:  0.796875
train loss:  0.4799456000328064
train gradient:  0.13408834345611656
iteration : 13958
train acc:  0.78125
train loss:  0.48168331384658813
train gradient:  0.139877143475513
iteration : 13959
train acc:  0.703125
train loss:  0.526470959186554
train gradient:  0.1604233161433843
iteration : 13960
train acc:  0.7578125
train loss:  0.49833548069000244
train gradient:  0.14510667021372675
iteration : 13961
train acc:  0.765625
train loss:  0.47561222314834595
train gradient:  0.10779042699035322
iteration : 13962
train acc:  0.78125
train loss:  0.4971707761287689
train gradient:  0.1309217228191589
iteration : 13963
train acc:  0.7890625
train loss:  0.4890516698360443
train gradient:  0.12833970312961396
iteration : 13964
train acc:  0.7578125
train loss:  0.48373693227767944
train gradient:  0.1272860235302873
iteration : 13965
train acc:  0.75
train loss:  0.47953855991363525
train gradient:  0.11736698136719802
iteration : 13966
train acc:  0.7734375
train loss:  0.43438783288002014
train gradient:  0.11635202031841858
iteration : 13967
train acc:  0.7578125
train loss:  0.47321033477783203
train gradient:  0.10618806556253951
iteration : 13968
train acc:  0.734375
train loss:  0.5310437679290771
train gradient:  0.12345168242974312
iteration : 13969
train acc:  0.6953125
train loss:  0.5192404985427856
train gradient:  0.12901058324434206
iteration : 13970
train acc:  0.75
train loss:  0.5269426107406616
train gradient:  0.1341774257353581
iteration : 13971
train acc:  0.734375
train loss:  0.5003975629806519
train gradient:  0.15951258087893133
iteration : 13972
train acc:  0.796875
train loss:  0.486491322517395
train gradient:  0.11632240655299328
iteration : 13973
train acc:  0.78125
train loss:  0.4181627333164215
train gradient:  0.08681881548500596
iteration : 13974
train acc:  0.796875
train loss:  0.4405137598514557
train gradient:  0.09968851437733418
iteration : 13975
train acc:  0.71875
train loss:  0.5379106402397156
train gradient:  0.1597033685935243
iteration : 13976
train acc:  0.703125
train loss:  0.5417587161064148
train gradient:  0.13448830724609218
iteration : 13977
train acc:  0.78125
train loss:  0.4395729899406433
train gradient:  0.10414068754356058
iteration : 13978
train acc:  0.7109375
train loss:  0.5243057608604431
train gradient:  0.11677220525303791
iteration : 13979
train acc:  0.7109375
train loss:  0.5017086863517761
train gradient:  0.10837219749615502
iteration : 13980
train acc:  0.7890625
train loss:  0.3983294665813446
train gradient:  0.09093662982668424
iteration : 13981
train acc:  0.7265625
train loss:  0.5236672163009644
train gradient:  0.14573254812350334
iteration : 13982
train acc:  0.7421875
train loss:  0.5221289992332458
train gradient:  0.1155784944213065
iteration : 13983
train acc:  0.75
train loss:  0.4375266432762146
train gradient:  0.08920272215244515
iteration : 13984
train acc:  0.75
train loss:  0.44781363010406494
train gradient:  0.09585444756417165
iteration : 13985
train acc:  0.78125
train loss:  0.4439127445220947
train gradient:  0.12354714793667344
iteration : 13986
train acc:  0.765625
train loss:  0.4456339180469513
train gradient:  0.09402889830022756
iteration : 13987
train acc:  0.78125
train loss:  0.42844176292419434
train gradient:  0.1309840485827573
iteration : 13988
train acc:  0.7265625
train loss:  0.4622032940387726
train gradient:  0.09671810818109139
iteration : 13989
train acc:  0.7890625
train loss:  0.4475976228713989
train gradient:  0.11609935581012355
iteration : 13990
train acc:  0.7734375
train loss:  0.5036956071853638
train gradient:  0.1257293330686198
iteration : 13991
train acc:  0.7109375
train loss:  0.5190993547439575
train gradient:  0.11772826034733178
iteration : 13992
train acc:  0.8203125
train loss:  0.44143277406692505
train gradient:  0.1278554581124703
iteration : 13993
train acc:  0.7734375
train loss:  0.4747137129306793
train gradient:  0.13303782722010438
iteration : 13994
train acc:  0.734375
train loss:  0.514338493347168
train gradient:  0.13344995989995023
iteration : 13995
train acc:  0.7890625
train loss:  0.39718443155288696
train gradient:  0.08108622246760426
iteration : 13996
train acc:  0.8203125
train loss:  0.44371896982192993
train gradient:  0.10285883335847318
iteration : 13997
train acc:  0.734375
train loss:  0.4840754568576813
train gradient:  0.1507737683004551
iteration : 13998
train acc:  0.7578125
train loss:  0.554311990737915
train gradient:  0.15631215927029726
iteration : 13999
train acc:  0.7421875
train loss:  0.4956555962562561
train gradient:  0.13704379302774744
iteration : 14000
train acc:  0.703125
train loss:  0.5838252305984497
train gradient:  0.1853929529701509
iteration : 14001
train acc:  0.7890625
train loss:  0.4429919421672821
train gradient:  0.11216730744409084
iteration : 14002
train acc:  0.78125
train loss:  0.5351248979568481
train gradient:  0.14137719761966994
iteration : 14003
train acc:  0.7578125
train loss:  0.44293510913848877
train gradient:  0.10796037923737538
iteration : 14004
train acc:  0.7109375
train loss:  0.5698884129524231
train gradient:  0.16876568809483555
iteration : 14005
train acc:  0.765625
train loss:  0.5086009502410889
train gradient:  0.1792525254941623
iteration : 14006
train acc:  0.7890625
train loss:  0.47972771525382996
train gradient:  0.14284150594976006
iteration : 14007
train acc:  0.78125
train loss:  0.431144654750824
train gradient:  0.10757207517373332
iteration : 14008
train acc:  0.8046875
train loss:  0.4849504828453064
train gradient:  0.13916263930277536
iteration : 14009
train acc:  0.828125
train loss:  0.38112691044807434
train gradient:  0.08062703552939969
iteration : 14010
train acc:  0.7578125
train loss:  0.48582911491394043
train gradient:  0.11320547687354916
iteration : 14011
train acc:  0.7109375
train loss:  0.4990062117576599
train gradient:  0.16778306973394227
iteration : 14012
train acc:  0.703125
train loss:  0.5278964638710022
train gradient:  0.15094298129712225
iteration : 14013
train acc:  0.7265625
train loss:  0.5335509777069092
train gradient:  0.1582651387579383
iteration : 14014
train acc:  0.75
train loss:  0.49646472930908203
train gradient:  0.09504570150432473
iteration : 14015
train acc:  0.75
train loss:  0.44213616847991943
train gradient:  0.10415625933200705
iteration : 14016
train acc:  0.765625
train loss:  0.4738433361053467
train gradient:  0.12524802188746517
iteration : 14017
train acc:  0.6796875
train loss:  0.567030668258667
train gradient:  0.21327627088320744
iteration : 14018
train acc:  0.7578125
train loss:  0.4620153307914734
train gradient:  0.11552080189022686
iteration : 14019
train acc:  0.7734375
train loss:  0.4498007893562317
train gradient:  0.10116446262525346
iteration : 14020
train acc:  0.75
train loss:  0.4978506863117218
train gradient:  0.1238668343433149
iteration : 14021
train acc:  0.7421875
train loss:  0.5099453926086426
train gradient:  0.13565064812145547
iteration : 14022
train acc:  0.75
train loss:  0.5452324748039246
train gradient:  0.16508937924908035
iteration : 14023
train acc:  0.7734375
train loss:  0.5067710876464844
train gradient:  0.13718936052879838
iteration : 14024
train acc:  0.765625
train loss:  0.4707198441028595
train gradient:  0.15611414984691369
iteration : 14025
train acc:  0.828125
train loss:  0.3843688666820526
train gradient:  0.0863762807073564
iteration : 14026
train acc:  0.75
train loss:  0.4488309323787689
train gradient:  0.13103538033277667
iteration : 14027
train acc:  0.765625
train loss:  0.44972777366638184
train gradient:  0.10716839821867852
iteration : 14028
train acc:  0.796875
train loss:  0.42293626070022583
train gradient:  0.09358483698560403
iteration : 14029
train acc:  0.796875
train loss:  0.4474821090698242
train gradient:  0.10072923788062908
iteration : 14030
train acc:  0.75
train loss:  0.4651831388473511
train gradient:  0.10313686266145541
iteration : 14031
train acc:  0.7578125
train loss:  0.4612194299697876
train gradient:  0.10813278088033225
iteration : 14032
train acc:  0.7421875
train loss:  0.4880062937736511
train gradient:  0.13435083905583334
iteration : 14033
train acc:  0.765625
train loss:  0.4957817792892456
train gradient:  0.1377412192755341
iteration : 14034
train acc:  0.765625
train loss:  0.4385072588920593
train gradient:  0.10118212622463531
iteration : 14035
train acc:  0.7734375
train loss:  0.47304680943489075
train gradient:  0.10260672229059131
iteration : 14036
train acc:  0.734375
train loss:  0.5180153846740723
train gradient:  0.14453682824837988
iteration : 14037
train acc:  0.734375
train loss:  0.4768473207950592
train gradient:  0.11430672678406412
iteration : 14038
train acc:  0.7109375
train loss:  0.5323809385299683
train gradient:  0.16935888643904645
iteration : 14039
train acc:  0.7109375
train loss:  0.534453272819519
train gradient:  0.149238682927204
iteration : 14040
train acc:  0.6484375
train loss:  0.509789764881134
train gradient:  0.12850119437988428
iteration : 14041
train acc:  0.7734375
train loss:  0.44143325090408325
train gradient:  0.0940741075189164
iteration : 14042
train acc:  0.7265625
train loss:  0.49301767349243164
train gradient:  0.12225380489632873
iteration : 14043
train acc:  0.6875
train loss:  0.5267723798751831
train gradient:  0.10822569301621779
iteration : 14044
train acc:  0.7109375
train loss:  0.5403759479522705
train gradient:  0.13228969416970898
iteration : 14045
train acc:  0.75
train loss:  0.504530131816864
train gradient:  0.1340770720565294
iteration : 14046
train acc:  0.7109375
train loss:  0.5289153456687927
train gradient:  0.12857823239832422
iteration : 14047
train acc:  0.734375
train loss:  0.505233108997345
train gradient:  0.19998260124367057
iteration : 14048
train acc:  0.7421875
train loss:  0.5464837551116943
train gradient:  0.13018551534064202
iteration : 14049
train acc:  0.65625
train loss:  0.5672114491462708
train gradient:  0.15871436263830574
iteration : 14050
train acc:  0.7578125
train loss:  0.4829193353652954
train gradient:  0.14797083304668796
iteration : 14051
train acc:  0.7890625
train loss:  0.5389971733093262
train gradient:  0.14705698312035043
iteration : 14052
train acc:  0.6953125
train loss:  0.576225757598877
train gradient:  0.14290893962593854
iteration : 14053
train acc:  0.7890625
train loss:  0.46910613775253296
train gradient:  0.13355684359147046
iteration : 14054
train acc:  0.7734375
train loss:  0.5043898224830627
train gradient:  0.12074509013933789
iteration : 14055
train acc:  0.75
train loss:  0.4788721203804016
train gradient:  0.09467251808796763
iteration : 14056
train acc:  0.7734375
train loss:  0.49475938081741333
train gradient:  0.12333270583735023
iteration : 14057
train acc:  0.7578125
train loss:  0.44749346375465393
train gradient:  0.11862317951123556
iteration : 14058
train acc:  0.7265625
train loss:  0.5079638957977295
train gradient:  0.12732682433092232
iteration : 14059
train acc:  0.78125
train loss:  0.4643639326095581
train gradient:  0.13470038926407635
iteration : 14060
train acc:  0.7265625
train loss:  0.4726514220237732
train gradient:  0.14486510414295273
iteration : 14061
train acc:  0.71875
train loss:  0.4970519542694092
train gradient:  0.11748540570227216
iteration : 14062
train acc:  0.7578125
train loss:  0.48397982120513916
train gradient:  0.13522020956420414
iteration : 14063
train acc:  0.7109375
train loss:  0.5314767360687256
train gradient:  0.12808587618878547
iteration : 14064
train acc:  0.7421875
train loss:  0.45359373092651367
train gradient:  0.09469824790379432
iteration : 14065
train acc:  0.765625
train loss:  0.4756219983100891
train gradient:  0.09433112441954253
iteration : 14066
train acc:  0.796875
train loss:  0.43992722034454346
train gradient:  0.09523514848955127
iteration : 14067
train acc:  0.6796875
train loss:  0.5355485081672668
train gradient:  0.1675545452472587
iteration : 14068
train acc:  0.734375
train loss:  0.5337020754814148
train gradient:  0.14575154341074015
iteration : 14069
train acc:  0.7734375
train loss:  0.4608469009399414
train gradient:  0.13949836721891284
iteration : 14070
train acc:  0.7421875
train loss:  0.4768022298812866
train gradient:  0.11457055530404303
iteration : 14071
train acc:  0.78125
train loss:  0.4688941240310669
train gradient:  0.12397119691733706
iteration : 14072
train acc:  0.6796875
train loss:  0.5325666069984436
train gradient:  0.13135372162228875
iteration : 14073
train acc:  0.7578125
train loss:  0.4663034677505493
train gradient:  0.10276099565119516
iteration : 14074
train acc:  0.734375
train loss:  0.5993421077728271
train gradient:  0.18288307386417874
iteration : 14075
train acc:  0.703125
train loss:  0.5469053983688354
train gradient:  0.13846035778292537
iteration : 14076
train acc:  0.71875
train loss:  0.5082019567489624
train gradient:  0.14680058242354443
iteration : 14077
train acc:  0.7109375
train loss:  0.4767705202102661
train gradient:  0.10693073331696382
iteration : 14078
train acc:  0.765625
train loss:  0.4981500804424286
train gradient:  0.12916700738117776
iteration : 14079
train acc:  0.765625
train loss:  0.44487929344177246
train gradient:  0.11113643340874221
iteration : 14080
train acc:  0.7734375
train loss:  0.45664867758750916
train gradient:  0.09394569339933992
iteration : 14081
train acc:  0.6953125
train loss:  0.5497890710830688
train gradient:  0.1636723685458949
iteration : 14082
train acc:  0.734375
train loss:  0.5416351556777954
train gradient:  0.14235764134936496
iteration : 14083
train acc:  0.765625
train loss:  0.45141738653182983
train gradient:  0.09421999012980797
iteration : 14084
train acc:  0.7890625
train loss:  0.423493891954422
train gradient:  0.07694442828751065
iteration : 14085
train acc:  0.734375
train loss:  0.46980398893356323
train gradient:  0.1141587851558408
iteration : 14086
train acc:  0.7265625
train loss:  0.5021877884864807
train gradient:  0.13021087365119374
iteration : 14087
train acc:  0.7265625
train loss:  0.5127724409103394
train gradient:  0.11385861931623406
iteration : 14088
train acc:  0.78125
train loss:  0.4575842618942261
train gradient:  0.1268314157593583
iteration : 14089
train acc:  0.703125
train loss:  0.5831061601638794
train gradient:  0.1490086681342339
iteration : 14090
train acc:  0.8125
train loss:  0.41202741861343384
train gradient:  0.09128980821632937
iteration : 14091
train acc:  0.7578125
train loss:  0.4463256597518921
train gradient:  0.12815938729041962
iteration : 14092
train acc:  0.71875
train loss:  0.4835547208786011
train gradient:  0.10389744460479175
iteration : 14093
train acc:  0.703125
train loss:  0.5289600491523743
train gradient:  0.11378265308153895
iteration : 14094
train acc:  0.7890625
train loss:  0.43350982666015625
train gradient:  0.10112726240750408
iteration : 14095
train acc:  0.78125
train loss:  0.48009079694747925
train gradient:  0.0972114548189
iteration : 14096
train acc:  0.6953125
train loss:  0.5602354407310486
train gradient:  0.1462537380329995
iteration : 14097
train acc:  0.7265625
train loss:  0.5149149894714355
train gradient:  0.14930065725502395
iteration : 14098
train acc:  0.7578125
train loss:  0.48726072907447815
train gradient:  0.11640541271912624
iteration : 14099
train acc:  0.6875
train loss:  0.522486686706543
train gradient:  0.126070538731536
iteration : 14100
train acc:  0.7109375
train loss:  0.5238463878631592
train gradient:  0.1598769701368818
iteration : 14101
train acc:  0.796875
train loss:  0.4476046562194824
train gradient:  0.11053697563192581
iteration : 14102
train acc:  0.765625
train loss:  0.472096711397171
train gradient:  0.11704529028779183
iteration : 14103
train acc:  0.7578125
train loss:  0.46666988730430603
train gradient:  0.110543553917072
iteration : 14104
train acc:  0.71875
train loss:  0.4896485209465027
train gradient:  0.10125378130958886
iteration : 14105
train acc:  0.71875
train loss:  0.5033961534500122
train gradient:  0.13022074448267218
iteration : 14106
train acc:  0.75
train loss:  0.47483029961586
train gradient:  0.11990643382714714
iteration : 14107
train acc:  0.7890625
train loss:  0.43607261776924133
train gradient:  0.10033671085967978
iteration : 14108
train acc:  0.765625
train loss:  0.4782450199127197
train gradient:  0.14326451295410764
iteration : 14109
train acc:  0.7578125
train loss:  0.4490429759025574
train gradient:  0.09302649313378858
iteration : 14110
train acc:  0.7734375
train loss:  0.47381141781806946
train gradient:  0.12803192227440124
iteration : 14111
train acc:  0.71875
train loss:  0.5152451395988464
train gradient:  0.1448317480366846
iteration : 14112
train acc:  0.765625
train loss:  0.4977817237377167
train gradient:  0.12228892902948478
iteration : 14113
train acc:  0.703125
train loss:  0.49710679054260254
train gradient:  0.12890629029529543
iteration : 14114
train acc:  0.734375
train loss:  0.5016588568687439
train gradient:  0.14126583258246306
iteration : 14115
train acc:  0.734375
train loss:  0.5377203822135925
train gradient:  0.12023032047091463
iteration : 14116
train acc:  0.78125
train loss:  0.44369998574256897
train gradient:  0.12649484238065017
iteration : 14117
train acc:  0.7890625
train loss:  0.4302765727043152
train gradient:  0.0856824565678196
iteration : 14118
train acc:  0.7421875
train loss:  0.5527129173278809
train gradient:  0.2089339575398241
iteration : 14119
train acc:  0.75
train loss:  0.4526520073413849
train gradient:  0.11260675771062828
iteration : 14120
train acc:  0.7265625
train loss:  0.5116982460021973
train gradient:  0.17358938565052337
iteration : 14121
train acc:  0.71875
train loss:  0.5532522201538086
train gradient:  0.16910312019289114
iteration : 14122
train acc:  0.7109375
train loss:  0.4982019066810608
train gradient:  0.10043487074715995
iteration : 14123
train acc:  0.6953125
train loss:  0.5909668803215027
train gradient:  0.23186750423375865
iteration : 14124
train acc:  0.734375
train loss:  0.49716421961784363
train gradient:  0.10453820025039072
iteration : 14125
train acc:  0.734375
train loss:  0.5160635709762573
train gradient:  0.13151726672695563
iteration : 14126
train acc:  0.734375
train loss:  0.47315171360969543
train gradient:  0.11858380882598132
iteration : 14127
train acc:  0.7265625
train loss:  0.5827350616455078
train gradient:  0.18292191293948085
iteration : 14128
train acc:  0.7734375
train loss:  0.44167065620422363
train gradient:  0.10035474804422538
iteration : 14129
train acc:  0.7578125
train loss:  0.4608515202999115
train gradient:  0.10581324266679629
iteration : 14130
train acc:  0.765625
train loss:  0.44129177927970886
train gradient:  0.10560098905293559
iteration : 14131
train acc:  0.765625
train loss:  0.4962072968482971
train gradient:  0.09920178118811336
iteration : 14132
train acc:  0.6953125
train loss:  0.553433895111084
train gradient:  0.13497104569774207
iteration : 14133
train acc:  0.78125
train loss:  0.4609910845756531
train gradient:  0.10178749016945865
iteration : 14134
train acc:  0.765625
train loss:  0.4724457859992981
train gradient:  0.1504038281836187
iteration : 14135
train acc:  0.796875
train loss:  0.42418158054351807
train gradient:  0.11540302329966873
iteration : 14136
train acc:  0.8203125
train loss:  0.3927995562553406
train gradient:  0.09154283219973125
iteration : 14137
train acc:  0.765625
train loss:  0.42576324939727783
train gradient:  0.09161604077807715
iteration : 14138
train acc:  0.6875
train loss:  0.5759055018424988
train gradient:  0.14513334080590956
iteration : 14139
train acc:  0.8203125
train loss:  0.4185854196548462
train gradient:  0.13088163108616704
iteration : 14140
train acc:  0.7734375
train loss:  0.4582973122596741
train gradient:  0.12204252834772746
iteration : 14141
train acc:  0.7421875
train loss:  0.5191329121589661
train gradient:  0.14360568506131988
iteration : 14142
train acc:  0.8203125
train loss:  0.454007089138031
train gradient:  0.09788473166889643
iteration : 14143
train acc:  0.71875
train loss:  0.5098676681518555
train gradient:  0.14206883290562033
iteration : 14144
train acc:  0.7578125
train loss:  0.47669392824172974
train gradient:  0.16763314165876694
iteration : 14145
train acc:  0.71875
train loss:  0.4680923819541931
train gradient:  0.10330891822798191
iteration : 14146
train acc:  0.75
train loss:  0.4427376985549927
train gradient:  0.08299243347309013
iteration : 14147
train acc:  0.703125
train loss:  0.626798152923584
train gradient:  0.1841055525009704
iteration : 14148
train acc:  0.75
train loss:  0.5166770219802856
train gradient:  0.137035198835947
iteration : 14149
train acc:  0.703125
train loss:  0.516187310218811
train gradient:  0.13237660104486726
iteration : 14150
train acc:  0.71875
train loss:  0.45192867517471313
train gradient:  0.11467508054236794
iteration : 14151
train acc:  0.7109375
train loss:  0.5122291445732117
train gradient:  0.10991416755644344
iteration : 14152
train acc:  0.75
train loss:  0.4797664284706116
train gradient:  0.10336142255060153
iteration : 14153
train acc:  0.78125
train loss:  0.46768319606781006
train gradient:  0.09954167841662227
iteration : 14154
train acc:  0.765625
train loss:  0.45483285188674927
train gradient:  0.11275679360272002
iteration : 14155
train acc:  0.6953125
train loss:  0.5632804036140442
train gradient:  0.13824550958520782
iteration : 14156
train acc:  0.6953125
train loss:  0.5193674564361572
train gradient:  0.12297705191021527
iteration : 14157
train acc:  0.75
train loss:  0.4897756576538086
train gradient:  0.10628939828692119
iteration : 14158
train acc:  0.796875
train loss:  0.42868781089782715
train gradient:  0.08738301961736367
iteration : 14159
train acc:  0.7890625
train loss:  0.42161038517951965
train gradient:  0.10847423138432553
iteration : 14160
train acc:  0.7421875
train loss:  0.4741787314414978
train gradient:  0.11514230255705217
iteration : 14161
train acc:  0.6953125
train loss:  0.5030585527420044
train gradient:  0.13039911578914642
iteration : 14162
train acc:  0.7421875
train loss:  0.4921398460865021
train gradient:  0.09130895168748175
iteration : 14163
train acc:  0.65625
train loss:  0.568504810333252
train gradient:  0.15854535700471825
iteration : 14164
train acc:  0.7109375
train loss:  0.5167891979217529
train gradient:  0.130596938141592
iteration : 14165
train acc:  0.6953125
train loss:  0.5467031598091125
train gradient:  0.13771923580915307
iteration : 14166
train acc:  0.765625
train loss:  0.5015318393707275
train gradient:  0.12564630172124483
iteration : 14167
train acc:  0.765625
train loss:  0.47118666768074036
train gradient:  0.12676703564741254
iteration : 14168
train acc:  0.7578125
train loss:  0.4934295415878296
train gradient:  0.15613879622812044
iteration : 14169
train acc:  0.6875
train loss:  0.5789107084274292
train gradient:  0.15947577126919557
iteration : 14170
train acc:  0.8203125
train loss:  0.4125834107398987
train gradient:  0.09743085114988755
iteration : 14171
train acc:  0.7421875
train loss:  0.5320296883583069
train gradient:  0.1719819300376036
iteration : 14172
train acc:  0.7890625
train loss:  0.43644481897354126
train gradient:  0.09447741835175195
iteration : 14173
train acc:  0.7109375
train loss:  0.5473470687866211
train gradient:  0.14388580887239083
iteration : 14174
train acc:  0.796875
train loss:  0.42994946241378784
train gradient:  0.12197379997951328
iteration : 14175
train acc:  0.75
train loss:  0.4812990128993988
train gradient:  0.12249988452290096
iteration : 14176
train acc:  0.7890625
train loss:  0.4901330769062042
train gradient:  0.09917938382315947
iteration : 14177
train acc:  0.8203125
train loss:  0.44343477487564087
train gradient:  0.10558794160340146
iteration : 14178
train acc:  0.7578125
train loss:  0.4714055061340332
train gradient:  0.11899223246417265
iteration : 14179
train acc:  0.796875
train loss:  0.44352221488952637
train gradient:  0.08060310179362891
iteration : 14180
train acc:  0.8125
train loss:  0.4445522129535675
train gradient:  0.09918812769172863
iteration : 14181
train acc:  0.7578125
train loss:  0.4771972596645355
train gradient:  0.12501683187754759
iteration : 14182
train acc:  0.734375
train loss:  0.5336158275604248
train gradient:  0.11506807967695223
iteration : 14183
train acc:  0.6953125
train loss:  0.530706524848938
train gradient:  0.152527394049557
iteration : 14184
train acc:  0.75
train loss:  0.4604306221008301
train gradient:  0.11352698553335505
iteration : 14185
train acc:  0.75
train loss:  0.5015095472335815
train gradient:  0.12489267133173335
iteration : 14186
train acc:  0.7777777777777778
train loss:  0.6436508297920227
train gradient:  1.716254745446523
val acc:  0.741273704492445
val f1:  0.7576514618994243
val confusion matrix:  [[66433 32177]
 [18849 79761]]

----------------------------------------new_epoch--------------------------------------

epoch:  1
iteration : 0
train acc:  0.6953125
train loss:  0.5269662141799927
train gradient:  0.158283663629598
iteration : 1
train acc:  0.71875
train loss:  0.5652961730957031
train gradient:  0.14233445605464515
iteration : 2
train acc:  0.671875
train loss:  0.5921913981437683
train gradient:  0.17848661739311622
iteration : 3
train acc:  0.8125
train loss:  0.48882243037223816
train gradient:  0.10660780464838969
iteration : 4
train acc:  0.7421875
train loss:  0.5382856130599976
train gradient:  0.18076266455251846
iteration : 5
train acc:  0.7734375
train loss:  0.49116289615631104
train gradient:  0.11913089857185208
iteration : 6
train acc:  0.7578125
train loss:  0.4399556815624237
train gradient:  0.09757813926450595
iteration : 7
train acc:  0.765625
train loss:  0.4415149986743927
train gradient:  0.12032416338722243
iteration : 8
train acc:  0.71875
train loss:  0.4925207197666168
train gradient:  0.12649952459505334
iteration : 9
train acc:  0.7890625
train loss:  0.4440624713897705
train gradient:  0.10779147319024336
iteration : 10
train acc:  0.75
train loss:  0.4925615191459656
train gradient:  0.12652972372033527
iteration : 11
train acc:  0.8125
train loss:  0.44239890575408936
train gradient:  0.11915410034202317
iteration : 12
train acc:  0.7421875
train loss:  0.5053445100784302
train gradient:  0.13236290474349963
iteration : 13
train acc:  0.7421875
train loss:  0.479647159576416
train gradient:  0.12452340641839989
iteration : 14
train acc:  0.7578125
train loss:  0.4455142915248871
train gradient:  0.10863235317724679
iteration : 15
train acc:  0.796875
train loss:  0.4919810891151428
train gradient:  0.11057828158811754
iteration : 16
train acc:  0.828125
train loss:  0.40795546770095825
train gradient:  0.09904630931108903
iteration : 17
train acc:  0.8046875
train loss:  0.4390789866447449
train gradient:  0.10542486979918246
iteration : 18
train acc:  0.75
train loss:  0.40598559379577637
train gradient:  0.09609199299852898
iteration : 19
train acc:  0.796875
train loss:  0.460025429725647
train gradient:  0.1243439735896984
iteration : 20
train acc:  0.7734375
train loss:  0.4372808337211609
train gradient:  0.08930540767610592
iteration : 21
train acc:  0.75
train loss:  0.4943573474884033
train gradient:  0.1155586730555796
iteration : 22
train acc:  0.7421875
train loss:  0.45970988273620605
train gradient:  0.11688534873493917
iteration : 23
train acc:  0.703125
train loss:  0.525786280632019
train gradient:  0.1782027916434923
iteration : 24
train acc:  0.7890625
train loss:  0.43687576055526733
train gradient:  0.11813407429387178
iteration : 25
train acc:  0.7890625
train loss:  0.4074941873550415
train gradient:  0.07794054365681191
iteration : 26
train acc:  0.6796875
train loss:  0.5466824769973755
train gradient:  0.12278413843809147
iteration : 27
train acc:  0.75
train loss:  0.5096170902252197
train gradient:  0.1452583789588055
iteration : 28
train acc:  0.6875
train loss:  0.5052921772003174
train gradient:  0.12378983256541501
iteration : 29
train acc:  0.671875
train loss:  0.5658174157142639
train gradient:  0.18132495887412042
iteration : 30
train acc:  0.7734375
train loss:  0.4529589116573334
train gradient:  0.09542273348700997
iteration : 31
train acc:  0.7734375
train loss:  0.4481009244918823
train gradient:  0.08620667599044052
iteration : 32
train acc:  0.7265625
train loss:  0.5270597338676453
train gradient:  0.12881360559955743
iteration : 33
train acc:  0.734375
train loss:  0.48767727613449097
train gradient:  0.11778317055231029
iteration : 34
train acc:  0.828125
train loss:  0.4130292236804962
train gradient:  0.0942535461580867
iteration : 35
train acc:  0.7109375
train loss:  0.5218851566314697
train gradient:  0.12087729086214427
iteration : 36
train acc:  0.7734375
train loss:  0.43217697739601135
train gradient:  0.09079668731753848
iteration : 37
train acc:  0.796875
train loss:  0.46793293952941895
train gradient:  0.17032446320677197
iteration : 38
train acc:  0.78125
train loss:  0.41012847423553467
train gradient:  0.08878552296219822
iteration : 39
train acc:  0.734375
train loss:  0.49821972846984863
train gradient:  0.11224888342162974
iteration : 40
train acc:  0.7890625
train loss:  0.4250738024711609
train gradient:  0.09113938758079342
iteration : 41
train acc:  0.765625
train loss:  0.454634428024292
train gradient:  0.11231866069024365
iteration : 42
train acc:  0.765625
train loss:  0.4953218102455139
train gradient:  0.11662198485136349
iteration : 43
train acc:  0.7734375
train loss:  0.4545360803604126
train gradient:  0.11155240078104217
iteration : 44
train acc:  0.75
train loss:  0.4774051606655121
train gradient:  0.15997052422002794
iteration : 45
train acc:  0.8046875
train loss:  0.43740904331207275
train gradient:  0.13504942615251428
iteration : 46
train acc:  0.7421875
train loss:  0.4681895673274994
train gradient:  0.10293586342885559
iteration : 47
train acc:  0.6875
train loss:  0.4974040985107422
train gradient:  0.13406074062572088
iteration : 48
train acc:  0.7890625
train loss:  0.4333818852901459
train gradient:  0.10204836985759916
iteration : 49
train acc:  0.6953125
train loss:  0.5271580219268799
train gradient:  0.14744874216141246
iteration : 50
train acc:  0.734375
train loss:  0.528905987739563
train gradient:  0.15304378288362291
iteration : 51
train acc:  0.7421875
train loss:  0.48185011744499207
train gradient:  0.09694235637251848
iteration : 52
train acc:  0.671875
train loss:  0.6088626980781555
train gradient:  0.149912706577125
iteration : 53
train acc:  0.765625
train loss:  0.4403282403945923
train gradient:  0.10427312035107639
iteration : 54
train acc:  0.734375
train loss:  0.47013649344444275
train gradient:  0.11534163239716902
iteration : 55
train acc:  0.78125
train loss:  0.4671328067779541
train gradient:  0.1252705990947538
iteration : 56
train acc:  0.75
train loss:  0.4834005832672119
train gradient:  0.10853345585993547
iteration : 57
train acc:  0.7265625
train loss:  0.5685316920280457
train gradient:  0.1318437783460134
iteration : 58
train acc:  0.7421875
train loss:  0.48950624465942383
train gradient:  0.1902622457875889
iteration : 59
train acc:  0.78125
train loss:  0.39919161796569824
train gradient:  0.11030328322751931
iteration : 60
train acc:  0.7578125
train loss:  0.4464331567287445
train gradient:  0.09857966879266017
iteration : 61
train acc:  0.734375
train loss:  0.5155675411224365
train gradient:  0.10808306838461004
iteration : 62
train acc:  0.6875
train loss:  0.5491421818733215
train gradient:  0.1348051568687768
iteration : 63
train acc:  0.7578125
train loss:  0.5108411908149719
train gradient:  0.11353841977368342
iteration : 64
train acc:  0.734375
train loss:  0.5041351318359375
train gradient:  0.11796388412784259
iteration : 65
train acc:  0.6796875
train loss:  0.49565738439559937
train gradient:  0.1273975678830308
iteration : 66
train acc:  0.71875
train loss:  0.5185579061508179
train gradient:  0.14565392870799532
iteration : 67
train acc:  0.7890625
train loss:  0.46782127022743225
train gradient:  0.11355309027769393
iteration : 68
train acc:  0.75
train loss:  0.4781951904296875
train gradient:  0.11875222507638628
iteration : 69
train acc:  0.734375
train loss:  0.5118603706359863
train gradient:  0.10751509194932436
iteration : 70
train acc:  0.78125
train loss:  0.4700685739517212
train gradient:  0.1503610500137833
iteration : 71
train acc:  0.703125
train loss:  0.5909645557403564
train gradient:  0.1809168838810355
iteration : 72
train acc:  0.75
train loss:  0.5145329236984253
train gradient:  0.1412182016453044
iteration : 73
train acc:  0.71875
train loss:  0.5228562355041504
train gradient:  0.14674989222545298
iteration : 74
train acc:  0.7421875
train loss:  0.5107229948043823
train gradient:  0.16189709952799197
iteration : 75
train acc:  0.8046875
train loss:  0.3914668560028076
train gradient:  0.08319753331123056
iteration : 76
train acc:  0.7734375
train loss:  0.4598222076892853
train gradient:  0.12023528432162012
iteration : 77
train acc:  0.78125
train loss:  0.45395442843437195
train gradient:  0.11479575466651637
iteration : 78
train acc:  0.7265625
train loss:  0.5241696238517761
train gradient:  0.1628274685191756
iteration : 79
train acc:  0.7421875
train loss:  0.44829148054122925
train gradient:  0.10581104340091459
iteration : 80
train acc:  0.765625
train loss:  0.4781474173069
train gradient:  0.09751151014857408
iteration : 81
train acc:  0.75
train loss:  0.48461881279945374
train gradient:  0.14600581063706325
iteration : 82
train acc:  0.75
train loss:  0.5023717880249023
train gradient:  0.10979118640551953
iteration : 83
train acc:  0.734375
train loss:  0.43436747789382935
train gradient:  0.0950328467672289
iteration : 84
train acc:  0.828125
train loss:  0.41991472244262695
train gradient:  0.10699074521930098
iteration : 85
train acc:  0.7890625
train loss:  0.5214464664459229
train gradient:  0.13654027638376345
iteration : 86
train acc:  0.828125
train loss:  0.415303111076355
train gradient:  0.10310806424830364
iteration : 87
train acc:  0.75
train loss:  0.4718056321144104
train gradient:  0.12857840346732125
iteration : 88
train acc:  0.71875
train loss:  0.4772612452507019
train gradient:  0.12620373857874406
iteration : 89
train acc:  0.7734375
train loss:  0.48200544714927673
train gradient:  0.1228278060320278
iteration : 90
train acc:  0.75
train loss:  0.4841073155403137
train gradient:  0.11913956511127283
iteration : 91
train acc:  0.765625
train loss:  0.43344905972480774
train gradient:  0.09823057332887898
iteration : 92
train acc:  0.7578125
train loss:  0.47266411781311035
train gradient:  0.1519710831994534
iteration : 93
train acc:  0.7109375
train loss:  0.4948917627334595
train gradient:  0.12669432734332756
iteration : 94
train acc:  0.7578125
train loss:  0.42792898416519165
train gradient:  0.0992037636543294
iteration : 95
train acc:  0.765625
train loss:  0.45898574590682983
train gradient:  0.11701719157647111
iteration : 96
train acc:  0.7109375
train loss:  0.4873600900173187
train gradient:  0.11987555100084557
iteration : 97
train acc:  0.8125
train loss:  0.4744649827480316
train gradient:  0.11160792755003594
iteration : 98
train acc:  0.7578125
train loss:  0.5066413879394531
train gradient:  0.11003716244895564
iteration : 99
train acc:  0.7578125
train loss:  0.4668932855129242
train gradient:  0.10813730485480935
iteration : 100
train acc:  0.7421875
train loss:  0.5001450181007385
train gradient:  0.15293816115582937
iteration : 101
train acc:  0.734375
train loss:  0.4802457094192505
train gradient:  0.121159175999138
iteration : 102
train acc:  0.6875
train loss:  0.5445852279663086
train gradient:  0.1760460121381107
iteration : 103
train acc:  0.6796875
train loss:  0.5826788544654846
train gradient:  0.18559021904421152
iteration : 104
train acc:  0.6796875
train loss:  0.5963221788406372
train gradient:  0.15334762600328666
iteration : 105
train acc:  0.796875
train loss:  0.42404893040657043
train gradient:  0.10896743633005324
iteration : 106
train acc:  0.75
train loss:  0.5247904658317566
train gradient:  0.16547134529884536
iteration : 107
train acc:  0.765625
train loss:  0.4070417582988739
train gradient:  0.09610199832792804
iteration : 108
train acc:  0.7421875
train loss:  0.48719269037246704
train gradient:  0.15119590587148674
iteration : 109
train acc:  0.8046875
train loss:  0.49378466606140137
train gradient:  0.1281274797372362
iteration : 110
train acc:  0.8046875
train loss:  0.42071330547332764
train gradient:  0.1049303140462774
iteration : 111
train acc:  0.7734375
train loss:  0.4372633397579193
train gradient:  0.12300050794481245
iteration : 112
train acc:  0.78125
train loss:  0.48681023716926575
train gradient:  0.13925883530316205
iteration : 113
train acc:  0.7109375
train loss:  0.5740713477134705
train gradient:  0.15315017077387622
iteration : 114
train acc:  0.7421875
train loss:  0.5487865209579468
train gradient:  0.19691915349402744
iteration : 115
train acc:  0.78125
train loss:  0.44985198974609375
train gradient:  0.1327742376748592
iteration : 116
train acc:  0.7109375
train loss:  0.4918660819530487
train gradient:  0.13853776479138727
iteration : 117
train acc:  0.765625
train loss:  0.51180100440979
train gradient:  0.1512611525487895
iteration : 118
train acc:  0.7734375
train loss:  0.44124680757522583
train gradient:  0.09595005981178545
iteration : 119
train acc:  0.6875
train loss:  0.5378058552742004
train gradient:  0.1574095849536603
iteration : 120
train acc:  0.796875
train loss:  0.4398993253707886
train gradient:  0.10810687393516527
iteration : 121
train acc:  0.671875
train loss:  0.5255914330482483
train gradient:  0.1727836000257616
iteration : 122
train acc:  0.7421875
train loss:  0.4690309762954712
train gradient:  0.15503648086353525
iteration : 123
train acc:  0.7421875
train loss:  0.46329742670059204
train gradient:  0.09963379774048274
iteration : 124
train acc:  0.7421875
train loss:  0.4721827507019043
train gradient:  0.10453570216336364
iteration : 125
train acc:  0.7265625
train loss:  0.49797433614730835
train gradient:  0.12025483493375656
iteration : 126
train acc:  0.75
train loss:  0.4661935567855835
train gradient:  0.12184094460168728
iteration : 127
train acc:  0.6484375
train loss:  0.5987440347671509
train gradient:  0.17481314245314383
iteration : 128
train acc:  0.78125
train loss:  0.41729649901390076
train gradient:  0.08551614877159593
iteration : 129
train acc:  0.7734375
train loss:  0.48830297589302063
train gradient:  0.10784749505062617
iteration : 130
train acc:  0.8046875
train loss:  0.36297303438186646
train gradient:  0.07108163737124214
iteration : 131
train acc:  0.6796875
train loss:  0.5025129318237305
train gradient:  0.12540250767027908
iteration : 132
train acc:  0.75
train loss:  0.45370224118232727
train gradient:  0.10185519010615633
iteration : 133
train acc:  0.8125
train loss:  0.47838011384010315
train gradient:  0.1394937145601729
iteration : 134
train acc:  0.7890625
train loss:  0.4310968518257141
train gradient:  0.10965476519799207
iteration : 135
train acc:  0.6953125
train loss:  0.5173705816268921
train gradient:  0.11521063781812889
iteration : 136
train acc:  0.7265625
train loss:  0.48321399092674255
train gradient:  0.13840433544280067
iteration : 137
train acc:  0.75
train loss:  0.4930500388145447
train gradient:  0.17300336906573194
iteration : 138
train acc:  0.796875
train loss:  0.42527657747268677
train gradient:  0.09601796892690624
iteration : 139
train acc:  0.734375
train loss:  0.47662803530693054
train gradient:  0.1263179429772029
iteration : 140
train acc:  0.7421875
train loss:  0.4850686192512512
train gradient:  0.0991193961886449
iteration : 141
train acc:  0.7265625
train loss:  0.5376889109611511
train gradient:  0.1527049576127641
iteration : 142
train acc:  0.703125
train loss:  0.5880664587020874
train gradient:  0.19887668676214176
iteration : 143
train acc:  0.7421875
train loss:  0.5428212285041809
train gradient:  0.12890320915598366
iteration : 144
train acc:  0.7734375
train loss:  0.43787533044815063
train gradient:  0.11863867243150768
iteration : 145
train acc:  0.734375
train loss:  0.47685927152633667
train gradient:  0.144210670307528
iteration : 146
train acc:  0.7578125
train loss:  0.5013500452041626
train gradient:  0.12303396519530334
iteration : 147
train acc:  0.71875
train loss:  0.5417832136154175
train gradient:  0.1705671116612733
iteration : 148
train acc:  0.7734375
train loss:  0.5052950978279114
train gradient:  0.13160841314175348
iteration : 149
train acc:  0.6953125
train loss:  0.5145481824874878
train gradient:  0.1740380759727034
iteration : 150
train acc:  0.7265625
train loss:  0.5423508882522583
train gradient:  0.16977672152237566
iteration : 151
train acc:  0.796875
train loss:  0.4721396565437317
train gradient:  0.14124658286622854
iteration : 152
train acc:  0.7109375
train loss:  0.5053087472915649
train gradient:  0.12273322005571108
iteration : 153
train acc:  0.78125
train loss:  0.48713284730911255
train gradient:  0.1432875751627104
iteration : 154
train acc:  0.7734375
train loss:  0.5033423900604248
train gradient:  0.1271808417612309
iteration : 155
train acc:  0.7734375
train loss:  0.4748023450374603
train gradient:  0.11561294299387247
iteration : 156
train acc:  0.7890625
train loss:  0.4542296528816223
train gradient:  0.1293623609508926
iteration : 157
train acc:  0.7734375
train loss:  0.4697501063346863
train gradient:  0.17832135501527188
iteration : 158
train acc:  0.7734375
train loss:  0.46374064683914185
train gradient:  0.11485335158167993
iteration : 159
train acc:  0.7890625
train loss:  0.41978245973587036
train gradient:  0.12483904036064727
iteration : 160
train acc:  0.7421875
train loss:  0.5330681204795837
train gradient:  0.16377804258251966
iteration : 161
train acc:  0.734375
train loss:  0.5002444982528687
train gradient:  0.12463219509554244
iteration : 162
train acc:  0.765625
train loss:  0.44942158460617065
train gradient:  0.10309681573318699
iteration : 163
train acc:  0.796875
train loss:  0.42321938276290894
train gradient:  0.12699404776005463
iteration : 164
train acc:  0.71875
train loss:  0.4912256598472595
train gradient:  0.1313866342960905
iteration : 165
train acc:  0.8125
train loss:  0.4408421218395233
train gradient:  0.09267039165640054
iteration : 166
train acc:  0.765625
train loss:  0.4692422151565552
train gradient:  0.12994945811317016
iteration : 167
train acc:  0.7421875
train loss:  0.5145653486251831
train gradient:  0.15943533124634685
iteration : 168
train acc:  0.78125
train loss:  0.4509701132774353
train gradient:  0.1352629798603575
iteration : 169
train acc:  0.7734375
train loss:  0.4415421187877655
train gradient:  0.1015013904048468
iteration : 170
train acc:  0.6640625
train loss:  0.5871585607528687
train gradient:  0.18920215607509194
iteration : 171
train acc:  0.7421875
train loss:  0.49024584889411926
train gradient:  0.1685123878032858
iteration : 172
train acc:  0.7421875
train loss:  0.547805905342102
train gradient:  0.17187601623978563
iteration : 173
train acc:  0.796875
train loss:  0.4541565477848053
train gradient:  0.12868455062302764
iteration : 174
train acc:  0.6875
train loss:  0.5388087034225464
train gradient:  0.17912115279327245
iteration : 175
train acc:  0.765625
train loss:  0.5174412727355957
train gradient:  0.14077225883361544
iteration : 176
train acc:  0.75
train loss:  0.5312525033950806
train gradient:  0.1606295000281136
iteration : 177
train acc:  0.7421875
train loss:  0.4440426230430603
train gradient:  0.12371232475799104
iteration : 178
train acc:  0.7890625
train loss:  0.45734918117523193
train gradient:  0.1441246277719731
iteration : 179
train acc:  0.78125
train loss:  0.47181880474090576
train gradient:  0.1342881461483038
iteration : 180
train acc:  0.7421875
train loss:  0.49945634603500366
train gradient:  0.11888680480879905
iteration : 181
train acc:  0.7265625
train loss:  0.49877607822418213
train gradient:  0.12998302333389014
iteration : 182
train acc:  0.7890625
train loss:  0.4177892804145813
train gradient:  0.0852551223115569
iteration : 183
train acc:  0.8203125
train loss:  0.4750292897224426
train gradient:  0.11500650339482771
iteration : 184
train acc:  0.7734375
train loss:  0.44771257042884827
train gradient:  0.1041824432711555
iteration : 185
train acc:  0.6328125
train loss:  0.5894664525985718
train gradient:  0.19293107872266047
iteration : 186
train acc:  0.765625
train loss:  0.5101101398468018
train gradient:  0.10788910825506214
iteration : 187
train acc:  0.75
train loss:  0.48066362738609314
train gradient:  0.09781129991546006
iteration : 188
train acc:  0.71875
train loss:  0.6008073091506958
train gradient:  0.15359992624347113
iteration : 189
train acc:  0.7734375
train loss:  0.4446312487125397
train gradient:  0.10756793102866535
iteration : 190
train acc:  0.75
train loss:  0.5388814210891724
train gradient:  0.1474410408203648
iteration : 191
train acc:  0.7109375
train loss:  0.5190975666046143
train gradient:  0.14576135251292155
iteration : 192
train acc:  0.7734375
train loss:  0.45033663511276245
train gradient:  0.10223724414694725
iteration : 193
train acc:  0.7421875
train loss:  0.4862772822380066
train gradient:  0.11813715161801355
iteration : 194
train acc:  0.7109375
train loss:  0.5575066804885864
train gradient:  0.1808956469919062
iteration : 195
train acc:  0.71875
train loss:  0.5583488941192627
train gradient:  0.16928024618583876
iteration : 196
train acc:  0.7421875
train loss:  0.4909215569496155
train gradient:  0.11145717329211995
iteration : 197
train acc:  0.796875
train loss:  0.4532500207424164
train gradient:  0.11414592614758695
iteration : 198
train acc:  0.734375
train loss:  0.5615770220756531
train gradient:  0.18215898418783014
iteration : 199
train acc:  0.734375
train loss:  0.5187405347824097
train gradient:  0.1161116650309533
iteration : 200
train acc:  0.7734375
train loss:  0.4374275505542755
train gradient:  0.10528535714062792
iteration : 201
train acc:  0.7265625
train loss:  0.46454426646232605
train gradient:  0.10099207060641172
iteration : 202
train acc:  0.734375
train loss:  0.5018290877342224
train gradient:  0.1129124610535729
iteration : 203
train acc:  0.7109375
train loss:  0.5270172357559204
train gradient:  0.18572665059818416
iteration : 204
train acc:  0.7421875
train loss:  0.5232264995574951
train gradient:  0.11950989759984315
iteration : 205
train acc:  0.7734375
train loss:  0.4991416335105896
train gradient:  0.12359800527943889
iteration : 206
train acc:  0.8125
train loss:  0.39869022369384766
train gradient:  0.11204746295085444
iteration : 207
train acc:  0.7421875
train loss:  0.5121251344680786
train gradient:  0.11485931244511287
iteration : 208
train acc:  0.75
train loss:  0.49051201343536377
train gradient:  0.10705326471461996
iteration : 209
train acc:  0.7578125
train loss:  0.4821694493293762
train gradient:  0.12299823515989582
iteration : 210
train acc:  0.75
train loss:  0.48555317521095276
train gradient:  0.13646192537601606
iteration : 211
train acc:  0.7109375
train loss:  0.494301974773407
train gradient:  0.1406807787586482
iteration : 212
train acc:  0.8125
train loss:  0.42120176553726196
train gradient:  0.10879909218824363
iteration : 213
train acc:  0.796875
train loss:  0.4213204085826874
train gradient:  0.10138476112451654
iteration : 214
train acc:  0.671875
train loss:  0.5544309616088867
train gradient:  0.18700069860150922
iteration : 215
train acc:  0.7265625
train loss:  0.48201170563697815
train gradient:  0.16918710599410514
iteration : 216
train acc:  0.734375
train loss:  0.48539096117019653
train gradient:  0.11248957101796575
iteration : 217
train acc:  0.75
train loss:  0.5042993426322937
train gradient:  0.12561742463501696
iteration : 218
train acc:  0.75
train loss:  0.4725087285041809
train gradient:  0.13642705010495337
iteration : 219
train acc:  0.7734375
train loss:  0.44321101903915405
train gradient:  0.10604775108760142
iteration : 220
train acc:  0.703125
train loss:  0.5326651930809021
train gradient:  0.11316868912822754
iteration : 221
train acc:  0.671875
train loss:  0.5266903638839722
train gradient:  0.1644236481416313
iteration : 222
train acc:  0.71875
train loss:  0.48223620653152466
train gradient:  0.1162618936747358
iteration : 223
train acc:  0.71875
train loss:  0.48542559146881104
train gradient:  0.11592563722806828
iteration : 224
train acc:  0.7734375
train loss:  0.4444199204444885
train gradient:  0.10776883728963511
iteration : 225
train acc:  0.734375
train loss:  0.501928985118866
train gradient:  0.13931979087251578
iteration : 226
train acc:  0.7734375
train loss:  0.4501718282699585
train gradient:  0.11468920825845512
iteration : 227
train acc:  0.7890625
train loss:  0.41190004348754883
train gradient:  0.09963668420814806
iteration : 228
train acc:  0.71875
train loss:  0.4912228286266327
train gradient:  0.14073580104706385
iteration : 229
train acc:  0.6875
train loss:  0.5692952871322632
train gradient:  0.19859898086195554
iteration : 230
train acc:  0.671875
train loss:  0.5648765563964844
train gradient:  0.16305645712156813
iteration : 231
train acc:  0.6953125
train loss:  0.5168781280517578
train gradient:  0.12976054994436653
iteration : 232
train acc:  0.7109375
train loss:  0.5186597108840942
train gradient:  0.11629615243073722
iteration : 233
train acc:  0.765625
train loss:  0.4352017343044281
train gradient:  0.09011306118601065
iteration : 234
train acc:  0.7265625
train loss:  0.5087020993232727
train gradient:  0.1330468183120821
iteration : 235
train acc:  0.71875
train loss:  0.5098239183425903
train gradient:  0.1255519529537112
iteration : 236
train acc:  0.796875
train loss:  0.45253247022628784
train gradient:  0.13380388931692588
iteration : 237
train acc:  0.7421875
train loss:  0.4847591519355774
train gradient:  0.10290673145669964
iteration : 238
train acc:  0.8125
train loss:  0.39223501086235046
train gradient:  0.09332284534228555
iteration : 239
train acc:  0.7890625
train loss:  0.39531388878822327
train gradient:  0.08188456503615325
iteration : 240
train acc:  0.7890625
train loss:  0.44506633281707764
train gradient:  0.0924583880585632
iteration : 241
train acc:  0.765625
train loss:  0.46639710664749146
train gradient:  0.12067146361704201
iteration : 242
train acc:  0.7578125
train loss:  0.4899594187736511
train gradient:  0.16145241077631917
iteration : 243
train acc:  0.7421875
train loss:  0.4593309760093689
train gradient:  0.10116170338130565
iteration : 244
train acc:  0.734375
train loss:  0.5325626134872437
train gradient:  0.14252098820486953
iteration : 245
train acc:  0.7578125
train loss:  0.4636229872703552
train gradient:  0.1293322851501575
iteration : 246
train acc:  0.734375
train loss:  0.5042526721954346
train gradient:  0.15760382155955796
iteration : 247
train acc:  0.703125
train loss:  0.5573040246963501
train gradient:  0.1602003105989589
iteration : 248
train acc:  0.828125
train loss:  0.404498428106308
train gradient:  0.097513939088239
iteration : 249
train acc:  0.7265625
train loss:  0.5264686346054077
train gradient:  0.15696598889829907
iteration : 250
train acc:  0.703125
train loss:  0.495988667011261
train gradient:  0.12542611578132018
iteration : 251
train acc:  0.765625
train loss:  0.5286975502967834
train gradient:  0.15356779320864372
iteration : 252
train acc:  0.7421875
train loss:  0.4978038966655731
train gradient:  0.12944668913361335
iteration : 253
train acc:  0.796875
train loss:  0.41316190361976624
train gradient:  0.11315277155904238
iteration : 254
train acc:  0.71875
train loss:  0.5288445353507996
train gradient:  0.15534462255549364
iteration : 255
train acc:  0.6875
train loss:  0.514091968536377
train gradient:  0.14161909976596776
iteration : 256
train acc:  0.8046875
train loss:  0.45122775435447693
train gradient:  0.11768928903994301
iteration : 257
train acc:  0.7265625
train loss:  0.5047550201416016
train gradient:  0.12446413548112169
iteration : 258
train acc:  0.796875
train loss:  0.4543944299221039
train gradient:  0.10555374237332286
iteration : 259
train acc:  0.765625
train loss:  0.44706299901008606
train gradient:  0.1290424444583747
iteration : 260
train acc:  0.75
train loss:  0.48334795236587524
train gradient:  0.13472608874818917
iteration : 261
train acc:  0.75
train loss:  0.480551540851593
train gradient:  0.11223558185923531
iteration : 262
train acc:  0.765625
train loss:  0.44291549921035767
train gradient:  0.09511248615063589
iteration : 263
train acc:  0.7734375
train loss:  0.4647402763366699
train gradient:  0.15694558218153343
iteration : 264
train acc:  0.71875
train loss:  0.5440425276756287
train gradient:  0.14436267180414572
iteration : 265
train acc:  0.796875
train loss:  0.503969669342041
train gradient:  0.13018994765434516
iteration : 266
train acc:  0.78125
train loss:  0.4614481031894684
train gradient:  0.11556164976834613
iteration : 267
train acc:  0.75
train loss:  0.4781995117664337
train gradient:  0.12916529654867004
iteration : 268
train acc:  0.765625
train loss:  0.49596351385116577
train gradient:  0.12970491722644967
iteration : 269
train acc:  0.8046875
train loss:  0.43083760142326355
train gradient:  0.0960184467552739
iteration : 270
train acc:  0.6484375
train loss:  0.5082339644432068
train gradient:  0.14286323016403424
iteration : 271
train acc:  0.8125
train loss:  0.42684125900268555
train gradient:  0.12895144992248295
iteration : 272
train acc:  0.71875
train loss:  0.4825924336910248
train gradient:  0.12799345305159773
iteration : 273
train acc:  0.765625
train loss:  0.43436357378959656
train gradient:  0.08076732964927442
iteration : 274
train acc:  0.7890625
train loss:  0.45963233709335327
train gradient:  0.10696507186557103
iteration : 275
train acc:  0.6640625
train loss:  0.557669997215271
train gradient:  0.15139684448271967
iteration : 276
train acc:  0.796875
train loss:  0.45546114444732666
train gradient:  0.11928988097280038
iteration : 277
train acc:  0.7421875
train loss:  0.530892550945282
train gradient:  0.1551564337389151
iteration : 278
train acc:  0.765625
train loss:  0.4488023519515991
train gradient:  0.10957114577671846
iteration : 279
train acc:  0.7421875
train loss:  0.5023410320281982
train gradient:  0.18682806017843298
iteration : 280
train acc:  0.765625
train loss:  0.4450525641441345
train gradient:  0.10637628249238146
iteration : 281
train acc:  0.7421875
train loss:  0.5094451904296875
train gradient:  0.11625507236797696
iteration : 282
train acc:  0.7421875
train loss:  0.5021772384643555
train gradient:  0.14763406454505512
iteration : 283
train acc:  0.8125
train loss:  0.4262394309043884
train gradient:  0.11013720259744708
iteration : 284
train acc:  0.7109375
train loss:  0.4896473288536072
train gradient:  0.15082253417605665
iteration : 285
train acc:  0.6875
train loss:  0.5518479347229004
train gradient:  0.17387538832341168
iteration : 286
train acc:  0.75
train loss:  0.4751439094543457
train gradient:  0.1221664653542529
iteration : 287
train acc:  0.71875
train loss:  0.4934929609298706
train gradient:  0.1296484931121281
iteration : 288
train acc:  0.7734375
train loss:  0.46948134899139404
train gradient:  0.11558602488710645
iteration : 289
train acc:  0.6875
train loss:  0.5031150579452515
train gradient:  0.12826253200415585
iteration : 290
train acc:  0.7734375
train loss:  0.44146808981895447
train gradient:  0.15644776511562664
iteration : 291
train acc:  0.734375
train loss:  0.5132871866226196
train gradient:  0.15051059514533816
iteration : 292
train acc:  0.75
train loss:  0.4610980749130249
train gradient:  0.12369891701479843
iteration : 293
train acc:  0.7734375
train loss:  0.43881040811538696
train gradient:  0.10534260262963085
iteration : 294
train acc:  0.7421875
train loss:  0.472675621509552
train gradient:  0.11708844639279348
iteration : 295
train acc:  0.7734375
train loss:  0.47300735116004944
train gradient:  0.11529695555475214
iteration : 296
train acc:  0.7890625
train loss:  0.46289610862731934
train gradient:  0.1040409258313841
iteration : 297
train acc:  0.7734375
train loss:  0.46250513195991516
train gradient:  0.13715710768449318
iteration : 298
train acc:  0.7421875
train loss:  0.47921222448349
train gradient:  0.1408702466057778
iteration : 299
train acc:  0.7890625
train loss:  0.5194562673568726
train gradient:  0.162176883957316
iteration : 300
train acc:  0.7578125
train loss:  0.4575309753417969
train gradient:  0.11820956469775773
iteration : 301
train acc:  0.8046875
train loss:  0.43733352422714233
train gradient:  0.09790783931708534
iteration : 302
train acc:  0.7578125
train loss:  0.4073643386363983
train gradient:  0.08445395205658039
iteration : 303
train acc:  0.7578125
train loss:  0.42400869727134705
train gradient:  0.11579054878012195
iteration : 304
train acc:  0.78125
train loss:  0.48561954498291016
train gradient:  0.1226774054904132
iteration : 305
train acc:  0.7421875
train loss:  0.5235030651092529
train gradient:  0.13734816838038172
iteration : 306
train acc:  0.796875
train loss:  0.4544503688812256
train gradient:  0.12471732381205694
iteration : 307
train acc:  0.75
train loss:  0.425636351108551
train gradient:  0.098239372725759
iteration : 308
train acc:  0.7578125
train loss:  0.49540117383003235
train gradient:  0.15517721703292703
iteration : 309
train acc:  0.75
train loss:  0.4830133318901062
train gradient:  0.13644042428798825
iteration : 310
train acc:  0.75
train loss:  0.44395211338996887
train gradient:  0.11531862878225298
iteration : 311
train acc:  0.75
train loss:  0.46529868245124817
train gradient:  0.09941502718521858
iteration : 312
train acc:  0.7578125
train loss:  0.47879737615585327
train gradient:  0.11424919729161849
iteration : 313
train acc:  0.796875
train loss:  0.4705720543861389
train gradient:  0.10164002888031322
iteration : 314
train acc:  0.7109375
train loss:  0.5040566921234131
train gradient:  0.12590630361362126
iteration : 315
train acc:  0.7734375
train loss:  0.4856969118118286
train gradient:  0.15486607915428718
iteration : 316
train acc:  0.75
train loss:  0.49578797817230225
train gradient:  0.14079653718509988
iteration : 317
train acc:  0.7578125
train loss:  0.46129000186920166
train gradient:  0.1304318608451766
iteration : 318
train acc:  0.765625
train loss:  0.4684535264968872
train gradient:  0.11054328037727858
iteration : 319
train acc:  0.765625
train loss:  0.4534616470336914
train gradient:  0.1192667232821207
iteration : 320
train acc:  0.75
train loss:  0.42232388257980347
train gradient:  0.09770158176407026
iteration : 321
train acc:  0.7734375
train loss:  0.5423910617828369
train gradient:  0.1744864428400072
iteration : 322
train acc:  0.7265625
train loss:  0.5147508382797241
train gradient:  0.13327662240005314
iteration : 323
train acc:  0.7734375
train loss:  0.43487411737442017
train gradient:  0.11147020150790908
iteration : 324
train acc:  0.7578125
train loss:  0.4634685814380646
train gradient:  0.11098797144779712
iteration : 325
train acc:  0.8046875
train loss:  0.4481322765350342
train gradient:  0.08879072527416851
iteration : 326
train acc:  0.7578125
train loss:  0.43937554955482483
train gradient:  0.11237122512041808
iteration : 327
train acc:  0.7890625
train loss:  0.43824565410614014
train gradient:  0.1411795880693412
iteration : 328
train acc:  0.78125
train loss:  0.48974093794822693
train gradient:  0.13846702082572326
iteration : 329
train acc:  0.796875
train loss:  0.45131054520606995
train gradient:  0.13972726748401842
iteration : 330
train acc:  0.796875
train loss:  0.4448322653770447
train gradient:  0.11077034494230377
iteration : 331
train acc:  0.7421875
train loss:  0.46238869428634644
train gradient:  0.12163846141452629
iteration : 332
train acc:  0.6953125
train loss:  0.545638918876648
train gradient:  0.13750253970828763
iteration : 333
train acc:  0.7421875
train loss:  0.5599939823150635
train gradient:  0.2180906654906319
iteration : 334
train acc:  0.7578125
train loss:  0.5174506306648254
train gradient:  0.15220079133930825
iteration : 335
train acc:  0.7734375
train loss:  0.4414306581020355
train gradient:  0.12385529927535108
iteration : 336
train acc:  0.734375
train loss:  0.4711483120918274
train gradient:  0.16331092204968994
iteration : 337
train acc:  0.765625
train loss:  0.44560104608535767
train gradient:  0.10324509689903487
iteration : 338
train acc:  0.78125
train loss:  0.5074400901794434
train gradient:  0.19945477510785406
iteration : 339
train acc:  0.703125
train loss:  0.5526346564292908
train gradient:  0.18758195775350536
iteration : 340
train acc:  0.8203125
train loss:  0.44227203726768494
train gradient:  0.10490437310520864
iteration : 341
train acc:  0.7421875
train loss:  0.5088096857070923
train gradient:  0.1367685948035663
iteration : 342
train acc:  0.65625
train loss:  0.5957193374633789
train gradient:  0.19845385788782244
iteration : 343
train acc:  0.71875
train loss:  0.5122156143188477
train gradient:  0.1357218055198598
iteration : 344
train acc:  0.734375
train loss:  0.48686641454696655
train gradient:  0.13360990590622296
iteration : 345
train acc:  0.7109375
train loss:  0.48562008142471313
train gradient:  0.12982591592535853
iteration : 346
train acc:  0.6953125
train loss:  0.5568537712097168
train gradient:  0.12886432793435365
iteration : 347
train acc:  0.78125
train loss:  0.49505430459976196
train gradient:  0.12736624867448684
iteration : 348
train acc:  0.75
train loss:  0.5034326910972595
train gradient:  0.1304650677066202
iteration : 349
train acc:  0.7265625
train loss:  0.5009589195251465
train gradient:  0.16441542373686036
iteration : 350
train acc:  0.6953125
train loss:  0.5615156888961792
train gradient:  0.15021726071947622
iteration : 351
train acc:  0.7421875
train loss:  0.46730315685272217
train gradient:  0.12292716935145355
iteration : 352
train acc:  0.703125
train loss:  0.5005718469619751
train gradient:  0.15122138593519652
iteration : 353
train acc:  0.7734375
train loss:  0.4782830476760864
train gradient:  0.12243158295424737
iteration : 354
train acc:  0.78125
train loss:  0.46628615260124207
train gradient:  0.11086465298769307
iteration : 355
train acc:  0.7890625
train loss:  0.4879528880119324
train gradient:  0.12478939789034739
iteration : 356
train acc:  0.7578125
train loss:  0.5014873743057251
train gradient:  0.10726430502584462
iteration : 357
train acc:  0.7734375
train loss:  0.46840527653694153
train gradient:  0.14280263844294888
iteration : 358
train acc:  0.7109375
train loss:  0.5295882821083069
train gradient:  0.17142214515635412
iteration : 359
train acc:  0.8125
train loss:  0.4579150080680847
train gradient:  0.12749467281323912
iteration : 360
train acc:  0.765625
train loss:  0.4567975401878357
train gradient:  0.09905971767201957
iteration : 361
train acc:  0.7734375
train loss:  0.4446151852607727
train gradient:  0.10593695884453683
iteration : 362
train acc:  0.7578125
train loss:  0.49960312247276306
train gradient:  0.10590029781281872
iteration : 363
train acc:  0.7421875
train loss:  0.5086382627487183
train gradient:  0.10897165114788887
iteration : 364
train acc:  0.71875
train loss:  0.5125059485435486
train gradient:  0.136430715151095
iteration : 365
train acc:  0.7421875
train loss:  0.46957242488861084
train gradient:  0.11924427450496786
iteration : 366
train acc:  0.7734375
train loss:  0.46452999114990234
train gradient:  0.11010660469442608
iteration : 367
train acc:  0.765625
train loss:  0.46428248286247253
train gradient:  0.11674782040776203
iteration : 368
train acc:  0.734375
train loss:  0.5378166437149048
train gradient:  0.12173585944888837
iteration : 369
train acc:  0.71875
train loss:  0.47930753231048584
train gradient:  0.12773289117786896
iteration : 370
train acc:  0.8125
train loss:  0.4070439040660858
train gradient:  0.08445584596461471
iteration : 371
train acc:  0.7109375
train loss:  0.4695826768875122
train gradient:  0.1317883148640287
iteration : 372
train acc:  0.765625
train loss:  0.43967491388320923
train gradient:  0.09816486583089033
iteration : 373
train acc:  0.71875
train loss:  0.48824039101600647
train gradient:  0.12118765579689861
iteration : 374
train acc:  0.8203125
train loss:  0.3980284333229065
train gradient:  0.09483564783181911
iteration : 375
train acc:  0.8125
train loss:  0.43126317858695984
train gradient:  0.09472863245874256
iteration : 376
train acc:  0.71875
train loss:  0.49249333143234253
train gradient:  0.11294875349733605
iteration : 377
train acc:  0.7890625
train loss:  0.4648798108100891
train gradient:  0.13050697415366785
iteration : 378
train acc:  0.796875
train loss:  0.46238377690315247
train gradient:  0.1110347076396715
iteration : 379
train acc:  0.78125
train loss:  0.45013660192489624
train gradient:  0.11419591510376603
iteration : 380
train acc:  0.6875
train loss:  0.5859220027923584
train gradient:  0.18988574576791825
iteration : 381
train acc:  0.75
train loss:  0.4627847671508789
train gradient:  0.11001977770442874
iteration : 382
train acc:  0.7734375
train loss:  0.48516881465911865
train gradient:  0.11470593814942166
iteration : 383
train acc:  0.75
train loss:  0.4532102346420288
train gradient:  0.12231590785451536
iteration : 384
train acc:  0.7109375
train loss:  0.5597525835037231
train gradient:  0.150498154268084
iteration : 385
train acc:  0.7578125
train loss:  0.47202903032302856
train gradient:  0.13034229714490614
iteration : 386
train acc:  0.703125
train loss:  0.5446087121963501
train gradient:  0.1574104905454256
iteration : 387
train acc:  0.7890625
train loss:  0.4385788142681122
train gradient:  0.14542820269753748
iteration : 388
train acc:  0.7734375
train loss:  0.45135748386383057
train gradient:  0.14363629297945407
iteration : 389
train acc:  0.8046875
train loss:  0.43439698219299316
train gradient:  0.11988797050036262
iteration : 390
train acc:  0.7734375
train loss:  0.46362221240997314
train gradient:  0.11653029225467182
iteration : 391
train acc:  0.7890625
train loss:  0.448020339012146
train gradient:  0.09996645758785985
iteration : 392
train acc:  0.78125
train loss:  0.44478484988212585
train gradient:  0.11401169127801387
iteration : 393
train acc:  0.765625
train loss:  0.45066678524017334
train gradient:  0.0991465121881836
iteration : 394
train acc:  0.75
train loss:  0.46601298451423645
train gradient:  0.11513694610648839
iteration : 395
train acc:  0.7734375
train loss:  0.4589485228061676
train gradient:  0.13095734449143687
iteration : 396
train acc:  0.7578125
train loss:  0.4645373523235321
train gradient:  0.10950594535963558
iteration : 397
train acc:  0.7265625
train loss:  0.4964439570903778
train gradient:  0.11556218629533689
iteration : 398
train acc:  0.7578125
train loss:  0.4936007261276245
train gradient:  0.14816176278545445
iteration : 399
train acc:  0.8046875
train loss:  0.4423674941062927
train gradient:  0.10953394993850157
iteration : 400
train acc:  0.765625
train loss:  0.446723610162735
train gradient:  0.09346653264672372
iteration : 401
train acc:  0.71875
train loss:  0.5014461278915405
train gradient:  0.1262474675668131
iteration : 402
train acc:  0.7421875
train loss:  0.4604431688785553
train gradient:  0.12919515831576794
iteration : 403
train acc:  0.7109375
train loss:  0.5306457281112671
train gradient:  0.16631846514681362
iteration : 404
train acc:  0.7734375
train loss:  0.4489673972129822
train gradient:  0.1138639431527616
iteration : 405
train acc:  0.7109375
train loss:  0.545366108417511
train gradient:  0.13778613736974987
iteration : 406
train acc:  0.7734375
train loss:  0.46156251430511475
train gradient:  0.11418447008268855
iteration : 407
train acc:  0.7734375
train loss:  0.44603437185287476
train gradient:  0.10846247875082468
iteration : 408
train acc:  0.7734375
train loss:  0.46554937958717346
train gradient:  0.11022789721730095
iteration : 409
train acc:  0.7109375
train loss:  0.5848414301872253
train gradient:  0.1883135454688521
iteration : 410
train acc:  0.71875
train loss:  0.49348190426826477
train gradient:  0.18245946891902087
iteration : 411
train acc:  0.6953125
train loss:  0.4889657199382782
train gradient:  0.1268062438911127
iteration : 412
train acc:  0.7421875
train loss:  0.494531512260437
train gradient:  0.1786240358973325
iteration : 413
train acc:  0.7578125
train loss:  0.4712027311325073
train gradient:  0.13449728489149965
iteration : 414
train acc:  0.78125
train loss:  0.44373762607574463
train gradient:  0.12798917425051576
iteration : 415
train acc:  0.734375
train loss:  0.5000256299972534
train gradient:  0.1382484092668839
iteration : 416
train acc:  0.7421875
train loss:  0.4579744338989258
train gradient:  0.09929253664444385
iteration : 417
train acc:  0.6640625
train loss:  0.5540661811828613
train gradient:  0.19688316515478715
iteration : 418
train acc:  0.78125
train loss:  0.43804997205734253
train gradient:  0.09793933003730494
iteration : 419
train acc:  0.78125
train loss:  0.45821326971054077
train gradient:  0.12720494635706941
iteration : 420
train acc:  0.7109375
train loss:  0.4889379143714905
train gradient:  0.15441581773688862
iteration : 421
train acc:  0.6796875
train loss:  0.5206868052482605
train gradient:  0.13938470081780474
iteration : 422
train acc:  0.75
train loss:  0.4705657362937927
train gradient:  0.11655994036732449
iteration : 423
train acc:  0.78125
train loss:  0.4497496485710144
train gradient:  0.12498315624592801
iteration : 424
train acc:  0.671875
train loss:  0.5970249176025391
train gradient:  0.19254559314413355
iteration : 425
train acc:  0.7734375
train loss:  0.47224539518356323
train gradient:  0.09938001674995683
iteration : 426
train acc:  0.7265625
train loss:  0.4588776230812073
train gradient:  0.14793906625381664
iteration : 427
train acc:  0.7578125
train loss:  0.43915432691574097
train gradient:  0.11101431912230413
iteration : 428
train acc:  0.7578125
train loss:  0.4946892261505127
train gradient:  0.12856937569892918
iteration : 429
train acc:  0.7265625
train loss:  0.4797077178955078
train gradient:  0.14573480539339922
iteration : 430
train acc:  0.78125
train loss:  0.4465707540512085
train gradient:  0.1260848408843545
iteration : 431
train acc:  0.7578125
train loss:  0.4337202310562134
train gradient:  0.11732143273719141
iteration : 432
train acc:  0.7734375
train loss:  0.49201637506484985
train gradient:  0.11335850508737458
iteration : 433
train acc:  0.703125
train loss:  0.5607120394706726
train gradient:  0.16251974847018183
iteration : 434
train acc:  0.7734375
train loss:  0.47265878319740295
train gradient:  0.11267650732037959
iteration : 435
train acc:  0.7578125
train loss:  0.4346187114715576
train gradient:  0.10229819291601722
iteration : 436
train acc:  0.734375
train loss:  0.49255985021591187
train gradient:  0.12349864077442482
iteration : 437
train acc:  0.7734375
train loss:  0.48961737751960754
train gradient:  0.10524352628466095
iteration : 438
train acc:  0.7265625
train loss:  0.48238319158554077
train gradient:  0.099991766833757
iteration : 439
train acc:  0.75
train loss:  0.47987571358680725
train gradient:  0.11559475594777763
iteration : 440
train acc:  0.7265625
train loss:  0.5128050446510315
train gradient:  0.166582761393625
iteration : 441
train acc:  0.7421875
train loss:  0.523537278175354
train gradient:  0.15081624539010063
iteration : 442
train acc:  0.734375
train loss:  0.4515623450279236
train gradient:  0.14525187427926534
iteration : 443
train acc:  0.7578125
train loss:  0.4601086974143982
train gradient:  0.15445852827645862
iteration : 444
train acc:  0.75
train loss:  0.44100576639175415
train gradient:  0.10309020299297103
iteration : 445
train acc:  0.7421875
train loss:  0.4922851324081421
train gradient:  0.16126215752713316
iteration : 446
train acc:  0.7109375
train loss:  0.5349540114402771
train gradient:  0.15826810180244166
iteration : 447
train acc:  0.7421875
train loss:  0.5638489723205566
train gradient:  0.18257632795441514
iteration : 448
train acc:  0.6953125
train loss:  0.5359036922454834
train gradient:  0.16738139385359213
iteration : 449
train acc:  0.703125
train loss:  0.5213907957077026
train gradient:  0.18125966553838657
iteration : 450
train acc:  0.7421875
train loss:  0.4419035315513611
train gradient:  0.11096539874761459
iteration : 451
train acc:  0.7578125
train loss:  0.5101339221000671
train gradient:  0.12954795388195872
iteration : 452
train acc:  0.703125
train loss:  0.5035712718963623
train gradient:  0.15027937006604164
iteration : 453
train acc:  0.6875
train loss:  0.5244029760360718
train gradient:  0.10876498513559685
iteration : 454
train acc:  0.75
train loss:  0.5168451070785522
train gradient:  0.12075470188182989
iteration : 455
train acc:  0.8046875
train loss:  0.3986719250679016
train gradient:  0.08220155985792005
iteration : 456
train acc:  0.75
train loss:  0.4899267554283142
train gradient:  0.14594019452233148
iteration : 457
train acc:  0.8203125
train loss:  0.40788140892982483
train gradient:  0.08651411690072949
iteration : 458
train acc:  0.75
train loss:  0.5318785905838013
train gradient:  0.14085164116951154
iteration : 459
train acc:  0.796875
train loss:  0.4235309660434723
train gradient:  0.10834869405041966
iteration : 460
train acc:  0.734375
train loss:  0.4797290563583374
train gradient:  0.13932070009952183
iteration : 461
train acc:  0.6796875
train loss:  0.5407302975654602
train gradient:  0.16311213050927836
iteration : 462
train acc:  0.7578125
train loss:  0.47802120447158813
train gradient:  0.10268585298929635
iteration : 463
train acc:  0.7890625
train loss:  0.4595128893852234
train gradient:  0.0999532891263296
iteration : 464
train acc:  0.71875
train loss:  0.5507326126098633
train gradient:  0.16128949055351705
iteration : 465
train acc:  0.7421875
train loss:  0.4829888939857483
train gradient:  0.11397996578202027
iteration : 466
train acc:  0.78125
train loss:  0.44730329513549805
train gradient:  0.13093941613853677
iteration : 467
train acc:  0.7578125
train loss:  0.47216111421585083
train gradient:  0.1175550829684319
iteration : 468
train acc:  0.6953125
train loss:  0.5106906890869141
train gradient:  0.15609364285488608
iteration : 469
train acc:  0.734375
train loss:  0.5321700572967529
train gradient:  0.14298400467982292
iteration : 470
train acc:  0.75
train loss:  0.47381162643432617
train gradient:  0.11011669715628872
iteration : 471
train acc:  0.7109375
train loss:  0.488468199968338
train gradient:  0.12644069085368023
iteration : 472
train acc:  0.7421875
train loss:  0.4856414198875427
train gradient:  0.12173063174063978
iteration : 473
train acc:  0.6875
train loss:  0.5145065188407898
train gradient:  0.11046813253268001
iteration : 474
train acc:  0.796875
train loss:  0.45501708984375
train gradient:  0.1093461938553803
iteration : 475
train acc:  0.7265625
train loss:  0.5249605774879456
train gradient:  0.12389372120650284
iteration : 476
train acc:  0.765625
train loss:  0.4840138852596283
train gradient:  0.11940352997874384
iteration : 477
train acc:  0.8515625
train loss:  0.40438252687454224
train gradient:  0.10687997278069598
iteration : 478
train acc:  0.7265625
train loss:  0.5530462861061096
train gradient:  0.19578794651520526
iteration : 479
train acc:  0.7421875
train loss:  0.4589206576347351
train gradient:  0.10982428762664656
iteration : 480
train acc:  0.8125
train loss:  0.416131854057312
train gradient:  0.09200034874955836
iteration : 481
train acc:  0.8515625
train loss:  0.3979870676994324
train gradient:  0.08317997735780865
iteration : 482
train acc:  0.7421875
train loss:  0.5086188912391663
train gradient:  0.14726506014044746
iteration : 483
train acc:  0.7421875
train loss:  0.4280060827732086
train gradient:  0.12338416688628265
iteration : 484
train acc:  0.8203125
train loss:  0.4323429465293884
train gradient:  0.08043193327716525
iteration : 485
train acc:  0.765625
train loss:  0.468569815158844
train gradient:  0.12264284886608026
iteration : 486
train acc:  0.703125
train loss:  0.5261011123657227
train gradient:  0.13951279882051704
iteration : 487
train acc:  0.7890625
train loss:  0.44131898880004883
train gradient:  0.09896542375647124
iteration : 488
train acc:  0.7265625
train loss:  0.5339096784591675
train gradient:  0.13842908196994141
iteration : 489
train acc:  0.734375
train loss:  0.48554447293281555
train gradient:  0.10744888858232718
iteration : 490
train acc:  0.7109375
train loss:  0.5714424848556519
train gradient:  0.19389954716157204
iteration : 491
train acc:  0.7578125
train loss:  0.46279028058052063
train gradient:  0.12915986658650394
iteration : 492
train acc:  0.7265625
train loss:  0.47554272413253784
train gradient:  0.12921057074952247
iteration : 493
train acc:  0.8125
train loss:  0.4498353898525238
train gradient:  0.10551308840642336
iteration : 494
train acc:  0.734375
train loss:  0.5464367866516113
train gradient:  0.24026245661985796
iteration : 495
train acc:  0.7265625
train loss:  0.4819180965423584
train gradient:  0.14816735519279278
iteration : 496
train acc:  0.703125
train loss:  0.5121192336082458
train gradient:  0.1233483100123525
iteration : 497
train acc:  0.84375
train loss:  0.42073583602905273
train gradient:  0.09042050828000844
iteration : 498
train acc:  0.7109375
train loss:  0.4797915816307068
train gradient:  0.12034215111128903
iteration : 499
train acc:  0.7890625
train loss:  0.4986651837825775
train gradient:  0.10926650459920408
iteration : 500
train acc:  0.8203125
train loss:  0.39230877161026
train gradient:  0.10284056147176318
iteration : 501
train acc:  0.6796875
train loss:  0.5294461846351624
train gradient:  0.1338235565589253
iteration : 502
train acc:  0.78125
train loss:  0.43093201518058777
train gradient:  0.08641539207244416
iteration : 503
train acc:  0.7890625
train loss:  0.4131218492984772
train gradient:  0.09467490153091052
iteration : 504
train acc:  0.734375
train loss:  0.4964563846588135
train gradient:  0.12789926699170395
iteration : 505
train acc:  0.7890625
train loss:  0.453066885471344
train gradient:  0.11468159437968428
iteration : 506
train acc:  0.765625
train loss:  0.5048450827598572
train gradient:  0.128153670084417
iteration : 507
train acc:  0.71875
train loss:  0.49512559175491333
train gradient:  0.1435178465131265
iteration : 508
train acc:  0.78125
train loss:  0.4354684352874756
train gradient:  0.10160863010181552
iteration : 509
train acc:  0.765625
train loss:  0.4508287310600281
train gradient:  0.11473359413129593
iteration : 510
train acc:  0.75
train loss:  0.45752596855163574
train gradient:  0.09045934207855105
iteration : 511
train acc:  0.796875
train loss:  0.4284645915031433
train gradient:  0.08496904304832317
iteration : 512
train acc:  0.75
train loss:  0.4970284402370453
train gradient:  0.13446039827780926
iteration : 513
train acc:  0.7578125
train loss:  0.4719098210334778
train gradient:  0.12847036276798712
iteration : 514
train acc:  0.78125
train loss:  0.4675406217575073
train gradient:  0.13251797595803821
iteration : 515
train acc:  0.7578125
train loss:  0.4674915075302124
train gradient:  0.11089115623649594
iteration : 516
train acc:  0.71875
train loss:  0.4820517897605896
train gradient:  0.15677110611604586
iteration : 517
train acc:  0.7109375
train loss:  0.5074918270111084
train gradient:  0.13054111584297745
iteration : 518
train acc:  0.7109375
train loss:  0.5195262432098389
train gradient:  0.1488604841888904
iteration : 519
train acc:  0.8359375
train loss:  0.3818405568599701
train gradient:  0.10175853818352078
iteration : 520
train acc:  0.703125
train loss:  0.4966265559196472
train gradient:  0.1219292702431095
iteration : 521
train acc:  0.7265625
train loss:  0.48881644010543823
train gradient:  0.11252027211285344
iteration : 522
train acc:  0.75
train loss:  0.5095546245574951
train gradient:  0.15758467595919207
iteration : 523
train acc:  0.7578125
train loss:  0.45950767397880554
train gradient:  0.10404077700135317
iteration : 524
train acc:  0.7109375
train loss:  0.554358720779419
train gradient:  0.16965790120058966
iteration : 525
train acc:  0.7578125
train loss:  0.5156087279319763
train gradient:  0.13377410999643158
iteration : 526
train acc:  0.75
train loss:  0.5569653511047363
train gradient:  0.17452694421196047
iteration : 527
train acc:  0.7421875
train loss:  0.49425455927848816
train gradient:  0.12895379599066975
iteration : 528
train acc:  0.7734375
train loss:  0.5017443895339966
train gradient:  0.16200410414265864
iteration : 529
train acc:  0.75
train loss:  0.49488401412963867
train gradient:  0.12017903161287675
iteration : 530
train acc:  0.7734375
train loss:  0.4108448028564453
train gradient:  0.0950302267811775
iteration : 531
train acc:  0.7421875
train loss:  0.45305323600769043
train gradient:  0.15631083231441079
iteration : 532
train acc:  0.7578125
train loss:  0.46369242668151855
train gradient:  0.10269950576312487
iteration : 533
train acc:  0.7421875
train loss:  0.4829699695110321
train gradient:  0.1366964210469178
iteration : 534
train acc:  0.7890625
train loss:  0.45781728625297546
train gradient:  0.10923239261307512
iteration : 535
train acc:  0.7890625
train loss:  0.45132526755332947
train gradient:  0.10190768928580536
iteration : 536
train acc:  0.7890625
train loss:  0.4937174916267395
train gradient:  0.1404415272014133
iteration : 537
train acc:  0.6796875
train loss:  0.5431622266769409
train gradient:  0.15458529396486698
iteration : 538
train acc:  0.7578125
train loss:  0.5264450907707214
train gradient:  0.179662945865814
iteration : 539
train acc:  0.7265625
train loss:  0.5524694919586182
train gradient:  0.14909250067581076
iteration : 540
train acc:  0.7265625
train loss:  0.5106244087219238
train gradient:  0.15783628905592806
iteration : 541
train acc:  0.796875
train loss:  0.45723408460617065
train gradient:  0.13947876512366014
iteration : 542
train acc:  0.7734375
train loss:  0.4670758545398712
train gradient:  0.124936313304207
iteration : 543
train acc:  0.7421875
train loss:  0.5214780569076538
train gradient:  0.1526268243040893
iteration : 544
train acc:  0.765625
train loss:  0.4872444272041321
train gradient:  0.1279272685301287
iteration : 545
train acc:  0.8125
train loss:  0.42806196212768555
train gradient:  0.10488507877404188
iteration : 546
train acc:  0.7421875
train loss:  0.5273890495300293
train gradient:  0.14947177139098589
iteration : 547
train acc:  0.765625
train loss:  0.4895598292350769
train gradient:  0.14289505635326344
iteration : 548
train acc:  0.75
train loss:  0.5154625177383423
train gradient:  0.13823405372171654
iteration : 549
train acc:  0.734375
train loss:  0.5058612823486328
train gradient:  0.11757656822939057
iteration : 550
train acc:  0.7890625
train loss:  0.4934992790222168
train gradient:  0.12422183016767044
iteration : 551
train acc:  0.75
train loss:  0.4316056966781616
train gradient:  0.09243424589009043
iteration : 552
train acc:  0.71875
train loss:  0.5137540102005005
train gradient:  0.16105343507776287
iteration : 553
train acc:  0.78125
train loss:  0.4958738088607788
train gradient:  0.16242338496679198
iteration : 554
train acc:  0.625
train loss:  0.6080821752548218
train gradient:  0.17161701468011192
iteration : 555
train acc:  0.671875
train loss:  0.5583757162094116
train gradient:  0.18604426840000576
iteration : 556
train acc:  0.75
train loss:  0.4607347249984741
train gradient:  0.1072320330778416
iteration : 557
train acc:  0.75
train loss:  0.5262872576713562
train gradient:  0.12971153790139067
iteration : 558
train acc:  0.7109375
train loss:  0.4777339696884155
train gradient:  0.12837641271301392
iteration : 559
train acc:  0.8203125
train loss:  0.42247506976127625
train gradient:  0.10981396454465078
iteration : 560
train acc:  0.7421875
train loss:  0.4660319685935974
train gradient:  0.10449376889129892
iteration : 561
train acc:  0.7890625
train loss:  0.4430597722530365
train gradient:  0.11225331166430817
iteration : 562
train acc:  0.75
train loss:  0.45876845717430115
train gradient:  0.10197569090864955
iteration : 563
train acc:  0.796875
train loss:  0.4440529942512512
train gradient:  0.12006685048488766
iteration : 564
train acc:  0.6796875
train loss:  0.5612484216690063
train gradient:  0.13004538771365876
iteration : 565
train acc:  0.7578125
train loss:  0.4746343493461609
train gradient:  0.1315542223238359
iteration : 566
train acc:  0.6875
train loss:  0.4985773265361786
train gradient:  0.1042369013226213
iteration : 567
train acc:  0.7890625
train loss:  0.43614402413368225
train gradient:  0.10950449640823205
iteration : 568
train acc:  0.71875
train loss:  0.5383591055870056
train gradient:  0.14442124496125086
iteration : 569
train acc:  0.7578125
train loss:  0.5215151309967041
train gradient:  0.1685013879796775
iteration : 570
train acc:  0.7421875
train loss:  0.4782722294330597
train gradient:  0.17895176952145037
iteration : 571
train acc:  0.7109375
train loss:  0.5567883253097534
train gradient:  0.1628015510337506
iteration : 572
train acc:  0.7734375
train loss:  0.44913503527641296
train gradient:  0.12740339186943733
iteration : 573
train acc:  0.7265625
train loss:  0.5573205947875977
train gradient:  0.15074370892370992
iteration : 574
train acc:  0.78125
train loss:  0.47358250617980957
train gradient:  0.10736989959483309
iteration : 575
train acc:  0.65625
train loss:  0.6372873783111572
train gradient:  0.21892257818003552
iteration : 576
train acc:  0.78125
train loss:  0.4573156535625458
train gradient:  0.10911638117624485
iteration : 577
train acc:  0.703125
train loss:  0.43118613958358765
train gradient:  0.09524263957364748
iteration : 578
train acc:  0.7109375
train loss:  0.550583004951477
train gradient:  0.1630932860560487
iteration : 579
train acc:  0.703125
train loss:  0.5183029770851135
train gradient:  0.14339281462773823
iteration : 580
train acc:  0.78125
train loss:  0.46874725818634033
train gradient:  0.11355496018342766
iteration : 581
train acc:  0.75
train loss:  0.5175987482070923
train gradient:  0.1308860055544279
iteration : 582
train acc:  0.7578125
train loss:  0.47688519954681396
train gradient:  0.10371916497688231
iteration : 583
train acc:  0.71875
train loss:  0.49847114086151123
train gradient:  0.1573498904284395
iteration : 584
train acc:  0.7578125
train loss:  0.4617554545402527
train gradient:  0.10050544936840082
iteration : 585
train acc:  0.734375
train loss:  0.4676271677017212
train gradient:  0.13198897360085354
iteration : 586
train acc:  0.7734375
train loss:  0.451524019241333
train gradient:  0.11991876000383629
iteration : 587
train acc:  0.7734375
train loss:  0.48194631934165955
train gradient:  0.14651323995362542
iteration : 588
train acc:  0.7578125
train loss:  0.4429161250591278
train gradient:  0.09315817916284483
iteration : 589
train acc:  0.7421875
train loss:  0.49489450454711914
train gradient:  0.10476100170617565
iteration : 590
train acc:  0.8125
train loss:  0.4232023358345032
train gradient:  0.08507094632129508
iteration : 591
train acc:  0.796875
train loss:  0.43461179733276367
train gradient:  0.10846618974825276
iteration : 592
train acc:  0.8046875
train loss:  0.4300677478313446
train gradient:  0.11832867994094945
iteration : 593
train acc:  0.7421875
train loss:  0.4555850327014923
train gradient:  0.09294272257034814
iteration : 594
train acc:  0.7734375
train loss:  0.4485192894935608
train gradient:  0.1322617171825117
iteration : 595
train acc:  0.7109375
train loss:  0.5119481682777405
train gradient:  0.12692677246062306
iteration : 596
train acc:  0.7578125
train loss:  0.4865942895412445
train gradient:  0.14867029049770675
iteration : 597
train acc:  0.75
train loss:  0.45816051959991455
train gradient:  0.11186221537337038
iteration : 598
train acc:  0.765625
train loss:  0.4820425808429718
train gradient:  0.14100862318432605
iteration : 599
train acc:  0.7734375
train loss:  0.45109468698501587
train gradient:  0.1202342014000249
iteration : 600
train acc:  0.7421875
train loss:  0.47399038076400757
train gradient:  0.13642459815245386
iteration : 601
train acc:  0.75
train loss:  0.47875139117240906
train gradient:  0.13338173684558136
iteration : 602
train acc:  0.734375
train loss:  0.5032271146774292
train gradient:  0.16127269302126873
iteration : 603
train acc:  0.75
train loss:  0.47403812408447266
train gradient:  0.15651713074423013
iteration : 604
train acc:  0.703125
train loss:  0.5298631191253662
train gradient:  0.17696444403668635
iteration : 605
train acc:  0.7421875
train loss:  0.46591368317604065
train gradient:  0.09038534213956136
iteration : 606
train acc:  0.796875
train loss:  0.48788467049598694
train gradient:  0.12104084894075237
iteration : 607
train acc:  0.7265625
train loss:  0.5412207245826721
train gradient:  0.19798347494688995
iteration : 608
train acc:  0.75
train loss:  0.46308404207229614
train gradient:  0.11701761755768744
iteration : 609
train acc:  0.671875
train loss:  0.5620263814926147
train gradient:  0.19005618639736505
iteration : 610
train acc:  0.8046875
train loss:  0.47205597162246704
train gradient:  0.13376159464293208
iteration : 611
train acc:  0.7578125
train loss:  0.4728906750679016
train gradient:  0.10211282236371416
iteration : 612
train acc:  0.75
train loss:  0.4805198907852173
train gradient:  0.13801002364136095
iteration : 613
train acc:  0.828125
train loss:  0.42190930247306824
train gradient:  0.08896381130190231
iteration : 614
train acc:  0.71875
train loss:  0.567846417427063
train gradient:  0.14137196069730582
iteration : 615
train acc:  0.7109375
train loss:  0.5187599658966064
train gradient:  0.12579097351101576
iteration : 616
train acc:  0.78125
train loss:  0.49995309114456177
train gradient:  0.12414850270531608
iteration : 617
train acc:  0.78125
train loss:  0.4750119149684906
train gradient:  0.1225774743420233
iteration : 618
train acc:  0.7734375
train loss:  0.47604119777679443
train gradient:  0.11528648172796888
iteration : 619
train acc:  0.703125
train loss:  0.6012840270996094
train gradient:  0.15690618506608917
iteration : 620
train acc:  0.734375
train loss:  0.5372440218925476
train gradient:  0.13805270316200005
iteration : 621
train acc:  0.7421875
train loss:  0.5047533512115479
train gradient:  0.14475128753130073
iteration : 622
train acc:  0.7734375
train loss:  0.48021233081817627
train gradient:  0.12876627431708065
iteration : 623
train acc:  0.671875
train loss:  0.5651758909225464
train gradient:  0.1507747149052131
iteration : 624
train acc:  0.8125
train loss:  0.40130817890167236
train gradient:  0.08922590902848808
iteration : 625
train acc:  0.6640625
train loss:  0.5554546117782593
train gradient:  0.14374264350616123
iteration : 626
train acc:  0.6796875
train loss:  0.5564298629760742
train gradient:  0.14141582236940659
iteration : 627
train acc:  0.7578125
train loss:  0.49338459968566895
train gradient:  0.15633254268667973
iteration : 628
train acc:  0.734375
train loss:  0.49663519859313965
train gradient:  0.15422909825029696
iteration : 629
train acc:  0.7421875
train loss:  0.47149500250816345
train gradient:  0.10625705389917289
iteration : 630
train acc:  0.8203125
train loss:  0.41324013471603394
train gradient:  0.08433655558219327
iteration : 631
train acc:  0.8046875
train loss:  0.45892924070358276
train gradient:  0.10815092026826517
iteration : 632
train acc:  0.765625
train loss:  0.5061832070350647
train gradient:  0.11396114097149151
iteration : 633
train acc:  0.75
train loss:  0.48105546832084656
train gradient:  0.11523357486605436
iteration : 634
train acc:  0.7578125
train loss:  0.46556994318962097
train gradient:  0.10845172087744008
iteration : 635
train acc:  0.7265625
train loss:  0.49526482820510864
train gradient:  0.13250233766777159
iteration : 636
train acc:  0.6875
train loss:  0.49325990676879883
train gradient:  0.12456063589986222
iteration : 637
train acc:  0.734375
train loss:  0.4874234199523926
train gradient:  0.11482294739104958
iteration : 638
train acc:  0.7578125
train loss:  0.44649773836135864
train gradient:  0.10539198721866443
iteration : 639
train acc:  0.65625
train loss:  0.5397112369537354
train gradient:  0.14843077429137966
iteration : 640
train acc:  0.7734375
train loss:  0.453703910112381
train gradient:  0.13294484130137166
iteration : 641
train acc:  0.7890625
train loss:  0.4844823181629181
train gradient:  0.10551395080126771
iteration : 642
train acc:  0.7578125
train loss:  0.4693107008934021
train gradient:  0.15907077809014478
iteration : 643
train acc:  0.796875
train loss:  0.4337688982486725
train gradient:  0.09668910501082373
iteration : 644
train acc:  0.8125
train loss:  0.4291115701198578
train gradient:  0.0936234160971847
iteration : 645
train acc:  0.7578125
train loss:  0.4526419937610626
train gradient:  0.10857683806899102
iteration : 646
train acc:  0.71875
train loss:  0.5149924755096436
train gradient:  0.14406953636522707
iteration : 647
train acc:  0.8125
train loss:  0.4410077929496765
train gradient:  0.10435965083713937
iteration : 648
train acc:  0.71875
train loss:  0.4832857847213745
train gradient:  0.11105381287375408
iteration : 649
train acc:  0.7890625
train loss:  0.484860360622406
train gradient:  0.11724935056849987
iteration : 650
train acc:  0.75
train loss:  0.48150795698165894
train gradient:  0.11335723030752391
iteration : 651
train acc:  0.734375
train loss:  0.4818970561027527
train gradient:  0.11271226962882795
iteration : 652
train acc:  0.7421875
train loss:  0.4817419648170471
train gradient:  0.12430132225186342
iteration : 653
train acc:  0.7578125
train loss:  0.48135316371917725
train gradient:  0.11727316213651873
iteration : 654
train acc:  0.78125
train loss:  0.5110973119735718
train gradient:  0.17118768690581618
iteration : 655
train acc:  0.7578125
train loss:  0.5375983715057373
train gradient:  0.21252561522339014
iteration : 656
train acc:  0.7890625
train loss:  0.4818291962146759
train gradient:  0.1211126166587238
iteration : 657
train acc:  0.8125
train loss:  0.39986491203308105
train gradient:  0.08158483042978146
iteration : 658
train acc:  0.7421875
train loss:  0.5222945213317871
train gradient:  0.16199532338712375
iteration : 659
train acc:  0.796875
train loss:  0.43081188201904297
train gradient:  0.0896003975005868
iteration : 660
train acc:  0.796875
train loss:  0.44108784198760986
train gradient:  0.10268403868453013
iteration : 661
train acc:  0.796875
train loss:  0.43494927883148193
train gradient:  0.10211768023603852
iteration : 662
train acc:  0.765625
train loss:  0.5294843316078186
train gradient:  0.15473670272615
iteration : 663
train acc:  0.640625
train loss:  0.5604809522628784
train gradient:  0.14304908188612936
iteration : 664
train acc:  0.8046875
train loss:  0.4167354702949524
train gradient:  0.08935489104570812
iteration : 665
train acc:  0.703125
train loss:  0.5114177465438843
train gradient:  0.12502201966004947
iteration : 666
train acc:  0.7890625
train loss:  0.4423542022705078
train gradient:  0.09639052365646354
iteration : 667
train acc:  0.71875
train loss:  0.49256089329719543
train gradient:  0.13948318953754565
iteration : 668
train acc:  0.7578125
train loss:  0.4736201763153076
train gradient:  0.12517025802938297
iteration : 669
train acc:  0.7265625
train loss:  0.5161299109458923
train gradient:  0.13633674399433374
iteration : 670
train acc:  0.734375
train loss:  0.5672571063041687
train gradient:  0.18920290971177786
iteration : 671
train acc:  0.78125
train loss:  0.4186939597129822
train gradient:  0.13104667418095944
iteration : 672
train acc:  0.8125
train loss:  0.4031962752342224
train gradient:  0.09494063672554208
iteration : 673
train acc:  0.8359375
train loss:  0.3684650659561157
train gradient:  0.08714208939134094
iteration : 674
train acc:  0.7109375
train loss:  0.554979681968689
train gradient:  0.16041438616445974
iteration : 675
train acc:  0.7421875
train loss:  0.4708443582057953
train gradient:  0.1064960050838678
iteration : 676
train acc:  0.78125
train loss:  0.4087526202201843
train gradient:  0.08248223260075555
iteration : 677
train acc:  0.78125
train loss:  0.4118172526359558
train gradient:  0.0841026198752824
iteration : 678
train acc:  0.6953125
train loss:  0.5292366743087769
train gradient:  0.1495144297658949
iteration : 679
train acc:  0.7109375
train loss:  0.523295521736145
train gradient:  0.11849802557108154
iteration : 680
train acc:  0.75
train loss:  0.48914211988449097
train gradient:  0.1202191482088137
iteration : 681
train acc:  0.7734375
train loss:  0.43249911069869995
train gradient:  0.11514641647297812
iteration : 682
train acc:  0.7890625
train loss:  0.42272084951400757
train gradient:  0.11713285598513719
iteration : 683
train acc:  0.6953125
train loss:  0.5765200853347778
train gradient:  0.19525345288735946
iteration : 684
train acc:  0.7578125
train loss:  0.5028179883956909
train gradient:  0.12068700374626498
iteration : 685
train acc:  0.8203125
train loss:  0.39245468378067017
train gradient:  0.09668959380790755
iteration : 686
train acc:  0.78125
train loss:  0.4721730053424835
train gradient:  0.11963538110836684
iteration : 687
train acc:  0.6953125
train loss:  0.508254885673523
train gradient:  0.11986016567955945
iteration : 688
train acc:  0.75
train loss:  0.48337340354919434
train gradient:  0.11112260232666361
iteration : 689
train acc:  0.7265625
train loss:  0.5167677402496338
train gradient:  0.13830495471096077
iteration : 690
train acc:  0.7734375
train loss:  0.48212751746177673
train gradient:  0.1277584120976759
iteration : 691
train acc:  0.7890625
train loss:  0.46377527713775635
train gradient:  0.14140507290156556
iteration : 692
train acc:  0.7421875
train loss:  0.5205338597297668
train gradient:  0.13421748099481176
iteration : 693
train acc:  0.7265625
train loss:  0.5232268571853638
train gradient:  0.12871824373548102
iteration : 694
train acc:  0.734375
train loss:  0.5006561279296875
train gradient:  0.09493620580828142
iteration : 695
train acc:  0.765625
train loss:  0.4368979334831238
train gradient:  0.12432191319289297
iteration : 696
train acc:  0.765625
train loss:  0.4608919620513916
train gradient:  0.10542150643399213
iteration : 697
train acc:  0.7109375
train loss:  0.4901289939880371
train gradient:  0.12102789528578736
iteration : 698
train acc:  0.765625
train loss:  0.4373827576637268
train gradient:  0.0934974932220045
iteration : 699
train acc:  0.8359375
train loss:  0.41395047307014465
train gradient:  0.10667470024084777
iteration : 700
train acc:  0.7109375
train loss:  0.5541538000106812
train gradient:  0.14407420998378098
iteration : 701
train acc:  0.7734375
train loss:  0.5043258666992188
train gradient:  0.11042216981329882
iteration : 702
train acc:  0.78125
train loss:  0.4401617646217346
train gradient:  0.11016053260156075
iteration : 703
train acc:  0.7734375
train loss:  0.43476149439811707
train gradient:  0.1056855861779344
iteration : 704
train acc:  0.7265625
train loss:  0.5070386528968811
train gradient:  0.14137545295352116
iteration : 705
train acc:  0.6640625
train loss:  0.5747144222259521
train gradient:  0.1914401678322457
iteration : 706
train acc:  0.8515625
train loss:  0.39441877603530884
train gradient:  0.10545086972277053
iteration : 707
train acc:  0.7890625
train loss:  0.42362624406814575
train gradient:  0.09216269648411633
iteration : 708
train acc:  0.7109375
train loss:  0.5108799338340759
train gradient:  0.10965339360928691
iteration : 709
train acc:  0.7265625
train loss:  0.49011677503585815
train gradient:  0.12287572118464411
iteration : 710
train acc:  0.8203125
train loss:  0.4256364703178406
train gradient:  0.08929673012227025
iteration : 711
train acc:  0.7890625
train loss:  0.4591039717197418
train gradient:  0.10767167560860372
iteration : 712
train acc:  0.7421875
train loss:  0.47542572021484375
train gradient:  0.11728666572344033
iteration : 713
train acc:  0.7421875
train loss:  0.5238676071166992
train gradient:  0.14115957333639367
iteration : 714
train acc:  0.7265625
train loss:  0.4687899649143219
train gradient:  0.1132260337659858
iteration : 715
train acc:  0.78125
train loss:  0.43195053935050964
train gradient:  0.10071794942761109
iteration : 716
train acc:  0.765625
train loss:  0.4700903296470642
train gradient:  0.10994431424393221
iteration : 717
train acc:  0.7578125
train loss:  0.4895031154155731
train gradient:  0.14650236476183104
iteration : 718
train acc:  0.703125
train loss:  0.5013716816902161
train gradient:  0.14859826585282776
iteration : 719
train acc:  0.7578125
train loss:  0.44056737422943115
train gradient:  0.10523146646746194
iteration : 720
train acc:  0.765625
train loss:  0.4506366550922394
train gradient:  0.12522845492053247
iteration : 721
train acc:  0.7578125
train loss:  0.47903716564178467
train gradient:  0.13931674491171891
iteration : 722
train acc:  0.7890625
train loss:  0.46609118580818176
train gradient:  0.16904669702144165
iteration : 723
train acc:  0.671875
train loss:  0.5562418103218079
train gradient:  0.15219782965159034
iteration : 724
train acc:  0.640625
train loss:  0.6334890723228455
train gradient:  0.19464103885941417
iteration : 725
train acc:  0.8046875
train loss:  0.4211198091506958
train gradient:  0.10182784328825052
iteration : 726
train acc:  0.796875
train loss:  0.4257718026638031
train gradient:  0.10703725320065746
iteration : 727
train acc:  0.734375
train loss:  0.4675189256668091
train gradient:  0.1069481492181031
iteration : 728
train acc:  0.765625
train loss:  0.45820870995521545
train gradient:  0.10975088749252375
iteration : 729
train acc:  0.8046875
train loss:  0.42058461904525757
train gradient:  0.10496454665919067
iteration : 730
train acc:  0.7421875
train loss:  0.4850670397281647
train gradient:  0.1514542352457135
iteration : 731
train acc:  0.6328125
train loss:  0.6108061075210571
train gradient:  0.1801818692801651
iteration : 732
train acc:  0.703125
train loss:  0.5203797817230225
train gradient:  0.1449956551424051
iteration : 733
train acc:  0.75
train loss:  0.48943546414375305
train gradient:  0.13326772344156668
iteration : 734
train acc:  0.7890625
train loss:  0.4798070788383484
train gradient:  0.12970938088327033
iteration : 735
train acc:  0.6875
train loss:  0.5379036664962769
train gradient:  0.13334617768740908
iteration : 736
train acc:  0.7421875
train loss:  0.4692300260066986
train gradient:  0.10820340895477848
iteration : 737
train acc:  0.8046875
train loss:  0.44349807500839233
train gradient:  0.09567615582122385
iteration : 738
train acc:  0.8359375
train loss:  0.4245149791240692
train gradient:  0.09933754707591148
iteration : 739
train acc:  0.7890625
train loss:  0.44653916358947754
train gradient:  0.10039245403424994
iteration : 740
train acc:  0.7109375
train loss:  0.4868188500404358
train gradient:  0.10360909919925233
iteration : 741
train acc:  0.71875
train loss:  0.4641110301017761
train gradient:  0.11617436836324554
iteration : 742
train acc:  0.7265625
train loss:  0.5517938137054443
train gradient:  0.17903727608221942
iteration : 743
train acc:  0.7578125
train loss:  0.5160549879074097
train gradient:  0.10177253590975147
iteration : 744
train acc:  0.734375
train loss:  0.49966827034950256
train gradient:  0.12685910263542224
iteration : 745
train acc:  0.7578125
train loss:  0.5564975738525391
train gradient:  0.21494861584645425
iteration : 746
train acc:  0.6953125
train loss:  0.5804122686386108
train gradient:  0.1464533609346192
iteration : 747
train acc:  0.7109375
train loss:  0.5395916700363159
train gradient:  0.15522838485447832
iteration : 748
train acc:  0.71875
train loss:  0.5058773756027222
train gradient:  0.12667861065330627
iteration : 749
train acc:  0.734375
train loss:  0.5041480660438538
train gradient:  0.1695214088128582
iteration : 750
train acc:  0.75
train loss:  0.4994732141494751
train gradient:  0.17234801583708742
iteration : 751
train acc:  0.734375
train loss:  0.5318989157676697
train gradient:  0.1284496539173217
iteration : 752
train acc:  0.796875
train loss:  0.44674307107925415
train gradient:  0.11595501905351953
iteration : 753
train acc:  0.765625
train loss:  0.49635049700737
train gradient:  0.13459287074134993
iteration : 754
train acc:  0.8046875
train loss:  0.4322202205657959
train gradient:  0.09884269432693292
iteration : 755
train acc:  0.78125
train loss:  0.4767071008682251
train gradient:  0.13017452890019376
iteration : 756
train acc:  0.7109375
train loss:  0.5221284031867981
train gradient:  0.15132139500204256
iteration : 757
train acc:  0.765625
train loss:  0.47669774293899536
train gradient:  0.1373408589849971
iteration : 758
train acc:  0.7421875
train loss:  0.5258973240852356
train gradient:  0.14109603736316134
iteration : 759
train acc:  0.71875
train loss:  0.5013401508331299
train gradient:  0.12344472239713777
iteration : 760
train acc:  0.7890625
train loss:  0.4101303517818451
train gradient:  0.10605738958630337
iteration : 761
train acc:  0.796875
train loss:  0.4802253842353821
train gradient:  0.11982627645316837
iteration : 762
train acc:  0.8046875
train loss:  0.42939293384552
train gradient:  0.12258259782235507
iteration : 763
train acc:  0.71875
train loss:  0.4961934983730316
train gradient:  0.12915299474332553
iteration : 764
train acc:  0.703125
train loss:  0.5201370716094971
train gradient:  0.1379975510928808
iteration : 765
train acc:  0.8046875
train loss:  0.44286787509918213
train gradient:  0.10608937491585076
iteration : 766
train acc:  0.7265625
train loss:  0.507866621017456
train gradient:  0.12287384622011181
iteration : 767
train acc:  0.765625
train loss:  0.4980362057685852
train gradient:  0.1427401247933135
iteration : 768
train acc:  0.8125
train loss:  0.42302513122558594
train gradient:  0.09885390888540371
iteration : 769
train acc:  0.765625
train loss:  0.515856146812439
train gradient:  0.1503545789545936
iteration : 770
train acc:  0.734375
train loss:  0.555462121963501
train gradient:  0.17031664550260464
iteration : 771
train acc:  0.7109375
train loss:  0.49304282665252686
train gradient:  0.11185653796404078
iteration : 772
train acc:  0.78125
train loss:  0.42590227723121643
train gradient:  0.09581648371149455
iteration : 773
train acc:  0.6875
train loss:  0.5449057221412659
train gradient:  0.16616548722189145
iteration : 774
train acc:  0.7734375
train loss:  0.4508945941925049
train gradient:  0.13237942039493095
iteration : 775
train acc:  0.6796875
train loss:  0.5769274234771729
train gradient:  0.19581514692918886
iteration : 776
train acc:  0.796875
train loss:  0.4031282961368561
train gradient:  0.08620481218788574
iteration : 777
train acc:  0.6796875
train loss:  0.5100271701812744
train gradient:  0.10809175440229715
iteration : 778
train acc:  0.765625
train loss:  0.4487205147743225
train gradient:  0.09589157923643679
iteration : 779
train acc:  0.7578125
train loss:  0.4642695188522339
train gradient:  0.10816538685472434
iteration : 780
train acc:  0.796875
train loss:  0.43677908182144165
train gradient:  0.09984157431821636
iteration : 781
train acc:  0.7109375
train loss:  0.6002864837646484
train gradient:  0.21388979470802133
iteration : 782
train acc:  0.7421875
train loss:  0.4980582594871521
train gradient:  0.13422379423846897
iteration : 783
train acc:  0.75
train loss:  0.5299040675163269
train gradient:  0.17252046818716488
iteration : 784
train acc:  0.7421875
train loss:  0.47421324253082275
train gradient:  0.12160688655942122
iteration : 785
train acc:  0.7734375
train loss:  0.49155378341674805
train gradient:  0.1358559595095618
iteration : 786
train acc:  0.7421875
train loss:  0.4989398419857025
train gradient:  0.11034409428080837
iteration : 787
train acc:  0.6796875
train loss:  0.5439784526824951
train gradient:  0.12407394708619476
iteration : 788
train acc:  0.65625
train loss:  0.6144983172416687
train gradient:  0.2239964561246153
iteration : 789
train acc:  0.703125
train loss:  0.4998858571052551
train gradient:  0.11678756848268763
iteration : 790
train acc:  0.71875
train loss:  0.46298080682754517
train gradient:  0.1075995343116869
iteration : 791
train acc:  0.8046875
train loss:  0.4064181447029114
train gradient:  0.0951102711880012
iteration : 792
train acc:  0.765625
train loss:  0.47813740372657776
train gradient:  0.12715249974299972
iteration : 793
train acc:  0.6796875
train loss:  0.5707843899726868
train gradient:  0.1800172868243902
iteration : 794
train acc:  0.78125
train loss:  0.49566730856895447
train gradient:  0.12442203919002867
iteration : 795
train acc:  0.7265625
train loss:  0.5804296135902405
train gradient:  0.17824397977530998
iteration : 796
train acc:  0.7265625
train loss:  0.5399521589279175
train gradient:  0.17581906869636088
iteration : 797
train acc:  0.7265625
train loss:  0.4792350232601166
train gradient:  0.1283210040754088
iteration : 798
train acc:  0.7578125
train loss:  0.4979037642478943
train gradient:  0.13613775382575688
iteration : 799
train acc:  0.78125
train loss:  0.4867335557937622
train gradient:  0.12703664924966157
iteration : 800
train acc:  0.7890625
train loss:  0.4605010151863098
train gradient:  0.13932648795372587
iteration : 801
train acc:  0.765625
train loss:  0.493042528629303
train gradient:  0.13786540049702595
iteration : 802
train acc:  0.734375
train loss:  0.4468339681625366
train gradient:  0.11658657261040352
iteration : 803
train acc:  0.6796875
train loss:  0.5266456604003906
train gradient:  0.16630019527271717
iteration : 804
train acc:  0.7578125
train loss:  0.45004358887672424
train gradient:  0.11443123328070946
iteration : 805
train acc:  0.75
train loss:  0.47844991087913513
train gradient:  0.11449904490658058
iteration : 806
train acc:  0.78125
train loss:  0.4817832410335541
train gradient:  0.13668659946565292
iteration : 807
train acc:  0.8203125
train loss:  0.3844146132469177
train gradient:  0.08296388333429147
iteration : 808
train acc:  0.78125
train loss:  0.44817519187927246
train gradient:  0.10832943285294125
iteration : 809
train acc:  0.6484375
train loss:  0.5573472380638123
train gradient:  0.14816843670386323
iteration : 810
train acc:  0.7578125
train loss:  0.5046929121017456
train gradient:  0.2148647846133928
iteration : 811
train acc:  0.765625
train loss:  0.4225391745567322
train gradient:  0.08301794306153276
iteration : 812
train acc:  0.71875
train loss:  0.4998400807380676
train gradient:  0.12880360052555034
iteration : 813
train acc:  0.6796875
train loss:  0.5059738159179688
train gradient:  0.17231405718720255
iteration : 814
train acc:  0.71875
train loss:  0.5161644220352173
train gradient:  0.12309226919560373
iteration : 815
train acc:  0.7109375
train loss:  0.5193238854408264
train gradient:  0.12838249310487537
iteration : 816
train acc:  0.7109375
train loss:  0.5051946043968201
train gradient:  0.13212357786937542
iteration : 817
train acc:  0.8046875
train loss:  0.4454762935638428
train gradient:  0.12723330693936824
iteration : 818
train acc:  0.796875
train loss:  0.4239974021911621
train gradient:  0.09153868687250925
iteration : 819
train acc:  0.6875
train loss:  0.4946906268596649
train gradient:  0.13408736550281636
iteration : 820
train acc:  0.7265625
train loss:  0.4485034644603729
train gradient:  0.09053442673652744
iteration : 821
train acc:  0.734375
train loss:  0.49873337149620056
train gradient:  0.13545644006898128
iteration : 822
train acc:  0.7109375
train loss:  0.5279291272163391
train gradient:  0.13543615639985324
iteration : 823
train acc:  0.75
train loss:  0.4924331307411194
train gradient:  0.125527577585781
iteration : 824
train acc:  0.7578125
train loss:  0.4846494793891907
train gradient:  0.12647211596333613
iteration : 825
train acc:  0.7578125
train loss:  0.45620203018188477
train gradient:  0.12379979600182298
iteration : 826
train acc:  0.796875
train loss:  0.4232698678970337
train gradient:  0.11425503928724719
iteration : 827
train acc:  0.75
train loss:  0.472089558839798
train gradient:  0.1455726403662148
iteration : 828
train acc:  0.734375
train loss:  0.5200466513633728
train gradient:  0.12046734445580028
iteration : 829
train acc:  0.7421875
train loss:  0.5173462629318237
train gradient:  0.13481433948788796
iteration : 830
train acc:  0.7734375
train loss:  0.48557913303375244
train gradient:  0.12449991870624424
iteration : 831
train acc:  0.6484375
train loss:  0.6201133728027344
train gradient:  0.20508698410275855
iteration : 832
train acc:  0.703125
train loss:  0.49737125635147095
train gradient:  0.12608806528759509
iteration : 833
train acc:  0.703125
train loss:  0.514236569404602
train gradient:  0.1280710949201296
iteration : 834
train acc:  0.765625
train loss:  0.489482045173645
train gradient:  0.11172482397087675
iteration : 835
train acc:  0.6953125
train loss:  0.5454584360122681
train gradient:  0.11768129939724695
iteration : 836
train acc:  0.7578125
train loss:  0.5147800445556641
train gradient:  0.17519423035266607
iteration : 837
train acc:  0.7421875
train loss:  0.506224513053894
train gradient:  0.11944854398304253
iteration : 838
train acc:  0.8046875
train loss:  0.42468464374542236
train gradient:  0.1089686700910794
iteration : 839
train acc:  0.8046875
train loss:  0.46057596802711487
train gradient:  0.11716609975317556
iteration : 840
train acc:  0.7421875
train loss:  0.47418296337127686
train gradient:  0.11344791113389956
iteration : 841
train acc:  0.71875
train loss:  0.4798748791217804
train gradient:  0.10589909313463218
iteration : 842
train acc:  0.6953125
train loss:  0.5313593745231628
train gradient:  0.1587283095528399
iteration : 843
train acc:  0.796875
train loss:  0.5093268156051636
train gradient:  0.1261931460301191
iteration : 844
train acc:  0.734375
train loss:  0.4647790193557739
train gradient:  0.09607896186338526
iteration : 845
train acc:  0.7578125
train loss:  0.5128821134567261
train gradient:  0.14030248299342046
iteration : 846
train acc:  0.8125
train loss:  0.4115987718105316
train gradient:  0.09508081544229681
iteration : 847
train acc:  0.703125
train loss:  0.6160650253295898
train gradient:  0.21238403799554578
iteration : 848
train acc:  0.765625
train loss:  0.4992084801197052
train gradient:  0.21877878644447352
iteration : 849
train acc:  0.640625
train loss:  0.5706350803375244
train gradient:  0.18106501385790658
iteration : 850
train acc:  0.8203125
train loss:  0.41035282611846924
train gradient:  0.1087122244824451
iteration : 851
train acc:  0.7734375
train loss:  0.46524733304977417
train gradient:  0.11107956796954577
iteration : 852
train acc:  0.7734375
train loss:  0.44141122698783875
train gradient:  0.11106002414754475
iteration : 853
train acc:  0.7890625
train loss:  0.43734100461006165
train gradient:  0.1062137776900855
iteration : 854
train acc:  0.6640625
train loss:  0.5243049263954163
train gradient:  0.11995657868493699
iteration : 855
train acc:  0.7578125
train loss:  0.4744071364402771
train gradient:  0.11452394525610947
iteration : 856
train acc:  0.6796875
train loss:  0.547518253326416
train gradient:  0.11573776259172767
iteration : 857
train acc:  0.6796875
train loss:  0.551598072052002
train gradient:  0.13544560058721555
iteration : 858
train acc:  0.7421875
train loss:  0.5304940938949585
train gradient:  0.1243338536378632
iteration : 859
train acc:  0.734375
train loss:  0.47475385665893555
train gradient:  0.10785384242197911
iteration : 860
train acc:  0.7265625
train loss:  0.5191866755485535
train gradient:  0.13397987095511263
iteration : 861
train acc:  0.7109375
train loss:  0.503982424736023
train gradient:  0.24146776102084175
iteration : 862
train acc:  0.796875
train loss:  0.4330425560474396
train gradient:  0.10825676268592639
iteration : 863
train acc:  0.765625
train loss:  0.4157603979110718
train gradient:  0.10747015045173447
iteration : 864
train acc:  0.7109375
train loss:  0.511787474155426
train gradient:  0.1455553864575717
iteration : 865
train acc:  0.7578125
train loss:  0.49539875984191895
train gradient:  0.18082592107501833
iteration : 866
train acc:  0.765625
train loss:  0.47959089279174805
train gradient:  0.10555518145878229
iteration : 867
train acc:  0.78125
train loss:  0.4496743083000183
train gradient:  0.13283138299921096
iteration : 868
train acc:  0.78125
train loss:  0.4148363471031189
train gradient:  0.11422807979974606
iteration : 869
train acc:  0.75
train loss:  0.48212188482284546
train gradient:  0.12808233622330553
iteration : 870
train acc:  0.7734375
train loss:  0.4526207447052002
train gradient:  0.11239470896283875
iteration : 871
train acc:  0.734375
train loss:  0.4863254129886627
train gradient:  0.12175056036012499
iteration : 872
train acc:  0.8203125
train loss:  0.4421675205230713
train gradient:  0.10216016256933279
iteration : 873
train acc:  0.7578125
train loss:  0.4745839238166809
train gradient:  0.124323408843097
iteration : 874
train acc:  0.75
train loss:  0.4788324236869812
train gradient:  0.12041528777269353
iteration : 875
train acc:  0.6953125
train loss:  0.4939979612827301
train gradient:  0.11077334933006046
iteration : 876
train acc:  0.7265625
train loss:  0.49189889430999756
train gradient:  0.18496863284439768
iteration : 877
train acc:  0.703125
train loss:  0.515625536441803
train gradient:  0.15111683599821668
iteration : 878
train acc:  0.7421875
train loss:  0.48167580366134644
train gradient:  0.1029427522823436
iteration : 879
train acc:  0.8203125
train loss:  0.4006281793117523
train gradient:  0.08886552429896583
iteration : 880
train acc:  0.7578125
train loss:  0.5037407875061035
train gradient:  0.1201851048951573
iteration : 881
train acc:  0.7734375
train loss:  0.4418170154094696
train gradient:  0.10086116979089153
iteration : 882
train acc:  0.84375
train loss:  0.34807902574539185
train gradient:  0.07456336698035018
iteration : 883
train acc:  0.75
train loss:  0.4822693169116974
train gradient:  0.12316167732169556
iteration : 884
train acc:  0.7890625
train loss:  0.4554148316383362
train gradient:  0.12492728814144821
iteration : 885
train acc:  0.75
train loss:  0.49532550573349
train gradient:  0.1447248809512391
iteration : 886
train acc:  0.796875
train loss:  0.4203275144100189
train gradient:  0.08517987394649644
iteration : 887
train acc:  0.796875
train loss:  0.477096825838089
train gradient:  0.1258353713626581
iteration : 888
train acc:  0.671875
train loss:  0.5572364330291748
train gradient:  0.15564662678008545
iteration : 889
train acc:  0.75
train loss:  0.48194053769111633
train gradient:  0.11872721811723577
iteration : 890
train acc:  0.6875
train loss:  0.5187009572982788
train gradient:  0.1279950503008489
iteration : 891
train acc:  0.734375
train loss:  0.4593504071235657
train gradient:  0.11964544567734939
iteration : 892
train acc:  0.7109375
train loss:  0.5337242484092712
train gradient:  0.1435619556538895
iteration : 893
train acc:  0.7265625
train loss:  0.4873594641685486
train gradient:  0.14672097836691944
iteration : 894
train acc:  0.7578125
train loss:  0.5052990913391113
train gradient:  0.13446578035100687
iteration : 895
train acc:  0.7265625
train loss:  0.5136867761611938
train gradient:  0.14389647112074067
iteration : 896
train acc:  0.765625
train loss:  0.46384990215301514
train gradient:  0.09581313530116961
iteration : 897
train acc:  0.75
train loss:  0.4407303035259247
train gradient:  0.09823623065121365
iteration : 898
train acc:  0.7578125
train loss:  0.4659038186073303
train gradient:  0.14872592940867618
iteration : 899
train acc:  0.734375
train loss:  0.44422006607055664
train gradient:  0.10413505597234736
iteration : 900
train acc:  0.8125
train loss:  0.40348726511001587
train gradient:  0.09255295230278492
iteration : 901
train acc:  0.7578125
train loss:  0.4203716516494751
train gradient:  0.08764508678071395
iteration : 902
train acc:  0.796875
train loss:  0.40492475032806396
train gradient:  0.0873392192168796
iteration : 903
train acc:  0.7890625
train loss:  0.4630669951438904
train gradient:  0.13096033949077224
iteration : 904
train acc:  0.7734375
train loss:  0.4606325924396515
train gradient:  0.11931518196055833
iteration : 905
train acc:  0.765625
train loss:  0.445781946182251
train gradient:  0.09131655275015543
iteration : 906
train acc:  0.6953125
train loss:  0.5774521827697754
train gradient:  0.1680946179463682
iteration : 907
train acc:  0.84375
train loss:  0.3827928304672241
train gradient:  0.085762881672067
iteration : 908
train acc:  0.78125
train loss:  0.4793761372566223
train gradient:  0.16499483726723074
iteration : 909
train acc:  0.765625
train loss:  0.46840763092041016
train gradient:  0.14113716872003085
iteration : 910
train acc:  0.71875
train loss:  0.5412694811820984
train gradient:  0.14545201754758405
iteration : 911
train acc:  0.765625
train loss:  0.5096783638000488
train gradient:  0.13300771183188398
iteration : 912
train acc:  0.75
train loss:  0.4192385673522949
train gradient:  0.09708930539565887
iteration : 913
train acc:  0.7578125
train loss:  0.5010604858398438
train gradient:  0.12289951837069713
iteration : 914
train acc:  0.7421875
train loss:  0.5024023056030273
train gradient:  0.11178124801125712
iteration : 915
train acc:  0.796875
train loss:  0.44566410779953003
train gradient:  0.13357880855981297
iteration : 916
train acc:  0.78125
train loss:  0.46700283885002136
train gradient:  0.10787196948467187
iteration : 917
train acc:  0.75
train loss:  0.4660990238189697
train gradient:  0.1632086189405727
iteration : 918
train acc:  0.703125
train loss:  0.5400470495223999
train gradient:  0.1455363978003234
iteration : 919
train acc:  0.7578125
train loss:  0.44063323736190796
train gradient:  0.11520401823016665
iteration : 920
train acc:  0.703125
train loss:  0.5116881728172302
train gradient:  0.13829987881369288
iteration : 921
train acc:  0.6875
train loss:  0.5556087493896484
train gradient:  0.1448182374315744
iteration : 922
train acc:  0.828125
train loss:  0.4288082420825958
train gradient:  0.08931167247363031
iteration : 923
train acc:  0.7109375
train loss:  0.5409365892410278
train gradient:  0.14607682274951492
iteration : 924
train acc:  0.75
train loss:  0.4811663031578064
train gradient:  0.15683194152200733
iteration : 925
train acc:  0.75
train loss:  0.46018093824386597
train gradient:  0.11532966609831828
iteration : 926
train acc:  0.75
train loss:  0.488432377576828
train gradient:  0.12741177784315988
iteration : 927
train acc:  0.8125
train loss:  0.42506837844848633
train gradient:  0.10148157613702813
iteration : 928
train acc:  0.8359375
train loss:  0.36477839946746826
train gradient:  0.07708996400569687
iteration : 929
train acc:  0.734375
train loss:  0.5052741765975952
train gradient:  0.1390925028685937
iteration : 930
train acc:  0.7578125
train loss:  0.44828343391418457
train gradient:  0.1025959886546716
iteration : 931
train acc:  0.78125
train loss:  0.48084211349487305
train gradient:  0.127627297475883
iteration : 932
train acc:  0.7109375
train loss:  0.4784025549888611
train gradient:  0.10560359520486688
iteration : 933
train acc:  0.8125
train loss:  0.4556230306625366
train gradient:  0.10624492544177445
iteration : 934
train acc:  0.71875
train loss:  0.5636550188064575
train gradient:  0.16487759673653718
iteration : 935
train acc:  0.75
train loss:  0.466308057308197
train gradient:  0.148296788426674
iteration : 936
train acc:  0.7890625
train loss:  0.43388497829437256
train gradient:  0.09728221496142905
iteration : 937
train acc:  0.7890625
train loss:  0.44305241107940674
train gradient:  0.12308351802731861
iteration : 938
train acc:  0.7421875
train loss:  0.5031366944313049
train gradient:  0.14137109299851947
iteration : 939
train acc:  0.7578125
train loss:  0.4201895594596863
train gradient:  0.08265050004092143
iteration : 940
train acc:  0.765625
train loss:  0.48422908782958984
train gradient:  0.09387099344800151
iteration : 941
train acc:  0.796875
train loss:  0.4483063817024231
train gradient:  0.11985415888073842
iteration : 942
train acc:  0.734375
train loss:  0.47694122791290283
train gradient:  0.09845070579984232
iteration : 943
train acc:  0.7734375
train loss:  0.4409262537956238
train gradient:  0.09896681164443058
iteration : 944
train acc:  0.7421875
train loss:  0.492412805557251
train gradient:  0.18589501711375822
iteration : 945
train acc:  0.7890625
train loss:  0.446557879447937
train gradient:  0.10927544948108514
iteration : 946
train acc:  0.8046875
train loss:  0.434145450592041
train gradient:  0.10946081564317138
iteration : 947
train acc:  0.703125
train loss:  0.49741336703300476
train gradient:  0.12128355846389051
iteration : 948
train acc:  0.71875
train loss:  0.5260826349258423
train gradient:  0.16798753394039653
iteration : 949
train acc:  0.734375
train loss:  0.4938919246196747
train gradient:  0.12129706278004253
iteration : 950
train acc:  0.6796875
train loss:  0.545188307762146
train gradient:  0.1570092534057001
iteration : 951
train acc:  0.84375
train loss:  0.39648866653442383
train gradient:  0.1064643068110416
iteration : 952
train acc:  0.7578125
train loss:  0.48450925946235657
train gradient:  0.1296263040032455
iteration : 953
train acc:  0.7421875
train loss:  0.5148594975471497
train gradient:  0.13846684985615215
iteration : 954
train acc:  0.78125
train loss:  0.4613562226295471
train gradient:  0.1299337783799709
iteration : 955
train acc:  0.75
train loss:  0.509307861328125
train gradient:  0.13876775136075864
iteration : 956
train acc:  0.765625
train loss:  0.4381024241447449
train gradient:  0.11044300019749934
iteration : 957
train acc:  0.6953125
train loss:  0.49927613139152527
train gradient:  0.17522397083162256
iteration : 958
train acc:  0.703125
train loss:  0.49542850255966187
train gradient:  0.16727959602805953
iteration : 959
train acc:  0.75
train loss:  0.5297266840934753
train gradient:  0.14479413791619142
iteration : 960
train acc:  0.7109375
train loss:  0.5147017240524292
train gradient:  0.13388872590603929
iteration : 961
train acc:  0.7734375
train loss:  0.4526495933532715
train gradient:  0.09618836849785688
iteration : 962
train acc:  0.703125
train loss:  0.49490073323249817
train gradient:  0.12419158063905891
iteration : 963
train acc:  0.78125
train loss:  0.41044867038726807
train gradient:  0.11064969559279257
iteration : 964
train acc:  0.7578125
train loss:  0.4603176414966583
train gradient:  0.12862513504880024
iteration : 965
train acc:  0.75
train loss:  0.4798086881637573
train gradient:  0.13331968660192536
iteration : 966
train acc:  0.7265625
train loss:  0.5504591464996338
train gradient:  0.15879890248336453
iteration : 967
train acc:  0.703125
train loss:  0.5170271992683411
train gradient:  0.11679659574822698
iteration : 968
train acc:  0.6796875
train loss:  0.5370602607727051
train gradient:  0.17953512109990272
iteration : 969
train acc:  0.71875
train loss:  0.5433547496795654
train gradient:  0.15295913849074178
iteration : 970
train acc:  0.734375
train loss:  0.4870966076850891
train gradient:  0.14987198258434462
iteration : 971
train acc:  0.78125
train loss:  0.4651334881782532
train gradient:  0.15496188137133682
iteration : 972
train acc:  0.7421875
train loss:  0.5026284456253052
train gradient:  0.12112393803051538
iteration : 973
train acc:  0.7578125
train loss:  0.456201434135437
train gradient:  0.12221285012713586
iteration : 974
train acc:  0.78125
train loss:  0.452318012714386
train gradient:  0.12024932004913451
iteration : 975
train acc:  0.7734375
train loss:  0.48595619201660156
train gradient:  0.15039646879146526
iteration : 976
train acc:  0.765625
train loss:  0.4522079527378082
train gradient:  0.11190774272411212
iteration : 977
train acc:  0.6640625
train loss:  0.578730583190918
train gradient:  0.14306598226314737
iteration : 978
train acc:  0.7265625
train loss:  0.5429781675338745
train gradient:  0.2233741943854356
iteration : 979
train acc:  0.7109375
train loss:  0.5552164316177368
train gradient:  0.14300549990255676
iteration : 980
train acc:  0.7109375
train loss:  0.5325706005096436
train gradient:  0.13360310847766116
iteration : 981
train acc:  0.78125
train loss:  0.4606393575668335
train gradient:  0.10139529086198942
iteration : 982
train acc:  0.7734375
train loss:  0.47706544399261475
train gradient:  0.09424808045108928
iteration : 983
train acc:  0.75
train loss:  0.45006683468818665
train gradient:  0.10040679777621268
iteration : 984
train acc:  0.7578125
train loss:  0.48064884543418884
train gradient:  0.11885200596557828
iteration : 985
train acc:  0.71875
train loss:  0.5092188119888306
train gradient:  0.1429815676443948
iteration : 986
train acc:  0.765625
train loss:  0.49789243936538696
train gradient:  0.1329597099683369
iteration : 987
train acc:  0.8125
train loss:  0.4181514084339142
train gradient:  0.07819361810437792
iteration : 988
train acc:  0.765625
train loss:  0.519436776638031
train gradient:  0.1368518452929477
iteration : 989
train acc:  0.7578125
train loss:  0.46907472610473633
train gradient:  0.12466851540408679
iteration : 990
train acc:  0.765625
train loss:  0.4377146363258362
train gradient:  0.10607804061320876
iteration : 991
train acc:  0.6796875
train loss:  0.511747419834137
train gradient:  0.14190706112868717
iteration : 992
train acc:  0.75
train loss:  0.46392205357551575
train gradient:  0.09788296732484345
iteration : 993
train acc:  0.765625
train loss:  0.5076342225074768
train gradient:  0.13610856506564606
iteration : 994
train acc:  0.7578125
train loss:  0.5287574529647827
train gradient:  0.16342066985464673
iteration : 995
train acc:  0.7890625
train loss:  0.49462056159973145
train gradient:  0.12917817517563998
iteration : 996
train acc:  0.7890625
train loss:  0.4474221467971802
train gradient:  0.12403619463886055
iteration : 997
train acc:  0.7578125
train loss:  0.5179362297058105
train gradient:  0.13864139610601722
iteration : 998
train acc:  0.796875
train loss:  0.4378565847873688
train gradient:  0.10968965455413371
iteration : 999
train acc:  0.734375
train loss:  0.49845314025878906
train gradient:  0.15410357076408776
iteration : 1000
train acc:  0.7890625
train loss:  0.4651052951812744
train gradient:  0.11666281893792045
iteration : 1001
train acc:  0.734375
train loss:  0.5204153060913086
train gradient:  0.17424063509958398
iteration : 1002
train acc:  0.8125
train loss:  0.44981351494789124
train gradient:  0.1204545315559205
iteration : 1003
train acc:  0.7421875
train loss:  0.5003685355186462
train gradient:  0.13564342622786574
iteration : 1004
train acc:  0.6953125
train loss:  0.5232808589935303
train gradient:  0.14664684101489528
iteration : 1005
train acc:  0.78125
train loss:  0.4413382411003113
train gradient:  0.1322098088327172
iteration : 1006
train acc:  0.7421875
train loss:  0.502665638923645
train gradient:  0.10480537532541896
iteration : 1007
train acc:  0.71875
train loss:  0.48717138171195984
train gradient:  0.14896554641331095
iteration : 1008
train acc:  0.625
train loss:  0.5796876549720764
train gradient:  0.14738402373235238
iteration : 1009
train acc:  0.765625
train loss:  0.4512726068496704
train gradient:  0.11422657195913832
iteration : 1010
train acc:  0.71875
train loss:  0.5095875859260559
train gradient:  0.1308463017879814
iteration : 1011
train acc:  0.765625
train loss:  0.45109206438064575
train gradient:  0.11743159049288844
iteration : 1012
train acc:  0.7890625
train loss:  0.43904954195022583
train gradient:  0.1353954240255914
iteration : 1013
train acc:  0.7890625
train loss:  0.4242749512195587
train gradient:  0.09828459285297705
iteration : 1014
train acc:  0.8359375
train loss:  0.4229758381843567
train gradient:  0.09183906814624716
iteration : 1015
train acc:  0.75
train loss:  0.45332881808280945
train gradient:  0.12410985556597762
iteration : 1016
train acc:  0.71875
train loss:  0.542945146560669
train gradient:  0.15689695800408093
iteration : 1017
train acc:  0.6953125
train loss:  0.5901675820350647
train gradient:  0.15418939249160196
iteration : 1018
train acc:  0.71875
train loss:  0.4822962284088135
train gradient:  0.12667074648766524
iteration : 1019
train acc:  0.703125
train loss:  0.5342941284179688
train gradient:  0.16745650300367737
iteration : 1020
train acc:  0.7890625
train loss:  0.4785366356372833
train gradient:  0.13953288270595204
iteration : 1021
train acc:  0.6796875
train loss:  0.5354320406913757
train gradient:  0.12951190136679946
iteration : 1022
train acc:  0.75
train loss:  0.47635412216186523
train gradient:  0.09397725680373882
iteration : 1023
train acc:  0.765625
train loss:  0.4611600339412689
train gradient:  0.13669193559109893
iteration : 1024
train acc:  0.671875
train loss:  0.5229241251945496
train gradient:  0.12705988346021074
iteration : 1025
train acc:  0.7578125
train loss:  0.4796070456504822
train gradient:  0.11671340079035343
iteration : 1026
train acc:  0.7265625
train loss:  0.519547700881958
train gradient:  0.15268856979942683
iteration : 1027
train acc:  0.7421875
train loss:  0.4852348268032074
train gradient:  0.13962171967159887
iteration : 1028
train acc:  0.828125
train loss:  0.3863697350025177
train gradient:  0.08815309178551078
iteration : 1029
train acc:  0.7734375
train loss:  0.44901615381240845
train gradient:  0.12171956898060296
iteration : 1030
train acc:  0.75
train loss:  0.454855740070343
train gradient:  0.10779648240266894
iteration : 1031
train acc:  0.75
train loss:  0.4978920817375183
train gradient:  0.12933070421495335
iteration : 1032
train acc:  0.7890625
train loss:  0.4530109763145447
train gradient:  0.09219086619464616
iteration : 1033
train acc:  0.7265625
train loss:  0.5150476694107056
train gradient:  0.14550827698539281
iteration : 1034
train acc:  0.6796875
train loss:  0.5241994857788086
train gradient:  0.14769360439453627
iteration : 1035
train acc:  0.796875
train loss:  0.4511864483356476
train gradient:  0.1006948128177609
iteration : 1036
train acc:  0.71875
train loss:  0.48666033148765564
train gradient:  0.16683391619945498
iteration : 1037
train acc:  0.7109375
train loss:  0.4898359775543213
train gradient:  0.13599153048959825
iteration : 1038
train acc:  0.765625
train loss:  0.46398672461509705
train gradient:  0.11033494036157977
iteration : 1039
train acc:  0.7578125
train loss:  0.44815799593925476
train gradient:  0.12817264338055925
iteration : 1040
train acc:  0.7265625
train loss:  0.5604749321937561
train gradient:  0.14748835770455876
iteration : 1041
train acc:  0.75
train loss:  0.4479597806930542
train gradient:  0.1200104422211472
iteration : 1042
train acc:  0.75
train loss:  0.4865016043186188
train gradient:  0.11956979464698143
iteration : 1043
train acc:  0.6953125
train loss:  0.5398118495941162
train gradient:  0.15345841313630257
iteration : 1044
train acc:  0.6953125
train loss:  0.5292376279830933
train gradient:  0.14811316853166834
iteration : 1045
train acc:  0.75
train loss:  0.4760993719100952
train gradient:  0.1390087428127409
iteration : 1046
train acc:  0.84375
train loss:  0.4598377048969269
train gradient:  0.10167012486600341
iteration : 1047
train acc:  0.7890625
train loss:  0.4562731087207794
train gradient:  0.1327690410014763
iteration : 1048
train acc:  0.8125
train loss:  0.4329933524131775
train gradient:  0.10265429403983924
iteration : 1049
train acc:  0.7109375
train loss:  0.474500447511673
train gradient:  0.12303877926495264
iteration : 1050
train acc:  0.7421875
train loss:  0.5458089113235474
train gradient:  0.15594170712799837
iteration : 1051
train acc:  0.7734375
train loss:  0.48216569423675537
train gradient:  0.10640883391411389
iteration : 1052
train acc:  0.734375
train loss:  0.5164955258369446
train gradient:  0.11456876178037659
iteration : 1053
train acc:  0.7578125
train loss:  0.5512242913246155
train gradient:  0.14087499650441176
iteration : 1054
train acc:  0.7109375
train loss:  0.5303119421005249
train gradient:  0.13562049812149704
iteration : 1055
train acc:  0.8125
train loss:  0.4198593199253082
train gradient:  0.09693310800846168
iteration : 1056
train acc:  0.796875
train loss:  0.43361416459083557
train gradient:  0.12705433873212033
iteration : 1057
train acc:  0.703125
train loss:  0.49443554878234863
train gradient:  0.12926666708937318
iteration : 1058
train acc:  0.7421875
train loss:  0.4798160791397095
train gradient:  0.12036455218215968
iteration : 1059
train acc:  0.734375
train loss:  0.5278213024139404
train gradient:  0.15632319795215388
iteration : 1060
train acc:  0.7578125
train loss:  0.47545546293258667
train gradient:  0.129271761067782
iteration : 1061
train acc:  0.6640625
train loss:  0.5280542969703674
train gradient:  0.12596786678735566
iteration : 1062
train acc:  0.78125
train loss:  0.46754688024520874
train gradient:  0.10873379182187856
iteration : 1063
train acc:  0.7421875
train loss:  0.49084073305130005
train gradient:  0.16069121966335015
iteration : 1064
train acc:  0.7421875
train loss:  0.49975770711898804
train gradient:  0.1424265803717733
iteration : 1065
train acc:  0.7578125
train loss:  0.5173070430755615
train gradient:  0.12466434362512212
iteration : 1066
train acc:  0.71875
train loss:  0.5108086466789246
train gradient:  0.16768407855780382
iteration : 1067
train acc:  0.78125
train loss:  0.47330811619758606
train gradient:  0.11733967872801122
iteration : 1068
train acc:  0.7734375
train loss:  0.44296902418136597
train gradient:  0.11319792974213934
iteration : 1069
train acc:  0.7109375
train loss:  0.47856131196022034
train gradient:  0.15197044987439007
iteration : 1070
train acc:  0.71875
train loss:  0.4844968914985657
train gradient:  0.1311904637218389
iteration : 1071
train acc:  0.7890625
train loss:  0.4340091347694397
train gradient:  0.09509362476014917
iteration : 1072
train acc:  0.6875
train loss:  0.5593518614768982
train gradient:  0.190398182644688
iteration : 1073
train acc:  0.7578125
train loss:  0.4921662211418152
train gradient:  0.15914655109778142
iteration : 1074
train acc:  0.7265625
train loss:  0.5515891313552856
train gradient:  0.14088151993723796
iteration : 1075
train acc:  0.7734375
train loss:  0.4317997097969055
train gradient:  0.08624539626347584
iteration : 1076
train acc:  0.75
train loss:  0.4116358458995819
train gradient:  0.08238138560885264
iteration : 1077
train acc:  0.7109375
train loss:  0.46864619851112366
train gradient:  0.11112740535456471
iteration : 1078
train acc:  0.75
train loss:  0.4872870445251465
train gradient:  0.14304686182933984
iteration : 1079
train acc:  0.796875
train loss:  0.47580084204673767
train gradient:  0.11677167778125438
iteration : 1080
train acc:  0.7578125
train loss:  0.47732576727867126
train gradient:  0.11636050202280354
iteration : 1081
train acc:  0.7421875
train loss:  0.49863359332084656
train gradient:  0.16693934326915888
iteration : 1082
train acc:  0.765625
train loss:  0.47711917757987976
train gradient:  0.13124069491532567
iteration : 1083
train acc:  0.7734375
train loss:  0.4828816056251526
train gradient:  0.12383807235273936
iteration : 1084
train acc:  0.6875
train loss:  0.5605460405349731
train gradient:  0.15962112626206887
iteration : 1085
train acc:  0.7265625
train loss:  0.5073208212852478
train gradient:  0.14350240569904787
iteration : 1086
train acc:  0.75
train loss:  0.45464980602264404
train gradient:  0.0915478009508194
iteration : 1087
train acc:  0.734375
train loss:  0.4607914090156555
train gradient:  0.12004322317334663
iteration : 1088
train acc:  0.71875
train loss:  0.5218443870544434
train gradient:  0.15544619886322447
iteration : 1089
train acc:  0.6953125
train loss:  0.5487043857574463
train gradient:  0.14417145804977957
iteration : 1090
train acc:  0.6796875
train loss:  0.5596867799758911
train gradient:  0.14516755560231082
iteration : 1091
train acc:  0.7265625
train loss:  0.4900735020637512
train gradient:  0.1658032400547516
iteration : 1092
train acc:  0.7578125
train loss:  0.4877798557281494
train gradient:  0.1414036539635942
iteration : 1093
train acc:  0.7890625
train loss:  0.46078839898109436
train gradient:  0.11560770078824928
iteration : 1094
train acc:  0.7734375
train loss:  0.5172673463821411
train gradient:  0.19484585958883863
iteration : 1095
train acc:  0.7890625
train loss:  0.4714645445346832
train gradient:  0.09027761392091306
iteration : 1096
train acc:  0.7890625
train loss:  0.41591593623161316
train gradient:  0.09307845940904955
iteration : 1097
train acc:  0.6953125
train loss:  0.5133347511291504
train gradient:  0.16758557641017302
iteration : 1098
train acc:  0.7265625
train loss:  0.5076100826263428
train gradient:  0.1474732024738975
iteration : 1099
train acc:  0.7109375
train loss:  0.5301316380500793
train gradient:  0.11560314633861739
iteration : 1100
train acc:  0.765625
train loss:  0.4475269317626953
train gradient:  0.09558811028871995
iteration : 1101
train acc:  0.7734375
train loss:  0.4421827793121338
train gradient:  0.11388097694828052
iteration : 1102
train acc:  0.6875
train loss:  0.5497603416442871
train gradient:  0.17465532255158
iteration : 1103
train acc:  0.7265625
train loss:  0.5615156292915344
train gradient:  0.1491623277479841
iteration : 1104
train acc:  0.75
train loss:  0.40504223108291626
train gradient:  0.09697098516500259
iteration : 1105
train acc:  0.78125
train loss:  0.4997364282608032
train gradient:  0.13327321581688392
iteration : 1106
train acc:  0.7421875
train loss:  0.45328038930892944
train gradient:  0.11512937223392573
iteration : 1107
train acc:  0.7734375
train loss:  0.43916499614715576
train gradient:  0.10487946806334489
iteration : 1108
train acc:  0.71875
train loss:  0.5202810764312744
train gradient:  0.13917338263509715
iteration : 1109
train acc:  0.7578125
train loss:  0.4199450612068176
train gradient:  0.08928307931943309
iteration : 1110
train acc:  0.7578125
train loss:  0.49823105335235596
train gradient:  0.11767642031375668
iteration : 1111
train acc:  0.6875
train loss:  0.5418819189071655
train gradient:  0.1511542024256014
iteration : 1112
train acc:  0.71875
train loss:  0.4908464550971985
train gradient:  0.11214222563282691
iteration : 1113
train acc:  0.640625
train loss:  0.5766277313232422
train gradient:  0.2001173121196592
iteration : 1114
train acc:  0.734375
train loss:  0.5298080444335938
train gradient:  0.14387556794223877
iteration : 1115
train acc:  0.7890625
train loss:  0.4660462439060211
train gradient:  0.12257772115622474
iteration : 1116
train acc:  0.8203125
train loss:  0.4715714454650879
train gradient:  0.13707222616588077
iteration : 1117
train acc:  0.671875
train loss:  0.5559389591217041
train gradient:  0.15279304135813698
iteration : 1118
train acc:  0.6484375
train loss:  0.6188110709190369
train gradient:  0.1811485667455594
iteration : 1119
train acc:  0.7578125
train loss:  0.44525089859962463
train gradient:  0.11432247330673201
iteration : 1120
train acc:  0.7421875
train loss:  0.46127116680145264
train gradient:  0.09487178316211127
iteration : 1121
train acc:  0.7421875
train loss:  0.44994765520095825
train gradient:  0.10764102570708527
iteration : 1122
train acc:  0.7578125
train loss:  0.46677643060684204
train gradient:  0.10340258856782847
iteration : 1123
train acc:  0.828125
train loss:  0.4323362410068512
train gradient:  0.12550486136512254
iteration : 1124
train acc:  0.8203125
train loss:  0.4223496913909912
train gradient:  0.08901392668388478
iteration : 1125
train acc:  0.734375
train loss:  0.45272746682167053
train gradient:  0.11168843684918076
iteration : 1126
train acc:  0.6796875
train loss:  0.5368402004241943
train gradient:  0.14062305337839137
iteration : 1127
train acc:  0.7421875
train loss:  0.5005296468734741
train gradient:  0.12502593394372757
iteration : 1128
train acc:  0.7578125
train loss:  0.46295851469039917
train gradient:  0.1356528216699528
iteration : 1129
train acc:  0.7421875
train loss:  0.5723263025283813
train gradient:  0.17112412704813687
iteration : 1130
train acc:  0.796875
train loss:  0.4150925874710083
train gradient:  0.1081066650251062
iteration : 1131
train acc:  0.6953125
train loss:  0.551162838935852
train gradient:  0.14781499128283115
iteration : 1132
train acc:  0.765625
train loss:  0.48512253165245056
train gradient:  0.12295668937026097
iteration : 1133
train acc:  0.7109375
train loss:  0.5193431377410889
train gradient:  0.12078380505710049
iteration : 1134
train acc:  0.7421875
train loss:  0.5217898488044739
train gradient:  0.15895224590664125
iteration : 1135
train acc:  0.7265625
train loss:  0.5156967043876648
train gradient:  0.1336153393256067
iteration : 1136
train acc:  0.7421875
train loss:  0.5530322194099426
train gradient:  0.1478583034161991
iteration : 1137
train acc:  0.765625
train loss:  0.43044257164001465
train gradient:  0.09141130363559818
iteration : 1138
train acc:  0.765625
train loss:  0.49916884303092957
train gradient:  0.10540029956190433
iteration : 1139
train acc:  0.6875
train loss:  0.5342387557029724
train gradient:  0.14117741297935246
iteration : 1140
train acc:  0.6953125
train loss:  0.45908743143081665
train gradient:  0.09766739287725114
iteration : 1141
train acc:  0.7109375
train loss:  0.4950217306613922
train gradient:  0.1123452497131661
iteration : 1142
train acc:  0.734375
train loss:  0.5257340669631958
train gradient:  0.1501546866689159
iteration : 1143
train acc:  0.765625
train loss:  0.49285340309143066
train gradient:  0.11427010169673928
iteration : 1144
train acc:  0.7265625
train loss:  0.48514124751091003
train gradient:  0.12794815335665927
iteration : 1145
train acc:  0.7109375
train loss:  0.49653613567352295
train gradient:  0.11855213154864683
iteration : 1146
train acc:  0.75
train loss:  0.46091294288635254
train gradient:  0.1156846548868441
iteration : 1147
train acc:  0.84375
train loss:  0.4159712791442871
train gradient:  0.07171942097379594
iteration : 1148
train acc:  0.7890625
train loss:  0.4588114023208618
train gradient:  0.1305365824566021
iteration : 1149
train acc:  0.7734375
train loss:  0.4365406632423401
train gradient:  0.08465781398184243
iteration : 1150
train acc:  0.78125
train loss:  0.4674642086029053
train gradient:  0.1207488535712816
iteration : 1151
train acc:  0.796875
train loss:  0.47351324558258057
train gradient:  0.11650237913036797
iteration : 1152
train acc:  0.8046875
train loss:  0.41475412249565125
train gradient:  0.10084148663227746
iteration : 1153
train acc:  0.7265625
train loss:  0.49812155961990356
train gradient:  0.13334398233362216
iteration : 1154
train acc:  0.7578125
train loss:  0.517351508140564
train gradient:  0.24098103890669284
iteration : 1155
train acc:  0.6640625
train loss:  0.6105620861053467
train gradient:  0.15395040882724595
iteration : 1156
train acc:  0.71875
train loss:  0.5165616273880005
train gradient:  0.15362291423629315
iteration : 1157
train acc:  0.7578125
train loss:  0.46929770708084106
train gradient:  0.10461844771935021
iteration : 1158
train acc:  0.8125
train loss:  0.4215444028377533
train gradient:  0.08430915364026755
iteration : 1159
train acc:  0.6484375
train loss:  0.6191463470458984
train gradient:  0.18510680105605604
iteration : 1160
train acc:  0.7421875
train loss:  0.515600323677063
train gradient:  0.12320335217813304
iteration : 1161
train acc:  0.8203125
train loss:  0.44962868094444275
train gradient:  0.1005154250515511
iteration : 1162
train acc:  0.734375
train loss:  0.5129770040512085
train gradient:  0.1339283170423092
iteration : 1163
train acc:  0.7109375
train loss:  0.48544880747795105
train gradient:  0.1245249208394647
iteration : 1164
train acc:  0.734375
train loss:  0.5267662405967712
train gradient:  0.16727505785204372
iteration : 1165
train acc:  0.8125
train loss:  0.4302181899547577
train gradient:  0.09170459154305532
iteration : 1166
train acc:  0.7421875
train loss:  0.5351545810699463
train gradient:  0.12181350821626889
iteration : 1167
train acc:  0.6796875
train loss:  0.5394487380981445
train gradient:  0.1493483787222541
iteration : 1168
train acc:  0.75
train loss:  0.480252742767334
train gradient:  0.13219361808663954
iteration : 1169
train acc:  0.7265625
train loss:  0.49599504470825195
train gradient:  0.13660700419623462
iteration : 1170
train acc:  0.640625
train loss:  0.6328352689743042
train gradient:  0.27930768892172175
iteration : 1171
train acc:  0.7265625
train loss:  0.5317257046699524
train gradient:  0.11575078679853895
iteration : 1172
train acc:  0.734375
train loss:  0.5235458016395569
train gradient:  0.16391711286591026
iteration : 1173
train acc:  0.796875
train loss:  0.4195516109466553
train gradient:  0.09836609077475168
iteration : 1174
train acc:  0.7578125
train loss:  0.4371485710144043
train gradient:  0.09364716403044555
iteration : 1175
train acc:  0.7265625
train loss:  0.4774736166000366
train gradient:  0.11117215430816943
iteration : 1176
train acc:  0.7421875
train loss:  0.465900719165802
train gradient:  0.10570547192462519
iteration : 1177
train acc:  0.7421875
train loss:  0.4878700375556946
train gradient:  0.13477275223900334
iteration : 1178
train acc:  0.71875
train loss:  0.48759570717811584
train gradient:  0.13565005408014408
iteration : 1179
train acc:  0.71875
train loss:  0.49041321873664856
train gradient:  0.1091841177095694
iteration : 1180
train acc:  0.78125
train loss:  0.44447165727615356
train gradient:  0.13596533195216964
iteration : 1181
train acc:  0.765625
train loss:  0.4641040861606598
train gradient:  0.08818889045857665
iteration : 1182
train acc:  0.765625
train loss:  0.5288988947868347
train gradient:  0.1218186800463144
iteration : 1183
train acc:  0.796875
train loss:  0.42565956711769104
train gradient:  0.1643926561682405
iteration : 1184
train acc:  0.71875
train loss:  0.5287062525749207
train gradient:  0.14347106125419523
iteration : 1185
train acc:  0.7265625
train loss:  0.5340026617050171
train gradient:  0.12263105419452387
iteration : 1186
train acc:  0.734375
train loss:  0.5388974547386169
train gradient:  0.1695262950909104
iteration : 1187
train acc:  0.7265625
train loss:  0.4818139672279358
train gradient:  0.10261308859839181
iteration : 1188
train acc:  0.765625
train loss:  0.452919065952301
train gradient:  0.11667717787807574
iteration : 1189
train acc:  0.78125
train loss:  0.43240904808044434
train gradient:  0.11012838462485129
iteration : 1190
train acc:  0.65625
train loss:  0.5534180402755737
train gradient:  0.15117290052653673
iteration : 1191
train acc:  0.7421875
train loss:  0.4591420888900757
train gradient:  0.10148013564629328
iteration : 1192
train acc:  0.7421875
train loss:  0.5431109666824341
train gradient:  0.14799780197171894
iteration : 1193
train acc:  0.7734375
train loss:  0.45448657870292664
train gradient:  0.1215625708544074
iteration : 1194
train acc:  0.7265625
train loss:  0.47461172938346863
train gradient:  0.12408985435234167
iteration : 1195
train acc:  0.7890625
train loss:  0.4227674901485443
train gradient:  0.09164378468798572
iteration : 1196
train acc:  0.734375
train loss:  0.5264381170272827
train gradient:  0.13934520840424586
iteration : 1197
train acc:  0.6796875
train loss:  0.5800826549530029
train gradient:  0.17934706806503048
iteration : 1198
train acc:  0.734375
train loss:  0.4963735044002533
train gradient:  0.09265791351449694
iteration : 1199
train acc:  0.78125
train loss:  0.4793281555175781
train gradient:  0.11097989864756536
iteration : 1200
train acc:  0.734375
train loss:  0.5223740935325623
train gradient:  0.15213781410519886
iteration : 1201
train acc:  0.7265625
train loss:  0.4829234778881073
train gradient:  0.11007705600299385
iteration : 1202
train acc:  0.75
train loss:  0.4893403649330139
train gradient:  0.13246259945446912
iteration : 1203
train acc:  0.734375
train loss:  0.5302859544754028
train gradient:  0.12370651734729615
iteration : 1204
train acc:  0.78125
train loss:  0.4424576759338379
train gradient:  0.11695402300192874
iteration : 1205
train acc:  0.796875
train loss:  0.4338087737560272
train gradient:  0.10417745421285503
iteration : 1206
train acc:  0.8125
train loss:  0.4424806237220764
train gradient:  0.084909327838217
iteration : 1207
train acc:  0.7734375
train loss:  0.49381598830223083
train gradient:  0.12811745862117055
iteration : 1208
train acc:  0.75
train loss:  0.5022633671760559
train gradient:  0.13789600508218253
iteration : 1209
train acc:  0.7421875
train loss:  0.4667656719684601
train gradient:  0.11646534633100943
iteration : 1210
train acc:  0.8203125
train loss:  0.4029380679130554
train gradient:  0.10222720316968976
iteration : 1211
train acc:  0.78125
train loss:  0.47422879934310913
train gradient:  0.11135969258273737
iteration : 1212
train acc:  0.78125
train loss:  0.4677479863166809
train gradient:  0.12188344862922317
iteration : 1213
train acc:  0.7265625
train loss:  0.5229397416114807
train gradient:  0.17314043926207084
iteration : 1214
train acc:  0.703125
train loss:  0.5566088557243347
train gradient:  0.1458220468386719
iteration : 1215
train acc:  0.703125
train loss:  0.556853175163269
train gradient:  0.1366914792416162
iteration : 1216
train acc:  0.7421875
train loss:  0.5319036245346069
train gradient:  0.14365393078882308
iteration : 1217
train acc:  0.765625
train loss:  0.47031325101852417
train gradient:  0.12913262835563177
iteration : 1218
train acc:  0.765625
train loss:  0.44625627994537354
train gradient:  0.09508762975984329
iteration : 1219
train acc:  0.78125
train loss:  0.42664775252342224
train gradient:  0.11828653535314218
iteration : 1220
train acc:  0.765625
train loss:  0.45392489433288574
train gradient:  0.09563841116387045
iteration : 1221
train acc:  0.7421875
train loss:  0.49580129981040955
train gradient:  0.13338717887277124
iteration : 1222
train acc:  0.7578125
train loss:  0.5275346040725708
train gradient:  0.16036867473278912
iteration : 1223
train acc:  0.8125
train loss:  0.4300360679626465
train gradient:  0.08196699614694564
iteration : 1224
train acc:  0.75
train loss:  0.4336107671260834
train gradient:  0.0975504035571195
iteration : 1225
train acc:  0.7421875
train loss:  0.4929006099700928
train gradient:  0.10803981573136286
iteration : 1226
train acc:  0.7734375
train loss:  0.43170416355133057
train gradient:  0.10075849165623627
iteration : 1227
train acc:  0.6796875
train loss:  0.5482386350631714
train gradient:  0.14782124780566708
iteration : 1228
train acc:  0.796875
train loss:  0.4443012475967407
train gradient:  0.11687223719681364
iteration : 1229
train acc:  0.7578125
train loss:  0.4666360318660736
train gradient:  0.11294879163355183
iteration : 1230
train acc:  0.765625
train loss:  0.513848066329956
train gradient:  0.15542597430426325
iteration : 1231
train acc:  0.71875
train loss:  0.5451491475105286
train gradient:  0.1466322518221348
iteration : 1232
train acc:  0.7578125
train loss:  0.4965096712112427
train gradient:  0.16598993423282077
iteration : 1233
train acc:  0.703125
train loss:  0.5448900461196899
train gradient:  0.11644759793179156
iteration : 1234
train acc:  0.765625
train loss:  0.4510176181793213
train gradient:  0.11535202719619814
iteration : 1235
train acc:  0.6875
train loss:  0.5644267797470093
train gradient:  0.1320736002292749
iteration : 1236
train acc:  0.734375
train loss:  0.49445852637290955
train gradient:  0.1221108487931595
iteration : 1237
train acc:  0.6953125
train loss:  0.5158576369285583
train gradient:  0.15043275207891715
iteration : 1238
train acc:  0.7109375
train loss:  0.4954518973827362
train gradient:  0.1178926992136809
iteration : 1239
train acc:  0.6953125
train loss:  0.5088258981704712
train gradient:  0.13397226515272187
iteration : 1240
train acc:  0.78125
train loss:  0.43170708417892456
train gradient:  0.08616686401562085
iteration : 1241
train acc:  0.765625
train loss:  0.48170527815818787
train gradient:  0.1220270634951947
iteration : 1242
train acc:  0.765625
train loss:  0.45751598477363586
train gradient:  0.1270703270339226
iteration : 1243
train acc:  0.78125
train loss:  0.4839821457862854
train gradient:  0.12033221620498633
iteration : 1244
train acc:  0.7734375
train loss:  0.4661853313446045
train gradient:  0.11317834765321914
iteration : 1245
train acc:  0.8046875
train loss:  0.4155360460281372
train gradient:  0.1425939359810855
iteration : 1246
train acc:  0.796875
train loss:  0.46374696493148804
train gradient:  0.15580817197214705
iteration : 1247
train acc:  0.7421875
train loss:  0.5285775661468506
train gradient:  0.1454275335742985
iteration : 1248
train acc:  0.734375
train loss:  0.47437381744384766
train gradient:  0.1425553249790078
iteration : 1249
train acc:  0.7421875
train loss:  0.545365571975708
train gradient:  0.1689701068569383
iteration : 1250
train acc:  0.7890625
train loss:  0.45480281114578247
train gradient:  0.10294372681182273
iteration : 1251
train acc:  0.78125
train loss:  0.444674015045166
train gradient:  0.11102060117534758
iteration : 1252
train acc:  0.7265625
train loss:  0.5270384550094604
train gradient:  0.12428642847763333
iteration : 1253
train acc:  0.796875
train loss:  0.4596864879131317
train gradient:  0.11951803217384697
iteration : 1254
train acc:  0.796875
train loss:  0.47176045179367065
train gradient:  0.12838842577758208
iteration : 1255
train acc:  0.8203125
train loss:  0.4172780513763428
train gradient:  0.1244365165813514
iteration : 1256
train acc:  0.78125
train loss:  0.46664726734161377
train gradient:  0.1244833864451431
iteration : 1257
train acc:  0.828125
train loss:  0.3787919878959656
train gradient:  0.07241304826575103
iteration : 1258
train acc:  0.78125
train loss:  0.4811645746231079
train gradient:  0.11456896381381348
iteration : 1259
train acc:  0.703125
train loss:  0.5553703904151917
train gradient:  0.14654211921970423
iteration : 1260
train acc:  0.828125
train loss:  0.40594804286956787
train gradient:  0.09665385752482865
iteration : 1261
train acc:  0.796875
train loss:  0.427558571100235
train gradient:  0.09654873937545275
iteration : 1262
train acc:  0.734375
train loss:  0.4902786612510681
train gradient:  0.11214661545043345
iteration : 1263
train acc:  0.7109375
train loss:  0.4881301522254944
train gradient:  0.11283293870636024
iteration : 1264
train acc:  0.796875
train loss:  0.4616042673587799
train gradient:  0.11193276651773451
iteration : 1265
train acc:  0.75
train loss:  0.5004743933677673
train gradient:  0.12543547303693586
iteration : 1266
train acc:  0.7890625
train loss:  0.4148657023906708
train gradient:  0.10823029422421075
iteration : 1267
train acc:  0.7265625
train loss:  0.4751341938972473
train gradient:  0.09924671941808985
iteration : 1268
train acc:  0.8046875
train loss:  0.4326504170894623
train gradient:  0.1412836755431936
iteration : 1269
train acc:  0.796875
train loss:  0.4362434148788452
train gradient:  0.123238894398503
iteration : 1270
train acc:  0.703125
train loss:  0.5180121064186096
train gradient:  0.11975100242840857
iteration : 1271
train acc:  0.71875
train loss:  0.49645957350730896
train gradient:  0.11486685760454177
iteration : 1272
train acc:  0.7421875
train loss:  0.4626294672489166
train gradient:  0.13399962954981245
iteration : 1273
train acc:  0.78125
train loss:  0.4612070918083191
train gradient:  0.10239531066278161
iteration : 1274
train acc:  0.6484375
train loss:  0.5354324579238892
train gradient:  0.12985823458377904
iteration : 1275
train acc:  0.71875
train loss:  0.5083174705505371
train gradient:  0.12341142021444683
iteration : 1276
train acc:  0.7890625
train loss:  0.4149632751941681
train gradient:  0.09391095091116479
iteration : 1277
train acc:  0.7890625
train loss:  0.49611032009124756
train gradient:  0.1327844716372243
iteration : 1278
train acc:  0.765625
train loss:  0.4765382409095764
train gradient:  0.14235764811838603
iteration : 1279
train acc:  0.734375
train loss:  0.4893074631690979
train gradient:  0.10663236543393616
iteration : 1280
train acc:  0.7421875
train loss:  0.5057250261306763
train gradient:  0.1629367327359375
iteration : 1281
train acc:  0.7265625
train loss:  0.4454098343849182
train gradient:  0.11918995044087675
iteration : 1282
train acc:  0.7734375
train loss:  0.45650923252105713
train gradient:  0.12728709763622403
iteration : 1283
train acc:  0.7734375
train loss:  0.444260835647583
train gradient:  0.09376114146058294
iteration : 1284
train acc:  0.7734375
train loss:  0.515735924243927
train gradient:  0.12287506288552939
iteration : 1285
train acc:  0.71875
train loss:  0.5328198671340942
train gradient:  0.15912195905178692
iteration : 1286
train acc:  0.6484375
train loss:  0.5318501591682434
train gradient:  0.13887743970082073
iteration : 1287
train acc:  0.7109375
train loss:  0.517546534538269
train gradient:  0.1347935087799223
iteration : 1288
train acc:  0.7421875
train loss:  0.48491591215133667
train gradient:  0.13856727118720374
iteration : 1289
train acc:  0.734375
train loss:  0.5139641165733337
train gradient:  0.11600249226150575
iteration : 1290
train acc:  0.71875
train loss:  0.48224225640296936
train gradient:  0.12531707921737056
iteration : 1291
train acc:  0.78125
train loss:  0.47424596548080444
train gradient:  0.10668984656559778
iteration : 1292
train acc:  0.703125
train loss:  0.4573262929916382
train gradient:  0.09458403544097228
iteration : 1293
train acc:  0.7890625
train loss:  0.4578571319580078
train gradient:  0.1149130865828357
iteration : 1294
train acc:  0.78125
train loss:  0.4984639883041382
train gradient:  0.13650954053390107
iteration : 1295
train acc:  0.828125
train loss:  0.41041237115859985
train gradient:  0.09529090777321284
iteration : 1296
train acc:  0.8359375
train loss:  0.4025439918041229
train gradient:  0.08143617613941886
iteration : 1297
train acc:  0.7890625
train loss:  0.4688059091567993
train gradient:  0.10189794647102009
iteration : 1298
train acc:  0.78125
train loss:  0.4390599727630615
train gradient:  0.09269010629232849
iteration : 1299
train acc:  0.8046875
train loss:  0.4393722414970398
train gradient:  0.10035271661265455
iteration : 1300
train acc:  0.765625
train loss:  0.4662950336933136
train gradient:  0.09763693249724009
iteration : 1301
train acc:  0.8515625
train loss:  0.3949684500694275
train gradient:  0.07523930946944146
iteration : 1302
train acc:  0.734375
train loss:  0.48941293358802795
train gradient:  0.13080142808698184
iteration : 1303
train acc:  0.734375
train loss:  0.5063455104827881
train gradient:  0.16250872815254125
iteration : 1304
train acc:  0.6875
train loss:  0.5449498295783997
train gradient:  0.15012031030302256
iteration : 1305
train acc:  0.71875
train loss:  0.48907554149627686
train gradient:  0.11443163897098237
iteration : 1306
train acc:  0.7890625
train loss:  0.4112846553325653
train gradient:  0.1144058022417128
iteration : 1307
train acc:  0.7578125
train loss:  0.4692240357398987
train gradient:  0.09111568163426606
iteration : 1308
train acc:  0.7421875
train loss:  0.48220905661582947
train gradient:  0.11791368952538672
iteration : 1309
train acc:  0.6875
train loss:  0.5380890369415283
train gradient:  0.17287791440809291
iteration : 1310
train acc:  0.734375
train loss:  0.49736538529396057
train gradient:  0.1256126518674381
iteration : 1311
train acc:  0.6953125
train loss:  0.5330557227134705
train gradient:  0.15979732006659736
iteration : 1312
train acc:  0.8046875
train loss:  0.44293680787086487
train gradient:  0.09916932864424938
iteration : 1313
train acc:  0.7734375
train loss:  0.4739445745944977
train gradient:  0.11364169697021374
iteration : 1314
train acc:  0.7265625
train loss:  0.45644640922546387
train gradient:  0.10361136320691318
iteration : 1315
train acc:  0.7265625
train loss:  0.47790074348449707
train gradient:  0.110804345009724
iteration : 1316
train acc:  0.75
train loss:  0.4861891269683838
train gradient:  0.1542209850936302
iteration : 1317
train acc:  0.8046875
train loss:  0.4202697277069092
train gradient:  0.07448343082481489
iteration : 1318
train acc:  0.7421875
train loss:  0.46450674533843994
train gradient:  0.12145453361292095
iteration : 1319
train acc:  0.78125
train loss:  0.4715206027030945
train gradient:  0.10600927918786726
iteration : 1320
train acc:  0.7734375
train loss:  0.47304874658584595
train gradient:  0.12637693452014842
iteration : 1321
train acc:  0.7109375
train loss:  0.5777473449707031
train gradient:  0.1451207082327166
iteration : 1322
train acc:  0.78125
train loss:  0.48128944635391235
train gradient:  0.1709423634861031
iteration : 1323
train acc:  0.734375
train loss:  0.5202017426490784
train gradient:  0.16185784016318205
iteration : 1324
train acc:  0.71875
train loss:  0.5188140869140625
train gradient:  0.13315856508484744
iteration : 1325
train acc:  0.75
train loss:  0.4451596736907959
train gradient:  0.09950238960388662
iteration : 1326
train acc:  0.71875
train loss:  0.5160781145095825
train gradient:  0.13714516236136176
iteration : 1327
train acc:  0.765625
train loss:  0.46856996417045593
train gradient:  0.12128302157000424
iteration : 1328
train acc:  0.6953125
train loss:  0.6009347438812256
train gradient:  0.14672024947710371
iteration : 1329
train acc:  0.78125
train loss:  0.4396648406982422
train gradient:  0.09017393611687133
iteration : 1330
train acc:  0.7578125
train loss:  0.4600643515586853
train gradient:  0.11484612944169369
iteration : 1331
train acc:  0.75
train loss:  0.4662809669971466
train gradient:  0.1551595849846435
iteration : 1332
train acc:  0.7578125
train loss:  0.466701477766037
train gradient:  0.13076045637209027
iteration : 1333
train acc:  0.6875
train loss:  0.5591168999671936
train gradient:  0.17171068249861443
iteration : 1334
train acc:  0.828125
train loss:  0.41246935725212097
train gradient:  0.10140969928802783
iteration : 1335
train acc:  0.7890625
train loss:  0.472675621509552
train gradient:  0.12858030160663067
iteration : 1336
train acc:  0.75
train loss:  0.5039379000663757
train gradient:  0.1705424869686833
iteration : 1337
train acc:  0.8046875
train loss:  0.4114024341106415
train gradient:  0.14719608234216716
iteration : 1338
train acc:  0.7421875
train loss:  0.5031676292419434
train gradient:  0.13507236397732456
iteration : 1339
train acc:  0.703125
train loss:  0.5339735746383667
train gradient:  0.11985113083057293
iteration : 1340
train acc:  0.796875
train loss:  0.45147398114204407
train gradient:  0.13014687540838754
iteration : 1341
train acc:  0.6953125
train loss:  0.5180462002754211
train gradient:  0.13962877002880453
iteration : 1342
train acc:  0.7578125
train loss:  0.46454304456710815
train gradient:  0.12453203790525813
iteration : 1343
train acc:  0.7421875
train loss:  0.4514063596725464
train gradient:  0.11021268817294302
iteration : 1344
train acc:  0.765625
train loss:  0.4379209578037262
train gradient:  0.10000648990761338
iteration : 1345
train acc:  0.765625
train loss:  0.447643518447876
train gradient:  0.10035576686413865
iteration : 1346
train acc:  0.75
train loss:  0.5162540674209595
train gradient:  0.1199415196760862
iteration : 1347
train acc:  0.7265625
train loss:  0.5345811247825623
train gradient:  0.13246332017302312
iteration : 1348
train acc:  0.6875
train loss:  0.5272976756095886
train gradient:  0.11781246494722108
iteration : 1349
train acc:  0.7890625
train loss:  0.436232328414917
train gradient:  0.1264263348910377
iteration : 1350
train acc:  0.7734375
train loss:  0.41626256704330444
train gradient:  0.12313134270527608
iteration : 1351
train acc:  0.765625
train loss:  0.4839302599430084
train gradient:  0.1411236216279117
iteration : 1352
train acc:  0.6953125
train loss:  0.5179229378700256
train gradient:  0.13437276950243815
iteration : 1353
train acc:  0.7578125
train loss:  0.4707280397415161
train gradient:  0.12503590907348866
iteration : 1354
train acc:  0.765625
train loss:  0.4768555164337158
train gradient:  0.11527025680279114
iteration : 1355
train acc:  0.8046875
train loss:  0.40178200602531433
train gradient:  0.08653313007180134
iteration : 1356
train acc:  0.7421875
train loss:  0.485532283782959
train gradient:  0.13372038738525888
iteration : 1357
train acc:  0.7109375
train loss:  0.4856569468975067
train gradient:  0.11495112783747982
iteration : 1358
train acc:  0.78125
train loss:  0.4517360329627991
train gradient:  0.16943103739757776
iteration : 1359
train acc:  0.71875
train loss:  0.4830918312072754
train gradient:  0.1161927676568058
iteration : 1360
train acc:  0.75
train loss:  0.5150141716003418
train gradient:  0.15988706939445352
iteration : 1361
train acc:  0.703125
train loss:  0.5733432173728943
train gradient:  0.16659168868799562
iteration : 1362
train acc:  0.78125
train loss:  0.497808039188385
train gradient:  0.12047541820946658
iteration : 1363
train acc:  0.7734375
train loss:  0.45856326818466187
train gradient:  0.08810029323217008
iteration : 1364
train acc:  0.6875
train loss:  0.5555230379104614
train gradient:  0.16415678889585242
iteration : 1365
train acc:  0.7734375
train loss:  0.4749971926212311
train gradient:  0.1204920945562667
iteration : 1366
train acc:  0.8359375
train loss:  0.40629956126213074
train gradient:  0.09209783663867831
iteration : 1367
train acc:  0.75
train loss:  0.5080227255821228
train gradient:  0.12517413113670317
iteration : 1368
train acc:  0.7109375
train loss:  0.5778011679649353
train gradient:  0.17792639657673984
iteration : 1369
train acc:  0.7890625
train loss:  0.47086918354034424
train gradient:  0.1142363355943917
iteration : 1370
train acc:  0.8125
train loss:  0.42298829555511475
train gradient:  0.08739140768546542
iteration : 1371
train acc:  0.75
train loss:  0.45894068479537964
train gradient:  0.11192564368721532
iteration : 1372
train acc:  0.75
train loss:  0.5219013690948486
train gradient:  0.1331028819515459
iteration : 1373
train acc:  0.671875
train loss:  0.5134101510047913
train gradient:  0.11925723171277568
iteration : 1374
train acc:  0.7421875
train loss:  0.45780980587005615
train gradient:  0.11610680439546264
iteration : 1375
train acc:  0.75
train loss:  0.4406058192253113
train gradient:  0.09941444266263556
iteration : 1376
train acc:  0.765625
train loss:  0.4700356721878052
train gradient:  0.15714367283248404
iteration : 1377
train acc:  0.765625
train loss:  0.5239207148551941
train gradient:  0.11613945216378162
iteration : 1378
train acc:  0.7890625
train loss:  0.43242743611335754
train gradient:  0.10262540877029601
iteration : 1379
train acc:  0.6796875
train loss:  0.5856804251670837
train gradient:  0.17529497815355183
iteration : 1380
train acc:  0.7109375
train loss:  0.520349383354187
train gradient:  0.11913812884290155
iteration : 1381
train acc:  0.7578125
train loss:  0.4643341898918152
train gradient:  0.08149111030382922
iteration : 1382
train acc:  0.765625
train loss:  0.47298458218574524
train gradient:  0.1239801355429446
iteration : 1383
train acc:  0.71875
train loss:  0.5537353754043579
train gradient:  0.1496736600241813
iteration : 1384
train acc:  0.8046875
train loss:  0.46420055627822876
train gradient:  0.1267187787888176
iteration : 1385
train acc:  0.7734375
train loss:  0.4806991219520569
train gradient:  0.13164702327737407
iteration : 1386
train acc:  0.6953125
train loss:  0.5473619103431702
train gradient:  0.13360720703024936
iteration : 1387
train acc:  0.7421875
train loss:  0.46303901076316833
train gradient:  0.16009221011816105
iteration : 1388
train acc:  0.7578125
train loss:  0.4878094792366028
train gradient:  0.12678405265338946
iteration : 1389
train acc:  0.7578125
train loss:  0.4823288023471832
train gradient:  0.11610268356995145
iteration : 1390
train acc:  0.734375
train loss:  0.5151971578598022
train gradient:  0.14101971729583707
iteration : 1391
train acc:  0.7421875
train loss:  0.5325577259063721
train gradient:  0.13942082943146988
iteration : 1392
train acc:  0.8046875
train loss:  0.4549623429775238
train gradient:  0.09945093165627959
iteration : 1393
train acc:  0.765625
train loss:  0.4911904036998749
train gradient:  0.12876164917147664
iteration : 1394
train acc:  0.7578125
train loss:  0.45767536759376526
train gradient:  0.11002123015265433
iteration : 1395
train acc:  0.78125
train loss:  0.4648100733757019
train gradient:  0.1509166098137359
iteration : 1396
train acc:  0.671875
train loss:  0.5355515480041504
train gradient:  0.1543296673716018
iteration : 1397
train acc:  0.765625
train loss:  0.46680131554603577
train gradient:  0.10390233873134556
iteration : 1398
train acc:  0.7578125
train loss:  0.45669272541999817
train gradient:  0.10409148116099004
iteration : 1399
train acc:  0.7890625
train loss:  0.45602285861968994
train gradient:  0.13073695732421028
iteration : 1400
train acc:  0.8359375
train loss:  0.4405874013900757
train gradient:  0.13976478662677255
iteration : 1401
train acc:  0.8125
train loss:  0.42470771074295044
train gradient:  0.13864511733131424
iteration : 1402
train acc:  0.75
train loss:  0.4849643111228943
train gradient:  0.12841070934487955
iteration : 1403
train acc:  0.6796875
train loss:  0.5105440020561218
train gradient:  0.1570628810180274
iteration : 1404
train acc:  0.7265625
train loss:  0.5091301202774048
train gradient:  0.11518845926037136
iteration : 1405
train acc:  0.75
train loss:  0.5205880403518677
train gradient:  0.14054576762049578
iteration : 1406
train acc:  0.765625
train loss:  0.44427695870399475
train gradient:  0.11315047315597511
iteration : 1407
train acc:  0.7890625
train loss:  0.4379180669784546
train gradient:  0.11019891845255052
iteration : 1408
train acc:  0.796875
train loss:  0.41383975744247437
train gradient:  0.09799229946925066
iteration : 1409
train acc:  0.7421875
train loss:  0.4849618375301361
train gradient:  0.09693562051422716
iteration : 1410
train acc:  0.7265625
train loss:  0.5195395946502686
train gradient:  0.1287085757719233
iteration : 1411
train acc:  0.6640625
train loss:  0.5540370941162109
train gradient:  0.1788853450831997
iteration : 1412
train acc:  0.7265625
train loss:  0.48778608441352844
train gradient:  0.12067347473972156
iteration : 1413
train acc:  0.734375
train loss:  0.5107588768005371
train gradient:  0.12412051758792565
iteration : 1414
train acc:  0.75
train loss:  0.4647892713546753
train gradient:  0.10731051959802293
iteration : 1415
train acc:  0.7421875
train loss:  0.4842757284641266
train gradient:  0.12003674327658752
iteration : 1416
train acc:  0.7734375
train loss:  0.5013604164123535
train gradient:  0.16996986085439209
iteration : 1417
train acc:  0.7421875
train loss:  0.44324278831481934
train gradient:  0.11254546826443888
iteration : 1418
train acc:  0.7421875
train loss:  0.47747868299484253
train gradient:  0.1091414336195776
iteration : 1419
train acc:  0.6796875
train loss:  0.5589576959609985
train gradient:  0.20237158904061964
iteration : 1420
train acc:  0.7734375
train loss:  0.4808666706085205
train gradient:  0.12188954331766043
iteration : 1421
train acc:  0.71875
train loss:  0.5160787105560303
train gradient:  0.13693317476143663
iteration : 1422
train acc:  0.7109375
train loss:  0.4877096712589264
train gradient:  0.1326704128175472
iteration : 1423
train acc:  0.8046875
train loss:  0.40970897674560547
train gradient:  0.08725084836746752
iteration : 1424
train acc:  0.7890625
train loss:  0.4137651026248932
train gradient:  0.0937480907865905
iteration : 1425
train acc:  0.765625
train loss:  0.46875184774398804
train gradient:  0.1418021373634188
iteration : 1426
train acc:  0.8125
train loss:  0.4461517930030823
train gradient:  0.12240124959428261
iteration : 1427
train acc:  0.7109375
train loss:  0.4611051678657532
train gradient:  0.13199580138875405
iteration : 1428
train acc:  0.6875
train loss:  0.5636783838272095
train gradient:  0.14715929015274953
iteration : 1429
train acc:  0.6953125
train loss:  0.513603925704956
train gradient:  0.12022990498347735
iteration : 1430
train acc:  0.78125
train loss:  0.42442840337753296
train gradient:  0.10437933853248708
iteration : 1431
train acc:  0.6875
train loss:  0.5414575338363647
train gradient:  0.15559226203513465
iteration : 1432
train acc:  0.71875
train loss:  0.5166760087013245
train gradient:  0.14214471308808507
iteration : 1433
train acc:  0.78125
train loss:  0.479004442691803
train gradient:  0.12258905878540394
iteration : 1434
train acc:  0.7265625
train loss:  0.5040735006332397
train gradient:  0.15216746398888215
iteration : 1435
train acc:  0.671875
train loss:  0.5245180130004883
train gradient:  0.17758433168700552
iteration : 1436
train acc:  0.7421875
train loss:  0.49838119745254517
train gradient:  0.1403427321483678
iteration : 1437
train acc:  0.734375
train loss:  0.5272856950759888
train gradient:  0.1796150874940861
iteration : 1438
train acc:  0.8046875
train loss:  0.4536890983581543
train gradient:  0.10460880432883721
iteration : 1439
train acc:  0.734375
train loss:  0.5135807991027832
train gradient:  0.11551010385721244
iteration : 1440
train acc:  0.828125
train loss:  0.4115718901157379
train gradient:  0.08361881047133846
iteration : 1441
train acc:  0.78125
train loss:  0.46470963954925537
train gradient:  0.1064070841189223
iteration : 1442
train acc:  0.7578125
train loss:  0.4589046835899353
train gradient:  0.10760346284267118
iteration : 1443
train acc:  0.7421875
train loss:  0.4526715874671936
train gradient:  0.09779377840971212
iteration : 1444
train acc:  0.796875
train loss:  0.3827364444732666
train gradient:  0.10264202024647283
iteration : 1445
train acc:  0.7578125
train loss:  0.4854511320590973
train gradient:  0.13925074731633552
iteration : 1446
train acc:  0.71875
train loss:  0.4749711751937866
train gradient:  0.13571112612111286
iteration : 1447
train acc:  0.6953125
train loss:  0.5700883865356445
train gradient:  0.15739068669160486
iteration : 1448
train acc:  0.7421875
train loss:  0.48510947823524475
train gradient:  0.10798597015157065
iteration : 1449
train acc:  0.7578125
train loss:  0.4804103374481201
train gradient:  0.10105591931961018
iteration : 1450
train acc:  0.75
train loss:  0.49277883768081665
train gradient:  0.10578522801167281
iteration : 1451
train acc:  0.734375
train loss:  0.5271971225738525
train gradient:  0.1401877107110885
iteration : 1452
train acc:  0.7734375
train loss:  0.4726056158542633
train gradient:  0.11178303985112159
iteration : 1453
train acc:  0.78125
train loss:  0.4928915202617645
train gradient:  0.13678320503336883
iteration : 1454
train acc:  0.84375
train loss:  0.40642082691192627
train gradient:  0.08617432525782483
iteration : 1455
train acc:  0.71875
train loss:  0.5035276412963867
train gradient:  0.141095118841218
iteration : 1456
train acc:  0.734375
train loss:  0.49960973858833313
train gradient:  0.1510522611886862
iteration : 1457
train acc:  0.7109375
train loss:  0.4948112368583679
train gradient:  0.17899540656942955
iteration : 1458
train acc:  0.7109375
train loss:  0.5500162243843079
train gradient:  0.1726929893692563
iteration : 1459
train acc:  0.7421875
train loss:  0.4834541380405426
train gradient:  0.13603621327241494
iteration : 1460
train acc:  0.7421875
train loss:  0.49434441328048706
train gradient:  0.11918312872209297
iteration : 1461
train acc:  0.765625
train loss:  0.436171293258667
train gradient:  0.1379201052732519
iteration : 1462
train acc:  0.7578125
train loss:  0.42654967308044434
train gradient:  0.08657151031875696
iteration : 1463
train acc:  0.8046875
train loss:  0.47418874502182007
train gradient:  0.1369208199769304
iteration : 1464
train acc:  0.7734375
train loss:  0.4088059365749359
train gradient:  0.0805714814112313
iteration : 1465
train acc:  0.7421875
train loss:  0.527877688407898
train gradient:  0.1136441075297292
iteration : 1466
train acc:  0.734375
train loss:  0.521021842956543
train gradient:  0.15009095751934998
iteration : 1467
train acc:  0.8515625
train loss:  0.4067389965057373
train gradient:  0.1126959503716554
iteration : 1468
train acc:  0.75
train loss:  0.5194224119186401
train gradient:  0.1170418952871845
iteration : 1469
train acc:  0.734375
train loss:  0.5040363073348999
train gradient:  0.10698546204776713
iteration : 1470
train acc:  0.7265625
train loss:  0.4871056377887726
train gradient:  0.10289017532385064
iteration : 1471
train acc:  0.75
train loss:  0.4835416078567505
train gradient:  0.11070865666568154
iteration : 1472
train acc:  0.7421875
train loss:  0.4711359143257141
train gradient:  0.13689629960877342
iteration : 1473
train acc:  0.7265625
train loss:  0.4520571827888489
train gradient:  0.09653530421724169
iteration : 1474
train acc:  0.6953125
train loss:  0.5048167705535889
train gradient:  0.12446687347416921
iteration : 1475
train acc:  0.7734375
train loss:  0.4790646433830261
train gradient:  0.13713248429050034
iteration : 1476
train acc:  0.7890625
train loss:  0.44398289918899536
train gradient:  0.11152027612169545
iteration : 1477
train acc:  0.7109375
train loss:  0.5085694193840027
train gradient:  0.14087824168435842
iteration : 1478
train acc:  0.78125
train loss:  0.4461403489112854
train gradient:  0.09882508043293689
iteration : 1479
train acc:  0.7578125
train loss:  0.5177817940711975
train gradient:  0.1755943834765604
iteration : 1480
train acc:  0.71875
train loss:  0.5337773561477661
train gradient:  0.1593068888530993
iteration : 1481
train acc:  0.75
train loss:  0.4330267906188965
train gradient:  0.0759298216593549
iteration : 1482
train acc:  0.765625
train loss:  0.4590243101119995
train gradient:  0.12851444484761881
iteration : 1483
train acc:  0.7734375
train loss:  0.4300450086593628
train gradient:  0.09378381980406852
iteration : 1484
train acc:  0.6796875
train loss:  0.5559366345405579
train gradient:  0.19368403764513947
iteration : 1485
train acc:  0.78125
train loss:  0.4600869417190552
train gradient:  0.1424891979559859
iteration : 1486
train acc:  0.75
train loss:  0.4689056873321533
train gradient:  0.13451048548741323
iteration : 1487
train acc:  0.734375
train loss:  0.528205394744873
train gradient:  0.13008530822893055
iteration : 1488
train acc:  0.765625
train loss:  0.47194814682006836
train gradient:  0.1272372837834416
iteration : 1489
train acc:  0.765625
train loss:  0.43561434745788574
train gradient:  0.09118483191932583
iteration : 1490
train acc:  0.8125
train loss:  0.47378110885620117
train gradient:  0.10718902438334892
iteration : 1491
train acc:  0.703125
train loss:  0.4947669506072998
train gradient:  0.11212292081010433
iteration : 1492
train acc:  0.8046875
train loss:  0.45918089151382446
train gradient:  0.09251847236860877
iteration : 1493
train acc:  0.7890625
train loss:  0.4056701362133026
train gradient:  0.09019211168341107
iteration : 1494
train acc:  0.78125
train loss:  0.44160863757133484
train gradient:  0.1266042652401427
iteration : 1495
train acc:  0.7734375
train loss:  0.4806302785873413
train gradient:  0.1202194497490398
iteration : 1496
train acc:  0.734375
train loss:  0.48281726241111755
train gradient:  0.1385558889887082
iteration : 1497
train acc:  0.7734375
train loss:  0.44250911474227905
train gradient:  0.1138078048494291
iteration : 1498
train acc:  0.7890625
train loss:  0.47778409719467163
train gradient:  0.14599930479016893
iteration : 1499
train acc:  0.8046875
train loss:  0.4663920998573303
train gradient:  0.11081669138983086
iteration : 1500
train acc:  0.71875
train loss:  0.4926616847515106
train gradient:  0.12454160519781024
iteration : 1501
train acc:  0.7421875
train loss:  0.4410790205001831
train gradient:  0.11258704316267179
iteration : 1502
train acc:  0.7265625
train loss:  0.5051320791244507
train gradient:  0.10888761481967038
iteration : 1503
train acc:  0.7109375
train loss:  0.5156877636909485
train gradient:  0.14979270552674684
iteration : 1504
train acc:  0.78125
train loss:  0.42807844281196594
train gradient:  0.10542801929340478
iteration : 1505
train acc:  0.7265625
train loss:  0.4990682005882263
train gradient:  0.12749991433977698
iteration : 1506
train acc:  0.78125
train loss:  0.43818962574005127
train gradient:  0.11002731215280401
iteration : 1507
train acc:  0.703125
train loss:  0.5486131906509399
train gradient:  0.15414844870728073
iteration : 1508
train acc:  0.7890625
train loss:  0.4415762424468994
train gradient:  0.10266340198384911
iteration : 1509
train acc:  0.7421875
train loss:  0.4815337657928467
train gradient:  0.1458944001249152
iteration : 1510
train acc:  0.7109375
train loss:  0.5250003337860107
train gradient:  0.13049295368196234
iteration : 1511
train acc:  0.703125
train loss:  0.5283993482589722
train gradient:  0.1454206167388732
iteration : 1512
train acc:  0.796875
train loss:  0.42146041989326477
train gradient:  0.10623397931320022
iteration : 1513
train acc:  0.7734375
train loss:  0.4582892060279846
train gradient:  0.13336864311182878
iteration : 1514
train acc:  0.78125
train loss:  0.45589691400527954
train gradient:  0.1255721582492925
iteration : 1515
train acc:  0.671875
train loss:  0.5383366346359253
train gradient:  0.19120162948702152
iteration : 1516
train acc:  0.78125
train loss:  0.4667436480522156
train gradient:  0.08751788507925787
iteration : 1517
train acc:  0.796875
train loss:  0.4290338158607483
train gradient:  0.1008166563786328
iteration : 1518
train acc:  0.765625
train loss:  0.5077790021896362
train gradient:  0.12633769963400232
iteration : 1519
train acc:  0.7421875
train loss:  0.5114355087280273
train gradient:  0.1782977768856617
iteration : 1520
train acc:  0.703125
train loss:  0.5115004777908325
train gradient:  0.11938729311636226
iteration : 1521
train acc:  0.71875
train loss:  0.4834706783294678
train gradient:  0.11539920621696031
iteration : 1522
train acc:  0.8046875
train loss:  0.4031926393508911
train gradient:  0.08481097539037921
iteration : 1523
train acc:  0.7421875
train loss:  0.5059405565261841
train gradient:  0.15799805225528327
iteration : 1524
train acc:  0.7578125
train loss:  0.45927324891090393
train gradient:  0.09453523934118717
iteration : 1525
train acc:  0.71875
train loss:  0.5118106007575989
train gradient:  0.14593353770855202
iteration : 1526
train acc:  0.7265625
train loss:  0.5364999771118164
train gradient:  0.1498192866043795
iteration : 1527
train acc:  0.7578125
train loss:  0.4784255623817444
train gradient:  0.12282364475480259
iteration : 1528
train acc:  0.7421875
train loss:  0.5023924112319946
train gradient:  0.12241438763364709
iteration : 1529
train acc:  0.6875
train loss:  0.4952201247215271
train gradient:  0.12818896300039978
iteration : 1530
train acc:  0.71875
train loss:  0.4795674979686737
train gradient:  0.1056494335115044
iteration : 1531
train acc:  0.7265625
train loss:  0.5069223642349243
train gradient:  0.16638308327554807
iteration : 1532
train acc:  0.78125
train loss:  0.5165525674819946
train gradient:  0.147160169384285
iteration : 1533
train acc:  0.71875
train loss:  0.48983389139175415
train gradient:  0.12257667520999976
iteration : 1534
train acc:  0.8203125
train loss:  0.39422714710235596
train gradient:  0.08011285981764493
iteration : 1535
train acc:  0.7578125
train loss:  0.5199407339096069
train gradient:  0.14112139828703235
iteration : 1536
train acc:  0.7890625
train loss:  0.4241010546684265
train gradient:  0.11124881567620133
iteration : 1537
train acc:  0.703125
train loss:  0.5127391219139099
train gradient:  0.1268604727189966
iteration : 1538
train acc:  0.8046875
train loss:  0.393554151058197
train gradient:  0.07795007778814567
iteration : 1539
train acc:  0.7578125
train loss:  0.4634959399700165
train gradient:  0.09967352339052736
iteration : 1540
train acc:  0.7734375
train loss:  0.483185350894928
train gradient:  0.1403865123057702
iteration : 1541
train acc:  0.703125
train loss:  0.5278400778770447
train gradient:  0.12828936849236466
iteration : 1542
train acc:  0.8125
train loss:  0.4259685277938843
train gradient:  0.08540254243194653
iteration : 1543
train acc:  0.734375
train loss:  0.5251132249832153
train gradient:  0.11915366782677038
iteration : 1544
train acc:  0.7421875
train loss:  0.4763096868991852
train gradient:  0.09824893475158472
iteration : 1545
train acc:  0.78125
train loss:  0.45391395688056946
train gradient:  0.11296992287214086
iteration : 1546
train acc:  0.7734375
train loss:  0.46108633279800415
train gradient:  0.11638503646051543
iteration : 1547
train acc:  0.8125
train loss:  0.397347092628479
train gradient:  0.1147055612372701
iteration : 1548
train acc:  0.75
train loss:  0.44914519786834717
train gradient:  0.10643692635845237
iteration : 1549
train acc:  0.71875
train loss:  0.4986833930015564
train gradient:  0.1333129040643814
iteration : 1550
train acc:  0.8125
train loss:  0.43912479281425476
train gradient:  0.1325571917568169
iteration : 1551
train acc:  0.71875
train loss:  0.5083410739898682
train gradient:  0.16345181028389
iteration : 1552
train acc:  0.796875
train loss:  0.4236679971218109
train gradient:  0.10465017784079421
iteration : 1553
train acc:  0.765625
train loss:  0.5213233828544617
train gradient:  0.18539388337722443
iteration : 1554
train acc:  0.7265625
train loss:  0.5433334708213806
train gradient:  0.14466890628899326
iteration : 1555
train acc:  0.71875
train loss:  0.48226600885391235
train gradient:  0.14690293486474257
iteration : 1556
train acc:  0.7109375
train loss:  0.5718244910240173
train gradient:  0.18111054556231226
iteration : 1557
train acc:  0.734375
train loss:  0.49704819917678833
train gradient:  0.13456671369763756
iteration : 1558
train acc:  0.78125
train loss:  0.4337664544582367
train gradient:  0.09617423680798654
iteration : 1559
train acc:  0.671875
train loss:  0.5835388898849487
train gradient:  0.1461649882350544
iteration : 1560
train acc:  0.75
train loss:  0.4299547076225281
train gradient:  0.0882558742721002
iteration : 1561
train acc:  0.78125
train loss:  0.5021227598190308
train gradient:  0.13675976360334685
iteration : 1562
train acc:  0.7421875
train loss:  0.4636925160884857
train gradient:  0.12784153502141457
iteration : 1563
train acc:  0.640625
train loss:  0.567223310470581
train gradient:  0.19415596164707222
iteration : 1564
train acc:  0.71875
train loss:  0.4655018150806427
train gradient:  0.11294893168272005
iteration : 1565
train acc:  0.8203125
train loss:  0.41047900915145874
train gradient:  0.091639664942619
iteration : 1566
train acc:  0.734375
train loss:  0.5016007423400879
train gradient:  0.16000997620863616
iteration : 1567
train acc:  0.7890625
train loss:  0.4399205446243286
train gradient:  0.09910316554166626
iteration : 1568
train acc:  0.71875
train loss:  0.48095691204071045
train gradient:  0.108288651797758
iteration : 1569
train acc:  0.7890625
train loss:  0.4531172513961792
train gradient:  0.10382329437638575
iteration : 1570
train acc:  0.7578125
train loss:  0.45528167486190796
train gradient:  0.1131247492632287
iteration : 1571
train acc:  0.75
train loss:  0.5432138442993164
train gradient:  0.13200320432963486
iteration : 1572
train acc:  0.7578125
train loss:  0.46324223279953003
train gradient:  0.1284881925350883
iteration : 1573
train acc:  0.7578125
train loss:  0.45750105381011963
train gradient:  0.1133078500471511
iteration : 1574
train acc:  0.78125
train loss:  0.4399290680885315
train gradient:  0.11387524170668009
iteration : 1575
train acc:  0.7578125
train loss:  0.4435886740684509
train gradient:  0.08601420838607325
iteration : 1576
train acc:  0.734375
train loss:  0.5234884023666382
train gradient:  0.15312628832363873
iteration : 1577
train acc:  0.7890625
train loss:  0.5044465661048889
train gradient:  0.12272830421566867
iteration : 1578
train acc:  0.765625
train loss:  0.47814154624938965
train gradient:  0.10590468113598826
iteration : 1579
train acc:  0.71875
train loss:  0.48435232043266296
train gradient:  0.11068335300470031
iteration : 1580
train acc:  0.7578125
train loss:  0.48315170407295227
train gradient:  0.13335106710913014
iteration : 1581
train acc:  0.75
train loss:  0.47992056608200073
train gradient:  0.11632104585703607
iteration : 1582
train acc:  0.765625
train loss:  0.47616681456565857
train gradient:  0.152439842927913
iteration : 1583
train acc:  0.75
train loss:  0.4467868208885193
train gradient:  0.10630296441509417
iteration : 1584
train acc:  0.7265625
train loss:  0.5647469758987427
train gradient:  0.1366401310925234
iteration : 1585
train acc:  0.78125
train loss:  0.47435492277145386
train gradient:  0.10547166524929227
iteration : 1586
train acc:  0.7734375
train loss:  0.4644630551338196
train gradient:  0.10991146932165619
iteration : 1587
train acc:  0.78125
train loss:  0.47279709577560425
train gradient:  0.12899516096885985
iteration : 1588
train acc:  0.75
train loss:  0.4420722424983978
train gradient:  0.09272602509596335
iteration : 1589
train acc:  0.7109375
train loss:  0.45674312114715576
train gradient:  0.09770131981898149
iteration : 1590
train acc:  0.7578125
train loss:  0.4438696503639221
train gradient:  0.1378354709166557
iteration : 1591
train acc:  0.65625
train loss:  0.5433472394943237
train gradient:  0.1704422633435434
iteration : 1592
train acc:  0.734375
train loss:  0.4761083424091339
train gradient:  0.12667392674191746
iteration : 1593
train acc:  0.7421875
train loss:  0.4783424139022827
train gradient:  0.11028349030859601
iteration : 1594
train acc:  0.71875
train loss:  0.4988720118999481
train gradient:  0.14571524294639288
iteration : 1595
train acc:  0.7734375
train loss:  0.46454882621765137
train gradient:  0.10523857379351782
iteration : 1596
train acc:  0.7421875
train loss:  0.4653114080429077
train gradient:  0.1369477312689299
iteration : 1597
train acc:  0.78125
train loss:  0.45619577169418335
train gradient:  0.1337078354519568
iteration : 1598
train acc:  0.7734375
train loss:  0.49137425422668457
train gradient:  0.12453697978347736
iteration : 1599
train acc:  0.7890625
train loss:  0.43956872820854187
train gradient:  0.07960202170298884
iteration : 1600
train acc:  0.7890625
train loss:  0.4448937177658081
train gradient:  0.11879147669057996
iteration : 1601
train acc:  0.78125
train loss:  0.48329800367355347
train gradient:  0.13293903624752873
iteration : 1602
train acc:  0.765625
train loss:  0.46294116973876953
train gradient:  0.09393521667974025
iteration : 1603
train acc:  0.8125
train loss:  0.4351142644882202
train gradient:  0.10183940542188158
iteration : 1604
train acc:  0.7421875
train loss:  0.552980363368988
train gradient:  0.13851718598746043
iteration : 1605
train acc:  0.734375
train loss:  0.49862387776374817
train gradient:  0.11418103952212416
iteration : 1606
train acc:  0.78125
train loss:  0.4719149172306061
train gradient:  0.12650460481996706
iteration : 1607
train acc:  0.7421875
train loss:  0.47048547863960266
train gradient:  0.09862519624413403
iteration : 1608
train acc:  0.828125
train loss:  0.44492679834365845
train gradient:  0.1083389342137606
iteration : 1609
train acc:  0.7421875
train loss:  0.4836803078651428
train gradient:  0.12035932898475978
iteration : 1610
train acc:  0.765625
train loss:  0.4626862108707428
train gradient:  0.12587741225362026
iteration : 1611
train acc:  0.671875
train loss:  0.5049979090690613
train gradient:  0.13037227613856223
iteration : 1612
train acc:  0.796875
train loss:  0.45560091733932495
train gradient:  0.11229870247053783
iteration : 1613
train acc:  0.7578125
train loss:  0.5194364786148071
train gradient:  0.14248140625689085
iteration : 1614
train acc:  0.796875
train loss:  0.4539501368999481
train gradient:  0.09441676138259107
iteration : 1615
train acc:  0.7421875
train loss:  0.4824727773666382
train gradient:  0.1242996853956453
iteration : 1616
train acc:  0.7109375
train loss:  0.5812264680862427
train gradient:  0.16133458590942834
iteration : 1617
train acc:  0.828125
train loss:  0.42738795280456543
train gradient:  0.1063442912102478
iteration : 1618
train acc:  0.7578125
train loss:  0.4478178024291992
train gradient:  0.09331677116083698
iteration : 1619
train acc:  0.6875
train loss:  0.586781919002533
train gradient:  0.1832424758639024
iteration : 1620
train acc:  0.7578125
train loss:  0.47241052985191345
train gradient:  0.12785776176396302
iteration : 1621
train acc:  0.75
train loss:  0.4831239879131317
train gradient:  0.14895402335769725
iteration : 1622
train acc:  0.7109375
train loss:  0.5149035453796387
train gradient:  0.12462376281527678
iteration : 1623
train acc:  0.78125
train loss:  0.4723021686077118
train gradient:  0.13550397193899372
iteration : 1624
train acc:  0.7421875
train loss:  0.4724251329898834
train gradient:  0.15973328497354733
iteration : 1625
train acc:  0.8046875
train loss:  0.4485045075416565
train gradient:  0.10140614544378922
iteration : 1626
train acc:  0.8203125
train loss:  0.4550763666629791
train gradient:  0.10455133959609396
iteration : 1627
train acc:  0.7734375
train loss:  0.49609434604644775
train gradient:  0.17012156850290258
iteration : 1628
train acc:  0.765625
train loss:  0.4554939270019531
train gradient:  0.12498783507023632
iteration : 1629
train acc:  0.7421875
train loss:  0.52718585729599
train gradient:  0.1564666353262535
iteration : 1630
train acc:  0.7890625
train loss:  0.4455682337284088
train gradient:  0.13014396175236395
iteration : 1631
train acc:  0.765625
train loss:  0.47732704877853394
train gradient:  0.10136198874101726
iteration : 1632
train acc:  0.71875
train loss:  0.5354211926460266
train gradient:  0.14264947494253893
iteration : 1633
train acc:  0.7578125
train loss:  0.5077090859413147
train gradient:  0.15644656470225227
iteration : 1634
train acc:  0.765625
train loss:  0.4801071286201477
train gradient:  0.13177831105196317
iteration : 1635
train acc:  0.7578125
train loss:  0.5328324437141418
train gradient:  0.14937856972444702
iteration : 1636
train acc:  0.75
train loss:  0.5140399932861328
train gradient:  0.1004445434752917
iteration : 1637
train acc:  0.7734375
train loss:  0.4493013620376587
train gradient:  0.11755244365098663
iteration : 1638
train acc:  0.8046875
train loss:  0.3719590902328491
train gradient:  0.06657256149254626
iteration : 1639
train acc:  0.7265625
train loss:  0.4682564437389374
train gradient:  0.12437629442095442
iteration : 1640
train acc:  0.6640625
train loss:  0.5404268503189087
train gradient:  0.15409678020884626
iteration : 1641
train acc:  0.703125
train loss:  0.5219337344169617
train gradient:  0.15496639144794377
iteration : 1642
train acc:  0.765625
train loss:  0.49719640612602234
train gradient:  0.11586055186987658
iteration : 1643
train acc:  0.671875
train loss:  0.5468220710754395
train gradient:  0.15220679842253765
iteration : 1644
train acc:  0.8125
train loss:  0.4073339104652405
train gradient:  0.09272468262887638
iteration : 1645
train acc:  0.6953125
train loss:  0.48567599058151245
train gradient:  0.12842290091170067
iteration : 1646
train acc:  0.8203125
train loss:  0.41337329149246216
train gradient:  0.11300325218592984
iteration : 1647
train acc:  0.7734375
train loss:  0.4638652205467224
train gradient:  0.16479222372723812
iteration : 1648
train acc:  0.7578125
train loss:  0.5104731321334839
train gradient:  0.12046184929615569
iteration : 1649
train acc:  0.7734375
train loss:  0.44079115986824036
train gradient:  0.09431453718995293
iteration : 1650
train acc:  0.8515625
train loss:  0.3855278491973877
train gradient:  0.09068321337134144
iteration : 1651
train acc:  0.78125
train loss:  0.4334644675254822
train gradient:  0.11919725145395468
iteration : 1652
train acc:  0.7890625
train loss:  0.45674192905426025
train gradient:  0.0961911176242342
iteration : 1653
train acc:  0.7265625
train loss:  0.47616851329803467
train gradient:  0.11158940803511008
iteration : 1654
train acc:  0.7890625
train loss:  0.4210885167121887
train gradient:  0.10794576540523826
iteration : 1655
train acc:  0.7421875
train loss:  0.4838826656341553
train gradient:  0.11963114394431014
iteration : 1656
train acc:  0.8046875
train loss:  0.4371163845062256
train gradient:  0.10692199163547139
iteration : 1657
train acc:  0.75
train loss:  0.46373680233955383
train gradient:  0.13689955339969892
iteration : 1658
train acc:  0.7265625
train loss:  0.4900963306427002
train gradient:  0.12087658748777774
iteration : 1659
train acc:  0.7265625
train loss:  0.47263678908348083
train gradient:  0.10544187519545492
iteration : 1660
train acc:  0.6875
train loss:  0.5148988366127014
train gradient:  0.1700092601770864
iteration : 1661
train acc:  0.7109375
train loss:  0.5049140453338623
train gradient:  0.1707906526326713
iteration : 1662
train acc:  0.75
train loss:  0.5021507143974304
train gradient:  0.1299649740214799
iteration : 1663
train acc:  0.75
train loss:  0.45127731561660767
train gradient:  0.11332647993093545
iteration : 1664
train acc:  0.6796875
train loss:  0.5878830552101135
train gradient:  0.1453744782207031
iteration : 1665
train acc:  0.7890625
train loss:  0.5128968954086304
train gradient:  0.15945740036617842
iteration : 1666
train acc:  0.7421875
train loss:  0.44010359048843384
train gradient:  0.09622294817743433
iteration : 1667
train acc:  0.6796875
train loss:  0.5364718437194824
train gradient:  0.2163061117457963
iteration : 1668
train acc:  0.734375
train loss:  0.4678818881511688
train gradient:  0.15393954675492388
iteration : 1669
train acc:  0.765625
train loss:  0.4434879422187805
train gradient:  0.13776994404712034
iteration : 1670
train acc:  0.78125
train loss:  0.4697081446647644
train gradient:  0.11249967657174724
iteration : 1671
train acc:  0.71875
train loss:  0.4988599121570587
train gradient:  0.16660202225813414
iteration : 1672
train acc:  0.734375
train loss:  0.5346498489379883
train gradient:  0.16262647837749597
iteration : 1673
train acc:  0.78125
train loss:  0.43029940128326416
train gradient:  0.09158333450202688
iteration : 1674
train acc:  0.828125
train loss:  0.4690944254398346
train gradient:  0.1562825614460419
iteration : 1675
train acc:  0.71875
train loss:  0.49410131573677063
train gradient:  0.11034609372598835
iteration : 1676
train acc:  0.734375
train loss:  0.48396116495132446
train gradient:  0.12108281950507863
iteration : 1677
train acc:  0.65625
train loss:  0.6145485639572144
train gradient:  0.19360209201565504
iteration : 1678
train acc:  0.765625
train loss:  0.44295310974121094
train gradient:  0.09957280417805534
iteration : 1679
train acc:  0.75
train loss:  0.48104310035705566
train gradient:  0.1307538468604436
iteration : 1680
train acc:  0.734375
train loss:  0.48749566078186035
train gradient:  0.12194382524611101
iteration : 1681
train acc:  0.7578125
train loss:  0.48781996965408325
train gradient:  0.11844068605796719
iteration : 1682
train acc:  0.796875
train loss:  0.46385130286216736
train gradient:  0.13862359332405144
iteration : 1683
train acc:  0.7421875
train loss:  0.5106379985809326
train gradient:  0.15655599227038944
iteration : 1684
train acc:  0.71875
train loss:  0.5215570330619812
train gradient:  0.12081321213111154
iteration : 1685
train acc:  0.734375
train loss:  0.4486638903617859
train gradient:  0.1245833056771429
iteration : 1686
train acc:  0.7265625
train loss:  0.49723854660987854
train gradient:  0.1275982740957213
iteration : 1687
train acc:  0.765625
train loss:  0.4745751619338989
train gradient:  0.11978211248487824
iteration : 1688
train acc:  0.8203125
train loss:  0.41855546832084656
train gradient:  0.09188146346599224
iteration : 1689
train acc:  0.7421875
train loss:  0.4894046485424042
train gradient:  0.13015921029812527
iteration : 1690
train acc:  0.734375
train loss:  0.4890795350074768
train gradient:  0.11908610661741642
iteration : 1691
train acc:  0.7734375
train loss:  0.4429107904434204
train gradient:  0.1149644577995798
iteration : 1692
train acc:  0.796875
train loss:  0.44756072759628296
train gradient:  0.08880809404090902
iteration : 1693
train acc:  0.7734375
train loss:  0.46156609058380127
train gradient:  0.11652138036187172
iteration : 1694
train acc:  0.6875
train loss:  0.5426427721977234
train gradient:  0.17555294225547935
iteration : 1695
train acc:  0.7578125
train loss:  0.5157302618026733
train gradient:  0.13265703447517574
iteration : 1696
train acc:  0.703125
train loss:  0.5710029006004333
train gradient:  0.13158992197166064
iteration : 1697
train acc:  0.7265625
train loss:  0.4471072554588318
train gradient:  0.08925807922121909
iteration : 1698
train acc:  0.7421875
train loss:  0.4656428098678589
train gradient:  0.15225863647741777
iteration : 1699
train acc:  0.7578125
train loss:  0.46600404381752014
train gradient:  0.10713458357601548
iteration : 1700
train acc:  0.75
train loss:  0.41617754101753235
train gradient:  0.07131995747589934
iteration : 1701
train acc:  0.71875
train loss:  0.5118857622146606
train gradient:  0.13142206541873347
iteration : 1702
train acc:  0.7109375
train loss:  0.5067485570907593
train gradient:  0.14717362678057416
iteration : 1703
train acc:  0.6953125
train loss:  0.5350456237792969
train gradient:  0.14086034833429406
iteration : 1704
train acc:  0.75
train loss:  0.4948725402355194
train gradient:  0.13619940108329476
iteration : 1705
train acc:  0.7265625
train loss:  0.5285293459892273
train gradient:  0.17777925655026075
iteration : 1706
train acc:  0.78125
train loss:  0.4547475576400757
train gradient:  0.11489384612212669
iteration : 1707
train acc:  0.796875
train loss:  0.4634714424610138
train gradient:  0.1086031130241458
iteration : 1708
train acc:  0.734375
train loss:  0.5003798007965088
train gradient:  0.11107868941444538
iteration : 1709
train acc:  0.75
train loss:  0.49727505445480347
train gradient:  0.1236193960262473
iteration : 1710
train acc:  0.7421875
train loss:  0.4819382429122925
train gradient:  0.14043776643673328
iteration : 1711
train acc:  0.7421875
train loss:  0.4986804127693176
train gradient:  0.14213075093110158
iteration : 1712
train acc:  0.7578125
train loss:  0.5358554720878601
train gradient:  0.15897097536640437
iteration : 1713
train acc:  0.78125
train loss:  0.48026591539382935
train gradient:  0.13953474336204819
iteration : 1714
train acc:  0.7421875
train loss:  0.4843907952308655
train gradient:  0.11497724178161133
iteration : 1715
train acc:  0.75
train loss:  0.4902978539466858
train gradient:  0.11557436014359627
iteration : 1716
train acc:  0.7578125
train loss:  0.5082817077636719
train gradient:  0.18576958189189574
iteration : 1717
train acc:  0.796875
train loss:  0.4266083836555481
train gradient:  0.10519509562028144
iteration : 1718
train acc:  0.7734375
train loss:  0.5081092715263367
train gradient:  0.1358951912154186
iteration : 1719
train acc:  0.8125
train loss:  0.3991248309612274
train gradient:  0.11501322645476733
iteration : 1720
train acc:  0.78125
train loss:  0.449855238199234
train gradient:  0.10561165254274565
iteration : 1721
train acc:  0.7734375
train loss:  0.4862041771411896
train gradient:  0.11070305778554057
iteration : 1722
train acc:  0.75
train loss:  0.45305028557777405
train gradient:  0.09810058761417591
iteration : 1723
train acc:  0.734375
train loss:  0.5069385766983032
train gradient:  0.15905558390068808
iteration : 1724
train acc:  0.7890625
train loss:  0.4547669291496277
train gradient:  0.1226483585155128
iteration : 1725
train acc:  0.7421875
train loss:  0.508354663848877
train gradient:  0.16118718458827724
iteration : 1726
train acc:  0.7890625
train loss:  0.4436948299407959
train gradient:  0.1224511467081827
iteration : 1727
train acc:  0.6484375
train loss:  0.5811423063278198
train gradient:  0.18313033407986407
iteration : 1728
train acc:  0.703125
train loss:  0.47290390729904175
train gradient:  0.13221641054105804
iteration : 1729
train acc:  0.734375
train loss:  0.5242177248001099
train gradient:  0.12821558329883748
iteration : 1730
train acc:  0.71875
train loss:  0.50782710313797
train gradient:  0.1202024372316768
iteration : 1731
train acc:  0.7421875
train loss:  0.526308536529541
train gradient:  0.18025966320057274
iteration : 1732
train acc:  0.78125
train loss:  0.4146443009376526
train gradient:  0.11367792591728924
iteration : 1733
train acc:  0.8046875
train loss:  0.520541787147522
train gradient:  0.15854606013924122
iteration : 1734
train acc:  0.78125
train loss:  0.4597306251525879
train gradient:  0.11622717999557584
iteration : 1735
train acc:  0.71875
train loss:  0.4795236587524414
train gradient:  0.125849624516845
iteration : 1736
train acc:  0.8125
train loss:  0.4431646466255188
train gradient:  0.11112229953369529
iteration : 1737
train acc:  0.71875
train loss:  0.5622259378433228
train gradient:  0.19729447901052763
iteration : 1738
train acc:  0.7578125
train loss:  0.4780786633491516
train gradient:  0.1117890938661084
iteration : 1739
train acc:  0.7421875
train loss:  0.5168099999427795
train gradient:  0.10900446280166697
iteration : 1740
train acc:  0.7265625
train loss:  0.48143500089645386
train gradient:  0.11319617498520688
iteration : 1741
train acc:  0.7734375
train loss:  0.41536933183670044
train gradient:  0.10183784794416544
iteration : 1742
train acc:  0.78125
train loss:  0.43957215547561646
train gradient:  0.12093472479610609
iteration : 1743
train acc:  0.7578125
train loss:  0.47572648525238037
train gradient:  0.12466448368974735
iteration : 1744
train acc:  0.671875
train loss:  0.5481866002082825
train gradient:  0.1720870641307068
iteration : 1745
train acc:  0.765625
train loss:  0.46315908432006836
train gradient:  0.13017163934758913
iteration : 1746
train acc:  0.828125
train loss:  0.41912758350372314
train gradient:  0.09607675291785328
iteration : 1747
train acc:  0.734375
train loss:  0.48555707931518555
train gradient:  0.11729794242395215
iteration : 1748
train acc:  0.7578125
train loss:  0.43118351697921753
train gradient:  0.09644527240540515
iteration : 1749
train acc:  0.75
train loss:  0.4554828405380249
train gradient:  0.11359413987763113
iteration : 1750
train acc:  0.7421875
train loss:  0.47279486060142517
train gradient:  0.1377640743583768
iteration : 1751
train acc:  0.7734375
train loss:  0.4653627872467041
train gradient:  0.09688959583209855
iteration : 1752
train acc:  0.7109375
train loss:  0.5016100406646729
train gradient:  0.14248528261811944
iteration : 1753
train acc:  0.8125
train loss:  0.39821475744247437
train gradient:  0.06927788911144914
iteration : 1754
train acc:  0.78125
train loss:  0.4573047459125519
train gradient:  0.11445153563223716
iteration : 1755
train acc:  0.6953125
train loss:  0.5125619173049927
train gradient:  0.12784138497557768
iteration : 1756
train acc:  0.734375
train loss:  0.5128084421157837
train gradient:  0.12421418831570322
iteration : 1757
train acc:  0.75
train loss:  0.48924386501312256
train gradient:  0.14526820899272674
iteration : 1758
train acc:  0.796875
train loss:  0.43779754638671875
train gradient:  0.10317995296914488
iteration : 1759
train acc:  0.7578125
train loss:  0.45605820417404175
train gradient:  0.109488721023998
iteration : 1760
train acc:  0.78125
train loss:  0.40460142493247986
train gradient:  0.10810509364312856
iteration : 1761
train acc:  0.734375
train loss:  0.47868871688842773
train gradient:  0.1107099738306953
iteration : 1762
train acc:  0.78125
train loss:  0.49839431047439575
train gradient:  0.1197888441324384
iteration : 1763
train acc:  0.71875
train loss:  0.5329289436340332
train gradient:  0.14107568809701793
iteration : 1764
train acc:  0.828125
train loss:  0.4121038019657135
train gradient:  0.08101649861118455
iteration : 1765
train acc:  0.6953125
train loss:  0.5866214632987976
train gradient:  0.1442246103616403
iteration : 1766
train acc:  0.7421875
train loss:  0.5083698630332947
train gradient:  0.1355558946115591
iteration : 1767
train acc:  0.6953125
train loss:  0.5343161821365356
train gradient:  0.17759793979676258
iteration : 1768
train acc:  0.7734375
train loss:  0.49870532751083374
train gradient:  0.11141580811103623
iteration : 1769
train acc:  0.7578125
train loss:  0.40966200828552246
train gradient:  0.0952042917566767
iteration : 1770
train acc:  0.7265625
train loss:  0.5189958214759827
train gradient:  0.14860345286262597
iteration : 1771
train acc:  0.796875
train loss:  0.4043165147304535
train gradient:  0.11021644298990643
iteration : 1772
train acc:  0.75
train loss:  0.4807637929916382
train gradient:  0.10438034770002273
iteration : 1773
train acc:  0.7578125
train loss:  0.485659658908844
train gradient:  0.1175570216382034
iteration : 1774
train acc:  0.7578125
train loss:  0.45589154958724976
train gradient:  0.11016805069839493
iteration : 1775
train acc:  0.671875
train loss:  0.5331842303276062
train gradient:  0.1578343504096664
iteration : 1776
train acc:  0.7890625
train loss:  0.42388826608657837
train gradient:  0.15249199364771687
iteration : 1777
train acc:  0.6796875
train loss:  0.5354347229003906
train gradient:  0.2089820421343691
iteration : 1778
train acc:  0.7265625
train loss:  0.5358965396881104
train gradient:  0.19048148878983376
iteration : 1779
train acc:  0.703125
train loss:  0.4831259548664093
train gradient:  0.11243180915516968
iteration : 1780
train acc:  0.75
train loss:  0.5222491025924683
train gradient:  0.16993452397301143
iteration : 1781
train acc:  0.796875
train loss:  0.4275529384613037
train gradient:  0.123281983394887
iteration : 1782
train acc:  0.7421875
train loss:  0.5131387710571289
train gradient:  0.15756547226988063
iteration : 1783
train acc:  0.7421875
train loss:  0.5116360783576965
train gradient:  0.17067432068238259
iteration : 1784
train acc:  0.828125
train loss:  0.3749828338623047
train gradient:  0.09529606025423086
iteration : 1785
train acc:  0.75
train loss:  0.4742196202278137
train gradient:  0.11133430730280539
iteration : 1786
train acc:  0.7578125
train loss:  0.48954853415489197
train gradient:  0.16901351025053327
iteration : 1787
train acc:  0.7265625
train loss:  0.47687435150146484
train gradient:  0.12152123616998223
iteration : 1788
train acc:  0.6796875
train loss:  0.5342081785202026
train gradient:  0.1274132764442441
iteration : 1789
train acc:  0.7890625
train loss:  0.470344215631485
train gradient:  0.12260134224014653
iteration : 1790
train acc:  0.765625
train loss:  0.47259584069252014
train gradient:  0.12968499929831956
iteration : 1791
train acc:  0.7578125
train loss:  0.4301334619522095
train gradient:  0.12788709321752403
iteration : 1792
train acc:  0.7578125
train loss:  0.47069838643074036
train gradient:  0.1256656059862476
iteration : 1793
train acc:  0.78125
train loss:  0.529820442199707
train gradient:  0.15391411139198247
iteration : 1794
train acc:  0.75
train loss:  0.4734154939651489
train gradient:  0.125859721541575
iteration : 1795
train acc:  0.7421875
train loss:  0.4967573881149292
train gradient:  0.1548348470887796
iteration : 1796
train acc:  0.671875
train loss:  0.5195736885070801
train gradient:  0.14476448915068185
iteration : 1797
train acc:  0.765625
train loss:  0.4971264600753784
train gradient:  0.1311071559992007
iteration : 1798
train acc:  0.75
train loss:  0.4543643295764923
train gradient:  0.13197205611229118
iteration : 1799
train acc:  0.765625
train loss:  0.4421006441116333
train gradient:  0.08885661575453913
iteration : 1800
train acc:  0.7421875
train loss:  0.5211835503578186
train gradient:  0.14750758661902164
iteration : 1801
train acc:  0.8046875
train loss:  0.432567298412323
train gradient:  0.08996588485978316
iteration : 1802
train acc:  0.8203125
train loss:  0.4144044518470764
train gradient:  0.12561937263429948
iteration : 1803
train acc:  0.796875
train loss:  0.42182475328445435
train gradient:  0.10318904954163342
iteration : 1804
train acc:  0.8046875
train loss:  0.48849910497665405
train gradient:  0.10901826351096884
iteration : 1805
train acc:  0.7734375
train loss:  0.47474536299705505
train gradient:  0.12903395812426732
iteration : 1806
train acc:  0.703125
train loss:  0.5209925174713135
train gradient:  0.12732399388571147
iteration : 1807
train acc:  0.7734375
train loss:  0.4359288811683655
train gradient:  0.10044344279089117
iteration : 1808
train acc:  0.7421875
train loss:  0.5558735132217407
train gradient:  0.1342795152228005
iteration : 1809
train acc:  0.71875
train loss:  0.5369727611541748
train gradient:  0.13064915324630078
iteration : 1810
train acc:  0.7421875
train loss:  0.511262834072113
train gradient:  0.10424705968928905
iteration : 1811
train acc:  0.7265625
train loss:  0.5598793625831604
train gradient:  0.13607554321011106
iteration : 1812
train acc:  0.734375
train loss:  0.4957907497882843
train gradient:  0.14277887617897334
iteration : 1813
train acc:  0.75
train loss:  0.5070184469223022
train gradient:  0.13981467621808996
iteration : 1814
train acc:  0.7265625
train loss:  0.5085495710372925
train gradient:  0.12740479490958972
iteration : 1815
train acc:  0.7265625
train loss:  0.5202330350875854
train gradient:  0.14759978770100718
iteration : 1816
train acc:  0.734375
train loss:  0.5534278154373169
train gradient:  0.13961747320507273
iteration : 1817
train acc:  0.7109375
train loss:  0.49504294991493225
train gradient:  0.11942469522517828
iteration : 1818
train acc:  0.7109375
train loss:  0.4760318398475647
train gradient:  0.1253980184204374
iteration : 1819
train acc:  0.7734375
train loss:  0.4696022868156433
train gradient:  0.12266793524214702
iteration : 1820
train acc:  0.7734375
train loss:  0.48943090438842773
train gradient:  0.1073578847233614
iteration : 1821
train acc:  0.765625
train loss:  0.42527326941490173
train gradient:  0.12467688250081727
iteration : 1822
train acc:  0.75
train loss:  0.4681869149208069
train gradient:  0.11844497055407063
iteration : 1823
train acc:  0.75
train loss:  0.47116243839263916
train gradient:  0.11938544529447458
iteration : 1824
train acc:  0.7265625
train loss:  0.5040227770805359
train gradient:  0.12412894648494127
iteration : 1825
train acc:  0.7109375
train loss:  0.5198280215263367
train gradient:  0.13797829373372422
iteration : 1826
train acc:  0.7109375
train loss:  0.5458332896232605
train gradient:  0.15179853615679492
iteration : 1827
train acc:  0.7734375
train loss:  0.4299156665802002
train gradient:  0.08711431795756347
iteration : 1828
train acc:  0.75
train loss:  0.4196731448173523
train gradient:  0.08913549109217574
iteration : 1829
train acc:  0.75
train loss:  0.45559489727020264
train gradient:  0.10619264208651608
iteration : 1830
train acc:  0.7578125
train loss:  0.4760412573814392
train gradient:  0.11174192922161516
iteration : 1831
train acc:  0.734375
train loss:  0.4887899160385132
train gradient:  0.11873876130730744
iteration : 1832
train acc:  0.7109375
train loss:  0.5205138921737671
train gradient:  0.17908715257659594
iteration : 1833
train acc:  0.703125
train loss:  0.5971221923828125
train gradient:  0.18732151071036918
iteration : 1834
train acc:  0.7421875
train loss:  0.46220362186431885
train gradient:  0.10875413719932164
iteration : 1835
train acc:  0.765625
train loss:  0.46356093883514404
train gradient:  0.09404204589955549
iteration : 1836
train acc:  0.7421875
train loss:  0.5325498580932617
train gradient:  0.11327529897724217
iteration : 1837
train acc:  0.7890625
train loss:  0.4116504490375519
train gradient:  0.09024315204475847
iteration : 1838
train acc:  0.7109375
train loss:  0.5304462909698486
train gradient:  0.1433492448242789
iteration : 1839
train acc:  0.7421875
train loss:  0.5193665623664856
train gradient:  0.16928937865687488
iteration : 1840
train acc:  0.7421875
train loss:  0.5079306960105896
train gradient:  0.12258405996354037
iteration : 1841
train acc:  0.8046875
train loss:  0.44071510434150696
train gradient:  0.09773166914492634
iteration : 1842
train acc:  0.7734375
train loss:  0.43563273549079895
train gradient:  0.08694455308478757
iteration : 1843
train acc:  0.640625
train loss:  0.6017699837684631
train gradient:  0.18357232158926892
iteration : 1844
train acc:  0.796875
train loss:  0.4697021245956421
train gradient:  0.1277894157517448
iteration : 1845
train acc:  0.75
train loss:  0.5266328454017639
train gradient:  0.14223703454617714
iteration : 1846
train acc:  0.7578125
train loss:  0.5023622512817383
train gradient:  0.15129053588194036
iteration : 1847
train acc:  0.7265625
train loss:  0.4686698317527771
train gradient:  0.09843784885174363
iteration : 1848
train acc:  0.78125
train loss:  0.4742959439754486
train gradient:  0.12003124336177405
iteration : 1849
train acc:  0.8046875
train loss:  0.4313456416130066
train gradient:  0.11262274623127352
iteration : 1850
train acc:  0.7734375
train loss:  0.5107887387275696
train gradient:  0.1080039192379045
iteration : 1851
train acc:  0.7265625
train loss:  0.5372138619422913
train gradient:  0.15261265039998007
iteration : 1852
train acc:  0.6640625
train loss:  0.604093074798584
train gradient:  0.13450372987658366
iteration : 1853
train acc:  0.71875
train loss:  0.5153632164001465
train gradient:  0.11279018301376964
iteration : 1854
train acc:  0.703125
train loss:  0.5197278261184692
train gradient:  0.12711114473081803
iteration : 1855
train acc:  0.7734375
train loss:  0.48669108748435974
train gradient:  0.11451237325762509
iteration : 1856
train acc:  0.8125
train loss:  0.4409942924976349
train gradient:  0.0991727721106108
iteration : 1857
train acc:  0.734375
train loss:  0.4818862974643707
train gradient:  0.0983667222227552
iteration : 1858
train acc:  0.7890625
train loss:  0.429484099149704
train gradient:  0.09689447112210468
iteration : 1859
train acc:  0.765625
train loss:  0.4525649845600128
train gradient:  0.09182021292906176
iteration : 1860
train acc:  0.734375
train loss:  0.5105414390563965
train gradient:  0.12236358264489165
iteration : 1861
train acc:  0.71875
train loss:  0.47568589448928833
train gradient:  0.12337457936259975
iteration : 1862
train acc:  0.734375
train loss:  0.5059092044830322
train gradient:  0.11949752193268535
iteration : 1863
train acc:  0.765625
train loss:  0.4294273555278778
train gradient:  0.09411776315405833
iteration : 1864
train acc:  0.75
train loss:  0.48033058643341064
train gradient:  0.11439041003239012
iteration : 1865
train acc:  0.640625
train loss:  0.5421113967895508
train gradient:  0.13447448873838252
iteration : 1866
train acc:  0.8125
train loss:  0.4122586250305176
train gradient:  0.09956080473113879
iteration : 1867
train acc:  0.7734375
train loss:  0.44182685017585754
train gradient:  0.10130575669215544
iteration : 1868
train acc:  0.71875
train loss:  0.5352476239204407
train gradient:  0.14244078235768398
iteration : 1869
train acc:  0.75
train loss:  0.5019131302833557
train gradient:  0.15469930404194127
iteration : 1870
train acc:  0.7265625
train loss:  0.4777822196483612
train gradient:  0.12186639546266728
iteration : 1871
train acc:  0.734375
train loss:  0.5171599388122559
train gradient:  0.13747543038552876
iteration : 1872
train acc:  0.765625
train loss:  0.4786273241043091
train gradient:  0.1385125468568812
iteration : 1873
train acc:  0.7421875
train loss:  0.5198123455047607
train gradient:  0.1409260343363488
iteration : 1874
train acc:  0.7734375
train loss:  0.5048959255218506
train gradient:  0.12992883887546783
iteration : 1875
train acc:  0.7265625
train loss:  0.47891515493392944
train gradient:  0.13708519205221636
iteration : 1876
train acc:  0.703125
train loss:  0.5030027627944946
train gradient:  0.12844171333943283
iteration : 1877
train acc:  0.75
train loss:  0.4520718455314636
train gradient:  0.11604794740391274
iteration : 1878
train acc:  0.7578125
train loss:  0.45548152923583984
train gradient:  0.10896475553474513
iteration : 1879
train acc:  0.6953125
train loss:  0.523403525352478
train gradient:  0.12857141746435152
iteration : 1880
train acc:  0.7109375
train loss:  0.5710185766220093
train gradient:  0.17308405584980285
iteration : 1881
train acc:  0.765625
train loss:  0.46129903197288513
train gradient:  0.11337305886504073
iteration : 1882
train acc:  0.6796875
train loss:  0.5552753806114197
train gradient:  0.15201555751321372
iteration : 1883
train acc:  0.8125
train loss:  0.43232178688049316
train gradient:  0.08908266906004306
iteration : 1884
train acc:  0.7421875
train loss:  0.5394879579544067
train gradient:  0.17042276437374426
iteration : 1885
train acc:  0.75
train loss:  0.4981030225753784
train gradient:  0.12643280510835575
iteration : 1886
train acc:  0.734375
train loss:  0.5311236381530762
train gradient:  0.11306287336108264
iteration : 1887
train acc:  0.8125
train loss:  0.47290486097335815
train gradient:  0.12044098245228016
iteration : 1888
train acc:  0.7421875
train loss:  0.5369784832000732
train gradient:  0.18375659717940776
iteration : 1889
train acc:  0.84375
train loss:  0.4265896677970886
train gradient:  0.0912642955578699
iteration : 1890
train acc:  0.8046875
train loss:  0.44969552755355835
train gradient:  0.10484672263247297
iteration : 1891
train acc:  0.8046875
train loss:  0.39136165380477905
train gradient:  0.08169769258044167
iteration : 1892
train acc:  0.6875
train loss:  0.558819591999054
train gradient:  0.14346431383243274
iteration : 1893
train acc:  0.78125
train loss:  0.5068682432174683
train gradient:  0.11331854127144501
iteration : 1894
train acc:  0.71875
train loss:  0.4833296239376068
train gradient:  0.11771808156459525
iteration : 1895
train acc:  0.71875
train loss:  0.4960273504257202
train gradient:  0.11423381566377115
iteration : 1896
train acc:  0.7109375
train loss:  0.5023229122161865
train gradient:  0.15931233889301988
iteration : 1897
train acc:  0.734375
train loss:  0.4879787266254425
train gradient:  0.10438372835903437
iteration : 1898
train acc:  0.78125
train loss:  0.4805091619491577
train gradient:  0.1176479717430665
iteration : 1899
train acc:  0.671875
train loss:  0.5391267538070679
train gradient:  0.1518503208138593
iteration : 1900
train acc:  0.78125
train loss:  0.44022905826568604
train gradient:  0.08964107079492095
iteration : 1901
train acc:  0.734375
train loss:  0.504911482334137
train gradient:  0.11948735903358448
iteration : 1902
train acc:  0.8046875
train loss:  0.4441084861755371
train gradient:  0.08626752374111397
iteration : 1903
train acc:  0.75
train loss:  0.4897550344467163
train gradient:  0.1256825653448583
iteration : 1904
train acc:  0.71875
train loss:  0.5176438689231873
train gradient:  0.15716439001611726
iteration : 1905
train acc:  0.734375
train loss:  0.4773235619068146
train gradient:  0.10349259348176391
iteration : 1906
train acc:  0.7734375
train loss:  0.43511736392974854
train gradient:  0.09495099060534234
iteration : 1907
train acc:  0.7109375
train loss:  0.5138822197914124
train gradient:  0.1373320950719063
iteration : 1908
train acc:  0.7421875
train loss:  0.45854637026786804
train gradient:  0.09905326575320855
iteration : 1909
train acc:  0.75
train loss:  0.5273975133895874
train gradient:  0.14163365072973805
iteration : 1910
train acc:  0.6640625
train loss:  0.5087838172912598
train gradient:  0.12941115315779123
iteration : 1911
train acc:  0.8203125
train loss:  0.3950780928134918
train gradient:  0.08259383784834962
iteration : 1912
train acc:  0.7734375
train loss:  0.4477936923503876
train gradient:  0.09274592117383199
iteration : 1913
train acc:  0.734375
train loss:  0.4980335831642151
train gradient:  0.10176009616820797
iteration : 1914
train acc:  0.796875
train loss:  0.4289536476135254
train gradient:  0.11954967552777261
iteration : 1915
train acc:  0.6953125
train loss:  0.5727722644805908
train gradient:  0.1429138421534568
iteration : 1916
train acc:  0.7421875
train loss:  0.4658174216747284
train gradient:  0.1194934565504323
iteration : 1917
train acc:  0.828125
train loss:  0.43023034930229187
train gradient:  0.08931969907798701
iteration : 1918
train acc:  0.7265625
train loss:  0.5727002620697021
train gradient:  0.15481232483921964
iteration : 1919
train acc:  0.84375
train loss:  0.4045730233192444
train gradient:  0.09762085939834658
iteration : 1920
train acc:  0.75
train loss:  0.5182889103889465
train gradient:  0.13139467823797113
iteration : 1921
train acc:  0.703125
train loss:  0.4950658082962036
train gradient:  0.1399207740788636
iteration : 1922
train acc:  0.7421875
train loss:  0.5242170095443726
train gradient:  0.14448682130563983
iteration : 1923
train acc:  0.75
train loss:  0.4666817784309387
train gradient:  0.11021871119456515
iteration : 1924
train acc:  0.7734375
train loss:  0.44465750455856323
train gradient:  0.10009034943685478
iteration : 1925
train acc:  0.7578125
train loss:  0.4895802140235901
train gradient:  0.1520995414508282
iteration : 1926
train acc:  0.71875
train loss:  0.5615224838256836
train gradient:  0.1459953620732131
iteration : 1927
train acc:  0.75
train loss:  0.517128586769104
train gradient:  0.11703793588751976
iteration : 1928
train acc:  0.71875
train loss:  0.5293223857879639
train gradient:  0.14730224079295917
iteration : 1929
train acc:  0.765625
train loss:  0.45953816175460815
train gradient:  0.10479371391702827
iteration : 1930
train acc:  0.7421875
train loss:  0.48970311880111694
train gradient:  0.11214480771764136
iteration : 1931
train acc:  0.8125
train loss:  0.418298602104187
train gradient:  0.10799506263881414
iteration : 1932
train acc:  0.7890625
train loss:  0.4571871757507324
train gradient:  0.10146478332966465
iteration : 1933
train acc:  0.75
train loss:  0.4559233486652374
train gradient:  0.08253445030245347
iteration : 1934
train acc:  0.828125
train loss:  0.44335874915122986
train gradient:  0.0945223713312976
iteration : 1935
train acc:  0.7578125
train loss:  0.4458077549934387
train gradient:  0.09851230890176561
iteration : 1936
train acc:  0.8203125
train loss:  0.39832693338394165
train gradient:  0.08341702366087628
iteration : 1937
train acc:  0.7109375
train loss:  0.5084857940673828
train gradient:  0.12725379498155717
iteration : 1938
train acc:  0.765625
train loss:  0.4989236891269684
train gradient:  0.1130030094780355
iteration : 1939
train acc:  0.78125
train loss:  0.4284675419330597
train gradient:  0.08662891023620084
iteration : 1940
train acc:  0.75
train loss:  0.45761817693710327
train gradient:  0.11280901882546088
iteration : 1941
train acc:  0.6953125
train loss:  0.5204942226409912
train gradient:  0.12028327779250847
iteration : 1942
train acc:  0.7421875
train loss:  0.4921717643737793
train gradient:  0.11837115734424825
iteration : 1943
train acc:  0.734375
train loss:  0.49968427419662476
train gradient:  0.12524844379196956
iteration : 1944
train acc:  0.7890625
train loss:  0.4335814416408539
train gradient:  0.08275836243674492
iteration : 1945
train acc:  0.78125
train loss:  0.4586896300315857
train gradient:  0.10408731359351227
iteration : 1946
train acc:  0.78125
train loss:  0.4340399503707886
train gradient:  0.08855999813782035
iteration : 1947
train acc:  0.7265625
train loss:  0.490320086479187
train gradient:  0.09819736762727237
iteration : 1948
train acc:  0.828125
train loss:  0.40096819400787354
train gradient:  0.08113537138160584
iteration : 1949
train acc:  0.71875
train loss:  0.5128836035728455
train gradient:  0.10772809262874769
iteration : 1950
train acc:  0.75
train loss:  0.5244849920272827
train gradient:  0.12035416959737447
iteration : 1951
train acc:  0.7890625
train loss:  0.46843379735946655
train gradient:  0.11984043234176711
iteration : 1952
train acc:  0.8125
train loss:  0.41940954327583313
train gradient:  0.09597408480638482
iteration : 1953
train acc:  0.71875
train loss:  0.5112839937210083
train gradient:  0.11508438841077512
iteration : 1954
train acc:  0.7578125
train loss:  0.4982685148715973
train gradient:  0.12971768120958535
iteration : 1955
train acc:  0.7734375
train loss:  0.47106504440307617
train gradient:  0.13080085277610343
iteration : 1956
train acc:  0.7578125
train loss:  0.5116716623306274
train gradient:  0.13294987042788908
iteration : 1957
train acc:  0.7734375
train loss:  0.4467095732688904
train gradient:  0.13747012340125425
iteration : 1958
train acc:  0.6953125
train loss:  0.5405747294425964
train gradient:  0.16742685978738278
iteration : 1959
train acc:  0.703125
train loss:  0.4767146110534668
train gradient:  0.12910491295022622
iteration : 1960
train acc:  0.7265625
train loss:  0.4954569637775421
train gradient:  0.1164526062672723
iteration : 1961
train acc:  0.7578125
train loss:  0.4808509945869446
train gradient:  0.11077618978370218
iteration : 1962
train acc:  0.7734375
train loss:  0.4843055009841919
train gradient:  0.14506192401640305
iteration : 1963
train acc:  0.7578125
train loss:  0.4550877809524536
train gradient:  0.13333591747207735
iteration : 1964
train acc:  0.6953125
train loss:  0.4780412018299103
train gradient:  0.11247886801239936
iteration : 1965
train acc:  0.7265625
train loss:  0.49758386611938477
train gradient:  0.11482357222511834
iteration : 1966
train acc:  0.7265625
train loss:  0.46855899691581726
train gradient:  0.143916399836374
iteration : 1967
train acc:  0.71875
train loss:  0.4896966218948364
train gradient:  0.13148898535577996
iteration : 1968
train acc:  0.7578125
train loss:  0.4793820381164551
train gradient:  0.12945730247102993
iteration : 1969
train acc:  0.7421875
train loss:  0.4644012451171875
train gradient:  0.12084786453899013
iteration : 1970
train acc:  0.703125
train loss:  0.5030054450035095
train gradient:  0.12547386346366524
iteration : 1971
train acc:  0.734375
train loss:  0.5381049513816833
train gradient:  0.1074705405603471
iteration : 1972
train acc:  0.7109375
train loss:  0.4690537452697754
train gradient:  0.11147669844160443
iteration : 1973
train acc:  0.7734375
train loss:  0.44513219594955444
train gradient:  0.09227492279388809
iteration : 1974
train acc:  0.8359375
train loss:  0.3970908522605896
train gradient:  0.09820154685267544
iteration : 1975
train acc:  0.8046875
train loss:  0.43327033519744873
train gradient:  0.08600749131855068
iteration : 1976
train acc:  0.7109375
train loss:  0.5029087066650391
train gradient:  0.16127455290942766
iteration : 1977
train acc:  0.7734375
train loss:  0.47794848680496216
train gradient:  0.14548386364362229
iteration : 1978
train acc:  0.796875
train loss:  0.4496219754219055
train gradient:  0.0813098433789936
iteration : 1979
train acc:  0.7578125
train loss:  0.4887392222881317
train gradient:  0.12394792287234392
iteration : 1980
train acc:  0.6875
train loss:  0.5219541788101196
train gradient:  0.17080866389903004
iteration : 1981
train acc:  0.671875
train loss:  0.5499565601348877
train gradient:  0.1373714494976589
iteration : 1982
train acc:  0.78125
train loss:  0.4032130837440491
train gradient:  0.08462006174324135
iteration : 1983
train acc:  0.71875
train loss:  0.49993348121643066
train gradient:  0.11514734598173747
iteration : 1984
train acc:  0.7421875
train loss:  0.49756038188934326
train gradient:  0.1222143351141643
iteration : 1985
train acc:  0.7109375
train loss:  0.5123156309127808
train gradient:  0.15074052452721437
iteration : 1986
train acc:  0.7734375
train loss:  0.4265158176422119
train gradient:  0.10933059146641953
iteration : 1987
train acc:  0.765625
train loss:  0.49185603857040405
train gradient:  0.1348848154790522
iteration : 1988
train acc:  0.8046875
train loss:  0.40166470408439636
train gradient:  0.09267100245097812
iteration : 1989
train acc:  0.78125
train loss:  0.4364531636238098
train gradient:  0.08250014879062302
iteration : 1990
train acc:  0.78125
train loss:  0.42680948972702026
train gradient:  0.07760926063003408
iteration : 1991
train acc:  0.7734375
train loss:  0.4522666335105896
train gradient:  0.11679637565930767
iteration : 1992
train acc:  0.7734375
train loss:  0.4606781601905823
train gradient:  0.09259831747330095
iteration : 1993
train acc:  0.7734375
train loss:  0.4359646737575531
train gradient:  0.10840339398272
iteration : 1994
train acc:  0.6640625
train loss:  0.5428249835968018
train gradient:  0.2085667489069261
iteration : 1995
train acc:  0.7421875
train loss:  0.4752471446990967
train gradient:  0.13157695803554337
iteration : 1996
train acc:  0.7421875
train loss:  0.5088566541671753
train gradient:  0.13954515525684358
iteration : 1997
train acc:  0.78125
train loss:  0.46182090044021606
train gradient:  0.09712626845342079
iteration : 1998
train acc:  0.78125
train loss:  0.4617760181427002
train gradient:  0.11165876387684308
iteration : 1999
train acc:  0.703125
train loss:  0.4770013689994812
train gradient:  0.15468596432099335
iteration : 2000
train acc:  0.859375
train loss:  0.3596567213535309
train gradient:  0.09247165099764833
iteration : 2001
train acc:  0.734375
train loss:  0.48722952604293823
train gradient:  0.13992880410439112
iteration : 2002
train acc:  0.75
train loss:  0.4789171814918518
train gradient:  0.14030791139978077
iteration : 2003
train acc:  0.75
train loss:  0.4634305238723755
train gradient:  0.11764458751334936
iteration : 2004
train acc:  0.75
train loss:  0.5162570476531982
train gradient:  0.12161218210850286
iteration : 2005
train acc:  0.703125
train loss:  0.5223125219345093
train gradient:  0.13752679176097735
iteration : 2006
train acc:  0.6796875
train loss:  0.5622235536575317
train gradient:  0.13669966692558333
iteration : 2007
train acc:  0.7265625
train loss:  0.45598942041397095
train gradient:  0.08079528692771917
iteration : 2008
train acc:  0.75
train loss:  0.501779317855835
train gradient:  0.1266453369870284
iteration : 2009
train acc:  0.7421875
train loss:  0.5274558067321777
train gradient:  0.1274290327398454
iteration : 2010
train acc:  0.8046875
train loss:  0.47329044342041016
train gradient:  0.10993945303132789
iteration : 2011
train acc:  0.7734375
train loss:  0.4960690140724182
train gradient:  0.1318375509150828
iteration : 2012
train acc:  0.6875
train loss:  0.5204306840896606
train gradient:  0.12871691840403504
iteration : 2013
train acc:  0.7578125
train loss:  0.4143509864807129
train gradient:  0.08296198504791834
iteration : 2014
train acc:  0.6953125
train loss:  0.5905297994613647
train gradient:  0.15826721879080854
iteration : 2015
train acc:  0.75
train loss:  0.5176960229873657
train gradient:  0.17978345364684234
iteration : 2016
train acc:  0.703125
train loss:  0.5133751630783081
train gradient:  0.12984099830663218
iteration : 2017
train acc:  0.734375
train loss:  0.4823310375213623
train gradient:  0.15014017638097013
iteration : 2018
train acc:  0.6953125
train loss:  0.5464227795600891
train gradient:  0.16590920156872996
iteration : 2019
train acc:  0.796875
train loss:  0.43105465173721313
train gradient:  0.10733404866887074
iteration : 2020
train acc:  0.734375
train loss:  0.5210860371589661
train gradient:  0.14921030381388123
iteration : 2021
train acc:  0.7421875
train loss:  0.5168517827987671
train gradient:  0.13182735060958553
iteration : 2022
train acc:  0.703125
train loss:  0.5350369811058044
train gradient:  0.14615202134109673
iteration : 2023
train acc:  0.7890625
train loss:  0.47003358602523804
train gradient:  0.0936673226968078
iteration : 2024
train acc:  0.75
train loss:  0.4786587059497833
train gradient:  0.10477516804130334
iteration : 2025
train acc:  0.7265625
train loss:  0.5249654054641724
train gradient:  0.15435689579982242
iteration : 2026
train acc:  0.734375
train loss:  0.5065406560897827
train gradient:  0.12442155691556094
iteration : 2027
train acc:  0.8203125
train loss:  0.44095635414123535
train gradient:  0.1079369971623435
iteration : 2028
train acc:  0.7890625
train loss:  0.46882331371307373
train gradient:  0.08579176202208826
iteration : 2029
train acc:  0.703125
train loss:  0.5236147046089172
train gradient:  0.16725905846738892
iteration : 2030
train acc:  0.8046875
train loss:  0.4042133688926697
train gradient:  0.09384083669802666
iteration : 2031
train acc:  0.78125
train loss:  0.45042288303375244
train gradient:  0.09402385389164568
iteration : 2032
train acc:  0.765625
train loss:  0.44737106561660767
train gradient:  0.10696080805062896
iteration : 2033
train acc:  0.7890625
train loss:  0.4425617754459381
train gradient:  0.10531566735794373
iteration : 2034
train acc:  0.75
train loss:  0.48981374502182007
train gradient:  0.14922067430922495
iteration : 2035
train acc:  0.703125
train loss:  0.4848933815956116
train gradient:  0.12802988426743295
iteration : 2036
train acc:  0.7890625
train loss:  0.4621378481388092
train gradient:  0.11735033901477167
iteration : 2037
train acc:  0.8046875
train loss:  0.5035206079483032
train gradient:  0.12622541706314527
iteration : 2038
train acc:  0.7890625
train loss:  0.41412150859832764
train gradient:  0.11232920581280559
iteration : 2039
train acc:  0.78125
train loss:  0.47543513774871826
train gradient:  0.12365889983747255
iteration : 2040
train acc:  0.703125
train loss:  0.49039921164512634
train gradient:  0.12534668428265472
iteration : 2041
train acc:  0.7265625
train loss:  0.5483657121658325
train gradient:  0.1661925440658361
iteration : 2042
train acc:  0.78125
train loss:  0.44037583470344543
train gradient:  0.08185141101341145
iteration : 2043
train acc:  0.796875
train loss:  0.4182038903236389
train gradient:  0.08366197818219938
iteration : 2044
train acc:  0.7578125
train loss:  0.4916306734085083
train gradient:  0.141651667307167
iteration : 2045
train acc:  0.828125
train loss:  0.4579213559627533
train gradient:  0.11887689999982336
iteration : 2046
train acc:  0.7265625
train loss:  0.5091160535812378
train gradient:  0.1634368058303094
iteration : 2047
train acc:  0.75
train loss:  0.47463053464889526
train gradient:  0.10564045939299883
iteration : 2048
train acc:  0.65625
train loss:  0.5234156847000122
train gradient:  0.13675718725582303
iteration : 2049
train acc:  0.7734375
train loss:  0.49303534626960754
train gradient:  0.10989905238323819
iteration : 2050
train acc:  0.71875
train loss:  0.5130331516265869
train gradient:  0.1322424302240503
iteration : 2051
train acc:  0.671875
train loss:  0.5394368171691895
train gradient:  0.1457021942622318
iteration : 2052
train acc:  0.734375
train loss:  0.48129865527153015
train gradient:  0.12442416552143598
iteration : 2053
train acc:  0.7578125
train loss:  0.4749875068664551
train gradient:  0.12443767485258773
iteration : 2054
train acc:  0.7421875
train loss:  0.5098255276679993
train gradient:  0.12397673316383158
iteration : 2055
train acc:  0.78125
train loss:  0.4384801983833313
train gradient:  0.09232954436821439
iteration : 2056
train acc:  0.734375
train loss:  0.478286474943161
train gradient:  0.11829695369923948
iteration : 2057
train acc:  0.8125
train loss:  0.4441819190979004
train gradient:  0.10801053109911694
iteration : 2058
train acc:  0.75
train loss:  0.4884302020072937
train gradient:  0.12492893962923837
iteration : 2059
train acc:  0.7734375
train loss:  0.5209001302719116
train gradient:  0.15648982364243857
iteration : 2060
train acc:  0.765625
train loss:  0.492715060710907
train gradient:  0.1540024581182639
iteration : 2061
train acc:  0.75
train loss:  0.45550429821014404
train gradient:  0.10433913158561324
iteration : 2062
train acc:  0.7734375
train loss:  0.4604645371437073
train gradient:  0.10216285695355808
iteration : 2063
train acc:  0.8046875
train loss:  0.4417921304702759
train gradient:  0.10217005783063954
iteration : 2064
train acc:  0.75
train loss:  0.4841655194759369
train gradient:  0.106143254452616
iteration : 2065
train acc:  0.7734375
train loss:  0.4850066006183624
train gradient:  0.20080074669854864
iteration : 2066
train acc:  0.75
train loss:  0.4476394057273865
train gradient:  0.08648075360091609
iteration : 2067
train acc:  0.765625
train loss:  0.4798886775970459
train gradient:  0.14601724238695618
iteration : 2068
train acc:  0.71875
train loss:  0.4867400527000427
train gradient:  0.1567981739938594
iteration : 2069
train acc:  0.75
train loss:  0.5224825739860535
train gradient:  0.16071720497896597
iteration : 2070
train acc:  0.75
train loss:  0.46878132224082947
train gradient:  0.11592321926345803
iteration : 2071
train acc:  0.7890625
train loss:  0.4224410951137543
train gradient:  0.09013071360844344
iteration : 2072
train acc:  0.734375
train loss:  0.47217249870300293
train gradient:  0.12370996986201745
iteration : 2073
train acc:  0.7421875
train loss:  0.49949216842651367
train gradient:  0.11220117790479449
iteration : 2074
train acc:  0.734375
train loss:  0.468869686126709
train gradient:  0.11272156298226772
iteration : 2075
train acc:  0.7734375
train loss:  0.5027761459350586
train gradient:  0.12722577776887822
iteration : 2076
train acc:  0.75
train loss:  0.4940042495727539
train gradient:  0.13504705550939877
iteration : 2077
train acc:  0.71875
train loss:  0.5130904912948608
train gradient:  0.14904070491277915
iteration : 2078
train acc:  0.671875
train loss:  0.5525578260421753
train gradient:  0.15918901212130177
iteration : 2079
train acc:  0.765625
train loss:  0.48851996660232544
train gradient:  0.10935311325325134
iteration : 2080
train acc:  0.796875
train loss:  0.4565681517124176
train gradient:  0.1518481383252764
iteration : 2081
train acc:  0.7578125
train loss:  0.47745561599731445
train gradient:  0.1107690615512159
iteration : 2082
train acc:  0.75
train loss:  0.48970845341682434
train gradient:  0.1104305974122278
iteration : 2083
train acc:  0.6640625
train loss:  0.5814697742462158
train gradient:  0.17549399835945084
iteration : 2084
train acc:  0.7265625
train loss:  0.5235085487365723
train gradient:  0.14549648068436127
iteration : 2085
train acc:  0.7578125
train loss:  0.46270519495010376
train gradient:  0.10944435469018685
iteration : 2086
train acc:  0.71875
train loss:  0.4993975758552551
train gradient:  0.111553905494684
iteration : 2087
train acc:  0.7734375
train loss:  0.4375537037849426
train gradient:  0.10141319267020392
iteration : 2088
train acc:  0.734375
train loss:  0.4866904318332672
train gradient:  0.11987865978140613
iteration : 2089
train acc:  0.7265625
train loss:  0.4720258116722107
train gradient:  0.11497600619117074
iteration : 2090
train acc:  0.7421875
train loss:  0.5091298222541809
train gradient:  0.12588674530963562
iteration : 2091
train acc:  0.75
train loss:  0.5117265582084656
train gradient:  0.1159258908093344
iteration : 2092
train acc:  0.796875
train loss:  0.406240850687027
train gradient:  0.08990462458847637
iteration : 2093
train acc:  0.75
train loss:  0.4758530259132385
train gradient:  0.11186278378777562
iteration : 2094
train acc:  0.6875
train loss:  0.5654638409614563
train gradient:  0.16870966185374858
iteration : 2095
train acc:  0.765625
train loss:  0.5065976977348328
train gradient:  0.1349843875345725
iteration : 2096
train acc:  0.7578125
train loss:  0.4488990306854248
train gradient:  0.13162275450410893
iteration : 2097
train acc:  0.6953125
train loss:  0.5267050266265869
train gradient:  0.124867582322777
iteration : 2098
train acc:  0.7734375
train loss:  0.43602392077445984
train gradient:  0.0950677859066291
iteration : 2099
train acc:  0.7421875
train loss:  0.5013220310211182
train gradient:  0.14972110310270426
iteration : 2100
train acc:  0.796875
train loss:  0.4262484312057495
train gradient:  0.11288008617299868
iteration : 2101
train acc:  0.78125
train loss:  0.4505804777145386
train gradient:  0.10412961619386352
iteration : 2102
train acc:  0.7109375
train loss:  0.5108754634857178
train gradient:  0.17266192288693263
iteration : 2103
train acc:  0.796875
train loss:  0.4629673957824707
train gradient:  0.10312422822148676
iteration : 2104
train acc:  0.75
train loss:  0.4807475209236145
train gradient:  0.10971865397240442
iteration : 2105
train acc:  0.75
train loss:  0.5018664598464966
train gradient:  0.1382381886729548
iteration : 2106
train acc:  0.765625
train loss:  0.4506482481956482
train gradient:  0.12248394517216048
iteration : 2107
train acc:  0.765625
train loss:  0.4583527743816376
train gradient:  0.11003932468460366
iteration : 2108
train acc:  0.8046875
train loss:  0.41481804847717285
train gradient:  0.08145767240902174
iteration : 2109
train acc:  0.6953125
train loss:  0.5133069157600403
train gradient:  0.1257390619087171
iteration : 2110
train acc:  0.7578125
train loss:  0.48278921842575073
train gradient:  0.12795475840653142
iteration : 2111
train acc:  0.7109375
train loss:  0.5092740058898926
train gradient:  0.10709949390209093
iteration : 2112
train acc:  0.7265625
train loss:  0.5094385147094727
train gradient:  0.1264579718333432
iteration : 2113
train acc:  0.6796875
train loss:  0.5069345235824585
train gradient:  0.12967902898589936
iteration : 2114
train acc:  0.6875
train loss:  0.5962280631065369
train gradient:  0.1827834498406818
iteration : 2115
train acc:  0.7265625
train loss:  0.46069204807281494
train gradient:  0.1140277771477612
iteration : 2116
train acc:  0.6484375
train loss:  0.6320137977600098
train gradient:  0.20373829540225724
iteration : 2117
train acc:  0.8046875
train loss:  0.40765130519866943
train gradient:  0.09624237155110298
iteration : 2118
train acc:  0.8046875
train loss:  0.42866408824920654
train gradient:  0.0975459113940211
iteration : 2119
train acc:  0.7265625
train loss:  0.5513997077941895
train gradient:  0.1474505759363161
iteration : 2120
train acc:  0.75
train loss:  0.5026336312294006
train gradient:  0.13536711849758504
iteration : 2121
train acc:  0.7734375
train loss:  0.4703157842159271
train gradient:  0.107885070066952
iteration : 2122
train acc:  0.765625
train loss:  0.5471707582473755
train gradient:  0.1688384988807639
iteration : 2123
train acc:  0.7734375
train loss:  0.5029938220977783
train gradient:  0.14586701150953552
iteration : 2124
train acc:  0.75
train loss:  0.4714696407318115
train gradient:  0.12179642458563608
iteration : 2125
train acc:  0.7265625
train loss:  0.5172401070594788
train gradient:  0.14457971081223042
iteration : 2126
train acc:  0.71875
train loss:  0.4577842056751251
train gradient:  0.12645937787840775
iteration : 2127
train acc:  0.6953125
train loss:  0.5368505716323853
train gradient:  0.14043896473589895
iteration : 2128
train acc:  0.7421875
train loss:  0.4738802909851074
train gradient:  0.12296955697611435
iteration : 2129
train acc:  0.8359375
train loss:  0.3875499665737152
train gradient:  0.06876072676660089
iteration : 2130
train acc:  0.7109375
train loss:  0.4915417432785034
train gradient:  0.11071145154245571
iteration : 2131
train acc:  0.75
train loss:  0.46690189838409424
train gradient:  0.10381881187666463
iteration : 2132
train acc:  0.7421875
train loss:  0.4548647403717041
train gradient:  0.1086154062102916
iteration : 2133
train acc:  0.71875
train loss:  0.46989110112190247
train gradient:  0.14354737515428917
iteration : 2134
train acc:  0.7421875
train loss:  0.570359468460083
train gradient:  0.16107581742981347
iteration : 2135
train acc:  0.734375
train loss:  0.47715064883232117
train gradient:  0.09845072470612072
iteration : 2136
train acc:  0.765625
train loss:  0.4573747217655182
train gradient:  0.11084603961518942
iteration : 2137
train acc:  0.7109375
train loss:  0.5764619708061218
train gradient:  0.17228659945999036
iteration : 2138
train acc:  0.8203125
train loss:  0.4408886432647705
train gradient:  0.12068342473638921
iteration : 2139
train acc:  0.78125
train loss:  0.4498768448829651
train gradient:  0.10961625690465604
iteration : 2140
train acc:  0.7578125
train loss:  0.5021669864654541
train gradient:  0.1774888040717238
iteration : 2141
train acc:  0.7109375
train loss:  0.5475651025772095
train gradient:  0.16804534908126853
iteration : 2142
train acc:  0.6875
train loss:  0.550696611404419
train gradient:  0.16890963347209886
iteration : 2143
train acc:  0.78125
train loss:  0.4688454866409302
train gradient:  0.09849930007348624
iteration : 2144
train acc:  0.71875
train loss:  0.48233911395072937
train gradient:  0.1288062241801274
iteration : 2145
train acc:  0.796875
train loss:  0.4418991804122925
train gradient:  0.09373216538591214
iteration : 2146
train acc:  0.7578125
train loss:  0.4179072380065918
train gradient:  0.10773689904917735
iteration : 2147
train acc:  0.734375
train loss:  0.4794197082519531
train gradient:  0.11870367925536526
iteration : 2148
train acc:  0.734375
train loss:  0.4934406876564026
train gradient:  0.14953517523964935
iteration : 2149
train acc:  0.7109375
train loss:  0.5751733183860779
train gradient:  0.18717740488430057
iteration : 2150
train acc:  0.7421875
train loss:  0.5285143256187439
train gradient:  0.10398767433935471
iteration : 2151
train acc:  0.765625
train loss:  0.4374232292175293
train gradient:  0.14159162315938165
iteration : 2152
train acc:  0.6953125
train loss:  0.5529716610908508
train gradient:  0.15022122373554997
iteration : 2153
train acc:  0.7578125
train loss:  0.48110103607177734
train gradient:  0.13293546768743678
iteration : 2154
train acc:  0.7265625
train loss:  0.4980553686618805
train gradient:  0.15045739236852262
iteration : 2155
train acc:  0.6953125
train loss:  0.5306580066680908
train gradient:  0.1235563518373409
iteration : 2156
train acc:  0.765625
train loss:  0.4885309934616089
train gradient:  0.14207620828195539
iteration : 2157
train acc:  0.6953125
train loss:  0.5231277942657471
train gradient:  0.1643464528149275
iteration : 2158
train acc:  0.734375
train loss:  0.4830601215362549
train gradient:  0.1053584559101948
iteration : 2159
train acc:  0.6875
train loss:  0.523012638092041
train gradient:  0.13532527650069803
iteration : 2160
train acc:  0.6953125
train loss:  0.5690931081771851
train gradient:  0.1997614122257324
iteration : 2161
train acc:  0.75
train loss:  0.4939776062965393
train gradient:  0.1223519269050908
iteration : 2162
train acc:  0.75
train loss:  0.46489691734313965
train gradient:  0.14530229080825507
iteration : 2163
train acc:  0.734375
train loss:  0.46799981594085693
train gradient:  0.13147019895236814
iteration : 2164
train acc:  0.828125
train loss:  0.3889823257923126
train gradient:  0.08941468950063357
iteration : 2165
train acc:  0.78125
train loss:  0.4547713100910187
train gradient:  0.09613080518208032
iteration : 2166
train acc:  0.7109375
train loss:  0.5661182403564453
train gradient:  0.16166889663624057
iteration : 2167
train acc:  0.7578125
train loss:  0.44641435146331787
train gradient:  0.11044476983603073
iteration : 2168
train acc:  0.7578125
train loss:  0.4678305387496948
train gradient:  0.12325795756870978
iteration : 2169
train acc:  0.7421875
train loss:  0.43505892157554626
train gradient:  0.09745253095340045
iteration : 2170
train acc:  0.6953125
train loss:  0.5139163732528687
train gradient:  0.11542003669504773
iteration : 2171
train acc:  0.7265625
train loss:  0.46805816888809204
train gradient:  0.09996900573899595
iteration : 2172
train acc:  0.7421875
train loss:  0.46362626552581787
train gradient:  0.10988573596891649
iteration : 2173
train acc:  0.6953125
train loss:  0.5339982509613037
train gradient:  0.16498744403168233
iteration : 2174
train acc:  0.75
train loss:  0.47354501485824585
train gradient:  0.10396383541051393
iteration : 2175
train acc:  0.7578125
train loss:  0.4603756368160248
train gradient:  0.13968408255859505
iteration : 2176
train acc:  0.734375
train loss:  0.4661773443222046
train gradient:  0.11961266388936656
iteration : 2177
train acc:  0.765625
train loss:  0.45127975940704346
train gradient:  0.10681799079687522
iteration : 2178
train acc:  0.75
train loss:  0.44174814224243164
train gradient:  0.10536750704752273
iteration : 2179
train acc:  0.734375
train loss:  0.5331718325614929
train gradient:  0.12132105655927493
iteration : 2180
train acc:  0.7578125
train loss:  0.5104014873504639
train gradient:  0.11172007506106016
iteration : 2181
train acc:  0.75
train loss:  0.44620460271835327
train gradient:  0.11042740718079569
iteration : 2182
train acc:  0.703125
train loss:  0.5599190592765808
train gradient:  0.16334735407210513
iteration : 2183
train acc:  0.7265625
train loss:  0.5017128586769104
train gradient:  0.10995214734417416
iteration : 2184
train acc:  0.75
train loss:  0.4435536563396454
train gradient:  0.10533104128550042
iteration : 2185
train acc:  0.6796875
train loss:  0.5926234722137451
train gradient:  0.22405077489746736
iteration : 2186
train acc:  0.7890625
train loss:  0.4026409685611725
train gradient:  0.08950106472963074
iteration : 2187
train acc:  0.765625
train loss:  0.44831007719039917
train gradient:  0.08775117505632445
iteration : 2188
train acc:  0.6796875
train loss:  0.5522359013557434
train gradient:  0.11542432639386925
iteration : 2189
train acc:  0.765625
train loss:  0.44889548420906067
train gradient:  0.09530639099060917
iteration : 2190
train acc:  0.7265625
train loss:  0.5083626508712769
train gradient:  0.1329562456739115
iteration : 2191
train acc:  0.78125
train loss:  0.4811515212059021
train gradient:  0.13005427022996358
iteration : 2192
train acc:  0.7265625
train loss:  0.5164471864700317
train gradient:  0.13791898950502046
iteration : 2193
train acc:  0.75
train loss:  0.4757848381996155
train gradient:  0.11515895690231011
iteration : 2194
train acc:  0.765625
train loss:  0.44514545798301697
train gradient:  0.09692263004002076
iteration : 2195
train acc:  0.71875
train loss:  0.4688134789466858
train gradient:  0.10758066579203734
iteration : 2196
train acc:  0.734375
train loss:  0.5419600605964661
train gradient:  0.14108556853621862
iteration : 2197
train acc:  0.75
train loss:  0.5394961833953857
train gradient:  0.14147215617228148
iteration : 2198
train acc:  0.7265625
train loss:  0.46543025970458984
train gradient:  0.12165933388178206
iteration : 2199
train acc:  0.703125
train loss:  0.5023174285888672
train gradient:  0.13780422399768877
iteration : 2200
train acc:  0.78125
train loss:  0.4367530345916748
train gradient:  0.11201724768717058
iteration : 2201
train acc:  0.71875
train loss:  0.5862029790878296
train gradient:  0.23320096893897696
iteration : 2202
train acc:  0.765625
train loss:  0.427518367767334
train gradient:  0.08317640407849321
iteration : 2203
train acc:  0.7578125
train loss:  0.48839178681373596
train gradient:  0.10850940472909534
iteration : 2204
train acc:  0.6875
train loss:  0.5501142144203186
train gradient:  0.1435313000069584
iteration : 2205
train acc:  0.7265625
train loss:  0.4523986876010895
train gradient:  0.11278917891673243
iteration : 2206
train acc:  0.75
train loss:  0.4502258598804474
train gradient:  0.1157782074923577
iteration : 2207
train acc:  0.7578125
train loss:  0.4333944320678711
train gradient:  0.10835888682533386
iteration : 2208
train acc:  0.7578125
train loss:  0.4582456946372986
train gradient:  0.1140172512649055
iteration : 2209
train acc:  0.71875
train loss:  0.5314900279045105
train gradient:  0.1394128604666489
iteration : 2210
train acc:  0.7578125
train loss:  0.4333243668079376
train gradient:  0.11000292411841897
iteration : 2211
train acc:  0.6640625
train loss:  0.4708363711833954
train gradient:  0.11800279869797342
iteration : 2212
train acc:  0.7890625
train loss:  0.4551730155944824
train gradient:  0.11224794324101185
iteration : 2213
train acc:  0.78125
train loss:  0.4925554692745209
train gradient:  0.11086903509678676
iteration : 2214
train acc:  0.703125
train loss:  0.4880027174949646
train gradient:  0.11814503012464414
iteration : 2215
train acc:  0.7890625
train loss:  0.42370665073394775
train gradient:  0.12818384669357175
iteration : 2216
train acc:  0.796875
train loss:  0.46838730573654175
train gradient:  0.11214534293199122
iteration : 2217
train acc:  0.78125
train loss:  0.4517443776130676
train gradient:  0.08791867788305803
iteration : 2218
train acc:  0.703125
train loss:  0.5284973382949829
train gradient:  0.12692966623127516
iteration : 2219
train acc:  0.734375
train loss:  0.4639977812767029
train gradient:  0.11027366257965522
iteration : 2220
train acc:  0.765625
train loss:  0.48706144094467163
train gradient:  0.2026604963505924
iteration : 2221
train acc:  0.7421875
train loss:  0.4847915768623352
train gradient:  0.1145420977631971
iteration : 2222
train acc:  0.765625
train loss:  0.45701879262924194
train gradient:  0.1007598763760002
iteration : 2223
train acc:  0.78125
train loss:  0.4821056127548218
train gradient:  0.11914643204318043
iteration : 2224
train acc:  0.6640625
train loss:  0.5110568404197693
train gradient:  0.11074001645447525
iteration : 2225
train acc:  0.8125
train loss:  0.5283876657485962
train gradient:  0.12569898368288115
iteration : 2226
train acc:  0.734375
train loss:  0.4993392825126648
train gradient:  0.11910205637246207
iteration : 2227
train acc:  0.7890625
train loss:  0.41588932275772095
train gradient:  0.07244758549502246
iteration : 2228
train acc:  0.7265625
train loss:  0.47292643785476685
train gradient:  0.0949044336598506
iteration : 2229
train acc:  0.71875
train loss:  0.49263569712638855
train gradient:  0.15780618907050287
iteration : 2230
train acc:  0.765625
train loss:  0.4845115840435028
train gradient:  0.14403664034515323
iteration : 2231
train acc:  0.8046875
train loss:  0.40985172986984253
train gradient:  0.09180583841110815
iteration : 2232
train acc:  0.703125
train loss:  0.5612214803695679
train gradient:  0.16417383372577998
iteration : 2233
train acc:  0.7578125
train loss:  0.5133734345436096
train gradient:  0.10996141363774481
iteration : 2234
train acc:  0.6875
train loss:  0.5321075320243835
train gradient:  0.13882340784372585
iteration : 2235
train acc:  0.71875
train loss:  0.4770861864089966
train gradient:  0.10175263320040834
iteration : 2236
train acc:  0.7109375
train loss:  0.49734875559806824
train gradient:  0.11337529241008869
iteration : 2237
train acc:  0.71875
train loss:  0.5086417198181152
train gradient:  0.15243498542728562
iteration : 2238
train acc:  0.734375
train loss:  0.5068814754486084
train gradient:  0.13542522221087355
iteration : 2239
train acc:  0.765625
train loss:  0.39937373995780945
train gradient:  0.09074877420978042
iteration : 2240
train acc:  0.7265625
train loss:  0.47170785069465637
train gradient:  0.11616866722606395
iteration : 2241
train acc:  0.7578125
train loss:  0.45083433389663696
train gradient:  0.13307186959160028
iteration : 2242
train acc:  0.7890625
train loss:  0.42295438051223755
train gradient:  0.09291005207787352
iteration : 2243
train acc:  0.7734375
train loss:  0.42776209115982056
train gradient:  0.11092617296225793
iteration : 2244
train acc:  0.71875
train loss:  0.4981071352958679
train gradient:  0.12353920008617368
iteration : 2245
train acc:  0.7578125
train loss:  0.46574166417121887
train gradient:  0.10606431722262316
iteration : 2246
train acc:  0.796875
train loss:  0.40645018219947815
train gradient:  0.10030955427125282
iteration : 2247
train acc:  0.7109375
train loss:  0.48369574546813965
train gradient:  0.11110537162816832
iteration : 2248
train acc:  0.703125
train loss:  0.5443077087402344
train gradient:  0.13468526520501767
iteration : 2249
train acc:  0.7265625
train loss:  0.44293659925460815
train gradient:  0.10231286314172959
iteration : 2250
train acc:  0.75
train loss:  0.4224371314048767
train gradient:  0.08864687338947795
iteration : 2251
train acc:  0.75
train loss:  0.5169901847839355
train gradient:  0.11771319176304622
iteration : 2252
train acc:  0.75
train loss:  0.5273112058639526
train gradient:  0.16808447634038629
iteration : 2253
train acc:  0.7421875
train loss:  0.4655952751636505
train gradient:  0.10902424303921712
iteration : 2254
train acc:  0.7421875
train loss:  0.4871799945831299
train gradient:  0.1350446261473435
iteration : 2255
train acc:  0.7578125
train loss:  0.4712291657924652
train gradient:  0.11814138741209705
iteration : 2256
train acc:  0.8046875
train loss:  0.4413977861404419
train gradient:  0.12314357127588554
iteration : 2257
train acc:  0.796875
train loss:  0.48530659079551697
train gradient:  0.11860515381869217
iteration : 2258
train acc:  0.796875
train loss:  0.38756871223449707
train gradient:  0.09165988780233675
iteration : 2259
train acc:  0.7578125
train loss:  0.5067655444145203
train gradient:  0.13731289484421172
iteration : 2260
train acc:  0.8203125
train loss:  0.4515042304992676
train gradient:  0.19285822882778106
iteration : 2261
train acc:  0.71875
train loss:  0.5131914615631104
train gradient:  0.14669485724209017
iteration : 2262
train acc:  0.75
train loss:  0.4538366198539734
train gradient:  0.12860574493266552
iteration : 2263
train acc:  0.8046875
train loss:  0.42712390422821045
train gradient:  0.09889291628762967
iteration : 2264
train acc:  0.6796875
train loss:  0.5199893712997437
train gradient:  0.12561335587256311
iteration : 2265
train acc:  0.78125
train loss:  0.4621560871601105
train gradient:  0.11043234978700288
iteration : 2266
train acc:  0.7421875
train loss:  0.4827769696712494
train gradient:  0.12357760264551523
iteration : 2267
train acc:  0.7578125
train loss:  0.5191872119903564
train gradient:  0.132365745193624
iteration : 2268
train acc:  0.7578125
train loss:  0.4281882643699646
train gradient:  0.09451186150789348
iteration : 2269
train acc:  0.7734375
train loss:  0.43770265579223633
train gradient:  0.07895124089163107
iteration : 2270
train acc:  0.7734375
train loss:  0.4669681787490845
train gradient:  0.11811199149031795
iteration : 2271
train acc:  0.765625
train loss:  0.4624265730381012
train gradient:  0.10632985829110142
iteration : 2272
train acc:  0.7578125
train loss:  0.4781408905982971
train gradient:  0.1216487465016577
iteration : 2273
train acc:  0.7734375
train loss:  0.5271952152252197
train gradient:  0.1496598443966627
iteration : 2274
train acc:  0.7578125
train loss:  0.5072126388549805
train gradient:  0.14873984723445588
iteration : 2275
train acc:  0.7265625
train loss:  0.4726549983024597
train gradient:  0.12117469877018625
iteration : 2276
train acc:  0.765625
train loss:  0.4677775204181671
train gradient:  0.13567543308480642
iteration : 2277
train acc:  0.71875
train loss:  0.4701254367828369
train gradient:  0.09301880252841957
iteration : 2278
train acc:  0.7734375
train loss:  0.4675341844558716
train gradient:  0.11746900105683991
iteration : 2279
train acc:  0.7421875
train loss:  0.5139638185501099
train gradient:  0.13447961671511774
iteration : 2280
train acc:  0.7578125
train loss:  0.4941016137599945
train gradient:  0.158832024275989
iteration : 2281
train acc:  0.765625
train loss:  0.43077296018600464
train gradient:  0.11065524173789397
iteration : 2282
train acc:  0.734375
train loss:  0.5393638610839844
train gradient:  0.1403960905566733
iteration : 2283
train acc:  0.7421875
train loss:  0.4853000044822693
train gradient:  0.12243760310126779
iteration : 2284
train acc:  0.75
train loss:  0.47898733615875244
train gradient:  0.1071400609935593
iteration : 2285
train acc:  0.703125
train loss:  0.546288251876831
train gradient:  0.16533437126397943
iteration : 2286
train acc:  0.796875
train loss:  0.4844212532043457
train gradient:  0.1136943192423764
iteration : 2287
train acc:  0.7578125
train loss:  0.5402475595474243
train gradient:  0.16709665405974453
iteration : 2288
train acc:  0.7734375
train loss:  0.451810359954834
train gradient:  0.1032333843397979
iteration : 2289
train acc:  0.7109375
train loss:  0.5588610172271729
train gradient:  0.1975417958023752
iteration : 2290
train acc:  0.734375
train loss:  0.49916473031044006
train gradient:  0.11682982283018446
iteration : 2291
train acc:  0.75
train loss:  0.4495103061199188
train gradient:  0.10872722135664652
iteration : 2292
train acc:  0.734375
train loss:  0.5133057832717896
train gradient:  0.10526865780761803
iteration : 2293
train acc:  0.78125
train loss:  0.4511938691139221
train gradient:  0.12063293435350898
iteration : 2294
train acc:  0.7578125
train loss:  0.4815939664840698
train gradient:  0.1496961012838378
iteration : 2295
train acc:  0.7578125
train loss:  0.5088423490524292
train gradient:  0.14192883512017243
iteration : 2296
train acc:  0.7109375
train loss:  0.5356175899505615
train gradient:  0.1648407750626063
iteration : 2297
train acc:  0.7421875
train loss:  0.49026384949684143
train gradient:  0.12951137360812442
iteration : 2298
train acc:  0.703125
train loss:  0.5630394220352173
train gradient:  0.14372484423604487
iteration : 2299
train acc:  0.796875
train loss:  0.4094027280807495
train gradient:  0.08811277064890798
iteration : 2300
train acc:  0.7734375
train loss:  0.4554330110549927
train gradient:  0.09627337528802742
iteration : 2301
train acc:  0.796875
train loss:  0.4729105532169342
train gradient:  0.11636863592052239
iteration : 2302
train acc:  0.734375
train loss:  0.4691552221775055
train gradient:  0.09365057713684569
iteration : 2303
train acc:  0.7734375
train loss:  0.4883577227592468
train gradient:  0.12513051180842605
iteration : 2304
train acc:  0.7734375
train loss:  0.5226753950119019
train gradient:  0.11406346148657026
iteration : 2305
train acc:  0.8046875
train loss:  0.4395637512207031
train gradient:  0.08873978512762407
iteration : 2306
train acc:  0.734375
train loss:  0.4893954396247864
train gradient:  0.11851983303232083
iteration : 2307
train acc:  0.8125
train loss:  0.43050074577331543
train gradient:  0.10418514427028559
iteration : 2308
train acc:  0.71875
train loss:  0.5459212064743042
train gradient:  0.14317452615961024
iteration : 2309
train acc:  0.78125
train loss:  0.4409993588924408
train gradient:  0.10792972324321962
iteration : 2310
train acc:  0.6953125
train loss:  0.555118978023529
train gradient:  0.14327890816855202
iteration : 2311
train acc:  0.7265625
train loss:  0.5457069873809814
train gradient:  0.14779407769131553
iteration : 2312
train acc:  0.8046875
train loss:  0.4211598038673401
train gradient:  0.08838777592724256
iteration : 2313
train acc:  0.65625
train loss:  0.6676487326622009
train gradient:  0.2226587404148725
iteration : 2314
train acc:  0.7109375
train loss:  0.5606727600097656
train gradient:  0.1384726955915384
iteration : 2315
train acc:  0.828125
train loss:  0.4433706998825073
train gradient:  0.090104132183034
iteration : 2316
train acc:  0.875
train loss:  0.39938467741012573
train gradient:  0.09024647770575836
iteration : 2317
train acc:  0.703125
train loss:  0.5431393384933472
train gradient:  0.15944294966888195
iteration : 2318
train acc:  0.7890625
train loss:  0.45010846853256226
train gradient:  0.11747968069942753
iteration : 2319
train acc:  0.7421875
train loss:  0.5138339996337891
train gradient:  0.13877896184141642
iteration : 2320
train acc:  0.75
train loss:  0.5728775262832642
train gradient:  0.13644744866151357
iteration : 2321
train acc:  0.734375
train loss:  0.5092678070068359
train gradient:  0.11877089662179498
iteration : 2322
train acc:  0.7578125
train loss:  0.5179712772369385
train gradient:  0.1484943876460173
iteration : 2323
train acc:  0.7578125
train loss:  0.4235713481903076
train gradient:  0.07402681442746387
iteration : 2324
train acc:  0.734375
train loss:  0.5107046365737915
train gradient:  0.10630630209446529
iteration : 2325
train acc:  0.7265625
train loss:  0.4889841675758362
train gradient:  0.12476424411966165
iteration : 2326
train acc:  0.6796875
train loss:  0.6100778579711914
train gradient:  0.1703913029828884
iteration : 2327
train acc:  0.7109375
train loss:  0.49361610412597656
train gradient:  0.10426624190416794
iteration : 2328
train acc:  0.8046875
train loss:  0.44000253081321716
train gradient:  0.12796264325336157
iteration : 2329
train acc:  0.71875
train loss:  0.515160083770752
train gradient:  0.11992800157284969
iteration : 2330
train acc:  0.7578125
train loss:  0.5480118989944458
train gradient:  0.13099910970162343
iteration : 2331
train acc:  0.7578125
train loss:  0.534990668296814
train gradient:  0.1484714822757221
iteration : 2332
train acc:  0.6796875
train loss:  0.5444067716598511
train gradient:  0.13511207547841037
iteration : 2333
train acc:  0.71875
train loss:  0.4726542830467224
train gradient:  0.10498119643663212
iteration : 2334
train acc:  0.7109375
train loss:  0.5464961528778076
train gradient:  0.13724293075606117
iteration : 2335
train acc:  0.7578125
train loss:  0.4618343710899353
train gradient:  0.11859173167642452
iteration : 2336
train acc:  0.7265625
train loss:  0.49835729598999023
train gradient:  0.13035335929950986
iteration : 2337
train acc:  0.8046875
train loss:  0.40127599239349365
train gradient:  0.08056075475309755
iteration : 2338
train acc:  0.78125
train loss:  0.456704318523407
train gradient:  0.1079419859921865
iteration : 2339
train acc:  0.75
train loss:  0.48025262355804443
train gradient:  0.10444491546287113
iteration : 2340
train acc:  0.75
train loss:  0.44960325956344604
train gradient:  0.10733449960449334
iteration : 2341
train acc:  0.75
train loss:  0.5089067220687866
train gradient:  0.1458563030712448
iteration : 2342
train acc:  0.8046875
train loss:  0.4192326068878174
train gradient:  0.07273274292671081
iteration : 2343
train acc:  0.6875
train loss:  0.4935891628265381
train gradient:  0.1452046179782109
iteration : 2344
train acc:  0.6953125
train loss:  0.5605542659759521
train gradient:  0.17063386291945215
iteration : 2345
train acc:  0.796875
train loss:  0.469920814037323
train gradient:  0.10569001189526787
iteration : 2346
train acc:  0.7265625
train loss:  0.5232151746749878
train gradient:  0.12958852713476754
iteration : 2347
train acc:  0.765625
train loss:  0.4634568691253662
train gradient:  0.11053983681270342
iteration : 2348
train acc:  0.6875
train loss:  0.5959404706954956
train gradient:  0.16550362784557493
iteration : 2349
train acc:  0.7890625
train loss:  0.46216273307800293
train gradient:  0.10775775148210925
iteration : 2350
train acc:  0.703125
train loss:  0.5228374004364014
train gradient:  0.1332444506239529
iteration : 2351
train acc:  0.7578125
train loss:  0.5484116077423096
train gradient:  0.14162155501958398
iteration : 2352
train acc:  0.75
train loss:  0.48046576976776123
train gradient:  0.1345823948427592
iteration : 2353
train acc:  0.765625
train loss:  0.4890158474445343
train gradient:  0.14160166352463294
iteration : 2354
train acc:  0.765625
train loss:  0.4898332953453064
train gradient:  0.10823948451627216
iteration : 2355
train acc:  0.703125
train loss:  0.5732278823852539
train gradient:  0.17833911820029258
iteration : 2356
train acc:  0.75
train loss:  0.5284124612808228
train gradient:  0.1326842369747162
iteration : 2357
train acc:  0.7421875
train loss:  0.44626349210739136
train gradient:  0.11397735315656071
iteration : 2358
train acc:  0.7890625
train loss:  0.48709118366241455
train gradient:  0.12019552819556768
iteration : 2359
train acc:  0.8046875
train loss:  0.4299388527870178
train gradient:  0.11770114147216965
iteration : 2360
train acc:  0.78125
train loss:  0.47631561756134033
train gradient:  0.1001296630960983
iteration : 2361
train acc:  0.78125
train loss:  0.440524160861969
train gradient:  0.08371804572871495
iteration : 2362
train acc:  0.765625
train loss:  0.43119239807128906
train gradient:  0.08337483402953315
iteration : 2363
train acc:  0.78125
train loss:  0.44616764783859253
train gradient:  0.100566394429745
iteration : 2364
train acc:  0.71875
train loss:  0.5010178089141846
train gradient:  0.13459952900502364
iteration : 2365
train acc:  0.8203125
train loss:  0.4129871129989624
train gradient:  0.09808785670923789
iteration : 2366
train acc:  0.7578125
train loss:  0.49586665630340576
train gradient:  0.09765102184888401
iteration : 2367
train acc:  0.71875
train loss:  0.5062699913978577
train gradient:  0.12496726174565949
iteration : 2368
train acc:  0.828125
train loss:  0.41221946477890015
train gradient:  0.08433058914001519
iteration : 2369
train acc:  0.7734375
train loss:  0.4052414894104004
train gradient:  0.08751357776402978
iteration : 2370
train acc:  0.7578125
train loss:  0.4607260823249817
train gradient:  0.11314411714885832
iteration : 2371
train acc:  0.7578125
train loss:  0.46570128202438354
train gradient:  0.13286980094311365
iteration : 2372
train acc:  0.75
train loss:  0.4615786671638489
train gradient:  0.11824854082558015
iteration : 2373
train acc:  0.6796875
train loss:  0.5785816311836243
train gradient:  0.15334685845328905
iteration : 2374
train acc:  0.6953125
train loss:  0.5211191177368164
train gradient:  0.13982226577783208
iteration : 2375
train acc:  0.71875
train loss:  0.5015673637390137
train gradient:  0.11932838760340721
iteration : 2376
train acc:  0.6953125
train loss:  0.610209584236145
train gradient:  0.15800059230519686
iteration : 2377
train acc:  0.78125
train loss:  0.4526629149913788
train gradient:  0.09356457730881114
iteration : 2378
train acc:  0.75
train loss:  0.4556775689125061
train gradient:  0.09405519970309907
iteration : 2379
train acc:  0.75
train loss:  0.43733468651771545
train gradient:  0.08835049001129963
iteration : 2380
train acc:  0.7265625
train loss:  0.5545781850814819
train gradient:  0.1776743665030343
iteration : 2381
train acc:  0.7421875
train loss:  0.4582931101322174
train gradient:  0.08715227964341797
iteration : 2382
train acc:  0.765625
train loss:  0.4600718319416046
train gradient:  0.11627982531946687
iteration : 2383
train acc:  0.78125
train loss:  0.43954819440841675
train gradient:  0.09353099433375507
iteration : 2384
train acc:  0.75
train loss:  0.4712532162666321
train gradient:  0.08540479765795027
iteration : 2385
train acc:  0.7265625
train loss:  0.5099344253540039
train gradient:  0.12963691481026637
iteration : 2386
train acc:  0.765625
train loss:  0.43095606565475464
train gradient:  0.09443562631576627
iteration : 2387
train acc:  0.78125
train loss:  0.455046683549881
train gradient:  0.10222272156938403
iteration : 2388
train acc:  0.765625
train loss:  0.4539145827293396
train gradient:  0.12886805111669006
iteration : 2389
train acc:  0.7421875
train loss:  0.49358367919921875
train gradient:  0.10183091837919533
iteration : 2390
train acc:  0.6640625
train loss:  0.5693703889846802
train gradient:  0.15348257691767297
iteration : 2391
train acc:  0.7421875
train loss:  0.4835004210472107
train gradient:  0.10954232747303252
iteration : 2392
train acc:  0.84375
train loss:  0.38982370495796204
train gradient:  0.07390778181071059
iteration : 2393
train acc:  0.78125
train loss:  0.4551495611667633
train gradient:  0.08612696413238406
iteration : 2394
train acc:  0.6484375
train loss:  0.5523563623428345
train gradient:  0.17612679565378908
iteration : 2395
train acc:  0.7578125
train loss:  0.48825496435165405
train gradient:  0.09899723083352577
iteration : 2396
train acc:  0.7421875
train loss:  0.4662627577781677
train gradient:  0.10954906726305043
iteration : 2397
train acc:  0.78125
train loss:  0.43961840867996216
train gradient:  0.12670925102498978
iteration : 2398
train acc:  0.8046875
train loss:  0.42641061544418335
train gradient:  0.08787534070747005
iteration : 2399
train acc:  0.7734375
train loss:  0.42282915115356445
train gradient:  0.09431886587607632
iteration : 2400
train acc:  0.796875
train loss:  0.4761860966682434
train gradient:  0.11063897623613075
iteration : 2401
train acc:  0.75
train loss:  0.48358508944511414
train gradient:  0.11705189725443235
iteration : 2402
train acc:  0.7734375
train loss:  0.5159327387809753
train gradient:  0.14390753792409378
iteration : 2403
train acc:  0.71875
train loss:  0.5826929807662964
train gradient:  0.20079505584413498
iteration : 2404
train acc:  0.765625
train loss:  0.45713338255882263
train gradient:  0.13636072644366076
iteration : 2405
train acc:  0.7890625
train loss:  0.41796594858169556
train gradient:  0.10138567968561958
iteration : 2406
train acc:  0.7578125
train loss:  0.45724114775657654
train gradient:  0.1127619318884707
iteration : 2407
train acc:  0.703125
train loss:  0.5077790021896362
train gradient:  0.1307114742173469
iteration : 2408
train acc:  0.6796875
train loss:  0.5313026309013367
train gradient:  0.17804439155913646
iteration : 2409
train acc:  0.765625
train loss:  0.4516506791114807
train gradient:  0.10845732369173741
iteration : 2410
train acc:  0.796875
train loss:  0.4744166433811188
train gradient:  0.12015813219146161
iteration : 2411
train acc:  0.734375
train loss:  0.497147798538208
train gradient:  0.1150701140087346
iteration : 2412
train acc:  0.7421875
train loss:  0.51057368516922
train gradient:  0.1331976243873655
iteration : 2413
train acc:  0.71875
train loss:  0.49128326773643494
train gradient:  0.1341626796800218
iteration : 2414
train acc:  0.734375
train loss:  0.5119606852531433
train gradient:  0.12611710780773355
iteration : 2415
train acc:  0.7734375
train loss:  0.47794902324676514
train gradient:  0.09762468896440779
iteration : 2416
train acc:  0.7734375
train loss:  0.4521532654762268
train gradient:  0.1279550856321352
iteration : 2417
train acc:  0.734375
train loss:  0.49315035343170166
train gradient:  0.11331063914267857
iteration : 2418
train acc:  0.7578125
train loss:  0.45903629064559937
train gradient:  0.10892682001671092
iteration : 2419
train acc:  0.703125
train loss:  0.48427730798721313
train gradient:  0.11939334064680257
iteration : 2420
train acc:  0.7421875
train loss:  0.45941054821014404
train gradient:  0.11437165379576686
iteration : 2421
train acc:  0.7578125
train loss:  0.4729102551937103
train gradient:  0.12779597421466457
iteration : 2422
train acc:  0.796875
train loss:  0.4640806317329407
train gradient:  0.11245155189571596
iteration : 2423
train acc:  0.7265625
train loss:  0.4629127085208893
train gradient:  0.11808476310145666
iteration : 2424
train acc:  0.765625
train loss:  0.5218915939331055
train gradient:  0.11839606640248883
iteration : 2425
train acc:  0.7734375
train loss:  0.41593173146247864
train gradient:  0.09052581157897352
iteration : 2426
train acc:  0.7265625
train loss:  0.5287230014801025
train gradient:  0.11537322875834785
iteration : 2427
train acc:  0.7109375
train loss:  0.48063427209854126
train gradient:  0.11627268305870829
iteration : 2428
train acc:  0.765625
train loss:  0.4950096905231476
train gradient:  0.0984525148049708
iteration : 2429
train acc:  0.703125
train loss:  0.5409895777702332
train gradient:  0.12853468688752093
iteration : 2430
train acc:  0.7890625
train loss:  0.44745221734046936
train gradient:  0.0949734202600378
iteration : 2431
train acc:  0.6953125
train loss:  0.5023086071014404
train gradient:  0.1509938996984404
iteration : 2432
train acc:  0.796875
train loss:  0.43128859996795654
train gradient:  0.10108935604803321
iteration : 2433
train acc:  0.7421875
train loss:  0.49101951718330383
train gradient:  0.11006170879746908
iteration : 2434
train acc:  0.78125
train loss:  0.4692697823047638
train gradient:  0.10811629807173777
iteration : 2435
train acc:  0.7578125
train loss:  0.4574238359928131
train gradient:  0.12069671009176049
iteration : 2436
train acc:  0.7734375
train loss:  0.4360552430152893
train gradient:  0.12304923416001823
iteration : 2437
train acc:  0.71875
train loss:  0.5213996767997742
train gradient:  0.11999374663451882
iteration : 2438
train acc:  0.703125
train loss:  0.5405323505401611
train gradient:  0.12693649639037188
iteration : 2439
train acc:  0.7578125
train loss:  0.4783121347427368
train gradient:  0.1434079365884187
iteration : 2440
train acc:  0.7578125
train loss:  0.43042972683906555
train gradient:  0.10111775356432022
iteration : 2441
train acc:  0.7265625
train loss:  0.49170929193496704
train gradient:  0.1288402294726716
iteration : 2442
train acc:  0.6875
train loss:  0.4983976483345032
train gradient:  0.096026429358839
iteration : 2443
train acc:  0.7578125
train loss:  0.4533289670944214
train gradient:  0.10561989106250869
iteration : 2444
train acc:  0.78125
train loss:  0.48070406913757324
train gradient:  0.10242484553928392
iteration : 2445
train acc:  0.7109375
train loss:  0.4644710421562195
train gradient:  0.10513858964778847
iteration : 2446
train acc:  0.78125
train loss:  0.4283974766731262
train gradient:  0.08717883467140304
iteration : 2447
train acc:  0.7890625
train loss:  0.4671052098274231
train gradient:  0.11589993246834966
iteration : 2448
train acc:  0.7734375
train loss:  0.45002299547195435
train gradient:  0.09147667810654563
iteration : 2449
train acc:  0.7734375
train loss:  0.500056803226471
train gradient:  0.12453126193826457
iteration : 2450
train acc:  0.7421875
train loss:  0.49804413318634033
train gradient:  0.14017835286436586
iteration : 2451
train acc:  0.6875
train loss:  0.5379326343536377
train gradient:  0.17725958399318448
iteration : 2452
train acc:  0.796875
train loss:  0.3934885859489441
train gradient:  0.08144275180351038
iteration : 2453
train acc:  0.7265625
train loss:  0.5535407662391663
train gradient:  0.1606040484087652
iteration : 2454
train acc:  0.6953125
train loss:  0.4943065941333771
train gradient:  0.12074747615096518
iteration : 2455
train acc:  0.7109375
train loss:  0.5241912603378296
train gradient:  0.10873973150378194
iteration : 2456
train acc:  0.6953125
train loss:  0.5101649761199951
train gradient:  0.14346380973854916
iteration : 2457
train acc:  0.84375
train loss:  0.3890588879585266
train gradient:  0.0829429014001656
iteration : 2458
train acc:  0.796875
train loss:  0.4532761573791504
train gradient:  0.13023403244828557
iteration : 2459
train acc:  0.765625
train loss:  0.48217326402664185
train gradient:  0.10055035673062122
iteration : 2460
train acc:  0.8046875
train loss:  0.428255558013916
train gradient:  0.10407003592910197
iteration : 2461
train acc:  0.75
train loss:  0.4355848729610443
train gradient:  0.09810461640889688
iteration : 2462
train acc:  0.703125
train loss:  0.51610267162323
train gradient:  0.12529692936183612
iteration : 2463
train acc:  0.7265625
train loss:  0.5061927437782288
train gradient:  0.15219945875662283
iteration : 2464
train acc:  0.7578125
train loss:  0.5236169099807739
train gradient:  0.12681105322256658
iteration : 2465
train acc:  0.75
train loss:  0.4611086845397949
train gradient:  0.1314688299850956
iteration : 2466
train acc:  0.7265625
train loss:  0.5153642892837524
train gradient:  0.13829234937724028
iteration : 2467
train acc:  0.7578125
train loss:  0.4224480986595154
train gradient:  0.09599927957498951
iteration : 2468
train acc:  0.734375
train loss:  0.4959539473056793
train gradient:  0.12614643748147336
iteration : 2469
train acc:  0.7578125
train loss:  0.4486842751502991
train gradient:  0.1562116367622558
iteration : 2470
train acc:  0.796875
train loss:  0.43410196900367737
train gradient:  0.10189874105244566
iteration : 2471
train acc:  0.71875
train loss:  0.5682430267333984
train gradient:  0.18011041565240082
iteration : 2472
train acc:  0.734375
train loss:  0.4931812882423401
train gradient:  0.11577233294441001
iteration : 2473
train acc:  0.6953125
train loss:  0.5301949381828308
train gradient:  0.12901375873484086
iteration : 2474
train acc:  0.8046875
train loss:  0.41435638070106506
train gradient:  0.08297783354798566
iteration : 2475
train acc:  0.7265625
train loss:  0.5793176889419556
train gradient:  0.1865920955893972
iteration : 2476
train acc:  0.78125
train loss:  0.4546390175819397
train gradient:  0.11379379453933683
iteration : 2477
train acc:  0.703125
train loss:  0.4783162772655487
train gradient:  0.1323219751499955
iteration : 2478
train acc:  0.7265625
train loss:  0.4802761673927307
train gradient:  0.11251635554068976
iteration : 2479
train acc:  0.7578125
train loss:  0.41496288776397705
train gradient:  0.1179855379286735
iteration : 2480
train acc:  0.8046875
train loss:  0.43050616979599
train gradient:  0.10978798372110794
iteration : 2481
train acc:  0.765625
train loss:  0.4899307191371918
train gradient:  0.15990606731862567
iteration : 2482
train acc:  0.75
train loss:  0.45083022117614746
train gradient:  0.10774423761297612
iteration : 2483
train acc:  0.703125
train loss:  0.5194993615150452
train gradient:  0.1287742177985947
iteration : 2484
train acc:  0.734375
train loss:  0.5465670824050903
train gradient:  0.16985114348999708
iteration : 2485
train acc:  0.703125
train loss:  0.5267367959022522
train gradient:  0.14043903347655184
iteration : 2486
train acc:  0.828125
train loss:  0.39428064227104187
train gradient:  0.09018719787590153
iteration : 2487
train acc:  0.7734375
train loss:  0.47967180609703064
train gradient:  0.10693585087947725
iteration : 2488
train acc:  0.78125
train loss:  0.482089102268219
train gradient:  0.11401778116463723
iteration : 2489
train acc:  0.8125
train loss:  0.4654473066329956
train gradient:  0.08869259585467489
iteration : 2490
train acc:  0.7421875
train loss:  0.4580560326576233
train gradient:  0.09390683597972364
iteration : 2491
train acc:  0.7109375
train loss:  0.5384568572044373
train gradient:  0.18269121000951183
iteration : 2492
train acc:  0.703125
train loss:  0.4946123957633972
train gradient:  0.10329527266680086
iteration : 2493
train acc:  0.7734375
train loss:  0.45754510164260864
train gradient:  0.12772364609298958
iteration : 2494
train acc:  0.8203125
train loss:  0.41005179286003113
train gradient:  0.10474485994919454
iteration : 2495
train acc:  0.8046875
train loss:  0.45949703454971313
train gradient:  0.11950454711292623
iteration : 2496
train acc:  0.7421875
train loss:  0.4574969410896301
train gradient:  0.08881126216729018
iteration : 2497
train acc:  0.734375
train loss:  0.4831850230693817
train gradient:  0.12027528800170277
iteration : 2498
train acc:  0.765625
train loss:  0.4766235947608948
train gradient:  0.1219854474210468
iteration : 2499
train acc:  0.75
train loss:  0.5415734052658081
train gradient:  0.14842215184933322
iteration : 2500
train acc:  0.75
train loss:  0.47864988446235657
train gradient:  0.10122550506205488
iteration : 2501
train acc:  0.8125
train loss:  0.41206568479537964
train gradient:  0.11143591355832472
iteration : 2502
train acc:  0.765625
train loss:  0.4697760343551636
train gradient:  0.10616354359419355
iteration : 2503
train acc:  0.734375
train loss:  0.5118560791015625
train gradient:  0.11712197579487776
iteration : 2504
train acc:  0.7265625
train loss:  0.5584378242492676
train gradient:  0.15938638947154005
iteration : 2505
train acc:  0.75
train loss:  0.5045603513717651
train gradient:  0.14749372150443546
iteration : 2506
train acc:  0.71875
train loss:  0.5390223264694214
train gradient:  0.126614836186925
iteration : 2507
train acc:  0.734375
train loss:  0.5031086206436157
train gradient:  0.12897855842786124
iteration : 2508
train acc:  0.6875
train loss:  0.5220713019371033
train gradient:  0.13815010911871595
iteration : 2509
train acc:  0.6953125
train loss:  0.5403017401695251
train gradient:  0.12566024596188477
iteration : 2510
train acc:  0.6796875
train loss:  0.5801379084587097
train gradient:  0.16286680811827342
iteration : 2511
train acc:  0.7734375
train loss:  0.46203166246414185
train gradient:  0.10652238235159535
iteration : 2512
train acc:  0.75
train loss:  0.5095466375350952
train gradient:  0.12712080972498768
iteration : 2513
train acc:  0.7890625
train loss:  0.39743733406066895
train gradient:  0.09383713651456779
iteration : 2514
train acc:  0.7734375
train loss:  0.4298037588596344
train gradient:  0.08256236370641963
iteration : 2515
train acc:  0.671875
train loss:  0.5633918642997742
train gradient:  0.14428403986913108
iteration : 2516
train acc:  0.71875
train loss:  0.5188955068588257
train gradient:  0.13848353714371503
iteration : 2517
train acc:  0.7578125
train loss:  0.44634443521499634
train gradient:  0.09429897730098759
iteration : 2518
train acc:  0.8046875
train loss:  0.4708464741706848
train gradient:  0.1253983887152148
iteration : 2519
train acc:  0.7421875
train loss:  0.4707702696323395
train gradient:  0.10381934844420426
iteration : 2520
train acc:  0.78125
train loss:  0.44527462124824524
train gradient:  0.11966998666781163
iteration : 2521
train acc:  0.640625
train loss:  0.5386275053024292
train gradient:  0.118004196050341
iteration : 2522
train acc:  0.8046875
train loss:  0.4010273814201355
train gradient:  0.09797867799520354
iteration : 2523
train acc:  0.7578125
train loss:  0.5018924474716187
train gradient:  0.12161356052653642
iteration : 2524
train acc:  0.8046875
train loss:  0.426360547542572
train gradient:  0.1176272695301816
iteration : 2525
train acc:  0.7421875
train loss:  0.5254713892936707
train gradient:  0.15528941238875116
iteration : 2526
train acc:  0.71875
train loss:  0.509721040725708
train gradient:  0.11365392268654317
iteration : 2527
train acc:  0.7734375
train loss:  0.475786417722702
train gradient:  0.12265032129508659
iteration : 2528
train acc:  0.765625
train loss:  0.49516770243644714
train gradient:  0.1583431079552131
iteration : 2529
train acc:  0.703125
train loss:  0.4674544930458069
train gradient:  0.10218411435338368
iteration : 2530
train acc:  0.703125
train loss:  0.49577465653419495
train gradient:  0.09695884700348557
iteration : 2531
train acc:  0.8515625
train loss:  0.3777969479560852
train gradient:  0.0663810104317132
iteration : 2532
train acc:  0.7421875
train loss:  0.5039205551147461
train gradient:  0.12714471606965955
iteration : 2533
train acc:  0.828125
train loss:  0.4096612334251404
train gradient:  0.09173026775649638
iteration : 2534
train acc:  0.7734375
train loss:  0.4480758309364319
train gradient:  0.10193386702679605
iteration : 2535
train acc:  0.75
train loss:  0.48535215854644775
train gradient:  0.1335469755171813
iteration : 2536
train acc:  0.765625
train loss:  0.4578530490398407
train gradient:  0.1073224806393474
iteration : 2537
train acc:  0.71875
train loss:  0.47893792390823364
train gradient:  0.14291165917889054
iteration : 2538
train acc:  0.765625
train loss:  0.48971760272979736
train gradient:  0.11537540567913807
iteration : 2539
train acc:  0.765625
train loss:  0.5278946161270142
train gradient:  0.13229650388604064
iteration : 2540
train acc:  0.8125
train loss:  0.40116962790489197
train gradient:  0.09947149920026657
iteration : 2541
train acc:  0.78125
train loss:  0.46718090772628784
train gradient:  0.09539935493608649
iteration : 2542
train acc:  0.78125
train loss:  0.5201141238212585
train gradient:  0.12678406992307656
iteration : 2543
train acc:  0.78125
train loss:  0.4250717759132385
train gradient:  0.09185673809074264
iteration : 2544
train acc:  0.75
train loss:  0.4633687138557434
train gradient:  0.1082126244195422
iteration : 2545
train acc:  0.6953125
train loss:  0.5381714105606079
train gradient:  0.13454424432172812
iteration : 2546
train acc:  0.7890625
train loss:  0.4229735732078552
train gradient:  0.1041166215538271
iteration : 2547
train acc:  0.734375
train loss:  0.5511409640312195
train gradient:  0.17981228747939457
iteration : 2548
train acc:  0.75
train loss:  0.48243844509124756
train gradient:  0.09695938338639133
iteration : 2549
train acc:  0.75
train loss:  0.49121421575546265
train gradient:  0.13308859504045364
iteration : 2550
train acc:  0.7265625
train loss:  0.4878661632537842
train gradient:  0.1526548115764465
iteration : 2551
train acc:  0.78125
train loss:  0.44241034984588623
train gradient:  0.10214657362477603
iteration : 2552
train acc:  0.7265625
train loss:  0.4912613034248352
train gradient:  0.13446596310376027
iteration : 2553
train acc:  0.7421875
train loss:  0.5367460250854492
train gradient:  0.18239814131281945
iteration : 2554
train acc:  0.765625
train loss:  0.4821534752845764
train gradient:  0.13598869206555983
iteration : 2555
train acc:  0.7734375
train loss:  0.4677411615848541
train gradient:  0.10454336900359949
iteration : 2556
train acc:  0.765625
train loss:  0.45536312460899353
train gradient:  0.1059511957785997
iteration : 2557
train acc:  0.7578125
train loss:  0.4275034964084625
train gradient:  0.10076738866193279
iteration : 2558
train acc:  0.8046875
train loss:  0.43991023302078247
train gradient:  0.10342460623008932
iteration : 2559
train acc:  0.7578125
train loss:  0.5373626351356506
train gradient:  0.16442846127102478
iteration : 2560
train acc:  0.7421875
train loss:  0.5140860080718994
train gradient:  0.1479916215796067
iteration : 2561
train acc:  0.6796875
train loss:  0.5615130662918091
train gradient:  0.1801333710192629
iteration : 2562
train acc:  0.8203125
train loss:  0.4182375371456146
train gradient:  0.09476957448094502
iteration : 2563
train acc:  0.7734375
train loss:  0.4634077847003937
train gradient:  0.1190765937319829
iteration : 2564
train acc:  0.75
train loss:  0.4869808554649353
train gradient:  0.13859599186607036
iteration : 2565
train acc:  0.7578125
train loss:  0.45929741859436035
train gradient:  0.11572239895815876
iteration : 2566
train acc:  0.78125
train loss:  0.4165480136871338
train gradient:  0.11134639655774994
iteration : 2567
train acc:  0.796875
train loss:  0.4440939426422119
train gradient:  0.09285550510767013
iteration : 2568
train acc:  0.765625
train loss:  0.48227834701538086
train gradient:  0.11136472213473714
iteration : 2569
train acc:  0.703125
train loss:  0.5111265182495117
train gradient:  0.13708806818217045
iteration : 2570
train acc:  0.7578125
train loss:  0.49650365114212036
train gradient:  0.1415496516274871
iteration : 2571
train acc:  0.6640625
train loss:  0.5918132066726685
train gradient:  0.169957025986759
iteration : 2572
train acc:  0.6875
train loss:  0.48873835802078247
train gradient:  0.1331303154920447
iteration : 2573
train acc:  0.8046875
train loss:  0.4616185426712036
train gradient:  0.14022502467792486
iteration : 2574
train acc:  0.7890625
train loss:  0.46619611978530884
train gradient:  0.11156620585243361
iteration : 2575
train acc:  0.75
train loss:  0.4440442621707916
train gradient:  0.10196022734907174
iteration : 2576
train acc:  0.7421875
train loss:  0.5135164260864258
train gradient:  0.1239508797399704
iteration : 2577
train acc:  0.65625
train loss:  0.5390386581420898
train gradient:  0.1256781400188028
iteration : 2578
train acc:  0.6484375
train loss:  0.5556472539901733
train gradient:  0.12406471803357512
iteration : 2579
train acc:  0.75
train loss:  0.49164044857025146
train gradient:  0.12881424322239604
iteration : 2580
train acc:  0.765625
train loss:  0.45655742287635803
train gradient:  0.12657589182372347
iteration : 2581
train acc:  0.7890625
train loss:  0.4483708143234253
train gradient:  0.1132765750608702
iteration : 2582
train acc:  0.7734375
train loss:  0.40695464611053467
train gradient:  0.09571070001328812
iteration : 2583
train acc:  0.7265625
train loss:  0.4760130047798157
train gradient:  0.12150316989433972
iteration : 2584
train acc:  0.7890625
train loss:  0.42949604988098145
train gradient:  0.08423792849714068
iteration : 2585
train acc:  0.7890625
train loss:  0.4563266336917877
train gradient:  0.1255238899654328
iteration : 2586
train acc:  0.765625
train loss:  0.45189350843429565
train gradient:  0.15387603941040334
iteration : 2587
train acc:  0.7109375
train loss:  0.5597888231277466
train gradient:  0.16278211205963697
iteration : 2588
train acc:  0.7890625
train loss:  0.41397011280059814
train gradient:  0.10303971525912178
iteration : 2589
train acc:  0.765625
train loss:  0.39944398403167725
train gradient:  0.07653861024173388
iteration : 2590
train acc:  0.7734375
train loss:  0.4753788411617279
train gradient:  0.12325019410308762
iteration : 2591
train acc:  0.7578125
train loss:  0.4345625936985016
train gradient:  0.136269863675758
iteration : 2592
train acc:  0.703125
train loss:  0.516391396522522
train gradient:  0.1443110196561208
iteration : 2593
train acc:  0.7421875
train loss:  0.48086196184158325
train gradient:  0.11620511827798667
iteration : 2594
train acc:  0.765625
train loss:  0.4581606984138489
train gradient:  0.11065937157744332
iteration : 2595
train acc:  0.71875
train loss:  0.48287978768348694
train gradient:  0.11765071667783508
iteration : 2596
train acc:  0.734375
train loss:  0.4953378140926361
train gradient:  0.10892843058483286
iteration : 2597
train acc:  0.765625
train loss:  0.4534223675727844
train gradient:  0.1165841751538101
iteration : 2598
train acc:  0.7578125
train loss:  0.43678411841392517
train gradient:  0.10656661232314281
iteration : 2599
train acc:  0.7578125
train loss:  0.4633088707923889
train gradient:  0.14147095095253337
iteration : 2600
train acc:  0.71875
train loss:  0.5020122528076172
train gradient:  0.12359263136312569
iteration : 2601
train acc:  0.71875
train loss:  0.5015387535095215
train gradient:  0.14735964563380458
iteration : 2602
train acc:  0.7734375
train loss:  0.4031887650489807
train gradient:  0.0819445708018595
iteration : 2603
train acc:  0.671875
train loss:  0.5258370637893677
train gradient:  0.1524190483549533
iteration : 2604
train acc:  0.7890625
train loss:  0.4198336601257324
train gradient:  0.10785106352194973
iteration : 2605
train acc:  0.7265625
train loss:  0.5070964097976685
train gradient:  0.12669340295530557
iteration : 2606
train acc:  0.703125
train loss:  0.5324721336364746
train gradient:  0.12946376008212904
iteration : 2607
train acc:  0.7265625
train loss:  0.5451636910438538
train gradient:  0.14798039519207595
iteration : 2608
train acc:  0.7734375
train loss:  0.4798659086227417
train gradient:  0.1318480864024652
iteration : 2609
train acc:  0.7421875
train loss:  0.4953150153160095
train gradient:  0.14986744150484632
iteration : 2610
train acc:  0.8203125
train loss:  0.47263139486312866
train gradient:  0.10971244689601946
iteration : 2611
train acc:  0.8515625
train loss:  0.3647475838661194
train gradient:  0.06500134406331784
iteration : 2612
train acc:  0.7734375
train loss:  0.43856120109558105
train gradient:  0.09773986216647146
iteration : 2613
train acc:  0.8125
train loss:  0.39750009775161743
train gradient:  0.084758687285978
iteration : 2614
train acc:  0.6953125
train loss:  0.508419394493103
train gradient:  0.13330202933117186
iteration : 2615
train acc:  0.7265625
train loss:  0.4956584572792053
train gradient:  0.12222805382191378
iteration : 2616
train acc:  0.734375
train loss:  0.47540390491485596
train gradient:  0.1291452385475999
iteration : 2617
train acc:  0.796875
train loss:  0.43804657459259033
train gradient:  0.11360054534774021
iteration : 2618
train acc:  0.765625
train loss:  0.4265313148498535
train gradient:  0.10860778939165976
iteration : 2619
train acc:  0.75
train loss:  0.49269750714302063
train gradient:  0.11980879844245643
iteration : 2620
train acc:  0.7265625
train loss:  0.45997482538223267
train gradient:  0.11106017724893452
iteration : 2621
train acc:  0.765625
train loss:  0.47946321964263916
train gradient:  0.12160588209755702
iteration : 2622
train acc:  0.7421875
train loss:  0.505497395992279
train gradient:  0.14157776315200676
iteration : 2623
train acc:  0.8125
train loss:  0.425140380859375
train gradient:  0.09965384163341935
iteration : 2624
train acc:  0.7421875
train loss:  0.5198747515678406
train gradient:  0.12687014702182692
iteration : 2625
train acc:  0.71875
train loss:  0.5532232522964478
train gradient:  0.11643982732904813
iteration : 2626
train acc:  0.7578125
train loss:  0.46601104736328125
train gradient:  0.13523669232156715
iteration : 2627
train acc:  0.6953125
train loss:  0.510777473449707
train gradient:  0.13134713022521105
iteration : 2628
train acc:  0.7421875
train loss:  0.5104684233665466
train gradient:  0.17576974793837288
iteration : 2629
train acc:  0.7578125
train loss:  0.48514625430107117
train gradient:  0.14566691040794566
iteration : 2630
train acc:  0.734375
train loss:  0.5324134826660156
train gradient:  0.17091550987143755
iteration : 2631
train acc:  0.6953125
train loss:  0.5681381225585938
train gradient:  0.16710523177766198
iteration : 2632
train acc:  0.796875
train loss:  0.43457895517349243
train gradient:  0.10609266408844735
iteration : 2633
train acc:  0.7890625
train loss:  0.49347126483917236
train gradient:  0.13765084809438996
iteration : 2634
train acc:  0.7421875
train loss:  0.47238948941230774
train gradient:  0.12526162751364728
iteration : 2635
train acc:  0.7578125
train loss:  0.48233500123023987
train gradient:  0.11692865354256202
iteration : 2636
train acc:  0.7421875
train loss:  0.49999427795410156
train gradient:  0.13860328529549396
iteration : 2637
train acc:  0.78125
train loss:  0.454132616519928
train gradient:  0.12544623148354622
iteration : 2638
train acc:  0.7421875
train loss:  0.5154550075531006
train gradient:  0.1559831035357706
iteration : 2639
train acc:  0.703125
train loss:  0.504544734954834
train gradient:  0.1285452915922822
iteration : 2640
train acc:  0.8203125
train loss:  0.4542977511882782
train gradient:  0.15346244961274147
iteration : 2641
train acc:  0.7890625
train loss:  0.43561065196990967
train gradient:  0.10660353830761025
iteration : 2642
train acc:  0.6875
train loss:  0.5664786696434021
train gradient:  0.1548657739637445
iteration : 2643
train acc:  0.765625
train loss:  0.4611864387989044
train gradient:  0.11988962421065859
iteration : 2644
train acc:  0.7421875
train loss:  0.49507981538772583
train gradient:  0.12672198668027496
iteration : 2645
train acc:  0.7734375
train loss:  0.45052599906921387
train gradient:  0.08555370012052933
iteration : 2646
train acc:  0.7265625
train loss:  0.49695688486099243
train gradient:  0.13342717573028162
iteration : 2647
train acc:  0.7734375
train loss:  0.432871013879776
train gradient:  0.11262075252385925
iteration : 2648
train acc:  0.71875
train loss:  0.47308051586151123
train gradient:  0.10056164098885802
iteration : 2649
train acc:  0.734375
train loss:  0.4777507483959198
train gradient:  0.11234541307958297
iteration : 2650
train acc:  0.7734375
train loss:  0.4608374834060669
train gradient:  0.1447715512712935
iteration : 2651
train acc:  0.796875
train loss:  0.4923412799835205
train gradient:  0.14844157694703025
iteration : 2652
train acc:  0.7734375
train loss:  0.5241303443908691
train gradient:  0.14618866553902904
iteration : 2653
train acc:  0.7265625
train loss:  0.5217578411102295
train gradient:  0.16688667544447722
iteration : 2654
train acc:  0.75
train loss:  0.4322592616081238
train gradient:  0.08383176635340205
iteration : 2655
train acc:  0.796875
train loss:  0.4631060063838959
train gradient:  0.09863132053065943
iteration : 2656
train acc:  0.7421875
train loss:  0.5186830163002014
train gradient:  0.12883259780117187
iteration : 2657
train acc:  0.75
train loss:  0.47953981161117554
train gradient:  0.1302945187822799
iteration : 2658
train acc:  0.7578125
train loss:  0.48523879051208496
train gradient:  0.11841546901845927
iteration : 2659
train acc:  0.71875
train loss:  0.5429723858833313
train gradient:  0.13789550511814536
iteration : 2660
train acc:  0.78125
train loss:  0.4236767888069153
train gradient:  0.07644146646214293
iteration : 2661
train acc:  0.7890625
train loss:  0.44116830825805664
train gradient:  0.1001431954795902
iteration : 2662
train acc:  0.765625
train loss:  0.4397510886192322
train gradient:  0.08009618535167475
iteration : 2663
train acc:  0.7734375
train loss:  0.4983036518096924
train gradient:  0.12041395029058888
iteration : 2664
train acc:  0.7734375
train loss:  0.45408737659454346
train gradient:  0.09359617433058638
iteration : 2665
train acc:  0.7265625
train loss:  0.488467276096344
train gradient:  0.14377287410709058
iteration : 2666
train acc:  0.7890625
train loss:  0.5154847502708435
train gradient:  0.11981993344505061
iteration : 2667
train acc:  0.765625
train loss:  0.4362344443798065
train gradient:  0.09484542061650893
iteration : 2668
train acc:  0.7734375
train loss:  0.4816310703754425
train gradient:  0.13108504025596696
iteration : 2669
train acc:  0.703125
train loss:  0.5418618321418762
train gradient:  0.15133081457770936
iteration : 2670
train acc:  0.8515625
train loss:  0.4072178304195404
train gradient:  0.07085135391471231
iteration : 2671
train acc:  0.7265625
train loss:  0.5347830653190613
train gradient:  0.13754313863486423
iteration : 2672
train acc:  0.7890625
train loss:  0.4990485608577728
train gradient:  0.16679368826763624
iteration : 2673
train acc:  0.7734375
train loss:  0.4133233428001404
train gradient:  0.08830280502946178
iteration : 2674
train acc:  0.7578125
train loss:  0.4824066758155823
train gradient:  0.1339610740311586
iteration : 2675
train acc:  0.734375
train loss:  0.49481499195098877
train gradient:  0.12408633777838339
iteration : 2676
train acc:  0.75
train loss:  0.4885791838169098
train gradient:  0.10335488248217983
iteration : 2677
train acc:  0.7421875
train loss:  0.499080628156662
train gradient:  0.11774257056793722
iteration : 2678
train acc:  0.7734375
train loss:  0.4483568072319031
train gradient:  0.11392121708234888
iteration : 2679
train acc:  0.796875
train loss:  0.41324326395988464
train gradient:  0.0856736244709387
iteration : 2680
train acc:  0.8203125
train loss:  0.38724398612976074
train gradient:  0.07688595741108345
iteration : 2681
train acc:  0.7109375
train loss:  0.5354543924331665
train gradient:  0.1608647206825593
iteration : 2682
train acc:  0.7890625
train loss:  0.42136216163635254
train gradient:  0.07445569244991941
iteration : 2683
train acc:  0.7578125
train loss:  0.4699496328830719
train gradient:  0.12056284320124148
iteration : 2684
train acc:  0.78125
train loss:  0.46546226739883423
train gradient:  0.10925514909106127
iteration : 2685
train acc:  0.7109375
train loss:  0.5255781412124634
train gradient:  0.1074662076084278
iteration : 2686
train acc:  0.7109375
train loss:  0.5190907716751099
train gradient:  0.13255910434259416
iteration : 2687
train acc:  0.828125
train loss:  0.4235054850578308
train gradient:  0.10131131402686741
iteration : 2688
train acc:  0.703125
train loss:  0.47124215960502625
train gradient:  0.11010100939558229
iteration : 2689
train acc:  0.734375
train loss:  0.5253730416297913
train gradient:  0.1607063098520562
iteration : 2690
train acc:  0.7890625
train loss:  0.4788258969783783
train gradient:  0.11206493005021699
iteration : 2691
train acc:  0.6796875
train loss:  0.5858743190765381
train gradient:  0.15528082793001724
iteration : 2692
train acc:  0.7890625
train loss:  0.42473429441452026
train gradient:  0.08028584063924804
iteration : 2693
train acc:  0.734375
train loss:  0.5054807066917419
train gradient:  0.1095984121062726
iteration : 2694
train acc:  0.796875
train loss:  0.46538254618644714
train gradient:  0.1159093381603649
iteration : 2695
train acc:  0.828125
train loss:  0.3777378797531128
train gradient:  0.09870668386973813
iteration : 2696
train acc:  0.75
train loss:  0.45539426803588867
train gradient:  0.10037757052438227
iteration : 2697
train acc:  0.703125
train loss:  0.513870358467102
train gradient:  0.12478152988477649
iteration : 2698
train acc:  0.7578125
train loss:  0.4860333800315857
train gradient:  0.1425440244855491
iteration : 2699
train acc:  0.7578125
train loss:  0.4340232014656067
train gradient:  0.1099643553876768
iteration : 2700
train acc:  0.71875
train loss:  0.5016066431999207
train gradient:  0.12305849415714798
iteration : 2701
train acc:  0.8203125
train loss:  0.4433678984642029
train gradient:  0.1094162149475178
iteration : 2702
train acc:  0.7109375
train loss:  0.4779919981956482
train gradient:  0.11718361187854354
iteration : 2703
train acc:  0.8046875
train loss:  0.44313234090805054
train gradient:  0.10845603595737457
iteration : 2704
train acc:  0.7421875
train loss:  0.5156055688858032
train gradient:  0.11919722832034207
iteration : 2705
train acc:  0.7734375
train loss:  0.5086753964424133
train gradient:  0.11128992625062288
iteration : 2706
train acc:  0.7578125
train loss:  0.45017939805984497
train gradient:  0.12956863581882144
iteration : 2707
train acc:  0.75
train loss:  0.46623215079307556
train gradient:  0.14044722093935447
iteration : 2708
train acc:  0.71875
train loss:  0.5195977687835693
train gradient:  0.1512518925513089
iteration : 2709
train acc:  0.75
train loss:  0.4973163902759552
train gradient:  0.1298290111527675
iteration : 2710
train acc:  0.7109375
train loss:  0.5105739831924438
train gradient:  0.119282142921575
iteration : 2711
train acc:  0.765625
train loss:  0.4730677008628845
train gradient:  0.08253788891204854
iteration : 2712
train acc:  0.765625
train loss:  0.4867706894874573
train gradient:  0.11589058977317475
iteration : 2713
train acc:  0.78125
train loss:  0.46341896057128906
train gradient:  0.09873342117764612
iteration : 2714
train acc:  0.78125
train loss:  0.4720214903354645
train gradient:  0.09857172897818106
iteration : 2715
train acc:  0.765625
train loss:  0.4189625382423401
train gradient:  0.09518817175559535
iteration : 2716
train acc:  0.8203125
train loss:  0.40100306272506714
train gradient:  0.10446412794620034
iteration : 2717
train acc:  0.7265625
train loss:  0.48751676082611084
train gradient:  0.1407393902923963
iteration : 2718
train acc:  0.75
train loss:  0.4776201546192169
train gradient:  0.13028845942686618
iteration : 2719
train acc:  0.765625
train loss:  0.4520641565322876
train gradient:  0.10285603107858586
iteration : 2720
train acc:  0.703125
train loss:  0.5477781891822815
train gradient:  0.14626086985501047
iteration : 2721
train acc:  0.734375
train loss:  0.48390689492225647
train gradient:  0.11057156800335563
iteration : 2722
train acc:  0.7578125
train loss:  0.4797930419445038
train gradient:  0.10204353991426844
iteration : 2723
train acc:  0.734375
train loss:  0.4891340136528015
train gradient:  0.12993627663959276
iteration : 2724
train acc:  0.71875
train loss:  0.4639628529548645
train gradient:  0.14073756795572134
iteration : 2725
train acc:  0.8125
train loss:  0.4072919487953186
train gradient:  0.0838778411141519
iteration : 2726
train acc:  0.7890625
train loss:  0.43272656202316284
train gradient:  0.09389652825088222
iteration : 2727
train acc:  0.75
train loss:  0.4910469949245453
train gradient:  0.1516581339713545
iteration : 2728
train acc:  0.7265625
train loss:  0.5182791352272034
train gradient:  0.1831174932569478
iteration : 2729
train acc:  0.7421875
train loss:  0.5261769890785217
train gradient:  0.15382880922139108
iteration : 2730
train acc:  0.703125
train loss:  0.5210511684417725
train gradient:  0.14628598530424647
iteration : 2731
train acc:  0.765625
train loss:  0.4708622694015503
train gradient:  0.12050724537350208
iteration : 2732
train acc:  0.7734375
train loss:  0.4546523690223694
train gradient:  0.0968055287318366
iteration : 2733
train acc:  0.765625
train loss:  0.4358431100845337
train gradient:  0.10361149647161043
iteration : 2734
train acc:  0.7578125
train loss:  0.5076377391815186
train gradient:  0.1419677719965729
iteration : 2735
train acc:  0.796875
train loss:  0.45965278148651123
train gradient:  0.12509213396045016
iteration : 2736
train acc:  0.6328125
train loss:  0.6105841398239136
train gradient:  0.1831541464907443
iteration : 2737
train acc:  0.7578125
train loss:  0.5075371265411377
train gradient:  0.1463158880313284
iteration : 2738
train acc:  0.75
train loss:  0.46990305185317993
train gradient:  0.11100374169230857
iteration : 2739
train acc:  0.7578125
train loss:  0.4655861258506775
train gradient:  0.10401745928196628
iteration : 2740
train acc:  0.734375
train loss:  0.42589882016181946
train gradient:  0.08530498594351552
iteration : 2741
train acc:  0.75
train loss:  0.497270405292511
train gradient:  0.15612599666306426
iteration : 2742
train acc:  0.7890625
train loss:  0.42839327454566956
train gradient:  0.11696133868523069
iteration : 2743
train acc:  0.796875
train loss:  0.44225966930389404
train gradient:  0.09562299382864331
iteration : 2744
train acc:  0.8125
train loss:  0.44289663434028625
train gradient:  0.10338751802204703
iteration : 2745
train acc:  0.7890625
train loss:  0.42531827092170715
train gradient:  0.09895749982853914
iteration : 2746
train acc:  0.6875
train loss:  0.5144433975219727
train gradient:  0.11405845127197858
iteration : 2747
train acc:  0.7578125
train loss:  0.48511314392089844
train gradient:  0.13204706261105967
iteration : 2748
train acc:  0.78125
train loss:  0.4389205873012543
train gradient:  0.09047720839837746
iteration : 2749
train acc:  0.75
train loss:  0.4766785502433777
train gradient:  0.1112627712816083
iteration : 2750
train acc:  0.7421875
train loss:  0.46544110774993896
train gradient:  0.11077668767398824
iteration : 2751
train acc:  0.734375
train loss:  0.5264740586280823
train gradient:  0.1426403804594814
iteration : 2752
train acc:  0.734375
train loss:  0.5126725435256958
train gradient:  0.14125812567818952
iteration : 2753
train acc:  0.734375
train loss:  0.48380404710769653
train gradient:  0.12190830238632451
iteration : 2754
train acc:  0.7890625
train loss:  0.45213061571121216
train gradient:  0.10977366693485548
iteration : 2755
train acc:  0.6953125
train loss:  0.542472779750824
train gradient:  0.1576030759688279
iteration : 2756
train acc:  0.796875
train loss:  0.48073193430900574
train gradient:  0.134456425912001
iteration : 2757
train acc:  0.75
train loss:  0.4957565367221832
train gradient:  0.13094819660780674
iteration : 2758
train acc:  0.7265625
train loss:  0.4862154424190521
train gradient:  0.10748640232768858
iteration : 2759
train acc:  0.7734375
train loss:  0.5196187496185303
train gradient:  0.16864706869756485
iteration : 2760
train acc:  0.7265625
train loss:  0.47515684366226196
train gradient:  0.13340832776805522
iteration : 2761
train acc:  0.75
train loss:  0.46524277329444885
train gradient:  0.11324671213174163
iteration : 2762
train acc:  0.71875
train loss:  0.5143906474113464
train gradient:  0.12549819634318332
iteration : 2763
train acc:  0.65625
train loss:  0.5293675065040588
train gradient:  0.13843024773113322
iteration : 2764
train acc:  0.8046875
train loss:  0.4304119348526001
train gradient:  0.1035925128662082
iteration : 2765
train acc:  0.7109375
train loss:  0.5427156686782837
train gradient:  0.15863278639985015
iteration : 2766
train acc:  0.71875
train loss:  0.5538674592971802
train gradient:  0.15064441929411732
iteration : 2767
train acc:  0.7421875
train loss:  0.45987582206726074
train gradient:  0.11104994232915377
iteration : 2768
train acc:  0.7734375
train loss:  0.5030694007873535
train gradient:  0.10690823662496059
iteration : 2769
train acc:  0.7578125
train loss:  0.5085893869400024
train gradient:  0.1078163672427772
iteration : 2770
train acc:  0.6484375
train loss:  0.6128630638122559
train gradient:  0.2051549939972153
iteration : 2771
train acc:  0.765625
train loss:  0.44466879963874817
train gradient:  0.07849458014834705
iteration : 2772
train acc:  0.7421875
train loss:  0.4990221858024597
train gradient:  0.12045443098255913
iteration : 2773
train acc:  0.71875
train loss:  0.4879441261291504
train gradient:  0.1311830120660581
iteration : 2774
train acc:  0.8046875
train loss:  0.46182990074157715
train gradient:  0.1106628877818152
iteration : 2775
train acc:  0.6796875
train loss:  0.4918646812438965
train gradient:  0.13028126576095606
iteration : 2776
train acc:  0.84375
train loss:  0.4174475371837616
train gradient:  0.14620351686441513
iteration : 2777
train acc:  0.7578125
train loss:  0.5362811088562012
train gradient:  0.13548857465622421
iteration : 2778
train acc:  0.7421875
train loss:  0.4666832983493805
train gradient:  0.09728053970231172
iteration : 2779
train acc:  0.78125
train loss:  0.46009546518325806
train gradient:  0.10890620338649003
iteration : 2780
train acc:  0.734375
train loss:  0.48991847038269043
train gradient:  0.10394255546608004
iteration : 2781
train acc:  0.7890625
train loss:  0.4320080876350403
train gradient:  0.10529205787920401
iteration : 2782
train acc:  0.75
train loss:  0.4910908341407776
train gradient:  0.12693596777488225
iteration : 2783
train acc:  0.7578125
train loss:  0.4557642638683319
train gradient:  0.10776220299039553
iteration : 2784
train acc:  0.7578125
train loss:  0.47290685772895813
train gradient:  0.1264663986976466
iteration : 2785
train acc:  0.78125
train loss:  0.48080649971961975
train gradient:  0.11368830719669351
iteration : 2786
train acc:  0.78125
train loss:  0.44061440229415894
train gradient:  0.1121212573767434
iteration : 2787
train acc:  0.7421875
train loss:  0.5497955083847046
train gradient:  0.193688997613355
iteration : 2788
train acc:  0.6953125
train loss:  0.5927672982215881
train gradient:  0.18113645116395205
iteration : 2789
train acc:  0.7421875
train loss:  0.5339716672897339
train gradient:  0.1577611913076491
iteration : 2790
train acc:  0.6875
train loss:  0.514729917049408
train gradient:  0.13730070420711143
iteration : 2791
train acc:  0.7265625
train loss:  0.4809577465057373
train gradient:  0.12177018640224725
iteration : 2792
train acc:  0.7578125
train loss:  0.5060328245162964
train gradient:  0.11039521403441988
iteration : 2793
train acc:  0.765625
train loss:  0.45604273676872253
train gradient:  0.09922846920383604
iteration : 2794
train acc:  0.7578125
train loss:  0.4922601580619812
train gradient:  0.10527616293818037
iteration : 2795
train acc:  0.75
train loss:  0.49037665128707886
train gradient:  0.10777455769107486
iteration : 2796
train acc:  0.7890625
train loss:  0.47464507818222046
train gradient:  0.10239723749443012
iteration : 2797
train acc:  0.7890625
train loss:  0.4640284776687622
train gradient:  0.10407558340141392
iteration : 2798
train acc:  0.7734375
train loss:  0.47566908597946167
train gradient:  0.13940478528699896
iteration : 2799
train acc:  0.7890625
train loss:  0.46949076652526855
train gradient:  0.1226801008054599
iteration : 2800
train acc:  0.734375
train loss:  0.520729124546051
train gradient:  0.14698844031241648
iteration : 2801
train acc:  0.8125
train loss:  0.4067164957523346
train gradient:  0.0971982921847794
iteration : 2802
train acc:  0.7578125
train loss:  0.4795718193054199
train gradient:  0.12176224353409032
iteration : 2803
train acc:  0.75
train loss:  0.48991984128952026
train gradient:  0.09851962146582519
iteration : 2804
train acc:  0.7890625
train loss:  0.4191127121448517
train gradient:  0.08174890034166815
iteration : 2805
train acc:  0.7265625
train loss:  0.48856818675994873
train gradient:  0.10658128674038307
iteration : 2806
train acc:  0.7265625
train loss:  0.46924230456352234
train gradient:  0.1059089685495512
iteration : 2807
train acc:  0.7890625
train loss:  0.4496559500694275
train gradient:  0.10716162159767052
iteration : 2808
train acc:  0.7265625
train loss:  0.4940541088581085
train gradient:  0.11337871166077297
iteration : 2809
train acc:  0.6875
train loss:  0.5388274788856506
train gradient:  0.1389975282528085
iteration : 2810
train acc:  0.7421875
train loss:  0.5198261737823486
train gradient:  0.1309949604706839
iteration : 2811
train acc:  0.8046875
train loss:  0.3972397446632385
train gradient:  0.10618899183796507
iteration : 2812
train acc:  0.7109375
train loss:  0.4346877336502075
train gradient:  0.09698361724718184
iteration : 2813
train acc:  0.75
train loss:  0.476127028465271
train gradient:  0.13614850087800384
iteration : 2814
train acc:  0.765625
train loss:  0.46506887674331665
train gradient:  0.1480659191235862
iteration : 2815
train acc:  0.7578125
train loss:  0.463664174079895
train gradient:  0.12880004000909612
iteration : 2816
train acc:  0.7578125
train loss:  0.4641290307044983
train gradient:  0.10775057447816974
iteration : 2817
train acc:  0.6953125
train loss:  0.5492919683456421
train gradient:  0.134781598588978
