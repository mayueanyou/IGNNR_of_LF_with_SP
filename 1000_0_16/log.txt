program start:
num_rounds= 0
node_emb_dim= 16

----------------------------------------new_epoch--------------------------------------

epoch:  0
iteration : 0
train acc:  0.5546875
train loss:  0.6880925893783569
train gradient:  0.40003492105086597
iteration : 1
train acc:  0.4765625
train loss:  0.7096403241157532
train gradient:  0.4621351170868292
iteration : 2
train acc:  0.5546875
train loss:  0.6996251344680786
train gradient:  0.4709887955946829
iteration : 3
train acc:  0.5625
train loss:  0.677983283996582
train gradient:  0.25128404024158885
iteration : 4
train acc:  0.5703125
train loss:  0.7065733671188354
train gradient:  0.27352300595820905
iteration : 5
train acc:  0.578125
train loss:  0.7047000527381897
train gradient:  0.23746302824278664
iteration : 6
train acc:  0.6015625
train loss:  0.6567739844322205
train gradient:  0.24907237274961186
iteration : 7
train acc:  0.578125
train loss:  0.6874668598175049
train gradient:  0.2587563701222671
iteration : 8
train acc:  0.5703125
train loss:  0.6972235441207886
train gradient:  0.3040993895766928
iteration : 9
train acc:  0.5546875
train loss:  0.6881953477859497
train gradient:  0.2568324030181807
iteration : 10
train acc:  0.59375
train loss:  0.6934371590614319
train gradient:  0.3289239544282351
iteration : 11
train acc:  0.5546875
train loss:  0.7164970636367798
train gradient:  0.3075932892588559
iteration : 12
train acc:  0.5859375
train loss:  0.6900910139083862
train gradient:  0.2510598300071094
iteration : 13
train acc:  0.6171875
train loss:  0.6874744892120361
train gradient:  0.22787260060529202
iteration : 14
train acc:  0.53125
train loss:  0.7052503824234009
train gradient:  0.2537555450327709
iteration : 15
train acc:  0.5859375
train loss:  0.6787034273147583
train gradient:  0.2974781750157321
iteration : 16
train acc:  0.65625
train loss:  0.6370768547058105
train gradient:  0.17015018557358774
iteration : 17
train acc:  0.5703125
train loss:  0.7124443650245667
train gradient:  0.2448806916931934
iteration : 18
train acc:  0.625
train loss:  0.6283441781997681
train gradient:  0.19370197770068184
iteration : 19
train acc:  0.6640625
train loss:  0.6460342407226562
train gradient:  0.24905846784641472
iteration : 20
train acc:  0.6640625
train loss:  0.6303379535675049
train gradient:  0.20550761012761204
iteration : 21
train acc:  0.6640625
train loss:  0.6301206350326538
train gradient:  0.17804622892583882
iteration : 22
train acc:  0.6953125
train loss:  0.620602011680603
train gradient:  0.16560413801926466
iteration : 23
train acc:  0.609375
train loss:  0.6680024862289429
train gradient:  0.2857035758220946
iteration : 24
train acc:  0.5703125
train loss:  0.6939882040023804
train gradient:  0.43061593762737055
iteration : 25
train acc:  0.6015625
train loss:  0.6253976821899414
train gradient:  0.162631065661486
iteration : 26
train acc:  0.671875
train loss:  0.6471211910247803
train gradient:  0.14571400772534962
iteration : 27
train acc:  0.6484375
train loss:  0.6195654273033142
train gradient:  0.16465339353119077
iteration : 28
train acc:  0.6171875
train loss:  0.6581557989120483
train gradient:  0.21575668781091642
iteration : 29
train acc:  0.578125
train loss:  0.6515635251998901
train gradient:  0.1892786791444966
iteration : 30
train acc:  0.6171875
train loss:  0.6518756747245789
train gradient:  0.17065628215149165
iteration : 31
train acc:  0.578125
train loss:  0.6813715100288391
train gradient:  0.26401178948055276
iteration : 32
train acc:  0.578125
train loss:  0.6542308330535889
train gradient:  0.2528287851911929
iteration : 33
train acc:  0.5625
train loss:  0.6765780448913574
train gradient:  0.24554148453451302
iteration : 34
train acc:  0.640625
train loss:  0.6410698890686035
train gradient:  0.2025948340398876
iteration : 35
train acc:  0.6640625
train loss:  0.631766140460968
train gradient:  0.18540968857380674
iteration : 36
train acc:  0.71875
train loss:  0.6109226942062378
train gradient:  0.19401047450908787
iteration : 37
train acc:  0.6171875
train loss:  0.654611349105835
train gradient:  0.17192134091730224
iteration : 38
train acc:  0.6171875
train loss:  0.6652606725692749
train gradient:  0.1965204190858023
iteration : 39
train acc:  0.5859375
train loss:  0.6395038366317749
train gradient:  0.1693822009025944
iteration : 40
train acc:  0.5859375
train loss:  0.672501802444458
train gradient:  0.2099799689631
iteration : 41
train acc:  0.625
train loss:  0.6693313121795654
train gradient:  0.19413079918990384
iteration : 42
train acc:  0.6484375
train loss:  0.6434735059738159
train gradient:  0.17320244157765574
iteration : 43
train acc:  0.6796875
train loss:  0.6249335408210754
train gradient:  0.18266938231960755
iteration : 44
train acc:  0.5546875
train loss:  0.6708939075469971
train gradient:  0.19095234956524068
iteration : 45
train acc:  0.609375
train loss:  0.6645071506500244
train gradient:  0.18298352679363936
iteration : 46
train acc:  0.5859375
train loss:  0.6942614912986755
train gradient:  0.23655636868873914
iteration : 47
train acc:  0.5859375
train loss:  0.6781288385391235
train gradient:  0.18644645227727757
iteration : 48
train acc:  0.7421875
train loss:  0.6061362624168396
train gradient:  0.16977521300410323
iteration : 49
train acc:  0.671875
train loss:  0.6204047203063965
train gradient:  0.13962688564055975
iteration : 50
train acc:  0.6171875
train loss:  0.6543020009994507
train gradient:  0.1664623657498111
iteration : 51
train acc:  0.59375
train loss:  0.664050817489624
train gradient:  0.20938633106768778
iteration : 52
train acc:  0.609375
train loss:  0.6475504636764526
train gradient:  0.3534676500487268
iteration : 53
train acc:  0.671875
train loss:  0.6221541166305542
train gradient:  0.1464810870603918
iteration : 54
train acc:  0.5859375
train loss:  0.6637468934059143
train gradient:  0.1638859585152671
iteration : 55
train acc:  0.5703125
train loss:  0.6469295620918274
train gradient:  0.1642687971160557
iteration : 56
train acc:  0.6171875
train loss:  0.6436153054237366
train gradient:  0.1316075506450824
iteration : 57
train acc:  0.609375
train loss:  0.6583737134933472
train gradient:  0.17447845919925184
iteration : 58
train acc:  0.59375
train loss:  0.6345124244689941
train gradient:  0.21901306014307728
iteration : 59
train acc:  0.609375
train loss:  0.6363077163696289
train gradient:  0.21158394165339486
iteration : 60
train acc:  0.6328125
train loss:  0.6598204970359802
train gradient:  0.188874708778859
iteration : 61
train acc:  0.6171875
train loss:  0.6680010557174683
train gradient:  0.18137958725587042
iteration : 62
train acc:  0.65625
train loss:  0.6320828199386597
train gradient:  0.17678679738169362
iteration : 63
train acc:  0.609375
train loss:  0.6578806638717651
train gradient:  0.24633219441757104
iteration : 64
train acc:  0.71875
train loss:  0.604759156703949
train gradient:  0.26766999764105426
iteration : 65
train acc:  0.671875
train loss:  0.6248612403869629
train gradient:  0.14617257412227164
iteration : 66
train acc:  0.5625
train loss:  0.6624029874801636
train gradient:  0.17481004016414067
iteration : 67
train acc:  0.5859375
train loss:  0.6649172902107239
train gradient:  0.18059076340569052
iteration : 68
train acc:  0.625
train loss:  0.6491364240646362
train gradient:  0.3848376185314306
iteration : 69
train acc:  0.640625
train loss:  0.6492787599563599
train gradient:  0.2042848532495024
iteration : 70
train acc:  0.6015625
train loss:  0.651648759841919
train gradient:  0.17955637872889785
iteration : 71
train acc:  0.6328125
train loss:  0.6216539740562439
train gradient:  0.19715241015629614
iteration : 72
train acc:  0.6796875
train loss:  0.6126923561096191
train gradient:  0.26027236648178287
iteration : 73
train acc:  0.5703125
train loss:  0.6633185148239136
train gradient:  0.2029932461425909
iteration : 74
train acc:  0.625
train loss:  0.6353665590286255
train gradient:  0.22957527323946136
iteration : 75
train acc:  0.5625
train loss:  0.6689483523368835
train gradient:  0.20153495578170305
iteration : 76
train acc:  0.59375
train loss:  0.6429712772369385
train gradient:  0.1632216129556314
iteration : 77
train acc:  0.6015625
train loss:  0.6684235334396362
train gradient:  0.16421725440365453
iteration : 78
train acc:  0.6171875
train loss:  0.6382941007614136
train gradient:  0.15692158871825934
iteration : 79
train acc:  0.671875
train loss:  0.6203345060348511
train gradient:  0.26203327805147675
iteration : 80
train acc:  0.578125
train loss:  0.6820257902145386
train gradient:  0.2517289917964635
iteration : 81
train acc:  0.6015625
train loss:  0.6930994987487793
train gradient:  0.2707361598977705
iteration : 82
train acc:  0.6171875
train loss:  0.6371463537216187
train gradient:  0.22858299198207432
iteration : 83
train acc:  0.6171875
train loss:  0.6370806694030762
train gradient:  0.19855823343829235
iteration : 84
train acc:  0.6171875
train loss:  0.6136786937713623
train gradient:  0.14477077650678544
iteration : 85
train acc:  0.640625
train loss:  0.6213335394859314
train gradient:  0.15793854209776192
iteration : 86
train acc:  0.6015625
train loss:  0.6830636262893677
train gradient:  0.23816182955525322
iteration : 87
train acc:  0.609375
train loss:  0.6724538803100586
train gradient:  0.2655219551338217
iteration : 88
train acc:  0.65625
train loss:  0.6285924911499023
train gradient:  0.18622781694569887
iteration : 89
train acc:  0.6484375
train loss:  0.630138635635376
train gradient:  0.1452053971139441
iteration : 90
train acc:  0.671875
train loss:  0.6203417778015137
train gradient:  0.1881808503108694
iteration : 91
train acc:  0.6171875
train loss:  0.6511995792388916
train gradient:  0.1649405379150021
iteration : 92
train acc:  0.6875
train loss:  0.6209173202514648
train gradient:  0.20193938071413214
iteration : 93
train acc:  0.6875
train loss:  0.5956809520721436
train gradient:  0.17407754995464525
iteration : 94
train acc:  0.6015625
train loss:  0.6598720550537109
train gradient:  0.17533592767020495
iteration : 95
train acc:  0.578125
train loss:  0.6833386421203613
train gradient:  0.2524822901672282
iteration : 96
train acc:  0.65625
train loss:  0.5934898257255554
train gradient:  0.18135389379300507
iteration : 97
train acc:  0.59375
train loss:  0.6530646085739136
train gradient:  0.1592788722897904
iteration : 98
train acc:  0.5703125
train loss:  0.6629646420478821
train gradient:  0.19548689065282615
iteration : 99
train acc:  0.625
train loss:  0.6424437165260315
train gradient:  0.14301531368271653
iteration : 100
train acc:  0.5703125
train loss:  0.668279767036438
train gradient:  0.20772619038680679
iteration : 101
train acc:  0.6875
train loss:  0.5955891609191895
train gradient:  0.12591301675925562
iteration : 102
train acc:  0.6171875
train loss:  0.6653964519500732
train gradient:  0.2007218370874582
iteration : 103
train acc:  0.6328125
train loss:  0.6213102340698242
train gradient:  0.1803122336138283
iteration : 104
train acc:  0.640625
train loss:  0.634125292301178
train gradient:  0.19107563080333007
iteration : 105
train acc:  0.546875
train loss:  0.6661449670791626
train gradient:  0.1640322916873946
iteration : 106
train acc:  0.6171875
train loss:  0.6420207619667053
train gradient:  0.18304041876028787
iteration : 107
train acc:  0.578125
train loss:  0.652932345867157
train gradient:  0.15876186536422793
iteration : 108
train acc:  0.640625
train loss:  0.6343868374824524
train gradient:  0.16421548732603658
iteration : 109
train acc:  0.671875
train loss:  0.6262671947479248
train gradient:  0.13893222296676694
iteration : 110
train acc:  0.625
train loss:  0.6246780157089233
train gradient:  0.1379254074984983
iteration : 111
train acc:  0.625
train loss:  0.6077492237091064
train gradient:  0.15948869162376655
iteration : 112
train acc:  0.6875
train loss:  0.5833131670951843
train gradient:  0.1603811978829508
iteration : 113
train acc:  0.6015625
train loss:  0.63740074634552
train gradient:  0.21347865580591294
iteration : 114
train acc:  0.59375
train loss:  0.6644628047943115
train gradient:  0.15430506855585066
iteration : 115
train acc:  0.671875
train loss:  0.6202442049980164
train gradient:  0.14921207808935158
iteration : 116
train acc:  0.6484375
train loss:  0.6470762491226196
train gradient:  0.210373227861485
iteration : 117
train acc:  0.6171875
train loss:  0.62872713804245
train gradient:  0.16709577496968364
iteration : 118
train acc:  0.703125
train loss:  0.5958981513977051
train gradient:  0.13913082289781484
iteration : 119
train acc:  0.5859375
train loss:  0.648385763168335
train gradient:  0.22057781157554135
iteration : 120
train acc:  0.6484375
train loss:  0.6315131783485413
train gradient:  0.14230109041105057
iteration : 121
train acc:  0.6875
train loss:  0.6090396642684937
train gradient:  0.1916574074812783
iteration : 122
train acc:  0.578125
train loss:  0.6380934715270996
train gradient:  0.18158361941207507
iteration : 123
train acc:  0.578125
train loss:  0.6492043137550354
train gradient:  0.1523326426805765
iteration : 124
train acc:  0.703125
train loss:  0.5763288736343384
train gradient:  0.1315199114260106
iteration : 125
train acc:  0.5703125
train loss:  0.6641334891319275
train gradient:  0.2142053349830439
iteration : 126
train acc:  0.6875
train loss:  0.6090760231018066
train gradient:  0.15592860947687937
iteration : 127
train acc:  0.7265625
train loss:  0.5954238772392273
train gradient:  0.1678862396900394
iteration : 128
train acc:  0.5703125
train loss:  0.6933801770210266
train gradient:  0.30063835070351036
iteration : 129
train acc:  0.6484375
train loss:  0.5964468717575073
train gradient:  0.1359863527409531
iteration : 130
train acc:  0.6015625
train loss:  0.628966748714447
train gradient:  0.14990147915964597
iteration : 131
train acc:  0.640625
train loss:  0.6148158311843872
train gradient:  0.1670666640941829
iteration : 132
train acc:  0.609375
train loss:  0.6543369293212891
train gradient:  0.20127196610086218
iteration : 133
train acc:  0.609375
train loss:  0.6324880719184875
train gradient:  0.1479076717409763
iteration : 134
train acc:  0.6640625
train loss:  0.5787882804870605
train gradient:  0.1701295391781373
iteration : 135
train acc:  0.59375
train loss:  0.6651163101196289
train gradient:  0.2837951537058573
iteration : 136
train acc:  0.5859375
train loss:  0.6706953048706055
train gradient:  0.16150897732876407
iteration : 137
train acc:  0.578125
train loss:  0.6913583278656006
train gradient:  0.2559214363040374
iteration : 138
train acc:  0.578125
train loss:  0.660211980342865
train gradient:  0.16640824392400363
iteration : 139
train acc:  0.6875
train loss:  0.616515576839447
train gradient:  0.1595430660498079
iteration : 140
train acc:  0.53125
train loss:  0.6807340383529663
train gradient:  0.27006254536155855
iteration : 141
train acc:  0.640625
train loss:  0.6212961673736572
train gradient:  0.35572125612867267
iteration : 142
train acc:  0.59375
train loss:  0.6478790044784546
train gradient:  0.26707083476375604
iteration : 143
train acc:  0.6171875
train loss:  0.6405409574508667
train gradient:  0.1667309031100626
iteration : 144
train acc:  0.6484375
train loss:  0.621099591255188
train gradient:  0.11891005518466789
iteration : 145
train acc:  0.6484375
train loss:  0.5910454988479614
train gradient:  0.18717561160156404
iteration : 146
train acc:  0.6796875
train loss:  0.6217451691627502
train gradient:  0.13072898038150976
iteration : 147
train acc:  0.6171875
train loss:  0.65464848279953
train gradient:  0.1513897774175989
iteration : 148
train acc:  0.6640625
train loss:  0.581559956073761
train gradient:  0.17929177699737378
iteration : 149
train acc:  0.6484375
train loss:  0.6546188592910767
train gradient:  0.22924120579122303
iteration : 150
train acc:  0.59375
train loss:  0.6147739291191101
train gradient:  0.18497656213437164
iteration : 151
train acc:  0.59375
train loss:  0.6537240147590637
train gradient:  0.14889801564353253
iteration : 152
train acc:  0.609375
train loss:  0.6576715707778931
train gradient:  0.14956467161660117
iteration : 153
train acc:  0.6484375
train loss:  0.6385996341705322
train gradient:  0.23271637484519897
iteration : 154
train acc:  0.671875
train loss:  0.6042104959487915
train gradient:  0.17078155390724375
iteration : 155
train acc:  0.640625
train loss:  0.6260806322097778
train gradient:  0.2549444139227046
iteration : 156
train acc:  0.609375
train loss:  0.6406197547912598
train gradient:  0.17165349014886147
iteration : 157
train acc:  0.6328125
train loss:  0.6257386207580566
train gradient:  0.17196634012605794
iteration : 158
train acc:  0.625
train loss:  0.6475265622138977
train gradient:  0.17274976821753957
iteration : 159
train acc:  0.6328125
train loss:  0.6075779795646667
train gradient:  0.15657559930431153
iteration : 160
train acc:  0.6171875
train loss:  0.6497734785079956
train gradient:  0.16754085511146005
iteration : 161
train acc:  0.609375
train loss:  0.6184029579162598
train gradient:  0.12460673665980382
iteration : 162
train acc:  0.6640625
train loss:  0.6254616379737854
train gradient:  0.17978142584059997
iteration : 163
train acc:  0.609375
train loss:  0.6404824256896973
train gradient:  0.12508399877439255
iteration : 164
train acc:  0.5625
train loss:  0.6598421931266785
train gradient:  0.15080122989069186
iteration : 165
train acc:  0.59375
train loss:  0.6372287273406982
train gradient:  0.1951011108581781
iteration : 166
train acc:  0.59375
train loss:  0.630103349685669
train gradient:  0.2822404611759693
iteration : 167
train acc:  0.640625
train loss:  0.6049882173538208
train gradient:  0.1595729444115872
iteration : 168
train acc:  0.5703125
train loss:  0.6933901309967041
train gradient:  0.19062188426266574
iteration : 169
train acc:  0.6484375
train loss:  0.6336146593093872
train gradient:  0.17703705955368942
iteration : 170
train acc:  0.671875
train loss:  0.5967929363250732
train gradient:  0.21415261442525074
iteration : 171
train acc:  0.6015625
train loss:  0.6905865669250488
train gradient:  0.23464086188649086
iteration : 172
train acc:  0.6640625
train loss:  0.6374912261962891
train gradient:  0.34177938375881883
iteration : 173
train acc:  0.6484375
train loss:  0.6278724670410156
train gradient:  0.15520896802873288
iteration : 174
train acc:  0.65625
train loss:  0.5996857285499573
train gradient:  0.15095781875422454
iteration : 175
train acc:  0.5546875
train loss:  0.6592769622802734
train gradient:  0.1600534981065772
iteration : 176
train acc:  0.65625
train loss:  0.6226346492767334
train gradient:  0.16231131324196407
iteration : 177
train acc:  0.6796875
train loss:  0.6167024374008179
train gradient:  0.18715779209832156
iteration : 178
train acc:  0.609375
train loss:  0.6398134231567383
train gradient:  0.1673819749013129
iteration : 179
train acc:  0.625
train loss:  0.6580039858818054
train gradient:  0.1855429255772219
iteration : 180
train acc:  0.609375
train loss:  0.6345406770706177
train gradient:  0.17222873573343928
iteration : 181
train acc:  0.6015625
train loss:  0.6423571705818176
train gradient:  0.15306882017168788
iteration : 182
train acc:  0.6875
train loss:  0.6053234338760376
train gradient:  0.1444771589572994
iteration : 183
train acc:  0.6640625
train loss:  0.6439803242683411
train gradient:  0.16452208184226386
iteration : 184
train acc:  0.6875
train loss:  0.6063956022262573
train gradient:  0.19467558339162364
iteration : 185
train acc:  0.640625
train loss:  0.6757829189300537
train gradient:  0.14283391452422087
iteration : 186
train acc:  0.671875
train loss:  0.6118834018707275
train gradient:  0.14652872526367947
iteration : 187
train acc:  0.578125
train loss:  0.6721126437187195
train gradient:  0.16630535769159605
iteration : 188
train acc:  0.75
train loss:  0.5643109083175659
train gradient:  0.1594072702405092
iteration : 189
train acc:  0.65625
train loss:  0.626444935798645
train gradient:  0.13384049450647317
iteration : 190
train acc:  0.6171875
train loss:  0.6525408029556274
train gradient:  0.173940407099899
iteration : 191
train acc:  0.703125
train loss:  0.5931585431098938
train gradient:  0.15778932334761916
iteration : 192
train acc:  0.609375
train loss:  0.6584264039993286
train gradient:  0.16474373635334544
iteration : 193
train acc:  0.6640625
train loss:  0.6190148591995239
train gradient:  0.14162570984608497
iteration : 194
train acc:  0.6796875
train loss:  0.595199704170227
train gradient:  0.1701816366440686
iteration : 195
train acc:  0.6953125
train loss:  0.5957270860671997
train gradient:  0.17932180699198133
iteration : 196
train acc:  0.703125
train loss:  0.6202481985092163
train gradient:  0.15281000086437574
iteration : 197
train acc:  0.6796875
train loss:  0.5905134677886963
train gradient:  0.15079618056624078
iteration : 198
train acc:  0.6640625
train loss:  0.6388212442398071
train gradient:  0.1746639515890017
iteration : 199
train acc:  0.6875
train loss:  0.585421085357666
train gradient:  0.13685907203276412
iteration : 200
train acc:  0.609375
train loss:  0.6476364135742188
train gradient:  0.15365869051028747
iteration : 201
train acc:  0.6484375
train loss:  0.6128883361816406
train gradient:  0.13462409542327666
iteration : 202
train acc:  0.7265625
train loss:  0.5646384954452515
train gradient:  0.14316855403037432
iteration : 203
train acc:  0.75
train loss:  0.5640048980712891
train gradient:  0.14897383499909844
iteration : 204
train acc:  0.6640625
train loss:  0.6233123540878296
train gradient:  0.1651619449270537
iteration : 205
train acc:  0.5859375
train loss:  0.6398123502731323
train gradient:  0.1699851505776927
iteration : 206
train acc:  0.640625
train loss:  0.6269888877868652
train gradient:  0.19308054711962336
iteration : 207
train acc:  0.6796875
train loss:  0.6091291308403015
train gradient:  0.15582228129374917
iteration : 208
train acc:  0.6875
train loss:  0.6062493324279785
train gradient:  0.12797738461800098
iteration : 209
train acc:  0.6015625
train loss:  0.6552251577377319
train gradient:  0.15673532874694093
iteration : 210
train acc:  0.59375
train loss:  0.647202730178833
train gradient:  0.23112714403113743
iteration : 211
train acc:  0.671875
train loss:  0.60163813829422
train gradient:  0.1809180785835144
iteration : 212
train acc:  0.59375
train loss:  0.6315600872039795
train gradient:  0.17134010589522697
iteration : 213
train acc:  0.6640625
train loss:  0.6407457590103149
train gradient:  0.14568565079912252
iteration : 214
train acc:  0.6015625
train loss:  0.6506617069244385
train gradient:  0.17747240078407556
iteration : 215
train acc:  0.6796875
train loss:  0.5811831951141357
train gradient:  0.1804120113809582
iteration : 216
train acc:  0.6796875
train loss:  0.6042140126228333
train gradient:  0.13797882040595388
iteration : 217
train acc:  0.6171875
train loss:  0.6550372242927551
train gradient:  0.21862793640137673
iteration : 218
train acc:  0.609375
train loss:  0.6519447565078735
train gradient:  0.15365773132388627
iteration : 219
train acc:  0.6640625
train loss:  0.5889624357223511
train gradient:  0.12357137403406686
iteration : 220
train acc:  0.6328125
train loss:  0.6332517862319946
train gradient:  0.20215025798992375
iteration : 221
train acc:  0.65625
train loss:  0.6059004068374634
train gradient:  0.2450533827750611
iteration : 222
train acc:  0.578125
train loss:  0.6547518968582153
train gradient:  0.1646828323146649
iteration : 223
train acc:  0.609375
train loss:  0.6181972026824951
train gradient:  0.15017656596454315
iteration : 224
train acc:  0.7578125
train loss:  0.5447773933410645
train gradient:  0.1339349162253053
iteration : 225
train acc:  0.59375
train loss:  0.6508815884590149
train gradient:  0.16637381252178932
iteration : 226
train acc:  0.6484375
train loss:  0.6038922071456909
train gradient:  0.14550661026785983
iteration : 227
train acc:  0.65625
train loss:  0.6177356839179993
train gradient:  0.2525587249610317
iteration : 228
train acc:  0.6484375
train loss:  0.5902364253997803
train gradient:  0.28436509362918383
iteration : 229
train acc:  0.6328125
train loss:  0.6088998317718506
train gradient:  0.21387169316449864
iteration : 230
train acc:  0.703125
train loss:  0.5727227926254272
train gradient:  0.11511489220283375
iteration : 231
train acc:  0.6484375
train loss:  0.6207453012466431
train gradient:  0.29205651346448985
iteration : 232
train acc:  0.6875
train loss:  0.5943108797073364
train gradient:  0.1286459617954031
iteration : 233
train acc:  0.7421875
train loss:  0.5772165060043335
train gradient:  0.15585523224063863
iteration : 234
train acc:  0.625
train loss:  0.6371400356292725
train gradient:  0.1802384419503102
iteration : 235
train acc:  0.609375
train loss:  0.6334109306335449
train gradient:  0.16868065579641114
iteration : 236
train acc:  0.671875
train loss:  0.6307781338691711
train gradient:  0.15414128560288595
iteration : 237
train acc:  0.5859375
train loss:  0.6408045887947083
train gradient:  0.17507850615445036
iteration : 238
train acc:  0.59375
train loss:  0.6668946743011475
train gradient:  0.16952595922882208
iteration : 239
train acc:  0.65625
train loss:  0.6087093353271484
train gradient:  0.1920125433759257
iteration : 240
train acc:  0.6328125
train loss:  0.6304371953010559
train gradient:  0.1718220860957548
iteration : 241
train acc:  0.6328125
train loss:  0.5793954133987427
train gradient:  0.15651895462668314
iteration : 242
train acc:  0.640625
train loss:  0.6006600260734558
train gradient:  0.1266066391081485
iteration : 243
train acc:  0.6875
train loss:  0.5864075422286987
train gradient:  0.162431592192311
iteration : 244
train acc:  0.671875
train loss:  0.6075392961502075
train gradient:  0.14323159942600225
iteration : 245
train acc:  0.6171875
train loss:  0.5941717624664307
train gradient:  0.18342253468987318
iteration : 246
train acc:  0.640625
train loss:  0.6437733769416809
train gradient:  0.1784089520960101
iteration : 247
train acc:  0.640625
train loss:  0.6367310881614685
train gradient:  0.15624997822027542
iteration : 248
train acc:  0.6875
train loss:  0.5981947183609009
train gradient:  0.11099119306393034
iteration : 249
train acc:  0.5859375
train loss:  0.6610220670700073
train gradient:  0.1854562074559754
iteration : 250
train acc:  0.6953125
train loss:  0.5630888938903809
train gradient:  0.14473239759358691
iteration : 251
train acc:  0.6015625
train loss:  0.6445011496543884
train gradient:  0.24010483922144113
iteration : 252
train acc:  0.6484375
train loss:  0.6158396601676941
train gradient:  0.1872737195605299
iteration : 253
train acc:  0.671875
train loss:  0.6182947158813477
train gradient:  0.17295266365949594
iteration : 254
train acc:  0.6875
train loss:  0.579282820224762
train gradient:  0.1485760649409393
iteration : 255
train acc:  0.640625
train loss:  0.6534911394119263
train gradient:  0.17272270607289064
iteration : 256
train acc:  0.625
train loss:  0.6254554986953735
train gradient:  0.18751961903280137
iteration : 257
train acc:  0.6171875
train loss:  0.6458322405815125
train gradient:  0.1524021084998492
iteration : 258
train acc:  0.6328125
train loss:  0.5913483500480652
train gradient:  0.1730494326614801
iteration : 259
train acc:  0.640625
train loss:  0.6331018209457397
train gradient:  0.13837705557982188
iteration : 260
train acc:  0.6328125
train loss:  0.6306441426277161
train gradient:  0.15603515333934098
iteration : 261
train acc:  0.6328125
train loss:  0.620404839515686
train gradient:  0.16262409684909646
iteration : 262
train acc:  0.59375
train loss:  0.6991064548492432
train gradient:  0.17787405582994376
iteration : 263
train acc:  0.5546875
train loss:  0.6888877153396606
train gradient:  0.21132207944734477
iteration : 264
train acc:  0.5625
train loss:  0.66163569688797
train gradient:  0.1845778250823455
iteration : 265
train acc:  0.6640625
train loss:  0.625934362411499
train gradient:  0.1619724887169851
iteration : 266
train acc:  0.671875
train loss:  0.596623420715332
train gradient:  0.12501420230402477
iteration : 267
train acc:  0.6640625
train loss:  0.5968793034553528
train gradient:  0.1823514025550319
iteration : 268
train acc:  0.6875
train loss:  0.5857330560684204
train gradient:  0.17015696681474313
iteration : 269
train acc:  0.75
train loss:  0.5746000409126282
train gradient:  0.14064446877903009
iteration : 270
train acc:  0.6328125
train loss:  0.5957733392715454
train gradient:  0.1562383558576247
iteration : 271
train acc:  0.59375
train loss:  0.6220760345458984
train gradient:  0.21730569062795152
iteration : 272
train acc:  0.609375
train loss:  0.630395770072937
train gradient:  0.23805709756681095
iteration : 273
train acc:  0.6484375
train loss:  0.6241209506988525
train gradient:  0.17624997576512502
iteration : 274
train acc:  0.5859375
train loss:  0.6643772721290588
train gradient:  0.18441356688859206
iteration : 275
train acc:  0.71875
train loss:  0.5792138576507568
train gradient:  0.1384020332635352
iteration : 276
train acc:  0.6484375
train loss:  0.6190066337585449
train gradient:  0.18494296177525635
iteration : 277
train acc:  0.6171875
train loss:  0.6540937423706055
train gradient:  0.17830475338463853
iteration : 278
train acc:  0.671875
train loss:  0.5969367027282715
train gradient:  0.1570038577079949
iteration : 279
train acc:  0.59375
train loss:  0.6594001054763794
train gradient:  0.17619385238047422
iteration : 280
train acc:  0.65625
train loss:  0.645147979259491
train gradient:  0.18384813791027224
iteration : 281
train acc:  0.6640625
train loss:  0.6008815169334412
train gradient:  0.1814898708461502
iteration : 282
train acc:  0.609375
train loss:  0.6248725652694702
train gradient:  0.14686188087239702
iteration : 283
train acc:  0.6015625
train loss:  0.6403939127922058
train gradient:  0.19974828248696472
iteration : 284
train acc:  0.640625
train loss:  0.6168591976165771
train gradient:  0.16751182853670749
iteration : 285
train acc:  0.6640625
train loss:  0.6430575847625732
train gradient:  0.1302835027004904
iteration : 286
train acc:  0.6484375
train loss:  0.6255695223808289
train gradient:  0.1790142552484446
iteration : 287
train acc:  0.6875
train loss:  0.5984747409820557
train gradient:  0.1684480685385309
iteration : 288
train acc:  0.640625
train loss:  0.6077104806900024
train gradient:  0.17137162296263136
iteration : 289
train acc:  0.671875
train loss:  0.6054051518440247
train gradient:  0.14596673792705073
iteration : 290
train acc:  0.671875
train loss:  0.5983847379684448
train gradient:  0.15176471512589165
iteration : 291
train acc:  0.671875
train loss:  0.5993447303771973
train gradient:  0.16736396685510235
iteration : 292
train acc:  0.6015625
train loss:  0.6460327506065369
train gradient:  0.16606033451642926
iteration : 293
train acc:  0.59375
train loss:  0.6316880583763123
train gradient:  0.1801757224260238
iteration : 294
train acc:  0.6171875
train loss:  0.6266168355941772
train gradient:  0.1807787142275979
iteration : 295
train acc:  0.6015625
train loss:  0.6567670106887817
train gradient:  0.18554143506144338
iteration : 296
train acc:  0.5703125
train loss:  0.6656332015991211
train gradient:  0.14621027543514387
iteration : 297
train acc:  0.65625
train loss:  0.6197253465652466
train gradient:  0.16127736562771675
iteration : 298
train acc:  0.6953125
train loss:  0.5851950645446777
train gradient:  0.19268079026239698
iteration : 299
train acc:  0.625
train loss:  0.6309918761253357
train gradient:  0.20377253477316692
iteration : 300
train acc:  0.6796875
train loss:  0.6059385538101196
train gradient:  0.13229029891093702
iteration : 301
train acc:  0.609375
train loss:  0.6104921698570251
train gradient:  0.16276178329961805
iteration : 302
train acc:  0.6015625
train loss:  0.6308913230895996
train gradient:  0.20035011194593932
iteration : 303
train acc:  0.65625
train loss:  0.6080708503723145
train gradient:  0.1424198039405846
iteration : 304
train acc:  0.6640625
train loss:  0.6042006015777588
train gradient:  0.14666200601412963
iteration : 305
train acc:  0.6328125
train loss:  0.6504582762718201
train gradient:  0.17826309304909305
iteration : 306
train acc:  0.7109375
train loss:  0.5937197804450989
train gradient:  0.16034216966175754
iteration : 307
train acc:  0.6484375
train loss:  0.5927583575248718
train gradient:  0.13023403955885193
iteration : 308
train acc:  0.65625
train loss:  0.6316174268722534
train gradient:  0.21220912437571776
iteration : 309
train acc:  0.6875
train loss:  0.5946015119552612
train gradient:  0.13323639219041195
iteration : 310
train acc:  0.6953125
train loss:  0.5876710414886475
train gradient:  0.20013476463551533
iteration : 311
train acc:  0.6484375
train loss:  0.6174783110618591
train gradient:  0.4088461285307151
iteration : 312
train acc:  0.703125
train loss:  0.5945217609405518
train gradient:  0.1709735374856326
iteration : 313
train acc:  0.6875
train loss:  0.5986877083778381
train gradient:  0.1786746415803328
iteration : 314
train acc:  0.6875
train loss:  0.5901288390159607
train gradient:  0.1801562961679344
iteration : 315
train acc:  0.640625
train loss:  0.6276514530181885
train gradient:  0.20320735314240712
iteration : 316
train acc:  0.6484375
train loss:  0.6398732662200928
train gradient:  0.20352292695188037
iteration : 317
train acc:  0.7109375
train loss:  0.6017638444900513
train gradient:  0.1436443735147734
iteration : 318
train acc:  0.6796875
train loss:  0.6227411031723022
train gradient:  0.1518768493457901
iteration : 319
train acc:  0.6015625
train loss:  0.6401833891868591
train gradient:  0.2803093921704946
iteration : 320
train acc:  0.625
train loss:  0.6306080222129822
train gradient:  0.16427400619724203
iteration : 321
train acc:  0.671875
train loss:  0.6111034154891968
train gradient:  0.18389231065487377
iteration : 322
train acc:  0.6015625
train loss:  0.6431360244750977
train gradient:  0.2136847744684775
iteration : 323
train acc:  0.671875
train loss:  0.5988088846206665
train gradient:  0.12131453360124614
iteration : 324
train acc:  0.5625
train loss:  0.6765604019165039
train gradient:  0.3112028719697533
iteration : 325
train acc:  0.6171875
train loss:  0.6111088991165161
train gradient:  0.14307867451646414
iteration : 326
train acc:  0.765625
train loss:  0.5513144731521606
train gradient:  0.1479276059387769
iteration : 327
train acc:  0.6328125
train loss:  0.6164923310279846
train gradient:  0.16325664462494466
iteration : 328
train acc:  0.65625
train loss:  0.6069464087486267
train gradient:  0.15772945503155783
iteration : 329
train acc:  0.7421875
train loss:  0.5598750710487366
train gradient:  0.17285249408918446
iteration : 330
train acc:  0.6640625
train loss:  0.5729724168777466
train gradient:  0.13587347015272414
iteration : 331
train acc:  0.6953125
train loss:  0.5722326040267944
train gradient:  0.14475622574037122
iteration : 332
train acc:  0.7421875
train loss:  0.5498886704444885
train gradient:  0.12114170666549107
iteration : 333
train acc:  0.7109375
train loss:  0.6083688735961914
train gradient:  0.14515278586598057
iteration : 334
train acc:  0.6171875
train loss:  0.6424452066421509
train gradient:  0.23865761661832677
iteration : 335
train acc:  0.6796875
train loss:  0.5967134833335876
train gradient:  0.12354472197262234
iteration : 336
train acc:  0.6875
train loss:  0.576732873916626
train gradient:  0.1484367474623074
iteration : 337
train acc:  0.640625
train loss:  0.6169923543930054
train gradient:  0.22772725888958956
iteration : 338
train acc:  0.640625
train loss:  0.6075046062469482
train gradient:  0.14880847624042082
iteration : 339
train acc:  0.640625
train loss:  0.6298983693122864
train gradient:  0.20243901468808517
iteration : 340
train acc:  0.65625
train loss:  0.565354585647583
train gradient:  0.13158368166733692
iteration : 341
train acc:  0.65625
train loss:  0.5707247853279114
train gradient:  0.19998168573418995
iteration : 342
train acc:  0.625
train loss:  0.6389784812927246
train gradient:  0.16089667180921263
iteration : 343
train acc:  0.671875
train loss:  0.5748636722564697
train gradient:  0.22491644958595858
iteration : 344
train acc:  0.6484375
train loss:  0.6038656830787659
train gradient:  0.1709587068138808
iteration : 345
train acc:  0.6796875
train loss:  0.5698672533035278
train gradient:  0.13927756640960137
iteration : 346
train acc:  0.6796875
train loss:  0.5818219184875488
train gradient:  0.16477865726718569
iteration : 347
train acc:  0.7578125
train loss:  0.5490562915802002
train gradient:  0.16389487019962157
iteration : 348
train acc:  0.6640625
train loss:  0.5947274565696716
train gradient:  0.25396115219858334
iteration : 349
train acc:  0.71875
train loss:  0.5865679979324341
train gradient:  0.1246597589541841
iteration : 350
train acc:  0.640625
train loss:  0.6116159558296204
train gradient:  0.17673791127555294
iteration : 351
train acc:  0.640625
train loss:  0.6003536581993103
train gradient:  0.16429867993653446
iteration : 352
train acc:  0.6796875
train loss:  0.6085082292556763
train gradient:  0.17220532866336327
iteration : 353
train acc:  0.703125
train loss:  0.5991981029510498
train gradient:  0.3964152367374624
iteration : 354
train acc:  0.65625
train loss:  0.627902626991272
train gradient:  0.17293191970422528
iteration : 355
train acc:  0.625
train loss:  0.6363422274589539
train gradient:  0.21545520262059925
iteration : 356
train acc:  0.6015625
train loss:  0.6285122632980347
train gradient:  0.1895074780562117
iteration : 357
train acc:  0.625
train loss:  0.6130001544952393
train gradient:  0.19198730328034957
iteration : 358
train acc:  0.625
train loss:  0.6326308250427246
train gradient:  0.280717399899559
iteration : 359
train acc:  0.6328125
train loss:  0.6657067537307739
train gradient:  0.2632871907938719
iteration : 360
train acc:  0.6640625
train loss:  0.6134225726127625
train gradient:  0.1553476589626424
iteration : 361
train acc:  0.6484375
train loss:  0.6453579068183899
train gradient:  0.20050607652378175
iteration : 362
train acc:  0.5703125
train loss:  0.6575573682785034
train gradient:  0.1763339987330117
iteration : 363
train acc:  0.6875
train loss:  0.5946180820465088
train gradient:  0.23472082818187462
iteration : 364
train acc:  0.6640625
train loss:  0.6009814143180847
train gradient:  0.13336704807854793
iteration : 365
train acc:  0.703125
train loss:  0.5777527689933777
train gradient:  0.16475525525857163
iteration : 366
train acc:  0.6640625
train loss:  0.5952874422073364
train gradient:  0.13255822552831098
iteration : 367
train acc:  0.6328125
train loss:  0.6196264028549194
train gradient:  0.21654279790777464
iteration : 368
train acc:  0.6875
train loss:  0.5479382276535034
train gradient:  0.1295082514351507
iteration : 369
train acc:  0.6328125
train loss:  0.6160236597061157
train gradient:  0.19340272505802086
iteration : 370
train acc:  0.703125
train loss:  0.5844377875328064
train gradient:  0.16112377359351898
iteration : 371
train acc:  0.5859375
train loss:  0.6151085495948792
train gradient:  0.1736727120580373
iteration : 372
train acc:  0.7109375
train loss:  0.5540323853492737
train gradient:  0.1619434147548087
iteration : 373
train acc:  0.6953125
train loss:  0.595319390296936
train gradient:  0.16689840927774052
iteration : 374
train acc:  0.765625
train loss:  0.5561888813972473
train gradient:  0.17387620642335633
iteration : 375
train acc:  0.65625
train loss:  0.6043516993522644
train gradient:  0.15860175259499565
iteration : 376
train acc:  0.671875
train loss:  0.6341128349304199
train gradient:  0.21171936634586036
iteration : 377
train acc:  0.6640625
train loss:  0.6024395227432251
train gradient:  0.1328410009905263
iteration : 378
train acc:  0.640625
train loss:  0.636150598526001
train gradient:  0.17794267286733567
iteration : 379
train acc:  0.6328125
train loss:  0.6014968156814575
train gradient:  0.23750814240816714
iteration : 380
train acc:  0.6171875
train loss:  0.6002187132835388
train gradient:  0.1478227763308318
iteration : 381
train acc:  0.6484375
train loss:  0.6101705431938171
train gradient:  0.1483945265961175
iteration : 382
train acc:  0.71875
train loss:  0.5791329145431519
train gradient:  0.16083629722286114
iteration : 383
train acc:  0.6015625
train loss:  0.6291384696960449
train gradient:  0.21645054618989934
iteration : 384
train acc:  0.640625
train loss:  0.6113350987434387
train gradient:  0.20597741268523806
iteration : 385
train acc:  0.7578125
train loss:  0.5791956782341003
train gradient:  0.16476823243868832
iteration : 386
train acc:  0.703125
train loss:  0.5744274854660034
train gradient:  0.171413037429608
iteration : 387
train acc:  0.671875
train loss:  0.6263363361358643
train gradient:  0.14120995072980833
iteration : 388
train acc:  0.640625
train loss:  0.5932743549346924
train gradient:  0.24511277612271898
iteration : 389
train acc:  0.6875
train loss:  0.6240826845169067
train gradient:  0.1729959740617669
iteration : 390
train acc:  0.625
train loss:  0.6572188138961792
train gradient:  0.4126764497945472
iteration : 391
train acc:  0.6484375
train loss:  0.6265501976013184
train gradient:  0.19447294489649486
iteration : 392
train acc:  0.6015625
train loss:  0.6457017064094543
train gradient:  0.2115082293866838
iteration : 393
train acc:  0.671875
train loss:  0.5833353996276855
train gradient:  0.14740308179066636
iteration : 394
train acc:  0.703125
train loss:  0.5675824880599976
train gradient:  0.14675779314793638
iteration : 395
train acc:  0.6171875
train loss:  0.6279411315917969
train gradient:  0.20567918468500568
iteration : 396
train acc:  0.6484375
train loss:  0.6185557842254639
train gradient:  0.1725191138758042
iteration : 397
train acc:  0.65625
train loss:  0.6361346244812012
train gradient:  0.16876033877053093
iteration : 398
train acc:  0.6953125
train loss:  0.6345629096031189
train gradient:  0.16389445984321432
iteration : 399
train acc:  0.6796875
train loss:  0.6103248596191406
train gradient:  0.1562694123077601
iteration : 400
train acc:  0.6796875
train loss:  0.5873006582260132
train gradient:  0.2184792135132419
iteration : 401
train acc:  0.65625
train loss:  0.60172438621521
train gradient:  0.15058707673934424
iteration : 402
train acc:  0.6875
train loss:  0.5941741466522217
train gradient:  0.17896997800129952
iteration : 403
train acc:  0.71875
train loss:  0.5545268654823303
train gradient:  0.1455981374367406
iteration : 404
train acc:  0.625
train loss:  0.626415491104126
train gradient:  0.18771181906007417
iteration : 405
train acc:  0.7734375
train loss:  0.5577588081359863
train gradient:  0.2100677575448438
iteration : 406
train acc:  0.6328125
train loss:  0.5975488424301147
train gradient:  0.1479254583092443
iteration : 407
train acc:  0.7265625
train loss:  0.587824285030365
train gradient:  0.18493853021972603
iteration : 408
train acc:  0.6796875
train loss:  0.6009576916694641
train gradient:  0.16057464919919961
iteration : 409
train acc:  0.765625
train loss:  0.5859890580177307
train gradient:  0.1424896980247261
iteration : 410
train acc:  0.671875
train loss:  0.5950255393981934
train gradient:  0.15402173183121123
iteration : 411
train acc:  0.6640625
train loss:  0.5798346996307373
train gradient:  0.17138774234279325
iteration : 412
train acc:  0.609375
train loss:  0.6172001361846924
train gradient:  0.16739539233833292
iteration : 413
train acc:  0.6796875
train loss:  0.6147412061691284
train gradient:  0.1656741365250541
iteration : 414
train acc:  0.625
train loss:  0.6436058282852173
train gradient:  0.1821021935065144
iteration : 415
train acc:  0.7421875
train loss:  0.5379072427749634
train gradient:  0.14325200337603938
iteration : 416
train acc:  0.703125
train loss:  0.5979241728782654
train gradient:  0.16471678051721356
iteration : 417
train acc:  0.703125
train loss:  0.5781998038291931
train gradient:  0.11794212207717673
iteration : 418
train acc:  0.6171875
train loss:  0.6335242986679077
train gradient:  0.2847256846168144
iteration : 419
train acc:  0.5546875
train loss:  0.6672987937927246
train gradient:  0.22601000041335204
iteration : 420
train acc:  0.6796875
train loss:  0.5943613052368164
train gradient:  0.17731100078826248
iteration : 421
train acc:  0.75
train loss:  0.5191872119903564
train gradient:  0.12625508619265352
iteration : 422
train acc:  0.625
train loss:  0.6259106397628784
train gradient:  0.1771323264745771
iteration : 423
train acc:  0.671875
train loss:  0.5946544408798218
train gradient:  0.14388605060136095
iteration : 424
train acc:  0.7265625
train loss:  0.5788317918777466
train gradient:  0.16474633591825671
iteration : 425
train acc:  0.6484375
train loss:  0.6116806268692017
train gradient:  0.16486723343029136
iteration : 426
train acc:  0.7265625
train loss:  0.5824815630912781
train gradient:  0.19291616943098178
iteration : 427
train acc:  0.6640625
train loss:  0.562031626701355
train gradient:  0.13558775225488776
iteration : 428
train acc:  0.6875
train loss:  0.595299482345581
train gradient:  0.14757484404128984
iteration : 429
train acc:  0.6796875
train loss:  0.5953940153121948
train gradient:  0.10607866678156012
iteration : 430
train acc:  0.640625
train loss:  0.6102086305618286
train gradient:  0.18195598849101746
iteration : 431
train acc:  0.6875
train loss:  0.6129562854766846
train gradient:  0.15332665501099157
iteration : 432
train acc:  0.6875
train loss:  0.5418628454208374
train gradient:  0.17621252324279407
iteration : 433
train acc:  0.765625
train loss:  0.5375459790229797
train gradient:  0.1153462946981771
iteration : 434
train acc:  0.6484375
train loss:  0.6136993765830994
train gradient:  0.1863753092573167
iteration : 435
train acc:  0.6953125
train loss:  0.5977151393890381
train gradient:  0.19144780221009397
iteration : 436
train acc:  0.5859375
train loss:  0.6435362100601196
train gradient:  0.2265011152427066
iteration : 437
train acc:  0.7109375
train loss:  0.5816009044647217
train gradient:  0.20414549601767532
iteration : 438
train acc:  0.640625
train loss:  0.596593976020813
train gradient:  0.18271822531669613
iteration : 439
train acc:  0.65625
train loss:  0.59605872631073
train gradient:  0.18419515787831942
iteration : 440
train acc:  0.703125
train loss:  0.5602208375930786
train gradient:  0.16339830396333418
iteration : 441
train acc:  0.625
train loss:  0.6351494789123535
train gradient:  0.26359242973756863
iteration : 442
train acc:  0.6796875
train loss:  0.5886025428771973
train gradient:  0.16718785930075825
iteration : 443
train acc:  0.671875
train loss:  0.6221910119056702
train gradient:  0.15740836394975305
iteration : 444
train acc:  0.640625
train loss:  0.6284887790679932
train gradient:  0.18926814706079725
iteration : 445
train acc:  0.5703125
train loss:  0.6526223421096802
train gradient:  0.16729182120242125
iteration : 446
train acc:  0.6875
train loss:  0.5611506700515747
train gradient:  0.12541196942102908
iteration : 447
train acc:  0.65625
train loss:  0.587623119354248
train gradient:  0.17782206039539303
iteration : 448
train acc:  0.6953125
train loss:  0.5831266641616821
train gradient:  0.14874539528803427
iteration : 449
train acc:  0.7265625
train loss:  0.5788297653198242
train gradient:  0.12685111053006026
iteration : 450
train acc:  0.65625
train loss:  0.6270238161087036
train gradient:  0.16197799697683507
iteration : 451
train acc:  0.703125
train loss:  0.555747926235199
train gradient:  0.14824933184923433
iteration : 452
train acc:  0.6953125
train loss:  0.576033353805542
train gradient:  0.19927885078386498
iteration : 453
train acc:  0.6328125
train loss:  0.611750066280365
train gradient:  0.17852215632825288
iteration : 454
train acc:  0.65625
train loss:  0.6249783635139465
train gradient:  0.24426575586073593
iteration : 455
train acc:  0.71875
train loss:  0.5565196871757507
train gradient:  0.1550866982518343
iteration : 456
train acc:  0.625
train loss:  0.6202219128608704
train gradient:  0.1845412492061465
iteration : 457
train acc:  0.6328125
train loss:  0.6103601455688477
train gradient:  0.18588938699272073
iteration : 458
train acc:  0.734375
train loss:  0.5697553157806396
train gradient:  0.13403712682870733
iteration : 459
train acc:  0.6484375
train loss:  0.6025654077529907
train gradient:  0.1299214635866575
iteration : 460
train acc:  0.703125
train loss:  0.5717415809631348
train gradient:  0.17950326359753654
iteration : 461
train acc:  0.6328125
train loss:  0.6192827224731445
train gradient:  0.13374931304913645
iteration : 462
train acc:  0.6328125
train loss:  0.6069253087043762
train gradient:  0.13298444175558524
iteration : 463
train acc:  0.640625
train loss:  0.6476301550865173
train gradient:  0.16929956014968683
iteration : 464
train acc:  0.7421875
train loss:  0.5608648061752319
train gradient:  0.17269414887367363
iteration : 465
train acc:  0.640625
train loss:  0.5971622467041016
train gradient:  0.1483066089429712
iteration : 466
train acc:  0.6640625
train loss:  0.5891866683959961
train gradient:  0.17367770475327982
iteration : 467
train acc:  0.7421875
train loss:  0.5395756363868713
train gradient:  0.13177494805252699
iteration : 468
train acc:  0.625
train loss:  0.6521631479263306
train gradient:  0.486426403869105
iteration : 469
train acc:  0.71875
train loss:  0.5894430875778198
train gradient:  0.23763087366648172
iteration : 470
train acc:  0.734375
train loss:  0.5346256494522095
train gradient:  0.13589702006282922
iteration : 471
train acc:  0.6171875
train loss:  0.6701074242591858
train gradient:  0.19068230445560996
iteration : 472
train acc:  0.6796875
train loss:  0.5809488296508789
train gradient:  0.14084536294644162
iteration : 473
train acc:  0.6640625
train loss:  0.6018433570861816
train gradient:  0.24824753613736827
iteration : 474
train acc:  0.6484375
train loss:  0.586477518081665
train gradient:  0.1670846532564721
iteration : 475
train acc:  0.65625
train loss:  0.5798237323760986
train gradient:  0.15652566041459665
iteration : 476
train acc:  0.703125
train loss:  0.5852786302566528
train gradient:  0.1618941782467183
iteration : 477
train acc:  0.703125
train loss:  0.5879324674606323
train gradient:  0.16524230178978794
iteration : 478
train acc:  0.6640625
train loss:  0.5726659893989563
train gradient:  0.15791389777920262
iteration : 479
train acc:  0.65625
train loss:  0.5877612829208374
train gradient:  0.13869525844525693
iteration : 480
train acc:  0.71875
train loss:  0.5436104536056519
train gradient:  0.161138556098851
iteration : 481
train acc:  0.671875
train loss:  0.6432602405548096
train gradient:  0.1556518787674458
iteration : 482
train acc:  0.75
train loss:  0.5620934963226318
train gradient:  0.18855942752468086
iteration : 483
train acc:  0.6953125
train loss:  0.5824427604675293
train gradient:  0.15985304897415076
iteration : 484
train acc:  0.6875
train loss:  0.5693951845169067
train gradient:  0.1505815597099795
iteration : 485
train acc:  0.640625
train loss:  0.636310875415802
train gradient:  0.21351225850194203
iteration : 486
train acc:  0.6796875
train loss:  0.623126208782196
train gradient:  0.23657235813545383
iteration : 487
train acc:  0.625
train loss:  0.6499424576759338
train gradient:  0.17287775979517456
iteration : 488
train acc:  0.703125
train loss:  0.5624089241027832
train gradient:  0.22593145081793986
iteration : 489
train acc:  0.6171875
train loss:  0.627744197845459
train gradient:  0.19458993832569416
iteration : 490
train acc:  0.6875
train loss:  0.5889156460762024
train gradient:  0.12835742671776618
iteration : 491
train acc:  0.671875
train loss:  0.628350555896759
train gradient:  0.22121820888568605
iteration : 492
train acc:  0.71875
train loss:  0.5610637664794922
train gradient:  0.14799296030680148
iteration : 493
train acc:  0.703125
train loss:  0.5953004956245422
train gradient:  0.21643462291057633
iteration : 494
train acc:  0.703125
train loss:  0.5623571276664734
train gradient:  0.20415306355511037
iteration : 495
train acc:  0.65625
train loss:  0.6175699234008789
train gradient:  0.17535958269098323
iteration : 496
train acc:  0.7890625
train loss:  0.49229946732521057
train gradient:  0.14458130709180916
iteration : 497
train acc:  0.6640625
train loss:  0.5807440280914307
train gradient:  0.18728333138977332
iteration : 498
train acc:  0.7265625
train loss:  0.5828953981399536
train gradient:  0.15056783786736178
iteration : 499
train acc:  0.546875
train loss:  0.6492151021957397
train gradient:  0.17384727359524738
iteration : 500
train acc:  0.609375
train loss:  0.6257412433624268
train gradient:  0.21048991127106137
iteration : 501
train acc:  0.7578125
train loss:  0.5533108711242676
train gradient:  0.1679153308543154
iteration : 502
train acc:  0.625
train loss:  0.6126490831375122
train gradient:  0.13344714309681288
iteration : 503
train acc:  0.6953125
train loss:  0.575395941734314
train gradient:  0.13629054175727873
iteration : 504
train acc:  0.5859375
train loss:  0.6444175243377686
train gradient:  0.2024030121648269
iteration : 505
train acc:  0.6875
train loss:  0.5691306591033936
train gradient:  0.167700965002388
iteration : 506
train acc:  0.703125
train loss:  0.5634542107582092
train gradient:  0.15632443740766694
iteration : 507
train acc:  0.7734375
train loss:  0.5351029634475708
train gradient:  0.15406130834310422
iteration : 508
train acc:  0.6796875
train loss:  0.5816932320594788
train gradient:  0.18661399728093134
iteration : 509
train acc:  0.546875
train loss:  0.6745991706848145
train gradient:  0.21566961170067894
iteration : 510
train acc:  0.734375
train loss:  0.5543859601020813
train gradient:  0.16160234563202425
iteration : 511
train acc:  0.71875
train loss:  0.5790786743164062
train gradient:  0.17698746464135473
iteration : 512
train acc:  0.671875
train loss:  0.6120250821113586
train gradient:  0.18029893996907248
iteration : 513
train acc:  0.671875
train loss:  0.5919277667999268
train gradient:  0.17037848906522662
iteration : 514
train acc:  0.671875
train loss:  0.5674246549606323
train gradient:  0.19600035789680612
iteration : 515
train acc:  0.6328125
train loss:  0.6504637002944946
train gradient:  0.20780902824458097
iteration : 516
train acc:  0.6640625
train loss:  0.6374928951263428
train gradient:  0.18890873353343765
iteration : 517
train acc:  0.6640625
train loss:  0.5981975793838501
train gradient:  0.22183428209677694
iteration : 518
train acc:  0.65625
train loss:  0.5825680494308472
train gradient:  0.14929586196555988
iteration : 519
train acc:  0.65625
train loss:  0.6261357665061951
train gradient:  0.13461793739010475
iteration : 520
train acc:  0.6875
train loss:  0.5776241421699524
train gradient:  0.2314575052953094
iteration : 521
train acc:  0.734375
train loss:  0.5321416854858398
train gradient:  0.1300568451255933
iteration : 522
train acc:  0.6953125
train loss:  0.5871642231941223
train gradient:  0.13607809903363433
iteration : 523
train acc:  0.703125
train loss:  0.5696130394935608
train gradient:  0.16017953749829517
iteration : 524
train acc:  0.734375
train loss:  0.5567176342010498
train gradient:  0.14735979736408866
iteration : 525
train acc:  0.6640625
train loss:  0.6028876900672913
train gradient:  0.20835585364965564
iteration : 526
train acc:  0.6328125
train loss:  0.594864010810852
train gradient:  0.17852607896866143
iteration : 527
train acc:  0.65625
train loss:  0.6164821982383728
train gradient:  0.2715817200832202
iteration : 528
train acc:  0.734375
train loss:  0.5474028587341309
train gradient:  0.16570367622442922
iteration : 529
train acc:  0.6796875
train loss:  0.5692164897918701
train gradient:  0.14790111737860495
iteration : 530
train acc:  0.7578125
train loss:  0.5186591744422913
train gradient:  0.16415165380542307
iteration : 531
train acc:  0.6953125
train loss:  0.579711377620697
train gradient:  0.1665366979403768
iteration : 532
train acc:  0.703125
train loss:  0.5857626795768738
train gradient:  0.1817634571576819
iteration : 533
train acc:  0.7109375
train loss:  0.5798419117927551
train gradient:  0.2155480612879721
iteration : 534
train acc:  0.65625
train loss:  0.5615510940551758
train gradient:  0.1470130703862787
iteration : 535
train acc:  0.625
train loss:  0.6070774793624878
train gradient:  0.1398218832913698
iteration : 536
train acc:  0.71875
train loss:  0.562298059463501
train gradient:  0.18904909115643845
iteration : 537
train acc:  0.6484375
train loss:  0.5821967124938965
train gradient:  0.1380139728581162
iteration : 538
train acc:  0.671875
train loss:  0.576560378074646
train gradient:  0.14399124820026
iteration : 539
train acc:  0.734375
train loss:  0.5478402376174927
train gradient:  0.1459766319757259
iteration : 540
train acc:  0.6953125
train loss:  0.5561510324478149
train gradient:  0.2784966705380679
iteration : 541
train acc:  0.71875
train loss:  0.557910680770874
train gradient:  0.17172445593289665
iteration : 542
train acc:  0.640625
train loss:  0.6103836297988892
train gradient:  0.18140694406842212
iteration : 543
train acc:  0.6953125
train loss:  0.5828623175621033
train gradient:  0.1534885620625234
iteration : 544
train acc:  0.6171875
train loss:  0.6213899850845337
train gradient:  0.21280912820015163
iteration : 545
train acc:  0.6484375
train loss:  0.6362124085426331
train gradient:  0.17885401872825127
iteration : 546
train acc:  0.671875
train loss:  0.5562427043914795
train gradient:  0.13908011276776344
iteration : 547
train acc:  0.7265625
train loss:  0.5384973287582397
train gradient:  0.12778800638809087
iteration : 548
train acc:  0.6875
train loss:  0.5701084136962891
train gradient:  0.17751299951324367
iteration : 549
train acc:  0.6640625
train loss:  0.5997438430786133
train gradient:  0.19101526312689965
iteration : 550
train acc:  0.703125
train loss:  0.5607795119285583
train gradient:  0.1823011468250802
iteration : 551
train acc:  0.6796875
train loss:  0.5740605592727661
train gradient:  0.1621076642679556
iteration : 552
train acc:  0.703125
train loss:  0.5964977741241455
train gradient:  0.3087296038886188
iteration : 553
train acc:  0.640625
train loss:  0.6203004121780396
train gradient:  0.18386262275662224
iteration : 554
train acc:  0.671875
train loss:  0.6312071084976196
train gradient:  0.2024285610470308
iteration : 555
train acc:  0.671875
train loss:  0.586168646812439
train gradient:  0.1246543415905127
iteration : 556
train acc:  0.6640625
train loss:  0.6186034679412842
train gradient:  0.15202147690043089
iteration : 557
train acc:  0.609375
train loss:  0.6249721050262451
train gradient:  0.20038064960909466
iteration : 558
train acc:  0.6640625
train loss:  0.5950543880462646
train gradient:  0.16019802865249783
iteration : 559
train acc:  0.671875
train loss:  0.6080427169799805
train gradient:  0.3270406653855344
iteration : 560
train acc:  0.6640625
train loss:  0.5794637203216553
train gradient:  0.20979807069568474
iteration : 561
train acc:  0.7265625
train loss:  0.5494437217712402
train gradient:  0.13469484225159356
iteration : 562
train acc:  0.71875
train loss:  0.5625219345092773
train gradient:  0.22745002660021382
iteration : 563
train acc:  0.703125
train loss:  0.5712440013885498
train gradient:  0.13849930485168582
iteration : 564
train acc:  0.6796875
train loss:  0.5719612836837769
train gradient:  0.2500370330247528
iteration : 565
train acc:  0.6171875
train loss:  0.6538273692131042
train gradient:  0.18896352578496722
iteration : 566
train acc:  0.640625
train loss:  0.602070689201355
train gradient:  0.147891871750452
iteration : 567
train acc:  0.6875
train loss:  0.5899595618247986
train gradient:  0.13673492958727715
iteration : 568
train acc:  0.609375
train loss:  0.6620715260505676
train gradient:  0.2810918805644971
iteration : 569
train acc:  0.6640625
train loss:  0.6108788251876831
train gradient:  0.17635931486017042
iteration : 570
train acc:  0.640625
train loss:  0.6044816374778748
train gradient:  0.15161734116770317
iteration : 571
train acc:  0.640625
train loss:  0.628474235534668
train gradient:  0.167948603365972
iteration : 572
train acc:  0.6796875
train loss:  0.5516316294670105
train gradient:  0.12507654868588533
iteration : 573
train acc:  0.6328125
train loss:  0.5960090756416321
train gradient:  0.22916198725818704
iteration : 574
train acc:  0.71875
train loss:  0.5107994079589844
train gradient:  0.13240667756328245
iteration : 575
train acc:  0.7109375
train loss:  0.5954488515853882
train gradient:  0.22394872423728945
iteration : 576
train acc:  0.6875
train loss:  0.5770075917243958
train gradient:  0.1684900666197756
iteration : 577
train acc:  0.6953125
train loss:  0.5910825133323669
train gradient:  0.16736144700501418
iteration : 578
train acc:  0.671875
train loss:  0.56947261095047
train gradient:  0.11559856700333608
iteration : 579
train acc:  0.6015625
train loss:  0.655900239944458
train gradient:  0.1627925133677024
iteration : 580
train acc:  0.6953125
train loss:  0.5854882001876831
train gradient:  0.18450238174519473
iteration : 581
train acc:  0.7421875
train loss:  0.5366256237030029
train gradient:  0.19052323601945656
iteration : 582
train acc:  0.7109375
train loss:  0.5754830837249756
train gradient:  0.1290482381698173
iteration : 583
train acc:  0.625
train loss:  0.6260244846343994
train gradient:  0.19319451681850763
iteration : 584
train acc:  0.6953125
train loss:  0.5772666931152344
train gradient:  0.13153847891851125
iteration : 585
train acc:  0.625
train loss:  0.6111733317375183
train gradient:  0.15084419792492632
iteration : 586
train acc:  0.703125
train loss:  0.5903263092041016
train gradient:  0.19102549664710738
iteration : 587
train acc:  0.671875
train loss:  0.5783759355545044
train gradient:  0.1508045556404435
iteration : 588
train acc:  0.65625
train loss:  0.6188112497329712
train gradient:  0.18508729847498437
iteration : 589
train acc:  0.609375
train loss:  0.6314699053764343
train gradient:  0.21719365043935523
iteration : 590
train acc:  0.6640625
train loss:  0.6370205283164978
train gradient:  0.18459272626431922
iteration : 591
train acc:  0.671875
train loss:  0.5915428400039673
train gradient:  0.16215177430963645
iteration : 592
train acc:  0.6640625
train loss:  0.5979543328285217
train gradient:  0.178520447357102
iteration : 593
train acc:  0.6640625
train loss:  0.6127604246139526
train gradient:  0.17077765394344474
iteration : 594
train acc:  0.6875
train loss:  0.6000584959983826
train gradient:  0.17203034499197578
iteration : 595
train acc:  0.6328125
train loss:  0.6148290634155273
train gradient:  0.3599800044071745
iteration : 596
train acc:  0.6640625
train loss:  0.5948265790939331
train gradient:  0.14535159774853282
iteration : 597
train acc:  0.6875
train loss:  0.589675784111023
train gradient:  0.12832864206287148
iteration : 598
train acc:  0.671875
train loss:  0.6105562448501587
train gradient:  0.2142769713625095
iteration : 599
train acc:  0.6875
train loss:  0.5683547258377075
train gradient:  0.12867703246982543
iteration : 600
train acc:  0.671875
train loss:  0.5828993320465088
train gradient:  0.1795320623243581
iteration : 601
train acc:  0.703125
train loss:  0.5350394248962402
train gradient:  0.18023201783705783
iteration : 602
train acc:  0.6015625
train loss:  0.6527940630912781
train gradient:  0.29491276309602166
iteration : 603
train acc:  0.6875
train loss:  0.595235288143158
train gradient:  0.24877512004369007
iteration : 604
train acc:  0.6796875
train loss:  0.5773727893829346
train gradient:  0.1492336515817393
iteration : 605
train acc:  0.703125
train loss:  0.5793483853340149
train gradient:  0.145117761452523
iteration : 606
train acc:  0.65625
train loss:  0.5788801908493042
train gradient:  0.21018222101886608
iteration : 607
train acc:  0.78125
train loss:  0.52228844165802
train gradient:  0.14165421481099078
iteration : 608
train acc:  0.7265625
train loss:  0.5461195707321167
train gradient:  0.14547369794540743
iteration : 609
train acc:  0.7265625
train loss:  0.5115773677825928
train gradient:  0.14239171458839445
iteration : 610
train acc:  0.734375
train loss:  0.5537158846855164
train gradient:  0.21079410597398768
iteration : 611
train acc:  0.7578125
train loss:  0.5378339886665344
train gradient:  0.1372070638282148
iteration : 612
train acc:  0.75
train loss:  0.5697722434997559
train gradient:  0.1293397084503367
iteration : 613
train acc:  0.65625
train loss:  0.6147717833518982
train gradient:  0.15921537722146284
iteration : 614
train acc:  0.6796875
train loss:  0.5691258907318115
train gradient:  0.11860837856280995
iteration : 615
train acc:  0.625
train loss:  0.6361266374588013
train gradient:  0.18400011800358246
iteration : 616
train acc:  0.6796875
train loss:  0.6061023473739624
train gradient:  0.21210795174615846
iteration : 617
train acc:  0.6953125
train loss:  0.576391339302063
train gradient:  0.12840150608876433
iteration : 618
train acc:  0.671875
train loss:  0.5594558715820312
train gradient:  0.178568113018735
iteration : 619
train acc:  0.703125
train loss:  0.5640349984169006
train gradient:  0.18492748832519254
iteration : 620
train acc:  0.71875
train loss:  0.5665433406829834
train gradient:  0.1459081472343234
iteration : 621
train acc:  0.6953125
train loss:  0.5980105400085449
train gradient:  0.1377255655047457
iteration : 622
train acc:  0.734375
train loss:  0.5575973987579346
train gradient:  0.1330725206895002
iteration : 623
train acc:  0.671875
train loss:  0.6248760223388672
train gradient:  0.21758953107046283
iteration : 624
train acc:  0.6171875
train loss:  0.6149851083755493
train gradient:  0.22105529197926208
iteration : 625
train acc:  0.6875
train loss:  0.5714640617370605
train gradient:  0.17498973904537668
iteration : 626
train acc:  0.640625
train loss:  0.5940548777580261
train gradient:  0.14805303936074476
iteration : 627
train acc:  0.6953125
train loss:  0.5421732664108276
train gradient:  0.21333726737926384
iteration : 628
train acc:  0.71875
train loss:  0.5506960153579712
train gradient:  0.16651183925648544
iteration : 629
train acc:  0.6328125
train loss:  0.6425641775131226
train gradient:  0.1999986221067081
iteration : 630
train acc:  0.6328125
train loss:  0.610649049282074
train gradient:  0.15781866715640894
iteration : 631
train acc:  0.6171875
train loss:  0.6288187503814697
train gradient:  0.19964182617043302
iteration : 632
train acc:  0.6796875
train loss:  0.6034751534461975
train gradient:  0.2207665350804519
iteration : 633
train acc:  0.6796875
train loss:  0.6017826795578003
train gradient:  0.1669114853768352
iteration : 634
train acc:  0.671875
train loss:  0.5769770741462708
train gradient:  0.16777896928630567
iteration : 635
train acc:  0.6875
train loss:  0.5927166938781738
train gradient:  0.28834876582927216
iteration : 636
train acc:  0.71875
train loss:  0.5466687679290771
train gradient:  0.13763181638760252
iteration : 637
train acc:  0.6796875
train loss:  0.6236761808395386
train gradient:  0.19627501350168564
iteration : 638
train acc:  0.6640625
train loss:  0.6167352199554443
train gradient:  0.15843574869601973
iteration : 639
train acc:  0.6953125
train loss:  0.5455447435379028
train gradient:  0.17486851164861755
iteration : 640
train acc:  0.6875
train loss:  0.5652532577514648
train gradient:  0.1463160919026766
iteration : 641
train acc:  0.625
train loss:  0.6655080318450928
train gradient:  0.2943039750572099
iteration : 642
train acc:  0.6796875
train loss:  0.5888020992279053
train gradient:  0.1725738390668347
iteration : 643
train acc:  0.6484375
train loss:  0.5902334451675415
train gradient:  0.16600225060486273
iteration : 644
train acc:  0.7890625
train loss:  0.5400853157043457
train gradient:  0.14530198660672916
iteration : 645
train acc:  0.671875
train loss:  0.5409543514251709
train gradient:  0.14335327720770985
iteration : 646
train acc:  0.625
train loss:  0.6137577891349792
train gradient:  0.1718715737602703
iteration : 647
train acc:  0.6953125
train loss:  0.5856707096099854
train gradient:  0.15636464775229042
iteration : 648
train acc:  0.6875
train loss:  0.5575114488601685
train gradient:  0.18823338122814268
iteration : 649
train acc:  0.7109375
train loss:  0.5909779071807861
train gradient:  0.19495527971209678
iteration : 650
train acc:  0.6953125
train loss:  0.6196017265319824
train gradient:  0.16634947115177212
iteration : 651
train acc:  0.703125
train loss:  0.5811799764633179
train gradient:  0.15892124792073872
iteration : 652
train acc:  0.7109375
train loss:  0.5764434337615967
train gradient:  0.19771183630069605
iteration : 653
train acc:  0.6953125
train loss:  0.6098021268844604
train gradient:  0.45342250507711496
iteration : 654
train acc:  0.71875
train loss:  0.5520464181900024
train gradient:  0.2103952123705755
iteration : 655
train acc:  0.59375
train loss:  0.6088684797286987
train gradient:  0.1863561153495742
iteration : 656
train acc:  0.6640625
train loss:  0.6060879230499268
train gradient:  0.1513342355756196
iteration : 657
train acc:  0.6953125
train loss:  0.5701419711112976
train gradient:  0.20851108312887484
iteration : 658
train acc:  0.640625
train loss:  0.6252635717391968
train gradient:  0.15137074832322273
iteration : 659
train acc:  0.6484375
train loss:  0.6013250350952148
train gradient:  0.21847465500286997
iteration : 660
train acc:  0.6796875
train loss:  0.5666091442108154
train gradient:  0.16562966715076383
iteration : 661
train acc:  0.7421875
train loss:  0.5445506572723389
train gradient:  0.13105124357010373
iteration : 662
train acc:  0.65625
train loss:  0.6173127889633179
train gradient:  0.14123591320470294
iteration : 663
train acc:  0.65625
train loss:  0.580568253993988
train gradient:  0.17454908165129265
iteration : 664
train acc:  0.609375
train loss:  0.6557697057723999
train gradient:  0.3961173999146318
iteration : 665
train acc:  0.6328125
train loss:  0.6363427639007568
train gradient:  0.16681397547883406
iteration : 666
train acc:  0.6484375
train loss:  0.5804755687713623
train gradient:  0.25411886917530163
iteration : 667
train acc:  0.6640625
train loss:  0.6214070916175842
train gradient:  0.39018736927140635
iteration : 668
train acc:  0.703125
train loss:  0.5862762928009033
train gradient:  0.16178155289360163
iteration : 669
train acc:  0.640625
train loss:  0.6138143539428711
train gradient:  0.16943789801153086
iteration : 670
train acc:  0.6484375
train loss:  0.627501368522644
train gradient:  0.1733311713011854
iteration : 671
train acc:  0.640625
train loss:  0.5940138101577759
train gradient:  0.18177668328087299
iteration : 672
train acc:  0.640625
train loss:  0.6174887418746948
train gradient:  0.22142695639517365
iteration : 673
train acc:  0.7265625
train loss:  0.5822843313217163
train gradient:  0.13312781810577135
iteration : 674
train acc:  0.640625
train loss:  0.6357641816139221
train gradient:  0.2047656153639629
iteration : 675
train acc:  0.6875
train loss:  0.560547947883606
train gradient:  0.15957667638467932
iteration : 676
train acc:  0.5859375
train loss:  0.6272304654121399
train gradient:  0.2377507221645158
iteration : 677
train acc:  0.671875
train loss:  0.6037642359733582
train gradient:  0.13428323421534816
iteration : 678
train acc:  0.7421875
train loss:  0.5630061030387878
train gradient:  0.1380984038437182
iteration : 679
train acc:  0.6484375
train loss:  0.6183894872665405
train gradient:  0.1962605962385548
iteration : 680
train acc:  0.671875
train loss:  0.6004837155342102
train gradient:  0.16850030949835523
iteration : 681
train acc:  0.6796875
train loss:  0.530912458896637
train gradient:  0.13183794856663716
iteration : 682
train acc:  0.6875
train loss:  0.5864713788032532
train gradient:  0.16654358283100887
iteration : 683
train acc:  0.6875
train loss:  0.5759823322296143
train gradient:  0.11542433022600143
iteration : 684
train acc:  0.703125
train loss:  0.5488506555557251
train gradient:  0.2220239362782827
iteration : 685
train acc:  0.6640625
train loss:  0.6011756062507629
train gradient:  0.16134322567039106
iteration : 686
train acc:  0.7265625
train loss:  0.5581334233283997
train gradient:  0.28276254447090116
iteration : 687
train acc:  0.609375
train loss:  0.6454730033874512
train gradient:  0.18587530350347395
iteration : 688
train acc:  0.625
train loss:  0.6542127132415771
train gradient:  0.1863317642577415
iteration : 689
train acc:  0.7265625
train loss:  0.5369096398353577
train gradient:  0.14747551400350048
iteration : 690
train acc:  0.59375
train loss:  0.6445580720901489
train gradient:  0.18259819405538041
iteration : 691
train acc:  0.6484375
train loss:  0.6120234131813049
train gradient:  0.1491043101785547
iteration : 692
train acc:  0.65625
train loss:  0.5969030857086182
train gradient:  0.1675459182811895
iteration : 693
train acc:  0.7109375
train loss:  0.5385982394218445
train gradient:  0.14966286409507093
iteration : 694
train acc:  0.6953125
train loss:  0.576715350151062
train gradient:  0.14667232586122664
iteration : 695
train acc:  0.6796875
train loss:  0.5981834530830383
train gradient:  0.23588039772850816
iteration : 696
train acc:  0.7265625
train loss:  0.5391314029693604
train gradient:  0.13121254551299358
iteration : 697
train acc:  0.6875
train loss:  0.5851186513900757
train gradient:  0.1979439201720029
iteration : 698
train acc:  0.6875
train loss:  0.5943425893783569
train gradient:  0.1289954451089794
iteration : 699
train acc:  0.75
train loss:  0.5226189494132996
train gradient:  0.17546581195806576
iteration : 700
train acc:  0.7421875
train loss:  0.5206410884857178
train gradient:  0.13020127158322886
iteration : 701
train acc:  0.7109375
train loss:  0.5429786443710327
train gradient:  0.14615417204128373
iteration : 702
train acc:  0.6640625
train loss:  0.5903308391571045
train gradient:  0.18383710128958797
iteration : 703
train acc:  0.703125
train loss:  0.5440724492073059
train gradient:  0.16434870348659975
iteration : 704
train acc:  0.6875
train loss:  0.5760784149169922
train gradient:  0.15252189627207116
iteration : 705
train acc:  0.6328125
train loss:  0.6488585472106934
train gradient:  0.21982449864830111
iteration : 706
train acc:  0.65625
train loss:  0.6063414812088013
train gradient:  0.1535393787055697
iteration : 707
train acc:  0.671875
train loss:  0.5728249549865723
train gradient:  0.19104142966256044
iteration : 708
train acc:  0.7578125
train loss:  0.5428529977798462
train gradient:  0.19716471790400253
iteration : 709
train acc:  0.71875
train loss:  0.5913448333740234
train gradient:  0.22902881312442652
iteration : 710
train acc:  0.75
train loss:  0.5441008806228638
train gradient:  0.16478852377291645
iteration : 711
train acc:  0.734375
train loss:  0.5333316326141357
train gradient:  0.13060123677811064
iteration : 712
train acc:  0.6875
train loss:  0.5772303342819214
train gradient:  0.15673602148001042
iteration : 713
train acc:  0.609375
train loss:  0.6510719060897827
train gradient:  0.2519652733797843
iteration : 714
train acc:  0.703125
train loss:  0.5503730773925781
train gradient:  0.13355668573445123
iteration : 715
train acc:  0.6015625
train loss:  0.6203158497810364
train gradient:  0.208651790626918
iteration : 716
train acc:  0.6640625
train loss:  0.572788417339325
train gradient:  0.12768955951831726
iteration : 717
train acc:  0.7109375
train loss:  0.5389546155929565
train gradient:  0.1952834767967974
iteration : 718
train acc:  0.6328125
train loss:  0.6062912940979004
train gradient:  0.18795582432321423
iteration : 719
train acc:  0.6953125
train loss:  0.5816940069198608
train gradient:  0.1776163997228209
iteration : 720
train acc:  0.6875
train loss:  0.6220971345901489
train gradient:  0.21305273744037423
iteration : 721
train acc:  0.7109375
train loss:  0.5553309917449951
train gradient:  0.14749027026670297
iteration : 722
train acc:  0.6171875
train loss:  0.6178174018859863
train gradient:  0.18817582616249762
iteration : 723
train acc:  0.640625
train loss:  0.5929638743400574
train gradient:  0.1727487372677191
iteration : 724
train acc:  0.6484375
train loss:  0.6265631914138794
train gradient:  0.3357806135183749
iteration : 725
train acc:  0.6953125
train loss:  0.562307596206665
train gradient:  0.12370845303706211
iteration : 726
train acc:  0.6953125
train loss:  0.607674241065979
train gradient:  0.27637596115332325
iteration : 727
train acc:  0.671875
train loss:  0.580095112323761
train gradient:  0.1372245812143736
iteration : 728
train acc:  0.6875
train loss:  0.5993071794509888
train gradient:  0.2384570025848397
iteration : 729
train acc:  0.6640625
train loss:  0.5781055688858032
train gradient:  0.17222973064721903
iteration : 730
train acc:  0.6640625
train loss:  0.5560562014579773
train gradient:  0.1483963017514749
iteration : 731
train acc:  0.6171875
train loss:  0.5931812524795532
train gradient:  0.18160488536866087
iteration : 732
train acc:  0.671875
train loss:  0.5890090465545654
train gradient:  0.15276805419150394
iteration : 733
train acc:  0.7578125
train loss:  0.5266017317771912
train gradient:  0.14920768739359752
iteration : 734
train acc:  0.65625
train loss:  0.5767912864685059
train gradient:  0.18210649022743808
iteration : 735
train acc:  0.6875
train loss:  0.5881130695343018
train gradient:  0.17998793454856993
iteration : 736
train acc:  0.671875
train loss:  0.5846655368804932
train gradient:  0.22315686431819565
iteration : 737
train acc:  0.6953125
train loss:  0.5781819820404053
train gradient:  0.2169786413126663
iteration : 738
train acc:  0.6953125
train loss:  0.6190516352653503
train gradient:  0.1703306232099898
iteration : 739
train acc:  0.7109375
train loss:  0.5642367601394653
train gradient:  0.15375006761538362
iteration : 740
train acc:  0.6484375
train loss:  0.6136586666107178
train gradient:  0.18283323831247333
iteration : 741
train acc:  0.71875
train loss:  0.5241352915763855
train gradient:  0.12626868488109977
iteration : 742
train acc:  0.6796875
train loss:  0.6085993051528931
train gradient:  0.1537071480239082
iteration : 743
train acc:  0.703125
train loss:  0.5752205848693848
train gradient:  0.23985188990125683
iteration : 744
train acc:  0.625
train loss:  0.6250109076499939
train gradient:  0.1709192298147611
iteration : 745
train acc:  0.7109375
train loss:  0.572494387626648
train gradient:  0.14126026579939685
iteration : 746
train acc:  0.671875
train loss:  0.6257728338241577
train gradient:  0.18204783558801965
iteration : 747
train acc:  0.703125
train loss:  0.5081874132156372
train gradient:  0.12578158181585855
iteration : 748
train acc:  0.65625
train loss:  0.5917221307754517
train gradient:  0.2729071485322243
iteration : 749
train acc:  0.6875
train loss:  0.567318856716156
train gradient:  0.14902992532186862
iteration : 750
train acc:  0.75
train loss:  0.5150752067565918
train gradient:  0.13267633480511767
iteration : 751
train acc:  0.734375
train loss:  0.5534460544586182
train gradient:  0.12741755821655776
iteration : 752
train acc:  0.6953125
train loss:  0.5641161203384399
train gradient:  0.15435973100085082
iteration : 753
train acc:  0.7578125
train loss:  0.5185132026672363
train gradient:  0.1377143982193299
iteration : 754
train acc:  0.65625
train loss:  0.6096857786178589
train gradient:  0.14699554721361324
iteration : 755
train acc:  0.625
train loss:  0.6033776998519897
train gradient:  0.18891909361290588
iteration : 756
train acc:  0.734375
train loss:  0.5595933198928833
train gradient:  0.12695168124928574
iteration : 757
train acc:  0.6953125
train loss:  0.5709719657897949
train gradient:  0.14988752271440495
iteration : 758
train acc:  0.6953125
train loss:  0.5871538519859314
train gradient:  0.177620321510139
iteration : 759
train acc:  0.71875
train loss:  0.5539839267730713
train gradient:  0.1457408239013961
iteration : 760
train acc:  0.703125
train loss:  0.5893793106079102
train gradient:  0.16762073695162055
iteration : 761
train acc:  0.6875
train loss:  0.5460022687911987
train gradient:  0.1760666975147281
iteration : 762
train acc:  0.671875
train loss:  0.5937419533729553
train gradient:  0.20891382135643616
iteration : 763
train acc:  0.703125
train loss:  0.5375648140907288
train gradient:  0.14987622799809405
iteration : 764
train acc:  0.6171875
train loss:  0.5939362049102783
train gradient:  0.18982283370516098
iteration : 765
train acc:  0.6484375
train loss:  0.5827025175094604
train gradient:  0.2439438012201851
iteration : 766
train acc:  0.7109375
train loss:  0.5618322491645813
train gradient:  0.14711945515376285
iteration : 767
train acc:  0.65625
train loss:  0.5724049806594849
train gradient:  0.17977040333132238
iteration : 768
train acc:  0.6640625
train loss:  0.5878586173057556
train gradient:  0.22072408750811678
iteration : 769
train acc:  0.6953125
train loss:  0.5822440385818481
train gradient:  0.1407275738805537
iteration : 770
train acc:  0.6796875
train loss:  0.5729180574417114
train gradient:  0.18301822928682063
iteration : 771
train acc:  0.6875
train loss:  0.5913674831390381
train gradient:  0.15019848957217757
iteration : 772
train acc:  0.6875
train loss:  0.5970073938369751
train gradient:  0.16097428209912085
iteration : 773
train acc:  0.578125
train loss:  0.6568859815597534
train gradient:  0.2542070709139303
iteration : 774
train acc:  0.6875
train loss:  0.645988404750824
train gradient:  0.1990569963338665
iteration : 775
train acc:  0.6875
train loss:  0.5537606477737427
train gradient:  0.131832459114266
iteration : 776
train acc:  0.6953125
train loss:  0.6084197759628296
train gradient:  0.16571002038428523
iteration : 777
train acc:  0.65625
train loss:  0.5485475659370422
train gradient:  0.1586621283866695
iteration : 778
train acc:  0.734375
train loss:  0.5214431285858154
train gradient:  0.15227310456184517
iteration : 779
train acc:  0.625
train loss:  0.6199218034744263
train gradient:  0.16917383368894198
iteration : 780
train acc:  0.6875
train loss:  0.5426440238952637
train gradient:  0.24209834050956403
iteration : 781
train acc:  0.6875
train loss:  0.574587345123291
train gradient:  0.15988984746555313
iteration : 782
train acc:  0.6953125
train loss:  0.5673624277114868
train gradient:  0.21770328652172505
iteration : 783
train acc:  0.6015625
train loss:  0.6310368776321411
train gradient:  0.1778389755431546
iteration : 784
train acc:  0.6328125
train loss:  0.5618895888328552
train gradient:  0.1387053161065579
iteration : 785
train acc:  0.6796875
train loss:  0.6302695274353027
train gradient:  0.2528213440041546
iteration : 786
train acc:  0.71875
train loss:  0.5794111490249634
train gradient:  0.17026459609103306
iteration : 787
train acc:  0.7109375
train loss:  0.5781607627868652
train gradient:  0.1667461351205059
iteration : 788
train acc:  0.7265625
train loss:  0.5306261777877808
train gradient:  0.13888487503171648
iteration : 789
train acc:  0.65625
train loss:  0.641265869140625
train gradient:  0.20290364469727246
iteration : 790
train acc:  0.7109375
train loss:  0.5380182862281799
train gradient:  0.18645175865025024
iteration : 791
train acc:  0.6640625
train loss:  0.5966806411743164
train gradient:  0.2145491440030331
iteration : 792
train acc:  0.6796875
train loss:  0.6173276305198669
train gradient:  0.15587587819933907
iteration : 793
train acc:  0.703125
train loss:  0.5555601119995117
train gradient:  0.16975349264401907
iteration : 794
train acc:  0.6484375
train loss:  0.5769224166870117
train gradient:  0.18202452939255842
iteration : 795
train acc:  0.6953125
train loss:  0.5821648836135864
train gradient:  0.1796098273203619
iteration : 796
train acc:  0.671875
train loss:  0.587519645690918
train gradient:  0.19853245902579003
iteration : 797
train acc:  0.6796875
train loss:  0.5508642792701721
train gradient:  0.16849812707791245
iteration : 798
train acc:  0.6875
train loss:  0.6188520193099976
train gradient:  0.18667301819479867
iteration : 799
train acc:  0.7109375
train loss:  0.5385688543319702
train gradient:  0.16589537794288756
iteration : 800
train acc:  0.65625
train loss:  0.5934762954711914
train gradient:  0.20790749075809148
iteration : 801
train acc:  0.640625
train loss:  0.5614314079284668
train gradient:  0.2173629885633124
iteration : 802
train acc:  0.640625
train loss:  0.590461254119873
train gradient:  0.1729887149914126
iteration : 803
train acc:  0.625
train loss:  0.6351446509361267
train gradient:  0.17109297675866036
iteration : 804
train acc:  0.65625
train loss:  0.5954177379608154
train gradient:  0.11649178241623324
iteration : 805
train acc:  0.6953125
train loss:  0.5805968642234802
train gradient:  0.18990753076278533
iteration : 806
train acc:  0.6640625
train loss:  0.6114646792411804
train gradient:  0.1571449792879388
iteration : 807
train acc:  0.609375
train loss:  0.6132872700691223
train gradient:  0.1807511130518349
iteration : 808
train acc:  0.625
train loss:  0.5758227109909058
train gradient:  0.23620943988763993
iteration : 809
train acc:  0.703125
train loss:  0.5574196577072144
train gradient:  0.21540622264473464
iteration : 810
train acc:  0.7265625
train loss:  0.5724261403083801
train gradient:  0.17713435757994084
iteration : 811
train acc:  0.7421875
train loss:  0.535179615020752
train gradient:  0.1650819256931318
iteration : 812
train acc:  0.7421875
train loss:  0.5019541382789612
train gradient:  0.12493540938337325
iteration : 813
train acc:  0.6328125
train loss:  0.6049215197563171
train gradient:  0.18478601349551993
iteration : 814
train acc:  0.6875
train loss:  0.6092526912689209
train gradient:  0.2077400197265256
iteration : 815
train acc:  0.671875
train loss:  0.5756968855857849
train gradient:  0.1391168631736306
iteration : 816
train acc:  0.65625
train loss:  0.5996216535568237
train gradient:  0.21628028212201217
iteration : 817
train acc:  0.6484375
train loss:  0.6104425191879272
train gradient:  0.1720265932718955
iteration : 818
train acc:  0.703125
train loss:  0.5457307696342468
train gradient:  0.12543205775847943
iteration : 819
train acc:  0.671875
train loss:  0.5741540193557739
train gradient:  0.16978856450579644
iteration : 820
train acc:  0.734375
train loss:  0.5224555730819702
train gradient:  0.11983709251456665
iteration : 821
train acc:  0.7421875
train loss:  0.5264750123023987
train gradient:  0.12953045631518992
iteration : 822
train acc:  0.671875
train loss:  0.5729715824127197
train gradient:  0.17739076805705006
iteration : 823
train acc:  0.625
train loss:  0.6151454448699951
train gradient:  0.21821153325915293
iteration : 824
train acc:  0.6796875
train loss:  0.5689237117767334
train gradient:  0.1759700650648729
iteration : 825
train acc:  0.6640625
train loss:  0.6018558740615845
train gradient:  0.1799695248432255
iteration : 826
train acc:  0.6875
train loss:  0.5741092562675476
train gradient:  0.19810506446159493
iteration : 827
train acc:  0.703125
train loss:  0.5431656837463379
train gradient:  0.15299885190080662
iteration : 828
train acc:  0.640625
train loss:  0.5945747494697571
train gradient:  0.13267005854730335
iteration : 829
train acc:  0.671875
train loss:  0.5871217846870422
train gradient:  0.2384501576561342
iteration : 830
train acc:  0.703125
train loss:  0.5463277101516724
train gradient:  0.12110315875913726
iteration : 831
train acc:  0.71875
train loss:  0.49886998534202576
train gradient:  0.15196166142564171
iteration : 832
train acc:  0.640625
train loss:  0.6124488115310669
train gradient:  0.19549319633107484
iteration : 833
train acc:  0.640625
train loss:  0.6488205194473267
train gradient:  0.24041069743709698
iteration : 834
train acc:  0.7109375
train loss:  0.5382989645004272
train gradient:  0.15591961481246952
iteration : 835
train acc:  0.6875
train loss:  0.5499835014343262
train gradient:  0.14661710266858982
iteration : 836
train acc:  0.6875
train loss:  0.5581696629524231
train gradient:  0.13019917803448391
iteration : 837
train acc:  0.59375
train loss:  0.6490398645401001
train gradient:  0.2131815728263959
iteration : 838
train acc:  0.671875
train loss:  0.5839534997940063
train gradient:  0.18119962102832404
iteration : 839
train acc:  0.71875
train loss:  0.5435569286346436
train gradient:  0.12367421695538354
iteration : 840
train acc:  0.6640625
train loss:  0.6154154539108276
train gradient:  0.15934991050113784
iteration : 841
train acc:  0.6953125
train loss:  0.5760396718978882
train gradient:  0.16285913049044942
iteration : 842
train acc:  0.6328125
train loss:  0.6359531879425049
train gradient:  0.28779205836113975
iteration : 843
train acc:  0.7421875
train loss:  0.5329101085662842
train gradient:  0.11606734194226576
iteration : 844
train acc:  0.7421875
train loss:  0.547453761100769
train gradient:  0.13376231439057673
iteration : 845
train acc:  0.6640625
train loss:  0.592575192451477
train gradient:  0.2920347372082818
iteration : 846
train acc:  0.6640625
train loss:  0.5585184693336487
train gradient:  0.14983285349825587
iteration : 847
train acc:  0.6328125
train loss:  0.6038237810134888
train gradient:  0.1922923849165341
iteration : 848
train acc:  0.703125
train loss:  0.5615599751472473
train gradient:  0.1503892652905073
iteration : 849
train acc:  0.703125
train loss:  0.5707692503929138
train gradient:  0.1830309017176608
iteration : 850
train acc:  0.734375
train loss:  0.5523428320884705
train gradient:  0.17433735678035478
iteration : 851
train acc:  0.71875
train loss:  0.5737506747245789
train gradient:  0.21454357920175804
iteration : 852
train acc:  0.6953125
train loss:  0.5478429198265076
train gradient:  0.1462540075507246
iteration : 853
train acc:  0.6953125
train loss:  0.5715336799621582
train gradient:  0.15301259124860972
iteration : 854
train acc:  0.7265625
train loss:  0.5275494456291199
train gradient:  0.12035688458933938
iteration : 855
train acc:  0.65625
train loss:  0.6302093863487244
train gradient:  0.1512534819694482
iteration : 856
train acc:  0.65625
train loss:  0.587905764579773
train gradient:  0.18103557865034792
iteration : 857
train acc:  0.6484375
train loss:  0.5805492401123047
train gradient:  0.1530963760449105
iteration : 858
train acc:  0.6484375
train loss:  0.6034163236618042
train gradient:  0.18136724454412392
iteration : 859
train acc:  0.6484375
train loss:  0.6116522550582886
train gradient:  0.2011215811893911
iteration : 860
train acc:  0.6875
train loss:  0.606567919254303
train gradient:  0.2068394229195712
iteration : 861
train acc:  0.734375
train loss:  0.5919927358627319
train gradient:  0.2602026193207134
iteration : 862
train acc:  0.734375
train loss:  0.5469631552696228
train gradient:  0.18434293699410564
iteration : 863
train acc:  0.6015625
train loss:  0.6433495283126831
train gradient:  0.21795760329350666
iteration : 864
train acc:  0.71875
train loss:  0.5511447787284851
train gradient:  0.1518276403845976
iteration : 865
train acc:  0.703125
train loss:  0.5632239580154419
train gradient:  0.14085008747920444
iteration : 866
train acc:  0.65625
train loss:  0.6031861901283264
train gradient:  0.17401236494004357
iteration : 867
train acc:  0.6328125
train loss:  0.6065796613693237
train gradient:  0.17099549336127762
iteration : 868
train acc:  0.7109375
train loss:  0.5439609289169312
train gradient:  0.14051414934148507
iteration : 869
train acc:  0.71875
train loss:  0.5687914490699768
train gradient:  0.18620333922604274
iteration : 870
train acc:  0.65625
train loss:  0.6172699928283691
train gradient:  0.2026268736546924
iteration : 871
train acc:  0.671875
train loss:  0.598604142665863
train gradient:  0.19185579313606557
iteration : 872
train acc:  0.7734375
train loss:  0.5153512954711914
train gradient:  0.22177938813984602
iteration : 873
train acc:  0.703125
train loss:  0.5891620516777039
train gradient:  0.14614006912762756
iteration : 874
train acc:  0.6875
train loss:  0.5832366347312927
train gradient:  0.21080729619325353
iteration : 875
train acc:  0.7265625
train loss:  0.5409289598464966
train gradient:  0.15452027639926183
iteration : 876
train acc:  0.65625
train loss:  0.640234112739563
train gradient:  0.3383356667822547
iteration : 877
train acc:  0.765625
train loss:  0.5046625733375549
train gradient:  0.1336047115406204
iteration : 878
train acc:  0.75
train loss:  0.5325664281845093
train gradient:  0.19287292447232973
iteration : 879
train acc:  0.625
train loss:  0.629363477230072
train gradient:  0.30675927291012206
iteration : 880
train acc:  0.6796875
train loss:  0.5752490758895874
train gradient:  0.18958091345688727
iteration : 881
train acc:  0.7109375
train loss:  0.5541771054267883
train gradient:  0.14186303569815908
iteration : 882
train acc:  0.65625
train loss:  0.6200966835021973
train gradient:  0.21038781309203314
iteration : 883
train acc:  0.6953125
train loss:  0.6210756301879883
train gradient:  0.1894242515199263
iteration : 884
train acc:  0.6796875
train loss:  0.5729498863220215
train gradient:  0.17737428922110204
iteration : 885
train acc:  0.6171875
train loss:  0.6302285194396973
train gradient:  0.2940723163986546
iteration : 886
train acc:  0.6796875
train loss:  0.5496990084648132
train gradient:  0.14720335330868278
iteration : 887
train acc:  0.734375
train loss:  0.5322568416595459
train gradient:  0.14654800003081841
iteration : 888
train acc:  0.6640625
train loss:  0.627363920211792
train gradient:  0.2362220168142563
iteration : 889
train acc:  0.640625
train loss:  0.6089872121810913
train gradient:  0.20709642547037432
iteration : 890
train acc:  0.703125
train loss:  0.5950411558151245
train gradient:  0.16782837180279553
iteration : 891
train acc:  0.6875
train loss:  0.5739223957061768
train gradient:  0.12508935411194766
iteration : 892
train acc:  0.828125
train loss:  0.45821166038513184
train gradient:  0.13156191853690372
iteration : 893
train acc:  0.671875
train loss:  0.6056612730026245
train gradient:  0.1845757457079264
iteration : 894
train acc:  0.7265625
train loss:  0.5347465872764587
train gradient:  0.1422884746312459
iteration : 895
train acc:  0.6875
train loss:  0.5869075059890747
train gradient:  0.12415302895252676
iteration : 896
train acc:  0.6953125
train loss:  0.5775101780891418
train gradient:  0.201394894290852
iteration : 897
train acc:  0.6796875
train loss:  0.5754771828651428
train gradient:  0.15818280279794483
iteration : 898
train acc:  0.765625
train loss:  0.522441029548645
train gradient:  0.17406632498205454
iteration : 899
train acc:  0.734375
train loss:  0.5140972137451172
train gradient:  0.14760165896991018
iteration : 900
train acc:  0.7109375
train loss:  0.575261652469635
train gradient:  0.11985157458227255
iteration : 901
train acc:  0.640625
train loss:  0.5919510126113892
train gradient:  0.14427029952791415
iteration : 902
train acc:  0.6640625
train loss:  0.6210082769393921
train gradient:  0.1991350862136549
iteration : 903
train acc:  0.71875
train loss:  0.5952051877975464
train gradient:  0.20644188982760067
iteration : 904
train acc:  0.78125
train loss:  0.5015509724617004
train gradient:  0.16341174690060806
iteration : 905
train acc:  0.7109375
train loss:  0.574694812297821
train gradient:  0.19342510402557123
iteration : 906
train acc:  0.6953125
train loss:  0.5802791118621826
train gradient:  0.1503766186053928
iteration : 907
train acc:  0.6796875
train loss:  0.6089701652526855
train gradient:  0.19143113438105527
iteration : 908
train acc:  0.6796875
train loss:  0.5820177793502808
train gradient:  0.15973733052881517
iteration : 909
train acc:  0.6953125
train loss:  0.5702320337295532
train gradient:  0.1519303418760076
iteration : 910
train acc:  0.671875
train loss:  0.5745939612388611
train gradient:  0.14203955640021132
iteration : 911
train acc:  0.765625
train loss:  0.5379093885421753
train gradient:  0.1594058512072378
iteration : 912
train acc:  0.625
train loss:  0.6026350259780884
train gradient:  0.1401179929037401
iteration : 913
train acc:  0.671875
train loss:  0.6130962371826172
train gradient:  0.28846314994219896
iteration : 914
train acc:  0.671875
train loss:  0.5796717405319214
train gradient:  0.21335235465361818
iteration : 915
train acc:  0.7578125
train loss:  0.507154107093811
train gradient:  0.146241597005013
iteration : 916
train acc:  0.7109375
train loss:  0.5283263325691223
train gradient:  0.2628794212910446
iteration : 917
train acc:  0.671875
train loss:  0.5883210897445679
train gradient:  0.15895931718187667
iteration : 918
train acc:  0.6953125
train loss:  0.5464451313018799
train gradient:  0.13724184657615687
iteration : 919
train acc:  0.671875
train loss:  0.6123816967010498
train gradient:  0.1690440734476859
iteration : 920
train acc:  0.703125
train loss:  0.58733069896698
train gradient:  0.19178873709131228
iteration : 921
train acc:  0.671875
train loss:  0.5868205428123474
train gradient:  0.17293898035027988
iteration : 922
train acc:  0.7109375
train loss:  0.5888634920120239
train gradient:  0.17458931309806125
iteration : 923
train acc:  0.71875
train loss:  0.5436848402023315
train gradient:  0.11802175943456565
iteration : 924
train acc:  0.671875
train loss:  0.5589607954025269
train gradient:  0.13390965925082815
iteration : 925
train acc:  0.6171875
train loss:  0.6145657896995544
train gradient:  0.15412447450468147
iteration : 926
train acc:  0.671875
train loss:  0.549202561378479
train gradient:  0.15517047449039217
iteration : 927
train acc:  0.609375
train loss:  0.591063380241394
train gradient:  0.13948928974366204
iteration : 928
train acc:  0.7109375
train loss:  0.5722420811653137
train gradient:  0.13729630185797215
iteration : 929
train acc:  0.625
train loss:  0.5824317932128906
train gradient:  0.1703196858577729
iteration : 930
train acc:  0.7421875
train loss:  0.5372977256774902
train gradient:  0.16025243608325243
iteration : 931
train acc:  0.6484375
train loss:  0.5968664884567261
train gradient:  0.1693255735875479
iteration : 932
train acc:  0.6796875
train loss:  0.5842910408973694
train gradient:  0.17582837566786394
iteration : 933
train acc:  0.6640625
train loss:  0.6037564277648926
train gradient:  0.21463906890898793
iteration : 934
train acc:  0.75
train loss:  0.5240857005119324
train gradient:  0.1513437356163466
iteration : 935
train acc:  0.65625
train loss:  0.6038936972618103
train gradient:  0.1917127468619138
iteration : 936
train acc:  0.734375
train loss:  0.5280094146728516
train gradient:  0.1576607770344531
iteration : 937
train acc:  0.703125
train loss:  0.5420535206794739
train gradient:  0.19951480600157395
iteration : 938
train acc:  0.6796875
train loss:  0.5623512268066406
train gradient:  0.16960240825809064
iteration : 939
train acc:  0.75
train loss:  0.5381426811218262
train gradient:  0.13802602874367415
iteration : 940
train acc:  0.671875
train loss:  0.5603688955307007
train gradient:  0.15735619812159113
iteration : 941
train acc:  0.5859375
train loss:  0.637729287147522
train gradient:  0.19130691648136441
iteration : 942
train acc:  0.65625
train loss:  0.5886804461479187
train gradient:  0.1641069648100406
iteration : 943
train acc:  0.671875
train loss:  0.5840415954589844
train gradient:  0.1944127616785921
iteration : 944
train acc:  0.6796875
train loss:  0.5943905115127563
train gradient:  0.25013562770173936
iteration : 945
train acc:  0.7578125
train loss:  0.5285732746124268
train gradient:  0.14462643515916623
iteration : 946
train acc:  0.703125
train loss:  0.5778063535690308
train gradient:  0.1927597258362224
iteration : 947
train acc:  0.71875
train loss:  0.518994927406311
train gradient:  0.13778337898947235
iteration : 948
train acc:  0.7109375
train loss:  0.5560377240180969
train gradient:  0.15459144032732658
iteration : 949
train acc:  0.625
train loss:  0.6291528940200806
train gradient:  0.16566520465048837
iteration : 950
train acc:  0.7109375
train loss:  0.5787979960441589
train gradient:  0.1886358365896964
iteration : 951
train acc:  0.671875
train loss:  0.5652744770050049
train gradient:  0.16899448188804378
iteration : 952
train acc:  0.6484375
train loss:  0.5851061940193176
train gradient:  0.1806566605076345
iteration : 953
train acc:  0.7109375
train loss:  0.5428165793418884
train gradient:  0.20804355848046413
iteration : 954
train acc:  0.609375
train loss:  0.6570618152618408
train gradient:  0.210163078447677
iteration : 955
train acc:  0.625
train loss:  0.586943507194519
train gradient:  0.2227316226972902
iteration : 956
train acc:  0.6796875
train loss:  0.573520302772522
train gradient:  0.15084486821905768
iteration : 957
train acc:  0.765625
train loss:  0.5421614646911621
train gradient:  0.13024348703259936
iteration : 958
train acc:  0.6796875
train loss:  0.5739530920982361
train gradient:  0.17895768332248616
iteration : 959
train acc:  0.671875
train loss:  0.6082611680030823
train gradient:  0.3454742379281967
iteration : 960
train acc:  0.6796875
train loss:  0.6256992816925049
train gradient:  0.15461871236728444
iteration : 961
train acc:  0.7890625
train loss:  0.5008807182312012
train gradient:  0.15322850251221615
iteration : 962
train acc:  0.671875
train loss:  0.5976247787475586
train gradient:  0.2454366752778035
iteration : 963
train acc:  0.6796875
train loss:  0.5800405740737915
train gradient:  0.15969574968547964
iteration : 964
train acc:  0.671875
train loss:  0.606162965297699
train gradient:  0.1746326414991483
iteration : 965
train acc:  0.671875
train loss:  0.6004084348678589
train gradient:  0.14383935107782397
iteration : 966
train acc:  0.6953125
train loss:  0.5606215596199036
train gradient:  0.13812978582820185
iteration : 967
train acc:  0.671875
train loss:  0.5487954616546631
train gradient:  0.22015095134270246
iteration : 968
train acc:  0.703125
train loss:  0.5385710597038269
train gradient:  0.14914059271734295
iteration : 969
train acc:  0.6796875
train loss:  0.5456221103668213
train gradient:  0.1651342722838304
iteration : 970
train acc:  0.7265625
train loss:  0.5291534066200256
train gradient:  0.11725027431836997
iteration : 971
train acc:  0.6640625
train loss:  0.590847373008728
train gradient:  0.2452447512695528
iteration : 972
train acc:  0.71875
train loss:  0.5430722832679749
train gradient:  0.1350299131334851
iteration : 973
train acc:  0.640625
train loss:  0.5617973804473877
train gradient:  0.1973773818751381
iteration : 974
train acc:  0.609375
train loss:  0.588543176651001
train gradient:  0.2154812640039267
iteration : 975
train acc:  0.703125
train loss:  0.5443158745765686
train gradient:  0.132387170100321
iteration : 976
train acc:  0.703125
train loss:  0.5711572766304016
train gradient:  0.1974154011620619
iteration : 977
train acc:  0.6640625
train loss:  0.5987541079521179
train gradient:  0.15904828172596114
iteration : 978
train acc:  0.671875
train loss:  0.5764183402061462
train gradient:  0.2055760272015948
iteration : 979
train acc:  0.734375
train loss:  0.5205677151679993
train gradient:  0.14200823646476846
iteration : 980
train acc:  0.75
train loss:  0.5250006318092346
train gradient:  0.15386595226371905
iteration : 981
train acc:  0.71875
train loss:  0.5498237609863281
train gradient:  0.1386158965773931
iteration : 982
train acc:  0.6796875
train loss:  0.5805089473724365
train gradient:  0.1725084119828166
iteration : 983
train acc:  0.65625
train loss:  0.5592913627624512
train gradient:  0.18733833307004374
iteration : 984
train acc:  0.7265625
train loss:  0.5196284055709839
train gradient:  0.1446268556086741
iteration : 985
train acc:  0.734375
train loss:  0.5243754386901855
train gradient:  0.13340040971965506
iteration : 986
train acc:  0.734375
train loss:  0.5423941612243652
train gradient:  0.19212970699113585
iteration : 987
train acc:  0.7578125
train loss:  0.5121437311172485
train gradient:  0.11915335889142019
iteration : 988
train acc:  0.7109375
train loss:  0.5627936124801636
train gradient:  0.1841527132871876
iteration : 989
train acc:  0.6953125
train loss:  0.5603165626525879
train gradient:  0.18737120055221534
iteration : 990
train acc:  0.765625
train loss:  0.5095142126083374
train gradient:  0.1476290784128362
iteration : 991
train acc:  0.71875
train loss:  0.5252912044525146
train gradient:  0.21292594331515546
iteration : 992
train acc:  0.71875
train loss:  0.5534673929214478
train gradient:  0.147950981847298
iteration : 993
train acc:  0.640625
train loss:  0.5811977386474609
train gradient:  0.203539065409368
iteration : 994
train acc:  0.703125
train loss:  0.5663052797317505
train gradient:  0.18889226923610308
iteration : 995
train acc:  0.7265625
train loss:  0.564598560333252
train gradient:  0.15877117001314064
iteration : 996
train acc:  0.640625
train loss:  0.6119211912155151
train gradient:  0.1874113358405325
iteration : 997
train acc:  0.6484375
train loss:  0.5959154963493347
train gradient:  0.2539818161843661
iteration : 998
train acc:  0.703125
train loss:  0.550715446472168
train gradient:  0.17753986745245
iteration : 999
train acc:  0.6640625
train loss:  0.5704640746116638
train gradient:  0.18467914424022816
iteration : 1000
train acc:  0.609375
train loss:  0.6408235430717468
train gradient:  0.24513473352775433
iteration : 1001
train acc:  0.6875
train loss:  0.5988784432411194
train gradient:  0.15455722390372995
iteration : 1002
train acc:  0.6328125
train loss:  0.5963761806488037
train gradient:  0.2070267322368658
iteration : 1003
train acc:  0.7421875
train loss:  0.5082414150238037
train gradient:  0.12215909338886649
iteration : 1004
train acc:  0.71875
train loss:  0.5861860513687134
train gradient:  0.22412382780311896
iteration : 1005
train acc:  0.8125
train loss:  0.484142005443573
train gradient:  0.14690805704739127
iteration : 1006
train acc:  0.75
train loss:  0.5111091136932373
train gradient:  0.12983979107105492
iteration : 1007
train acc:  0.6171875
train loss:  0.587717592716217
train gradient:  0.21117157021995472
iteration : 1008
train acc:  0.6640625
train loss:  0.5662256479263306
train gradient:  0.17096298126412862
iteration : 1009
train acc:  0.6953125
train loss:  0.557084321975708
train gradient:  0.17104924780380687
iteration : 1010
train acc:  0.671875
train loss:  0.5869303345680237
train gradient:  0.2055679639211947
iteration : 1011
train acc:  0.7578125
train loss:  0.5467989444732666
train gradient:  0.1951153224853065
iteration : 1012
train acc:  0.796875
train loss:  0.531378984451294
train gradient:  0.1682883545260316
iteration : 1013
train acc:  0.7109375
train loss:  0.5407199263572693
train gradient:  0.1330542447848705
iteration : 1014
train acc:  0.6015625
train loss:  0.6966698169708252
train gradient:  0.29045598272956874
iteration : 1015
train acc:  0.6796875
train loss:  0.583242654800415
train gradient:  0.20804135410826632
iteration : 1016
train acc:  0.7109375
train loss:  0.5671396255493164
train gradient:  0.16748502641671856
iteration : 1017
train acc:  0.65625
train loss:  0.613608181476593
train gradient:  0.23745497860805512
iteration : 1018
train acc:  0.6484375
train loss:  0.630898118019104
train gradient:  0.16279872121159594
iteration : 1019
train acc:  0.7265625
train loss:  0.5640499591827393
train gradient:  0.20107541138392507
iteration : 1020
train acc:  0.65625
train loss:  0.5797824859619141
train gradient:  0.22157152602911834
iteration : 1021
train acc:  0.6171875
train loss:  0.6223776936531067
train gradient:  0.2859728872657525
iteration : 1022
train acc:  0.6484375
train loss:  0.6008208990097046
train gradient:  0.17615485778200862
iteration : 1023
train acc:  0.7265625
train loss:  0.5476446151733398
train gradient:  0.16216914639566504
iteration : 1024
train acc:  0.671875
train loss:  0.5934958457946777
train gradient:  0.20452138449693255
iteration : 1025
train acc:  0.6953125
train loss:  0.5738940238952637
train gradient:  0.19512502351739386
iteration : 1026
train acc:  0.6875
train loss:  0.541499674320221
train gradient:  0.1661102601282476
iteration : 1027
train acc:  0.65625
train loss:  0.6096794605255127
train gradient:  0.2154053394240819
iteration : 1028
train acc:  0.6953125
train loss:  0.5555787682533264
train gradient:  0.16447621250673386
iteration : 1029
train acc:  0.65625
train loss:  0.5969297885894775
train gradient:  0.16554535460717243
iteration : 1030
train acc:  0.609375
train loss:  0.6518869400024414
train gradient:  0.28603335581496103
iteration : 1031
train acc:  0.7265625
train loss:  0.5340617895126343
train gradient:  0.14103595806857527
iteration : 1032
train acc:  0.6484375
train loss:  0.598427414894104
train gradient:  0.16691480702070824
iteration : 1033
train acc:  0.7265625
train loss:  0.5737686157226562
train gradient:  0.1442355890616557
iteration : 1034
train acc:  0.71875
train loss:  0.5262640714645386
train gradient:  0.13592316485615513
iteration : 1035
train acc:  0.7578125
train loss:  0.5206557512283325
train gradient:  0.15222904752877986
iteration : 1036
train acc:  0.6796875
train loss:  0.5484412908554077
train gradient:  0.14353081196101353
iteration : 1037
train acc:  0.6875
train loss:  0.561226487159729
train gradient:  0.1921860934320644
iteration : 1038
train acc:  0.734375
train loss:  0.5119452476501465
train gradient:  0.14963598124936245
iteration : 1039
train acc:  0.6640625
train loss:  0.5897030830383301
train gradient:  0.2044066049208898
iteration : 1040
train acc:  0.6953125
train loss:  0.5492885112762451
train gradient:  0.15236910767436446
iteration : 1041
train acc:  0.734375
train loss:  0.5530204772949219
train gradient:  0.14281835306515997
iteration : 1042
train acc:  0.734375
train loss:  0.5604667067527771
train gradient:  0.16389908407095677
iteration : 1043
train acc:  0.765625
train loss:  0.5314808487892151
train gradient:  0.20648820919993305
iteration : 1044
train acc:  0.6640625
train loss:  0.6257469058036804
train gradient:  0.23728085955878975
iteration : 1045
train acc:  0.5859375
train loss:  0.6640214920043945
train gradient:  0.2714369448584377
iteration : 1046
train acc:  0.6875
train loss:  0.5420970916748047
train gradient:  0.21054142557355504
iteration : 1047
train acc:  0.6953125
train loss:  0.5451286435127258
train gradient:  0.17814227038698288
iteration : 1048
train acc:  0.6796875
train loss:  0.5566939115524292
train gradient:  0.19097649084156948
iteration : 1049
train acc:  0.8046875
train loss:  0.4744373559951782
train gradient:  0.1543740287383023
iteration : 1050
train acc:  0.6875
train loss:  0.555862307548523
train gradient:  0.18731442736025677
iteration : 1051
train acc:  0.6953125
train loss:  0.5581530332565308
train gradient:  0.1267659095442607
iteration : 1052
train acc:  0.703125
train loss:  0.5513215661048889
train gradient:  0.16260357037726908
iteration : 1053
train acc:  0.7109375
train loss:  0.5492175221443176
train gradient:  0.15217416735446987
iteration : 1054
train acc:  0.6484375
train loss:  0.6207380890846252
train gradient:  0.24686688498294745
iteration : 1055
train acc:  0.7265625
train loss:  0.5171996355056763
train gradient:  0.11321226545358032
iteration : 1056
train acc:  0.703125
train loss:  0.5450758337974548
train gradient:  0.15622462243427565
iteration : 1057
train acc:  0.7265625
train loss:  0.540701150894165
train gradient:  0.12772604447765362
iteration : 1058
train acc:  0.65625
train loss:  0.6747584939002991
train gradient:  0.21806670425925742
iteration : 1059
train acc:  0.6953125
train loss:  0.5450479388237
train gradient:  0.2725079594396842
iteration : 1060
train acc:  0.703125
train loss:  0.5547462701797485
train gradient:  0.17627654963956696
iteration : 1061
train acc:  0.6796875
train loss:  0.5964452624320984
train gradient:  0.16163705842013215
iteration : 1062
train acc:  0.6875
train loss:  0.5664359331130981
train gradient:  0.28556208332224103
iteration : 1063
train acc:  0.703125
train loss:  0.5346285700798035
train gradient:  0.2141052753161205
iteration : 1064
train acc:  0.71875
train loss:  0.5389831066131592
train gradient:  0.18033185819914813
iteration : 1065
train acc:  0.6328125
train loss:  0.5893397927284241
train gradient:  0.17568789662996198
iteration : 1066
train acc:  0.7421875
train loss:  0.5049533843994141
train gradient:  0.12620724234546377
iteration : 1067
train acc:  0.6796875
train loss:  0.5857458114624023
train gradient:  0.20424065338268754
iteration : 1068
train acc:  0.7109375
train loss:  0.5267325639724731
train gradient:  0.136950595827736
iteration : 1069
train acc:  0.640625
train loss:  0.6421352624893188
train gradient:  0.2517019661775337
iteration : 1070
train acc:  0.6796875
train loss:  0.568939208984375
train gradient:  0.16785512184151158
iteration : 1071
train acc:  0.7734375
train loss:  0.5120577812194824
train gradient:  0.1572259269353783
iteration : 1072
train acc:  0.6875
train loss:  0.5451381206512451
train gradient:  0.1253571897649334
iteration : 1073
train acc:  0.6171875
train loss:  0.5979578495025635
train gradient:  0.148234647798132
iteration : 1074
train acc:  0.6796875
train loss:  0.5803529024124146
train gradient:  0.2975810926568379
iteration : 1075
train acc:  0.6328125
train loss:  0.6138988137245178
train gradient:  0.3026152453183687
iteration : 1076
train acc:  0.7109375
train loss:  0.571553647518158
train gradient:  0.18376449536045453
iteration : 1077
train acc:  0.7109375
train loss:  0.5430452823638916
train gradient:  0.13576165615356828
iteration : 1078
train acc:  0.671875
train loss:  0.6178414821624756
train gradient:  0.20230283469793497
iteration : 1079
train acc:  0.7421875
train loss:  0.5390410423278809
train gradient:  0.12719704738508147
iteration : 1080
train acc:  0.7109375
train loss:  0.5013341307640076
train gradient:  0.15619296472927174
iteration : 1081
train acc:  0.7265625
train loss:  0.5232860445976257
train gradient:  0.15491174030343047
iteration : 1082
train acc:  0.8203125
train loss:  0.4924502968788147
train gradient:  0.15422396664765137
iteration : 1083
train acc:  0.6328125
train loss:  0.6710848808288574
train gradient:  0.2672704470290527
iteration : 1084
train acc:  0.625
train loss:  0.6481777429580688
train gradient:  0.23577028443799297
iteration : 1085
train acc:  0.703125
train loss:  0.5333009362220764
train gradient:  0.25197510548132496
iteration : 1086
train acc:  0.71875
train loss:  0.5549935698509216
train gradient:  0.149455537915034
iteration : 1087
train acc:  0.6875
train loss:  0.558989405632019
train gradient:  0.16910385156943225
iteration : 1088
train acc:  0.6640625
train loss:  0.5611630082130432
train gradient:  0.14504392437684843
iteration : 1089
train acc:  0.75
train loss:  0.5102930665016174
train gradient:  0.1433965028824189
iteration : 1090
train acc:  0.671875
train loss:  0.6020666360855103
train gradient:  0.16618348124251597
iteration : 1091
train acc:  0.71875
train loss:  0.5499652028083801
train gradient:  0.20598079571759845
iteration : 1092
train acc:  0.703125
train loss:  0.5629013776779175
train gradient:  0.17023848776407124
iteration : 1093
train acc:  0.6484375
train loss:  0.6112484931945801
train gradient:  0.192576396975415
iteration : 1094
train acc:  0.8046875
train loss:  0.4762386083602905
train gradient:  0.15963474548420054
iteration : 1095
train acc:  0.7109375
train loss:  0.56316739320755
train gradient:  0.16665712855624953
iteration : 1096
train acc:  0.640625
train loss:  0.6107611656188965
train gradient:  0.24080572446071213
iteration : 1097
train acc:  0.71875
train loss:  0.5450948476791382
train gradient:  0.14281691466665036
iteration : 1098
train acc:  0.625
train loss:  0.6271757483482361
train gradient:  0.23638639055813762
iteration : 1099
train acc:  0.6640625
train loss:  0.5477844476699829
train gradient:  0.19591407768317662
iteration : 1100
train acc:  0.6328125
train loss:  0.6227912902832031
train gradient:  0.2601477266156262
iteration : 1101
train acc:  0.7421875
train loss:  0.5536409616470337
train gradient:  0.1412900658508966
iteration : 1102
train acc:  0.671875
train loss:  0.5846027135848999
train gradient:  0.1869289401932802
iteration : 1103
train acc:  0.734375
train loss:  0.5040897727012634
train gradient:  0.15488304601676822
iteration : 1104
train acc:  0.6796875
train loss:  0.5889844298362732
train gradient:  0.16875252506955707
iteration : 1105
train acc:  0.703125
train loss:  0.5510750412940979
train gradient:  0.1818580201977293
iteration : 1106
train acc:  0.7265625
train loss:  0.5535502433776855
train gradient:  0.17052166278968145
iteration : 1107
train acc:  0.7109375
train loss:  0.5276035666465759
train gradient:  0.17976951431449834
iteration : 1108
train acc:  0.6328125
train loss:  0.6071457266807556
train gradient:  0.22932457789530597
iteration : 1109
train acc:  0.65625
train loss:  0.6278607845306396
train gradient:  0.24311344565971382
iteration : 1110
train acc:  0.75
train loss:  0.5286203622817993
train gradient:  0.19634400850720635
iteration : 1111
train acc:  0.6875
train loss:  0.553852379322052
train gradient:  0.18090601882391563
iteration : 1112
train acc:  0.6484375
train loss:  0.6286619901657104
train gradient:  0.17960303385283233
iteration : 1113
train acc:  0.71875
train loss:  0.5377331376075745
train gradient:  0.16679092718484448
iteration : 1114
train acc:  0.6953125
train loss:  0.5861561894416809
train gradient:  0.1919566172445829
iteration : 1115
train acc:  0.65625
train loss:  0.5744414925575256
train gradient:  0.21470302617667247
iteration : 1116
train acc:  0.6640625
train loss:  0.6035096645355225
train gradient:  0.16059843916148414
iteration : 1117
train acc:  0.6796875
train loss:  0.6073883771896362
train gradient:  0.18497381878613248
iteration : 1118
train acc:  0.6875
train loss:  0.5766897201538086
train gradient:  0.17989270168641505
iteration : 1119
train acc:  0.6640625
train loss:  0.5844792127609253
train gradient:  0.22634031310776553
iteration : 1120
train acc:  0.71875
train loss:  0.5194451808929443
train gradient:  0.17588058862137915
iteration : 1121
train acc:  0.6875
train loss:  0.5869516730308533
train gradient:  0.16678023145365947
iteration : 1122
train acc:  0.6640625
train loss:  0.6097198724746704
train gradient:  0.18582172084092577
iteration : 1123
train acc:  0.640625
train loss:  0.6051418781280518
train gradient:  0.17796129505491945
iteration : 1124
train acc:  0.703125
train loss:  0.5502817630767822
train gradient:  0.16777816074824187
iteration : 1125
train acc:  0.734375
train loss:  0.521179735660553
train gradient:  0.13077647435303674
iteration : 1126
train acc:  0.6640625
train loss:  0.5633974075317383
train gradient:  0.20486641477613865
iteration : 1127
train acc:  0.609375
train loss:  0.6157571077346802
train gradient:  0.2064715724302987
iteration : 1128
train acc:  0.6484375
train loss:  0.6078627705574036
train gradient:  0.16271710383596286
iteration : 1129
train acc:  0.6796875
train loss:  0.5894016623497009
train gradient:  0.1584863399977395
iteration : 1130
train acc:  0.734375
train loss:  0.5083349347114563
train gradient:  0.28048870581827295
iteration : 1131
train acc:  0.734375
train loss:  0.5459108948707581
train gradient:  0.1626170920062246
iteration : 1132
train acc:  0.703125
train loss:  0.560747504234314
train gradient:  0.19285790779821552
iteration : 1133
train acc:  0.6796875
train loss:  0.5354344844818115
train gradient:  0.19053155851296746
iteration : 1134
train acc:  0.6484375
train loss:  0.5784098505973816
train gradient:  0.13721479316577465
iteration : 1135
train acc:  0.7421875
train loss:  0.49687135219573975
train gradient:  0.16931303849542434
iteration : 1136
train acc:  0.6328125
train loss:  0.595537543296814
train gradient:  0.19225505741828555
iteration : 1137
train acc:  0.703125
train loss:  0.5562266707420349
train gradient:  0.1802512665822717
iteration : 1138
train acc:  0.7734375
train loss:  0.5391390323638916
train gradient:  0.1573674759046096
iteration : 1139
train acc:  0.7578125
train loss:  0.5292812585830688
train gradient:  0.12886982057724775
iteration : 1140
train acc:  0.7109375
train loss:  0.5471947193145752
train gradient:  0.14966837885940049
iteration : 1141
train acc:  0.6953125
train loss:  0.5664293766021729
train gradient:  0.1783465977177397
iteration : 1142
train acc:  0.609375
train loss:  0.6520073413848877
train gradient:  0.38490591945325314
iteration : 1143
train acc:  0.71875
train loss:  0.5378012657165527
train gradient:  0.15244589230037536
iteration : 1144
train acc:  0.6484375
train loss:  0.5930233001708984
train gradient:  0.17730970245810837
iteration : 1145
train acc:  0.75
train loss:  0.49613064527511597
train gradient:  0.15313789508407322
iteration : 1146
train acc:  0.7109375
train loss:  0.5671778917312622
train gradient:  0.1817589721819985
iteration : 1147
train acc:  0.78125
train loss:  0.4962422251701355
train gradient:  0.1331214711257551
iteration : 1148
train acc:  0.6796875
train loss:  0.5947222709655762
train gradient:  0.18729290860024148
iteration : 1149
train acc:  0.6953125
train loss:  0.5413049459457397
train gradient:  0.14265138198690383
iteration : 1150
train acc:  0.6953125
train loss:  0.557968258857727
train gradient:  0.22756490594751794
iteration : 1151
train acc:  0.7265625
train loss:  0.555550217628479
train gradient:  0.16250957476529612
iteration : 1152
train acc:  0.6875
train loss:  0.5790815353393555
train gradient:  0.18853750639186379
iteration : 1153
train acc:  0.6796875
train loss:  0.5579354763031006
train gradient:  0.13217120597414117
iteration : 1154
train acc:  0.6484375
train loss:  0.5959558486938477
train gradient:  0.16251555142312768
iteration : 1155
train acc:  0.7265625
train loss:  0.5311928987503052
train gradient:  0.14362360071233668
iteration : 1156
train acc:  0.6953125
train loss:  0.6247565150260925
train gradient:  0.22038931449288673
iteration : 1157
train acc:  0.6640625
train loss:  0.6234272122383118
train gradient:  0.21645103351764694
iteration : 1158
train acc:  0.7265625
train loss:  0.5554656982421875
train gradient:  0.1656931942675961
iteration : 1159
train acc:  0.6796875
train loss:  0.5870561599731445
train gradient:  0.1475255452087912
iteration : 1160
train acc:  0.734375
train loss:  0.5217098593711853
train gradient:  0.15879768192836702
iteration : 1161
train acc:  0.7109375
train loss:  0.5503844022750854
train gradient:  0.13296234337987398
iteration : 1162
train acc:  0.6484375
train loss:  0.5893517136573792
train gradient:  0.18809009365787116
iteration : 1163
train acc:  0.6015625
train loss:  0.625230073928833
train gradient:  0.22442093549910097
iteration : 1164
train acc:  0.6953125
train loss:  0.5678021907806396
train gradient:  0.15982830252811875
iteration : 1165
train acc:  0.6640625
train loss:  0.5981950759887695
train gradient:  0.2246362482760963
iteration : 1166
train acc:  0.6015625
train loss:  0.6319210529327393
train gradient:  0.22664001034905987
iteration : 1167
train acc:  0.65625
train loss:  0.5883470177650452
train gradient:  0.18411760848823094
iteration : 1168
train acc:  0.71875
train loss:  0.6102062463760376
train gradient:  0.15740166230192532
iteration : 1169
train acc:  0.7109375
train loss:  0.584068775177002
train gradient:  0.1616541939915559
iteration : 1170
train acc:  0.65625
train loss:  0.5959874391555786
train gradient:  0.18018390853358873
iteration : 1171
train acc:  0.703125
train loss:  0.6027889251708984
train gradient:  0.28283180599945396
iteration : 1172
train acc:  0.6953125
train loss:  0.5412430167198181
train gradient:  0.17105211661348863
iteration : 1173
train acc:  0.6640625
train loss:  0.6206467747688293
train gradient:  0.27779397604752976
iteration : 1174
train acc:  0.7421875
train loss:  0.5285179018974304
train gradient:  0.1563199524212754
iteration : 1175
train acc:  0.6640625
train loss:  0.5505764484405518
train gradient:  0.18741921412843857
iteration : 1176
train acc:  0.6953125
train loss:  0.5761375427246094
train gradient:  0.186306335414817
iteration : 1177
train acc:  0.7265625
train loss:  0.5515034198760986
train gradient:  0.16292076524946983
iteration : 1178
train acc:  0.6953125
train loss:  0.5586316585540771
train gradient:  0.16073283299469465
iteration : 1179
train acc:  0.65625
train loss:  0.6306325793266296
train gradient:  0.20269888885029286
iteration : 1180
train acc:  0.578125
train loss:  0.6438165307044983
train gradient:  0.2122460985516234
iteration : 1181
train acc:  0.6796875
train loss:  0.5890785455703735
train gradient:  0.21251453996520914
iteration : 1182
train acc:  0.75
train loss:  0.554979681968689
train gradient:  0.18541712199238836
iteration : 1183
train acc:  0.671875
train loss:  0.560145378112793
train gradient:  0.14427337769697637
iteration : 1184
train acc:  0.7109375
train loss:  0.5980486869812012
train gradient:  0.1698469177555122
iteration : 1185
train acc:  0.8125
train loss:  0.499863862991333
train gradient:  0.16791152879439813
iteration : 1186
train acc:  0.6875
train loss:  0.5860792398452759
train gradient:  0.19432677086337852
iteration : 1187
train acc:  0.6796875
train loss:  0.5528544783592224
train gradient:  0.13017332347229665
iteration : 1188
train acc:  0.71875
train loss:  0.5270047187805176
train gradient:  0.1711182225182197
iteration : 1189
train acc:  0.7109375
train loss:  0.5242765545845032
train gradient:  0.12033165036709624
iteration : 1190
train acc:  0.765625
train loss:  0.518039345741272
train gradient:  0.20204547933383343
iteration : 1191
train acc:  0.6953125
train loss:  0.5525590777397156
train gradient:  0.1286150873562917
iteration : 1192
train acc:  0.6484375
train loss:  0.5944801568984985
train gradient:  0.19727346052222175
iteration : 1193
train acc:  0.671875
train loss:  0.6406733393669128
train gradient:  0.1913641551217278
iteration : 1194
train acc:  0.6875
train loss:  0.5796514749526978
train gradient:  0.1556932339246788
iteration : 1195
train acc:  0.6171875
train loss:  0.6905649304389954
train gradient:  0.2897768233867711
iteration : 1196
train acc:  0.6953125
train loss:  0.5614010095596313
train gradient:  0.2899298026558584
iteration : 1197
train acc:  0.7265625
train loss:  0.5571160316467285
train gradient:  0.17555350100330513
iteration : 1198
train acc:  0.640625
train loss:  0.6271645426750183
train gradient:  0.18592467812680646
iteration : 1199
train acc:  0.765625
train loss:  0.5470610857009888
train gradient:  0.146950233695046
iteration : 1200
train acc:  0.6875
train loss:  0.6029097437858582
train gradient:  0.1777501298605874
iteration : 1201
train acc:  0.65625
train loss:  0.595730185508728
train gradient:  0.19437569071950325
iteration : 1202
train acc:  0.703125
train loss:  0.5758686065673828
train gradient:  0.21489675315591164
iteration : 1203
train acc:  0.7578125
train loss:  0.5339884161949158
train gradient:  0.15807675069447813
iteration : 1204
train acc:  0.671875
train loss:  0.58411705493927
train gradient:  0.1894185861686063
iteration : 1205
train acc:  0.6953125
train loss:  0.5559226274490356
train gradient:  0.157607094166098
iteration : 1206
train acc:  0.6875
train loss:  0.5932996273040771
train gradient:  0.151418670657531
iteration : 1207
train acc:  0.6796875
train loss:  0.5405558347702026
train gradient:  0.14066176415231332
iteration : 1208
train acc:  0.65625
train loss:  0.5954478979110718
train gradient:  0.18518906171201666
iteration : 1209
train acc:  0.71875
train loss:  0.538314700126648
train gradient:  0.27665640128563385
iteration : 1210
train acc:  0.71875
train loss:  0.5517679452896118
train gradient:  0.14231114054413863
iteration : 1211
train acc:  0.734375
train loss:  0.5573760867118835
train gradient:  0.19589314675194103
iteration : 1212
train acc:  0.6953125
train loss:  0.558302104473114
train gradient:  0.23635389748724106
iteration : 1213
train acc:  0.6796875
train loss:  0.5652018189430237
train gradient:  0.16123502512054283
iteration : 1214
train acc:  0.734375
train loss:  0.5345726609230042
train gradient:  0.12714514141375385
iteration : 1215
train acc:  0.71875
train loss:  0.5576040744781494
train gradient:  0.17809133271303856
iteration : 1216
train acc:  0.671875
train loss:  0.5837287902832031
train gradient:  0.1850985681195478
iteration : 1217
train acc:  0.75
train loss:  0.5183357000350952
train gradient:  0.13265658316112178
iteration : 1218
train acc:  0.6328125
train loss:  0.607667088508606
train gradient:  0.2604749795931304
iteration : 1219
train acc:  0.7265625
train loss:  0.56830894947052
train gradient:  0.18273077001139545
iteration : 1220
train acc:  0.703125
train loss:  0.56126868724823
train gradient:  0.14297683615643353
iteration : 1221
train acc:  0.6796875
train loss:  0.5593869686126709
train gradient:  0.12830799580874022
iteration : 1222
train acc:  0.640625
train loss:  0.5999644994735718
train gradient:  0.23870566711378838
iteration : 1223
train acc:  0.71875
train loss:  0.519961953163147
train gradient:  0.15716948139868875
iteration : 1224
train acc:  0.6953125
train loss:  0.5831500887870789
train gradient:  0.14717316152927667
iteration : 1225
train acc:  0.6640625
train loss:  0.578620195388794
train gradient:  0.15716862651643226
iteration : 1226
train acc:  0.765625
train loss:  0.5231730937957764
train gradient:  0.1519063602684659
iteration : 1227
train acc:  0.6484375
train loss:  0.5821514129638672
train gradient:  0.19214295309354704
iteration : 1228
train acc:  0.734375
train loss:  0.5484408736228943
train gradient:  0.14998634328373806
iteration : 1229
train acc:  0.6796875
train loss:  0.6041099429130554
train gradient:  0.13971833726024863
iteration : 1230
train acc:  0.6875
train loss:  0.6213363409042358
train gradient:  0.2528684796941596
iteration : 1231
train acc:  0.75
train loss:  0.5478734970092773
train gradient:  0.1715324029120642
iteration : 1232
train acc:  0.7265625
train loss:  0.5143535137176514
train gradient:  0.25565769985379216
iteration : 1233
train acc:  0.7109375
train loss:  0.5922486186027527
train gradient:  0.1524370285789771
iteration : 1234
train acc:  0.703125
train loss:  0.5487380623817444
train gradient:  0.13990497980685665
iteration : 1235
train acc:  0.7109375
train loss:  0.5716345906257629
train gradient:  0.1626591281750202
iteration : 1236
train acc:  0.7421875
train loss:  0.5223231315612793
train gradient:  0.16113647116882934
iteration : 1237
train acc:  0.7578125
train loss:  0.5140729546546936
train gradient:  0.1988668581636569
iteration : 1238
train acc:  0.609375
train loss:  0.6112465858459473
train gradient:  0.16603701328659448
iteration : 1239
train acc:  0.6640625
train loss:  0.5721853971481323
train gradient:  0.20922020613856634
iteration : 1240
train acc:  0.671875
train loss:  0.5916184186935425
train gradient:  0.15578501832007027
iteration : 1241
train acc:  0.6875
train loss:  0.5883792638778687
train gradient:  0.17171422839309186
iteration : 1242
train acc:  0.6953125
train loss:  0.6037555932998657
train gradient:  0.17025165801464176
iteration : 1243
train acc:  0.7109375
train loss:  0.5588309168815613
train gradient:  0.1703165752872638
iteration : 1244
train acc:  0.6953125
train loss:  0.5797536969184875
train gradient:  0.22030009787009328
iteration : 1245
train acc:  0.6640625
train loss:  0.5644434690475464
train gradient:  0.1486104500047797
iteration : 1246
train acc:  0.703125
train loss:  0.5172549486160278
train gradient:  0.14637234103103858
iteration : 1247
train acc:  0.6171875
train loss:  0.5999446511268616
train gradient:  0.23080227193436048
iteration : 1248
train acc:  0.6875
train loss:  0.5807328820228577
train gradient:  0.23710887792603766
iteration : 1249
train acc:  0.6953125
train loss:  0.5864368081092834
train gradient:  0.13842166338657375
iteration : 1250
train acc:  0.703125
train loss:  0.539473831653595
train gradient:  0.1393207670573348
iteration : 1251
train acc:  0.734375
train loss:  0.5353758931159973
train gradient:  0.13819417233899822
iteration : 1252
train acc:  0.71875
train loss:  0.5366795063018799
train gradient:  0.1443913695986742
iteration : 1253
train acc:  0.7109375
train loss:  0.5477242469787598
train gradient:  0.16796725643536048
iteration : 1254
train acc:  0.625
train loss:  0.653901219367981
train gradient:  0.22526188239272332
iteration : 1255
train acc:  0.6640625
train loss:  0.5994662046432495
train gradient:  0.19613846309759245
iteration : 1256
train acc:  0.6953125
train loss:  0.5960547924041748
train gradient:  0.15416031953169734
iteration : 1257
train acc:  0.7109375
train loss:  0.5191513299942017
train gradient:  0.1475971763338732
iteration : 1258
train acc:  0.640625
train loss:  0.6203299760818481
train gradient:  0.148902863162894
iteration : 1259
train acc:  0.703125
train loss:  0.5835272073745728
train gradient:  0.17202898704626507
iteration : 1260
train acc:  0.671875
train loss:  0.6370746493339539
train gradient:  0.2198775273384811
iteration : 1261
train acc:  0.71875
train loss:  0.5473549365997314
train gradient:  0.16178763967216953
iteration : 1262
train acc:  0.7734375
train loss:  0.45569074153900146
train gradient:  0.11559362845304615
iteration : 1263
train acc:  0.71875
train loss:  0.5824133157730103
train gradient:  0.17525140835078376
iteration : 1264
train acc:  0.6328125
train loss:  0.5881772041320801
train gradient:  0.19902100795218616
iteration : 1265
train acc:  0.6328125
train loss:  0.6016590595245361
train gradient:  0.15326235023234902
iteration : 1266
train acc:  0.6953125
train loss:  0.5642088651657104
train gradient:  0.13355396423107252
iteration : 1267
train acc:  0.6328125
train loss:  0.6466062068939209
train gradient:  0.19870755809272855
iteration : 1268
train acc:  0.6484375
train loss:  0.5921511054039001
train gradient:  0.15491917280302664
iteration : 1269
train acc:  0.75
train loss:  0.5584497451782227
train gradient:  0.18499772240328966
iteration : 1270
train acc:  0.703125
train loss:  0.5304424166679382
train gradient:  0.13920869390692547
iteration : 1271
train acc:  0.6484375
train loss:  0.5833095908164978
train gradient:  0.14980333827683578
iteration : 1272
train acc:  0.75
train loss:  0.5567306280136108
train gradient:  0.15101327762979458
iteration : 1273
train acc:  0.6328125
train loss:  0.5802973508834839
train gradient:  0.17364821782290357
iteration : 1274
train acc:  0.71875
train loss:  0.5285634398460388
train gradient:  0.15018916905990762
iteration : 1275
train acc:  0.734375
train loss:  0.5615954399108887
train gradient:  0.14714397795840695
iteration : 1276
train acc:  0.7265625
train loss:  0.5088667869567871
train gradient:  0.13558087451263473
iteration : 1277
train acc:  0.6875
train loss:  0.5411858558654785
train gradient:  0.1717349671887306
iteration : 1278
train acc:  0.6953125
train loss:  0.5244749784469604
train gradient:  0.15318435700573413
iteration : 1279
train acc:  0.6640625
train loss:  0.5657344460487366
train gradient:  0.1601858590946974
iteration : 1280
train acc:  0.71875
train loss:  0.5005216598510742
train gradient:  0.13205067469058168
iteration : 1281
train acc:  0.7421875
train loss:  0.5186773538589478
train gradient:  0.16218362895428642
iteration : 1282
train acc:  0.75
train loss:  0.5361191034317017
train gradient:  0.21628370407098108
iteration : 1283
train acc:  0.71875
train loss:  0.5320894718170166
train gradient:  0.13590055130338738
iteration : 1284
train acc:  0.6875
train loss:  0.5399168133735657
train gradient:  0.18486218554055306
iteration : 1285
train acc:  0.6875
train loss:  0.5597241520881653
train gradient:  0.1267155892992527
iteration : 1286
train acc:  0.6875
train loss:  0.5441702604293823
train gradient:  0.15127784138046757
iteration : 1287
train acc:  0.828125
train loss:  0.43852755427360535
train gradient:  0.14435604356069953
iteration : 1288
train acc:  0.7421875
train loss:  0.508610725402832
train gradient:  0.13521121128326735
iteration : 1289
train acc:  0.6796875
train loss:  0.560094952583313
train gradient:  0.15597432165669353
iteration : 1290
train acc:  0.6953125
train loss:  0.5614861249923706
train gradient:  0.20325900807198724
iteration : 1291
train acc:  0.6796875
train loss:  0.5519465208053589
train gradient:  0.13556848134749855
iteration : 1292
train acc:  0.6171875
train loss:  0.5858656167984009
train gradient:  0.1619983584698079
iteration : 1293
train acc:  0.6875
train loss:  0.5678684711456299
train gradient:  0.18380413696466785
iteration : 1294
train acc:  0.7578125
train loss:  0.4932437837123871
train gradient:  0.13654868031799983
iteration : 1295
train acc:  0.703125
train loss:  0.5278449058532715
train gradient:  0.19108033136556282
iteration : 1296
train acc:  0.7109375
train loss:  0.5621559619903564
train gradient:  0.16938218728657295
iteration : 1297
train acc:  0.75
train loss:  0.5244624614715576
train gradient:  0.15219896921372203
iteration : 1298
train acc:  0.6875
train loss:  0.6125277280807495
train gradient:  0.1739283013036716
iteration : 1299
train acc:  0.6953125
train loss:  0.5658882856369019
train gradient:  0.14521568509933208
iteration : 1300
train acc:  0.6953125
train loss:  0.5526331663131714
train gradient:  0.15574458274891587
iteration : 1301
train acc:  0.671875
train loss:  0.5811198949813843
train gradient:  0.17343085270504766
iteration : 1302
train acc:  0.6171875
train loss:  0.6316903233528137
train gradient:  0.19498321419133102
iteration : 1303
train acc:  0.6796875
train loss:  0.5630585551261902
train gradient:  0.17341011659029346
iteration : 1304
train acc:  0.6796875
train loss:  0.5642995238304138
train gradient:  0.17427457796065518
iteration : 1305
train acc:  0.734375
train loss:  0.5562408566474915
train gradient:  0.23184364250009523
iteration : 1306
train acc:  0.7109375
train loss:  0.5441789031028748
train gradient:  0.15221832968130622
iteration : 1307
train acc:  0.578125
train loss:  0.6658836603164673
train gradient:  0.22635638215238185
iteration : 1308
train acc:  0.75
train loss:  0.49211081862449646
train gradient:  0.13695780412019415
iteration : 1309
train acc:  0.734375
train loss:  0.5457980632781982
train gradient:  0.1823397540542433
iteration : 1310
train acc:  0.6796875
train loss:  0.5526077747344971
train gradient:  0.2191899612418889
iteration : 1311
train acc:  0.78125
train loss:  0.497586190700531
train gradient:  0.1656779932157446
iteration : 1312
train acc:  0.7734375
train loss:  0.5006491541862488
train gradient:  0.14984529867843552
iteration : 1313
train acc:  0.6796875
train loss:  0.5378788709640503
train gradient:  0.15465565970902873
iteration : 1314
train acc:  0.6484375
train loss:  0.6290248036384583
train gradient:  0.20847532973452723
iteration : 1315
train acc:  0.75
train loss:  0.5576760768890381
train gradient:  0.17632924797772598
iteration : 1316
train acc:  0.671875
train loss:  0.6081126928329468
train gradient:  0.20023463018326246
iteration : 1317
train acc:  0.71875
train loss:  0.5206103920936584
train gradient:  0.1005734861139508
iteration : 1318
train acc:  0.6875
train loss:  0.5827987194061279
train gradient:  0.20468223174780237
iteration : 1319
train acc:  0.71875
train loss:  0.5855035781860352
train gradient:  0.23843576735437327
iteration : 1320
train acc:  0.6640625
train loss:  0.5848745107650757
train gradient:  0.19016929292895693
iteration : 1321
train acc:  0.71875
train loss:  0.5608189105987549
train gradient:  0.14399960361206318
iteration : 1322
train acc:  0.6171875
train loss:  0.603486180305481
train gradient:  0.19174177747575655
iteration : 1323
train acc:  0.609375
train loss:  0.6361826062202454
train gradient:  0.26863094544488875
iteration : 1324
train acc:  0.625
train loss:  0.6053067445755005
train gradient:  0.22409777901798864
iteration : 1325
train acc:  0.703125
train loss:  0.5577020645141602
train gradient:  0.16350135644538952
iteration : 1326
train acc:  0.6953125
train loss:  0.5577971935272217
train gradient:  0.15683765501275698
iteration : 1327
train acc:  0.6953125
train loss:  0.532214879989624
train gradient:  0.14438298786810555
iteration : 1328
train acc:  0.6640625
train loss:  0.5818830728530884
train gradient:  0.17403953758328572
iteration : 1329
train acc:  0.671875
train loss:  0.5893808603286743
train gradient:  0.22861614447004597
iteration : 1330
train acc:  0.7578125
train loss:  0.5039390325546265
train gradient:  0.11578060627446715
iteration : 1331
train acc:  0.7109375
train loss:  0.5795745849609375
train gradient:  0.2064570000223857
iteration : 1332
train acc:  0.65625
train loss:  0.553952693939209
train gradient:  0.19329515399105573
iteration : 1333
train acc:  0.6875
train loss:  0.5659835338592529
train gradient:  0.1441263905350092
iteration : 1334
train acc:  0.703125
train loss:  0.5357367992401123
train gradient:  0.2095778858782695
iteration : 1335
train acc:  0.71875
train loss:  0.525438129901886
train gradient:  0.17612395254248908
iteration : 1336
train acc:  0.7109375
train loss:  0.5396984815597534
train gradient:  0.13833231571027887
iteration : 1337
train acc:  0.671875
train loss:  0.5503777265548706
train gradient:  0.19412576709426027
iteration : 1338
train acc:  0.765625
train loss:  0.5398330688476562
train gradient:  0.16356403995201235
iteration : 1339
train acc:  0.6484375
train loss:  0.6095255613327026
train gradient:  0.19908437898682196
iteration : 1340
train acc:  0.671875
train loss:  0.5847476124763489
train gradient:  0.17310327980464416
iteration : 1341
train acc:  0.75
train loss:  0.5188717842102051
train gradient:  0.17217040543989118
iteration : 1342
train acc:  0.75
train loss:  0.5262302756309509
train gradient:  0.18005003823677063
iteration : 1343
train acc:  0.671875
train loss:  0.5813944935798645
train gradient:  0.17552103722356333
iteration : 1344
train acc:  0.6171875
train loss:  0.6414029598236084
train gradient:  0.19481955446742366
iteration : 1345
train acc:  0.734375
train loss:  0.5621804594993591
train gradient:  0.15381188570216273
iteration : 1346
train acc:  0.734375
train loss:  0.5295280814170837
train gradient:  0.13251230155752305
iteration : 1347
train acc:  0.671875
train loss:  0.6026801466941833
train gradient:  0.14719318610000887
iteration : 1348
train acc:  0.7109375
train loss:  0.5693622827529907
train gradient:  0.1782465697954549
iteration : 1349
train acc:  0.734375
train loss:  0.5249464511871338
train gradient:  0.1882455567663249
iteration : 1350
train acc:  0.65625
train loss:  0.5609687566757202
train gradient:  0.15123494450081632
iteration : 1351
train acc:  0.71875
train loss:  0.5102678537368774
train gradient:  0.14745609617270256
iteration : 1352
train acc:  0.6796875
train loss:  0.5701162815093994
train gradient:  0.17352575292642614
iteration : 1353
train acc:  0.703125
train loss:  0.5285079479217529
train gradient:  0.22266204818688334
iteration : 1354
train acc:  0.6953125
train loss:  0.5705417394638062
train gradient:  0.1173359737228632
iteration : 1355
train acc:  0.7265625
train loss:  0.5263139009475708
train gradient:  0.148002264508234
iteration : 1356
train acc:  0.6953125
train loss:  0.5894873142242432
train gradient:  0.16175670861858588
iteration : 1357
train acc:  0.7734375
train loss:  0.532860279083252
train gradient:  0.14775376772969842
iteration : 1358
train acc:  0.6640625
train loss:  0.5883135795593262
train gradient:  0.15422238774358865
iteration : 1359
train acc:  0.7421875
train loss:  0.5449151396751404
train gradient:  0.20822151539395378
iteration : 1360
train acc:  0.7109375
train loss:  0.549285888671875
train gradient:  0.15325614976310298
iteration : 1361
train acc:  0.6796875
train loss:  0.5631357431411743
train gradient:  0.21425710287965327
iteration : 1362
train acc:  0.7421875
train loss:  0.5211016535758972
train gradient:  0.19234407641429274
iteration : 1363
train acc:  0.671875
train loss:  0.6022131443023682
train gradient:  0.17194771405071413
iteration : 1364
train acc:  0.7421875
train loss:  0.5411805510520935
train gradient:  0.16907299003412898
iteration : 1365
train acc:  0.6484375
train loss:  0.5559998750686646
train gradient:  0.2064798929918385
iteration : 1366
train acc:  0.6953125
train loss:  0.6109453439712524
train gradient:  0.2122953722620548
iteration : 1367
train acc:  0.6875
train loss:  0.5776052474975586
train gradient:  0.14095567825168526
iteration : 1368
train acc:  0.7265625
train loss:  0.564306378364563
train gradient:  0.19509473358249751
iteration : 1369
train acc:  0.6484375
train loss:  0.5822400450706482
train gradient:  0.16486328678081819
iteration : 1370
train acc:  0.7109375
train loss:  0.544107973575592
train gradient:  0.1805853709628774
iteration : 1371
train acc:  0.703125
train loss:  0.5361150503158569
train gradient:  0.217666537392678
iteration : 1372
train acc:  0.6953125
train loss:  0.5374879240989685
train gradient:  0.1348090666059821
iteration : 1373
train acc:  0.6328125
train loss:  0.6185778379440308
train gradient:  0.18014709523595834
iteration : 1374
train acc:  0.7109375
train loss:  0.5491002798080444
train gradient:  0.15487319784272685
iteration : 1375
train acc:  0.6875
train loss:  0.5456604361534119
train gradient:  0.14364921610466735
iteration : 1376
train acc:  0.6640625
train loss:  0.5918651223182678
train gradient:  0.18899166135528744
iteration : 1377
train acc:  0.75
train loss:  0.5028375387191772
train gradient:  0.1427267306002526
iteration : 1378
train acc:  0.734375
train loss:  0.5036636590957642
train gradient:  0.1293985584946684
iteration : 1379
train acc:  0.6796875
train loss:  0.5258699655532837
train gradient:  0.15093209038372768
iteration : 1380
train acc:  0.75
train loss:  0.5166016817092896
train gradient:  0.21544730497306191
iteration : 1381
train acc:  0.7109375
train loss:  0.5636497735977173
train gradient:  0.14862819603588567
iteration : 1382
train acc:  0.71875
train loss:  0.5851119160652161
train gradient:  0.18307633803474413
iteration : 1383
train acc:  0.8203125
train loss:  0.47533857822418213
train gradient:  0.12128384427901848
iteration : 1384
train acc:  0.703125
train loss:  0.5893223881721497
train gradient:  0.16099406246743228
iteration : 1385
train acc:  0.6015625
train loss:  0.6422815918922424
train gradient:  0.24449877229805772
iteration : 1386
train acc:  0.6640625
train loss:  0.5775155425071716
train gradient:  0.22540902611391755
iteration : 1387
train acc:  0.703125
train loss:  0.5766682624816895
train gradient:  0.22520704022616864
iteration : 1388
train acc:  0.734375
train loss:  0.5634669065475464
train gradient:  0.22524581266688576
iteration : 1389
train acc:  0.7109375
train loss:  0.6153813004493713
train gradient:  0.23872215670188437
iteration : 1390
train acc:  0.59375
train loss:  0.6254869699478149
train gradient:  0.18925047605561093
iteration : 1391
train acc:  0.7109375
train loss:  0.5464447736740112
train gradient:  0.14919063262512267
iteration : 1392
train acc:  0.703125
train loss:  0.5123344659805298
train gradient:  0.17773620376819232
iteration : 1393
train acc:  0.640625
train loss:  0.599494218826294
train gradient:  0.22166413597055634
iteration : 1394
train acc:  0.7109375
train loss:  0.5803278088569641
train gradient:  0.22928206473405588
iteration : 1395
train acc:  0.7265625
train loss:  0.5055858492851257
train gradient:  0.14905153351605233
iteration : 1396
train acc:  0.6640625
train loss:  0.6271220445632935
train gradient:  0.21662584123936673
iteration : 1397
train acc:  0.734375
train loss:  0.5423723459243774
train gradient:  0.16588571342935093
iteration : 1398
train acc:  0.6875
train loss:  0.5802233815193176
train gradient:  0.20635154864635427
iteration : 1399
train acc:  0.734375
train loss:  0.5733397006988525
train gradient:  0.2278095954187424
iteration : 1400
train acc:  0.6640625
train loss:  0.5569760203361511
train gradient:  0.22097044063171936
iteration : 1401
train acc:  0.71875
train loss:  0.5543374419212341
train gradient:  0.16269445701794072
iteration : 1402
train acc:  0.75
train loss:  0.5108264684677124
train gradient:  0.1643843724087652
iteration : 1403
train acc:  0.71875
train loss:  0.5090157985687256
train gradient:  0.18260851402855854
iteration : 1404
train acc:  0.7265625
train loss:  0.5230445861816406
train gradient:  0.12672758232139264
iteration : 1405
train acc:  0.71875
train loss:  0.5532019138336182
train gradient:  0.15608092298102202
iteration : 1406
train acc:  0.6796875
train loss:  0.5922490358352661
train gradient:  0.14776551530583007
iteration : 1407
train acc:  0.7265625
train loss:  0.5528014898300171
train gradient:  0.19870920908396053
iteration : 1408
train acc:  0.703125
train loss:  0.5785001516342163
train gradient:  0.19369596959134386
iteration : 1409
train acc:  0.6796875
train loss:  0.5663835406303406
train gradient:  0.13746247846077736
iteration : 1410
train acc:  0.7578125
train loss:  0.5139286518096924
train gradient:  0.17203445154489938
iteration : 1411
train acc:  0.625
train loss:  0.6491150856018066
train gradient:  0.21079045961922188
iteration : 1412
train acc:  0.734375
train loss:  0.5132601261138916
train gradient:  0.14698976020597326
iteration : 1413
train acc:  0.6484375
train loss:  0.5931389927864075
train gradient:  0.2531050713999613
iteration : 1414
train acc:  0.71875
train loss:  0.5554128885269165
train gradient:  0.2573000484869335
iteration : 1415
train acc:  0.6484375
train loss:  0.5985040664672852
train gradient:  0.19308097269782568
iteration : 1416
train acc:  0.7421875
train loss:  0.522290050983429
train gradient:  0.18163165596665498
iteration : 1417
train acc:  0.75
train loss:  0.532929539680481
train gradient:  0.1402301207890575
iteration : 1418
train acc:  0.6796875
train loss:  0.6311757564544678
train gradient:  0.2065510055047452
iteration : 1419
train acc:  0.7265625
train loss:  0.5167627930641174
train gradient:  0.23537770073959077
iteration : 1420
train acc:  0.765625
train loss:  0.5076311826705933
train gradient:  0.1337714797685561
iteration : 1421
train acc:  0.734375
train loss:  0.5617707967758179
train gradient:  0.16734340549027954
iteration : 1422
train acc:  0.7578125
train loss:  0.5119731426239014
train gradient:  0.166347364322964
iteration : 1423
train acc:  0.7109375
train loss:  0.5389503240585327
train gradient:  0.16934616466134433
iteration : 1424
train acc:  0.703125
train loss:  0.5522595643997192
train gradient:  0.17808577193860262
iteration : 1425
train acc:  0.71875
train loss:  0.5375696420669556
train gradient:  0.12990484340107894
iteration : 1426
train acc:  0.7109375
train loss:  0.5706264972686768
train gradient:  0.1772741407739454
iteration : 1427
train acc:  0.734375
train loss:  0.5320589542388916
train gradient:  0.15771824487577746
iteration : 1428
train acc:  0.671875
train loss:  0.5588091611862183
train gradient:  0.1626838730639502
iteration : 1429
train acc:  0.734375
train loss:  0.5786724090576172
train gradient:  0.31324161121270194
iteration : 1430
train acc:  0.7578125
train loss:  0.5185102820396423
train gradient:  0.15788884913077672
iteration : 1431
train acc:  0.671875
train loss:  0.5668054819107056
train gradient:  0.2335438511632978
iteration : 1432
train acc:  0.765625
train loss:  0.4895658493041992
train gradient:  0.13846969663481284
iteration : 1433
train acc:  0.78125
train loss:  0.4806244969367981
train gradient:  0.11891746748895304
iteration : 1434
train acc:  0.7421875
train loss:  0.5294893980026245
train gradient:  0.20179392009692926
iteration : 1435
train acc:  0.65625
train loss:  0.5876859426498413
train gradient:  0.15812146826853937
iteration : 1436
train acc:  0.6953125
train loss:  0.5633150339126587
train gradient:  0.1620756637719136
iteration : 1437
train acc:  0.65625
train loss:  0.5709269046783447
train gradient:  0.14739568876023096
iteration : 1438
train acc:  0.640625
train loss:  0.6114492416381836
train gradient:  0.17903466193255538
iteration : 1439
train acc:  0.640625
train loss:  0.5610596537590027
train gradient:  0.18643818748303564
iteration : 1440
train acc:  0.75
train loss:  0.5334322452545166
train gradient:  0.13794496707100207
iteration : 1441
train acc:  0.71875
train loss:  0.5202025771141052
train gradient:  0.17381206704131014
iteration : 1442
train acc:  0.6953125
train loss:  0.5600481033325195
train gradient:  0.14880305269088684
iteration : 1443
train acc:  0.6328125
train loss:  0.6313624382019043
train gradient:  0.35177344544223477
iteration : 1444
train acc:  0.6796875
train loss:  0.5819755792617798
train gradient:  0.17278702791357312
iteration : 1445
train acc:  0.734375
train loss:  0.5371184945106506
train gradient:  0.14613334862003918
iteration : 1446
train acc:  0.7109375
train loss:  0.5424415469169617
train gradient:  0.12197554867366991
iteration : 1447
train acc:  0.65625
train loss:  0.589360773563385
train gradient:  0.19200590234711037
iteration : 1448
train acc:  0.796875
train loss:  0.5109116435050964
train gradient:  0.15073221421848343
iteration : 1449
train acc:  0.6640625
train loss:  0.5719262361526489
train gradient:  0.1683252557160636
iteration : 1450
train acc:  0.6953125
train loss:  0.5896718502044678
train gradient:  0.2034372255617652
iteration : 1451
train acc:  0.640625
train loss:  0.6102643609046936
train gradient:  0.23568198596504697
iteration : 1452
train acc:  0.71875
train loss:  0.5218344330787659
train gradient:  0.1407845998308973
iteration : 1453
train acc:  0.7265625
train loss:  0.549817681312561
train gradient:  0.13729149828027845
iteration : 1454
train acc:  0.7265625
train loss:  0.5329868793487549
train gradient:  0.15595802590054342
iteration : 1455
train acc:  0.734375
train loss:  0.52733314037323
train gradient:  0.1231590567271759
iteration : 1456
train acc:  0.71875
train loss:  0.5341775417327881
train gradient:  0.1917979479140201
iteration : 1457
train acc:  0.7109375
train loss:  0.5112894773483276
train gradient:  0.17986403299006254
iteration : 1458
train acc:  0.6640625
train loss:  0.5871899127960205
train gradient:  0.15972769169383438
iteration : 1459
train acc:  0.671875
train loss:  0.5848791599273682
train gradient:  0.17836678718751306
iteration : 1460
train acc:  0.75
train loss:  0.5146129131317139
train gradient:  0.16056060925884041
iteration : 1461
train acc:  0.7265625
train loss:  0.5491406917572021
train gradient:  0.2979728799623606
iteration : 1462
train acc:  0.7421875
train loss:  0.5304034948348999
train gradient:  0.18496037957744776
iteration : 1463
train acc:  0.703125
train loss:  0.5213456749916077
train gradient:  0.14149408639088476
iteration : 1464
train acc:  0.6875
train loss:  0.5398019552230835
train gradient:  0.16665245033171325
iteration : 1465
train acc:  0.6640625
train loss:  0.5913224816322327
train gradient:  0.2156518051853541
iteration : 1466
train acc:  0.671875
train loss:  0.5745784044265747
train gradient:  0.17204846767616486
iteration : 1467
train acc:  0.6796875
train loss:  0.5660117864608765
train gradient:  0.1559841331968307
iteration : 1468
train acc:  0.71875
train loss:  0.5460084676742554
train gradient:  0.12822886454031154
iteration : 1469
train acc:  0.734375
train loss:  0.497694194316864
train gradient:  0.23072845393710617
iteration : 1470
train acc:  0.6875
train loss:  0.5594302415847778
train gradient:  0.19618137881023498
iteration : 1471
train acc:  0.703125
train loss:  0.5270756483078003
train gradient:  0.16871332014325774
iteration : 1472
train acc:  0.8046875
train loss:  0.47291603684425354
train gradient:  0.14638836677451336
iteration : 1473
train acc:  0.6875
train loss:  0.5695286393165588
train gradient:  0.18753239406683214
iteration : 1474
train acc:  0.6328125
train loss:  0.5834856033325195
train gradient:  0.2372491344358153
iteration : 1475
train acc:  0.765625
train loss:  0.52655029296875
train gradient:  0.17415858657411243
iteration : 1476
train acc:  0.796875
train loss:  0.4900885820388794
train gradient:  0.16530630566229598
iteration : 1477
train acc:  0.65625
train loss:  0.6266388893127441
train gradient:  0.3382443615042271
iteration : 1478
train acc:  0.71875
train loss:  0.530092716217041
train gradient:  0.15365481617591806
iteration : 1479
train acc:  0.6328125
train loss:  0.6643946170806885
train gradient:  0.20467519448945998
iteration : 1480
train acc:  0.703125
train loss:  0.5802067518234253
train gradient:  0.2245369083252599
iteration : 1481
train acc:  0.71875
train loss:  0.5837142467498779
train gradient:  0.22202335954695288
iteration : 1482
train acc:  0.671875
train loss:  0.6154971122741699
train gradient:  0.226289207468404
iteration : 1483
train acc:  0.671875
train loss:  0.5681105852127075
train gradient:  0.24890136191543555
iteration : 1484
train acc:  0.6640625
train loss:  0.5400897264480591
train gradient:  0.15418182584334772
iteration : 1485
train acc:  0.765625
train loss:  0.5167940258979797
train gradient:  0.19658170245648415
iteration : 1486
train acc:  0.6953125
train loss:  0.526839017868042
train gradient:  0.16578928853755331
iteration : 1487
train acc:  0.703125
train loss:  0.5399816036224365
train gradient:  0.24851125950888592
iteration : 1488
train acc:  0.6875
train loss:  0.5551594495773315
train gradient:  0.190711704714059
iteration : 1489
train acc:  0.703125
train loss:  0.5226336717605591
train gradient:  0.13241846955569037
iteration : 1490
train acc:  0.6640625
train loss:  0.6007665395736694
train gradient:  0.16876690764570226
iteration : 1491
train acc:  0.6484375
train loss:  0.6445749998092651
train gradient:  0.20215034020051142
iteration : 1492
train acc:  0.7109375
train loss:  0.5677392482757568
train gradient:  0.26194585501654366
iteration : 1493
train acc:  0.734375
train loss:  0.5210425853729248
train gradient:  0.1662365090022728
iteration : 1494
train acc:  0.6796875
train loss:  0.6098896265029907
train gradient:  0.2000581034367971
iteration : 1495
train acc:  0.6640625
train loss:  0.5805660486221313
train gradient:  0.1881756414886417
iteration : 1496
train acc:  0.6640625
train loss:  0.5615708827972412
train gradient:  0.25411530173337427
iteration : 1497
train acc:  0.75
train loss:  0.5540736317634583
train gradient:  0.17757893486522006
iteration : 1498
train acc:  0.640625
train loss:  0.5769118070602417
train gradient:  0.19119630427033657
iteration : 1499
train acc:  0.6953125
train loss:  0.5322778224945068
train gradient:  0.15036129073775306
iteration : 1500
train acc:  0.734375
train loss:  0.5243405103683472
train gradient:  0.1527950600313447
iteration : 1501
train acc:  0.6875
train loss:  0.5841938257217407
train gradient:  0.2243269476116963
iteration : 1502
train acc:  0.6796875
train loss:  0.5866881608963013
train gradient:  0.19265887218646466
iteration : 1503
train acc:  0.6875
train loss:  0.5447746515274048
train gradient:  0.140930530855576
iteration : 1504
train acc:  0.7578125
train loss:  0.5139995813369751
train gradient:  0.12658633718202533
iteration : 1505
train acc:  0.6796875
train loss:  0.5985571146011353
train gradient:  0.17183930516089735
iteration : 1506
train acc:  0.640625
train loss:  0.6086840629577637
train gradient:  0.21355634018373668
iteration : 1507
train acc:  0.640625
train loss:  0.5904459357261658
train gradient:  0.1748775937562976
iteration : 1508
train acc:  0.6640625
train loss:  0.5863631963729858
train gradient:  0.3811986861069849
iteration : 1509
train acc:  0.6640625
train loss:  0.6160258054733276
train gradient:  0.19340027771339732
iteration : 1510
train acc:  0.6875
train loss:  0.575080156326294
train gradient:  0.17699181751419582
iteration : 1511
train acc:  0.7109375
train loss:  0.5623281002044678
train gradient:  0.15539520431850073
iteration : 1512
train acc:  0.7265625
train loss:  0.5357207655906677
train gradient:  0.12389784137864937
iteration : 1513
train acc:  0.6875
train loss:  0.5831866264343262
train gradient:  0.23819772172891313
iteration : 1514
train acc:  0.6953125
train loss:  0.5373298525810242
train gradient:  0.1604466810890424
iteration : 1515
train acc:  0.6484375
train loss:  0.6038798093795776
train gradient:  0.1948136300378273
iteration : 1516
train acc:  0.6953125
train loss:  0.574715256690979
train gradient:  0.1867975856962878
iteration : 1517
train acc:  0.703125
train loss:  0.5345773696899414
train gradient:  0.1878033476900577
iteration : 1518
train acc:  0.71875
train loss:  0.5192692875862122
train gradient:  0.14614270833018456
iteration : 1519
train acc:  0.6875
train loss:  0.5717704892158508
train gradient:  0.16749172011996832
iteration : 1520
train acc:  0.7421875
train loss:  0.5421873331069946
train gradient:  0.23536586160177841
iteration : 1521
train acc:  0.7578125
train loss:  0.5255696773529053
train gradient:  0.18066678671616274
iteration : 1522
train acc:  0.671875
train loss:  0.5744809508323669
train gradient:  0.1884177291108824
iteration : 1523
train acc:  0.6953125
train loss:  0.5566678047180176
train gradient:  0.15017030872414058
iteration : 1524
train acc:  0.7265625
train loss:  0.5102705359458923
train gradient:  0.15378338270591796
iteration : 1525
train acc:  0.703125
train loss:  0.5503897070884705
train gradient:  0.1609759181486552
iteration : 1526
train acc:  0.65625
train loss:  0.5844658017158508
train gradient:  0.22438870847158743
iteration : 1527
train acc:  0.703125
train loss:  0.5402027368545532
train gradient:  0.1739136578535835
iteration : 1528
train acc:  0.7265625
train loss:  0.5439949035644531
train gradient:  0.16294378288769315
iteration : 1529
train acc:  0.734375
train loss:  0.5381895303726196
train gradient:  0.13178016787086305
iteration : 1530
train acc:  0.78125
train loss:  0.5048459768295288
train gradient:  0.16300991533314618
iteration : 1531
train acc:  0.6875
train loss:  0.5627457499504089
train gradient:  0.17958083416447224
iteration : 1532
train acc:  0.7421875
train loss:  0.5120638012886047
train gradient:  0.19184954455594525
iteration : 1533
train acc:  0.671875
train loss:  0.6345993876457214
train gradient:  0.2432975792564335
iteration : 1534
train acc:  0.6875
train loss:  0.5921100378036499
train gradient:  0.17287914783153235
iteration : 1535
train acc:  0.734375
train loss:  0.5256161093711853
train gradient:  0.14517570975395583
iteration : 1536
train acc:  0.734375
train loss:  0.5084774494171143
train gradient:  0.17600656703195566
iteration : 1537
train acc:  0.6484375
train loss:  0.6213705539703369
train gradient:  0.21983428976250335
iteration : 1538
train acc:  0.734375
train loss:  0.5226394534111023
train gradient:  0.16264385343858256
iteration : 1539
train acc:  0.734375
train loss:  0.5140848755836487
train gradient:  0.15417316675769113
iteration : 1540
train acc:  0.5703125
train loss:  0.6437315940856934
train gradient:  0.24713846255137384
iteration : 1541
train acc:  0.6875
train loss:  0.5223249793052673
train gradient:  0.14663931021269255
iteration : 1542
train acc:  0.6796875
train loss:  0.5741451382637024
train gradient:  0.18521357276622025
iteration : 1543
train acc:  0.703125
train loss:  0.5632325410842896
train gradient:  0.19342120591394624
iteration : 1544
train acc:  0.734375
train loss:  0.5159304141998291
train gradient:  0.16874429903808158
iteration : 1545
train acc:  0.6953125
train loss:  0.5368105173110962
train gradient:  0.2229819187661332
iteration : 1546
train acc:  0.7109375
train loss:  0.5374610424041748
train gradient:  0.14065899540727392
iteration : 1547
train acc:  0.609375
train loss:  0.6269550323486328
train gradient:  0.1786959490265499
iteration : 1548
train acc:  0.640625
train loss:  0.6185883283615112
train gradient:  0.19962907467999616
iteration : 1549
train acc:  0.71875
train loss:  0.5351381301879883
train gradient:  0.16718061189787836
iteration : 1550
train acc:  0.7109375
train loss:  0.5463603734970093
train gradient:  0.14796762715943287
iteration : 1551
train acc:  0.71875
train loss:  0.5341739654541016
train gradient:  0.16620092296872788
iteration : 1552
train acc:  0.6796875
train loss:  0.5888711214065552
train gradient:  0.19089200868326625
iteration : 1553
train acc:  0.640625
train loss:  0.625795841217041
train gradient:  0.2196618135698314
iteration : 1554
train acc:  0.7421875
train loss:  0.4995461702346802
train gradient:  0.16743187883360106
iteration : 1555
train acc:  0.671875
train loss:  0.535330593585968
train gradient:  0.1677987813313883
iteration : 1556
train acc:  0.7265625
train loss:  0.5435839891433716
train gradient:  0.16348102719205956
iteration : 1557
train acc:  0.6015625
train loss:  0.6192003488540649
train gradient:  0.18281344950295572
iteration : 1558
train acc:  0.6796875
train loss:  0.6024450063705444
train gradient:  0.20651247245798127
iteration : 1559
train acc:  0.7109375
train loss:  0.5636141300201416
train gradient:  0.2670532244321211
iteration : 1560
train acc:  0.703125
train loss:  0.5095545053482056
train gradient:  0.19414538290157907
iteration : 1561
train acc:  0.7578125
train loss:  0.5313800573348999
train gradient:  0.16032710985796728
iteration : 1562
train acc:  0.6484375
train loss:  0.5906760692596436
train gradient:  0.1831726214049997
iteration : 1563
train acc:  0.6953125
train loss:  0.536792516708374
train gradient:  0.17341948056864426
iteration : 1564
train acc:  0.6640625
train loss:  0.5257222652435303
train gradient:  0.16437990414670517
iteration : 1565
train acc:  0.703125
train loss:  0.5498434901237488
train gradient:  0.15653083088594177
iteration : 1566
train acc:  0.6875
train loss:  0.5630237460136414
train gradient:  0.19488888880130617
iteration : 1567
train acc:  0.6328125
train loss:  0.6368663311004639
train gradient:  0.22690330042940834
iteration : 1568
train acc:  0.609375
train loss:  0.6471476554870605
train gradient:  0.22139377805980687
iteration : 1569
train acc:  0.7109375
train loss:  0.514189600944519
train gradient:  0.18962413278118767
iteration : 1570
train acc:  0.7265625
train loss:  0.535017192363739
train gradient:  0.13737014538817488
iteration : 1571
train acc:  0.6796875
train loss:  0.5809727907180786
train gradient:  0.16195719654881474
iteration : 1572
train acc:  0.6171875
train loss:  0.5828356742858887
train gradient:  0.22425682257696877
iteration : 1573
train acc:  0.7734375
train loss:  0.5174319744110107
train gradient:  0.1490425903362526
iteration : 1574
train acc:  0.6953125
train loss:  0.5497368574142456
train gradient:  0.15373525133493499
iteration : 1575
train acc:  0.7578125
train loss:  0.526089072227478
train gradient:  0.12628393002142663
iteration : 1576
train acc:  0.71875
train loss:  0.5350422263145447
train gradient:  0.14928092409007987
iteration : 1577
train acc:  0.71875
train loss:  0.5117732286453247
train gradient:  0.2353026976601253
iteration : 1578
train acc:  0.734375
train loss:  0.5473121404647827
train gradient:  0.19159096414146276
iteration : 1579
train acc:  0.7421875
train loss:  0.4912213087081909
train gradient:  0.13992705806543404
iteration : 1580
train acc:  0.703125
train loss:  0.5471287369728088
train gradient:  0.17420471937627457
iteration : 1581
train acc:  0.6328125
train loss:  0.6733392477035522
train gradient:  0.2765161788646017
iteration : 1582
train acc:  0.6796875
train loss:  0.5610959529876709
train gradient:  0.1687759752493862
iteration : 1583
train acc:  0.703125
train loss:  0.5416790246963501
train gradient:  0.16653360070340437
iteration : 1584
train acc:  0.7734375
train loss:  0.5033553838729858
train gradient:  0.13768634284362774
iteration : 1585
train acc:  0.78125
train loss:  0.4699629843235016
train gradient:  0.15693121591303943
iteration : 1586
train acc:  0.6640625
train loss:  0.5700869560241699
train gradient:  0.15171291227437492
iteration : 1587
train acc:  0.6875
train loss:  0.5713468790054321
train gradient:  0.23643623862452462
iteration : 1588
train acc:  0.7421875
train loss:  0.526218593120575
train gradient:  0.14368683772963453
iteration : 1589
train acc:  0.71875
train loss:  0.5462406873703003
train gradient:  0.14728291943752164
iteration : 1590
train acc:  0.7734375
train loss:  0.5316451787948608
train gradient:  0.21436690441418643
iteration : 1591
train acc:  0.671875
train loss:  0.5838456749916077
train gradient:  0.16231964453843767
iteration : 1592
train acc:  0.7109375
train loss:  0.5601596832275391
train gradient:  0.1521031927135546
iteration : 1593
train acc:  0.6796875
train loss:  0.592705249786377
train gradient:  0.19555514500629378
iteration : 1594
train acc:  0.6484375
train loss:  0.6152611374855042
train gradient:  0.24673287381240852
iteration : 1595
train acc:  0.671875
train loss:  0.6022800207138062
train gradient:  0.1672270561371118
iteration : 1596
train acc:  0.6640625
train loss:  0.5591762661933899
train gradient:  0.20784974113734472
iteration : 1597
train acc:  0.65625
train loss:  0.5893151760101318
train gradient:  0.19866370321328314
iteration : 1598
train acc:  0.7578125
train loss:  0.5292291641235352
train gradient:  0.1730956137629789
iteration : 1599
train acc:  0.6171875
train loss:  0.5866786241531372
train gradient:  0.18349768183709728
iteration : 1600
train acc:  0.65625
train loss:  0.5709399580955505
train gradient:  0.23812003195685016
iteration : 1601
train acc:  0.703125
train loss:  0.6009251475334167
train gradient:  0.23852225136658453
iteration : 1602
train acc:  0.765625
train loss:  0.5295757055282593
train gradient:  0.13410368949037352
iteration : 1603
train acc:  0.7265625
train loss:  0.5042706727981567
train gradient:  0.1686970019709802
iteration : 1604
train acc:  0.6953125
train loss:  0.559339165687561
train gradient:  0.20481836477148313
iteration : 1605
train acc:  0.75
train loss:  0.5090281963348389
train gradient:  0.14937800871644383
iteration : 1606
train acc:  0.6328125
train loss:  0.5778429508209229
train gradient:  0.20000498316175566
iteration : 1607
train acc:  0.6484375
train loss:  0.598192572593689
train gradient:  0.21995932585853467
iteration : 1608
train acc:  0.7734375
train loss:  0.5225292444229126
train gradient:  0.15803345590704
iteration : 1609
train acc:  0.6875
train loss:  0.5303000807762146
train gradient:  0.15722380527218652
iteration : 1610
train acc:  0.75
train loss:  0.5324394106864929
train gradient:  0.13588745028303983
iteration : 1611
train acc:  0.7109375
train loss:  0.5539828538894653
train gradient:  0.22808717036283752
iteration : 1612
train acc:  0.625
train loss:  0.6410148739814758
train gradient:  0.21280714820413676
iteration : 1613
train acc:  0.734375
train loss:  0.5250920653343201
train gradient:  0.13826112649215766
iteration : 1614
train acc:  0.703125
train loss:  0.5654662847518921
train gradient:  0.17082150889240783
iteration : 1615
train acc:  0.6875
train loss:  0.6045724153518677
train gradient:  0.14527096491599079
iteration : 1616
train acc:  0.7421875
train loss:  0.5088099241256714
train gradient:  0.18817286197342872
iteration : 1617
train acc:  0.734375
train loss:  0.547905683517456
train gradient:  0.17394728847561797
iteration : 1618
train acc:  0.6796875
train loss:  0.5546636581420898
train gradient:  0.13526799406952045
iteration : 1619
train acc:  0.75
train loss:  0.5699341297149658
train gradient:  0.24888417166982268
iteration : 1620
train acc:  0.7265625
train loss:  0.5728882551193237
train gradient:  0.23394673791868806
iteration : 1621
train acc:  0.6640625
train loss:  0.6190378665924072
train gradient:  0.2746378809887595
iteration : 1622
train acc:  0.6796875
train loss:  0.6088463068008423
train gradient:  0.1806852060449449
iteration : 1623
train acc:  0.6484375
train loss:  0.6575653553009033
train gradient:  0.2195830545918
iteration : 1624
train acc:  0.7109375
train loss:  0.522820234298706
train gradient:  0.19859926195165298
iteration : 1625
train acc:  0.71875
train loss:  0.5658400058746338
train gradient:  0.18381860366833258
iteration : 1626
train acc:  0.7265625
train loss:  0.5451763868331909
train gradient:  0.1911690348134918
iteration : 1627
train acc:  0.6640625
train loss:  0.5639375448226929
train gradient:  0.22513750716876224
iteration : 1628
train acc:  0.734375
train loss:  0.5092822313308716
train gradient:  0.15321644490017602
iteration : 1629
train acc:  0.640625
train loss:  0.6252777576446533
train gradient:  0.19502083479179358
iteration : 1630
train acc:  0.6640625
train loss:  0.5921767950057983
train gradient:  0.18038028758180885
iteration : 1631
train acc:  0.65625
train loss:  0.643639326095581
train gradient:  0.18823866345185766
iteration : 1632
train acc:  0.6796875
train loss:  0.5511025786399841
train gradient:  0.2410960058909889
iteration : 1633
train acc:  0.734375
train loss:  0.5150448083877563
train gradient:  0.1623814504822445
iteration : 1634
train acc:  0.65625
train loss:  0.6318033933639526
train gradient:  0.24400789545532994
iteration : 1635
train acc:  0.640625
train loss:  0.6224403381347656
train gradient:  0.2059315268962933
iteration : 1636
train acc:  0.703125
train loss:  0.5563952922821045
train gradient:  0.16228245958297516
iteration : 1637
train acc:  0.6875
train loss:  0.5865815877914429
train gradient:  0.19419567561543088
iteration : 1638
train acc:  0.703125
train loss:  0.5073566436767578
train gradient:  0.1561558938291278
iteration : 1639
train acc:  0.6953125
train loss:  0.5857810378074646
train gradient:  0.22525120275168042
iteration : 1640
train acc:  0.65625
train loss:  0.6087686419487
train gradient:  0.19516532976318296
iteration : 1641
train acc:  0.703125
train loss:  0.5806929469108582
train gradient:  0.17808952895836302
iteration : 1642
train acc:  0.7578125
train loss:  0.5063319802284241
train gradient:  0.1518600903529007
iteration : 1643
train acc:  0.6875
train loss:  0.6013688445091248
train gradient:  0.2420636206971085
iteration : 1644
train acc:  0.6640625
train loss:  0.5980358123779297
train gradient:  0.22611216140895535
iteration : 1645
train acc:  0.7109375
train loss:  0.5355116724967957
train gradient:  0.1536276732538404
iteration : 1646
train acc:  0.7265625
train loss:  0.5314313173294067
train gradient:  0.1318039075851105
iteration : 1647
train acc:  0.734375
train loss:  0.5149407386779785
train gradient:  0.13771360066069627
iteration : 1648
train acc:  0.65625
train loss:  0.5752850770950317
train gradient:  0.2063806246055636
iteration : 1649
train acc:  0.65625
train loss:  0.5552077293395996
train gradient:  0.18423361451728054
iteration : 1650
train acc:  0.65625
train loss:  0.5642372369766235
train gradient:  0.1825609897073091
iteration : 1651
train acc:  0.703125
train loss:  0.5591301321983337
train gradient:  0.20362578522737246
iteration : 1652
train acc:  0.7265625
train loss:  0.5339444875717163
train gradient:  0.17278664533407628
iteration : 1653
train acc:  0.6171875
train loss:  0.616205632686615
train gradient:  0.21171321720743186
iteration : 1654
train acc:  0.625
train loss:  0.593609094619751
train gradient:  0.15220833831564778
iteration : 1655
train acc:  0.7265625
train loss:  0.5701209902763367
train gradient:  0.18302267993620203
iteration : 1656
train acc:  0.6953125
train loss:  0.5642826557159424
train gradient:  0.16938935785429762
iteration : 1657
train acc:  0.7421875
train loss:  0.5000601410865784
train gradient:  0.15457300302722504
iteration : 1658
train acc:  0.6484375
train loss:  0.6221106052398682
train gradient:  0.20951271802588245
iteration : 1659
train acc:  0.703125
train loss:  0.5271227359771729
train gradient:  0.12384594655309596
iteration : 1660
train acc:  0.6796875
train loss:  0.5764005184173584
train gradient:  0.17448589422705288
iteration : 1661
train acc:  0.75
train loss:  0.5118622779846191
train gradient:  0.14850801426790386
iteration : 1662
train acc:  0.71875
train loss:  0.5495617985725403
train gradient:  0.17119846326259208
iteration : 1663
train acc:  0.6796875
train loss:  0.5391525030136108
train gradient:  0.12730478025573594
iteration : 1664
train acc:  0.7734375
train loss:  0.5152366161346436
train gradient:  0.16632085607572888
iteration : 1665
train acc:  0.671875
train loss:  0.5617889165878296
train gradient:  0.1628263272762778
iteration : 1666
train acc:  0.7265625
train loss:  0.5564519166946411
train gradient:  0.18137844081786136
iteration : 1667
train acc:  0.7265625
train loss:  0.5355280637741089
train gradient:  0.2198329899107112
iteration : 1668
train acc:  0.703125
train loss:  0.5770670771598816
train gradient:  0.2030162421351332
iteration : 1669
train acc:  0.703125
train loss:  0.5226179957389832
train gradient:  0.19474422694339077
iteration : 1670
train acc:  0.71875
train loss:  0.5465667843818665
train gradient:  0.2524654538945701
iteration : 1671
train acc:  0.640625
train loss:  0.5878959894180298
train gradient:  0.19884960829907233
iteration : 1672
train acc:  0.6875
train loss:  0.5361346006393433
train gradient:  0.1871453537558001
iteration : 1673
train acc:  0.703125
train loss:  0.5232666730880737
train gradient:  0.1414674218925391
iteration : 1674
train acc:  0.6328125
train loss:  0.6282809376716614
train gradient:  0.22859728198776966
iteration : 1675
train acc:  0.6796875
train loss:  0.5404629707336426
train gradient:  0.14167589639906183
iteration : 1676
train acc:  0.6640625
train loss:  0.5775718688964844
train gradient:  0.16899917072452433
iteration : 1677
train acc:  0.7578125
train loss:  0.5120587348937988
train gradient:  0.13112606225490148
iteration : 1678
train acc:  0.625
train loss:  0.6177065372467041
train gradient:  0.25519497489203125
iteration : 1679
train acc:  0.6953125
train loss:  0.5699366331100464
train gradient:  0.17411175784883898
iteration : 1680
train acc:  0.7109375
train loss:  0.5722498297691345
train gradient:  0.19009835820495033
iteration : 1681
train acc:  0.65625
train loss:  0.5630700588226318
train gradient:  0.22644402135450542
iteration : 1682
train acc:  0.7265625
train loss:  0.544974148273468
train gradient:  0.19365696768403712
iteration : 1683
train acc:  0.6484375
train loss:  0.6072235107421875
train gradient:  0.23089567606511618
iteration : 1684
train acc:  0.6875
train loss:  0.5725967288017273
train gradient:  0.1362159849670242
iteration : 1685
train acc:  0.75
train loss:  0.5478198528289795
train gradient:  0.15496944020505427
iteration : 1686
train acc:  0.7109375
train loss:  0.5062171816825867
train gradient:  0.14438338111994603
iteration : 1687
train acc:  0.6875
train loss:  0.5607327818870544
train gradient:  0.21354766828898528
iteration : 1688
train acc:  0.7421875
train loss:  0.5186629295349121
train gradient:  0.15365861822566051
iteration : 1689
train acc:  0.7265625
train loss:  0.5268398523330688
train gradient:  0.16047818124848068
iteration : 1690
train acc:  0.7109375
train loss:  0.535813570022583
train gradient:  0.163255201357763
iteration : 1691
train acc:  0.71875
train loss:  0.5239332318305969
train gradient:  0.1247450453740719
iteration : 1692
train acc:  0.7578125
train loss:  0.49358510971069336
train gradient:  0.1316447635991765
iteration : 1693
train acc:  0.6796875
train loss:  0.5583418607711792
train gradient:  0.16096865637353253
iteration : 1694
train acc:  0.7265625
train loss:  0.5330447554588318
train gradient:  0.18093110467329665
iteration : 1695
train acc:  0.671875
train loss:  0.5693173408508301
train gradient:  0.15126883451559556
iteration : 1696
train acc:  0.6171875
train loss:  0.582414448261261
train gradient:  0.1689536874285447
iteration : 1697
train acc:  0.6796875
train loss:  0.5565647482872009
train gradient:  0.15340835269479808
iteration : 1698
train acc:  0.734375
train loss:  0.5154104232788086
train gradient:  0.14402593867191177
iteration : 1699
train acc:  0.703125
train loss:  0.5378837585449219
train gradient:  0.1799272227914298
iteration : 1700
train acc:  0.7109375
train loss:  0.5078229904174805
train gradient:  0.1345251178095972
iteration : 1701
train acc:  0.6796875
train loss:  0.5263457894325256
train gradient:  0.2416076519995603
iteration : 1702
train acc:  0.671875
train loss:  0.5870085954666138
train gradient:  0.20859704432081289
iteration : 1703
train acc:  0.640625
train loss:  0.6278359293937683
train gradient:  0.22277116396694135
iteration : 1704
train acc:  0.6640625
train loss:  0.5702205896377563
train gradient:  0.17455131827550013
iteration : 1705
train acc:  0.6875
train loss:  0.5438346862792969
train gradient:  0.3482506118753515
iteration : 1706
train acc:  0.6796875
train loss:  0.5375353693962097
train gradient:  0.14784518122972334
iteration : 1707
train acc:  0.6640625
train loss:  0.6175054907798767
train gradient:  0.19465683328239697
iteration : 1708
train acc:  0.671875
train loss:  0.5720069408416748
train gradient:  0.17726208713276093
iteration : 1709
train acc:  0.65625
train loss:  0.6306732892990112
train gradient:  0.22295180569455292
iteration : 1710
train acc:  0.7421875
train loss:  0.5011774301528931
train gradient:  0.15332065241585122
iteration : 1711
train acc:  0.7109375
train loss:  0.5878232717514038
train gradient:  0.18287171024871324
iteration : 1712
train acc:  0.6484375
train loss:  0.5955682992935181
train gradient:  0.1690304004340818
iteration : 1713
train acc:  0.6796875
train loss:  0.5604639053344727
train gradient:  0.1707390570379026
iteration : 1714
train acc:  0.703125
train loss:  0.5250582695007324
train gradient:  0.16765641898838596
iteration : 1715
train acc:  0.7109375
train loss:  0.5533515810966492
train gradient:  0.16165584612838552
iteration : 1716
train acc:  0.7578125
train loss:  0.528885006904602
train gradient:  0.15519510861304647
iteration : 1717
train acc:  0.734375
train loss:  0.5000938177108765
train gradient:  0.14663662485282786
iteration : 1718
train acc:  0.703125
train loss:  0.5588591694831848
train gradient:  0.22798482956798832
iteration : 1719
train acc:  0.75
train loss:  0.5209897756576538
train gradient:  0.146925209927681
iteration : 1720
train acc:  0.71875
train loss:  0.5444329977035522
train gradient:  0.14836835988348723
iteration : 1721
train acc:  0.7265625
train loss:  0.5326694250106812
train gradient:  0.13958298250630863
iteration : 1722
train acc:  0.6640625
train loss:  0.5498960018157959
train gradient:  0.14039821288710896
iteration : 1723
train acc:  0.6484375
train loss:  0.6076303720474243
train gradient:  0.31364441454251807
iteration : 1724
train acc:  0.6953125
train loss:  0.6039345264434814
train gradient:  0.18652440796545924
iteration : 1725
train acc:  0.7109375
train loss:  0.5499044060707092
train gradient:  0.14892830609756752
iteration : 1726
train acc:  0.6171875
train loss:  0.6095477342605591
train gradient:  0.18107700301120777
iteration : 1727
train acc:  0.7421875
train loss:  0.5287551283836365
train gradient:  0.20394879279004458
iteration : 1728
train acc:  0.7421875
train loss:  0.54255211353302
train gradient:  0.14597794185773877
iteration : 1729
train acc:  0.703125
train loss:  0.5087571144104004
train gradient:  0.14162884441230061
iteration : 1730
train acc:  0.71875
train loss:  0.558986485004425
train gradient:  0.17045743947713948
iteration : 1731
train acc:  0.7109375
train loss:  0.5830421447753906
train gradient:  0.19143410218312018
iteration : 1732
train acc:  0.671875
train loss:  0.5453437566757202
train gradient:  0.1507507580393975
iteration : 1733
train acc:  0.7109375
train loss:  0.5710121393203735
train gradient:  0.27863992802073123
iteration : 1734
train acc:  0.6875
train loss:  0.5717417001724243
train gradient:  0.14334642748076692
iteration : 1735
train acc:  0.734375
train loss:  0.5535717010498047
train gradient:  0.1533407919060648
iteration : 1736
train acc:  0.625
train loss:  0.6052621603012085
train gradient:  0.18427463222511026
iteration : 1737
train acc:  0.7421875
train loss:  0.4947049617767334
train gradient:  0.18796236092076346
iteration : 1738
train acc:  0.71875
train loss:  0.5680835247039795
train gradient:  0.15290054598536607
iteration : 1739
train acc:  0.765625
train loss:  0.537147045135498
train gradient:  0.18357248837469803
iteration : 1740
train acc:  0.7265625
train loss:  0.564279317855835
train gradient:  0.1485094644034784
iteration : 1741
train acc:  0.703125
train loss:  0.5749826431274414
train gradient:  0.1907846950481024
iteration : 1742
train acc:  0.65625
train loss:  0.6127159595489502
train gradient:  0.18235153487281963
iteration : 1743
train acc:  0.703125
train loss:  0.5074905157089233
train gradient:  0.13025135144675543
iteration : 1744
train acc:  0.6875
train loss:  0.5423685908317566
train gradient:  0.19922708759219487
iteration : 1745
train acc:  0.6875
train loss:  0.5579630136489868
train gradient:  0.130136612988724
iteration : 1746
train acc:  0.6953125
train loss:  0.5711548328399658
train gradient:  0.1983737295728597
iteration : 1747
train acc:  0.7578125
train loss:  0.5151667594909668
train gradient:  0.1842614879975974
iteration : 1748
train acc:  0.7109375
train loss:  0.5232185125350952
train gradient:  0.14174720560329218
iteration : 1749
train acc:  0.765625
train loss:  0.4828284978866577
train gradient:  0.12255206391181225
iteration : 1750
train acc:  0.7890625
train loss:  0.48687756061553955
train gradient:  0.12333625382978708
iteration : 1751
train acc:  0.6328125
train loss:  0.563557505607605
train gradient:  0.15105035682908063
iteration : 1752
train acc:  0.6953125
train loss:  0.6224098205566406
train gradient:  0.23989950214924766
iteration : 1753
train acc:  0.8203125
train loss:  0.4856548607349396
train gradient:  0.1824196574423154
iteration : 1754
train acc:  0.75
train loss:  0.5063192844390869
train gradient:  0.1800009960967696
iteration : 1755
train acc:  0.6875
train loss:  0.5435981750488281
train gradient:  0.1527827777623713
iteration : 1756
train acc:  0.7265625
train loss:  0.49849748611450195
train gradient:  0.1450958852437989
iteration : 1757
train acc:  0.6328125
train loss:  0.6211085319519043
train gradient:  0.19143332086992232
iteration : 1758
train acc:  0.7265625
train loss:  0.5815727710723877
train gradient:  0.2529694661483793
iteration : 1759
train acc:  0.734375
train loss:  0.5635546445846558
train gradient:  0.18222110140331843
iteration : 1760
train acc:  0.6875
train loss:  0.5603330731391907
train gradient:  0.15341413842014148
iteration : 1761
train acc:  0.671875
train loss:  0.6076791286468506
train gradient:  0.28597585187082475
iteration : 1762
train acc:  0.7109375
train loss:  0.543829083442688
train gradient:  0.13324221171420716
iteration : 1763
train acc:  0.703125
train loss:  0.5902355313301086
train gradient:  0.16558211431663428
iteration : 1764
train acc:  0.6953125
train loss:  0.5093181133270264
train gradient:  0.17008107441972808
iteration : 1765
train acc:  0.6953125
train loss:  0.5259899497032166
train gradient:  0.13820180862357836
iteration : 1766
train acc:  0.7265625
train loss:  0.5255541205406189
train gradient:  0.15512790380709035
iteration : 1767
train acc:  0.734375
train loss:  0.5332666635513306
train gradient:  0.1944136835463322
iteration : 1768
train acc:  0.640625
train loss:  0.6177534461021423
train gradient:  0.20563141017976458
iteration : 1769
train acc:  0.6875
train loss:  0.5874742865562439
train gradient:  0.22773159141767674
iteration : 1770
train acc:  0.6953125
train loss:  0.557335615158081
train gradient:  0.17535597389655566
iteration : 1771
train acc:  0.671875
train loss:  0.5662174820899963
train gradient:  0.19334138527570693
iteration : 1772
train acc:  0.796875
train loss:  0.47771090269088745
train gradient:  0.19024270997550302
iteration : 1773
train acc:  0.765625
train loss:  0.5026273727416992
train gradient:  0.19097877466480462
iteration : 1774
train acc:  0.703125
train loss:  0.575802206993103
train gradient:  0.15953851798227606
iteration : 1775
train acc:  0.6953125
train loss:  0.5760007500648499
train gradient:  0.21081122513696798
iteration : 1776
train acc:  0.6171875
train loss:  0.6192207932472229
train gradient:  0.2110828070414999
iteration : 1777
train acc:  0.7109375
train loss:  0.5983493328094482
train gradient:  0.2407500769115989
iteration : 1778
train acc:  0.65625
train loss:  0.5830878615379333
train gradient:  0.2307014190150759
iteration : 1779
train acc:  0.6953125
train loss:  0.5152992010116577
train gradient:  0.15581141999493545
iteration : 1780
train acc:  0.6953125
train loss:  0.5474526286125183
train gradient:  0.15985812574122732
iteration : 1781
train acc:  0.7734375
train loss:  0.518471896648407
train gradient:  0.13482124519794858
iteration : 1782
train acc:  0.7109375
train loss:  0.5594222545623779
train gradient:  0.1352087904688054
iteration : 1783
train acc:  0.671875
train loss:  0.5492777824401855
train gradient:  0.1635991871042764
iteration : 1784
train acc:  0.7890625
train loss:  0.458278626203537
train gradient:  0.1336960220006548
iteration : 1785
train acc:  0.6328125
train loss:  0.5634802579879761
train gradient:  0.14355613542776882
iteration : 1786
train acc:  0.71875
train loss:  0.5410478115081787
train gradient:  0.16886236410372024
iteration : 1787
train acc:  0.734375
train loss:  0.5049546360969543
train gradient:  0.14382953571282364
iteration : 1788
train acc:  0.71875
train loss:  0.501721203327179
train gradient:  0.12973517489982708
iteration : 1789
train acc:  0.671875
train loss:  0.5638216137886047
train gradient:  0.161754145539386
iteration : 1790
train acc:  0.75
train loss:  0.5268734097480774
train gradient:  0.12228642872092203
iteration : 1791
train acc:  0.6953125
train loss:  0.5244480967521667
train gradient:  0.14941440217840543
iteration : 1792
train acc:  0.7734375
train loss:  0.4799790680408478
train gradient:  0.1506317522074102
iteration : 1793
train acc:  0.7421875
train loss:  0.5464363694190979
train gradient:  0.1584418663997446
iteration : 1794
train acc:  0.6875
train loss:  0.552360475063324
train gradient:  0.16835851832905724
iteration : 1795
train acc:  0.7421875
train loss:  0.5041294097900391
train gradient:  0.12505856754173839
iteration : 1796
train acc:  0.671875
train loss:  0.5707533955574036
train gradient:  0.15441202429096462
iteration : 1797
train acc:  0.640625
train loss:  0.5475464463233948
train gradient:  0.21920678754937745
iteration : 1798
train acc:  0.7265625
train loss:  0.4986093044281006
train gradient:  0.15968573075259634
iteration : 1799
train acc:  0.671875
train loss:  0.6118655204772949
train gradient:  0.20635851014662449
iteration : 1800
train acc:  0.6875
train loss:  0.5485329031944275
train gradient:  0.23456117478135857
iteration : 1801
train acc:  0.6640625
train loss:  0.5765624046325684
train gradient:  0.19747755576829729
iteration : 1802
train acc:  0.734375
train loss:  0.5202676653862
train gradient:  0.12652504262445144
iteration : 1803
train acc:  0.7578125
train loss:  0.4966713488101959
train gradient:  0.14188932626006123
iteration : 1804
train acc:  0.6796875
train loss:  0.5788141489028931
train gradient:  0.17919752473220282
iteration : 1805
train acc:  0.7265625
train loss:  0.5590634346008301
train gradient:  0.16689281926416255
iteration : 1806
train acc:  0.7578125
train loss:  0.5473858714103699
train gradient:  0.13129790699361707
iteration : 1807
train acc:  0.6953125
train loss:  0.5405372381210327
train gradient:  0.12885652598903632
iteration : 1808
train acc:  0.6796875
train loss:  0.608318030834198
train gradient:  0.16048567157659832
iteration : 1809
train acc:  0.734375
train loss:  0.5276707410812378
train gradient:  0.15785041939573125
iteration : 1810
train acc:  0.6796875
train loss:  0.5803756713867188
train gradient:  0.1932328879905327
iteration : 1811
train acc:  0.65625
train loss:  0.601915180683136
train gradient:  0.20847095210642186
iteration : 1812
train acc:  0.7265625
train loss:  0.5286084413528442
train gradient:  0.14597360928283404
iteration : 1813
train acc:  0.6796875
train loss:  0.5681367516517639
train gradient:  0.22790331091567584
iteration : 1814
train acc:  0.6171875
train loss:  0.6478614807128906
train gradient:  0.2188988526159895
iteration : 1815
train acc:  0.765625
train loss:  0.5417253971099854
train gradient:  0.14687305412289803
iteration : 1816
train acc:  0.75
train loss:  0.520072340965271
train gradient:  0.14544510666453875
iteration : 1817
train acc:  0.6640625
train loss:  0.5626559257507324
train gradient:  0.1449507468846012
iteration : 1818
train acc:  0.71875
train loss:  0.5568490624427795
train gradient:  0.16667625106205977
iteration : 1819
train acc:  0.6953125
train loss:  0.5164250731468201
train gradient:  0.18102196642500723
iteration : 1820
train acc:  0.7265625
train loss:  0.508851170539856
train gradient:  0.1326335962095246
iteration : 1821
train acc:  0.6640625
train loss:  0.5902838706970215
train gradient:  0.172804354876541
iteration : 1822
train acc:  0.703125
train loss:  0.5448209047317505
train gradient:  0.15205390454715104
iteration : 1823
train acc:  0.71875
train loss:  0.5480687022209167
train gradient:  0.16818947581227134
iteration : 1824
train acc:  0.703125
train loss:  0.5514401197433472
train gradient:  0.13472302515023876
iteration : 1825
train acc:  0.7265625
train loss:  0.5068269371986389
train gradient:  0.14637555614751313
iteration : 1826
train acc:  0.734375
train loss:  0.5573099851608276
train gradient:  0.20142286656240826
iteration : 1827
train acc:  0.6328125
train loss:  0.5713097453117371
train gradient:  0.21670223985271755
iteration : 1828
train acc:  0.65625
train loss:  0.5915149450302124
train gradient:  0.3112273493847898
iteration : 1829
train acc:  0.703125
train loss:  0.5101518034934998
train gradient:  0.144064817799807
iteration : 1830
train acc:  0.7109375
train loss:  0.5580877065658569
train gradient:  0.13299956920770878
iteration : 1831
train acc:  0.6953125
train loss:  0.5997129678726196
train gradient:  0.1811771385654996
iteration : 1832
train acc:  0.6953125
train loss:  0.5530474781990051
train gradient:  0.17393373372162335
iteration : 1833
train acc:  0.6484375
train loss:  0.6193254590034485
train gradient:  0.2639267300004259
iteration : 1834
train acc:  0.671875
train loss:  0.6291263103485107
train gradient:  0.23722894647406345
iteration : 1835
train acc:  0.734375
train loss:  0.5281184315681458
train gradient:  0.15599429317636349
iteration : 1836
train acc:  0.765625
train loss:  0.5179957747459412
train gradient:  0.15385076995340852
iteration : 1837
train acc:  0.734375
train loss:  0.5167869329452515
train gradient:  0.12693360303988743
iteration : 1838
train acc:  0.6640625
train loss:  0.5640614032745361
train gradient:  0.23094576189069438
iteration : 1839
train acc:  0.765625
train loss:  0.5156301856040955
train gradient:  0.16620740434378728
iteration : 1840
train acc:  0.7421875
train loss:  0.5032440423965454
train gradient:  0.2053611278803526
iteration : 1841
train acc:  0.7421875
train loss:  0.5230145454406738
train gradient:  0.20052762258230272
iteration : 1842
train acc:  0.671875
train loss:  0.5984562635421753
train gradient:  0.24039904654827338
iteration : 1843
train acc:  0.7734375
train loss:  0.5287039279937744
train gradient:  0.1318550247179208
iteration : 1844
train acc:  0.71875
train loss:  0.5385398268699646
train gradient:  0.14139804226822617
iteration : 1845
train acc:  0.7421875
train loss:  0.5495730042457581
train gradient:  0.21774637257980073
iteration : 1846
train acc:  0.7109375
train loss:  0.5436977744102478
train gradient:  0.17801643352484292
iteration : 1847
train acc:  0.6796875
train loss:  0.5826855301856995
train gradient:  0.22109908679283768
iteration : 1848
train acc:  0.7734375
train loss:  0.5034047365188599
train gradient:  0.20840076103109917
iteration : 1849
train acc:  0.609375
train loss:  0.6210413575172424
train gradient:  0.23162763069527587
iteration : 1850
train acc:  0.703125
train loss:  0.531151533126831
train gradient:  0.20963587511486514
iteration : 1851
train acc:  0.6640625
train loss:  0.574584424495697
train gradient:  0.21301135171733376
iteration : 1852
train acc:  0.671875
train loss:  0.5724388957023621
train gradient:  0.1830301207496906
iteration : 1853
train acc:  0.6796875
train loss:  0.5800375938415527
train gradient:  0.15636249633458701
iteration : 1854
train acc:  0.6953125
train loss:  0.5030477046966553
train gradient:  0.10505298381796095
iteration : 1855
train acc:  0.671875
train loss:  0.5981364250183105
train gradient:  0.16755928047633226
iteration : 1856
train acc:  0.734375
train loss:  0.5149282217025757
train gradient:  0.14395318085461384
iteration : 1857
train acc:  0.671875
train loss:  0.5769376754760742
train gradient:  0.20479573038073035
iteration : 1858
train acc:  0.71875
train loss:  0.5328099727630615
train gradient:  0.14536564533898355
iteration : 1859
train acc:  0.7265625
train loss:  0.5312387347221375
train gradient:  0.15455772871404488
iteration : 1860
train acc:  0.7109375
train loss:  0.5031939744949341
train gradient:  0.12382425375057814
iteration : 1861
train acc:  0.7734375
train loss:  0.5216202735900879
train gradient:  0.18967780633245485
iteration : 1862
train acc:  0.7578125
train loss:  0.48657262325286865
train gradient:  0.12925774367057352
iteration : 1863
train acc:  0.7265625
train loss:  0.5133182406425476
train gradient:  0.11275076696404195
iteration : 1864
train acc:  0.6796875
train loss:  0.5460613965988159
train gradient:  0.1573778853180975
iteration : 1865
train acc:  0.7421875
train loss:  0.48122650384902954
train gradient:  0.2059828001828292
iteration : 1866
train acc:  0.71875
train loss:  0.5054582357406616
train gradient:  0.18854292886439844
iteration : 1867
train acc:  0.75
train loss:  0.48473042249679565
train gradient:  0.1464119389077854
iteration : 1868
train acc:  0.75
train loss:  0.5010131597518921
train gradient:  0.13307571960815184
iteration : 1869
train acc:  0.7265625
train loss:  0.5759092569351196
train gradient:  0.21087529331000027
iteration : 1870
train acc:  0.65625
train loss:  0.5677182078361511
train gradient:  0.21180829787151892
iteration : 1871
train acc:  0.71875
train loss:  0.504622220993042
train gradient:  0.14797502911732563
iteration : 1872
train acc:  0.6875
train loss:  0.5713763236999512
train gradient:  0.19871348185822155
iteration : 1873
train acc:  0.734375
train loss:  0.5200270414352417
train gradient:  0.13510968964394057
iteration : 1874
train acc:  0.6875
train loss:  0.5885075926780701
train gradient:  0.28026978864294577
iteration : 1875
train acc:  0.7578125
train loss:  0.49762392044067383
train gradient:  0.13448810866963415
iteration : 1876
train acc:  0.6953125
train loss:  0.5793671607971191
train gradient:  0.21459763956440375
iteration : 1877
train acc:  0.7109375
train loss:  0.5267314314842224
train gradient:  0.1381783280870838
iteration : 1878
train acc:  0.6015625
train loss:  0.6294692158699036
train gradient:  0.2302280894530275
iteration : 1879
train acc:  0.703125
train loss:  0.5116965174674988
train gradient:  0.15421135262650615
iteration : 1880
train acc:  0.625
train loss:  0.6013907194137573
train gradient:  0.1717339040850263
iteration : 1881
train acc:  0.65625
train loss:  0.554999828338623
train gradient:  0.16381539590843192
iteration : 1882
train acc:  0.71875
train loss:  0.5009015798568726
train gradient:  0.16107811957106313
iteration : 1883
train acc:  0.6015625
train loss:  0.6456258296966553
train gradient:  0.2466955955994432
iteration : 1884
train acc:  0.6015625
train loss:  0.6527912616729736
train gradient:  0.2090580482697725
iteration : 1885
train acc:  0.7578125
train loss:  0.5142837762832642
train gradient:  0.13623120647952675
iteration : 1886
train acc:  0.703125
train loss:  0.5052255392074585
train gradient:  0.15567341977796506
iteration : 1887
train acc:  0.7734375
train loss:  0.544767439365387
train gradient:  0.1386776350771661
iteration : 1888
train acc:  0.6875
train loss:  0.5818007588386536
train gradient:  0.2134931359987416
iteration : 1889
train acc:  0.7578125
train loss:  0.5476931929588318
train gradient:  0.17200359482022842
iteration : 1890
train acc:  0.734375
train loss:  0.5220151543617249
train gradient:  0.14994094337847055
iteration : 1891
train acc:  0.6875
train loss:  0.5611106157302856
train gradient:  0.19198458927832476
iteration : 1892
train acc:  0.6796875
train loss:  0.5427473783493042
train gradient:  0.19332691949376873
iteration : 1893
train acc:  0.6328125
train loss:  0.5929693579673767
train gradient:  0.21685195770839877
iteration : 1894
train acc:  0.7421875
train loss:  0.5553482174873352
train gradient:  0.1540731407284428
iteration : 1895
train acc:  0.8203125
train loss:  0.4621482789516449
train gradient:  0.13334260841403014
iteration : 1896
train acc:  0.75
train loss:  0.4897516071796417
train gradient:  0.17024829320775675
iteration : 1897
train acc:  0.7265625
train loss:  0.5303040742874146
train gradient:  0.15753270981399348
iteration : 1898
train acc:  0.7265625
train loss:  0.5317556858062744
train gradient:  0.19950466564725483
iteration : 1899
train acc:  0.7265625
train loss:  0.5262893438339233
train gradient:  0.3193270364392118
iteration : 1900
train acc:  0.7890625
train loss:  0.49210187792778015
train gradient:  0.16548074525176587
iteration : 1901
train acc:  0.7109375
train loss:  0.5455577373504639
train gradient:  0.14638562630590551
iteration : 1902
train acc:  0.703125
train loss:  0.5176169872283936
train gradient:  0.1779917672139876
iteration : 1903
train acc:  0.7421875
train loss:  0.5331717729568481
train gradient:  0.12811610553406827
iteration : 1904
train acc:  0.671875
train loss:  0.5944201350212097
train gradient:  0.22341223446340913
iteration : 1905
train acc:  0.765625
train loss:  0.517487645149231
train gradient:  0.17958504019898353
iteration : 1906
train acc:  0.6640625
train loss:  0.5522533059120178
train gradient:  0.13454574975928024
iteration : 1907
train acc:  0.671875
train loss:  0.5547579526901245
train gradient:  0.14343649783804566
iteration : 1908
train acc:  0.6796875
train loss:  0.5360608100891113
train gradient:  0.13649650074902203
iteration : 1909
train acc:  0.6328125
train loss:  0.6226494312286377
train gradient:  0.27175250904322856
iteration : 1910
train acc:  0.6875
train loss:  0.5978214740753174
train gradient:  0.26851033354722853
iteration : 1911
train acc:  0.6875
train loss:  0.5711549520492554
train gradient:  0.15453326472158618
iteration : 1912
train acc:  0.6953125
train loss:  0.5447039604187012
train gradient:  0.2296564594498678
iteration : 1913
train acc:  0.6875
train loss:  0.5433218479156494
train gradient:  0.15510502813763127
iteration : 1914
train acc:  0.65625
train loss:  0.5907134413719177
train gradient:  0.2469078688887414
iteration : 1915
train acc:  0.6953125
train loss:  0.5493820905685425
train gradient:  0.1894143994846213
iteration : 1916
train acc:  0.7109375
train loss:  0.5206097364425659
train gradient:  0.19496914572941257
iteration : 1917
train acc:  0.71875
train loss:  0.5711487531661987
train gradient:  0.1624211517210736
iteration : 1918
train acc:  0.703125
train loss:  0.6012670993804932
train gradient:  0.19611712536390807
iteration : 1919
train acc:  0.7421875
train loss:  0.4917401671409607
train gradient:  0.13721571984856698
iteration : 1920
train acc:  0.7265625
train loss:  0.5344421863555908
train gradient:  0.1294048505144515
iteration : 1921
train acc:  0.6796875
train loss:  0.5806800127029419
train gradient:  0.2197066340248976
iteration : 1922
train acc:  0.6953125
train loss:  0.5908087491989136
train gradient:  0.17661942132624092
iteration : 1923
train acc:  0.75
train loss:  0.510050892829895
train gradient:  0.12642572412055092
iteration : 1924
train acc:  0.7421875
train loss:  0.507061243057251
train gradient:  0.1385291599233542
iteration : 1925
train acc:  0.703125
train loss:  0.5333583354949951
train gradient:  0.14667386034972701
iteration : 1926
train acc:  0.6953125
train loss:  0.5856661796569824
train gradient:  0.16381630082622975
iteration : 1927
train acc:  0.7265625
train loss:  0.5738092660903931
train gradient:  0.2614343880894506
iteration : 1928
train acc:  0.671875
train loss:  0.5914593935012817
train gradient:  0.26692143535322094
iteration : 1929
train acc:  0.71875
train loss:  0.5230345726013184
train gradient:  0.14127866973874276
iteration : 1930
train acc:  0.6953125
train loss:  0.5414836406707764
train gradient:  0.16429067316634893
iteration : 1931
train acc:  0.6484375
train loss:  0.5593962073326111
train gradient:  0.17693514455928205
iteration : 1932
train acc:  0.65625
train loss:  0.5610367059707642
train gradient:  0.19782310034741826
iteration : 1933
train acc:  0.6953125
train loss:  0.5615133047103882
train gradient:  0.140525170809563
iteration : 1934
train acc:  0.7265625
train loss:  0.50443434715271
train gradient:  0.18740573231011654
iteration : 1935
train acc:  0.71875
train loss:  0.5344043970108032
train gradient:  0.1443855944888683
iteration : 1936
train acc:  0.640625
train loss:  0.6439420580863953
train gradient:  0.21247277773140097
iteration : 1937
train acc:  0.7734375
train loss:  0.49457573890686035
train gradient:  0.13662471738919485
iteration : 1938
train acc:  0.7421875
train loss:  0.517682671546936
train gradient:  0.14077336992178002
iteration : 1939
train acc:  0.7421875
train loss:  0.48719432950019836
train gradient:  0.15052555878835655
iteration : 1940
train acc:  0.71875
train loss:  0.5384317636489868
train gradient:  0.13537970898203439
iteration : 1941
train acc:  0.71875
train loss:  0.5408029556274414
train gradient:  0.1676711877066543
iteration : 1942
train acc:  0.7109375
train loss:  0.5304895639419556
train gradient:  0.13465596891567339
iteration : 1943
train acc:  0.7109375
train loss:  0.5517557263374329
train gradient:  0.16555754042654
iteration : 1944
train acc:  0.7265625
train loss:  0.5306074619293213
train gradient:  0.17388022750588866
iteration : 1945
train acc:  0.65625
train loss:  0.5748609900474548
train gradient:  0.15754532469251004
iteration : 1946
train acc:  0.7265625
train loss:  0.5251315832138062
train gradient:  0.16331460260002179
iteration : 1947
train acc:  0.8046875
train loss:  0.4879494309425354
train gradient:  0.14581470996582935
iteration : 1948
train acc:  0.6953125
train loss:  0.5447263717651367
train gradient:  0.18608973478554008
iteration : 1949
train acc:  0.7265625
train loss:  0.5459169745445251
train gradient:  0.1666414873868588
iteration : 1950
train acc:  0.703125
train loss:  0.537481963634491
train gradient:  0.22238559460334345
iteration : 1951
train acc:  0.6953125
train loss:  0.5419172048568726
train gradient:  0.13373224950823506
iteration : 1952
train acc:  0.703125
train loss:  0.5811084508895874
train gradient:  0.22283912829114078
iteration : 1953
train acc:  0.6484375
train loss:  0.5782698392868042
train gradient:  0.16412225459430785
iteration : 1954
train acc:  0.671875
train loss:  0.5855327248573303
train gradient:  0.23633917858220943
iteration : 1955
train acc:  0.6953125
train loss:  0.574033260345459
train gradient:  0.17634799157875125
iteration : 1956
train acc:  0.7265625
train loss:  0.5582606196403503
train gradient:  0.1612066283334424
iteration : 1957
train acc:  0.71875
train loss:  0.5284048318862915
train gradient:  0.19429930607759466
iteration : 1958
train acc:  0.6875
train loss:  0.5883989334106445
train gradient:  0.23690905757016856
iteration : 1959
train acc:  0.75
train loss:  0.5123564004898071
train gradient:  0.20960806419335443
iteration : 1960
train acc:  0.6953125
train loss:  0.5631831884384155
train gradient:  0.18376894100589908
iteration : 1961
train acc:  0.734375
train loss:  0.5394415259361267
train gradient:  0.17951281133177124
iteration : 1962
train acc:  0.625
train loss:  0.6795014142990112
train gradient:  0.2180985586334371
iteration : 1963
train acc:  0.7421875
train loss:  0.524530827999115
train gradient:  0.17102865933404002
iteration : 1964
train acc:  0.6796875
train loss:  0.5500928163528442
train gradient:  0.15556867958377585
iteration : 1965
train acc:  0.71875
train loss:  0.5164591670036316
train gradient:  0.17567953966782135
iteration : 1966
train acc:  0.75
train loss:  0.5385555028915405
train gradient:  0.22463641187716282
iteration : 1967
train acc:  0.75
train loss:  0.5526408553123474
train gradient:  0.19554910229772093
iteration : 1968
train acc:  0.6796875
train loss:  0.5380722284317017
train gradient:  0.14840066589052386
iteration : 1969
train acc:  0.6875
train loss:  0.5709564685821533
train gradient:  0.14307326161473485
iteration : 1970
train acc:  0.71875
train loss:  0.5725284814834595
train gradient:  0.17434116456765014
iteration : 1971
train acc:  0.75
train loss:  0.4918528199195862
train gradient:  0.12665861063454673
iteration : 1972
train acc:  0.6796875
train loss:  0.6123367547988892
train gradient:  0.21692152934534123
iteration : 1973
train acc:  0.6875
train loss:  0.5786012411117554
train gradient:  0.1724574293044982
iteration : 1974
train acc:  0.7109375
train loss:  0.5190831422805786
train gradient:  0.16834633071959215
iteration : 1975
train acc:  0.734375
train loss:  0.5156911015510559
train gradient:  0.14998013830786577
iteration : 1976
train acc:  0.7109375
train loss:  0.5428060293197632
train gradient:  0.18768288804343403
iteration : 1977
train acc:  0.71875
train loss:  0.5221941471099854
train gradient:  0.16628862392548505
iteration : 1978
train acc:  0.6796875
train loss:  0.5652351379394531
train gradient:  0.16788988359596063
iteration : 1979
train acc:  0.6796875
train loss:  0.6459189653396606
train gradient:  0.2148709189476233
iteration : 1980
train acc:  0.7890625
train loss:  0.45329660177230835
train gradient:  0.14095289828514235
iteration : 1981
train acc:  0.765625
train loss:  0.5067706108093262
train gradient:  0.17429814814718048
iteration : 1982
train acc:  0.71875
train loss:  0.5807985663414001
train gradient:  0.1700367571194125
iteration : 1983
train acc:  0.6640625
train loss:  0.579060435295105
train gradient:  0.21410969199443006
iteration : 1984
train acc:  0.6953125
train loss:  0.5804227590560913
train gradient:  0.19027093705277326
iteration : 1985
train acc:  0.71875
train loss:  0.48974987864494324
train gradient:  0.20778157289773946
iteration : 1986
train acc:  0.75
train loss:  0.494014173746109
train gradient:  0.20774384218586936
iteration : 1987
train acc:  0.7421875
train loss:  0.5039430856704712
train gradient:  0.11362553347232193
iteration : 1988
train acc:  0.7421875
train loss:  0.5394529700279236
train gradient:  0.1882204050838242
iteration : 1989
train acc:  0.703125
train loss:  0.5375916957855225
train gradient:  0.1404575375647077
iteration : 1990
train acc:  0.75
train loss:  0.4920174479484558
train gradient:  0.1266801003586156
iteration : 1991
train acc:  0.671875
train loss:  0.5753941535949707
train gradient:  0.18276279053984176
iteration : 1992
train acc:  0.75
train loss:  0.5684922933578491
train gradient:  0.17526480107294723
iteration : 1993
train acc:  0.75
train loss:  0.46869900822639465
train gradient:  0.11301519922047634
iteration : 1994
train acc:  0.734375
train loss:  0.5024175643920898
train gradient:  0.148625447984969
iteration : 1995
train acc:  0.7109375
train loss:  0.5424423813819885
train gradient:  0.19514832533766402
iteration : 1996
train acc:  0.671875
train loss:  0.5721399784088135
train gradient:  0.15533194952928478
iteration : 1997
train acc:  0.71875
train loss:  0.5522806644439697
train gradient:  0.16211625843705424
iteration : 1998
train acc:  0.734375
train loss:  0.5111953020095825
train gradient:  0.14163806028121506
iteration : 1999
train acc:  0.71875
train loss:  0.54751056432724
train gradient:  0.1596258408246882
iteration : 2000
train acc:  0.6953125
train loss:  0.5454310178756714
train gradient:  0.1496120244350881
iteration : 2001
train acc:  0.7109375
train loss:  0.5167397260665894
train gradient:  0.1965288847264246
iteration : 2002
train acc:  0.71875
train loss:  0.5004161596298218
train gradient:  0.16540490930618962
iteration : 2003
train acc:  0.703125
train loss:  0.5806946158409119
train gradient:  0.22195504162518845
iteration : 2004
train acc:  0.671875
train loss:  0.5888895988464355
train gradient:  0.18577477406315068
iteration : 2005
train acc:  0.6796875
train loss:  0.5997289419174194
train gradient:  0.16631514370451442
iteration : 2006
train acc:  0.703125
train loss:  0.5555071830749512
train gradient:  0.1349320237774176
iteration : 2007
train acc:  0.734375
train loss:  0.5146626830101013
train gradient:  0.13725886214033736
iteration : 2008
train acc:  0.6796875
train loss:  0.5503972768783569
train gradient:  0.16523657631848376
iteration : 2009
train acc:  0.703125
train loss:  0.5550127029418945
train gradient:  0.21074036164532775
iteration : 2010
train acc:  0.7265625
train loss:  0.5298619270324707
train gradient:  0.1536477997081936
iteration : 2011
train acc:  0.7109375
train loss:  0.5692960023880005
train gradient:  0.2067680351283942
iteration : 2012
train acc:  0.7421875
train loss:  0.5172346830368042
train gradient:  0.1605241791203058
iteration : 2013
train acc:  0.7109375
train loss:  0.5398237705230713
train gradient:  0.22512303253995386
iteration : 2014
train acc:  0.6796875
train loss:  0.6234804391860962
train gradient:  0.17910103378288758
iteration : 2015
train acc:  0.7265625
train loss:  0.5733298063278198
train gradient:  0.1908134090862816
iteration : 2016
train acc:  0.6328125
train loss:  0.6065340042114258
train gradient:  0.1969695298284243
iteration : 2017
train acc:  0.671875
train loss:  0.5797771215438843
train gradient:  0.161935539957882
iteration : 2018
train acc:  0.671875
train loss:  0.5206924676895142
train gradient:  0.1161789391087763
iteration : 2019
train acc:  0.6171875
train loss:  0.5875864028930664
train gradient:  0.19835680110130963
iteration : 2020
train acc:  0.6875
train loss:  0.5626161098480225
train gradient:  0.3126045589234353
iteration : 2021
train acc:  0.7421875
train loss:  0.5029582977294922
train gradient:  0.12149377230589611
iteration : 2022
train acc:  0.7734375
train loss:  0.4876798987388611
train gradient:  0.15355956813921423
iteration : 2023
train acc:  0.6484375
train loss:  0.6357967853546143
train gradient:  0.23212143598127
iteration : 2024
train acc:  0.6328125
train loss:  0.6115089058876038
train gradient:  0.19630073834665066
iteration : 2025
train acc:  0.765625
train loss:  0.4891747236251831
train gradient:  0.13737896983139086
iteration : 2026
train acc:  0.6953125
train loss:  0.5518073439598083
train gradient:  0.17590563554599875
iteration : 2027
train acc:  0.6953125
train loss:  0.6226704716682434
train gradient:  0.2093627105346565
iteration : 2028
train acc:  0.7421875
train loss:  0.5179744958877563
train gradient:  0.14976473044040456
iteration : 2029
train acc:  0.8046875
train loss:  0.47728607058525085
train gradient:  0.12488797861438108
iteration : 2030
train acc:  0.6328125
train loss:  0.669540524482727
train gradient:  0.29233140985363326
iteration : 2031
train acc:  0.65625
train loss:  0.5966528654098511
train gradient:  0.21782301441802254
iteration : 2032
train acc:  0.734375
train loss:  0.5290478467941284
train gradient:  0.15008891933368895
iteration : 2033
train acc:  0.671875
train loss:  0.5588411092758179
train gradient:  0.14251801120190424
iteration : 2034
train acc:  0.6875
train loss:  0.5951027870178223
train gradient:  0.27413954689280023
iteration : 2035
train acc:  0.7109375
train loss:  0.5900101661682129
train gradient:  0.15096084889488598
iteration : 2036
train acc:  0.6484375
train loss:  0.575760543346405
train gradient:  0.198565349620342
iteration : 2037
train acc:  0.7578125
train loss:  0.5177606344223022
train gradient:  0.14414719041371468
iteration : 2038
train acc:  0.6796875
train loss:  0.5995028018951416
train gradient:  0.214240020533702
iteration : 2039
train acc:  0.671875
train loss:  0.6000070571899414
train gradient:  0.2499385756592465
iteration : 2040
train acc:  0.7421875
train loss:  0.5207229852676392
train gradient:  0.19527276754733308
iteration : 2041
train acc:  0.6953125
train loss:  0.6135032176971436
train gradient:  0.15872919696759052
iteration : 2042
train acc:  0.703125
train loss:  0.5104612112045288
train gradient:  0.13173565577061214
iteration : 2043
train acc:  0.71875
train loss:  0.5455414056777954
train gradient:  0.14056226178649306
iteration : 2044
train acc:  0.7109375
train loss:  0.5718809366226196
train gradient:  0.15924482718898517
iteration : 2045
train acc:  0.765625
train loss:  0.4951271712779999
train gradient:  0.16020612340443707
iteration : 2046
train acc:  0.71875
train loss:  0.577560544013977
train gradient:  0.17042105284661144
iteration : 2047
train acc:  0.6640625
train loss:  0.5725707411766052
train gradient:  0.15333118562371406
iteration : 2048
train acc:  0.6328125
train loss:  0.5870320796966553
train gradient:  0.21991819782712602
iteration : 2049
train acc:  0.671875
train loss:  0.5480480194091797
train gradient:  0.20627937089674658
iteration : 2050
train acc:  0.6484375
train loss:  0.5651792287826538
train gradient:  0.15275846005929747
iteration : 2051
train acc:  0.703125
train loss:  0.5296722054481506
train gradient:  0.1626903247683553
iteration : 2052
train acc:  0.765625
train loss:  0.49960219860076904
train gradient:  0.12776852490412344
iteration : 2053
train acc:  0.6875
train loss:  0.6177258491516113
train gradient:  0.2412666451605215
iteration : 2054
train acc:  0.6640625
train loss:  0.5560693740844727
train gradient:  0.22861809063288302
iteration : 2055
train acc:  0.65625
train loss:  0.6108363270759583
train gradient:  0.19629187591228286
iteration : 2056
train acc:  0.6796875
train loss:  0.601036787033081
train gradient:  0.21943810082182255
iteration : 2057
train acc:  0.65625
train loss:  0.5428887605667114
train gradient:  0.18700633894718982
iteration : 2058
train acc:  0.6796875
train loss:  0.5672111511230469
train gradient:  0.20680566735287548
iteration : 2059
train acc:  0.71875
train loss:  0.5199093818664551
train gradient:  0.20741937656937376
iteration : 2060
train acc:  0.7421875
train loss:  0.5076102018356323
train gradient:  0.14089393002984504
iteration : 2061
train acc:  0.6171875
train loss:  0.6367989778518677
train gradient:  0.2619733052297233
iteration : 2062
train acc:  0.7890625
train loss:  0.49137410521507263
train gradient:  0.13912464373226685
iteration : 2063
train acc:  0.7421875
train loss:  0.5167528390884399
train gradient:  0.1573314340129598
iteration : 2064
train acc:  0.7265625
train loss:  0.4834466576576233
train gradient:  0.1279031448756679
iteration : 2065
train acc:  0.703125
train loss:  0.5683335065841675
train gradient:  0.26349790144273083
iteration : 2066
train acc:  0.703125
train loss:  0.6055141687393188
train gradient:  0.19551087644112236
iteration : 2067
train acc:  0.71875
train loss:  0.5408840179443359
train gradient:  0.13590679488762464
iteration : 2068
train acc:  0.703125
train loss:  0.5883129835128784
train gradient:  0.1776675232704804
iteration : 2069
train acc:  0.671875
train loss:  0.5666259527206421
train gradient:  0.20565442420843663
iteration : 2070
train acc:  0.765625
train loss:  0.49192240834236145
train gradient:  0.12813545894028078
iteration : 2071
train acc:  0.6875
train loss:  0.5725642442703247
train gradient:  0.18356360064377492
iteration : 2072
train acc:  0.7421875
train loss:  0.5664054751396179
train gradient:  0.1672637068521644
iteration : 2073
train acc:  0.7421875
train loss:  0.5202746987342834
train gradient:  0.14553625959154948
iteration : 2074
train acc:  0.7265625
train loss:  0.5554956197738647
train gradient:  0.13721351619392702
iteration : 2075
train acc:  0.6796875
train loss:  0.547309160232544
train gradient:  0.17034052971907568
iteration : 2076
train acc:  0.6953125
train loss:  0.5694280862808228
train gradient:  0.16875502647217655
iteration : 2077
train acc:  0.71875
train loss:  0.6040134429931641
train gradient:  0.1939926186218042
iteration : 2078
train acc:  0.671875
train loss:  0.5513832569122314
train gradient:  0.19630219049689518
iteration : 2079
train acc:  0.6875
train loss:  0.5546823143959045
train gradient:  0.20434417063473004
iteration : 2080
train acc:  0.6953125
train loss:  0.5217031240463257
train gradient:  0.20031137418246162
iteration : 2081
train acc:  0.7109375
train loss:  0.5092610120773315
train gradient:  0.14239349490268843
iteration : 2082
train acc:  0.734375
train loss:  0.5198476910591125
train gradient:  0.16201081309993004
iteration : 2083
train acc:  0.71875
train loss:  0.5032820701599121
train gradient:  0.14429882470280223
iteration : 2084
train acc:  0.6796875
train loss:  0.5367282629013062
train gradient:  0.1626640987911715
iteration : 2085
train acc:  0.6640625
train loss:  0.5905991196632385
train gradient:  0.194066396506957
iteration : 2086
train acc:  0.6796875
train loss:  0.5874648094177246
train gradient:  0.2263758008991692
iteration : 2087
train acc:  0.7421875
train loss:  0.5074710845947266
train gradient:  0.13298025537764735
iteration : 2088
train acc:  0.7421875
train loss:  0.5054226517677307
train gradient:  0.1402507132862974
iteration : 2089
train acc:  0.7109375
train loss:  0.5194990038871765
train gradient:  0.16793708859370188
iteration : 2090
train acc:  0.7109375
train loss:  0.5490578413009644
train gradient:  0.24585229142477996
iteration : 2091
train acc:  0.7265625
train loss:  0.5508478879928589
train gradient:  0.20666603387694588
iteration : 2092
train acc:  0.7578125
train loss:  0.5261345505714417
train gradient:  0.15744007222867357
iteration : 2093
train acc:  0.625
train loss:  0.6399258375167847
train gradient:  0.24858092527096737
iteration : 2094
train acc:  0.7578125
train loss:  0.5130494832992554
train gradient:  0.14467694551133758
iteration : 2095
train acc:  0.7265625
train loss:  0.47363775968551636
train gradient:  0.14605711593015389
iteration : 2096
train acc:  0.703125
train loss:  0.5237988829612732
train gradient:  0.1524612179930857
iteration : 2097
train acc:  0.6953125
train loss:  0.5549497604370117
train gradient:  0.16122656843505834
iteration : 2098
train acc:  0.65625
train loss:  0.5898663401603699
train gradient:  0.22185167542494139
iteration : 2099
train acc:  0.6875
train loss:  0.610358715057373
train gradient:  0.20440173055851724
iteration : 2100
train acc:  0.7265625
train loss:  0.5442838668823242
train gradient:  0.15829766639786966
iteration : 2101
train acc:  0.71875
train loss:  0.5158230066299438
train gradient:  0.15980283174340856
iteration : 2102
train acc:  0.71875
train loss:  0.5263383388519287
train gradient:  0.15076403417505438
iteration : 2103
train acc:  0.734375
train loss:  0.5764454007148743
train gradient:  0.19503845683223592
iteration : 2104
train acc:  0.734375
train loss:  0.506385087966919
train gradient:  0.12207601465938832
iteration : 2105
train acc:  0.6796875
train loss:  0.5251889228820801
train gradient:  0.15536115933661238
iteration : 2106
train acc:  0.6953125
train loss:  0.5455478429794312
train gradient:  0.1491086051984128
iteration : 2107
train acc:  0.765625
train loss:  0.49471527338027954
train gradient:  0.12919596277389725
iteration : 2108
train acc:  0.703125
train loss:  0.5465237498283386
train gradient:  0.13731431972672864
iteration : 2109
train acc:  0.7578125
train loss:  0.5163124203681946
train gradient:  0.16612035417347154
iteration : 2110
train acc:  0.703125
train loss:  0.5484671592712402
train gradient:  0.1987997937103092
iteration : 2111
train acc:  0.703125
train loss:  0.6004177331924438
train gradient:  0.20769354842418108
iteration : 2112
train acc:  0.6953125
train loss:  0.5151365399360657
train gradient:  0.20106929551805452
iteration : 2113
train acc:  0.6875
train loss:  0.5800414085388184
train gradient:  0.14741125059529742
iteration : 2114
train acc:  0.6796875
train loss:  0.5984421968460083
train gradient:  0.19440326383582868
iteration : 2115
train acc:  0.6953125
train loss:  0.5407811999320984
train gradient:  0.16931172501279193
iteration : 2116
train acc:  0.71875
train loss:  0.5456078052520752
train gradient:  0.2039098442952974
iteration : 2117
train acc:  0.6171875
train loss:  0.5948792695999146
train gradient:  0.17389111088025647
iteration : 2118
train acc:  0.7109375
train loss:  0.534018874168396
train gradient:  0.12151466194133849
iteration : 2119
train acc:  0.734375
train loss:  0.5451553463935852
train gradient:  0.19319497487011167
iteration : 2120
train acc:  0.671875
train loss:  0.5509078502655029
train gradient:  0.2514047859736527
iteration : 2121
train acc:  0.5703125
train loss:  0.6206115484237671
train gradient:  0.2008745473138597
iteration : 2122
train acc:  0.671875
train loss:  0.5690270662307739
train gradient:  0.20307850282190792
iteration : 2123
train acc:  0.65625
train loss:  0.5834656953811646
train gradient:  0.15548397959672333
iteration : 2124
train acc:  0.6796875
train loss:  0.5407501459121704
train gradient:  0.18457292772821549
iteration : 2125
train acc:  0.7109375
train loss:  0.5373777747154236
train gradient:  0.1397029605515226
iteration : 2126
train acc:  0.703125
train loss:  0.5498855113983154
train gradient:  0.17433450750378388
iteration : 2127
train acc:  0.75
train loss:  0.5034022331237793
train gradient:  0.12578864174746643
iteration : 2128
train acc:  0.703125
train loss:  0.5608714818954468
train gradient:  0.18160313645773557
iteration : 2129
train acc:  0.75
train loss:  0.46478304266929626
train gradient:  0.1220492067498284
iteration : 2130
train acc:  0.703125
train loss:  0.5725475549697876
train gradient:  0.17521955049996557
iteration : 2131
train acc:  0.71875
train loss:  0.4935942590236664
train gradient:  0.15379091982759968
iteration : 2132
train acc:  0.765625
train loss:  0.49790680408477783
train gradient:  0.15239766545912764
iteration : 2133
train acc:  0.7109375
train loss:  0.576142430305481
train gradient:  0.1515844547617915
iteration : 2134
train acc:  0.7890625
train loss:  0.48149049282073975
train gradient:  0.1598523173084675
iteration : 2135
train acc:  0.71875
train loss:  0.5308566093444824
train gradient:  0.15615943158805218
iteration : 2136
train acc:  0.703125
train loss:  0.4777795076370239
train gradient:  0.12231013813527367
iteration : 2137
train acc:  0.7109375
train loss:  0.5884867906570435
train gradient:  0.1617140062279448
iteration : 2138
train acc:  0.75
train loss:  0.49957185983657837
train gradient:  0.16389794304585303
iteration : 2139
train acc:  0.765625
train loss:  0.502678632736206
train gradient:  0.22547951692253576
iteration : 2140
train acc:  0.7421875
train loss:  0.47989314794540405
train gradient:  0.15466242504224106
iteration : 2141
train acc:  0.5703125
train loss:  0.6909539699554443
train gradient:  0.39735080216188823
iteration : 2142
train acc:  0.75
train loss:  0.522723913192749
train gradient:  0.1615521739116307
iteration : 2143
train acc:  0.75
train loss:  0.5531283617019653
train gradient:  0.18874518839969057
iteration : 2144
train acc:  0.734375
train loss:  0.4871399998664856
train gradient:  0.1438786428572672
iteration : 2145
train acc:  0.6875
train loss:  0.6131724119186401
train gradient:  0.18831001535224737
iteration : 2146
train acc:  0.6484375
train loss:  0.6336658000946045
train gradient:  0.19957420512645307
iteration : 2147
train acc:  0.7734375
train loss:  0.5132073163986206
train gradient:  0.16205280708789993
iteration : 2148
train acc:  0.7578125
train loss:  0.4930727183818817
train gradient:  0.16674148948466583
iteration : 2149
train acc:  0.6953125
train loss:  0.5899090766906738
train gradient:  0.23407368549095303
iteration : 2150
train acc:  0.7265625
train loss:  0.5320871472358704
train gradient:  0.15908968329140533
iteration : 2151
train acc:  0.7109375
train loss:  0.5509836077690125
train gradient:  0.1490326239163574
iteration : 2152
train acc:  0.7265625
train loss:  0.5420730113983154
train gradient:  0.1740204814504746
iteration : 2153
train acc:  0.734375
train loss:  0.5494465827941895
train gradient:  0.14230127547684662
iteration : 2154
train acc:  0.71875
train loss:  0.5239837169647217
train gradient:  0.1381810032875732
iteration : 2155
train acc:  0.671875
train loss:  0.5665063858032227
train gradient:  0.3610512743147005
iteration : 2156
train acc:  0.7421875
train loss:  0.5173068046569824
train gradient:  0.15635875145136421
iteration : 2157
train acc:  0.671875
train loss:  0.5862516164779663
train gradient:  0.16648568893063048
iteration : 2158
train acc:  0.6875
train loss:  0.5606172680854797
train gradient:  0.18728573237994445
iteration : 2159
train acc:  0.7109375
train loss:  0.5192113518714905
train gradient:  0.17506887034664215
iteration : 2160
train acc:  0.703125
train loss:  0.5630894899368286
train gradient:  0.1551779009694889
iteration : 2161
train acc:  0.7109375
train loss:  0.5240365266799927
train gradient:  0.1363846372739583
iteration : 2162
train acc:  0.6953125
train loss:  0.5415166616439819
train gradient:  0.17765587370884003
iteration : 2163
train acc:  0.6171875
train loss:  0.5838025808334351
train gradient:  0.23303675177612032
iteration : 2164
train acc:  0.765625
train loss:  0.49229419231414795
train gradient:  0.12349916647722423
iteration : 2165
train acc:  0.734375
train loss:  0.5728411078453064
train gradient:  0.1585610962280857
iteration : 2166
train acc:  0.7578125
train loss:  0.5130868554115295
train gradient:  0.15802841164379255
iteration : 2167
train acc:  0.6875
train loss:  0.5748511552810669
train gradient:  0.1811180887416957
iteration : 2168
train acc:  0.703125
train loss:  0.5336217880249023
train gradient:  0.1991819787509323
iteration : 2169
train acc:  0.65625
train loss:  0.5971769094467163
train gradient:  0.17290315909222978
iteration : 2170
train acc:  0.703125
train loss:  0.5598969459533691
train gradient:  0.15550266226893233
iteration : 2171
train acc:  0.734375
train loss:  0.5026558637619019
train gradient:  0.1903201694127881
iteration : 2172
train acc:  0.640625
train loss:  0.5774521231651306
train gradient:  0.1560241681166078
iteration : 2173
train acc:  0.6796875
train loss:  0.5711719393730164
train gradient:  0.16055286761712667
iteration : 2174
train acc:  0.71875
train loss:  0.5507456660270691
train gradient:  0.143252648713975
iteration : 2175
train acc:  0.6484375
train loss:  0.6252182722091675
train gradient:  0.20510205483195315
iteration : 2176
train acc:  0.6796875
train loss:  0.5807304382324219
train gradient:  0.17468098701117185
iteration : 2177
train acc:  0.7578125
train loss:  0.5195013880729675
train gradient:  0.15823570047627789
iteration : 2178
train acc:  0.71875
train loss:  0.5616109371185303
train gradient:  0.17049114814203747
iteration : 2179
train acc:  0.6953125
train loss:  0.5705872178077698
train gradient:  0.18169138426931614
iteration : 2180
train acc:  0.7265625
train loss:  0.5077264308929443
train gradient:  0.13164143533448602
iteration : 2181
train acc:  0.6953125
train loss:  0.5916959047317505
train gradient:  0.1869720036871536
iteration : 2182
train acc:  0.6796875
train loss:  0.5584627389907837
train gradient:  0.18281358972937092
iteration : 2183
train acc:  0.6796875
train loss:  0.5481770634651184
train gradient:  0.1721567809327455
iteration : 2184
train acc:  0.6953125
train loss:  0.5470840930938721
train gradient:  0.2669454649020869
iteration : 2185
train acc:  0.78125
train loss:  0.5204215049743652
train gradient:  0.19081562224556756
iteration : 2186
train acc:  0.7109375
train loss:  0.5287553071975708
train gradient:  0.1353662009483165
iteration : 2187
train acc:  0.7109375
train loss:  0.5268592238426208
train gradient:  0.18491829317804886
iteration : 2188
train acc:  0.71875
train loss:  0.5541616678237915
train gradient:  0.18523196713958517
iteration : 2189
train acc:  0.7421875
train loss:  0.512326717376709
train gradient:  0.16555525271989402
iteration : 2190
train acc:  0.7109375
train loss:  0.5195969343185425
train gradient:  0.17623081130953955
iteration : 2191
train acc:  0.6796875
train loss:  0.5895944237709045
train gradient:  0.201021341050547
iteration : 2192
train acc:  0.6796875
train loss:  0.5867927670478821
train gradient:  0.16437743154728995
iteration : 2193
train acc:  0.6640625
train loss:  0.5884977579116821
train gradient:  0.19941933589934024
iteration : 2194
train acc:  0.765625
train loss:  0.5211435556411743
train gradient:  0.14132278449632688
iteration : 2195
train acc:  0.765625
train loss:  0.5026731491088867
train gradient:  0.18017783257577993
iteration : 2196
train acc:  0.7421875
train loss:  0.5081771612167358
train gradient:  0.1686857698666901
iteration : 2197
train acc:  0.6796875
train loss:  0.5849287509918213
train gradient:  0.20969146897365898
iteration : 2198
train acc:  0.78125
train loss:  0.49668338894844055
train gradient:  0.18073136711599097
iteration : 2199
train acc:  0.6640625
train loss:  0.5771164298057556
train gradient:  0.20074739804698935
iteration : 2200
train acc:  0.6796875
train loss:  0.5469610095024109
train gradient:  0.13031898848683882
iteration : 2201
train acc:  0.6953125
train loss:  0.5901212692260742
train gradient:  0.2332750651448765
iteration : 2202
train acc:  0.6328125
train loss:  0.6341155171394348
train gradient:  0.3133580021199805
iteration : 2203
train acc:  0.703125
train loss:  0.5670245885848999
train gradient:  0.1976677101019487
iteration : 2204
train acc:  0.7265625
train loss:  0.49571189284324646
train gradient:  0.15547274659139032
iteration : 2205
train acc:  0.6875
train loss:  0.5690017342567444
train gradient:  0.27167698441430765
iteration : 2206
train acc:  0.7578125
train loss:  0.4877142608165741
train gradient:  0.13040231210224076
iteration : 2207
train acc:  0.7734375
train loss:  0.5070720911026001
train gradient:  0.166919536755701
iteration : 2208
train acc:  0.703125
train loss:  0.5415058135986328
train gradient:  0.20056947965764668
iteration : 2209
train acc:  0.703125
train loss:  0.5640013217926025
train gradient:  0.18827381702702412
iteration : 2210
train acc:  0.7578125
train loss:  0.5113968253135681
train gradient:  0.1208605098587567
iteration : 2211
train acc:  0.75
train loss:  0.5178375840187073
train gradient:  0.1585128079234282
iteration : 2212
train acc:  0.6875
train loss:  0.5464286804199219
train gradient:  0.17855793292445735
iteration : 2213
train acc:  0.703125
train loss:  0.5085722208023071
train gradient:  0.14061656363980973
iteration : 2214
train acc:  0.7109375
train loss:  0.5350286960601807
train gradient:  0.17237598726807662
iteration : 2215
train acc:  0.7265625
train loss:  0.5202276110649109
train gradient:  0.22400446656293227
iteration : 2216
train acc:  0.6875
train loss:  0.5388031601905823
train gradient:  0.15160858242612596
iteration : 2217
train acc:  0.6796875
train loss:  0.5432628393173218
train gradient:  0.16392490430381718
iteration : 2218
train acc:  0.7109375
train loss:  0.522463858127594
train gradient:  0.13634417635472734
iteration : 2219
train acc:  0.75
train loss:  0.5226622223854065
train gradient:  0.288701830368873
iteration : 2220
train acc:  0.703125
train loss:  0.5395621061325073
train gradient:  0.20139808530835374
iteration : 2221
train acc:  0.7421875
train loss:  0.5140894651412964
train gradient:  0.1466261879464556
iteration : 2222
train acc:  0.7578125
train loss:  0.49979767203330994
train gradient:  0.14991225758740107
iteration : 2223
train acc:  0.7578125
train loss:  0.5278091430664062
train gradient:  0.15055798671613918
iteration : 2224
train acc:  0.65625
train loss:  0.58518385887146
train gradient:  0.16078363253518105
iteration : 2225
train acc:  0.75
train loss:  0.5256240367889404
train gradient:  0.1968745097265602
iteration : 2226
train acc:  0.71875
train loss:  0.5198716521263123
train gradient:  0.154869703533782
iteration : 2227
train acc:  0.7265625
train loss:  0.5022141337394714
train gradient:  0.17074113828148496
iteration : 2228
train acc:  0.7578125
train loss:  0.5072343349456787
train gradient:  0.169894529732861
iteration : 2229
train acc:  0.6953125
train loss:  0.5454757809638977
train gradient:  0.16554345115897728
iteration : 2230
train acc:  0.7109375
train loss:  0.5292631983757019
train gradient:  0.16305930410683098
iteration : 2231
train acc:  0.6796875
train loss:  0.5759948492050171
train gradient:  0.23369064120243477
iteration : 2232
train acc:  0.75
train loss:  0.5281854271888733
train gradient:  0.15786400856586952
iteration : 2233
train acc:  0.7421875
train loss:  0.5082340240478516
train gradient:  0.15884394895031317
iteration : 2234
train acc:  0.7109375
train loss:  0.5422582030296326
train gradient:  0.15950574212616436
iteration : 2235
train acc:  0.765625
train loss:  0.5248066186904907
train gradient:  0.17355643796778297
iteration : 2236
train acc:  0.75
train loss:  0.5193551182746887
train gradient:  0.15496527284517533
iteration : 2237
train acc:  0.6953125
train loss:  0.5562829971313477
train gradient:  0.18979383455758614
iteration : 2238
train acc:  0.765625
train loss:  0.47925251722335815
train gradient:  0.11296575871477887
iteration : 2239
train acc:  0.65625
train loss:  0.5436974167823792
train gradient:  0.13978784815110787
iteration : 2240
train acc:  0.703125
train loss:  0.515088677406311
train gradient:  0.15673545250184856
iteration : 2241
train acc:  0.7109375
train loss:  0.5092403888702393
train gradient:  0.19075332779811494
iteration : 2242
train acc:  0.71875
train loss:  0.5309319496154785
train gradient:  0.1779161918426355
iteration : 2243
train acc:  0.6796875
train loss:  0.5719264149665833
train gradient:  0.1897152250854605
iteration : 2244
train acc:  0.6640625
train loss:  0.5879049301147461
train gradient:  0.19093746053415803
iteration : 2245
train acc:  0.7734375
train loss:  0.544991672039032
train gradient:  0.1944900200000786
iteration : 2246
train acc:  0.734375
train loss:  0.5476869344711304
train gradient:  0.15107846651701162
iteration : 2247
train acc:  0.734375
train loss:  0.5132243633270264
train gradient:  0.14951703357203433
iteration : 2248
train acc:  0.6328125
train loss:  0.6068524122238159
train gradient:  0.22880139536533448
iteration : 2249
train acc:  0.6640625
train loss:  0.5955264568328857
train gradient:  0.19010474220104356
iteration : 2250
train acc:  0.6796875
train loss:  0.5627198815345764
train gradient:  0.18227481451546473
iteration : 2251
train acc:  0.796875
train loss:  0.48025310039520264
train gradient:  0.16944502341658657
iteration : 2252
train acc:  0.6640625
train loss:  0.5980250835418701
train gradient:  0.20665715599019086
iteration : 2253
train acc:  0.7421875
train loss:  0.5509312152862549
train gradient:  0.14618037056107683
iteration : 2254
train acc:  0.640625
train loss:  0.6200309991836548
train gradient:  0.23341278900597723
iteration : 2255
train acc:  0.7734375
train loss:  0.4624028503894806
train gradient:  0.12555771904598495
iteration : 2256
train acc:  0.734375
train loss:  0.563447117805481
train gradient:  0.21676866809653272
iteration : 2257
train acc:  0.671875
train loss:  0.6030884981155396
train gradient:  0.26222594239011005
iteration : 2258
train acc:  0.6953125
train loss:  0.5138595104217529
train gradient:  0.16492315583128542
iteration : 2259
train acc:  0.7109375
train loss:  0.5356346964836121
train gradient:  0.19192463185617237
iteration : 2260
train acc:  0.734375
train loss:  0.5219749212265015
train gradient:  0.17061701897419812
iteration : 2261
train acc:  0.6640625
train loss:  0.541254460811615
train gradient:  0.1564495125191342
iteration : 2262
train acc:  0.75
train loss:  0.46980488300323486
train gradient:  0.13766727239947169
iteration : 2263
train acc:  0.7109375
train loss:  0.59154212474823
train gradient:  0.19551405910354436
iteration : 2264
train acc:  0.8125
train loss:  0.501014232635498
train gradient:  0.14928480298318816
iteration : 2265
train acc:  0.6953125
train loss:  0.5785896182060242
train gradient:  0.17270829680288838
iteration : 2266
train acc:  0.765625
train loss:  0.537056565284729
train gradient:  0.1548725901477322
iteration : 2267
train acc:  0.6640625
train loss:  0.5582725405693054
train gradient:  0.17827624418911903
iteration : 2268
train acc:  0.6796875
train loss:  0.5765465497970581
train gradient:  0.20179506117678744
iteration : 2269
train acc:  0.7421875
train loss:  0.5191277861595154
train gradient:  0.267041889686688
iteration : 2270
train acc:  0.6875
train loss:  0.5769696831703186
train gradient:  0.18315925017870752
iteration : 2271
train acc:  0.7890625
train loss:  0.4972092807292938
train gradient:  0.11439844642785406
iteration : 2272
train acc:  0.765625
train loss:  0.5045057535171509
train gradient:  0.15329761652556673
iteration : 2273
train acc:  0.7734375
train loss:  0.47483786940574646
train gradient:  0.14606436647749133
iteration : 2274
train acc:  0.6328125
train loss:  0.5750960111618042
train gradient:  0.20627177238015668
iteration : 2275
train acc:  0.703125
train loss:  0.5144413709640503
train gradient:  0.20007211191254473
iteration : 2276
train acc:  0.671875
train loss:  0.5818319320678711
train gradient:  0.1741694746763632
iteration : 2277
train acc:  0.7265625
train loss:  0.5499163866043091
train gradient:  0.14013799022685994
iteration : 2278
train acc:  0.7265625
train loss:  0.5367862582206726
train gradient:  0.18487180128609076
iteration : 2279
train acc:  0.7265625
train loss:  0.5087932348251343
train gradient:  0.14927438239588608
iteration : 2280
train acc:  0.7265625
train loss:  0.5409127473831177
train gradient:  0.1623235471276297
iteration : 2281
train acc:  0.6875
train loss:  0.5852967500686646
train gradient:  0.2270830194177736
iteration : 2282
train acc:  0.7265625
train loss:  0.5002201795578003
train gradient:  0.1668922538910821
iteration : 2283
train acc:  0.6875
train loss:  0.6036338806152344
train gradient:  0.2394370462682271
iteration : 2284
train acc:  0.6953125
train loss:  0.5967217683792114
train gradient:  0.32132353262884544
iteration : 2285
train acc:  0.7109375
train loss:  0.5377026200294495
train gradient:  0.15909753376992503
iteration : 2286
train acc:  0.6953125
train loss:  0.5516443252563477
train gradient:  0.16576365200243126
iteration : 2287
train acc:  0.6953125
train loss:  0.5709384679794312
train gradient:  0.22274394009219167
iteration : 2288
train acc:  0.7734375
train loss:  0.47308099269866943
train gradient:  0.13758929020296892
iteration : 2289
train acc:  0.6328125
train loss:  0.6068611741065979
train gradient:  0.20935808682465923
iteration : 2290
train acc:  0.6953125
train loss:  0.5251847505569458
train gradient:  0.14174740091078902
iteration : 2291
train acc:  0.703125
train loss:  0.5420327186584473
train gradient:  0.14079426426833094
iteration : 2292
train acc:  0.7421875
train loss:  0.5370898842811584
train gradient:  0.14465692812593564
iteration : 2293
train acc:  0.7109375
train loss:  0.5617258548736572
train gradient:  0.17511314181162435
iteration : 2294
train acc:  0.7109375
train loss:  0.5152905583381653
train gradient:  0.18532333913270915
iteration : 2295
train acc:  0.6875
train loss:  0.5700340867042542
train gradient:  0.2751527521285801
iteration : 2296
train acc:  0.734375
train loss:  0.533576488494873
train gradient:  0.1475050391057155
iteration : 2297
train acc:  0.6875
train loss:  0.5987026691436768
train gradient:  0.2105263501906851
iteration : 2298
train acc:  0.6796875
train loss:  0.5735886096954346
train gradient:  0.18042869880489587
iteration : 2299
train acc:  0.765625
train loss:  0.49707698822021484
train gradient:  0.15987364805003976
iteration : 2300
train acc:  0.6875
train loss:  0.5032848119735718
train gradient:  0.15035651749512502
iteration : 2301
train acc:  0.75
train loss:  0.5311347246170044
train gradient:  0.14631486302551927
iteration : 2302
train acc:  0.734375
train loss:  0.5215911865234375
train gradient:  0.12836584337639095
iteration : 2303
train acc:  0.6953125
train loss:  0.5746158361434937
train gradient:  0.22115367752406623
iteration : 2304
train acc:  0.6953125
train loss:  0.5559120178222656
train gradient:  0.17097755693634709
iteration : 2305
train acc:  0.7109375
train loss:  0.5462898015975952
train gradient:  0.1813349792908852
iteration : 2306
train acc:  0.640625
train loss:  0.5849953889846802
train gradient:  0.2854075093007094
iteration : 2307
train acc:  0.734375
train loss:  0.5676290988922119
train gradient:  0.17830398594906544
iteration : 2308
train acc:  0.6796875
train loss:  0.5469326972961426
train gradient:  0.1830436502594297
iteration : 2309
train acc:  0.6640625
train loss:  0.5458296537399292
train gradient:  0.16144784450600347
iteration : 2310
train acc:  0.703125
train loss:  0.5134667158126831
train gradient:  0.1561228933635359
iteration : 2311
train acc:  0.6875
train loss:  0.5563119053840637
train gradient:  0.15133410341371928
iteration : 2312
train acc:  0.7265625
train loss:  0.5322442054748535
train gradient:  0.1303584398621206
iteration : 2313
train acc:  0.71875
train loss:  0.5190264582633972
train gradient:  0.15780480416495912
iteration : 2314
train acc:  0.7109375
train loss:  0.5685903429985046
train gradient:  0.17688382546810977
iteration : 2315
train acc:  0.6875
train loss:  0.5658948421478271
train gradient:  0.1634280237629859
iteration : 2316
train acc:  0.7265625
train loss:  0.5125830173492432
train gradient:  0.13447551381565678
iteration : 2317
train acc:  0.734375
train loss:  0.5412890911102295
train gradient:  0.18970369798052195
iteration : 2318
train acc:  0.6875
train loss:  0.5667074918746948
train gradient:  0.15973662561490798
iteration : 2319
train acc:  0.7421875
train loss:  0.5589680075645447
train gradient:  0.15754114097599245
iteration : 2320
train acc:  0.765625
train loss:  0.5160629749298096
train gradient:  0.13955237565363088
iteration : 2321
train acc:  0.671875
train loss:  0.616398811340332
train gradient:  0.2184370291470457
iteration : 2322
train acc:  0.65625
train loss:  0.6106064319610596
train gradient:  0.21904963254052587
iteration : 2323
train acc:  0.6328125
train loss:  0.6214107871055603
train gradient:  0.2619601635191186
iteration : 2324
train acc:  0.6875
train loss:  0.5740829706192017
train gradient:  0.23798058303351272
iteration : 2325
train acc:  0.6953125
train loss:  0.5084580183029175
train gradient:  0.19342753891527503
iteration : 2326
train acc:  0.7265625
train loss:  0.5453276634216309
train gradient:  0.18045935166003646
iteration : 2327
train acc:  0.7265625
train loss:  0.5057334899902344
train gradient:  0.13433890913364266
iteration : 2328
train acc:  0.703125
train loss:  0.5266317129135132
train gradient:  0.15433021461706373
iteration : 2329
train acc:  0.6796875
train loss:  0.5766032934188843
train gradient:  0.2006012157147755
iteration : 2330
train acc:  0.671875
train loss:  0.5729745030403137
train gradient:  0.20211518571785586
iteration : 2331
train acc:  0.7109375
train loss:  0.5332018136978149
train gradient:  0.16271883147996016
iteration : 2332
train acc:  0.71875
train loss:  0.5130362510681152
train gradient:  0.17546518194358063
iteration : 2333
train acc:  0.6953125
train loss:  0.5459935665130615
train gradient:  0.20908907604524493
iteration : 2334
train acc:  0.6875
train loss:  0.5547177791595459
train gradient:  0.17648857202652182
iteration : 2335
train acc:  0.6875
train loss:  0.5224267244338989
train gradient:  0.14344471043352436
iteration : 2336
train acc:  0.734375
train loss:  0.501863956451416
train gradient:  0.15179715994266876
iteration : 2337
train acc:  0.703125
train loss:  0.5596782565116882
train gradient:  0.16534570718010208
iteration : 2338
train acc:  0.75
train loss:  0.49836111068725586
train gradient:  0.2131113091900031
iteration : 2339
train acc:  0.7265625
train loss:  0.5322656631469727
train gradient:  0.18365900943494967
iteration : 2340
train acc:  0.6640625
train loss:  0.5429109334945679
train gradient:  0.1669999018056775
iteration : 2341
train acc:  0.703125
train loss:  0.5378780961036682
train gradient:  0.15977656684967093
iteration : 2342
train acc:  0.7109375
train loss:  0.5164521932601929
train gradient:  0.1749004504971712
iteration : 2343
train acc:  0.75
train loss:  0.5008054971694946
train gradient:  0.18549628020881456
iteration : 2344
train acc:  0.765625
train loss:  0.48971325159072876
train gradient:  0.1446607282297636
iteration : 2345
train acc:  0.7265625
train loss:  0.5556433796882629
train gradient:  0.20237984820133514
iteration : 2346
train acc:  0.671875
train loss:  0.5254647731781006
train gradient:  0.1702879722301076
iteration : 2347
train acc:  0.75
train loss:  0.5734248757362366
train gradient:  0.17598133290513124
iteration : 2348
train acc:  0.7421875
train loss:  0.48343467712402344
train gradient:  0.1335523995654522
iteration : 2349
train acc:  0.6640625
train loss:  0.5758297443389893
train gradient:  0.1858463666013041
iteration : 2350
train acc:  0.7109375
train loss:  0.5156511664390564
train gradient:  0.17465390713101475
iteration : 2351
train acc:  0.7578125
train loss:  0.49527615308761597
train gradient:  0.15874058069111507
iteration : 2352
train acc:  0.71875
train loss:  0.5560247898101807
train gradient:  0.1722847190829804
iteration : 2353
train acc:  0.7109375
train loss:  0.531424880027771
train gradient:  0.17905385293872633
iteration : 2354
train acc:  0.7890625
train loss:  0.5177929997444153
train gradient:  0.175269492374872
iteration : 2355
train acc:  0.703125
train loss:  0.5621777772903442
train gradient:  0.18875834570811267
iteration : 2356
train acc:  0.8046875
train loss:  0.4502710700035095
train gradient:  0.16403183587034392
iteration : 2357
train acc:  0.7265625
train loss:  0.5226187705993652
train gradient:  0.18946927235960084
iteration : 2358
train acc:  0.8203125
train loss:  0.4707340598106384
train gradient:  0.18170749703654765
iteration : 2359
train acc:  0.8046875
train loss:  0.4816271662712097
train gradient:  0.12660651198347017
iteration : 2360
train acc:  0.734375
train loss:  0.5037033557891846
train gradient:  0.14873467657234601
iteration : 2361
train acc:  0.71875
train loss:  0.5421395301818848
train gradient:  0.17401558849458487
iteration : 2362
train acc:  0.671875
train loss:  0.6049888134002686
train gradient:  0.25730794936765033
iteration : 2363
train acc:  0.765625
train loss:  0.4857572913169861
train gradient:  0.1484244421634187
iteration : 2364
train acc:  0.6875
train loss:  0.5451071858406067
train gradient:  0.1651966424737071
iteration : 2365
train acc:  0.71875
train loss:  0.566037654876709
train gradient:  0.19886542506832197
iteration : 2366
train acc:  0.671875
train loss:  0.5917038321495056
train gradient:  0.20703681232953097
iteration : 2367
train acc:  0.734375
train loss:  0.5337451100349426
train gradient:  0.18536886574325945
iteration : 2368
train acc:  0.6953125
train loss:  0.5849051475524902
train gradient:  0.21796616400395974
iteration : 2369
train acc:  0.6796875
train loss:  0.5771563053131104
train gradient:  0.2693737447358096
iteration : 2370
train acc:  0.71875
train loss:  0.5263975858688354
train gradient:  0.16926641463289305
iteration : 2371
train acc:  0.703125
train loss:  0.6214925646781921
train gradient:  0.2560938999969607
iteration : 2372
train acc:  0.7109375
train loss:  0.5676864385604858
train gradient:  0.1908768975691155
iteration : 2373
train acc:  0.6796875
train loss:  0.5909338593482971
train gradient:  0.16990394655254915
iteration : 2374
train acc:  0.7109375
train loss:  0.5258321762084961
train gradient:  0.15845126857196756
iteration : 2375
train acc:  0.6953125
train loss:  0.6140916347503662
train gradient:  0.2110968403626473
iteration : 2376
train acc:  0.7109375
train loss:  0.515163242816925
train gradient:  0.16173026202791901
iteration : 2377
train acc:  0.6796875
train loss:  0.6528007984161377
train gradient:  0.3187356676221694
iteration : 2378
train acc:  0.734375
train loss:  0.529226541519165
train gradient:  0.16629986228915627
iteration : 2379
train acc:  0.6796875
train loss:  0.4841125011444092
train gradient:  0.14600649169460045
iteration : 2380
train acc:  0.7734375
train loss:  0.5280146598815918
train gradient:  0.13427901934115993
iteration : 2381
train acc:  0.7578125
train loss:  0.5121898651123047
train gradient:  0.16716210048349617
iteration : 2382
train acc:  0.640625
train loss:  0.602783203125
train gradient:  0.15793258648040945
iteration : 2383
train acc:  0.65625
train loss:  0.5777011513710022
train gradient:  0.2016603351517712
iteration : 2384
train acc:  0.71875
train loss:  0.4983362555503845
train gradient:  0.1385815187097702
iteration : 2385
train acc:  0.75
train loss:  0.5316891670227051
train gradient:  0.13252079501305802
iteration : 2386
train acc:  0.65625
train loss:  0.6038013100624084
train gradient:  0.19108257504055765
iteration : 2387
train acc:  0.75
train loss:  0.5102471709251404
train gradient:  0.14956698805009108
iteration : 2388
train acc:  0.7265625
train loss:  0.4854432940483093
train gradient:  0.11953431617187248
iteration : 2389
train acc:  0.6875
train loss:  0.5296773314476013
train gradient:  0.15244763067589384
iteration : 2390
train acc:  0.7734375
train loss:  0.5020624399185181
train gradient:  0.13931399108599962
iteration : 2391
train acc:  0.6953125
train loss:  0.5009033679962158
train gradient:  0.1385403684288074
iteration : 2392
train acc:  0.7109375
train loss:  0.5408280491828918
train gradient:  0.18576564012116187
iteration : 2393
train acc:  0.75
train loss:  0.53279709815979
train gradient:  0.16419883967351478
iteration : 2394
train acc:  0.7890625
train loss:  0.46320924162864685
train gradient:  0.11228620876529415
iteration : 2395
train acc:  0.6953125
train loss:  0.5610033273696899
train gradient:  0.1492915400304848
iteration : 2396
train acc:  0.703125
train loss:  0.557948112487793
train gradient:  0.1774021565469811
iteration : 2397
train acc:  0.671875
train loss:  0.5402837991714478
train gradient:  0.136277578627515
iteration : 2398
train acc:  0.75
train loss:  0.5137357115745544
train gradient:  0.11865142354882324
iteration : 2399
train acc:  0.6953125
train loss:  0.5744763612747192
train gradient:  0.20828191348411715
iteration : 2400
train acc:  0.703125
train loss:  0.5406004786491394
train gradient:  0.21315736533190294
iteration : 2401
train acc:  0.734375
train loss:  0.5413482785224915
train gradient:  0.15421318481455992
iteration : 2402
train acc:  0.71875
train loss:  0.5442220568656921
train gradient:  0.15684168425512862
iteration : 2403
train acc:  0.6640625
train loss:  0.5742090940475464
train gradient:  0.1719992478747518
iteration : 2404
train acc:  0.734375
train loss:  0.5399473905563354
train gradient:  0.13662810380246493
iteration : 2405
train acc:  0.671875
train loss:  0.5390484929084778
train gradient:  0.15259701384982172
iteration : 2406
train acc:  0.6796875
train loss:  0.5790993571281433
train gradient:  0.20429581021249388
iteration : 2407
train acc:  0.7265625
train loss:  0.5137883424758911
train gradient:  0.1449863636176282
iteration : 2408
train acc:  0.7265625
train loss:  0.5051231980323792
train gradient:  0.14864230927532585
iteration : 2409
train acc:  0.6640625
train loss:  0.5586504340171814
train gradient:  0.1892609970453864
iteration : 2410
train acc:  0.6796875
train loss:  0.6047121286392212
train gradient:  0.20225839853745958
iteration : 2411
train acc:  0.6796875
train loss:  0.5929827094078064
train gradient:  0.20231321443946282
iteration : 2412
train acc:  0.734375
train loss:  0.6100552082061768
train gradient:  0.19285207448715008
iteration : 2413
train acc:  0.6953125
train loss:  0.5469418168067932
train gradient:  0.1794034982938224
iteration : 2414
train acc:  0.7109375
train loss:  0.5501799583435059
train gradient:  0.17172682199403316
iteration : 2415
train acc:  0.7265625
train loss:  0.5315366387367249
train gradient:  0.13919011976410403
iteration : 2416
train acc:  0.75
train loss:  0.4967813491821289
train gradient:  0.1355807186536913
iteration : 2417
train acc:  0.625
train loss:  0.6311549544334412
train gradient:  0.20581003243293783
iteration : 2418
train acc:  0.7890625
train loss:  0.5052914619445801
train gradient:  0.17869949305572547
iteration : 2419
train acc:  0.6796875
train loss:  0.5745610594749451
train gradient:  0.21266979161553126
iteration : 2420
train acc:  0.7421875
train loss:  0.4990331828594208
train gradient:  0.10275468619975893
iteration : 2421
train acc:  0.734375
train loss:  0.4998968839645386
train gradient:  0.15035463389986864
iteration : 2422
train acc:  0.75
train loss:  0.5201136469841003
train gradient:  0.1537817079717855
iteration : 2423
train acc:  0.6953125
train loss:  0.5329667329788208
train gradient:  0.1646650279406146
iteration : 2424
train acc:  0.7578125
train loss:  0.5147957801818848
train gradient:  0.12239845206655044
iteration : 2425
train acc:  0.7578125
train loss:  0.5120635628700256
train gradient:  0.18040436158796735
iteration : 2426
train acc:  0.71875
train loss:  0.5388332605361938
train gradient:  0.15538860926929843
iteration : 2427
train acc:  0.7578125
train loss:  0.5071187019348145
train gradient:  0.15445351448252398
iteration : 2428
train acc:  0.7421875
train loss:  0.5542925596237183
train gradient:  0.13516702630597333
iteration : 2429
train acc:  0.7265625
train loss:  0.518997311592102
train gradient:  0.14122226141474475
iteration : 2430
train acc:  0.78125
train loss:  0.5118194818496704
train gradient:  0.14238845165140906
iteration : 2431
train acc:  0.7578125
train loss:  0.5465836524963379
train gradient:  0.15862291923550884
iteration : 2432
train acc:  0.7578125
train loss:  0.4686143398284912
train gradient:  0.150212569391032
iteration : 2433
train acc:  0.671875
train loss:  0.6189623475074768
train gradient:  0.1687487755076516
iteration : 2434
train acc:  0.703125
train loss:  0.5880547761917114
train gradient:  0.26056044172425974
iteration : 2435
train acc:  0.734375
train loss:  0.5310418605804443
train gradient:  0.16628922399246593
iteration : 2436
train acc:  0.6875
train loss:  0.6128066778182983
train gradient:  0.2116537678779803
iteration : 2437
train acc:  0.71875
train loss:  0.5665226578712463
train gradient:  0.1346062386392898
iteration : 2438
train acc:  0.7265625
train loss:  0.5233873128890991
train gradient:  0.19572246814645744
iteration : 2439
train acc:  0.703125
train loss:  0.546202540397644
train gradient:  0.152225451540185
iteration : 2440
train acc:  0.7109375
train loss:  0.49630311131477356
train gradient:  0.13130685143594856
iteration : 2441
train acc:  0.71875
train loss:  0.5600493550300598
train gradient:  0.17213162045530858
iteration : 2442
train acc:  0.640625
train loss:  0.5819779634475708
train gradient:  0.21456294624179623
iteration : 2443
train acc:  0.7265625
train loss:  0.5006874203681946
train gradient:  0.14858075427445583
iteration : 2444
train acc:  0.6796875
train loss:  0.5737582445144653
train gradient:  0.18785430061544978
iteration : 2445
train acc:  0.6875
train loss:  0.5711343288421631
train gradient:  0.2021311690560545
iteration : 2446
train acc:  0.703125
train loss:  0.5782566070556641
train gradient:  0.22827652769302342
iteration : 2447
train acc:  0.71875
train loss:  0.6078219413757324
train gradient:  0.21615130932384363
iteration : 2448
train acc:  0.703125
train loss:  0.5495056509971619
train gradient:  0.17653831571994422
iteration : 2449
train acc:  0.7578125
train loss:  0.4956434369087219
train gradient:  0.13403615105555094
iteration : 2450
train acc:  0.6953125
train loss:  0.5912714004516602
train gradient:  0.19271178892161678
iteration : 2451
train acc:  0.703125
train loss:  0.5765095949172974
train gradient:  0.18541713000756593
iteration : 2452
train acc:  0.7265625
train loss:  0.545681893825531
train gradient:  0.13755070077482598
iteration : 2453
train acc:  0.65625
train loss:  0.5612718462944031
train gradient:  0.12887420213684214
iteration : 2454
train acc:  0.6875
train loss:  0.5630627870559692
train gradient:  0.19990574640021172
iteration : 2455
train acc:  0.7265625
train loss:  0.5327883958816528
train gradient:  0.15384693522555454
iteration : 2456
train acc:  0.796875
train loss:  0.4635503888130188
train gradient:  0.1355702452018946
iteration : 2457
train acc:  0.7265625
train loss:  0.5646085739135742
train gradient:  0.22001758709068353
iteration : 2458
train acc:  0.625
train loss:  0.6438302993774414
train gradient:  0.19914212723529853
iteration : 2459
train acc:  0.6953125
train loss:  0.5560721158981323
train gradient:  0.167026693040619
iteration : 2460
train acc:  0.6484375
train loss:  0.6029967069625854
train gradient:  0.16442083333625512
iteration : 2461
train acc:  0.6640625
train loss:  0.609447181224823
train gradient:  0.1963643617018846
iteration : 2462
train acc:  0.671875
train loss:  0.5582491755485535
train gradient:  0.14226755633325652
iteration : 2463
train acc:  0.7109375
train loss:  0.5594298839569092
train gradient:  0.1702263589544237
iteration : 2464
train acc:  0.71875
train loss:  0.5132058262825012
train gradient:  0.16033252977220314
iteration : 2465
train acc:  0.6953125
train loss:  0.5712555646896362
train gradient:  0.20908764653627532
iteration : 2466
train acc:  0.7734375
train loss:  0.4994504749774933
train gradient:  0.1553326850857396
iteration : 2467
train acc:  0.75
train loss:  0.5198192596435547
train gradient:  0.16458458198053205
iteration : 2468
train acc:  0.671875
train loss:  0.5969829559326172
train gradient:  0.18935509310422194
iteration : 2469
train acc:  0.671875
train loss:  0.5501025915145874
train gradient:  0.16440439847741917
iteration : 2470
train acc:  0.6875
train loss:  0.5621610879898071
train gradient:  0.1581381598947968
iteration : 2471
train acc:  0.75
train loss:  0.5070935487747192
train gradient:  0.17732737496623624
iteration : 2472
train acc:  0.6953125
train loss:  0.5868387222290039
train gradient:  0.1586869051487717
iteration : 2473
train acc:  0.7421875
train loss:  0.49398231506347656
train gradient:  0.1448854118636256
iteration : 2474
train acc:  0.5859375
train loss:  0.6690533757209778
train gradient:  0.23059914879410293
iteration : 2475
train acc:  0.6328125
train loss:  0.6748040914535522
train gradient:  0.30406608010775765
iteration : 2476
train acc:  0.65625
train loss:  0.5946038961410522
train gradient:  0.17646357950579866
iteration : 2477
train acc:  0.7421875
train loss:  0.5368025302886963
train gradient:  0.21403739688601775
iteration : 2478
train acc:  0.703125
train loss:  0.5043303370475769
train gradient:  0.12210404934254637
iteration : 2479
train acc:  0.6171875
train loss:  0.6318624019622803
train gradient:  0.1898924899013464
iteration : 2480
train acc:  0.7734375
train loss:  0.4809204339981079
train gradient:  0.12254633007847862
iteration : 2481
train acc:  0.703125
train loss:  0.5477061867713928
train gradient:  0.197251225027875
iteration : 2482
train acc:  0.671875
train loss:  0.5767102241516113
train gradient:  0.18069482113045265
iteration : 2483
train acc:  0.7421875
train loss:  0.5205612182617188
train gradient:  0.1909854578567466
iteration : 2484
train acc:  0.765625
train loss:  0.5156819820404053
train gradient:  0.1505513108329948
iteration : 2485
train acc:  0.734375
train loss:  0.5405052900314331
train gradient:  0.17519682587828445
iteration : 2486
train acc:  0.6875
train loss:  0.5375595092773438
train gradient:  0.12409847645211809
iteration : 2487
train acc:  0.6640625
train loss:  0.5628776550292969
train gradient:  0.17520035542441542
iteration : 2488
train acc:  0.6484375
train loss:  0.5786828994750977
train gradient:  0.16402642544985713
iteration : 2489
train acc:  0.6796875
train loss:  0.6043341159820557
train gradient:  0.24070864122917407
iteration : 2490
train acc:  0.75
train loss:  0.5325256586074829
train gradient:  0.2188081520383081
iteration : 2491
train acc:  0.703125
train loss:  0.5282750725746155
train gradient:  0.132098308467872
iteration : 2492
train acc:  0.71875
train loss:  0.5222594141960144
train gradient:  0.177543936526209
iteration : 2493
train acc:  0.6953125
train loss:  0.5517623424530029
train gradient:  0.18079557584026798
iteration : 2494
train acc:  0.75
train loss:  0.5074433088302612
train gradient:  0.1477952786987513
iteration : 2495
train acc:  0.734375
train loss:  0.5493743419647217
train gradient:  0.1365339704881321
iteration : 2496
train acc:  0.7734375
train loss:  0.4815126061439514
train gradient:  0.1309269054475894
iteration : 2497
train acc:  0.71875
train loss:  0.5498262643814087
train gradient:  0.20023699433350034
iteration : 2498
train acc:  0.7109375
train loss:  0.5438743829727173
train gradient:  0.13689864410393415
iteration : 2499
train acc:  0.65625
train loss:  0.5903307199478149
train gradient:  0.16679298943238335
iteration : 2500
train acc:  0.71875
train loss:  0.5459791421890259
train gradient:  0.18091401185892264
iteration : 2501
train acc:  0.7265625
train loss:  0.5422720313072205
train gradient:  0.14644198434507755
iteration : 2502
train acc:  0.7265625
train loss:  0.5215713977813721
train gradient:  0.1754661032154716
iteration : 2503
train acc:  0.765625
train loss:  0.49960553646087646
train gradient:  0.14177616958479022
iteration : 2504
train acc:  0.71875
train loss:  0.5365989804267883
train gradient:  0.17214655156829795
iteration : 2505
train acc:  0.7109375
train loss:  0.5615719556808472
train gradient:  0.17154299104977966
iteration : 2506
train acc:  0.75
train loss:  0.49404147267341614
train gradient:  0.15251428485697452
iteration : 2507
train acc:  0.7109375
train loss:  0.5316059589385986
train gradient:  0.18424836162458824
iteration : 2508
train acc:  0.75
train loss:  0.5020582675933838
train gradient:  0.19388449425150128
iteration : 2509
train acc:  0.65625
train loss:  0.5596719980239868
train gradient:  0.22801818632100002
iteration : 2510
train acc:  0.765625
train loss:  0.48485541343688965
train gradient:  0.13815961819587447
iteration : 2511
train acc:  0.6640625
train loss:  0.5626513957977295
train gradient:  0.20046959470036715
iteration : 2512
train acc:  0.6796875
train loss:  0.5939810276031494
train gradient:  0.21785690615123293
iteration : 2513
train acc:  0.765625
train loss:  0.4915848970413208
train gradient:  0.12437224317513534
iteration : 2514
train acc:  0.7890625
train loss:  0.4852522313594818
train gradient:  0.15584995054208065
iteration : 2515
train acc:  0.7109375
train loss:  0.5254360437393188
train gradient:  0.12446310385226679
iteration : 2516
train acc:  0.7265625
train loss:  0.5159034729003906
train gradient:  0.14370869968674327
iteration : 2517
train acc:  0.7734375
train loss:  0.49031931161880493
train gradient:  0.14333539096390815
iteration : 2518
train acc:  0.6875
train loss:  0.5447651147842407
train gradient:  0.19763177923845743
iteration : 2519
train acc:  0.671875
train loss:  0.5710346102714539
train gradient:  0.23888165667043693
iteration : 2520
train acc:  0.7109375
train loss:  0.5834333300590515
train gradient:  0.20616103020314963
iteration : 2521
train acc:  0.703125
train loss:  0.5511701107025146
train gradient:  0.18387142186778194
iteration : 2522
train acc:  0.6953125
train loss:  0.5478951930999756
train gradient:  0.17829462095472637
iteration : 2523
train acc:  0.6875
train loss:  0.5469119548797607
train gradient:  0.1523746806870288
iteration : 2524
train acc:  0.703125
train loss:  0.547432005405426
train gradient:  0.17428513286188502
iteration : 2525
train acc:  0.6953125
train loss:  0.5596712827682495
train gradient:  0.167653914931749
iteration : 2526
train acc:  0.734375
train loss:  0.49786117672920227
train gradient:  0.14438118021563018
iteration : 2527
train acc:  0.7109375
train loss:  0.5506404638290405
train gradient:  0.1438885662488552
iteration : 2528
train acc:  0.6953125
train loss:  0.5625442862510681
train gradient:  0.18534448096817857
iteration : 2529
train acc:  0.671875
train loss:  0.6082476377487183
train gradient:  0.2165402283566031
iteration : 2530
train acc:  0.6875
train loss:  0.5183789730072021
train gradient:  0.13702886410856002
iteration : 2531
train acc:  0.7578125
train loss:  0.4828795790672302
train gradient:  0.2610892109793169
iteration : 2532
train acc:  0.71875
train loss:  0.5533285140991211
train gradient:  0.18924410189419621
iteration : 2533
train acc:  0.7265625
train loss:  0.5091209411621094
train gradient:  0.1537885928141346
iteration : 2534
train acc:  0.7421875
train loss:  0.5026876926422119
train gradient:  0.13747741321396256
iteration : 2535
train acc:  0.703125
train loss:  0.5142914056777954
train gradient:  0.1866533612538667
iteration : 2536
train acc:  0.7109375
train loss:  0.5484264492988586
train gradient:  0.19038171044291563
iteration : 2537
train acc:  0.734375
train loss:  0.5403645038604736
train gradient:  0.15522965870273286
iteration : 2538
train acc:  0.671875
train loss:  0.6148695945739746
train gradient:  0.22265811525815576
iteration : 2539
train acc:  0.78125
train loss:  0.47681915760040283
train gradient:  0.10362536278306676
iteration : 2540
train acc:  0.6875
train loss:  0.5534765720367432
train gradient:  0.1405280013801933
iteration : 2541
train acc:  0.765625
train loss:  0.45096683502197266
train gradient:  0.14685257074313623
iteration : 2542
train acc:  0.765625
train loss:  0.4762529730796814
train gradient:  0.13671404997682965
iteration : 2543
train acc:  0.7421875
train loss:  0.5133200883865356
train gradient:  0.16278351256042045
iteration : 2544
train acc:  0.671875
train loss:  0.5769976377487183
train gradient:  0.15826515676719954
iteration : 2545
train acc:  0.71875
train loss:  0.5486278533935547
train gradient:  0.156860008846889
iteration : 2546
train acc:  0.703125
train loss:  0.6047790050506592
train gradient:  0.18666155907198612
iteration : 2547
train acc:  0.78125
train loss:  0.49937596917152405
train gradient:  0.14714479628477073
iteration : 2548
train acc:  0.7421875
train loss:  0.5591799020767212
train gradient:  0.19912631071166137
iteration : 2549
train acc:  0.734375
train loss:  0.5469387769699097
train gradient:  0.18323125956058825
iteration : 2550
train acc:  0.6875
train loss:  0.5582515597343445
train gradient:  0.18289795869052325
iteration : 2551
train acc:  0.6953125
train loss:  0.6149322986602783
train gradient:  0.21009247419989005
iteration : 2552
train acc:  0.6953125
train loss:  0.5352782011032104
train gradient:  0.18176352825542064
iteration : 2553
train acc:  0.7421875
train loss:  0.5192347168922424
train gradient:  0.1948415723997055
iteration : 2554
train acc:  0.7109375
train loss:  0.5374029874801636
train gradient:  0.15063873992576765
iteration : 2555
train acc:  0.7109375
train loss:  0.5192742347717285
train gradient:  0.16881716125338778
iteration : 2556
train acc:  0.7578125
train loss:  0.5180585384368896
train gradient:  0.16052320773804013
iteration : 2557
train acc:  0.71875
train loss:  0.5474756956100464
train gradient:  0.16603420240016303
iteration : 2558
train acc:  0.671875
train loss:  0.545592725276947
train gradient:  0.18020106638319794
iteration : 2559
train acc:  0.75
train loss:  0.5260772109031677
train gradient:  0.15973050876045033
iteration : 2560
train acc:  0.8046875
train loss:  0.45680689811706543
train gradient:  0.13773253833064275
iteration : 2561
train acc:  0.6875
train loss:  0.5143376588821411
train gradient:  0.15938393889060268
iteration : 2562
train acc:  0.7109375
train loss:  0.5515938997268677
train gradient:  0.19532429017356842
iteration : 2563
train acc:  0.7578125
train loss:  0.4779019355773926
train gradient:  0.17180350652700677
iteration : 2564
train acc:  0.6875
train loss:  0.6190963983535767
train gradient:  0.17182294947441767
iteration : 2565
train acc:  0.6875
train loss:  0.5690011382102966
train gradient:  0.19066549226246982
iteration : 2566
train acc:  0.7265625
train loss:  0.5060678720474243
train gradient:  0.2229514396601298
iteration : 2567
train acc:  0.71875
train loss:  0.4582545757293701
train gradient:  0.1033941114650117
iteration : 2568
train acc:  0.78125
train loss:  0.4542200565338135
train gradient:  0.13144786458336574
iteration : 2569
train acc:  0.6953125
train loss:  0.5613124370574951
train gradient:  0.21838902315887115
iteration : 2570
train acc:  0.7578125
train loss:  0.49239251017570496
train gradient:  0.15299534982912066
iteration : 2571
train acc:  0.7109375
train loss:  0.5214236974716187
train gradient:  0.20887805192240033
iteration : 2572
train acc:  0.75
train loss:  0.5416676998138428
train gradient:  0.18076714139072725
iteration : 2573
train acc:  0.734375
train loss:  0.48518091440200806
train gradient:  0.15837345616928333
iteration : 2574
train acc:  0.7421875
train loss:  0.5339671969413757
train gradient:  0.19728562694834942
iteration : 2575
train acc:  0.71875
train loss:  0.4891480803489685
train gradient:  0.14726757441133345
iteration : 2576
train acc:  0.6796875
train loss:  0.5673528909683228
train gradient:  0.19734832024988638
iteration : 2577
train acc:  0.671875
train loss:  0.5433599948883057
train gradient:  0.15123567204991734
iteration : 2578
train acc:  0.6171875
train loss:  0.6462662220001221
train gradient:  0.19692066688214194
iteration : 2579
train acc:  0.6484375
train loss:  0.6049011945724487
train gradient:  0.24074426605333002
iteration : 2580
train acc:  0.6484375
train loss:  0.5840142369270325
train gradient:  0.20053593928843677
iteration : 2581
train acc:  0.7421875
train loss:  0.5051653981208801
train gradient:  0.16237839797886794
iteration : 2582
train acc:  0.65625
train loss:  0.561335027217865
train gradient:  0.1606497565395894
iteration : 2583
train acc:  0.7421875
train loss:  0.46320879459381104
train gradient:  0.11505229515659036
iteration : 2584
train acc:  0.78125
train loss:  0.48493725061416626
train gradient:  0.14559025430898248
iteration : 2585
train acc:  0.625
train loss:  0.5890949368476868
train gradient:  0.1858333804481439
iteration : 2586
train acc:  0.7734375
train loss:  0.48214754462242126
train gradient:  0.18549912612002661
iteration : 2587
train acc:  0.7734375
train loss:  0.5356240272521973
train gradient:  0.18733003925872266
iteration : 2588
train acc:  0.7109375
train loss:  0.5489003658294678
train gradient:  0.15819280668662666
iteration : 2589
train acc:  0.6953125
train loss:  0.5761867165565491
train gradient:  0.19353765038653128
iteration : 2590
train acc:  0.71875
train loss:  0.5400130748748779
train gradient:  0.17784201021148038
iteration : 2591
train acc:  0.6875
train loss:  0.5426262021064758
train gradient:  0.16953576826667546
iteration : 2592
train acc:  0.7265625
train loss:  0.5176466107368469
train gradient:  0.1270432524380109
iteration : 2593
train acc:  0.640625
train loss:  0.6296138167381287
train gradient:  0.2253697230385509
iteration : 2594
train acc:  0.6640625
train loss:  0.5793884992599487
train gradient:  0.1894990315009371
iteration : 2595
train acc:  0.6953125
train loss:  0.5363879203796387
train gradient:  0.19931520192278634
iteration : 2596
train acc:  0.75
train loss:  0.5011352300643921
train gradient:  0.16112518710660328
iteration : 2597
train acc:  0.7109375
train loss:  0.5370358228683472
train gradient:  0.14992318783028374
iteration : 2598
train acc:  0.796875
train loss:  0.48051100969314575
train gradient:  0.13217077888169693
iteration : 2599
train acc:  0.71875
train loss:  0.544683575630188
train gradient:  0.18030171179941543
iteration : 2600
train acc:  0.765625
train loss:  0.449781209230423
train gradient:  0.13737505261017863
iteration : 2601
train acc:  0.6640625
train loss:  0.5734692811965942
train gradient:  0.206792526365053
iteration : 2602
train acc:  0.75
train loss:  0.5376048684120178
train gradient:  0.184075515018307
iteration : 2603
train acc:  0.7109375
train loss:  0.5870547294616699
train gradient:  0.17652074045828703
iteration : 2604
train acc:  0.703125
train loss:  0.5568441152572632
train gradient:  0.17941567000870506
iteration : 2605
train acc:  0.75
train loss:  0.5111056566238403
train gradient:  0.15282549476988427
iteration : 2606
train acc:  0.7109375
train loss:  0.5250096917152405
train gradient:  0.25216179119961135
iteration : 2607
train acc:  0.7421875
train loss:  0.49929213523864746
train gradient:  0.17626554421587737
iteration : 2608
train acc:  0.765625
train loss:  0.49262535572052
train gradient:  0.1441672573256601
iteration : 2609
train acc:  0.71875
train loss:  0.5223737955093384
train gradient:  0.17135552929454384
iteration : 2610
train acc:  0.7421875
train loss:  0.5196047425270081
train gradient:  0.15385243326740156
iteration : 2611
train acc:  0.7265625
train loss:  0.5013173818588257
train gradient:  0.20315860741977643
iteration : 2612
train acc:  0.75
train loss:  0.5427548885345459
train gradient:  0.2190828334409578
iteration : 2613
train acc:  0.71875
train loss:  0.5137995481491089
train gradient:  0.18902311791851328
iteration : 2614
train acc:  0.7421875
train loss:  0.53966224193573
train gradient:  0.2129510795387477
iteration : 2615
train acc:  0.6875
train loss:  0.5294704437255859
train gradient:  0.19983292114634632
iteration : 2616
train acc:  0.7421875
train loss:  0.47956669330596924
train gradient:  0.1718080180634351
iteration : 2617
train acc:  0.75
train loss:  0.5145992040634155
train gradient:  0.14903374326276364
iteration : 2618
train acc:  0.734375
train loss:  0.5243391990661621
train gradient:  0.17231252830234234
iteration : 2619
train acc:  0.6875
train loss:  0.5508583784103394
train gradient:  0.17460824443083062
iteration : 2620
train acc:  0.703125
train loss:  0.5808588862419128
train gradient:  0.20078395762312645
iteration : 2621
train acc:  0.7421875
train loss:  0.5223121643066406
train gradient:  0.19990116377886036
iteration : 2622
train acc:  0.703125
train loss:  0.5752272605895996
train gradient:  0.2011467355330811
iteration : 2623
train acc:  0.71875
train loss:  0.5344287753105164
train gradient:  0.15901385273298813
iteration : 2624
train acc:  0.71875
train loss:  0.5202904939651489
train gradient:  0.13784456485793367
iteration : 2625
train acc:  0.671875
train loss:  0.5845146179199219
train gradient:  0.1650933553346028
iteration : 2626
train acc:  0.640625
train loss:  0.5721902251243591
train gradient:  0.16546052913110687
iteration : 2627
train acc:  0.6875
train loss:  0.5268247127532959
train gradient:  0.19417815715333786
iteration : 2628
train acc:  0.703125
train loss:  0.5318741202354431
train gradient:  0.13464900968642338
iteration : 2629
train acc:  0.6171875
train loss:  0.5931239128112793
train gradient:  0.15790369774083635
iteration : 2630
train acc:  0.75
train loss:  0.5248048901557922
train gradient:  0.13278290144958582
iteration : 2631
train acc:  0.6640625
train loss:  0.5630273818969727
train gradient:  0.19741974264102152
iteration : 2632
train acc:  0.703125
train loss:  0.5164754390716553
train gradient:  0.1758159332452958
iteration : 2633
train acc:  0.7421875
train loss:  0.5316407680511475
train gradient:  0.1784569205226028
iteration : 2634
train acc:  0.7265625
train loss:  0.5170225501060486
train gradient:  0.15929290904717175
iteration : 2635
train acc:  0.6640625
train loss:  0.585002064704895
train gradient:  0.25637644595231746
iteration : 2636
train acc:  0.703125
train loss:  0.5487222671508789
train gradient:  0.1767921720039337
iteration : 2637
train acc:  0.6953125
train loss:  0.5692633986473083
train gradient:  0.23971183131588247
iteration : 2638
train acc:  0.703125
train loss:  0.5571589469909668
train gradient:  0.19230523145217882
iteration : 2639
train acc:  0.71875
train loss:  0.5331649780273438
train gradient:  0.18044046684828208
iteration : 2640
train acc:  0.671875
train loss:  0.5670396089553833
train gradient:  0.19163942139156032
iteration : 2641
train acc:  0.640625
train loss:  0.6673537492752075
train gradient:  0.2322540418284475
iteration : 2642
train acc:  0.65625
train loss:  0.6235291957855225
train gradient:  0.20039555949872456
iteration : 2643
train acc:  0.7734375
train loss:  0.5052151083946228
train gradient:  0.18003807557131013
iteration : 2644
train acc:  0.7421875
train loss:  0.48962247371673584
train gradient:  0.14001445737655727
iteration : 2645
train acc:  0.7421875
train loss:  0.5357835292816162
train gradient:  0.16220517695964481
iteration : 2646
train acc:  0.796875
train loss:  0.4923461675643921
train gradient:  0.1538283573896908
iteration : 2647
train acc:  0.65625
train loss:  0.5710136890411377
train gradient:  0.17521799489799111
iteration : 2648
train acc:  0.6953125
train loss:  0.5530693531036377
train gradient:  0.16343331667280503
iteration : 2649
train acc:  0.7578125
train loss:  0.4949079751968384
train gradient:  0.1627240055930429
iteration : 2650
train acc:  0.71875
train loss:  0.5260089635848999
train gradient:  0.18949674541598172
iteration : 2651
train acc:  0.7109375
train loss:  0.5291668772697449
train gradient:  0.18783194867254732
iteration : 2652
train acc:  0.71875
train loss:  0.5077362656593323
train gradient:  0.17683831747625928
iteration : 2653
train acc:  0.7265625
train loss:  0.5265572667121887
train gradient:  0.13197272491061393
iteration : 2654
train acc:  0.734375
train loss:  0.518837571144104
train gradient:  0.1820026062936086
iteration : 2655
train acc:  0.671875
train loss:  0.5930531024932861
train gradient:  0.1878930340707966
iteration : 2656
train acc:  0.6484375
train loss:  0.5973254442214966
train gradient:  0.21215325738008783
iteration : 2657
train acc:  0.75
train loss:  0.5141907930374146
train gradient:  0.14800866586365155
iteration : 2658
train acc:  0.6796875
train loss:  0.5783150792121887
train gradient:  0.1860968757208729
iteration : 2659
train acc:  0.703125
train loss:  0.4992097020149231
train gradient:  0.21215898497803548
iteration : 2660
train acc:  0.6953125
train loss:  0.521416425704956
train gradient:  0.1781617249433053
iteration : 2661
train acc:  0.703125
train loss:  0.5679466724395752
train gradient:  0.1941471421084282
iteration : 2662
train acc:  0.703125
train loss:  0.590384304523468
train gradient:  0.17408688496596988
iteration : 2663
train acc:  0.7734375
train loss:  0.46599143743515015
train gradient:  0.16458046621635927
iteration : 2664
train acc:  0.671875
train loss:  0.5258132219314575
train gradient:  0.15313262712679082
iteration : 2665
train acc:  0.6953125
train loss:  0.5874899625778198
train gradient:  0.23587421699508504
iteration : 2666
train acc:  0.7109375
train loss:  0.5621724128723145
train gradient:  0.189670944947529
iteration : 2667
train acc:  0.703125
train loss:  0.5528137683868408
train gradient:  0.20663607048408084
iteration : 2668
train acc:  0.765625
train loss:  0.5382713079452515
train gradient:  0.208126360801753
iteration : 2669
train acc:  0.6796875
train loss:  0.5391765832901001
train gradient:  0.15662758575347235
iteration : 2670
train acc:  0.6953125
train loss:  0.5435643196105957
train gradient:  0.20839566682297597
iteration : 2671
train acc:  0.7265625
train loss:  0.5193437933921814
train gradient:  0.14549999576735406
iteration : 2672
train acc:  0.734375
train loss:  0.5257662534713745
train gradient:  0.1785400606959594
iteration : 2673
train acc:  0.6796875
train loss:  0.5524458885192871
train gradient:  0.18501555320170185
iteration : 2674
train acc:  0.6796875
train loss:  0.5750455856323242
train gradient:  0.1960689926709555
iteration : 2675
train acc:  0.71875
train loss:  0.5696215629577637
train gradient:  0.2016597308514671
iteration : 2676
train acc:  0.703125
train loss:  0.552416205406189
train gradient:  0.1571675668941936
iteration : 2677
train acc:  0.7109375
train loss:  0.553120493888855
train gradient:  0.15496645764455802
iteration : 2678
train acc:  0.7578125
train loss:  0.44777071475982666
train gradient:  0.11967825723242626
iteration : 2679
train acc:  0.7109375
train loss:  0.5137588977813721
train gradient:  0.1983183463589994
iteration : 2680
train acc:  0.625
train loss:  0.5615620017051697
train gradient:  0.2264639027659362
iteration : 2681
train acc:  0.7109375
train loss:  0.5466804504394531
train gradient:  0.144553440830889
iteration : 2682
train acc:  0.734375
train loss:  0.5369935035705566
train gradient:  0.19327112874067878
iteration : 2683
train acc:  0.6640625
train loss:  0.5680823922157288
train gradient:  0.21126956578958372
iteration : 2684
train acc:  0.734375
train loss:  0.5371094942092896
train gradient:  0.20766888058340618
iteration : 2685
train acc:  0.7109375
train loss:  0.5260484218597412
train gradient:  0.1532336450189669
iteration : 2686
train acc:  0.6953125
train loss:  0.5660123825073242
train gradient:  0.1868009855657561
iteration : 2687
train acc:  0.734375
train loss:  0.49500739574432373
train gradient:  0.14583059232867626
iteration : 2688
train acc:  0.78125
train loss:  0.44984766840934753
train gradient:  0.13529116522660378
iteration : 2689
train acc:  0.7578125
train loss:  0.4844174385070801
train gradient:  0.12307403386624835
iteration : 2690
train acc:  0.71875
train loss:  0.5295147895812988
train gradient:  0.1758922784290538
iteration : 2691
train acc:  0.71875
train loss:  0.5369771718978882
train gradient:  0.19496565742013744
iteration : 2692
train acc:  0.6953125
train loss:  0.5368128418922424
train gradient:  0.1444876507911052
iteration : 2693
train acc:  0.7109375
train loss:  0.5424121618270874
train gradient:  0.15539212504902816
iteration : 2694
train acc:  0.7109375
train loss:  0.541149377822876
train gradient:  0.15324005498825816
iteration : 2695
train acc:  0.6953125
train loss:  0.5137858390808105
train gradient:  0.15546594806530314
iteration : 2696
train acc:  0.71875
train loss:  0.5356276631355286
train gradient:  0.13980812568837966
iteration : 2697
train acc:  0.7578125
train loss:  0.5011968016624451
train gradient:  0.158060322816018
iteration : 2698
train acc:  0.7265625
train loss:  0.5327621698379517
train gradient:  0.17712961055479132
iteration : 2699
train acc:  0.71875
train loss:  0.5213679075241089
train gradient:  0.1668972306657434
iteration : 2700
train acc:  0.6171875
train loss:  0.5991722345352173
train gradient:  0.16694928093434844
iteration : 2701
train acc:  0.6796875
train loss:  0.5516393184661865
train gradient:  0.16828059274737675
iteration : 2702
train acc:  0.6484375
train loss:  0.578464150428772
train gradient:  0.17894891051435552
iteration : 2703
train acc:  0.7265625
train loss:  0.5699214339256287
train gradient:  0.17291779571139437
iteration : 2704
train acc:  0.734375
train loss:  0.5000348687171936
train gradient:  0.19033949044013615
iteration : 2705
train acc:  0.703125
train loss:  0.5361506938934326
train gradient:  0.15482022435903456
iteration : 2706
train acc:  0.6875
train loss:  0.5764565467834473
train gradient:  0.25718262250999724
iteration : 2707
train acc:  0.6953125
train loss:  0.5336877703666687
train gradient:  0.20672070213133945
iteration : 2708
train acc:  0.734375
train loss:  0.5319327712059021
train gradient:  0.2644572288722237
iteration : 2709
train acc:  0.703125
train loss:  0.5467817783355713
train gradient:  0.1789567525658033
iteration : 2710
train acc:  0.6640625
train loss:  0.5614463686943054
train gradient:  0.17274441734868423
iteration : 2711
train acc:  0.6796875
train loss:  0.5363919138908386
train gradient:  0.14286773743715672
iteration : 2712
train acc:  0.71875
train loss:  0.531161367893219
train gradient:  0.1417361165972934
iteration : 2713
train acc:  0.8125
train loss:  0.407510906457901
train gradient:  0.11219375096138781
iteration : 2714
train acc:  0.6953125
train loss:  0.534507155418396
train gradient:  0.14250846840877057
iteration : 2715
train acc:  0.71875
train loss:  0.5334141254425049
train gradient:  0.1416787058290419
iteration : 2716
train acc:  0.6640625
train loss:  0.536170244216919
train gradient:  0.1471484829693845
iteration : 2717
train acc:  0.828125
train loss:  0.43444567918777466
train gradient:  0.11061983175581491
iteration : 2718
train acc:  0.71875
train loss:  0.5296965837478638
train gradient:  0.15243122925768035
iteration : 2719
train acc:  0.796875
train loss:  0.4761594235897064
train gradient:  0.12559524724291649
iteration : 2720
train acc:  0.7421875
train loss:  0.5385205745697021
train gradient:  0.18628977140127528
iteration : 2721
train acc:  0.7734375
train loss:  0.4847657084465027
train gradient:  0.13052459444496484
iteration : 2722
train acc:  0.7265625
train loss:  0.5269902348518372
train gradient:  0.17244015619441783
iteration : 2723
train acc:  0.7265625
train loss:  0.49889591336250305
train gradient:  0.1225668057968171
iteration : 2724
train acc:  0.7265625
train loss:  0.5753829479217529
train gradient:  0.16771023163197857
iteration : 2725
train acc:  0.7109375
train loss:  0.5439771413803101
train gradient:  0.14992161002871268
iteration : 2726
train acc:  0.7578125
train loss:  0.5277218818664551
train gradient:  0.16689894285875218
iteration : 2727
train acc:  0.7578125
train loss:  0.4584207236766815
train gradient:  0.12393929017988992
iteration : 2728
train acc:  0.703125
train loss:  0.5753718614578247
train gradient:  0.2518788022139934
iteration : 2729
train acc:  0.765625
train loss:  0.47214603424072266
train gradient:  0.13264640155314825
iteration : 2730
train acc:  0.6328125
train loss:  0.6253583431243896
train gradient:  0.2484970780659331
iteration : 2731
train acc:  0.7421875
train loss:  0.5066279172897339
train gradient:  0.157773630583472
iteration : 2732
train acc:  0.765625
train loss:  0.5272831320762634
train gradient:  0.1673639746111307
iteration : 2733
train acc:  0.7578125
train loss:  0.5275014638900757
train gradient:  0.147481888241086
iteration : 2734
train acc:  0.703125
train loss:  0.5401631593704224
train gradient:  0.18248971264240918
iteration : 2735
train acc:  0.671875
train loss:  0.5881557464599609
train gradient:  0.18385837929441617
iteration : 2736
train acc:  0.703125
train loss:  0.5635297298431396
train gradient:  0.1836777150266569
iteration : 2737
train acc:  0.6875
train loss:  0.5396990776062012
train gradient:  0.1814270608303184
iteration : 2738
train acc:  0.671875
train loss:  0.5639318227767944
train gradient:  0.1727106564778994
iteration : 2739
train acc:  0.7109375
train loss:  0.5519489049911499
train gradient:  0.18114728900475682
iteration : 2740
train acc:  0.6875
train loss:  0.5358444452285767
train gradient:  0.16538523407149455
iteration : 2741
train acc:  0.6640625
train loss:  0.5775343775749207
train gradient:  0.17678818842730623
iteration : 2742
train acc:  0.609375
train loss:  0.61838698387146
train gradient:  0.27596816959786163
iteration : 2743
train acc:  0.6640625
train loss:  0.5753933191299438
train gradient:  0.18701147148463299
iteration : 2744
train acc:  0.6953125
train loss:  0.527289867401123
train gradient:  0.1478844208332878
iteration : 2745
train acc:  0.671875
train loss:  0.5529593229293823
train gradient:  0.13936590340420374
iteration : 2746
train acc:  0.7734375
train loss:  0.5212109088897705
train gradient:  0.14388549690013253
iteration : 2747
train acc:  0.7734375
train loss:  0.48497721552848816
train gradient:  0.10390023105284511
iteration : 2748
train acc:  0.765625
train loss:  0.47240304946899414
train gradient:  0.15265190284432034
iteration : 2749
train acc:  0.625
train loss:  0.5624126195907593
train gradient:  0.15539241007217303
iteration : 2750
train acc:  0.671875
train loss:  0.5918418765068054
train gradient:  0.2760322389232452
iteration : 2751
train acc:  0.640625
train loss:  0.647087812423706
train gradient:  0.2174161683011498
iteration : 2752
train acc:  0.765625
train loss:  0.5015851259231567
train gradient:  0.1649350100637596
iteration : 2753
train acc:  0.6171875
train loss:  0.5892276167869568
train gradient:  0.25984964026851154
iteration : 2754
train acc:  0.703125
train loss:  0.555411696434021
train gradient:  0.18746338048118355
iteration : 2755
train acc:  0.6953125
train loss:  0.5491992831230164
train gradient:  0.15096920363420133
iteration : 2756
train acc:  0.7265625
train loss:  0.5537471771240234
train gradient:  0.13346762550807562
iteration : 2757
train acc:  0.6875
train loss:  0.6581498384475708
train gradient:  0.2568487688363871
iteration : 2758
train acc:  0.7265625
train loss:  0.5529869198799133
train gradient:  0.2282086223298299
iteration : 2759
train acc:  0.7265625
train loss:  0.5020363330841064
train gradient:  0.15424750614138963
iteration : 2760
train acc:  0.625
train loss:  0.6469565629959106
train gradient:  0.2529808353275526
iteration : 2761
train acc:  0.7109375
train loss:  0.5329819321632385
train gradient:  0.1710624374486876
iteration : 2762
train acc:  0.7265625
train loss:  0.5547525882720947
train gradient:  0.14943928833017092
iteration : 2763
train acc:  0.734375
train loss:  0.5102136135101318
train gradient:  0.16467482444777923
iteration : 2764
train acc:  0.6796875
train loss:  0.6108565926551819
train gradient:  0.1921104323451423
iteration : 2765
train acc:  0.6484375
train loss:  0.5489414930343628
train gradient:  0.23210557381039124
iteration : 2766
train acc:  0.734375
train loss:  0.5453646779060364
train gradient:  0.16735910198409723
iteration : 2767
train acc:  0.7421875
train loss:  0.5020077228546143
train gradient:  0.17722191016625988
iteration : 2768
train acc:  0.7109375
train loss:  0.543175220489502
train gradient:  0.1461476696560371
iteration : 2769
train acc:  0.71875
train loss:  0.5466073751449585
train gradient:  0.26517838069062644
iteration : 2770
train acc:  0.6796875
train loss:  0.5644689798355103
train gradient:  0.16587084717155803
iteration : 2771
train acc:  0.7421875
train loss:  0.49336904287338257
train gradient:  0.16457074098235397
iteration : 2772
train acc:  0.6953125
train loss:  0.5649641752243042
train gradient:  0.15528311669445982
iteration : 2773
train acc:  0.75
train loss:  0.5423344373703003
train gradient:  0.21809482577185602
iteration : 2774
train acc:  0.671875
train loss:  0.6067061424255371
train gradient:  0.17494931319789014
iteration : 2775
train acc:  0.6796875
train loss:  0.5580054521560669
train gradient:  0.1895458235616026
iteration : 2776
train acc:  0.703125
train loss:  0.5355032682418823
train gradient:  0.13741359600768877
iteration : 2777
train acc:  0.703125
train loss:  0.5386067628860474
train gradient:  0.15513663895015176
iteration : 2778
train acc:  0.734375
train loss:  0.5170680284500122
train gradient:  0.18609295511379964
iteration : 2779
train acc:  0.703125
train loss:  0.5226072669029236
train gradient:  0.171054241913245
iteration : 2780
train acc:  0.78125
train loss:  0.4812924563884735
train gradient:  0.1581798567095783
iteration : 2781
train acc:  0.625
train loss:  0.6026597619056702
train gradient:  0.2382132677130056
iteration : 2782
train acc:  0.703125
train loss:  0.5344871282577515
train gradient:  0.1753458868684316
iteration : 2783
train acc:  0.671875
train loss:  0.5213049054145813
train gradient:  0.1539832456076442
iteration : 2784
train acc:  0.7578125
train loss:  0.49730777740478516
train gradient:  0.1390919361677806
iteration : 2785
train acc:  0.71875
train loss:  0.5173823833465576
train gradient:  0.1979246857579569
iteration : 2786
train acc:  0.625
train loss:  0.5831500887870789
train gradient:  0.18862976195166925
iteration : 2787
train acc:  0.7265625
train loss:  0.5707530975341797
train gradient:  0.15789539386859286
iteration : 2788
train acc:  0.7109375
train loss:  0.560553789138794
train gradient:  0.15803884754108635
iteration : 2789
train acc:  0.6875
train loss:  0.5441842079162598
train gradient:  0.17298676680593628
iteration : 2790
train acc:  0.7109375
train loss:  0.5660573244094849
train gradient:  0.1876938021842251
iteration : 2791
train acc:  0.65625
train loss:  0.5614429712295532
train gradient:  0.13629703819563788
iteration : 2792
train acc:  0.7578125
train loss:  0.48070040345191956
train gradient:  0.15031820293106463
iteration : 2793
train acc:  0.75
train loss:  0.5624260306358337
train gradient:  0.15659867212486345
iteration : 2794
train acc:  0.71875
train loss:  0.569103479385376
train gradient:  0.13960543516257973
iteration : 2795
train acc:  0.78125
train loss:  0.5019216537475586
train gradient:  0.14092081699070685
iteration : 2796
train acc:  0.7421875
train loss:  0.47239333391189575
train gradient:  0.12862620640234496
iteration : 2797
train acc:  0.6640625
train loss:  0.5527933835983276
train gradient:  0.18228027232065427
iteration : 2798
train acc:  0.7421875
train loss:  0.4984744191169739
train gradient:  0.18058146549594423
iteration : 2799
train acc:  0.703125
train loss:  0.5496523976325989
train gradient:  0.19049981506021196
iteration : 2800
train acc:  0.71875
train loss:  0.5074877738952637
train gradient:  0.12793812548595762
iteration : 2801
train acc:  0.671875
train loss:  0.5752048492431641
train gradient:  0.18690933720398797
iteration : 2802
train acc:  0.7578125
train loss:  0.5027647018432617
train gradient:  0.1417742798655593
iteration : 2803
train acc:  0.734375
train loss:  0.5463346242904663
train gradient:  0.1719936975852203
iteration : 2804
train acc:  0.6875
train loss:  0.616524875164032
train gradient:  0.3370190606060329
iteration : 2805
train acc:  0.7265625
train loss:  0.5142920017242432
train gradient:  0.1394496559460201
iteration : 2806
train acc:  0.7734375
train loss:  0.4846510887145996
train gradient:  0.13182025047413198
iteration : 2807
train acc:  0.7421875
train loss:  0.5477397441864014
train gradient:  0.20942580200984176
iteration : 2808
train acc:  0.75
train loss:  0.518736720085144
train gradient:  0.12526388332638588
iteration : 2809
train acc:  0.6484375
train loss:  0.622989296913147
train gradient:  0.2315930430463534
iteration : 2810
train acc:  0.640625
train loss:  0.576652467250824
train gradient:  0.2102201461511063
iteration : 2811
train acc:  0.796875
train loss:  0.4890950918197632
train gradient:  0.1233266311250902
iteration : 2812
train acc:  0.7421875
train loss:  0.5292186737060547
train gradient:  0.14406297625439124
iteration : 2813
train acc:  0.6640625
train loss:  0.5455901026725769
train gradient:  0.216002936120056
iteration : 2814
train acc:  0.71875
train loss:  0.5307323932647705
train gradient:  0.12946818712954597
iteration : 2815
train acc:  0.7109375
train loss:  0.5647699236869812
train gradient:  0.25540899297062974
iteration : 2816
train acc:  0.7421875
train loss:  0.49927061796188354
train gradient:  0.1473123539200619
iteration : 2817
train acc:  0.828125
train loss:  0.40477898716926575
train gradient:  0.10310326535508957
iteration : 2818
train acc:  0.6484375
train loss:  0.6110373139381409
train gradient:  0.2137617542644603
iteration : 2819
train acc:  0.6796875
train loss:  0.5603076219558716
train gradient:  0.1560191722023841
iteration : 2820
train acc:  0.6796875
train loss:  0.6098194122314453
train gradient:  0.24110569651009417
iteration : 2821
train acc:  0.765625
train loss:  0.5080265998840332
train gradient:  0.23119143132422565
iteration : 2822
train acc:  0.671875
train loss:  0.5720705389976501
train gradient:  0.1632902273437694
iteration : 2823
train acc:  0.671875
train loss:  0.552954912185669
train gradient:  0.17611302670685647
iteration : 2824
train acc:  0.6171875
train loss:  0.6109213829040527
train gradient:  0.22128163489507907
iteration : 2825
train acc:  0.7265625
train loss:  0.5478397607803345
train gradient:  0.2443703930696034
iteration : 2826
train acc:  0.71875
train loss:  0.5762960910797119
train gradient:  0.19620519749988952
iteration : 2827
train acc:  0.71875
train loss:  0.48695775866508484
train gradient:  0.12451994876853696
iteration : 2828
train acc:  0.640625
train loss:  0.6000264883041382
train gradient:  0.2010160574433117
iteration : 2829
train acc:  0.65625
train loss:  0.5729802250862122
train gradient:  0.17229242814062834
iteration : 2830
train acc:  0.6171875
train loss:  0.5800155997276306
train gradient:  0.16397216338875595
iteration : 2831
train acc:  0.671875
train loss:  0.5789628028869629
train gradient:  0.17622208609247456
iteration : 2832
train acc:  0.7890625
train loss:  0.5191264152526855
train gradient:  0.17905978706702275
iteration : 2833
train acc:  0.703125
train loss:  0.5658496618270874
train gradient:  0.16749269510442463
iteration : 2834
train acc:  0.7109375
train loss:  0.5579088926315308
train gradient:  0.16165045617402102
iteration : 2835
train acc:  0.7265625
train loss:  0.5203383564949036
train gradient:  0.1480948720781306
iteration : 2836
train acc:  0.6640625
train loss:  0.5848008990287781
train gradient:  0.18030779381858655
iteration : 2837
train acc:  0.734375
train loss:  0.5093691349029541
train gradient:  0.15333081416585195
iteration : 2838
train acc:  0.71875
train loss:  0.5116967558860779
train gradient:  0.1379535389664604
iteration : 2839
train acc:  0.6328125
train loss:  0.5977391004562378
train gradient:  0.17683460426148173
iteration : 2840
train acc:  0.6953125
train loss:  0.5333302021026611
train gradient:  0.18066473958491164
iteration : 2841
train acc:  0.7421875
train loss:  0.5067207217216492
train gradient:  0.172430954231307
iteration : 2842
train acc:  0.6953125
train loss:  0.5911886692047119
train gradient:  0.17203460224781908
iteration : 2843
train acc:  0.7890625
train loss:  0.47598379850387573
train gradient:  0.1838527071232078
iteration : 2844
train acc:  0.796875
train loss:  0.46548548340797424
train gradient:  0.1579160647411308
iteration : 2845
train acc:  0.75
train loss:  0.5129529237747192
train gradient:  0.16896629491720244
iteration : 2846
train acc:  0.671875
train loss:  0.5805397629737854
train gradient:  0.17518810464318404
iteration : 2847
train acc:  0.7109375
train loss:  0.5860433578491211
train gradient:  0.1934687637565154
iteration : 2848
train acc:  0.78125
train loss:  0.4792635142803192
train gradient:  0.1568820731227772
iteration : 2849
train acc:  0.6875
train loss:  0.5622165203094482
train gradient:  0.15729844122618641
iteration : 2850
train acc:  0.7265625
train loss:  0.55657958984375
train gradient:  0.17461357012671563
iteration : 2851
train acc:  0.796875
train loss:  0.4953914284706116
train gradient:  0.13643001299093446
iteration : 2852
train acc:  0.6953125
train loss:  0.5701985359191895
train gradient:  0.24248099839476867
iteration : 2853
train acc:  0.703125
train loss:  0.5466758012771606
train gradient:  0.14495747537544765
iteration : 2854
train acc:  0.765625
train loss:  0.5165759921073914
train gradient:  0.14571733901854436
iteration : 2855
train acc:  0.6953125
train loss:  0.5291261076927185
train gradient:  0.14401883455276815
iteration : 2856
train acc:  0.6328125
train loss:  0.6273941993713379
train gradient:  0.22008129523812325
iteration : 2857
train acc:  0.625
train loss:  0.591519832611084
train gradient:  0.22174064587110706
iteration : 2858
train acc:  0.6484375
train loss:  0.595676839351654
train gradient:  0.22992848217439793
iteration : 2859
train acc:  0.6640625
train loss:  0.5866348147392273
train gradient:  0.16662297219128797
iteration : 2860
train acc:  0.7734375
train loss:  0.5181777477264404
train gradient:  0.16625278715445463
iteration : 2861
train acc:  0.6953125
train loss:  0.5307782888412476
train gradient:  0.14451876956157877
iteration : 2862
train acc:  0.703125
train loss:  0.5237520933151245
train gradient:  0.17997938159395427
iteration : 2863
train acc:  0.7421875
train loss:  0.49759140610694885
train gradient:  0.16896249136243785
iteration : 2864
train acc:  0.7421875
train loss:  0.4958312213420868
train gradient:  0.154209260064757
iteration : 2865
train acc:  0.671875
train loss:  0.5696247220039368
train gradient:  0.22524617718830436
iteration : 2866
train acc:  0.7265625
train loss:  0.5391749739646912
train gradient:  0.1408772648588752
iteration : 2867
train acc:  0.6640625
train loss:  0.5567700266838074
train gradient:  0.1669648904947802
iteration : 2868
train acc:  0.71875
train loss:  0.5243806838989258
train gradient:  0.1446009149075443
iteration : 2869
train acc:  0.6640625
train loss:  0.56975919008255
train gradient:  0.17270623582335726
iteration : 2870
train acc:  0.65625
train loss:  0.5612425804138184
train gradient:  0.18451127889835142
iteration : 2871
train acc:  0.6640625
train loss:  0.6099468469619751
train gradient:  0.17713418683061638
iteration : 2872
train acc:  0.75
train loss:  0.5370814800262451
train gradient:  0.15605484492363794
iteration : 2873
train acc:  0.671875
train loss:  0.5771484375
train gradient:  0.18572741093343215
iteration : 2874
train acc:  0.7109375
train loss:  0.5079560279846191
train gradient:  0.14480865680088112
iteration : 2875
train acc:  0.765625
train loss:  0.5308868885040283
train gradient:  0.15732248418806868
iteration : 2876
train acc:  0.71875
train loss:  0.49857813119888306
train gradient:  0.1288300616701723
iteration : 2877
train acc:  0.7265625
train loss:  0.5190335512161255
train gradient:  0.16658199804577442
iteration : 2878
train acc:  0.7890625
train loss:  0.49337393045425415
train gradient:  0.155941824841555
iteration : 2879
train acc:  0.8046875
train loss:  0.44742488861083984
train gradient:  0.13721608486631115
iteration : 2880
train acc:  0.8046875
train loss:  0.46736085414886475
train gradient:  0.1619154853649511
iteration : 2881
train acc:  0.75
train loss:  0.5133160352706909
train gradient:  0.16343353833702784
iteration : 2882
train acc:  0.71875
train loss:  0.5672177076339722
train gradient:  0.21747037400534802
iteration : 2883
train acc:  0.703125
train loss:  0.5179564952850342
train gradient:  0.13486315976157792
iteration : 2884
train acc:  0.734375
train loss:  0.4890879690647125
train gradient:  0.14074359989706237
iteration : 2885
train acc:  0.734375
train loss:  0.5309971570968628
train gradient:  0.17483685616119127
iteration : 2886
train acc:  0.671875
train loss:  0.5459716320037842
train gradient:  0.18048879931003217
iteration : 2887
train acc:  0.671875
train loss:  0.5564712285995483
train gradient:  0.14199145900503146
iteration : 2888
train acc:  0.7265625
train loss:  0.5073012709617615
train gradient:  0.1478874946165747
iteration : 2889
train acc:  0.734375
train loss:  0.571413516998291
train gradient:  0.220064803023111
iteration : 2890
train acc:  0.7265625
train loss:  0.5492009520530701
train gradient:  0.14138914476368714
iteration : 2891
train acc:  0.7265625
train loss:  0.5324122905731201
train gradient:  0.1534390425856592
iteration : 2892
train acc:  0.796875
train loss:  0.4938393235206604
train gradient:  0.13399684339794166
iteration : 2893
train acc:  0.71875
train loss:  0.5444432497024536
train gradient:  0.1453263121771129
iteration : 2894
train acc:  0.734375
train loss:  0.5546402931213379
train gradient:  0.16219242745134616
iteration : 2895
train acc:  0.7421875
train loss:  0.47098249197006226
train gradient:  0.10681846918784958
iteration : 2896
train acc:  0.7109375
train loss:  0.5385829210281372
train gradient:  0.2370559462740162
iteration : 2897
train acc:  0.703125
train loss:  0.5438966155052185
train gradient:  0.12605405921840102
iteration : 2898
train acc:  0.765625
train loss:  0.4860612154006958
train gradient:  0.14665501421830546
iteration : 2899
train acc:  0.65625
train loss:  0.5681551098823547
train gradient:  0.1758643351567452
iteration : 2900
train acc:  0.6796875
train loss:  0.5718992948532104
train gradient:  0.2011088849518729
iteration : 2901
train acc:  0.6875
train loss:  0.5138508081436157
train gradient:  0.14951625117663947
iteration : 2902
train acc:  0.71875
train loss:  0.5083851218223572
train gradient:  0.1959431876106395
iteration : 2903
train acc:  0.7109375
train loss:  0.5028728246688843
train gradient:  0.15238659185609826
iteration : 2904
train acc:  0.75
train loss:  0.5275373458862305
train gradient:  0.15027699616404566
iteration : 2905
train acc:  0.6875
train loss:  0.5537616014480591
train gradient:  0.17999141681503614
iteration : 2906
train acc:  0.7109375
train loss:  0.5173273682594299
train gradient:  0.17120040641112697
iteration : 2907
train acc:  0.71875
train loss:  0.5429624915122986
train gradient:  0.20042817360557008
iteration : 2908
train acc:  0.7578125
train loss:  0.5192049741744995
train gradient:  0.23884588777217636
iteration : 2909
train acc:  0.7109375
train loss:  0.557784914970398
train gradient:  0.13621687912190972
iteration : 2910
train acc:  0.703125
train loss:  0.6001111268997192
train gradient:  0.19757930737330548
iteration : 2911
train acc:  0.734375
train loss:  0.524399995803833
train gradient:  0.21888844355400933
iteration : 2912
train acc:  0.6875
train loss:  0.5135396718978882
train gradient:  0.15358494891449023
iteration : 2913
train acc:  0.7265625
train loss:  0.5453799962997437
train gradient:  0.1687426553063759
iteration : 2914
train acc:  0.734375
train loss:  0.534361720085144
train gradient:  0.15414904895628057
iteration : 2915
train acc:  0.734375
train loss:  0.4913303852081299
train gradient:  0.12138497582566336
iteration : 2916
train acc:  0.6640625
train loss:  0.561543881893158
train gradient:  0.17252005534162673
iteration : 2917
train acc:  0.71875
train loss:  0.539003312587738
train gradient:  0.15803472196813512
iteration : 2918
train acc:  0.6953125
train loss:  0.5402382016181946
train gradient:  0.16918400245321524
iteration : 2919
train acc:  0.71875
train loss:  0.49811774492263794
train gradient:  0.14043902381465173
iteration : 2920
train acc:  0.734375
train loss:  0.5017217993736267
train gradient:  0.13276981053083337
iteration : 2921
train acc:  0.6953125
train loss:  0.576971173286438
train gradient:  0.1663693344869353
iteration : 2922
train acc:  0.6796875
train loss:  0.5630625486373901
train gradient:  0.1689338221565139
iteration : 2923
train acc:  0.6875
train loss:  0.5436097383499146
train gradient:  0.16027814550617758
iteration : 2924
train acc:  0.734375
train loss:  0.5132318735122681
train gradient:  0.22624392947278169
iteration : 2925
train acc:  0.6796875
train loss:  0.6027064323425293
train gradient:  0.26531049859528344
iteration : 2926
train acc:  0.7421875
train loss:  0.5209357142448425
train gradient:  0.14107589069953413
iteration : 2927
train acc:  0.7109375
train loss:  0.498025119304657
train gradient:  0.1500962140429804
iteration : 2928
train acc:  0.7734375
train loss:  0.4454599618911743
train gradient:  0.13775166016352092
iteration : 2929
train acc:  0.7109375
train loss:  0.5249946117401123
train gradient:  0.16326268713527728
iteration : 2930
train acc:  0.7109375
train loss:  0.5818710327148438
train gradient:  0.19080388999016487
iteration : 2931
train acc:  0.703125
train loss:  0.5214820504188538
train gradient:  0.17529566780597178
iteration : 2932
train acc:  0.6328125
train loss:  0.5603867769241333
train gradient:  0.15418132886779665
iteration : 2933
train acc:  0.7265625
train loss:  0.47972068190574646
train gradient:  0.15321158129767026
iteration : 2934
train acc:  0.6953125
train loss:  0.552062451839447
train gradient:  0.19379930537827195
iteration : 2935
train acc:  0.75
train loss:  0.48197054862976074
train gradient:  0.17349399648674693
iteration : 2936
train acc:  0.7421875
train loss:  0.5291424989700317
train gradient:  0.23745153556285337
iteration : 2937
train acc:  0.6953125
train loss:  0.5551697015762329
train gradient:  0.20374152451348412
iteration : 2938
train acc:  0.640625
train loss:  0.5792319178581238
train gradient:  0.20964556830469427
iteration : 2939
train acc:  0.6953125
train loss:  0.545147180557251
train gradient:  0.2879724497141337
iteration : 2940
train acc:  0.703125
train loss:  0.5283719301223755
train gradient:  0.17835750669846978
iteration : 2941
train acc:  0.703125
train loss:  0.5443236827850342
train gradient:  0.1693412882320684
iteration : 2942
train acc:  0.671875
train loss:  0.5693202018737793
train gradient:  0.2618557867520621
iteration : 2943
train acc:  0.7265625
train loss:  0.5251387357711792
train gradient:  0.13953266276638404
iteration : 2944
train acc:  0.78125
train loss:  0.4633522927761078
train gradient:  0.14316322686056152
iteration : 2945
train acc:  0.7421875
train loss:  0.48729854822158813
train gradient:  0.20255268771226081
iteration : 2946
train acc:  0.75
train loss:  0.5421431064605713
train gradient:  0.12976637322242518
iteration : 2947
train acc:  0.7421875
train loss:  0.5305523872375488
train gradient:  0.1716866211952217
iteration : 2948
train acc:  0.734375
train loss:  0.5197633504867554
train gradient:  0.16214012312591214
iteration : 2949
train acc:  0.6796875
train loss:  0.5384296774864197
train gradient:  0.13802987579590972
iteration : 2950
train acc:  0.71875
train loss:  0.5492107272148132
train gradient:  0.20331852867283084
iteration : 2951
train acc:  0.6796875
train loss:  0.5542460680007935
train gradient:  0.17063553424074282
iteration : 2952
train acc:  0.7578125
train loss:  0.4903939366340637
train gradient:  0.1794455893075333
iteration : 2953
train acc:  0.71875
train loss:  0.5705869197845459
train gradient:  0.17983197408046092
iteration : 2954
train acc:  0.6015625
train loss:  0.6587289571762085
train gradient:  0.22669214324903542
iteration : 2955
train acc:  0.8046875
train loss:  0.4589075446128845
train gradient:  0.13632431894722521
iteration : 2956
train acc:  0.78125
train loss:  0.46701931953430176
train gradient:  0.12082311804253888
iteration : 2957
train acc:  0.734375
train loss:  0.5218748450279236
train gradient:  0.13427738728267508
iteration : 2958
train acc:  0.7109375
train loss:  0.613019585609436
train gradient:  0.16654443215861992
iteration : 2959
train acc:  0.7734375
train loss:  0.491595596075058
train gradient:  0.1613058948658432
iteration : 2960
train acc:  0.7265625
train loss:  0.48889780044555664
train gradient:  0.13001335076149573
iteration : 2961
train acc:  0.6953125
train loss:  0.5942108631134033
train gradient:  0.21264028923390815
iteration : 2962
train acc:  0.7421875
train loss:  0.514841616153717
train gradient:  0.13993307704466085
iteration : 2963
train acc:  0.71875
train loss:  0.5454869270324707
train gradient:  0.21484705063326437
iteration : 2964
train acc:  0.6953125
train loss:  0.5626248121261597
train gradient:  0.16829634787237963
iteration : 2965
train acc:  0.75
train loss:  0.4462520182132721
train gradient:  0.12212720494447184
iteration : 2966
train acc:  0.7421875
train loss:  0.5062302350997925
train gradient:  0.14517845276955393
iteration : 2967
train acc:  0.6640625
train loss:  0.5629944205284119
train gradient:  0.1569764708417778
iteration : 2968
train acc:  0.765625
train loss:  0.5051314830780029
train gradient:  0.13956839716507327
iteration : 2969
train acc:  0.671875
train loss:  0.5759939551353455
train gradient:  0.15949027197135798
iteration : 2970
train acc:  0.7265625
train loss:  0.5668919086456299
train gradient:  0.17822158042455688
iteration : 2971
train acc:  0.765625
train loss:  0.48499345779418945
train gradient:  0.12595576765819852
iteration : 2972
train acc:  0.71875
train loss:  0.5433008670806885
train gradient:  0.22014952126216386
iteration : 2973
train acc:  0.8125
train loss:  0.43642663955688477
train gradient:  0.11334356215745338
iteration : 2974
train acc:  0.6953125
train loss:  0.5551900863647461
train gradient:  0.20067493638471395
iteration : 2975
train acc:  0.75
train loss:  0.5285971164703369
train gradient:  0.2576124523528389
iteration : 2976
train acc:  0.703125
train loss:  0.5977455377578735
train gradient:  0.17931753052317834
iteration : 2977
train acc:  0.734375
train loss:  0.5522043704986572
train gradient:  0.1834660796986345
iteration : 2978
train acc:  0.7265625
train loss:  0.5557512044906616
train gradient:  0.1864274878503664
iteration : 2979
train acc:  0.7109375
train loss:  0.491606205701828
train gradient:  0.1766370967362152
iteration : 2980
train acc:  0.75
train loss:  0.5185638070106506
train gradient:  0.135265572745507
iteration : 2981
train acc:  0.7109375
train loss:  0.5652991533279419
train gradient:  0.2230148563229848
iteration : 2982
train acc:  0.7578125
train loss:  0.49384361505508423
train gradient:  0.1683880849766658
iteration : 2983
train acc:  0.7265625
train loss:  0.5285265445709229
train gradient:  0.13670179890678863
iteration : 2984
train acc:  0.7734375
train loss:  0.4759195148944855
train gradient:  0.13622769782898386
iteration : 2985
train acc:  0.671875
train loss:  0.5550515651702881
train gradient:  0.16587819307070945
iteration : 2986
train acc:  0.75
train loss:  0.4857868552207947
train gradient:  0.13697217625388247
iteration : 2987
train acc:  0.6796875
train loss:  0.5459630489349365
train gradient:  0.1930416434054249
iteration : 2988
train acc:  0.796875
train loss:  0.46180298924446106
train gradient:  0.10946861571674922
iteration : 2989
train acc:  0.75
train loss:  0.514188289642334
train gradient:  0.14548305112639082
iteration : 2990
train acc:  0.734375
train loss:  0.48352670669555664
train gradient:  0.12181215137121591
iteration : 2991
train acc:  0.734375
train loss:  0.519773542881012
train gradient:  0.19195043253655586
iteration : 2992
train acc:  0.7421875
train loss:  0.4901219606399536
train gradient:  0.14831035254321678
iteration : 2993
train acc:  0.765625
train loss:  0.5135881304740906
train gradient:  0.18001490433996464
iteration : 2994
train acc:  0.703125
train loss:  0.5724265575408936
train gradient:  0.16108054176548336
iteration : 2995
train acc:  0.671875
train loss:  0.5782723426818848
train gradient:  0.24543225195999036
iteration : 2996
train acc:  0.703125
train loss:  0.5046256184577942
train gradient:  0.14403372797608086
iteration : 2997
train acc:  0.6953125
train loss:  0.5215339660644531
train gradient:  0.16642162522918075
iteration : 2998
train acc:  0.71875
train loss:  0.5299786329269409
train gradient:  0.12859372434217484
iteration : 2999
train acc:  0.7734375
train loss:  0.4884563386440277
train gradient:  0.14988229630309613
iteration : 3000
train acc:  0.6796875
train loss:  0.6053298711776733
train gradient:  0.22301628543438956
iteration : 3001
train acc:  0.6875
train loss:  0.5657672882080078
train gradient:  0.20437489193251285
iteration : 3002
train acc:  0.75
train loss:  0.5221022367477417
train gradient:  0.1662492793868613
iteration : 3003
train acc:  0.7109375
train loss:  0.5160645842552185
train gradient:  0.1384117224399784
iteration : 3004
train acc:  0.7265625
train loss:  0.5199637413024902
train gradient:  0.16002919582754588
iteration : 3005
train acc:  0.71875
train loss:  0.5489749312400818
train gradient:  0.15580531250538274
iteration : 3006
train acc:  0.734375
train loss:  0.5472853183746338
train gradient:  0.2269736694581584
iteration : 3007
train acc:  0.765625
train loss:  0.5012744665145874
train gradient:  0.15432183828278867
iteration : 3008
train acc:  0.8125
train loss:  0.44664451479911804
train gradient:  0.12566542916290363
iteration : 3009
train acc:  0.7265625
train loss:  0.5583394765853882
train gradient:  0.18996377797873232
iteration : 3010
train acc:  0.75
train loss:  0.5005048513412476
train gradient:  0.15749460100858628
iteration : 3011
train acc:  0.65625
train loss:  0.575981855392456
train gradient:  0.22185482646828406
iteration : 3012
train acc:  0.6484375
train loss:  0.64491868019104
train gradient:  0.2029769903145019
iteration : 3013
train acc:  0.7265625
train loss:  0.5156170129776001
train gradient:  0.18215885867713863
iteration : 3014
train acc:  0.765625
train loss:  0.5013177394866943
train gradient:  0.12314968945596683
iteration : 3015
train acc:  0.734375
train loss:  0.49967214465141296
train gradient:  0.15070113962238324
iteration : 3016
train acc:  0.6796875
train loss:  0.5633813142776489
train gradient:  0.17737544668336966
iteration : 3017
train acc:  0.671875
train loss:  0.5939182043075562
train gradient:  0.1563030489059273
iteration : 3018
train acc:  0.6796875
train loss:  0.5556316375732422
train gradient:  0.2070655754692169
iteration : 3019
train acc:  0.7578125
train loss:  0.5015636682510376
train gradient:  0.16251948056509585
iteration : 3020
train acc:  0.640625
train loss:  0.5806365609169006
train gradient:  0.15962063712238533
iteration : 3021
train acc:  0.7109375
train loss:  0.5225213766098022
train gradient:  0.12345392168114148
iteration : 3022
train acc:  0.71875
train loss:  0.49129998683929443
train gradient:  0.14278683747474066
iteration : 3023
train acc:  0.7109375
train loss:  0.531570553779602
train gradient:  0.14476302913361844
iteration : 3024
train acc:  0.734375
train loss:  0.4976766109466553
train gradient:  0.16796500382820526
iteration : 3025
train acc:  0.75
train loss:  0.5495097637176514
train gradient:  0.1698227922576646
iteration : 3026
train acc:  0.765625
train loss:  0.51423180103302
train gradient:  0.16891030213423083
iteration : 3027
train acc:  0.671875
train loss:  0.608039140701294
train gradient:  0.27584616326510475
iteration : 3028
train acc:  0.71875
train loss:  0.5528713464736938
train gradient:  0.14594582189716782
iteration : 3029
train acc:  0.6640625
train loss:  0.5980962514877319
train gradient:  0.22764085045927362
iteration : 3030
train acc:  0.7421875
train loss:  0.49983471632003784
train gradient:  0.15688803323918737
iteration : 3031
train acc:  0.7109375
train loss:  0.5297379493713379
train gradient:  0.16419599835451995
iteration : 3032
train acc:  0.7890625
train loss:  0.48002997040748596
train gradient:  0.16766201960555305
iteration : 3033
train acc:  0.7265625
train loss:  0.5440503358840942
train gradient:  0.16107760677503452
iteration : 3034
train acc:  0.6640625
train loss:  0.5505402088165283
train gradient:  0.20050717932929107
iteration : 3035
train acc:  0.671875
train loss:  0.5648572444915771
train gradient:  0.24309275515432952
iteration : 3036
train acc:  0.6796875
train loss:  0.5464613437652588
train gradient:  0.17739401797340817
iteration : 3037
train acc:  0.671875
train loss:  0.5128917694091797
train gradient:  0.15023957188042983
iteration : 3038
train acc:  0.71875
train loss:  0.5271850824356079
train gradient:  0.22982281274475447
iteration : 3039
train acc:  0.7578125
train loss:  0.5302170515060425
train gradient:  0.18906011619719323
iteration : 3040
train acc:  0.6953125
train loss:  0.561931312084198
train gradient:  0.17847131736363955
iteration : 3041
train acc:  0.71875
train loss:  0.5389072895050049
train gradient:  0.1532769396134288
iteration : 3042
train acc:  0.7421875
train loss:  0.5057006478309631
train gradient:  0.14014503649130294
iteration : 3043
train acc:  0.625
train loss:  0.6164544224739075
train gradient:  0.19359316275299004
iteration : 3044
train acc:  0.6875
train loss:  0.5292116403579712
train gradient:  0.1648819173520955
iteration : 3045
train acc:  0.703125
train loss:  0.5394530296325684
train gradient:  0.16051617184882372
iteration : 3046
train acc:  0.65625
train loss:  0.6077253222465515
train gradient:  0.23652456226605162
iteration : 3047
train acc:  0.6484375
train loss:  0.6139259338378906
train gradient:  0.2105089228761973
iteration : 3048
train acc:  0.7734375
train loss:  0.48206546902656555
train gradient:  0.12842641918940267
iteration : 3049
train acc:  0.71875
train loss:  0.5253971815109253
train gradient:  0.15798909969880603
iteration : 3050
train acc:  0.734375
train loss:  0.4934675395488739
train gradient:  0.1451090492737716
iteration : 3051
train acc:  0.6875
train loss:  0.5236243009567261
train gradient:  0.17795413324888743
iteration : 3052
train acc:  0.71875
train loss:  0.53391033411026
train gradient:  0.1570169466951431
iteration : 3053
train acc:  0.6953125
train loss:  0.5611921548843384
train gradient:  0.15956050504242397
iteration : 3054
train acc:  0.671875
train loss:  0.5579214096069336
train gradient:  0.17746433074912177
iteration : 3055
train acc:  0.6875
train loss:  0.5884903073310852
train gradient:  0.16274090665459368
iteration : 3056
train acc:  0.6796875
train loss:  0.6000617742538452
train gradient:  0.1747858399900652
iteration : 3057
train acc:  0.6640625
train loss:  0.5260204076766968
train gradient:  0.13140060665821252
iteration : 3058
train acc:  0.6953125
train loss:  0.5204038023948669
train gradient:  0.16191621953139962
iteration : 3059
train acc:  0.6875
train loss:  0.5744233131408691
train gradient:  0.19373385433859286
iteration : 3060
train acc:  0.7421875
train loss:  0.5074231028556824
train gradient:  0.1983170061275746
iteration : 3061
train acc:  0.7265625
train loss:  0.5280848741531372
train gradient:  0.12730051457416913
iteration : 3062
train acc:  0.71875
train loss:  0.5249911546707153
train gradient:  0.18261814794343886
iteration : 3063
train acc:  0.71875
train loss:  0.5172430276870728
train gradient:  0.14077866280956972
iteration : 3064
train acc:  0.7109375
train loss:  0.5486728549003601
train gradient:  0.12958842511065682
iteration : 3065
train acc:  0.7421875
train loss:  0.5008621215820312
train gradient:  0.13878556670394104
iteration : 3066
train acc:  0.7734375
train loss:  0.5139998197555542
train gradient:  0.17495639255999684
iteration : 3067
train acc:  0.71875
train loss:  0.559258759021759
train gradient:  0.17014119321199345
iteration : 3068
train acc:  0.6171875
train loss:  0.6377012133598328
train gradient:  0.20708881297671394
iteration : 3069
train acc:  0.7578125
train loss:  0.49498826265335083
train gradient:  0.17767100049565715
iteration : 3070
train acc:  0.6796875
train loss:  0.5611598491668701
train gradient:  0.2180635498750461
iteration : 3071
train acc:  0.7265625
train loss:  0.5118305683135986
train gradient:  0.15575436356164307
iteration : 3072
train acc:  0.71875
train loss:  0.5086275339126587
train gradient:  0.14056258246328796
iteration : 3073
train acc:  0.7421875
train loss:  0.45973271131515503
train gradient:  0.1225943515248964
iteration : 3074
train acc:  0.6953125
train loss:  0.5240541696548462
train gradient:  0.15575996476928622
iteration : 3075
train acc:  0.78125
train loss:  0.5120406150817871
train gradient:  0.18669711908235115
iteration : 3076
train acc:  0.7265625
train loss:  0.5429172515869141
train gradient:  0.14864671353282322
iteration : 3077
train acc:  0.65625
train loss:  0.5766793489456177
train gradient:  0.19984703191206796
iteration : 3078
train acc:  0.71875
train loss:  0.530002772808075
train gradient:  0.20206275055509348
iteration : 3079
train acc:  0.734375
train loss:  0.4928683340549469
train gradient:  0.12314152050120947
iteration : 3080
train acc:  0.703125
train loss:  0.5630061030387878
train gradient:  0.21863530245937896
iteration : 3081
train acc:  0.703125
train loss:  0.5978485345840454
train gradient:  0.20950093752711568
iteration : 3082
train acc:  0.6171875
train loss:  0.5959489345550537
train gradient:  0.21574065634285208
iteration : 3083
train acc:  0.734375
train loss:  0.5618655681610107
train gradient:  0.16696948853988314
iteration : 3084
train acc:  0.6875
train loss:  0.5596854090690613
train gradient:  0.18944596633473076
iteration : 3085
train acc:  0.7578125
train loss:  0.4958171546459198
train gradient:  0.16368382598931325
iteration : 3086
train acc:  0.75
train loss:  0.4735909700393677
train gradient:  0.12085562811320599
iteration : 3087
train acc:  0.734375
train loss:  0.5736424326896667
train gradient:  0.17006858025336904
iteration : 3088
train acc:  0.71875
train loss:  0.5130401849746704
train gradient:  0.1829100623271407
iteration : 3089
train acc:  0.765625
train loss:  0.5088691711425781
train gradient:  0.12711252480586874
iteration : 3090
train acc:  0.6640625
train loss:  0.5979880094528198
train gradient:  0.18835451010453985
iteration : 3091
train acc:  0.75
train loss:  0.5478807687759399
train gradient:  0.14967004703915107
iteration : 3092
train acc:  0.78125
train loss:  0.4690393805503845
train gradient:  0.13738802240621584
iteration : 3093
train acc:  0.6875
train loss:  0.539181113243103
train gradient:  0.1785453475414981
iteration : 3094
train acc:  0.8203125
train loss:  0.45465874671936035
train gradient:  0.18277917914203687
iteration : 3095
train acc:  0.6796875
train loss:  0.6044425368309021
train gradient:  0.20511828657006784
iteration : 3096
train acc:  0.6484375
train loss:  0.5737961530685425
train gradient:  0.20208981549719446
iteration : 3097
train acc:  0.7578125
train loss:  0.5004986524581909
train gradient:  0.1385416099344713
iteration : 3098
train acc:  0.703125
train loss:  0.5241458415985107
train gradient:  0.16515246725524746
iteration : 3099
train acc:  0.71875
train loss:  0.5791791081428528
train gradient:  0.17000516968478607
iteration : 3100
train acc:  0.6796875
train loss:  0.5166471004486084
train gradient:  0.1423289420083783
iteration : 3101
train acc:  0.71875
train loss:  0.49067461490631104
train gradient:  0.13776353341072747
iteration : 3102
train acc:  0.7578125
train loss:  0.5404860377311707
train gradient:  0.17927988259673955
iteration : 3103
train acc:  0.7109375
train loss:  0.5008018016815186
train gradient:  0.1409516536158521
iteration : 3104
train acc:  0.671875
train loss:  0.5535391569137573
train gradient:  0.17455408374179093
iteration : 3105
train acc:  0.734375
train loss:  0.49490708112716675
train gradient:  0.17407065205410613
iteration : 3106
train acc:  0.703125
train loss:  0.5456964373588562
train gradient:  0.19125917627580558
iteration : 3107
train acc:  0.671875
train loss:  0.5604135394096375
train gradient:  0.20079920493459358
iteration : 3108
train acc:  0.7109375
train loss:  0.5261354446411133
train gradient:  0.16857684005374507
iteration : 3109
train acc:  0.671875
train loss:  0.515967607498169
train gradient:  0.1523813829499382
iteration : 3110
train acc:  0.703125
train loss:  0.5303999185562134
train gradient:  0.13267009696597423
iteration : 3111
train acc:  0.75
train loss:  0.5324004292488098
train gradient:  0.14315562484953326
iteration : 3112
train acc:  0.703125
train loss:  0.56950843334198
train gradient:  0.1716158906719782
iteration : 3113
train acc:  0.7421875
train loss:  0.5216594338417053
train gradient:  0.1266414162654239
iteration : 3114
train acc:  0.7421875
train loss:  0.5649353265762329
train gradient:  0.2232492671086878
iteration : 3115
train acc:  0.703125
train loss:  0.5417816042900085
train gradient:  0.1443609493063257
iteration : 3116
train acc:  0.6640625
train loss:  0.5311143398284912
train gradient:  0.15909264242287666
iteration : 3117
train acc:  0.75
train loss:  0.5186081528663635
train gradient:  0.19084094651020772
iteration : 3118
train acc:  0.65625
train loss:  0.5824031829833984
train gradient:  0.1773909827853971
iteration : 3119
train acc:  0.671875
train loss:  0.6043604016304016
train gradient:  0.18768529025295738
iteration : 3120
train acc:  0.6640625
train loss:  0.587372899055481
train gradient:  0.1894984658708752
iteration : 3121
train acc:  0.6796875
train loss:  0.5767611265182495
train gradient:  0.18241003027406688
iteration : 3122
train acc:  0.7109375
train loss:  0.527614951133728
train gradient:  0.14855917972449278
iteration : 3123
train acc:  0.6875
train loss:  0.5607078075408936
train gradient:  0.2209681746222174
iteration : 3124
train acc:  0.671875
train loss:  0.5786502361297607
train gradient:  0.17731811384551657
iteration : 3125
train acc:  0.6875
train loss:  0.5833812355995178
train gradient:  0.16811534311022278
iteration : 3126
train acc:  0.71875
train loss:  0.5202299952507019
train gradient:  0.22876660769560492
iteration : 3127
train acc:  0.703125
train loss:  0.5478852391242981
train gradient:  0.1764414758712476
iteration : 3128
train acc:  0.734375
train loss:  0.5064128637313843
train gradient:  0.21032457041733016
iteration : 3129
train acc:  0.78125
train loss:  0.5011938810348511
train gradient:  0.16381635886949225
iteration : 3130
train acc:  0.6953125
train loss:  0.5617932677268982
train gradient:  0.2034429439606425
iteration : 3131
train acc:  0.6640625
train loss:  0.5843067169189453
train gradient:  0.24259908758776894
iteration : 3132
train acc:  0.75
train loss:  0.49697965383529663
train gradient:  0.13490209309169116
iteration : 3133
train acc:  0.6953125
train loss:  0.5353142023086548
train gradient:  0.15422990561203942
iteration : 3134
train acc:  0.7109375
train loss:  0.5312439203262329
train gradient:  0.2066212256684743
iteration : 3135
train acc:  0.640625
train loss:  0.6280331611633301
train gradient:  0.2561152227417871
iteration : 3136
train acc:  0.75
train loss:  0.5238282084465027
train gradient:  0.17414228686777344
iteration : 3137
train acc:  0.75
train loss:  0.4976610541343689
train gradient:  0.1820051013153511
iteration : 3138
train acc:  0.765625
train loss:  0.4965342879295349
train gradient:  0.1336652672516202
iteration : 3139
train acc:  0.7109375
train loss:  0.5487686395645142
train gradient:  0.1411556046357446
iteration : 3140
train acc:  0.7265625
train loss:  0.5891318321228027
train gradient:  0.20459193426868758
iteration : 3141
train acc:  0.6796875
train loss:  0.5977458953857422
train gradient:  0.22165014663585633
iteration : 3142
train acc:  0.7265625
train loss:  0.5170019865036011
train gradient:  0.1695865185329712
iteration : 3143
train acc:  0.6328125
train loss:  0.6251136064529419
train gradient:  0.2672669415483273
iteration : 3144
train acc:  0.6953125
train loss:  0.5283288359642029
train gradient:  0.19017311416619498
iteration : 3145
train acc:  0.7734375
train loss:  0.47860679030418396
train gradient:  0.14830514061313735
iteration : 3146
train acc:  0.7265625
train loss:  0.5333514213562012
train gradient:  0.13499804856020295
iteration : 3147
train acc:  0.734375
train loss:  0.4937116801738739
train gradient:  0.13027869999391611
iteration : 3148
train acc:  0.7109375
train loss:  0.5259813070297241
train gradient:  0.16646526998933375
iteration : 3149
train acc:  0.65625
train loss:  0.5979386568069458
train gradient:  0.18128521680896606
iteration : 3150
train acc:  0.75
train loss:  0.5178699493408203
train gradient:  0.12053871539369442
iteration : 3151
train acc:  0.7734375
train loss:  0.5327352285385132
train gradient:  0.1460546727530972
iteration : 3152
train acc:  0.7578125
train loss:  0.5280842781066895
train gradient:  0.17304415822132146
iteration : 3153
train acc:  0.7109375
train loss:  0.5485419034957886
train gradient:  0.16926671683059677
iteration : 3154
train acc:  0.6640625
train loss:  0.5919733047485352
train gradient:  0.19117102208429976
iteration : 3155
train acc:  0.71875
train loss:  0.5666671991348267
train gradient:  0.18363120737180152
iteration : 3156
train acc:  0.7109375
train loss:  0.4999627470970154
train gradient:  0.20026239899412865
iteration : 3157
train acc:  0.6953125
train loss:  0.5184114575386047
train gradient:  0.1541243013056573
iteration : 3158
train acc:  0.6796875
train loss:  0.5791565179824829
train gradient:  0.16079108647931722
iteration : 3159
train acc:  0.703125
train loss:  0.6351178884506226
train gradient:  0.3614461671444563
iteration : 3160
train acc:  0.7421875
train loss:  0.53042072057724
train gradient:  0.15658210830399105
iteration : 3161
train acc:  0.796875
train loss:  0.48862993717193604
train gradient:  0.19255544563831722
iteration : 3162
train acc:  0.71875
train loss:  0.5564078092575073
train gradient:  0.2563269227165269
iteration : 3163
train acc:  0.7421875
train loss:  0.5196462273597717
train gradient:  0.16957341747742022
iteration : 3164
train acc:  0.6640625
train loss:  0.5696107149124146
train gradient:  0.19688445289439743
iteration : 3165
train acc:  0.71875
train loss:  0.5476733446121216
train gradient:  0.18985461610075705
iteration : 3166
train acc:  0.7890625
train loss:  0.5089395642280579
train gradient:  0.15190918893739014
iteration : 3167
train acc:  0.75
train loss:  0.5393276810646057
train gradient:  0.1969468570465387
iteration : 3168
train acc:  0.671875
train loss:  0.5400670766830444
train gradient:  0.16550961314890933
iteration : 3169
train acc:  0.6796875
train loss:  0.5633772611618042
train gradient:  0.21157553572483007
iteration : 3170
train acc:  0.7109375
train loss:  0.5284569263458252
train gradient:  0.19091951282527608
iteration : 3171
train acc:  0.6953125
train loss:  0.6110975742340088
train gradient:  0.23077469872427248
iteration : 3172
train acc:  0.75
train loss:  0.47042062878608704
train gradient:  0.13909739128438436
iteration : 3173
train acc:  0.6875
train loss:  0.5817753076553345
train gradient:  0.21534028639785024
iteration : 3174
train acc:  0.7265625
train loss:  0.4964350461959839
train gradient:  0.1455675620164276
iteration : 3175
train acc:  0.71875
train loss:  0.5048056244850159
train gradient:  0.18427796678919167
iteration : 3176
train acc:  0.7109375
train loss:  0.5172144174575806
train gradient:  0.13951025408297996
iteration : 3177
train acc:  0.8125
train loss:  0.472517728805542
train gradient:  0.1412254226348791
iteration : 3178
train acc:  0.75
train loss:  0.5280548334121704
train gradient:  0.1397088291591186
iteration : 3179
train acc:  0.703125
train loss:  0.517680287361145
train gradient:  0.18769339637055515
iteration : 3180
train acc:  0.7578125
train loss:  0.47820621728897095
train gradient:  0.12336875767122246
iteration : 3181
train acc:  0.6796875
train loss:  0.5736641883850098
train gradient:  0.1737223611233072
iteration : 3182
train acc:  0.75
train loss:  0.4891144037246704
train gradient:  0.1592706644875113
iteration : 3183
train acc:  0.640625
train loss:  0.5937943458557129
train gradient:  0.2002524465582129
iteration : 3184
train acc:  0.703125
train loss:  0.5835012197494507
train gradient:  0.14641941195859942
iteration : 3185
train acc:  0.671875
train loss:  0.5869185924530029
train gradient:  0.22716138955463056
iteration : 3186
train acc:  0.7890625
train loss:  0.4493335485458374
train gradient:  0.11824576269469891
iteration : 3187
train acc:  0.6953125
train loss:  0.5570053458213806
train gradient:  0.1683888482132722
iteration : 3188
train acc:  0.7109375
train loss:  0.5452671051025391
train gradient:  0.15661499993130312
iteration : 3189
train acc:  0.71875
train loss:  0.5314405560493469
train gradient:  0.17497957916542378
iteration : 3190
train acc:  0.6953125
train loss:  0.5154125690460205
train gradient:  0.1704416374890256
iteration : 3191
train acc:  0.75
train loss:  0.5139192938804626
train gradient:  0.1820399859642484
iteration : 3192
train acc:  0.71875
train loss:  0.5451627373695374
train gradient:  0.1433554718537059
iteration : 3193
train acc:  0.7734375
train loss:  0.46241921186447144
train gradient:  0.1367935025416499
iteration : 3194
train acc:  0.6953125
train loss:  0.5802867412567139
train gradient:  0.20767567647598462
iteration : 3195
train acc:  0.71875
train loss:  0.5218284130096436
train gradient:  0.1422798022904679
iteration : 3196
train acc:  0.734375
train loss:  0.5650843381881714
train gradient:  0.25018847901453717
iteration : 3197
train acc:  0.8046875
train loss:  0.4673566222190857
train gradient:  0.13397456126398916
iteration : 3198
train acc:  0.6796875
train loss:  0.559796154499054
train gradient:  0.1910964184081776
iteration : 3199
train acc:  0.734375
train loss:  0.5315841436386108
train gradient:  0.1792100277544394
iteration : 3200
train acc:  0.765625
train loss:  0.47465378046035767
train gradient:  0.1626093021898014
iteration : 3201
train acc:  0.6171875
train loss:  0.6316828727722168
train gradient:  0.2080397096902894
iteration : 3202
train acc:  0.7109375
train loss:  0.5770343542098999
train gradient:  0.17657213002759503
iteration : 3203
train acc:  0.75
train loss:  0.49259525537490845
train gradient:  0.24117887488548573
iteration : 3204
train acc:  0.796875
train loss:  0.5149857401847839
train gradient:  0.1557434975328867
iteration : 3205
train acc:  0.71875
train loss:  0.5473222136497498
train gradient:  0.18133995956987536
iteration : 3206
train acc:  0.75
train loss:  0.4893034100532532
train gradient:  0.1880028874562052
iteration : 3207
train acc:  0.71875
train loss:  0.5226705074310303
train gradient:  0.15940652059850646
iteration : 3208
train acc:  0.7109375
train loss:  0.5759178400039673
train gradient:  0.19343357049561366
iteration : 3209
train acc:  0.71875
train loss:  0.5264012217521667
train gradient:  0.17254095536813038
iteration : 3210
train acc:  0.6796875
train loss:  0.5194470286369324
train gradient:  0.1853125288696959
iteration : 3211
train acc:  0.7734375
train loss:  0.4556199610233307
train gradient:  0.1499235307392587
iteration : 3212
train acc:  0.703125
train loss:  0.5110785365104675
train gradient:  0.2133047006491281
iteration : 3213
train acc:  0.640625
train loss:  0.5575556755065918
train gradient:  0.1517901704971123
iteration : 3214
train acc:  0.6796875
train loss:  0.5401298403739929
train gradient:  0.13848470986857653
iteration : 3215
train acc:  0.8046875
train loss:  0.48732006549835205
train gradient:  0.17046443764211197
iteration : 3216
train acc:  0.75
train loss:  0.5215578675270081
train gradient:  0.18454862916567644
iteration : 3217
train acc:  0.703125
train loss:  0.5962673425674438
train gradient:  0.2035915381334839
iteration : 3218
train acc:  0.71875
train loss:  0.5277090072631836
train gradient:  0.16364575400975664
iteration : 3219
train acc:  0.671875
train loss:  0.5503156185150146
train gradient:  0.19593007492853265
iteration : 3220
train acc:  0.65625
train loss:  0.5719971656799316
train gradient:  0.2305120401129656
iteration : 3221
train acc:  0.7578125
train loss:  0.524463415145874
train gradient:  0.1519893776014893
iteration : 3222
train acc:  0.8125
train loss:  0.4476301074028015
train gradient:  0.14057965657327137
iteration : 3223
train acc:  0.7890625
train loss:  0.46910524368286133
train gradient:  0.13349855454844314
iteration : 3224
train acc:  0.7578125
train loss:  0.48075830936431885
train gradient:  0.13999199963524228
iteration : 3225
train acc:  0.7578125
train loss:  0.512451171875
train gradient:  0.13111610224071257
iteration : 3226
train acc:  0.65625
train loss:  0.5461037755012512
train gradient:  0.1779526933052753
iteration : 3227
train acc:  0.75
train loss:  0.49335718154907227
train gradient:  0.1143802545977225
iteration : 3228
train acc:  0.7265625
train loss:  0.5420595407485962
train gradient:  0.1755499213964917
iteration : 3229
train acc:  0.7734375
train loss:  0.4502035975456238
train gradient:  0.13826828526606327
iteration : 3230
train acc:  0.734375
train loss:  0.6017143130302429
train gradient:  0.21924784518583362
iteration : 3231
train acc:  0.734375
train loss:  0.5244626998901367
train gradient:  0.18753830278861128
iteration : 3232
train acc:  0.75
train loss:  0.4853758215904236
train gradient:  0.12656800181013766
iteration : 3233
train acc:  0.75
train loss:  0.5131575465202332
train gradient:  0.1332520691275781
iteration : 3234
train acc:  0.765625
train loss:  0.4997619390487671
train gradient:  0.1253843727673874
iteration : 3235
train acc:  0.7421875
train loss:  0.4806738495826721
train gradient:  0.1495796675494701
iteration : 3236
train acc:  0.7578125
train loss:  0.47411876916885376
train gradient:  0.1383134668030735
iteration : 3237
train acc:  0.6640625
train loss:  0.5751621127128601
train gradient:  0.24817831413565594
iteration : 3238
train acc:  0.7265625
train loss:  0.5090956687927246
train gradient:  0.13278867810002581
iteration : 3239
train acc:  0.71875
train loss:  0.5037192702293396
train gradient:  0.1287461239244566
iteration : 3240
train acc:  0.78125
train loss:  0.4667801260948181
train gradient:  0.16188013397488396
iteration : 3241
train acc:  0.671875
train loss:  0.5923169851303101
train gradient:  0.256692803160144
iteration : 3242
train acc:  0.71875
train loss:  0.5129330158233643
train gradient:  0.14143666789562237
iteration : 3243
train acc:  0.7109375
train loss:  0.575478732585907
train gradient:  0.2025797587129832
iteration : 3244
train acc:  0.78125
train loss:  0.491507351398468
train gradient:  0.19247220954773142
iteration : 3245
train acc:  0.78125
train loss:  0.49081385135650635
train gradient:  0.13972235617459705
iteration : 3246
train acc:  0.7109375
train loss:  0.5818933844566345
train gradient:  0.17102991061888118
iteration : 3247
train acc:  0.6953125
train loss:  0.5327544212341309
train gradient:  0.23273533916119343
iteration : 3248
train acc:  0.765625
train loss:  0.4994131624698639
train gradient:  0.15000576700645196
iteration : 3249
train acc:  0.71875
train loss:  0.5095053911209106
train gradient:  0.14404114813157076
iteration : 3250
train acc:  0.71875
train loss:  0.4972059726715088
train gradient:  0.1549789007488881
iteration : 3251
train acc:  0.7734375
train loss:  0.49930375814437866
train gradient:  0.1514438947233094
iteration : 3252
train acc:  0.6640625
train loss:  0.5989079475402832
train gradient:  0.17522670458857065
iteration : 3253
train acc:  0.6953125
train loss:  0.5590406656265259
train gradient:  0.1930267082409472
iteration : 3254
train acc:  0.7734375
train loss:  0.4704538881778717
train gradient:  0.16143261337612438
iteration : 3255
train acc:  0.6796875
train loss:  0.5261573195457458
train gradient:  0.17589508131842688
iteration : 3256
train acc:  0.7109375
train loss:  0.547996997833252
train gradient:  0.20647109717072037
iteration : 3257
train acc:  0.6953125
train loss:  0.5689330101013184
train gradient:  0.18283915083372546
iteration : 3258
train acc:  0.6953125
train loss:  0.6087437868118286
train gradient:  0.23475478526497687
iteration : 3259
train acc:  0.71875
train loss:  0.5361407995223999
train gradient:  0.22256714728946247
iteration : 3260
train acc:  0.6875
train loss:  0.5233509540557861
train gradient:  0.1659783171493686
iteration : 3261
train acc:  0.7265625
train loss:  0.5223683714866638
train gradient:  0.1802281160345827
iteration : 3262
train acc:  0.75
train loss:  0.534705638885498
train gradient:  0.17089031350027228
iteration : 3263
train acc:  0.7265625
train loss:  0.5171114802360535
train gradient:  0.16156028558284297
iteration : 3264
train acc:  0.65625
train loss:  0.5926585793495178
train gradient:  0.18042068464791505
iteration : 3265
train acc:  0.7421875
train loss:  0.5264649391174316
train gradient:  0.1651606142184079
iteration : 3266
train acc:  0.78125
train loss:  0.4800030589103699
train gradient:  0.14399174911267715
iteration : 3267
train acc:  0.7578125
train loss:  0.5258813500404358
train gradient:  0.14806636254926736
iteration : 3268
train acc:  0.6796875
train loss:  0.5631240606307983
train gradient:  0.16551743853127793
iteration : 3269
train acc:  0.7265625
train loss:  0.5687898397445679
train gradient:  0.1720698874161497
iteration : 3270
train acc:  0.765625
train loss:  0.47630512714385986
train gradient:  0.143824486651205
iteration : 3271
train acc:  0.703125
train loss:  0.5435307621955872
train gradient:  0.15663496390183906
iteration : 3272
train acc:  0.6875
train loss:  0.5694276094436646
train gradient:  0.1833685548834778
iteration : 3273
train acc:  0.71875
train loss:  0.47030842304229736
train gradient:  0.14544142266662918
iteration : 3274
train acc:  0.703125
train loss:  0.5171908140182495
train gradient:  0.20535249326446692
iteration : 3275
train acc:  0.71875
train loss:  0.5369380116462708
train gradient:  0.156394369802067
iteration : 3276
train acc:  0.7890625
train loss:  0.4577646851539612
train gradient:  0.11895956851590504
iteration : 3277
train acc:  0.640625
train loss:  0.5978406667709351
train gradient:  0.2829446323348496
iteration : 3278
train acc:  0.671875
train loss:  0.5767085552215576
train gradient:  0.20742331013507856
iteration : 3279
train acc:  0.6875
train loss:  0.5349996089935303
train gradient:  0.16784372709076145
iteration : 3280
train acc:  0.7421875
train loss:  0.5556806325912476
train gradient:  0.21789943737492015
iteration : 3281
train acc:  0.765625
train loss:  0.47734904289245605
train gradient:  0.13397543700872755
iteration : 3282
train acc:  0.71875
train loss:  0.5257071256637573
train gradient:  0.1833101454139216
iteration : 3283
train acc:  0.7265625
train loss:  0.5171817541122437
train gradient:  0.14543651142113795
iteration : 3284
train acc:  0.7890625
train loss:  0.4587332308292389
train gradient:  0.22662317550953717
iteration : 3285
train acc:  0.6640625
train loss:  0.5758388042449951
train gradient:  0.2491952633763126
iteration : 3286
train acc:  0.6796875
train loss:  0.564303457736969
train gradient:  0.18705959440896858
iteration : 3287
train acc:  0.78125
train loss:  0.465822696685791
train gradient:  0.14795488268767343
iteration : 3288
train acc:  0.703125
train loss:  0.5093514323234558
train gradient:  0.19721221094084185
iteration : 3289
train acc:  0.6640625
train loss:  0.5502400398254395
train gradient:  0.20731704455105007
iteration : 3290
train acc:  0.7578125
train loss:  0.49210429191589355
train gradient:  0.14587002072350608
iteration : 3291
train acc:  0.671875
train loss:  0.6042505502700806
train gradient:  0.2334078015458072
iteration : 3292
train acc:  0.6328125
train loss:  0.6374444961547852
train gradient:  0.2337495666820315
iteration : 3293
train acc:  0.7578125
train loss:  0.4941226840019226
train gradient:  0.15214455055882214
iteration : 3294
train acc:  0.6796875
train loss:  0.5906603336334229
train gradient:  0.17106732523431795
iteration : 3295
train acc:  0.6484375
train loss:  0.600962221622467
train gradient:  0.21848790463997422
iteration : 3296
train acc:  0.6953125
train loss:  0.5639349222183228
train gradient:  0.1898700456893777
iteration : 3297
train acc:  0.734375
train loss:  0.5532809495925903
train gradient:  0.1832848473532937
iteration : 3298
train acc:  0.78125
train loss:  0.5437288880348206
train gradient:  0.1397336155231853
iteration : 3299
train acc:  0.765625
train loss:  0.5027785301208496
train gradient:  0.13154829448728816
iteration : 3300
train acc:  0.71875
train loss:  0.5099461674690247
train gradient:  0.1683712877006307
iteration : 3301
train acc:  0.7578125
train loss:  0.4785403609275818
train gradient:  0.139202688060356
iteration : 3302
train acc:  0.75
train loss:  0.544432520866394
train gradient:  0.199227315776879
iteration : 3303
train acc:  0.6875
train loss:  0.5665935277938843
train gradient:  0.20718994686933556
iteration : 3304
train acc:  0.703125
train loss:  0.5287169218063354
train gradient:  0.1697946631181551
iteration : 3305
train acc:  0.671875
train loss:  0.5844607949256897
train gradient:  0.1922505866709775
iteration : 3306
train acc:  0.75
train loss:  0.5131577253341675
train gradient:  0.15548940217096607
iteration : 3307
train acc:  0.7890625
train loss:  0.4729614853858948
train gradient:  0.16139709978588054
iteration : 3308
train acc:  0.6796875
train loss:  0.5100462436676025
train gradient:  0.14461018387939314
iteration : 3309
train acc:  0.703125
train loss:  0.5198158025741577
train gradient:  0.15492515824364134
iteration : 3310
train acc:  0.7421875
train loss:  0.5169811248779297
train gradient:  0.1865164125373392
iteration : 3311
train acc:  0.7421875
train loss:  0.5206324458122253
train gradient:  0.1758351114158156
iteration : 3312
train acc:  0.6953125
train loss:  0.5603833198547363
train gradient:  0.17775227910191146
iteration : 3313
train acc:  0.703125
train loss:  0.5386168360710144
train gradient:  0.18486141274438955
iteration : 3314
train acc:  0.71875
train loss:  0.5247370004653931
train gradient:  0.15186832187553095
iteration : 3315
train acc:  0.765625
train loss:  0.49247515201568604
train gradient:  0.20164236337574268
iteration : 3316
train acc:  0.71875
train loss:  0.5633164644241333
train gradient:  0.1720253095991786
iteration : 3317
train acc:  0.734375
train loss:  0.5482058525085449
train gradient:  0.15233034481017757
iteration : 3318
train acc:  0.71875
train loss:  0.5535053610801697
train gradient:  0.22145665077722065
iteration : 3319
train acc:  0.6171875
train loss:  0.692226767539978
train gradient:  0.2952407660575865
iteration : 3320
train acc:  0.671875
train loss:  0.5250775814056396
train gradient:  0.18304174328486422
iteration : 3321
train acc:  0.6796875
train loss:  0.5530045032501221
train gradient:  0.17827554708116033
iteration : 3322
train acc:  0.734375
train loss:  0.5390297770500183
train gradient:  0.1945022464152461
iteration : 3323
train acc:  0.7421875
train loss:  0.48283836245536804
train gradient:  0.13962477999239617
iteration : 3324
train acc:  0.7109375
train loss:  0.5223057270050049
train gradient:  0.19838664040351472
iteration : 3325
train acc:  0.7265625
train loss:  0.5403452515602112
train gradient:  0.1609292814591383
iteration : 3326
train acc:  0.7578125
train loss:  0.5218455791473389
train gradient:  0.17298187058210718
iteration : 3327
train acc:  0.6640625
train loss:  0.5973979234695435
train gradient:  0.18851036765940862
iteration : 3328
train acc:  0.6640625
train loss:  0.580459475517273
train gradient:  0.18159234393547657
iteration : 3329
train acc:  0.6796875
train loss:  0.5940306186676025
train gradient:  0.16176633593426037
iteration : 3330
train acc:  0.7109375
train loss:  0.525245189666748
train gradient:  0.12249917018938683
iteration : 3331
train acc:  0.734375
train loss:  0.5159436464309692
train gradient:  0.14347889172827233
iteration : 3332
train acc:  0.6875
train loss:  0.5611819624900818
train gradient:  0.2801172939415606
iteration : 3333
train acc:  0.71875
train loss:  0.5426344871520996
train gradient:  0.17608855642203028
iteration : 3334
train acc:  0.6953125
train loss:  0.5231727361679077
train gradient:  0.1757204687237024
iteration : 3335
train acc:  0.7734375
train loss:  0.49812471866607666
train gradient:  0.11858766991878905
iteration : 3336
train acc:  0.7265625
train loss:  0.5108062028884888
train gradient:  0.14156335697307254
iteration : 3337
train acc:  0.78125
train loss:  0.5057392716407776
train gradient:  0.20308398244214182
iteration : 3338
train acc:  0.75
train loss:  0.47121793031692505
train gradient:  0.15987927507783933
iteration : 3339
train acc:  0.6484375
train loss:  0.5560073256492615
train gradient:  0.16064246561175527
iteration : 3340
train acc:  0.796875
train loss:  0.4701552987098694
train gradient:  0.1484296208287877
iteration : 3341
train acc:  0.7265625
train loss:  0.5089774131774902
train gradient:  0.1456404442682835
iteration : 3342
train acc:  0.7578125
train loss:  0.49897488951683044
train gradient:  0.14655748952042835
iteration : 3343
train acc:  0.7265625
train loss:  0.5277891159057617
train gradient:  0.17364557078748127
iteration : 3344
train acc:  0.6484375
train loss:  0.6098233461380005
train gradient:  0.2099498638524568
iteration : 3345
train acc:  0.703125
train loss:  0.5379825830459595
train gradient:  0.18253171941970608
iteration : 3346
train acc:  0.7265625
train loss:  0.5043109655380249
train gradient:  0.16258858562290923
iteration : 3347
train acc:  0.75
train loss:  0.4734245240688324
train gradient:  0.1210756640571939
iteration : 3348
train acc:  0.6953125
train loss:  0.5123998522758484
train gradient:  0.1543559499639176
iteration : 3349
train acc:  0.703125
train loss:  0.5286684036254883
train gradient:  0.14811096417919561
iteration : 3350
train acc:  0.703125
train loss:  0.5852211117744446
train gradient:  0.23385682603450647
iteration : 3351
train acc:  0.75
train loss:  0.5301601886749268
train gradient:  0.14531744287206666
iteration : 3352
train acc:  0.734375
train loss:  0.5423437356948853
train gradient:  0.13982943476272602
iteration : 3353
train acc:  0.6640625
train loss:  0.5951354503631592
train gradient:  0.17310506097092443
iteration : 3354
train acc:  0.703125
train loss:  0.573312520980835
train gradient:  0.14788948623840628
iteration : 3355
train acc:  0.7421875
train loss:  0.5647600889205933
train gradient:  0.15528204970291432
iteration : 3356
train acc:  0.7578125
train loss:  0.49349525570869446
train gradient:  0.12278393107981697
iteration : 3357
train acc:  0.6796875
train loss:  0.5481687188148499
train gradient:  0.17020024058903419
iteration : 3358
train acc:  0.7421875
train loss:  0.5028232336044312
train gradient:  0.12644304239000037
iteration : 3359
train acc:  0.734375
train loss:  0.5424466133117676
train gradient:  0.1958529202188906
iteration : 3360
train acc:  0.71875
train loss:  0.4940806031227112
train gradient:  0.1213433918992377
iteration : 3361
train acc:  0.671875
train loss:  0.5766862034797668
train gradient:  0.16343082565649264
iteration : 3362
train acc:  0.703125
train loss:  0.5587785243988037
train gradient:  0.1572264140954529
iteration : 3363
train acc:  0.6640625
train loss:  0.5666563510894775
train gradient:  0.15238678661567118
iteration : 3364
train acc:  0.6640625
train loss:  0.6391621232032776
train gradient:  0.18946589873402603
iteration : 3365
train acc:  0.671875
train loss:  0.5294532775878906
train gradient:  0.18427264194326348
iteration : 3366
train acc:  0.6484375
train loss:  0.6104774475097656
train gradient:  0.182140246441118
iteration : 3367
train acc:  0.65625
train loss:  0.5361663699150085
train gradient:  0.14075010492408152
iteration : 3368
train acc:  0.859375
train loss:  0.4196321666240692
train gradient:  0.09643854018205124
iteration : 3369
train acc:  0.796875
train loss:  0.4782717823982239
train gradient:  0.12122335597184697
iteration : 3370
train acc:  0.7734375
train loss:  0.4782424569129944
train gradient:  0.1219052223219147
iteration : 3371
train acc:  0.7578125
train loss:  0.4947850704193115
train gradient:  0.155240308149649
iteration : 3372
train acc:  0.6875
train loss:  0.5361219048500061
train gradient:  0.12160896109017331
iteration : 3373
train acc:  0.6953125
train loss:  0.5737318992614746
train gradient:  0.15731372749892736
iteration : 3374
train acc:  0.75
train loss:  0.5013600587844849
train gradient:  0.13330977655785367
iteration : 3375
train acc:  0.6796875
train loss:  0.5542066097259521
train gradient:  0.18252472211487566
iteration : 3376
train acc:  0.7109375
train loss:  0.5044036507606506
train gradient:  0.1613341959203587
iteration : 3377
train acc:  0.7109375
train loss:  0.5122177600860596
train gradient:  0.18116057335855673
iteration : 3378
train acc:  0.671875
train loss:  0.5447660088539124
train gradient:  0.16748158639595628
iteration : 3379
train acc:  0.703125
train loss:  0.5674324035644531
train gradient:  0.2253662448507146
iteration : 3380
train acc:  0.7421875
train loss:  0.4764564633369446
train gradient:  0.1635646990989161
iteration : 3381
train acc:  0.71875
train loss:  0.5263620615005493
train gradient:  0.15037588730592194
iteration : 3382
train acc:  0.7421875
train loss:  0.44702696800231934
train gradient:  0.13589960339465523
iteration : 3383
train acc:  0.765625
train loss:  0.483259379863739
train gradient:  0.1315264138986041
iteration : 3384
train acc:  0.703125
train loss:  0.5412843227386475
train gradient:  0.1373321351249198
iteration : 3385
train acc:  0.7109375
train loss:  0.5347400307655334
train gradient:  0.15742365485160986
iteration : 3386
train acc:  0.71875
train loss:  0.5382157564163208
train gradient:  0.13259065069341736
iteration : 3387
train acc:  0.609375
train loss:  0.638959527015686
train gradient:  0.24576391854474755
iteration : 3388
train acc:  0.703125
train loss:  0.5466108918190002
train gradient:  0.13790699006869683
iteration : 3389
train acc:  0.65625
train loss:  0.5739297866821289
train gradient:  0.13657646541066354
iteration : 3390
train acc:  0.6953125
train loss:  0.5290255546569824
train gradient:  0.15467746377414823
iteration : 3391
train acc:  0.734375
train loss:  0.5429118275642395
train gradient:  0.15204920768369323
iteration : 3392
train acc:  0.703125
train loss:  0.5202754139900208
train gradient:  0.1368840056658437
iteration : 3393
train acc:  0.7109375
train loss:  0.48347076773643494
train gradient:  0.13143791156971263
iteration : 3394
train acc:  0.703125
train loss:  0.5188117027282715
train gradient:  0.11292015929901154
iteration : 3395
train acc:  0.6640625
train loss:  0.5679945349693298
train gradient:  0.1534705925244016
iteration : 3396
train acc:  0.7265625
train loss:  0.51813805103302
train gradient:  0.15590790029484775
iteration : 3397
train acc:  0.734375
train loss:  0.5521859526634216
train gradient:  0.2096031321208155
iteration : 3398
train acc:  0.71875
train loss:  0.5168433785438538
train gradient:  0.16135946292376005
iteration : 3399
train acc:  0.7109375
train loss:  0.5229285955429077
train gradient:  0.17136591848565613
iteration : 3400
train acc:  0.71875
train loss:  0.5255163908004761
train gradient:  0.1539506779064656
iteration : 3401
train acc:  0.75
train loss:  0.47662150859832764
train gradient:  0.11580421518877695
iteration : 3402
train acc:  0.7890625
train loss:  0.48524439334869385
train gradient:  0.23945957995749856
iteration : 3403
train acc:  0.7265625
train loss:  0.5193766951560974
train gradient:  0.1317348288441803
iteration : 3404
train acc:  0.6953125
train loss:  0.5224623680114746
train gradient:  0.1379332097822946
iteration : 3405
train acc:  0.734375
train loss:  0.5343030691146851
train gradient:  0.155808411521294
iteration : 3406
train acc:  0.7265625
train loss:  0.49408143758773804
train gradient:  0.15645305078314548
iteration : 3407
train acc:  0.78125
train loss:  0.49556589126586914
train gradient:  0.10699530187303498
iteration : 3408
train acc:  0.6953125
train loss:  0.5194336175918579
train gradient:  0.16583575548133866
iteration : 3409
train acc:  0.6953125
train loss:  0.5687010288238525
train gradient:  0.19542395138199886
iteration : 3410
train acc:  0.7578125
train loss:  0.4677157402038574
train gradient:  0.14018590380608592
iteration : 3411
train acc:  0.65625
train loss:  0.5842208862304688
train gradient:  0.24773154696769284
iteration : 3412
train acc:  0.671875
train loss:  0.5659370422363281
train gradient:  0.21257953896978155
iteration : 3413
train acc:  0.7578125
train loss:  0.5054162740707397
train gradient:  0.1294611548291728
iteration : 3414
train acc:  0.671875
train loss:  0.5697771310806274
train gradient:  0.21892027778972917
iteration : 3415
train acc:  0.65625
train loss:  0.5677964687347412
train gradient:  0.13977673936207385
iteration : 3416
train acc:  0.734375
train loss:  0.501850962638855
train gradient:  0.15484155163770535
iteration : 3417
train acc:  0.7421875
train loss:  0.527687668800354
train gradient:  0.15173361421942483
iteration : 3418
train acc:  0.765625
train loss:  0.48681050539016724
train gradient:  0.12524460213733607
iteration : 3419
train acc:  0.703125
train loss:  0.49913662672042847
train gradient:  0.13660803410566846
iteration : 3420
train acc:  0.71875
train loss:  0.519722580909729
train gradient:  0.15178092586797742
iteration : 3421
train acc:  0.7421875
train loss:  0.5343396663665771
train gradient:  0.1516302027351331
iteration : 3422
train acc:  0.71875
train loss:  0.5578303337097168
train gradient:  0.21582880547000688
iteration : 3423
train acc:  0.6875
train loss:  0.5693461894989014
train gradient:  0.16575239359504818
iteration : 3424
train acc:  0.71875
train loss:  0.5138982534408569
train gradient:  0.15981348818395053
iteration : 3425
train acc:  0.765625
train loss:  0.4807380735874176
train gradient:  0.1390335751901789
iteration : 3426
train acc:  0.734375
train loss:  0.495717853307724
train gradient:  0.12303921084551032
iteration : 3427
train acc:  0.71875
train loss:  0.5108497738838196
train gradient:  0.2122131953684256
iteration : 3428
train acc:  0.65625
train loss:  0.5786404609680176
train gradient:  0.21303114549280933
iteration : 3429
train acc:  0.6875
train loss:  0.599602460861206
train gradient:  0.20843508208079498
iteration : 3430
train acc:  0.75
train loss:  0.4884883165359497
train gradient:  0.14565268377766333
iteration : 3431
train acc:  0.6953125
train loss:  0.5477612018585205
train gradient:  0.16387667208994178
iteration : 3432
train acc:  0.703125
train loss:  0.5787820816040039
train gradient:  0.15773080778180293
iteration : 3433
train acc:  0.6484375
train loss:  0.5759440660476685
train gradient:  0.18714923107199938
iteration : 3434
train acc:  0.65625
train loss:  0.579442024230957
train gradient:  0.14658586814822763
iteration : 3435
train acc:  0.734375
train loss:  0.5561856031417847
train gradient:  0.16650474136621146
iteration : 3436
train acc:  0.6953125
train loss:  0.5284677147865295
train gradient:  0.14754090167791453
iteration : 3437
train acc:  0.6640625
train loss:  0.5930173397064209
train gradient:  0.2600126726168268
iteration : 3438
train acc:  0.7734375
train loss:  0.47161561250686646
train gradient:  0.1451327085776442
iteration : 3439
train acc:  0.625
train loss:  0.5878288149833679
train gradient:  0.3198415199376042
iteration : 3440
train acc:  0.7109375
train loss:  0.5696005821228027
train gradient:  0.16992349612536228
iteration : 3441
train acc:  0.75
train loss:  0.5027033090591431
train gradient:  0.14229317657031493
iteration : 3442
train acc:  0.75
train loss:  0.4880477786064148
train gradient:  0.12118335427130678
iteration : 3443
train acc:  0.78125
train loss:  0.4978000521659851
train gradient:  0.15517475892151855
iteration : 3444
train acc:  0.6875
train loss:  0.49532878398895264
train gradient:  0.11838368238793513
iteration : 3445
train acc:  0.6875
train loss:  0.5647867321968079
train gradient:  0.16478015600551077
iteration : 3446
train acc:  0.734375
train loss:  0.4885385036468506
train gradient:  0.1904536800005903
iteration : 3447
train acc:  0.703125
train loss:  0.5363302230834961
train gradient:  0.16119317397741728
iteration : 3448
train acc:  0.7109375
train loss:  0.5248417854309082
train gradient:  0.17547927886636044
iteration : 3449
train acc:  0.7421875
train loss:  0.5300816893577576
train gradient:  0.1689207531284037
iteration : 3450
train acc:  0.71875
train loss:  0.49997758865356445
train gradient:  0.17580649409229915
iteration : 3451
train acc:  0.71875
train loss:  0.5736732482910156
train gradient:  0.2108325909404099
iteration : 3452
train acc:  0.6796875
train loss:  0.5541699528694153
train gradient:  0.1708729071559254
iteration : 3453
train acc:  0.7734375
train loss:  0.5137084722518921
train gradient:  0.12144507836636372
iteration : 3454
train acc:  0.765625
train loss:  0.5033052563667297
train gradient:  0.14349684347976926
iteration : 3455
train acc:  0.6328125
train loss:  0.6078863143920898
train gradient:  0.2534305694146217
iteration : 3456
train acc:  0.765625
train loss:  0.4939870238304138
train gradient:  0.12145377628741623
iteration : 3457
train acc:  0.703125
train loss:  0.5250204801559448
train gradient:  0.13443120900903283
iteration : 3458
train acc:  0.765625
train loss:  0.487482488155365
train gradient:  0.13224509343434218
iteration : 3459
train acc:  0.71875
train loss:  0.5520809888839722
train gradient:  0.17961668747337184
iteration : 3460
train acc:  0.78125
train loss:  0.4970190227031708
train gradient:  0.15319233784576314
iteration : 3461
train acc:  0.71875
train loss:  0.5246803760528564
train gradient:  0.15183688262968661
iteration : 3462
train acc:  0.7109375
train loss:  0.5387887954711914
train gradient:  0.1798182223110633
iteration : 3463
train acc:  0.7890625
train loss:  0.49364691972732544
train gradient:  0.16391204404571388
iteration : 3464
train acc:  0.7265625
train loss:  0.5520427227020264
train gradient:  0.15653668062774495
iteration : 3465
train acc:  0.7109375
train loss:  0.5228699445724487
train gradient:  0.1560637817812754
iteration : 3466
train acc:  0.7578125
train loss:  0.4618288278579712
train gradient:  0.11639544260475788
iteration : 3467
train acc:  0.7890625
train loss:  0.4696222245693207
train gradient:  0.16601206550808584
iteration : 3468
train acc:  0.6953125
train loss:  0.564668595790863
train gradient:  0.20315430951226185
iteration : 3469
train acc:  0.7578125
train loss:  0.4820542633533478
train gradient:  0.11001747723002873
iteration : 3470
train acc:  0.7265625
train loss:  0.5216752290725708
train gradient:  0.13743668832043265
iteration : 3471
train acc:  0.6640625
train loss:  0.5649642944335938
train gradient:  0.17042542125400062
iteration : 3472
train acc:  0.75
train loss:  0.4927317798137665
train gradient:  0.14151826117501345
iteration : 3473
train acc:  0.7109375
train loss:  0.518718957901001
train gradient:  0.16627798343808758
iteration : 3474
train acc:  0.71875
train loss:  0.540807843208313
train gradient:  0.15503041121428476
iteration : 3475
train acc:  0.6953125
train loss:  0.5355219841003418
train gradient:  0.13244141358731676
iteration : 3476
train acc:  0.6015625
train loss:  0.6604334115982056
train gradient:  0.27642252234145387
iteration : 3477
train acc:  0.703125
train loss:  0.5693018436431885
train gradient:  0.17373809506968163
iteration : 3478
train acc:  0.7421875
train loss:  0.49177008867263794
train gradient:  0.12379185362332275
iteration : 3479
train acc:  0.71875
train loss:  0.5544869303703308
train gradient:  0.15703258304924708
iteration : 3480
train acc:  0.7265625
train loss:  0.5569883584976196
train gradient:  0.21010212119145233
iteration : 3481
train acc:  0.703125
train loss:  0.5403575301170349
train gradient:  0.15129679358720843
iteration : 3482
train acc:  0.765625
train loss:  0.4418656527996063
train gradient:  0.11843691799606729
iteration : 3483
train acc:  0.71875
train loss:  0.542714536190033
train gradient:  0.16751567519098048
iteration : 3484
train acc:  0.7265625
train loss:  0.5688313245773315
train gradient:  0.208487425744666
iteration : 3485
train acc:  0.7578125
train loss:  0.491606205701828
train gradient:  0.12453231722264073
iteration : 3486
train acc:  0.7265625
train loss:  0.47869354486465454
train gradient:  0.16824303568472482
iteration : 3487
train acc:  0.703125
train loss:  0.6157721281051636
train gradient:  0.2629942150065783
iteration : 3488
train acc:  0.8125
train loss:  0.4410220980644226
train gradient:  0.11722152418894902
iteration : 3489
train acc:  0.6328125
train loss:  0.6104604005813599
train gradient:  0.17349633562557115
iteration : 3490
train acc:  0.6953125
train loss:  0.542221188545227
train gradient:  0.1579602193950117
iteration : 3491
train acc:  0.703125
train loss:  0.536170482635498
train gradient:  0.16917489887004872
iteration : 3492
train acc:  0.7109375
train loss:  0.5214499235153198
train gradient:  0.16700774990551315
iteration : 3493
train acc:  0.75
train loss:  0.5312795042991638
train gradient:  0.12580622453687043
iteration : 3494
train acc:  0.7734375
train loss:  0.5143958926200867
train gradient:  0.18622547273665974
iteration : 3495
train acc:  0.75
train loss:  0.47348710894584656
train gradient:  0.25623298027619434
iteration : 3496
train acc:  0.703125
train loss:  0.5170367956161499
train gradient:  0.13215438238481386
iteration : 3497
train acc:  0.71875
train loss:  0.48194408416748047
train gradient:  0.131332553315483
iteration : 3498
train acc:  0.65625
train loss:  0.5459426045417786
train gradient:  0.19992733456950548
iteration : 3499
train acc:  0.6875
train loss:  0.6003331542015076
train gradient:  0.18151147920392915
iteration : 3500
train acc:  0.7109375
train loss:  0.5841939449310303
train gradient:  0.21327699894721147
iteration : 3501
train acc:  0.7734375
train loss:  0.4833263158798218
train gradient:  0.1254829759828014
iteration : 3502
train acc:  0.78125
train loss:  0.4528506100177765
train gradient:  0.1318120176245111
iteration : 3503
train acc:  0.71875
train loss:  0.5225054025650024
train gradient:  0.18263890608049232
iteration : 3504
train acc:  0.7109375
train loss:  0.5609435439109802
train gradient:  0.1732808829490327
iteration : 3505
train acc:  0.75
train loss:  0.5386034846305847
train gradient:  0.2383177006087246
iteration : 3506
train acc:  0.734375
train loss:  0.5194595456123352
train gradient:  0.15482840606919918
iteration : 3507
train acc:  0.6796875
train loss:  0.6257853507995605
train gradient:  0.21914936470293975
iteration : 3508
train acc:  0.765625
train loss:  0.47096580266952515
train gradient:  0.14134308886396335
iteration : 3509
train acc:  0.6875
train loss:  0.5759930610656738
train gradient:  0.21159644415895704
iteration : 3510
train acc:  0.734375
train loss:  0.5390706062316895
train gradient:  0.1745293405084603
iteration : 3511
train acc:  0.71875
train loss:  0.526687502861023
train gradient:  0.22508507447599996
iteration : 3512
train acc:  0.734375
train loss:  0.5630882382392883
train gradient:  0.17762643912131038
iteration : 3513
train acc:  0.703125
train loss:  0.5149444937705994
train gradient:  0.15583220355837424
iteration : 3514
train acc:  0.765625
train loss:  0.5192654728889465
train gradient:  0.15435281398716505
iteration : 3515
train acc:  0.75
train loss:  0.5165920257568359
train gradient:  0.15608633109041214
iteration : 3516
train acc:  0.7578125
train loss:  0.4820193350315094
train gradient:  0.1442267639954139
iteration : 3517
train acc:  0.6875
train loss:  0.5555278062820435
train gradient:  0.1672840209267051
iteration : 3518
train acc:  0.703125
train loss:  0.565252423286438
train gradient:  0.1597602493897412
iteration : 3519
train acc:  0.765625
train loss:  0.47078949213027954
train gradient:  0.13706520562122076
iteration : 3520
train acc:  0.71875
train loss:  0.5192124843597412
train gradient:  0.14352995867717996
iteration : 3521
train acc:  0.78125
train loss:  0.4866742491722107
train gradient:  0.12967985155739528
iteration : 3522
train acc:  0.6953125
train loss:  0.5464504957199097
train gradient:  0.1727730797621723
iteration : 3523
train acc:  0.625
train loss:  0.5947349071502686
train gradient:  0.23715421273810872
iteration : 3524
train acc:  0.7421875
train loss:  0.5360540151596069
train gradient:  0.1844206637889817
iteration : 3525
train acc:  0.6953125
train loss:  0.5530138611793518
train gradient:  0.1817234080105398
iteration : 3526
train acc:  0.734375
train loss:  0.49074527621269226
train gradient:  0.11546557864294604
iteration : 3527
train acc:  0.7265625
train loss:  0.5363874435424805
train gradient:  0.1802344485000888
iteration : 3528
train acc:  0.6796875
train loss:  0.6113954782485962
train gradient:  0.2398598559087295
iteration : 3529
train acc:  0.71875
train loss:  0.5314039587974548
train gradient:  0.17186066097150154
iteration : 3530
train acc:  0.75
train loss:  0.5060453414916992
train gradient:  0.1519920154231752
iteration : 3531
train acc:  0.71875
train loss:  0.5516033172607422
train gradient:  0.16613953550603006
iteration : 3532
train acc:  0.7109375
train loss:  0.5536901354789734
train gradient:  0.23943614421599185
iteration : 3533
train acc:  0.71875
train loss:  0.5282625555992126
train gradient:  0.1781034623070035
iteration : 3534
train acc:  0.7890625
train loss:  0.47262299060821533
train gradient:  0.1791791991859084
iteration : 3535
train acc:  0.7109375
train loss:  0.5418809652328491
train gradient:  0.24956721545765542
iteration : 3536
train acc:  0.6875
train loss:  0.5381780862808228
train gradient:  0.1632564296767819
iteration : 3537
train acc:  0.6640625
train loss:  0.5672318935394287
train gradient:  0.21995268179143806
iteration : 3538
train acc:  0.7421875
train loss:  0.522925853729248
train gradient:  0.191699810105734
iteration : 3539
train acc:  0.734375
train loss:  0.4826313853263855
train gradient:  0.14703243427849866
iteration : 3540
train acc:  0.6953125
train loss:  0.5567139983177185
train gradient:  0.3166410722189356
iteration : 3541
train acc:  0.734375
train loss:  0.5124048590660095
train gradient:  0.1494891659756616
iteration : 3542
train acc:  0.734375
train loss:  0.5435597896575928
train gradient:  0.14526248636497735
iteration : 3543
train acc:  0.71875
train loss:  0.557763934135437
train gradient:  0.17423328170384095
iteration : 3544
train acc:  0.7265625
train loss:  0.5366016626358032
train gradient:  0.13562389278781495
iteration : 3545
train acc:  0.7421875
train loss:  0.48120632767677307
train gradient:  0.12373110439398217
iteration : 3546
train acc:  0.7578125
train loss:  0.48490479588508606
train gradient:  0.14384733019099077
iteration : 3547
train acc:  0.7265625
train loss:  0.5377129316329956
train gradient:  0.16093907594850315
iteration : 3548
train acc:  0.7734375
train loss:  0.5306318402290344
train gradient:  0.16725318739369005
iteration : 3549
train acc:  0.6796875
train loss:  0.5367352962493896
train gradient:  0.1976318428676912
iteration : 3550
train acc:  0.7578125
train loss:  0.4709022045135498
train gradient:  0.1259757965626953
iteration : 3551
train acc:  0.7578125
train loss:  0.47328463196754456
train gradient:  0.1472311035198105
iteration : 3552
train acc:  0.734375
train loss:  0.5350916385650635
train gradient:  0.18284359273462247
iteration : 3553
train acc:  0.75
train loss:  0.5080389976501465
train gradient:  0.13521908448164632
iteration : 3554
train acc:  0.6875
train loss:  0.552640974521637
train gradient:  0.17484962835200957
iteration : 3555
train acc:  0.78125
train loss:  0.47754931449890137
train gradient:  0.16273505965847945
iteration : 3556
train acc:  0.6484375
train loss:  0.5635900497436523
train gradient:  0.17039135061106125
iteration : 3557
train acc:  0.71875
train loss:  0.5414754748344421
train gradient:  0.14171297876400352
iteration : 3558
train acc:  0.7421875
train loss:  0.5335783958435059
train gradient:  0.16135366418551947
iteration : 3559
train acc:  0.7265625
train loss:  0.544836163520813
train gradient:  0.16891217012360044
iteration : 3560
train acc:  0.75
train loss:  0.4980596899986267
train gradient:  0.133729356211025
iteration : 3561
train acc:  0.7109375
train loss:  0.5280574560165405
train gradient:  0.19705027574935546
iteration : 3562
train acc:  0.75
train loss:  0.5330781936645508
train gradient:  0.14351310162460007
iteration : 3563
train acc:  0.7265625
train loss:  0.5604155659675598
train gradient:  0.16993998962197115
iteration : 3564
train acc:  0.71875
train loss:  0.49380427598953247
train gradient:  0.12433747042810527
iteration : 3565
train acc:  0.734375
train loss:  0.5722689628601074
train gradient:  0.1574426412187499
iteration : 3566
train acc:  0.71875
train loss:  0.512313723564148
train gradient:  0.14166407093251993
iteration : 3567
train acc:  0.7109375
train loss:  0.5225538611412048
train gradient:  0.13730657195714224
iteration : 3568
train acc:  0.703125
train loss:  0.5013583302497864
train gradient:  0.15024242011371886
iteration : 3569
train acc:  0.7421875
train loss:  0.49713724851608276
train gradient:  0.13665229774157792
iteration : 3570
train acc:  0.6953125
train loss:  0.587031900882721
train gradient:  0.20361628487800534
iteration : 3571
train acc:  0.734375
train loss:  0.5266290307044983
train gradient:  0.13217439478617882
iteration : 3572
train acc:  0.734375
train loss:  0.5103102922439575
train gradient:  0.153898463758882
iteration : 3573
train acc:  0.71875
train loss:  0.5261954665184021
train gradient:  0.12008003013462042
iteration : 3574
train acc:  0.7421875
train loss:  0.5123766660690308
train gradient:  0.14299573093427448
iteration : 3575
train acc:  0.71875
train loss:  0.5205956697463989
train gradient:  0.15533062706962453
iteration : 3576
train acc:  0.7578125
train loss:  0.5067914128303528
train gradient:  0.16323036484076558
iteration : 3577
train acc:  0.7578125
train loss:  0.48744386434555054
train gradient:  0.13031924599419714
iteration : 3578
train acc:  0.703125
train loss:  0.5020841360092163
train gradient:  0.12052266116525925
iteration : 3579
train acc:  0.7734375
train loss:  0.4785931706428528
train gradient:  0.13662604853974972
iteration : 3580
train acc:  0.6796875
train loss:  0.509965717792511
train gradient:  0.1785523139622615
iteration : 3581
train acc:  0.7265625
train loss:  0.5499603152275085
train gradient:  0.18410605754103454
iteration : 3582
train acc:  0.703125
train loss:  0.5026232600212097
train gradient:  0.1366955433133425
iteration : 3583
train acc:  0.703125
train loss:  0.563046932220459
train gradient:  0.17961879141746312
iteration : 3584
train acc:  0.7109375
train loss:  0.5144681334495544
train gradient:  0.15972648729457486
iteration : 3585
train acc:  0.734375
train loss:  0.49294954538345337
train gradient:  0.1501485704970269
iteration : 3586
train acc:  0.71875
train loss:  0.51601243019104
train gradient:  0.16461717271301873
iteration : 3587
train acc:  0.7109375
train loss:  0.5122821927070618
train gradient:  0.14448843848836868
iteration : 3588
train acc:  0.7265625
train loss:  0.5202863812446594
train gradient:  0.16053665685384766
iteration : 3589
train acc:  0.703125
train loss:  0.5492298603057861
train gradient:  0.17736289051949677
iteration : 3590
train acc:  0.6953125
train loss:  0.5462266206741333
train gradient:  0.1864810249479006
iteration : 3591
train acc:  0.796875
train loss:  0.4659762382507324
train gradient:  0.18451623021961783
iteration : 3592
train acc:  0.796875
train loss:  0.44623327255249023
train gradient:  0.11198721575942035
iteration : 3593
train acc:  0.671875
train loss:  0.5303935408592224
train gradient:  0.17981774242345028
iteration : 3594
train acc:  0.6015625
train loss:  0.6551684737205505
train gradient:  0.28807885943193706
iteration : 3595
train acc:  0.640625
train loss:  0.6277061700820923
train gradient:  0.31196796216950695
iteration : 3596
train acc:  0.734375
train loss:  0.5453790426254272
train gradient:  0.13799743326622788
iteration : 3597
train acc:  0.6484375
train loss:  0.6387036442756653
train gradient:  0.2666989137506072
iteration : 3598
train acc:  0.7109375
train loss:  0.5729626417160034
train gradient:  0.23021694904878842
iteration : 3599
train acc:  0.7265625
train loss:  0.5205355286598206
train gradient:  0.19058624461991075
iteration : 3600
train acc:  0.6875
train loss:  0.5484680533409119
train gradient:  0.203212761485829
iteration : 3601
train acc:  0.6640625
train loss:  0.5545651912689209
train gradient:  0.2032789685175696
iteration : 3602
train acc:  0.7109375
train loss:  0.5427231192588806
train gradient:  0.16692755437755818
iteration : 3603
train acc:  0.7890625
train loss:  0.44383159279823303
train gradient:  0.20255212731341832
iteration : 3604
train acc:  0.734375
train loss:  0.5591606497764587
train gradient:  0.17004177861564795
iteration : 3605
train acc:  0.765625
train loss:  0.5209084749221802
train gradient:  0.16277179987556445
iteration : 3606
train acc:  0.71875
train loss:  0.5350332260131836
train gradient:  0.15671414703465403
iteration : 3607
train acc:  0.6796875
train loss:  0.611359715461731
train gradient:  0.20486973638720346
iteration : 3608
train acc:  0.6875
train loss:  0.5140149593353271
train gradient:  0.14051681328039023
iteration : 3609
train acc:  0.7265625
train loss:  0.5055946707725525
train gradient:  0.1825396421316579
iteration : 3610
train acc:  0.7265625
train loss:  0.5356081128120422
train gradient:  0.23462521800766545
iteration : 3611
train acc:  0.6796875
train loss:  0.5661798715591431
train gradient:  0.20205901193170517
iteration : 3612
train acc:  0.703125
train loss:  0.516776442527771
train gradient:  0.1658072818604368
iteration : 3613
train acc:  0.796875
train loss:  0.5129812955856323
train gradient:  0.14068371959767162
iteration : 3614
train acc:  0.703125
train loss:  0.4917822480201721
train gradient:  0.15777510364464334
iteration : 3615
train acc:  0.6640625
train loss:  0.6044303774833679
train gradient:  0.22212822624682177
iteration : 3616
train acc:  0.6953125
train loss:  0.5740268230438232
train gradient:  0.16505776652002846
iteration : 3617
train acc:  0.7265625
train loss:  0.5260797739028931
train gradient:  0.15700024960924114
iteration : 3618
train acc:  0.671875
train loss:  0.5709986686706543
train gradient:  0.18270351490378
iteration : 3619
train acc:  0.7265625
train loss:  0.46304574608802795
train gradient:  0.13568059599137158
iteration : 3620
train acc:  0.75
train loss:  0.5120038986206055
train gradient:  0.13330368634027265
iteration : 3621
train acc:  0.6875
train loss:  0.5498281717300415
train gradient:  0.17578685858049567
iteration : 3622
train acc:  0.75
train loss:  0.5212470293045044
train gradient:  0.171287571633528
iteration : 3623
train acc:  0.7109375
train loss:  0.5177440643310547
train gradient:  0.209018920041958
iteration : 3624
train acc:  0.7109375
train loss:  0.4939367473125458
train gradient:  0.1327903313879003
iteration : 3625
train acc:  0.7265625
train loss:  0.49495190382003784
train gradient:  0.13794950225751668
iteration : 3626
train acc:  0.7109375
train loss:  0.5141438245773315
train gradient:  0.16417278632264176
iteration : 3627
train acc:  0.7890625
train loss:  0.4924095869064331
train gradient:  0.14490895593398895
iteration : 3628
train acc:  0.7265625
train loss:  0.5235376954078674
train gradient:  0.14800380569700938
iteration : 3629
train acc:  0.78125
train loss:  0.47822362184524536
train gradient:  0.17004150230146858
iteration : 3630
train acc:  0.8046875
train loss:  0.4813246726989746
train gradient:  0.19787106902164503
iteration : 3631
train acc:  0.734375
train loss:  0.5131534337997437
train gradient:  0.1482273074447392
iteration : 3632
train acc:  0.7109375
train loss:  0.559313952922821
train gradient:  0.2444839647818423
iteration : 3633
train acc:  0.7265625
train loss:  0.5380822420120239
train gradient:  0.19045089682933264
iteration : 3634
train acc:  0.84375
train loss:  0.4043349027633667
train gradient:  0.12608980832501532
iteration : 3635
train acc:  0.78125
train loss:  0.47829821705818176
train gradient:  0.15848516370095125
iteration : 3636
train acc:  0.7265625
train loss:  0.5500247478485107
train gradient:  0.1645808301396842
iteration : 3637
train acc:  0.640625
train loss:  0.5553873777389526
train gradient:  0.1765786220027726
iteration : 3638
train acc:  0.71875
train loss:  0.554802417755127
train gradient:  0.1763329840808769
iteration : 3639
train acc:  0.7265625
train loss:  0.5525691509246826
train gradient:  0.13631391856474623
iteration : 3640
train acc:  0.6640625
train loss:  0.5794157981872559
train gradient:  0.2059112335097214
iteration : 3641
train acc:  0.765625
train loss:  0.45342081785202026
train gradient:  0.10913995836408191
iteration : 3642
train acc:  0.7265625
train loss:  0.4816816747188568
train gradient:  0.13976161169235746
iteration : 3643
train acc:  0.7421875
train loss:  0.511996865272522
train gradient:  0.1460301601849328
iteration : 3644
train acc:  0.71875
train loss:  0.5417900681495667
train gradient:  0.15289865187710044
iteration : 3645
train acc:  0.7578125
train loss:  0.47154855728149414
train gradient:  0.1332850752570488
iteration : 3646
train acc:  0.7265625
train loss:  0.5076407790184021
train gradient:  0.1481188951603428
iteration : 3647
train acc:  0.7265625
train loss:  0.5096440315246582
train gradient:  0.15583176992905495
iteration : 3648
train acc:  0.7890625
train loss:  0.4551960527896881
train gradient:  0.13627890846881519
iteration : 3649
train acc:  0.7578125
train loss:  0.50699782371521
train gradient:  0.21374876743318566
iteration : 3650
train acc:  0.8046875
train loss:  0.47718462347984314
train gradient:  0.12078497240132878
iteration : 3651
train acc:  0.75
train loss:  0.48709794878959656
train gradient:  0.17904409237111862
iteration : 3652
train acc:  0.6875
train loss:  0.5829735994338989
train gradient:  0.21372108344678267
iteration : 3653
train acc:  0.75
train loss:  0.4824283719062805
train gradient:  0.1698140818575899
iteration : 3654
train acc:  0.7578125
train loss:  0.47189420461654663
train gradient:  0.14225323356261857
iteration : 3655
train acc:  0.6953125
train loss:  0.5384875535964966
train gradient:  0.1915073818836675
iteration : 3656
train acc:  0.71875
train loss:  0.5381644368171692
train gradient:  0.18999063894923768
iteration : 3657
train acc:  0.703125
train loss:  0.5423252582550049
train gradient:  0.2536709484191445
iteration : 3658
train acc:  0.703125
train loss:  0.5640791654586792
train gradient:  0.16483981547086424
iteration : 3659
train acc:  0.703125
train loss:  0.5282623767852783
train gradient:  0.16776229946759125
iteration : 3660
train acc:  0.6875
train loss:  0.5359445810317993
train gradient:  0.14595510565944236
iteration : 3661
train acc:  0.765625
train loss:  0.5089371204376221
train gradient:  0.20233129104476502
iteration : 3662
train acc:  0.7734375
train loss:  0.4732953608036041
train gradient:  0.12440724550023911
iteration : 3663
train acc:  0.7421875
train loss:  0.5239707231521606
train gradient:  0.168254698052838
iteration : 3664
train acc:  0.6875
train loss:  0.5498870611190796
train gradient:  0.1652074956286788
iteration : 3665
train acc:  0.75
train loss:  0.4904049336910248
train gradient:  0.14752709197217972
iteration : 3666
train acc:  0.703125
train loss:  0.5537139773368835
train gradient:  0.15609403412125838
iteration : 3667
train acc:  0.71875
train loss:  0.5438345074653625
train gradient:  0.16119799057140946
iteration : 3668
train acc:  0.6875
train loss:  0.5838581919670105
train gradient:  0.20911486651615652
iteration : 3669
train acc:  0.71875
train loss:  0.5127424597740173
train gradient:  0.1594648662627577
iteration : 3670
train acc:  0.75
train loss:  0.4914451241493225
train gradient:  0.14960373578422814
iteration : 3671
train acc:  0.671875
train loss:  0.5670347809791565
train gradient:  0.22014555965312688
iteration : 3672
train acc:  0.8125
train loss:  0.45040959119796753
train gradient:  0.11620693736072492
iteration : 3673
train acc:  0.7421875
train loss:  0.5532240867614746
train gradient:  0.19678878974734795
iteration : 3674
train acc:  0.71875
train loss:  0.509122908115387
train gradient:  0.1351621209404476
iteration : 3675
train acc:  0.7421875
train loss:  0.4948377013206482
train gradient:  0.13075141249455247
iteration : 3676
train acc:  0.765625
train loss:  0.4915611147880554
train gradient:  0.18769196664835164
iteration : 3677
train acc:  0.75
train loss:  0.5473501682281494
train gradient:  0.14704814406507527
iteration : 3678
train acc:  0.734375
train loss:  0.509915828704834
train gradient:  0.22536071319736672
iteration : 3679
train acc:  0.7421875
train loss:  0.5147542953491211
train gradient:  0.10239504154010107
iteration : 3680
train acc:  0.7265625
train loss:  0.5011323690414429
train gradient:  0.15961632809915255
iteration : 3681
train acc:  0.734375
train loss:  0.5652040243148804
train gradient:  0.2502374952851908
iteration : 3682
train acc:  0.6953125
train loss:  0.5450488328933716
train gradient:  0.20125501765336673
iteration : 3683
train acc:  0.734375
train loss:  0.5258774757385254
train gradient:  0.15222801421590973
iteration : 3684
train acc:  0.7890625
train loss:  0.5073760747909546
train gradient:  0.12679362346031034
iteration : 3685
train acc:  0.7578125
train loss:  0.5230046510696411
train gradient:  0.1594427491596842
iteration : 3686
train acc:  0.671875
train loss:  0.6324788331985474
train gradient:  0.25263606700754926
iteration : 3687
train acc:  0.6953125
train loss:  0.5246194005012512
train gradient:  0.15478332245058105
iteration : 3688
train acc:  0.75
train loss:  0.4909482002258301
train gradient:  0.14388392058513627
iteration : 3689
train acc:  0.71875
train loss:  0.5120621919631958
train gradient:  0.16207353592234613
iteration : 3690
train acc:  0.7109375
train loss:  0.504310667514801
train gradient:  0.13966297936790087
iteration : 3691
train acc:  0.734375
train loss:  0.49360769987106323
train gradient:  0.15809690231211554
iteration : 3692
train acc:  0.6875
train loss:  0.6094260811805725
train gradient:  0.2109237741658912
iteration : 3693
train acc:  0.8125
train loss:  0.4522794187068939
train gradient:  0.20032163373817272
iteration : 3694
train acc:  0.7109375
train loss:  0.5499320030212402
train gradient:  0.14110388523822198
iteration : 3695
train acc:  0.6328125
train loss:  0.6197295188903809
train gradient:  0.1969899216939089
iteration : 3696
train acc:  0.796875
train loss:  0.4559417963027954
train gradient:  0.11771121493506317
iteration : 3697
train acc:  0.7265625
train loss:  0.5615622997283936
train gradient:  0.15038768084909002
iteration : 3698
train acc:  0.7578125
train loss:  0.47204235196113586
train gradient:  0.1762099493540531
iteration : 3699
train acc:  0.7421875
train loss:  0.5269995927810669
train gradient:  0.16679776737994598
iteration : 3700
train acc:  0.6171875
train loss:  0.608962893486023
train gradient:  0.249031500881764
iteration : 3701
train acc:  0.8046875
train loss:  0.5194582939147949
train gradient:  0.1881381359197074
iteration : 3702
train acc:  0.6484375
train loss:  0.6307746171951294
train gradient:  0.23227453316386282
iteration : 3703
train acc:  0.671875
train loss:  0.5485520362854004
train gradient:  0.21554572566608704
iteration : 3704
train acc:  0.703125
train loss:  0.5807874202728271
train gradient:  0.18167469054784158
iteration : 3705
train acc:  0.6953125
train loss:  0.5270953178405762
train gradient:  0.15712704143082062
iteration : 3706
train acc:  0.7265625
train loss:  0.49309390783309937
train gradient:  0.12713326777823775
iteration : 3707
train acc:  0.640625
train loss:  0.5657365322113037
train gradient:  0.16464117258652683
iteration : 3708
train acc:  0.765625
train loss:  0.49726438522338867
train gradient:  0.1374136616595665
iteration : 3709
train acc:  0.796875
train loss:  0.510814905166626
train gradient:  0.2032580757743187
iteration : 3710
train acc:  0.6796875
train loss:  0.5532388687133789
train gradient:  0.2367460477111299
iteration : 3711
train acc:  0.734375
train loss:  0.49781346321105957
train gradient:  0.152862615586019
iteration : 3712
train acc:  0.671875
train loss:  0.547166109085083
train gradient:  0.2032494339494864
iteration : 3713
train acc:  0.71875
train loss:  0.4973343014717102
train gradient:  0.15064015372239936
iteration : 3714
train acc:  0.7265625
train loss:  0.49001455307006836
train gradient:  0.13767054372283716
iteration : 3715
train acc:  0.7421875
train loss:  0.4941335916519165
train gradient:  0.12684797847075852
iteration : 3716
train acc:  0.703125
train loss:  0.5418824553489685
train gradient:  0.19883200346726784
iteration : 3717
train acc:  0.7265625
train loss:  0.5414021015167236
train gradient:  0.15154309024425652
iteration : 3718
train acc:  0.7265625
train loss:  0.5694912075996399
train gradient:  0.16750988782911197
iteration : 3719
train acc:  0.6640625
train loss:  0.5661981105804443
train gradient:  0.18193220892254464
iteration : 3720
train acc:  0.71875
train loss:  0.5660048723220825
train gradient:  0.1802546038048901
iteration : 3721
train acc:  0.6015625
train loss:  0.6200878024101257
train gradient:  0.17318797817056303
iteration : 3722
train acc:  0.6484375
train loss:  0.6075013875961304
train gradient:  0.24819830775832602
iteration : 3723
train acc:  0.671875
train loss:  0.5755614042282104
train gradient:  0.19918310648486243
iteration : 3724
train acc:  0.671875
train loss:  0.6160327196121216
train gradient:  0.20371523498296215
iteration : 3725
train acc:  0.7421875
train loss:  0.5073026418685913
train gradient:  0.15737845811769247
iteration : 3726
train acc:  0.7734375
train loss:  0.48332512378692627
train gradient:  0.1428189307390088
iteration : 3727
train acc:  0.7265625
train loss:  0.5281832218170166
train gradient:  0.17348592162905924
iteration : 3728
train acc:  0.7890625
train loss:  0.49177098274230957
train gradient:  0.1302860161145152
iteration : 3729
train acc:  0.75
train loss:  0.5341815948486328
train gradient:  0.20761026136267824
iteration : 3730
train acc:  0.75
train loss:  0.49420928955078125
train gradient:  0.12052398701511402
iteration : 3731
train acc:  0.765625
train loss:  0.4814366102218628
train gradient:  0.14438098376696853
iteration : 3732
train acc:  0.6953125
train loss:  0.564624547958374
train gradient:  0.1861201013061798
iteration : 3733
train acc:  0.65625
train loss:  0.5907329320907593
train gradient:  0.18701720409666833
iteration : 3734
train acc:  0.71875
train loss:  0.4953802227973938
train gradient:  0.15542720904623505
iteration : 3735
train acc:  0.7265625
train loss:  0.50190269947052
train gradient:  0.1691546563104006
iteration : 3736
train acc:  0.71875
train loss:  0.5583494901657104
train gradient:  0.2135756509749881
iteration : 3737
train acc:  0.703125
train loss:  0.5829732418060303
train gradient:  0.18149526215192405
iteration : 3738
train acc:  0.703125
train loss:  0.5751200318336487
train gradient:  0.17099138907338415
iteration : 3739
train acc:  0.734375
train loss:  0.5295628309249878
train gradient:  0.16330195759165206
iteration : 3740
train acc:  0.7109375
train loss:  0.5526052713394165
train gradient:  0.16233420898810214
iteration : 3741
train acc:  0.8046875
train loss:  0.49264663457870483
train gradient:  0.17191307165538533
iteration : 3742
train acc:  0.703125
train loss:  0.5137657523155212
train gradient:  0.1780277832463891
iteration : 3743
train acc:  0.7265625
train loss:  0.5616377592086792
train gradient:  0.18527735885367036
iteration : 3744
train acc:  0.7578125
train loss:  0.46341025829315186
train gradient:  0.11606408692706727
iteration : 3745
train acc:  0.71875
train loss:  0.547560453414917
train gradient:  0.1736025360340432
iteration : 3746
train acc:  0.6796875
train loss:  0.5724269151687622
train gradient:  0.17535285687454444
iteration : 3747
train acc:  0.71875
train loss:  0.5257223844528198
train gradient:  0.18947545696076273
iteration : 3748
train acc:  0.6953125
train loss:  0.5291840434074402
train gradient:  0.1501554801059301
iteration : 3749
train acc:  0.703125
train loss:  0.5691483020782471
train gradient:  0.14986282087814706
iteration : 3750
train acc:  0.75
train loss:  0.5083415508270264
train gradient:  0.1381698132632418
iteration : 3751
train acc:  0.6796875
train loss:  0.5511289238929749
train gradient:  0.20995426290071678
iteration : 3752
train acc:  0.765625
train loss:  0.49755972623825073
train gradient:  0.1486373242903211
iteration : 3753
train acc:  0.7265625
train loss:  0.513024628162384
train gradient:  0.1300194735943761
iteration : 3754
train acc:  0.765625
train loss:  0.49001771211624146
train gradient:  0.10609911598855035
iteration : 3755
train acc:  0.7109375
train loss:  0.5641568899154663
train gradient:  0.14640448212796858
iteration : 3756
train acc:  0.6953125
train loss:  0.542731761932373
train gradient:  0.1375902130770425
iteration : 3757
train acc:  0.6953125
train loss:  0.5952059030532837
train gradient:  0.18431724922952997
iteration : 3758
train acc:  0.71875
train loss:  0.5883322358131409
train gradient:  0.13664037820589287
iteration : 3759
train acc:  0.734375
train loss:  0.4993358850479126
train gradient:  0.16125858228109574
iteration : 3760
train acc:  0.6953125
train loss:  0.6137843728065491
train gradient:  0.17014808189014574
iteration : 3761
train acc:  0.7578125
train loss:  0.4966753423213959
train gradient:  0.15096905084997714
iteration : 3762
train acc:  0.7265625
train loss:  0.5117403864860535
train gradient:  0.14703185829580656
iteration : 3763
train acc:  0.6875
train loss:  0.577581524848938
train gradient:  0.15965061533938613
iteration : 3764
train acc:  0.734375
train loss:  0.555385947227478
train gradient:  0.1881543602990807
iteration : 3765
train acc:  0.734375
train loss:  0.5375819206237793
train gradient:  0.1749441887836875
iteration : 3766
train acc:  0.7578125
train loss:  0.48730289936065674
train gradient:  0.18143292954056778
iteration : 3767
train acc:  0.7109375
train loss:  0.5473345518112183
train gradient:  0.2023814753683698
iteration : 3768
train acc:  0.7265625
train loss:  0.48697641491889954
train gradient:  0.12858945763901408
iteration : 3769
train acc:  0.78125
train loss:  0.5077652931213379
train gradient:  0.12187477837609241
iteration : 3770
train acc:  0.6640625
train loss:  0.6031319499015808
train gradient:  0.17036203632540356
iteration : 3771
train acc:  0.765625
train loss:  0.4860851466655731
train gradient:  0.12856897331945183
iteration : 3772
train acc:  0.7421875
train loss:  0.5170586705207825
train gradient:  0.15674994898824235
iteration : 3773
train acc:  0.703125
train loss:  0.4875507354736328
train gradient:  0.12508937789401914
iteration : 3774
train acc:  0.7578125
train loss:  0.48537230491638184
train gradient:  0.1739882227289402
iteration : 3775
train acc:  0.71875
train loss:  0.5696632266044617
train gradient:  0.16619509271566843
iteration : 3776
train acc:  0.7421875
train loss:  0.5698034763336182
train gradient:  0.22538189677166925
iteration : 3777
train acc:  0.75
train loss:  0.5063607692718506
train gradient:  0.20774966809250245
iteration : 3778
train acc:  0.734375
train loss:  0.5188273191452026
train gradient:  0.21579276480818643
iteration : 3779
train acc:  0.734375
train loss:  0.5503991842269897
train gradient:  0.1762337604587711
iteration : 3780
train acc:  0.65625
train loss:  0.6084092259407043
train gradient:  0.1666366784244789
iteration : 3781
train acc:  0.75
train loss:  0.4977281093597412
train gradient:  0.17475141314186587
iteration : 3782
train acc:  0.7265625
train loss:  0.5716885328292847
train gradient:  0.18043130907924126
iteration : 3783
train acc:  0.734375
train loss:  0.4822566509246826
train gradient:  0.11543662198182153
iteration : 3784
train acc:  0.671875
train loss:  0.5350799560546875
train gradient:  0.13884284876879024
iteration : 3785
train acc:  0.6875
train loss:  0.5253899097442627
train gradient:  0.1870173922062252
iteration : 3786
train acc:  0.78125
train loss:  0.48242759704589844
train gradient:  0.12391627089667495
iteration : 3787
train acc:  0.7265625
train loss:  0.5299492478370667
train gradient:  0.16236377586749645
iteration : 3788
train acc:  0.6875
train loss:  0.5321136713027954
train gradient:  0.20167054240592608
iteration : 3789
train acc:  0.671875
train loss:  0.5972909331321716
train gradient:  0.26169572924765183
iteration : 3790
train acc:  0.6796875
train loss:  0.5928102135658264
train gradient:  0.1422226688940606
iteration : 3791
train acc:  0.75
train loss:  0.5039699673652649
train gradient:  0.1938289143475909
iteration : 3792
train acc:  0.765625
train loss:  0.49622002243995667
train gradient:  0.1319231999734451
iteration : 3793
train acc:  0.640625
train loss:  0.6449843049049377
train gradient:  0.18326529850926768
iteration : 3794
train acc:  0.734375
train loss:  0.5227841138839722
train gradient:  0.13052900304704995
iteration : 3795
train acc:  0.7421875
train loss:  0.49710506200790405
train gradient:  0.1972620947497743
iteration : 3796
train acc:  0.7265625
train loss:  0.531775712966919
train gradient:  0.1573826903906868
iteration : 3797
train acc:  0.78125
train loss:  0.4804038405418396
train gradient:  0.17131162023776308
iteration : 3798
train acc:  0.765625
train loss:  0.5045421719551086
train gradient:  0.15020779694716882
iteration : 3799
train acc:  0.703125
train loss:  0.5301620960235596
train gradient:  0.16618176255427622
iteration : 3800
train acc:  0.65625
train loss:  0.656307578086853
train gradient:  0.261143986008777
iteration : 3801
train acc:  0.6875
train loss:  0.5784022808074951
train gradient:  0.16297469223575112
iteration : 3802
train acc:  0.640625
train loss:  0.5427305102348328
train gradient:  0.16937229161887568
iteration : 3803
train acc:  0.7734375
train loss:  0.47217780351638794
train gradient:  0.13944425437971175
iteration : 3804
train acc:  0.7578125
train loss:  0.4482920169830322
train gradient:  0.142621984830739
iteration : 3805
train acc:  0.75
train loss:  0.4812265634536743
train gradient:  0.13196026928556154
iteration : 3806
train acc:  0.6640625
train loss:  0.5565066933631897
train gradient:  0.15539968674347882
iteration : 3807
train acc:  0.703125
train loss:  0.5512452125549316
train gradient:  0.15787615216857814
iteration : 3808
train acc:  0.6796875
train loss:  0.536389172077179
train gradient:  0.14604247216363775
iteration : 3809
train acc:  0.75
train loss:  0.48878008127212524
train gradient:  0.11844566839750034
iteration : 3810
train acc:  0.703125
train loss:  0.549731969833374
train gradient:  0.15947311365081296
iteration : 3811
train acc:  0.765625
train loss:  0.50596022605896
train gradient:  0.15991822808152203
iteration : 3812
train acc:  0.7265625
train loss:  0.5262874364852905
train gradient:  0.11389909586303644
iteration : 3813
train acc:  0.7734375
train loss:  0.4850391745567322
train gradient:  0.13205879178060392
iteration : 3814
train acc:  0.7578125
train loss:  0.4693734645843506
train gradient:  0.131447029675549
iteration : 3815
train acc:  0.71875
train loss:  0.5408975481987
train gradient:  0.1575945369260065
iteration : 3816
train acc:  0.71875
train loss:  0.5761775970458984
train gradient:  0.1634038324751228
iteration : 3817
train acc:  0.671875
train loss:  0.5816091895103455
train gradient:  0.17311735741283843
iteration : 3818
train acc:  0.640625
train loss:  0.5718739032745361
train gradient:  0.17126946161950074
iteration : 3819
train acc:  0.703125
train loss:  0.5473724603652954
train gradient:  0.20851170663174834
iteration : 3820
train acc:  0.703125
train loss:  0.5401186943054199
train gradient:  0.14635245458707433
iteration : 3821
train acc:  0.65625
train loss:  0.6025183200836182
train gradient:  0.2038409943001797
iteration : 3822
train acc:  0.71875
train loss:  0.5274132490158081
train gradient:  0.1981103420321149
iteration : 3823
train acc:  0.7109375
train loss:  0.5639879703521729
train gradient:  0.18087231070106857
iteration : 3824
train acc:  0.7734375
train loss:  0.4981745481491089
train gradient:  0.14652777706359532
iteration : 3825
train acc:  0.71875
train loss:  0.5222145915031433
train gradient:  0.14573101206920397
iteration : 3826
train acc:  0.7109375
train loss:  0.5619816780090332
train gradient:  0.13847370447452872
iteration : 3827
train acc:  0.75
train loss:  0.5135623812675476
train gradient:  0.171958900397027
iteration : 3828
train acc:  0.7421875
train loss:  0.5218371748924255
train gradient:  0.1579551080690979
iteration : 3829
train acc:  0.6953125
train loss:  0.5222100019454956
train gradient:  0.1319678254798636
iteration : 3830
train acc:  0.7421875
train loss:  0.5157686471939087
train gradient:  0.1457805305383128
iteration : 3831
train acc:  0.7890625
train loss:  0.4672338366508484
train gradient:  0.12529862189688054
iteration : 3832
train acc:  0.7578125
train loss:  0.5208039283752441
train gradient:  0.15652379867282978
iteration : 3833
train acc:  0.6640625
train loss:  0.5676622986793518
train gradient:  0.1587657784473798
iteration : 3834
train acc:  0.703125
train loss:  0.5206587314605713
train gradient:  0.15376698971002184
iteration : 3835
train acc:  0.6640625
train loss:  0.5864633321762085
train gradient:  0.16784223244177088
iteration : 3836
train acc:  0.71875
train loss:  0.5282966494560242
train gradient:  0.19287846930262514
iteration : 3837
train acc:  0.6875
train loss:  0.5596633553504944
train gradient:  0.1647427047453221
iteration : 3838
train acc:  0.7109375
train loss:  0.550934910774231
train gradient:  0.1752030550693529
iteration : 3839
train acc:  0.7734375
train loss:  0.51164710521698
train gradient:  0.1494186335238392
iteration : 3840
train acc:  0.7265625
train loss:  0.526496171951294
train gradient:  0.1443825523045309
iteration : 3841
train acc:  0.7578125
train loss:  0.48768454790115356
train gradient:  0.16699513253090023
iteration : 3842
train acc:  0.6875
train loss:  0.5460456609725952
train gradient:  0.15669895381596038
iteration : 3843
train acc:  0.7421875
train loss:  0.47744020819664
train gradient:  0.14670374271153852
iteration : 3844
train acc:  0.7109375
train loss:  0.5485111474990845
train gradient:  0.16611395395972817
iteration : 3845
train acc:  0.671875
train loss:  0.5518161058425903
train gradient:  0.17375445662554595
iteration : 3846
train acc:  0.6640625
train loss:  0.5652114152908325
train gradient:  0.1509846220758426
iteration : 3847
train acc:  0.671875
train loss:  0.5976434946060181
train gradient:  0.18302467745401857
iteration : 3848
train acc:  0.7109375
train loss:  0.5163522958755493
train gradient:  0.14780280175409427
iteration : 3849
train acc:  0.6953125
train loss:  0.5610572695732117
train gradient:  0.15210951691823496
iteration : 3850
train acc:  0.71875
train loss:  0.5384668111801147
train gradient:  0.16534731890447885
iteration : 3851
train acc:  0.7109375
train loss:  0.518560528755188
train gradient:  0.13741353301146148
iteration : 3852
train acc:  0.7421875
train loss:  0.5146090388298035
train gradient:  0.15030412400888613
iteration : 3853
train acc:  0.796875
train loss:  0.49031203985214233
train gradient:  0.17044091865518818
iteration : 3854
train acc:  0.75
train loss:  0.508582592010498
train gradient:  0.11334393181591206
iteration : 3855
train acc:  0.71875
train loss:  0.5288169980049133
train gradient:  0.15752995234855302
iteration : 3856
train acc:  0.75
train loss:  0.4925975799560547
train gradient:  0.1460318554446245
iteration : 3857
train acc:  0.71875
train loss:  0.4817572832107544
train gradient:  0.13746130242860555
iteration : 3858
train acc:  0.703125
train loss:  0.5419919490814209
train gradient:  0.1919471525098726
iteration : 3859
train acc:  0.65625
train loss:  0.5602402091026306
train gradient:  0.15079567385053216
iteration : 3860
train acc:  0.734375
train loss:  0.49719882011413574
train gradient:  0.1313393868574589
iteration : 3861
train acc:  0.7109375
train loss:  0.5156019926071167
train gradient:  0.12945736382882195
iteration : 3862
train acc:  0.78125
train loss:  0.4835936427116394
train gradient:  0.1282879907673965
iteration : 3863
train acc:  0.7890625
train loss:  0.48706260323524475
train gradient:  0.10636647547038637
iteration : 3864
train acc:  0.671875
train loss:  0.6021031141281128
train gradient:  0.16164382382600498
iteration : 3865
train acc:  0.78125
train loss:  0.44225215911865234
train gradient:  0.10483652753537422
iteration : 3866
train acc:  0.75
train loss:  0.481904536485672
train gradient:  0.1661610955145998
iteration : 3867
train acc:  0.7578125
train loss:  0.4691694676876068
train gradient:  0.13759959631695767
iteration : 3868
train acc:  0.78125
train loss:  0.43600159883499146
train gradient:  0.16217691236196785
iteration : 3869
train acc:  0.7265625
train loss:  0.5286339521408081
train gradient:  0.14554836805549315
iteration : 3870
train acc:  0.71875
train loss:  0.5103504657745361
train gradient:  0.14162364398861604
iteration : 3871
train acc:  0.734375
train loss:  0.5277191996574402
train gradient:  0.22033150989680517
iteration : 3872
train acc:  0.7109375
train loss:  0.5394240617752075
train gradient:  0.15955127854339368
iteration : 3873
train acc:  0.7890625
train loss:  0.4711933732032776
train gradient:  0.11745818674822012
iteration : 3874
train acc:  0.7265625
train loss:  0.5326914191246033
train gradient:  0.19339489101096052
iteration : 3875
train acc:  0.7890625
train loss:  0.44894421100616455
train gradient:  0.1344025835787086
iteration : 3876
train acc:  0.78125
train loss:  0.4705389142036438
train gradient:  0.11546478841619132
iteration : 3877
train acc:  0.7578125
train loss:  0.4731992185115814
train gradient:  0.1658068657623435
iteration : 3878
train acc:  0.734375
train loss:  0.5332558155059814
train gradient:  0.15204375758124966
iteration : 3879
train acc:  0.59375
train loss:  0.6826661825180054
train gradient:  0.25132965814504926
iteration : 3880
train acc:  0.6796875
train loss:  0.5811957120895386
train gradient:  0.1373623223167538
iteration : 3881
train acc:  0.734375
train loss:  0.49992135167121887
train gradient:  0.12448333025312586
iteration : 3882
train acc:  0.6796875
train loss:  0.6186013221740723
train gradient:  0.24849355090997766
iteration : 3883
train acc:  0.6875
train loss:  0.5295780897140503
train gradient:  0.15956481118030474
iteration : 3884
train acc:  0.6484375
train loss:  0.5925586223602295
train gradient:  0.2054068571442237
iteration : 3885
train acc:  0.7265625
train loss:  0.5286707878112793
train gradient:  0.19995705324000257
iteration : 3886
train acc:  0.7734375
train loss:  0.4442196190357208
train gradient:  0.16094040378871122
iteration : 3887
train acc:  0.7265625
train loss:  0.5042189955711365
train gradient:  0.15730423123516862
iteration : 3888
train acc:  0.6796875
train loss:  0.5321369171142578
train gradient:  0.14243298976991875
iteration : 3889
train acc:  0.671875
train loss:  0.5524680614471436
train gradient:  0.1774048996403061
iteration : 3890
train acc:  0.75
train loss:  0.5351707935333252
train gradient:  0.17908795911156716
iteration : 3891
train acc:  0.703125
train loss:  0.544101357460022
train gradient:  0.14300346624193677
iteration : 3892
train acc:  0.7578125
train loss:  0.5312542915344238
train gradient:  0.19447025034728865
iteration : 3893
train acc:  0.65625
train loss:  0.5918034315109253
train gradient:  0.15679981075580926
iteration : 3894
train acc:  0.6875
train loss:  0.5128825902938843
train gradient:  0.11585890598175519
iteration : 3895
train acc:  0.734375
train loss:  0.5023801326751709
train gradient:  0.15240488421873985
iteration : 3896
train acc:  0.734375
train loss:  0.5031877756118774
train gradient:  0.15710194768996702
iteration : 3897
train acc:  0.671875
train loss:  0.5977725982666016
train gradient:  0.18792296197961286
iteration : 3898
train acc:  0.7578125
train loss:  0.5184954404830933
train gradient:  0.1860583167772672
iteration : 3899
train acc:  0.734375
train loss:  0.4995458722114563
train gradient:  0.15305527679446795
iteration : 3900
train acc:  0.78125
train loss:  0.4954046308994293
train gradient:  0.18891289396967914
iteration : 3901
train acc:  0.703125
train loss:  0.5408186912536621
train gradient:  0.17490863166799908
iteration : 3902
train acc:  0.640625
train loss:  0.5579566955566406
train gradient:  0.16438689157144942
iteration : 3903
train acc:  0.65625
train loss:  0.5395065546035767
train gradient:  0.14114235608590753
iteration : 3904
train acc:  0.7265625
train loss:  0.5112138390541077
train gradient:  0.16614015622900966
iteration : 3905
train acc:  0.703125
train loss:  0.5062424540519714
train gradient:  0.22390840931787717
iteration : 3906
train acc:  0.6953125
train loss:  0.5310216546058655
train gradient:  0.16436873136647293
iteration : 3907
train acc:  0.7109375
train loss:  0.5594387054443359
train gradient:  0.23817599125484018
iteration : 3908
train acc:  0.7109375
train loss:  0.5354293584823608
train gradient:  0.1718920056516141
iteration : 3909
train acc:  0.75
train loss:  0.4876992702484131
train gradient:  0.15745621792389847
iteration : 3910
train acc:  0.7109375
train loss:  0.5517477989196777
train gradient:  0.14664344334116258
iteration : 3911
train acc:  0.734375
train loss:  0.4771074056625366
train gradient:  0.10726406169028507
iteration : 3912
train acc:  0.7890625
train loss:  0.45351892709732056
train gradient:  0.13133926178166827
iteration : 3913
train acc:  0.7578125
train loss:  0.4764035642147064
train gradient:  0.14183142546495117
iteration : 3914
train acc:  0.75
train loss:  0.4723985493183136
train gradient:  0.15341122679429053
iteration : 3915
train acc:  0.703125
train loss:  0.5116490721702576
train gradient:  0.20393597511102352
iteration : 3916
train acc:  0.734375
train loss:  0.515684962272644
train gradient:  0.17891366163043418
iteration : 3917
train acc:  0.71875
train loss:  0.5086507797241211
train gradient:  0.14671248787018296
iteration : 3918
train acc:  0.6875
train loss:  0.5461623668670654
train gradient:  0.17600123619402658
iteration : 3919
train acc:  0.6953125
train loss:  0.572460412979126
train gradient:  0.23942943907074477
iteration : 3920
train acc:  0.734375
train loss:  0.524613618850708
train gradient:  0.1612815712353514
iteration : 3921
train acc:  0.7265625
train loss:  0.49706733226776123
train gradient:  0.14084401607587968
iteration : 3922
train acc:  0.7109375
train loss:  0.5455442070960999
train gradient:  0.13498785269742886
iteration : 3923
train acc:  0.6953125
train loss:  0.5397979617118835
train gradient:  0.1814478884859846
iteration : 3924
train acc:  0.7421875
train loss:  0.474368691444397
train gradient:  0.12759559405576373
iteration : 3925
train acc:  0.640625
train loss:  0.6205897927284241
train gradient:  0.17852509611170603
iteration : 3926
train acc:  0.6796875
train loss:  0.5867730379104614
train gradient:  0.16914199791797904
iteration : 3927
train acc:  0.75
train loss:  0.5135186910629272
train gradient:  0.13513478280850724
iteration : 3928
train acc:  0.7109375
train loss:  0.5664259195327759
train gradient:  0.1804257241654171
iteration : 3929
train acc:  0.75
train loss:  0.5059961676597595
train gradient:  0.13836211263975684
iteration : 3930
train acc:  0.7109375
train loss:  0.5411350727081299
train gradient:  0.14668014442350075
iteration : 3931
train acc:  0.7578125
train loss:  0.47017136216163635
train gradient:  0.13556814942326745
iteration : 3932
train acc:  0.734375
train loss:  0.5346415042877197
train gradient:  0.1407313132273374
iteration : 3933
train acc:  0.7890625
train loss:  0.45239707827568054
train gradient:  0.1546978535888157
iteration : 3934
train acc:  0.71875
train loss:  0.558251142501831
train gradient:  0.165366374856194
iteration : 3935
train acc:  0.6875
train loss:  0.5295020341873169
train gradient:  0.15017655867890498
iteration : 3936
train acc:  0.734375
train loss:  0.47180449962615967
train gradient:  0.1336130715779183
iteration : 3937
train acc:  0.796875
train loss:  0.47552305459976196
train gradient:  0.1133986317358919
iteration : 3938
train acc:  0.7734375
train loss:  0.5235507488250732
train gradient:  0.14163166980502256
iteration : 3939
train acc:  0.765625
train loss:  0.4525226950645447
train gradient:  0.11869430488345455
iteration : 3940
train acc:  0.75
train loss:  0.5254838466644287
train gradient:  0.1324769509952559
iteration : 3941
train acc:  0.71875
train loss:  0.5285413265228271
train gradient:  0.15047877209805072
iteration : 3942
train acc:  0.734375
train loss:  0.4642395079135895
train gradient:  0.14190154162150465
iteration : 3943
train acc:  0.7578125
train loss:  0.4778929352760315
train gradient:  0.12905649253364843
iteration : 3944
train acc:  0.765625
train loss:  0.515109658241272
train gradient:  0.16713649000232506
iteration : 3945
train acc:  0.6484375
train loss:  0.5903153419494629
train gradient:  0.17199270142130788
iteration : 3946
train acc:  0.7265625
train loss:  0.5043571591377258
train gradient:  0.17411195524990758
iteration : 3947
train acc:  0.75
train loss:  0.5058024525642395
train gradient:  0.19733698569957003
iteration : 3948
train acc:  0.765625
train loss:  0.5167898535728455
train gradient:  0.1441813928962084
iteration : 3949
train acc:  0.7734375
train loss:  0.4691580533981323
train gradient:  0.1393907193414645
iteration : 3950
train acc:  0.6640625
train loss:  0.5996599197387695
train gradient:  0.20606084228398402
iteration : 3951
train acc:  0.734375
train loss:  0.4911035895347595
train gradient:  0.14692900874096765
iteration : 3952
train acc:  0.7578125
train loss:  0.48804956674575806
train gradient:  0.14076344999748663
iteration : 3953
train acc:  0.7265625
train loss:  0.5202014446258545
train gradient:  0.1603210339161267
iteration : 3954
train acc:  0.734375
train loss:  0.5341232419013977
train gradient:  0.21583693607430826
iteration : 3955
train acc:  0.796875
train loss:  0.449026882648468
train gradient:  0.14554854274912
iteration : 3956
train acc:  0.6640625
train loss:  0.5467780232429504
train gradient:  0.1670606969969723
iteration : 3957
train acc:  0.734375
train loss:  0.5321770310401917
train gradient:  0.153595199893816
iteration : 3958
train acc:  0.7734375
train loss:  0.4617493152618408
train gradient:  0.1232273226455664
iteration : 3959
train acc:  0.765625
train loss:  0.5147324800491333
train gradient:  0.1393244089770791
iteration : 3960
train acc:  0.703125
train loss:  0.5326845645904541
train gradient:  0.1781088082230115
iteration : 3961
train acc:  0.7421875
train loss:  0.5422425866127014
train gradient:  0.16217504889789716
iteration : 3962
train acc:  0.75
train loss:  0.4845139980316162
train gradient:  0.12376818545673175
iteration : 3963
train acc:  0.765625
train loss:  0.5426812171936035
train gradient:  0.1745624002907813
iteration : 3964
train acc:  0.71875
train loss:  0.5268990993499756
train gradient:  0.1882365858079384
iteration : 3965
train acc:  0.7109375
train loss:  0.5652883052825928
train gradient:  0.18401556486453222
iteration : 3966
train acc:  0.75
train loss:  0.5162540674209595
train gradient:  0.17111291697657138
iteration : 3967
train acc:  0.7421875
train loss:  0.5275548696517944
train gradient:  0.1675050414830833
iteration : 3968
train acc:  0.71875
train loss:  0.559739887714386
train gradient:  0.17827282057373828
iteration : 3969
train acc:  0.703125
train loss:  0.5166974067687988
train gradient:  0.13034516647806987
iteration : 3970
train acc:  0.75
train loss:  0.46707186102867126
train gradient:  0.17030242715481725
iteration : 3971
train acc:  0.765625
train loss:  0.452935129404068
train gradient:  0.14423957068382598
iteration : 3972
train acc:  0.6875
train loss:  0.5609748363494873
train gradient:  0.15732571929049355
iteration : 3973
train acc:  0.7265625
train loss:  0.5185744762420654
train gradient:  0.1761171371917835
iteration : 3974
train acc:  0.7265625
train loss:  0.5362102389335632
train gradient:  0.17333078041422773
iteration : 3975
train acc:  0.703125
train loss:  0.5703388452529907
train gradient:  0.19359364767188375
iteration : 3976
train acc:  0.703125
train loss:  0.5242862701416016
train gradient:  0.1731600675387719
iteration : 3977
train acc:  0.71875
train loss:  0.5544828176498413
train gradient:  0.15138936308834144
iteration : 3978
train acc:  0.7265625
train loss:  0.5231346487998962
train gradient:  0.1248258252540547
iteration : 3979
train acc:  0.7734375
train loss:  0.48965126276016235
train gradient:  0.16901324055869507
iteration : 3980
train acc:  0.703125
train loss:  0.5300551056861877
train gradient:  0.15188087637716907
iteration : 3981
train acc:  0.7734375
train loss:  0.5039960741996765
train gradient:  0.17419563697586368
iteration : 3982
train acc:  0.7578125
train loss:  0.44240665435791016
train gradient:  0.15220058650156187
iteration : 3983
train acc:  0.6484375
train loss:  0.613232433795929
train gradient:  0.26674663162542717
iteration : 3984
train acc:  0.6328125
train loss:  0.5888989567756653
train gradient:  0.21422551478758403
iteration : 3985
train acc:  0.78125
train loss:  0.48470839858055115
train gradient:  0.13404347941419542
iteration : 3986
train acc:  0.7421875
train loss:  0.4802655279636383
train gradient:  0.2076343204735946
iteration : 3987
train acc:  0.6875
train loss:  0.5399212837219238
train gradient:  0.18842341803467139
iteration : 3988
train acc:  0.671875
train loss:  0.5329030752182007
train gradient:  0.1507273284577472
iteration : 3989
train acc:  0.7734375
train loss:  0.510869026184082
train gradient:  0.1589306621949525
iteration : 3990
train acc:  0.7421875
train loss:  0.5054934024810791
train gradient:  0.13924667835015436
iteration : 3991
train acc:  0.734375
train loss:  0.49037477374076843
train gradient:  0.19021050863140262
iteration : 3992
train acc:  0.6328125
train loss:  0.6626087427139282
train gradient:  0.20636077785946336
iteration : 3993
train acc:  0.7734375
train loss:  0.47752171754837036
train gradient:  0.14321826376652125
iteration : 3994
train acc:  0.71875
train loss:  0.5206795930862427
train gradient:  0.1651650813311497
iteration : 3995
train acc:  0.7890625
train loss:  0.4827755093574524
train gradient:  0.14666439788450192
iteration : 3996
train acc:  0.6953125
train loss:  0.5235714912414551
train gradient:  0.14212433940403102
iteration : 3997
train acc:  0.78125
train loss:  0.5097814202308655
train gradient:  0.15772127666564328
iteration : 3998
train acc:  0.7265625
train loss:  0.5116772651672363
train gradient:  0.1593613879146895
iteration : 3999
train acc:  0.703125
train loss:  0.5949481725692749
train gradient:  0.20167368932613638
iteration : 4000
train acc:  0.7265625
train loss:  0.49064385890960693
train gradient:  0.16588622041674228
iteration : 4001
train acc:  0.7109375
train loss:  0.5571093559265137
train gradient:  0.16864652896093862
iteration : 4002
train acc:  0.671875
train loss:  0.587533712387085
train gradient:  0.1626876023584956
iteration : 4003
train acc:  0.6953125
train loss:  0.5391380786895752
train gradient:  0.22239800468912427
iteration : 4004
train acc:  0.7109375
train loss:  0.5208249688148499
train gradient:  0.14522303874216547
iteration : 4005
train acc:  0.75
train loss:  0.49184101819992065
train gradient:  0.14611423536600993
iteration : 4006
train acc:  0.6640625
train loss:  0.5731391906738281
train gradient:  0.19795588495720007
iteration : 4007
train acc:  0.7734375
train loss:  0.47507530450820923
train gradient:  0.1345896139512578
iteration : 4008
train acc:  0.6953125
train loss:  0.5676103830337524
train gradient:  0.14859229781439431
iteration : 4009
train acc:  0.7734375
train loss:  0.5062268376350403
train gradient:  0.12089599676344476
iteration : 4010
train acc:  0.75
train loss:  0.4950695335865021
train gradient:  0.1676283308327075
iteration : 4011
train acc:  0.703125
train loss:  0.5195040702819824
train gradient:  0.1505176123926814
iteration : 4012
train acc:  0.734375
train loss:  0.5644447803497314
train gradient:  0.14481329672672427
iteration : 4013
train acc:  0.7734375
train loss:  0.5181439518928528
train gradient:  0.153397397590487
iteration : 4014
train acc:  0.6953125
train loss:  0.5306048393249512
train gradient:  0.15475668753245786
iteration : 4015
train acc:  0.75
train loss:  0.4998091459274292
train gradient:  0.13938186953374937
iteration : 4016
train acc:  0.7265625
train loss:  0.5464456081390381
train gradient:  0.17775927511670178
iteration : 4017
train acc:  0.7421875
train loss:  0.5070144534111023
train gradient:  0.1278042304193624
iteration : 4018
train acc:  0.671875
train loss:  0.5736384391784668
train gradient:  0.22769511777110807
iteration : 4019
train acc:  0.6875
train loss:  0.5513303279876709
train gradient:  0.18934951948381107
iteration : 4020
train acc:  0.671875
train loss:  0.6425697803497314
train gradient:  0.21262096648047088
iteration : 4021
train acc:  0.7421875
train loss:  0.5092939138412476
train gradient:  0.18776510325919338
iteration : 4022
train acc:  0.71875
train loss:  0.5076764225959778
train gradient:  0.18956460654829044
iteration : 4023
train acc:  0.734375
train loss:  0.5810043811798096
train gradient:  0.2072265992033976
iteration : 4024
train acc:  0.7421875
train loss:  0.536514163017273
train gradient:  0.18895983739046796
iteration : 4025
train acc:  0.7421875
train loss:  0.4950612187385559
train gradient:  0.12110123514662591
iteration : 4026
train acc:  0.7578125
train loss:  0.47646450996398926
train gradient:  0.13369962965739102
iteration : 4027
train acc:  0.7265625
train loss:  0.5139370560646057
train gradient:  0.16573609900717493
iteration : 4028
train acc:  0.6796875
train loss:  0.5463361144065857
train gradient:  0.17405759280952166
iteration : 4029
train acc:  0.703125
train loss:  0.5054367184638977
train gradient:  0.14898668958039388
iteration : 4030
train acc:  0.6796875
train loss:  0.5691772103309631
train gradient:  0.1828664990244413
iteration : 4031
train acc:  0.703125
train loss:  0.5102817416191101
train gradient:  0.1784335398338895
iteration : 4032
train acc:  0.7421875
train loss:  0.4796115458011627
train gradient:  0.1350105408451707
iteration : 4033
train acc:  0.609375
train loss:  0.628524661064148
train gradient:  0.23238986799361824
iteration : 4034
train acc:  0.6484375
train loss:  0.5996785759925842
train gradient:  0.15348136113322647
iteration : 4035
train acc:  0.734375
train loss:  0.5474737286567688
train gradient:  0.14279136322730135
iteration : 4036
train acc:  0.6953125
train loss:  0.5585677623748779
train gradient:  0.21527435249812393
iteration : 4037
train acc:  0.7265625
train loss:  0.5064215660095215
train gradient:  0.12755725443364985
iteration : 4038
train acc:  0.703125
train loss:  0.5138707756996155
train gradient:  0.12013166626382318
iteration : 4039
train acc:  0.6953125
train loss:  0.5310044288635254
train gradient:  0.1254768114124715
iteration : 4040
train acc:  0.71875
train loss:  0.5879522562026978
train gradient:  0.19390177960892607
iteration : 4041
train acc:  0.7109375
train loss:  0.48374733328819275
train gradient:  0.15085960475112986
iteration : 4042
train acc:  0.78125
train loss:  0.4786342680454254
train gradient:  0.15697616132074071
iteration : 4043
train acc:  0.671875
train loss:  0.5406814813613892
train gradient:  0.14267679863531924
iteration : 4044
train acc:  0.7265625
train loss:  0.5021018981933594
train gradient:  0.13288726932916095
iteration : 4045
train acc:  0.6484375
train loss:  0.5660222768783569
train gradient:  0.16163128335989785
iteration : 4046
train acc:  0.734375
train loss:  0.5135531425476074
train gradient:  0.1576591129008702
iteration : 4047
train acc:  0.6796875
train loss:  0.528120756149292
train gradient:  0.18061592991725084
iteration : 4048
train acc:  0.65625
train loss:  0.560041069984436
train gradient:  0.14985942068274521
iteration : 4049
train acc:  0.7578125
train loss:  0.4961552321910858
train gradient:  0.1413779043765589
iteration : 4050
train acc:  0.71875
train loss:  0.5276195406913757
train gradient:  0.17381224333666465
iteration : 4051
train acc:  0.703125
train loss:  0.5841571092605591
train gradient:  0.16437355290029132
iteration : 4052
train acc:  0.7421875
train loss:  0.5515370965003967
train gradient:  0.15874973904643042
iteration : 4053
train acc:  0.7421875
train loss:  0.47453951835632324
train gradient:  0.14131978166290263
iteration : 4054
train acc:  0.6875
train loss:  0.5365981459617615
train gradient:  0.14116014039619285
iteration : 4055
train acc:  0.6953125
train loss:  0.5773501396179199
train gradient:  0.18154538869794512
iteration : 4056
train acc:  0.703125
train loss:  0.5511428117752075
train gradient:  0.14808793808137227
iteration : 4057
train acc:  0.7109375
train loss:  0.5282652974128723
train gradient:  0.1865726096726022
iteration : 4058
train acc:  0.6796875
train loss:  0.5745137929916382
train gradient:  0.1693551849302987
iteration : 4059
train acc:  0.71875
train loss:  0.5426558256149292
train gradient:  0.23782859392629746
iteration : 4060
train acc:  0.7265625
train loss:  0.5104339122772217
train gradient:  0.12254527072562361
iteration : 4061
train acc:  0.7421875
train loss:  0.5141741037368774
train gradient:  0.20025063344250418
iteration : 4062
train acc:  0.7578125
train loss:  0.49967148900032043
train gradient:  0.1693238996856572
iteration : 4063
train acc:  0.71875
train loss:  0.5194643139839172
train gradient:  0.13700611080184377
iteration : 4064
train acc:  0.7265625
train loss:  0.5327730178833008
train gradient:  0.15350277863674322
iteration : 4065
train acc:  0.75
train loss:  0.5257859230041504
train gradient:  0.17405995602370763
iteration : 4066
train acc:  0.6640625
train loss:  0.5538671016693115
train gradient:  0.18298882309119402
iteration : 4067
train acc:  0.7265625
train loss:  0.5133132934570312
train gradient:  0.16948379249657322
iteration : 4068
train acc:  0.71875
train loss:  0.564079761505127
train gradient:  0.15237615334968285
iteration : 4069
train acc:  0.7578125
train loss:  0.5015117526054382
train gradient:  0.1686981483939843
iteration : 4070
train acc:  0.734375
train loss:  0.5545117855072021
train gradient:  0.17144999868495903
iteration : 4071
train acc:  0.6875
train loss:  0.5083042979240417
train gradient:  0.18561130640758533
iteration : 4072
train acc:  0.71875
train loss:  0.5874348878860474
train gradient:  0.19360306554951878
iteration : 4073
train acc:  0.765625
train loss:  0.5130503177642822
train gradient:  0.13925603123373487
iteration : 4074
train acc:  0.6953125
train loss:  0.5326857566833496
train gradient:  0.16898093249479124
iteration : 4075
train acc:  0.734375
train loss:  0.4673466682434082
train gradient:  0.12346427299123974
iteration : 4076
train acc:  0.671875
train loss:  0.5720089673995972
train gradient:  0.16542602416246635
iteration : 4077
train acc:  0.7421875
train loss:  0.5259931683540344
train gradient:  0.1828638520516755
iteration : 4078
train acc:  0.7265625
train loss:  0.5201092958450317
train gradient:  0.1357066300457696
iteration : 4079
train acc:  0.7109375
train loss:  0.5447069406509399
train gradient:  0.2091933099123203
iteration : 4080
train acc:  0.765625
train loss:  0.4903664290904999
train gradient:  0.13189028694227128
iteration : 4081
train acc:  0.7109375
train loss:  0.551856517791748
train gradient:  0.1823408733662366
iteration : 4082
train acc:  0.7734375
train loss:  0.5065584182739258
train gradient:  0.14339270512074576
iteration : 4083
train acc:  0.734375
train loss:  0.5265763401985168
train gradient:  0.1479946629191548
iteration : 4084
train acc:  0.7109375
train loss:  0.5876693725585938
train gradient:  0.191786762374844
iteration : 4085
train acc:  0.671875
train loss:  0.5763089060783386
train gradient:  0.15026932236628943
iteration : 4086
train acc:  0.6640625
train loss:  0.563957929611206
train gradient:  0.13857172986993077
iteration : 4087
train acc:  0.6640625
train loss:  0.5541022419929504
train gradient:  0.18013972045912768
iteration : 4088
train acc:  0.765625
train loss:  0.4816698729991913
train gradient:  0.19268030271378983
iteration : 4089
train acc:  0.6484375
train loss:  0.5383467674255371
train gradient:  0.16563415839477436
iteration : 4090
train acc:  0.7265625
train loss:  0.49769115447998047
train gradient:  0.16757053560639068
iteration : 4091
train acc:  0.765625
train loss:  0.5144739747047424
train gradient:  0.13011062353446912
iteration : 4092
train acc:  0.6953125
train loss:  0.5025896430015564
train gradient:  0.13650110772003818
iteration : 4093
train acc:  0.7109375
train loss:  0.5215619802474976
train gradient:  0.13123005987049005
iteration : 4094
train acc:  0.71875
train loss:  0.5368732810020447
train gradient:  0.145959555109425
iteration : 4095
train acc:  0.6875
train loss:  0.5205442309379578
train gradient:  0.1503053878273748
iteration : 4096
train acc:  0.6484375
train loss:  0.6242713928222656
train gradient:  0.2348939242178434
iteration : 4097
train acc:  0.78125
train loss:  0.5081246495246887
train gradient:  0.15901492155087615
iteration : 4098
train acc:  0.7265625
train loss:  0.49768346548080444
train gradient:  0.10671865756575576
iteration : 4099
train acc:  0.6953125
train loss:  0.5311916470527649
train gradient:  0.14542709529095607
iteration : 4100
train acc:  0.7734375
train loss:  0.47339463233947754
train gradient:  0.12365345632628764
iteration : 4101
train acc:  0.7109375
train loss:  0.5314595699310303
train gradient:  0.12751051654012388
iteration : 4102
train acc:  0.6796875
train loss:  0.5534194111824036
train gradient:  0.2214771404441556
iteration : 4103
train acc:  0.6640625
train loss:  0.6160253286361694
train gradient:  0.16349641069644624
iteration : 4104
train acc:  0.734375
train loss:  0.49337223172187805
train gradient:  0.13909214803246922
iteration : 4105
train acc:  0.6796875
train loss:  0.5691696405410767
train gradient:  0.20577827548091202
iteration : 4106
train acc:  0.6953125
train loss:  0.5432164669036865
train gradient:  0.15440271998766625
iteration : 4107
train acc:  0.7265625
train loss:  0.5266676545143127
train gradient:  0.2162207468910044
iteration : 4108
train acc:  0.765625
train loss:  0.47827357053756714
train gradient:  0.1366479660084332
iteration : 4109
train acc:  0.7578125
train loss:  0.5191696882247925
train gradient:  0.14698954250419333
iteration : 4110
train acc:  0.796875
train loss:  0.49940016865730286
train gradient:  0.1686001427885436
iteration : 4111
train acc:  0.71875
train loss:  0.5122236013412476
train gradient:  0.13998895049041796
iteration : 4112
train acc:  0.734375
train loss:  0.49582046270370483
train gradient:  0.13704371623704914
iteration : 4113
train acc:  0.7265625
train loss:  0.4723705053329468
train gradient:  0.14467447847103304
iteration : 4114
train acc:  0.765625
train loss:  0.5014246106147766
train gradient:  0.13624231980915924
iteration : 4115
train acc:  0.796875
train loss:  0.443006694316864
train gradient:  0.1274058514595361
iteration : 4116
train acc:  0.796875
train loss:  0.4654405415058136
train gradient:  0.14721967409787245
iteration : 4117
train acc:  0.71875
train loss:  0.5192815065383911
train gradient:  0.1438636425614065
iteration : 4118
train acc:  0.6875
train loss:  0.5907901525497437
train gradient:  0.19180282508781238
iteration : 4119
train acc:  0.703125
train loss:  0.5013784170150757
train gradient:  0.13837462097201858
iteration : 4120
train acc:  0.65625
train loss:  0.6021831631660461
train gradient:  0.1693773167943787
iteration : 4121
train acc:  0.71875
train loss:  0.5533720850944519
train gradient:  0.19321669776394002
iteration : 4122
train acc:  0.7578125
train loss:  0.48896193504333496
train gradient:  0.10535066622845686
iteration : 4123
train acc:  0.7421875
train loss:  0.5267649292945862
train gradient:  0.16526498381581595
iteration : 4124
train acc:  0.8203125
train loss:  0.4310406446456909
train gradient:  0.10477604611517062
iteration : 4125
train acc:  0.6796875
train loss:  0.5325457453727722
train gradient:  0.16537913803552237
iteration : 4126
train acc:  0.6328125
train loss:  0.6252583265304565
train gradient:  0.16759279747083108
iteration : 4127
train acc:  0.7734375
train loss:  0.4803531765937805
train gradient:  0.1256802914381482
iteration : 4128
train acc:  0.7578125
train loss:  0.5192817449569702
train gradient:  0.16653293308235545
iteration : 4129
train acc:  0.7421875
train loss:  0.49029970169067383
train gradient:  0.15930218944048258
iteration : 4130
train acc:  0.71875
train loss:  0.5477274656295776
train gradient:  0.12395366563802485
iteration : 4131
train acc:  0.71875
train loss:  0.508131742477417
train gradient:  0.12233446183864721
iteration : 4132
train acc:  0.703125
train loss:  0.5348246097564697
train gradient:  0.20104975461156221
iteration : 4133
train acc:  0.7109375
train loss:  0.5767626762390137
train gradient:  0.1957731317783898
iteration : 4134
train acc:  0.6796875
train loss:  0.5674610137939453
train gradient:  0.17136561014374013
iteration : 4135
train acc:  0.7109375
train loss:  0.49997857213020325
train gradient:  0.1430281653093252
iteration : 4136
train acc:  0.7578125
train loss:  0.49140238761901855
train gradient:  0.14686775844283984
iteration : 4137
train acc:  0.75
train loss:  0.47772377729415894
train gradient:  0.1279889381879134
iteration : 4138
train acc:  0.8046875
train loss:  0.48572927713394165
train gradient:  0.11856196254440396
iteration : 4139
train acc:  0.671875
train loss:  0.5603540539741516
train gradient:  0.18991693181280395
iteration : 4140
train acc:  0.7421875
train loss:  0.4989321231842041
train gradient:  0.14688333098859319
iteration : 4141
train acc:  0.703125
train loss:  0.534597635269165
train gradient:  0.14895336441537818
iteration : 4142
train acc:  0.7265625
train loss:  0.4860032796859741
train gradient:  0.12769133299287566
iteration : 4143
train acc:  0.7734375
train loss:  0.4851438105106354
train gradient:  0.1282647943327841
iteration : 4144
train acc:  0.7421875
train loss:  0.5593059062957764
train gradient:  0.1520069425357077
iteration : 4145
train acc:  0.7578125
train loss:  0.4543764889240265
train gradient:  0.11848983983519709
iteration : 4146
train acc:  0.734375
train loss:  0.47692427039146423
train gradient:  0.22207464533114746
iteration : 4147
train acc:  0.765625
train loss:  0.4848904311656952
train gradient:  0.12791762786626149
iteration : 4148
train acc:  0.703125
train loss:  0.5415928363800049
train gradient:  0.1976773906185046
iteration : 4149
train acc:  0.7265625
train loss:  0.49451979994773865
train gradient:  0.15529924784446902
iteration : 4150
train acc:  0.78125
train loss:  0.4643964171409607
train gradient:  0.1481638438097006
iteration : 4151
train acc:  0.7734375
train loss:  0.47375917434692383
train gradient:  0.11918403370265847
iteration : 4152
train acc:  0.7421875
train loss:  0.4560045599937439
train gradient:  0.1702966630296476
iteration : 4153
train acc:  0.703125
train loss:  0.5350865125656128
train gradient:  0.1717991518107856
iteration : 4154
train acc:  0.75
train loss:  0.5099648237228394
train gradient:  0.17580136672481017
iteration : 4155
train acc:  0.71875
train loss:  0.5609233379364014
train gradient:  0.16190176285505808
iteration : 4156
train acc:  0.6875
train loss:  0.5748291015625
train gradient:  0.1761495742796837
iteration : 4157
train acc:  0.75
train loss:  0.4962242841720581
train gradient:  0.15064762652873032
iteration : 4158
train acc:  0.65625
train loss:  0.5826398134231567
train gradient:  0.19201701663883214
iteration : 4159
train acc:  0.828125
train loss:  0.41369155049324036
train gradient:  0.14840908731119531
iteration : 4160
train acc:  0.7421875
train loss:  0.5461170077323914
train gradient:  0.19745378332318592
iteration : 4161
train acc:  0.7109375
train loss:  0.5168470144271851
train gradient:  0.12232221628838383
iteration : 4162
train acc:  0.7109375
train loss:  0.5481012463569641
train gradient:  0.17231914606708953
iteration : 4163
train acc:  0.6953125
train loss:  0.5364259481430054
train gradient:  0.15769783393840126
iteration : 4164
train acc:  0.75
train loss:  0.4754636883735657
train gradient:  0.12230181714536628
iteration : 4165
train acc:  0.6796875
train loss:  0.5479235649108887
train gradient:  0.1488536918802683
iteration : 4166
train acc:  0.7265625
train loss:  0.5609912276268005
train gradient:  0.16415621733194094
iteration : 4167
train acc:  0.71875
train loss:  0.46113961935043335
train gradient:  0.11539219590523939
iteration : 4168
train acc:  0.6875
train loss:  0.6292012929916382
train gradient:  0.20853453032042818
iteration : 4169
train acc:  0.7421875
train loss:  0.5347499847412109
train gradient:  0.16471032607188946
iteration : 4170
train acc:  0.7578125
train loss:  0.47741085290908813
train gradient:  0.16639987269448878
iteration : 4171
train acc:  0.71875
train loss:  0.5093340873718262
train gradient:  0.11857491999657131
iteration : 4172
train acc:  0.71875
train loss:  0.5793731212615967
train gradient:  0.2111911132696294
iteration : 4173
train acc:  0.6328125
train loss:  0.6224813461303711
train gradient:  0.23188458162616227
iteration : 4174
train acc:  0.6796875
train loss:  0.5283282399177551
train gradient:  0.1910563224499076
iteration : 4175
train acc:  0.8125
train loss:  0.4233400821685791
train gradient:  0.1585183073365611
iteration : 4176
train acc:  0.6953125
train loss:  0.5346637964248657
train gradient:  0.1406439890082235
iteration : 4177
train acc:  0.7421875
train loss:  0.572638988494873
train gradient:  0.1552736746811101
iteration : 4178
train acc:  0.796875
train loss:  0.4620509445667267
train gradient:  0.21867703887280932
iteration : 4179
train acc:  0.71875
train loss:  0.55257248878479
train gradient:  0.13708652114807593
iteration : 4180
train acc:  0.6953125
train loss:  0.5512838959693909
train gradient:  0.1638688690108599
iteration : 4181
train acc:  0.734375
train loss:  0.5005385279655457
train gradient:  0.2221937054804682
iteration : 4182
train acc:  0.765625
train loss:  0.4482143223285675
train gradient:  0.1754203244829044
iteration : 4183
train acc:  0.625
train loss:  0.6197875738143921
train gradient:  0.19419687065702218
iteration : 4184
train acc:  0.71875
train loss:  0.5349067449569702
train gradient:  0.1735317176612285
iteration : 4185
train acc:  0.734375
train loss:  0.5416947603225708
train gradient:  0.14949587005906867
iteration : 4186
train acc:  0.734375
train loss:  0.5487393736839294
train gradient:  0.15062531036903337
iteration : 4187
train acc:  0.6484375
train loss:  0.5680343508720398
train gradient:  0.13739751778599063
iteration : 4188
train acc:  0.7578125
train loss:  0.48950308561325073
train gradient:  0.15300062482785134
iteration : 4189
train acc:  0.75
train loss:  0.489408940076828
train gradient:  0.1188014298772978
iteration : 4190
train acc:  0.640625
train loss:  0.5875837206840515
train gradient:  0.22316901823336474
iteration : 4191
train acc:  0.7421875
train loss:  0.4679605960845947
train gradient:  0.1215356686100407
iteration : 4192
train acc:  0.7734375
train loss:  0.47720789909362793
train gradient:  0.12693845893988787
iteration : 4193
train acc:  0.765625
train loss:  0.49950718879699707
train gradient:  0.15330377182030824
iteration : 4194
train acc:  0.6875
train loss:  0.5356296300888062
train gradient:  0.15163863084004686
iteration : 4195
train acc:  0.6953125
train loss:  0.5312910079956055
train gradient:  0.15115448023782985
iteration : 4196
train acc:  0.734375
train loss:  0.5068463087081909
train gradient:  0.15210981849428168
iteration : 4197
train acc:  0.75
train loss:  0.5197994112968445
train gradient:  0.1924578736250176
iteration : 4198
train acc:  0.7421875
train loss:  0.5214530229568481
train gradient:  0.15792110245193847
iteration : 4199
train acc:  0.7421875
train loss:  0.4830418527126312
train gradient:  0.15670611708994425
iteration : 4200
train acc:  0.78125
train loss:  0.4629829227924347
train gradient:  0.18325131568060055
iteration : 4201
train acc:  0.71875
train loss:  0.4838297367095947
train gradient:  0.12672859316638907
iteration : 4202
train acc:  0.7421875
train loss:  0.5111503601074219
train gradient:  0.1765544975327608
iteration : 4203
train acc:  0.7265625
train loss:  0.5591981410980225
train gradient:  0.14253723838588409
iteration : 4204
train acc:  0.703125
train loss:  0.5165829062461853
train gradient:  0.16047371895796508
iteration : 4205
train acc:  0.7109375
train loss:  0.49175113439559937
train gradient:  0.17169862469815356
iteration : 4206
train acc:  0.7265625
train loss:  0.47956690192222595
train gradient:  0.12052359549945868
iteration : 4207
train acc:  0.7109375
train loss:  0.5593783259391785
train gradient:  0.14477042517238098
iteration : 4208
train acc:  0.703125
train loss:  0.5513992309570312
train gradient:  0.16123235447126283
iteration : 4209
train acc:  0.6796875
train loss:  0.5520622730255127
train gradient:  0.1778360396332347
iteration : 4210
train acc:  0.6953125
train loss:  0.5222035050392151
train gradient:  0.1551953023069587
iteration : 4211
train acc:  0.6875
train loss:  0.5463724732398987
train gradient:  0.16925257176577996
iteration : 4212
train acc:  0.765625
train loss:  0.46377113461494446
train gradient:  0.13145653135846047
iteration : 4213
train acc:  0.640625
train loss:  0.5970015525817871
train gradient:  0.27645976840892955
iteration : 4214
train acc:  0.71875
train loss:  0.48347923159599304
train gradient:  0.13377554272642322
iteration : 4215
train acc:  0.6953125
train loss:  0.5601646304130554
train gradient:  0.144150948332414
iteration : 4216
train acc:  0.6796875
train loss:  0.5311168432235718
train gradient:  0.15536617808443454
iteration : 4217
train acc:  0.6640625
train loss:  0.5627042055130005
train gradient:  0.1668298453329798
iteration : 4218
train acc:  0.7890625
train loss:  0.49656736850738525
train gradient:  0.12837579071520633
iteration : 4219
train acc:  0.734375
train loss:  0.486189603805542
train gradient:  0.1481374954614158
iteration : 4220
train acc:  0.7890625
train loss:  0.45926937460899353
train gradient:  0.13976130462149938
iteration : 4221
train acc:  0.7265625
train loss:  0.5289828181266785
train gradient:  0.14770503005209043
iteration : 4222
train acc:  0.65625
train loss:  0.5653236508369446
train gradient:  0.20256124564409111
iteration : 4223
train acc:  0.6875
train loss:  0.5520569086074829
train gradient:  0.1612157104774739
iteration : 4224
train acc:  0.7109375
train loss:  0.5822867155075073
train gradient:  0.19061387005608163
iteration : 4225
train acc:  0.7578125
train loss:  0.5546941161155701
train gradient:  0.17906487036982094
iteration : 4226
train acc:  0.7265625
train loss:  0.5061527490615845
train gradient:  0.14823838787586463
iteration : 4227
train acc:  0.65625
train loss:  0.5648014545440674
train gradient:  0.15760580501428223
iteration : 4228
train acc:  0.703125
train loss:  0.5686721801757812
train gradient:  0.15547494162386533
iteration : 4229
train acc:  0.671875
train loss:  0.6190522909164429
train gradient:  0.2476819989403214
iteration : 4230
train acc:  0.65625
train loss:  0.5678758025169373
train gradient:  0.16387259500030804
iteration : 4231
train acc:  0.71875
train loss:  0.49008941650390625
train gradient:  0.14405476268727363
iteration : 4232
train acc:  0.7109375
train loss:  0.5292379856109619
train gradient:  0.18997882980074582
iteration : 4233
train acc:  0.78125
train loss:  0.46703338623046875
train gradient:  0.16351955452822603
iteration : 4234
train acc:  0.7578125
train loss:  0.46648404002189636
train gradient:  0.14326060305674276
iteration : 4235
train acc:  0.734375
train loss:  0.5009386539459229
train gradient:  0.15139428265508137
iteration : 4236
train acc:  0.6953125
train loss:  0.585900068283081
train gradient:  0.1830189878303287
iteration : 4237
train acc:  0.734375
train loss:  0.5491111278533936
train gradient:  0.22466353530417355
iteration : 4238
train acc:  0.75
train loss:  0.5619670152664185
train gradient:  0.16020132500398326
iteration : 4239
train acc:  0.703125
train loss:  0.5586647987365723
train gradient:  0.15791092291254966
iteration : 4240
train acc:  0.7109375
train loss:  0.5361324548721313
train gradient:  0.17347377382877452
iteration : 4241
train acc:  0.6953125
train loss:  0.5524507761001587
train gradient:  0.15482615596553395
iteration : 4242
train acc:  0.7265625
train loss:  0.48935842514038086
train gradient:  0.13656970520914768
iteration : 4243
train acc:  0.7890625
train loss:  0.4701159596443176
train gradient:  0.15609070958867016
iteration : 4244
train acc:  0.71875
train loss:  0.4964166581630707
train gradient:  0.12691977218408618
iteration : 4245
train acc:  0.7265625
train loss:  0.5272768139839172
train gradient:  0.14981570752099094
iteration : 4246
train acc:  0.7578125
train loss:  0.5364426970481873
train gradient:  0.14876616483840222
iteration : 4247
train acc:  0.7578125
train loss:  0.49085456132888794
train gradient:  0.13875709012603687
iteration : 4248
train acc:  0.75
train loss:  0.47661933302879333
train gradient:  0.14571393595359322
iteration : 4249
train acc:  0.6953125
train loss:  0.5408024191856384
train gradient:  0.21644836906783887
iteration : 4250
train acc:  0.6875
train loss:  0.6128731966018677
train gradient:  0.21356310844328238
iteration : 4251
train acc:  0.7578125
train loss:  0.5376192927360535
train gradient:  0.15324307877839985
iteration : 4252
train acc:  0.71875
train loss:  0.580088198184967
train gradient:  0.22333765549769316
iteration : 4253
train acc:  0.6953125
train loss:  0.5722728967666626
train gradient:  0.17218468306605333
iteration : 4254
train acc:  0.71875
train loss:  0.5633661150932312
train gradient:  0.19708910436568514
iteration : 4255
train acc:  0.7109375
train loss:  0.552564263343811
train gradient:  0.16370777693314542
iteration : 4256
train acc:  0.7734375
train loss:  0.5142244100570679
train gradient:  0.2066034045710155
iteration : 4257
train acc:  0.7109375
train loss:  0.5223785042762756
train gradient:  0.13212855996634176
iteration : 4258
train acc:  0.734375
train loss:  0.5434294939041138
train gradient:  0.1449346393045849
iteration : 4259
train acc:  0.6484375
train loss:  0.6585231423377991
train gradient:  0.22425115044452543
iteration : 4260
train acc:  0.75
train loss:  0.4873904287815094
train gradient:  0.13315710846916368
iteration : 4261
train acc:  0.765625
train loss:  0.4932204782962799
train gradient:  0.19404140714908535
iteration : 4262
train acc:  0.6796875
train loss:  0.5376909375190735
train gradient:  0.17297089370555724
iteration : 4263
train acc:  0.71875
train loss:  0.4972307085990906
train gradient:  0.11827470955962388
iteration : 4264
train acc:  0.7421875
train loss:  0.47519707679748535
train gradient:  0.1116750039950549
iteration : 4265
train acc:  0.7421875
train loss:  0.527367353439331
train gradient:  0.1360607441081412
iteration : 4266
train acc:  0.765625
train loss:  0.5633964538574219
train gradient:  0.11400085133644983
iteration : 4267
train acc:  0.7890625
train loss:  0.45554450154304504
train gradient:  0.10674438640826721
iteration : 4268
train acc:  0.6796875
train loss:  0.5450975298881531
train gradient:  0.16746901543638731
iteration : 4269
train acc:  0.6640625
train loss:  0.5710762739181519
train gradient:  0.16570159662496986
iteration : 4270
train acc:  0.671875
train loss:  0.5595952272415161
train gradient:  0.19777154526756413
iteration : 4271
train acc:  0.7421875
train loss:  0.48901620507240295
train gradient:  0.14096392266353763
iteration : 4272
train acc:  0.671875
train loss:  0.578284740447998
train gradient:  0.14839955118325218
iteration : 4273
train acc:  0.671875
train loss:  0.5481533408164978
train gradient:  0.17699382170368852
iteration : 4274
train acc:  0.78125
train loss:  0.525688648223877
train gradient:  0.16322435619044062
iteration : 4275
train acc:  0.78125
train loss:  0.4977818727493286
train gradient:  0.13673455635950893
iteration : 4276
train acc:  0.6640625
train loss:  0.594849169254303
train gradient:  0.18461929166557603
iteration : 4277
train acc:  0.78125
train loss:  0.4683411717414856
train gradient:  0.14705289425173568
iteration : 4278
train acc:  0.765625
train loss:  0.4798583388328552
train gradient:  0.11769279254428255
iteration : 4279
train acc:  0.71875
train loss:  0.5233652591705322
train gradient:  0.14084143244049124
iteration : 4280
train acc:  0.75
train loss:  0.533546507358551
train gradient:  0.20157005022843238
iteration : 4281
train acc:  0.7265625
train loss:  0.49207139015197754
train gradient:  0.1273661739847694
iteration : 4282
train acc:  0.7109375
train loss:  0.5021893382072449
train gradient:  0.10797449832243818
iteration : 4283
train acc:  0.734375
train loss:  0.5314927101135254
train gradient:  0.1812608504693921
iteration : 4284
train acc:  0.71875
train loss:  0.48634761571884155
train gradient:  0.15275371791629822
iteration : 4285
train acc:  0.7421875
train loss:  0.5054422616958618
train gradient:  0.1529459028602353
iteration : 4286
train acc:  0.703125
train loss:  0.5089969038963318
train gradient:  0.1498390401401044
iteration : 4287
train acc:  0.65625
train loss:  0.6136513948440552
train gradient:  0.17483483735203154
iteration : 4288
train acc:  0.6875
train loss:  0.5507301688194275
train gradient:  0.15831036871398893
iteration : 4289
train acc:  0.6484375
train loss:  0.6017487645149231
train gradient:  0.16447627889445893
iteration : 4290
train acc:  0.71875
train loss:  0.48000138998031616
train gradient:  0.125381714008504
iteration : 4291
train acc:  0.75
train loss:  0.50941002368927
train gradient:  0.1798864053352781
iteration : 4292
train acc:  0.65625
train loss:  0.582040011882782
train gradient:  0.20844467447471737
iteration : 4293
train acc:  0.7578125
train loss:  0.4952184557914734
train gradient:  0.11673451257900033
iteration : 4294
train acc:  0.71875
train loss:  0.554154098033905
train gradient:  0.1520910555042515
iteration : 4295
train acc:  0.703125
train loss:  0.5488607287406921
train gradient:  0.1512984010665072
iteration : 4296
train acc:  0.7734375
train loss:  0.4903191924095154
train gradient:  0.1296992617646112
iteration : 4297
train acc:  0.6640625
train loss:  0.5212630033493042
train gradient:  0.14005778121122764
iteration : 4298
train acc:  0.7109375
train loss:  0.5229836702346802
train gradient:  0.16646093363088332
iteration : 4299
train acc:  0.7265625
train loss:  0.5053038597106934
train gradient:  0.16721753536282205
iteration : 4300
train acc:  0.6953125
train loss:  0.542008638381958
train gradient:  0.1318904958615124
iteration : 4301
train acc:  0.6328125
train loss:  0.6434981226921082
train gradient:  0.24496918646863713
iteration : 4302
train acc:  0.6875
train loss:  0.5455144643783569
train gradient:  0.16924806684964766
iteration : 4303
train acc:  0.6015625
train loss:  0.5962627530097961
train gradient:  0.2224622364787781
iteration : 4304
train acc:  0.640625
train loss:  0.6014885902404785
train gradient:  0.1973653363933654
iteration : 4305
train acc:  0.7109375
train loss:  0.5263126492500305
train gradient:  0.13993031734989259
iteration : 4306
train acc:  0.7890625
train loss:  0.4727489948272705
train gradient:  0.14557272184608414
iteration : 4307
train acc:  0.7109375
train loss:  0.5488399267196655
train gradient:  0.14113922937545348
iteration : 4308
train acc:  0.6875
train loss:  0.5574777126312256
train gradient:  0.15481355699106394
iteration : 4309
train acc:  0.671875
train loss:  0.5663349032402039
train gradient:  0.1561405531410774
iteration : 4310
train acc:  0.7578125
train loss:  0.455303430557251
train gradient:  0.2029873042968892
iteration : 4311
train acc:  0.640625
train loss:  0.5611645579338074
train gradient:  0.17863197896492705
iteration : 4312
train acc:  0.7421875
train loss:  0.4923925995826721
train gradient:  0.17301819253782835
iteration : 4313
train acc:  0.7265625
train loss:  0.5889958143234253
train gradient:  0.20941679029630583
iteration : 4314
train acc:  0.6953125
train loss:  0.5360198020935059
train gradient:  0.13492434055337432
iteration : 4315
train acc:  0.7734375
train loss:  0.49058690667152405
train gradient:  0.17188585669301298
iteration : 4316
train acc:  0.6953125
train loss:  0.5517715215682983
train gradient:  0.2007720450357362
iteration : 4317
train acc:  0.6796875
train loss:  0.5398111343383789
train gradient:  0.19762678059311378
iteration : 4318
train acc:  0.703125
train loss:  0.5245038270950317
train gradient:  0.20960710332562996
iteration : 4319
train acc:  0.75
train loss:  0.5006144046783447
train gradient:  0.11656823317292596
iteration : 4320
train acc:  0.765625
train loss:  0.5038114190101624
train gradient:  0.13236597961891938
iteration : 4321
train acc:  0.7421875
train loss:  0.47628989815711975
train gradient:  0.11540154286725157
iteration : 4322
train acc:  0.7109375
train loss:  0.5047576427459717
train gradient:  0.13064062612154848
iteration : 4323
train acc:  0.6328125
train loss:  0.5745304822921753
train gradient:  0.18941309100255754
iteration : 4324
train acc:  0.7734375
train loss:  0.4570143520832062
train gradient:  0.13564943874859037
iteration : 4325
train acc:  0.765625
train loss:  0.4733947515487671
train gradient:  0.1739320740076558
iteration : 4326
train acc:  0.6796875
train loss:  0.5796366930007935
train gradient:  0.1817732995640846
iteration : 4327
train acc:  0.7421875
train loss:  0.5300954580307007
train gradient:  0.15731215089123712
iteration : 4328
train acc:  0.7109375
train loss:  0.5178248286247253
train gradient:  0.12921798551697522
iteration : 4329
train acc:  0.734375
train loss:  0.5263881683349609
train gradient:  0.17636317270889296
iteration : 4330
train acc:  0.734375
train loss:  0.4906729459762573
train gradient:  0.13890488355815728
iteration : 4331
train acc:  0.765625
train loss:  0.5041540265083313
train gradient:  0.1649514052701378
iteration : 4332
train acc:  0.75
train loss:  0.5157897472381592
train gradient:  0.15549831072301223
iteration : 4333
train acc:  0.6875
train loss:  0.5639040470123291
train gradient:  0.19970602775185042
iteration : 4334
train acc:  0.71875
train loss:  0.4970484673976898
train gradient:  0.1322820083424776
iteration : 4335
train acc:  0.6953125
train loss:  0.5392738580703735
train gradient:  0.16062150307892753
iteration : 4336
train acc:  0.7265625
train loss:  0.5512653589248657
train gradient:  0.15700682976266558
iteration : 4337
train acc:  0.7421875
train loss:  0.539985179901123
train gradient:  0.16251369703680357
iteration : 4338
train acc:  0.6953125
train loss:  0.5464591979980469
train gradient:  0.19416402079204448
iteration : 4339
train acc:  0.703125
train loss:  0.5225821137428284
train gradient:  0.1418376136406475
iteration : 4340
train acc:  0.6875
train loss:  0.5504830479621887
train gradient:  0.19955297826231605
iteration : 4341
train acc:  0.765625
train loss:  0.5280839204788208
train gradient:  0.2098300821423701
iteration : 4342
train acc:  0.7421875
train loss:  0.5448893308639526
train gradient:  0.19349627268366243
iteration : 4343
train acc:  0.703125
train loss:  0.5395668745040894
train gradient:  0.16660422969753536
iteration : 4344
train acc:  0.765625
train loss:  0.4690924882888794
train gradient:  0.15332861339047993
iteration : 4345
train acc:  0.7109375
train loss:  0.5462197065353394
train gradient:  0.15781500563341255
iteration : 4346
train acc:  0.703125
train loss:  0.5738744735717773
train gradient:  0.20121851003408747
iteration : 4347
train acc:  0.6796875
train loss:  0.563666582107544
train gradient:  0.13474808535305668
iteration : 4348
train acc:  0.6796875
train loss:  0.5539272427558899
train gradient:  0.2748765910901458
iteration : 4349
train acc:  0.7734375
train loss:  0.49894288182258606
train gradient:  0.18947875598242847
iteration : 4350
train acc:  0.75
train loss:  0.48547500371932983
train gradient:  0.13487963058485658
iteration : 4351
train acc:  0.7421875
train loss:  0.5010116696357727
train gradient:  0.13208823599219935
iteration : 4352
train acc:  0.796875
train loss:  0.46964913606643677
train gradient:  0.12340012888856718
iteration : 4353
train acc:  0.734375
train loss:  0.5166807174682617
train gradient:  0.1256027603602237
iteration : 4354
train acc:  0.734375
train loss:  0.510957658290863
train gradient:  0.13410077708078105
iteration : 4355
train acc:  0.734375
train loss:  0.4977148473262787
train gradient:  0.16202620753752178
iteration : 4356
train acc:  0.671875
train loss:  0.522804856300354
train gradient:  0.17101611558328583
iteration : 4357
train acc:  0.796875
train loss:  0.4775007665157318
train gradient:  0.13979082997174075
iteration : 4358
train acc:  0.765625
train loss:  0.4754529595375061
train gradient:  0.15759072696989132
iteration : 4359
train acc:  0.765625
train loss:  0.5101155042648315
train gradient:  0.15834813237161388
iteration : 4360
train acc:  0.7890625
train loss:  0.4590960443019867
train gradient:  0.14171499825811099
iteration : 4361
train acc:  0.765625
train loss:  0.48206019401550293
train gradient:  0.14739206033128605
iteration : 4362
train acc:  0.671875
train loss:  0.532990038394928
train gradient:  0.17401884717184
iteration : 4363
train acc:  0.734375
train loss:  0.5291910171508789
train gradient:  0.1211135704757639
iteration : 4364
train acc:  0.6953125
train loss:  0.5091292858123779
train gradient:  0.1776212368612926
iteration : 4365
train acc:  0.734375
train loss:  0.532916784286499
train gradient:  0.13121366605937823
iteration : 4366
train acc:  0.671875
train loss:  0.5831582546234131
train gradient:  0.17174453188111782
iteration : 4367
train acc:  0.6953125
train loss:  0.5911948084831238
train gradient:  0.1973332319476164
iteration : 4368
train acc:  0.7578125
train loss:  0.5029513835906982
train gradient:  0.13602871290902158
iteration : 4369
train acc:  0.7578125
train loss:  0.507206916809082
train gradient:  0.20772446801525624
iteration : 4370
train acc:  0.78125
train loss:  0.4517747163772583
train gradient:  0.12685655669005924
iteration : 4371
train acc:  0.765625
train loss:  0.4651993215084076
train gradient:  0.1078967684739756
iteration : 4372
train acc:  0.7109375
train loss:  0.510197639465332
train gradient:  0.2056577820536312
iteration : 4373
train acc:  0.703125
train loss:  0.4763484001159668
train gradient:  0.1583243040211334
iteration : 4374
train acc:  0.6875
train loss:  0.5680375099182129
train gradient:  0.17502728296059256
iteration : 4375
train acc:  0.734375
train loss:  0.5151910185813904
train gradient:  0.1873763262446524
iteration : 4376
train acc:  0.734375
train loss:  0.47697293758392334
train gradient:  0.15425698948115585
iteration : 4377
train acc:  0.6640625
train loss:  0.5718268156051636
train gradient:  0.19981734388346256
iteration : 4378
train acc:  0.703125
train loss:  0.5156299471855164
train gradient:  0.16632258912198292
iteration : 4379
train acc:  0.8046875
train loss:  0.4545632004737854
train gradient:  0.12313694872331606
iteration : 4380
train acc:  0.6953125
train loss:  0.6000431180000305
train gradient:  0.1659708526450066
iteration : 4381
train acc:  0.6875
train loss:  0.5455683469772339
train gradient:  0.14702173969117965
iteration : 4382
train acc:  0.7109375
train loss:  0.501210629940033
train gradient:  0.11026657719902799
iteration : 4383
train acc:  0.671875
train loss:  0.5484796762466431
train gradient:  0.24433957537421092
iteration : 4384
train acc:  0.6796875
train loss:  0.5152630805969238
train gradient:  0.13671935631390614
iteration : 4385
train acc:  0.8046875
train loss:  0.4847308099269867
train gradient:  0.12140264447892792
iteration : 4386
train acc:  0.6875
train loss:  0.563495397567749
train gradient:  0.18213913540685817
iteration : 4387
train acc:  0.734375
train loss:  0.5239743590354919
train gradient:  0.13215817307547753
iteration : 4388
train acc:  0.765625
train loss:  0.4587881863117218
train gradient:  0.12208160459292595
iteration : 4389
train acc:  0.6953125
train loss:  0.5219972729682922
train gradient:  0.17955333291319553
iteration : 4390
train acc:  0.75
train loss:  0.4969666600227356
train gradient:  0.1437415451000335
iteration : 4391
train acc:  0.6796875
train loss:  0.5350502729415894
train gradient:  0.13617750149162597
iteration : 4392
train acc:  0.6875
train loss:  0.5816834568977356
train gradient:  0.2584822929454853
iteration : 4393
train acc:  0.71875
train loss:  0.517268717288971
train gradient:  0.20821688182051112
iteration : 4394
train acc:  0.7265625
train loss:  0.49473461508750916
train gradient:  0.13874906168154755
iteration : 4395
train acc:  0.6953125
train loss:  0.5288842916488647
train gradient:  0.15886969155440617
iteration : 4396
train acc:  0.734375
train loss:  0.4863601326942444
train gradient:  0.13191312548159495
iteration : 4397
train acc:  0.7734375
train loss:  0.48779749870300293
train gradient:  0.12488611680425461
iteration : 4398
train acc:  0.7421875
train loss:  0.4614276885986328
train gradient:  0.13657881222236967
iteration : 4399
train acc:  0.6796875
train loss:  0.5469605922698975
train gradient:  0.21154423526801358
iteration : 4400
train acc:  0.7265625
train loss:  0.5088929533958435
train gradient:  0.130905773189178
iteration : 4401
train acc:  0.703125
train loss:  0.5698099136352539
train gradient:  0.20339702631382583
iteration : 4402
train acc:  0.6875
train loss:  0.5360566973686218
train gradient:  0.16422473702743606
iteration : 4403
train acc:  0.7109375
train loss:  0.5511379837989807
train gradient:  0.2039008671926814
iteration : 4404
train acc:  0.6328125
train loss:  0.5634905695915222
train gradient:  0.19548220650449388
iteration : 4405
train acc:  0.7265625
train loss:  0.5114575624465942
train gradient:  0.1117938795915291
iteration : 4406
train acc:  0.75
train loss:  0.5244124531745911
train gradient:  0.14794565121974412
iteration : 4407
train acc:  0.7265625
train loss:  0.5076054334640503
train gradient:  0.17756734163077614
iteration : 4408
train acc:  0.765625
train loss:  0.46188777685165405
train gradient:  0.14613401707755225
iteration : 4409
train acc:  0.71875
train loss:  0.5103837251663208
train gradient:  0.14730581173518487
iteration : 4410
train acc:  0.7734375
train loss:  0.45615053176879883
train gradient:  0.12145543147121345
iteration : 4411
train acc:  0.7265625
train loss:  0.5181647539138794
train gradient:  0.18455635146859084
iteration : 4412
train acc:  0.7109375
train loss:  0.551903486251831
train gradient:  0.1397854957944682
iteration : 4413
train acc:  0.7578125
train loss:  0.5072760581970215
train gradient:  0.14513104366258822
iteration : 4414
train acc:  0.6640625
train loss:  0.5926622748374939
train gradient:  0.1995551682188692
iteration : 4415
train acc:  0.75
train loss:  0.5256360769271851
train gradient:  0.17530782115385404
iteration : 4416
train acc:  0.71875
train loss:  0.5392934083938599
train gradient:  0.18626482551535645
iteration : 4417
train acc:  0.703125
train loss:  0.5379592776298523
train gradient:  0.1714197545838964
iteration : 4418
train acc:  0.671875
train loss:  0.5659440159797668
train gradient:  0.18993892465760273
iteration : 4419
train acc:  0.71875
train loss:  0.5040494799613953
train gradient:  0.1382638793219889
iteration : 4420
train acc:  0.671875
train loss:  0.5950750112533569
train gradient:  0.16730605439317975
iteration : 4421
train acc:  0.703125
train loss:  0.5495486259460449
train gradient:  0.14961821968742423
iteration : 4422
train acc:  0.71875
train loss:  0.5080946683883667
train gradient:  0.13761646542095038
iteration : 4423
train acc:  0.734375
train loss:  0.5423856377601624
train gradient:  0.14156395660635518
iteration : 4424
train acc:  0.734375
train loss:  0.5169767737388611
train gradient:  0.13389169618333197
iteration : 4425
train acc:  0.71875
train loss:  0.5460027456283569
train gradient:  0.1507563538054636
iteration : 4426
train acc:  0.640625
train loss:  0.5647442936897278
train gradient:  0.16335319419320565
iteration : 4427
train acc:  0.7109375
train loss:  0.525991678237915
train gradient:  0.21474522425497328
iteration : 4428
train acc:  0.7734375
train loss:  0.5228923559188843
train gradient:  0.18843610640585623
iteration : 4429
train acc:  0.671875
train loss:  0.5514489412307739
train gradient:  0.19214844802699282
iteration : 4430
train acc:  0.7578125
train loss:  0.5109032392501831
train gradient:  0.16672209732240229
iteration : 4431
train acc:  0.796875
train loss:  0.4319390058517456
train gradient:  0.10852438315106988
iteration : 4432
train acc:  0.8046875
train loss:  0.45713260769844055
train gradient:  0.15835316311735897
iteration : 4433
train acc:  0.7265625
train loss:  0.4807444214820862
train gradient:  0.19308740849807476
iteration : 4434
train acc:  0.734375
train loss:  0.5509425401687622
train gradient:  0.13724836424478568
iteration : 4435
train acc:  0.71875
train loss:  0.4846315383911133
train gradient:  0.14635903996660768
iteration : 4436
train acc:  0.71875
train loss:  0.5126240849494934
train gradient:  0.13925598004093154
iteration : 4437
train acc:  0.7265625
train loss:  0.500200629234314
train gradient:  0.12034684219831852
iteration : 4438
train acc:  0.7578125
train loss:  0.5300836563110352
train gradient:  0.1732979127999787
iteration : 4439
train acc:  0.734375
train loss:  0.5418749451637268
train gradient:  0.1483641300083715
iteration : 4440
train acc:  0.671875
train loss:  0.5913380980491638
train gradient:  0.15306125657124275
iteration : 4441
train acc:  0.7421875
train loss:  0.5141635537147522
train gradient:  0.12523909239858566
iteration : 4442
train acc:  0.7578125
train loss:  0.5136375427246094
train gradient:  0.1367088104151058
iteration : 4443
train acc:  0.7421875
train loss:  0.4930323362350464
train gradient:  0.1735031974303059
iteration : 4444
train acc:  0.734375
train loss:  0.4757150113582611
train gradient:  0.13158974133349782
iteration : 4445
train acc:  0.7734375
train loss:  0.5085930824279785
train gradient:  0.1254213074273347
iteration : 4446
train acc:  0.703125
train loss:  0.5493290424346924
train gradient:  0.203857371553299
iteration : 4447
train acc:  0.7734375
train loss:  0.5166935920715332
train gradient:  0.16726199355659038
iteration : 4448
train acc:  0.8359375
train loss:  0.4290863871574402
train gradient:  0.1202702863901328
iteration : 4449
train acc:  0.7265625
train loss:  0.5298919677734375
train gradient:  0.15689457097090376
iteration : 4450
train acc:  0.703125
train loss:  0.4990498125553131
train gradient:  0.14029226871235945
iteration : 4451
train acc:  0.7421875
train loss:  0.49992188811302185
train gradient:  0.15849956358337441
iteration : 4452
train acc:  0.7578125
train loss:  0.5288236141204834
train gradient:  0.18686642591316815
iteration : 4453
train acc:  0.6484375
train loss:  0.626655101776123
train gradient:  0.22991309348176958
iteration : 4454
train acc:  0.6875
train loss:  0.6177784204483032
train gradient:  0.263470226974748
iteration : 4455
train acc:  0.7421875
train loss:  0.514856219291687
train gradient:  0.1876558018565868
iteration : 4456
train acc:  0.65625
train loss:  0.5502730011940002
train gradient:  0.18900414121352943
iteration : 4457
train acc:  0.765625
train loss:  0.5107630491256714
train gradient:  0.1225401163608339
iteration : 4458
train acc:  0.6953125
train loss:  0.5285612344741821
train gradient:  0.16576119135101092
iteration : 4459
train acc:  0.75
train loss:  0.468730092048645
train gradient:  0.14120323406579582
iteration : 4460
train acc:  0.6796875
train loss:  0.5169780254364014
train gradient:  0.18940805366549307
iteration : 4461
train acc:  0.7421875
train loss:  0.5332466959953308
train gradient:  0.13524754291654284
iteration : 4462
train acc:  0.671875
train loss:  0.5542215704917908
train gradient:  0.1678946633979807
iteration : 4463
train acc:  0.6796875
train loss:  0.5479850769042969
train gradient:  0.19307949959488596
iteration : 4464
train acc:  0.78125
train loss:  0.4366235136985779
train gradient:  0.10542709784035724
iteration : 4465
train acc:  0.7109375
train loss:  0.5055381655693054
train gradient:  0.15532305111868916
iteration : 4466
train acc:  0.640625
train loss:  0.5811343789100647
train gradient:  0.1641676349557939
iteration : 4467
train acc:  0.765625
train loss:  0.5184426307678223
train gradient:  0.19970839795701717
iteration : 4468
train acc:  0.75
train loss:  0.4862865209579468
train gradient:  0.15798111454304306
iteration : 4469
train acc:  0.65625
train loss:  0.5862955451011658
train gradient:  0.2123265470718258
iteration : 4470
train acc:  0.7265625
train loss:  0.5038771033287048
train gradient:  0.13978680619947956
iteration : 4471
train acc:  0.6875
train loss:  0.5634607076644897
train gradient:  0.15918055280242133
iteration : 4472
train acc:  0.7265625
train loss:  0.5465560555458069
train gradient:  0.19316883445765587
iteration : 4473
train acc:  0.703125
train loss:  0.5212591886520386
train gradient:  0.14712109194210388
iteration : 4474
train acc:  0.7890625
train loss:  0.4824165105819702
train gradient:  0.1298426537545557
iteration : 4475
train acc:  0.7109375
train loss:  0.5274025201797485
train gradient:  0.1710120937061381
iteration : 4476
train acc:  0.71875
train loss:  0.4808639585971832
train gradient:  0.13009525089459067
iteration : 4477
train acc:  0.671875
train loss:  0.5145589113235474
train gradient:  0.15047018287064307
iteration : 4478
train acc:  0.7421875
train loss:  0.5033514499664307
train gradient:  0.13228064930355976
iteration : 4479
train acc:  0.7265625
train loss:  0.5130782127380371
train gradient:  0.16102887205132255
iteration : 4480
train acc:  0.703125
train loss:  0.557327151298523
train gradient:  0.27510276328021654
iteration : 4481
train acc:  0.78125
train loss:  0.42172402143478394
train gradient:  0.09973647486800923
iteration : 4482
train acc:  0.734375
train loss:  0.5184183120727539
train gradient:  0.13353845247810525
iteration : 4483
train acc:  0.7109375
train loss:  0.5772514343261719
train gradient:  0.265570594892661
iteration : 4484
train acc:  0.75
train loss:  0.4643338918685913
train gradient:  0.14088034822713696
iteration : 4485
train acc:  0.6796875
train loss:  0.5366396307945251
train gradient:  0.18694731315687352
iteration : 4486
train acc:  0.6796875
train loss:  0.5284570455551147
train gradient:  0.17514228443992041
iteration : 4487
train acc:  0.6953125
train loss:  0.5692012310028076
train gradient:  0.1565793458627838
iteration : 4488
train acc:  0.7421875
train loss:  0.527625560760498
train gradient:  0.15623857893711174
iteration : 4489
train acc:  0.7109375
train loss:  0.5022227764129639
train gradient:  0.15220487226476412
iteration : 4490
train acc:  0.640625
train loss:  0.5847676992416382
train gradient:  0.2045461516927307
iteration : 4491
train acc:  0.7734375
train loss:  0.48189038038253784
train gradient:  0.18259207507703745
iteration : 4492
train acc:  0.6796875
train loss:  0.590552806854248
train gradient:  0.2602584888115558
iteration : 4493
train acc:  0.7734375
train loss:  0.4810759723186493
train gradient:  0.14376592917481057
iteration : 4494
train acc:  0.734375
train loss:  0.46261268854141235
train gradient:  0.1348138202799104
iteration : 4495
train acc:  0.734375
train loss:  0.5317886471748352
train gradient:  0.14179266998168355
iteration : 4496
train acc:  0.75
train loss:  0.48756474256515503
train gradient:  0.1341646843472015
iteration : 4497
train acc:  0.765625
train loss:  0.492291659116745
train gradient:  0.16473745351650343
iteration : 4498
train acc:  0.75
train loss:  0.48892736434936523
train gradient:  0.14563045957733384
iteration : 4499
train acc:  0.6640625
train loss:  0.5422012805938721
train gradient:  0.1262516133610448
iteration : 4500
train acc:  0.671875
train loss:  0.5692147612571716
train gradient:  0.1704937539524024
iteration : 4501
train acc:  0.7265625
train loss:  0.5262974500656128
train gradient:  0.15620963562844098
iteration : 4502
train acc:  0.796875
train loss:  0.4501503109931946
train gradient:  0.12566248072381078
iteration : 4503
train acc:  0.71875
train loss:  0.5253369212150574
train gradient:  0.187572183685212
iteration : 4504
train acc:  0.734375
train loss:  0.5109008550643921
train gradient:  0.1453215363688144
iteration : 4505
train acc:  0.734375
train loss:  0.5307680368423462
train gradient:  0.18023962157665824
iteration : 4506
train acc:  0.765625
train loss:  0.4884149432182312
train gradient:  0.15083934004718164
iteration : 4507
train acc:  0.6953125
train loss:  0.5175891518592834
train gradient:  0.1639456651902952
iteration : 4508
train acc:  0.7421875
train loss:  0.4831531345844269
train gradient:  0.12604579428594304
iteration : 4509
train acc:  0.703125
train loss:  0.5211393237113953
train gradient:  0.18187348871567693
iteration : 4510
train acc:  0.734375
train loss:  0.4758818447589874
train gradient:  0.14154041702781167
iteration : 4511
train acc:  0.6796875
train loss:  0.5961383581161499
train gradient:  0.3483201573169744
iteration : 4512
train acc:  0.71875
train loss:  0.534315824508667
train gradient:  0.1937117789764795
iteration : 4513
train acc:  0.703125
train loss:  0.5432837009429932
train gradient:  0.1748967424873652
iteration : 4514
train acc:  0.703125
train loss:  0.6040087938308716
train gradient:  0.1996878516856514
iteration : 4515
train acc:  0.7265625
train loss:  0.5416550040245056
train gradient:  0.15565820263014846
iteration : 4516
train acc:  0.796875
train loss:  0.43730002641677856
train gradient:  0.11246484084761707
iteration : 4517
train acc:  0.7109375
train loss:  0.4956027865409851
train gradient:  0.12827588824503802
iteration : 4518
train acc:  0.7421875
train loss:  0.4892032742500305
train gradient:  0.21982676792588293
iteration : 4519
train acc:  0.796875
train loss:  0.4942471385002136
train gradient:  0.16921140978222804
iteration : 4520
train acc:  0.703125
train loss:  0.5104944109916687
train gradient:  0.17236556850329277
iteration : 4521
train acc:  0.7578125
train loss:  0.4942920207977295
train gradient:  0.12043647434412971
iteration : 4522
train acc:  0.6953125
train loss:  0.5736305117607117
train gradient:  0.14590832786931163
iteration : 4523
train acc:  0.6953125
train loss:  0.5923138856887817
train gradient:  0.2707409721488865
iteration : 4524
train acc:  0.65625
train loss:  0.5818817019462585
train gradient:  0.18384562232380466
iteration : 4525
train acc:  0.8046875
train loss:  0.4815649390220642
train gradient:  0.12327816576847168
iteration : 4526
train acc:  0.7578125
train loss:  0.5146458745002747
train gradient:  0.1694532634853233
iteration : 4527
train acc:  0.765625
train loss:  0.44553929567337036
train gradient:  0.11673531289402277
iteration : 4528
train acc:  0.703125
train loss:  0.5162526369094849
train gradient:  0.14199257816416477
iteration : 4529
train acc:  0.7578125
train loss:  0.4782678782939911
train gradient:  0.13138908830705118
iteration : 4530
train acc:  0.6953125
train loss:  0.5656764507293701
train gradient:  0.19880338176733364
iteration : 4531
train acc:  0.7421875
train loss:  0.5240594744682312
train gradient:  0.13888700065301215
iteration : 4532
train acc:  0.7890625
train loss:  0.4368530511856079
train gradient:  0.13216612724883536
iteration : 4533
train acc:  0.703125
train loss:  0.5622646808624268
train gradient:  0.17982507930119054
iteration : 4534
train acc:  0.6796875
train loss:  0.5969524383544922
train gradient:  0.1533882670385627
iteration : 4535
train acc:  0.6328125
train loss:  0.5791510939598083
train gradient:  0.15279099915109018
iteration : 4536
train acc:  0.78125
train loss:  0.4591028094291687
train gradient:  0.15181388699509818
iteration : 4537
train acc:  0.7265625
train loss:  0.4878845810890198
train gradient:  0.16628986697274836
iteration : 4538
train acc:  0.7265625
train loss:  0.49180641770362854
train gradient:  0.1284566769446222
iteration : 4539
train acc:  0.6875
train loss:  0.5724149346351624
train gradient:  0.15501488009480535
iteration : 4540
train acc:  0.7421875
train loss:  0.48457175493240356
train gradient:  0.15492152939880405
iteration : 4541
train acc:  0.671875
train loss:  0.552910327911377
train gradient:  0.1655905132200553
iteration : 4542
train acc:  0.71875
train loss:  0.5624798536300659
train gradient:  0.19559608853528015
iteration : 4543
train acc:  0.6875
train loss:  0.5411148071289062
train gradient:  0.16944932525056577
iteration : 4544
train acc:  0.7421875
train loss:  0.5022227764129639
train gradient:  0.1586332717776867
iteration : 4545
train acc:  0.765625
train loss:  0.479402095079422
train gradient:  0.11061904793325576
iteration : 4546
train acc:  0.7109375
train loss:  0.5275164842605591
train gradient:  0.17650176641212678
iteration : 4547
train acc:  0.6796875
train loss:  0.550500214099884
train gradient:  0.19761265513358933
iteration : 4548
train acc:  0.796875
train loss:  0.46531325578689575
train gradient:  0.12479437360980626
iteration : 4549
train acc:  0.7734375
train loss:  0.45783647894859314
train gradient:  0.1047114554337364
iteration : 4550
train acc:  0.71875
train loss:  0.560781717300415
train gradient:  0.17122155338181055
iteration : 4551
train acc:  0.6875
train loss:  0.5514873266220093
train gradient:  0.14956135890589214
iteration : 4552
train acc:  0.734375
train loss:  0.5549704432487488
train gradient:  0.16059990991888784
iteration : 4553
train acc:  0.7890625
train loss:  0.4757022261619568
train gradient:  0.14094509342357509
iteration : 4554
train acc:  0.734375
train loss:  0.531373143196106
train gradient:  0.21339013676462057
iteration : 4555
train acc:  0.6953125
train loss:  0.5399590730667114
train gradient:  0.16351947475777884
iteration : 4556
train acc:  0.71875
train loss:  0.5326755046844482
train gradient:  0.17536163538748345
iteration : 4557
train acc:  0.8046875
train loss:  0.4887179732322693
train gradient:  0.12791375084114678
iteration : 4558
train acc:  0.765625
train loss:  0.5019897222518921
train gradient:  0.12074197382765041
iteration : 4559
train acc:  0.71875
train loss:  0.5457120537757874
train gradient:  0.17992622913597256
iteration : 4560
train acc:  0.703125
train loss:  0.519241452217102
train gradient:  0.12676193447506262
iteration : 4561
train acc:  0.7734375
train loss:  0.48191940784454346
train gradient:  0.11615305395316983
iteration : 4562
train acc:  0.7890625
train loss:  0.4692608714103699
train gradient:  0.1435656917187456
iteration : 4563
train acc:  0.8046875
train loss:  0.4806936979293823
train gradient:  0.15287583835948232
iteration : 4564
train acc:  0.7578125
train loss:  0.5208214521408081
train gradient:  0.15036379276567996
iteration : 4565
train acc:  0.734375
train loss:  0.5040706992149353
train gradient:  0.14361078263165458
iteration : 4566
train acc:  0.7421875
train loss:  0.5177716016769409
train gradient:  0.21782946670824893
iteration : 4567
train acc:  0.734375
train loss:  0.5120984315872192
train gradient:  0.1395501100622395
iteration : 4568
train acc:  0.6796875
train loss:  0.5805987119674683
train gradient:  0.1973654675384945
iteration : 4569
train acc:  0.7109375
train loss:  0.5648914575576782
train gradient:  0.18044478827643817
iteration : 4570
train acc:  0.671875
train loss:  0.5848346948623657
train gradient:  0.21399228611728574
iteration : 4571
train acc:  0.7734375
train loss:  0.48053300380706787
train gradient:  0.16632794430162479
iteration : 4572
train acc:  0.7265625
train loss:  0.5452786684036255
train gradient:  0.18868107340727094
iteration : 4573
train acc:  0.7421875
train loss:  0.492255836725235
train gradient:  0.151539176360537
iteration : 4574
train acc:  0.7421875
train loss:  0.5251815319061279
train gradient:  0.1386090542177419
iteration : 4575
train acc:  0.71875
train loss:  0.5385452508926392
train gradient:  0.13882311454027071
iteration : 4576
train acc:  0.71875
train loss:  0.49123692512512207
train gradient:  0.18071935295392821
iteration : 4577
train acc:  0.7578125
train loss:  0.5031552910804749
train gradient:  0.13624672239140734
iteration : 4578
train acc:  0.8125
train loss:  0.4261716604232788
train gradient:  0.11361211871592682
iteration : 4579
train acc:  0.78125
train loss:  0.48834091424942017
train gradient:  0.17112172872483528
iteration : 4580
train acc:  0.78125
train loss:  0.44978925585746765
train gradient:  0.13076488882788803
iteration : 4581
train acc:  0.7265625
train loss:  0.5161381959915161
train gradient:  0.1576311866101915
iteration : 4582
train acc:  0.7265625
train loss:  0.49346333742141724
train gradient:  0.12559803510234935
iteration : 4583
train acc:  0.75
train loss:  0.494884729385376
train gradient:  0.1447435294498349
iteration : 4584
train acc:  0.75
train loss:  0.4849674105644226
train gradient:  0.1270637215162112
iteration : 4585
train acc:  0.7890625
train loss:  0.45835715532302856
train gradient:  0.11224371812462856
iteration : 4586
train acc:  0.7265625
train loss:  0.5107470154762268
train gradient:  0.1586542878989876
iteration : 4587
train acc:  0.6796875
train loss:  0.5673155188560486
train gradient:  0.22082349896335812
iteration : 4588
train acc:  0.6640625
train loss:  0.5886447429656982
train gradient:  0.1929831580369319
iteration : 4589
train acc:  0.6328125
train loss:  0.5685752630233765
train gradient:  0.14628683663651104
iteration : 4590
train acc:  0.703125
train loss:  0.5382455587387085
train gradient:  0.1796790577534635
iteration : 4591
train acc:  0.734375
train loss:  0.4962831437587738
train gradient:  0.16643538992442827
iteration : 4592
train acc:  0.6484375
train loss:  0.613064169883728
train gradient:  0.1965917461313626
iteration : 4593
train acc:  0.75
train loss:  0.5319240689277649
train gradient:  0.21915968685864112
iteration : 4594
train acc:  0.65625
train loss:  0.5517829656600952
train gradient:  0.1695053498326975
iteration : 4595
train acc:  0.8203125
train loss:  0.45430415868759155
train gradient:  0.14140721111541543
iteration : 4596
train acc:  0.7421875
train loss:  0.5045730471611023
train gradient:  0.16600771959007382
iteration : 4597
train acc:  0.71875
train loss:  0.5477434396743774
train gradient:  0.15691561779771862
iteration : 4598
train acc:  0.7109375
train loss:  0.5271692276000977
train gradient:  0.13347675881972923
iteration : 4599
train acc:  0.75
train loss:  0.5077815055847168
train gradient:  0.12395914052297173
iteration : 4600
train acc:  0.6484375
train loss:  0.5816212296485901
train gradient:  0.18474257959955853
iteration : 4601
train acc:  0.6953125
train loss:  0.567762017250061
train gradient:  0.18952929014404649
iteration : 4602
train acc:  0.703125
train loss:  0.5265904664993286
train gradient:  0.21375899420522437
iteration : 4603
train acc:  0.7734375
train loss:  0.467040479183197
train gradient:  0.12286018627852782
iteration : 4604
train acc:  0.6875
train loss:  0.5677156448364258
train gradient:  0.14367202302855475
iteration : 4605
train acc:  0.7265625
train loss:  0.5047871470451355
train gradient:  0.15256100093148808
iteration : 4606
train acc:  0.7265625
train loss:  0.5257415175437927
train gradient:  0.15949493636207018
iteration : 4607
train acc:  0.78125
train loss:  0.46636131405830383
train gradient:  0.1460293732668361
iteration : 4608
train acc:  0.7890625
train loss:  0.4662374258041382
train gradient:  0.2100469583138934
iteration : 4609
train acc:  0.734375
train loss:  0.5011101961135864
train gradient:  0.16673371727970163
iteration : 4610
train acc:  0.734375
train loss:  0.4916493594646454
train gradient:  0.1352746194413805
iteration : 4611
train acc:  0.703125
train loss:  0.527795135974884
train gradient:  0.26077159651611403
iteration : 4612
train acc:  0.734375
train loss:  0.5084608793258667
train gradient:  0.14004004179849167
iteration : 4613
train acc:  0.7421875
train loss:  0.47312411665916443
train gradient:  0.19133705629200873
iteration : 4614
train acc:  0.671875
train loss:  0.5517204999923706
train gradient:  0.17577428356698904
iteration : 4615
train acc:  0.7265625
train loss:  0.47446703910827637
train gradient:  0.10437929697963501
iteration : 4616
train acc:  0.6484375
train loss:  0.5338521599769592
train gradient:  0.1419662623401506
iteration : 4617
train acc:  0.6953125
train loss:  0.5787961483001709
train gradient:  0.19672807599042497
iteration : 4618
train acc:  0.7578125
train loss:  0.4814611077308655
train gradient:  0.14036684941100597
iteration : 4619
train acc:  0.6875
train loss:  0.5421833395957947
train gradient:  0.15410532442378166
iteration : 4620
train acc:  0.6875
train loss:  0.6032623648643494
train gradient:  0.17434771062963228
iteration : 4621
train acc:  0.8046875
train loss:  0.4363461136817932
train gradient:  0.10004485585069058
iteration : 4622
train acc:  0.71875
train loss:  0.5199587345123291
train gradient:  0.15937490927158232
iteration : 4623
train acc:  0.7421875
train loss:  0.4585401117801666
train gradient:  0.13656228206933382
iteration : 4624
train acc:  0.6875
train loss:  0.5399692058563232
train gradient:  0.17759055316336214
iteration : 4625
train acc:  0.765625
train loss:  0.45060211420059204
train gradient:  0.10910259313035994
iteration : 4626
train acc:  0.7265625
train loss:  0.5110659599304199
train gradient:  0.12571909374673512
iteration : 4627
train acc:  0.6953125
train loss:  0.5494633316993713
train gradient:  0.16443833428731297
iteration : 4628
train acc:  0.7265625
train loss:  0.5322964787483215
train gradient:  0.14899740158615896
iteration : 4629
train acc:  0.703125
train loss:  0.5665092468261719
train gradient:  0.20298247964079982
iteration : 4630
train acc:  0.703125
train loss:  0.5331169366836548
train gradient:  0.13933513496093594
iteration : 4631
train acc:  0.78125
train loss:  0.46365898847579956
train gradient:  0.1344752450038139
iteration : 4632
train acc:  0.7421875
train loss:  0.5164791345596313
train gradient:  0.14241225010342698
iteration : 4633
train acc:  0.75
train loss:  0.5072779655456543
train gradient:  0.15171300550680583
iteration : 4634
train acc:  0.6875
train loss:  0.5587943196296692
train gradient:  0.15795717823346628
iteration : 4635
train acc:  0.6953125
train loss:  0.5725371837615967
train gradient:  0.19244209935851536
iteration : 4636
train acc:  0.65625
train loss:  0.5868967175483704
train gradient:  0.19497666055916324
iteration : 4637
train acc:  0.734375
train loss:  0.5153716802597046
train gradient:  0.1270596919186887
iteration : 4638
train acc:  0.7265625
train loss:  0.5402238368988037
train gradient:  0.15915152216250866
iteration : 4639
train acc:  0.71875
train loss:  0.5611653327941895
train gradient:  0.16440059092293086
iteration : 4640
train acc:  0.6875
train loss:  0.5054529309272766
train gradient:  0.16196033511710836
iteration : 4641
train acc:  0.6875
train loss:  0.535675048828125
train gradient:  0.16605159590983293
iteration : 4642
train acc:  0.7578125
train loss:  0.4769713878631592
train gradient:  0.15114964048258017
iteration : 4643
train acc:  0.765625
train loss:  0.4653528332710266
train gradient:  0.13781502948204694
iteration : 4644
train acc:  0.671875
train loss:  0.542351245880127
train gradient:  0.16108989916802463
iteration : 4645
train acc:  0.6484375
train loss:  0.5575094223022461
train gradient:  0.2008773374262527
iteration : 4646
train acc:  0.765625
train loss:  0.4402884244918823
train gradient:  0.13583242825465053
iteration : 4647
train acc:  0.6953125
train loss:  0.5327296257019043
train gradient:  0.1673981399720863
iteration : 4648
train acc:  0.7421875
train loss:  0.47530263662338257
train gradient:  0.18221519536230682
iteration : 4649
train acc:  0.6953125
train loss:  0.5464051365852356
train gradient:  0.1594543094528631
iteration : 4650
train acc:  0.75
train loss:  0.4785282015800476
train gradient:  0.13847961062513242
iteration : 4651
train acc:  0.765625
train loss:  0.4561420977115631
train gradient:  0.11878336800528567
iteration : 4652
train acc:  0.6640625
train loss:  0.5712074041366577
train gradient:  0.22248017681068805
iteration : 4653
train acc:  0.703125
train loss:  0.5242015719413757
train gradient:  0.14271359264858513
iteration : 4654
train acc:  0.7265625
train loss:  0.5026940107345581
train gradient:  0.20607074075262155
iteration : 4655
train acc:  0.6953125
train loss:  0.5092496275901794
train gradient:  0.16192132474436846
iteration : 4656
train acc:  0.7734375
train loss:  0.4925489127635956
train gradient:  0.1356452400557592
iteration : 4657
train acc:  0.7109375
train loss:  0.5105922818183899
train gradient:  0.15937443594470338
iteration : 4658
train acc:  0.7734375
train loss:  0.4625394940376282
train gradient:  0.11331095455388779
iteration : 4659
train acc:  0.734375
train loss:  0.5222113132476807
train gradient:  0.1544932588098642
iteration : 4660
train acc:  0.6796875
train loss:  0.5709298849105835
train gradient:  0.17497178748315542
iteration : 4661
train acc:  0.7109375
train loss:  0.5193250775337219
train gradient:  0.13444819188394322
iteration : 4662
train acc:  0.765625
train loss:  0.5272930264472961
train gradient:  0.152358374812751
iteration : 4663
train acc:  0.8046875
train loss:  0.43459346890449524
train gradient:  0.1402117859078566
iteration : 4664
train acc:  0.78125
train loss:  0.4941343367099762
train gradient:  0.15306087841928157
iteration : 4665
train acc:  0.6484375
train loss:  0.5867975950241089
train gradient:  0.22559120531473048
iteration : 4666
train acc:  0.71875
train loss:  0.5031498074531555
train gradient:  0.14394250897522132
iteration : 4667
train acc:  0.703125
train loss:  0.5200389623641968
train gradient:  0.15329960646730034
iteration : 4668
train acc:  0.734375
train loss:  0.47560375928878784
train gradient:  0.1423484515475807
iteration : 4669
train acc:  0.7421875
train loss:  0.4850772023200989
train gradient:  0.11333619213490372
iteration : 4670
train acc:  0.734375
train loss:  0.5185123085975647
train gradient:  0.18118708108937362
iteration : 4671
train acc:  0.7734375
train loss:  0.4849165380001068
train gradient:  0.1297477999273643
iteration : 4672
train acc:  0.6484375
train loss:  0.5370482802391052
train gradient:  0.2073057326037947
iteration : 4673
train acc:  0.7734375
train loss:  0.477214515209198
train gradient:  0.16313737323498395
iteration : 4674
train acc:  0.765625
train loss:  0.4807329475879669
train gradient:  0.13089562705398644
iteration : 4675
train acc:  0.703125
train loss:  0.6189790964126587
train gradient:  0.247437052074873
iteration : 4676
train acc:  0.765625
train loss:  0.5174179077148438
train gradient:  0.19070251078559752
iteration : 4677
train acc:  0.7578125
train loss:  0.5556354522705078
train gradient:  0.15069004492004717
iteration : 4678
train acc:  0.75
train loss:  0.5104605555534363
train gradient:  0.16013387726074774
iteration : 4679
train acc:  0.7265625
train loss:  0.5168355107307434
train gradient:  0.12763832840250675
iteration : 4680
train acc:  0.71875
train loss:  0.5169069766998291
train gradient:  0.14926451907771005
iteration : 4681
train acc:  0.78125
train loss:  0.4776293933391571
train gradient:  0.1738612422200388
iteration : 4682
train acc:  0.7578125
train loss:  0.5020487308502197
train gradient:  0.13339560867967598
iteration : 4683
train acc:  0.6875
train loss:  0.5271634459495544
train gradient:  0.13468224483472002
iteration : 4684
train acc:  0.75
train loss:  0.479353129863739
train gradient:  0.12917873037926247
iteration : 4685
train acc:  0.6796875
train loss:  0.6073864698410034
train gradient:  0.18904043622586525
iteration : 4686
train acc:  0.71875
train loss:  0.5503944158554077
train gradient:  0.16723073619255455
iteration : 4687
train acc:  0.7578125
train loss:  0.4536457061767578
train gradient:  0.12551979094936408
iteration : 4688
train acc:  0.7421875
train loss:  0.5347590446472168
train gradient:  0.1617110105093666
iteration : 4689
train acc:  0.640625
train loss:  0.5951305031776428
train gradient:  0.17508149528172307
iteration : 4690
train acc:  0.75
train loss:  0.4797802269458771
train gradient:  0.10824851655018612
iteration : 4691
train acc:  0.7734375
train loss:  0.47201457619667053
train gradient:  0.12561849716282522
iteration : 4692
train acc:  0.6640625
train loss:  0.5176721811294556
train gradient:  0.1702578473896793
iteration : 4693
train acc:  0.6796875
train loss:  0.5254561901092529
train gradient:  0.16104909353733465
iteration : 4694
train acc:  0.6796875
train loss:  0.5945675373077393
train gradient:  0.16455395863367084
iteration : 4695
train acc:  0.8046875
train loss:  0.4496150612831116
train gradient:  0.10266573839631184
iteration : 4696
train acc:  0.6953125
train loss:  0.5278154611587524
train gradient:  0.16029221348913508
iteration : 4697
train acc:  0.7109375
train loss:  0.5144854187965393
train gradient:  0.15075444452368075
iteration : 4698
train acc:  0.71875
train loss:  0.5395413637161255
train gradient:  0.1788085078035389
iteration : 4699
train acc:  0.6953125
train loss:  0.5479786396026611
train gradient:  0.17520460075371955
iteration : 4700
train acc:  0.6875
train loss:  0.5697274208068848
train gradient:  0.20360801346251062
iteration : 4701
train acc:  0.6796875
train loss:  0.5304193496704102
train gradient:  0.1623506235704932
iteration : 4702
train acc:  0.71875
train loss:  0.510961651802063
train gradient:  0.13939285070897187
iteration : 4703
train acc:  0.6953125
train loss:  0.541846513748169
train gradient:  0.23489263701478474
iteration : 4704
train acc:  0.75
train loss:  0.521462619304657
train gradient:  0.20033702198220749
iteration : 4705
train acc:  0.6640625
train loss:  0.5289628505706787
train gradient:  0.17602380886719027
iteration : 4706
train acc:  0.6875
train loss:  0.59894859790802
train gradient:  0.22938663889954025
iteration : 4707
train acc:  0.6328125
train loss:  0.5913971662521362
train gradient:  0.1851785668191866
iteration : 4708
train acc:  0.734375
train loss:  0.4986184239387512
train gradient:  0.1385269518456103
iteration : 4709
train acc:  0.6796875
train loss:  0.5497649312019348
train gradient:  0.15490871353704624
iteration : 4710
train acc:  0.734375
train loss:  0.547332763671875
train gradient:  0.16456009114714804
iteration : 4711
train acc:  0.703125
train loss:  0.4944993257522583
train gradient:  0.1573706167318527
iteration : 4712
train acc:  0.6328125
train loss:  0.5858185291290283
train gradient:  0.1975526951542221
iteration : 4713
train acc:  0.7734375
train loss:  0.48856258392333984
train gradient:  0.15259650964681173
iteration : 4714
train acc:  0.75
train loss:  0.5147144794464111
train gradient:  0.1490500677845834
iteration : 4715
train acc:  0.7734375
train loss:  0.47406715154647827
train gradient:  0.11875473897282186
iteration : 4716
train acc:  0.7578125
train loss:  0.4825206995010376
train gradient:  0.13008919002005429
iteration : 4717
train acc:  0.6953125
train loss:  0.5691019296646118
train gradient:  0.16324165670824103
iteration : 4718
train acc:  0.765625
train loss:  0.4324275553226471
train gradient:  0.1533271860080154
iteration : 4719
train acc:  0.765625
train loss:  0.4639948010444641
train gradient:  0.10040472375408231
iteration : 4720
train acc:  0.65625
train loss:  0.6207401752471924
train gradient:  0.21160938525741785
iteration : 4721
train acc:  0.6875
train loss:  0.5402158498764038
train gradient:  0.17230312158554867
iteration : 4722
train acc:  0.75
train loss:  0.4432046413421631
train gradient:  0.10662302576182166
iteration : 4723
train acc:  0.7734375
train loss:  0.47448471188545227
train gradient:  0.12499119995132568
iteration : 4724
train acc:  0.6875
train loss:  0.5553737878799438
train gradient:  0.18020170202296548
iteration : 4725
train acc:  0.8203125
train loss:  0.4500633478164673
train gradient:  0.18919774951527313
iteration : 4726
train acc:  0.7421875
train loss:  0.4974542260169983
train gradient:  0.1345238026826097
iteration : 4727
train acc:  0.6875
train loss:  0.5400192141532898
train gradient:  0.15357988581043935
iteration : 4728
train acc:  0.7109375
train loss:  0.5353904366493225
train gradient:  0.14037686117293524
iteration : 4729
train acc:  0.703125
train loss:  0.5023156404495239
train gradient:  0.15214617076365106
iteration : 4730
train acc:  0.7265625
train loss:  0.5047256946563721
train gradient:  0.14303926887030366
iteration : 4731
train acc:  0.7734375
train loss:  0.44509971141815186
train gradient:  0.13947221208790228
iteration : 4732
train acc:  0.6640625
train loss:  0.5357153415679932
train gradient:  0.16620098943686717
iteration : 4733
train acc:  0.6875
train loss:  0.5689324736595154
train gradient:  0.1969534794474872
iteration : 4734
train acc:  0.7421875
train loss:  0.49124202132225037
train gradient:  0.14659259059438012
iteration : 4735
train acc:  0.765625
train loss:  0.48590266704559326
train gradient:  0.15108144771067472
iteration : 4736
train acc:  0.671875
train loss:  0.5164727568626404
train gradient:  0.13748095574354643
iteration : 4737
train acc:  0.75
train loss:  0.4789383113384247
train gradient:  0.14236073599265509
iteration : 4738
train acc:  0.7421875
train loss:  0.477300763130188
train gradient:  0.12913525369983064
iteration : 4739
train acc:  0.7265625
train loss:  0.517781138420105
train gradient:  0.1476692834563904
iteration : 4740
train acc:  0.75
train loss:  0.5084084272384644
train gradient:  0.14858124169894668
iteration : 4741
train acc:  0.7734375
train loss:  0.48063814640045166
train gradient:  0.11511304408023923
iteration : 4742
train acc:  0.7734375
train loss:  0.4687422215938568
train gradient:  0.10214881854696153
iteration : 4743
train acc:  0.765625
train loss:  0.5091014504432678
train gradient:  0.17548207206274696
iteration : 4744
train acc:  0.671875
train loss:  0.5113538503646851
train gradient:  0.14987827463690737
iteration : 4745
train acc:  0.703125
train loss:  0.5427379608154297
train gradient:  0.1541657935254327
iteration : 4746
train acc:  0.7421875
train loss:  0.47070765495300293
train gradient:  0.16130889784994806
iteration : 4747
train acc:  0.765625
train loss:  0.5350338220596313
train gradient:  0.19618836065221734
iteration : 4748
train acc:  0.75
train loss:  0.4943810999393463
train gradient:  0.1497858758507402
iteration : 4749
train acc:  0.6640625
train loss:  0.5833781957626343
train gradient:  0.17656165756618097
iteration : 4750
train acc:  0.7265625
train loss:  0.5245710015296936
train gradient:  0.16286716187258904
iteration : 4751
train acc:  0.703125
train loss:  0.5317829847335815
train gradient:  0.15423933526220926
iteration : 4752
train acc:  0.7265625
train loss:  0.4665248394012451
train gradient:  0.1377478088524934
iteration : 4753
train acc:  0.7578125
train loss:  0.5102341175079346
train gradient:  0.13594471702921523
iteration : 4754
train acc:  0.734375
train loss:  0.49221524596214294
train gradient:  0.13414190863048076
iteration : 4755
train acc:  0.78125
train loss:  0.49067044258117676
train gradient:  0.18077285784756236
iteration : 4756
train acc:  0.671875
train loss:  0.5708350539207458
train gradient:  0.16175201230683922
iteration : 4757
train acc:  0.6875
train loss:  0.5418732166290283
train gradient:  0.1343832876357083
iteration : 4758
train acc:  0.75
train loss:  0.5245522260665894
train gradient:  0.16835877040580227
iteration : 4759
train acc:  0.71875
train loss:  0.5641579627990723
train gradient:  0.19881894978088194
iteration : 4760
train acc:  0.75
train loss:  0.49058252573013306
train gradient:  0.13442758154777365
iteration : 4761
train acc:  0.8203125
train loss:  0.4481889605522156
train gradient:  0.13113930031413756
iteration : 4762
train acc:  0.7265625
train loss:  0.48060715198516846
train gradient:  0.14785439810927475
iteration : 4763
train acc:  0.734375
train loss:  0.4997475743293762
train gradient:  0.17088002047071757
iteration : 4764
train acc:  0.6953125
train loss:  0.5893975496292114
train gradient:  0.20258420931957444
iteration : 4765
train acc:  0.6953125
train loss:  0.537701427936554
train gradient:  0.16931148208346697
iteration : 4766
train acc:  0.7890625
train loss:  0.49015164375305176
train gradient:  0.16904771909839617
iteration : 4767
train acc:  0.7421875
train loss:  0.46949446201324463
train gradient:  0.1522303347779601
iteration : 4768
train acc:  0.75
train loss:  0.5084543824195862
train gradient:  0.16412157967475116
iteration : 4769
train acc:  0.7421875
train loss:  0.5072551369667053
train gradient:  0.1412961243592919
iteration : 4770
train acc:  0.734375
train loss:  0.4876689016819
train gradient:  0.14052380738668116
iteration : 4771
train acc:  0.75
train loss:  0.49024856090545654
train gradient:  0.14219249667527412
iteration : 4772
train acc:  0.796875
train loss:  0.4633800983428955
train gradient:  0.15405543339958183
iteration : 4773
train acc:  0.703125
train loss:  0.6047202348709106
train gradient:  0.23220149786308597
iteration : 4774
train acc:  0.7265625
train loss:  0.4774397611618042
train gradient:  0.12464694494058408
iteration : 4775
train acc:  0.6484375
train loss:  0.5308166146278381
train gradient:  0.1953591721511691
iteration : 4776
train acc:  0.7109375
train loss:  0.5261904001235962
train gradient:  0.14541019558262125
iteration : 4777
train acc:  0.703125
train loss:  0.518141508102417
train gradient:  0.14590305053006702
iteration : 4778
train acc:  0.7421875
train loss:  0.513229489326477
train gradient:  0.17545963426209715
iteration : 4779
train acc:  0.6796875
train loss:  0.6085014343261719
train gradient:  0.17925893677809265
iteration : 4780
train acc:  0.7265625
train loss:  0.5152824521064758
train gradient:  0.13451301481148875
iteration : 4781
train acc:  0.6640625
train loss:  0.5796995162963867
train gradient:  0.16907461823282222
iteration : 4782
train acc:  0.703125
train loss:  0.5551938414573669
train gradient:  0.15660915324354932
iteration : 4783
train acc:  0.6875
train loss:  0.5633528232574463
train gradient:  0.17824925234773503
iteration : 4784
train acc:  0.734375
train loss:  0.4870392084121704
train gradient:  0.12461185107644963
iteration : 4785
train acc:  0.7109375
train loss:  0.5257706046104431
train gradient:  0.14333076062538475
iteration : 4786
train acc:  0.7265625
train loss:  0.5017555952072144
train gradient:  0.143768600238746
iteration : 4787
train acc:  0.78125
train loss:  0.45571231842041016
train gradient:  0.12744839258800383
iteration : 4788
train acc:  0.6640625
train loss:  0.5706788301467896
train gradient:  0.19045039332320246
iteration : 4789
train acc:  0.78125
train loss:  0.45403480529785156
train gradient:  0.13430135940540824
iteration : 4790
train acc:  0.703125
train loss:  0.5552254915237427
train gradient:  0.19769531694715034
iteration : 4791
train acc:  0.75
train loss:  0.516155481338501
train gradient:  0.1417331331041362
iteration : 4792
train acc:  0.7109375
train loss:  0.5785607099533081
train gradient:  0.23069853239676935
iteration : 4793
train acc:  0.75
train loss:  0.5081498622894287
train gradient:  0.16621994550680774
iteration : 4794
train acc:  0.765625
train loss:  0.4664452075958252
train gradient:  0.11815621303330512
iteration : 4795
train acc:  0.765625
train loss:  0.4609999358654022
train gradient:  0.13896740980246886
iteration : 4796
train acc:  0.8046875
train loss:  0.42584678530693054
train gradient:  0.10837920847549005
iteration : 4797
train acc:  0.625
train loss:  0.6031585931777954
train gradient:  0.2125434916803306
iteration : 4798
train acc:  0.671875
train loss:  0.5760774612426758
train gradient:  0.20883993785227795
iteration : 4799
train acc:  0.6875
train loss:  0.5887625813484192
train gradient:  0.2525608597433768
iteration : 4800
train acc:  0.75
train loss:  0.48624756932258606
train gradient:  0.1429217950758624
iteration : 4801
train acc:  0.7421875
train loss:  0.5313668847084045
train gradient:  0.17161999010174742
iteration : 4802
train acc:  0.8203125
train loss:  0.4738680124282837
train gradient:  0.14358700943327038
iteration : 4803
train acc:  0.765625
train loss:  0.48600897192955017
train gradient:  0.1344792436449592
iteration : 4804
train acc:  0.765625
train loss:  0.471550852060318
train gradient:  0.15106087463402296
iteration : 4805
train acc:  0.6875
train loss:  0.5763081312179565
train gradient:  0.21602919198531584
iteration : 4806
train acc:  0.75
train loss:  0.4961751699447632
train gradient:  0.200156609185985
iteration : 4807
train acc:  0.71875
train loss:  0.49956923723220825
train gradient:  0.13333418882629308
iteration : 4808
train acc:  0.734375
train loss:  0.46838438510894775
train gradient:  0.12160918244954359
iteration : 4809
train acc:  0.6875
train loss:  0.5802024602890015
train gradient:  0.14924185457087866
iteration : 4810
train acc:  0.796875
train loss:  0.45938512682914734
train gradient:  0.12566350844653584
iteration : 4811
train acc:  0.796875
train loss:  0.4812784194946289
train gradient:  0.14583970732465054
iteration : 4812
train acc:  0.7578125
train loss:  0.4591619074344635
train gradient:  0.11099577885311224
iteration : 4813
train acc:  0.75
train loss:  0.4745383858680725
train gradient:  0.1368950415108308
iteration : 4814
train acc:  0.734375
train loss:  0.4774106740951538
train gradient:  0.12555115344428489
iteration : 4815
train acc:  0.71875
train loss:  0.5418081283569336
train gradient:  0.2526687970689716
iteration : 4816
train acc:  0.71875
train loss:  0.51781165599823
train gradient:  0.1453495845819059
iteration : 4817
train acc:  0.7265625
train loss:  0.515399694442749
train gradient:  0.17245460280085637
iteration : 4818
train acc:  0.671875
train loss:  0.5944775938987732
train gradient:  0.1959493707908162
iteration : 4819
train acc:  0.78125
train loss:  0.496807336807251
train gradient:  0.1562147686869154
iteration : 4820
train acc:  0.7421875
train loss:  0.5097965598106384
train gradient:  0.14309451852249055
iteration : 4821
train acc:  0.7265625
train loss:  0.48478275537490845
train gradient:  0.13107053364250848
iteration : 4822
train acc:  0.7578125
train loss:  0.4913138747215271
train gradient:  0.14520260746257194
iteration : 4823
train acc:  0.75
train loss:  0.4678335189819336
train gradient:  0.1685084581776049
iteration : 4824
train acc:  0.7734375
train loss:  0.468546986579895
train gradient:  0.11246822333348604
iteration : 4825
train acc:  0.7109375
train loss:  0.5193803310394287
train gradient:  0.1628349633529445
iteration : 4826
train acc:  0.7109375
train loss:  0.5545224547386169
train gradient:  0.1833018002489086
iteration : 4827
train acc:  0.7734375
train loss:  0.4646943509578705
train gradient:  0.12174859990581484
iteration : 4828
train acc:  0.7265625
train loss:  0.5211551785469055
train gradient:  0.18812265789546176
iteration : 4829
train acc:  0.7734375
train loss:  0.4917469024658203
train gradient:  0.11469668413404377
iteration : 4830
train acc:  0.765625
train loss:  0.48081913590431213
train gradient:  0.13895113891449123
iteration : 4831
train acc:  0.6875
train loss:  0.5530093312263489
train gradient:  0.16551629798211756
iteration : 4832
train acc:  0.7734375
train loss:  0.46309834718704224
train gradient:  0.12634429305491035
iteration : 4833
train acc:  0.671875
train loss:  0.5656609535217285
train gradient:  0.20274865886787524
iteration : 4834
train acc:  0.7265625
train loss:  0.4960883855819702
train gradient:  0.14108348445915614
iteration : 4835
train acc:  0.7734375
train loss:  0.47958528995513916
train gradient:  0.12529904965736366
iteration : 4836
train acc:  0.7109375
train loss:  0.5651434659957886
train gradient:  0.1529854825453936
iteration : 4837
train acc:  0.7578125
train loss:  0.5303064584732056
train gradient:  0.160708800659
iteration : 4838
train acc:  0.7265625
train loss:  0.5041303634643555
train gradient:  0.14648134767683724
iteration : 4839
train acc:  0.6875
train loss:  0.6149340867996216
train gradient:  0.21868665995433217
iteration : 4840
train acc:  0.7421875
train loss:  0.5152210593223572
train gradient:  0.15806261902710828
iteration : 4841
train acc:  0.7265625
train loss:  0.5378924608230591
train gradient:  0.1924055770963412
iteration : 4842
train acc:  0.7578125
train loss:  0.49054282903671265
train gradient:  0.17101838787737522
iteration : 4843
train acc:  0.7734375
train loss:  0.4533020257949829
train gradient:  0.11589759214373682
iteration : 4844
train acc:  0.7265625
train loss:  0.4914797842502594
train gradient:  0.12431639373687145
iteration : 4845
train acc:  0.6875
train loss:  0.6153570413589478
train gradient:  0.21468604931337137
iteration : 4846
train acc:  0.71875
train loss:  0.5121411085128784
train gradient:  0.13748606476702527
iteration : 4847
train acc:  0.6953125
train loss:  0.5453164577484131
train gradient:  0.1711562695953614
iteration : 4848
train acc:  0.7890625
train loss:  0.5099114179611206
train gradient:  0.15561624394734652
iteration : 4849
train acc:  0.78125
train loss:  0.4756564497947693
train gradient:  0.1736736427217332
iteration : 4850
train acc:  0.703125
train loss:  0.5782496333122253
train gradient:  0.20322389587013034
iteration : 4851
train acc:  0.7578125
train loss:  0.5112853646278381
train gradient:  0.17859547271811532
iteration : 4852
train acc:  0.6953125
train loss:  0.5096238851547241
train gradient:  0.1435915194380067
iteration : 4853
train acc:  0.6796875
train loss:  0.607315182685852
train gradient:  0.24139571751176597
iteration : 4854
train acc:  0.703125
train loss:  0.49231672286987305
train gradient:  0.11391144983621614
iteration : 4855
train acc:  0.734375
train loss:  0.5500524640083313
train gradient:  0.17545485362612293
iteration : 4856
train acc:  0.6875
train loss:  0.5832400321960449
train gradient:  0.18519022433200738
iteration : 4857
train acc:  0.7421875
train loss:  0.5276988744735718
train gradient:  0.179997422618561
iteration : 4858
train acc:  0.7265625
train loss:  0.532281219959259
train gradient:  0.18506163850316626
iteration : 4859
train acc:  0.765625
train loss:  0.4797758460044861
train gradient:  0.15936114656328357
iteration : 4860
train acc:  0.765625
train loss:  0.49630075693130493
train gradient:  0.14565466865795185
iteration : 4861
train acc:  0.765625
train loss:  0.5315728187561035
train gradient:  0.19340323654801317
iteration : 4862
train acc:  0.7578125
train loss:  0.493421733379364
train gradient:  0.19599564221396404
iteration : 4863
train acc:  0.734375
train loss:  0.483172744512558
train gradient:  0.12299197964507347
iteration : 4864
train acc:  0.7421875
train loss:  0.5012394189834595
train gradient:  0.18760887124880637
iteration : 4865
train acc:  0.6953125
train loss:  0.5693951845169067
train gradient:  0.1487268157077521
iteration : 4866
train acc:  0.71875
train loss:  0.5047610998153687
train gradient:  0.12762242011615033
iteration : 4867
train acc:  0.7109375
train loss:  0.5079478025436401
train gradient:  0.12556537112084243
iteration : 4868
train acc:  0.75
train loss:  0.5196002721786499
train gradient:  0.17349608299615005
iteration : 4869
train acc:  0.703125
train loss:  0.5329532623291016
train gradient:  0.14832852126594065
iteration : 4870
train acc:  0.6953125
train loss:  0.5705610513687134
train gradient:  0.16704609985434432
iteration : 4871
train acc:  0.765625
train loss:  0.5239342451095581
train gradient:  0.17973277028571372
iteration : 4872
train acc:  0.703125
train loss:  0.5207282900810242
train gradient:  0.15081489707681436
iteration : 4873
train acc:  0.7421875
train loss:  0.48433712124824524
train gradient:  0.11823977041154166
iteration : 4874
train acc:  0.7421875
train loss:  0.49480676651000977
train gradient:  0.1983975533923792
iteration : 4875
train acc:  0.703125
train loss:  0.574442982673645
train gradient:  0.1421001938826989
iteration : 4876
train acc:  0.765625
train loss:  0.4769725203514099
train gradient:  0.17042636140685452
iteration : 4877
train acc:  0.6796875
train loss:  0.5864981412887573
train gradient:  0.19226688328237657
iteration : 4878
train acc:  0.7421875
train loss:  0.4919460713863373
train gradient:  0.12025053212764766
iteration : 4879
train acc:  0.6640625
train loss:  0.5854394435882568
train gradient:  0.16811444098084083
iteration : 4880
train acc:  0.7421875
train loss:  0.5116946697235107
train gradient:  0.15438806891064072
iteration : 4881
train acc:  0.7578125
train loss:  0.48747262358665466
train gradient:  0.17085803488117546
iteration : 4882
train acc:  0.7421875
train loss:  0.4933597445487976
train gradient:  0.15155728768258597
iteration : 4883
train acc:  0.71875
train loss:  0.5583884716033936
train gradient:  0.20948219194135428
iteration : 4884
train acc:  0.7265625
train loss:  0.5230839252471924
train gradient:  0.1790777842648656
iteration : 4885
train acc:  0.75
train loss:  0.5039929151535034
train gradient:  0.12618678322301313
iteration : 4886
train acc:  0.6796875
train loss:  0.5645485520362854
train gradient:  0.25917450257859115
iteration : 4887
train acc:  0.71875
train loss:  0.5519106388092041
train gradient:  0.15631630056095452
iteration : 4888
train acc:  0.703125
train loss:  0.5400011539459229
train gradient:  0.14921397876854844
iteration : 4889
train acc:  0.75
train loss:  0.48462921380996704
train gradient:  0.13981915687569624
iteration : 4890
train acc:  0.734375
train loss:  0.5501284599304199
train gradient:  0.21503096796799762
iteration : 4891
train acc:  0.7578125
train loss:  0.44895708560943604
train gradient:  0.1417421484781674
iteration : 4892
train acc:  0.8203125
train loss:  0.4432663917541504
train gradient:  0.11820043057722787
iteration : 4893
train acc:  0.65625
train loss:  0.6632550954818726
train gradient:  0.25453840633399677
iteration : 4894
train acc:  0.7578125
train loss:  0.4594517946243286
train gradient:  0.14805410679101036
iteration : 4895
train acc:  0.7109375
train loss:  0.5519352555274963
train gradient:  0.1663784884711653
iteration : 4896
train acc:  0.6796875
train loss:  0.5518258213996887
train gradient:  0.1681748829140179
iteration : 4897
train acc:  0.6796875
train loss:  0.5385842323303223
train gradient:  0.15166353788500064
iteration : 4898
train acc:  0.6640625
train loss:  0.5402877330780029
train gradient:  0.13754445500803428
iteration : 4899
train acc:  0.7578125
train loss:  0.503090500831604
train gradient:  0.1141630997220373
iteration : 4900
train acc:  0.734375
train loss:  0.5273145437240601
train gradient:  0.16116843526015895
iteration : 4901
train acc:  0.7109375
train loss:  0.5098642110824585
train gradient:  0.15767566989379764
iteration : 4902
train acc:  0.6484375
train loss:  0.5935307145118713
train gradient:  0.25602203832765
iteration : 4903
train acc:  0.734375
train loss:  0.4434187412261963
train gradient:  0.11707725141165443
iteration : 4904
train acc:  0.6484375
train loss:  0.6061301231384277
train gradient:  0.20340457698223763
iteration : 4905
train acc:  0.7734375
train loss:  0.4990609884262085
train gradient:  0.15021259795502645
iteration : 4906
train acc:  0.6640625
train loss:  0.5489230155944824
train gradient:  0.14056899545277846
iteration : 4907
train acc:  0.6875
train loss:  0.6006371974945068
train gradient:  0.1858252569264114
iteration : 4908
train acc:  0.7421875
train loss:  0.5452062487602234
train gradient:  0.2035711392377948
iteration : 4909
train acc:  0.734375
train loss:  0.5610837340354919
train gradient:  0.15331237097877504
iteration : 4910
train acc:  0.6953125
train loss:  0.5121785998344421
train gradient:  0.14597484322660093
iteration : 4911
train acc:  0.7421875
train loss:  0.46350976824760437
train gradient:  0.1339033925855699
iteration : 4912
train acc:  0.703125
train loss:  0.5342940092086792
train gradient:  0.1608593426971854
iteration : 4913
train acc:  0.6875
train loss:  0.5352310538291931
train gradient:  0.1747389020229066
iteration : 4914
train acc:  0.6875
train loss:  0.5625108480453491
train gradient:  0.20375772803275843
iteration : 4915
train acc:  0.703125
train loss:  0.5150542259216309
train gradient:  0.17582779938734555
iteration : 4916
train acc:  0.75
train loss:  0.4831039607524872
train gradient:  0.12240006514175751
iteration : 4917
train acc:  0.7109375
train loss:  0.5078242421150208
train gradient:  0.16459106826481404
iteration : 4918
train acc:  0.6875
train loss:  0.5366734266281128
train gradient:  0.15383114832527692
iteration : 4919
train acc:  0.7265625
train loss:  0.5257679224014282
train gradient:  0.16777835475444414
iteration : 4920
train acc:  0.7578125
train loss:  0.4603540301322937
train gradient:  0.11984746075637162
iteration : 4921
train acc:  0.7734375
train loss:  0.5010523796081543
train gradient:  0.15244059116207706
iteration : 4922
train acc:  0.765625
train loss:  0.46877017617225647
train gradient:  0.11508787941559263
iteration : 4923
train acc:  0.734375
train loss:  0.5328660607337952
train gradient:  0.13283414905573498
iteration : 4924
train acc:  0.703125
train loss:  0.5824767351150513
train gradient:  0.20803808377193372
iteration : 4925
train acc:  0.7265625
train loss:  0.5124257206916809
train gradient:  0.16784968954971946
iteration : 4926
train acc:  0.7578125
train loss:  0.4920232892036438
train gradient:  0.14383976115299432
iteration : 4927
train acc:  0.75
train loss:  0.5340290069580078
train gradient:  0.1625664532390585
iteration : 4928
train acc:  0.7734375
train loss:  0.47880426049232483
train gradient:  0.12425568248157061
iteration : 4929
train acc:  0.703125
train loss:  0.5352389812469482
train gradient:  0.15662025772796268
iteration : 4930
train acc:  0.78125
train loss:  0.43597954511642456
train gradient:  0.09867895296683088
iteration : 4931
train acc:  0.7265625
train loss:  0.4943569302558899
train gradient:  0.1619267832843012
iteration : 4932
train acc:  0.6640625
train loss:  0.5579781532287598
train gradient:  0.18384866885281415
iteration : 4933
train acc:  0.703125
train loss:  0.5117183923721313
train gradient:  0.15319239267372536
iteration : 4934
train acc:  0.78125
train loss:  0.5174848437309265
train gradient:  0.1333994677930173
iteration : 4935
train acc:  0.703125
train loss:  0.522720217704773
train gradient:  0.1412624055398689
iteration : 4936
train acc:  0.734375
train loss:  0.5072827339172363
train gradient:  0.13174052629967756
iteration : 4937
train acc:  0.7421875
train loss:  0.4921484589576721
train gradient:  0.11249198463414287
iteration : 4938
train acc:  0.78125
train loss:  0.46886807680130005
train gradient:  0.11913151454092673
iteration : 4939
train acc:  0.7734375
train loss:  0.5158725380897522
train gradient:  0.16161741308372252
iteration : 4940
train acc:  0.6953125
train loss:  0.5590330362319946
train gradient:  0.17618360274832684
iteration : 4941
train acc:  0.6953125
train loss:  0.5139118432998657
train gradient:  0.13827255070057567
iteration : 4942
train acc:  0.75
train loss:  0.5019408464431763
train gradient:  0.1668585128885284
iteration : 4943
train acc:  0.703125
train loss:  0.5355346202850342
train gradient:  0.14418923054921978
iteration : 4944
train acc:  0.734375
train loss:  0.5531041622161865
train gradient:  0.18930411135058872
iteration : 4945
train acc:  0.734375
train loss:  0.5101124048233032
train gradient:  0.1515731806384693
iteration : 4946
train acc:  0.7421875
train loss:  0.5250496864318848
train gradient:  0.13391596825245372
iteration : 4947
train acc:  0.7265625
train loss:  0.5094737410545349
train gradient:  0.125768202550944
iteration : 4948
train acc:  0.703125
train loss:  0.5646257400512695
train gradient:  0.14888668206424022
iteration : 4949
train acc:  0.7578125
train loss:  0.5165448188781738
train gradient:  0.22911762716207007
iteration : 4950
train acc:  0.71875
train loss:  0.4894145727157593
train gradient:  0.15245315513023627
iteration : 4951
train acc:  0.7265625
train loss:  0.5063409805297852
train gradient:  0.15564167639553328
iteration : 4952
train acc:  0.6875
train loss:  0.6139688491821289
train gradient:  0.2525984508622865
iteration : 4953
train acc:  0.7265625
train loss:  0.4998956322669983
train gradient:  0.13337308223735217
iteration : 4954
train acc:  0.671875
train loss:  0.54844731092453
train gradient:  0.16961835073410073
iteration : 4955
train acc:  0.75
train loss:  0.4703524112701416
train gradient:  0.20128980270947938
iteration : 4956
train acc:  0.7109375
train loss:  0.49142327904701233
train gradient:  0.1395497944247284
iteration : 4957
train acc:  0.703125
train loss:  0.5334093570709229
train gradient:  0.17483513661985972
iteration : 4958
train acc:  0.7109375
train loss:  0.5430779457092285
train gradient:  0.15522911879787743
iteration : 4959
train acc:  0.640625
train loss:  0.5944599509239197
train gradient:  0.17597818747464083
iteration : 4960
train acc:  0.703125
train loss:  0.5506967306137085
train gradient:  0.16268742381539478
iteration : 4961
train acc:  0.765625
train loss:  0.46676725149154663
train gradient:  0.11252148370116599
iteration : 4962
train acc:  0.7109375
train loss:  0.5783698558807373
train gradient:  0.16732380030096172
iteration : 4963
train acc:  0.796875
train loss:  0.5091714859008789
train gradient:  0.1686235144432508
iteration : 4964
train acc:  0.703125
train loss:  0.49928557872772217
train gradient:  0.12439732877138902
iteration : 4965
train acc:  0.7734375
train loss:  0.4541790783405304
train gradient:  0.1089070775242564
iteration : 4966
train acc:  0.6484375
train loss:  0.5916246175765991
train gradient:  0.21395124597785634
iteration : 4967
train acc:  0.703125
train loss:  0.5241456627845764
train gradient:  0.14775714787589384
iteration : 4968
train acc:  0.6953125
train loss:  0.547583818435669
train gradient:  0.1647039624623389
iteration : 4969
train acc:  0.765625
train loss:  0.4652221202850342
train gradient:  0.12077795172344147
iteration : 4970
train acc:  0.6953125
train loss:  0.5555862784385681
train gradient:  0.18220678031125584
iteration : 4971
train acc:  0.75
train loss:  0.5281003713607788
train gradient:  0.14593294755773203
iteration : 4972
train acc:  0.6796875
train loss:  0.5358076095581055
train gradient:  0.17723986167852487
iteration : 4973
train acc:  0.703125
train loss:  0.5394775867462158
train gradient:  0.1542963315537414
iteration : 4974
train acc:  0.7890625
train loss:  0.5021715760231018
train gradient:  0.1393323354483898
iteration : 4975
train acc:  0.7578125
train loss:  0.48012617230415344
train gradient:  0.14509001017769504
iteration : 4976
train acc:  0.7265625
train loss:  0.5509743094444275
train gradient:  0.14656703642475422
iteration : 4977
train acc:  0.7421875
train loss:  0.49229753017425537
train gradient:  0.13943431765779635
iteration : 4978
train acc:  0.7265625
train loss:  0.5153286457061768
train gradient:  0.1616755684047072
iteration : 4979
train acc:  0.734375
train loss:  0.5481915473937988
train gradient:  0.17652876075397111
iteration : 4980
train acc:  0.7421875
train loss:  0.5273946523666382
train gradient:  0.15991315120010485
iteration : 4981
train acc:  0.78125
train loss:  0.4434238374233246
train gradient:  0.1373631563006765
iteration : 4982
train acc:  0.7421875
train loss:  0.5111005902290344
train gradient:  0.19219510733579992
iteration : 4983
train acc:  0.7421875
train loss:  0.50044184923172
train gradient:  0.13172083529019646
iteration : 4984
train acc:  0.6953125
train loss:  0.5001131296157837
train gradient:  0.20582799935234014
iteration : 4985
train acc:  0.796875
train loss:  0.44827476143836975
train gradient:  0.12688012530505818
iteration : 4986
train acc:  0.7421875
train loss:  0.48465704917907715
train gradient:  0.12238810383774101
iteration : 4987
train acc:  0.7578125
train loss:  0.4766876995563507
train gradient:  0.13100039050135825
iteration : 4988
train acc:  0.75
train loss:  0.48091214895248413
train gradient:  0.10830997057932108
iteration : 4989
train acc:  0.71875
train loss:  0.5239885449409485
train gradient:  0.14394091600977665
iteration : 4990
train acc:  0.6796875
train loss:  0.5189598798751831
train gradient:  0.15226926074065844
iteration : 4991
train acc:  0.71875
train loss:  0.4879722595214844
train gradient:  0.12537478611919664
iteration : 4992
train acc:  0.71875
train loss:  0.5070258378982544
train gradient:  0.13786848538488808
iteration : 4993
train acc:  0.78125
train loss:  0.40329158306121826
train gradient:  0.09554215291213863
iteration : 4994
train acc:  0.65625
train loss:  0.5801934003829956
train gradient:  0.18550337675639728
iteration : 4995
train acc:  0.734375
train loss:  0.5006604194641113
train gradient:  0.17998205234519704
iteration : 4996
train acc:  0.7578125
train loss:  0.5024842023849487
train gradient:  0.14154649526462162
iteration : 4997
train acc:  0.75
train loss:  0.4809867739677429
train gradient:  0.11778576819873843
iteration : 4998
train acc:  0.734375
train loss:  0.5499515533447266
train gradient:  0.15739100223240327
iteration : 4999
train acc:  0.734375
train loss:  0.5112055540084839
train gradient:  0.18383597988626027
iteration : 5000
train acc:  0.7421875
train loss:  0.527616560459137
train gradient:  0.15220320497497225
iteration : 5001
train acc:  0.6953125
train loss:  0.5099785327911377
train gradient:  0.14518170030526512
iteration : 5002
train acc:  0.6875
train loss:  0.5684230923652649
train gradient:  0.1805094534635084
iteration : 5003
train acc:  0.6953125
train loss:  0.5326310992240906
train gradient:  0.13375316411544508
iteration : 5004
train acc:  0.78125
train loss:  0.4851432144641876
train gradient:  0.14435090453716196
iteration : 5005
train acc:  0.6484375
train loss:  0.5924772024154663
train gradient:  0.18634614859777165
iteration : 5006
train acc:  0.6484375
train loss:  0.6324655413627625
train gradient:  0.17351332701951844
iteration : 5007
train acc:  0.7578125
train loss:  0.4499615430831909
train gradient:  0.1267757854947732
iteration : 5008
train acc:  0.71875
train loss:  0.5109878778457642
train gradient:  0.138349755946621
iteration : 5009
train acc:  0.703125
train loss:  0.528610348701477
train gradient:  0.1577077469083667
iteration : 5010
train acc:  0.7734375
train loss:  0.48394399881362915
train gradient:  0.1293075156798612
iteration : 5011
train acc:  0.6796875
train loss:  0.5172639489173889
train gradient:  0.13463640956251677
iteration : 5012
train acc:  0.71875
train loss:  0.4909536838531494
train gradient:  0.11409403278809933
iteration : 5013
train acc:  0.71875
train loss:  0.5227319002151489
train gradient:  0.17542664763010468
iteration : 5014
train acc:  0.703125
train loss:  0.5277702808380127
train gradient:  0.14788111648637972
iteration : 5015
train acc:  0.7265625
train loss:  0.5118666887283325
train gradient:  0.12746689629638047
iteration : 5016
train acc:  0.7109375
train loss:  0.522870659828186
train gradient:  0.14347474162212834
iteration : 5017
train acc:  0.7421875
train loss:  0.5261827707290649
train gradient:  0.1265567065486136
iteration : 5018
train acc:  0.7578125
train loss:  0.5061947107315063
train gradient:  0.12972153168840045
iteration : 5019
train acc:  0.7265625
train loss:  0.5442368984222412
train gradient:  0.19246246185087115
iteration : 5020
train acc:  0.78125
train loss:  0.4555983543395996
train gradient:  0.1529975451595422
iteration : 5021
train acc:  0.671875
train loss:  0.5455223321914673
train gradient:  0.18088951642551399
iteration : 5022
train acc:  0.796875
train loss:  0.4780423045158386
train gradient:  0.1337501189549658
iteration : 5023
train acc:  0.7265625
train loss:  0.49607276916503906
train gradient:  0.13386918003640708
iteration : 5024
train acc:  0.6953125
train loss:  0.5147671699523926
train gradient:  0.1390291329937566
iteration : 5025
train acc:  0.7734375
train loss:  0.4621569514274597
train gradient:  0.14693476248995824
iteration : 5026
train acc:  0.78125
train loss:  0.4127051830291748
train gradient:  0.11276593175634816
iteration : 5027
train acc:  0.6484375
train loss:  0.6598515510559082
train gradient:  0.18826328561150427
iteration : 5028
train acc:  0.65625
train loss:  0.570243239402771
train gradient:  0.20080157046748504
iteration : 5029
train acc:  0.7421875
train loss:  0.4693964123725891
train gradient:  0.10980046281152507
iteration : 5030
train acc:  0.703125
train loss:  0.5101155042648315
train gradient:  0.14283697664280987
iteration : 5031
train acc:  0.6796875
train loss:  0.5441634058952332
train gradient:  0.1619742330102808
iteration : 5032
train acc:  0.7890625
train loss:  0.47776490449905396
train gradient:  0.11855271332208518
iteration : 5033
train acc:  0.765625
train loss:  0.46934378147125244
train gradient:  0.10303525381160503
iteration : 5034
train acc:  0.71875
train loss:  0.5097529292106628
train gradient:  0.13529480021310708
iteration : 5035
train acc:  0.7734375
train loss:  0.47758230566978455
train gradient:  0.1466302663714305
iteration : 5036
train acc:  0.75
train loss:  0.5016640424728394
train gradient:  0.18107636421304163
iteration : 5037
train acc:  0.7890625
train loss:  0.5490686893463135
train gradient:  0.16152753529512098
iteration : 5038
train acc:  0.734375
train loss:  0.49103060364723206
train gradient:  0.14350998469815784
iteration : 5039
train acc:  0.78125
train loss:  0.45675981044769287
train gradient:  0.1131446032252999
iteration : 5040
train acc:  0.71875
train loss:  0.5481153130531311
train gradient:  0.15737729589377283
iteration : 5041
train acc:  0.765625
train loss:  0.43113189935684204
train gradient:  0.11664476275148146
iteration : 5042
train acc:  0.7734375
train loss:  0.4507470428943634
train gradient:  0.14167343461862666
iteration : 5043
train acc:  0.6875
train loss:  0.5021706819534302
train gradient:  0.13896105063520234
iteration : 5044
train acc:  0.640625
train loss:  0.5869684219360352
train gradient:  0.1705854927906626
iteration : 5045
train acc:  0.75
train loss:  0.47954264283180237
train gradient:  0.13064444859119012
iteration : 5046
train acc:  0.7578125
train loss:  0.5469181537628174
train gradient:  0.14353617288576595
iteration : 5047
train acc:  0.7109375
train loss:  0.5396517515182495
train gradient:  0.1552899761734861
iteration : 5048
train acc:  0.6953125
train loss:  0.541447639465332
train gradient:  0.16239438768860714
iteration : 5049
train acc:  0.7734375
train loss:  0.46033990383148193
train gradient:  0.11765833245288357
iteration : 5050
train acc:  0.6875
train loss:  0.5613049268722534
train gradient:  0.1618106542109991
iteration : 5051
train acc:  0.703125
train loss:  0.5942887663841248
train gradient:  0.17441725907857533
iteration : 5052
train acc:  0.6875
train loss:  0.49891361594200134
train gradient:  0.12485478644532089
iteration : 5053
train acc:  0.6875
train loss:  0.5978463292121887
train gradient:  0.17363034755334963
iteration : 5054
train acc:  0.7578125
train loss:  0.47435465455055237
train gradient:  0.1364948374164004
iteration : 5055
train acc:  0.7109375
train loss:  0.5279154181480408
train gradient:  0.12913397844243862
iteration : 5056
train acc:  0.734375
train loss:  0.4827282428741455
train gradient:  0.12418586992355804
iteration : 5057
train acc:  0.7265625
train loss:  0.4969683289527893
train gradient:  0.11443573713694999
iteration : 5058
train acc:  0.6796875
train loss:  0.618513822555542
train gradient:  0.25624651089606004
iteration : 5059
train acc:  0.7265625
train loss:  0.494533896446228
train gradient:  0.14470956045701483
iteration : 5060
train acc:  0.7109375
train loss:  0.5311630368232727
train gradient:  0.13810030809918666
iteration : 5061
train acc:  0.8125
train loss:  0.4378243088722229
train gradient:  0.11632098102660676
iteration : 5062
train acc:  0.765625
train loss:  0.4835418462753296
train gradient:  0.12477825024935012
iteration : 5063
train acc:  0.765625
train loss:  0.4722933769226074
train gradient:  0.1405064060986372
iteration : 5064
train acc:  0.734375
train loss:  0.49432116746902466
train gradient:  0.16728957202880107
iteration : 5065
train acc:  0.7265625
train loss:  0.4918365776538849
train gradient:  0.14885658867603951
iteration : 5066
train acc:  0.71875
train loss:  0.5067969560623169
train gradient:  0.1407918322663147
iteration : 5067
train acc:  0.6640625
train loss:  0.5908766984939575
train gradient:  0.2067044266002337
iteration : 5068
train acc:  0.7578125
train loss:  0.42267146706581116
train gradient:  0.10429881266583256
iteration : 5069
train acc:  0.828125
train loss:  0.4502931237220764
train gradient:  0.10507836395608168
iteration : 5070
train acc:  0.7109375
train loss:  0.552736759185791
train gradient:  0.15191310843694905
iteration : 5071
train acc:  0.7265625
train loss:  0.5264149308204651
train gradient:  0.1846840472083658
iteration : 5072
train acc:  0.734375
train loss:  0.5010418891906738
train gradient:  0.14341478992475384
iteration : 5073
train acc:  0.6953125
train loss:  0.5840691328048706
train gradient:  0.1670019710002053
iteration : 5074
train acc:  0.7109375
train loss:  0.4779784083366394
train gradient:  0.1415747692562403
iteration : 5075
train acc:  0.7421875
train loss:  0.4820878207683563
train gradient:  0.1689268289837298
iteration : 5076
train acc:  0.7734375
train loss:  0.4699375033378601
train gradient:  0.17994588041716414
iteration : 5077
train acc:  0.6796875
train loss:  0.5613434314727783
train gradient:  0.17695807257784124
iteration : 5078
train acc:  0.7734375
train loss:  0.5410906076431274
train gradient:  0.14468627259118833
iteration : 5079
train acc:  0.703125
train loss:  0.5276805758476257
train gradient:  0.15771012164874532
iteration : 5080
train acc:  0.7578125
train loss:  0.5069695711135864
train gradient:  0.12554847434215738
iteration : 5081
train acc:  0.71875
train loss:  0.5218982696533203
train gradient:  0.17039821545500994
iteration : 5082
train acc:  0.71875
train loss:  0.5198138952255249
train gradient:  0.13654241027280245
iteration : 5083
train acc:  0.71875
train loss:  0.5087513327598572
train gradient:  0.13299286927966458
iteration : 5084
train acc:  0.78125
train loss:  0.45662569999694824
train gradient:  0.13259827263745455
iteration : 5085
train acc:  0.7734375
train loss:  0.49137455224990845
train gradient:  0.13978674729487484
iteration : 5086
train acc:  0.75
train loss:  0.4953494966030121
train gradient:  0.11811150644998022
iteration : 5087
train acc:  0.7265625
train loss:  0.5409595966339111
train gradient:  0.17691287412345963
iteration : 5088
train acc:  0.765625
train loss:  0.47678443789482117
train gradient:  0.1411381629214034
iteration : 5089
train acc:  0.7265625
train loss:  0.5373400449752808
train gradient:  0.13906237734161564
iteration : 5090
train acc:  0.7265625
train loss:  0.4781559705734253
train gradient:  0.13371128391734488
iteration : 5091
train acc:  0.7265625
train loss:  0.5441826581954956
train gradient:  0.15876927094477264
iteration : 5092
train acc:  0.8203125
train loss:  0.4321286380290985
train gradient:  0.11400882381920781
iteration : 5093
train acc:  0.796875
train loss:  0.44671007990837097
train gradient:  0.09448077995069128
iteration : 5094
train acc:  0.6953125
train loss:  0.5538055896759033
train gradient:  0.1771389262763915
iteration : 5095
train acc:  0.71875
train loss:  0.5468644499778748
train gradient:  0.16760637596392325
iteration : 5096
train acc:  0.703125
train loss:  0.5045031309127808
train gradient:  0.13496756234337726
iteration : 5097
train acc:  0.7734375
train loss:  0.4689349830150604
train gradient:  0.1364426216596917
iteration : 5098
train acc:  0.7734375
train loss:  0.49061068892478943
train gradient:  0.153973751131023
iteration : 5099
train acc:  0.71875
train loss:  0.5320786237716675
train gradient:  0.16522729480448478
iteration : 5100
train acc:  0.7421875
train loss:  0.49659550189971924
train gradient:  0.21471668442274672
iteration : 5101
train acc:  0.75
train loss:  0.5270686745643616
train gradient:  0.1445116240674224
iteration : 5102
train acc:  0.671875
train loss:  0.5744537115097046
train gradient:  0.17586424937486966
iteration : 5103
train acc:  0.6640625
train loss:  0.5850522518157959
train gradient:  0.19777478037830143
iteration : 5104
train acc:  0.7578125
train loss:  0.483419269323349
train gradient:  0.12825925420878753
iteration : 5105
train acc:  0.6640625
train loss:  0.5603320598602295
train gradient:  0.18411729400947735
iteration : 5106
train acc:  0.7578125
train loss:  0.5294286012649536
train gradient:  0.15895175392781835
iteration : 5107
train acc:  0.765625
train loss:  0.48532724380493164
train gradient:  0.16452045302226753
iteration : 5108
train acc:  0.7265625
train loss:  0.5188224911689758
train gradient:  0.14211406329714998
iteration : 5109
train acc:  0.7734375
train loss:  0.5058501958847046
train gradient:  0.12441682878886054
iteration : 5110
train acc:  0.65625
train loss:  0.5589807033538818
train gradient:  0.1567799256949064
iteration : 5111
train acc:  0.7421875
train loss:  0.4819377660751343
train gradient:  0.12232811338444022
iteration : 5112
train acc:  0.6875
train loss:  0.5935823917388916
train gradient:  0.16375665717667914
iteration : 5113
train acc:  0.7421875
train loss:  0.5235281586647034
train gradient:  0.1428168953437377
iteration : 5114
train acc:  0.7890625
train loss:  0.4849702715873718
train gradient:  0.1282191814078385
iteration : 5115
train acc:  0.734375
train loss:  0.5046699643135071
train gradient:  0.13499578807166862
iteration : 5116
train acc:  0.703125
train loss:  0.521903395652771
train gradient:  0.185493709677054
iteration : 5117
train acc:  0.7734375
train loss:  0.4695214629173279
train gradient:  0.11876724818801099
iteration : 5118
train acc:  0.7109375
train loss:  0.4833613634109497
train gradient:  0.15673292313847392
iteration : 5119
train acc:  0.7578125
train loss:  0.49124595522880554
train gradient:  0.14767851997117534
iteration : 5120
train acc:  0.7265625
train loss:  0.4876019358634949
train gradient:  0.12965308316363575
iteration : 5121
train acc:  0.7421875
train loss:  0.4480704665184021
train gradient:  0.11456508540341775
iteration : 5122
train acc:  0.703125
train loss:  0.5311427712440491
train gradient:  0.15749573061055094
iteration : 5123
train acc:  0.7265625
train loss:  0.5494060516357422
train gradient:  0.1389758652239084
iteration : 5124
train acc:  0.71875
train loss:  0.5453108549118042
train gradient:  0.1569989428476693
iteration : 5125
train acc:  0.6484375
train loss:  0.5821751356124878
train gradient:  0.2191327412461368
iteration : 5126
train acc:  0.7421875
train loss:  0.4570811986923218
train gradient:  0.14049453350411134
iteration : 5127
train acc:  0.671875
train loss:  0.5483414530754089
train gradient:  0.14072644279926785
iteration : 5128
train acc:  0.703125
train loss:  0.5454775094985962
train gradient:  0.16745270278704005
iteration : 5129
train acc:  0.75
train loss:  0.47369927167892456
train gradient:  0.15813308907516177
iteration : 5130
train acc:  0.5859375
train loss:  0.6446736454963684
train gradient:  0.22791318357581958
iteration : 5131
train acc:  0.6953125
train loss:  0.5057952404022217
train gradient:  0.18304695245235567
iteration : 5132
train acc:  0.7734375
train loss:  0.46607059240341187
train gradient:  0.16342964707456592
iteration : 5133
train acc:  0.765625
train loss:  0.4722979664802551
train gradient:  0.13121698926631464
iteration : 5134
train acc:  0.78125
train loss:  0.4850863814353943
train gradient:  0.12410677824157423
iteration : 5135
train acc:  0.6953125
train loss:  0.5521489381790161
train gradient:  0.16272717393607328
iteration : 5136
train acc:  0.703125
train loss:  0.5285714864730835
train gradient:  0.1500864140049534
iteration : 5137
train acc:  0.7109375
train loss:  0.49901673197746277
train gradient:  0.13736889604192848
iteration : 5138
train acc:  0.7578125
train loss:  0.47742700576782227
train gradient:  0.13266673330028017
iteration : 5139
train acc:  0.7109375
train loss:  0.5811644792556763
train gradient:  0.1802149233166428
iteration : 5140
train acc:  0.765625
train loss:  0.4751614034175873
train gradient:  0.1451908421133448
iteration : 5141
train acc:  0.8046875
train loss:  0.44087356328964233
train gradient:  0.11811084578904903
iteration : 5142
train acc:  0.75
train loss:  0.4652034044265747
train gradient:  0.11753925776500966
iteration : 5143
train acc:  0.703125
train loss:  0.5118606090545654
train gradient:  0.17150182065465672
iteration : 5144
train acc:  0.7890625
train loss:  0.4811221957206726
train gradient:  0.13495149476546572
iteration : 5145
train acc:  0.703125
train loss:  0.5406898856163025
train gradient:  0.14610731576037375
iteration : 5146
train acc:  0.7265625
train loss:  0.5515391826629639
train gradient:  0.21230025803945413
iteration : 5147
train acc:  0.6796875
train loss:  0.6022667288780212
train gradient:  0.18909976016182772
iteration : 5148
train acc:  0.7109375
train loss:  0.5608383417129517
train gradient:  0.13387898012294516
iteration : 5149
train acc:  0.7421875
train loss:  0.528681755065918
train gradient:  0.16475845098092107
iteration : 5150
train acc:  0.734375
train loss:  0.4509996771812439
train gradient:  0.09881950823577876
iteration : 5151
train acc:  0.640625
train loss:  0.548781156539917
train gradient:  0.12614714969438295
iteration : 5152
train acc:  0.7421875
train loss:  0.4747581481933594
train gradient:  0.16085059498020887
iteration : 5153
train acc:  0.7578125
train loss:  0.5525333881378174
train gradient:  0.22608777176254513
iteration : 5154
train acc:  0.7421875
train loss:  0.5167636871337891
train gradient:  0.13222009928241807
iteration : 5155
train acc:  0.6875
train loss:  0.5421915650367737
train gradient:  0.1605563476300539
iteration : 5156
train acc:  0.734375
train loss:  0.5548067092895508
train gradient:  0.15190754450941135
iteration : 5157
train acc:  0.671875
train loss:  0.5655021071434021
train gradient:  0.15961041344521532
iteration : 5158
train acc:  0.734375
train loss:  0.5038310885429382
train gradient:  0.11717284154898225
iteration : 5159
train acc:  0.7265625
train loss:  0.48438236117362976
train gradient:  0.15850019296192894
iteration : 5160
train acc:  0.7109375
train loss:  0.5451271533966064
train gradient:  0.1952480272287359
iteration : 5161
train acc:  0.734375
train loss:  0.5198429822921753
train gradient:  0.18853384114460414
iteration : 5162
train acc:  0.8359375
train loss:  0.38992831110954285
train gradient:  0.10855537325170642
iteration : 5163
train acc:  0.7265625
train loss:  0.49725669622421265
train gradient:  0.1366795393459636
iteration : 5164
train acc:  0.8046875
train loss:  0.46412336826324463
train gradient:  0.12167590214550236
iteration : 5165
train acc:  0.6875
train loss:  0.5376921892166138
train gradient:  0.15700331611534393
iteration : 5166
train acc:  0.734375
train loss:  0.5129457712173462
train gradient:  0.15712919622805094
iteration : 5167
train acc:  0.75
train loss:  0.478177934885025
train gradient:  0.1398274685255817
iteration : 5168
train acc:  0.75
train loss:  0.48995548486709595
train gradient:  0.14888821672203933
iteration : 5169
train acc:  0.7109375
train loss:  0.5458569526672363
train gradient:  0.15405464168192964
iteration : 5170
train acc:  0.671875
train loss:  0.6109579801559448
train gradient:  0.2283890462515134
iteration : 5171
train acc:  0.734375
train loss:  0.5046188235282898
train gradient:  0.161519386407443
iteration : 5172
train acc:  0.7578125
train loss:  0.46744588017463684
train gradient:  0.14877107812395612
iteration : 5173
train acc:  0.6953125
train loss:  0.579504132270813
train gradient:  0.1665383704751362
iteration : 5174
train acc:  0.75
train loss:  0.508587658405304
train gradient:  0.1698644342079796
iteration : 5175
train acc:  0.7265625
train loss:  0.5542757511138916
train gradient:  0.15733657681601726
iteration : 5176
train acc:  0.75
train loss:  0.5211774110794067
train gradient:  0.15277076662818956
iteration : 5177
train acc:  0.703125
train loss:  0.5189194083213806
train gradient:  0.18608620752148597
iteration : 5178
train acc:  0.7734375
train loss:  0.4544264078140259
train gradient:  0.0986477716017624
iteration : 5179
train acc:  0.703125
train loss:  0.5594881772994995
train gradient:  0.17159371443223986
iteration : 5180
train acc:  0.75
train loss:  0.4915180206298828
train gradient:  0.11500539292327655
iteration : 5181
train acc:  0.7421875
train loss:  0.5410423278808594
train gradient:  0.15197393599816078
iteration : 5182
train acc:  0.7734375
train loss:  0.4524792432785034
train gradient:  0.12002202394974505
iteration : 5183
train acc:  0.65625
train loss:  0.586044430732727
train gradient:  0.18850791630597397
iteration : 5184
train acc:  0.703125
train loss:  0.5295767784118652
train gradient:  0.1739477752175202
iteration : 5185
train acc:  0.6796875
train loss:  0.5536437630653381
train gradient:  0.16206339852205504
iteration : 5186
train acc:  0.6953125
train loss:  0.5383145809173584
train gradient:  0.14723415689480346
iteration : 5187
train acc:  0.6953125
train loss:  0.6079264879226685
train gradient:  0.16904804598762002
iteration : 5188
train acc:  0.7890625
train loss:  0.47735345363616943
train gradient:  0.13614783744417813
iteration : 5189
train acc:  0.765625
train loss:  0.4958295226097107
train gradient:  0.17299561688701748
iteration : 5190
train acc:  0.765625
train loss:  0.47696998715400696
train gradient:  0.12825539720535017
iteration : 5191
train acc:  0.7109375
train loss:  0.551534116268158
train gradient:  0.17914192911994492
iteration : 5192
train acc:  0.6953125
train loss:  0.5250614881515503
train gradient:  0.14266585845613
iteration : 5193
train acc:  0.7734375
train loss:  0.4456448554992676
train gradient:  0.11478436127422775
iteration : 5194
train acc:  0.7734375
train loss:  0.4609435796737671
train gradient:  0.1070863258838489
iteration : 5195
train acc:  0.6875
train loss:  0.5751376152038574
train gradient:  0.1481593239183766
iteration : 5196
train acc:  0.7890625
train loss:  0.4870140552520752
train gradient:  0.15036082383650906
iteration : 5197
train acc:  0.7578125
train loss:  0.44970008730888367
train gradient:  0.1310666404037418
iteration : 5198
train acc:  0.6875
train loss:  0.4804292917251587
train gradient:  0.10290783173728527
iteration : 5199
train acc:  0.7109375
train loss:  0.5248757004737854
train gradient:  0.150228746328469
iteration : 5200
train acc:  0.75
train loss:  0.558293342590332
train gradient:  0.1773481303992916
iteration : 5201
train acc:  0.78125
train loss:  0.4461580812931061
train gradient:  0.12266543956098072
iteration : 5202
train acc:  0.6875
train loss:  0.5229071378707886
train gradient:  0.14643473315008107
iteration : 5203
train acc:  0.6796875
train loss:  0.5740649104118347
train gradient:  0.19705861055767127
iteration : 5204
train acc:  0.703125
train loss:  0.5809460878372192
train gradient:  0.1933906014436671
iteration : 5205
train acc:  0.78125
train loss:  0.4569752812385559
train gradient:  0.11062763750824062
iteration : 5206
train acc:  0.7265625
train loss:  0.5276902914047241
train gradient:  0.16189378227231033
iteration : 5207
train acc:  0.734375
train loss:  0.4645622968673706
train gradient:  0.14128194123868476
iteration : 5208
train acc:  0.6875
train loss:  0.5523213744163513
train gradient:  0.16158706526822586
iteration : 5209
train acc:  0.75
train loss:  0.511703610420227
train gradient:  0.17407537190340955
iteration : 5210
train acc:  0.7578125
train loss:  0.487911194562912
train gradient:  0.12253746141052511
iteration : 5211
train acc:  0.734375
train loss:  0.5352420210838318
train gradient:  0.15640027746503435
iteration : 5212
train acc:  0.671875
train loss:  0.5482355952262878
train gradient:  0.18288365679796903
iteration : 5213
train acc:  0.7109375
train loss:  0.5465273857116699
train gradient:  0.19310257249763463
iteration : 5214
train acc:  0.6875
train loss:  0.5419774651527405
train gradient:  0.20503641641488363
iteration : 5215
train acc:  0.7421875
train loss:  0.5185720920562744
train gradient:  0.1595997524532443
iteration : 5216
train acc:  0.78125
train loss:  0.4560842514038086
train gradient:  0.12598493809159275
iteration : 5217
train acc:  0.6875
train loss:  0.5574386715888977
train gradient:  0.17299073200226464
iteration : 5218
train acc:  0.7265625
train loss:  0.5013213157653809
train gradient:  0.13135603002841623
iteration : 5219
train acc:  0.734375
train loss:  0.5378095507621765
train gradient:  0.16691294458625097
iteration : 5220
train acc:  0.765625
train loss:  0.4312452971935272
train gradient:  0.09355010949027047
iteration : 5221
train acc:  0.7421875
train loss:  0.4986304044723511
train gradient:  0.11787162397398705
iteration : 5222
train acc:  0.7109375
train loss:  0.542128324508667
train gradient:  0.13761638219011807
iteration : 5223
train acc:  0.7109375
train loss:  0.5435572862625122
train gradient:  0.14879019699028267
iteration : 5224
train acc:  0.8203125
train loss:  0.43673813343048096
train gradient:  0.14131299476883435
iteration : 5225
train acc:  0.71875
train loss:  0.5169317722320557
train gradient:  0.14855765388818615
iteration : 5226
train acc:  0.7265625
train loss:  0.5561590194702148
train gradient:  0.1422492734653057
iteration : 5227
train acc:  0.71875
train loss:  0.5207059383392334
train gradient:  0.1507030964722236
iteration : 5228
train acc:  0.765625
train loss:  0.5125356316566467
train gradient:  0.17282990853049074
iteration : 5229
train acc:  0.765625
train loss:  0.4940418601036072
train gradient:  0.15366303346752042
iteration : 5230
train acc:  0.7265625
train loss:  0.4962814152240753
train gradient:  0.13047740968866578
iteration : 5231
train acc:  0.765625
train loss:  0.4867019057273865
train gradient:  0.14725247200096953
iteration : 5232
train acc:  0.71875
train loss:  0.5119457244873047
train gradient:  0.11878600529127924
iteration : 5233
train acc:  0.703125
train loss:  0.5016905069351196
train gradient:  0.1792797204070677
iteration : 5234
train acc:  0.734375
train loss:  0.5193936824798584
train gradient:  0.13234068122722709
iteration : 5235
train acc:  0.71875
train loss:  0.535751223564148
train gradient:  0.16739741858983032
iteration : 5236
train acc:  0.6953125
train loss:  0.6106472611427307
train gradient:  0.2428840902668582
iteration : 5237
train acc:  0.7578125
train loss:  0.47327548265457153
train gradient:  0.15434814326955648
iteration : 5238
train acc:  0.671875
train loss:  0.6118241548538208
train gradient:  0.20229324198698753
iteration : 5239
train acc:  0.7734375
train loss:  0.4570445120334625
train gradient:  0.11806271753807517
iteration : 5240
train acc:  0.7421875
train loss:  0.4865347146987915
train gradient:  0.11366678598274249
iteration : 5241
train acc:  0.703125
train loss:  0.5359101295471191
train gradient:  0.15040297958684068
iteration : 5242
train acc:  0.75
train loss:  0.49245303869247437
train gradient:  0.18147033482320496
iteration : 5243
train acc:  0.78125
train loss:  0.48849040269851685
train gradient:  0.128726700073942
iteration : 5244
train acc:  0.7109375
train loss:  0.49916815757751465
train gradient:  0.13904992989567272
iteration : 5245
train acc:  0.75
train loss:  0.5051593780517578
train gradient:  0.13678728807395785
iteration : 5246
train acc:  0.6875
train loss:  0.5682758688926697
train gradient:  0.14654007940500152
iteration : 5247
train acc:  0.7421875
train loss:  0.5170406103134155
train gradient:  0.13752338925255342
iteration : 5248
train acc:  0.8125
train loss:  0.45430493354797363
train gradient:  0.13155986891345056
iteration : 5249
train acc:  0.6875
train loss:  0.5668229460716248
train gradient:  0.17946657738050908
iteration : 5250
train acc:  0.765625
train loss:  0.4435637593269348
train gradient:  0.10224009028331792
iteration : 5251
train acc:  0.703125
train loss:  0.591819167137146
train gradient:  0.25847204846669797
iteration : 5252
train acc:  0.703125
train loss:  0.5142745971679688
train gradient:  0.14670159162201268
iteration : 5253
train acc:  0.7421875
train loss:  0.4893949031829834
train gradient:  0.13019757657337352
iteration : 5254
train acc:  0.671875
train loss:  0.5912115573883057
train gradient:  0.18035627496283688
iteration : 5255
train acc:  0.7578125
train loss:  0.4941484034061432
train gradient:  0.1288686575373729
iteration : 5256
train acc:  0.6953125
train loss:  0.5914209485054016
train gradient:  0.23063549993384286
iteration : 5257
train acc:  0.671875
train loss:  0.6327018737792969
train gradient:  0.23416907569667028
iteration : 5258
train acc:  0.6953125
train loss:  0.5470671653747559
train gradient:  0.15912854046858305
iteration : 5259
train acc:  0.7578125
train loss:  0.48249438405036926
train gradient:  0.19872183556235856
iteration : 5260
train acc:  0.71875
train loss:  0.47663822770118713
train gradient:  0.11787705369177251
iteration : 5261
train acc:  0.7265625
train loss:  0.5441344976425171
train gradient:  0.1448064693196322
iteration : 5262
train acc:  0.7109375
train loss:  0.511265218257904
train gradient:  0.1426166099393647
iteration : 5263
train acc:  0.78125
train loss:  0.46994292736053467
train gradient:  0.11621843660340174
iteration : 5264
train acc:  0.7265625
train loss:  0.5154625773429871
train gradient:  0.12251603843333331
iteration : 5265
train acc:  0.7734375
train loss:  0.4696381390094757
train gradient:  0.13398728059436693
iteration : 5266
train acc:  0.7265625
train loss:  0.5197054743766785
train gradient:  0.14459115504613532
iteration : 5267
train acc:  0.71875
train loss:  0.5367333292961121
train gradient:  0.17273144070186555
iteration : 5268
train acc:  0.640625
train loss:  0.5599664449691772
train gradient:  0.15907736244104337
iteration : 5269
train acc:  0.6953125
train loss:  0.6095904111862183
train gradient:  0.18666865173033168
iteration : 5270
train acc:  0.796875
train loss:  0.45505034923553467
train gradient:  0.12636581334850105
iteration : 5271
train acc:  0.7109375
train loss:  0.5113679766654968
train gradient:  0.1773448167100421
iteration : 5272
train acc:  0.7578125
train loss:  0.49091243743896484
train gradient:  0.12426571808791974
iteration : 5273
train acc:  0.6640625
train loss:  0.569561243057251
train gradient:  0.1948530393438061
iteration : 5274
train acc:  0.7109375
train loss:  0.5289687514305115
train gradient:  0.134822972750934
iteration : 5275
train acc:  0.7890625
train loss:  0.4690728187561035
train gradient:  0.16031558811185714
iteration : 5276
train acc:  0.71875
train loss:  0.5287689566612244
train gradient:  0.1402202264821808
iteration : 5277
train acc:  0.640625
train loss:  0.5731712579727173
train gradient:  0.15201005704709483
iteration : 5278
train acc:  0.7578125
train loss:  0.4481619596481323
train gradient:  0.12819802544637532
iteration : 5279
train acc:  0.75
train loss:  0.5449202656745911
train gradient:  0.1476950044734076
iteration : 5280
train acc:  0.703125
train loss:  0.5464984178543091
train gradient:  0.17021645430830268
iteration : 5281
train acc:  0.7109375
train loss:  0.5629705190658569
train gradient:  0.2078412148993149
iteration : 5282
train acc:  0.703125
train loss:  0.549187183380127
train gradient:  0.16855637395452405
iteration : 5283
train acc:  0.703125
train loss:  0.4953230023384094
train gradient:  0.15319357054860705
iteration : 5284
train acc:  0.7578125
train loss:  0.4913468062877655
train gradient:  0.15379275596733416
iteration : 5285
train acc:  0.703125
train loss:  0.562837541103363
train gradient:  0.16528451220094653
iteration : 5286
train acc:  0.7734375
train loss:  0.4598076045513153
train gradient:  0.11272385376083555
iteration : 5287
train acc:  0.75
train loss:  0.48835521936416626
train gradient:  0.12936425685582564
iteration : 5288
train acc:  0.7109375
train loss:  0.5787264108657837
train gradient:  0.18721027756186664
iteration : 5289
train acc:  0.6796875
train loss:  0.5521141886711121
train gradient:  0.16266793973811405
iteration : 5290
train acc:  0.7734375
train loss:  0.4380606412887573
train gradient:  0.12003038119219102
iteration : 5291
train acc:  0.7109375
train loss:  0.5280305743217468
train gradient:  0.14987390602477252
iteration : 5292
train acc:  0.75
train loss:  0.4602223038673401
train gradient:  0.11160793615241417
iteration : 5293
train acc:  0.7265625
train loss:  0.5103683471679688
train gradient:  0.12725047249618093
iteration : 5294
train acc:  0.75
train loss:  0.48518437147140503
train gradient:  0.1308375600057286
iteration : 5295
train acc:  0.734375
train loss:  0.46606695652008057
train gradient:  0.09990929422059085
iteration : 5296
train acc:  0.75
train loss:  0.5032330751419067
train gradient:  0.21552490914472971
iteration : 5297
train acc:  0.734375
train loss:  0.5346481800079346
train gradient:  0.13061663154082662
iteration : 5298
train acc:  0.7109375
train loss:  0.5019549131393433
train gradient:  0.1313651129006842
iteration : 5299
train acc:  0.75
train loss:  0.5500698685646057
train gradient:  0.19330043657874735
iteration : 5300
train acc:  0.671875
train loss:  0.5651524066925049
train gradient:  0.1873510475680883
iteration : 5301
train acc:  0.7734375
train loss:  0.47169604897499084
train gradient:  0.13234703786260554
iteration : 5302
train acc:  0.78125
train loss:  0.4527750611305237
train gradient:  0.0991021378943316
iteration : 5303
train acc:  0.703125
train loss:  0.504149854183197
train gradient:  0.12713861956714453
iteration : 5304
train acc:  0.734375
train loss:  0.5061036348342896
train gradient:  0.19869603671421937
iteration : 5305
train acc:  0.7265625
train loss:  0.48345160484313965
train gradient:  0.12326465435905266
iteration : 5306
train acc:  0.765625
train loss:  0.4951150119304657
train gradient:  0.158722813134341
iteration : 5307
train acc:  0.75
train loss:  0.5023236274719238
train gradient:  0.15080969307236197
iteration : 5308
train acc:  0.71875
train loss:  0.5248708128929138
train gradient:  0.12719941046057132
iteration : 5309
train acc:  0.7578125
train loss:  0.49161040782928467
train gradient:  0.14360988967357013
iteration : 5310
train acc:  0.65625
train loss:  0.5718005299568176
train gradient:  0.16602922208173776
iteration : 5311
train acc:  0.7578125
train loss:  0.5137357711791992
train gradient:  0.14392207398869197
iteration : 5312
train acc:  0.6875
train loss:  0.5842127799987793
train gradient:  0.15806356096751256
iteration : 5313
train acc:  0.71875
train loss:  0.564816415309906
train gradient:  0.15715679003589184
iteration : 5314
train acc:  0.6953125
train loss:  0.5512793660163879
train gradient:  0.214842110262858
iteration : 5315
train acc:  0.7578125
train loss:  0.46797171235084534
train gradient:  0.14310548393081157
iteration : 5316
train acc:  0.6640625
train loss:  0.58259117603302
train gradient:  0.19275094047894376
iteration : 5317
train acc:  0.7265625
train loss:  0.5308517217636108
train gradient:  0.14502258692648248
iteration : 5318
train acc:  0.734375
train loss:  0.5191497206687927
train gradient:  0.20221599805834017
iteration : 5319
train acc:  0.6640625
train loss:  0.5675423741340637
train gradient:  0.16173842725203913
iteration : 5320
train acc:  0.6796875
train loss:  0.5870485901832581
train gradient:  0.17747203405678819
iteration : 5321
train acc:  0.7421875
train loss:  0.5014604926109314
train gradient:  0.1427225717900095
iteration : 5322
train acc:  0.7421875
train loss:  0.4819905757904053
train gradient:  0.13990784481460056
iteration : 5323
train acc:  0.75
train loss:  0.49494582414627075
train gradient:  0.13977799170807115
iteration : 5324
train acc:  0.7421875
train loss:  0.5416624546051025
train gradient:  0.14686481859005673
iteration : 5325
train acc:  0.7734375
train loss:  0.4822732210159302
train gradient:  0.13641424535393146
iteration : 5326
train acc:  0.7734375
train loss:  0.46069878339767456
train gradient:  0.10641015539365913
iteration : 5327
train acc:  0.7734375
train loss:  0.4825732111930847
train gradient:  0.11297953774196866
iteration : 5328
train acc:  0.734375
train loss:  0.5258931517601013
train gradient:  0.1283837517854236
iteration : 5329
train acc:  0.7734375
train loss:  0.4955448806285858
train gradient:  0.119947436997558
iteration : 5330
train acc:  0.703125
train loss:  0.5564023852348328
train gradient:  0.18431364257898328
iteration : 5331
train acc:  0.6171875
train loss:  0.6522798538208008
train gradient:  0.2961625284990807
iteration : 5332
train acc:  0.703125
train loss:  0.5038685202598572
train gradient:  0.12662774420518977
iteration : 5333
train acc:  0.6484375
train loss:  0.5376379489898682
train gradient:  0.19649885183332433
iteration : 5334
train acc:  0.765625
train loss:  0.5202342867851257
train gradient:  0.14853484090281555
iteration : 5335
train acc:  0.734375
train loss:  0.5149075388908386
train gradient:  0.12768280450678837
iteration : 5336
train acc:  0.734375
train loss:  0.5259915590286255
train gradient:  0.1959444786541923
iteration : 5337
train acc:  0.734375
train loss:  0.48438793420791626
train gradient:  0.1477842079725476
iteration : 5338
train acc:  0.7109375
train loss:  0.5274841785430908
train gradient:  0.1271392827351553
iteration : 5339
train acc:  0.7421875
train loss:  0.49153557419776917
train gradient:  0.14486506721742687
iteration : 5340
train acc:  0.7421875
train loss:  0.5807167887687683
train gradient:  0.1941833105586796
iteration : 5341
train acc:  0.7421875
train loss:  0.48812562227249146
train gradient:  0.10505722287684027
iteration : 5342
train acc:  0.75
train loss:  0.5270901322364807
train gradient:  0.13198011792709857
iteration : 5343
train acc:  0.7890625
train loss:  0.4475119113922119
train gradient:  0.0991166676721381
iteration : 5344
train acc:  0.671875
train loss:  0.558742880821228
train gradient:  0.15370789322488831
iteration : 5345
train acc:  0.6953125
train loss:  0.547958493232727
train gradient:  0.16162752033306432
iteration : 5346
train acc:  0.7890625
train loss:  0.48389044404029846
train gradient:  0.1299480308006224
iteration : 5347
train acc:  0.71875
train loss:  0.5549157857894897
train gradient:  0.13212055818192614
iteration : 5348
train acc:  0.7890625
train loss:  0.49157094955444336
train gradient:  0.13668068492645588
iteration : 5349
train acc:  0.78125
train loss:  0.4571327269077301
train gradient:  0.11904075673210031
iteration : 5350
train acc:  0.828125
train loss:  0.47042116522789
train gradient:  0.11527745445781751
iteration : 5351
train acc:  0.7578125
train loss:  0.5043841004371643
train gradient:  0.10869663165824517
iteration : 5352
train acc:  0.765625
train loss:  0.486248642206192
train gradient:  0.13910819952786924
iteration : 5353
train acc:  0.75
train loss:  0.487170934677124
train gradient:  0.1470770572253416
iteration : 5354
train acc:  0.75
train loss:  0.5116905570030212
train gradient:  0.14036219217143014
iteration : 5355
train acc:  0.7109375
train loss:  0.5099297761917114
train gradient:  0.17226243256442175
iteration : 5356
train acc:  0.734375
train loss:  0.49860262870788574
train gradient:  0.11845050620399436
iteration : 5357
train acc:  0.7734375
train loss:  0.48582521080970764
train gradient:  0.14304052643659126
iteration : 5358
train acc:  0.78125
train loss:  0.42851054668426514
train gradient:  0.12490086190929359
iteration : 5359
train acc:  0.8203125
train loss:  0.38897520303726196
train gradient:  0.11539720775692668
iteration : 5360
train acc:  0.828125
train loss:  0.43397286534309387
train gradient:  0.13330780883462817
iteration : 5361
train acc:  0.78125
train loss:  0.4531130790710449
train gradient:  0.13873706955599052
iteration : 5362
train acc:  0.703125
train loss:  0.5346006155014038
train gradient:  0.13905611222988784
iteration : 5363
train acc:  0.6640625
train loss:  0.5540728569030762
train gradient:  0.15885754640835847
iteration : 5364
train acc:  0.75
train loss:  0.4896174669265747
train gradient:  0.11375599308857537
iteration : 5365
train acc:  0.7109375
train loss:  0.5239531993865967
train gradient:  0.14365101931951407
iteration : 5366
train acc:  0.7734375
train loss:  0.49476251006126404
train gradient:  0.14328599263963504
iteration : 5367
train acc:  0.75
train loss:  0.537844181060791
train gradient:  0.14300424818472168
iteration : 5368
train acc:  0.75
train loss:  0.5501163005828857
train gradient:  0.13896056265449386
iteration : 5369
train acc:  0.6484375
train loss:  0.6250536441802979
train gradient:  0.2860402472812549
iteration : 5370
train acc:  0.7734375
train loss:  0.46636009216308594
train gradient:  0.1496792287227663
iteration : 5371
train acc:  0.75
train loss:  0.46353864669799805
train gradient:  0.11181734756910104
iteration : 5372
train acc:  0.7109375
train loss:  0.5543305277824402
train gradient:  0.14795532040828485
iteration : 5373
train acc:  0.7421875
train loss:  0.5396455526351929
train gradient:  0.14311192926425584
iteration : 5374
train acc:  0.7578125
train loss:  0.5178895592689514
train gradient:  0.16935123027483961
iteration : 5375
train acc:  0.65625
train loss:  0.554162859916687
train gradient:  0.17009674924543894
iteration : 5376
train acc:  0.6484375
train loss:  0.5776844620704651
train gradient:  0.2227200646393591
iteration : 5377
train acc:  0.7890625
train loss:  0.4775027930736542
train gradient:  0.12040029367687106
iteration : 5378
train acc:  0.8046875
train loss:  0.4521443843841553
train gradient:  0.11744912121581698
iteration : 5379
train acc:  0.703125
train loss:  0.5422996878623962
train gradient:  0.17498245721197944
iteration : 5380
train acc:  0.7265625
train loss:  0.5326067805290222
train gradient:  0.18194370557619835
iteration : 5381
train acc:  0.765625
train loss:  0.45120471715927124
train gradient:  0.13066712675572023
iteration : 5382
train acc:  0.734375
train loss:  0.5263272523880005
train gradient:  0.13142074573982
iteration : 5383
train acc:  0.7109375
train loss:  0.5139982104301453
train gradient:  0.19910398170176064
iteration : 5384
train acc:  0.71875
train loss:  0.5464808344841003
train gradient:  0.13091434973937588
iteration : 5385
train acc:  0.671875
train loss:  0.5791881680488586
train gradient:  0.16284672053341748
iteration : 5386
train acc:  0.734375
train loss:  0.5208837985992432
train gradient:  0.13434457547952983
iteration : 5387
train acc:  0.765625
train loss:  0.4553254246711731
train gradient:  0.0971403166876834
iteration : 5388
train acc:  0.7421875
train loss:  0.4944069981575012
train gradient:  0.09702607404516717
iteration : 5389
train acc:  0.6484375
train loss:  0.5758700966835022
train gradient:  0.14611655325640038
iteration : 5390
train acc:  0.7578125
train loss:  0.4534301161766052
train gradient:  0.11452091411086737
iteration : 5391
train acc:  0.6640625
train loss:  0.5904660224914551
train gradient:  0.19709647465836616
iteration : 5392
train acc:  0.7265625
train loss:  0.5182305574417114
train gradient:  0.12766091468171584
iteration : 5393
train acc:  0.7734375
train loss:  0.4545850455760956
train gradient:  0.09576557110412885
iteration : 5394
train acc:  0.6875
train loss:  0.532978892326355
train gradient:  0.14949637242960348
iteration : 5395
train acc:  0.75
train loss:  0.5017001032829285
train gradient:  0.11556566040014123
iteration : 5396
train acc:  0.7890625
train loss:  0.48034191131591797
train gradient:  0.1334624909491302
iteration : 5397
train acc:  0.7265625
train loss:  0.4800001084804535
train gradient:  0.12212137509814909
iteration : 5398
train acc:  0.7421875
train loss:  0.5170789957046509
train gradient:  0.14101626571770748
iteration : 5399
train acc:  0.765625
train loss:  0.42846646904945374
train gradient:  0.12276861095982432
iteration : 5400
train acc:  0.6875
train loss:  0.5606204271316528
train gradient:  0.1454251854686865
iteration : 5401
train acc:  0.703125
train loss:  0.5355396270751953
train gradient:  0.145394077535895
iteration : 5402
train acc:  0.7421875
train loss:  0.533189594745636
train gradient:  0.1562935391910132
iteration : 5403
train acc:  0.7109375
train loss:  0.5285719037055969
train gradient:  0.13399663853414412
iteration : 5404
train acc:  0.6484375
train loss:  0.584158718585968
train gradient:  0.21266298870939998
iteration : 5405
train acc:  0.734375
train loss:  0.5001676082611084
train gradient:  0.13216909535912394
iteration : 5406
train acc:  0.828125
train loss:  0.4391719698905945
train gradient:  0.11964985854573722
iteration : 5407
train acc:  0.71875
train loss:  0.5052614212036133
train gradient:  0.129268086027655
iteration : 5408
train acc:  0.7890625
train loss:  0.44138818979263306
train gradient:  0.09505330199102544
iteration : 5409
train acc:  0.7109375
train loss:  0.5376834869384766
train gradient:  0.17467074432904744
iteration : 5410
train acc:  0.7265625
train loss:  0.5145097970962524
train gradient:  0.10952812301857653
iteration : 5411
train acc:  0.65625
train loss:  0.5698481798171997
train gradient:  0.17577369804832677
iteration : 5412
train acc:  0.7265625
train loss:  0.5376752018928528
train gradient:  0.1730317462393864
iteration : 5413
train acc:  0.734375
train loss:  0.5388456583023071
train gradient:  0.14409490768158167
iteration : 5414
train acc:  0.6796875
train loss:  0.5855720043182373
train gradient:  0.19620291183904986
iteration : 5415
train acc:  0.6796875
train loss:  0.5356340408325195
train gradient:  0.11595428320835466
iteration : 5416
train acc:  0.6875
train loss:  0.5081625580787659
train gradient:  0.15172311214649048
iteration : 5417
train acc:  0.703125
train loss:  0.5352753400802612
train gradient:  0.18948691707305063
iteration : 5418
train acc:  0.734375
train loss:  0.5184297561645508
train gradient:  0.16684625363978023
iteration : 5419
train acc:  0.71875
train loss:  0.5270018577575684
train gradient:  0.20933637621452694
iteration : 5420
train acc:  0.75
train loss:  0.5378905534744263
train gradient:  0.16838736292347836
iteration : 5421
train acc:  0.765625
train loss:  0.519829511642456
train gradient:  0.19178314214068637
iteration : 5422
train acc:  0.65625
train loss:  0.5654988288879395
train gradient:  0.17725472158269526
iteration : 5423
train acc:  0.7578125
train loss:  0.5071980953216553
train gradient:  0.19004749388088338
iteration : 5424
train acc:  0.7421875
train loss:  0.5137972831726074
train gradient:  0.13744891694488998
iteration : 5425
train acc:  0.734375
train loss:  0.5088977813720703
train gradient:  0.15442995605794102
iteration : 5426
train acc:  0.6953125
train loss:  0.5042049884796143
train gradient:  0.12665336311662345
iteration : 5427
train acc:  0.71875
train loss:  0.4823364019393921
train gradient:  0.16418934156867263
iteration : 5428
train acc:  0.78125
train loss:  0.44372329115867615
train gradient:  0.12503667230199964
iteration : 5429
train acc:  0.7421875
train loss:  0.5284974575042725
train gradient:  0.15654152369268587
iteration : 5430
train acc:  0.7421875
train loss:  0.49571532011032104
train gradient:  0.1337496371091335
iteration : 5431
train acc:  0.7109375
train loss:  0.536635160446167
train gradient:  0.16949680111772053
iteration : 5432
train acc:  0.6640625
train loss:  0.5673408508300781
train gradient:  0.16048306721709635
iteration : 5433
train acc:  0.7109375
train loss:  0.5528604984283447
train gradient:  0.13996202373668348
iteration : 5434
train acc:  0.7109375
train loss:  0.6149734258651733
train gradient:  0.1799993423918027
iteration : 5435
train acc:  0.7109375
train loss:  0.4989483952522278
train gradient:  0.14251153739717115
iteration : 5436
train acc:  0.6796875
train loss:  0.5209532380104065
train gradient:  0.11299248667753918
iteration : 5437
train acc:  0.8203125
train loss:  0.4452432096004486
train gradient:  0.13317014756867462
iteration : 5438
train acc:  0.78125
train loss:  0.4529901146888733
train gradient:  0.10825704233995202
iteration : 5439
train acc:  0.6796875
train loss:  0.5534689426422119
train gradient:  0.16072197771651967
iteration : 5440
train acc:  0.7265625
train loss:  0.5344846248626709
train gradient:  0.1418877540302176
iteration : 5441
train acc:  0.6796875
train loss:  0.5992757081985474
train gradient:  0.18781802642243273
iteration : 5442
train acc:  0.7421875
train loss:  0.4799434542655945
train gradient:  0.10930671843935924
iteration : 5443
train acc:  0.78125
train loss:  0.48913830518722534
train gradient:  0.15415891214559796
iteration : 5444
train acc:  0.71875
train loss:  0.5282958745956421
train gradient:  0.17175803764172592
iteration : 5445
train acc:  0.703125
train loss:  0.5676965117454529
train gradient:  0.17422934741086568
iteration : 5446
train acc:  0.7265625
train loss:  0.5156452655792236
train gradient:  0.1560030618516801
iteration : 5447
train acc:  0.7265625
train loss:  0.5104512572288513
train gradient:  0.14939567558978606
iteration : 5448
train acc:  0.734375
train loss:  0.53327476978302
train gradient:  0.14618695219417333
iteration : 5449
train acc:  0.75
train loss:  0.5622467994689941
train gradient:  0.18868472025347496
iteration : 5450
train acc:  0.7265625
train loss:  0.4879041314125061
train gradient:  0.1270815120851219
iteration : 5451
train acc:  0.6484375
train loss:  0.5672107934951782
train gradient:  0.16015987715964963
iteration : 5452
train acc:  0.71875
train loss:  0.5696600675582886
train gradient:  0.1523293009266567
iteration : 5453
train acc:  0.734375
train loss:  0.5227072834968567
train gradient:  0.1272039191258561
iteration : 5454
train acc:  0.6875
train loss:  0.6139233112335205
train gradient:  0.18049418386137778
iteration : 5455
train acc:  0.828125
train loss:  0.40203940868377686
train gradient:  0.10226069351278372
iteration : 5456
train acc:  0.671875
train loss:  0.5201535224914551
train gradient:  0.14589324270113613
iteration : 5457
train acc:  0.7734375
train loss:  0.45239609479904175
train gradient:  0.11237390874174535
iteration : 5458
train acc:  0.6953125
train loss:  0.5759457349777222
train gradient:  0.21470440887931858
iteration : 5459
train acc:  0.6953125
train loss:  0.598530650138855
train gradient:  0.19376523665557557
iteration : 5460
train acc:  0.8046875
train loss:  0.44045814871788025
train gradient:  0.10797445484594008
iteration : 5461
train acc:  0.7421875
train loss:  0.511176586151123
train gradient:  0.14127843898386583
iteration : 5462
train acc:  0.7265625
train loss:  0.5040819644927979
train gradient:  0.13537663597338845
iteration : 5463
train acc:  0.7734375
train loss:  0.4301009178161621
train gradient:  0.10877811491500812
iteration : 5464
train acc:  0.71875
train loss:  0.4775679111480713
train gradient:  0.10809291462279465
iteration : 5465
train acc:  0.6953125
train loss:  0.4927351176738739
train gradient:  0.13169018373909658
iteration : 5466
train acc:  0.796875
train loss:  0.49077022075653076
train gradient:  0.12799905975003872
iteration : 5467
train acc:  0.703125
train loss:  0.5614761114120483
train gradient:  0.16540420371428782
iteration : 5468
train acc:  0.71875
train loss:  0.5320444107055664
train gradient:  0.1757747037618949
iteration : 5469
train acc:  0.7578125
train loss:  0.4675992429256439
train gradient:  0.16012097423313099
iteration : 5470
train acc:  0.7578125
train loss:  0.5186688899993896
train gradient:  0.13400645115102452
iteration : 5471
train acc:  0.7421875
train loss:  0.5134742259979248
train gradient:  0.1525713295090193
iteration : 5472
train acc:  0.734375
train loss:  0.5396934747695923
train gradient:  0.16722652873455632
iteration : 5473
train acc:  0.7421875
train loss:  0.5032235383987427
train gradient:  0.16657094555011384
iteration : 5474
train acc:  0.7421875
train loss:  0.5346454381942749
train gradient:  0.16473508315454746
iteration : 5475
train acc:  0.7421875
train loss:  0.5282273292541504
train gradient:  0.1359700305476989
iteration : 5476
train acc:  0.6953125
train loss:  0.5410933494567871
train gradient:  0.14851305061209435
iteration : 5477
train acc:  0.7890625
train loss:  0.475585401058197
train gradient:  0.10952399642101546
iteration : 5478
train acc:  0.796875
train loss:  0.45852571725845337
train gradient:  0.11000798706663377
iteration : 5479
train acc:  0.765625
train loss:  0.4749927818775177
train gradient:  0.12387665999317754
iteration : 5480
train acc:  0.75
train loss:  0.5176471471786499
train gradient:  0.14454881502287636
iteration : 5481
train acc:  0.7578125
train loss:  0.5068514347076416
train gradient:  0.1294206248006347
iteration : 5482
train acc:  0.6875
train loss:  0.5305432081222534
train gradient:  0.16132357618597756
iteration : 5483
train acc:  0.7734375
train loss:  0.4968242645263672
train gradient:  0.1559640110038499
iteration : 5484
train acc:  0.7421875
train loss:  0.5119509696960449
train gradient:  0.11601216092169238
iteration : 5485
train acc:  0.7265625
train loss:  0.5458885431289673
train gradient:  0.14695162546055077
iteration : 5486
train acc:  0.734375
train loss:  0.5141127705574036
train gradient:  0.1561723674824818
iteration : 5487
train acc:  0.7890625
train loss:  0.46221083402633667
train gradient:  0.13137948313676479
iteration : 5488
train acc:  0.6796875
train loss:  0.5394710302352905
train gradient:  0.16855349847437523
iteration : 5489
train acc:  0.75
train loss:  0.48488593101501465
train gradient:  0.1301975861855919
iteration : 5490
train acc:  0.7734375
train loss:  0.5069133639335632
train gradient:  0.13259197278825338
iteration : 5491
train acc:  0.765625
train loss:  0.47797146439552307
train gradient:  0.14614352193030677
iteration : 5492
train acc:  0.71875
train loss:  0.5087082982063293
train gradient:  0.14698192854765058
iteration : 5493
train acc:  0.6640625
train loss:  0.5716836452484131
train gradient:  0.1433782107175593
iteration : 5494
train acc:  0.7734375
train loss:  0.5139925479888916
train gradient:  0.12900677469103777
iteration : 5495
train acc:  0.7421875
train loss:  0.4535568356513977
train gradient:  0.12026989609623051
iteration : 5496
train acc:  0.734375
train loss:  0.4869788885116577
train gradient:  0.11703620756843254
iteration : 5497
train acc:  0.7421875
train loss:  0.46622464060783386
train gradient:  0.12120745879867934
iteration : 5498
train acc:  0.7265625
train loss:  0.4899939000606537
train gradient:  0.1300064875347952
iteration : 5499
train acc:  0.734375
train loss:  0.5063186883926392
train gradient:  0.14173154271014998
iteration : 5500
train acc:  0.765625
train loss:  0.46965211629867554
train gradient:  0.12157435415665742
iteration : 5501
train acc:  0.7890625
train loss:  0.4767027795314789
train gradient:  0.14626577911390198
iteration : 5502
train acc:  0.7109375
train loss:  0.5107932090759277
train gradient:  0.13758616979886668
iteration : 5503
train acc:  0.7421875
train loss:  0.5309807062149048
train gradient:  0.15822075831277038
iteration : 5504
train acc:  0.7109375
train loss:  0.5567142367362976
train gradient:  0.17008712620468186
iteration : 5505
train acc:  0.7265625
train loss:  0.49065694212913513
train gradient:  0.16320893547662607
iteration : 5506
train acc:  0.7265625
train loss:  0.4664302170276642
train gradient:  0.11119735087807492
iteration : 5507
train acc:  0.6796875
train loss:  0.5916591286659241
train gradient:  0.17825902608677135
iteration : 5508
train acc:  0.7578125
train loss:  0.4792713522911072
train gradient:  0.13558516644007718
iteration : 5509
train acc:  0.703125
train loss:  0.5795577168464661
train gradient:  0.21275090750811487
iteration : 5510
train acc:  0.7109375
train loss:  0.5378357172012329
train gradient:  0.15276795100526325
iteration : 5511
train acc:  0.734375
train loss:  0.4894678592681885
train gradient:  0.13170378606275976
iteration : 5512
train acc:  0.734375
train loss:  0.47434258460998535
train gradient:  0.12266842314583032
iteration : 5513
train acc:  0.6796875
train loss:  0.5373733043670654
train gradient:  0.1859336836738416
iteration : 5514
train acc:  0.671875
train loss:  0.5544763803482056
train gradient:  0.13802148026710714
iteration : 5515
train acc:  0.7578125
train loss:  0.5137450098991394
train gradient:  0.1457978366128075
iteration : 5516
train acc:  0.765625
train loss:  0.4574750065803528
train gradient:  0.12144553323983724
iteration : 5517
train acc:  0.7265625
train loss:  0.5121352672576904
train gradient:  0.12323171490336869
iteration : 5518
train acc:  0.75
train loss:  0.505512535572052
train gradient:  0.12659570198480197
iteration : 5519
train acc:  0.6484375
train loss:  0.5928725004196167
train gradient:  0.23165507486138698
iteration : 5520
train acc:  0.6953125
train loss:  0.6224360466003418
train gradient:  0.23405828406878845
iteration : 5521
train acc:  0.75
train loss:  0.4787517786026001
train gradient:  0.1307355264038193
iteration : 5522
train acc:  0.671875
train loss:  0.5229578018188477
train gradient:  0.22355705353867789
iteration : 5523
train acc:  0.6875
train loss:  0.5742212533950806
train gradient:  0.1678134340966661
iteration : 5524
train acc:  0.703125
train loss:  0.5262767672538757
train gradient:  0.14430113293162955
iteration : 5525
train acc:  0.7421875
train loss:  0.5028296709060669
train gradient:  0.15244390973763955
iteration : 5526
train acc:  0.671875
train loss:  0.582243800163269
train gradient:  0.15778615651307357
iteration : 5527
train acc:  0.796875
train loss:  0.419098436832428
train gradient:  0.09854718328741002
iteration : 5528
train acc:  0.7578125
train loss:  0.46465998888015747
train gradient:  0.10292060249508778
iteration : 5529
train acc:  0.78125
train loss:  0.4669604003429413
train gradient:  0.10996220203906652
iteration : 5530
train acc:  0.625
train loss:  0.586876630783081
train gradient:  0.16180181459320792
iteration : 5531
train acc:  0.71875
train loss:  0.548934817314148
train gradient:  0.13010856671646603
iteration : 5532
train acc:  0.796875
train loss:  0.46922624111175537
train gradient:  0.1481986520395065
iteration : 5533
train acc:  0.6796875
train loss:  0.6075120568275452
train gradient:  0.17893112055327998
iteration : 5534
train acc:  0.7109375
train loss:  0.5384405851364136
train gradient:  0.19151339593814165
iteration : 5535
train acc:  0.7734375
train loss:  0.48978251218795776
train gradient:  0.11986920633328059
iteration : 5536
train acc:  0.75
train loss:  0.48334574699401855
train gradient:  0.13301017316541153
iteration : 5537
train acc:  0.75
train loss:  0.5070414543151855
train gradient:  0.1633108582658166
iteration : 5538
train acc:  0.7421875
train loss:  0.49308377504348755
train gradient:  0.10506194288430597
iteration : 5539
train acc:  0.78125
train loss:  0.4772697687149048
train gradient:  0.14673607685142243
iteration : 5540
train acc:  0.734375
train loss:  0.5026482343673706
train gradient:  0.1473220144769142
iteration : 5541
train acc:  0.7421875
train loss:  0.5700390338897705
train gradient:  0.18720205277636187
iteration : 5542
train acc:  0.78125
train loss:  0.4528895616531372
train gradient:  0.12736077447690775
iteration : 5543
train acc:  0.734375
train loss:  0.5286557078361511
train gradient:  0.15428263376507995
iteration : 5544
train acc:  0.71875
train loss:  0.49470528960227966
train gradient:  0.13286172744979247
iteration : 5545
train acc:  0.7109375
train loss:  0.5392488241195679
train gradient:  0.16131232956448815
iteration : 5546
train acc:  0.7421875
train loss:  0.493429571390152
train gradient:  0.1390463696979631
iteration : 5547
train acc:  0.7578125
train loss:  0.4726237654685974
train gradient:  0.16497287740108174
iteration : 5548
train acc:  0.6328125
train loss:  0.5726358890533447
train gradient:  0.16426878438491127
iteration : 5549
train acc:  0.78125
train loss:  0.4495631456375122
train gradient:  0.13327420422463165
iteration : 5550
train acc:  0.75
train loss:  0.47177213430404663
train gradient:  0.11490283453833938
iteration : 5551
train acc:  0.7265625
train loss:  0.5225512981414795
train gradient:  0.14275111768600277
iteration : 5552
train acc:  0.7734375
train loss:  0.439191073179245
train gradient:  0.10555383426647051
iteration : 5553
train acc:  0.671875
train loss:  0.5621412992477417
train gradient:  0.15439693123420536
iteration : 5554
train acc:  0.71875
train loss:  0.5105851292610168
train gradient:  0.14670501503388178
iteration : 5555
train acc:  0.75
train loss:  0.46410176157951355
train gradient:  0.10919536131310413
iteration : 5556
train acc:  0.7109375
train loss:  0.4782361090183258
train gradient:  0.13838009697383558
iteration : 5557
train acc:  0.7421875
train loss:  0.5139352083206177
train gradient:  0.161056636944022
iteration : 5558
train acc:  0.765625
train loss:  0.49751511216163635
train gradient:  0.16268336712927473
iteration : 5559
train acc:  0.75
train loss:  0.4594596028327942
train gradient:  0.09756439464701978
iteration : 5560
train acc:  0.78125
train loss:  0.44956159591674805
train gradient:  0.0959257930303668
iteration : 5561
train acc:  0.734375
train loss:  0.5719285607337952
train gradient:  0.15027439160996858
iteration : 5562
train acc:  0.703125
train loss:  0.5610302090644836
train gradient:  0.14648874022780284
iteration : 5563
train acc:  0.7109375
train loss:  0.5259609818458557
train gradient:  0.13705011904306783
iteration : 5564
train acc:  0.7109375
train loss:  0.5021535158157349
train gradient:  0.14857464533452724
iteration : 5565
train acc:  0.765625
train loss:  0.47788792848587036
train gradient:  0.1577013869679248
iteration : 5566
train acc:  0.6875
train loss:  0.5700126886367798
train gradient:  0.1445229770778747
iteration : 5567
train acc:  0.703125
train loss:  0.5667599439620972
train gradient:  0.15017864474493356
iteration : 5568
train acc:  0.6796875
train loss:  0.554673969745636
train gradient:  0.18633172704524845
iteration : 5569
train acc:  0.71875
train loss:  0.5349907875061035
train gradient:  0.15496709185919272
iteration : 5570
train acc:  0.7578125
train loss:  0.45445460081100464
train gradient:  0.14304037999302532
iteration : 5571
train acc:  0.7421875
train loss:  0.5615334510803223
train gradient:  0.15685129846888796
iteration : 5572
train acc:  0.7265625
train loss:  0.4972867965698242
train gradient:  0.1292906856082619
iteration : 5573
train acc:  0.71875
train loss:  0.5280776023864746
train gradient:  0.1466001788225917
iteration : 5574
train acc:  0.7421875
train loss:  0.5157862305641174
train gradient:  0.16094380302258326
iteration : 5575
train acc:  0.75
train loss:  0.5000491142272949
train gradient:  0.0962608866042951
iteration : 5576
train acc:  0.703125
train loss:  0.5210468769073486
train gradient:  0.15619056728625635
iteration : 5577
train acc:  0.71875
train loss:  0.5249015092849731
train gradient:  0.1804103667213583
iteration : 5578
train acc:  0.7734375
train loss:  0.4264184832572937
train gradient:  0.09579750620198904
iteration : 5579
train acc:  0.75
train loss:  0.4888451099395752
train gradient:  0.1298260242658851
iteration : 5580
train acc:  0.7421875
train loss:  0.5213990211486816
train gradient:  0.13399113843547772
iteration : 5581
train acc:  0.7578125
train loss:  0.48235398530960083
train gradient:  0.15660363848021333
iteration : 5582
train acc:  0.7890625
train loss:  0.44972193241119385
train gradient:  0.14048476015045183
iteration : 5583
train acc:  0.7265625
train loss:  0.5461525917053223
train gradient:  0.18847998173200708
iteration : 5584
train acc:  0.703125
train loss:  0.5546371340751648
train gradient:  0.16892004066102992
iteration : 5585
train acc:  0.734375
train loss:  0.5352683067321777
train gradient:  0.14601869805527246
iteration : 5586
train acc:  0.6875
train loss:  0.5316016674041748
train gradient:  0.17098875184828144
iteration : 5587
train acc:  0.75
train loss:  0.4996882379055023
train gradient:  0.13718652459659014
iteration : 5588
train acc:  0.7265625
train loss:  0.5739606022834778
train gradient:  0.2022999157970105
iteration : 5589
train acc:  0.6875
train loss:  0.5771597027778625
train gradient:  0.23782948991916553
iteration : 5590
train acc:  0.7265625
train loss:  0.5579175353050232
train gradient:  0.18052475218892253
iteration : 5591
train acc:  0.765625
train loss:  0.4861629605293274
train gradient:  0.15219856194080558
iteration : 5592
train acc:  0.6953125
train loss:  0.5791492462158203
train gradient:  0.18462578850357353
iteration : 5593
train acc:  0.7578125
train loss:  0.5078179836273193
train gradient:  0.14105902959752414
iteration : 5594
train acc:  0.6796875
train loss:  0.5723317861557007
train gradient:  0.16970735288205824
iteration : 5595
train acc:  0.671875
train loss:  0.5868473052978516
train gradient:  0.17299165003598893
iteration : 5596
train acc:  0.703125
train loss:  0.5299544930458069
train gradient:  0.1825216138112951
iteration : 5597
train acc:  0.7734375
train loss:  0.4729781448841095
train gradient:  0.11286607418683042
iteration : 5598
train acc:  0.71875
train loss:  0.5722572803497314
train gradient:  0.20408538552586059
iteration : 5599
train acc:  0.75
train loss:  0.49861347675323486
train gradient:  0.14831927541174386
iteration : 5600
train acc:  0.703125
train loss:  0.5218565464019775
train gradient:  0.15378267624118508
iteration : 5601
train acc:  0.71875
train loss:  0.522332489490509
train gradient:  0.13603775149249958
iteration : 5602
train acc:  0.7109375
train loss:  0.5122610926628113
train gradient:  0.12460931507700299
iteration : 5603
train acc:  0.75
train loss:  0.5488069653511047
train gradient:  0.14747111590899942
iteration : 5604
train acc:  0.7578125
train loss:  0.48371708393096924
train gradient:  0.13136883445142633
iteration : 5605
train acc:  0.671875
train loss:  0.553601086139679
train gradient:  0.1395057717154531
iteration : 5606
train acc:  0.7265625
train loss:  0.5468053221702576
train gradient:  0.15191106385144548
iteration : 5607
train acc:  0.765625
train loss:  0.5006104111671448
train gradient:  0.11152518870840834
iteration : 5608
train acc:  0.7265625
train loss:  0.5522096157073975
train gradient:  0.19741760247082846
iteration : 5609
train acc:  0.6953125
train loss:  0.4988125264644623
train gradient:  0.14166769772488225
iteration : 5610
train acc:  0.796875
train loss:  0.44728341698646545
train gradient:  0.11973974437415741
iteration : 5611
train acc:  0.78125
train loss:  0.46810275316238403
train gradient:  0.136579668903047
iteration : 5612
train acc:  0.75
train loss:  0.5086154937744141
train gradient:  0.1355539726272012
iteration : 5613
train acc:  0.7421875
train loss:  0.5214581489562988
train gradient:  0.2015259137532212
iteration : 5614
train acc:  0.703125
train loss:  0.5321439504623413
train gradient:  0.17779271891943257
iteration : 5615
train acc:  0.7265625
train loss:  0.5472341775894165
train gradient:  0.1894648963512606
iteration : 5616
train acc:  0.7421875
train loss:  0.46416211128234863
train gradient:  0.11918507334595317
iteration : 5617
train acc:  0.7578125
train loss:  0.48773202300071716
train gradient:  0.12143703246243968
iteration : 5618
train acc:  0.75
train loss:  0.4739264249801636
train gradient:  0.12936905758088568
iteration : 5619
train acc:  0.75
train loss:  0.477765828371048
train gradient:  0.19760008346872854
iteration : 5620
train acc:  0.671875
train loss:  0.651205837726593
train gradient:  0.18646522823419054
iteration : 5621
train acc:  0.75
train loss:  0.5458356738090515
train gradient:  0.16516197653217723
iteration : 5622
train acc:  0.671875
train loss:  0.5700019598007202
train gradient:  0.1618887306157166
iteration : 5623
train acc:  0.765625
train loss:  0.4874238669872284
train gradient:  0.12262384287559282
iteration : 5624
train acc:  0.78125
train loss:  0.4877215623855591
train gradient:  0.14260785930955283
iteration : 5625
train acc:  0.8203125
train loss:  0.45976507663726807
train gradient:  0.1324605841013422
iteration : 5626
train acc:  0.7578125
train loss:  0.45420223474502563
train gradient:  0.10838623478387825
iteration : 5627
train acc:  0.6796875
train loss:  0.5325709581375122
train gradient:  0.20016946377218664
iteration : 5628
train acc:  0.765625
train loss:  0.47253653407096863
train gradient:  0.1176036822235029
iteration : 5629
train acc:  0.7265625
train loss:  0.5495001077651978
train gradient:  0.15939330902357932
iteration : 5630
train acc:  0.703125
train loss:  0.5398380756378174
train gradient:  0.13873373096061956
iteration : 5631
train acc:  0.7265625
train loss:  0.4788106083869934
train gradient:  0.14266823010256424
iteration : 5632
train acc:  0.75
train loss:  0.5113277435302734
train gradient:  0.17266095991971225
iteration : 5633
train acc:  0.7734375
train loss:  0.46199801564216614
train gradient:  0.14527091911472156
iteration : 5634
train acc:  0.6875
train loss:  0.5381746292114258
train gradient:  0.2180368148993122
iteration : 5635
train acc:  0.75
train loss:  0.4914628267288208
train gradient:  0.11630183210399447
iteration : 5636
train acc:  0.71875
train loss:  0.5017350912094116
train gradient:  0.13841891835796222
iteration : 5637
train acc:  0.7890625
train loss:  0.5090194940567017
train gradient:  0.167886738828863
iteration : 5638
train acc:  0.765625
train loss:  0.4448646903038025
train gradient:  0.10655046757797867
iteration : 5639
train acc:  0.734375
train loss:  0.5450087785720825
train gradient:  0.18891521984836068
iteration : 5640
train acc:  0.6953125
train loss:  0.5255607962608337
train gradient:  0.12454299642971077
iteration : 5641
train acc:  0.78125
train loss:  0.473461389541626
train gradient:  0.12862369183279923
iteration : 5642
train acc:  0.7421875
train loss:  0.46977484226226807
train gradient:  0.1435270402418518
iteration : 5643
train acc:  0.6875
train loss:  0.5218225717544556
train gradient:  0.19314369824702887
iteration : 5644
train acc:  0.734375
train loss:  0.5157756805419922
train gradient:  0.18067242505799275
iteration : 5645
train acc:  0.703125
train loss:  0.5284417867660522
train gradient:  0.17304774584780946
iteration : 5646
train acc:  0.71875
train loss:  0.5014826059341431
train gradient:  0.132422711032551
iteration : 5647
train acc:  0.7890625
train loss:  0.4445207417011261
train gradient:  0.1072808845488278
iteration : 5648
train acc:  0.6875
train loss:  0.5332094430923462
train gradient:  0.12181992851744822
iteration : 5649
train acc:  0.7421875
train loss:  0.5157629251480103
train gradient:  0.12118677564099614
iteration : 5650
train acc:  0.8046875
train loss:  0.4398420453071594
train gradient:  0.14153044803557344
iteration : 5651
train acc:  0.796875
train loss:  0.46991661190986633
train gradient:  0.15330641739102613
iteration : 5652
train acc:  0.71875
train loss:  0.5065948963165283
train gradient:  0.12261511206114563
iteration : 5653
train acc:  0.71875
train loss:  0.5423396825790405
train gradient:  0.15933015484329416
iteration : 5654
train acc:  0.7890625
train loss:  0.4546593725681305
train gradient:  0.10549215525474014
iteration : 5655
train acc:  0.75
train loss:  0.5160447955131531
train gradient:  0.1573391304321931
iteration : 5656
train acc:  0.734375
train loss:  0.4824743866920471
train gradient:  0.13226853759372498
iteration : 5657
train acc:  0.75
train loss:  0.5116504430770874
train gradient:  0.17156395377875067
iteration : 5658
train acc:  0.75
train loss:  0.4969831705093384
train gradient:  0.1563363842017947
iteration : 5659
train acc:  0.75
train loss:  0.532342791557312
train gradient:  0.16709604648324886
iteration : 5660
train acc:  0.6953125
train loss:  0.6499748826026917
train gradient:  0.34583967213737765
iteration : 5661
train acc:  0.78125
train loss:  0.48703134059906006
train gradient:  0.12787648665710682
iteration : 5662
train acc:  0.734375
train loss:  0.5044171810150146
train gradient:  0.13442780222559775
iteration : 5663
train acc:  0.6875
train loss:  0.5323025584220886
train gradient:  0.13916346233652946
iteration : 5664
train acc:  0.6953125
train loss:  0.5968912839889526
train gradient:  0.19290944691700207
iteration : 5665
train acc:  0.734375
train loss:  0.5178329944610596
train gradient:  0.12623061713076278
iteration : 5666
train acc:  0.71875
train loss:  0.5286747217178345
train gradient:  0.19259626644709898
iteration : 5667
train acc:  0.7421875
train loss:  0.4691632390022278
train gradient:  0.1333931173153468
iteration : 5668
train acc:  0.765625
train loss:  0.5095832347869873
train gradient:  0.16051083617338732
iteration : 5669
train acc:  0.6953125
train loss:  0.5682735443115234
train gradient:  0.1612123656583861
iteration : 5670
train acc:  0.6953125
train loss:  0.5700363516807556
train gradient:  0.14187575093707078
iteration : 5671
train acc:  0.7890625
train loss:  0.44988518953323364
train gradient:  0.1362312501691626
iteration : 5672
train acc:  0.671875
train loss:  0.561490535736084
train gradient:  0.21331059105929384
iteration : 5673
train acc:  0.734375
train loss:  0.5086450576782227
train gradient:  0.15978640213256934
iteration : 5674
train acc:  0.703125
train loss:  0.5605560541152954
train gradient:  0.17485610824901351
iteration : 5675
train acc:  0.7109375
train loss:  0.4822583794593811
train gradient:  0.13176410897594376
iteration : 5676
train acc:  0.765625
train loss:  0.4819284677505493
train gradient:  0.13014510884802644
iteration : 5677
train acc:  0.765625
train loss:  0.4869404137134552
train gradient:  0.17815398841878338
iteration : 5678
train acc:  0.765625
train loss:  0.4373031556606293
train gradient:  0.12126274130789803
iteration : 5679
train acc:  0.7421875
train loss:  0.5972712635993958
train gradient:  0.23490389493177
iteration : 5680
train acc:  0.703125
train loss:  0.514130711555481
train gradient:  0.12456586277537181
iteration : 5681
train acc:  0.734375
train loss:  0.484915554523468
train gradient:  0.1763669418728147
iteration : 5682
train acc:  0.703125
train loss:  0.5519651770591736
train gradient:  0.16242199480357522
iteration : 5683
train acc:  0.6953125
train loss:  0.5623127222061157
train gradient:  0.14609002564906082
iteration : 5684
train acc:  0.6953125
train loss:  0.5014016032218933
train gradient:  0.15709691823824362
iteration : 5685
train acc:  0.7109375
train loss:  0.5362903475761414
train gradient:  0.1328536647273419
iteration : 5686
train acc:  0.7734375
train loss:  0.48194870352745056
train gradient:  0.1650165110320609
iteration : 5687
train acc:  0.75
train loss:  0.5089210867881775
train gradient:  0.14689312454737347
iteration : 5688
train acc:  0.6875
train loss:  0.5275383591651917
train gradient:  0.17957721229958634
iteration : 5689
train acc:  0.734375
train loss:  0.5244454145431519
train gradient:  0.12423140496035266
iteration : 5690
train acc:  0.75
train loss:  0.499733567237854
train gradient:  0.18707549562110165
iteration : 5691
train acc:  0.75
train loss:  0.4690784215927124
train gradient:  0.2506795406276644
iteration : 5692
train acc:  0.7421875
train loss:  0.47429224848747253
train gradient:  0.16247998594768134
iteration : 5693
train acc:  0.796875
train loss:  0.4223899245262146
train gradient:  0.09796092532199954
iteration : 5694
train acc:  0.7421875
train loss:  0.4916097819805145
train gradient:  0.161352089488895
iteration : 5695
train acc:  0.7265625
train loss:  0.5076152086257935
train gradient:  0.15779874260298987
iteration : 5696
train acc:  0.7578125
train loss:  0.4870072603225708
train gradient:  0.13159851022649735
iteration : 5697
train acc:  0.6953125
train loss:  0.5342323184013367
train gradient:  0.15802444830556406
iteration : 5698
train acc:  0.7265625
train loss:  0.4892699122428894
train gradient:  0.13383589235873083
iteration : 5699
train acc:  0.6875
train loss:  0.5596435070037842
train gradient:  0.1594377540575889
iteration : 5700
train acc:  0.671875
train loss:  0.5365082621574402
train gradient:  0.15301312516245724
iteration : 5701
train acc:  0.71875
train loss:  0.48963040113449097
train gradient:  0.18429495103867627
iteration : 5702
train acc:  0.7109375
train loss:  0.49642783403396606
train gradient:  0.1558106112963652
iteration : 5703
train acc:  0.6953125
train loss:  0.5737082362174988
train gradient:  0.20109025491763438
iteration : 5704
train acc:  0.734375
train loss:  0.48743462562561035
train gradient:  0.18648514365879326
iteration : 5705
train acc:  0.734375
train loss:  0.518528938293457
train gradient:  0.25362643446532995
iteration : 5706
train acc:  0.7265625
train loss:  0.5224082469940186
train gradient:  0.16122740336396654
iteration : 5707
train acc:  0.6953125
train loss:  0.5656111240386963
train gradient:  0.143231838825723
iteration : 5708
train acc:  0.7421875
train loss:  0.4753701090812683
train gradient:  0.1255424513734678
iteration : 5709
train acc:  0.7578125
train loss:  0.4818294048309326
train gradient:  0.1365220759821131
iteration : 5710
train acc:  0.7265625
train loss:  0.5359537601470947
train gradient:  0.12583855760544105
iteration : 5711
train acc:  0.6796875
train loss:  0.5264896154403687
train gradient:  0.18654257493944454
iteration : 5712
train acc:  0.703125
train loss:  0.5410684943199158
train gradient:  0.15694166046736763
iteration : 5713
train acc:  0.7578125
train loss:  0.4882529675960541
train gradient:  0.11395382419330581
iteration : 5714
train acc:  0.7265625
train loss:  0.5240236520767212
train gradient:  0.14073968819353816
iteration : 5715
train acc:  0.640625
train loss:  0.5511672496795654
train gradient:  0.16945483341280193
iteration : 5716
train acc:  0.7265625
train loss:  0.48797711730003357
train gradient:  0.13411802496684194
iteration : 5717
train acc:  0.75
train loss:  0.4826515018939972
train gradient:  0.11679670956567298
iteration : 5718
train acc:  0.671875
train loss:  0.553290605545044
train gradient:  0.12841844618358633
iteration : 5719
train acc:  0.734375
train loss:  0.4778895974159241
train gradient:  0.13376187961396752
iteration : 5720
train acc:  0.75
train loss:  0.4889048635959625
train gradient:  0.17818919649346826
iteration : 5721
train acc:  0.6875
train loss:  0.5095778703689575
train gradient:  0.10865738287288919
iteration : 5722
train acc:  0.7109375
train loss:  0.5219962000846863
train gradient:  0.17379160820607925
iteration : 5723
train acc:  0.65625
train loss:  0.6186231970787048
train gradient:  0.20313024627164372
iteration : 5724
train acc:  0.7421875
train loss:  0.49377888441085815
train gradient:  0.12671213620771665
iteration : 5725
train acc:  0.7109375
train loss:  0.539390504360199
train gradient:  0.3267936307721982
iteration : 5726
train acc:  0.765625
train loss:  0.5162510275840759
train gradient:  0.1649600224713107
iteration : 5727
train acc:  0.75
train loss:  0.4729102849960327
train gradient:  0.12975947543466068
iteration : 5728
train acc:  0.7109375
train loss:  0.564806342124939
train gradient:  0.15180335244680926
iteration : 5729
train acc:  0.71875
train loss:  0.5323861837387085
train gradient:  0.15992099646692037
iteration : 5730
train acc:  0.734375
train loss:  0.46450406312942505
train gradient:  0.11251664082119535
iteration : 5731
train acc:  0.7734375
train loss:  0.44831687211990356
train gradient:  0.12041200745827431
iteration : 5732
train acc:  0.734375
train loss:  0.48370566964149475
train gradient:  0.13397901050810512
iteration : 5733
train acc:  0.6953125
train loss:  0.5756234526634216
train gradient:  0.14781285608158307
iteration : 5734
train acc:  0.7421875
train loss:  0.500262439250946
train gradient:  0.1390181771059641
iteration : 5735
train acc:  0.7734375
train loss:  0.4641774296760559
train gradient:  0.09694642474137322
iteration : 5736
train acc:  0.7265625
train loss:  0.5069395899772644
train gradient:  0.11258512621153748
iteration : 5737
train acc:  0.6875
train loss:  0.5494569540023804
train gradient:  0.15464488392558184
iteration : 5738
train acc:  0.703125
train loss:  0.5030486583709717
train gradient:  0.13113755498919732
iteration : 5739
train acc:  0.7109375
train loss:  0.5412002801895142
train gradient:  0.15380907628475507
iteration : 5740
train acc:  0.7734375
train loss:  0.5167697668075562
train gradient:  0.15550449869129204
iteration : 5741
train acc:  0.703125
train loss:  0.5313087701797485
train gradient:  0.1593695850662591
iteration : 5742
train acc:  0.7265625
train loss:  0.4994008243083954
train gradient:  0.10396190196287598
iteration : 5743
train acc:  0.640625
train loss:  0.6234564781188965
train gradient:  0.1849949978823578
iteration : 5744
train acc:  0.734375
train loss:  0.4919629991054535
train gradient:  0.14711268701442393
iteration : 5745
train acc:  0.796875
train loss:  0.47362273931503296
train gradient:  0.11837726328337003
iteration : 5746
train acc:  0.7265625
train loss:  0.5628305673599243
train gradient:  0.16389434076284115
iteration : 5747
train acc:  0.734375
train loss:  0.4438456892967224
train gradient:  0.10814088416713447
iteration : 5748
train acc:  0.6796875
train loss:  0.5751427412033081
train gradient:  0.21356797714640718
iteration : 5749
train acc:  0.7421875
train loss:  0.5439345836639404
train gradient:  0.13683711594705092
iteration : 5750
train acc:  0.7734375
train loss:  0.49425119161605835
train gradient:  0.15701207325648991
iteration : 5751
train acc:  0.78125
train loss:  0.45373043417930603
train gradient:  0.1458054080383359
iteration : 5752
train acc:  0.71875
train loss:  0.4994218051433563
train gradient:  0.1402378015416056
iteration : 5753
train acc:  0.71875
train loss:  0.5348584651947021
train gradient:  0.180474428423604
iteration : 5754
train acc:  0.6875
train loss:  0.5640199184417725
train gradient:  0.1473314580827127
iteration : 5755
train acc:  0.7734375
train loss:  0.48666834831237793
train gradient:  0.12795157329324963
iteration : 5756
train acc:  0.7578125
train loss:  0.4842504858970642
train gradient:  0.13162075535490902
iteration : 5757
train acc:  0.7109375
train loss:  0.5449254512786865
train gradient:  0.19811006578557244
iteration : 5758
train acc:  0.7109375
train loss:  0.4910590946674347
train gradient:  0.14007156229734086
iteration : 5759
train acc:  0.734375
train loss:  0.5303841233253479
train gradient:  0.1418460073573391
iteration : 5760
train acc:  0.75
train loss:  0.500853419303894
train gradient:  0.15660641243589618
iteration : 5761
train acc:  0.7734375
train loss:  0.4496068060398102
train gradient:  0.10613872858787965
iteration : 5762
train acc:  0.6875
train loss:  0.5676504373550415
train gradient:  0.17212500319027715
iteration : 5763
train acc:  0.6953125
train loss:  0.5165667533874512
train gradient:  0.15788972280978103
iteration : 5764
train acc:  0.7265625
train loss:  0.49689576029777527
train gradient:  0.1169238425972084
iteration : 5765
train acc:  0.7421875
train loss:  0.5345673561096191
train gradient:  0.12805106860589377
iteration : 5766
train acc:  0.6953125
train loss:  0.5218879580497742
train gradient:  0.18146357355572829
iteration : 5767
train acc:  0.765625
train loss:  0.4597471356391907
train gradient:  0.14210697621394813
iteration : 5768
train acc:  0.6796875
train loss:  0.4944620132446289
train gradient:  0.1273956813286278
iteration : 5769
train acc:  0.71875
train loss:  0.5155762434005737
train gradient:  0.15612948501844864
iteration : 5770
train acc:  0.765625
train loss:  0.5269047021865845
train gradient:  0.12593230398492788
iteration : 5771
train acc:  0.6796875
train loss:  0.5449537038803101
train gradient:  0.16685827267584824
iteration : 5772
train acc:  0.6953125
train loss:  0.5750693082809448
train gradient:  0.17966764908007996
iteration : 5773
train acc:  0.765625
train loss:  0.4428859353065491
train gradient:  0.13593367976182258
iteration : 5774
train acc:  0.71875
train loss:  0.5074858665466309
train gradient:  0.13156878312135484
iteration : 5775
train acc:  0.7109375
train loss:  0.5315504670143127
train gradient:  0.18571315307643327
iteration : 5776
train acc:  0.765625
train loss:  0.4724929928779602
train gradient:  0.11918366317998684
iteration : 5777
train acc:  0.734375
train loss:  0.5062520503997803
train gradient:  0.16434422306450636
iteration : 5778
train acc:  0.7734375
train loss:  0.4807125926017761
train gradient:  0.13207672044981084
iteration : 5779
train acc:  0.6796875
train loss:  0.5739809274673462
train gradient:  0.1703263892290368
iteration : 5780
train acc:  0.7109375
train loss:  0.507272481918335
train gradient:  0.13902924467579425
iteration : 5781
train acc:  0.7734375
train loss:  0.47387540340423584
train gradient:  0.13202728246409431
iteration : 5782
train acc:  0.6640625
train loss:  0.5446333885192871
train gradient:  0.1652640542306828
iteration : 5783
train acc:  0.6484375
train loss:  0.5782946944236755
train gradient:  0.18345776907897227
iteration : 5784
train acc:  0.734375
train loss:  0.5440649390220642
train gradient:  0.1351167736302617
iteration : 5785
train acc:  0.7578125
train loss:  0.5240229368209839
train gradient:  0.14383359755495
iteration : 5786
train acc:  0.7734375
train loss:  0.44827961921691895
train gradient:  0.10256663022848983
iteration : 5787
train acc:  0.734375
train loss:  0.4901989698410034
train gradient:  0.1701879119148192
iteration : 5788
train acc:  0.7265625
train loss:  0.5585536360740662
train gradient:  0.19137344640314596
iteration : 5789
train acc:  0.734375
train loss:  0.4611889123916626
train gradient:  0.1262742786595133
iteration : 5790
train acc:  0.7265625
train loss:  0.4754422605037689
train gradient:  0.10683146004277137
iteration : 5791
train acc:  0.6953125
train loss:  0.5311716794967651
train gradient:  0.15622464780064954
iteration : 5792
train acc:  0.7109375
train loss:  0.4997691512107849
train gradient:  0.12309371112852502
iteration : 5793
train acc:  0.7109375
train loss:  0.5603615045547485
train gradient:  0.15568019951328949
iteration : 5794
train acc:  0.8125
train loss:  0.45743870735168457
train gradient:  0.10954979145613764
iteration : 5795
train acc:  0.765625
train loss:  0.4569311738014221
train gradient:  0.14337946072666646
iteration : 5796
train acc:  0.65625
train loss:  0.5931236743927002
train gradient:  0.1801746986718526
iteration : 5797
train acc:  0.7578125
train loss:  0.4947267770767212
train gradient:  0.14852445794094038
iteration : 5798
train acc:  0.8203125
train loss:  0.4551088511943817
train gradient:  0.10339919417520708
iteration : 5799
train acc:  0.71875
train loss:  0.5021407604217529
train gradient:  0.12070650162977588
iteration : 5800
train acc:  0.78125
train loss:  0.49393704533576965
train gradient:  0.14078663777715283
iteration : 5801
train acc:  0.7421875
train loss:  0.49253010749816895
train gradient:  0.11102029356642704
iteration : 5802
train acc:  0.734375
train loss:  0.5516178607940674
train gradient:  0.15088450412284365
iteration : 5803
train acc:  0.71875
train loss:  0.550323486328125
train gradient:  0.16622287866831814
iteration : 5804
train acc:  0.7109375
train loss:  0.5102671384811401
train gradient:  0.1822777575314016
iteration : 5805
train acc:  0.703125
train loss:  0.5227500200271606
train gradient:  0.1650605233810592
iteration : 5806
train acc:  0.7265625
train loss:  0.4721057116985321
train gradient:  0.13491417988112758
iteration : 5807
train acc:  0.7578125
train loss:  0.5110658407211304
train gradient:  0.1316549767471426
iteration : 5808
train acc:  0.75
train loss:  0.5045552849769592
train gradient:  0.12042699324145197
iteration : 5809
train acc:  0.7421875
train loss:  0.4868759214878082
train gradient:  0.12334695353731363
iteration : 5810
train acc:  0.765625
train loss:  0.4811975955963135
train gradient:  0.1318293786292164
iteration : 5811
train acc:  0.7578125
train loss:  0.4659104347229004
train gradient:  0.14454874124980127
iteration : 5812
train acc:  0.671875
train loss:  0.586236298084259
train gradient:  0.2164306663883031
iteration : 5813
train acc:  0.8046875
train loss:  0.4291551113128662
train gradient:  0.10782380915571053
iteration : 5814
train acc:  0.75
train loss:  0.5348612070083618
train gradient:  0.18485122786498764
iteration : 5815
train acc:  0.6640625
train loss:  0.5639950633049011
train gradient:  0.16987518386793524
iteration : 5816
train acc:  0.703125
train loss:  0.5067941546440125
train gradient:  0.12918875763758303
iteration : 5817
train acc:  0.6484375
train loss:  0.5455899238586426
train gradient:  0.15323084773621426
iteration : 5818
train acc:  0.734375
train loss:  0.49260449409484863
train gradient:  0.12523254200878992
iteration : 5819
train acc:  0.7109375
train loss:  0.4937339723110199
train gradient:  0.12769739828757604
iteration : 5820
train acc:  0.734375
train loss:  0.544556200504303
train gradient:  0.1551348869964993
iteration : 5821
train acc:  0.7734375
train loss:  0.46111807227134705
train gradient:  0.15214693490908615
iteration : 5822
train acc:  0.7421875
train loss:  0.5142483115196228
train gradient:  0.13027721893342809
iteration : 5823
train acc:  0.8125
train loss:  0.4509083926677704
train gradient:  0.11489052278097119
iteration : 5824
train acc:  0.78125
train loss:  0.461234986782074
train gradient:  0.12345527823362445
iteration : 5825
train acc:  0.6796875
train loss:  0.5275090336799622
train gradient:  0.1457685412622936
iteration : 5826
train acc:  0.703125
train loss:  0.5632504224777222
train gradient:  0.21630196297114035
iteration : 5827
train acc:  0.7109375
train loss:  0.5457448363304138
train gradient:  0.20098315839197511
iteration : 5828
train acc:  0.7734375
train loss:  0.46762755513191223
train gradient:  0.12362657919836022
iteration : 5829
train acc:  0.7421875
train loss:  0.5064250826835632
train gradient:  0.12746540894143304
iteration : 5830
train acc:  0.75
train loss:  0.5384231805801392
train gradient:  0.19408739832205318
iteration : 5831
train acc:  0.7265625
train loss:  0.49222707748413086
train gradient:  0.13718240812721422
iteration : 5832
train acc:  0.7265625
train loss:  0.5138195753097534
train gradient:  0.15297047742300118
iteration : 5833
train acc:  0.7734375
train loss:  0.4978524446487427
train gradient:  0.1373253088684816
iteration : 5834
train acc:  0.6953125
train loss:  0.5053394436836243
train gradient:  0.15686042615898194
iteration : 5835
train acc:  0.7734375
train loss:  0.4776449501514435
train gradient:  0.12756652623480916
iteration : 5836
train acc:  0.7421875
train loss:  0.5042209625244141
train gradient:  0.1324238943433454
iteration : 5837
train acc:  0.7578125
train loss:  0.4964236617088318
train gradient:  0.17980807606643634
iteration : 5838
train acc:  0.75
train loss:  0.4418347477912903
train gradient:  0.10851238614132246
iteration : 5839
train acc:  0.75
train loss:  0.4958152770996094
train gradient:  0.13330058072167073
iteration : 5840
train acc:  0.765625
train loss:  0.4805980920791626
train gradient:  0.16947584140657995
iteration : 5841
train acc:  0.7265625
train loss:  0.47999799251556396
train gradient:  0.1352475226973403
iteration : 5842
train acc:  0.6875
train loss:  0.6019889116287231
train gradient:  0.18418361951643442
iteration : 5843
train acc:  0.765625
train loss:  0.5208066701889038
train gradient:  0.17816599831181362
iteration : 5844
train acc:  0.7109375
train loss:  0.5393192768096924
train gradient:  0.14870853893037117
iteration : 5845
train acc:  0.7265625
train loss:  0.49478086829185486
train gradient:  0.1504595581712337
iteration : 5846
train acc:  0.6484375
train loss:  0.6514465808868408
train gradient:  0.1972334459144424
iteration : 5847
train acc:  0.75
train loss:  0.4801034927368164
train gradient:  0.12450307106273484
iteration : 5848
train acc:  0.7734375
train loss:  0.48597657680511475
train gradient:  0.15117759525322452
iteration : 5849
train acc:  0.734375
train loss:  0.514122486114502
train gradient:  0.17476722299443403
iteration : 5850
train acc:  0.7265625
train loss:  0.5399336814880371
train gradient:  0.1589201492762609
iteration : 5851
train acc:  0.734375
train loss:  0.532371997833252
train gradient:  0.14218639623374263
iteration : 5852
train acc:  0.6796875
train loss:  0.5303674340248108
train gradient:  0.1836882108195797
iteration : 5853
train acc:  0.6796875
train loss:  0.5324788689613342
train gradient:  0.14335852579877578
iteration : 5854
train acc:  0.71875
train loss:  0.5098134279251099
train gradient:  0.1527914335380937
iteration : 5855
train acc:  0.7265625
train loss:  0.5005345940589905
train gradient:  0.123020876385853
iteration : 5856
train acc:  0.734375
train loss:  0.5382630228996277
train gradient:  0.1709778264496642
iteration : 5857
train acc:  0.7265625
train loss:  0.5031981468200684
train gradient:  0.15685081577343118
iteration : 5858
train acc:  0.6796875
train loss:  0.5089294910430908
train gradient:  0.18685396731818948
iteration : 5859
train acc:  0.7578125
train loss:  0.46854260563850403
train gradient:  0.12891798479920544
iteration : 5860
train acc:  0.6640625
train loss:  0.5375273823738098
train gradient:  0.16453300316442981
iteration : 5861
train acc:  0.71875
train loss:  0.4749964475631714
train gradient:  0.14155564743069934
iteration : 5862
train acc:  0.7109375
train loss:  0.5233056545257568
train gradient:  0.14094994525722582
iteration : 5863
train acc:  0.7578125
train loss:  0.4562467932701111
train gradient:  0.10429488735626499
iteration : 5864
train acc:  0.7265625
train loss:  0.49766454100608826
train gradient:  0.1342679659056366
iteration : 5865
train acc:  0.7734375
train loss:  0.48208773136138916
train gradient:  0.15197150916013435
iteration : 5866
train acc:  0.7421875
train loss:  0.46654289960861206
train gradient:  0.1449612148791668
iteration : 5867
train acc:  0.703125
train loss:  0.5044090747833252
train gradient:  0.1552477146902677
iteration : 5868
train acc:  0.7421875
train loss:  0.5025424361228943
train gradient:  0.11595901260108628
iteration : 5869
train acc:  0.7265625
train loss:  0.5611262917518616
train gradient:  0.19104941349206864
iteration : 5870
train acc:  0.75
train loss:  0.48833155632019043
train gradient:  0.13827708677306658
iteration : 5871
train acc:  0.671875
train loss:  0.5487252473831177
train gradient:  0.13633077195908694
iteration : 5872
train acc:  0.7578125
train loss:  0.5076471567153931
train gradient:  0.16551148569240706
iteration : 5873
train acc:  0.7109375
train loss:  0.5418768525123596
train gradient:  0.15777817373664674
iteration : 5874
train acc:  0.7265625
train loss:  0.48183274269104004
train gradient:  0.15526545781281714
iteration : 5875
train acc:  0.703125
train loss:  0.6013892889022827
train gradient:  0.1924557833145582
iteration : 5876
train acc:  0.75
train loss:  0.46984827518463135
train gradient:  0.12705231327234648
iteration : 5877
train acc:  0.6875
train loss:  0.5578583478927612
train gradient:  0.1570347569292777
iteration : 5878
train acc:  0.71875
train loss:  0.5084360837936401
train gradient:  0.21044761563839187
iteration : 5879
train acc:  0.7421875
train loss:  0.5213836431503296
train gradient:  0.15449248748207475
iteration : 5880
train acc:  0.71875
train loss:  0.5301122665405273
train gradient:  0.1527049876891628
iteration : 5881
train acc:  0.7421875
train loss:  0.4938684403896332
train gradient:  0.14471394081656597
iteration : 5882
train acc:  0.7421875
train loss:  0.5196290016174316
train gradient:  0.12164361326493137
iteration : 5883
train acc:  0.7578125
train loss:  0.450975626707077
train gradient:  0.11259910006124005
iteration : 5884
train acc:  0.734375
train loss:  0.4994954764842987
train gradient:  0.14672989293546207
iteration : 5885
train acc:  0.7265625
train loss:  0.5189473628997803
train gradient:  0.18025316108421108
iteration : 5886
train acc:  0.7734375
train loss:  0.5409644842147827
train gradient:  0.20310237676401124
iteration : 5887
train acc:  0.7421875
train loss:  0.4709504246711731
train gradient:  0.12505655280304226
iteration : 5888
train acc:  0.7734375
train loss:  0.3993881940841675
train gradient:  0.1133630230992428
iteration : 5889
train acc:  0.734375
train loss:  0.5330410003662109
train gradient:  0.13304781799781626
iteration : 5890
train acc:  0.7734375
train loss:  0.4620788097381592
train gradient:  0.11497735755564831
iteration : 5891
train acc:  0.7578125
train loss:  0.47971126437187195
train gradient:  0.13019749839545564
iteration : 5892
train acc:  0.7734375
train loss:  0.48534655570983887
train gradient:  0.139367212834755
iteration : 5893
train acc:  0.640625
train loss:  0.575103759765625
train gradient:  0.19012470375136964
iteration : 5894
train acc:  0.75
train loss:  0.5069271326065063
train gradient:  0.1521770851907167
iteration : 5895
train acc:  0.796875
train loss:  0.4334385097026825
train gradient:  0.1119731301208937
iteration : 5896
train acc:  0.71875
train loss:  0.45739778876304626
train gradient:  0.13303912829128145
iteration : 5897
train acc:  0.703125
train loss:  0.5457483530044556
train gradient:  0.17922912223512433
iteration : 5898
train acc:  0.828125
train loss:  0.43557924032211304
train gradient:  0.11948008139627178
iteration : 5899
train acc:  0.7734375
train loss:  0.481823593378067
train gradient:  0.14924511816492736
iteration : 5900
train acc:  0.703125
train loss:  0.5448160171508789
train gradient:  0.14248156998436962
iteration : 5901
train acc:  0.734375
train loss:  0.5055463314056396
train gradient:  0.15343553842545987
iteration : 5902
train acc:  0.765625
train loss:  0.45163053274154663
train gradient:  0.13730126023063127
iteration : 5903
train acc:  0.8046875
train loss:  0.44539421796798706
train gradient:  0.11661984823998929
iteration : 5904
train acc:  0.7578125
train loss:  0.5221883058547974
train gradient:  0.12545367192746043
iteration : 5905
train acc:  0.6796875
train loss:  0.547559380531311
train gradient:  0.15806499783746136
iteration : 5906
train acc:  0.6953125
train loss:  0.5407138466835022
train gradient:  0.15316748315716971
iteration : 5907
train acc:  0.7734375
train loss:  0.5061146020889282
train gradient:  0.16421894587532013
iteration : 5908
train acc:  0.78125
train loss:  0.5011429190635681
train gradient:  0.12843972810380763
iteration : 5909
train acc:  0.7421875
train loss:  0.5279585123062134
train gradient:  0.1521592300756654
iteration : 5910
train acc:  0.75
train loss:  0.5230730772018433
train gradient:  0.1475837204985586
iteration : 5911
train acc:  0.6875
train loss:  0.5496145486831665
train gradient:  0.18351582233695068
iteration : 5912
train acc:  0.7265625
train loss:  0.4716801941394806
train gradient:  0.12069426737765572
iteration : 5913
train acc:  0.6796875
train loss:  0.5598481893539429
train gradient:  0.18508185193735383
iteration : 5914
train acc:  0.75
train loss:  0.46190229058265686
train gradient:  0.1081796968796297
iteration : 5915
train acc:  0.75
train loss:  0.5193144083023071
train gradient:  0.18758020103496215
iteration : 5916
train acc:  0.7890625
train loss:  0.5043827295303345
train gradient:  0.1595839433111858
iteration : 5917
train acc:  0.6796875
train loss:  0.5685722231864929
train gradient:  0.15388737097929628
iteration : 5918
train acc:  0.7578125
train loss:  0.49611979722976685
train gradient:  0.14992049206689922
iteration : 5919
train acc:  0.796875
train loss:  0.4273313581943512
train gradient:  0.1436097267509348
iteration : 5920
train acc:  0.8046875
train loss:  0.44198209047317505
train gradient:  0.14789919664764914
iteration : 5921
train acc:  0.640625
train loss:  0.5784603357315063
train gradient:  0.16191071601222093
iteration : 5922
train acc:  0.703125
train loss:  0.5531355142593384
train gradient:  0.14958307386518244
iteration : 5923
train acc:  0.7265625
train loss:  0.514676570892334
train gradient:  0.1432008719255989
iteration : 5924
train acc:  0.703125
train loss:  0.5135535001754761
train gradient:  0.12521929805174803
iteration : 5925
train acc:  0.765625
train loss:  0.4433380961418152
train gradient:  0.12966705004702977
iteration : 5926
train acc:  0.765625
train loss:  0.5043065547943115
train gradient:  0.14465596150647073
iteration : 5927
train acc:  0.703125
train loss:  0.5004133582115173
train gradient:  0.15215006572831286
iteration : 5928
train acc:  0.75
train loss:  0.4760291278362274
train gradient:  0.14428331667559674
iteration : 5929
train acc:  0.78125
train loss:  0.49009865522384644
train gradient:  0.11663341680338282
iteration : 5930
train acc:  0.75
train loss:  0.4989522099494934
train gradient:  0.18851761226761665
iteration : 5931
train acc:  0.71875
train loss:  0.5018050670623779
train gradient:  0.17462883816810182
iteration : 5932
train acc:  0.6875
train loss:  0.5856668949127197
train gradient:  0.15992307468542022
iteration : 5933
train acc:  0.7109375
train loss:  0.4891815781593323
train gradient:  0.1696090031918488
iteration : 5934
train acc:  0.734375
train loss:  0.5157665014266968
train gradient:  0.15721107763431574
iteration : 5935
train acc:  0.75
train loss:  0.49171581864356995
train gradient:  0.11815227728332685
iteration : 5936
train acc:  0.7421875
train loss:  0.49720868468284607
train gradient:  0.15501277211115908
iteration : 5937
train acc:  0.765625
train loss:  0.5052992105484009
train gradient:  0.14049462101554352
iteration : 5938
train acc:  0.734375
train loss:  0.47702041268348694
train gradient:  0.15005839511751057
iteration : 5939
train acc:  0.6875
train loss:  0.529505729675293
train gradient:  0.15059037654195884
iteration : 5940
train acc:  0.6875
train loss:  0.5170044302940369
train gradient:  0.14634162899586312
iteration : 5941
train acc:  0.71875
train loss:  0.5548855662345886
train gradient:  0.18462849632087436
iteration : 5942
train acc:  0.734375
train loss:  0.5097393989562988
train gradient:  0.1599002154843378
iteration : 5943
train acc:  0.734375
train loss:  0.45341116189956665
train gradient:  0.11136909156050873
iteration : 5944
train acc:  0.71875
train loss:  0.522432267665863
train gradient:  0.14329988990023043
iteration : 5945
train acc:  0.734375
train loss:  0.49424850940704346
train gradient:  0.13105908808188219
iteration : 5946
train acc:  0.7421875
train loss:  0.4698169231414795
train gradient:  0.1292906261697006
iteration : 5947
train acc:  0.78125
train loss:  0.45603418350219727
train gradient:  0.11172979327065413
iteration : 5948
train acc:  0.6875
train loss:  0.46072524785995483
train gradient:  0.12083472722202504
iteration : 5949
train acc:  0.7109375
train loss:  0.5110976099967957
train gradient:  0.15649812778286287
iteration : 5950
train acc:  0.78125
train loss:  0.441677987575531
train gradient:  0.10869139673941734
iteration : 5951
train acc:  0.78125
train loss:  0.4439539313316345
train gradient:  0.12665225324663096
iteration : 5952
train acc:  0.7265625
train loss:  0.6053632497787476
train gradient:  0.17851018844135963
iteration : 5953
train acc:  0.6875
train loss:  0.5423624515533447
train gradient:  0.14397492374524917
iteration : 5954
train acc:  0.765625
train loss:  0.4793272912502289
train gradient:  0.16161720261963045
iteration : 5955
train acc:  0.6875
train loss:  0.5362575650215149
train gradient:  0.1875968579830033
iteration : 5956
train acc:  0.734375
train loss:  0.4800775349140167
train gradient:  0.13147529203233616
iteration : 5957
train acc:  0.7109375
train loss:  0.540225088596344
train gradient:  0.17299081879200595
iteration : 5958
train acc:  0.734375
train loss:  0.5295579433441162
train gradient:  0.14635667565623794
iteration : 5959
train acc:  0.734375
train loss:  0.5258690118789673
train gradient:  0.18555244925894943
iteration : 5960
train acc:  0.71875
train loss:  0.5119951367378235
train gradient:  0.14869401661838624
iteration : 5961
train acc:  0.7265625
train loss:  0.5280664563179016
train gradient:  0.1427223939339315
iteration : 5962
train acc:  0.6953125
train loss:  0.5915673971176147
train gradient:  0.17590504749796582
iteration : 5963
train acc:  0.7890625
train loss:  0.4286503791809082
train gradient:  0.12198362726159045
iteration : 5964
train acc:  0.7578125
train loss:  0.5143560171127319
train gradient:  0.1323322253970829
iteration : 5965
train acc:  0.7578125
train loss:  0.5545253753662109
train gradient:  0.18167065072739239
iteration : 5966
train acc:  0.75
train loss:  0.5309377908706665
train gradient:  0.11946930994318349
iteration : 5967
train acc:  0.78125
train loss:  0.48087257146835327
train gradient:  0.1871706528688591
iteration : 5968
train acc:  0.71875
train loss:  0.5358819961547852
train gradient:  0.14834477513820254
iteration : 5969
train acc:  0.6953125
train loss:  0.5572537183761597
train gradient:  0.20308750605566578
iteration : 5970
train acc:  0.734375
train loss:  0.5094923973083496
train gradient:  0.12666024810017665
iteration : 5971
train acc:  0.75
train loss:  0.5167852640151978
train gradient:  0.1781788339808668
iteration : 5972
train acc:  0.7734375
train loss:  0.49131301045417786
train gradient:  0.1379137328951321
iteration : 5973
train acc:  0.7734375
train loss:  0.458157479763031
train gradient:  0.11118227223219847
iteration : 5974
train acc:  0.734375
train loss:  0.529367208480835
train gradient:  0.127633808759089
iteration : 5975
train acc:  0.7734375
train loss:  0.4759085178375244
train gradient:  0.14910352768942048
iteration : 5976
train acc:  0.7109375
train loss:  0.5592842102050781
train gradient:  0.2105754556544056
iteration : 5977
train acc:  0.6796875
train loss:  0.5323289632797241
train gradient:  0.19400358586513505
iteration : 5978
train acc:  0.7265625
train loss:  0.5096266269683838
train gradient:  0.1312001533765854
iteration : 5979
train acc:  0.6796875
train loss:  0.6246923208236694
train gradient:  0.1774795090508089
iteration : 5980
train acc:  0.71875
train loss:  0.5902143120765686
train gradient:  0.19906081924579827
iteration : 5981
train acc:  0.7578125
train loss:  0.4986799955368042
train gradient:  0.14414337378266567
iteration : 5982
train acc:  0.7421875
train loss:  0.508232831954956
train gradient:  0.1387117228373
iteration : 5983
train acc:  0.765625
train loss:  0.47693002223968506
train gradient:  0.13249859568949113
iteration : 5984
train acc:  0.7890625
train loss:  0.4654994308948517
train gradient:  0.15521545616626453
iteration : 5985
train acc:  0.75
train loss:  0.4719003438949585
train gradient:  0.12480037374518063
iteration : 5986
train acc:  0.703125
train loss:  0.5728667378425598
train gradient:  0.14998896767701853
iteration : 5987
train acc:  0.8046875
train loss:  0.42788785696029663
train gradient:  0.11465851216178168
iteration : 5988
train acc:  0.7734375
train loss:  0.5161207914352417
train gradient:  0.16138132370125347
iteration : 5989
train acc:  0.75
train loss:  0.4817359149456024
train gradient:  0.11241457467800921
iteration : 5990
train acc:  0.7578125
train loss:  0.5414607524871826
train gradient:  0.14348847080516247
iteration : 5991
train acc:  0.75
train loss:  0.48349177837371826
train gradient:  0.11136061529175793
iteration : 5992
train acc:  0.703125
train loss:  0.543586790561676
train gradient:  0.19995449560186124
iteration : 5993
train acc:  0.7421875
train loss:  0.5027409195899963
train gradient:  0.13975824242936316
iteration : 5994
train acc:  0.609375
train loss:  0.5973882079124451
train gradient:  0.20457305895569738
iteration : 5995
train acc:  0.6796875
train loss:  0.5393500328063965
train gradient:  0.13492204731379484
iteration : 5996
train acc:  0.78125
train loss:  0.44534415006637573
train gradient:  0.1336015958798294
iteration : 5997
train acc:  0.765625
train loss:  0.44109857082366943
train gradient:  0.13076274906027868
iteration : 5998
train acc:  0.6953125
train loss:  0.5711318850517273
train gradient:  0.1763311880289375
iteration : 5999
train acc:  0.7734375
train loss:  0.46989190578460693
train gradient:  0.1376666627634825
iteration : 6000
train acc:  0.6796875
train loss:  0.6267080307006836
train gradient:  0.2056667268125315
iteration : 6001
train acc:  0.7421875
train loss:  0.54313063621521
train gradient:  0.155727414215652
iteration : 6002
train acc:  0.734375
train loss:  0.4967397153377533
train gradient:  0.15922278597553952
iteration : 6003
train acc:  0.6953125
train loss:  0.5560725927352905
train gradient:  0.14208559645994767
iteration : 6004
train acc:  0.7265625
train loss:  0.5106815695762634
train gradient:  0.14649579997575068
iteration : 6005
train acc:  0.7734375
train loss:  0.45851629972457886
train gradient:  0.1712086246002598
iteration : 6006
train acc:  0.765625
train loss:  0.4726685583591461
train gradient:  0.17913770789846234
iteration : 6007
train acc:  0.6640625
train loss:  0.537096381187439
train gradient:  0.16044820101958301
iteration : 6008
train acc:  0.7421875
train loss:  0.4719899296760559
train gradient:  0.1231609737165206
iteration : 6009
train acc:  0.8125
train loss:  0.4141935706138611
train gradient:  0.10732472475649187
iteration : 6010
train acc:  0.75
train loss:  0.45473939180374146
train gradient:  0.13111641779173178
iteration : 6011
train acc:  0.6796875
train loss:  0.6095432639122009
train gradient:  0.16926217616995734
iteration : 6012
train acc:  0.71875
train loss:  0.5369643568992615
train gradient:  0.16597665754426766
iteration : 6013
train acc:  0.65625
train loss:  0.530730128288269
train gradient:  0.15276569386347746
iteration : 6014
train acc:  0.75
train loss:  0.4911637306213379
train gradient:  0.15969547742920187
iteration : 6015
train acc:  0.6796875
train loss:  0.5840526819229126
train gradient:  0.1801620678400465
iteration : 6016
train acc:  0.78125
train loss:  0.47475665807724
train gradient:  0.16882692543291739
iteration : 6017
train acc:  0.7265625
train loss:  0.46596401929855347
train gradient:  0.12554958950978032
iteration : 6018
train acc:  0.796875
train loss:  0.4459384083747864
train gradient:  0.1246947686096786
iteration : 6019
train acc:  0.7265625
train loss:  0.537338137626648
train gradient:  0.1879795239190542
iteration : 6020
train acc:  0.75
train loss:  0.4473755657672882
train gradient:  0.15329389841586982
iteration : 6021
train acc:  0.7421875
train loss:  0.5604643821716309
train gradient:  0.1462495294032494
iteration : 6022
train acc:  0.7109375
train loss:  0.5125759840011597
train gradient:  0.19616130552398614
iteration : 6023
train acc:  0.71875
train loss:  0.5177710056304932
train gradient:  0.16868219818982041
iteration : 6024
train acc:  0.765625
train loss:  0.5251007080078125
train gradient:  0.1466707928839665
iteration : 6025
train acc:  0.6953125
train loss:  0.553476095199585
train gradient:  0.19479602414121822
iteration : 6026
train acc:  0.6484375
train loss:  0.5571474432945251
train gradient:  0.16551900143523463
iteration : 6027
train acc:  0.7734375
train loss:  0.5676785111427307
train gradient:  0.1670898691203532
iteration : 6028
train acc:  0.703125
train loss:  0.5403178334236145
train gradient:  0.14809939200954275
iteration : 6029
train acc:  0.7109375
train loss:  0.573765754699707
train gradient:  0.1914369388103626
iteration : 6030
train acc:  0.6953125
train loss:  0.5763993263244629
train gradient:  0.14520010867501343
iteration : 6031
train acc:  0.6796875
train loss:  0.5501855611801147
train gradient:  0.1346284882509888
iteration : 6032
train acc:  0.734375
train loss:  0.49257519841194153
train gradient:  0.12995777087355642
iteration : 6033
train acc:  0.7421875
train loss:  0.4904879927635193
train gradient:  0.1509506622889421
iteration : 6034
train acc:  0.7578125
train loss:  0.4857736825942993
train gradient:  0.11410439191452107
iteration : 6035
train acc:  0.734375
train loss:  0.4643070101737976
train gradient:  0.14175590300215157
iteration : 6036
train acc:  0.7890625
train loss:  0.4544575810432434
train gradient:  0.11589431471514959
iteration : 6037
train acc:  0.734375
train loss:  0.5471079349517822
train gradient:  0.18973813816400198
iteration : 6038
train acc:  0.671875
train loss:  0.6471860408782959
train gradient:  0.29151042346311457
iteration : 6039
train acc:  0.7578125
train loss:  0.5157434940338135
train gradient:  0.14497816655881396
iteration : 6040
train acc:  0.734375
train loss:  0.5037872791290283
train gradient:  0.13253026281320288
iteration : 6041
train acc:  0.671875
train loss:  0.5744562149047852
train gradient:  0.16718398096774506
iteration : 6042
train acc:  0.671875
train loss:  0.5587620735168457
train gradient:  0.1867764904458069
iteration : 6043
train acc:  0.625
train loss:  0.5802984237670898
train gradient:  0.18103187719042152
iteration : 6044
train acc:  0.6796875
train loss:  0.5717172026634216
train gradient:  0.18630273310496137
iteration : 6045
train acc:  0.71875
train loss:  0.5089647769927979
train gradient:  0.13072050996302098
iteration : 6046
train acc:  0.7421875
train loss:  0.5221668481826782
train gradient:  0.1734319401517449
iteration : 6047
train acc:  0.703125
train loss:  0.5328675508499146
train gradient:  0.14717708747762842
iteration : 6048
train acc:  0.671875
train loss:  0.5467106699943542
train gradient:  0.16698287347244506
iteration : 6049
train acc:  0.75
train loss:  0.4981377124786377
train gradient:  0.14488919260032956
iteration : 6050
train acc:  0.6953125
train loss:  0.5423321723937988
train gradient:  0.14717439555766149
iteration : 6051
train acc:  0.7421875
train loss:  0.5024036169052124
train gradient:  0.13932839389464383
iteration : 6052
train acc:  0.7109375
train loss:  0.4754932224750519
train gradient:  0.12385828195854089
iteration : 6053
train acc:  0.671875
train loss:  0.6389219760894775
train gradient:  0.2377580992221553
iteration : 6054
train acc:  0.7578125
train loss:  0.46514466404914856
train gradient:  0.10550609654938223
iteration : 6055
train acc:  0.734375
train loss:  0.5089336633682251
train gradient:  0.1618263562319263
iteration : 6056
train acc:  0.7109375
train loss:  0.5099749565124512
train gradient:  0.12361034527164778
iteration : 6057
train acc:  0.703125
train loss:  0.5138508081436157
train gradient:  0.13760649669951885
iteration : 6058
train acc:  0.8046875
train loss:  0.4384026527404785
train gradient:  0.10130479502995329
iteration : 6059
train acc:  0.6953125
train loss:  0.5286664962768555
train gradient:  0.13590018680415095
iteration : 6060
train acc:  0.7265625
train loss:  0.524032711982727
train gradient:  0.13545506852621486
iteration : 6061
train acc:  0.7421875
train loss:  0.5015854239463806
train gradient:  0.13206704319200815
iteration : 6062
train acc:  0.6875
train loss:  0.53336501121521
train gradient:  0.1579286661850956
iteration : 6063
train acc:  0.6953125
train loss:  0.5094484090805054
train gradient:  0.10622753952809372
iteration : 6064
train acc:  0.78125
train loss:  0.440156489610672
train gradient:  0.12486463668734642
iteration : 6065
train acc:  0.7265625
train loss:  0.5028306245803833
train gradient:  0.15233901551062606
iteration : 6066
train acc:  0.765625
train loss:  0.47182679176330566
train gradient:  0.15819891133546143
iteration : 6067
train acc:  0.7421875
train loss:  0.5408195853233337
train gradient:  0.1362965445085682
iteration : 6068
train acc:  0.734375
train loss:  0.48625391721725464
train gradient:  0.15157943586607509
iteration : 6069
train acc:  0.671875
train loss:  0.5389863848686218
train gradient:  0.12595566797980873
iteration : 6070
train acc:  0.671875
train loss:  0.5404562950134277
train gradient:  0.1283888556019488
iteration : 6071
train acc:  0.7890625
train loss:  0.5024402141571045
train gradient:  0.1544152374994054
iteration : 6072
train acc:  0.7421875
train loss:  0.5169110298156738
train gradient:  0.15353924251115192
iteration : 6073
train acc:  0.7265625
train loss:  0.5496661067008972
train gradient:  0.1355489109698258
iteration : 6074
train acc:  0.75
train loss:  0.48993802070617676
train gradient:  0.13895283493761076
iteration : 6075
train acc:  0.71875
train loss:  0.500529408454895
train gradient:  0.13174806357192803
iteration : 6076
train acc:  0.78125
train loss:  0.4382063150405884
train gradient:  0.09504745488704801
iteration : 6077
train acc:  0.71875
train loss:  0.4791584610939026
train gradient:  0.10909079847131449
iteration : 6078
train acc:  0.7421875
train loss:  0.5123045444488525
train gradient:  0.12428384444188544
iteration : 6079
train acc:  0.6953125
train loss:  0.5198397636413574
train gradient:  0.12815981436699134
iteration : 6080
train acc:  0.734375
train loss:  0.5216423869132996
train gradient:  0.12756189206059465
iteration : 6081
train acc:  0.71875
train loss:  0.46811342239379883
train gradient:  0.11221671351076085
iteration : 6082
train acc:  0.8125
train loss:  0.48929089307785034
train gradient:  0.14866320770069938
iteration : 6083
train acc:  0.8125
train loss:  0.4281960427761078
train gradient:  0.12836464521264968
iteration : 6084
train acc:  0.7890625
train loss:  0.46443745493888855
train gradient:  0.10807262531178768
iteration : 6085
train acc:  0.78125
train loss:  0.4816274046897888
train gradient:  0.1105191076166124
iteration : 6086
train acc:  0.7734375
train loss:  0.48718729615211487
train gradient:  0.13375481372233972
iteration : 6087
train acc:  0.75
train loss:  0.5181127786636353
train gradient:  0.17354529354738218
iteration : 6088
train acc:  0.7734375
train loss:  0.47440284490585327
train gradient:  0.17314000474925334
iteration : 6089
train acc:  0.7421875
train loss:  0.5132750868797302
train gradient:  0.14875476151241754
iteration : 6090
train acc:  0.7578125
train loss:  0.50347900390625
train gradient:  0.12511046020995814
iteration : 6091
train acc:  0.7734375
train loss:  0.5127844214439392
train gradient:  0.2487162876376966
iteration : 6092
train acc:  0.6796875
train loss:  0.6004712581634521
train gradient:  0.2592566505280576
iteration : 6093
train acc:  0.796875
train loss:  0.4362163543701172
train gradient:  0.11565331422312854
iteration : 6094
train acc:  0.6875
train loss:  0.54017174243927
train gradient:  0.1455819501918325
iteration : 6095
train acc:  0.7109375
train loss:  0.5323722958564758
train gradient:  0.16211009627626158
iteration : 6096
train acc:  0.6953125
train loss:  0.5635089874267578
train gradient:  0.15822263424277097
iteration : 6097
train acc:  0.734375
train loss:  0.48507484793663025
train gradient:  0.1422774051757632
iteration : 6098
train acc:  0.75
train loss:  0.47556909918785095
train gradient:  0.14327539189508953
iteration : 6099
train acc:  0.6640625
train loss:  0.5594769716262817
train gradient:  0.14888732162278775
iteration : 6100
train acc:  0.671875
train loss:  0.5866395831108093
train gradient:  0.17656732390857652
iteration : 6101
train acc:  0.75
train loss:  0.49762189388275146
train gradient:  0.15582059659804054
iteration : 6102
train acc:  0.734375
train loss:  0.49242493510246277
train gradient:  0.1358304560855446
iteration : 6103
train acc:  0.6796875
train loss:  0.5525968074798584
train gradient:  0.15207139082473942
iteration : 6104
train acc:  0.7578125
train loss:  0.48983442783355713
train gradient:  0.11380325816416996
iteration : 6105
train acc:  0.8046875
train loss:  0.44458848237991333
train gradient:  0.0910890558768792
iteration : 6106
train acc:  0.7421875
train loss:  0.47670337557792664
train gradient:  0.13516526667908846
iteration : 6107
train acc:  0.6953125
train loss:  0.5231997966766357
train gradient:  0.15212141274887342
iteration : 6108
train acc:  0.78125
train loss:  0.42932987213134766
train gradient:  0.0861796434616969
iteration : 6109
train acc:  0.734375
train loss:  0.4505879878997803
train gradient:  0.09330536715614793
iteration : 6110
train acc:  0.6796875
train loss:  0.5199236273765564
train gradient:  0.13944194351403183
iteration : 6111
train acc:  0.75
train loss:  0.46119552850723267
train gradient:  0.10362476876521756
iteration : 6112
train acc:  0.71875
train loss:  0.5214473009109497
train gradient:  0.13045505078171046
iteration : 6113
train acc:  0.7578125
train loss:  0.5716597437858582
train gradient:  0.15047837211923804
iteration : 6114
train acc:  0.703125
train loss:  0.5811828374862671
train gradient:  0.16312507450698793
iteration : 6115
train acc:  0.7265625
train loss:  0.5078397989273071
train gradient:  0.11641822307693014
iteration : 6116
train acc:  0.6953125
train loss:  0.6263649463653564
train gradient:  0.29441844090929603
iteration : 6117
train acc:  0.671875
train loss:  0.5581498742103577
train gradient:  0.15590832557472578
iteration : 6118
train acc:  0.75
train loss:  0.4776744842529297
train gradient:  0.15202007399820397
iteration : 6119
train acc:  0.7734375
train loss:  0.4888153672218323
train gradient:  0.14918662931713916
iteration : 6120
train acc:  0.7734375
train loss:  0.44079330563545227
train gradient:  0.09754936459493896
iteration : 6121
train acc:  0.734375
train loss:  0.5162301063537598
train gradient:  0.13680723669341038
iteration : 6122
train acc:  0.8125
train loss:  0.459500253200531
train gradient:  0.10639044426405989
iteration : 6123
train acc:  0.703125
train loss:  0.5207786560058594
train gradient:  0.13364295247199307
iteration : 6124
train acc:  0.6953125
train loss:  0.5120989084243774
train gradient:  0.221256661676604
iteration : 6125
train acc:  0.78125
train loss:  0.4324907064437866
train gradient:  0.11756716622048036
iteration : 6126
train acc:  0.71875
train loss:  0.4749568700790405
train gradient:  0.12250979993002581
iteration : 6127
train acc:  0.671875
train loss:  0.560699999332428
train gradient:  0.16080329152060308
iteration : 6128
train acc:  0.671875
train loss:  0.5717642307281494
train gradient:  0.16393072075286547
iteration : 6129
train acc:  0.734375
train loss:  0.5256526470184326
train gradient:  0.14855778147521734
iteration : 6130
train acc:  0.7421875
train loss:  0.5217868089675903
train gradient:  0.16032861348678829
iteration : 6131
train acc:  0.7578125
train loss:  0.48075366020202637
train gradient:  0.14830989654439833
iteration : 6132
train acc:  0.7265625
train loss:  0.48476070165634155
train gradient:  0.12371181912627734
iteration : 6133
train acc:  0.7421875
train loss:  0.5145032405853271
train gradient:  0.15420076870505012
iteration : 6134
train acc:  0.7109375
train loss:  0.5118741989135742
train gradient:  0.13910919711006253
iteration : 6135
train acc:  0.7578125
train loss:  0.5374096035957336
train gradient:  0.17300352021957444
iteration : 6136
train acc:  0.734375
train loss:  0.5023772716522217
train gradient:  0.17414753206494232
iteration : 6137
train acc:  0.71875
train loss:  0.5195020437240601
train gradient:  0.1342435408222286
iteration : 6138
train acc:  0.7265625
train loss:  0.5084157586097717
train gradient:  0.12683336379735585
iteration : 6139
train acc:  0.7734375
train loss:  0.48748305439949036
train gradient:  0.11376014701207204
iteration : 6140
train acc:  0.6640625
train loss:  0.5602647066116333
train gradient:  0.18632787656808075
iteration : 6141
train acc:  0.6953125
train loss:  0.5468449592590332
train gradient:  0.2276721538178367
iteration : 6142
train acc:  0.6953125
train loss:  0.5325964689254761
train gradient:  0.15214101621786086
iteration : 6143
train acc:  0.734375
train loss:  0.5275296568870544
train gradient:  0.14994750406920465
iteration : 6144
train acc:  0.671875
train loss:  0.5914543867111206
train gradient:  0.16107750178907862
iteration : 6145
train acc:  0.7578125
train loss:  0.4886818528175354
train gradient:  0.1381292889512733
iteration : 6146
train acc:  0.7109375
train loss:  0.5079578161239624
train gradient:  0.11070078813827891
iteration : 6147
train acc:  0.7421875
train loss:  0.5051877498626709
train gradient:  0.13336544427009728
iteration : 6148
train acc:  0.75
train loss:  0.46780312061309814
train gradient:  0.12025230126477808
iteration : 6149
train acc:  0.7265625
train loss:  0.491659939289093
train gradient:  0.11333067090139233
iteration : 6150
train acc:  0.75
train loss:  0.4765426814556122
train gradient:  0.12198402813457332
iteration : 6151
train acc:  0.7421875
train loss:  0.5430833101272583
train gradient:  0.14559237447285006
iteration : 6152
train acc:  0.734375
train loss:  0.49986013770103455
train gradient:  0.18270314155696854
iteration : 6153
train acc:  0.765625
train loss:  0.5039083957672119
train gradient:  0.19997490917734126
iteration : 6154
train acc:  0.8359375
train loss:  0.43705636262893677
train gradient:  0.10845357026000384
iteration : 6155
train acc:  0.7421875
train loss:  0.48839467763900757
train gradient:  0.13697803947088033
iteration : 6156
train acc:  0.75
train loss:  0.49764710664749146
train gradient:  0.15207830488416618
iteration : 6157
train acc:  0.6640625
train loss:  0.5437749624252319
train gradient:  0.1736601126950289
iteration : 6158
train acc:  0.7265625
train loss:  0.5089414715766907
train gradient:  0.16007477993230035
iteration : 6159
train acc:  0.796875
train loss:  0.4378524422645569
train gradient:  0.11677256680438793
iteration : 6160
train acc:  0.7578125
train loss:  0.49195483326911926
train gradient:  0.1334881853157
iteration : 6161
train acc:  0.7578125
train loss:  0.48891615867614746
train gradient:  0.18066763729921131
iteration : 6162
train acc:  0.75
train loss:  0.45219624042510986
train gradient:  0.14102343342272258
iteration : 6163
train acc:  0.6796875
train loss:  0.5781492590904236
train gradient:  0.17132697132619884
iteration : 6164
train acc:  0.75
train loss:  0.4816296696662903
train gradient:  0.16272767507613803
iteration : 6165
train acc:  0.75
train loss:  0.5006587505340576
train gradient:  0.13335430677756516
iteration : 6166
train acc:  0.7109375
train loss:  0.5927077531814575
train gradient:  0.2208602229199113
iteration : 6167
train acc:  0.7890625
train loss:  0.4506293535232544
train gradient:  0.10738753360722096
iteration : 6168
train acc:  0.765625
train loss:  0.5050245523452759
train gradient:  0.14371943880256088
iteration : 6169
train acc:  0.734375
train loss:  0.5004705190658569
train gradient:  0.1272669747046201
iteration : 6170
train acc:  0.7890625
train loss:  0.4404780864715576
train gradient:  0.1089081543662729
iteration : 6171
train acc:  0.71875
train loss:  0.5448181629180908
train gradient:  0.12852609121876424
iteration : 6172
train acc:  0.8125
train loss:  0.43509238958358765
train gradient:  0.11578199900835738
iteration : 6173
train acc:  0.7421875
train loss:  0.5305612683296204
train gradient:  0.1610353231967111
iteration : 6174
train acc:  0.703125
train loss:  0.559865415096283
train gradient:  0.15131646503640978
iteration : 6175
train acc:  0.671875
train loss:  0.5823078155517578
train gradient:  0.17115782090024256
iteration : 6176
train acc:  0.6875
train loss:  0.5665478706359863
train gradient:  0.17952480504927373
iteration : 6177
train acc:  0.7421875
train loss:  0.5041078329086304
train gradient:  0.13156551956313398
iteration : 6178
train acc:  0.6953125
train loss:  0.5050745010375977
train gradient:  0.172839753959941
iteration : 6179
train acc:  0.7578125
train loss:  0.42832690477371216
train gradient:  0.10722429941256899
iteration : 6180
train acc:  0.6953125
train loss:  0.539519190788269
train gradient:  0.134492855623361
iteration : 6181
train acc:  0.703125
train loss:  0.4792247712612152
train gradient:  0.14559706142350587
iteration : 6182
train acc:  0.6953125
train loss:  0.5424398183822632
train gradient:  0.1562586363386076
iteration : 6183
train acc:  0.75
train loss:  0.49546438455581665
train gradient:  0.14955567099584938
iteration : 6184
train acc:  0.6875
train loss:  0.5535980463027954
train gradient:  0.23447777752535087
iteration : 6185
train acc:  0.7109375
train loss:  0.5092753767967224
train gradient:  0.15027576900088124
iteration : 6186
train acc:  0.671875
train loss:  0.5397018194198608
train gradient:  0.2519759339600672
iteration : 6187
train acc:  0.7734375
train loss:  0.4893447756767273
train gradient:  0.13329276933659065
iteration : 6188
train acc:  0.71875
train loss:  0.48322510719299316
train gradient:  0.1251301261186758
iteration : 6189
train acc:  0.765625
train loss:  0.446930468082428
train gradient:  0.12278187068919967
iteration : 6190
train acc:  0.703125
train loss:  0.5308652520179749
train gradient:  0.1418453255978021
iteration : 6191
train acc:  0.640625
train loss:  0.5732253789901733
train gradient:  0.183527990525317
iteration : 6192
train acc:  0.7578125
train loss:  0.487113356590271
train gradient:  0.12578246881733185
iteration : 6193
train acc:  0.75
train loss:  0.505375862121582
train gradient:  0.17960661925964252
iteration : 6194
train acc:  0.703125
train loss:  0.5179464817047119
train gradient:  0.14288679444723967
iteration : 6195
train acc:  0.734375
train loss:  0.5407499074935913
train gradient:  0.18076398157559437
iteration : 6196
train acc:  0.7578125
train loss:  0.47887879610061646
train gradient:  0.12492049237482893
iteration : 6197
train acc:  0.765625
train loss:  0.4848406910896301
train gradient:  0.11966095818399507
iteration : 6198
train acc:  0.734375
train loss:  0.4826449751853943
train gradient:  0.11887045972159575
iteration : 6199
train acc:  0.6953125
train loss:  0.5712523460388184
train gradient:  0.16134149270979053
iteration : 6200
train acc:  0.7421875
train loss:  0.5646806955337524
train gradient:  0.15403713910646605
iteration : 6201
train acc:  0.7890625
train loss:  0.45718300342559814
train gradient:  0.1347213117236571
iteration : 6202
train acc:  0.78125
train loss:  0.47083574533462524
train gradient:  0.14794785872672217
iteration : 6203
train acc:  0.640625
train loss:  0.5861502885818481
train gradient:  0.1829950630985565
iteration : 6204
train acc:  0.7578125
train loss:  0.527086615562439
train gradient:  0.14551508994487478
iteration : 6205
train acc:  0.7578125
train loss:  0.5131111741065979
train gradient:  0.1394360285921306
iteration : 6206
train acc:  0.6875
train loss:  0.554751992225647
train gradient:  0.1649551893382739
iteration : 6207
train acc:  0.7421875
train loss:  0.5527662038803101
train gradient:  0.1707629423087934
iteration : 6208
train acc:  0.7109375
train loss:  0.5648186802864075
train gradient:  0.1652939162410514
iteration : 6209
train acc:  0.78125
train loss:  0.462182879447937
train gradient:  0.10519041014771983
iteration : 6210
train acc:  0.75
train loss:  0.5080927610397339
train gradient:  0.15222809835597334
iteration : 6211
train acc:  0.671875
train loss:  0.5443800687789917
train gradient:  0.12828211606700823
iteration : 6212
train acc:  0.71875
train loss:  0.5146054029464722
train gradient:  0.11989772711143272
iteration : 6213
train acc:  0.7265625
train loss:  0.5218164324760437
train gradient:  0.18032193257926582
iteration : 6214
train acc:  0.703125
train loss:  0.5438332557678223
train gradient:  0.18401393998906218
iteration : 6215
train acc:  0.7109375
train loss:  0.5771365165710449
train gradient:  0.16698838790108433
iteration : 6216
train acc:  0.734375
train loss:  0.5347109436988831
train gradient:  0.16019128656808668
iteration : 6217
train acc:  0.65625
train loss:  0.608504056930542
train gradient:  0.17851047913538198
iteration : 6218
train acc:  0.75
train loss:  0.4730082154273987
train gradient:  0.11123471364368302
iteration : 6219
train acc:  0.7265625
train loss:  0.5029944777488708
train gradient:  0.12164614812114745
iteration : 6220
train acc:  0.7421875
train loss:  0.5079279541969299
train gradient:  0.14787053766431557
iteration : 6221
train acc:  0.71875
train loss:  0.5215563774108887
train gradient:  0.12611738420184804
iteration : 6222
train acc:  0.6875
train loss:  0.5523789525032043
train gradient:  0.22676695971125346
iteration : 6223
train acc:  0.6953125
train loss:  0.5561776757240295
train gradient:  0.1449788615961708
iteration : 6224
train acc:  0.6796875
train loss:  0.569222629070282
train gradient:  0.17286552912822067
iteration : 6225
train acc:  0.7421875
train loss:  0.48612263798713684
train gradient:  0.13954317074999062
iteration : 6226
train acc:  0.703125
train loss:  0.5254512429237366
train gradient:  0.14344920434851854
iteration : 6227
train acc:  0.6875
train loss:  0.48719337582588196
train gradient:  0.11987835368077365
iteration : 6228
train acc:  0.7421875
train loss:  0.5297995805740356
train gradient:  0.17233986644389349
iteration : 6229
train acc:  0.75
train loss:  0.4731423258781433
train gradient:  0.1268666617659944
iteration : 6230
train acc:  0.7578125
train loss:  0.5028337240219116
train gradient:  0.13014599524022477
iteration : 6231
train acc:  0.75
train loss:  0.49629610776901245
train gradient:  0.13739208103140987
iteration : 6232
train acc:  0.7734375
train loss:  0.41620007157325745
train gradient:  0.122466565684497
iteration : 6233
train acc:  0.7265625
train loss:  0.49603158235549927
train gradient:  0.14419859466733606
iteration : 6234
train acc:  0.7265625
train loss:  0.5138977766036987
train gradient:  0.17423451756503733
iteration : 6235
train acc:  0.7265625
train loss:  0.5471193790435791
train gradient:  0.17087833863550045
iteration : 6236
train acc:  0.7890625
train loss:  0.48218297958374023
train gradient:  0.12478879801974486
iteration : 6237
train acc:  0.734375
train loss:  0.4941721558570862
train gradient:  0.12508022841116267
iteration : 6238
train acc:  0.7265625
train loss:  0.5132887959480286
train gradient:  0.12670758882758495
iteration : 6239
train acc:  0.7109375
train loss:  0.48990750312805176
train gradient:  0.13528923544206656
iteration : 6240
train acc:  0.796875
train loss:  0.4635898470878601
train gradient:  0.15641914028321569
iteration : 6241
train acc:  0.703125
train loss:  0.5073026418685913
train gradient:  0.15909940965575847
iteration : 6242
train acc:  0.765625
train loss:  0.4656665325164795
train gradient:  0.11819527799621277
iteration : 6243
train acc:  0.6875
train loss:  0.5912551283836365
train gradient:  0.14696103348351824
iteration : 6244
train acc:  0.8046875
train loss:  0.4706140160560608
train gradient:  0.13185986073889536
iteration : 6245
train acc:  0.703125
train loss:  0.5357924103736877
train gradient:  0.16318349473380045
iteration : 6246
train acc:  0.6953125
train loss:  0.5515936613082886
train gradient:  0.1801492674155396
iteration : 6247
train acc:  0.6875
train loss:  0.5441175699234009
train gradient:  0.12639324490816523
iteration : 6248
train acc:  0.75
train loss:  0.4751160144805908
train gradient:  0.128862225632778
iteration : 6249
train acc:  0.6953125
train loss:  0.527805507183075
train gradient:  0.13934861056748993
iteration : 6250
train acc:  0.7421875
train loss:  0.49549150466918945
train gradient:  0.11893083033168411
iteration : 6251
train acc:  0.7265625
train loss:  0.4872737526893616
train gradient:  0.11631803891611761
iteration : 6252
train acc:  0.71875
train loss:  0.5503250360488892
train gradient:  0.1548171403552541
iteration : 6253
train acc:  0.75
train loss:  0.49326449632644653
train gradient:  0.11909866623425452
iteration : 6254
train acc:  0.765625
train loss:  0.4845258593559265
train gradient:  0.12867493438846672
iteration : 6255
train acc:  0.6875
train loss:  0.5157527923583984
train gradient:  0.151554240933433
iteration : 6256
train acc:  0.75
train loss:  0.4944022297859192
train gradient:  0.1451657140264538
iteration : 6257
train acc:  0.734375
train loss:  0.5038710832595825
train gradient:  0.14187006741769403
iteration : 6258
train acc:  0.609375
train loss:  0.589879035949707
train gradient:  0.18471644461135023
iteration : 6259
train acc:  0.7734375
train loss:  0.5048357248306274
train gradient:  0.12452255010142889
iteration : 6260
train acc:  0.7265625
train loss:  0.5671955943107605
train gradient:  0.19164051935765086
iteration : 6261
train acc:  0.7890625
train loss:  0.4482550024986267
train gradient:  0.13286331457706577
iteration : 6262
train acc:  0.71875
train loss:  0.5780802965164185
train gradient:  0.1424902294969641
iteration : 6263
train acc:  0.7734375
train loss:  0.4855388104915619
train gradient:  0.14233636687519974
iteration : 6264
train acc:  0.734375
train loss:  0.5153701305389404
train gradient:  0.12480572868951362
iteration : 6265
train acc:  0.78125
train loss:  0.43231502175331116
train gradient:  0.09019609174893312
iteration : 6266
train acc:  0.7265625
train loss:  0.4861580729484558
train gradient:  0.13676259336158003
iteration : 6267
train acc:  0.75
train loss:  0.4948761463165283
train gradient:  0.12542207045149095
iteration : 6268
train acc:  0.6875
train loss:  0.5635004639625549
train gradient:  0.12850704642697275
iteration : 6269
train acc:  0.671875
train loss:  0.5198145508766174
train gradient:  0.11489741940264583
iteration : 6270
train acc:  0.7890625
train loss:  0.455147385597229
train gradient:  0.1718229213831015
iteration : 6271
train acc:  0.8046875
train loss:  0.462563693523407
train gradient:  0.12384370761746594
iteration : 6272
train acc:  0.7578125
train loss:  0.5145100355148315
train gradient:  0.1240467756750935
iteration : 6273
train acc:  0.75
train loss:  0.49024471640586853
train gradient:  0.11970657755513701
iteration : 6274
train acc:  0.734375
train loss:  0.5264467000961304
train gradient:  0.1248902944576656
iteration : 6275
train acc:  0.7578125
train loss:  0.4672156274318695
train gradient:  0.1731381623721615
iteration : 6276
train acc:  0.6796875
train loss:  0.5136867761611938
train gradient:  0.15233510565525396
iteration : 6277
train acc:  0.7890625
train loss:  0.4791348874568939
train gradient:  0.1164306549137117
iteration : 6278
train acc:  0.71875
train loss:  0.5430095195770264
train gradient:  0.16215869057724852
iteration : 6279
train acc:  0.703125
train loss:  0.5464742183685303
train gradient:  0.1863625964536241
iteration : 6280
train acc:  0.6796875
train loss:  0.5773457884788513
train gradient:  0.20049367694252823
iteration : 6281
train acc:  0.671875
train loss:  0.5353732109069824
train gradient:  0.164678591539208
iteration : 6282
train acc:  0.7734375
train loss:  0.4509614109992981
train gradient:  0.1022931327996416
iteration : 6283
train acc:  0.7265625
train loss:  0.5419012308120728
train gradient:  0.16373181052711086
iteration : 6284
train acc:  0.78125
train loss:  0.47178328037261963
train gradient:  0.0929830478648989
iteration : 6285
train acc:  0.7578125
train loss:  0.5108847618103027
train gradient:  0.12873141890200374
iteration : 6286
train acc:  0.7578125
train loss:  0.4911655783653259
train gradient:  0.16759672372355233
iteration : 6287
train acc:  0.7109375
train loss:  0.5437038540840149
train gradient:  0.17786680501666946
iteration : 6288
train acc:  0.765625
train loss:  0.5129474997520447
train gradient:  0.13826632320872817
iteration : 6289
train acc:  0.65625
train loss:  0.5764337778091431
train gradient:  0.1799900726559461
iteration : 6290
train acc:  0.7109375
train loss:  0.5451968312263489
train gradient:  0.1549595062862133
iteration : 6291
train acc:  0.6796875
train loss:  0.5242429375648499
train gradient:  0.13906696375410144
iteration : 6292
train acc:  0.6484375
train loss:  0.6035071015357971
train gradient:  0.18163349148551303
iteration : 6293
train acc:  0.78125
train loss:  0.4972626864910126
train gradient:  0.11488109124921196
iteration : 6294
train acc:  0.765625
train loss:  0.45688608288764954
train gradient:  0.14544125348308007
iteration : 6295
train acc:  0.6171875
train loss:  0.5849714279174805
train gradient:  0.14317817717272338
iteration : 6296
train acc:  0.7421875
train loss:  0.5088910460472107
train gradient:  0.14610083303993532
iteration : 6297
train acc:  0.7734375
train loss:  0.48515772819519043
train gradient:  0.162913639456299
iteration : 6298
train acc:  0.6875
train loss:  0.48287180066108704
train gradient:  0.11162173100865344
iteration : 6299
train acc:  0.71875
train loss:  0.5279343128204346
train gradient:  0.17887222880745435
iteration : 6300
train acc:  0.71875
train loss:  0.5763166546821594
train gradient:  0.1722136424923285
iteration : 6301
train acc:  0.75
train loss:  0.5017112493515015
train gradient:  0.11954301037609852
iteration : 6302
train acc:  0.7421875
train loss:  0.4693925380706787
train gradient:  0.1183960010144022
iteration : 6303
train acc:  0.7265625
train loss:  0.5209730863571167
train gradient:  0.14782666443912035
iteration : 6304
train acc:  0.7734375
train loss:  0.47503796219825745
train gradient:  0.14647473318139115
iteration : 6305
train acc:  0.796875
train loss:  0.4629722833633423
train gradient:  0.11272758090027417
iteration : 6306
train acc:  0.75
train loss:  0.4852069318294525
train gradient:  0.10538007642820975
iteration : 6307
train acc:  0.734375
train loss:  0.558208703994751
train gradient:  0.18636583022538017
iteration : 6308
train acc:  0.71875
train loss:  0.5239622592926025
train gradient:  0.1446346910186435
iteration : 6309
train acc:  0.734375
train loss:  0.4893449544906616
train gradient:  0.161362617779168
iteration : 6310
train acc:  0.7265625
train loss:  0.5232666730880737
train gradient:  0.1251355212883758
iteration : 6311
train acc:  0.734375
train loss:  0.5143156051635742
train gradient:  0.18770400414256122
iteration : 6312
train acc:  0.734375
train loss:  0.5219064354896545
train gradient:  0.15049522506708451
iteration : 6313
train acc:  0.734375
train loss:  0.4855366349220276
train gradient:  0.15429547535340443
iteration : 6314
train acc:  0.703125
train loss:  0.49345874786376953
train gradient:  0.14605550973645465
iteration : 6315
train acc:  0.6953125
train loss:  0.560181200504303
train gradient:  0.1782845428388763
iteration : 6316
train acc:  0.7265625
train loss:  0.49175387620925903
train gradient:  0.1745678205358045
iteration : 6317
train acc:  0.75
train loss:  0.5067198276519775
train gradient:  0.16216354052519377
iteration : 6318
train acc:  0.7421875
train loss:  0.4946395754814148
train gradient:  0.1268293311437547
iteration : 6319
train acc:  0.703125
train loss:  0.5293483734130859
train gradient:  0.1361709448373442
iteration : 6320
train acc:  0.734375
train loss:  0.5661226511001587
train gradient:  0.14348231948810392
iteration : 6321
train acc:  0.7265625
train loss:  0.5101020932197571
train gradient:  0.12369234080988832
iteration : 6322
train acc:  0.7265625
train loss:  0.5208996534347534
train gradient:  0.17775456409753299
iteration : 6323
train acc:  0.734375
train loss:  0.5076366662979126
train gradient:  0.1304284673216194
iteration : 6324
train acc:  0.6953125
train loss:  0.5443891286849976
train gradient:  0.16058179915629955
iteration : 6325
train acc:  0.71875
train loss:  0.4998425245285034
train gradient:  0.12576687819652183
iteration : 6326
train acc:  0.7421875
train loss:  0.4953494071960449
train gradient:  0.1385679552993968
iteration : 6327
train acc:  0.75
train loss:  0.505940318107605
train gradient:  0.14325315128985605
iteration : 6328
train acc:  0.7734375
train loss:  0.49723124504089355
train gradient:  0.12422324148582073
iteration : 6329
train acc:  0.7421875
train loss:  0.5268129110336304
train gradient:  0.14171554175147316
iteration : 6330
train acc:  0.671875
train loss:  0.5535048246383667
train gradient:  0.20879788753242967
iteration : 6331
train acc:  0.734375
train loss:  0.5280594825744629
train gradient:  0.1263745689421123
iteration : 6332
train acc:  0.7421875
train loss:  0.5195598602294922
train gradient:  0.11415432357791933
iteration : 6333
train acc:  0.78125
train loss:  0.4652140438556671
train gradient:  0.15071249149604377
iteration : 6334
train acc:  0.75
train loss:  0.5660394430160522
train gradient:  0.175423352878389
iteration : 6335
train acc:  0.75
train loss:  0.491762638092041
train gradient:  0.11052565685923718
iteration : 6336
train acc:  0.6875
train loss:  0.5348905920982361
train gradient:  0.1547732142109286
iteration : 6337
train acc:  0.7421875
train loss:  0.5018935203552246
train gradient:  0.1274696807964596
iteration : 6338
train acc:  0.78125
train loss:  0.4536387026309967
train gradient:  0.10339848202985641
iteration : 6339
train acc:  0.84375
train loss:  0.450916051864624
train gradient:  0.12785180987826267
iteration : 6340
train acc:  0.703125
train loss:  0.5325474143028259
train gradient:  0.1249061032815862
iteration : 6341
train acc:  0.796875
train loss:  0.47196751832962036
train gradient:  0.13248194809804362
iteration : 6342
train acc:  0.796875
train loss:  0.4246327579021454
train gradient:  0.08262624579457926
iteration : 6343
train acc:  0.6953125
train loss:  0.5298492312431335
train gradient:  0.1763129983352043
iteration : 6344
train acc:  0.734375
train loss:  0.5225012898445129
train gradient:  0.12132799117923497
iteration : 6345
train acc:  0.7890625
train loss:  0.4982300400733948
train gradient:  0.10808211845414654
iteration : 6346
train acc:  0.8125
train loss:  0.4513216018676758
train gradient:  0.11145227661580076
iteration : 6347
train acc:  0.78125
train loss:  0.46566715836524963
train gradient:  0.13134397976908435
iteration : 6348
train acc:  0.7578125
train loss:  0.44775301218032837
train gradient:  0.1299145066527782
iteration : 6349
train acc:  0.75
train loss:  0.5049489736557007
train gradient:  0.12263693390227473
iteration : 6350
train acc:  0.671875
train loss:  0.5157216191291809
train gradient:  0.11903086372033485
iteration : 6351
train acc:  0.734375
train loss:  0.5243486166000366
train gradient:  0.12171930341093443
iteration : 6352
train acc:  0.71875
train loss:  0.501042366027832
train gradient:  0.13094386783522022
iteration : 6353
train acc:  0.734375
train loss:  0.4976094365119934
train gradient:  0.14532747174763777
iteration : 6354
train acc:  0.7734375
train loss:  0.497671902179718
train gradient:  0.11890430270608493
iteration : 6355
train acc:  0.6640625
train loss:  0.5758101344108582
train gradient:  0.16820368079220943
iteration : 6356
train acc:  0.765625
train loss:  0.5051741003990173
train gradient:  0.11344280589682024
iteration : 6357
train acc:  0.734375
train loss:  0.5931118130683899
train gradient:  0.18039868095527073
iteration : 6358
train acc:  0.765625
train loss:  0.518343448638916
train gradient:  0.13097858326729836
iteration : 6359
train acc:  0.7890625
train loss:  0.45892205834388733
train gradient:  0.10648639980435938
iteration : 6360
train acc:  0.7734375
train loss:  0.4697580337524414
train gradient:  0.1446408739545691
iteration : 6361
train acc:  0.6953125
train loss:  0.5096091628074646
train gradient:  0.18000059719011857
iteration : 6362
train acc:  0.6953125
train loss:  0.5285458564758301
train gradient:  0.1592718285899964
iteration : 6363
train acc:  0.7578125
train loss:  0.5582799315452576
train gradient:  0.17249678420570594
iteration : 6364
train acc:  0.75
train loss:  0.5012224912643433
train gradient:  0.15005136738487487
iteration : 6365
train acc:  0.71875
train loss:  0.48153966665267944
train gradient:  0.1489882425540466
iteration : 6366
train acc:  0.75
train loss:  0.4843599200248718
train gradient:  0.11966768655608583
iteration : 6367
train acc:  0.703125
train loss:  0.5342475175857544
train gradient:  0.14477184408345203
iteration : 6368
train acc:  0.7109375
train loss:  0.5222106575965881
train gradient:  0.13780098260892282
iteration : 6369
train acc:  0.6953125
train loss:  0.5523651242256165
train gradient:  0.13872408883984694
iteration : 6370
train acc:  0.703125
train loss:  0.5326231718063354
train gradient:  0.15841400731696498
iteration : 6371
train acc:  0.78125
train loss:  0.5077618360519409
train gradient:  0.15968862696553082
iteration : 6372
train acc:  0.7578125
train loss:  0.5012688040733337
train gradient:  0.1461247715046166
iteration : 6373
train acc:  0.75
train loss:  0.49485868215560913
train gradient:  0.14613106114913338
iteration : 6374
train acc:  0.7265625
train loss:  0.5636048316955566
train gradient:  0.15283916061021846
iteration : 6375
train acc:  0.6953125
train loss:  0.5596301555633545
train gradient:  0.14758969819226483
iteration : 6376
train acc:  0.6953125
train loss:  0.5493868589401245
train gradient:  0.14493882148942638
iteration : 6377
train acc:  0.71875
train loss:  0.4909900426864624
train gradient:  0.14369271896418123
iteration : 6378
train acc:  0.765625
train loss:  0.5201776027679443
train gradient:  0.17456490384261236
iteration : 6379
train acc:  0.7578125
train loss:  0.45571884512901306
train gradient:  0.11415629529409717
iteration : 6380
train acc:  0.765625
train loss:  0.49002605676651
train gradient:  0.14810697751393428
iteration : 6381
train acc:  0.71875
train loss:  0.5766923427581787
train gradient:  0.16075137398894196
iteration : 6382
train acc:  0.7421875
train loss:  0.4814051687717438
train gradient:  0.1740368696636016
iteration : 6383
train acc:  0.734375
train loss:  0.49588289856910706
train gradient:  0.13117801480474772
iteration : 6384
train acc:  0.8125
train loss:  0.3820744752883911
train gradient:  0.10839793258983096
iteration : 6385
train acc:  0.78125
train loss:  0.44557368755340576
train gradient:  0.10344762329101426
iteration : 6386
train acc:  0.703125
train loss:  0.517037034034729
train gradient:  0.12710636365069655
iteration : 6387
train acc:  0.6640625
train loss:  0.5579801797866821
train gradient:  0.13294461937609015
iteration : 6388
train acc:  0.703125
train loss:  0.540273904800415
train gradient:  0.14783596883983768
iteration : 6389
train acc:  0.6640625
train loss:  0.6052884459495544
train gradient:  0.1836428231810423
iteration : 6390
train acc:  0.765625
train loss:  0.49260225892066956
train gradient:  0.11855040067600235
iteration : 6391
train acc:  0.6875
train loss:  0.6026524901390076
train gradient:  0.2007872181529642
iteration : 6392
train acc:  0.71875
train loss:  0.5311323404312134
train gradient:  0.18492919629853294
iteration : 6393
train acc:  0.75
train loss:  0.530290424823761
train gradient:  0.1408424823119353
iteration : 6394
train acc:  0.734375
train loss:  0.4689823091030121
train gradient:  0.09269481820048737
iteration : 6395
train acc:  0.6875
train loss:  0.49210500717163086
train gradient:  0.14517322989991072
iteration : 6396
train acc:  0.78125
train loss:  0.4878329634666443
train gradient:  0.13672487436916492
iteration : 6397
train acc:  0.65625
train loss:  0.5747863054275513
train gradient:  0.15907698576993554
iteration : 6398
train acc:  0.734375
train loss:  0.5066685080528259
train gradient:  0.11896990682875427
iteration : 6399
train acc:  0.6953125
train loss:  0.5526115894317627
train gradient:  0.24489703862713194
iteration : 6400
train acc:  0.703125
train loss:  0.5263772010803223
train gradient:  0.11353385575214239
iteration : 6401
train acc:  0.6953125
train loss:  0.5011105537414551
train gradient:  0.114616915614482
iteration : 6402
train acc:  0.6953125
train loss:  0.545709490776062
train gradient:  0.17595122068178892
iteration : 6403
train acc:  0.7890625
train loss:  0.45559534430503845
train gradient:  0.1434637382420197
iteration : 6404
train acc:  0.7578125
train loss:  0.45746979117393494
train gradient:  0.12486911274452475
iteration : 6405
train acc:  0.7421875
train loss:  0.5079694390296936
train gradient:  0.13941812262245695
iteration : 6406
train acc:  0.7265625
train loss:  0.5240836143493652
train gradient:  0.14532729025757124
iteration : 6407
train acc:  0.7578125
train loss:  0.47741055488586426
train gradient:  0.1264449420935184
iteration : 6408
train acc:  0.6953125
train loss:  0.5239530801773071
train gradient:  0.1591754093152586
iteration : 6409
train acc:  0.796875
train loss:  0.4668198823928833
train gradient:  0.09702777661498074
iteration : 6410
train acc:  0.7265625
train loss:  0.537503719329834
train gradient:  0.14887815660928597
iteration : 6411
train acc:  0.765625
train loss:  0.45406338572502136
train gradient:  0.10984622736490063
iteration : 6412
train acc:  0.6015625
train loss:  0.6078245043754578
train gradient:  0.22648711478976508
iteration : 6413
train acc:  0.7109375
train loss:  0.5624700784683228
train gradient:  0.17084135083785915
iteration : 6414
train acc:  0.75
train loss:  0.45428556203842163
train gradient:  0.11069873503683468
iteration : 6415
train acc:  0.7421875
train loss:  0.4944593608379364
train gradient:  0.13952112717755305
iteration : 6416
train acc:  0.703125
train loss:  0.5471324920654297
train gradient:  0.15572315401753933
iteration : 6417
train acc:  0.7421875
train loss:  0.5085882544517517
train gradient:  0.20139923839614912
iteration : 6418
train acc:  0.734375
train loss:  0.5493819713592529
train gradient:  0.1267715595728219
iteration : 6419
train acc:  0.703125
train loss:  0.49548009037971497
train gradient:  0.12215116102611895
iteration : 6420
train acc:  0.7265625
train loss:  0.5493290424346924
train gradient:  0.21594737940210582
iteration : 6421
train acc:  0.71875
train loss:  0.5240460634231567
train gradient:  0.16120799206711722
iteration : 6422
train acc:  0.7109375
train loss:  0.5482569932937622
train gradient:  0.16241062832957323
iteration : 6423
train acc:  0.734375
train loss:  0.5194655060768127
train gradient:  0.14401365460589297
iteration : 6424
train acc:  0.7578125
train loss:  0.49021756649017334
train gradient:  0.13612467226952255
iteration : 6425
train acc:  0.734375
train loss:  0.5194650888442993
train gradient:  0.13641233968796504
iteration : 6426
train acc:  0.703125
train loss:  0.5546789169311523
train gradient:  0.1667781915704835
iteration : 6427
train acc:  0.7265625
train loss:  0.5592195987701416
train gradient:  0.1490401941613091
iteration : 6428
train acc:  0.6875
train loss:  0.507457971572876
train gradient:  0.15624660944405258
iteration : 6429
train acc:  0.765625
train loss:  0.46806734800338745
train gradient:  0.17926703866596216
iteration : 6430
train acc:  0.7265625
train loss:  0.4782578647136688
train gradient:  0.12501726399795576
iteration : 6431
train acc:  0.71875
train loss:  0.528858482837677
train gradient:  0.16823955830626866
iteration : 6432
train acc:  0.7578125
train loss:  0.5037312507629395
train gradient:  0.13767368021416257
iteration : 6433
train acc:  0.6875
train loss:  0.5614187717437744
train gradient:  0.16083528401600009
iteration : 6434
train acc:  0.65625
train loss:  0.6191890239715576
train gradient:  0.16849006972145225
iteration : 6435
train acc:  0.75
train loss:  0.4943259358406067
train gradient:  0.13955576173335166
iteration : 6436
train acc:  0.71875
train loss:  0.5056543350219727
train gradient:  0.1340555980609843
iteration : 6437
train acc:  0.671875
train loss:  0.5484593510627747
train gradient:  0.16164710133074456
iteration : 6438
train acc:  0.734375
train loss:  0.5150449872016907
train gradient:  0.18761930533781632
iteration : 6439
train acc:  0.6796875
train loss:  0.5705699920654297
train gradient:  0.26561224482443213
iteration : 6440
train acc:  0.6953125
train loss:  0.49463242292404175
train gradient:  0.17511246493942995
iteration : 6441
train acc:  0.78125
train loss:  0.4476337134838104
train gradient:  0.10646978801160815
iteration : 6442
train acc:  0.7578125
train loss:  0.5101585388183594
train gradient:  0.13185492508266256
iteration : 6443
train acc:  0.71875
train loss:  0.5171347856521606
train gradient:  0.12407337120189688
iteration : 6444
train acc:  0.703125
train loss:  0.5319152474403381
train gradient:  0.12072691478528018
iteration : 6445
train acc:  0.75
train loss:  0.5547338128089905
train gradient:  0.12316975541255243
iteration : 6446
train acc:  0.6796875
train loss:  0.5376842021942139
train gradient:  0.12766176237838606
iteration : 6447
train acc:  0.7109375
train loss:  0.5043637156486511
train gradient:  0.11305232209266566
iteration : 6448
train acc:  0.6953125
train loss:  0.5394141674041748
train gradient:  0.15372301049879883
iteration : 6449
train acc:  0.7890625
train loss:  0.46248337626457214
train gradient:  0.1043536061258491
iteration : 6450
train acc:  0.6796875
train loss:  0.5110666155815125
train gradient:  0.1219495835698858
iteration : 6451
train acc:  0.734375
train loss:  0.5530248880386353
train gradient:  0.1523348403817406
iteration : 6452
train acc:  0.6953125
train loss:  0.5761882066726685
train gradient:  0.2547680125662896
iteration : 6453
train acc:  0.671875
train loss:  0.542819082736969
train gradient:  0.13772556955014473
iteration : 6454
train acc:  0.796875
train loss:  0.4848283529281616
train gradient:  0.10555934554951511
iteration : 6455
train acc:  0.8125
train loss:  0.4602254331111908
train gradient:  0.12178303850872145
iteration : 6456
train acc:  0.7265625
train loss:  0.6056905388832092
train gradient:  0.22587130572369374
iteration : 6457
train acc:  0.765625
train loss:  0.47169625759124756
train gradient:  0.12143026221069343
iteration : 6458
train acc:  0.6953125
train loss:  0.5398399829864502
train gradient:  0.15594163031404004
iteration : 6459
train acc:  0.7265625
train loss:  0.510686993598938
train gradient:  0.16101057037509514
iteration : 6460
train acc:  0.7265625
train loss:  0.5731407403945923
train gradient:  0.2688422465766707
iteration : 6461
train acc:  0.6953125
train loss:  0.5373231768608093
train gradient:  0.12852166929450293
iteration : 6462
train acc:  0.7109375
train loss:  0.5621777772903442
train gradient:  0.175010210560394
iteration : 6463
train acc:  0.7578125
train loss:  0.49205806851387024
train gradient:  0.16099771831921855
iteration : 6464
train acc:  0.78125
train loss:  0.46895962953567505
train gradient:  0.14724908321419067
iteration : 6465
train acc:  0.7421875
train loss:  0.498802125453949
train gradient:  0.1042056768338691
iteration : 6466
train acc:  0.78125
train loss:  0.47382378578186035
train gradient:  0.13409449721904032
iteration : 6467
train acc:  0.7578125
train loss:  0.456347793340683
train gradient:  0.11275028504212219
iteration : 6468
train acc:  0.765625
train loss:  0.46756917238235474
train gradient:  0.12761767227156712
iteration : 6469
train acc:  0.65625
train loss:  0.5351971983909607
train gradient:  0.1566274239830099
iteration : 6470
train acc:  0.7578125
train loss:  0.48310646414756775
train gradient:  0.13620124308730217
iteration : 6471
train acc:  0.7734375
train loss:  0.4927312135696411
train gradient:  0.12618382420735785
iteration : 6472
train acc:  0.765625
train loss:  0.4736558496952057
train gradient:  0.13232974594247832
iteration : 6473
train acc:  0.71875
train loss:  0.543725848197937
train gradient:  0.18754389993949017
iteration : 6474
train acc:  0.7265625
train loss:  0.488731324672699
train gradient:  0.10441394279144292
iteration : 6475
train acc:  0.7265625
train loss:  0.5138987898826599
train gradient:  0.13507891721336984
iteration : 6476
train acc:  0.7265625
train loss:  0.49698442220687866
train gradient:  0.1379019276490019
iteration : 6477
train acc:  0.7421875
train loss:  0.5152420401573181
train gradient:  0.1601858408707563
iteration : 6478
train acc:  0.7421875
train loss:  0.5188484787940979
train gradient:  0.12809158965931405
iteration : 6479
train acc:  0.6875
train loss:  0.546442985534668
train gradient:  0.16079599117786292
iteration : 6480
train acc:  0.7109375
train loss:  0.5189248919487
train gradient:  0.12048895951497342
iteration : 6481
train acc:  0.734375
train loss:  0.5188952684402466
train gradient:  0.15589555011954287
iteration : 6482
train acc:  0.7421875
train loss:  0.47232919931411743
train gradient:  0.11306274030879075
iteration : 6483
train acc:  0.7109375
train loss:  0.5441734790802002
train gradient:  0.16136931915255548
iteration : 6484
train acc:  0.6796875
train loss:  0.5449395775794983
train gradient:  0.18260381898903777
iteration : 6485
train acc:  0.671875
train loss:  0.5513066053390503
train gradient:  0.14246251784735833
iteration : 6486
train acc:  0.7265625
train loss:  0.5435957908630371
train gradient:  0.16834756912430004
iteration : 6487
train acc:  0.71875
train loss:  0.5305162072181702
train gradient:  0.1713862981621537
iteration : 6488
train acc:  0.734375
train loss:  0.5558874607086182
train gradient:  0.18289252940224088
iteration : 6489
train acc:  0.7265625
train loss:  0.5467353463172913
train gradient:  0.1400582498375943
iteration : 6490
train acc:  0.7265625
train loss:  0.5220919847488403
train gradient:  0.14171546666472762
iteration : 6491
train acc:  0.75
train loss:  0.5240166187286377
train gradient:  0.2100838909071226
iteration : 6492
train acc:  0.734375
train loss:  0.5179638862609863
train gradient:  0.11182064190854664
iteration : 6493
train acc:  0.71875
train loss:  0.535193920135498
train gradient:  0.10688556423977971
iteration : 6494
train acc:  0.75
train loss:  0.49573516845703125
train gradient:  0.1357649717592759
iteration : 6495
train acc:  0.6640625
train loss:  0.5477752685546875
train gradient:  0.15838772937346784
iteration : 6496
train acc:  0.75
train loss:  0.4723868668079376
train gradient:  0.13124718637490554
iteration : 6497
train acc:  0.7578125
train loss:  0.5207249522209167
train gradient:  0.13821835742920593
iteration : 6498
train acc:  0.71875
train loss:  0.5460186004638672
train gradient:  0.2317095887948386
iteration : 6499
train acc:  0.8359375
train loss:  0.41322463750839233
train gradient:  0.1318749007102549
iteration : 6500
train acc:  0.71875
train loss:  0.5528413653373718
train gradient:  0.16991696149035335
iteration : 6501
train acc:  0.78125
train loss:  0.4782492518424988
train gradient:  0.11632283674756633
iteration : 6502
train acc:  0.7578125
train loss:  0.4644792675971985
train gradient:  0.09777704141219903
iteration : 6503
train acc:  0.7109375
train loss:  0.5435631275177002
train gradient:  0.15762154517500496
iteration : 6504
train acc:  0.6953125
train loss:  0.5121234655380249
train gradient:  0.1284386897979275
iteration : 6505
train acc:  0.6875
train loss:  0.5568848252296448
train gradient:  0.1838762695296678
iteration : 6506
train acc:  0.765625
train loss:  0.4687381088733673
train gradient:  0.10716547334256506
iteration : 6507
train acc:  0.6953125
train loss:  0.4643261432647705
train gradient:  0.09354779644326545
iteration : 6508
train acc:  0.734375
train loss:  0.48733019828796387
train gradient:  0.11754196391549512
iteration : 6509
train acc:  0.75
train loss:  0.4808395504951477
train gradient:  0.13973791133313007
iteration : 6510
train acc:  0.671875
train loss:  0.5576082468032837
train gradient:  0.19433532232842138
iteration : 6511
train acc:  0.6328125
train loss:  0.621863842010498
train gradient:  0.19653644588969837
iteration : 6512
train acc:  0.7421875
train loss:  0.48000597953796387
train gradient:  0.1325039796762518
iteration : 6513
train acc:  0.6953125
train loss:  0.591363251209259
train gradient:  0.1650127495443705
iteration : 6514
train acc:  0.734375
train loss:  0.47204285860061646
train gradient:  0.11722799770170989
iteration : 6515
train acc:  0.71875
train loss:  0.5287662744522095
train gradient:  0.16232722822938625
iteration : 6516
train acc:  0.75
train loss:  0.5256965160369873
train gradient:  0.11515788477987798
iteration : 6517
train acc:  0.78125
train loss:  0.4577036201953888
train gradient:  0.09409115140058669
iteration : 6518
train acc:  0.875
train loss:  0.39474332332611084
train gradient:  0.09384639263237779
iteration : 6519
train acc:  0.7421875
train loss:  0.5292981266975403
train gradient:  0.15192517783563464
iteration : 6520
train acc:  0.78125
train loss:  0.5141730308532715
train gradient:  0.12559435567778227
iteration : 6521
train acc:  0.7578125
train loss:  0.52753746509552
train gradient:  0.13642066631125396
iteration : 6522
train acc:  0.7265625
train loss:  0.46565574407577515
train gradient:  0.1139082365139237
iteration : 6523
train acc:  0.640625
train loss:  0.5195987820625305
train gradient:  0.12093728344090332
iteration : 6524
train acc:  0.71875
train loss:  0.5084869265556335
train gradient:  0.14581146954651533
iteration : 6525
train acc:  0.78125
train loss:  0.43304702639579773
train gradient:  0.1338417842232293
iteration : 6526
train acc:  0.7109375
train loss:  0.5504050254821777
train gradient:  0.158456578374915
iteration : 6527
train acc:  0.7578125
train loss:  0.5157994031906128
train gradient:  0.1611391814617159
iteration : 6528
train acc:  0.71875
train loss:  0.5547833442687988
train gradient:  0.1612515824225617
iteration : 6529
train acc:  0.75
train loss:  0.4900107979774475
train gradient:  0.09825904515857448
iteration : 6530
train acc:  0.7578125
train loss:  0.5162502527236938
train gradient:  0.14904859884994026
iteration : 6531
train acc:  0.796875
train loss:  0.46148574352264404
train gradient:  0.13193987423384723
iteration : 6532
train acc:  0.75
train loss:  0.4912244975566864
train gradient:  0.1423894252906498
iteration : 6533
train acc:  0.734375
train loss:  0.453953355550766
train gradient:  0.09417894395425552
iteration : 6534
train acc:  0.7578125
train loss:  0.4905746579170227
train gradient:  0.1411021368627547
iteration : 6535
train acc:  0.703125
train loss:  0.5646249651908875
train gradient:  0.20232378359956157
iteration : 6536
train acc:  0.765625
train loss:  0.48318028450012207
train gradient:  0.1351351327157779
iteration : 6537
train acc:  0.65625
train loss:  0.5804771184921265
train gradient:  0.18952438806933192
iteration : 6538
train acc:  0.703125
train loss:  0.5577706098556519
train gradient:  0.15478026123000882
iteration : 6539
train acc:  0.78125
train loss:  0.4689732789993286
train gradient:  0.144145487335085
iteration : 6540
train acc:  0.71875
train loss:  0.541828989982605
train gradient:  0.12124594030938196
iteration : 6541
train acc:  0.7734375
train loss:  0.45128968358039856
train gradient:  0.09804746207373342
iteration : 6542
train acc:  0.71875
train loss:  0.5428089499473572
train gradient:  0.17064405955533168
iteration : 6543
train acc:  0.78125
train loss:  0.45066994428634644
train gradient:  0.11396268105813917
iteration : 6544
train acc:  0.765625
train loss:  0.4611743986606598
train gradient:  0.10898386035484252
iteration : 6545
train acc:  0.7890625
train loss:  0.4688866436481476
train gradient:  0.1264361689425949
iteration : 6546
train acc:  0.734375
train loss:  0.5361019968986511
train gradient:  0.15501555428372893
iteration : 6547
train acc:  0.7421875
train loss:  0.5107108354568481
train gradient:  0.14730416128998752
iteration : 6548
train acc:  0.75
train loss:  0.506914496421814
train gradient:  0.17987874513360588
iteration : 6549
train acc:  0.7421875
train loss:  0.49028217792510986
train gradient:  0.1279587391886673
iteration : 6550
train acc:  0.65625
train loss:  0.5470204949378967
train gradient:  0.18083706720906978
iteration : 6551
train acc:  0.75
train loss:  0.49062323570251465
train gradient:  0.2000454148175045
iteration : 6552
train acc:  0.6796875
train loss:  0.5494954586029053
train gradient:  0.18743679436816993
iteration : 6553
train acc:  0.75
train loss:  0.4642521142959595
train gradient:  0.13193965528045629
iteration : 6554
train acc:  0.734375
train loss:  0.504098653793335
train gradient:  0.12868458785890155
iteration : 6555
train acc:  0.6953125
train loss:  0.5730085372924805
train gradient:  0.17172455378360024
iteration : 6556
train acc:  0.7109375
train loss:  0.4976702928543091
train gradient:  0.1285082520845896
iteration : 6557
train acc:  0.7109375
train loss:  0.4889252483844757
train gradient:  0.1597374407892711
iteration : 6558
train acc:  0.7265625
train loss:  0.4996599555015564
train gradient:  0.1203600794542889
iteration : 6559
train acc:  0.6953125
train loss:  0.5473454594612122
train gradient:  0.1557464139283148
iteration : 6560
train acc:  0.7421875
train loss:  0.48878535628318787
train gradient:  0.13219620697938425
iteration : 6561
train acc:  0.671875
train loss:  0.5948587656021118
train gradient:  0.22819121701150585
iteration : 6562
train acc:  0.765625
train loss:  0.4584392011165619
train gradient:  0.09990857388029341
iteration : 6563
train acc:  0.765625
train loss:  0.4531104564666748
train gradient:  0.10055836203777416
iteration : 6564
train acc:  0.734375
train loss:  0.47442877292633057
train gradient:  0.0955602174158861
iteration : 6565
train acc:  0.796875
train loss:  0.47889530658721924
train gradient:  0.14404493167754834
iteration : 6566
train acc:  0.703125
train loss:  0.5769352912902832
train gradient:  0.24358170187013756
iteration : 6567
train acc:  0.7109375
train loss:  0.48104149103164673
train gradient:  0.12269817032530733
iteration : 6568
train acc:  0.6484375
train loss:  0.5406433939933777
train gradient:  0.14938897705961396
iteration : 6569
train acc:  0.796875
train loss:  0.4964173436164856
train gradient:  0.13745733140263316
iteration : 6570
train acc:  0.75
train loss:  0.4839770495891571
train gradient:  0.10652231502735507
iteration : 6571
train acc:  0.7421875
train loss:  0.5215058326721191
train gradient:  0.1461754513970886
iteration : 6572
train acc:  0.734375
train loss:  0.5329703092575073
train gradient:  0.12643424042990073
iteration : 6573
train acc:  0.7421875
train loss:  0.48351046442985535
train gradient:  0.14324395104819804
iteration : 6574
train acc:  0.78125
train loss:  0.43366536498069763
train gradient:  0.09464622283360215
iteration : 6575
train acc:  0.7265625
train loss:  0.5625895261764526
train gradient:  0.1540960084277107
iteration : 6576
train acc:  0.71875
train loss:  0.533798336982727
train gradient:  0.1384043259858208
iteration : 6577
train acc:  0.71875
train loss:  0.5171514749526978
train gradient:  0.14753059455984846
iteration : 6578
train acc:  0.6953125
train loss:  0.5932697057723999
train gradient:  0.17682261313769654
iteration : 6579
train acc:  0.765625
train loss:  0.5297173261642456
train gradient:  0.1884354741971067
iteration : 6580
train acc:  0.7578125
train loss:  0.45449236035346985
train gradient:  0.10116242589450661
iteration : 6581
train acc:  0.7265625
train loss:  0.4751035273075104
train gradient:  0.12358616578028173
iteration : 6582
train acc:  0.7734375
train loss:  0.4573742747306824
train gradient:  0.09739101744930534
iteration : 6583
train acc:  0.7265625
train loss:  0.5579534769058228
train gradient:  0.17353595748177775
iteration : 6584
train acc:  0.71875
train loss:  0.46584516763687134
train gradient:  0.1273524522060665
iteration : 6585
train acc:  0.7734375
train loss:  0.49066078662872314
train gradient:  0.1331112388514314
iteration : 6586
train acc:  0.734375
train loss:  0.529697835445404
train gradient:  0.15711710277831648
iteration : 6587
train acc:  0.7109375
train loss:  0.5420812368392944
train gradient:  0.18236554401559013
iteration : 6588
train acc:  0.765625
train loss:  0.4736476540565491
train gradient:  0.11314059246315279
iteration : 6589
train acc:  0.71875
train loss:  0.5530422329902649
train gradient:  0.2022457863168918
iteration : 6590
train acc:  0.7578125
train loss:  0.5005868077278137
train gradient:  0.1159607858352912
iteration : 6591
train acc:  0.6640625
train loss:  0.5638925433158875
train gradient:  0.18968245132075318
iteration : 6592
train acc:  0.71875
train loss:  0.499711275100708
train gradient:  0.13459085284189315
iteration : 6593
train acc:  0.8046875
train loss:  0.46509674191474915
train gradient:  0.09914663526194953
iteration : 6594
train acc:  0.796875
train loss:  0.4685443341732025
train gradient:  0.14530597855944177
iteration : 6595
train acc:  0.7421875
train loss:  0.4754434823989868
train gradient:  0.12161102001045522
iteration : 6596
train acc:  0.734375
train loss:  0.49064940214157104
train gradient:  0.17092285428736476
iteration : 6597
train acc:  0.671875
train loss:  0.653161883354187
train gradient:  0.23890113957595582
iteration : 6598
train acc:  0.765625
train loss:  0.48576775193214417
train gradient:  0.11725273508083622
iteration : 6599
train acc:  0.7109375
train loss:  0.5194178819656372
train gradient:  0.14076460146395298
iteration : 6600
train acc:  0.7421875
train loss:  0.49402961134910583
train gradient:  0.13505960022480312
iteration : 6601
train acc:  0.796875
train loss:  0.44887617230415344
train gradient:  0.10644533942531151
iteration : 6602
train acc:  0.7890625
train loss:  0.43167364597320557
train gradient:  0.10725370075124413
iteration : 6603
train acc:  0.7890625
train loss:  0.4576168656349182
train gradient:  0.10046291765574501
iteration : 6604
train acc:  0.703125
train loss:  0.5759457349777222
train gradient:  0.22709130495936716
iteration : 6605
train acc:  0.6640625
train loss:  0.5531010627746582
train gradient:  0.13649298833651524
iteration : 6606
train acc:  0.7109375
train loss:  0.5739275217056274
train gradient:  0.19443579434727495
iteration : 6607
train acc:  0.7421875
train loss:  0.49427974224090576
train gradient:  0.11274600265575777
iteration : 6608
train acc:  0.7109375
train loss:  0.5366291999816895
train gradient:  0.1612410757383525
iteration : 6609
train acc:  0.734375
train loss:  0.5381343364715576
train gradient:  0.17115602028811694
iteration : 6610
train acc:  0.75
train loss:  0.4486408233642578
train gradient:  0.126757919922177
iteration : 6611
train acc:  0.7109375
train loss:  0.5223779678344727
train gradient:  0.13272983305487857
iteration : 6612
train acc:  0.7265625
train loss:  0.5309978723526001
train gradient:  0.17020676396386936
iteration : 6613
train acc:  0.6875
train loss:  0.5262334942817688
train gradient:  0.1299518842935377
iteration : 6614
train acc:  0.7421875
train loss:  0.49907949566841125
train gradient:  0.14843770409238388
iteration : 6615
train acc:  0.7734375
train loss:  0.45907390117645264
train gradient:  0.09901587957881548
iteration : 6616
train acc:  0.640625
train loss:  0.5363772511482239
train gradient:  0.16122699237690571
iteration : 6617
train acc:  0.7890625
train loss:  0.4681866765022278
train gradient:  0.1431507109414207
iteration : 6618
train acc:  0.6875
train loss:  0.5595778822898865
train gradient:  0.18699191009386745
iteration : 6619
train acc:  0.7734375
train loss:  0.46912074089050293
train gradient:  0.13036731875593544
iteration : 6620
train acc:  0.765625
train loss:  0.4483777582645416
train gradient:  0.1269872552116178
iteration : 6621
train acc:  0.7421875
train loss:  0.48317867517471313
train gradient:  0.11071120093478912
iteration : 6622
train acc:  0.78125
train loss:  0.4666679799556732
train gradient:  0.12898684073063096
iteration : 6623
train acc:  0.7734375
train loss:  0.48950067162513733
train gradient:  0.10681507459419731
iteration : 6624
train acc:  0.59375
train loss:  0.6217440366744995
train gradient:  0.2251684227144548
iteration : 6625
train acc:  0.7265625
train loss:  0.5522916316986084
train gradient:  0.15793010343035951
iteration : 6626
train acc:  0.7109375
train loss:  0.5517972707748413
train gradient:  0.18918061332932534
iteration : 6627
train acc:  0.7265625
train loss:  0.5134008526802063
train gradient:  0.14430003042121894
iteration : 6628
train acc:  0.7265625
train loss:  0.4868544340133667
train gradient:  0.15593306232645016
iteration : 6629
train acc:  0.6953125
train loss:  0.5638408660888672
train gradient:  0.1433179392746779
iteration : 6630
train acc:  0.703125
train loss:  0.5143826603889465
train gradient:  0.13999834207903242
iteration : 6631
train acc:  0.7265625
train loss:  0.4894289970397949
train gradient:  0.13308816292692682
iteration : 6632
train acc:  0.765625
train loss:  0.4747994542121887
train gradient:  0.12784205694175269
iteration : 6633
train acc:  0.7109375
train loss:  0.501167356967926
train gradient:  0.1614892281079357
iteration : 6634
train acc:  0.71875
train loss:  0.49016469717025757
train gradient:  0.1243757492753306
iteration : 6635
train acc:  0.7109375
train loss:  0.543302595615387
train gradient:  0.12246231796616944
iteration : 6636
train acc:  0.7421875
train loss:  0.4831872284412384
train gradient:  0.1220738993699204
iteration : 6637
train acc:  0.6640625
train loss:  0.5421778559684753
train gradient:  0.20168745812034583
iteration : 6638
train acc:  0.734375
train loss:  0.5152125358581543
train gradient:  0.12617698194143012
iteration : 6639
train acc:  0.734375
train loss:  0.5787920951843262
train gradient:  0.1831773637247206
iteration : 6640
train acc:  0.7890625
train loss:  0.4211605191230774
train gradient:  0.09340337326765455
iteration : 6641
train acc:  0.65625
train loss:  0.5875257253646851
train gradient:  0.17067753259076507
iteration : 6642
train acc:  0.6640625
train loss:  0.5616599321365356
train gradient:  0.14803059849928196
iteration : 6643
train acc:  0.78125
train loss:  0.4752890467643738
train gradient:  0.11768098329460148
iteration : 6644
train acc:  0.796875
train loss:  0.4734342098236084
train gradient:  0.14571259960574046
iteration : 6645
train acc:  0.7578125
train loss:  0.4902724623680115
train gradient:  0.12237134932367165
iteration : 6646
train acc:  0.671875
train loss:  0.5517966747283936
train gradient:  0.18117796162909078
iteration : 6647
train acc:  0.7265625
train loss:  0.5101546049118042
train gradient:  0.14311129707668438
iteration : 6648
train acc:  0.734375
train loss:  0.5207662582397461
train gradient:  0.146345957509884
iteration : 6649
train acc:  0.703125
train loss:  0.4946191608905792
train gradient:  0.15443802294744263
iteration : 6650
train acc:  0.7265625
train loss:  0.5182123184204102
train gradient:  0.13122800066668902
iteration : 6651
train acc:  0.6953125
train loss:  0.5152708292007446
train gradient:  0.1141001312011066
iteration : 6652
train acc:  0.7734375
train loss:  0.4787772297859192
train gradient:  0.10620622071268825
iteration : 6653
train acc:  0.8046875
train loss:  0.46371597051620483
train gradient:  0.10492309854320041
iteration : 6654
train acc:  0.7578125
train loss:  0.45610469579696655
train gradient:  0.09400088401167572
iteration : 6655
train acc:  0.703125
train loss:  0.5456773042678833
train gradient:  0.12924850266715518
iteration : 6656
train acc:  0.703125
train loss:  0.5018870234489441
train gradient:  0.13538782954317952
iteration : 6657
train acc:  0.7265625
train loss:  0.5441393852233887
train gradient:  0.1393264286319412
iteration : 6658
train acc:  0.71875
train loss:  0.5325144529342651
train gradient:  0.17475379281266798
iteration : 6659
train acc:  0.7578125
train loss:  0.46413424611091614
train gradient:  0.1251065550570619
iteration : 6660
train acc:  0.75
train loss:  0.4962705969810486
train gradient:  0.13404480381947914
iteration : 6661
train acc:  0.7265625
train loss:  0.5179979801177979
train gradient:  0.11512009136610153
iteration : 6662
train acc:  0.7109375
train loss:  0.5223249197006226
train gradient:  0.1587392696993326
iteration : 6663
train acc:  0.7578125
train loss:  0.4974135458469391
train gradient:  0.14573659595984745
iteration : 6664
train acc:  0.7734375
train loss:  0.4601857662200928
train gradient:  0.1159996208461596
iteration : 6665
train acc:  0.78125
train loss:  0.46981245279312134
train gradient:  0.17760507795681293
iteration : 6666
train acc:  0.6796875
train loss:  0.5811144709587097
train gradient:  0.16869204834976576
iteration : 6667
train acc:  0.7734375
train loss:  0.4317159354686737
train gradient:  0.11362587879531721
iteration : 6668
train acc:  0.6953125
train loss:  0.5634915828704834
train gradient:  0.17303682790606376
iteration : 6669
train acc:  0.7265625
train loss:  0.48332011699676514
train gradient:  0.10785835082074402
iteration : 6670
train acc:  0.734375
train loss:  0.4799076318740845
train gradient:  0.1398451822898073
iteration : 6671
train acc:  0.7890625
train loss:  0.43313735723495483
train gradient:  0.11132166896081447
iteration : 6672
train acc:  0.7890625
train loss:  0.46944910287857056
train gradient:  0.11684857427568333
iteration : 6673
train acc:  0.7890625
train loss:  0.5211089849472046
train gradient:  0.13620563833963129
iteration : 6674
train acc:  0.703125
train loss:  0.48341548442840576
train gradient:  0.1406583380322129
iteration : 6675
train acc:  0.734375
train loss:  0.4877106547355652
train gradient:  0.14443540679942046
iteration : 6676
train acc:  0.7578125
train loss:  0.50175940990448
train gradient:  0.11284107545993703
iteration : 6677
train acc:  0.71875
train loss:  0.5177785158157349
train gradient:  0.14741269249155864
iteration : 6678
train acc:  0.765625
train loss:  0.47776326537132263
train gradient:  0.12938398845959226
iteration : 6679
train acc:  0.7109375
train loss:  0.5463778972625732
train gradient:  0.16472701958462277
iteration : 6680
train acc:  0.7578125
train loss:  0.4877042770385742
train gradient:  0.13352277685584907
iteration : 6681
train acc:  0.7109375
train loss:  0.5307915806770325
train gradient:  0.11709708166089411
iteration : 6682
train acc:  0.7578125
train loss:  0.48058760166168213
train gradient:  0.10901703616598472
iteration : 6683
train acc:  0.7109375
train loss:  0.5252374410629272
train gradient:  0.13752323057874846
iteration : 6684
train acc:  0.7421875
train loss:  0.48406580090522766
train gradient:  0.12407734977349805
iteration : 6685
train acc:  0.7578125
train loss:  0.479628324508667
train gradient:  0.13390654302531052
iteration : 6686
train acc:  0.7265625
train loss:  0.4816403388977051
train gradient:  0.13386712050188398
iteration : 6687
train acc:  0.7578125
train loss:  0.5656566619873047
train gradient:  0.17689010639057368
iteration : 6688
train acc:  0.734375
train loss:  0.5456054210662842
train gradient:  0.1512548089305809
iteration : 6689
train acc:  0.734375
train loss:  0.5366989374160767
train gradient:  0.1430979696856585
iteration : 6690
train acc:  0.734375
train loss:  0.504723846912384
train gradient:  0.15346028728353334
iteration : 6691
train acc:  0.6875
train loss:  0.5196906328201294
train gradient:  0.12655124458351816
iteration : 6692
train acc:  0.78125
train loss:  0.42799830436706543
train gradient:  0.09858807881986874
iteration : 6693
train acc:  0.734375
train loss:  0.5035720467567444
train gradient:  0.10450190933626854
iteration : 6694
train acc:  0.6875
train loss:  0.5487085580825806
train gradient:  0.14734104847748702
iteration : 6695
train acc:  0.78125
train loss:  0.47897812724113464
train gradient:  0.11318728329904608
iteration : 6696
train acc:  0.6875
train loss:  0.4961928427219391
train gradient:  0.1243698440534986
iteration : 6697
train acc:  0.6875
train loss:  0.5070286989212036
train gradient:  0.13919613330015243
iteration : 6698
train acc:  0.7890625
train loss:  0.436885267496109
train gradient:  0.10867463164651872
iteration : 6699
train acc:  0.7265625
train loss:  0.5386629104614258
train gradient:  0.11833231489718016
iteration : 6700
train acc:  0.6796875
train loss:  0.5211249589920044
train gradient:  0.12309354511892305
iteration : 6701
train acc:  0.7421875
train loss:  0.4821706712245941
train gradient:  0.1169657540777716
iteration : 6702
train acc:  0.7421875
train loss:  0.5328973531723022
train gradient:  0.1632403482281956
iteration : 6703
train acc:  0.71875
train loss:  0.4878791570663452
train gradient:  0.15912557360097576
iteration : 6704
train acc:  0.7109375
train loss:  0.5640599727630615
train gradient:  0.180851740733046
iteration : 6705
train acc:  0.734375
train loss:  0.44133126735687256
train gradient:  0.10059732054648542
iteration : 6706
train acc:  0.7421875
train loss:  0.49519890546798706
train gradient:  0.14100767161965927
iteration : 6707
train acc:  0.75
train loss:  0.45905542373657227
train gradient:  0.12587953532827295
iteration : 6708
train acc:  0.7578125
train loss:  0.48987072706222534
train gradient:  0.14795071112071134
iteration : 6709
train acc:  0.8046875
train loss:  0.4639813005924225
train gradient:  0.16099151330585376
iteration : 6710
train acc:  0.7109375
train loss:  0.5414138436317444
train gradient:  0.17977234865437208
iteration : 6711
train acc:  0.703125
train loss:  0.5345247387886047
train gradient:  0.1887122175056421
iteration : 6712
train acc:  0.75
train loss:  0.5096871256828308
train gradient:  0.19754510248392476
iteration : 6713
train acc:  0.7578125
train loss:  0.4963565468788147
train gradient:  0.1444345926659621
iteration : 6714
train acc:  0.703125
train loss:  0.5426151752471924
train gradient:  0.18436011902197802
iteration : 6715
train acc:  0.7265625
train loss:  0.5450310111045837
train gradient:  0.1965355006604313
iteration : 6716
train acc:  0.7578125
train loss:  0.4688936471939087
train gradient:  0.15391847715813708
iteration : 6717
train acc:  0.7109375
train loss:  0.5125817656517029
train gradient:  0.15175003735315246
iteration : 6718
train acc:  0.734375
train loss:  0.5346392393112183
train gradient:  0.1861398847885276
iteration : 6719
train acc:  0.65625
train loss:  0.5899922847747803
train gradient:  0.16843123358980738
iteration : 6720
train acc:  0.6484375
train loss:  0.6283608675003052
train gradient:  0.2086677784202861
iteration : 6721
train acc:  0.765625
train loss:  0.506720781326294
train gradient:  0.1923508488378436
iteration : 6722
train acc:  0.75
train loss:  0.529631495475769
train gradient:  0.2623616771679278
iteration : 6723
train acc:  0.7734375
train loss:  0.48127686977386475
train gradient:  0.1607458808986743
iteration : 6724
train acc:  0.7265625
train loss:  0.53331059217453
train gradient:  0.13200012132373729
iteration : 6725
train acc:  0.796875
train loss:  0.4846515655517578
train gradient:  0.1299335812919128
iteration : 6726
train acc:  0.75
train loss:  0.49259239435195923
train gradient:  0.13159459540550228
iteration : 6727
train acc:  0.7421875
train loss:  0.5182496309280396
train gradient:  0.16520814225798247
iteration : 6728
train acc:  0.7109375
train loss:  0.4804808497428894
train gradient:  0.15727424433546613
iteration : 6729
train acc:  0.6640625
train loss:  0.5393800139427185
train gradient:  0.1521504614779347
iteration : 6730
train acc:  0.7109375
train loss:  0.5207368731498718
train gradient:  0.12441011960064573
iteration : 6731
train acc:  0.796875
train loss:  0.4648863673210144
train gradient:  0.13768990718262664
iteration : 6732
train acc:  0.7734375
train loss:  0.49496281147003174
train gradient:  0.1300618003630413
iteration : 6733
train acc:  0.71875
train loss:  0.5317542552947998
train gradient:  0.1493294473304746
iteration : 6734
train acc:  0.7265625
train loss:  0.4929973781108856
train gradient:  0.1521970136291127
iteration : 6735
train acc:  0.7265625
train loss:  0.5096972584724426
train gradient:  0.14824642333848226
iteration : 6736
train acc:  0.7421875
train loss:  0.5214768648147583
train gradient:  0.16647504411743402
iteration : 6737
train acc:  0.703125
train loss:  0.5589677095413208
train gradient:  0.1644768812340009
iteration : 6738
train acc:  0.7578125
train loss:  0.48978978395462036
train gradient:  0.14492291663050766
iteration : 6739
train acc:  0.6953125
train loss:  0.5799496173858643
train gradient:  0.17141930601387745
iteration : 6740
train acc:  0.671875
train loss:  0.5747709274291992
train gradient:  0.17939341619811872
iteration : 6741
train acc:  0.7578125
train loss:  0.440236896276474
train gradient:  0.11069077746627198
iteration : 6742
train acc:  0.671875
train loss:  0.5513297915458679
train gradient:  0.1420867735569381
iteration : 6743
train acc:  0.75
train loss:  0.49092555046081543
train gradient:  0.11873791958745103
iteration : 6744
train acc:  0.671875
train loss:  0.5640126466751099
train gradient:  0.13615326503512115
iteration : 6745
train acc:  0.7109375
train loss:  0.5388666391372681
train gradient:  0.1456259611573348
iteration : 6746
train acc:  0.734375
train loss:  0.4956187605857849
train gradient:  0.1234911490629695
iteration : 6747
train acc:  0.71875
train loss:  0.5227460265159607
train gradient:  0.1495179834836923
iteration : 6748
train acc:  0.8359375
train loss:  0.42502719163894653
train gradient:  0.11159851362804163
iteration : 6749
train acc:  0.7265625
train loss:  0.500222384929657
train gradient:  0.14516997255671843
iteration : 6750
train acc:  0.71875
train loss:  0.5506114959716797
train gradient:  0.15278275132427546
iteration : 6751
train acc:  0.7578125
train loss:  0.47473862767219543
train gradient:  0.1251818930793104
iteration : 6752
train acc:  0.671875
train loss:  0.5534213781356812
train gradient:  0.2294014074674124
iteration : 6753
train acc:  0.7421875
train loss:  0.5511112213134766
train gradient:  0.17023867233879697
iteration : 6754
train acc:  0.828125
train loss:  0.5028164386749268
train gradient:  0.13696944349732165
iteration : 6755
train acc:  0.65625
train loss:  0.5096883773803711
train gradient:  0.15761341533852252
iteration : 6756
train acc:  0.734375
train loss:  0.47642752528190613
train gradient:  0.11021895192561514
iteration : 6757
train acc:  0.7734375
train loss:  0.5002161264419556
train gradient:  0.13882236127183908
iteration : 6758
train acc:  0.734375
train loss:  0.4897482395172119
train gradient:  0.12966937981519827
iteration : 6759
train acc:  0.71875
train loss:  0.5148628950119019
train gradient:  0.15146050181582954
iteration : 6760
train acc:  0.765625
train loss:  0.5362247228622437
train gradient:  0.14280114297774893
iteration : 6761
train acc:  0.71875
train loss:  0.5276460647583008
train gradient:  0.1439677427462971
iteration : 6762
train acc:  0.734375
train loss:  0.5204572677612305
train gradient:  0.1641472840152759
iteration : 6763
train acc:  0.6953125
train loss:  0.5327054262161255
train gradient:  0.17523592744393496
iteration : 6764
train acc:  0.765625
train loss:  0.5051493048667908
train gradient:  0.15934826103223754
iteration : 6765
train acc:  0.765625
train loss:  0.47261810302734375
train gradient:  0.14462332695023597
iteration : 6766
train acc:  0.71875
train loss:  0.5248176455497742
train gradient:  0.14717768976129347
iteration : 6767
train acc:  0.7109375
train loss:  0.519209623336792
train gradient:  0.14372936944048625
iteration : 6768
train acc:  0.71875
train loss:  0.5518764853477478
train gradient:  0.16134796024615833
iteration : 6769
train acc:  0.765625
train loss:  0.48522722721099854
train gradient:  0.11527230838855156
iteration : 6770
train acc:  0.765625
train loss:  0.4571654796600342
train gradient:  0.1316713105476956
iteration : 6771
train acc:  0.6484375
train loss:  0.5660123825073242
train gradient:  0.1518853552595106
iteration : 6772
train acc:  0.7421875
train loss:  0.5069824457168579
train gradient:  0.12477431054674157
iteration : 6773
train acc:  0.6640625
train loss:  0.5606929659843445
train gradient:  0.17867621419414217
iteration : 6774
train acc:  0.65625
train loss:  0.5764468312263489
train gradient:  0.1534392171935977
iteration : 6775
train acc:  0.7265625
train loss:  0.4976160526275635
train gradient:  0.1456904273772655
iteration : 6776
train acc:  0.703125
train loss:  0.5569511651992798
train gradient:  0.17415127291626914
iteration : 6777
train acc:  0.734375
train loss:  0.5534247159957886
train gradient:  0.18641918890006215
iteration : 6778
train acc:  0.8046875
train loss:  0.46650731563568115
train gradient:  0.1404927385391862
iteration : 6779
train acc:  0.7109375
train loss:  0.5581817626953125
train gradient:  0.14711378731466168
iteration : 6780
train acc:  0.7421875
train loss:  0.49865978956222534
train gradient:  0.13421638415944084
iteration : 6781
train acc:  0.7578125
train loss:  0.4900211691856384
train gradient:  0.11555304400535593
iteration : 6782
train acc:  0.7265625
train loss:  0.536409854888916
train gradient:  0.16151105645860808
iteration : 6783
train acc:  0.7578125
train loss:  0.5278294682502747
train gradient:  0.13736540427891158
iteration : 6784
train acc:  0.7421875
train loss:  0.5163711309432983
train gradient:  0.14084770237840177
iteration : 6785
train acc:  0.671875
train loss:  0.5589046478271484
train gradient:  0.1786472306605527
iteration : 6786
train acc:  0.7734375
train loss:  0.46615004539489746
train gradient:  0.1153508165983202
iteration : 6787
train acc:  0.734375
train loss:  0.48344501852989197
train gradient:  0.1369117187953143
iteration : 6788
train acc:  0.7578125
train loss:  0.47708213329315186
train gradient:  0.12033285719434013
iteration : 6789
train acc:  0.7109375
train loss:  0.5003942251205444
train gradient:  0.14131153402344948
iteration : 6790
train acc:  0.7109375
train loss:  0.537561297416687
train gradient:  0.1455222999007581
iteration : 6791
train acc:  0.7265625
train loss:  0.5352962017059326
train gradient:  0.13447404897104304
iteration : 6792
train acc:  0.75
train loss:  0.5324856042861938
train gradient:  0.14949636800821908
iteration : 6793
train acc:  0.7421875
train loss:  0.46171602606773376
train gradient:  0.14038081811430345
iteration : 6794
train acc:  0.7578125
train loss:  0.438089519739151
train gradient:  0.09594485193786219
iteration : 6795
train acc:  0.8125
train loss:  0.4738343358039856
train gradient:  0.13299333605223135
iteration : 6796
train acc:  0.6953125
train loss:  0.5362337827682495
train gradient:  0.12471748830919462
iteration : 6797
train acc:  0.7265625
train loss:  0.4939398467540741
train gradient:  0.15208041047810844
iteration : 6798
train acc:  0.796875
train loss:  0.49293968081474304
train gradient:  0.14120584478987835
iteration : 6799
train acc:  0.734375
train loss:  0.5063199996948242
train gradient:  0.1321266731277429
iteration : 6800
train acc:  0.7734375
train loss:  0.4694831967353821
train gradient:  0.14377361923625082
iteration : 6801
train acc:  0.6015625
train loss:  0.6271456480026245
train gradient:  0.20339953276802328
iteration : 6802
train acc:  0.7578125
train loss:  0.45652350783348083
train gradient:  0.09966648398826235
iteration : 6803
train acc:  0.765625
train loss:  0.5588334202766418
train gradient:  0.18200430948068835
iteration : 6804
train acc:  0.7265625
train loss:  0.478763222694397
train gradient:  0.12602192087123548
iteration : 6805
train acc:  0.7578125
train loss:  0.47766372561454773
train gradient:  0.11306842238849604
iteration : 6806
train acc:  0.734375
train loss:  0.507797122001648
train gradient:  0.14439483892687738
iteration : 6807
train acc:  0.7265625
train loss:  0.5275193452835083
train gradient:  0.13464135073295064
iteration : 6808
train acc:  0.765625
train loss:  0.5130883455276489
train gradient:  0.1516332020515887
iteration : 6809
train acc:  0.7734375
train loss:  0.4805743396282196
train gradient:  0.12752886402752373
iteration : 6810
train acc:  0.71875
train loss:  0.5069131255149841
train gradient:  0.12934118451959448
iteration : 6811
train acc:  0.6953125
train loss:  0.5603542923927307
train gradient:  0.13833468499306917
iteration : 6812
train acc:  0.8125
train loss:  0.4411768317222595
train gradient:  0.12521555520669503
iteration : 6813
train acc:  0.765625
train loss:  0.48217979073524475
train gradient:  0.1373930958587839
iteration : 6814
train acc:  0.7265625
train loss:  0.46834346652030945
train gradient:  0.1021840768736491
iteration : 6815
train acc:  0.765625
train loss:  0.4992510974407196
train gradient:  0.13869966694792893
iteration : 6816
train acc:  0.796875
train loss:  0.4613344371318817
train gradient:  0.1069011439668237
iteration : 6817
train acc:  0.7890625
train loss:  0.4707128405570984
train gradient:  0.12631740009170928
iteration : 6818
train acc:  0.6328125
train loss:  0.5888195037841797
train gradient:  0.20046652863405073
iteration : 6819
train acc:  0.7109375
train loss:  0.5003540515899658
train gradient:  0.15902279109674777
iteration : 6820
train acc:  0.78125
train loss:  0.49072614312171936
train gradient:  0.15615090001334014
iteration : 6821
train acc:  0.671875
train loss:  0.5802047252655029
train gradient:  0.19834481363032685
iteration : 6822
train acc:  0.7421875
train loss:  0.5384366512298584
train gradient:  0.18047557892193022
iteration : 6823
train acc:  0.671875
train loss:  0.546209990978241
train gradient:  0.13156685018234127
iteration : 6824
train acc:  0.78125
train loss:  0.4770715832710266
train gradient:  0.14015721997401487
iteration : 6825
train acc:  0.75
train loss:  0.4957003593444824
train gradient:  0.1310002628554885
iteration : 6826
train acc:  0.7265625
train loss:  0.5208824276924133
train gradient:  0.11256692140650444
iteration : 6827
train acc:  0.7890625
train loss:  0.4348389506340027
train gradient:  0.10983666939170951
iteration : 6828
train acc:  0.7578125
train loss:  0.4902440309524536
train gradient:  0.140997941459042
iteration : 6829
train acc:  0.765625
train loss:  0.4977474808692932
train gradient:  0.12808045126979867
iteration : 6830
train acc:  0.78125
train loss:  0.4665754437446594
train gradient:  0.1606112491598098
iteration : 6831
train acc:  0.8125
train loss:  0.4609871506690979
train gradient:  0.14333530614943285
iteration : 6832
train acc:  0.6953125
train loss:  0.5362527370452881
train gradient:  0.1871403823526896
iteration : 6833
train acc:  0.6796875
train loss:  0.5545663833618164
train gradient:  0.18350777569736182
iteration : 6834
train acc:  0.8125
train loss:  0.4564915895462036
train gradient:  0.12380313103034526
iteration : 6835
train acc:  0.6875
train loss:  0.5385749340057373
train gradient:  0.1972616524753227
iteration : 6836
train acc:  0.734375
train loss:  0.5272308588027954
train gradient:  0.16337041085682344
iteration : 6837
train acc:  0.71875
train loss:  0.5110857486724854
train gradient:  0.1254535372858619
iteration : 6838
train acc:  0.7578125
train loss:  0.42833197116851807
train gradient:  0.13797339469751285
iteration : 6839
train acc:  0.765625
train loss:  0.5043554306030273
train gradient:  0.16906450126897465
iteration : 6840
train acc:  0.765625
train loss:  0.44976815581321716
train gradient:  0.11043835976681486
iteration : 6841
train acc:  0.6484375
train loss:  0.5285617113113403
train gradient:  0.14860660322386748
iteration : 6842
train acc:  0.7421875
train loss:  0.5186333656311035
train gradient:  0.15408910231008888
iteration : 6843
train acc:  0.75
train loss:  0.4687342643737793
train gradient:  0.11362166017413049
iteration : 6844
train acc:  0.765625
train loss:  0.48605039715766907
train gradient:  0.17434651547330382
iteration : 6845
train acc:  0.7265625
train loss:  0.5164344310760498
train gradient:  0.16008955280669435
iteration : 6846
train acc:  0.703125
train loss:  0.5308297872543335
train gradient:  0.1480610486185161
iteration : 6847
train acc:  0.7578125
train loss:  0.4618287682533264
train gradient:  0.11656480457027542
iteration : 6848
train acc:  0.7578125
train loss:  0.47275668382644653
train gradient:  0.11809402706706917
iteration : 6849
train acc:  0.7421875
train loss:  0.5055555105209351
train gradient:  0.1701201964349358
iteration : 6850
train acc:  0.6484375
train loss:  0.5877953171730042
train gradient:  0.15875944733097946
iteration : 6851
train acc:  0.71875
train loss:  0.5455607175827026
train gradient:  0.15326274360154932
iteration : 6852
train acc:  0.7421875
train loss:  0.5596782565116882
train gradient:  0.15535303996761626
iteration : 6853
train acc:  0.7734375
train loss:  0.4631311297416687
train gradient:  0.1374898132209143
iteration : 6854
train acc:  0.734375
train loss:  0.48066043853759766
train gradient:  0.1207473233555957
iteration : 6855
train acc:  0.765625
train loss:  0.488420307636261
train gradient:  0.13186001863248348
iteration : 6856
train acc:  0.6953125
train loss:  0.6088089346885681
train gradient:  0.16865683014554772
iteration : 6857
train acc:  0.7265625
train loss:  0.5430155992507935
train gradient:  0.14349016180347096
iteration : 6858
train acc:  0.7109375
train loss:  0.5188888311386108
train gradient:  0.1579418116845157
iteration : 6859
train acc:  0.75
train loss:  0.4969821572303772
train gradient:  0.1569111419247155
iteration : 6860
train acc:  0.7578125
train loss:  0.5067821741104126
train gradient:  0.13744222196074357
iteration : 6861
train acc:  0.734375
train loss:  0.5120753645896912
train gradient:  0.14667580683400575
iteration : 6862
train acc:  0.6484375
train loss:  0.6008705496788025
train gradient:  0.22645523550171648
iteration : 6863
train acc:  0.75
train loss:  0.49208423495292664
train gradient:  0.13716506559750818
iteration : 6864
train acc:  0.7265625
train loss:  0.5301626920700073
train gradient:  0.14613619788211343
iteration : 6865
train acc:  0.8203125
train loss:  0.4217342436313629
train gradient:  0.10816401904759645
iteration : 6866
train acc:  0.75
train loss:  0.5059695243835449
train gradient:  0.12367156655306985
iteration : 6867
train acc:  0.6953125
train loss:  0.6385782957077026
train gradient:  0.23640874688628893
iteration : 6868
train acc:  0.71875
train loss:  0.5437727570533752
train gradient:  0.13085047791738777
iteration : 6869
train acc:  0.6640625
train loss:  0.5349919199943542
train gradient:  0.17223485555246773
iteration : 6870
train acc:  0.8125
train loss:  0.4399825930595398
train gradient:  0.12555016557553875
iteration : 6871
train acc:  0.7109375
train loss:  0.48476696014404297
train gradient:  0.169732438675448
iteration : 6872
train acc:  0.8359375
train loss:  0.4006175994873047
train gradient:  0.09037956286903452
iteration : 6873
train acc:  0.765625
train loss:  0.4659397006034851
train gradient:  0.1104906237095295
iteration : 6874
train acc:  0.734375
train loss:  0.542015552520752
train gradient:  0.18887367278302697
iteration : 6875
train acc:  0.7578125
train loss:  0.48684632778167725
train gradient:  0.15943570462397336
iteration : 6876
train acc:  0.703125
train loss:  0.5040102005004883
train gradient:  0.15772110759813618
iteration : 6877
train acc:  0.703125
train loss:  0.5528140068054199
train gradient:  0.14983731047248605
iteration : 6878
train acc:  0.7421875
train loss:  0.478990763425827
train gradient:  0.11452828538900268
iteration : 6879
train acc:  0.609375
train loss:  0.617695689201355
train gradient:  0.16843602600709176
iteration : 6880
train acc:  0.7421875
train loss:  0.5086919665336609
train gradient:  0.175421659410811
iteration : 6881
train acc:  0.6875
train loss:  0.5528242588043213
train gradient:  0.20395194026675234
iteration : 6882
train acc:  0.65625
train loss:  0.5917576551437378
train gradient:  0.229428578031186
iteration : 6883
train acc:  0.7890625
train loss:  0.4351704716682434
train gradient:  0.12344430233271314
iteration : 6884
train acc:  0.78125
train loss:  0.443966805934906
train gradient:  0.12696139844159993
iteration : 6885
train acc:  0.75
train loss:  0.4605580270290375
train gradient:  0.12906574887679292
iteration : 6886
train acc:  0.6953125
train loss:  0.5428032875061035
train gradient:  0.16693263714364837
iteration : 6887
train acc:  0.71875
train loss:  0.4967384338378906
train gradient:  0.13996284037200712
iteration : 6888
train acc:  0.734375
train loss:  0.48416823148727417
train gradient:  0.12181440887493822
iteration : 6889
train acc:  0.75
train loss:  0.4901931881904602
train gradient:  0.1620264414070689
iteration : 6890
train acc:  0.625
train loss:  0.5618051290512085
train gradient:  0.19127841908242088
iteration : 6891
train acc:  0.671875
train loss:  0.6128732562065125
train gradient:  0.24408135590687352
iteration : 6892
train acc:  0.7421875
train loss:  0.4443933665752411
train gradient:  0.10125657610528992
iteration : 6893
train acc:  0.7265625
train loss:  0.5546982288360596
train gradient:  0.16655525358396767
iteration : 6894
train acc:  0.71875
train loss:  0.48582762479782104
train gradient:  0.13365697572393317
iteration : 6895
train acc:  0.765625
train loss:  0.4517582356929779
train gradient:  0.1126748571049217
iteration : 6896
train acc:  0.8125
train loss:  0.4212194085121155
train gradient:  0.10915740301885679
iteration : 6897
train acc:  0.8515625
train loss:  0.4304646849632263
train gradient:  0.16353056565973334
iteration : 6898
train acc:  0.7265625
train loss:  0.5141979455947876
train gradient:  0.12994905080506347
iteration : 6899
train acc:  0.7421875
train loss:  0.47905898094177246
train gradient:  0.1199041993032366
iteration : 6900
train acc:  0.7109375
train loss:  0.5351549983024597
train gradient:  0.14505344469234022
iteration : 6901
train acc:  0.671875
train loss:  0.5443154573440552
train gradient:  0.1602492432505362
iteration : 6902
train acc:  0.625
train loss:  0.5902463793754578
train gradient:  0.2096085802167948
iteration : 6903
train acc:  0.640625
train loss:  0.6171100735664368
train gradient:  0.22007496651955064
iteration : 6904
train acc:  0.671875
train loss:  0.5348871350288391
train gradient:  0.14614475773494495
iteration : 6905
train acc:  0.796875
train loss:  0.4778914451599121
train gradient:  0.13396880192008873
iteration : 6906
train acc:  0.7109375
train loss:  0.5834466218948364
train gradient:  0.17615759149760357
iteration : 6907
train acc:  0.71875
train loss:  0.5318310260772705
train gradient:  0.1674195480853361
iteration : 6908
train acc:  0.7265625
train loss:  0.49431493878364563
train gradient:  0.1455203555233039
iteration : 6909
train acc:  0.71875
train loss:  0.5450483560562134
train gradient:  0.1811613138320961
iteration : 6910
train acc:  0.7734375
train loss:  0.5065171718597412
train gradient:  0.1380233169506549
iteration : 6911
train acc:  0.7109375
train loss:  0.5815472602844238
train gradient:  0.16384366932363958
iteration : 6912
train acc:  0.7734375
train loss:  0.4551263749599457
train gradient:  0.11341446601484251
iteration : 6913
train acc:  0.796875
train loss:  0.438113808631897
train gradient:  0.12402670850457201
iteration : 6914
train acc:  0.6484375
train loss:  0.6370933055877686
train gradient:  0.18022855204157667
iteration : 6915
train acc:  0.734375
train loss:  0.5469295978546143
train gradient:  0.19500112839011152
iteration : 6916
train acc:  0.703125
train loss:  0.539900541305542
train gradient:  0.13755326849436011
iteration : 6917
train acc:  0.734375
train loss:  0.48139476776123047
train gradient:  0.13948761026512171
iteration : 6918
train acc:  0.7734375
train loss:  0.4746760129928589
train gradient:  0.1348045550201053
iteration : 6919
train acc:  0.734375
train loss:  0.486637145280838
train gradient:  0.10874688808416898
iteration : 6920
train acc:  0.71875
train loss:  0.495197057723999
train gradient:  0.14925831623750435
iteration : 6921
train acc:  0.78125
train loss:  0.43718504905700684
train gradient:  0.09103960267291385
iteration : 6922
train acc:  0.734375
train loss:  0.4924178123474121
train gradient:  0.1331169302054404
iteration : 6923
train acc:  0.8046875
train loss:  0.4784926474094391
train gradient:  0.10951923802332322
iteration : 6924
train acc:  0.7734375
train loss:  0.4973177909851074
train gradient:  0.12734444540864423
iteration : 6925
train acc:  0.7265625
train loss:  0.47310444712638855
train gradient:  0.11141832661645118
iteration : 6926
train acc:  0.6796875
train loss:  0.5676433444023132
train gradient:  0.18503385873361472
iteration : 6927
train acc:  0.7734375
train loss:  0.42442333698272705
train gradient:  0.09017713388289256
iteration : 6928
train acc:  0.7578125
train loss:  0.46067628264427185
train gradient:  0.10834869442238056
iteration : 6929
train acc:  0.7421875
train loss:  0.48885613679885864
train gradient:  0.09848345665403577
iteration : 6930
train acc:  0.71875
train loss:  0.48026180267333984
train gradient:  0.12845685607672735
iteration : 6931
train acc:  0.8125
train loss:  0.43064039945602417
train gradient:  0.09842003820960057
iteration : 6932
train acc:  0.71875
train loss:  0.5148507952690125
train gradient:  0.13487392090311107
iteration : 6933
train acc:  0.75
train loss:  0.5243736505508423
train gradient:  0.15979477921793617
iteration : 6934
train acc:  0.7265625
train loss:  0.5465983152389526
train gradient:  0.22193325021802635
iteration : 6935
train acc:  0.734375
train loss:  0.4726213812828064
train gradient:  0.1271809548656096
iteration : 6936
train acc:  0.7578125
train loss:  0.4621204137802124
train gradient:  0.10082171028658723
iteration : 6937
train acc:  0.7578125
train loss:  0.5167280435562134
train gradient:  0.15516870842174874
iteration : 6938
train acc:  0.71875
train loss:  0.4990493655204773
train gradient:  0.16149467044882898
iteration : 6939
train acc:  0.6953125
train loss:  0.4825647473335266
train gradient:  0.12405674951519555
iteration : 6940
train acc:  0.6953125
train loss:  0.4964972734451294
train gradient:  0.1587166544327086
iteration : 6941
train acc:  0.8046875
train loss:  0.46578913927078247
train gradient:  0.11932915795403158
iteration : 6942
train acc:  0.7265625
train loss:  0.4829469323158264
train gradient:  0.15794398914627422
iteration : 6943
train acc:  0.71875
train loss:  0.48736751079559326
train gradient:  0.12745385366519874
iteration : 6944
train acc:  0.703125
train loss:  0.5583444237709045
train gradient:  0.16231779168739388
iteration : 6945
train acc:  0.6796875
train loss:  0.580237627029419
train gradient:  0.1945308338485574
iteration : 6946
train acc:  0.7734375
train loss:  0.42992866039276123
train gradient:  0.11109604691417341
iteration : 6947
train acc:  0.6796875
train loss:  0.5275841951370239
train gradient:  0.14143198149191516
iteration : 6948
train acc:  0.7265625
train loss:  0.49743157625198364
train gradient:  0.13896998920884818
iteration : 6949
train acc:  0.71875
train loss:  0.4975641965866089
train gradient:  0.1439496188653595
iteration : 6950
train acc:  0.71875
train loss:  0.4947887659072876
train gradient:  0.17060233709244335
iteration : 6951
train acc:  0.71875
train loss:  0.47598132491111755
train gradient:  0.13138009192176453
iteration : 6952
train acc:  0.6875
train loss:  0.5714688301086426
train gradient:  0.15291832237913772
iteration : 6953
train acc:  0.6640625
train loss:  0.5790842771530151
train gradient:  0.16213587664297
iteration : 6954
train acc:  0.7265625
train loss:  0.50065678358078
train gradient:  0.1507164339972537
iteration : 6955
train acc:  0.734375
train loss:  0.5176591873168945
train gradient:  0.15859706484824415
iteration : 6956
train acc:  0.828125
train loss:  0.43026602268218994
train gradient:  0.12821813329426227
iteration : 6957
train acc:  0.7890625
train loss:  0.47630858421325684
train gradient:  0.13118341215297186
iteration : 6958
train acc:  0.7265625
train loss:  0.5456855297088623
train gradient:  0.14790764942760926
iteration : 6959
train acc:  0.796875
train loss:  0.4471624493598938
train gradient:  0.10959157605518285
iteration : 6960
train acc:  0.7265625
train loss:  0.5371477603912354
train gradient:  0.19638683701913334
iteration : 6961
train acc:  0.734375
train loss:  0.46509361267089844
train gradient:  0.12285606674845789
iteration : 6962
train acc:  0.703125
train loss:  0.4926409125328064
train gradient:  0.10895361401242278
iteration : 6963
train acc:  0.796875
train loss:  0.4062061607837677
train gradient:  0.13129035933069533
iteration : 6964
train acc:  0.796875
train loss:  0.4327189028263092
train gradient:  0.09631138674969043
iteration : 6965
train acc:  0.7578125
train loss:  0.4749453067779541
train gradient:  0.12194369742380635
iteration : 6966
train acc:  0.7265625
train loss:  0.5213844180107117
train gradient:  0.15461125880633547
iteration : 6967
train acc:  0.6953125
train loss:  0.5386841297149658
train gradient:  0.20195008534644965
iteration : 6968
train acc:  0.8203125
train loss:  0.41588395833969116
train gradient:  0.09123155001609944
iteration : 6969
train acc:  0.796875
train loss:  0.43514955043792725
train gradient:  0.14224166866338753
iteration : 6970
train acc:  0.765625
train loss:  0.4807215631008148
train gradient:  0.15747421904741754
iteration : 6971
train acc:  0.7421875
train loss:  0.5239866375923157
train gradient:  0.18701250834867156
iteration : 6972
train acc:  0.7421875
train loss:  0.5412788987159729
train gradient:  0.14842115819753599
iteration : 6973
train acc:  0.703125
train loss:  0.49175822734832764
train gradient:  0.1465716030640708
iteration : 6974
train acc:  0.6953125
train loss:  0.5792548060417175
train gradient:  0.1904336316979352
iteration : 6975
train acc:  0.78125
train loss:  0.4727799892425537
train gradient:  0.1421517087389791
iteration : 6976
train acc:  0.734375
train loss:  0.46912530064582825
train gradient:  0.10356828364559735
iteration : 6977
train acc:  0.7109375
train loss:  0.5202459096908569
train gradient:  0.13815326388457924
iteration : 6978
train acc:  0.765625
train loss:  0.518991231918335
train gradient:  0.1286880320056542
iteration : 6979
train acc:  0.6875
train loss:  0.5334517359733582
train gradient:  0.16860465328737445
iteration : 6980
train acc:  0.7421875
train loss:  0.5074621438980103
train gradient:  0.1635057280900475
iteration : 6981
train acc:  0.71875
train loss:  0.5280362367630005
train gradient:  0.1596767181778131
iteration : 6982
train acc:  0.7890625
train loss:  0.4480777084827423
train gradient:  0.11190506397415466
iteration : 6983
train acc:  0.703125
train loss:  0.5407585501670837
train gradient:  0.20413194515201383
iteration : 6984
train acc:  0.7421875
train loss:  0.5114977359771729
train gradient:  0.14463520132936777
iteration : 6985
train acc:  0.734375
train loss:  0.5281287431716919
train gradient:  0.13640585973241687
iteration : 6986
train acc:  0.7109375
train loss:  0.48768842220306396
train gradient:  0.13161675364822573
iteration : 6987
train acc:  0.7578125
train loss:  0.4948888421058655
train gradient:  0.13346019264270387
iteration : 6988
train acc:  0.71875
train loss:  0.48257285356521606
train gradient:  0.1279417905420929
iteration : 6989
train acc:  0.8125
train loss:  0.44268426299095154
train gradient:  0.14833108808636072
iteration : 6990
train acc:  0.765625
train loss:  0.4856444299221039
train gradient:  0.1190252369244889
iteration : 6991
train acc:  0.71875
train loss:  0.5517167448997498
train gradient:  0.18013082564175348
iteration : 6992
train acc:  0.7734375
train loss:  0.49114805459976196
train gradient:  0.14806580922045176
iteration : 6993
train acc:  0.6953125
train loss:  0.5048313736915588
train gradient:  0.12253276421002479
iteration : 6994
train acc:  0.6328125
train loss:  0.5928726196289062
train gradient:  0.1783799971302282
iteration : 6995
train acc:  0.78125
train loss:  0.498662531375885
train gradient:  0.12400166963331019
iteration : 6996
train acc:  0.703125
train loss:  0.49570968747138977
train gradient:  0.14821178488470405
iteration : 6997
train acc:  0.7421875
train loss:  0.4975442886352539
train gradient:  0.13303202569017591
iteration : 6998
train acc:  0.7734375
train loss:  0.45599740743637085
train gradient:  0.11689205243283235
iteration : 6999
train acc:  0.703125
train loss:  0.5556163787841797
train gradient:  0.15849144048687164
iteration : 7000
train acc:  0.8203125
train loss:  0.431347131729126
train gradient:  0.11949063719852013
iteration : 7001
train acc:  0.796875
train loss:  0.4605334401130676
train gradient:  0.11267181105830311
iteration : 7002
train acc:  0.7890625
train loss:  0.48092249035835266
train gradient:  0.16491485445805362
iteration : 7003
train acc:  0.7734375
train loss:  0.4471311867237091
train gradient:  0.12166908121468506
iteration : 7004
train acc:  0.7421875
train loss:  0.5045452117919922
train gradient:  0.14874174328708345
iteration : 7005
train acc:  0.7109375
train loss:  0.5116676688194275
train gradient:  0.13891190323801111
iteration : 7006
train acc:  0.765625
train loss:  0.43176957964897156
train gradient:  0.1136336588615019
iteration : 7007
train acc:  0.7109375
train loss:  0.5162739157676697
train gradient:  0.13858199455799375
iteration : 7008
train acc:  0.6953125
train loss:  0.4748380780220032
train gradient:  0.13397774789076133
iteration : 7009
train acc:  0.703125
train loss:  0.5414921045303345
train gradient:  0.207292053582932
iteration : 7010
train acc:  0.703125
train loss:  0.5570170879364014
train gradient:  0.14784067632666525
iteration : 7011
train acc:  0.75
train loss:  0.5220741629600525
train gradient:  0.14830811653417206
iteration : 7012
train acc:  0.6875
train loss:  0.5543541312217712
train gradient:  0.17368741696025117
iteration : 7013
train acc:  0.7109375
train loss:  0.5677469968795776
train gradient:  0.1392938989282668
iteration : 7014
train acc:  0.7734375
train loss:  0.483391135931015
train gradient:  0.11190214953420034
iteration : 7015
train acc:  0.734375
train loss:  0.5215237140655518
train gradient:  0.17299752191436768
iteration : 7016
train acc:  0.6953125
train loss:  0.6178069114685059
train gradient:  0.19631468029380517
iteration : 7017
train acc:  0.7421875
train loss:  0.47524410486221313
train gradient:  0.13453206205352136
iteration : 7018
train acc:  0.703125
train loss:  0.5230847597122192
train gradient:  0.13008245876083696
iteration : 7019
train acc:  0.7734375
train loss:  0.47189128398895264
train gradient:  0.1601765989737107
iteration : 7020
train acc:  0.671875
train loss:  0.5891117453575134
train gradient:  0.22869424208247768
iteration : 7021
train acc:  0.75
train loss:  0.4961525499820709
train gradient:  0.15513820574779488
iteration : 7022
train acc:  0.765625
train loss:  0.5071409940719604
train gradient:  0.14873711858541566
iteration : 7023
train acc:  0.7734375
train loss:  0.5233309864997864
train gradient:  0.14347654987803624
iteration : 7024
train acc:  0.765625
train loss:  0.4752669930458069
train gradient:  0.12225397687219287
iteration : 7025
train acc:  0.734375
train loss:  0.5036719441413879
train gradient:  0.1424625924480588
iteration : 7026
train acc:  0.734375
train loss:  0.5084774494171143
train gradient:  0.14648836717476757
iteration : 7027
train acc:  0.734375
train loss:  0.4920613169670105
train gradient:  0.1197640970335993
iteration : 7028
train acc:  0.8046875
train loss:  0.48312121629714966
train gradient:  0.11292946388673067
iteration : 7029
train acc:  0.6875
train loss:  0.5180239677429199
train gradient:  0.12818180245485797
iteration : 7030
train acc:  0.671875
train loss:  0.5005463361740112
train gradient:  0.14388386435113043
iteration : 7031
train acc:  0.7734375
train loss:  0.4597143530845642
train gradient:  0.13582032504776703
iteration : 7032
train acc:  0.7578125
train loss:  0.47523194551467896
train gradient:  0.1333722322104307
iteration : 7033
train acc:  0.7421875
train loss:  0.5079564452171326
train gradient:  0.16184199857493314
iteration : 7034
train acc:  0.7109375
train loss:  0.5293012857437134
train gradient:  0.12956329656182697
iteration : 7035
train acc:  0.7109375
train loss:  0.5149210095405579
train gradient:  0.13127337946811085
iteration : 7036
train acc:  0.703125
train loss:  0.49946755170822144
train gradient:  0.11210051676290178
iteration : 7037
train acc:  0.765625
train loss:  0.4924534559249878
train gradient:  0.1118558247916386
iteration : 7038
train acc:  0.7109375
train loss:  0.5602539777755737
train gradient:  0.17557551608140387
iteration : 7039
train acc:  0.7109375
train loss:  0.5218674540519714
train gradient:  0.13819315854862443
iteration : 7040
train acc:  0.7265625
train loss:  0.5057932734489441
train gradient:  0.13473755627792253
iteration : 7041
train acc:  0.7890625
train loss:  0.4499656558036804
train gradient:  0.1622961466035674
iteration : 7042
train acc:  0.7265625
train loss:  0.556846559047699
train gradient:  0.16060171265031303
iteration : 7043
train acc:  0.7734375
train loss:  0.4432607889175415
train gradient:  0.13380380355046184
iteration : 7044
train acc:  0.7421875
train loss:  0.4747571647167206
train gradient:  0.1286271591119783
iteration : 7045
train acc:  0.7578125
train loss:  0.463980108499527
train gradient:  0.18793856695251754
iteration : 7046
train acc:  0.75
train loss:  0.501941442489624
train gradient:  0.13266627703501283
iteration : 7047
train acc:  0.734375
train loss:  0.4843287467956543
train gradient:  0.13240172900047134
iteration : 7048
train acc:  0.765625
train loss:  0.47448956966400146
train gradient:  0.15121216073139365
iteration : 7049
train acc:  0.8046875
train loss:  0.41622769832611084
train gradient:  0.08947528966092008
iteration : 7050
train acc:  0.765625
train loss:  0.5008607506752014
train gradient:  0.14278667199636275
iteration : 7051
train acc:  0.7578125
train loss:  0.4494571089744568
train gradient:  0.12844719029852208
iteration : 7052
train acc:  0.7890625
train loss:  0.4488223195075989
train gradient:  0.12144855827580862
iteration : 7053
train acc:  0.7421875
train loss:  0.4467965066432953
train gradient:  0.130330918252681
iteration : 7054
train acc:  0.78125
train loss:  0.47633296251296997
train gradient:  0.13551920468934253
iteration : 7055
train acc:  0.7109375
train loss:  0.5608676075935364
train gradient:  0.16505089134094802
iteration : 7056
train acc:  0.734375
train loss:  0.4742140769958496
train gradient:  0.15584516540271995
iteration : 7057
train acc:  0.7734375
train loss:  0.44786107540130615
train gradient:  0.12073324651660816
iteration : 7058
train acc:  0.75
train loss:  0.4923669993877411
train gradient:  0.12010269198790371
iteration : 7059
train acc:  0.7578125
train loss:  0.4424093961715698
train gradient:  0.11221299558736791
iteration : 7060
train acc:  0.7578125
train loss:  0.4785194396972656
train gradient:  0.1354369354491113
iteration : 7061
train acc:  0.765625
train loss:  0.4905940890312195
train gradient:  0.1657987638586324
iteration : 7062
train acc:  0.7421875
train loss:  0.4935622215270996
train gradient:  0.11512498751261507
iteration : 7063
train acc:  0.703125
train loss:  0.5158820748329163
train gradient:  0.15091183748453796
iteration : 7064
train acc:  0.734375
train loss:  0.5096880197525024
train gradient:  0.12617037463400566
iteration : 7065
train acc:  0.7265625
train loss:  0.5385822653770447
train gradient:  0.1870164298958874
iteration : 7066
train acc:  0.671875
train loss:  0.5746273398399353
train gradient:  0.19863327361948835
iteration : 7067
train acc:  0.7578125
train loss:  0.47597402334213257
train gradient:  0.13711651936150993
iteration : 7068
train acc:  0.7265625
train loss:  0.532981276512146
train gradient:  0.17274053455912952
iteration : 7069
train acc:  0.65625
train loss:  0.5371166467666626
train gradient:  0.18420014476132374
iteration : 7070
train acc:  0.765625
train loss:  0.4972551465034485
train gradient:  0.10769259931854082
iteration : 7071
train acc:  0.671875
train loss:  0.5936772227287292
train gradient:  0.18738889046805357
iteration : 7072
train acc:  0.8125
train loss:  0.44958817958831787
train gradient:  0.14647035880553463
iteration : 7073
train acc:  0.7109375
train loss:  0.49231982231140137
train gradient:  0.1534858117399363
iteration : 7074
train acc:  0.75
train loss:  0.49355754256248474
train gradient:  0.1312685826661733
iteration : 7075
train acc:  0.7578125
train loss:  0.48283278942108154
train gradient:  0.13618020960914914
iteration : 7076
train acc:  0.703125
train loss:  0.5150505304336548
train gradient:  0.13699658174897011
iteration : 7077
train acc:  0.71875
train loss:  0.5491158962249756
train gradient:  0.15105555844439422
iteration : 7078
train acc:  0.671875
train loss:  0.5594302415847778
train gradient:  0.13133336592638342
iteration : 7079
train acc:  0.6953125
train loss:  0.4997442364692688
train gradient:  0.15423991316731855
iteration : 7080
train acc:  0.671875
train loss:  0.5368341207504272
train gradient:  0.19651710970452158
iteration : 7081
train acc:  0.6953125
train loss:  0.5684763193130493
train gradient:  0.24440900081939737
iteration : 7082
train acc:  0.6953125
train loss:  0.5491009950637817
train gradient:  0.17543586738810885
iteration : 7083
train acc:  0.7578125
train loss:  0.4990619421005249
train gradient:  0.1334765152168251
iteration : 7084
train acc:  0.640625
train loss:  0.6055724620819092
train gradient:  0.18212677397487714
iteration : 7085
train acc:  0.703125
train loss:  0.5465220212936401
train gradient:  0.1531358474909482
iteration : 7086
train acc:  0.703125
train loss:  0.476021409034729
train gradient:  0.1519283667980124
iteration : 7087
train acc:  0.7890625
train loss:  0.5070621967315674
train gradient:  0.1445701129634992
iteration : 7088
train acc:  0.75
train loss:  0.5015974640846252
train gradient:  0.16384599295471108
iteration : 7089
train acc:  0.6640625
train loss:  0.5612378120422363
train gradient:  0.1621195573066152
iteration : 7090
train acc:  0.7421875
train loss:  0.4663354158401489
train gradient:  0.13531124764918329
iteration : 7091
train acc:  0.7421875
train loss:  0.48978787660598755
train gradient:  0.14757385646698296
iteration : 7092
train acc:  0.765625
train loss:  0.5127315521240234
train gradient:  0.12779641234120964
iteration : 7093
train acc:  0.7578125
train loss:  0.5160096883773804
train gradient:  0.1594238096253015
iteration : 7094
train acc:  0.78125
train loss:  0.48032861948013306
train gradient:  0.12578491762241523
iteration : 7095
train acc:  0.765625
train loss:  0.47112807631492615
train gradient:  0.1446324846631882
iteration : 7096
train acc:  0.7265625
train loss:  0.4729158282279968
train gradient:  0.12048889709823853
iteration : 7097
train acc:  0.75
train loss:  0.4680466651916504
train gradient:  0.1233019099425413
iteration : 7098
train acc:  0.7578125
train loss:  0.4718535244464874
train gradient:  0.1386062957627321
iteration : 7099
train acc:  0.6796875
train loss:  0.5785449743270874
train gradient:  0.16504418553601385
iteration : 7100
train acc:  0.78125
train loss:  0.5030747652053833
train gradient:  0.16813283652759134
iteration : 7101
train acc:  0.7109375
train loss:  0.5669082403182983
train gradient:  0.2117242684894233
iteration : 7102
train acc:  0.7734375
train loss:  0.49979472160339355
train gradient:  0.16349244667595447
iteration : 7103
train acc:  0.7578125
train loss:  0.48255622386932373
train gradient:  0.11119764441944094
iteration : 7104
train acc:  0.8125
train loss:  0.45272624492645264
train gradient:  0.12440983380419003
iteration : 7105
train acc:  0.7109375
train loss:  0.5819462537765503
train gradient:  0.22125149101599972
iteration : 7106
train acc:  0.6484375
train loss:  0.5332443714141846
train gradient:  0.1456094628635683
iteration : 7107
train acc:  0.7578125
train loss:  0.5384853482246399
train gradient:  0.16198781323702743
iteration : 7108
train acc:  0.671875
train loss:  0.5327070951461792
train gradient:  0.18968603161793968
iteration : 7109
train acc:  0.78125
train loss:  0.451776385307312
train gradient:  0.10403989863013456
iteration : 7110
train acc:  0.7421875
train loss:  0.4633140563964844
train gradient:  0.11680019995337297
iteration : 7111
train acc:  0.7578125
train loss:  0.5238944292068481
train gradient:  0.18387491980047246
iteration : 7112
train acc:  0.7421875
train loss:  0.511368989944458
train gradient:  0.16123707171261648
iteration : 7113
train acc:  0.734375
train loss:  0.5094115138053894
train gradient:  0.13721232830920796
iteration : 7114
train acc:  0.734375
train loss:  0.49890807271003723
train gradient:  0.11879387814344919
iteration : 7115
train acc:  0.71875
train loss:  0.5254088640213013
train gradient:  0.15223875525873043
iteration : 7116
train acc:  0.7734375
train loss:  0.46625953912734985
train gradient:  0.13718117038437116
iteration : 7117
train acc:  0.6953125
train loss:  0.5776448249816895
train gradient:  0.17266611446650132
iteration : 7118
train acc:  0.71875
train loss:  0.47558915615081787
train gradient:  0.15105186165088635
iteration : 7119
train acc:  0.7578125
train loss:  0.46879786252975464
train gradient:  0.11939303553348175
iteration : 7120
train acc:  0.765625
train loss:  0.46562623977661133
train gradient:  0.12331434846664246
iteration : 7121
train acc:  0.6875
train loss:  0.5124068856239319
train gradient:  0.11791842776982923
iteration : 7122
train acc:  0.7578125
train loss:  0.4887109398841858
train gradient:  0.20184847719247329
iteration : 7123
train acc:  0.78125
train loss:  0.49413272738456726
train gradient:  0.15483190485994452
iteration : 7124
train acc:  0.7265625
train loss:  0.5430867671966553
train gradient:  0.16120818536287848
iteration : 7125
train acc:  0.71875
train loss:  0.5069509744644165
train gradient:  0.13983314302894378
iteration : 7126
train acc:  0.796875
train loss:  0.4458514451980591
train gradient:  0.12847571016351056
iteration : 7127
train acc:  0.765625
train loss:  0.5099257230758667
train gradient:  0.12440126193881026
iteration : 7128
train acc:  0.7578125
train loss:  0.5046160221099854
train gradient:  0.12838976630076132
iteration : 7129
train acc:  0.6953125
train loss:  0.5339189767837524
train gradient:  0.15490369610236504
iteration : 7130
train acc:  0.6953125
train loss:  0.5447808504104614
train gradient:  0.2129445534792499
iteration : 7131
train acc:  0.71875
train loss:  0.5277190208435059
train gradient:  0.20546334779715314
iteration : 7132
train acc:  0.7578125
train loss:  0.4971460700035095
train gradient:  0.16399202782123584
iteration : 7133
train acc:  0.7734375
train loss:  0.4470560550689697
train gradient:  0.12003386628884212
iteration : 7134
train acc:  0.796875
train loss:  0.42638954520225525
train gradient:  0.08578172223167659
iteration : 7135
train acc:  0.7109375
train loss:  0.5134142637252808
train gradient:  0.16147523125512717
iteration : 7136
train acc:  0.7421875
train loss:  0.5265645980834961
train gradient:  0.1970382114929365
iteration : 7137
train acc:  0.734375
train loss:  0.5147230625152588
train gradient:  0.15364440268757676
iteration : 7138
train acc:  0.765625
train loss:  0.4618838429450989
train gradient:  0.11773856835795082
iteration : 7139
train acc:  0.7578125
train loss:  0.46469131112098694
train gradient:  0.1252755039579896
iteration : 7140
train acc:  0.7734375
train loss:  0.4892127513885498
train gradient:  0.14631787645150768
iteration : 7141
train acc:  0.7578125
train loss:  0.5053322315216064
train gradient:  0.1573820898585916
iteration : 7142
train acc:  0.703125
train loss:  0.509894847869873
train gradient:  0.12920571673447145
iteration : 7143
train acc:  0.734375
train loss:  0.5105758905410767
train gradient:  0.1491413485527326
iteration : 7144
train acc:  0.734375
train loss:  0.5055620074272156
train gradient:  0.13212275933184436
iteration : 7145
train acc:  0.78125
train loss:  0.4706078767776489
train gradient:  0.1528462913016731
iteration : 7146
train acc:  0.75
train loss:  0.5079789161682129
train gradient:  0.13443125255436583
iteration : 7147
train acc:  0.734375
train loss:  0.47740235924720764
train gradient:  0.12070606006187769
iteration : 7148
train acc:  0.75
train loss:  0.4952339828014374
train gradient:  0.12322971137137491
iteration : 7149
train acc:  0.703125
train loss:  0.5453368425369263
train gradient:  0.15407796113433622
iteration : 7150
train acc:  0.6953125
train loss:  0.5050820112228394
train gradient:  0.21304028370017924
iteration : 7151
train acc:  0.6640625
train loss:  0.5582368969917297
train gradient:  0.17954069085346613
iteration : 7152
train acc:  0.765625
train loss:  0.5252392888069153
train gradient:  0.14359723685755532
iteration : 7153
train acc:  0.78125
train loss:  0.43815726041793823
train gradient:  0.11461142287684759
iteration : 7154
train acc:  0.71875
train loss:  0.5209859609603882
train gradient:  0.1704781223399833
iteration : 7155
train acc:  0.765625
train loss:  0.4389374554157257
train gradient:  0.10657980209390493
iteration : 7156
train acc:  0.71875
train loss:  0.5210756063461304
train gradient:  0.2396121893735801
iteration : 7157
train acc:  0.71875
train loss:  0.5393088459968567
train gradient:  0.14098245454196787
iteration : 7158
train acc:  0.640625
train loss:  0.5560646057128906
train gradient:  0.145347232847025
iteration : 7159
train acc:  0.734375
train loss:  0.5099893808364868
train gradient:  0.15046220811292127
iteration : 7160
train acc:  0.7578125
train loss:  0.4811208248138428
train gradient:  0.12697074604079192
iteration : 7161
train acc:  0.7265625
train loss:  0.49401500821113586
train gradient:  0.13971257013900534
iteration : 7162
train acc:  0.6484375
train loss:  0.5703209638595581
train gradient:  0.15832660639256418
iteration : 7163
train acc:  0.671875
train loss:  0.5110373497009277
train gradient:  0.14339155575902857
iteration : 7164
train acc:  0.6484375
train loss:  0.5666316747665405
train gradient:  0.14086035146668963
iteration : 7165
train acc:  0.7578125
train loss:  0.5001609325408936
train gradient:  0.1340781592339561
iteration : 7166
train acc:  0.734375
train loss:  0.5491585731506348
train gradient:  0.17627031783215175
iteration : 7167
train acc:  0.8046875
train loss:  0.4354521632194519
train gradient:  0.1260702490029031
iteration : 7168
train acc:  0.75
train loss:  0.5093647241592407
train gradient:  0.13345560297649645
iteration : 7169
train acc:  0.7109375
train loss:  0.5022993087768555
train gradient:  0.1300407875948275
iteration : 7170
train acc:  0.8046875
train loss:  0.47794514894485474
train gradient:  0.1203872003995995
iteration : 7171
train acc:  0.7109375
train loss:  0.5343631505966187
train gradient:  0.23740226824321797
iteration : 7172
train acc:  0.828125
train loss:  0.44943562150001526
train gradient:  0.15030803823447925
iteration : 7173
train acc:  0.78125
train loss:  0.4670957326889038
train gradient:  0.12444318145634073
iteration : 7174
train acc:  0.8125
train loss:  0.4447033405303955
train gradient:  0.11462358948477207
iteration : 7175
train acc:  0.7578125
train loss:  0.4904018044471741
train gradient:  0.13962716569376873
iteration : 7176
train acc:  0.765625
train loss:  0.47757554054260254
train gradient:  0.12787216423971798
iteration : 7177
train acc:  0.7734375
train loss:  0.4859880208969116
train gradient:  0.12164748426909093
iteration : 7178
train acc:  0.7421875
train loss:  0.5003771781921387
train gradient:  0.14019552335231533
iteration : 7179
train acc:  0.7421875
train loss:  0.4983007311820984
train gradient:  0.14335628853696375
iteration : 7180
train acc:  0.8046875
train loss:  0.41509321331977844
train gradient:  0.10429341560236179
iteration : 7181
train acc:  0.7734375
train loss:  0.4908311069011688
train gradient:  0.10879232002521802
iteration : 7182
train acc:  0.7421875
train loss:  0.4974370002746582
train gradient:  0.20498001936237834
iteration : 7183
train acc:  0.7109375
train loss:  0.5691016316413879
train gradient:  0.1494829377241037
iteration : 7184
train acc:  0.7578125
train loss:  0.45568883419036865
train gradient:  0.16475385887677388
iteration : 7185
train acc:  0.8046875
train loss:  0.45819365978240967
train gradient:  0.13123210428461607
iteration : 7186
train acc:  0.75
train loss:  0.5260180234909058
train gradient:  0.14267712129501203
iteration : 7187
train acc:  0.75
train loss:  0.45051872730255127
train gradient:  0.118414352251983
iteration : 7188
train acc:  0.6796875
train loss:  0.5654548406600952
train gradient:  0.16487700396261476
iteration : 7189
train acc:  0.8203125
train loss:  0.44137120246887207
train gradient:  0.11615638880751653
iteration : 7190
train acc:  0.6875
train loss:  0.5169987678527832
train gradient:  0.1185562722411022
iteration : 7191
train acc:  0.734375
train loss:  0.48560619354248047
train gradient:  0.10734142821819369
iteration : 7192
train acc:  0.7734375
train loss:  0.5157396793365479
train gradient:  0.13901226619443652
iteration : 7193
train acc:  0.8203125
train loss:  0.47103551030158997
train gradient:  0.11349813509679288
iteration : 7194
train acc:  0.78125
train loss:  0.49002838134765625
train gradient:  0.1417536128383189
iteration : 7195
train acc:  0.7578125
train loss:  0.4905588924884796
train gradient:  0.13855839302777867
iteration : 7196
train acc:  0.7734375
train loss:  0.5010423064231873
train gradient:  0.16709438320137715
iteration : 7197
train acc:  0.78125
train loss:  0.44636043906211853
train gradient:  0.10725510999690835
iteration : 7198
train acc:  0.703125
train loss:  0.5220192670822144
train gradient:  0.1742323797239136
iteration : 7199
train acc:  0.71875
train loss:  0.5140978693962097
train gradient:  0.1684351900438385
iteration : 7200
train acc:  0.75
train loss:  0.4809068739414215
train gradient:  0.14630715217347473
iteration : 7201
train acc:  0.7578125
train loss:  0.5111086368560791
train gradient:  0.15372162924685595
iteration : 7202
train acc:  0.7421875
train loss:  0.48894402384757996
train gradient:  0.15886825564862128
iteration : 7203
train acc:  0.828125
train loss:  0.4479983150959015
train gradient:  0.19868400130783964
iteration : 7204
train acc:  0.7734375
train loss:  0.4427502751350403
train gradient:  0.13642301527995965
iteration : 7205
train acc:  0.703125
train loss:  0.5197280049324036
train gradient:  0.13517245645112136
iteration : 7206
train acc:  0.796875
train loss:  0.48630034923553467
train gradient:  0.1148290085883493
iteration : 7207
train acc:  0.7421875
train loss:  0.5386009216308594
train gradient:  0.1690261063389651
iteration : 7208
train acc:  0.71875
train loss:  0.5163127183914185
train gradient:  0.19162482309000778
iteration : 7209
train acc:  0.7734375
train loss:  0.46144962310791016
train gradient:  0.1454921455285048
iteration : 7210
train acc:  0.765625
train loss:  0.5090819597244263
train gradient:  0.14336690332073362
iteration : 7211
train acc:  0.7109375
train loss:  0.5135407447814941
train gradient:  0.1435161451885952
iteration : 7212
train acc:  0.703125
train loss:  0.5280179381370544
train gradient:  0.15755729708705882
iteration : 7213
train acc:  0.6640625
train loss:  0.5137996673583984
train gradient:  0.15458835072820787
iteration : 7214
train acc:  0.734375
train loss:  0.5330925583839417
train gradient:  0.14264272051582536
iteration : 7215
train acc:  0.7265625
train loss:  0.5402318239212036
train gradient:  0.14370711634455954
iteration : 7216
train acc:  0.7265625
train loss:  0.5227624773979187
train gradient:  0.19922777780422313
iteration : 7217
train acc:  0.6875
train loss:  0.4957456588745117
train gradient:  0.13896408014777129
iteration : 7218
train acc:  0.7578125
train loss:  0.5409442782402039
train gradient:  0.17266935724593296
iteration : 7219
train acc:  0.734375
train loss:  0.5602445602416992
train gradient:  0.18655191642718436
iteration : 7220
train acc:  0.75
train loss:  0.4607250988483429
train gradient:  0.11801819097698128
iteration : 7221
train acc:  0.796875
train loss:  0.4769246578216553
train gradient:  0.16398367151763957
iteration : 7222
train acc:  0.8125
train loss:  0.4394958019256592
train gradient:  0.1370733978450805
iteration : 7223
train acc:  0.8125
train loss:  0.4246225953102112
train gradient:  0.12628839803869338
iteration : 7224
train acc:  0.71875
train loss:  0.47068819403648376
train gradient:  0.13988532799799755
iteration : 7225
train acc:  0.7578125
train loss:  0.4710129201412201
train gradient:  0.14215537617891044
iteration : 7226
train acc:  0.8046875
train loss:  0.4433143436908722
train gradient:  0.1345668181605236
iteration : 7227
train acc:  0.7890625
train loss:  0.43230143189430237
train gradient:  0.12719142666945243
iteration : 7228
train acc:  0.6953125
train loss:  0.5925387144088745
train gradient:  0.166450784539044
iteration : 7229
train acc:  0.7421875
train loss:  0.5081986784934998
train gradient:  0.1384037875519586
iteration : 7230
train acc:  0.6953125
train loss:  0.5445547699928284
train gradient:  0.19806709916083715
iteration : 7231
train acc:  0.6484375
train loss:  0.6111310720443726
train gradient:  0.24474683361690208
iteration : 7232
train acc:  0.7109375
train loss:  0.5342534780502319
train gradient:  0.15614024255019296
iteration : 7233
train acc:  0.6796875
train loss:  0.5030043125152588
train gradient:  0.1558024230818088
iteration : 7234
train acc:  0.8046875
train loss:  0.4506312906742096
train gradient:  0.10462705336809046
iteration : 7235
train acc:  0.703125
train loss:  0.5934330821037292
train gradient:  0.19834010460433502
iteration : 7236
train acc:  0.7109375
train loss:  0.45773839950561523
train gradient:  0.12380255764054987
iteration : 7237
train acc:  0.703125
train loss:  0.5157172679901123
train gradient:  0.13376579208952782
iteration : 7238
train acc:  0.6875
train loss:  0.4934016466140747
train gradient:  0.1510945510676651
iteration : 7239
train acc:  0.7578125
train loss:  0.4943472146987915
train gradient:  0.13699835160900514
iteration : 7240
train acc:  0.734375
train loss:  0.5100641846656799
train gradient:  0.18369471701876564
iteration : 7241
train acc:  0.7890625
train loss:  0.4634909927845001
train gradient:  0.12765500059409546
iteration : 7242
train acc:  0.6953125
train loss:  0.5303714275360107
train gradient:  0.13604513176051253
iteration : 7243
train acc:  0.71875
train loss:  0.529836893081665
train gradient:  0.16113751721960257
iteration : 7244
train acc:  0.7109375
train loss:  0.5103514194488525
train gradient:  0.1504017692289259
iteration : 7245
train acc:  0.7578125
train loss:  0.4248761236667633
train gradient:  0.13730277769258906
iteration : 7246
train acc:  0.7265625
train loss:  0.5406314134597778
train gradient:  0.21921399381651993
iteration : 7247
train acc:  0.765625
train loss:  0.49400660395622253
train gradient:  0.12602742720648355
iteration : 7248
train acc:  0.7109375
train loss:  0.5413828492164612
train gradient:  0.16235229605371393
iteration : 7249
train acc:  0.6796875
train loss:  0.5547233819961548
train gradient:  0.1307834825741264
iteration : 7250
train acc:  0.7265625
train loss:  0.5202723741531372
train gradient:  0.15941374328136854
iteration : 7251
train acc:  0.7578125
train loss:  0.45755696296691895
train gradient:  0.12524030845915693
iteration : 7252
train acc:  0.8125
train loss:  0.44433221220970154
train gradient:  0.11239865497344655
iteration : 7253
train acc:  0.765625
train loss:  0.4963177442550659
train gradient:  0.14928344364666415
iteration : 7254
train acc:  0.71875
train loss:  0.535671055316925
train gradient:  0.18379378182727432
iteration : 7255
train acc:  0.7578125
train loss:  0.4796435832977295
train gradient:  0.1385128954478784
iteration : 7256
train acc:  0.7578125
train loss:  0.4459533095359802
train gradient:  0.11611739369725087
iteration : 7257
train acc:  0.78125
train loss:  0.4482418894767761
train gradient:  0.19607470065151344
iteration : 7258
train acc:  0.6953125
train loss:  0.5338329076766968
train gradient:  0.15418957867808863
iteration : 7259
train acc:  0.7421875
train loss:  0.5227199196815491
train gradient:  0.1360395074543657
iteration : 7260
train acc:  0.7265625
train loss:  0.4958283305168152
train gradient:  0.161804030103385
iteration : 7261
train acc:  0.7109375
train loss:  0.5043004155158997
train gradient:  0.13679127528138652
iteration : 7262
train acc:  0.6796875
train loss:  0.5717204213142395
train gradient:  0.19938597180580414
iteration : 7263
train acc:  0.75
train loss:  0.4988349676132202
train gradient:  0.11423740580016624
iteration : 7264
train acc:  0.7890625
train loss:  0.4887002110481262
train gradient:  0.14379901344071305
iteration : 7265
train acc:  0.734375
train loss:  0.5690812468528748
train gradient:  0.18102591225375844
iteration : 7266
train acc:  0.7265625
train loss:  0.5150862336158752
train gradient:  0.12602569676092423
iteration : 7267
train acc:  0.7109375
train loss:  0.49015066027641296
train gradient:  0.15328776384430637
iteration : 7268
train acc:  0.765625
train loss:  0.43083634972572327
train gradient:  0.11563452230291735
iteration : 7269
train acc:  0.703125
train loss:  0.5083225965499878
train gradient:  0.15390500517763667
iteration : 7270
train acc:  0.7265625
train loss:  0.4892142415046692
train gradient:  0.13795025342139525
iteration : 7271
train acc:  0.71875
train loss:  0.4879845678806305
train gradient:  0.1210635115452025
iteration : 7272
train acc:  0.7578125
train loss:  0.4873028099536896
train gradient:  0.1688474411484829
iteration : 7273
train acc:  0.703125
train loss:  0.48046618700027466
train gradient:  0.15428207384965287
iteration : 7274
train acc:  0.734375
train loss:  0.5072202682495117
train gradient:  0.15239224813012164
iteration : 7275
train acc:  0.7421875
train loss:  0.45607227087020874
train gradient:  0.12446290707902503
iteration : 7276
train acc:  0.7734375
train loss:  0.4778243899345398
train gradient:  0.1384128939520758
iteration : 7277
train acc:  0.7421875
train loss:  0.4597584009170532
train gradient:  0.1052255948548462
iteration : 7278
train acc:  0.7265625
train loss:  0.5040540099143982
train gradient:  0.18479278848442715
iteration : 7279
train acc:  0.8125
train loss:  0.44798755645751953
train gradient:  0.12541118810103813
iteration : 7280
train acc:  0.75
train loss:  0.5265862941741943
train gradient:  0.1500311822766794
iteration : 7281
train acc:  0.703125
train loss:  0.5271046161651611
train gradient:  0.15855330720893174
iteration : 7282
train acc:  0.7578125
train loss:  0.45541468262672424
train gradient:  0.10627535692033398
iteration : 7283
train acc:  0.6796875
train loss:  0.5679291486740112
train gradient:  0.19508022973834732
iteration : 7284
train acc:  0.75
train loss:  0.47149309515953064
train gradient:  0.13461331080955652
iteration : 7285
train acc:  0.7265625
train loss:  0.5547581315040588
train gradient:  0.17363693346883974
iteration : 7286
train acc:  0.6796875
train loss:  0.5377058386802673
train gradient:  0.16726873730220182
iteration : 7287
train acc:  0.7109375
train loss:  0.5192725658416748
train gradient:  0.1606014159484131
iteration : 7288
train acc:  0.75
train loss:  0.5802241563796997
train gradient:  0.16457314134898027
iteration : 7289
train acc:  0.7421875
train loss:  0.4849478006362915
train gradient:  0.11873314570282363
iteration : 7290
train acc:  0.6953125
train loss:  0.5196655988693237
train gradient:  0.14983915239082968
iteration : 7291
train acc:  0.734375
train loss:  0.5157732963562012
train gradient:  0.16966392926820134
iteration : 7292
train acc:  0.703125
train loss:  0.4984304904937744
train gradient:  0.13946110691195343
iteration : 7293
train acc:  0.7578125
train loss:  0.5120118856430054
train gradient:  0.14711048892308856
iteration : 7294
train acc:  0.7890625
train loss:  0.4242415428161621
train gradient:  0.09643361812442548
iteration : 7295
train acc:  0.6953125
train loss:  0.5347017049789429
train gradient:  0.14235306200771275
iteration : 7296
train acc:  0.78125
train loss:  0.5158007740974426
train gradient:  0.13652357598064924
iteration : 7297
train acc:  0.6640625
train loss:  0.5706073045730591
train gradient:  0.15311679691159788
iteration : 7298
train acc:  0.7109375
train loss:  0.5233960151672363
train gradient:  0.14879783243606054
iteration : 7299
train acc:  0.765625
train loss:  0.5054876804351807
train gradient:  0.17036530949364276
iteration : 7300
train acc:  0.7890625
train loss:  0.4444003105163574
train gradient:  0.12933544478613654
iteration : 7301
train acc:  0.71875
train loss:  0.5299298763275146
train gradient:  0.15114114480998797
iteration : 7302
train acc:  0.6953125
train loss:  0.5638678073883057
train gradient:  0.19784672902049005
iteration : 7303
train acc:  0.75
train loss:  0.4428613781929016
train gradient:  0.12869225334260126
iteration : 7304
train acc:  0.734375
train loss:  0.5074220895767212
train gradient:  0.13218023297395293
iteration : 7305
train acc:  0.765625
train loss:  0.4477638900279999
train gradient:  0.11676746131993553
iteration : 7306
train acc:  0.7265625
train loss:  0.4933502674102783
train gradient:  0.11629663840517128
iteration : 7307
train acc:  0.75
train loss:  0.5160245299339294
train gradient:  0.2711083702082824
iteration : 7308
train acc:  0.7890625
train loss:  0.44910958409309387
train gradient:  0.11989536254682093
iteration : 7309
train acc:  0.765625
train loss:  0.4646924138069153
train gradient:  0.14475298236559006
iteration : 7310
train acc:  0.796875
train loss:  0.4380079507827759
train gradient:  0.08462888070304331
iteration : 7311
train acc:  0.6953125
train loss:  0.5261902213096619
train gradient:  0.16417368468413818
iteration : 7312
train acc:  0.7578125
train loss:  0.4727356731891632
train gradient:  0.14813759959386402
iteration : 7313
train acc:  0.8203125
train loss:  0.3998250961303711
train gradient:  0.0943834379658408
iteration : 7314
train acc:  0.78125
train loss:  0.45249781012535095
train gradient:  0.13253480449772387
iteration : 7315
train acc:  0.6328125
train loss:  0.5438612699508667
train gradient:  0.1674624146278152
iteration : 7316
train acc:  0.765625
train loss:  0.47624343633651733
train gradient:  0.11734489984220628
iteration : 7317
train acc:  0.6953125
train loss:  0.5636346340179443
train gradient:  0.22886246752812447
iteration : 7318
train acc:  0.796875
train loss:  0.4390275478363037
train gradient:  0.09417501185227699
iteration : 7319
train acc:  0.765625
train loss:  0.5184948444366455
train gradient:  0.13792468399318025
iteration : 7320
train acc:  0.75
train loss:  0.4597206711769104
train gradient:  0.13425570322939084
iteration : 7321
train acc:  0.7734375
train loss:  0.48057490587234497
train gradient:  0.14996738503351065
iteration : 7322
train acc:  0.75
train loss:  0.4885011613368988
train gradient:  0.1603284851817014
iteration : 7323
train acc:  0.671875
train loss:  0.5794678926467896
train gradient:  0.15356508707114502
iteration : 7324
train acc:  0.7578125
train loss:  0.45523422956466675
train gradient:  0.10884423465894684
iteration : 7325
train acc:  0.765625
train loss:  0.49829164147377014
train gradient:  0.14545641938522638
iteration : 7326
train acc:  0.703125
train loss:  0.5114121437072754
train gradient:  0.12840535437955214
iteration : 7327
train acc:  0.703125
train loss:  0.5385628938674927
train gradient:  0.16335471994393758
iteration : 7328
train acc:  0.6875
train loss:  0.5622216463088989
train gradient:  0.16600889817021786
iteration : 7329
train acc:  0.7578125
train loss:  0.5008818507194519
train gradient:  0.15119711195750365
iteration : 7330
train acc:  0.7421875
train loss:  0.5016096234321594
train gradient:  0.13558718165804334
iteration : 7331
train acc:  0.7578125
train loss:  0.4541953504085541
train gradient:  0.12023009102075018
iteration : 7332
train acc:  0.7109375
train loss:  0.5143928527832031
train gradient:  0.1950170313747449
iteration : 7333
train acc:  0.78125
train loss:  0.473848819732666
train gradient:  0.1302278967984795
iteration : 7334
train acc:  0.7265625
train loss:  0.5299739241600037
train gradient:  0.19902688507815358
iteration : 7335
train acc:  0.6640625
train loss:  0.5877951383590698
train gradient:  0.16013177688379865
iteration : 7336
train acc:  0.7109375
train loss:  0.5221596956253052
train gradient:  0.2223685309502817
iteration : 7337
train acc:  0.6953125
train loss:  0.5425282120704651
train gradient:  0.16944508820813545
iteration : 7338
train acc:  0.7890625
train loss:  0.493914395570755
train gradient:  0.16737569291221766
iteration : 7339
train acc:  0.703125
train loss:  0.5522845387458801
train gradient:  0.18363673133017822
iteration : 7340
train acc:  0.7265625
train loss:  0.49585047364234924
train gradient:  0.12818837685466397
iteration : 7341
train acc:  0.78125
train loss:  0.4617769718170166
train gradient:  0.1192538634963803
iteration : 7342
train acc:  0.7734375
train loss:  0.5239675641059875
train gradient:  0.16758359659567548
iteration : 7343
train acc:  0.7734375
train loss:  0.4493328630924225
train gradient:  0.11825343514201055
iteration : 7344
train acc:  0.7890625
train loss:  0.46771568059921265
train gradient:  0.12178615874479144
iteration : 7345
train acc:  0.7421875
train loss:  0.4890584945678711
train gradient:  0.10754465936325153
iteration : 7346
train acc:  0.765625
train loss:  0.5131239891052246
train gradient:  0.1504362846201001
iteration : 7347
train acc:  0.6953125
train loss:  0.5748721957206726
train gradient:  0.24470413675645097
iteration : 7348
train acc:  0.75
train loss:  0.47972673177719116
train gradient:  0.11475742408227373
iteration : 7349
train acc:  0.8046875
train loss:  0.43360328674316406
train gradient:  0.14910954372235546
iteration : 7350
train acc:  0.703125
train loss:  0.533683717250824
train gradient:  0.22659462733650576
iteration : 7351
train acc:  0.703125
train loss:  0.549270510673523
train gradient:  0.21665202963487265
iteration : 7352
train acc:  0.7734375
train loss:  0.49557554721832275
train gradient:  0.145906495117738
iteration : 7353
train acc:  0.6953125
train loss:  0.5666993260383606
train gradient:  0.19380346964573497
iteration : 7354
train acc:  0.78125
train loss:  0.4672689437866211
train gradient:  0.1260596774007873
iteration : 7355
train acc:  0.6953125
train loss:  0.5574529767036438
train gradient:  0.14987746596032642
iteration : 7356
train acc:  0.7265625
train loss:  0.5684391260147095
train gradient:  0.1825420861948552
iteration : 7357
train acc:  0.6484375
train loss:  0.5796673893928528
train gradient:  0.16693350967352655
iteration : 7358
train acc:  0.6640625
train loss:  0.5283910036087036
train gradient:  0.14673342858507424
iteration : 7359
train acc:  0.7421875
train loss:  0.5329443216323853
train gradient:  0.186708262491112
iteration : 7360
train acc:  0.7109375
train loss:  0.49860209226608276
train gradient:  0.16598478342705303
iteration : 7361
train acc:  0.71875
train loss:  0.4959757924079895
train gradient:  0.1899488805808487
iteration : 7362
train acc:  0.6796875
train loss:  0.5507420301437378
train gradient:  0.16184215283666137
iteration : 7363
train acc:  0.71875
train loss:  0.4893246591091156
train gradient:  0.1555326716628896
iteration : 7364
train acc:  0.75
train loss:  0.48111748695373535
train gradient:  0.15232258071624888
iteration : 7365
train acc:  0.6484375
train loss:  0.604570746421814
train gradient:  0.19271128350622946
iteration : 7366
train acc:  0.6953125
train loss:  0.6164720058441162
train gradient:  0.1838218538174255
iteration : 7367
train acc:  0.71875
train loss:  0.528846800327301
train gradient:  0.12785993161607925
iteration : 7368
train acc:  0.671875
train loss:  0.6112064123153687
train gradient:  0.21228154640401634
iteration : 7369
train acc:  0.625
train loss:  0.6293957829475403
train gradient:  0.19799420378788807
iteration : 7370
train acc:  0.7265625
train loss:  0.482448011636734
train gradient:  0.11666575307637692
iteration : 7371
train acc:  0.7265625
train loss:  0.5045322179794312
train gradient:  0.1686216009105384
iteration : 7372
train acc:  0.7578125
train loss:  0.4701109528541565
train gradient:  0.12851902763612646
iteration : 7373
train acc:  0.71875
train loss:  0.6059321761131287
train gradient:  0.17692388896135303
iteration : 7374
train acc:  0.6796875
train loss:  0.5721453428268433
train gradient:  0.19181567568287178
iteration : 7375
train acc:  0.7421875
train loss:  0.45409247279167175
train gradient:  0.1294217689288087
iteration : 7376
train acc:  0.734375
train loss:  0.4832354784011841
train gradient:  0.1430415393379803
iteration : 7377
train acc:  0.765625
train loss:  0.5078263878822327
train gradient:  0.1384962814748049
iteration : 7378
train acc:  0.734375
train loss:  0.5205873250961304
train gradient:  0.16895452068504402
iteration : 7379
train acc:  0.7890625
train loss:  0.4318111538887024
train gradient:  0.10799808241477049
iteration : 7380
train acc:  0.734375
train loss:  0.5632956027984619
train gradient:  0.17766880657878203
iteration : 7381
train acc:  0.796875
train loss:  0.48986154794692993
train gradient:  0.1367964174619718
iteration : 7382
train acc:  0.7578125
train loss:  0.5253714919090271
train gradient:  0.1458614260045515
iteration : 7383
train acc:  0.640625
train loss:  0.6149286031723022
train gradient:  0.16220804287014828
iteration : 7384
train acc:  0.78125
train loss:  0.4795745313167572
train gradient:  0.10483264200679641
iteration : 7385
train acc:  0.8046875
train loss:  0.39836740493774414
train gradient:  0.11509047706526238
iteration : 7386
train acc:  0.7265625
train loss:  0.5485095977783203
train gradient:  0.18217948107403714
iteration : 7387
train acc:  0.734375
train loss:  0.5653564929962158
train gradient:  0.14529311176866827
iteration : 7388
train acc:  0.765625
train loss:  0.4662109315395355
train gradient:  0.15591593539362408
iteration : 7389
train acc:  0.7109375
train loss:  0.5333213806152344
train gradient:  0.1577536953147049
iteration : 7390
train acc:  0.7734375
train loss:  0.4674503803253174
train gradient:  0.13924941402993357
iteration : 7391
train acc:  0.8125
train loss:  0.4180126190185547
train gradient:  0.11009307579720942
iteration : 7392
train acc:  0.65625
train loss:  0.5766273736953735
train gradient:  0.18592691693013783
iteration : 7393
train acc:  0.7578125
train loss:  0.45992106199264526
train gradient:  0.16248989180392823
iteration : 7394
train acc:  0.765625
train loss:  0.4588654637336731
train gradient:  0.11973490195220267
iteration : 7395
train acc:  0.734375
train loss:  0.4812941253185272
train gradient:  0.14249341621928058
iteration : 7396
train acc:  0.703125
train loss:  0.5314253568649292
train gradient:  0.11916833070660914
iteration : 7397
train acc:  0.7421875
train loss:  0.5456709861755371
train gradient:  0.15356606659711516
iteration : 7398
train acc:  0.7265625
train loss:  0.5040810704231262
train gradient:  0.11873384938654162
iteration : 7399
train acc:  0.7109375
train loss:  0.5342260599136353
train gradient:  0.16231840968887423
iteration : 7400
train acc:  0.6796875
train loss:  0.5622932314872742
train gradient:  0.18541807053787474
iteration : 7401
train acc:  0.75
train loss:  0.4711266756057739
train gradient:  0.11384319423369281
iteration : 7402
train acc:  0.734375
train loss:  0.4971684217453003
train gradient:  0.16506683008680417
iteration : 7403
train acc:  0.765625
train loss:  0.48920854926109314
train gradient:  0.1344704921143198
iteration : 7404
train acc:  0.7265625
train loss:  0.4774198532104492
train gradient:  0.13946379860413327
iteration : 7405
train acc:  0.8046875
train loss:  0.4408777952194214
train gradient:  0.09922380440645386
iteration : 7406
train acc:  0.8359375
train loss:  0.4502839148044586
train gradient:  0.1250545085843709
iteration : 7407
train acc:  0.734375
train loss:  0.49586236476898193
train gradient:  0.13513675997797853
iteration : 7408
train acc:  0.6796875
train loss:  0.5461093187332153
train gradient:  0.1648487746627024
iteration : 7409
train acc:  0.6953125
train loss:  0.5423150062561035
train gradient:  0.15030015311604694
iteration : 7410
train acc:  0.734375
train loss:  0.46831226348876953
train gradient:  0.11482678497678063
iteration : 7411
train acc:  0.6875
train loss:  0.5720926523208618
train gradient:  0.1996077559577838
iteration : 7412
train acc:  0.7109375
train loss:  0.5325942635536194
train gradient:  0.14308921634255428
iteration : 7413
train acc:  0.6875
train loss:  0.548567533493042
train gradient:  0.1378515096928212
iteration : 7414
train acc:  0.7265625
train loss:  0.5109983682632446
train gradient:  0.12553788671070332
iteration : 7415
train acc:  0.703125
train loss:  0.5345088839530945
train gradient:  0.1390752788230778
iteration : 7416
train acc:  0.765625
train loss:  0.5270833373069763
train gradient:  0.13720191270995952
iteration : 7417
train acc:  0.75
train loss:  0.4913434386253357
train gradient:  0.13098586325302755
iteration : 7418
train acc:  0.6484375
train loss:  0.6194542646408081
train gradient:  0.20730007273225814
iteration : 7419
train acc:  0.7890625
train loss:  0.4921048879623413
train gradient:  0.18980785634635355
iteration : 7420
train acc:  0.71875
train loss:  0.49828967452049255
train gradient:  0.1388315404430699
iteration : 7421
train acc:  0.75
train loss:  0.47112762928009033
train gradient:  0.13742699199702985
iteration : 7422
train acc:  0.7109375
train loss:  0.5089139938354492
train gradient:  0.16057143645420502
iteration : 7423
train acc:  0.7421875
train loss:  0.5262681245803833
train gradient:  0.15571748446836328
iteration : 7424
train acc:  0.703125
train loss:  0.5250223875045776
train gradient:  0.15230872800893724
iteration : 7425
train acc:  0.7109375
train loss:  0.5339466333389282
train gradient:  0.14031593129525824
iteration : 7426
train acc:  0.8203125
train loss:  0.4283277690410614
train gradient:  0.10094710365823685
iteration : 7427
train acc:  0.765625
train loss:  0.4864385426044464
train gradient:  0.12282827150665276
iteration : 7428
train acc:  0.75
train loss:  0.4360370635986328
train gradient:  0.13099941815019517
iteration : 7429
train acc:  0.78125
train loss:  0.49281325936317444
train gradient:  0.12079790491422089
iteration : 7430
train acc:  0.7734375
train loss:  0.4897604286670685
train gradient:  0.13890542059594366
iteration : 7431
train acc:  0.7734375
train loss:  0.4733668565750122
train gradient:  0.12283222685771059
iteration : 7432
train acc:  0.7265625
train loss:  0.5374412536621094
train gradient:  0.12961612211862011
iteration : 7433
train acc:  0.7578125
train loss:  0.49657002091407776
train gradient:  0.15551565762164576
iteration : 7434
train acc:  0.7734375
train loss:  0.4743223190307617
train gradient:  0.11526950172899765
iteration : 7435
train acc:  0.671875
train loss:  0.5557354688644409
train gradient:  0.19110615080517634
iteration : 7436
train acc:  0.765625
train loss:  0.46881794929504395
train gradient:  0.10971500164898611
iteration : 7437
train acc:  0.7421875
train loss:  0.5262329578399658
train gradient:  0.13107015197001998
iteration : 7438
train acc:  0.75
train loss:  0.46897074580192566
train gradient:  0.11093187307480373
iteration : 7439
train acc:  0.6796875
train loss:  0.5710158348083496
train gradient:  0.19304476871298382
iteration : 7440
train acc:  0.75
train loss:  0.49412283301353455
train gradient:  0.12699389964025454
iteration : 7441
train acc:  0.7109375
train loss:  0.5580483078956604
train gradient:  0.17335859917996288
iteration : 7442
train acc:  0.734375
train loss:  0.5245838165283203
train gradient:  0.13322214732064036
iteration : 7443
train acc:  0.7109375
train loss:  0.5498071908950806
train gradient:  0.1915816671023254
iteration : 7444
train acc:  0.78125
train loss:  0.46297675371170044
train gradient:  0.11661542035180068
iteration : 7445
train acc:  0.765625
train loss:  0.5489233136177063
train gradient:  0.17816718962320466
iteration : 7446
train acc:  0.71875
train loss:  0.5154199600219727
train gradient:  0.14030524134502426
iteration : 7447
train acc:  0.6875
train loss:  0.5817552804946899
train gradient:  0.1732968823877814
iteration : 7448
train acc:  0.7578125
train loss:  0.4466394782066345
train gradient:  0.11601949733656129
iteration : 7449
train acc:  0.7890625
train loss:  0.44701087474823
train gradient:  0.11096057760568902
iteration : 7450
train acc:  0.7734375
train loss:  0.4568343162536621
train gradient:  0.10865312836854897
iteration : 7451
train acc:  0.71875
train loss:  0.5194029211997986
train gradient:  0.14370572571219492
iteration : 7452
train acc:  0.75
train loss:  0.5171639919281006
train gradient:  0.19081359680245671
iteration : 7453
train acc:  0.7109375
train loss:  0.5532087683677673
train gradient:  0.13481670908175053
iteration : 7454
train acc:  0.7734375
train loss:  0.4614138901233673
train gradient:  0.1502164344552344
iteration : 7455
train acc:  0.7421875
train loss:  0.5274121761322021
train gradient:  0.15595298326765744
iteration : 7456
train acc:  0.7734375
train loss:  0.4828145205974579
train gradient:  0.15401626756984768
iteration : 7457
train acc:  0.765625
train loss:  0.47606611251831055
train gradient:  0.12819615780452373
iteration : 7458
train acc:  0.7109375
train loss:  0.47707730531692505
train gradient:  0.12287171553888712
iteration : 7459
train acc:  0.71875
train loss:  0.4881497025489807
train gradient:  0.1435325099017994
iteration : 7460
train acc:  0.7109375
train loss:  0.591046154499054
train gradient:  0.1610529855699604
iteration : 7461
train acc:  0.75
train loss:  0.47022709250450134
train gradient:  0.10730532430002553
iteration : 7462
train acc:  0.7734375
train loss:  0.4855315089225769
train gradient:  0.12097698998434578
iteration : 7463
train acc:  0.703125
train loss:  0.5438568592071533
train gradient:  0.1831060457211341
iteration : 7464
train acc:  0.7890625
train loss:  0.4972746670246124
train gradient:  0.12225453062950511
iteration : 7465
train acc:  0.6875
train loss:  0.5991402864456177
train gradient:  0.15773676189509994
iteration : 7466
train acc:  0.7421875
train loss:  0.4681004285812378
train gradient:  0.12247786108415626
iteration : 7467
train acc:  0.75
train loss:  0.5274044275283813
train gradient:  0.12165837909702264
iteration : 7468
train acc:  0.7578125
train loss:  0.4607850909233093
train gradient:  0.12188695489606925
iteration : 7469
train acc:  0.6953125
train loss:  0.5346146821975708
train gradient:  0.15586273920744864
iteration : 7470
train acc:  0.6796875
train loss:  0.5449934005737305
train gradient:  0.17400812723409198
iteration : 7471
train acc:  0.84375
train loss:  0.3966638445854187
train gradient:  0.11368222071391569
iteration : 7472
train acc:  0.6484375
train loss:  0.5711846351623535
train gradient:  0.13169918078091086
iteration : 7473
train acc:  0.734375
train loss:  0.5091590881347656
train gradient:  0.15178101617981782
iteration : 7474
train acc:  0.6875
train loss:  0.5722775459289551
train gradient:  0.16286366124978263
iteration : 7475
train acc:  0.75
train loss:  0.48329704999923706
train gradient:  0.1198692783962269
iteration : 7476
train acc:  0.7890625
train loss:  0.4680215120315552
train gradient:  0.10956223208618056
iteration : 7477
train acc:  0.6953125
train loss:  0.5578659176826477
train gradient:  0.1516244868614314
iteration : 7478
train acc:  0.6953125
train loss:  0.5355271697044373
train gradient:  0.1521847316832507
iteration : 7479
train acc:  0.7265625
train loss:  0.5849541425704956
train gradient:  0.22412441084955947
iteration : 7480
train acc:  0.6875
train loss:  0.5744332075119019
train gradient:  0.13522752574775443
iteration : 7481
train acc:  0.71875
train loss:  0.5150934457778931
train gradient:  0.13578795730367638
iteration : 7482
train acc:  0.703125
train loss:  0.5048331022262573
train gradient:  0.1360730565043692
iteration : 7483
train acc:  0.75
train loss:  0.4848547577857971
train gradient:  0.11116923048454276
iteration : 7484
train acc:  0.75
train loss:  0.4907706379890442
train gradient:  0.12649744026135862
iteration : 7485
train acc:  0.75
train loss:  0.4691728949546814
train gradient:  0.13829565793410392
iteration : 7486
train acc:  0.6796875
train loss:  0.5750336647033691
train gradient:  0.1853992959063398
iteration : 7487
train acc:  0.6484375
train loss:  0.5817093849182129
train gradient:  0.1548830622422351
iteration : 7488
train acc:  0.7109375
train loss:  0.5273479223251343
train gradient:  0.14714082251450594
iteration : 7489
train acc:  0.7578125
train loss:  0.47091495990753174
train gradient:  0.10295128086951617
iteration : 7490
train acc:  0.7578125
train loss:  0.5307295322418213
train gradient:  0.16609280799470466
iteration : 7491
train acc:  0.7890625
train loss:  0.45451778173446655
train gradient:  0.11812641257432926
iteration : 7492
train acc:  0.71875
train loss:  0.47688597440719604
train gradient:  0.10580363698833192
iteration : 7493
train acc:  0.75
train loss:  0.4537666440010071
train gradient:  0.08745948834935387
iteration : 7494
train acc:  0.7265625
train loss:  0.4976770877838135
train gradient:  0.12466783111623382
iteration : 7495
train acc:  0.6953125
train loss:  0.5232922434806824
train gradient:  0.1408121994094464
iteration : 7496
train acc:  0.7109375
train loss:  0.525780975818634
train gradient:  0.17566351688632753
iteration : 7497
train acc:  0.765625
train loss:  0.5323362350463867
train gradient:  0.1451279322608062
iteration : 7498
train acc:  0.7421875
train loss:  0.4884219169616699
train gradient:  0.12304007349069047
iteration : 7499
train acc:  0.7421875
train loss:  0.5060856342315674
train gradient:  0.14804570336734474
iteration : 7500
train acc:  0.796875
train loss:  0.5001109838485718
train gradient:  0.13601520612506104
iteration : 7501
train acc:  0.7421875
train loss:  0.53074049949646
train gradient:  0.13854794973318746
iteration : 7502
train acc:  0.78125
train loss:  0.46560513973236084
train gradient:  0.1202156827777243
iteration : 7503
train acc:  0.765625
train loss:  0.485096275806427
train gradient:  0.12735458274418576
iteration : 7504
train acc:  0.7265625
train loss:  0.542185366153717
train gradient:  0.13941486888643329
iteration : 7505
train acc:  0.7578125
train loss:  0.5135254859924316
train gradient:  0.14407016371694922
iteration : 7506
train acc:  0.7421875
train loss:  0.5029117465019226
train gradient:  0.1274576186084479
iteration : 7507
train acc:  0.71875
train loss:  0.5276727676391602
train gradient:  0.16172093356556827
iteration : 7508
train acc:  0.7421875
train loss:  0.4902384877204895
train gradient:  0.1367948103988615
iteration : 7509
train acc:  0.71875
train loss:  0.516836404800415
train gradient:  0.13794569670333215
iteration : 7510
train acc:  0.71875
train loss:  0.5502874255180359
train gradient:  0.14796215154769093
iteration : 7511
train acc:  0.7734375
train loss:  0.4356831908226013
train gradient:  0.09996154113839945
iteration : 7512
train acc:  0.6875
train loss:  0.5369523763656616
train gradient:  0.14232344868188604
iteration : 7513
train acc:  0.6796875
train loss:  0.5543879270553589
train gradient:  0.14665516362298353
iteration : 7514
train acc:  0.78125
train loss:  0.4680355191230774
train gradient:  0.1368257871301811
iteration : 7515
train acc:  0.7109375
train loss:  0.5072802901268005
train gradient:  0.13233853523250294
iteration : 7516
train acc:  0.7109375
train loss:  0.552549421787262
train gradient:  0.15667625020870848
iteration : 7517
train acc:  0.7109375
train loss:  0.5249154567718506
train gradient:  0.13520039105349924
iteration : 7518
train acc:  0.7109375
train loss:  0.5211856365203857
train gradient:  0.15460105125019896
iteration : 7519
train acc:  0.7421875
train loss:  0.5750206708908081
train gradient:  0.219916081298866
iteration : 7520
train acc:  0.7890625
train loss:  0.45919084548950195
train gradient:  0.11837492657593218
iteration : 7521
train acc:  0.75
train loss:  0.5480827689170837
train gradient:  0.17016108863905127
iteration : 7522
train acc:  0.75
train loss:  0.4975773096084595
train gradient:  0.14428416862431281
iteration : 7523
train acc:  0.7578125
train loss:  0.4745694100856781
train gradient:  0.11770112924687016
iteration : 7524
train acc:  0.734375
train loss:  0.4820736050605774
train gradient:  0.12149525726530408
iteration : 7525
train acc:  0.6875
train loss:  0.545838475227356
train gradient:  0.16829068356354893
iteration : 7526
train acc:  0.8046875
train loss:  0.4517304301261902
train gradient:  0.12134212676194835
iteration : 7527
train acc:  0.7734375
train loss:  0.467168390750885
train gradient:  0.12322329315857336
iteration : 7528
train acc:  0.78125
train loss:  0.4564415514469147
train gradient:  0.11483740725141987
iteration : 7529
train acc:  0.71875
train loss:  0.46716004610061646
train gradient:  0.11166709086077668
iteration : 7530
train acc:  0.6875
train loss:  0.5793997049331665
train gradient:  0.20706125246530832
iteration : 7531
train acc:  0.7578125
train loss:  0.44814056158065796
train gradient:  0.10675350364406733
iteration : 7532
train acc:  0.7265625
train loss:  0.49201884865760803
train gradient:  0.12826275770549825
iteration : 7533
train acc:  0.75
train loss:  0.460984468460083
train gradient:  0.13385488095235631
iteration : 7534
train acc:  0.7265625
train loss:  0.5094093084335327
train gradient:  0.13145733935932413
iteration : 7535
train acc:  0.7265625
train loss:  0.48569029569625854
train gradient:  0.11405647891704476
iteration : 7536
train acc:  0.7421875
train loss:  0.5194768905639648
train gradient:  0.12917114163936375
iteration : 7537
train acc:  0.6640625
train loss:  0.5564175844192505
train gradient:  0.17813928218510428
iteration : 7538
train acc:  0.7421875
train loss:  0.4885962903499603
train gradient:  0.1093614710616244
iteration : 7539
train acc:  0.75
train loss:  0.47124040126800537
train gradient:  0.11541830672416382
iteration : 7540
train acc:  0.7265625
train loss:  0.49925917387008667
train gradient:  0.12901960803791812
iteration : 7541
train acc:  0.671875
train loss:  0.580410361289978
train gradient:  0.15869023568103285
iteration : 7542
train acc:  0.671875
train loss:  0.5870428085327148
train gradient:  0.17164419401148745
iteration : 7543
train acc:  0.7578125
train loss:  0.4965780973434448
train gradient:  0.14548977877737956
iteration : 7544
train acc:  0.8359375
train loss:  0.3999732732772827
train gradient:  0.10084255078383147
iteration : 7545
train acc:  0.671875
train loss:  0.5502499341964722
train gradient:  0.1405625283458587
iteration : 7546
train acc:  0.7421875
train loss:  0.4632449746131897
train gradient:  0.12299173357108845
iteration : 7547
train acc:  0.7421875
train loss:  0.521420955657959
train gradient:  0.15546248348132663
iteration : 7548
train acc:  0.734375
train loss:  0.5185799598693848
train gradient:  0.12557697689819058
iteration : 7549
train acc:  0.734375
train loss:  0.5520281791687012
train gradient:  0.14162305180792772
iteration : 7550
train acc:  0.6640625
train loss:  0.5811533331871033
train gradient:  0.21271797604868964
iteration : 7551
train acc:  0.7109375
train loss:  0.5499458909034729
train gradient:  0.17257896108183002
iteration : 7552
train acc:  0.75
train loss:  0.48448407649993896
train gradient:  0.1762830653159143
iteration : 7553
train acc:  0.7578125
train loss:  0.4405938982963562
train gradient:  0.10815663773957561
iteration : 7554
train acc:  0.6796875
train loss:  0.5335320234298706
train gradient:  0.1610613484619444
iteration : 7555
train acc:  0.65625
train loss:  0.5426373481750488
train gradient:  0.18449150913473533
iteration : 7556
train acc:  0.7578125
train loss:  0.4925767481327057
train gradient:  0.16088161184910132
iteration : 7557
train acc:  0.7109375
train loss:  0.5185230374336243
train gradient:  0.1614144891614312
iteration : 7558
train acc:  0.796875
train loss:  0.4445808231830597
train gradient:  0.10743833443778539
iteration : 7559
train acc:  0.7734375
train loss:  0.42531901597976685
train gradient:  0.11487348610712753
iteration : 7560
train acc:  0.7265625
train loss:  0.5446573495864868
train gradient:  0.1554113733200101
iteration : 7561
train acc:  0.7109375
train loss:  0.5182383060455322
train gradient:  0.12951200472814273
iteration : 7562
train acc:  0.734375
train loss:  0.5590931177139282
train gradient:  0.14527169044032315
iteration : 7563
train acc:  0.6484375
train loss:  0.551563560962677
train gradient:  0.19904476149346295
iteration : 7564
train acc:  0.71875
train loss:  0.501925528049469
train gradient:  0.1463404279175347
iteration : 7565
train acc:  0.7265625
train loss:  0.5393348932266235
train gradient:  0.13766932792080516
iteration : 7566
train acc:  0.7578125
train loss:  0.47204840183258057
train gradient:  0.13712730689133446
iteration : 7567
train acc:  0.734375
train loss:  0.5166811943054199
train gradient:  0.14826736633375637
iteration : 7568
train acc:  0.7890625
train loss:  0.4580049514770508
train gradient:  0.09958340175911316
iteration : 7569
train acc:  0.7109375
train loss:  0.5276225805282593
train gradient:  0.17011204057698875
iteration : 7570
train acc:  0.734375
train loss:  0.5238851308822632
train gradient:  0.12558959683480614
iteration : 7571
train acc:  0.671875
train loss:  0.579242467880249
train gradient:  0.16622139316579826
iteration : 7572
train acc:  0.75
train loss:  0.5003359317779541
train gradient:  0.1298291500200354
iteration : 7573
train acc:  0.765625
train loss:  0.47602248191833496
train gradient:  0.11219550997513152
iteration : 7574
train acc:  0.7421875
train loss:  0.49790629744529724
train gradient:  0.1338151192095021
iteration : 7575
train acc:  0.7734375
train loss:  0.47043049335479736
train gradient:  0.11134795557885832
iteration : 7576
train acc:  0.6484375
train loss:  0.6130005121231079
train gradient:  0.21852274421539597
iteration : 7577
train acc:  0.71875
train loss:  0.539818525314331
train gradient:  0.13321101234607335
iteration : 7578
train acc:  0.71875
train loss:  0.48023971915245056
train gradient:  0.13122606409755838
iteration : 7579
train acc:  0.71875
train loss:  0.4856036603450775
train gradient:  0.11721899256393144
iteration : 7580
train acc:  0.7578125
train loss:  0.45183515548706055
train gradient:  0.12208140688793319
iteration : 7581
train acc:  0.7578125
train loss:  0.5151740312576294
train gradient:  0.15696590831723906
iteration : 7582
train acc:  0.7578125
train loss:  0.48684823513031006
train gradient:  0.16818637694511634
iteration : 7583
train acc:  0.7578125
train loss:  0.4595891833305359
train gradient:  0.112724894582313
iteration : 7584
train acc:  0.6484375
train loss:  0.575517475605011
train gradient:  0.1473734549178773
iteration : 7585
train acc:  0.6953125
train loss:  0.5380212068557739
train gradient:  0.19464810636747787
iteration : 7586
train acc:  0.7578125
train loss:  0.4730073809623718
train gradient:  0.10951812966360998
iteration : 7587
train acc:  0.71875
train loss:  0.5683530569076538
train gradient:  0.17597834765618942
iteration : 7588
train acc:  0.7421875
train loss:  0.49517232179641724
train gradient:  0.14489176436618512
iteration : 7589
train acc:  0.75
train loss:  0.5169119238853455
train gradient:  0.1638215809312726
iteration : 7590
train acc:  0.6875
train loss:  0.526775598526001
train gradient:  0.13099162774830198
iteration : 7591
train acc:  0.7578125
train loss:  0.45956242084503174
train gradient:  0.14041386372429351
iteration : 7592
train acc:  0.7265625
train loss:  0.511854887008667
train gradient:  0.13681058957940212
iteration : 7593
train acc:  0.75
train loss:  0.4440315365791321
train gradient:  0.09641309708832908
iteration : 7594
train acc:  0.828125
train loss:  0.4097955822944641
train gradient:  0.139494085453221
iteration : 7595
train acc:  0.75
train loss:  0.44356438517570496
train gradient:  0.09307532458841115
iteration : 7596
train acc:  0.734375
train loss:  0.49355846643447876
train gradient:  0.1716580953328129
iteration : 7597
train acc:  0.7265625
train loss:  0.503252387046814
train gradient:  0.16010275283816705
iteration : 7598
train acc:  0.75
train loss:  0.5229158401489258
train gradient:  0.13415979631185715
iteration : 7599
train acc:  0.7109375
train loss:  0.5229706764221191
train gradient:  0.16146623896804652
iteration : 7600
train acc:  0.7109375
train loss:  0.4767780303955078
train gradient:  0.15216787938735932
iteration : 7601
train acc:  0.671875
train loss:  0.5673794746398926
train gradient:  0.19627784245242635
iteration : 7602
train acc:  0.75
train loss:  0.5151375532150269
train gradient:  0.13102854349770687
iteration : 7603
train acc:  0.703125
train loss:  0.5489377379417419
train gradient:  0.15170147184844177
iteration : 7604
train acc:  0.6796875
train loss:  0.570315957069397
train gradient:  0.20687852264466938
iteration : 7605
train acc:  0.765625
train loss:  0.5048495531082153
train gradient:  0.11410866035137213
iteration : 7606
train acc:  0.7578125
train loss:  0.4809756278991699
train gradient:  0.12204875678566317
iteration : 7607
train acc:  0.7265625
train loss:  0.4720918536186218
train gradient:  0.11703057899279784
iteration : 7608
train acc:  0.7421875
train loss:  0.49964550137519836
train gradient:  0.14036710285935855
iteration : 7609
train acc:  0.75
train loss:  0.4961373805999756
train gradient:  0.11749279711991303
iteration : 7610
train acc:  0.7109375
train loss:  0.49115341901779175
train gradient:  0.1331252407452766
iteration : 7611
train acc:  0.7265625
train loss:  0.5250427722930908
train gradient:  0.13444676585272936
iteration : 7612
train acc:  0.671875
train loss:  0.5269025564193726
train gradient:  0.20573171473951873
iteration : 7613
train acc:  0.7734375
train loss:  0.49388861656188965
train gradient:  0.12440688407811414
iteration : 7614
train acc:  0.8046875
train loss:  0.4699857234954834
train gradient:  0.12877728827275484
iteration : 7615
train acc:  0.7890625
train loss:  0.44009870290756226
train gradient:  0.12480996546737573
iteration : 7616
train acc:  0.6875
train loss:  0.5208972692489624
train gradient:  0.13079887725065065
iteration : 7617
train acc:  0.71875
train loss:  0.5308566689491272
train gradient:  0.15894197382222322
iteration : 7618
train acc:  0.71875
train loss:  0.4780631959438324
train gradient:  0.1339303562961231
iteration : 7619
train acc:  0.7734375
train loss:  0.47051671147346497
train gradient:  0.11913528836569522
iteration : 7620
train acc:  0.71875
train loss:  0.5254248380661011
train gradient:  0.1803075372593531
iteration : 7621
train acc:  0.734375
train loss:  0.5067179799079895
train gradient:  0.1550482455178304
iteration : 7622
train acc:  0.7734375
train loss:  0.45899564027786255
train gradient:  0.144384218160721
iteration : 7623
train acc:  0.6875
train loss:  0.5282449722290039
train gradient:  0.12404255114992924
iteration : 7624
train acc:  0.703125
train loss:  0.541719913482666
train gradient:  0.1802226427429135
iteration : 7625
train acc:  0.75
train loss:  0.4637058973312378
train gradient:  0.14211271826314203
iteration : 7626
train acc:  0.765625
train loss:  0.4432756304740906
train gradient:  0.11295489176266606
iteration : 7627
train acc:  0.6953125
train loss:  0.5328603982925415
train gradient:  0.14305111330549974
iteration : 7628
train acc:  0.703125
train loss:  0.5357788801193237
train gradient:  0.13912155104014845
iteration : 7629
train acc:  0.6875
train loss:  0.5853374004364014
train gradient:  0.18206857161045797
iteration : 7630
train acc:  0.7265625
train loss:  0.5056387186050415
train gradient:  0.1485241051800114
iteration : 7631
train acc:  0.796875
train loss:  0.49744150042533875
train gradient:  0.21627969854977047
iteration : 7632
train acc:  0.7265625
train loss:  0.5096030831336975
train gradient:  0.17892106303923394
iteration : 7633
train acc:  0.734375
train loss:  0.47578731179237366
train gradient:  0.1359686877874652
iteration : 7634
train acc:  0.765625
train loss:  0.4923142194747925
train gradient:  0.1422696150555218
iteration : 7635
train acc:  0.7265625
train loss:  0.501955509185791
train gradient:  0.13196570506746913
iteration : 7636
train acc:  0.7265625
train loss:  0.5124301314353943
train gradient:  0.16238303227621445
iteration : 7637
train acc:  0.671875
train loss:  0.561194658279419
train gradient:  0.19351208598777384
iteration : 7638
train acc:  0.796875
train loss:  0.4341340661048889
train gradient:  0.11255639173878006
iteration : 7639
train acc:  0.71875
train loss:  0.5120433568954468
train gradient:  0.1482049003437309
iteration : 7640
train acc:  0.7265625
train loss:  0.519087016582489
train gradient:  0.11688307514452856
iteration : 7641
train acc:  0.6796875
train loss:  0.5532593131065369
train gradient:  0.1812811161263334
iteration : 7642
train acc:  0.6953125
train loss:  0.5343412160873413
train gradient:  0.20686955552477437
iteration : 7643
train acc:  0.78125
train loss:  0.44621267914772034
train gradient:  0.09912551823833296
iteration : 7644
train acc:  0.7421875
train loss:  0.49155569076538086
train gradient:  0.13032637158655347
iteration : 7645
train acc:  0.6640625
train loss:  0.641947329044342
train gradient:  0.21134415192369715
iteration : 7646
train acc:  0.734375
train loss:  0.5056450366973877
train gradient:  0.14254164229348942
iteration : 7647
train acc:  0.78125
train loss:  0.4627351760864258
train gradient:  0.126789319701668
iteration : 7648
train acc:  0.75
train loss:  0.43238306045532227
train gradient:  0.08115074053218169
iteration : 7649
train acc:  0.71875
train loss:  0.5258821249008179
train gradient:  0.1317265672590533
iteration : 7650
train acc:  0.6875
train loss:  0.5694519281387329
train gradient:  0.1822795776329495
iteration : 7651
train acc:  0.6484375
train loss:  0.6069185733795166
train gradient:  0.16517509406087447
iteration : 7652
train acc:  0.7265625
train loss:  0.5116955637931824
train gradient:  0.13456809651836443
iteration : 7653
train acc:  0.7734375
train loss:  0.4980679750442505
train gradient:  0.16085986231899374
iteration : 7654
train acc:  0.7421875
train loss:  0.5358270406723022
train gradient:  0.16408990397618153
iteration : 7655
train acc:  0.7265625
train loss:  0.5303999185562134
train gradient:  0.14668470209457501
iteration : 7656
train acc:  0.734375
train loss:  0.4820293188095093
train gradient:  0.13229954678163208
iteration : 7657
train acc:  0.765625
train loss:  0.5181277990341187
train gradient:  0.12652182201902543
iteration : 7658
train acc:  0.7265625
train loss:  0.5269746780395508
train gradient:  0.15769826895522787
iteration : 7659
train acc:  0.7578125
train loss:  0.4909328520298004
train gradient:  0.1243757382430979
iteration : 7660
train acc:  0.796875
train loss:  0.45157289505004883
train gradient:  0.1363016810314696
iteration : 7661
train acc:  0.828125
train loss:  0.42864909768104553
train gradient:  0.10382115623696335
iteration : 7662
train acc:  0.7734375
train loss:  0.4765724837779999
train gradient:  0.11725162948607584
iteration : 7663
train acc:  0.8203125
train loss:  0.43545204401016235
train gradient:  0.1399906434168754
iteration : 7664
train acc:  0.78125
train loss:  0.43376004695892334
train gradient:  0.09945317093191221
iteration : 7665
train acc:  0.71875
train loss:  0.4981507658958435
train gradient:  0.12600186091051324
iteration : 7666
train acc:  0.6953125
train loss:  0.5626196265220642
train gradient:  0.1461429470561128
iteration : 7667
train acc:  0.7421875
train loss:  0.5245822072029114
train gradient:  0.14749236139164434
iteration : 7668
train acc:  0.7421875
train loss:  0.48897650837898254
train gradient:  0.167576626348929
iteration : 7669
train acc:  0.8125
train loss:  0.4184475541114807
train gradient:  0.10694193608176011
iteration : 7670
train acc:  0.7265625
train loss:  0.5256242752075195
train gradient:  0.14817538067124666
iteration : 7671
train acc:  0.7890625
train loss:  0.4224005937576294
train gradient:  0.11699412745712584
iteration : 7672
train acc:  0.796875
train loss:  0.4695417881011963
train gradient:  0.1108448825197582
iteration : 7673
train acc:  0.71875
train loss:  0.5618774890899658
train gradient:  0.14774550369371964
iteration : 7674
train acc:  0.7265625
train loss:  0.4907304048538208
train gradient:  0.12371963684303912
iteration : 7675
train acc:  0.71875
train loss:  0.4936921000480652
train gradient:  0.10357007571773956
iteration : 7676
train acc:  0.7421875
train loss:  0.474276602268219
train gradient:  0.10753729281389719
iteration : 7677
train acc:  0.71875
train loss:  0.6419512033462524
train gradient:  0.2031510744649641
iteration : 7678
train acc:  0.703125
train loss:  0.5276331305503845
train gradient:  0.20697500912136269
iteration : 7679
train acc:  0.6484375
train loss:  0.6063563227653503
train gradient:  0.15746375154623804
iteration : 7680
train acc:  0.7734375
train loss:  0.4341718554496765
train gradient:  0.1152927946237161
iteration : 7681
train acc:  0.7421875
train loss:  0.49113333225250244
train gradient:  0.171809442389543
iteration : 7682
train acc:  0.796875
train loss:  0.4328880310058594
train gradient:  0.10756306676033964
iteration : 7683
train acc:  0.7421875
train loss:  0.46856963634490967
train gradient:  0.1321792686919261
iteration : 7684
train acc:  0.71875
train loss:  0.46912693977355957
train gradient:  0.14090771127791085
iteration : 7685
train acc:  0.78125
train loss:  0.4832698106765747
train gradient:  0.1194146683054791
iteration : 7686
train acc:  0.6953125
train loss:  0.48638013005256653
train gradient:  0.15073818550986473
iteration : 7687
train acc:  0.71875
train loss:  0.490483820438385
train gradient:  0.12826447788500006
iteration : 7688
train acc:  0.7421875
train loss:  0.4989932179450989
train gradient:  0.14569126251956588
iteration : 7689
train acc:  0.71875
train loss:  0.5285124778747559
train gradient:  0.15866595676133174
iteration : 7690
train acc:  0.7265625
train loss:  0.4658208191394806
train gradient:  0.12755888463117793
iteration : 7691
train acc:  0.6953125
train loss:  0.5424015522003174
train gradient:  0.15403428626774285
iteration : 7692
train acc:  0.7890625
train loss:  0.4583066999912262
train gradient:  0.12917256353086673
iteration : 7693
train acc:  0.8359375
train loss:  0.4323209524154663
train gradient:  0.10774010629773395
iteration : 7694
train acc:  0.7734375
train loss:  0.4834485650062561
train gradient:  0.12512203785917292
iteration : 7695
train acc:  0.6953125
train loss:  0.5565762519836426
train gradient:  0.15658109303368392
iteration : 7696
train acc:  0.65625
train loss:  0.6100950241088867
train gradient:  0.20671059607350226
iteration : 7697
train acc:  0.7890625
train loss:  0.4814039468765259
train gradient:  0.12709373307773292
iteration : 7698
train acc:  0.7578125
train loss:  0.5053063631057739
train gradient:  0.13949040201789364
iteration : 7699
train acc:  0.71875
train loss:  0.4399046003818512
train gradient:  0.10837503863882351
iteration : 7700
train acc:  0.765625
train loss:  0.4837798774242401
train gradient:  0.12723788399139593
iteration : 7701
train acc:  0.71875
train loss:  0.5701804161071777
train gradient:  0.20326407465162538
iteration : 7702
train acc:  0.6875
train loss:  0.5381469130516052
train gradient:  0.17872988737866916
iteration : 7703
train acc:  0.78125
train loss:  0.4199039936065674
train gradient:  0.08712352144087801
iteration : 7704
train acc:  0.671875
train loss:  0.5629842877388
train gradient:  0.1629661937922154
iteration : 7705
train acc:  0.75
train loss:  0.4869092106819153
train gradient:  0.13145037282617997
iteration : 7706
train acc:  0.734375
train loss:  0.5465875864028931
train gradient:  0.15011500325163285
iteration : 7707
train acc:  0.671875
train loss:  0.5630356073379517
train gradient:  0.16271329770966075
iteration : 7708
train acc:  0.75
train loss:  0.49816635251045227
train gradient:  0.11968143637033639
iteration : 7709
train acc:  0.71875
train loss:  0.5177389979362488
train gradient:  0.16243057550969303
iteration : 7710
train acc:  0.671875
train loss:  0.5821624994277954
train gradient:  0.16149748364531002
iteration : 7711
train acc:  0.7421875
train loss:  0.5014077425003052
train gradient:  0.12954374013426248
iteration : 7712
train acc:  0.65625
train loss:  0.6079010963439941
train gradient:  0.17264786395446163
iteration : 7713
train acc:  0.703125
train loss:  0.5844625234603882
train gradient:  0.16869774382850403
iteration : 7714
train acc:  0.71875
train loss:  0.526550829410553
train gradient:  0.16229255865352174
iteration : 7715
train acc:  0.71875
train loss:  0.5531613826751709
train gradient:  0.125177537335681
iteration : 7716
train acc:  0.6484375
train loss:  0.609839677810669
train gradient:  0.16974696632215708
iteration : 7717
train acc:  0.7734375
train loss:  0.4698294401168823
train gradient:  0.1269003613444512
iteration : 7718
train acc:  0.7109375
train loss:  0.5381270051002502
train gradient:  0.16130538865616384
iteration : 7719
train acc:  0.734375
train loss:  0.4724367558956146
train gradient:  0.09883314769911576
iteration : 7720
train acc:  0.765625
train loss:  0.4664023518562317
train gradient:  0.14030868440377559
iteration : 7721
train acc:  0.75
train loss:  0.4873022437095642
train gradient:  0.20139833407262192
iteration : 7722
train acc:  0.6796875
train loss:  0.5319041013717651
train gradient:  0.16718838728148427
iteration : 7723
train acc:  0.7265625
train loss:  0.5161908864974976
train gradient:  0.1731426765884753
iteration : 7724
train acc:  0.6875
train loss:  0.5576821565628052
train gradient:  0.14947124061256212
iteration : 7725
train acc:  0.75
train loss:  0.5038343667984009
train gradient:  0.16350462752899653
iteration : 7726
train acc:  0.75
train loss:  0.47348326444625854
train gradient:  0.12385580708780669
iteration : 7727
train acc:  0.703125
train loss:  0.517224907875061
train gradient:  0.13610804040768362
iteration : 7728
train acc:  0.78125
train loss:  0.48712053894996643
train gradient:  0.10788729684754778
iteration : 7729
train acc:  0.7578125
train loss:  0.44433119893074036
train gradient:  0.1031806135350218
iteration : 7730
train acc:  0.75
train loss:  0.4919251799583435
train gradient:  0.12223755688300701
iteration : 7731
train acc:  0.7578125
train loss:  0.4989757537841797
train gradient:  0.14567468541917966
iteration : 7732
train acc:  0.6640625
train loss:  0.5635911226272583
train gradient:  0.17998078258996966
iteration : 7733
train acc:  0.71875
train loss:  0.4934614300727844
train gradient:  0.13012555990356495
iteration : 7734
train acc:  0.796875
train loss:  0.5129806995391846
train gradient:  0.13222630405218244
iteration : 7735
train acc:  0.734375
train loss:  0.4972018599510193
train gradient:  0.1572096583207288
iteration : 7736
train acc:  0.609375
train loss:  0.5718350410461426
train gradient:  0.16642267693798418
iteration : 7737
train acc:  0.671875
train loss:  0.5812582969665527
train gradient:  0.16108293044693583
iteration : 7738
train acc:  0.8515625
train loss:  0.4009208679199219
train gradient:  0.08343405195063448
iteration : 7739
train acc:  0.7265625
train loss:  0.4950035810470581
train gradient:  0.1349971104669458
iteration : 7740
train acc:  0.78125
train loss:  0.4702591300010681
train gradient:  0.13527787571664157
iteration : 7741
train acc:  0.734375
train loss:  0.51897132396698
train gradient:  0.13478057019611167
iteration : 7742
train acc:  0.7578125
train loss:  0.46950119733810425
train gradient:  0.1087631507879705
iteration : 7743
train acc:  0.84375
train loss:  0.4165523648262024
train gradient:  0.11000166887666386
iteration : 7744
train acc:  0.71875
train loss:  0.5684821605682373
train gradient:  0.18081708828557563
iteration : 7745
train acc:  0.7265625
train loss:  0.4895123839378357
train gradient:  0.11943068382398324
iteration : 7746
train acc:  0.703125
train loss:  0.5437484383583069
train gradient:  0.15966087382055444
iteration : 7747
train acc:  0.671875
train loss:  0.5758673548698425
train gradient:  0.17913788251352575
iteration : 7748
train acc:  0.7578125
train loss:  0.4877663254737854
train gradient:  0.1181047847892809
iteration : 7749
train acc:  0.71875
train loss:  0.5338784456253052
train gradient:  0.13699265091887086
iteration : 7750
train acc:  0.7421875
train loss:  0.4877375364303589
train gradient:  0.1230077655964409
iteration : 7751
train acc:  0.625
train loss:  0.6424042582511902
train gradient:  0.22467127658426694
iteration : 7752
train acc:  0.7578125
train loss:  0.45496636629104614
train gradient:  0.1473240937017156
iteration : 7753
train acc:  0.65625
train loss:  0.5154110193252563
train gradient:  0.1679136695960977
iteration : 7754
train acc:  0.6796875
train loss:  0.5608907341957092
train gradient:  0.14883532979663106
iteration : 7755
train acc:  0.71875
train loss:  0.4857867956161499
train gradient:  0.12660645099288947
iteration : 7756
train acc:  0.7421875
train loss:  0.5072586536407471
train gradient:  0.13617831631441302
iteration : 7757
train acc:  0.7421875
train loss:  0.500910758972168
train gradient:  0.1188929163682437
iteration : 7758
train acc:  0.6953125
train loss:  0.5649821758270264
train gradient:  0.1879167117809312
iteration : 7759
train acc:  0.6640625
train loss:  0.6041409373283386
train gradient:  0.14758714679226803
iteration : 7760
train acc:  0.7265625
train loss:  0.5308036208152771
train gradient:  0.13847161627011062
iteration : 7761
train acc:  0.78125
train loss:  0.538949191570282
train gradient:  0.1366565440658874
iteration : 7762
train acc:  0.671875
train loss:  0.5554906725883484
train gradient:  0.1900063565541234
iteration : 7763
train acc:  0.6953125
train loss:  0.5570327639579773
train gradient:  0.14504723394088515
iteration : 7764
train acc:  0.734375
train loss:  0.4789493680000305
train gradient:  0.14606562985302737
iteration : 7765
train acc:  0.71875
train loss:  0.5093069076538086
train gradient:  0.10814321397114948
iteration : 7766
train acc:  0.71875
train loss:  0.5620120763778687
train gradient:  0.19834553287401407
iteration : 7767
train acc:  0.7265625
train loss:  0.49606120586395264
train gradient:  0.16650982971896505
iteration : 7768
train acc:  0.6875
train loss:  0.5510345697402954
train gradient:  0.15194022215717046
iteration : 7769
train acc:  0.7578125
train loss:  0.47583597898483276
train gradient:  0.11822351421756637
iteration : 7770
train acc:  0.7734375
train loss:  0.4886244833469391
train gradient:  0.1536332219169418
iteration : 7771
train acc:  0.640625
train loss:  0.6048163771629333
train gradient:  0.17598890866745964
iteration : 7772
train acc:  0.7265625
train loss:  0.4853287637233734
train gradient:  0.119185187234635
iteration : 7773
train acc:  0.7734375
train loss:  0.4809722304344177
train gradient:  0.12193676213564761
iteration : 7774
train acc:  0.765625
train loss:  0.4994111955165863
train gradient:  0.1507423133195164
iteration : 7775
train acc:  0.7734375
train loss:  0.4705636501312256
train gradient:  0.13310508123632975
iteration : 7776
train acc:  0.6953125
train loss:  0.5796397924423218
train gradient:  0.20924207708327253
iteration : 7777
train acc:  0.78125
train loss:  0.43507814407348633
train gradient:  0.10983185066285357
iteration : 7778
train acc:  0.7734375
train loss:  0.5031818747520447
train gradient:  0.17293077881488791
iteration : 7779
train acc:  0.6015625
train loss:  0.6840569972991943
train gradient:  0.20305250366393382
iteration : 7780
train acc:  0.765625
train loss:  0.4913913905620575
train gradient:  0.14248406287375015
iteration : 7781
train acc:  0.6953125
train loss:  0.5111091732978821
train gradient:  0.12196158219299488
iteration : 7782
train acc:  0.7421875
train loss:  0.4775954782962799
train gradient:  0.09640457172104056
iteration : 7783
train acc:  0.71875
train loss:  0.5204907655715942
train gradient:  0.14999531866015986
iteration : 7784
train acc:  0.7578125
train loss:  0.5086225271224976
train gradient:  0.14523815660736636
iteration : 7785
train acc:  0.671875
train loss:  0.5165278315544128
train gradient:  0.1674972052133875
iteration : 7786
train acc:  0.734375
train loss:  0.4634888768196106
train gradient:  0.10931469522616796
iteration : 7787
train acc:  0.671875
train loss:  0.5575842261314392
train gradient:  0.14743171925741405
iteration : 7788
train acc:  0.7265625
train loss:  0.5041030645370483
train gradient:  0.1377603635915549
iteration : 7789
train acc:  0.796875
train loss:  0.45868051052093506
train gradient:  0.160307280896967
iteration : 7790
train acc:  0.7578125
train loss:  0.44784414768218994
train gradient:  0.11836466548812513
iteration : 7791
train acc:  0.71875
train loss:  0.5505795478820801
train gradient:  0.15950976779521814
iteration : 7792
train acc:  0.7578125
train loss:  0.4810452163219452
train gradient:  0.11280735225201063
iteration : 7793
train acc:  0.6796875
train loss:  0.5990374088287354
train gradient:  0.23075048244866317
iteration : 7794
train acc:  0.65625
train loss:  0.5480449795722961
train gradient:  0.17213110870233378
iteration : 7795
train acc:  0.71875
train loss:  0.5105434656143188
train gradient:  0.14569576037180398
iteration : 7796
train acc:  0.75
train loss:  0.505968451499939
train gradient:  0.15324221332608068
iteration : 7797
train acc:  0.765625
train loss:  0.4853178858757019
train gradient:  0.11255884644738437
iteration : 7798
train acc:  0.7265625
train loss:  0.47959646582603455
train gradient:  0.13689758190278448
iteration : 7799
train acc:  0.7734375
train loss:  0.47737443447113037
train gradient:  0.13725931485476628
iteration : 7800
train acc:  0.7890625
train loss:  0.43516072630882263
train gradient:  0.09893761724934225
iteration : 7801
train acc:  0.7265625
train loss:  0.5211764574050903
train gradient:  0.20104021348783319
iteration : 7802
train acc:  0.75
train loss:  0.5408287048339844
train gradient:  0.1297241971763845
iteration : 7803
train acc:  0.7578125
train loss:  0.495025634765625
train gradient:  0.12685977632914572
iteration : 7804
train acc:  0.7734375
train loss:  0.5038711428642273
train gradient:  0.11679877747219863
iteration : 7805
train acc:  0.7734375
train loss:  0.4795076251029968
train gradient:  0.1431609408822756
iteration : 7806
train acc:  0.7421875
train loss:  0.521947979927063
train gradient:  0.13684654089002798
iteration : 7807
train acc:  0.7578125
train loss:  0.5128697156906128
train gradient:  0.1427300113879132
iteration : 7808
train acc:  0.765625
train loss:  0.48215967416763306
train gradient:  0.13859554313563946
iteration : 7809
train acc:  0.7421875
train loss:  0.5212404727935791
train gradient:  0.1516818009424627
iteration : 7810
train acc:  0.7109375
train loss:  0.4890042841434479
train gradient:  0.1371098757758634
iteration : 7811
train acc:  0.765625
train loss:  0.4362552762031555
train gradient:  0.1112762685191449
iteration : 7812
train acc:  0.7578125
train loss:  0.5059490203857422
train gradient:  0.1427852803649936
iteration : 7813
train acc:  0.7734375
train loss:  0.48423823714256287
train gradient:  0.12740618161806216
iteration : 7814
train acc:  0.7890625
train loss:  0.4778980612754822
train gradient:  0.16484338186304487
iteration : 7815
train acc:  0.78125
train loss:  0.46792083978652954
train gradient:  0.1387390961012093
iteration : 7816
train acc:  0.7265625
train loss:  0.5254800319671631
train gradient:  0.13231722913026897
iteration : 7817
train acc:  0.765625
train loss:  0.4742441773414612
train gradient:  0.11738805494636233
iteration : 7818
train acc:  0.7890625
train loss:  0.4513816833496094
train gradient:  0.0957395487518213
iteration : 7819
train acc:  0.7421875
train loss:  0.5321482419967651
train gradient:  0.1266177049891874
iteration : 7820
train acc:  0.703125
train loss:  0.5007030963897705
train gradient:  0.1255199797110504
iteration : 7821
train acc:  0.7421875
train loss:  0.4450286328792572
train gradient:  0.10486608309175106
iteration : 7822
train acc:  0.71875
train loss:  0.5313507318496704
train gradient:  0.13433389734487874
iteration : 7823
train acc:  0.7578125
train loss:  0.5023199915885925
train gradient:  0.1313219936602862
iteration : 7824
train acc:  0.8125
train loss:  0.43533825874328613
train gradient:  0.12199643535238927
iteration : 7825
train acc:  0.6875
train loss:  0.5691478848457336
train gradient:  0.15335180729950376
iteration : 7826
train acc:  0.734375
train loss:  0.5082399249076843
train gradient:  0.14749229181281467
iteration : 7827
train acc:  0.734375
train loss:  0.46199116110801697
train gradient:  0.12101864473715788
iteration : 7828
train acc:  0.734375
train loss:  0.47983646392822266
train gradient:  0.10034792002174728
iteration : 7829
train acc:  0.765625
train loss:  0.4942590594291687
train gradient:  0.163403728880261
iteration : 7830
train acc:  0.7578125
train loss:  0.45047324895858765
train gradient:  0.1277809680749335
iteration : 7831
train acc:  0.703125
train loss:  0.5251843333244324
train gradient:  0.1322092878743433
iteration : 7832
train acc:  0.7421875
train loss:  0.5198637247085571
train gradient:  0.2497076831616942
iteration : 7833
train acc:  0.78125
train loss:  0.48964670300483704
train gradient:  0.13253483352525214
iteration : 7834
train acc:  0.7109375
train loss:  0.5051152110099792
train gradient:  0.14632807891121236
iteration : 7835
train acc:  0.78125
train loss:  0.45435643196105957
train gradient:  0.14116285409826904
iteration : 7836
train acc:  0.7578125
train loss:  0.4642382264137268
train gradient:  0.12027550549872412
iteration : 7837
train acc:  0.7578125
train loss:  0.4994957447052002
train gradient:  0.15568868808635988
iteration : 7838
train acc:  0.734375
train loss:  0.4943496882915497
train gradient:  0.13573681746843697
iteration : 7839
train acc:  0.671875
train loss:  0.5603438019752502
train gradient:  0.13564338407222204
iteration : 7840
train acc:  0.7265625
train loss:  0.5155175924301147
train gradient:  0.14434066221343494
iteration : 7841
train acc:  0.7421875
train loss:  0.4923783540725708
train gradient:  0.13008238957942359
iteration : 7842
train acc:  0.65625
train loss:  0.5725812911987305
train gradient:  0.1593811238641068
iteration : 7843
train acc:  0.703125
train loss:  0.507843017578125
train gradient:  0.13353884392443977
iteration : 7844
train acc:  0.734375
train loss:  0.4854718744754791
train gradient:  0.13573022814558017
iteration : 7845
train acc:  0.7421875
train loss:  0.4605495035648346
train gradient:  0.12240086275691894
iteration : 7846
train acc:  0.71875
train loss:  0.5389000773429871
train gradient:  0.16127348642344497
iteration : 7847
train acc:  0.6875
train loss:  0.596156120300293
train gradient:  0.1887470298772645
iteration : 7848
train acc:  0.765625
train loss:  0.5397098064422607
train gradient:  0.17312797984902206
iteration : 7849
train acc:  0.75
train loss:  0.5039817690849304
train gradient:  0.14685097191907903
iteration : 7850
train acc:  0.7109375
train loss:  0.5066330432891846
train gradient:  0.15140186619732715
iteration : 7851
train acc:  0.7265625
train loss:  0.49582788348197937
train gradient:  0.1334879679352488
iteration : 7852
train acc:  0.6796875
train loss:  0.5267583131790161
train gradient:  0.13106907264346002
iteration : 7853
train acc:  0.71875
train loss:  0.5386373996734619
train gradient:  0.13495332300666052
iteration : 7854
train acc:  0.8125
train loss:  0.450791597366333
train gradient:  0.10734357704010958
iteration : 7855
train acc:  0.71875
train loss:  0.5306096076965332
train gradient:  0.16947190664820094
iteration : 7856
train acc:  0.7421875
train loss:  0.5131392478942871
train gradient:  0.11102929088825973
iteration : 7857
train acc:  0.6875
train loss:  0.5469558238983154
train gradient:  0.14391880212564606
iteration : 7858
train acc:  0.7734375
train loss:  0.502423107624054
train gradient:  0.14886802715259662
iteration : 7859
train acc:  0.6953125
train loss:  0.5441122055053711
train gradient:  0.13073194046332592
iteration : 7860
train acc:  0.6953125
train loss:  0.5510519742965698
train gradient:  0.13797811817883143
iteration : 7861
train acc:  0.7578125
train loss:  0.4936184287071228
train gradient:  0.12727691109676992
iteration : 7862
train acc:  0.734375
train loss:  0.4928748607635498
train gradient:  0.1373165585926343
iteration : 7863
train acc:  0.765625
train loss:  0.4590136408805847
train gradient:  0.13234694072095124
iteration : 7864
train acc:  0.75
train loss:  0.5191645622253418
train gradient:  0.13444209658164297
iteration : 7865
train acc:  0.734375
train loss:  0.4642474353313446
train gradient:  0.12130988190237521
iteration : 7866
train acc:  0.7578125
train loss:  0.5245727300643921
train gradient:  0.148973222243461
iteration : 7867
train acc:  0.8125
train loss:  0.42468079924583435
train gradient:  0.105764377621559
iteration : 7868
train acc:  0.765625
train loss:  0.4775850176811218
train gradient:  0.11095535980740578
iteration : 7869
train acc:  0.7890625
train loss:  0.42364344000816345
train gradient:  0.0930122177697108
iteration : 7870
train acc:  0.734375
train loss:  0.4776192605495453
train gradient:  0.12909384898403564
iteration : 7871
train acc:  0.7265625
train loss:  0.5301158428192139
train gradient:  0.10997023121010925
iteration : 7872
train acc:  0.7109375
train loss:  0.48794233798980713
train gradient:  0.09960112472861757
iteration : 7873
train acc:  0.765625
train loss:  0.4734216034412384
train gradient:  0.10954632196748647
iteration : 7874
train acc:  0.71875
train loss:  0.49703162908554077
train gradient:  0.13772569785875718
iteration : 7875
train acc:  0.7421875
train loss:  0.48457545042037964
train gradient:  0.16020157386413383
iteration : 7876
train acc:  0.703125
train loss:  0.5445513725280762
train gradient:  0.1570018629236059
iteration : 7877
train acc:  0.7890625
train loss:  0.46954792737960815
train gradient:  0.11371626627107848
iteration : 7878
train acc:  0.734375
train loss:  0.5096813440322876
train gradient:  0.1136446187376216
iteration : 7879
train acc:  0.6796875
train loss:  0.5564975142478943
train gradient:  0.16349731540911794
iteration : 7880
train acc:  0.75
train loss:  0.483722448348999
train gradient:  0.15511428113303638
iteration : 7881
train acc:  0.6640625
train loss:  0.5531690120697021
train gradient:  0.1888098752919884
iteration : 7882
train acc:  0.765625
train loss:  0.47553551197052
train gradient:  0.12016275903277826
iteration : 7883
train acc:  0.734375
train loss:  0.504673421382904
train gradient:  0.15990034289602773
iteration : 7884
train acc:  0.7578125
train loss:  0.4584618806838989
train gradient:  0.1003846120389706
iteration : 7885
train acc:  0.7109375
train loss:  0.5223722457885742
train gradient:  0.1385624288433614
iteration : 7886
train acc:  0.71875
train loss:  0.5566240549087524
train gradient:  0.1805674872011973
iteration : 7887
train acc:  0.71875
train loss:  0.4696803092956543
train gradient:  0.11276573963335663
iteration : 7888
train acc:  0.7109375
train loss:  0.49387502670288086
train gradient:  0.1639958151690104
iteration : 7889
train acc:  0.7421875
train loss:  0.5178840160369873
train gradient:  0.12599397165206422
iteration : 7890
train acc:  0.78125
train loss:  0.4240710437297821
train gradient:  0.10677837941477758
iteration : 7891
train acc:  0.7265625
train loss:  0.5144167542457581
train gradient:  0.13546528261587398
iteration : 7892
train acc:  0.71875
train loss:  0.5304946303367615
train gradient:  0.1338339137299771
iteration : 7893
train acc:  0.78125
train loss:  0.44342178106307983
train gradient:  0.13308630590623
iteration : 7894
train acc:  0.7890625
train loss:  0.4598086178302765
train gradient:  0.1235861574957396
iteration : 7895
train acc:  0.7890625
train loss:  0.43997320532798767
train gradient:  0.10879953957564563
iteration : 7896
train acc:  0.734375
train loss:  0.4522243142127991
train gradient:  0.13185494652983037
iteration : 7897
train acc:  0.7109375
train loss:  0.48701098561286926
train gradient:  0.12854165169841048
iteration : 7898
train acc:  0.7578125
train loss:  0.46616464853286743
train gradient:  0.10760455874768905
iteration : 7899
train acc:  0.7109375
train loss:  0.5583996176719666
train gradient:  0.18960883653850547
iteration : 7900
train acc:  0.8046875
train loss:  0.5039480924606323
train gradient:  0.2094909955078655
iteration : 7901
train acc:  0.7578125
train loss:  0.48475563526153564
train gradient:  0.12093646094921151
iteration : 7902
train acc:  0.7265625
train loss:  0.5239031314849854
train gradient:  0.1558159964366885
iteration : 7903
train acc:  0.734375
train loss:  0.4962015151977539
train gradient:  0.18653165047148446
iteration : 7904
train acc:  0.734375
train loss:  0.515950083732605
train gradient:  0.16313329285611633
iteration : 7905
train acc:  0.7734375
train loss:  0.4677923321723938
train gradient:  0.10904646988159439
iteration : 7906
train acc:  0.75
train loss:  0.4969697892665863
train gradient:  0.1478333035181606
iteration : 7907
train acc:  0.71875
train loss:  0.5053826570510864
train gradient:  0.18720608337813932
iteration : 7908
train acc:  0.7265625
train loss:  0.5210756659507751
train gradient:  0.1318157651028903
iteration : 7909
train acc:  0.7109375
train loss:  0.4986705780029297
train gradient:  0.1556088776499128
iteration : 7910
train acc:  0.703125
train loss:  0.5583151578903198
train gradient:  0.17407347133175977
iteration : 7911
train acc:  0.765625
train loss:  0.4859236478805542
train gradient:  0.12207652069581358
iteration : 7912
train acc:  0.765625
train loss:  0.5326257348060608
train gradient:  0.15365622611028062
iteration : 7913
train acc:  0.796875
train loss:  0.47729238867759705
train gradient:  0.12903168034248771
iteration : 7914
train acc:  0.7421875
train loss:  0.46614086627960205
train gradient:  0.12280184772052265
iteration : 7915
train acc:  0.703125
train loss:  0.5555102825164795
train gradient:  0.16242086380423004
iteration : 7916
train acc:  0.6953125
train loss:  0.5370337963104248
train gradient:  0.17424283417111652
iteration : 7917
train acc:  0.6796875
train loss:  0.5464000105857849
train gradient:  0.16735478272518328
iteration : 7918
train acc:  0.765625
train loss:  0.42832210659980774
train gradient:  0.09302956688948054
iteration : 7919
train acc:  0.671875
train loss:  0.5288037061691284
train gradient:  0.13248631442814404
iteration : 7920
train acc:  0.765625
train loss:  0.45638084411621094
train gradient:  0.11015890198482856
iteration : 7921
train acc:  0.78125
train loss:  0.4563751816749573
train gradient:  0.11232810772110226
iteration : 7922
train acc:  0.703125
train loss:  0.5224090814590454
train gradient:  0.15564511127640324
iteration : 7923
train acc:  0.7890625
train loss:  0.48647576570510864
train gradient:  0.11180440723302455
iteration : 7924
train acc:  0.703125
train loss:  0.5258278250694275
train gradient:  0.13687752490366467
iteration : 7925
train acc:  0.78125
train loss:  0.4430944323539734
train gradient:  0.12391423367297918
iteration : 7926
train acc:  0.7265625
train loss:  0.5210883617401123
train gradient:  0.18529047828676398
iteration : 7927
train acc:  0.75
train loss:  0.47864097356796265
train gradient:  0.1243092262845404
iteration : 7928
train acc:  0.828125
train loss:  0.3905925750732422
train gradient:  0.09550139546105665
iteration : 7929
train acc:  0.609375
train loss:  0.6337650418281555
train gradient:  0.22822249903520148
iteration : 7930
train acc:  0.828125
train loss:  0.42176541686058044
train gradient:  0.1023901369685689
iteration : 7931
train acc:  0.8125
train loss:  0.45461344718933105
train gradient:  0.12359482991808823
iteration : 7932
train acc:  0.75
train loss:  0.5364745855331421
train gradient:  0.1604724693141826
iteration : 7933
train acc:  0.71875
train loss:  0.4636634588241577
train gradient:  0.12747522885078333
iteration : 7934
train acc:  0.7421875
train loss:  0.5388134121894836
train gradient:  0.19643307773954588
iteration : 7935
train acc:  0.734375
train loss:  0.45601993799209595
train gradient:  0.11470894026743356
iteration : 7936
train acc:  0.7734375
train loss:  0.5213524103164673
train gradient:  0.1664235085985949
iteration : 7937
train acc:  0.7578125
train loss:  0.5017229318618774
train gradient:  0.15942476947786371
iteration : 7938
train acc:  0.7734375
train loss:  0.4899364113807678
train gradient:  0.11461280846201706
iteration : 7939
train acc:  0.734375
train loss:  0.5147247314453125
train gradient:  0.14265609167405796
iteration : 7940
train acc:  0.6953125
train loss:  0.5858884453773499
train gradient:  0.18340970379220367
iteration : 7941
train acc:  0.734375
train loss:  0.564569354057312
train gradient:  0.15621368398066754
iteration : 7942
train acc:  0.765625
train loss:  0.4516600966453552
train gradient:  0.15067099241549953
iteration : 7943
train acc:  0.6953125
train loss:  0.5352409482002258
train gradient:  0.13904194335745435
iteration : 7944
train acc:  0.6953125
train loss:  0.5230960845947266
train gradient:  0.16794362822793052
iteration : 7945
train acc:  0.71875
train loss:  0.5146616697311401
train gradient:  0.14652044580771906
iteration : 7946
train acc:  0.75
train loss:  0.5463457107543945
train gradient:  0.20089071233315625
iteration : 7947
train acc:  0.78125
train loss:  0.49492597579956055
train gradient:  0.13902157167307294
iteration : 7948
train acc:  0.8046875
train loss:  0.4479580521583557
train gradient:  0.10181685934634015
iteration : 7949
train acc:  0.7578125
train loss:  0.4318058490753174
train gradient:  0.0926567326660363
iteration : 7950
train acc:  0.7578125
train loss:  0.4713861346244812
train gradient:  0.11012103140224369
iteration : 7951
train acc:  0.75
train loss:  0.5324137210845947
train gradient:  0.17080075505033754
iteration : 7952
train acc:  0.7421875
train loss:  0.5216845273971558
train gradient:  0.15214274634881308
iteration : 7953
train acc:  0.78125
train loss:  0.45876508951187134
train gradient:  0.10906013427110392
iteration : 7954
train acc:  0.734375
train loss:  0.524335503578186
train gradient:  0.15527632097662736
iteration : 7955
train acc:  0.65625
train loss:  0.5521376132965088
train gradient:  0.12175687354398937
iteration : 7956
train acc:  0.7578125
train loss:  0.5331827402114868
train gradient:  0.1619364444831381
iteration : 7957
train acc:  0.78125
train loss:  0.47709470987319946
train gradient:  0.11034319995027765
iteration : 7958
train acc:  0.734375
train loss:  0.5321648120880127
train gradient:  0.11777321483284305
iteration : 7959
train acc:  0.7265625
train loss:  0.4921625852584839
train gradient:  0.110603161407518
iteration : 7960
train acc:  0.71875
train loss:  0.5396225452423096
train gradient:  0.15300102036177288
iteration : 7961
train acc:  0.7578125
train loss:  0.4949660003185272
train gradient:  0.140384733286438
iteration : 7962
train acc:  0.6953125
train loss:  0.5281504392623901
train gradient:  0.12512743099625928
iteration : 7963
train acc:  0.7421875
train loss:  0.4635782539844513
train gradient:  0.10995549965195907
iteration : 7964
train acc:  0.8125
train loss:  0.46654751896858215
train gradient:  0.14589927461630495
iteration : 7965
train acc:  0.7578125
train loss:  0.5307208299636841
train gradient:  0.12306598011579348
iteration : 7966
train acc:  0.765625
train loss:  0.48817554116249084
train gradient:  0.1386188542918546
iteration : 7967
train acc:  0.6796875
train loss:  0.5638743042945862
train gradient:  0.150204944624503
iteration : 7968
train acc:  0.671875
train loss:  0.5436244010925293
train gradient:  0.1455234920582873
iteration : 7969
train acc:  0.7421875
train loss:  0.4781329035758972
train gradient:  0.12591182766533696
iteration : 7970
train acc:  0.6875
train loss:  0.5449139475822449
train gradient:  0.15759802345819324
iteration : 7971
train acc:  0.7265625
train loss:  0.48352473974227905
train gradient:  0.12984707021650843
iteration : 7972
train acc:  0.765625
train loss:  0.4796334505081177
train gradient:  0.11753755150268282
iteration : 7973
train acc:  0.703125
train loss:  0.5505466461181641
train gradient:  0.21623370513234091
iteration : 7974
train acc:  0.7421875
train loss:  0.44994956254959106
train gradient:  0.11278299069674644
iteration : 7975
train acc:  0.7421875
train loss:  0.5102002024650574
train gradient:  0.1274104666606854
iteration : 7976
train acc:  0.6796875
train loss:  0.49066662788391113
train gradient:  0.14712546257015857
iteration : 7977
train acc:  0.7109375
train loss:  0.5043240189552307
train gradient:  0.10937764432502578
iteration : 7978
train acc:  0.7734375
train loss:  0.42148059606552124
train gradient:  0.10598576078993742
iteration : 7979
train acc:  0.78125
train loss:  0.47815507650375366
train gradient:  0.16369349155216567
iteration : 7980
train acc:  0.765625
train loss:  0.4740520417690277
train gradient:  0.1437484038065952
iteration : 7981
train acc:  0.6875
train loss:  0.5642115473747253
train gradient:  0.17069756195703328
iteration : 7982
train acc:  0.734375
train loss:  0.5272339582443237
train gradient:  0.12539847825252773
iteration : 7983
train acc:  0.765625
train loss:  0.4734477400779724
train gradient:  0.13026471775727147
iteration : 7984
train acc:  0.71875
train loss:  0.5046628713607788
train gradient:  0.12776032740415877
iteration : 7985
train acc:  0.7734375
train loss:  0.5349464416503906
train gradient:  0.13954445200814092
iteration : 7986
train acc:  0.734375
train loss:  0.5386494398117065
train gradient:  0.16570365072792254
iteration : 7987
train acc:  0.7734375
train loss:  0.4468919038772583
train gradient:  0.11401018498387865
iteration : 7988
train acc:  0.6953125
train loss:  0.5084784626960754
train gradient:  0.13171061303811077
iteration : 7989
train acc:  0.6953125
train loss:  0.5863950848579407
train gradient:  0.13293603634396017
iteration : 7990
train acc:  0.7109375
train loss:  0.5004587173461914
train gradient:  0.16685644523775356
iteration : 7991
train acc:  0.75
train loss:  0.48149028420448303
train gradient:  0.14120195554220488
iteration : 7992
train acc:  0.734375
train loss:  0.535637617111206
train gradient:  0.17528584472497893
iteration : 7993
train acc:  0.765625
train loss:  0.4505534768104553
train gradient:  0.11215433294951459
iteration : 7994
train acc:  0.75
train loss:  0.5225644111633301
train gradient:  0.15483002033295887
iteration : 7995
train acc:  0.7265625
train loss:  0.5202875137329102
train gradient:  0.12313867407991096
iteration : 7996
train acc:  0.7734375
train loss:  0.4856563210487366
train gradient:  0.12070338931879282
iteration : 7997
train acc:  0.6875
train loss:  0.5247845649719238
train gradient:  0.12514087232345802
iteration : 7998
train acc:  0.734375
train loss:  0.5003126859664917
train gradient:  0.12236559069762812
iteration : 7999
train acc:  0.7578125
train loss:  0.46841591596603394
train gradient:  0.14759867818610428
iteration : 8000
train acc:  0.734375
train loss:  0.5570539236068726
train gradient:  0.16382170675169577
iteration : 8001
train acc:  0.6875
train loss:  0.5145055651664734
train gradient:  0.13670676196587
iteration : 8002
train acc:  0.6875
train loss:  0.5580736398696899
train gradient:  0.15161538829720558
iteration : 8003
train acc:  0.7734375
train loss:  0.45593345165252686
train gradient:  0.11087690196252378
iteration : 8004
train acc:  0.75
train loss:  0.453815758228302
train gradient:  0.13014158677346674
iteration : 8005
train acc:  0.7421875
train loss:  0.5194865465164185
train gradient:  0.15279498970200617
iteration : 8006
train acc:  0.7734375
train loss:  0.4297584593296051
train gradient:  0.10064731820232031
iteration : 8007
train acc:  0.7421875
train loss:  0.502530574798584
train gradient:  0.14480980022975587
iteration : 8008
train acc:  0.6484375
train loss:  0.5963845252990723
train gradient:  0.22515126203500224
iteration : 8009
train acc:  0.75
train loss:  0.5280717611312866
train gradient:  0.13803000745678964
iteration : 8010
train acc:  0.7578125
train loss:  0.5206196308135986
train gradient:  0.15670028286799287
iteration : 8011
train acc:  0.7578125
train loss:  0.45324987173080444
train gradient:  0.12771739755001832
iteration : 8012
train acc:  0.8203125
train loss:  0.39686521887779236
train gradient:  0.08164901746067436
iteration : 8013
train acc:  0.6875
train loss:  0.5440859794616699
train gradient:  0.12898431648261854
iteration : 8014
train acc:  0.765625
train loss:  0.4899635910987854
train gradient:  0.11003316098833557
iteration : 8015
train acc:  0.6875
train loss:  0.5807526111602783
train gradient:  0.17489453681627803
iteration : 8016
train acc:  0.6328125
train loss:  0.5614901185035706
train gradient:  0.1832223157212184
iteration : 8017
train acc:  0.75
train loss:  0.5375434160232544
train gradient:  0.17892224513087673
iteration : 8018
train acc:  0.734375
train loss:  0.48935574293136597
train gradient:  0.11946596807794761
iteration : 8019
train acc:  0.8125
train loss:  0.4713502526283264
train gradient:  0.12018668688414696
iteration : 8020
train acc:  0.71875
train loss:  0.5387468338012695
train gradient:  0.16165783722287086
iteration : 8021
train acc:  0.78125
train loss:  0.4611668884754181
train gradient:  0.10522965316116216
iteration : 8022
train acc:  0.8046875
train loss:  0.45549654960632324
train gradient:  0.1041296559773105
iteration : 8023
train acc:  0.7578125
train loss:  0.47704625129699707
train gradient:  0.13255977724511958
iteration : 8024
train acc:  0.7734375
train loss:  0.4767341911792755
train gradient:  0.10991158231151593
iteration : 8025
train acc:  0.703125
train loss:  0.5317791700363159
train gradient:  0.14919036537655916
iteration : 8026
train acc:  0.75
train loss:  0.4469620883464813
train gradient:  0.08255550975252238
iteration : 8027
train acc:  0.6953125
train loss:  0.5197863578796387
train gradient:  0.16647187030868077
iteration : 8028
train acc:  0.75
train loss:  0.4850925803184509
train gradient:  0.13226522740204247
iteration : 8029
train acc:  0.7578125
train loss:  0.517861545085907
train gradient:  0.14875418185084227
iteration : 8030
train acc:  0.6875
train loss:  0.5818844437599182
train gradient:  0.1731834196798614
iteration : 8031
train acc:  0.78125
train loss:  0.47440963983535767
train gradient:  0.13499313525692422
iteration : 8032
train acc:  0.765625
train loss:  0.48119181394577026
train gradient:  0.10689178518533378
iteration : 8033
train acc:  0.7578125
train loss:  0.4554596245288849
train gradient:  0.1273288866225729
iteration : 8034
train acc:  0.7578125
train loss:  0.5184618234634399
train gradient:  0.12505513072073607
iteration : 8035
train acc:  0.8203125
train loss:  0.4219053089618683
train gradient:  0.10483227165583442
iteration : 8036
train acc:  0.7890625
train loss:  0.4419592618942261
train gradient:  0.13420013145024845
iteration : 8037
train acc:  0.765625
train loss:  0.49765920639038086
train gradient:  0.14709792418407977
iteration : 8038
train acc:  0.6796875
train loss:  0.532354474067688
train gradient:  0.13695943348073455
iteration : 8039
train acc:  0.765625
train loss:  0.484103262424469
train gradient:  0.10859912496753707
iteration : 8040
train acc:  0.75
train loss:  0.5572422742843628
train gradient:  0.16085695328991834
iteration : 8041
train acc:  0.7421875
train loss:  0.4708690345287323
train gradient:  0.13987561648398006
iteration : 8042
train acc:  0.7109375
train loss:  0.5223795175552368
train gradient:  0.10896990976886507
iteration : 8043
train acc:  0.7578125
train loss:  0.478843629360199
train gradient:  0.12526918886454672
iteration : 8044
train acc:  0.7734375
train loss:  0.5160731673240662
train gradient:  0.1226359836366572
iteration : 8045
train acc:  0.8046875
train loss:  0.4449498653411865
train gradient:  0.11289126367581738
iteration : 8046
train acc:  0.7421875
train loss:  0.5745091438293457
train gradient:  0.16297420234730453
iteration : 8047
train acc:  0.765625
train loss:  0.48883694410324097
train gradient:  0.13284811866927196
iteration : 8048
train acc:  0.765625
train loss:  0.473524272441864
train gradient:  0.11420463976577709
iteration : 8049
train acc:  0.7421875
train loss:  0.4860967993736267
train gradient:  0.13939165113319713
iteration : 8050
train acc:  0.703125
train loss:  0.5528185367584229
train gradient:  0.16861085897668449
iteration : 8051
train acc:  0.6796875
train loss:  0.6000093817710876
train gradient:  0.15317011000391595
iteration : 8052
train acc:  0.6953125
train loss:  0.5237277746200562
train gradient:  0.12095355983768066
iteration : 8053
train acc:  0.7890625
train loss:  0.4423357844352722
train gradient:  0.13149482133476242
iteration : 8054
train acc:  0.6875
train loss:  0.5649780035018921
train gradient:  0.174212138396095
iteration : 8055
train acc:  0.78125
train loss:  0.45635730028152466
train gradient:  0.11342133882539596
iteration : 8056
train acc:  0.6015625
train loss:  0.6189572811126709
train gradient:  0.1737352335365061
iteration : 8057
train acc:  0.8046875
train loss:  0.4253830313682556
train gradient:  0.08468523708156202
iteration : 8058
train acc:  0.8046875
train loss:  0.43116769194602966
train gradient:  0.09542614285863339
iteration : 8059
train acc:  0.7265625
train loss:  0.48465707898139954
train gradient:  0.11288019805626151
iteration : 8060
train acc:  0.71875
train loss:  0.5095260143280029
train gradient:  0.14508581124081255
iteration : 8061
train acc:  0.71875
train loss:  0.5442594289779663
train gradient:  0.15827812728642549
iteration : 8062
train acc:  0.7578125
train loss:  0.47300344705581665
train gradient:  0.12240623185949312
iteration : 8063
train acc:  0.6953125
train loss:  0.5499100089073181
train gradient:  0.16879720950998628
iteration : 8064
train acc:  0.7890625
train loss:  0.4755983352661133
train gradient:  0.1487979251737771
iteration : 8065
train acc:  0.7578125
train loss:  0.5008211731910706
train gradient:  0.12181568942869896
iteration : 8066
train acc:  0.734375
train loss:  0.5051620006561279
train gradient:  0.13597040031734153
iteration : 8067
train acc:  0.7265625
train loss:  0.5606663227081299
train gradient:  0.14867839904294705
iteration : 8068
train acc:  0.75
train loss:  0.5265203714370728
train gradient:  0.1801989532190357
iteration : 8069
train acc:  0.734375
train loss:  0.47941854596138
train gradient:  0.14177822315942412
iteration : 8070
train acc:  0.71875
train loss:  0.5374056696891785
train gradient:  0.13336298023897236
iteration : 8071
train acc:  0.7421875
train loss:  0.4901820123195648
train gradient:  0.140238338520795
iteration : 8072
train acc:  0.7890625
train loss:  0.46098706126213074
train gradient:  0.10386532504020814
iteration : 8073
train acc:  0.703125
train loss:  0.5171688795089722
train gradient:  0.12815570547707975
iteration : 8074
train acc:  0.71875
train loss:  0.5088781714439392
train gradient:  0.1560620299834253
iteration : 8075
train acc:  0.75
train loss:  0.4690895676612854
train gradient:  0.17457886334022987
iteration : 8076
train acc:  0.7578125
train loss:  0.49750837683677673
train gradient:  0.12686789622134148
iteration : 8077
train acc:  0.7734375
train loss:  0.4510592818260193
train gradient:  0.09625890171196211
iteration : 8078
train acc:  0.7578125
train loss:  0.5035871267318726
train gradient:  0.14934146378903979
iteration : 8079
train acc:  0.7578125
train loss:  0.4562346935272217
train gradient:  0.09018544099980111
iteration : 8080
train acc:  0.7734375
train loss:  0.4771398901939392
train gradient:  0.11030052201876508
iteration : 8081
train acc:  0.7734375
train loss:  0.4471372961997986
train gradient:  0.13077566913593133
iteration : 8082
train acc:  0.7578125
train loss:  0.5278874039649963
train gradient:  0.17378984083991078
iteration : 8083
train acc:  0.6953125
train loss:  0.5157291293144226
train gradient:  0.15250092219220407
iteration : 8084
train acc:  0.734375
train loss:  0.5379704236984253
train gradient:  0.16417235390612206
iteration : 8085
train acc:  0.7734375
train loss:  0.4844202399253845
train gradient:  0.18917200089864283
iteration : 8086
train acc:  0.7421875
train loss:  0.4918469488620758
train gradient:  0.12971000329124754
iteration : 8087
train acc:  0.71875
train loss:  0.5305873155593872
train gradient:  0.1321252736282645
iteration : 8088
train acc:  0.7734375
train loss:  0.4685383141040802
train gradient:  0.10762162657187152
iteration : 8089
train acc:  0.75
train loss:  0.5008862018585205
train gradient:  0.11655171891959115
iteration : 8090
train acc:  0.7578125
train loss:  0.5007774829864502
train gradient:  0.12114780668504865
iteration : 8091
train acc:  0.7265625
train loss:  0.49586015939712524
train gradient:  0.11919269869848148
iteration : 8092
train acc:  0.6953125
train loss:  0.5374120473861694
train gradient:  0.14716631754387882
iteration : 8093
train acc:  0.734375
train loss:  0.5101077556610107
train gradient:  0.1315346101790319
iteration : 8094
train acc:  0.6875
train loss:  0.5366126298904419
train gradient:  0.1450875141376064
iteration : 8095
train acc:  0.7109375
train loss:  0.5028090476989746
train gradient:  0.1302205740241439
iteration : 8096
train acc:  0.7578125
train loss:  0.4858633875846863
train gradient:  0.11930506629346262
iteration : 8097
train acc:  0.7265625
train loss:  0.5265059471130371
train gradient:  0.13883652412742564
iteration : 8098
train acc:  0.7578125
train loss:  0.46780815720558167
train gradient:  0.14552132910762305
iteration : 8099
train acc:  0.7109375
train loss:  0.5399578809738159
train gradient:  0.1517897973255487
iteration : 8100
train acc:  0.765625
train loss:  0.47052764892578125
train gradient:  0.13313914510852246
iteration : 8101
train acc:  0.78125
train loss:  0.49091091752052307
train gradient:  0.1473122863589796
iteration : 8102
train acc:  0.7109375
train loss:  0.4931698441505432
train gradient:  0.13416705640083193
iteration : 8103
train acc:  0.796875
train loss:  0.4402882754802704
train gradient:  0.09978800577810622
iteration : 8104
train acc:  0.796875
train loss:  0.46167561411857605
train gradient:  0.11408742360318824
iteration : 8105
train acc:  0.71875
train loss:  0.5069805979728699
train gradient:  0.19232456080931015
iteration : 8106
train acc:  0.6875
train loss:  0.5182418823242188
train gradient:  0.11737871817220881
iteration : 8107
train acc:  0.703125
train loss:  0.5378828048706055
train gradient:  0.14870583360392053
iteration : 8108
train acc:  0.7578125
train loss:  0.4683763384819031
train gradient:  0.14515027284200954
iteration : 8109
train acc:  0.71875
train loss:  0.5116008520126343
train gradient:  0.1453036118573844
iteration : 8110
train acc:  0.734375
train loss:  0.49897632002830505
train gradient:  0.14587580980097611
iteration : 8111
train acc:  0.765625
train loss:  0.4935849606990814
train gradient:  0.12234335307169225
iteration : 8112
train acc:  0.7109375
train loss:  0.47881317138671875
train gradient:  0.12963980073547524
iteration : 8113
train acc:  0.6875
train loss:  0.5180846452713013
train gradient:  0.13458195674529116
iteration : 8114
train acc:  0.8125
train loss:  0.44653621315956116
train gradient:  0.12691054694691617
iteration : 8115
train acc:  0.71875
train loss:  0.5316165685653687
train gradient:  0.14514235418334595
iteration : 8116
train acc:  0.765625
train loss:  0.4854341149330139
train gradient:  0.14399552813195882
iteration : 8117
train acc:  0.7578125
train loss:  0.4674327075481415
train gradient:  0.12008473940365703
iteration : 8118
train acc:  0.7734375
train loss:  0.45789146423339844
train gradient:  0.09770595867586411
iteration : 8119
train acc:  0.734375
train loss:  0.543366551399231
train gradient:  0.18946740344088786
iteration : 8120
train acc:  0.6875
train loss:  0.5243117809295654
train gradient:  0.14444394873879757
iteration : 8121
train acc:  0.75
train loss:  0.5162776708602905
train gradient:  0.1382979077349758
iteration : 8122
train acc:  0.7578125
train loss:  0.4942944347858429
train gradient:  0.12925385099928718
iteration : 8123
train acc:  0.765625
train loss:  0.4650574326515198
train gradient:  0.10186090421044366
iteration : 8124
train acc:  0.71875
train loss:  0.5631779432296753
train gradient:  0.17162322852493844
iteration : 8125
train acc:  0.71875
train loss:  0.47416260838508606
train gradient:  0.10591332061760787
iteration : 8126
train acc:  0.75
train loss:  0.49343204498291016
train gradient:  0.11923386380492994
iteration : 8127
train acc:  0.7578125
train loss:  0.4704914093017578
train gradient:  0.12233477841733847
iteration : 8128
train acc:  0.7578125
train loss:  0.44915831089019775
train gradient:  0.10975429822348619
iteration : 8129
train acc:  0.671875
train loss:  0.6039326786994934
train gradient:  0.16146662108733226
iteration : 8130
train acc:  0.796875
train loss:  0.41077837347984314
train gradient:  0.10931060575212125
iteration : 8131
train acc:  0.7578125
train loss:  0.46997857093811035
train gradient:  0.10786004919976937
iteration : 8132
train acc:  0.6875
train loss:  0.5000830888748169
train gradient:  0.15357189318763564
iteration : 8133
train acc:  0.7109375
train loss:  0.5467565059661865
train gradient:  0.15507498983503015
iteration : 8134
train acc:  0.7578125
train loss:  0.5126428008079529
train gradient:  0.15519251957178581
iteration : 8135
train acc:  0.765625
train loss:  0.51747727394104
train gradient:  0.12770854161334588
iteration : 8136
train acc:  0.7734375
train loss:  0.5047035813331604
train gradient:  0.1359913616083848
iteration : 8137
train acc:  0.6953125
train loss:  0.5003858804702759
train gradient:  0.14587113991278328
iteration : 8138
train acc:  0.6953125
train loss:  0.5344860553741455
train gradient:  0.13547385555734198
iteration : 8139
train acc:  0.7265625
train loss:  0.5189578533172607
train gradient:  0.14913067375247985
iteration : 8140
train acc:  0.7734375
train loss:  0.5343775749206543
train gradient:  0.14098007086791536
iteration : 8141
train acc:  0.6953125
train loss:  0.5235880017280579
train gradient:  0.15037454755137103
iteration : 8142
train acc:  0.75
train loss:  0.5229737758636475
train gradient:  0.1470330748328132
iteration : 8143
train acc:  0.703125
train loss:  0.5396867394447327
train gradient:  0.14714072809947804
iteration : 8144
train acc:  0.6953125
train loss:  0.5726484060287476
train gradient:  0.15125041256183086
iteration : 8145
train acc:  0.703125
train loss:  0.5141608715057373
train gradient:  0.1396613266449357
iteration : 8146
train acc:  0.7734375
train loss:  0.4535945653915405
train gradient:  0.13897315210094957
iteration : 8147
train acc:  0.75
train loss:  0.48367151618003845
train gradient:  0.13532395807795447
iteration : 8148
train acc:  0.7421875
train loss:  0.44543808698654175
train gradient:  0.12238907598325452
iteration : 8149
train acc:  0.7734375
train loss:  0.46478262543678284
train gradient:  0.10659897940559293
iteration : 8150
train acc:  0.734375
train loss:  0.48003312945365906
train gradient:  0.09741275444287985
iteration : 8151
train acc:  0.75
train loss:  0.4754202961921692
train gradient:  0.10236366631420005
iteration : 8152
train acc:  0.7578125
train loss:  0.5169963836669922
train gradient:  0.1354561055888499
iteration : 8153
train acc:  0.78125
train loss:  0.46504122018814087
train gradient:  0.13699081036046418
iteration : 8154
train acc:  0.7578125
train loss:  0.4647759795188904
train gradient:  0.1187751734248275
iteration : 8155
train acc:  0.7734375
train loss:  0.45779502391815186
train gradient:  0.10084307300721376
iteration : 8156
train acc:  0.71875
train loss:  0.5321312546730042
train gradient:  0.1327667038286094
iteration : 8157
train acc:  0.7421875
train loss:  0.51429283618927
train gradient:  0.14072860317200436
iteration : 8158
train acc:  0.7734375
train loss:  0.4853867292404175
train gradient:  0.14576693365213197
iteration : 8159
train acc:  0.78125
train loss:  0.49872642755508423
train gradient:  0.12966604257378916
iteration : 8160
train acc:  0.7890625
train loss:  0.4773889183998108
train gradient:  0.1464471808831233
iteration : 8161
train acc:  0.7265625
train loss:  0.4847904145717621
train gradient:  0.12460120191042524
iteration : 8162
train acc:  0.765625
train loss:  0.4883941411972046
train gradient:  0.1708092567296457
iteration : 8163
train acc:  0.7109375
train loss:  0.5602853894233704
train gradient:  0.16468129733993392
iteration : 8164
train acc:  0.7109375
train loss:  0.5106950402259827
train gradient:  0.12242664560319078
iteration : 8165
train acc:  0.71875
train loss:  0.5147415399551392
train gradient:  0.16602628736947783
iteration : 8166
train acc:  0.8046875
train loss:  0.4348214566707611
train gradient:  0.09889176387215848
iteration : 8167
train acc:  0.7578125
train loss:  0.4892492890357971
train gradient:  0.11833068606718868
iteration : 8168
train acc:  0.75
train loss:  0.5011183619499207
train gradient:  0.12388519513449742
iteration : 8169
train acc:  0.7265625
train loss:  0.47131383419036865
train gradient:  0.13962340779127103
iteration : 8170
train acc:  0.796875
train loss:  0.48989933729171753
train gradient:  0.146379178316629
iteration : 8171
train acc:  0.703125
train loss:  0.5358009338378906
train gradient:  0.14378495976885414
iteration : 8172
train acc:  0.6953125
train loss:  0.5157663822174072
train gradient:  0.16276732420296364
iteration : 8173
train acc:  0.71875
train loss:  0.527138352394104
train gradient:  0.12735295052669532
iteration : 8174
train acc:  0.765625
train loss:  0.4766453504562378
train gradient:  0.1235787492789434
iteration : 8175
train acc:  0.828125
train loss:  0.42087027430534363
train gradient:  0.09467648069093065
iteration : 8176
train acc:  0.7890625
train loss:  0.4990279972553253
train gradient:  0.12745130780133546
iteration : 8177
train acc:  0.7421875
train loss:  0.5492274165153503
train gradient:  0.16538902734761907
iteration : 8178
train acc:  0.75
train loss:  0.488749623298645
train gradient:  0.14702780178159455
iteration : 8179
train acc:  0.7734375
train loss:  0.4993797540664673
train gradient:  0.1570100199827284
iteration : 8180
train acc:  0.7109375
train loss:  0.4865140914916992
train gradient:  0.14599151740081623
iteration : 8181
train acc:  0.6640625
train loss:  0.5211198329925537
train gradient:  0.16346049206939317
iteration : 8182
train acc:  0.7109375
train loss:  0.5090801119804382
train gradient:  0.13839370472266244
iteration : 8183
train acc:  0.796875
train loss:  0.4145317077636719
train gradient:  0.1060840174532313
iteration : 8184
train acc:  0.7265625
train loss:  0.5541983842849731
train gradient:  0.13289859133470822
iteration : 8185
train acc:  0.7265625
train loss:  0.5566266179084778
train gradient:  0.15849727561933852
iteration : 8186
train acc:  0.6953125
train loss:  0.5689626336097717
train gradient:  0.16876515580882767
iteration : 8187
train acc:  0.765625
train loss:  0.46608585119247437
train gradient:  0.1431312118507389
iteration : 8188
train acc:  0.65625
train loss:  0.6013997197151184
train gradient:  0.20986888168988932
iteration : 8189
train acc:  0.765625
train loss:  0.4765923321247101
train gradient:  0.12316906098615271
iteration : 8190
train acc:  0.6875
train loss:  0.578956127166748
train gradient:  0.1462545975355733
iteration : 8191
train acc:  0.703125
train loss:  0.5248876214027405
train gradient:  0.17056576806636647
iteration : 8192
train acc:  0.7578125
train loss:  0.49988865852355957
train gradient:  0.13129417236773178
iteration : 8193
train acc:  0.75
train loss:  0.4905819296836853
train gradient:  0.13476758172432368
iteration : 8194
train acc:  0.765625
train loss:  0.4736695885658264
train gradient:  0.1202340059306735
iteration : 8195
train acc:  0.734375
train loss:  0.5428069233894348
train gradient:  0.1510086614403221
iteration : 8196
train acc:  0.8203125
train loss:  0.4283812642097473
train gradient:  0.1029940558540849
iteration : 8197
train acc:  0.75
train loss:  0.5376230478286743
train gradient:  0.12741944150558132
iteration : 8198
train acc:  0.7265625
train loss:  0.5304908752441406
train gradient:  0.2054237473598775
iteration : 8199
train acc:  0.734375
train loss:  0.5171394348144531
train gradient:  0.13112202623819444
iteration : 8200
train acc:  0.7578125
train loss:  0.4781458079814911
train gradient:  0.12466900669669023
iteration : 8201
train acc:  0.7734375
train loss:  0.4754354953765869
train gradient:  0.12337658216117661
iteration : 8202
train acc:  0.78125
train loss:  0.4951777458190918
train gradient:  0.12506402769316466
iteration : 8203
train acc:  0.7578125
train loss:  0.47525036334991455
train gradient:  0.13000342207567772
iteration : 8204
train acc:  0.7421875
train loss:  0.5018029808998108
train gradient:  0.11480696971585014
iteration : 8205
train acc:  0.7265625
train loss:  0.5377750992774963
train gradient:  0.14549928417825667
iteration : 8206
train acc:  0.71875
train loss:  0.5613877177238464
train gradient:  0.19881343184631634
iteration : 8207
train acc:  0.7265625
train loss:  0.4937736392021179
train gradient:  0.1508750222928351
iteration : 8208
train acc:  0.75
train loss:  0.480901837348938
train gradient:  0.11770302408338935
iteration : 8209
train acc:  0.734375
train loss:  0.508654773235321
train gradient:  0.14067348112373357
iteration : 8210
train acc:  0.78125
train loss:  0.47045546770095825
train gradient:  0.12017128146245463
iteration : 8211
train acc:  0.7734375
train loss:  0.446348637342453
train gradient:  0.11388691968217265
iteration : 8212
train acc:  0.7578125
train loss:  0.468141108751297
train gradient:  0.10338299187121497
iteration : 8213
train acc:  0.734375
train loss:  0.5526305437088013
train gradient:  0.1549180069532149
iteration : 8214
train acc:  0.7109375
train loss:  0.5216000080108643
train gradient:  0.11979999297776638
iteration : 8215
train acc:  0.734375
train loss:  0.5097295641899109
train gradient:  0.17386131379266756
iteration : 8216
train acc:  0.7265625
train loss:  0.5036245584487915
train gradient:  0.1665779383062853
iteration : 8217
train acc:  0.78125
train loss:  0.48534008860588074
train gradient:  0.14957263584665614
iteration : 8218
train acc:  0.78125
train loss:  0.44994038343429565
train gradient:  0.11342872707486587
iteration : 8219
train acc:  0.6953125
train loss:  0.5641615390777588
train gradient:  0.15166053179887118
iteration : 8220
train acc:  0.671875
train loss:  0.555186927318573
train gradient:  0.15173928095906852
iteration : 8221
train acc:  0.703125
train loss:  0.5246239304542542
train gradient:  0.16620909878681137
iteration : 8222
train acc:  0.703125
train loss:  0.5641838312149048
train gradient:  0.15978328850279183
iteration : 8223
train acc:  0.7578125
train loss:  0.520098090171814
train gradient:  0.15589574885326496
iteration : 8224
train acc:  0.7421875
train loss:  0.529381275177002
train gradient:  0.14628811334906727
iteration : 8225
train acc:  0.7109375
train loss:  0.5528351664543152
train gradient:  0.1725123503204676
iteration : 8226
train acc:  0.7265625
train loss:  0.5241719484329224
train gradient:  0.16613340551452171
iteration : 8227
train acc:  0.7734375
train loss:  0.47020870447158813
train gradient:  0.12305950185181737
iteration : 8228
train acc:  0.7109375
train loss:  0.5296552181243896
train gradient:  0.1287068844641305
iteration : 8229
train acc:  0.7578125
train loss:  0.45059826970100403
train gradient:  0.11202091976730154
iteration : 8230
train acc:  0.71875
train loss:  0.4882117807865143
train gradient:  0.1116709367668687
iteration : 8231
train acc:  0.7265625
train loss:  0.48640236258506775
train gradient:  0.11579128316066914
iteration : 8232
train acc:  0.7421875
train loss:  0.5339984893798828
train gradient:  0.14906231525045788
iteration : 8233
train acc:  0.7734375
train loss:  0.46171727776527405
train gradient:  0.11202544667129265
iteration : 8234
train acc:  0.7109375
train loss:  0.5331734418869019
train gradient:  0.17180621761388576
iteration : 8235
train acc:  0.703125
train loss:  0.5244609117507935
train gradient:  0.15058430583679983
iteration : 8236
train acc:  0.71875
train loss:  0.5449935793876648
train gradient:  0.13638319513630084
iteration : 8237
train acc:  0.7890625
train loss:  0.44273972511291504
train gradient:  0.12145081096480383
iteration : 8238
train acc:  0.7421875
train loss:  0.5248436331748962
train gradient:  0.16366518420937226
iteration : 8239
train acc:  0.78125
train loss:  0.4834560751914978
train gradient:  0.13714193264898483
iteration : 8240
train acc:  0.6796875
train loss:  0.5272500514984131
train gradient:  0.14490048521804166
iteration : 8241
train acc:  0.7578125
train loss:  0.5107424259185791
train gradient:  0.12387128388196153
iteration : 8242
train acc:  0.8203125
train loss:  0.4549575448036194
train gradient:  0.13137040701114472
iteration : 8243
train acc:  0.7890625
train loss:  0.4723888039588928
train gradient:  0.13557444937422103
iteration : 8244
train acc:  0.8046875
train loss:  0.4315783679485321
train gradient:  0.11044089062120382
iteration : 8245
train acc:  0.7734375
train loss:  0.48306307196617126
train gradient:  0.12775215655276234
iteration : 8246
train acc:  0.703125
train loss:  0.4955447018146515
train gradient:  0.18769207696716017
iteration : 8247
train acc:  0.734375
train loss:  0.5889825820922852
train gradient:  0.19097409993221265
iteration : 8248
train acc:  0.8046875
train loss:  0.4503421187400818
train gradient:  0.144673144763944
iteration : 8249
train acc:  0.765625
train loss:  0.4642093777656555
train gradient:  0.12651350647347306
iteration : 8250
train acc:  0.7734375
train loss:  0.4509580731391907
train gradient:  0.09989198806371413
iteration : 8251
train acc:  0.71875
train loss:  0.5170653462409973
train gradient:  0.1345342465948324
iteration : 8252
train acc:  0.828125
train loss:  0.4149208068847656
train gradient:  0.09585266747346848
iteration : 8253
train acc:  0.78125
train loss:  0.44865378737449646
train gradient:  0.11739491704617955
iteration : 8254
train acc:  0.7421875
train loss:  0.4911569654941559
train gradient:  0.11113321851963023
iteration : 8255
train acc:  0.7109375
train loss:  0.48393309116363525
train gradient:  0.12069046223043113
iteration : 8256
train acc:  0.6953125
train loss:  0.5506125688552856
train gradient:  0.18137501021401065
iteration : 8257
train acc:  0.8046875
train loss:  0.43819326162338257
train gradient:  0.10582168762346383
iteration : 8258
train acc:  0.6484375
train loss:  0.5450481176376343
train gradient:  0.1482367800352061
iteration : 8259
train acc:  0.734375
train loss:  0.52754145860672
train gradient:  0.14109063008495198
iteration : 8260
train acc:  0.734375
train loss:  0.472442626953125
train gradient:  0.12869906540576828
iteration : 8261
train acc:  0.671875
train loss:  0.6260976791381836
train gradient:  0.23705654638508794
iteration : 8262
train acc:  0.7421875
train loss:  0.4926705062389374
train gradient:  0.13215137852885395
iteration : 8263
train acc:  0.765625
train loss:  0.5094261169433594
train gradient:  0.15209838894575572
iteration : 8264
train acc:  0.7578125
train loss:  0.4779466390609741
train gradient:  0.14012664791885227
iteration : 8265
train acc:  0.7265625
train loss:  0.5861917734146118
train gradient:  0.18035026143478733
iteration : 8266
train acc:  0.734375
train loss:  0.5236548185348511
train gradient:  0.1612672467759257
iteration : 8267
train acc:  0.796875
train loss:  0.4299333095550537
train gradient:  0.09071821476230384
iteration : 8268
train acc:  0.7265625
train loss:  0.4933042526245117
train gradient:  0.1296860669311425
iteration : 8269
train acc:  0.7421875
train loss:  0.4683506190776825
train gradient:  0.11279398125043748
iteration : 8270
train acc:  0.8125
train loss:  0.45737069845199585
train gradient:  0.11868603006099854
iteration : 8271
train acc:  0.703125
train loss:  0.5106143951416016
train gradient:  0.13132801845275738
iteration : 8272
train acc:  0.703125
train loss:  0.49457427859306335
train gradient:  0.11704520335345105
iteration : 8273
train acc:  0.6875
train loss:  0.5401204228401184
train gradient:  0.1453120681893753
iteration : 8274
train acc:  0.703125
train loss:  0.5140380859375
train gradient:  0.18395549117640445
iteration : 8275
train acc:  0.6875
train loss:  0.5269582271575928
train gradient:  0.13138823253827003
iteration : 8276
train acc:  0.7734375
train loss:  0.47688496112823486
train gradient:  0.12786678210545804
iteration : 8277
train acc:  0.8515625
train loss:  0.4251655638217926
train gradient:  0.08977754793214478
iteration : 8278
train acc:  0.703125
train loss:  0.552415132522583
train gradient:  0.17580806992631293
iteration : 8279
train acc:  0.71875
train loss:  0.49913570284843445
train gradient:  0.14527431456366097
iteration : 8280
train acc:  0.6796875
train loss:  0.5658597350120544
train gradient:  0.1610268722491832
iteration : 8281
train acc:  0.71875
train loss:  0.4967100918292999
train gradient:  0.17008396224728933
iteration : 8282
train acc:  0.734375
train loss:  0.5279281139373779
train gradient:  0.12512090613632093
iteration : 8283
train acc:  0.75
train loss:  0.5223817229270935
train gradient:  0.14490275415989648
iteration : 8284
train acc:  0.7109375
train loss:  0.49735984206199646
train gradient:  0.12346263717977189
iteration : 8285
train acc:  0.7109375
train loss:  0.5035428404808044
train gradient:  0.14734852254090153
iteration : 8286
train acc:  0.734375
train loss:  0.4552490711212158
train gradient:  0.12475649450719573
iteration : 8287
train acc:  0.7421875
train loss:  0.45987391471862793
train gradient:  0.12329830952055561
iteration : 8288
train acc:  0.7421875
train loss:  0.4632945656776428
train gradient:  0.14389510291443147
iteration : 8289
train acc:  0.671875
train loss:  0.5642598271369934
train gradient:  0.16021566968851236
iteration : 8290
train acc:  0.7109375
train loss:  0.4880937337875366
train gradient:  0.12088663782423723
iteration : 8291
train acc:  0.703125
train loss:  0.510382890701294
train gradient:  0.14395310523611546
iteration : 8292
train acc:  0.7421875
train loss:  0.5035912990570068
train gradient:  0.12920003291161564
iteration : 8293
train acc:  0.6875
train loss:  0.5562969446182251
train gradient:  0.2824275690273991
iteration : 8294
train acc:  0.7421875
train loss:  0.49396252632141113
train gradient:  0.1469668103810588
iteration : 8295
train acc:  0.7265625
train loss:  0.504846453666687
train gradient:  0.14582966322805657
iteration : 8296
train acc:  0.71875
train loss:  0.4734615683555603
train gradient:  0.10555064814098387
iteration : 8297
train acc:  0.6484375
train loss:  0.5752459168434143
train gradient:  0.2033638628706802
iteration : 8298
train acc:  0.6953125
train loss:  0.4918602406978607
train gradient:  0.12558672327131043
iteration : 8299
train acc:  0.7890625
train loss:  0.4350537657737732
train gradient:  0.11204086553132808
iteration : 8300
train acc:  0.734375
train loss:  0.4892360270023346
train gradient:  0.11865889801983985
iteration : 8301
train acc:  0.71875
train loss:  0.48897814750671387
train gradient:  0.1342095719556224
iteration : 8302
train acc:  0.71875
train loss:  0.48837506771087646
train gradient:  0.10847854989980625
iteration : 8303
train acc:  0.7734375
train loss:  0.5021300315856934
train gradient:  0.1504275095212209
iteration : 8304
train acc:  0.734375
train loss:  0.4906153082847595
train gradient:  0.16249795917742857
iteration : 8305
train acc:  0.7421875
train loss:  0.4731229841709137
train gradient:  0.10280243699728087
iteration : 8306
train acc:  0.734375
train loss:  0.5323727130889893
train gradient:  0.16817608907140935
iteration : 8307
train acc:  0.7109375
train loss:  0.5253052711486816
train gradient:  0.14681899258471753
iteration : 8308
train acc:  0.6953125
train loss:  0.5181321501731873
train gradient:  0.12151725023475043
iteration : 8309
train acc:  0.7265625
train loss:  0.5359784960746765
train gradient:  0.14881414127249776
iteration : 8310
train acc:  0.7578125
train loss:  0.4816492795944214
train gradient:  0.1343932071728965
iteration : 8311
train acc:  0.78125
train loss:  0.4330712556838989
train gradient:  0.09489245321598079
iteration : 8312
train acc:  0.7734375
train loss:  0.5041606426239014
train gradient:  0.13064637597498774
iteration : 8313
train acc:  0.734375
train loss:  0.5406527519226074
train gradient:  0.14005166430249866
iteration : 8314
train acc:  0.78125
train loss:  0.442404180765152
train gradient:  0.12650279009639753
iteration : 8315
train acc:  0.6484375
train loss:  0.6190271377563477
train gradient:  0.1878882876252967
iteration : 8316
train acc:  0.765625
train loss:  0.44335222244262695
train gradient:  0.09114100511603766
iteration : 8317
train acc:  0.71875
train loss:  0.5102596282958984
train gradient:  0.15588289974280858
iteration : 8318
train acc:  0.78125
train loss:  0.49256661534309387
train gradient:  0.13217322945148147
iteration : 8319
train acc:  0.6953125
train loss:  0.55205237865448
train gradient:  0.19682273563722041
iteration : 8320
train acc:  0.671875
train loss:  0.5121444463729858
train gradient:  0.15975995535285725
iteration : 8321
train acc:  0.734375
train loss:  0.5292100310325623
train gradient:  0.13863962278119338
iteration : 8322
train acc:  0.78125
train loss:  0.4422692060470581
train gradient:  0.12114261189856276
iteration : 8323
train acc:  0.71875
train loss:  0.5347779393196106
train gradient:  0.16735901373470335
iteration : 8324
train acc:  0.7265625
train loss:  0.5084109306335449
train gradient:  0.13332476314013866
iteration : 8325
train acc:  0.7578125
train loss:  0.4738868772983551
train gradient:  0.14584569938795022
iteration : 8326
train acc:  0.6875
train loss:  0.5666149854660034
train gradient:  0.15445520432708673
iteration : 8327
train acc:  0.75
train loss:  0.4720477759838104
train gradient:  0.13343468859114038
iteration : 8328
train acc:  0.734375
train loss:  0.5069741010665894
train gradient:  0.13033800331823225
iteration : 8329
train acc:  0.7421875
train loss:  0.4851221442222595
train gradient:  0.12802619474885282
iteration : 8330
train acc:  0.796875
train loss:  0.4959501624107361
train gradient:  0.16412059735146545
iteration : 8331
train acc:  0.734375
train loss:  0.5125678777694702
train gradient:  0.13011072185769607
iteration : 8332
train acc:  0.71875
train loss:  0.5296289324760437
train gradient:  0.18164769452740026
iteration : 8333
train acc:  0.7421875
train loss:  0.476656973361969
train gradient:  0.12363255272952078
iteration : 8334
train acc:  0.8828125
train loss:  0.3586746156215668
train gradient:  0.07456035015197687
iteration : 8335
train acc:  0.7265625
train loss:  0.5941750407218933
train gradient:  0.15155018112446578
iteration : 8336
train acc:  0.8203125
train loss:  0.4564000070095062
train gradient:  0.11790889547504114
iteration : 8337
train acc:  0.71875
train loss:  0.520689070224762
train gradient:  0.14747384823787507
iteration : 8338
train acc:  0.765625
train loss:  0.4563347101211548
train gradient:  0.10762325455162863
iteration : 8339
train acc:  0.796875
train loss:  0.44360601902008057
train gradient:  0.1335742530780149
iteration : 8340
train acc:  0.703125
train loss:  0.5623330473899841
train gradient:  0.1926528965588833
iteration : 8341
train acc:  0.78125
train loss:  0.4354708790779114
train gradient:  0.09665483794827882
iteration : 8342
train acc:  0.703125
train loss:  0.5021710395812988
train gradient:  0.12292172668266914
iteration : 8343
train acc:  0.71875
train loss:  0.5330625772476196
train gradient:  0.19225391122798757
iteration : 8344
train acc:  0.78125
train loss:  0.5232580900192261
train gradient:  0.2203435799915272
iteration : 8345
train acc:  0.671875
train loss:  0.5641645193099976
train gradient:  0.20994522797443926
iteration : 8346
train acc:  0.71875
train loss:  0.5084832906723022
train gradient:  0.17678915644458182
iteration : 8347
train acc:  0.7109375
train loss:  0.4931277930736542
train gradient:  0.18263185330931334
iteration : 8348
train acc:  0.6796875
train loss:  0.618011474609375
train gradient:  0.2142637867474761
iteration : 8349
train acc:  0.7421875
train loss:  0.4538174569606781
train gradient:  0.1058426014367211
iteration : 8350
train acc:  0.7265625
train loss:  0.5441908240318298
train gradient:  0.18149226441096378
iteration : 8351
train acc:  0.671875
train loss:  0.572411298751831
train gradient:  0.17077979407848753
iteration : 8352
train acc:  0.765625
train loss:  0.4802265465259552
train gradient:  0.1655984960879591
iteration : 8353
train acc:  0.7890625
train loss:  0.47165200114250183
train gradient:  0.12564099493399317
iteration : 8354
train acc:  0.796875
train loss:  0.4656159281730652
train gradient:  0.12986591692092384
iteration : 8355
train acc:  0.6875
train loss:  0.5539255738258362
train gradient:  0.1393524251491431
iteration : 8356
train acc:  0.6875
train loss:  0.5503532886505127
train gradient:  0.19367207334075126
iteration : 8357
train acc:  0.6953125
train loss:  0.5634825229644775
train gradient:  0.19816518118559967
iteration : 8358
train acc:  0.71875
train loss:  0.5228268504142761
train gradient:  0.16381111462579767
iteration : 8359
train acc:  0.828125
train loss:  0.47811561822891235
train gradient:  0.11452141740480554
iteration : 8360
train acc:  0.8046875
train loss:  0.46329373121261597
train gradient:  0.1295124897527615
iteration : 8361
train acc:  0.78125
train loss:  0.4509035348892212
train gradient:  0.14048804637110618
iteration : 8362
train acc:  0.734375
train loss:  0.4916805028915405
train gradient:  0.12013618182796836
iteration : 8363
train acc:  0.7578125
train loss:  0.5039551258087158
train gradient:  0.12841323016260336
iteration : 8364
train acc:  0.6796875
train loss:  0.5196055173873901
train gradient:  0.13584219094980632
iteration : 8365
train acc:  0.7421875
train loss:  0.495407372713089
train gradient:  0.16588964813094773
iteration : 8366
train acc:  0.734375
train loss:  0.5361983776092529
train gradient:  0.14606619573054902
iteration : 8367
train acc:  0.7578125
train loss:  0.46688514947891235
train gradient:  0.11599192346237358
iteration : 8368
train acc:  0.8046875
train loss:  0.4332011938095093
train gradient:  0.11970965823060506
iteration : 8369
train acc:  0.6796875
train loss:  0.6098661422729492
train gradient:  0.21454452430384457
iteration : 8370
train acc:  0.78125
train loss:  0.449974000453949
train gradient:  0.1202572148103591
iteration : 8371
train acc:  0.765625
train loss:  0.46104100346565247
train gradient:  0.09309236617267656
iteration : 8372
train acc:  0.7734375
train loss:  0.4173077344894409
train gradient:  0.09967714313100141
iteration : 8373
train acc:  0.7578125
train loss:  0.5395809412002563
train gradient:  0.1111765492048614
iteration : 8374
train acc:  0.734375
train loss:  0.46448349952697754
train gradient:  0.09915599859445288
iteration : 8375
train acc:  0.7734375
train loss:  0.46003085374832153
train gradient:  0.11392747054429968
iteration : 8376
train acc:  0.6953125
train loss:  0.4980452060699463
train gradient:  0.1617772810631141
iteration : 8377
train acc:  0.734375
train loss:  0.520156979560852
train gradient:  0.1217050037128487
iteration : 8378
train acc:  0.7109375
train loss:  0.5657083988189697
train gradient:  0.1870895334705935
iteration : 8379
train acc:  0.7734375
train loss:  0.4988147020339966
train gradient:  0.1162301575934133
iteration : 8380
train acc:  0.7734375
train loss:  0.5428284406661987
train gradient:  0.19733173757466455
iteration : 8381
train acc:  0.7265625
train loss:  0.4989921450614929
train gradient:  0.1101988918530448
iteration : 8382
train acc:  0.75
train loss:  0.4801432490348816
train gradient:  0.12873129555607107
iteration : 8383
train acc:  0.7265625
train loss:  0.5384526252746582
train gradient:  0.18481751670431662
iteration : 8384
train acc:  0.671875
train loss:  0.5249886512756348
train gradient:  0.14160057708742263
iteration : 8385
train acc:  0.78125
train loss:  0.4256094992160797
train gradient:  0.08418888724675347
iteration : 8386
train acc:  0.765625
train loss:  0.5217821598052979
train gradient:  0.14173628582932157
iteration : 8387
train acc:  0.6953125
train loss:  0.5482564568519592
train gradient:  0.2514061834489948
iteration : 8388
train acc:  0.765625
train loss:  0.511529803276062
train gradient:  0.12995238902787343
iteration : 8389
train acc:  0.8203125
train loss:  0.45029985904693604
train gradient:  0.0866517828751733
iteration : 8390
train acc:  0.75
train loss:  0.5500515699386597
train gradient:  0.19442682808946893
iteration : 8391
train acc:  0.7421875
train loss:  0.47781869769096375
train gradient:  0.11703268211791629
iteration : 8392
train acc:  0.7578125
train loss:  0.46473681926727295
train gradient:  0.10636645114403846
iteration : 8393
train acc:  0.75
train loss:  0.48897746205329895
train gradient:  0.16416567187118258
iteration : 8394
train acc:  0.7265625
train loss:  0.5311594009399414
train gradient:  0.1438597691658367
iteration : 8395
train acc:  0.765625
train loss:  0.5453781485557556
train gradient:  0.16591541954213618
iteration : 8396
train acc:  0.7421875
train loss:  0.46157681941986084
train gradient:  0.09458581632016723
iteration : 8397
train acc:  0.65625
train loss:  0.5866235494613647
train gradient:  0.14888038161613565
iteration : 8398
train acc:  0.6640625
train loss:  0.5418021082878113
train gradient:  0.20058392760467153
iteration : 8399
train acc:  0.75
train loss:  0.48437461256980896
train gradient:  0.10692375471257015
iteration : 8400
train acc:  0.7265625
train loss:  0.5078665614128113
train gradient:  0.13069967250772932
iteration : 8401
train acc:  0.7578125
train loss:  0.4989466071128845
train gradient:  0.12292831753065034
iteration : 8402
train acc:  0.78125
train loss:  0.4322693943977356
train gradient:  0.10218534561063503
iteration : 8403
train acc:  0.7265625
train loss:  0.451264888048172
train gradient:  0.12131566257248297
iteration : 8404
train acc:  0.75
train loss:  0.4597581624984741
train gradient:  0.13939530081576973
iteration : 8405
train acc:  0.765625
train loss:  0.503913402557373
train gradient:  0.151598427081907
iteration : 8406
train acc:  0.7734375
train loss:  0.4861728549003601
train gradient:  0.12714825832750762
iteration : 8407
train acc:  0.8359375
train loss:  0.3842923045158386
train gradient:  0.08053274617377165
iteration : 8408
train acc:  0.7265625
train loss:  0.5167686939239502
train gradient:  0.12192011142380327
iteration : 8409
train acc:  0.7734375
train loss:  0.43830108642578125
train gradient:  0.07794902024608742
iteration : 8410
train acc:  0.734375
train loss:  0.5807102918624878
train gradient:  0.18890195359276496
iteration : 8411
train acc:  0.765625
train loss:  0.4438965320587158
train gradient:  0.10904591450621502
iteration : 8412
train acc:  0.7578125
train loss:  0.5021547079086304
train gradient:  0.10566448205547938
iteration : 8413
train acc:  0.71875
train loss:  0.5489972829818726
train gradient:  0.15649461672490853
iteration : 8414
train acc:  0.7421875
train loss:  0.4646316170692444
train gradient:  0.10850751725970763
iteration : 8415
train acc:  0.78125
train loss:  0.4728482961654663
train gradient:  0.12593240991980864
iteration : 8416
train acc:  0.6875
train loss:  0.5652400255203247
train gradient:  0.1800166757318713
iteration : 8417
train acc:  0.78125
train loss:  0.44918638467788696
train gradient:  0.12523678138607816
iteration : 8418
train acc:  0.75
train loss:  0.5090595483779907
train gradient:  0.11441697061476395
iteration : 8419
train acc:  0.734375
train loss:  0.5439313650131226
train gradient:  0.12563028769397241
iteration : 8420
train acc:  0.671875
train loss:  0.5799804329872131
train gradient:  0.16895612533544918
iteration : 8421
train acc:  0.6796875
train loss:  0.526140570640564
train gradient:  0.16597971141418616
iteration : 8422
train acc:  0.6953125
train loss:  0.5400323867797852
train gradient:  0.1297746250569628
iteration : 8423
train acc:  0.7421875
train loss:  0.4888121485710144
train gradient:  0.10311097746990175
iteration : 8424
train acc:  0.65625
train loss:  0.6028941869735718
train gradient:  0.21786652634901238
iteration : 8425
train acc:  0.8125
train loss:  0.4317615032196045
train gradient:  0.09828587651371716
iteration : 8426
train acc:  0.7265625
train loss:  0.4708867073059082
train gradient:  0.1245300034067898
iteration : 8427
train acc:  0.71875
train loss:  0.5207116603851318
train gradient:  0.12869821416357272
iteration : 8428
train acc:  0.7421875
train loss:  0.4829961061477661
train gradient:  0.09694116089130916
iteration : 8429
train acc:  0.78125
train loss:  0.42257532477378845
train gradient:  0.10494213919696942
iteration : 8430
train acc:  0.6875
train loss:  0.5383535623550415
train gradient:  0.19755627887096688
iteration : 8431
train acc:  0.7578125
train loss:  0.5199770331382751
train gradient:  0.12922063370153564
iteration : 8432
train acc:  0.75
train loss:  0.43007323145866394
train gradient:  0.12156613422387962
iteration : 8433
train acc:  0.6953125
train loss:  0.4859093427658081
train gradient:  0.16877583840572336
iteration : 8434
train acc:  0.71875
train loss:  0.5043522119522095
train gradient:  0.12189651394573969
iteration : 8435
train acc:  0.7265625
train loss:  0.5248544812202454
train gradient:  0.15156806777968268
iteration : 8436
train acc:  0.7578125
train loss:  0.4932559132575989
train gradient:  0.14817613806709373
iteration : 8437
train acc:  0.703125
train loss:  0.46880751848220825
train gradient:  0.1000051945390683
iteration : 8438
train acc:  0.765625
train loss:  0.4655512869358063
train gradient:  0.12610378423451643
iteration : 8439
train acc:  0.734375
train loss:  0.5187485814094543
train gradient:  0.1424376427359828
iteration : 8440
train acc:  0.7265625
train loss:  0.5228095650672913
train gradient:  0.14392397881206895
iteration : 8441
train acc:  0.734375
train loss:  0.5190293788909912
train gradient:  0.14795215341007234
iteration : 8442
train acc:  0.6796875
train loss:  0.5710994005203247
train gradient:  0.21350237273036038
iteration : 8443
train acc:  0.71875
train loss:  0.50401371717453
train gradient:  0.1707550315824023
iteration : 8444
train acc:  0.6953125
train loss:  0.5310547351837158
train gradient:  0.10440183791610591
iteration : 8445
train acc:  0.703125
train loss:  0.5273748636245728
train gradient:  0.1385313580818976
iteration : 8446
train acc:  0.78125
train loss:  0.45377516746520996
train gradient:  0.12839590540848134
iteration : 8447
train acc:  0.7265625
train loss:  0.5362569093704224
train gradient:  0.13099849192575197
iteration : 8448
train acc:  0.6640625
train loss:  0.6106864213943481
train gradient:  0.19759597542776122
iteration : 8449
train acc:  0.765625
train loss:  0.43677112460136414
train gradient:  0.12611666709019426
iteration : 8450
train acc:  0.703125
train loss:  0.554222583770752
train gradient:  0.13304386530958365
iteration : 8451
train acc:  0.78125
train loss:  0.5123899579048157
train gradient:  0.15676105341964613
iteration : 8452
train acc:  0.765625
train loss:  0.4869278371334076
train gradient:  0.14157858380999305
iteration : 8453
train acc:  0.765625
train loss:  0.4774596393108368
train gradient:  0.10556878574318278
iteration : 8454
train acc:  0.6953125
train loss:  0.5325486660003662
train gradient:  0.13006728021973005
iteration : 8455
train acc:  0.734375
train loss:  0.48485681414604187
train gradient:  0.1414511569164437
iteration : 8456
train acc:  0.7265625
train loss:  0.5201046466827393
train gradient:  0.13383909046746514
iteration : 8457
train acc:  0.671875
train loss:  0.5470428466796875
train gradient:  0.15964703184214268
iteration : 8458
train acc:  0.734375
train loss:  0.5466830730438232
train gradient:  0.2037673271081471
iteration : 8459
train acc:  0.6953125
train loss:  0.5592687129974365
train gradient:  0.16160745674974777
iteration : 8460
train acc:  0.71875
train loss:  0.5402462482452393
train gradient:  0.19262946198441067
iteration : 8461
train acc:  0.6875
train loss:  0.5301682949066162
train gradient:  0.17633797529209283
iteration : 8462
train acc:  0.71875
train loss:  0.539588451385498
train gradient:  0.16346979216537932
iteration : 8463
train acc:  0.7890625
train loss:  0.4670238494873047
train gradient:  0.13307009327858652
iteration : 8464
train acc:  0.7421875
train loss:  0.5170561075210571
train gradient:  0.15697351997659187
iteration : 8465
train acc:  0.7890625
train loss:  0.45447540283203125
train gradient:  0.13551569438846822
iteration : 8466
train acc:  0.6640625
train loss:  0.564081072807312
train gradient:  0.16316778345780497
iteration : 8467
train acc:  0.7265625
train loss:  0.5279663801193237
train gradient:  0.15227666851051075
iteration : 8468
train acc:  0.7421875
train loss:  0.4857085347175598
train gradient:  0.11105327692836163
iteration : 8469
train acc:  0.7734375
train loss:  0.4705754518508911
train gradient:  0.14254867343747174
iteration : 8470
train acc:  0.6875
train loss:  0.5828285813331604
train gradient:  0.20180573084143288
iteration : 8471
train acc:  0.703125
train loss:  0.6076393723487854
train gradient:  0.19450791902714798
iteration : 8472
train acc:  0.765625
train loss:  0.41287755966186523
train gradient:  0.10128294171391677
iteration : 8473
train acc:  0.7109375
train loss:  0.4821501672267914
train gradient:  0.12811788818981784
iteration : 8474
train acc:  0.765625
train loss:  0.4970988929271698
train gradient:  0.1292547501321591
iteration : 8475
train acc:  0.7734375
train loss:  0.45841559767723083
train gradient:  0.10349700204436328
iteration : 8476
train acc:  0.734375
train loss:  0.5184149742126465
train gradient:  0.11738006161277446
iteration : 8477
train acc:  0.75
train loss:  0.43808138370513916
train gradient:  0.12353278653100865
iteration : 8478
train acc:  0.75
train loss:  0.4807434678077698
train gradient:  0.12287508761897636
iteration : 8479
train acc:  0.734375
train loss:  0.5022344589233398
train gradient:  0.10969027608449317
iteration : 8480
train acc:  0.7421875
train loss:  0.5371083617210388
train gradient:  0.1376382996062303
iteration : 8481
train acc:  0.703125
train loss:  0.5027377009391785
train gradient:  0.1455955861788074
iteration : 8482
train acc:  0.6484375
train loss:  0.5412495136260986
train gradient:  0.17643032206143516
iteration : 8483
train acc:  0.7734375
train loss:  0.49644210934638977
train gradient:  0.14251593399690893
iteration : 8484
train acc:  0.7734375
train loss:  0.4464741051197052
train gradient:  0.11360320210891454
iteration : 8485
train acc:  0.703125
train loss:  0.5430972576141357
train gradient:  0.15546373488295975
iteration : 8486
train acc:  0.734375
train loss:  0.4827374219894409
train gradient:  0.14462852461686876
iteration : 8487
train acc:  0.75
train loss:  0.487690806388855
train gradient:  0.12862231017481657
iteration : 8488
train acc:  0.8125
train loss:  0.4322037994861603
train gradient:  0.105458349645219
iteration : 8489
train acc:  0.71875
train loss:  0.4753926992416382
train gradient:  0.10282860094889801
iteration : 8490
train acc:  0.7890625
train loss:  0.45256343483924866
train gradient:  0.10898036872745875
iteration : 8491
train acc:  0.671875
train loss:  0.583654522895813
train gradient:  0.15117436911444745
iteration : 8492
train acc:  0.6796875
train loss:  0.5083427429199219
train gradient:  0.10289257026018474
iteration : 8493
train acc:  0.7265625
train loss:  0.4941655695438385
train gradient:  0.12657499669217664
iteration : 8494
train acc:  0.7421875
train loss:  0.5241957902908325
train gradient:  0.1156056134857771
iteration : 8495
train acc:  0.703125
train loss:  0.5572733879089355
train gradient:  0.21076444889613982
iteration : 8496
train acc:  0.765625
train loss:  0.4553658962249756
train gradient:  0.12498127450219994
iteration : 8497
train acc:  0.7578125
train loss:  0.5161373019218445
train gradient:  0.12447607976497031
iteration : 8498
train acc:  0.765625
train loss:  0.5149591565132141
train gradient:  0.1530484281008515
iteration : 8499
train acc:  0.7578125
train loss:  0.49372416734695435
train gradient:  0.1661604840249601
iteration : 8500
train acc:  0.7421875
train loss:  0.5037952065467834
train gradient:  0.1490088887861494
iteration : 8501
train acc:  0.7265625
train loss:  0.5736387968063354
train gradient:  0.1673971106941626
iteration : 8502
train acc:  0.8125
train loss:  0.4779502749443054
train gradient:  0.15418833254501368
iteration : 8503
train acc:  0.8046875
train loss:  0.4612411856651306
train gradient:  0.11478186827201647
iteration : 8504
train acc:  0.6640625
train loss:  0.5221675634384155
train gradient:  0.1693207947255933
iteration : 8505
train acc:  0.7578125
train loss:  0.4286881685256958
train gradient:  0.0931639272031847
iteration : 8506
train acc:  0.7265625
train loss:  0.4938737750053406
train gradient:  0.13516160383948816
iteration : 8507
train acc:  0.7421875
train loss:  0.4871542155742645
train gradient:  0.11717210409366224
iteration : 8508
train acc:  0.6953125
train loss:  0.49064213037490845
train gradient:  0.1337009268031463
iteration : 8509
train acc:  0.6875
train loss:  0.5403053760528564
train gradient:  0.17658556071201398
iteration : 8510
train acc:  0.6875
train loss:  0.5987378358840942
train gradient:  0.1804436698849824
iteration : 8511
train acc:  0.7421875
train loss:  0.4983581304550171
train gradient:  0.13370883324349964
iteration : 8512
train acc:  0.7265625
train loss:  0.49191775918006897
train gradient:  0.12542536238572238
iteration : 8513
train acc:  0.703125
train loss:  0.5201622247695923
train gradient:  0.1784296009087452
iteration : 8514
train acc:  0.734375
train loss:  0.5268983840942383
train gradient:  0.13176462032720695
iteration : 8515
train acc:  0.75
train loss:  0.49009501934051514
train gradient:  0.1255698140812929
iteration : 8516
train acc:  0.6953125
train loss:  0.5032113790512085
train gradient:  0.15864926460896356
iteration : 8517
train acc:  0.6953125
train loss:  0.5230898857116699
train gradient:  0.1450995691281543
iteration : 8518
train acc:  0.7109375
train loss:  0.5172261595726013
train gradient:  0.1473966469195468
iteration : 8519
train acc:  0.6796875
train loss:  0.5732642412185669
train gradient:  0.1474593496007337
iteration : 8520
train acc:  0.765625
train loss:  0.46727144718170166
train gradient:  0.10436746632681211
iteration : 8521
train acc:  0.78125
train loss:  0.46348094940185547
train gradient:  0.1141495725773929
iteration : 8522
train acc:  0.8203125
train loss:  0.40102314949035645
train gradient:  0.08258570402523727
iteration : 8523
train acc:  0.703125
train loss:  0.5641482472419739
train gradient:  0.16956515087684937
iteration : 8524
train acc:  0.75
train loss:  0.4450008273124695
train gradient:  0.1261380949334372
iteration : 8525
train acc:  0.671875
train loss:  0.5424886345863342
train gradient:  0.16700766738795225
iteration : 8526
train acc:  0.7265625
train loss:  0.5258905291557312
train gradient:  0.133709039984978
iteration : 8527
train acc:  0.703125
train loss:  0.5135664343833923
train gradient:  0.14153541386505003
iteration : 8528
train acc:  0.75
train loss:  0.48095306754112244
train gradient:  0.11447477159192075
iteration : 8529
train acc:  0.7734375
train loss:  0.4845142960548401
train gradient:  0.10899133361599919
iteration : 8530
train acc:  0.6875
train loss:  0.5369768142700195
train gradient:  0.15710027899654772
iteration : 8531
train acc:  0.765625
train loss:  0.4774269759654999
train gradient:  0.11550889732213813
iteration : 8532
train acc:  0.78125
train loss:  0.500800371170044
train gradient:  0.11970177082916457
iteration : 8533
train acc:  0.703125
train loss:  0.4892948865890503
train gradient:  0.12618120333193233
iteration : 8534
train acc:  0.75
train loss:  0.5306775569915771
train gradient:  0.14907963827342802
iteration : 8535
train acc:  0.7890625
train loss:  0.4439636766910553
train gradient:  0.0982385426776848
iteration : 8536
train acc:  0.703125
train loss:  0.5502378940582275
train gradient:  0.12313719460602819
iteration : 8537
train acc:  0.703125
train loss:  0.5011903643608093
train gradient:  0.12446970091982666
iteration : 8538
train acc:  0.6796875
train loss:  0.5537872314453125
train gradient:  0.14797359158400358
iteration : 8539
train acc:  0.640625
train loss:  0.6115923523902893
train gradient:  0.15541217172822502
iteration : 8540
train acc:  0.7890625
train loss:  0.43943482637405396
train gradient:  0.11874169357109568
iteration : 8541
train acc:  0.7265625
train loss:  0.5008713006973267
train gradient:  0.15017032950719683
iteration : 8542
train acc:  0.7734375
train loss:  0.48208609223365784
train gradient:  0.13115837955955026
iteration : 8543
train acc:  0.71875
train loss:  0.551657497882843
train gradient:  0.14661763332690597
iteration : 8544
train acc:  0.75
train loss:  0.46250301599502563
train gradient:  0.12273237190196841
iteration : 8545
train acc:  0.78125
train loss:  0.4359896779060364
train gradient:  0.10996628206517357
iteration : 8546
train acc:  0.7734375
train loss:  0.4504660964012146
train gradient:  0.08769896846591176
iteration : 8547
train acc:  0.765625
train loss:  0.5058274865150452
train gradient:  0.1256929894748065
iteration : 8548
train acc:  0.7421875
train loss:  0.4913785457611084
train gradient:  0.11615839794039852
iteration : 8549
train acc:  0.8046875
train loss:  0.44050735235214233
train gradient:  0.10976465062363361
iteration : 8550
train acc:  0.7734375
train loss:  0.5352413654327393
train gradient:  0.15286806818780652
iteration : 8551
train acc:  0.796875
train loss:  0.4309343695640564
train gradient:  0.12515622674306454
iteration : 8552
train acc:  0.7734375
train loss:  0.46874457597732544
train gradient:  0.1068948355273742
iteration : 8553
train acc:  0.71875
train loss:  0.5283596515655518
train gradient:  0.11773899132978022
iteration : 8554
train acc:  0.6875
train loss:  0.5543886423110962
train gradient:  0.15346493864634336
iteration : 8555
train acc:  0.75
train loss:  0.5248858332633972
train gradient:  0.12569115241295378
iteration : 8556
train acc:  0.7421875
train loss:  0.5487765073776245
train gradient:  0.15540085286300392
iteration : 8557
train acc:  0.75
train loss:  0.47001025080680847
train gradient:  0.11137316498498837
iteration : 8558
train acc:  0.71875
train loss:  0.5493102669715881
train gradient:  0.13146919435032295
iteration : 8559
train acc:  0.7421875
train loss:  0.49429094791412354
train gradient:  0.11049693010890846
iteration : 8560
train acc:  0.7265625
train loss:  0.5006742477416992
train gradient:  0.13891647485922018
iteration : 8561
train acc:  0.671875
train loss:  0.540753960609436
train gradient:  0.1511661790101508
iteration : 8562
train acc:  0.7265625
train loss:  0.46356528997421265
train gradient:  0.14212913509964337
iteration : 8563
train acc:  0.734375
train loss:  0.5482473373413086
train gradient:  0.1447227238275551
iteration : 8564
train acc:  0.765625
train loss:  0.483930766582489
train gradient:  0.11040554748040282
iteration : 8565
train acc:  0.7890625
train loss:  0.4640958905220032
train gradient:  0.0964308757330723
iteration : 8566
train acc:  0.7578125
train loss:  0.4673665165901184
train gradient:  0.1372394477480935
iteration : 8567
train acc:  0.765625
train loss:  0.472465455532074
train gradient:  0.12298786076349016
iteration : 8568
train acc:  0.7734375
train loss:  0.45075422525405884
train gradient:  0.12862451031980046
iteration : 8569
train acc:  0.75
train loss:  0.48747679591178894
train gradient:  0.14462129682921443
iteration : 8570
train acc:  0.7578125
train loss:  0.4253901541233063
train gradient:  0.11586290712493856
iteration : 8571
train acc:  0.734375
train loss:  0.5275721549987793
train gradient:  0.14514527026126608
iteration : 8572
train acc:  0.7734375
train loss:  0.4642907381057739
train gradient:  0.10019030712390355
iteration : 8573
train acc:  0.8125
train loss:  0.5052198171615601
train gradient:  0.15951762882250267
iteration : 8574
train acc:  0.6953125
train loss:  0.5355560183525085
train gradient:  0.14781751371824176
iteration : 8575
train acc:  0.7578125
train loss:  0.5492186546325684
train gradient:  0.1781259106526139
iteration : 8576
train acc:  0.71875
train loss:  0.49620020389556885
train gradient:  0.13697836129878213
iteration : 8577
train acc:  0.6328125
train loss:  0.6218172311782837
train gradient:  0.2042214486069339
iteration : 8578
train acc:  0.7421875
train loss:  0.465506911277771
train gradient:  0.10966474471241758
iteration : 8579
train acc:  0.734375
train loss:  0.518358051776886
train gradient:  0.15205989372315565
iteration : 8580
train acc:  0.75
train loss:  0.5056281685829163
train gradient:  0.15145864661244895
iteration : 8581
train acc:  0.7109375
train loss:  0.527051568031311
train gradient:  0.14142806888050094
iteration : 8582
train acc:  0.7734375
train loss:  0.4815482795238495
train gradient:  0.1296690142030773
iteration : 8583
train acc:  0.765625
train loss:  0.4712415337562561
train gradient:  0.10248604025132803
iteration : 8584
train acc:  0.6484375
train loss:  0.5992364883422852
train gradient:  0.17320855514484698
iteration : 8585
train acc:  0.7734375
train loss:  0.48499274253845215
train gradient:  0.1161949417516569
iteration : 8586
train acc:  0.71875
train loss:  0.512161374092102
train gradient:  0.13334109883630899
iteration : 8587
train acc:  0.7265625
train loss:  0.5220412611961365
train gradient:  0.11697158900203414
iteration : 8588
train acc:  0.7109375
train loss:  0.5308961868286133
train gradient:  0.13184375313158492
iteration : 8589
train acc:  0.7265625
train loss:  0.48791444301605225
train gradient:  0.1276973452521366
iteration : 8590
train acc:  0.75
train loss:  0.48510006070137024
train gradient:  0.14363091972306694
iteration : 8591
train acc:  0.7109375
train loss:  0.5484321117401123
train gradient:  0.12943297428680206
iteration : 8592
train acc:  0.7109375
train loss:  0.523206353187561
train gradient:  0.13264117243524556
iteration : 8593
train acc:  0.6953125
train loss:  0.61332768201828
train gradient:  0.19736678144608952
iteration : 8594
train acc:  0.71875
train loss:  0.48388218879699707
train gradient:  0.14705035157122692
iteration : 8595
train acc:  0.7734375
train loss:  0.4681294560432434
train gradient:  0.11066605760848613
iteration : 8596
train acc:  0.7578125
train loss:  0.5388511419296265
train gradient:  0.13559256318024418
iteration : 8597
train acc:  0.7734375
train loss:  0.44490665197372437
train gradient:  0.11690306649745612
iteration : 8598
train acc:  0.6953125
train loss:  0.5938832759857178
train gradient:  0.16250331564644563
iteration : 8599
train acc:  0.78125
train loss:  0.470884770154953
train gradient:  0.13664992136436838
iteration : 8600
train acc:  0.6953125
train loss:  0.548161506652832
train gradient:  0.15284823095960914
iteration : 8601
train acc:  0.734375
train loss:  0.5207269191741943
train gradient:  0.15079524604580774
iteration : 8602
train acc:  0.7109375
train loss:  0.6200709342956543
train gradient:  0.19175616258193068
iteration : 8603
train acc:  0.7421875
train loss:  0.5051783323287964
train gradient:  0.13880837848034966
iteration : 8604
train acc:  0.734375
train loss:  0.4999259412288666
train gradient:  0.12335046378826887
iteration : 8605
train acc:  0.7578125
train loss:  0.5117985606193542
train gradient:  0.17416259716948135
iteration : 8606
train acc:  0.71875
train loss:  0.4694429337978363
train gradient:  0.12139312726173895
iteration : 8607
train acc:  0.78125
train loss:  0.4324067234992981
train gradient:  0.11911418185344315
iteration : 8608
train acc:  0.7265625
train loss:  0.48582592606544495
train gradient:  0.12475615728181046
iteration : 8609
train acc:  0.7265625
train loss:  0.5410891175270081
train gradient:  0.12163291991121485
iteration : 8610
train acc:  0.75
train loss:  0.49623894691467285
train gradient:  0.13672247087928852
iteration : 8611
train acc:  0.734375
train loss:  0.5047767758369446
train gradient:  0.13236813318585317
iteration : 8612
train acc:  0.71875
train loss:  0.5352152585983276
train gradient:  0.14890572561169252
iteration : 8613
train acc:  0.703125
train loss:  0.516817569732666
train gradient:  0.14931980376717013
iteration : 8614
train acc:  0.78125
train loss:  0.4868215024471283
train gradient:  0.1295374416383961
iteration : 8615
train acc:  0.65625
train loss:  0.515766978263855
train gradient:  0.1289232795029857
iteration : 8616
train acc:  0.734375
train loss:  0.47019362449645996
train gradient:  0.1258253086404871
iteration : 8617
train acc:  0.734375
train loss:  0.5048490762710571
train gradient:  0.12975507745673165
iteration : 8618
train acc:  0.796875
train loss:  0.42397111654281616
train gradient:  0.08215664601680822
iteration : 8619
train acc:  0.8203125
train loss:  0.39830076694488525
train gradient:  0.09473050989089676
iteration : 8620
train acc:  0.7578125
train loss:  0.5147255063056946
train gradient:  0.12917004104628432
iteration : 8621
train acc:  0.8203125
train loss:  0.4531545639038086
train gradient:  0.10926955920987814
iteration : 8622
train acc:  0.734375
train loss:  0.48594117164611816
train gradient:  0.13681040006826217
iteration : 8623
train acc:  0.765625
train loss:  0.4654569625854492
train gradient:  0.14112724110015637
iteration : 8624
train acc:  0.7265625
train loss:  0.5109795928001404
train gradient:  0.14963676951103946
iteration : 8625
train acc:  0.78125
train loss:  0.42981284856796265
train gradient:  0.15144863528181426
iteration : 8626
train acc:  0.703125
train loss:  0.49686747789382935
train gradient:  0.11824510498735491
iteration : 8627
train acc:  0.703125
train loss:  0.5018837451934814
train gradient:  0.13116604773185003
iteration : 8628
train acc:  0.796875
train loss:  0.4482380747795105
train gradient:  0.1045178734367343
iteration : 8629
train acc:  0.7421875
train loss:  0.5106993317604065
train gradient:  0.12829665967071735
iteration : 8630
train acc:  0.75
train loss:  0.46782684326171875
train gradient:  0.12224902906362028
iteration : 8631
train acc:  0.765625
train loss:  0.52528977394104
train gradient:  0.1287397815307446
iteration : 8632
train acc:  0.7265625
train loss:  0.5140728950500488
train gradient:  0.15087413529326732
iteration : 8633
train acc:  0.7734375
train loss:  0.46646180748939514
train gradient:  0.11132409455188756
iteration : 8634
train acc:  0.75
train loss:  0.542878270149231
train gradient:  0.14307633136872755
iteration : 8635
train acc:  0.71875
train loss:  0.498388409614563
train gradient:  0.12944344098432858
iteration : 8636
train acc:  0.71875
train loss:  0.4935275912284851
train gradient:  0.14393892179261109
iteration : 8637
train acc:  0.703125
train loss:  0.5207659006118774
train gradient:  0.11826669970623346
iteration : 8638
train acc:  0.7734375
train loss:  0.4702577590942383
train gradient:  0.11332108069644689
iteration : 8639
train acc:  0.734375
train loss:  0.5040051937103271
train gradient:  0.150881685602748
iteration : 8640
train acc:  0.7890625
train loss:  0.47509557008743286
train gradient:  0.1131021431282899
iteration : 8641
train acc:  0.71875
train loss:  0.5140838027000427
train gradient:  0.13786609017867488
iteration : 8642
train acc:  0.6328125
train loss:  0.5526995658874512
train gradient:  0.14220406284339124
iteration : 8643
train acc:  0.78125
train loss:  0.4468775689601898
train gradient:  0.12249647035740918
iteration : 8644
train acc:  0.703125
train loss:  0.5546493530273438
train gradient:  0.15624089646917805
iteration : 8645
train acc:  0.6328125
train loss:  0.5522537231445312
train gradient:  0.13878834675563906
iteration : 8646
train acc:  0.7578125
train loss:  0.5483948588371277
train gradient:  0.14898787161805055
iteration : 8647
train acc:  0.7265625
train loss:  0.5149635672569275
train gradient:  0.1460260582812869
iteration : 8648
train acc:  0.7265625
train loss:  0.5139158368110657
train gradient:  0.12875328324944985
iteration : 8649
train acc:  0.78125
train loss:  0.4808487296104431
train gradient:  0.12979795099787056
iteration : 8650
train acc:  0.75
train loss:  0.45589813590049744
train gradient:  0.13019877950310732
iteration : 8651
train acc:  0.7109375
train loss:  0.5284968614578247
train gradient:  0.15513317174178473
iteration : 8652
train acc:  0.640625
train loss:  0.579098105430603
train gradient:  0.14727232408860247
iteration : 8653
train acc:  0.8359375
train loss:  0.43188342452049255
train gradient:  0.11860092097572986
iteration : 8654
train acc:  0.8203125
train loss:  0.4707797169685364
train gradient:  0.11058915626606222
iteration : 8655
train acc:  0.65625
train loss:  0.604628324508667
train gradient:  0.1566476243128076
iteration : 8656
train acc:  0.71875
train loss:  0.5116637945175171
train gradient:  0.1367704167667493
iteration : 8657
train acc:  0.71875
train loss:  0.5600153803825378
train gradient:  0.19178961530159577
iteration : 8658
train acc:  0.6796875
train loss:  0.5629108548164368
train gradient:  0.1571570213536042
iteration : 8659
train acc:  0.6640625
train loss:  0.5916690230369568
train gradient:  0.16372170756825777
iteration : 8660
train acc:  0.765625
train loss:  0.5064517259597778
train gradient:  0.14142619926668365
iteration : 8661
train acc:  0.765625
train loss:  0.46737566590309143
train gradient:  0.18442171886310216
iteration : 8662
train acc:  0.765625
train loss:  0.46757540106773376
train gradient:  0.14341597830117245
iteration : 8663
train acc:  0.7734375
train loss:  0.5050050020217896
train gradient:  0.13110369234807304
iteration : 8664
train acc:  0.78125
train loss:  0.4592370390892029
train gradient:  0.13803375943573007
iteration : 8665
train acc:  0.765625
train loss:  0.5081886649131775
train gradient:  0.1515002993977851
iteration : 8666
train acc:  0.703125
train loss:  0.5296471118927002
train gradient:  0.14296348363545652
iteration : 8667
train acc:  0.765625
train loss:  0.4929004907608032
train gradient:  0.13006656168132968
iteration : 8668
train acc:  0.734375
train loss:  0.5092185735702515
train gradient:  0.11295970025806974
iteration : 8669
train acc:  0.75
train loss:  0.5294232368469238
train gradient:  0.13372147327426817
iteration : 8670
train acc:  0.7265625
train loss:  0.5062613487243652
train gradient:  0.12091474305924704
iteration : 8671
train acc:  0.7265625
train loss:  0.49567270278930664
train gradient:  0.14210959012277344
iteration : 8672
train acc:  0.7421875
train loss:  0.4832727313041687
train gradient:  0.12621790289810808
iteration : 8673
train acc:  0.75
train loss:  0.49675482511520386
train gradient:  0.13123437824584935
iteration : 8674
train acc:  0.6875
train loss:  0.5302923917770386
train gradient:  0.15533390889809195
iteration : 8675
train acc:  0.734375
train loss:  0.5371341705322266
train gradient:  0.17132565289164542
iteration : 8676
train acc:  0.8125
train loss:  0.46704208850860596
train gradient:  0.11335998788801228
iteration : 8677
train acc:  0.6796875
train loss:  0.5556294918060303
train gradient:  0.16871554709731987
iteration : 8678
train acc:  0.703125
train loss:  0.5113252997398376
train gradient:  0.11523446869858553
iteration : 8679
train acc:  0.734375
train loss:  0.5196590423583984
train gradient:  0.12931180281788485
iteration : 8680
train acc:  0.7734375
train loss:  0.49922776222229004
train gradient:  0.13712939544211977
iteration : 8681
train acc:  0.7109375
train loss:  0.51019287109375
train gradient:  0.14505574116157455
iteration : 8682
train acc:  0.6875
train loss:  0.5907673835754395
train gradient:  0.17435215167888174
iteration : 8683
train acc:  0.796875
train loss:  0.4559127688407898
train gradient:  0.1464497007374214
iteration : 8684
train acc:  0.7734375
train loss:  0.43947312235832214
train gradient:  0.09723126281377725
iteration : 8685
train acc:  0.65625
train loss:  0.624455451965332
train gradient:  0.16715897574139305
iteration : 8686
train acc:  0.75
train loss:  0.47855645418167114
train gradient:  0.11700043363636722
iteration : 8687
train acc:  0.734375
train loss:  0.5346231460571289
train gradient:  0.13358919264107466
iteration : 8688
train acc:  0.703125
train loss:  0.5012052059173584
train gradient:  0.12458942661632673
iteration : 8689
train acc:  0.734375
train loss:  0.5028854608535767
train gradient:  0.13294378111336722
iteration : 8690
train acc:  0.765625
train loss:  0.5039783716201782
train gradient:  0.13437943416245932
iteration : 8691
train acc:  0.7578125
train loss:  0.47035032510757446
train gradient:  0.1278885371732978
iteration : 8692
train acc:  0.671875
train loss:  0.5408300161361694
train gradient:  0.12239332413460996
iteration : 8693
train acc:  0.71875
train loss:  0.5158407688140869
train gradient:  0.16638924413677167
iteration : 8694
train acc:  0.71875
train loss:  0.5381935834884644
train gradient:  0.1513710139104027
iteration : 8695
train acc:  0.6953125
train loss:  0.5286517143249512
train gradient:  0.14716219602309255
iteration : 8696
train acc:  0.7421875
train loss:  0.47502315044403076
train gradient:  0.1113126322082745
iteration : 8697
train acc:  0.6953125
train loss:  0.48520541191101074
train gradient:  0.1092339474376743
iteration : 8698
train acc:  0.6875
train loss:  0.5864431858062744
train gradient:  0.14670035974164236
iteration : 8699
train acc:  0.7578125
train loss:  0.46249765157699585
train gradient:  0.11752886206445984
iteration : 8700
train acc:  0.6796875
train loss:  0.5784105658531189
train gradient:  0.1860110896081109
iteration : 8701
train acc:  0.6796875
train loss:  0.5767614245414734
train gradient:  0.13901538204283415
iteration : 8702
train acc:  0.6875
train loss:  0.5206565260887146
train gradient:  0.1324088346891219
iteration : 8703
train acc:  0.7265625
train loss:  0.5602787137031555
train gradient:  0.18490851401356556
iteration : 8704
train acc:  0.671875
train loss:  0.5594013333320618
train gradient:  0.18160045375057482
iteration : 8705
train acc:  0.65625
train loss:  0.5800517797470093
train gradient:  0.14582908411258752
iteration : 8706
train acc:  0.796875
train loss:  0.453186959028244
train gradient:  0.10530354702010815
iteration : 8707
train acc:  0.7109375
train loss:  0.5205929279327393
train gradient:  0.12652571924583517
iteration : 8708
train acc:  0.796875
train loss:  0.4934287369251251
train gradient:  0.12743290048179146
iteration : 8709
train acc:  0.75
train loss:  0.4685521423816681
train gradient:  0.13346552567408543
iteration : 8710
train acc:  0.65625
train loss:  0.5499675273895264
train gradient:  0.12879233189222844
iteration : 8711
train acc:  0.7421875
train loss:  0.5983372926712036
train gradient:  0.16219410270356796
iteration : 8712
train acc:  0.75
train loss:  0.5550719499588013
train gradient:  0.16877893839541436
iteration : 8713
train acc:  0.7421875
train loss:  0.4736657738685608
train gradient:  0.09930758535346251
iteration : 8714
train acc:  0.75
train loss:  0.5423347353935242
train gradient:  0.14378091571403895
iteration : 8715
train acc:  0.734375
train loss:  0.4824751615524292
train gradient:  0.12089776606589732
iteration : 8716
train acc:  0.65625
train loss:  0.5952762365341187
train gradient:  0.15555735398929477
iteration : 8717
train acc:  0.7109375
train loss:  0.5233437418937683
train gradient:  0.1284267416921372
iteration : 8718
train acc:  0.71875
train loss:  0.47737252712249756
train gradient:  0.09912371850905105
iteration : 8719
train acc:  0.8203125
train loss:  0.42877820134162903
train gradient:  0.0939631852822863
iteration : 8720
train acc:  0.6953125
train loss:  0.5700207948684692
train gradient:  0.16819084433097178
iteration : 8721
train acc:  0.640625
train loss:  0.6159390807151794
train gradient:  0.23139997666187195
iteration : 8722
train acc:  0.7890625
train loss:  0.49477821588516235
train gradient:  0.14121754559265537
iteration : 8723
train acc:  0.75
train loss:  0.5312256813049316
train gradient:  0.16191668146733812
iteration : 8724
train acc:  0.75
train loss:  0.4471893906593323
train gradient:  0.128843634164718
iteration : 8725
train acc:  0.78125
train loss:  0.4439226984977722
train gradient:  0.08814293528672706
iteration : 8726
train acc:  0.75
train loss:  0.4829535484313965
train gradient:  0.11005217361565306
iteration : 8727
train acc:  0.8046875
train loss:  0.4462815523147583
train gradient:  0.1224554224985587
iteration : 8728
train acc:  0.640625
train loss:  0.5859605073928833
train gradient:  0.15464072033639514
iteration : 8729
train acc:  0.84375
train loss:  0.3885202407836914
train gradient:  0.0791272080059782
iteration : 8730
train acc:  0.6953125
train loss:  0.5506935119628906
train gradient:  0.1430637997973601
iteration : 8731
train acc:  0.71875
train loss:  0.46572816371917725
train gradient:  0.10057467190421328
iteration : 8732
train acc:  0.7421875
train loss:  0.5241819620132446
train gradient:  0.14948929712785652
iteration : 8733
train acc:  0.734375
train loss:  0.5143665671348572
train gradient:  0.12968194677599626
iteration : 8734
train acc:  0.765625
train loss:  0.5006591081619263
train gradient:  0.14505972564873776
iteration : 8735
train acc:  0.7109375
train loss:  0.5127377510070801
train gradient:  0.12145656296041164
iteration : 8736
train acc:  0.8125
train loss:  0.4326949119567871
train gradient:  0.0881725589639652
iteration : 8737
train acc:  0.8203125
train loss:  0.432611882686615
train gradient:  0.10522360887481591
iteration : 8738
train acc:  0.7578125
train loss:  0.49672943353652954
train gradient:  0.11231981404023117
iteration : 8739
train acc:  0.71875
train loss:  0.5233628749847412
train gradient:  0.13308165568821634
iteration : 8740
train acc:  0.7109375
train loss:  0.4916382133960724
train gradient:  0.10831729530772612
iteration : 8741
train acc:  0.7578125
train loss:  0.4650470018386841
train gradient:  0.09991017483244072
iteration : 8742
train acc:  0.75
train loss:  0.47973763942718506
train gradient:  0.17541097615308676
iteration : 8743
train acc:  0.7265625
train loss:  0.491992712020874
train gradient:  0.11525535177560597
iteration : 8744
train acc:  0.6953125
train loss:  0.5157272219657898
train gradient:  0.15620295665221018
iteration : 8745
train acc:  0.7578125
train loss:  0.5307786464691162
train gradient:  0.1135768501207545
iteration : 8746
train acc:  0.734375
train loss:  0.49185022711753845
train gradient:  0.16282330425630653
iteration : 8747
train acc:  0.828125
train loss:  0.4265248775482178
train gradient:  0.11108052605702443
iteration : 8748
train acc:  0.65625
train loss:  0.5580458641052246
train gradient:  0.1756972202832133
iteration : 8749
train acc:  0.7421875
train loss:  0.5217572450637817
train gradient:  0.13763962240668365
iteration : 8750
train acc:  0.71875
train loss:  0.5268592238426208
train gradient:  0.12902185694396265
iteration : 8751
train acc:  0.71875
train loss:  0.46226996183395386
train gradient:  0.14552288439772457
iteration : 8752
train acc:  0.7734375
train loss:  0.4437289834022522
train gradient:  0.09243089921602467
iteration : 8753
train acc:  0.7421875
train loss:  0.5299383997917175
train gradient:  0.1520359017600685
iteration : 8754
train acc:  0.6953125
train loss:  0.5127626657485962
train gradient:  0.18549891331142915
iteration : 8755
train acc:  0.78125
train loss:  0.4509257376194
train gradient:  0.09103598305028478
iteration : 8756
train acc:  0.75
train loss:  0.47095876932144165
train gradient:  0.1298617567963848
iteration : 8757
train acc:  0.75
train loss:  0.4793641269207001
train gradient:  0.15775532365410905
iteration : 8758
train acc:  0.7578125
train loss:  0.5055707693099976
train gradient:  0.14085594486433003
iteration : 8759
train acc:  0.78125
train loss:  0.4673239588737488
train gradient:  0.15078597195023136
iteration : 8760
train acc:  0.7578125
train loss:  0.46646004915237427
train gradient:  0.14067465056119036
iteration : 8761
train acc:  0.7265625
train loss:  0.4903876483440399
train gradient:  0.13592064976829515
iteration : 8762
train acc:  0.71875
train loss:  0.5139399170875549
train gradient:  0.1163229150277833
iteration : 8763
train acc:  0.8359375
train loss:  0.4245260953903198
train gradient:  0.10476046868401524
iteration : 8764
train acc:  0.8359375
train loss:  0.39683040976524353
train gradient:  0.08938838314924064
iteration : 8765
train acc:  0.7265625
train loss:  0.4765913784503937
train gradient:  0.11080324454151409
iteration : 8766
train acc:  0.7578125
train loss:  0.5176688432693481
train gradient:  0.13229141303372333
iteration : 8767
train acc:  0.7890625
train loss:  0.43546009063720703
train gradient:  0.09553131470099276
iteration : 8768
train acc:  0.7109375
train loss:  0.5332155227661133
train gradient:  0.16360272046290314
iteration : 8769
train acc:  0.7578125
train loss:  0.48642805218696594
train gradient:  0.10769193911177849
iteration : 8770
train acc:  0.640625
train loss:  0.5730916261672974
train gradient:  0.17166904573817182
iteration : 8771
train acc:  0.7421875
train loss:  0.49898725748062134
train gradient:  0.15024101071332413
iteration : 8772
train acc:  0.8203125
train loss:  0.47302842140197754
train gradient:  0.16857383417321586
iteration : 8773
train acc:  0.75
train loss:  0.44388866424560547
train gradient:  0.10433263375005154
iteration : 8774
train acc:  0.703125
train loss:  0.5065334439277649
train gradient:  0.10583667308853857
iteration : 8775
train acc:  0.7578125
train loss:  0.48019060492515564
train gradient:  0.12479487811091512
iteration : 8776
train acc:  0.625
train loss:  0.5934622287750244
train gradient:  0.21142946214746794
iteration : 8777
train acc:  0.7109375
train loss:  0.5110755562782288
train gradient:  0.13342065068912518
iteration : 8778
train acc:  0.75
train loss:  0.4679810702800751
train gradient:  0.13682467305632273
iteration : 8779
train acc:  0.71875
train loss:  0.5525976419448853
train gradient:  0.15071889336038247
iteration : 8780
train acc:  0.7421875
train loss:  0.5118061900138855
train gradient:  0.1414466896855152
iteration : 8781
train acc:  0.6640625
train loss:  0.533002495765686
train gradient:  0.1354276276712602
iteration : 8782
train acc:  0.8125
train loss:  0.4310709238052368
train gradient:  0.09511040847223169
iteration : 8783
train acc:  0.7734375
train loss:  0.5050449371337891
train gradient:  0.1261158265016336
iteration : 8784
train acc:  0.78125
train loss:  0.4841507077217102
train gradient:  0.14013904474554706
iteration : 8785
train acc:  0.765625
train loss:  0.48088571429252625
train gradient:  0.10874081850777015
iteration : 8786
train acc:  0.703125
train loss:  0.5456042885780334
train gradient:  0.14840906967426665
iteration : 8787
train acc:  0.65625
train loss:  0.5366063714027405
train gradient:  0.15225116562336075
iteration : 8788
train acc:  0.734375
train loss:  0.5225622653961182
train gradient:  0.1293066092726373
iteration : 8789
train acc:  0.7578125
train loss:  0.49236226081848145
train gradient:  0.13489917897402523
iteration : 8790
train acc:  0.71875
train loss:  0.47230809926986694
train gradient:  0.10206594479455222
iteration : 8791
train acc:  0.734375
train loss:  0.5576813817024231
train gradient:  0.17548560782078026
iteration : 8792
train acc:  0.7421875
train loss:  0.49085429310798645
train gradient:  0.1123418714979208
iteration : 8793
train acc:  0.765625
train loss:  0.5318005084991455
train gradient:  0.1201705948105803
iteration : 8794
train acc:  0.734375
train loss:  0.4643799364566803
train gradient:  0.10915600631669747
iteration : 8795
train acc:  0.65625
train loss:  0.5337305068969727
train gradient:  0.15300754396210597
iteration : 8796
train acc:  0.7734375
train loss:  0.5032644271850586
train gradient:  0.1580189386224769
iteration : 8797
train acc:  0.75
train loss:  0.47494271397590637
train gradient:  0.11598202733907764
iteration : 8798
train acc:  0.703125
train loss:  0.5472348928451538
train gradient:  0.1562241742468304
iteration : 8799
train acc:  0.703125
train loss:  0.5354781746864319
train gradient:  0.16954176030754575
iteration : 8800
train acc:  0.71875
train loss:  0.5078064203262329
train gradient:  0.12451624340753492
iteration : 8801
train acc:  0.765625
train loss:  0.4615783095359802
train gradient:  0.09666283942505735
iteration : 8802
train acc:  0.6875
train loss:  0.5555338859558105
train gradient:  0.21032021625968222
iteration : 8803
train acc:  0.734375
train loss:  0.4591348469257355
train gradient:  0.13900489403369276
iteration : 8804
train acc:  0.6875
train loss:  0.546714723110199
train gradient:  0.1736886284435124
iteration : 8805
train acc:  0.7421875
train loss:  0.49411192536354065
train gradient:  0.13517085507390694
iteration : 8806
train acc:  0.8125
train loss:  0.42291972041130066
train gradient:  0.12935693451412972
iteration : 8807
train acc:  0.765625
train loss:  0.44927772879600525
train gradient:  0.11559125994814144
iteration : 8808
train acc:  0.6796875
train loss:  0.5125516653060913
train gradient:  0.14635938834084744
iteration : 8809
train acc:  0.6640625
train loss:  0.5614505410194397
train gradient:  0.13022934602747077
iteration : 8810
train acc:  0.7421875
train loss:  0.489082008600235
train gradient:  0.10353311062414206
iteration : 8811
train acc:  0.65625
train loss:  0.6214722394943237
train gradient:  0.2087344636335084
iteration : 8812
train acc:  0.8359375
train loss:  0.4077278971672058
train gradient:  0.09529804193489433
iteration : 8813
train acc:  0.734375
train loss:  0.47434699535369873
train gradient:  0.10590962141240867
iteration : 8814
train acc:  0.75
train loss:  0.4670785963535309
train gradient:  0.1340965579271859
iteration : 8815
train acc:  0.7109375
train loss:  0.5541411638259888
train gradient:  0.1567886417321437
iteration : 8816
train acc:  0.8125
train loss:  0.40226197242736816
train gradient:  0.08334804625627261
iteration : 8817
train acc:  0.8046875
train loss:  0.46103474497795105
train gradient:  0.16259803782137205
iteration : 8818
train acc:  0.7109375
train loss:  0.5144426226615906
train gradient:  0.1542665710424433
iteration : 8819
train acc:  0.75
train loss:  0.48650044202804565
train gradient:  0.13625452408487665
iteration : 8820
train acc:  0.75
train loss:  0.47940608859062195
train gradient:  0.1343317446091092
iteration : 8821
train acc:  0.7734375
train loss:  0.4429599642753601
train gradient:  0.13604902014065687
iteration : 8822
train acc:  0.703125
train loss:  0.5448631048202515
train gradient:  0.2209911439540513
iteration : 8823
train acc:  0.78125
train loss:  0.4948262572288513
train gradient:  0.14709634196620808
iteration : 8824
train acc:  0.78125
train loss:  0.48854270577430725
train gradient:  0.10516269605278221
iteration : 8825
train acc:  0.71875
train loss:  0.47422197461128235
train gradient:  0.09597104089441276
iteration : 8826
train acc:  0.7890625
train loss:  0.47512906789779663
train gradient:  0.13425004926052003
iteration : 8827
train acc:  0.7578125
train loss:  0.5550808906555176
train gradient:  0.19011742693943004
iteration : 8828
train acc:  0.734375
train loss:  0.48235800862312317
train gradient:  0.1274207583875773
iteration : 8829
train acc:  0.765625
train loss:  0.4884409010410309
train gradient:  0.1365703747742688
iteration : 8830
train acc:  0.6796875
train loss:  0.5438566207885742
train gradient:  0.13797975115720715
iteration : 8831
train acc:  0.734375
train loss:  0.5063877105712891
train gradient:  0.17034782903181767
iteration : 8832
train acc:  0.734375
train loss:  0.5036518573760986
train gradient:  0.14520531101670103
iteration : 8833
train acc:  0.7265625
train loss:  0.48345130681991577
train gradient:  0.12647810201278276
iteration : 8834
train acc:  0.734375
train loss:  0.5101690292358398
train gradient:  0.18005783116383656
iteration : 8835
train acc:  0.6640625
train loss:  0.505940318107605
train gradient:  0.12714265933248925
iteration : 8836
train acc:  0.8046875
train loss:  0.4500199556350708
train gradient:  0.11531389130725007
iteration : 8837
train acc:  0.703125
train loss:  0.5378224849700928
train gradient:  0.16100276232469718
iteration : 8838
train acc:  0.6796875
train loss:  0.5467269420623779
train gradient:  0.14796932712653577
iteration : 8839
train acc:  0.7265625
train loss:  0.543006181716919
train gradient:  0.14287611561142194
iteration : 8840
train acc:  0.7578125
train loss:  0.47209930419921875
train gradient:  0.10796712958745877
iteration : 8841
train acc:  0.8125
train loss:  0.47934794425964355
train gradient:  0.14163546437264463
iteration : 8842
train acc:  0.703125
train loss:  0.5610358119010925
train gradient:  0.16177593760843528
iteration : 8843
train acc:  0.734375
train loss:  0.48396173119544983
train gradient:  0.10436423642458958
iteration : 8844
train acc:  0.7734375
train loss:  0.4731982946395874
train gradient:  0.1242513591465367
iteration : 8845
train acc:  0.7578125
train loss:  0.5058394074440002
train gradient:  0.12478458918545558
iteration : 8846
train acc:  0.71875
train loss:  0.4991077482700348
train gradient:  0.12144331013013655
iteration : 8847
train acc:  0.8046875
train loss:  0.47449588775634766
train gradient:  0.1706004096660616
iteration : 8848
train acc:  0.765625
train loss:  0.46534842252731323
train gradient:  0.12337733441900123
iteration : 8849
train acc:  0.7890625
train loss:  0.48167216777801514
train gradient:  0.1452279003603562
iteration : 8850
train acc:  0.7578125
train loss:  0.49487006664276123
train gradient:  0.1349877158842844
iteration : 8851
train acc:  0.7265625
train loss:  0.510168731212616
train gradient:  0.12282176108348672
iteration : 8852
train acc:  0.71875
train loss:  0.49619758129119873
train gradient:  0.12915722858283085
iteration : 8853
train acc:  0.8125
train loss:  0.4518515467643738
train gradient:  0.11451731941807787
iteration : 8854
train acc:  0.78125
train loss:  0.4061979651451111
train gradient:  0.10166904227617515
iteration : 8855
train acc:  0.6484375
train loss:  0.6059183478355408
train gradient:  0.21349146441733233
iteration : 8856
train acc:  0.6953125
train loss:  0.5513792037963867
train gradient:  0.1717606989160127
iteration : 8857
train acc:  0.703125
train loss:  0.5130637884140015
train gradient:  0.13023600041182865
iteration : 8858
train acc:  0.796875
train loss:  0.44215133786201477
train gradient:  0.11041935508751213
iteration : 8859
train acc:  0.78125
train loss:  0.4287191927433014
train gradient:  0.09046081229783667
iteration : 8860
train acc:  0.6953125
train loss:  0.49105510115623474
train gradient:  0.12435647320169521
iteration : 8861
train acc:  0.71875
train loss:  0.5290614366531372
train gradient:  0.1748995977611567
iteration : 8862
train acc:  0.6953125
train loss:  0.5907151103019714
train gradient:  0.17749400368854257
iteration : 8863
train acc:  0.7578125
train loss:  0.46711698174476624
train gradient:  0.13120825260328048
iteration : 8864
train acc:  0.7578125
train loss:  0.4767184257507324
train gradient:  0.11361592946356311
iteration : 8865
train acc:  0.7421875
train loss:  0.4818600118160248
train gradient:  0.15857709506825785
iteration : 8866
train acc:  0.6640625
train loss:  0.5991436243057251
train gradient:  0.20958173582481343
iteration : 8867
train acc:  0.734375
train loss:  0.4871349334716797
train gradient:  0.12095078823648386
iteration : 8868
train acc:  0.7578125
train loss:  0.49619802832603455
train gradient:  0.10704928610147549
iteration : 8869
train acc:  0.7734375
train loss:  0.5015457272529602
train gradient:  0.12219694872707342
iteration : 8870
train acc:  0.7578125
train loss:  0.476582795381546
train gradient:  0.12519219929641995
iteration : 8871
train acc:  0.7578125
train loss:  0.508954644203186
train gradient:  0.11771963474822679
iteration : 8872
train acc:  0.7421875
train loss:  0.5115028619766235
train gradient:  0.14880362858187124
iteration : 8873
train acc:  0.7265625
train loss:  0.6063939929008484
train gradient:  0.1959099449381958
iteration : 8874
train acc:  0.7734375
train loss:  0.4861181974411011
train gradient:  0.17098259127443446
iteration : 8875
train acc:  0.671875
train loss:  0.5894625782966614
train gradient:  0.1608197088659835
iteration : 8876
train acc:  0.734375
train loss:  0.5184484124183655
train gradient:  0.14247651254620405
iteration : 8877
train acc:  0.7109375
train loss:  0.49441975355148315
train gradient:  0.12350029451012991
iteration : 8878
train acc:  0.78125
train loss:  0.45424264669418335
train gradient:  0.11017243333429207
iteration : 8879
train acc:  0.6875
train loss:  0.5556479692459106
train gradient:  0.13949849541449355
iteration : 8880
train acc:  0.75
train loss:  0.4920926094055176
train gradient:  0.14678146067587533
iteration : 8881
train acc:  0.7890625
train loss:  0.4305572807788849
train gradient:  0.11337783563311693
iteration : 8882
train acc:  0.7734375
train loss:  0.44856226444244385
train gradient:  0.11723054982943028
iteration : 8883
train acc:  0.6484375
train loss:  0.5608700513839722
train gradient:  0.14886228475434188
iteration : 8884
train acc:  0.78125
train loss:  0.46379175782203674
train gradient:  0.12147409357614788
iteration : 8885
train acc:  0.796875
train loss:  0.4182109832763672
train gradient:  0.09849423430407336
iteration : 8886
train acc:  0.7421875
train loss:  0.5270164012908936
train gradient:  0.1293038562215294
iteration : 8887
train acc:  0.7890625
train loss:  0.46391767263412476
train gradient:  0.12306733149469146
iteration : 8888
train acc:  0.7421875
train loss:  0.5281251668930054
train gradient:  0.1785746357288307
iteration : 8889
train acc:  0.78125
train loss:  0.47932368516921997
train gradient:  0.11068007673705962
iteration : 8890
train acc:  0.7578125
train loss:  0.5057390928268433
train gradient:  0.12450005315809788
iteration : 8891
train acc:  0.71875
train loss:  0.5109427571296692
train gradient:  0.13003158752100685
iteration : 8892
train acc:  0.703125
train loss:  0.5248932838439941
train gradient:  0.11775657869516973
iteration : 8893
train acc:  0.6953125
train loss:  0.5934665203094482
train gradient:  0.1763168913614089
iteration : 8894
train acc:  0.6796875
train loss:  0.5543699264526367
train gradient:  0.17250610490030782
iteration : 8895
train acc:  0.765625
train loss:  0.43620115518569946
train gradient:  0.10918907008539126
iteration : 8896
train acc:  0.7578125
train loss:  0.503069281578064
train gradient:  0.11100054917599438
iteration : 8897
train acc:  0.7265625
train loss:  0.5430920124053955
train gradient:  0.14738841089373428
iteration : 8898
train acc:  0.7109375
train loss:  0.5033921599388123
train gradient:  0.13025967580308162
iteration : 8899
train acc:  0.796875
train loss:  0.4420056641101837
train gradient:  0.11230563659848082
iteration : 8900
train acc:  0.6953125
train loss:  0.6341333389282227
train gradient:  0.20544900758998902
iteration : 8901
train acc:  0.6875
train loss:  0.5344915390014648
train gradient:  0.1433008848728291
iteration : 8902
train acc:  0.734375
train loss:  0.47732993960380554
train gradient:  0.15759503332633928
iteration : 8903
train acc:  0.7734375
train loss:  0.45433634519577026
train gradient:  0.11576212844163809
iteration : 8904
train acc:  0.8125
train loss:  0.39689093828201294
train gradient:  0.10594854682646221
iteration : 8905
train acc:  0.8046875
train loss:  0.40415072441101074
train gradient:  0.10914893555474796
iteration : 8906
train acc:  0.7109375
train loss:  0.48445191979408264
train gradient:  0.13952348235833462
iteration : 8907
train acc:  0.7578125
train loss:  0.5016298890113831
train gradient:  0.14074654975579018
iteration : 8908
train acc:  0.7421875
train loss:  0.48841744661331177
train gradient:  0.13940850591435927
iteration : 8909
train acc:  0.765625
train loss:  0.5199555158615112
train gradient:  0.1855041200561306
iteration : 8910
train acc:  0.7109375
train loss:  0.4811974763870239
train gradient:  0.10706219394547056
iteration : 8911
train acc:  0.7109375
train loss:  0.601601243019104
train gradient:  0.18041064606448357
iteration : 8912
train acc:  0.7109375
train loss:  0.5209810137748718
train gradient:  0.12348542685810232
iteration : 8913
train acc:  0.6875
train loss:  0.5892575979232788
train gradient:  0.1633374326730706
iteration : 8914
train acc:  0.765625
train loss:  0.4687991142272949
train gradient:  0.17010485855313462
iteration : 8915
train acc:  0.7109375
train loss:  0.5254497528076172
train gradient:  0.11838128420790464
iteration : 8916
train acc:  0.7734375
train loss:  0.4577118456363678
train gradient:  0.1222208158077505
iteration : 8917
train acc:  0.84375
train loss:  0.4364701211452484
train gradient:  0.1068419225985953
iteration : 8918
train acc:  0.7578125
train loss:  0.46891865134239197
train gradient:  0.11889842758981298
iteration : 8919
train acc:  0.640625
train loss:  0.5834808349609375
train gradient:  0.19244251064135728
iteration : 8920
train acc:  0.703125
train loss:  0.5591418743133545
train gradient:  0.1520579038447164
iteration : 8921
train acc:  0.7734375
train loss:  0.4533900022506714
train gradient:  0.10032588876145832
iteration : 8922
train acc:  0.703125
train loss:  0.5137192010879517
train gradient:  0.13765721660186547
iteration : 8923
train acc:  0.7109375
train loss:  0.5358591079711914
train gradient:  0.14353359669535753
iteration : 8924
train acc:  0.84375
train loss:  0.418670654296875
train gradient:  0.11712461917189876
iteration : 8925
train acc:  0.6796875
train loss:  0.5128883123397827
train gradient:  0.1299108085942886
iteration : 8926
train acc:  0.765625
train loss:  0.49220940470695496
train gradient:  0.13803401096988382
iteration : 8927
train acc:  0.6953125
train loss:  0.5104950666427612
train gradient:  0.13882982336077154
iteration : 8928
train acc:  0.7578125
train loss:  0.467499315738678
train gradient:  0.11238362533018616
iteration : 8929
train acc:  0.8125
train loss:  0.4506877660751343
train gradient:  0.1365027490015135
iteration : 8930
train acc:  0.75
train loss:  0.470126748085022
train gradient:  0.15874701610073316
iteration : 8931
train acc:  0.703125
train loss:  0.5469628572463989
train gradient:  0.15858094680629126
iteration : 8932
train acc:  0.78125
train loss:  0.4494762122631073
train gradient:  0.10845726170120662
iteration : 8933
train acc:  0.6953125
train loss:  0.5986259579658508
train gradient:  0.21921685829459944
iteration : 8934
train acc:  0.71875
train loss:  0.5197391510009766
train gradient:  0.14442734648240454
iteration : 8935
train acc:  0.8359375
train loss:  0.42392462491989136
train gradient:  0.11744292344256398
iteration : 8936
train acc:  0.7265625
train loss:  0.5542572736740112
train gradient:  0.1451381229715127
iteration : 8937
train acc:  0.828125
train loss:  0.4326595067977905
train gradient:  0.12876806771790533
iteration : 8938
train acc:  0.75
train loss:  0.4711940586566925
train gradient:  0.13999991494348418
iteration : 8939
train acc:  0.78125
train loss:  0.4660889506340027
train gradient:  0.09821692976816537
iteration : 8940
train acc:  0.7890625
train loss:  0.4687170684337616
train gradient:  0.12265128136641197
iteration : 8941
train acc:  0.78125
train loss:  0.43966835737228394
train gradient:  0.12637883752938672
iteration : 8942
train acc:  0.6953125
train loss:  0.49524766206741333
train gradient:  0.12086452208037068
iteration : 8943
train acc:  0.7421875
train loss:  0.4852954149246216
train gradient:  0.1592836542001747
iteration : 8944
train acc:  0.71875
train loss:  0.5533113479614258
train gradient:  0.16096591946264738
iteration : 8945
train acc:  0.734375
train loss:  0.5207247734069824
train gradient:  0.1308970252539904
iteration : 8946
train acc:  0.7578125
train loss:  0.4976191222667694
train gradient:  0.13660091653929204
iteration : 8947
train acc:  0.734375
train loss:  0.481243371963501
train gradient:  0.11918406713488211
iteration : 8948
train acc:  0.7265625
train loss:  0.4920784533023834
train gradient:  0.13527074034524655
iteration : 8949
train acc:  0.734375
train loss:  0.4792138338088989
train gradient:  0.11969243466168628
iteration : 8950
train acc:  0.765625
train loss:  0.4797227382659912
train gradient:  0.15750536329074327
iteration : 8951
train acc:  0.6875
train loss:  0.5760757327079773
train gradient:  0.17207053162357772
iteration : 8952
train acc:  0.734375
train loss:  0.5325586795806885
train gradient:  0.16025576586222307
iteration : 8953
train acc:  0.6796875
train loss:  0.5622454881668091
train gradient:  0.14358868821957788
iteration : 8954
train acc:  0.7734375
train loss:  0.5080114006996155
train gradient:  0.16033097167244226
iteration : 8955
train acc:  0.7109375
train loss:  0.50705885887146
train gradient:  0.13241623790826965
iteration : 8956
train acc:  0.6875
train loss:  0.5635722875595093
train gradient:  0.15335425961904167
iteration : 8957
train acc:  0.71875
train loss:  0.5072736740112305
train gradient:  0.13244200177603888
iteration : 8958
train acc:  0.6796875
train loss:  0.5676801204681396
train gradient:  0.1900524016422473
iteration : 8959
train acc:  0.78125
train loss:  0.418271541595459
train gradient:  0.08199166301082707
iteration : 8960
train acc:  0.71875
train loss:  0.49927353858947754
train gradient:  0.1262602765667239
iteration : 8961
train acc:  0.7265625
train loss:  0.49920591711997986
train gradient:  0.13417777310902843
iteration : 8962
train acc:  0.71875
train loss:  0.5368161201477051
train gradient:  0.16613753572137857
iteration : 8963
train acc:  0.7578125
train loss:  0.4526337683200836
train gradient:  0.11016264252296672
iteration : 8964
train acc:  0.6953125
train loss:  0.4782734513282776
train gradient:  0.12106238408382032
iteration : 8965
train acc:  0.765625
train loss:  0.5003633499145508
train gradient:  0.17459638549952033
iteration : 8966
train acc:  0.765625
train loss:  0.49266842007637024
train gradient:  0.12388654819298711
iteration : 8967
train acc:  0.7421875
train loss:  0.5062094330787659
train gradient:  0.12229430953011086
iteration : 8968
train acc:  0.7421875
train loss:  0.5191576480865479
train gradient:  0.1467786889508459
iteration : 8969
train acc:  0.7109375
train loss:  0.535140335559845
train gradient:  0.20406573182540205
iteration : 8970
train acc:  0.7421875
train loss:  0.5139129757881165
train gradient:  0.16364240300386668
iteration : 8971
train acc:  0.6953125
train loss:  0.5173150897026062
train gradient:  0.17151828131508715
iteration : 8972
train acc:  0.7421875
train loss:  0.5108978748321533
train gradient:  0.12274585296669624
iteration : 8973
train acc:  0.6875
train loss:  0.5414566397666931
train gradient:  0.17889932958725535
iteration : 8974
train acc:  0.765625
train loss:  0.45184049010276794
train gradient:  0.09794940793354952
iteration : 8975
train acc:  0.7421875
train loss:  0.4812863767147064
train gradient:  0.12583680519524207
iteration : 8976
train acc:  0.765625
train loss:  0.49955275654792786
train gradient:  0.11592208784606445
iteration : 8977
train acc:  0.71875
train loss:  0.4952782392501831
train gradient:  0.13837585835222252
iteration : 8978
train acc:  0.734375
train loss:  0.49241238832473755
train gradient:  0.15091702038610966
iteration : 8979
train acc:  0.78125
train loss:  0.47034400701522827
train gradient:  0.11471507378542696
iteration : 8980
train acc:  0.7578125
train loss:  0.46867746114730835
train gradient:  0.1325138336621282
iteration : 8981
train acc:  0.796875
train loss:  0.457518994808197
train gradient:  0.10632202688240984
iteration : 8982
train acc:  0.671875
train loss:  0.5483265519142151
train gradient:  0.14671283157814985
iteration : 8983
train acc:  0.71875
train loss:  0.4965246021747589
train gradient:  0.16366736282647285
iteration : 8984
train acc:  0.7109375
train loss:  0.5279052257537842
train gradient:  0.14191188312617925
iteration : 8985
train acc:  0.71875
train loss:  0.4646967351436615
train gradient:  0.1389173073127447
iteration : 8986
train acc:  0.8203125
train loss:  0.41447913646698
train gradient:  0.08566621508421973
iteration : 8987
train acc:  0.7265625
train loss:  0.5267371535301208
train gradient:  0.16012114396716653
iteration : 8988
train acc:  0.75
train loss:  0.46753305196762085
train gradient:  0.11260831921853945
iteration : 8989
train acc:  0.7109375
train loss:  0.511406660079956
train gradient:  0.12466905022265883
iteration : 8990
train acc:  0.765625
train loss:  0.4925789535045624
train gradient:  0.12038178287343382
iteration : 8991
train acc:  0.7578125
train loss:  0.4697648882865906
train gradient:  0.1284364022626417
iteration : 8992
train acc:  0.7421875
train loss:  0.521465003490448
train gradient:  0.14353679849121354
iteration : 8993
train acc:  0.8203125
train loss:  0.42465299367904663
train gradient:  0.09337781380071551
iteration : 8994
train acc:  0.6640625
train loss:  0.5356562733650208
train gradient:  0.18271719473825027
iteration : 8995
train acc:  0.78125
train loss:  0.5003573894500732
train gradient:  0.14275508687853888
iteration : 8996
train acc:  0.7265625
train loss:  0.5165354609489441
train gradient:  0.17973573209409888
iteration : 8997
train acc:  0.734375
train loss:  0.476126492023468
train gradient:  0.12109923320038024
iteration : 8998
train acc:  0.671875
train loss:  0.5828936100006104
train gradient:  0.16460113729013293
iteration : 8999
train acc:  0.734375
train loss:  0.49602240324020386
train gradient:  0.16194088250345923
iteration : 9000
train acc:  0.7109375
train loss:  0.5500177145004272
train gradient:  0.13517621433550245
iteration : 9001
train acc:  0.75
train loss:  0.5532345771789551
train gradient:  0.1632083141002404
iteration : 9002
train acc:  0.75
train loss:  0.49291208386421204
train gradient:  0.13093937547100581
iteration : 9003
train acc:  0.75
train loss:  0.4981745779514313
train gradient:  0.12980884427621445
iteration : 9004
train acc:  0.6640625
train loss:  0.5492505431175232
train gradient:  0.1597223516367849
iteration : 9005
train acc:  0.6875
train loss:  0.5390952825546265
train gradient:  0.18227040119392762
iteration : 9006
train acc:  0.7734375
train loss:  0.4824066162109375
train gradient:  0.12702393338009627
iteration : 9007
train acc:  0.71875
train loss:  0.530668318271637
train gradient:  0.14723387912458213
iteration : 9008
train acc:  0.796875
train loss:  0.42425331473350525
train gradient:  0.1281746262052217
iteration : 9009
train acc:  0.75
train loss:  0.5062013268470764
train gradient:  0.1593769723141819
iteration : 9010
train acc:  0.6328125
train loss:  0.5776337385177612
train gradient:  0.1809820808337822
iteration : 9011
train acc:  0.7265625
train loss:  0.5176616907119751
train gradient:  0.12975984682466976
iteration : 9012
train acc:  0.7578125
train loss:  0.48894137144088745
train gradient:  0.17139842077683737
iteration : 9013
train acc:  0.7578125
train loss:  0.43464282155036926
train gradient:  0.1198857477980936
iteration : 9014
train acc:  0.75
train loss:  0.4796333909034729
train gradient:  0.15865832002733482
iteration : 9015
train acc:  0.796875
train loss:  0.44265568256378174
train gradient:  0.13365937994558694
iteration : 9016
train acc:  0.7578125
train loss:  0.49057430028915405
train gradient:  0.1529660537488846
iteration : 9017
train acc:  0.6875
train loss:  0.540825366973877
train gradient:  0.16333483884154432
iteration : 9018
train acc:  0.703125
train loss:  0.5048646330833435
train gradient:  0.15436272950140703
iteration : 9019
train acc:  0.765625
train loss:  0.4500138759613037
train gradient:  0.14043219062438012
iteration : 9020
train acc:  0.7578125
train loss:  0.47771352529525757
train gradient:  0.11317263188715268
iteration : 9021
train acc:  0.765625
train loss:  0.48915696144104004
train gradient:  0.11399960853376317
iteration : 9022
train acc:  0.703125
train loss:  0.5279639959335327
train gradient:  0.14366802350938607
iteration : 9023
train acc:  0.7109375
train loss:  0.5365006327629089
train gradient:  0.1418201175081144
iteration : 9024
train acc:  0.78125
train loss:  0.4642329514026642
train gradient:  0.1198960879122484
iteration : 9025
train acc:  0.71875
train loss:  0.4665013551712036
train gradient:  0.09985662058014805
iteration : 9026
train acc:  0.7265625
train loss:  0.4536723792552948
train gradient:  0.11830913916730736
iteration : 9027
train acc:  0.765625
train loss:  0.5189558267593384
train gradient:  0.14508931184646556
iteration : 9028
train acc:  0.734375
train loss:  0.48886990547180176
train gradient:  0.10682076630174354
iteration : 9029
train acc:  0.7109375
train loss:  0.5067610740661621
train gradient:  0.12997868567983473
iteration : 9030
train acc:  0.703125
train loss:  0.5560425519943237
train gradient:  0.15369996385998752
iteration : 9031
train acc:  0.765625
train loss:  0.47422587871551514
train gradient:  0.11792748076280865
iteration : 9032
train acc:  0.7265625
train loss:  0.5100837349891663
train gradient:  0.13529645661148798
iteration : 9033
train acc:  0.7265625
train loss:  0.47592514753341675
train gradient:  0.12088587504274463
iteration : 9034
train acc:  0.78125
train loss:  0.48204997181892395
train gradient:  0.13183970775978476
iteration : 9035
train acc:  0.7734375
train loss:  0.45818954706192017
train gradient:  0.11930344175224467
iteration : 9036
train acc:  0.7421875
train loss:  0.4778333902359009
train gradient:  0.13475545831647484
iteration : 9037
train acc:  0.8125
train loss:  0.4529270827770233
train gradient:  0.1329322200059817
iteration : 9038
train acc:  0.8046875
train loss:  0.4732666015625
train gradient:  0.1291550055769728
iteration : 9039
train acc:  0.78125
train loss:  0.4425034523010254
train gradient:  0.13676360743776583
iteration : 9040
train acc:  0.7109375
train loss:  0.5408012270927429
train gradient:  0.16266798819667228
iteration : 9041
train acc:  0.75
train loss:  0.49123355746269226
train gradient:  0.13885711155462332
iteration : 9042
train acc:  0.7421875
train loss:  0.4961375594139099
train gradient:  0.12633508660559767
iteration : 9043
train acc:  0.7734375
train loss:  0.5201336741447449
train gradient:  0.15244600091574986
iteration : 9044
train acc:  0.7734375
train loss:  0.4773947298526764
train gradient:  0.12862087800037186
iteration : 9045
train acc:  0.703125
train loss:  0.5104383230209351
train gradient:  0.11167800862556605
iteration : 9046
train acc:  0.75
train loss:  0.4876919388771057
train gradient:  0.13447153628402442
iteration : 9047
train acc:  0.71875
train loss:  0.5313174724578857
train gradient:  0.1555638611538857
iteration : 9048
train acc:  0.71875
train loss:  0.5267122983932495
train gradient:  0.1394571691757847
iteration : 9049
train acc:  0.7421875
train loss:  0.5131369829177856
train gradient:  0.15073619144190759
iteration : 9050
train acc:  0.765625
train loss:  0.4394952654838562
train gradient:  0.13772993845444048
iteration : 9051
train acc:  0.71875
train loss:  0.5444839000701904
train gradient:  0.1427190623287493
iteration : 9052
train acc:  0.6953125
train loss:  0.4855617880821228
train gradient:  0.1348775555000291
iteration : 9053
train acc:  0.7421875
train loss:  0.49138784408569336
train gradient:  0.13830111608929785
iteration : 9054
train acc:  0.703125
train loss:  0.5647111535072327
train gradient:  0.16379665564362406
iteration : 9055
train acc:  0.7734375
train loss:  0.5154854655265808
train gradient:  0.11614304363000216
iteration : 9056
train acc:  0.7265625
train loss:  0.5273206233978271
train gradient:  0.13398749454078188
iteration : 9057
train acc:  0.671875
train loss:  0.5245962142944336
train gradient:  0.1373736928197155
iteration : 9058
train acc:  0.71875
train loss:  0.5814758539199829
train gradient:  0.18891183687669488
iteration : 9059
train acc:  0.7578125
train loss:  0.46128684282302856
train gradient:  0.13429420490333704
iteration : 9060
train acc:  0.671875
train loss:  0.5250047445297241
train gradient:  0.14172546825637317
iteration : 9061
train acc:  0.765625
train loss:  0.537326455116272
train gradient:  0.14018151079344282
iteration : 9062
train acc:  0.765625
train loss:  0.4592471122741699
train gradient:  0.1025631540754106
iteration : 9063
train acc:  0.7578125
train loss:  0.45270317792892456
train gradient:  0.11661754211821206
iteration : 9064
train acc:  0.796875
train loss:  0.4135780334472656
train gradient:  0.09394525307504287
iteration : 9065
train acc:  0.765625
train loss:  0.48882290720939636
train gradient:  0.12526037517987476
iteration : 9066
train acc:  0.6796875
train loss:  0.5255170464515686
train gradient:  0.1291125737802858
iteration : 9067
train acc:  0.78125
train loss:  0.4629082679748535
train gradient:  0.1288766190922339
iteration : 9068
train acc:  0.6796875
train loss:  0.5573372840881348
train gradient:  0.18066987130858708
iteration : 9069
train acc:  0.65625
train loss:  0.5675323605537415
train gradient:  0.15707578265962588
iteration : 9070
train acc:  0.7421875
train loss:  0.5016367435455322
train gradient:  0.14476364750330006
iteration : 9071
train acc:  0.6640625
train loss:  0.5399156808853149
train gradient:  0.16100155335749033
iteration : 9072
train acc:  0.75
train loss:  0.47295212745666504
train gradient:  0.1664712836625662
iteration : 9073
train acc:  0.7109375
train loss:  0.5439382791519165
train gradient:  0.1665910586005081
iteration : 9074
train acc:  0.8125
train loss:  0.4699300229549408
train gradient:  0.09687961111018288
iteration : 9075
train acc:  0.734375
train loss:  0.46743568778038025
train gradient:  0.14586847529632357
iteration : 9076
train acc:  0.7109375
train loss:  0.4975424110889435
train gradient:  0.11326763418622776
iteration : 9077
train acc:  0.703125
train loss:  0.5059080123901367
train gradient:  0.16188722371412154
iteration : 9078
train acc:  0.6796875
train loss:  0.5929165482521057
train gradient:  0.15846503108601442
iteration : 9079
train acc:  0.6875
train loss:  0.5131717920303345
train gradient:  0.13904941718363756
iteration : 9080
train acc:  0.6796875
train loss:  0.5719137787818909
train gradient:  0.15432046164523064
iteration : 9081
train acc:  0.765625
train loss:  0.49046361446380615
train gradient:  0.17546650538679182
iteration : 9082
train acc:  0.7890625
train loss:  0.43307411670684814
train gradient:  0.08843068716979328
iteration : 9083
train acc:  0.7734375
train loss:  0.5168551206588745
train gradient:  0.14831527475069173
iteration : 9084
train acc:  0.640625
train loss:  0.630950391292572
train gradient:  0.25747273585671365
iteration : 9085
train acc:  0.75
train loss:  0.5020289421081543
train gradient:  0.12395680091934468
iteration : 9086
train acc:  0.6953125
train loss:  0.5386623740196228
train gradient:  0.13025325605951096
iteration : 9087
train acc:  0.703125
train loss:  0.5107313394546509
train gradient:  0.20604364012763615
iteration : 9088
train acc:  0.765625
train loss:  0.4572104811668396
train gradient:  0.11751679961775248
iteration : 9089
train acc:  0.7421875
train loss:  0.49867746233940125
train gradient:  0.21084440621034162
iteration : 9090
train acc:  0.71875
train loss:  0.4631192088127136
train gradient:  0.10736431229601795
iteration : 9091
train acc:  0.71875
train loss:  0.4987761676311493
train gradient:  0.1369078438309822
iteration : 9092
train acc:  0.75
train loss:  0.4577181935310364
train gradient:  0.0862961409238919
iteration : 9093
train acc:  0.75
train loss:  0.46631482243537903
train gradient:  0.11237176321404801
iteration : 9094
train acc:  0.7265625
train loss:  0.49790993332862854
train gradient:  0.12120998854672836
iteration : 9095
train acc:  0.6796875
train loss:  0.541069746017456
train gradient:  0.16019378606283896
iteration : 9096
train acc:  0.71875
train loss:  0.5183778405189514
train gradient:  0.15503296764608376
iteration : 9097
train acc:  0.7890625
train loss:  0.45392054319381714
train gradient:  0.14257468888697009
iteration : 9098
train acc:  0.7265625
train loss:  0.5149796605110168
train gradient:  0.12823451814436976
iteration : 9099
train acc:  0.671875
train loss:  0.5796783566474915
train gradient:  0.28519079620783083
iteration : 9100
train acc:  0.6953125
train loss:  0.5437419414520264
train gradient:  0.14700416595165955
iteration : 9101
train acc:  0.71875
train loss:  0.4681485891342163
train gradient:  0.1192743741287071
iteration : 9102
train acc:  0.765625
train loss:  0.46744444966316223
train gradient:  0.11285915238254159
iteration : 9103
train acc:  0.75
train loss:  0.47314977645874023
train gradient:  0.11083873431176819
iteration : 9104
train acc:  0.6796875
train loss:  0.5467780828475952
train gradient:  0.1856274862702753
iteration : 9105
train acc:  0.7109375
train loss:  0.5330249071121216
train gradient:  0.13888199094726977
iteration : 9106
train acc:  0.7265625
train loss:  0.5704635381698608
train gradient:  0.1755896241952583
iteration : 9107
train acc:  0.734375
train loss:  0.4303780198097229
train gradient:  0.17303552761564478
iteration : 9108
train acc:  0.7109375
train loss:  0.5615085959434509
train gradient:  0.17202579728369974
iteration : 9109
train acc:  0.71875
train loss:  0.5318101644515991
train gradient:  0.1921027571292771
iteration : 9110
train acc:  0.7578125
train loss:  0.4822581112384796
train gradient:  0.13229563296655444
iteration : 9111
train acc:  0.75
train loss:  0.48357951641082764
train gradient:  0.1310917774139972
iteration : 9112
train acc:  0.8046875
train loss:  0.40991687774658203
train gradient:  0.0890764762110318
iteration : 9113
train acc:  0.6796875
train loss:  0.5401102304458618
train gradient:  0.12702678041938756
iteration : 9114
train acc:  0.7265625
train loss:  0.48624566197395325
train gradient:  0.1065165649805636
iteration : 9115
train acc:  0.71875
train loss:  0.5465011596679688
train gradient:  0.1653551543213152
iteration : 9116
train acc:  0.7890625
train loss:  0.4551752805709839
train gradient:  0.15240498456478477
iteration : 9117
train acc:  0.7734375
train loss:  0.4769422113895416
train gradient:  0.10446186756287872
iteration : 9118
train acc:  0.6640625
train loss:  0.5612934827804565
train gradient:  0.15716074060330884
iteration : 9119
train acc:  0.6875
train loss:  0.5324183702468872
train gradient:  0.15281815863227108
iteration : 9120
train acc:  0.703125
train loss:  0.5145381689071655
train gradient:  0.1536177862986
iteration : 9121
train acc:  0.703125
train loss:  0.5100278854370117
train gradient:  0.16252274883965384
iteration : 9122
train acc:  0.7578125
train loss:  0.44447118043899536
train gradient:  0.12678921537930288
iteration : 9123
train acc:  0.8125
train loss:  0.45481574535369873
train gradient:  0.11480621440761564
iteration : 9124
train acc:  0.7265625
train loss:  0.5309195518493652
train gradient:  0.15149949142422614
iteration : 9125
train acc:  0.7265625
train loss:  0.5020300149917603
train gradient:  0.10715297910284464
iteration : 9126
train acc:  0.703125
train loss:  0.5653167366981506
train gradient:  0.20093730227474976
iteration : 9127
train acc:  0.75
train loss:  0.4442717134952545
train gradient:  0.09129906752644772
iteration : 9128
train acc:  0.78125
train loss:  0.527584433555603
train gradient:  0.1508631118170743
iteration : 9129
train acc:  0.78125
train loss:  0.45714759826660156
train gradient:  0.12356264004331152
iteration : 9130
train acc:  0.7578125
train loss:  0.468464732170105
train gradient:  0.1141840285797807
iteration : 9131
train acc:  0.71875
train loss:  0.49180155992507935
train gradient:  0.12782931488471505
iteration : 9132
train acc:  0.78125
train loss:  0.4462891221046448
train gradient:  0.10083492516060216
iteration : 9133
train acc:  0.75
train loss:  0.49327942728996277
train gradient:  0.12933689460537293
iteration : 9134
train acc:  0.6953125
train loss:  0.547915518283844
train gradient:  0.17675911012124385
iteration : 9135
train acc:  0.734375
train loss:  0.4614471197128296
train gradient:  0.10396710394343824
iteration : 9136
train acc:  0.7265625
train loss:  0.5121960043907166
train gradient:  0.12211096627325975
iteration : 9137
train acc:  0.78125
train loss:  0.44878315925598145
train gradient:  0.12306396950940561
iteration : 9138
train acc:  0.7109375
train loss:  0.544851541519165
train gradient:  0.17579359689719254
iteration : 9139
train acc:  0.734375
train loss:  0.549347996711731
train gradient:  0.16494983973315225
iteration : 9140
train acc:  0.7578125
train loss:  0.49397942423820496
train gradient:  0.1228578645866355
iteration : 9141
train acc:  0.7578125
train loss:  0.4891833961009979
train gradient:  0.10328727612028085
iteration : 9142
train acc:  0.7578125
train loss:  0.5410269498825073
train gradient:  0.14906186255404755
iteration : 9143
train acc:  0.734375
train loss:  0.5246884822845459
train gradient:  0.13466906696559766
iteration : 9144
train acc:  0.7109375
train loss:  0.48513561487197876
train gradient:  0.12339377704925787
iteration : 9145
train acc:  0.6875
train loss:  0.5085318088531494
train gradient:  0.13996502281975987
iteration : 9146
train acc:  0.6640625
train loss:  0.6152766942977905
train gradient:  0.20050546563871244
iteration : 9147
train acc:  0.6875
train loss:  0.5192599296569824
train gradient:  0.14918945011006074
iteration : 9148
train acc:  0.765625
train loss:  0.4958120286464691
train gradient:  0.11916651929345014
iteration : 9149
train acc:  0.7265625
train loss:  0.5126631855964661
train gradient:  0.17733773262249275
iteration : 9150
train acc:  0.7734375
train loss:  0.4471082091331482
train gradient:  0.10452564153776425
iteration : 9151
train acc:  0.7890625
train loss:  0.46092092990875244
train gradient:  0.10483798619666101
iteration : 9152
train acc:  0.75
train loss:  0.5019161701202393
train gradient:  0.15120436648737656
iteration : 9153
train acc:  0.7890625
train loss:  0.4925636053085327
train gradient:  0.14887080527996793
iteration : 9154
train acc:  0.7421875
train loss:  0.48546135425567627
train gradient:  0.11393019306923256
iteration : 9155
train acc:  0.75
train loss:  0.4984050989151001
train gradient:  0.11412328557118145
iteration : 9156
train acc:  0.71875
train loss:  0.4947943687438965
train gradient:  0.10931400060946272
iteration : 9157
train acc:  0.7109375
train loss:  0.521529495716095
train gradient:  0.1968646297345657
iteration : 9158
train acc:  0.7265625
train loss:  0.4713999629020691
train gradient:  0.11921576058086746
iteration : 9159
train acc:  0.8046875
train loss:  0.4437732696533203
train gradient:  0.12258872770177082
iteration : 9160
train acc:  0.75
train loss:  0.4934087097644806
train gradient:  0.15226375387116772
iteration : 9161
train acc:  0.78125
train loss:  0.47639745473861694
train gradient:  0.09751757210978979
iteration : 9162
train acc:  0.75
train loss:  0.4600095748901367
train gradient:  0.11507167903667631
iteration : 9163
train acc:  0.703125
train loss:  0.5194672346115112
train gradient:  0.12481487484695682
iteration : 9164
train acc:  0.7890625
train loss:  0.4580990672111511
train gradient:  0.1335373753122884
iteration : 9165
train acc:  0.734375
train loss:  0.5275747776031494
train gradient:  0.1541705211093647
iteration : 9166
train acc:  0.7421875
train loss:  0.5306211709976196
train gradient:  0.155347010078886
iteration : 9167
train acc:  0.796875
train loss:  0.45972850918769836
train gradient:  0.1364449800767485
iteration : 9168
train acc:  0.734375
train loss:  0.4712740480899811
train gradient:  0.14808565692619852
iteration : 9169
train acc:  0.796875
train loss:  0.45908287167549133
train gradient:  0.10464884445781122
iteration : 9170
train acc:  0.78125
train loss:  0.5223321318626404
train gradient:  0.156525648592988
iteration : 9171
train acc:  0.765625
train loss:  0.5223665237426758
train gradient:  0.1377605197903245
iteration : 9172
train acc:  0.7265625
train loss:  0.5163264274597168
train gradient:  0.1404453931912423
iteration : 9173
train acc:  0.71875
train loss:  0.5150690078735352
train gradient:  0.1263702856056134
iteration : 9174
train acc:  0.7265625
train loss:  0.5097222924232483
train gradient:  0.14472935638023735
iteration : 9175
train acc:  0.7734375
train loss:  0.4837978482246399
train gradient:  0.1269728652628136
iteration : 9176
train acc:  0.734375
train loss:  0.5103378295898438
train gradient:  0.1175711389704647
iteration : 9177
train acc:  0.71875
train loss:  0.5541430115699768
train gradient:  0.14086573476575753
iteration : 9178
train acc:  0.7109375
train loss:  0.5175875425338745
train gradient:  0.16640102599754014
iteration : 9179
train acc:  0.75
train loss:  0.5995444059371948
train gradient:  0.15894933751696327
iteration : 9180
train acc:  0.6875
train loss:  0.5480408072471619
train gradient:  0.17067288662915286
iteration : 9181
train acc:  0.7578125
train loss:  0.47886884212493896
train gradient:  0.1299057230843274
iteration : 9182
train acc:  0.7109375
train loss:  0.46801596879959106
train gradient:  0.10543373514757338
iteration : 9183
train acc:  0.6875
train loss:  0.5354298949241638
train gradient:  0.1465367149575189
iteration : 9184
train acc:  0.6953125
train loss:  0.5329237580299377
train gradient:  0.1397039200626944
iteration : 9185
train acc:  0.765625
train loss:  0.466608464717865
train gradient:  0.11799856147392754
iteration : 9186
train acc:  0.75
train loss:  0.5456486940383911
train gradient:  0.15058630233724812
iteration : 9187
train acc:  0.8125
train loss:  0.41187748312950134
train gradient:  0.09724096715617057
iteration : 9188
train acc:  0.7265625
train loss:  0.5270054340362549
train gradient:  0.14545866911633173
iteration : 9189
train acc:  0.7421875
train loss:  0.4892919957637787
train gradient:  0.11129264258220284
iteration : 9190
train acc:  0.7265625
train loss:  0.48113352060317993
train gradient:  0.10162888965915375
iteration : 9191
train acc:  0.71875
train loss:  0.5552705526351929
train gradient:  0.1436417082690317
iteration : 9192
train acc:  0.6953125
train loss:  0.5555493235588074
train gradient:  0.18453943130053424
iteration : 9193
train acc:  0.71875
train loss:  0.5700393319129944
train gradient:  0.16936926171590894
iteration : 9194
train acc:  0.8046875
train loss:  0.4522412419319153
train gradient:  0.09756540356076694
iteration : 9195
train acc:  0.7265625
train loss:  0.520462155342102
train gradient:  0.14923156244005842
iteration : 9196
train acc:  0.7734375
train loss:  0.47459322214126587
train gradient:  0.12417974983174145
iteration : 9197
train acc:  0.75
train loss:  0.4723452627658844
train gradient:  0.10855476373804183
iteration : 9198
train acc:  0.65625
train loss:  0.5177981853485107
train gradient:  0.12208625803373911
iteration : 9199
train acc:  0.703125
train loss:  0.5080740451812744
train gradient:  0.11904454573510277
iteration : 9200
train acc:  0.8203125
train loss:  0.41428571939468384
train gradient:  0.0898802084516455
iteration : 9201
train acc:  0.6953125
train loss:  0.5136863589286804
train gradient:  0.10689788871437768
iteration : 9202
train acc:  0.8046875
train loss:  0.3966814875602722
train gradient:  0.10413998735945693
iteration : 9203
train acc:  0.734375
train loss:  0.5001919269561768
train gradient:  0.11780467928476988
iteration : 9204
train acc:  0.8046875
train loss:  0.42184293270111084
train gradient:  0.09880024499297316
iteration : 9205
train acc:  0.7578125
train loss:  0.4556453227996826
train gradient:  0.09906482251475499
iteration : 9206
train acc:  0.78125
train loss:  0.44606930017471313
train gradient:  0.0990289277097267
iteration : 9207
train acc:  0.734375
train loss:  0.4517684578895569
train gradient:  0.10126496529572504
iteration : 9208
train acc:  0.6640625
train loss:  0.5278627872467041
train gradient:  0.15186563905524864
iteration : 9209
train acc:  0.71875
train loss:  0.5323749780654907
train gradient:  0.1363355254687198
iteration : 9210
train acc:  0.7265625
train loss:  0.5008260011672974
train gradient:  0.1565885468852474
iteration : 9211
train acc:  0.734375
train loss:  0.503925621509552
train gradient:  0.16932241238897772
iteration : 9212
train acc:  0.7421875
train loss:  0.48155689239501953
train gradient:  0.11406726343759024
iteration : 9213
train acc:  0.7265625
train loss:  0.473297119140625
train gradient:  0.10121716252440807
iteration : 9214
train acc:  0.765625
train loss:  0.46556469798088074
train gradient:  0.11915057240810681
iteration : 9215
train acc:  0.703125
train loss:  0.508233904838562
train gradient:  0.11876592875651425
iteration : 9216
train acc:  0.703125
train loss:  0.5211607217788696
train gradient:  0.16576064527752005
iteration : 9217
train acc:  0.7421875
train loss:  0.5057350993156433
train gradient:  0.1264904430088159
iteration : 9218
train acc:  0.7734375
train loss:  0.5025653839111328
train gradient:  0.15574918085731088
iteration : 9219
train acc:  0.7109375
train loss:  0.5463595986366272
train gradient:  0.1647456890899942
iteration : 9220
train acc:  0.7734375
train loss:  0.47536081075668335
train gradient:  0.1038793233780065
iteration : 9221
train acc:  0.78125
train loss:  0.550125002861023
train gradient:  0.18325959609936016
iteration : 9222
train acc:  0.6171875
train loss:  0.6027830839157104
train gradient:  0.19977211433683625
iteration : 9223
train acc:  0.6875
train loss:  0.5032599568367004
train gradient:  0.11998452077800034
iteration : 9224
train acc:  0.8046875
train loss:  0.41457921266555786
train gradient:  0.09941445693811672
iteration : 9225
train acc:  0.7734375
train loss:  0.46297165751457214
train gradient:  0.08887403732742127
iteration : 9226
train acc:  0.7578125
train loss:  0.47085320949554443
train gradient:  0.09130941966743236
iteration : 9227
train acc:  0.6953125
train loss:  0.5200985670089722
train gradient:  0.1503210204751648
iteration : 9228
train acc:  0.7421875
train loss:  0.4698560833930969
train gradient:  0.12481670556751265
iteration : 9229
train acc:  0.7265625
train loss:  0.4941486716270447
train gradient:  0.12636111903229785
iteration : 9230
train acc:  0.6875
train loss:  0.5436516404151917
train gradient:  0.20811363743038466
iteration : 9231
train acc:  0.796875
train loss:  0.45135992765426636
train gradient:  0.11387380973491998
iteration : 9232
train acc:  0.71875
train loss:  0.48209792375564575
train gradient:  0.10659229120666458
iteration : 9233
train acc:  0.71875
train loss:  0.5629339218139648
train gradient:  0.16297405035407436
iteration : 9234
train acc:  0.6875
train loss:  0.5884305834770203
train gradient:  0.1502251054238713
iteration : 9235
train acc:  0.7890625
train loss:  0.5188456773757935
train gradient:  0.12551746732980718
iteration : 9236
train acc:  0.734375
train loss:  0.4948875308036804
train gradient:  0.10636048268789236
iteration : 9237
train acc:  0.7734375
train loss:  0.48960864543914795
train gradient:  0.1424432334044063
iteration : 9238
train acc:  0.6796875
train loss:  0.5787875056266785
train gradient:  0.15184223678614334
iteration : 9239
train acc:  0.6796875
train loss:  0.6197088956832886
train gradient:  0.19476325090469
iteration : 9240
train acc:  0.734375
train loss:  0.4764379560947418
train gradient:  0.11055484438296034
iteration : 9241
train acc:  0.75
train loss:  0.5231897830963135
train gradient:  0.15017892293799434
iteration : 9242
train acc:  0.71875
train loss:  0.587803840637207
train gradient:  0.16727859471445705
iteration : 9243
train acc:  0.7890625
train loss:  0.4916895627975464
train gradient:  0.14649102996832408
iteration : 9244
train acc:  0.7734375
train loss:  0.4590378403663635
train gradient:  0.12316998767257366
iteration : 9245
train acc:  0.7734375
train loss:  0.46315816044807434
train gradient:  0.1168723004637199
iteration : 9246
train acc:  0.7421875
train loss:  0.5154415369033813
train gradient:  0.1248740080125025
iteration : 9247
train acc:  0.6640625
train loss:  0.5528398752212524
train gradient:  0.16282944900369267
iteration : 9248
train acc:  0.796875
train loss:  0.41673851013183594
train gradient:  0.1276461731377722
iteration : 9249
train acc:  0.765625
train loss:  0.4341055452823639
train gradient:  0.09927509145730122
iteration : 9250
train acc:  0.7265625
train loss:  0.5379460453987122
train gradient:  0.16805347823297367
iteration : 9251
train acc:  0.7109375
train loss:  0.48717570304870605
train gradient:  0.12566312681542785
iteration : 9252
train acc:  0.7421875
train loss:  0.4772666096687317
train gradient:  0.10315340535654759
iteration : 9253
train acc:  0.671875
train loss:  0.5061941742897034
train gradient:  0.13821230111358784
iteration : 9254
train acc:  0.7578125
train loss:  0.49304527044296265
train gradient:  0.13002901125699512
iteration : 9255
train acc:  0.75
train loss:  0.5011792182922363
train gradient:  0.1311921891199031
iteration : 9256
train acc:  0.65625
train loss:  0.5854101777076721
train gradient:  0.1964486602547711
iteration : 9257
train acc:  0.703125
train loss:  0.543042778968811
train gradient:  0.1645618481479363
iteration : 9258
train acc:  0.8125
train loss:  0.47714096307754517
train gradient:  0.1274032126673177
iteration : 9259
train acc:  0.7578125
train loss:  0.4863840639591217
train gradient:  0.1525340974110801
iteration : 9260
train acc:  0.7109375
train loss:  0.517604649066925
train gradient:  0.11523335348481673
iteration : 9261
train acc:  0.6796875
train loss:  0.5243340134620667
train gradient:  0.14752177068091032
iteration : 9262
train acc:  0.7890625
train loss:  0.4813384413719177
train gradient:  0.15960263587695905
iteration : 9263
train acc:  0.78125
train loss:  0.4899146258831024
train gradient:  0.13140029384419366
iteration : 9264
train acc:  0.8046875
train loss:  0.4560055136680603
train gradient:  0.11048905851493307
iteration : 9265
train acc:  0.75
train loss:  0.5063924193382263
train gradient:  0.1377893817533471
iteration : 9266
train acc:  0.71875
train loss:  0.5022398829460144
train gradient:  0.11955225012126056
iteration : 9267
train acc:  0.7578125
train loss:  0.4526943564414978
train gradient:  0.1119665029887307
iteration : 9268
train acc:  0.7578125
train loss:  0.4559932053089142
train gradient:  0.09520780508326362
iteration : 9269
train acc:  0.71875
train loss:  0.5739409327507019
train gradient:  0.16390263978548442
iteration : 9270
train acc:  0.7734375
train loss:  0.4975336194038391
train gradient:  0.1669281097829713
iteration : 9271
train acc:  0.7734375
train loss:  0.5349937677383423
train gradient:  0.13574606132085074
iteration : 9272
train acc:  0.765625
train loss:  0.47636449337005615
train gradient:  0.16540763892039076
iteration : 9273
train acc:  0.71875
train loss:  0.5463411808013916
train gradient:  0.13882052958560337
iteration : 9274
train acc:  0.6953125
train loss:  0.5324876308441162
train gradient:  0.15910611167111582
iteration : 9275
train acc:  0.7421875
train loss:  0.5275086164474487
train gradient:  0.1383998173790864
iteration : 9276
train acc:  0.7578125
train loss:  0.4430387020111084
train gradient:  0.10321129653588607
iteration : 9277
train acc:  0.71875
train loss:  0.4792165160179138
train gradient:  0.1200033189286518
iteration : 9278
train acc:  0.71875
train loss:  0.5003992319107056
train gradient:  0.15055643141749023
iteration : 9279
train acc:  0.6875
train loss:  0.5465680360794067
train gradient:  0.17618512147037763
iteration : 9280
train acc:  0.65625
train loss:  0.6076980829238892
train gradient:  0.19107349204669458
iteration : 9281
train acc:  0.7734375
train loss:  0.44622451066970825
train gradient:  0.1300465711897687
iteration : 9282
train acc:  0.7578125
train loss:  0.4368668496608734
train gradient:  0.1087172402012829
iteration : 9283
train acc:  0.765625
train loss:  0.4703485667705536
train gradient:  0.15654694388982737
iteration : 9284
train acc:  0.6875
train loss:  0.5549046993255615
train gradient:  0.18422901303738298
iteration : 9285
train acc:  0.6953125
train loss:  0.544745683670044
train gradient:  0.16511596181932658
iteration : 9286
train acc:  0.6953125
train loss:  0.5294705629348755
train gradient:  0.12771082385484986
iteration : 9287
train acc:  0.7265625
train loss:  0.4964185655117035
train gradient:  0.14217063655093665
iteration : 9288
train acc:  0.734375
train loss:  0.4866160452365875
train gradient:  0.1180341556777766
iteration : 9289
train acc:  0.671875
train loss:  0.5223981142044067
train gradient:  0.13079490643284133
iteration : 9290
train acc:  0.796875
train loss:  0.4396088123321533
train gradient:  0.0949182303408216
iteration : 9291
train acc:  0.6953125
train loss:  0.5311002731323242
train gradient:  0.14199855172916803
iteration : 9292
train acc:  0.7734375
train loss:  0.4622587561607361
train gradient:  0.12077802508308784
iteration : 9293
train acc:  0.8046875
train loss:  0.4570634365081787
train gradient:  0.11844543909415173
iteration : 9294
train acc:  0.75
train loss:  0.44991832971572876
train gradient:  0.1255307942260519
iteration : 9295
train acc:  0.7109375
train loss:  0.5008307695388794
train gradient:  0.11594877502638372
iteration : 9296
train acc:  0.7265625
train loss:  0.5014628171920776
train gradient:  0.12974580977500055
iteration : 9297
train acc:  0.75
train loss:  0.4976606070995331
train gradient:  0.15738865645984873
iteration : 9298
train acc:  0.671875
train loss:  0.5571764707565308
train gradient:  0.1352009266114662
iteration : 9299
train acc:  0.734375
train loss:  0.49545449018478394
train gradient:  0.13515184412787512
iteration : 9300
train acc:  0.703125
train loss:  0.5000497102737427
train gradient:  0.1426085274189543
iteration : 9301
train acc:  0.75
train loss:  0.519528865814209
train gradient:  0.14342120527648958
iteration : 9302
train acc:  0.75
train loss:  0.4680454134941101
train gradient:  0.11515392487288002
iteration : 9303
train acc:  0.7265625
train loss:  0.48609310388565063
train gradient:  0.15432169357551997
iteration : 9304
train acc:  0.7265625
train loss:  0.5215044021606445
train gradient:  0.140982247907843
iteration : 9305
train acc:  0.796875
train loss:  0.4810590445995331
train gradient:  0.14905919762238995
iteration : 9306
train acc:  0.7421875
train loss:  0.4851202368736267
train gradient:  0.12666515986580007
iteration : 9307
train acc:  0.7265625
train loss:  0.4922289252281189
train gradient:  0.15644125945432108
iteration : 9308
train acc:  0.734375
train loss:  0.5085940957069397
train gradient:  0.17262998273996638
iteration : 9309
train acc:  0.71875
train loss:  0.48386895656585693
train gradient:  0.12818427278588082
iteration : 9310
train acc:  0.7265625
train loss:  0.511833131313324
train gradient:  0.12296462211555839
iteration : 9311
train acc:  0.625
train loss:  0.6619297862052917
train gradient:  0.24963680609941513
iteration : 9312
train acc:  0.75
train loss:  0.48401880264282227
train gradient:  0.11668076579068064
iteration : 9313
train acc:  0.7578125
train loss:  0.5168440937995911
train gradient:  0.13297158045792057
iteration : 9314
train acc:  0.734375
train loss:  0.55194091796875
train gradient:  0.16100868601172927
iteration : 9315
train acc:  0.71875
train loss:  0.522770881652832
train gradient:  0.15773812219381927
iteration : 9316
train acc:  0.7578125
train loss:  0.4929012656211853
train gradient:  0.1506108352772903
iteration : 9317
train acc:  0.7421875
train loss:  0.480501651763916
train gradient:  0.14711600432753483
iteration : 9318
train acc:  0.7109375
train loss:  0.5588110685348511
train gradient:  0.15962252352306797
iteration : 9319
train acc:  0.6875
train loss:  0.5785495042800903
train gradient:  0.19441900712766547
iteration : 9320
train acc:  0.671875
train loss:  0.5508015155792236
train gradient:  0.13209376776742135
iteration : 9321
train acc:  0.6953125
train loss:  0.492941290140152
train gradient:  0.14182144039595462
iteration : 9322
train acc:  0.703125
train loss:  0.5370413064956665
train gradient:  0.15154865970879827
iteration : 9323
train acc:  0.7109375
train loss:  0.5344662666320801
train gradient:  0.15608169219339085
iteration : 9324
train acc:  0.75
train loss:  0.5381504893302917
train gradient:  0.12696575835488744
iteration : 9325
train acc:  0.7421875
train loss:  0.5024881958961487
train gradient:  0.12683218949902034
iteration : 9326
train acc:  0.7421875
train loss:  0.4887368083000183
train gradient:  0.12001197274060067
iteration : 9327
train acc:  0.75
train loss:  0.49396854639053345
train gradient:  0.1366421086694815
iteration : 9328
train acc:  0.734375
train loss:  0.4610707759857178
train gradient:  0.1057140175949689
iteration : 9329
train acc:  0.7265625
train loss:  0.480865478515625
train gradient:  0.10233957180616485
iteration : 9330
train acc:  0.734375
train loss:  0.4663226902484894
train gradient:  0.1623674127473228
iteration : 9331
train acc:  0.7421875
train loss:  0.49323901534080505
train gradient:  0.14610868143241648
iteration : 9332
train acc:  0.75
train loss:  0.5036449432373047
train gradient:  0.15990836224038135
iteration : 9333
train acc:  0.7734375
train loss:  0.49962717294692993
train gradient:  0.14051054987167994
iteration : 9334
train acc:  0.796875
train loss:  0.42942824959754944
train gradient:  0.10013685945850322
iteration : 9335
train acc:  0.7265625
train loss:  0.5191075801849365
train gradient:  0.11838948196408777
iteration : 9336
train acc:  0.7734375
train loss:  0.4660490155220032
train gradient:  0.10849380312651456
iteration : 9337
train acc:  0.734375
train loss:  0.4860251545906067
train gradient:  0.14507554774817805
iteration : 9338
train acc:  0.765625
train loss:  0.4959011673927307
train gradient:  0.11995137144847758
iteration : 9339
train acc:  0.7578125
train loss:  0.47910475730895996
train gradient:  0.1077791797292384
iteration : 9340
train acc:  0.703125
train loss:  0.5547530055046082
train gradient:  0.16741502308686024
iteration : 9341
train acc:  0.6953125
train loss:  0.49087756872177124
train gradient:  0.17155199092010764
iteration : 9342
train acc:  0.734375
train loss:  0.499584823846817
train gradient:  0.15321918647225646
iteration : 9343
train acc:  0.7890625
train loss:  0.4521547555923462
train gradient:  0.12075261050177706
iteration : 9344
train acc:  0.8203125
train loss:  0.42030489444732666
train gradient:  0.09620559492216778
iteration : 9345
train acc:  0.671875
train loss:  0.5922931432723999
train gradient:  0.17657896326549333
iteration : 9346
train acc:  0.7734375
train loss:  0.43813425302505493
train gradient:  0.1102598893276482
iteration : 9347
train acc:  0.71875
train loss:  0.5264273881912231
train gradient:  0.16187080845147667
iteration : 9348
train acc:  0.75
train loss:  0.46687623858451843
train gradient:  0.11165144617723465
iteration : 9349
train acc:  0.765625
train loss:  0.4507124125957489
train gradient:  0.10644021944846596
iteration : 9350
train acc:  0.7421875
train loss:  0.5002902746200562
train gradient:  0.13677410890171093
iteration : 9351
train acc:  0.7734375
train loss:  0.43768489360809326
train gradient:  0.09620703005575296
iteration : 9352
train acc:  0.75
train loss:  0.4996389150619507
train gradient:  0.1123291439668621
iteration : 9353
train acc:  0.78125
train loss:  0.4957452118396759
train gradient:  0.1024144511112318
iteration : 9354
train acc:  0.7421875
train loss:  0.4683041572570801
train gradient:  0.1304740586571111
iteration : 9355
train acc:  0.7734375
train loss:  0.47443512082099915
train gradient:  0.14484664281422288
iteration : 9356
train acc:  0.6875
train loss:  0.5375965237617493
train gradient:  0.13541658751475041
iteration : 9357
train acc:  0.671875
train loss:  0.5373415946960449
train gradient:  0.15411860429197752
iteration : 9358
train acc:  0.7734375
train loss:  0.5143033266067505
train gradient:  0.15287665104170817
iteration : 9359
train acc:  0.7109375
train loss:  0.5041459798812866
train gradient:  0.14428472529222147
iteration : 9360
train acc:  0.7890625
train loss:  0.4456983208656311
train gradient:  0.09999396266340504
iteration : 9361
train acc:  0.7890625
train loss:  0.42296311259269714
train gradient:  0.09809682162159329
iteration : 9362
train acc:  0.8125
train loss:  0.4357335865497589
train gradient:  0.0911359690539779
iteration : 9363
train acc:  0.6796875
train loss:  0.5522453784942627
train gradient:  0.16525360618838286
iteration : 9364
train acc:  0.6875
train loss:  0.4799356460571289
train gradient:  0.12151113783648324
iteration : 9365
train acc:  0.71875
train loss:  0.5387756824493408
train gradient:  0.15008632095522206
iteration : 9366
train acc:  0.6953125
train loss:  0.5383509397506714
train gradient:  0.15532992387890543
iteration : 9367
train acc:  0.765625
train loss:  0.49241915345191956
train gradient:  0.1436837942605521
iteration : 9368
train acc:  0.6953125
train loss:  0.5175063610076904
train gradient:  0.1484973685267904
iteration : 9369
train acc:  0.8125
train loss:  0.4315791726112366
train gradient:  0.1251779924441057
iteration : 9370
train acc:  0.734375
train loss:  0.46917542815208435
train gradient:  0.1184381809422115
iteration : 9371
train acc:  0.6953125
train loss:  0.5328012108802795
train gradient:  0.14336669494379092
iteration : 9372
train acc:  0.6484375
train loss:  0.6342827081680298
train gradient:  0.21305405866078825
iteration : 9373
train acc:  0.765625
train loss:  0.4516564905643463
train gradient:  0.12186342785745835
iteration : 9374
train acc:  0.7265625
train loss:  0.5357252359390259
train gradient:  0.13505474089361869
iteration : 9375
train acc:  0.6875
train loss:  0.5703079700469971
train gradient:  0.17841761794095345
iteration : 9376
train acc:  0.8125
train loss:  0.42069920897483826
train gradient:  0.11149827571937819
iteration : 9377
train acc:  0.828125
train loss:  0.3896854519844055
train gradient:  0.07497313914912004
iteration : 9378
train acc:  0.6953125
train loss:  0.5251561403274536
train gradient:  0.1457894892553157
iteration : 9379
train acc:  0.75
train loss:  0.4845074713230133
train gradient:  0.13896525032812432
iteration : 9380
train acc:  0.7578125
train loss:  0.5092226266860962
train gradient:  0.12399145368800142
iteration : 9381
train acc:  0.78125
train loss:  0.48770949244499207
train gradient:  0.12635392174752358
iteration : 9382
train acc:  0.75
train loss:  0.4699443280696869
train gradient:  0.12427218157405084
iteration : 9383
train acc:  0.7734375
train loss:  0.4947366416454315
train gradient:  0.1347723536433395
iteration : 9384
train acc:  0.7578125
train loss:  0.42126455903053284
train gradient:  0.0892788158918645
iteration : 9385
train acc:  0.703125
train loss:  0.5033606290817261
train gradient:  0.11078351203327479
iteration : 9386
train acc:  0.7578125
train loss:  0.41890931129455566
train gradient:  0.08722773876097314
iteration : 9387
train acc:  0.6875
train loss:  0.5544910430908203
train gradient:  0.15353399538037416
iteration : 9388
train acc:  0.703125
train loss:  0.5409339070320129
train gradient:  0.14810836231071073
iteration : 9389
train acc:  0.765625
train loss:  0.45660170912742615
train gradient:  0.12525582107839106
iteration : 9390
train acc:  0.7109375
train loss:  0.5474883913993835
train gradient:  0.15985332893822923
iteration : 9391
train acc:  0.765625
train loss:  0.5138753652572632
train gradient:  0.14012205689989476
iteration : 9392
train acc:  0.671875
train loss:  0.5333964824676514
train gradient:  0.1448575924671225
iteration : 9393
train acc:  0.7265625
train loss:  0.495305597782135
train gradient:  0.12891409411292726
iteration : 9394
train acc:  0.7421875
train loss:  0.4873772859573364
train gradient:  0.12894092129455575
iteration : 9395
train acc:  0.6875
train loss:  0.540363073348999
train gradient:  0.17091898991783522
iteration : 9396
train acc:  0.765625
train loss:  0.5005912780761719
train gradient:  0.1089351368370294
iteration : 9397
train acc:  0.734375
train loss:  0.49338242411613464
train gradient:  0.1327619922857759
iteration : 9398
train acc:  0.734375
train loss:  0.501317024230957
train gradient:  0.12879005148931055
iteration : 9399
train acc:  0.7109375
train loss:  0.5056127309799194
train gradient:  0.15019135265415512
iteration : 9400
train acc:  0.734375
train loss:  0.475064218044281
train gradient:  0.10430492475463614
iteration : 9401
train acc:  0.6796875
train loss:  0.5320271253585815
train gradient:  0.16824711617962118
iteration : 9402
train acc:  0.765625
train loss:  0.5347669720649719
train gradient:  0.16206125336211508
iteration : 9403
train acc:  0.78125
train loss:  0.4526902139186859
train gradient:  0.09258606388180393
iteration : 9404
train acc:  0.6953125
train loss:  0.6181250810623169
train gradient:  0.2376698410298517
iteration : 9405
train acc:  0.703125
train loss:  0.4826500713825226
train gradient:  0.1391131017608493
iteration : 9406
train acc:  0.71875
train loss:  0.5175156593322754
train gradient:  0.1047157460048282
iteration : 9407
train acc:  0.734375
train loss:  0.46183672547340393
train gradient:  0.16979464118042686
iteration : 9408
train acc:  0.65625
train loss:  0.5565831065177917
train gradient:  0.15327322402096683
iteration : 9409
train acc:  0.8125
train loss:  0.42032191157341003
train gradient:  0.09315130917436229
iteration : 9410
train acc:  0.8046875
train loss:  0.4419263005256653
train gradient:  0.10516819291776248
iteration : 9411
train acc:  0.6640625
train loss:  0.5251436233520508
train gradient:  0.16186320602762916
iteration : 9412
train acc:  0.734375
train loss:  0.47730761766433716
train gradient:  0.10108054635422092
iteration : 9413
train acc:  0.75
train loss:  0.48384207487106323
train gradient:  0.1057289215770002
iteration : 9414
train acc:  0.8046875
train loss:  0.4180545210838318
train gradient:  0.10599062924837911
iteration : 9415
train acc:  0.734375
train loss:  0.49484968185424805
train gradient:  0.1243980352922461
iteration : 9416
train acc:  0.6953125
train loss:  0.5855353474617004
train gradient:  0.1961607572744145
iteration : 9417
train acc:  0.7421875
train loss:  0.47035035490989685
train gradient:  0.12686369398994646
iteration : 9418
train acc:  0.7734375
train loss:  0.4255461096763611
train gradient:  0.12195132826351292
iteration : 9419
train acc:  0.7109375
train loss:  0.5256004333496094
train gradient:  0.1302660323778989
iteration : 9420
train acc:  0.6875
train loss:  0.5238368511199951
train gradient:  0.1398660611462741
iteration : 9421
train acc:  0.7421875
train loss:  0.5184949636459351
train gradient:  0.14310501505834095
iteration : 9422
train acc:  0.7109375
train loss:  0.5090522766113281
train gradient:  0.13259639795782632
iteration : 9423
train acc:  0.6953125
train loss:  0.4869432747364044
train gradient:  0.1188188187020726
iteration : 9424
train acc:  0.796875
train loss:  0.42103996872901917
train gradient:  0.11805844871466793
iteration : 9425
train acc:  0.734375
train loss:  0.5003607869148254
train gradient:  0.12927756534888124
iteration : 9426
train acc:  0.71875
train loss:  0.5342307090759277
train gradient:  0.15342720385728287
iteration : 9427
train acc:  0.71875
train loss:  0.5038725733757019
train gradient:  0.12396765643799511
iteration : 9428
train acc:  0.78125
train loss:  0.4533390402793884
train gradient:  0.11079324690274212
iteration : 9429
train acc:  0.6484375
train loss:  0.5599942803382874
train gradient:  0.1416797728363657
iteration : 9430
train acc:  0.734375
train loss:  0.517821729183197
train gradient:  0.15425866018397616
iteration : 9431
train acc:  0.7734375
train loss:  0.4404734969139099
train gradient:  0.11656417215017592
iteration : 9432
train acc:  0.7890625
train loss:  0.4538322687149048
train gradient:  0.1268973826733807
iteration : 9433
train acc:  0.65625
train loss:  0.5785968899726868
train gradient:  0.1784672509074846
iteration : 9434
train acc:  0.7578125
train loss:  0.5013434886932373
train gradient:  0.11379321338967496
iteration : 9435
train acc:  0.6875
train loss:  0.5330809950828552
train gradient:  0.14948864754655922
iteration : 9436
train acc:  0.71875
train loss:  0.4955487847328186
train gradient:  0.10761652605300451
iteration : 9437
train acc:  0.7109375
train loss:  0.52245032787323
train gradient:  0.12407776542996961
iteration : 9438
train acc:  0.703125
train loss:  0.5420958399772644
train gradient:  0.1800482541748235
iteration : 9439
train acc:  0.6640625
train loss:  0.5465301275253296
train gradient:  0.15373826616224787
iteration : 9440
train acc:  0.6796875
train loss:  0.5500741600990295
train gradient:  0.16295191867047754
iteration : 9441
train acc:  0.78125
train loss:  0.442817747592926
train gradient:  0.10363693884424163
iteration : 9442
train acc:  0.734375
train loss:  0.4820077419281006
train gradient:  0.13214005699523712
iteration : 9443
train acc:  0.75
train loss:  0.48333361744880676
train gradient:  0.13013847211250423
iteration : 9444
train acc:  0.7734375
train loss:  0.45827072858810425
train gradient:  0.13724964275314766
iteration : 9445
train acc:  0.7890625
train loss:  0.447532057762146
train gradient:  0.11627657788617507
iteration : 9446
train acc:  0.71875
train loss:  0.5292418599128723
train gradient:  0.16039875089676062
iteration : 9447
train acc:  0.6875
train loss:  0.5315550565719604
train gradient:  0.1271135232748601
iteration : 9448
train acc:  0.7109375
train loss:  0.5048720836639404
train gradient:  0.14749033234523118
iteration : 9449
train acc:  0.703125
train loss:  0.5097619295120239
train gradient:  0.12385128980732756
iteration : 9450
train acc:  0.6953125
train loss:  0.5411896705627441
train gradient:  0.15291918585116815
iteration : 9451
train acc:  0.7421875
train loss:  0.4740316867828369
train gradient:  0.12936608691380533
iteration : 9452
train acc:  0.734375
train loss:  0.4904310703277588
train gradient:  0.12801666011994473
iteration : 9453
train acc:  0.703125
train loss:  0.5095040202140808
train gradient:  0.12970106231101158
iteration : 9454
train acc:  0.75
train loss:  0.5329127311706543
train gradient:  0.12020852583344556
iteration : 9455
train acc:  0.703125
train loss:  0.5279623866081238
train gradient:  0.1468831379430067
iteration : 9456
train acc:  0.796875
train loss:  0.46483710408210754
train gradient:  0.11397548218411784
iteration : 9457
train acc:  0.7109375
train loss:  0.514419674873352
train gradient:  0.13279460487189992
iteration : 9458
train acc:  0.7890625
train loss:  0.43328601121902466
train gradient:  0.12167821160440571
iteration : 9459
train acc:  0.703125
train loss:  0.5059979557991028
train gradient:  0.13087379847091674
iteration : 9460
train acc:  0.6796875
train loss:  0.5848912000656128
train gradient:  0.17291191447436222
iteration : 9461
train acc:  0.6953125
train loss:  0.4907541871070862
train gradient:  0.13093022049186948
iteration : 9462
train acc:  0.71875
train loss:  0.5092883110046387
train gradient:  0.15629879484419018
iteration : 9463
train acc:  0.78125
train loss:  0.4298638105392456
train gradient:  0.09273063114347822
iteration : 9464
train acc:  0.6875
train loss:  0.5438807010650635
train gradient:  0.1543928141188782
iteration : 9465
train acc:  0.6953125
train loss:  0.5227439403533936
train gradient:  0.14028368180061093
iteration : 9466
train acc:  0.6328125
train loss:  0.5626472234725952
train gradient:  0.18367768043168783
iteration : 9467
train acc:  0.7734375
train loss:  0.4712921679019928
train gradient:  0.1094469530638067
iteration : 9468
train acc:  0.7890625
train loss:  0.45345446467399597
train gradient:  0.11607729424005003
iteration : 9469
train acc:  0.7890625
train loss:  0.4092186689376831
train gradient:  0.09747273542453169
iteration : 9470
train acc:  0.734375
train loss:  0.5038660764694214
train gradient:  0.1497675465157571
iteration : 9471
train acc:  0.8359375
train loss:  0.4462270140647888
train gradient:  0.12517729112538897
iteration : 9472
train acc:  0.7578125
train loss:  0.507917582988739
train gradient:  0.14230263758451672
iteration : 9473
train acc:  0.7578125
train loss:  0.4952270984649658
train gradient:  0.12590286102447026
iteration : 9474
train acc:  0.7578125
train loss:  0.48138427734375
train gradient:  0.16558547628359105
iteration : 9475
train acc:  0.7890625
train loss:  0.4517768621444702
train gradient:  0.13189102484049964
iteration : 9476
train acc:  0.7734375
train loss:  0.4827534556388855
train gradient:  0.12129247822227428
iteration : 9477
train acc:  0.7734375
train loss:  0.46585172414779663
train gradient:  0.11852641190182053
iteration : 9478
train acc:  0.734375
train loss:  0.49542349576950073
train gradient:  0.1279662548411482
iteration : 9479
train acc:  0.671875
train loss:  0.49425145983695984
train gradient:  0.12276166787805652
iteration : 9480
train acc:  0.7265625
train loss:  0.4691612422466278
train gradient:  0.10289847730466897
iteration : 9481
train acc:  0.7578125
train loss:  0.4726077914237976
train gradient:  0.11597727788013076
iteration : 9482
train acc:  0.7578125
train loss:  0.5155652761459351
train gradient:  0.15064381537912222
iteration : 9483
train acc:  0.7578125
train loss:  0.4463340640068054
train gradient:  0.147067327645094
iteration : 9484
train acc:  0.796875
train loss:  0.45329952239990234
train gradient:  0.10468901045858538
iteration : 9485
train acc:  0.734375
train loss:  0.5241735577583313
train gradient:  0.1375940460617564
iteration : 9486
train acc:  0.703125
train loss:  0.51015305519104
train gradient:  0.12572891320065094
iteration : 9487
train acc:  0.71875
train loss:  0.49095067381858826
train gradient:  0.12256599509217074
iteration : 9488
train acc:  0.7578125
train loss:  0.45863860845565796
train gradient:  0.13993355245256028
iteration : 9489
train acc:  0.71875
train loss:  0.5263639092445374
train gradient:  0.13850704839209793
iteration : 9490
train acc:  0.75
train loss:  0.4794136583805084
train gradient:  0.13318694504054512
iteration : 9491
train acc:  0.8125
train loss:  0.4218764901161194
train gradient:  0.10801578157529136
iteration : 9492
train acc:  0.734375
train loss:  0.5072892904281616
train gradient:  0.172011295601304
iteration : 9493
train acc:  0.7421875
train loss:  0.45706725120544434
train gradient:  0.11446433025896596
iteration : 9494
train acc:  0.7109375
train loss:  0.5037168264389038
train gradient:  0.15180767144829221
iteration : 9495
train acc:  0.8046875
train loss:  0.39509230852127075
train gradient:  0.07786596793297818
iteration : 9496
train acc:  0.7421875
train loss:  0.5201108455657959
train gradient:  0.1743204686956124
iteration : 9497
train acc:  0.7265625
train loss:  0.4794679582118988
train gradient:  0.1268577806007427
iteration : 9498
train acc:  0.7578125
train loss:  0.5252473950386047
train gradient:  0.12871864644509035
iteration : 9499
train acc:  0.734375
train loss:  0.47872012853622437
train gradient:  0.14388117953895108
iteration : 9500
train acc:  0.734375
train loss:  0.5063614845275879
train gradient:  0.12331726707109324
iteration : 9501
train acc:  0.7890625
train loss:  0.45352283120155334
train gradient:  0.10730230609004757
iteration : 9502
train acc:  0.71875
train loss:  0.47762495279312134
train gradient:  0.12977243768700167
iteration : 9503
train acc:  0.6875
train loss:  0.5104660987854004
train gradient:  0.16261775327213734
iteration : 9504
train acc:  0.765625
train loss:  0.451819509267807
train gradient:  0.13356492307799972
iteration : 9505
train acc:  0.7734375
train loss:  0.4805176854133606
train gradient:  0.1315065490541332
iteration : 9506
train acc:  0.6875
train loss:  0.525492250919342
train gradient:  0.13208350963884302
iteration : 9507
train acc:  0.7890625
train loss:  0.4421845078468323
train gradient:  0.1173097618095891
iteration : 9508
train acc:  0.734375
train loss:  0.5120176076889038
train gradient:  0.14831735036773142
iteration : 9509
train acc:  0.6796875
train loss:  0.616999089717865
train gradient:  0.16306611744358918
iteration : 9510
train acc:  0.7421875
train loss:  0.49805864691734314
train gradient:  0.14387385867784208
iteration : 9511
train acc:  0.703125
train loss:  0.5353701114654541
train gradient:  0.13106769303575838
iteration : 9512
train acc:  0.7109375
train loss:  0.5284258723258972
train gradient:  0.14105828131975173
iteration : 9513
train acc:  0.71875
train loss:  0.540636420249939
train gradient:  0.16333192665640134
iteration : 9514
train acc:  0.78125
train loss:  0.43818700313568115
train gradient:  0.1095911004535556
iteration : 9515
train acc:  0.7109375
train loss:  0.5063470005989075
train gradient:  0.13630701688172875
iteration : 9516
train acc:  0.75
train loss:  0.47559690475463867
train gradient:  0.11634298492232346
iteration : 9517
train acc:  0.7265625
train loss:  0.5286348462104797
train gradient:  0.16798123886419525
iteration : 9518
train acc:  0.7578125
train loss:  0.46525079011917114
train gradient:  0.11117488734281028
iteration : 9519
train acc:  0.8046875
train loss:  0.4898267984390259
train gradient:  0.10475516471693432
iteration : 9520
train acc:  0.671875
train loss:  0.5464729070663452
train gradient:  0.16699355388767895
iteration : 9521
train acc:  0.703125
train loss:  0.4949875771999359
train gradient:  0.18766764648915785
iteration : 9522
train acc:  0.84375
train loss:  0.42760518193244934
train gradient:  0.12660633532366988
iteration : 9523
train acc:  0.78125
train loss:  0.4797380268573761
train gradient:  0.11889055614735822
iteration : 9524
train acc:  0.703125
train loss:  0.487718403339386
train gradient:  0.11154177429077741
iteration : 9525
train acc:  0.671875
train loss:  0.5783384442329407
train gradient:  0.15598412489861951
iteration : 9526
train acc:  0.7265625
train loss:  0.5638214349746704
train gradient:  0.15788078303289946
iteration : 9527
train acc:  0.6875
train loss:  0.5713772773742676
train gradient:  0.15819842426810599
iteration : 9528
train acc:  0.828125
train loss:  0.44409576058387756
train gradient:  0.11689119081958486
iteration : 9529
train acc:  0.7421875
train loss:  0.5287062525749207
train gradient:  0.17399719239639255
iteration : 9530
train acc:  0.8125
train loss:  0.43085551261901855
train gradient:  0.08138751205224977
iteration : 9531
train acc:  0.734375
train loss:  0.49412551522254944
train gradient:  0.15570197330988245
iteration : 9532
train acc:  0.734375
train loss:  0.5305745601654053
train gradient:  0.181646702432666
iteration : 9533
train acc:  0.7265625
train loss:  0.5177779197692871
train gradient:  0.13400082006741748
iteration : 9534
train acc:  0.7578125
train loss:  0.5130289196968079
train gradient:  0.13526462530036937
iteration : 9535
train acc:  0.7109375
train loss:  0.5463235974311829
train gradient:  0.23261160270874448
iteration : 9536
train acc:  0.75
train loss:  0.47942453622817993
train gradient:  0.14329813084470278
iteration : 9537
train acc:  0.7734375
train loss:  0.4662649631500244
train gradient:  0.12661668354091313
iteration : 9538
train acc:  0.703125
train loss:  0.5317014455795288
train gradient:  0.14885260492979951
iteration : 9539
train acc:  0.8125
train loss:  0.44857650995254517
train gradient:  0.12185834227311995
iteration : 9540
train acc:  0.7578125
train loss:  0.5129690170288086
train gradient:  0.12812821967142426
iteration : 9541
train acc:  0.7890625
train loss:  0.47993528842926025
train gradient:  0.19250562996144177
iteration : 9542
train acc:  0.765625
train loss:  0.4679196774959564
train gradient:  0.12938272416320531
iteration : 9543
train acc:  0.703125
train loss:  0.5324211120605469
train gradient:  0.15460989620821186
iteration : 9544
train acc:  0.6953125
train loss:  0.5384590029716492
train gradient:  0.15164542455447116
iteration : 9545
train acc:  0.765625
train loss:  0.4748826324939728
train gradient:  0.1241268900559057
iteration : 9546
train acc:  0.7421875
train loss:  0.5564982891082764
train gradient:  0.14660447448010036
iteration : 9547
train acc:  0.796875
train loss:  0.474082350730896
train gradient:  0.1333254050303208
iteration : 9548
train acc:  0.734375
train loss:  0.5512963533401489
train gradient:  0.1650276012764402
iteration : 9549
train acc:  0.7265625
train loss:  0.4969792366027832
train gradient:  0.12521130501180006
iteration : 9550
train acc:  0.7421875
train loss:  0.521937370300293
train gradient:  0.13494124543136782
iteration : 9551
train acc:  0.7265625
train loss:  0.48913878202438354
train gradient:  0.1562625347977396
iteration : 9552
train acc:  0.765625
train loss:  0.4905068278312683
train gradient:  0.1369843116174692
iteration : 9553
train acc:  0.75
train loss:  0.4414588212966919
train gradient:  0.09707086057759368
iteration : 9554
train acc:  0.734375
train loss:  0.4940037131309509
train gradient:  0.14440396352463591
iteration : 9555
train acc:  0.734375
train loss:  0.5434871912002563
train gradient:  0.15973232590027225
iteration : 9556
train acc:  0.7578125
train loss:  0.4616279900074005
train gradient:  0.11351443121637861
iteration : 9557
train acc:  0.703125
train loss:  0.48870182037353516
train gradient:  0.1161444767157679
iteration : 9558
train acc:  0.78125
train loss:  0.4504722058773041
train gradient:  0.10010954475504104
iteration : 9559
train acc:  0.6875
train loss:  0.5437414646148682
train gradient:  0.11999201960423973
iteration : 9560
train acc:  0.734375
train loss:  0.5478100776672363
train gradient:  0.17124108943150246
iteration : 9561
train acc:  0.765625
train loss:  0.44204574823379517
train gradient:  0.1198093040779803
iteration : 9562
train acc:  0.75
train loss:  0.45663195848464966
train gradient:  0.11282535589071105
iteration : 9563
train acc:  0.703125
train loss:  0.5069039463996887
train gradient:  0.12292218400804678
iteration : 9564
train acc:  0.75
train loss:  0.507910966873169
train gradient:  0.13530890795005482
iteration : 9565
train acc:  0.703125
train loss:  0.6018603444099426
train gradient:  0.19400090029437206
iteration : 9566
train acc:  0.734375
train loss:  0.5090364813804626
train gradient:  0.1332851269234233
iteration : 9567
train acc:  0.796875
train loss:  0.42891019582748413
train gradient:  0.14113950830018937
iteration : 9568
train acc:  0.71875
train loss:  0.5519397854804993
train gradient:  0.1320789403329644
iteration : 9569
train acc:  0.765625
train loss:  0.4336642026901245
train gradient:  0.11280575184082595
iteration : 9570
train acc:  0.8125
train loss:  0.4468134343624115
train gradient:  0.10142590162138825
iteration : 9571
train acc:  0.734375
train loss:  0.5213122963905334
train gradient:  0.13757775853045312
iteration : 9572
train acc:  0.71875
train loss:  0.5044457912445068
train gradient:  0.1350579385602712
iteration : 9573
train acc:  0.765625
train loss:  0.4372456967830658
train gradient:  0.15377484438855177
iteration : 9574
train acc:  0.703125
train loss:  0.523787260055542
train gradient:  0.13346531340469042
iteration : 9575
train acc:  0.7109375
train loss:  0.5039299130439758
train gradient:  0.13571350665522833
iteration : 9576
train acc:  0.6484375
train loss:  0.5877550840377808
train gradient:  0.1664041283743885
iteration : 9577
train acc:  0.6796875
train loss:  0.5286567211151123
train gradient:  0.16426074116334283
iteration : 9578
train acc:  0.6875
train loss:  0.5299181938171387
train gradient:  0.14966814808500262
iteration : 9579
train acc:  0.734375
train loss:  0.49763888120651245
train gradient:  0.1140062996772653
iteration : 9580
train acc:  0.765625
train loss:  0.48359400033950806
train gradient:  0.12868254691021033
iteration : 9581
train acc:  0.6875
train loss:  0.5725002884864807
train gradient:  0.18391438685290284
iteration : 9582
train acc:  0.78125
train loss:  0.41720426082611084
train gradient:  0.08881360224760931
iteration : 9583
train acc:  0.7578125
train loss:  0.4888722896575928
train gradient:  0.1321900089103623
iteration : 9584
train acc:  0.78125
train loss:  0.4429490566253662
train gradient:  0.09746211473744773
iteration : 9585
train acc:  0.734375
train loss:  0.496232271194458
train gradient:  0.14517885534304026
iteration : 9586
train acc:  0.7265625
train loss:  0.5158557891845703
train gradient:  0.13838549152212326
iteration : 9587
train acc:  0.7265625
train loss:  0.5467474460601807
train gradient:  0.16520428726814468
iteration : 9588
train acc:  0.7578125
train loss:  0.47462183237075806
train gradient:  0.13740429754556677
iteration : 9589
train acc:  0.7109375
train loss:  0.5349888205528259
train gradient:  0.1380283137878623
iteration : 9590
train acc:  0.671875
train loss:  0.6089919805526733
train gradient:  0.2177853212216645
iteration : 9591
train acc:  0.71875
train loss:  0.46796032786369324
train gradient:  0.13239609363814786
iteration : 9592
train acc:  0.6875
train loss:  0.5236923098564148
train gradient:  0.1181558042770038
iteration : 9593
train acc:  0.65625
train loss:  0.6027165055274963
train gradient:  0.15429224370351025
iteration : 9594
train acc:  0.7109375
train loss:  0.5125782489776611
train gradient:  0.15024878159717367
iteration : 9595
train acc:  0.7578125
train loss:  0.45974278450012207
train gradient:  0.10353414525475146
iteration : 9596
train acc:  0.765625
train loss:  0.49382662773132324
train gradient:  0.1045801429295551
iteration : 9597
train acc:  0.796875
train loss:  0.42783012986183167
train gradient:  0.10241161718449218
iteration : 9598
train acc:  0.7421875
train loss:  0.4562273621559143
train gradient:  0.0993448824200238
iteration : 9599
train acc:  0.71875
train loss:  0.5380469560623169
train gradient:  0.18763086204061818
iteration : 9600
train acc:  0.703125
train loss:  0.5172119140625
train gradient:  0.12715100431553944
iteration : 9601
train acc:  0.6875
train loss:  0.6003101468086243
train gradient:  0.1715210127604147
iteration : 9602
train acc:  0.6953125
train loss:  0.6028934121131897
train gradient:  0.23109688632614356
iteration : 9603
train acc:  0.75
train loss:  0.456133633852005
train gradient:  0.13351582434716658
iteration : 9604
train acc:  0.7421875
train loss:  0.485803484916687
train gradient:  0.11753042396365983
iteration : 9605
train acc:  0.71875
train loss:  0.5632858276367188
train gradient:  0.14874162801544039
iteration : 9606
train acc:  0.71875
train loss:  0.5427316427230835
train gradient:  0.16695877936642867
iteration : 9607
train acc:  0.796875
train loss:  0.4932290017604828
train gradient:  0.1571179125868722
iteration : 9608
train acc:  0.765625
train loss:  0.5056798458099365
train gradient:  0.1266715541243219
iteration : 9609
train acc:  0.703125
train loss:  0.5332220792770386
train gradient:  0.16839783497599342
iteration : 9610
train acc:  0.7734375
train loss:  0.4587894678115845
train gradient:  0.11302514481478906
iteration : 9611
train acc:  0.78125
train loss:  0.5183246731758118
train gradient:  0.19958279312070798
iteration : 9612
train acc:  0.7265625
train loss:  0.5471576452255249
train gradient:  0.16225736175795866
iteration : 9613
train acc:  0.7578125
train loss:  0.4541630446910858
train gradient:  0.09152466151807936
iteration : 9614
train acc:  0.7265625
train loss:  0.4952091574668884
train gradient:  0.10607697012182295
iteration : 9615
train acc:  0.7109375
train loss:  0.4795098304748535
train gradient:  0.11661836135570121
iteration : 9616
train acc:  0.75
train loss:  0.4753767251968384
train gradient:  0.1338512041788919
iteration : 9617
train acc:  0.703125
train loss:  0.5466567277908325
train gradient:  0.11653644260726205
iteration : 9618
train acc:  0.78125
train loss:  0.46084684133529663
train gradient:  0.1052328514377778
iteration : 9619
train acc:  0.6953125
train loss:  0.5127127170562744
train gradient:  0.11701319806292279
iteration : 9620
train acc:  0.78125
train loss:  0.4944586753845215
train gradient:  0.14288406718024865
iteration : 9621
train acc:  0.78125
train loss:  0.44759419560432434
train gradient:  0.09802269931496295
iteration : 9622
train acc:  0.7734375
train loss:  0.43766385316848755
train gradient:  0.10056733425298313
iteration : 9623
train acc:  0.65625
train loss:  0.5460898876190186
train gradient:  0.167896725659066
iteration : 9624
train acc:  0.734375
train loss:  0.49060511589050293
train gradient:  0.10494158713609182
iteration : 9625
train acc:  0.75
train loss:  0.5024406313896179
train gradient:  0.11974838696173189
iteration : 9626
train acc:  0.78125
train loss:  0.4659935534000397
train gradient:  0.11067380277410875
iteration : 9627
train acc:  0.7109375
train loss:  0.5402281284332275
train gradient:  0.12686675623823077
iteration : 9628
train acc:  0.7265625
train loss:  0.49954989552497864
train gradient:  0.10804678200565504
iteration : 9629
train acc:  0.7734375
train loss:  0.42770010232925415
train gradient:  0.09461599616676108
iteration : 9630
train acc:  0.8125
train loss:  0.4631271958351135
train gradient:  0.12684688511716022
iteration : 9631
train acc:  0.7734375
train loss:  0.505974292755127
train gradient:  0.14096526718808738
iteration : 9632
train acc:  0.75
train loss:  0.46068334579467773
train gradient:  0.11747687303149978
iteration : 9633
train acc:  0.7109375
train loss:  0.4993942975997925
train gradient:  0.11425943933771152
iteration : 9634
train acc:  0.7734375
train loss:  0.49255049228668213
train gradient:  0.11347629563327664
iteration : 9635
train acc:  0.7578125
train loss:  0.475555956363678
train gradient:  0.13353126158890793
iteration : 9636
train acc:  0.7265625
train loss:  0.5016008615493774
train gradient:  0.12903078606375257
iteration : 9637
train acc:  0.7265625
train loss:  0.4808579683303833
train gradient:  0.1154349153601518
iteration : 9638
train acc:  0.7578125
train loss:  0.5659147500991821
train gradient:  0.2002195598045259
iteration : 9639
train acc:  0.6640625
train loss:  0.5402536392211914
train gradient:  0.13169825763485005
iteration : 9640
train acc:  0.7890625
train loss:  0.46597906947135925
train gradient:  0.12478241702023071
iteration : 9641
train acc:  0.734375
train loss:  0.5306705236434937
train gradient:  0.13554484089887114
iteration : 9642
train acc:  0.703125
train loss:  0.49940377473831177
train gradient:  0.12147161694807364
iteration : 9643
train acc:  0.78125
train loss:  0.44189682602882385
train gradient:  0.09369453559177494
iteration : 9644
train acc:  0.7109375
train loss:  0.5498709678649902
train gradient:  0.14026164849620898
iteration : 9645
train acc:  0.6796875
train loss:  0.5237997770309448
train gradient:  0.1737068371238364
iteration : 9646
train acc:  0.7734375
train loss:  0.4301230013370514
train gradient:  0.12613016918645
iteration : 9647
train acc:  0.78125
train loss:  0.4359409213066101
train gradient:  0.09936258531076622
iteration : 9648
train acc:  0.703125
train loss:  0.5139443278312683
train gradient:  0.13435642128672237
iteration : 9649
train acc:  0.78125
train loss:  0.445036917924881
train gradient:  0.11591425406710681
iteration : 9650
train acc:  0.7734375
train loss:  0.46257510781288147
train gradient:  0.10287695959624116
iteration : 9651
train acc:  0.765625
train loss:  0.465590238571167
train gradient:  0.12438274485375854
iteration : 9652
train acc:  0.7578125
train loss:  0.46629491448402405
train gradient:  0.13811656292744043
iteration : 9653
train acc:  0.7109375
train loss:  0.5390788912773132
train gradient:  0.1608595110696034
iteration : 9654
train acc:  0.7578125
train loss:  0.49886685609817505
train gradient:  0.12356923754716677
iteration : 9655
train acc:  0.796875
train loss:  0.40842205286026
train gradient:  0.1057299741492059
iteration : 9656
train acc:  0.7421875
train loss:  0.4989939332008362
train gradient:  0.11631986120794853
iteration : 9657
train acc:  0.796875
train loss:  0.4820077419281006
train gradient:  0.14185028432575808
iteration : 9658
train acc:  0.7734375
train loss:  0.5042558312416077
train gradient:  0.14273010322948654
iteration : 9659
train acc:  0.75
train loss:  0.516774594783783
train gradient:  0.1308734137851127
iteration : 9660
train acc:  0.765625
train loss:  0.480324387550354
train gradient:  0.1122816191489619
iteration : 9661
train acc:  0.75
train loss:  0.4666253626346588
train gradient:  0.10662994885575747
iteration : 9662
train acc:  0.6953125
train loss:  0.5281018018722534
train gradient:  0.11682177853862928
iteration : 9663
train acc:  0.7265625
train loss:  0.557788610458374
train gradient:  0.18625741828257364
iteration : 9664
train acc:  0.734375
train loss:  0.4717828333377838
train gradient:  0.12075397408210241
iteration : 9665
train acc:  0.7109375
train loss:  0.501954197883606
train gradient:  0.14253108522500924
iteration : 9666
train acc:  0.703125
train loss:  0.5470216274261475
train gradient:  0.12976409778096673
iteration : 9667
train acc:  0.78125
train loss:  0.4579097330570221
train gradient:  0.12487101386824996
iteration : 9668
train acc:  0.796875
train loss:  0.45786231756210327
train gradient:  0.10672037313646368
iteration : 9669
train acc:  0.734375
train loss:  0.4927881360054016
train gradient:  0.1433865912626912
iteration : 9670
train acc:  0.7578125
train loss:  0.4984738230705261
train gradient:  0.11816191888561459
iteration : 9671
train acc:  0.6640625
train loss:  0.5824642777442932
train gradient:  0.18633323031596918
iteration : 9672
train acc:  0.7421875
train loss:  0.5190685987472534
train gradient:  0.13799749282371726
iteration : 9673
train acc:  0.8203125
train loss:  0.3839826285839081
train gradient:  0.0936466263010081
iteration : 9674
train acc:  0.7109375
train loss:  0.5603393912315369
train gradient:  0.15595654619971044
iteration : 9675
train acc:  0.7109375
train loss:  0.5382050275802612
train gradient:  0.16126675290401332
iteration : 9676
train acc:  0.7265625
train loss:  0.48637068271636963
train gradient:  0.12346404722332709
iteration : 9677
train acc:  0.8125
train loss:  0.48155272006988525
train gradient:  0.12838202055632703
iteration : 9678
train acc:  0.7265625
train loss:  0.531928539276123
train gradient:  0.13136991832752198
iteration : 9679
train acc:  0.7109375
train loss:  0.5462188720703125
train gradient:  0.16014359901518443
iteration : 9680
train acc:  0.7109375
train loss:  0.4820529818534851
train gradient:  0.0999845659889194
iteration : 9681
train acc:  0.6171875
train loss:  0.6244705319404602
train gradient:  0.20545367173057713
iteration : 9682
train acc:  0.734375
train loss:  0.5146206021308899
train gradient:  0.11884447732737509
iteration : 9683
train acc:  0.734375
train loss:  0.48663705587387085
train gradient:  0.15524483788822457
iteration : 9684
train acc:  0.734375
train loss:  0.5174282789230347
train gradient:  0.1280414134462997
iteration : 9685
train acc:  0.7578125
train loss:  0.48743635416030884
train gradient:  0.14064921351818066
iteration : 9686
train acc:  0.6875
train loss:  0.5210868716239929
train gradient:  0.1354341255943659
iteration : 9687
train acc:  0.6953125
train loss:  0.5496166944503784
train gradient:  0.1440588711396988
iteration : 9688
train acc:  0.734375
train loss:  0.5019845962524414
train gradient:  0.13054223518424016
iteration : 9689
train acc:  0.7109375
train loss:  0.49751153588294983
train gradient:  0.1550635335182775
iteration : 9690
train acc:  0.734375
train loss:  0.48743492364883423
train gradient:  0.1186454346455392
iteration : 9691
train acc:  0.6875
train loss:  0.558496356010437
train gradient:  0.17583137867989168
iteration : 9692
train acc:  0.75
train loss:  0.4894513785839081
train gradient:  0.1381033876293128
iteration : 9693
train acc:  0.7265625
train loss:  0.48997679352760315
train gradient:  0.15063562471984085
iteration : 9694
train acc:  0.78125
train loss:  0.47614461183547974
train gradient:  0.1221256925364
iteration : 9695
train acc:  0.71875
train loss:  0.5003576874732971
train gradient:  0.11462674671632121
iteration : 9696
train acc:  0.71875
train loss:  0.5010261535644531
train gradient:  0.14233173985975375
iteration : 9697
train acc:  0.734375
train loss:  0.5193065404891968
train gradient:  0.1484778820300413
iteration : 9698
train acc:  0.6484375
train loss:  0.5723941922187805
train gradient:  0.17187634037307897
iteration : 9699
train acc:  0.703125
train loss:  0.5003520250320435
train gradient:  0.1341817038529991
iteration : 9700
train acc:  0.75
train loss:  0.49653467535972595
train gradient:  0.1391310746214377
iteration : 9701
train acc:  0.703125
train loss:  0.5424467921257019
train gradient:  0.13942296964227602
iteration : 9702
train acc:  0.78125
train loss:  0.4219234585762024
train gradient:  0.10615248180854249
iteration : 9703
train acc:  0.703125
train loss:  0.48058244585990906
train gradient:  0.11853405572904188
iteration : 9704
train acc:  0.8046875
train loss:  0.42350396513938904
train gradient:  0.10469362662128846
iteration : 9705
train acc:  0.796875
train loss:  0.4426206946372986
train gradient:  0.09481536665180262
iteration : 9706
train acc:  0.765625
train loss:  0.4562610387802124
train gradient:  0.10382956261891317
iteration : 9707
train acc:  0.8125
train loss:  0.4447364807128906
train gradient:  0.09860816582514416
iteration : 9708
train acc:  0.7421875
train loss:  0.5086747407913208
train gradient:  0.16514244592696964
iteration : 9709
train acc:  0.6640625
train loss:  0.5377175807952881
train gradient:  0.22921057348593415
iteration : 9710
train acc:  0.7265625
train loss:  0.5113058686256409
train gradient:  0.10901409641035072
iteration : 9711
train acc:  0.7734375
train loss:  0.49270230531692505
train gradient:  0.12788212317010805
iteration : 9712
train acc:  0.7578125
train loss:  0.4967270493507385
train gradient:  0.12101877411890305
iteration : 9713
train acc:  0.7421875
train loss:  0.49030351638793945
train gradient:  0.11881869332942693
iteration : 9714
train acc:  0.6796875
train loss:  0.5694320201873779
train gradient:  0.1594762204555944
iteration : 9715
train acc:  0.78125
train loss:  0.4348905384540558
train gradient:  0.09301018209860243
iteration : 9716
train acc:  0.7578125
train loss:  0.4880579710006714
train gradient:  0.12408648123963913
iteration : 9717
train acc:  0.7734375
train loss:  0.5055732727050781
train gradient:  0.1464317253914425
iteration : 9718
train acc:  0.71875
train loss:  0.5408766865730286
train gradient:  0.21480645763891798
iteration : 9719
train acc:  0.7578125
train loss:  0.5543549656867981
train gradient:  0.15515743492070427
iteration : 9720
train acc:  0.6953125
train loss:  0.5547318458557129
train gradient:  0.14115816155472888
iteration : 9721
train acc:  0.734375
train loss:  0.47170060873031616
train gradient:  0.10967488336968793
iteration : 9722
train acc:  0.7578125
train loss:  0.5202351808547974
train gradient:  0.1562368757419691
iteration : 9723
train acc:  0.7578125
train loss:  0.4468321204185486
train gradient:  0.10625424931711563
iteration : 9724
train acc:  0.7265625
train loss:  0.5192140936851501
train gradient:  0.1446503298465932
iteration : 9725
train acc:  0.7578125
train loss:  0.4662749171257019
train gradient:  0.1171760344384316
iteration : 9726
train acc:  0.7734375
train loss:  0.4716906249523163
train gradient:  0.08771176922833629
iteration : 9727
train acc:  0.78125
train loss:  0.4810604453086853
train gradient:  0.14340318643957745
iteration : 9728
train acc:  0.703125
train loss:  0.5394202470779419
train gradient:  0.12400882355283983
iteration : 9729
train acc:  0.734375
train loss:  0.5102599859237671
train gradient:  0.14757410751464978
iteration : 9730
train acc:  0.71875
train loss:  0.5119824409484863
train gradient:  0.12766443478891693
iteration : 9731
train acc:  0.75
train loss:  0.5084255337715149
train gradient:  0.1314751877183232
iteration : 9732
train acc:  0.625
train loss:  0.5614421367645264
train gradient:  0.1415025026976814
iteration : 9733
train acc:  0.7734375
train loss:  0.490256130695343
train gradient:  0.13636091337138395
iteration : 9734
train acc:  0.8125
train loss:  0.4425497353076935
train gradient:  0.08849664121457004
iteration : 9735
train acc:  0.71875
train loss:  0.49491631984710693
train gradient:  0.14713236152525114
iteration : 9736
train acc:  0.6484375
train loss:  0.5666241645812988
train gradient:  0.16751971191710635
iteration : 9737
train acc:  0.7265625
train loss:  0.5439847707748413
train gradient:  0.15226819741721664
iteration : 9738
train acc:  0.6875
train loss:  0.5480766892433167
train gradient:  0.17776549297325017
iteration : 9739
train acc:  0.78125
train loss:  0.4697590470314026
train gradient:  0.11643361857866792
iteration : 9740
train acc:  0.734375
train loss:  0.47967469692230225
train gradient:  0.11920026594009708
iteration : 9741
train acc:  0.7109375
train loss:  0.5511687994003296
train gradient:  0.15157346982220243
iteration : 9742
train acc:  0.734375
train loss:  0.48225754499435425
train gradient:  0.17029423032342483
iteration : 9743
train acc:  0.7734375
train loss:  0.4771426022052765
train gradient:  0.13178634379177515
iteration : 9744
train acc:  0.7578125
train loss:  0.4136512279510498
train gradient:  0.09403730501763014
iteration : 9745
train acc:  0.75
train loss:  0.5064486265182495
train gradient:  0.12988023012567568
iteration : 9746
train acc:  0.78125
train loss:  0.4889737665653229
train gradient:  0.10319213411423198
iteration : 9747
train acc:  0.7265625
train loss:  0.4774326980113983
train gradient:  0.11921910678542945
iteration : 9748
train acc:  0.8125
train loss:  0.41414451599121094
train gradient:  0.1032022371415747
iteration : 9749
train acc:  0.7421875
train loss:  0.5091737508773804
train gradient:  0.13878681590067987
iteration : 9750
train acc:  0.734375
train loss:  0.521479606628418
train gradient:  0.1289490406714786
iteration : 9751
train acc:  0.7578125
train loss:  0.5097872018814087
train gradient:  0.136569347079861
iteration : 9752
train acc:  0.734375
train loss:  0.5355626940727234
train gradient:  0.1608543455408148
iteration : 9753
train acc:  0.78125
train loss:  0.45014938712120056
train gradient:  0.12803634563180902
iteration : 9754
train acc:  0.71875
train loss:  0.48690035939216614
train gradient:  0.10164958081338853
iteration : 9755
train acc:  0.6953125
train loss:  0.5592374801635742
train gradient:  0.12215885802785574
iteration : 9756
train acc:  0.6875
train loss:  0.5223364233970642
train gradient:  0.12204320131535452
iteration : 9757
train acc:  0.7109375
train loss:  0.556011974811554
train gradient:  0.13207458440516742
iteration : 9758
train acc:  0.734375
train loss:  0.5337580442428589
train gradient:  0.13049962645224025
iteration : 9759
train acc:  0.7578125
train loss:  0.4572799503803253
train gradient:  0.12999951930956077
iteration : 9760
train acc:  0.7578125
train loss:  0.45320361852645874
train gradient:  0.1065692526037524
iteration : 9761
train acc:  0.7734375
train loss:  0.46666935086250305
train gradient:  0.09908984225177792
iteration : 9762
train acc:  0.734375
train loss:  0.5552294850349426
train gradient:  0.16122810773339466
iteration : 9763
train acc:  0.796875
train loss:  0.413047194480896
train gradient:  0.10054415792575375
iteration : 9764
train acc:  0.7265625
train loss:  0.5133300423622131
train gradient:  0.12299963905975488
iteration : 9765
train acc:  0.765625
train loss:  0.4841654896736145
train gradient:  0.14500813142271057
iteration : 9766
train acc:  0.7109375
train loss:  0.5150041580200195
train gradient:  0.13667329473569761
iteration : 9767
train acc:  0.71875
train loss:  0.5080953240394592
train gradient:  0.13054888448237084
iteration : 9768
train acc:  0.7265625
train loss:  0.5381551384925842
train gradient:  0.13490584504054737
iteration : 9769
train acc:  0.7265625
train loss:  0.5143212080001831
train gradient:  0.1467950507836523
iteration : 9770
train acc:  0.75
train loss:  0.4558837413787842
train gradient:  0.10003571922635408
iteration : 9771
train acc:  0.7421875
train loss:  0.5367449522018433
train gradient:  0.13143033232647822
iteration : 9772
train acc:  0.7421875
train loss:  0.5743746161460876
train gradient:  0.20592788830403802
iteration : 9773
train acc:  0.71875
train loss:  0.549297571182251
train gradient:  0.17112679375991108
iteration : 9774
train acc:  0.71875
train loss:  0.5656882524490356
train gradient:  0.16947049086313615
iteration : 9775
train acc:  0.703125
train loss:  0.5690588355064392
train gradient:  0.15353895390989647
iteration : 9776
train acc:  0.6953125
train loss:  0.5197117328643799
train gradient:  0.11945796141285973
iteration : 9777
train acc:  0.734375
train loss:  0.5143181085586548
train gradient:  0.13057844585686684
iteration : 9778
train acc:  0.7890625
train loss:  0.5050572156906128
train gradient:  0.1178060482767387
iteration : 9779
train acc:  0.7265625
train loss:  0.5507251024246216
train gradient:  0.15169362591160557
iteration : 9780
train acc:  0.7890625
train loss:  0.4655744433403015
train gradient:  0.09106650174916793
iteration : 9781
train acc:  0.78125
train loss:  0.46480947732925415
train gradient:  0.09605955890253298
iteration : 9782
train acc:  0.7890625
train loss:  0.4068129360675812
train gradient:  0.08612466109225413
iteration : 9783
train acc:  0.796875
train loss:  0.43849027156829834
train gradient:  0.10338419726793056
iteration : 9784
train acc:  0.703125
train loss:  0.5077561736106873
train gradient:  0.1502325857691431
iteration : 9785
train acc:  0.7265625
train loss:  0.5019080638885498
train gradient:  0.1485467189051014
iteration : 9786
train acc:  0.7265625
train loss:  0.5465309619903564
train gradient:  0.13771794071244273
iteration : 9787
train acc:  0.75
train loss:  0.48507100343704224
train gradient:  0.1238285855089555
iteration : 9788
train acc:  0.765625
train loss:  0.5004964470863342
train gradient:  0.1210759699375105
iteration : 9789
train acc:  0.734375
train loss:  0.48519381880760193
train gradient:  0.1304595892125982
iteration : 9790
train acc:  0.7265625
train loss:  0.49245303869247437
train gradient:  0.10102957786124614
iteration : 9791
train acc:  0.6953125
train loss:  0.5377528071403503
train gradient:  0.14569758347915512
iteration : 9792
train acc:  0.6953125
train loss:  0.4894106984138489
train gradient:  0.11903135552924653
iteration : 9793
train acc:  0.7890625
train loss:  0.46765145659446716
train gradient:  0.11297909000723662
iteration : 9794
train acc:  0.765625
train loss:  0.4602862000465393
train gradient:  0.12342868222531225
iteration : 9795
train acc:  0.75
train loss:  0.48705345392227173
train gradient:  0.13429841133700282
iteration : 9796
train acc:  0.7109375
train loss:  0.5441689491271973
train gradient:  0.12949681332778817
iteration : 9797
train acc:  0.71875
train loss:  0.5573404431343079
train gradient:  0.1776257850695177
iteration : 9798
train acc:  0.7578125
train loss:  0.46903079748153687
train gradient:  0.10745488358199239
iteration : 9799
train acc:  0.7734375
train loss:  0.4772941470146179
train gradient:  0.11031146044641894
iteration : 9800
train acc:  0.7265625
train loss:  0.5312596559524536
train gradient:  0.21735256490496419
iteration : 9801
train acc:  0.75
train loss:  0.4892771244049072
train gradient:  0.10382909054648895
iteration : 9802
train acc:  0.7578125
train loss:  0.49855026602745056
train gradient:  0.14382588288480136
iteration : 9803
train acc:  0.7421875
train loss:  0.4670642614364624
train gradient:  0.09803317369214505
iteration : 9804
train acc:  0.765625
train loss:  0.4901502728462219
train gradient:  0.11720267221004653
iteration : 9805
train acc:  0.7109375
train loss:  0.4920513331890106
train gradient:  0.15061275947737712
iteration : 9806
train acc:  0.7109375
train loss:  0.4918883740901947
train gradient:  0.13516573790275038
iteration : 9807
train acc:  0.671875
train loss:  0.519703209400177
train gradient:  0.1940559198823229
iteration : 9808
train acc:  0.765625
train loss:  0.46504688262939453
train gradient:  0.11650945453339495
iteration : 9809
train acc:  0.78125
train loss:  0.4497358798980713
train gradient:  0.0962159434305529
iteration : 9810
train acc:  0.71875
train loss:  0.518276035785675
train gradient:  0.12185336514782212
iteration : 9811
train acc:  0.7109375
train loss:  0.5228216648101807
train gradient:  0.1841323716017046
iteration : 9812
train acc:  0.6953125
train loss:  0.5751466751098633
train gradient:  0.17168643596717892
iteration : 9813
train acc:  0.78125
train loss:  0.46407976746559143
train gradient:  0.1196557425319869
iteration : 9814
train acc:  0.6953125
train loss:  0.5145815014839172
train gradient:  0.12768228925948916
iteration : 9815
train acc:  0.6875
train loss:  0.5132769346237183
train gradient:  0.14354182357486595
iteration : 9816
train acc:  0.7265625
train loss:  0.5456404685974121
train gradient:  0.17925035737011358
iteration : 9817
train acc:  0.7265625
train loss:  0.5169426202774048
train gradient:  0.1677785802668968
iteration : 9818
train acc:  0.7421875
train loss:  0.4847744107246399
train gradient:  0.09917145221445482
iteration : 9819
train acc:  0.7421875
train loss:  0.44329893589019775
train gradient:  0.10290214879520143
iteration : 9820
train acc:  0.796875
train loss:  0.4392867386341095
train gradient:  0.09884150131458881
iteration : 9821
train acc:  0.828125
train loss:  0.4295109510421753
train gradient:  0.09622934804440807
iteration : 9822
train acc:  0.71875
train loss:  0.5159640312194824
train gradient:  0.12256440531444399
iteration : 9823
train acc:  0.75
train loss:  0.4838081896305084
train gradient:  0.16417133045954774
iteration : 9824
train acc:  0.7734375
train loss:  0.48725610971450806
train gradient:  0.166503569105974
iteration : 9825
train acc:  0.75
train loss:  0.47716522216796875
train gradient:  0.12490679771881623
iteration : 9826
train acc:  0.703125
train loss:  0.49586427211761475
train gradient:  0.10974301522852267
iteration : 9827
train acc:  0.765625
train loss:  0.4607970416545868
train gradient:  0.10283296349969458
iteration : 9828
train acc:  0.6796875
train loss:  0.638472318649292
train gradient:  0.19773096507056542
iteration : 9829
train acc:  0.703125
train loss:  0.5525047779083252
train gradient:  0.16425644381273374
iteration : 9830
train acc:  0.765625
train loss:  0.47651082277297974
train gradient:  0.09209830906203739
iteration : 9831
train acc:  0.7265625
train loss:  0.49410176277160645
train gradient:  0.1147649019702968
iteration : 9832
train acc:  0.7578125
train loss:  0.4612167477607727
train gradient:  0.1200239024077791
iteration : 9833
train acc:  0.734375
train loss:  0.49116700887680054
train gradient:  0.13051856499597664
iteration : 9834
train acc:  0.7578125
train loss:  0.5137141942977905
train gradient:  0.12454811629706856
iteration : 9835
train acc:  0.765625
train loss:  0.4553493559360504
train gradient:  0.13622014526741166
iteration : 9836
train acc:  0.75
train loss:  0.5149513483047485
train gradient:  0.14045361214080826
iteration : 9837
train acc:  0.703125
train loss:  0.5624613165855408
train gradient:  0.1477140667450907
iteration : 9838
train acc:  0.734375
train loss:  0.5065059065818787
train gradient:  0.12801576454091618
iteration : 9839
train acc:  0.7265625
train loss:  0.5584441423416138
train gradient:  0.14040476182427375
iteration : 9840
train acc:  0.765625
train loss:  0.4969351291656494
train gradient:  0.11472701739593309
iteration : 9841
train acc:  0.71875
train loss:  0.5117056369781494
train gradient:  0.12185472594257454
iteration : 9842
train acc:  0.7578125
train loss:  0.48338907957077026
train gradient:  0.1096301269242905
iteration : 9843
train acc:  0.7265625
train loss:  0.48530012369155884
train gradient:  0.14199404226957452
iteration : 9844
train acc:  0.7578125
train loss:  0.4445797801017761
train gradient:  0.1047061418228871
iteration : 9845
train acc:  0.75
train loss:  0.5267652273178101
train gradient:  0.12512812096553236
iteration : 9846
train acc:  0.71875
train loss:  0.4618634283542633
train gradient:  0.10876046374250507
iteration : 9847
train acc:  0.7109375
train loss:  0.5523185729980469
train gradient:  0.1652745819051627
iteration : 9848
train acc:  0.75
train loss:  0.45419567823410034
train gradient:  0.09193412409585561
iteration : 9849
train acc:  0.734375
train loss:  0.4586648643016815
train gradient:  0.09881614866195038
iteration : 9850
train acc:  0.7265625
train loss:  0.4543660283088684
train gradient:  0.12145264844682342
iteration : 9851
train acc:  0.6953125
train loss:  0.5360426902770996
train gradient:  0.12797903422753057
iteration : 9852
train acc:  0.71875
train loss:  0.5000883340835571
train gradient:  0.1077911098447579
iteration : 9853
train acc:  0.6953125
train loss:  0.5569125413894653
train gradient:  0.15374216139640615
iteration : 9854
train acc:  0.6796875
train loss:  0.5462849140167236
train gradient:  0.16230960390711163
iteration : 9855
train acc:  0.7578125
train loss:  0.4843839108943939
train gradient:  0.12998978247517678
iteration : 9856
train acc:  0.7265625
train loss:  0.4871906042098999
train gradient:  0.14217777227661677
iteration : 9857
train acc:  0.7578125
train loss:  0.5401268601417542
train gradient:  0.16159660988381452
iteration : 9858
train acc:  0.703125
train loss:  0.5292264223098755
train gradient:  0.13858510175608735
iteration : 9859
train acc:  0.78125
train loss:  0.4406758248806
train gradient:  0.09677503633599205
iteration : 9860
train acc:  0.7890625
train loss:  0.4420575499534607
train gradient:  0.11329075187346362
iteration : 9861
train acc:  0.7421875
train loss:  0.5219396352767944
train gradient:  0.14466629913627238
iteration : 9862
train acc:  0.7109375
train loss:  0.5245396494865417
train gradient:  0.15002890606666147
iteration : 9863
train acc:  0.7109375
train loss:  0.5279518961906433
train gradient:  0.11928634809039165
iteration : 9864
train acc:  0.7734375
train loss:  0.5134134292602539
train gradient:  0.13686681575531012
iteration : 9865
train acc:  0.7578125
train loss:  0.5143876075744629
train gradient:  0.1455690061097342
iteration : 9866
train acc:  0.75
train loss:  0.4903518557548523
train gradient:  0.14874495429620466
iteration : 9867
train acc:  0.7890625
train loss:  0.46903932094573975
train gradient:  0.12951456905601122
iteration : 9868
train acc:  0.71875
train loss:  0.5389082431793213
train gradient:  0.1317538529361331
iteration : 9869
train acc:  0.734375
train loss:  0.5324946641921997
train gradient:  0.13165449885226282
iteration : 9870
train acc:  0.6953125
train loss:  0.5812340378761292
train gradient:  0.19527088696848605
iteration : 9871
train acc:  0.8046875
train loss:  0.4430239200592041
train gradient:  0.10730719366106883
iteration : 9872
train acc:  0.7421875
train loss:  0.48911768198013306
train gradient:  0.10059974245317264
iteration : 9873
train acc:  0.703125
train loss:  0.5390377044677734
train gradient:  0.16744224823196305
iteration : 9874
train acc:  0.7109375
train loss:  0.5095475912094116
train gradient:  0.14565398567604892
iteration : 9875
train acc:  0.7734375
train loss:  0.4383878707885742
train gradient:  0.12253014116888125
iteration : 9876
train acc:  0.75
train loss:  0.4511362314224243
train gradient:  0.1609403140646009
iteration : 9877
train acc:  0.8046875
train loss:  0.45831695199012756
train gradient:  0.1110489011686776
iteration : 9878
train acc:  0.7421875
train loss:  0.5077675580978394
train gradient:  0.12833129966713555
iteration : 9879
train acc:  0.75
train loss:  0.5242024660110474
train gradient:  0.12973537995574727
iteration : 9880
train acc:  0.765625
train loss:  0.46848899126052856
train gradient:  0.09781569188041124
iteration : 9881
train acc:  0.7421875
train loss:  0.4884602427482605
train gradient:  0.11751028852969166
iteration : 9882
train acc:  0.75
train loss:  0.5294814705848694
train gradient:  0.1433580813229553
iteration : 9883
train acc:  0.7578125
train loss:  0.5002277493476868
train gradient:  0.11025020764729568
iteration : 9884
train acc:  0.7421875
train loss:  0.48617738485336304
train gradient:  0.14524755742855072
iteration : 9885
train acc:  0.796875
train loss:  0.5202363729476929
train gradient:  0.11621848207424768
iteration : 9886
train acc:  0.703125
train loss:  0.5479190945625305
train gradient:  0.15120554784910112
iteration : 9887
train acc:  0.6953125
train loss:  0.5276581645011902
train gradient:  0.12951389874310937
iteration : 9888
train acc:  0.7890625
train loss:  0.45105570554733276
train gradient:  0.11790225766496189
iteration : 9889
train acc:  0.71875
train loss:  0.5437672734260559
train gradient:  0.13070474001958293
iteration : 9890
train acc:  0.7265625
train loss:  0.506755530834198
train gradient:  0.12725127350097779
iteration : 9891
train acc:  0.71875
train loss:  0.5345516800880432
train gradient:  0.13724865430658534
iteration : 9892
train acc:  0.7578125
train loss:  0.44138142466545105
train gradient:  0.11666398275646478
iteration : 9893
train acc:  0.78125
train loss:  0.49208492040634155
train gradient:  0.12827736739901358
iteration : 9894
train acc:  0.6796875
train loss:  0.5087783336639404
train gradient:  0.12056285163644025
iteration : 9895
train acc:  0.765625
train loss:  0.4685014486312866
train gradient:  0.10466969014276074
iteration : 9896
train acc:  0.75
train loss:  0.47353774309158325
train gradient:  0.12707926912102935
iteration : 9897
train acc:  0.765625
train loss:  0.45465022325515747
train gradient:  0.10492224757123347
iteration : 9898
train acc:  0.625
train loss:  0.5768746137619019
train gradient:  0.1529618392696099
iteration : 9899
train acc:  0.7109375
train loss:  0.5433906316757202
train gradient:  0.1748233709037817
iteration : 9900
train acc:  0.7578125
train loss:  0.47319915890693665
train gradient:  0.12119815136610201
iteration : 9901
train acc:  0.84375
train loss:  0.41533362865448
train gradient:  0.11806647999108165
iteration : 9902
train acc:  0.7578125
train loss:  0.4470648467540741
train gradient:  0.09921870694204551
iteration : 9903
train acc:  0.7265625
train loss:  0.5958391427993774
train gradient:  0.14746948387651274
iteration : 9904
train acc:  0.765625
train loss:  0.4992637634277344
train gradient:  0.1325469994328638
iteration : 9905
train acc:  0.734375
train loss:  0.5267413854598999
train gradient:  0.20040153620736262
iteration : 9906
train acc:  0.71875
train loss:  0.5007469654083252
train gradient:  0.1402356114314161
iteration : 9907
train acc:  0.734375
train loss:  0.4802536070346832
train gradient:  0.14793895859271705
iteration : 9908
train acc:  0.796875
train loss:  0.45511898398399353
train gradient:  0.1234051580339305
iteration : 9909
train acc:  0.8125
train loss:  0.4398697316646576
train gradient:  0.10006952230720194
iteration : 9910
train acc:  0.734375
train loss:  0.5072416067123413
train gradient:  0.138021810612734
iteration : 9911
train acc:  0.6953125
train loss:  0.5228874683380127
train gradient:  0.14847855156579987
iteration : 9912
train acc:  0.7265625
train loss:  0.5566266775131226
train gradient:  0.16054982219439057
iteration : 9913
train acc:  0.7734375
train loss:  0.49302929639816284
train gradient:  0.1759862568422701
iteration : 9914
train acc:  0.734375
train loss:  0.5316252708435059
train gradient:  0.1556341243519241
iteration : 9915
train acc:  0.7578125
train loss:  0.5040202140808105
train gradient:  0.1086419486257632
iteration : 9916
train acc:  0.703125
train loss:  0.5592712163925171
train gradient:  0.1863301463312312
iteration : 9917
train acc:  0.7578125
train loss:  0.5132622718811035
train gradient:  0.13466186426443133
iteration : 9918
train acc:  0.75
train loss:  0.4902716279029846
train gradient:  0.13688610301605225
iteration : 9919
train acc:  0.7265625
train loss:  0.4770287871360779
train gradient:  0.14444683615355353
iteration : 9920
train acc:  0.734375
train loss:  0.5006125569343567
train gradient:  0.1412936655511523
iteration : 9921
train acc:  0.7109375
train loss:  0.5114193558692932
train gradient:  0.13058551953824143
iteration : 9922
train acc:  0.71875
train loss:  0.5311449766159058
train gradient:  0.12957512018569856
iteration : 9923
train acc:  0.7265625
train loss:  0.5238705277442932
train gradient:  0.1343041927415506
iteration : 9924
train acc:  0.703125
train loss:  0.45771145820617676
train gradient:  0.10768311480398601
iteration : 9925
train acc:  0.71875
train loss:  0.5197919607162476
train gradient:  0.14172565587711083
iteration : 9926
train acc:  0.75
train loss:  0.4611772298812866
train gradient:  0.10138843762452245
iteration : 9927
train acc:  0.6484375
train loss:  0.5726518630981445
train gradient:  0.1684142248898466
iteration : 9928
train acc:  0.71875
train loss:  0.5890750288963318
train gradient:  0.1950842138345364
iteration : 9929
train acc:  0.6875
train loss:  0.5181567668914795
train gradient:  0.1610663433856092
iteration : 9930
train acc:  0.7265625
train loss:  0.5209356546401978
train gradient:  0.13539016733026912
iteration : 9931
train acc:  0.7734375
train loss:  0.45296138525009155
train gradient:  0.08375777657512112
iteration : 9932
train acc:  0.7265625
train loss:  0.5225166082382202
train gradient:  0.1151432411634701
iteration : 9933
train acc:  0.7109375
train loss:  0.5195910930633545
train gradient:  0.1236398704296227
iteration : 9934
train acc:  0.8046875
train loss:  0.4385204613208771
train gradient:  0.09971536147118115
iteration : 9935
train acc:  0.765625
train loss:  0.48166051506996155
train gradient:  0.1009866079266223
iteration : 9936
train acc:  0.7265625
train loss:  0.4916045069694519
train gradient:  0.10663388571921174
iteration : 9937
train acc:  0.7890625
train loss:  0.4598330855369568
train gradient:  0.1343316541791501
iteration : 9938
train acc:  0.8046875
train loss:  0.4724389612674713
train gradient:  0.13097877652934029
iteration : 9939
train acc:  0.671875
train loss:  0.5701463222503662
train gradient:  0.15370381898284763
iteration : 9940
train acc:  0.7734375
train loss:  0.4453676640987396
train gradient:  0.09928754686451909
iteration : 9941
train acc:  0.71875
train loss:  0.4915502965450287
train gradient:  0.12780170399951146
iteration : 9942
train acc:  0.7421875
train loss:  0.5283405780792236
train gradient:  0.15072464051704543
iteration : 9943
train acc:  0.75
train loss:  0.48532965779304504
train gradient:  0.14320919987097283
iteration : 9944
train acc:  0.7265625
train loss:  0.498163640499115
train gradient:  0.13063346021867844
iteration : 9945
train acc:  0.6953125
train loss:  0.49756014347076416
train gradient:  0.10555140911710383
iteration : 9946
train acc:  0.7734375
train loss:  0.4539318084716797
train gradient:  0.12492513340707466
iteration : 9947
train acc:  0.7421875
train loss:  0.5272090435028076
train gradient:  0.15123627486249477
iteration : 9948
train acc:  0.703125
train loss:  0.5349788665771484
train gradient:  0.14606419826679656
iteration : 9949
train acc:  0.7578125
train loss:  0.47490859031677246
train gradient:  0.1330713189560167
iteration : 9950
train acc:  0.6796875
train loss:  0.5950104594230652
train gradient:  0.1781731943100523
iteration : 9951
train acc:  0.796875
train loss:  0.444428026676178
train gradient:  0.12547066993991599
iteration : 9952
train acc:  0.7421875
train loss:  0.5253872871398926
train gradient:  0.14363995492780252
iteration : 9953
train acc:  0.71875
train loss:  0.4905781149864197
train gradient:  0.11901417539879877
iteration : 9954
train acc:  0.671875
train loss:  0.5821603536605835
train gradient:  0.1797215824148111
iteration : 9955
train acc:  0.8359375
train loss:  0.42640411853790283
train gradient:  0.12062268557809461
iteration : 9956
train acc:  0.7109375
train loss:  0.5435094833374023
train gradient:  0.15685878317195068
iteration : 9957
train acc:  0.6953125
train loss:  0.5019206404685974
train gradient:  0.11382283344626903
iteration : 9958
train acc:  0.734375
train loss:  0.4991188049316406
train gradient:  0.12494481828544422
iteration : 9959
train acc:  0.734375
train loss:  0.4661204218864441
train gradient:  0.12154384404893981
iteration : 9960
train acc:  0.7421875
train loss:  0.4644699692726135
train gradient:  0.13057714032790732
iteration : 9961
train acc:  0.7578125
train loss:  0.46728095412254333
train gradient:  0.11326602694508771
iteration : 9962
train acc:  0.71875
train loss:  0.48393136262893677
train gradient:  0.10337844138631994
iteration : 9963
train acc:  0.71875
train loss:  0.49792760610580444
train gradient:  0.1168798055961764
iteration : 9964
train acc:  0.8203125
train loss:  0.4258834719657898
train gradient:  0.0828209119186561
iteration : 9965
train acc:  0.703125
train loss:  0.4755958914756775
train gradient:  0.13518679148189733
iteration : 9966
train acc:  0.7421875
train loss:  0.518896222114563
train gradient:  0.14986220318438048
iteration : 9967
train acc:  0.7890625
train loss:  0.43574658036231995
train gradient:  0.112980814176558
iteration : 9968
train acc:  0.734375
train loss:  0.5290026664733887
train gradient:  0.13374839195509436
iteration : 9969
train acc:  0.71875
train loss:  0.4932781755924225
train gradient:  0.12973280326776088
iteration : 9970
train acc:  0.78125
train loss:  0.4690713584423065
train gradient:  0.11307255911053223
iteration : 9971
train acc:  0.6875
train loss:  0.5452705025672913
train gradient:  0.17273468533567943
iteration : 9972
train acc:  0.7421875
train loss:  0.5760332345962524
train gradient:  0.14599375285471333
iteration : 9973
train acc:  0.7109375
train loss:  0.5278646945953369
train gradient:  0.14856354187868795
iteration : 9974
train acc:  0.7109375
train loss:  0.5237261652946472
train gradient:  0.1384287111124398
iteration : 9975
train acc:  0.671875
train loss:  0.5551353693008423
train gradient:  0.1361112667132981
iteration : 9976
train acc:  0.734375
train loss:  0.4499655067920685
train gradient:  0.10031403638773065
iteration : 9977
train acc:  0.7265625
train loss:  0.4654752016067505
train gradient:  0.10166969238663331
iteration : 9978
train acc:  0.765625
train loss:  0.4696809947490692
train gradient:  0.11136225029402731
iteration : 9979
train acc:  0.765625
train loss:  0.5053120851516724
train gradient:  0.15481270285590187
iteration : 9980
train acc:  0.6640625
train loss:  0.49315163493156433
train gradient:  0.13167189440247118
iteration : 9981
train acc:  0.7578125
train loss:  0.4653981328010559
train gradient:  0.1109380962950441
iteration : 9982
train acc:  0.765625
train loss:  0.47938215732574463
train gradient:  0.12094419726604638
iteration : 9983
train acc:  0.7734375
train loss:  0.4642626941204071
train gradient:  0.10949645167861777
iteration : 9984
train acc:  0.765625
train loss:  0.48195725679397583
train gradient:  0.14615503668810456
iteration : 9985
train acc:  0.7265625
train loss:  0.5115750432014465
train gradient:  0.14554691396988462
iteration : 9986
train acc:  0.7109375
train loss:  0.549705445766449
train gradient:  0.15469508634655046
iteration : 9987
train acc:  0.6796875
train loss:  0.5985963344573975
train gradient:  0.17611196080362834
iteration : 9988
train acc:  0.71875
train loss:  0.5469889044761658
train gradient:  0.1715674489401059
iteration : 9989
train acc:  0.734375
train loss:  0.45983120799064636
train gradient:  0.0975783082746904
iteration : 9990
train acc:  0.7265625
train loss:  0.5349630117416382
train gradient:  0.17604333817156548
iteration : 9991
train acc:  0.6953125
train loss:  0.5323938131332397
train gradient:  0.13023988898505895
iteration : 9992
train acc:  0.7265625
train loss:  0.5007712841033936
train gradient:  0.11984690873541738
iteration : 9993
train acc:  0.75
train loss:  0.4939200282096863
train gradient:  0.15258197673370955
iteration : 9994
train acc:  0.7578125
train loss:  0.501541018486023
train gradient:  0.11532161653313834
iteration : 9995
train acc:  0.734375
train loss:  0.4900934398174286
train gradient:  0.13494663107127292
iteration : 9996
train acc:  0.7578125
train loss:  0.47336217761039734
train gradient:  0.1747921852587545
iteration : 9997
train acc:  0.6796875
train loss:  0.5140712261199951
train gradient:  0.13144636437936336
iteration : 9998
train acc:  0.7265625
train loss:  0.492493599653244
train gradient:  0.15624937459945942
iteration : 9999
train acc:  0.765625
train loss:  0.5174967050552368
train gradient:  0.1371096542705989
iteration : 10000
train acc:  0.7109375
train loss:  0.507779061794281
train gradient:  0.1385001596891413
iteration : 10001
train acc:  0.6953125
train loss:  0.5249788761138916
train gradient:  0.13314489230033014
iteration : 10002
train acc:  0.734375
train loss:  0.5159398317337036
train gradient:  0.17726873320628966
iteration : 10003
train acc:  0.7421875
train loss:  0.5074183344841003
train gradient:  0.13371502480664488
iteration : 10004
train acc:  0.7265625
train loss:  0.5178493857383728
train gradient:  0.1320611019444926
iteration : 10005
train acc:  0.71875
train loss:  0.5603501796722412
train gradient:  0.14318317738127323
iteration : 10006
train acc:  0.796875
train loss:  0.5273715257644653
train gradient:  0.11664187401285238
iteration : 10007
train acc:  0.6875
train loss:  0.5595846176147461
train gradient:  0.15604402869439787
iteration : 10008
train acc:  0.7890625
train loss:  0.46975594758987427
train gradient:  0.13300201401995604
iteration : 10009
train acc:  0.7421875
train loss:  0.48895999789237976
train gradient:  0.13029392985044813
iteration : 10010
train acc:  0.7578125
train loss:  0.4428405463695526
train gradient:  0.09481846483463091
iteration : 10011
train acc:  0.7109375
train loss:  0.46782830357551575
train gradient:  0.11638297307075991
iteration : 10012
train acc:  0.6875
train loss:  0.4801409840583801
train gradient:  0.1163465172966638
iteration : 10013
train acc:  0.7734375
train loss:  0.5054306387901306
train gradient:  0.11714969063726166
iteration : 10014
train acc:  0.640625
train loss:  0.5913714170455933
train gradient:  0.1947124045351252
iteration : 10015
train acc:  0.6953125
train loss:  0.5599242448806763
train gradient:  0.1887299221874632
iteration : 10016
train acc:  0.7109375
train loss:  0.4939175248146057
train gradient:  0.13793411629867147
iteration : 10017
train acc:  0.78125
train loss:  0.4787176251411438
train gradient:  0.09636184083157852
iteration : 10018
train acc:  0.7265625
train loss:  0.4807196855545044
train gradient:  0.11443669748110091
iteration : 10019
train acc:  0.734375
train loss:  0.4985663890838623
train gradient:  0.15557098096424404
iteration : 10020
train acc:  0.6796875
train loss:  0.5747455358505249
train gradient:  0.1450164584439259
iteration : 10021
train acc:  0.7578125
train loss:  0.48523715138435364
train gradient:  0.1421246816591485
iteration : 10022
train acc:  0.75
train loss:  0.5211181044578552
train gradient:  0.1249639279201437
iteration : 10023
train acc:  0.703125
train loss:  0.5369223356246948
train gradient:  0.13071712161142984
iteration : 10024
train acc:  0.78125
train loss:  0.4901454448699951
train gradient:  0.10827207889585556
iteration : 10025
train acc:  0.7734375
train loss:  0.45742326974868774
train gradient:  0.1282090316471869
iteration : 10026
train acc:  0.7734375
train loss:  0.474872887134552
train gradient:  0.11876430955632913
iteration : 10027
train acc:  0.75
train loss:  0.4866151809692383
train gradient:  0.12715609674764877
iteration : 10028
train acc:  0.75
train loss:  0.46133673191070557
train gradient:  0.13481397062951267
iteration : 10029
train acc:  0.6875
train loss:  0.5138871669769287
train gradient:  0.1202776803861066
iteration : 10030
train acc:  0.75
train loss:  0.46924686431884766
train gradient:  0.12268665797594124
iteration : 10031
train acc:  0.7421875
train loss:  0.4734051823616028
train gradient:  0.14696722498078868
iteration : 10032
train acc:  0.6953125
train loss:  0.543601930141449
train gradient:  0.13782521078266877
iteration : 10033
train acc:  0.7421875
train loss:  0.4345380663871765
train gradient:  0.1475263256223161
iteration : 10034
train acc:  0.7421875
train loss:  0.46757036447525024
train gradient:  0.12801868606066202
iteration : 10035
train acc:  0.7734375
train loss:  0.5234048366546631
train gradient:  0.1439604066946047
iteration : 10036
train acc:  0.6875
train loss:  0.541103720664978
train gradient:  0.1360046792782959
iteration : 10037
train acc:  0.765625
train loss:  0.4791540205478668
train gradient:  0.14409691094159888
iteration : 10038
train acc:  0.6796875
train loss:  0.5706014633178711
train gradient:  0.16704510368162834
iteration : 10039
train acc:  0.78125
train loss:  0.45038914680480957
train gradient:  0.13862793205125257
iteration : 10040
train acc:  0.7890625
train loss:  0.41124197840690613
train gradient:  0.11284626453335495
iteration : 10041
train acc:  0.6953125
train loss:  0.5349732637405396
train gradient:  0.18448679922391606
iteration : 10042
train acc:  0.75
train loss:  0.48636847734451294
train gradient:  0.1324246396337233
iteration : 10043
train acc:  0.6875
train loss:  0.5427489280700684
train gradient:  0.15699154032515528
iteration : 10044
train acc:  0.75
train loss:  0.4587897062301636
train gradient:  0.0913519509398714
iteration : 10045
train acc:  0.7421875
train loss:  0.4922034740447998
train gradient:  0.1332326482258392
iteration : 10046
train acc:  0.8125
train loss:  0.449080228805542
train gradient:  0.12966009057801214
iteration : 10047
train acc:  0.78125
train loss:  0.484533429145813
train gradient:  0.142315654359697
iteration : 10048
train acc:  0.6875
train loss:  0.5686078667640686
train gradient:  0.15171845561616593
iteration : 10049
train acc:  0.7578125
train loss:  0.47549349069595337
train gradient:  0.1413404873250883
iteration : 10050
train acc:  0.734375
train loss:  0.47810959815979004
train gradient:  0.10964879422472112
iteration : 10051
train acc:  0.734375
train loss:  0.48358529806137085
train gradient:  0.11159817181789436
iteration : 10052
train acc:  0.7265625
train loss:  0.4976767599582672
train gradient:  0.1600761704091221
iteration : 10053
train acc:  0.7109375
train loss:  0.47099068760871887
train gradient:  0.10786496905214531
iteration : 10054
train acc:  0.75
train loss:  0.49720194935798645
train gradient:  0.10594004689392778
iteration : 10055
train acc:  0.765625
train loss:  0.49275946617126465
train gradient:  0.1322527341231483
iteration : 10056
train acc:  0.75
train loss:  0.45614609122276306
train gradient:  0.12534553744892324
iteration : 10057
train acc:  0.7890625
train loss:  0.43544477224349976
train gradient:  0.09867161575077563
iteration : 10058
train acc:  0.6953125
train loss:  0.5310452580451965
train gradient:  0.1441674171978436
iteration : 10059
train acc:  0.7578125
train loss:  0.449643075466156
train gradient:  0.16345589348284953
iteration : 10060
train acc:  0.6953125
train loss:  0.48766931891441345
train gradient:  0.14151909895691778
iteration : 10061
train acc:  0.7578125
train loss:  0.46140992641448975
train gradient:  0.10392312185767211
iteration : 10062
train acc:  0.6875
train loss:  0.5065006017684937
train gradient:  0.13858413916647444
iteration : 10063
train acc:  0.7109375
train loss:  0.5327150821685791
train gradient:  0.12649326517814138
iteration : 10064
train acc:  0.734375
train loss:  0.49221575260162354
train gradient:  0.15770915313013303
iteration : 10065
train acc:  0.7421875
train loss:  0.41736578941345215
train gradient:  0.09934162215696529
iteration : 10066
train acc:  0.78125
train loss:  0.4737370014190674
train gradient:  0.11818234187174725
iteration : 10067
train acc:  0.78125
train loss:  0.4591630697250366
train gradient:  0.12386411496584775
iteration : 10068
train acc:  0.71875
train loss:  0.5110387802124023
train gradient:  0.12555101109625794
iteration : 10069
train acc:  0.7109375
train loss:  0.5150512456893921
train gradient:  0.13975395614331432
iteration : 10070
train acc:  0.765625
train loss:  0.45187532901763916
train gradient:  0.11625046894659646
iteration : 10071
train acc:  0.7734375
train loss:  0.4972360134124756
train gradient:  0.12268281045903004
iteration : 10072
train acc:  0.8125
train loss:  0.41033926606178284
train gradient:  0.10924489223978945
iteration : 10073
train acc:  0.6875
train loss:  0.5640586018562317
train gradient:  0.1492850544646323
iteration : 10074
train acc:  0.7734375
train loss:  0.5291950106620789
train gradient:  0.12992723025750988
iteration : 10075
train acc:  0.796875
train loss:  0.43976786732673645
train gradient:  0.11437225430568698
iteration : 10076
train acc:  0.7421875
train loss:  0.5558842420578003
train gradient:  0.16047157054393146
iteration : 10077
train acc:  0.75
train loss:  0.48611289262771606
train gradient:  0.1325998488027629
iteration : 10078
train acc:  0.78125
train loss:  0.4723188877105713
train gradient:  0.12918212735008044
iteration : 10079
train acc:  0.703125
train loss:  0.6072856187820435
train gradient:  0.22586263643687943
iteration : 10080
train acc:  0.765625
train loss:  0.4590384066104889
train gradient:  0.10533623375178035
iteration : 10081
train acc:  0.7421875
train loss:  0.5283085703849792
train gradient:  0.18884290535979142
iteration : 10082
train acc:  0.71875
train loss:  0.5163236856460571
train gradient:  0.14274291740905282
iteration : 10083
train acc:  0.734375
train loss:  0.49694734811782837
train gradient:  0.13777885690777825
iteration : 10084
train acc:  0.71875
train loss:  0.5369194746017456
train gradient:  0.14212964706776587
iteration : 10085
train acc:  0.75
train loss:  0.4425618052482605
train gradient:  0.09992889526968632
iteration : 10086
train acc:  0.765625
train loss:  0.469948410987854
train gradient:  0.09545704455902526
iteration : 10087
train acc:  0.6796875
train loss:  0.5164121985435486
train gradient:  0.12287186923261509
iteration : 10088
train acc:  0.703125
train loss:  0.5535386800765991
train gradient:  0.1645090531743732
iteration : 10089
train acc:  0.7109375
train loss:  0.521730899810791
train gradient:  0.15271710952504464
iteration : 10090
train acc:  0.7578125
train loss:  0.504828929901123
train gradient:  0.1500992135994053
iteration : 10091
train acc:  0.6875
train loss:  0.5110772848129272
train gradient:  0.13954358990696566
iteration : 10092
train acc:  0.7578125
train loss:  0.5020327568054199
train gradient:  0.12407902831510023
iteration : 10093
train acc:  0.734375
train loss:  0.5293916463851929
train gradient:  0.18176710292981585
iteration : 10094
train acc:  0.671875
train loss:  0.5489698648452759
train gradient:  0.15240367670077215
iteration : 10095
train acc:  0.7578125
train loss:  0.5600277185440063
train gradient:  0.161662916152905
iteration : 10096
train acc:  0.7421875
train loss:  0.5188474655151367
train gradient:  0.12895879793437529
iteration : 10097
train acc:  0.6796875
train loss:  0.5396417379379272
train gradient:  0.16207215990249588
iteration : 10098
train acc:  0.75
train loss:  0.47188565135002136
train gradient:  0.104920825850506
iteration : 10099
train acc:  0.7578125
train loss:  0.42903202772140503
train gradient:  0.12258800263895511
iteration : 10100
train acc:  0.7578125
train loss:  0.46877771615982056
train gradient:  0.11825055769840509
iteration : 10101
train acc:  0.7734375
train loss:  0.45715028047561646
train gradient:  0.12250693643958825
iteration : 10102
train acc:  0.7265625
train loss:  0.4989461302757263
train gradient:  0.1238687519001738
iteration : 10103
train acc:  0.75
train loss:  0.4794144034385681
train gradient:  0.13429943344034923
iteration : 10104
train acc:  0.8359375
train loss:  0.4331318736076355
train gradient:  0.10632758914456804
iteration : 10105
train acc:  0.8125
train loss:  0.4508711099624634
train gradient:  0.09423533389197344
iteration : 10106
train acc:  0.6484375
train loss:  0.5588637590408325
train gradient:  0.1256365835610361
iteration : 10107
train acc:  0.703125
train loss:  0.54567950963974
train gradient:  0.178318439841888
iteration : 10108
train acc:  0.6953125
train loss:  0.5277445316314697
train gradient:  0.13639876841328874
iteration : 10109
train acc:  0.734375
train loss:  0.4619995951652527
train gradient:  0.12186907765580236
iteration : 10110
train acc:  0.6484375
train loss:  0.5747843980789185
train gradient:  0.16322167345856678
iteration : 10111
train acc:  0.5625
train loss:  0.6027541160583496
train gradient:  0.18441526522301216
iteration : 10112
train acc:  0.7421875
train loss:  0.49662303924560547
train gradient:  0.13614630958125662
iteration : 10113
train acc:  0.75
train loss:  0.4668882489204407
train gradient:  0.11597017348005151
iteration : 10114
train acc:  0.734375
train loss:  0.5168238878250122
train gradient:  0.12248103541150877
iteration : 10115
train acc:  0.734375
train loss:  0.5263886451721191
train gradient:  0.1341121038988753
iteration : 10116
train acc:  0.7421875
train loss:  0.4873150885105133
train gradient:  0.12781392212262077
iteration : 10117
train acc:  0.671875
train loss:  0.5860993266105652
train gradient:  0.2109717493556248
iteration : 10118
train acc:  0.765625
train loss:  0.5078531503677368
train gradient:  0.15136413568491847
iteration : 10119
train acc:  0.7890625
train loss:  0.4438199996948242
train gradient:  0.11043780758844583
iteration : 10120
train acc:  0.7265625
train loss:  0.5359910726547241
train gradient:  0.14902353923739625
iteration : 10121
train acc:  0.7265625
train loss:  0.5102815628051758
train gradient:  0.13209011212087346
iteration : 10122
train acc:  0.7109375
train loss:  0.5272613763809204
train gradient:  0.12039996597101014
iteration : 10123
train acc:  0.7265625
train loss:  0.47848182916641235
train gradient:  0.1452910093055101
iteration : 10124
train acc:  0.71875
train loss:  0.5313830375671387
train gradient:  0.1224117956375221
iteration : 10125
train acc:  0.734375
train loss:  0.4662562608718872
train gradient:  0.1413607452656863
iteration : 10126
train acc:  0.78125
train loss:  0.4814317226409912
train gradient:  0.13562680765729979
iteration : 10127
train acc:  0.71875
train loss:  0.4647098183631897
train gradient:  0.09474616782817738
iteration : 10128
train acc:  0.7421875
train loss:  0.46098780632019043
train gradient:  0.10886072044682248
iteration : 10129
train acc:  0.703125
train loss:  0.5532392263412476
train gradient:  0.15504074261525835
iteration : 10130
train acc:  0.734375
train loss:  0.5440069437026978
train gradient:  0.1621413242164555
iteration : 10131
train acc:  0.734375
train loss:  0.47896742820739746
train gradient:  0.10191040003322961
iteration : 10132
train acc:  0.7890625
train loss:  0.4552783966064453
train gradient:  0.09609280512181283
iteration : 10133
train acc:  0.765625
train loss:  0.49035340547561646
train gradient:  0.1303162914591408
iteration : 10134
train acc:  0.7265625
train loss:  0.45546650886535645
train gradient:  0.1191086131472061
iteration : 10135
train acc:  0.796875
train loss:  0.4502785801887512
train gradient:  0.11598757231956568
iteration : 10136
train acc:  0.7421875
train loss:  0.5258076190948486
train gradient:  0.12720664809606824
iteration : 10137
train acc:  0.7578125
train loss:  0.45400774478912354
train gradient:  0.09932336166568585
iteration : 10138
train acc:  0.7578125
train loss:  0.43367817997932434
train gradient:  0.09175801076319517
iteration : 10139
train acc:  0.671875
train loss:  0.5229543447494507
train gradient:  0.1215515796598876
iteration : 10140
train acc:  0.71875
train loss:  0.45608001947402954
train gradient:  0.10905741186939111
iteration : 10141
train acc:  0.7421875
train loss:  0.4851130247116089
train gradient:  0.1348357026630648
iteration : 10142
train acc:  0.7734375
train loss:  0.4978708028793335
train gradient:  0.12615636216257975
iteration : 10143
train acc:  0.734375
train loss:  0.5356990098953247
train gradient:  0.13532335319469319
iteration : 10144
train acc:  0.71875
train loss:  0.4980149269104004
train gradient:  0.123485195503039
iteration : 10145
train acc:  0.7578125
train loss:  0.4539493918418884
train gradient:  0.11367424215617077
iteration : 10146
train acc:  0.7578125
train loss:  0.4705241024494171
train gradient:  0.10774299305812705
iteration : 10147
train acc:  0.765625
train loss:  0.48372867703437805
train gradient:  0.1282965537826039
iteration : 10148
train acc:  0.765625
train loss:  0.47202959656715393
train gradient:  0.1355306969567825
iteration : 10149
train acc:  0.7578125
train loss:  0.4577232003211975
train gradient:  0.10798751338483677
iteration : 10150
train acc:  0.8046875
train loss:  0.4455893635749817
train gradient:  0.11201887330924709
iteration : 10151
train acc:  0.734375
train loss:  0.5114467144012451
train gradient:  0.14325986489662687
iteration : 10152
train acc:  0.703125
train loss:  0.5247560739517212
train gradient:  0.1292706572771467
iteration : 10153
train acc:  0.7265625
train loss:  0.5345399379730225
train gradient:  0.18457298595237703
iteration : 10154
train acc:  0.671875
train loss:  0.5519978404045105
train gradient:  0.15165707423415814
iteration : 10155
train acc:  0.7109375
train loss:  0.5218280553817749
train gradient:  0.15077161083602936
iteration : 10156
train acc:  0.7421875
train loss:  0.5019042491912842
train gradient:  0.15211494276037357
iteration : 10157
train acc:  0.7109375
train loss:  0.5942288637161255
train gradient:  0.16357288140950377
iteration : 10158
train acc:  0.71875
train loss:  0.5717891454696655
train gradient:  0.18089211845954095
iteration : 10159
train acc:  0.78125
train loss:  0.4505443274974823
train gradient:  0.10320692842504926
iteration : 10160
train acc:  0.7578125
train loss:  0.4830664396286011
train gradient:  0.10981026710251404
iteration : 10161
train acc:  0.7578125
train loss:  0.5407087802886963
train gradient:  0.17880910833764585
iteration : 10162
train acc:  0.734375
train loss:  0.5030494928359985
train gradient:  0.12760091108050808
iteration : 10163
train acc:  0.765625
train loss:  0.44335585832595825
train gradient:  0.0938056527287097
iteration : 10164
train acc:  0.734375
train loss:  0.48970896005630493
train gradient:  0.1293729708606329
iteration : 10165
train acc:  0.7578125
train loss:  0.47428038716316223
train gradient:  0.12156768598647133
iteration : 10166
train acc:  0.734375
train loss:  0.4863457679748535
train gradient:  0.11465434140309527
iteration : 10167
train acc:  0.78125
train loss:  0.476714164018631
train gradient:  0.11852052289735661
iteration : 10168
train acc:  0.765625
train loss:  0.4525822401046753
train gradient:  0.1152924556455116
iteration : 10169
train acc:  0.7265625
train loss:  0.49938246607780457
train gradient:  0.10855416179900174
iteration : 10170
train acc:  0.71875
train loss:  0.5168846845626831
train gradient:  0.13456400517290895
iteration : 10171
train acc:  0.7421875
train loss:  0.5726329684257507
train gradient:  0.15683626364809305
iteration : 10172
train acc:  0.78125
train loss:  0.506551206111908
train gradient:  0.1318002352939262
iteration : 10173
train acc:  0.6328125
train loss:  0.5945483446121216
train gradient:  0.16414609848287792
iteration : 10174
train acc:  0.7421875
train loss:  0.4480070471763611
train gradient:  0.09347470240262232
iteration : 10175
train acc:  0.703125
train loss:  0.5727432370185852
train gradient:  0.15454819865567104
iteration : 10176
train acc:  0.734375
train loss:  0.4814335107803345
train gradient:  0.11242559294165004
iteration : 10177
train acc:  0.734375
train loss:  0.5344434976577759
train gradient:  0.1524361964600714
iteration : 10178
train acc:  0.7734375
train loss:  0.43065983057022095
train gradient:  0.08837292171684646
iteration : 10179
train acc:  0.6796875
train loss:  0.6131091713905334
train gradient:  0.22207321333475583
iteration : 10180
train acc:  0.796875
train loss:  0.44671630859375
train gradient:  0.13172152937301357
iteration : 10181
train acc:  0.7265625
train loss:  0.48384326696395874
train gradient:  0.11615003548527857
iteration : 10182
train acc:  0.7578125
train loss:  0.5025119185447693
train gradient:  0.10798716071286066
iteration : 10183
train acc:  0.7734375
train loss:  0.469961941242218
train gradient:  0.11706050523211385
iteration : 10184
train acc:  0.7890625
train loss:  0.423438161611557
train gradient:  0.12067953357120123
iteration : 10185
train acc:  0.7265625
train loss:  0.5533046722412109
train gradient:  0.13466465721012333
iteration : 10186
train acc:  0.7734375
train loss:  0.483710378408432
train gradient:  0.10466126273725962
iteration : 10187
train acc:  0.7109375
train loss:  0.5411686897277832
train gradient:  0.13329756938037446
iteration : 10188
train acc:  0.8125
train loss:  0.40985095500946045
train gradient:  0.11978679811668663
iteration : 10189
train acc:  0.734375
train loss:  0.5015248656272888
train gradient:  0.12468840873553107
iteration : 10190
train acc:  0.78125
train loss:  0.49265390634536743
train gradient:  0.12957908734708934
iteration : 10191
train acc:  0.7109375
train loss:  0.482478529214859
train gradient:  0.11287148009087336
iteration : 10192
train acc:  0.7109375
train loss:  0.5235798954963684
train gradient:  0.11541940210476946
iteration : 10193
train acc:  0.703125
train loss:  0.5317466259002686
train gradient:  0.16575192563092617
iteration : 10194
train acc:  0.75
train loss:  0.44585609436035156
train gradient:  0.10105022782709905
iteration : 10195
train acc:  0.7734375
train loss:  0.46909624338150024
train gradient:  0.1288131538906263
iteration : 10196
train acc:  0.75
train loss:  0.4471743106842041
train gradient:  0.11088713097570968
iteration : 10197
train acc:  0.796875
train loss:  0.4735601842403412
train gradient:  0.12720549351110316
iteration : 10198
train acc:  0.765625
train loss:  0.5082182288169861
train gradient:  0.1224049149566334
iteration : 10199
train acc:  0.703125
train loss:  0.5177237391471863
train gradient:  0.1393313993234272
iteration : 10200
train acc:  0.7109375
train loss:  0.5022330284118652
train gradient:  0.14702998301463854
iteration : 10201
train acc:  0.75
train loss:  0.5084577798843384
train gradient:  0.11709570827568096
iteration : 10202
train acc:  0.7734375
train loss:  0.485011488199234
train gradient:  0.12712451054908303
iteration : 10203
train acc:  0.7578125
train loss:  0.4747558534145355
train gradient:  0.11538612493351137
iteration : 10204
train acc:  0.7421875
train loss:  0.48505449295043945
train gradient:  0.10411392792055775
iteration : 10205
train acc:  0.734375
train loss:  0.4764634370803833
train gradient:  0.11554568322202664
iteration : 10206
train acc:  0.7734375
train loss:  0.4562610983848572
train gradient:  0.11651586096687315
iteration : 10207
train acc:  0.7578125
train loss:  0.5016014575958252
train gradient:  0.1309490652033738
iteration : 10208
train acc:  0.8046875
train loss:  0.4175577163696289
train gradient:  0.10426750756485846
iteration : 10209
train acc:  0.7890625
train loss:  0.43735674023628235
train gradient:  0.12709417228905875
iteration : 10210
train acc:  0.7421875
train loss:  0.5464379787445068
train gradient:  0.1390063934579701
iteration : 10211
train acc:  0.6484375
train loss:  0.5363459587097168
train gradient:  0.12990183827754825
iteration : 10212
train acc:  0.734375
train loss:  0.5273388028144836
train gradient:  0.17272868277764905
iteration : 10213
train acc:  0.7421875
train loss:  0.5149276852607727
train gradient:  0.11601188289242126
iteration : 10214
train acc:  0.6796875
train loss:  0.5205475091934204
train gradient:  0.11048990791738146
iteration : 10215
train acc:  0.7265625
train loss:  0.5781396627426147
train gradient:  0.1419435660281838
iteration : 10216
train acc:  0.78125
train loss:  0.4155052602291107
train gradient:  0.08317969035476697
iteration : 10217
train acc:  0.734375
train loss:  0.5235487222671509
train gradient:  0.13418722839948916
iteration : 10218
train acc:  0.75
train loss:  0.5512750148773193
train gradient:  0.1396730511752278
iteration : 10219
train acc:  0.6484375
train loss:  0.5830638408660889
train gradient:  0.1802657761394687
iteration : 10220
train acc:  0.7421875
train loss:  0.48030439019203186
train gradient:  0.1439074745358472
iteration : 10221
train acc:  0.7421875
train loss:  0.46724599599838257
train gradient:  0.12809687008013418
iteration : 10222
train acc:  0.7578125
train loss:  0.4321594536304474
train gradient:  0.11193868999153823
iteration : 10223
train acc:  0.7734375
train loss:  0.4598686397075653
train gradient:  0.1262107787976916
iteration : 10224
train acc:  0.7109375
train loss:  0.5387846827507019
train gradient:  0.1369026385629655
iteration : 10225
train acc:  0.7109375
train loss:  0.49025294184684753
train gradient:  0.13411951031741892
iteration : 10226
train acc:  0.7578125
train loss:  0.46914124488830566
train gradient:  0.11101804877807138
iteration : 10227
train acc:  0.671875
train loss:  0.5882306098937988
train gradient:  0.20639644064266474
iteration : 10228
train acc:  0.75
train loss:  0.5069447159767151
train gradient:  0.11286059582575665
iteration : 10229
train acc:  0.7421875
train loss:  0.5120854377746582
train gradient:  0.11016753546029306
iteration : 10230
train acc:  0.75
train loss:  0.5170974135398865
train gradient:  0.15085498221321642
iteration : 10231
train acc:  0.875
train loss:  0.3840639591217041
train gradient:  0.11036551394309077
iteration : 10232
train acc:  0.6484375
train loss:  0.5590371489524841
train gradient:  0.17188104810288724
iteration : 10233
train acc:  0.6953125
train loss:  0.5304470658302307
train gradient:  0.12781384635361062
iteration : 10234
train acc:  0.765625
train loss:  0.4888083338737488
train gradient:  0.1684536094721218
iteration : 10235
train acc:  0.78125
train loss:  0.5173091888427734
train gradient:  0.12097408109385552
iteration : 10236
train acc:  0.71875
train loss:  0.4728900194168091
train gradient:  0.12267084401293744
iteration : 10237
train acc:  0.7109375
train loss:  0.5060405731201172
train gradient:  0.12164510804839183
iteration : 10238
train acc:  0.71875
train loss:  0.548896074295044
train gradient:  0.12308286933897479
iteration : 10239
train acc:  0.78125
train loss:  0.4365958869457245
train gradient:  0.10292081042260294
iteration : 10240
train acc:  0.78125
train loss:  0.45616793632507324
train gradient:  0.10785589624552826
iteration : 10241
train acc:  0.71875
train loss:  0.5051796436309814
train gradient:  0.14249296239674752
iteration : 10242
train acc:  0.734375
train loss:  0.5280098915100098
train gradient:  0.15822892146587456
iteration : 10243
train acc:  0.7265625
train loss:  0.5639782547950745
train gradient:  0.11521420667235666
iteration : 10244
train acc:  0.671875
train loss:  0.5421894788742065
train gradient:  0.14306709928731626
iteration : 10245
train acc:  0.703125
train loss:  0.49881279468536377
train gradient:  0.1563390539406016
iteration : 10246
train acc:  0.765625
train loss:  0.4795817732810974
train gradient:  0.10744148977006743
iteration : 10247
train acc:  0.7734375
train loss:  0.48541468381881714
train gradient:  0.13683812989553773
iteration : 10248
train acc:  0.734375
train loss:  0.5246092677116394
train gradient:  0.14450568083378829
iteration : 10249
train acc:  0.78125
train loss:  0.4396199882030487
train gradient:  0.09621348516827763
iteration : 10250
train acc:  0.796875
train loss:  0.44516533613204956
train gradient:  0.08087652225046774
iteration : 10251
train acc:  0.765625
train loss:  0.4806061387062073
train gradient:  0.10321992885343763
iteration : 10252
train acc:  0.7734375
train loss:  0.4586503505706787
train gradient:  0.10872732474397422
iteration : 10253
train acc:  0.7421875
train loss:  0.4813136160373688
train gradient:  0.13805893101221284
iteration : 10254
train acc:  0.6953125
train loss:  0.5512781739234924
train gradient:  0.1836867859000525
iteration : 10255
train acc:  0.671875
train loss:  0.5681869983673096
train gradient:  0.15909611944632884
iteration : 10256
train acc:  0.71875
train loss:  0.48189687728881836
train gradient:  0.10765752968043664
iteration : 10257
train acc:  0.75
train loss:  0.4660676121711731
train gradient:  0.12111819266827205
iteration : 10258
train acc:  0.71875
train loss:  0.525090754032135
train gradient:  0.13221946364843254
iteration : 10259
train acc:  0.7265625
train loss:  0.5558512210845947
train gradient:  0.17838510503389482
iteration : 10260
train acc:  0.734375
train loss:  0.4568074941635132
train gradient:  0.10283957225701831
iteration : 10261
train acc:  0.7265625
train loss:  0.628986120223999
train gradient:  0.20584619797113607
iteration : 10262
train acc:  0.7734375
train loss:  0.5214643478393555
train gradient:  0.1388250469692953
iteration : 10263
train acc:  0.6875
train loss:  0.5594295263290405
train gradient:  0.1518158300148012
iteration : 10264
train acc:  0.8046875
train loss:  0.3836171627044678
train gradient:  0.07236490753509224
iteration : 10265
train acc:  0.734375
train loss:  0.512169361114502
train gradient:  0.12482727472639973
iteration : 10266
train acc:  0.7421875
train loss:  0.49209871888160706
train gradient:  0.10294645153163656
iteration : 10267
train acc:  0.75
train loss:  0.46081823110580444
train gradient:  0.1637574560477335
iteration : 10268
train acc:  0.796875
train loss:  0.3988489508628845
train gradient:  0.08955045724408557
iteration : 10269
train acc:  0.703125
train loss:  0.4948401153087616
train gradient:  0.11082880491303189
iteration : 10270
train acc:  0.7421875
train loss:  0.4876839220523834
train gradient:  0.11366230882600582
iteration : 10271
train acc:  0.7265625
train loss:  0.4942176938056946
train gradient:  0.15098582301759036
iteration : 10272
train acc:  0.7421875
train loss:  0.4556036591529846
train gradient:  0.1125298040607179
iteration : 10273
train acc:  0.7890625
train loss:  0.47211411595344543
train gradient:  0.10486089069338271
iteration : 10274
train acc:  0.75
train loss:  0.4937024712562561
train gradient:  0.13194826632507062
iteration : 10275
train acc:  0.8203125
train loss:  0.4248371124267578
train gradient:  0.10623739005441987
iteration : 10276
train acc:  0.7578125
train loss:  0.43530264496803284
train gradient:  0.11814771052249243
iteration : 10277
train acc:  0.6484375
train loss:  0.5480877757072449
train gradient:  0.17659597821476117
iteration : 10278
train acc:  0.71875
train loss:  0.5655621290206909
train gradient:  0.1615862634910284
iteration : 10279
train acc:  0.703125
train loss:  0.5252512097358704
train gradient:  0.1512984080108102
iteration : 10280
train acc:  0.7890625
train loss:  0.47545069456100464
train gradient:  0.11184087400506726
iteration : 10281
train acc:  0.7890625
train loss:  0.4165886342525482
train gradient:  0.10185017611978837
iteration : 10282
train acc:  0.7109375
train loss:  0.5216259956359863
train gradient:  0.12857896116893042
iteration : 10283
train acc:  0.8046875
train loss:  0.42832639813423157
train gradient:  0.07657695218913715
iteration : 10284
train acc:  0.7265625
train loss:  0.5116845369338989
train gradient:  0.14093376445847616
iteration : 10285
train acc:  0.828125
train loss:  0.4361388087272644
train gradient:  0.10500907058589362
iteration : 10286
train acc:  0.765625
train loss:  0.4715906083583832
train gradient:  0.09817922809752982
iteration : 10287
train acc:  0.7578125
train loss:  0.4811578094959259
train gradient:  0.13342422812476776
iteration : 10288
train acc:  0.7265625
train loss:  0.4908040761947632
train gradient:  0.153541806843848
iteration : 10289
train acc:  0.734375
train loss:  0.5073798894882202
train gradient:  0.18254843073166788
iteration : 10290
train acc:  0.703125
train loss:  0.5624995231628418
train gradient:  0.14113156000655583
iteration : 10291
train acc:  0.7265625
train loss:  0.5038894414901733
train gradient:  0.1409953908684154
iteration : 10292
train acc:  0.6953125
train loss:  0.5355725288391113
train gradient:  0.13453526712121613
iteration : 10293
train acc:  0.7734375
train loss:  0.48741069436073303
train gradient:  0.16154731219183366
iteration : 10294
train acc:  0.734375
train loss:  0.5200575590133667
train gradient:  0.1253543970804403
iteration : 10295
train acc:  0.828125
train loss:  0.40939295291900635
train gradient:  0.09590625855805945
iteration : 10296
train acc:  0.734375
train loss:  0.46816545724868774
train gradient:  0.13015967992193667
iteration : 10297
train acc:  0.71875
train loss:  0.5218505859375
train gradient:  0.17015693019473638
iteration : 10298
train acc:  0.75
train loss:  0.45798009634017944
train gradient:  0.10436836760784814
iteration : 10299
train acc:  0.6953125
train loss:  0.526655912399292
train gradient:  0.12690454093605374
iteration : 10300
train acc:  0.7421875
train loss:  0.460806667804718
train gradient:  0.10534020720888987
iteration : 10301
train acc:  0.78125
train loss:  0.4776286482810974
train gradient:  0.11129394356715183
iteration : 10302
train acc:  0.734375
train loss:  0.5204774737358093
train gradient:  0.14267851056236242
iteration : 10303
train acc:  0.7265625
train loss:  0.5091375112533569
train gradient:  0.12125049941888938
iteration : 10304
train acc:  0.7421875
train loss:  0.45078417658805847
train gradient:  0.10229530772696947
iteration : 10305
train acc:  0.7265625
train loss:  0.5161921977996826
train gradient:  0.1305683376905184
iteration : 10306
train acc:  0.7265625
train loss:  0.49289679527282715
train gradient:  0.1360263201589174
iteration : 10307
train acc:  0.7578125
train loss:  0.465069979429245
train gradient:  0.10431312411659621
iteration : 10308
train acc:  0.71875
train loss:  0.49109846353530884
train gradient:  0.14153126711560537
iteration : 10309
train acc:  0.734375
train loss:  0.49066925048828125
train gradient:  0.11244527765288287
iteration : 10310
train acc:  0.7265625
train loss:  0.4982224702835083
train gradient:  0.13635405749530566
iteration : 10311
train acc:  0.7265625
train loss:  0.505002498626709
train gradient:  0.1549915391499349
iteration : 10312
train acc:  0.765625
train loss:  0.4474679231643677
train gradient:  0.0976616608772073
iteration : 10313
train acc:  0.7265625
train loss:  0.516895055770874
train gradient:  0.1525721693206748
iteration : 10314
train acc:  0.7578125
train loss:  0.4636685848236084
train gradient:  0.1276116276977107
iteration : 10315
train acc:  0.734375
train loss:  0.4919910728931427
train gradient:  0.14097676331071785
iteration : 10316
train acc:  0.7421875
train loss:  0.4374653398990631
train gradient:  0.09320911089719748
iteration : 10317
train acc:  0.734375
train loss:  0.5326902270317078
train gradient:  0.15067687797519336
iteration : 10318
train acc:  0.734375
train loss:  0.517389714717865
train gradient:  0.14815749951772098
iteration : 10319
train acc:  0.6015625
train loss:  0.5888750553131104
train gradient:  0.16696970478604578
iteration : 10320
train acc:  0.796875
train loss:  0.44396501779556274
train gradient:  0.10258262504638807
iteration : 10321
train acc:  0.71875
train loss:  0.5661788582801819
train gradient:  0.15568836945917824
iteration : 10322
train acc:  0.765625
train loss:  0.4452979564666748
train gradient:  0.1070198659729695
iteration : 10323
train acc:  0.703125
train loss:  0.5458229780197144
train gradient:  0.1458748714387625
iteration : 10324
train acc:  0.7109375
train loss:  0.5844138264656067
train gradient:  0.15934077352153048
iteration : 10325
train acc:  0.6640625
train loss:  0.5055768489837646
train gradient:  0.13069966841029373
iteration : 10326
train acc:  0.7421875
train loss:  0.49375230073928833
train gradient:  0.14377413769320263
iteration : 10327
train acc:  0.7734375
train loss:  0.42832040786743164
train gradient:  0.09386172240179673
iteration : 10328
train acc:  0.703125
train loss:  0.5292888283729553
train gradient:  0.11350509531091708
iteration : 10329
train acc:  0.75
train loss:  0.5043624639511108
train gradient:  0.11337892360987233
iteration : 10330
train acc:  0.75
train loss:  0.5015240907669067
train gradient:  0.1063920003652252
iteration : 10331
train acc:  0.640625
train loss:  0.6346442699432373
train gradient:  0.1840241892335881
iteration : 10332
train acc:  0.7421875
train loss:  0.4931783676147461
train gradient:  0.1579560105186662
iteration : 10333
train acc:  0.75
train loss:  0.46154364943504333
train gradient:  0.11676782787970255
iteration : 10334
train acc:  0.65625
train loss:  0.5480544567108154
train gradient:  0.15431512209555898
iteration : 10335
train acc:  0.765625
train loss:  0.4791874885559082
train gradient:  0.10973971174061554
iteration : 10336
train acc:  0.7109375
train loss:  0.5143974423408508
train gradient:  0.13706040213687437
iteration : 10337
train acc:  0.7265625
train loss:  0.49281081557273865
train gradient:  0.12053129084701177
iteration : 10338
train acc:  0.78125
train loss:  0.45100662112236023
train gradient:  0.11963851867975216
iteration : 10339
train acc:  0.7421875
train loss:  0.5133332014083862
train gradient:  0.12740387930487412
iteration : 10340
train acc:  0.703125
train loss:  0.5346900224685669
train gradient:  0.13352926101309134
iteration : 10341
train acc:  0.765625
train loss:  0.475890576839447
train gradient:  0.12392654074590709
iteration : 10342
train acc:  0.828125
train loss:  0.40107762813568115
train gradient:  0.08071705481856625
iteration : 10343
train acc:  0.7109375
train loss:  0.521399736404419
train gradient:  0.15928297248549467
iteration : 10344
train acc:  0.7578125
train loss:  0.4492452144622803
train gradient:  0.12671492206192633
iteration : 10345
train acc:  0.75
train loss:  0.4943965673446655
train gradient:  0.12387892358189213
iteration : 10346
train acc:  0.859375
train loss:  0.34652358293533325
train gradient:  0.08586581063429717
iteration : 10347
train acc:  0.6796875
train loss:  0.6012691259384155
train gradient:  0.16835402263668447
iteration : 10348
train acc:  0.765625
train loss:  0.4810703992843628
train gradient:  0.12168911273700939
iteration : 10349
train acc:  0.7578125
train loss:  0.48255765438079834
train gradient:  0.10578144993771119
iteration : 10350
train acc:  0.6875
train loss:  0.54874587059021
train gradient:  0.1174852553849007
iteration : 10351
train acc:  0.8203125
train loss:  0.4143322706222534
train gradient:  0.08340641590828796
iteration : 10352
train acc:  0.7890625
train loss:  0.43319272994995117
train gradient:  0.10099852092093582
iteration : 10353
train acc:  0.6953125
train loss:  0.48588621616363525
train gradient:  0.12830089761228336
iteration : 10354
train acc:  0.8515625
train loss:  0.3736456632614136
train gradient:  0.09813117840395207
iteration : 10355
train acc:  0.734375
train loss:  0.4676353931427002
train gradient:  0.1292201965807329
iteration : 10356
train acc:  0.6640625
train loss:  0.5417598485946655
train gradient:  0.1422907580738954
iteration : 10357
train acc:  0.7890625
train loss:  0.48199185729026794
train gradient:  0.12648627320131928
iteration : 10358
train acc:  0.7265625
train loss:  0.4975641965866089
train gradient:  0.12902703410146582
iteration : 10359
train acc:  0.6875
train loss:  0.5453793406486511
train gradient:  0.15068859271708718
iteration : 10360
train acc:  0.6953125
train loss:  0.5315127372741699
train gradient:  0.16394611617659144
iteration : 10361
train acc:  0.6953125
train loss:  0.493095725774765
train gradient:  0.11068837807847084
iteration : 10362
train acc:  0.7578125
train loss:  0.5038946866989136
train gradient:  0.13600204473843963
iteration : 10363
train acc:  0.7109375
train loss:  0.5512444972991943
train gradient:  0.20090844159869337
iteration : 10364
train acc:  0.703125
train loss:  0.5143324136734009
train gradient:  0.11985728701086411
iteration : 10365
train acc:  0.75
train loss:  0.48692786693573
train gradient:  0.1394814322966096
iteration : 10366
train acc:  0.703125
train loss:  0.5027347207069397
train gradient:  0.11433746137952967
iteration : 10367
train acc:  0.7421875
train loss:  0.482842355966568
train gradient:  0.11008494164781141
iteration : 10368
train acc:  0.734375
train loss:  0.49562928080558777
train gradient:  0.1372638612012898
iteration : 10369
train acc:  0.71875
train loss:  0.573132336139679
train gradient:  0.20304142868066932
iteration : 10370
train acc:  0.8046875
train loss:  0.44625020027160645
train gradient:  0.10153785181615424
iteration : 10371
train acc:  0.703125
train loss:  0.5795588493347168
train gradient:  0.17435483722240747
iteration : 10372
train acc:  0.7890625
train loss:  0.4493388235569
train gradient:  0.11250093450819133
iteration : 10373
train acc:  0.7734375
train loss:  0.4354141354560852
train gradient:  0.13414191374386242
iteration : 10374
train acc:  0.7265625
train loss:  0.49832960963249207
train gradient:  0.10475386962138296
iteration : 10375
train acc:  0.765625
train loss:  0.48730742931365967
train gradient:  0.12967172618223696
iteration : 10376
train acc:  0.7578125
train loss:  0.4679626226425171
train gradient:  0.10175275933235553
iteration : 10377
train acc:  0.796875
train loss:  0.4979948401451111
train gradient:  0.13030265418891235
iteration : 10378
train acc:  0.703125
train loss:  0.5505017042160034
train gradient:  0.1494034237446634
iteration : 10379
train acc:  0.7734375
train loss:  0.4628499746322632
train gradient:  0.1038070020759139
iteration : 10380
train acc:  0.7265625
train loss:  0.5428973436355591
train gradient:  0.14369153754516586
iteration : 10381
train acc:  0.7421875
train loss:  0.47115272283554077
train gradient:  0.10578899334058325
iteration : 10382
train acc:  0.7734375
train loss:  0.4797125458717346
train gradient:  0.111681686197463
iteration : 10383
train acc:  0.7734375
train loss:  0.47590166330337524
train gradient:  0.11718348235004368
iteration : 10384
train acc:  0.75
train loss:  0.4563382565975189
train gradient:  0.11886974894176773
iteration : 10385
train acc:  0.703125
train loss:  0.5001721382141113
train gradient:  0.13881465973246324
iteration : 10386
train acc:  0.796875
train loss:  0.5042093992233276
train gradient:  0.155006015129044
iteration : 10387
train acc:  0.75
train loss:  0.5028225779533386
train gradient:  0.14398431706529063
iteration : 10388
train acc:  0.7578125
train loss:  0.4596264958381653
train gradient:  0.11167477428980423
iteration : 10389
train acc:  0.75
train loss:  0.5240409970283508
train gradient:  0.1602441101484542
iteration : 10390
train acc:  0.75
train loss:  0.5279858112335205
train gradient:  0.1609106559305346
iteration : 10391
train acc:  0.7578125
train loss:  0.4453238248825073
train gradient:  0.10445622133756141
iteration : 10392
train acc:  0.7421875
train loss:  0.5154061317443848
train gradient:  0.16138806433472608
iteration : 10393
train acc:  0.765625
train loss:  0.4741174876689911
train gradient:  0.12802172540286824
iteration : 10394
train acc:  0.7265625
train loss:  0.4573586881160736
train gradient:  0.10456669540535359
iteration : 10395
train acc:  0.7109375
train loss:  0.5290560722351074
train gradient:  0.19273880767617574
iteration : 10396
train acc:  0.7265625
train loss:  0.5358436703681946
train gradient:  0.15618843271427874
iteration : 10397
train acc:  0.7109375
train loss:  0.542295515537262
train gradient:  0.14809542271457565
iteration : 10398
train acc:  0.7734375
train loss:  0.4910717308521271
train gradient:  0.11696719898939636
iteration : 10399
train acc:  0.6875
train loss:  0.5367152690887451
train gradient:  0.13187381345406007
iteration : 10400
train acc:  0.703125
train loss:  0.5158213973045349
train gradient:  0.1349297305875311
iteration : 10401
train acc:  0.7734375
train loss:  0.47536513209342957
train gradient:  0.09768059401212512
iteration : 10402
train acc:  0.703125
train loss:  0.5423098802566528
train gradient:  0.1497078301805227
iteration : 10403
train acc:  0.78125
train loss:  0.48510414361953735
train gradient:  0.12729165040815876
iteration : 10404
train acc:  0.703125
train loss:  0.49925124645233154
train gradient:  0.14262298697155149
iteration : 10405
train acc:  0.75
train loss:  0.48010677099227905
train gradient:  0.11844861312841937
iteration : 10406
train acc:  0.7421875
train loss:  0.5236764550209045
train gradient:  0.15693933376139943
iteration : 10407
train acc:  0.8125
train loss:  0.4557301998138428
train gradient:  0.1057556710158473
iteration : 10408
train acc:  0.6953125
train loss:  0.5113515853881836
train gradient:  0.14480998413736618
iteration : 10409
train acc:  0.7734375
train loss:  0.4903230667114258
train gradient:  0.12269735062871281
iteration : 10410
train acc:  0.7421875
train loss:  0.5039234161376953
train gradient:  0.13138503720468797
iteration : 10411
train acc:  0.7578125
train loss:  0.4559565782546997
train gradient:  0.10424094273082145
iteration : 10412
train acc:  0.7421875
train loss:  0.5136287808418274
train gradient:  0.12216730498122548
iteration : 10413
train acc:  0.7421875
train loss:  0.4916609227657318
train gradient:  0.1640944357969541
iteration : 10414
train acc:  0.7734375
train loss:  0.47690802812576294
train gradient:  0.1324369741598655
iteration : 10415
train acc:  0.796875
train loss:  0.43603014945983887
train gradient:  0.08252324084330925
iteration : 10416
train acc:  0.71875
train loss:  0.4976475238800049
train gradient:  0.13294212952424939
iteration : 10417
train acc:  0.71875
train loss:  0.4906025528907776
train gradient:  0.12479489502041291
iteration : 10418
train acc:  0.7265625
train loss:  0.5314213037490845
train gradient:  0.14212135801426912
iteration : 10419
train acc:  0.7265625
train loss:  0.495408296585083
train gradient:  0.13135402287886355
iteration : 10420
train acc:  0.671875
train loss:  0.5321606993675232
train gradient:  0.13834635396323308
iteration : 10421
train acc:  0.734375
train loss:  0.48914337158203125
train gradient:  0.11068777483140238
iteration : 10422
train acc:  0.8125
train loss:  0.5123202204704285
train gradient:  0.13899848826980726
iteration : 10423
train acc:  0.8046875
train loss:  0.43907058238983154
train gradient:  0.10692648429549342
iteration : 10424
train acc:  0.75
train loss:  0.430088073015213
train gradient:  0.0848565161003599
iteration : 10425
train acc:  0.75
train loss:  0.4869179129600525
train gradient:  0.11887520004514093
iteration : 10426
train acc:  0.8125
train loss:  0.4590712785720825
train gradient:  0.11195980456952305
iteration : 10427
train acc:  0.75
train loss:  0.4592764973640442
train gradient:  0.11323094131270432
iteration : 10428
train acc:  0.7421875
train loss:  0.47440022230148315
train gradient:  0.11574843646230346
iteration : 10429
train acc:  0.78125
train loss:  0.43799251317977905
train gradient:  0.11797759974287723
iteration : 10430
train acc:  0.703125
train loss:  0.5325758457183838
train gradient:  0.13908021919506208
iteration : 10431
train acc:  0.734375
train loss:  0.506104588508606
train gradient:  0.1307149389258754
iteration : 10432
train acc:  0.6796875
train loss:  0.5390290021896362
train gradient:  0.15637001501502146
iteration : 10433
train acc:  0.7890625
train loss:  0.4742361009120941
train gradient:  0.1634411649107942
iteration : 10434
train acc:  0.765625
train loss:  0.4611679017543793
train gradient:  0.10052707529042718
iteration : 10435
train acc:  0.71875
train loss:  0.4701330065727234
train gradient:  0.11261796233203748
iteration : 10436
train acc:  0.7421875
train loss:  0.4793323576450348
train gradient:  0.13952410792736303
iteration : 10437
train acc:  0.75
train loss:  0.4925200939178467
train gradient:  0.11445072969603537
iteration : 10438
train acc:  0.75
train loss:  0.4598066210746765
train gradient:  0.11761305732215285
iteration : 10439
train acc:  0.6796875
train loss:  0.5875861644744873
train gradient:  0.17414555969030598
iteration : 10440
train acc:  0.7890625
train loss:  0.4075627028942108
train gradient:  0.1144420946451884
iteration : 10441
train acc:  0.75
train loss:  0.5151228308677673
train gradient:  0.14436412148105798
iteration : 10442
train acc:  0.6796875
train loss:  0.50859135389328
train gradient:  0.1382564767185427
iteration : 10443
train acc:  0.78125
train loss:  0.4579634666442871
train gradient:  0.11233021455382575
iteration : 10444
train acc:  0.6953125
train loss:  0.48409178853034973
train gradient:  0.10736921577987187
iteration : 10445
train acc:  0.8359375
train loss:  0.4358397126197815
train gradient:  0.12701876882733987
iteration : 10446
train acc:  0.7890625
train loss:  0.49978360533714294
train gradient:  0.18429266708636616
iteration : 10447
train acc:  0.7734375
train loss:  0.4491560459136963
train gradient:  0.108520685342221
iteration : 10448
train acc:  0.7421875
train loss:  0.4985208213329315
train gradient:  0.10348446554835836
iteration : 10449
train acc:  0.765625
train loss:  0.4432167410850525
train gradient:  0.09910587842823196
iteration : 10450
train acc:  0.7265625
train loss:  0.47688227891921997
train gradient:  0.1679657397732891
iteration : 10451
train acc:  0.765625
train loss:  0.4750922620296478
train gradient:  0.15542063779140886
iteration : 10452
train acc:  0.75
train loss:  0.48183122277259827
train gradient:  0.10902658792297046
iteration : 10453
train acc:  0.7578125
train loss:  0.5044645071029663
train gradient:  0.14457667638928978
iteration : 10454
train acc:  0.75
train loss:  0.4951029419898987
train gradient:  0.13134517489855696
iteration : 10455
train acc:  0.7421875
train loss:  0.512820839881897
train gradient:  0.15449547399918157
iteration : 10456
train acc:  0.7265625
train loss:  0.5106827020645142
train gradient:  0.1276444221241393
iteration : 10457
train acc:  0.84375
train loss:  0.4153243601322174
train gradient:  0.10013358499289528
iteration : 10458
train acc:  0.71875
train loss:  0.5599976181983948
train gradient:  0.1525594712867228
iteration : 10459
train acc:  0.7421875
train loss:  0.4928569197654724
train gradient:  0.15891990086527322
iteration : 10460
train acc:  0.7578125
train loss:  0.5400378704071045
train gradient:  0.18381577668742638
iteration : 10461
train acc:  0.7734375
train loss:  0.4613836407661438
train gradient:  0.11294524218089978
iteration : 10462
train acc:  0.7265625
train loss:  0.5178662538528442
train gradient:  0.13383683241457706
iteration : 10463
train acc:  0.796875
train loss:  0.5063459277153015
train gradient:  0.1307840051747871
iteration : 10464
train acc:  0.71875
train loss:  0.5055068731307983
train gradient:  0.1482298505693374
iteration : 10465
train acc:  0.765625
train loss:  0.5155614614486694
train gradient:  0.12012289561505703
iteration : 10466
train acc:  0.8046875
train loss:  0.45887067914009094
train gradient:  0.11532195104864602
iteration : 10467
train acc:  0.7578125
train loss:  0.4625347852706909
train gradient:  0.13228749797900724
iteration : 10468
train acc:  0.7890625
train loss:  0.4412332773208618
train gradient:  0.10587217791911382
iteration : 10469
train acc:  0.8125
train loss:  0.4427528381347656
train gradient:  0.10808729035001519
iteration : 10470
train acc:  0.6953125
train loss:  0.5460051894187927
train gradient:  0.16541313638033978
iteration : 10471
train acc:  0.7734375
train loss:  0.44115567207336426
train gradient:  0.1035796818329848
iteration : 10472
train acc:  0.7265625
train loss:  0.4433891177177429
train gradient:  0.09833291273812068
iteration : 10473
train acc:  0.7265625
train loss:  0.550916314125061
train gradient:  0.13956216723982195
iteration : 10474
train acc:  0.7421875
train loss:  0.49873360991477966
train gradient:  0.13806874426532928
iteration : 10475
train acc:  0.71875
train loss:  0.5194110870361328
train gradient:  0.14923194707108878
iteration : 10476
train acc:  0.765625
train loss:  0.43692746758461
train gradient:  0.10285371034778058
iteration : 10477
train acc:  0.7734375
train loss:  0.4568549394607544
train gradient:  0.11453872559262775
iteration : 10478
train acc:  0.7421875
train loss:  0.4778172969818115
train gradient:  0.13677368117639227
iteration : 10479
train acc:  0.7734375
train loss:  0.43161725997924805
train gradient:  0.10672933888029164
iteration : 10480
train acc:  0.78125
train loss:  0.418451189994812
train gradient:  0.11464961440034772
iteration : 10481
train acc:  0.71875
train loss:  0.5267705917358398
train gradient:  0.15348743795748848
iteration : 10482
train acc:  0.71875
train loss:  0.5198900699615479
train gradient:  0.15127255507455406
iteration : 10483
train acc:  0.71875
train loss:  0.5153728127479553
train gradient:  0.11093357041560171
iteration : 10484
train acc:  0.7578125
train loss:  0.4563658535480499
train gradient:  0.10042262191048261
iteration : 10485
train acc:  0.734375
train loss:  0.4844321310520172
train gradient:  0.1296368195671857
iteration : 10486
train acc:  0.796875
train loss:  0.4230169355869293
train gradient:  0.1052092195794177
iteration : 10487
train acc:  0.7734375
train loss:  0.4738464951515198
train gradient:  0.13446915365670603
iteration : 10488
train acc:  0.7578125
train loss:  0.46119511127471924
train gradient:  0.09535064606118107
iteration : 10489
train acc:  0.765625
train loss:  0.45219314098358154
train gradient:  0.116609582288282
iteration : 10490
train acc:  0.7421875
train loss:  0.5213648080825806
train gradient:  0.14495656535695756
iteration : 10491
train acc:  0.7421875
train loss:  0.4685787558555603
train gradient:  0.11775223687546521
iteration : 10492
train acc:  0.703125
train loss:  0.5258630514144897
train gradient:  0.16377354687934675
iteration : 10493
train acc:  0.7578125
train loss:  0.45663362741470337
train gradient:  0.1112724570681082
iteration : 10494
train acc:  0.8125
train loss:  0.4312366247177124
train gradient:  0.09588003659567845
iteration : 10495
train acc:  0.75
train loss:  0.4836520552635193
train gradient:  0.12470312622038777
iteration : 10496
train acc:  0.796875
train loss:  0.4483209252357483
train gradient:  0.11549695788116772
iteration : 10497
train acc:  0.8359375
train loss:  0.3950124979019165
train gradient:  0.10446915640808946
iteration : 10498
train acc:  0.734375
train loss:  0.5548579692840576
train gradient:  0.1667322048658499
iteration : 10499
train acc:  0.796875
train loss:  0.48247143626213074
train gradient:  0.11096185600329632
iteration : 10500
train acc:  0.734375
train loss:  0.4822254478931427
train gradient:  0.13915357324183997
iteration : 10501
train acc:  0.7265625
train loss:  0.5401135087013245
train gradient:  0.177202201940585
iteration : 10502
train acc:  0.734375
train loss:  0.4864187240600586
train gradient:  0.11249424005355206
iteration : 10503
train acc:  0.7421875
train loss:  0.43969613313674927
train gradient:  0.09893734155886663
iteration : 10504
train acc:  0.7890625
train loss:  0.4599704146385193
train gradient:  0.16372157382110714
iteration : 10505
train acc:  0.7578125
train loss:  0.5122064352035522
train gradient:  0.13842355845099424
iteration : 10506
train acc:  0.796875
train loss:  0.43293726444244385
train gradient:  0.11080077069432152
iteration : 10507
train acc:  0.7109375
train loss:  0.5172759294509888
train gradient:  0.14035773630018145
iteration : 10508
train acc:  0.7734375
train loss:  0.46694281697273254
train gradient:  0.17894415209922196
iteration : 10509
train acc:  0.734375
train loss:  0.49424487352371216
train gradient:  0.15735305966612792
iteration : 10510
train acc:  0.78125
train loss:  0.49460917711257935
train gradient:  0.14437789532310394
iteration : 10511
train acc:  0.71875
train loss:  0.49311167001724243
train gradient:  0.12059895134922181
iteration : 10512
train acc:  0.765625
train loss:  0.522650957107544
train gradient:  0.1346954247095291
iteration : 10513
train acc:  0.7734375
train loss:  0.472905695438385
train gradient:  0.14274153686966967
iteration : 10514
train acc:  0.7421875
train loss:  0.496171772480011
train gradient:  0.12830770831390156
iteration : 10515
train acc:  0.75
train loss:  0.46024760603904724
train gradient:  0.11284233616740079
iteration : 10516
train acc:  0.7421875
train loss:  0.49330732226371765
train gradient:  0.11630318543435376
iteration : 10517
train acc:  0.7265625
train loss:  0.5121320486068726
train gradient:  0.1531895696607004
iteration : 10518
train acc:  0.6953125
train loss:  0.5098269581794739
train gradient:  0.13934901124483345
iteration : 10519
train acc:  0.734375
train loss:  0.4947061538696289
train gradient:  0.13068150425439468
iteration : 10520
train acc:  0.7578125
train loss:  0.5784265995025635
train gradient:  0.18540257337273341
iteration : 10521
train acc:  0.78125
train loss:  0.43472421169281006
train gradient:  0.08632257170647305
iteration : 10522
train acc:  0.6328125
train loss:  0.6307615041732788
train gradient:  0.1763223800117778
iteration : 10523
train acc:  0.7265625
train loss:  0.5053969621658325
train gradient:  0.13135292232263288
iteration : 10524
train acc:  0.75
train loss:  0.5136734247207642
train gradient:  0.17281919403902707
iteration : 10525
train acc:  0.78125
train loss:  0.4600900411605835
train gradient:  0.11296894630487789
iteration : 10526
train acc:  0.7265625
train loss:  0.5361392498016357
train gradient:  0.1379750236710554
iteration : 10527
train acc:  0.7265625
train loss:  0.4721844792366028
train gradient:  0.11737005654790163
iteration : 10528
train acc:  0.703125
train loss:  0.4927144944667816
train gradient:  0.1287145015539618
iteration : 10529
train acc:  0.7578125
train loss:  0.4958531856536865
train gradient:  0.12370540914616818
iteration : 10530
train acc:  0.734375
train loss:  0.48248425126075745
train gradient:  0.14596498596833718
iteration : 10531
train acc:  0.7421875
train loss:  0.5372894406318665
train gradient:  0.17161073155926176
iteration : 10532
train acc:  0.6875
train loss:  0.5464879274368286
train gradient:  0.13548111965268203
iteration : 10533
train acc:  0.734375
train loss:  0.5236112475395203
train gradient:  0.17316485332685033
iteration : 10534
train acc:  0.7109375
train loss:  0.5330460071563721
train gradient:  0.2173193620926619
iteration : 10535
train acc:  0.765625
train loss:  0.4828282594680786
train gradient:  0.12359863251233226
iteration : 10536
train acc:  0.7265625
train loss:  0.521543025970459
train gradient:  0.1541299158189345
iteration : 10537
train acc:  0.7734375
train loss:  0.47847697138786316
train gradient:  0.13472091219715218
iteration : 10538
train acc:  0.671875
train loss:  0.518242597579956
train gradient:  0.17600326609443312
iteration : 10539
train acc:  0.7890625
train loss:  0.425152063369751
train gradient:  0.09643253086335886
iteration : 10540
train acc:  0.75
train loss:  0.4498104453086853
train gradient:  0.13872134246531692
iteration : 10541
train acc:  0.7578125
train loss:  0.4866392910480499
train gradient:  0.10778009226879971
iteration : 10542
train acc:  0.78125
train loss:  0.5061107873916626
train gradient:  0.12524060273793974
iteration : 10543
train acc:  0.71875
train loss:  0.5249190926551819
train gradient:  0.18080705057339547
iteration : 10544
train acc:  0.7578125
train loss:  0.4904945492744446
train gradient:  0.16680439042168438
iteration : 10545
train acc:  0.8203125
train loss:  0.4094700217247009
train gradient:  0.13473649129826198
iteration : 10546
train acc:  0.78125
train loss:  0.490001380443573
train gradient:  0.13562357773122352
iteration : 10547
train acc:  0.734375
train loss:  0.5530561208724976
train gradient:  0.17386330360370306
iteration : 10548
train acc:  0.78125
train loss:  0.4415629208087921
train gradient:  0.09546856528844254
iteration : 10549
train acc:  0.765625
train loss:  0.4582935571670532
train gradient:  0.10842138114735667
iteration : 10550
train acc:  0.734375
train loss:  0.48893022537231445
train gradient:  0.1133133808345913
iteration : 10551
train acc:  0.6953125
train loss:  0.5061401724815369
train gradient:  0.14561503174853627
iteration : 10552
train acc:  0.796875
train loss:  0.5078459978103638
train gradient:  0.11649932325721957
iteration : 10553
train acc:  0.6875
train loss:  0.5334640741348267
train gradient:  0.14908739750895378
iteration : 10554
train acc:  0.7421875
train loss:  0.5095404982566833
train gradient:  0.09216908539544402
iteration : 10555
train acc:  0.78125
train loss:  0.45730966329574585
train gradient:  0.1196233779150793
iteration : 10556
train acc:  0.78125
train loss:  0.41700229048728943
train gradient:  0.10671099825399086
iteration : 10557
train acc:  0.75
train loss:  0.4800150692462921
train gradient:  0.15048334461134147
iteration : 10558
train acc:  0.796875
train loss:  0.4355909824371338
train gradient:  0.10009436209009334
iteration : 10559
train acc:  0.7578125
train loss:  0.47581422328948975
train gradient:  0.11363853016280116
iteration : 10560
train acc:  0.703125
train loss:  0.5312286615371704
train gradient:  0.18940419713594703
iteration : 10561
train acc:  0.7734375
train loss:  0.49732357263565063
train gradient:  0.12962207873603393
iteration : 10562
train acc:  0.8046875
train loss:  0.46348339319229126
train gradient:  0.10521850716736228
iteration : 10563
train acc:  0.7265625
train loss:  0.5442404747009277
train gradient:  0.19263611661107727
iteration : 10564
train acc:  0.7109375
train loss:  0.5542488098144531
train gradient:  0.14752876030592477
iteration : 10565
train acc:  0.7265625
train loss:  0.5058205723762512
train gradient:  0.1249687081897012
iteration : 10566
train acc:  0.765625
train loss:  0.4751584827899933
train gradient:  0.11582185037415642
iteration : 10567
train acc:  0.7421875
train loss:  0.5088634490966797
train gradient:  0.12760429342700474
iteration : 10568
train acc:  0.8046875
train loss:  0.4247305989265442
train gradient:  0.10941956339458478
iteration : 10569
train acc:  0.75
train loss:  0.5149083733558655
train gradient:  0.14342851399539275
iteration : 10570
train acc:  0.7421875
train loss:  0.4822920262813568
train gradient:  0.13038316024261265
iteration : 10571
train acc:  0.8203125
train loss:  0.425894558429718
train gradient:  0.12164501069507941
iteration : 10572
train acc:  0.75
train loss:  0.4990696310997009
train gradient:  0.10470250095489236
iteration : 10573
train acc:  0.734375
train loss:  0.4773674011230469
train gradient:  0.12056084892357062
iteration : 10574
train acc:  0.7421875
train loss:  0.4615165889263153
train gradient:  0.11567076411605907
iteration : 10575
train acc:  0.75
train loss:  0.5193452835083008
train gradient:  0.15737347049303188
iteration : 10576
train acc:  0.75
train loss:  0.4872676730155945
train gradient:  0.14690615031582832
iteration : 10577
train acc:  0.7734375
train loss:  0.4732261300086975
train gradient:  0.12793218446152899
iteration : 10578
train acc:  0.7265625
train loss:  0.4733077883720398
train gradient:  0.11889459931910128
iteration : 10579
train acc:  0.7421875
train loss:  0.46746981143951416
train gradient:  0.1381222598945154
iteration : 10580
train acc:  0.71875
train loss:  0.5039709210395813
train gradient:  0.13921819251041379
iteration : 10581
train acc:  0.7421875
train loss:  0.4694043695926666
train gradient:  0.11264737412944203
iteration : 10582
train acc:  0.7421875
train loss:  0.49276506900787354
train gradient:  0.13883927844079946
iteration : 10583
train acc:  0.8203125
train loss:  0.397647500038147
train gradient:  0.08000381489760261
iteration : 10584
train acc:  0.7578125
train loss:  0.46471160650253296
train gradient:  0.11409397493018075
iteration : 10585
train acc:  0.703125
train loss:  0.5444412231445312
train gradient:  0.20700583568500613
iteration : 10586
train acc:  0.6953125
train loss:  0.5493615865707397
train gradient:  0.14517487322791628
iteration : 10587
train acc:  0.75
train loss:  0.489353209733963
train gradient:  0.13684215651786488
iteration : 10588
train acc:  0.75
train loss:  0.5097566246986389
train gradient:  0.14859478805341558
iteration : 10589
train acc:  0.71875
train loss:  0.4689495861530304
train gradient:  0.09502476457123003
iteration : 10590
train acc:  0.734375
train loss:  0.4713335931301117
train gradient:  0.13534614836284975
iteration : 10591
train acc:  0.7890625
train loss:  0.43208062648773193
train gradient:  0.1246310454033353
iteration : 10592
train acc:  0.75
train loss:  0.4829696714878082
train gradient:  0.1282573800386848
iteration : 10593
train acc:  0.6953125
train loss:  0.5591691732406616
train gradient:  0.15309602472859923
iteration : 10594
train acc:  0.7421875
train loss:  0.4778904914855957
train gradient:  0.12011599089860253
iteration : 10595
train acc:  0.7578125
train loss:  0.48564115166664124
train gradient:  0.14298735894238668
iteration : 10596
train acc:  0.71875
train loss:  0.5228639841079712
train gradient:  0.15219864489187157
iteration : 10597
train acc:  0.71875
train loss:  0.48548537492752075
train gradient:  0.1270778425783613
iteration : 10598
train acc:  0.734375
train loss:  0.5075892806053162
train gradient:  0.12667119400039042
iteration : 10599
train acc:  0.6796875
train loss:  0.5374437570571899
train gradient:  0.15346686946588375
iteration : 10600
train acc:  0.765625
train loss:  0.48469969630241394
train gradient:  0.10022670154792512
iteration : 10601
train acc:  0.6640625
train loss:  0.6098816990852356
train gradient:  0.20902534776825205
iteration : 10602
train acc:  0.734375
train loss:  0.5257550477981567
train gradient:  0.1293095969006493
iteration : 10603
train acc:  0.703125
train loss:  0.5492914915084839
train gradient:  0.16433129474127128
iteration : 10604
train acc:  0.7109375
train loss:  0.4886203408241272
train gradient:  0.14669402811958748
iteration : 10605
train acc:  0.78125
train loss:  0.4761708378791809
train gradient:  0.11524118925243472
iteration : 10606
train acc:  0.7265625
train loss:  0.5008136630058289
train gradient:  0.14934778076705663
iteration : 10607
train acc:  0.7890625
train loss:  0.46603602170944214
train gradient:  0.12819762901001347
iteration : 10608
train acc:  0.6875
train loss:  0.5151602029800415
train gradient:  0.15700917261758113
iteration : 10609
train acc:  0.7734375
train loss:  0.4634860157966614
train gradient:  0.12369924417046081
iteration : 10610
train acc:  0.7109375
train loss:  0.5527912378311157
train gradient:  0.14966154903331336
iteration : 10611
train acc:  0.6875
train loss:  0.551420271396637
train gradient:  0.17302400460316214
iteration : 10612
train acc:  0.796875
train loss:  0.48169875144958496
train gradient:  0.13821852890812036
iteration : 10613
train acc:  0.703125
train loss:  0.51081782579422
train gradient:  0.146278313427877
iteration : 10614
train acc:  0.7421875
train loss:  0.5093176960945129
train gradient:  0.15444473884371462
iteration : 10615
train acc:  0.7890625
train loss:  0.44917169213294983
train gradient:  0.10977152910595514
iteration : 10616
train acc:  0.703125
train loss:  0.5364418029785156
train gradient:  0.17737065020359724
iteration : 10617
train acc:  0.71875
train loss:  0.5229972004890442
train gradient:  0.16219787247299344
iteration : 10618
train acc:  0.78125
train loss:  0.46137845516204834
train gradient:  0.10419060625115546
iteration : 10619
train acc:  0.7265625
train loss:  0.5170457363128662
train gradient:  0.13429450111149682
iteration : 10620
train acc:  0.78125
train loss:  0.43630456924438477
train gradient:  0.08567346167094371
iteration : 10621
train acc:  0.7265625
train loss:  0.48343124985694885
train gradient:  0.13325695217741385
iteration : 10622
train acc:  0.8046875
train loss:  0.44989195466041565
train gradient:  0.1193412889023246
iteration : 10623
train acc:  0.703125
train loss:  0.5059208869934082
train gradient:  0.1507576980297067
iteration : 10624
train acc:  0.78125
train loss:  0.46154841780662537
train gradient:  0.10511260433449741
iteration : 10625
train acc:  0.7421875
train loss:  0.49869683384895325
train gradient:  0.17042828947151337
iteration : 10626
train acc:  0.7421875
train loss:  0.4993899166584015
train gradient:  0.1292977950582873
iteration : 10627
train acc:  0.7578125
train loss:  0.5038121938705444
train gradient:  0.13547100480949759
iteration : 10628
train acc:  0.7734375
train loss:  0.4043971300125122
train gradient:  0.10097630492607214
iteration : 10629
train acc:  0.7109375
train loss:  0.5256859660148621
train gradient:  0.12319166031812909
iteration : 10630
train acc:  0.71875
train loss:  0.4744924008846283
train gradient:  0.11569256570353884
iteration : 10631
train acc:  0.7109375
train loss:  0.5551106929779053
train gradient:  0.16205734305886035
iteration : 10632
train acc:  0.734375
train loss:  0.5061503648757935
train gradient:  0.14687849040522172
iteration : 10633
train acc:  0.765625
train loss:  0.46233874559402466
train gradient:  0.11632120320415991
iteration : 10634
train acc:  0.734375
train loss:  0.5290424227714539
train gradient:  0.14032896007761053
iteration : 10635
train acc:  0.78125
train loss:  0.514784574508667
train gradient:  0.1301092819420589
iteration : 10636
train acc:  0.796875
train loss:  0.4399890899658203
train gradient:  0.0917784617067685
iteration : 10637
train acc:  0.71875
train loss:  0.5691050291061401
train gradient:  0.1868050423410584
iteration : 10638
train acc:  0.7578125
train loss:  0.48802661895751953
train gradient:  0.13592838398178472
iteration : 10639
train acc:  0.7109375
train loss:  0.5125479698181152
train gradient:  0.10830493971762156
iteration : 10640
train acc:  0.8046875
train loss:  0.42669877409935
train gradient:  0.08625040887895849
iteration : 10641
train acc:  0.7109375
train loss:  0.5476640462875366
train gradient:  0.14942841410538624
iteration : 10642
train acc:  0.7265625
train loss:  0.4781533479690552
train gradient:  0.09668753000345212
iteration : 10643
train acc:  0.703125
train loss:  0.5508662462234497
train gradient:  0.16262191663394926
iteration : 10644
train acc:  0.71875
train loss:  0.48469796776771545
train gradient:  0.13071892655580925
iteration : 10645
train acc:  0.7734375
train loss:  0.4493544101715088
train gradient:  0.10997006394227879
iteration : 10646
train acc:  0.7265625
train loss:  0.4729248881340027
train gradient:  0.12196854573216119
iteration : 10647
train acc:  0.7421875
train loss:  0.5027695894241333
train gradient:  0.12357294834228345
iteration : 10648
train acc:  0.703125
train loss:  0.5457488298416138
train gradient:  0.1452241368506778
iteration : 10649
train acc:  0.7578125
train loss:  0.4537067711353302
train gradient:  0.12065035961857742
iteration : 10650
train acc:  0.765625
train loss:  0.4788503646850586
train gradient:  0.13103231246229913
iteration : 10651
train acc:  0.71875
train loss:  0.5164144039154053
train gradient:  0.16607317521797144
iteration : 10652
train acc:  0.703125
train loss:  0.5509886741638184
train gradient:  0.1651522755405129
iteration : 10653
train acc:  0.7578125
train loss:  0.439519464969635
train gradient:  0.11723117129799271
iteration : 10654
train acc:  0.7890625
train loss:  0.4360022246837616
train gradient:  0.11231327455341822
iteration : 10655
train acc:  0.7734375
train loss:  0.43951767683029175
train gradient:  0.12204089219043489
iteration : 10656
train acc:  0.765625
train loss:  0.5179359912872314
train gradient:  0.14833524610819843
iteration : 10657
train acc:  0.71875
train loss:  0.5946090221405029
train gradient:  0.20012242930770557
iteration : 10658
train acc:  0.703125
train loss:  0.4895162284374237
train gradient:  0.1274343312196992
iteration : 10659
train acc:  0.78125
train loss:  0.4713672399520874
train gradient:  0.1257737064945908
iteration : 10660
train acc:  0.8203125
train loss:  0.40578705072402954
train gradient:  0.08774741191050242
iteration : 10661
train acc:  0.78125
train loss:  0.4537963569164276
train gradient:  0.11885866999826357
iteration : 10662
train acc:  0.75
train loss:  0.5042896270751953
train gradient:  0.11785524025998975
iteration : 10663
train acc:  0.6796875
train loss:  0.567982017993927
train gradient:  0.17369473426499432
iteration : 10664
train acc:  0.7109375
train loss:  0.5132842063903809
train gradient:  0.11303095891651736
iteration : 10665
train acc:  0.6875
train loss:  0.5362170934677124
train gradient:  0.1314694328785075
iteration : 10666
train acc:  0.703125
train loss:  0.48351162672042847
train gradient:  0.12650659439261808
iteration : 10667
train acc:  0.796875
train loss:  0.49872449040412903
train gradient:  0.13438568728995337
iteration : 10668
train acc:  0.7734375
train loss:  0.46606698632240295
train gradient:  0.12546658999608995
iteration : 10669
train acc:  0.765625
train loss:  0.4372028410434723
train gradient:  0.11333782286351818
iteration : 10670
train acc:  0.7265625
train loss:  0.48415446281433105
train gradient:  0.12112720108906884
iteration : 10671
train acc:  0.71875
train loss:  0.498325914144516
train gradient:  0.14502529386909396
iteration : 10672
train acc:  0.6640625
train loss:  0.5811828374862671
train gradient:  0.1703003751196445
iteration : 10673
train acc:  0.796875
train loss:  0.4296366274356842
train gradient:  0.10809431777483433
iteration : 10674
train acc:  0.8515625
train loss:  0.41109663248062134
train gradient:  0.12866218737019414
iteration : 10675
train acc:  0.7578125
train loss:  0.44944530725479126
train gradient:  0.09785548397484403
iteration : 10676
train acc:  0.734375
train loss:  0.5006438493728638
train gradient:  0.14321981699297615
iteration : 10677
train acc:  0.6796875
train loss:  0.5835986733436584
train gradient:  0.19220811794961057
iteration : 10678
train acc:  0.7265625
train loss:  0.5606127977371216
train gradient:  0.15488071267859632
iteration : 10679
train acc:  0.734375
train loss:  0.5432648658752441
train gradient:  0.18907964991237403
iteration : 10680
train acc:  0.7734375
train loss:  0.5322815179824829
train gradient:  0.1473098693228429
iteration : 10681
train acc:  0.6953125
train loss:  0.5290164351463318
train gradient:  0.16006944056271866
iteration : 10682
train acc:  0.75
train loss:  0.4489544630050659
train gradient:  0.13052437499611522
iteration : 10683
train acc:  0.7890625
train loss:  0.4823620915412903
train gradient:  0.1339388622065424
iteration : 10684
train acc:  0.734375
train loss:  0.4995749294757843
train gradient:  0.15048025764127976
iteration : 10685
train acc:  0.7265625
train loss:  0.45062872767448425
train gradient:  0.1217264866646664
iteration : 10686
train acc:  0.7265625
train loss:  0.5101609230041504
train gradient:  0.14670440626797698
iteration : 10687
train acc:  0.7265625
train loss:  0.5315871238708496
train gradient:  0.1252298845410525
iteration : 10688
train acc:  0.7265625
train loss:  0.5378286838531494
train gradient:  0.1213770930524372
iteration : 10689
train acc:  0.8515625
train loss:  0.38314443826675415
train gradient:  0.09117083134269334
iteration : 10690
train acc:  0.7265625
train loss:  0.535719096660614
train gradient:  0.1490559451889524
iteration : 10691
train acc:  0.7421875
train loss:  0.5024696588516235
train gradient:  0.12473634670803646
iteration : 10692
train acc:  0.7734375
train loss:  0.480475515127182
train gradient:  0.12873080155982014
iteration : 10693
train acc:  0.6953125
train loss:  0.5578272342681885
train gradient:  0.16768070989933587
iteration : 10694
train acc:  0.703125
train loss:  0.5479379892349243
train gradient:  0.15939828203442113
iteration : 10695
train acc:  0.6953125
train loss:  0.4987054467201233
train gradient:  0.13735271236926416
iteration : 10696
train acc:  0.7265625
train loss:  0.4817737340927124
train gradient:  0.13623430021651017
iteration : 10697
train acc:  0.734375
train loss:  0.4896569848060608
train gradient:  0.1550419377780759
iteration : 10698
train acc:  0.71875
train loss:  0.48220598697662354
train gradient:  0.1291039609216752
iteration : 10699
train acc:  0.6875
train loss:  0.5248126983642578
train gradient:  0.14669424650153837
iteration : 10700
train acc:  0.78125
train loss:  0.470511794090271
train gradient:  0.12650090356233934
iteration : 10701
train acc:  0.7421875
train loss:  0.5115692019462585
train gradient:  0.13272630223889603
iteration : 10702
train acc:  0.7421875
train loss:  0.5004866123199463
train gradient:  0.12272575974628204
iteration : 10703
train acc:  0.8046875
train loss:  0.4198834300041199
train gradient:  0.10333844762234906
iteration : 10704
train acc:  0.6953125
train loss:  0.5204156041145325
train gradient:  0.1329872284930062
iteration : 10705
train acc:  0.796875
train loss:  0.4338931143283844
train gradient:  0.08807991774978538
iteration : 10706
train acc:  0.7421875
train loss:  0.5006474256515503
train gradient:  0.14370059355718945
iteration : 10707
train acc:  0.7578125
train loss:  0.4896669387817383
train gradient:  0.12634389715872596
iteration : 10708
train acc:  0.7265625
train loss:  0.5068508386611938
train gradient:  0.09416222677747192
iteration : 10709
train acc:  0.71875
train loss:  0.47599709033966064
train gradient:  0.1126535047463594
iteration : 10710
train acc:  0.7578125
train loss:  0.44522035121917725
train gradient:  0.0983895272023926
iteration : 10711
train acc:  0.7578125
train loss:  0.48306408524513245
train gradient:  0.11937786321288359
iteration : 10712
train acc:  0.7421875
train loss:  0.5012390613555908
train gradient:  0.1282890388920464
iteration : 10713
train acc:  0.7890625
train loss:  0.47353029251098633
train gradient:  0.10733169939478687
iteration : 10714
train acc:  0.71875
train loss:  0.5036871433258057
train gradient:  0.16138881571366676
iteration : 10715
train acc:  0.78125
train loss:  0.46063560247421265
train gradient:  0.10189435618552933
iteration : 10716
train acc:  0.7421875
train loss:  0.4456480145454407
train gradient:  0.09799377918851604
iteration : 10717
train acc:  0.8046875
train loss:  0.4141799211502075
train gradient:  0.08620110845892492
iteration : 10718
train acc:  0.7734375
train loss:  0.44443032145500183
train gradient:  0.12875141580919983
iteration : 10719
train acc:  0.765625
train loss:  0.485037237405777
train gradient:  0.14553413907440266
iteration : 10720
train acc:  0.7578125
train loss:  0.4745948910713196
train gradient:  0.1355130009408376
iteration : 10721
train acc:  0.75
train loss:  0.49451911449432373
train gradient:  0.15432902225764478
iteration : 10722
train acc:  0.7578125
train loss:  0.4819200932979584
train gradient:  0.11808383480458504
iteration : 10723
train acc:  0.7265625
train loss:  0.4905063509941101
train gradient:  0.11987544880426033
iteration : 10724
train acc:  0.765625
train loss:  0.4595576822757721
train gradient:  0.10285265219949144
iteration : 10725
train acc:  0.8359375
train loss:  0.4038483500480652
train gradient:  0.09938171217944518
iteration : 10726
train acc:  0.7265625
train loss:  0.49446922540664673
train gradient:  0.11934744849911826
iteration : 10727
train acc:  0.7578125
train loss:  0.4397145211696625
train gradient:  0.11846723808396618
iteration : 10728
train acc:  0.7734375
train loss:  0.42879539728164673
train gradient:  0.09883772844190254
iteration : 10729
train acc:  0.7109375
train loss:  0.518060028553009
train gradient:  0.13279792713285518
iteration : 10730
train acc:  0.7421875
train loss:  0.483326256275177
train gradient:  0.1127354162383566
iteration : 10731
train acc:  0.7421875
train loss:  0.46895766258239746
train gradient:  0.11722029374508275
iteration : 10732
train acc:  0.7265625
train loss:  0.5260363817214966
train gradient:  0.11898001345542282
iteration : 10733
train acc:  0.7890625
train loss:  0.49139562249183655
train gradient:  0.12860189327610685
iteration : 10734
train acc:  0.8125
train loss:  0.46193909645080566
train gradient:  0.12426786416440493
iteration : 10735
train acc:  0.7109375
train loss:  0.5619708895683289
train gradient:  0.13910555486153653
iteration : 10736
train acc:  0.7734375
train loss:  0.4742392301559448
train gradient:  0.11620138297682242
iteration : 10737
train acc:  0.703125
train loss:  0.531320333480835
train gradient:  0.12491807014080551
iteration : 10738
train acc:  0.7265625
train loss:  0.5128904581069946
train gradient:  0.1493927874539997
iteration : 10739
train acc:  0.7578125
train loss:  0.4993506073951721
train gradient:  0.11657062674587554
iteration : 10740
train acc:  0.7578125
train loss:  0.5174094438552856
train gradient:  0.14480772530305525
iteration : 10741
train acc:  0.78125
train loss:  0.42901551723480225
train gradient:  0.1160972234790845
iteration : 10742
train acc:  0.703125
train loss:  0.5194258689880371
train gradient:  0.12946096616642766
iteration : 10743
train acc:  0.7265625
train loss:  0.4857324957847595
train gradient:  0.13409506174772393
iteration : 10744
train acc:  0.703125
train loss:  0.5395252704620361
train gradient:  0.13294514318105377
iteration : 10745
train acc:  0.7421875
train loss:  0.4784229099750519
train gradient:  0.1360329965768212
iteration : 10746
train acc:  0.7421875
train loss:  0.4731462299823761
train gradient:  0.10861633367575664
iteration : 10747
train acc:  0.75
train loss:  0.46400541067123413
train gradient:  0.10404855474762016
iteration : 10748
train acc:  0.7421875
train loss:  0.5163865685462952
train gradient:  0.12786998427428575
iteration : 10749
train acc:  0.78125
train loss:  0.4796930253505707
train gradient:  0.1117501342966811
iteration : 10750
train acc:  0.7890625
train loss:  0.44277510046958923
train gradient:  0.10624099301674565
iteration : 10751
train acc:  0.6953125
train loss:  0.5283064842224121
train gradient:  0.1461262208522754
iteration : 10752
train acc:  0.7265625
train loss:  0.49220478534698486
train gradient:  0.14730656230553185
iteration : 10753
train acc:  0.765625
train loss:  0.43582668900489807
train gradient:  0.09371845426294848
iteration : 10754
train acc:  0.75
train loss:  0.5446867346763611
train gradient:  0.15019960810433047
iteration : 10755
train acc:  0.7734375
train loss:  0.4840207099914551
train gradient:  0.12232199469033364
iteration : 10756
train acc:  0.75
train loss:  0.5176773071289062
train gradient:  0.12922739148030576
iteration : 10757
train acc:  0.75
train loss:  0.48261332511901855
train gradient:  0.13734146119073873
iteration : 10758
train acc:  0.765625
train loss:  0.44146856665611267
train gradient:  0.12778596100877437
iteration : 10759
train acc:  0.75
train loss:  0.4733154773712158
train gradient:  0.11051213514565358
iteration : 10760
train acc:  0.6875
train loss:  0.5720519423484802
train gradient:  0.1443817807664315
iteration : 10761
train acc:  0.7734375
train loss:  0.4795493185520172
train gradient:  0.10761106915872938
iteration : 10762
train acc:  0.71875
train loss:  0.5036977529525757
train gradient:  0.14030143531957107
iteration : 10763
train acc:  0.734375
train loss:  0.45994138717651367
train gradient:  0.09825788144677267
iteration : 10764
train acc:  0.75
train loss:  0.48392626643180847
train gradient:  0.12063154814753649
iteration : 10765
train acc:  0.7421875
train loss:  0.48115676641464233
train gradient:  0.10565030719662362
iteration : 10766
train acc:  0.7890625
train loss:  0.47974932193756104
train gradient:  0.09362086351099481
iteration : 10767
train acc:  0.625
train loss:  0.5623549818992615
train gradient:  0.1815666618046224
iteration : 10768
train acc:  0.7734375
train loss:  0.48847901821136475
train gradient:  0.11699661777119097
iteration : 10769
train acc:  0.6953125
train loss:  0.5587198138237
train gradient:  0.16512266660644154
iteration : 10770
train acc:  0.7109375
train loss:  0.5177934169769287
train gradient:  0.1629168103273326
iteration : 10771
train acc:  0.765625
train loss:  0.4525924623012543
train gradient:  0.09942769373081418
iteration : 10772
train acc:  0.765625
train loss:  0.46147865056991577
train gradient:  0.11785904931942297
iteration : 10773
train acc:  0.7109375
train loss:  0.5193917155265808
train gradient:  0.1497852321101314
iteration : 10774
train acc:  0.7421875
train loss:  0.4919840693473816
train gradient:  0.13226444747243776
iteration : 10775
train acc:  0.796875
train loss:  0.400374174118042
train gradient:  0.10864191220931899
iteration : 10776
train acc:  0.7890625
train loss:  0.43397581577301025
train gradient:  0.09904202369492196
iteration : 10777
train acc:  0.71875
train loss:  0.47628915309906006
train gradient:  0.15642287652708803
iteration : 10778
train acc:  0.796875
train loss:  0.42588010430336
train gradient:  0.11874045345934917
iteration : 10779
train acc:  0.65625
train loss:  0.5371069312095642
train gradient:  0.13752928111265128
iteration : 10780
train acc:  0.7734375
train loss:  0.45813655853271484
train gradient:  0.1243670287281814
iteration : 10781
train acc:  0.7109375
train loss:  0.4756435453891754
train gradient:  0.13008894804706805
iteration : 10782
train acc:  0.765625
train loss:  0.5049475431442261
train gradient:  0.1316761637443971
iteration : 10783
train acc:  0.7109375
train loss:  0.5347411632537842
train gradient:  0.1497711242674472
iteration : 10784
train acc:  0.7421875
train loss:  0.47073301672935486
train gradient:  0.10907797074294967
iteration : 10785
train acc:  0.6796875
train loss:  0.5131211876869202
train gradient:  0.1467833178667941
iteration : 10786
train acc:  0.6875
train loss:  0.5253084897994995
train gradient:  0.15319628087463824
iteration : 10787
train acc:  0.71875
train loss:  0.517810583114624
train gradient:  0.14684783010699953
iteration : 10788
train acc:  0.7890625
train loss:  0.44508224725723267
train gradient:  0.11228565945338365
iteration : 10789
train acc:  0.65625
train loss:  0.6353441476821899
train gradient:  0.2136845580299001
iteration : 10790
train acc:  0.7265625
train loss:  0.46171772480010986
train gradient:  0.10793284781516518
iteration : 10791
train acc:  0.75
train loss:  0.4489898085594177
train gradient:  0.12426325629635285
iteration : 10792
train acc:  0.78125
train loss:  0.43941545486450195
train gradient:  0.09606773770543571
iteration : 10793
train acc:  0.7421875
train loss:  0.4831444323062897
train gradient:  0.15097891733704308
iteration : 10794
train acc:  0.7109375
train loss:  0.5350704193115234
train gradient:  0.18061619271128473
iteration : 10795
train acc:  0.7265625
train loss:  0.49973544478416443
train gradient:  0.11158860962240598
iteration : 10796
train acc:  0.7265625
train loss:  0.540248453617096
train gradient:  0.13545237128206716
iteration : 10797
train acc:  0.7109375
train loss:  0.46761995553970337
train gradient:  0.11020075690999384
iteration : 10798
train acc:  0.734375
train loss:  0.49949681758880615
train gradient:  0.13570243900194373
iteration : 10799
train acc:  0.7421875
train loss:  0.4999978542327881
train gradient:  0.11457449509172395
iteration : 10800
train acc:  0.75
train loss:  0.47792625427246094
train gradient:  0.12487757668656171
iteration : 10801
train acc:  0.7890625
train loss:  0.4720122814178467
train gradient:  0.11987996480257743
iteration : 10802
train acc:  0.703125
train loss:  0.5275087952613831
train gradient:  0.18119280726435671
iteration : 10803
train acc:  0.7265625
train loss:  0.480707049369812
train gradient:  0.11685326086979066
iteration : 10804
train acc:  0.703125
train loss:  0.5470127463340759
train gradient:  0.15691215948564807
iteration : 10805
train acc:  0.7578125
train loss:  0.48231756687164307
train gradient:  0.12607464721226197
iteration : 10806
train acc:  0.7890625
train loss:  0.48390766978263855
train gradient:  0.11071750017520686
iteration : 10807
train acc:  0.734375
train loss:  0.5404926538467407
train gradient:  0.14456189137931208
iteration : 10808
train acc:  0.7265625
train loss:  0.5462725162506104
train gradient:  0.13643653758579488
iteration : 10809
train acc:  0.703125
train loss:  0.5263783931732178
train gradient:  0.14295499353010488
iteration : 10810
train acc:  0.78125
train loss:  0.4400154948234558
train gradient:  0.14664445023132217
iteration : 10811
train acc:  0.75
train loss:  0.5089645981788635
train gradient:  0.16909047211215222
iteration : 10812
train acc:  0.7578125
train loss:  0.45959004759788513
train gradient:  0.15405482944668197
iteration : 10813
train acc:  0.7109375
train loss:  0.5744820833206177
train gradient:  0.187538991406469
iteration : 10814
train acc:  0.6875
train loss:  0.5301783084869385
train gradient:  0.1400575021326057
iteration : 10815
train acc:  0.7265625
train loss:  0.5055267214775085
train gradient:  0.13276761227078446
iteration : 10816
train acc:  0.6953125
train loss:  0.5196799039840698
train gradient:  0.1441499378734461
iteration : 10817
train acc:  0.7734375
train loss:  0.46748650074005127
train gradient:  0.12427129703678622
iteration : 10818
train acc:  0.75
train loss:  0.5058132410049438
train gradient:  0.16077984359373215
iteration : 10819
train acc:  0.734375
train loss:  0.5038560628890991
train gradient:  0.17208843666179463
iteration : 10820
train acc:  0.6953125
train loss:  0.5286063551902771
train gradient:  0.13935405187313316
iteration : 10821
train acc:  0.78125
train loss:  0.4597131907939911
train gradient:  0.10503888012361345
iteration : 10822
train acc:  0.71875
train loss:  0.561519980430603
train gradient:  0.1695269100611071
iteration : 10823
train acc:  0.84375
train loss:  0.4330093264579773
train gradient:  0.12076257223043518
iteration : 10824
train acc:  0.734375
train loss:  0.5358996391296387
train gradient:  0.1252036095326719
iteration : 10825
train acc:  0.734375
train loss:  0.5208954215049744
train gradient:  0.1427736875714451
iteration : 10826
train acc:  0.7578125
train loss:  0.5022917985916138
train gradient:  0.15957769953282563
iteration : 10827
train acc:  0.765625
train loss:  0.43493956327438354
train gradient:  0.10014574142937778
iteration : 10828
train acc:  0.7109375
train loss:  0.5180580615997314
train gradient:  0.1335316056879764
iteration : 10829
train acc:  0.7734375
train loss:  0.4479290544986725
train gradient:  0.12019949772923379
iteration : 10830
train acc:  0.71875
train loss:  0.5119688510894775
train gradient:  0.12892877797928137
iteration : 10831
train acc:  0.7109375
train loss:  0.560039758682251
train gradient:  0.14359039702176934
iteration : 10832
train acc:  0.765625
train loss:  0.4514644145965576
train gradient:  0.1129000149518393
iteration : 10833
train acc:  0.703125
train loss:  0.5642424821853638
train gradient:  0.15181378308779048
iteration : 10834
train acc:  0.703125
train loss:  0.5752848386764526
train gradient:  0.2054716460777094
iteration : 10835
train acc:  0.734375
train loss:  0.47330135107040405
train gradient:  0.11357438774655755
iteration : 10836
train acc:  0.734375
train loss:  0.5626779794692993
train gradient:  0.15342127517870316
iteration : 10837
train acc:  0.765625
train loss:  0.4780752658843994
train gradient:  0.13483942399561583
iteration : 10838
train acc:  0.75
train loss:  0.48976391553878784
train gradient:  0.1328687039186299
iteration : 10839
train acc:  0.703125
train loss:  0.5214751958847046
train gradient:  0.1369356106627339
iteration : 10840
train acc:  0.71875
train loss:  0.5138182640075684
train gradient:  0.12902197708776258
iteration : 10841
train acc:  0.765625
train loss:  0.503850519657135
train gradient:  0.1498665499429173
iteration : 10842
train acc:  0.71875
train loss:  0.5368043184280396
train gradient:  0.13449769619376226
iteration : 10843
train acc:  0.75
train loss:  0.5072510242462158
train gradient:  0.13964474029475601
iteration : 10844
train acc:  0.7109375
train loss:  0.4814799427986145
train gradient:  0.11081074333083962
iteration : 10845
train acc:  0.8046875
train loss:  0.4223417043685913
train gradient:  0.09390287532417546
iteration : 10846
train acc:  0.7578125
train loss:  0.5148545503616333
train gradient:  0.11468424541423806
iteration : 10847
train acc:  0.734375
train loss:  0.497285932302475
train gradient:  0.1453576904391684
iteration : 10848
train acc:  0.7578125
train loss:  0.4845910966396332
train gradient:  0.1386086484680799
iteration : 10849
train acc:  0.7421875
train loss:  0.5040442943572998
train gradient:  0.15360905169705763
iteration : 10850
train acc:  0.7734375
train loss:  0.4369075298309326
train gradient:  0.08894838361231892
iteration : 10851
train acc:  0.7265625
train loss:  0.5059565901756287
train gradient:  0.12412007412757117
iteration : 10852
train acc:  0.78125
train loss:  0.43215417861938477
train gradient:  0.09148849952923073
iteration : 10853
train acc:  0.78125
train loss:  0.4533844292163849
train gradient:  0.10988519356637122
iteration : 10854
train acc:  0.6953125
train loss:  0.5415602922439575
train gradient:  0.12670611451420793
iteration : 10855
train acc:  0.7265625
train loss:  0.4829496443271637
train gradient:  0.1410934859838097
iteration : 10856
train acc:  0.6953125
train loss:  0.5222525596618652
train gradient:  0.15653380725431892
iteration : 10857
train acc:  0.7109375
train loss:  0.5483474135398865
train gradient:  0.16349581209414293
iteration : 10858
train acc:  0.8125
train loss:  0.41557201743125916
train gradient:  0.08735092728201532
iteration : 10859
train acc:  0.7578125
train loss:  0.491150438785553
train gradient:  0.11001282008896407
iteration : 10860
train acc:  0.734375
train loss:  0.5355659127235413
train gradient:  0.15848584299894075
iteration : 10861
train acc:  0.7109375
train loss:  0.5549823045730591
train gradient:  0.15633872627540657
iteration : 10862
train acc:  0.7890625
train loss:  0.4174674153327942
train gradient:  0.10731245474081809
iteration : 10863
train acc:  0.7265625
train loss:  0.5009670257568359
train gradient:  0.14662109708249627
iteration : 10864
train acc:  0.7265625
train loss:  0.4736020267009735
train gradient:  0.1672433104758978
iteration : 10865
train acc:  0.7578125
train loss:  0.4991587996482849
train gradient:  0.12026194599527017
iteration : 10866
train acc:  0.7109375
train loss:  0.5134252309799194
train gradient:  0.1561601233357443
iteration : 10867
train acc:  0.703125
train loss:  0.495425283908844
train gradient:  0.14004201125882193
iteration : 10868
train acc:  0.7734375
train loss:  0.5389809608459473
train gradient:  0.1916007337732523
iteration : 10869
train acc:  0.7421875
train loss:  0.4686430096626282
train gradient:  0.11512688025803608
iteration : 10870
train acc:  0.75
train loss:  0.4834081828594208
train gradient:  0.13463541474918944
iteration : 10871
train acc:  0.6875
train loss:  0.5171962380409241
train gradient:  0.12157700891510421
iteration : 10872
train acc:  0.75
train loss:  0.4844336211681366
train gradient:  0.12351825750223955
iteration : 10873
train acc:  0.765625
train loss:  0.4638916850090027
train gradient:  0.10802577230924904
iteration : 10874
train acc:  0.7578125
train loss:  0.4882194995880127
train gradient:  0.14111669727483445
iteration : 10875
train acc:  0.6953125
train loss:  0.5504964590072632
train gradient:  0.1463978215148789
iteration : 10876
train acc:  0.7265625
train loss:  0.45902204513549805
train gradient:  0.11592718873841158
iteration : 10877
train acc:  0.78125
train loss:  0.49459129571914673
train gradient:  0.15352049176016036
iteration : 10878
train acc:  0.734375
train loss:  0.47103792428970337
train gradient:  0.10462411803491634
iteration : 10879
train acc:  0.6953125
train loss:  0.5360167622566223
train gradient:  0.18021298377781092
iteration : 10880
train acc:  0.65625
train loss:  0.502816915512085
train gradient:  0.13211450723719634
iteration : 10881
train acc:  0.7265625
train loss:  0.5285711288452148
train gradient:  0.14678322515464465
iteration : 10882
train acc:  0.765625
train loss:  0.5030626058578491
train gradient:  0.18206268128623454
iteration : 10883
train acc:  0.7421875
train loss:  0.47648680210113525
train gradient:  0.12637123224735763
iteration : 10884
train acc:  0.734375
train loss:  0.4989030361175537
train gradient:  0.12400273430999195
iteration : 10885
train acc:  0.75
train loss:  0.5210881233215332
train gradient:  0.1593591970108869
iteration : 10886
train acc:  0.78125
train loss:  0.4309843182563782
train gradient:  0.09383589368665603
iteration : 10887
train acc:  0.765625
train loss:  0.46160951256752014
train gradient:  0.10929889662066042
iteration : 10888
train acc:  0.6953125
train loss:  0.505674421787262
train gradient:  0.1435717723060615
iteration : 10889
train acc:  0.796875
train loss:  0.43987172842025757
train gradient:  0.11267427961708132
iteration : 10890
train acc:  0.6953125
train loss:  0.5717170834541321
train gradient:  0.17961995941988762
iteration : 10891
train acc:  0.7421875
train loss:  0.4587002396583557
train gradient:  0.12086609406660459
iteration : 10892
train acc:  0.796875
train loss:  0.4496168792247772
train gradient:  0.09365587755556018
iteration : 10893
train acc:  0.703125
train loss:  0.5611370801925659
train gradient:  0.16099163743692496
iteration : 10894
train acc:  0.6953125
train loss:  0.5545526742935181
train gradient:  0.16689820404678235
iteration : 10895
train acc:  0.7421875
train loss:  0.5002075433731079
train gradient:  0.12841089872910866
iteration : 10896
train acc:  0.6640625
train loss:  0.5248974561691284
train gradient:  0.14711708419230757
iteration : 10897
train acc:  0.734375
train loss:  0.4959576725959778
train gradient:  0.10791298376280889
iteration : 10898
train acc:  0.7421875
train loss:  0.5536659955978394
train gradient:  0.15374537678943978
iteration : 10899
train acc:  0.75
train loss:  0.5015744566917419
train gradient:  0.13867577993181932
iteration : 10900
train acc:  0.7578125
train loss:  0.516672670841217
train gradient:  0.14903639943089497
iteration : 10901
train acc:  0.6953125
train loss:  0.5264583826065063
train gradient:  0.12411336085786459
iteration : 10902
train acc:  0.765625
train loss:  0.4929366111755371
train gradient:  0.13298512195640777
iteration : 10903
train acc:  0.7421875
train loss:  0.4938977062702179
train gradient:  0.1108210644331119
iteration : 10904
train acc:  0.6875
train loss:  0.6182347536087036
train gradient:  0.18788639075872132
iteration : 10905
train acc:  0.7265625
train loss:  0.4496983289718628
train gradient:  0.09901014206927487
iteration : 10906
train acc:  0.7734375
train loss:  0.4980030059814453
train gradient:  0.14167868434452838
iteration : 10907
train acc:  0.75
train loss:  0.5097764134407043
train gradient:  0.1485108430062367
iteration : 10908
train acc:  0.8046875
train loss:  0.4529171884059906
train gradient:  0.10040167678714375
iteration : 10909
train acc:  0.7109375
train loss:  0.5212825536727905
train gradient:  0.13760926175838756
iteration : 10910
train acc:  0.7578125
train loss:  0.4875914454460144
train gradient:  0.1420115660707988
iteration : 10911
train acc:  0.8125
train loss:  0.46304526925086975
train gradient:  0.09558197764643593
iteration : 10912
train acc:  0.8046875
train loss:  0.4608213007450104
train gradient:  0.10775158355972003
iteration : 10913
train acc:  0.71875
train loss:  0.565915584564209
train gradient:  0.17989854863683527
iteration : 10914
train acc:  0.8046875
train loss:  0.44253090023994446
train gradient:  0.09512103110181593
iteration : 10915
train acc:  0.6875
train loss:  0.6152839660644531
train gradient:  0.1977891853921816
iteration : 10916
train acc:  0.78125
train loss:  0.49446168541908264
train gradient:  0.13584673366682698
iteration : 10917
train acc:  0.7109375
train loss:  0.5237228870391846
train gradient:  0.13989387411165105
iteration : 10918
train acc:  0.71875
train loss:  0.5227488279342651
train gradient:  0.1347720979728231
iteration : 10919
train acc:  0.71875
train loss:  0.5281601548194885
train gradient:  0.1438565868805482
iteration : 10920
train acc:  0.734375
train loss:  0.47150492668151855
train gradient:  0.09920434328817491
iteration : 10921
train acc:  0.734375
train loss:  0.48038187623023987
train gradient:  0.11849787153652613
iteration : 10922
train acc:  0.7421875
train loss:  0.5106073617935181
train gradient:  0.14628450634917237
iteration : 10923
train acc:  0.7421875
train loss:  0.46557697653770447
train gradient:  0.12341148598853012
iteration : 10924
train acc:  0.7265625
train loss:  0.5075293779373169
train gradient:  0.1357464917737934
iteration : 10925
train acc:  0.765625
train loss:  0.4746980369091034
train gradient:  0.12902084552803186
iteration : 10926
train acc:  0.7578125
train loss:  0.4512544572353363
train gradient:  0.12561455883317335
iteration : 10927
train acc:  0.765625
train loss:  0.46252939105033875
train gradient:  0.12465730915443674
iteration : 10928
train acc:  0.7578125
train loss:  0.4523478150367737
train gradient:  0.1160933994920428
iteration : 10929
train acc:  0.7421875
train loss:  0.45566004514694214
train gradient:  0.1158245146843026
iteration : 10930
train acc:  0.7578125
train loss:  0.45119503140449524
train gradient:  0.12923420614524317
iteration : 10931
train acc:  0.75
train loss:  0.501753568649292
train gradient:  0.16109366619030963
iteration : 10932
train acc:  0.796875
train loss:  0.41688331961631775
train gradient:  0.08332502570673916
iteration : 10933
train acc:  0.7109375
train loss:  0.5267246961593628
train gradient:  0.13389776677060178
iteration : 10934
train acc:  0.71875
train loss:  0.5165475606918335
train gradient:  0.1220333672596215
iteration : 10935
train acc:  0.7421875
train loss:  0.5379824042320251
train gradient:  0.15743555896677697
iteration : 10936
train acc:  0.7578125
train loss:  0.50645911693573
train gradient:  0.1083883428490841
iteration : 10937
train acc:  0.8203125
train loss:  0.4832231402397156
train gradient:  0.12502965946706018
iteration : 10938
train acc:  0.7109375
train loss:  0.48764270544052124
train gradient:  0.12181434205027461
iteration : 10939
train acc:  0.734375
train loss:  0.527540922164917
train gradient:  0.1295074697222287
iteration : 10940
train acc:  0.734375
train loss:  0.508240282535553
train gradient:  0.12063716668906942
iteration : 10941
train acc:  0.7109375
train loss:  0.5568459033966064
train gradient:  0.16297087080823403
iteration : 10942
train acc:  0.6796875
train loss:  0.4925544857978821
train gradient:  0.11507452313861556
iteration : 10943
train acc:  0.796875
train loss:  0.4301512837409973
train gradient:  0.09076378132752971
iteration : 10944
train acc:  0.6875
train loss:  0.4886298179626465
train gradient:  0.1150637139729252
iteration : 10945
train acc:  0.7265625
train loss:  0.48279649019241333
train gradient:  0.10230934694216377
iteration : 10946
train acc:  0.71875
train loss:  0.5536034107208252
train gradient:  0.18663036499075675
iteration : 10947
train acc:  0.734375
train loss:  0.5139932632446289
train gradient:  0.13417909491104107
iteration : 10948
train acc:  0.7265625
train loss:  0.5615822076797485
train gradient:  0.18929633019497785
iteration : 10949
train acc:  0.7109375
train loss:  0.5154399871826172
train gradient:  0.12083192480339397
iteration : 10950
train acc:  0.703125
train loss:  0.4988524317741394
train gradient:  0.1579514486071728
iteration : 10951
train acc:  0.7421875
train loss:  0.4837367534637451
train gradient:  0.11074350484389431
iteration : 10952
train acc:  0.7265625
train loss:  0.47550469636917114
train gradient:  0.10120785620630648
iteration : 10953
train acc:  0.71875
train loss:  0.47975724935531616
train gradient:  0.12590141109013553
iteration : 10954
train acc:  0.734375
train loss:  0.5011062622070312
train gradient:  0.16316111457947524
iteration : 10955
train acc:  0.7734375
train loss:  0.481179416179657
train gradient:  0.1186313432681317
iteration : 10956
train acc:  0.8046875
train loss:  0.44793900847435
train gradient:  0.10399584209554324
iteration : 10957
train acc:  0.7421875
train loss:  0.5142397880554199
train gradient:  0.14972121522174786
iteration : 10958
train acc:  0.6875
train loss:  0.543121874332428
train gradient:  0.12716715681762047
iteration : 10959
train acc:  0.671875
train loss:  0.5370209813117981
train gradient:  0.1311434318495332
iteration : 10960
train acc:  0.8125
train loss:  0.4256347417831421
train gradient:  0.08963512863541008
iteration : 10961
train acc:  0.78125
train loss:  0.4470566511154175
train gradient:  0.09705398591652134
iteration : 10962
train acc:  0.7578125
train loss:  0.4961116313934326
train gradient:  0.12229669578677747
iteration : 10963
train acc:  0.6796875
train loss:  0.5297577381134033
train gradient:  0.11596927994705294
iteration : 10964
train acc:  0.71875
train loss:  0.5518125295639038
train gradient:  0.13477440694180512
iteration : 10965
train acc:  0.734375
train loss:  0.5193162560462952
train gradient:  0.14352003426696164
iteration : 10966
train acc:  0.78125
train loss:  0.4392451047897339
train gradient:  0.0876297438093893
iteration : 10967
train acc:  0.75
train loss:  0.531403124332428
train gradient:  0.15232571951706542
iteration : 10968
train acc:  0.7421875
train loss:  0.5165827870368958
train gradient:  0.14362020524901797
iteration : 10969
train acc:  0.6796875
train loss:  0.5472526550292969
train gradient:  0.14374419581947068
iteration : 10970
train acc:  0.734375
train loss:  0.4267381429672241
train gradient:  0.1042967534864674
iteration : 10971
train acc:  0.75
train loss:  0.49221569299697876
train gradient:  0.11737739517835867
iteration : 10972
train acc:  0.7421875
train loss:  0.5146946907043457
train gradient:  0.15796456846264037
iteration : 10973
train acc:  0.734375
train loss:  0.4251394271850586
train gradient:  0.10361022759561599
iteration : 10974
train acc:  0.671875
train loss:  0.5520867109298706
train gradient:  0.15646590483691009
iteration : 10975
train acc:  0.7734375
train loss:  0.512279748916626
train gradient:  0.10874749544810247
iteration : 10976
train acc:  0.734375
train loss:  0.4961698651313782
train gradient:  0.13133553889955418
iteration : 10977
train acc:  0.6875
train loss:  0.5968843102455139
train gradient:  0.17350930166788914
iteration : 10978
train acc:  0.7109375
train loss:  0.5343960523605347
train gradient:  0.20500391071056248
iteration : 10979
train acc:  0.71875
train loss:  0.4821925759315491
train gradient:  0.1153474257242686
iteration : 10980
train acc:  0.671875
train loss:  0.5239157676696777
train gradient:  0.12840411845679917
iteration : 10981
train acc:  0.7578125
train loss:  0.4789973199367523
train gradient:  0.10727322955113179
iteration : 10982
train acc:  0.734375
train loss:  0.5680599212646484
train gradient:  0.1912423808406812
iteration : 10983
train acc:  0.6796875
train loss:  0.5079306960105896
train gradient:  0.1338628337268515
iteration : 10984
train acc:  0.7578125
train loss:  0.4545794725418091
train gradient:  0.1323573059588915
iteration : 10985
train acc:  0.7734375
train loss:  0.42846786975860596
train gradient:  0.10525578419634721
iteration : 10986
train acc:  0.671875
train loss:  0.5623728632926941
train gradient:  0.17158088312381026
iteration : 10987
train acc:  0.6640625
train loss:  0.5130279064178467
train gradient:  0.1654393193459583
iteration : 10988
train acc:  0.7421875
train loss:  0.4564686119556427
train gradient:  0.09019780785468089
iteration : 10989
train acc:  0.796875
train loss:  0.4384413957595825
train gradient:  0.11843358700334056
iteration : 10990
train acc:  0.65625
train loss:  0.5802369117736816
train gradient:  0.14316512578859747
iteration : 10991
train acc:  0.7734375
train loss:  0.44072720408439636
train gradient:  0.10297911992225514
iteration : 10992
train acc:  0.7265625
train loss:  0.5118857622146606
train gradient:  0.13063823839592592
iteration : 10993
train acc:  0.7734375
train loss:  0.48784130811691284
train gradient:  0.12970089310318345
iteration : 10994
train acc:  0.71875
train loss:  0.5090262293815613
train gradient:  0.1152516302408494
iteration : 10995
train acc:  0.796875
train loss:  0.43853771686553955
train gradient:  0.10240985793431776
iteration : 10996
train acc:  0.7578125
train loss:  0.46024513244628906
train gradient:  0.12177658845669193
iteration : 10997
train acc:  0.8203125
train loss:  0.44285494089126587
train gradient:  0.09016779255094402
iteration : 10998
train acc:  0.7265625
train loss:  0.48374149203300476
train gradient:  0.09514579114311607
iteration : 10999
train acc:  0.7578125
train loss:  0.4868444800376892
train gradient:  0.10247934282713891
iteration : 11000
train acc:  0.78125
train loss:  0.4626503586769104
train gradient:  0.16664145807922226
iteration : 11001
train acc:  0.7734375
train loss:  0.49339157342910767
train gradient:  0.12709345059557267
iteration : 11002
train acc:  0.75
train loss:  0.476444810628891
train gradient:  0.14202313940371508
iteration : 11003
train acc:  0.671875
train loss:  0.5069361925125122
train gradient:  0.11799422238931469
iteration : 11004
train acc:  0.734375
train loss:  0.510493278503418
train gradient:  0.11814225142105016
iteration : 11005
train acc:  0.7109375
train loss:  0.4534074366092682
train gradient:  0.12040986933893807
iteration : 11006
train acc:  0.6953125
train loss:  0.5176807045936584
train gradient:  0.12617530660173415
iteration : 11007
train acc:  0.6484375
train loss:  0.558150053024292
train gradient:  0.1324583440759558
iteration : 11008
train acc:  0.703125
train loss:  0.5455756783485413
train gradient:  0.14296099939218704
iteration : 11009
train acc:  0.7265625
train loss:  0.5505843162536621
train gradient:  0.15607311137670143
iteration : 11010
train acc:  0.703125
train loss:  0.4994257390499115
train gradient:  0.12271354106619689
iteration : 11011
train acc:  0.78125
train loss:  0.5149010419845581
train gradient:  0.14237417006503789
iteration : 11012
train acc:  0.7109375
train loss:  0.5616178512573242
train gradient:  0.17660276225972202
iteration : 11013
train acc:  0.7890625
train loss:  0.44921863079071045
train gradient:  0.09884106284338001
iteration : 11014
train acc:  0.78125
train loss:  0.46428462862968445
train gradient:  0.10669043691879544
iteration : 11015
train acc:  0.7421875
train loss:  0.5185519456863403
train gradient:  0.13915460443338823
iteration : 11016
train acc:  0.6953125
train loss:  0.5240287780761719
train gradient:  0.17411018856170424
iteration : 11017
train acc:  0.7265625
train loss:  0.48444291949272156
train gradient:  0.10528653241265488
iteration : 11018
train acc:  0.6796875
train loss:  0.5566854476928711
train gradient:  0.22723751229560288
iteration : 11019
train acc:  0.75
train loss:  0.4929346740245819
train gradient:  0.12785312477893057
iteration : 11020
train acc:  0.6953125
train loss:  0.4516039490699768
train gradient:  0.10301518140514732
iteration : 11021
train acc:  0.671875
train loss:  0.5670764446258545
train gradient:  0.15052494945238343
iteration : 11022
train acc:  0.7421875
train loss:  0.4979991316795349
train gradient:  0.1389974600698968
iteration : 11023
train acc:  0.6796875
train loss:  0.5347504615783691
train gradient:  0.16200304674270355
iteration : 11024
train acc:  0.734375
train loss:  0.494617760181427
train gradient:  0.13771232012420842
iteration : 11025
train acc:  0.7578125
train loss:  0.4697250425815582
train gradient:  0.16131200910660118
iteration : 11026
train acc:  0.7734375
train loss:  0.4385879635810852
train gradient:  0.1130097288750758
iteration : 11027
train acc:  0.703125
train loss:  0.5494983792304993
train gradient:  0.16483326581291718
iteration : 11028
train acc:  0.75
train loss:  0.454825222492218
train gradient:  0.11645777301737571
iteration : 11029
train acc:  0.7890625
train loss:  0.44413989782333374
train gradient:  0.10010881295473804
iteration : 11030
train acc:  0.7265625
train loss:  0.5232661366462708
train gradient:  0.17777779103867264
iteration : 11031
train acc:  0.7109375
train loss:  0.5256631374359131
train gradient:  0.13316835972267071
iteration : 11032
train acc:  0.7734375
train loss:  0.45222678780555725
train gradient:  0.12473725402528581
iteration : 11033
train acc:  0.7734375
train loss:  0.44430088996887207
train gradient:  0.10842033945818734
iteration : 11034
train acc:  0.703125
train loss:  0.5387683510780334
train gradient:  0.19909210162749186
iteration : 11035
train acc:  0.796875
train loss:  0.42416101694107056
train gradient:  0.10195265291176527
iteration : 11036
train acc:  0.8046875
train loss:  0.43447762727737427
train gradient:  0.11942867992255841
iteration : 11037
train acc:  0.671875
train loss:  0.5907840728759766
train gradient:  0.16364705191265178
iteration : 11038
train acc:  0.7734375
train loss:  0.422959566116333
train gradient:  0.10093162850587192
iteration : 11039
train acc:  0.765625
train loss:  0.4951700270175934
train gradient:  0.11780077715654776
iteration : 11040
train acc:  0.7734375
train loss:  0.464893639087677
train gradient:  0.09169514280280067
iteration : 11041
train acc:  0.765625
train loss:  0.49786287546157837
train gradient:  0.14078535520682423
iteration : 11042
train acc:  0.7109375
train loss:  0.5621082782745361
train gradient:  0.1588831763151664
iteration : 11043
train acc:  0.734375
train loss:  0.48067665100097656
train gradient:  0.12179705580899504
iteration : 11044
train acc:  0.78125
train loss:  0.4277224540710449
train gradient:  0.0817987781405004
iteration : 11045
train acc:  0.6953125
train loss:  0.5862250328063965
train gradient:  0.17163565225359426
iteration : 11046
train acc:  0.703125
train loss:  0.4841376543045044
train gradient:  0.09977983895830023
iteration : 11047
train acc:  0.75
train loss:  0.5045095086097717
train gradient:  0.14714030764631122
iteration : 11048
train acc:  0.796875
train loss:  0.46715375781059265
train gradient:  0.11221289150063457
iteration : 11049
train acc:  0.71875
train loss:  0.4856976270675659
train gradient:  0.11505644111922719
iteration : 11050
train acc:  0.7890625
train loss:  0.48124468326568604
train gradient:  0.11148583265863964
iteration : 11051
train acc:  0.8203125
train loss:  0.41269057989120483
train gradient:  0.08701490932465875
iteration : 11052
train acc:  0.7109375
train loss:  0.547701895236969
train gradient:  0.16713039555654133
iteration : 11053
train acc:  0.7109375
train loss:  0.48695218563079834
train gradient:  0.10835900440137038
iteration : 11054
train acc:  0.734375
train loss:  0.4876560568809509
train gradient:  0.1730234421156352
iteration : 11055
train acc:  0.765625
train loss:  0.4604465067386627
train gradient:  0.13793628790475415
iteration : 11056
train acc:  0.75
train loss:  0.43283551931381226
train gradient:  0.08560016301642186
iteration : 11057
train acc:  0.7890625
train loss:  0.4914856553077698
train gradient:  0.13203728127648645
iteration : 11058
train acc:  0.6953125
train loss:  0.5372567176818848
train gradient:  0.14504118566047203
iteration : 11059
train acc:  0.6875
train loss:  0.540503203868866
train gradient:  0.15048078548481092
iteration : 11060
train acc:  0.7578125
train loss:  0.45769643783569336
train gradient:  0.10952162209689383
iteration : 11061
train acc:  0.7421875
train loss:  0.485079824924469
train gradient:  0.1017711098756125
iteration : 11062
train acc:  0.765625
train loss:  0.4591487646102905
train gradient:  0.12405412809632212
iteration : 11063
train acc:  0.71875
train loss:  0.5349655151367188
train gradient:  0.1769047896550336
iteration : 11064
train acc:  0.734375
train loss:  0.47948119044303894
train gradient:  0.11990694482940835
iteration : 11065
train acc:  0.765625
train loss:  0.4309918284416199
train gradient:  0.09772871034362135
iteration : 11066
train acc:  0.7734375
train loss:  0.4386617839336395
train gradient:  0.11468567725435018
iteration : 11067
train acc:  0.7734375
train loss:  0.4414759874343872
train gradient:  0.09958659564902121
iteration : 11068
train acc:  0.7265625
train loss:  0.4826841354370117
train gradient:  0.11408372490096712
iteration : 11069
train acc:  0.7421875
train loss:  0.47500506043434143
train gradient:  0.11366725675990115
iteration : 11070
train acc:  0.8203125
train loss:  0.40163683891296387
train gradient:  0.08735093900965996
iteration : 11071
train acc:  0.7734375
train loss:  0.4618960916996002
train gradient:  0.11928678898887964
iteration : 11072
train acc:  0.7578125
train loss:  0.4429786801338196
train gradient:  0.10396785787301553
iteration : 11073
train acc:  0.703125
train loss:  0.47524523735046387
train gradient:  0.10617257895739368
iteration : 11074
train acc:  0.7421875
train loss:  0.47473880648612976
train gradient:  0.11541755184733048
iteration : 11075
train acc:  0.7109375
train loss:  0.5177320241928101
train gradient:  0.15000511815045842
iteration : 11076
train acc:  0.7421875
train loss:  0.4706368148326874
train gradient:  0.12663132708685892
iteration : 11077
train acc:  0.7578125
train loss:  0.49657684564590454
train gradient:  0.10889290615765686
iteration : 11078
train acc:  0.75
train loss:  0.4737245440483093
train gradient:  0.13902161195286417
iteration : 11079
train acc:  0.671875
train loss:  0.5525802373886108
train gradient:  0.14323853941065118
iteration : 11080
train acc:  0.7421875
train loss:  0.450290322303772
train gradient:  0.10482956266999528
iteration : 11081
train acc:  0.734375
train loss:  0.5410625338554382
train gradient:  0.1717345850835525
iteration : 11082
train acc:  0.71875
train loss:  0.5088646411895752
train gradient:  0.12328041912278323
iteration : 11083
train acc:  0.765625
train loss:  0.49963122606277466
train gradient:  0.11008524174946692
iteration : 11084
train acc:  0.71875
train loss:  0.5525881052017212
train gradient:  0.15731848816610844
iteration : 11085
train acc:  0.703125
train loss:  0.5143790245056152
train gradient:  0.14638278975738117
iteration : 11086
train acc:  0.78125
train loss:  0.45217180252075195
train gradient:  0.10736159906564889
iteration : 11087
train acc:  0.703125
train loss:  0.52793288230896
train gradient:  0.14255173891002532
iteration : 11088
train acc:  0.7734375
train loss:  0.4380038380622864
train gradient:  0.09673574163281216
iteration : 11089
train acc:  0.71875
train loss:  0.5460022687911987
train gradient:  0.1383411494449444
iteration : 11090
train acc:  0.6953125
train loss:  0.5584080815315247
train gradient:  0.1824770843971673
iteration : 11091
train acc:  0.7109375
train loss:  0.5321425199508667
train gradient:  0.17656027692529638
iteration : 11092
train acc:  0.75
train loss:  0.481658399105072
train gradient:  0.11651985256804535
iteration : 11093
train acc:  0.828125
train loss:  0.41021424531936646
train gradient:  0.09913884317646254
iteration : 11094
train acc:  0.78125
train loss:  0.441679447889328
train gradient:  0.1075345070831626
iteration : 11095
train acc:  0.71875
train loss:  0.4930832087993622
train gradient:  0.12027334387614326
iteration : 11096
train acc:  0.734375
train loss:  0.501279354095459
train gradient:  0.1256769469693078
iteration : 11097
train acc:  0.7578125
train loss:  0.4917486011981964
train gradient:  0.10471947908777861
iteration : 11098
train acc:  0.859375
train loss:  0.3864245116710663
train gradient:  0.09667841065532828
iteration : 11099
train acc:  0.71875
train loss:  0.5111404657363892
train gradient:  0.13903271770809267
iteration : 11100
train acc:  0.734375
train loss:  0.5201928615570068
train gradient:  0.12363629556447968
iteration : 11101
train acc:  0.71875
train loss:  0.4998597502708435
train gradient:  0.15367892265503796
iteration : 11102
train acc:  0.7265625
train loss:  0.5324690341949463
train gradient:  0.1413951461553345
iteration : 11103
train acc:  0.78125
train loss:  0.4364769458770752
train gradient:  0.1365994326049807
iteration : 11104
train acc:  0.78125
train loss:  0.4873966574668884
train gradient:  0.1320552544356826
iteration : 11105
train acc:  0.7421875
train loss:  0.4851192533969879
train gradient:  0.1115204924267993
iteration : 11106
train acc:  0.7421875
train loss:  0.4960858225822449
train gradient:  0.14904694594783238
iteration : 11107
train acc:  0.6640625
train loss:  0.5911887884140015
train gradient:  0.18377159076658378
iteration : 11108
train acc:  0.7109375
train loss:  0.49091434478759766
train gradient:  0.14482475561420124
iteration : 11109
train acc:  0.7890625
train loss:  0.4414747655391693
train gradient:  0.10542872227252352
iteration : 11110
train acc:  0.71875
train loss:  0.4976164996623993
train gradient:  0.1430442180190904
iteration : 11111
train acc:  0.734375
train loss:  0.5471376180648804
train gradient:  0.12912900743231562
iteration : 11112
train acc:  0.7734375
train loss:  0.46635696291923523
train gradient:  0.1374419519507601
iteration : 11113
train acc:  0.796875
train loss:  0.4384409785270691
train gradient:  0.10008540859306791
iteration : 11114
train acc:  0.7578125
train loss:  0.458590030670166
train gradient:  0.11624030844728071
iteration : 11115
train acc:  0.6953125
train loss:  0.5428838729858398
train gradient:  0.12653121092266292
iteration : 11116
train acc:  0.7734375
train loss:  0.4979906976222992
train gradient:  0.15997053349204648
iteration : 11117
train acc:  0.703125
train loss:  0.5394454002380371
train gradient:  0.1760996724975537
iteration : 11118
train acc:  0.828125
train loss:  0.4369042217731476
train gradient:  0.11859701612550239
iteration : 11119
train acc:  0.78125
train loss:  0.4728475511074066
train gradient:  0.09903459381209266
iteration : 11120
train acc:  0.7734375
train loss:  0.4661892056465149
train gradient:  0.127812263719298
iteration : 11121
train acc:  0.7578125
train loss:  0.4562596082687378
train gradient:  0.1176318211857928
iteration : 11122
train acc:  0.7734375
train loss:  0.4596211016178131
train gradient:  0.12470998903296769
iteration : 11123
train acc:  0.7578125
train loss:  0.48740237951278687
train gradient:  0.12947202093150165
iteration : 11124
train acc:  0.7265625
train loss:  0.4809569716453552
train gradient:  0.10625404458919947
iteration : 11125
train acc:  0.8203125
train loss:  0.38381248712539673
train gradient:  0.08932195602770686
iteration : 11126
train acc:  0.75
train loss:  0.46018314361572266
train gradient:  0.10240927115789653
iteration : 11127
train acc:  0.7109375
train loss:  0.5247190594673157
train gradient:  0.1491313799201618
iteration : 11128
train acc:  0.78125
train loss:  0.4438176453113556
train gradient:  0.1297789120943288
iteration : 11129
train acc:  0.7265625
train loss:  0.5513514280319214
train gradient:  0.13734932054701127
iteration : 11130
train acc:  0.78125
train loss:  0.4434262216091156
train gradient:  0.10073462442677356
iteration : 11131
train acc:  0.7890625
train loss:  0.44920814037323
train gradient:  0.11912485819877255
iteration : 11132
train acc:  0.78125
train loss:  0.44934651255607605
train gradient:  0.1018217339111839
iteration : 11133
train acc:  0.78125
train loss:  0.47196823358535767
train gradient:  0.10586624425361423
iteration : 11134
train acc:  0.7421875
train loss:  0.4935877323150635
train gradient:  0.15688441097551586
iteration : 11135
train acc:  0.765625
train loss:  0.4933943748474121
train gradient:  0.12168620529251452
iteration : 11136
train acc:  0.7265625
train loss:  0.46353569626808167
train gradient:  0.1353147612220086
iteration : 11137
train acc:  0.765625
train loss:  0.47792524099349976
train gradient:  0.1090368438122046
iteration : 11138
train acc:  0.7109375
train loss:  0.5012874603271484
train gradient:  0.1505869531581571
iteration : 11139
train acc:  0.7578125
train loss:  0.5106556415557861
train gradient:  0.12498266663962043
iteration : 11140
train acc:  0.6875
train loss:  0.5270327925682068
train gradient:  0.2062864562285271
iteration : 11141
train acc:  0.6796875
train loss:  0.6055252552032471
train gradient:  0.22685981389542004
iteration : 11142
train acc:  0.6796875
train loss:  0.5362430810928345
train gradient:  0.12662355682914328
iteration : 11143
train acc:  0.703125
train loss:  0.5484327077865601
train gradient:  0.1311502661068524
iteration : 11144
train acc:  0.71875
train loss:  0.5338412523269653
train gradient:  0.1465952167520057
iteration : 11145
train acc:  0.765625
train loss:  0.5271668434143066
train gradient:  0.16389200039978274
iteration : 11146
train acc:  0.765625
train loss:  0.4918433427810669
train gradient:  0.12522490695962338
iteration : 11147
train acc:  0.78125
train loss:  0.44521987438201904
train gradient:  0.18238649310091992
iteration : 11148
train acc:  0.703125
train loss:  0.5343185067176819
train gradient:  0.12725699486718983
iteration : 11149
train acc:  0.7265625
train loss:  0.511111319065094
train gradient:  0.14838484738281327
iteration : 11150
train acc:  0.796875
train loss:  0.48123419284820557
train gradient:  0.11109523823249375
iteration : 11151
train acc:  0.7109375
train loss:  0.5499923229217529
train gradient:  0.1526632146050742
iteration : 11152
train acc:  0.78125
train loss:  0.466525137424469
train gradient:  0.11413301579415648
iteration : 11153
train acc:  0.7734375
train loss:  0.49554282426834106
train gradient:  0.14962925460095955
iteration : 11154
train acc:  0.6953125
train loss:  0.5386418104171753
train gradient:  0.1997171457821742
iteration : 11155
train acc:  0.75
train loss:  0.5150901079177856
train gradient:  0.12698944812581403
iteration : 11156
train acc:  0.6953125
train loss:  0.5386661887168884
train gradient:  0.14567203168465284
iteration : 11157
train acc:  0.7265625
train loss:  0.4834775924682617
train gradient:  0.11716959050034687
iteration : 11158
train acc:  0.765625
train loss:  0.48734986782073975
train gradient:  0.14126605921143962
iteration : 11159
train acc:  0.7734375
train loss:  0.45603904128074646
train gradient:  0.11286302040023477
iteration : 11160
train acc:  0.796875
train loss:  0.47680217027664185
train gradient:  0.10545612135310549
iteration : 11161
train acc:  0.7421875
train loss:  0.4809378981590271
train gradient:  0.11350577936246421
iteration : 11162
train acc:  0.765625
train loss:  0.4869769215583801
train gradient:  0.1119134235355574
iteration : 11163
train acc:  0.7109375
train loss:  0.47478926181793213
train gradient:  0.10522143074751684
iteration : 11164
train acc:  0.703125
train loss:  0.5190045833587646
train gradient:  0.139526001483807
iteration : 11165
train acc:  0.65625
train loss:  0.6340031623840332
train gradient:  0.1841120941326707
iteration : 11166
train acc:  0.6953125
train loss:  0.5187711715698242
train gradient:  0.1343401873590273
iteration : 11167
train acc:  0.7265625
train loss:  0.5654842853546143
train gradient:  0.16311270667120192
iteration : 11168
train acc:  0.7265625
train loss:  0.5116022825241089
train gradient:  0.12343880328540757
iteration : 11169
train acc:  0.796875
train loss:  0.4392543435096741
train gradient:  0.09697187367274555
iteration : 11170
train acc:  0.78125
train loss:  0.42430031299591064
train gradient:  0.09346312020505418
iteration : 11171
train acc:  0.6953125
train loss:  0.49150484800338745
train gradient:  0.09936080902552497
iteration : 11172
train acc:  0.7890625
train loss:  0.4561024010181427
train gradient:  0.10761224959293149
iteration : 11173
train acc:  0.765625
train loss:  0.4456617832183838
train gradient:  0.11387600883671102
iteration : 11174
train acc:  0.6640625
train loss:  0.506362795829773
train gradient:  0.1078779342587171
iteration : 11175
train acc:  0.703125
train loss:  0.5199902653694153
train gradient:  0.12744545316761907
iteration : 11176
train acc:  0.71875
train loss:  0.5434006452560425
train gradient:  0.17724492059672958
iteration : 11177
train acc:  0.7265625
train loss:  0.5212606191635132
train gradient:  0.15715552357515783
iteration : 11178
train acc:  0.78125
train loss:  0.5173038244247437
train gradient:  0.17362975913177642
iteration : 11179
train acc:  0.734375
train loss:  0.48695698380470276
train gradient:  0.15882770077599034
iteration : 11180
train acc:  0.71875
train loss:  0.45858490467071533
train gradient:  0.12123058927136092
iteration : 11181
train acc:  0.765625
train loss:  0.45373690128326416
train gradient:  0.1098570095315025
iteration : 11182
train acc:  0.75
train loss:  0.48412182927131653
train gradient:  0.11399065866837452
iteration : 11183
train acc:  0.7578125
train loss:  0.46832728385925293
train gradient:  0.11470439331307113
iteration : 11184
train acc:  0.734375
train loss:  0.482576847076416
train gradient:  0.12553820505042354
iteration : 11185
train acc:  0.7734375
train loss:  0.456787645816803
train gradient:  0.09267227210849352
iteration : 11186
train acc:  0.734375
train loss:  0.46599629521369934
train gradient:  0.11698319650492973
iteration : 11187
train acc:  0.7265625
train loss:  0.48679396510124207
train gradient:  0.11336449877173092
iteration : 11188
train acc:  0.8046875
train loss:  0.4827200174331665
train gradient:  0.14008005394134745
iteration : 11189
train acc:  0.7109375
train loss:  0.5270291566848755
train gradient:  0.12588824769833357
iteration : 11190
train acc:  0.8046875
train loss:  0.46319082379341125
train gradient:  0.11348319104169981
iteration : 11191
train acc:  0.765625
train loss:  0.4437233805656433
train gradient:  0.1160167998066567
iteration : 11192
train acc:  0.703125
train loss:  0.4835958480834961
train gradient:  0.13673753826174428
iteration : 11193
train acc:  0.734375
train loss:  0.5054993629455566
train gradient:  0.14016553387464692
iteration : 11194
train acc:  0.75
train loss:  0.4781201481819153
train gradient:  0.12375508997105876
iteration : 11195
train acc:  0.7734375
train loss:  0.4128187894821167
train gradient:  0.1180246474019799
iteration : 11196
train acc:  0.7421875
train loss:  0.48395073413848877
train gradient:  0.17072432532125253
iteration : 11197
train acc:  0.78125
train loss:  0.4627692699432373
train gradient:  0.13542067111027722
iteration : 11198
train acc:  0.734375
train loss:  0.5075736045837402
train gradient:  0.12848950337519002
iteration : 11199
train acc:  0.75
train loss:  0.49093887209892273
train gradient:  0.125338002475758
iteration : 11200
train acc:  0.828125
train loss:  0.4790601134300232
train gradient:  0.13732917611669726
iteration : 11201
train acc:  0.71875
train loss:  0.47717714309692383
train gradient:  0.12076568697890532
iteration : 11202
train acc:  0.7578125
train loss:  0.46984511613845825
train gradient:  0.11856014060976561
iteration : 11203
train acc:  0.7109375
train loss:  0.5466500520706177
train gradient:  0.12359120194498024
iteration : 11204
train acc:  0.8359375
train loss:  0.4160992503166199
train gradient:  0.11580275214466211
iteration : 11205
train acc:  0.7578125
train loss:  0.459030419588089
train gradient:  0.09595991402884044
iteration : 11206
train acc:  0.78125
train loss:  0.46366268396377563
train gradient:  0.1194297352078381
iteration : 11207
train acc:  0.6953125
train loss:  0.49507811665534973
train gradient:  0.10577346146756919
iteration : 11208
train acc:  0.7578125
train loss:  0.4701411724090576
train gradient:  0.12362784934073585
iteration : 11209
train acc:  0.7421875
train loss:  0.49519896507263184
train gradient:  0.11207232745490375
iteration : 11210
train acc:  0.703125
train loss:  0.48704397678375244
train gradient:  0.12333880174233568
iteration : 11211
train acc:  0.7578125
train loss:  0.44721466302871704
train gradient:  0.09023365444084368
iteration : 11212
train acc:  0.7109375
train loss:  0.5355798006057739
train gradient:  0.1371706911048955
iteration : 11213
train acc:  0.71875
train loss:  0.5180363655090332
train gradient:  0.1447336097847072
iteration : 11214
train acc:  0.71875
train loss:  0.5201687812805176
train gradient:  0.1410635113033591
iteration : 11215
train acc:  0.7734375
train loss:  0.46072709560394287
train gradient:  0.12141411728894741
iteration : 11216
train acc:  0.7578125
train loss:  0.45786264538764954
train gradient:  0.11134690209820768
iteration : 11217
train acc:  0.75
train loss:  0.44711315631866455
train gradient:  0.09490559807594819
iteration : 11218
train acc:  0.6875
train loss:  0.5027759075164795
train gradient:  0.13494794037880375
iteration : 11219
train acc:  0.7265625
train loss:  0.5166939496994019
train gradient:  0.11512498078364411
iteration : 11220
train acc:  0.71875
train loss:  0.5352115631103516
train gradient:  0.12940227288149642
iteration : 11221
train acc:  0.7421875
train loss:  0.49974071979522705
train gradient:  0.15029667999747537
iteration : 11222
train acc:  0.6875
train loss:  0.4960857033729553
train gradient:  0.14483118119119048
iteration : 11223
train acc:  0.703125
train loss:  0.4932391047477722
train gradient:  0.1180648506409659
iteration : 11224
train acc:  0.734375
train loss:  0.4916466772556305
train gradient:  0.10954508669268173
iteration : 11225
train acc:  0.6875
train loss:  0.5262921452522278
train gradient:  0.13965130999437228
iteration : 11226
train acc:  0.7421875
train loss:  0.5530221462249756
train gradient:  0.1705467806031381
iteration : 11227
train acc:  0.734375
train loss:  0.5084705352783203
train gradient:  0.1557886546481111
iteration : 11228
train acc:  0.6875
train loss:  0.5858743190765381
train gradient:  0.17346692492771293
iteration : 11229
train acc:  0.78125
train loss:  0.4592369794845581
train gradient:  0.11000791746260312
iteration : 11230
train acc:  0.6796875
train loss:  0.49914130568504333
train gradient:  0.1214134423053822
iteration : 11231
train acc:  0.8203125
train loss:  0.3770861029624939
train gradient:  0.07224215732608019
iteration : 11232
train acc:  0.734375
train loss:  0.4792083501815796
train gradient:  0.13886785210833114
iteration : 11233
train acc:  0.7421875
train loss:  0.5264914631843567
train gradient:  0.16671869949435425
iteration : 11234
train acc:  0.765625
train loss:  0.4357300400733948
train gradient:  0.07313938567774006
iteration : 11235
train acc:  0.7421875
train loss:  0.4682155251502991
train gradient:  0.1320902726166793
iteration : 11236
train acc:  0.7734375
train loss:  0.45872753858566284
train gradient:  0.1229124010155878
iteration : 11237
train acc:  0.703125
train loss:  0.6203257441520691
train gradient:  0.18910099235066452
iteration : 11238
train acc:  0.7421875
train loss:  0.5090699195861816
train gradient:  0.13151528894103004
iteration : 11239
train acc:  0.765625
train loss:  0.5000000596046448
train gradient:  0.14860685997827158
iteration : 11240
train acc:  0.6875
train loss:  0.5880519151687622
train gradient:  0.15748277639936215
iteration : 11241
train acc:  0.7578125
train loss:  0.4838789403438568
train gradient:  0.1268447228611402
iteration : 11242
train acc:  0.7421875
train loss:  0.4491389989852905
train gradient:  0.10976329054510958
iteration : 11243
train acc:  0.7578125
train loss:  0.4740338623523712
train gradient:  0.14954778440645675
iteration : 11244
train acc:  0.7265625
train loss:  0.4710511565208435
train gradient:  0.11127965063459541
iteration : 11245
train acc:  0.7421875
train loss:  0.5194605588912964
train gradient:  0.13972482691015037
iteration : 11246
train acc:  0.734375
train loss:  0.46706950664520264
train gradient:  0.08669591825928587
iteration : 11247
train acc:  0.75
train loss:  0.48921188712120056
train gradient:  0.13941787226985874
iteration : 11248
train acc:  0.7265625
train loss:  0.4807437062263489
train gradient:  0.11527094898970003
iteration : 11249
train acc:  0.75
train loss:  0.46951210498809814
train gradient:  0.10678057701723681
iteration : 11250
train acc:  0.71875
train loss:  0.5029420852661133
train gradient:  0.1168766344965211
iteration : 11251
train acc:  0.765625
train loss:  0.5070817470550537
train gradient:  0.1306324542999428
iteration : 11252
train acc:  0.7265625
train loss:  0.49235087633132935
train gradient:  0.13622586683769103
iteration : 11253
train acc:  0.7109375
train loss:  0.5066850185394287
train gradient:  0.1466306939257395
iteration : 11254
train acc:  0.8203125
train loss:  0.42985379695892334
train gradient:  0.10707526582089046
iteration : 11255
train acc:  0.6953125
train loss:  0.6054525971412659
train gradient:  0.18534243675341536
iteration : 11256
train acc:  0.703125
train loss:  0.49077513813972473
train gradient:  0.12694735735014406
iteration : 11257
train acc:  0.7265625
train loss:  0.45876964926719666
train gradient:  0.12911625168281926
iteration : 11258
train acc:  0.7109375
train loss:  0.5206295251846313
train gradient:  0.11769597364119692
iteration : 11259
train acc:  0.6328125
train loss:  0.6035396456718445
train gradient:  0.16473507442041158
iteration : 11260
train acc:  0.7734375
train loss:  0.44965192675590515
train gradient:  0.0974854621505415
iteration : 11261
train acc:  0.7265625
train loss:  0.4707541763782501
train gradient:  0.09914629771001991
iteration : 11262
train acc:  0.7890625
train loss:  0.4200811982154846
train gradient:  0.12420139396525809
iteration : 11263
train acc:  0.7890625
train loss:  0.47188007831573486
train gradient:  0.13374615610223445
iteration : 11264
train acc:  0.7734375
train loss:  0.44798511266708374
train gradient:  0.14051515588296573
iteration : 11265
train acc:  0.75
train loss:  0.5268166065216064
train gradient:  0.14995324652417524
iteration : 11266
train acc:  0.765625
train loss:  0.477852463722229
train gradient:  0.1303233595860621
iteration : 11267
train acc:  0.8359375
train loss:  0.4095439910888672
train gradient:  0.129115944433729
iteration : 11268
train acc:  0.734375
train loss:  0.4940158426761627
train gradient:  0.11765330440528601
iteration : 11269
train acc:  0.7890625
train loss:  0.4899385869503021
train gradient:  0.11849488756044091
iteration : 11270
train acc:  0.75
train loss:  0.4660504162311554
train gradient:  0.1464252770108298
iteration : 11271
train acc:  0.75
train loss:  0.48580604791641235
train gradient:  0.11770248494824533
iteration : 11272
train acc:  0.7578125
train loss:  0.4399717152118683
train gradient:  0.11476304952986402
iteration : 11273
train acc:  0.78125
train loss:  0.4729533791542053
train gradient:  0.11535178404774106
iteration : 11274
train acc:  0.6796875
train loss:  0.524027943611145
train gradient:  0.12812329403800748
iteration : 11275
train acc:  0.7734375
train loss:  0.47249212861061096
train gradient:  0.11411183079151059
iteration : 11276
train acc:  0.78125
train loss:  0.4197419285774231
train gradient:  0.13272191239598222
iteration : 11277
train acc:  0.7265625
train loss:  0.4926515221595764
train gradient:  0.11050468841034423
iteration : 11278
train acc:  0.7734375
train loss:  0.44466644525527954
train gradient:  0.12173376909603237
iteration : 11279
train acc:  0.765625
train loss:  0.45551392436027527
train gradient:  0.11173064681628786
iteration : 11280
train acc:  0.7265625
train loss:  0.5415133237838745
train gradient:  0.17132009523390812
iteration : 11281
train acc:  0.765625
train loss:  0.431510865688324
train gradient:  0.11216203367653986
iteration : 11282
train acc:  0.765625
train loss:  0.5079659819602966
train gradient:  0.144826344002209
iteration : 11283
train acc:  0.75
train loss:  0.5179706811904907
train gradient:  0.16184615337781205
iteration : 11284
train acc:  0.7578125
train loss:  0.47137880325317383
train gradient:  0.14604299632825077
iteration : 11285
train acc:  0.7421875
train loss:  0.49490517377853394
train gradient:  0.13829821206108228
iteration : 11286
train acc:  0.71875
train loss:  0.510698139667511
train gradient:  0.1190041456445882
iteration : 11287
train acc:  0.765625
train loss:  0.4877368211746216
train gradient:  0.113768431445179
iteration : 11288
train acc:  0.734375
train loss:  0.47880640625953674
train gradient:  0.10860461355735337
iteration : 11289
train acc:  0.765625
train loss:  0.4416847229003906
train gradient:  0.11643815232642694
iteration : 11290
train acc:  0.7734375
train loss:  0.47864776849746704
train gradient:  0.12073744616878827
iteration : 11291
train acc:  0.75
train loss:  0.4653109312057495
train gradient:  0.13266512759573929
iteration : 11292
train acc:  0.734375
train loss:  0.4673241972923279
train gradient:  0.1486557764378974
iteration : 11293
train acc:  0.765625
train loss:  0.4746977388858795
train gradient:  0.1078037147896067
iteration : 11294
train acc:  0.734375
train loss:  0.4602257013320923
train gradient:  0.10338846937521888
iteration : 11295
train acc:  0.6953125
train loss:  0.504176676273346
train gradient:  0.12505739831238932
iteration : 11296
train acc:  0.734375
train loss:  0.5075001120567322
train gradient:  0.1365883172295182
iteration : 11297
train acc:  0.7265625
train loss:  0.5002602338790894
train gradient:  0.1632056735389524
iteration : 11298
train acc:  0.7265625
train loss:  0.5414183139801025
train gradient:  0.17649918030880557
iteration : 11299
train acc:  0.7109375
train loss:  0.5528784990310669
train gradient:  0.18637802248668084
iteration : 11300
train acc:  0.7109375
train loss:  0.4956890940666199
train gradient:  0.12609581413994514
iteration : 11301
train acc:  0.75
train loss:  0.5084509253501892
train gradient:  0.10713797216883919
iteration : 11302
train acc:  0.828125
train loss:  0.45371729135513306
train gradient:  0.10768864817122215
iteration : 11303
train acc:  0.78125
train loss:  0.47899526357650757
train gradient:  0.12888147293377739
iteration : 11304
train acc:  0.7734375
train loss:  0.43464094400405884
train gradient:  0.08167469263145977
iteration : 11305
train acc:  0.671875
train loss:  0.6548360586166382
train gradient:  0.20652814243345194
iteration : 11306
train acc:  0.6875
train loss:  0.5594652891159058
train gradient:  0.15441553105792677
iteration : 11307
train acc:  0.7421875
train loss:  0.4675918221473694
train gradient:  0.11455375615569714
iteration : 11308
train acc:  0.7890625
train loss:  0.5153377056121826
train gradient:  0.1479518460078011
iteration : 11309
train acc:  0.7421875
train loss:  0.47837066650390625
train gradient:  0.11644261444406326
iteration : 11310
train acc:  0.7578125
train loss:  0.46645310521125793
train gradient:  0.12125267933017465
iteration : 11311
train acc:  0.7578125
train loss:  0.5444844961166382
train gradient:  0.17530570078919988
iteration : 11312
train acc:  0.765625
train loss:  0.4894154369831085
train gradient:  0.12788937782389054
iteration : 11313
train acc:  0.78125
train loss:  0.4837720990180969
train gradient:  0.12402765667605183
iteration : 11314
train acc:  0.7890625
train loss:  0.44337978959083557
train gradient:  0.1319013605202703
iteration : 11315
train acc:  0.6953125
train loss:  0.5797948241233826
train gradient:  0.1591337516125984
iteration : 11316
train acc:  0.7265625
train loss:  0.5313166975975037
train gradient:  0.1810093427420414
iteration : 11317
train acc:  0.78125
train loss:  0.41605186462402344
train gradient:  0.09665208448944179
iteration : 11318
train acc:  0.7421875
train loss:  0.4688299000263214
train gradient:  0.12261609136266435
iteration : 11319
train acc:  0.6796875
train loss:  0.542176365852356
train gradient:  0.16187517777520466
iteration : 11320
train acc:  0.7265625
train loss:  0.5164321660995483
train gradient:  0.14763001261816144
iteration : 11321
train acc:  0.7734375
train loss:  0.46588823199272156
train gradient:  0.09576488881290532
iteration : 11322
train acc:  0.7421875
train loss:  0.5015177130699158
train gradient:  0.1359737775967535
iteration : 11323
train acc:  0.7421875
train loss:  0.46519237756729126
train gradient:  0.12855516956187246
iteration : 11324
train acc:  0.7265625
train loss:  0.4957953691482544
train gradient:  0.1483116875838954
iteration : 11325
train acc:  0.7734375
train loss:  0.5021937489509583
train gradient:  0.12208924727402914
iteration : 11326
train acc:  0.8046875
train loss:  0.435746431350708
train gradient:  0.11821890328217555
iteration : 11327
train acc:  0.6875
train loss:  0.5177816152572632
train gradient:  0.12794933347450127
iteration : 11328
train acc:  0.78125
train loss:  0.4656071662902832
train gradient:  0.12941477340074076
iteration : 11329
train acc:  0.75
train loss:  0.5502628087997437
train gradient:  0.12254397715505157
iteration : 11330
train acc:  0.7109375
train loss:  0.5049545168876648
train gradient:  0.13439235107634018
iteration : 11331
train acc:  0.7421875
train loss:  0.4998010993003845
train gradient:  0.1209221367934982
iteration : 11332
train acc:  0.671875
train loss:  0.5272681713104248
train gradient:  0.12508688256464068
iteration : 11333
train acc:  0.671875
train loss:  0.5671891570091248
train gradient:  0.21685181321431227
iteration : 11334
train acc:  0.7890625
train loss:  0.48325270414352417
train gradient:  0.12223576463145759
iteration : 11335
train acc:  0.7734375
train loss:  0.4531438946723938
train gradient:  0.09305127443219025
iteration : 11336
train acc:  0.7421875
train loss:  0.43846455216407776
train gradient:  0.12224562564708312
iteration : 11337
train acc:  0.75
train loss:  0.48378628492355347
train gradient:  0.10196661885377802
iteration : 11338
train acc:  0.7109375
train loss:  0.5325928926467896
train gradient:  0.15232987476973758
iteration : 11339
train acc:  0.8125
train loss:  0.4043029844760895
train gradient:  0.09564667788348316
iteration : 11340
train acc:  0.765625
train loss:  0.4858371913433075
train gradient:  0.10254258215713663
iteration : 11341
train acc:  0.8125
train loss:  0.41491925716400146
train gradient:  0.103757107627279
iteration : 11342
train acc:  0.75
train loss:  0.5051997900009155
train gradient:  0.13237411235650315
iteration : 11343
train acc:  0.765625
train loss:  0.4335229694843292
train gradient:  0.11039146921822476
iteration : 11344
train acc:  0.75
train loss:  0.49047037959098816
train gradient:  0.13046133133346288
iteration : 11345
train acc:  0.7109375
train loss:  0.5049439668655396
train gradient:  0.16503458295901413
iteration : 11346
train acc:  0.78125
train loss:  0.4731830358505249
train gradient:  0.15573290091573055
iteration : 11347
train acc:  0.7421875
train loss:  0.5017915964126587
train gradient:  0.1372484515381323
iteration : 11348
train acc:  0.75
train loss:  0.47547829151153564
train gradient:  0.12162755858487129
iteration : 11349
train acc:  0.765625
train loss:  0.443760484457016
train gradient:  0.10255150040548967
iteration : 11350
train acc:  0.75
train loss:  0.49348270893096924
train gradient:  0.1555609062229441
iteration : 11351
train acc:  0.765625
train loss:  0.4481791853904724
train gradient:  0.12355000595334124
iteration : 11352
train acc:  0.7734375
train loss:  0.4843288064002991
train gradient:  0.11382283493657085
iteration : 11353
train acc:  0.6953125
train loss:  0.5423669219017029
train gradient:  0.13076807220173947
iteration : 11354
train acc:  0.8046875
train loss:  0.43477559089660645
train gradient:  0.1257850955230239
iteration : 11355
train acc:  0.6171875
train loss:  0.5791176557540894
train gradient:  0.1889167875649706
iteration : 11356
train acc:  0.7265625
train loss:  0.5316640138626099
train gradient:  0.15932323897325307
iteration : 11357
train acc:  0.7421875
train loss:  0.46647143363952637
train gradient:  0.11985756651181337
iteration : 11358
train acc:  0.7421875
train loss:  0.448476642370224
train gradient:  0.09953671547637254
iteration : 11359
train acc:  0.8046875
train loss:  0.4528695046901703
train gradient:  0.1266718528149404
iteration : 11360
train acc:  0.7890625
train loss:  0.4319634735584259
train gradient:  0.09552228712726563
iteration : 11361
train acc:  0.796875
train loss:  0.45315787196159363
train gradient:  0.10062464262751755
iteration : 11362
train acc:  0.7734375
train loss:  0.4906468093395233
train gradient:  0.13225405174204086
iteration : 11363
train acc:  0.7734375
train loss:  0.4687345623970032
train gradient:  0.11028473815502693
iteration : 11364
train acc:  0.828125
train loss:  0.4568938910961151
train gradient:  0.13517432200161175
iteration : 11365
train acc:  0.765625
train loss:  0.5090674161911011
train gradient:  0.14794178458004897
iteration : 11366
train acc:  0.7421875
train loss:  0.5146828889846802
train gradient:  0.13684859865640658
iteration : 11367
train acc:  0.7578125
train loss:  0.4663275182247162
train gradient:  0.11633365795395226
iteration : 11368
train acc:  0.78125
train loss:  0.4575725793838501
train gradient:  0.13163394543909016
iteration : 11369
train acc:  0.75
train loss:  0.4619198441505432
train gradient:  0.111089535714429
iteration : 11370
train acc:  0.7421875
train loss:  0.4626767933368683
train gradient:  0.10103488137020392
iteration : 11371
train acc:  0.6953125
train loss:  0.53163743019104
train gradient:  0.15724885741340994
iteration : 11372
train acc:  0.7265625
train loss:  0.5527307391166687
train gradient:  0.1640257625767232
iteration : 11373
train acc:  0.6875
train loss:  0.6335168480873108
train gradient:  0.22885004528816563
iteration : 11374
train acc:  0.796875
train loss:  0.46390843391418457
train gradient:  0.11922625274406128
iteration : 11375
train acc:  0.765625
train loss:  0.4874765872955322
train gradient:  0.1139903996532791
iteration : 11376
train acc:  0.7734375
train loss:  0.41724056005477905
train gradient:  0.09097440670423383
iteration : 11377
train acc:  0.7734375
train loss:  0.4371236562728882
train gradient:  0.08691373565232104
iteration : 11378
train acc:  0.7421875
train loss:  0.4966667592525482
train gradient:  0.1319451298408774
iteration : 11379
train acc:  0.765625
train loss:  0.4914530813694
train gradient:  0.13071735199884965
iteration : 11380
train acc:  0.7265625
train loss:  0.5388208031654358
train gradient:  0.14640020665991071
iteration : 11381
train acc:  0.78125
train loss:  0.4588402509689331
train gradient:  0.10098437630706106
iteration : 11382
train acc:  0.8046875
train loss:  0.4655580520629883
train gradient:  0.20862636681740754
iteration : 11383
train acc:  0.765625
train loss:  0.4981975257396698
train gradient:  0.1505769737788625
iteration : 11384
train acc:  0.734375
train loss:  0.48022782802581787
train gradient:  0.12207696351465333
iteration : 11385
train acc:  0.7734375
train loss:  0.4996580481529236
train gradient:  0.1300695236942016
iteration : 11386
train acc:  0.71875
train loss:  0.5117778778076172
train gradient:  0.1417504620093853
iteration : 11387
train acc:  0.734375
train loss:  0.5015110969543457
train gradient:  0.12273789986885818
iteration : 11388
train acc:  0.7734375
train loss:  0.485423743724823
train gradient:  0.1313051513907229
iteration : 11389
train acc:  0.7421875
train loss:  0.4878084659576416
train gradient:  0.15267655589995
iteration : 11390
train acc:  0.734375
train loss:  0.5241459012031555
train gradient:  0.1576476236791743
iteration : 11391
train acc:  0.796875
train loss:  0.43391597270965576
train gradient:  0.09734407899281612
iteration : 11392
train acc:  0.6796875
train loss:  0.4996474087238312
train gradient:  0.14818325392967693
iteration : 11393
train acc:  0.8125
train loss:  0.4137629270553589
train gradient:  0.09614285008686808
iteration : 11394
train acc:  0.7265625
train loss:  0.5773300528526306
train gradient:  0.19401598331996112
iteration : 11395
train acc:  0.796875
train loss:  0.4190051257610321
train gradient:  0.08961640094537665
iteration : 11396
train acc:  0.7734375
train loss:  0.4575352668762207
train gradient:  0.15078792664547752
iteration : 11397
train acc:  0.7265625
train loss:  0.4270496666431427
train gradient:  0.0942300485531025
iteration : 11398
train acc:  0.7265625
train loss:  0.5181925296783447
train gradient:  0.13714174054829692
iteration : 11399
train acc:  0.71875
train loss:  0.5007838010787964
train gradient:  0.13493725401477646
iteration : 11400
train acc:  0.78125
train loss:  0.41482770442962646
train gradient:  0.09382807011847177
iteration : 11401
train acc:  0.71875
train loss:  0.5632263422012329
train gradient:  0.161939705059241
iteration : 11402
train acc:  0.734375
train loss:  0.49330437183380127
train gradient:  0.1299318658305837
iteration : 11403
train acc:  0.828125
train loss:  0.4536980390548706
train gradient:  0.10732276360496674
iteration : 11404
train acc:  0.7421875
train loss:  0.4803108274936676
train gradient:  0.1465476660444756
iteration : 11405
train acc:  0.6953125
train loss:  0.49861347675323486
train gradient:  0.11570677468530531
iteration : 11406
train acc:  0.6015625
train loss:  0.6060380935668945
train gradient:  0.16863252090992456
iteration : 11407
train acc:  0.75
train loss:  0.4979948401451111
train gradient:  0.1702452318004529
iteration : 11408
train acc:  0.78125
train loss:  0.47259271144866943
train gradient:  0.09759492339535
iteration : 11409
train acc:  0.7265625
train loss:  0.5326167345046997
train gradient:  0.16607458755781684
iteration : 11410
train acc:  0.8125
train loss:  0.3939056992530823
train gradient:  0.08101791829861103
iteration : 11411
train acc:  0.7734375
train loss:  0.47025787830352783
train gradient:  0.11511839102297063
iteration : 11412
train acc:  0.78125
train loss:  0.44963622093200684
train gradient:  0.09900726529374279
iteration : 11413
train acc:  0.8203125
train loss:  0.42067480087280273
train gradient:  0.09663813716322575
iteration : 11414
train acc:  0.765625
train loss:  0.47257933020591736
train gradient:  0.11742934983167241
iteration : 11415
train acc:  0.75
train loss:  0.4764307141304016
train gradient:  0.11724993676251921
iteration : 11416
train acc:  0.6640625
train loss:  0.5395827889442444
train gradient:  0.11085149345370475
iteration : 11417
train acc:  0.734375
train loss:  0.5901845693588257
train gradient:  0.16922754753271185
iteration : 11418
train acc:  0.7421875
train loss:  0.5003753900527954
train gradient:  0.14087591027094193
iteration : 11419
train acc:  0.7421875
train loss:  0.4826158881187439
train gradient:  0.10409232628819517
iteration : 11420
train acc:  0.765625
train loss:  0.46282070875167847
train gradient:  0.10537793321020139
iteration : 11421
train acc:  0.8203125
train loss:  0.4358360767364502
train gradient:  0.13667350979454007
iteration : 11422
train acc:  0.734375
train loss:  0.5288141965866089
train gradient:  0.16945541805139758
iteration : 11423
train acc:  0.8203125
train loss:  0.4553273022174835
train gradient:  0.11037678418351292
iteration : 11424
train acc:  0.7109375
train loss:  0.5194166898727417
train gradient:  0.15645869377127514
iteration : 11425
train acc:  0.75
train loss:  0.4310746192932129
train gradient:  0.09683710855220017
iteration : 11426
train acc:  0.765625
train loss:  0.4963967800140381
train gradient:  0.1533330591566483
iteration : 11427
train acc:  0.734375
train loss:  0.4911104738712311
train gradient:  0.11167668447019825
iteration : 11428
train acc:  0.671875
train loss:  0.5046805739402771
train gradient:  0.13338149620592693
iteration : 11429
train acc:  0.6875
train loss:  0.5442759990692139
train gradient:  0.1647619855620256
iteration : 11430
train acc:  0.7265625
train loss:  0.4847060441970825
train gradient:  0.11352496930014311
iteration : 11431
train acc:  0.78125
train loss:  0.48975101113319397
train gradient:  0.13633721867529108
iteration : 11432
train acc:  0.734375
train loss:  0.5168982744216919
train gradient:  0.18495252569061527
iteration : 11433
train acc:  0.7109375
train loss:  0.5513447523117065
train gradient:  0.15908738584088994
iteration : 11434
train acc:  0.78125
train loss:  0.47599202394485474
train gradient:  0.10995635961015
iteration : 11435
train acc:  0.765625
train loss:  0.47436270117759705
train gradient:  0.10559811357178482
iteration : 11436
train acc:  0.765625
train loss:  0.4627663791179657
train gradient:  0.12278595772882668
iteration : 11437
train acc:  0.78125
train loss:  0.4720991849899292
train gradient:  0.1076305791958849
iteration : 11438
train acc:  0.71875
train loss:  0.5150206685066223
train gradient:  0.15039239824290468
iteration : 11439
train acc:  0.7421875
train loss:  0.4750123918056488
train gradient:  0.11211436133303776
iteration : 11440
train acc:  0.8046875
train loss:  0.46791261434555054
train gradient:  0.11386069541922736
iteration : 11441
train acc:  0.8046875
train loss:  0.4299764633178711
train gradient:  0.10879978430548967
iteration : 11442
train acc:  0.703125
train loss:  0.5243773460388184
train gradient:  0.15624891319582557
iteration : 11443
train acc:  0.765625
train loss:  0.4787467420101166
train gradient:  0.12623354845104062
iteration : 11444
train acc:  0.671875
train loss:  0.562251091003418
train gradient:  0.19156651632823601
iteration : 11445
train acc:  0.7109375
train loss:  0.5806320905685425
train gradient:  0.19718960102783234
iteration : 11446
train acc:  0.6875
train loss:  0.5602164268493652
train gradient:  0.18247424378784213
iteration : 11447
train acc:  0.734375
train loss:  0.5091046094894409
train gradient:  0.12271420500053139
iteration : 11448
train acc:  0.734375
train loss:  0.5060549974441528
train gradient:  0.13419444560708882
iteration : 11449
train acc:  0.71875
train loss:  0.48929768800735474
train gradient:  0.1629345568741254
iteration : 11450
train acc:  0.7109375
train loss:  0.572521984577179
train gradient:  0.17814236359410557
iteration : 11451
train acc:  0.7109375
train loss:  0.5196144580841064
train gradient:  0.12157486687782339
iteration : 11452
train acc:  0.765625
train loss:  0.4883579611778259
train gradient:  0.1417526230513656
iteration : 11453
train acc:  0.765625
train loss:  0.4888937771320343
train gradient:  0.10713729505868966
iteration : 11454
train acc:  0.7890625
train loss:  0.4351995587348938
train gradient:  0.0923421926766693
iteration : 11455
train acc:  0.765625
train loss:  0.43100765347480774
train gradient:  0.11140777869884824
iteration : 11456
train acc:  0.734375
train loss:  0.5203430652618408
train gradient:  0.22593189910907935
iteration : 11457
train acc:  0.765625
train loss:  0.5060418844223022
train gradient:  0.14958814375819873
iteration : 11458
train acc:  0.8125
train loss:  0.42629700899124146
train gradient:  0.12095274634917756
iteration : 11459
train acc:  0.7578125
train loss:  0.4706448018550873
train gradient:  0.14514564711016797
iteration : 11460
train acc:  0.640625
train loss:  0.5566892623901367
train gradient:  0.14817564159069713
iteration : 11461
train acc:  0.7421875
train loss:  0.5691495537757874
train gradient:  0.15882163385963524
iteration : 11462
train acc:  0.7265625
train loss:  0.5131853222846985
train gradient:  0.1702950094254161
iteration : 11463
train acc:  0.734375
train loss:  0.4802480936050415
train gradient:  0.14906611353023097
iteration : 11464
train acc:  0.7265625
train loss:  0.5459595918655396
train gradient:  0.1612994442736686
iteration : 11465
train acc:  0.7734375
train loss:  0.4950083792209625
train gradient:  0.13301922809586997
iteration : 11466
train acc:  0.71875
train loss:  0.5148679614067078
train gradient:  0.1725740447992645
iteration : 11467
train acc:  0.71875
train loss:  0.4978862404823303
train gradient:  0.1224119317782275
iteration : 11468
train acc:  0.7734375
train loss:  0.48839402198791504
train gradient:  0.1408367055115623
iteration : 11469
train acc:  0.7109375
train loss:  0.5010284185409546
train gradient:  0.15255734183624114
iteration : 11470
train acc:  0.7109375
train loss:  0.5144611597061157
train gradient:  0.14143478853424646
iteration : 11471
train acc:  0.65625
train loss:  0.5287758708000183
train gradient:  0.15475487000488888
iteration : 11472
train acc:  0.796875
train loss:  0.48538774251937866
train gradient:  0.1293182601131168
iteration : 11473
train acc:  0.6796875
train loss:  0.6328504085540771
train gradient:  0.2132240554950499
iteration : 11474
train acc:  0.75
train loss:  0.49463820457458496
train gradient:  0.13609407685756544
iteration : 11475
train acc:  0.765625
train loss:  0.4728681445121765
train gradient:  0.11761026591554415
iteration : 11476
train acc:  0.7421875
train loss:  0.5316253304481506
train gradient:  0.16124635362359152
iteration : 11477
train acc:  0.6953125
train loss:  0.5208085775375366
train gradient:  0.15249056086162446
iteration : 11478
train acc:  0.71875
train loss:  0.47134333848953247
train gradient:  0.13468956637809026
iteration : 11479
train acc:  0.7421875
train loss:  0.5012784004211426
train gradient:  0.14541331309071734
iteration : 11480
train acc:  0.7578125
train loss:  0.44488731026649475
train gradient:  0.10958583544567872
iteration : 11481
train acc:  0.78125
train loss:  0.44398465752601624
train gradient:  0.09844992770363882
iteration : 11482
train acc:  0.7578125
train loss:  0.44302693009376526
train gradient:  0.12676192456128837
iteration : 11483
train acc:  0.75
train loss:  0.5047855973243713
train gradient:  0.1309437211467665
iteration : 11484
train acc:  0.734375
train loss:  0.5098646879196167
train gradient:  0.12018456528642353
iteration : 11485
train acc:  0.734375
train loss:  0.48346418142318726
train gradient:  0.1264217754792205
iteration : 11486
train acc:  0.796875
train loss:  0.41938289999961853
train gradient:  0.11303359551000933
iteration : 11487
train acc:  0.7109375
train loss:  0.4784895181655884
train gradient:  0.11892518197808091
iteration : 11488
train acc:  0.75
train loss:  0.45941412448883057
train gradient:  0.11383497743190742
iteration : 11489
train acc:  0.734375
train loss:  0.5124197006225586
train gradient:  0.12076073875933328
iteration : 11490
train acc:  0.703125
train loss:  0.5499000549316406
train gradient:  0.16991101511047407
iteration : 11491
train acc:  0.703125
train loss:  0.569645345211029
train gradient:  0.153817818890905
iteration : 11492
train acc:  0.7578125
train loss:  0.45296114683151245
train gradient:  0.11556312248255918
iteration : 11493
train acc:  0.796875
train loss:  0.42360782623291016
train gradient:  0.11479686683526706
iteration : 11494
train acc:  0.6484375
train loss:  0.6471939086914062
train gradient:  0.2505064969732814
iteration : 11495
train acc:  0.703125
train loss:  0.5284328460693359
train gradient:  0.1709124505094725
iteration : 11496
train acc:  0.6640625
train loss:  0.5620167255401611
train gradient:  0.16631238612706137
iteration : 11497
train acc:  0.75
train loss:  0.4921107292175293
train gradient:  0.12992332067581977
iteration : 11498
train acc:  0.734375
train loss:  0.5313538312911987
train gradient:  0.13609673809466039
iteration : 11499
train acc:  0.7890625
train loss:  0.46730726957321167
train gradient:  0.09867176855369758
iteration : 11500
train acc:  0.6875
train loss:  0.5127688646316528
train gradient:  0.14892490694010607
iteration : 11501
train acc:  0.7578125
train loss:  0.5217112302780151
train gradient:  0.11765654339048912
iteration : 11502
train acc:  0.765625
train loss:  0.504796028137207
train gradient:  0.16703418748370075
iteration : 11503
train acc:  0.7734375
train loss:  0.504684329032898
train gradient:  0.12121859861758498
iteration : 11504
train acc:  0.765625
train loss:  0.5572471022605896
train gradient:  0.16644008159589568
iteration : 11505
train acc:  0.7265625
train loss:  0.5483895540237427
train gradient:  0.1468280150144734
iteration : 11506
train acc:  0.78125
train loss:  0.4811311960220337
train gradient:  0.11105415524273425
iteration : 11507
train acc:  0.75
train loss:  0.46752920746803284
train gradient:  0.12532677985834728
iteration : 11508
train acc:  0.71875
train loss:  0.4528679847717285
train gradient:  0.11126829686263681
iteration : 11509
train acc:  0.78125
train loss:  0.46996286511421204
train gradient:  0.12944646861597203
iteration : 11510
train acc:  0.8203125
train loss:  0.46312078833580017
train gradient:  0.1035000634617449
iteration : 11511
train acc:  0.765625
train loss:  0.4819316267967224
train gradient:  0.11971400036456913
iteration : 11512
train acc:  0.78125
train loss:  0.5124543905258179
train gradient:  0.10556513348773033
iteration : 11513
train acc:  0.7109375
train loss:  0.5241754055023193
train gradient:  0.1305379468421654
iteration : 11514
train acc:  0.7890625
train loss:  0.4817761778831482
train gradient:  0.13034543242756408
iteration : 11515
train acc:  0.8125
train loss:  0.4723981022834778
train gradient:  0.17690423906890262
iteration : 11516
train acc:  0.7578125
train loss:  0.48693186044692993
train gradient:  0.12527066145767196
iteration : 11517
train acc:  0.734375
train loss:  0.503288209438324
train gradient:  0.1271510684198472
iteration : 11518
train acc:  0.765625
train loss:  0.4255393445491791
train gradient:  0.09217293669519418
iteration : 11519
train acc:  0.7421875
train loss:  0.5349990129470825
train gradient:  0.1711417699693705
iteration : 11520
train acc:  0.75
train loss:  0.5101400017738342
train gradient:  0.1282783552465131
iteration : 11521
train acc:  0.78125
train loss:  0.4622948467731476
train gradient:  0.10806679322636775
iteration : 11522
train acc:  0.7890625
train loss:  0.4257551431655884
train gradient:  0.09766818203905936
iteration : 11523
train acc:  0.6328125
train loss:  0.5413373112678528
train gradient:  0.15306246047356425
iteration : 11524
train acc:  0.71875
train loss:  0.5110092163085938
train gradient:  0.14442072696348196
iteration : 11525
train acc:  0.75
train loss:  0.5199772715568542
train gradient:  0.13100211033480197
iteration : 11526
train acc:  0.6953125
train loss:  0.5251919031143188
train gradient:  0.15619443170771996
iteration : 11527
train acc:  0.734375
train loss:  0.4837645888328552
train gradient:  0.12837401087607483
iteration : 11528
train acc:  0.7578125
train loss:  0.4669541120529175
train gradient:  0.12109780708641209
iteration : 11529
train acc:  0.7578125
train loss:  0.4833734929561615
train gradient:  0.13666419663018467
iteration : 11530
train acc:  0.6875
train loss:  0.5204957127571106
train gradient:  0.12869079789640828
iteration : 11531
train acc:  0.7734375
train loss:  0.4953034818172455
train gradient:  0.12412096913296145
iteration : 11532
train acc:  0.6484375
train loss:  0.5748295187950134
train gradient:  0.1892669893606096
iteration : 11533
train acc:  0.7578125
train loss:  0.4536221921443939
train gradient:  0.10867129470004647
iteration : 11534
train acc:  0.6640625
train loss:  0.6180151700973511
train gradient:  0.1942563133376505
iteration : 11535
train acc:  0.78125
train loss:  0.4483046531677246
train gradient:  0.11317995581849187
iteration : 11536
train acc:  0.7265625
train loss:  0.5562502145767212
train gradient:  0.19335458493173846
iteration : 11537
train acc:  0.71875
train loss:  0.528693675994873
train gradient:  0.15318931554251458
iteration : 11538
train acc:  0.75
train loss:  0.48804208636283875
train gradient:  0.11712339838841912
iteration : 11539
train acc:  0.6953125
train loss:  0.5693743824958801
train gradient:  0.15885765368062174
iteration : 11540
train acc:  0.7578125
train loss:  0.5802639722824097
train gradient:  0.20906042842424757
iteration : 11541
train acc:  0.796875
train loss:  0.43645069003105164
train gradient:  0.10252468522807741
iteration : 11542
train acc:  0.8203125
train loss:  0.4387891888618469
train gradient:  0.09834061580315247
iteration : 11543
train acc:  0.6796875
train loss:  0.5728613138198853
train gradient:  0.18625852757878714
iteration : 11544
train acc:  0.6875
train loss:  0.5442431569099426
train gradient:  0.14382056149455297
iteration : 11545
train acc:  0.6875
train loss:  0.5434205532073975
train gradient:  0.1260734485114985
iteration : 11546
train acc:  0.7265625
train loss:  0.4784296751022339
train gradient:  0.1691799218005242
iteration : 11547
train acc:  0.6953125
train loss:  0.530060887336731
train gradient:  0.1334659118902491
iteration : 11548
train acc:  0.7890625
train loss:  0.42011404037475586
train gradient:  0.09511477268039384
iteration : 11549
train acc:  0.765625
train loss:  0.5214493274688721
train gradient:  0.14686121480582653
iteration : 11550
train acc:  0.6796875
train loss:  0.5029302835464478
train gradient:  0.1172551629545165
iteration : 11551
train acc:  0.6953125
train loss:  0.5393738746643066
train gradient:  0.1534780558379526
iteration : 11552
train acc:  0.640625
train loss:  0.6420055627822876
train gradient:  0.2347319984186837
iteration : 11553
train acc:  0.7578125
train loss:  0.505859911441803
train gradient:  0.11612834144354539
iteration : 11554
train acc:  0.7890625
train loss:  0.4484148621559143
train gradient:  0.14548764716992946
iteration : 11555
train acc:  0.7265625
train loss:  0.4996802806854248
train gradient:  0.15351671942132206
iteration : 11556
train acc:  0.6953125
train loss:  0.5124646425247192
train gradient:  0.1616804600163417
iteration : 11557
train acc:  0.7109375
train loss:  0.501316487789154
train gradient:  0.10712035540888958
iteration : 11558
train acc:  0.7421875
train loss:  0.4607510566711426
train gradient:  0.13154054086890252
iteration : 11559
train acc:  0.8046875
train loss:  0.42819154262542725
train gradient:  0.1163991142055606
iteration : 11560
train acc:  0.828125
train loss:  0.38410601019859314
train gradient:  0.08227910436342402
iteration : 11561
train acc:  0.71875
train loss:  0.5217543840408325
train gradient:  0.12042625116364367
iteration : 11562
train acc:  0.7578125
train loss:  0.46891915798187256
train gradient:  0.13600900899862478
iteration : 11563
train acc:  0.796875
train loss:  0.4248730540275574
train gradient:  0.0993093200189523
iteration : 11564
train acc:  0.7578125
train loss:  0.4587133824825287
train gradient:  0.12523104049720452
iteration : 11565
train acc:  0.7890625
train loss:  0.4622146487236023
train gradient:  0.11031072415740467
iteration : 11566
train acc:  0.8125
train loss:  0.415155291557312
train gradient:  0.10626468186006154
iteration : 11567
train acc:  0.765625
train loss:  0.500930666923523
train gradient:  0.14623361173832325
iteration : 11568
train acc:  0.78125
train loss:  0.44417276978492737
train gradient:  0.10069966605221098
iteration : 11569
train acc:  0.71875
train loss:  0.48048675060272217
train gradient:  0.10385651496533531
iteration : 11570
train acc:  0.7265625
train loss:  0.49799445271492004
train gradient:  0.14211214779548742
iteration : 11571
train acc:  0.6953125
train loss:  0.5420759916305542
train gradient:  0.1488747653687626
iteration : 11572
train acc:  0.8046875
train loss:  0.4298630356788635
train gradient:  0.11886182076326887
iteration : 11573
train acc:  0.7890625
train loss:  0.44196656346321106
train gradient:  0.10160621499188792
iteration : 11574
train acc:  0.71875
train loss:  0.5104989409446716
train gradient:  0.12620563152777003
iteration : 11575
train acc:  0.796875
train loss:  0.4941171109676361
train gradient:  0.1306817527566818
iteration : 11576
train acc:  0.7421875
train loss:  0.48822805285453796
train gradient:  0.11263218689726723
iteration : 11577
train acc:  0.75
train loss:  0.47121235728263855
train gradient:  0.10304310078039287
iteration : 11578
train acc:  0.7109375
train loss:  0.5520873069763184
train gradient:  0.20623344646761305
iteration : 11579
train acc:  0.7578125
train loss:  0.4441159963607788
train gradient:  0.12051190577507266
iteration : 11580
train acc:  0.7265625
train loss:  0.548539400100708
train gradient:  0.14032523726848548
iteration : 11581
train acc:  0.7265625
train loss:  0.5707336664199829
train gradient:  0.191348063498888
iteration : 11582
train acc:  0.765625
train loss:  0.4579053223133087
train gradient:  0.11065088729929566
iteration : 11583
train acc:  0.7890625
train loss:  0.4646068215370178
train gradient:  0.10607104791405428
iteration : 11584
train acc:  0.765625
train loss:  0.5085046887397766
train gradient:  0.12336813891407335
iteration : 11585
train acc:  0.7421875
train loss:  0.4881400763988495
train gradient:  0.13001029424018246
iteration : 11586
train acc:  0.75
train loss:  0.498279869556427
train gradient:  0.14065405970142228
iteration : 11587
train acc:  0.796875
train loss:  0.46519842743873596
train gradient:  0.12236744692092166
iteration : 11588
train acc:  0.734375
train loss:  0.5038567781448364
train gradient:  0.12659484650716346
iteration : 11589
train acc:  0.7265625
train loss:  0.5065988302230835
train gradient:  0.10802914971399956
iteration : 11590
train acc:  0.8046875
train loss:  0.4560675024986267
train gradient:  0.13385120848518717
iteration : 11591
train acc:  0.7421875
train loss:  0.5289393663406372
train gradient:  0.1348093533063524
iteration : 11592
train acc:  0.765625
train loss:  0.4618077874183655
train gradient:  0.15621654651529793
iteration : 11593
train acc:  0.7421875
train loss:  0.5061542987823486
train gradient:  0.12505597975634258
iteration : 11594
train acc:  0.828125
train loss:  0.4441138505935669
train gradient:  0.10609017993806363
iteration : 11595
train acc:  0.8046875
train loss:  0.4161909818649292
train gradient:  0.09959339399482578
iteration : 11596
train acc:  0.78125
train loss:  0.44754186272621155
train gradient:  0.12751963564066437
iteration : 11597
train acc:  0.8046875
train loss:  0.4358487129211426
train gradient:  0.10298277530589955
iteration : 11598
train acc:  0.7890625
train loss:  0.4380194842815399
train gradient:  0.10445067093151016
iteration : 11599
train acc:  0.71875
train loss:  0.5061897039413452
train gradient:  0.13317803270434214
iteration : 11600
train acc:  0.7890625
train loss:  0.4540385901927948
train gradient:  0.10590285061699499
iteration : 11601
train acc:  0.7578125
train loss:  0.46365463733673096
train gradient:  0.12383351743200618
iteration : 11602
train acc:  0.7578125
train loss:  0.46764320135116577
train gradient:  0.10802684844518616
iteration : 11603
train acc:  0.6875
train loss:  0.5390297174453735
train gradient:  0.16173084834487922
iteration : 11604
train acc:  0.6015625
train loss:  0.6199192404747009
train gradient:  0.18149716003203376
iteration : 11605
train acc:  0.78125
train loss:  0.45069411396980286
train gradient:  0.10440059845175775
iteration : 11606
train acc:  0.6640625
train loss:  0.5559267401695251
train gradient:  0.1664623037156987
iteration : 11607
train acc:  0.7109375
train loss:  0.5154730081558228
train gradient:  0.14012681734859106
iteration : 11608
train acc:  0.78125
train loss:  0.44056451320648193
train gradient:  0.0909635810798549
iteration : 11609
train acc:  0.7265625
train loss:  0.5678253769874573
train gradient:  0.14643404873394084
iteration : 11610
train acc:  0.7734375
train loss:  0.46253466606140137
train gradient:  0.13245067430893417
iteration : 11611
train acc:  0.6640625
train loss:  0.5988876819610596
train gradient:  0.16937824494110879
iteration : 11612
train acc:  0.7890625
train loss:  0.4560663104057312
train gradient:  0.12144071497936902
iteration : 11613
train acc:  0.796875
train loss:  0.45990559458732605
train gradient:  0.1234951629683174
iteration : 11614
train acc:  0.734375
train loss:  0.4920363426208496
train gradient:  0.11220519468554421
iteration : 11615
train acc:  0.7890625
train loss:  0.42539769411087036
train gradient:  0.1204610632257137
iteration : 11616
train acc:  0.78125
train loss:  0.46007266640663147
train gradient:  0.12119422117413743
iteration : 11617
train acc:  0.8125
train loss:  0.4799768030643463
train gradient:  0.10870468635723005
iteration : 11618
train acc:  0.625
train loss:  0.6114463806152344
train gradient:  0.18213801982083666
iteration : 11619
train acc:  0.8125
train loss:  0.4315057098865509
train gradient:  0.10160532758221653
iteration : 11620
train acc:  0.7421875
train loss:  0.4769991934299469
train gradient:  0.12390124358415064
iteration : 11621
train acc:  0.671875
train loss:  0.5771101713180542
train gradient:  0.17471847929994525
iteration : 11622
train acc:  0.75
train loss:  0.48589515686035156
train gradient:  0.1358542310235087
iteration : 11623
train acc:  0.6953125
train loss:  0.5258336663246155
train gradient:  0.1562053063392433
iteration : 11624
train acc:  0.734375
train loss:  0.5144762992858887
train gradient:  0.1504194895237111
iteration : 11625
train acc:  0.796875
train loss:  0.494687557220459
train gradient:  0.12001806227333554
iteration : 11626
train acc:  0.71875
train loss:  0.5153079032897949
train gradient:  0.15970153868298778
iteration : 11627
train acc:  0.71875
train loss:  0.4972265660762787
train gradient:  0.12861790971013554
iteration : 11628
train acc:  0.765625
train loss:  0.5200494527816772
train gradient:  0.14009270500819238
iteration : 11629
train acc:  0.8125
train loss:  0.3788304328918457
train gradient:  0.09133449922294361
iteration : 11630
train acc:  0.7578125
train loss:  0.44001516699790955
train gradient:  0.08357734537261309
iteration : 11631
train acc:  0.8046875
train loss:  0.43699371814727783
train gradient:  0.08742938694061288
iteration : 11632
train acc:  0.734375
train loss:  0.5451838970184326
train gradient:  0.15469817695796229
iteration : 11633
train acc:  0.6484375
train loss:  0.5750689506530762
train gradient:  0.15934905163296972
iteration : 11634
train acc:  0.8203125
train loss:  0.4554806351661682
train gradient:  0.1160492546500355
iteration : 11635
train acc:  0.78125
train loss:  0.43129026889801025
train gradient:  0.09879039790418213
iteration : 11636
train acc:  0.8125
train loss:  0.45945659279823303
train gradient:  0.11866895792566942
iteration : 11637
train acc:  0.75
train loss:  0.5181299448013306
train gradient:  0.12639587403942604
iteration : 11638
train acc:  0.7578125
train loss:  0.4786064326763153
train gradient:  0.12166024939604951
iteration : 11639
train acc:  0.71875
train loss:  0.5269131660461426
train gradient:  0.11722296299632795
iteration : 11640
train acc:  0.8046875
train loss:  0.4680987596511841
train gradient:  0.11955459717733481
iteration : 11641
train acc:  0.734375
train loss:  0.526547908782959
train gradient:  0.1774406598436285
iteration : 11642
train acc:  0.75
train loss:  0.4881817698478699
train gradient:  0.1315784181910532
iteration : 11643
train acc:  0.734375
train loss:  0.5032634735107422
train gradient:  0.11173785524723258
iteration : 11644
train acc:  0.75
train loss:  0.4776504337787628
train gradient:  0.13527228780154238
iteration : 11645
train acc:  0.734375
train loss:  0.5272966027259827
train gradient:  0.14052898682820508
iteration : 11646
train acc:  0.7265625
train loss:  0.5225657224655151
train gradient:  0.14038272805040744
iteration : 11647
train acc:  0.7265625
train loss:  0.5183347463607788
train gradient:  0.13261066750493353
iteration : 11648
train acc:  0.796875
train loss:  0.4669349491596222
train gradient:  0.13347079426120034
iteration : 11649
train acc:  0.7578125
train loss:  0.45671316981315613
train gradient:  0.1388818777519507
iteration : 11650
train acc:  0.7421875
train loss:  0.48769786953926086
train gradient:  0.13028047609788518
iteration : 11651
train acc:  0.7734375
train loss:  0.48196572065353394
train gradient:  0.10503036623782863
iteration : 11652
train acc:  0.703125
train loss:  0.5711480379104614
train gradient:  0.16269531511620733
iteration : 11653
train acc:  0.6875
train loss:  0.5464061498641968
train gradient:  0.13005018840829863
iteration : 11654
train acc:  0.75
train loss:  0.4983384609222412
train gradient:  0.1311159931336493
iteration : 11655
train acc:  0.7265625
train loss:  0.4696299731731415
train gradient:  0.1263273905731272
iteration : 11656
train acc:  0.734375
train loss:  0.5017625093460083
train gradient:  0.16130155836859028
iteration : 11657
train acc:  0.734375
train loss:  0.457766056060791
train gradient:  0.1060565999881945
iteration : 11658
train acc:  0.7421875
train loss:  0.523421585559845
train gradient:  0.157969061510284
iteration : 11659
train acc:  0.734375
train loss:  0.5055781006813049
train gradient:  0.16559353120292808
iteration : 11660
train acc:  0.7734375
train loss:  0.48169228434562683
train gradient:  0.13675390061304354
iteration : 11661
train acc:  0.78125
train loss:  0.4891483783721924
train gradient:  0.10346586927772496
iteration : 11662
train acc:  0.796875
train loss:  0.40012800693511963
train gradient:  0.07676926593573738
iteration : 11663
train acc:  0.7578125
train loss:  0.5039241313934326
train gradient:  0.1257568613142147
iteration : 11664
train acc:  0.7734375
train loss:  0.41465920209884644
train gradient:  0.09576349272121021
iteration : 11665
train acc:  0.6875
train loss:  0.49641817808151245
train gradient:  0.13228622669655649
iteration : 11666
train acc:  0.7734375
train loss:  0.4192810356616974
train gradient:  0.1043617325584104
iteration : 11667
train acc:  0.71875
train loss:  0.5367041826248169
train gradient:  0.15993886288713882
iteration : 11668
train acc:  0.75
train loss:  0.4919842481613159
train gradient:  0.11287915153942654
iteration : 11669
train acc:  0.734375
train loss:  0.5241049528121948
train gradient:  0.13100365143033554
iteration : 11670
train acc:  0.7421875
train loss:  0.4861908555030823
train gradient:  0.11426346423773258
iteration : 11671
train acc:  0.796875
train loss:  0.44603049755096436
train gradient:  0.1110130554182533
iteration : 11672
train acc:  0.65625
train loss:  0.5590015649795532
train gradient:  0.1399227180735426
iteration : 11673
train acc:  0.78125
train loss:  0.4639749526977539
train gradient:  0.13801947098287853
iteration : 11674
train acc:  0.7265625
train loss:  0.5343145132064819
train gradient:  0.1493034428011638
iteration : 11675
train acc:  0.796875
train loss:  0.44865283370018005
train gradient:  0.09115331604409795
iteration : 11676
train acc:  0.7265625
train loss:  0.478483110666275
train gradient:  0.1201326313078502
iteration : 11677
train acc:  0.765625
train loss:  0.477301687002182
train gradient:  0.1538053300714637
iteration : 11678
train acc:  0.71875
train loss:  0.5134469270706177
train gradient:  0.12301016362898053
iteration : 11679
train acc:  0.7421875
train loss:  0.49594929814338684
train gradient:  0.1326966756113807
iteration : 11680
train acc:  0.7109375
train loss:  0.5268987417221069
train gradient:  0.13726988603023257
iteration : 11681
train acc:  0.7265625
train loss:  0.5370670557022095
train gradient:  0.13355065546430755
iteration : 11682
train acc:  0.7421875
train loss:  0.501002311706543
train gradient:  0.12080190812763149
iteration : 11683
train acc:  0.7265625
train loss:  0.4772868752479553
train gradient:  0.11496561847137311
iteration : 11684
train acc:  0.734375
train loss:  0.4575146734714508
train gradient:  0.10733046172025182
iteration : 11685
train acc:  0.7421875
train loss:  0.469105064868927
train gradient:  0.11797166806279219
iteration : 11686
train acc:  0.8203125
train loss:  0.43827077746391296
train gradient:  0.10420811784176144
iteration : 11687
train acc:  0.7734375
train loss:  0.46003296971321106
train gradient:  0.09582871078607096
iteration : 11688
train acc:  0.7109375
train loss:  0.5259729623794556
train gradient:  0.11486345160979956
iteration : 11689
train acc:  0.765625
train loss:  0.4999609887599945
train gradient:  0.1339050023035387
iteration : 11690
train acc:  0.765625
train loss:  0.4560461938381195
train gradient:  0.11860263483255994
iteration : 11691
train acc:  0.78125
train loss:  0.44039586186408997
train gradient:  0.09433029816302174
iteration : 11692
train acc:  0.7109375
train loss:  0.5687367916107178
train gradient:  0.14464000064070143
iteration : 11693
train acc:  0.625
train loss:  0.5536658763885498
train gradient:  0.14903951147180633
iteration : 11694
train acc:  0.7421875
train loss:  0.5332431793212891
train gradient:  0.14435761360793536
iteration : 11695
train acc:  0.75
train loss:  0.5044448971748352
train gradient:  0.11336609703757142
iteration : 11696
train acc:  0.7421875
train loss:  0.4597804546356201
train gradient:  0.09625510233290038
iteration : 11697
train acc:  0.71875
train loss:  0.5308118462562561
train gradient:  0.17038230637214694
iteration : 11698
train acc:  0.6796875
train loss:  0.5283657312393188
train gradient:  0.16207668338162567
iteration : 11699
train acc:  0.7890625
train loss:  0.4897209405899048
train gradient:  0.14544610073878703
iteration : 11700
train acc:  0.75
train loss:  0.492677241563797
train gradient:  0.11621460856698261
iteration : 11701
train acc:  0.734375
train loss:  0.5006295442581177
train gradient:  0.1336907384664622
iteration : 11702
train acc:  0.6953125
train loss:  0.506194531917572
train gradient:  0.13572197939632724
iteration : 11703
train acc:  0.765625
train loss:  0.477344274520874
train gradient:  0.11062572894495468
iteration : 11704
train acc:  0.7109375
train loss:  0.5500800609588623
train gradient:  0.14753882663333662
iteration : 11705
train acc:  0.7109375
train loss:  0.5362802147865295
train gradient:  0.12675317547368048
iteration : 11706
train acc:  0.6484375
train loss:  0.5344861745834351
train gradient:  0.15335129739267844
iteration : 11707
train acc:  0.7734375
train loss:  0.5085246562957764
train gradient:  0.1541307132469297
iteration : 11708
train acc:  0.7421875
train loss:  0.48292839527130127
train gradient:  0.11552274264120928
iteration : 11709
train acc:  0.765625
train loss:  0.4477156102657318
train gradient:  0.10521472677389129
iteration : 11710
train acc:  0.7421875
train loss:  0.49490243196487427
train gradient:  0.129030214644347
iteration : 11711
train acc:  0.71875
train loss:  0.5179581642150879
train gradient:  0.10276050346693433
iteration : 11712
train acc:  0.75
train loss:  0.5124706029891968
train gradient:  0.11725383874220136
iteration : 11713
train acc:  0.6953125
train loss:  0.5014448761940002
train gradient:  0.13346024056317252
iteration : 11714
train acc:  0.6953125
train loss:  0.5452792644500732
train gradient:  0.14921357339620397
iteration : 11715
train acc:  0.6875
train loss:  0.5403739213943481
train gradient:  0.156837546401978
iteration : 11716
train acc:  0.734375
train loss:  0.5214285850524902
train gradient:  0.11589166840058222
iteration : 11717
train acc:  0.765625
train loss:  0.5067909359931946
train gradient:  0.17001712792315263
iteration : 11718
train acc:  0.7734375
train loss:  0.4827933609485626
train gradient:  0.1086164200203302
iteration : 11719
train acc:  0.7578125
train loss:  0.46672961115837097
train gradient:  0.10614081665407103
iteration : 11720
train acc:  0.765625
train loss:  0.42984744906425476
train gradient:  0.10056597969879834
iteration : 11721
train acc:  0.6953125
train loss:  0.5009031295776367
train gradient:  0.1267355907857436
iteration : 11722
train acc:  0.734375
train loss:  0.5230404138565063
train gradient:  0.14244998724481672
iteration : 11723
train acc:  0.7109375
train loss:  0.49881523847579956
train gradient:  0.152951186215139
iteration : 11724
train acc:  0.7421875
train loss:  0.5003345608711243
train gradient:  0.1344324691929162
iteration : 11725
train acc:  0.7890625
train loss:  0.46788716316223145
train gradient:  0.10706190715653315
iteration : 11726
train acc:  0.8125
train loss:  0.42440366744995117
train gradient:  0.11806073929227358
iteration : 11727
train acc:  0.75
train loss:  0.44641125202178955
train gradient:  0.09669974141390064
iteration : 11728
train acc:  0.7734375
train loss:  0.4356703758239746
train gradient:  0.11358295120098927
iteration : 11729
train acc:  0.8046875
train loss:  0.46834492683410645
train gradient:  0.09027406520521816
iteration : 11730
train acc:  0.734375
train loss:  0.4747295081615448
train gradient:  0.11775659513529103
iteration : 11731
train acc:  0.6953125
train loss:  0.5274733304977417
train gradient:  0.13698448252126505
iteration : 11732
train acc:  0.8125
train loss:  0.39842793345451355
train gradient:  0.0754892395416359
iteration : 11733
train acc:  0.8125
train loss:  0.4294990301132202
train gradient:  0.10366353525654116
iteration : 11734
train acc:  0.78125
train loss:  0.40671366453170776
train gradient:  0.08922283377095129
iteration : 11735
train acc:  0.7578125
train loss:  0.5005738735198975
train gradient:  0.12116560558456499
iteration : 11736
train acc:  0.765625
train loss:  0.4896741211414337
train gradient:  0.10783937025061192
iteration : 11737
train acc:  0.78125
train loss:  0.4408647418022156
train gradient:  0.10260022536901581
iteration : 11738
train acc:  0.765625
train loss:  0.42583706974983215
train gradient:  0.10147189061535514
iteration : 11739
train acc:  0.6953125
train loss:  0.5082727074623108
train gradient:  0.13807729746273523
iteration : 11740
train acc:  0.734375
train loss:  0.4877662658691406
train gradient:  0.1085169990038015
iteration : 11741
train acc:  0.71875
train loss:  0.4782039523124695
train gradient:  0.12078993998458255
iteration : 11742
train acc:  0.75
train loss:  0.5343742370605469
train gradient:  0.14635778666199922
iteration : 11743
train acc:  0.6953125
train loss:  0.5908745527267456
train gradient:  0.18872148568440208
iteration : 11744
train acc:  0.7265625
train loss:  0.4623033404350281
train gradient:  0.11647013108991658
iteration : 11745
train acc:  0.7578125
train loss:  0.4854733645915985
train gradient:  0.10597630071818145
iteration : 11746
train acc:  0.7109375
train loss:  0.5134260058403015
train gradient:  0.13960093212446223
iteration : 11747
train acc:  0.765625
train loss:  0.48109927773475647
train gradient:  0.11672124016617193
iteration : 11748
train acc:  0.7734375
train loss:  0.5083781480789185
train gradient:  0.13748569822342915
iteration : 11749
train acc:  0.734375
train loss:  0.5110914707183838
train gradient:  0.16310445836688647
iteration : 11750
train acc:  0.75
train loss:  0.46736329793930054
train gradient:  0.09462898556298886
iteration : 11751
train acc:  0.71875
train loss:  0.5153052806854248
train gradient:  0.1158359396867409
iteration : 11752
train acc:  0.734375
train loss:  0.5483815670013428
train gradient:  0.14751848705021373
iteration : 11753
train acc:  0.7890625
train loss:  0.4240407347679138
train gradient:  0.08713602866962511
iteration : 11754
train acc:  0.75
train loss:  0.5607911348342896
train gradient:  0.14661356493220548
iteration : 11755
train acc:  0.7265625
train loss:  0.46455031633377075
train gradient:  0.1122218547528202
iteration : 11756
train acc:  0.78125
train loss:  0.45573586225509644
train gradient:  0.09586666735186396
iteration : 11757
train acc:  0.796875
train loss:  0.4317207336425781
train gradient:  0.1371621262657709
iteration : 11758
train acc:  0.7578125
train loss:  0.5022035837173462
train gradient:  0.1599235859748223
iteration : 11759
train acc:  0.7421875
train loss:  0.4633687734603882
train gradient:  0.09663376091389843
iteration : 11760
train acc:  0.8203125
train loss:  0.39704734086990356
train gradient:  0.09254545008285055
iteration : 11761
train acc:  0.75
train loss:  0.5070010423660278
train gradient:  0.13415737234054986
iteration : 11762
train acc:  0.75
train loss:  0.49770021438598633
train gradient:  0.14541149428708922
iteration : 11763
train acc:  0.6640625
train loss:  0.547453761100769
train gradient:  0.14732685912231905
iteration : 11764
train acc:  0.734375
train loss:  0.47607022523880005
train gradient:  0.14349385876223456
iteration : 11765
train acc:  0.765625
train loss:  0.47047168016433716
train gradient:  0.1291781947473415
iteration : 11766
train acc:  0.75
train loss:  0.47297215461730957
train gradient:  0.11228473931987346
iteration : 11767
train acc:  0.7578125
train loss:  0.4565357267856598
train gradient:  0.10940592765789091
iteration : 11768
train acc:  0.7890625
train loss:  0.4741446077823639
train gradient:  0.12214146282295107
iteration : 11769
train acc:  0.71875
train loss:  0.5716573596000671
train gradient:  0.1588509221097091
iteration : 11770
train acc:  0.796875
train loss:  0.5011664628982544
train gradient:  0.15621316021268986
iteration : 11771
train acc:  0.7421875
train loss:  0.4951365292072296
train gradient:  0.1147178349341468
iteration : 11772
train acc:  0.78125
train loss:  0.4700966477394104
train gradient:  0.11202082401158693
iteration : 11773
train acc:  0.84375
train loss:  0.4378287196159363
train gradient:  0.13797710998417653
iteration : 11774
train acc:  0.6953125
train loss:  0.5365025401115417
train gradient:  0.16523517218451206
iteration : 11775
train acc:  0.6796875
train loss:  0.5328658223152161
train gradient:  0.13165077261407898
iteration : 11776
train acc:  0.7265625
train loss:  0.5158493518829346
train gradient:  0.1307675625703991
iteration : 11777
train acc:  0.6953125
train loss:  0.6227173805236816
train gradient:  0.1823347807577682
iteration : 11778
train acc:  0.78125
train loss:  0.4560993015766144
train gradient:  0.09335107916131535
iteration : 11779
train acc:  0.7578125
train loss:  0.45126664638519287
train gradient:  0.10321871994384434
iteration : 11780
train acc:  0.765625
train loss:  0.4456471800804138
train gradient:  0.11618206154709468
iteration : 11781
train acc:  0.7890625
train loss:  0.4381014108657837
train gradient:  0.10822640440640383
iteration : 11782
train acc:  0.7890625
train loss:  0.45167893171310425
train gradient:  0.12820441897381438
iteration : 11783
train acc:  0.7578125
train loss:  0.4641154706478119
train gradient:  0.1162192312641801
iteration : 11784
train acc:  0.7109375
train loss:  0.5624229907989502
train gradient:  0.1393515441338296
iteration : 11785
train acc:  0.828125
train loss:  0.45772525668144226
train gradient:  0.10540687525027456
iteration : 11786
train acc:  0.78125
train loss:  0.4843134880065918
train gradient:  0.15356601468310344
iteration : 11787
train acc:  0.71875
train loss:  0.46617865562438965
train gradient:  0.11356667324539234
iteration : 11788
train acc:  0.75
train loss:  0.45364540815353394
train gradient:  0.108186591955419
iteration : 11789
train acc:  0.75
train loss:  0.5012788772583008
train gradient:  0.1502272918297079
iteration : 11790
train acc:  0.7578125
train loss:  0.5436033010482788
train gradient:  0.14926104072643853
iteration : 11791
train acc:  0.7890625
train loss:  0.47042691707611084
train gradient:  0.12035858309952081
iteration : 11792
train acc:  0.78125
train loss:  0.45346799492836
train gradient:  0.09453975315830243
iteration : 11793
train acc:  0.7265625
train loss:  0.5018624067306519
train gradient:  0.13187900075699394
iteration : 11794
train acc:  0.71875
train loss:  0.481524795293808
train gradient:  0.10789221587341905
iteration : 11795
train acc:  0.7109375
train loss:  0.5473217964172363
train gradient:  0.1437055393874413
iteration : 11796
train acc:  0.7265625
train loss:  0.48361736536026
train gradient:  0.13019692907874902
iteration : 11797
train acc:  0.75
train loss:  0.48157578706741333
train gradient:  0.11973438879739265
iteration : 11798
train acc:  0.6953125
train loss:  0.5112450122833252
train gradient:  0.12067146379474726
iteration : 11799
train acc:  0.7421875
train loss:  0.48310813307762146
train gradient:  0.12675287800697255
iteration : 11800
train acc:  0.7421875
train loss:  0.4624495804309845
train gradient:  0.13747261898698881
iteration : 11801
train acc:  0.765625
train loss:  0.48827069997787476
train gradient:  0.11071171271466343
iteration : 11802
train acc:  0.8046875
train loss:  0.4264864921569824
train gradient:  0.10590483656733485
iteration : 11803
train acc:  0.71875
train loss:  0.5024386644363403
train gradient:  0.15708028349617653
iteration : 11804
train acc:  0.6875
train loss:  0.5415650606155396
train gradient:  0.13029713678618943
iteration : 11805
train acc:  0.7421875
train loss:  0.45996716618537903
train gradient:  0.09936217010231663
iteration : 11806
train acc:  0.7421875
train loss:  0.5060302019119263
train gradient:  0.11790448233469417
iteration : 11807
train acc:  0.7734375
train loss:  0.42920830845832825
train gradient:  0.09915043977440065
iteration : 11808
train acc:  0.828125
train loss:  0.42633992433547974
train gradient:  0.09214243071888913
iteration : 11809
train acc:  0.7890625
train loss:  0.46429821848869324
train gradient:  0.11219702924613104
iteration : 11810
train acc:  0.765625
train loss:  0.44291073083877563
train gradient:  0.10849550435475327
iteration : 11811
train acc:  0.7734375
train loss:  0.41306644678115845
train gradient:  0.0970002080992375
iteration : 11812
train acc:  0.765625
train loss:  0.4796201288700104
train gradient:  0.14532506236988235
iteration : 11813
train acc:  0.65625
train loss:  0.5877078175544739
train gradient:  0.1713199927496527
iteration : 11814
train acc:  0.7734375
train loss:  0.5028290748596191
train gradient:  0.14877915148811438
iteration : 11815
train acc:  0.7734375
train loss:  0.4682561159133911
train gradient:  0.1399432869332514
iteration : 11816
train acc:  0.75
train loss:  0.5143229961395264
train gradient:  0.13520240276123596
iteration : 11817
train acc:  0.7890625
train loss:  0.4168827533721924
train gradient:  0.0966826573273818
iteration : 11818
train acc:  0.7421875
train loss:  0.48583269119262695
train gradient:  0.11984841221168895
iteration : 11819
train acc:  0.796875
train loss:  0.4580458402633667
train gradient:  0.10433682817887813
iteration : 11820
train acc:  0.71875
train loss:  0.49957603216171265
train gradient:  0.1420665734514211
iteration : 11821
train acc:  0.7265625
train loss:  0.5033327341079712
train gradient:  0.1173250194383131
iteration : 11822
train acc:  0.71875
train loss:  0.5001674890518188
train gradient:  0.1459664936897198
iteration : 11823
train acc:  0.765625
train loss:  0.4847571551799774
train gradient:  0.12421491732148163
iteration : 11824
train acc:  0.7578125
train loss:  0.4624286890029907
train gradient:  0.13770846695223504
iteration : 11825
train acc:  0.734375
train loss:  0.47865360975265503
train gradient:  0.12047299566144964
iteration : 11826
train acc:  0.71875
train loss:  0.5306354761123657
train gradient:  0.13770320749475368
iteration : 11827
train acc:  0.78125
train loss:  0.4528905153274536
train gradient:  0.1177205376905483
iteration : 11828
train acc:  0.7109375
train loss:  0.518657922744751
train gradient:  0.14070188389669824
iteration : 11829
train acc:  0.6796875
train loss:  0.5760378837585449
train gradient:  0.1417540797148366
iteration : 11830
train acc:  0.75
train loss:  0.48704174160957336
train gradient:  0.12400066948569574
iteration : 11831
train acc:  0.8125
train loss:  0.4487778842449188
train gradient:  0.12064081787285155
iteration : 11832
train acc:  0.6796875
train loss:  0.5774315595626831
train gradient:  0.18223622884176494
iteration : 11833
train acc:  0.8046875
train loss:  0.4583672881126404
train gradient:  0.10853964486173011
iteration : 11834
train acc:  0.7109375
train loss:  0.4976496696472168
train gradient:  0.11378678343158481
iteration : 11835
train acc:  0.71875
train loss:  0.5283658504486084
train gradient:  0.16007878079424678
iteration : 11836
train acc:  0.7734375
train loss:  0.47454017400741577
train gradient:  0.1374254705264547
iteration : 11837
train acc:  0.75
train loss:  0.49321311712265015
train gradient:  0.11373271420475112
iteration : 11838
train acc:  0.65625
train loss:  0.6321128606796265
train gradient:  0.19110411167265226
iteration : 11839
train acc:  0.7109375
train loss:  0.5054816007614136
train gradient:  0.14101832757735144
iteration : 11840
train acc:  0.7890625
train loss:  0.47393572330474854
train gradient:  0.11606218475644368
iteration : 11841
train acc:  0.765625
train loss:  0.47221168875694275
train gradient:  0.10789817685769265
iteration : 11842
train acc:  0.7421875
train loss:  0.5151136517524719
train gradient:  0.1473963077485258
iteration : 11843
train acc:  0.7421875
train loss:  0.4016883373260498
train gradient:  0.08684270798107052
iteration : 11844
train acc:  0.71875
train loss:  0.4982941150665283
train gradient:  0.12378517477128145
iteration : 11845
train acc:  0.7265625
train loss:  0.5018684267997742
train gradient:  0.14437279399270458
iteration : 11846
train acc:  0.7734375
train loss:  0.5042004585266113
train gradient:  0.13976085493355916
iteration : 11847
train acc:  0.734375
train loss:  0.49858418107032776
train gradient:  0.15498819294827176
iteration : 11848
train acc:  0.75
train loss:  0.48555099964141846
train gradient:  0.11221205647619079
iteration : 11849
train acc:  0.6484375
train loss:  0.621555745601654
train gradient:  0.1621480104763392
iteration : 11850
train acc:  0.75
train loss:  0.520709216594696
train gradient:  0.15662214944367625
iteration : 11851
train acc:  0.765625
train loss:  0.431793749332428
train gradient:  0.10216305148831716
iteration : 11852
train acc:  0.7421875
train loss:  0.4937504529953003
train gradient:  0.14062935206202473
iteration : 11853
train acc:  0.796875
train loss:  0.4692104160785675
train gradient:  0.12835626438809702
iteration : 11854
train acc:  0.8125
train loss:  0.4643581509590149
train gradient:  0.13127979321856204
iteration : 11855
train acc:  0.734375
train loss:  0.5066030025482178
train gradient:  0.10277955957888098
iteration : 11856
train acc:  0.7265625
train loss:  0.5494790077209473
train gradient:  0.1443121475815846
iteration : 11857
train acc:  0.7421875
train loss:  0.47500622272491455
train gradient:  0.11210018932855323
iteration : 11858
train acc:  0.6640625
train loss:  0.5591236352920532
train gradient:  0.1362532553097901
iteration : 11859
train acc:  0.6953125
train loss:  0.5046434998512268
train gradient:  0.15706969595853393
iteration : 11860
train acc:  0.7265625
train loss:  0.5029792785644531
train gradient:  0.13114841197290944
iteration : 11861
train acc:  0.796875
train loss:  0.4526054263114929
train gradient:  0.08852542622536666
iteration : 11862
train acc:  0.7734375
train loss:  0.49290570616722107
train gradient:  0.12982222567959462
iteration : 11863
train acc:  0.75
train loss:  0.5470305681228638
train gradient:  0.11996254833455962
iteration : 11864
train acc:  0.7578125
train loss:  0.5105832815170288
train gradient:  0.11791613904202763
iteration : 11865
train acc:  0.7890625
train loss:  0.4347095787525177
train gradient:  0.09546364608479152
iteration : 11866
train acc:  0.796875
train loss:  0.4009915590286255
train gradient:  0.10780076786965544
iteration : 11867
train acc:  0.703125
train loss:  0.4921174943447113
train gradient:  0.11679579850782966
iteration : 11868
train acc:  0.8125
train loss:  0.431937575340271
train gradient:  0.11199146789937779
iteration : 11869
train acc:  0.796875
train loss:  0.4273774325847626
train gradient:  0.08453638850615956
iteration : 11870
train acc:  0.7265625
train loss:  0.5180875062942505
train gradient:  0.11584457456395493
iteration : 11871
train acc:  0.7734375
train loss:  0.4628092646598816
train gradient:  0.1564859371373899
iteration : 11872
train acc:  0.7578125
train loss:  0.43288207054138184
train gradient:  0.08920793801797672
iteration : 11873
train acc:  0.765625
train loss:  0.5183419585227966
train gradient:  0.12088737714322559
iteration : 11874
train acc:  0.7890625
train loss:  0.43661969900131226
train gradient:  0.10628233993997832
iteration : 11875
train acc:  0.7421875
train loss:  0.4642465114593506
train gradient:  0.10805567000452979
iteration : 11876
train acc:  0.7109375
train loss:  0.5298494100570679
train gradient:  0.13554381420277997
iteration : 11877
train acc:  0.78125
train loss:  0.4842469394207001
train gradient:  0.12739391056263316
iteration : 11878
train acc:  0.7578125
train loss:  0.4941306710243225
train gradient:  0.13284446836091998
iteration : 11879
train acc:  0.734375
train loss:  0.4992154538631439
train gradient:  0.14262248979117348
iteration : 11880
train acc:  0.78125
train loss:  0.4877336025238037
train gradient:  0.11550065284054419
iteration : 11881
train acc:  0.765625
train loss:  0.42583349347114563
train gradient:  0.10875607983774037
iteration : 11882
train acc:  0.7578125
train loss:  0.4987576901912689
train gradient:  0.11741133362300737
iteration : 11883
train acc:  0.703125
train loss:  0.5499317646026611
train gradient:  0.1652073126227693
iteration : 11884
train acc:  0.75
train loss:  0.5042058825492859
train gradient:  0.14573951943854263
iteration : 11885
train acc:  0.6875
train loss:  0.5327421426773071
train gradient:  0.11631977253001702
iteration : 11886
train acc:  0.765625
train loss:  0.5072029829025269
train gradient:  0.11521135958826272
iteration : 11887
train acc:  0.7734375
train loss:  0.5171344876289368
train gradient:  0.1476632748318763
iteration : 11888
train acc:  0.7421875
train loss:  0.5206036567687988
train gradient:  0.13837025204655545
iteration : 11889
train acc:  0.6953125
train loss:  0.49973976612091064
train gradient:  0.11377706669847648
iteration : 11890
train acc:  0.7734375
train loss:  0.4337056875228882
train gradient:  0.09148875081481993
iteration : 11891
train acc:  0.78125
train loss:  0.4102478325366974
train gradient:  0.09116285176662063
iteration : 11892
train acc:  0.7734375
train loss:  0.4881713092327118
train gradient:  0.13794458946497212
iteration : 11893
train acc:  0.765625
train loss:  0.48968860507011414
train gradient:  0.12339360083494624
iteration : 11894
train acc:  0.7109375
train loss:  0.5458682179450989
train gradient:  0.13398852681948745
iteration : 11895
train acc:  0.75
train loss:  0.5460056066513062
train gradient:  0.16997695777336785
iteration : 11896
train acc:  0.7265625
train loss:  0.5211906433105469
train gradient:  0.12362283450223897
iteration : 11897
train acc:  0.7578125
train loss:  0.4588744044303894
train gradient:  0.12384550585441846
iteration : 11898
train acc:  0.7734375
train loss:  0.4708607792854309
train gradient:  0.12083779659637692
iteration : 11899
train acc:  0.6796875
train loss:  0.5627161264419556
train gradient:  0.15612382269530006
iteration : 11900
train acc:  0.7578125
train loss:  0.4679606258869171
train gradient:  0.11696233716849015
iteration : 11901
train acc:  0.7578125
train loss:  0.503424882888794
train gradient:  0.10781624669527327
iteration : 11902
train acc:  0.7578125
train loss:  0.45143863558769226
train gradient:  0.09214285745068809
iteration : 11903
train acc:  0.7421875
train loss:  0.4447210431098938
train gradient:  0.11347262447932009
iteration : 11904
train acc:  0.7734375
train loss:  0.4410986304283142
train gradient:  0.11197582472364155
iteration : 11905
train acc:  0.8046875
train loss:  0.4422347843647003
train gradient:  0.10229915784329867
iteration : 11906
train acc:  0.75
train loss:  0.5094187259674072
train gradient:  0.13575475444335905
iteration : 11907
train acc:  0.7421875
train loss:  0.43945133686065674
train gradient:  0.08890888847343781
iteration : 11908
train acc:  0.8125
train loss:  0.42375364899635315
train gradient:  0.0922630833426243
iteration : 11909
train acc:  0.6953125
train loss:  0.5441414713859558
train gradient:  0.11150349624009134
iteration : 11910
train acc:  0.796875
train loss:  0.4234898090362549
train gradient:  0.09603126299554912
iteration : 11911
train acc:  0.734375
train loss:  0.506661057472229
train gradient:  0.11412397067442344
iteration : 11912
train acc:  0.6953125
train loss:  0.5313407182693481
train gradient:  0.16383840484187495
iteration : 11913
train acc:  0.703125
train loss:  0.5035272240638733
train gradient:  0.12945457180995967
iteration : 11914
train acc:  0.7734375
train loss:  0.45837950706481934
train gradient:  0.10057019376469219
iteration : 11915
train acc:  0.796875
train loss:  0.481837660074234
train gradient:  0.08994271253735783
iteration : 11916
train acc:  0.71875
train loss:  0.5319069623947144
train gradient:  0.16267903450388413
iteration : 11917
train acc:  0.765625
train loss:  0.486216276884079
train gradient:  0.14632661876713127
iteration : 11918
train acc:  0.734375
train loss:  0.4967050552368164
train gradient:  0.18320113155502105
iteration : 11919
train acc:  0.7421875
train loss:  0.4558659493923187
train gradient:  0.11990111561503848
iteration : 11920
train acc:  0.75
train loss:  0.5400753021240234
train gradient:  0.21775501227818808
iteration : 11921
train acc:  0.78125
train loss:  0.4493156671524048
train gradient:  0.1014462540479935
iteration : 11922
train acc:  0.7421875
train loss:  0.469853937625885
train gradient:  0.1405075563375343
iteration : 11923
train acc:  0.78125
train loss:  0.5166383385658264
train gradient:  0.10982443769699306
iteration : 11924
train acc:  0.7109375
train loss:  0.5425219535827637
train gradient:  0.1591565022620016
iteration : 11925
train acc:  0.8203125
train loss:  0.3882635235786438
train gradient:  0.07376889018964239
iteration : 11926
train acc:  0.7890625
train loss:  0.4400201439857483
train gradient:  0.10492328546129646
iteration : 11927
train acc:  0.7578125
train loss:  0.494404137134552
train gradient:  0.15283144587169092
iteration : 11928
train acc:  0.6953125
train loss:  0.574540376663208
train gradient:  0.16702497891955337
iteration : 11929
train acc:  0.734375
train loss:  0.5162526369094849
train gradient:  0.13362592975903143
iteration : 11930
train acc:  0.734375
train loss:  0.5282285213470459
train gradient:  0.13047279898025427
iteration : 11931
train acc:  0.7578125
train loss:  0.5176520347595215
train gradient:  0.14155702931537445
iteration : 11932
train acc:  0.8125
train loss:  0.4525912404060364
train gradient:  0.1112161618108967
iteration : 11933
train acc:  0.703125
train loss:  0.5868418216705322
train gradient:  0.15536633261439275
iteration : 11934
train acc:  0.8203125
train loss:  0.3964545428752899
train gradient:  0.0811706252800614
iteration : 11935
train acc:  0.78125
train loss:  0.4585196077823639
train gradient:  0.11046814425627884
iteration : 11936
train acc:  0.7890625
train loss:  0.45258957147598267
train gradient:  0.11114405327746638
iteration : 11937
train acc:  0.734375
train loss:  0.47397780418395996
train gradient:  0.09360518256161363
iteration : 11938
train acc:  0.7421875
train loss:  0.5082501769065857
train gradient:  0.1658737733756253
iteration : 11939
train acc:  0.75
train loss:  0.46956077218055725
train gradient:  0.1106671758308717
iteration : 11940
train acc:  0.65625
train loss:  0.5811071395874023
train gradient:  0.16027617875172856
iteration : 11941
train acc:  0.7421875
train loss:  0.5005134344100952
train gradient:  0.10633063347984192
iteration : 11942
train acc:  0.7890625
train loss:  0.4605041444301605
train gradient:  0.11750096979670897
iteration : 11943
train acc:  0.6875
train loss:  0.49853062629699707
train gradient:  0.12063707626937274
iteration : 11944
train acc:  0.7890625
train loss:  0.480190634727478
train gradient:  0.10907621473884005
iteration : 11945
train acc:  0.7578125
train loss:  0.4956289827823639
train gradient:  0.12079559583283567
iteration : 11946
train acc:  0.796875
train loss:  0.4041890501976013
train gradient:  0.086411110411763
iteration : 11947
train acc:  0.78125
train loss:  0.45333215594291687
train gradient:  0.12258925825813059
iteration : 11948
train acc:  0.75
train loss:  0.5016084313392639
train gradient:  0.147370833858178
iteration : 11949
train acc:  0.7421875
train loss:  0.42033544182777405
train gradient:  0.07799754588466636
iteration : 11950
train acc:  0.7265625
train loss:  0.5444572567939758
train gradient:  0.12549105239320701
iteration : 11951
train acc:  0.8125
train loss:  0.4409327507019043
train gradient:  0.1084591912244808
iteration : 11952
train acc:  0.8203125
train loss:  0.47485417127609253
train gradient:  0.14130066652042536
iteration : 11953
train acc:  0.6796875
train loss:  0.5858871936798096
train gradient:  0.20033450808016112
iteration : 11954
train acc:  0.75
train loss:  0.470664381980896
train gradient:  0.1173674012744947
iteration : 11955
train acc:  0.6953125
train loss:  0.5687152147293091
train gradient:  0.14827575854367944
iteration : 11956
train acc:  0.6484375
train loss:  0.574378490447998
train gradient:  0.17394663770041477
iteration : 11957
train acc:  0.6640625
train loss:  0.5887433886528015
train gradient:  0.21782923680694932
iteration : 11958
train acc:  0.765625
train loss:  0.4620457887649536
train gradient:  0.11877896493495263
iteration : 11959
train acc:  0.7265625
train loss:  0.47778308391571045
train gradient:  0.1275319028981859
iteration : 11960
train acc:  0.7265625
train loss:  0.5160902142524719
train gradient:  0.16542818601065207
iteration : 11961
train acc:  0.71875
train loss:  0.5253416299819946
train gradient:  0.13056877596545213
iteration : 11962
train acc:  0.734375
train loss:  0.4871743619441986
train gradient:  0.11884538851750741
iteration : 11963
train acc:  0.78125
train loss:  0.4575856626033783
train gradient:  0.1447290453010139
iteration : 11964
train acc:  0.8046875
train loss:  0.41727182269096375
train gradient:  0.0947801156224756
iteration : 11965
train acc:  0.7578125
train loss:  0.5341585874557495
train gradient:  0.12695616350427177
iteration : 11966
train acc:  0.734375
train loss:  0.5258748531341553
train gradient:  0.16988733182538246
iteration : 11967
train acc:  0.7265625
train loss:  0.4810159206390381
train gradient:  0.11324339533427323
iteration : 11968
train acc:  0.7109375
train loss:  0.5154833197593689
train gradient:  0.1242255254155147
iteration : 11969
train acc:  0.734375
train loss:  0.4399562478065491
train gradient:  0.09794978299984015
iteration : 11970
train acc:  0.78125
train loss:  0.4779261648654938
train gradient:  0.09827319107643688
iteration : 11971
train acc:  0.7109375
train loss:  0.5097833871841431
train gradient:  0.11368051599903414
iteration : 11972
train acc:  0.6953125
train loss:  0.5738060474395752
train gradient:  0.16371684265139602
iteration : 11973
train acc:  0.7421875
train loss:  0.5004838109016418
train gradient:  0.12779959042700384
iteration : 11974
train acc:  0.7421875
train loss:  0.4823605418205261
train gradient:  0.11056909811885603
iteration : 11975
train acc:  0.7421875
train loss:  0.5188889503479004
train gradient:  0.11352685967729491
iteration : 11976
train acc:  0.671875
train loss:  0.5404613018035889
train gradient:  0.12924845933845883
iteration : 11977
train acc:  0.7890625
train loss:  0.4393154978752136
train gradient:  0.14718710882803088
iteration : 11978
train acc:  0.7890625
train loss:  0.46278154850006104
train gradient:  0.12087135336209187
iteration : 11979
train acc:  0.8046875
train loss:  0.47162967920303345
train gradient:  0.11999027246722212
iteration : 11980
train acc:  0.71875
train loss:  0.5287007689476013
train gradient:  0.15238836466810443
iteration : 11981
train acc:  0.7578125
train loss:  0.4970051646232605
train gradient:  0.13385732774911074
iteration : 11982
train acc:  0.7421875
train loss:  0.4759799838066101
train gradient:  0.12293692902711441
iteration : 11983
train acc:  0.7109375
train loss:  0.5125901699066162
train gradient:  0.16756772455954522
iteration : 11984
train acc:  0.703125
train loss:  0.5147616863250732
train gradient:  0.12082142587293847
iteration : 11985
train acc:  0.7421875
train loss:  0.4712516665458679
train gradient:  0.12318132555629771
iteration : 11986
train acc:  0.7578125
train loss:  0.5030819177627563
train gradient:  0.15330004023014107
iteration : 11987
train acc:  0.765625
train loss:  0.4938790202140808
train gradient:  0.14043842407039125
iteration : 11988
train acc:  0.78125
train loss:  0.42923086881637573
train gradient:  0.09264897451528148
iteration : 11989
train acc:  0.6875
train loss:  0.5274859666824341
train gradient:  0.12807599616135087
iteration : 11990
train acc:  0.796875
train loss:  0.49220705032348633
train gradient:  0.14181154439025107
iteration : 11991
train acc:  0.7890625
train loss:  0.5098609924316406
train gradient:  0.11212454898272449
iteration : 11992
train acc:  0.75
train loss:  0.46271976828575134
train gradient:  0.09285766975055064
iteration : 11993
train acc:  0.71875
train loss:  0.5353217720985413
train gradient:  0.12728276719455228
iteration : 11994
train acc:  0.75
train loss:  0.47973552346229553
train gradient:  0.11673857776015482
iteration : 11995
train acc:  0.734375
train loss:  0.4779541492462158
train gradient:  0.13267293285581822
iteration : 11996
train acc:  0.8125
train loss:  0.47649431228637695
train gradient:  0.10356300440662027
iteration : 11997
train acc:  0.703125
train loss:  0.48922640085220337
train gradient:  0.13606025793556792
iteration : 11998
train acc:  0.8125
train loss:  0.4223303496837616
train gradient:  0.08259636032764422
iteration : 11999
train acc:  0.6953125
train loss:  0.5428727865219116
train gradient:  0.1679964206816597
iteration : 12000
train acc:  0.6953125
train loss:  0.4814041256904602
train gradient:  0.14219319756408305
iteration : 12001
train acc:  0.703125
train loss:  0.4988809823989868
train gradient:  0.12868393943627232
iteration : 12002
train acc:  0.703125
train loss:  0.534935474395752
train gradient:  0.1658452559213487
iteration : 12003
train acc:  0.7421875
train loss:  0.45111727714538574
train gradient:  0.12787649681208568
iteration : 12004
train acc:  0.6953125
train loss:  0.5103966593742371
train gradient:  0.13699381977071928
iteration : 12005
train acc:  0.7265625
train loss:  0.5146085619926453
train gradient:  0.12204373494946233
iteration : 12006
train acc:  0.7265625
train loss:  0.4614739716053009
train gradient:  0.1281578417658285
iteration : 12007
train acc:  0.7421875
train loss:  0.452871710062027
train gradient:  0.11330740956352812
iteration : 12008
train acc:  0.734375
train loss:  0.47446751594543457
train gradient:  0.1080888883982797
iteration : 12009
train acc:  0.6640625
train loss:  0.5505291223526001
train gradient:  0.16517130122331675
iteration : 12010
train acc:  0.703125
train loss:  0.5455262064933777
train gradient:  0.1420766541423076
iteration : 12011
train acc:  0.75
train loss:  0.5076502561569214
train gradient:  0.11093758040214918
iteration : 12012
train acc:  0.7109375
train loss:  0.5066742897033691
train gradient:  0.15344846296011172
iteration : 12013
train acc:  0.8046875
train loss:  0.41768527030944824
train gradient:  0.0863938157546926
iteration : 12014
train acc:  0.7265625
train loss:  0.530414342880249
train gradient:  0.1809824389129105
iteration : 12015
train acc:  0.8125
train loss:  0.4062308371067047
train gradient:  0.10147123542073429
iteration : 12016
train acc:  0.796875
train loss:  0.46991318464279175
train gradient:  0.13245961954845453
iteration : 12017
train acc:  0.7109375
train loss:  0.5036088228225708
train gradient:  0.11944256890088426
iteration : 12018
train acc:  0.8125
train loss:  0.4650377631187439
train gradient:  0.136081366179232
iteration : 12019
train acc:  0.75
train loss:  0.4766037166118622
train gradient:  0.1284033613330928
iteration : 12020
train acc:  0.6875
train loss:  0.5163276195526123
train gradient:  0.1419849305898345
iteration : 12021
train acc:  0.78125
train loss:  0.4510165750980377
train gradient:  0.0985259785994347
iteration : 12022
train acc:  0.7109375
train loss:  0.5121166110038757
train gradient:  0.1258707264746997
iteration : 12023
train acc:  0.84375
train loss:  0.41157716512680054
train gradient:  0.10922170097934754
iteration : 12024
train acc:  0.765625
train loss:  0.4474264085292816
train gradient:  0.10988166523871011
iteration : 12025
train acc:  0.78125
train loss:  0.4309726357460022
train gradient:  0.13402399216157507
iteration : 12026
train acc:  0.7890625
train loss:  0.4187254309654236
train gradient:  0.07698813258092108
iteration : 12027
train acc:  0.7421875
train loss:  0.45805883407592773
train gradient:  0.11894892857638019
iteration : 12028
train acc:  0.734375
train loss:  0.4392474293708801
train gradient:  0.09797429251230712
iteration : 12029
train acc:  0.7578125
train loss:  0.5370767116546631
train gradient:  0.11192219132836669
iteration : 12030
train acc:  0.75
train loss:  0.4856530427932739
train gradient:  0.145797271394758
iteration : 12031
train acc:  0.6796875
train loss:  0.6041091680526733
train gradient:  0.18349369558305606
iteration : 12032
train acc:  0.78125
train loss:  0.4424174427986145
train gradient:  0.12856916531751267
iteration : 12033
train acc:  0.7421875
train loss:  0.5407274961471558
train gradient:  0.12839297639010622
iteration : 12034
train acc:  0.78125
train loss:  0.4469883441925049
train gradient:  0.12307559038167581
iteration : 12035
train acc:  0.734375
train loss:  0.4756621718406677
train gradient:  0.11218210270976182
iteration : 12036
train acc:  0.8046875
train loss:  0.44432467222213745
train gradient:  0.09987054721698924
iteration : 12037
train acc:  0.7890625
train loss:  0.42291387915611267
train gradient:  0.08177283736834158
iteration : 12038
train acc:  0.796875
train loss:  0.4670853018760681
train gradient:  0.12322602769384736
iteration : 12039
train acc:  0.6953125
train loss:  0.5175002217292786
train gradient:  0.13734458399913674
iteration : 12040
train acc:  0.703125
train loss:  0.576412558555603
train gradient:  0.19834550506632523
iteration : 12041
train acc:  0.734375
train loss:  0.5130288600921631
train gradient:  0.11214561382740514
iteration : 12042
train acc:  0.7421875
train loss:  0.5591888427734375
train gradient:  0.14562800205049847
iteration : 12043
train acc:  0.8203125
train loss:  0.40002113580703735
train gradient:  0.09677253109141166
iteration : 12044
train acc:  0.734375
train loss:  0.5399439930915833
train gradient:  0.1529332472932991
iteration : 12045
train acc:  0.78125
train loss:  0.47974565625190735
train gradient:  0.1157275376979375
iteration : 12046
train acc:  0.7734375
train loss:  0.5171587467193604
train gradient:  0.12446853622412417
iteration : 12047
train acc:  0.671875
train loss:  0.5458410978317261
train gradient:  0.12751750625863661
iteration : 12048
train acc:  0.6171875
train loss:  0.6006054878234863
train gradient:  0.16587497505145732
iteration : 12049
train acc:  0.765625
train loss:  0.4665415287017822
train gradient:  0.11196588603467754
iteration : 12050
train acc:  0.7578125
train loss:  0.4654892683029175
train gradient:  0.10808579517985627
iteration : 12051
train acc:  0.71875
train loss:  0.49560970067977905
train gradient:  0.14303453181705672
iteration : 12052
train acc:  0.7421875
train loss:  0.536285936832428
train gradient:  0.12515372774589117
iteration : 12053
train acc:  0.7578125
train loss:  0.48978856205940247
train gradient:  0.12678115606171167
iteration : 12054
train acc:  0.7578125
train loss:  0.5035558938980103
train gradient:  0.15025071886328514
iteration : 12055
train acc:  0.7578125
train loss:  0.5047851800918579
train gradient:  0.15266560438235205
iteration : 12056
train acc:  0.703125
train loss:  0.5485103726387024
train gradient:  0.14911991710922262
iteration : 12057
train acc:  0.75
train loss:  0.46114057302474976
train gradient:  0.11030750928206923
iteration : 12058
train acc:  0.703125
train loss:  0.542205810546875
train gradient:  0.1513329247480975
iteration : 12059
train acc:  0.7265625
train loss:  0.5059573650360107
train gradient:  0.13274363259669203
iteration : 12060
train acc:  0.703125
train loss:  0.5795432925224304
train gradient:  0.15762463200785815
iteration : 12061
train acc:  0.7265625
train loss:  0.47136038541793823
train gradient:  0.09503436029083302
iteration : 12062
train acc:  0.7421875
train loss:  0.5145045518875122
train gradient:  0.11458674377182598
iteration : 12063
train acc:  0.7890625
train loss:  0.44600677490234375
train gradient:  0.11044701401608442
iteration : 12064
train acc:  0.8046875
train loss:  0.44632792472839355
train gradient:  0.0890884895482257
iteration : 12065
train acc:  0.765625
train loss:  0.46251851320266724
train gradient:  0.08614720031473953
iteration : 12066
train acc:  0.78125
train loss:  0.46240854263305664
train gradient:  0.11389079767287419
iteration : 12067
train acc:  0.734375
train loss:  0.47644054889678955
train gradient:  0.1076172169458567
iteration : 12068
train acc:  0.7890625
train loss:  0.453902006149292
train gradient:  0.10903031073878454
iteration : 12069
train acc:  0.6796875
train loss:  0.5134426355361938
train gradient:  0.12369421526761441
iteration : 12070
train acc:  0.796875
train loss:  0.4201211929321289
train gradient:  0.10583244917687035
iteration : 12071
train acc:  0.765625
train loss:  0.47779980301856995
train gradient:  0.09621461955238791
iteration : 12072
train acc:  0.703125
train loss:  0.5204123258590698
train gradient:  0.1455259762048547
iteration : 12073
train acc:  0.796875
train loss:  0.4544656276702881
train gradient:  0.10805785229518107
iteration : 12074
train acc:  0.8203125
train loss:  0.4108087420463562
train gradient:  0.08027684986995526
iteration : 12075
train acc:  0.671875
train loss:  0.5676093697547913
train gradient:  0.14214581831908302
iteration : 12076
train acc:  0.7734375
train loss:  0.4680492877960205
train gradient:  0.11102832569765193
iteration : 12077
train acc:  0.703125
train loss:  0.5138300657272339
train gradient:  0.15866948038658238
iteration : 12078
train acc:  0.7734375
train loss:  0.4475417733192444
train gradient:  0.11971782944436944
iteration : 12079
train acc:  0.75
train loss:  0.4602726697921753
train gradient:  0.10977275291397534
iteration : 12080
train acc:  0.6953125
train loss:  0.532230019569397
train gradient:  0.1446564918869222
iteration : 12081
train acc:  0.7421875
train loss:  0.4918869137763977
train gradient:  0.13528076336210274
iteration : 12082
train acc:  0.7265625
train loss:  0.5136993527412415
train gradient:  0.12531717136362483
iteration : 12083
train acc:  0.78125
train loss:  0.4568951427936554
train gradient:  0.09458509697948837
iteration : 12084
train acc:  0.7265625
train loss:  0.5000587701797485
train gradient:  0.1322346503706513
iteration : 12085
train acc:  0.7734375
train loss:  0.44884711503982544
train gradient:  0.11732179924664794
iteration : 12086
train acc:  0.7578125
train loss:  0.5173113346099854
train gradient:  0.16539152962867731
iteration : 12087
train acc:  0.7421875
train loss:  0.4993630051612854
train gradient:  0.14461067329982272
iteration : 12088
train acc:  0.796875
train loss:  0.4457949995994568
train gradient:  0.12258891557370116
iteration : 12089
train acc:  0.7421875
train loss:  0.5079762935638428
train gradient:  0.1040269816151356
iteration : 12090
train acc:  0.71875
train loss:  0.5374866724014282
train gradient:  0.13288702697927496
iteration : 12091
train acc:  0.7578125
train loss:  0.4917635917663574
train gradient:  0.14647119382727064
iteration : 12092
train acc:  0.75
train loss:  0.45875898003578186
train gradient:  0.10459106291061772
iteration : 12093
train acc:  0.7265625
train loss:  0.5148370265960693
train gradient:  0.137800750206448
iteration : 12094
train acc:  0.7578125
train loss:  0.4910902976989746
train gradient:  0.13583545973729924
iteration : 12095
train acc:  0.7421875
train loss:  0.4959203600883484
train gradient:  0.14623202466795082
iteration : 12096
train acc:  0.6953125
train loss:  0.5162249803543091
train gradient:  0.13321706542748812
iteration : 12097
train acc:  0.7734375
train loss:  0.46521681547164917
train gradient:  0.13469753408835455
iteration : 12098
train acc:  0.6796875
train loss:  0.5140333771705627
train gradient:  0.169992319319126
iteration : 12099
train acc:  0.734375
train loss:  0.45970532298088074
train gradient:  0.1034935205737807
iteration : 12100
train acc:  0.7421875
train loss:  0.5056973099708557
train gradient:  0.11642184424163991
iteration : 12101
train acc:  0.7578125
train loss:  0.4970755875110626
train gradient:  0.1316913793977582
iteration : 12102
train acc:  0.7109375
train loss:  0.5470370054244995
train gradient:  0.16151598602412304
iteration : 12103
train acc:  0.703125
train loss:  0.5199247598648071
train gradient:  0.12034423208590395
iteration : 12104
train acc:  0.71875
train loss:  0.5218622088432312
train gradient:  0.12513214146854568
iteration : 12105
train acc:  0.7578125
train loss:  0.4964221715927124
train gradient:  0.13966272744271777
iteration : 12106
train acc:  0.734375
train loss:  0.47830069065093994
train gradient:  0.15657195715731498
iteration : 12107
train acc:  0.7109375
train loss:  0.501262903213501
train gradient:  0.12476588565430619
iteration : 12108
train acc:  0.7734375
train loss:  0.43009263277053833
train gradient:  0.0892975152256761
iteration : 12109
train acc:  0.7578125
train loss:  0.46588218212127686
train gradient:  0.12096706439455424
iteration : 12110
train acc:  0.7890625
train loss:  0.48158276081085205
train gradient:  0.12159194809102618
iteration : 12111
train acc:  0.7109375
train loss:  0.5400776267051697
train gradient:  0.12637861438499742
iteration : 12112
train acc:  0.734375
train loss:  0.520523190498352
train gradient:  0.14352985690033077
iteration : 12113
train acc:  0.78125
train loss:  0.45496511459350586
train gradient:  0.12002611823983549
iteration : 12114
train acc:  0.7265625
train loss:  0.48286643624305725
train gradient:  0.11490288137508631
iteration : 12115
train acc:  0.734375
train loss:  0.5233917236328125
train gradient:  0.12051730518903007
iteration : 12116
train acc:  0.734375
train loss:  0.4831058382987976
train gradient:  0.11241286331573382
iteration : 12117
train acc:  0.7109375
train loss:  0.5324535369873047
train gradient:  0.15849963971469072
iteration : 12118
train acc:  0.7578125
train loss:  0.4330376386642456
train gradient:  0.10293459410581939
iteration : 12119
train acc:  0.6875
train loss:  0.6092472672462463
train gradient:  0.161005163747917
iteration : 12120
train acc:  0.71875
train loss:  0.4596363306045532
train gradient:  0.0866111197801164
iteration : 12121
train acc:  0.7109375
train loss:  0.5041982531547546
train gradient:  0.12037064697374572
iteration : 12122
train acc:  0.71875
train loss:  0.5391504764556885
train gradient:  0.14861066149386432
iteration : 12123
train acc:  0.8203125
train loss:  0.4237991273403168
train gradient:  0.12357872379542893
iteration : 12124
train acc:  0.7578125
train loss:  0.451701283454895
train gradient:  0.10382346164762965
iteration : 12125
train acc:  0.7734375
train loss:  0.45046260952949524
train gradient:  0.0965129620868478
iteration : 12126
train acc:  0.6875
train loss:  0.5152838230133057
train gradient:  0.14610471504118033
iteration : 12127
train acc:  0.7421875
train loss:  0.5004315376281738
train gradient:  0.1501376499787367
iteration : 12128
train acc:  0.75
train loss:  0.4904696047306061
train gradient:  0.10109058437656503
iteration : 12129
train acc:  0.734375
train loss:  0.4970000088214874
train gradient:  0.12200247877133673
iteration : 12130
train acc:  0.640625
train loss:  0.5414472818374634
train gradient:  0.12647172660565925
iteration : 12131
train acc:  0.734375
train loss:  0.47578349709510803
train gradient:  0.1281709061904191
iteration : 12132
train acc:  0.7734375
train loss:  0.4771220088005066
train gradient:  0.14838002360982921
iteration : 12133
train acc:  0.7265625
train loss:  0.5169371366500854
train gradient:  0.11828406151074297
iteration : 12134
train acc:  0.703125
train loss:  0.5617048144340515
train gradient:  0.1471012322573333
iteration : 12135
train acc:  0.7109375
train loss:  0.5460596680641174
train gradient:  0.13502566653203532
iteration : 12136
train acc:  0.7109375
train loss:  0.5345555543899536
train gradient:  0.13772902729293318
iteration : 12137
train acc:  0.71875
train loss:  0.4988899230957031
train gradient:  0.11391070376152237
iteration : 12138
train acc:  0.6875
train loss:  0.561265230178833
train gradient:  0.15058766707127005
iteration : 12139
train acc:  0.796875
train loss:  0.44187355041503906
train gradient:  0.10966305341729522
iteration : 12140
train acc:  0.7421875
train loss:  0.45879095792770386
train gradient:  0.10017319472424095
iteration : 12141
train acc:  0.7890625
train loss:  0.457455039024353
train gradient:  0.10921293471348185
iteration : 12142
train acc:  0.7578125
train loss:  0.48357242345809937
train gradient:  0.1251264505396234
iteration : 12143
train acc:  0.6953125
train loss:  0.5469541549682617
train gradient:  0.15509892484925425
iteration : 12144
train acc:  0.734375
train loss:  0.4917655289173126
train gradient:  0.10423322230383344
iteration : 12145
train acc:  0.7578125
train loss:  0.48250696063041687
train gradient:  0.123128805247783
iteration : 12146
train acc:  0.75
train loss:  0.4846471846103668
train gradient:  0.10763108647155074
iteration : 12147
train acc:  0.765625
train loss:  0.47663986682891846
train gradient:  0.1254773026828243
iteration : 12148
train acc:  0.796875
train loss:  0.4274374842643738
train gradient:  0.09193467784594679
iteration : 12149
train acc:  0.7578125
train loss:  0.5110247135162354
train gradient:  0.14005832892202585
iteration : 12150
train acc:  0.8359375
train loss:  0.36861586570739746
train gradient:  0.07388491762829662
iteration : 12151
train acc:  0.8203125
train loss:  0.4135715067386627
train gradient:  0.08541321209751421
iteration : 12152
train acc:  0.6953125
train loss:  0.4979415535926819
train gradient:  0.14109692781409688
iteration : 12153
train acc:  0.75
train loss:  0.45409736037254333
train gradient:  0.12610775364673527
iteration : 12154
train acc:  0.640625
train loss:  0.5565650463104248
train gradient:  0.1628979994001744
iteration : 12155
train acc:  0.7890625
train loss:  0.4411884844303131
train gradient:  0.09432509613544085
iteration : 12156
train acc:  0.78125
train loss:  0.48761606216430664
train gradient:  0.1395198024626443
iteration : 12157
train acc:  0.734375
train loss:  0.5177772641181946
train gradient:  0.14521675643672627
iteration : 12158
train acc:  0.7421875
train loss:  0.5248394012451172
train gradient:  0.13717058905549623
iteration : 12159
train acc:  0.703125
train loss:  0.5423120260238647
train gradient:  0.1624018844143829
iteration : 12160
train acc:  0.7265625
train loss:  0.5147578716278076
train gradient:  0.14678261037353224
iteration : 12161
train acc:  0.765625
train loss:  0.4849783480167389
train gradient:  0.13102369806419356
iteration : 12162
train acc:  0.765625
train loss:  0.5267339944839478
train gradient:  0.11364082166544305
iteration : 12163
train acc:  0.734375
train loss:  0.5046542882919312
train gradient:  0.12378530853124266
iteration : 12164
train acc:  0.7734375
train loss:  0.4862677752971649
train gradient:  0.11625019400436784
iteration : 12165
train acc:  0.765625
train loss:  0.46087509393692017
train gradient:  0.11480191779904579
iteration : 12166
train acc:  0.7265625
train loss:  0.5035411715507507
train gradient:  0.10294770320597516
iteration : 12167
train acc:  0.765625
train loss:  0.4668387472629547
train gradient:  0.10325271433529655
iteration : 12168
train acc:  0.7578125
train loss:  0.49581941962242126
train gradient:  0.11683080598468654
iteration : 12169
train acc:  0.7734375
train loss:  0.4898720979690552
train gradient:  0.11711438317015684
iteration : 12170
train acc:  0.6796875
train loss:  0.5188401341438293
train gradient:  0.1447868156522053
iteration : 12171
train acc:  0.8125
train loss:  0.4559296667575836
train gradient:  0.10776334489098137
iteration : 12172
train acc:  0.75
train loss:  0.5045563578605652
train gradient:  0.11884296655648446
iteration : 12173
train acc:  0.7265625
train loss:  0.48210564255714417
train gradient:  0.11416212509647028
iteration : 12174
train acc:  0.7578125
train loss:  0.4435606002807617
train gradient:  0.11013682962752272
iteration : 12175
train acc:  0.71875
train loss:  0.5506521463394165
train gradient:  0.15257423745597076
iteration : 12176
train acc:  0.765625
train loss:  0.42525017261505127
train gradient:  0.09142242759181754
iteration : 12177
train acc:  0.6796875
train loss:  0.5929327011108398
train gradient:  0.16944083035052615
iteration : 12178
train acc:  0.7734375
train loss:  0.48681455850601196
train gradient:  0.11770193217280296
iteration : 12179
train acc:  0.7421875
train loss:  0.4447647035121918
train gradient:  0.10027992408519498
iteration : 12180
train acc:  0.734375
train loss:  0.5761270523071289
train gradient:  0.13167838482215
iteration : 12181
train acc:  0.7734375
train loss:  0.4403647184371948
train gradient:  0.09093402473296076
iteration : 12182
train acc:  0.71875
train loss:  0.4511951208114624
train gradient:  0.09689963296896845
iteration : 12183
train acc:  0.7109375
train loss:  0.5150175094604492
train gradient:  0.1263779134358402
iteration : 12184
train acc:  0.765625
train loss:  0.49720650911331177
train gradient:  0.09903881348155232
iteration : 12185
train acc:  0.7109375
train loss:  0.5126761198043823
train gradient:  0.12745139553589369
iteration : 12186
train acc:  0.75
train loss:  0.4902026653289795
train gradient:  0.10269550261426369
iteration : 12187
train acc:  0.796875
train loss:  0.4559502601623535
train gradient:  0.11872789014242091
iteration : 12188
train acc:  0.671875
train loss:  0.5454173684120178
train gradient:  0.134881999298806
iteration : 12189
train acc:  0.75
train loss:  0.470119833946228
train gradient:  0.12013684986312281
iteration : 12190
train acc:  0.6796875
train loss:  0.5934015512466431
train gradient:  0.16134769146443045
iteration : 12191
train acc:  0.71875
train loss:  0.49268341064453125
train gradient:  0.12454566238237885
iteration : 12192
train acc:  0.75
train loss:  0.4974122643470764
train gradient:  0.12725589826668482
iteration : 12193
train acc:  0.7578125
train loss:  0.45243144035339355
train gradient:  0.10536835448855722
iteration : 12194
train acc:  0.71875
train loss:  0.5272939205169678
train gradient:  0.13297674601844012
iteration : 12195
train acc:  0.7421875
train loss:  0.49989083409309387
train gradient:  0.13042019344329586
iteration : 12196
train acc:  0.734375
train loss:  0.47939279675483704
train gradient:  0.11668589036063555
iteration : 12197
train acc:  0.7421875
train loss:  0.49425631761550903
train gradient:  0.09812012281692667
iteration : 12198
train acc:  0.6640625
train loss:  0.49289700388908386
train gradient:  0.10856117160848315
iteration : 12199
train acc:  0.828125
train loss:  0.40458622574806213
train gradient:  0.0896258299162659
iteration : 12200
train acc:  0.765625
train loss:  0.4797075092792511
train gradient:  0.13889812495643897
iteration : 12201
train acc:  0.765625
train loss:  0.4753873944282532
train gradient:  0.10489740688501985
iteration : 12202
train acc:  0.7890625
train loss:  0.4054701626300812
train gradient:  0.09392867566847875
iteration : 12203
train acc:  0.75
train loss:  0.4564700126647949
train gradient:  0.12282186798381557
iteration : 12204
train acc:  0.8046875
train loss:  0.45008420944213867
train gradient:  0.13221818582239822
iteration : 12205
train acc:  0.8125
train loss:  0.44105011224746704
train gradient:  0.10053329540557758
iteration : 12206
train acc:  0.765625
train loss:  0.5313908457756042
train gradient:  0.17437519092691856
iteration : 12207
train acc:  0.71875
train loss:  0.5276994109153748
train gradient:  0.13905845365492192
iteration : 12208
train acc:  0.703125
train loss:  0.5406225323677063
train gradient:  0.15154862848070316
iteration : 12209
train acc:  0.765625
train loss:  0.4797302484512329
train gradient:  0.11373945087443058
iteration : 12210
train acc:  0.71875
train loss:  0.4780803620815277
train gradient:  0.12173641879594514
iteration : 12211
train acc:  0.6875
train loss:  0.5017674565315247
train gradient:  0.12664725207861494
iteration : 12212
train acc:  0.75
train loss:  0.5650119185447693
train gradient:  0.1512968601975018
iteration : 12213
train acc:  0.75
train loss:  0.47961437702178955
train gradient:  0.1415648953861558
iteration : 12214
train acc:  0.71875
train loss:  0.47918903827667236
train gradient:  0.11303365662438865
iteration : 12215
train acc:  0.7890625
train loss:  0.48318031430244446
train gradient:  0.10919731322337306
iteration : 12216
train acc:  0.75
train loss:  0.4920680522918701
train gradient:  0.16626740566820347
iteration : 12217
train acc:  0.7421875
train loss:  0.46921008825302124
train gradient:  0.111334881117839
iteration : 12218
train acc:  0.75
train loss:  0.48170238733291626
train gradient:  0.12830866294582838
iteration : 12219
train acc:  0.6328125
train loss:  0.6353175640106201
train gradient:  0.19916774476463409
iteration : 12220
train acc:  0.6796875
train loss:  0.548792839050293
train gradient:  0.1360503218701946
iteration : 12221
train acc:  0.71875
train loss:  0.5113131999969482
train gradient:  0.11727523501824298
iteration : 12222
train acc:  0.6953125
train loss:  0.4918050169944763
train gradient:  0.11126243429912736
iteration : 12223
train acc:  0.6796875
train loss:  0.566699743270874
train gradient:  0.17200666922474603
iteration : 12224
train acc:  0.75
train loss:  0.4297183156013489
train gradient:  0.11341122852512893
iteration : 12225
train acc:  0.703125
train loss:  0.5111397504806519
train gradient:  0.1344546699403625
iteration : 12226
train acc:  0.7890625
train loss:  0.44118714332580566
train gradient:  0.1038563221003108
iteration : 12227
train acc:  0.7578125
train loss:  0.47500714659690857
train gradient:  0.10446665166266204
iteration : 12228
train acc:  0.75
train loss:  0.4845992922782898
train gradient:  0.10586676184715846
iteration : 12229
train acc:  0.6484375
train loss:  0.5636844635009766
train gradient:  0.15183163921063042
iteration : 12230
train acc:  0.671875
train loss:  0.5212316513061523
train gradient:  0.12684685108152352
iteration : 12231
train acc:  0.7421875
train loss:  0.4973699450492859
train gradient:  0.12317638438218989
iteration : 12232
train acc:  0.7109375
train loss:  0.480787068605423
train gradient:  0.09303995869111344
iteration : 12233
train acc:  0.7421875
train loss:  0.45657020807266235
train gradient:  0.11101387357118989
iteration : 12234
train acc:  0.734375
train loss:  0.53279048204422
train gradient:  0.1368453301340613
iteration : 12235
train acc:  0.7265625
train loss:  0.4587457478046417
train gradient:  0.12384344000180517
iteration : 12236
train acc:  0.7421875
train loss:  0.47421836853027344
train gradient:  0.12354331810743531
iteration : 12237
train acc:  0.7578125
train loss:  0.46099191904067993
train gradient:  0.09996102261669874
iteration : 12238
train acc:  0.75
train loss:  0.5151543617248535
train gradient:  0.12441742310803545
iteration : 12239
train acc:  0.71875
train loss:  0.5515537261962891
train gradient:  0.13407863962669697
iteration : 12240
train acc:  0.7578125
train loss:  0.47487330436706543
train gradient:  0.09766616555659409
iteration : 12241
train acc:  0.796875
train loss:  0.4402933120727539
train gradient:  0.12552255473188992
iteration : 12242
train acc:  0.796875
train loss:  0.4759191572666168
train gradient:  0.10034425318248294
iteration : 12243
train acc:  0.7265625
train loss:  0.5100066661834717
train gradient:  0.1283410241901985
iteration : 12244
train acc:  0.8046875
train loss:  0.46942079067230225
train gradient:  0.11093403733942805
iteration : 12245
train acc:  0.71875
train loss:  0.5319474339485168
train gradient:  0.1379447568841192
iteration : 12246
train acc:  0.6875
train loss:  0.5809303522109985
train gradient:  0.17793395574190002
iteration : 12247
train acc:  0.765625
train loss:  0.45122280716896057
train gradient:  0.12547285589033774
iteration : 12248
train acc:  0.71875
train loss:  0.49315527081489563
train gradient:  0.11166821289159078
iteration : 12249
train acc:  0.7421875
train loss:  0.48841172456741333
train gradient:  0.10018581348500535
iteration : 12250
train acc:  0.75
train loss:  0.4859752058982849
train gradient:  0.12091393875685288
iteration : 12251
train acc:  0.765625
train loss:  0.43142759799957275
train gradient:  0.11005065538917508
iteration : 12252
train acc:  0.7109375
train loss:  0.5222808122634888
train gradient:  0.15979718806546062
iteration : 12253
train acc:  0.765625
train loss:  0.4665808379650116
train gradient:  0.10982119196447787
iteration : 12254
train acc:  0.7421875
train loss:  0.522321343421936
train gradient:  0.11808175621483845
iteration : 12255
train acc:  0.75
train loss:  0.5459474325180054
train gradient:  0.14296553667915576
iteration : 12256
train acc:  0.6875
train loss:  0.5317920446395874
train gradient:  0.13372278684024824
iteration : 12257
train acc:  0.765625
train loss:  0.44897371530532837
train gradient:  0.09833271196226834
iteration : 12258
train acc:  0.7734375
train loss:  0.45028525590896606
train gradient:  0.09792659907836204
iteration : 12259
train acc:  0.71875
train loss:  0.4579662084579468
train gradient:  0.1388060609534537
iteration : 12260
train acc:  0.703125
train loss:  0.5599527359008789
train gradient:  0.13712873992159672
iteration : 12261
train acc:  0.828125
train loss:  0.40965795516967773
train gradient:  0.098408950060005
iteration : 12262
train acc:  0.7265625
train loss:  0.4526478946208954
train gradient:  0.12968378280983578
iteration : 12263
train acc:  0.671875
train loss:  0.5285273790359497
train gradient:  0.1442934789516301
iteration : 12264
train acc:  0.734375
train loss:  0.5008194446563721
train gradient:  0.12134951545289595
iteration : 12265
train acc:  0.734375
train loss:  0.5106748342514038
train gradient:  0.14765427967480405
iteration : 12266
train acc:  0.7890625
train loss:  0.45221710205078125
train gradient:  0.09859785864401305
iteration : 12267
train acc:  0.7265625
train loss:  0.4767613112926483
train gradient:  0.09694171891265779
iteration : 12268
train acc:  0.671875
train loss:  0.5182061791419983
train gradient:  0.13522651058008356
iteration : 12269
train acc:  0.796875
train loss:  0.4220949411392212
train gradient:  0.08691810818541681
iteration : 12270
train acc:  0.7421875
train loss:  0.4977623224258423
train gradient:  0.11657939887936879
iteration : 12271
train acc:  0.6953125
train loss:  0.5555258393287659
train gradient:  0.14573625409491126
iteration : 12272
train acc:  0.78125
train loss:  0.46134838461875916
train gradient:  0.1037368654168679
iteration : 12273
train acc:  0.7109375
train loss:  0.5462262034416199
train gradient:  0.13454704152957195
iteration : 12274
train acc:  0.7578125
train loss:  0.4953572452068329
train gradient:  0.12234098168998112
iteration : 12275
train acc:  0.7265625
train loss:  0.5007638931274414
train gradient:  0.1600642170099566
iteration : 12276
train acc:  0.6875
train loss:  0.5239273905754089
train gradient:  0.15817813973782047
iteration : 12277
train acc:  0.7734375
train loss:  0.47068071365356445
train gradient:  0.11363970042835682
iteration : 12278
train acc:  0.7265625
train loss:  0.5315870046615601
train gradient:  0.14584200467418976
iteration : 12279
train acc:  0.78125
train loss:  0.4456796646118164
train gradient:  0.10033494946978211
iteration : 12280
train acc:  0.703125
train loss:  0.5141832828521729
train gradient:  0.13179103389209348
iteration : 12281
train acc:  0.7734375
train loss:  0.4188447892665863
train gradient:  0.10457490084800623
iteration : 12282
train acc:  0.796875
train loss:  0.4389834403991699
train gradient:  0.09619223692198527
iteration : 12283
train acc:  0.6953125
train loss:  0.5164343118667603
train gradient:  0.1434258290344635
iteration : 12284
train acc:  0.734375
train loss:  0.5200445652008057
train gradient:  0.144125324513862
iteration : 12285
train acc:  0.7265625
train loss:  0.48969218134880066
train gradient:  0.11766326934200617
iteration : 12286
train acc:  0.6953125
train loss:  0.506687581539154
train gradient:  0.12603280732004946
iteration : 12287
train acc:  0.703125
train loss:  0.5333385467529297
train gradient:  0.12652589902538441
iteration : 12288
train acc:  0.75
train loss:  0.4849189817905426
train gradient:  0.10266570424094718
iteration : 12289
train acc:  0.734375
train loss:  0.5004040002822876
train gradient:  0.13503974190457474
iteration : 12290
train acc:  0.765625
train loss:  0.49420177936553955
train gradient:  0.12856547328777856
iteration : 12291
train acc:  0.6796875
train loss:  0.5016041994094849
train gradient:  0.12246502908286658
iteration : 12292
train acc:  0.7421875
train loss:  0.4591885209083557
train gradient:  0.1153306478172157
iteration : 12293
train acc:  0.7734375
train loss:  0.42836159467697144
train gradient:  0.08573528885180196
iteration : 12294
train acc:  0.7265625
train loss:  0.5277315974235535
train gradient:  0.11110963851128255
iteration : 12295
train acc:  0.796875
train loss:  0.44143378734588623
train gradient:  0.10734149782422847
iteration : 12296
train acc:  0.828125
train loss:  0.4261225461959839
train gradient:  0.10143939638627537
iteration : 12297
train acc:  0.7109375
train loss:  0.5059729218482971
train gradient:  0.15971105456535425
iteration : 12298
train acc:  0.71875
train loss:  0.4607720673084259
train gradient:  0.09816361604784342
iteration : 12299
train acc:  0.703125
train loss:  0.555478572845459
train gradient:  0.15969403260025583
iteration : 12300
train acc:  0.7890625
train loss:  0.45534831285476685
train gradient:  0.11580211483330671
iteration : 12301
train acc:  0.75
train loss:  0.46608638763427734
train gradient:  0.11361805147671285
iteration : 12302
train acc:  0.6953125
train loss:  0.5229942798614502
train gradient:  0.12185692163364958
iteration : 12303
train acc:  0.7734375
train loss:  0.4087631404399872
train gradient:  0.08470820536916024
iteration : 12304
train acc:  0.734375
train loss:  0.4903545677661896
train gradient:  0.13566415858933417
iteration : 12305
train acc:  0.734375
train loss:  0.48193660378456116
train gradient:  0.15248541780599842
iteration : 12306
train acc:  0.7578125
train loss:  0.4813661575317383
train gradient:  0.1097475062336198
iteration : 12307
train acc:  0.75
train loss:  0.5133894681930542
train gradient:  0.13560484137084364
iteration : 12308
train acc:  0.7109375
train loss:  0.5876709222793579
train gradient:  0.14147137454837658
iteration : 12309
train acc:  0.78125
train loss:  0.48305222392082214
train gradient:  0.13101276500281814
iteration : 12310
train acc:  0.7578125
train loss:  0.4622322618961334
train gradient:  0.12801553265418436
iteration : 12311
train acc:  0.765625
train loss:  0.5089319348335266
train gradient:  0.11329485497246022
iteration : 12312
train acc:  0.71875
train loss:  0.4801081120967865
train gradient:  0.10536357905719854
iteration : 12313
train acc:  0.6875
train loss:  0.5976599454879761
train gradient:  0.18211271175038268
iteration : 12314
train acc:  0.8046875
train loss:  0.43607187271118164
train gradient:  0.11602223112219306
iteration : 12315
train acc:  0.7890625
train loss:  0.4221437871456146
train gradient:  0.1128494553243603
iteration : 12316
train acc:  0.7265625
train loss:  0.4917241930961609
train gradient:  0.12281898045456816
iteration : 12317
train acc:  0.765625
train loss:  0.4611544609069824
train gradient:  0.12436843607388057
iteration : 12318
train acc:  0.75
train loss:  0.45503610372543335
train gradient:  0.11361904031709424
iteration : 12319
train acc:  0.734375
train loss:  0.4757174253463745
train gradient:  0.11889856663776993
iteration : 12320
train acc:  0.75
train loss:  0.4775272607803345
train gradient:  0.14029997488487372
iteration : 12321
train acc:  0.7421875
train loss:  0.49901771545410156
train gradient:  0.10448182132664725
iteration : 12322
train acc:  0.7421875
train loss:  0.49675852060317993
train gradient:  0.11912393907739685
iteration : 12323
train acc:  0.78125
train loss:  0.4678940176963806
train gradient:  0.13919013197486862
iteration : 12324
train acc:  0.765625
train loss:  0.4863125681877136
train gradient:  0.13428378741177885
iteration : 12325
train acc:  0.6328125
train loss:  0.5520331859588623
train gradient:  0.1682248584274353
iteration : 12326
train acc:  0.78125
train loss:  0.4491208791732788
train gradient:  0.13223923412169425
iteration : 12327
train acc:  0.7890625
train loss:  0.47462818026542664
train gradient:  0.1284908390879621
iteration : 12328
train acc:  0.8046875
train loss:  0.4627220034599304
train gradient:  0.11115395025173411
iteration : 12329
train acc:  0.7109375
train loss:  0.5602476000785828
train gradient:  0.13196343566273389
iteration : 12330
train acc:  0.75
train loss:  0.49705204367637634
train gradient:  0.10960008379257323
iteration : 12331
train acc:  0.7578125
train loss:  0.5204507112503052
train gradient:  0.13898642349974233
iteration : 12332
train acc:  0.703125
train loss:  0.5376375913619995
train gradient:  0.15052166444365622
iteration : 12333
train acc:  0.6875
train loss:  0.5676987171173096
train gradient:  0.16473145853161458
iteration : 12334
train acc:  0.734375
train loss:  0.5049469470977783
train gradient:  0.1471526916716066
iteration : 12335
train acc:  0.765625
train loss:  0.4619324207305908
train gradient:  0.12171364189905264
iteration : 12336
train acc:  0.7265625
train loss:  0.5313123464584351
train gradient:  0.1262830879836923
iteration : 12337
train acc:  0.765625
train loss:  0.45489001274108887
train gradient:  0.10513248869588264
iteration : 12338
train acc:  0.7265625
train loss:  0.4784139096736908
train gradient:  0.09534005214789541
iteration : 12339
train acc:  0.765625
train loss:  0.4902350604534149
train gradient:  0.11860100464406471
iteration : 12340
train acc:  0.703125
train loss:  0.4983500838279724
train gradient:  0.11669600283528034
iteration : 12341
train acc:  0.7109375
train loss:  0.5030340552330017
train gradient:  0.19526873900923536
iteration : 12342
train acc:  0.828125
train loss:  0.37804877758026123
train gradient:  0.06557941587413511
iteration : 12343
train acc:  0.7578125
train loss:  0.52942955493927
train gradient:  0.1307479742524837
iteration : 12344
train acc:  0.6875
train loss:  0.48955678939819336
train gradient:  0.13863325518754022
iteration : 12345
train acc:  0.75
train loss:  0.4886581599712372
train gradient:  0.10357704733985135
iteration : 12346
train acc:  0.8203125
train loss:  0.42072007060050964
train gradient:  0.11351925988546087
iteration : 12347
train acc:  0.7109375
train loss:  0.5136297941207886
train gradient:  0.13612412236493615
iteration : 12348
train acc:  0.75
train loss:  0.48911383748054504
train gradient:  0.11492994244502105
iteration : 12349
train acc:  0.7421875
train loss:  0.48732322454452515
train gradient:  0.10099331113845196
iteration : 12350
train acc:  0.734375
train loss:  0.485906720161438
train gradient:  0.144894397550143
iteration : 12351
train acc:  0.7421875
train loss:  0.4910326600074768
train gradient:  0.11858014784715729
iteration : 12352
train acc:  0.796875
train loss:  0.49027469754219055
train gradient:  0.10440715501524137
iteration : 12353
train acc:  0.7890625
train loss:  0.44915443658828735
train gradient:  0.10280894969043412
iteration : 12354
train acc:  0.765625
train loss:  0.46781066060066223
train gradient:  0.09920681701308665
iteration : 12355
train acc:  0.734375
train loss:  0.5224184393882751
train gradient:  0.1486537758060668
iteration : 12356
train acc:  0.734375
train loss:  0.47763925790786743
train gradient:  0.12990742386367804
iteration : 12357
train acc:  0.7578125
train loss:  0.47321879863739014
train gradient:  0.1688558259382303
iteration : 12358
train acc:  0.6796875
train loss:  0.592767596244812
train gradient:  0.16699341822596245
iteration : 12359
train acc:  0.7265625
train loss:  0.4611189067363739
train gradient:  0.11527374302115259
iteration : 12360
train acc:  0.75
train loss:  0.5028737783432007
train gradient:  0.1106585856092096
iteration : 12361
train acc:  0.7421875
train loss:  0.48592063784599304
train gradient:  0.13594221307894583
iteration : 12362
train acc:  0.7109375
train loss:  0.5077371597290039
train gradient:  0.15583987876808195
iteration : 12363
train acc:  0.75
train loss:  0.500218391418457
train gradient:  0.12300916117573273
iteration : 12364
train acc:  0.765625
train loss:  0.4765714406967163
train gradient:  0.11650472148856367
iteration : 12365
train acc:  0.7421875
train loss:  0.49060216546058655
train gradient:  0.10821436485523918
iteration : 12366
train acc:  0.7890625
train loss:  0.43202805519104004
train gradient:  0.09493488834199733
iteration : 12367
train acc:  0.7109375
train loss:  0.4538436532020569
train gradient:  0.10906020691640338
iteration : 12368
train acc:  0.703125
train loss:  0.5712555646896362
train gradient:  0.16748785154504006
iteration : 12369
train acc:  0.8125
train loss:  0.40732496976852417
train gradient:  0.09428054424101087
iteration : 12370
train acc:  0.7109375
train loss:  0.5151427984237671
train gradient:  0.12933873690009556
iteration : 12371
train acc:  0.765625
train loss:  0.4638552665710449
train gradient:  0.10627905749400432
iteration : 12372
train acc:  0.625
train loss:  0.6265593767166138
train gradient:  0.17356291743951724
iteration : 12373
train acc:  0.765625
train loss:  0.4782719910144806
train gradient:  0.10289831230238099
iteration : 12374
train acc:  0.75
train loss:  0.5452704429626465
train gradient:  0.15829766231759676
iteration : 12375
train acc:  0.71875
train loss:  0.4861406683921814
train gradient:  0.11655168680897504
iteration : 12376
train acc:  0.6328125
train loss:  0.6110875010490417
train gradient:  0.1971439008404037
iteration : 12377
train acc:  0.7109375
train loss:  0.5313953161239624
train gradient:  0.15492084892540942
iteration : 12378
train acc:  0.8046875
train loss:  0.4362540543079376
train gradient:  0.08084288132677385
iteration : 12379
train acc:  0.7734375
train loss:  0.4324106276035309
train gradient:  0.11088163160042125
iteration : 12380
train acc:  0.8046875
train loss:  0.4336240589618683
train gradient:  0.09406195696886588
iteration : 12381
train acc:  0.7578125
train loss:  0.4572119414806366
train gradient:  0.0876377387033776
iteration : 12382
train acc:  0.734375
train loss:  0.49209997057914734
train gradient:  0.12671456759433442
iteration : 12383
train acc:  0.8203125
train loss:  0.4251716136932373
train gradient:  0.09217282522387409
iteration : 12384
train acc:  0.7421875
train loss:  0.5704410076141357
train gradient:  0.15027206213754907
iteration : 12385
train acc:  0.703125
train loss:  0.5594918131828308
train gradient:  0.1877675901140547
iteration : 12386
train acc:  0.7421875
train loss:  0.5083396434783936
train gradient:  0.11312829059936184
iteration : 12387
train acc:  0.796875
train loss:  0.4438765347003937
train gradient:  0.1246676115382456
iteration : 12388
train acc:  0.7578125
train loss:  0.48785024881362915
train gradient:  0.10134741242575579
iteration : 12389
train acc:  0.71875
train loss:  0.5046756863594055
train gradient:  0.12672785174524437
iteration : 12390
train acc:  0.75
train loss:  0.48354804515838623
train gradient:  0.10931334571746427
iteration : 12391
train acc:  0.8125
train loss:  0.44207000732421875
train gradient:  0.11393016244266523
iteration : 12392
train acc:  0.7109375
train loss:  0.5176781415939331
train gradient:  0.16134565452614927
iteration : 12393
train acc:  0.6953125
train loss:  0.469740092754364
train gradient:  0.1151068950973557
iteration : 12394
train acc:  0.7421875
train loss:  0.4574684500694275
train gradient:  0.12419694945883229
iteration : 12395
train acc:  0.765625
train loss:  0.48458996415138245
train gradient:  0.13492770358184378
iteration : 12396
train acc:  0.71875
train loss:  0.5457264184951782
train gradient:  0.17029429766071325
iteration : 12397
train acc:  0.6953125
train loss:  0.5217106342315674
train gradient:  0.11767619719273688
iteration : 12398
train acc:  0.7421875
train loss:  0.4660133719444275
train gradient:  0.13378298768896235
iteration : 12399
train acc:  0.734375
train loss:  0.5420644283294678
train gradient:  0.15398030823363362
iteration : 12400
train acc:  0.8125
train loss:  0.45745062828063965
train gradient:  0.08892270376830877
iteration : 12401
train acc:  0.7578125
train loss:  0.45552247762680054
train gradient:  0.11347931234272712
iteration : 12402
train acc:  0.765625
train loss:  0.43964940309524536
train gradient:  0.11082611390665696
iteration : 12403
train acc:  0.703125
train loss:  0.5363762378692627
train gradient:  0.17237333825553403
iteration : 12404
train acc:  0.8046875
train loss:  0.44297271966934204
train gradient:  0.09843096862256058
iteration : 12405
train acc:  0.7578125
train loss:  0.4913668930530548
train gradient:  0.1485142459390225
iteration : 12406
train acc:  0.734375
train loss:  0.5209078192710876
train gradient:  0.11646559753201718
iteration : 12407
train acc:  0.765625
train loss:  0.47977861762046814
train gradient:  0.11757384429499759
iteration : 12408
train acc:  0.7734375
train loss:  0.4873746633529663
train gradient:  0.11859238797113893
iteration : 12409
train acc:  0.6796875
train loss:  0.573833703994751
train gradient:  0.14595861265118823
iteration : 12410
train acc:  0.7890625
train loss:  0.42966988682746887
train gradient:  0.12552667345573237
iteration : 12411
train acc:  0.75
train loss:  0.4836081862449646
train gradient:  0.18102104257305612
iteration : 12412
train acc:  0.6796875
train loss:  0.5182133913040161
train gradient:  0.14610178871437351
iteration : 12413
train acc:  0.6875
train loss:  0.540898323059082
train gradient:  0.1707570165012478
iteration : 12414
train acc:  0.6640625
train loss:  0.5370688438415527
train gradient:  0.12709321691323672
iteration : 12415
train acc:  0.7421875
train loss:  0.5225129127502441
train gradient:  0.1581128832263458
iteration : 12416
train acc:  0.75
train loss:  0.5014655590057373
train gradient:  0.12914944771195574
iteration : 12417
train acc:  0.7421875
train loss:  0.5027741193771362
train gradient:  0.13995347343064957
iteration : 12418
train acc:  0.734375
train loss:  0.48912402987480164
train gradient:  0.1198175733987473
iteration : 12419
train acc:  0.828125
train loss:  0.41885247826576233
train gradient:  0.0959888667891116
iteration : 12420
train acc:  0.71875
train loss:  0.5623657703399658
train gradient:  0.18588871000672869
iteration : 12421
train acc:  0.7265625
train loss:  0.4839210510253906
train gradient:  0.1034060477186732
iteration : 12422
train acc:  0.6796875
train loss:  0.5565490126609802
train gradient:  0.13750378041723882
iteration : 12423
train acc:  0.6953125
train loss:  0.4824550151824951
train gradient:  0.11125726326094636
iteration : 12424
train acc:  0.765625
train loss:  0.499426007270813
train gradient:  0.12236320445756847
iteration : 12425
train acc:  0.78125
train loss:  0.4440619945526123
train gradient:  0.10743049222466444
iteration : 12426
train acc:  0.765625
train loss:  0.42509955167770386
train gradient:  0.102425928075396
iteration : 12427
train acc:  0.7421875
train loss:  0.537260890007019
train gradient:  0.18614360672949318
iteration : 12428
train acc:  0.75
train loss:  0.44652730226516724
train gradient:  0.08954942886396984
iteration : 12429
train acc:  0.7578125
train loss:  0.4773443341255188
train gradient:  0.12564427261216862
iteration : 12430
train acc:  0.7734375
train loss:  0.45203280448913574
train gradient:  0.1018143207362257
iteration : 12431
train acc:  0.6953125
train loss:  0.5339287519454956
train gradient:  0.10297119070604464
iteration : 12432
train acc:  0.765625
train loss:  0.5101702213287354
train gradient:  0.15013280160932987
iteration : 12433
train acc:  0.7265625
train loss:  0.459856778383255
train gradient:  0.10236839033174665
iteration : 12434
train acc:  0.78125
train loss:  0.4760700464248657
train gradient:  0.13517768028722807
iteration : 12435
train acc:  0.7734375
train loss:  0.4509913921356201
train gradient:  0.11019700989302401
iteration : 12436
train acc:  0.734375
train loss:  0.5164050459861755
train gradient:  0.16631289008717554
iteration : 12437
train acc:  0.7734375
train loss:  0.4263426661491394
train gradient:  0.0874454715165266
iteration : 12438
train acc:  0.7890625
train loss:  0.47114479541778564
train gradient:  0.10677162377125023
iteration : 12439
train acc:  0.734375
train loss:  0.5715205669403076
train gradient:  0.2352747932028525
iteration : 12440
train acc:  0.6953125
train loss:  0.5528407096862793
train gradient:  0.12182697715770148
iteration : 12441
train acc:  0.734375
train loss:  0.49110227823257446
train gradient:  0.1341434880167687
iteration : 12442
train acc:  0.7578125
train loss:  0.4717099666595459
train gradient:  0.1225239570578903
iteration : 12443
train acc:  0.78125
train loss:  0.46001312136650085
train gradient:  0.11019789781436287
iteration : 12444
train acc:  0.7578125
train loss:  0.45671480894088745
train gradient:  0.08311460283085265
iteration : 12445
train acc:  0.78125
train loss:  0.46507978439331055
train gradient:  0.10355730046982975
iteration : 12446
train acc:  0.7265625
train loss:  0.5162847638130188
train gradient:  0.1390900435626009
iteration : 12447
train acc:  0.7578125
train loss:  0.46720555424690247
train gradient:  0.09900553042515703
iteration : 12448
train acc:  0.7734375
train loss:  0.5005965232849121
train gradient:  0.11281323790009186
iteration : 12449
train acc:  0.671875
train loss:  0.5895008444786072
train gradient:  0.13928141996135662
iteration : 12450
train acc:  0.6953125
train loss:  0.49443677067756653
train gradient:  0.14901772743688863
iteration : 12451
train acc:  0.765625
train loss:  0.49318939447402954
train gradient:  0.11758873812917772
iteration : 12452
train acc:  0.7734375
train loss:  0.46333467960357666
train gradient:  0.10186389431962532
iteration : 12453
train acc:  0.7578125
train loss:  0.46832746267318726
train gradient:  0.12211542426316692
iteration : 12454
train acc:  0.7890625
train loss:  0.4595365822315216
train gradient:  0.11883929672552243
iteration : 12455
train acc:  0.7109375
train loss:  0.4991178810596466
train gradient:  0.11521091339575702
iteration : 12456
train acc:  0.703125
train loss:  0.4849932789802551
train gradient:  0.1240942194463861
iteration : 12457
train acc:  0.734375
train loss:  0.49413812160491943
train gradient:  0.12542049904323913
iteration : 12458
train acc:  0.703125
train loss:  0.516139805316925
train gradient:  0.13788889515956904
iteration : 12459
train acc:  0.7421875
train loss:  0.4862877130508423
train gradient:  0.14791585699534746
iteration : 12460
train acc:  0.7578125
train loss:  0.4392327070236206
train gradient:  0.09031274242827796
iteration : 12461
train acc:  0.7890625
train loss:  0.49066126346588135
train gradient:  0.11524028161965083
iteration : 12462
train acc:  0.734375
train loss:  0.5531928539276123
train gradient:  0.1333257707457468
iteration : 12463
train acc:  0.6484375
train loss:  0.5507916212081909
train gradient:  0.1257578863271021
iteration : 12464
train acc:  0.7421875
train loss:  0.49410480260849
train gradient:  0.13068920570386422
iteration : 12465
train acc:  0.7578125
train loss:  0.4960486888885498
train gradient:  0.11188398109327234
iteration : 12466
train acc:  0.7421875
train loss:  0.5001288056373596
train gradient:  0.15619841891915448
iteration : 12467
train acc:  0.6796875
train loss:  0.5571873188018799
train gradient:  0.14410125712008387
iteration : 12468
train acc:  0.7265625
train loss:  0.5376225709915161
train gradient:  0.11288334799104464
iteration : 12469
train acc:  0.796875
train loss:  0.44226735830307007
train gradient:  0.12671442375833392
iteration : 12470
train acc:  0.703125
train loss:  0.5249805450439453
train gradient:  0.1412300004340613
iteration : 12471
train acc:  0.734375
train loss:  0.4915434718132019
train gradient:  0.10846832277634456
iteration : 12472
train acc:  0.796875
train loss:  0.3998225927352905
train gradient:  0.0860492283646135
iteration : 12473
train acc:  0.671875
train loss:  0.47601574659347534
train gradient:  0.12753234245827128
iteration : 12474
train acc:  0.734375
train loss:  0.5030661821365356
train gradient:  0.13443834696907814
iteration : 12475
train acc:  0.734375
train loss:  0.5102874040603638
train gradient:  0.11242747276648148
iteration : 12476
train acc:  0.734375
train loss:  0.464834064245224
train gradient:  0.09803923935632783
iteration : 12477
train acc:  0.6484375
train loss:  0.5120103359222412
train gradient:  0.1433740381240998
iteration : 12478
train acc:  0.765625
train loss:  0.4735037684440613
train gradient:  0.10729697450562488
iteration : 12479
train acc:  0.703125
train loss:  0.4710736870765686
train gradient:  0.10410622110107305
iteration : 12480
train acc:  0.78125
train loss:  0.4855097532272339
train gradient:  0.11625671998204999
iteration : 12481
train acc:  0.7109375
train loss:  0.568061113357544
train gradient:  0.1839912984042772
iteration : 12482
train acc:  0.7421875
train loss:  0.46061617136001587
train gradient:  0.10021543113233164
iteration : 12483
train acc:  0.75
train loss:  0.48968058824539185
train gradient:  0.1645761659651902
iteration : 12484
train acc:  0.7265625
train loss:  0.5177378058433533
train gradient:  0.1320647420065565
iteration : 12485
train acc:  0.734375
train loss:  0.4816455543041229
train gradient:  0.11754425117473971
iteration : 12486
train acc:  0.8046875
train loss:  0.4634648859500885
train gradient:  0.09613416161273035
iteration : 12487
train acc:  0.8203125
train loss:  0.46394550800323486
train gradient:  0.10616111560953319
iteration : 12488
train acc:  0.8125
train loss:  0.4373823404312134
train gradient:  0.10801593743286532
iteration : 12489
train acc:  0.75
train loss:  0.4929952621459961
train gradient:  0.14270686981571168
iteration : 12490
train acc:  0.8203125
train loss:  0.4194301962852478
train gradient:  0.10349142439561089
iteration : 12491
train acc:  0.671875
train loss:  0.6108787059783936
train gradient:  0.20974917478836352
iteration : 12492
train acc:  0.7109375
train loss:  0.5063591003417969
train gradient:  0.11905333791500838
iteration : 12493
train acc:  0.6953125
train loss:  0.5452130436897278
train gradient:  0.15619049642750094
iteration : 12494
train acc:  0.765625
train loss:  0.4974178969860077
train gradient:  0.1509981174433464
iteration : 12495
train acc:  0.7421875
train loss:  0.5238034129142761
train gradient:  0.13719823018954622
iteration : 12496
train acc:  0.6953125
train loss:  0.4801768362522125
train gradient:  0.10135552086100784
iteration : 12497
train acc:  0.78125
train loss:  0.48431068658828735
train gradient:  0.13294514540036753
iteration : 12498
train acc:  0.734375
train loss:  0.46792757511138916
train gradient:  0.1029575862379976
iteration : 12499
train acc:  0.765625
train loss:  0.4880949854850769
train gradient:  0.11989738272861503
iteration : 12500
train acc:  0.78125
train loss:  0.5144475102424622
train gradient:  0.11948665923747177
iteration : 12501
train acc:  0.7578125
train loss:  0.43977880477905273
train gradient:  0.10379053135716323
iteration : 12502
train acc:  0.7109375
train loss:  0.4986054301261902
train gradient:  0.10183917217871433
iteration : 12503
train acc:  0.765625
train loss:  0.46303287148475647
train gradient:  0.12235866985819102
iteration : 12504
train acc:  0.7421875
train loss:  0.4888572096824646
train gradient:  0.1397340454125429
iteration : 12505
train acc:  0.8359375
train loss:  0.4410497546195984
train gradient:  0.11457100089165619
iteration : 12506
train acc:  0.6875
train loss:  0.530471682548523
train gradient:  0.11892479819102621
iteration : 12507
train acc:  0.734375
train loss:  0.4968976378440857
train gradient:  0.11001858019757474
iteration : 12508
train acc:  0.6640625
train loss:  0.5517961978912354
train gradient:  0.13777904684035824
iteration : 12509
train acc:  0.7421875
train loss:  0.4784163236618042
train gradient:  0.09802338507202273
iteration : 12510
train acc:  0.71875
train loss:  0.5063130855560303
train gradient:  0.12033794661598095
iteration : 12511
train acc:  0.734375
train loss:  0.49702948331832886
train gradient:  0.13020018924924087
iteration : 12512
train acc:  0.7265625
train loss:  0.4789242446422577
train gradient:  0.10854351372671187
iteration : 12513
train acc:  0.78125
train loss:  0.4969533681869507
train gradient:  0.12970480117417404
iteration : 12514
train acc:  0.71875
train loss:  0.480642169713974
train gradient:  0.14425941909973455
iteration : 12515
train acc:  0.734375
train loss:  0.5166369080543518
train gradient:  0.12065631598930704
iteration : 12516
train acc:  0.84375
train loss:  0.441768616437912
train gradient:  0.10411215764769807
iteration : 12517
train acc:  0.7734375
train loss:  0.4712759256362915
train gradient:  0.13069133568304003
iteration : 12518
train acc:  0.7265625
train loss:  0.5035489797592163
train gradient:  0.14375781937945728
iteration : 12519
train acc:  0.703125
train loss:  0.5293111801147461
train gradient:  0.13908514625156612
iteration : 12520
train acc:  0.7578125
train loss:  0.4841102957725525
train gradient:  0.14460043066025985
iteration : 12521
train acc:  0.7421875
train loss:  0.485908567905426
train gradient:  0.1308653690169418
iteration : 12522
train acc:  0.734375
train loss:  0.47335201501846313
train gradient:  0.09489110268708915
iteration : 12523
train acc:  0.71875
train loss:  0.5001391172409058
train gradient:  0.13369946582569706
iteration : 12524
train acc:  0.75
train loss:  0.4994518756866455
train gradient:  0.10100474963616195
iteration : 12525
train acc:  0.7109375
train loss:  0.5129825472831726
train gradient:  0.10122410783586425
iteration : 12526
train acc:  0.75
train loss:  0.4562824070453644
train gradient:  0.10783848049837218
iteration : 12527
train acc:  0.7734375
train loss:  0.4766698479652405
train gradient:  0.11241285665594418
iteration : 12528
train acc:  0.6796875
train loss:  0.49583369493484497
train gradient:  0.13640951348458105
iteration : 12529
train acc:  0.7109375
train loss:  0.4995894432067871
train gradient:  0.18064334354545003
iteration : 12530
train acc:  0.703125
train loss:  0.6019456386566162
train gradient:  0.1563504850633346
iteration : 12531
train acc:  0.765625
train loss:  0.4203062653541565
train gradient:  0.14295317049166467
iteration : 12532
train acc:  0.765625
train loss:  0.4515194296836853
train gradient:  0.10446603716018646
iteration : 12533
train acc:  0.734375
train loss:  0.4828355610370636
train gradient:  0.11687133976442317
iteration : 12534
train acc:  0.6875
train loss:  0.5246798992156982
train gradient:  0.11828143842999533
iteration : 12535
train acc:  0.7578125
train loss:  0.5499288439750671
train gradient:  0.16289567941805527
iteration : 12536
train acc:  0.734375
train loss:  0.5598025321960449
train gradient:  0.1333700439587434
iteration : 12537
train acc:  0.734375
train loss:  0.4680149555206299
train gradient:  0.12212343037917639
iteration : 12538
train acc:  0.71875
train loss:  0.48736387491226196
train gradient:  0.11769869967281303
iteration : 12539
train acc:  0.8203125
train loss:  0.4237687587738037
train gradient:  0.10732293574081922
iteration : 12540
train acc:  0.7421875
train loss:  0.4722844064235687
train gradient:  0.12691758361168873
iteration : 12541
train acc:  0.6953125
train loss:  0.5055297613143921
train gradient:  0.1055537574834006
iteration : 12542
train acc:  0.71875
train loss:  0.4712621867656708
train gradient:  0.1050554542052764
iteration : 12543
train acc:  0.7734375
train loss:  0.4475790858268738
train gradient:  0.08906711416765606
iteration : 12544
train acc:  0.75
train loss:  0.4718664884567261
train gradient:  0.10012868865585135
iteration : 12545
train acc:  0.7578125
train loss:  0.4738191068172455
train gradient:  0.13504855238579186
iteration : 12546
train acc:  0.71875
train loss:  0.503028154373169
train gradient:  0.13152602206937553
iteration : 12547
train acc:  0.734375
train loss:  0.5092016458511353
train gradient:  0.1473059746674078
iteration : 12548
train acc:  0.71875
train loss:  0.5050735473632812
train gradient:  0.13862118763464643
iteration : 12549
train acc:  0.75
train loss:  0.48493003845214844
train gradient:  0.15140171213998133
iteration : 12550
train acc:  0.6953125
train loss:  0.5488589406013489
train gradient:  0.13551203823853814
iteration : 12551
train acc:  0.7265625
train loss:  0.4605376720428467
train gradient:  0.12971205939218822
iteration : 12552
train acc:  0.7734375
train loss:  0.4532851576805115
train gradient:  0.12327507942631057
iteration : 12553
train acc:  0.7578125
train loss:  0.49277517199516296
train gradient:  0.1229554631173311
iteration : 12554
train acc:  0.7578125
train loss:  0.49489355087280273
train gradient:  0.1315124046047233
iteration : 12555
train acc:  0.71875
train loss:  0.5396595001220703
train gradient:  0.12200965253957886
iteration : 12556
train acc:  0.78125
train loss:  0.5231964588165283
train gradient:  0.11999065821557572
iteration : 12557
train acc:  0.6796875
train loss:  0.5074089765548706
train gradient:  0.12755790877722534
iteration : 12558
train acc:  0.7890625
train loss:  0.42866042256355286
train gradient:  0.11284429714812584
iteration : 12559
train acc:  0.71875
train loss:  0.4840465784072876
train gradient:  0.1080915229595371
iteration : 12560
train acc:  0.7734375
train loss:  0.43451857566833496
train gradient:  0.10301066395551647
iteration : 12561
train acc:  0.7265625
train loss:  0.5347000956535339
train gradient:  0.1271607262386747
iteration : 12562
train acc:  0.6640625
train loss:  0.5397523641586304
train gradient:  0.14059682590709832
iteration : 12563
train acc:  0.7109375
train loss:  0.5539835691452026
train gradient:  0.16974460139432423
iteration : 12564
train acc:  0.78125
train loss:  0.44241195917129517
train gradient:  0.08234837037811345
iteration : 12565
train acc:  0.796875
train loss:  0.4570498764514923
train gradient:  0.09161961654469837
iteration : 12566
train acc:  0.7890625
train loss:  0.45563116669654846
train gradient:  0.08653822100899326
iteration : 12567
train acc:  0.75
train loss:  0.48335355520248413
train gradient:  0.10625212404497666
iteration : 12568
train acc:  0.7421875
train loss:  0.5143649578094482
train gradient:  0.11496234575869342
iteration : 12569
train acc:  0.8125
train loss:  0.3969094753265381
train gradient:  0.07586901019602535
iteration : 12570
train acc:  0.7265625
train loss:  0.4855038821697235
train gradient:  0.1264572299448876
iteration : 12571
train acc:  0.6953125
train loss:  0.5278635621070862
train gradient:  0.11700882407267865
iteration : 12572
train acc:  0.7421875
train loss:  0.47422993183135986
train gradient:  0.10384772295290844
iteration : 12573
train acc:  0.71875
train loss:  0.5545198917388916
train gradient:  0.1543964055684922
iteration : 12574
train acc:  0.78125
train loss:  0.4989895820617676
train gradient:  0.1122081826579676
iteration : 12575
train acc:  0.796875
train loss:  0.43895676732063293
train gradient:  0.09461653814603314
iteration : 12576
train acc:  0.796875
train loss:  0.4593849182128906
train gradient:  0.11811213313670965
iteration : 12577
train acc:  0.765625
train loss:  0.48569440841674805
train gradient:  0.16564011155197564
iteration : 12578
train acc:  0.7890625
train loss:  0.3995313346385956
train gradient:  0.08326705406076498
iteration : 12579
train acc:  0.8046875
train loss:  0.4428565800189972
train gradient:  0.10205965656118798
iteration : 12580
train acc:  0.734375
train loss:  0.4664694666862488
train gradient:  0.11129141488046813
iteration : 12581
train acc:  0.671875
train loss:  0.5206928253173828
train gradient:  0.1419608080256744
iteration : 12582
train acc:  0.703125
train loss:  0.5425944328308105
train gradient:  0.14266667255457122
iteration : 12583
train acc:  0.7421875
train loss:  0.4822779893875122
train gradient:  0.1342406278831359
iteration : 12584
train acc:  0.6875
train loss:  0.555482029914856
train gradient:  0.19559413944642096
iteration : 12585
train acc:  0.6796875
train loss:  0.5810147523880005
train gradient:  0.1507860429662732
iteration : 12586
train acc:  0.734375
train loss:  0.49616295099258423
train gradient:  0.1314568198465699
iteration : 12587
train acc:  0.7734375
train loss:  0.4479680061340332
train gradient:  0.11443778218164585
iteration : 12588
train acc:  0.734375
train loss:  0.5384427905082703
train gradient:  0.16158540972318391
iteration : 12589
train acc:  0.6875
train loss:  0.5595750212669373
train gradient:  0.1284509071042837
iteration : 12590
train acc:  0.7734375
train loss:  0.48271775245666504
train gradient:  0.10364593950510438
iteration : 12591
train acc:  0.6171875
train loss:  0.6623731255531311
train gradient:  0.21128777182584188
iteration : 12592
train acc:  0.6953125
train loss:  0.549258828163147
train gradient:  0.16016158658262025
iteration : 12593
train acc:  0.6875
train loss:  0.5338558554649353
train gradient:  0.1294258351784838
iteration : 12594
train acc:  0.7421875
train loss:  0.4602503776550293
train gradient:  0.12295753083190071
iteration : 12595
train acc:  0.7265625
train loss:  0.4958059787750244
train gradient:  0.12947424139948405
iteration : 12596
train acc:  0.703125
train loss:  0.5034500360488892
train gradient:  0.15015806992383213
iteration : 12597
train acc:  0.703125
train loss:  0.49885135889053345
train gradient:  0.13400561776162556
iteration : 12598
train acc:  0.703125
train loss:  0.5717762112617493
train gradient:  0.17155323485465673
iteration : 12599
train acc:  0.8125
train loss:  0.44269275665283203
train gradient:  0.08285892717223073
iteration : 12600
train acc:  0.75
train loss:  0.5009218454360962
train gradient:  0.11098784155532804
iteration : 12601
train acc:  0.734375
train loss:  0.49284255504608154
train gradient:  0.117540402787621
iteration : 12602
train acc:  0.6796875
train loss:  0.5310771465301514
train gradient:  0.13213709618436809
iteration : 12603
train acc:  0.7109375
train loss:  0.5116016268730164
train gradient:  0.15624966613735014
iteration : 12604
train acc:  0.703125
train loss:  0.5177722573280334
train gradient:  0.12768927659425017
iteration : 12605
train acc:  0.796875
train loss:  0.4433627724647522
train gradient:  0.09673968730514852
iteration : 12606
train acc:  0.7578125
train loss:  0.48907339572906494
train gradient:  0.13950137124229317
iteration : 12607
train acc:  0.7109375
train loss:  0.5200517773628235
train gradient:  0.12710244952649705
iteration : 12608
train acc:  0.71875
train loss:  0.48733091354370117
train gradient:  0.10273543875749591
iteration : 12609
train acc:  0.75
train loss:  0.4669204354286194
train gradient:  0.11715307932697533
iteration : 12610
train acc:  0.71875
train loss:  0.45993608236312866
train gradient:  0.1075094318753781
iteration : 12611
train acc:  0.7421875
train loss:  0.48996639251708984
train gradient:  0.13537489637215222
iteration : 12612
train acc:  0.71875
train loss:  0.5332114696502686
train gradient:  0.18365412167045228
iteration : 12613
train acc:  0.7734375
train loss:  0.4624308943748474
train gradient:  0.12852213485503095
iteration : 12614
train acc:  0.7265625
train loss:  0.5146841406822205
train gradient:  0.15084895983072835
iteration : 12615
train acc:  0.75
train loss:  0.507186770439148
train gradient:  0.1326733943352333
iteration : 12616
train acc:  0.671875
train loss:  0.5262538194656372
train gradient:  0.12006299329527696
iteration : 12617
train acc:  0.7421875
train loss:  0.47771352529525757
train gradient:  0.11490178868747786
iteration : 12618
train acc:  0.78125
train loss:  0.45552346110343933
train gradient:  0.08590205796636968
iteration : 12619
train acc:  0.7109375
train loss:  0.5320116877555847
train gradient:  0.12430570882275808
iteration : 12620
train acc:  0.7578125
train loss:  0.4967997074127197
train gradient:  0.10966860209023852
iteration : 12621
train acc:  0.7421875
train loss:  0.49552828073501587
train gradient:  0.10646965255642866
iteration : 12622
train acc:  0.7109375
train loss:  0.4930305480957031
train gradient:  0.18606744113965118
iteration : 12623
train acc:  0.75
train loss:  0.479441374540329
train gradient:  0.12448317160805049
iteration : 12624
train acc:  0.75
train loss:  0.4603465795516968
train gradient:  0.15951112256320987
iteration : 12625
train acc:  0.796875
train loss:  0.5004823207855225
train gradient:  0.09601865354486017
iteration : 12626
train acc:  0.6953125
train loss:  0.5513571500778198
train gradient:  0.13919882277074264
iteration : 12627
train acc:  0.7734375
train loss:  0.4287584722042084
train gradient:  0.08775827414826447
iteration : 12628
train acc:  0.75
train loss:  0.4128023087978363
train gradient:  0.06704312213190708
iteration : 12629
train acc:  0.7265625
train loss:  0.5006822347640991
train gradient:  0.10750607821994759
iteration : 12630
train acc:  0.75
train loss:  0.4520588517189026
train gradient:  0.11702936345663287
iteration : 12631
train acc:  0.71875
train loss:  0.4835202693939209
train gradient:  0.1203586408847353
iteration : 12632
train acc:  0.7421875
train loss:  0.5023311376571655
train gradient:  0.14371040589376274
iteration : 12633
train acc:  0.6875
train loss:  0.5147637724876404
train gradient:  0.13311582613019116
iteration : 12634
train acc:  0.7421875
train loss:  0.5111027359962463
train gradient:  0.1253970349056613
iteration : 12635
train acc:  0.7578125
train loss:  0.49188703298568726
train gradient:  0.10936177700903181
iteration : 12636
train acc:  0.734375
train loss:  0.4992952346801758
train gradient:  0.13479232835049332
iteration : 12637
train acc:  0.7265625
train loss:  0.518254280090332
train gradient:  0.14715202366636543
iteration : 12638
train acc:  0.6953125
train loss:  0.5425307750701904
train gradient:  0.13443863950347906
iteration : 12639
train acc:  0.7734375
train loss:  0.47975486516952515
train gradient:  0.12338815704509021
iteration : 12640
train acc:  0.75
train loss:  0.523249626159668
train gradient:  0.11817068938263603
iteration : 12641
train acc:  0.6796875
train loss:  0.5061441659927368
train gradient:  0.13061413244136078
iteration : 12642
train acc:  0.8046875
train loss:  0.412065327167511
train gradient:  0.09549551969722898
iteration : 12643
train acc:  0.7578125
train loss:  0.46429571509361267
train gradient:  0.08527602176408677
iteration : 12644
train acc:  0.734375
train loss:  0.49132436513900757
train gradient:  0.14284723892766288
iteration : 12645
train acc:  0.765625
train loss:  0.48475247621536255
train gradient:  0.12765057574177957
iteration : 12646
train acc:  0.765625
train loss:  0.4269335865974426
train gradient:  0.0969619145768558
iteration : 12647
train acc:  0.7734375
train loss:  0.4578775465488434
train gradient:  0.1211098249542235
iteration : 12648
train acc:  0.6875
train loss:  0.498814195394516
train gradient:  0.10756591376562101
iteration : 12649
train acc:  0.7578125
train loss:  0.5106697082519531
train gradient:  0.12394513591390782
iteration : 12650
train acc:  0.78125
train loss:  0.4694332480430603
train gradient:  0.12296280784206409
iteration : 12651
train acc:  0.7421875
train loss:  0.4995904862880707
train gradient:  0.13360632541135087
iteration : 12652
train acc:  0.765625
train loss:  0.46071720123291016
train gradient:  0.09732581387882272
iteration : 12653
train acc:  0.7421875
train loss:  0.5178838968276978
train gradient:  0.14530710419588214
iteration : 12654
train acc:  0.703125
train loss:  0.48469364643096924
train gradient:  0.0894034225234929
iteration : 12655
train acc:  0.78125
train loss:  0.4775698781013489
train gradient:  0.13635882152395612
iteration : 12656
train acc:  0.7265625
train loss:  0.4899994730949402
train gradient:  0.10363252296263978
iteration : 12657
train acc:  0.7265625
train loss:  0.5150395035743713
train gradient:  0.11738549952756828
iteration : 12658
train acc:  0.78125
train loss:  0.4304860234260559
train gradient:  0.09819790304246026
iteration : 12659
train acc:  0.7578125
train loss:  0.5620744228363037
train gradient:  0.21633385548532752
iteration : 12660
train acc:  0.7578125
train loss:  0.47099122405052185
train gradient:  0.1051656506306899
iteration : 12661
train acc:  0.6953125
train loss:  0.5741969347000122
train gradient:  0.14536938165109548
iteration : 12662
train acc:  0.78125
train loss:  0.4646075367927551
train gradient:  0.10452223429530437
iteration : 12663
train acc:  0.765625
train loss:  0.4437083601951599
train gradient:  0.09867984388456139
iteration : 12664
train acc:  0.734375
train loss:  0.5173107981681824
train gradient:  0.13949653087109473
iteration : 12665
train acc:  0.7421875
train loss:  0.4768681824207306
train gradient:  0.12202289593602797
iteration : 12666
train acc:  0.7109375
train loss:  0.4814877510070801
train gradient:  0.1190231642155385
iteration : 12667
train acc:  0.8125
train loss:  0.4311278760433197
train gradient:  0.09871339566033965
iteration : 12668
train acc:  0.7109375
train loss:  0.5721917748451233
train gradient:  0.16321806934456928
iteration : 12669
train acc:  0.65625
train loss:  0.5744889378547668
train gradient:  0.1538814591544877
iteration : 12670
train acc:  0.671875
train loss:  0.5354433059692383
train gradient:  0.13440347473191774
iteration : 12671
train acc:  0.8203125
train loss:  0.4015088677406311
train gradient:  0.10149360761589335
iteration : 12672
train acc:  0.734375
train loss:  0.5038336515426636
train gradient:  0.14309596866759544
iteration : 12673
train acc:  0.7734375
train loss:  0.45602548122406006
train gradient:  0.10315928401892314
iteration : 12674
train acc:  0.765625
train loss:  0.48233911395072937
train gradient:  0.10188959373581088
iteration : 12675
train acc:  0.765625
train loss:  0.513269305229187
train gradient:  0.12505686523593196
iteration : 12676
train acc:  0.796875
train loss:  0.46902287006378174
train gradient:  0.15639906588560187
iteration : 12677
train acc:  0.7421875
train loss:  0.46500760316848755
train gradient:  0.08843297831667689
iteration : 12678
train acc:  0.7109375
train loss:  0.5631693601608276
train gradient:  0.13696218264982485
iteration : 12679
train acc:  0.8046875
train loss:  0.44498753547668457
train gradient:  0.09693326827706925
iteration : 12680
train acc:  0.765625
train loss:  0.5004028081893921
train gradient:  0.11551815274522888
iteration : 12681
train acc:  0.71875
train loss:  0.517990231513977
train gradient:  0.12167919897485127
iteration : 12682
train acc:  0.7265625
train loss:  0.4645630717277527
train gradient:  0.11187946693029585
iteration : 12683
train acc:  0.796875
train loss:  0.4066317081451416
train gradient:  0.07950020817140388
iteration : 12684
train acc:  0.7578125
train loss:  0.48268985748291016
train gradient:  0.12192834752040163
iteration : 12685
train acc:  0.71875
train loss:  0.49862709641456604
train gradient:  0.15466911461785482
iteration : 12686
train acc:  0.7265625
train loss:  0.4813278913497925
train gradient:  0.11448540109272665
iteration : 12687
train acc:  0.7890625
train loss:  0.4746003746986389
train gradient:  0.10089932929261736
iteration : 12688
train acc:  0.6953125
train loss:  0.4695396423339844
train gradient:  0.1402754527541418
iteration : 12689
train acc:  0.7265625
train loss:  0.5406109094619751
train gradient:  0.1544598956868483
iteration : 12690
train acc:  0.71875
train loss:  0.4520070552825928
train gradient:  0.10119501361973406
iteration : 12691
train acc:  0.7734375
train loss:  0.48714694380760193
train gradient:  0.12184189577165028
iteration : 12692
train acc:  0.6875
train loss:  0.5183404684066772
train gradient:  0.12764696550697835
iteration : 12693
train acc:  0.7734375
train loss:  0.47752851247787476
train gradient:  0.11715508463616629
iteration : 12694
train acc:  0.7109375
train loss:  0.45580732822418213
train gradient:  0.12010664474207962
iteration : 12695
train acc:  0.75
train loss:  0.44238895177841187
train gradient:  0.08829290969580314
iteration : 12696
train acc:  0.8125
train loss:  0.44814205169677734
train gradient:  0.11892269910393997
iteration : 12697
train acc:  0.78125
train loss:  0.44400477409362793
train gradient:  0.13518885143575832
iteration : 12698
train acc:  0.765625
train loss:  0.47451353073120117
train gradient:  0.10445005443858695
iteration : 12699
train acc:  0.765625
train loss:  0.4489760994911194
train gradient:  0.1155984887770332
iteration : 12700
train acc:  0.7890625
train loss:  0.46511876583099365
train gradient:  0.12532630181322163
iteration : 12701
train acc:  0.7265625
train loss:  0.5503626465797424
train gradient:  0.1748293499740808
iteration : 12702
train acc:  0.71875
train loss:  0.4858754277229309
train gradient:  0.09196309280723805
iteration : 12703
train acc:  0.7890625
train loss:  0.47238826751708984
train gradient:  0.09842268858755707
iteration : 12704
train acc:  0.828125
train loss:  0.4339919090270996
train gradient:  0.11222953065567198
iteration : 12705
train acc:  0.796875
train loss:  0.41602933406829834
train gradient:  0.08757212439522433
iteration : 12706
train acc:  0.734375
train loss:  0.525413453578949
train gradient:  0.15437920680699313
iteration : 12707
train acc:  0.7578125
train loss:  0.5428399443626404
train gradient:  0.1772077259850548
iteration : 12708
train acc:  0.71875
train loss:  0.5272538661956787
train gradient:  0.11818703348775747
iteration : 12709
train acc:  0.734375
train loss:  0.510125458240509
train gradient:  0.13787333843419558
iteration : 12710
train acc:  0.71875
train loss:  0.4628823399543762
train gradient:  0.09710612003979802
iteration : 12711
train acc:  0.71875
train loss:  0.47283709049224854
train gradient:  0.13741769838596607
iteration : 12712
train acc:  0.7734375
train loss:  0.4393620491027832
train gradient:  0.0988352029244728
iteration : 12713
train acc:  0.7265625
train loss:  0.5143253207206726
train gradient:  0.11248572812648912
iteration : 12714
train acc:  0.7265625
train loss:  0.45574483275413513
train gradient:  0.10130604015619267
iteration : 12715
train acc:  0.75
train loss:  0.46181902289390564
train gradient:  0.10174544721932975
iteration : 12716
train acc:  0.703125
train loss:  0.5401671528816223
train gradient:  0.18114248057334598
iteration : 12717
train acc:  0.7421875
train loss:  0.5157801508903503
train gradient:  0.13160162123718921
iteration : 12718
train acc:  0.703125
train loss:  0.5523573756217957
train gradient:  0.14544823517092836
iteration : 12719
train acc:  0.6640625
train loss:  0.5766983032226562
train gradient:  0.20630706155385986
iteration : 12720
train acc:  0.7265625
train loss:  0.45988714694976807
train gradient:  0.09663682112638716
iteration : 12721
train acc:  0.7578125
train loss:  0.5214440822601318
train gradient:  0.17487038225064272
iteration : 12722
train acc:  0.71875
train loss:  0.4645604193210602
train gradient:  0.12141801596956142
iteration : 12723
train acc:  0.78125
train loss:  0.4658746123313904
train gradient:  0.11484416066887554
iteration : 12724
train acc:  0.8125
train loss:  0.41229525208473206
train gradient:  0.08895970178833283
iteration : 12725
train acc:  0.7734375
train loss:  0.44087865948677063
train gradient:  0.12908264947359377
iteration : 12726
train acc:  0.734375
train loss:  0.5336018800735474
train gradient:  0.1397965911041401
iteration : 12727
train acc:  0.6875
train loss:  0.5012294054031372
train gradient:  0.12813185649920356
iteration : 12728
train acc:  0.7421875
train loss:  0.5435028076171875
train gradient:  0.16607595891930932
iteration : 12729
train acc:  0.71875
train loss:  0.45847344398498535
train gradient:  0.11351656062164045
iteration : 12730
train acc:  0.7421875
train loss:  0.5220217704772949
train gradient:  0.13253213215975454
iteration : 12731
train acc:  0.765625
train loss:  0.4547863006591797
train gradient:  0.09598394011500314
iteration : 12732
train acc:  0.734375
train loss:  0.5286677479743958
train gradient:  0.12541691341481676
iteration : 12733
train acc:  0.8046875
train loss:  0.43419939279556274
train gradient:  0.11804938165738797
iteration : 12734
train acc:  0.765625
train loss:  0.4288625121116638
train gradient:  0.09148744957842911
iteration : 12735
train acc:  0.7109375
train loss:  0.5266952514648438
train gradient:  0.13027795397970812
iteration : 12736
train acc:  0.8125
train loss:  0.47895997762680054
train gradient:  0.1533159225151136
iteration : 12737
train acc:  0.78125
train loss:  0.43375882506370544
train gradient:  0.09811031129655089
iteration : 12738
train acc:  0.75
train loss:  0.4756372570991516
train gradient:  0.09443538402256542
iteration : 12739
train acc:  0.7265625
train loss:  0.5574952960014343
train gradient:  0.1483112255650545
iteration : 12740
train acc:  0.78125
train loss:  0.47727781534194946
train gradient:  0.09801005336116093
iteration : 12741
train acc:  0.703125
train loss:  0.5045167207717896
train gradient:  0.1165823889680495
iteration : 12742
train acc:  0.7734375
train loss:  0.4515126645565033
train gradient:  0.09700650123438241
iteration : 12743
train acc:  0.7734375
train loss:  0.4877689480781555
train gradient:  0.13071755249003492
iteration : 12744
train acc:  0.7734375
train loss:  0.4687315821647644
train gradient:  0.17997311331188104
iteration : 12745
train acc:  0.6953125
train loss:  0.5153181552886963
train gradient:  0.14714651611540913
iteration : 12746
train acc:  0.71875
train loss:  0.5392730236053467
train gradient:  0.1280511536733922
iteration : 12747
train acc:  0.7734375
train loss:  0.48705002665519714
train gradient:  0.115658597692206
iteration : 12748
train acc:  0.828125
train loss:  0.404369592666626
train gradient:  0.08963386595476464
iteration : 12749
train acc:  0.75
train loss:  0.4819297194480896
train gradient:  0.13235993397993362
iteration : 12750
train acc:  0.7421875
train loss:  0.5552577376365662
train gradient:  0.14671123177818185
iteration : 12751
train acc:  0.7421875
train loss:  0.4499526023864746
train gradient:  0.10740080894003447
iteration : 12752
train acc:  0.78125
train loss:  0.44454002380371094
train gradient:  0.10700202746046826
iteration : 12753
train acc:  0.7421875
train loss:  0.4675332307815552
train gradient:  0.10339685400514728
iteration : 12754
train acc:  0.8046875
train loss:  0.3901076316833496
train gradient:  0.08580770478906156
iteration : 12755
train acc:  0.734375
train loss:  0.46538686752319336
train gradient:  0.1097145786573485
iteration : 12756
train acc:  0.765625
train loss:  0.5053325295448303
train gradient:  0.14069069380533286
iteration : 12757
train acc:  0.734375
train loss:  0.4801236391067505
train gradient:  0.13329894940143613
iteration : 12758
train acc:  0.75
train loss:  0.48478108644485474
train gradient:  0.15264920190378067
iteration : 12759
train acc:  0.7578125
train loss:  0.4431080222129822
train gradient:  0.09552438280937708
iteration : 12760
train acc:  0.7421875
train loss:  0.5317211151123047
train gradient:  0.17721470626495955
iteration : 12761
train acc:  0.671875
train loss:  0.5415560007095337
train gradient:  0.11897189376516498
iteration : 12762
train acc:  0.734375
train loss:  0.5475000143051147
train gradient:  0.19799912342176268
iteration : 12763
train acc:  0.78125
train loss:  0.48323971033096313
train gradient:  0.13403066589656934
iteration : 12764
train acc:  0.75
train loss:  0.5221192240715027
train gradient:  0.15995752574984057
iteration : 12765
train acc:  0.7578125
train loss:  0.48594290018081665
train gradient:  0.11538429939084935
iteration : 12766
train acc:  0.7578125
train loss:  0.41178375482559204
train gradient:  0.13389098861629
iteration : 12767
train acc:  0.7109375
train loss:  0.5496358275413513
train gradient:  0.1382199358884451
iteration : 12768
train acc:  0.796875
train loss:  0.4279976189136505
train gradient:  0.10458268606322974
iteration : 12769
train acc:  0.75
train loss:  0.45776253938674927
train gradient:  0.1327913286175675
iteration : 12770
train acc:  0.75
train loss:  0.4778438210487366
train gradient:  0.10535355884598899
iteration : 12771
train acc:  0.7578125
train loss:  0.5135060548782349
train gradient:  0.14276143698005467
iteration : 12772
train acc:  0.75
train loss:  0.46540361642837524
train gradient:  0.1246619002279121
iteration : 12773
train acc:  0.703125
train loss:  0.489374041557312
train gradient:  0.10163965444936215
iteration : 12774
train acc:  0.7265625
train loss:  0.49479448795318604
train gradient:  0.1213009552311435
iteration : 12775
train acc:  0.75
train loss:  0.503154993057251
train gradient:  0.12791411231995714
iteration : 12776
train acc:  0.765625
train loss:  0.4635646343231201
train gradient:  0.15869467717724334
iteration : 12777
train acc:  0.7734375
train loss:  0.4662124216556549
train gradient:  0.11823703649872756
iteration : 12778
train acc:  0.8046875
train loss:  0.37431007623672485
train gradient:  0.07498711381243481
iteration : 12779
train acc:  0.7734375
train loss:  0.43621474504470825
train gradient:  0.1041088091380032
iteration : 12780
train acc:  0.7578125
train loss:  0.45057040452957153
train gradient:  0.11500891691754962
iteration : 12781
train acc:  0.7109375
train loss:  0.47547298669815063
train gradient:  0.11564934487324013
iteration : 12782
train acc:  0.75
train loss:  0.49310925602912903
train gradient:  0.13338273642235232
iteration : 12783
train acc:  0.7109375
train loss:  0.504585325717926
train gradient:  0.12937540733232109
iteration : 12784
train acc:  0.7734375
train loss:  0.44105416536331177
train gradient:  0.16057460655549988
iteration : 12785
train acc:  0.7265625
train loss:  0.4794929027557373
train gradient:  0.1073951771139813
iteration : 12786
train acc:  0.7734375
train loss:  0.4834279716014862
train gradient:  0.11416303914513866
iteration : 12787
train acc:  0.75
train loss:  0.4858798384666443
train gradient:  0.1135439128966929
iteration : 12788
train acc:  0.75
train loss:  0.5417513251304626
train gradient:  0.15477393504802345
iteration : 12789
train acc:  0.7265625
train loss:  0.4982510507106781
train gradient:  0.12257985207033711
iteration : 12790
train acc:  0.71875
train loss:  0.49573439359664917
train gradient:  0.14422242372388533
iteration : 12791
train acc:  0.734375
train loss:  0.5104584097862244
train gradient:  0.13127162757000654
iteration : 12792
train acc:  0.6875
train loss:  0.5556719899177551
train gradient:  0.1331209751773138
iteration : 12793
train acc:  0.765625
train loss:  0.47671347856521606
train gradient:  0.12167520267998987
iteration : 12794
train acc:  0.734375
train loss:  0.5444290637969971
train gradient:  0.1607582077046973
iteration : 12795
train acc:  0.71875
train loss:  0.5204407572746277
train gradient:  0.1164126348250376
iteration : 12796
train acc:  0.7578125
train loss:  0.5072498321533203
train gradient:  0.11473956091525338
iteration : 12797
train acc:  0.8203125
train loss:  0.44707685708999634
train gradient:  0.11416174385308145
iteration : 12798
train acc:  0.6875
train loss:  0.572952926158905
train gradient:  0.1367672151559548
iteration : 12799
train acc:  0.8359375
train loss:  0.3825966715812683
train gradient:  0.07816790670766557
iteration : 12800
train acc:  0.6875
train loss:  0.541585385799408
train gradient:  0.15002382913685688
iteration : 12801
train acc:  0.78125
train loss:  0.43852293491363525
train gradient:  0.09734391208020374
iteration : 12802
train acc:  0.6875
train loss:  0.5126193165779114
train gradient:  0.1152124397768379
iteration : 12803
train acc:  0.703125
train loss:  0.48629599809646606
train gradient:  0.12354378169408106
iteration : 12804
train acc:  0.71875
train loss:  0.5229039192199707
train gradient:  0.14893758133015197
iteration : 12805
train acc:  0.7578125
train loss:  0.48550355434417725
train gradient:  0.1320811456272386
iteration : 12806
train acc:  0.75
train loss:  0.46704965829849243
train gradient:  0.12085351614127815
iteration : 12807
train acc:  0.734375
train loss:  0.45970600843429565
train gradient:  0.11052381481401749
iteration : 12808
train acc:  0.765625
train loss:  0.4795589745044708
train gradient:  0.14584366720592112
iteration : 12809
train acc:  0.703125
train loss:  0.5944182276725769
train gradient:  0.16897240629528665
iteration : 12810
train acc:  0.71875
train loss:  0.5412992835044861
train gradient:  0.20866926348251175
iteration : 12811
train acc:  0.78125
train loss:  0.4805951416492462
train gradient:  0.1373593568478903
iteration : 12812
train acc:  0.7421875
train loss:  0.48258793354034424
train gradient:  0.12424389038565158
iteration : 12813
train acc:  0.7578125
train loss:  0.46689021587371826
train gradient:  0.1370549186870581
iteration : 12814
train acc:  0.75
train loss:  0.5402505397796631
train gradient:  0.13195942506981675
iteration : 12815
train acc:  0.75
train loss:  0.5520710349082947
train gradient:  0.15134143554653923
iteration : 12816
train acc:  0.78125
train loss:  0.4675886631011963
train gradient:  0.129044666507305
iteration : 12817
train acc:  0.7578125
train loss:  0.49178922176361084
train gradient:  0.12870748555428252
iteration : 12818
train acc:  0.71875
train loss:  0.4951033890247345
train gradient:  0.12496490675932387
iteration : 12819
train acc:  0.7109375
train loss:  0.479576975107193
train gradient:  0.131818983117863
iteration : 12820
train acc:  0.6875
train loss:  0.5153091549873352
train gradient:  0.141463218377238
iteration : 12821
train acc:  0.78125
train loss:  0.48567330837249756
train gradient:  0.1217608815139925
iteration : 12822
train acc:  0.7109375
train loss:  0.46672695875167847
train gradient:  0.1095905604500796
iteration : 12823
train acc:  0.7578125
train loss:  0.4910694360733032
train gradient:  0.11283620909943276
iteration : 12824
train acc:  0.8359375
train loss:  0.4100205898284912
train gradient:  0.07862850979154447
iteration : 12825
train acc:  0.7890625
train loss:  0.43025320768356323
train gradient:  0.1095469542875858
iteration : 12826
train acc:  0.671875
train loss:  0.5491544604301453
train gradient:  0.15689639061241795
iteration : 12827
train acc:  0.7421875
train loss:  0.5365855693817139
train gradient:  0.15745676075029022
iteration : 12828
train acc:  0.7421875
train loss:  0.5565174221992493
train gradient:  0.16083666695162835
iteration : 12829
train acc:  0.75
train loss:  0.4624684751033783
train gradient:  0.13633565471156345
iteration : 12830
train acc:  0.7578125
train loss:  0.4498720169067383
train gradient:  0.0988451402120019
iteration : 12831
train acc:  0.75
train loss:  0.48431694507598877
train gradient:  0.10890500043746294
iteration : 12832
train acc:  0.796875
train loss:  0.41546306014060974
train gradient:  0.11635754812970628
iteration : 12833
train acc:  0.75
train loss:  0.5204529762268066
train gradient:  0.14702229811347492
iteration : 12834
train acc:  0.78125
train loss:  0.4481232762336731
train gradient:  0.10653783220991227
iteration : 12835
train acc:  0.796875
train loss:  0.48285141587257385
train gradient:  0.14172570588039002
iteration : 12836
train acc:  0.75
train loss:  0.44704127311706543
train gradient:  0.10368674994121721
iteration : 12837
train acc:  0.734375
train loss:  0.5503798723220825
train gradient:  0.15138080313391833
iteration : 12838
train acc:  0.7109375
train loss:  0.51380455493927
train gradient:  0.11492765315650817
iteration : 12839
train acc:  0.7734375
train loss:  0.4547375738620758
train gradient:  0.09768818927150748
iteration : 12840
train acc:  0.7578125
train loss:  0.49001753330230713
train gradient:  0.15825722469998213
iteration : 12841
train acc:  0.71875
train loss:  0.4896593391895294
train gradient:  0.16901183639624806
iteration : 12842
train acc:  0.6953125
train loss:  0.5355433821678162
train gradient:  0.12691525409921375
iteration : 12843
train acc:  0.7578125
train loss:  0.4885675311088562
train gradient:  0.1375139208590247
iteration : 12844
train acc:  0.78125
train loss:  0.5019444823265076
train gradient:  0.12769954146985002
iteration : 12845
train acc:  0.796875
train loss:  0.43888986110687256
train gradient:  0.11025147669647442
iteration : 12846
train acc:  0.7890625
train loss:  0.4868166148662567
train gradient:  0.11959545827038685
iteration : 12847
train acc:  0.7265625
train loss:  0.5223076343536377
train gradient:  0.13307422524515375
iteration : 12848
train acc:  0.703125
train loss:  0.47792190313339233
train gradient:  0.11674796721868266
iteration : 12849
train acc:  0.7578125
train loss:  0.4516206383705139
train gradient:  0.10005126790824975
iteration : 12850
train acc:  0.6875
train loss:  0.5129666924476624
train gradient:  0.16486036452306357
iteration : 12851
train acc:  0.7890625
train loss:  0.4734543561935425
train gradient:  0.12176814590151745
iteration : 12852
train acc:  0.6953125
train loss:  0.5091284513473511
train gradient:  0.14676882476733027
iteration : 12853
train acc:  0.7421875
train loss:  0.45679527521133423
train gradient:  0.13222288886243547
iteration : 12854
train acc:  0.75
train loss:  0.49380362033843994
train gradient:  0.1130943920819884
iteration : 12855
train acc:  0.78125
train loss:  0.48927435278892517
train gradient:  0.13616821062638473
iteration : 12856
train acc:  0.7265625
train loss:  0.5647980570793152
train gradient:  0.2130070607257121
iteration : 12857
train acc:  0.796875
train loss:  0.43205034732818604
train gradient:  0.1046465180358061
iteration : 12858
train acc:  0.75
train loss:  0.4884553551673889
train gradient:  0.10781047826973057
iteration : 12859
train acc:  0.6953125
train loss:  0.5471256971359253
train gradient:  0.1601778259364126
iteration : 12860
train acc:  0.640625
train loss:  0.5943350791931152
train gradient:  0.18260524581970536
iteration : 12861
train acc:  0.7421875
train loss:  0.4537907540798187
train gradient:  0.10621023857614173
iteration : 12862
train acc:  0.84375
train loss:  0.4188684821128845
train gradient:  0.09515395978113124
iteration : 12863
train acc:  0.765625
train loss:  0.4499807357788086
train gradient:  0.11618731476900324
iteration : 12864
train acc:  0.71875
train loss:  0.5425174832344055
train gradient:  0.15323564452991378
iteration : 12865
train acc:  0.71875
train loss:  0.52333664894104
train gradient:  0.14425530019760202
iteration : 12866
train acc:  0.6796875
train loss:  0.5669804215431213
train gradient:  0.1843973831261876
iteration : 12867
train acc:  0.7578125
train loss:  0.4484799802303314
train gradient:  0.11488839811419303
iteration : 12868
train acc:  0.6875
train loss:  0.5304257869720459
train gradient:  0.1399974783773879
iteration : 12869
train acc:  0.7578125
train loss:  0.48626163601875305
train gradient:  0.10262469173438127
iteration : 12870
train acc:  0.75
train loss:  0.5120450854301453
train gradient:  0.12551299537976707
iteration : 12871
train acc:  0.7578125
train loss:  0.46134865283966064
train gradient:  0.09456851839945667
iteration : 12872
train acc:  0.7734375
train loss:  0.44856971502304077
train gradient:  0.09312840835430335
iteration : 12873
train acc:  0.7578125
train loss:  0.46168941259384155
train gradient:  0.09907897638303866
iteration : 12874
train acc:  0.75
train loss:  0.4386614263057709
train gradient:  0.0940486710131703
iteration : 12875
train acc:  0.7890625
train loss:  0.44074875116348267
train gradient:  0.0949302090007249
iteration : 12876
train acc:  0.6328125
train loss:  0.6079298257827759
train gradient:  0.2116121026596549
iteration : 12877
train acc:  0.7734375
train loss:  0.46117347478866577
train gradient:  0.10751012091379811
iteration : 12878
train acc:  0.6640625
train loss:  0.5845390558242798
train gradient:  0.1587641704753054
iteration : 12879
train acc:  0.75
train loss:  0.45005813241004944
train gradient:  0.08470696550574457
iteration : 12880
train acc:  0.71875
train loss:  0.5340968370437622
train gradient:  0.1838490723793595
iteration : 12881
train acc:  0.7265625
train loss:  0.5537115335464478
train gradient:  0.16817185384741795
iteration : 12882
train acc:  0.703125
train loss:  0.5500738620758057
train gradient:  0.14627533410845361
iteration : 12883
train acc:  0.7734375
train loss:  0.44025397300720215
train gradient:  0.09579676823652761
iteration : 12884
train acc:  0.6484375
train loss:  0.5493372678756714
train gradient:  0.12336926339700412
iteration : 12885
train acc:  0.6875
train loss:  0.5361900329589844
train gradient:  0.15931066641372682
iteration : 12886
train acc:  0.75
train loss:  0.5134317278862
train gradient:  0.1395473236865971
iteration : 12887
train acc:  0.8359375
train loss:  0.41824662685394287
train gradient:  0.1018846168173595
iteration : 12888
train acc:  0.6875
train loss:  0.5091947317123413
train gradient:  0.1321544809348555
iteration : 12889
train acc:  0.7578125
train loss:  0.448055237531662
train gradient:  0.10878286953452473
iteration : 12890
train acc:  0.671875
train loss:  0.5691021680831909
train gradient:  0.15250886481500137
iteration : 12891
train acc:  0.71875
train loss:  0.5194283723831177
train gradient:  0.12892302533373623
iteration : 12892
train acc:  0.734375
train loss:  0.4390026926994324
train gradient:  0.10865189488719988
iteration : 12893
train acc:  0.7734375
train loss:  0.4859274625778198
train gradient:  0.10687854084941675
iteration : 12894
train acc:  0.7109375
train loss:  0.4508582353591919
train gradient:  0.0968039049532184
iteration : 12895
train acc:  0.7734375
train loss:  0.5046588778495789
train gradient:  0.12894525776226679
iteration : 12896
train acc:  0.796875
train loss:  0.4570470452308655
train gradient:  0.11399271495894206
iteration : 12897
train acc:  0.8046875
train loss:  0.4294317960739136
train gradient:  0.0960311130099643
iteration : 12898
train acc:  0.71875
train loss:  0.4968700706958771
train gradient:  0.11729780285323403
iteration : 12899
train acc:  0.734375
train loss:  0.4957931637763977
train gradient:  0.14731368868994144
iteration : 12900
train acc:  0.703125
train loss:  0.5094215273857117
train gradient:  0.12569473924149346
iteration : 12901
train acc:  0.7578125
train loss:  0.5274775624275208
train gradient:  0.13085469382312892
iteration : 12902
train acc:  0.796875
train loss:  0.44933021068573
train gradient:  0.09379211085017738
iteration : 12903
train acc:  0.8203125
train loss:  0.41327571868896484
train gradient:  0.08777300555084458
iteration : 12904
train acc:  0.7265625
train loss:  0.5084258913993835
train gradient:  0.11297012725043831
iteration : 12905
train acc:  0.78125
train loss:  0.4423656165599823
train gradient:  0.11882247063465101
iteration : 12906
train acc:  0.75
train loss:  0.48683154582977295
train gradient:  0.1313763602815672
iteration : 12907
train acc:  0.734375
train loss:  0.5155858993530273
train gradient:  0.11728665961073774
iteration : 12908
train acc:  0.71875
train loss:  0.49264049530029297
train gradient:  0.17127488446751815
iteration : 12909
train acc:  0.7578125
train loss:  0.46776366233825684
train gradient:  0.10023691903488954
iteration : 12910
train acc:  0.7109375
train loss:  0.5787216424942017
train gradient:  0.14968813202386913
iteration : 12911
train acc:  0.7890625
train loss:  0.4217372536659241
train gradient:  0.08977598674354878
iteration : 12912
train acc:  0.7578125
train loss:  0.4471440315246582
train gradient:  0.09486908898481031
iteration : 12913
train acc:  0.7734375
train loss:  0.4797801971435547
train gradient:  0.11694322326300288
iteration : 12914
train acc:  0.7265625
train loss:  0.47353121638298035
train gradient:  0.11422711244844275
iteration : 12915
train acc:  0.6796875
train loss:  0.5499405264854431
train gradient:  0.14729900758936015
iteration : 12916
train acc:  0.7265625
train loss:  0.5028595924377441
train gradient:  0.15099113741534198
iteration : 12917
train acc:  0.71875
train loss:  0.5051745772361755
train gradient:  0.13035516476672354
iteration : 12918
train acc:  0.6875
train loss:  0.48860135674476624
train gradient:  0.10941338437646674
iteration : 12919
train acc:  0.8203125
train loss:  0.44337761402130127
train gradient:  0.12377174015172789
iteration : 12920
train acc:  0.6875
train loss:  0.5280585885047913
train gradient:  0.1290815891880207
iteration : 12921
train acc:  0.6796875
train loss:  0.5637272000312805
train gradient:  0.11041415982127169
iteration : 12922
train acc:  0.8046875
train loss:  0.44527846574783325
train gradient:  0.09059687206969366
iteration : 12923
train acc:  0.734375
train loss:  0.461900919675827
train gradient:  0.1181182306169975
iteration : 12924
train acc:  0.75
train loss:  0.5162695050239563
train gradient:  0.122479081144999
iteration : 12925
train acc:  0.7734375
train loss:  0.48136547207832336
train gradient:  0.11061295752928665
iteration : 12926
train acc:  0.796875
train loss:  0.40937286615371704
train gradient:  0.08127249942942338
iteration : 12927
train acc:  0.75
train loss:  0.4725053906440735
train gradient:  0.10040044590222488
iteration : 12928
train acc:  0.796875
train loss:  0.44510066509246826
train gradient:  0.09813800973768172
iteration : 12929
train acc:  0.7265625
train loss:  0.4996408522129059
train gradient:  0.12778286316319779
iteration : 12930
train acc:  0.7265625
train loss:  0.48519468307495117
train gradient:  0.12783048214680154
iteration : 12931
train acc:  0.6953125
train loss:  0.5263421535491943
train gradient:  0.14224600931878834
iteration : 12932
train acc:  0.7734375
train loss:  0.4750362038612366
train gradient:  0.14228226932961333
iteration : 12933
train acc:  0.7890625
train loss:  0.4702376127243042
train gradient:  0.10714905303828935
iteration : 12934
train acc:  0.7265625
train loss:  0.5179890394210815
train gradient:  0.12306629593134219
iteration : 12935
train acc:  0.71875
train loss:  0.5032510161399841
train gradient:  0.12006976567426421
iteration : 12936
train acc:  0.7890625
train loss:  0.46519505977630615
train gradient:  0.10511077228579017
iteration : 12937
train acc:  0.6953125
train loss:  0.5363650321960449
train gradient:  0.14655414941888076
iteration : 12938
train acc:  0.828125
train loss:  0.4048318862915039
train gradient:  0.07412142358361137
iteration : 12939
train acc:  0.765625
train loss:  0.4851604700088501
train gradient:  0.1295120757191179
iteration : 12940
train acc:  0.6953125
train loss:  0.5627045035362244
train gradient:  0.14495587532877202
iteration : 12941
train acc:  0.7421875
train loss:  0.48084011673927307
train gradient:  0.13754546871307283
iteration : 12942
train acc:  0.7265625
train loss:  0.5414576530456543
train gradient:  0.14874892110607554
iteration : 12943
train acc:  0.7109375
train loss:  0.5147545337677002
train gradient:  0.12794279930040056
iteration : 12944
train acc:  0.7734375
train loss:  0.4739186465740204
train gradient:  0.10080897399000166
iteration : 12945
train acc:  0.7421875
train loss:  0.5624520182609558
train gradient:  0.16346248343342204
iteration : 12946
train acc:  0.765625
train loss:  0.49007749557495117
train gradient:  0.13652216827014274
iteration : 12947
train acc:  0.8515625
train loss:  0.4259740710258484
train gradient:  0.08814576848317517
iteration : 12948
train acc:  0.765625
train loss:  0.4467371702194214
train gradient:  0.11160968893861731
iteration : 12949
train acc:  0.796875
train loss:  0.4260057210922241
train gradient:  0.09669050932158613
iteration : 12950
train acc:  0.734375
train loss:  0.5177305936813354
train gradient:  0.13448022761375902
iteration : 12951
train acc:  0.7109375
train loss:  0.5004194378852844
train gradient:  0.15665075815947588
iteration : 12952
train acc:  0.7109375
train loss:  0.4907781183719635
train gradient:  0.10351027301611775
iteration : 12953
train acc:  0.765625
train loss:  0.49564439058303833
train gradient:  0.12615804528454458
iteration : 12954
train acc:  0.7109375
train loss:  0.48191267251968384
train gradient:  0.11218264691694833
iteration : 12955
train acc:  0.7421875
train loss:  0.5213143825531006
train gradient:  0.14398293614761462
iteration : 12956
train acc:  0.8203125
train loss:  0.4116279184818268
train gradient:  0.11623994006516498
iteration : 12957
train acc:  0.7578125
train loss:  0.48570317029953003
train gradient:  0.14581239319163025
iteration : 12958
train acc:  0.7734375
train loss:  0.4596196115016937
train gradient:  0.10308630684559632
iteration : 12959
train acc:  0.734375
train loss:  0.5125082731246948
train gradient:  0.1553477990443521
iteration : 12960
train acc:  0.7109375
train loss:  0.5202344059944153
train gradient:  0.12901147783241274
iteration : 12961
train acc:  0.7890625
train loss:  0.43647271394729614
train gradient:  0.08129575459183455
iteration : 12962
train acc:  0.6640625
train loss:  0.5307585000991821
train gradient:  0.1250271474943464
iteration : 12963
train acc:  0.6875
train loss:  0.5247799158096313
train gradient:  0.13716072051741895
iteration : 12964
train acc:  0.7109375
train loss:  0.5138482451438904
train gradient:  0.18700347908154866
iteration : 12965
train acc:  0.75
train loss:  0.4592541456222534
train gradient:  0.10683510142328717
iteration : 12966
train acc:  0.75
train loss:  0.4850058853626251
train gradient:  0.1281910664849339
iteration : 12967
train acc:  0.7578125
train loss:  0.4640633463859558
train gradient:  0.11692201410457607
iteration : 12968
train acc:  0.65625
train loss:  0.6216932535171509
train gradient:  0.19863219396949472
iteration : 12969
train acc:  0.7734375
train loss:  0.4358898997306824
train gradient:  0.1210875679099852
iteration : 12970
train acc:  0.6875
train loss:  0.5202325582504272
train gradient:  0.1384955444564388
iteration : 12971
train acc:  0.6875
train loss:  0.5447896122932434
train gradient:  0.13256554348531224
iteration : 12972
train acc:  0.7421875
train loss:  0.48330920934677124
train gradient:  0.11970259625380761
iteration : 12973
train acc:  0.7578125
train loss:  0.49026116728782654
train gradient:  0.15664693701853993
iteration : 12974
train acc:  0.7421875
train loss:  0.49827948212623596
train gradient:  0.130524841265495
iteration : 12975
train acc:  0.7890625
train loss:  0.4297023117542267
train gradient:  0.10739900322201901
iteration : 12976
train acc:  0.7734375
train loss:  0.467191219329834
train gradient:  0.11417765296165504
iteration : 12977
train acc:  0.6796875
train loss:  0.5591813325881958
train gradient:  0.15437085844211879
iteration : 12978
train acc:  0.65625
train loss:  0.5670835971832275
train gradient:  0.18273904639563382
iteration : 12979
train acc:  0.7734375
train loss:  0.40902674198150635
train gradient:  0.09161708154808541
iteration : 12980
train acc:  0.71875
train loss:  0.4846404790878296
train gradient:  0.13164428416548266
iteration : 12981
train acc:  0.71875
train loss:  0.48774081468582153
train gradient:  0.123496024389934
iteration : 12982
train acc:  0.7734375
train loss:  0.4696747660636902
train gradient:  0.16066272359954897
iteration : 12983
train acc:  0.7265625
train loss:  0.5101145505905151
train gradient:  0.12587600488497236
iteration : 12984
train acc:  0.7890625
train loss:  0.413267582654953
train gradient:  0.08783102953430383
iteration : 12985
train acc:  0.6953125
train loss:  0.5794612169265747
train gradient:  0.15978211820215676
iteration : 12986
train acc:  0.8046875
train loss:  0.4621461033821106
train gradient:  0.10817140105304941
iteration : 12987
train acc:  0.75
train loss:  0.45704835653305054
train gradient:  0.09692960241960046
iteration : 12988
train acc:  0.71875
train loss:  0.4962114095687866
train gradient:  0.10839034643305615
iteration : 12989
train acc:  0.78125
train loss:  0.42944324016571045
train gradient:  0.1225824000096675
iteration : 12990
train acc:  0.6953125
train loss:  0.5380699038505554
train gradient:  0.12223540621231216
iteration : 12991
train acc:  0.7578125
train loss:  0.44619911909103394
train gradient:  0.09977729756991831
iteration : 12992
train acc:  0.75
train loss:  0.5104923248291016
train gradient:  0.14040894155489012
iteration : 12993
train acc:  0.7734375
train loss:  0.46066418290138245
train gradient:  0.10003465681416533
iteration : 12994
train acc:  0.7578125
train loss:  0.5150362849235535
train gradient:  0.16259517925163952
iteration : 12995
train acc:  0.7421875
train loss:  0.47453993558883667
train gradient:  0.14458205974347527
iteration : 12996
train acc:  0.7734375
train loss:  0.503056526184082
train gradient:  0.11004123904829659
iteration : 12997
train acc:  0.7265625
train loss:  0.5091098546981812
train gradient:  0.1447979034783375
iteration : 12998
train acc:  0.8125
train loss:  0.47788184881210327
train gradient:  0.11945909760267118
iteration : 12999
train acc:  0.7421875
train loss:  0.47082775831222534
train gradient:  0.11978605467394543
iteration : 13000
train acc:  0.6796875
train loss:  0.5334194302558899
train gradient:  0.15532534682780774
iteration : 13001
train acc:  0.8203125
train loss:  0.43047115206718445
train gradient:  0.09900960088522343
iteration : 13002
train acc:  0.8203125
train loss:  0.4474971294403076
train gradient:  0.09973669676885176
iteration : 13003
train acc:  0.7734375
train loss:  0.4751240313053131
train gradient:  0.11561819922270913
iteration : 13004
train acc:  0.7578125
train loss:  0.46699950098991394
train gradient:  0.11017723115832136
iteration : 13005
train acc:  0.734375
train loss:  0.4558594822883606
train gradient:  0.09972548115974664
iteration : 13006
train acc:  0.71875
train loss:  0.5270827412605286
train gradient:  0.12811993462361881
iteration : 13007
train acc:  0.7578125
train loss:  0.45700323581695557
train gradient:  0.10696734776450285
iteration : 13008
train acc:  0.7109375
train loss:  0.48388585448265076
train gradient:  0.12029855298872656
iteration : 13009
train acc:  0.765625
train loss:  0.47749432921409607
train gradient:  0.13345496581106492
iteration : 13010
train acc:  0.8046875
train loss:  0.43454986810684204
train gradient:  0.1070153598153634
iteration : 13011
train acc:  0.7109375
train loss:  0.5493313074111938
train gradient:  0.1550699963440376
iteration : 13012
train acc:  0.78125
train loss:  0.5021111965179443
train gradient:  0.13719122771918804
iteration : 13013
train acc:  0.734375
train loss:  0.46792176365852356
train gradient:  0.10784122159163743
iteration : 13014
train acc:  0.6875
train loss:  0.5930784344673157
train gradient:  0.15238792573935805
iteration : 13015
train acc:  0.75
train loss:  0.5160207748413086
train gradient:  0.11472074256420516
iteration : 13016
train acc:  0.71875
train loss:  0.47455352544784546
train gradient:  0.15208946683775737
iteration : 13017
train acc:  0.7109375
train loss:  0.5112082958221436
train gradient:  0.10320095542585048
iteration : 13018
train acc:  0.7265625
train loss:  0.49118566513061523
train gradient:  0.12444927182686408
iteration : 13019
train acc:  0.78125
train loss:  0.4180110692977905
train gradient:  0.09458712645061045
iteration : 13020
train acc:  0.71875
train loss:  0.5304824709892273
train gradient:  0.12050434083657015
iteration : 13021
train acc:  0.765625
train loss:  0.5127058029174805
train gradient:  0.16195100748640462
iteration : 13022
train acc:  0.7890625
train loss:  0.4382304847240448
train gradient:  0.11263723376114655
iteration : 13023
train acc:  0.734375
train loss:  0.4958375096321106
train gradient:  0.13477436835407638
iteration : 13024
train acc:  0.7890625
train loss:  0.4325929582118988
train gradient:  0.0938108062942037
iteration : 13025
train acc:  0.6953125
train loss:  0.5376003980636597
train gradient:  0.17373005710979772
iteration : 13026
train acc:  0.78125
train loss:  0.4881265163421631
train gradient:  0.1307870476671246
iteration : 13027
train acc:  0.703125
train loss:  0.5169060230255127
train gradient:  0.15293913129387326
iteration : 13028
train acc:  0.75
train loss:  0.4784586727619171
train gradient:  0.12709993454652047
iteration : 13029
train acc:  0.703125
train loss:  0.47806695103645325
train gradient:  0.12406911845571886
iteration : 13030
train acc:  0.7734375
train loss:  0.45473629236221313
train gradient:  0.10238350996825067
iteration : 13031
train acc:  0.8203125
train loss:  0.43274080753326416
train gradient:  0.08202265687415745
iteration : 13032
train acc:  0.7265625
train loss:  0.5255545377731323
train gradient:  0.1713021014339054
iteration : 13033
train acc:  0.734375
train loss:  0.48236268758773804
train gradient:  0.12274495257645009
iteration : 13034
train acc:  0.7109375
train loss:  0.5036802291870117
train gradient:  0.10387704196017382
iteration : 13035
train acc:  0.734375
train loss:  0.48323431611061096
train gradient:  0.11088799140741597
iteration : 13036
train acc:  0.8125
train loss:  0.41356784105300903
train gradient:  0.12048277167160232
iteration : 13037
train acc:  0.703125
train loss:  0.523689329624176
train gradient:  0.1616513561655331
iteration : 13038
train acc:  0.703125
train loss:  0.5619316101074219
train gradient:  0.13552824577379088
iteration : 13039
train acc:  0.765625
train loss:  0.4636070430278778
train gradient:  0.13967240864026104
iteration : 13040
train acc:  0.78125
train loss:  0.4230222702026367
train gradient:  0.08963210493596523
iteration : 13041
train acc:  0.7109375
train loss:  0.5735740661621094
train gradient:  0.18557069228227308
iteration : 13042
train acc:  0.7421875
train loss:  0.4591163396835327
train gradient:  0.09058765032187319
iteration : 13043
train acc:  0.7578125
train loss:  0.48266494274139404
train gradient:  0.10245455779030262
iteration : 13044
train acc:  0.765625
train loss:  0.45967617630958557
train gradient:  0.11808918321018085
iteration : 13045
train acc:  0.703125
train loss:  0.49761515855789185
train gradient:  0.15681123936694885
iteration : 13046
train acc:  0.640625
train loss:  0.571411669254303
train gradient:  0.19230627073037113
iteration : 13047
train acc:  0.7109375
train loss:  0.5212082266807556
train gradient:  0.14619487542972825
iteration : 13048
train acc:  0.7734375
train loss:  0.481158584356308
train gradient:  0.10027735224800775
iteration : 13049
train acc:  0.6796875
train loss:  0.536910355091095
train gradient:  0.1497544222228214
iteration : 13050
train acc:  0.71875
train loss:  0.5788537859916687
train gradient:  0.1616245844706788
iteration : 13051
train acc:  0.734375
train loss:  0.4694597125053406
train gradient:  0.09293176379484662
iteration : 13052
train acc:  0.765625
train loss:  0.49167850613594055
train gradient:  0.1293065172190913
iteration : 13053
train acc:  0.7890625
train loss:  0.5019435882568359
train gradient:  0.12213675530867901
iteration : 13054
train acc:  0.7734375
train loss:  0.4498831033706665
train gradient:  0.12329114136530539
iteration : 13055
train acc:  0.7734375
train loss:  0.4635779857635498
train gradient:  0.12581000251593064
iteration : 13056
train acc:  0.7265625
train loss:  0.5474220514297485
train gradient:  0.1511408644253328
iteration : 13057
train acc:  0.71875
train loss:  0.5008357763290405
train gradient:  0.11142889700680365
iteration : 13058
train acc:  0.8125
train loss:  0.4349505603313446
train gradient:  0.09223921705249193
iteration : 13059
train acc:  0.765625
train loss:  0.4946281313896179
train gradient:  0.11876573420696473
iteration : 13060
train acc:  0.7890625
train loss:  0.4533589482307434
train gradient:  0.11708201659044899
iteration : 13061
train acc:  0.8125
train loss:  0.41540759801864624
train gradient:  0.10661293552916747
iteration : 13062
train acc:  0.75
train loss:  0.4695603549480438
train gradient:  0.10630696309835751
iteration : 13063
train acc:  0.796875
train loss:  0.44202473759651184
train gradient:  0.10845499248957417
iteration : 13064
train acc:  0.6953125
train loss:  0.5309869647026062
train gradient:  0.16049094228900712
iteration : 13065
train acc:  0.6640625
train loss:  0.5186262726783752
train gradient:  0.13320254581441332
iteration : 13066
train acc:  0.71875
train loss:  0.48402026295661926
train gradient:  0.11655637893302188
iteration : 13067
train acc:  0.6953125
train loss:  0.584953248500824
train gradient:  0.1896792187400114
iteration : 13068
train acc:  0.7265625
train loss:  0.5115049481391907
train gradient:  0.1239207032300142
iteration : 13069
train acc:  0.75
train loss:  0.5443837642669678
train gradient:  0.15444995140662543
iteration : 13070
train acc:  0.765625
train loss:  0.43608027696609497
train gradient:  0.10630264038871434
iteration : 13071
train acc:  0.7421875
train loss:  0.49012207984924316
train gradient:  0.10872224319373175
iteration : 13072
train acc:  0.75
train loss:  0.4944498538970947
train gradient:  0.12368680634570323
iteration : 13073
train acc:  0.703125
train loss:  0.497584730386734
train gradient:  0.10607819520667348
iteration : 13074
train acc:  0.7421875
train loss:  0.5063288807868958
train gradient:  0.11320180210475451
iteration : 13075
train acc:  0.7578125
train loss:  0.5183509588241577
train gradient:  0.12751526660423484
iteration : 13076
train acc:  0.71875
train loss:  0.5057147145271301
train gradient:  0.13704882458396195
iteration : 13077
train acc:  0.7421875
train loss:  0.4957929253578186
train gradient:  0.11643422335580855
iteration : 13078
train acc:  0.78125
train loss:  0.4692569375038147
train gradient:  0.09413956718835981
iteration : 13079
train acc:  0.765625
train loss:  0.4307628870010376
train gradient:  0.09449321184292536
iteration : 13080
train acc:  0.765625
train loss:  0.4722633361816406
train gradient:  0.09463480496956163
iteration : 13081
train acc:  0.71875
train loss:  0.4575769305229187
train gradient:  0.10450958064961445
iteration : 13082
train acc:  0.7109375
train loss:  0.47848841547966003
train gradient:  0.0975794040655813
iteration : 13083
train acc:  0.765625
train loss:  0.5184293985366821
train gradient:  0.10389941148598496
iteration : 13084
train acc:  0.7578125
train loss:  0.5192616581916809
train gradient:  0.11046287328339076
iteration : 13085
train acc:  0.7578125
train loss:  0.4847565293312073
train gradient:  0.16830489044342264
iteration : 13086
train acc:  0.765625
train loss:  0.44661664962768555
train gradient:  0.08351857273209246
iteration : 13087
train acc:  0.7265625
train loss:  0.5605069994926453
train gradient:  0.15015013400283986
iteration : 13088
train acc:  0.7421875
train loss:  0.4976387917995453
train gradient:  0.12408992659114072
iteration : 13089
train acc:  0.71875
train loss:  0.48253101110458374
train gradient:  0.10637836428630541
iteration : 13090
train acc:  0.8125
train loss:  0.4107021689414978
train gradient:  0.06770100044467445
iteration : 13091
train acc:  0.71875
train loss:  0.4746268391609192
train gradient:  0.10463147554296058
iteration : 13092
train acc:  0.78125
train loss:  0.4464528560638428
train gradient:  0.087531925802715
iteration : 13093
train acc:  0.796875
train loss:  0.4203726351261139
train gradient:  0.08268348229722078
iteration : 13094
train acc:  0.6953125
train loss:  0.5912806987762451
train gradient:  0.1782499444941251
iteration : 13095
train acc:  0.6796875
train loss:  0.5348482728004456
train gradient:  0.1489056452505525
iteration : 13096
train acc:  0.765625
train loss:  0.48111605644226074
train gradient:  0.11677681543364431
iteration : 13097
train acc:  0.6875
train loss:  0.5453933477401733
train gradient:  0.13771218330829588
iteration : 13098
train acc:  0.7734375
train loss:  0.42880064249038696
train gradient:  0.09553855730821424
iteration : 13099
train acc:  0.7734375
train loss:  0.4872835576534271
train gradient:  0.14870661000920726
iteration : 13100
train acc:  0.796875
train loss:  0.4487308859825134
train gradient:  0.12513474465890168
iteration : 13101
train acc:  0.734375
train loss:  0.4755043387413025
train gradient:  0.129388529390854
iteration : 13102
train acc:  0.796875
train loss:  0.4369862973690033
train gradient:  0.10045694265103963
iteration : 13103
train acc:  0.7265625
train loss:  0.49387186765670776
train gradient:  0.13058414407933835
iteration : 13104
train acc:  0.7890625
train loss:  0.4533509612083435
train gradient:  0.09552670388166468
iteration : 13105
train acc:  0.7265625
train loss:  0.49350985884666443
train gradient:  0.13996359705869382
iteration : 13106
train acc:  0.6875
train loss:  0.5575007200241089
train gradient:  0.12238825023227885
iteration : 13107
train acc:  0.8046875
train loss:  0.42283204197883606
train gradient:  0.08374801903119697
iteration : 13108
train acc:  0.7734375
train loss:  0.46198999881744385
train gradient:  0.1011284347750673
iteration : 13109
train acc:  0.75
train loss:  0.46274077892303467
train gradient:  0.10647942437278161
iteration : 13110
train acc:  0.7890625
train loss:  0.49706101417541504
train gradient:  0.1252541773703087
iteration : 13111
train acc:  0.6953125
train loss:  0.5309267640113831
train gradient:  0.1325112373891802
iteration : 13112
train acc:  0.734375
train loss:  0.5054182410240173
train gradient:  0.12757259837096174
iteration : 13113
train acc:  0.7109375
train loss:  0.50174480676651
train gradient:  0.12360705309613511
iteration : 13114
train acc:  0.6875
train loss:  0.4943310022354126
train gradient:  0.10858585667720745
iteration : 13115
train acc:  0.75
train loss:  0.47594985365867615
train gradient:  0.12252619310538346
iteration : 13116
train acc:  0.6875
train loss:  0.5445177555084229
train gradient:  0.1942831364084834
iteration : 13117
train acc:  0.71875
train loss:  0.49283507466316223
train gradient:  0.12557162189436719
iteration : 13118
train acc:  0.71875
train loss:  0.5608975291252136
train gradient:  0.14519202195229958
iteration : 13119
train acc:  0.75
train loss:  0.4607272744178772
train gradient:  0.08026211816314756
iteration : 13120
train acc:  0.7578125
train loss:  0.46489906311035156
train gradient:  0.11938042978780654
iteration : 13121
train acc:  0.7734375
train loss:  0.41941511631011963
train gradient:  0.0911573553634844
iteration : 13122
train acc:  0.6953125
train loss:  0.5252147912979126
train gradient:  0.1279818769154984
iteration : 13123
train acc:  0.6953125
train loss:  0.4943836033344269
train gradient:  0.14986057683334303
iteration : 13124
train acc:  0.75
train loss:  0.493855744600296
train gradient:  0.10934668065674687
iteration : 13125
train acc:  0.6953125
train loss:  0.5410556793212891
train gradient:  0.126709934179765
iteration : 13126
train acc:  0.734375
train loss:  0.5224613547325134
train gradient:  0.1336057981820732
iteration : 13127
train acc:  0.7578125
train loss:  0.5290954113006592
train gradient:  0.1555504796889353
iteration : 13128
train acc:  0.703125
train loss:  0.5331882834434509
train gradient:  0.11264634753434705
iteration : 13129
train acc:  0.7421875
train loss:  0.61570143699646
train gradient:  0.13880677676592013
iteration : 13130
train acc:  0.71875
train loss:  0.5082241296768188
train gradient:  0.11428372193100875
iteration : 13131
train acc:  0.734375
train loss:  0.5023877620697021
train gradient:  0.1296838553713449
iteration : 13132
train acc:  0.7265625
train loss:  0.42547810077667236
train gradient:  0.09405596644037933
iteration : 13133
train acc:  0.8125
train loss:  0.4694518446922302
train gradient:  0.11805749667999337
iteration : 13134
train acc:  0.671875
train loss:  0.5617945790290833
train gradient:  0.13146478641342324
iteration : 13135
train acc:  0.734375
train loss:  0.4703953266143799
train gradient:  0.09613054826851034
iteration : 13136
train acc:  0.7890625
train loss:  0.47478052973747253
train gradient:  0.0937581766914624
iteration : 13137
train acc:  0.75
train loss:  0.4832391142845154
train gradient:  0.15320760841106568
iteration : 13138
train acc:  0.8203125
train loss:  0.4392800033092499
train gradient:  0.12529774962457435
iteration : 13139
train acc:  0.7421875
train loss:  0.5111080408096313
train gradient:  0.133246002512776
iteration : 13140
train acc:  0.8046875
train loss:  0.46157315373420715
train gradient:  0.1414044398254105
iteration : 13141
train acc:  0.7421875
train loss:  0.4584592878818512
train gradient:  0.08993122248813346
iteration : 13142
train acc:  0.7734375
train loss:  0.45258912444114685
train gradient:  0.10123162587846167
iteration : 13143
train acc:  0.796875
train loss:  0.41796818375587463
train gradient:  0.08181001975857796
iteration : 13144
train acc:  0.7734375
train loss:  0.4792347252368927
train gradient:  0.11822725069994056
iteration : 13145
train acc:  0.7734375
train loss:  0.4382436275482178
train gradient:  0.10561977702235424
iteration : 13146
train acc:  0.7734375
train loss:  0.4572766423225403
train gradient:  0.11351834227572416
iteration : 13147
train acc:  0.734375
train loss:  0.49477481842041016
train gradient:  0.12191315150805919
iteration : 13148
train acc:  0.7890625
train loss:  0.4734266698360443
train gradient:  0.16464657172374625
iteration : 13149
train acc:  0.765625
train loss:  0.5129733681678772
train gradient:  0.14739129006771978
iteration : 13150
train acc:  0.7734375
train loss:  0.48398125171661377
train gradient:  0.12711578793227143
iteration : 13151
train acc:  0.7578125
train loss:  0.5091297626495361
train gradient:  0.14319722411611419
iteration : 13152
train acc:  0.7890625
train loss:  0.5124558210372925
train gradient:  0.13768978764487405
iteration : 13153
train acc:  0.75
train loss:  0.47443830966949463
train gradient:  0.14365706218310548
iteration : 13154
train acc:  0.734375
train loss:  0.5283409357070923
train gradient:  0.1418877263905735
iteration : 13155
train acc:  0.7578125
train loss:  0.4587072730064392
train gradient:  0.11115466055751765
iteration : 13156
train acc:  0.7578125
train loss:  0.48697081208229065
train gradient:  0.1254449154837437
iteration : 13157
train acc:  0.765625
train loss:  0.4781166613101959
train gradient:  0.11174532450552548
iteration : 13158
train acc:  0.78125
train loss:  0.406181275844574
train gradient:  0.06652352306337674
iteration : 13159
train acc:  0.703125
train loss:  0.5016542673110962
train gradient:  0.11357597951314641
iteration : 13160
train acc:  0.75
train loss:  0.47305068373680115
train gradient:  0.08968532565865169
iteration : 13161
train acc:  0.7578125
train loss:  0.48174381256103516
train gradient:  0.124710293760459
iteration : 13162
train acc:  0.796875
train loss:  0.460358589887619
train gradient:  0.10908968541940248
iteration : 13163
train acc:  0.734375
train loss:  0.47868233919143677
train gradient:  0.11865431008892492
iteration : 13164
train acc:  0.765625
train loss:  0.5025555491447449
train gradient:  0.150504262865662
iteration : 13165
train acc:  0.71875
train loss:  0.5227880477905273
train gradient:  0.1630163414673518
iteration : 13166
train acc:  0.7890625
train loss:  0.49207445979118347
train gradient:  0.12563056482479096
iteration : 13167
train acc:  0.71875
train loss:  0.522082507610321
train gradient:  0.15599267889773893
iteration : 13168
train acc:  0.7734375
train loss:  0.47260022163391113
train gradient:  0.10553913156174807
iteration : 13169
train acc:  0.734375
train loss:  0.4873611330986023
train gradient:  0.12028102693008577
iteration : 13170
train acc:  0.7109375
train loss:  0.5412629246711731
train gradient:  0.1508823300652789
iteration : 13171
train acc:  0.6875
train loss:  0.5260219573974609
train gradient:  0.12598709312466758
iteration : 13172
train acc:  0.7734375
train loss:  0.4677466154098511
train gradient:  0.13005920446407476
iteration : 13173
train acc:  0.78125
train loss:  0.42205604910850525
train gradient:  0.08701738475049815
iteration : 13174
train acc:  0.734375
train loss:  0.49043339490890503
train gradient:  0.12607530062969546
iteration : 13175
train acc:  0.7578125
train loss:  0.45484015345573425
train gradient:  0.0958549563863821
iteration : 13176
train acc:  0.6796875
train loss:  0.6325958967208862
train gradient:  0.1767809412866519
iteration : 13177
train acc:  0.765625
train loss:  0.4730358421802521
train gradient:  0.11988591132469772
iteration : 13178
train acc:  0.75
train loss:  0.47033509612083435
train gradient:  0.11698145740422657
iteration : 13179
train acc:  0.7578125
train loss:  0.45918962359428406
train gradient:  0.12707866377742516
iteration : 13180
train acc:  0.7109375
train loss:  0.5171723961830139
train gradient:  0.11804960392144898
iteration : 13181
train acc:  0.7265625
train loss:  0.525599479675293
train gradient:  0.11291135370636676
iteration : 13182
train acc:  0.6953125
train loss:  0.520979642868042
train gradient:  0.1307289089434901
iteration : 13183
train acc:  0.734375
train loss:  0.47219303250312805
train gradient:  0.11801282041863138
iteration : 13184
train acc:  0.796875
train loss:  0.4284735918045044
train gradient:  0.09828424119733982
iteration : 13185
train acc:  0.71875
train loss:  0.5289515852928162
train gradient:  0.10696239174343303
iteration : 13186
train acc:  0.703125
train loss:  0.5337358713150024
train gradient:  0.12063622736202505
iteration : 13187
train acc:  0.703125
train loss:  0.5359878540039062
train gradient:  0.11286809115947133
iteration : 13188
train acc:  0.734375
train loss:  0.5151506662368774
train gradient:  0.11534453555858143
iteration : 13189
train acc:  0.828125
train loss:  0.41445493698120117
train gradient:  0.1163991992848546
iteration : 13190
train acc:  0.78125
train loss:  0.4194082021713257
train gradient:  0.08707258012589454
iteration : 13191
train acc:  0.734375
train loss:  0.5206681489944458
train gradient:  0.11060584669686423
iteration : 13192
train acc:  0.75
train loss:  0.47668641805648804
train gradient:  0.09591274128704842
iteration : 13193
train acc:  0.7578125
train loss:  0.476049542427063
train gradient:  0.12392776018828107
iteration : 13194
train acc:  0.7265625
train loss:  0.5529104471206665
train gradient:  0.17749369999999837
iteration : 13195
train acc:  0.828125
train loss:  0.4029061198234558
train gradient:  0.10357748187874946
iteration : 13196
train acc:  0.734375
train loss:  0.5047991275787354
train gradient:  0.12837752650361883
iteration : 13197
train acc:  0.7890625
train loss:  0.4958357810974121
train gradient:  0.151398692954195
iteration : 13198
train acc:  0.8203125
train loss:  0.45113953948020935
train gradient:  0.1180195786708003
iteration : 13199
train acc:  0.75
train loss:  0.5145962834358215
train gradient:  0.16430792077738132
iteration : 13200
train acc:  0.71875
train loss:  0.4967825710773468
train gradient:  0.11431584153768981
iteration : 13201
train acc:  0.7734375
train loss:  0.4558714032173157
train gradient:  0.10380106718013567
iteration : 13202
train acc:  0.7734375
train loss:  0.4259040355682373
train gradient:  0.08843399333857449
iteration : 13203
train acc:  0.7578125
train loss:  0.470031201839447
train gradient:  0.12291588813752945
iteration : 13204
train acc:  0.7734375
train loss:  0.4498394727706909
train gradient:  0.11158046731774372
iteration : 13205
train acc:  0.8359375
train loss:  0.4538753628730774
train gradient:  0.10562057293709178
iteration : 13206
train acc:  0.828125
train loss:  0.38483601808547974
train gradient:  0.08803108839019902
iteration : 13207
train acc:  0.6953125
train loss:  0.5177439451217651
train gradient:  0.14537306564276875
iteration : 13208
train acc:  0.71875
train loss:  0.5042829513549805
train gradient:  0.1180684768836791
iteration : 13209
train acc:  0.7890625
train loss:  0.4792274236679077
train gradient:  0.1316538324424483
iteration : 13210
train acc:  0.78125
train loss:  0.4516896903514862
train gradient:  0.09272921533257458
iteration : 13211
train acc:  0.6796875
train loss:  0.5342401266098022
train gradient:  0.1446287801724716
iteration : 13212
train acc:  0.78125
train loss:  0.4368976950645447
train gradient:  0.08232877623324233
iteration : 13213
train acc:  0.671875
train loss:  0.5974258184432983
train gradient:  0.1692022134909218
iteration : 13214
train acc:  0.8359375
train loss:  0.41755080223083496
train gradient:  0.10782182305591843
iteration : 13215
train acc:  0.6953125
train loss:  0.5268085598945618
train gradient:  0.14583503508388868
iteration : 13216
train acc:  0.8203125
train loss:  0.38252976536750793
train gradient:  0.07760821283303886
iteration : 13217
train acc:  0.7109375
train loss:  0.5614559650421143
train gradient:  0.1556343010694074
iteration : 13218
train acc:  0.7734375
train loss:  0.46393078565597534
train gradient:  0.12921871940088836
iteration : 13219
train acc:  0.7265625
train loss:  0.5249243378639221
train gradient:  0.13982820356820386
iteration : 13220
train acc:  0.7734375
train loss:  0.4608290195465088
train gradient:  0.09355185960064609
iteration : 13221
train acc:  0.734375
train loss:  0.468733549118042
train gradient:  0.12542605465608847
iteration : 13222
train acc:  0.765625
train loss:  0.4815370738506317
train gradient:  0.1281812855146334
iteration : 13223
train acc:  0.71875
train loss:  0.506018877029419
train gradient:  0.11541310599955687
iteration : 13224
train acc:  0.78125
train loss:  0.5066462159156799
train gradient:  0.13260441029569522
iteration : 13225
train acc:  0.8046875
train loss:  0.44606631994247437
train gradient:  0.09769869734207247
iteration : 13226
train acc:  0.7734375
train loss:  0.45133453607559204
train gradient:  0.11748065850728731
iteration : 13227
train acc:  0.71875
train loss:  0.4638160467147827
train gradient:  0.145543292342089
iteration : 13228
train acc:  0.7890625
train loss:  0.43321993947029114
train gradient:  0.12980910001610593
iteration : 13229
train acc:  0.75
train loss:  0.4732574224472046
train gradient:  0.11747816809797053
iteration : 13230
train acc:  0.7734375
train loss:  0.47918811440467834
train gradient:  0.14001693146941194
iteration : 13231
train acc:  0.7265625
train loss:  0.4916125535964966
train gradient:  0.12887545226605807
iteration : 13232
train acc:  0.71875
train loss:  0.4876415729522705
train gradient:  0.1185609367876889
iteration : 13233
train acc:  0.8203125
train loss:  0.39457857608795166
train gradient:  0.0863008371647447
iteration : 13234
train acc:  0.734375
train loss:  0.4531046748161316
train gradient:  0.08858119011529955
iteration : 13235
train acc:  0.7578125
train loss:  0.48969757556915283
train gradient:  0.1279360936268931
iteration : 13236
train acc:  0.734375
train loss:  0.4957715570926666
train gradient:  0.11467976079795979
iteration : 13237
train acc:  0.78125
train loss:  0.4922011196613312
train gradient:  0.11141502953418018
iteration : 13238
train acc:  0.71875
train loss:  0.4756143093109131
train gradient:  0.1210497737125566
iteration : 13239
train acc:  0.7265625
train loss:  0.4749610424041748
train gradient:  0.09775070058028312
iteration : 13240
train acc:  0.7421875
train loss:  0.49266761541366577
train gradient:  0.13028265885684565
iteration : 13241
train acc:  0.7109375
train loss:  0.5493093729019165
train gradient:  0.14425234908693596
iteration : 13242
train acc:  0.796875
train loss:  0.4511539936065674
train gradient:  0.09534592764280958
iteration : 13243
train acc:  0.78125
train loss:  0.4469255805015564
train gradient:  0.11658710277838037
iteration : 13244
train acc:  0.7734375
train loss:  0.46393436193466187
train gradient:  0.15072564978338163
iteration : 13245
train acc:  0.75
train loss:  0.4615512788295746
train gradient:  0.1074542231460976
iteration : 13246
train acc:  0.703125
train loss:  0.5436068177223206
train gradient:  0.16762197382398691
iteration : 13247
train acc:  0.75
train loss:  0.4535989761352539
train gradient:  0.1222130052631947
iteration : 13248
train acc:  0.65625
train loss:  0.5409642457962036
train gradient:  0.1672701330618402
iteration : 13249
train acc:  0.6953125
train loss:  0.4833027720451355
train gradient:  0.12336610960054704
iteration : 13250
train acc:  0.703125
train loss:  0.5500435829162598
train gradient:  0.13621144869218485
iteration : 13251
train acc:  0.765625
train loss:  0.49063628911972046
train gradient:  0.14942317437379618
iteration : 13252
train acc:  0.765625
train loss:  0.46773433685302734
train gradient:  0.10690573161245613
iteration : 13253
train acc:  0.75
train loss:  0.4697858691215515
train gradient:  0.12484794982211223
iteration : 13254
train acc:  0.71875
train loss:  0.5035725831985474
train gradient:  0.11176722695428529
iteration : 13255
train acc:  0.7890625
train loss:  0.46402066946029663
train gradient:  0.12037684455095142
iteration : 13256
train acc:  0.71875
train loss:  0.5223297476768494
train gradient:  0.11766805234508299
iteration : 13257
train acc:  0.7578125
train loss:  0.47707492113113403
train gradient:  0.12027621160584043
iteration : 13258
train acc:  0.6796875
train loss:  0.5626038908958435
train gradient:  0.20341525732090943
iteration : 13259
train acc:  0.7578125
train loss:  0.5481884479522705
train gradient:  0.16189769831623824
iteration : 13260
train acc:  0.765625
train loss:  0.46503809094429016
train gradient:  0.09914779332046797
iteration : 13261
train acc:  0.765625
train loss:  0.459975004196167
train gradient:  0.12282784367708025
iteration : 13262
train acc:  0.7578125
train loss:  0.49376440048217773
train gradient:  0.14585364741009968
iteration : 13263
train acc:  0.7421875
train loss:  0.5279614925384521
train gradient:  0.11272174627540599
iteration : 13264
train acc:  0.75
train loss:  0.46131157875061035
train gradient:  0.11159123616441421
iteration : 13265
train acc:  0.75
train loss:  0.5220243334770203
train gradient:  0.12648739135207435
iteration : 13266
train acc:  0.765625
train loss:  0.46304625272750854
train gradient:  0.1276734464659946
iteration : 13267
train acc:  0.7265625
train loss:  0.4754694998264313
train gradient:  0.10575557644859956
iteration : 13268
train acc:  0.7734375
train loss:  0.5019998550415039
train gradient:  0.14703522840434124
iteration : 13269
train acc:  0.765625
train loss:  0.46110180020332336
train gradient:  0.11923471001396932
iteration : 13270
train acc:  0.7578125
train loss:  0.4725479781627655
train gradient:  0.11081428875344242
iteration : 13271
train acc:  0.7109375
train loss:  0.5440541505813599
train gradient:  0.13573157587563434
iteration : 13272
train acc:  0.734375
train loss:  0.5090923309326172
train gradient:  0.10586847376956983
iteration : 13273
train acc:  0.7578125
train loss:  0.540528416633606
train gradient:  0.1433287850461846
iteration : 13274
train acc:  0.7734375
train loss:  0.4839766025543213
train gradient:  0.12246762135948061
iteration : 13275
train acc:  0.7265625
train loss:  0.5277570486068726
train gradient:  0.12330220467224678
iteration : 13276
train acc:  0.734375
train loss:  0.48043107986450195
train gradient:  0.11592895463073191
iteration : 13277
train acc:  0.703125
train loss:  0.5154780149459839
train gradient:  0.15320538496610078
iteration : 13278
train acc:  0.7265625
train loss:  0.4823446273803711
train gradient:  0.15031577213722475
iteration : 13279
train acc:  0.7265625
train loss:  0.5247762203216553
train gradient:  0.16798473958710797
iteration : 13280
train acc:  0.6875
train loss:  0.514067530632019
train gradient:  0.15120198444303756
iteration : 13281
train acc:  0.75
train loss:  0.49932318925857544
train gradient:  0.17201475213558842
iteration : 13282
train acc:  0.796875
train loss:  0.4370138347148895
train gradient:  0.11605151185205366
iteration : 13283
train acc:  0.8125
train loss:  0.440946102142334
train gradient:  0.10892302722769603
iteration : 13284
train acc:  0.7890625
train loss:  0.4054774343967438
train gradient:  0.08120002969676278
iteration : 13285
train acc:  0.7578125
train loss:  0.465718150138855
train gradient:  0.11861117492317702
iteration : 13286
train acc:  0.7421875
train loss:  0.5247203707695007
train gradient:  0.15847578197466639
iteration : 13287
train acc:  0.7578125
train loss:  0.5024949908256531
train gradient:  0.12259421654534218
iteration : 13288
train acc:  0.7109375
train loss:  0.5580211281776428
train gradient:  0.17869196620366914
iteration : 13289
train acc:  0.734375
train loss:  0.4882090091705322
train gradient:  0.12121806486454748
iteration : 13290
train acc:  0.765625
train loss:  0.46891266107559204
train gradient:  0.12703865455748672
iteration : 13291
train acc:  0.7578125
train loss:  0.49596869945526123
train gradient:  0.16001597241470014
iteration : 13292
train acc:  0.7578125
train loss:  0.5332016944885254
train gradient:  0.12319831730798333
iteration : 13293
train acc:  0.71875
train loss:  0.5326257944107056
train gradient:  0.1211101046061755
iteration : 13294
train acc:  0.75
train loss:  0.5011324882507324
train gradient:  0.12836439136329572
iteration : 13295
train acc:  0.734375
train loss:  0.46782901883125305
train gradient:  0.11491577476037224
iteration : 13296
train acc:  0.7109375
train loss:  0.5199143886566162
train gradient:  0.11846625173125797
iteration : 13297
train acc:  0.734375
train loss:  0.48458680510520935
train gradient:  0.1336844911800527
iteration : 13298
train acc:  0.7578125
train loss:  0.46755415201187134
train gradient:  0.10589249382278945
iteration : 13299
train acc:  0.75
train loss:  0.5098323225975037
train gradient:  0.12486110339631544
iteration : 13300
train acc:  0.796875
train loss:  0.43224501609802246
train gradient:  0.08482420795212117
iteration : 13301
train acc:  0.8359375
train loss:  0.41378265619277954
train gradient:  0.09231271049695934
iteration : 13302
train acc:  0.7265625
train loss:  0.5572052597999573
train gradient:  0.14320522398447466
iteration : 13303
train acc:  0.75
train loss:  0.4576353430747986
train gradient:  0.11238844508725644
iteration : 13304
train acc:  0.78125
train loss:  0.44329679012298584
train gradient:  0.09845807433309525
iteration : 13305
train acc:  0.7734375
train loss:  0.5588563084602356
train gradient:  0.1386049043126027
iteration : 13306
train acc:  0.6953125
train loss:  0.48683714866638184
train gradient:  0.09886459176318133
iteration : 13307
train acc:  0.703125
train loss:  0.5113354325294495
train gradient:  0.14961197547507227
iteration : 13308
train acc:  0.75
train loss:  0.48120537400245667
train gradient:  0.11336786210518249
iteration : 13309
train acc:  0.734375
train loss:  0.5100343227386475
train gradient:  0.12334527064516598
iteration : 13310
train acc:  0.7265625
train loss:  0.4759896397590637
train gradient:  0.09221824322569606
iteration : 13311
train acc:  0.65625
train loss:  0.557193398475647
train gradient:  0.12871130725792496
iteration : 13312
train acc:  0.7734375
train loss:  0.4534479081630707
train gradient:  0.11824962279281113
iteration : 13313
train acc:  0.71875
train loss:  0.5018807649612427
train gradient:  0.138275326196578
iteration : 13314
train acc:  0.703125
train loss:  0.5714603662490845
train gradient:  0.142488265228909
iteration : 13315
train acc:  0.7578125
train loss:  0.46730032563209534
train gradient:  0.1634701886067907
iteration : 13316
train acc:  0.8125
train loss:  0.4618920683860779
train gradient:  0.11707213621466135
iteration : 13317
train acc:  0.75
train loss:  0.47995734214782715
train gradient:  0.11219054886746964
iteration : 13318
train acc:  0.7578125
train loss:  0.4696032702922821
train gradient:  0.09898860085477398
iteration : 13319
train acc:  0.6953125
train loss:  0.5499712228775024
train gradient:  0.13193114472692297
iteration : 13320
train acc:  0.7734375
train loss:  0.4873199164867401
train gradient:  0.12196491471164436
iteration : 13321
train acc:  0.8046875
train loss:  0.4214221239089966
train gradient:  0.09538088976326306
iteration : 13322
train acc:  0.7265625
train loss:  0.5009849071502686
train gradient:  0.12248458199216826
iteration : 13323
train acc:  0.7421875
train loss:  0.5258569717407227
train gradient:  0.15923207046931664
iteration : 13324
train acc:  0.7421875
train loss:  0.5136027336120605
train gradient:  0.12465967314148958
iteration : 13325
train acc:  0.765625
train loss:  0.4888073205947876
train gradient:  0.12052630857756796
iteration : 13326
train acc:  0.7421875
train loss:  0.5262609124183655
train gradient:  0.14237456079534946
iteration : 13327
train acc:  0.734375
train loss:  0.4885752201080322
train gradient:  0.1070772431279717
iteration : 13328
train acc:  0.7109375
train loss:  0.5165928602218628
train gradient:  0.13226311363339233
iteration : 13329
train acc:  0.796875
train loss:  0.48507171869277954
train gradient:  0.13322967287454407
iteration : 13330
train acc:  0.796875
train loss:  0.43207597732543945
train gradient:  0.09282004270125013
iteration : 13331
train acc:  0.71875
train loss:  0.5302883386611938
train gradient:  0.12110634957605833
iteration : 13332
train acc:  0.7734375
train loss:  0.42237794399261475
train gradient:  0.09294256202278554
iteration : 13333
train acc:  0.7578125
train loss:  0.4490005373954773
train gradient:  0.10969717673641873
iteration : 13334
train acc:  0.8046875
train loss:  0.39532509446144104
train gradient:  0.07549047662643174
iteration : 13335
train acc:  0.6875
train loss:  0.5283176302909851
train gradient:  0.12111944390991855
iteration : 13336
train acc:  0.6484375
train loss:  0.6022868156433105
train gradient:  0.20187877479203692
iteration : 13337
train acc:  0.734375
train loss:  0.5039504766464233
train gradient:  0.12315532375904167
iteration : 13338
train acc:  0.7421875
train loss:  0.47902512550354004
train gradient:  0.13749206083494347
iteration : 13339
train acc:  0.78125
train loss:  0.41597768664360046
train gradient:  0.10398050164547717
iteration : 13340
train acc:  0.7578125
train loss:  0.4500904679298401
train gradient:  0.10349387352675932
iteration : 13341
train acc:  0.7734375
train loss:  0.45540860295295715
train gradient:  0.09083737221984524
iteration : 13342
train acc:  0.71875
train loss:  0.5196380615234375
train gradient:  0.14352157403404545
iteration : 13343
train acc:  0.796875
train loss:  0.4508621096611023
train gradient:  0.11839550220898747
iteration : 13344
train acc:  0.7734375
train loss:  0.5080703496932983
train gradient:  0.11253818182830026
iteration : 13345
train acc:  0.7578125
train loss:  0.490003764629364
train gradient:  0.11044027473987923
iteration : 13346
train acc:  0.7578125
train loss:  0.5352966785430908
train gradient:  0.1454938709988188
iteration : 13347
train acc:  0.703125
train loss:  0.59103924036026
train gradient:  0.1725369115128843
iteration : 13348
train acc:  0.75
train loss:  0.4385848641395569
train gradient:  0.08765256779689218
iteration : 13349
train acc:  0.7734375
train loss:  0.4511455297470093
train gradient:  0.09011499728595185
iteration : 13350
train acc:  0.734375
train loss:  0.44718748331069946
train gradient:  0.10639892544453107
iteration : 13351
train acc:  0.8203125
train loss:  0.4228438436985016
train gradient:  0.08929018877504968
iteration : 13352
train acc:  0.7578125
train loss:  0.5351094007492065
train gradient:  0.1619751002553973
iteration : 13353
train acc:  0.6640625
train loss:  0.6309887766838074
train gradient:  0.20045674438878153
iteration : 13354
train acc:  0.65625
train loss:  0.5596233606338501
train gradient:  0.16525715942569272
iteration : 13355
train acc:  0.75
train loss:  0.4398784637451172
train gradient:  0.09670547753944352
iteration : 13356
train acc:  0.7421875
train loss:  0.4968862235546112
train gradient:  0.10382871875273854
iteration : 13357
train acc:  0.765625
train loss:  0.4494841694831848
train gradient:  0.09561171104889585
iteration : 13358
train acc:  0.8046875
train loss:  0.3711763620376587
train gradient:  0.07882114662710597
iteration : 13359
train acc:  0.78125
train loss:  0.4678724408149719
train gradient:  0.11056911227332525
iteration : 13360
train acc:  0.7421875
train loss:  0.4853023290634155
train gradient:  0.1234689433167756
iteration : 13361
train acc:  0.7109375
train loss:  0.4820764660835266
train gradient:  0.11278390532672064
iteration : 13362
train acc:  0.78125
train loss:  0.4528008699417114
train gradient:  0.10093096971716982
iteration : 13363
train acc:  0.71875
train loss:  0.45954424142837524
train gradient:  0.13121089500738303
iteration : 13364
train acc:  0.7578125
train loss:  0.5616849064826965
train gradient:  0.12471814320243545
iteration : 13365
train acc:  0.7734375
train loss:  0.46365368366241455
train gradient:  0.11944443119078979
iteration : 13366
train acc:  0.7890625
train loss:  0.47065144777297974
train gradient:  0.11045362943639142
iteration : 13367
train acc:  0.7265625
train loss:  0.5272021889686584
train gradient:  0.11853170006721936
iteration : 13368
train acc:  0.765625
train loss:  0.4762319326400757
train gradient:  0.12006228097993082
iteration : 13369
train acc:  0.7578125
train loss:  0.4203945994377136
train gradient:  0.09466069366623034
iteration : 13370
train acc:  0.75
train loss:  0.4681105613708496
train gradient:  0.105076439783078
iteration : 13371
train acc:  0.703125
train loss:  0.4808253049850464
train gradient:  0.1332974093158683
iteration : 13372
train acc:  0.765625
train loss:  0.4599139392375946
train gradient:  0.09417682964506287
iteration : 13373
train acc:  0.71875
train loss:  0.5295135974884033
train gradient:  0.14672938088699944
iteration : 13374
train acc:  0.7734375
train loss:  0.4794703722000122
train gradient:  0.11633720037824011
iteration : 13375
train acc:  0.78125
train loss:  0.447737455368042
train gradient:  0.08634940476150463
iteration : 13376
train acc:  0.71875
train loss:  0.5367567539215088
train gradient:  0.1553632510807031
iteration : 13377
train acc:  0.7890625
train loss:  0.4642059803009033
train gradient:  0.1163912571298351
iteration : 13378
train acc:  0.7578125
train loss:  0.5171536207199097
train gradient:  0.14116835243204157
iteration : 13379
train acc:  0.7265625
train loss:  0.5116966962814331
train gradient:  0.12048614866022236
iteration : 13380
train acc:  0.71875
train loss:  0.5057920217514038
train gradient:  0.12599513606884716
iteration : 13381
train acc:  0.7578125
train loss:  0.47159868478775024
train gradient:  0.10477908373320832
iteration : 13382
train acc:  0.7265625
train loss:  0.4932408034801483
train gradient:  0.12061169987696242
iteration : 13383
train acc:  0.71875
train loss:  0.4899865984916687
train gradient:  0.1358952030791427
iteration : 13384
train acc:  0.7734375
train loss:  0.445468008518219
train gradient:  0.09549407979783228
iteration : 13385
train acc:  0.734375
train loss:  0.46819788217544556
train gradient:  0.11849526410032309
iteration : 13386
train acc:  0.6796875
train loss:  0.5348199605941772
train gradient:  0.13634637612702152
iteration : 13387
train acc:  0.7578125
train loss:  0.48239192366600037
train gradient:  0.09943232206825332
iteration : 13388
train acc:  0.7421875
train loss:  0.49382585287094116
train gradient:  0.12228218901192829
iteration : 13389
train acc:  0.84375
train loss:  0.40590324997901917
train gradient:  0.09081228123459036
iteration : 13390
train acc:  0.7578125
train loss:  0.4890601336956024
train gradient:  0.09670661856543604
iteration : 13391
train acc:  0.7421875
train loss:  0.4947319030761719
train gradient:  0.12482017016245696
iteration : 13392
train acc:  0.7421875
train loss:  0.4768713116645813
train gradient:  0.1148210286889526
iteration : 13393
train acc:  0.7578125
train loss:  0.5005191564559937
train gradient:  0.11489893925655814
iteration : 13394
train acc:  0.75
train loss:  0.45708203315734863
train gradient:  0.0981768505044645
iteration : 13395
train acc:  0.6953125
train loss:  0.5708888173103333
train gradient:  0.2033277612140912
iteration : 13396
train acc:  0.703125
train loss:  0.5378701686859131
train gradient:  0.1904657090651935
iteration : 13397
train acc:  0.71875
train loss:  0.4821747839450836
train gradient:  0.1367545627422136
iteration : 13398
train acc:  0.734375
train loss:  0.4542844593524933
train gradient:  0.10384294253738942
iteration : 13399
train acc:  0.703125
train loss:  0.5250341892242432
train gradient:  0.12968849933433152
iteration : 13400
train acc:  0.8046875
train loss:  0.41439560055732727
train gradient:  0.09177336301152005
iteration : 13401
train acc:  0.765625
train loss:  0.48649996519088745
train gradient:  0.09157573032615512
iteration : 13402
train acc:  0.7421875
train loss:  0.4748661518096924
train gradient:  0.11891781152417079
iteration : 13403
train acc:  0.703125
train loss:  0.5362839698791504
train gradient:  0.14534395276887607
iteration : 13404
train acc:  0.703125
train loss:  0.5168285369873047
train gradient:  0.12488949053046972
iteration : 13405
train acc:  0.71875
train loss:  0.49488675594329834
train gradient:  0.12303531918120116
iteration : 13406
train acc:  0.765625
train loss:  0.45371323823928833
train gradient:  0.10863702258544507
iteration : 13407
train acc:  0.703125
train loss:  0.5107825994491577
train gradient:  0.1247972325339809
iteration : 13408
train acc:  0.7109375
train loss:  0.4853203296661377
train gradient:  0.10279071811939279
iteration : 13409
train acc:  0.7578125
train loss:  0.4664219915866852
train gradient:  0.10895207024809823
iteration : 13410
train acc:  0.8125
train loss:  0.40983304381370544
train gradient:  0.10081164598295428
iteration : 13411
train acc:  0.734375
train loss:  0.4886487126350403
train gradient:  0.12268062075218068
iteration : 13412
train acc:  0.7421875
train loss:  0.44554436206817627
train gradient:  0.09586517845126519
iteration : 13413
train acc:  0.7265625
train loss:  0.5082240104675293
train gradient:  0.14739935428489187
iteration : 13414
train acc:  0.7734375
train loss:  0.4695882499217987
train gradient:  0.12392882292935826
iteration : 13415
train acc:  0.7421875
train loss:  0.5048843622207642
train gradient:  0.12591138493840354
iteration : 13416
train acc:  0.7421875
train loss:  0.5017884969711304
train gradient:  0.15533123835788126
iteration : 13417
train acc:  0.75
train loss:  0.46545809507369995
train gradient:  0.11252545225902806
iteration : 13418
train acc:  0.7265625
train loss:  0.5002121329307556
train gradient:  0.13630606587309607
iteration : 13419
train acc:  0.7578125
train loss:  0.4355009198188782
train gradient:  0.1244041503228694
iteration : 13420
train acc:  0.703125
train loss:  0.5538097620010376
train gradient:  0.13222329282939088
iteration : 13421
train acc:  0.734375
train loss:  0.5013384819030762
train gradient:  0.11970523629348792
iteration : 13422
train acc:  0.703125
train loss:  0.5228584408760071
train gradient:  0.14852409179875606
iteration : 13423
train acc:  0.8125
train loss:  0.42169371247291565
train gradient:  0.10520451148733942
iteration : 13424
train acc:  0.8203125
train loss:  0.4239806532859802
train gradient:  0.10605671427131856
iteration : 13425
train acc:  0.71875
train loss:  0.5169064998626709
train gradient:  0.15482809987886134
iteration : 13426
train acc:  0.7109375
train loss:  0.5508383512496948
train gradient:  0.1298299221389777
iteration : 13427
train acc:  0.7421875
train loss:  0.4715709984302521
train gradient:  0.11322163859091532
iteration : 13428
train acc:  0.75
train loss:  0.4857984781265259
train gradient:  0.11178897718489592
iteration : 13429
train acc:  0.7578125
train loss:  0.44130921363830566
train gradient:  0.09570130059772675
iteration : 13430
train acc:  0.765625
train loss:  0.45328205823898315
train gradient:  0.1093209831472874
iteration : 13431
train acc:  0.703125
train loss:  0.5099755525588989
train gradient:  0.13890993450396988
iteration : 13432
train acc:  0.7734375
train loss:  0.4937979578971863
train gradient:  0.1222352451383855
iteration : 13433
train acc:  0.609375
train loss:  0.6255967020988464
train gradient:  0.16910770754359766
iteration : 13434
train acc:  0.734375
train loss:  0.5105438232421875
train gradient:  0.13768644748742614
iteration : 13435
train acc:  0.7890625
train loss:  0.48562493920326233
train gradient:  0.13058424099442184
iteration : 13436
train acc:  0.8359375
train loss:  0.3706764280796051
train gradient:  0.07162652485726714
iteration : 13437
train acc:  0.765625
train loss:  0.46471938490867615
train gradient:  0.10154912260796083
iteration : 13438
train acc:  0.6875
train loss:  0.5690281987190247
train gradient:  0.1432900416811767
iteration : 13439
train acc:  0.734375
train loss:  0.4859168231487274
train gradient:  0.10567215674551743
iteration : 13440
train acc:  0.7421875
train loss:  0.5030766725540161
train gradient:  0.15885537741376016
iteration : 13441
train acc:  0.734375
train loss:  0.5210186839103699
train gradient:  0.14548382712051175
iteration : 13442
train acc:  0.78125
train loss:  0.43381866812705994
train gradient:  0.09912809653416747
iteration : 13443
train acc:  0.828125
train loss:  0.42297446727752686
train gradient:  0.11894114244147795
iteration : 13444
train acc:  0.8125
train loss:  0.39800959825515747
train gradient:  0.07147984423070904
iteration : 13445
train acc:  0.78125
train loss:  0.5119489431381226
train gradient:  0.12341962330483042
iteration : 13446
train acc:  0.7578125
train loss:  0.5433897376060486
train gradient:  0.16940909811532956
iteration : 13447
train acc:  0.859375
train loss:  0.3811177909374237
train gradient:  0.10640240066381675
iteration : 13448
train acc:  0.734375
train loss:  0.5667207837104797
train gradient:  0.13985704099812518
iteration : 13449
train acc:  0.6796875
train loss:  0.515571653842926
train gradient:  0.1358882091600713
iteration : 13450
train acc:  0.7421875
train loss:  0.5185877084732056
train gradient:  0.12352860523803914
iteration : 13451
train acc:  0.765625
train loss:  0.45292598009109497
train gradient:  0.12582688310853074
iteration : 13452
train acc:  0.7890625
train loss:  0.4357462525367737
train gradient:  0.0928342831198608
iteration : 13453
train acc:  0.7734375
train loss:  0.5150635242462158
train gradient:  0.14254303179887212
iteration : 13454
train acc:  0.7578125
train loss:  0.5144904851913452
train gradient:  0.13698910107845463
iteration : 13455
train acc:  0.7109375
train loss:  0.5631937384605408
train gradient:  0.15163494184983828
iteration : 13456
train acc:  0.7421875
train loss:  0.5012639760971069
train gradient:  0.1467607544220726
iteration : 13457
train acc:  0.7734375
train loss:  0.47353458404541016
train gradient:  0.1298823161031032
iteration : 13458
train acc:  0.7734375
train loss:  0.4757853150367737
train gradient:  0.10748812353930695
iteration : 13459
train acc:  0.734375
train loss:  0.4866865873336792
train gradient:  0.12903783530313334
iteration : 13460
train acc:  0.7734375
train loss:  0.45090997219085693
train gradient:  0.10680545731867168
iteration : 13461
train acc:  0.7734375
train loss:  0.42965200543403625
train gradient:  0.07197014318171974
iteration : 13462
train acc:  0.734375
train loss:  0.5306797027587891
train gradient:  0.14525842861121852
iteration : 13463
train acc:  0.828125
train loss:  0.4128575325012207
train gradient:  0.1078025960132739
iteration : 13464
train acc:  0.8203125
train loss:  0.3880946934223175
train gradient:  0.08127676985021115
iteration : 13465
train acc:  0.71875
train loss:  0.5543136596679688
train gradient:  0.140255087687858
iteration : 13466
train acc:  0.75
train loss:  0.47405946254730225
train gradient:  0.11806989993517311
iteration : 13467
train acc:  0.765625
train loss:  0.48430800437927246
train gradient:  0.10887424445395716
iteration : 13468
train acc:  0.78125
train loss:  0.4712131917476654
train gradient:  0.11437684665218611
iteration : 13469
train acc:  0.734375
train loss:  0.5029723644256592
train gradient:  0.1093396628732671
iteration : 13470
train acc:  0.78125
train loss:  0.45233631134033203
train gradient:  0.11609989960521634
iteration : 13471
train acc:  0.78125
train loss:  0.45087915658950806
train gradient:  0.09765069897106377
iteration : 13472
train acc:  0.765625
train loss:  0.48394280672073364
train gradient:  0.09546623897823094
iteration : 13473
train acc:  0.703125
train loss:  0.564297080039978
train gradient:  0.1456861942117471
iteration : 13474
train acc:  0.71875
train loss:  0.541539192199707
train gradient:  0.13035038841890045
iteration : 13475
train acc:  0.7890625
train loss:  0.45265862345695496
train gradient:  0.09020238676382143
iteration : 13476
train acc:  0.7890625
train loss:  0.47765976190567017
train gradient:  0.1146778809963877
iteration : 13477
train acc:  0.71875
train loss:  0.6030856370925903
train gradient:  0.15258333526208212
iteration : 13478
train acc:  0.796875
train loss:  0.44643938541412354
train gradient:  0.10746583169672626
iteration : 13479
train acc:  0.7578125
train loss:  0.4704432487487793
train gradient:  0.12292093484557186
iteration : 13480
train acc:  0.703125
train loss:  0.5146079659461975
train gradient:  0.12110246521844022
iteration : 13481
train acc:  0.7421875
train loss:  0.47878479957580566
train gradient:  0.11437298565801991
iteration : 13482
train acc:  0.71875
train loss:  0.5275071859359741
train gradient:  0.12129720477741288
iteration : 13483
train acc:  0.78125
train loss:  0.4765284061431885
train gradient:  0.11221168740499915
iteration : 13484
train acc:  0.78125
train loss:  0.4218030571937561
train gradient:  0.10660655822206262
iteration : 13485
train acc:  0.71875
train loss:  0.5491876006126404
train gradient:  0.1362555732528783
iteration : 13486
train acc:  0.75
train loss:  0.47401848435401917
train gradient:  0.11397061535002727
iteration : 13487
train acc:  0.8046875
train loss:  0.44022154808044434
train gradient:  0.09129266992618972
iteration : 13488
train acc:  0.7421875
train loss:  0.43099090456962585
train gradient:  0.1121477770683582
iteration : 13489
train acc:  0.703125
train loss:  0.5367072820663452
train gradient:  0.15530617075536415
iteration : 13490
train acc:  0.7890625
train loss:  0.4596802294254303
train gradient:  0.10124088105610017
iteration : 13491
train acc:  0.7578125
train loss:  0.41293102502822876
train gradient:  0.08022282610878667
iteration : 13492
train acc:  0.71875
train loss:  0.5037244558334351
train gradient:  0.13090358231267252
iteration : 13493
train acc:  0.6796875
train loss:  0.5283641815185547
train gradient:  0.14940301600521821
iteration : 13494
train acc:  0.7578125
train loss:  0.4782567024230957
train gradient:  0.11943700881080795
iteration : 13495
train acc:  0.7578125
train loss:  0.4801713228225708
train gradient:  0.09385773154666219
iteration : 13496
train acc:  0.7421875
train loss:  0.4583406448364258
train gradient:  0.10705407556129207
iteration : 13497
train acc:  0.796875
train loss:  0.4527437388896942
train gradient:  0.1101608675425868
iteration : 13498
train acc:  0.703125
train loss:  0.49045610427856445
train gradient:  0.13308301638304693
iteration : 13499
train acc:  0.8046875
train loss:  0.4529848098754883
train gradient:  0.10247709195286141
iteration : 13500
train acc:  0.734375
train loss:  0.5209080576896667
train gradient:  0.15296692209652926
iteration : 13501
train acc:  0.734375
train loss:  0.49396681785583496
train gradient:  0.14505802599330622
iteration : 13502
train acc:  0.75
train loss:  0.5164563655853271
train gradient:  0.13375337116653546
iteration : 13503
train acc:  0.78125
train loss:  0.44261616468429565
train gradient:  0.1010467685494994
iteration : 13504
train acc:  0.75
train loss:  0.5246110558509827
train gradient:  0.15675275604441125
iteration : 13505
train acc:  0.734375
train loss:  0.4399559199810028
train gradient:  0.0914136880928819
iteration : 13506
train acc:  0.75
train loss:  0.4649561643600464
train gradient:  0.09412850556243085
iteration : 13507
train acc:  0.75
train loss:  0.5099393129348755
train gradient:  0.18369700929303145
iteration : 13508
train acc:  0.8203125
train loss:  0.42035844922065735
train gradient:  0.10306153182618129
iteration : 13509
train acc:  0.765625
train loss:  0.4745490550994873
train gradient:  0.11065367167318192
iteration : 13510
train acc:  0.7890625
train loss:  0.4215036928653717
train gradient:  0.10085134646188557
iteration : 13511
train acc:  0.796875
train loss:  0.4874154329299927
train gradient:  0.10984754545426804
iteration : 13512
train acc:  0.6953125
train loss:  0.5230076909065247
train gradient:  0.1108453126716865
iteration : 13513
train acc:  0.6953125
train loss:  0.527032196521759
train gradient:  0.14597259112962602
iteration : 13514
train acc:  0.7421875
train loss:  0.4736277759075165
train gradient:  0.11313254550643595
iteration : 13515
train acc:  0.8046875
train loss:  0.42113855481147766
train gradient:  0.10412081862223979
iteration : 13516
train acc:  0.78125
train loss:  0.4430961608886719
train gradient:  0.08967873399490742
iteration : 13517
train acc:  0.7421875
train loss:  0.48587530851364136
train gradient:  0.11569335104839822
iteration : 13518
train acc:  0.734375
train loss:  0.4802287220954895
train gradient:  0.1158676111417555
iteration : 13519
train acc:  0.671875
train loss:  0.6081935167312622
train gradient:  0.16456480594878806
iteration : 13520
train acc:  0.78125
train loss:  0.5086601972579956
train gradient:  0.13411912582601807
iteration : 13521
train acc:  0.7421875
train loss:  0.5044407844543457
train gradient:  0.1321249190055309
iteration : 13522
train acc:  0.7890625
train loss:  0.4208991527557373
train gradient:  0.11414973575407127
iteration : 13523
train acc:  0.71875
train loss:  0.48784804344177246
train gradient:  0.13313231004985876
iteration : 13524
train acc:  0.671875
train loss:  0.552099347114563
train gradient:  0.17714250394133674
iteration : 13525
train acc:  0.703125
train loss:  0.5230107307434082
train gradient:  0.1582146362910902
iteration : 13526
train acc:  0.765625
train loss:  0.47030988335609436
train gradient:  0.14405877951530244
iteration : 13527
train acc:  0.8203125
train loss:  0.4145101308822632
train gradient:  0.09117828695661595
iteration : 13528
train acc:  0.7421875
train loss:  0.4790278375148773
train gradient:  0.10839191016705337
iteration : 13529
train acc:  0.75
train loss:  0.4588027596473694
train gradient:  0.11568895799812001
iteration : 13530
train acc:  0.7734375
train loss:  0.4741057753562927
train gradient:  0.12573662271883104
iteration : 13531
train acc:  0.7109375
train loss:  0.4897800087928772
train gradient:  0.13422396878430976
iteration : 13532
train acc:  0.7265625
train loss:  0.5079344511032104
train gradient:  0.12093697769116689
iteration : 13533
train acc:  0.7109375
train loss:  0.5200717449188232
train gradient:  0.13228626464858562
iteration : 13534
train acc:  0.734375
train loss:  0.47515201568603516
train gradient:  0.13125019715347352
iteration : 13535
train acc:  0.6796875
train loss:  0.5899999737739563
train gradient:  0.1831320409758168
iteration : 13536
train acc:  0.734375
train loss:  0.4859742224216461
train gradient:  0.13835792387653495
iteration : 13537
train acc:  0.703125
train loss:  0.5244306921958923
train gradient:  0.10439575029103527
iteration : 13538
train acc:  0.75
train loss:  0.47659510374069214
train gradient:  0.13184697384696492
iteration : 13539
train acc:  0.7734375
train loss:  0.4469972848892212
train gradient:  0.09892704833904797
iteration : 13540
train acc:  0.7578125
train loss:  0.49976006150245667
train gradient:  0.1285590904652119
iteration : 13541
train acc:  0.7265625
train loss:  0.4627135992050171
train gradient:  0.10622646942451305
iteration : 13542
train acc:  0.7421875
train loss:  0.4475596249103546
train gradient:  0.08927892986124168
iteration : 13543
train acc:  0.796875
train loss:  0.47469210624694824
train gradient:  0.13204515730746538
iteration : 13544
train acc:  0.71875
train loss:  0.452093243598938
train gradient:  0.12312126766709364
iteration : 13545
train acc:  0.7421875
train loss:  0.468885600566864
train gradient:  0.13147562952132058
iteration : 13546
train acc:  0.7265625
train loss:  0.48794394731521606
train gradient:  0.12623794587059578
iteration : 13547
train acc:  0.8203125
train loss:  0.39734336733818054
train gradient:  0.08055725105415368
iteration : 13548
train acc:  0.7734375
train loss:  0.5301052331924438
train gradient:  0.19130857181324046
iteration : 13549
train acc:  0.7734375
train loss:  0.46965909004211426
train gradient:  0.14917624862441542
iteration : 13550
train acc:  0.7734375
train loss:  0.5013258457183838
train gradient:  0.1331431954493678
iteration : 13551
train acc:  0.75
train loss:  0.4996269643306732
train gradient:  0.16931868452084325
iteration : 13552
train acc:  0.7265625
train loss:  0.5257431864738464
train gradient:  0.12806058126219466
iteration : 13553
train acc:  0.734375
train loss:  0.4553016722202301
train gradient:  0.12199542301583212
iteration : 13554
train acc:  0.7421875
train loss:  0.4978558421134949
train gradient:  0.13789621180517708
iteration : 13555
train acc:  0.7109375
train loss:  0.5236191153526306
train gradient:  0.11575431154710782
iteration : 13556
train acc:  0.796875
train loss:  0.4809309244155884
train gradient:  0.11416451760553287
iteration : 13557
train acc:  0.7734375
train loss:  0.4330782890319824
train gradient:  0.11437553838372348
iteration : 13558
train acc:  0.6953125
train loss:  0.4964647591114044
train gradient:  0.12565106863513514
iteration : 13559
train acc:  0.65625
train loss:  0.5530338287353516
train gradient:  0.16454092193784056
iteration : 13560
train acc:  0.71875
train loss:  0.45491188764572144
train gradient:  0.10532719551658419
iteration : 13561
train acc:  0.71875
train loss:  0.5424526929855347
train gradient:  0.1544040162223974
iteration : 13562
train acc:  0.7578125
train loss:  0.44764000177383423
train gradient:  0.11092180638426978
iteration : 13563
train acc:  0.6640625
train loss:  0.5353137850761414
train gradient:  0.1405062239113387
iteration : 13564
train acc:  0.7734375
train loss:  0.45476478338241577
train gradient:  0.10077458188991503
iteration : 13565
train acc:  0.625
train loss:  0.5833456516265869
train gradient:  0.15348931924567955
iteration : 13566
train acc:  0.7734375
train loss:  0.47601115703582764
train gradient:  0.10342247951049799
iteration : 13567
train acc:  0.75
train loss:  0.49449220299720764
train gradient:  0.10336397766180513
iteration : 13568
train acc:  0.7890625
train loss:  0.43087464570999146
train gradient:  0.11255626745624656
iteration : 13569
train acc:  0.796875
train loss:  0.421450138092041
train gradient:  0.12250705055119486
iteration : 13570
train acc:  0.765625
train loss:  0.47493815422058105
train gradient:  0.118086961262385
iteration : 13571
train acc:  0.7109375
train loss:  0.5751643180847168
train gradient:  0.14864789038254153
iteration : 13572
train acc:  0.7421875
train loss:  0.4487736225128174
train gradient:  0.1038860760415976
iteration : 13573
train acc:  0.7734375
train loss:  0.4854421317577362
train gradient:  0.15020843931554378
iteration : 13574
train acc:  0.7890625
train loss:  0.4357297718524933
train gradient:  0.13245398373512468
iteration : 13575
train acc:  0.765625
train loss:  0.44989150762557983
train gradient:  0.10659794221645154
iteration : 13576
train acc:  0.7734375
train loss:  0.4772224426269531
train gradient:  0.11727880688125122
iteration : 13577
train acc:  0.78125
train loss:  0.42148274183273315
train gradient:  0.08020646044916252
iteration : 13578
train acc:  0.7734375
train loss:  0.5457842350006104
train gradient:  0.1375077872191564
iteration : 13579
train acc:  0.703125
train loss:  0.5034270286560059
train gradient:  0.12859971483047228
iteration : 13580
train acc:  0.7265625
train loss:  0.5310994386672974
train gradient:  0.16674615726436748
iteration : 13581
train acc:  0.7265625
train loss:  0.5012063384056091
train gradient:  0.12634729354785543
iteration : 13582
train acc:  0.78125
train loss:  0.44649335741996765
train gradient:  0.09567820781596559
iteration : 13583
train acc:  0.7109375
train loss:  0.5289250016212463
train gradient:  0.17366251494906748
iteration : 13584
train acc:  0.7578125
train loss:  0.511559009552002
train gradient:  0.12236504900123657
iteration : 13585
train acc:  0.7578125
train loss:  0.5813273191452026
train gradient:  0.15802966745232921
iteration : 13586
train acc:  0.7109375
train loss:  0.5077590346336365
train gradient:  0.1322922787735956
iteration : 13587
train acc:  0.7265625
train loss:  0.509792685508728
train gradient:  0.15689043355870455
iteration : 13588
train acc:  0.734375
train loss:  0.452903687953949
train gradient:  0.10530046598430141
iteration : 13589
train acc:  0.765625
train loss:  0.5030077695846558
train gradient:  0.12067951783293354
iteration : 13590
train acc:  0.75
train loss:  0.4874911308288574
train gradient:  0.138087687567992
iteration : 13591
train acc:  0.765625
train loss:  0.5005414485931396
train gradient:  0.15293319291396917
iteration : 13592
train acc:  0.75
train loss:  0.46601903438568115
train gradient:  0.13491002783435976
iteration : 13593
train acc:  0.765625
train loss:  0.5104287266731262
train gradient:  0.14563688956921728
iteration : 13594
train acc:  0.7421875
train loss:  0.4930461645126343
train gradient:  0.1311100680047341
iteration : 13595
train acc:  0.6640625
train loss:  0.52153080701828
train gradient:  0.11620305752511437
iteration : 13596
train acc:  0.7265625
train loss:  0.45059898495674133
train gradient:  0.11503479680381401
iteration : 13597
train acc:  0.71875
train loss:  0.47899892926216125
train gradient:  0.12240618752852742
iteration : 13598
train acc:  0.71875
train loss:  0.5699979066848755
train gradient:  0.17351502411158443
iteration : 13599
train acc:  0.7578125
train loss:  0.4350360035896301
train gradient:  0.08678121361909903
iteration : 13600
train acc:  0.8203125
train loss:  0.43034136295318604
train gradient:  0.08180024504092638
iteration : 13601
train acc:  0.7265625
train loss:  0.46889179944992065
train gradient:  0.09625657420661184
iteration : 13602
train acc:  0.765625
train loss:  0.46744805574417114
train gradient:  0.11263914440693043
iteration : 13603
train acc:  0.7734375
train loss:  0.42688047885894775
train gradient:  0.11695061457443771
iteration : 13604
train acc:  0.71875
train loss:  0.5883209705352783
train gradient:  0.1593076393763724
iteration : 13605
train acc:  0.7421875
train loss:  0.5635044574737549
train gradient:  0.1590760976623942
iteration : 13606
train acc:  0.7265625
train loss:  0.5334976315498352
train gradient:  0.1705086014880175
iteration : 13607
train acc:  0.75
train loss:  0.4709484875202179
train gradient:  0.1003029048545142
iteration : 13608
train acc:  0.7109375
train loss:  0.5047652721405029
train gradient:  0.14024389894699468
iteration : 13609
train acc:  0.75
train loss:  0.4783954322338104
train gradient:  0.16787328639952934
iteration : 13610
train acc:  0.6875
train loss:  0.5017173290252686
train gradient:  0.14789869608462608
iteration : 13611
train acc:  0.7265625
train loss:  0.4746013879776001
train gradient:  0.10962774030221194
iteration : 13612
train acc:  0.75
train loss:  0.5306321382522583
train gradient:  0.15260787145763013
iteration : 13613
train acc:  0.71875
train loss:  0.5174435973167419
train gradient:  0.13542454815672844
iteration : 13614
train acc:  0.6953125
train loss:  0.4811122417449951
train gradient:  0.11146806492558646
iteration : 13615
train acc:  0.8046875
train loss:  0.405719518661499
train gradient:  0.0937817663859297
iteration : 13616
train acc:  0.6640625
train loss:  0.6464600563049316
train gradient:  0.21617262746145294
iteration : 13617
train acc:  0.7578125
train loss:  0.4642621874809265
train gradient:  0.11783557054874172
iteration : 13618
train acc:  0.75
train loss:  0.4621293544769287
train gradient:  0.09812708501805076
iteration : 13619
train acc:  0.78125
train loss:  0.4949202239513397
train gradient:  0.1304013417444076
iteration : 13620
train acc:  0.7265625
train loss:  0.45543172955513
train gradient:  0.11683267642120468
iteration : 13621
train acc:  0.734375
train loss:  0.4125790297985077
train gradient:  0.091359365661047
iteration : 13622
train acc:  0.7734375
train loss:  0.455381840467453
train gradient:  0.12577279948975753
iteration : 13623
train acc:  0.7734375
train loss:  0.4525604844093323
train gradient:  0.10779913916989474
iteration : 13624
train acc:  0.765625
train loss:  0.44588327407836914
train gradient:  0.0999741007133345
iteration : 13625
train acc:  0.7578125
train loss:  0.47429487109184265
train gradient:  0.10610702743527826
iteration : 13626
train acc:  0.75
train loss:  0.4852472245693207
train gradient:  0.12980517053940804
iteration : 13627
train acc:  0.7421875
train loss:  0.5530816316604614
train gradient:  0.1811172816980093
iteration : 13628
train acc:  0.71875
train loss:  0.5246482491493225
train gradient:  0.17864617677229327
iteration : 13629
train acc:  0.8125
train loss:  0.448514848947525
train gradient:  0.1006799649362764
iteration : 13630
train acc:  0.671875
train loss:  0.5676555633544922
train gradient:  0.1576725521201957
iteration : 13631
train acc:  0.765625
train loss:  0.4433700442314148
train gradient:  0.1022239182908264
iteration : 13632
train acc:  0.8046875
train loss:  0.46980297565460205
train gradient:  0.13602558058028763
iteration : 13633
train acc:  0.71875
train loss:  0.5339123606681824
train gradient:  0.13898311791666304
iteration : 13634
train acc:  0.7421875
train loss:  0.5055829286575317
train gradient:  0.14286529886815003
iteration : 13635
train acc:  0.71875
train loss:  0.47530341148376465
train gradient:  0.10915410532324224
iteration : 13636
train acc:  0.6953125
train loss:  0.5302070379257202
train gradient:  0.14454176663748947
iteration : 13637
train acc:  0.75
train loss:  0.5133810043334961
train gradient:  0.11932992908914215
iteration : 13638
train acc:  0.796875
train loss:  0.4819354712963104
train gradient:  0.11123767516135574
iteration : 13639
train acc:  0.7109375
train loss:  0.5690860748291016
train gradient:  0.13721457598537912
iteration : 13640
train acc:  0.7421875
train loss:  0.4738742411136627
train gradient:  0.15545557250689362
iteration : 13641
train acc:  0.7265625
train loss:  0.5384976863861084
train gradient:  0.12996219360824415
iteration : 13642
train acc:  0.6328125
train loss:  0.5593633055686951
train gradient:  0.13683942417569947
iteration : 13643
train acc:  0.734375
train loss:  0.46953824162483215
train gradient:  0.10921045118325223
iteration : 13644
train acc:  0.6484375
train loss:  0.5769280791282654
train gradient:  0.17762829446320466
iteration : 13645
train acc:  0.765625
train loss:  0.4615285396575928
train gradient:  0.11352677528722165
iteration : 13646
train acc:  0.765625
train loss:  0.5014770030975342
train gradient:  0.1325815092137731
iteration : 13647
train acc:  0.6875
train loss:  0.5444206595420837
train gradient:  0.13944437266521031
iteration : 13648
train acc:  0.7265625
train loss:  0.4700438678264618
train gradient:  0.14412335111021682
iteration : 13649
train acc:  0.71875
train loss:  0.5005730390548706
train gradient:  0.13495551160197042
iteration : 13650
train acc:  0.734375
train loss:  0.49749115109443665
train gradient:  0.11494673047527773
iteration : 13651
train acc:  0.7734375
train loss:  0.46029728651046753
train gradient:  0.1500236996255256
iteration : 13652
train acc:  0.7734375
train loss:  0.4252951145172119
train gradient:  0.1230551838324
iteration : 13653
train acc:  0.765625
train loss:  0.46989771723747253
train gradient:  0.11479568483324963
iteration : 13654
train acc:  0.7265625
train loss:  0.49448296427726746
train gradient:  0.13874571522789383
iteration : 13655
train acc:  0.7578125
train loss:  0.4481939971446991
train gradient:  0.09954565262952073
iteration : 13656
train acc:  0.765625
train loss:  0.47294241189956665
train gradient:  0.11405563101694785
iteration : 13657
train acc:  0.796875
train loss:  0.4628799259662628
train gradient:  0.142268718884781
iteration : 13658
train acc:  0.7734375
train loss:  0.45165345072746277
train gradient:  0.07962852623864083
iteration : 13659
train acc:  0.7265625
train loss:  0.5476587414741516
train gradient:  0.1663527949814466
iteration : 13660
train acc:  0.703125
train loss:  0.5231668949127197
train gradient:  0.1534279956149679
iteration : 13661
train acc:  0.8125
train loss:  0.4109618663787842
train gradient:  0.0925629610379057
iteration : 13662
train acc:  0.7109375
train loss:  0.5619590878486633
train gradient:  0.16153418708806966
iteration : 13663
train acc:  0.7734375
train loss:  0.4601919651031494
train gradient:  0.10835451446504721
iteration : 13664
train acc:  0.8203125
train loss:  0.4065098762512207
train gradient:  0.11080881497829842
iteration : 13665
train acc:  0.7734375
train loss:  0.45955032110214233
train gradient:  0.1100659390645269
iteration : 13666
train acc:  0.7421875
train loss:  0.4812307357788086
train gradient:  0.13148331510231803
iteration : 13667
train acc:  0.7109375
train loss:  0.46846550703048706
train gradient:  0.10535141605572278
iteration : 13668
train acc:  0.6953125
train loss:  0.49424365162849426
train gradient:  0.1159130301065433
iteration : 13669
train acc:  0.78125
train loss:  0.43523067235946655
train gradient:  0.09470139010616933
iteration : 13670
train acc:  0.7421875
train loss:  0.528839111328125
train gradient:  0.17008912857313496
iteration : 13671
train acc:  0.75
train loss:  0.47322365641593933
train gradient:  0.1042133441902944
iteration : 13672
train acc:  0.75
train loss:  0.504229724407196
train gradient:  0.12219565437798667
iteration : 13673
train acc:  0.71875
train loss:  0.5051857233047485
train gradient:  0.13061964857076733
iteration : 13674
train acc:  0.78125
train loss:  0.4557904601097107
train gradient:  0.09330041539816947
iteration : 13675
train acc:  0.75
train loss:  0.4792631268501282
train gradient:  0.11729594780243252
iteration : 13676
train acc:  0.8125
train loss:  0.3893963098526001
train gradient:  0.09065590000060603
iteration : 13677
train acc:  0.734375
train loss:  0.528430700302124
train gradient:  0.12537899612157577
iteration : 13678
train acc:  0.7109375
train loss:  0.5256443023681641
train gradient:  0.13480700109544647
iteration : 13679
train acc:  0.703125
train loss:  0.4924086332321167
train gradient:  0.13926562093625902
iteration : 13680
train acc:  0.6796875
train loss:  0.5288788080215454
train gradient:  0.13750565473827658
iteration : 13681
train acc:  0.6953125
train loss:  0.5208297967910767
train gradient:  0.1713305607738313
iteration : 13682
train acc:  0.828125
train loss:  0.45027750730514526
train gradient:  0.10249916600734064
iteration : 13683
train acc:  0.7265625
train loss:  0.5445804595947266
train gradient:  0.18744731410683096
iteration : 13684
train acc:  0.703125
train loss:  0.5454446077346802
train gradient:  0.13847722528911688
iteration : 13685
train acc:  0.71875
train loss:  0.5335943698883057
train gradient:  0.19228216334684695
iteration : 13686
train acc:  0.75
train loss:  0.4915003180503845
train gradient:  0.11747797182209158
iteration : 13687
train acc:  0.8046875
train loss:  0.45340901613235474
train gradient:  0.13500605725717368
iteration : 13688
train acc:  0.7421875
train loss:  0.5011448860168457
train gradient:  0.1121459141850812
iteration : 13689
train acc:  0.7109375
train loss:  0.5374970436096191
train gradient:  0.13835803660912013
iteration : 13690
train acc:  0.765625
train loss:  0.49780839681625366
train gradient:  0.11684508286666408
iteration : 13691
train acc:  0.6953125
train loss:  0.5318289995193481
train gradient:  0.1276740148647646
iteration : 13692
train acc:  0.8125
train loss:  0.4386308193206787
train gradient:  0.10265853366022384
iteration : 13693
train acc:  0.7109375
train loss:  0.4828808009624481
train gradient:  0.12059747215989518
iteration : 13694
train acc:  0.75
train loss:  0.5013630390167236
train gradient:  0.12234370964264889
iteration : 13695
train acc:  0.75
train loss:  0.4669944643974304
train gradient:  0.12757519940766185
iteration : 13696
train acc:  0.71875
train loss:  0.5186364054679871
train gradient:  0.14550019901771294
iteration : 13697
train acc:  0.7265625
train loss:  0.49539467692375183
train gradient:  0.12246325538457553
iteration : 13698
train acc:  0.71875
train loss:  0.5335901975631714
train gradient:  0.1445175709032992
iteration : 13699
train acc:  0.8359375
train loss:  0.41849732398986816
train gradient:  0.08333026624607832
iteration : 13700
train acc:  0.75
train loss:  0.5132790207862854
train gradient:  0.16590686903079838
iteration : 13701
train acc:  0.75
train loss:  0.4565173387527466
train gradient:  0.08637737540282384
iteration : 13702
train acc:  0.7890625
train loss:  0.46613138914108276
train gradient:  0.12156682825600047
iteration : 13703
train acc:  0.7109375
train loss:  0.5786296129226685
train gradient:  0.15007189255189962
iteration : 13704
train acc:  0.7421875
train loss:  0.5131117105484009
train gradient:  0.1283501989007379
iteration : 13705
train acc:  0.6796875
train loss:  0.5209395885467529
train gradient:  0.15560905670106578
iteration : 13706
train acc:  0.7578125
train loss:  0.47077009081840515
train gradient:  0.1093095453271663
iteration : 13707
train acc:  0.765625
train loss:  0.47702205181121826
train gradient:  0.11486671323383565
iteration : 13708
train acc:  0.734375
train loss:  0.5343910455703735
train gradient:  0.19293980759876708
iteration : 13709
train acc:  0.71875
train loss:  0.4862876534461975
train gradient:  0.11920707913573458
iteration : 13710
train acc:  0.7890625
train loss:  0.4443560242652893
train gradient:  0.10255201310253795
iteration : 13711
train acc:  0.8046875
train loss:  0.45589107275009155
train gradient:  0.0952134216998309
iteration : 13712
train acc:  0.65625
train loss:  0.5818229913711548
train gradient:  0.17432706040074275
iteration : 13713
train acc:  0.7265625
train loss:  0.488518089056015
train gradient:  0.11293902438975569
iteration : 13714
train acc:  0.765625
train loss:  0.4585089087486267
train gradient:  0.10937610162884226
iteration : 13715
train acc:  0.78125
train loss:  0.4861330986022949
train gradient:  0.12144297906273466
iteration : 13716
train acc:  0.796875
train loss:  0.45391160249710083
train gradient:  0.08703649136200266
iteration : 13717
train acc:  0.7890625
train loss:  0.5068904757499695
train gradient:  0.13882477793367706
iteration : 13718
train acc:  0.7421875
train loss:  0.4643412232398987
train gradient:  0.1042379789571422
iteration : 13719
train acc:  0.703125
train loss:  0.5015826225280762
train gradient:  0.1252148812539961
iteration : 13720
train acc:  0.75
train loss:  0.49036574363708496
train gradient:  0.11204508655982696
iteration : 13721
train acc:  0.765625
train loss:  0.5047844648361206
train gradient:  0.13530174044146764
iteration : 13722
train acc:  0.75
train loss:  0.5033859014511108
train gradient:  0.09515332102646111
iteration : 13723
train acc:  0.7265625
train loss:  0.5109250545501709
train gradient:  0.10063195439081515
iteration : 13724
train acc:  0.671875
train loss:  0.5137300491333008
train gradient:  0.13664638481237704
iteration : 13725
train acc:  0.7421875
train loss:  0.5287129282951355
train gradient:  0.130116661543
iteration : 13726
train acc:  0.6640625
train loss:  0.5196230411529541
train gradient:  0.13670644365773232
iteration : 13727
train acc:  0.75
train loss:  0.49540549516677856
train gradient:  0.10418068922082424
iteration : 13728
train acc:  0.765625
train loss:  0.45871809124946594
train gradient:  0.11385031424918825
iteration : 13729
train acc:  0.7421875
train loss:  0.4933498203754425
train gradient:  0.12054631024068659
iteration : 13730
train acc:  0.75
train loss:  0.44528645277023315
train gradient:  0.09191444928720144
iteration : 13731
train acc:  0.7265625
train loss:  0.5460922718048096
train gradient:  0.12696534759115555
iteration : 13732
train acc:  0.75
train loss:  0.4347076117992401
train gradient:  0.10535195452948373
iteration : 13733
train acc:  0.7578125
train loss:  0.47564682364463806
train gradient:  0.17039695108167535
iteration : 13734
train acc:  0.71875
train loss:  0.49770915508270264
train gradient:  0.11088690340634995
iteration : 13735
train acc:  0.7265625
train loss:  0.48869359493255615
train gradient:  0.10857787612866096
iteration : 13736
train acc:  0.6640625
train loss:  0.5820975303649902
train gradient:  0.1632365490186411
iteration : 13737
train acc:  0.78125
train loss:  0.4800679087638855
train gradient:  0.10175515223194354
iteration : 13738
train acc:  0.78125
train loss:  0.4504276514053345
train gradient:  0.09853669756629245
iteration : 13739
train acc:  0.7578125
train loss:  0.47345584630966187
train gradient:  0.1137921388361336
iteration : 13740
train acc:  0.71875
train loss:  0.46224650740623474
train gradient:  0.11008761037738661
iteration : 13741
train acc:  0.7578125
train loss:  0.44382792711257935
train gradient:  0.076676663243867
iteration : 13742
train acc:  0.7265625
train loss:  0.5044822096824646
train gradient:  0.1250537716954902
iteration : 13743
train acc:  0.765625
train loss:  0.4196496605873108
train gradient:  0.09926644769044782
iteration : 13744
train acc:  0.7578125
train loss:  0.5105888843536377
train gradient:  0.14890373977740778
iteration : 13745
train acc:  0.765625
train loss:  0.4749711751937866
train gradient:  0.11182688657509071
iteration : 13746
train acc:  0.7109375
train loss:  0.47377991676330566
train gradient:  0.09893461052924007
iteration : 13747
train acc:  0.7421875
train loss:  0.46004635095596313
train gradient:  0.12332053562841552
iteration : 13748
train acc:  0.703125
train loss:  0.49794426560401917
train gradient:  0.1287375117189067
iteration : 13749
train acc:  0.7265625
train loss:  0.4924270510673523
train gradient:  0.11005879266462607
iteration : 13750
train acc:  0.71875
train loss:  0.5057141184806824
train gradient:  0.11016901025608095
iteration : 13751
train acc:  0.7265625
train loss:  0.5099120140075684
train gradient:  0.12456137250765927
iteration : 13752
train acc:  0.75
train loss:  0.46376535296440125
train gradient:  0.12911645157466872
iteration : 13753
train acc:  0.7109375
train loss:  0.5351316928863525
train gradient:  0.15049274542157248
iteration : 13754
train acc:  0.7734375
train loss:  0.461852103471756
train gradient:  0.1290970989144747
iteration : 13755
train acc:  0.6953125
train loss:  0.5563041567802429
train gradient:  0.16674241468460632
iteration : 13756
train acc:  0.7578125
train loss:  0.5165786147117615
train gradient:  0.11423783003906644
iteration : 13757
train acc:  0.6640625
train loss:  0.5188027620315552
train gradient:  0.12596956191208952
iteration : 13758
train acc:  0.7578125
train loss:  0.46242350339889526
train gradient:  0.10430586069738415
iteration : 13759
train acc:  0.765625
train loss:  0.5253213047981262
train gradient:  0.1878479315018033
iteration : 13760
train acc:  0.7734375
train loss:  0.47374656796455383
train gradient:  0.10096079439507652
iteration : 13761
train acc:  0.71875
train loss:  0.5075368881225586
train gradient:  0.13503584081069514
iteration : 13762
train acc:  0.7109375
train loss:  0.5270777940750122
train gradient:  0.13139345946704262
iteration : 13763
train acc:  0.75
train loss:  0.4637479782104492
train gradient:  0.11089422450010403
iteration : 13764
train acc:  0.78125
train loss:  0.4976152181625366
train gradient:  0.12439154574550071
iteration : 13765
train acc:  0.71875
train loss:  0.49027854204177856
train gradient:  0.11308760723126032
iteration : 13766
train acc:  0.7421875
train loss:  0.48728951811790466
train gradient:  0.12081898320867554
iteration : 13767
train acc:  0.8046875
train loss:  0.3900224268436432
train gradient:  0.0817399738094998
iteration : 13768
train acc:  0.6875
train loss:  0.5023703575134277
train gradient:  0.1381022502537011
iteration : 13769
train acc:  0.7265625
train loss:  0.4790918827056885
train gradient:  0.11122687864494606
iteration : 13770
train acc:  0.78125
train loss:  0.4617820382118225
train gradient:  0.13411069372066092
iteration : 13771
train acc:  0.8359375
train loss:  0.4041098654270172
train gradient:  0.09485897175882321
iteration : 13772
train acc:  0.8046875
train loss:  0.4311264753341675
train gradient:  0.09424919769829858
iteration : 13773
train acc:  0.6953125
train loss:  0.5610916614532471
train gradient:  0.14967098162328113
iteration : 13774
train acc:  0.7890625
train loss:  0.4100741744041443
train gradient:  0.11334958806781244
iteration : 13775
train acc:  0.7890625
train loss:  0.434135764837265
train gradient:  0.10826134443085511
iteration : 13776
train acc:  0.71875
train loss:  0.49971476197242737
train gradient:  0.10301772988211819
iteration : 13777
train acc:  0.71875
train loss:  0.4756256341934204
train gradient:  0.14555089093678614
iteration : 13778
train acc:  0.7578125
train loss:  0.5525287389755249
train gradient:  0.13458026003291187
iteration : 13779
train acc:  0.7265625
train loss:  0.4792027175426483
train gradient:  0.11866479993397637
iteration : 13780
train acc:  0.7109375
train loss:  0.5531982183456421
train gradient:  0.16632658395385966
iteration : 13781
train acc:  0.765625
train loss:  0.47197362780570984
train gradient:  0.10568816576565493
iteration : 13782
train acc:  0.75
train loss:  0.4738248586654663
train gradient:  0.15794855048001305
iteration : 13783
train acc:  0.796875
train loss:  0.42458170652389526
train gradient:  0.0766393158327838
iteration : 13784
train acc:  0.734375
train loss:  0.5144997239112854
train gradient:  0.13389400825769954
iteration : 13785
train acc:  0.78125
train loss:  0.44192421436309814
train gradient:  0.11882603735038398
iteration : 13786
train acc:  0.7578125
train loss:  0.5074347257614136
train gradient:  0.11119006653649621
iteration : 13787
train acc:  0.6953125
train loss:  0.5494943857192993
train gradient:  0.13739309386434126
iteration : 13788
train acc:  0.6953125
train loss:  0.5161764025688171
train gradient:  0.12604092419612983
iteration : 13789
train acc:  0.75
train loss:  0.453058660030365
train gradient:  0.10278719438934124
iteration : 13790
train acc:  0.7890625
train loss:  0.42457297444343567
train gradient:  0.0821759371319129
iteration : 13791
train acc:  0.75
train loss:  0.4572422504425049
train gradient:  0.09416103330839697
iteration : 13792
train acc:  0.765625
train loss:  0.4438682496547699
train gradient:  0.10112161489082241
iteration : 13793
train acc:  0.765625
train loss:  0.4407079219818115
train gradient:  0.09133730243450848
iteration : 13794
train acc:  0.7578125
train loss:  0.43816322088241577
train gradient:  0.09268315024098275
iteration : 13795
train acc:  0.6796875
train loss:  0.5303908586502075
train gradient:  0.13429626426048222
iteration : 13796
train acc:  0.7109375
train loss:  0.5288897752761841
train gradient:  0.11893361507498904
iteration : 13797
train acc:  0.75
train loss:  0.47899267077445984
train gradient:  0.10941371583257067
iteration : 13798
train acc:  0.765625
train loss:  0.4637991487979889
train gradient:  0.09452711134768099
iteration : 13799
train acc:  0.6953125
train loss:  0.5174438953399658
train gradient:  0.11935140595866814
iteration : 13800
train acc:  0.703125
train loss:  0.5276916027069092
train gradient:  0.1326083811741982
iteration : 13801
train acc:  0.7421875
train loss:  0.4427778124809265
train gradient:  0.11897048337805469
iteration : 13802
train acc:  0.7421875
train loss:  0.47891998291015625
train gradient:  0.09979538704974723
iteration : 13803
train acc:  0.7578125
train loss:  0.4743320345878601
train gradient:  0.12021561853628172
iteration : 13804
train acc:  0.75
train loss:  0.47591668367385864
train gradient:  0.12680511916129003
iteration : 13805
train acc:  0.7421875
train loss:  0.4901268184185028
train gradient:  0.14351044142395272
iteration : 13806
train acc:  0.7578125
train loss:  0.48223283886909485
train gradient:  0.09265872178677309
iteration : 13807
train acc:  0.7109375
train loss:  0.5543166995048523
train gradient:  0.1533183348948618
iteration : 13808
train acc:  0.7109375
train loss:  0.5623682141304016
train gradient:  0.1681089116990494
iteration : 13809
train acc:  0.78125
train loss:  0.4360010325908661
train gradient:  0.08591206230104463
iteration : 13810
train acc:  0.8125
train loss:  0.4682915508747101
train gradient:  0.14384422938168934
iteration : 13811
train acc:  0.7578125
train loss:  0.4756056070327759
train gradient:  0.11760021843195922
iteration : 13812
train acc:  0.6953125
train loss:  0.5092331171035767
train gradient:  0.13131642332114524
iteration : 13813
train acc:  0.765625
train loss:  0.44263845682144165
train gradient:  0.09234194361695207
iteration : 13814
train acc:  0.6875
train loss:  0.5651030540466309
train gradient:  0.15653831149609843
iteration : 13815
train acc:  0.71875
train loss:  0.4892277419567108
train gradient:  0.12894769203956524
iteration : 13816
train acc:  0.734375
train loss:  0.46960771083831787
train gradient:  0.11911682219354898
iteration : 13817
train acc:  0.8125
train loss:  0.44372227787971497
train gradient:  0.11643806759265696
iteration : 13818
train acc:  0.765625
train loss:  0.4597397446632385
train gradient:  0.14197767608227407
iteration : 13819
train acc:  0.703125
train loss:  0.5359002947807312
train gradient:  0.11131353416868171
iteration : 13820
train acc:  0.71875
train loss:  0.5740891098976135
train gradient:  0.16765903338566332
iteration : 13821
train acc:  0.7265625
train loss:  0.4826558530330658
train gradient:  0.12639341431033912
iteration : 13822
train acc:  0.7734375
train loss:  0.4377990961074829
train gradient:  0.10223966204694154
iteration : 13823
train acc:  0.734375
train loss:  0.4567521810531616
train gradient:  0.10923964052757995
iteration : 13824
train acc:  0.734375
train loss:  0.43330836296081543
train gradient:  0.098215676397173
iteration : 13825
train acc:  0.7265625
train loss:  0.492147296667099
train gradient:  0.12681334941103328
iteration : 13826
train acc:  0.7734375
train loss:  0.43466049432754517
train gradient:  0.10089250262642778
iteration : 13827
train acc:  0.7890625
train loss:  0.37675297260284424
train gradient:  0.07387352732281491
iteration : 13828
train acc:  0.734375
train loss:  0.47205832600593567
train gradient:  0.11717828695207977
iteration : 13829
train acc:  0.7109375
train loss:  0.49650129675865173
train gradient:  0.11672046808784527
iteration : 13830
train acc:  0.6484375
train loss:  0.5697335004806519
train gradient:  0.15850021408665438
iteration : 13831
train acc:  0.703125
train loss:  0.5650119185447693
train gradient:  0.14218435530218124
iteration : 13832
train acc:  0.7421875
train loss:  0.4628335237503052
train gradient:  0.09883272866036208
iteration : 13833
train acc:  0.8046875
train loss:  0.4672742784023285
train gradient:  0.10935406311424041
iteration : 13834
train acc:  0.71875
train loss:  0.5382465124130249
train gradient:  0.1290830132104232
iteration : 13835
train acc:  0.734375
train loss:  0.50058913230896
train gradient:  0.13364529502006517
iteration : 13836
train acc:  0.75
train loss:  0.5024344325065613
train gradient:  0.1229319873357039
iteration : 13837
train acc:  0.6796875
train loss:  0.5532300472259521
train gradient:  0.1317535222135114
iteration : 13838
train acc:  0.71875
train loss:  0.502366840839386
train gradient:  0.10442989560710493
iteration : 13839
train acc:  0.7734375
train loss:  0.46599021553993225
train gradient:  0.14532942726141806
iteration : 13840
train acc:  0.765625
train loss:  0.4440041482448578
train gradient:  0.09882842510741789
iteration : 13841
train acc:  0.7734375
train loss:  0.44743970036506653
train gradient:  0.08322038962654628
iteration : 13842
train acc:  0.71875
train loss:  0.5089126229286194
train gradient:  0.1367194390320693
iteration : 13843
train acc:  0.78125
train loss:  0.4112589359283447
train gradient:  0.09819438750227809
iteration : 13844
train acc:  0.765625
train loss:  0.4698207378387451
train gradient:  0.08690863094620502
iteration : 13845
train acc:  0.6484375
train loss:  0.5853003859519958
train gradient:  0.15979520902772631
iteration : 13846
train acc:  0.7265625
train loss:  0.4898350238800049
train gradient:  0.13512838681901043
iteration : 13847
train acc:  0.6484375
train loss:  0.5913565158843994
train gradient:  0.17455276620132168
iteration : 13848
train acc:  0.75
train loss:  0.48834627866744995
train gradient:  0.11701369188713841
iteration : 13849
train acc:  0.7734375
train loss:  0.46647849678993225
train gradient:  0.1248798173367677
iteration : 13850
train acc:  0.8125
train loss:  0.4520232677459717
train gradient:  0.1024808884052654
iteration : 13851
train acc:  0.7578125
train loss:  0.4759106934070587
train gradient:  0.1410476489547452
iteration : 13852
train acc:  0.78125
train loss:  0.44269686937332153
train gradient:  0.12643502888904523
iteration : 13853
train acc:  0.796875
train loss:  0.48206424713134766
train gradient:  0.11501900839564541
iteration : 13854
train acc:  0.7890625
train loss:  0.46756255626678467
train gradient:  0.1013267304874249
iteration : 13855
train acc:  0.703125
train loss:  0.5705787539482117
train gradient:  0.10974910663895493
iteration : 13856
train acc:  0.703125
train loss:  0.5389779210090637
train gradient:  0.15791535139383273
iteration : 13857
train acc:  0.7109375
train loss:  0.49998655915260315
train gradient:  0.12317379253612465
iteration : 13858
train acc:  0.765625
train loss:  0.4879382848739624
train gradient:  0.12735395376141623
iteration : 13859
train acc:  0.75
train loss:  0.5188215970993042
train gradient:  0.14816440508297432
iteration : 13860
train acc:  0.7890625
train loss:  0.4375215768814087
train gradient:  0.10636633620301766
iteration : 13861
train acc:  0.7890625
train loss:  0.4644047021865845
train gradient:  0.10754152390888357
iteration : 13862
train acc:  0.7421875
train loss:  0.5583041310310364
train gradient:  0.16784340511600526
iteration : 13863
train acc:  0.765625
train loss:  0.4437439441680908
train gradient:  0.09798342584477326
iteration : 13864
train acc:  0.6953125
train loss:  0.5510516166687012
train gradient:  0.1289637375168916
iteration : 13865
train acc:  0.7421875
train loss:  0.4981350004673004
train gradient:  0.10938659909343132
iteration : 13866
train acc:  0.734375
train loss:  0.45349645614624023
train gradient:  0.10452802760202044
iteration : 13867
train acc:  0.6953125
train loss:  0.5522627234458923
train gradient:  0.1512298597044983
iteration : 13868
train acc:  0.671875
train loss:  0.5477217435836792
train gradient:  0.13674474319036206
iteration : 13869
train acc:  0.796875
train loss:  0.41169530153274536
train gradient:  0.08316470402680597
iteration : 13870
train acc:  0.7734375
train loss:  0.4707391858100891
train gradient:  0.1291431124122745
iteration : 13871
train acc:  0.7109375
train loss:  0.5523695945739746
train gradient:  0.15655291133645347
iteration : 13872
train acc:  0.78125
train loss:  0.43714776635169983
train gradient:  0.09921639677490914
iteration : 13873
train acc:  0.75
train loss:  0.47852200269699097
train gradient:  0.1118025594111113
iteration : 13874
train acc:  0.734375
train loss:  0.45245304703712463
train gradient:  0.10378354617525225
iteration : 13875
train acc:  0.78125
train loss:  0.44534409046173096
train gradient:  0.11791029586442296
iteration : 13876
train acc:  0.6953125
train loss:  0.5367841720581055
train gradient:  0.14099030668764656
iteration : 13877
train acc:  0.78125
train loss:  0.43328216671943665
train gradient:  0.09688709018926543
iteration : 13878
train acc:  0.7890625
train loss:  0.4531875550746918
train gradient:  0.10909274359407116
iteration : 13879
train acc:  0.8203125
train loss:  0.429736852645874
train gradient:  0.09709318129320008
iteration : 13880
train acc:  0.703125
train loss:  0.5472598075866699
train gradient:  0.15403626695515016
iteration : 13881
train acc:  0.8203125
train loss:  0.48961561918258667
train gradient:  0.12725711979386528
iteration : 13882
train acc:  0.6875
train loss:  0.5687589645385742
train gradient:  0.13900932677722114
iteration : 13883
train acc:  0.7734375
train loss:  0.5023980140686035
train gradient:  0.12772757370460494
iteration : 13884
train acc:  0.6796875
train loss:  0.5260416269302368
train gradient:  0.13462111802599344
iteration : 13885
train acc:  0.75
train loss:  0.4924176335334778
train gradient:  0.11866885706368989
iteration : 13886
train acc:  0.75
train loss:  0.5285221934318542
train gradient:  0.1788430712573131
iteration : 13887
train acc:  0.78125
train loss:  0.42976629734039307
train gradient:  0.1072645325106073
iteration : 13888
train acc:  0.765625
train loss:  0.4799537658691406
train gradient:  0.12496041859500888
iteration : 13889
train acc:  0.7734375
train loss:  0.43924805521965027
train gradient:  0.11528628547127105
iteration : 13890
train acc:  0.765625
train loss:  0.517011821269989
train gradient:  0.14557736246055178
iteration : 13891
train acc:  0.7265625
train loss:  0.5074272155761719
train gradient:  0.16530024731388432
iteration : 13892
train acc:  0.703125
train loss:  0.5210921764373779
train gradient:  0.122732912830257
iteration : 13893
train acc:  0.7578125
train loss:  0.46207040548324585
train gradient:  0.10407906985762683
iteration : 13894
train acc:  0.78125
train loss:  0.45426061749458313
train gradient:  0.11488075955200931
iteration : 13895
train acc:  0.71875
train loss:  0.4533597230911255
train gradient:  0.10257934538568403
iteration : 13896
train acc:  0.6796875
train loss:  0.5222466588020325
train gradient:  0.12574771955180272
iteration : 13897
train acc:  0.78125
train loss:  0.4596559405326843
train gradient:  0.10019895079911657
iteration : 13898
train acc:  0.7734375
train loss:  0.4786517322063446
train gradient:  0.11130315542028756
iteration : 13899
train acc:  0.765625
train loss:  0.4811549782752991
train gradient:  0.0909901343621092
iteration : 13900
train acc:  0.765625
train loss:  0.41637423634529114
train gradient:  0.11089877427381994
iteration : 13901
train acc:  0.7734375
train loss:  0.4409055709838867
train gradient:  0.10709003165077041
iteration : 13902
train acc:  0.7109375
train loss:  0.4939979612827301
train gradient:  0.10783098539268761
iteration : 13903
train acc:  0.71875
train loss:  0.5670711994171143
train gradient:  0.12720013884961306
iteration : 13904
train acc:  0.71875
train loss:  0.5204708576202393
train gradient:  0.12617953515075067
iteration : 13905
train acc:  0.7578125
train loss:  0.4214358925819397
train gradient:  0.09700384560827276
iteration : 13906
train acc:  0.7578125
train loss:  0.4720102548599243
train gradient:  0.1123131344058676
iteration : 13907
train acc:  0.7890625
train loss:  0.47092264890670776
train gradient:  0.11830705047767738
iteration : 13908
train acc:  0.7734375
train loss:  0.5004746317863464
train gradient:  0.12428508987312191
iteration : 13909
train acc:  0.7421875
train loss:  0.4765274226665497
train gradient:  0.1290823691623314
iteration : 13910
train acc:  0.671875
train loss:  0.48524928092956543
train gradient:  0.10263519974896727
iteration : 13911
train acc:  0.6875
train loss:  0.5725163221359253
train gradient:  0.15755853554803756
iteration : 13912
train acc:  0.7734375
train loss:  0.470429003238678
train gradient:  0.10467550703028779
iteration : 13913
train acc:  0.7734375
train loss:  0.42893195152282715
train gradient:  0.10902983710937852
iteration : 13914
train acc:  0.703125
train loss:  0.496005654335022
train gradient:  0.13719172237190314
iteration : 13915
train acc:  0.7890625
train loss:  0.4266332983970642
train gradient:  0.09726711997808073
iteration : 13916
train acc:  0.6953125
train loss:  0.5791016221046448
train gradient:  0.1630876321759768
iteration : 13917
train acc:  0.71875
train loss:  0.508811354637146
train gradient:  0.1036096794122028
iteration : 13918
train acc:  0.75
train loss:  0.48969709873199463
train gradient:  0.10905286956797518
iteration : 13919
train acc:  0.71875
train loss:  0.49979734420776367
train gradient:  0.10544283183431614
iteration : 13920
train acc:  0.7734375
train loss:  0.4607865810394287
train gradient:  0.12280723025571572
iteration : 13921
train acc:  0.7109375
train loss:  0.48584628105163574
train gradient:  0.12408045213371764
iteration : 13922
train acc:  0.7578125
train loss:  0.46848148107528687
train gradient:  0.11526139594705469
iteration : 13923
train acc:  0.7734375
train loss:  0.4943889379501343
train gradient:  0.14392193668696604
iteration : 13924
train acc:  0.78125
train loss:  0.4959247410297394
train gradient:  0.14855112288649647
iteration : 13925
train acc:  0.6875
train loss:  0.5460872650146484
train gradient:  0.12097717186606867
iteration : 13926
train acc:  0.765625
train loss:  0.47661206126213074
train gradient:  0.12835574117461088
iteration : 13927
train acc:  0.796875
train loss:  0.448483407497406
train gradient:  0.1323536946378146
iteration : 13928
train acc:  0.7890625
train loss:  0.4269161820411682
train gradient:  0.10804308384439122
iteration : 13929
train acc:  0.8046875
train loss:  0.4296116828918457
train gradient:  0.11061166312936736
iteration : 13930
train acc:  0.7109375
train loss:  0.49000367522239685
train gradient:  0.09439795326207086
iteration : 13931
train acc:  0.8515625
train loss:  0.416620135307312
train gradient:  0.09351390566711816
iteration : 13932
train acc:  0.7265625
train loss:  0.505125880241394
train gradient:  0.11630684673780618
iteration : 13933
train acc:  0.7578125
train loss:  0.47382310032844543
train gradient:  0.10865229749941782
iteration : 13934
train acc:  0.7734375
train loss:  0.47340744733810425
train gradient:  0.12991677543912045
iteration : 13935
train acc:  0.6875
train loss:  0.510667622089386
train gradient:  0.12548388735458088
iteration : 13936
train acc:  0.6640625
train loss:  0.5617421865463257
train gradient:  0.17227823884787247
iteration : 13937
train acc:  0.7265625
train loss:  0.5236248970031738
train gradient:  0.1402500738362773
iteration : 13938
train acc:  0.75
train loss:  0.48486024141311646
train gradient:  0.14658154798888334
iteration : 13939
train acc:  0.7109375
train loss:  0.5408751368522644
train gradient:  0.12481354149320185
iteration : 13940
train acc:  0.7734375
train loss:  0.45948857069015503
train gradient:  0.10862424994317636
iteration : 13941
train acc:  0.7578125
train loss:  0.4926289916038513
train gradient:  0.13949636634144857
iteration : 13942
train acc:  0.7421875
train loss:  0.5084501504898071
train gradient:  0.11547005117144071
iteration : 13943
train acc:  0.71875
train loss:  0.5136370658874512
train gradient:  0.12353652173123841
iteration : 13944
train acc:  0.7265625
train loss:  0.47579288482666016
train gradient:  0.09878681432826769
iteration : 13945
train acc:  0.8046875
train loss:  0.40863969922065735
train gradient:  0.08932901777604856
iteration : 13946
train acc:  0.7265625
train loss:  0.5098042488098145
train gradient:  0.1265218751809722
iteration : 13947
train acc:  0.7578125
train loss:  0.4868164360523224
train gradient:  0.10785566849034678
iteration : 13948
train acc:  0.671875
train loss:  0.5493636131286621
train gradient:  0.13164185273182127
iteration : 13949
train acc:  0.6875
train loss:  0.5095588564872742
train gradient:  0.1663572925307204
iteration : 13950
train acc:  0.8125
train loss:  0.4146918058395386
train gradient:  0.08895726937151523
iteration : 13951
train acc:  0.7109375
train loss:  0.4814694821834564
train gradient:  0.12109764750007705
iteration : 13952
train acc:  0.78125
train loss:  0.5256314277648926
train gradient:  0.14381899427672779
iteration : 13953
train acc:  0.765625
train loss:  0.48248720169067383
train gradient:  0.13279201350187658
iteration : 13954
train acc:  0.734375
train loss:  0.4924546480178833
train gradient:  0.15168487114708024
iteration : 13955
train acc:  0.8125
train loss:  0.4123663306236267
train gradient:  0.0875964789504642
iteration : 13956
train acc:  0.765625
train loss:  0.434689998626709
train gradient:  0.09529106769323603
iteration : 13957
train acc:  0.8203125
train loss:  0.4563562273979187
train gradient:  0.10748850555721831
iteration : 13958
train acc:  0.7265625
train loss:  0.5458412170410156
train gradient:  0.15494316517977513
iteration : 13959
train acc:  0.75
train loss:  0.4534068703651428
train gradient:  0.12037604744714234
iteration : 13960
train acc:  0.75
train loss:  0.46771493554115295
train gradient:  0.1255402931413191
iteration : 13961
train acc:  0.6953125
train loss:  0.5297476053237915
train gradient:  0.1433408830968328
iteration : 13962
train acc:  0.7578125
train loss:  0.46543315052986145
train gradient:  0.12819830923052772
iteration : 13963
train acc:  0.7265625
train loss:  0.4948996901512146
train gradient:  0.12148279802758198
iteration : 13964
train acc:  0.8125
train loss:  0.42793554067611694
train gradient:  0.09185859811650973
iteration : 13965
train acc:  0.7578125
train loss:  0.5063668489456177
train gradient:  0.12333043053544086
iteration : 13966
train acc:  0.78125
train loss:  0.4520472586154938
train gradient:  0.10175597374510638
iteration : 13967
train acc:  0.7265625
train loss:  0.5170862674713135
train gradient:  0.1359204072396986
iteration : 13968
train acc:  0.71875
train loss:  0.4673638343811035
train gradient:  0.12327868163931274
iteration : 13969
train acc:  0.7109375
train loss:  0.5163648128509521
train gradient:  0.1266141915706783
iteration : 13970
train acc:  0.71875
train loss:  0.5016375780105591
train gradient:  0.12676578191387605
iteration : 13971
train acc:  0.7421875
train loss:  0.5074429512023926
train gradient:  0.15868861676368362
iteration : 13972
train acc:  0.7109375
train loss:  0.46108555793762207
train gradient:  0.10390998598322636
iteration : 13973
train acc:  0.8125
train loss:  0.415065735578537
train gradient:  0.10022046971092513
iteration : 13974
train acc:  0.7421875
train loss:  0.46700501441955566
train gradient:  0.12297437295235335
iteration : 13975
train acc:  0.703125
train loss:  0.5637468695640564
train gradient:  0.13913200214160626
iteration : 13976
train acc:  0.7421875
train loss:  0.5132840871810913
train gradient:  0.12275644772929635
iteration : 13977
train acc:  0.7890625
train loss:  0.44920065999031067
train gradient:  0.11039266350622445
iteration : 13978
train acc:  0.796875
train loss:  0.4337577819824219
train gradient:  0.10291263517529826
iteration : 13979
train acc:  0.734375
train loss:  0.5117399096488953
train gradient:  0.12013977148635567
iteration : 13980
train acc:  0.796875
train loss:  0.4409233033657074
train gradient:  0.10482661396615003
iteration : 13981
train acc:  0.7265625
train loss:  0.47711843252182007
train gradient:  0.11576552454555206
iteration : 13982
train acc:  0.734375
train loss:  0.5047115087509155
train gradient:  0.1160405272220511
iteration : 13983
train acc:  0.703125
train loss:  0.5503435134887695
train gradient:  0.1359368405512544
iteration : 13984
train acc:  0.78125
train loss:  0.5082712173461914
train gradient:  0.12288435402030695
iteration : 13985
train acc:  0.7578125
train loss:  0.4974551796913147
train gradient:  0.10988585576543393
iteration : 13986
train acc:  0.65625
train loss:  0.5886240005493164
train gradient:  0.15783236628202185
iteration : 13987
train acc:  0.7265625
train loss:  0.4779964089393616
train gradient:  0.11541485303414818
iteration : 13988
train acc:  0.75
train loss:  0.505778968334198
train gradient:  0.1399090480863351
iteration : 13989
train acc:  0.7734375
train loss:  0.4600467383861542
train gradient:  0.10485802329043198
iteration : 13990
train acc:  0.7734375
train loss:  0.40805524587631226
train gradient:  0.07874296583063058
iteration : 13991
train acc:  0.6953125
train loss:  0.5352184176445007
train gradient:  0.11880724400400934
iteration : 13992
train acc:  0.7265625
train loss:  0.4417298436164856
train gradient:  0.09702453201787908
iteration : 13993
train acc:  0.7421875
train loss:  0.46808797121047974
train gradient:  0.12352529130962521
iteration : 13994
train acc:  0.7578125
train loss:  0.5404940843582153
train gradient:  0.14488959726059847
iteration : 13995
train acc:  0.765625
train loss:  0.500091552734375
train gradient:  0.11766888914286257
iteration : 13996
train acc:  0.7265625
train loss:  0.5018399953842163
train gradient:  0.14754074059598177
iteration : 13997
train acc:  0.7578125
train loss:  0.4676988124847412
train gradient:  0.11145118821933195
iteration : 13998
train acc:  0.8046875
train loss:  0.4007836580276489
train gradient:  0.07866826071584002
iteration : 13999
train acc:  0.703125
train loss:  0.5416259765625
train gradient:  0.11811548425293342
iteration : 14000
train acc:  0.7734375
train loss:  0.4091380536556244
train gradient:  0.0850404528429956
iteration : 14001
train acc:  0.7578125
train loss:  0.511327862739563
train gradient:  0.16431663811210134
iteration : 14002
train acc:  0.828125
train loss:  0.4442923069000244
train gradient:  0.0978871006136382
iteration : 14003
train acc:  0.78125
train loss:  0.4266469478607178
train gradient:  0.12292447729988103
iteration : 14004
train acc:  0.78125
train loss:  0.4911671578884125
train gradient:  0.14068562139698515
iteration : 14005
train acc:  0.7734375
train loss:  0.43407684564590454
train gradient:  0.07879590847361265
iteration : 14006
train acc:  0.75
train loss:  0.48831504583358765
train gradient:  0.12238271906050666
iteration : 14007
train acc:  0.7109375
train loss:  0.5493319034576416
train gradient:  0.14470305897473562
iteration : 14008
train acc:  0.7578125
train loss:  0.4991949796676636
train gradient:  0.14030834362977568
iteration : 14009
train acc:  0.7890625
train loss:  0.4477494955062866
train gradient:  0.13318941136003176
iteration : 14010
train acc:  0.78125
train loss:  0.46292340755462646
train gradient:  0.1456807931651503
iteration : 14011
train acc:  0.7421875
train loss:  0.49623411893844604
train gradient:  0.11411487886325676
iteration : 14012
train acc:  0.7421875
train loss:  0.48430967330932617
train gradient:  0.11746862094139553
iteration : 14013
train acc:  0.6953125
train loss:  0.6199889183044434
train gradient:  0.1508146995221697
iteration : 14014
train acc:  0.734375
train loss:  0.5209594964981079
train gradient:  0.13524825991496603
iteration : 14015
train acc:  0.8046875
train loss:  0.4649636745452881
train gradient:  0.11623896003583133
iteration : 14016
train acc:  0.7109375
train loss:  0.5127080082893372
train gradient:  0.11261161648288155
iteration : 14017
train acc:  0.734375
train loss:  0.5058407783508301
train gradient:  0.12896169918996125
iteration : 14018
train acc:  0.71875
train loss:  0.46910351514816284
train gradient:  0.12556019876834099
iteration : 14019
train acc:  0.796875
train loss:  0.47718504071235657
train gradient:  0.10814922951344949
iteration : 14020
train acc:  0.6953125
train loss:  0.5042225122451782
train gradient:  0.14352740393540409
iteration : 14021
train acc:  0.7578125
train loss:  0.5011948943138123
train gradient:  0.09381826934835852
iteration : 14022
train acc:  0.765625
train loss:  0.4458860754966736
train gradient:  0.09138674231212697
iteration : 14023
train acc:  0.7578125
train loss:  0.4733811020851135
train gradient:  0.12955329432562374
iteration : 14024
train acc:  0.7578125
train loss:  0.47344842553138733
train gradient:  0.13272356408864652
iteration : 14025
train acc:  0.7578125
train loss:  0.441057413816452
train gradient:  0.10127449266845213
iteration : 14026
train acc:  0.78125
train loss:  0.45443809032440186
train gradient:  0.12811530561590978
iteration : 14027
train acc:  0.7265625
train loss:  0.5094510316848755
train gradient:  0.1343865373963128
iteration : 14028
train acc:  0.7421875
train loss:  0.4706764817237854
train gradient:  0.09914797376100765
iteration : 14029
train acc:  0.7890625
train loss:  0.39496374130249023
train gradient:  0.07574386105847675
iteration : 14030
train acc:  0.75
train loss:  0.47737908363342285
train gradient:  0.10206348570826451
iteration : 14031
train acc:  0.6953125
train loss:  0.5477734804153442
train gradient:  0.16418350674574933
iteration : 14032
train acc:  0.78125
train loss:  0.414028525352478
train gradient:  0.09306447300127041
iteration : 14033
train acc:  0.8046875
train loss:  0.4521044194698334
train gradient:  0.1161923105862934
iteration : 14034
train acc:  0.734375
train loss:  0.5268246531486511
train gradient:  0.13386464603428627
iteration : 14035
train acc:  0.765625
train loss:  0.46237123012542725
train gradient:  0.14200472229981415
iteration : 14036
train acc:  0.71875
train loss:  0.5056704878807068
train gradient:  0.13382336750917567
iteration : 14037
train acc:  0.7109375
train loss:  0.5153997540473938
train gradient:  0.14172275800781786
iteration : 14038
train acc:  0.765625
train loss:  0.4388505816459656
train gradient:  0.08959975329900464
iteration : 14039
train acc:  0.75
train loss:  0.48581552505493164
train gradient:  0.1262609772239231
iteration : 14040
train acc:  0.7265625
train loss:  0.5015567541122437
train gradient:  0.11942592755029545
iteration : 14041
train acc:  0.71875
train loss:  0.5504070520401001
train gradient:  0.12797069479693798
iteration : 14042
train acc:  0.8125
train loss:  0.3919978141784668
train gradient:  0.1021774150663985
iteration : 14043
train acc:  0.6953125
train loss:  0.5276973247528076
train gradient:  0.13247342954697994
iteration : 14044
train acc:  0.7890625
train loss:  0.42443209886550903
train gradient:  0.09759877088113486
iteration : 14045
train acc:  0.7265625
train loss:  0.446196973323822
train gradient:  0.09252752888952019
iteration : 14046
train acc:  0.78125
train loss:  0.43850940465927124
train gradient:  0.1099175976340741
iteration : 14047
train acc:  0.6953125
train loss:  0.5094120502471924
train gradient:  0.13074342235154351
iteration : 14048
train acc:  0.703125
train loss:  0.5383738279342651
train gradient:  0.11765505801846292
iteration : 14049
train acc:  0.71875
train loss:  0.487224280834198
train gradient:  0.15990529247786672
iteration : 14050
train acc:  0.703125
train loss:  0.5444201827049255
train gradient:  0.15665273963844634
iteration : 14051
train acc:  0.7109375
train loss:  0.4957251250743866
train gradient:  0.13512612368833654
iteration : 14052
train acc:  0.7109375
train loss:  0.5022820830345154
train gradient:  0.1265254998499942
iteration : 14053
train acc:  0.7109375
train loss:  0.5178635120391846
train gradient:  0.1282765539525448
iteration : 14054
train acc:  0.765625
train loss:  0.49370062351226807
train gradient:  0.11962948020458457
iteration : 14055
train acc:  0.7890625
train loss:  0.4636727273464203
train gradient:  0.1282972145359305
iteration : 14056
train acc:  0.7890625
train loss:  0.49761736392974854
train gradient:  0.11551789419528283
iteration : 14057
train acc:  0.75
train loss:  0.4928908348083496
train gradient:  0.10668096779905988
iteration : 14058
train acc:  0.7890625
train loss:  0.4360623359680176
train gradient:  0.09985671782264495
iteration : 14059
train acc:  0.7265625
train loss:  0.5018245577812195
train gradient:  0.11859387677601325
iteration : 14060
train acc:  0.7578125
train loss:  0.47199249267578125
train gradient:  0.12185461892684309
iteration : 14061
train acc:  0.7265625
train loss:  0.497518926858902
train gradient:  0.12273533582625781
iteration : 14062
train acc:  0.6953125
train loss:  0.5078210830688477
train gradient:  0.1331088954318099
iteration : 14063
train acc:  0.7890625
train loss:  0.49171918630599976
train gradient:  0.15023193121420103
iteration : 14064
train acc:  0.796875
train loss:  0.46564793586730957
train gradient:  0.1415859462951336
iteration : 14065
train acc:  0.7421875
train loss:  0.5140782594680786
train gradient:  0.14097598827417
iteration : 14066
train acc:  0.8125
train loss:  0.43301573395729065
train gradient:  0.1034663456624998
iteration : 14067
train acc:  0.7734375
train loss:  0.4711737036705017
train gradient:  0.09722223364221474
iteration : 14068
train acc:  0.75
train loss:  0.4906812310218811
train gradient:  0.11653329697494925
iteration : 14069
train acc:  0.7578125
train loss:  0.45829832553863525
train gradient:  0.14080204803326413
iteration : 14070
train acc:  0.7734375
train loss:  0.4601423740386963
train gradient:  0.10091582216341201
iteration : 14071
train acc:  0.7578125
train loss:  0.4925037920475006
train gradient:  0.12581695424736336
iteration : 14072
train acc:  0.7421875
train loss:  0.5130839347839355
train gradient:  0.137035541348006
iteration : 14073
train acc:  0.7734375
train loss:  0.42226701974868774
train gradient:  0.08666582633184136
iteration : 14074
train acc:  0.7578125
train loss:  0.4982408881187439
train gradient:  0.1520412586943869
iteration : 14075
train acc:  0.8046875
train loss:  0.4338386058807373
train gradient:  0.08907994198830377
iteration : 14076
train acc:  0.78125
train loss:  0.4596642553806305
train gradient:  0.1185842608299807
iteration : 14077
train acc:  0.8203125
train loss:  0.40926313400268555
train gradient:  0.08022078152219603
iteration : 14078
train acc:  0.7890625
train loss:  0.4232381582260132
train gradient:  0.09439031466935893
iteration : 14079
train acc:  0.765625
train loss:  0.4368034303188324
train gradient:  0.09157982850509365
iteration : 14080
train acc:  0.7578125
train loss:  0.45174065232276917
train gradient:  0.09947394393316746
iteration : 14081
train acc:  0.8046875
train loss:  0.4944800138473511
train gradient:  0.17273446924837182
iteration : 14082
train acc:  0.7734375
train loss:  0.4462714195251465
train gradient:  0.09067583208293634
iteration : 14083
train acc:  0.7421875
train loss:  0.4686117470264435
train gradient:  0.11983089995659602
iteration : 14084
train acc:  0.7109375
train loss:  0.5002071261405945
train gradient:  0.11379297466677597
iteration : 14085
train acc:  0.7734375
train loss:  0.4692453145980835
train gradient:  0.0970858427470146
iteration : 14086
train acc:  0.7265625
train loss:  0.5041900873184204
train gradient:  0.12599718405724106
iteration : 14087
train acc:  0.640625
train loss:  0.5686917901039124
train gradient:  0.1767841933084234
iteration : 14088
train acc:  0.703125
train loss:  0.5837317705154419
train gradient:  0.18197913270128946
iteration : 14089
train acc:  0.7578125
train loss:  0.4267841875553131
train gradient:  0.0804841423513407
iteration : 14090
train acc:  0.7109375
train loss:  0.47001761198043823
train gradient:  0.11516090061135689
iteration : 14091
train acc:  0.6796875
train loss:  0.6056488752365112
train gradient:  0.14908608966237757
iteration : 14092
train acc:  0.7890625
train loss:  0.44696033000946045
train gradient:  0.09736065151880548
iteration : 14093
train acc:  0.7578125
train loss:  0.47212105989456177
train gradient:  0.11434473491514605
iteration : 14094
train acc:  0.6953125
train loss:  0.5533995628356934
train gradient:  0.14638295351063912
iteration : 14095
train acc:  0.7109375
train loss:  0.48227280378341675
train gradient:  0.12046413206574405
iteration : 14096
train acc:  0.78125
train loss:  0.4358051121234894
train gradient:  0.10400573277064733
iteration : 14097
train acc:  0.7890625
train loss:  0.4484594464302063
train gradient:  0.09290958028573904
iteration : 14098
train acc:  0.7109375
train loss:  0.5451374053955078
train gradient:  0.1570306970939937
iteration : 14099
train acc:  0.75
train loss:  0.5110639929771423
train gradient:  0.1252572767700082
iteration : 14100
train acc:  0.7421875
train loss:  0.4992409348487854
train gradient:  0.13973157615238058
iteration : 14101
train acc:  0.75
train loss:  0.46132898330688477
train gradient:  0.11927842820306875
iteration : 14102
train acc:  0.7890625
train loss:  0.4534420073032379
train gradient:  0.10281924971799092
iteration : 14103
train acc:  0.7265625
train loss:  0.5446717143058777
train gradient:  0.17417815009434345
iteration : 14104
train acc:  0.6953125
train loss:  0.5837953090667725
train gradient:  0.1882708970119673
iteration : 14105
train acc:  0.765625
train loss:  0.47533488273620605
train gradient:  0.10742370890622832
iteration : 14106
train acc:  0.8046875
train loss:  0.43746861815452576
train gradient:  0.09950795777996398
iteration : 14107
train acc:  0.75
train loss:  0.4429383873939514
train gradient:  0.1300113633760446
iteration : 14108
train acc:  0.734375
train loss:  0.5144641399383545
train gradient:  0.1500403707047855
iteration : 14109
train acc:  0.7265625
train loss:  0.5054852962493896
train gradient:  0.13084857485788987
iteration : 14110
train acc:  0.734375
train loss:  0.5108104944229126
train gradient:  0.14109422825456003
iteration : 14111
train acc:  0.7421875
train loss:  0.5025912523269653
train gradient:  0.1332414690856791
iteration : 14112
train acc:  0.7734375
train loss:  0.43918806314468384
train gradient:  0.11683421177081564
iteration : 14113
train acc:  0.765625
train loss:  0.4350723624229431
train gradient:  0.11530305880288227
iteration : 14114
train acc:  0.671875
train loss:  0.5576045513153076
train gradient:  0.15783015337071538
iteration : 14115
train acc:  0.7421875
train loss:  0.49438750743865967
train gradient:  0.10042399265663791
iteration : 14116
train acc:  0.671875
train loss:  0.5306116342544556
train gradient:  0.1439774829902895
iteration : 14117
train acc:  0.65625
train loss:  0.6379058361053467
train gradient:  0.22797354313917714
iteration : 14118
train acc:  0.765625
train loss:  0.43891724944114685
train gradient:  0.0889779304544192
iteration : 14119
train acc:  0.75
train loss:  0.490106463432312
train gradient:  0.14996232211181526
iteration : 14120
train acc:  0.703125
train loss:  0.5276986360549927
train gradient:  0.1280254307316322
iteration : 14121
train acc:  0.75
train loss:  0.4623374342918396
train gradient:  0.10707927417422393
iteration : 14122
train acc:  0.7578125
train loss:  0.46922528743743896
train gradient:  0.10433976896634152
iteration : 14123
train acc:  0.7109375
train loss:  0.4878663420677185
train gradient:  0.1094182513649296
iteration : 14124
train acc:  0.703125
train loss:  0.5528885126113892
train gradient:  0.14986242330503535
iteration : 14125
train acc:  0.7421875
train loss:  0.48911052942276
train gradient:  0.11905892782464919
iteration : 14126
train acc:  0.703125
train loss:  0.5678564310073853
train gradient:  0.1392490229456802
iteration : 14127
train acc:  0.734375
train loss:  0.5054435133934021
train gradient:  0.14062056913780852
iteration : 14128
train acc:  0.8046875
train loss:  0.3811056613922119
train gradient:  0.08338803007392752
iteration : 14129
train acc:  0.75
train loss:  0.4741894006729126
train gradient:  0.10973760791402186
iteration : 14130
train acc:  0.765625
train loss:  0.4701097011566162
train gradient:  0.1039162109590932
iteration : 14131
train acc:  0.734375
train loss:  0.4768892228603363
train gradient:  0.1005788369899443
iteration : 14132
train acc:  0.7890625
train loss:  0.44967272877693176
train gradient:  0.11284594829316698
iteration : 14133
train acc:  0.7265625
train loss:  0.46430325508117676
train gradient:  0.09891824061840115
iteration : 14134
train acc:  0.6640625
train loss:  0.5610624551773071
train gradient:  0.155745365512715
iteration : 14135
train acc:  0.8046875
train loss:  0.43432021141052246
train gradient:  0.08765053214720692
iteration : 14136
train acc:  0.765625
train loss:  0.47509557008743286
train gradient:  0.11106325952654515
iteration : 14137
train acc:  0.7109375
train loss:  0.54410320520401
train gradient:  0.13769874710051788
iteration : 14138
train acc:  0.7578125
train loss:  0.4240102767944336
train gradient:  0.09279439740944305
iteration : 14139
train acc:  0.734375
train loss:  0.5270197987556458
train gradient:  0.12839746470898755
iteration : 14140
train acc:  0.71875
train loss:  0.5269513130187988
train gradient:  0.14201645947009944
iteration : 14141
train acc:  0.734375
train loss:  0.521358847618103
train gradient:  0.1362491492554215
iteration : 14142
train acc:  0.7421875
train loss:  0.48417121171951294
train gradient:  0.12372983798765252
iteration : 14143
train acc:  0.734375
train loss:  0.5083509683609009
train gradient:  0.14731184911376494
iteration : 14144
train acc:  0.734375
train loss:  0.49614593386650085
train gradient:  0.12390635346758735
iteration : 14145
train acc:  0.71875
train loss:  0.5193261504173279
train gradient:  0.14442111974649768
iteration : 14146
train acc:  0.75
train loss:  0.4799630045890808
train gradient:  0.11728747319481615
iteration : 14147
train acc:  0.7578125
train loss:  0.48820358514785767
train gradient:  0.11606834324706078
iteration : 14148
train acc:  0.6953125
train loss:  0.5286988615989685
train gradient:  0.16475835830663008
iteration : 14149
train acc:  0.734375
train loss:  0.5260401964187622
train gradient:  0.14487547957629832
iteration : 14150
train acc:  0.7890625
train loss:  0.44449907541275024
train gradient:  0.12230867658979083
iteration : 14151
train acc:  0.7421875
train loss:  0.4782211482524872
train gradient:  0.09890047161195206
iteration : 14152
train acc:  0.78125
train loss:  0.49134108424186707
train gradient:  0.14068020786284552
iteration : 14153
train acc:  0.7109375
train loss:  0.5507622957229614
train gradient:  0.1363157760562718
iteration : 14154
train acc:  0.703125
train loss:  0.5436739325523376
train gradient:  0.1651006369685512
iteration : 14155
train acc:  0.7890625
train loss:  0.43834567070007324
train gradient:  0.09883309775419136
iteration : 14156
train acc:  0.75
train loss:  0.46237456798553467
train gradient:  0.0918372851967881
iteration : 14157
train acc:  0.71875
train loss:  0.5043878555297852
train gradient:  0.1349426192521086
iteration : 14158
train acc:  0.8359375
train loss:  0.41163939237594604
train gradient:  0.1106698949255537
iteration : 14159
train acc:  0.7578125
train loss:  0.45984870195388794
train gradient:  0.13385845668270147
iteration : 14160
train acc:  0.734375
train loss:  0.5178147554397583
train gradient:  0.11349620197004481
iteration : 14161
train acc:  0.71875
train loss:  0.49682170152664185
train gradient:  0.11565431494756487
iteration : 14162
train acc:  0.765625
train loss:  0.4236915111541748
train gradient:  0.091116084567887
iteration : 14163
train acc:  0.6953125
train loss:  0.5457254648208618
train gradient:  0.12624429313305768
iteration : 14164
train acc:  0.7421875
train loss:  0.4758630394935608
train gradient:  0.11512082123349737
iteration : 14165
train acc:  0.6953125
train loss:  0.49339789152145386
train gradient:  0.11922414584699752
iteration : 14166
train acc:  0.765625
train loss:  0.5219900608062744
train gradient:  0.1501525910086562
iteration : 14167
train acc:  0.7421875
train loss:  0.5024736523628235
train gradient:  0.1234252468859436
iteration : 14168
train acc:  0.8046875
train loss:  0.4127745032310486
train gradient:  0.09481646300054113
iteration : 14169
train acc:  0.6484375
train loss:  0.6130241751670837
train gradient:  0.18727134828257833
iteration : 14170
train acc:  0.828125
train loss:  0.42968136072158813
train gradient:  0.09153239467265448
iteration : 14171
train acc:  0.7578125
train loss:  0.46593615412712097
train gradient:  0.10825587933813707
iteration : 14172
train acc:  0.8203125
train loss:  0.41837090253829956
train gradient:  0.09677562039567379
iteration : 14173
train acc:  0.703125
train loss:  0.5363848805427551
train gradient:  0.1296258032566653
iteration : 14174
train acc:  0.6875
train loss:  0.5385173559188843
train gradient:  0.19841529557427223
iteration : 14175
train acc:  0.8046875
train loss:  0.4302480220794678
train gradient:  0.11069891678634138
iteration : 14176
train acc:  0.7734375
train loss:  0.4302663207054138
train gradient:  0.08185518304370881
iteration : 14177
train acc:  0.71875
train loss:  0.47677409648895264
train gradient:  0.10299027878669824
iteration : 14178
train acc:  0.7734375
train loss:  0.5143047571182251
train gradient:  0.1470669643265833
iteration : 14179
train acc:  0.765625
train loss:  0.44013139605522156
train gradient:  0.10565757374388907
iteration : 14180
train acc:  0.71875
train loss:  0.46993017196655273
train gradient:  0.1114693718625339
iteration : 14181
train acc:  0.7734375
train loss:  0.4910663962364197
train gradient:  0.13246915492610656
iteration : 14182
train acc:  0.734375
train loss:  0.43689021468162537
train gradient:  0.10152260795541397
iteration : 14183
train acc:  0.796875
train loss:  0.449396550655365
train gradient:  0.1093426200502847
iteration : 14184
train acc:  0.734375
train loss:  0.4907609224319458
train gradient:  0.11504829925720242
iteration : 14185
train acc:  0.734375
train loss:  0.5268711447715759
train gradient:  0.13657449352751
iteration : 14186
train acc:  0.8333333333333334
train loss:  0.4148463010787964
train gradient:  0.7735814758774794
val acc:  0.7420241354832167
val f1:  0.7539010728554982
val confusion matrix:  [[68412 30198]
 [20680 77930]]

----------------------------------------new_epoch--------------------------------------

epoch:  1
iteration : 0
train acc:  0.71875
train loss:  0.5041310787200928
train gradient:  0.10910295890302935
iteration : 1
train acc:  0.8515625
train loss:  0.40122318267822266
train gradient:  0.113936595420547
iteration : 2
train acc:  0.84375
train loss:  0.3806309998035431
train gradient:  0.07703247000364323
iteration : 3
train acc:  0.8046875
train loss:  0.43055957555770874
train gradient:  0.096484037571372
iteration : 4
train acc:  0.8359375
train loss:  0.37860995531082153
train gradient:  0.08620210312221903
iteration : 5
train acc:  0.703125
train loss:  0.508030891418457
train gradient:  0.13082966549287092
iteration : 6
train acc:  0.71875
train loss:  0.5174943804740906
train gradient:  0.14521953561398415
iteration : 7
train acc:  0.7109375
train loss:  0.5184410214424133
train gradient:  0.1501127191067127
iteration : 8
train acc:  0.7578125
train loss:  0.48652184009552
train gradient:  0.12645522916550322
iteration : 9
train acc:  0.7734375
train loss:  0.4439406991004944
train gradient:  0.12071878438938922
iteration : 10
train acc:  0.7109375
train loss:  0.5038009881973267
train gradient:  0.14360460823644583
iteration : 11
train acc:  0.796875
train loss:  0.4644189178943634
train gradient:  0.10288119672643642
iteration : 12
train acc:  0.71875
train loss:  0.5698922276496887
train gradient:  0.14207945991548643
iteration : 13
train acc:  0.7578125
train loss:  0.43320178985595703
train gradient:  0.09200884258631004
iteration : 14
train acc:  0.7578125
train loss:  0.47083738446235657
train gradient:  0.1300413547699375
iteration : 15
train acc:  0.734375
train loss:  0.4622601568698883
train gradient:  0.09869780301649618
iteration : 16
train acc:  0.7578125
train loss:  0.44561517238616943
train gradient:  0.11730205693845978
iteration : 17
train acc:  0.8046875
train loss:  0.4146527051925659
train gradient:  0.11707859034560002
iteration : 18
train acc:  0.7265625
train loss:  0.5013163089752197
train gradient:  0.11547470616103667
iteration : 19
train acc:  0.671875
train loss:  0.5496510863304138
train gradient:  0.12880064795977397
iteration : 20
train acc:  0.7734375
train loss:  0.44871360063552856
train gradient:  0.0917373070688791
iteration : 21
train acc:  0.765625
train loss:  0.5211265683174133
train gradient:  0.15518927559766485
iteration : 22
train acc:  0.75
train loss:  0.5185251235961914
train gradient:  0.13954654376139802
iteration : 23
train acc:  0.7890625
train loss:  0.43135422468185425
train gradient:  0.10775675900748627
iteration : 24
train acc:  0.7109375
train loss:  0.46364492177963257
train gradient:  0.11275103277095948
iteration : 25
train acc:  0.6640625
train loss:  0.5627315044403076
train gradient:  0.13640777776393642
iteration : 26
train acc:  0.78125
train loss:  0.4402773976325989
train gradient:  0.09078012314272606
iteration : 27
train acc:  0.7890625
train loss:  0.4825384318828583
train gradient:  0.11456006200873439
iteration : 28
train acc:  0.6796875
train loss:  0.5623082518577576
train gradient:  0.1454223425526443
iteration : 29
train acc:  0.765625
train loss:  0.4718635678291321
train gradient:  0.15533580892335624
iteration : 30
train acc:  0.7109375
train loss:  0.5246075987815857
train gradient:  0.13144213766745189
iteration : 31
train acc:  0.7109375
train loss:  0.5096218585968018
train gradient:  0.12059571581857378
iteration : 32
train acc:  0.7578125
train loss:  0.5251449942588806
train gradient:  0.1380679438676271
iteration : 33
train acc:  0.7109375
train loss:  0.5154216289520264
train gradient:  0.12032462032818293
iteration : 34
train acc:  0.703125
train loss:  0.5664293766021729
train gradient:  0.2069471125239316
iteration : 35
train acc:  0.7578125
train loss:  0.4845584034919739
train gradient:  0.14156895686477228
iteration : 36
train acc:  0.78125
train loss:  0.46820706129074097
train gradient:  0.10247804086988291
iteration : 37
train acc:  0.7734375
train loss:  0.5040094256401062
train gradient:  0.11044747440665205
iteration : 38
train acc:  0.7734375
train loss:  0.5308406352996826
train gradient:  0.13559360631353662
iteration : 39
train acc:  0.7734375
train loss:  0.45987850427627563
train gradient:  0.1268830841046678
iteration : 40
train acc:  0.8046875
train loss:  0.44321393966674805
train gradient:  0.10140847412193725
iteration : 41
train acc:  0.703125
train loss:  0.5471895933151245
train gradient:  0.2129046211600127
iteration : 42
train acc:  0.7890625
train loss:  0.4480644464492798
train gradient:  0.09582035294383716
iteration : 43
train acc:  0.6953125
train loss:  0.5207381248474121
train gradient:  0.13715857731332504
iteration : 44
train acc:  0.7734375
train loss:  0.41555047035217285
train gradient:  0.0889592668550502
iteration : 45
train acc:  0.75
train loss:  0.4626244008541107
train gradient:  0.09944277026062595
iteration : 46
train acc:  0.7890625
train loss:  0.4207528829574585
train gradient:  0.09598106086368224
iteration : 47
train acc:  0.71875
train loss:  0.5236785411834717
train gradient:  0.14201886050643214
iteration : 48
train acc:  0.671875
train loss:  0.5496689081192017
train gradient:  0.16131832008778707
iteration : 49
train acc:  0.7578125
train loss:  0.4335101842880249
train gradient:  0.1208431830043642
iteration : 50
train acc:  0.8046875
train loss:  0.4192351698875427
train gradient:  0.09097124572462342
iteration : 51
train acc:  0.75
train loss:  0.44342607259750366
train gradient:  0.11270065273716234
iteration : 52
train acc:  0.7421875
train loss:  0.47461193799972534
train gradient:  0.08942892726982993
iteration : 53
train acc:  0.765625
train loss:  0.43993017077445984
train gradient:  0.10105305526576588
iteration : 54
train acc:  0.796875
train loss:  0.4294772744178772
train gradient:  0.08626758904627513
iteration : 55
train acc:  0.7734375
train loss:  0.4381582736968994
train gradient:  0.07860172519766068
iteration : 56
train acc:  0.75
train loss:  0.43695443868637085
train gradient:  0.08643442939695972
iteration : 57
train acc:  0.7578125
train loss:  0.5023237466812134
train gradient:  0.15167284343518533
iteration : 58
train acc:  0.734375
train loss:  0.4577539563179016
train gradient:  0.1070859211435313
iteration : 59
train acc:  0.625
train loss:  0.5675392746925354
train gradient:  0.14183685308572178
iteration : 60
train acc:  0.78125
train loss:  0.4444795250892639
train gradient:  0.10744834586489362
iteration : 61
train acc:  0.8046875
train loss:  0.4740888178348541
train gradient:  0.09718600371394696
iteration : 62
train acc:  0.7734375
train loss:  0.43284928798675537
train gradient:  0.11248396280347958
iteration : 63
train acc:  0.7734375
train loss:  0.4844377040863037
train gradient:  0.1403260310164241
iteration : 64
train acc:  0.875
train loss:  0.41791287064552307
train gradient:  0.07490981803575604
iteration : 65
train acc:  0.7265625
train loss:  0.48334193229675293
train gradient:  0.125495182212439
iteration : 66
train acc:  0.7265625
train loss:  0.5202350616455078
train gradient:  0.11614613905204887
iteration : 67
train acc:  0.734375
train loss:  0.5336402058601379
train gradient:  0.20631995769936382
iteration : 68
train acc:  0.78125
train loss:  0.4327702522277832
train gradient:  0.11151742544366693
iteration : 69
train acc:  0.6953125
train loss:  0.520775556564331
train gradient:  0.14902860032934379
iteration : 70
train acc:  0.7890625
train loss:  0.4489539861679077
train gradient:  0.11754831887465586
iteration : 71
train acc:  0.78125
train loss:  0.41180166602134705
train gradient:  0.0844320469496555
iteration : 72
train acc:  0.78125
train loss:  0.4403993785381317
train gradient:  0.09384427662416858
iteration : 73
train acc:  0.796875
train loss:  0.41685307025909424
train gradient:  0.11133204864964746
iteration : 74
train acc:  0.7421875
train loss:  0.5459483861923218
train gradient:  0.13781303714515913
iteration : 75
train acc:  0.7578125
train loss:  0.4483395516872406
train gradient:  0.10102990163212168
iteration : 76
train acc:  0.7265625
train loss:  0.5274168252944946
train gradient:  0.15783843333299502
iteration : 77
train acc:  0.7890625
train loss:  0.5298933982849121
train gradient:  0.13709298185870905
iteration : 78
train acc:  0.78125
train loss:  0.5032228231430054
train gradient:  0.12435406257107864
iteration : 79
train acc:  0.71875
train loss:  0.47609764337539673
train gradient:  0.13962571721103814
iteration : 80
train acc:  0.75
train loss:  0.5009889602661133
train gradient:  0.151402491517347
iteration : 81
train acc:  0.7265625
train loss:  0.49669772386550903
train gradient:  0.1295944165751703
iteration : 82
train acc:  0.71875
train loss:  0.4863293766975403
train gradient:  0.13970492203778312
iteration : 83
train acc:  0.7734375
train loss:  0.4471210539340973
train gradient:  0.10422212821544387
iteration : 84
train acc:  0.7109375
train loss:  0.4958694279193878
train gradient:  0.14188460167539588
iteration : 85
train acc:  0.765625
train loss:  0.4817034900188446
train gradient:  0.09985354540078949
iteration : 86
train acc:  0.734375
train loss:  0.48515743017196655
train gradient:  0.09781063790873265
iteration : 87
train acc:  0.71875
train loss:  0.5667607188224792
train gradient:  0.13855736196113685
iteration : 88
train acc:  0.796875
train loss:  0.40840011835098267
train gradient:  0.10846360253018869
iteration : 89
train acc:  0.6875
train loss:  0.5094336867332458
train gradient:  0.12368849340555313
iteration : 90
train acc:  0.7890625
train loss:  0.4283301830291748
train gradient:  0.10391483374331592
iteration : 91
train acc:  0.6875
train loss:  0.5349125862121582
train gradient:  0.14058579943277197
iteration : 92
train acc:  0.734375
train loss:  0.5062292218208313
train gradient:  0.14382624002926145
iteration : 93
train acc:  0.7578125
train loss:  0.4564374089241028
train gradient:  0.09940539569849595
iteration : 94
train acc:  0.75
train loss:  0.45184892416000366
train gradient:  0.09039679050889958
iteration : 95
train acc:  0.734375
train loss:  0.5121320486068726
train gradient:  0.14317739951900443
iteration : 96
train acc:  0.6328125
train loss:  0.6137911081314087
train gradient:  0.1634179548821334
iteration : 97
train acc:  0.734375
train loss:  0.49068111181259155
train gradient:  0.12884083724173567
iteration : 98
train acc:  0.75
train loss:  0.492717981338501
train gradient:  0.14612519485541997
iteration : 99
train acc:  0.7734375
train loss:  0.42982345819473267
train gradient:  0.13252194685318724
iteration : 100
train acc:  0.7109375
train loss:  0.5379865169525146
train gradient:  0.1372643156359208
iteration : 101
train acc:  0.734375
train loss:  0.5178540945053101
train gradient:  0.1332970574999101
iteration : 102
train acc:  0.71875
train loss:  0.4992378354072571
train gradient:  0.1658132812696843
iteration : 103
train acc:  0.7421875
train loss:  0.4560885429382324
train gradient:  0.11557650076489522
iteration : 104
train acc:  0.6953125
train loss:  0.5079152584075928
train gradient:  0.1541652955787514
iteration : 105
train acc:  0.8046875
train loss:  0.45664650201797485
train gradient:  0.1415465646399578
iteration : 106
train acc:  0.703125
train loss:  0.4996057152748108
train gradient:  0.154298657391706
iteration : 107
train acc:  0.7734375
train loss:  0.44474560022354126
train gradient:  0.10724596681102544
iteration : 108
train acc:  0.6953125
train loss:  0.5027436017990112
train gradient:  0.13190112673447074
iteration : 109
train acc:  0.78125
train loss:  0.43064308166503906
train gradient:  0.08422916069808352
iteration : 110
train acc:  0.75
train loss:  0.5178255438804626
train gradient:  0.15444613680499977
iteration : 111
train acc:  0.7265625
train loss:  0.5095635652542114
train gradient:  0.1310312298198853
iteration : 112
train acc:  0.8203125
train loss:  0.40345627069473267
train gradient:  0.11378701130232263
iteration : 113
train acc:  0.734375
train loss:  0.4669128954410553
train gradient:  0.12079265533311795
iteration : 114
train acc:  0.7109375
train loss:  0.5154334306716919
train gradient:  0.10448584755527833
iteration : 115
train acc:  0.7890625
train loss:  0.47741255164146423
train gradient:  0.10549275792296726
iteration : 116
train acc:  0.796875
train loss:  0.4279348850250244
train gradient:  0.09649881414280946
iteration : 117
train acc:  0.7734375
train loss:  0.4499956965446472
train gradient:  0.11713555213548296
iteration : 118
train acc:  0.796875
train loss:  0.46165236830711365
train gradient:  0.1309878512899567
iteration : 119
train acc:  0.7421875
train loss:  0.46756497025489807
train gradient:  0.10777035884277757
iteration : 120
train acc:  0.7109375
train loss:  0.5094624757766724
train gradient:  0.14247933615796565
iteration : 121
train acc:  0.7421875
train loss:  0.5067519545555115
train gradient:  0.11872528483899585
iteration : 122
train acc:  0.7890625
train loss:  0.46489018201828003
train gradient:  0.11447199775344108
iteration : 123
train acc:  0.8046875
train loss:  0.42399758100509644
train gradient:  0.09363627013175507
iteration : 124
train acc:  0.7109375
train loss:  0.47512567043304443
train gradient:  0.10777614366647431
iteration : 125
train acc:  0.6953125
train loss:  0.5156815648078918
train gradient:  0.14687031671333872
iteration : 126
train acc:  0.765625
train loss:  0.4734884202480316
train gradient:  0.12782228073022656
iteration : 127
train acc:  0.71875
train loss:  0.490543007850647
train gradient:  0.12339401973608781
iteration : 128
train acc:  0.78125
train loss:  0.4256802797317505
train gradient:  0.09525252454523443
iteration : 129
train acc:  0.75
train loss:  0.46609967947006226
train gradient:  0.14528129061467113
iteration : 130
train acc:  0.671875
train loss:  0.523048996925354
train gradient:  0.12944394603881101
iteration : 131
train acc:  0.75
train loss:  0.518256664276123
train gradient:  0.1268758388203636
iteration : 132
train acc:  0.765625
train loss:  0.45223894715309143
train gradient:  0.1135072938257507
iteration : 133
train acc:  0.7734375
train loss:  0.4589889645576477
train gradient:  0.0976862883308842
iteration : 134
train acc:  0.8203125
train loss:  0.4189295768737793
train gradient:  0.08606701453765336
iteration : 135
train acc:  0.71875
train loss:  0.48597025871276855
train gradient:  0.12430943130304732
iteration : 136
train acc:  0.75
train loss:  0.4928831458091736
train gradient:  0.15038805116276677
iteration : 137
train acc:  0.796875
train loss:  0.4701252579689026
train gradient:  0.11980952106960258
iteration : 138
train acc:  0.734375
train loss:  0.48812463879585266
train gradient:  0.1395257022945476
iteration : 139
train acc:  0.734375
train loss:  0.4839734435081482
train gradient:  0.09929913909526417
iteration : 140
train acc:  0.7421875
train loss:  0.49544692039489746
train gradient:  0.118548100309104
iteration : 141
train acc:  0.78125
train loss:  0.4821223318576813
train gradient:  0.11445153159418389
iteration : 142
train acc:  0.7421875
train loss:  0.5179785490036011
train gradient:  0.12593040259364893
iteration : 143
train acc:  0.71875
train loss:  0.480002760887146
train gradient:  0.106891093088094
iteration : 144
train acc:  0.7890625
train loss:  0.45798712968826294
train gradient:  0.11672314996882827
iteration : 145
train acc:  0.7109375
train loss:  0.5324610471725464
train gradient:  0.12245083975381557
iteration : 146
train acc:  0.7109375
train loss:  0.5028694868087769
train gradient:  0.14594861304380374
iteration : 147
train acc:  0.7734375
train loss:  0.4736398458480835
train gradient:  0.11125253636927077
iteration : 148
train acc:  0.71875
train loss:  0.517398476600647
train gradient:  0.14794222171371507
iteration : 149
train acc:  0.71875
train loss:  0.4786776900291443
train gradient:  0.1139065737328919
iteration : 150
train acc:  0.7109375
train loss:  0.49320435523986816
train gradient:  0.16527332180364845
iteration : 151
train acc:  0.734375
train loss:  0.5067034959793091
train gradient:  0.11078530310951686
iteration : 152
train acc:  0.7578125
train loss:  0.45526760816574097
train gradient:  0.12689349841990274
iteration : 153
train acc:  0.7734375
train loss:  0.4433429539203644
train gradient:  0.08851311852455404
iteration : 154
train acc:  0.703125
train loss:  0.526394248008728
train gradient:  0.1406750287695362
iteration : 155
train acc:  0.71875
train loss:  0.4916180670261383
train gradient:  0.12666493200404683
iteration : 156
train acc:  0.7578125
train loss:  0.47253650426864624
train gradient:  0.13651789211212964
iteration : 157
train acc:  0.7265625
train loss:  0.4996218681335449
train gradient:  0.11751407352567249
iteration : 158
train acc:  0.8359375
train loss:  0.4539681077003479
train gradient:  0.09478467462409194
iteration : 159
train acc:  0.7265625
train loss:  0.46224114298820496
train gradient:  0.10969719086698528
iteration : 160
train acc:  0.7421875
train loss:  0.46124351024627686
train gradient:  0.10556257243694607
iteration : 161
train acc:  0.7109375
train loss:  0.5304790735244751
train gradient:  0.13287635237309037
iteration : 162
train acc:  0.75
train loss:  0.43665170669555664
train gradient:  0.11881552358594308
iteration : 163
train acc:  0.78125
train loss:  0.47055956721305847
train gradient:  0.15194967824704156
iteration : 164
train acc:  0.6796875
train loss:  0.5462431311607361
train gradient:  0.14515563288983635
iteration : 165
train acc:  0.71875
train loss:  0.5306724309921265
train gradient:  0.13632333479358724
iteration : 166
train acc:  0.796875
train loss:  0.44274601340293884
train gradient:  0.12118424247375938
iteration : 167
train acc:  0.78125
train loss:  0.5000255703926086
train gradient:  0.16152782075612865
iteration : 168
train acc:  0.8046875
train loss:  0.4212029278278351
train gradient:  0.0989195889836128
iteration : 169
train acc:  0.828125
train loss:  0.39699339866638184
train gradient:  0.08312586859913683
iteration : 170
train acc:  0.75
train loss:  0.49432891607284546
train gradient:  0.12117808786662781
iteration : 171
train acc:  0.78125
train loss:  0.47012031078338623
train gradient:  0.09655059235856901
iteration : 172
train acc:  0.7734375
train loss:  0.44698366522789
train gradient:  0.11398212848816294
iteration : 173
train acc:  0.6953125
train loss:  0.5165960788726807
train gradient:  0.11495494006345798
iteration : 174
train acc:  0.734375
train loss:  0.49957075715065
train gradient:  0.12738074689928833
iteration : 175
train acc:  0.703125
train loss:  0.4703983962535858
train gradient:  0.10209223787076568
iteration : 176
train acc:  0.78125
train loss:  0.4309731721878052
train gradient:  0.10441454790905672
iteration : 177
train acc:  0.78125
train loss:  0.5067301988601685
train gradient:  0.1320401176490953
iteration : 178
train acc:  0.78125
train loss:  0.4651971459388733
train gradient:  0.09874828959927164
iteration : 179
train acc:  0.734375
train loss:  0.5200484991073608
train gradient:  0.13282624267872226
iteration : 180
train acc:  0.78125
train loss:  0.42645883560180664
train gradient:  0.10649444324222222
iteration : 181
train acc:  0.71875
train loss:  0.462552011013031
train gradient:  0.12664105621518457
iteration : 182
train acc:  0.828125
train loss:  0.42372456192970276
train gradient:  0.09249692141811325
iteration : 183
train acc:  0.6953125
train loss:  0.6039879322052002
train gradient:  0.26253695621021017
iteration : 184
train acc:  0.6484375
train loss:  0.5632518529891968
train gradient:  0.16234889711731182
iteration : 185
train acc:  0.7890625
train loss:  0.4616309106349945
train gradient:  0.12066012021425167
iteration : 186
train acc:  0.765625
train loss:  0.43778207898139954
train gradient:  0.1189912644287415
iteration : 187
train acc:  0.796875
train loss:  0.45552825927734375
train gradient:  0.12424138107834128
iteration : 188
train acc:  0.8125
train loss:  0.406780868768692
train gradient:  0.08379461275609243
iteration : 189
train acc:  0.7421875
train loss:  0.5138015747070312
train gradient:  0.11856778914527219
iteration : 190
train acc:  0.7734375
train loss:  0.4526432454586029
train gradient:  0.09322052596790346
iteration : 191
train acc:  0.796875
train loss:  0.42782002687454224
train gradient:  0.10841329742229915
iteration : 192
train acc:  0.765625
train loss:  0.4885733127593994
train gradient:  0.12522272048008862
iteration : 193
train acc:  0.71875
train loss:  0.49761348962783813
train gradient:  0.11994711583523478
iteration : 194
train acc:  0.7265625
train loss:  0.5032672882080078
train gradient:  0.1218146752489129
iteration : 195
train acc:  0.8046875
train loss:  0.44692128896713257
train gradient:  0.10433904511672391
iteration : 196
train acc:  0.7890625
train loss:  0.41011694073677063
train gradient:  0.09701377985205484
iteration : 197
train acc:  0.7265625
train loss:  0.4591050148010254
train gradient:  0.10504650938731881
iteration : 198
train acc:  0.640625
train loss:  0.5722719430923462
train gradient:  0.14752894870173777
iteration : 199
train acc:  0.7421875
train loss:  0.448383629322052
train gradient:  0.107763728253509
iteration : 200
train acc:  0.734375
train loss:  0.47638723254203796
train gradient:  0.10845290621361485
iteration : 201
train acc:  0.734375
train loss:  0.5319278836250305
train gradient:  0.12136652966301574
iteration : 202
train acc:  0.796875
train loss:  0.47607502341270447
train gradient:  0.15675565290557897
iteration : 203
train acc:  0.6953125
train loss:  0.5204886198043823
train gradient:  0.12903452362007745
iteration : 204
train acc:  0.8046875
train loss:  0.4349387288093567
train gradient:  0.10638517047274425
iteration : 205
train acc:  0.6875
train loss:  0.5721960067749023
train gradient:  0.16558396855826302
iteration : 206
train acc:  0.7578125
train loss:  0.5103274583816528
train gradient:  0.15558251469373552
iteration : 207
train acc:  0.7734375
train loss:  0.43408888578414917
train gradient:  0.0954983590457341
iteration : 208
train acc:  0.8125
train loss:  0.47100451588630676
train gradient:  0.15084700214304447
iteration : 209
train acc:  0.75
train loss:  0.4345545768737793
train gradient:  0.08885005375658893
iteration : 210
train acc:  0.7265625
train loss:  0.5106680393218994
train gradient:  0.133769754758168
iteration : 211
train acc:  0.765625
train loss:  0.41744908690452576
train gradient:  0.09863641488627174
iteration : 212
train acc:  0.7734375
train loss:  0.4596549868583679
train gradient:  0.11732688179554002
iteration : 213
train acc:  0.765625
train loss:  0.4764784574508667
train gradient:  0.12680528583261416
iteration : 214
train acc:  0.75
train loss:  0.48981335759162903
train gradient:  0.12700661952836168
iteration : 215
train acc:  0.7578125
train loss:  0.45687371492385864
train gradient:  0.11862653068023461
iteration : 216
train acc:  0.6796875
train loss:  0.5783937573432922
train gradient:  0.17285839508938927
iteration : 217
train acc:  0.75
train loss:  0.5172099471092224
train gradient:  0.15079359194816339
iteration : 218
train acc:  0.703125
train loss:  0.530632495880127
train gradient:  0.1440109379953366
iteration : 219
train acc:  0.703125
train loss:  0.5718998908996582
train gradient:  0.19218744715824415
iteration : 220
train acc:  0.7890625
train loss:  0.44664597511291504
train gradient:  0.1114409812704306
iteration : 221
train acc:  0.71875
train loss:  0.5198380947113037
train gradient:  0.13305250956540607
iteration : 222
train acc:  0.75
train loss:  0.42510586977005005
train gradient:  0.09016560983654157
iteration : 223
train acc:  0.7109375
train loss:  0.5084645748138428
train gradient:  0.1295517453477465
iteration : 224
train acc:  0.6875
train loss:  0.5395783185958862
train gradient:  0.16903633534442242
iteration : 225
train acc:  0.7109375
train loss:  0.5433200597763062
train gradient:  0.22281974199560908
iteration : 226
train acc:  0.7421875
train loss:  0.48392459750175476
train gradient:  0.11719341974498103
iteration : 227
train acc:  0.734375
train loss:  0.49629878997802734
train gradient:  0.12028672087245719
iteration : 228
train acc:  0.78125
train loss:  0.4629269242286682
train gradient:  0.09341349288446198
iteration : 229
train acc:  0.765625
train loss:  0.45140784978866577
train gradient:  0.11347925762928542
iteration : 230
train acc:  0.7421875
train loss:  0.555014431476593
train gradient:  0.14321132323459795
iteration : 231
train acc:  0.765625
train loss:  0.4480697512626648
train gradient:  0.11225389198566531
iteration : 232
train acc:  0.7265625
train loss:  0.4630989134311676
train gradient:  0.09897888086020601
iteration : 233
train acc:  0.7421875
train loss:  0.5183966159820557
train gradient:  0.15767456135808483
iteration : 234
train acc:  0.78125
train loss:  0.4640982151031494
train gradient:  0.10315891662830783
iteration : 235
train acc:  0.78125
train loss:  0.4459537863731384
train gradient:  0.09281464708755391
iteration : 236
train acc:  0.78125
train loss:  0.44465386867523193
train gradient:  0.10204965478665731
iteration : 237
train acc:  0.8046875
train loss:  0.4613155424594879
train gradient:  0.1056844590858193
iteration : 238
train acc:  0.7578125
train loss:  0.4733310639858246
train gradient:  0.12249972208601435
iteration : 239
train acc:  0.8125
train loss:  0.4221069812774658
train gradient:  0.10078568931013891
iteration : 240
train acc:  0.734375
train loss:  0.49631601572036743
train gradient:  0.1330798753070453
iteration : 241
train acc:  0.78125
train loss:  0.4385819435119629
train gradient:  0.12360634916936754
iteration : 242
train acc:  0.734375
train loss:  0.4570794701576233
train gradient:  0.11539966356710145
iteration : 243
train acc:  0.78125
train loss:  0.43473923206329346
train gradient:  0.11297723285369447
iteration : 244
train acc:  0.7109375
train loss:  0.5426727533340454
train gradient:  0.2096653003123261
iteration : 245
train acc:  0.7578125
train loss:  0.45914342999458313
train gradient:  0.10723081560964717
iteration : 246
train acc:  0.71875
train loss:  0.5032514929771423
train gradient:  0.12486970297123647
iteration : 247
train acc:  0.7421875
train loss:  0.48288846015930176
train gradient:  0.12333573531381033
iteration : 248
train acc:  0.734375
train loss:  0.49782198667526245
train gradient:  0.13886631854750375
iteration : 249
train acc:  0.7265625
train loss:  0.5043973326683044
train gradient:  0.1518431953536988
iteration : 250
train acc:  0.78125
train loss:  0.49362292885780334
train gradient:  0.14471712792504826
iteration : 251
train acc:  0.71875
train loss:  0.5220869183540344
train gradient:  0.12861571978714387
iteration : 252
train acc:  0.6640625
train loss:  0.5541192889213562
train gradient:  0.15949736742113452
iteration : 253
train acc:  0.6953125
train loss:  0.5490740537643433
train gradient:  0.14159175869188562
iteration : 254
train acc:  0.78125
train loss:  0.42201441526412964
train gradient:  0.10788299697837518
iteration : 255
train acc:  0.78125
train loss:  0.42717111110687256
train gradient:  0.09364947003499388
iteration : 256
train acc:  0.75
train loss:  0.47200489044189453
train gradient:  0.1307465072135559
iteration : 257
train acc:  0.8125
train loss:  0.41462722420692444
train gradient:  0.08214373582035969
iteration : 258
train acc:  0.8125
train loss:  0.40914973616600037
train gradient:  0.1374828886381115
iteration : 259
train acc:  0.7265625
train loss:  0.48145419359207153
train gradient:  0.109996878881647
iteration : 260
train acc:  0.765625
train loss:  0.4534856379032135
train gradient:  0.10616673235632496
iteration : 261
train acc:  0.7890625
train loss:  0.3782545328140259
train gradient:  0.09477391504805512
iteration : 262
train acc:  0.7578125
train loss:  0.4774553179740906
train gradient:  0.11422567746759335
iteration : 263
train acc:  0.75
train loss:  0.4805912673473358
train gradient:  0.12738074757006057
iteration : 264
train acc:  0.7421875
train loss:  0.4776933789253235
train gradient:  0.1081293679999601
iteration : 265
train acc:  0.7109375
train loss:  0.4676051437854767
train gradient:  0.08945670395347047
iteration : 266
train acc:  0.8203125
train loss:  0.3843720257282257
train gradient:  0.08631384468751552
iteration : 267
train acc:  0.7890625
train loss:  0.4850669503211975
train gradient:  0.1113412407122962
iteration : 268
train acc:  0.71875
train loss:  0.5499201416969299
train gradient:  0.12414698825753648
iteration : 269
train acc:  0.75
train loss:  0.4803028702735901
train gradient:  0.1085071086748733
iteration : 270
train acc:  0.7734375
train loss:  0.4678681492805481
train gradient:  0.12970035152430792
iteration : 271
train acc:  0.71875
train loss:  0.5473790168762207
train gradient:  0.15089632325328198
iteration : 272
train acc:  0.78125
train loss:  0.522580623626709
train gradient:  0.1584056839310318
iteration : 273
train acc:  0.75
train loss:  0.4298153519630432
train gradient:  0.10472103977495714
iteration : 274
train acc:  0.7265625
train loss:  0.5214585065841675
train gradient:  0.16834615258862384
iteration : 275
train acc:  0.7421875
train loss:  0.48062536120414734
train gradient:  0.10830996495680449
iteration : 276
train acc:  0.7578125
train loss:  0.4787015914916992
train gradient:  0.10777303368854799
iteration : 277
train acc:  0.7890625
train loss:  0.4261865019798279
train gradient:  0.07767001959354318
iteration : 278
train acc:  0.7421875
train loss:  0.5197191834449768
train gradient:  0.13724664546611126
iteration : 279
train acc:  0.671875
train loss:  0.549811601638794
train gradient:  0.15674613068882154
iteration : 280
train acc:  0.703125
train loss:  0.5370762944221497
train gradient:  0.11984413275662918
iteration : 281
train acc:  0.71875
train loss:  0.5236738920211792
train gradient:  0.16244320082555347
iteration : 282
train acc:  0.71875
train loss:  0.47932660579681396
train gradient:  0.12779693636569883
iteration : 283
train acc:  0.7109375
train loss:  0.5358884334564209
train gradient:  0.14196822483821636
iteration : 284
train acc:  0.7734375
train loss:  0.41051405668258667
train gradient:  0.13347770703494902
iteration : 285
train acc:  0.7109375
train loss:  0.516250491142273
train gradient:  0.13434618836694134
iteration : 286
train acc:  0.734375
train loss:  0.48628121614456177
train gradient:  0.13335417536966948
iteration : 287
train acc:  0.828125
train loss:  0.3980429768562317
train gradient:  0.09318067334206348
iteration : 288
train acc:  0.7890625
train loss:  0.4614042639732361
train gradient:  0.1041946635033462
iteration : 289
train acc:  0.7578125
train loss:  0.49020087718963623
train gradient:  0.14392013146083787
iteration : 290
train acc:  0.71875
train loss:  0.521343469619751
train gradient:  0.12460788880057855
iteration : 291
train acc:  0.7578125
train loss:  0.477792352437973
train gradient:  0.12111580310696216
iteration : 292
train acc:  0.7578125
train loss:  0.4461546838283539
train gradient:  0.11517965415903968
iteration : 293
train acc:  0.7265625
train loss:  0.5089917778968811
train gradient:  0.09658320341443574
iteration : 294
train acc:  0.7578125
train loss:  0.4313388466835022
train gradient:  0.084789220689746
iteration : 295
train acc:  0.8359375
train loss:  0.36277949810028076
train gradient:  0.07524820653565274
iteration : 296
train acc:  0.7890625
train loss:  0.4389724135398865
train gradient:  0.12262764796863354
iteration : 297
train acc:  0.6953125
train loss:  0.5582999587059021
train gradient:  0.16977438679432916
iteration : 298
train acc:  0.8046875
train loss:  0.42914536595344543
train gradient:  0.09958264913322666
iteration : 299
train acc:  0.7578125
train loss:  0.46805253624916077
train gradient:  0.11824223224452761
iteration : 300
train acc:  0.6953125
train loss:  0.49255043268203735
train gradient:  0.1272626685313327
iteration : 301
train acc:  0.8125
train loss:  0.416519433259964
train gradient:  0.12738710731185052
iteration : 302
train acc:  0.75
train loss:  0.4761562943458557
train gradient:  0.0948814495310496
iteration : 303
train acc:  0.6953125
train loss:  0.49789631366729736
train gradient:  0.12160707548073184
iteration : 304
train acc:  0.78125
train loss:  0.4622882008552551
train gradient:  0.1264259640271797
iteration : 305
train acc:  0.8203125
train loss:  0.4473252594470978
train gradient:  0.1053664033458229
iteration : 306
train acc:  0.7421875
train loss:  0.508028507232666
train gradient:  0.12514579943479873
iteration : 307
train acc:  0.8125
train loss:  0.5117920637130737
train gradient:  0.1316177549025783
iteration : 308
train acc:  0.7734375
train loss:  0.42441925406455994
train gradient:  0.09780581527987554
iteration : 309
train acc:  0.78125
train loss:  0.4386198818683624
train gradient:  0.10003905510676994
iteration : 310
train acc:  0.7265625
train loss:  0.5587639212608337
train gradient:  0.13970817546604816
iteration : 311
train acc:  0.71875
train loss:  0.5193440318107605
train gradient:  0.13005481078303777
iteration : 312
train acc:  0.734375
train loss:  0.5344197750091553
train gradient:  0.13450607175487672
iteration : 313
train acc:  0.75
train loss:  0.47100022435188293
train gradient:  0.10444648676332056
iteration : 314
train acc:  0.78125
train loss:  0.4316848814487457
train gradient:  0.11571648993291259
iteration : 315
train acc:  0.7421875
train loss:  0.5480150580406189
train gradient:  0.13359552599060995
iteration : 316
train acc:  0.78125
train loss:  0.4596497416496277
train gradient:  0.10068813168260494
iteration : 317
train acc:  0.8203125
train loss:  0.4461079239845276
train gradient:  0.10455082640156574
iteration : 318
train acc:  0.8046875
train loss:  0.42265206575393677
train gradient:  0.09478672132231346
iteration : 319
train acc:  0.828125
train loss:  0.4036482572555542
train gradient:  0.08167583827915617
iteration : 320
train acc:  0.7265625
train loss:  0.5434203743934631
train gradient:  0.1381581276556742
iteration : 321
train acc:  0.8515625
train loss:  0.39284640550613403
train gradient:  0.06885777988261987
iteration : 322
train acc:  0.71875
train loss:  0.5183178186416626
train gradient:  0.13159853851880743
iteration : 323
train acc:  0.7578125
train loss:  0.48562225699424744
train gradient:  0.12309269858072823
iteration : 324
train acc:  0.75
train loss:  0.4375298023223877
train gradient:  0.0941214726783496
iteration : 325
train acc:  0.6640625
train loss:  0.5256409645080566
train gradient:  0.14115630113904507
iteration : 326
train acc:  0.78125
train loss:  0.4097938537597656
train gradient:  0.08760047183086513
iteration : 327
train acc:  0.765625
train loss:  0.4395918846130371
train gradient:  0.08342146480041325
iteration : 328
train acc:  0.703125
train loss:  0.5594482421875
train gradient:  0.14849969210706332
iteration : 329
train acc:  0.7578125
train loss:  0.4266985356807709
train gradient:  0.10388517395742503
iteration : 330
train acc:  0.71875
train loss:  0.4713160991668701
train gradient:  0.11645535232808758
iteration : 331
train acc:  0.7109375
train loss:  0.5156506896018982
train gradient:  0.20736169613978686
iteration : 332
train acc:  0.71875
train loss:  0.4904565215110779
train gradient:  0.11143047949128128
iteration : 333
train acc:  0.703125
train loss:  0.5797138214111328
train gradient:  0.18077816267124508
iteration : 334
train acc:  0.6796875
train loss:  0.6070940494537354
train gradient:  0.18079061847489308
iteration : 335
train acc:  0.765625
train loss:  0.42655253410339355
train gradient:  0.10959555027021302
iteration : 336
train acc:  0.78125
train loss:  0.46304845809936523
train gradient:  0.11052592149157457
iteration : 337
train acc:  0.7578125
train loss:  0.4816386103630066
train gradient:  0.14056294187667462
iteration : 338
train acc:  0.78125
train loss:  0.4299512207508087
train gradient:  0.11752453586503311
iteration : 339
train acc:  0.6953125
train loss:  0.4776145815849304
train gradient:  0.1318982885282436
iteration : 340
train acc:  0.7578125
train loss:  0.45940962433815
train gradient:  0.12225374953310061
iteration : 341
train acc:  0.65625
train loss:  0.5501126646995544
train gradient:  0.17004237234551708
iteration : 342
train acc:  0.7421875
train loss:  0.5037505626678467
train gradient:  0.1351246440278135
iteration : 343
train acc:  0.71875
train loss:  0.5119829177856445
train gradient:  0.12277950495693589
iteration : 344
train acc:  0.734375
train loss:  0.4797250032424927
train gradient:  0.13405609770586494
iteration : 345
train acc:  0.734375
train loss:  0.5530660152435303
train gradient:  0.18855330673522924
iteration : 346
train acc:  0.7265625
train loss:  0.48333269357681274
train gradient:  0.1166439736858216
iteration : 347
train acc:  0.7890625
train loss:  0.4652024507522583
train gradient:  0.13583313044839224
iteration : 348
train acc:  0.78125
train loss:  0.4711648225784302
train gradient:  0.12774477093044875
iteration : 349
train acc:  0.734375
train loss:  0.5194119811058044
train gradient:  0.1514874835570511
iteration : 350
train acc:  0.7265625
train loss:  0.5168697834014893
train gradient:  0.14045491841457863
iteration : 351
train acc:  0.765625
train loss:  0.48998990654945374
train gradient:  0.13390458037420905
iteration : 352
train acc:  0.7265625
train loss:  0.47385117411613464
train gradient:  0.10682209816377301
iteration : 353
train acc:  0.7109375
train loss:  0.5731991529464722
train gradient:  0.15883719643078753
iteration : 354
train acc:  0.8125
train loss:  0.4477900266647339
train gradient:  0.11495850843392981
iteration : 355
train acc:  0.796875
train loss:  0.4165550768375397
train gradient:  0.11789094232667652
iteration : 356
train acc:  0.7578125
train loss:  0.49643781781196594
train gradient:  0.12317633593903776
iteration : 357
train acc:  0.7265625
train loss:  0.49250879883766174
train gradient:  0.1180424141465771
iteration : 358
train acc:  0.6953125
train loss:  0.576841413974762
train gradient:  0.20967328795888548
iteration : 359
train acc:  0.7734375
train loss:  0.419378399848938
train gradient:  0.1174964583251147
iteration : 360
train acc:  0.75
train loss:  0.4841654896736145
train gradient:  0.11179511682879793
iteration : 361
train acc:  0.765625
train loss:  0.4730985164642334
train gradient:  0.1352883459894683
iteration : 362
train acc:  0.78125
train loss:  0.428602933883667
train gradient:  0.10245104114884127
iteration : 363
train acc:  0.7578125
train loss:  0.4764818549156189
train gradient:  0.12434479839006186
iteration : 364
train acc:  0.7265625
train loss:  0.5038841962814331
train gradient:  0.11500472248169069
iteration : 365
train acc:  0.7109375
train loss:  0.5152039527893066
train gradient:  0.14470710603846326
iteration : 366
train acc:  0.734375
train loss:  0.46129176020622253
train gradient:  0.14649780203774238
iteration : 367
train acc:  0.78125
train loss:  0.4496336579322815
train gradient:  0.10657934907251126
iteration : 368
train acc:  0.8359375
train loss:  0.3828847110271454
train gradient:  0.07283357011143447
iteration : 369
train acc:  0.71875
train loss:  0.5351275205612183
train gradient:  0.12589872934040297
iteration : 370
train acc:  0.7734375
train loss:  0.41125181317329407
train gradient:  0.10431299336057781
iteration : 371
train acc:  0.734375
train loss:  0.4762423634529114
train gradient:  0.12961008071489283
iteration : 372
train acc:  0.75
train loss:  0.4832538962364197
train gradient:  0.15371765503058465
iteration : 373
train acc:  0.765625
train loss:  0.44839033484458923
train gradient:  0.12329797735771376
iteration : 374
train acc:  0.78125
train loss:  0.467462956905365
train gradient:  0.13476869347417442
iteration : 375
train acc:  0.7421875
train loss:  0.5397878289222717
train gradient:  0.15798251471682773
iteration : 376
train acc:  0.7109375
train loss:  0.4926603138446808
train gradient:  0.1511966459754811
iteration : 377
train acc:  0.8125
train loss:  0.40147829055786133
train gradient:  0.09643659802277481
iteration : 378
train acc:  0.7578125
train loss:  0.43930327892303467
train gradient:  0.09098261390050971
iteration : 379
train acc:  0.6953125
train loss:  0.5402548313140869
train gradient:  0.16504357893410596
iteration : 380
train acc:  0.765625
train loss:  0.4646502137184143
train gradient:  0.09848778874120816
iteration : 381
train acc:  0.7890625
train loss:  0.4253096282482147
train gradient:  0.09283783402642044
iteration : 382
train acc:  0.75
train loss:  0.4861105680465698
train gradient:  0.11183271782281687
iteration : 383
train acc:  0.734375
train loss:  0.50214022397995
train gradient:  0.13247338533683278
iteration : 384
train acc:  0.71875
train loss:  0.44500747323036194
train gradient:  0.11628808873058134
iteration : 385
train acc:  0.7109375
train loss:  0.5181615352630615
train gradient:  0.1270739370851276
iteration : 386
train acc:  0.734375
train loss:  0.4991884231567383
train gradient:  0.14850836370961223
iteration : 387
train acc:  0.734375
train loss:  0.530651867389679
train gradient:  0.13133429920712317
iteration : 388
train acc:  0.796875
train loss:  0.49375784397125244
train gradient:  0.12479863978042045
iteration : 389
train acc:  0.75
train loss:  0.4429391622543335
train gradient:  0.10202780755963035
iteration : 390
train acc:  0.765625
train loss:  0.49069744348526
train gradient:  0.12060937021910902
iteration : 391
train acc:  0.6640625
train loss:  0.6258914470672607
train gradient:  0.19566570983961917
iteration : 392
train acc:  0.7578125
train loss:  0.46787920594215393
train gradient:  0.09666939097525656
iteration : 393
train acc:  0.7265625
train loss:  0.47250354290008545
train gradient:  0.1043030246022343
iteration : 394
train acc:  0.84375
train loss:  0.3978056311607361
train gradient:  0.08747270100568606
iteration : 395
train acc:  0.7265625
train loss:  0.5002473592758179
train gradient:  0.12634579471441826
iteration : 396
train acc:  0.78125
train loss:  0.47577744722366333
train gradient:  0.1077168643376339
iteration : 397
train acc:  0.703125
train loss:  0.5337016582489014
train gradient:  0.13458559479518417
iteration : 398
train acc:  0.6953125
train loss:  0.4963243007659912
train gradient:  0.10283803326843906
iteration : 399
train acc:  0.71875
train loss:  0.4643241763114929
train gradient:  0.11032525657236056
iteration : 400
train acc:  0.7578125
train loss:  0.4702783524990082
train gradient:  0.10660714720979987
iteration : 401
train acc:  0.6953125
train loss:  0.597845196723938
train gradient:  0.18208550730029588
iteration : 402
train acc:  0.7578125
train loss:  0.4968469440937042
train gradient:  0.16149720939604678
iteration : 403
train acc:  0.828125
train loss:  0.3868646025657654
train gradient:  0.08354410843559643
iteration : 404
train acc:  0.65625
train loss:  0.5961024761199951
train gradient:  0.1528743668590828
iteration : 405
train acc:  0.71875
train loss:  0.4856443405151367
train gradient:  0.1152694248658761
iteration : 406
train acc:  0.71875
train loss:  0.528889000415802
train gradient:  0.14368311684834517
iteration : 407
train acc:  0.734375
train loss:  0.5369000434875488
train gradient:  0.14767865676639547
iteration : 408
train acc:  0.7578125
train loss:  0.4612971544265747
train gradient:  0.1250931139814368
iteration : 409
train acc:  0.7421875
train loss:  0.5167109966278076
train gradient:  0.1286019261556566
iteration : 410
train acc:  0.7578125
train loss:  0.5340685844421387
train gradient:  0.17868917095075307
iteration : 411
train acc:  0.7734375
train loss:  0.4646114706993103
train gradient:  0.1145165833188335
iteration : 412
train acc:  0.7109375
train loss:  0.5303256511688232
train gradient:  0.1298843294855479
iteration : 413
train acc:  0.75
train loss:  0.4632970094680786
train gradient:  0.1132990619711319
iteration : 414
train acc:  0.7578125
train loss:  0.46053582429885864
train gradient:  0.1356947318661094
iteration : 415
train acc:  0.8203125
train loss:  0.4043240547180176
train gradient:  0.08791407825635668
iteration : 416
train acc:  0.6953125
train loss:  0.5559178590774536
train gradient:  0.15587482123912655
iteration : 417
train acc:  0.828125
train loss:  0.3999367654323578
train gradient:  0.06939923860704382
iteration : 418
train acc:  0.78125
train loss:  0.4606003165245056
train gradient:  0.12478347597758005
iteration : 419
train acc:  0.75
train loss:  0.46394696831703186
train gradient:  0.10954059253593262
iteration : 420
train acc:  0.796875
train loss:  0.4169365167617798
train gradient:  0.10756718714862544
iteration : 421
train acc:  0.6640625
train loss:  0.556451678276062
train gradient:  0.14429256650978933
iteration : 422
train acc:  0.734375
train loss:  0.51203453540802
train gradient:  0.13180202691612186
iteration : 423
train acc:  0.7265625
train loss:  0.5241931676864624
train gradient:  0.15032734986878
iteration : 424
train acc:  0.7421875
train loss:  0.539489209651947
train gradient:  0.13378743795234732
iteration : 425
train acc:  0.78125
train loss:  0.41499045491218567
train gradient:  0.0897510777094446
iteration : 426
train acc:  0.7421875
train loss:  0.5289413928985596
train gradient:  0.11373505127070538
iteration : 427
train acc:  0.7421875
train loss:  0.46234849095344543
train gradient:  0.11157355894395211
iteration : 428
train acc:  0.78125
train loss:  0.5251045227050781
train gradient:  0.14468619006109912
iteration : 429
train acc:  0.765625
train loss:  0.4614306390285492
train gradient:  0.09397952832814699
iteration : 430
train acc:  0.703125
train loss:  0.6076750159263611
train gradient:  0.18426889005584554
iteration : 431
train acc:  0.7734375
train loss:  0.3931550979614258
train gradient:  0.09396625044699959
iteration : 432
train acc:  0.75
train loss:  0.49236035346984863
train gradient:  0.15450500816629242
iteration : 433
train acc:  0.7109375
train loss:  0.5532126426696777
train gradient:  0.13813998147193257
iteration : 434
train acc:  0.6796875
train loss:  0.513367235660553
train gradient:  0.11528226587007924
iteration : 435
train acc:  0.7578125
train loss:  0.48117226362228394
train gradient:  0.12878262400148444
iteration : 436
train acc:  0.7109375
train loss:  0.5328226089477539
train gradient:  0.14059174159491344
iteration : 437
train acc:  0.71875
train loss:  0.4883593022823334
train gradient:  0.14095223272720586
iteration : 438
train acc:  0.6953125
train loss:  0.5345209836959839
train gradient:  0.1374480038699315
iteration : 439
train acc:  0.7734375
train loss:  0.48111778497695923
train gradient:  0.11917507028861989
iteration : 440
train acc:  0.7578125
train loss:  0.4161069691181183
train gradient:  0.0926465799726145
iteration : 441
train acc:  0.8046875
train loss:  0.4374563694000244
train gradient:  0.14685749679589066
iteration : 442
train acc:  0.8046875
train loss:  0.3945853114128113
train gradient:  0.07898001113587143
iteration : 443
train acc:  0.71875
train loss:  0.4938080906867981
train gradient:  0.09812528717883666
iteration : 444
train acc:  0.7265625
train loss:  0.5295531749725342
train gradient:  0.15193663714159428
iteration : 445
train acc:  0.734375
train loss:  0.4968397617340088
train gradient:  0.10711782643519345
iteration : 446
train acc:  0.6875
train loss:  0.5839948654174805
train gradient:  0.19566826536810372
iteration : 447
train acc:  0.7890625
train loss:  0.4233854413032532
train gradient:  0.10040976911183706
iteration : 448
train acc:  0.734375
train loss:  0.47631949186325073
train gradient:  0.1004196271933819
iteration : 449
train acc:  0.7265625
train loss:  0.5174822807312012
train gradient:  0.13884209086030952
iteration : 450
train acc:  0.7734375
train loss:  0.4348669946193695
train gradient:  0.09669348060734262
iteration : 451
train acc:  0.75
train loss:  0.42995625734329224
train gradient:  0.08610405975068194
iteration : 452
train acc:  0.65625
train loss:  0.5709088444709778
train gradient:  0.14567344642339525
iteration : 453
train acc:  0.75
train loss:  0.4567427933216095
train gradient:  0.11886677465698726
iteration : 454
train acc:  0.796875
train loss:  0.4731821119785309
train gradient:  0.10348911771604646
iteration : 455
train acc:  0.71875
train loss:  0.49489378929138184
train gradient:  0.11464990602030392
iteration : 456
train acc:  0.7890625
train loss:  0.4141443073749542
train gradient:  0.10128722320282854
iteration : 457
train acc:  0.828125
train loss:  0.3808960020542145
train gradient:  0.10044592917992613
iteration : 458
train acc:  0.734375
train loss:  0.44524866342544556
train gradient:  0.1111700648674282
iteration : 459
train acc:  0.7265625
train loss:  0.500596284866333
train gradient:  0.11911724237719239
iteration : 460
train acc:  0.71875
train loss:  0.5277742147445679
train gradient:  0.11769354641461378
iteration : 461
train acc:  0.796875
train loss:  0.46472275257110596
train gradient:  0.09679907582500079
iteration : 462
train acc:  0.71875
train loss:  0.5382983088493347
train gradient:  0.18476547975198096
iteration : 463
train acc:  0.7578125
train loss:  0.48225218057632446
train gradient:  0.15334929083497906
iteration : 464
train acc:  0.765625
train loss:  0.4979800581932068
train gradient:  0.12564650166409305
iteration : 465
train acc:  0.734375
train loss:  0.502508282661438
train gradient:  0.12382233586684295
iteration : 466
train acc:  0.703125
train loss:  0.5292728543281555
train gradient:  0.12485541092580293
iteration : 467
train acc:  0.8046875
train loss:  0.4420139789581299
train gradient:  0.11877240147668931
iteration : 468
train acc:  0.7890625
train loss:  0.4642844498157501
train gradient:  0.1332526281786974
iteration : 469
train acc:  0.78125
train loss:  0.43933677673339844
train gradient:  0.0908936412244816
iteration : 470
train acc:  0.75
train loss:  0.4347928464412689
train gradient:  0.15683134742691557
iteration : 471
train acc:  0.7578125
train loss:  0.4579446315765381
train gradient:  0.1161316525760285
iteration : 472
train acc:  0.6953125
train loss:  0.5858027338981628
train gradient:  0.1702022074341386
iteration : 473
train acc:  0.7578125
train loss:  0.45933544635772705
train gradient:  0.10210782959544426
iteration : 474
train acc:  0.7734375
train loss:  0.45733755826950073
train gradient:  0.1225012150225662
iteration : 475
train acc:  0.8125
train loss:  0.4322904348373413
train gradient:  0.09251601954078886
iteration : 476
train acc:  0.796875
train loss:  0.4463748335838318
train gradient:  0.10911462408634597
iteration : 477
train acc:  0.7421875
train loss:  0.49861449003219604
train gradient:  0.13615341359976957
iteration : 478
train acc:  0.71875
train loss:  0.4951544404029846
train gradient:  0.12233376451166632
iteration : 479
train acc:  0.7578125
train loss:  0.4754185080528259
train gradient:  0.13042498819358894
iteration : 480
train acc:  0.8203125
train loss:  0.4262537360191345
train gradient:  0.1098241605446635
iteration : 481
train acc:  0.78125
train loss:  0.44086790084838867
train gradient:  0.09436052590955268
iteration : 482
train acc:  0.7578125
train loss:  0.5007485747337341
train gradient:  0.11769190120967626
iteration : 483
train acc:  0.78125
train loss:  0.4261699318885803
train gradient:  0.09464727062703293
iteration : 484
train acc:  0.7421875
train loss:  0.5059036612510681
train gradient:  0.12085146769679277
iteration : 485
train acc:  0.7109375
train loss:  0.5868940353393555
train gradient:  0.14479720705260396
iteration : 486
train acc:  0.765625
train loss:  0.46575164794921875
train gradient:  0.09635016468887037
iteration : 487
train acc:  0.78125
train loss:  0.45616787672042847
train gradient:  0.10340787445303383
iteration : 488
train acc:  0.7890625
train loss:  0.39726197719573975
train gradient:  0.09226174584536169
iteration : 489
train acc:  0.7421875
train loss:  0.5190327763557434
train gradient:  0.12674581069587823
iteration : 490
train acc:  0.75
train loss:  0.450334370136261
train gradient:  0.1082985283756984
iteration : 491
train acc:  0.828125
train loss:  0.3833332657814026
train gradient:  0.07622160215711242
iteration : 492
train acc:  0.75
train loss:  0.45218127965927124
train gradient:  0.11407470891344754
iteration : 493
train acc:  0.7578125
train loss:  0.48734527826309204
train gradient:  0.11301620563608847
iteration : 494
train acc:  0.703125
train loss:  0.49396586418151855
train gradient:  0.13159138337502702
iteration : 495
train acc:  0.765625
train loss:  0.4851020574569702
train gradient:  0.10574849771536597
iteration : 496
train acc:  0.6875
train loss:  0.4842246472835541
train gradient:  0.13110076789969527
iteration : 497
train acc:  0.75
train loss:  0.5066714882850647
train gradient:  0.11141138509010642
iteration : 498
train acc:  0.7890625
train loss:  0.43273890018463135
train gradient:  0.11328429197164558
iteration : 499
train acc:  0.75
train loss:  0.5382453203201294
train gradient:  0.153969105761885
iteration : 500
train acc:  0.71875
train loss:  0.4978458881378174
train gradient:  0.132396002421827
iteration : 501
train acc:  0.6640625
train loss:  0.5795934200286865
train gradient:  0.16751180868481969
iteration : 502
train acc:  0.7890625
train loss:  0.3913346827030182
train gradient:  0.07296252971843613
iteration : 503
train acc:  0.7578125
train loss:  0.5216755270957947
train gradient:  0.12066866126463295
iteration : 504
train acc:  0.734375
train loss:  0.48474550247192383
train gradient:  0.14510074387136107
iteration : 505
train acc:  0.7578125
train loss:  0.4926031231880188
train gradient:  0.14852428232433573
iteration : 506
train acc:  0.796875
train loss:  0.4311765432357788
train gradient:  0.09405584524018451
iteration : 507
train acc:  0.71875
train loss:  0.5091301202774048
train gradient:  0.137413306882041
iteration : 508
train acc:  0.7578125
train loss:  0.4887405335903168
train gradient:  0.10629006816525728
iteration : 509
train acc:  0.7578125
train loss:  0.4506528079509735
train gradient:  0.10545104725113313
iteration : 510
train acc:  0.7578125
train loss:  0.441832959651947
train gradient:  0.11141924627593322
iteration : 511
train acc:  0.7421875
train loss:  0.4776279330253601
train gradient:  0.10766376949530061
iteration : 512
train acc:  0.7109375
train loss:  0.5559178590774536
train gradient:  0.15463186051873434
iteration : 513
train acc:  0.6875
train loss:  0.5296812653541565
train gradient:  0.15100435152011588
iteration : 514
train acc:  0.7421875
train loss:  0.5115573406219482
train gradient:  0.10205202295790834
iteration : 515
train acc:  0.7421875
train loss:  0.5090275406837463
train gradient:  0.11714395616609233
iteration : 516
train acc:  0.703125
train loss:  0.5192712545394897
train gradient:  0.15154285201373333
iteration : 517
train acc:  0.7109375
train loss:  0.504829466342926
train gradient:  0.1219738765095765
iteration : 518
train acc:  0.65625
train loss:  0.5199568271636963
train gradient:  0.1825539888455222
iteration : 519
train acc:  0.75
train loss:  0.45144009590148926
train gradient:  0.1156862319980995
iteration : 520
train acc:  0.84375
train loss:  0.35866373777389526
train gradient:  0.07865243041641613
iteration : 521
train acc:  0.71875
train loss:  0.5603167414665222
train gradient:  0.18184119080152478
iteration : 522
train acc:  0.671875
train loss:  0.5155539512634277
train gradient:  0.1321794584768979
iteration : 523
train acc:  0.8125
train loss:  0.46809422969818115
train gradient:  0.12617790385760538
iteration : 524
train acc:  0.7578125
train loss:  0.45928090810775757
train gradient:  0.11431943154062973
iteration : 525
train acc:  0.7421875
train loss:  0.5100612640380859
train gradient:  0.1368073606357668
iteration : 526
train acc:  0.703125
train loss:  0.5016981363296509
train gradient:  0.15637148194554934
iteration : 527
train acc:  0.7578125
train loss:  0.46821412444114685
train gradient:  0.10845836161560586
iteration : 528
train acc:  0.7734375
train loss:  0.43919503688812256
train gradient:  0.10456784062503847
iteration : 529
train acc:  0.7890625
train loss:  0.46350300312042236
train gradient:  0.0944587160448966
iteration : 530
train acc:  0.7890625
train loss:  0.40468794107437134
train gradient:  0.10154869940232339
iteration : 531
train acc:  0.671875
train loss:  0.5471131801605225
train gradient:  0.12881338387544994
iteration : 532
train acc:  0.7734375
train loss:  0.4419240951538086
train gradient:  0.12060113477346696
iteration : 533
train acc:  0.78125
train loss:  0.4745293855667114
train gradient:  0.16459368240074929
iteration : 534
train acc:  0.734375
train loss:  0.5001120567321777
train gradient:  0.12879920655412452
iteration : 535
train acc:  0.75
train loss:  0.4775490164756775
train gradient:  0.12111926368130146
iteration : 536
train acc:  0.765625
train loss:  0.4836210012435913
train gradient:  0.11955594411558715
iteration : 537
train acc:  0.703125
train loss:  0.5545405149459839
train gradient:  0.1304445253801354
iteration : 538
train acc:  0.71875
train loss:  0.5106961727142334
train gradient:  0.12378720739765822
iteration : 539
train acc:  0.6484375
train loss:  0.5594365000724792
train gradient:  0.1639379970159593
iteration : 540
train acc:  0.7421875
train loss:  0.5081310868263245
train gradient:  0.1195750569866136
iteration : 541
train acc:  0.75
train loss:  0.44460612535476685
train gradient:  0.10919119014009507
iteration : 542
train acc:  0.859375
train loss:  0.37659066915512085
train gradient:  0.08385271169867298
iteration : 543
train acc:  0.703125
train loss:  0.537095308303833
train gradient:  0.15924006999421086
iteration : 544
train acc:  0.7421875
train loss:  0.4762800931930542
train gradient:  0.1317445087608403
iteration : 545
train acc:  0.75
train loss:  0.454636812210083
train gradient:  0.09619382306495775
iteration : 546
train acc:  0.78125
train loss:  0.41609621047973633
train gradient:  0.09840999803983622
iteration : 547
train acc:  0.7421875
train loss:  0.5185555815696716
train gradient:  0.14408080211420687
iteration : 548
train acc:  0.7109375
train loss:  0.5327356457710266
train gradient:  0.17161827792082562
iteration : 549
train acc:  0.7421875
train loss:  0.4831644594669342
train gradient:  0.11715822617316858
iteration : 550
train acc:  0.7421875
train loss:  0.49397653341293335
train gradient:  0.09288232480850812
iteration : 551
train acc:  0.75
train loss:  0.5024742484092712
train gradient:  0.1503111888255501
iteration : 552
train acc:  0.6953125
train loss:  0.5524089932441711
train gradient:  0.16390448401130567
iteration : 553
train acc:  0.796875
train loss:  0.445214182138443
train gradient:  0.09184055653135391
iteration : 554
train acc:  0.8046875
train loss:  0.40146100521087646
train gradient:  0.10771459222742726
iteration : 555
train acc:  0.7578125
train loss:  0.5080980658531189
train gradient:  0.14767797100006222
iteration : 556
train acc:  0.765625
train loss:  0.44592052698135376
train gradient:  0.11075781076235368
iteration : 557
train acc:  0.6640625
train loss:  0.5849719643592834
train gradient:  0.16967399357683144
iteration : 558
train acc:  0.71875
train loss:  0.4889463186264038
train gradient:  0.11318260905639183
iteration : 559
train acc:  0.8125
train loss:  0.4015917181968689
train gradient:  0.09005922523807407
iteration : 560
train acc:  0.75
train loss:  0.5190620422363281
train gradient:  0.1504397933008102
iteration : 561
train acc:  0.7890625
train loss:  0.44953715801239014
train gradient:  0.10282325651625
iteration : 562
train acc:  0.671875
train loss:  0.5145555734634399
train gradient:  0.14518596894688468
iteration : 563
train acc:  0.78125
train loss:  0.46254751086235046
train gradient:  0.12789839641215084
iteration : 564
train acc:  0.78125
train loss:  0.4448457360267639
train gradient:  0.09659771069670199
iteration : 565
train acc:  0.765625
train loss:  0.4824598431587219
train gradient:  0.14554089147565186
iteration : 566
train acc:  0.671875
train loss:  0.5550565123558044
train gradient:  0.15680905480960375
iteration : 567
train acc:  0.734375
train loss:  0.4822440445423126
train gradient:  0.10988002867190247
iteration : 568
train acc:  0.7421875
train loss:  0.5168020725250244
train gradient:  0.12684704115692721
iteration : 569
train acc:  0.7578125
train loss:  0.5251545906066895
train gradient:  0.14057347403994894
iteration : 570
train acc:  0.71875
train loss:  0.6031460762023926
train gradient:  0.18850600480467583
iteration : 571
train acc:  0.765625
train loss:  0.4481293559074402
train gradient:  0.1007190928111538
iteration : 572
train acc:  0.6875
train loss:  0.5141922235488892
train gradient:  0.12380879480600021
iteration : 573
train acc:  0.765625
train loss:  0.49507105350494385
train gradient:  0.11509638007563684
iteration : 574
train acc:  0.78125
train loss:  0.4482763409614563
train gradient:  0.10114395664938845
iteration : 575
train acc:  0.796875
train loss:  0.3929665684700012
train gradient:  0.09194671033743061
iteration : 576
train acc:  0.75
train loss:  0.4378395974636078
train gradient:  0.13234657920388096
iteration : 577
train acc:  0.7890625
train loss:  0.48183831572532654
train gradient:  0.12472200412704468
iteration : 578
train acc:  0.859375
train loss:  0.38924506306648254
train gradient:  0.08473878613084748
iteration : 579
train acc:  0.796875
train loss:  0.4217594861984253
train gradient:  0.09758671741330029
iteration : 580
train acc:  0.75
train loss:  0.4556598663330078
train gradient:  0.1020817882584757
iteration : 581
train acc:  0.8203125
train loss:  0.41325610876083374
train gradient:  0.08149759348704452
iteration : 582
train acc:  0.71875
train loss:  0.5171647071838379
train gradient:  0.15391324129340794
iteration : 583
train acc:  0.796875
train loss:  0.3961185812950134
train gradient:  0.08161276251204921
iteration : 584
train acc:  0.6484375
train loss:  0.6253383159637451
train gradient:  0.19992577274344708
iteration : 585
train acc:  0.8203125
train loss:  0.40474405884742737
train gradient:  0.08881233125773401
iteration : 586
train acc:  0.6796875
train loss:  0.5556449890136719
train gradient:  0.1465875771926774
iteration : 587
train acc:  0.78125
train loss:  0.4858795702457428
train gradient:  0.11684199742697872
iteration : 588
train acc:  0.75
train loss:  0.5031992793083191
train gradient:  0.12149338142899242
iteration : 589
train acc:  0.8125
train loss:  0.44526442885398865
train gradient:  0.11080855877754728
iteration : 590
train acc:  0.796875
train loss:  0.4428893029689789
train gradient:  0.08772177855962439
iteration : 591
train acc:  0.71875
train loss:  0.5282082557678223
train gradient:  0.14411435289777275
iteration : 592
train acc:  0.765625
train loss:  0.5243706703186035
train gradient:  0.12872953051514135
iteration : 593
train acc:  0.7890625
train loss:  0.5198221802711487
train gradient:  0.1687236178429493
iteration : 594
train acc:  0.7890625
train loss:  0.4530896544456482
train gradient:  0.18128468936292796
iteration : 595
train acc:  0.8203125
train loss:  0.48626601696014404
train gradient:  0.1302297039641105
iteration : 596
train acc:  0.828125
train loss:  0.38871556520462036
train gradient:  0.1069517517121716
iteration : 597
train acc:  0.734375
train loss:  0.5001242160797119
train gradient:  0.1074448394140317
iteration : 598
train acc:  0.7734375
train loss:  0.4382881224155426
train gradient:  0.1120597107213662
iteration : 599
train acc:  0.7578125
train loss:  0.4494788646697998
train gradient:  0.11941859763690012
iteration : 600
train acc:  0.7421875
train loss:  0.4885779619216919
train gradient:  0.1231047219192844
iteration : 601
train acc:  0.75
train loss:  0.4761260151863098
train gradient:  0.12375377124091476
iteration : 602
train acc:  0.7109375
train loss:  0.5523865222930908
train gradient:  0.20112982425188147
iteration : 603
train acc:  0.765625
train loss:  0.43942782282829285
train gradient:  0.1201744898365867
iteration : 604
train acc:  0.734375
train loss:  0.4762403964996338
train gradient:  0.10597818556099582
iteration : 605
train acc:  0.7109375
train loss:  0.5465060472488403
train gradient:  0.15494956156182377
iteration : 606
train acc:  0.734375
train loss:  0.47946858406066895
train gradient:  0.11783202696909846
iteration : 607
train acc:  0.75
train loss:  0.4530315101146698
train gradient:  0.10471358196563291
iteration : 608
train acc:  0.8046875
train loss:  0.4105323255062103
train gradient:  0.09309738725999975
iteration : 609
train acc:  0.734375
train loss:  0.4589084982872009
train gradient:  0.13526051584160265
iteration : 610
train acc:  0.8515625
train loss:  0.38729721307754517
train gradient:  0.0902143918712465
iteration : 611
train acc:  0.6875
train loss:  0.6009780168533325
train gradient:  0.19694130401139442
iteration : 612
train acc:  0.75
train loss:  0.5936963558197021
train gradient:  0.19500099010647065
iteration : 613
train acc:  0.71875
train loss:  0.49597644805908203
train gradient:  0.11857053325530673
iteration : 614
train acc:  0.734375
train loss:  0.559093713760376
train gradient:  0.15803384302446788
iteration : 615
train acc:  0.703125
train loss:  0.5013601779937744
train gradient:  0.14302575491040637
iteration : 616
train acc:  0.8125
train loss:  0.4542894959449768
train gradient:  0.12039606189141866
iteration : 617
train acc:  0.78125
train loss:  0.4646833539009094
train gradient:  0.09421858146818632
iteration : 618
train acc:  0.796875
train loss:  0.46218061447143555
train gradient:  0.09569868511271011
iteration : 619
train acc:  0.7421875
train loss:  0.4515330195426941
train gradient:  0.08262389893679907
iteration : 620
train acc:  0.6953125
train loss:  0.5352187752723694
train gradient:  0.12384896236082556
iteration : 621
train acc:  0.7421875
train loss:  0.49582961201667786
train gradient:  0.0999162521712738
iteration : 622
train acc:  0.78125
train loss:  0.4390510618686676
train gradient:  0.10828758327664445
iteration : 623
train acc:  0.7265625
train loss:  0.5002129673957825
train gradient:  0.12625451099986595
iteration : 624
train acc:  0.75
train loss:  0.4593208432197571
train gradient:  0.09279980019477571
iteration : 625
train acc:  0.78125
train loss:  0.44110119342803955
train gradient:  0.10891728343146263
iteration : 626
train acc:  0.796875
train loss:  0.4229101240634918
train gradient:  0.09794596953274037
iteration : 627
train acc:  0.7734375
train loss:  0.4997444152832031
train gradient:  0.11161228288741812
iteration : 628
train acc:  0.765625
train loss:  0.44583481550216675
train gradient:  0.09691002285463313
iteration : 629
train acc:  0.7421875
train loss:  0.4990369975566864
train gradient:  0.11940852698446165
iteration : 630
train acc:  0.7265625
train loss:  0.4936954081058502
train gradient:  0.10315377049916649
iteration : 631
train acc:  0.7421875
train loss:  0.47644978761672974
train gradient:  0.10397749267879869
iteration : 632
train acc:  0.8203125
train loss:  0.4116891324520111
train gradient:  0.1011505322579104
iteration : 633
train acc:  0.7421875
train loss:  0.5258192420005798
train gradient:  0.15352590492492396
iteration : 634
train acc:  0.734375
train loss:  0.5255276560783386
train gradient:  0.11751529454482773
iteration : 635
train acc:  0.7578125
train loss:  0.5183364152908325
train gradient:  0.23824732968814763
iteration : 636
train acc:  0.8203125
train loss:  0.4132525324821472
train gradient:  0.10383829498248308
iteration : 637
train acc:  0.75
train loss:  0.4538206458091736
train gradient:  0.09785718107532151
iteration : 638
train acc:  0.765625
train loss:  0.40002286434173584
train gradient:  0.07791143604279169
iteration : 639
train acc:  0.828125
train loss:  0.4291112422943115
train gradient:  0.10425281111940359
iteration : 640
train acc:  0.7109375
train loss:  0.4948882460594177
train gradient:  0.12111997486349244
iteration : 641
train acc:  0.765625
train loss:  0.48031383752822876
train gradient:  0.10389880087053059
iteration : 642
train acc:  0.71875
train loss:  0.5095219612121582
train gradient:  0.1175836851631153
iteration : 643
train acc:  0.71875
train loss:  0.48966872692108154
train gradient:  0.1110679120243342
iteration : 644
train acc:  0.7890625
train loss:  0.41587626934051514
train gradient:  0.09667844615283801
iteration : 645
train acc:  0.6953125
train loss:  0.5614892244338989
train gradient:  0.16848687030923332
iteration : 646
train acc:  0.7578125
train loss:  0.4522722363471985
train gradient:  0.10621265073291539
iteration : 647
train acc:  0.71875
train loss:  0.5050593614578247
train gradient:  0.12462465373118217
iteration : 648
train acc:  0.765625
train loss:  0.45803752541542053
train gradient:  0.10747730119410766
iteration : 649
train acc:  0.765625
train loss:  0.4738779664039612
train gradient:  0.10883747253923082
iteration : 650
train acc:  0.734375
train loss:  0.5175163745880127
train gradient:  0.11633427780042527
iteration : 651
train acc:  0.78125
train loss:  0.46962738037109375
train gradient:  0.09447526534730263
iteration : 652
train acc:  0.71875
train loss:  0.4979771077632904
train gradient:  0.12406147062354933
iteration : 653
train acc:  0.75
train loss:  0.4847283959388733
train gradient:  0.11611515422284409
iteration : 654
train acc:  0.734375
train loss:  0.538722574710846
train gradient:  0.16863497902119712
iteration : 655
train acc:  0.8125
train loss:  0.4437018036842346
train gradient:  0.10312281702686078
iteration : 656
train acc:  0.6875
train loss:  0.5283191204071045
train gradient:  0.13339838064411175
iteration : 657
train acc:  0.75
train loss:  0.5039373636245728
train gradient:  0.13347548106013554
iteration : 658
train acc:  0.8046875
train loss:  0.4465101659297943
train gradient:  0.10815153384762813
iteration : 659
train acc:  0.78125
train loss:  0.47563812136650085
train gradient:  0.13918310082228225
iteration : 660
train acc:  0.7265625
train loss:  0.4894222021102905
train gradient:  0.14811382320005567
iteration : 661
train acc:  0.796875
train loss:  0.5009326934814453
train gradient:  0.153626425584788
iteration : 662
train acc:  0.75
train loss:  0.45364078879356384
train gradient:  0.10275974072380391
iteration : 663
train acc:  0.765625
train loss:  0.4581649601459503
train gradient:  0.10766863799199895
iteration : 664
train acc:  0.78125
train loss:  0.49485814571380615
train gradient:  0.15569600875298084
iteration : 665
train acc:  0.7421875
train loss:  0.5044622421264648
train gradient:  0.15136414144654922
iteration : 666
train acc:  0.765625
train loss:  0.4878823459148407
train gradient:  0.10660612586956016
iteration : 667
train acc:  0.7421875
train loss:  0.4435015022754669
train gradient:  0.10264170541764384
iteration : 668
train acc:  0.765625
train loss:  0.494327574968338
train gradient:  0.1014636263226446
iteration : 669
train acc:  0.765625
train loss:  0.4965991973876953
train gradient:  0.12389889254042992
iteration : 670
train acc:  0.7265625
train loss:  0.5149231553077698
train gradient:  0.13695630862595476
iteration : 671
train acc:  0.8671875
train loss:  0.3915899395942688
train gradient:  0.09043533897605684
iteration : 672
train acc:  0.796875
train loss:  0.45578256249427795
train gradient:  0.11135158606451234
iteration : 673
train acc:  0.796875
train loss:  0.47218406200408936
train gradient:  0.10797264308122201
iteration : 674
train acc:  0.734375
train loss:  0.4980784058570862
train gradient:  0.14284934135288335
iteration : 675
train acc:  0.703125
train loss:  0.5360100865364075
train gradient:  0.12404244725493625
iteration : 676
train acc:  0.7734375
train loss:  0.4712713956832886
train gradient:  0.1186805026036439
iteration : 677
train acc:  0.7578125
train loss:  0.49897515773773193
train gradient:  0.09845878249127185
iteration : 678
train acc:  0.7109375
train loss:  0.5503580570220947
train gradient:  0.15142956797195223
iteration : 679
train acc:  0.765625
train loss:  0.4756978154182434
train gradient:  0.11342113795878908
iteration : 680
train acc:  0.7109375
train loss:  0.5016460418701172
train gradient:  0.1346104558278492
iteration : 681
train acc:  0.734375
train loss:  0.4905451834201813
train gradient:  0.14083217040542345
iteration : 682
train acc:  0.7578125
train loss:  0.49765726923942566
train gradient:  0.1579701755242257
iteration : 683
train acc:  0.75
train loss:  0.47155967354774475
train gradient:  0.10910662566756946
iteration : 684
train acc:  0.7265625
train loss:  0.5020782351493835
train gradient:  0.1201272213338385
iteration : 685
train acc:  0.7734375
train loss:  0.4219655990600586
train gradient:  0.10287194525926775
iteration : 686
train acc:  0.8203125
train loss:  0.4391764998435974
train gradient:  0.10978707216018609
iteration : 687
train acc:  0.75
train loss:  0.42397207021713257
train gradient:  0.07804824066164062
iteration : 688
train acc:  0.7265625
train loss:  0.5466046929359436
train gradient:  0.136755974128349
iteration : 689
train acc:  0.7265625
train loss:  0.5268673896789551
train gradient:  0.12756047322755834
iteration : 690
train acc:  0.7109375
train loss:  0.5404059290885925
train gradient:  0.15225005974012906
iteration : 691
train acc:  0.734375
train loss:  0.5109226107597351
train gradient:  0.12400999697304378
iteration : 692
train acc:  0.7265625
train loss:  0.5173126459121704
train gradient:  0.13375058529570866
iteration : 693
train acc:  0.75
train loss:  0.47332900762557983
train gradient:  0.1311667476649579
iteration : 694
train acc:  0.7578125
train loss:  0.4558541178703308
train gradient:  0.09480986781402743
iteration : 695
train acc:  0.7265625
train loss:  0.5143082141876221
train gradient:  0.12899783978702176
iteration : 696
train acc:  0.71875
train loss:  0.5459374785423279
train gradient:  0.13329116377551387
iteration : 697
train acc:  0.796875
train loss:  0.4809245467185974
train gradient:  0.10536987502889798
iteration : 698
train acc:  0.71875
train loss:  0.4972020387649536
train gradient:  0.1643904449240307
iteration : 699
train acc:  0.7734375
train loss:  0.4158886671066284
train gradient:  0.12015144689233621
iteration : 700
train acc:  0.7734375
train loss:  0.4480712413787842
train gradient:  0.11501004322121011
iteration : 701
train acc:  0.6953125
train loss:  0.4871024489402771
train gradient:  0.11036030674845668
iteration : 702
train acc:  0.78125
train loss:  0.5084220170974731
train gradient:  0.1317470290088663
iteration : 703
train acc:  0.828125
train loss:  0.38776111602783203
train gradient:  0.08601057401479094
iteration : 704
train acc:  0.7890625
train loss:  0.4310971796512604
train gradient:  0.10544279149142066
iteration : 705
train acc:  0.734375
train loss:  0.44664037227630615
train gradient:  0.11421311076492863
iteration : 706
train acc:  0.734375
train loss:  0.48630398511886597
train gradient:  0.13917946561357009
iteration : 707
train acc:  0.71875
train loss:  0.549468994140625
train gradient:  0.15296246051250795
iteration : 708
train acc:  0.765625
train loss:  0.44079864025115967
train gradient:  0.10824032651105682
iteration : 709
train acc:  0.75
train loss:  0.4277534484863281
train gradient:  0.08383172866659377
iteration : 710
train acc:  0.765625
train loss:  0.47505199909210205
train gradient:  0.10734697199006031
iteration : 711
train acc:  0.7578125
train loss:  0.48230111598968506
train gradient:  0.1302560195185882
iteration : 712
train acc:  0.7421875
train loss:  0.5180260539054871
train gradient:  0.1490916394784987
iteration : 713
train acc:  0.7578125
train loss:  0.4829156696796417
train gradient:  0.13020336389756726
iteration : 714
train acc:  0.7734375
train loss:  0.4134679138660431
train gradient:  0.08449481367550823
iteration : 715
train acc:  0.75
train loss:  0.4782406985759735
train gradient:  0.12714825341386257
iteration : 716
train acc:  0.7421875
train loss:  0.4675324559211731
train gradient:  0.13644986484561414
iteration : 717
train acc:  0.734375
train loss:  0.4650205075740814
train gradient:  0.09497812614894363
iteration : 718
train acc:  0.7734375
train loss:  0.4451497197151184
train gradient:  0.08089268437290577
iteration : 719
train acc:  0.7421875
train loss:  0.4914087653160095
train gradient:  0.1330934075306643
iteration : 720
train acc:  0.71875
train loss:  0.5462529063224792
train gradient:  0.1539304438509232
iteration : 721
train acc:  0.6953125
train loss:  0.5056361556053162
train gradient:  0.15742451369943994
iteration : 722
train acc:  0.8203125
train loss:  0.43706533312797546
train gradient:  0.12009480498472253
iteration : 723
train acc:  0.75
train loss:  0.4755907356739044
train gradient:  0.10060947158060633
iteration : 724
train acc:  0.7265625
train loss:  0.4733431041240692
train gradient:  0.0898241844835903
iteration : 725
train acc:  0.7265625
train loss:  0.5096338987350464
train gradient:  0.13079828255519482
iteration : 726
train acc:  0.7421875
train loss:  0.5022901296615601
train gradient:  0.13647289750055294
iteration : 727
train acc:  0.78125
train loss:  0.4904250502586365
train gradient:  0.10916951659568118
iteration : 728
train acc:  0.7265625
train loss:  0.49908745288848877
train gradient:  0.16073890988580436
iteration : 729
train acc:  0.71875
train loss:  0.542784571647644
train gradient:  0.14724820417297196
iteration : 730
train acc:  0.7578125
train loss:  0.42597028613090515
train gradient:  0.08778457869781714
iteration : 731
train acc:  0.7265625
train loss:  0.5282031297683716
train gradient:  0.13006764354932548
iteration : 732
train acc:  0.78125
train loss:  0.5306192636489868
train gradient:  0.14322296103103913
iteration : 733
train acc:  0.75
train loss:  0.4603852331638336
train gradient:  0.10877145754267263
iteration : 734
train acc:  0.734375
train loss:  0.45695167779922485
train gradient:  0.1016076606305478
iteration : 735
train acc:  0.765625
train loss:  0.448714017868042
train gradient:  0.10730721004480917
iteration : 736
train acc:  0.7421875
train loss:  0.5202144384384155
train gradient:  0.12498284092271968
iteration : 737
train acc:  0.765625
train loss:  0.4527297019958496
train gradient:  0.12008927918103789
iteration : 738
train acc:  0.765625
train loss:  0.4372006952762604
train gradient:  0.08507587495464818
iteration : 739
train acc:  0.8125
train loss:  0.41735124588012695
train gradient:  0.09525605873727994
iteration : 740
train acc:  0.765625
train loss:  0.4788878858089447
train gradient:  0.13196807424760582
iteration : 741
train acc:  0.7109375
train loss:  0.5550986528396606
train gradient:  0.13952101036250958
iteration : 742
train acc:  0.7421875
train loss:  0.5366220474243164
train gradient:  0.1699155609432021
iteration : 743
train acc:  0.7421875
train loss:  0.53752601146698
train gradient:  0.13201865633476725
iteration : 744
train acc:  0.7421875
train loss:  0.5020016431808472
train gradient:  0.1123943863633302
iteration : 745
train acc:  0.75
train loss:  0.4719970226287842
train gradient:  0.09816274503181836
iteration : 746
train acc:  0.7421875
train loss:  0.49077749252319336
train gradient:  0.11658864760252828
iteration : 747
train acc:  0.8046875
train loss:  0.4381953775882721
train gradient:  0.14876938089841943
iteration : 748
train acc:  0.8671875
train loss:  0.37255656719207764
train gradient:  0.07423138249322084
iteration : 749
train acc:  0.75
train loss:  0.49403107166290283
train gradient:  0.11751122046251666
iteration : 750
train acc:  0.71875
train loss:  0.5290057063102722
train gradient:  0.13300966703703743
iteration : 751
train acc:  0.7890625
train loss:  0.4080946743488312
train gradient:  0.09034820446484336
iteration : 752
train acc:  0.796875
train loss:  0.5206630825996399
train gradient:  0.1080095533691754
iteration : 753
train acc:  0.7265625
train loss:  0.4901702404022217
train gradient:  0.10840584163200923
iteration : 754
train acc:  0.7578125
train loss:  0.48264986276626587
train gradient:  0.12992490748769767
iteration : 755
train acc:  0.75
train loss:  0.4833756685256958
train gradient:  0.1055702323993068
iteration : 756
train acc:  0.765625
train loss:  0.4575582444667816
train gradient:  0.10718303590710271
iteration : 757
train acc:  0.75
train loss:  0.4554380774497986
train gradient:  0.10947469682746937
iteration : 758
train acc:  0.7890625
train loss:  0.4515153467655182
train gradient:  0.09979263597347768
iteration : 759
train acc:  0.734375
train loss:  0.5174334049224854
train gradient:  0.1271769246958077
iteration : 760
train acc:  0.78125
train loss:  0.4383346438407898
train gradient:  0.10340857371068214
iteration : 761
train acc:  0.8125
train loss:  0.42772817611694336
train gradient:  0.10755643323444292
iteration : 762
train acc:  0.71875
train loss:  0.4900856614112854
train gradient:  0.12372051685848785
iteration : 763
train acc:  0.75
train loss:  0.48707151412963867
train gradient:  0.11204502604063107
iteration : 764
train acc:  0.765625
train loss:  0.4539044201374054
train gradient:  0.10378422304398313
iteration : 765
train acc:  0.7265625
train loss:  0.5455992817878723
train gradient:  0.15264495901111946
iteration : 766
train acc:  0.6953125
train loss:  0.5154595375061035
train gradient:  0.14463286899472894
iteration : 767
train acc:  0.734375
train loss:  0.48577427864074707
train gradient:  0.13737630567181675
iteration : 768
train acc:  0.7421875
train loss:  0.48662519454956055
train gradient:  0.12972754433601044
iteration : 769
train acc:  0.828125
train loss:  0.41838520765304565
train gradient:  0.10090450976191034
iteration : 770
train acc:  0.7734375
train loss:  0.4541983902454376
train gradient:  0.0970766231971197
iteration : 771
train acc:  0.796875
train loss:  0.5004426836967468
train gradient:  0.11019832223165
iteration : 772
train acc:  0.703125
train loss:  0.4869298040866852
train gradient:  0.12083299637185431
iteration : 773
train acc:  0.734375
train loss:  0.48900291323661804
train gradient:  0.14237036402202508
iteration : 774
train acc:  0.6875
train loss:  0.5632203817367554
train gradient:  0.18272226270535571
iteration : 775
train acc:  0.703125
train loss:  0.5696131587028503
train gradient:  0.17471563944700286
iteration : 776
train acc:  0.71875
train loss:  0.5205576419830322
train gradient:  0.15354322943265514
iteration : 777
train acc:  0.765625
train loss:  0.38929247856140137
train gradient:  0.08250612516314354
iteration : 778
train acc:  0.7578125
train loss:  0.49509456753730774
train gradient:  0.11469765090042922
iteration : 779
train acc:  0.7734375
train loss:  0.492487370967865
train gradient:  0.10965992736576623
iteration : 780
train acc:  0.7421875
train loss:  0.4539186358451843
train gradient:  0.11063280171707135
iteration : 781
train acc:  0.7265625
train loss:  0.5115707516670227
train gradient:  0.1370081785005156
iteration : 782
train acc:  0.8046875
train loss:  0.42090141773223877
train gradient:  0.10130559467387618
iteration : 783
train acc:  0.71875
train loss:  0.6019330024719238
train gradient:  0.15682447443138164
iteration : 784
train acc:  0.703125
train loss:  0.4729243516921997
train gradient:  0.12032944333437487
iteration : 785
train acc:  0.765625
train loss:  0.4861094355583191
train gradient:  0.09941976421418094
iteration : 786
train acc:  0.7578125
train loss:  0.43450191617012024
train gradient:  0.10845219936066271
iteration : 787
train acc:  0.8359375
train loss:  0.4002377986907959
train gradient:  0.07774326508442235
iteration : 788
train acc:  0.7265625
train loss:  0.5872669219970703
train gradient:  0.15250569642186299
iteration : 789
train acc:  0.75
train loss:  0.48412758111953735
train gradient:  0.11227131288327771
iteration : 790
train acc:  0.828125
train loss:  0.42940038442611694
train gradient:  0.0946493038101892
iteration : 791
train acc:  0.7265625
train loss:  0.46231356263160706
train gradient:  0.09936659233635663
iteration : 792
train acc:  0.765625
train loss:  0.4535272419452667
train gradient:  0.10249204338409397
iteration : 793
train acc:  0.6953125
train loss:  0.506893515586853
train gradient:  0.12612989603082125
iteration : 794
train acc:  0.71875
train loss:  0.5083114504814148
train gradient:  0.1472459555011827
iteration : 795
train acc:  0.7734375
train loss:  0.44430217146873474
train gradient:  0.08768621169887506
iteration : 796
train acc:  0.7890625
train loss:  0.42259442806243896
train gradient:  0.089300675800443
iteration : 797
train acc:  0.7421875
train loss:  0.4551194906234741
train gradient:  0.09488235850877191
iteration : 798
train acc:  0.734375
train loss:  0.5250166058540344
train gradient:  0.14554483796178858
iteration : 799
train acc:  0.796875
train loss:  0.4439106583595276
train gradient:  0.10214224977805099
iteration : 800
train acc:  0.75
train loss:  0.46553993225097656
train gradient:  0.1070535936294993
iteration : 801
train acc:  0.765625
train loss:  0.4551219642162323
train gradient:  0.13714709001264697
iteration : 802
train acc:  0.8359375
train loss:  0.43563592433929443
train gradient:  0.11850621446021305
iteration : 803
train acc:  0.75
train loss:  0.5036109685897827
train gradient:  0.12170095374215448
iteration : 804
train acc:  0.8046875
train loss:  0.4477403163909912
train gradient:  0.0913989078110763
iteration : 805
train acc:  0.7578125
train loss:  0.4715520739555359
train gradient:  0.10937548109405837
iteration : 806
train acc:  0.7890625
train loss:  0.4334835708141327
train gradient:  0.10612211767852753
iteration : 807
train acc:  0.75
train loss:  0.5449549555778503
train gradient:  0.16490344675666774
iteration : 808
train acc:  0.71875
train loss:  0.5298662185668945
train gradient:  0.15650616551283325
iteration : 809
train acc:  0.7265625
train loss:  0.485307902097702
train gradient:  0.10888067571686182
iteration : 810
train acc:  0.7421875
train loss:  0.45030561089515686
train gradient:  0.088321624915872
iteration : 811
train acc:  0.7265625
train loss:  0.4657566547393799
train gradient:  0.121042215862492
iteration : 812
train acc:  0.78125
train loss:  0.48862317204475403
train gradient:  0.14115481600693314
iteration : 813
train acc:  0.84375
train loss:  0.39270251989364624
train gradient:  0.07358729792089852
iteration : 814
train acc:  0.7421875
train loss:  0.45126980543136597
train gradient:  0.12229223632617282
iteration : 815
train acc:  0.734375
train loss:  0.49150168895721436
train gradient:  0.14033776163360634
iteration : 816
train acc:  0.7578125
train loss:  0.47019362449645996
train gradient:  0.11502232660689472
iteration : 817
train acc:  0.8046875
train loss:  0.48099392652511597
train gradient:  0.11378876722790958
iteration : 818
train acc:  0.7734375
train loss:  0.4489743709564209
train gradient:  0.12550323008697456
iteration : 819
train acc:  0.75
train loss:  0.4667534828186035
train gradient:  0.11480419409086476
iteration : 820
train acc:  0.7578125
train loss:  0.4460277557373047
train gradient:  0.11958049553004758
iteration : 821
train acc:  0.7734375
train loss:  0.5188941359519958
train gradient:  0.14422670721109837
iteration : 822
train acc:  0.71875
train loss:  0.531758189201355
train gradient:  0.15092473413236585
iteration : 823
train acc:  0.7890625
train loss:  0.43592390418052673
train gradient:  0.09285500954079316
iteration : 824
train acc:  0.78125
train loss:  0.4759248197078705
train gradient:  0.12519906346898757
iteration : 825
train acc:  0.703125
train loss:  0.5662286281585693
train gradient:  0.16335071941762902
iteration : 826
train acc:  0.7578125
train loss:  0.49149471521377563
train gradient:  0.11200761173682842
iteration : 827
train acc:  0.75
train loss:  0.5048055648803711
train gradient:  0.1253359589262888
iteration : 828
train acc:  0.7109375
train loss:  0.5854068398475647
train gradient:  0.18136637680949397
iteration : 829
train acc:  0.7578125
train loss:  0.47064220905303955
train gradient:  0.11324136112284408
iteration : 830
train acc:  0.7109375
train loss:  0.4974205493927002
train gradient:  0.11854261932952628
iteration : 831
train acc:  0.71875
train loss:  0.4746156334877014
train gradient:  0.09795897778108374
iteration : 832
train acc:  0.7734375
train loss:  0.4350108504295349
train gradient:  0.13364351448928957
iteration : 833
train acc:  0.7890625
train loss:  0.4703269302845001
train gradient:  0.14264592702446433
iteration : 834
train acc:  0.7578125
train loss:  0.497232049703598
train gradient:  0.12931167346673636
iteration : 835
train acc:  0.7578125
train loss:  0.4467563033103943
train gradient:  0.1018947695622379
iteration : 836
train acc:  0.765625
train loss:  0.5098496675491333
train gradient:  0.12165710215368268
iteration : 837
train acc:  0.765625
train loss:  0.490809828042984
train gradient:  0.13312843775754352
iteration : 838
train acc:  0.78125
train loss:  0.4356914758682251
train gradient:  0.08738623774149402
iteration : 839
train acc:  0.8125
train loss:  0.4296388626098633
train gradient:  0.10692067741480464
iteration : 840
train acc:  0.75
train loss:  0.43852195143699646
train gradient:  0.10176334856449036
iteration : 841
train acc:  0.6875
train loss:  0.5578461289405823
train gradient:  0.19686959322739644
iteration : 842
train acc:  0.7734375
train loss:  0.4259999990463257
train gradient:  0.09108950454089333
iteration : 843
train acc:  0.7109375
train loss:  0.5166405439376831
train gradient:  0.15984902105357224
iteration : 844
train acc:  0.734375
train loss:  0.4919695556163788
train gradient:  0.12200346886697876
iteration : 845
train acc:  0.7578125
train loss:  0.47188302874565125
train gradient:  0.1203484386829273
iteration : 846
train acc:  0.7578125
train loss:  0.4463685154914856
train gradient:  0.14255185466581288
iteration : 847
train acc:  0.8203125
train loss:  0.4343681335449219
train gradient:  0.11419148796242187
iteration : 848
train acc:  0.78125
train loss:  0.4173841178417206
train gradient:  0.08841821613021439
iteration : 849
train acc:  0.7421875
train loss:  0.49054351449012756
train gradient:  0.1298328067607662
iteration : 850
train acc:  0.703125
train loss:  0.5473208427429199
train gradient:  0.17278778291704397
iteration : 851
train acc:  0.7890625
train loss:  0.42064934968948364
train gradient:  0.10673867784880038
iteration : 852
train acc:  0.75
train loss:  0.49269020557403564
train gradient:  0.10806303967746354
iteration : 853
train acc:  0.828125
train loss:  0.45278581976890564
train gradient:  0.16526952022351424
iteration : 854
train acc:  0.6875
train loss:  0.5953899621963501
train gradient:  0.17840109723760353
iteration : 855
train acc:  0.7265625
train loss:  0.5346876382827759
train gradient:  0.14229733556850438
iteration : 856
train acc:  0.671875
train loss:  0.5324666500091553
train gradient:  0.14446669610255136
iteration : 857
train acc:  0.8046875
train loss:  0.4430599510669708
train gradient:  0.09387665428754913
iteration : 858
train acc:  0.796875
train loss:  0.4517028331756592
train gradient:  0.09384874464578022
iteration : 859
train acc:  0.6796875
train loss:  0.5203889608383179
train gradient:  0.13674647630364686
iteration : 860
train acc:  0.828125
train loss:  0.44617578387260437
train gradient:  0.10765243578753385
iteration : 861
train acc:  0.703125
train loss:  0.5905968546867371
train gradient:  0.17146485110110432
iteration : 862
train acc:  0.78125
train loss:  0.457929790019989
train gradient:  0.12251345846875418
iteration : 863
train acc:  0.7265625
train loss:  0.4499116539955139
train gradient:  0.12116035447932653
iteration : 864
train acc:  0.6875
train loss:  0.5039129853248596
train gradient:  0.13633842146841735
iteration : 865
train acc:  0.8046875
train loss:  0.4136803448200226
train gradient:  0.10210693857909921
iteration : 866
train acc:  0.7265625
train loss:  0.502592921257019
train gradient:  0.13213415491850306
iteration : 867
train acc:  0.8046875
train loss:  0.4395091235637665
train gradient:  0.11438459832244992
iteration : 868
train acc:  0.765625
train loss:  0.492052286863327
train gradient:  0.1390367817029533
iteration : 869
train acc:  0.7421875
train loss:  0.459710031747818
train gradient:  0.11775461982875507
iteration : 870
train acc:  0.796875
train loss:  0.3870256543159485
train gradient:  0.07359158196362216
iteration : 871
train acc:  0.6953125
train loss:  0.526250422000885
train gradient:  0.10525902267011253
iteration : 872
train acc:  0.7421875
train loss:  0.51827073097229
train gradient:  0.12803754777577842
iteration : 873
train acc:  0.71875
train loss:  0.524940013885498
train gradient:  0.14338182364754753
iteration : 874
train acc:  0.8046875
train loss:  0.45535215735435486
train gradient:  0.11460191633382925
iteration : 875
train acc:  0.6640625
train loss:  0.5214235782623291
train gradient:  0.11531331811051275
iteration : 876
train acc:  0.75
train loss:  0.4350169003009796
train gradient:  0.07990427756170797
iteration : 877
train acc:  0.71875
train loss:  0.5172433853149414
train gradient:  0.1340038875849518
iteration : 878
train acc:  0.7265625
train loss:  0.4760392904281616
train gradient:  0.11432821290151417
iteration : 879
train acc:  0.703125
train loss:  0.5068373680114746
train gradient:  0.15074462540678316
iteration : 880
train acc:  0.71875
train loss:  0.4880966246128082
train gradient:  0.12230114836114071
iteration : 881
train acc:  0.765625
train loss:  0.45643481612205505
train gradient:  0.1095933759422815
iteration : 882
train acc:  0.7421875
train loss:  0.4638339877128601
train gradient:  0.137006072206746
iteration : 883
train acc:  0.7109375
train loss:  0.5701214671134949
train gradient:  0.1722868797692974
iteration : 884
train acc:  0.7109375
train loss:  0.5401451587677002
train gradient:  0.15043115141342606
iteration : 885
train acc:  0.7421875
train loss:  0.46846702694892883
train gradient:  0.1012725879033535
iteration : 886
train acc:  0.8125
train loss:  0.3916245400905609
train gradient:  0.08650174089708745
iteration : 887
train acc:  0.6953125
train loss:  0.4862840175628662
train gradient:  0.13312251935133385
iteration : 888
train acc:  0.7734375
train loss:  0.48353612422943115
train gradient:  0.12771557686414267
iteration : 889
train acc:  0.7421875
train loss:  0.5081726312637329
train gradient:  0.14706478011036636
iteration : 890
train acc:  0.828125
train loss:  0.3896844983100891
train gradient:  0.09379392487201987
iteration : 891
train acc:  0.7578125
train loss:  0.4906632900238037
train gradient:  0.1600169409973905
iteration : 892
train acc:  0.7890625
train loss:  0.3981378674507141
train gradient:  0.09937431403589507
iteration : 893
train acc:  0.796875
train loss:  0.42432358860969543
train gradient:  0.13340959874450836
iteration : 894
train acc:  0.7578125
train loss:  0.5032854080200195
train gradient:  0.112791916971009
iteration : 895
train acc:  0.8203125
train loss:  0.4242675006389618
train gradient:  0.08899259974283964
iteration : 896
train acc:  0.7421875
train loss:  0.5362713932991028
train gradient:  0.13700442191341383
iteration : 897
train acc:  0.6875
train loss:  0.5517690181732178
train gradient:  0.13516176422427958
iteration : 898
train acc:  0.7578125
train loss:  0.490208238363266
train gradient:  0.12189104799734411
iteration : 899
train acc:  0.765625
train loss:  0.48325324058532715
train gradient:  0.11410313410222764
iteration : 900
train acc:  0.703125
train loss:  0.4799678325653076
train gradient:  0.10784806084511858
iteration : 901
train acc:  0.7265625
train loss:  0.4954291880130768
train gradient:  0.11795323638690682
iteration : 902
train acc:  0.7578125
train loss:  0.4864427447319031
train gradient:  0.12802265410638
iteration : 903
train acc:  0.7890625
train loss:  0.4523087739944458
train gradient:  0.09866255836772063
iteration : 904
train acc:  0.703125
train loss:  0.5083335638046265
train gradient:  0.11737131456399355
iteration : 905
train acc:  0.765625
train loss:  0.4706886112689972
train gradient:  0.12306972140871321
iteration : 906
train acc:  0.8125
train loss:  0.430547833442688
train gradient:  0.09559234360210646
iteration : 907
train acc:  0.78125
train loss:  0.46155938506126404
train gradient:  0.09843304150724479
iteration : 908
train acc:  0.7109375
train loss:  0.5181962251663208
train gradient:  0.11232352442635483
iteration : 909
train acc:  0.7265625
train loss:  0.5116333961486816
train gradient:  0.17222147717834602
iteration : 910
train acc:  0.8125
train loss:  0.4380294680595398
train gradient:  0.0921807607935254
iteration : 911
train acc:  0.7109375
train loss:  0.5017653703689575
train gradient:  0.13503638398426498
iteration : 912
train acc:  0.734375
train loss:  0.4825437068939209
train gradient:  0.14916590815611294
iteration : 913
train acc:  0.765625
train loss:  0.5039214491844177
train gradient:  0.14552097204746517
iteration : 914
train acc:  0.7265625
train loss:  0.513629674911499
train gradient:  0.14218203658610956
iteration : 915
train acc:  0.6796875
train loss:  0.5763829946517944
train gradient:  0.15359766778803846
iteration : 916
train acc:  0.6953125
train loss:  0.46402615308761597
train gradient:  0.09272042866275775
iteration : 917
train acc:  0.75
train loss:  0.5579438805580139
train gradient:  0.14765861681574693
iteration : 918
train acc:  0.734375
train loss:  0.485828161239624
train gradient:  0.15941623856691525
iteration : 919
train acc:  0.8203125
train loss:  0.4591468274593353
train gradient:  0.13147640873690594
iteration : 920
train acc:  0.765625
train loss:  0.46921178698539734
train gradient:  0.1147230223992152
iteration : 921
train acc:  0.7265625
train loss:  0.5556855201721191
train gradient:  0.15893347210817674
iteration : 922
train acc:  0.8125
train loss:  0.43043994903564453
train gradient:  0.08831878820714682
iteration : 923
train acc:  0.765625
train loss:  0.5219413042068481
train gradient:  0.13698592406565796
iteration : 924
train acc:  0.7421875
train loss:  0.5049842596054077
train gradient:  0.1407898321192979
iteration : 925
train acc:  0.7578125
train loss:  0.49818623065948486
train gradient:  0.11801571384431385
iteration : 926
train acc:  0.6875
train loss:  0.498422235250473
train gradient:  0.13560641671279366
iteration : 927
train acc:  0.7890625
train loss:  0.4679906368255615
train gradient:  0.10684052170875032
iteration : 928
train acc:  0.671875
train loss:  0.5152990818023682
train gradient:  0.1283806694828037
iteration : 929
train acc:  0.7578125
train loss:  0.4612485468387604
train gradient:  0.13213258797149927
iteration : 930
train acc:  0.7265625
train loss:  0.44064977765083313
train gradient:  0.12219498702582768
iteration : 931
train acc:  0.7578125
train loss:  0.4468906819820404
train gradient:  0.09728877947167254
iteration : 932
train acc:  0.7265625
train loss:  0.4823833107948303
train gradient:  0.1330037300202429
iteration : 933
train acc:  0.796875
train loss:  0.42897191643714905
train gradient:  0.10982677227194222
iteration : 934
train acc:  0.78125
train loss:  0.4497421383857727
train gradient:  0.0908206502927268
iteration : 935
train acc:  0.8125
train loss:  0.409420371055603
train gradient:  0.08494750293268455
iteration : 936
train acc:  0.796875
train loss:  0.5078690648078918
train gradient:  0.1643591877589171
iteration : 937
train acc:  0.734375
train loss:  0.4714725613594055
train gradient:  0.10622365490738414
iteration : 938
train acc:  0.734375
train loss:  0.4522439241409302
train gradient:  0.10667072197093048
iteration : 939
train acc:  0.6796875
train loss:  0.5792846083641052
train gradient:  0.1763076707295046
iteration : 940
train acc:  0.7734375
train loss:  0.5045878887176514
train gradient:  0.12016136720579926
iteration : 941
train acc:  0.75
train loss:  0.4493151605129242
train gradient:  0.10212550671409384
iteration : 942
train acc:  0.7421875
train loss:  0.5172291398048401
train gradient:  0.1539521744979317
iteration : 943
train acc:  0.7578125
train loss:  0.486747682094574
train gradient:  0.15810371939322065
iteration : 944
train acc:  0.8203125
train loss:  0.4323887526988983
train gradient:  0.10237428067061285
iteration : 945
train acc:  0.703125
train loss:  0.45068222284317017
train gradient:  0.1056549535351196
iteration : 946
train acc:  0.71875
train loss:  0.46162480115890503
train gradient:  0.11793828771848931
iteration : 947
train acc:  0.78125
train loss:  0.4669073224067688
train gradient:  0.08326228487390785
iteration : 948
train acc:  0.71875
train loss:  0.5699611902236938
train gradient:  0.17265042844452355
iteration : 949
train acc:  0.8203125
train loss:  0.442915141582489
train gradient:  0.09949190847593012
iteration : 950
train acc:  0.75
train loss:  0.5032743215560913
train gradient:  0.10084147363658413
iteration : 951
train acc:  0.6953125
train loss:  0.504032552242279
train gradient:  0.11637929725226712
iteration : 952
train acc:  0.765625
train loss:  0.5096346735954285
train gradient:  0.11752388289505356
iteration : 953
train acc:  0.640625
train loss:  0.5597776174545288
train gradient:  0.17863366453691892
iteration : 954
train acc:  0.765625
train loss:  0.5198373198509216
train gradient:  0.12868116094191295
iteration : 955
train acc:  0.7421875
train loss:  0.51530522108078
train gradient:  0.12753493392278364
iteration : 956
train acc:  0.7265625
train loss:  0.5119225382804871
train gradient:  0.14792193179306132
iteration : 957
train acc:  0.7578125
train loss:  0.48978155851364136
train gradient:  0.11585749427307546
iteration : 958
train acc:  0.796875
train loss:  0.4601745307445526
train gradient:  0.15939186967875799
iteration : 959
train acc:  0.7421875
train loss:  0.5025991201400757
train gradient:  0.11949916966131886
iteration : 960
train acc:  0.7734375
train loss:  0.4487057030200958
train gradient:  0.1138262645431803
iteration : 961
train acc:  0.7734375
train loss:  0.41207361221313477
train gradient:  0.0917984817115851
iteration : 962
train acc:  0.734375
train loss:  0.5016669631004333
train gradient:  0.14702235634048566
iteration : 963
train acc:  0.6875
train loss:  0.5625120401382446
train gradient:  0.2032829222077683
iteration : 964
train acc:  0.7890625
train loss:  0.4524633586406708
train gradient:  0.09528482231315175
iteration : 965
train acc:  0.734375
train loss:  0.4597274661064148
train gradient:  0.09964772423320988
iteration : 966
train acc:  0.7734375
train loss:  0.44696280360221863
train gradient:  0.11730676972053725
iteration : 967
train acc:  0.7734375
train loss:  0.4881702661514282
train gradient:  0.12268216153911499
iteration : 968
train acc:  0.75
train loss:  0.5041177272796631
train gradient:  0.1423869726702301
iteration : 969
train acc:  0.6953125
train loss:  0.49280846118927
train gradient:  0.14775833010767553
iteration : 970
train acc:  0.6953125
train loss:  0.5369275212287903
train gradient:  0.12686735491046114
iteration : 971
train acc:  0.734375
train loss:  0.5338344573974609
train gradient:  0.10250144723536059
iteration : 972
train acc:  0.75
train loss:  0.5051999092102051
train gradient:  0.13071281779599686
iteration : 973
train acc:  0.75
train loss:  0.43209439516067505
train gradient:  0.1188051647944102
iteration : 974
train acc:  0.75
train loss:  0.5054301023483276
train gradient:  0.13978263764926707
iteration : 975
train acc:  0.7421875
train loss:  0.4834934175014496
train gradient:  0.13680456933259383
iteration : 976
train acc:  0.7109375
train loss:  0.5455210208892822
train gradient:  0.16096993716772096
iteration : 977
train acc:  0.7265625
train loss:  0.49031203985214233
train gradient:  0.13757028274233862
iteration : 978
train acc:  0.75
train loss:  0.4825148582458496
train gradient:  0.11207165181614286
iteration : 979
train acc:  0.7265625
train loss:  0.5003517270088196
train gradient:  0.12825814603224656
iteration : 980
train acc:  0.6875
train loss:  0.5714865922927856
train gradient:  0.1808576598918118
iteration : 981
train acc:  0.7421875
train loss:  0.4454945921897888
train gradient:  0.12119134849749029
iteration : 982
train acc:  0.7734375
train loss:  0.47021085023880005
train gradient:  0.0926664130616151
iteration : 983
train acc:  0.734375
train loss:  0.5245721936225891
train gradient:  0.13081062062942536
iteration : 984
train acc:  0.765625
train loss:  0.4497930407524109
train gradient:  0.1011359222016145
iteration : 985
train acc:  0.765625
train loss:  0.48197370767593384
train gradient:  0.1119505523930268
iteration : 986
train acc:  0.7265625
train loss:  0.4588888883590698
train gradient:  0.10342110642167764
iteration : 987
train acc:  0.7265625
train loss:  0.4824194312095642
train gradient:  0.11346086215878211
iteration : 988
train acc:  0.734375
train loss:  0.4917229413986206
train gradient:  0.1573678093304346
iteration : 989
train acc:  0.703125
train loss:  0.536984920501709
train gradient:  0.13416548663712902
iteration : 990
train acc:  0.7578125
train loss:  0.49617454409599304
train gradient:  0.12908057041812526
iteration : 991
train acc:  0.734375
train loss:  0.4822978377342224
train gradient:  0.12484964149466449
iteration : 992
train acc:  0.71875
train loss:  0.5736878514289856
train gradient:  0.17618607930930924
iteration : 993
train acc:  0.6953125
train loss:  0.5671852231025696
train gradient:  0.12862693428965016
iteration : 994
train acc:  0.6640625
train loss:  0.5320539474487305
train gradient:  0.1362849000162238
iteration : 995
train acc:  0.7421875
train loss:  0.5557256937026978
train gradient:  0.14263042222342626
iteration : 996
train acc:  0.734375
train loss:  0.492282509803772
train gradient:  0.13408620897487317
iteration : 997
train acc:  0.75
train loss:  0.47634947299957275
train gradient:  0.12577959828763263
iteration : 998
train acc:  0.7578125
train loss:  0.46725690364837646
train gradient:  0.10644121676086589
iteration : 999
train acc:  0.78125
train loss:  0.4639730453491211
train gradient:  0.09976330611849606
iteration : 1000
train acc:  0.734375
train loss:  0.5441031455993652
train gradient:  0.14726103334382518
iteration : 1001
train acc:  0.7265625
train loss:  0.4808517396450043
train gradient:  0.10923839666169999
iteration : 1002
train acc:  0.6953125
train loss:  0.5495476126670837
train gradient:  0.1460831812037357
iteration : 1003
train acc:  0.7421875
train loss:  0.4854396879673004
train gradient:  0.11028165456680658
iteration : 1004
train acc:  0.75
train loss:  0.4395538568496704
train gradient:  0.09158552780437895
iteration : 1005
train acc:  0.71875
train loss:  0.5264351963996887
train gradient:  0.11516729238637914
iteration : 1006
train acc:  0.7265625
train loss:  0.480988085269928
train gradient:  0.12835405133802974
iteration : 1007
train acc:  0.7265625
train loss:  0.5494512319564819
train gradient:  0.17685397615080362
iteration : 1008
train acc:  0.7109375
train loss:  0.5149000883102417
train gradient:  0.16106171436801428
iteration : 1009
train acc:  0.7265625
train loss:  0.5319496393203735
train gradient:  0.13672790308608535
iteration : 1010
train acc:  0.7734375
train loss:  0.4431707262992859
train gradient:  0.08845704503588431
iteration : 1011
train acc:  0.75
train loss:  0.466884970664978
train gradient:  0.1140556420175651
iteration : 1012
train acc:  0.8125
train loss:  0.4310647249221802
train gradient:  0.06879016568797185
iteration : 1013
train acc:  0.71875
train loss:  0.4747852683067322
train gradient:  0.1147041615067698
iteration : 1014
train acc:  0.75
train loss:  0.4949488639831543
train gradient:  0.12208377298659102
iteration : 1015
train acc:  0.7578125
train loss:  0.46304017305374146
train gradient:  0.09009629811503292
iteration : 1016
train acc:  0.7265625
train loss:  0.47558853030204773
train gradient:  0.09778236275734238
iteration : 1017
train acc:  0.7421875
train loss:  0.54021155834198
train gradient:  0.11861646020141674
iteration : 1018
train acc:  0.7421875
train loss:  0.4449905753135681
train gradient:  0.10789924885292239
iteration : 1019
train acc:  0.765625
train loss:  0.5101099610328674
train gradient:  0.11148869395076644
iteration : 1020
train acc:  0.78125
train loss:  0.4368162751197815
train gradient:  0.10193470159611978
iteration : 1021
train acc:  0.78125
train loss:  0.47357362508773804
train gradient:  0.09750015800931248
iteration : 1022
train acc:  0.71875
train loss:  0.5284131765365601
train gradient:  0.13591342627400294
iteration : 1023
train acc:  0.8203125
train loss:  0.4113667607307434
train gradient:  0.09470293484980677
iteration : 1024
train acc:  0.75
train loss:  0.49219465255737305
train gradient:  0.11288617940177031
iteration : 1025
train acc:  0.7421875
train loss:  0.49946334958076477
train gradient:  0.1342841134319802
iteration : 1026
train acc:  0.796875
train loss:  0.4376082122325897
train gradient:  0.08183228004564778
iteration : 1027
train acc:  0.75
train loss:  0.4658263623714447
train gradient:  0.10429157920582098
iteration : 1028
train acc:  0.7109375
train loss:  0.47697582840919495
train gradient:  0.11895323006853027
iteration : 1029
train acc:  0.7421875
train loss:  0.4949832558631897
train gradient:  0.08891592073770965
iteration : 1030
train acc:  0.7265625
train loss:  0.4508574903011322
train gradient:  0.10744098288729712
iteration : 1031
train acc:  0.7265625
train loss:  0.46385037899017334
train gradient:  0.0856107469648467
iteration : 1032
train acc:  0.765625
train loss:  0.49722737073898315
train gradient:  0.13949373789768538
iteration : 1033
train acc:  0.703125
train loss:  0.5352160334587097
train gradient:  0.15415474163048376
iteration : 1034
train acc:  0.7109375
train loss:  0.5125778913497925
train gradient:  0.14587280325588065
iteration : 1035
train acc:  0.65625
train loss:  0.5615332722663879
train gradient:  0.1306089510407353
iteration : 1036
train acc:  0.7578125
train loss:  0.4332708716392517
train gradient:  0.09918840247760222
iteration : 1037
train acc:  0.8046875
train loss:  0.41701972484588623
train gradient:  0.09886657250342264
iteration : 1038
train acc:  0.765625
train loss:  0.47575563192367554
train gradient:  0.10997066781778439
iteration : 1039
train acc:  0.765625
train loss:  0.44553136825561523
train gradient:  0.09699537540810026
iteration : 1040
train acc:  0.828125
train loss:  0.45710524916648865
train gradient:  0.12009467857119943
iteration : 1041
train acc:  0.796875
train loss:  0.4345859885215759
train gradient:  0.10959911235569687
iteration : 1042
train acc:  0.7734375
train loss:  0.4266236424446106
train gradient:  0.10530942190370461
iteration : 1043
train acc:  0.8046875
train loss:  0.4851130247116089
train gradient:  0.12418322566393787
iteration : 1044
train acc:  0.7734375
train loss:  0.4553147554397583
train gradient:  0.12192866339509567
iteration : 1045
train acc:  0.6953125
train loss:  0.5215264558792114
train gradient:  0.1382843784259071
iteration : 1046
train acc:  0.8046875
train loss:  0.47442933917045593
train gradient:  0.12198479117082546
iteration : 1047
train acc:  0.75
train loss:  0.504850447177887
train gradient:  0.12843868156441224
iteration : 1048
train acc:  0.6875
train loss:  0.5155606269836426
train gradient:  0.12663764600758845
iteration : 1049
train acc:  0.734375
train loss:  0.5015997886657715
train gradient:  0.11528198149004329
iteration : 1050
train acc:  0.6875
train loss:  0.5454896688461304
train gradient:  0.12555131553263738
iteration : 1051
train acc:  0.75
train loss:  0.511103630065918
train gradient:  0.15689812380759102
iteration : 1052
train acc:  0.7578125
train loss:  0.5094908475875854
train gradient:  0.14936899578310875
iteration : 1053
train acc:  0.7265625
train loss:  0.5293058156967163
train gradient:  0.1517332413432306
iteration : 1054
train acc:  0.6484375
train loss:  0.545174241065979
train gradient:  0.13810584760232214
iteration : 1055
train acc:  0.7734375
train loss:  0.4382386803627014
train gradient:  0.08839266496323603
iteration : 1056
train acc:  0.796875
train loss:  0.4180428981781006
train gradient:  0.0986405539220311
iteration : 1057
train acc:  0.7109375
train loss:  0.5132033824920654
train gradient:  0.12201914661236157
iteration : 1058
train acc:  0.765625
train loss:  0.4753440022468567
train gradient:  0.12630979356050215
iteration : 1059
train acc:  0.765625
train loss:  0.49266505241394043
train gradient:  0.14018742970290238
iteration : 1060
train acc:  0.71875
train loss:  0.49810492992401123
train gradient:  0.13684927036730482
iteration : 1061
train acc:  0.7734375
train loss:  0.4488239884376526
train gradient:  0.09365087811052573
iteration : 1062
train acc:  0.7578125
train loss:  0.473202645778656
train gradient:  0.1066826916788972
iteration : 1063
train acc:  0.71875
train loss:  0.5091391801834106
train gradient:  0.13395095391730644
iteration : 1064
train acc:  0.75
train loss:  0.47643327713012695
train gradient:  0.1311715197540631
iteration : 1065
train acc:  0.6875
train loss:  0.6197142004966736
train gradient:  0.16681637010413208
iteration : 1066
train acc:  0.7578125
train loss:  0.48790544271469116
train gradient:  0.12491744824571166
iteration : 1067
train acc:  0.7265625
train loss:  0.5283676981925964
train gradient:  0.1338936904601958
iteration : 1068
train acc:  0.7890625
train loss:  0.4390372633934021
train gradient:  0.12381142842166201
iteration : 1069
train acc:  0.7734375
train loss:  0.47460028529167175
train gradient:  0.11261191760158797
iteration : 1070
train acc:  0.78125
train loss:  0.4731612801551819
train gradient:  0.09872876959008503
iteration : 1071
train acc:  0.6953125
train loss:  0.513978898525238
train gradient:  0.14023889405037804
iteration : 1072
train acc:  0.71875
train loss:  0.517860472202301
train gradient:  0.1331251675648753
iteration : 1073
train acc:  0.7109375
train loss:  0.5564913153648376
train gradient:  0.14684007990999343
iteration : 1074
train acc:  0.7109375
train loss:  0.5082917809486389
train gradient:  0.10774733555173048
iteration : 1075
train acc:  0.8046875
train loss:  0.4631047248840332
train gradient:  0.12631650093265295
iteration : 1076
train acc:  0.7265625
train loss:  0.5211207866668701
train gradient:  0.1874371560667683
iteration : 1077
train acc:  0.734375
train loss:  0.5102587938308716
train gradient:  0.10959421422026255
iteration : 1078
train acc:  0.6796875
train loss:  0.564672589302063
train gradient:  0.16559249219928446
iteration : 1079
train acc:  0.71875
train loss:  0.4805082678794861
train gradient:  0.11045005713114765
iteration : 1080
train acc:  0.796875
train loss:  0.471645712852478
train gradient:  0.1346140399642034
iteration : 1081
train acc:  0.6640625
train loss:  0.5469743013381958
train gradient:  0.18655910366781173
iteration : 1082
train acc:  0.71875
train loss:  0.4855177402496338
train gradient:  0.13526100555951248
iteration : 1083
train acc:  0.796875
train loss:  0.45817333459854126
train gradient:  0.10928901093067367
iteration : 1084
train acc:  0.7421875
train loss:  0.5419241786003113
train gradient:  0.15123283725132336
iteration : 1085
train acc:  0.71875
train loss:  0.5101296901702881
train gradient:  0.1716116750211043
iteration : 1086
train acc:  0.75
train loss:  0.4483133852481842
train gradient:  0.11132901499050758
iteration : 1087
train acc:  0.6796875
train loss:  0.5506026744842529
train gradient:  0.15513364361458168
iteration : 1088
train acc:  0.78125
train loss:  0.4117695689201355
train gradient:  0.0915301397050198
iteration : 1089
train acc:  0.75
train loss:  0.49319207668304443
train gradient:  0.10387435418072283
iteration : 1090
train acc:  0.765625
train loss:  0.4770904779434204
train gradient:  0.13671446567855022
iteration : 1091
train acc:  0.7578125
train loss:  0.4751942753791809
train gradient:  0.11252091058492208
iteration : 1092
train acc:  0.7890625
train loss:  0.40628325939178467
train gradient:  0.08520914111716886
iteration : 1093
train acc:  0.7734375
train loss:  0.43588557839393616
train gradient:  0.11459907412811193
iteration : 1094
train acc:  0.7265625
train loss:  0.5076935291290283
train gradient:  0.11304524509564938
iteration : 1095
train acc:  0.734375
train loss:  0.4469955563545227
train gradient:  0.11129280518726617
iteration : 1096
train acc:  0.7578125
train loss:  0.5020159482955933
train gradient:  0.14020380072292515
iteration : 1097
train acc:  0.765625
train loss:  0.48275408148765564
train gradient:  0.12195950159969582
iteration : 1098
train acc:  0.6640625
train loss:  0.5890262126922607
train gradient:  0.227848899512926
iteration : 1099
train acc:  0.7578125
train loss:  0.47415539622306824
train gradient:  0.17630977210115742
iteration : 1100
train acc:  0.75
train loss:  0.5188542604446411
train gradient:  0.13104083802303812
iteration : 1101
train acc:  0.7265625
train loss:  0.4511561989784241
train gradient:  0.09762725834005552
iteration : 1102
train acc:  0.75
train loss:  0.47402068972587585
train gradient:  0.09985137889359848
iteration : 1103
train acc:  0.7421875
train loss:  0.4650707542896271
train gradient:  0.11834149686300505
iteration : 1104
train acc:  0.765625
train loss:  0.4862729609012604
train gradient:  0.13359654910213195
iteration : 1105
train acc:  0.703125
train loss:  0.5113057494163513
train gradient:  0.12179433780809529
iteration : 1106
train acc:  0.7109375
train loss:  0.5163516998291016
train gradient:  0.15593768575752442
iteration : 1107
train acc:  0.7890625
train loss:  0.4116005301475525
train gradient:  0.10362737057754826
iteration : 1108
train acc:  0.7421875
train loss:  0.464092493057251
train gradient:  0.12278794489519426
iteration : 1109
train acc:  0.78125
train loss:  0.46211591362953186
train gradient:  0.11093268572536388
iteration : 1110
train acc:  0.6640625
train loss:  0.579584538936615
train gradient:  0.1427995211495509
iteration : 1111
train acc:  0.78125
train loss:  0.456059992313385
train gradient:  0.09151018038428017
iteration : 1112
train acc:  0.7109375
train loss:  0.53179931640625
train gradient:  0.1516460883737875
iteration : 1113
train acc:  0.703125
train loss:  0.5483701825141907
train gradient:  0.1471571793107097
iteration : 1114
train acc:  0.7578125
train loss:  0.5140253305435181
train gradient:  0.13931788959189578
iteration : 1115
train acc:  0.7890625
train loss:  0.46603214740753174
train gradient:  0.10939327488256459
iteration : 1116
train acc:  0.7109375
train loss:  0.4591500759124756
train gradient:  0.0953340381609246
iteration : 1117
train acc:  0.8203125
train loss:  0.43148350715637207
train gradient:  0.08986390981311973
iteration : 1118
train acc:  0.71875
train loss:  0.500081479549408
train gradient:  0.12803203401173552
iteration : 1119
train acc:  0.703125
train loss:  0.5103324055671692
train gradient:  0.12781422153699057
iteration : 1120
train acc:  0.7578125
train loss:  0.48567381501197815
train gradient:  0.11289364013169619
iteration : 1121
train acc:  0.7265625
train loss:  0.4821707606315613
train gradient:  0.12029639506776253
iteration : 1122
train acc:  0.765625
train loss:  0.4696442484855652
train gradient:  0.09590559521433288
iteration : 1123
train acc:  0.7734375
train loss:  0.482086718082428
train gradient:  0.10568306484478011
iteration : 1124
train acc:  0.8046875
train loss:  0.42784667015075684
train gradient:  0.12308274782904688
iteration : 1125
train acc:  0.7578125
train loss:  0.46334773302078247
train gradient:  0.11280695767709494
iteration : 1126
train acc:  0.7109375
train loss:  0.4845871329307556
train gradient:  0.09540245743191483
iteration : 1127
train acc:  0.8125
train loss:  0.46418264508247375
train gradient:  0.11234196858155739
iteration : 1128
train acc:  0.6953125
train loss:  0.5587214231491089
train gradient:  0.17189241788976306
iteration : 1129
train acc:  0.78125
train loss:  0.41870445013046265
train gradient:  0.0976139895461889
iteration : 1130
train acc:  0.7265625
train loss:  0.5196429491043091
train gradient:  0.14442440264906586
iteration : 1131
train acc:  0.7109375
train loss:  0.4857634902000427
train gradient:  0.1493199902697428
iteration : 1132
train acc:  0.734375
train loss:  0.5032528638839722
train gradient:  0.1351908421306971
iteration : 1133
train acc:  0.7265625
train loss:  0.47287851572036743
train gradient:  0.10159116406383703
iteration : 1134
train acc:  0.75
train loss:  0.45977914333343506
train gradient:  0.10209535627738783
iteration : 1135
train acc:  0.7421875
train loss:  0.450223445892334
train gradient:  0.08691985298039581
iteration : 1136
train acc:  0.671875
train loss:  0.5084195137023926
train gradient:  0.11357733007570868
iteration : 1137
train acc:  0.71875
train loss:  0.5020469427108765
train gradient:  0.15153788869747528
iteration : 1138
train acc:  0.7109375
train loss:  0.5262678861618042
train gradient:  0.1333555996105283
iteration : 1139
train acc:  0.75
train loss:  0.45461905002593994
train gradient:  0.11184073786611695
iteration : 1140
train acc:  0.703125
train loss:  0.5116674304008484
train gradient:  0.12825911180809443
iteration : 1141
train acc:  0.765625
train loss:  0.48080024123191833
train gradient:  0.13838903781065337
iteration : 1142
train acc:  0.7109375
train loss:  0.49392247200012207
train gradient:  0.10696225807458579
iteration : 1143
train acc:  0.71875
train loss:  0.46282222867012024
train gradient:  0.11180625230294965
iteration : 1144
train acc:  0.78125
train loss:  0.45639538764953613
train gradient:  0.1078929216407345
iteration : 1145
train acc:  0.7734375
train loss:  0.45122021436691284
train gradient:  0.09626336178506015
iteration : 1146
train acc:  0.734375
train loss:  0.49366140365600586
train gradient:  0.1252255222522517
iteration : 1147
train acc:  0.7421875
train loss:  0.48273342847824097
train gradient:  0.0999573818720523
iteration : 1148
train acc:  0.7734375
train loss:  0.48433253169059753
train gradient:  0.10556430021519604
iteration : 1149
train acc:  0.703125
train loss:  0.5074163675308228
train gradient:  0.0878411926639092
iteration : 1150
train acc:  0.75
train loss:  0.47214120626449585
train gradient:  0.09495324359630619
iteration : 1151
train acc:  0.7734375
train loss:  0.508769690990448
train gradient:  0.12490874532528098
iteration : 1152
train acc:  0.734375
train loss:  0.520205020904541
train gradient:  0.12203406953375436
iteration : 1153
train acc:  0.765625
train loss:  0.45748990774154663
train gradient:  0.10702700181395843
iteration : 1154
train acc:  0.75
train loss:  0.4759868383407593
train gradient:  0.12291349745456995
iteration : 1155
train acc:  0.7109375
train loss:  0.5268380641937256
train gradient:  0.14196066691451215
iteration : 1156
train acc:  0.8046875
train loss:  0.47061216831207275
train gradient:  0.11088897590877148
iteration : 1157
train acc:  0.796875
train loss:  0.41441571712493896
train gradient:  0.0824260501968472
iteration : 1158
train acc:  0.7421875
train loss:  0.4808552861213684
train gradient:  0.09392086636307219
iteration : 1159
train acc:  0.75
train loss:  0.4825935363769531
train gradient:  0.1101187386767959
iteration : 1160
train acc:  0.7578125
train loss:  0.49126842617988586
train gradient:  0.11869268585832926
iteration : 1161
train acc:  0.7734375
train loss:  0.4468507170677185
train gradient:  0.1016085838641275
iteration : 1162
train acc:  0.71875
train loss:  0.5295311808586121
train gradient:  0.14161580814169628
iteration : 1163
train acc:  0.7421875
train loss:  0.4686082601547241
train gradient:  0.11375425018356893
iteration : 1164
train acc:  0.765625
train loss:  0.480451375246048
train gradient:  0.12286604212983587
iteration : 1165
train acc:  0.796875
train loss:  0.4049004316329956
train gradient:  0.09587332624889533
iteration : 1166
train acc:  0.78125
train loss:  0.4412776231765747
train gradient:  0.07515961581462315
iteration : 1167
train acc:  0.7421875
train loss:  0.5103583931922913
train gradient:  0.14031670362640258
iteration : 1168
train acc:  0.7890625
train loss:  0.451052188873291
train gradient:  0.12886578979443353
iteration : 1169
train acc:  0.78125
train loss:  0.4523453116416931
train gradient:  0.08701472918913099
iteration : 1170
train acc:  0.734375
train loss:  0.431285560131073
train gradient:  0.10161946721493255
iteration : 1171
train acc:  0.71875
train loss:  0.4538717567920685
train gradient:  0.11105371836663051
iteration : 1172
train acc:  0.7265625
train loss:  0.5504482984542847
train gradient:  0.20164027828478354
iteration : 1173
train acc:  0.796875
train loss:  0.47922712564468384
train gradient:  0.10718451169030091
iteration : 1174
train acc:  0.78125
train loss:  0.46484804153442383
train gradient:  0.11154362287844209
iteration : 1175
train acc:  0.8046875
train loss:  0.4263204038143158
train gradient:  0.09370836722534491
iteration : 1176
train acc:  0.734375
train loss:  0.47069549560546875
train gradient:  0.0919722489187085
iteration : 1177
train acc:  0.6875
train loss:  0.551838219165802
train gradient:  0.16341893579605435
iteration : 1178
train acc:  0.7890625
train loss:  0.4981488287448883
train gradient:  0.12778785812874818
iteration : 1179
train acc:  0.703125
train loss:  0.49934592843055725
train gradient:  0.11816029286953826
iteration : 1180
train acc:  0.796875
train loss:  0.48181232810020447
train gradient:  0.12961609323343537
iteration : 1181
train acc:  0.7578125
train loss:  0.4657607078552246
train gradient:  0.13203557369058821
iteration : 1182
train acc:  0.7734375
train loss:  0.5164675712585449
train gradient:  0.1395355740002101
iteration : 1183
train acc:  0.7890625
train loss:  0.44703781604766846
train gradient:  0.10923897504498349
iteration : 1184
train acc:  0.7265625
train loss:  0.45318639278411865
train gradient:  0.08758950367936273
iteration : 1185
train acc:  0.6953125
train loss:  0.5303828716278076
train gradient:  0.1433401785958613
iteration : 1186
train acc:  0.828125
train loss:  0.39780279994010925
train gradient:  0.10300185945202604
iteration : 1187
train acc:  0.796875
train loss:  0.4428952932357788
train gradient:  0.09917112054120601
iteration : 1188
train acc:  0.71875
train loss:  0.5350707769393921
train gradient:  0.16267963564248952
iteration : 1189
train acc:  0.71875
train loss:  0.5100778341293335
train gradient:  0.11653092579065083
iteration : 1190
train acc:  0.7421875
train loss:  0.4433886408805847
train gradient:  0.09112244598303275
iteration : 1191
train acc:  0.8046875
train loss:  0.4351222515106201
train gradient:  0.09754373674055115
iteration : 1192
train acc:  0.7265625
train loss:  0.4849432408809662
train gradient:  0.11070742430502635
iteration : 1193
train acc:  0.734375
train loss:  0.4773997664451599
train gradient:  0.11795749590102309
iteration : 1194
train acc:  0.8046875
train loss:  0.4716729521751404
train gradient:  0.10874076632824481
iteration : 1195
train acc:  0.7890625
train loss:  0.4619000256061554
train gradient:  0.14689946866547204
iteration : 1196
train acc:  0.796875
train loss:  0.424224853515625
train gradient:  0.10998754108054482
iteration : 1197
train acc:  0.75
train loss:  0.5071433782577515
train gradient:  0.11015887132338889
iteration : 1198
train acc:  0.6171875
train loss:  0.5519251227378845
train gradient:  0.1737759262025651
iteration : 1199
train acc:  0.734375
train loss:  0.5032275319099426
train gradient:  0.13085674042619694
iteration : 1200
train acc:  0.7265625
train loss:  0.5563721656799316
train gradient:  0.19900299445560174
iteration : 1201
train acc:  0.6953125
train loss:  0.5426729917526245
train gradient:  0.14773978325305373
iteration : 1202
train acc:  0.7109375
train loss:  0.5809313058853149
train gradient:  0.16584909502820916
iteration : 1203
train acc:  0.78125
train loss:  0.41078510880470276
train gradient:  0.08682109585957869
iteration : 1204
train acc:  0.7421875
train loss:  0.47317588329315186
train gradient:  0.14456663136021797
iteration : 1205
train acc:  0.7890625
train loss:  0.4436151385307312
train gradient:  0.12501585257834516
iteration : 1206
train acc:  0.734375
train loss:  0.470836341381073
train gradient:  0.10922596615890946
iteration : 1207
train acc:  0.734375
train loss:  0.5069618225097656
train gradient:  0.13261955363020167
iteration : 1208
train acc:  0.7421875
train loss:  0.5736581683158875
train gradient:  0.15526223377637172
iteration : 1209
train acc:  0.796875
train loss:  0.4774835407733917
train gradient:  0.1263355344993886
iteration : 1210
train acc:  0.7578125
train loss:  0.5305246710777283
train gradient:  0.14123463617135604
iteration : 1211
train acc:  0.734375
train loss:  0.5052477717399597
train gradient:  0.09909374123943612
iteration : 1212
train acc:  0.7578125
train loss:  0.4928761124610901
train gradient:  0.13719401575825152
iteration : 1213
train acc:  0.7890625
train loss:  0.4253842532634735
train gradient:  0.09995284233548579
iteration : 1214
train acc:  0.765625
train loss:  0.48699623346328735
train gradient:  0.1007203582469884
iteration : 1215
train acc:  0.7578125
train loss:  0.4450143575668335
train gradient:  0.09218220673531322
iteration : 1216
train acc:  0.8203125
train loss:  0.423151433467865
train gradient:  0.08170500086926381
iteration : 1217
train acc:  0.7578125
train loss:  0.5011071562767029
train gradient:  0.13285426621545232
iteration : 1218
train acc:  0.7421875
train loss:  0.48072147369384766
train gradient:  0.12374738402978809
iteration : 1219
train acc:  0.7890625
train loss:  0.45630115270614624
train gradient:  0.09292378491310845
iteration : 1220
train acc:  0.7578125
train loss:  0.4727902114391327
train gradient:  0.11755875555677585
iteration : 1221
train acc:  0.6796875
train loss:  0.5492475032806396
train gradient:  0.14478978720285252
iteration : 1222
train acc:  0.765625
train loss:  0.4740784466266632
train gradient:  0.11636667176856003
iteration : 1223
train acc:  0.8203125
train loss:  0.42899930477142334
train gradient:  0.08825356916991374
iteration : 1224
train acc:  0.7421875
train loss:  0.4849374294281006
train gradient:  0.11830232984355375
iteration : 1225
train acc:  0.7421875
train loss:  0.5083197355270386
train gradient:  0.12970486370881631
iteration : 1226
train acc:  0.71875
train loss:  0.5384652018547058
train gradient:  0.13569168406534377
iteration : 1227
train acc:  0.7265625
train loss:  0.48507824540138245
train gradient:  0.1384357853866043
iteration : 1228
train acc:  0.7421875
train loss:  0.4788450002670288
train gradient:  0.10408339826641773
iteration : 1229
train acc:  0.7578125
train loss:  0.4525816738605499
train gradient:  0.11012181736875735
iteration : 1230
train acc:  0.75
train loss:  0.45701712369918823
train gradient:  0.08268731593663581
iteration : 1231
train acc:  0.7578125
train loss:  0.4491836428642273
train gradient:  0.09500849313912711
iteration : 1232
train acc:  0.703125
train loss:  0.5692497491836548
train gradient:  0.18596503095588032
iteration : 1233
train acc:  0.734375
train loss:  0.5014607906341553
train gradient:  0.09451897636582327
iteration : 1234
train acc:  0.8125
train loss:  0.3854474723339081
train gradient:  0.09948800471952868
iteration : 1235
train acc:  0.71875
train loss:  0.4425913095474243
train gradient:  0.09677114589351536
iteration : 1236
train acc:  0.7734375
train loss:  0.47849327325820923
train gradient:  0.12044379513145614
iteration : 1237
train acc:  0.7578125
train loss:  0.47724151611328125
train gradient:  0.10587869689463668
iteration : 1238
train acc:  0.8046875
train loss:  0.4288300573825836
train gradient:  0.10553378351247561
iteration : 1239
train acc:  0.7890625
train loss:  0.46832796931266785
train gradient:  0.09143177280537719
iteration : 1240
train acc:  0.7109375
train loss:  0.47707366943359375
train gradient:  0.10805202113720516
iteration : 1241
train acc:  0.7890625
train loss:  0.43534237146377563
train gradient:  0.09654505644074735
iteration : 1242
train acc:  0.78125
train loss:  0.4688287079334259
train gradient:  0.11914086367278279
iteration : 1243
train acc:  0.8046875
train loss:  0.41992703080177307
train gradient:  0.09882196640762766
iteration : 1244
train acc:  0.734375
train loss:  0.4933478832244873
train gradient:  0.12690620165522684
iteration : 1245
train acc:  0.7421875
train loss:  0.4731212258338928
train gradient:  0.11435845708431902
iteration : 1246
train acc:  0.78125
train loss:  0.45540380477905273
train gradient:  0.12962297519716748
iteration : 1247
train acc:  0.8046875
train loss:  0.46099114418029785
train gradient:  0.14237904505199067
iteration : 1248
train acc:  0.8046875
train loss:  0.42940032482147217
train gradient:  0.10400651538208433
iteration : 1249
train acc:  0.75
train loss:  0.528194010257721
train gradient:  0.1375433915379104
iteration : 1250
train acc:  0.765625
train loss:  0.47008663415908813
train gradient:  0.1128084466294294
iteration : 1251
train acc:  0.71875
train loss:  0.49049341678619385
train gradient:  0.12045492397038934
iteration : 1252
train acc:  0.7578125
train loss:  0.48232513666152954
train gradient:  0.11245598300297262
iteration : 1253
train acc:  0.7421875
train loss:  0.44596028327941895
train gradient:  0.1210599387219254
iteration : 1254
train acc:  0.765625
train loss:  0.4599481225013733
train gradient:  0.12404161900591848
iteration : 1255
train acc:  0.75
train loss:  0.4771605432033539
train gradient:  0.1118399846534532
iteration : 1256
train acc:  0.8359375
train loss:  0.43426233530044556
train gradient:  0.10042976125265977
iteration : 1257
train acc:  0.796875
train loss:  0.482283353805542
train gradient:  0.12418594059638204
iteration : 1258
train acc:  0.6796875
train loss:  0.530929446220398
train gradient:  0.11406691720939278
iteration : 1259
train acc:  0.7109375
train loss:  0.4526592791080475
train gradient:  0.10426626731814315
iteration : 1260
train acc:  0.7890625
train loss:  0.37666943669319153
train gradient:  0.07959710404459866
iteration : 1261
train acc:  0.796875
train loss:  0.40596288442611694
train gradient:  0.08623562365169661
iteration : 1262
train acc:  0.71875
train loss:  0.5476933717727661
train gradient:  0.14489652655099572
iteration : 1263
train acc:  0.7734375
train loss:  0.46117669343948364
train gradient:  0.09917793196478057
iteration : 1264
train acc:  0.8515625
train loss:  0.415044367313385
train gradient:  0.08935711506452808
iteration : 1265
train acc:  0.7578125
train loss:  0.45705538988113403
train gradient:  0.12076216159826017
iteration : 1266
train acc:  0.8125
train loss:  0.4350416660308838
train gradient:  0.10091352833608616
iteration : 1267
train acc:  0.7109375
train loss:  0.49443238973617554
train gradient:  0.10918339293013693
iteration : 1268
train acc:  0.734375
train loss:  0.5045521855354309
train gradient:  0.13948922453782714
iteration : 1269
train acc:  0.78125
train loss:  0.4757484197616577
train gradient:  0.1299517496028308
iteration : 1270
train acc:  0.7734375
train loss:  0.4627271294593811
train gradient:  0.12600626188763164
iteration : 1271
train acc:  0.734375
train loss:  0.5007946491241455
train gradient:  0.0997978947254893
iteration : 1272
train acc:  0.71875
train loss:  0.5149465799331665
train gradient:  0.1528051177096329
iteration : 1273
train acc:  0.78125
train loss:  0.4679267704486847
train gradient:  0.13408406424058578
iteration : 1274
train acc:  0.734375
train loss:  0.4902240037918091
train gradient:  0.11778769842317378
iteration : 1275
train acc:  0.828125
train loss:  0.41408032178878784
train gradient:  0.10644752151894786
iteration : 1276
train acc:  0.7734375
train loss:  0.5303868055343628
train gradient:  0.14738258620879746
iteration : 1277
train acc:  0.7109375
train loss:  0.5654433965682983
train gradient:  0.16863269205796583
iteration : 1278
train acc:  0.796875
train loss:  0.4462481141090393
train gradient:  0.10206937903306468
iteration : 1279
train acc:  0.7109375
train loss:  0.48410123586654663
train gradient:  0.09387711952048458
iteration : 1280
train acc:  0.734375
train loss:  0.4626835584640503
train gradient:  0.10199287864011997
iteration : 1281
train acc:  0.7734375
train loss:  0.43460404872894287
train gradient:  0.10452533281866783
iteration : 1282
train acc:  0.75
train loss:  0.4982984960079193
train gradient:  0.09935546989309074
iteration : 1283
train acc:  0.7421875
train loss:  0.44490110874176025
train gradient:  0.0872559722851198
iteration : 1284
train acc:  0.75
train loss:  0.4720515310764313
train gradient:  0.1108413857807559
iteration : 1285
train acc:  0.8125
train loss:  0.3950154483318329
train gradient:  0.0932183675950432
iteration : 1286
train acc:  0.7734375
train loss:  0.5136352777481079
train gradient:  0.1404661043557918
iteration : 1287
train acc:  0.75
train loss:  0.480024516582489
train gradient:  0.10245413769031023
iteration : 1288
train acc:  0.6796875
train loss:  0.5361294746398926
train gradient:  0.1252124801244656
iteration : 1289
train acc:  0.75
train loss:  0.5197145342826843
train gradient:  0.14492022375541458
iteration : 1290
train acc:  0.796875
train loss:  0.45651501417160034
train gradient:  0.12250904955091943
iteration : 1291
train acc:  0.765625
train loss:  0.43552663922309875
train gradient:  0.09525170530236374
iteration : 1292
train acc:  0.703125
train loss:  0.5286259651184082
train gradient:  0.14991749045540298
iteration : 1293
train acc:  0.7421875
train loss:  0.43588021397590637
train gradient:  0.08495217995903134
iteration : 1294
train acc:  0.7578125
train loss:  0.4799669086933136
train gradient:  0.12168285119289346
iteration : 1295
train acc:  0.8203125
train loss:  0.402878999710083
train gradient:  0.09071219184689334
iteration : 1296
train acc:  0.7890625
train loss:  0.43707284331321716
train gradient:  0.10553907193468065
iteration : 1297
train acc:  0.7421875
train loss:  0.48474499583244324
train gradient:  0.10386474023207487
iteration : 1298
train acc:  0.7734375
train loss:  0.539799690246582
train gradient:  0.1525146673874515
iteration : 1299
train acc:  0.7109375
train loss:  0.5581374168395996
train gradient:  0.16494773599024237
iteration : 1300
train acc:  0.8046875
train loss:  0.44892460107803345
train gradient:  0.12084512011239756
iteration : 1301
train acc:  0.75
train loss:  0.45409727096557617
train gradient:  0.09950802357026477
iteration : 1302
train acc:  0.796875
train loss:  0.47475743293762207
train gradient:  0.13033003205783056
iteration : 1303
train acc:  0.7109375
train loss:  0.5489165782928467
train gradient:  0.15230091514754535
iteration : 1304
train acc:  0.75
train loss:  0.45850545167922974
train gradient:  0.11111103841255116
iteration : 1305
train acc:  0.8203125
train loss:  0.35738053917884827
train gradient:  0.08060618421264051
iteration : 1306
train acc:  0.796875
train loss:  0.4467645287513733
train gradient:  0.11886810565004612
iteration : 1307
train acc:  0.75
train loss:  0.4880964756011963
train gradient:  0.11689045663580434
iteration : 1308
train acc:  0.8046875
train loss:  0.46203136444091797
train gradient:  0.1046799833902098
iteration : 1309
train acc:  0.6796875
train loss:  0.536878228187561
train gradient:  0.1620507670085205
iteration : 1310
train acc:  0.7109375
train loss:  0.49362796545028687
train gradient:  0.13429979833730238
iteration : 1311
train acc:  0.75
train loss:  0.4222470223903656
train gradient:  0.08845022637761472
iteration : 1312
train acc:  0.796875
train loss:  0.48278599977493286
train gradient:  0.11186523780027992
iteration : 1313
train acc:  0.78125
train loss:  0.46075546741485596
train gradient:  0.12340898543766322
iteration : 1314
train acc:  0.765625
train loss:  0.4777676463127136
train gradient:  0.09765354943146509
iteration : 1315
train acc:  0.7734375
train loss:  0.4722391366958618
train gradient:  0.10300888540338755
iteration : 1316
train acc:  0.734375
train loss:  0.45775896310806274
train gradient:  0.1052020269147556
iteration : 1317
train acc:  0.7421875
train loss:  0.48388615250587463
train gradient:  0.11178858626276673
iteration : 1318
train acc:  0.734375
train loss:  0.52364182472229
train gradient:  0.12176028292750772
iteration : 1319
train acc:  0.7890625
train loss:  0.4647442698478699
train gradient:  0.10995755706744395
iteration : 1320
train acc:  0.7734375
train loss:  0.4824554920196533
train gradient:  0.1253864405961655
iteration : 1321
train acc:  0.78125
train loss:  0.4572939872741699
train gradient:  0.12521316773731334
iteration : 1322
train acc:  0.7734375
train loss:  0.453258216381073
train gradient:  0.10372330689521812
iteration : 1323
train acc:  0.7578125
train loss:  0.4877856373786926
train gradient:  0.11066788011375722
iteration : 1324
train acc:  0.7578125
train loss:  0.5268387198448181
train gradient:  0.13871918070874578
iteration : 1325
train acc:  0.796875
train loss:  0.4468267560005188
train gradient:  0.10429070146349387
iteration : 1326
train acc:  0.7890625
train loss:  0.4375874698162079
train gradient:  0.10579638335846835
iteration : 1327
train acc:  0.7578125
train loss:  0.4732668399810791
train gradient:  0.10802718566807024
iteration : 1328
train acc:  0.828125
train loss:  0.3768617510795593
train gradient:  0.06930087504446178
iteration : 1329
train acc:  0.7578125
train loss:  0.47355058789253235
train gradient:  0.12326323267618804
iteration : 1330
train acc:  0.765625
train loss:  0.48092061281204224
train gradient:  0.15297115998552602
iteration : 1331
train acc:  0.6796875
train loss:  0.5554050207138062
train gradient:  0.16437082136441
iteration : 1332
train acc:  0.6875
train loss:  0.545356273651123
train gradient:  0.1672912863344948
iteration : 1333
train acc:  0.7421875
train loss:  0.4625370502471924
train gradient:  0.11909218415676633
iteration : 1334
train acc:  0.71875
train loss:  0.4803575277328491
train gradient:  0.11002292712093302
iteration : 1335
train acc:  0.71875
train loss:  0.5596851706504822
train gradient:  0.15870333426809397
iteration : 1336
train acc:  0.71875
train loss:  0.5019477605819702
train gradient:  0.12518311287587125
iteration : 1337
train acc:  0.75
train loss:  0.4895492196083069
train gradient:  0.12054614196708457
iteration : 1338
train acc:  0.765625
train loss:  0.5067968368530273
train gradient:  0.17524078650326375
iteration : 1339
train acc:  0.75
train loss:  0.4594411253929138
train gradient:  0.10230343335508137
iteration : 1340
train acc:  0.8125
train loss:  0.4067572057247162
train gradient:  0.0841505164612426
iteration : 1341
train acc:  0.7421875
train loss:  0.5177872776985168
train gradient:  0.14201406481443996
iteration : 1342
train acc:  0.6875
train loss:  0.6144276857376099
train gradient:  0.21988339731541806
iteration : 1343
train acc:  0.8046875
train loss:  0.4432010352611542
train gradient:  0.12035765270313277
iteration : 1344
train acc:  0.78125
train loss:  0.4522766172885895
train gradient:  0.12466059089885242
iteration : 1345
train acc:  0.6953125
train loss:  0.5444779992103577
train gradient:  0.16197775526687502
iteration : 1346
train acc:  0.7265625
train loss:  0.5057468414306641
train gradient:  0.1379353552053143
iteration : 1347
train acc:  0.796875
train loss:  0.47816532850265503
train gradient:  0.1313648021745611
iteration : 1348
train acc:  0.703125
train loss:  0.5271273851394653
train gradient:  0.15362009693245235
iteration : 1349
train acc:  0.671875
train loss:  0.6297533512115479
train gradient:  0.15919615046796798
iteration : 1350
train acc:  0.765625
train loss:  0.4714091718196869
train gradient:  0.12628655612672082
iteration : 1351
train acc:  0.7265625
train loss:  0.5098974704742432
train gradient:  0.11507946372864755
iteration : 1352
train acc:  0.7890625
train loss:  0.43882107734680176
train gradient:  0.10900808323059422
iteration : 1353
train acc:  0.7265625
train loss:  0.5121363401412964
train gradient:  0.13949465487692075
iteration : 1354
train acc:  0.796875
train loss:  0.42387261986732483
train gradient:  0.08852133731179061
iteration : 1355
train acc:  0.7265625
train loss:  0.4843878149986267
train gradient:  0.13559316157260123
iteration : 1356
train acc:  0.78125
train loss:  0.44211477041244507
train gradient:  0.10963422156047185
iteration : 1357
train acc:  0.8203125
train loss:  0.41095975041389465
train gradient:  0.11665866726996126
iteration : 1358
train acc:  0.7265625
train loss:  0.49165475368499756
train gradient:  0.09882851303343879
iteration : 1359
train acc:  0.7578125
train loss:  0.4735558331012726
train gradient:  0.14848231784342322
iteration : 1360
train acc:  0.7109375
train loss:  0.5214956998825073
train gradient:  0.2145164045795794
iteration : 1361
train acc:  0.7734375
train loss:  0.4775255620479584
train gradient:  0.1667129552569367
iteration : 1362
train acc:  0.7109375
train loss:  0.4747198224067688
train gradient:  0.11686721973338222
iteration : 1363
train acc:  0.6640625
train loss:  0.5434608459472656
train gradient:  0.13977735072588748
iteration : 1364
train acc:  0.75
train loss:  0.49330857396125793
train gradient:  0.11197106940431474
iteration : 1365
train acc:  0.734375
train loss:  0.5042387247085571
train gradient:  0.12803883006208272
iteration : 1366
train acc:  0.734375
train loss:  0.45584845542907715
train gradient:  0.11326419612423853
iteration : 1367
train acc:  0.6796875
train loss:  0.5690695643424988
train gradient:  0.18155788627311148
iteration : 1368
train acc:  0.7421875
train loss:  0.5440129041671753
train gradient:  0.13055015072027987
iteration : 1369
train acc:  0.7734375
train loss:  0.4871981143951416
train gradient:  0.1385570340833729
iteration : 1370
train acc:  0.7265625
train loss:  0.43997055292129517
train gradient:  0.08002251847168446
iteration : 1371
train acc:  0.7578125
train loss:  0.43631547689437866
train gradient:  0.09707387938566625
iteration : 1372
train acc:  0.75
train loss:  0.48801979422569275
train gradient:  0.11302026694484064
iteration : 1373
train acc:  0.7421875
train loss:  0.5010328888893127
train gradient:  0.14586134550978458
iteration : 1374
train acc:  0.6953125
train loss:  0.5595340728759766
train gradient:  0.17377502922894789
iteration : 1375
train acc:  0.71875
train loss:  0.525243878364563
train gradient:  0.11406700881931386
iteration : 1376
train acc:  0.7734375
train loss:  0.4367005228996277
train gradient:  0.08773162953352144
iteration : 1377
train acc:  0.671875
train loss:  0.5115294456481934
train gradient:  0.13963982740493258
iteration : 1378
train acc:  0.71875
train loss:  0.5088815093040466
train gradient:  0.12114106612007866
iteration : 1379
train acc:  0.7265625
train loss:  0.48776769638061523
train gradient:  0.11473273887796928
iteration : 1380
train acc:  0.7734375
train loss:  0.4395846128463745
train gradient:  0.10691213691873375
iteration : 1381
train acc:  0.7109375
train loss:  0.4551365077495575
train gradient:  0.10942573015114336
iteration : 1382
train acc:  0.765625
train loss:  0.4846597909927368
train gradient:  0.12579114022453824
iteration : 1383
train acc:  0.78125
train loss:  0.43932533264160156
train gradient:  0.11173166694242297
iteration : 1384
train acc:  0.78125
train loss:  0.4471431076526642
train gradient:  0.14739470351405798
iteration : 1385
train acc:  0.7421875
train loss:  0.4818490445613861
train gradient:  0.11682951033557008
iteration : 1386
train acc:  0.78125
train loss:  0.4626466631889343
train gradient:  0.10192886159808857
iteration : 1387
train acc:  0.7734375
train loss:  0.4686110317707062
train gradient:  0.1009847435903009
iteration : 1388
train acc:  0.7421875
train loss:  0.45941078662872314
train gradient:  0.11504585980890104
iteration : 1389
train acc:  0.734375
train loss:  0.47200194001197815
train gradient:  0.11406811284213621
iteration : 1390
train acc:  0.75
train loss:  0.5065635442733765
train gradient:  0.12592267967514345
iteration : 1391
train acc:  0.71875
train loss:  0.4955645799636841
train gradient:  0.12710408208856094
iteration : 1392
train acc:  0.7890625
train loss:  0.4482121467590332
train gradient:  0.10504145228136597
iteration : 1393
train acc:  0.7421875
train loss:  0.4846741557121277
train gradient:  0.13555634209495535
iteration : 1394
train acc:  0.8359375
train loss:  0.3883857727050781
train gradient:  0.08037619816539082
iteration : 1395
train acc:  0.7421875
train loss:  0.48771965503692627
train gradient:  0.12263678655329388
iteration : 1396
train acc:  0.796875
train loss:  0.45264971256256104
train gradient:  0.1151165615395221
iteration : 1397
train acc:  0.7265625
train loss:  0.5135651230812073
train gradient:  0.15671506228800436
iteration : 1398
train acc:  0.7109375
train loss:  0.5254031419754028
train gradient:  0.13951336115164464
iteration : 1399
train acc:  0.7109375
train loss:  0.48258817195892334
train gradient:  0.14945612446243353
iteration : 1400
train acc:  0.828125
train loss:  0.44706597924232483
train gradient:  0.1317052443873627
iteration : 1401
train acc:  0.8046875
train loss:  0.44251543283462524
train gradient:  0.11089351673006285
iteration : 1402
train acc:  0.6953125
train loss:  0.5365604758262634
train gradient:  0.134301802732256
iteration : 1403
train acc:  0.671875
train loss:  0.5384472012519836
train gradient:  0.14789503152039174
iteration : 1404
train acc:  0.765625
train loss:  0.46956968307495117
train gradient:  0.1345489945394996
iteration : 1405
train acc:  0.6796875
train loss:  0.5512682795524597
train gradient:  0.17891986572542312
iteration : 1406
train acc:  0.7421875
train loss:  0.48528045415878296
train gradient:  0.11971913790869373
iteration : 1407
train acc:  0.7109375
train loss:  0.5842359066009521
train gradient:  0.14587385131062286
iteration : 1408
train acc:  0.7578125
train loss:  0.43586593866348267
train gradient:  0.09692667708214076
iteration : 1409
train acc:  0.734375
train loss:  0.5050017237663269
train gradient:  0.14630859349127928
iteration : 1410
train acc:  0.7578125
train loss:  0.5046299695968628
train gradient:  0.14416333591400868
iteration : 1411
train acc:  0.734375
train loss:  0.45218709111213684
train gradient:  0.11706938493869883
iteration : 1412
train acc:  0.78125
train loss:  0.401603639125824
train gradient:  0.08346946865598932
iteration : 1413
train acc:  0.765625
train loss:  0.5082626938819885
train gradient:  0.1389262559361052
iteration : 1414
train acc:  0.7578125
train loss:  0.5417993068695068
train gradient:  0.1332701251977505
iteration : 1415
train acc:  0.7265625
train loss:  0.5151684880256653
train gradient:  0.1428206543687324
iteration : 1416
train acc:  0.7265625
train loss:  0.4891234040260315
train gradient:  0.09705893355909254
iteration : 1417
train acc:  0.7265625
train loss:  0.48763135075569153
train gradient:  0.123629966342537
iteration : 1418
train acc:  0.765625
train loss:  0.45618176460266113
train gradient:  0.11646328122520153
iteration : 1419
train acc:  0.7265625
train loss:  0.530913233757019
train gradient:  0.12155044493421673
iteration : 1420
train acc:  0.7421875
train loss:  0.4951840043067932
train gradient:  0.142791231283494
iteration : 1421
train acc:  0.75
train loss:  0.4441491365432739
train gradient:  0.11294722214156579
iteration : 1422
train acc:  0.7734375
train loss:  0.47904014587402344
train gradient:  0.15245556696555818
iteration : 1423
train acc:  0.7734375
train loss:  0.4624263644218445
train gradient:  0.12672936922837832
iteration : 1424
train acc:  0.8046875
train loss:  0.4322540760040283
train gradient:  0.08973161618724722
iteration : 1425
train acc:  0.6875
train loss:  0.563796877861023
train gradient:  0.13949018387220388
iteration : 1426
train acc:  0.703125
train loss:  0.5491424798965454
train gradient:  0.1540770302741596
iteration : 1427
train acc:  0.75
train loss:  0.45016688108444214
train gradient:  0.11591436421095934
iteration : 1428
train acc:  0.7734375
train loss:  0.42564064264297485
train gradient:  0.09692462559141295
iteration : 1429
train acc:  0.7265625
train loss:  0.5016545057296753
train gradient:  0.11994857489200585
iteration : 1430
train acc:  0.71875
train loss:  0.5603628158569336
train gradient:  0.15885860513023953
iteration : 1431
train acc:  0.71875
train loss:  0.4937242269515991
train gradient:  0.10653975840966
iteration : 1432
train acc:  0.78125
train loss:  0.40348148345947266
train gradient:  0.10060464019593104
iteration : 1433
train acc:  0.8203125
train loss:  0.439731240272522
train gradient:  0.10419416224146778
iteration : 1434
train acc:  0.71875
train loss:  0.4725795090198517
train gradient:  0.10900755395509944
iteration : 1435
train acc:  0.703125
train loss:  0.5512989163398743
train gradient:  0.13744446780806294
iteration : 1436
train acc:  0.7734375
train loss:  0.4527391791343689
train gradient:  0.10685960660091103
iteration : 1437
train acc:  0.78125
train loss:  0.43583381175994873
train gradient:  0.102735555936785
iteration : 1438
train acc:  0.7734375
train loss:  0.4590071141719818
train gradient:  0.10711960229678559
iteration : 1439
train acc:  0.6875
train loss:  0.5777857899665833
train gradient:  0.16589643673416
iteration : 1440
train acc:  0.7890625
train loss:  0.4810437262058258
train gradient:  0.1448397051156553
iteration : 1441
train acc:  0.7890625
train loss:  0.4779975414276123
train gradient:  0.10482756666638307
iteration : 1442
train acc:  0.7265625
train loss:  0.47843801975250244
train gradient:  0.11054700784464916
iteration : 1443
train acc:  0.7421875
train loss:  0.5502889156341553
train gradient:  0.15501270211888396
iteration : 1444
train acc:  0.703125
train loss:  0.5075513124465942
train gradient:  0.1260455211533755
iteration : 1445
train acc:  0.65625
train loss:  0.5847032070159912
train gradient:  0.17331868840892578
iteration : 1446
train acc:  0.7734375
train loss:  0.4584440588951111
train gradient:  0.10734740704958697
iteration : 1447
train acc:  0.671875
train loss:  0.5138852596282959
train gradient:  0.12288156771846212
iteration : 1448
train acc:  0.8046875
train loss:  0.47210782766342163
train gradient:  0.12233557860489874
iteration : 1449
train acc:  0.7421875
train loss:  0.44347402453422546
train gradient:  0.08660599088663419
iteration : 1450
train acc:  0.6796875
train loss:  0.5313675999641418
train gradient:  0.1524818555215841
iteration : 1451
train acc:  0.734375
train loss:  0.5303771495819092
train gradient:  0.12145704956593543
iteration : 1452
train acc:  0.75
train loss:  0.4931630790233612
train gradient:  0.12623684684314182
iteration : 1453
train acc:  0.765625
train loss:  0.5005689263343811
train gradient:  0.12108528646759101
iteration : 1454
train acc:  0.7734375
train loss:  0.47653061151504517
train gradient:  0.13985978588579345
iteration : 1455
train acc:  0.8125
train loss:  0.429015189409256
train gradient:  0.10427097660911723
iteration : 1456
train acc:  0.734375
train loss:  0.5691218376159668
train gradient:  0.18316843917413594
iteration : 1457
train acc:  0.7578125
train loss:  0.4903845191001892
train gradient:  0.15323823983070373
iteration : 1458
train acc:  0.7421875
train loss:  0.4658504128456116
train gradient:  0.11322330956779181
iteration : 1459
train acc:  0.765625
train loss:  0.43949633836746216
train gradient:  0.10588356084466186
iteration : 1460
train acc:  0.734375
train loss:  0.4768117666244507
train gradient:  0.12466826361924895
iteration : 1461
train acc:  0.7890625
train loss:  0.4888649880886078
train gradient:  0.1473691094220524
iteration : 1462
train acc:  0.7265625
train loss:  0.49853721261024475
train gradient:  0.14084248295950158
iteration : 1463
train acc:  0.734375
train loss:  0.513762354850769
train gradient:  0.12778724102114364
iteration : 1464
train acc:  0.703125
train loss:  0.5411213040351868
train gradient:  0.13417836948694642
iteration : 1465
train acc:  0.7265625
train loss:  0.4593956470489502
train gradient:  0.1163337576564934
iteration : 1466
train acc:  0.7265625
train loss:  0.4989602565765381
train gradient:  0.10195533507322463
iteration : 1467
train acc:  0.765625
train loss:  0.42170190811157227
train gradient:  0.11668636909470909
iteration : 1468
train acc:  0.8046875
train loss:  0.4433950185775757
train gradient:  0.09204067401142087
iteration : 1469
train acc:  0.6953125
train loss:  0.5266246795654297
train gradient:  0.12530429655115122
iteration : 1470
train acc:  0.7578125
train loss:  0.4676212966442108
train gradient:  0.11783072903228932
iteration : 1471
train acc:  0.765625
train loss:  0.4486989676952362
train gradient:  0.0946340449837171
iteration : 1472
train acc:  0.765625
train loss:  0.5392515659332275
train gradient:  0.14633233539850962
iteration : 1473
train acc:  0.6953125
train loss:  0.5263914465904236
train gradient:  0.12090015590135185
iteration : 1474
train acc:  0.6953125
train loss:  0.5272185802459717
train gradient:  0.16175773425491485
iteration : 1475
train acc:  0.828125
train loss:  0.4213756024837494
train gradient:  0.09941907943216931
iteration : 1476
train acc:  0.71875
train loss:  0.5187113285064697
train gradient:  0.12555799146038588
iteration : 1477
train acc:  0.78125
train loss:  0.43269509077072144
train gradient:  0.09406380639424576
iteration : 1478
train acc:  0.6875
train loss:  0.5103099346160889
train gradient:  0.12619773222438832
iteration : 1479
train acc:  0.765625
train loss:  0.45292192697525024
train gradient:  0.09107924735623919
iteration : 1480
train acc:  0.765625
train loss:  0.48804521560668945
train gradient:  0.12027417075900569
iteration : 1481
train acc:  0.78125
train loss:  0.4689275026321411
train gradient:  0.1101194717139409
iteration : 1482
train acc:  0.7265625
train loss:  0.480040967464447
train gradient:  0.10819040026517811
iteration : 1483
train acc:  0.7578125
train loss:  0.4883071184158325
train gradient:  0.14244665805284623
iteration : 1484
train acc:  0.7421875
train loss:  0.4994772970676422
train gradient:  0.10978581456053302
iteration : 1485
train acc:  0.71875
train loss:  0.5562781095504761
train gradient:  0.18769832489819702
iteration : 1486
train acc:  0.796875
train loss:  0.444500207901001
train gradient:  0.1270000069959751
iteration : 1487
train acc:  0.6875
train loss:  0.5246385335922241
train gradient:  0.15107640020668492
iteration : 1488
train acc:  0.7578125
train loss:  0.46242550015449524
train gradient:  0.11962707314971797
iteration : 1489
train acc:  0.75
train loss:  0.45039471983909607
train gradient:  0.09553955075295217
iteration : 1490
train acc:  0.7578125
train loss:  0.45265793800354004
train gradient:  0.09053647991989208
iteration : 1491
train acc:  0.7578125
train loss:  0.45122116804122925
train gradient:  0.10652753927106345
iteration : 1492
train acc:  0.75
train loss:  0.4728662967681885
train gradient:  0.1224734078414227
iteration : 1493
train acc:  0.7734375
train loss:  0.457585871219635
train gradient:  0.11781770428534334
iteration : 1494
train acc:  0.78125
train loss:  0.4630313515663147
train gradient:  0.1174767755878042
iteration : 1495
train acc:  0.78125
train loss:  0.43444180488586426
train gradient:  0.11851119979836952
iteration : 1496
train acc:  0.7109375
train loss:  0.49977394938468933
train gradient:  0.12574680943733285
iteration : 1497
train acc:  0.7421875
train loss:  0.4768120348453522
train gradient:  0.11180953268703737
iteration : 1498
train acc:  0.78125
train loss:  0.4530554711818695
train gradient:  0.1012295152903083
iteration : 1499
train acc:  0.734375
train loss:  0.5115218162536621
train gradient:  0.13481205616042063
iteration : 1500
train acc:  0.71875
train loss:  0.512057363986969
train gradient:  0.11656641310612484
iteration : 1501
train acc:  0.796875
train loss:  0.4576759934425354
train gradient:  0.11437321655698172
iteration : 1502
train acc:  0.7421875
train loss:  0.48382535576820374
train gradient:  0.1137899863260458
iteration : 1503
train acc:  0.734375
train loss:  0.46592581272125244
train gradient:  0.09836317653372471
iteration : 1504
train acc:  0.7734375
train loss:  0.4614945650100708
train gradient:  0.09396023824038129
iteration : 1505
train acc:  0.7734375
train loss:  0.4929608404636383
train gradient:  0.13032148929688533
iteration : 1506
train acc:  0.7265625
train loss:  0.547112226486206
train gradient:  0.13024746297474965
iteration : 1507
train acc:  0.796875
train loss:  0.4382071793079376
train gradient:  0.09960119865670167
iteration : 1508
train acc:  0.734375
train loss:  0.5519232749938965
train gradient:  0.1409148016647252
iteration : 1509
train acc:  0.7421875
train loss:  0.47395026683807373
train gradient:  0.12457939082127444
iteration : 1510
train acc:  0.7265625
train loss:  0.46931737661361694
train gradient:  0.12637549427902
iteration : 1511
train acc:  0.6953125
train loss:  0.5671942234039307
train gradient:  0.15591706437645464
iteration : 1512
train acc:  0.75
train loss:  0.48823660612106323
train gradient:  0.12467918018491563
iteration : 1513
train acc:  0.6875
train loss:  0.49642401933670044
train gradient:  0.14090389702695175
iteration : 1514
train acc:  0.7109375
train loss:  0.4988682270050049
train gradient:  0.15188783877101664
iteration : 1515
train acc:  0.75
train loss:  0.4874725043773651
train gradient:  0.12016842288240678
iteration : 1516
train acc:  0.7265625
train loss:  0.5387485027313232
train gradient:  0.17562877786986406
iteration : 1517
train acc:  0.734375
train loss:  0.5011669993400574
train gradient:  0.14405070500561265
iteration : 1518
train acc:  0.828125
train loss:  0.4055507779121399
train gradient:  0.08734842375252574
iteration : 1519
train acc:  0.7734375
train loss:  0.4581429362297058
train gradient:  0.12289287362940476
iteration : 1520
train acc:  0.7578125
train loss:  0.49389463663101196
train gradient:  0.12426581023305994
iteration : 1521
train acc:  0.7578125
train loss:  0.4541926383972168
train gradient:  0.09883965779009145
iteration : 1522
train acc:  0.7265625
train loss:  0.5032159686088562
train gradient:  0.13852861589073884
iteration : 1523
train acc:  0.734375
train loss:  0.47760510444641113
train gradient:  0.09344057500152621
iteration : 1524
train acc:  0.8203125
train loss:  0.460828959941864
train gradient:  0.11632982236674794
iteration : 1525
train acc:  0.6953125
train loss:  0.5014901757240295
train gradient:  0.1173873119626456
iteration : 1526
train acc:  0.671875
train loss:  0.5206530094146729
train gradient:  0.0996154360461133
iteration : 1527
train acc:  0.78125
train loss:  0.4793909788131714
train gradient:  0.1037445594085015
iteration : 1528
train acc:  0.796875
train loss:  0.4761827290058136
train gradient:  0.12767589524814837
iteration : 1529
train acc:  0.7265625
train loss:  0.47636669874191284
train gradient:  0.10307838078488975
iteration : 1530
train acc:  0.7109375
train loss:  0.5007790923118591
train gradient:  0.1409129357538278
iteration : 1531
train acc:  0.734375
train loss:  0.5211770534515381
train gradient:  0.14524692499589298
iteration : 1532
train acc:  0.7734375
train loss:  0.44560176134109497
train gradient:  0.09956159364952533
iteration : 1533
train acc:  0.71875
train loss:  0.4995940327644348
train gradient:  0.17913417703294718
iteration : 1534
train acc:  0.78125
train loss:  0.43743735551834106
train gradient:  0.09010620634343568
iteration : 1535
train acc:  0.7734375
train loss:  0.4290705919265747
train gradient:  0.08674251379963274
iteration : 1536
train acc:  0.8046875
train loss:  0.42434048652648926
train gradient:  0.11793570463597357
iteration : 1537
train acc:  0.7578125
train loss:  0.47230178117752075
train gradient:  0.10209816307729996
iteration : 1538
train acc:  0.7890625
train loss:  0.462268590927124
train gradient:  0.11843241306383032
iteration : 1539
train acc:  0.7421875
train loss:  0.49760404229164124
train gradient:  0.1244023581260625
iteration : 1540
train acc:  0.7734375
train loss:  0.4537334442138672
train gradient:  0.11052115570668215
iteration : 1541
train acc:  0.75
train loss:  0.4893496334552765
train gradient:  0.16724178222285715
iteration : 1542
train acc:  0.8046875
train loss:  0.36109986901283264
train gradient:  0.06785682392545114
iteration : 1543
train acc:  0.7421875
train loss:  0.4621969163417816
train gradient:  0.10801818537495068
iteration : 1544
train acc:  0.734375
train loss:  0.49670639634132385
train gradient:  0.1363853882675454
iteration : 1545
train acc:  0.7109375
train loss:  0.5260305404663086
train gradient:  0.1403100082436052
iteration : 1546
train acc:  0.7265625
train loss:  0.49120408296585083
train gradient:  0.13848027510356997
iteration : 1547
train acc:  0.75
train loss:  0.45517194271087646
train gradient:  0.12836690304924642
iteration : 1548
train acc:  0.765625
train loss:  0.49122223258018494
train gradient:  0.1273743215428053
iteration : 1549
train acc:  0.8125
train loss:  0.4503301978111267
train gradient:  0.12140050787351976
iteration : 1550
train acc:  0.71875
train loss:  0.4993988573551178
train gradient:  0.13137347947743241
iteration : 1551
train acc:  0.71875
train loss:  0.49549055099487305
train gradient:  0.12364660958103514
iteration : 1552
train acc:  0.6875
train loss:  0.5440956950187683
train gradient:  0.13810961974069652
iteration : 1553
train acc:  0.6875
train loss:  0.5813536643981934
train gradient:  0.15951736573397715
iteration : 1554
train acc:  0.7734375
train loss:  0.42108216881752014
train gradient:  0.10429530032118528
iteration : 1555
train acc:  0.7578125
train loss:  0.4872472286224365
train gradient:  0.12276442550363063
iteration : 1556
train acc:  0.65625
train loss:  0.5651875138282776
train gradient:  0.1738348945042696
iteration : 1557
train acc:  0.7421875
train loss:  0.4828724265098572
train gradient:  0.11272609867385368
iteration : 1558
train acc:  0.78125
train loss:  0.4066172242164612
train gradient:  0.0984301681532451
iteration : 1559
train acc:  0.734375
train loss:  0.46266716718673706
train gradient:  0.10178056731146001
iteration : 1560
train acc:  0.7734375
train loss:  0.43720316886901855
train gradient:  0.09194283659499711
iteration : 1561
train acc:  0.7578125
train loss:  0.46458473801612854
train gradient:  0.0967706512259569
iteration : 1562
train acc:  0.765625
train loss:  0.4664795398712158
train gradient:  0.08965319988956427
iteration : 1563
train acc:  0.7421875
train loss:  0.502267599105835
train gradient:  0.1348016067638667
iteration : 1564
train acc:  0.7421875
train loss:  0.48709625005722046
train gradient:  0.11258496090449266
iteration : 1565
train acc:  0.7578125
train loss:  0.4780274033546448
train gradient:  0.12546836352470406
iteration : 1566
train acc:  0.65625
train loss:  0.5658600330352783
train gradient:  0.20223734063701357
iteration : 1567
train acc:  0.734375
train loss:  0.4909648895263672
train gradient:  0.12960451587752897
iteration : 1568
train acc:  0.71875
train loss:  0.5107921361923218
train gradient:  0.11397176697040774
iteration : 1569
train acc:  0.7734375
train loss:  0.4587225317955017
train gradient:  0.12786848305877774
iteration : 1570
train acc:  0.8125
train loss:  0.4264147877693176
train gradient:  0.09132123044996607
iteration : 1571
train acc:  0.765625
train loss:  0.5004884004592896
train gradient:  0.11091451438909497
iteration : 1572
train acc:  0.71875
train loss:  0.6473478674888611
train gradient:  0.19139934270302716
iteration : 1573
train acc:  0.8359375
train loss:  0.410297691822052
train gradient:  0.0836279113953321
iteration : 1574
train acc:  0.671875
train loss:  0.5028225779533386
train gradient:  0.13395266708904016
iteration : 1575
train acc:  0.7109375
train loss:  0.5362558960914612
train gradient:  0.13505495343611684
iteration : 1576
train acc:  0.6953125
train loss:  0.5373114943504333
train gradient:  0.13243536493282215
iteration : 1577
train acc:  0.671875
train loss:  0.5413650274276733
train gradient:  0.14263291906805647
iteration : 1578
train acc:  0.765625
train loss:  0.4510699510574341
train gradient:  0.10333774459638656
iteration : 1579
train acc:  0.734375
train loss:  0.44972914457321167
train gradient:  0.10743578077800488
iteration : 1580
train acc:  0.7421875
train loss:  0.47456759214401245
train gradient:  0.12923927523448897
iteration : 1581
train acc:  0.734375
train loss:  0.438961386680603
train gradient:  0.09333313714139556
iteration : 1582
train acc:  0.7109375
train loss:  0.5238362550735474
train gradient:  0.14953881519474693
iteration : 1583
train acc:  0.765625
train loss:  0.44019925594329834
train gradient:  0.11100412281699754
iteration : 1584
train acc:  0.7578125
train loss:  0.4801008701324463
train gradient:  0.10998054917717362
iteration : 1585
train acc:  0.7578125
train loss:  0.45624375343322754
train gradient:  0.11636366392757454
iteration : 1586
train acc:  0.796875
train loss:  0.4317436218261719
train gradient:  0.09996538715327662
iteration : 1587
train acc:  0.734375
train loss:  0.5308794975280762
train gradient:  0.13030403135785681
iteration : 1588
train acc:  0.6953125
train loss:  0.47841575741767883
train gradient:  0.11588228618382498
iteration : 1589
train acc:  0.71875
train loss:  0.503068208694458
train gradient:  0.133999310620459
iteration : 1590
train acc:  0.8046875
train loss:  0.48235535621643066
train gradient:  0.11805750593873957
iteration : 1591
train acc:  0.765625
train loss:  0.46288689970970154
train gradient:  0.09581331055832719
iteration : 1592
train acc:  0.765625
train loss:  0.48671838641166687
train gradient:  0.12614802450694942
iteration : 1593
train acc:  0.765625
train loss:  0.44862234592437744
train gradient:  0.09984977769270793
iteration : 1594
train acc:  0.7734375
train loss:  0.48961716890335083
train gradient:  0.11012731124389576
iteration : 1595
train acc:  0.765625
train loss:  0.4522593021392822
train gradient:  0.08708610301747574
iteration : 1596
train acc:  0.703125
train loss:  0.5672428607940674
train gradient:  0.13292361179605428
iteration : 1597
train acc:  0.75
train loss:  0.5003816485404968
train gradient:  0.13311466632765745
iteration : 1598
train acc:  0.7421875
train loss:  0.5005847215652466
train gradient:  0.1331472919467147
iteration : 1599
train acc:  0.734375
train loss:  0.5251075625419617
train gradient:  0.17204472430083195
iteration : 1600
train acc:  0.71875
train loss:  0.4930119514465332
train gradient:  0.09369118046365943
iteration : 1601
train acc:  0.8125
train loss:  0.4209076464176178
train gradient:  0.09063889136148084
iteration : 1602
train acc:  0.7421875
train loss:  0.47016897797584534
train gradient:  0.11018820253122194
iteration : 1603
train acc:  0.8046875
train loss:  0.4613294005393982
train gradient:  0.11386492816286041
iteration : 1604
train acc:  0.7421875
train loss:  0.48610928654670715
train gradient:  0.11454704168364549
iteration : 1605
train acc:  0.78125
train loss:  0.4471362233161926
train gradient:  0.12117617558987316
iteration : 1606
train acc:  0.8125
train loss:  0.40030696988105774
train gradient:  0.08894197792910649
iteration : 1607
train acc:  0.71875
train loss:  0.5395491123199463
train gradient:  0.1586909680342199
iteration : 1608
train acc:  0.8125
train loss:  0.4631381034851074
train gradient:  0.1200063897091697
iteration : 1609
train acc:  0.6796875
train loss:  0.5297945141792297
train gradient:  0.16658129389269793
iteration : 1610
train acc:  0.703125
train loss:  0.5093625783920288
train gradient:  0.13989407041248775
iteration : 1611
train acc:  0.65625
train loss:  0.5504984855651855
train gradient:  0.13167048355926242
iteration : 1612
train acc:  0.765625
train loss:  0.49577948451042175
train gradient:  0.13190801541690156
iteration : 1613
train acc:  0.7578125
train loss:  0.490153431892395
train gradient:  0.10733926340246436
iteration : 1614
train acc:  0.703125
train loss:  0.5822934508323669
train gradient:  0.18625607227713348
iteration : 1615
train acc:  0.71875
train loss:  0.5385807752609253
train gradient:  0.14808429036630483
iteration : 1616
train acc:  0.7890625
train loss:  0.42956140637397766
train gradient:  0.1261375371502268
iteration : 1617
train acc:  0.7421875
train loss:  0.45608043670654297
train gradient:  0.1319291390588988
iteration : 1618
train acc:  0.7578125
train loss:  0.4822583794593811
train gradient:  0.11500989170380517
iteration : 1619
train acc:  0.734375
train loss:  0.4958934485912323
train gradient:  0.13698543313252906
iteration : 1620
train acc:  0.7578125
train loss:  0.43424874544143677
train gradient:  0.08815552225323704
iteration : 1621
train acc:  0.78125
train loss:  0.4326038658618927
train gradient:  0.10704277960312095
iteration : 1622
train acc:  0.7578125
train loss:  0.46751633286476135
train gradient:  0.11460565811828942
iteration : 1623
train acc:  0.84375
train loss:  0.40493422746658325
train gradient:  0.07884484577496485
iteration : 1624
train acc:  0.71875
train loss:  0.491079181432724
train gradient:  0.1064284766334784
iteration : 1625
train acc:  0.7421875
train loss:  0.4348450303077698
train gradient:  0.10082253917091383
iteration : 1626
train acc:  0.84375
train loss:  0.3770262897014618
train gradient:  0.07200138229334156
iteration : 1627
train acc:  0.7734375
train loss:  0.44516101479530334
train gradient:  0.11907905639008055
iteration : 1628
train acc:  0.7578125
train loss:  0.4826863408088684
train gradient:  0.12446941381132345
iteration : 1629
train acc:  0.78125
train loss:  0.4863682985305786
train gradient:  0.1244902250972027
iteration : 1630
train acc:  0.7421875
train loss:  0.4739913046360016
train gradient:  0.11197426125058214
iteration : 1631
train acc:  0.7890625
train loss:  0.4698566198348999
train gradient:  0.11401290602503757
iteration : 1632
train acc:  0.8046875
train loss:  0.4292148947715759
train gradient:  0.08120219912051349
iteration : 1633
train acc:  0.796875
train loss:  0.44842541217803955
train gradient:  0.11332871903269205
iteration : 1634
train acc:  0.765625
train loss:  0.4597114026546478
train gradient:  0.09115924667175439
iteration : 1635
train acc:  0.7578125
train loss:  0.49792835116386414
train gradient:  0.12709745928138116
iteration : 1636
train acc:  0.78125
train loss:  0.4698016047477722
train gradient:  0.14945270750577808
iteration : 1637
train acc:  0.7421875
train loss:  0.49705231189727783
train gradient:  0.1152062491914463
iteration : 1638
train acc:  0.7578125
train loss:  0.543029248714447
train gradient:  0.1738459015021321
iteration : 1639
train acc:  0.71875
train loss:  0.4951753616333008
train gradient:  0.12885512388226739
iteration : 1640
train acc:  0.7421875
train loss:  0.5150715112686157
train gradient:  0.15227319117755325
iteration : 1641
train acc:  0.7578125
train loss:  0.4511968493461609
train gradient:  0.11360145582449774
iteration : 1642
train acc:  0.8046875
train loss:  0.43129420280456543
train gradient:  0.10572276798226203
iteration : 1643
train acc:  0.7421875
train loss:  0.49448829889297485
train gradient:  0.10789217932883526
iteration : 1644
train acc:  0.75
train loss:  0.495882511138916
train gradient:  0.1137570666904241
iteration : 1645
train acc:  0.7109375
train loss:  0.4964175820350647
train gradient:  0.10480955685583138
iteration : 1646
train acc:  0.8359375
train loss:  0.40969109535217285
train gradient:  0.09776544269202483
iteration : 1647
train acc:  0.8046875
train loss:  0.3877069354057312
train gradient:  0.09025689235296712
iteration : 1648
train acc:  0.7578125
train loss:  0.4719535708427429
train gradient:  0.09906739318873851
iteration : 1649
train acc:  0.7890625
train loss:  0.4170491099357605
train gradient:  0.11445455103337233
iteration : 1650
train acc:  0.7890625
train loss:  0.4753580093383789
train gradient:  0.13080433723265011
iteration : 1651
train acc:  0.6796875
train loss:  0.5648430585861206
train gradient:  0.17033616389119194
iteration : 1652
train acc:  0.7890625
train loss:  0.4942300021648407
train gradient:  0.1530188926026209
iteration : 1653
train acc:  0.7734375
train loss:  0.4622235596179962
train gradient:  0.11364187758745875
iteration : 1654
train acc:  0.7109375
train loss:  0.4787731468677521
train gradient:  0.10782531558721821
iteration : 1655
train acc:  0.6953125
train loss:  0.4980381727218628
train gradient:  0.11438580913392643
iteration : 1656
train acc:  0.71875
train loss:  0.5122082829475403
train gradient:  0.1384129003816859
iteration : 1657
train acc:  0.7578125
train loss:  0.45902395248413086
train gradient:  0.09613409019068797
iteration : 1658
train acc:  0.75
train loss:  0.468551367521286
train gradient:  0.10947949004942183
iteration : 1659
train acc:  0.7578125
train loss:  0.45183065533638
train gradient:  0.09490669704431183
iteration : 1660
train acc:  0.7421875
train loss:  0.4649559557437897
train gradient:  0.11701641419546852
iteration : 1661
train acc:  0.671875
train loss:  0.5797860622406006
train gradient:  0.18270294519531
iteration : 1662
train acc:  0.7265625
train loss:  0.5397935509681702
train gradient:  0.13859116991955706
iteration : 1663
train acc:  0.796875
train loss:  0.44331154227256775
train gradient:  0.11306597920822609
iteration : 1664
train acc:  0.734375
train loss:  0.45899850130081177
train gradient:  0.11346288823554984
iteration : 1665
train acc:  0.7265625
train loss:  0.4715944826602936
train gradient:  0.10545009965639582
iteration : 1666
train acc:  0.734375
train loss:  0.5226077437400818
train gradient:  0.12388767472974037
iteration : 1667
train acc:  0.7578125
train loss:  0.4599437713623047
train gradient:  0.09060135406277704
iteration : 1668
train acc:  0.7421875
train loss:  0.4873674809932709
train gradient:  0.1254733278405159
iteration : 1669
train acc:  0.8046875
train loss:  0.4107116460800171
train gradient:  0.10120944690902345
iteration : 1670
train acc:  0.828125
train loss:  0.3750932216644287
train gradient:  0.06455307018958664
iteration : 1671
train acc:  0.765625
train loss:  0.4622376561164856
train gradient:  0.1194448934219694
iteration : 1672
train acc:  0.7109375
train loss:  0.47556787729263306
train gradient:  0.12128659474924644
iteration : 1673
train acc:  0.7109375
train loss:  0.46326544880867004
train gradient:  0.10833782123601475
iteration : 1674
train acc:  0.7421875
train loss:  0.625309944152832
train gradient:  0.25658934747791134
iteration : 1675
train acc:  0.75
train loss:  0.5401411652565002
train gradient:  0.15140657261368176
iteration : 1676
train acc:  0.78125
train loss:  0.4516682028770447
train gradient:  0.12453488875046441
iteration : 1677
train acc:  0.7734375
train loss:  0.44255560636520386
train gradient:  0.1191202473439883
iteration : 1678
train acc:  0.796875
train loss:  0.40774106979370117
train gradient:  0.08323834888401332
iteration : 1679
train acc:  0.7890625
train loss:  0.449648916721344
train gradient:  0.10088382832680937
iteration : 1680
train acc:  0.6796875
train loss:  0.5490614175796509
train gradient:  0.13189188605353785
iteration : 1681
train acc:  0.7265625
train loss:  0.5034902691841125
train gradient:  0.12586202597499302
iteration : 1682
train acc:  0.8515625
train loss:  0.39473438262939453
train gradient:  0.08235684145432035
iteration : 1683
train acc:  0.734375
train loss:  0.5057733058929443
train gradient:  0.12329387630899383
iteration : 1684
train acc:  0.75
train loss:  0.49358105659484863
train gradient:  0.12391863458041945
iteration : 1685
train acc:  0.734375
train loss:  0.46340447664260864
train gradient:  0.1339412443177787
iteration : 1686
train acc:  0.796875
train loss:  0.42767322063446045
train gradient:  0.11298115592738193
iteration : 1687
train acc:  0.7578125
train loss:  0.4603937268257141
train gradient:  0.10196426898947569
iteration : 1688
train acc:  0.703125
train loss:  0.6026867032051086
train gradient:  0.17106672404293857
iteration : 1689
train acc:  0.7890625
train loss:  0.4281056821346283
train gradient:  0.09029473852341845
iteration : 1690
train acc:  0.7109375
train loss:  0.551838219165802
train gradient:  0.16732587872948035
iteration : 1691
train acc:  0.7578125
train loss:  0.46950313448905945
train gradient:  0.1046952323869364
iteration : 1692
train acc:  0.734375
train loss:  0.5210402607917786
train gradient:  0.12930641641781365
iteration : 1693
train acc:  0.734375
train loss:  0.4971952438354492
train gradient:  0.13363634640242167
iteration : 1694
train acc:  0.7734375
train loss:  0.49613460898399353
train gradient:  0.1501854082780552
iteration : 1695
train acc:  0.75
train loss:  0.5043909549713135
train gradient:  0.1290481587740533
iteration : 1696
train acc:  0.75
train loss:  0.5397971868515015
train gradient:  0.12393624878299114
iteration : 1697
train acc:  0.7578125
train loss:  0.5138229131698608
train gradient:  0.12258247305155515
iteration : 1698
train acc:  0.7578125
train loss:  0.47856536507606506
train gradient:  0.1209733709207643
iteration : 1699
train acc:  0.78125
train loss:  0.44588297605514526
train gradient:  0.09713354431954777
iteration : 1700
train acc:  0.765625
train loss:  0.4807834029197693
train gradient:  0.1152252313186827
iteration : 1701
train acc:  0.7265625
train loss:  0.4586424231529236
train gradient:  0.10852595319084149
iteration : 1702
train acc:  0.7890625
train loss:  0.4943947196006775
train gradient:  0.094818962049397
iteration : 1703
train acc:  0.8125
train loss:  0.4085838794708252
train gradient:  0.08533571330527508
iteration : 1704
train acc:  0.7265625
train loss:  0.4843940734863281
train gradient:  0.1369964018017929
iteration : 1705
train acc:  0.7421875
train loss:  0.4828699231147766
train gradient:  0.13888663187484648
iteration : 1706
train acc:  0.703125
train loss:  0.5486384630203247
train gradient:  0.11379196387669001
iteration : 1707
train acc:  0.8046875
train loss:  0.37416714429855347
train gradient:  0.08954540577614598
iteration : 1708
train acc:  0.796875
train loss:  0.41445860266685486
train gradient:  0.08806763821543398
iteration : 1709
train acc:  0.8046875
train loss:  0.42288658022880554
train gradient:  0.1113615486280838
iteration : 1710
train acc:  0.71875
train loss:  0.5364136099815369
train gradient:  0.13526547238454667
iteration : 1711
train acc:  0.7109375
train loss:  0.5007306337356567
train gradient:  0.11040501680811679
iteration : 1712
train acc:  0.65625
train loss:  0.575329601764679
train gradient:  0.1595667743294809
iteration : 1713
train acc:  0.765625
train loss:  0.4856222867965698
train gradient:  0.11764519011719743
iteration : 1714
train acc:  0.78125
train loss:  0.45594024658203125
train gradient:  0.11397989371799677
iteration : 1715
train acc:  0.671875
train loss:  0.534959614276886
train gradient:  0.13314988529636657
iteration : 1716
train acc:  0.7265625
train loss:  0.5172578692436218
train gradient:  0.12084622413000924
iteration : 1717
train acc:  0.703125
train loss:  0.5162320137023926
train gradient:  0.17315115803916356
iteration : 1718
train acc:  0.7890625
train loss:  0.48010963201522827
train gradient:  0.15294685713171885
iteration : 1719
train acc:  0.7265625
train loss:  0.4720929265022278
train gradient:  0.11792325361713552
iteration : 1720
train acc:  0.7265625
train loss:  0.5027560591697693
train gradient:  0.21078176346580696
iteration : 1721
train acc:  0.71875
train loss:  0.5128556489944458
train gradient:  0.1651971495833253
iteration : 1722
train acc:  0.78125
train loss:  0.5124185681343079
train gradient:  0.130534485963584
iteration : 1723
train acc:  0.71875
train loss:  0.48689359426498413
train gradient:  0.14721898470448427
iteration : 1724
train acc:  0.796875
train loss:  0.4313089847564697
train gradient:  0.08299442920477833
iteration : 1725
train acc:  0.734375
train loss:  0.5099605321884155
train gradient:  0.14323669716392956
iteration : 1726
train acc:  0.6953125
train loss:  0.5970028638839722
train gradient:  0.15928564415155216
iteration : 1727
train acc:  0.7421875
train loss:  0.49490270018577576
train gradient:  0.12905372201886486
iteration : 1728
train acc:  0.6796875
train loss:  0.5430806875228882
train gradient:  0.14517250981534122
iteration : 1729
train acc:  0.7109375
train loss:  0.5332025289535522
train gradient:  0.16744586845882808
iteration : 1730
train acc:  0.6640625
train loss:  0.5231502056121826
train gradient:  0.13303991458701503
iteration : 1731
train acc:  0.765625
train loss:  0.4451923370361328
train gradient:  0.09685539555625261
iteration : 1732
train acc:  0.671875
train loss:  0.5781232714653015
train gradient:  0.20230376527558322
iteration : 1733
train acc:  0.7109375
train loss:  0.45456811785697937
train gradient:  0.09241097037255364
iteration : 1734
train acc:  0.7578125
train loss:  0.49117612838745117
train gradient:  0.11684282549692993
iteration : 1735
train acc:  0.7578125
train loss:  0.4691200852394104
train gradient:  0.09874121535523568
iteration : 1736
train acc:  0.71875
train loss:  0.49955397844314575
train gradient:  0.15662328158903044
iteration : 1737
train acc:  0.7578125
train loss:  0.5068356990814209
train gradient:  0.13693781343468406
iteration : 1738
train acc:  0.7265625
train loss:  0.4903568923473358
train gradient:  0.10483834117135918
iteration : 1739
train acc:  0.671875
train loss:  0.6015596389770508
train gradient:  0.17572628764903891
iteration : 1740
train acc:  0.734375
train loss:  0.48690804839134216
train gradient:  0.13678459665343515
iteration : 1741
train acc:  0.765625
train loss:  0.4845046401023865
train gradient:  0.1270282348009891
iteration : 1742
train acc:  0.7421875
train loss:  0.4474005699157715
train gradient:  0.10606765499920857
iteration : 1743
train acc:  0.703125
train loss:  0.49809515476226807
train gradient:  0.1268302100224173
iteration : 1744
train acc:  0.7265625
train loss:  0.4919399321079254
train gradient:  0.11069744296206901
iteration : 1745
train acc:  0.78125
train loss:  0.4293181300163269
train gradient:  0.09976435867118458
iteration : 1746
train acc:  0.734375
train loss:  0.5358853340148926
train gradient:  0.1450898558603379
iteration : 1747
train acc:  0.8125
train loss:  0.397257924079895
train gradient:  0.0907219468417162
iteration : 1748
train acc:  0.671875
train loss:  0.5614537000656128
train gradient:  0.1758543950269854
iteration : 1749
train acc:  0.7734375
train loss:  0.45610296726226807
train gradient:  0.10029081822075368
iteration : 1750
train acc:  0.7578125
train loss:  0.4459574222564697
train gradient:  0.10011504029472948
iteration : 1751
train acc:  0.703125
train loss:  0.5241073369979858
train gradient:  0.15295690234042286
iteration : 1752
train acc:  0.765625
train loss:  0.4645439386367798
train gradient:  0.10380262650024075
iteration : 1753
train acc:  0.6953125
train loss:  0.5786080956459045
train gradient:  0.17399962151218706
iteration : 1754
train acc:  0.7265625
train loss:  0.520846962928772
train gradient:  0.11285558769118847
iteration : 1755
train acc:  0.7265625
train loss:  0.49982455372810364
train gradient:  0.13847828096567166
iteration : 1756
train acc:  0.6875
train loss:  0.5936856865882874
train gradient:  0.2091134283880185
iteration : 1757
train acc:  0.7265625
train loss:  0.5131597518920898
train gradient:  0.15614957218135372
iteration : 1758
train acc:  0.7265625
train loss:  0.5166521668434143
train gradient:  0.1417226619661911
iteration : 1759
train acc:  0.78125
train loss:  0.4303063452243805
train gradient:  0.13156119322901083
iteration : 1760
train acc:  0.7421875
train loss:  0.4894137680530548
train gradient:  0.12973418162972
iteration : 1761
train acc:  0.734375
train loss:  0.4699818789958954
train gradient:  0.10026080800797567
iteration : 1762
train acc:  0.71875
train loss:  0.483744740486145
train gradient:  0.11583425308862394
iteration : 1763
train acc:  0.7109375
train loss:  0.5609608888626099
train gradient:  0.14186011300538476
iteration : 1764
train acc:  0.734375
train loss:  0.4940206706523895
train gradient:  0.13471611453572319
iteration : 1765
train acc:  0.828125
train loss:  0.4033815264701843
train gradient:  0.09979537704610826
iteration : 1766
train acc:  0.734375
train loss:  0.4931240677833557
train gradient:  0.1397522712611281
iteration : 1767
train acc:  0.75
train loss:  0.4444970488548279
train gradient:  0.10525877759281019
iteration : 1768
train acc:  0.7265625
train loss:  0.5095032453536987
train gradient:  0.12670270301627395
iteration : 1769
train acc:  0.75
train loss:  0.4721723198890686
train gradient:  0.11464130530573043
iteration : 1770
train acc:  0.7109375
train loss:  0.5079439878463745
train gradient:  0.10298974657410466
iteration : 1771
train acc:  0.7890625
train loss:  0.40543580055236816
train gradient:  0.0793207559128075
iteration : 1772
train acc:  0.703125
train loss:  0.4931592643260956
train gradient:  0.12197819148873727
iteration : 1773
train acc:  0.765625
train loss:  0.5220697522163391
train gradient:  0.12049852641534224
iteration : 1774
train acc:  0.78125
train loss:  0.4749513864517212
train gradient:  0.10057176044314402
iteration : 1775
train acc:  0.7890625
train loss:  0.41270923614501953
train gradient:  0.09598745063523859
iteration : 1776
train acc:  0.7265625
train loss:  0.48936009407043457
train gradient:  0.12412524913091338
iteration : 1777
train acc:  0.7421875
train loss:  0.5005459785461426
train gradient:  0.13394090139338133
iteration : 1778
train acc:  0.7578125
train loss:  0.47459548711776733
train gradient:  0.1286033854867344
iteration : 1779
train acc:  0.8046875
train loss:  0.41443467140197754
train gradient:  0.07403189841046773
iteration : 1780
train acc:  0.8125
train loss:  0.47435012459754944
train gradient:  0.10082785392236042
iteration : 1781
train acc:  0.6640625
train loss:  0.5633381605148315
train gradient:  0.15275406946570225
iteration : 1782
train acc:  0.796875
train loss:  0.43131279945373535
train gradient:  0.10291267424308805
iteration : 1783
train acc:  0.7578125
train loss:  0.4474254250526428
train gradient:  0.12255755372086824
iteration : 1784
train acc:  0.8046875
train loss:  0.4248165488243103
train gradient:  0.09840675329529698
iteration : 1785
train acc:  0.765625
train loss:  0.4498109817504883
train gradient:  0.09892732695504504
iteration : 1786
train acc:  0.7734375
train loss:  0.4474402070045471
train gradient:  0.13712070847948674
iteration : 1787
train acc:  0.765625
train loss:  0.4245327115058899
train gradient:  0.07581077734423203
iteration : 1788
train acc:  0.734375
train loss:  0.5088189840316772
train gradient:  0.1263881270727981
iteration : 1789
train acc:  0.75
train loss:  0.5109888315200806
train gradient:  0.12878416498175255
iteration : 1790
train acc:  0.6640625
train loss:  0.5802278518676758
train gradient:  0.16203394729559373
iteration : 1791
train acc:  0.7421875
train loss:  0.49704593420028687
train gradient:  0.1060022384118226
iteration : 1792
train acc:  0.75
train loss:  0.4813462495803833
train gradient:  0.1606082251423826
iteration : 1793
train acc:  0.6953125
train loss:  0.49335676431655884
train gradient:  0.12800797260046498
iteration : 1794
train acc:  0.7578125
train loss:  0.47074925899505615
train gradient:  0.11400761317716869
iteration : 1795
train acc:  0.75
train loss:  0.4885287880897522
train gradient:  0.09266521277786102
iteration : 1796
train acc:  0.796875
train loss:  0.44567111134529114
train gradient:  0.14752948811567096
iteration : 1797
train acc:  0.78125
train loss:  0.422801673412323
train gradient:  0.09719840754175448
iteration : 1798
train acc:  0.7734375
train loss:  0.4683133661746979
train gradient:  0.11077513773477943
iteration : 1799
train acc:  0.8046875
train loss:  0.41883254051208496
train gradient:  0.11662603426624052
iteration : 1800
train acc:  0.734375
train loss:  0.476922869682312
train gradient:  0.12738358844942643
iteration : 1801
train acc:  0.7109375
train loss:  0.51357501745224
train gradient:  0.1177732530542707
iteration : 1802
train acc:  0.6953125
train loss:  0.4960384964942932
train gradient:  0.11655493556782767
iteration : 1803
train acc:  0.7734375
train loss:  0.44219014048576355
train gradient:  0.12339867405384594
iteration : 1804
train acc:  0.765625
train loss:  0.47934284806251526
train gradient:  0.1049758998923701
iteration : 1805
train acc:  0.7265625
train loss:  0.5527544617652893
train gradient:  0.12954604824423044
iteration : 1806
train acc:  0.75
train loss:  0.5035219788551331
train gradient:  0.16433401687679777
iteration : 1807
train acc:  0.71875
train loss:  0.5679321885108948
train gradient:  0.12047504986929436
iteration : 1808
train acc:  0.765625
train loss:  0.45387300848960876
train gradient:  0.11158016595969135
iteration : 1809
train acc:  0.765625
train loss:  0.47060924768447876
train gradient:  0.11760889920572998
iteration : 1810
train acc:  0.71875
train loss:  0.5273691415786743
train gradient:  0.11669206356137427
iteration : 1811
train acc:  0.7265625
train loss:  0.480754554271698
train gradient:  0.1119471516848163
iteration : 1812
train acc:  0.8359375
train loss:  0.42632293701171875
train gradient:  0.10010461100143433
iteration : 1813
train acc:  0.8046875
train loss:  0.38324546813964844
train gradient:  0.0767346730539193
iteration : 1814
train acc:  0.6953125
train loss:  0.5343919992446899
train gradient:  0.12946165964533296
iteration : 1815
train acc:  0.71875
train loss:  0.49252161383628845
train gradient:  0.10272857669189088
iteration : 1816
train acc:  0.7109375
train loss:  0.5556052327156067
train gradient:  0.14103286657476993
iteration : 1817
train acc:  0.640625
train loss:  0.5981471538543701
train gradient:  0.17841537795559004
iteration : 1818
train acc:  0.7734375
train loss:  0.4834499955177307
train gradient:  0.11079789607195233
iteration : 1819
train acc:  0.7734375
train loss:  0.4491063356399536
train gradient:  0.10411785751785838
iteration : 1820
train acc:  0.71875
train loss:  0.5290586948394775
train gradient:  0.12979786135909283
iteration : 1821
train acc:  0.71875
train loss:  0.46093931794166565
train gradient:  0.11704534957517367
iteration : 1822
train acc:  0.7734375
train loss:  0.46546000242233276
train gradient:  0.1264291149921992
iteration : 1823
train acc:  0.7421875
train loss:  0.5084740519523621
train gradient:  0.12444105103012043
iteration : 1824
train acc:  0.6875
train loss:  0.5522221922874451
train gradient:  0.1637449767047619
iteration : 1825
train acc:  0.703125
train loss:  0.5309550762176514
train gradient:  0.12551679868091656
iteration : 1826
train acc:  0.7578125
train loss:  0.5022977590560913
train gradient:  0.09613771384684767
iteration : 1827
train acc:  0.75
train loss:  0.49579739570617676
train gradient:  0.1019167902405258
iteration : 1828
train acc:  0.7421875
train loss:  0.4995846152305603
train gradient:  0.10933033360233588
iteration : 1829
train acc:  0.8203125
train loss:  0.48785197734832764
train gradient:  0.08643648045936576
iteration : 1830
train acc:  0.828125
train loss:  0.4291155934333801
train gradient:  0.10417777887866242
iteration : 1831
train acc:  0.7421875
train loss:  0.5555661916732788
train gradient:  0.1500105561141572
iteration : 1832
train acc:  0.7421875
train loss:  0.46837475895881653
train gradient:  0.11780414873721917
iteration : 1833
train acc:  0.796875
train loss:  0.38453084230422974
train gradient:  0.08062157368757944
iteration : 1834
train acc:  0.6875
train loss:  0.5699988007545471
train gradient:  0.13547539656568564
iteration : 1835
train acc:  0.7890625
train loss:  0.4360884428024292
train gradient:  0.09412236726383917
iteration : 1836
train acc:  0.7734375
train loss:  0.5265549421310425
train gradient:  0.13698423196892984
iteration : 1837
train acc:  0.6953125
train loss:  0.5122998356819153
train gradient:  0.1204050149908019
iteration : 1838
train acc:  0.7265625
train loss:  0.5599439740180969
train gradient:  0.1660682058906101
iteration : 1839
train acc:  0.734375
train loss:  0.555878758430481
train gradient:  0.14185603550891174
iteration : 1840
train acc:  0.7734375
train loss:  0.45344191789627075
train gradient:  0.08307716149922184
iteration : 1841
train acc:  0.75
train loss:  0.46163809299468994
train gradient:  0.13100620625917397
iteration : 1842
train acc:  0.734375
train loss:  0.5001521110534668
train gradient:  0.11199572416255864
iteration : 1843
train acc:  0.75
train loss:  0.43901005387306213
train gradient:  0.09780936319829088
iteration : 1844
train acc:  0.7265625
train loss:  0.5366256237030029
train gradient:  0.17351103927318473
iteration : 1845
train acc:  0.8203125
train loss:  0.4740765690803528
train gradient:  0.10786889947627087
iteration : 1846
train acc:  0.71875
train loss:  0.491232693195343
train gradient:  0.10215208341936437
iteration : 1847
train acc:  0.7421875
train loss:  0.47926899790763855
train gradient:  0.11606659469438985
iteration : 1848
train acc:  0.71875
train loss:  0.5220816135406494
train gradient:  0.13741202706744113
iteration : 1849
train acc:  0.6875
train loss:  0.5133680105209351
train gradient:  0.110369149564002
iteration : 1850
train acc:  0.7890625
train loss:  0.46178385615348816
train gradient:  0.14577372652819742
iteration : 1851
train acc:  0.8359375
train loss:  0.4329562783241272
train gradient:  0.0917660873767383
iteration : 1852
train acc:  0.71875
train loss:  0.460655152797699
train gradient:  0.12154025302754715
iteration : 1853
train acc:  0.78125
train loss:  0.4627847373485565
train gradient:  0.10388282829904984
iteration : 1854
train acc:  0.7109375
train loss:  0.4762778878211975
train gradient:  0.11397597197015745
iteration : 1855
train acc:  0.75
train loss:  0.4703255295753479
train gradient:  0.11551466172625637
iteration : 1856
train acc:  0.765625
train loss:  0.46869486570358276
train gradient:  0.11151377448904515
iteration : 1857
train acc:  0.75
train loss:  0.48457491397857666
train gradient:  0.11393352244535247
iteration : 1858
train acc:  0.6953125
train loss:  0.5322818756103516
train gradient:  0.12478758207509123
iteration : 1859
train acc:  0.78125
train loss:  0.4483463168144226
train gradient:  0.12521539502459297
iteration : 1860
train acc:  0.734375
train loss:  0.5223097801208496
train gradient:  0.1113297393855898
iteration : 1861
train acc:  0.7265625
train loss:  0.4620994031429291
train gradient:  0.13201013783584897
iteration : 1862
train acc:  0.7734375
train loss:  0.4571058750152588
train gradient:  0.10115840433865177
iteration : 1863
train acc:  0.7265625
train loss:  0.5156556367874146
train gradient:  0.09566357783821422
iteration : 1864
train acc:  0.8359375
train loss:  0.4224451184272766
train gradient:  0.10973147319524526
iteration : 1865
train acc:  0.7890625
train loss:  0.4239952564239502
train gradient:  0.10096864627562001
iteration : 1866
train acc:  0.7265625
train loss:  0.5235361456871033
train gradient:  0.11408056128590571
iteration : 1867
train acc:  0.6875
train loss:  0.5689610838890076
train gradient:  0.14099220041799482
iteration : 1868
train acc:  0.7578125
train loss:  0.4792666435241699
train gradient:  0.12342273090800565
iteration : 1869
train acc:  0.8046875
train loss:  0.4796464443206787
train gradient:  0.11002992143843734
iteration : 1870
train acc:  0.7109375
train loss:  0.5257833003997803
train gradient:  0.1664011476154567
iteration : 1871
train acc:  0.8046875
train loss:  0.4191315174102783
train gradient:  0.08728979594254554
iteration : 1872
train acc:  0.78125
train loss:  0.46069052815437317
train gradient:  0.10559232526711239
iteration : 1873
train acc:  0.75
train loss:  0.44288548827171326
train gradient:  0.09105564314882314
iteration : 1874
train acc:  0.765625
train loss:  0.49053558707237244
train gradient:  0.12153871575896998
iteration : 1875
train acc:  0.7265625
train loss:  0.4911293685436249
train gradient:  0.11044598022228536
iteration : 1876
train acc:  0.765625
train loss:  0.44921550154685974
train gradient:  0.12298947375228082
iteration : 1877
train acc:  0.734375
train loss:  0.5090112686157227
train gradient:  0.09681518910032574
iteration : 1878
train acc:  0.7578125
train loss:  0.48258960247039795
train gradient:  0.12068946918914794
iteration : 1879
train acc:  0.75
train loss:  0.45432043075561523
train gradient:  0.1297079052056625
iteration : 1880
train acc:  0.703125
train loss:  0.5082967281341553
train gradient:  0.13385236148410878
iteration : 1881
train acc:  0.7890625
train loss:  0.5024496912956238
train gradient:  0.12306633194696323
iteration : 1882
train acc:  0.75
train loss:  0.516426146030426
train gradient:  0.12745633469519457
iteration : 1883
train acc:  0.75
train loss:  0.44891390204429626
train gradient:  0.1281725722816805
iteration : 1884
train acc:  0.8046875
train loss:  0.43306732177734375
train gradient:  0.0849510763860452
iteration : 1885
train acc:  0.6796875
train loss:  0.525749683380127
train gradient:  0.1853548439591743
iteration : 1886
train acc:  0.7109375
train loss:  0.46444272994995117
train gradient:  0.10388995861712413
iteration : 1887
train acc:  0.7578125
train loss:  0.42797303199768066
train gradient:  0.10653475085763958
iteration : 1888
train acc:  0.7734375
train loss:  0.47313186526298523
train gradient:  0.11631830417814516
iteration : 1889
train acc:  0.765625
train loss:  0.4428415596485138
train gradient:  0.1037761972303763
iteration : 1890
train acc:  0.7421875
train loss:  0.481961727142334
train gradient:  0.1248531227754801
iteration : 1891
train acc:  0.7734375
train loss:  0.43764445185661316
train gradient:  0.11437332670411106
iteration : 1892
train acc:  0.75
train loss:  0.5273934006690979
train gradient:  0.14422429080503374
iteration : 1893
train acc:  0.7734375
train loss:  0.4436677098274231
train gradient:  0.08838961923950843
iteration : 1894
train acc:  0.8125
train loss:  0.4175804555416107
train gradient:  0.09486405299689739
iteration : 1895
train acc:  0.7421875
train loss:  0.4604577422142029
train gradient:  0.10977790553664833
iteration : 1896
train acc:  0.796875
train loss:  0.49160924553871155
train gradient:  0.11530852487615552
iteration : 1897
train acc:  0.75
train loss:  0.5374502539634705
train gradient:  0.13543740262215925
iteration : 1898
train acc:  0.71875
train loss:  0.5202234387397766
train gradient:  0.1397156992002452
iteration : 1899
train acc:  0.7890625
train loss:  0.39134904742240906
train gradient:  0.09285274496338553
iteration : 1900
train acc:  0.734375
train loss:  0.4667094647884369
train gradient:  0.09856873127611634
iteration : 1901
train acc:  0.8203125
train loss:  0.46077078580856323
train gradient:  0.09942602380120966
iteration : 1902
train acc:  0.75
train loss:  0.44746285676956177
train gradient:  0.10926678609594817
iteration : 1903
train acc:  0.7890625
train loss:  0.4917134642601013
train gradient:  0.11732742912100735
iteration : 1904
train acc:  0.7265625
train loss:  0.5141156315803528
train gradient:  0.1002343611932609
iteration : 1905
train acc:  0.7421875
train loss:  0.4874999523162842
train gradient:  0.11176743039600717
iteration : 1906
train acc:  0.734375
train loss:  0.49491575360298157
train gradient:  0.1555412093314403
iteration : 1907
train acc:  0.765625
train loss:  0.4715445041656494
train gradient:  0.12821221568549335
iteration : 1908
train acc:  0.8125
train loss:  0.40604063868522644
train gradient:  0.10230799090991967
iteration : 1909
train acc:  0.765625
train loss:  0.48489731550216675
train gradient:  0.12623392117857474
iteration : 1910
train acc:  0.7421875
train loss:  0.48647111654281616
train gradient:  0.10864659704297834
iteration : 1911
train acc:  0.6875
train loss:  0.5019145607948303
train gradient:  0.10036891210033698
iteration : 1912
train acc:  0.6875
train loss:  0.5233035087585449
train gradient:  0.11883155524411639
iteration : 1913
train acc:  0.7890625
train loss:  0.42953193187713623
train gradient:  0.1064893847747684
iteration : 1914
train acc:  0.7578125
train loss:  0.4474375247955322
train gradient:  0.12748718330951472
iteration : 1915
train acc:  0.75
train loss:  0.4865706264972687
train gradient:  0.12068527898103412
iteration : 1916
train acc:  0.7265625
train loss:  0.5328097939491272
train gradient:  0.15160791368557747
iteration : 1917
train acc:  0.7109375
train loss:  0.539863646030426
train gradient:  0.13846920770170215
iteration : 1918
train acc:  0.7421875
train loss:  0.4502134919166565
train gradient:  0.10147847745716101
iteration : 1919
train acc:  0.75
train loss:  0.5288456678390503
train gradient:  0.15339016635738711
iteration : 1920
train acc:  0.7578125
train loss:  0.5128089189529419
train gradient:  0.14553018652274485
iteration : 1921
train acc:  0.7109375
train loss:  0.49985185265541077
train gradient:  0.13167630801529967
iteration : 1922
train acc:  0.7421875
train loss:  0.49493885040283203
train gradient:  0.11276431391705319
iteration : 1923
train acc:  0.8203125
train loss:  0.4343356788158417
train gradient:  0.07245196585687436
iteration : 1924
train acc:  0.703125
train loss:  0.4771203100681305
train gradient:  0.1122544749020847
iteration : 1925
train acc:  0.7578125
train loss:  0.49905991554260254
train gradient:  0.11728765993961632
iteration : 1926
train acc:  0.7734375
train loss:  0.48881077766418457
train gradient:  0.12674947219121335
iteration : 1927
train acc:  0.703125
train loss:  0.5001826286315918
train gradient:  0.13563855201981645
iteration : 1928
train acc:  0.7109375
train loss:  0.5331117510795593
train gradient:  0.14057992935716662
iteration : 1929
train acc:  0.7578125
train loss:  0.4827849268913269
train gradient:  0.10520685850181291
iteration : 1930
train acc:  0.78125
train loss:  0.5120013356208801
train gradient:  0.1091680092903827
iteration : 1931
train acc:  0.703125
train loss:  0.536456823348999
train gradient:  0.13543667215545596
iteration : 1932
train acc:  0.6875
train loss:  0.5548884868621826
train gradient:  0.13133293922992018
iteration : 1933
train acc:  0.7421875
train loss:  0.48319345712661743
train gradient:  0.11501672795806646
iteration : 1934
train acc:  0.7734375
train loss:  0.48681288957595825
train gradient:  0.12126808884149304
iteration : 1935
train acc:  0.765625
train loss:  0.4970586895942688
train gradient:  0.10760120380715875
iteration : 1936
train acc:  0.7109375
train loss:  0.4751332402229309
train gradient:  0.09802499887894318
iteration : 1937
train acc:  0.7734375
train loss:  0.43460172414779663
train gradient:  0.10670460486001605
iteration : 1938
train acc:  0.7578125
train loss:  0.46991974115371704
train gradient:  0.12420630122695026
iteration : 1939
train acc:  0.6953125
train loss:  0.4841969609260559
train gradient:  0.1062934251280194
iteration : 1940
train acc:  0.6796875
train loss:  0.5309480428695679
train gradient:  0.14932532067013599
iteration : 1941
train acc:  0.6875
train loss:  0.5001214742660522
train gradient:  0.11105123918140314
iteration : 1942
train acc:  0.734375
train loss:  0.5072076916694641
train gradient:  0.1191099238833049
iteration : 1943
train acc:  0.71875
train loss:  0.5014970898628235
train gradient:  0.13342384849404143
iteration : 1944
train acc:  0.7734375
train loss:  0.4848809242248535
train gradient:  0.0980987444913117
iteration : 1945
train acc:  0.6640625
train loss:  0.5517385005950928
train gradient:  0.14377285285402058
iteration : 1946
train acc:  0.765625
train loss:  0.4533360004425049
train gradient:  0.0894827781212945
iteration : 1947
train acc:  0.7578125
train loss:  0.5018603205680847
train gradient:  0.12314147181874954
iteration : 1948
train acc:  0.7421875
train loss:  0.484707772731781
train gradient:  0.09861797640233755
iteration : 1949
train acc:  0.7578125
train loss:  0.467436283826828
train gradient:  0.09943193308180906
iteration : 1950
train acc:  0.703125
train loss:  0.5451819896697998
train gradient:  0.175181526488803
iteration : 1951
train acc:  0.71875
train loss:  0.5073237419128418
train gradient:  0.11606696063126123
iteration : 1952
train acc:  0.7578125
train loss:  0.4840083718299866
train gradient:  0.11336280634768811
iteration : 1953
train acc:  0.7578125
train loss:  0.507454514503479
train gradient:  0.12098067873105957
iteration : 1954
train acc:  0.7265625
train loss:  0.4459003210067749
train gradient:  0.08417896168421467
iteration : 1955
train acc:  0.7734375
train loss:  0.4374825954437256
train gradient:  0.08665027839701468
iteration : 1956
train acc:  0.71875
train loss:  0.5098651647567749
train gradient:  0.14019461491611038
iteration : 1957
train acc:  0.7421875
train loss:  0.4789552092552185
train gradient:  0.11325903775252763
iteration : 1958
train acc:  0.7421875
train loss:  0.507667064666748
train gradient:  0.1355384321521831
iteration : 1959
train acc:  0.796875
train loss:  0.4473456144332886
train gradient:  0.1269038015162784
iteration : 1960
train acc:  0.7265625
train loss:  0.4972913861274719
train gradient:  0.13386270872436767
iteration : 1961
train acc:  0.84375
train loss:  0.3936939239501953
train gradient:  0.0992697011522748
iteration : 1962
train acc:  0.8046875
train loss:  0.40355184674263
train gradient:  0.08689258414111913
iteration : 1963
train acc:  0.78125
train loss:  0.42297619581222534
train gradient:  0.09131866393687281
iteration : 1964
train acc:  0.75
train loss:  0.48717808723449707
train gradient:  0.10476254237073492
iteration : 1965
train acc:  0.765625
train loss:  0.46345335245132446
train gradient:  0.11333420138178955
iteration : 1966
train acc:  0.78125
train loss:  0.44996362924575806
train gradient:  0.10611217722328026
iteration : 1967
train acc:  0.703125
train loss:  0.5550838708877563
train gradient:  0.17629253454582053
iteration : 1968
train acc:  0.7578125
train loss:  0.4765636920928955
train gradient:  0.1419522939806236
iteration : 1969
train acc:  0.7265625
train loss:  0.43222737312316895
train gradient:  0.0868453470156986
iteration : 1970
train acc:  0.671875
train loss:  0.558932900428772
train gradient:  0.17134106056278833
iteration : 1971
train acc:  0.78125
train loss:  0.451779305934906
train gradient:  0.09741413956811662
iteration : 1972
train acc:  0.8046875
train loss:  0.43691906332969666
train gradient:  0.09501646236787915
iteration : 1973
train acc:  0.75
train loss:  0.5050125122070312
train gradient:  0.11697937473319656
iteration : 1974
train acc:  0.6796875
train loss:  0.5345220565795898
train gradient:  0.1406269564956611
iteration : 1975
train acc:  0.7734375
train loss:  0.43668699264526367
train gradient:  0.09730674394656273
iteration : 1976
train acc:  0.6953125
train loss:  0.5273120403289795
train gradient:  0.14468053825883403
iteration : 1977
train acc:  0.765625
train loss:  0.4480723440647125
train gradient:  0.13221906014463533
iteration : 1978
train acc:  0.8671875
train loss:  0.35816192626953125
train gradient:  0.0878987991192818
iteration : 1979
train acc:  0.734375
train loss:  0.5058379173278809
train gradient:  0.12824953182789683
iteration : 1980
train acc:  0.7265625
train loss:  0.4875490367412567
train gradient:  0.13983476769485984
iteration : 1981
train acc:  0.7734375
train loss:  0.4581785798072815
train gradient:  0.11724791842124489
iteration : 1982
train acc:  0.7578125
train loss:  0.48879480361938477
train gradient:  0.11564272567778167
iteration : 1983
train acc:  0.7265625
train loss:  0.4810650646686554
train gradient:  0.10415837196149755
iteration : 1984
train acc:  0.765625
train loss:  0.5366926193237305
train gradient:  0.12321898883672051
iteration : 1985
train acc:  0.7265625
train loss:  0.4906536042690277
train gradient:  0.16442680338085175
iteration : 1986
train acc:  0.7421875
train loss:  0.44906431436538696
train gradient:  0.11848293492268484
iteration : 1987
train acc:  0.703125
train loss:  0.5364575982093811
train gradient:  0.22321881513553385
iteration : 1988
train acc:  0.6953125
train loss:  0.5411444902420044
train gradient:  0.14679996553104396
iteration : 1989
train acc:  0.7734375
train loss:  0.42592501640319824
train gradient:  0.06754040489560992
iteration : 1990
train acc:  0.8359375
train loss:  0.4104872941970825
train gradient:  0.1094448662116394
iteration : 1991
train acc:  0.7734375
train loss:  0.45800840854644775
train gradient:  0.0991293631834707
iteration : 1992
train acc:  0.7578125
train loss:  0.4728274345397949
train gradient:  0.12971267414146245
iteration : 1993
train acc:  0.6953125
train loss:  0.5050023794174194
train gradient:  0.12022599360916081
iteration : 1994
train acc:  0.75
train loss:  0.5092675685882568
train gradient:  0.15048238381121187
iteration : 1995
train acc:  0.828125
train loss:  0.4040980339050293
train gradient:  0.12886838517278082
iteration : 1996
train acc:  0.765625
train loss:  0.4322199821472168
train gradient:  0.1140191238961843
iteration : 1997
train acc:  0.875
train loss:  0.3855525851249695
train gradient:  0.09270255162507135
iteration : 1998
train acc:  0.796875
train loss:  0.4317472577095032
train gradient:  0.10430752207863178
iteration : 1999
train acc:  0.8125
train loss:  0.40836960077285767
train gradient:  0.08545244392610914
iteration : 2000
train acc:  0.71875
train loss:  0.48970866203308105
train gradient:  0.10702634569360125
iteration : 2001
train acc:  0.6484375
train loss:  0.570425271987915
train gradient:  0.14501800802026582
iteration : 2002
train acc:  0.765625
train loss:  0.4860904812812805
train gradient:  0.11761300291073083
iteration : 2003
train acc:  0.7578125
train loss:  0.4543953835964203
train gradient:  0.11008193657382556
iteration : 2004
train acc:  0.8203125
train loss:  0.40252164006233215
train gradient:  0.11445645393798773
iteration : 2005
train acc:  0.75
train loss:  0.4695020318031311
train gradient:  0.1342084732520008
iteration : 2006
train acc:  0.7890625
train loss:  0.4819806218147278
train gradient:  0.13655015480183536
iteration : 2007
train acc:  0.7734375
train loss:  0.47080570459365845
train gradient:  0.0928288003758449
iteration : 2008
train acc:  0.8203125
train loss:  0.40130773186683655
train gradient:  0.08320304653672349
iteration : 2009
train acc:  0.75
train loss:  0.48669713735580444
train gradient:  0.11297519540543563
iteration : 2010
train acc:  0.765625
train loss:  0.5242006778717041
train gradient:  0.1497343493392269
iteration : 2011
train acc:  0.7109375
train loss:  0.5285178422927856
train gradient:  0.12638240945313647
iteration : 2012
train acc:  0.734375
train loss:  0.4813750088214874
train gradient:  0.14041014468165797
iteration : 2013
train acc:  0.8203125
train loss:  0.34275883436203003
train gradient:  0.06622802163853686
iteration : 2014
train acc:  0.7265625
train loss:  0.5017629861831665
train gradient:  0.12073839029908777
iteration : 2015
train acc:  0.75
train loss:  0.4752887487411499
train gradient:  0.11424504814299966
iteration : 2016
train acc:  0.828125
train loss:  0.4279358386993408
train gradient:  0.10646794289130737
iteration : 2017
train acc:  0.765625
train loss:  0.4601115584373474
train gradient:  0.12005829351618907
iteration : 2018
train acc:  0.8125
train loss:  0.4477865397930145
train gradient:  0.10062344409919124
iteration : 2019
train acc:  0.71875
train loss:  0.5086642503738403
train gradient:  0.13152384395555966
iteration : 2020
train acc:  0.734375
train loss:  0.48175424337387085
train gradient:  0.12936383096146742
iteration : 2021
train acc:  0.7109375
train loss:  0.5254170894622803
train gradient:  0.11224464824958662
iteration : 2022
train acc:  0.7109375
train loss:  0.5094221830368042
train gradient:  0.150491211174133
iteration : 2023
train acc:  0.7734375
train loss:  0.4374213218688965
train gradient:  0.10178650675476997
iteration : 2024
train acc:  0.78125
train loss:  0.5103696584701538
train gradient:  0.11600640437468081
iteration : 2025
train acc:  0.7890625
train loss:  0.4517971873283386
train gradient:  0.11259741589226112
iteration : 2026
train acc:  0.7890625
train loss:  0.4474508464336395
train gradient:  0.1030920841797074
iteration : 2027
train acc:  0.8046875
train loss:  0.41144871711730957
train gradient:  0.07937593023369269
iteration : 2028
train acc:  0.7578125
train loss:  0.46870338916778564
train gradient:  0.11634328182722789
iteration : 2029
train acc:  0.78125
train loss:  0.4953625202178955
train gradient:  0.1349686463135147
iteration : 2030
train acc:  0.734375
train loss:  0.5284329056739807
train gradient:  0.14112200098791916
iteration : 2031
train acc:  0.7734375
train loss:  0.49917691946029663
train gradient:  0.1437810976804981
iteration : 2032
train acc:  0.7890625
train loss:  0.45112645626068115
train gradient:  0.11385680712681376
iteration : 2033
train acc:  0.8203125
train loss:  0.42374974489212036
train gradient:  0.11274498799344163
iteration : 2034
train acc:  0.7109375
train loss:  0.48094266653060913
train gradient:  0.12088516219124149
iteration : 2035
train acc:  0.734375
train loss:  0.4574088454246521
train gradient:  0.09155097114788281
iteration : 2036
train acc:  0.734375
train loss:  0.5070898532867432
train gradient:  0.12332362880843303
iteration : 2037
train acc:  0.7421875
train loss:  0.6102942824363708
train gradient:  0.20587400348207918
iteration : 2038
train acc:  0.6796875
train loss:  0.56184983253479
train gradient:  0.1313258924159321
iteration : 2039
train acc:  0.7265625
train loss:  0.524979829788208
train gradient:  0.14254044167337765
iteration : 2040
train acc:  0.78125
train loss:  0.4675111472606659
train gradient:  0.1199660758764251
iteration : 2041
train acc:  0.796875
train loss:  0.4267343282699585
train gradient:  0.08830340402679251
iteration : 2042
train acc:  0.796875
train loss:  0.4283875823020935
train gradient:  0.09517224538622439
iteration : 2043
train acc:  0.7578125
train loss:  0.45557692646980286
train gradient:  0.08473074659408883
iteration : 2044
train acc:  0.7421875
train loss:  0.4733772575855255
train gradient:  0.10687703263822208
iteration : 2045
train acc:  0.6875
train loss:  0.4835413098335266
train gradient:  0.11945350948025561
iteration : 2046
train acc:  0.7734375
train loss:  0.41619059443473816
train gradient:  0.0890836716249181
iteration : 2047
train acc:  0.75
train loss:  0.46764230728149414
train gradient:  0.13169067779445953
iteration : 2048
train acc:  0.7265625
train loss:  0.5131769180297852
train gradient:  0.15012706607399856
iteration : 2049
train acc:  0.7421875
train loss:  0.44035351276397705
train gradient:  0.11760236866585978
iteration : 2050
train acc:  0.703125
train loss:  0.5119270086288452
train gradient:  0.13290371253864916
iteration : 2051
train acc:  0.828125
train loss:  0.4460119903087616
train gradient:  0.10574976048722347
iteration : 2052
train acc:  0.8046875
train loss:  0.39010608196258545
train gradient:  0.08583154949303433
iteration : 2053
train acc:  0.7734375
train loss:  0.49506303668022156
train gradient:  0.12226445277453213
iteration : 2054
train acc:  0.78125
train loss:  0.4357679784297943
train gradient:  0.0960657797992316
iteration : 2055
train acc:  0.7578125
train loss:  0.47206541895866394
train gradient:  0.11025180312554408
iteration : 2056
train acc:  0.8125
train loss:  0.40749669075012207
train gradient:  0.07674228427835013
iteration : 2057
train acc:  0.7578125
train loss:  0.4697497487068176
train gradient:  0.10137161590393837
iteration : 2058
train acc:  0.7578125
train loss:  0.5186606049537659
train gradient:  0.1300061710856611
iteration : 2059
train acc:  0.7265625
train loss:  0.5582400560379028
train gradient:  0.16669769198950551
iteration : 2060
train acc:  0.7734375
train loss:  0.4176962375640869
train gradient:  0.10973376221831314
iteration : 2061
train acc:  0.796875
train loss:  0.49169468879699707
train gradient:  0.14792143255381301
iteration : 2062
train acc:  0.78125
train loss:  0.4322062134742737
train gradient:  0.1152631046077943
iteration : 2063
train acc:  0.78125
train loss:  0.42138880491256714
train gradient:  0.0911203780807122
iteration : 2064
train acc:  0.75
train loss:  0.4748735725879669
train gradient:  0.09729485187668672
iteration : 2065
train acc:  0.8203125
train loss:  0.3868080973625183
train gradient:  0.09781381963033996
iteration : 2066
train acc:  0.7265625
train loss:  0.44277846813201904
train gradient:  0.14340856416286035
iteration : 2067
train acc:  0.7578125
train loss:  0.5079400539398193
train gradient:  0.1291757632416694
iteration : 2068
train acc:  0.7578125
train loss:  0.4440337121486664
train gradient:  0.11462906421456874
iteration : 2069
train acc:  0.7578125
train loss:  0.46857744455337524
train gradient:  0.11927952670037567
iteration : 2070
train acc:  0.7421875
train loss:  0.44648075103759766
train gradient:  0.095987328397857
iteration : 2071
train acc:  0.71875
train loss:  0.4387013912200928
train gradient:  0.10869855712617614
iteration : 2072
train acc:  0.71875
train loss:  0.5280992984771729
train gradient:  0.12045493302658368
iteration : 2073
train acc:  0.71875
train loss:  0.5327935218811035
train gradient:  0.11222205394766559
iteration : 2074
train acc:  0.7109375
train loss:  0.552329957485199
train gradient:  0.16438732235698475
iteration : 2075
train acc:  0.7890625
train loss:  0.4460504353046417
train gradient:  0.11847047958856943
iteration : 2076
train acc:  0.78125
train loss:  0.4757082760334015
train gradient:  0.1206534779573531
iteration : 2077
train acc:  0.734375
train loss:  0.504212498664856
train gradient:  0.12095798024445356
iteration : 2078
train acc:  0.8046875
train loss:  0.41660141944885254
train gradient:  0.10778319538415616
iteration : 2079
train acc:  0.78125
train loss:  0.4622410535812378
train gradient:  0.09308869717209302
iteration : 2080
train acc:  0.7578125
train loss:  0.5151029825210571
train gradient:  0.14137491184294543
iteration : 2081
train acc:  0.8046875
train loss:  0.39115989208221436
train gradient:  0.07718889386549908
iteration : 2082
train acc:  0.7421875
train loss:  0.47411081194877625
train gradient:  0.13409643389951592
iteration : 2083
train acc:  0.7578125
train loss:  0.48143336176872253
train gradient:  0.1194953346001482
iteration : 2084
train acc:  0.7421875
train loss:  0.5009592175483704
train gradient:  0.137962511364786
iteration : 2085
train acc:  0.734375
train loss:  0.4780222177505493
train gradient:  0.11610012919551767
iteration : 2086
train acc:  0.65625
train loss:  0.5650343894958496
train gradient:  0.1403987009685909
iteration : 2087
train acc:  0.796875
train loss:  0.42082488536834717
train gradient:  0.09289907193141077
iteration : 2088
train acc:  0.6875
train loss:  0.527079701423645
train gradient:  0.14441626146860492
iteration : 2089
train acc:  0.7890625
train loss:  0.4637463092803955
train gradient:  0.09766849087236024
iteration : 2090
train acc:  0.75
train loss:  0.45314955711364746
train gradient:  0.11402625237669224
iteration : 2091
train acc:  0.7578125
train loss:  0.45897507667541504
train gradient:  0.1018275894951678
iteration : 2092
train acc:  0.703125
train loss:  0.4881269931793213
train gradient:  0.1388886441002992
iteration : 2093
train acc:  0.7109375
train loss:  0.5417138338088989
train gradient:  0.15951117749107538
iteration : 2094
train acc:  0.6640625
train loss:  0.5616096258163452
train gradient:  0.16055970363017535
iteration : 2095
train acc:  0.7734375
train loss:  0.47573864459991455
train gradient:  0.12022452778532335
iteration : 2096
train acc:  0.75
train loss:  0.4125725030899048
train gradient:  0.08936621204161668
iteration : 2097
train acc:  0.7109375
train loss:  0.4640771448612213
train gradient:  0.10923553596305004
iteration : 2098
train acc:  0.765625
train loss:  0.47222810983657837
train gradient:  0.12999990918873908
iteration : 2099
train acc:  0.7265625
train loss:  0.4747503399848938
train gradient:  0.12280460193080178
iteration : 2100
train acc:  0.765625
train loss:  0.45041096210479736
train gradient:  0.09161798374671844
iteration : 2101
train acc:  0.71875
train loss:  0.4470575153827667
train gradient:  0.08832086100175347
iteration : 2102
train acc:  0.78125
train loss:  0.4837574064731598
train gradient:  0.14222882142595078
iteration : 2103
train acc:  0.8359375
train loss:  0.4246087670326233
train gradient:  0.10105361667984199
iteration : 2104
train acc:  0.7109375
train loss:  0.529757022857666
train gradient:  0.1586023348988554
iteration : 2105
train acc:  0.78125
train loss:  0.4676797688007355
train gradient:  0.10140921108985786
iteration : 2106
train acc:  0.6953125
train loss:  0.5661267042160034
train gradient:  0.15227612351388436
iteration : 2107
train acc:  0.78125
train loss:  0.46304911375045776
train gradient:  0.125368971044298
iteration : 2108
train acc:  0.7578125
train loss:  0.4811503291130066
train gradient:  0.12091535896488753
iteration : 2109
train acc:  0.7578125
train loss:  0.44415512681007385
train gradient:  0.106847475559943
iteration : 2110
train acc:  0.78125
train loss:  0.5120881795883179
train gradient:  0.12623645759452287
iteration : 2111
train acc:  0.71875
train loss:  0.4783991575241089
train gradient:  0.11101290183200217
iteration : 2112
train acc:  0.7421875
train loss:  0.4785670042037964
train gradient:  0.09732119599277096
iteration : 2113
train acc:  0.796875
train loss:  0.4458320140838623
train gradient:  0.12730165111428515
iteration : 2114
train acc:  0.8359375
train loss:  0.4046974778175354
train gradient:  0.09170533859295636
iteration : 2115
train acc:  0.75
train loss:  0.43705886602401733
train gradient:  0.09629967419962916
iteration : 2116
train acc:  0.796875
train loss:  0.4111793041229248
train gradient:  0.08968309721956162
iteration : 2117
train acc:  0.703125
train loss:  0.5040014386177063
train gradient:  0.11219278657212038
iteration : 2118
train acc:  0.765625
train loss:  0.4463871717453003
train gradient:  0.11813442454926724
iteration : 2119
train acc:  0.7265625
train loss:  0.4971699118614197
train gradient:  0.10907815173143653
iteration : 2120
train acc:  0.7421875
train loss:  0.49289196729660034
train gradient:  0.13973046462983518
iteration : 2121
train acc:  0.7265625
train loss:  0.46578747034072876
train gradient:  0.12167402756195465
iteration : 2122
train acc:  0.796875
train loss:  0.42017054557800293
train gradient:  0.08616188897885903
iteration : 2123
train acc:  0.765625
train loss:  0.4402543902397156
train gradient:  0.09511237319416387
iteration : 2124
train acc:  0.7890625
train loss:  0.46865010261535645
train gradient:  0.11349730227858426
iteration : 2125
train acc:  0.7109375
train loss:  0.5293238162994385
train gradient:  0.12007340681419525
iteration : 2126
train acc:  0.703125
train loss:  0.48392975330352783
train gradient:  0.10964357453258247
iteration : 2127
train acc:  0.703125
train loss:  0.5150375366210938
train gradient:  0.11613209826352085
iteration : 2128
train acc:  0.671875
train loss:  0.56669020652771
train gradient:  0.18075635496013573
iteration : 2129
train acc:  0.71875
train loss:  0.4898560345172882
train gradient:  0.14090628140298825
iteration : 2130
train acc:  0.7421875
train loss:  0.4605294466018677
train gradient:  0.12433890758988422
iteration : 2131
train acc:  0.859375
train loss:  0.390458881855011
train gradient:  0.06213157570176091
iteration : 2132
train acc:  0.71875
train loss:  0.4686976671218872
train gradient:  0.1326749643573823
iteration : 2133
train acc:  0.7890625
train loss:  0.4184771776199341
train gradient:  0.10870624633156174
iteration : 2134
train acc:  0.765625
train loss:  0.42737534642219543
train gradient:  0.09478464030689294
iteration : 2135
train acc:  0.765625
train loss:  0.5001128315925598
train gradient:  0.1163298453919794
iteration : 2136
train acc:  0.6953125
train loss:  0.5545405149459839
train gradient:  0.13324006194323723
iteration : 2137
train acc:  0.7734375
train loss:  0.4239491820335388
train gradient:  0.08768196483289224
iteration : 2138
train acc:  0.7734375
train loss:  0.4285310208797455
train gradient:  0.10845047588745965
iteration : 2139
train acc:  0.75
train loss:  0.5007146596908569
train gradient:  0.14293157201776296
iteration : 2140
train acc:  0.703125
train loss:  0.5329598188400269
train gradient:  0.15298036706320678
iteration : 2141
train acc:  0.6953125
train loss:  0.47343379259109497
train gradient:  0.09923765898618273
iteration : 2142
train acc:  0.6875
train loss:  0.5048445463180542
train gradient:  0.14420117977711566
iteration : 2143
train acc:  0.828125
train loss:  0.3873136043548584
train gradient:  0.08413868074809122
iteration : 2144
train acc:  0.7890625
train loss:  0.42911937832832336
train gradient:  0.09398551419648231
iteration : 2145
train acc:  0.7109375
train loss:  0.5467783212661743
train gradient:  0.135458830312704
iteration : 2146
train acc:  0.71875
train loss:  0.5485605597496033
train gradient:  0.12909212623384697
iteration : 2147
train acc:  0.7421875
train loss:  0.5162293314933777
train gradient:  0.1574592028765025
iteration : 2148
train acc:  0.71875
train loss:  0.4943985342979431
train gradient:  0.1160106332842208
iteration : 2149
train acc:  0.734375
train loss:  0.4882621765136719
train gradient:  0.11055789541319908
iteration : 2150
train acc:  0.7265625
train loss:  0.4996606707572937
train gradient:  0.14409118928573206
iteration : 2151
train acc:  0.75
train loss:  0.5178295373916626
train gradient:  0.12444882520230456
iteration : 2152
train acc:  0.7890625
train loss:  0.49236416816711426
train gradient:  0.1215523390358084
iteration : 2153
train acc:  0.765625
train loss:  0.4871870279312134
train gradient:  0.13601432399788121
iteration : 2154
train acc:  0.703125
train loss:  0.49508780241012573
train gradient:  0.09494116254139699
iteration : 2155
train acc:  0.7578125
train loss:  0.4811819791793823
train gradient:  0.13187092594848754
iteration : 2156
train acc:  0.7734375
train loss:  0.45464184880256653
train gradient:  0.09401217708483837
iteration : 2157
train acc:  0.7109375
train loss:  0.46261677145957947
train gradient:  0.0945194737502862
iteration : 2158
train acc:  0.7421875
train loss:  0.48140600323677063
train gradient:  0.11779810059004854
iteration : 2159
train acc:  0.6796875
train loss:  0.586970865726471
train gradient:  0.17079361016915576
iteration : 2160
train acc:  0.7578125
train loss:  0.45269012451171875
train gradient:  0.10785029716906289
iteration : 2161
train acc:  0.7421875
train loss:  0.4482942819595337
train gradient:  0.09527292563782085
iteration : 2162
train acc:  0.765625
train loss:  0.4951149821281433
train gradient:  0.12229517822912084
iteration : 2163
train acc:  0.7109375
train loss:  0.5042145252227783
train gradient:  0.15623048643352844
iteration : 2164
train acc:  0.7421875
train loss:  0.48122844099998474
train gradient:  0.10247851852010047
iteration : 2165
train acc:  0.75
train loss:  0.5105463862419128
train gradient:  0.12512612795758998
iteration : 2166
train acc:  0.6875
train loss:  0.5209783315658569
train gradient:  0.14270018619598418
iteration : 2167
train acc:  0.765625
train loss:  0.4528926610946655
train gradient:  0.1016252642609036
iteration : 2168
train acc:  0.7578125
train loss:  0.4826961159706116
train gradient:  0.1249491718424829
iteration : 2169
train acc:  0.734375
train loss:  0.5033025741577148
train gradient:  0.14213000082889068
iteration : 2170
train acc:  0.6953125
train loss:  0.518282949924469
train gradient:  0.1376221339912197
iteration : 2171
train acc:  0.671875
train loss:  0.5788983702659607
train gradient:  0.16394594090717116
iteration : 2172
train acc:  0.8203125
train loss:  0.4232971668243408
train gradient:  0.0764014919821933
iteration : 2173
train acc:  0.734375
train loss:  0.5541411638259888
train gradient:  0.18303048892971613
iteration : 2174
train acc:  0.7421875
train loss:  0.502967357635498
train gradient:  0.1256462924056345
iteration : 2175
train acc:  0.7578125
train loss:  0.4589637517929077
train gradient:  0.12342803389576017
iteration : 2176
train acc:  0.78125
train loss:  0.44825470447540283
train gradient:  0.09284775120637848
iteration : 2177
train acc:  0.7421875
train loss:  0.4858805239200592
train gradient:  0.13924027519956658
iteration : 2178
train acc:  0.7421875
train loss:  0.5126445889472961
train gradient:  0.1411795080580172
iteration : 2179
train acc:  0.6875
train loss:  0.5282853245735168
train gradient:  0.1490331956375569
iteration : 2180
train acc:  0.765625
train loss:  0.42403414845466614
train gradient:  0.09213067748161788
iteration : 2181
train acc:  0.7421875
train loss:  0.49224650859832764
train gradient:  0.1279700853179292
iteration : 2182
train acc:  0.7421875
train loss:  0.5051558017730713
train gradient:  0.13704560816907652
iteration : 2183
train acc:  0.7890625
train loss:  0.46213698387145996
train gradient:  0.08818776205911015
iteration : 2184
train acc:  0.734375
train loss:  0.4481982886791229
train gradient:  0.1045883248131259
iteration : 2185
train acc:  0.7578125
train loss:  0.49345505237579346
train gradient:  0.1103863446926942
iteration : 2186
train acc:  0.75
train loss:  0.4675457179546356
train gradient:  0.09324768860490884
iteration : 2187
train acc:  0.78125
train loss:  0.44766080379486084
train gradient:  0.11442251660661039
iteration : 2188
train acc:  0.796875
train loss:  0.4841921925544739
train gradient:  0.12105819828668316
iteration : 2189
train acc:  0.75
train loss:  0.44176456332206726
train gradient:  0.10386296154767864
iteration : 2190
train acc:  0.75
train loss:  0.4905421733856201
train gradient:  0.10553812252045439
iteration : 2191
train acc:  0.8046875
train loss:  0.4654269218444824
train gradient:  0.10118803842884744
iteration : 2192
train acc:  0.7890625
train loss:  0.44561997056007385
train gradient:  0.0866406561764639
iteration : 2193
train acc:  0.7734375
train loss:  0.4434983730316162
train gradient:  0.10124183313381496
iteration : 2194
train acc:  0.75
train loss:  0.46712809801101685
train gradient:  0.1289226496725515
iteration : 2195
train acc:  0.7890625
train loss:  0.43256646394729614
train gradient:  0.0788628643845395
iteration : 2196
train acc:  0.765625
train loss:  0.49140676856040955
train gradient:  0.12491198930946294
iteration : 2197
train acc:  0.7890625
train loss:  0.4665716290473938
train gradient:  0.1148853130711541
iteration : 2198
train acc:  0.6875
train loss:  0.5230337381362915
train gradient:  0.14950351119980657
iteration : 2199
train acc:  0.75
train loss:  0.48535701632499695
train gradient:  0.12913836967965736
iteration : 2200
train acc:  0.7734375
train loss:  0.5515446662902832
train gradient:  0.19336524714834152
iteration : 2201
train acc:  0.7734375
train loss:  0.45121699571609497
train gradient:  0.09356876272407186
iteration : 2202
train acc:  0.7265625
train loss:  0.5478427410125732
train gradient:  0.14095462152147686
iteration : 2203
train acc:  0.7265625
train loss:  0.48850345611572266
train gradient:  0.11476094638561793
iteration : 2204
train acc:  0.765625
train loss:  0.4710305333137512
train gradient:  0.12760476937660237
iteration : 2205
train acc:  0.8125
train loss:  0.42616546154022217
train gradient:  0.08870325232257983
iteration : 2206
train acc:  0.796875
train loss:  0.4388863146305084
train gradient:  0.10636326033734467
iteration : 2207
train acc:  0.7421875
train loss:  0.5251350998878479
train gradient:  0.12854221124795945
iteration : 2208
train acc:  0.78125
train loss:  0.4583204388618469
train gradient:  0.10849790655101535
iteration : 2209
train acc:  0.7421875
train loss:  0.4597522020339966
train gradient:  0.10715323360798742
iteration : 2210
train acc:  0.7578125
train loss:  0.5379736423492432
train gradient:  0.14970515929550873
iteration : 2211
train acc:  0.7421875
train loss:  0.49683475494384766
train gradient:  0.10916851813283777
iteration : 2212
train acc:  0.7109375
train loss:  0.5328686237335205
train gradient:  0.12953549251224017
iteration : 2213
train acc:  0.7109375
train loss:  0.49458789825439453
train gradient:  0.12702020731094127
iteration : 2214
train acc:  0.75
train loss:  0.5241983532905579
train gradient:  0.1319880371646564
iteration : 2215
train acc:  0.7578125
train loss:  0.5399906635284424
train gradient:  0.13813368423290356
iteration : 2216
train acc:  0.6640625
train loss:  0.5512512922286987
train gradient:  0.13478311654013878
iteration : 2217
train acc:  0.71875
train loss:  0.5328315496444702
train gradient:  0.13714084087612416
iteration : 2218
train acc:  0.7265625
train loss:  0.4940934181213379
train gradient:  0.12112218057283579
iteration : 2219
train acc:  0.75
train loss:  0.48654693365097046
train gradient:  0.11546563907791718
iteration : 2220
train acc:  0.7578125
train loss:  0.4663546681404114
train gradient:  0.10903041904839321
iteration : 2221
train acc:  0.75
train loss:  0.48368072509765625
train gradient:  0.11563341859589234
iteration : 2222
train acc:  0.7734375
train loss:  0.4464743137359619
train gradient:  0.1007467813660219
iteration : 2223
train acc:  0.6953125
train loss:  0.48904624581336975
train gradient:  0.14428924115718128
iteration : 2224
train acc:  0.734375
train loss:  0.4751947224140167
train gradient:  0.1014430206764797
iteration : 2225
train acc:  0.7734375
train loss:  0.4983232915401459
train gradient:  0.1420325107546236
iteration : 2226
train acc:  0.7890625
train loss:  0.42966800928115845
train gradient:  0.1104374827159403
iteration : 2227
train acc:  0.7578125
train loss:  0.45324957370758057
train gradient:  0.1130423557695875
iteration : 2228
train acc:  0.78125
train loss:  0.4779161214828491
train gradient:  0.09430102590811436
iteration : 2229
train acc:  0.7265625
train loss:  0.5302257537841797
train gradient:  0.14618700563852835
iteration : 2230
train acc:  0.7578125
train loss:  0.4714726209640503
train gradient:  0.1534277083647393
iteration : 2231
train acc:  0.8359375
train loss:  0.4287310838699341
train gradient:  0.10058091338525993
iteration : 2232
train acc:  0.703125
train loss:  0.5173697471618652
train gradient:  0.11604793746778332
iteration : 2233
train acc:  0.7421875
train loss:  0.4530962109565735
train gradient:  0.0989800715445658
iteration : 2234
train acc:  0.7734375
train loss:  0.4858149290084839
train gradient:  0.11131842558387513
iteration : 2235
train acc:  0.7265625
train loss:  0.5119612216949463
train gradient:  0.1233835927998798
iteration : 2236
train acc:  0.7421875
train loss:  0.533943772315979
train gradient:  0.11682609967090057
iteration : 2237
train acc:  0.7265625
train loss:  0.48146867752075195
train gradient:  0.10638031889723562
iteration : 2238
train acc:  0.71875
train loss:  0.4988812506198883
train gradient:  0.11840213788514739
iteration : 2239
train acc:  0.765625
train loss:  0.44286102056503296
train gradient:  0.09980542747102078
iteration : 2240
train acc:  0.7421875
train loss:  0.5242339968681335
train gradient:  0.16644126451598007
iteration : 2241
train acc:  0.828125
train loss:  0.39609241485595703
train gradient:  0.09024784141222726
iteration : 2242
train acc:  0.7265625
train loss:  0.4958171546459198
train gradient:  0.13231815143970188
iteration : 2243
train acc:  0.765625
train loss:  0.45379674434661865
train gradient:  0.12642888122498172
iteration : 2244
train acc:  0.8046875
train loss:  0.4126630425453186
train gradient:  0.08119435675779771
iteration : 2245
train acc:  0.7890625
train loss:  0.42866086959838867
train gradient:  0.09128299563454564
iteration : 2246
train acc:  0.71875
train loss:  0.5012754201889038
train gradient:  0.16529633078128264
iteration : 2247
train acc:  0.8046875
train loss:  0.4257124364376068
train gradient:  0.09594305643498956
iteration : 2248
train acc:  0.7578125
train loss:  0.5021367073059082
train gradient:  0.123695024244509
iteration : 2249
train acc:  0.7421875
train loss:  0.5079839825630188
train gradient:  0.11742851098723006
iteration : 2250
train acc:  0.7890625
train loss:  0.4133602976799011
train gradient:  0.11284885002459066
iteration : 2251
train acc:  0.7421875
train loss:  0.48835039138793945
train gradient:  0.09810460791588677
iteration : 2252
train acc:  0.7421875
train loss:  0.4642086327075958
train gradient:  0.10712443778541308
iteration : 2253
train acc:  0.734375
train loss:  0.4816482663154602
train gradient:  0.126948569458055
iteration : 2254
train acc:  0.7578125
train loss:  0.49357685446739197
train gradient:  0.11713272390525324
iteration : 2255
train acc:  0.6875
train loss:  0.5126381516456604
train gradient:  0.13961955383457708
iteration : 2256
train acc:  0.71875
train loss:  0.4686359763145447
train gradient:  0.1121678149418432
iteration : 2257
train acc:  0.7421875
train loss:  0.4333575963973999
train gradient:  0.09345925931839123
iteration : 2258
train acc:  0.6875
train loss:  0.5625759363174438
train gradient:  0.16157781290488832
iteration : 2259
train acc:  0.78125
train loss:  0.4266340136528015
train gradient:  0.10138597470047685
iteration : 2260
train acc:  0.78125
train loss:  0.4694104790687561
train gradient:  0.09195879493832744
iteration : 2261
train acc:  0.703125
train loss:  0.5564179420471191
train gradient:  0.17726508278964512
iteration : 2262
train acc:  0.7109375
train loss:  0.5632742643356323
train gradient:  0.17414121887240291
iteration : 2263
train acc:  0.7890625
train loss:  0.43470290303230286
train gradient:  0.1037332451197874
iteration : 2264
train acc:  0.734375
train loss:  0.5132279396057129
train gradient:  0.13165782466301756
iteration : 2265
train acc:  0.796875
train loss:  0.4404168725013733
train gradient:  0.09610265925135876
iteration : 2266
train acc:  0.78125
train loss:  0.4278854727745056
train gradient:  0.08510479601820084
iteration : 2267
train acc:  0.78125
train loss:  0.47656604647636414
train gradient:  0.10965324785593507
iteration : 2268
train acc:  0.7578125
train loss:  0.47884753346443176
train gradient:  0.09634664046837692
iteration : 2269
train acc:  0.7421875
train loss:  0.44732749462127686
train gradient:  0.09954494334286995
iteration : 2270
train acc:  0.7421875
train loss:  0.5435347557067871
train gradient:  0.17195471896037287
iteration : 2271
train acc:  0.7578125
train loss:  0.5451925992965698
train gradient:  0.1661425981540522
iteration : 2272
train acc:  0.8125
train loss:  0.4467164874076843
train gradient:  0.13732840667206309
iteration : 2273
train acc:  0.7578125
train loss:  0.4701077938079834
train gradient:  0.12029575146736547
iteration : 2274
train acc:  0.7734375
train loss:  0.4769345819950104
train gradient:  0.10864922756774315
iteration : 2275
train acc:  0.7578125
train loss:  0.4830459654331207
train gradient:  0.11013946058729433
iteration : 2276
train acc:  0.796875
train loss:  0.43067166209220886
train gradient:  0.10528618581008566
iteration : 2277
train acc:  0.7265625
train loss:  0.49214813113212585
train gradient:  0.12210794263432445
iteration : 2278
train acc:  0.71875
train loss:  0.5152353644371033
train gradient:  0.13848811784010276
iteration : 2279
train acc:  0.7265625
train loss:  0.5226486921310425
train gradient:  0.1288475154143806
iteration : 2280
train acc:  0.7578125
train loss:  0.5380806922912598
train gradient:  0.166028545241811
iteration : 2281
train acc:  0.75
train loss:  0.46461230516433716
train gradient:  0.09832066791492104
iteration : 2282
train acc:  0.78125
train loss:  0.45136427879333496
train gradient:  0.12432937772444658
iteration : 2283
train acc:  0.703125
train loss:  0.5021476149559021
train gradient:  0.11356230695668428
iteration : 2284
train acc:  0.7734375
train loss:  0.47737231850624084
train gradient:  0.09794555858809662
iteration : 2285
train acc:  0.8125
train loss:  0.44684213399887085
train gradient:  0.10436602801510653
iteration : 2286
train acc:  0.796875
train loss:  0.4265291094779968
train gradient:  0.12170818533286754
iteration : 2287
train acc:  0.7265625
train loss:  0.47240251302719116
train gradient:  0.13092247101522508
iteration : 2288
train acc:  0.78125
train loss:  0.4387863278388977
train gradient:  0.09829627668363142
iteration : 2289
train acc:  0.828125
train loss:  0.4432007968425751
train gradient:  0.09947232584478939
iteration : 2290
train acc:  0.75
train loss:  0.49885159730911255
train gradient:  0.12118329753712653
iteration : 2291
train acc:  0.7421875
train loss:  0.45397332310676575
train gradient:  0.08893593391861931
iteration : 2292
train acc:  0.7265625
train loss:  0.5826795697212219
train gradient:  0.17852874518829967
iteration : 2293
train acc:  0.7578125
train loss:  0.4374459981918335
train gradient:  0.09332828611678595
iteration : 2294
train acc:  0.7421875
train loss:  0.4993511438369751
train gradient:  0.16643433283734485
iteration : 2295
train acc:  0.734375
train loss:  0.48570477962493896
train gradient:  0.131461775158084
iteration : 2296
train acc:  0.7734375
train loss:  0.4743168354034424
train gradient:  0.12185150473900437
iteration : 2297
train acc:  0.6796875
train loss:  0.5696271657943726
train gradient:  0.13488864391629887
iteration : 2298
train acc:  0.6640625
train loss:  0.5767949223518372
train gradient:  0.1603384617257008
iteration : 2299
train acc:  0.7265625
train loss:  0.5119994878768921
train gradient:  0.125756363651367
iteration : 2300
train acc:  0.71875
train loss:  0.5383806228637695
train gradient:  0.14271387532189972
iteration : 2301
train acc:  0.7734375
train loss:  0.508415699005127
train gradient:  0.09522182658123725
iteration : 2302
train acc:  0.765625
train loss:  0.42934924364089966
train gradient:  0.08792617986962399
iteration : 2303
train acc:  0.75
train loss:  0.5337863564491272
train gradient:  0.1448753318954575
iteration : 2304
train acc:  0.7734375
train loss:  0.45856335759162903
train gradient:  0.10825272553792346
iteration : 2305
train acc:  0.7265625
train loss:  0.5363688468933105
train gradient:  0.12338600644265692
iteration : 2306
train acc:  0.734375
train loss:  0.5106744766235352
train gradient:  0.19136076866732593
iteration : 2307
train acc:  0.71875
train loss:  0.5106778144836426
train gradient:  0.12481853726348878
iteration : 2308
train acc:  0.796875
train loss:  0.40241876244544983
train gradient:  0.08851111252034334
iteration : 2309
train acc:  0.7890625
train loss:  0.4348137378692627
train gradient:  0.09635742127641807
iteration : 2310
train acc:  0.7421875
train loss:  0.4502221941947937
train gradient:  0.1097370259645169
iteration : 2311
train acc:  0.75
train loss:  0.48132532835006714
train gradient:  0.1206583278601895
iteration : 2312
train acc:  0.7734375
train loss:  0.41334831714630127
train gradient:  0.11410890418685822
iteration : 2313
train acc:  0.7734375
train loss:  0.487803190946579
train gradient:  0.12484373644466833
iteration : 2314
train acc:  0.7890625
train loss:  0.5010393261909485
train gradient:  0.11489211694093177
iteration : 2315
train acc:  0.765625
train loss:  0.47592097520828247
train gradient:  0.10310772391343925
iteration : 2316
train acc:  0.828125
train loss:  0.403698593378067
train gradient:  0.0942411019986387
iteration : 2317
train acc:  0.7421875
train loss:  0.45314300060272217
train gradient:  0.10472195664187495
iteration : 2318
train acc:  0.8125
train loss:  0.39949876070022583
train gradient:  0.07518154834629544
iteration : 2319
train acc:  0.828125
train loss:  0.4025009274482727
train gradient:  0.08193957617712869
iteration : 2320
train acc:  0.75
train loss:  0.47160524129867554
train gradient:  0.13126938828669246
iteration : 2321
train acc:  0.6796875
train loss:  0.5162582397460938
train gradient:  0.13429658863461097
iteration : 2322
train acc:  0.734375
train loss:  0.5151814818382263
train gradient:  0.10164084783927592
iteration : 2323
train acc:  0.765625
train loss:  0.4177429676055908
train gradient:  0.09388459837271479
iteration : 2324
train acc:  0.8046875
train loss:  0.3742762506008148
train gradient:  0.0633806018858052
iteration : 2325
train acc:  0.75
train loss:  0.496665894985199
train gradient:  0.1276755433421874
iteration : 2326
train acc:  0.7265625
train loss:  0.5111414194107056
train gradient:  0.1323240598337021
iteration : 2327
train acc:  0.78125
train loss:  0.4210517406463623
train gradient:  0.08049407181517837
iteration : 2328
train acc:  0.7890625
train loss:  0.4602183997631073
train gradient:  0.10145573434872268
iteration : 2329
train acc:  0.7890625
train loss:  0.4383906126022339
train gradient:  0.09030243713045444
iteration : 2330
train acc:  0.8203125
train loss:  0.4562482237815857
train gradient:  0.09676211786260233
iteration : 2331
train acc:  0.75
train loss:  0.5167466402053833
train gradient:  0.14388749573965814
iteration : 2332
train acc:  0.8125
train loss:  0.38147592544555664
train gradient:  0.0744124997630185
iteration : 2333
train acc:  0.7734375
train loss:  0.44264858961105347
train gradient:  0.08598982098908865
iteration : 2334
train acc:  0.7265625
train loss:  0.4681762456893921
train gradient:  0.10520320802669597
iteration : 2335
train acc:  0.7734375
train loss:  0.47698330879211426
train gradient:  0.09356164536708417
iteration : 2336
train acc:  0.734375
train loss:  0.4888991415500641
train gradient:  0.12529503504563647
iteration : 2337
train acc:  0.75
train loss:  0.4827292561531067
train gradient:  0.1216433345956101
iteration : 2338
train acc:  0.765625
train loss:  0.47837984561920166
train gradient:  0.126922614847986
iteration : 2339
train acc:  0.7890625
train loss:  0.4052600860595703
train gradient:  0.07978373765799074
iteration : 2340
train acc:  0.7890625
train loss:  0.4251849949359894
train gradient:  0.09112881222633844
iteration : 2341
train acc:  0.78125
train loss:  0.4071179926395416
train gradient:  0.10398343684499053
iteration : 2342
train acc:  0.75
train loss:  0.4807404577732086
train gradient:  0.14652437640644983
iteration : 2343
train acc:  0.765625
train loss:  0.42874830961227417
train gradient:  0.09071924241111948
iteration : 2344
train acc:  0.734375
train loss:  0.4952681064605713
train gradient:  0.13254798562627199
iteration : 2345
train acc:  0.703125
train loss:  0.5067740678787231
train gradient:  0.1303893738759923
iteration : 2346
train acc:  0.7734375
train loss:  0.48029059171676636
train gradient:  0.12294499669786453
iteration : 2347
train acc:  0.7421875
train loss:  0.5073510408401489
train gradient:  0.11910487430458756
iteration : 2348
train acc:  0.71875
train loss:  0.48809748888015747
train gradient:  0.12881849279439048
iteration : 2349
train acc:  0.7109375
train loss:  0.583480715751648
train gradient:  0.1750565678925436
iteration : 2350
train acc:  0.78125
train loss:  0.44001829624176025
train gradient:  0.10275678549304225
iteration : 2351
train acc:  0.7890625
train loss:  0.43022388219833374
train gradient:  0.11594215184984331
iteration : 2352
train acc:  0.7578125
train loss:  0.44923168420791626
train gradient:  0.1146702060943327
iteration : 2353
train acc:  0.765625
train loss:  0.4818273186683655
train gradient:  0.1621667987008611
iteration : 2354
train acc:  0.75
train loss:  0.5272780060768127
train gradient:  0.13019389077463972
iteration : 2355
train acc:  0.71875
train loss:  0.5015703439712524
train gradient:  0.13118133151443054
iteration : 2356
train acc:  0.7265625
train loss:  0.4755267798900604
train gradient:  0.12181142807599042
iteration : 2357
train acc:  0.7578125
train loss:  0.46207430958747864
train gradient:  0.13044771649268103
iteration : 2358
train acc:  0.7890625
train loss:  0.4417914152145386
train gradient:  0.12269879924721093
iteration : 2359
train acc:  0.7734375
train loss:  0.4502781927585602
train gradient:  0.10976169907372976
iteration : 2360
train acc:  0.765625
train loss:  0.5007777214050293
train gradient:  0.15003709617895522
iteration : 2361
train acc:  0.7421875
train loss:  0.5092914700508118
train gradient:  0.11812392951686893
iteration : 2362
train acc:  0.734375
train loss:  0.4904085099697113
train gradient:  0.12946348670014496
iteration : 2363
train acc:  0.734375
train loss:  0.469196081161499
train gradient:  0.11579095042740883
iteration : 2364
train acc:  0.7265625
train loss:  0.4763420522212982
train gradient:  0.12719990903555856
iteration : 2365
train acc:  0.7265625
train loss:  0.4908914566040039
train gradient:  0.11764819228893561
iteration : 2366
train acc:  0.7421875
train loss:  0.5118722319602966
train gradient:  0.14175001237932072
iteration : 2367
train acc:  0.734375
train loss:  0.47174298763275146
train gradient:  0.1413449900383414
iteration : 2368
train acc:  0.7421875
train loss:  0.4785056412220001
train gradient:  0.12638966800097076
iteration : 2369
train acc:  0.765625
train loss:  0.47802436351776123
train gradient:  0.1008435696256337
iteration : 2370
train acc:  0.78125
train loss:  0.4301880896091461
train gradient:  0.10840925426846655
iteration : 2371
train acc:  0.7265625
train loss:  0.5201900005340576
train gradient:  0.1425678670248262
iteration : 2372
train acc:  0.84375
train loss:  0.38712137937545776
train gradient:  0.09089250388535366
iteration : 2373
train acc:  0.7421875
train loss:  0.507870078086853
train gradient:  0.12342961297531392
iteration : 2374
train acc:  0.8125
train loss:  0.43287214636802673
train gradient:  0.11033604760633381
iteration : 2375
train acc:  0.703125
train loss:  0.5389329791069031
train gradient:  0.180396282367149
iteration : 2376
train acc:  0.7265625
train loss:  0.4711849093437195
train gradient:  0.11921804519262874
iteration : 2377
train acc:  0.8125
train loss:  0.40461838245391846
train gradient:  0.10746271136814797
iteration : 2378
train acc:  0.71875
train loss:  0.5005420446395874
train gradient:  0.10442898764013629
iteration : 2379
train acc:  0.65625
train loss:  0.5504389405250549
train gradient:  0.15524135632600555
iteration : 2380
train acc:  0.734375
train loss:  0.5582469701766968
train gradient:  0.1988196917300223
iteration : 2381
train acc:  0.7265625
train loss:  0.47508126497268677
train gradient:  0.10399189092498298
iteration : 2382
train acc:  0.7890625
train loss:  0.4341149926185608
train gradient:  0.1176460949710217
iteration : 2383
train acc:  0.7421875
train loss:  0.50307297706604
train gradient:  0.12174113591830484
iteration : 2384
train acc:  0.671875
train loss:  0.584816575050354
train gradient:  0.17546611363180006
iteration : 2385
train acc:  0.78125
train loss:  0.4692328870296478
train gradient:  0.138832251014567
iteration : 2386
train acc:  0.703125
train loss:  0.5376325249671936
train gradient:  0.1352797193551091
iteration : 2387
train acc:  0.765625
train loss:  0.5082291960716248
train gradient:  0.12876804739087214
iteration : 2388
train acc:  0.7265625
train loss:  0.5247933268547058
train gradient:  0.12709597882276819
iteration : 2389
train acc:  0.84375
train loss:  0.36836761236190796
train gradient:  0.06883190180321484
iteration : 2390
train acc:  0.8046875
train loss:  0.4311032295227051
train gradient:  0.08677177214685823
iteration : 2391
train acc:  0.75
train loss:  0.5007240772247314
train gradient:  0.17000347678173336
iteration : 2392
train acc:  0.8046875
train loss:  0.47245633602142334
train gradient:  0.1373054107870597
iteration : 2393
train acc:  0.765625
train loss:  0.4382892847061157
train gradient:  0.09783993087117007
iteration : 2394
train acc:  0.6875
train loss:  0.5268721580505371
train gradient:  0.1325024851735002
iteration : 2395
train acc:  0.71875
train loss:  0.5180201530456543
train gradient:  0.1320242736016274
iteration : 2396
train acc:  0.7578125
train loss:  0.41114139556884766
train gradient:  0.09833819429661117
iteration : 2397
train acc:  0.7578125
train loss:  0.4669347405433655
train gradient:  0.10408886946294861
iteration : 2398
train acc:  0.796875
train loss:  0.43924933671951294
train gradient:  0.09302381650124822
iteration : 2399
train acc:  0.7578125
train loss:  0.4258396029472351
train gradient:  0.10911391380472607
iteration : 2400
train acc:  0.75
train loss:  0.5102500915527344
train gradient:  0.11853010795050006
iteration : 2401
train acc:  0.765625
train loss:  0.45422154664993286
train gradient:  0.09493486363088387
iteration : 2402
train acc:  0.7734375
train loss:  0.4514945149421692
train gradient:  0.11966622088893232
iteration : 2403
train acc:  0.6328125
train loss:  0.5540900230407715
train gradient:  0.13051237028588492
iteration : 2404
train acc:  0.8046875
train loss:  0.4879990220069885
train gradient:  0.1026917891905224
iteration : 2405
train acc:  0.7578125
train loss:  0.4980151653289795
train gradient:  0.16179114148830082
iteration : 2406
train acc:  0.7265625
train loss:  0.5591388940811157
train gradient:  0.19800471535353198
iteration : 2407
train acc:  0.75
train loss:  0.4916696846485138
train gradient:  0.12731724624337115
iteration : 2408
train acc:  0.7421875
train loss:  0.44931817054748535
train gradient:  0.11729318857168743
iteration : 2409
train acc:  0.7265625
train loss:  0.4886934161186218
train gradient:  0.13395959429976448
iteration : 2410
train acc:  0.7265625
train loss:  0.5157681703567505
train gradient:  0.1246178972017608
iteration : 2411
train acc:  0.78125
train loss:  0.4278569221496582
train gradient:  0.1016152872675405
iteration : 2412
train acc:  0.8046875
train loss:  0.41471758484840393
train gradient:  0.08043985714019875
iteration : 2413
train acc:  0.78125
train loss:  0.41714048385620117
train gradient:  0.11291197406736081
iteration : 2414
train acc:  0.78125
train loss:  0.47215667366981506
train gradient:  0.11818442184183926
iteration : 2415
train acc:  0.78125
train loss:  0.4523051083087921
train gradient:  0.11034139885980324
iteration : 2416
train acc:  0.71875
train loss:  0.4725167155265808
train gradient:  0.13570935333839523
iteration : 2417
train acc:  0.796875
train loss:  0.447445273399353
train gradient:  0.10205159672934304
iteration : 2418
train acc:  0.6953125
train loss:  0.5415711402893066
train gradient:  0.12959292770065745
iteration : 2419
train acc:  0.7421875
train loss:  0.526720404624939
train gradient:  0.13783203060681343
iteration : 2420
train acc:  0.7421875
train loss:  0.4641266465187073
train gradient:  0.09972382983356066
iteration : 2421
train acc:  0.7578125
train loss:  0.4715593755245209
train gradient:  0.12638413103268464
iteration : 2422
train acc:  0.765625
train loss:  0.47622987627983093
train gradient:  0.15132747632936983
iteration : 2423
train acc:  0.7421875
train loss:  0.5045009851455688
train gradient:  0.13285697411662786
iteration : 2424
train acc:  0.7890625
train loss:  0.43607914447784424
train gradient:  0.09799564436664185
iteration : 2425
train acc:  0.75
train loss:  0.4978034198284149
train gradient:  0.10522804713964784
iteration : 2426
train acc:  0.8046875
train loss:  0.4290080666542053
train gradient:  0.09083339913634199
iteration : 2427
train acc:  0.734375
train loss:  0.45940425992012024
train gradient:  0.09666035245810706
iteration : 2428
train acc:  0.796875
train loss:  0.4449895918369293
train gradient:  0.09986008443148317
iteration : 2429
train acc:  0.8515625
train loss:  0.4326191246509552
train gradient:  0.0955319161638261
iteration : 2430
train acc:  0.78125
train loss:  0.4375683665275574
train gradient:  0.10921536654449492
iteration : 2431
train acc:  0.6875
train loss:  0.5362151265144348
train gradient:  0.15328477785537847
iteration : 2432
train acc:  0.7421875
train loss:  0.5280958414077759
train gradient:  0.1227429811279077
iteration : 2433
train acc:  0.7578125
train loss:  0.44950711727142334
train gradient:  0.11440192700773767
iteration : 2434
train acc:  0.75
train loss:  0.5668324828147888
train gradient:  0.13956730816948743
iteration : 2435
train acc:  0.8046875
train loss:  0.4061874747276306
train gradient:  0.09734148521712951
iteration : 2436
train acc:  0.78125
train loss:  0.4340019226074219
train gradient:  0.09711928661284655
iteration : 2437
train acc:  0.765625
train loss:  0.4298308193683624
train gradient:  0.0871385464920287
iteration : 2438
train acc:  0.7734375
train loss:  0.47321152687072754
train gradient:  0.1360287382755108
iteration : 2439
train acc:  0.7578125
train loss:  0.51838219165802
train gradient:  0.12629234565203337
iteration : 2440
train acc:  0.671875
train loss:  0.6083470582962036
train gradient:  0.16144069133856595
iteration : 2441
train acc:  0.7578125
train loss:  0.4390491545200348
train gradient:  0.09817767047355364
iteration : 2442
train acc:  0.8203125
train loss:  0.42676812410354614
train gradient:  0.09205042106704676
iteration : 2443
train acc:  0.75
train loss:  0.4945921003818512
train gradient:  0.16642115515693295
iteration : 2444
train acc:  0.78125
train loss:  0.4448169767856598
train gradient:  0.11001957209106773
iteration : 2445
train acc:  0.703125
train loss:  0.4822860658168793
train gradient:  0.12533550853974845
iteration : 2446
train acc:  0.75
train loss:  0.47517454624176025
train gradient:  0.11163971012262659
iteration : 2447
train acc:  0.765625
train loss:  0.48108571767807007
train gradient:  0.11997896568681553
iteration : 2448
train acc:  0.8046875
train loss:  0.39760634303092957
train gradient:  0.06577638039048095
iteration : 2449
train acc:  0.734375
train loss:  0.5246350765228271
train gradient:  0.13328139764079813
iteration : 2450
train acc:  0.6640625
train loss:  0.5277470350265503
train gradient:  0.14246629642095338
iteration : 2451
train acc:  0.71875
train loss:  0.5000206232070923
train gradient:  0.1204161505706017
iteration : 2452
train acc:  0.6796875
train loss:  0.5226512551307678
train gradient:  0.11954376181766763
iteration : 2453
train acc:  0.7734375
train loss:  0.48035120964050293
train gradient:  0.11439462804752314
iteration : 2454
train acc:  0.7890625
train loss:  0.4546319544315338
train gradient:  0.13232509118528196
iteration : 2455
train acc:  0.71875
train loss:  0.5419485569000244
train gradient:  0.1536170781789184
iteration : 2456
train acc:  0.71875
train loss:  0.5236467719078064
train gradient:  0.1434447138453754
iteration : 2457
train acc:  0.7578125
train loss:  0.46285977959632874
train gradient:  0.10891419704028671
iteration : 2458
train acc:  0.7265625
train loss:  0.547835648059845
train gradient:  0.15537343424058941
iteration : 2459
train acc:  0.7578125
train loss:  0.45564818382263184
train gradient:  0.09644672235399532
iteration : 2460
train acc:  0.671875
train loss:  0.522464394569397
train gradient:  0.12232215934626021
iteration : 2461
train acc:  0.7890625
train loss:  0.4485818147659302
train gradient:  0.10135045207690119
iteration : 2462
train acc:  0.7421875
train loss:  0.4864391088485718
train gradient:  0.11369545682714105
iteration : 2463
train acc:  0.7109375
train loss:  0.5178611874580383
train gradient:  0.12648264404651066
iteration : 2464
train acc:  0.703125
train loss:  0.48126021027565
train gradient:  0.09850086444718573
iteration : 2465
train acc:  0.765625
train loss:  0.4653615951538086
train gradient:  0.10981382456392721
iteration : 2466
train acc:  0.7578125
train loss:  0.47253113985061646
train gradient:  0.11726625303960098
iteration : 2467
train acc:  0.7421875
train loss:  0.5173749327659607
train gradient:  0.13364316473422394
iteration : 2468
train acc:  0.7890625
train loss:  0.46969252824783325
train gradient:  0.12418367437649595
iteration : 2469
train acc:  0.796875
train loss:  0.42454850673675537
train gradient:  0.09731571530184527
iteration : 2470
train acc:  0.7578125
train loss:  0.45945411920547485
train gradient:  0.09769610877805257
iteration : 2471
train acc:  0.765625
train loss:  0.4621785283088684
train gradient:  0.10839931001992073
iteration : 2472
train acc:  0.7421875
train loss:  0.4616924226284027
train gradient:  0.08896755902152441
iteration : 2473
train acc:  0.7265625
train loss:  0.49619466066360474
train gradient:  0.1348477821221609
iteration : 2474
train acc:  0.7109375
train loss:  0.5472230315208435
train gradient:  0.1345233033774093
iteration : 2475
train acc:  0.7890625
train loss:  0.4295467734336853
train gradient:  0.1343045159167003
iteration : 2476
train acc:  0.78125
train loss:  0.4909108877182007
train gradient:  0.10627348915864449
iteration : 2477
train acc:  0.8203125
train loss:  0.39812004566192627
train gradient:  0.1050341127545795
iteration : 2478
train acc:  0.796875
train loss:  0.4079066514968872
train gradient:  0.10065144179168306
iteration : 2479
train acc:  0.765625
train loss:  0.5124971866607666
train gradient:  0.12836275011127157
iteration : 2480
train acc:  0.8125
train loss:  0.43221962451934814
train gradient:  0.08679383983277769
iteration : 2481
train acc:  0.8046875
train loss:  0.4147840142250061
train gradient:  0.0702908264462619
iteration : 2482
train acc:  0.7265625
train loss:  0.4957115650177002
train gradient:  0.12346401145583465
iteration : 2483
train acc:  0.734375
train loss:  0.5241371393203735
train gradient:  0.1330271036681195
iteration : 2484
train acc:  0.734375
train loss:  0.46657228469848633
train gradient:  0.10223114900275605
iteration : 2485
train acc:  0.765625
train loss:  0.4817555546760559
train gradient:  0.1409031401580253
iteration : 2486
train acc:  0.6875
train loss:  0.48906052112579346
train gradient:  0.13969300929171077
iteration : 2487
train acc:  0.7109375
train loss:  0.5136041641235352
train gradient:  0.1289232265074618
iteration : 2488
train acc:  0.78125
train loss:  0.483264297246933
train gradient:  0.10476290718484699
iteration : 2489
train acc:  0.703125
train loss:  0.49217429757118225
train gradient:  0.10672270989026343
iteration : 2490
train acc:  0.75
train loss:  0.48812535405158997
train gradient:  0.11924433630963183
iteration : 2491
train acc:  0.8125
train loss:  0.42042917013168335
train gradient:  0.08346002748443655
iteration : 2492
train acc:  0.78125
train loss:  0.46653375029563904
train gradient:  0.1867130759979154
iteration : 2493
train acc:  0.7578125
train loss:  0.43579161167144775
train gradient:  0.09837493522736415
iteration : 2494
train acc:  0.703125
train loss:  0.48537760972976685
train gradient:  0.09814835234027355
iteration : 2495
train acc:  0.7265625
train loss:  0.4985947608947754
train gradient:  0.1209902986988689
iteration : 2496
train acc:  0.765625
train loss:  0.459737628698349
train gradient:  0.10185447875247768
iteration : 2497
train acc:  0.65625
train loss:  0.4883403778076172
train gradient:  0.10477359020904813
iteration : 2498
train acc:  0.75
train loss:  0.46747922897338867
train gradient:  0.11671543903305914
iteration : 2499
train acc:  0.7265625
train loss:  0.4826999306678772
train gradient:  0.1173148337838927
iteration : 2500
train acc:  0.7578125
train loss:  0.46855491399765015
train gradient:  0.12296409840331889
iteration : 2501
train acc:  0.78125
train loss:  0.4412729740142822
train gradient:  0.11016941689290925
iteration : 2502
train acc:  0.6875
train loss:  0.5401626825332642
train gradient:  0.1324289022234548
iteration : 2503
train acc:  0.6640625
train loss:  0.5681672096252441
train gradient:  0.15192555332375401
iteration : 2504
train acc:  0.75
train loss:  0.4977242648601532
train gradient:  0.13823311648689374
iteration : 2505
train acc:  0.71875
train loss:  0.4964107275009155
train gradient:  0.1441605412882138
iteration : 2506
train acc:  0.7265625
train loss:  0.5484750866889954
train gradient:  0.15781372762832546
iteration : 2507
train acc:  0.75
train loss:  0.5103839635848999
train gradient:  0.11263979556892745
iteration : 2508
train acc:  0.6796875
train loss:  0.5360904932022095
train gradient:  0.12159010365425416
iteration : 2509
train acc:  0.6796875
train loss:  0.5726848244667053
train gradient:  0.13000719026527532
iteration : 2510
train acc:  0.765625
train loss:  0.47377726435661316
train gradient:  0.11802863928754183
iteration : 2511
train acc:  0.6796875
train loss:  0.5196460485458374
train gradient:  0.17908805083768156
iteration : 2512
train acc:  0.7734375
train loss:  0.4907488226890564
train gradient:  0.1210575096237177
iteration : 2513
train acc:  0.7890625
train loss:  0.45606207847595215
train gradient:  0.09910922434926997
iteration : 2514
train acc:  0.6953125
train loss:  0.5175960063934326
train gradient:  0.12377709887344145
iteration : 2515
train acc:  0.75
train loss:  0.4765722155570984
train gradient:  0.1260115019225912
iteration : 2516
train acc:  0.8046875
train loss:  0.4654077887535095
train gradient:  0.12153334281355604
iteration : 2517
train acc:  0.75
train loss:  0.476144015789032
train gradient:  0.10993033410819976
iteration : 2518
train acc:  0.75
train loss:  0.437599241733551
train gradient:  0.0842375900261725
iteration : 2519
train acc:  0.7109375
train loss:  0.495769202709198
train gradient:  0.13225689690885034
iteration : 2520
train acc:  0.7890625
train loss:  0.4846383035182953
train gradient:  0.11620508616209164
iteration : 2521
train acc:  0.734375
train loss:  0.5249359607696533
train gradient:  0.1224795341508992
iteration : 2522
train acc:  0.75
train loss:  0.4861432611942291
train gradient:  0.1265160338879415
iteration : 2523
train acc:  0.7890625
train loss:  0.4379163384437561
train gradient:  0.11749920930106997
iteration : 2524
train acc:  0.7109375
train loss:  0.504265308380127
train gradient:  0.11083802209514493
iteration : 2525
train acc:  0.7578125
train loss:  0.47866833209991455
train gradient:  0.10465070121256681
iteration : 2526
train acc:  0.796875
train loss:  0.4079146981239319
train gradient:  0.10892293205021297
iteration : 2527
train acc:  0.7734375
train loss:  0.44628989696502686
train gradient:  0.1068690050275437
iteration : 2528
train acc:  0.7109375
train loss:  0.4884822964668274
train gradient:  0.12245443699931202
iteration : 2529
train acc:  0.75
train loss:  0.4743332266807556
train gradient:  0.1291498978704178
iteration : 2530
train acc:  0.6875
train loss:  0.5699078440666199
train gradient:  0.1330620327539378
iteration : 2531
train acc:  0.796875
train loss:  0.41162675619125366
train gradient:  0.07317009423674344
iteration : 2532
train acc:  0.7578125
train loss:  0.4697078466415405
train gradient:  0.12278269603543568
iteration : 2533
train acc:  0.734375
train loss:  0.5395739078521729
train gradient:  0.18544902425770038
iteration : 2534
train acc:  0.734375
train loss:  0.49977022409439087
train gradient:  0.11273238747802411
iteration : 2535
train acc:  0.6875
train loss:  0.530099630355835
train gradient:  0.1372374006935988
iteration : 2536
train acc:  0.71875
train loss:  0.4857848882675171
train gradient:  0.12937724927305627
iteration : 2537
train acc:  0.8125
train loss:  0.3841875493526459
train gradient:  0.0891885558517781
iteration : 2538
train acc:  0.6875
train loss:  0.5492457151412964
train gradient:  0.13164340994793522
iteration : 2539
train acc:  0.7265625
train loss:  0.5019330978393555
train gradient:  0.11774338215475597
iteration : 2540
train acc:  0.6640625
train loss:  0.5619292259216309
train gradient:  0.16247342864531883
iteration : 2541
train acc:  0.6875
train loss:  0.5131656527519226
train gradient:  0.1619357512718398
iteration : 2542
train acc:  0.7734375
train loss:  0.45815572142601013
train gradient:  0.11969625104873381
iteration : 2543
train acc:  0.78125
train loss:  0.40887659788131714
train gradient:  0.09650321582713385
iteration : 2544
train acc:  0.7421875
train loss:  0.5042852163314819
train gradient:  0.11910951109569506
iteration : 2545
train acc:  0.78125
train loss:  0.4562245011329651
train gradient:  0.13924936828309464
iteration : 2546
train acc:  0.7109375
train loss:  0.5236708521842957
train gradient:  0.10223873046718537
iteration : 2547
train acc:  0.7734375
train loss:  0.411128968000412
train gradient:  0.08882225431757848
iteration : 2548
train acc:  0.7109375
train loss:  0.47711849212646484
train gradient:  0.11098326059277783
iteration : 2549
train acc:  0.78125
train loss:  0.46339935064315796
train gradient:  0.11138114287937967
iteration : 2550
train acc:  0.7109375
train loss:  0.5051795840263367
train gradient:  0.09597771753397484
iteration : 2551
train acc:  0.7734375
train loss:  0.4609365463256836
train gradient:  0.09022488520584236
iteration : 2552
train acc:  0.734375
train loss:  0.44647541642189026
train gradient:  0.091600970225691
iteration : 2553
train acc:  0.7734375
train loss:  0.4836829900741577
train gradient:  0.12601084558991577
iteration : 2554
train acc:  0.7265625
train loss:  0.5259517431259155
train gradient:  0.1432435945413808
iteration : 2555
train acc:  0.78125
train loss:  0.5033407211303711
train gradient:  0.15017757337637339
iteration : 2556
train acc:  0.7109375
train loss:  0.4811713695526123
train gradient:  0.10545078892882018
iteration : 2557
train acc:  0.7734375
train loss:  0.48780691623687744
train gradient:  0.09845454475260983
iteration : 2558
train acc:  0.7890625
train loss:  0.5066040754318237
train gradient:  0.11841766411106053
iteration : 2559
train acc:  0.7734375
train loss:  0.44669586420059204
train gradient:  0.09248991980357782
iteration : 2560
train acc:  0.8125
train loss:  0.4084357023239136
train gradient:  0.08047634510247825
iteration : 2561
train acc:  0.71875
train loss:  0.4886534810066223
train gradient:  0.10977194019620168
iteration : 2562
train acc:  0.7890625
train loss:  0.4364299178123474
train gradient:  0.0864109494552794
iteration : 2563
train acc:  0.7734375
train loss:  0.45569273829460144
train gradient:  0.097810370875195
iteration : 2564
train acc:  0.6875
train loss:  0.5730204582214355
train gradient:  0.13714978273446418
iteration : 2565
train acc:  0.7578125
train loss:  0.4299866557121277
train gradient:  0.09070813723145066
iteration : 2566
train acc:  0.8125
train loss:  0.46188151836395264
train gradient:  0.12382579941962388
iteration : 2567
train acc:  0.796875
train loss:  0.4308093190193176
train gradient:  0.09136817605016748
iteration : 2568
train acc:  0.8203125
train loss:  0.42982250452041626
train gradient:  0.10105768949646143
iteration : 2569
train acc:  0.78125
train loss:  0.46715211868286133
train gradient:  0.11199752265257579
iteration : 2570
train acc:  0.796875
train loss:  0.4224357008934021
train gradient:  0.09555891843493253
iteration : 2571
train acc:  0.7734375
train loss:  0.4828479290008545
train gradient:  0.10044920259237196
iteration : 2572
train acc:  0.7890625
train loss:  0.4153027832508087
train gradient:  0.10957394538655595
iteration : 2573
train acc:  0.734375
train loss:  0.530958890914917
train gradient:  0.16387759442983102
iteration : 2574
train acc:  0.8046875
train loss:  0.42219194769859314
train gradient:  0.07054538810205721
iteration : 2575
train acc:  0.7109375
train loss:  0.49284809827804565
train gradient:  0.1168208288804596
iteration : 2576
train acc:  0.7734375
train loss:  0.4988411068916321
train gradient:  0.1210489591082296
iteration : 2577
train acc:  0.75
train loss:  0.4793025851249695
train gradient:  0.1225179307124714
iteration : 2578
train acc:  0.7578125
train loss:  0.46758732199668884
train gradient:  0.10576065905223439
iteration : 2579
train acc:  0.75
train loss:  0.42249661684036255
train gradient:  0.09275930934761295
iteration : 2580
train acc:  0.6484375
train loss:  0.5452405214309692
train gradient:  0.14657640972359526
iteration : 2581
train acc:  0.78125
train loss:  0.45662450790405273
train gradient:  0.09334274718331056
iteration : 2582
train acc:  0.71875
train loss:  0.532261312007904
train gradient:  0.13930575775451912
iteration : 2583
train acc:  0.7578125
train loss:  0.47018563747406006
train gradient:  0.1219573645997576
iteration : 2584
train acc:  0.7734375
train loss:  0.45655885338783264
train gradient:  0.0964008618974244
iteration : 2585
train acc:  0.71875
train loss:  0.47772008180618286
train gradient:  0.11531581097082214
iteration : 2586
train acc:  0.7734375
train loss:  0.43720826506614685
train gradient:  0.09540766363136462
iteration : 2587
train acc:  0.734375
train loss:  0.4875659942626953
train gradient:  0.0991973655596958
iteration : 2588
train acc:  0.75
train loss:  0.4457499384880066
train gradient:  0.09040054021054368
iteration : 2589
train acc:  0.7421875
train loss:  0.4622541666030884
train gradient:  0.11546860839657423
iteration : 2590
train acc:  0.7421875
train loss:  0.4841541647911072
train gradient:  0.11576275085969298
iteration : 2591
train acc:  0.8125
train loss:  0.40206044912338257
train gradient:  0.08480343202449302
iteration : 2592
train acc:  0.75
train loss:  0.4508056044578552
train gradient:  0.0958479874373601
iteration : 2593
train acc:  0.6953125
train loss:  0.5238030552864075
train gradient:  0.11308205648753794
iteration : 2594
train acc:  0.6796875
train loss:  0.5463958382606506
train gradient:  0.14543269691945482
iteration : 2595
train acc:  0.765625
train loss:  0.43863561749458313
train gradient:  0.10524828777590607
iteration : 2596
train acc:  0.703125
train loss:  0.5369317531585693
train gradient:  0.16873058161691706
iteration : 2597
train acc:  0.78125
train loss:  0.48223209381103516
train gradient:  0.16001636079498058
iteration : 2598
train acc:  0.7421875
train loss:  0.5127906799316406
train gradient:  0.11403457857901862
iteration : 2599
train acc:  0.765625
train loss:  0.4776807427406311
train gradient:  0.1350030412895064
iteration : 2600
train acc:  0.734375
train loss:  0.470763623714447
train gradient:  0.10834880889151155
iteration : 2601
train acc:  0.7421875
train loss:  0.4889286756515503
train gradient:  0.13882203149521075
iteration : 2602
train acc:  0.8203125
train loss:  0.43616414070129395
train gradient:  0.11898022649319251
iteration : 2603
train acc:  0.765625
train loss:  0.4288305342197418
train gradient:  0.0942612159687364
iteration : 2604
train acc:  0.8203125
train loss:  0.41693347692489624
train gradient:  0.14006457453817495
iteration : 2605
train acc:  0.7578125
train loss:  0.4581735134124756
train gradient:  0.09088416408605106
iteration : 2606
train acc:  0.703125
train loss:  0.48411819338798523
train gradient:  0.11512254861709734
iteration : 2607
train acc:  0.7578125
train loss:  0.4801715612411499
train gradient:  0.10763392813776346
iteration : 2608
train acc:  0.7421875
train loss:  0.49164459109306335
train gradient:  0.1556721790789678
iteration : 2609
train acc:  0.7890625
train loss:  0.48562556505203247
train gradient:  0.14152594599550716
iteration : 2610
train acc:  0.7734375
train loss:  0.47351887822151184
train gradient:  0.12439381779856162
iteration : 2611
train acc:  0.765625
train loss:  0.49703341722488403
train gradient:  0.14528159586562178
iteration : 2612
train acc:  0.8046875
train loss:  0.43364912271499634
train gradient:  0.10436632460689224
iteration : 2613
train acc:  0.6640625
train loss:  0.5631679892539978
train gradient:  0.15009423383379716
iteration : 2614
train acc:  0.734375
train loss:  0.4672151505947113
train gradient:  0.11413521545109662
iteration : 2615
train acc:  0.78125
train loss:  0.45438894629478455
train gradient:  0.10454978700113586
iteration : 2616
train acc:  0.7578125
train loss:  0.48977822065353394
train gradient:  0.14028766464873102
iteration : 2617
train acc:  0.8203125
train loss:  0.37927013635635376
train gradient:  0.06815005713508564
iteration : 2618
train acc:  0.7265625
train loss:  0.5056076645851135
train gradient:  0.10642863231228847
iteration : 2619
train acc:  0.796875
train loss:  0.4515115022659302
train gradient:  0.10505412721944633
iteration : 2620
train acc:  0.7109375
train loss:  0.5895838737487793
train gradient:  0.17721473959760448
iteration : 2621
train acc:  0.7265625
train loss:  0.5061979293823242
train gradient:  0.13917702034848378
iteration : 2622
train acc:  0.8046875
train loss:  0.4499453902244568
train gradient:  0.09637454439088342
iteration : 2623
train acc:  0.796875
train loss:  0.4756328761577606
train gradient:  0.10939568116000059
iteration : 2624
train acc:  0.65625
train loss:  0.612445592880249
train gradient:  0.1400671680511119
iteration : 2625
train acc:  0.765625
train loss:  0.4272831678390503
train gradient:  0.08822278407655658
iteration : 2626
train acc:  0.7734375
train loss:  0.4148464798927307
train gradient:  0.08767114724374422
iteration : 2627
train acc:  0.6875
train loss:  0.5350643396377563
train gradient:  0.12903679256961298
iteration : 2628
train acc:  0.734375
train loss:  0.4610181450843811
train gradient:  0.08856910817461026
iteration : 2629
train acc:  0.71875
train loss:  0.5027741193771362
train gradient:  0.11879768855178668
iteration : 2630
train acc:  0.7421875
train loss:  0.49492132663726807
train gradient:  0.10440049884988677
iteration : 2631
train acc:  0.765625
train loss:  0.4051339328289032
train gradient:  0.084853213618327
iteration : 2632
train acc:  0.734375
train loss:  0.5210801362991333
train gradient:  0.12609047389741967
iteration : 2633
train acc:  0.796875
train loss:  0.423038512468338
train gradient:  0.08613011789895282
iteration : 2634
train acc:  0.8046875
train loss:  0.46527934074401855
train gradient:  0.1161751702760371
iteration : 2635
train acc:  0.640625
train loss:  0.5755707621574402
train gradient:  0.1702379609946783
iteration : 2636
train acc:  0.6484375
train loss:  0.5594137907028198
train gradient:  0.1376556959650968
iteration : 2637
train acc:  0.7578125
train loss:  0.4493337571620941
train gradient:  0.11291854652532937
iteration : 2638
train acc:  0.765625
train loss:  0.460100919008255
train gradient:  0.0961077387197884
iteration : 2639
train acc:  0.71875
train loss:  0.535158634185791
train gradient:  0.16015384846298444
iteration : 2640
train acc:  0.703125
train loss:  0.49698659777641296
train gradient:  0.13082062367690095
iteration : 2641
train acc:  0.7265625
train loss:  0.4283238351345062
train gradient:  0.10906735136920438
iteration : 2642
train acc:  0.78125
train loss:  0.39937159419059753
train gradient:  0.09864554528716457
iteration : 2643
train acc:  0.8046875
train loss:  0.4565814137458801
train gradient:  0.12118556248068484
iteration : 2644
train acc:  0.8203125
train loss:  0.3977269232273102
train gradient:  0.0878193890119224
iteration : 2645
train acc:  0.75
train loss:  0.5122690200805664
train gradient:  0.12788047057316687
iteration : 2646
train acc:  0.734375
train loss:  0.47775495052337646
train gradient:  0.10127339632769952
iteration : 2647
train acc:  0.7734375
train loss:  0.4994692802429199
train gradient:  0.13107629308924734
iteration : 2648
train acc:  0.7265625
train loss:  0.5591144561767578
train gradient:  0.1272321941351085
iteration : 2649
train acc:  0.8125
train loss:  0.427978515625
train gradient:  0.14421998934083846
iteration : 2650
train acc:  0.7734375
train loss:  0.47513896226882935
train gradient:  0.10042168051928159
iteration : 2651
train acc:  0.7890625
train loss:  0.4057684540748596
train gradient:  0.10245555934831226
iteration : 2652
train acc:  0.6640625
train loss:  0.5712422728538513
train gradient:  0.1444622724157818
iteration : 2653
train acc:  0.734375
train loss:  0.506588339805603
train gradient:  0.1302701692771786
iteration : 2654
train acc:  0.78125
train loss:  0.45739084482192993
train gradient:  0.10623122106724057
iteration : 2655
train acc:  0.7890625
train loss:  0.44303715229034424
train gradient:  0.08965136905229402
iteration : 2656
train acc:  0.78125
train loss:  0.46600985527038574
train gradient:  0.1124114038733007
iteration : 2657
train acc:  0.6953125
train loss:  0.5160269141197205
train gradient:  0.13914289149204673
iteration : 2658
train acc:  0.8125
train loss:  0.3754245340824127
train gradient:  0.07873484438185448
iteration : 2659
train acc:  0.796875
train loss:  0.429507315158844
train gradient:  0.09557948429297741
iteration : 2660
train acc:  0.7578125
train loss:  0.4546170234680176
train gradient:  0.10058185444451746
iteration : 2661
train acc:  0.734375
train loss:  0.48814016580581665
train gradient:  0.12119929501117832
iteration : 2662
train acc:  0.7109375
train loss:  0.4576084017753601
train gradient:  0.12222368755923918
iteration : 2663
train acc:  0.78125
train loss:  0.47630569338798523
train gradient:  0.11814509535910903
iteration : 2664
train acc:  0.75
train loss:  0.4753984212875366
train gradient:  0.12047108224658155
iteration : 2665
train acc:  0.7265625
train loss:  0.5020425915718079
train gradient:  0.12021449016273326
iteration : 2666
train acc:  0.75
train loss:  0.4496750831604004
train gradient:  0.11129692822570997
iteration : 2667
train acc:  0.78125
train loss:  0.4646565914154053
train gradient:  0.09613151766001005
iteration : 2668
train acc:  0.71875
train loss:  0.4717114269733429
train gradient:  0.11663417530960285
iteration : 2669
train acc:  0.7421875
train loss:  0.4815537929534912
train gradient:  0.09806248949470274
iteration : 2670
train acc:  0.734375
train loss:  0.5424825549125671
train gradient:  0.17407157322934008
iteration : 2671
train acc:  0.7109375
train loss:  0.5343369245529175
train gradient:  0.12515326884931693
iteration : 2672
train acc:  0.796875
train loss:  0.4161139130592346
train gradient:  0.08421856945931433
iteration : 2673
train acc:  0.8125
train loss:  0.4920788109302521
train gradient:  0.1302255629154541
iteration : 2674
train acc:  0.71875
train loss:  0.5441566109657288
train gradient:  0.20168392349738673
iteration : 2675
train acc:  0.703125
train loss:  0.5126644372940063
train gradient:  0.1697832932829063
iteration : 2676
train acc:  0.75
train loss:  0.46010255813598633
train gradient:  0.1176553262503954
iteration : 2677
train acc:  0.828125
train loss:  0.38630932569503784
train gradient:  0.08120187721400525
iteration : 2678
train acc:  0.765625
train loss:  0.4492340683937073
train gradient:  0.09422657057761298
iteration : 2679
train acc:  0.6484375
train loss:  0.5888753533363342
train gradient:  0.18437519806990832
iteration : 2680
train acc:  0.8203125
train loss:  0.4505748450756073
train gradient:  0.10102761041268983
iteration : 2681
train acc:  0.7265625
train loss:  0.4533814787864685
train gradient:  0.12952287537744145
iteration : 2682
train acc:  0.734375
train loss:  0.48610803484916687
train gradient:  0.10204553148184038
iteration : 2683
train acc:  0.7265625
train loss:  0.5229151248931885
train gradient:  0.1471041394575468
iteration : 2684
train acc:  0.78125
train loss:  0.44070905447006226
train gradient:  0.10094724937397852
iteration : 2685
train acc:  0.765625
train loss:  0.45837709307670593
train gradient:  0.10735603016654438
iteration : 2686
train acc:  0.7109375
train loss:  0.5227219462394714
train gradient:  0.13930424045338546
iteration : 2687
train acc:  0.7421875
train loss:  0.49641382694244385
train gradient:  0.10882345885238671
iteration : 2688
train acc:  0.7578125
train loss:  0.4750146269798279
train gradient:  0.14193862389434225
iteration : 2689
train acc:  0.734375
train loss:  0.47752124071121216
train gradient:  0.10145708333067731
iteration : 2690
train acc:  0.828125
train loss:  0.39049577713012695
train gradient:  0.07553081983595504
iteration : 2691
train acc:  0.7421875
train loss:  0.43523406982421875
train gradient:  0.08917000833416555
iteration : 2692
train acc:  0.75
train loss:  0.45915186405181885
train gradient:  0.09806285378182752
iteration : 2693
train acc:  0.796875
train loss:  0.40524542331695557
train gradient:  0.07327171960370576
iteration : 2694
train acc:  0.703125
train loss:  0.5366125106811523
train gradient:  0.14007933721850208
iteration : 2695
train acc:  0.7578125
train loss:  0.4704468250274658
train gradient:  0.08569411677066784
iteration : 2696
train acc:  0.703125
train loss:  0.47601383924484253
train gradient:  0.10871201691206096
iteration : 2697
train acc:  0.7578125
train loss:  0.5072801113128662
train gradient:  0.12249642505180076
iteration : 2698
train acc:  0.7421875
train loss:  0.4989459216594696
train gradient:  0.11488691782499513
iteration : 2699
train acc:  0.828125
train loss:  0.41564008593559265
train gradient:  0.08706204421405757
iteration : 2700
train acc:  0.7734375
train loss:  0.4647914469242096
train gradient:  0.09706126161916635
iteration : 2701
train acc:  0.734375
train loss:  0.5291910171508789
train gradient:  0.15143137121229178
iteration : 2702
train acc:  0.7890625
train loss:  0.3994148373603821
train gradient:  0.08433629715572173
iteration : 2703
train acc:  0.75
train loss:  0.5155935883522034
train gradient:  0.12413330606178719
iteration : 2704
train acc:  0.75
train loss:  0.4628349840641022
train gradient:  0.09348603652745793
iteration : 2705
train acc:  0.734375
train loss:  0.5203037858009338
train gradient:  0.15356766756862889
iteration : 2706
train acc:  0.7890625
train loss:  0.3951900601387024
train gradient:  0.06864473002329394
iteration : 2707
train acc:  0.7890625
train loss:  0.4495299160480499
train gradient:  0.10119388810453082
iteration : 2708
train acc:  0.7421875
train loss:  0.5081969499588013
train gradient:  0.12891421951485338
iteration : 2709
train acc:  0.7578125
train loss:  0.45013362169265747
train gradient:  0.08769041383106659
iteration : 2710
train acc:  0.7890625
train loss:  0.4345669448375702
train gradient:  0.09156415278825956
iteration : 2711
train acc:  0.828125
train loss:  0.35926270484924316
train gradient:  0.07193761502827317
iteration : 2712
train acc:  0.71875
train loss:  0.49115535616874695
train gradient:  0.13834717995902732
iteration : 2713
train acc:  0.75
train loss:  0.4768896698951721
train gradient:  0.11300362320469395
iteration : 2714
train acc:  0.7734375
train loss:  0.4258825182914734
train gradient:  0.08623764336254827
iteration : 2715
train acc:  0.7890625
train loss:  0.432070791721344
train gradient:  0.08677236267047114
iteration : 2716
train acc:  0.75
train loss:  0.4700069725513458
train gradient:  0.11249026560342543
iteration : 2717
train acc:  0.6875
train loss:  0.5060485601425171
train gradient:  0.13975293577808454
iteration : 2718
train acc:  0.765625
train loss:  0.43366459012031555
train gradient:  0.08989671026160514
iteration : 2719
train acc:  0.6953125
train loss:  0.49798768758773804
train gradient:  0.11168195003987674
iteration : 2720
train acc:  0.7890625
train loss:  0.42970043420791626
train gradient:  0.12493757914734833
iteration : 2721
train acc:  0.6796875
train loss:  0.5484766364097595
train gradient:  0.1634960515204434
iteration : 2722
train acc:  0.703125
train loss:  0.5288503170013428
train gradient:  0.128303493301284
iteration : 2723
train acc:  0.7890625
train loss:  0.45788705348968506
train gradient:  0.10775381699170518
iteration : 2724
train acc:  0.765625
train loss:  0.4666842222213745
train gradient:  0.13397087836048754
iteration : 2725
train acc:  0.78125
train loss:  0.4128316044807434
train gradient:  0.09538846481625937
iteration : 2726
train acc:  0.7421875
train loss:  0.5090698003768921
train gradient:  0.11991854102628924
iteration : 2727
train acc:  0.765625
train loss:  0.4535892605781555
train gradient:  0.10454085171013593
iteration : 2728
train acc:  0.7890625
train loss:  0.4560664892196655
train gradient:  0.10368994504281114
iteration : 2729
train acc:  0.7421875
train loss:  0.48452746868133545
train gradient:  0.12385689906015208
iteration : 2730
train acc:  0.796875
train loss:  0.4172532260417938
train gradient:  0.08193997136402217
iteration : 2731
train acc:  0.7578125
train loss:  0.4552209973335266
train gradient:  0.10388166550352655
iteration : 2732
train acc:  0.7890625
train loss:  0.46613308787345886
train gradient:  0.09636393624101215
iteration : 2733
train acc:  0.6328125
train loss:  0.5708054304122925
train gradient:  0.16656779998217236
iteration : 2734
train acc:  0.78125
train loss:  0.5031903982162476
train gradient:  0.15026200646334736
iteration : 2735
train acc:  0.7734375
train loss:  0.47027894854545593
train gradient:  0.10151998479742576
iteration : 2736
train acc:  0.6796875
train loss:  0.5955357551574707
train gradient:  0.20709819794547277
iteration : 2737
train acc:  0.8125
train loss:  0.41969358921051025
train gradient:  0.11431971077670175
iteration : 2738
train acc:  0.7265625
train loss:  0.5117882490158081
train gradient:  0.16149226033765118
iteration : 2739
train acc:  0.765625
train loss:  0.47307389974594116
train gradient:  0.10377064740257273
iteration : 2740
train acc:  0.765625
train loss:  0.4739266633987427
train gradient:  0.1328970327132824
iteration : 2741
train acc:  0.7734375
train loss:  0.4542061686515808
train gradient:  0.13295956125354058
iteration : 2742
train acc:  0.796875
train loss:  0.416681170463562
train gradient:  0.10427756152721271
iteration : 2743
train acc:  0.7265625
train loss:  0.49990320205688477
train gradient:  0.19077134653527222
iteration : 2744
train acc:  0.75
train loss:  0.5038896799087524
train gradient:  0.11847036480797696
iteration : 2745
train acc:  0.7421875
train loss:  0.4721522033214569
train gradient:  0.11830179892030544
iteration : 2746
train acc:  0.796875
train loss:  0.45755359530448914
train gradient:  0.12366935378674737
iteration : 2747
train acc:  0.7890625
train loss:  0.43816879391670227
train gradient:  0.10172934082000651
iteration : 2748
train acc:  0.78125
train loss:  0.5037575364112854
train gradient:  0.17568888097947388
iteration : 2749
train acc:  0.75
train loss:  0.47121623158454895
train gradient:  0.11970538853888572
iteration : 2750
train acc:  0.8046875
train loss:  0.4187808632850647
train gradient:  0.10588728234445964
iteration : 2751
train acc:  0.75
train loss:  0.47233107686042786
train gradient:  0.10561543272090186
iteration : 2752
train acc:  0.765625
train loss:  0.44738584756851196
train gradient:  0.11599796216303176
iteration : 2753
train acc:  0.7890625
train loss:  0.4305739402770996
train gradient:  0.09219703612254142
iteration : 2754
train acc:  0.8046875
train loss:  0.4702939987182617
train gradient:  0.1151815745538352
iteration : 2755
train acc:  0.7734375
train loss:  0.4981003701686859
train gradient:  0.1046859841416837
iteration : 2756
train acc:  0.8125
train loss:  0.4448266625404358
train gradient:  0.08969438585602414
iteration : 2757
train acc:  0.75
train loss:  0.48823225498199463
train gradient:  0.09598494280821487
iteration : 2758
train acc:  0.796875
train loss:  0.4332444667816162
train gradient:  0.09339539642175014
iteration : 2759
train acc:  0.734375
train loss:  0.4763077199459076
train gradient:  0.1187119191722041
iteration : 2760
train acc:  0.7890625
train loss:  0.4825737774372101
train gradient:  0.11780790372137341
iteration : 2761
train acc:  0.8203125
train loss:  0.411082923412323
train gradient:  0.08188935841656218
iteration : 2762
train acc:  0.78125
train loss:  0.41402891278266907
train gradient:  0.07888621511305353
iteration : 2763
train acc:  0.7421875
train loss:  0.4559704065322876
train gradient:  0.10503651399074095
iteration : 2764
train acc:  0.7578125
train loss:  0.4413684010505676
train gradient:  0.12132403213350745
iteration : 2765
train acc:  0.7265625
train loss:  0.49634143710136414
train gradient:  0.11921484629320565
iteration : 2766
train acc:  0.75
train loss:  0.4721866250038147
train gradient:  0.12787384448734632
iteration : 2767
train acc:  0.75
train loss:  0.4905625581741333
train gradient:  0.14936516440531894
iteration : 2768
train acc:  0.765625
train loss:  0.4902240037918091
train gradient:  0.12608211810079767
iteration : 2769
train acc:  0.765625
train loss:  0.4534362852573395
train gradient:  0.11181959249834203
iteration : 2770
train acc:  0.6640625
train loss:  0.5506471991539001
train gradient:  0.1424145292512083
iteration : 2771
train acc:  0.765625
train loss:  0.4372420012950897
train gradient:  0.11536354798643918
iteration : 2772
train acc:  0.7734375
train loss:  0.4127476215362549
train gradient:  0.11582912910778992
iteration : 2773
train acc:  0.7890625
train loss:  0.4315977096557617
train gradient:  0.10364697990052077
iteration : 2774
train acc:  0.75
train loss:  0.5032956004142761
train gradient:  0.11721321219289785
iteration : 2775
train acc:  0.7734375
train loss:  0.45435380935668945
train gradient:  0.11638369176957604
iteration : 2776
train acc:  0.6953125
train loss:  0.4540228843688965
train gradient:  0.10664309722312153
iteration : 2777
train acc:  0.6953125
train loss:  0.5284518599510193
train gradient:  0.13602985664951006
iteration : 2778
train acc:  0.796875
train loss:  0.41982775926589966
train gradient:  0.1055882421639676
iteration : 2779
train acc:  0.75
train loss:  0.424548864364624
train gradient:  0.08586621352983173
iteration : 2780
train acc:  0.765625
train loss:  0.4161449372768402
train gradient:  0.08896988846544852
iteration : 2781
train acc:  0.734375
train loss:  0.4627058506011963
train gradient:  0.10964352901852584
iteration : 2782
train acc:  0.7890625
train loss:  0.4307329058647156
train gradient:  0.09912854869584647
iteration : 2783
train acc:  0.7734375
train loss:  0.4379383325576782
train gradient:  0.0933464541409918
iteration : 2784
train acc:  0.6953125
train loss:  0.5513659119606018
train gradient:  0.18019810855162027
iteration : 2785
train acc:  0.734375
train loss:  0.45837637782096863
train gradient:  0.11112965921397115
iteration : 2786
train acc:  0.75
train loss:  0.4507483243942261
train gradient:  0.10300841839933375
iteration : 2787
train acc:  0.71875
train loss:  0.47820764780044556
train gradient:  0.1138711629266851
iteration : 2788
train acc:  0.7734375
train loss:  0.43381381034851074
train gradient:  0.0886680766937832
iteration : 2789
train acc:  0.8046875
train loss:  0.4159911870956421
train gradient:  0.09836151329399512
iteration : 2790
train acc:  0.8046875
train loss:  0.4406360387802124
train gradient:  0.14169202991020702
iteration : 2791
train acc:  0.8046875
train loss:  0.3909657597541809
train gradient:  0.09047755003332934
iteration : 2792
train acc:  0.8125
train loss:  0.43037864565849304
train gradient:  0.09779240199720557
iteration : 2793
train acc:  0.765625
train loss:  0.5406378507614136
train gradient:  0.15880503004069446
iteration : 2794
train acc:  0.8046875
train loss:  0.4301624000072479
train gradient:  0.0919104968378054
iteration : 2795
train acc:  0.671875
train loss:  0.5402845144271851
train gradient:  0.15498266605783897
iteration : 2796
train acc:  0.78125
train loss:  0.4092922806739807
train gradient:  0.0955864925546327
iteration : 2797
train acc:  0.71875
train loss:  0.5367193222045898
train gradient:  0.15391793710745422
iteration : 2798
train acc:  0.7265625
train loss:  0.5033890604972839
train gradient:  0.13450121671057386
iteration : 2799
train acc:  0.75
train loss:  0.5060935616493225
train gradient:  0.13293618240771282
iteration : 2800
train acc:  0.75
train loss:  0.46981996297836304
train gradient:  0.10472842645075647
iteration : 2801
train acc:  0.6875
train loss:  0.532149076461792
train gradient:  0.13823801356026047
iteration : 2802
train acc:  0.7421875
train loss:  0.461513489484787
train gradient:  0.10734550335092322
iteration : 2803
train acc:  0.6875
train loss:  0.49210602045059204
train gradient:  0.13501282978362744
iteration : 2804
train acc:  0.78125
train loss:  0.4365278482437134
train gradient:  0.10604897586374122
iteration : 2805
train acc:  0.765625
train loss:  0.44173455238342285
train gradient:  0.11662040393934354
iteration : 2806
train acc:  0.8046875
train loss:  0.4103138744831085
train gradient:  0.10565894889065852
iteration : 2807
train acc:  0.734375
train loss:  0.5581046342849731
train gradient:  0.1626706360025193
iteration : 2808
train acc:  0.7265625
train loss:  0.49159353971481323
train gradient:  0.1453892679193886
iteration : 2809
train acc:  0.734375
train loss:  0.5185325741767883
train gradient:  0.14028967052431027
iteration : 2810
train acc:  0.734375
train loss:  0.5128847360610962
train gradient:  0.12120354680987414
iteration : 2811
train acc:  0.7109375
train loss:  0.517933189868927
train gradient:  0.1343776249019621
iteration : 2812
train acc:  0.828125
train loss:  0.41134113073349
train gradient:  0.11571600736465422
iteration : 2813
train acc:  0.7421875
train loss:  0.4774455428123474
train gradient:  0.09828117305099492
iteration : 2814
train acc:  0.7421875
train loss:  0.47249913215637207
train gradient:  0.10988469404478529
iteration : 2815
train acc:  0.671875
train loss:  0.5898904800415039
train gradient:  0.21838932837005776
iteration : 2816
train acc:  0.796875
train loss:  0.4780547022819519
train gradient:  0.12424413489012458
iteration : 2817
train acc:  0.8046875
train loss:  0.4737955331802368
train gradient:  0.15245266221264908
iteration : 2818
train acc:  0.75
train loss:  0.4804234802722931
train gradient:  0.14029783836721194
iteration : 2819
train acc:  0.765625
train loss:  0.45451533794403076
train gradient:  0.10332573514701957
iteration : 2820
train acc:  0.6953125
train loss:  0.5076013803482056
train gradient:  0.15727760832286883
iteration : 2821
train acc:  0.7890625
train loss:  0.4408988058567047
train gradient:  0.10583457340984605
iteration : 2822
train acc:  0.7734375
train loss:  0.4983995854854584
train gradient:  0.14475873432734224
iteration : 2823
train acc:  0.71875
train loss:  0.4756203293800354
train gradient:  0.11097017807879489
iteration : 2824
train acc:  0.7421875
train loss:  0.5265786647796631
train gradient:  0.12186436126693308
iteration : 2825
train acc:  0.78125
train loss:  0.4790165424346924
train gradient:  0.1360921614871065
iteration : 2826
train acc:  0.7734375
train loss:  0.42368921637535095
train gradient:  0.09157309774382894
iteration : 2827
train acc:  0.71875
train loss:  0.46504831314086914
train gradient:  0.1009140477762877
iteration : 2828
train acc:  0.703125
train loss:  0.49159812927246094
train gradient:  0.11444001951214891
iteration : 2829
train acc:  0.703125
train loss:  0.5652023553848267
train gradient:  0.13519332316430688
iteration : 2830
train acc:  0.78125
train loss:  0.43218499422073364
train gradient:  0.09977666357821162
iteration : 2831
train acc:  0.734375
train loss:  0.5265140533447266
train gradient:  0.1913043968751275
iteration : 2832
train acc:  0.734375
train loss:  0.4678495526313782
train gradient:  0.09315554266471936
iteration : 2833
train acc:  0.7890625
train loss:  0.41895240545272827
train gradient:  0.10176777013701716
iteration : 2834
train acc:  0.6640625
train loss:  0.5387786626815796
train gradient:  0.13641405543514712
iteration : 2835
train acc:  0.78125
train loss:  0.422710657119751
train gradient:  0.08305694969257256
iteration : 2836
train acc:  0.6875
train loss:  0.5763145685195923
train gradient:  0.1440506903552034
iteration : 2837
train acc:  0.6875
train loss:  0.5595989227294922
train gradient:  0.14600583380814436
iteration : 2838
train acc:  0.78125
train loss:  0.4445594549179077
train gradient:  0.11502027337288676
iteration : 2839
train acc:  0.7421875
train loss:  0.47787564992904663
train gradient:  0.12478743893984137
iteration : 2840
train acc:  0.734375
train loss:  0.5620067119598389
train gradient:  0.18049626056324897
iteration : 2841
train acc:  0.75
train loss:  0.4722026288509369
train gradient:  0.10779972397545397
iteration : 2842
train acc:  0.7421875
train loss:  0.45851585268974304
train gradient:  0.1117477868451423
iteration : 2843
train acc:  0.8046875
train loss:  0.44207969307899475
train gradient:  0.09140957773194135
iteration : 2844
train acc:  0.7890625
train loss:  0.4340541958808899
train gradient:  0.09185771761273123
iteration : 2845
train acc:  0.78125
train loss:  0.4199104607105255
train gradient:  0.08081476368598182
iteration : 2846
train acc:  0.796875
train loss:  0.4785987436771393
train gradient:  0.10022993602616537
iteration : 2847
train acc:  0.7578125
train loss:  0.48630815744400024
train gradient:  0.13035363369233954
iteration : 2848
train acc:  0.734375
train loss:  0.4851299524307251
train gradient:  0.09428631250788828
iteration : 2849
train acc:  0.7578125
train loss:  0.5025737285614014
train gradient:  0.15167132973368863
iteration : 2850
train acc:  0.734375
train loss:  0.5262231826782227
train gradient:  0.1455974278013457
iteration : 2851
train acc:  0.7734375
train loss:  0.5182716250419617
train gradient:  0.14279299996875783
iteration : 2852
train acc:  0.8046875
train loss:  0.4815530478954315
train gradient:  0.11439801920745789
iteration : 2853
train acc:  0.7578125
train loss:  0.5053625702857971
train gradient:  0.12065372536595079
iteration : 2854
train acc:  0.875
train loss:  0.38200271129608154
train gradient:  0.08790810519013041
iteration : 2855
train acc:  0.7109375
train loss:  0.5443321466445923
train gradient:  0.15505368476781262
iteration : 2856
train acc:  0.65625
train loss:  0.5363173484802246
train gradient:  0.1298489147388131
iteration : 2857
train acc:  0.7578125
train loss:  0.47301021218299866
train gradient:  0.11620984432255126
iteration : 2858
train acc:  0.7734375
train loss:  0.41999268531799316
train gradient:  0.0867731747416813
iteration : 2859
train acc:  0.8125
train loss:  0.40904879570007324
train gradient:  0.09578898875212494
iteration : 2860
train acc:  0.734375
train loss:  0.49788907170295715
train gradient:  0.1401813073007679
iteration : 2861
train acc:  0.765625
train loss:  0.4707975387573242
train gradient:  0.09956861819435354
iteration : 2862
train acc:  0.71875
train loss:  0.4693577289581299
train gradient:  0.09217845887892973
iteration : 2863
train acc:  0.7421875
train loss:  0.5079125761985779
train gradient:  0.1245672555083247
iteration : 2864
train acc:  0.7578125
train loss:  0.4574141502380371
train gradient:  0.09903352705523702
iteration : 2865
train acc:  0.8125
train loss:  0.4027200937271118
train gradient:  0.07617126196236208
iteration : 2866
train acc:  0.6171875
train loss:  0.5972979068756104
train gradient:  0.178942176637988
iteration : 2867
train acc:  0.7890625
train loss:  0.3993343114852905
train gradient:  0.08964527057745521
iteration : 2868
train acc:  0.734375
train loss:  0.4412870407104492
train gradient:  0.08836541851839039
iteration : 2869
train acc:  0.7421875
train loss:  0.4463573694229126
train gradient:  0.10878247645999307
iteration : 2870
train acc:  0.734375
train loss:  0.5560064911842346
train gradient:  0.13881687325688774
iteration : 2871
train acc:  0.796875
train loss:  0.4382822811603546
train gradient:  0.09513945492456313
iteration : 2872
train acc:  0.7734375
train loss:  0.46830594539642334
train gradient:  0.10459154873162328
iteration : 2873
train acc:  0.7734375
train loss:  0.45294100046157837
train gradient:  0.09695051215429311
iteration : 2874
train acc:  0.75
train loss:  0.4796731472015381
train gradient:  0.12152208507705027
iteration : 2875
train acc:  0.75
train loss:  0.44546791911125183
train gradient:  0.1304912070459107
iteration : 2876
train acc:  0.7421875
train loss:  0.46417659521102905
train gradient:  0.10159713974686418
iteration : 2877
train acc:  0.734375
train loss:  0.4955100417137146
train gradient:  0.1537375910656819
iteration : 2878
train acc:  0.7734375
train loss:  0.46503686904907227
train gradient:  0.10904548963864592
iteration : 2879
train acc:  0.796875
train loss:  0.4106610119342804
train gradient:  0.10234692009205444
iteration : 2880
train acc:  0.75
train loss:  0.5236471891403198
train gradient:  0.11690469029768971
iteration : 2881
train acc:  0.75
train loss:  0.4860222637653351
train gradient:  0.11576892238446441
iteration : 2882
train acc:  0.7578125
train loss:  0.5687819719314575
train gradient:  0.1545544925274075
iteration : 2883
train acc:  0.8046875
train loss:  0.39533478021621704
train gradient:  0.07915721094278519
iteration : 2884
train acc:  0.78125
train loss:  0.42669743299484253
train gradient:  0.09806992152348531
iteration : 2885
train acc:  0.8046875
train loss:  0.4147610068321228
train gradient:  0.11374871266795034
iteration : 2886
train acc:  0.734375
train loss:  0.4898344874382019
train gradient:  0.12500627151916932
iteration : 2887
train acc:  0.796875
train loss:  0.4456568956375122
train gradient:  0.08821053975920357
iteration : 2888
train acc:  0.7421875
train loss:  0.49364304542541504
train gradient:  0.1376150517663799
iteration : 2889
train acc:  0.734375
train loss:  0.4980044662952423
train gradient:  0.15469511324190205
iteration : 2890
train acc:  0.6875
train loss:  0.5606106519699097
train gradient:  0.17990269929662356
iteration : 2891
train acc:  0.78125
train loss:  0.46162110567092896
train gradient:  0.11501517793893648
iteration : 2892
train acc:  0.703125
train loss:  0.5876364707946777
train gradient:  0.15689249494452923
iteration : 2893
train acc:  0.7421875
train loss:  0.4447513818740845
train gradient:  0.1188222475435729
iteration : 2894
train acc:  0.765625
train loss:  0.4939558506011963
train gradient:  0.1755486879266956
iteration : 2895
train acc:  0.6796875
train loss:  0.5379558801651001
train gradient:  0.15653814645267766
iteration : 2896
train acc:  0.7421875
train loss:  0.5156101584434509
train gradient:  0.12309189875715915
iteration : 2897
train acc:  0.7734375
train loss:  0.4371235966682434
train gradient:  0.08857076762235164
iteration : 2898
train acc:  0.6953125
train loss:  0.5153622627258301
train gradient:  0.13152188765994516
iteration : 2899
train acc:  0.7421875
train loss:  0.4980815052986145
train gradient:  0.13975934737920934
iteration : 2900
train acc:  0.7890625
train loss:  0.4592772424221039
train gradient:  0.09490883948359498
iteration : 2901
train acc:  0.7421875
train loss:  0.4587137699127197
train gradient:  0.10537781022538881
iteration : 2902
train acc:  0.8125
train loss:  0.46630966663360596
train gradient:  0.09820767450679073
iteration : 2903
train acc:  0.75
train loss:  0.5227893590927124
train gradient:  0.1764917212705984
iteration : 2904
train acc:  0.765625
train loss:  0.4925120770931244
train gradient:  0.12139050720047397
iteration : 2905
train acc:  0.6953125
train loss:  0.5293364524841309
train gradient:  0.1470192925508046
iteration : 2906
train acc:  0.75
train loss:  0.49533823132514954
train gradient:  0.16552830032340493
iteration : 2907
train acc:  0.78125
train loss:  0.4482906758785248
train gradient:  0.1006358084960164
iteration : 2908
train acc:  0.7734375
train loss:  0.43499496579170227
train gradient:  0.11291227117006254
iteration : 2909
train acc:  0.7265625
train loss:  0.46810874342918396
train gradient:  0.09938796480075968
iteration : 2910
train acc:  0.7578125
train loss:  0.4603796601295471
train gradient:  0.12623820837941824
iteration : 2911
train acc:  0.75
train loss:  0.5000461339950562
train gradient:  0.1294072819952769
iteration : 2912
train acc:  0.7890625
train loss:  0.4262884855270386
train gradient:  0.10551172864137413
iteration : 2913
train acc:  0.7265625
train loss:  0.5073146224021912
train gradient:  0.11849259593625974
iteration : 2914
train acc:  0.7890625
train loss:  0.4634712338447571
train gradient:  0.10326754852816275
iteration : 2915
train acc:  0.7265625
train loss:  0.541804850101471
train gradient:  0.13406181314519694
iteration : 2916
train acc:  0.7421875
train loss:  0.4974588453769684
train gradient:  0.12529996226746665
iteration : 2917
train acc:  0.765625
train loss:  0.5013622045516968
train gradient:  0.14213349115098645
iteration : 2918
train acc:  0.78125
train loss:  0.4576607048511505
train gradient:  0.0983533227587348
iteration : 2919
train acc:  0.8125
train loss:  0.39336922764778137
train gradient:  0.0880482500405993
iteration : 2920
train acc:  0.734375
train loss:  0.4923173487186432
train gradient:  0.12730452006189524
iteration : 2921
train acc:  0.78125
train loss:  0.4543217718601227
train gradient:  0.13432284179400764
iteration : 2922
train acc:  0.7421875
train loss:  0.5026181936264038
train gradient:  0.1408192158003561
iteration : 2923
train acc:  0.7109375
train loss:  0.4827408492565155
train gradient:  0.10718641284888025
iteration : 2924
train acc:  0.765625
train loss:  0.440692275762558
train gradient:  0.10136040204859251
iteration : 2925
train acc:  0.75
train loss:  0.47548654675483704
train gradient:  0.10349743993872436
iteration : 2926
train acc:  0.71875
train loss:  0.542786717414856
train gradient:  0.14406795160463615
iteration : 2927
train acc:  0.78125
train loss:  0.530983567237854
train gradient:  0.13053346958130563
iteration : 2928
train acc:  0.78125
train loss:  0.4617820978164673
train gradient:  0.11227745544302892
iteration : 2929
train acc:  0.7265625
train loss:  0.5119836926460266
train gradient:  0.16158577485652265
iteration : 2930
train acc:  0.8125
train loss:  0.4232524633407593
train gradient:  0.1010279982988624
iteration : 2931
train acc:  0.734375
train loss:  0.4494900703430176
train gradient:  0.10081541873030142
iteration : 2932
train acc:  0.765625
train loss:  0.4646693468093872
train gradient:  0.10317936048498212
iteration : 2933
train acc:  0.7890625
train loss:  0.42941543459892273
train gradient:  0.09132916849259005
iteration : 2934
train acc:  0.7421875
train loss:  0.463321328163147
train gradient:  0.10942733110052771
iteration : 2935
train acc:  0.6953125
train loss:  0.514116644859314
train gradient:  0.10825484699577578
iteration : 2936
train acc:  0.7109375
train loss:  0.47425541281700134
train gradient:  0.13640623221833156
iteration : 2937
train acc:  0.75
train loss:  0.5013223886489868
train gradient:  0.1452956658533172
iteration : 2938
train acc:  0.7578125
train loss:  0.43536803126335144
train gradient:  0.11663347603939317
iteration : 2939
train acc:  0.734375
train loss:  0.5043382048606873
train gradient:  0.10572637082382916
iteration : 2940
train acc:  0.703125
train loss:  0.5102350115776062
train gradient:  0.13208504704464286
iteration : 2941
train acc:  0.75
train loss:  0.4763689637184143
train gradient:  0.1283476374733485
iteration : 2942
train acc:  0.78125
train loss:  0.45823803544044495
train gradient:  0.1141133998889767
iteration : 2943
train acc:  0.78125
train loss:  0.4702011048793793
train gradient:  0.12837208670407635
iteration : 2944
train acc:  0.765625
train loss:  0.4579586684703827
train gradient:  0.12378587459744068
iteration : 2945
train acc:  0.671875
train loss:  0.4894463121891022
train gradient:  0.12048910013074877
iteration : 2946
train acc:  0.7421875
train loss:  0.5190222859382629
train gradient:  0.14151286026672022
iteration : 2947
train acc:  0.75
train loss:  0.48161035776138306
train gradient:  0.12619301621883486
iteration : 2948
train acc:  0.8125
train loss:  0.40692758560180664
train gradient:  0.10089944684041452
iteration : 2949
train acc:  0.7578125
train loss:  0.45379239320755005
train gradient:  0.12288320528940179
iteration : 2950
train acc:  0.7734375
train loss:  0.4977850019931793
train gradient:  0.10887781013092701
iteration : 2951
train acc:  0.75
train loss:  0.47504034638404846
train gradient:  0.1120925126827336
iteration : 2952
train acc:  0.7578125
train loss:  0.5151910185813904
train gradient:  0.16051760019463013
iteration : 2953
train acc:  0.71875
train loss:  0.4635029435157776
train gradient:  0.12048043149496163
iteration : 2954
train acc:  0.7109375
train loss:  0.5229958295822144
train gradient:  0.13011468111060512
iteration : 2955
train acc:  0.7578125
train loss:  0.5019323229789734
train gradient:  0.11360720983959448
iteration : 2956
train acc:  0.796875
train loss:  0.46696197986602783
train gradient:  0.10036352453865886
iteration : 2957
train acc:  0.7890625
train loss:  0.4508190155029297
train gradient:  0.12295039312517185
iteration : 2958
train acc:  0.75
train loss:  0.4895932078361511
train gradient:  0.14192482093329434
iteration : 2959
train acc:  0.7109375
train loss:  0.5717436671257019
train gradient:  0.17682269469239012
iteration : 2960
train acc:  0.796875
train loss:  0.4081075191497803
train gradient:  0.08734352876071506
iteration : 2961
train acc:  0.734375
train loss:  0.5214687585830688
train gradient:  0.12135392375929295
iteration : 2962
train acc:  0.71875
train loss:  0.5006841421127319
train gradient:  0.12376663346546793
iteration : 2963
train acc:  0.828125
train loss:  0.40207797288894653
train gradient:  0.09087947460989747
iteration : 2964
train acc:  0.75
train loss:  0.5029705762863159
train gradient:  0.15415125038151775
iteration : 2965
train acc:  0.7890625
train loss:  0.4397660195827484
train gradient:  0.09443344310410282
iteration : 2966
train acc:  0.7890625
train loss:  0.4480164349079132
train gradient:  0.10340395333687367
iteration : 2967
train acc:  0.7734375
train loss:  0.48327767848968506
train gradient:  0.12062335660798561
iteration : 2968
train acc:  0.765625
train loss:  0.42336219549179077
train gradient:  0.09510566924545542
iteration : 2969
train acc:  0.8203125
train loss:  0.3967020809650421
train gradie