program start:
num_rounds= 3
node_emb_dim= 16

----------------------------------------new_epoch--------------------------------------

epoch:  0
iteration : 0
train acc:  0.4296875
train loss:  0.7073176503181458
train gradient:  2.3418345454726213
iteration : 1
train acc:  0.578125
train loss:  0.6953296065330505
train gradient:  3.543648447778791
iteration : 2
train acc:  0.5390625
train loss:  0.6781425476074219
train gradient:  1.9337655147988715
iteration : 3
train acc:  0.484375
train loss:  0.6952757239341736
train gradient:  4.798134074984311
iteration : 4
train acc:  0.46875
train loss:  0.7332406640052795
train gradient:  3.9087714677686582
iteration : 5
train acc:  0.5390625
train loss:  0.6997723579406738
train gradient:  3.123360569227937
iteration : 6
train acc:  0.5390625
train loss:  0.6737858057022095
train gradient:  1.5681105725848705
iteration : 7
train acc:  0.5390625
train loss:  0.6913974285125732
train gradient:  1.6079257917910195
iteration : 8
train acc:  0.5234375
train loss:  0.6806470155715942
train gradient:  1.1852497820711507
iteration : 9
train acc:  0.625
train loss:  0.6782381534576416
train gradient:  1.3489745772927635
iteration : 10
train acc:  0.515625
train loss:  0.7198836803436279
train gradient:  1.846009313274124
iteration : 11
train acc:  0.546875
train loss:  0.6704204082489014
train gradient:  1.548016390342219
iteration : 12
train acc:  0.609375
train loss:  0.6717689037322998
train gradient:  1.466327380395034
iteration : 13
train acc:  0.5078125
train loss:  0.6972115635871887
train gradient:  1.1503326647086376
iteration : 14
train acc:  0.578125
train loss:  0.6496351957321167
train gradient:  0.7029700647010312
iteration : 15
train acc:  0.640625
train loss:  0.6549039483070374
train gradient:  0.8757459837260249
iteration : 16
train acc:  0.5546875
train loss:  0.7056335806846619
train gradient:  0.5229502242298707
iteration : 17
train acc:  0.5078125
train loss:  0.7174022197723389
train gradient:  0.6993436561664542
iteration : 18
train acc:  0.5625
train loss:  0.6654573678970337
train gradient:  0.6701864225932898
iteration : 19
train acc:  0.5546875
train loss:  0.7100818157196045
train gradient:  1.5782717063352913
iteration : 20
train acc:  0.546875
train loss:  0.6865530014038086
train gradient:  0.4937461048362925
iteration : 21
train acc:  0.5546875
train loss:  0.7267011404037476
train gradient:  1.6803334359957849
iteration : 22
train acc:  0.6171875
train loss:  0.6569875478744507
train gradient:  0.675399492663181
iteration : 23
train acc:  0.59375
train loss:  0.6694784164428711
train gradient:  0.9009964550269981
iteration : 24
train acc:  0.59375
train loss:  0.695878803730011
train gradient:  1.6229125856164563
iteration : 25
train acc:  0.5390625
train loss:  0.7007236480712891
train gradient:  0.390978179592238
iteration : 26
train acc:  0.6171875
train loss:  0.6488243341445923
train gradient:  0.42305784137567354
iteration : 27
train acc:  0.6015625
train loss:  0.6376060247421265
train gradient:  0.28103132026751526
iteration : 28
train acc:  0.6015625
train loss:  0.679462730884552
train gradient:  1.5540700857536982
iteration : 29
train acc:  0.609375
train loss:  0.6910403966903687
train gradient:  0.8574132567253027
iteration : 30
train acc:  0.59375
train loss:  0.6893513202667236
train gradient:  0.5553429921425022
iteration : 31
train acc:  0.53125
train loss:  0.7499061822891235
train gradient:  1.6379306790863877
iteration : 32
train acc:  0.5859375
train loss:  0.6715120673179626
train gradient:  0.40772154486435536
iteration : 33
train acc:  0.5703125
train loss:  0.6736533641815186
train gradient:  0.505566575959991
iteration : 34
train acc:  0.5390625
train loss:  0.6992107033729553
train gradient:  0.8778376609562251
iteration : 35
train acc:  0.5859375
train loss:  0.6724303364753723
train gradient:  0.5471516124769138
iteration : 36
train acc:  0.578125
train loss:  0.6790408492088318
train gradient:  0.911864562945411
iteration : 37
train acc:  0.59375
train loss:  0.6595243215560913
train gradient:  0.3968852409200933
iteration : 38
train acc:  0.5546875
train loss:  0.7015610337257385
train gradient:  0.6360369456685817
iteration : 39
train acc:  0.65625
train loss:  0.6584521532058716
train gradient:  0.5360044550611952
iteration : 40
train acc:  0.6484375
train loss:  0.6552413702011108
train gradient:  0.5258634265702213
iteration : 41
train acc:  0.6328125
train loss:  0.6667661666870117
train gradient:  0.49182194612495655
iteration : 42
train acc:  0.6015625
train loss:  0.6476728320121765
train gradient:  0.3263889171182008
iteration : 43
train acc:  0.53125
train loss:  0.6952722668647766
train gradient:  0.4964880128788039
iteration : 44
train acc:  0.578125
train loss:  0.6565120220184326
train gradient:  0.34525380821105567
iteration : 45
train acc:  0.609375
train loss:  0.6625127792358398
train gradient:  0.3546186740267893
iteration : 46
train acc:  0.65625
train loss:  0.642182469367981
train gradient:  0.26173359168817223
iteration : 47
train acc:  0.6640625
train loss:  0.6456347107887268
train gradient:  0.2440898628900123
iteration : 48
train acc:  0.609375
train loss:  0.6588043570518494
train gradient:  0.4286738510788878
iteration : 49
train acc:  0.6484375
train loss:  0.6498369574546814
train gradient:  0.20546915044725672
iteration : 50
train acc:  0.546875
train loss:  0.6695267558097839
train gradient:  0.269650129688542
iteration : 51
train acc:  0.6171875
train loss:  0.6567419767379761
train gradient:  0.3719585613762362
iteration : 52
train acc:  0.5390625
train loss:  0.6963742971420288
train gradient:  0.5111466957340223
iteration : 53
train acc:  0.5234375
train loss:  0.7085269689559937
train gradient:  1.2077860891621717
iteration : 54
train acc:  0.625
train loss:  0.6640321612358093
train gradient:  0.5482571542381389
iteration : 55
train acc:  0.703125
train loss:  0.6551723480224609
train gradient:  0.7647913625957062
iteration : 56
train acc:  0.625
train loss:  0.6611495614051819
train gradient:  0.21166388369928263
iteration : 57
train acc:  0.5390625
train loss:  0.6845871210098267
train gradient:  0.5113993295604433
iteration : 58
train acc:  0.6015625
train loss:  0.6842156648635864
train gradient:  0.4425183403001349
iteration : 59
train acc:  0.5859375
train loss:  0.6530473232269287
train gradient:  0.36107676232579716
iteration : 60
train acc:  0.5625
train loss:  0.7048342227935791
train gradient:  0.5618211881324406
iteration : 61
train acc:  0.6640625
train loss:  0.6206074357032776
train gradient:  0.39514034812499826
iteration : 62
train acc:  0.640625
train loss:  0.6317715048789978
train gradient:  0.2781937911373837
iteration : 63
train acc:  0.640625
train loss:  0.649832546710968
train gradient:  0.3158015165737675
iteration : 64
train acc:  0.6328125
train loss:  0.6573164463043213
train gradient:  0.3001652453131335
iteration : 65
train acc:  0.6171875
train loss:  0.6569340825080872
train gradient:  0.1666151804054748
iteration : 66
train acc:  0.6484375
train loss:  0.6408390998840332
train gradient:  0.12184051027114483
iteration : 67
train acc:  0.5859375
train loss:  0.6925570964813232
train gradient:  0.5958987186863796
iteration : 68
train acc:  0.6328125
train loss:  0.6517218947410583
train gradient:  0.1859131418022753
iteration : 69
train acc:  0.59375
train loss:  0.6611011028289795
train gradient:  0.25187650092407504
iteration : 70
train acc:  0.59375
train loss:  0.6689927577972412
train gradient:  0.4526883372769192
iteration : 71
train acc:  0.6484375
train loss:  0.6585959792137146
train gradient:  0.7500998061084759
iteration : 72
train acc:  0.609375
train loss:  0.6505947113037109
train gradient:  0.25388513375778343
iteration : 73
train acc:  0.6015625
train loss:  0.6714300513267517
train gradient:  0.49665918057634545
iteration : 74
train acc:  0.6484375
train loss:  0.6544684171676636
train gradient:  0.3004434866467589
iteration : 75
train acc:  0.59375
train loss:  0.6480230689048767
train gradient:  0.22883913729912597
iteration : 76
train acc:  0.546875
train loss:  0.7130787968635559
train gradient:  0.6088618666604011
iteration : 77
train acc:  0.6328125
train loss:  0.6416810750961304
train gradient:  0.4475859105141032
iteration : 78
train acc:  0.6484375
train loss:  0.6370012164115906
train gradient:  0.33613833501811474
iteration : 79
train acc:  0.6484375
train loss:  0.6325162053108215
train gradient:  0.668772324838332
iteration : 80
train acc:  0.6171875
train loss:  0.6518697738647461
train gradient:  0.4339429428313577
iteration : 81
train acc:  0.578125
train loss:  0.6661484241485596
train gradient:  0.459198781848703
iteration : 82
train acc:  0.640625
train loss:  0.6425701975822449
train gradient:  0.9041724605906638
iteration : 83
train acc:  0.640625
train loss:  0.6599104404449463
train gradient:  0.6932965331959919
iteration : 84
train acc:  0.6953125
train loss:  0.6031510829925537
train gradient:  0.32503950852565044
iteration : 85
train acc:  0.53125
train loss:  0.680621325969696
train gradient:  0.5493914937182454
iteration : 86
train acc:  0.59375
train loss:  0.6646729111671448
train gradient:  0.44322918884609364
iteration : 87
train acc:  0.546875
train loss:  0.6933362483978271
train gradient:  0.69630283369706
iteration : 88
train acc:  0.609375
train loss:  0.6627746820449829
train gradient:  0.6329598952944371
iteration : 89
train acc:  0.578125
train loss:  0.6899224519729614
train gradient:  0.6290163987038302
iteration : 90
train acc:  0.59375
train loss:  0.6771337985992432
train gradient:  0.5768123253757889
iteration : 91
train acc:  0.625
train loss:  0.6493571996688843
train gradient:  0.4944217218400685
iteration : 92
train acc:  0.6875
train loss:  0.6205899715423584
train gradient:  0.3494225895192654
iteration : 93
train acc:  0.6484375
train loss:  0.6631559133529663
train gradient:  0.4356586024172125
iteration : 94
train acc:  0.6015625
train loss:  0.6372614502906799
train gradient:  0.274929908891838
iteration : 95
train acc:  0.6171875
train loss:  0.6228810548782349
train gradient:  0.3135181298428609
iteration : 96
train acc:  0.5390625
train loss:  0.6840307712554932
train gradient:  0.4243757663442177
iteration : 97
train acc:  0.6640625
train loss:  0.6297975182533264
train gradient:  0.4293967038930037
iteration : 98
train acc:  0.671875
train loss:  0.6132723093032837
train gradient:  0.3820109607699802
iteration : 99
train acc:  0.6640625
train loss:  0.6783527135848999
train gradient:  0.8326284232692718
iteration : 100
train acc:  0.609375
train loss:  0.6766082048416138
train gradient:  1.6993517903244317
iteration : 101
train acc:  0.5859375
train loss:  0.6672407388687134
train gradient:  0.79200255059759
iteration : 102
train acc:  0.6875
train loss:  0.602093517780304
train gradient:  0.4261345197303276
iteration : 103
train acc:  0.625
train loss:  0.6424396634101868
train gradient:  0.4618518103112082
iteration : 104
train acc:  0.6484375
train loss:  0.6351543664932251
train gradient:  0.4894694035828511
iteration : 105
train acc:  0.6328125
train loss:  0.6497459411621094
train gradient:  0.4775078901400509
iteration : 106
train acc:  0.6171875
train loss:  0.6287133693695068
train gradient:  0.47374454489750834
iteration : 107
train acc:  0.5546875
train loss:  0.7064851522445679
train gradient:  0.9754013636589034
iteration : 108
train acc:  0.6484375
train loss:  0.6188089847564697
train gradient:  0.4029780968326868
iteration : 109
train acc:  0.640625
train loss:  0.6327399015426636
train gradient:  0.2794669602534158
iteration : 110
train acc:  0.6015625
train loss:  0.6725461483001709
train gradient:  1.1536143282578535
iteration : 111
train acc:  0.6796875
train loss:  0.6137624979019165
train gradient:  0.6072799660560094
iteration : 112
train acc:  0.5703125
train loss:  0.6595305800437927
train gradient:  0.4519456897554017
iteration : 113
train acc:  0.6953125
train loss:  0.5941729545593262
train gradient:  0.5226267232859987
iteration : 114
train acc:  0.6640625
train loss:  0.6341232061386108
train gradient:  0.5751453381605525
iteration : 115
train acc:  0.671875
train loss:  0.6197082996368408
train gradient:  0.34376083769368687
iteration : 116
train acc:  0.6796875
train loss:  0.6284967660903931
train gradient:  0.5344847200408369
iteration : 117
train acc:  0.5859375
train loss:  0.6581171154975891
train gradient:  1.066266383238907
iteration : 118
train acc:  0.671875
train loss:  0.6087541580200195
train gradient:  0.4012060214398078
iteration : 119
train acc:  0.65625
train loss:  0.6181743144989014
train gradient:  0.5201108053987731
iteration : 120
train acc:  0.5625
train loss:  0.6788608431816101
train gradient:  0.45616126002365986
iteration : 121
train acc:  0.625
train loss:  0.6142979860305786
train gradient:  0.43514649254133037
iteration : 122
train acc:  0.65625
train loss:  0.6075208187103271
train gradient:  0.3117928201798941
iteration : 123
train acc:  0.71875
train loss:  0.5842095017433167
train gradient:  0.4975834859059066
iteration : 124
train acc:  0.7265625
train loss:  0.5735519528388977
train gradient:  0.41691254268103906
iteration : 125
train acc:  0.6328125
train loss:  0.6326322555541992
train gradient:  0.638420280921964
iteration : 126
train acc:  0.640625
train loss:  0.6305598616600037
train gradient:  0.44104080447801763
iteration : 127
train acc:  0.7265625
train loss:  0.5750342607498169
train gradient:  0.5174491446102343
iteration : 128
train acc:  0.6640625
train loss:  0.6034512519836426
train gradient:  0.355089538002832
iteration : 129
train acc:  0.7109375
train loss:  0.5801491737365723
train gradient:  0.5727895756809711
iteration : 130
train acc:  0.578125
train loss:  0.6609092950820923
train gradient:  1.2553271234656613
iteration : 131
train acc:  0.6484375
train loss:  0.6110193729400635
train gradient:  1.2879030489524486
iteration : 132
train acc:  0.671875
train loss:  0.627133846282959
train gradient:  0.5912051696082135
iteration : 133
train acc:  0.7109375
train loss:  0.6182246208190918
train gradient:  0.7426028352217979
iteration : 134
train acc:  0.6171875
train loss:  0.6784612536430359
train gradient:  1.0044149833085232
iteration : 135
train acc:  0.5859375
train loss:  0.6271563768386841
train gradient:  0.498899053629671
iteration : 136
train acc:  0.6015625
train loss:  0.6192872524261475
train gradient:  0.75623119428347
iteration : 137
train acc:  0.6953125
train loss:  0.6028976440429688
train gradient:  0.6677499062836656
iteration : 138
train acc:  0.6640625
train loss:  0.6305655837059021
train gradient:  0.7361422363451485
iteration : 139
train acc:  0.6484375
train loss:  0.6143985986709595
train gradient:  0.7029827769490914
iteration : 140
train acc:  0.59375
train loss:  0.6647684574127197
train gradient:  0.7153789660629529
iteration : 141
train acc:  0.59375
train loss:  0.6748878359794617
train gradient:  0.5659427782671895
iteration : 142
train acc:  0.5859375
train loss:  0.6493291854858398
train gradient:  0.5036907506983636
iteration : 143
train acc:  0.6484375
train loss:  0.6385732889175415
train gradient:  0.7647219374883234
iteration : 144
train acc:  0.6640625
train loss:  0.6127991676330566
train gradient:  0.5539156615954982
iteration : 145
train acc:  0.6484375
train loss:  0.6039563417434692
train gradient:  0.7569866610490754
iteration : 146
train acc:  0.671875
train loss:  0.5857731103897095
train gradient:  0.41645739285075367
iteration : 147
train acc:  0.71875
train loss:  0.6073375344276428
train gradient:  0.4452746185533314
iteration : 148
train acc:  0.6796875
train loss:  0.579508364200592
train gradient:  0.39906794961430087
iteration : 149
train acc:  0.578125
train loss:  0.6606094837188721
train gradient:  0.859960014284973
iteration : 150
train acc:  0.640625
train loss:  0.6064858436584473
train gradient:  0.8007515931046791
iteration : 151
train acc:  0.6484375
train loss:  0.5710715055465698
train gradient:  0.4556637132221628
iteration : 152
train acc:  0.59375
train loss:  0.6149735450744629
train gradient:  0.4698772410351405
iteration : 153
train acc:  0.625
train loss:  0.6218770742416382
train gradient:  0.47695961632676526
iteration : 154
train acc:  0.6171875
train loss:  0.622734546661377
train gradient:  0.46021310198565074
iteration : 155
train acc:  0.6015625
train loss:  0.6224826574325562
train gradient:  0.4461078936124652
iteration : 156
train acc:  0.6953125
train loss:  0.5942993760108948
train gradient:  0.40687443468468515
iteration : 157
train acc:  0.6171875
train loss:  0.6414752006530762
train gradient:  0.7123379245275853
iteration : 158
train acc:  0.6328125
train loss:  0.6021341681480408
train gradient:  0.6079283973123307
iteration : 159
train acc:  0.5859375
train loss:  0.6581681966781616
train gradient:  1.0400650061557568
iteration : 160
train acc:  0.609375
train loss:  0.6948530077934265
train gradient:  0.768202268158493
iteration : 161
train acc:  0.609375
train loss:  0.6293462514877319
train gradient:  0.5560498156986873
iteration : 162
train acc:  0.5546875
train loss:  0.6581333875656128
train gradient:  0.5241674721026616
iteration : 163
train acc:  0.640625
train loss:  0.5974591970443726
train gradient:  0.4216832134700539
iteration : 164
train acc:  0.7265625
train loss:  0.561599850654602
train gradient:  0.35521811492047356
iteration : 165
train acc:  0.5859375
train loss:  0.6345168352127075
train gradient:  0.785153614413591
iteration : 166
train acc:  0.671875
train loss:  0.595622181892395
train gradient:  0.4121498317456011
iteration : 167
train acc:  0.71875
train loss:  0.5646048784255981
train gradient:  0.4559706905242632
iteration : 168
train acc:  0.6328125
train loss:  0.6330462694168091
train gradient:  0.8099204266929073
iteration : 169
train acc:  0.6328125
train loss:  0.6365012526512146
train gradient:  0.2845269069979002
iteration : 170
train acc:  0.5703125
train loss:  0.664726972579956
train gradient:  0.5692083215296379
iteration : 171
train acc:  0.734375
train loss:  0.5608768463134766
train gradient:  0.3460416236096981
iteration : 172
train acc:  0.609375
train loss:  0.6143112182617188
train gradient:  0.324657864456713
iteration : 173
train acc:  0.6484375
train loss:  0.6008578538894653
train gradient:  0.82473344450331
iteration : 174
train acc:  0.6484375
train loss:  0.6435375213623047
train gradient:  0.4919318490081117
iteration : 175
train acc:  0.6015625
train loss:  0.651965320110321
train gradient:  0.3744889262345175
iteration : 176
train acc:  0.609375
train loss:  0.6269891262054443
train gradient:  0.3918861452315795
iteration : 177
train acc:  0.6796875
train loss:  0.5816442370414734
train gradient:  0.32613037445936766
iteration : 178
train acc:  0.6484375
train loss:  0.5987231135368347
train gradient:  0.41774697434403574
iteration : 179
train acc:  0.6640625
train loss:  0.5910347104072571
train gradient:  0.334743149582026
iteration : 180
train acc:  0.65625
train loss:  0.6032516360282898
train gradient:  0.16463721088000516
iteration : 181
train acc:  0.671875
train loss:  0.5947695970535278
train gradient:  0.28976049887891414
iteration : 182
train acc:  0.6796875
train loss:  0.5845505595207214
train gradient:  0.27831943239323464
iteration : 183
train acc:  0.6171875
train loss:  0.6215183734893799
train gradient:  0.2773225095481872
iteration : 184
train acc:  0.671875
train loss:  0.6010940074920654
train gradient:  0.3404383736614664
iteration : 185
train acc:  0.625
train loss:  0.6453864574432373
train gradient:  0.6135873020714353
iteration : 186
train acc:  0.65625
train loss:  0.6290583610534668
train gradient:  0.7032351445495213
iteration : 187
train acc:  0.671875
train loss:  0.6197220683097839
train gradient:  0.6588187870438327
iteration : 188
train acc:  0.640625
train loss:  0.6275845170021057
train gradient:  0.41890765500243704
iteration : 189
train acc:  0.6796875
train loss:  0.5903574824333191
train gradient:  0.6987043607172314
iteration : 190
train acc:  0.6796875
train loss:  0.5883822441101074
train gradient:  0.45596827322701006
iteration : 191
train acc:  0.578125
train loss:  0.6852339506149292
train gradient:  0.6473036651105395
iteration : 192
train acc:  0.671875
train loss:  0.5983812212944031
train gradient:  0.6056776941801298
iteration : 193
train acc:  0.6484375
train loss:  0.6366222500801086
train gradient:  0.5546433579499153
iteration : 194
train acc:  0.7265625
train loss:  0.5500296354293823
train gradient:  0.8930121884639061
iteration : 195
train acc:  0.703125
train loss:  0.574779212474823
train gradient:  1.2358381851333204
iteration : 196
train acc:  0.703125
train loss:  0.5945144295692444
train gradient:  0.557725721539115
iteration : 197
train acc:  0.609375
train loss:  0.6476907730102539
train gradient:  0.8354045499054819
iteration : 198
train acc:  0.6015625
train loss:  0.6636915802955627
train gradient:  0.8177101286835871
iteration : 199
train acc:  0.625
train loss:  0.6081486940383911
train gradient:  0.36802290344035693
iteration : 200
train acc:  0.640625
train loss:  0.6353296637535095
train gradient:  0.43505494660238353
iteration : 201
train acc:  0.703125
train loss:  0.584304928779602
train gradient:  0.5052744442288326
iteration : 202
train acc:  0.71875
train loss:  0.6200296878814697
train gradient:  0.5013714386745345
iteration : 203
train acc:  0.75
train loss:  0.5376536846160889
train gradient:  0.3320817457540637
iteration : 204
train acc:  0.6484375
train loss:  0.6348714828491211
train gradient:  0.8640335545838616
iteration : 205
train acc:  0.6875
train loss:  0.6266319155693054
train gradient:  0.5547912830257011
iteration : 206
train acc:  0.609375
train loss:  0.6364573836326599
train gradient:  0.4403884960729238
iteration : 207
train acc:  0.6953125
train loss:  0.6221919059753418
train gradient:  0.503471786242093
iteration : 208
train acc:  0.6171875
train loss:  0.6468051671981812
train gradient:  0.3168111824111054
iteration : 209
train acc:  0.703125
train loss:  0.5842920541763306
train gradient:  0.3090450942869298
iteration : 210
train acc:  0.65625
train loss:  0.6457318067550659
train gradient:  0.5830507337998254
iteration : 211
train acc:  0.6875
train loss:  0.591715395450592
train gradient:  0.45009980579482756
iteration : 212
train acc:  0.703125
train loss:  0.597312331199646
train gradient:  0.3291002767737176
iteration : 213
train acc:  0.7265625
train loss:  0.5670793056488037
train gradient:  0.41189402306971956
iteration : 214
train acc:  0.6875
train loss:  0.5823056101799011
train gradient:  0.3068508015905145
iteration : 215
train acc:  0.765625
train loss:  0.5384853482246399
train gradient:  0.3286722657622095
iteration : 216
train acc:  0.5546875
train loss:  0.656673789024353
train gradient:  0.6667748176728279
iteration : 217
train acc:  0.6328125
train loss:  0.621584415435791
train gradient:  0.43939575025379746
iteration : 218
train acc:  0.65625
train loss:  0.6233174800872803
train gradient:  0.6742458657313816
iteration : 219
train acc:  0.6015625
train loss:  0.6295126676559448
train gradient:  0.5355321268024107
iteration : 220
train acc:  0.671875
train loss:  0.5764692425727844
train gradient:  0.4578916059323474
iteration : 221
train acc:  0.6796875
train loss:  0.574120044708252
train gradient:  0.5762583282559658
iteration : 222
train acc:  0.7109375
train loss:  0.5789903402328491
train gradient:  0.5844453341250201
iteration : 223
train acc:  0.6796875
train loss:  0.5927819013595581
train gradient:  0.3520837006057905
iteration : 224
train acc:  0.7421875
train loss:  0.5402893424034119
train gradient:  0.341033755441113
iteration : 225
train acc:  0.6875
train loss:  0.5731593370437622
train gradient:  0.4367733601288223
iteration : 226
train acc:  0.703125
train loss:  0.6041755676269531
train gradient:  0.6271106836674705
iteration : 227
train acc:  0.6484375
train loss:  0.5961489081382751
train gradient:  0.35917040949368645
iteration : 228
train acc:  0.6640625
train loss:  0.5597862005233765
train gradient:  0.47406626813934744
iteration : 229
train acc:  0.578125
train loss:  0.6350648403167725
train gradient:  0.6394398760500984
iteration : 230
train acc:  0.6640625
train loss:  0.6031002998352051
train gradient:  0.5513353424070685
iteration : 231
train acc:  0.6875
train loss:  0.5879905223846436
train gradient:  0.37926638403822877
iteration : 232
train acc:  0.6640625
train loss:  0.5871243476867676
train gradient:  0.5288296620736657
iteration : 233
train acc:  0.6875
train loss:  0.5693686008453369
train gradient:  0.48384796089342763
iteration : 234
train acc:  0.6953125
train loss:  0.573338508605957
train gradient:  0.41733534163667285
iteration : 235
train acc:  0.609375
train loss:  0.6311194896697998
train gradient:  0.6389527619814028
iteration : 236
train acc:  0.6640625
train loss:  0.567838728427887
train gradient:  0.423587828630819
iteration : 237
train acc:  0.6875
train loss:  0.618590772151947
train gradient:  0.7030373126810807
iteration : 238
train acc:  0.6875
train loss:  0.6227741837501526
train gradient:  0.7400775539679986
iteration : 239
train acc:  0.6640625
train loss:  0.6079502105712891
train gradient:  1.0469516463489148
iteration : 240
train acc:  0.7109375
train loss:  0.5688651204109192
train gradient:  0.43952837250408217
iteration : 241
train acc:  0.75
train loss:  0.5013478398323059
train gradient:  0.34252369800845234
iteration : 242
train acc:  0.6171875
train loss:  0.6687800884246826
train gradient:  0.6941550436549325
iteration : 243
train acc:  0.7109375
train loss:  0.556764543056488
train gradient:  0.4299388989685249
iteration : 244
train acc:  0.671875
train loss:  0.6085249185562134
train gradient:  0.5846128032653205
iteration : 245
train acc:  0.6484375
train loss:  0.5895444750785828
train gradient:  0.4700903943679164
iteration : 246
train acc:  0.6796875
train loss:  0.6142491102218628
train gradient:  0.7450409285500942
iteration : 247
train acc:  0.671875
train loss:  0.5737172365188599
train gradient:  0.5737157560342987
iteration : 248
train acc:  0.6875
train loss:  0.5867685079574585
train gradient:  0.31946137386968493
iteration : 249
train acc:  0.703125
train loss:  0.5819482207298279
train gradient:  0.4060457241063585
iteration : 250
train acc:  0.640625
train loss:  0.6373025178909302
train gradient:  0.611393482239891
iteration : 251
train acc:  0.71875
train loss:  0.5303750038146973
train gradient:  0.6355759011357216
iteration : 252
train acc:  0.6953125
train loss:  0.5593318939208984
train gradient:  0.5213371815199641
iteration : 253
train acc:  0.6328125
train loss:  0.6277573704719543
train gradient:  0.3296500425198612
iteration : 254
train acc:  0.6875
train loss:  0.6076923608779907
train gradient:  0.47063006182842937
iteration : 255
train acc:  0.671875
train loss:  0.5810409784317017
train gradient:  0.5193752936499317
iteration : 256
train acc:  0.671875
train loss:  0.6149091720581055
train gradient:  0.4495970322605576
iteration : 257
train acc:  0.6953125
train loss:  0.5546652674674988
train gradient:  0.4312449524669058
iteration : 258
train acc:  0.6171875
train loss:  0.5914173126220703
train gradient:  0.4810651264255713
iteration : 259
train acc:  0.7421875
train loss:  0.5552127957344055
train gradient:  0.4938279937614409
iteration : 260
train acc:  0.7265625
train loss:  0.595928430557251
train gradient:  0.8547609484373448
iteration : 261
train acc:  0.6953125
train loss:  0.5330972671508789
train gradient:  0.2980404071325613
iteration : 262
train acc:  0.6640625
train loss:  0.6024919748306274
train gradient:  0.9926530872581834
iteration : 263
train acc:  0.6875
train loss:  0.5663893222808838
train gradient:  0.41688149827130355
iteration : 264
train acc:  0.7109375
train loss:  0.5862628817558289
train gradient:  0.501484109445267
iteration : 265
train acc:  0.71875
train loss:  0.5583686828613281
train gradient:  0.4107888952449909
iteration : 266
train acc:  0.640625
train loss:  0.5943328738212585
train gradient:  0.46750938859880015
iteration : 267
train acc:  0.6484375
train loss:  0.6150474548339844
train gradient:  0.8159632320222607
iteration : 268
train acc:  0.6484375
train loss:  0.6358860731124878
train gradient:  0.9543423132416209
iteration : 269
train acc:  0.71875
train loss:  0.5651840567588806
train gradient:  0.48599692766801184
iteration : 270
train acc:  0.6796875
train loss:  0.5459780097007751
train gradient:  0.42257516425945674
iteration : 271
train acc:  0.6953125
train loss:  0.5605264902114868
train gradient:  0.7024373913266739
iteration : 272
train acc:  0.7265625
train loss:  0.5570775270462036
train gradient:  0.6068614175685374
iteration : 273
train acc:  0.7578125
train loss:  0.5320048928260803
train gradient:  0.7781582288865223
iteration : 274
train acc:  0.7265625
train loss:  0.6061900854110718
train gradient:  0.78344140447365
iteration : 275
train acc:  0.703125
train loss:  0.5504257678985596
train gradient:  0.7030907589659706
iteration : 276
train acc:  0.7578125
train loss:  0.4995369613170624
train gradient:  0.5389726821501631
iteration : 277
train acc:  0.6640625
train loss:  0.570402204990387
train gradient:  0.3238961594665529
iteration : 278
train acc:  0.6484375
train loss:  0.5951896905899048
train gradient:  0.8888234757293697
iteration : 279
train acc:  0.625
train loss:  0.6157824397087097
train gradient:  0.78058560399875
iteration : 280
train acc:  0.703125
train loss:  0.594652533531189
train gradient:  0.8866713051930599
iteration : 281
train acc:  0.6171875
train loss:  0.6723453998565674
train gradient:  0.7097089252766942
iteration : 282
train acc:  0.5859375
train loss:  0.6648948192596436
train gradient:  0.7856553722220898
iteration : 283
train acc:  0.65625
train loss:  0.6302458643913269
train gradient:  0.6650741335407241
iteration : 284
train acc:  0.703125
train loss:  0.5638519525527954
train gradient:  0.5832466918903348
iteration : 285
train acc:  0.6875
train loss:  0.5781720876693726
train gradient:  0.5331521452747459
iteration : 286
train acc:  0.765625
train loss:  0.5131772756576538
train gradient:  0.374908654766686
iteration : 287
train acc:  0.625
train loss:  0.6292595863342285
train gradient:  0.4892246602672578
iteration : 288
train acc:  0.6953125
train loss:  0.56099534034729
train gradient:  0.3028542723136994
iteration : 289
train acc:  0.6796875
train loss:  0.5651295185089111
train gradient:  0.5722943961234177
iteration : 290
train acc:  0.6875
train loss:  0.5893006324768066
train gradient:  0.3634460015738927
iteration : 291
train acc:  0.6953125
train loss:  0.5960730314254761
train gradient:  1.0895619251389899
iteration : 292
train acc:  0.6953125
train loss:  0.5764952898025513
train gradient:  0.42693441572590807
iteration : 293
train acc:  0.71875
train loss:  0.5708388686180115
train gradient:  0.22685306659622495
iteration : 294
train acc:  0.6015625
train loss:  0.6387234926223755
train gradient:  0.6305084144742625
iteration : 295
train acc:  0.65625
train loss:  0.624542236328125
train gradient:  0.567628589323097
iteration : 296
train acc:  0.6796875
train loss:  0.5990431308746338
train gradient:  0.6663362678781165
iteration : 297
train acc:  0.6328125
train loss:  0.6591709852218628
train gradient:  0.6842403423537762
iteration : 298
train acc:  0.71875
train loss:  0.5871782302856445
train gradient:  0.32116194096514705
iteration : 299
train acc:  0.7109375
train loss:  0.5537213087081909
train gradient:  0.37723085344709745
iteration : 300
train acc:  0.8125
train loss:  0.5037596821784973
train gradient:  0.3425721296715336
iteration : 301
train acc:  0.7109375
train loss:  0.5898768901824951
train gradient:  0.6875088148822783
iteration : 302
train acc:  0.7109375
train loss:  0.5554634928703308
train gradient:  0.24491945288588157
iteration : 303
train acc:  0.6875
train loss:  0.5951153039932251
train gradient:  0.5417085770392995
iteration : 304
train acc:  0.6875
train loss:  0.6033289432525635
train gradient:  0.4228967284678568
iteration : 305
train acc:  0.6875
train loss:  0.5683450698852539
train gradient:  0.4527558108543984
iteration : 306
train acc:  0.671875
train loss:  0.5862863063812256
train gradient:  0.4043316058351595
iteration : 307
train acc:  0.7109375
train loss:  0.5964405536651611
train gradient:  0.7064350672046685
iteration : 308
train acc:  0.5625
train loss:  0.6656567454338074
train gradient:  0.5914625036249177
iteration : 309
train acc:  0.6953125
train loss:  0.5537058115005493
train gradient:  0.2965296998457699
iteration : 310
train acc:  0.6640625
train loss:  0.585171639919281
train gradient:  0.690916235281027
iteration : 311
train acc:  0.625
train loss:  0.6164788007736206
train gradient:  0.501462079921788
iteration : 312
train acc:  0.75
train loss:  0.5275403261184692
train gradient:  0.44162453802139345
iteration : 313
train acc:  0.703125
train loss:  0.5648947358131409
train gradient:  0.39025482830298225
iteration : 314
train acc:  0.7421875
train loss:  0.5227874517440796
train gradient:  0.3350099488653541
iteration : 315
train acc:  0.7265625
train loss:  0.5702658891677856
train gradient:  0.6438741810578202
iteration : 316
train acc:  0.6484375
train loss:  0.5970914363861084
train gradient:  0.37219974069756256
iteration : 317
train acc:  0.6953125
train loss:  0.5856150388717651
train gradient:  0.6185413097714902
iteration : 318
train acc:  0.6875
train loss:  0.5672628879547119
train gradient:  0.6006680442434111
iteration : 319
train acc:  0.640625
train loss:  0.5949106812477112
train gradient:  0.4137339909513132
iteration : 320
train acc:  0.671875
train loss:  0.5811277627944946
train gradient:  0.642010371308184
iteration : 321
train acc:  0.7109375
train loss:  0.5440813302993774
train gradient:  0.5903500509569204
iteration : 322
train acc:  0.6328125
train loss:  0.5984680652618408
train gradient:  0.5129549901581775
iteration : 323
train acc:  0.703125
train loss:  0.5761634111404419
train gradient:  0.6107814270103844
iteration : 324
train acc:  0.7421875
train loss:  0.5760205984115601
train gradient:  0.5205613968140241
iteration : 325
train acc:  0.7421875
train loss:  0.5306705236434937
train gradient:  0.501645543059513
iteration : 326
train acc:  0.6875
train loss:  0.6105672717094421
train gradient:  0.37426785399951545
iteration : 327
train acc:  0.6484375
train loss:  0.6302043199539185
train gradient:  0.5361220624639074
iteration : 328
train acc:  0.6875
train loss:  0.5705543756484985
train gradient:  0.5253585219041905
iteration : 329
train acc:  0.703125
train loss:  0.5405985713005066
train gradient:  0.5029756160244923
iteration : 330
train acc:  0.75
train loss:  0.5573755502700806
train gradient:  0.6463462729998056
iteration : 331
train acc:  0.625
train loss:  0.5663530826568604
train gradient:  0.6011498576523944
iteration : 332
train acc:  0.6640625
train loss:  0.5930719375610352
train gradient:  0.8990392336891134
iteration : 333
train acc:  0.703125
train loss:  0.5601269006729126
train gradient:  0.5266748331451948
iteration : 334
train acc:  0.609375
train loss:  0.6022570133209229
train gradient:  0.5993992906737993
iteration : 335
train acc:  0.65625
train loss:  0.6140132546424866
train gradient:  0.4745872713482743
iteration : 336
train acc:  0.6875
train loss:  0.6142908334732056
train gradient:  0.5720602527188982
iteration : 337
train acc:  0.78125
train loss:  0.5158117413520813
train gradient:  0.5364306289988985
iteration : 338
train acc:  0.6875
train loss:  0.5640733242034912
train gradient:  0.6763044064199463
iteration : 339
train acc:  0.703125
train loss:  0.5959377288818359
train gradient:  0.6043310306819376
iteration : 340
train acc:  0.6953125
train loss:  0.5425797700881958
train gradient:  0.4283093273728825
iteration : 341
train acc:  0.6640625
train loss:  0.5636003613471985
train gradient:  0.31472062083285984
iteration : 342
train acc:  0.609375
train loss:  0.6294815540313721
train gradient:  1.106204741970848
iteration : 343
train acc:  0.6640625
train loss:  0.5757583379745483
train gradient:  0.5965489349739796
iteration : 344
train acc:  0.640625
train loss:  0.6549237370491028
train gradient:  0.8431890914122029
iteration : 345
train acc:  0.6796875
train loss:  0.574113667011261
train gradient:  0.9627830846900933
iteration : 346
train acc:  0.7265625
train loss:  0.5307245254516602
train gradient:  0.45134848506037634
iteration : 347
train acc:  0.6640625
train loss:  0.5842778086662292
train gradient:  0.28947072577364297
iteration : 348
train acc:  0.7109375
train loss:  0.5672876834869385
train gradient:  0.4858308480865581
iteration : 349
train acc:  0.703125
train loss:  0.5870742797851562
train gradient:  0.49593608111197574
iteration : 350
train acc:  0.6953125
train loss:  0.5563197135925293
train gradient:  0.43204203622111503
iteration : 351
train acc:  0.625
train loss:  0.6305508017539978
train gradient:  0.5800886098241181
iteration : 352
train acc:  0.6796875
train loss:  0.6167891025543213
train gradient:  0.7036452599796352
iteration : 353
train acc:  0.6328125
train loss:  0.6121554970741272
train gradient:  0.48046729087851986
iteration : 354
train acc:  0.6875
train loss:  0.5523934364318848
train gradient:  0.5446370938014726
iteration : 355
train acc:  0.75
train loss:  0.5356982350349426
train gradient:  0.30602259273561916
iteration : 356
train acc:  0.671875
train loss:  0.5765738487243652
train gradient:  0.36953468260028965
iteration : 357
train acc:  0.671875
train loss:  0.5922369360923767
train gradient:  0.5583451836617932
iteration : 358
train acc:  0.6796875
train loss:  0.5890241861343384
train gradient:  0.5380553744133768
iteration : 359
train acc:  0.7265625
train loss:  0.5584516525268555
train gradient:  0.3903703232600476
iteration : 360
train acc:  0.671875
train loss:  0.6022129058837891
train gradient:  0.3682251694367155
iteration : 361
train acc:  0.7109375
train loss:  0.5482717752456665
train gradient:  0.6046693247174144
iteration : 362
train acc:  0.7578125
train loss:  0.491468608379364
train gradient:  0.33459879602482157
iteration : 363
train acc:  0.71875
train loss:  0.5628759860992432
train gradient:  0.47499472886558924
iteration : 364
train acc:  0.6484375
train loss:  0.5707628130912781
train gradient:  0.5222318365180286
iteration : 365
train acc:  0.78125
train loss:  0.4731801748275757
train gradient:  0.4090848802560163
iteration : 366
train acc:  0.734375
train loss:  0.5088762640953064
train gradient:  0.5213701816815197
iteration : 367
train acc:  0.71875
train loss:  0.5929715633392334
train gradient:  1.14089503218061
iteration : 368
train acc:  0.640625
train loss:  0.5959824323654175
train gradient:  0.42373110040266554
iteration : 369
train acc:  0.6640625
train loss:  0.5715821981430054
train gradient:  0.605821280108663
iteration : 370
train acc:  0.734375
train loss:  0.5178453922271729
train gradient:  0.6656530791893742
iteration : 371
train acc:  0.7265625
train loss:  0.5425023436546326
train gradient:  0.31205586032684407
iteration : 372
train acc:  0.6953125
train loss:  0.5837848782539368
train gradient:  0.6601031980021816
iteration : 373
train acc:  0.6875
train loss:  0.5640606880187988
train gradient:  0.45374683724093
iteration : 374
train acc:  0.6953125
train loss:  0.5665497779846191
train gradient:  0.6504538343350226
iteration : 375
train acc:  0.7265625
train loss:  0.5330308675765991
train gradient:  0.43512247904861806
iteration : 376
train acc:  0.734375
train loss:  0.5151503682136536
train gradient:  0.5105815498799291
iteration : 377
train acc:  0.7578125
train loss:  0.5437597036361694
train gradient:  0.573194222871569
iteration : 378
train acc:  0.625
train loss:  0.6332900524139404
train gradient:  0.8691960261583213
iteration : 379
train acc:  0.640625
train loss:  0.6059372425079346
train gradient:  0.7298261525128324
iteration : 380
train acc:  0.609375
train loss:  0.638532817363739
train gradient:  0.6870688968188747
iteration : 381
train acc:  0.640625
train loss:  0.5528160929679871
train gradient:  0.5146873124491127
iteration : 382
train acc:  0.703125
train loss:  0.5617690682411194
train gradient:  0.6245965019394867
iteration : 383
train acc:  0.7890625
train loss:  0.5134295225143433
train gradient:  0.3823554940681305
iteration : 384
train acc:  0.765625
train loss:  0.5079564452171326
train gradient:  0.5332371560498916
iteration : 385
train acc:  0.703125
train loss:  0.5144821405410767
train gradient:  0.33730760296398954
iteration : 386
train acc:  0.6875
train loss:  0.5575464367866516
train gradient:  0.5863343555667323
iteration : 387
train acc:  0.6796875
train loss:  0.6043864488601685
train gradient:  0.528155170357028
iteration : 388
train acc:  0.7265625
train loss:  0.5522743463516235
train gradient:  0.7844416146479163
iteration : 389
train acc:  0.7109375
train loss:  0.5324984192848206
train gradient:  0.45671502628914185
iteration : 390
train acc:  0.734375
train loss:  0.5552318096160889
train gradient:  0.5997991922530101
iteration : 391
train acc:  0.671875
train loss:  0.6197224855422974
train gradient:  0.5894455143080571
iteration : 392
train acc:  0.6796875
train loss:  0.5718935132026672
train gradient:  0.585786541954347
iteration : 393
train acc:  0.6640625
train loss:  0.6202701926231384
train gradient:  0.5612159836284913
iteration : 394
train acc:  0.65625
train loss:  0.558042585849762
train gradient:  0.39869325028908104
iteration : 395
train acc:  0.7421875
train loss:  0.5148033499717712
train gradient:  0.4513249318837386
iteration : 396
train acc:  0.640625
train loss:  0.5885584354400635
train gradient:  0.4193043025729167
iteration : 397
train acc:  0.625
train loss:  0.5979465246200562
train gradient:  0.49927248766874976
iteration : 398
train acc:  0.6484375
train loss:  0.5631153583526611
train gradient:  0.44852072460499726
iteration : 399
train acc:  0.6640625
train loss:  0.573357880115509
train gradient:  0.5240882757865306
iteration : 400
train acc:  0.75
train loss:  0.5264096260070801
train gradient:  0.4488747204053915
iteration : 401
train acc:  0.7265625
train loss:  0.5248674154281616
train gradient:  0.4359758811678751
iteration : 402
train acc:  0.75
train loss:  0.5319137573242188
train gradient:  0.8357381397907544
iteration : 403
train acc:  0.75
train loss:  0.51579749584198
train gradient:  0.464505836862654
iteration : 404
train acc:  0.65625
train loss:  0.5664970874786377
train gradient:  0.493043954425959
iteration : 405
train acc:  0.7109375
train loss:  0.5568914413452148
train gradient:  0.4816268614710374
iteration : 406
train acc:  0.671875
train loss:  0.6246064901351929
train gradient:  0.6453925357397918
iteration : 407
train acc:  0.7109375
train loss:  0.5295324921607971
train gradient:  0.45876543563231814
iteration : 408
train acc:  0.7421875
train loss:  0.5320139527320862
train gradient:  0.3841547052160287
iteration : 409
train acc:  0.6953125
train loss:  0.5332989692687988
train gradient:  0.38026413784622126
iteration : 410
train acc:  0.6953125
train loss:  0.5856753587722778
train gradient:  0.6926294808401104
iteration : 411
train acc:  0.703125
train loss:  0.5631064176559448
train gradient:  0.49370037795850824
iteration : 412
train acc:  0.671875
train loss:  0.5940549373626709
train gradient:  0.4582761633428335
iteration : 413
train acc:  0.7421875
train loss:  0.49663245677948
train gradient:  0.5288373134964737
iteration : 414
train acc:  0.703125
train loss:  0.5767831802368164
train gradient:  0.6309501668751916
iteration : 415
train acc:  0.7890625
train loss:  0.5137100219726562
train gradient:  0.4706632591969208
iteration : 416
train acc:  0.6875
train loss:  0.6069314479827881
train gradient:  0.5512959671639989
iteration : 417
train acc:  0.703125
train loss:  0.5465617775917053
train gradient:  0.5085006821941079
iteration : 418
train acc:  0.71875
train loss:  0.5922610759735107
train gradient:  0.5537130712645282
iteration : 419
train acc:  0.7109375
train loss:  0.568375289440155
train gradient:  0.7325320450432758
iteration : 420
train acc:  0.6875
train loss:  0.5393439531326294
train gradient:  0.4659537522482275
iteration : 421
train acc:  0.6875
train loss:  0.5345299243927002
train gradient:  0.3884918879225845
iteration : 422
train acc:  0.671875
train loss:  0.5632608532905579
train gradient:  0.47925923797125486
iteration : 423
train acc:  0.796875
train loss:  0.45318174362182617
train gradient:  0.36132422862581
iteration : 424
train acc:  0.71875
train loss:  0.5707806348800659
train gradient:  0.41761265602562037
iteration : 425
train acc:  0.7109375
train loss:  0.585761547088623
train gradient:  0.4590222390590236
iteration : 426
train acc:  0.734375
train loss:  0.5196495056152344
train gradient:  0.4624503114316495
iteration : 427
train acc:  0.734375
train loss:  0.5011152029037476
train gradient:  0.4776197164328648
iteration : 428
train acc:  0.765625
train loss:  0.5025374889373779
train gradient:  0.30129546302808724
iteration : 429
train acc:  0.7578125
train loss:  0.5259143114089966
train gradient:  0.329660034852989
iteration : 430
train acc:  0.6328125
train loss:  0.6296665072441101
train gradient:  0.5927585269173696
iteration : 431
train acc:  0.7265625
train loss:  0.5103603601455688
train gradient:  0.3939095555636436
iteration : 432
train acc:  0.796875
train loss:  0.49403494596481323
train gradient:  0.3964970298161523
iteration : 433
train acc:  0.7734375
train loss:  0.5097508430480957
train gradient:  0.5438408790610783
iteration : 434
train acc:  0.734375
train loss:  0.5137871503829956
train gradient:  0.3934494515501903
iteration : 435
train acc:  0.6875
train loss:  0.5507912635803223
train gradient:  0.3276702800976812
iteration : 436
train acc:  0.75
train loss:  0.5349006056785583
train gradient:  0.5585360996476224
iteration : 437
train acc:  0.734375
train loss:  0.5833711624145508
train gradient:  0.5726793626709614
iteration : 438
train acc:  0.734375
train loss:  0.5054981112480164
train gradient:  0.48338758098077156
iteration : 439
train acc:  0.78125
train loss:  0.4769213795661926
train gradient:  0.30223822656215527
iteration : 440
train acc:  0.765625
train loss:  0.501201868057251
train gradient:  0.5201147345642682
iteration : 441
train acc:  0.75
train loss:  0.49054017663002014
train gradient:  0.4666405379443297
iteration : 442
train acc:  0.71875
train loss:  0.5989758372306824
train gradient:  0.7202863635927428
iteration : 443
train acc:  0.78125
train loss:  0.4638654887676239
train gradient:  0.35779813279451883
iteration : 444
train acc:  0.796875
train loss:  0.5011412501335144
train gradient:  0.39933456021628844
iteration : 445
train acc:  0.78125
train loss:  0.525477945804596
train gradient:  0.6002944819718155
iteration : 446
train acc:  0.71875
train loss:  0.5383963584899902
train gradient:  0.6665875107021332
iteration : 447
train acc:  0.71875
train loss:  0.5585184693336487
train gradient:  0.5018166750680827
iteration : 448
train acc:  0.6953125
train loss:  0.5697277188301086
train gradient:  0.5749003761883034
iteration : 449
train acc:  0.7421875
train loss:  0.5013512372970581
train gradient:  0.42958878011524165
iteration : 450
train acc:  0.703125
train loss:  0.5628648996353149
train gradient:  0.5945037851904054
iteration : 451
train acc:  0.734375
train loss:  0.5733981728553772
train gradient:  0.8904149440623812
iteration : 452
train acc:  0.6328125
train loss:  0.6403937339782715
train gradient:  1.0560017275558546
iteration : 453
train acc:  0.796875
train loss:  0.4825555682182312
train gradient:  0.5469772764759709
iteration : 454
train acc:  0.6484375
train loss:  0.6230412721633911
train gradient:  0.8540826294521087
iteration : 455
train acc:  0.78125
train loss:  0.4933852255344391
train gradient:  0.5372548945580073
iteration : 456
train acc:  0.71875
train loss:  0.5857861638069153
train gradient:  0.5088707324911318
iteration : 457
train acc:  0.640625
train loss:  0.5846391320228577
train gradient:  1.0021686969517543
iteration : 458
train acc:  0.7109375
train loss:  0.5571662187576294
train gradient:  0.5121237848527715
iteration : 459
train acc:  0.75
train loss:  0.5078570246696472
train gradient:  0.6234943962291591
iteration : 460
train acc:  0.7890625
train loss:  0.48126348853111267
train gradient:  0.3454823318657245
iteration : 461
train acc:  0.71875
train loss:  0.5346660017967224
train gradient:  0.45174792092934135
iteration : 462
train acc:  0.75
train loss:  0.5192466974258423
train gradient:  0.4640658925423105
iteration : 463
train acc:  0.7109375
train loss:  0.5642489194869995
train gradient:  0.4494978942908294
iteration : 464
train acc:  0.671875
train loss:  0.5741559267044067
train gradient:  0.53734375188977
iteration : 465
train acc:  0.734375
train loss:  0.4785152077674866
train gradient:  0.40598860623881317
iteration : 466
train acc:  0.6875
train loss:  0.6212466955184937
train gradient:  0.7930573171794204
iteration : 467
train acc:  0.7421875
train loss:  0.5679301619529724
train gradient:  0.7372887326952774
iteration : 468
train acc:  0.7109375
train loss:  0.595002293586731
train gradient:  0.5753537077878975
iteration : 469
train acc:  0.7109375
train loss:  0.5400094985961914
train gradient:  0.5203352666129795
iteration : 470
train acc:  0.7734375
train loss:  0.5020822286605835
train gradient:  0.4624640819869809
iteration : 471
train acc:  0.71875
train loss:  0.5410128235816956
train gradient:  0.42603288251979166
iteration : 472
train acc:  0.734375
train loss:  0.5627940893173218
train gradient:  0.5227314169383235
iteration : 473
train acc:  0.65625
train loss:  0.614669680595398
train gradient:  0.44060643377832015
iteration : 474
train acc:  0.6640625
train loss:  0.6109066605567932
train gradient:  0.5444036427366871
iteration : 475
train acc:  0.703125
train loss:  0.5932392477989197
train gradient:  0.4663537020629217
iteration : 476
train acc:  0.7734375
train loss:  0.49421122670173645
train gradient:  0.33433527216032793
iteration : 477
train acc:  0.734375
train loss:  0.47985386848449707
train gradient:  0.34826679372504904
iteration : 478
train acc:  0.78125
train loss:  0.4940204322338104
train gradient:  0.3438166237892718
iteration : 479
train acc:  0.71875
train loss:  0.5958821177482605
train gradient:  0.48951128635472063
iteration : 480
train acc:  0.7421875
train loss:  0.5225818157196045
train gradient:  0.47381051308766625
iteration : 481
train acc:  0.640625
train loss:  0.5993224382400513
train gradient:  0.6272914791312754
iteration : 482
train acc:  0.71875
train loss:  0.5148751735687256
train gradient:  0.39418748611056026
iteration : 483
train acc:  0.78125
train loss:  0.4541955888271332
train gradient:  0.4180386907539616
iteration : 484
train acc:  0.8203125
train loss:  0.46875664591789246
train gradient:  0.29490987869822655
iteration : 485
train acc:  0.78125
train loss:  0.5343177318572998
train gradient:  0.4825567840203091
iteration : 486
train acc:  0.6875
train loss:  0.5575065612792969
train gradient:  0.4586956123935701
iteration : 487
train acc:  0.765625
train loss:  0.5182516574859619
train gradient:  0.4147032941929748
iteration : 488
train acc:  0.6953125
train loss:  0.5509427785873413
train gradient:  0.6038475679025365
iteration : 489
train acc:  0.84375
train loss:  0.42657750844955444
train gradient:  0.3693969974381589
iteration : 490
train acc:  0.71875
train loss:  0.46477797627449036
train gradient:  0.49414102374122565
iteration : 491
train acc:  0.6796875
train loss:  0.562996506690979
train gradient:  0.5876959717103343
iteration : 492
train acc:  0.7265625
train loss:  0.5539343953132629
train gradient:  0.5341060273414524
iteration : 493
train acc:  0.75
train loss:  0.5618036985397339
train gradient:  0.5920816703431144
iteration : 494
train acc:  0.6640625
train loss:  0.5955921411514282
train gradient:  0.578206210460445
iteration : 495
train acc:  0.734375
train loss:  0.5382199287414551
train gradient:  0.5570962259169946
iteration : 496
train acc:  0.7265625
train loss:  0.527612566947937
train gradient:  0.39162601068378294
iteration : 497
train acc:  0.7265625
train loss:  0.5006706714630127
train gradient:  0.45847439525971395
iteration : 498
train acc:  0.765625
train loss:  0.4987606406211853
train gradient:  0.33219992313347696
iteration : 499
train acc:  0.765625
train loss:  0.4806751310825348
train gradient:  0.534769994296525
iteration : 500
train acc:  0.8125
train loss:  0.48772886395454407
train gradient:  0.4148828950191811
iteration : 501
train acc:  0.7109375
train loss:  0.6104928255081177
train gradient:  0.6532117688877626
iteration : 502
train acc:  0.671875
train loss:  0.5775099992752075
train gradient:  0.5605481390327967
iteration : 503
train acc:  0.7421875
train loss:  0.4956551492214203
train gradient:  0.39875061940329887
iteration : 504
train acc:  0.71875
train loss:  0.5245450735092163
train gradient:  0.4019476916388453
iteration : 505
train acc:  0.7109375
train loss:  0.5471694469451904
train gradient:  0.5140269121759409
iteration : 506
train acc:  0.75
train loss:  0.5057022571563721
train gradient:  0.3822983606824481
iteration : 507
train acc:  0.7734375
train loss:  0.4695117473602295
train gradient:  0.24727859497736038
iteration : 508
train acc:  0.703125
train loss:  0.5457984805107117
train gradient:  0.6091512282132685
iteration : 509
train acc:  0.75
train loss:  0.5217356085777283
train gradient:  0.46402698443612195
iteration : 510
train acc:  0.71875
train loss:  0.5565021634101868
train gradient:  0.3285033822307417
iteration : 511
train acc:  0.734375
train loss:  0.5192602872848511
train gradient:  0.2836998766513847
iteration : 512
train acc:  0.7109375
train loss:  0.5954722166061401
train gradient:  0.6766935914539938
iteration : 513
train acc:  0.6953125
train loss:  0.5498607158660889
train gradient:  0.37946681112044556
iteration : 514
train acc:  0.765625
train loss:  0.4878446161746979
train gradient:  0.3614978896003061
iteration : 515
train acc:  0.7109375
train loss:  0.5358022451400757
train gradient:  0.49044462991840954
iteration : 516
train acc:  0.7421875
train loss:  0.5169970989227295
train gradient:  0.4311400909172734
iteration : 517
train acc:  0.734375
train loss:  0.5707747936248779
train gradient:  0.5807055255964847
iteration : 518
train acc:  0.765625
train loss:  0.5110170841217041
train gradient:  0.4325646698095349
iteration : 519
train acc:  0.7265625
train loss:  0.5632178783416748
train gradient:  0.7603889767126382
iteration : 520
train acc:  0.734375
train loss:  0.5433728098869324
train gradient:  0.38626449699460536
iteration : 521
train acc:  0.6796875
train loss:  0.575089693069458
train gradient:  0.637305693346881
iteration : 522
train acc:  0.796875
train loss:  0.4731070399284363
train gradient:  0.45205126010570573
iteration : 523
train acc:  0.640625
train loss:  0.5470483899116516
train gradient:  0.5071072240920256
iteration : 524
train acc:  0.765625
train loss:  0.5171900391578674
train gradient:  0.5928329634487859
iteration : 525
train acc:  0.734375
train loss:  0.5181401968002319
train gradient:  0.49998241595769877
iteration : 526
train acc:  0.734375
train loss:  0.5508770942687988
train gradient:  0.5556559001568248
iteration : 527
train acc:  0.6875
train loss:  0.5336253643035889
train gradient:  0.5941281238061253
iteration : 528
train acc:  0.7109375
train loss:  0.6165591478347778
train gradient:  0.6294545706621041
iteration : 529
train acc:  0.71875
train loss:  0.5541602373123169
train gradient:  0.7265198780506519
iteration : 530
train acc:  0.71875
train loss:  0.5263290405273438
train gradient:  0.5352508188294879
iteration : 531
train acc:  0.6953125
train loss:  0.5802048444747925
train gradient:  0.5942031724599439
iteration : 532
train acc:  0.796875
train loss:  0.4934253692626953
train gradient:  0.47787009206971676
iteration : 533
train acc:  0.7421875
train loss:  0.5241175889968872
train gradient:  0.7283669123922224
iteration : 534
train acc:  0.7265625
train loss:  0.5077328085899353
train gradient:  0.49531954791128174
iteration : 535
train acc:  0.7265625
train loss:  0.4982393980026245
train gradient:  0.44536425060529145
iteration : 536
train acc:  0.7578125
train loss:  0.5270565748214722
train gradient:  0.5161409225808891
iteration : 537
train acc:  0.71875
train loss:  0.5368505120277405
train gradient:  0.32093462575814696
iteration : 538
train acc:  0.671875
train loss:  0.5609740018844604
train gradient:  0.461798507243036
iteration : 539
train acc:  0.7734375
train loss:  0.5254199504852295
train gradient:  0.5553727456038664
iteration : 540
train acc:  0.8046875
train loss:  0.46909141540527344
train gradient:  0.5087788096719135
iteration : 541
train acc:  0.7734375
train loss:  0.5311998128890991
train gradient:  0.40444081717967867
iteration : 542
train acc:  0.734375
train loss:  0.540513277053833
train gradient:  0.5030300880149817
iteration : 543
train acc:  0.71875
train loss:  0.5199687480926514
train gradient:  0.4200702392075341
iteration : 544
train acc:  0.7578125
train loss:  0.5369173288345337
train gradient:  0.6397392576448069
iteration : 545
train acc:  0.6875
train loss:  0.601936399936676
train gradient:  0.49130179471887847
iteration : 546
train acc:  0.71875
train loss:  0.5641124248504639
train gradient:  0.5481057500611575
iteration : 547
train acc:  0.6640625
train loss:  0.5924502611160278
train gradient:  0.7227211746903974
iteration : 548
train acc:  0.6953125
train loss:  0.5684719085693359
train gradient:  0.43349167299057495
iteration : 549
train acc:  0.6875
train loss:  0.5880745649337769
train gradient:  0.6545784088663777
iteration : 550
train acc:  0.6953125
train loss:  0.5225790739059448
train gradient:  0.49973452206091334
iteration : 551
train acc:  0.8125
train loss:  0.43379271030426025
train gradient:  0.274525432151413
iteration : 552
train acc:  0.7734375
train loss:  0.4834173619747162
train gradient:  0.3996695941036258
iteration : 553
train acc:  0.671875
train loss:  0.552402913570404
train gradient:  0.43169119566342895
iteration : 554
train acc:  0.6953125
train loss:  0.5443186163902283
train gradient:  0.45716133442674844
iteration : 555
train acc:  0.71875
train loss:  0.5717822313308716
train gradient:  0.7396609022937505
iteration : 556
train acc:  0.828125
train loss:  0.45665067434310913
train gradient:  0.40780536430046344
iteration : 557
train acc:  0.75
train loss:  0.4696391522884369
train gradient:  0.4334560209256147
iteration : 558
train acc:  0.7890625
train loss:  0.47422999143600464
train gradient:  0.40076245133234395
iteration : 559
train acc:  0.75
train loss:  0.5062592029571533
train gradient:  0.4233180641556937
iteration : 560
train acc:  0.765625
train loss:  0.49439796805381775
train gradient:  0.5232814245649525
iteration : 561
train acc:  0.765625
train loss:  0.49074262380599976
train gradient:  0.5570863037711151
iteration : 562
train acc:  0.765625
train loss:  0.504447340965271
train gradient:  0.5697513421469452
iteration : 563
train acc:  0.7109375
train loss:  0.5219864249229431
train gradient:  0.8556098862390105
iteration : 564
train acc:  0.7734375
train loss:  0.5107398629188538
train gradient:  0.35634188891942165
iteration : 565
train acc:  0.75
train loss:  0.5493298768997192
train gradient:  0.5680590057845741
iteration : 566
train acc:  0.7421875
train loss:  0.5400025844573975
train gradient:  0.4649357995861934
iteration : 567
train acc:  0.734375
train loss:  0.5763552784919739
train gradient:  0.8893921117035182
iteration : 568
train acc:  0.765625
train loss:  0.491068571805954
train gradient:  0.452119766064433
iteration : 569
train acc:  0.765625
train loss:  0.5149146318435669
train gradient:  0.4890666705323341
iteration : 570
train acc:  0.75
train loss:  0.5153284072875977
train gradient:  0.5092001490910623
iteration : 571
train acc:  0.7265625
train loss:  0.5426837205886841
train gradient:  0.653542723211589
iteration : 572
train acc:  0.7734375
train loss:  0.4425475597381592
train gradient:  0.2946388422061979
iteration : 573
train acc:  0.7734375
train loss:  0.5103139281272888
train gradient:  0.48032359842162353
iteration : 574
train acc:  0.7109375
train loss:  0.5240870714187622
train gradient:  0.6023591774905049
iteration : 575
train acc:  0.75
train loss:  0.53336501121521
train gradient:  0.4222541832585415
iteration : 576
train acc:  0.671875
train loss:  0.6064002513885498
train gradient:  0.6459702389812123
iteration : 577
train acc:  0.75
train loss:  0.49385952949523926
train gradient:  0.6582074652312684
iteration : 578
train acc:  0.765625
train loss:  0.5664579272270203
train gradient:  0.5737516206696415
iteration : 579
train acc:  0.7109375
train loss:  0.5208109617233276
train gradient:  0.6150202617246567
iteration : 580
train acc:  0.6875
train loss:  0.5518975257873535
train gradient:  0.5710324242164045
iteration : 581
train acc:  0.734375
train loss:  0.5523016452789307
train gradient:  0.6683235994954414
iteration : 582
train acc:  0.78125
train loss:  0.47586750984191895
train gradient:  0.49209375985112297
iteration : 583
train acc:  0.71875
train loss:  0.5153622627258301
train gradient:  0.3147023205086197
iteration : 584
train acc:  0.7734375
train loss:  0.46118345856666565
train gradient:  0.4727203153889659
iteration : 585
train acc:  0.734375
train loss:  0.512537956237793
train gradient:  0.8880303442029125
iteration : 586
train acc:  0.6328125
train loss:  0.6399926543235779
train gradient:  0.8183110863465899
iteration : 587
train acc:  0.7578125
train loss:  0.4966278672218323
train gradient:  1.0373007659791027
iteration : 588
train acc:  0.765625
train loss:  0.5274451971054077
train gradient:  0.6894689453430873
iteration : 589
train acc:  0.609375
train loss:  0.6479750275611877
train gradient:  0.7438730198315485
iteration : 590
train acc:  0.7109375
train loss:  0.5504258871078491
train gradient:  0.5082727772710629
iteration : 591
train acc:  0.7578125
train loss:  0.49075615406036377
train gradient:  0.6312224799057747
iteration : 592
train acc:  0.7421875
train loss:  0.5149561166763306
train gradient:  0.4879182637169411
iteration : 593
train acc:  0.75
train loss:  0.51016765832901
train gradient:  0.638846255595143
iteration : 594
train acc:  0.71875
train loss:  0.5348676443099976
train gradient:  0.6633442637750588
iteration : 595
train acc:  0.7578125
train loss:  0.4658842980861664
train gradient:  0.38181326650349146
iteration : 596
train acc:  0.8125
train loss:  0.4561852812767029
train gradient:  0.3773711321310403
iteration : 597
train acc:  0.7421875
train loss:  0.5087718963623047
train gradient:  0.6191776004575916
iteration : 598
train acc:  0.734375
train loss:  0.548954963684082
train gradient:  0.4607600333059029
iteration : 599
train acc:  0.765625
train loss:  0.46327173709869385
train gradient:  0.39107613992095036
iteration : 600
train acc:  0.734375
train loss:  0.48764532804489136
train gradient:  0.39257661619794026
iteration : 601
train acc:  0.7578125
train loss:  0.4816270172595978
train gradient:  0.4778143086223893
iteration : 602
train acc:  0.703125
train loss:  0.5938493013381958
train gradient:  0.5498071023346748
iteration : 603
train acc:  0.734375
train loss:  0.5225168466567993
train gradient:  0.5716819285608217
iteration : 604
train acc:  0.6953125
train loss:  0.5989108085632324
train gradient:  0.5036318574869872
iteration : 605
train acc:  0.6796875
train loss:  0.5656665563583374
train gradient:  0.6618881441005248
iteration : 606
train acc:  0.765625
train loss:  0.5021276473999023
train gradient:  0.46803696679132306
iteration : 607
train acc:  0.8046875
train loss:  0.42792588472366333
train gradient:  0.3628493170501341
iteration : 608
train acc:  0.8046875
train loss:  0.46322011947631836
train gradient:  0.39561102492803973
iteration : 609
train acc:  0.796875
train loss:  0.4649907350540161
train gradient:  0.5352309563746399
iteration : 610
train acc:  0.6875
train loss:  0.5374598503112793
train gradient:  0.5327367196939772
iteration : 611
train acc:  0.796875
train loss:  0.46315157413482666
train gradient:  0.44912610382805107
iteration : 612
train acc:  0.75
train loss:  0.5056486129760742
train gradient:  0.5542913581645452
iteration : 613
train acc:  0.765625
train loss:  0.4860972762107849
train gradient:  0.5405700279677816
iteration : 614
train acc:  0.8203125
train loss:  0.42278409004211426
train gradient:  0.40698419588867707
iteration : 615
train acc:  0.765625
train loss:  0.45657289028167725
train gradient:  0.32955321986190556
iteration : 616
train acc:  0.7109375
train loss:  0.49761685729026794
train gradient:  0.4380198925509085
iteration : 617
train acc:  0.78125
train loss:  0.4842466711997986
train gradient:  0.468573640076
iteration : 618
train acc:  0.7734375
train loss:  0.48497140407562256
train gradient:  0.6283972202798527
iteration : 619
train acc:  0.7578125
train loss:  0.5177531838417053
train gradient:  0.5395256815639171
iteration : 620
train acc:  0.78125
train loss:  0.4991753101348877
train gradient:  0.3974079118690558
iteration : 621
train acc:  0.8046875
train loss:  0.4981480836868286
train gradient:  0.47914639674971843
iteration : 622
train acc:  0.765625
train loss:  0.5220357179641724
train gradient:  0.554532751882629
iteration : 623
train acc:  0.8046875
train loss:  0.4875115156173706
train gradient:  0.468542984509509
iteration : 624
train acc:  0.7109375
train loss:  0.4584202766418457
train gradient:  0.5521945950914475
iteration : 625
train acc:  0.765625
train loss:  0.4922463297843933
train gradient:  0.5709998031309873
iteration : 626
train acc:  0.7890625
train loss:  0.48513373732566833
train gradient:  0.44585565529150306
iteration : 627
train acc:  0.796875
train loss:  0.4188193678855896
train gradient:  0.34976424268660455
iteration : 628
train acc:  0.7109375
train loss:  0.5990557074546814
train gradient:  0.8411049356068452
iteration : 629
train acc:  0.7890625
train loss:  0.46615099906921387
train gradient:  0.5228460621966802
iteration : 630
train acc:  0.71875
train loss:  0.5677216053009033
train gradient:  0.7850996779768353
iteration : 631
train acc:  0.71875
train loss:  0.5483124852180481
train gradient:  0.4655139378337361
iteration : 632
train acc:  0.703125
train loss:  0.5457934141159058
train gradient:  0.6487695532191717
iteration : 633
train acc:  0.7734375
train loss:  0.46212390065193176
train gradient:  0.44193048834765375
iteration : 634
train acc:  0.765625
train loss:  0.5423358082771301
train gradient:  0.6239275974866977
iteration : 635
train acc:  0.765625
train loss:  0.48056289553642273
train gradient:  0.5496943767949984
iteration : 636
train acc:  0.71875
train loss:  0.5684742331504822
train gradient:  0.8102489752513138
iteration : 637
train acc:  0.7890625
train loss:  0.4850010871887207
train gradient:  0.5918530806788682
iteration : 638
train acc:  0.65625
train loss:  0.6613366007804871
train gradient:  0.859228253257998
iteration : 639
train acc:  0.7734375
train loss:  0.44221797585487366
train gradient:  0.43230659133161026
iteration : 640
train acc:  0.7734375
train loss:  0.4972327649593353
train gradient:  0.5073293556177536
iteration : 641
train acc:  0.7890625
train loss:  0.4928770065307617
train gradient:  0.4901433917601158
iteration : 642
train acc:  0.7265625
train loss:  0.5125768780708313
train gradient:  0.4819607711478973
iteration : 643
train acc:  0.7265625
train loss:  0.5211508274078369
train gradient:  0.5834337077808996
iteration : 644
train acc:  0.8515625
train loss:  0.4273007810115814
train gradient:  0.4307168245862744
iteration : 645
train acc:  0.84375
train loss:  0.38898491859436035
train gradient:  0.42265916246330465
iteration : 646
train acc:  0.671875
train loss:  0.5786440968513489
train gradient:  0.746738336643636
iteration : 647
train acc:  0.7421875
train loss:  0.5087983012199402
train gradient:  0.5373561010118657
iteration : 648
train acc:  0.75
train loss:  0.5202775001525879
train gradient:  0.6752590963590361
iteration : 649
train acc:  0.8046875
train loss:  0.4350855052471161
train gradient:  0.4258906949922498
iteration : 650
train acc:  0.8046875
train loss:  0.4541887938976288
train gradient:  0.45208442004170146
iteration : 651
train acc:  0.796875
train loss:  0.48479846119880676
train gradient:  0.6364037813053958
iteration : 652
train acc:  0.7890625
train loss:  0.46488749980926514
train gradient:  0.5452489109422327
iteration : 653
train acc:  0.75
train loss:  0.5029771327972412
train gradient:  0.5383191170976447
iteration : 654
train acc:  0.7578125
train loss:  0.49750176072120667
train gradient:  0.6874086458089086
iteration : 655
train acc:  0.734375
train loss:  0.5225852727890015
train gradient:  0.5483892069367485
iteration : 656
train acc:  0.71875
train loss:  0.5353395342826843
train gradient:  0.579903210788699
iteration : 657
train acc:  0.7890625
train loss:  0.43207859992980957
train gradient:  0.43190775342045007
iteration : 658
train acc:  0.7890625
train loss:  0.4555170238018036
train gradient:  0.41134071245575904
iteration : 659
train acc:  0.7109375
train loss:  0.5232306122779846
train gradient:  0.6991503773119185
iteration : 660
train acc:  0.8125
train loss:  0.46174123883247375
train gradient:  0.36644657378961876
iteration : 661
train acc:  0.734375
train loss:  0.5346102714538574
train gradient:  0.5487930768284804
iteration : 662
train acc:  0.8046875
train loss:  0.46318337321281433
train gradient:  0.3597138668533708
iteration : 663
train acc:  0.796875
train loss:  0.43910589814186096
train gradient:  0.33198781751893025
iteration : 664
train acc:  0.7421875
train loss:  0.5391571521759033
train gradient:  1.0677253370473865
iteration : 665
train acc:  0.734375
train loss:  0.5555671453475952
train gradient:  0.6969700219569153
iteration : 666
train acc:  0.828125
train loss:  0.44864824414253235
train gradient:  0.5285060123445489
iteration : 667
train acc:  0.7578125
train loss:  0.5257785320281982
train gradient:  0.5950152197775831
iteration : 668
train acc:  0.75
train loss:  0.4791758954524994
train gradient:  0.4311879974450695
iteration : 669
train acc:  0.8046875
train loss:  0.42297208309173584
train gradient:  0.45883377906683565
iteration : 670
train acc:  0.765625
train loss:  0.4830264449119568
train gradient:  0.4487345039414124
iteration : 671
train acc:  0.7578125
train loss:  0.47619935870170593
train gradient:  0.4918791553447001
iteration : 672
train acc:  0.703125
train loss:  0.5377042293548584
train gradient:  0.8068565930739393
iteration : 673
train acc:  0.828125
train loss:  0.4500005543231964
train gradient:  0.41632165794679726
iteration : 674
train acc:  0.7734375
train loss:  0.5167960524559021
train gradient:  0.6727095974352291
iteration : 675
train acc:  0.7265625
train loss:  0.5027012825012207
train gradient:  0.4879611626527278
iteration : 676
train acc:  0.7265625
train loss:  0.5101825594902039
train gradient:  0.5030933638232801
iteration : 677
train acc:  0.78125
train loss:  0.4625478684902191
train gradient:  0.989756435880006
iteration : 678
train acc:  0.765625
train loss:  0.4718811511993408
train gradient:  0.5474668676624604
iteration : 679
train acc:  0.765625
train loss:  0.469393789768219
train gradient:  0.5271081243429891
iteration : 680
train acc:  0.7109375
train loss:  0.5241836905479431
train gradient:  0.6733377112296086
iteration : 681
train acc:  0.7890625
train loss:  0.4719536006450653
train gradient:  0.47020044102560293
iteration : 682
train acc:  0.734375
train loss:  0.5626565217971802
train gradient:  0.655961655145423
iteration : 683
train acc:  0.7109375
train loss:  0.5367785692214966
train gradient:  0.640123098576794
iteration : 684
train acc:  0.7109375
train loss:  0.5296366214752197
train gradient:  0.5292786343708292
iteration : 685
train acc:  0.796875
train loss:  0.4388922452926636
train gradient:  0.39902647402512326
iteration : 686
train acc:  0.7265625
train loss:  0.5125924348831177
train gradient:  0.5803254949499421
iteration : 687
train acc:  0.7578125
train loss:  0.4722088873386383
train gradient:  0.45291245914903
iteration : 688
train acc:  0.7265625
train loss:  0.4937363266944885
train gradient:  0.49637345104044467
iteration : 689
train acc:  0.765625
train loss:  0.522441565990448
train gradient:  0.3529958467706136
iteration : 690
train acc:  0.7421875
train loss:  0.4651620388031006
train gradient:  0.470721555732911
iteration : 691
train acc:  0.7734375
train loss:  0.49245500564575195
train gradient:  0.5538862960525708
iteration : 692
train acc:  0.71875
train loss:  0.5533475279808044
train gradient:  0.4468825029811695
iteration : 693
train acc:  0.734375
train loss:  0.5233441591262817
train gradient:  0.5038542569590038
iteration : 694
train acc:  0.78125
train loss:  0.4787300229072571
train gradient:  0.4051730413120367
iteration : 695
train acc:  0.7109375
train loss:  0.5437144637107849
train gradient:  0.5254764609799821
iteration : 696
train acc:  0.8046875
train loss:  0.44042444229125977
train gradient:  0.38445313713916
iteration : 697
train acc:  0.7890625
train loss:  0.468731164932251
train gradient:  0.3239679584424486
iteration : 698
train acc:  0.734375
train loss:  0.5699899792671204
train gradient:  0.7407458143092893
iteration : 699
train acc:  0.7421875
train loss:  0.5295724272727966
train gradient:  0.5843849648554262
iteration : 700
train acc:  0.6875
train loss:  0.5726519227027893
train gradient:  0.6081130458099615
iteration : 701
train acc:  0.78125
train loss:  0.4451341927051544
train gradient:  0.3422344374334826
iteration : 702
train acc:  0.765625
train loss:  0.4424254894256592
train gradient:  0.36689967117901817
iteration : 703
train acc:  0.7109375
train loss:  0.6044561862945557
train gradient:  0.82281980564153
iteration : 704
train acc:  0.71875
train loss:  0.5019537806510925
train gradient:  0.3908041905799189
iteration : 705
train acc:  0.7265625
train loss:  0.578734278678894
train gradient:  0.6234032281521453
iteration : 706
train acc:  0.7890625
train loss:  0.47731471061706543
train gradient:  0.40112278414166186
iteration : 707
train acc:  0.7265625
train loss:  0.5008741617202759
train gradient:  0.36023386630569976
iteration : 708
train acc:  0.8359375
train loss:  0.45099806785583496
train gradient:  0.4027352333119464
iteration : 709
train acc:  0.765625
train loss:  0.4646923840045929
train gradient:  0.49944596451063566
iteration : 710
train acc:  0.734375
train loss:  0.5063733458518982
train gradient:  0.5301676062824128
iteration : 711
train acc:  0.7421875
train loss:  0.4622785449028015
train gradient:  0.5234342576134541
iteration : 712
train acc:  0.7578125
train loss:  0.5396378040313721
train gradient:  0.732803688930785
iteration : 713
train acc:  0.671875
train loss:  0.5531392097473145
train gradient:  0.5127839423416405
iteration : 714
train acc:  0.7734375
train loss:  0.48423081636428833
train gradient:  0.5275509117405279
iteration : 715
train acc:  0.7421875
train loss:  0.47518983483314514
train gradient:  0.5110089909771722
iteration : 716
train acc:  0.7578125
train loss:  0.48304885625839233
train gradient:  0.5121712855819767
iteration : 717
train acc:  0.765625
train loss:  0.4306635856628418
train gradient:  0.32149246483981614
iteration : 718
train acc:  0.765625
train loss:  0.4520670771598816
train gradient:  0.3961468245243832
iteration : 719
train acc:  0.7578125
train loss:  0.5257310271263123
train gradient:  0.5952103689578137
iteration : 720
train acc:  0.6640625
train loss:  0.5521284341812134
train gradient:  0.624354268112167
iteration : 721
train acc:  0.765625
train loss:  0.47891390323638916
train gradient:  0.5938792691221082
iteration : 722
train acc:  0.7265625
train loss:  0.5538995265960693
train gradient:  0.4469322082213195
iteration : 723
train acc:  0.8046875
train loss:  0.4097477197647095
train gradient:  0.3188787847436037
iteration : 724
train acc:  0.7265625
train loss:  0.5634564161300659
train gradient:  0.5564481718157079
iteration : 725
train acc:  0.78125
train loss:  0.473027765750885
train gradient:  0.3509211821259389
iteration : 726
train acc:  0.84375
train loss:  0.3873199224472046
train gradient:  0.3091741110485303
iteration : 727
train acc:  0.7578125
train loss:  0.5140031576156616
train gradient:  0.5068866504142822
iteration : 728
train acc:  0.7578125
train loss:  0.5057702660560608
train gradient:  0.7246648167028895
iteration : 729
train acc:  0.71875
train loss:  0.5385197997093201
train gradient:  0.554111667804305
iteration : 730
train acc:  0.7109375
train loss:  0.5324465036392212
train gradient:  0.522276170595732
iteration : 731
train acc:  0.7421875
train loss:  0.49236351251602173
train gradient:  1.1603255377766108
iteration : 732
train acc:  0.7890625
train loss:  0.4560313820838928
train gradient:  0.5328678338829775
iteration : 733
train acc:  0.765625
train loss:  0.4792509078979492
train gradient:  0.6292499784783374
iteration : 734
train acc:  0.828125
train loss:  0.41098862886428833
train gradient:  0.472126012933438
iteration : 735
train acc:  0.75
train loss:  0.4732092320919037
train gradient:  0.4927035171060479
iteration : 736
train acc:  0.765625
train loss:  0.5256478190422058
train gradient:  0.5868220604313239
iteration : 737
train acc:  0.7421875
train loss:  0.5250741839408875
train gradient:  0.3547867650550887
iteration : 738
train acc:  0.796875
train loss:  0.46527335047721863
train gradient:  0.47532977154938855
iteration : 739
train acc:  0.78125
train loss:  0.5163957476615906
train gradient:  0.674341364339579
iteration : 740
train acc:  0.796875
train loss:  0.4301568567752838
train gradient:  0.5249728950406072
iteration : 741
train acc:  0.765625
train loss:  0.525415301322937
train gradient:  0.3485094358709112
iteration : 742
train acc:  0.7578125
train loss:  0.4548737406730652
train gradient:  0.5024056843868581
iteration : 743
train acc:  0.75
train loss:  0.5193436741828918
train gradient:  0.5327505571726795
iteration : 744
train acc:  0.8125
train loss:  0.44454291462898254
train gradient:  0.5072440027519618
iteration : 745
train acc:  0.734375
train loss:  0.5004180669784546
train gradient:  0.6129632099463193
iteration : 746
train acc:  0.796875
train loss:  0.46472322940826416
train gradient:  0.41291366087498593
iteration : 747
train acc:  0.71875
train loss:  0.4843006432056427
train gradient:  0.5139630245884762
iteration : 748
train acc:  0.7890625
train loss:  0.4812733232975006
train gradient:  0.47670304299771266
iteration : 749
train acc:  0.71875
train loss:  0.5465718507766724
train gradient:  0.6976012642426543
iteration : 750
train acc:  0.703125
train loss:  0.5034217834472656
train gradient:  0.47809716381694817
iteration : 751
train acc:  0.78125
train loss:  0.5058183670043945
train gradient:  0.5263319947045345
iteration : 752
train acc:  0.7109375
train loss:  0.5749388933181763
train gradient:  0.5635310108357565
iteration : 753
train acc:  0.7265625
train loss:  0.550686776638031
train gradient:  0.5354070831838141
iteration : 754
train acc:  0.7890625
train loss:  0.44060367345809937
train gradient:  0.4187543537322221
iteration : 755
train acc:  0.7109375
train loss:  0.5042237043380737
train gradient:  0.48702040642713973
iteration : 756
train acc:  0.6875
train loss:  0.5452108383178711
train gradient:  0.7180898965029272
iteration : 757
train acc:  0.765625
train loss:  0.4927559494972229
train gradient:  0.5019673611471741
iteration : 758
train acc:  0.7734375
train loss:  0.4339791536331177
train gradient:  0.49425641860908254
iteration : 759
train acc:  0.71875
train loss:  0.5627347826957703
train gradient:  0.5000120848527152
iteration : 760
train acc:  0.765625
train loss:  0.46830660104751587
train gradient:  0.42476189018413396
iteration : 761
train acc:  0.734375
train loss:  0.47157031297683716
train gradient:  0.6751979964953209
iteration : 762
train acc:  0.8125
train loss:  0.4442223310470581
train gradient:  0.3626666683365111
iteration : 763
train acc:  0.8359375
train loss:  0.40366896986961365
train gradient:  0.3232889440737642
iteration : 764
train acc:  0.71875
train loss:  0.5282381772994995
train gradient:  0.5134349029762652
iteration : 765
train acc:  0.7734375
train loss:  0.5028414130210876
train gradient:  0.45775851428293085
iteration : 766
train acc:  0.8359375
train loss:  0.41694849729537964
train gradient:  0.34654246133358346
iteration : 767
train acc:  0.7421875
train loss:  0.5335961580276489
train gradient:  0.5632448140802699
iteration : 768
train acc:  0.734375
train loss:  0.5299441814422607
train gradient:  0.9848814959115633
iteration : 769
train acc:  0.703125
train loss:  0.5027948021888733
train gradient:  0.6169306598780251
iteration : 770
train acc:  0.78125
train loss:  0.5209568738937378
train gradient:  0.4848551965676719
iteration : 771
train acc:  0.8125
train loss:  0.4489743113517761
train gradient:  0.45252295366597817
iteration : 772
train acc:  0.765625
train loss:  0.47666051983833313
train gradient:  0.43640937983042793
iteration : 773
train acc:  0.8203125
train loss:  0.4445993900299072
train gradient:  0.39310372821552614
iteration : 774
train acc:  0.734375
train loss:  0.4756108224391937
train gradient:  0.4853416328371852
iteration : 775
train acc:  0.734375
train loss:  0.49653398990631104
train gradient:  0.5488646986108063
iteration : 776
train acc:  0.8046875
train loss:  0.41843339800834656
train gradient:  0.6064749101007921
iteration : 777
train acc:  0.8359375
train loss:  0.4450686573982239
train gradient:  0.5832442539078513
iteration : 778
train acc:  0.7421875
train loss:  0.4871307611465454
train gradient:  0.615768729739202
iteration : 779
train acc:  0.71875
train loss:  0.4894563853740692
train gradient:  0.4855623519403806
iteration : 780
train acc:  0.75
train loss:  0.48702141642570496
train gradient:  0.5125598790588521
iteration : 781
train acc:  0.734375
train loss:  0.5144121646881104
train gradient:  0.564879965336837
iteration : 782
train acc:  0.75
train loss:  0.5052307844161987
train gradient:  0.54017632346826
iteration : 783
train acc:  0.703125
train loss:  0.6027382612228394
train gradient:  0.8542375835196789
iteration : 784
train acc:  0.734375
train loss:  0.48992782831192017
train gradient:  0.5890455728439405
iteration : 785
train acc:  0.828125
train loss:  0.44717952609062195
train gradient:  0.5894740095457002
iteration : 786
train acc:  0.7421875
train loss:  0.4502512216567993
train gradient:  0.5028510755029361
iteration : 787
train acc:  0.78125
train loss:  0.45102348923683167
train gradient:  0.5724222724529195
iteration : 788
train acc:  0.7265625
train loss:  0.5410246849060059
train gradient:  0.6934688490344372
iteration : 789
train acc:  0.78125
train loss:  0.4454957842826843
train gradient:  0.50208917387634
iteration : 790
train acc:  0.78125
train loss:  0.48227596282958984
train gradient:  0.3756604232536669
iteration : 791
train acc:  0.78125
train loss:  0.4674786627292633
train gradient:  0.4682507617382039
iteration : 792
train acc:  0.75
train loss:  0.5044069886207581
train gradient:  0.4219149102183546
iteration : 793
train acc:  0.8046875
train loss:  0.4294053316116333
train gradient:  0.32088491812335573
iteration : 794
train acc:  0.7890625
train loss:  0.45336994528770447
train gradient:  0.34723682427729347
iteration : 795
train acc:  0.75
train loss:  0.5176580548286438
train gradient:  0.6861522027303699
iteration : 796
train acc:  0.734375
train loss:  0.5111212730407715
train gradient:  0.6582786175302253
iteration : 797
train acc:  0.8046875
train loss:  0.4858526885509491
train gradient:  0.5994773678770722
iteration : 798
train acc:  0.765625
train loss:  0.4681727886199951
train gradient:  0.5514521782292885
iteration : 799
train acc:  0.7734375
train loss:  0.472133994102478
train gradient:  0.5364811246444848
iteration : 800
train acc:  0.796875
train loss:  0.4856255054473877
train gradient:  0.6127401194487139
iteration : 801
train acc:  0.7421875
train loss:  0.5304676294326782
train gradient:  0.6760810510780609
iteration : 802
train acc:  0.7890625
train loss:  0.458918958902359
train gradient:  0.5359230852923553
iteration : 803
train acc:  0.7109375
train loss:  0.5556824803352356
train gradient:  0.6154132199147752
iteration : 804
train acc:  0.7734375
train loss:  0.47314557433128357
train gradient:  0.5886028739775208
iteration : 805
train acc:  0.765625
train loss:  0.5091114044189453
train gradient:  0.6456238323032677
iteration : 806
train acc:  0.734375
train loss:  0.5583627820014954
train gradient:  0.7979213998420447
iteration : 807
train acc:  0.796875
train loss:  0.4149066209793091
train gradient:  0.4884237199285729
iteration : 808
train acc:  0.8046875
train loss:  0.4554402232170105
train gradient:  0.47083158894445537
iteration : 809
train acc:  0.78125
train loss:  0.5031503438949585
train gradient:  0.5325245959045288
iteration : 810
train acc:  0.8359375
train loss:  0.3980619013309479
train gradient:  0.4423391009658053
iteration : 811
train acc:  0.734375
train loss:  0.5399208664894104
train gradient:  0.6367877262026037
iteration : 812
train acc:  0.8125
train loss:  0.4292675852775574
train gradient:  0.6172604157030276
iteration : 813
train acc:  0.703125
train loss:  0.5562970638275146
train gradient:  0.5822481334680497
iteration : 814
train acc:  0.8203125
train loss:  0.44426900148391724
train gradient:  0.577777773992129
iteration : 815
train acc:  0.8359375
train loss:  0.4894566535949707
train gradient:  0.6159028627567732
iteration : 816
train acc:  0.7265625
train loss:  0.5236525535583496
train gradient:  0.7866616422454008
iteration : 817
train acc:  0.78125
train loss:  0.44739389419555664
train gradient:  0.4049931042106916
iteration : 818
train acc:  0.734375
train loss:  0.44396552443504333
train gradient:  0.41906582028844314
iteration : 819
train acc:  0.75
train loss:  0.5104324221611023
train gradient:  0.7089147468828678
iteration : 820
train acc:  0.796875
train loss:  0.4530382454395294
train gradient:  0.5137286658588913
iteration : 821
train acc:  0.8046875
train loss:  0.45927026867866516
train gradient:  0.52022653873015
iteration : 822
train acc:  0.6875
train loss:  0.5684788227081299
train gradient:  0.9533471365342953
iteration : 823
train acc:  0.765625
train loss:  0.49837756156921387
train gradient:  0.6843835398315191
iteration : 824
train acc:  0.828125
train loss:  0.4114440977573395
train gradient:  0.5651305794733058
iteration : 825
train acc:  0.78125
train loss:  0.4753018319606781
train gradient:  0.45209398014208063
iteration : 826
train acc:  0.7578125
train loss:  0.46938320994377136
train gradient:  0.3957889292445415
iteration : 827
train acc:  0.765625
train loss:  0.4829782545566559
train gradient:  0.6049979442151971
iteration : 828
train acc:  0.7421875
train loss:  0.5287920236587524
train gradient:  0.6254396985392947
iteration : 829
train acc:  0.7578125
train loss:  0.512836217880249
train gradient:  0.41050079597155814
iteration : 830
train acc:  0.828125
train loss:  0.39208120107650757
train gradient:  0.4632172458336019
iteration : 831
train acc:  0.734375
train loss:  0.5021394491195679
train gradient:  0.6578612155130923
iteration : 832
train acc:  0.7890625
train loss:  0.4328930675983429
train gradient:  0.42350163230191046
iteration : 833
train acc:  0.7109375
train loss:  0.528047502040863
train gradient:  0.5868744309231355
iteration : 834
train acc:  0.71875
train loss:  0.5149721503257751
train gradient:  0.6200773590475473
iteration : 835
train acc:  0.75
train loss:  0.4867892861366272
train gradient:  0.6617231218152049
iteration : 836
train acc:  0.8046875
train loss:  0.45477864146232605
train gradient:  0.40490163214938013
iteration : 837
train acc:  0.8046875
train loss:  0.4228596091270447
train gradient:  0.4201215470986947
iteration : 838
train acc:  0.7734375
train loss:  0.4971776306629181
train gradient:  0.5282290910023966
iteration : 839
train acc:  0.75
train loss:  0.5414941310882568
train gradient:  0.6792126701781976
iteration : 840
train acc:  0.703125
train loss:  0.5420562028884888
train gradient:  0.6531623398258588
iteration : 841
train acc:  0.6875
train loss:  0.5556780099868774
train gradient:  0.5418360207223816
iteration : 842
train acc:  0.8046875
train loss:  0.44626113772392273
train gradient:  0.5736330269266724
iteration : 843
train acc:  0.8046875
train loss:  0.48502081632614136
train gradient:  0.6413311582048896
iteration : 844
train acc:  0.7265625
train loss:  0.5644305944442749
train gradient:  0.6807799750183728
iteration : 845
train acc:  0.75
train loss:  0.5063871145248413
train gradient:  0.46906638193442324
iteration : 846
train acc:  0.8359375
train loss:  0.38971325755119324
train gradient:  0.5726433405972593
iteration : 847
train acc:  0.765625
train loss:  0.5077475309371948
train gradient:  0.4705864036222233
iteration : 848
train acc:  0.734375
train loss:  0.548121452331543
train gradient:  0.7412314802638182
iteration : 849
train acc:  0.796875
train loss:  0.42505109310150146
train gradient:  0.4161205200812858
iteration : 850
train acc:  0.78125
train loss:  0.49831125140190125
train gradient:  0.5177719464181318
iteration : 851
train acc:  0.765625
train loss:  0.44817525148391724
train gradient:  0.7577795249762306
iteration : 852
train acc:  0.671875
train loss:  0.5538166165351868
train gradient:  0.6801716330849706
iteration : 853
train acc:  0.8046875
train loss:  0.43619072437286377
train gradient:  0.5088804326647431
iteration : 854
train acc:  0.828125
train loss:  0.42992833256721497
train gradient:  0.5176776625375948
iteration : 855
train acc:  0.6484375
train loss:  0.5782718062400818
train gradient:  0.751770239054671
iteration : 856
train acc:  0.75
train loss:  0.5364985466003418
train gradient:  0.4932855125872337
iteration : 857
train acc:  0.8125
train loss:  0.42860615253448486
train gradient:  0.44306723992299607
iteration : 858
train acc:  0.828125
train loss:  0.4682403802871704
train gradient:  0.47898250924596514
iteration : 859
train acc:  0.71875
train loss:  0.5893392562866211
train gradient:  0.7310415900727796
iteration : 860
train acc:  0.7890625
train loss:  0.4609578251838684
train gradient:  0.4835122801280327
iteration : 861
train acc:  0.7109375
train loss:  0.5237628221511841
train gradient:  0.47736969055517364
iteration : 862
train acc:  0.78125
train loss:  0.4363650977611542
train gradient:  0.4075973338920932
iteration : 863
train acc:  0.7734375
train loss:  0.4223363399505615
train gradient:  0.41504991045930767
iteration : 864
train acc:  0.8046875
train loss:  0.40522629022598267
train gradient:  0.4275227195701623
iteration : 865
train acc:  0.7734375
train loss:  0.47761809825897217
train gradient:  0.6792107319707742
iteration : 866
train acc:  0.7890625
train loss:  0.47147536277770996
train gradient:  0.9562245322028282
iteration : 867
train acc:  0.734375
train loss:  0.5420764684677124
train gradient:  0.6181373202387501
iteration : 868
train acc:  0.8359375
train loss:  0.3989543616771698
train gradient:  0.45084446950962614
iteration : 869
train acc:  0.71875
train loss:  0.5574347972869873
train gradient:  0.6570107064890887
iteration : 870
train acc:  0.7578125
train loss:  0.498884916305542
train gradient:  0.6137793742303167
iteration : 871
train acc:  0.734375
train loss:  0.5111422538757324
train gradient:  0.4504026127012572
iteration : 872
train acc:  0.7265625
train loss:  0.5161631107330322
train gradient:  0.6048011632812009
iteration : 873
train acc:  0.78125
train loss:  0.4648922383785248
train gradient:  0.38374576336479954
iteration : 874
train acc:  0.7890625
train loss:  0.48357030749320984
train gradient:  0.4237033558106503
iteration : 875
train acc:  0.78125
train loss:  0.4737273156642914
train gradient:  0.5233553160453447
iteration : 876
train acc:  0.7890625
train loss:  0.4453599750995636
train gradient:  0.5131970025750092
iteration : 877
train acc:  0.7734375
train loss:  0.45917537808418274
train gradient:  0.3105857863832154
iteration : 878
train acc:  0.78125
train loss:  0.4344101548194885
train gradient:  0.2970034924851422
iteration : 879
train acc:  0.7109375
train loss:  0.5589336156845093
train gradient:  0.7113872458535846
iteration : 880
train acc:  0.7421875
train loss:  0.47601252794265747
train gradient:  0.5621982342525895
iteration : 881
train acc:  0.7578125
train loss:  0.49490538239479065
train gradient:  0.45292462568739655
iteration : 882
train acc:  0.75
train loss:  0.5440548658370972
train gradient:  0.5919143003521261
iteration : 883
train acc:  0.765625
train loss:  0.5053771138191223
train gradient:  0.7371579216339816
iteration : 884
train acc:  0.75
train loss:  0.4409986734390259
train gradient:  0.42507182304267055
iteration : 885
train acc:  0.78125
train loss:  0.45143771171569824
train gradient:  0.4008197555946858
iteration : 886
train acc:  0.78125
train loss:  0.49976855516433716
train gradient:  0.5098561190152598
iteration : 887
train acc:  0.8125
train loss:  0.4072679281234741
train gradient:  0.37826176611787476
iteration : 888
train acc:  0.7734375
train loss:  0.48908567428588867
train gradient:  0.46407150119831136
iteration : 889
train acc:  0.7421875
train loss:  0.5470366477966309
train gradient:  0.6807356732188397
iteration : 890
train acc:  0.7578125
train loss:  0.5007097721099854
train gradient:  0.4935964053162418
iteration : 891
train acc:  0.84375
train loss:  0.437446653842926
train gradient:  0.4290511556676987
iteration : 892
train acc:  0.7265625
train loss:  0.503355860710144
train gradient:  0.44739964107813707
iteration : 893
train acc:  0.8125
train loss:  0.4241519868373871
train gradient:  0.4749797371399903
iteration : 894
train acc:  0.7734375
train loss:  0.43375253677368164
train gradient:  0.5493114965004728
iteration : 895
train acc:  0.828125
train loss:  0.3722458779811859
train gradient:  0.3758386503280732
iteration : 896
train acc:  0.84375
train loss:  0.3987988829612732
train gradient:  0.42826038091135205
iteration : 897
train acc:  0.7734375
train loss:  0.48695108294487
train gradient:  0.4204928732886745
iteration : 898
train acc:  0.7578125
train loss:  0.4723498225212097
train gradient:  0.6475879116356152
iteration : 899
train acc:  0.8046875
train loss:  0.4474834203720093
train gradient:  0.6811152218727841
iteration : 900
train acc:  0.734375
train loss:  0.47872063517570496
train gradient:  0.6538473220408214
iteration : 901
train acc:  0.796875
train loss:  0.48674142360687256
train gradient:  0.7761128431615685
iteration : 902
train acc:  0.7578125
train loss:  0.45659831166267395
train gradient:  0.5637070070028469
iteration : 903
train acc:  0.75
train loss:  0.5521619319915771
train gradient:  0.5652166513481369
iteration : 904
train acc:  0.7890625
train loss:  0.44864219427108765
train gradient:  0.5972769137223933
iteration : 905
train acc:  0.796875
train loss:  0.43028494715690613
train gradient:  0.3910946514226729
iteration : 906
train acc:  0.703125
train loss:  0.6126736998558044
train gradient:  0.8361908573767186
iteration : 907
train acc:  0.75
train loss:  0.5029916167259216
train gradient:  0.517883990894678
iteration : 908
train acc:  0.7578125
train loss:  0.46239325404167175
train gradient:  0.6104441116006398
iteration : 909
train acc:  0.7890625
train loss:  0.515524685382843
train gradient:  0.49528584668875664
iteration : 910
train acc:  0.796875
train loss:  0.4329993426799774
train gradient:  0.49475187517201386
iteration : 911
train acc:  0.765625
train loss:  0.4514543414115906
train gradient:  0.45946906224869144
iteration : 912
train acc:  0.84375
train loss:  0.4015679657459259
train gradient:  0.48692140237376635
iteration : 913
train acc:  0.734375
train loss:  0.5023711323738098
train gradient:  0.5174856028370871
iteration : 914
train acc:  0.7421875
train loss:  0.5054795145988464
train gradient:  0.7038804432164087
iteration : 915
train acc:  0.765625
train loss:  0.5016113519668579
train gradient:  0.6432756894384998
iteration : 916
train acc:  0.6953125
train loss:  0.6195651292800903
train gradient:  0.7851881033803334
iteration : 917
train acc:  0.8125
train loss:  0.4276384115219116
train gradient:  0.47396104096251923
iteration : 918
train acc:  0.8203125
train loss:  0.4312993288040161
train gradient:  0.41605210361949907
iteration : 919
train acc:  0.7734375
train loss:  0.48458945751190186
train gradient:  0.5347403670555687
iteration : 920
train acc:  0.78125
train loss:  0.5119903683662415
train gradient:  0.6518337828364797
iteration : 921
train acc:  0.8125
train loss:  0.46000152826309204
train gradient:  0.4701462788288254
iteration : 922
train acc:  0.75
train loss:  0.45673924684524536
train gradient:  0.5252969458410199
iteration : 923
train acc:  0.7734375
train loss:  0.43645673990249634
train gradient:  0.46399278052915194
iteration : 924
train acc:  0.7421875
train loss:  0.49849992990493774
train gradient:  0.7597537493643346
iteration : 925
train acc:  0.78125
train loss:  0.45383816957473755
train gradient:  0.44232850668833085
iteration : 926
train acc:  0.7578125
train loss:  0.4830590784549713
train gradient:  0.5358877977914561
iteration : 927
train acc:  0.8125
train loss:  0.40750813484191895
train gradient:  0.6119172157270815
iteration : 928
train acc:  0.796875
train loss:  0.45817887783050537
train gradient:  0.4452317725038789
iteration : 929
train acc:  0.7734375
train loss:  0.49429070949554443
train gradient:  0.48753802577773153
iteration : 930
train acc:  0.78125
train loss:  0.46646934747695923
train gradient:  0.6699352374892298
iteration : 931
train acc:  0.734375
train loss:  0.5554660558700562
train gradient:  0.7867169333560882
iteration : 932
train acc:  0.765625
train loss:  0.5483472347259521
train gradient:  0.6245055305633839
iteration : 933
train acc:  0.765625
train loss:  0.49362123012542725
train gradient:  0.5542254492521193
iteration : 934
train acc:  0.8203125
train loss:  0.4816437363624573
train gradient:  0.4980180196591895
iteration : 935
train acc:  0.7734375
train loss:  0.4206925630569458
train gradient:  0.4571246860492377
iteration : 936
train acc:  0.7578125
train loss:  0.5170418620109558
train gradient:  0.5205412998504989
iteration : 937
train acc:  0.765625
train loss:  0.5088267922401428
train gradient:  0.5589410158338487
iteration : 938
train acc:  0.7578125
train loss:  0.5129920244216919
train gradient:  0.6570117940694623
iteration : 939
train acc:  0.765625
train loss:  0.5014923214912415
train gradient:  0.5772644232449788
iteration : 940
train acc:  0.75
train loss:  0.47841382026672363
train gradient:  0.5245982022645486
iteration : 941
train acc:  0.8515625
train loss:  0.44013434648513794
train gradient:  0.5135107161557659
iteration : 942
train acc:  0.765625
train loss:  0.4333251714706421
train gradient:  0.4061392713439425
iteration : 943
train acc:  0.7890625
train loss:  0.41809067130088806
train gradient:  0.48831337207467135
iteration : 944
train acc:  0.8515625
train loss:  0.41991370916366577
train gradient:  0.5973221597150657
iteration : 945
train acc:  0.7578125
train loss:  0.4392228126525879
train gradient:  0.47595536864458093
iteration : 946
train acc:  0.8125
train loss:  0.4223250150680542
train gradient:  0.574762414641467
iteration : 947
train acc:  0.7578125
train loss:  0.48361238837242126
train gradient:  0.6287532975493828
iteration : 948
train acc:  0.7421875
train loss:  0.4888599216938019
train gradient:  0.4917931124255621
iteration : 949
train acc:  0.7734375
train loss:  0.48500239849090576
train gradient:  0.5344841784121983
iteration : 950
train acc:  0.8203125
train loss:  0.4025683104991913
train gradient:  0.4952096717544982
iteration : 951
train acc:  0.7890625
train loss:  0.4476920962333679
train gradient:  0.48381783678947543
iteration : 952
train acc:  0.7890625
train loss:  0.43627744913101196
train gradient:  0.44533747370443777
iteration : 953
train acc:  0.828125
train loss:  0.38763898611068726
train gradient:  1.9577667373479357
iteration : 954
train acc:  0.7265625
train loss:  0.4932312071323395
train gradient:  0.6789394503393085
iteration : 955
train acc:  0.8125
train loss:  0.4727369248867035
train gradient:  0.4680885017364742
iteration : 956
train acc:  0.765625
train loss:  0.4661039710044861
train gradient:  0.4804409114151632
iteration : 957
train acc:  0.8125
train loss:  0.41837432980537415
train gradient:  0.40832697284301017
iteration : 958
train acc:  0.8046875
train loss:  0.4192385971546173
train gradient:  0.34442606403654824
iteration : 959
train acc:  0.7578125
train loss:  0.44005173444747925
train gradient:  0.7198528967451333
iteration : 960
train acc:  0.7890625
train loss:  0.4338070750236511
train gradient:  0.5104889177613945
iteration : 961
train acc:  0.8203125
train loss:  0.43400901556015015
train gradient:  0.4751878142328994
iteration : 962
train acc:  0.8125
train loss:  0.43133825063705444
train gradient:  0.5098897967157401
iteration : 963
train acc:  0.8046875
train loss:  0.4247249960899353
train gradient:  0.4769275963887622
iteration : 964
train acc:  0.8359375
train loss:  0.4171825647354126
train gradient:  0.4996174584627746
iteration : 965
train acc:  0.8203125
train loss:  0.41903311014175415
train gradient:  0.461520212835274
iteration : 966
train acc:  0.765625
train loss:  0.4304320812225342
train gradient:  0.3966887798922915
iteration : 967
train acc:  0.8125
train loss:  0.4810320734977722
train gradient:  0.6892486899553146
iteration : 968
train acc:  0.8125
train loss:  0.4524381160736084
train gradient:  0.49956797346391574
iteration : 969
train acc:  0.84375
train loss:  0.4007498621940613
train gradient:  0.7574357513953802
iteration : 970
train acc:  0.7578125
train loss:  0.4936818778514862
train gradient:  0.5613152677932427
iteration : 971
train acc:  0.859375
train loss:  0.3819890022277832
train gradient:  0.42917466452540004
iteration : 972
train acc:  0.828125
train loss:  0.4159184992313385
train gradient:  0.4696392805400474
iteration : 973
train acc:  0.7890625
train loss:  0.4371963143348694
train gradient:  0.5891331608388344
iteration : 974
train acc:  0.78125
train loss:  0.4053177833557129
train gradient:  0.5034055507100345
iteration : 975
train acc:  0.703125
train loss:  0.5245790481567383
train gradient:  0.9212111890725535
iteration : 976
train acc:  0.8203125
train loss:  0.4691891372203827
train gradient:  0.7043416548398072
iteration : 977
train acc:  0.8046875
train loss:  0.4376005530357361
train gradient:  0.5964207337654442
iteration : 978
train acc:  0.828125
train loss:  0.4210858941078186
train gradient:  0.45589275447976924
iteration : 979
train acc:  0.7265625
train loss:  0.5079967379570007
train gradient:  0.6277491612034858
iteration : 980
train acc:  0.7890625
train loss:  0.47104769945144653
train gradient:  0.4798913477916972
iteration : 981
train acc:  0.7578125
train loss:  0.4759395718574524
train gradient:  0.499144789213879
iteration : 982
train acc:  0.796875
train loss:  0.4264650344848633
train gradient:  0.4565404105175368
iteration : 983
train acc:  0.7578125
train loss:  0.5341405868530273
train gradient:  0.5932988485500166
iteration : 984
train acc:  0.75
train loss:  0.5349969267845154
train gradient:  0.8025902059918567
iteration : 985
train acc:  0.75
train loss:  0.4925411343574524
train gradient:  0.5538311936826873
iteration : 986
train acc:  0.7109375
train loss:  0.5528939366340637
train gradient:  0.7356680539933013
iteration : 987
train acc:  0.7421875
train loss:  0.5069869160652161
train gradient:  0.6498460058648099
iteration : 988
train acc:  0.7578125
train loss:  0.43693095445632935
train gradient:  0.4755294644650639
iteration : 989
train acc:  0.7734375
train loss:  0.5164446830749512
train gradient:  0.5601631484154493
iteration : 990
train acc:  0.765625
train loss:  0.4528338313102722
train gradient:  0.4878350782308108
iteration : 991
train acc:  0.7890625
train loss:  0.455613374710083
train gradient:  0.5876449416353983
iteration : 992
train acc:  0.859375
train loss:  0.46254217624664307
train gradient:  0.5591891323091199
iteration : 993
train acc:  0.796875
train loss:  0.4410620629787445
train gradient:  0.42275047563918866
iteration : 994
train acc:  0.78125
train loss:  0.4655078649520874
train gradient:  0.4274794319468321
iteration : 995
train acc:  0.7265625
train loss:  0.5548309683799744
train gradient:  0.5519249693944126
iteration : 996
train acc:  0.7421875
train loss:  0.5448267459869385
train gradient:  0.6779283383104066
iteration : 997
train acc:  0.765625
train loss:  0.49005430936813354
train gradient:  0.5636026319394424
iteration : 998
train acc:  0.7890625
train loss:  0.4583786725997925
train gradient:  0.4972106877240494
iteration : 999
train acc:  0.7578125
train loss:  0.45208948850631714
train gradient:  0.41302539158582247
iteration : 1000
train acc:  0.78125
train loss:  0.4448026120662689
train gradient:  0.4414556843047879
iteration : 1001
train acc:  0.7265625
train loss:  0.5222400426864624
train gradient:  0.5536056041431578
iteration : 1002
train acc:  0.8203125
train loss:  0.4328901767730713
train gradient:  0.4067357789529556
iteration : 1003
train acc:  0.7734375
train loss:  0.4610000550746918
train gradient:  0.5180612196478811
iteration : 1004
train acc:  0.796875
train loss:  0.4513799250125885
train gradient:  0.3791795444276645
iteration : 1005
train acc:  0.8125
train loss:  0.3706854581832886
train gradient:  0.3564563290244979
iteration : 1006
train acc:  0.7890625
train loss:  0.4228893518447876
train gradient:  0.4186195853880569
iteration : 1007
train acc:  0.7578125
train loss:  0.48647594451904297
train gradient:  0.48276364235405533
iteration : 1008
train acc:  0.8203125
train loss:  0.37599217891693115
train gradient:  0.3919786270407004
iteration : 1009
train acc:  0.8046875
train loss:  0.44508856534957886
train gradient:  0.4558669348471642
iteration : 1010
train acc:  0.78125
train loss:  0.426127552986145
train gradient:  0.4917620339889351
iteration : 1011
train acc:  0.7421875
train loss:  0.5252538919448853
train gradient:  0.8322610608828905
iteration : 1012
train acc:  0.859375
train loss:  0.39477303624153137
train gradient:  0.2651797740539769
iteration : 1013
train acc:  0.7734375
train loss:  0.506339430809021
train gradient:  0.4894958303190799
iteration : 1014
train acc:  0.7890625
train loss:  0.4945754408836365
train gradient:  0.5478913403579473
iteration : 1015
train acc:  0.7890625
train loss:  0.4027758836746216
train gradient:  0.4773977774902355
iteration : 1016
train acc:  0.796875
train loss:  0.45941275358200073
train gradient:  0.397708668736803
iteration : 1017
train acc:  0.765625
train loss:  0.49157071113586426
train gradient:  0.46874208648108134
iteration : 1018
train acc:  0.75
train loss:  0.5173128247261047
train gradient:  0.521257295372544
iteration : 1019
train acc:  0.7734375
train loss:  0.4769462049007416
train gradient:  0.5025597919712529
iteration : 1020
train acc:  0.7578125
train loss:  0.5074356198310852
train gradient:  0.3979803307525708
iteration : 1021
train acc:  0.765625
train loss:  0.4942903220653534
train gradient:  0.694366255629765
iteration : 1022
train acc:  0.78125
train loss:  0.4673479199409485
train gradient:  0.44725464459779013
iteration : 1023
train acc:  0.7109375
train loss:  0.5561429858207703
train gradient:  0.581616125492814
iteration : 1024
train acc:  0.7421875
train loss:  0.5652782320976257
train gradient:  0.6138591535057407
iteration : 1025
train acc:  0.765625
train loss:  0.4931643605232239
train gradient:  0.45192298883288956
iteration : 1026
train acc:  0.7890625
train loss:  0.4357750415802002
train gradient:  0.5413217670774035
iteration : 1027
train acc:  0.7734375
train loss:  0.4631509780883789
train gradient:  0.3883097393726002
iteration : 1028
train acc:  0.7734375
train loss:  0.4947497248649597
train gradient:  0.7125572902546113
iteration : 1029
train acc:  0.8046875
train loss:  0.39724287390708923
train gradient:  0.41663692593556934
iteration : 1030
train acc:  0.75
train loss:  0.48341965675354004
train gradient:  0.624012594634847
iteration : 1031
train acc:  0.78125
train loss:  0.4541281461715698
train gradient:  0.4339653635820096
iteration : 1032
train acc:  0.7890625
train loss:  0.4244462251663208
train gradient:  0.39713531655605805
iteration : 1033
train acc:  0.828125
train loss:  0.39781150221824646
train gradient:  0.34776665311618654
iteration : 1034
train acc:  0.703125
train loss:  0.5122084617614746
train gradient:  0.6917685376859717
iteration : 1035
train acc:  0.7734375
train loss:  0.5238465070724487
train gradient:  0.7080108323486322
iteration : 1036
train acc:  0.8203125
train loss:  0.4184512495994568
train gradient:  0.3168293832331271
iteration : 1037
train acc:  0.7890625
train loss:  0.48331183195114136
train gradient:  0.5435888484586628
iteration : 1038
train acc:  0.734375
train loss:  0.5102939009666443
train gradient:  0.5065071168117841
iteration : 1039
train acc:  0.75
train loss:  0.5040940046310425
train gradient:  0.6689016633727141
iteration : 1040
train acc:  0.734375
train loss:  0.4907863438129425
train gradient:  0.4073329955699368
iteration : 1041
train acc:  0.765625
train loss:  0.461162269115448
train gradient:  0.5975949450706461
iteration : 1042
train acc:  0.8046875
train loss:  0.45015910267829895
train gradient:  0.4712148178024257
iteration : 1043
train acc:  0.75
train loss:  0.5282471179962158
train gradient:  0.610538500959329
iteration : 1044
train acc:  0.828125
train loss:  0.4199179708957672
train gradient:  0.3612151217249169
iteration : 1045
train acc:  0.796875
train loss:  0.46476036310195923
train gradient:  0.4893071111493072
iteration : 1046
train acc:  0.8515625
train loss:  0.399233877658844
train gradient:  0.41726032863553125
iteration : 1047
train acc:  0.796875
train loss:  0.4864983856678009
train gradient:  0.6300883578380205
iteration : 1048
train acc:  0.796875
train loss:  0.39089590311050415
train gradient:  0.32362628971350593
iteration : 1049
train acc:  0.7578125
train loss:  0.5056583881378174
train gradient:  0.5938238185743886
iteration : 1050
train acc:  0.78125
train loss:  0.448606938123703
train gradient:  0.36802111780939845
iteration : 1051
train acc:  0.78125
train loss:  0.45864635705947876
train gradient:  0.5766364614246401
iteration : 1052
train acc:  0.8046875
train loss:  0.42881494760513306
train gradient:  0.438830808597397
iteration : 1053
train acc:  0.78125
train loss:  0.4577016234397888
train gradient:  0.4632436580346651
iteration : 1054
train acc:  0.75
train loss:  0.4686110317707062
train gradient:  0.6434052257404721
iteration : 1055
train acc:  0.765625
train loss:  0.4271612763404846
train gradient:  0.614129642597071
iteration : 1056
train acc:  0.7734375
train loss:  0.5125719904899597
train gradient:  0.5865872862631594
iteration : 1057
train acc:  0.765625
train loss:  0.43350693583488464
train gradient:  0.494572118533801
iteration : 1058
train acc:  0.8125
train loss:  0.4079923927783966
train gradient:  0.32638851468180125
iteration : 1059
train acc:  0.8203125
train loss:  0.40783530473709106
train gradient:  0.38079113772816325
iteration : 1060
train acc:  0.828125
train loss:  0.3639800548553467
train gradient:  0.32110039167839244
iteration : 1061
train acc:  0.765625
train loss:  0.4620493948459625
train gradient:  0.3724965629396878
iteration : 1062
train acc:  0.8203125
train loss:  0.43568912148475647
train gradient:  0.5015860320479882
iteration : 1063
train acc:  0.78125
train loss:  0.4430013597011566
train gradient:  0.4118407335542597
iteration : 1064
train acc:  0.828125
train loss:  0.41428759694099426
train gradient:  0.5012366681526939
iteration : 1065
train acc:  0.8046875
train loss:  0.4619710147380829
train gradient:  0.519732209908853
iteration : 1066
train acc:  0.78125
train loss:  0.46498990058898926
train gradient:  0.5162780554767034
iteration : 1067
train acc:  0.7734375
train loss:  0.485351026058197
train gradient:  0.45499666146242934
iteration : 1068
train acc:  0.78125
train loss:  0.43958500027656555
train gradient:  0.5009368035521599
iteration : 1069
train acc:  0.7734375
train loss:  0.5147275924682617
train gradient:  0.6216994262915558
iteration : 1070
train acc:  0.7734375
train loss:  0.4471520781517029
train gradient:  0.47335718615572103
iteration : 1071
train acc:  0.78125
train loss:  0.45480847358703613
train gradient:  0.38618427441935976
iteration : 1072
train acc:  0.828125
train loss:  0.3607228994369507
train gradient:  0.38920254946021166
iteration : 1073
train acc:  0.7421875
train loss:  0.5700758695602417
train gradient:  0.656546826077529
iteration : 1074
train acc:  0.828125
train loss:  0.3812572658061981
train gradient:  0.3759226379194773
iteration : 1075
train acc:  0.84375
train loss:  0.3822952210903168
train gradient:  0.39602929792725367
iteration : 1076
train acc:  0.8203125
train loss:  0.40516746044158936
train gradient:  0.3699694288964458
iteration : 1077
train acc:  0.7890625
train loss:  0.4409204423427582
train gradient:  0.6196573366750306
iteration : 1078
train acc:  0.8125
train loss:  0.404123991727829
train gradient:  0.35542350289026037
iteration : 1079
train acc:  0.75
train loss:  0.5389561653137207
train gradient:  0.6228762286971139
iteration : 1080
train acc:  0.7578125
train loss:  0.49285390973091125
train gradient:  0.48546549531017397
iteration : 1081
train acc:  0.8046875
train loss:  0.45281943678855896
train gradient:  0.4536329525573179
iteration : 1082
train acc:  0.75
train loss:  0.5357421636581421
train gradient:  0.652879021728653
iteration : 1083
train acc:  0.8125
train loss:  0.3860824704170227
train gradient:  0.42509770107437017
iteration : 1084
train acc:  0.8046875
train loss:  0.4216511845588684
train gradient:  0.41247835844767794
iteration : 1085
train acc:  0.765625
train loss:  0.5113604664802551
train gradient:  0.7464310689821065
iteration : 1086
train acc:  0.7109375
train loss:  0.4660426080226898
train gradient:  0.6114420147231776
iteration : 1087
train acc:  0.734375
train loss:  0.5014528632164001
train gradient:  0.6030066777341534
iteration : 1088
train acc:  0.8125
train loss:  0.44413065910339355
train gradient:  0.6472835547167461
iteration : 1089
train acc:  0.78125
train loss:  0.462279736995697
train gradient:  0.5214950162494262
iteration : 1090
train acc:  0.84375
train loss:  0.3955405652523041
train gradient:  0.4322766196832047
iteration : 1091
train acc:  0.7421875
train loss:  0.5301046371459961
train gradient:  0.6285502083112334
iteration : 1092
train acc:  0.7421875
train loss:  0.5292887687683105
train gradient:  0.7199155121294567
iteration : 1093
train acc:  0.828125
train loss:  0.4378018379211426
train gradient:  0.45896744944664375
iteration : 1094
train acc:  0.8125
train loss:  0.4132952094078064
train gradient:  0.3656167363045063
iteration : 1095
train acc:  0.796875
train loss:  0.4495390057563782
train gradient:  0.5042096144983221
iteration : 1096
train acc:  0.8046875
train loss:  0.41691744327545166
train gradient:  0.4424840340574949
iteration : 1097
train acc:  0.8046875
train loss:  0.4350932836532593
train gradient:  0.4997588351800321
iteration : 1098
train acc:  0.75
train loss:  0.4642006754875183
train gradient:  0.49914087580557237
iteration : 1099
train acc:  0.8203125
train loss:  0.4192363917827606
train gradient:  0.3940335961288546
iteration : 1100
train acc:  0.796875
train loss:  0.43755921721458435
train gradient:  0.40855254010182845
iteration : 1101
train acc:  0.7734375
train loss:  0.486362099647522
train gradient:  0.5426744161575203
iteration : 1102
train acc:  0.8515625
train loss:  0.3731679618358612
train gradient:  0.3693522798257946
iteration : 1103
train acc:  0.8359375
train loss:  0.4369434416294098
train gradient:  0.4186221146299059
iteration : 1104
train acc:  0.7890625
train loss:  0.4355529844760895
train gradient:  0.41109055296610825
iteration : 1105
train acc:  0.8046875
train loss:  0.3967578709125519
train gradient:  0.45685842244539276
iteration : 1106
train acc:  0.8046875
train loss:  0.4281507134437561
train gradient:  0.2964870330539844
iteration : 1107
train acc:  0.7890625
train loss:  0.3991045355796814
train gradient:  0.5721017968211513
iteration : 1108
train acc:  0.765625
train loss:  0.4437141418457031
train gradient:  0.5886890791780552
iteration : 1109
train acc:  0.765625
train loss:  0.42453819513320923
train gradient:  0.4392318107835331
iteration : 1110
train acc:  0.7421875
train loss:  0.4507964253425598
train gradient:  0.5748603403310233
iteration : 1111
train acc:  0.8515625
train loss:  0.3567975163459778
train gradient:  0.32129189087411386
iteration : 1112
train acc:  0.7890625
train loss:  0.4280328154563904
train gradient:  0.5454711466516173
iteration : 1113
train acc:  0.765625
train loss:  0.4780135452747345
train gradient:  0.512638987597251
iteration : 1114
train acc:  0.7578125
train loss:  0.4702886939048767
train gradient:  0.48426709408910124
iteration : 1115
train acc:  0.7265625
train loss:  0.48642322421073914
train gradient:  0.4585100130393842
iteration : 1116
train acc:  0.8359375
train loss:  0.43647313117980957
train gradient:  0.4942985073109282
iteration : 1117
train acc:  0.71875
train loss:  0.5115242600440979
train gradient:  0.700128253488997
iteration : 1118
train acc:  0.8359375
train loss:  0.36411064863204956
train gradient:  0.38058894033781043
iteration : 1119
train acc:  0.75
train loss:  0.4900221526622772
train gradient:  0.4939883569814566
iteration : 1120
train acc:  0.75
train loss:  0.48076578974723816
train gradient:  0.6340325924720036
iteration : 1121
train acc:  0.765625
train loss:  0.5529007911682129
train gradient:  0.7733764443479383
iteration : 1122
train acc:  0.8515625
train loss:  0.36796629428863525
train gradient:  0.34575565029954713
iteration : 1123
train acc:  0.7734375
train loss:  0.46851474046707153
train gradient:  0.672153583247883
iteration : 1124
train acc:  0.7734375
train loss:  0.43423670530319214
train gradient:  0.5779305449103606
iteration : 1125
train acc:  0.7421875
train loss:  0.5083476901054382
train gradient:  0.6668797364388006
iteration : 1126
train acc:  0.7578125
train loss:  0.4611598253250122
train gradient:  0.6094222291982632
iteration : 1127
train acc:  0.7734375
train loss:  0.4437512159347534
train gradient:  0.5983919918516123
iteration : 1128
train acc:  0.7421875
train loss:  0.5336530208587646
train gradient:  0.7019793683293087
iteration : 1129
train acc:  0.765625
train loss:  0.45217427611351013
train gradient:  0.6714220646966351
iteration : 1130
train acc:  0.7421875
train loss:  0.4776921272277832
train gradient:  0.6235977658495555
iteration : 1131
train acc:  0.7578125
train loss:  0.4800682067871094
train gradient:  0.5415782328776764
iteration : 1132
train acc:  0.6953125
train loss:  0.5145222544670105
train gradient:  0.5387336865823108
iteration : 1133
train acc:  0.8125
train loss:  0.4387475848197937
train gradient:  0.5701512389204229
iteration : 1134
train acc:  0.734375
train loss:  0.5209378004074097
train gradient:  0.6402826623604919
iteration : 1135
train acc:  0.7578125
train loss:  0.43777167797088623
train gradient:  0.4631671802497645
iteration : 1136
train acc:  0.7578125
train loss:  0.4899598956108093
train gradient:  0.6029995924836928
iteration : 1137
train acc:  0.78125
train loss:  0.4726578891277313
train gradient:  0.41519811418193964
iteration : 1138
train acc:  0.7421875
train loss:  0.484224408864975
train gradient:  0.5384785960822513
iteration : 1139
train acc:  0.75
train loss:  0.5347138047218323
train gradient:  0.5781391922678818
iteration : 1140
train acc:  0.703125
train loss:  0.5727053880691528
train gradient:  1.0336606885457569
iteration : 1141
train acc:  0.8046875
train loss:  0.4541856646537781
train gradient:  0.819417655240183
iteration : 1142
train acc:  0.78125
train loss:  0.48174917697906494
train gradient:  0.6847944008629272
iteration : 1143
train acc:  0.8046875
train loss:  0.4441950023174286
train gradient:  0.4414205430377866
iteration : 1144
train acc:  0.75
train loss:  0.47245875000953674
train gradient:  0.7953933974365479
iteration : 1145
train acc:  0.765625
train loss:  0.48231828212738037
train gradient:  0.4974846282394745
iteration : 1146
train acc:  0.84375
train loss:  0.3864971399307251
train gradient:  0.475888739120507
iteration : 1147
train acc:  0.7734375
train loss:  0.4520074725151062
train gradient:  0.4071356493069554
iteration : 1148
train acc:  0.75
train loss:  0.5123526453971863
train gradient:  0.585134589488121
iteration : 1149
train acc:  0.8203125
train loss:  0.39848393201828003
train gradient:  0.3501807435412871
iteration : 1150
train acc:  0.7578125
train loss:  0.5058852434158325
train gradient:  0.561521360752546
iteration : 1151
train acc:  0.7734375
train loss:  0.46762484312057495
train gradient:  0.5498436019231173
iteration : 1152
train acc:  0.75
train loss:  0.47107958793640137
train gradient:  0.3421651911709932
iteration : 1153
train acc:  0.8203125
train loss:  0.41176357865333557
train gradient:  0.3366478918401031
iteration : 1154
train acc:  0.875
train loss:  0.38742223381996155
train gradient:  0.3593090668541333
iteration : 1155
train acc:  0.8125
train loss:  0.38426655530929565
train gradient:  0.3392746095620534
iteration : 1156
train acc:  0.8046875
train loss:  0.45620962977409363
train gradient:  0.35151799337443757
iteration : 1157
train acc:  0.8203125
train loss:  0.4188944697380066
train gradient:  0.6014280758058406
iteration : 1158
train acc:  0.75
train loss:  0.460903525352478
train gradient:  0.48011035257253126
iteration : 1159
train acc:  0.765625
train loss:  0.4638217091560364
train gradient:  0.35881552215048734
iteration : 1160
train acc:  0.828125
train loss:  0.39499568939208984
train gradient:  0.27572292002792215
iteration : 1161
train acc:  0.8359375
train loss:  0.4551907479763031
train gradient:  0.5232025062805993
iteration : 1162
train acc:  0.8125
train loss:  0.4921322464942932
train gradient:  0.4700167900533167
iteration : 1163
train acc:  0.765625
train loss:  0.4710126519203186
train gradient:  0.5419087731045884
iteration : 1164
train acc:  0.78125
train loss:  0.4869014024734497
train gradient:  0.4379350913058976
iteration : 1165
train acc:  0.8046875
train loss:  0.43573784828186035
train gradient:  0.30559030639370866
iteration : 1166
train acc:  0.859375
train loss:  0.36978912353515625
train gradient:  0.3293783780546795
iteration : 1167
train acc:  0.8359375
train loss:  0.40758174657821655
train gradient:  0.4112180801355233
iteration : 1168
train acc:  0.8671875
train loss:  0.38920900225639343
train gradient:  0.3528883602786075
iteration : 1169
train acc:  0.796875
train loss:  0.39506176114082336
train gradient:  0.34397315929117295
iteration : 1170
train acc:  0.7578125
train loss:  0.4895729422569275
train gradient:  0.4878534352176975
iteration : 1171
train acc:  0.7734375
train loss:  0.5017601847648621
train gradient:  0.7252868330459289
iteration : 1172
train acc:  0.8359375
train loss:  0.4129089117050171
train gradient:  0.4502990918493727
iteration : 1173
train acc:  0.7734375
train loss:  0.4014509320259094
train gradient:  0.2924927916989488
iteration : 1174
train acc:  0.7890625
train loss:  0.5070633888244629
train gradient:  0.430195388755856
iteration : 1175
train acc:  0.8515625
train loss:  0.3839385509490967
train gradient:  0.36443715853623543
iteration : 1176
train acc:  0.8125
train loss:  0.41989073157310486
train gradient:  0.5253895405058943
iteration : 1177
train acc:  0.8125
train loss:  0.44957423210144043
train gradient:  0.6246222265936461
iteration : 1178
train acc:  0.8359375
train loss:  0.42759469151496887
train gradient:  0.4297563963001871
iteration : 1179
train acc:  0.796875
train loss:  0.42493438720703125
train gradient:  0.37413587952114
iteration : 1180
train acc:  0.7890625
train loss:  0.4543968737125397
train gradient:  0.38376929952572253
iteration : 1181
train acc:  0.75
train loss:  0.4950482249259949
train gradient:  0.5865442517928131
iteration : 1182
train acc:  0.8046875
train loss:  0.4483393430709839
train gradient:  0.6347607724321878
iteration : 1183
train acc:  0.796875
train loss:  0.4521823525428772
train gradient:  0.5003423985402049
iteration : 1184
train acc:  0.7890625
train loss:  0.49605920910835266
train gradient:  0.5345373834609859
iteration : 1185
train acc:  0.7421875
train loss:  0.5660046339035034
train gradient:  0.8079798020300804
iteration : 1186
train acc:  0.8046875
train loss:  0.447100430727005
train gradient:  0.4841534748516549
iteration : 1187
train acc:  0.8046875
train loss:  0.4990633428096771
train gradient:  0.5246062189425973
iteration : 1188
train acc:  0.8046875
train loss:  0.41846704483032227
train gradient:  0.3671181623231197
iteration : 1189
train acc:  0.7578125
train loss:  0.45969292521476746
train gradient:  0.4362853439779513
iteration : 1190
train acc:  0.7734375
train loss:  0.44251662492752075
train gradient:  0.42233073463720927
iteration : 1191
train acc:  0.7890625
train loss:  0.4191552400588989
train gradient:  0.4232054141203878
iteration : 1192
train acc:  0.765625
train loss:  0.47751158475875854
train gradient:  0.38479371333657086
iteration : 1193
train acc:  0.875
train loss:  0.37286558747291565
train gradient:  0.292925290010398
iteration : 1194
train acc:  0.78125
train loss:  0.43868374824523926
train gradient:  0.2980796768040865
iteration : 1195
train acc:  0.8046875
train loss:  0.4858300983905792
train gradient:  0.5057455604379864
iteration : 1196
train acc:  0.875
train loss:  0.39390015602111816
train gradient:  0.34361361837175763
iteration : 1197
train acc:  0.7890625
train loss:  0.43182915449142456
train gradient:  0.4662414342091087
iteration : 1198
train acc:  0.8125
train loss:  0.4250227212905884
train gradient:  0.35043850945273425
iteration : 1199
train acc:  0.7734375
train loss:  0.5102630853652954
train gradient:  0.5536143532164216
iteration : 1200
train acc:  0.7890625
train loss:  0.43964776396751404
train gradient:  0.49460960350506006
iteration : 1201
train acc:  0.828125
train loss:  0.4368526041507721
train gradient:  0.4705282667068514
iteration : 1202
train acc:  0.828125
train loss:  0.4092281758785248
train gradient:  0.5367293680935998
iteration : 1203
train acc:  0.7578125
train loss:  0.49597007036209106
train gradient:  0.515348179506989
iteration : 1204
train acc:  0.828125
train loss:  0.41448670625686646
train gradient:  0.3892401461273324
iteration : 1205
train acc:  0.8125
train loss:  0.37807345390319824
train gradient:  0.3378856610274115
iteration : 1206
train acc:  0.8203125
train loss:  0.42466267943382263
train gradient:  0.44684246952531975
iteration : 1207
train acc:  0.734375
train loss:  0.4588404893875122
train gradient:  0.36812069303125455
iteration : 1208
train acc:  0.8125
train loss:  0.49403661489486694
train gradient:  0.4229551716066874
iteration : 1209
train acc:  0.828125
train loss:  0.41759538650512695
train gradient:  0.4192405203887933
iteration : 1210
train acc:  0.8359375
train loss:  0.37551483511924744
train gradient:  0.428633934278949
iteration : 1211
train acc:  0.7890625
train loss:  0.4706018269062042
train gradient:  0.5879255569730146
iteration : 1212
train acc:  0.7421875
train loss:  0.5142311453819275
train gradient:  0.5174881073543731
iteration : 1213
train acc:  0.78125
train loss:  0.43304502964019775
train gradient:  0.5976253836883196
iteration : 1214
train acc:  0.7890625
train loss:  0.4638262391090393
train gradient:  0.5235930124461984
iteration : 1215
train acc:  0.8046875
train loss:  0.44085919857025146
train gradient:  0.5050733696483647
iteration : 1216
train acc:  0.7421875
train loss:  0.4980459213256836
train gradient:  0.5760507412148402
iteration : 1217
train acc:  0.828125
train loss:  0.4481770992279053
train gradient:  0.4380849543861145
iteration : 1218
train acc:  0.765625
train loss:  0.48801833391189575
train gradient:  0.5987806312101477
iteration : 1219
train acc:  0.78125
train loss:  0.4529706835746765
train gradient:  0.6344507705099658
iteration : 1220
train acc:  0.8203125
train loss:  0.431416392326355
train gradient:  0.47917993056309227
iteration : 1221
train acc:  0.828125
train loss:  0.3930724263191223
train gradient:  0.43001411808937823
iteration : 1222
train acc:  0.7734375
train loss:  0.4864630699157715
train gradient:  0.4813479411658421
iteration : 1223
train acc:  0.8203125
train loss:  0.43157967925071716
train gradient:  0.47178104860583064
iteration : 1224
train acc:  0.84375
train loss:  0.4004670977592468
train gradient:  0.4450547386238025
iteration : 1225
train acc:  0.78125
train loss:  0.48683032393455505
train gradient:  0.6301746410997335
iteration : 1226
train acc:  0.78125
train loss:  0.3725351095199585
train gradient:  0.3716234806083716
iteration : 1227
train acc:  0.8359375
train loss:  0.36480116844177246
train gradient:  0.3553852565377794
iteration : 1228
train acc:  0.796875
train loss:  0.3916045129299164
train gradient:  0.3112057038102156
iteration : 1229
train acc:  0.8515625
train loss:  0.3557088077068329
train gradient:  0.4079753205373925
iteration : 1230
train acc:  0.7890625
train loss:  0.47074511647224426
train gradient:  0.4042092288659261
iteration : 1231
train acc:  0.84375
train loss:  0.41134169697761536
train gradient:  0.26644300395477344
iteration : 1232
train acc:  0.875
train loss:  0.33718931674957275
train gradient:  0.3222965634419008
iteration : 1233
train acc:  0.7890625
train loss:  0.44074946641921997
train gradient:  0.42707704471561114
iteration : 1234
train acc:  0.7734375
train loss:  0.507506251335144
train gradient:  0.7206992124343026
iteration : 1235
train acc:  0.8125
train loss:  0.4089239835739136
train gradient:  0.5289883029274649
iteration : 1236
train acc:  0.828125
train loss:  0.4203319251537323
train gradient:  0.4047929355835272
iteration : 1237
train acc:  0.796875
train loss:  0.44675642251968384
train gradient:  0.43070007213381284
iteration : 1238
train acc:  0.8203125
train loss:  0.40584444999694824
train gradient:  0.4467334443588205
iteration : 1239
train acc:  0.8125
train loss:  0.46389949321746826
train gradient:  0.4907524938249615
iteration : 1240
train acc:  0.8046875
train loss:  0.43917518854141235
train gradient:  0.42241272763020965
iteration : 1241
train acc:  0.7890625
train loss:  0.4657725989818573
train gradient:  0.745521400674671
iteration : 1242
train acc:  0.7890625
train loss:  0.4529752731323242
train gradient:  0.3568324520531716
iteration : 1243
train acc:  0.828125
train loss:  0.40153610706329346
train gradient:  0.32466901267954973
iteration : 1244
train acc:  0.78125
train loss:  0.4425864815711975
train gradient:  0.38940654433361727
iteration : 1245
train acc:  0.7734375
train loss:  0.43700963258743286
train gradient:  0.630924713348486
iteration : 1246
train acc:  0.8046875
train loss:  0.45235076546669006
train gradient:  0.42605701841376314
iteration : 1247
train acc:  0.8046875
train loss:  0.4495047926902771
train gradient:  0.41123074993493186
iteration : 1248
train acc:  0.765625
train loss:  0.48823896050453186
train gradient:  0.5836641005428094
iteration : 1249
train acc:  0.828125
train loss:  0.45041579008102417
train gradient:  0.5077863238360779
iteration : 1250
train acc:  0.796875
train loss:  0.43176376819610596
train gradient:  0.5386835348264657
iteration : 1251
train acc:  0.828125
train loss:  0.3965379595756531
train gradient:  0.38606608717305296
iteration : 1252
train acc:  0.8359375
train loss:  0.42936158180236816
train gradient:  0.4151666937531524
iteration : 1253
train acc:  0.796875
train loss:  0.4387882947921753
train gradient:  0.5121368370092796
iteration : 1254
train acc:  0.8046875
train loss:  0.41475218534469604
train gradient:  0.38128619143264
iteration : 1255
train acc:  0.7578125
train loss:  0.4250549376010895
train gradient:  0.38451464719443124
iteration : 1256
train acc:  0.796875
train loss:  0.43820300698280334
train gradient:  0.6148870873868125
iteration : 1257
train acc:  0.7734375
train loss:  0.4377449154853821
train gradient:  0.5732119090901081
iteration : 1258
train acc:  0.78125
train loss:  0.4812163710594177
train gradient:  0.52530279518475
iteration : 1259
train acc:  0.7890625
train loss:  0.3852422535419464
train gradient:  0.4280925317407408
iteration : 1260
train acc:  0.8046875
train loss:  0.4175111651420593
train gradient:  0.4297462897876106
iteration : 1261
train acc:  0.8046875
train loss:  0.36975425481796265
train gradient:  0.31877351181045865
iteration : 1262
train acc:  0.8046875
train loss:  0.4803333282470703
train gradient:  0.44055232743673817
iteration : 1263
train acc:  0.7734375
train loss:  0.46311652660369873
train gradient:  0.4707484381472038
iteration : 1264
train acc:  0.8515625
train loss:  0.39191198348999023
train gradient:  0.4753778297199685
iteration : 1265
train acc:  0.8359375
train loss:  0.3858758807182312
train gradient:  0.3286664576769953
iteration : 1266
train acc:  0.765625
train loss:  0.5190249681472778
train gradient:  0.5656141717846543
iteration : 1267
train acc:  0.828125
train loss:  0.38289815187454224
train gradient:  0.38720361825504995
iteration : 1268
train acc:  0.84375
train loss:  0.4118945002555847
train gradient:  0.4153290946745714
iteration : 1269
train acc:  0.8046875
train loss:  0.46758657693862915
train gradient:  0.7290342561912686
iteration : 1270
train acc:  0.828125
train loss:  0.39446914196014404
train gradient:  0.33325833281846984
iteration : 1271
train acc:  0.78125
train loss:  0.41305917501449585
train gradient:  0.47162031245737873
iteration : 1272
train acc:  0.8125
train loss:  0.4392305314540863
train gradient:  0.46965336479098857
iteration : 1273
train acc:  0.765625
train loss:  0.483052134513855
train gradient:  0.5084543322579955
iteration : 1274
train acc:  0.8671875
train loss:  0.3331446349620819
train gradient:  0.3573201159130817
iteration : 1275
train acc:  0.7890625
train loss:  0.45948055386543274
train gradient:  0.7158939174838956
iteration : 1276
train acc:  0.734375
train loss:  0.5522313117980957
train gradient:  0.7569469925783505
iteration : 1277
train acc:  0.796875
train loss:  0.46249526739120483
train gradient:  0.5132102356650164
iteration : 1278
train acc:  0.7578125
train loss:  0.48876088857650757
train gradient:  0.5530172438035373
iteration : 1279
train acc:  0.828125
train loss:  0.4130157232284546
train gradient:  0.48287972538995133
iteration : 1280
train acc:  0.78125
train loss:  0.41969361901283264
train gradient:  0.39653479698373795
iteration : 1281
train acc:  0.796875
train loss:  0.4135090708732605
train gradient:  0.4037993962842918
iteration : 1282
train acc:  0.7265625
train loss:  0.46382057666778564
train gradient:  0.5736298349513926
iteration : 1283
train acc:  0.75
train loss:  0.46545082330703735
train gradient:  0.45515475079067935
iteration : 1284
train acc:  0.7578125
train loss:  0.5023954510688782
train gradient:  0.6910689514662023
iteration : 1285
train acc:  0.7890625
train loss:  0.46694400906562805
train gradient:  0.5786615950553626
iteration : 1286
train acc:  0.8125
train loss:  0.41707083582878113
train gradient:  0.41193977482832045
iteration : 1287
train acc:  0.8203125
train loss:  0.44568800926208496
train gradient:  0.4893426643644064
iteration : 1288
train acc:  0.7578125
train loss:  0.47352272272109985
train gradient:  0.4643942415404773
iteration : 1289
train acc:  0.78125
train loss:  0.47608786821365356
train gradient:  0.6081821259709324
iteration : 1290
train acc:  0.78125
train loss:  0.4493880867958069
train gradient:  0.41432148401338814
iteration : 1291
train acc:  0.7890625
train loss:  0.47873610258102417
train gradient:  0.7787185012185696
iteration : 1292
train acc:  0.734375
train loss:  0.5099742412567139
train gradient:  0.5448782528802668
iteration : 1293
train acc:  0.7890625
train loss:  0.4701923131942749
train gradient:  0.4815556961533652
iteration : 1294
train acc:  0.796875
train loss:  0.40996819734573364
train gradient:  0.4186065026628489
iteration : 1295
train acc:  0.7890625
train loss:  0.4221614599227905
train gradient:  0.48741415744388855
iteration : 1296
train acc:  0.765625
train loss:  0.4316149353981018
train gradient:  0.4313810187972655
iteration : 1297
train acc:  0.734375
train loss:  0.5090045928955078
train gradient:  0.6481610399522506
iteration : 1298
train acc:  0.8203125
train loss:  0.40245598554611206
train gradient:  0.3967850391841431
iteration : 1299
train acc:  0.78125
train loss:  0.423249751329422
train gradient:  0.4913256341795007
iteration : 1300
train acc:  0.71875
train loss:  0.5111855268478394
train gradient:  0.5537659052325359
iteration : 1301
train acc:  0.765625
train loss:  0.4874982237815857
train gradient:  0.5318233253009237
iteration : 1302
train acc:  0.75
train loss:  0.4947686791419983
train gradient:  0.5605756373203787
iteration : 1303
train acc:  0.84375
train loss:  0.36326491832733154
train gradient:  0.3238006316637537
iteration : 1304
train acc:  0.8046875
train loss:  0.41265183687210083
train gradient:  0.43502523579265173
iteration : 1305
train acc:  0.7578125
train loss:  0.48392295837402344
train gradient:  0.5562370802634327
iteration : 1306
train acc:  0.7734375
train loss:  0.5023447871208191
train gradient:  0.6201524548095572
iteration : 1307
train acc:  0.7578125
train loss:  0.465081125497818
train gradient:  0.5343371217452404
iteration : 1308
train acc:  0.7734375
train loss:  0.48558542132377625
train gradient:  0.46694926339147813
iteration : 1309
train acc:  0.7890625
train loss:  0.5010346174240112
train gradient:  0.5012349722193368
iteration : 1310
train acc:  0.84375
train loss:  0.3804119825363159
train gradient:  0.36884289548674976
iteration : 1311
train acc:  0.796875
train loss:  0.4467688202857971
train gradient:  0.634712383912456
iteration : 1312
train acc:  0.78125
train loss:  0.48029473423957825
train gradient:  0.35712673291099833
iteration : 1313
train acc:  0.765625
train loss:  0.46045392751693726
train gradient:  0.5839497808173895
iteration : 1314
train acc:  0.828125
train loss:  0.4116571247577667
train gradient:  0.41129191097977397
iteration : 1315
train acc:  0.8046875
train loss:  0.46653005480766296
train gradient:  0.43199450580911825
iteration : 1316
train acc:  0.7421875
train loss:  0.4471607506275177
train gradient:  0.5674961885708615
iteration : 1317
train acc:  0.828125
train loss:  0.4025142192840576
train gradient:  0.40878472889340284
iteration : 1318
train acc:  0.8359375
train loss:  0.42553529143333435
train gradient:  0.532691728230835
iteration : 1319
train acc:  0.7578125
train loss:  0.5049422979354858
train gradient:  0.42075675124214573
iteration : 1320
train acc:  0.7578125
train loss:  0.49190449714660645
train gradient:  0.4987721944138695
iteration : 1321
train acc:  0.7578125
train loss:  0.478444367647171
train gradient:  0.6145531844123234
iteration : 1322
train acc:  0.84375
train loss:  0.4079371988773346
train gradient:  0.4492872831977481
iteration : 1323
train acc:  0.7890625
train loss:  0.38598594069480896
train gradient:  0.4836727424163289
iteration : 1324
train acc:  0.7734375
train loss:  0.4469393789768219
train gradient:  0.5092135334696273
iteration : 1325
train acc:  0.78125
train loss:  0.3985705077648163
train gradient:  0.42814880757306195
iteration : 1326
train acc:  0.78125
train loss:  0.4586021900177002
train gradient:  0.492760857556258
iteration : 1327
train acc:  0.8125
train loss:  0.43672865629196167
train gradient:  0.4116513025217287
iteration : 1328
train acc:  0.7578125
train loss:  0.4611220955848694
train gradient:  0.525651890943637
iteration : 1329
train acc:  0.765625
train loss:  0.4564288854598999
train gradient:  0.43106191118151127
iteration : 1330
train acc:  0.75
train loss:  0.45654594898223877
train gradient:  0.40161246402319145
iteration : 1331
train acc:  0.765625
train loss:  0.4633815884590149
train gradient:  0.42192157544532216
iteration : 1332
train acc:  0.8125
train loss:  0.4108981490135193
train gradient:  0.28121378254105506
iteration : 1333
train acc:  0.8046875
train loss:  0.4399249255657196
train gradient:  0.3310328859216493
iteration : 1334
train acc:  0.8203125
train loss:  0.46221041679382324
train gradient:  0.40484842022913764
iteration : 1335
train acc:  0.78125
train loss:  0.4759663939476013
train gradient:  0.5840997878358278
iteration : 1336
train acc:  0.7734375
train loss:  0.5041053295135498
train gradient:  0.4668055274272833
iteration : 1337
train acc:  0.765625
train loss:  0.45835205912590027
train gradient:  0.47578758573756236
iteration : 1338
train acc:  0.8046875
train loss:  0.4865990877151489
train gradient:  0.6367256580622379
iteration : 1339
train acc:  0.796875
train loss:  0.39886486530303955
train gradient:  0.312796235881995
iteration : 1340
train acc:  0.796875
train loss:  0.4105410873889923
train gradient:  0.336097479021911
iteration : 1341
train acc:  0.859375
train loss:  0.36870884895324707
train gradient:  0.37058428662638465
iteration : 1342
train acc:  0.765625
train loss:  0.4749510884284973
train gradient:  1.3370074744247187
iteration : 1343
train acc:  0.765625
train loss:  0.48401084542274475
train gradient:  0.3778753713821618
iteration : 1344
train acc:  0.8125
train loss:  0.3934631943702698
train gradient:  0.3590919576880289
iteration : 1345
train acc:  0.859375
train loss:  0.3943009376525879
train gradient:  0.3462786616162908
iteration : 1346
train acc:  0.8046875
train loss:  0.468150794506073
train gradient:  0.46295123369032404
iteration : 1347
train acc:  0.7265625
train loss:  0.49244359135627747
train gradient:  0.755422104559065
iteration : 1348
train acc:  0.765625
train loss:  0.44397643208503723
train gradient:  0.49284673144181385
iteration : 1349
train acc:  0.8125
train loss:  0.39840614795684814
train gradient:  0.36654454475356896
iteration : 1350
train acc:  0.8046875
train loss:  0.41218501329421997
train gradient:  0.37806359357631225
iteration : 1351
train acc:  0.8046875
train loss:  0.43487924337387085
train gradient:  0.4407750368557248
iteration : 1352
train acc:  0.78125
train loss:  0.42658889293670654
train gradient:  0.47321127769425747
iteration : 1353
train acc:  0.8359375
train loss:  0.386176735162735
train gradient:  0.36303059735168297
iteration : 1354
train acc:  0.8359375
train loss:  0.38401344418525696
train gradient:  0.37822712922010826
iteration : 1355
train acc:  0.8046875
train loss:  0.44204363226890564
train gradient:  0.45760459335497616
iteration : 1356
train acc:  0.8359375
train loss:  0.3544120788574219
train gradient:  0.4120465602735844
iteration : 1357
train acc:  0.84375
train loss:  0.3833693265914917
train gradient:  0.36312722944788206
iteration : 1358
train acc:  0.796875
train loss:  0.45385313034057617
train gradient:  0.5058009262177867
iteration : 1359
train acc:  0.78125
train loss:  0.4649193584918976
train gradient:  0.4359674679944302
iteration : 1360
train acc:  0.8125
train loss:  0.41549384593963623
train gradient:  0.43179504778508604
iteration : 1361
train acc:  0.8046875
train loss:  0.47753509879112244
train gradient:  0.635961310859616
iteration : 1362
train acc:  0.796875
train loss:  0.40773481130599976
train gradient:  0.35202964123372704
iteration : 1363
train acc:  0.828125
train loss:  0.4221467077732086
train gradient:  0.41896811138381657
iteration : 1364
train acc:  0.796875
train loss:  0.45064717531204224
train gradient:  0.5632113658140403
iteration : 1365
train acc:  0.7734375
train loss:  0.4852578043937683
train gradient:  0.5496133248463873
iteration : 1366
train acc:  0.8125
train loss:  0.4228881895542145
train gradient:  0.44656773421496165
iteration : 1367
train acc:  0.7890625
train loss:  0.49021241068840027
train gradient:  0.5728274067859963
iteration : 1368
train acc:  0.78125
train loss:  0.5086898803710938
train gradient:  0.5666517232427388
iteration : 1369
train acc:  0.8125
train loss:  0.4115597605705261
train gradient:  0.4532389201457678
iteration : 1370
train acc:  0.8125
train loss:  0.4034176468849182
train gradient:  0.48511523726130995
iteration : 1371
train acc:  0.765625
train loss:  0.4363389015197754
train gradient:  0.5216100113495781
iteration : 1372
train acc:  0.796875
train loss:  0.41722387075424194
train gradient:  0.5001849238264764
iteration : 1373
train acc:  0.765625
train loss:  0.4552918076515198
train gradient:  0.5863795888791957
iteration : 1374
train acc:  0.8203125
train loss:  0.4102422297000885
train gradient:  0.4102597564633804
iteration : 1375
train acc:  0.8125
train loss:  0.4957904517650604
train gradient:  0.4974596382760227
iteration : 1376
train acc:  0.765625
train loss:  0.45615649223327637
train gradient:  0.4492954801384686
iteration : 1377
train acc:  0.7734375
train loss:  0.48079270124435425
train gradient:  0.44492833853934177
iteration : 1378
train acc:  0.7578125
train loss:  0.4349524974822998
train gradient:  0.6456927460061881
iteration : 1379
train acc:  0.6875
train loss:  0.5597068071365356
train gradient:  0.8431473677617551
iteration : 1380
train acc:  0.8515625
train loss:  0.39294180274009705
train gradient:  0.39679466432449795
iteration : 1381
train acc:  0.75
train loss:  0.4891126751899719
train gradient:  0.6208424923867673
iteration : 1382
train acc:  0.828125
train loss:  0.3471461236476898
train gradient:  0.33875994231944045
iteration : 1383
train acc:  0.8046875
train loss:  0.4016786813735962
train gradient:  0.33228693002040377
iteration : 1384
train acc:  0.71875
train loss:  0.5729231834411621
train gradient:  0.6793322945452052
iteration : 1385
train acc:  0.765625
train loss:  0.46590155363082886
train gradient:  0.4643269257847709
iteration : 1386
train acc:  0.7109375
train loss:  0.537803053855896
train gradient:  0.6973671514346269
iteration : 1387
train acc:  0.84375
train loss:  0.399019718170166
train gradient:  0.36451906797472483
iteration : 1388
train acc:  0.828125
train loss:  0.45825856924057007
train gradient:  0.48199335003806776
iteration : 1389
train acc:  0.796875
train loss:  0.47605860233306885
train gradient:  0.5583161087459358
iteration : 1390
train acc:  0.7578125
train loss:  0.53743577003479
train gradient:  0.6983592758320237
iteration : 1391
train acc:  0.8359375
train loss:  0.40781477093696594
train gradient:  0.46635119883399423
iteration : 1392
train acc:  0.8515625
train loss:  0.3505905270576477
train gradient:  0.387435556503066
iteration : 1393
train acc:  0.7734375
train loss:  0.44119375944137573
train gradient:  0.3737949883817175
iteration : 1394
train acc:  0.8046875
train loss:  0.4141620993614197
train gradient:  0.4234023323847845
iteration : 1395
train acc:  0.8125
train loss:  0.4439724087715149
train gradient:  0.4427822578049861
iteration : 1396
train acc:  0.7109375
train loss:  0.6127883195877075
train gradient:  0.7535572486832572
iteration : 1397
train acc:  0.8359375
train loss:  0.39791011810302734
train gradient:  0.4378562570070144
iteration : 1398
train acc:  0.796875
train loss:  0.4574264883995056
train gradient:  0.5207412582522145
iteration : 1399
train acc:  0.765625
train loss:  0.5110176801681519
train gradient:  0.5885588535714373
iteration : 1400
train acc:  0.796875
train loss:  0.4408167004585266
train gradient:  0.32447412913105034
iteration : 1401
train acc:  0.7890625
train loss:  0.4349176585674286
train gradient:  0.4025944814512636
iteration : 1402
train acc:  0.828125
train loss:  0.3759732246398926
train gradient:  0.42872508447285856
iteration : 1403
train acc:  0.7109375
train loss:  0.554027259349823
train gradient:  0.7003504700764182
iteration : 1404
train acc:  0.7578125
train loss:  0.45667600631713867
train gradient:  0.4047346291176834
iteration : 1405
train acc:  0.75
train loss:  0.48754459619522095
train gradient:  0.5099292080984521
iteration : 1406
train acc:  0.8515625
train loss:  0.3798975348472595
train gradient:  0.3838092072650977
iteration : 1407
train acc:  0.8359375
train loss:  0.3787909746170044
train gradient:  0.29524310121626707
iteration : 1408
train acc:  0.7890625
train loss:  0.42502087354660034
train gradient:  0.3439756744366355
iteration : 1409
train acc:  0.796875
train loss:  0.43753939867019653
train gradient:  0.36729963909165647
iteration : 1410
train acc:  0.796875
train loss:  0.44786474108695984
train gradient:  0.5186793907222629
iteration : 1411
train acc:  0.75
train loss:  0.49818456172943115
train gradient:  0.5321418171293781
iteration : 1412
train acc:  0.8359375
train loss:  0.40097448229789734
train gradient:  0.3634791324662522
iteration : 1413
train acc:  0.8125
train loss:  0.4691852033138275
train gradient:  0.33130146774289
iteration : 1414
train acc:  0.7890625
train loss:  0.4507569670677185
train gradient:  0.4423690532632918
iteration : 1415
train acc:  0.75
train loss:  0.45872223377227783
train gradient:  0.4520922693452482
iteration : 1416
train acc:  0.8203125
train loss:  0.4131889045238495
train gradient:  0.5922784286104115
iteration : 1417
train acc:  0.7578125
train loss:  0.47759827971458435
train gradient:  0.5278779592830678
iteration : 1418
train acc:  0.78125
train loss:  0.4521949887275696
train gradient:  0.41983080037488313
iteration : 1419
train acc:  0.7734375
train loss:  0.44620847702026367
train gradient:  0.569277409984738
iteration : 1420
train acc:  0.71875
train loss:  0.5546953678131104
train gradient:  0.7808782575769642
iteration : 1421
train acc:  0.796875
train loss:  0.44706130027770996
train gradient:  0.38796842337480475
iteration : 1422
train acc:  0.828125
train loss:  0.4163886308670044
train gradient:  0.3828781587458923
iteration : 1423
train acc:  0.765625
train loss:  0.4739827513694763
train gradient:  0.48376515277181925
iteration : 1424
train acc:  0.75
train loss:  0.507017195224762
train gradient:  0.4168682753890829
iteration : 1425
train acc:  0.765625
train loss:  0.4423642158508301
train gradient:  0.3983424630834849
iteration : 1426
train acc:  0.7265625
train loss:  0.5543797612190247
train gradient:  0.617924287060265
iteration : 1427
train acc:  0.8359375
train loss:  0.3846919536590576
train gradient:  0.4150065665382254
iteration : 1428
train acc:  0.8359375
train loss:  0.4109979271888733
train gradient:  0.3361543214750567
iteration : 1429
train acc:  0.8515625
train loss:  0.37651222944259644
train gradient:  0.42634646430356415
iteration : 1430
train acc:  0.796875
train loss:  0.4316520392894745
train gradient:  0.3763653024787996
iteration : 1431
train acc:  0.8125
train loss:  0.3949594497680664
train gradient:  0.34303136780094523
iteration : 1432
train acc:  0.8671875
train loss:  0.3354610800743103
train gradient:  0.2672425544345412
iteration : 1433
train acc:  0.796875
train loss:  0.46086376905441284
train gradient:  0.523210159880906
iteration : 1434
train acc:  0.75
train loss:  0.4394124150276184
train gradient:  0.37034904001530067
iteration : 1435
train acc:  0.7734375
train loss:  0.47479403018951416
train gradient:  0.46777045218095564
iteration : 1436
train acc:  0.828125
train loss:  0.3928200602531433
train gradient:  0.403417281205142
iteration : 1437
train acc:  0.796875
train loss:  0.47774145007133484
train gradient:  0.5447659317660468
iteration : 1438
train acc:  0.796875
train loss:  0.4040815234184265
train gradient:  0.43024967729240493
iteration : 1439
train acc:  0.84375
train loss:  0.43006670475006104
train gradient:  0.5481432894834152
iteration : 1440
train acc:  0.84375
train loss:  0.3663713335990906
train gradient:  0.29121492135050375
iteration : 1441
train acc:  0.8125
train loss:  0.39089417457580566
train gradient:  0.33667406160673924
iteration : 1442
train acc:  0.84375
train loss:  0.38844776153564453
train gradient:  0.34407931965733096
iteration : 1443
train acc:  0.8203125
train loss:  0.4073522090911865
train gradient:  0.43711045072867466
iteration : 1444
train acc:  0.7421875
train loss:  0.4784732460975647
train gradient:  0.6048063377027292
iteration : 1445
train acc:  0.8359375
train loss:  0.43288958072662354
train gradient:  0.5009286112948753
iteration : 1446
train acc:  0.796875
train loss:  0.44985389709472656
train gradient:  0.47501831284870727
iteration : 1447
train acc:  0.8046875
train loss:  0.4543476700782776
train gradient:  0.6163607290001828
iteration : 1448
train acc:  0.7265625
train loss:  0.510925829410553
train gradient:  0.5629840284272902
iteration : 1449
train acc:  0.8125
train loss:  0.4205526113510132
train gradient:  0.42327324295850377
iteration : 1450
train acc:  0.8046875
train loss:  0.4253392219543457
train gradient:  0.4150388402700565
iteration : 1451
train acc:  0.8125
train loss:  0.3898649215698242
train gradient:  0.44944229782898754
iteration : 1452
train acc:  0.828125
train loss:  0.4150457978248596
train gradient:  0.5093281850881174
iteration : 1453
train acc:  0.8671875
train loss:  0.3259804844856262
train gradient:  0.2974166397351256
iteration : 1454
train acc:  0.7265625
train loss:  0.491687536239624
train gradient:  0.6879914354939747
iteration : 1455
train acc:  0.8203125
train loss:  0.36625462770462036
train gradient:  0.33797618806152974
iteration : 1456
train acc:  0.7890625
train loss:  0.431826114654541
train gradient:  0.6996533705593295
iteration : 1457
train acc:  0.875
train loss:  0.3700263202190399
train gradient:  0.4685241449971937
iteration : 1458
train acc:  0.8828125
train loss:  0.3873996436595917
train gradient:  0.42287923030219515
iteration : 1459
train acc:  0.765625
train loss:  0.5315332412719727
train gradient:  0.6472306215251941
iteration : 1460
train acc:  0.8203125
train loss:  0.40437641739845276
train gradient:  0.4198630735932573
iteration : 1461
train acc:  0.7265625
train loss:  0.5171380043029785
train gradient:  0.7270711335770543
iteration : 1462
train acc:  0.75
train loss:  0.45447802543640137
train gradient:  0.5441240854838225
iteration : 1463
train acc:  0.8828125
train loss:  0.337683767080307
train gradient:  0.4473548161879695
iteration : 1464
train acc:  0.8046875
train loss:  0.46567660570144653
train gradient:  0.6258849001164457
iteration : 1465
train acc:  0.796875
train loss:  0.44414958357810974
train gradient:  0.43274502225017025
iteration : 1466
train acc:  0.7734375
train loss:  0.4758748412132263
train gradient:  0.5804535066633956
iteration : 1467
train acc:  0.8125
train loss:  0.4326125383377075
train gradient:  0.49197717390897555
iteration : 1468
train acc:  0.75
train loss:  0.5193992853164673
train gradient:  0.6061833043032856
iteration : 1469
train acc:  0.765625
train loss:  0.43845081329345703
train gradient:  0.6147105396805653
iteration : 1470
train acc:  0.796875
train loss:  0.42481404542922974
train gradient:  0.4469345409916969
iteration : 1471
train acc:  0.7890625
train loss:  0.44572290778160095
train gradient:  0.42016316911719864
iteration : 1472
train acc:  0.84375
train loss:  0.36223146319389343
train gradient:  0.302422684913764
iteration : 1473
train acc:  0.8359375
train loss:  0.3877902626991272
train gradient:  0.44845476822864483
iteration : 1474
train acc:  0.828125
train loss:  0.39662837982177734
train gradient:  0.3185497933061436
iteration : 1475
train acc:  0.859375
train loss:  0.3828592300415039
train gradient:  0.40063558092838286
iteration : 1476
train acc:  0.765625
train loss:  0.4971792697906494
train gradient:  0.6927935938698665
iteration : 1477
train acc:  0.7890625
train loss:  0.4559791088104248
train gradient:  0.41599886374673056
iteration : 1478
train acc:  0.7421875
train loss:  0.5335580110549927
train gradient:  0.7709026475879874
iteration : 1479
train acc:  0.8203125
train loss:  0.3985345959663391
train gradient:  0.29350687845968404
iteration : 1480
train acc:  0.765625
train loss:  0.4866567552089691
train gradient:  0.47814955127655995
iteration : 1481
train acc:  0.796875
train loss:  0.41993552446365356
train gradient:  0.32320864899907203
iteration : 1482
train acc:  0.8125
train loss:  0.4400797486305237
train gradient:  0.3525334649557082
iteration : 1483
train acc:  0.7265625
train loss:  0.5478659868240356
train gradient:  0.48038237105806514
iteration : 1484
train acc:  0.734375
train loss:  0.4736877679824829
train gradient:  0.5565423277641395
iteration : 1485
train acc:  0.7890625
train loss:  0.40589427947998047
train gradient:  0.3422495312042837
iteration : 1486
train acc:  0.8359375
train loss:  0.41792720556259155
train gradient:  0.3989942482855488
iteration : 1487
train acc:  0.7265625
train loss:  0.5618160367012024
train gradient:  1.1300507271396225
iteration : 1488
train acc:  0.75
train loss:  0.5095689296722412
train gradient:  0.4869030595283598
iteration : 1489
train acc:  0.75
train loss:  0.4481498599052429
train gradient:  0.4204668976796302
iteration : 1490
train acc:  0.765625
train loss:  0.4633519947528839
train gradient:  0.4189280075695716
iteration : 1491
train acc:  0.7578125
train loss:  0.47929301857948303
train gradient:  0.4695570726247927
iteration : 1492
train acc:  0.8046875
train loss:  0.40850958228111267
train gradient:  0.5571409072852679
iteration : 1493
train acc:  0.8125
train loss:  0.4484879970550537
train gradient:  0.47481480433406575
iteration : 1494
train acc:  0.875
train loss:  0.3376295864582062
train gradient:  0.3073622674578231
iteration : 1495
train acc:  0.7734375
train loss:  0.4770858883857727
train gradient:  0.4137718838418542
iteration : 1496
train acc:  0.765625
train loss:  0.4703218638896942
train gradient:  0.36771919944063314
iteration : 1497
train acc:  0.7265625
train loss:  0.5112357139587402
train gradient:  0.45401968046808094
iteration : 1498
train acc:  0.796875
train loss:  0.4042471945285797
train gradient:  0.28180536326930633
iteration : 1499
train acc:  0.765625
train loss:  0.47232508659362793
train gradient:  0.41717915656498605
iteration : 1500
train acc:  0.8203125
train loss:  0.43036895990371704
train gradient:  0.33106671848316627
iteration : 1501
train acc:  0.796875
train loss:  0.4349712133407593
train gradient:  0.3135578033361405
iteration : 1502
train acc:  0.875
train loss:  0.31911712884902954
train gradient:  0.2496361305269856
iteration : 1503
train acc:  0.7734375
train loss:  0.473488986492157
train gradient:  0.39583115277098435
iteration : 1504
train acc:  0.8359375
train loss:  0.35751864314079285
train gradient:  0.24692514514739422
iteration : 1505
train acc:  0.7890625
train loss:  0.4383178949356079
train gradient:  0.42982501943268603
iteration : 1506
train acc:  0.78125
train loss:  0.44852402806282043
train gradient:  0.4635312792507371
iteration : 1507
train acc:  0.8046875
train loss:  0.43390148878097534
train gradient:  0.3800828345806744
iteration : 1508
train acc:  0.828125
train loss:  0.4018663763999939
train gradient:  0.32840453970163447
iteration : 1509
train acc:  0.7578125
train loss:  0.45915138721466064
train gradient:  0.39418966932997596
iteration : 1510
train acc:  0.8203125
train loss:  0.3798653781414032
train gradient:  0.25358637969161657
iteration : 1511
train acc:  0.7734375
train loss:  0.4152093529701233
train gradient:  0.3917217811998459
iteration : 1512
train acc:  0.796875
train loss:  0.38605913519859314
train gradient:  0.32637202177198915
iteration : 1513
train acc:  0.8359375
train loss:  0.4036685824394226
train gradient:  0.31504156388119947
iteration : 1514
train acc:  0.8046875
train loss:  0.4427972137928009
train gradient:  0.39541003144265696
iteration : 1515
train acc:  0.8046875
train loss:  0.4099050760269165
train gradient:  0.4187620214489797
iteration : 1516
train acc:  0.8046875
train loss:  0.4077757000923157
train gradient:  0.5067714687860815
iteration : 1517
train acc:  0.7890625
train loss:  0.40666472911834717
train gradient:  0.38396469255106846
iteration : 1518
train acc:  0.8203125
train loss:  0.43385422229766846
train gradient:  0.35133234315109313
iteration : 1519
train acc:  0.7890625
train loss:  0.4415225684642792
train gradient:  0.5020278353982561
iteration : 1520
train acc:  0.828125
train loss:  0.4343884587287903
train gradient:  0.37936692859312826
iteration : 1521
train acc:  0.8671875
train loss:  0.322920024394989
train gradient:  0.37462023105934766
iteration : 1522
train acc:  0.7265625
train loss:  0.5256623029708862
train gradient:  0.891130700709322
iteration : 1523
train acc:  0.8203125
train loss:  0.4399985671043396
train gradient:  0.41433661906073393
iteration : 1524
train acc:  0.8515625
train loss:  0.3755705654621124
train gradient:  0.36641601532830287
iteration : 1525
train acc:  0.7890625
train loss:  0.454774409532547
train gradient:  0.4832946224622181
iteration : 1526
train acc:  0.765625
train loss:  0.41231971979141235
train gradient:  0.3496332431611217
iteration : 1527
train acc:  0.828125
train loss:  0.3429819345474243
train gradient:  0.27180668843622685
iteration : 1528
train acc:  0.875
train loss:  0.34406936168670654
train gradient:  0.2381108811136351
iteration : 1529
train acc:  0.8203125
train loss:  0.33869844675064087
train gradient:  0.3694449019891895
iteration : 1530
train acc:  0.796875
train loss:  0.4017004668712616
train gradient:  0.3513956416962969
iteration : 1531
train acc:  0.7578125
train loss:  0.44104835391044617
train gradient:  0.4941939441617553
iteration : 1532
train acc:  0.8203125
train loss:  0.43155887722969055
train gradient:  0.6221668535685245
iteration : 1533
train acc:  0.8125
train loss:  0.4264698028564453
train gradient:  0.3819843177475652
iteration : 1534
train acc:  0.8515625
train loss:  0.38328802585601807
train gradient:  0.3996435004057461
iteration : 1535
train acc:  0.84375
train loss:  0.37902647256851196
train gradient:  0.3899456691885672
iteration : 1536
train acc:  0.78125
train loss:  0.450944721698761
train gradient:  0.43491293905224837
iteration : 1537
train acc:  0.7890625
train loss:  0.45082029700279236
train gradient:  0.4474074578894258
iteration : 1538
train acc:  0.8203125
train loss:  0.46350929141044617
train gradient:  0.8038604184460914
iteration : 1539
train acc:  0.8203125
train loss:  0.39524269104003906
train gradient:  0.38750580973635196
iteration : 1540
train acc:  0.859375
train loss:  0.312336266040802
train gradient:  0.3314551053607594
iteration : 1541
train acc:  0.8125
train loss:  0.40707603096961975
train gradient:  0.4335548829941355
iteration : 1542
train acc:  0.8046875
train loss:  0.4108794927597046
train gradient:  0.3969557558174971
iteration : 1543
train acc:  0.8203125
train loss:  0.4028400778770447
train gradient:  0.4273298563579992
iteration : 1544
train acc:  0.8359375
train loss:  0.43304958939552307
train gradient:  0.3953643034223762
iteration : 1545
train acc:  0.7578125
train loss:  0.42228734493255615
train gradient:  0.5295548118497168
iteration : 1546
train acc:  0.7890625
train loss:  0.4300052225589752
train gradient:  0.45944756041555734
iteration : 1547
train acc:  0.8046875
train loss:  0.4268333911895752
train gradient:  0.4218061743724992
iteration : 1548
train acc:  0.828125
train loss:  0.40328696370124817
train gradient:  0.6372692238727259
iteration : 1549
train acc:  0.8203125
train loss:  0.40314456820487976
train gradient:  0.398748354834327
iteration : 1550
train acc:  0.859375
train loss:  0.3563191890716553
train gradient:  0.4189704593119497
iteration : 1551
train acc:  0.7421875
train loss:  0.5548185110092163
train gradient:  1.1955125571043568
iteration : 1552
train acc:  0.84375
train loss:  0.40284696221351624
train gradient:  0.36252608044560575
iteration : 1553
train acc:  0.7890625
train loss:  0.4494304656982422
train gradient:  0.4420042814076855
iteration : 1554
train acc:  0.8515625
train loss:  0.3364730477333069
train gradient:  0.40847393849111985
iteration : 1555
train acc:  0.8203125
train loss:  0.4100531339645386
train gradient:  0.4513318303234971
iteration : 1556
train acc:  0.8203125
train loss:  0.4232023358345032
train gradient:  0.568433230241478
iteration : 1557
train acc:  0.7890625
train loss:  0.4372079372406006
train gradient:  0.3773172032064161
iteration : 1558
train acc:  0.7890625
train loss:  0.41902869939804077
train gradient:  0.5051911030729951
iteration : 1559
train acc:  0.859375
train loss:  0.3677625060081482
train gradient:  0.32018637121253163
iteration : 1560
train acc:  0.75
train loss:  0.4718632698059082
train gradient:  0.551778383948367
iteration : 1561
train acc:  0.8046875
train loss:  0.4267842173576355
train gradient:  0.45691672376853776
iteration : 1562
train acc:  0.8125
train loss:  0.38994768261909485
train gradient:  0.3499265776173581
iteration : 1563
train acc:  0.8359375
train loss:  0.39349669218063354
train gradient:  0.42755503671389455
iteration : 1564
train acc:  0.7734375
train loss:  0.4060104489326477
train gradient:  0.5446857825987738
iteration : 1565
train acc:  0.7578125
train loss:  0.5432908535003662
train gradient:  0.8125314619335742
iteration : 1566
train acc:  0.7421875
train loss:  0.5215019583702087
train gradient:  0.7838191136536885
iteration : 1567
train acc:  0.8125
train loss:  0.3746246099472046
train gradient:  0.41235052453386695
iteration : 1568
train acc:  0.75
train loss:  0.524479329586029
train gradient:  0.8044980478602325
iteration : 1569
train acc:  0.8671875
train loss:  0.34196728467941284
train gradient:  0.42409801131140884
iteration : 1570
train acc:  0.8125
train loss:  0.41949325799942017
train gradient:  0.45666887998715805
iteration : 1571
train acc:  0.796875
train loss:  0.4381004273891449
train gradient:  0.5315181434868909
iteration : 1572
train acc:  0.7890625
train loss:  0.44229573011398315
train gradient:  0.44913120765331055
iteration : 1573
train acc:  0.78125
train loss:  0.41969820857048035
train gradient:  0.41658631717097905
iteration : 1574
train acc:  0.78125
train loss:  0.47040319442749023
train gradient:  0.4572237947176208
iteration : 1575
train acc:  0.859375
train loss:  0.3616423010826111
train gradient:  0.4215295906879923
iteration : 1576
train acc:  0.8125
train loss:  0.43606314063072205
train gradient:  0.5793063912143013
iteration : 1577
train acc:  0.7734375
train loss:  0.503383457660675
train gradient:  0.7589494564129854
iteration : 1578
train acc:  0.8046875
train loss:  0.37645095586776733
train gradient:  0.35903899048946275
iteration : 1579
train acc:  0.8203125
train loss:  0.36018073558807373
train gradient:  0.37032852008028183
iteration : 1580
train acc:  0.7890625
train loss:  0.47276240587234497
train gradient:  0.4953664382057101
iteration : 1581
train acc:  0.8203125
train loss:  0.4055873155593872
train gradient:  0.471774905090668
iteration : 1582
train acc:  0.84375
train loss:  0.39972472190856934
train gradient:  0.4642008512619824
iteration : 1583
train acc:  0.8359375
train loss:  0.4375286400318146
train gradient:  0.49196452605083146
iteration : 1584
train acc:  0.7734375
train loss:  0.512115478515625
train gradient:  0.5114003974156626
iteration : 1585
train acc:  0.8359375
train loss:  0.44649195671081543
train gradient:  0.5541653170912741
iteration : 1586
train acc:  0.7890625
train loss:  0.43894273042678833
train gradient:  0.5057843633998432
iteration : 1587
train acc:  0.84375
train loss:  0.3994113802909851
train gradient:  0.39184315982376894
iteration : 1588
train acc:  0.828125
train loss:  0.4193666875362396
train gradient:  0.37326147425619755
iteration : 1589
train acc:  0.8203125
train loss:  0.3636767864227295
train gradient:  0.3480470760005491
iteration : 1590
train acc:  0.8046875
train loss:  0.42297878861427307
train gradient:  0.45868664652024077
iteration : 1591
train acc:  0.7734375
train loss:  0.45464739203453064
train gradient:  0.47118434740656406
iteration : 1592
train acc:  0.8359375
train loss:  0.3674052655696869
train gradient:  0.40422756236832585
iteration : 1593
train acc:  0.7578125
train loss:  0.46787357330322266
train gradient:  0.3967710126479984
iteration : 1594
train acc:  0.7578125
train loss:  0.4443119764328003
train gradient:  0.5246816278913876
iteration : 1595
train acc:  0.8125
train loss:  0.42745015025138855
train gradient:  0.34303233279227047
iteration : 1596
train acc:  0.8515625
train loss:  0.38254737854003906
train gradient:  0.3894516708343682
iteration : 1597
train acc:  0.828125
train loss:  0.4119158983230591
train gradient:  0.3215920418382796
iteration : 1598
train acc:  0.78125
train loss:  0.4137484133243561
train gradient:  0.34491026699432015
iteration : 1599
train acc:  0.8359375
train loss:  0.43866097927093506
train gradient:  0.46116888218067537
iteration : 1600
train acc:  0.875
train loss:  0.3356005549430847
train gradient:  0.34285567686009216
iteration : 1601
train acc:  0.796875
train loss:  0.4480278491973877
train gradient:  0.5890060841312688
iteration : 1602
train acc:  0.828125
train loss:  0.3727760910987854
train gradient:  0.32662098413423735
iteration : 1603
train acc:  0.7578125
train loss:  0.3986857831478119
train gradient:  0.34480483993025723
iteration : 1604
train acc:  0.8046875
train loss:  0.47060859203338623
train gradient:  0.5001814877081696
iteration : 1605
train acc:  0.828125
train loss:  0.3730788230895996
train gradient:  0.41785013774732044
iteration : 1606
train acc:  0.8203125
train loss:  0.40096497535705566
train gradient:  0.3696576880141846
iteration : 1607
train acc:  0.75
train loss:  0.5524530410766602
train gradient:  0.6437237947431053
iteration : 1608
train acc:  0.828125
train loss:  0.43100985884666443
train gradient:  0.5097845412724791
iteration : 1609
train acc:  0.7265625
train loss:  0.5066179037094116
train gradient:  0.4672625940300391
iteration : 1610
train acc:  0.796875
train loss:  0.3761538565158844
train gradient:  0.5346005060037435
iteration : 1611
train acc:  0.796875
train loss:  0.41911780834198
train gradient:  0.36386350729681116
iteration : 1612
train acc:  0.8046875
train loss:  0.4288102388381958
train gradient:  0.6282265904357366
iteration : 1613
train acc:  0.7734375
train loss:  0.47037336230278015
train gradient:  0.5511052115586413
iteration : 1614
train acc:  0.8125
train loss:  0.4083130359649658
train gradient:  0.44081676232428907
iteration : 1615
train acc:  0.796875
train loss:  0.5249295830726624
train gradient:  0.5726006606739904
iteration : 1616
train acc:  0.7890625
train loss:  0.39480191469192505
train gradient:  0.41559759280375824
iteration : 1617
train acc:  0.7890625
train loss:  0.430298388004303
train gradient:  0.5025648577056058
iteration : 1618
train acc:  0.8203125
train loss:  0.39362430572509766
train gradient:  0.3744460961259274
iteration : 1619
train acc:  0.8125
train loss:  0.4461266100406647
train gradient:  0.6484116112741386
iteration : 1620
train acc:  0.7421875
train loss:  0.49428635835647583
train gradient:  0.5457295153943805
iteration : 1621
train acc:  0.7578125
train loss:  0.4545760154724121
train gradient:  0.42150776088536007
iteration : 1622
train acc:  0.8046875
train loss:  0.45651090145111084
train gradient:  0.5780398896302569
iteration : 1623
train acc:  0.796875
train loss:  0.4146048128604889
train gradient:  0.5445679332704666
iteration : 1624
train acc:  0.8203125
train loss:  0.3992273509502411
train gradient:  0.4048490390913137
iteration : 1625
train acc:  0.75
train loss:  0.48552435636520386
train gradient:  0.4480334162349824
iteration : 1626
train acc:  0.8359375
train loss:  0.36276498436927795
train gradient:  0.3301787957946746
iteration : 1627
train acc:  0.7890625
train loss:  0.49184858798980713
train gradient:  0.4036922401098309
iteration : 1628
train acc:  0.890625
train loss:  0.31725144386291504
train gradient:  0.29396256236424323
iteration : 1629
train acc:  0.7734375
train loss:  0.4185291528701782
train gradient:  0.3693579987278318
iteration : 1630
train acc:  0.7265625
train loss:  0.5525282621383667
train gradient:  0.6699246487142391
iteration : 1631
train acc:  0.7890625
train loss:  0.4413314759731293
train gradient:  0.36363085417982105
iteration : 1632
train acc:  0.8046875
train loss:  0.4177027940750122
train gradient:  0.45831667800286974
iteration : 1633
train acc:  0.7734375
train loss:  0.48211342096328735
train gradient:  0.43838618655998524
iteration : 1634
train acc:  0.7890625
train loss:  0.4164316654205322
train gradient:  0.45097371660906904
iteration : 1635
train acc:  0.7734375
train loss:  0.48504000902175903
train gradient:  1.537157683306279
iteration : 1636
train acc:  0.8046875
train loss:  0.4132124185562134
train gradient:  0.48953445013364805
iteration : 1637
train acc:  0.765625
train loss:  0.47200170159339905
train gradient:  0.3754747691004584
iteration : 1638
train acc:  0.859375
train loss:  0.3816039562225342
train gradient:  0.43243231054560766
iteration : 1639
train acc:  0.75
train loss:  0.488362193107605
train gradient:  0.4848540675597734
iteration : 1640
train acc:  0.7421875
train loss:  0.5487762689590454
train gradient:  0.6917926172246054
iteration : 1641
train acc:  0.8515625
train loss:  0.3875875473022461
train gradient:  0.27767768822537153
iteration : 1642
train acc:  0.84375
train loss:  0.3885509967803955
train gradient:  0.438034833404666
iteration : 1643
train acc:  0.8125
train loss:  0.4441947340965271
train gradient:  0.38806521619482326
iteration : 1644
train acc:  0.78125
train loss:  0.5094517469406128
train gradient:  0.604992663907204
iteration : 1645
train acc:  0.8125
train loss:  0.40314847230911255
train gradient:  0.371475766697957
iteration : 1646
train acc:  0.828125
train loss:  0.37309184670448303
train gradient:  0.3020003251737087
iteration : 1647
train acc:  0.7734375
train loss:  0.46952009201049805
train gradient:  0.4774892880802087
iteration : 1648
train acc:  0.8203125
train loss:  0.36685723066329956
train gradient:  0.25818335544381743
iteration : 1649
train acc:  0.7890625
train loss:  0.4555644989013672
train gradient:  0.41258520193775094
iteration : 1650
train acc:  0.8359375
train loss:  0.36325860023498535
train gradient:  0.2714457906444916
iteration : 1651
train acc:  0.8125
train loss:  0.40843427181243896
train gradient:  0.3882230523464501
iteration : 1652
train acc:  0.8046875
train loss:  0.45612600445747375
train gradient:  0.35340191236792895
iteration : 1653
train acc:  0.7578125
train loss:  0.44199031591415405
train gradient:  0.38320468492894727
iteration : 1654
train acc:  0.8203125
train loss:  0.4564611613750458
train gradient:  0.4976385097071749
iteration : 1655
train acc:  0.8515625
train loss:  0.40549546480178833
train gradient:  0.33573635986330796
iteration : 1656
train acc:  0.8046875
train loss:  0.4401596486568451
train gradient:  0.42467199532681793
iteration : 1657
train acc:  0.765625
train loss:  0.4384889602661133
train gradient:  0.40091147465650945
iteration : 1658
train acc:  0.8046875
train loss:  0.4157373905181885
train gradient:  0.3618571725497889
iteration : 1659
train acc:  0.7578125
train loss:  0.4664439857006073
train gradient:  0.4915297007207997
iteration : 1660
train acc:  0.796875
train loss:  0.45286720991134644
train gradient:  0.4041493629704674
iteration : 1661
train acc:  0.8046875
train loss:  0.4085667133331299
train gradient:  0.3234662108047256
iteration : 1662
train acc:  0.7890625
train loss:  0.4493405818939209
train gradient:  0.3582835254292161
iteration : 1663
train acc:  0.8828125
train loss:  0.38354766368865967
train gradient:  0.30121660024642616
iteration : 1664
train acc:  0.765625
train loss:  0.5172138214111328
train gradient:  0.527995057542858
iteration : 1665
train acc:  0.78125
train loss:  0.4512994885444641
train gradient:  0.4717924419865985
iteration : 1666
train acc:  0.78125
train loss:  0.4698438048362732
train gradient:  0.5000101320035915
iteration : 1667
train acc:  0.828125
train loss:  0.37925758957862854
train gradient:  0.36762884836648646
iteration : 1668
train acc:  0.84375
train loss:  0.3719760477542877
train gradient:  0.3144692702737291
iteration : 1669
train acc:  0.7734375
train loss:  0.48729681968688965
train gradient:  0.522495626287625
iteration : 1670
train acc:  0.8125
train loss:  0.43446797132492065
train gradient:  0.38260788245848953
iteration : 1671
train acc:  0.8046875
train loss:  0.4065132439136505
train gradient:  0.3114252753020509
iteration : 1672
train acc:  0.765625
train loss:  0.46223530173301697
train gradient:  0.36833399246217047
iteration : 1673
train acc:  0.8125
train loss:  0.400437593460083
train gradient:  0.3852704122608493
iteration : 1674
train acc:  0.8125
train loss:  0.4365348219871521
train gradient:  0.2929520831926134
iteration : 1675
train acc:  0.78125
train loss:  0.46190977096557617
train gradient:  0.6709525727391099
iteration : 1676
train acc:  0.75
train loss:  0.4722762703895569
train gradient:  0.6101035574910088
iteration : 1677
train acc:  0.8203125
train loss:  0.3928239643573761
train gradient:  0.406722886959567
iteration : 1678
train acc:  0.71875
train loss:  0.46907031536102295
train gradient:  0.43247238655074816
iteration : 1679
train acc:  0.7734375
train loss:  0.3898199498653412
train gradient:  0.3683301593876577
iteration : 1680
train acc:  0.8125
train loss:  0.40836644172668457
train gradient:  0.3578411921637703
iteration : 1681
train acc:  0.8359375
train loss:  0.440834105014801
train gradient:  0.38126083093008595
iteration : 1682
train acc:  0.8515625
train loss:  0.36295294761657715
train gradient:  0.3841288596934958
iteration : 1683
train acc:  0.828125
train loss:  0.39067545533180237
train gradient:  0.46729525570987046
iteration : 1684
train acc:  0.734375
train loss:  0.4793071150779724
train gradient:  0.6665269598081798
iteration : 1685
train acc:  0.7890625
train loss:  0.42641881108283997
train gradient:  0.35062506125126874
iteration : 1686
train acc:  0.828125
train loss:  0.4635589122772217
train gradient:  0.4122490847563857
iteration : 1687
train acc:  0.8359375
train loss:  0.4552658200263977
train gradient:  0.4910452467537483
iteration : 1688
train acc:  0.75
train loss:  0.4946666359901428
train gradient:  0.5827882090562722
iteration : 1689
train acc:  0.734375
train loss:  0.5092740058898926
train gradient:  0.6458503495837787
iteration : 1690
train acc:  0.84375
train loss:  0.4112367630004883
train gradient:  0.31955785288905186
iteration : 1691
train acc:  0.796875
train loss:  0.4278411865234375
train gradient:  0.49722188300370485
iteration : 1692
train acc:  0.8203125
train loss:  0.3754523694515228
train gradient:  0.41689045866453556
iteration : 1693
train acc:  0.7890625
train loss:  0.43846237659454346
train gradient:  0.41515836220887553
iteration : 1694
train acc:  0.8515625
train loss:  0.3491416573524475
train gradient:  0.27800388782327384
iteration : 1695
train acc:  0.765625
train loss:  0.43219250440597534
train gradient:  0.4657130622686407
iteration : 1696
train acc:  0.8125
train loss:  0.4169067442417145
train gradient:  0.3696697729297153
iteration : 1697
train acc:  0.78125
train loss:  0.4301745891571045
train gradient:  0.4153941901735018
iteration : 1698
train acc:  0.859375
train loss:  0.3406528830528259
train gradient:  0.21203497458847553
iteration : 1699
train acc:  0.7890625
train loss:  0.4235913157463074
train gradient:  0.5544031086666504
iteration : 1700
train acc:  0.8125
train loss:  0.40622806549072266
train gradient:  0.37380283126659036
iteration : 1701
train acc:  0.7734375
train loss:  0.4525611996650696
train gradient:  0.40059299620089917
iteration : 1702
train acc:  0.8203125
train loss:  0.3579905331134796
train gradient:  0.3095853536980243
iteration : 1703
train acc:  0.8828125
train loss:  0.3331989645957947
train gradient:  0.3321228480658905
iteration : 1704
train acc:  0.734375
train loss:  0.4993029534816742
train gradient:  0.6359201965252504
iteration : 1705
train acc:  0.8671875
train loss:  0.36763086915016174
train gradient:  0.3669415431407512
iteration : 1706
train acc:  0.8515625
train loss:  0.4282604455947876
train gradient:  0.4646946216330863
iteration : 1707
train acc:  0.8046875
train loss:  0.43975692987442017
train gradient:  0.4087827724992578
iteration : 1708
train acc:  0.7890625
train loss:  0.4799773097038269
train gradient:  0.4949315776269997
iteration : 1709
train acc:  0.7734375
train loss:  0.46896517276763916
train gradient:  0.47704591860031115
iteration : 1710
train acc:  0.75
train loss:  0.47051167488098145
train gradient:  0.4649682709696175
iteration : 1711
train acc:  0.765625
train loss:  0.44906482100486755
train gradient:  0.5031399975809213
iteration : 1712
train acc:  0.859375
train loss:  0.3563210368156433
train gradient:  0.319069215349947
iteration : 1713
train acc:  0.7734375
train loss:  0.41527247428894043
train gradient:  0.4098468613370135
iteration : 1714
train acc:  0.8359375
train loss:  0.40259626507759094
train gradient:  0.5155908908766278
iteration : 1715
train acc:  0.8125
train loss:  0.37900304794311523
train gradient:  0.41017185313467874
iteration : 1716
train acc:  0.78125
train loss:  0.48736298084259033
train gradient:  0.6841514676550123
iteration : 1717
train acc:  0.796875
train loss:  0.4142725467681885
train gradient:  0.4570070867285402
iteration : 1718
train acc:  0.75
train loss:  0.49482548236846924
train gradient:  0.5291394643023846
iteration : 1719
train acc:  0.8125
train loss:  0.3656288981437683
train gradient:  0.44076748789856696
iteration : 1720
train acc:  0.7734375
train loss:  0.44818150997161865
train gradient:  0.4964806192101005
iteration : 1721
train acc:  0.8046875
train loss:  0.4799439311027527
train gradient:  0.6083879736885107
iteration : 1722
train acc:  0.7890625
train loss:  0.44695836305618286
train gradient:  0.4495295720515555
iteration : 1723
train acc:  0.8046875
train loss:  0.466244101524353
train gradient:  0.47146698432606043
iteration : 1724
train acc:  0.7734375
train loss:  0.43580007553100586
train gradient:  0.45828345839753976
iteration : 1725
train acc:  0.8046875
train loss:  0.3893755078315735
train gradient:  0.4085470069894735
iteration : 1726
train acc:  0.8203125
train loss:  0.4049900770187378
train gradient:  0.37704232529647663
iteration : 1727
train acc:  0.875
train loss:  0.33844825625419617
train gradient:  0.26518521859136474
iteration : 1728
train acc:  0.8203125
train loss:  0.401084840297699
train gradient:  0.45253355306100385
iteration : 1729
train acc:  0.8203125
train loss:  0.38060086965560913
train gradient:  0.40078287145586194
iteration : 1730
train acc:  0.78125
train loss:  0.47124820947647095
train gradient:  0.541564539940329
iteration : 1731
train acc:  0.78125
train loss:  0.41579297184944153
train gradient:  0.46373908544297476
iteration : 1732
train acc:  0.859375
train loss:  0.36833006143569946
train gradient:  0.2986958802486341
iteration : 1733
train acc:  0.8046875
train loss:  0.37827056646347046
train gradient:  0.35838102551619644
iteration : 1734
train acc:  0.8125
train loss:  0.39465218782424927
train gradient:  0.3853565519506162
iteration : 1735
train acc:  0.7265625
train loss:  0.5113067626953125
train gradient:  0.5613679331770327
iteration : 1736
train acc:  0.875
train loss:  0.3527567982673645
train gradient:  0.35426607824579487
iteration : 1737
train acc:  0.765625
train loss:  0.4789656400680542
train gradient:  0.5256899639331425
iteration : 1738
train acc:  0.828125
train loss:  0.35129398107528687
train gradient:  0.2839752371231278
iteration : 1739
train acc:  0.734375
train loss:  0.44091829657554626
train gradient:  0.44914255131878267
iteration : 1740
train acc:  0.8125
train loss:  0.3899935781955719
train gradient:  0.3822468448966689
iteration : 1741
train acc:  0.7890625
train loss:  0.5093720555305481
train gradient:  0.48382819998476284
iteration : 1742
train acc:  0.7890625
train loss:  0.4159787595272064
train gradient:  0.3354169374285831
iteration : 1743
train acc:  0.84375
train loss:  0.3717689514160156
train gradient:  0.2789251859657396
iteration : 1744
train acc:  0.9140625
train loss:  0.2770835757255554
train gradient:  0.20885322933963185
iteration : 1745
train acc:  0.703125
train loss:  0.5401602983474731
train gradient:  0.5952829271261784
iteration : 1746
train acc:  0.75
train loss:  0.4865202009677887
train gradient:  0.47453983566187186
iteration : 1747
train acc:  0.7421875
train loss:  0.5320678949356079
train gradient:  0.5676896594518068
iteration : 1748
train acc:  0.859375
train loss:  0.30826807022094727
train gradient:  0.2699858838092636
iteration : 1749
train acc:  0.8359375
train loss:  0.35759487748146057
train gradient:  0.3625251023051671
iteration : 1750
train acc:  0.8515625
train loss:  0.3600120544433594
train gradient:  0.33599597194567254
iteration : 1751
train acc:  0.796875
train loss:  0.39471304416656494
train gradient:  0.3584859073278566
iteration : 1752
train acc:  0.8125
train loss:  0.41103947162628174
train gradient:  0.43499424726254693
iteration : 1753
train acc:  0.7734375
train loss:  0.44945183396339417
train gradient:  0.46347583555532745
iteration : 1754
train acc:  0.7890625
train loss:  0.4328395128250122
train gradient:  0.5888553313374524
iteration : 1755
train acc:  0.8203125
train loss:  0.45299434661865234
train gradient:  0.5242343999372091
iteration : 1756
train acc:  0.765625
train loss:  0.49303528666496277
train gradient:  0.6827027615994247
iteration : 1757
train acc:  0.7578125
train loss:  0.4926914870738983
train gradient:  0.6076793562457775
iteration : 1758
train acc:  0.8203125
train loss:  0.41868799924850464
train gradient:  0.3586541109131219
iteration : 1759
train acc:  0.7890625
train loss:  0.471699982881546
train gradient:  0.6244543723383262
iteration : 1760
train acc:  0.765625
train loss:  0.42402228713035583
train gradient:  0.5138461724992328
iteration : 1761
train acc:  0.8515625
train loss:  0.36919164657592773
train gradient:  0.39873420001551346
iteration : 1762
train acc:  0.765625
train loss:  0.47022324800491333
train gradient:  0.4946653383323439
iteration : 1763
train acc:  0.8046875
train loss:  0.44880712032318115
train gradient:  0.5278234261117574
iteration : 1764
train acc:  0.75
train loss:  0.48059383034706116
train gradient:  0.5156516950354834
iteration : 1765
train acc:  0.8359375
train loss:  0.4508352279663086
train gradient:  0.5730523055101735
iteration : 1766
train acc:  0.8046875
train loss:  0.44555649161338806
train gradient:  0.3987178326359274
iteration : 1767
train acc:  0.796875
train loss:  0.39235836267471313
train gradient:  0.3101147817068575
iteration : 1768
train acc:  0.8203125
train loss:  0.4211164712905884
train gradient:  0.29695671273979296
iteration : 1769
train acc:  0.7734375
train loss:  0.47726866602897644
train gradient:  0.4281954814900639
iteration : 1770
train acc:  0.84375
train loss:  0.3902342915534973
train gradient:  0.3027513401122175
iteration : 1771
train acc:  0.8359375
train loss:  0.43606817722320557
train gradient:  0.33393309477842165
iteration : 1772
train acc:  0.875
train loss:  0.36004942655563354
train gradient:  0.5274682722216852
iteration : 1773
train acc:  0.8828125
train loss:  0.3265427350997925
train gradient:  0.27420210820505775
iteration : 1774
train acc:  0.8359375
train loss:  0.3946779668331146
train gradient:  0.542339893086921
iteration : 1775
train acc:  0.84375
train loss:  0.33981263637542725
train gradient:  0.3030070670960613
iteration : 1776
train acc:  0.8046875
train loss:  0.4238995611667633
train gradient:  0.36017067308998524
iteration : 1777
train acc:  0.75
train loss:  0.4835372567176819
train gradient:  0.4173040321146776
iteration : 1778
train acc:  0.8046875
train loss:  0.4320410490036011
train gradient:  0.37487260939706823
iteration : 1779
train acc:  0.78125
train loss:  0.46153146028518677
train gradient:  0.42139255814018756
iteration : 1780
train acc:  0.796875
train loss:  0.44920527935028076
train gradient:  0.4792682325514266
iteration : 1781
train acc:  0.828125
train loss:  0.4435911178588867
train gradient:  0.4026823438538879
iteration : 1782
train acc:  0.8046875
train loss:  0.4246741533279419
train gradient:  0.3793124208359658
iteration : 1783
train acc:  0.828125
train loss:  0.4049634039402008
train gradient:  0.4094893669486887
iteration : 1784
train acc:  0.796875
train loss:  0.4114345610141754
train gradient:  0.486131060505309
iteration : 1785
train acc:  0.8203125
train loss:  0.42515355348587036
train gradient:  0.42090151586624075
iteration : 1786
train acc:  0.875
train loss:  0.32875165343284607
train gradient:  0.37392625257970646
iteration : 1787
train acc:  0.8515625
train loss:  0.38072580099105835
train gradient:  0.36305888884216037
iteration : 1788
train acc:  0.8125
train loss:  0.41164177656173706
train gradient:  0.33975692926691853
iteration : 1789
train acc:  0.8046875
train loss:  0.3956529498100281
train gradient:  0.36886034432130177
iteration : 1790
train acc:  0.8359375
train loss:  0.3723672032356262
train gradient:  0.43098366643990144
iteration : 1791
train acc:  0.84375
train loss:  0.3532825708389282
train gradient:  0.31874747257967245
iteration : 1792
train acc:  0.8125
train loss:  0.45572414994239807
train gradient:  0.4695509893188289
iteration : 1793
train acc:  0.8046875
train loss:  0.3994410037994385
train gradient:  0.30893364960683845
iteration : 1794
train acc:  0.8203125
train loss:  0.41273730993270874
train gradient:  0.4579430002034081
iteration : 1795
train acc:  0.875
train loss:  0.3512531816959381
train gradient:  0.2282116099778425
iteration : 1796
train acc:  0.7421875
train loss:  0.49785763025283813
train gradient:  0.5686335493817931
iteration : 1797
train acc:  0.7734375
train loss:  0.44353538751602173
train gradient:  0.43243574822305014
iteration : 1798
train acc:  0.84375
train loss:  0.4358496367931366
train gradient:  0.44979734129796944
iteration : 1799
train acc:  0.796875
train loss:  0.4083753824234009
train gradient:  0.4653417787969862
iteration : 1800
train acc:  0.7890625
train loss:  0.43763023614883423
train gradient:  0.5467915256439337
iteration : 1801
train acc:  0.8203125
train loss:  0.4113588333129883
train gradient:  0.45629261629344064
iteration : 1802
train acc:  0.828125
train loss:  0.4455195367336273
train gradient:  0.679120255695189
iteration : 1803
train acc:  0.828125
train loss:  0.3748132586479187
train gradient:  0.35268885443348885
iteration : 1804
train acc:  0.7734375
train loss:  0.4640490710735321
train gradient:  0.4693002957479484
iteration : 1805
train acc:  0.84375
train loss:  0.33576124906539917
train gradient:  0.3601985609806207
iteration : 1806
train acc:  0.8359375
train loss:  0.3712785840034485
train gradient:  0.3318929621964328
iteration : 1807
train acc:  0.7734375
train loss:  0.45859408378601074
train gradient:  0.5276353698390139
iteration : 1808
train acc:  0.8671875
train loss:  0.372979998588562
train gradient:  0.32768259052117327
iteration : 1809
train acc:  0.7265625
train loss:  0.5380368232727051
train gradient:  0.5837563029081035
iteration : 1810
train acc:  0.8203125
train loss:  0.43339747190475464
train gradient:  0.4555741203481131
iteration : 1811
train acc:  0.8125
train loss:  0.44083917140960693
train gradient:  0.5666682171115892
iteration : 1812
train acc:  0.796875
train loss:  0.4437476396560669
train gradient:  0.507341400523467
iteration : 1813
train acc:  0.8046875
train loss:  0.3974388539791107
train gradient:  0.456725846133171
iteration : 1814
train acc:  0.859375
train loss:  0.3285737633705139
train gradient:  0.38441714184794634
iteration : 1815
train acc:  0.859375
train loss:  0.3913969397544861
train gradient:  0.4245675200370465
iteration : 1816
train acc:  0.8515625
train loss:  0.37506312131881714
train gradient:  0.33386731175036466
iteration : 1817
train acc:  0.8359375
train loss:  0.38243579864501953
train gradient:  0.29607424038732283
iteration : 1818
train acc:  0.7890625
train loss:  0.4065062999725342
train gradient:  0.34209552878048405
iteration : 1819
train acc:  0.78125
train loss:  0.4491754174232483
train gradient:  0.46781444780626696
iteration : 1820
train acc:  0.796875
train loss:  0.44759100675582886
train gradient:  0.5089219768226779
iteration : 1821
train acc:  0.78125
train loss:  0.4719887971878052
train gradient:  0.4608331935461599
iteration : 1822
train acc:  0.9140625
train loss:  0.3092730641365051
train gradient:  0.23225588746356254
iteration : 1823
train acc:  0.8125
train loss:  0.45327839255332947
train gradient:  0.559986414464676
iteration : 1824
train acc:  0.78125
train loss:  0.5070353746414185
train gradient:  0.6444147705518816
iteration : 1825
train acc:  0.828125
train loss:  0.4123463034629822
train gradient:  0.3600826812008517
iteration : 1826
train acc:  0.796875
train loss:  0.4065651297569275
train gradient:  0.523076354207382
iteration : 1827
train acc:  0.8359375
train loss:  0.39674973487854004
train gradient:  0.3702466685897501
iteration : 1828
train acc:  0.8203125
train loss:  0.3965107798576355
train gradient:  0.3465783426027605
iteration : 1829
train acc:  0.828125
train loss:  0.38989120721817017
train gradient:  0.3669718211486887
iteration : 1830
train acc:  0.828125
train loss:  0.41639408469200134
train gradient:  0.4417618791307883
iteration : 1831
train acc:  0.796875
train loss:  0.4085412919521332
train gradient:  0.43678411198854594
iteration : 1832
train acc:  0.8125
train loss:  0.4228585362434387
train gradient:  0.5201501605639247
iteration : 1833
train acc:  0.828125
train loss:  0.34969258308410645
train gradient:  0.361520403698999
iteration : 1834
train acc:  0.8515625
train loss:  0.37663131952285767
train gradient:  0.3474995506995964
iteration : 1835
train acc:  0.828125
train loss:  0.3862181603908539
train gradient:  0.5176585524917418
iteration : 1836
train acc:  0.8203125
train loss:  0.41899365186691284
train gradient:  0.3866769409797055
iteration : 1837
train acc:  0.84375
train loss:  0.37531328201293945
train gradient:  0.3980592032614654
iteration : 1838
train acc:  0.8125
train loss:  0.4362652897834778
train gradient:  0.4039093779454846
iteration : 1839
train acc:  0.8046875
train loss:  0.43680471181869507
train gradient:  0.35584278200469976
iteration : 1840
train acc:  0.7890625
train loss:  0.46889224648475647
train gradient:  0.3765193452235524
iteration : 1841
train acc:  0.8515625
train loss:  0.41468173265457153
train gradient:  0.32886147828323836
iteration : 1842
train acc:  0.8515625
train loss:  0.34179168939590454
train gradient:  0.3357981599749127
iteration : 1843
train acc:  0.8046875
train loss:  0.351984441280365
train gradient:  0.45535551277005126
iteration : 1844
train acc:  0.78125
train loss:  0.41409724950790405
train gradient:  0.412927504516238
iteration : 1845
train acc:  0.796875
train loss:  0.40306925773620605
train gradient:  0.4457601492576819
iteration : 1846
train acc:  0.8671875
train loss:  0.3461124897003174
train gradient:  0.3481216963142924
iteration : 1847
train acc:  0.796875
train loss:  0.39777326583862305
train gradient:  0.469225783489874
iteration : 1848
train acc:  0.8515625
train loss:  0.371573805809021
train gradient:  0.28183029260559844
iteration : 1849
train acc:  0.8515625
train loss:  0.35982081294059753
train gradient:  0.3900498641380079
iteration : 1850
train acc:  0.875
train loss:  0.31459224224090576
train gradient:  0.2523575419277866
iteration : 1851
train acc:  0.859375
train loss:  0.3640289902687073
train gradient:  0.35493137941498626
iteration : 1852
train acc:  0.7421875
train loss:  0.5121017098426819
train gradient:  0.6650294425289052
iteration : 1853
train acc:  0.7421875
train loss:  0.4660191833972931
train gradient:  0.5318888044222474
iteration : 1854
train acc:  0.8203125
train loss:  0.3812456727027893
train gradient:  0.34841287432202367
iteration : 1855
train acc:  0.828125
train loss:  0.3776547312736511
train gradient:  0.5258056347419455
iteration : 1856
train acc:  0.7734375
train loss:  0.49013978242874146
train gradient:  0.7993717232202089
iteration : 1857
train acc:  0.796875
train loss:  0.4574519991874695
train gradient:  0.3860057499156694
iteration : 1858
train acc:  0.8046875
train loss:  0.3805559277534485
train gradient:  0.5844288756108011
iteration : 1859
train acc:  0.859375
train loss:  0.3464292287826538
train gradient:  0.38074121481502327
iteration : 1860
train acc:  0.78125
train loss:  0.48737475275993347
train gradient:  0.7201937064347952
iteration : 1861
train acc:  0.8046875
train loss:  0.40644729137420654
train gradient:  0.38443197083416125
iteration : 1862
train acc:  0.8125
train loss:  0.42050421237945557
train gradient:  0.3718475705335944
iteration : 1863
train acc:  0.84375
train loss:  0.3962692618370056
train gradient:  0.39268614050213524
iteration : 1864
train acc:  0.84375
train loss:  0.35443150997161865
train gradient:  0.4421562503724401
iteration : 1865
train acc:  0.8515625
train loss:  0.34609851241111755
train gradient:  0.4711394835884415
iteration : 1866
train acc:  0.7890625
train loss:  0.4339427351951599
train gradient:  0.5390784963144615
iteration : 1867
train acc:  0.734375
train loss:  0.48027798533439636
train gradient:  0.5189126251350573
iteration : 1868
train acc:  0.796875
train loss:  0.38927072286605835
train gradient:  0.5610798908861332
iteration : 1869
train acc:  0.8203125
train loss:  0.37794095277786255
train gradient:  0.44446422858163165
iteration : 1870
train acc:  0.7578125
train loss:  0.48424896597862244
train gradient:  0.6186048506749935
iteration : 1871
train acc:  0.84375
train loss:  0.33138108253479004
train gradient:  0.34769993893144074
iteration : 1872
train acc:  0.7578125
train loss:  0.4287484884262085
train gradient:  0.4966978766744372
iteration : 1873
train acc:  0.8125
train loss:  0.4225296080112457
train gradient:  0.5173214823846195
iteration : 1874
train acc:  0.7734375
train loss:  0.46864980459213257
train gradient:  0.506590682900173
iteration : 1875
train acc:  0.8359375
train loss:  0.4093019664287567
train gradient:  0.4273594444078219
iteration : 1876
train acc:  0.8671875
train loss:  0.40733101963996887
train gradient:  0.3392915377285397
iteration : 1877
train acc:  0.7890625
train loss:  0.42544037103652954
train gradient:  0.5211582196912028
iteration : 1878
train acc:  0.8203125
train loss:  0.4122951626777649
train gradient:  0.6778580556047802
iteration : 1879
train acc:  0.8046875
train loss:  0.39420270919799805
train gradient:  0.3100194127252101
iteration : 1880
train acc:  0.84375
train loss:  0.36687129735946655
train gradient:  0.43283304195389183
iteration : 1881
train acc:  0.8125
train loss:  0.38643360137939453
train gradient:  0.3833693709678212
iteration : 1882
train acc:  0.78125
train loss:  0.4367979168891907
train gradient:  0.5550320543727747
iteration : 1883
train acc:  0.765625
train loss:  0.4479439854621887
train gradient:  0.7441698421598968
iteration : 1884
train acc:  0.8046875
train loss:  0.3979886770248413
train gradient:  0.5182610858415628
iteration : 1885
train acc:  0.7890625
train loss:  0.4198935925960541
train gradient:  0.5270221941574662
iteration : 1886
train acc:  0.8984375
train loss:  0.31105321645736694
train gradient:  0.40079832726523024
iteration : 1887
train acc:  0.8046875
train loss:  0.40230000019073486
train gradient:  0.49306029301386495
iteration : 1888
train acc:  0.7734375
train loss:  0.48418399691581726
train gradient:  0.5696621162305648
iteration : 1889
train acc:  0.7890625
train loss:  0.3825262784957886
train gradient:  0.42638173194412543
iteration : 1890
train acc:  0.828125
train loss:  0.4639832377433777
train gradient:  0.5305701138859391
iteration : 1891
train acc:  0.765625
train loss:  0.43036407232284546
train gradient:  0.37419559037387357
iteration : 1892
train acc:  0.78125
train loss:  0.4616926908493042
train gradient:  0.6888678142584242
iteration : 1893
train acc:  0.828125
train loss:  0.3755033612251282
train gradient:  0.35021021637836575
iteration : 1894
train acc:  0.828125
train loss:  0.35376790165901184
train gradient:  0.3165287423163946
iteration : 1895
train acc:  0.8125
train loss:  0.39329490065574646
train gradient:  0.47296638387988993
iteration : 1896
train acc:  0.75
train loss:  0.5072672963142395
train gradient:  0.7170223883947002
iteration : 1897
train acc:  0.8515625
train loss:  0.3437611162662506
train gradient:  0.29766069604007633
iteration : 1898
train acc:  0.78125
train loss:  0.4651598334312439
train gradient:  0.6790502172405488
iteration : 1899
train acc:  0.7890625
train loss:  0.44670742750167847
train gradient:  0.5996545620776702
iteration : 1900
train acc:  0.796875
train loss:  0.38883471488952637
train gradient:  0.5681698271644438
iteration : 1901
train acc:  0.7890625
train loss:  0.4399014711380005
train gradient:  0.3675246401582758
iteration : 1902
train acc:  0.8359375
train loss:  0.3155031204223633
train gradient:  0.2625113024410808
iteration : 1903
train acc:  0.8125
train loss:  0.3881450295448303
train gradient:  0.32959272859225824
iteration : 1904
train acc:  0.734375
train loss:  0.5055198073387146
train gradient:  0.6626295758127829
iteration : 1905
train acc:  0.8359375
train loss:  0.3838682174682617
train gradient:  0.38663571383310774
iteration : 1906
train acc:  0.765625
train loss:  0.46171504259109497
train gradient:  0.4330927622391928
iteration : 1907
train acc:  0.8125
train loss:  0.3777681589126587
train gradient:  0.39676931068895244
iteration : 1908
train acc:  0.8046875
train loss:  0.4216940701007843
train gradient:  0.5578917338090297
iteration : 1909
train acc:  0.8359375
train loss:  0.4123845100402832
train gradient:  0.46912894436166086
iteration : 1910
train acc:  0.796875
train loss:  0.4063073396682739
train gradient:  0.7463126530257383
iteration : 1911
train acc:  0.8046875
train loss:  0.4344828128814697
train gradient:  0.5628273084206024
iteration : 1912
train acc:  0.8046875
train loss:  0.409646213054657
train gradient:  0.5655546516892143
iteration : 1913
train acc:  0.8203125
train loss:  0.41710156202316284
train gradient:  0.48533679993468976
iteration : 1914
train acc:  0.796875
train loss:  0.4079497754573822
train gradient:  0.38257318196416684
iteration : 1915
train acc:  0.828125
train loss:  0.37632444500923157
train gradient:  0.5653621716336721
iteration : 1916
train acc:  0.8359375
train loss:  0.3777036666870117
train gradient:  0.47828745234296327
iteration : 1917
train acc:  0.8046875
train loss:  0.3839411437511444
train gradient:  0.4233606504846067
iteration : 1918
train acc:  0.796875
train loss:  0.39445510506629944
train gradient:  0.3835445352389189
iteration : 1919
train acc:  0.7734375
train loss:  0.492475301027298
train gradient:  0.4465831897392654
iteration : 1920
train acc:  0.78125
train loss:  0.5023965239524841
train gradient:  0.611194265461037
iteration : 1921
train acc:  0.7890625
train loss:  0.4825649559497833
train gradient:  0.4405689041479503
iteration : 1922
train acc:  0.734375
train loss:  0.5519567728042603
train gradient:  0.7008728046310426
iteration : 1923
train acc:  0.7890625
train loss:  0.46639102697372437
train gradient:  0.46334953098689247
iteration : 1924
train acc:  0.859375
train loss:  0.392921507358551
train gradient:  0.2751071184544694
iteration : 1925
train acc:  0.875
train loss:  0.3371984362602234
train gradient:  0.4055761101922333
iteration : 1926
train acc:  0.8359375
train loss:  0.40246307849884033
train gradient:  0.40873355748822043
iteration : 1927
train acc:  0.8125
train loss:  0.37910038232803345
train gradient:  0.4144030816478786
iteration : 1928
train acc:  0.7890625
train loss:  0.4117947220802307
train gradient:  0.49819929281241276
iteration : 1929
train acc:  0.8046875
train loss:  0.39457613229751587
train gradient:  0.3767311150950756
iteration : 1930
train acc:  0.7890625
train loss:  0.440823495388031
train gradient:  0.36880021855050593
iteration : 1931
train acc:  0.8125
train loss:  0.48998838663101196
train gradient:  0.5679627044142735
iteration : 1932
train acc:  0.8125
train loss:  0.41067981719970703
train gradient:  0.442529201942391
iteration : 1933
train acc:  0.7890625
train loss:  0.5584071278572083
train gradient:  0.7465342583493071
iteration : 1934
train acc:  0.7421875
train loss:  0.41850271821022034
train gradient:  0.3096406792647185
iteration : 1935
train acc:  0.828125
train loss:  0.4312772750854492
train gradient:  0.26681847154918076
iteration : 1936
train acc:  0.828125
train loss:  0.44339093565940857
train gradient:  0.4871061573163953
iteration : 1937
train acc:  0.7734375
train loss:  0.42099347710609436
train gradient:  0.38094797845698103
iteration : 1938
train acc:  0.7890625
train loss:  0.4062700867652893
train gradient:  0.30193401730063135
iteration : 1939
train acc:  0.75
train loss:  0.5462452173233032
train gradient:  0.4856466058783269
iteration : 1940
train acc:  0.7734375
train loss:  0.4154074788093567
train gradient:  0.36570644217764414
iteration : 1941
train acc:  0.8203125
train loss:  0.42068055272102356
train gradient:  0.2857155813914327
iteration : 1942
train acc:  0.796875
train loss:  0.43607985973358154
train gradient:  0.4691345233264261
iteration : 1943
train acc:  0.796875
train loss:  0.43426910042762756
train gradient:  0.3825606663115854
iteration : 1944
train acc:  0.84375
train loss:  0.35158565640449524
train gradient:  0.26125108994700963
iteration : 1945
train acc:  0.796875
train loss:  0.4089495539665222
train gradient:  0.36445564189938345
iteration : 1946
train acc:  0.765625
train loss:  0.4747846722602844
train gradient:  0.8568036217521962
iteration : 1947
train acc:  0.7890625
train loss:  0.45643457770347595
train gradient:  0.35335678828136957
iteration : 1948
train acc:  0.8125
train loss:  0.375422865152359
train gradient:  0.33981463768532066
iteration : 1949
train acc:  0.796875
train loss:  0.4129570722579956
train gradient:  0.41402085931958016
iteration : 1950
train acc:  0.828125
train loss:  0.3844203054904938
train gradient:  0.25392056203180535
iteration : 1951
train acc:  0.78125
train loss:  0.4172983467578888
train gradient:  0.43035249845541573
iteration : 1952
train acc:  0.859375
train loss:  0.3993220925331116
train gradient:  0.3554525431047823
iteration : 1953
train acc:  0.84375
train loss:  0.3766097128391266
train gradient:  0.304345524347563
iteration : 1954
train acc:  0.8125
train loss:  0.4642527103424072
train gradient:  0.45015365719441264
iteration : 1955
train acc:  0.8515625
train loss:  0.3109148144721985
train gradient:  0.317221103095174
iteration : 1956
train acc:  0.8046875
train loss:  0.45260751247406006
train gradient:  0.47606624611246584
iteration : 1957
train acc:  0.828125
train loss:  0.3901386260986328
train gradient:  0.26112540464830947
iteration : 1958
train acc:  0.84375
train loss:  0.3527162969112396
train gradient:  0.287806258971298
iteration : 1959
train acc:  0.8046875
train loss:  0.4350554943084717
train gradient:  0.49184164280090287
iteration : 1960
train acc:  0.859375
train loss:  0.3284718692302704
train gradient:  0.24963893316397714
iteration : 1961
train acc:  0.859375
train loss:  0.3893280029296875
train gradient:  0.39773155010391825
iteration : 1962
train acc:  0.828125
train loss:  0.42444008588790894
train gradient:  0.3739444414738147
iteration : 1963
train acc:  0.78125
train loss:  0.4267250597476959
train gradient:  0.49676230195023147
iteration : 1964
train acc:  0.859375
train loss:  0.3514081835746765
train gradient:  0.34414959620492724
iteration : 1965
train acc:  0.7890625
train loss:  0.42088115215301514
train gradient:  0.5267853181935316
iteration : 1966
train acc:  0.828125
train loss:  0.4164876937866211
train gradient:  0.36425255739109763
iteration : 1967
train acc:  0.7734375
train loss:  0.43489623069763184
train gradient:  0.4234329389257051
iteration : 1968
train acc:  0.828125
train loss:  0.41517385840415955
train gradient:  0.49909615800659746
iteration : 1969
train acc:  0.8359375
train loss:  0.43579035997390747
train gradient:  0.3652612134537466
iteration : 1970
train acc:  0.828125
train loss:  0.3812417685985565
train gradient:  0.40122391276000674
iteration : 1971
train acc:  0.78125
train loss:  0.4534301459789276
train gradient:  0.6046299205295673
iteration : 1972
train acc:  0.796875
train loss:  0.4324224889278412
train gradient:  0.4411873464599946
iteration : 1973
train acc:  0.8671875
train loss:  0.35407745838165283
train gradient:  0.2860595569762493
iteration : 1974
train acc:  0.8359375
train loss:  0.37751126289367676
train gradient:  0.37616690758478094
iteration : 1975
train acc:  0.8125
train loss:  0.43499642610549927
train gradient:  0.3766463140967325
iteration : 1976
train acc:  0.828125
train loss:  0.3694843053817749
train gradient:  0.34329437417940223
iteration : 1977
train acc:  0.7890625
train loss:  0.4668768644332886
train gradient:  0.6168056117285631
iteration : 1978
train acc:  0.796875
train loss:  0.45264846086502075
train gradient:  0.6190067071747509
iteration : 1979
train acc:  0.7890625
train loss:  0.4114743769168854
train gradient:  0.44073069567351597
iteration : 1980
train acc:  0.7890625
train loss:  0.45823973417282104
train gradient:  0.5733205532308141
iteration : 1981
train acc:  0.859375
train loss:  0.3679165840148926
train gradient:  0.2936432168176631
iteration : 1982
train acc:  0.859375
train loss:  0.335578978061676
train gradient:  0.3321029188348682
iteration : 1983
train acc:  0.8359375
train loss:  0.39341428875923157
train gradient:  0.24353352075308826
iteration : 1984
train acc:  0.7890625
train loss:  0.39720678329467773
train gradient:  0.5825652834268781
iteration : 1985
train acc:  0.828125
train loss:  0.3892071545124054
train gradient:  0.318713019586233
iteration : 1986
train acc:  0.8203125
train loss:  0.43973636627197266
train gradient:  0.505910500185426
iteration : 1987
train acc:  0.8203125
train loss:  0.40678614377975464
train gradient:  0.38719891910207577
iteration : 1988
train acc:  0.828125
train loss:  0.3789371848106384
train gradient:  0.3284866545128864
iteration : 1989
train acc:  0.7734375
train loss:  0.5252552628517151
train gradient:  0.5387892351094486
iteration : 1990
train acc:  0.8046875
train loss:  0.4294947385787964
train gradient:  0.449411522421048
iteration : 1991
train acc:  0.828125
train loss:  0.44928407669067383
train gradient:  0.4359058140401669
iteration : 1992
train acc:  0.78125
train loss:  0.47652149200439453
train gradient:  0.6132271398435638
iteration : 1993
train acc:  0.7109375
train loss:  0.5700751543045044
train gradient:  0.75381920143759
iteration : 1994
train acc:  0.765625
train loss:  0.44081249833106995
train gradient:  0.4419335321946153
iteration : 1995
train acc:  0.8046875
train loss:  0.4447619616985321
train gradient:  0.3646710318132584
iteration : 1996
train acc:  0.8359375
train loss:  0.3378237187862396
train gradient:  0.3115079401577237
iteration : 1997
train acc:  0.84375
train loss:  0.3829496502876282
train gradient:  0.5225204938126173
iteration : 1998
train acc:  0.7578125
train loss:  0.47717800736427307
train gradient:  0.4655314323243897
iteration : 1999
train acc:  0.7578125
train loss:  0.41015011072158813
train gradient:  0.3917657871885761
iteration : 2000
train acc:  0.8359375
train loss:  0.4257566034793854
train gradient:  0.3867925078902094
iteration : 2001
train acc:  0.828125
train loss:  0.3514848053455353
train gradient:  0.28444300977505554
iteration : 2002
train acc:  0.796875
train loss:  0.4377664029598236
train gradient:  0.3311617105390454
iteration : 2003
train acc:  0.8515625
train loss:  0.3346387445926666
train gradient:  0.2932110539530728
iteration : 2004
train acc:  0.828125
train loss:  0.4538845717906952
train gradient:  0.5005841467160834
iteration : 2005
train acc:  0.84375
train loss:  0.40388935804367065
train gradient:  0.3054294963063921
iteration : 2006
train acc:  0.8203125
train loss:  0.39218974113464355
train gradient:  0.36892847003541335
iteration : 2007
train acc:  0.8125
train loss:  0.3921574354171753
train gradient:  0.4518572684526869
iteration : 2008
train acc:  0.8359375
train loss:  0.41003474593162537
train gradient:  0.3592167229080765
iteration : 2009
train acc:  0.78125
train loss:  0.41712498664855957
train gradient:  0.5850362751903277
iteration : 2010
train acc:  0.84375
train loss:  0.42623260617256165
train gradient:  0.363610281527784
iteration : 2011
train acc:  0.796875
train loss:  0.422929048538208
train gradient:  0.4296973129976852
iteration : 2012
train acc:  0.8046875
train loss:  0.40969589352607727
train gradient:  0.5462328899785664
iteration : 2013
train acc:  0.890625
train loss:  0.35699689388275146
train gradient:  0.2881805481526434
iteration : 2014
train acc:  0.828125
train loss:  0.34881293773651123
train gradient:  0.31800548615459373
iteration : 2015
train acc:  0.7578125
train loss:  0.49606505036354065
train gradient:  0.5529875536476734
iteration : 2016
train acc:  0.8515625
train loss:  0.37643009424209595
train gradient:  0.36950306038975345
iteration : 2017
train acc:  0.828125
train loss:  0.3795207738876343
train gradient:  0.33160939223312846
iteration : 2018
train acc:  0.765625
train loss:  0.5057831406593323
train gradient:  0.5477212237390175
iteration : 2019
train acc:  0.765625
train loss:  0.5074504613876343
train gradient:  0.575478489805541
iteration : 2020
train acc:  0.8203125
train loss:  0.40209123492240906
train gradient:  0.3497509705008928
iteration : 2021
train acc:  0.78125
train loss:  0.4245166778564453
train gradient:  0.3490410618711013
iteration : 2022
train acc:  0.7890625
train loss:  0.42966532707214355
train gradient:  0.49963505812609543
iteration : 2023
train acc:  0.8203125
train loss:  0.4607604444026947
train gradient:  0.5753787563206616
iteration : 2024
train acc:  0.8359375
train loss:  0.33134427666664124
train gradient:  0.2967097951373232
iteration : 2025
train acc:  0.7734375
train loss:  0.48481398820877075
train gradient:  0.3855806643203945
iteration : 2026
train acc:  0.71875
train loss:  0.5351347923278809
train gradient:  0.6195486679850508
iteration : 2027
train acc:  0.796875
train loss:  0.43328046798706055
train gradient:  0.38316436527384834
iteration : 2028
train acc:  0.7421875
train loss:  0.48926156759262085
train gradient:  0.42269973180497217
iteration : 2029
train acc:  0.78125
train loss:  0.44067293405532837
train gradient:  0.47339855619000004
iteration : 2030
train acc:  0.8046875
train loss:  0.38407033681869507
train gradient:  0.2711756102339837
iteration : 2031
train acc:  0.8203125
train loss:  0.3808003067970276
train gradient:  0.3501440719056369
iteration : 2032
train acc:  0.78125
train loss:  0.44705405831336975
train gradient:  0.42099670820289087
iteration : 2033
train acc:  0.828125
train loss:  0.34877675771713257
train gradient:  0.39777284274103925
iteration : 2034
train acc:  0.765625
train loss:  0.4443628191947937
train gradient:  0.5746499339758557
iteration : 2035
train acc:  0.796875
train loss:  0.47034111618995667
train gradient:  0.42426017366803037
iteration : 2036
train acc:  0.796875
train loss:  0.49413004517555237
train gradient:  0.4683989651934581
iteration : 2037
train acc:  0.8125
train loss:  0.42621544003486633
train gradient:  0.40894759420402477
iteration : 2038
train acc:  0.84375
train loss:  0.40338629484176636
train gradient:  0.3034112679294166
iteration : 2039
train acc:  0.828125
train loss:  0.33818909525871277
train gradient:  0.2990912315531964
iteration : 2040
train acc:  0.8671875
train loss:  0.3716696798801422
train gradient:  0.2517433822492296
iteration : 2041
train acc:  0.78125
train loss:  0.49925166368484497
train gradient:  0.4819469082309571
iteration : 2042
train acc:  0.8046875
train loss:  0.4556150734424591
train gradient:  0.3893919770596473
iteration : 2043
train acc:  0.875
train loss:  0.3492394685745239
train gradient:  0.3744095282631659
iteration : 2044
train acc:  0.8046875
train loss:  0.4826882481575012
train gradient:  0.5087196228959714
iteration : 2045
train acc:  0.8046875
train loss:  0.3856397867202759
train gradient:  0.29449608767392993
iteration : 2046
train acc:  0.71875
train loss:  0.5290735960006714
train gradient:  0.5992071554074531
iteration : 2047
train acc:  0.7734375
train loss:  0.44559431076049805
train gradient:  0.4350852049441514
iteration : 2048
train acc:  0.8203125
train loss:  0.38091373443603516
train gradient:  0.2397735312684633
iteration : 2049
train acc:  0.8125
train loss:  0.37882980704307556
train gradient:  0.333464509941446
iteration : 2050
train acc:  0.8046875
train loss:  0.4378536343574524
train gradient:  0.33731812547227746
iteration : 2051
train acc:  0.7890625
train loss:  0.43513256311416626
train gradient:  0.5159548688023745
iteration : 2052
train acc:  0.828125
train loss:  0.3740338087081909
train gradient:  0.30617264975881603
iteration : 2053
train acc:  0.7890625
train loss:  0.41887110471725464
train gradient:  0.3893855700087456
iteration : 2054
train acc:  0.8671875
train loss:  0.3353751301765442
train gradient:  0.2593831520117702
iteration : 2055
train acc:  0.84375
train loss:  0.3745570778846741
train gradient:  0.32059614304413947
iteration : 2056
train acc:  0.78125
train loss:  0.4444126486778259
train gradient:  0.3398206226129168
iteration : 2057
train acc:  0.78125
train loss:  0.5112605690956116
train gradient:  0.5504286332038884
iteration : 2058
train acc:  0.8125
train loss:  0.4167982041835785
train gradient:  0.3800172015010625
iteration : 2059
train acc:  0.796875
train loss:  0.43088170886039734
train gradient:  0.37237588674651867
iteration : 2060
train acc:  0.84375
train loss:  0.35777974128723145
train gradient:  0.3064112467217352
iteration : 2061
train acc:  0.8046875
train loss:  0.4489707350730896
train gradient:  0.4578343969484792
iteration : 2062
train acc:  0.8359375
train loss:  0.38082972168922424
train gradient:  0.4369452136071587
iteration : 2063
train acc:  0.796875
train loss:  0.5087283253669739
train gradient:  0.38447407960338914
iteration : 2064
train acc:  0.84375
train loss:  0.3600218892097473
train gradient:  0.26895804284439345
iteration : 2065
train acc:  0.796875
train loss:  0.4491748809814453
train gradient:  0.4061538111768662
iteration : 2066
train acc:  0.7890625
train loss:  0.43796002864837646
train gradient:  0.5066530321741846
iteration : 2067
train acc:  0.8203125
train loss:  0.4162856936454773
train gradient:  0.5835246034119309
iteration : 2068
train acc:  0.875
train loss:  0.3261292278766632
train gradient:  0.3073032333736739
iteration : 2069
train acc:  0.7734375
train loss:  0.48447635769844055
train gradient:  0.5145966126052346
iteration : 2070
train acc:  0.7890625
train loss:  0.41649413108825684
train gradient:  0.3087273460538071
iteration : 2071
train acc:  0.765625
train loss:  0.4649144113063812
train gradient:  0.377822933350077
iteration : 2072
train acc:  0.78125
train loss:  0.4537181258201599
train gradient:  0.6358805267053882
iteration : 2073
train acc:  0.8515625
train loss:  0.32273009419441223
train gradient:  0.27297843141075784
iteration : 2074
train acc:  0.765625
train loss:  0.49129587411880493
train gradient:  0.5067181398721587
iteration : 2075
train acc:  0.8359375
train loss:  0.36108076572418213
train gradient:  0.302061269511909
iteration : 2076
train acc:  0.765625
train loss:  0.49316710233688354
train gradient:  0.33764975772138256
iteration : 2077
train acc:  0.8046875
train loss:  0.4233095645904541
train gradient:  0.30892361980689226
iteration : 2078
train acc:  0.8046875
train loss:  0.44941818714141846
train gradient:  0.5098764676744446
iteration : 2079
train acc:  0.8125
train loss:  0.38334277272224426
train gradient:  0.25385663540426784
iteration : 2080
train acc:  0.8046875
train loss:  0.4748263955116272
train gradient:  0.45017083704960337
iteration : 2081
train acc:  0.828125
train loss:  0.4102818965911865
train gradient:  0.4602494020281363
iteration : 2082
train acc:  0.7578125
train loss:  0.46278858184814453
train gradient:  0.485822749064552
iteration : 2083
train acc:  0.78125
train loss:  0.5055354833602905
train gradient:  0.69087010932372
iteration : 2084
train acc:  0.8125
train loss:  0.4091407060623169
train gradient:  0.38950917962818504
iteration : 2085
train acc:  0.8203125
train loss:  0.3756241500377655
train gradient:  0.44405762310679936
iteration : 2086
train acc:  0.828125
train loss:  0.43270960450172424
train gradient:  0.3424175978227225
iteration : 2087
train acc:  0.8046875
train loss:  0.3906674385070801
train gradient:  0.27544859469338284
iteration : 2088
train acc:  0.8359375
train loss:  0.4270976185798645
train gradient:  0.41234624507782425
iteration : 2089
train acc:  0.8203125
train loss:  0.38042664527893066
train gradient:  0.23838879781537073
iteration : 2090
train acc:  0.8359375
train loss:  0.37414491176605225
train gradient:  0.313559197709057
iteration : 2091
train acc:  0.796875
train loss:  0.4321519136428833
train gradient:  0.398824009864575
iteration : 2092
train acc:  0.7890625
train loss:  0.4153220057487488
train gradient:  0.41020966667029285
iteration : 2093
train acc:  0.828125
train loss:  0.41508662700653076
train gradient:  0.36559896399945035
iteration : 2094
train acc:  0.828125
train loss:  0.40241739153862
train gradient:  0.3201800141026632
iteration : 2095
train acc:  0.8125
train loss:  0.3515944480895996
train gradient:  0.32129108892342795
iteration : 2096
train acc:  0.8125
train loss:  0.3955621123313904
train gradient:  0.3975782523039995
iteration : 2097
train acc:  0.8046875
train loss:  0.43853646516799927
train gradient:  0.32741887746867304
iteration : 2098
train acc:  0.8125
train loss:  0.3738810420036316
train gradient:  0.3334599790315561
iteration : 2099
train acc:  0.75
train loss:  0.5211072564125061
train gradient:  0.5599264879669017
iteration : 2100
train acc:  0.8125
train loss:  0.3798828125
train gradient:  0.3718112328989962
iteration : 2101
train acc:  0.8046875
train loss:  0.3907020390033722
train gradient:  0.2498228174864121
iteration : 2102
train acc:  0.7734375
train loss:  0.4216386079788208
train gradient:  0.420069529126573
iteration : 2103
train acc:  0.78125
train loss:  0.4752156734466553
train gradient:  0.5240485312585803
iteration : 2104
train acc:  0.7890625
train loss:  0.4426054060459137
train gradient:  0.5382848248944587
iteration : 2105
train acc:  0.7734375
train loss:  0.41366150975227356
train gradient:  0.2972008458292543
iteration : 2106
train acc:  0.765625
train loss:  0.4758377969264984
train gradient:  0.4848624210747549
iteration : 2107
train acc:  0.84375
train loss:  0.38643842935562134
train gradient:  0.26968849087851376
iteration : 2108
train acc:  0.78125
train loss:  0.4378158450126648
train gradient:  0.38194680687399857
iteration : 2109
train acc:  0.859375
train loss:  0.344438374042511
train gradient:  0.2928626162147702
iteration : 2110
train acc:  0.8125
train loss:  0.36827269196510315
train gradient:  0.3494694296773118
iteration : 2111
train acc:  0.8203125
train loss:  0.42441922426223755
train gradient:  0.3880315713551732
iteration : 2112
train acc:  0.78125
train loss:  0.4274443984031677
train gradient:  0.5750236701503307
iteration : 2113
train acc:  0.8671875
train loss:  0.346807599067688
train gradient:  0.3410011403944509
iteration : 2114
train acc:  0.78125
train loss:  0.41836628317832947
train gradient:  0.3807546937723729
iteration : 2115
train acc:  0.7890625
train loss:  0.4093394875526428
train gradient:  0.33457716088428796
iteration : 2116
train acc:  0.78125
train loss:  0.4705478549003601
train gradient:  0.5038610918513203
iteration : 2117
train acc:  0.8359375
train loss:  0.3829440176486969
train gradient:  0.42997257661458077
iteration : 2118
train acc:  0.8203125
train loss:  0.4389532208442688
train gradient:  0.4063685205620875
iteration : 2119
train acc:  0.828125
train loss:  0.35462474822998047
train gradient:  0.3902146599072991
iteration : 2120
train acc:  0.8359375
train loss:  0.3412690758705139
train gradient:  0.2469768871154902
iteration : 2121
train acc:  0.78125
train loss:  0.4428407549858093
train gradient:  0.430246701687741
iteration : 2122
train acc:  0.8359375
train loss:  0.37247201800346375
train gradient:  0.3343558876841383
iteration : 2123
train acc:  0.8203125
train loss:  0.3868841528892517
train gradient:  0.2710361821096223
iteration : 2124
train acc:  0.859375
train loss:  0.3430013656616211
train gradient:  0.33726676905791075
iteration : 2125
train acc:  0.8125
train loss:  0.3834327161312103
train gradient:  0.33100326231493427
iteration : 2126
train acc:  0.78125
train loss:  0.4628983438014984
train gradient:  0.5467311180891905
iteration : 2127
train acc:  0.84375
train loss:  0.40315306186676025
train gradient:  0.35610264956930515
iteration : 2128
train acc:  0.796875
train loss:  0.4120321273803711
train gradient:  0.41032196562358564
iteration : 2129
train acc:  0.859375
train loss:  0.37195318937301636
train gradient:  0.30989880009019666
iteration : 2130
train acc:  0.8359375
train loss:  0.35629451274871826
train gradient:  0.27077551029260455
iteration : 2131
train acc:  0.8125
train loss:  0.4364820718765259
train gradient:  0.4499977753728896
iteration : 2132
train acc:  0.8125
train loss:  0.40001702308654785
train gradient:  0.4231450052617663
iteration : 2133
train acc:  0.875
train loss:  0.34349244832992554
train gradient:  0.23593450695641893
iteration : 2134
train acc:  0.8203125
train loss:  0.4285696744918823
train gradient:  0.3433345511714378
iteration : 2135
train acc:  0.828125
train loss:  0.44033563137054443
train gradient:  0.4229364392249944
iteration : 2136
train acc:  0.796875
train loss:  0.38327834010124207
train gradient:  0.39404272396701556
iteration : 2137
train acc:  0.8125
train loss:  0.40402060747146606
train gradient:  0.5535585791357936
iteration : 2138
train acc:  0.859375
train loss:  0.3360934555530548
train gradient:  0.3014790794582431
iteration : 2139
train acc:  0.78125
train loss:  0.45536115765571594
train gradient:  0.602736506962874
iteration : 2140
train acc:  0.8515625
train loss:  0.4177680015563965
train gradient:  0.5539831424214481
iteration : 2141
train acc:  0.84375
train loss:  0.4094248414039612
train gradient:  0.3522682498093458
iteration : 2142
train acc:  0.8515625
train loss:  0.35487687587738037
train gradient:  0.2873037854731995
iteration : 2143
train acc:  0.8046875
train loss:  0.3948584198951721
train gradient:  0.4287880388092875
iteration : 2144
train acc:  0.7421875
train loss:  0.5566941499710083
train gradient:  0.6524617829167699
iteration : 2145
train acc:  0.84375
train loss:  0.3616778254508972
train gradient:  0.26093106088424844
iteration : 2146
train acc:  0.7890625
train loss:  0.4590311646461487
train gradient:  0.3072503523336425
iteration : 2147
train acc:  0.78125
train loss:  0.4226777255535126
train gradient:  0.45670268641595574
iteration : 2148
train acc:  0.8203125
train loss:  0.38291943073272705
train gradient:  0.342443218302595
iteration : 2149
train acc:  0.78125
train loss:  0.4281654953956604
train gradient:  0.4002016549175989
iteration : 2150
train acc:  0.8359375
train loss:  0.38609403371810913
train gradient:  0.3757955309348209
iteration : 2151
train acc:  0.796875
train loss:  0.4189286231994629
train gradient:  0.4282961273900032
iteration : 2152
train acc:  0.7890625
train loss:  0.3969002962112427
train gradient:  0.3643307596625723
iteration : 2153
train acc:  0.8125
train loss:  0.44469591975212097
train gradient:  0.48323152006283526
iteration : 2154
train acc:  0.8125
train loss:  0.4146943688392639
train gradient:  0.4253923469506479
iteration : 2155
train acc:  0.8671875
train loss:  0.3587961196899414
train gradient:  0.2815041084938527
iteration : 2156
train acc:  0.8359375
train loss:  0.4007701873779297
train gradient:  0.33217579372835737
iteration : 2157
train acc:  0.828125
train loss:  0.35740727186203003
train gradient:  0.3000112788794272
iteration : 2158
train acc:  0.7890625
train loss:  0.47174474596977234
train gradient:  0.3879768746594742
iteration : 2159
train acc:  0.828125
train loss:  0.4149492383003235
train gradient:  0.42528288360621047
iteration : 2160
train acc:  0.7890625
train loss:  0.4272732734680176
train gradient:  0.4875428448222639
iteration : 2161
train acc:  0.8046875
train loss:  0.3997546136379242
train gradient:  0.31969458599435774
iteration : 2162
train acc:  0.7890625
train loss:  0.43764057755470276
train gradient:  0.4529314060743684
iteration : 2163
train acc:  0.78125
train loss:  0.474092572927475
train gradient:  0.40851117809110365
iteration : 2164
train acc:  0.796875
train loss:  0.4472053647041321
train gradient:  0.4611771199092656
iteration : 2165
train acc:  0.7890625
train loss:  0.40389174222946167
train gradient:  0.297398455437072
iteration : 2166
train acc:  0.765625
train loss:  0.49272042512893677
train gradient:  0.5462759060208673
iteration : 2167
train acc:  0.828125
train loss:  0.405185341835022
train gradient:  0.36312060523240697
iteration : 2168
train acc:  0.8046875
train loss:  0.37933486700057983
train gradient:  0.3919242859274207
iteration : 2169
train acc:  0.8203125
train loss:  0.39042699337005615
train gradient:  0.4098640889920072
iteration : 2170
train acc:  0.8125
train loss:  0.42243343591690063
train gradient:  0.37329246277529676
iteration : 2171
train acc:  0.828125
train loss:  0.3947809636592865
train gradient:  0.37108664528461754
iteration : 2172
train acc:  0.8359375
train loss:  0.36595675349235535
train gradient:  0.3830886459485531
iteration : 2173
train acc:  0.7421875
train loss:  0.4809824228286743
train gradient:  0.505993084738527
iteration : 2174
train acc:  0.7734375
train loss:  0.43361133337020874
train gradient:  0.3566659213502086
iteration : 2175
train acc:  0.8203125
train loss:  0.3425760269165039
train gradient:  0.4293343994738989
iteration : 2176
train acc:  0.78125
train loss:  0.4164220094680786
train gradient:  0.29999278195299744
iteration : 2177
train acc:  0.8046875
train loss:  0.4261510670185089
train gradient:  0.3532407209488581
iteration : 2178
train acc:  0.8359375
train loss:  0.3935369849205017
train gradient:  0.31592328711972584
iteration : 2179
train acc:  0.875
train loss:  0.35054248571395874
train gradient:  0.33550257733899747
iteration : 2180
train acc:  0.8203125
train loss:  0.3937617540359497
train gradient:  0.4007311973168593
iteration : 2181
train acc:  0.8359375
train loss:  0.4085966646671295
train gradient:  0.2967576101064823
iteration : 2182
train acc:  0.7890625
train loss:  0.5047845244407654
train gradient:  0.486605928105092
iteration : 2183
train acc:  0.8359375
train loss:  0.3597099184989929
train gradient:  0.23735185796476374
iteration : 2184
train acc:  0.8046875
train loss:  0.4122071862220764
train gradient:  0.31915845877441146
iteration : 2185
train acc:  0.8359375
train loss:  0.39501285552978516
train gradient:  0.39718453281505633
iteration : 2186
train acc:  0.875
train loss:  0.32722803950309753
train gradient:  0.2511881939292142
iteration : 2187
train acc:  0.796875
train loss:  0.48476850986480713
train gradient:  0.554001786463304
iteration : 2188
train acc:  0.859375
train loss:  0.3513484001159668
train gradient:  0.3305785597421314
iteration : 2189
train acc:  0.7421875
train loss:  0.4708753526210785
train gradient:  0.38837261022198455
iteration : 2190
train acc:  0.8046875
train loss:  0.44106221199035645
train gradient:  0.40791269059989865
iteration : 2191
train acc:  0.8671875
train loss:  0.3198994994163513
train gradient:  0.22682759399172225
iteration : 2192
train acc:  0.8515625
train loss:  0.4542251527309418
train gradient:  0.3541033826244267
iteration : 2193
train acc:  0.796875
train loss:  0.4381728768348694
train gradient:  0.5143470973314188
iteration : 2194
train acc:  0.8828125
train loss:  0.27730751037597656
train gradient:  0.2918119460760228
iteration : 2195
train acc:  0.875
train loss:  0.3638765215873718
train gradient:  0.29492643928984336
iteration : 2196
train acc:  0.7890625
train loss:  0.4341057240962982
train gradient:  0.4741491693764697
iteration : 2197
train acc:  0.8125
train loss:  0.4149191975593567
train gradient:  0.32706988293920425
iteration : 2198
train acc:  0.828125
train loss:  0.3743709325790405
train gradient:  0.3109636441751625
iteration : 2199
train acc:  0.796875
train loss:  0.42613208293914795
train gradient:  0.38006063617217367
iteration : 2200
train acc:  0.8125
train loss:  0.4230239987373352
train gradient:  0.4926776538803666
iteration : 2201
train acc:  0.8828125
train loss:  0.32510095834732056
train gradient:  0.2837930065833111
iteration : 2202
train acc:  0.859375
train loss:  0.3672128915786743
train gradient:  0.3568585690111645
iteration : 2203
train acc:  0.7734375
train loss:  0.4212934374809265
train gradient:  0.35190414353824134
iteration : 2204
train acc:  0.84375
train loss:  0.40159666538238525
train gradient:  0.4048920515603695
iteration : 2205
train acc:  0.7890625
train loss:  0.430416464805603
train gradient:  0.5141962136347901
iteration : 2206
train acc:  0.84375
train loss:  0.40389394760131836
train gradient:  0.3602335349267789
iteration : 2207
train acc:  0.8515625
train loss:  0.3826918601989746
train gradient:  0.3758592169868159
iteration : 2208
train acc:  0.8203125
train loss:  0.3897205591201782
train gradient:  0.3438674167562764
iteration : 2209
train acc:  0.828125
train loss:  0.4083904027938843
train gradient:  0.4401486778253676
iteration : 2210
train acc:  0.7578125
train loss:  0.4964101016521454
train gradient:  0.52981110404806
iteration : 2211
train acc:  0.8359375
train loss:  0.3298195004463196
train gradient:  0.5211773698466293
iteration : 2212
train acc:  0.75
train loss:  0.497288316488266
train gradient:  0.5686867485809575
iteration : 2213
train acc:  0.8125
train loss:  0.39703357219696045
train gradient:  0.3172558159916726
iteration : 2214
train acc:  0.8046875
train loss:  0.4357718229293823
train gradient:  0.3996341578498347
iteration : 2215
train acc:  0.78125
train loss:  0.44616448879241943
train gradient:  0.4225883346991286
iteration : 2216
train acc:  0.765625
train loss:  0.5098081231117249
train gradient:  0.47262729340580034
iteration : 2217
train acc:  0.8515625
train loss:  0.3718469738960266
train gradient:  0.23184529075932886
iteration : 2218
train acc:  0.8515625
train loss:  0.3403502106666565
train gradient:  0.29756193067307407
iteration : 2219
train acc:  0.796875
train loss:  0.4283771514892578
train gradient:  0.48522413890076255
iteration : 2220
train acc:  0.859375
train loss:  0.3325480818748474
train gradient:  0.23778113769848563
iteration : 2221
train acc:  0.8671875
train loss:  0.3133835792541504
train gradient:  0.37708448683126305
iteration : 2222
train acc:  0.7734375
train loss:  0.4540078341960907
train gradient:  0.4950141227030458
iteration : 2223
train acc:  0.8125
train loss:  0.41174519062042236
train gradient:  0.46109844230623864
iteration : 2224
train acc:  0.8203125
train loss:  0.42952656745910645
train gradient:  0.5279169020581761
iteration : 2225
train acc:  0.75
train loss:  0.47375422716140747
train gradient:  0.3808271907983686
iteration : 2226
train acc:  0.8671875
train loss:  0.3600371479988098
train gradient:  0.2914573988378167
iteration : 2227
train acc:  0.78125
train loss:  0.47015857696533203
train gradient:  0.476270720038579
iteration : 2228
train acc:  0.7734375
train loss:  0.4673061966896057
train gradient:  0.4131390453522106
iteration : 2229
train acc:  0.796875
train loss:  0.4530184268951416
train gradient:  0.39519382594060415
iteration : 2230
train acc:  0.734375
train loss:  0.4388636350631714
train gradient:  0.4736851542491081
iteration : 2231
train acc:  0.796875
train loss:  0.4361419677734375
train gradient:  0.6402323564674737
iteration : 2232
train acc:  0.8125
train loss:  0.42442786693573
train gradient:  0.3927440767091896
iteration : 2233
train acc:  0.8046875
train loss:  0.40968793630599976
train gradient:  0.3307842898317717
iteration : 2234
train acc:  0.8125
train loss:  0.3809787631034851
train gradient:  0.38249931594541
iteration : 2235
train acc:  0.8515625
train loss:  0.32330119609832764
train gradient:  0.27673793939114727
iteration : 2236
train acc:  0.84375
train loss:  0.4431830942630768
train gradient:  0.46811621863210456
iteration : 2237
train acc:  0.8359375
train loss:  0.401346892118454
train gradient:  0.3366869250324956
iteration : 2238
train acc:  0.78125
train loss:  0.44340771436691284
train gradient:  0.4086736590467546
iteration : 2239
train acc:  0.7890625
train loss:  0.3878658711910248
train gradient:  0.2761529317503423
iteration : 2240
train acc:  0.8359375
train loss:  0.35969915986061096
train gradient:  0.2946581599999137
iteration : 2241
train acc:  0.765625
train loss:  0.47442734241485596
train gradient:  0.39665665976519976
iteration : 2242
train acc:  0.875
train loss:  0.3382153809070587
train gradient:  0.25734165425560385
iteration : 2243
train acc:  0.8203125
train loss:  0.4326069951057434
train gradient:  0.46098484744646634
iteration : 2244
train acc:  0.859375
train loss:  0.3633924722671509
train gradient:  0.29009611029428284
iteration : 2245
train acc:  0.7890625
train loss:  0.42966312170028687
train gradient:  0.521415879148811
iteration : 2246
train acc:  0.8515625
train loss:  0.3853908181190491
train gradient:  0.40284303637111474
iteration : 2247
train acc:  0.8359375
train loss:  0.3393077254295349
train gradient:  0.3046942909520314
iteration : 2248
train acc:  0.828125
train loss:  0.37424159049987793
train gradient:  0.3997681947478251
iteration : 2249
train acc:  0.78125
train loss:  0.4361339807510376
train gradient:  0.3983964704547086
iteration : 2250
train acc:  0.875
train loss:  0.32885804772377014
train gradient:  0.24653351900970338
iteration : 2251
train acc:  0.8671875
train loss:  0.38519036769866943
train gradient:  0.3473169100017415
iteration : 2252
train acc:  0.8125
train loss:  0.38247179985046387
train gradient:  0.27672611233877153
iteration : 2253
train acc:  0.8125
train loss:  0.45357412099838257
train gradient:  0.31552983356202335
iteration : 2254
train acc:  0.765625
train loss:  0.44671911001205444
train gradient:  0.3767156391618845
iteration : 2255
train acc:  0.828125
train loss:  0.3853892982006073
train gradient:  0.34457309081099513
iteration : 2256
train acc:  0.8515625
train loss:  0.3499646782875061
train gradient:  0.3582590660552575
iteration : 2257
train acc:  0.796875
train loss:  0.3761098384857178
train gradient:  0.31662291097181927
iteration : 2258
train acc:  0.8046875
train loss:  0.4325632154941559
train gradient:  0.4860789536783336
iteration : 2259
train acc:  0.828125
train loss:  0.3451513648033142
train gradient:  0.29323092060942185
iteration : 2260
train acc:  0.796875
train loss:  0.410763144493103
train gradient:  0.35713421368416726
iteration : 2261
train acc:  0.8046875
train loss:  0.46777814626693726
train gradient:  0.6610543965915683
iteration : 2262
train acc:  0.8359375
train loss:  0.39744699001312256
train gradient:  0.42843439870319056
iteration : 2263
train acc:  0.828125
train loss:  0.37935811281204224
train gradient:  0.359490650380683
iteration : 2264
train acc:  0.828125
train loss:  0.364972323179245
train gradient:  0.28025495140443496
iteration : 2265
train acc:  0.84375
train loss:  0.3808112144470215
train gradient:  0.3022467778256973
iteration : 2266
train acc:  0.8046875
train loss:  0.4264858365058899
train gradient:  0.41719000785467103
iteration : 2267
train acc:  0.765625
train loss:  0.5283167362213135
train gradient:  0.5618532072733042
iteration : 2268
train acc:  0.796875
train loss:  0.4057512879371643
train gradient:  0.3709786506250015
iteration : 2269
train acc:  0.734375
train loss:  0.5201339721679688
train gradient:  0.5590788308021118
iteration : 2270
train acc:  0.8515625
train loss:  0.4001239240169525
train gradient:  0.40608927140874407
iteration : 2271
train acc:  0.828125
train loss:  0.35857635736465454
train gradient:  0.5240467539192273
iteration : 2272
train acc:  0.7890625
train loss:  0.4233247637748718
train gradient:  0.4956715007324569
iteration : 2273
train acc:  0.7734375
train loss:  0.47774258255958557
train gradient:  0.5502854958200691
iteration : 2274
train acc:  0.796875
train loss:  0.41734081506729126
train gradient:  0.495993864192635
iteration : 2275
train acc:  0.7578125
train loss:  0.45124703645706177
train gradient:  0.3759799421017435
iteration : 2276
train acc:  0.8359375
train loss:  0.430889368057251
train gradient:  0.6614293691400441
iteration : 2277
train acc:  0.8125
train loss:  0.4119775891304016
train gradient:  0.44857291643754355
iteration : 2278
train acc:  0.8046875
train loss:  0.40886759757995605
train gradient:  0.4020220374475168
iteration : 2279
train acc:  0.84375
train loss:  0.40624040365219116
train gradient:  0.3302656211339212
iteration : 2280
train acc:  0.8046875
train loss:  0.3892762064933777
train gradient:  0.4298256153905877
iteration : 2281
train acc:  0.7890625
train loss:  0.4393070936203003
train gradient:  0.4081128712305223
iteration : 2282
train acc:  0.8359375
train loss:  0.39584672451019287
train gradient:  0.3701474310571496
iteration : 2283
train acc:  0.7734375
train loss:  0.4319833219051361
train gradient:  0.3317666332234306
iteration : 2284
train acc:  0.7890625
train loss:  0.45018425583839417
train gradient:  0.40987735185887575
iteration : 2285
train acc:  0.828125
train loss:  0.4145883023738861
train gradient:  0.4635174156287209
iteration : 2286
train acc:  0.7578125
train loss:  0.5285382270812988
train gradient:  0.412633835424558
iteration : 2287
train acc:  0.8203125
train loss:  0.3984817862510681
train gradient:  0.3539156383929164
iteration : 2288
train acc:  0.7890625
train loss:  0.3836517333984375
train gradient:  0.3553218262721367
iteration : 2289
train acc:  0.8359375
train loss:  0.36886143684387207
train gradient:  0.32577747384776506
iteration : 2290
train acc:  0.8515625
train loss:  0.4011955261230469
train gradient:  0.43636408988046554
iteration : 2291
train acc:  0.8125
train loss:  0.3772042393684387
train gradient:  0.42625789581096846
iteration : 2292
train acc:  0.8203125
train loss:  0.40212827920913696
train gradient:  0.3021426429030438
iteration : 2293
train acc:  0.8359375
train loss:  0.3953174650669098
train gradient:  0.2548461200400302
iteration : 2294
train acc:  0.8046875
train loss:  0.39428240060806274
train gradient:  0.303504722614041
iteration : 2295
train acc:  0.8359375
train loss:  0.430711030960083
train gradient:  0.5032324595608564
iteration : 2296
train acc:  0.796875
train loss:  0.41899406909942627
train gradient:  0.5198034581770912
iteration : 2297
train acc:  0.84375
train loss:  0.39405208826065063
train gradient:  0.4097564819562117
iteration : 2298
train acc:  0.8671875
train loss:  0.3477727770805359
train gradient:  0.2442200076783006
iteration : 2299
train acc:  0.8125
train loss:  0.44751590490341187
train gradient:  0.4020893337729319
iteration : 2300
train acc:  0.828125
train loss:  0.39475852251052856
train gradient:  0.27904351877244293
iteration : 2301
train acc:  0.796875
train loss:  0.4341154396533966
train gradient:  0.40301528401706094
iteration : 2302
train acc:  0.828125
train loss:  0.3748379349708557
train gradient:  0.2641148815816299
iteration : 2303
train acc:  0.7890625
train loss:  0.4348679482936859
train gradient:  0.3210202979872236
iteration : 2304
train acc:  0.7890625
train loss:  0.4051254689693451
train gradient:  0.2807093472781785
iteration : 2305
train acc:  0.78125
train loss:  0.44676363468170166
train gradient:  0.6809514221689525
iteration : 2306
train acc:  0.859375
train loss:  0.3367232382297516
train gradient:  0.24304676387175955
iteration : 2307
train acc:  0.8203125
train loss:  0.4005642533302307
train gradient:  0.5465522506582499
iteration : 2308
train acc:  0.8125
train loss:  0.3973352313041687
train gradient:  0.36373105550281837
iteration : 2309
train acc:  0.7734375
train loss:  0.49663230776786804
train gradient:  0.6487628285818947
iteration : 2310
train acc:  0.796875
train loss:  0.4952014684677124
train gradient:  0.5113405329145906
iteration : 2311
train acc:  0.7890625
train loss:  0.4213404655456543
train gradient:  0.4044229698592859
iteration : 2312
train acc:  0.8359375
train loss:  0.3662455379962921
train gradient:  0.3374608914170918
iteration : 2313
train acc:  0.8515625
train loss:  0.36832141876220703
train gradient:  0.33167012309655675
iteration : 2314
train acc:  0.7734375
train loss:  0.4484695792198181
train gradient:  0.4052673014487152
iteration : 2315
train acc:  0.7890625
train loss:  0.41347023844718933
train gradient:  0.3923592544766407
iteration : 2316
train acc:  0.8359375
train loss:  0.4046739339828491
train gradient:  0.33944182134163675
iteration : 2317
train acc:  0.7421875
train loss:  0.4879418611526489
train gradient:  0.4130032524983775
iteration : 2318
train acc:  0.875
train loss:  0.3832973837852478
train gradient:  0.25542712505791015
iteration : 2319
train acc:  0.7890625
train loss:  0.4412010908126831
train gradient:  0.34358132932844304
iteration : 2320
train acc:  0.7890625
train loss:  0.4594506621360779
train gradient:  0.3837091108195238
iteration : 2321
train acc:  0.796875
train loss:  0.40260493755340576
train gradient:  0.30293374876743406
iteration : 2322
train acc:  0.8125
train loss:  0.4596649408340454
train gradient:  0.37505656954144073
iteration : 2323
train acc:  0.828125
train loss:  0.44040486216545105
train gradient:  0.3303477520569938
iteration : 2324
train acc:  0.828125
train loss:  0.37732163071632385
train gradient:  0.29343334129010656
iteration : 2325
train acc:  0.828125
train loss:  0.40510833263397217
train gradient:  0.4570172709030116
iteration : 2326
train acc:  0.828125
train loss:  0.4438219666481018
train gradient:  0.5143300966069166
iteration : 2327
train acc:  0.7734375
train loss:  0.4621180295944214
train gradient:  0.38623734031571044
iteration : 2328
train acc:  0.8515625
train loss:  0.36579206585884094
train gradient:  0.321854317203578
iteration : 2329
train acc:  0.7578125
train loss:  0.5026639699935913
train gradient:  0.4298389512448147
iteration : 2330
train acc:  0.8046875
train loss:  0.42228659987449646
train gradient:  0.34431850824216675
iteration : 2331
train acc:  0.8046875
train loss:  0.4338099956512451
train gradient:  0.3806395575731651
iteration : 2332
train acc:  0.84375
train loss:  0.35111138224601746
train gradient:  0.36104287224925774
iteration : 2333
train acc:  0.8203125
train loss:  0.4062504172325134
train gradient:  0.3157293140480357
iteration : 2334
train acc:  0.8203125
train loss:  0.43064630031585693
train gradient:  0.3246033632645967
iteration : 2335
train acc:  0.7890625
train loss:  0.4242231249809265
train gradient:  0.45456072402950903
iteration : 2336
train acc:  0.8671875
train loss:  0.3323744535446167
train gradient:  0.2731301679803614
iteration : 2337
train acc:  0.8203125
train loss:  0.3799309730529785
train gradient:  0.33437837168285217
iteration : 2338
train acc:  0.796875
train loss:  0.41808372735977173
train gradient:  0.5293804585953739
iteration : 2339
train acc:  0.765625
train loss:  0.4796697497367859
train gradient:  0.3849175540204505
iteration : 2340
train acc:  0.8203125
train loss:  0.41509324312210083
train gradient:  0.3843838763850324
iteration : 2341
train acc:  0.8046875
train loss:  0.3678031861782074
train gradient:  0.4560880643095823
iteration : 2342
train acc:  0.796875
train loss:  0.4617399573326111
train gradient:  0.6013211558409792
iteration : 2343
train acc:  0.828125
train loss:  0.41334718465805054
train gradient:  0.545720449348861
iteration : 2344
train acc:  0.78125
train loss:  0.3862817883491516
train gradient:  0.28578875474738186
iteration : 2345
train acc:  0.921875
train loss:  0.3112882673740387
train gradient:  0.28581339097982167
iteration : 2346
train acc:  0.8203125
train loss:  0.38545936346054077
train gradient:  0.36381726405185927
iteration : 2347
train acc:  0.7890625
train loss:  0.42098477482795715
train gradient:  0.3881953257699039
iteration : 2348
train acc:  0.75
train loss:  0.4901737570762634
train gradient:  0.5719967395706336
iteration : 2349
train acc:  0.8046875
train loss:  0.44622254371643066
train gradient:  0.5193828570986413
iteration : 2350
train acc:  0.7890625
train loss:  0.404529869556427
train gradient:  0.42684206212180303
iteration : 2351
train acc:  0.78125
train loss:  0.42937856912612915
train gradient:  0.4375421702899011
iteration : 2352
train acc:  0.765625
train loss:  0.44978490471839905
train gradient:  0.4779056459894103
iteration : 2353
train acc:  0.7890625
train loss:  0.4223951995372772
train gradient:  0.48492773961097935
iteration : 2354
train acc:  0.8203125
train loss:  0.40750038623809814
train gradient:  0.5458065200219719
iteration : 2355
train acc:  0.8125
train loss:  0.419549822807312
train gradient:  0.408306455611958
iteration : 2356
train acc:  0.859375
train loss:  0.37798815965652466
train gradient:  0.4615817473434151
iteration : 2357
train acc:  0.8203125
train loss:  0.37964028120040894
train gradient:  0.3721935980769406
iteration : 2358
train acc:  0.859375
train loss:  0.3710711598396301
train gradient:  0.30685054412444485
iteration : 2359
train acc:  0.8046875
train loss:  0.35412460565567017
train gradient:  0.2878759822603395
iteration : 2360
train acc:  0.8359375
train loss:  0.3787282109260559
train gradient:  0.3291412372429578
iteration : 2361
train acc:  0.796875
train loss:  0.39463233947753906
train gradient:  0.37365504773343783
iteration : 2362
train acc:  0.8984375
train loss:  0.3186241388320923
train gradient:  0.23265350981274344
iteration : 2363
train acc:  0.8203125
train loss:  0.39664125442504883
train gradient:  0.42367773445501855
iteration : 2364
train acc:  0.8203125
train loss:  0.424283504486084
train gradient:  0.40820370412231094
iteration : 2365
train acc:  0.8515625
train loss:  0.3837505578994751
train gradient:  0.36389507039669605
iteration : 2366
train acc:  0.8359375
train loss:  0.3450089693069458
train gradient:  0.26679414874387086
iteration : 2367
train acc:  0.859375
train loss:  0.35563361644744873
train gradient:  0.34984367062785554
iteration : 2368
train acc:  0.84375
train loss:  0.35716864466667175
train gradient:  0.35915175783774234
iteration : 2369
train acc:  0.8203125
train loss:  0.4205009937286377
train gradient:  0.4621679657635419
iteration : 2370
train acc:  0.8359375
train loss:  0.38949108123779297
train gradient:  0.5070603216228842
iteration : 2371
train acc:  0.828125
train loss:  0.37693965435028076
train gradient:  0.3600088579208748
iteration : 2372
train acc:  0.828125
train loss:  0.3582940101623535
train gradient:  0.31414219783519215
iteration : 2373
train acc:  0.8046875
train loss:  0.36198702454566956
train gradient:  0.2798935126717264
iteration : 2374
train acc:  0.84375
train loss:  0.37593281269073486
train gradient:  0.3316515090104849
iteration : 2375
train acc:  0.8046875
train loss:  0.4605399966239929
train gradient:  0.557604488871441
iteration : 2376
train acc:  0.828125
train loss:  0.3846137821674347
train gradient:  0.3406783013838677
iteration : 2377
train acc:  0.8671875
train loss:  0.3089209794998169
train gradient:  0.3182080877357492
iteration : 2378
train acc:  0.8828125
train loss:  0.3053823411464691
train gradient:  0.21877561661001216
iteration : 2379
train acc:  0.8359375
train loss:  0.3339468240737915
train gradient:  0.31611582206958916
iteration : 2380
train acc:  0.7890625
train loss:  0.3927578330039978
train gradient:  0.33842586270151725
iteration : 2381
train acc:  0.8359375
train loss:  0.3675183355808258
train gradient:  0.43962643440553095
iteration : 2382
train acc:  0.7890625
train loss:  0.47015219926834106
train gradient:  0.5006234971982915
iteration : 2383
train acc:  0.875
train loss:  0.3214292526245117
train gradient:  0.251422959096523
iteration : 2384
train acc:  0.8046875
train loss:  0.4374932050704956
train gradient:  0.40550277061752726
iteration : 2385
train acc:  0.765625
train loss:  0.5133002996444702
train gradient:  0.6067849538275888
iteration : 2386
train acc:  0.7890625
train loss:  0.4166240692138672
train gradient:  0.43950369054356975
iteration : 2387
train acc:  0.8515625
train loss:  0.40603119134902954
train gradient:  0.3290548781373911
iteration : 2388
train acc:  0.828125
train loss:  0.36589866876602173
train gradient:  0.3244108851125795
iteration : 2389
train acc:  0.8046875
train loss:  0.4674633741378784
train gradient:  0.3412509834662366
iteration : 2390
train acc:  0.8671875
train loss:  0.35297271609306335
train gradient:  0.33478789815407456
iteration : 2391
train acc:  0.84375
train loss:  0.3841477632522583
train gradient:  0.3226866447139211
iteration : 2392
train acc:  0.828125
train loss:  0.332567036151886
train gradient:  0.2524800264794274
iteration : 2393
train acc:  0.84375
train loss:  0.3532995581626892
train gradient:  0.29244947483635353
iteration : 2394
train acc:  0.828125
train loss:  0.3846395015716553
train gradient:  0.4241803367920905
iteration : 2395
train acc:  0.84375
train loss:  0.3751713037490845
train gradient:  0.3213820795910727
iteration : 2396
train acc:  0.8515625
train loss:  0.357992947101593
train gradient:  0.2975682960929269
iteration : 2397
train acc:  0.8515625
train loss:  0.35487207770347595
train gradient:  0.26608674499462315
iteration : 2398
train acc:  0.8203125
train loss:  0.39567017555236816
train gradient:  0.4353616699418482
iteration : 2399
train acc:  0.8203125
train loss:  0.3482048511505127
train gradient:  0.4906496530737847
iteration : 2400
train acc:  0.875
train loss:  0.3517417311668396
train gradient:  0.30191683047153317
iteration : 2401
train acc:  0.75
train loss:  0.47625917196273804
train gradient:  0.5046931327239657
iteration : 2402
train acc:  0.7578125
train loss:  0.520614504814148
train gradient:  0.6121451902344829
iteration : 2403
train acc:  0.8046875
train loss:  0.4313630759716034
train gradient:  0.35067769901118007
iteration : 2404
train acc:  0.796875
train loss:  0.3848525881767273
train gradient:  0.34764131013484406
iteration : 2405
train acc:  0.828125
train loss:  0.40195295214653015
train gradient:  0.36294294996956317
iteration : 2406
train acc:  0.75
train loss:  0.4442979097366333
train gradient:  0.5114414554466677
iteration : 2407
train acc:  0.828125
train loss:  0.3581699728965759
train gradient:  0.382978528314324
iteration : 2408
train acc:  0.8046875
train loss:  0.5195674896240234
train gradient:  0.4531454572888072
iteration : 2409
train acc:  0.7578125
train loss:  0.5367662906646729
train gradient:  0.5453133901666424
iteration : 2410
train acc:  0.8046875
train loss:  0.39720767736434937
train gradient:  0.7259554020252833
iteration : 2411
train acc:  0.8359375
train loss:  0.37470436096191406
train gradient:  0.38287997246380706
iteration : 2412
train acc:  0.875
train loss:  0.3347131907939911
train gradient:  0.28131410841347637
iteration : 2413
train acc:  0.796875
train loss:  0.4381920099258423
train gradient:  0.4657527573188668
iteration : 2414
train acc:  0.7890625
train loss:  0.41795340180397034
train gradient:  0.34995907657173997
iteration : 2415
train acc:  0.859375
train loss:  0.29406481981277466
train gradient:  0.21734620560125162
iteration : 2416
train acc:  0.7734375
train loss:  0.4532816410064697
train gradient:  0.45085018815889594
iteration : 2417
train acc:  0.8125
train loss:  0.37764111161231995
train gradient:  0.34641722802747776
iteration : 2418
train acc:  0.765625
train loss:  0.48088929057121277
train gradient:  0.48680014946942873
iteration : 2419
train acc:  0.796875
train loss:  0.37408068776130676
train gradient:  0.37841115934121183
iteration : 2420
train acc:  0.8125
train loss:  0.3738093376159668
train gradient:  0.4824138560665527
iteration : 2421
train acc:  0.7890625
train loss:  0.4342411160469055
train gradient:  0.4414569437974478
iteration : 2422
train acc:  0.8359375
train loss:  0.3590138256549835
train gradient:  0.2612694417141513
iteration : 2423
train acc:  0.796875
train loss:  0.39997386932373047
train gradient:  0.3101899814857147
iteration : 2424
train acc:  0.8125
train loss:  0.44093966484069824
train gradient:  0.4007878888025853
iteration : 2425
train acc:  0.7578125
train loss:  0.4714328944683075
train gradient:  0.37469546370742424
iteration : 2426
train acc:  0.8359375
train loss:  0.38140296936035156
train gradient:  0.33659831326471734
iteration : 2427
train acc:  0.84375
train loss:  0.3590152859687805
train gradient:  0.22043757697730726
iteration : 2428
train acc:  0.7578125
train loss:  0.47838491201400757
train gradient:  0.5058330641381604
iteration : 2429
train acc:  0.84375
train loss:  0.38841989636421204
train gradient:  0.30259696996376273
iteration : 2430
train acc:  0.8359375
train loss:  0.42491239309310913
train gradient:  0.28852237134621544
iteration : 2431
train acc:  0.8515625
train loss:  0.35221290588378906
train gradient:  0.2509207088980592
iteration : 2432
train acc:  0.875
train loss:  0.3203980624675751
train gradient:  0.21685815774230893
iteration : 2433
train acc:  0.78125
train loss:  0.42788317799568176
train gradient:  0.3581447073857887
iteration : 2434
train acc:  0.8828125
train loss:  0.3043041229248047
train gradient:  0.27079816515692184
iteration : 2435
train acc:  0.78125
train loss:  0.4391269385814667
train gradient:  0.3377219253522064
iteration : 2436
train acc:  0.8671875
train loss:  0.32391121983528137
train gradient:  0.3405959454998583
iteration : 2437
train acc:  0.8046875
train loss:  0.44650447368621826
train gradient:  0.39207636595021106
iteration : 2438
train acc:  0.8671875
train loss:  0.3132537603378296
train gradient:  0.2818905563397905
iteration : 2439
train acc:  0.796875
train loss:  0.3882535398006439
train gradient:  0.4051850427513447
iteration : 2440
train acc:  0.8125
train loss:  0.4443841874599457
train gradient:  0.4111459808688171
iteration : 2441
train acc:  0.875
train loss:  0.34205207228660583
train gradient:  0.439763785921738
iteration : 2442
train acc:  0.8359375
train loss:  0.43931844830513
train gradient:  0.3510711170505067
iteration : 2443
train acc:  0.796875
train loss:  0.39313507080078125
train gradient:  0.4978699555698546
iteration : 2444
train acc:  0.8203125
train loss:  0.38243502378463745
train gradient:  0.3740686241128959
iteration : 2445
train acc:  0.8203125
train loss:  0.42017143964767456
train gradient:  0.37152720988382304
iteration : 2446
train acc:  0.7890625
train loss:  0.4882364571094513
train gradient:  0.4330555906504827
iteration : 2447
train acc:  0.8125
train loss:  0.41510701179504395
train gradient:  0.3765457248557455
iteration : 2448
train acc:  0.8359375
train loss:  0.4207068681716919
train gradient:  0.4541719002585352
iteration : 2449
train acc:  0.859375
train loss:  0.36876195669174194
train gradient:  0.5469036500518573
iteration : 2450
train acc:  0.78125
train loss:  0.43288934230804443
train gradient:  0.4662914928629554
iteration : 2451
train acc:  0.8515625
train loss:  0.35188889503479004
train gradient:  0.2317482105924704
iteration : 2452
train acc:  0.828125
train loss:  0.35524982213974
train gradient:  0.23001231545307566
iteration : 2453
train acc:  0.8671875
train loss:  0.36285027861595154
train gradient:  0.27509258495767885
iteration : 2454
train acc:  0.859375
train loss:  0.36894679069519043
train gradient:  0.32508042927646813
iteration : 2455
train acc:  0.875
train loss:  0.30660343170166016
train gradient:  0.217260988645204
iteration : 2456
train acc:  0.796875
train loss:  0.4449538290500641
train gradient:  0.355601907616833
iteration : 2457
train acc:  0.78125
train loss:  0.4618246555328369
train gradient:  0.4882101621661064
iteration : 2458
train acc:  0.7578125
train loss:  0.4620107114315033
train gradient:  0.46677085719932726
iteration : 2459
train acc:  0.8125
train loss:  0.4347391128540039
train gradient:  0.3654150480401946
iteration : 2460
train acc:  0.796875
train loss:  0.40985608100891113
train gradient:  0.4222343560191222
iteration : 2461
train acc:  0.8203125
train loss:  0.35984566807746887
train gradient:  0.24776773416513284
iteration : 2462
train acc:  0.7890625
train loss:  0.4599314332008362
train gradient:  0.5119660972090163
iteration : 2463
train acc:  0.8046875
train loss:  0.4100309908390045
train gradient:  0.3416866223303959
iteration : 2464
train acc:  0.828125
train loss:  0.4030042290687561
train gradient:  0.40070868868345816
iteration : 2465
train acc:  0.828125
train loss:  0.3921207785606384
train gradient:  0.4080586329402882
iteration : 2466
train acc:  0.796875
train loss:  0.41341254115104675
train gradient:  0.6033659400699145
iteration : 2467
train acc:  0.75
train loss:  0.45837709307670593
train gradient:  0.4660423374614892
iteration : 2468
train acc:  0.875
train loss:  0.3679639995098114
train gradient:  0.36174887785575294
iteration : 2469
train acc:  0.7890625
train loss:  0.416099488735199
train gradient:  0.31202987402799126
iteration : 2470
train acc:  0.8671875
train loss:  0.34292978048324585
train gradient:  0.23242004831786528
iteration : 2471
train acc:  0.8125
train loss:  0.4149646461009979
train gradient:  0.3377992895546584
iteration : 2472
train acc:  0.8125
train loss:  0.37901386618614197
train gradient:  0.3398884041688721
iteration : 2473
train acc:  0.8125
train loss:  0.3921688497066498
train gradient:  0.3192212914456968
iteration : 2474
train acc:  0.75
train loss:  0.42119091749191284
train gradient:  0.35558920613179806
iteration : 2475
train acc:  0.7890625
train loss:  0.46364009380340576
train gradient:  0.5212627778175557
iteration : 2476
train acc:  0.765625
train loss:  0.4436078667640686
train gradient:  0.4044358300426363
iteration : 2477
train acc:  0.84375
train loss:  0.40246832370758057
train gradient:  0.29165371799864737
iteration : 2478
train acc:  0.8203125
train loss:  0.44007107615470886
train gradient:  0.36830423122551276
iteration : 2479
train acc:  0.828125
train loss:  0.40681856870651245
train gradient:  0.27984452837295426
iteration : 2480
train acc:  0.890625
train loss:  0.3149370551109314
train gradient:  0.29946617956306915
iteration : 2481
train acc:  0.8046875
train loss:  0.46370700001716614
train gradient:  0.610589492640606
iteration : 2482
train acc:  0.859375
train loss:  0.3835544288158417
train gradient:  0.3595198362898453
iteration : 2483
train acc:  0.8359375
train loss:  0.3632102608680725
train gradient:  0.3154385071827319
iteration : 2484
train acc:  0.84375
train loss:  0.3446306586265564
train gradient:  0.2927809043410419
iteration : 2485
train acc:  0.84375
train loss:  0.3924350142478943
train gradient:  0.39852319676688924
iteration : 2486
train acc:  0.84375
train loss:  0.37856972217559814
train gradient:  0.42484654916137377
iteration : 2487
train acc:  0.890625
train loss:  0.3256438970565796
train gradient:  0.28871864549544557
iteration : 2488
train acc:  0.7734375
train loss:  0.45549899339675903
train gradient:  0.3656982082408983
iteration : 2489
train acc:  0.796875
train loss:  0.4615667462348938
train gradient:  0.6538059797866049
iteration : 2490
train acc:  0.796875
train loss:  0.42577800154685974
train gradient:  0.38140148678180674
iteration : 2491
train acc:  0.8125
train loss:  0.3836633563041687
train gradient:  0.3243791133332092
iteration : 2492
train acc:  0.796875
train loss:  0.4391604959964752
train gradient:  0.3529037025938194
iteration : 2493
train acc:  0.8203125
train loss:  0.3934471011161804
train gradient:  0.33733596930682735
iteration : 2494
train acc:  0.8046875
train loss:  0.42320218682289124
train gradient:  0.7498966097521997
iteration : 2495
train acc:  0.796875
train loss:  0.46838247776031494
train gradient:  0.44334942977456526
iteration : 2496
train acc:  0.8203125
train loss:  0.3322923481464386
train gradient:  0.2663591456617442
iteration : 2497
train acc:  0.8046875
train loss:  0.4121759533882141
train gradient:  0.3785049418770465
iteration : 2498
train acc:  0.859375
train loss:  0.33963149785995483
train gradient:  0.2426849427064794
iteration : 2499
train acc:  0.8359375
train loss:  0.36217087507247925
train gradient:  0.29456983065708
iteration : 2500
train acc:  0.78125
train loss:  0.4224343001842499
train gradient:  0.36914746514954566
iteration : 2501
train acc:  0.7890625
train loss:  0.45612597465515137
train gradient:  0.45906914907047386
iteration : 2502
train acc:  0.8125
train loss:  0.38010627031326294
train gradient:  0.303282351637131
iteration : 2503
train acc:  0.8828125
train loss:  0.3133496046066284
train gradient:  0.2514881794313461
iteration : 2504
train acc:  0.828125
train loss:  0.381858766078949
train gradient:  0.3784928745409357
iteration : 2505
train acc:  0.859375
train loss:  0.36457955837249756
train gradient:  0.24797995057176997
iteration : 2506
train acc:  0.796875
train loss:  0.4706212878227234
train gradient:  0.49277237688868497
iteration : 2507
train acc:  0.7578125
train loss:  0.41888895630836487
train gradient:  0.3745626179051763
iteration : 2508
train acc:  0.8203125
train loss:  0.3981474041938782
train gradient:  0.29159113589434177
iteration : 2509
train acc:  0.84375
train loss:  0.32887279987335205
train gradient:  0.3574728751120319
iteration : 2510
train acc:  0.8515625
train loss:  0.35520461201667786
train gradient:  0.34506532310853777
iteration : 2511
train acc:  0.8671875
train loss:  0.35497117042541504
train gradient:  0.3142284809193997
iteration : 2512
train acc:  0.890625
train loss:  0.30937665700912476
train gradient:  0.27077296745325413
iteration : 2513
train acc:  0.78125
train loss:  0.5185791254043579
train gradient:  0.9227333878979275
iteration : 2514
train acc:  0.8046875
train loss:  0.41812828183174133
train gradient:  0.33792185460027363
iteration : 2515
train acc:  0.796875
train loss:  0.41252002120018005
train gradient:  0.3904206488371651
iteration : 2516
train acc:  0.8046875
train loss:  0.38685178756713867
train gradient:  0.4129730014058781
iteration : 2517
train acc:  0.828125
train loss:  0.40859007835388184
train gradient:  0.31969653896818
iteration : 2518
train acc:  0.8125
train loss:  0.39261436462402344
train gradient:  0.4572648635592105
iteration : 2519
train acc:  0.8828125
train loss:  0.32926318049430847
train gradient:  0.3024355831856636
iteration : 2520
train acc:  0.78125
train loss:  0.39697355031967163
train gradient:  0.3946360607164129
iteration : 2521
train acc:  0.8359375
train loss:  0.3442409634590149
train gradient:  0.42143132425677327
iteration : 2522
train acc:  0.8203125
train loss:  0.3455614447593689
train gradient:  0.2884781375117633
iteration : 2523
train acc:  0.8125
train loss:  0.372154176235199
train gradient:  0.33727523305075763
iteration : 2524
train acc:  0.8359375
train loss:  0.31254804134368896
train gradient:  0.3486666187279677
iteration : 2525
train acc:  0.8984375
train loss:  0.3589766025543213
train gradient:  0.3419974755419908
iteration : 2526
train acc:  0.84375
train loss:  0.3602864742279053
train gradient:  0.2551671333792905
iteration : 2527
train acc:  0.859375
train loss:  0.3416758179664612
train gradient:  0.29339895076606975
iteration : 2528
train acc:  0.84375
train loss:  0.34542587399482727
train gradient:  0.2472762295524209
iteration : 2529
train acc:  0.8125
train loss:  0.4638333022594452
train gradient:  0.5696865516374536
iteration : 2530
train acc:  0.7578125
train loss:  0.5138585567474365
train gradient:  0.5002357694370327
iteration : 2531
train acc:  0.7890625
train loss:  0.4020981788635254
train gradient:  0.5504927873637848
iteration : 2532
train acc:  0.8359375
train loss:  0.4294184744358063
train gradient:  0.41926741384860816
iteration : 2533
train acc:  0.875
train loss:  0.3219088315963745
train gradient:  0.21423893436900399
iteration : 2534
train acc:  0.8515625
train loss:  0.3239423930644989
train gradient:  0.29220801697375953
iteration : 2535
train acc:  0.8125
train loss:  0.4249415993690491
train gradient:  0.2903011220898327
iteration : 2536
train acc:  0.8203125
train loss:  0.3920234143733978
train gradient:  0.4251202434348778
iteration : 2537
train acc:  0.859375
train loss:  0.37147441506385803
train gradient:  0.33936763818728616
iteration : 2538
train acc:  0.8046875
train loss:  0.4682900905609131
train gradient:  0.43091622202066265
iteration : 2539
train acc:  0.84375
train loss:  0.4359855055809021
train gradient:  0.6136637934691229
iteration : 2540
train acc:  0.8515625
train loss:  0.3144809901714325
train gradient:  0.26671377878843205
iteration : 2541
train acc:  0.8515625
train loss:  0.3588328957557678
train gradient:  0.308696172512267
iteration : 2542
train acc:  0.828125
train loss:  0.4023290276527405
train gradient:  0.37395399967686516
iteration : 2543
train acc:  0.8125
train loss:  0.4084334075450897
train gradient:  0.4168902617834622
iteration : 2544
train acc:  0.875
train loss:  0.3216109871864319
train gradient:  0.21382864943750896
iteration : 2545
train acc:  0.8671875
train loss:  0.34542810916900635
train gradient:  0.2724113610904223
iteration : 2546
train acc:  0.7890625
train loss:  0.40465813875198364
train gradient:  0.29611218100383463
iteration : 2547
train acc:  0.8671875
train loss:  0.3565236032009125
train gradient:  0.25122862518936995
iteration : 2548
train acc:  0.8203125
train loss:  0.43477416038513184
train gradient:  0.40369881460868623
iteration : 2549
train acc:  0.84375
train loss:  0.3800150156021118
train gradient:  0.3078294061877768
iteration : 2550
train acc:  0.8046875
train loss:  0.38592007756233215
train gradient:  0.33468293584210435
iteration : 2551
train acc:  0.796875
train loss:  0.42925387620925903
train gradient:  0.3833707076653708
iteration : 2552
train acc:  0.78125
train loss:  0.42914193868637085
train gradient:  0.39046403104537236
iteration : 2553
train acc:  0.828125
train loss:  0.3992317318916321
train gradient:  0.4223990397138418
iteration : 2554
train acc:  0.8046875
train loss:  0.43823814392089844
train gradient:  0.4757937691603324
iteration : 2555
train acc:  0.8046875
train loss:  0.3920140266418457
train gradient:  0.26402056643972177
iteration : 2556
train acc:  0.78125
train loss:  0.4698340892791748
train gradient:  0.35510646582186384
iteration : 2557
train acc:  0.84375
train loss:  0.45651179552078247
train gradient:  0.3917635183090677
iteration : 2558
train acc:  0.8671875
train loss:  0.348057359457016
train gradient:  0.23787938947024856
iteration : 2559
train acc:  0.84375
train loss:  0.41833245754241943
train gradient:  0.4499469556638627
iteration : 2560
train acc:  0.8125
train loss:  0.38043415546417236
train gradient:  0.319221114934567
iteration : 2561
train acc:  0.828125
train loss:  0.37954360246658325
train gradient:  0.3250976135186361
iteration : 2562
train acc:  0.8125
train loss:  0.4060492515563965
train gradient:  0.3296613722015351
iteration : 2563
train acc:  0.84375
train loss:  0.36298948526382446
train gradient:  0.3381993721785156
iteration : 2564
train acc:  0.828125
train loss:  0.3943727910518646
train gradient:  0.339066731990145
iteration : 2565
train acc:  0.796875
train loss:  0.4239724576473236
train gradient:  0.5336288726717866
iteration : 2566
train acc:  0.8125
train loss:  0.41519051790237427
train gradient:  0.33891146911170345
iteration : 2567
train acc:  0.84375
train loss:  0.38988935947418213
train gradient:  0.3301249811704575
iteration : 2568
train acc:  0.875
train loss:  0.3397960960865021
train gradient:  0.3417860338535249
iteration : 2569
train acc:  0.828125
train loss:  0.3954692482948303
train gradient:  0.29370696605263324
iteration : 2570
train acc:  0.828125
train loss:  0.3236626386642456
train gradient:  0.3965256545925824
iteration : 2571
train acc:  0.8671875
train loss:  0.33309856057167053
train gradient:  0.2414333290002229
iteration : 2572
train acc:  0.8125
train loss:  0.4680587649345398
train gradient:  0.6213680491930128
iteration : 2573
train acc:  0.84375
train loss:  0.3682674467563629
train gradient:  0.33349696602424783
iteration : 2574
train acc:  0.828125
train loss:  0.3854024410247803
train gradient:  0.34028164871513195
iteration : 2575
train acc:  0.84375
train loss:  0.3978675305843353
train gradient:  0.4451183609483194
iteration : 2576
train acc:  0.8515625
train loss:  0.30917131900787354
train gradient:  0.26424265351802856
iteration : 2577
train acc:  0.8046875
train loss:  0.4198497235774994
train gradient:  0.4891263526330273
iteration : 2578
train acc:  0.8515625
train loss:  0.3762328028678894
train gradient:  0.341275136476932
iteration : 2579
train acc:  0.8515625
train loss:  0.3538402318954468
train gradient:  0.29984199673537837
iteration : 2580
train acc:  0.7890625
train loss:  0.4687919616699219
train gradient:  0.44135652531984193
iteration : 2581
train acc:  0.7578125
train loss:  0.49328407645225525
train gradient:  0.486984880598554
iteration : 2582
train acc:  0.8046875
train loss:  0.41392290592193604
train gradient:  0.3708138981967118
iteration : 2583
train acc:  0.8203125
train loss:  0.4026316702365875
train gradient:  0.40907542631818633
iteration : 2584
train acc:  0.796875
train loss:  0.4115341305732727
train gradient:  0.3227983125098709
iteration : 2585
train acc:  0.8125
train loss:  0.4654581546783447
train gradient:  0.5845172391237448
iteration : 2586
train acc:  0.828125
train loss:  0.3472457230091095
train gradient:  0.530070996673873
iteration : 2587
train acc:  0.8125
train loss:  0.40476953983306885
train gradient:  0.34859059965795003
iteration : 2588
train acc:  0.8515625
train loss:  0.34289926290512085
train gradient:  0.3144956701272675
iteration : 2589
train acc:  0.8125
train loss:  0.4674873352050781
train gradient:  0.44827266522139814
iteration : 2590
train acc:  0.7890625
train loss:  0.40018582344055176
train gradient:  0.39855949380319733
iteration : 2591
train acc:  0.8671875
train loss:  0.31789183616638184
train gradient:  0.36126217390686854
iteration : 2592
train acc:  0.7734375
train loss:  0.4323619604110718
train gradient:  0.41962752356152117
iteration : 2593
train acc:  0.8515625
train loss:  0.33118149638175964
train gradient:  0.26676317637372376
iteration : 2594
train acc:  0.796875
train loss:  0.3922310173511505
train gradient:  0.3555065322450236
iteration : 2595
train acc:  0.8125
train loss:  0.399858295917511
train gradient:  0.31801155308125123
iteration : 2596
train acc:  0.78125
train loss:  0.43481943011283875
train gradient:  0.41886498532310984
iteration : 2597
train acc:  0.8203125
train loss:  0.3787655532360077
train gradient:  0.27243506386472816
iteration : 2598
train acc:  0.890625
train loss:  0.30228501558303833
train gradient:  0.20349300152931732
iteration : 2599
train acc:  0.8203125
train loss:  0.4284771680831909
train gradient:  0.467105032019606
iteration : 2600
train acc:  0.7890625
train loss:  0.3882622718811035
train gradient:  0.4460141239342193
iteration : 2601
train acc:  0.8515625
train loss:  0.3701770305633545
train gradient:  0.35864956406112697
iteration : 2602
train acc:  0.796875
train loss:  0.41598591208457947
train gradient:  0.51271675205621
iteration : 2603
train acc:  0.8203125
train loss:  0.34690824151039124
train gradient:  0.2630111351298489
iteration : 2604
train acc:  0.8671875
train loss:  0.36945170164108276
train gradient:  0.3864798703823249
iteration : 2605
train acc:  0.828125
train loss:  0.37907809019088745
train gradient:  0.3826313542623316
iteration : 2606
train acc:  0.75
train loss:  0.4292565882205963
train gradient:  0.36134860442979977
iteration : 2607
train acc:  0.8359375
train loss:  0.3695099353790283
train gradient:  0.36685110325307885
iteration : 2608
train acc:  0.8046875
train loss:  0.4124952554702759
train gradient:  0.4026505222097577
iteration : 2609
train acc:  0.84375
train loss:  0.3606444001197815
train gradient:  0.2911654681779349
iteration : 2610
train acc:  0.8359375
train loss:  0.36111634969711304
train gradient:  0.3429638339821413
iteration : 2611
train acc:  0.875
train loss:  0.30370116233825684
train gradient:  0.27651797807756007
iteration : 2612
train acc:  0.7578125
train loss:  0.5212299823760986
train gradient:  0.621420855493277
iteration : 2613
train acc:  0.8359375
train loss:  0.36697059869766235
train gradient:  0.3009933348616295
iteration : 2614
train acc:  0.8046875
train loss:  0.45096340775489807
train gradient:  0.3868801008058116
iteration : 2615
train acc:  0.7890625
train loss:  0.4234955608844757
train gradient:  0.310013004843505
iteration : 2616
train acc:  0.84375
train loss:  0.36851149797439575
train gradient:  0.3462900864024111
iteration : 2617
train acc:  0.859375
train loss:  0.35227686166763306
train gradient:  0.3255159832849238
iteration : 2618
train acc:  0.828125
train loss:  0.4414748549461365
train gradient:  0.7218379747317483
iteration : 2619
train acc:  0.8359375
train loss:  0.3919306993484497
train gradient:  0.3168185214196009
iteration : 2620
train acc:  0.7890625
train loss:  0.4660041332244873
train gradient:  0.42798673038841145
iteration : 2621
train acc:  0.8125
train loss:  0.392993688583374
train gradient:  0.3561459850589568
iteration : 2622
train acc:  0.8046875
train loss:  0.3659474849700928
train gradient:  0.34036964106952616
iteration : 2623
train acc:  0.8671875
train loss:  0.3250223398208618
train gradient:  0.3216717839659057
iteration : 2624
train acc:  0.8203125
train loss:  0.44749242067337036
train gradient:  0.38887590415739687
iteration : 2625
train acc:  0.7578125
train loss:  0.48624497652053833
train gradient:  0.4415700385653786
iteration : 2626
train acc:  0.8515625
train loss:  0.3900856375694275
train gradient:  0.4871125602313259
iteration : 2627
train acc:  0.828125
train loss:  0.38267555832862854
train gradient:  0.394055522090196
iteration : 2628
train acc:  0.8359375
train loss:  0.38250648975372314
train gradient:  0.3576612181699317
iteration : 2629
train acc:  0.8125
train loss:  0.39158332347869873
train gradient:  0.39454637593636793
iteration : 2630
train acc:  0.828125
train loss:  0.3560580015182495
train gradient:  0.2652755049682919
iteration : 2631
train acc:  0.8359375
train loss:  0.34881407022476196
train gradient:  0.25631893288489366
iteration : 2632
train acc:  0.84375
train loss:  0.36685192584991455
train gradient:  0.3167674463405061
iteration : 2633
train acc:  0.8125
train loss:  0.4478752613067627
train gradient:  0.4395666908974699
iteration : 2634
train acc:  0.78125
train loss:  0.49546709656715393
train gradient:  0.4644808221690278
iteration : 2635
train acc:  0.78125
train loss:  0.42374104261398315
train gradient:  0.4092947639551202
iteration : 2636
train acc:  0.84375
train loss:  0.38244152069091797
train gradient:  0.33311086413611335
iteration : 2637
train acc:  0.8515625
train loss:  0.4052175283432007
train gradient:  0.39152926305417624
iteration : 2638
train acc:  0.8046875
train loss:  0.4203607738018036
train gradient:  0.4072456572445517
iteration : 2639
train acc:  0.8515625
train loss:  0.34185150265693665
train gradient:  0.27082403202502314
iteration : 2640
train acc:  0.84375
train loss:  0.35253483057022095
train gradient:  0.28363952563088424
iteration : 2641
train acc:  0.8828125
train loss:  0.345842182636261
train gradient:  0.3600317786463377
iteration : 2642
train acc:  0.796875
train loss:  0.42556941509246826
train gradient:  0.4111036903954642
iteration : 2643
train acc:  0.7890625
train loss:  0.452541321516037
train gradient:  0.5486775206854736
iteration : 2644
train acc:  0.90625
train loss:  0.30976566672325134
train gradient:  0.4543364211163274
iteration : 2645
train acc:  0.796875
train loss:  0.4221671521663666
train gradient:  0.35835773683108085
iteration : 2646
train acc:  0.828125
train loss:  0.4134218096733093
train gradient:  0.4226341379623251
iteration : 2647
train acc:  0.8046875
train loss:  0.38441964983940125
train gradient:  0.44583811823746233
iteration : 2648
train acc:  0.8046875
train loss:  0.42562252283096313
train gradient:  0.39691061678377065
iteration : 2649
train acc:  0.8203125
train loss:  0.3801085352897644
train gradient:  0.2957379231535628
iteration : 2650
train acc:  0.8203125
train loss:  0.3910917043685913
train gradient:  0.3213385413860529
iteration : 2651
train acc:  0.859375
train loss:  0.31803297996520996
train gradient:  0.2647199318499823
iteration : 2652
train acc:  0.8125
train loss:  0.4101770520210266
train gradient:  0.49075319286420915
iteration : 2653
train acc:  0.765625
train loss:  0.5356212854385376
train gradient:  0.5749537112361616
iteration : 2654
train acc:  0.84375
train loss:  0.33783653378486633
train gradient:  0.2825381345636315
iteration : 2655
train acc:  0.828125
train loss:  0.4191001355648041
train gradient:  0.34783030097210454
iteration : 2656
train acc:  0.8203125
train loss:  0.39116573333740234
train gradient:  0.2891075279418339
iteration : 2657
train acc:  0.796875
train loss:  0.41438794136047363
train gradient:  0.3621334720643187
iteration : 2658
train acc:  0.796875
train loss:  0.46525436639785767
train gradient:  0.4400557738347273
iteration : 2659
train acc:  0.8671875
train loss:  0.3216531574726105
train gradient:  0.27959533684847143
iteration : 2660
train acc:  0.828125
train loss:  0.39944300055503845
train gradient:  0.29826228353658324
iteration : 2661
train acc:  0.84375
train loss:  0.3931984305381775
train gradient:  0.2919890616849842
iteration : 2662
train acc:  0.8515625
train loss:  0.36926406621932983
train gradient:  0.3976340051475709
iteration : 2663
train acc:  0.8359375
train loss:  0.371465802192688
train gradient:  0.27256296306046657
iteration : 2664
train acc:  0.7890625
train loss:  0.4072989821434021
train gradient:  0.3198467335982339
iteration : 2665
train acc:  0.8203125
train loss:  0.4162551760673523
train gradient:  0.52021726950453
iteration : 2666
train acc:  0.8828125
train loss:  0.3252919018268585
train gradient:  0.3212219570509813
iteration : 2667
train acc:  0.8359375
train loss:  0.4047333598136902
train gradient:  0.29551583401963305
iteration : 2668
train acc:  0.859375
train loss:  0.3718823492527008
train gradient:  0.2865295504459801
iteration : 2669
train acc:  0.84375
train loss:  0.32674503326416016
train gradient:  0.3288222084998037
iteration : 2670
train acc:  0.8515625
train loss:  0.36114442348480225
train gradient:  0.3105073194256265
iteration : 2671
train acc:  0.828125
train loss:  0.3361762464046478
train gradient:  0.22280587441478833
iteration : 2672
train acc:  0.8671875
train loss:  0.3300073444843292
train gradient:  0.39910731105215674
iteration : 2673
train acc:  0.796875
train loss:  0.4000700116157532
train gradient:  0.3835211500156779
iteration : 2674
train acc:  0.875
train loss:  0.34329310059547424
train gradient:  0.2970107091755116
iteration : 2675
train acc:  0.796875
train loss:  0.4650619626045227
train gradient:  0.49900192183891223
iteration : 2676
train acc:  0.828125
train loss:  0.37476229667663574
train gradient:  0.3026674187007505
iteration : 2677
train acc:  0.8125
train loss:  0.3964677155017853
train gradient:  0.33060139172440506
iteration : 2678
train acc:  0.8203125
train loss:  0.3924201726913452
train gradient:  0.3193143304082207
iteration : 2679
train acc:  0.765625
train loss:  0.46076563000679016
train gradient:  0.5576998458484299
iteration : 2680
train acc:  0.8046875
train loss:  0.4408610463142395
train gradient:  0.4239560301914883
iteration : 2681
train acc:  0.8515625
train loss:  0.3213502764701843
train gradient:  0.26273341011890994
iteration : 2682
train acc:  0.7890625
train loss:  0.44350841641426086
train gradient:  0.4810074356614701
iteration : 2683
train acc:  0.8515625
train loss:  0.3963523507118225
train gradient:  0.362919704390073
iteration : 2684
train acc:  0.828125
train loss:  0.34109848737716675
train gradient:  0.3182454162604275
iteration : 2685
train acc:  0.8515625
train loss:  0.33747628331184387
train gradient:  0.2885250399973253
iteration : 2686
train acc:  0.859375
train loss:  0.3415457010269165
train gradient:  0.2889304659436197
iteration : 2687
train acc:  0.796875
train loss:  0.4306837320327759
train gradient:  0.7394359668450184
iteration : 2688
train acc:  0.8125
train loss:  0.40417635440826416
train gradient:  0.3328555989965917
iteration : 2689
train acc:  0.78125
train loss:  0.4001602232456207
train gradient:  0.3618931962450933
iteration : 2690
train acc:  0.765625
train loss:  0.49013084173202515
train gradient:  0.45009379932612237
iteration : 2691
train acc:  0.875
train loss:  0.3388599753379822
train gradient:  0.2667520076167752
iteration : 2692
train acc:  0.7890625
train loss:  0.39689958095550537
train gradient:  0.30145203306043733
iteration : 2693
train acc:  0.84375
train loss:  0.3675960302352905
train gradient:  0.2917502769589051
iteration : 2694
train acc:  0.8359375
train loss:  0.392294704914093
train gradient:  0.31584982584519067
iteration : 2695
train acc:  0.8203125
train loss:  0.3670039772987366
train gradient:  0.36257108548161354
iteration : 2696
train acc:  0.7890625
train loss:  0.4428708553314209
train gradient:  0.5059260394801167
iteration : 2697
train acc:  0.75
train loss:  0.4555704891681671
train gradient:  0.5273855128924237
iteration : 2698
train acc:  0.859375
train loss:  0.35118919610977173
train gradient:  0.24956738111526727
iteration : 2699
train acc:  0.796875
train loss:  0.4315512180328369
train gradient:  0.3473609156160502
iteration : 2700
train acc:  0.8359375
train loss:  0.3529103696346283
train gradient:  0.35875300245659
iteration : 2701
train acc:  0.8515625
train loss:  0.37911397218704224
train gradient:  0.38523880876641586
iteration : 2702
train acc:  0.8046875
train loss:  0.4296726584434509
train gradient:  0.32926140122939307
iteration : 2703
train acc:  0.8828125
train loss:  0.32734668254852295
train gradient:  0.31488453547264506
iteration : 2704
train acc:  0.8359375
train loss:  0.3464621901512146
train gradient:  0.2969645538603813
iteration : 2705
train acc:  0.8046875
train loss:  0.420025110244751
train gradient:  0.5502954184474442
iteration : 2706
train acc:  0.8046875
train loss:  0.37800681591033936
train gradient:  0.4242134532583814
iteration : 2707
train acc:  0.7890625
train loss:  0.4669148921966553
train gradient:  0.370226425774311
iteration : 2708
train acc:  0.7890625
train loss:  0.4318091571331024
train gradient:  0.4518391300866756
iteration : 2709
train acc:  0.84375
train loss:  0.3483486771583557
train gradient:  0.24402519013200763
iteration : 2710
train acc:  0.828125
train loss:  0.3639568090438843
train gradient:  0.21551303140632316
iteration : 2711
train acc:  0.8359375
train loss:  0.3479412794113159
train gradient:  0.29270791557292186
iteration : 2712
train acc:  0.78125
train loss:  0.4351431131362915
train gradient:  0.419943657387732
iteration : 2713
train acc:  0.8359375
train loss:  0.41899168491363525
train gradient:  0.43285904857822416
iteration : 2714
train acc:  0.8359375
train loss:  0.3777243196964264
train gradient:  0.2903023801453712
iteration : 2715
train acc:  0.78125
train loss:  0.4781642556190491
train gradient:  0.4005542656141992
iteration : 2716
train acc:  0.8125
train loss:  0.4747629761695862
train gradient:  0.4400263705170753
iteration : 2717
train acc:  0.7734375
train loss:  0.4594716727733612
train gradient:  0.4202939578675846
iteration : 2718
train acc:  0.8515625
train loss:  0.35081368684768677
train gradient:  0.2587541839137655
iteration : 2719
train acc:  0.875
train loss:  0.34748294949531555
train gradient:  0.2218398067880743
iteration : 2720
train acc:  0.84375
train loss:  0.36739271879196167
train gradient:  0.32522149081048635
iteration : 2721
train acc:  0.828125
train loss:  0.3560710549354553
train gradient:  0.2492343214450632
iteration : 2722
train acc:  0.8046875
train loss:  0.39921048283576965
train gradient:  0.40328898011035447
iteration : 2723
train acc:  0.796875
train loss:  0.44483354687690735
train gradient:  0.4830110053661035
iteration : 2724
train acc:  0.84375
train loss:  0.31756389141082764
train gradient:  0.2227147204265812
iteration : 2725
train acc:  0.8203125
train loss:  0.38398033380508423
train gradient:  0.38714684045916026
iteration : 2726
train acc:  0.8203125
train loss:  0.4084177613258362
train gradient:  0.2692295492966935
iteration : 2727
train acc:  0.8125
train loss:  0.42229163646698
train gradient:  0.3019305780600238
iteration : 2728
train acc:  0.8046875
train loss:  0.3592533469200134
train gradient:  0.29061213041182427
iteration : 2729
train acc:  0.859375
train loss:  0.3328585624694824
train gradient:  0.2258740264063307
iteration : 2730
train acc:  0.8515625
train loss:  0.35258811712265015
train gradient:  0.27311646569300463
iteration : 2731
train acc:  0.7890625
train loss:  0.4307085871696472
train gradient:  0.5370077421455272
iteration : 2732
train acc:  0.7890625
train loss:  0.44479766488075256
train gradient:  0.43729285444261473
iteration : 2733
train acc:  0.7734375
train loss:  0.4279439151287079
train gradient:  0.662355333541073
iteration : 2734
train acc:  0.75
train loss:  0.45271244645118713
train gradient:  0.38612480140598165
iteration : 2735
train acc:  0.84375
train loss:  0.33162033557891846
train gradient:  0.24738398492056096
iteration : 2736
train acc:  0.859375
train loss:  0.31208688020706177
train gradient:  0.20669942657920007
iteration : 2737
train acc:  0.8125
train loss:  0.40295884013175964
train gradient:  0.27094962694278396
iteration : 2738
train acc:  0.8046875
train loss:  0.4412378966808319
train gradient:  0.5525271554612303
iteration : 2739
train acc:  0.8515625
train loss:  0.38986194133758545
train gradient:  0.5692578683987635
iteration : 2740
train acc:  0.796875
train loss:  0.3943578600883484
train gradient:  0.30736579476667397
iteration : 2741
train acc:  0.8515625
train loss:  0.34099531173706055
train gradient:  0.23868265866057897
iteration : 2742
train acc:  0.8515625
train loss:  0.3231604993343353
train gradient:  0.2370483615151281
iteration : 2743
train acc:  0.828125
train loss:  0.4248446822166443
train gradient:  0.4126358301580154
iteration : 2744
train acc:  0.8515625
train loss:  0.3702051043510437
train gradient:  0.2888595525996133
iteration : 2745
train acc:  0.828125
train loss:  0.34516096115112305
train gradient:  0.2816188040839597
iteration : 2746
train acc:  0.8515625
train loss:  0.36458292603492737
train gradient:  0.33325526715938864
iteration : 2747
train acc:  0.78125
train loss:  0.4428197145462036
train gradient:  0.5006195879626978
iteration : 2748
train acc:  0.7890625
train loss:  0.41962653398513794
train gradient:  0.3497014375801463
iteration : 2749
train acc:  0.8359375
train loss:  0.3699289560317993
train gradient:  0.38025917087602956
iteration : 2750
train acc:  0.8515625
train loss:  0.35866641998291016
train gradient:  0.2921148450113656
iteration : 2751
train acc:  0.828125
train loss:  0.3596058785915375
train gradient:  0.4282184267802701
iteration : 2752
train acc:  0.8828125
train loss:  0.3396171033382416
train gradient:  0.3042283428165867
iteration : 2753
train acc:  0.828125
train loss:  0.36919188499450684
train gradient:  0.3593807956420864
iteration : 2754
train acc:  0.859375
train loss:  0.348480761051178
train gradient:  0.24393252965213044
iteration : 2755
train acc:  0.8359375
train loss:  0.36850404739379883
train gradient:  0.35354527103865097
iteration : 2756
train acc:  0.8046875
train loss:  0.41348233819007874
train gradient:  0.39468890402387957
iteration : 2757
train acc:  0.8359375
train loss:  0.36687737703323364
train gradient:  0.24629365301952325
iteration : 2758
train acc:  0.8203125
train loss:  0.38972559571266174
train gradient:  0.3661707801009528
iteration : 2759
train acc:  0.8046875
train loss:  0.5071303844451904
train gradient:  0.666894009238507
iteration : 2760
train acc:  0.796875
train loss:  0.4052709937095642
train gradient:  0.41448495502254495
iteration : 2761
train acc:  0.796875
train loss:  0.41482970118522644
train gradient:  0.3473115536756864
iteration : 2762
train acc:  0.8359375
train loss:  0.3622192144393921
train gradient:  0.3354323148711496
iteration : 2763
train acc:  0.7890625
train loss:  0.4212077856063843
train gradient:  0.3388022822151023
iteration : 2764
train acc:  0.8515625
train loss:  0.317740261554718
train gradient:  0.324537787754837
iteration : 2765
train acc:  0.8359375
train loss:  0.3351815938949585
train gradient:  0.2490163976038894
iteration : 2766
train acc:  0.8203125
train loss:  0.37658488750457764
train gradient:  0.4025327950804811
iteration : 2767
train acc:  0.8828125
train loss:  0.27637988328933716
train gradient:  0.1972380197331592
iteration : 2768
train acc:  0.828125
train loss:  0.39212971925735474
train gradient:  0.2973961858114791
iteration : 2769
train acc:  0.84375
train loss:  0.3357417583465576
train gradient:  0.3453263862914442
iteration : 2770
train acc:  0.8046875
train loss:  0.39897871017456055
train gradient:  0.42883345376399806
iteration : 2771
train acc:  0.796875
train loss:  0.4167206287384033
train gradient:  0.34392841033562815
iteration : 2772
train acc:  0.8671875
train loss:  0.3136906921863556
train gradient:  0.28636980433556675
iteration : 2773
train acc:  0.84375
train loss:  0.3770332932472229
train gradient:  0.439188071935367
iteration : 2774
train acc:  0.8671875
train loss:  0.33393949270248413
train gradient:  0.366418837066514
iteration : 2775
train acc:  0.7734375
train loss:  0.42215046286582947
train gradient:  0.43605141385215873
iteration : 2776
train acc:  0.8046875
train loss:  0.4379901885986328
train gradient:  0.5234747216256117
iteration : 2777
train acc:  0.8046875
train loss:  0.4144958257675171
train gradient:  0.4613742210945267
iteration : 2778
train acc:  0.8203125
train loss:  0.4066527187824249
train gradient:  0.5073190866591146
iteration : 2779
train acc:  0.8359375
train loss:  0.3452444076538086
train gradient:  0.29610096576198774
iteration : 2780
train acc:  0.875
train loss:  0.34873244166374207
train gradient:  0.3981171948742708
iteration : 2781
train acc:  0.765625
train loss:  0.4757845997810364
train gradient:  0.5655648924583554
iteration : 2782
train acc:  0.84375
train loss:  0.34074175357818604
train gradient:  0.2975950567417219
iteration : 2783
train acc:  0.84375
train loss:  0.3193182349205017
train gradient:  0.36142135743153336
iteration : 2784
train acc:  0.8515625
train loss:  0.3390768766403198
train gradient:  0.38419277854256045
iteration : 2785
train acc:  0.796875
train loss:  0.4230785369873047
train gradient:  0.41478005494983367
iteration : 2786
train acc:  0.7734375
train loss:  0.44587522745132446
train gradient:  0.39123435255298306
iteration : 2787
train acc:  0.828125
train loss:  0.3237581253051758
train gradient:  0.2964396464748274
iteration : 2788
train acc:  0.8203125
train loss:  0.4175325334072113
train gradient:  0.4173272738773525
iteration : 2789
train acc:  0.796875
train loss:  0.40888410806655884
train gradient:  0.4908291939820565
iteration : 2790
train acc:  0.8046875
train loss:  0.3814491629600525
train gradient:  0.4188324071029023
iteration : 2791
train acc:  0.8671875
train loss:  0.334392786026001
train gradient:  0.36835794741011213
iteration : 2792
train acc:  0.8203125
train loss:  0.3918917179107666
train gradient:  0.42624132989150354
iteration : 2793
train acc:  0.8125
train loss:  0.40294840931892395
train gradient:  0.30670015167106734
iteration : 2794
train acc:  0.828125
train loss:  0.3288503587245941
train gradient:  0.40103130423271827
iteration : 2795
train acc:  0.8125
train loss:  0.44834592938423157
train gradient:  0.5434392512438438
iteration : 2796
train acc:  0.8671875
train loss:  0.32510852813720703
train gradient:  0.2872179778488392
iteration : 2797
train acc:  0.84375
train loss:  0.3200112581253052
train gradient:  0.24359750759114235
iteration : 2798
train acc:  0.84375
train loss:  0.34409627318382263
train gradient:  0.37441236286328755
iteration : 2799
train acc:  0.8203125
train loss:  0.41193723678588867
train gradient:  0.42211649215318886
iteration : 2800
train acc:  0.8125
train loss:  0.4321167767047882
train gradient:  0.518126774969074
iteration : 2801
train acc:  0.8203125
train loss:  0.4135957360267639
train gradient:  0.409803676047513
iteration : 2802
train acc:  0.859375
train loss:  0.3267834186553955
train gradient:  0.3161984042074645
iteration : 2803
train acc:  0.890625
train loss:  0.3179587423801422
train gradient:  0.3359356040650596
iteration : 2804
train acc:  0.828125
train loss:  0.36248672008514404
train gradient:  0.33051594214423513
iteration : 2805
train acc:  0.8203125
train loss:  0.34994107484817505
train gradient:  0.3529868979846112
iteration : 2806
train acc:  0.796875
train loss:  0.44355741143226624
train gradient:  0.6135539081743426
iteration : 2807
train acc:  0.875
train loss:  0.3982241153717041
train gradient:  0.3723239402630467
iteration : 2808
train acc:  0.7734375
train loss:  0.47304636240005493
train gradient:  0.7156840550675555
iteration : 2809
train acc:  0.734375
train loss:  0.4866134524345398
train gradient:  0.7318517964594425
iteration : 2810
train acc:  0.859375
train loss:  0.3201954960823059
train gradient:  0.34492504091146986
iteration : 2811
train acc:  0.8125
train loss:  0.4006149470806122
train gradient:  0.27903075048355835
iteration : 2812
train acc:  0.890625
train loss:  0.3448163866996765
train gradient:  0.31728705090228715
iteration : 2813
train acc:  0.84375
train loss:  0.35132962465286255
train gradient:  0.270246734393558
iteration : 2814
train acc:  0.84375
train loss:  0.3935633599758148
train gradient:  0.374157929889852
iteration : 2815
train acc:  0.828125
train loss:  0.42003515362739563
train gradient:  0.35059777301804795
iteration : 2816
train acc:  0.8046875
train loss:  0.4237914979457855
train gradient:  0.3641134543853925
iteration : 2817
train acc:  0.8359375
train loss:  0.3820408880710602
train gradient:  0.3900394240709431
iteration : 2818
train acc:  0.8046875
train loss:  0.44178903102874756
train gradient:  0.5189587841472881
iteration : 2819
train acc:  0.8125
train loss:  0.3968428075313568
train gradient:  0.3402165283214124
iteration : 2820
train acc:  0.796875
train loss:  0.41771090030670166
train gradient:  0.3700563748790896
iteration : 2821
train acc:  0.8515625
train loss:  0.3281862139701843
train gradient:  0.2538759272428294
iteration : 2822
train acc:  0.859375
train loss:  0.3169858455657959
train gradient:  0.27174957557485224
iteration : 2823
train acc:  0.8359375
train loss:  0.34565916657447815
train gradient:  0.34218176932031225
iteration : 2824
train acc:  0.8828125
train loss:  0.3016852140426636
train gradient:  0.22007190017556894
iteration : 2825
train acc:  0.8359375
train loss:  0.3500746786594391
train gradient:  0.25665915882097523
iteration : 2826
train acc:  0.78125
train loss:  0.4200502932071686
train gradient:  0.32160994285900274
iteration : 2827
train acc:  0.8515625
train loss:  0.3384796380996704
train gradient:  0.29972454839405815
iteration : 2828
train acc:  0.8359375
train loss:  0.3263770043849945
train gradient:  0.3277959216272211
iteration : 2829
train acc:  0.8359375
train loss:  0.37236928939819336
train gradient:  0.4405552628774204
iteration : 2830
train acc:  0.78125
train loss:  0.48288315534591675
train gradient:  0.5011414272972984
iteration : 2831
train acc:  0.8828125
train loss:  0.3253902792930603
train gradient:  0.23820632254304935
iteration : 2832
train acc:  0.8203125
train loss:  0.40277099609375
train gradient:  0.4714285599851767
iteration : 2833
train acc:  0.78125
train loss:  0.45003384351730347
train gradient:  0.5026329143533166
iteration : 2834
train acc:  0.828125
train loss:  0.3462091088294983
train gradient:  0.3533592986898667
iteration : 2835
train acc:  0.8125
train loss:  0.33575326204299927
train gradient:  0.2742758932565123
iteration : 2836
train acc:  0.796875
train loss:  0.42949944734573364
train gradient:  0.4286309386877089
iteration : 2837
train acc:  0.78125
train loss:  0.4380115568637848
train gradient:  0.543131363906217
iteration : 2838
train acc:  0.8046875
train loss:  0.42813313007354736
train gradient:  0.545593952361042
iteration : 2839
train acc:  0.859375
train loss:  0.3412763476371765
train gradient:  0.5157084900505581
iteration : 2840
train acc:  0.7890625
train loss:  0.4197986125946045
train gradient:  0.7414772925013007
iteration : 2841
train acc:  0.8359375
train loss:  0.3695693910121918
train gradient:  0.3648210240528691
iteration : 2842
train acc:  0.8203125
train loss:  0.3832630217075348
train gradient:  0.48629506678460027
iteration : 2843
train acc:  0.8125
train loss:  0.4232119917869568
train gradient:  0.341032779757157
iteration : 2844
train acc:  0.8671875
train loss:  0.3408505618572235
train gradient:  0.30153035349136476
iteration : 2845
train acc:  0.84375
train loss:  0.3688720464706421
train gradient:  0.5851314651865059
iteration : 2846
train acc:  0.8203125
train loss:  0.3430042564868927
train gradient:  0.3015577718720239
iteration : 2847
train acc:  0.859375
train loss:  0.37347227334976196
train gradient:  0.3294839968035184
iteration : 2848
train acc:  0.8125
train loss:  0.4110042452812195
train gradient:  0.3935687468280517
iteration : 2849
train acc:  0.8359375
train loss:  0.3598945736885071
train gradient:  0.7005329102810854
iteration : 2850
train acc:  0.796875
train loss:  0.43505749106407166
train gradient:  0.38809139708512064
iteration : 2851
train acc:  0.84375
train loss:  0.3140559792518616
train gradient:  0.21511524441102495
iteration : 2852
train acc:  0.7890625
train loss:  0.3880866467952728
train gradient:  0.4286115612235495
iteration : 2853
train acc:  0.8359375
train loss:  0.4393361210823059
train gradient:  0.5518265783862768
iteration : 2854
train acc:  0.8359375
train loss:  0.37596142292022705
train gradient:  0.5351247585511961
iteration : 2855
train acc:  0.890625
train loss:  0.32789257168769836
train gradient:  0.2641990942838167
iteration : 2856
train acc:  0.8671875
train loss:  0.34282490611076355
train gradient:  0.47858297697584046
iteration : 2857
train acc:  0.8046875
train loss:  0.4226618707180023
train gradient:  0.4520338699270893
iteration : 2858
train acc:  0.8203125
train loss:  0.4013121724128723
train gradient:  0.4155868374468268
iteration : 2859
train acc:  0.828125
train loss:  0.3497193455696106
train gradient:  0.3464224512258393
iteration : 2860
train acc:  0.796875
train loss:  0.39742130041122437
train gradient:  0.34381340260094145
iteration : 2861
train acc:  0.7734375
train loss:  0.4846687316894531
train gradient:  0.5990735113216182
iteration : 2862
train acc:  0.8203125
train loss:  0.39715471863746643
train gradient:  0.44665623264433485
iteration : 2863
train acc:  0.8125
train loss:  0.4223254323005676
train gradient:  0.7761175967530552
iteration : 2864
train acc:  0.796875
train loss:  0.40688949823379517
train gradient:  0.3141891148934793
iteration : 2865
train acc:  0.796875
train loss:  0.46704399585723877
train gradient:  0.5122388180820218
iteration : 2866
train acc:  0.8671875
train loss:  0.3258076310157776
train gradient:  0.3583413968452791
iteration : 2867
train acc:  0.8125
train loss:  0.4114859998226166
train gradient:  0.4711248997013406
iteration : 2868
train acc:  0.8359375
train loss:  0.3849535584449768
train gradient:  0.3202319131541544
iteration : 2869
train acc:  0.796875
train loss:  0.3957958221435547
train gradient:  0.4851100296122095
iteration : 2870
train acc:  0.84375
train loss:  0.37738364934921265
train gradient:  0.2723250994220521
iteration : 2871
train acc:  0.8125
train loss:  0.4238716959953308
train gradient:  0.3823468052144536
iteration : 2872
train acc:  0.8828125
train loss:  0.31539785861968994
train gradient:  0.3356872381638806
iteration : 2873
train acc:  0.8046875
train loss:  0.43253445625305176
train gradient:  0.4679445125976129
iteration : 2874
train acc:  0.84375
train loss:  0.38669854402542114
train gradient:  0.3411265386767751
iteration : 2875
train acc:  0.828125
train loss:  0.3295302093029022
train gradient:  0.2931076111147361
iteration : 2876
train acc:  0.84375
train loss:  0.3759692311286926
train gradient:  0.3306465487061843
iteration : 2877
train acc:  0.875
train loss:  0.3118554949760437
train gradient:  0.24017440355304098
iteration : 2878
train acc:  0.796875
train loss:  0.4428113102912903
train gradient:  0.5215317205584575
iteration : 2879
train acc:  0.8515625
train loss:  0.3487388491630554
train gradient:  0.287042094126529
iteration : 2880
train acc:  0.7890625
train loss:  0.43119463324546814
train gradient:  0.5577322042317007
iteration : 2881
train acc:  0.84375
train loss:  0.3886316418647766
train gradient:  0.30477369887700584
iteration : 2882
train acc:  0.84375
train loss:  0.3763704001903534
train gradient:  0.41128108318067497
iteration : 2883
train acc:  0.828125
train loss:  0.35531994700431824
train gradient:  0.22968623786622971
iteration : 2884
train acc:  0.78125
train loss:  0.3872876763343811
train gradient:  0.38641484097577095
iteration : 2885
train acc:  0.8359375
train loss:  0.36301466822624207
train gradient:  0.30086096191939704
iteration : 2886
train acc:  0.8046875
train loss:  0.43288150429725647
train gradient:  0.45235949809500864
iteration : 2887
train acc:  0.8359375
train loss:  0.38913315534591675
train gradient:  0.2942365574515343
iteration : 2888
train acc:  0.875
train loss:  0.33845120668411255
train gradient:  0.22934975768312604
iteration : 2889
train acc:  0.7578125
train loss:  0.48103201389312744
train gradient:  0.47612093238824543
iteration : 2890
train acc:  0.84375
train loss:  0.4148789644241333
train gradient:  0.3436281670041828
iteration : 2891
train acc:  0.8203125
train loss:  0.38504838943481445
train gradient:  0.35977196003889744
iteration : 2892
train acc:  0.828125
train loss:  0.37490519881248474
train gradient:  0.29749824748577625
iteration : 2893
train acc:  0.8984375
train loss:  0.3009796738624573
train gradient:  0.23929416706494303
iteration : 2894
train acc:  0.7890625
train loss:  0.47263121604919434
train gradient:  0.5508203042441718
iteration : 2895
train acc:  0.8671875
train loss:  0.33658331632614136
train gradient:  0.26245507213255215
iteration : 2896
train acc:  0.84375
train loss:  0.40832895040512085
train gradient:  0.41085068376615547
iteration : 2897
train acc:  0.7734375
train loss:  0.472409188747406
train gradient:  0.4413820874757518
iteration : 2898
train acc:  0.765625
train loss:  0.4883021116256714
train gradient:  0.5317269604038961
iteration : 2899
train acc:  0.875
train loss:  0.3341284394264221
train gradient:  0.256597628403853
iteration : 2900
train acc:  0.7890625
train loss:  0.3895663619041443
train gradient:  0.3103664679049505
iteration : 2901
train acc:  0.8515625
train loss:  0.3347811698913574
train gradient:  0.32589952471266564
iteration : 2902
train acc:  0.8046875
train loss:  0.38984888792037964
train gradient:  0.29835876852479776
iteration : 2903
train acc:  0.8515625
train loss:  0.372856080532074
train gradient:  0.3993442841628132
iteration : 2904
train acc:  0.7890625
train loss:  0.4501146078109741
train gradient:  0.34490290938527557
iteration : 2905
train acc:  0.8515625
train loss:  0.3439083695411682
train gradient:  0.23390206955796702
iteration : 2906
train acc:  0.828125
train loss:  0.3527500033378601
train gradient:  0.2962374331042573
iteration : 2907
train acc:  0.875
train loss:  0.3365563154220581
train gradient:  0.2888301343637588
iteration : 2908
train acc:  0.8125
train loss:  0.359468936920166
train gradient:  0.34035931363143795
iteration : 2909
train acc:  0.8125
train loss:  0.423311322927475
train gradient:  0.406366821483584
iteration : 2910
train acc:  0.828125
train loss:  0.3315633535385132
train gradient:  0.22597571097210578
iteration : 2911
train acc:  0.8515625
train loss:  0.38141703605651855
train gradient:  0.26613428673073247
iteration : 2912
train acc:  0.8125
train loss:  0.41080939769744873
train gradient:  0.31676452385312254
iteration : 2913
train acc:  0.8359375
train loss:  0.3640120029449463
train gradient:  0.29841527104161525
iteration : 2914
train acc:  0.875
train loss:  0.36578232049942017
train gradient:  0.29466818414679213
iteration : 2915
train acc:  0.765625
train loss:  0.41848108172416687
train gradient:  0.5444209352584175
iteration : 2916
train acc:  0.8515625
train loss:  0.33939480781555176
train gradient:  0.2968550972963906
iteration : 2917
train acc:  0.8515625
train loss:  0.3296608626842499
train gradient:  0.23379846964317996
iteration : 2918
train acc:  0.84375
train loss:  0.3341290056705475
train gradient:  0.2759355641606689
iteration : 2919
train acc:  0.8515625
train loss:  0.3185429871082306
train gradient:  0.2364093229549346
iteration : 2920
train acc:  0.828125
train loss:  0.3321940302848816
train gradient:  0.27706314599060333
iteration : 2921
train acc:  0.8125
train loss:  0.4014270305633545
train gradient:  0.5486910191855198
iteration : 2922
train acc:  0.875
train loss:  0.3642154932022095
train gradient:  0.3083780302697726
iteration : 2923
train acc:  0.78125
train loss:  0.4497760534286499
train gradient:  0.39854741117802667
iteration : 2924
train acc:  0.828125
train loss:  0.39906448125839233
train gradient:  0.3322602124209216
iteration : 2925
train acc:  0.8125
train loss:  0.4499748945236206
train gradient:  0.43054505678280736
iteration : 2926
train acc:  0.8359375
train loss:  0.3638220429420471
train gradient:  0.3073539100159641
iteration : 2927
train acc:  0.8515625
train loss:  0.34531325101852417
train gradient:  0.27404457569891316
iteration : 2928
train acc:  0.8046875
train loss:  0.38899466395378113
train gradient:  0.45544068016262207
iteration : 2929
train acc:  0.8671875
train loss:  0.2995317578315735
train gradient:  0.36491648168732393
iteration : 2930
train acc:  0.8046875
train loss:  0.38739848136901855
train gradient:  0.30389183638636774
iteration : 2931
train acc:  0.7734375
train loss:  0.49858373403549194
train gradient:  0.6995189103386277
iteration : 2932
train acc:  0.828125
train loss:  0.3714958131313324
train gradient:  0.34891351441643026
iteration : 2933
train acc:  0.8203125
train loss:  0.38082683086395264
train gradient:  0.519672245194735
iteration : 2934
train acc:  0.8515625
train loss:  0.3406624495983124
train gradient:  0.32175722735867135
iteration : 2935
train acc:  0.8515625
train loss:  0.34716910123825073
train gradient:  0.3180859271635016
iteration : 2936
train acc:  0.84375
train loss:  0.3335239291191101
train gradient:  0.26558722010716834
iteration : 2937
train acc:  0.8515625
train loss:  0.35105663537979126
train gradient:  0.30229488457510006
iteration : 2938
train acc:  0.7890625
train loss:  0.41509076952934265
train gradient:  0.4026323403979707
iteration : 2939
train acc:  0.8515625
train loss:  0.34807002544403076
train gradient:  0.24731570589521285
iteration : 2940
train acc:  0.8046875
train loss:  0.39701199531555176
train gradient:  0.2770941145987381
iteration : 2941
train acc:  0.7890625
train loss:  0.448322057723999
train gradient:  0.46486502753033665
iteration : 2942
train acc:  0.78125
train loss:  0.37184885144233704
train gradient:  0.3125200245081965
iteration : 2943
train acc:  0.8203125
train loss:  0.4258626103401184
train gradient:  0.4049344089854092
iteration : 2944
train acc:  0.875
train loss:  0.32341283559799194
train gradient:  0.40717435446560707
iteration : 2945
train acc:  0.8125
train loss:  0.37901443243026733
train gradient:  0.28531848030316465
iteration : 2946
train acc:  0.8125
train loss:  0.44978100061416626
train gradient:  0.3935341024524605
iteration : 2947
train acc:  0.765625
train loss:  0.4405226707458496
train gradient:  0.47343849905753654
iteration : 2948
train acc:  0.8125
train loss:  0.3810136914253235
train gradient:  0.36341434565622505
iteration : 2949
train acc:  0.875
train loss:  0.3681631088256836
train gradient:  0.2786063402688096
iteration : 2950
train acc:  0.8515625
train loss:  0.339103102684021
train gradient:  0.30444954897987414
iteration : 2951
train acc:  0.828125
train loss:  0.3789823353290558
train gradient:  0.35280856483933987
iteration : 2952
train acc:  0.859375
train loss:  0.3369462490081787
train gradient:  0.2099054040472877
iteration : 2953
train acc:  0.8515625
train loss:  0.3149157762527466
train gradient:  0.2667616121244512
iteration : 2954
train acc:  0.84375
train loss:  0.3793974816799164
train gradient:  0.4129387610249215
iteration : 2955
train acc:  0.8671875
train loss:  0.3304890990257263
train gradient:  0.3074353268882455
iteration : 2956
train acc:  0.8828125
train loss:  0.3161822259426117
train gradient:  0.314733439310465
iteration : 2957
train acc:  0.859375
train loss:  0.3374626040458679
train gradient:  0.345724425384572
iteration : 2958
train acc:  0.8515625
train loss:  0.3868350684642792
train gradient:  0.41113000643264946
iteration : 2959
train acc:  0.8125
train loss:  0.4231416583061218
train gradient:  0.3626908729914443
iteration : 2960
train acc:  0.8125
train loss:  0.35565418004989624
train gradient:  0.37911209497622267
iteration : 2961
train acc:  0.8671875
train loss:  0.37059536576271057
train gradient:  0.3367465394310983
iteration : 2962
train acc:  0.828125
train loss:  0.3257092237472534
train gradient:  0.30997090703309443
iteration : 2963
train acc:  0.8671875
train loss:  0.3306174874305725
train gradient:  0.2685579523573915
iteration : 2964
train acc:  0.796875
train loss:  0.4077545404434204
train gradient:  0.4657616584326445
iteration : 2965
train acc:  0.8203125
train loss:  0.40054142475128174
train gradient:  0.35073490238242894
iteration : 2966
train acc:  0.828125
train loss:  0.3773760199546814
train gradient:  0.3227154544924473
iteration : 2967
train acc:  0.8515625
train loss:  0.32303300499916077
train gradient:  0.23213348003770143
iteration : 2968
train acc:  0.875
train loss:  0.30178096890449524
train gradient:  0.30707295798536005
iteration : 2969
train acc:  0.8203125
train loss:  0.42453187704086304
train gradient:  0.446034508192527
iteration : 2970
train acc:  0.8203125
train loss:  0.32884931564331055
train gradient:  0.3339141249957231
iteration : 2971
train acc:  0.8359375
train loss:  0.3594333529472351
train gradient:  0.44357325368104267
iteration : 2972
train acc:  0.8515625
train loss:  0.32494598627090454
train gradient:  0.28117517676688647
iteration : 2973
train acc:  0.8515625
train loss:  0.33046942949295044
train gradient:  0.4084316049253182
iteration : 2974
train acc:  0.8359375
train loss:  0.3558274507522583
train gradient:  0.44764612426098266
iteration : 2975
train acc:  0.8359375
train loss:  0.41350454092025757
train gradient:  0.5079390116278344
iteration : 2976
train acc:  0.8046875
train loss:  0.38409531116485596
train gradient:  0.3280830212056292
iteration : 2977
train acc:  0.7890625
train loss:  0.5002644062042236
train gradient:  0.5522657166144201
iteration : 2978
train acc:  0.8125
train loss:  0.4529583156108856
train gradient:  0.42821023533323876
iteration : 2979
train acc:  0.875
train loss:  0.3408728539943695
train gradient:  0.29367512148233116
iteration : 2980
train acc:  0.8046875
train loss:  0.406909316778183
train gradient:  0.3958810055279687
iteration : 2981
train acc:  0.8203125
train loss:  0.3804359436035156
train gradient:  0.503196787838343
iteration : 2982
train acc:  0.90625
train loss:  0.2398536652326584
train gradient:  0.22699488941729334
iteration : 2983
train acc:  0.78125
train loss:  0.4219891130924225
train gradient:  0.4130512891373382
iteration : 2984
train acc:  0.7890625
train loss:  0.4185669422149658
train gradient:  0.5112932915097861
iteration : 2985
train acc:  0.8203125
train loss:  0.426133394241333
train gradient:  0.47517443461701014
iteration : 2986
train acc:  0.8203125
train loss:  0.3965733051300049
train gradient:  0.3657364733607906
iteration : 2987
train acc:  0.828125
train loss:  0.36964547634124756
train gradient:  0.38537589282887774
iteration : 2988
train acc:  0.8203125
train loss:  0.40296679735183716
train gradient:  0.5482351704017786
iteration : 2989
train acc:  0.8046875
train loss:  0.43157392740249634
train gradient:  0.3967470917951672
iteration : 2990
train acc:  0.84375
train loss:  0.4142681360244751
train gradient:  0.3169659254858081
iteration : 2991
train acc:  0.8046875
train loss:  0.3692651391029358
train gradient:  0.3375527475286603
iteration : 2992
train acc:  0.8359375
train loss:  0.3893476724624634
train gradient:  0.4498449421760781
iteration : 2993
train acc:  0.8359375
train loss:  0.337028443813324
train gradient:  0.2888455338397568
iteration : 2994
train acc:  0.7734375
train loss:  0.44593292474746704
train gradient:  0.4990632035345858
iteration : 2995
train acc:  0.8125
train loss:  0.4311116635799408
train gradient:  0.4624207423202736
iteration : 2996
train acc:  0.84375
train loss:  0.3871634006500244
train gradient:  0.55287452448688
iteration : 2997
train acc:  0.7890625
train loss:  0.3738797903060913
train gradient:  0.3698776582490457
iteration : 2998
train acc:  0.8984375
train loss:  0.3128834068775177
train gradient:  0.23812759087086918
iteration : 2999
train acc:  0.875
train loss:  0.3474360704421997
train gradient:  0.29646152458181496
iteration : 3000
train acc:  0.8046875
train loss:  0.3756462335586548
train gradient:  0.29553041211367387
iteration : 3001
train acc:  0.8125
train loss:  0.3644968271255493
train gradient:  0.29462942463955333
iteration : 3002
train acc:  0.8359375
train loss:  0.37894976139068604
train gradient:  0.42885639774778356
iteration : 3003
train acc:  0.8359375
train loss:  0.378935307264328
train gradient:  0.2529381086118393
iteration : 3004
train acc:  0.8359375
train loss:  0.3668908178806305
train gradient:  0.2933522556665935
iteration : 3005
train acc:  0.765625
train loss:  0.4557100236415863
train gradient:  0.4073597181515198
iteration : 3006
train acc:  0.859375
train loss:  0.3153538703918457
train gradient:  0.22258749080056184
iteration : 3007
train acc:  0.875
train loss:  0.3410378694534302
train gradient:  0.23967958685096002
iteration : 3008
train acc:  0.8984375
train loss:  0.28953710198402405
train gradient:  0.3160740356833375
iteration : 3009
train acc:  0.8203125
train loss:  0.4013221859931946
train gradient:  0.47126599880716347
iteration : 3010
train acc:  0.8203125
train loss:  0.39984187483787537
train gradient:  0.4695494292381486
iteration : 3011
train acc:  0.7421875
train loss:  0.47498375177383423
train gradient:  0.5163219706950511
iteration : 3012
train acc:  0.78125
train loss:  0.4493527412414551
train gradient:  0.4410766247459239
iteration : 3013
train acc:  0.8984375
train loss:  0.2832648456096649
train gradient:  0.2890745416705031
iteration : 3014
train acc:  0.8515625
train loss:  0.41227492690086365
train gradient:  0.4187839673978677
iteration : 3015
train acc:  0.7890625
train loss:  0.387315034866333
train gradient:  0.35920664896839294
iteration : 3016
train acc:  0.890625
train loss:  0.39898285269737244
train gradient:  0.25926774697756083
iteration : 3017
train acc:  0.8125
train loss:  0.4226020574569702
train gradient:  0.6268372643889965
iteration : 3018
train acc:  0.8046875
train loss:  0.405531644821167
train gradient:  0.4167206874568946
iteration : 3019
train acc:  0.7890625
train loss:  0.42755651473999023
train gradient:  0.49330655140835694
iteration : 3020
train acc:  0.796875
train loss:  0.4324740171432495
train gradient:  0.3107189049457706
iteration : 3021
train acc:  0.8515625
train loss:  0.337238073348999
train gradient:  0.2844692559142226
iteration : 3022
train acc:  0.828125
train loss:  0.36159515380859375
train gradient:  0.38767579568545224
iteration : 3023
train acc:  0.828125
train loss:  0.367361843585968
train gradient:  0.3602342443640193
iteration : 3024
train acc:  0.8046875
train loss:  0.40219295024871826
train gradient:  0.38811258629832396
iteration : 3025
train acc:  0.859375
train loss:  0.3306129574775696
train gradient:  0.25966163429865685
iteration : 3026
train acc:  0.8203125
train loss:  0.4403950870037079
train gradient:  0.47059710103071095
iteration : 3027
train acc:  0.90625
train loss:  0.2860434055328369
train gradient:  0.15134486150117732
iteration : 3028
train acc:  0.875
train loss:  0.32692813873291016
train gradient:  0.3314371770296012
iteration : 3029
train acc:  0.765625
train loss:  0.4421984553337097
train gradient:  0.4609156737035953
iteration : 3030
train acc:  0.78125
train loss:  0.44598591327667236
train gradient:  0.6415540835972771
iteration : 3031
train acc:  0.765625
train loss:  0.4754680395126343
train gradient:  0.5059529949230163
iteration : 3032
train acc:  0.875
train loss:  0.35523682832717896
train gradient:  0.35212858128948976
iteration : 3033
train acc:  0.8359375
train loss:  0.3596467673778534
train gradient:  0.31372591199861954
iteration : 3034
train acc:  0.8359375
train loss:  0.37977758049964905
train gradient:  0.29905592975637063
iteration : 3035
train acc:  0.796875
train loss:  0.42694610357284546
train gradient:  0.41421384108511045
iteration : 3036
train acc:  0.8515625
train loss:  0.40136659145355225
train gradient:  0.38492786295108766
iteration : 3037
train acc:  0.890625
train loss:  0.3103446066379547
train gradient:  0.32633668392859455
iteration : 3038
train acc:  0.765625
train loss:  0.5188083648681641
train gradient:  0.6182644456955113
iteration : 3039
train acc:  0.8671875
train loss:  0.35861051082611084
train gradient:  0.25082785754401205
iteration : 3040
train acc:  0.75
train loss:  0.4549270272254944
train gradient:  0.46987097355619184
iteration : 3041
train acc:  0.8359375
train loss:  0.3774212598800659
train gradient:  0.4846644022128913
iteration : 3042
train acc:  0.8203125
train loss:  0.3629889488220215
train gradient:  0.28963704086652026
iteration : 3043
train acc:  0.8125
train loss:  0.4647562503814697
train gradient:  0.4810252679462183
iteration : 3044
train acc:  0.84375
train loss:  0.36989644169807434
train gradient:  0.30091866541686224
iteration : 3045
train acc:  0.8515625
train loss:  0.3663973808288574
train gradient:  0.32702743171624776
iteration : 3046
train acc:  0.8359375
train loss:  0.35142651200294495
train gradient:  0.23095560856671477
iteration : 3047
train acc:  0.828125
train loss:  0.3954769968986511
train gradient:  0.2891656221329021
iteration : 3048
train acc:  0.8984375
train loss:  0.2695215344429016
train gradient:  0.20642152465616476
iteration : 3049
train acc:  0.796875
train loss:  0.41055887937545776
train gradient:  0.347722461985975
iteration : 3050
train acc:  0.84375
train loss:  0.34780454635620117
train gradient:  0.25803999756263185
iteration : 3051
train acc:  0.8046875
train loss:  0.4262318015098572
train gradient:  0.3431715516923738
iteration : 3052
train acc:  0.8984375
train loss:  0.3210970163345337
train gradient:  0.27774253983158564
iteration : 3053
train acc:  0.75
train loss:  0.49144935607910156
train gradient:  0.4088306586793516
iteration : 3054
train acc:  0.8515625
train loss:  0.41364896297454834
train gradient:  0.44094178445225235
iteration : 3055
train acc:  0.8046875
train loss:  0.35135287046432495
train gradient:  0.23578081756021008
iteration : 3056
train acc:  0.8046875
train loss:  0.38120293617248535
train gradient:  0.28777462535020293
iteration : 3057
train acc:  0.8515625
train loss:  0.3465005159378052
train gradient:  0.35959158128678637
iteration : 3058
train acc:  0.8046875
train loss:  0.36707422137260437
train gradient:  0.33577924422093314
iteration : 3059
train acc:  0.78125
train loss:  0.45718833804130554
train gradient:  0.4049292889345223
iteration : 3060
train acc:  0.84375
train loss:  0.37490761280059814
train gradient:  0.3836775603101093
iteration : 3061
train acc:  0.78125
train loss:  0.45763006806373596
train gradient:  0.4188132243185881
iteration : 3062
train acc:  0.875
train loss:  0.3522037863731384
train gradient:  0.5565085838857765
iteration : 3063
train acc:  0.828125
train loss:  0.3741634786128998
train gradient:  0.3237027492547229
iteration : 3064
train acc:  0.8125
train loss:  0.3687133193016052
train gradient:  0.3277696560503257
iteration : 3065
train acc:  0.875
train loss:  0.3011915683746338
train gradient:  0.2649838665577221
iteration : 3066
train acc:  0.8359375
train loss:  0.3769594728946686
train gradient:  0.22650338859366248
iteration : 3067
train acc:  0.8515625
train loss:  0.3153012692928314
train gradient:  0.23407699985995278
iteration : 3068
train acc:  0.84375
train loss:  0.3888164460659027
train gradient:  0.2920700014530144
iteration : 3069
train acc:  0.78125
train loss:  0.4710291624069214
train gradient:  0.4968593901639715
iteration : 3070
train acc:  0.859375
train loss:  0.3462173640727997
train gradient:  0.2898188531434884
iteration : 3071
train acc:  0.8125
train loss:  0.41946160793304443
train gradient:  0.37964892519778404
iteration : 3072
train acc:  0.8046875
train loss:  0.3993214964866638
train gradient:  0.2485841393405651
iteration : 3073
train acc:  0.859375
train loss:  0.35559114813804626
train gradient:  0.30089573158242144
iteration : 3074
train acc:  0.8359375
train loss:  0.3546578586101532
train gradient:  0.25941010684589194
iteration : 3075
train acc:  0.8203125
train loss:  0.3843247890472412
train gradient:  0.26492734284821157
iteration : 3076
train acc:  0.8046875
train loss:  0.42415040731430054
train gradient:  0.3990701809059249
iteration : 3077
train acc:  0.8203125
train loss:  0.41342681646347046
train gradient:  0.42551294405376155
iteration : 3078
train acc:  0.859375
train loss:  0.3546134829521179
train gradient:  0.3677614038072632
iteration : 3079
train acc:  0.84375
train loss:  0.3899088203907013
train gradient:  0.4179133836207979
iteration : 3080
train acc:  0.8125
train loss:  0.4037867486476898
train gradient:  0.27415481441372413
iteration : 3081
train acc:  0.8359375
train loss:  0.35643109679222107
train gradient:  0.3202125225624112
iteration : 3082
train acc:  0.78125
train loss:  0.3899320960044861
train gradient:  0.33106575701164564
iteration : 3083
train acc:  0.8046875
train loss:  0.3984546661376953
train gradient:  0.33877998854333546
iteration : 3084
train acc:  0.8125
train loss:  0.40247803926467896
train gradient:  0.45618224221294684
iteration : 3085
train acc:  0.828125
train loss:  0.3936489224433899
train gradient:  0.2441011623846814
iteration : 3086
train acc:  0.828125
train loss:  0.3863663375377655
train gradient:  0.2871190057380218
iteration : 3087
train acc:  0.796875
train loss:  0.44334226846694946
train gradient:  0.374515164399813
iteration : 3088
train acc:  0.7890625
train loss:  0.42528238892555237
train gradient:  0.4038942325645938
iteration : 3089
train acc:  0.8125
train loss:  0.38446277379989624
train gradient:  0.3535578177573784
iteration : 3090
train acc:  0.8359375
train loss:  0.3980313539505005
train gradient:  0.4040256429053909
iteration : 3091
train acc:  0.84375
train loss:  0.35038259625434875
train gradient:  0.22682876861621576
iteration : 3092
train acc:  0.8203125
train loss:  0.3589320778846741
train gradient:  0.34650647172974547
iteration : 3093
train acc:  0.875
train loss:  0.3368626534938812
train gradient:  0.21840813307754658
iteration : 3094
train acc:  0.8671875
train loss:  0.3607119619846344
train gradient:  0.3190192347535959
iteration : 3095
train acc:  0.8203125
train loss:  0.3898342251777649
train gradient:  0.3803022311587465
iteration : 3096
train acc:  0.84375
train loss:  0.3763549327850342
train gradient:  0.3251027529535008
iteration : 3097
train acc:  0.765625
train loss:  0.4604029655456543
train gradient:  0.5064115572171133
iteration : 3098
train acc:  0.8515625
train loss:  0.35323166847229004
train gradient:  0.2610554996430534
iteration : 3099
train acc:  0.8046875
train loss:  0.39877772331237793
train gradient:  0.32049071038110805
iteration : 3100
train acc:  0.9140625
train loss:  0.3048133850097656
train gradient:  0.2233687617165618
iteration : 3101
train acc:  0.8359375
train loss:  0.3729429841041565
train gradient:  0.40234521377907617
iteration : 3102
train acc:  0.8046875
train loss:  0.4040403366088867
train gradient:  0.41152705530195377
iteration : 3103
train acc:  0.875
train loss:  0.3253323435783386
train gradient:  0.30444384630914123
iteration : 3104
train acc:  0.8359375
train loss:  0.34750431776046753
train gradient:  0.20937962122712545
iteration : 3105
train acc:  0.8515625
train loss:  0.44398003816604614
train gradient:  0.3456941727425965
iteration : 3106
train acc:  0.8203125
train loss:  0.4680942893028259
train gradient:  0.5528359854182722
iteration : 3107
train acc:  0.8125
train loss:  0.3445679545402527
train gradient:  0.27571000772012316
iteration : 3108
train acc:  0.8125
train loss:  0.4389921724796295
train gradient:  0.3428134553776323
iteration : 3109
train acc:  0.8515625
train loss:  0.35688191652297974
train gradient:  0.2953090590964335
iteration : 3110
train acc:  0.84375
train loss:  0.30713844299316406
train gradient:  0.1965449086070512
iteration : 3111
train acc:  0.859375
train loss:  0.32142525911331177
train gradient:  0.2832767952873178
iteration : 3112
train acc:  0.8203125
train loss:  0.4358980357646942
train gradient:  0.5747755311812965
iteration : 3113
train acc:  0.8515625
train loss:  0.36751264333724976
train gradient:  0.2960679510786169
iteration : 3114
train acc:  0.8203125
train loss:  0.4455205798149109
train gradient:  0.4144189912461106
iteration : 3115
train acc:  0.8515625
train loss:  0.38837286829948425
train gradient:  0.4040005116896821
iteration : 3116
train acc:  0.8125
train loss:  0.36122697591781616
train gradient:  0.2922855199947762
iteration : 3117
train acc:  0.84375
train loss:  0.34651249647140503
train gradient:  0.293009320220641
iteration : 3118
train acc:  0.796875
train loss:  0.4260183572769165
train gradient:  0.4256892876591565
iteration : 3119
train acc:  0.8671875
train loss:  0.38011711835861206
train gradient:  0.3435954497929678
iteration : 3120
train acc:  0.8125
train loss:  0.45839497447013855
train gradient:  0.6169059653632059
iteration : 3121
train acc:  0.796875
train loss:  0.3664095997810364
train gradient:  0.24084972625432607
iteration : 3122
train acc:  0.8359375
train loss:  0.31692856550216675
train gradient:  0.23438974422803763
iteration : 3123
train acc:  0.796875
train loss:  0.42851364612579346
train gradient:  0.3804082214838299
iteration : 3124
train acc:  0.8515625
train loss:  0.35263100266456604
train gradient:  0.324682291405541
iteration : 3125
train acc:  0.78125
train loss:  0.43390434980392456
train gradient:  0.4006443983828041
iteration : 3126
train acc:  0.859375
train loss:  0.28455549478530884
train gradient:  0.20038931262284704
iteration : 3127
train acc:  0.8046875
train loss:  0.49349039793014526
train gradient:  0.5277211629812167
iteration : 3128
train acc:  0.84375
train loss:  0.3539828658103943
train gradient:  0.2881238651602832
iteration : 3129
train acc:  0.875
train loss:  0.3103025257587433
train gradient:  0.23303020157019677
iteration : 3130
train acc:  0.828125
train loss:  0.3836277723312378
train gradient:  0.3132821988578354
iteration : 3131
train acc:  0.8203125
train loss:  0.3749116063117981
train gradient:  0.3920876884913002
iteration : 3132
train acc:  0.859375
train loss:  0.3093377351760864
train gradient:  0.19791989555170428
iteration : 3133
train acc:  0.7578125
train loss:  0.4556998610496521
train gradient:  0.44069327930575414
iteration : 3134
train acc:  0.7890625
train loss:  0.42351973056793213
train gradient:  0.4394684264680264
iteration : 3135
train acc:  0.84375
train loss:  0.36288750171661377
train gradient:  0.3336289825348105
iteration : 3136
train acc:  0.875
train loss:  0.3331860899925232
train gradient:  0.2119126278758387
iteration : 3137
train acc:  0.796875
train loss:  0.39829033613204956
train gradient:  0.30497090045309644
iteration : 3138
train acc:  0.859375
train loss:  0.3572970926761627
train gradient:  0.24428628125927584
iteration : 3139
train acc:  0.8046875
train loss:  0.40834352374076843
train gradient:  0.2932744470762866
iteration : 3140
train acc:  0.8046875
train loss:  0.3888111710548401
train gradient:  0.3936903114999091
iteration : 3141
train acc:  0.8203125
train loss:  0.4054798483848572
train gradient:  0.2698234670062045
iteration : 3142
train acc:  0.828125
train loss:  0.4229573607444763
train gradient:  0.3514251257199586
iteration : 3143
train acc:  0.8359375
train loss:  0.3352869153022766
train gradient:  0.2794616083552756
iteration : 3144
train acc:  0.8203125
train loss:  0.4945317208766937
train gradient:  0.41140480272811447
iteration : 3145
train acc:  0.890625
train loss:  0.28444015979766846
train gradient:  0.20198524889212752
iteration : 3146
train acc:  0.8828125
train loss:  0.3528357744216919
train gradient:  0.2588887888222109
iteration : 3147
train acc:  0.8828125
train loss:  0.33601799607276917
train gradient:  0.2521690344701066
iteration : 3148
train acc:  0.8125
train loss:  0.46074846386909485
train gradient:  0.5126856655514859
iteration : 3149
train acc:  0.8125
train loss:  0.42508068680763245
train gradient:  0.4207853274055471
iteration : 3150
train acc:  0.8125
train loss:  0.3980857729911804
train gradient:  0.4300072675843936
iteration : 3151
train acc:  0.7890625
train loss:  0.4912217855453491
train gradient:  0.6528432157305852
iteration : 3152
train acc:  0.8828125
train loss:  0.31556856632232666
train gradient:  0.21558737830177782
iteration : 3153
train acc:  0.859375
train loss:  0.40570420026779175
train gradient:  0.35795270121344647
iteration : 3154
train acc:  0.8203125
train loss:  0.40722179412841797
train gradient:  0.35355992581982565
iteration : 3155
train acc:  0.796875
train loss:  0.40090399980545044
train gradient:  0.4569779383034975
iteration : 3156
train acc:  0.8671875
train loss:  0.2906368374824524
train gradient:  0.25769130527744016
iteration : 3157
train acc:  0.796875
train loss:  0.41920506954193115
train gradient:  0.4063602595289021
iteration : 3158
train acc:  0.8125
train loss:  0.3723580837249756
train gradient:  0.36397785485054995
iteration : 3159
train acc:  0.859375
train loss:  0.36283358931541443
train gradient:  0.36747136810683384
iteration : 3160
train acc:  0.828125
train loss:  0.40525737404823303
train gradient:  0.39028435065477446
iteration : 3161
train acc:  0.859375
train loss:  0.3318631052970886
train gradient:  0.2928534419204914
iteration : 3162
train acc:  0.8515625
train loss:  0.36067503690719604
train gradient:  0.33623609883048244
iteration : 3163
train acc:  0.8359375
train loss:  0.35959330201148987
train gradient:  0.30786873882329796
iteration : 3164
train acc:  0.8203125
train loss:  0.466014564037323
train gradient:  0.5157399444405054
iteration : 3165
train acc:  0.8046875
train loss:  0.4443064332008362
train gradient:  0.44077300010286313
iteration : 3166
train acc:  0.7890625
train loss:  0.4575263261795044
train gradient:  0.3935426286730541
iteration : 3167
train acc:  0.765625
train loss:  0.5009639263153076
train gradient:  0.5321399203150805
iteration : 3168
train acc:  0.8046875
train loss:  0.36341437697410583
train gradient:  0.36235612998087807
iteration : 3169
train acc:  0.8046875
train loss:  0.44170355796813965
train gradient:  0.3746029508000766
iteration : 3170
train acc:  0.8125
train loss:  0.37087082862854004
train gradient:  0.3701302994307158
iteration : 3171
train acc:  0.8125
train loss:  0.41024017333984375
train gradient:  0.33061191869260836
iteration : 3172
train acc:  0.84375
train loss:  0.37105071544647217
train gradient:  0.33464273352002044
iteration : 3173
train acc:  0.890625
train loss:  0.2837284803390503
train gradient:  0.22768595489945936
iteration : 3174
train acc:  0.8515625
train loss:  0.3548961281776428
train gradient:  0.21857554287227007
iteration : 3175
train acc:  0.7734375
train loss:  0.48209935426712036
train gradient:  0.5046568149714652
iteration : 3176
train acc:  0.8125
train loss:  0.385378360748291
train gradient:  0.27817472302410784
iteration : 3177
train acc:  0.8203125
train loss:  0.406911164522171
train gradient:  0.39712264299834826
iteration : 3178
train acc:  0.8203125
train loss:  0.39855217933654785
train gradient:  0.3147292784306919
iteration : 3179
train acc:  0.796875
train loss:  0.4387405514717102
train gradient:  0.3655463914469275
iteration : 3180
train acc:  0.78125
train loss:  0.3544212877750397
train gradient:  0.35338742877629903
iteration : 3181
train acc:  0.8984375
train loss:  0.2838425040245056
train gradient:  0.22724182481013894
iteration : 3182
train acc:  0.8359375
train loss:  0.35891997814178467
train gradient:  0.3452714246122507
iteration : 3183
train acc:  0.8125
train loss:  0.4188759922981262
train gradient:  0.25622118166805635
iteration : 3184
train acc:  0.8125
train loss:  0.38278499245643616
train gradient:  0.32149640210127034
iteration : 3185
train acc:  0.875
train loss:  0.2777695655822754
train gradient:  0.17088740243234643
iteration : 3186
train acc:  0.8515625
train loss:  0.34623008966445923
train gradient:  0.28767829124245653
iteration : 3187
train acc:  0.8203125
train loss:  0.39778947830200195
train gradient:  0.3631185803122563
iteration : 3188
train acc:  0.796875
train loss:  0.415046751499176
train gradient:  0.3338523742455679
iteration : 3189
train acc:  0.859375
train loss:  0.33815276622772217
train gradient:  0.2356126245087649
iteration : 3190
train acc:  0.765625
train loss:  0.5045648813247681
train gradient:  0.447851918885744
iteration : 3191
train acc:  0.8203125
train loss:  0.4152657389640808
train gradient:  0.42656084544020223
iteration : 3192
train acc:  0.859375
train loss:  0.3423593044281006
train gradient:  0.27648336362494097
iteration : 3193
train acc:  0.875
train loss:  0.33666276931762695
train gradient:  0.25207148079961056
iteration : 3194
train acc:  0.859375
train loss:  0.31961843371391296
train gradient:  0.24989686290274085
iteration : 3195
train acc:  0.84375
train loss:  0.4623304605484009
train gradient:  0.3706368117929822
iteration : 3196
train acc:  0.8828125
train loss:  0.3280312120914459
train gradient:  0.2556059894948562
iteration : 3197
train acc:  0.8125
train loss:  0.36627984046936035
train gradient:  0.30022557422428986
iteration : 3198
train acc:  0.8046875
train loss:  0.43897807598114014
train gradient:  0.41587365213802413
iteration : 3199
train acc:  0.8359375
train loss:  0.3772696852684021
train gradient:  0.4425459649978022
iteration : 3200
train acc:  0.8125
train loss:  0.41170522570610046
train gradient:  0.4251642752831543
iteration : 3201
train acc:  0.8828125
train loss:  0.31815165281295776
train gradient:  0.3014827644642931
iteration : 3202
train acc:  0.890625
train loss:  0.29028820991516113
train gradient:  0.18362804275345873
iteration : 3203
train acc:  0.8125
train loss:  0.4376673698425293
train gradient:  0.4380495539788515
iteration : 3204
train acc:  0.8046875
train loss:  0.36620184779167175
train gradient:  0.4540423956521932
iteration : 3205
train acc:  0.8515625
train loss:  0.31923919916152954
train gradient:  0.31124207571354445
iteration : 3206
train acc:  0.78125
train loss:  0.4751710593700409
train gradient:  0.6237184152828779
iteration : 3207
train acc:  0.8046875
train loss:  0.40224289894104004
train gradient:  0.28073484231600465
iteration : 3208
train acc:  0.8125
train loss:  0.3891271650791168
train gradient:  0.33665922471108956
iteration : 3209
train acc:  0.8984375
train loss:  0.29917019605636597
train gradient:  0.23415908200438537
iteration : 3210
train acc:  0.8125
train loss:  0.4146302342414856
train gradient:  0.409572151641805
iteration : 3211
train acc:  0.875
train loss:  0.34959226846694946
train gradient:  0.2775321973989323
iteration : 3212
train acc:  0.828125
train loss:  0.36747968196868896
train gradient:  0.2444454948829179
iteration : 3213
train acc:  0.9140625
train loss:  0.26437169313430786
train gradient:  0.27605843854352485
iteration : 3214
train acc:  0.8828125
train loss:  0.3117300868034363
train gradient:  0.3724948753573475
iteration : 3215
train acc:  0.796875
train loss:  0.43352586030960083
train gradient:  0.686503000382618
iteration : 3216
train acc:  0.8359375
train loss:  0.3658803105354309
train gradient:  0.26914494060584454
iteration : 3217
train acc:  0.875
train loss:  0.2873193919658661
train gradient:  0.19372231670501674
iteration : 3218
train acc:  0.796875
train loss:  0.39270973205566406
train gradient:  0.2999256044747526
iteration : 3219
train acc:  0.828125
train loss:  0.3693273961544037
train gradient:  0.25866434559954465
iteration : 3220
train acc:  0.8359375
train loss:  0.41659480333328247
train gradient:  0.3860264353680588
iteration : 3221
train acc:  0.8671875
train loss:  0.3227394223213196
train gradient:  0.33788540199354455
iteration : 3222
train acc:  0.8671875
train loss:  0.30577966570854187
train gradient:  0.3146395658563318
iteration : 3223
train acc:  0.8203125
train loss:  0.355668842792511
train gradient:  0.2425165446560535
iteration : 3224
train acc:  0.796875
train loss:  0.5061826705932617
train gradient:  0.7415494557076654
iteration : 3225
train acc:  0.859375
train loss:  0.3283371329307556
train gradient:  0.2565306101896908
iteration : 3226
train acc:  0.875
train loss:  0.32016247510910034
train gradient:  0.2546706549398521
iteration : 3227
train acc:  0.8359375
train loss:  0.37065133452415466
train gradient:  0.30787411031285905
iteration : 3228
train acc:  0.8359375
train loss:  0.4102686047554016
train gradient:  0.47786530824088225
iteration : 3229
train acc:  0.8828125
train loss:  0.28779730200767517
train gradient:  0.250014892719132
iteration : 3230
train acc:  0.8359375
train loss:  0.34159207344055176
train gradient:  0.27556115142023385
iteration : 3231
train acc:  0.84375
train loss:  0.3906952142715454
train gradient:  0.3174138283912595
iteration : 3232
train acc:  0.7734375
train loss:  0.391262412071228
train gradient:  0.4174227370794086
iteration : 3233
train acc:  0.7890625
train loss:  0.4525125026702881
train gradient:  0.47047593033564905
iteration : 3234
train acc:  0.796875
train loss:  0.3904055953025818
train gradient:  0.3551837025664064
iteration : 3235
train acc:  0.8203125
train loss:  0.36301687359809875
train gradient:  0.45670850476296926
iteration : 3236
train acc:  0.828125
train loss:  0.3654593825340271
train gradient:  0.280961468100814
iteration : 3237
train acc:  0.8125
train loss:  0.3546586334705353
train gradient:  0.3979058966307493
iteration : 3238
train acc:  0.84375
train loss:  0.38440465927124023
train gradient:  0.4467103135683177
iteration : 3239
train acc:  0.890625
train loss:  0.3077082335948944
train gradient:  0.3175066471557541
iteration : 3240
train acc:  0.890625
train loss:  0.32141703367233276
train gradient:  0.24627876498653428
iteration : 3241
train acc:  0.859375
train loss:  0.3445175886154175
train gradient:  0.36234757127097095
iteration : 3242
train acc:  0.7890625
train loss:  0.49593502283096313
train gradient:  0.8100629813398638
iteration : 3243
train acc:  0.828125
train loss:  0.36432474851608276
train gradient:  0.28710860511065533
iteration : 3244
train acc:  0.90625
train loss:  0.23118706047534943
train gradient:  0.16739633317569014
iteration : 3245
train acc:  0.8046875
train loss:  0.4731551706790924
train gradient:  0.4645347082707424
iteration : 3246
train acc:  0.8515625
train loss:  0.3689839839935303
train gradient:  0.33464195921771217
iteration : 3247
train acc:  0.8125
train loss:  0.378188818693161
train gradient:  0.34275392100892355
iteration : 3248
train acc:  0.8203125
train loss:  0.35691606998443604
train gradient:  0.2946033082252638
iteration : 3249
train acc:  0.8515625
train loss:  0.35312098264694214
train gradient:  0.35330653958736513
iteration : 3250
train acc:  0.796875
train loss:  0.4432670474052429
train gradient:  0.3800064536496707
iteration : 3251
train acc:  0.859375
train loss:  0.34521055221557617
train gradient:  0.2100889684656379
iteration : 3252
train acc:  0.828125
train loss:  0.3736284375190735
train gradient:  0.3389783944145601
iteration : 3253
train acc:  0.890625
train loss:  0.3091711401939392
train gradient:  0.2767176401550715
iteration : 3254
train acc:  0.8125
train loss:  0.3777456283569336
train gradient:  0.28262596825060715
iteration : 3255
train acc:  0.7890625
train loss:  0.40794187784194946
train gradient:  0.39732326977320925
iteration : 3256
train acc:  0.859375
train loss:  0.39728283882141113
train gradient:  0.33973154647932735
iteration : 3257
train acc:  0.8125
train loss:  0.4037173092365265
train gradient:  0.40146123672321077
iteration : 3258
train acc:  0.828125
train loss:  0.3883800804615021
train gradient:  0.2814807723409325
iteration : 3259
train acc:  0.859375
train loss:  0.40504345297813416
train gradient:  0.3765810653145681
iteration : 3260
train acc:  0.8515625
train loss:  0.3551807999610901
train gradient:  0.25120422845706847
iteration : 3261
train acc:  0.8359375
train loss:  0.3699798285961151
train gradient:  0.45531515998702987
iteration : 3262
train acc:  0.8359375
train loss:  0.3673948049545288
train gradient:  0.4022949554361657
iteration : 3263
train acc:  0.828125
train loss:  0.3749365210533142
train gradient:  0.2679234235908493
iteration : 3264
train acc:  0.8359375
train loss:  0.35702234506607056
train gradient:  0.3265216718574917
iteration : 3265
train acc:  0.859375
train loss:  0.3201198875904083
train gradient:  0.2295128758543444
iteration : 3266
train acc:  0.8125
train loss:  0.3718169331550598
train gradient:  0.25919811299619444
iteration : 3267
train acc:  0.8125
train loss:  0.4192458689212799
train gradient:  0.4722361540642541
iteration : 3268
train acc:  0.859375
train loss:  0.35342830419540405
train gradient:  0.3722666005772029
iteration : 3269
train acc:  0.8203125
train loss:  0.41637420654296875
train gradient:  0.3534912113799252
iteration : 3270
train acc:  0.859375
train loss:  0.367435097694397
train gradient:  0.3751518424397147
iteration : 3271
train acc:  0.875
train loss:  0.3359229564666748
train gradient:  0.2511270983910726
iteration : 3272
train acc:  0.8515625
train loss:  0.39083975553512573
train gradient:  0.23976359842082448
iteration : 3273
train acc:  0.8203125
train loss:  0.4149180054664612
train gradient:  0.41244883490273326
iteration : 3274
train acc:  0.8046875
train loss:  0.46014684438705444
train gradient:  0.47560751688899144
iteration : 3275
train acc:  0.796875
train loss:  0.4822956919670105
train gradient:  0.41764060462258906
iteration : 3276
train acc:  0.859375
train loss:  0.3025391697883606
train gradient:  0.25196778995670777
iteration : 3277
train acc:  0.8046875
train loss:  0.39244040846824646
train gradient:  0.32788793925894416
iteration : 3278
train acc:  0.8203125
train loss:  0.41853776574134827
train gradient:  0.3645122809153696
iteration : 3279
train acc:  0.828125
train loss:  0.43015968799591064
train gradient:  0.32498059270144664
iteration : 3280
train acc:  0.8515625
train loss:  0.3481213450431824
train gradient:  0.32866164182676627
iteration : 3281
train acc:  0.8203125
train loss:  0.39629483222961426
train gradient:  0.3447383656401838
iteration : 3282
train acc:  0.8671875
train loss:  0.35174259543418884
train gradient:  0.2903193698784849
iteration : 3283
train acc:  0.796875
train loss:  0.40955355763435364
train gradient:  0.35733640709776326
iteration : 3284
train acc:  0.875
train loss:  0.3114442229270935
train gradient:  0.21244037077807382
iteration : 3285
train acc:  0.8984375
train loss:  0.2946575880050659
train gradient:  0.17900246337457712
iteration : 3286
train acc:  0.7578125
train loss:  0.5297983884811401
train gradient:  0.66479640661058
iteration : 3287
train acc:  0.8671875
train loss:  0.3382616341114044
train gradient:  0.31035422322349293
iteration : 3288
train acc:  0.8046875
train loss:  0.4218044877052307
train gradient:  0.4017133875224933
iteration : 3289
train acc:  0.796875
train loss:  0.4713793396949768
train gradient:  0.35994976923312433
iteration : 3290
train acc:  0.84375
train loss:  0.3119153380393982
train gradient:  0.17140515159345812
iteration : 3291
train acc:  0.828125
train loss:  0.36906784772872925
train gradient:  0.24374545056979624
iteration : 3292
train acc:  0.8203125
train loss:  0.42334890365600586
train gradient:  0.266511947586343
iteration : 3293
train acc:  0.8671875
train loss:  0.3225852847099304
train gradient:  0.2215349911106682
iteration : 3294
train acc:  0.8046875
train loss:  0.3952898383140564
train gradient:  0.5960860819296678
iteration : 3295
train acc:  0.828125
train loss:  0.36922648549079895
train gradient:  0.30968657801938654
iteration : 3296
train acc:  0.8203125
train loss:  0.4037555754184723
train gradient:  0.726883305097805
iteration : 3297
train acc:  0.8671875
train loss:  0.336207777261734
train gradient:  0.3058296490091073
iteration : 3298
train acc:  0.8828125
train loss:  0.32904937863349915
train gradient:  0.3194874262427877
iteration : 3299
train acc:  0.828125
train loss:  0.3662227988243103
train gradient:  0.3388327586945664
iteration : 3300
train acc:  0.84375
train loss:  0.423783540725708
train gradient:  0.5360101490893333
iteration : 3301
train acc:  0.8125
train loss:  0.40590545535087585
train gradient:  0.3152609884135305
iteration : 3302
train acc:  0.7578125
train loss:  0.4752885699272156
train gradient:  0.4803349872538929
iteration : 3303
train acc:  0.8203125
train loss:  0.39064890146255493
train gradient:  0.3610547969374796
iteration : 3304
train acc:  0.8046875
train loss:  0.3610772490501404
train gradient:  0.24690363251636405
iteration : 3305
train acc:  0.9296875
train loss:  0.2524062991142273
train gradient:  0.21389200379082726
iteration : 3306
train acc:  0.828125
train loss:  0.4015224575996399
train gradient:  0.308299900994265
iteration : 3307
train acc:  0.7734375
train loss:  0.46584057807922363
train gradient:  0.40079855171796647
iteration : 3308
train acc:  0.8125
train loss:  0.4099959135055542
train gradient:  0.34039039067081384
iteration : 3309
train acc:  0.78125
train loss:  0.427803635597229
train gradient:  0.29622789506479535
iteration : 3310
train acc:  0.8515625
train loss:  0.36406397819519043
train gradient:  0.2563683437488631
iteration : 3311
train acc:  0.8515625
train loss:  0.3473895490169525
train gradient:  0.28748555139837917
iteration : 3312
train acc:  0.828125
train loss:  0.33023884892463684
train gradient:  0.2994091638785671
iteration : 3313
train acc:  0.8046875
train loss:  0.42909979820251465
train gradient:  0.33781640134259605
iteration : 3314
train acc:  0.84375
train loss:  0.36648496985435486
train gradient:  0.32576366868580187
iteration : 3315
train acc:  0.84375
train loss:  0.40921932458877563
train gradient:  0.4338290249537604
iteration : 3316
train acc:  0.859375
train loss:  0.3180311620235443
train gradient:  0.245385670253222
iteration : 3317
train acc:  0.796875
train loss:  0.4353187382221222
train gradient:  0.4298790063088475
iteration : 3318
train acc:  0.875
train loss:  0.32340875267982483
train gradient:  0.21195346889284788
iteration : 3319
train acc:  0.84375
train loss:  0.36346620321273804
train gradient:  0.3169839397511893
iteration : 3320
train acc:  0.828125
train loss:  0.37431642413139343
train gradient:  0.3386940469728851
iteration : 3321
train acc:  0.796875
train loss:  0.40692275762557983
train gradient:  0.3683549081864514
iteration : 3322
train acc:  0.890625
train loss:  0.3003524839878082
train gradient:  0.25463058314993114
iteration : 3323
train acc:  0.859375
train loss:  0.3034712076187134
train gradient:  0.22291224481237515
iteration : 3324
train acc:  0.828125
train loss:  0.4033471941947937
train gradient:  0.3091663133205008
iteration : 3325
train acc:  0.859375
train loss:  0.3316153883934021
train gradient:  0.305851890944877
iteration : 3326
train acc:  0.8203125
train loss:  0.4232255816459656
train gradient:  0.3549683224634474
iteration : 3327
train acc:  0.8203125
train loss:  0.3912616968154907
train gradient:  0.3154612837238635
iteration : 3328
train acc:  0.78125
train loss:  0.3831849992275238
train gradient:  0.26517189302139255
iteration : 3329
train acc:  0.7734375
train loss:  0.4424438178539276
train gradient:  0.35163223688249745
iteration : 3330
train acc:  0.796875
train loss:  0.38999050855636597
train gradient:  0.3703927635629308
iteration : 3331
train acc:  0.875
train loss:  0.3151945471763611
train gradient:  0.18621761967324832
iteration : 3332
train acc:  0.84375
train loss:  0.3223522901535034
train gradient:  0.24867816387825134
iteration : 3333
train acc:  0.8828125
train loss:  0.32417845726013184
train gradient:  0.23934646152639988
iteration : 3334
train acc:  0.7890625
train loss:  0.501234769821167
train gradient:  0.43886646429727244
iteration : 3335
train acc:  0.8203125
train loss:  0.35720425844192505
train gradient:  0.3181288675623441
iteration : 3336
train acc:  0.8515625
train loss:  0.31629568338394165
train gradient:  0.32089895841805094
iteration : 3337
train acc:  0.8359375
train loss:  0.38535118103027344
train gradient:  0.2708223257505382
iteration : 3338
train acc:  0.84375
train loss:  0.3746049702167511
train gradient:  0.4089178880555201
iteration : 3339
train acc:  0.8203125
train loss:  0.3370303511619568
train gradient:  0.2417123762956498
iteration : 3340
train acc:  0.8203125
train loss:  0.33154332637786865
train gradient:  0.2207101963882877
iteration : 3341
train acc:  0.8515625
train loss:  0.30432602763175964
train gradient:  0.17998096939474653
iteration : 3342
train acc:  0.8046875
train loss:  0.42130911350250244
train gradient:  0.3164172962774976
iteration : 3343
train acc:  0.8515625
train loss:  0.35889866948127747
train gradient:  0.40213521250595363
iteration : 3344
train acc:  0.7578125
train loss:  0.5147033929824829
train gradient:  0.5312894985953609
iteration : 3345
train acc:  0.8046875
train loss:  0.4276103377342224
train gradient:  0.36094316468857474
iteration : 3346
train acc:  0.828125
train loss:  0.34082192182540894
train gradient:  0.2288378204061568
iteration : 3347
train acc:  0.859375
train loss:  0.29063737392425537
train gradient:  0.2381487005342789
iteration : 3348
train acc:  0.8046875
train loss:  0.47641658782958984
train gradient:  0.3863094105066017
iteration : 3349
train acc:  0.8828125
train loss:  0.3452807664871216
train gradient:  0.32085012389043077
iteration : 3350
train acc:  0.8515625
train loss:  0.3381369113922119
train gradient:  0.29216716409915716
iteration : 3351
train acc:  0.8125
train loss:  0.40371668338775635
train gradient:  0.2916866491157627
iteration : 3352
train acc:  0.8515625
train loss:  0.32998570799827576
train gradient:  0.2614131408872125
iteration : 3353
train acc:  0.828125
train loss:  0.35015714168548584
train gradient:  0.4141703699674268
iteration : 3354
train acc:  0.8125
train loss:  0.4249238073825836
train gradient:  0.3503533532349959
iteration : 3355
train acc:  0.78125
train loss:  0.42024150490760803
train gradient:  0.38651579039002215
iteration : 3356
train acc:  0.8515625
train loss:  0.35263335704803467
train gradient:  0.3422164674224251
iteration : 3357
train acc:  0.796875
train loss:  0.39731085300445557
train gradient:  0.41414019344763237
iteration : 3358
train acc:  0.8046875
train loss:  0.4034418761730194
train gradient:  0.4360710819720825
iteration : 3359
train acc:  0.859375
train loss:  0.34718674421310425
train gradient:  0.3604753198835786
iteration : 3360
train acc:  0.84375
train loss:  0.3600587546825409
train gradient:  0.35123560769790424
iteration : 3361
train acc:  0.7890625
train loss:  0.4502248167991638
train gradient:  0.4960352918581613
iteration : 3362
train acc:  0.8125
train loss:  0.39352571964263916
train gradient:  0.30424201659712197
iteration : 3363
train acc:  0.828125
train loss:  0.42211949825286865
train gradient:  0.3111420055084727
iteration : 3364
train acc:  0.828125
train loss:  0.38010984659194946
train gradient:  0.20611228810136017
iteration : 3365
train acc:  0.7890625
train loss:  0.48167547583580017
train gradient:  0.5021469765091107
iteration : 3366
train acc:  0.765625
train loss:  0.4699990451335907
train gradient:  0.5032843680803142
iteration : 3367
train acc:  0.8125
train loss:  0.4183503985404968
train gradient:  0.3794067746841793
iteration : 3368
train acc:  0.828125
train loss:  0.3608429729938507
train gradient:  0.22316602505103142
iteration : 3369
train acc:  0.859375
train loss:  0.35564345121383667
train gradient:  0.28990637864859004
iteration : 3370
train acc:  0.828125
train loss:  0.36550480127334595
train gradient:  0.266718013273594
iteration : 3371
train acc:  0.84375
train loss:  0.36505210399627686
train gradient:  0.34340766506222536
iteration : 3372
train acc:  0.828125
train loss:  0.39778393507003784
train gradient:  0.34062853664469356
iteration : 3373
train acc:  0.8515625
train loss:  0.36758142709732056
train gradient:  0.3002821989748753
iteration : 3374
train acc:  0.8671875
train loss:  0.3369297385215759
train gradient:  0.33509286023886553
iteration : 3375
train acc:  0.78125
train loss:  0.4628969430923462
train gradient:  0.44790892250282244
iteration : 3376
train acc:  0.859375
train loss:  0.3243018090724945
train gradient:  0.288935233409048
iteration : 3377
train acc:  0.8828125
train loss:  0.2963509261608124
train gradient:  0.2535857591905385
iteration : 3378
train acc:  0.7890625
train loss:  0.367037832736969
train gradient:  0.3006933057685487
iteration : 3379
train acc:  0.828125
train loss:  0.36193951964378357
train gradient:  0.26135697502831223
iteration : 3380
train acc:  0.828125
train loss:  0.3590504229068756
train gradient:  0.32033280614523457
iteration : 3381
train acc:  0.84375
train loss:  0.33982759714126587
train gradient:  0.21785935532255934
iteration : 3382
train acc:  0.84375
train loss:  0.3430679440498352
train gradient:  0.22093633408019658
iteration : 3383
train acc:  0.8203125
train loss:  0.3287560045719147
train gradient:  0.3062870007172576
iteration : 3384
train acc:  0.796875
train loss:  0.38486531376838684
train gradient:  0.31122458553490984
iteration : 3385
train acc:  0.796875
train loss:  0.42911386489868164
train gradient:  0.34592877294762936
iteration : 3386
train acc:  0.828125
train loss:  0.40626847743988037
train gradient:  0.4685038412095207
iteration : 3387
train acc:  0.78125
train loss:  0.45006063580513
train gradient:  0.5291590297187836
iteration : 3388
train acc:  0.8828125
train loss:  0.30969732999801636
train gradient:  0.22098698424690888
iteration : 3389
train acc:  0.8046875
train loss:  0.4103993773460388
train gradient:  0.3449752701116125
iteration : 3390
train acc:  0.8203125
train loss:  0.384019672870636
train gradient:  0.33156210853527845
iteration : 3391
train acc:  0.828125
train loss:  0.3853013813495636
train gradient:  0.2752240759794338
iteration : 3392
train acc:  0.8828125
train loss:  0.37745413184165955
train gradient:  0.24816759893682067
iteration : 3393
train acc:  0.7890625
train loss:  0.39061611890792847
train gradient:  0.3139152671175473
iteration : 3394
train acc:  0.828125
train loss:  0.3915129601955414
train gradient:  0.28717461094132474
iteration : 3395
train acc:  0.84375
train loss:  0.4027673602104187
train gradient:  0.2802103118425991
iteration : 3396
train acc:  0.8125
train loss:  0.367605060338974
train gradient:  0.27195877898110427
iteration : 3397
train acc:  0.8203125
train loss:  0.3828774392604828
train gradient:  0.360036335524475
iteration : 3398
train acc:  0.8125
train loss:  0.41522353887557983
train gradient:  0.3069190935845597
iteration : 3399
train acc:  0.8671875
train loss:  0.3426455557346344
train gradient:  0.3360030135667222
iteration : 3400
train acc:  0.8515625
train loss:  0.36068111658096313
train gradient:  0.3814373256867512
iteration : 3401
train acc:  0.8359375
train loss:  0.4079964756965637
train gradient:  0.36823709967128737
iteration : 3402
train acc:  0.828125
train loss:  0.3724387288093567
train gradient:  0.2950425589332034
iteration : 3403
train acc:  0.8671875
train loss:  0.319202184677124
train gradient:  0.2780212234503554
iteration : 3404
train acc:  0.7890625
train loss:  0.3881613612174988
train gradient:  0.33754316009623103
iteration : 3405
train acc:  0.8671875
train loss:  0.32835131883621216
train gradient:  0.2126100879684351
iteration : 3406
train acc:  0.8359375
train loss:  0.319882869720459
train gradient:  0.25131963818337905
iteration : 3407
train acc:  0.7890625
train loss:  0.4542073607444763
train gradient:  0.4658636620578434
iteration : 3408
train acc:  0.8671875
train loss:  0.3395625352859497
train gradient:  0.36153095598535856
iteration : 3409
train acc:  0.7734375
train loss:  0.4959146976470947
train gradient:  0.8247452893797207
iteration : 3410
train acc:  0.84375
train loss:  0.36198168992996216
train gradient:  0.2941901262646721
iteration : 3411
train acc:  0.84375
train loss:  0.40949419140815735
train gradient:  0.3758755062847846
iteration : 3412
train acc:  0.875
train loss:  0.345368891954422
train gradient:  0.37945890183655534
iteration : 3413
train acc:  0.75
train loss:  0.5874460339546204
train gradient:  0.6869923071530812
iteration : 3414
train acc:  0.8125
train loss:  0.3683229088783264
train gradient:  0.27370090950346776
iteration : 3415
train acc:  0.8046875
train loss:  0.36739200353622437
train gradient:  0.35309524431415334
iteration : 3416
train acc:  0.8515625
train loss:  0.3101006746292114
train gradient:  0.2276281599729017
iteration : 3417
train acc:  0.84375
train loss:  0.3875519335269928
train gradient:  0.39362645449955463
iteration : 3418
train acc:  0.828125
train loss:  0.3404181897640228
train gradient:  0.3105631844937394
iteration : 3419
train acc:  0.828125
train loss:  0.38364139199256897
train gradient:  0.325423802534305
iteration : 3420
train acc:  0.875
train loss:  0.29950058460235596
train gradient:  0.3386973539964031
iteration : 3421
train acc:  0.8046875
train loss:  0.4533929228782654
train gradient:  0.5198425350417042
iteration : 3422
train acc:  0.8515625
train loss:  0.329351544380188
train gradient:  0.3071955034993245
iteration : 3423
train acc:  0.8125
train loss:  0.3848671615123749
train gradient:  0.35857662687686304
iteration : 3424
train acc:  0.8359375
train loss:  0.3609507083892822
train gradient:  0.23463678054178622
iteration : 3425
train acc:  0.859375
train loss:  0.32533401250839233
train gradient:  0.2857809498002765
iteration : 3426
train acc:  0.828125
train loss:  0.3826568126678467
train gradient:  0.41807073580590903
iteration : 3427
train acc:  0.7421875
train loss:  0.4440554082393646
train gradient:  0.4136007388841544
iteration : 3428
train acc:  0.7890625
train loss:  0.43549609184265137
train gradient:  0.48207925950409963
iteration : 3429
train acc:  0.7890625
train loss:  0.4483293890953064
train gradient:  0.4944865946552517
iteration : 3430
train acc:  0.8828125
train loss:  0.28557148575782776
train gradient:  0.19984535293689185
iteration : 3431
train acc:  0.8203125
train loss:  0.3332284688949585
train gradient:  0.25298374079235214
iteration : 3432
train acc:  0.828125
train loss:  0.4051562547683716
train gradient:  0.47313565426027393
iteration : 3433
train acc:  0.7890625
train loss:  0.4588789939880371
train gradient:  0.43644930525669284
iteration : 3434
train acc:  0.8515625
train loss:  0.35291093587875366
train gradient:  0.3233438151047871
iteration : 3435
train acc:  0.8515625
train loss:  0.350830614566803
train gradient:  0.3327762976170146
iteration : 3436
train acc:  0.796875
train loss:  0.36198118329048157
train gradient:  0.3612661431830187
iteration : 3437
train acc:  0.8203125
train loss:  0.408442884683609
train gradient:  0.3441958442175523
iteration : 3438
train acc:  0.796875
train loss:  0.45463913679122925
train gradient:  0.49793143206513085
iteration : 3439
train acc:  0.8203125
train loss:  0.37095654010772705
train gradient:  0.24707287860255797
iteration : 3440
train acc:  0.8671875
train loss:  0.3202817440032959
train gradient:  0.20662524252879125
iteration : 3441
train acc:  0.7734375
train loss:  0.4059112071990967
train gradient:  0.376823574951477
iteration : 3442
train acc:  0.859375
train loss:  0.3510652780532837
train gradient:  0.19193310106794492
iteration : 3443
train acc:  0.8125
train loss:  0.4080120921134949
train gradient:  0.26702337416517724
iteration : 3444
train acc:  0.8046875
train loss:  0.38542312383651733
train gradient:  0.27899580405060376
iteration : 3445
train acc:  0.7890625
train loss:  0.4145016074180603
train gradient:  0.31996400678353626
iteration : 3446
train acc:  0.84375
train loss:  0.3819132149219513
train gradient:  0.23739614294013112
iteration : 3447
train acc:  0.8203125
train loss:  0.35691672563552856
train gradient:  0.3592871254549748
iteration : 3448
train acc:  0.8671875
train loss:  0.3216369152069092
train gradient:  0.3288470044536516
iteration : 3449
train acc:  0.8515625
train loss:  0.3624535799026489
train gradient:  0.26090417934732163
iteration : 3450
train acc:  0.7421875
train loss:  0.5645989775657654
train gradient:  0.6065407165038476
iteration : 3451
train acc:  0.8359375
train loss:  0.3976050019264221
train gradient:  0.32946923238759285
iteration : 3452
train acc:  0.84375
train loss:  0.38709592819213867
train gradient:  0.29531506936771906
iteration : 3453
train acc:  0.9296875
train loss:  0.275696337223053
train gradient:  0.19309481849402232
iteration : 3454
train acc:  0.828125
train loss:  0.3862718939781189
train gradient:  0.22561705415287922
iteration : 3455
train acc:  0.8359375
train loss:  0.4429379105567932
train gradient:  0.34755159904522837
iteration : 3456
train acc:  0.765625
train loss:  0.4735618233680725
train gradient:  0.4408995221215034
iteration : 3457
train acc:  0.890625
train loss:  0.30021780729293823
train gradient:  0.2091436809189025
iteration : 3458
train acc:  0.7421875
train loss:  0.47911524772644043
train gradient:  0.4631293307238835
iteration : 3459
train acc:  0.8125
train loss:  0.3471481204032898
train gradient:  0.2758126622754934
iteration : 3460
train acc:  0.859375
train loss:  0.3063250184059143
train gradient:  0.2179520507201645
iteration : 3461
train acc:  0.84375
train loss:  0.3816542625427246
train gradient:  0.43720142220040425
iteration : 3462
train acc:  0.8515625
train loss:  0.3363192081451416
train gradient:  0.3509053610581456
iteration : 3463
train acc:  0.828125
train loss:  0.3746480345726013
train gradient:  0.39201614789678496
iteration : 3464
train acc:  0.828125
train loss:  0.3553372025489807
train gradient:  0.3365230111560156
iteration : 3465
train acc:  0.859375
train loss:  0.33642393350601196
train gradient:  0.20513260390995525
iteration : 3466
train acc:  0.8203125
train loss:  0.3765068054199219
train gradient:  0.3367808517258983
iteration : 3467
train acc:  0.859375
train loss:  0.33835700154304504
train gradient:  0.23969174083514538
iteration : 3468
train acc:  0.8046875
train loss:  0.3939152657985687
train gradient:  0.2945791577169214
iteration : 3469
train acc:  0.8671875
train loss:  0.3660270571708679
train gradient:  0.29116363601940837
iteration : 3470
train acc:  0.890625
train loss:  0.3251473307609558
train gradient:  0.24680413376772037
iteration : 3471
train acc:  0.859375
train loss:  0.3673192262649536
train gradient:  0.3076080992390345
iteration : 3472
train acc:  0.90625
train loss:  0.28186577558517456
train gradient:  0.21487484658450817
iteration : 3473
train acc:  0.796875
train loss:  0.4303589463233948
train gradient:  0.3468356112278156
iteration : 3474
train acc:  0.8671875
train loss:  0.2854381799697876
train gradient:  0.2226221775474842
iteration : 3475
train acc:  0.8125
train loss:  0.40898486971855164
train gradient:  0.4087123568458526
iteration : 3476
train acc:  0.828125
train loss:  0.4070427417755127
train gradient:  0.3478036174515277
iteration : 3477
train acc:  0.84375
train loss:  0.3781852424144745
train gradient:  0.3468923906673732
iteration : 3478
train acc:  0.78125
train loss:  0.459567129611969
train gradient:  0.4430569686658647
iteration : 3479
train acc:  0.765625
train loss:  0.47496092319488525
train gradient:  0.5338462363718518
iteration : 3480
train acc:  0.8828125
train loss:  0.29775041341781616
train gradient:  0.25536985273432666
iteration : 3481
train acc:  0.8359375
train loss:  0.38260409235954285
train gradient:  0.29149645103941024
iteration : 3482
train acc:  0.8671875
train loss:  0.3685789108276367
train gradient:  0.45176386234747756
iteration : 3483
train acc:  0.8046875
train loss:  0.4010578989982605
train gradient:  0.2587026346039041
iteration : 3484
train acc:  0.796875
train loss:  0.47776904702186584
train gradient:  0.4017995240830296
iteration : 3485
train acc:  0.875
train loss:  0.3183397650718689
train gradient:  0.22841883826947182
iteration : 3486
train acc:  0.828125
train loss:  0.39386147260665894
train gradient:  0.26335585431768493
iteration : 3487
train acc:  0.859375
train loss:  0.34728100895881653
train gradient:  0.27212202313266226
iteration : 3488
train acc:  0.8359375
train loss:  0.36738526821136475
train gradient:  0.5529561608474634
iteration : 3489
train acc:  0.8515625
train loss:  0.3194330632686615
train gradient:  0.23623120670919068
iteration : 3490
train acc:  0.859375
train loss:  0.3569885492324829
train gradient:  0.2425948002008714
iteration : 3491
train acc:  0.890625
train loss:  0.32427293062210083
train gradient:  0.2325789671980138
iteration : 3492
train acc:  0.7890625
train loss:  0.3558725118637085
train gradient:  0.28044670605999117
iteration : 3493
train acc:  0.84375
train loss:  0.3380850553512573
train gradient:  0.23509401310307554
iteration : 3494
train acc:  0.84375
train loss:  0.3501935601234436
train gradient:  0.3613155466284538
iteration : 3495
train acc:  0.828125
train loss:  0.4091012179851532
train gradient:  0.3796199935577825
iteration : 3496
train acc:  0.8203125
train loss:  0.4231191873550415
train gradient:  0.389019851439143
iteration : 3497
train acc:  0.8359375
train loss:  0.4401344656944275
train gradient:  0.37670122268646533
iteration : 3498
train acc:  0.875
train loss:  0.29644185304641724
train gradient:  0.22045233050522856
iteration : 3499
train acc:  0.84375
train loss:  0.3437410891056061
train gradient:  0.30172496732556997
iteration : 3500
train acc:  0.84375
train loss:  0.36604470014572144
train gradient:  0.2954984557319674
iteration : 3501
train acc:  0.875
train loss:  0.3251613676548004
train gradient:  0.390803099499143
iteration : 3502
train acc:  0.84375
train loss:  0.3618226647377014
train gradient:  0.2812224870548182
iteration : 3503
train acc:  0.828125
train loss:  0.3675069510936737
train gradient:  0.33170883023800574
iteration : 3504
train acc:  0.7578125
train loss:  0.5466598868370056
train gradient:  0.6126174138177304
iteration : 3505
train acc:  0.84375
train loss:  0.3532100319862366
train gradient:  0.38017032511682575
iteration : 3506
train acc:  0.8125
train loss:  0.3536223769187927
train gradient:  0.27736204293537686
iteration : 3507
train acc:  0.890625
train loss:  0.3092014193534851
train gradient:  0.24104742905973253
iteration : 3508
train acc:  0.8359375
train loss:  0.3734193742275238
train gradient:  0.6423332170102002
iteration : 3509
train acc:  0.8125
train loss:  0.38822588324546814
train gradient:  0.378645594636268
iteration : 3510
train acc:  0.7734375
train loss:  0.43761521577835083
train gradient:  0.45749831137370783
iteration : 3511
train acc:  0.8359375
train loss:  0.38552433252334595
train gradient:  0.4919361917821419
iteration : 3512
train acc:  0.875
train loss:  0.29625260829925537
train gradient:  0.2671854500875803
iteration : 3513
train acc:  0.84375
train loss:  0.32188403606414795
train gradient:  0.20795847804391815
iteration : 3514
train acc:  0.84375
train loss:  0.3454577922821045
train gradient:  0.2679138332887466
iteration : 3515
train acc:  0.8203125
train loss:  0.37735462188720703
train gradient:  0.34882356219859995
iteration : 3516
train acc:  0.828125
train loss:  0.4066924452781677
train gradient:  0.4631821587297283
iteration : 3517
train acc:  0.7578125
train loss:  0.46493029594421387
train gradient:  0.45866636185059556
iteration : 3518
train acc:  0.8203125
train loss:  0.403261661529541
train gradient:  0.39767244970110693
iteration : 3519
train acc:  0.8359375
train loss:  0.37827712297439575
train gradient:  0.2917690549528971
iteration : 3520
train acc:  0.875
train loss:  0.3095800578594208
train gradient:  0.3128076750564454
iteration : 3521
train acc:  0.8671875
train loss:  0.29862451553344727
train gradient:  0.3051188601921331
iteration : 3522
train acc:  0.8515625
train loss:  0.3444606065750122
train gradient:  0.40435629951432034
iteration : 3523
train acc:  0.8125
train loss:  0.4070814549922943
train gradient:  0.32814543998234663
iteration : 3524
train acc:  0.8203125
train loss:  0.3812529742717743
train gradient:  0.5247534454433267
iteration : 3525
train acc:  0.734375
train loss:  0.5030303597450256
train gradient:  0.5313044137689407
iteration : 3526
train acc:  0.8046875
train loss:  0.4069068133831024
train gradient:  0.3343697119689074
iteration : 3527
train acc:  0.8828125
train loss:  0.30305904150009155
train gradient:  0.21662920064814914
iteration : 3528
train acc:  0.8203125
train loss:  0.35680270195007324
train gradient:  0.2791620982010567
iteration : 3529
train acc:  0.8125
train loss:  0.39554455876350403
train gradient:  0.28343463112126316
iteration : 3530
train acc:  0.8046875
train loss:  0.381408154964447
train gradient:  0.31551891789392855
iteration : 3531
train acc:  0.7578125
train loss:  0.5176535844802856
train gradient:  0.48649940680096715
iteration : 3532
train acc:  0.8671875
train loss:  0.3726039230823517
train gradient:  0.34448419788922596
iteration : 3533
train acc:  0.8125
train loss:  0.40571242570877075
train gradient:  0.3508032635828966
iteration : 3534
train acc:  0.859375
train loss:  0.35726362466812134
train gradient:  0.3111023166399205
iteration : 3535
train acc:  0.875
train loss:  0.3068380355834961
train gradient:  0.19436353496351683
iteration : 3536
train acc:  0.828125
train loss:  0.40309613943099976
train gradient:  0.32298522891315107
iteration : 3537
train acc:  0.8125
train loss:  0.3984460234642029
train gradient:  0.2952669973866307
iteration : 3538
train acc:  0.828125
train loss:  0.3870810270309448
train gradient:  0.38162772532672345
iteration : 3539
train acc:  0.828125
train loss:  0.416900098323822
train gradient:  0.3433174772618507
iteration : 3540
train acc:  0.84375
train loss:  0.3693941831588745
train gradient:  0.236315163576406
iteration : 3541
train acc:  0.8203125
train loss:  0.3695151209831238
train gradient:  0.2389891821444351
iteration : 3542
train acc:  0.859375
train loss:  0.3136768341064453
train gradient:  0.20601414993053668
iteration : 3543
train acc:  0.8359375
train loss:  0.38433876633644104
train gradient:  0.2761034498714354
iteration : 3544
train acc:  0.8359375
train loss:  0.3592997193336487
train gradient:  0.4650756578741031
iteration : 3545
train acc:  0.859375
train loss:  0.3008003830909729
train gradient:  0.21015961680804082
iteration : 3546
train acc:  0.875
train loss:  0.2637840509414673
train gradient:  0.19480173134990872
iteration : 3547
train acc:  0.8828125
train loss:  0.3439767360687256
train gradient:  0.33378653487683013
iteration : 3548
train acc:  0.84375
train loss:  0.46832913160324097
train gradient:  0.455091567319861
iteration : 3549
train acc:  0.796875
train loss:  0.43709036707878113
train gradient:  0.3133314000316573
iteration : 3550
train acc:  0.8046875
train loss:  0.3649883568286896
train gradient:  0.5498786874869439
iteration : 3551
train acc:  0.828125
train loss:  0.39697402715682983
train gradient:  0.32353419359766955
iteration : 3552
train acc:  0.8046875
train loss:  0.41598284244537354
train gradient:  0.33953979680303525
iteration : 3553
train acc:  0.7890625
train loss:  0.46968770027160645
train gradient:  0.3590083906771996
iteration : 3554
train acc:  0.8203125
train loss:  0.35434961318969727
train gradient:  0.2064423535284616
iteration : 3555
train acc:  0.84375
train loss:  0.3584465980529785
train gradient:  0.22694168643319257
iteration : 3556
train acc:  0.8125
train loss:  0.3744485378265381
train gradient:  0.3424694152286872
iteration : 3557
train acc:  0.8984375
train loss:  0.2995609939098358
train gradient:  0.21453185962256902
iteration : 3558
train acc:  0.8515625
train loss:  0.30027082562446594
train gradient:  0.20633326645688976
iteration : 3559
train acc:  0.7890625
train loss:  0.4013307988643646
train gradient:  0.28200702158364355
iteration : 3560
train acc:  0.8515625
train loss:  0.3725252151489258
train gradient:  0.2897443967057315
iteration : 3561
train acc:  0.828125
train loss:  0.38926756381988525
train gradient:  0.34402164166007904
iteration : 3562
train acc:  0.828125
train loss:  0.3581085503101349
train gradient:  0.29960407585466503
iteration : 3563
train acc:  0.8203125
train loss:  0.388531357049942
train gradient:  0.28121548121049605
iteration : 3564
train acc:  0.796875
train loss:  0.3768358826637268
train gradient:  0.33602358323402987
iteration : 3565
train acc:  0.84375
train loss:  0.3275272846221924
train gradient:  0.29239516411989525
iteration : 3566
train acc:  0.8203125
train loss:  0.3713454306125641
train gradient:  0.3263038467710842
iteration : 3567
train acc:  0.8671875
train loss:  0.349235475063324
train gradient:  0.25072475045214715
iteration : 3568
train acc:  0.84375
train loss:  0.37454327940940857
train gradient:  0.2951211857464815
iteration : 3569
train acc:  0.875
train loss:  0.3078220784664154
train gradient:  0.25501346122406254
iteration : 3570
train acc:  0.875
train loss:  0.33371421694755554
train gradient:  0.2981992901714778
iteration : 3571
train acc:  0.8515625
train loss:  0.3619389832019806
train gradient:  0.3363469546190037
iteration : 3572
train acc:  0.8515625
train loss:  0.367123007774353
train gradient:  0.29181021579282523
iteration : 3573
train acc:  0.8203125
train loss:  0.43515831232070923
train gradient:  0.35018784285904175
iteration : 3574
train acc:  0.8671875
train loss:  0.32343363761901855
train gradient:  0.2971700945566901
iteration : 3575
train acc:  0.875
train loss:  0.2941743731498718
train gradient:  0.20429033910748806
iteration : 3576
train acc:  0.890625
train loss:  0.28670501708984375
train gradient:  0.22738369091374233
iteration : 3577
train acc:  0.8359375
train loss:  0.37697023153305054
train gradient:  0.35554164220391266
iteration : 3578
train acc:  0.859375
train loss:  0.3726928234100342
train gradient:  0.32122837856182607
iteration : 3579
train acc:  0.8515625
train loss:  0.36104023456573486
train gradient:  0.26576544074599023
iteration : 3580
train acc:  0.8359375
train loss:  0.4139907956123352
train gradient:  0.4239403334394339
iteration : 3581
train acc:  0.8515625
train loss:  0.3627404570579529
train gradient:  0.24164146539603631
iteration : 3582
train acc:  0.84375
train loss:  0.35445940494537354
train gradient:  0.25281968828518075
iteration : 3583
train acc:  0.828125
train loss:  0.3819957375526428
train gradient:  0.37494611195412364
iteration : 3584
train acc:  0.8359375
train loss:  0.3576943576335907
train gradient:  0.28730731842942747
iteration : 3585
train acc:  0.8046875
train loss:  0.40351709723472595
train gradient:  0.3842343125141114
iteration : 3586
train acc:  0.8203125
train loss:  0.3993073105812073
train gradient:  0.38066672306410254
iteration : 3587
train acc:  0.78125
train loss:  0.5062947869300842
train gradient:  0.4910199658043486
iteration : 3588
train acc:  0.8671875
train loss:  0.3340146541595459
train gradient:  0.30900497363124724
iteration : 3589
train acc:  0.8515625
train loss:  0.38971200585365295
train gradient:  0.39495564186030285
iteration : 3590
train acc:  0.7890625
train loss:  0.4459078907966614
train gradient:  0.57674843672137
iteration : 3591
train acc:  0.8046875
train loss:  0.39377498626708984
train gradient:  0.3730091137375821
iteration : 3592
train acc:  0.8046875
train loss:  0.40223872661590576
train gradient:  0.3524288340833833
iteration : 3593
train acc:  0.8671875
train loss:  0.3486718535423279
train gradient:  0.3024803976847924
iteration : 3594
train acc:  0.7890625
train loss:  0.42499735951423645
train gradient:  0.34831094071883145
iteration : 3595
train acc:  0.90625
train loss:  0.2908374071121216
train gradient:  0.21287795811397242
iteration : 3596
train acc:  0.84375
train loss:  0.3336087167263031
train gradient:  0.22824272936302692
iteration : 3597
train acc:  0.8125
train loss:  0.4184083342552185
train gradient:  0.3379457526144102
iteration : 3598
train acc:  0.7734375
train loss:  0.47560542821884155
train gradient:  1.1221236159950254
iteration : 3599
train acc:  0.84375
train loss:  0.35232141613960266
train gradient:  0.240344137923242
iteration : 3600
train acc:  0.859375
train loss:  0.2950778603553772
train gradient:  0.23137625851706398
iteration : 3601
train acc:  0.8046875
train loss:  0.41675928235054016
train gradient:  0.3508932981093168
iteration : 3602
train acc:  0.8359375
train loss:  0.36821532249450684
train gradient:  0.28623628320657934
iteration : 3603
train acc:  0.859375
train loss:  0.3140143156051636
train gradient:  0.27385783589183516
iteration : 3604
train acc:  0.859375
train loss:  0.3302605152130127
train gradient:  0.27758429558281067
iteration : 3605
train acc:  0.7890625
train loss:  0.3840876817703247
train gradient:  0.29648615899192393
iteration : 3606
train acc:  0.8359375
train loss:  0.3214297592639923
train gradient:  0.30651217040426604
iteration : 3607
train acc:  0.8046875
train loss:  0.37561360001564026
train gradient:  0.46017848009417306
iteration : 3608
train acc:  0.828125
train loss:  0.38521042466163635
train gradient:  0.4668707298078922
iteration : 3609
train acc:  0.8046875
train loss:  0.4075468182563782
train gradient:  0.3081846956118925
iteration : 3610
train acc:  0.8046875
train loss:  0.42010706663131714
train gradient:  0.45778856086003317
iteration : 3611
train acc:  0.7890625
train loss:  0.3970438838005066
train gradient:  0.40811198903472906
iteration : 3612
train acc:  0.8046875
train loss:  0.38846027851104736
train gradient:  0.31116876956626344
iteration : 3613
train acc:  0.8828125
train loss:  0.3328935205936432
train gradient:  0.19061824697753993
iteration : 3614
train acc:  0.78125
train loss:  0.4172186553478241
train gradient:  0.3330864712175147
iteration : 3615
train acc:  0.8984375
train loss:  0.2963351607322693
train gradient:  0.36234178432962477
iteration : 3616
train acc:  0.8203125
train loss:  0.3660825788974762
train gradient:  0.3355694782230261
iteration : 3617
train acc:  0.8515625
train loss:  0.3781731128692627
train gradient:  0.4377862821436402
iteration : 3618
train acc:  0.8203125
train loss:  0.3286812901496887
train gradient:  0.2461721362240593
iteration : 3619
train acc:  0.8515625
train loss:  0.33761724829673767
train gradient:  0.1852166814931816
iteration : 3620
train acc:  0.875
train loss:  0.334960401058197
train gradient:  0.2580378804814632
iteration : 3621
train acc:  0.796875
train loss:  0.4171944260597229
train gradient:  0.28231835771858604
iteration : 3622
train acc:  0.9140625
train loss:  0.2547202408313751
train gradient:  0.16144884818228541
iteration : 3623
train acc:  0.8203125
train loss:  0.3800060749053955
train gradient:  0.3961345847325745
iteration : 3624
train acc:  0.8046875
train loss:  0.41675594449043274
train gradient:  0.3069677377030578
iteration : 3625
train acc:  0.859375
train loss:  0.31100359559059143
train gradient:  0.24045779863378586
iteration : 3626
train acc:  0.75
train loss:  0.5382975935935974
train gradient:  0.5876106262347429
iteration : 3627
train acc:  0.78125
train loss:  0.43149685859680176
train gradient:  0.30520509171455007
iteration : 3628
train acc:  0.84375
train loss:  0.322130024433136
train gradient:  0.23024946042602018
iteration : 3629
train acc:  0.8125
train loss:  0.35724902153015137
train gradient:  0.3562432014892844
iteration : 3630
train acc:  0.8125
train loss:  0.45012277364730835
train gradient:  0.343720795564014
iteration : 3631
train acc:  0.9140625
train loss:  0.27002814412117004
train gradient:  0.18476482647322004
iteration : 3632
train acc:  0.8359375
train loss:  0.3628728687763214
train gradient:  0.21268826237663135
iteration : 3633
train acc:  0.8671875
train loss:  0.35473260283470154
train gradient:  0.25473223160296576
iteration : 3634
train acc:  0.828125
train loss:  0.39670807123184204
train gradient:  0.3880939062691736
iteration : 3635
train acc:  0.8671875
train loss:  0.3390354514122009
train gradient:  0.3804883851156042
iteration : 3636
train acc:  0.84375
train loss:  0.32739466428756714
train gradient:  0.23162904386749544
iteration : 3637
train acc:  0.8046875
train loss:  0.407340943813324
train gradient:  0.3108115568078715
iteration : 3638
train acc:  0.8671875
train loss:  0.3225862383842468
train gradient:  0.20509941750645966
iteration : 3639
train acc:  0.84375
train loss:  0.3948533535003662
train gradient:  0.2481913955626712
iteration : 3640
train acc:  0.8125
train loss:  0.3851318061351776
train gradient:  0.23359349255146786
iteration : 3641
train acc:  0.8125
train loss:  0.35767990350723267
train gradient:  0.4244644025153051
iteration : 3642
train acc:  0.8359375
train loss:  0.35220345854759216
train gradient:  0.435602974195847
iteration : 3643
train acc:  0.7734375
train loss:  0.4377245306968689
train gradient:  0.40817667314023726
iteration : 3644
train acc:  0.875
train loss:  0.3014838695526123
train gradient:  0.25103912971592557
iteration : 3645
train acc:  0.8671875
train loss:  0.36751818656921387
train gradient:  0.32742186338517876
iteration : 3646
train acc:  0.875
train loss:  0.32011711597442627
train gradient:  0.20524326599827136
iteration : 3647
train acc:  0.828125
train loss:  0.3559417128562927
train gradient:  0.2586475320873048
iteration : 3648
train acc:  0.84375
train loss:  0.35950857400894165
train gradient:  0.3825051811642158
iteration : 3649
train acc:  0.8203125
train loss:  0.4418637454509735
train gradient:  0.40079016194425815
iteration : 3650
train acc:  0.8046875
train loss:  0.40802454948425293
train gradient:  0.47671461785993974
iteration : 3651
train acc:  0.8125
train loss:  0.4769299030303955
train gradient:  0.5427083763073458
iteration : 3652
train acc:  0.8671875
train loss:  0.2962116599082947
train gradient:  0.22150384802208345
iteration : 3653
train acc:  0.875
train loss:  0.2951083481311798
train gradient:  0.3616278182715608
iteration : 3654
train acc:  0.8203125
train loss:  0.4067201614379883
train gradient:  0.39229907480934784
iteration : 3655
train acc:  0.8984375
train loss:  0.2884594798088074
train gradient:  0.2883331579908611
iteration : 3656
train acc:  0.8046875
train loss:  0.3973543643951416
train gradient:  0.3325079896929713
iteration : 3657
train acc:  0.8359375
train loss:  0.42225220799446106
train gradient:  0.38076671191402434
iteration : 3658
train acc:  0.8203125
train loss:  0.35540205240249634
train gradient:  0.28368231134296873
iteration : 3659
train acc:  0.8203125
train loss:  0.4561656415462494
train gradient:  0.3477103401915658
iteration : 3660
train acc:  0.8671875
train loss:  0.30912935733795166
train gradient:  0.2746630210506474
iteration : 3661
train acc:  0.8515625
train loss:  0.3276541233062744
train gradient:  0.2336004727652622
iteration : 3662
train acc:  0.8203125
train loss:  0.3813185393810272
train gradient:  0.3709903136159295
iteration : 3663
train acc:  0.8515625
train loss:  0.3582594394683838
train gradient:  0.37349233141019655
iteration : 3664
train acc:  0.78125
train loss:  0.385036438703537
train gradient:  0.3248553300917506
iteration : 3665
train acc:  0.8359375
train loss:  0.3561961054801941
train gradient:  0.23992574422537644
iteration : 3666
train acc:  0.8125
train loss:  0.44704413414001465
train gradient:  0.399772678437121
iteration : 3667
train acc:  0.828125
train loss:  0.398104190826416
train gradient:  0.3385917345776231
iteration : 3668
train acc:  0.8359375
train loss:  0.42823636531829834
train gradient:  0.34800871449905973
iteration : 3669
train acc:  0.828125
train loss:  0.35602182149887085
train gradient:  0.23501098566207246
iteration : 3670
train acc:  0.8359375
train loss:  0.4234480857849121
train gradient:  0.34206092504570607
iteration : 3671
train acc:  0.828125
train loss:  0.34976160526275635
train gradient:  0.29955488320029705
iteration : 3672
train acc:  0.828125
train loss:  0.3732358515262604
train gradient:  0.3629812409901451
iteration : 3673
train acc:  0.8515625
train loss:  0.3352311849594116
train gradient:  0.2412403543203964
iteration : 3674
train acc:  0.8828125
train loss:  0.3695727288722992
train gradient:  0.3163065913090573
iteration : 3675
train acc:  0.84375
train loss:  0.3245158791542053
train gradient:  0.261224511345075
iteration : 3676
train acc:  0.7890625
train loss:  0.43886587023735046
train gradient:  0.4287028576831928
iteration : 3677
train acc:  0.8203125
train loss:  0.37991732358932495
train gradient:  0.34149112282956195
iteration : 3678
train acc:  0.8359375
train loss:  0.3946830630302429
train gradient:  0.3285661332864875
iteration : 3679
train acc:  0.8125
train loss:  0.44837498664855957
train gradient:  0.43681907330241293
iteration : 3680
train acc:  0.7890625
train loss:  0.4124408960342407
train gradient:  0.40379273717299935
iteration : 3681
train acc:  0.8125
train loss:  0.42105889320373535
train gradient:  0.5480991442060605
iteration : 3682
train acc:  0.828125
train loss:  0.362282931804657
train gradient:  0.35442052411137337
iteration : 3683
train acc:  0.8046875
train loss:  0.4199206829071045
train gradient:  0.30164793966492703
iteration : 3684
train acc:  0.8046875
train loss:  0.4033568501472473
train gradient:  0.33386516628304136
iteration : 3685
train acc:  0.8359375
train loss:  0.34449875354766846
train gradient:  0.27163247340130353
iteration : 3686
train acc:  0.84375
train loss:  0.3941379189491272
train gradient:  0.3162580558656262
iteration : 3687
train acc:  0.859375
train loss:  0.39368319511413574
train gradient:  0.289801177748191
iteration : 3688
train acc:  0.8515625
train loss:  0.3624998927116394
train gradient:  0.311110269133862
iteration : 3689
train acc:  0.8359375
train loss:  0.42115548253059387
train gradient:  0.43130157909631184
iteration : 3690
train acc:  0.8125
train loss:  0.4832761883735657
train gradient:  0.48896305176742577
iteration : 3691
train acc:  0.8515625
train loss:  0.3586272597312927
train gradient:  0.30210130457510254
iteration : 3692
train acc:  0.8515625
train loss:  0.3287963569164276
train gradient:  0.1922848748265958
iteration : 3693
train acc:  0.84375
train loss:  0.34489232301712036
train gradient:  0.18070746807341223
iteration : 3694
train acc:  0.875
train loss:  0.32221341133117676
train gradient:  0.23767782156851516
iteration : 3695
train acc:  0.84375
train loss:  0.36400461196899414
train gradient:  0.4782658172982866
iteration : 3696
train acc:  0.859375
train loss:  0.3730158507823944
train gradient:  0.2825971733053117
iteration : 3697
train acc:  0.8125
train loss:  0.3903319239616394
train gradient:  0.343651141065087
iteration : 3698
train acc:  0.8359375
train loss:  0.3608609437942505
train gradient:  0.2677376166534991
iteration : 3699
train acc:  0.8671875
train loss:  0.28698089718818665
train gradient:  0.1779696445868127
iteration : 3700
train acc:  0.8203125
train loss:  0.3773404359817505
train gradient:  0.28873671835715353
iteration : 3701
train acc:  0.890625
train loss:  0.3126890957355499
train gradient:  0.21087947455164313
iteration : 3702
train acc:  0.8359375
train loss:  0.3851690888404846
train gradient:  0.27947739456170617
iteration : 3703
train acc:  0.8828125
train loss:  0.2674810588359833
train gradient:  0.17535741296641727
iteration : 3704
train acc:  0.8203125
train loss:  0.36587435007095337
train gradient:  0.28789661829832686
iteration : 3705
train acc:  0.84375
train loss:  0.331964373588562
train gradient:  0.26269381635024824
iteration : 3706
train acc:  0.8203125
train loss:  0.40450966358184814
train gradient:  0.43544444652689135
iteration : 3707
train acc:  0.8515625
train loss:  0.29606395959854126
train gradient:  0.2135661460276197
iteration : 3708
train acc:  0.8359375
train loss:  0.3606952428817749
train gradient:  0.25347719290148973
iteration : 3709
train acc:  0.875
train loss:  0.3200967311859131
train gradient:  0.2896745602108134
iteration : 3710
train acc:  0.8828125
train loss:  0.30461248755455017
train gradient:  0.46066199744915043
iteration : 3711
train acc:  0.78125
train loss:  0.45833802223205566
train gradient:  0.33512281381024844
iteration : 3712
train acc:  0.8203125
train loss:  0.3419692814350128
train gradient:  0.25098446631337423
iteration : 3713
train acc:  0.8671875
train loss:  0.3336604833602905
train gradient:  0.2619040355547503
iteration : 3714
train acc:  0.7890625
train loss:  0.4939037263393402
train gradient:  0.520727145986478
iteration : 3715
train acc:  0.8046875
train loss:  0.431570827960968
train gradient:  0.5018110563694443
iteration : 3716
train acc:  0.84375
train loss:  0.35250043869018555
train gradient:  0.30350702930589163
iteration : 3717
train acc:  0.8046875
train loss:  0.38315263390541077
train gradient:  0.385757119053941
iteration : 3718
train acc:  0.7734375
train loss:  0.4631175994873047
train gradient:  0.5133395372884044
iteration : 3719
train acc:  0.8125
train loss:  0.36183106899261475
train gradient:  0.33723156484868905
iteration : 3720
train acc:  0.8125
train loss:  0.3653966784477234
train gradient:  0.2579773082442366
iteration : 3721
train acc:  0.8828125
train loss:  0.3726855516433716
train gradient:  0.3597385759927013
iteration : 3722
train acc:  0.8125
train loss:  0.38151562213897705
train gradient:  0.3422820310778779
iteration : 3723
train acc:  0.890625
train loss:  0.3297473192214966
train gradient:  0.26257956617665973
iteration : 3724
train acc:  0.875
train loss:  0.32166290283203125
train gradient:  0.26840771407596575
iteration : 3725
train acc:  0.84375
train loss:  0.3948013186454773
train gradient:  0.3215587486176063
iteration : 3726
train acc:  0.8125
train loss:  0.39628857374191284
train gradient:  0.3765635169248777
iteration : 3727
train acc:  0.78125
train loss:  0.42452672123908997
train gradient:  0.32754815475979987
iteration : 3728
train acc:  0.90625
train loss:  0.2663583755493164
train gradient:  0.21232084032893783
iteration : 3729
train acc:  0.828125
train loss:  0.4187387526035309
train gradient:  0.439740895611805
iteration : 3730
train acc:  0.859375
train loss:  0.3061477839946747
train gradient:  0.21546465999280232
iteration : 3731
train acc:  0.8828125
train loss:  0.31770652532577515
train gradient:  0.2354193593518928
iteration : 3732
train acc:  0.75
train loss:  0.5499923825263977
train gradient:  0.5190110534345971
iteration : 3733
train acc:  0.8515625
train loss:  0.3492244482040405
train gradient:  0.26271588556347636
iteration : 3734
train acc:  0.78125
train loss:  0.43847429752349854
train gradient:  0.4146139724054181
iteration : 3735
train acc:  0.875
train loss:  0.3218294382095337
train gradient:  0.2169568246315278
iteration : 3736
train acc:  0.859375
train loss:  0.33251795172691345
train gradient:  0.2532543194687015
iteration : 3737
train acc:  0.8203125
train loss:  0.3746613562107086
train gradient:  0.27600678679958557
iteration : 3738
train acc:  0.875
train loss:  0.3289160132408142
train gradient:  0.26111760653671806
iteration : 3739
train acc:  0.8203125
train loss:  0.3944369852542877
train gradient:  0.3333864443718687
iteration : 3740
train acc:  0.8515625
train loss:  0.3664991855621338
train gradient:  0.3488213222475283
iteration : 3741
train acc:  0.859375
train loss:  0.3690205216407776
train gradient:  0.35115713599754195
iteration : 3742
train acc:  0.8203125
train loss:  0.402997761964798
train gradient:  0.302478880524076
iteration : 3743
train acc:  0.8203125
train loss:  0.39423316717147827
train gradient:  0.3450359385126161
iteration : 3744
train acc:  0.859375
train loss:  0.2941197156906128
train gradient:  0.2579002921890393
iteration : 3745
train acc:  0.7578125
train loss:  0.43953365087509155
train gradient:  0.4413277095945879
iteration : 3746
train acc:  0.7734375
train loss:  0.40628811717033386
train gradient:  0.43876360849039386
iteration : 3747
train acc:  0.8671875
train loss:  0.3553771674633026
train gradient:  0.45050910252836834
iteration : 3748
train acc:  0.8359375
train loss:  0.3451157212257385
train gradient:  0.2073002213776205
iteration : 3749
train acc:  0.796875
train loss:  0.365683376789093
train gradient:  0.30599505531614757
iteration : 3750
train acc:  0.8125
train loss:  0.3751155436038971
train gradient:  0.2568831776013201
iteration : 3751
train acc:  0.796875
train loss:  0.41163522005081177
train gradient:  0.5084101609709109
iteration : 3752
train acc:  0.8515625
train loss:  0.3162996172904968
train gradient:  0.26233674021947834
iteration : 3753
train acc:  0.8203125
train loss:  0.3849482536315918
train gradient:  0.2386852320304059
iteration : 3754
train acc:  0.78125
train loss:  0.42438122630119324
train gradient:  0.38498655163809764
iteration : 3755
train acc:  0.8671875
train loss:  0.2898064851760864
train gradient:  0.22629679697407323
iteration : 3756
train acc:  0.84375
train loss:  0.36107736825942993
train gradient:  0.46258047123662943
iteration : 3757
train acc:  0.875
train loss:  0.3759956359863281
train gradient:  0.36498491023421564
iteration : 3758
train acc:  0.8046875
train loss:  0.3614007830619812
train gradient:  0.28695584183896955
iteration : 3759
train acc:  0.875
train loss:  0.3493454158306122
train gradient:  0.312083769745735
iteration : 3760
train acc:  0.828125
train loss:  0.36690670251846313
train gradient:  0.3637893501363056
iteration : 3761
train acc:  0.8828125
train loss:  0.33012792468070984
train gradient:  0.2394558822381534
iteration : 3762
train acc:  0.8359375
train loss:  0.34184107184410095
train gradient:  0.1939683930668371
iteration : 3763
train acc:  0.859375
train loss:  0.3236526846885681
train gradient:  0.28883202772802946
iteration : 3764
train acc:  0.8515625
train loss:  0.3381330370903015
train gradient:  0.22722615377071545
iteration : 3765
train acc:  0.8359375
train loss:  0.4080788195133209
train gradient:  0.33372164904827656
iteration : 3766
train acc:  0.8359375
train loss:  0.3992917835712433
train gradient:  0.39756871615803246
iteration : 3767
train acc:  0.78125
train loss:  0.46694210171699524
train gradient:  0.35461366963550517
iteration : 3768
train acc:  0.84375
train loss:  0.38480469584465027
train gradient:  0.2771673980382497
iteration : 3769
train acc:  0.8984375
train loss:  0.32072359323501587
train gradient:  0.18133396574035243
iteration : 3770
train acc:  0.8046875
train loss:  0.39877820014953613
train gradient:  0.3202738215020357
iteration : 3771
train acc:  0.84375
train loss:  0.42373156547546387
train gradient:  0.39698208376241095
iteration : 3772
train acc:  0.859375
train loss:  0.38444167375564575
train gradient:  0.43105814299332407
iteration : 3773
train acc:  0.796875
train loss:  0.509894609451294
train gradient:  0.48003454249306826
iteration : 3774
train acc:  0.8828125
train loss:  0.30476143956184387
train gradient:  0.18415717315869062
iteration : 3775
train acc:  0.8359375
train loss:  0.44152647256851196
train gradient:  0.3582635711744795
iteration : 3776
train acc:  0.8125
train loss:  0.39604687690734863
train gradient:  0.33481796154219445
iteration : 3777
train acc:  0.8515625
train loss:  0.31110697984695435
train gradient:  0.29740548419456003
iteration : 3778
train acc:  0.828125
train loss:  0.33992958068847656
train gradient:  0.3167868173134944
iteration : 3779
train acc:  0.859375
train loss:  0.38883018493652344
train gradient:  0.31429564098315793
iteration : 3780
train acc:  0.8671875
train loss:  0.34041786193847656
train gradient:  0.20623025043070414
iteration : 3781
train acc:  0.796875
train loss:  0.4587523937225342
train gradient:  0.5625118012768813
iteration : 3782
train acc:  0.796875
train loss:  0.4305644631385803
train gradient:  0.36103226453975956
iteration : 3783
train acc:  0.84375
train loss:  0.4136810004711151
train gradient:  0.5678129839970572
iteration : 3784
train acc:  0.8046875
train loss:  0.38785579800605774
train gradient:  0.42273148756861856
iteration : 3785
train acc:  0.796875
train loss:  0.3807041645050049
train gradient:  0.35677548307547113
iteration : 3786
train acc:  0.765625
train loss:  0.49439507722854614
train gradient:  0.48734190221893325
iteration : 3787
train acc:  0.7890625
train loss:  0.42564356327056885
train gradient:  0.4072200576501624
iteration : 3788
train acc:  0.859375
train loss:  0.31536242365837097
train gradient:  0.22745415720955686
iteration : 3789
train acc:  0.90625
train loss:  0.33445483446121216
train gradient:  0.20923465619973708
iteration : 3790
train acc:  0.8203125
train loss:  0.3811091184616089
train gradient:  0.38101199268721747
iteration : 3791
train acc:  0.859375
train loss:  0.3368910551071167
train gradient:  0.20726017482409675
iteration : 3792
train acc:  0.921875
train loss:  0.27004528045654297
train gradient:  0.17884548093889446
iteration : 3793
train acc:  0.84375
train loss:  0.3524567484855652
train gradient:  0.25288525381030896
iteration : 3794
train acc:  0.84375
train loss:  0.33291059732437134
train gradient:  0.2788172112498716
iteration : 3795
train acc:  0.8828125
train loss:  0.327204167842865
train gradient:  0.2671093963694604
iteration : 3796
train acc:  0.84375
train loss:  0.3552215099334717
train gradient:  0.33234081990791114
iteration : 3797
train acc:  0.78125
train loss:  0.39592641592025757
train gradient:  0.25011271884216274
iteration : 3798
train acc:  0.84375
train loss:  0.35887759923934937
train gradient:  0.29927438332711326
iteration : 3799
train acc:  0.8125
train loss:  0.37763315439224243
train gradient:  0.33824170502914375
iteration : 3800
train acc:  0.8125
train loss:  0.38185715675354004
train gradient:  0.33454948619219393
iteration : 3801
train acc:  0.8046875
train loss:  0.38047581911087036
train gradient:  0.301376497307022
iteration : 3802
train acc:  0.796875
train loss:  0.40962326526641846
train gradient:  0.32088865787569687
iteration : 3803
train acc:  0.8515625
train loss:  0.31032460927963257
train gradient:  0.20476617946826073
iteration : 3804
train acc:  0.8515625
train loss:  0.3416876196861267
train gradient:  0.28626255192679306
iteration : 3805
train acc:  0.84375
train loss:  0.3460478186607361
train gradient:  0.19983161394307047
iteration : 3806
train acc:  0.828125
train loss:  0.4165634214878082
train gradient:  0.2873536466645527
iteration : 3807
train acc:  0.8671875
train loss:  0.3341957628726959
train gradient:  0.20308227950442934
iteration : 3808
train acc:  0.765625
train loss:  0.40472695231437683
train gradient:  0.3043391363882421
iteration : 3809
train acc:  0.84375
train loss:  0.34285280108451843
train gradient:  0.2626094093162983
iteration : 3810
train acc:  0.8125
train loss:  0.4559117555618286
train gradient:  0.4946940491814472
iteration : 3811
train acc:  0.890625
train loss:  0.2617344856262207
train gradient:  0.18812664220397396
iteration : 3812
train acc:  0.90625
train loss:  0.274299681186676
train gradient:  0.17414528936476528
iteration : 3813
train acc:  0.921875
train loss:  0.26341259479522705
train gradient:  0.17056219278901685
iteration : 3814
train acc:  0.8671875
train loss:  0.31825998425483704
train gradient:  0.28125027640303674
iteration : 3815
train acc:  0.84375
train loss:  0.3816365897655487
train gradient:  0.4027168191576131
iteration : 3816
train acc:  0.90625
train loss:  0.29577434062957764
train gradient:  0.18947186313503417
iteration : 3817
train acc:  0.859375
train loss:  0.3048568665981293
train gradient:  0.2378704299752673
iteration : 3818
train acc:  0.859375
train loss:  0.4106326699256897
train gradient:  0.39580819552235996
iteration : 3819
train acc:  0.890625
train loss:  0.30590489506721497
train gradient:  0.1504802731206339
iteration : 3820
train acc:  0.8125
train loss:  0.37455958127975464
train gradient:  0.2865958273134864
iteration : 3821
train acc:  0.890625
train loss:  0.32664698362350464
train gradient:  0.29761091362250575
iteration : 3822
train acc:  0.875
train loss:  0.2929774820804596
train gradient:  0.2089657884267669
iteration : 3823
train acc:  0.84375
train loss:  0.35185909271240234
train gradient:  0.38063879009847484
iteration : 3824
train acc:  0.8359375
train loss:  0.35983389616012573
train gradient:  0.3179413017707946
iteration : 3825
train acc:  0.8984375
train loss:  0.2904417812824249
train gradient:  0.1937041159249081
iteration : 3826
train acc:  0.8046875
train loss:  0.41061267256736755
train gradient:  0.42661703196231493
iteration : 3827
train acc:  0.828125
train loss:  0.3626648783683777
train gradient:  0.3650209166893599
iteration : 3828
train acc:  0.859375
train loss:  0.3241172730922699
train gradient:  0.7131432632815483
iteration : 3829
train acc:  0.828125
train loss:  0.40273651480674744
train gradient:  0.453899156507867
iteration : 3830
train acc:  0.8203125
train loss:  0.36892908811569214
train gradient:  0.27183155113494334
iteration : 3831
train acc:  0.859375
train loss:  0.3045249581336975
train gradient:  0.3463213566710397
iteration : 3832
train acc:  0.796875
train loss:  0.3834255635738373
train gradient:  0.4660729508960941
iteration : 3833
train acc:  0.828125
train loss:  0.3431811034679413
train gradient:  0.284535629831834
iteration : 3834
train acc:  0.8515625
train loss:  0.3821602761745453
train gradient:  0.2996975639959315
iteration : 3835
train acc:  0.828125
train loss:  0.39320480823516846
train gradient:  0.45221895410118423
iteration : 3836
train acc:  0.875
train loss:  0.27825939655303955
train gradient:  0.19429237956562956
iteration : 3837
train acc:  0.84375
train loss:  0.340542733669281
train gradient:  0.311797428459508
iteration : 3838
train acc:  0.8671875
train loss:  0.3035891056060791
train gradient:  0.2893882744267533
iteration : 3839
train acc:  0.859375
train loss:  0.2997171878814697
train gradient:  0.26653577385430616
iteration : 3840
train acc:  0.859375
train loss:  0.33136850595474243
train gradient:  0.338845876615927
iteration : 3841
train acc:  0.796875
train loss:  0.3857804834842682
train gradient:  0.5115089741476783
iteration : 3842
train acc:  0.890625
train loss:  0.35949841141700745
train gradient:  0.3043046710904855
iteration : 3843
train acc:  0.828125
train loss:  0.44493985176086426
train gradient:  0.47189268022166186
iteration : 3844
train acc:  0.8203125
train loss:  0.4361168146133423
train gradient:  0.6754487084274262
iteration : 3845
train acc:  0.8515625
train loss:  0.3367137908935547
train gradient:  0.3174799395047068
iteration : 3846
train acc:  0.7890625
train loss:  0.46639367938041687
train gradient:  0.5560481498435765
iteration : 3847
train acc:  0.8046875
train loss:  0.4177882969379425
train gradient:  0.48251941416287325
iteration : 3848
train acc:  0.8671875
train loss:  0.33887767791748047
train gradient:  0.32332519681849625
iteration : 3849
train acc:  0.859375
train loss:  0.33395928144454956
train gradient:  0.2805266624119187
iteration : 3850
train acc:  0.890625
train loss:  0.2755747437477112
train gradient:  0.16539167894326537
iteration : 3851
train acc:  0.875
train loss:  0.2924986183643341
train gradient:  0.2898141547163661
iteration : 3852
train acc:  0.84375
train loss:  0.33644571900367737
train gradient:  0.3077771887966147
iteration : 3853
train acc:  0.8125
train loss:  0.41188085079193115
train gradient:  0.3575392339729813
iteration : 3854
train acc:  0.765625
train loss:  0.4017300009727478
train gradient:  0.7924969273191524
iteration : 3855
train acc:  0.84375
train loss:  0.3349614143371582
train gradient:  0.34601085133794296
iteration : 3856
train acc:  0.828125
train loss:  0.3299506604671478
train gradient:  0.31929393869570916
iteration : 3857
train acc:  0.8828125
train loss:  0.2893332839012146
train gradient:  0.2149331558875027
iteration : 3858
train acc:  0.8046875
train loss:  0.37765300273895264
train gradient:  0.40510936663377756
iteration : 3859
train acc:  0.859375
train loss:  0.2889406383037567
train gradient:  0.23846886213518287
iteration : 3860
train acc:  0.8203125
train loss:  0.36340826749801636
train gradient:  0.389923052137457
iteration : 3861
train acc:  0.84375
train loss:  0.32125124335289
train gradient:  0.4005565944261695
iteration : 3862
train acc:  0.8359375
train loss:  0.4365210235118866
train gradient:  0.4489605112089901
iteration : 3863
train acc:  0.8203125
train loss:  0.420378178358078
train gradient:  0.40396308997729446
iteration : 3864
train acc:  0.8125
train loss:  0.35788577795028687
train gradient:  0.3445272184472608
iteration : 3865
train acc:  0.828125
train loss:  0.400762140750885
train gradient:  0.4534749889676006
iteration : 3866
train acc:  0.8671875
train loss:  0.3251189887523651
train gradient:  0.20618659658615565
iteration : 3867
train acc:  0.8359375
train loss:  0.3468112051486969
train gradient:  0.22144219883261224
iteration : 3868
train acc:  0.7734375
train loss:  0.48866215348243713
train gradient:  0.3259722895841476
iteration : 3869
train acc:  0.8515625
train loss:  0.3630792200565338
train gradient:  0.3840434217591389
iteration : 3870
train acc:  0.8515625
train loss:  0.39164674282073975
train gradient:  0.2896783530809746
iteration : 3871
train acc:  0.859375
train loss:  0.3100459575653076
train gradient:  0.19207425992686863
iteration : 3872
train acc:  0.875
train loss:  0.3472244441509247
train gradient:  0.2693613035498876
iteration : 3873
train acc:  0.8203125
train loss:  0.40390288829803467
train gradient:  0.39056999307854695
iteration : 3874
train acc:  0.8671875
train loss:  0.36188074946403503
train gradient:  0.38515919683304656
iteration : 3875
train acc:  0.859375
train loss:  0.3971877992153168
train gradient:  0.7944216118133005
iteration : 3876
train acc:  0.8125
train loss:  0.39606574177742004
train gradient:  0.3944257506192328
iteration : 3877
train acc:  0.828125
train loss:  0.31718796491622925
train gradient:  0.2007621373433235
iteration : 3878
train acc:  0.828125
train loss:  0.4046364426612854
train gradient:  0.4016656248639166
iteration : 3879
train acc:  0.8046875
train loss:  0.424289345741272
train gradient:  0.4642712153930001
iteration : 3880
train acc:  0.859375
train loss:  0.32472294569015503
train gradient:  0.2724978198604316
iteration : 3881
train acc:  0.8671875
train loss:  0.30468153953552246
train gradient:  0.22741631700103848
iteration : 3882
train acc:  0.8125
train loss:  0.37595510482788086
train gradient:  0.31954559688486645
iteration : 3883
train acc:  0.84375
train loss:  0.3633718490600586
train gradient:  0.3221563875332092
iteration : 3884
train acc:  0.8203125
train loss:  0.37979644536972046
train gradient:  0.28441674777641063
iteration : 3885
train acc:  0.8203125
train loss:  0.3659697473049164
train gradient:  0.23676483427390765
iteration : 3886
train acc:  0.8671875
train loss:  0.31847083568573
train gradient:  0.2584082331200683
iteration : 3887
train acc:  0.8125
train loss:  0.3947822153568268
train gradient:  0.3584539475606814
iteration : 3888
train acc:  0.859375
train loss:  0.38784265518188477
train gradient:  0.32575897405298404
iteration : 3889
train acc:  0.90625
train loss:  0.28452008962631226
train gradient:  0.2855889520994185
iteration : 3890
train acc:  0.828125
train loss:  0.3896867036819458
train gradient:  0.2741717134566435
iteration : 3891
train acc:  0.8984375
train loss:  0.2645682096481323
train gradient:  0.18332986301237292
iteration : 3892
train acc:  0.8046875
train loss:  0.42358410358428955
train gradient:  0.6022625647133066
iteration : 3893
train acc:  0.8203125
train loss:  0.4041319489479065
train gradient:  0.32528152275329497
iteration : 3894
train acc:  0.75
train loss:  0.44421911239624023
train gradient:  0.37107397242601203
iteration : 3895
train acc:  0.828125
train loss:  0.40975886583328247
train gradient:  0.4591970725552464
iteration : 3896
train acc:  0.84375
train loss:  0.3695868253707886
train gradient:  0.3872408180365975
iteration : 3897
train acc:  0.8671875
train loss:  0.3285730183124542
train gradient:  0.32697069167754256
iteration : 3898
train acc:  0.828125
train loss:  0.35358792543411255
train gradient:  0.2755950423638675
iteration : 3899
train acc:  0.84375
train loss:  0.3221868872642517
train gradient:  0.3651118105309183
iteration : 3900
train acc:  0.90625
train loss:  0.27314668893814087
train gradient:  0.23799925315819181
iteration : 3901
train acc:  0.890625
train loss:  0.2811064124107361
train gradient:  0.28405775338227096
iteration : 3902
train acc:  0.8515625
train loss:  0.36148640513420105
train gradient:  0.31457993670839407
iteration : 3903
train acc:  0.875
train loss:  0.30109095573425293
train gradient:  0.21417700072025248
iteration : 3904
train acc:  0.8359375
train loss:  0.4236307144165039
train gradient:  0.7439511942756355
iteration : 3905
train acc:  0.875
train loss:  0.276220440864563
train gradient:  0.18347313730795298
iteration : 3906
train acc:  0.8359375
train loss:  0.32366806268692017
train gradient:  0.2228920553603817
iteration : 3907
train acc:  0.875
train loss:  0.29170453548431396
train gradient:  0.21675839491401658
iteration : 3908
train acc:  0.828125
train loss:  0.46147245168685913
train gradient:  0.49140776329766267
iteration : 3909
train acc:  0.8671875
train loss:  0.30627530813217163
train gradient:  0.2971150891708608
iteration : 3910
train acc:  0.828125
train loss:  0.35528889298439026
train gradient:  0.27846086494722766
iteration : 3911
train acc:  0.8203125
train loss:  0.38025036454200745
train gradient:  0.37951193864399047
iteration : 3912
train acc:  0.8671875
train loss:  0.32975590229034424
train gradient:  0.39358027583093774
iteration : 3913
train acc:  0.90625
train loss:  0.2649027109146118
train gradient:  0.2868596830349339
iteration : 3914
train acc:  0.8046875
train loss:  0.44223082065582275
train gradient:  0.3263856748157389
iteration : 3915
train acc:  0.8046875
train loss:  0.43752118945121765
train gradient:  0.39142460970082754
iteration : 3916
train acc:  0.84375
train loss:  0.3744453191757202
train gradient:  0.3479055535710281
iteration : 3917
train acc:  0.8359375
train loss:  0.3324219882488251
train gradient:  0.21252919704969975
iteration : 3918
train acc:  0.8671875
train loss:  0.3326532542705536
train gradient:  0.25701856974583787
iteration : 3919
train acc:  0.8046875
train loss:  0.3615694046020508
train gradient:  0.26177107715417824
iteration : 3920
train acc:  0.78125
train loss:  0.44038426876068115
train gradient:  0.43413979524770563
iteration : 3921
train acc:  0.8515625
train loss:  0.34647631645202637
train gradient:  0.33204618744691455
iteration : 3922
train acc:  0.8515625
train loss:  0.33506760001182556
train gradient:  0.2192730710049507
iteration : 3923
train acc:  0.8671875
train loss:  0.38267773389816284
train gradient:  0.29411580838225926
iteration : 3924
train acc:  0.796875
train loss:  0.4721153676509857
train gradient:  0.5567902051679816
iteration : 3925
train acc:  0.8515625
train loss:  0.4164511561393738
train gradient:  0.3570501377332184
iteration : 3926
train acc:  0.8203125
train loss:  0.3503321409225464
train gradient:  0.3428450215954742
iteration : 3927
train acc:  0.890625
train loss:  0.29125508666038513
train gradient:  0.3143174745223556
iteration : 3928
train acc:  0.859375
train loss:  0.33532047271728516
train gradient:  0.2918697202045091
iteration : 3929
train acc:  0.796875
train loss:  0.3714596629142761
train gradient:  0.3909775803948735
iteration : 3930
train acc:  0.8046875
train loss:  0.4916679859161377
train gradient:  0.744142873381577
iteration : 3931
train acc:  0.8515625
train loss:  0.3193209767341614
train gradient:  0.3015365975414315
iteration : 3932
train acc:  0.8359375
train loss:  0.3310014009475708
train gradient:  0.21262054322734236
iteration : 3933
train acc:  0.7578125
train loss:  0.5054538249969482
train gradient:  0.5819210857243721
iteration : 3934
train acc:  0.890625
train loss:  0.32589593529701233
train gradient:  0.26091898890114357
iteration : 3935
train acc:  0.8515625
train loss:  0.34457582235336304
train gradient:  0.31291845947114555
iteration : 3936
train acc:  0.8515625
train loss:  0.3086640238761902
train gradient:  0.3046833527726952
iteration : 3937
train acc:  0.8359375
train loss:  0.41823285818099976
train gradient:  0.3897035720893364
iteration : 3938
train acc:  0.828125
train loss:  0.3551722466945648
train gradient:  0.32014119832135934
iteration : 3939
train acc:  0.8046875
train loss:  0.3791475296020508
train gradient:  0.3483273841114796
iteration : 3940
train acc:  0.828125
train loss:  0.4151562750339508
train gradient:  0.34927883828087875
iteration : 3941
train acc:  0.84375
train loss:  0.3389384150505066
train gradient:  0.3027038274041201
iteration : 3942
train acc:  0.8046875
train loss:  0.3804463744163513
train gradient:  0.3618668367955754
iteration : 3943
train acc:  0.8203125
train loss:  0.35958170890808105
train gradient:  0.336025598652082
iteration : 3944
train acc:  0.84375
train loss:  0.3121659755706787
train gradient:  0.20608630182121535
iteration : 3945
train acc:  0.8828125
train loss:  0.3479974567890167
train gradient:  0.2702749177233484
iteration : 3946
train acc:  0.8203125
train loss:  0.3508000373840332
train gradient:  0.29994534411975426
iteration : 3947
train acc:  0.8359375
train loss:  0.4025995135307312
train gradient:  0.36898246888617
iteration : 3948
train acc:  0.8203125
train loss:  0.33200711011886597
train gradient:  0.23132476847855438
iteration : 3949
train acc:  0.8203125
train loss:  0.37346744537353516
train gradient:  0.22314758713661614
iteration : 3950
train acc:  0.875
train loss:  0.29337742924690247
train gradient:  0.2512428578198846
iteration : 3951
train acc:  0.78125
train loss:  0.48552048206329346
train gradient:  0.5204854193815945
iteration : 3952
train acc:  0.859375
train loss:  0.33382371068000793
train gradient:  0.3070566953097351
iteration : 3953
train acc:  0.84375
train loss:  0.33945727348327637
train gradient:  0.19389221676979151
iteration : 3954
train acc:  0.8828125
train loss:  0.3309696912765503
train gradient:  0.27272321165022834
iteration : 3955
train acc:  0.8046875
train loss:  0.4455533027648926
train gradient:  0.372678848557814
iteration : 3956
train acc:  0.8046875
train loss:  0.36764001846313477
train gradient:  0.31769219035583485
iteration : 3957
train acc:  0.8515625
train loss:  0.3983767330646515
train gradient:  0.3209705487707022
iteration : 3958
train acc:  0.7578125
train loss:  0.4808124601840973
train gradient:  0.5011283291528466
iteration : 3959
train acc:  0.8515625
train loss:  0.38138097524642944
train gradient:  0.280649343016817
iteration : 3960
train acc:  0.8515625
train loss:  0.32241323590278625
train gradient:  0.19597270391423155
iteration : 3961
train acc:  0.828125
train loss:  0.37424203753471375
train gradient:  0.30795764541868903
iteration : 3962
train acc:  0.7578125
train loss:  0.46264293789863586
train gradient:  0.42773727714778736
iteration : 3963
train acc:  0.8203125
train loss:  0.3502189517021179
train gradient:  0.29750482138998724
iteration : 3964
train acc:  0.8125
train loss:  0.41489410400390625
train gradient:  0.32352726609265015
iteration : 3965
train acc:  0.859375
train loss:  0.3101469874382019
train gradient:  0.2709486748732924
iteration : 3966
train acc:  0.828125
train loss:  0.4045565724372864
train gradient:  0.2866363682382144
iteration : 3967
train acc:  0.8671875
train loss:  0.3009484112262726
train gradient:  0.2006713101837301
iteration : 3968
train acc:  0.859375
train loss:  0.31048017740249634
train gradient:  0.22704330291673513
iteration : 3969
train acc:  0.8359375
train loss:  0.35129857063293457
train gradient:  0.3321303867053771
iteration : 3970
train acc:  0.859375
train loss:  0.3082405924797058
train gradient:  0.21211543140728206
iteration : 3971
train acc:  0.859375
train loss:  0.3652127981185913
train gradient:  0.2913247559744477
iteration : 3972
train acc:  0.8046875
train loss:  0.38346970081329346
train gradient:  0.30627109746139886
iteration : 3973
train acc:  0.8359375
train loss:  0.3489101231098175
train gradient:  0.18594619875067353
iteration : 3974
train acc:  0.8515625
train loss:  0.378035306930542
train gradient:  0.29441093760379977
iteration : 3975
train acc:  0.890625
train loss:  0.3261573910713196
train gradient:  0.22286184377276275
iteration : 3976
train acc:  0.8203125
train loss:  0.3387274444103241
train gradient:  0.4373312983759082
iteration : 3977
train acc:  0.8515625
train loss:  0.3510090112686157
train gradient:  0.34415582635534014
iteration : 3978
train acc:  0.875
train loss:  0.31108295917510986
train gradient:  0.1830818833552178
iteration : 3979
train acc:  0.8671875
train loss:  0.32536089420318604
train gradient:  0.23192949383227804
iteration : 3980
train acc:  0.8359375
train loss:  0.42125260829925537
train gradient:  0.31136121677262224
iteration : 3981
train acc:  0.828125
train loss:  0.3880142271518707
train gradient:  0.36452221488307013
iteration : 3982
train acc:  0.8203125
train loss:  0.36357638239860535
train gradient:  0.28007017086631286
iteration : 3983
train acc:  0.8359375
train loss:  0.36249828338623047
train gradient:  0.204883806452576
iteration : 3984
train acc:  0.796875
train loss:  0.3767290711402893
train gradient:  0.3831793213900511
iteration : 3985
train acc:  0.9140625
train loss:  0.28226181864738464
train gradient:  0.20425415249394419
iteration : 3986
train acc:  0.8125
train loss:  0.45975643396377563
train gradient:  0.40834666535899916
iteration : 3987
train acc:  0.796875
train loss:  0.5119184255599976
train gradient:  0.48041081527350593
iteration : 3988
train acc:  0.84375
train loss:  0.36483219265937805
train gradient:  0.27301495675381054
iteration : 3989
train acc:  0.828125
train loss:  0.34412890672683716
train gradient:  0.2979138040828143
iteration : 3990
train acc:  0.8828125
train loss:  0.3049147129058838
train gradient:  0.21783212932283508
iteration : 3991
train acc:  0.8046875
train loss:  0.37154334783554077
train gradient:  0.2743223335621774
iteration : 3992
train acc:  0.828125
train loss:  0.3403208255767822
train gradient:  0.3271510703343013
iteration : 3993
train acc:  0.8125
train loss:  0.3419199585914612
train gradient:  0.19716065545435624
iteration : 3994
train acc:  0.8671875
train loss:  0.2897513210773468
train gradient:  0.24514080396849425
iteration : 3995
train acc:  0.8671875
train loss:  0.32481440901756287
train gradient:  0.22208469265632808
iteration : 3996
train acc:  0.828125
train loss:  0.4709576368331909
train gradient:  0.5639031880658012
iteration : 3997
train acc:  0.859375
train loss:  0.3744441866874695
train gradient:  0.2606508906848106
iteration : 3998
train acc:  0.8359375
train loss:  0.35659998655319214
train gradient:  0.292655944926192
iteration : 3999
train acc:  0.84375
train loss:  0.38246315717697144
train gradient:  0.27983808324720966
iteration : 4000
train acc:  0.875
train loss:  0.3138720393180847
train gradient:  0.21761796335140737
iteration : 4001
train acc:  0.8359375
train loss:  0.37600237131118774
train gradient:  0.4045816246755002
iteration : 4002
train acc:  0.859375
train loss:  0.36548203229904175
train gradient:  0.22613058566784197
iteration : 4003
train acc:  0.859375
train loss:  0.3286101222038269
train gradient:  0.27742214597564285
iteration : 4004
train acc:  0.84375
train loss:  0.3828499913215637
train gradient:  0.29411438461093464
iteration : 4005
train acc:  0.828125
train loss:  0.31948649883270264
train gradient:  0.20990724349577486
iteration : 4006
train acc:  0.84375
train loss:  0.3172025680541992
train gradient:  0.3031507589793825
iteration : 4007
train acc:  0.8359375
train loss:  0.36061322689056396
train gradient:  0.39106580742950353
iteration : 4008
train acc:  0.8359375
train loss:  0.3865024149417877
train gradient:  0.3240550295965296
iteration : 4009
train acc:  0.8203125
train loss:  0.37988966703414917
train gradient:  0.3138826021184556
iteration : 4010
train acc:  0.8125
train loss:  0.5010144114494324
train gradient:  0.3948655520759268
iteration : 4011
train acc:  0.8828125
train loss:  0.2865934371948242
train gradient:  0.3072877638005128
iteration : 4012
train acc:  0.78125
train loss:  0.41846024990081787
train gradient:  0.4270896328994034
iteration : 4013
train acc:  0.84375
train loss:  0.29651808738708496
train gradient:  0.17772409038781173
iteration : 4014
train acc:  0.890625
train loss:  0.2313244640827179
train gradient:  0.18053880156312585
iteration : 4015
train acc:  0.828125
train loss:  0.3540233373641968
train gradient:  0.3154399760861158
iteration : 4016
train acc:  0.8515625
train loss:  0.33395135402679443
train gradient:  0.2069316623540055
iteration : 4017
train acc:  0.8125
train loss:  0.368253231048584
train gradient:  0.3247296925512938
iteration : 4018
train acc:  0.8828125
train loss:  0.3020249307155609
train gradient:  0.27920362686028893
iteration : 4019
train acc:  0.8359375
train loss:  0.3326551616191864
train gradient:  0.23987254987538717
iteration : 4020
train acc:  0.8125
train loss:  0.3437601327896118
train gradient:  0.29804378743480636
iteration : 4021
train acc:  0.8359375
train loss:  0.3892255425453186
train gradient:  0.39797189443639824
iteration : 4022
train acc:  0.890625
train loss:  0.29675033688545227
train gradient:  0.19942995160414687
iteration : 4023
train acc:  0.8046875
train loss:  0.4212697446346283
train gradient:  0.3403889120877382
iteration : 4024
train acc:  0.84375
train loss:  0.3491443395614624
train gradient:  0.2858325330074208
iteration : 4025
train acc:  0.8359375
train loss:  0.405917763710022
train gradient:  0.3838968582947892
iteration : 4026
train acc:  0.84375
train loss:  0.37025755643844604
train gradient:  0.37031193332799134
iteration : 4027
train acc:  0.84375
train loss:  0.33418649435043335
train gradient:  0.23395809232628106
iteration : 4028
train acc:  0.828125
train loss:  0.30599653720855713
train gradient:  0.23037629388538267
iteration : 4029
train acc:  0.828125
train loss:  0.3533851206302643
train gradient:  0.2872958754819627
iteration : 4030
train acc:  0.8046875
train loss:  0.4078525900840759
train gradient:  0.2950787882583102
iteration : 4031
train acc:  0.8203125
train loss:  0.3604341745376587
train gradient:  0.3633981054688005
iteration : 4032
train acc:  0.8203125
train loss:  0.3928825259208679
train gradient:  0.2940698690335055
iteration : 4033
train acc:  0.828125
train loss:  0.38566315174102783
train gradient:  0.32497438219896946
iteration : 4034
train acc:  0.859375
train loss:  0.3254658579826355
train gradient:  0.3806370952693778
iteration : 4035
train acc:  0.8359375
train loss:  0.393463671207428
train gradient:  0.43868146310915374
iteration : 4036
train acc:  0.8515625
train loss:  0.3539271354675293
train gradient:  0.3565200206392321
iteration : 4037
train acc:  0.8828125
train loss:  0.3322235345840454
train gradient:  0.32288988584333844
iteration : 4038
train acc:  0.765625
train loss:  0.4400641918182373
train gradient:  0.40654085028823994
iteration : 4039
train acc:  0.8671875
train loss:  0.31226277351379395
train gradient:  0.3666382552912062
iteration : 4040
train acc:  0.8125
train loss:  0.3868323266506195
train gradient:  0.35726316180125955
iteration : 4041
train acc:  0.890625
train loss:  0.3007300794124603
train gradient:  0.2931487847034543
iteration : 4042
train acc:  0.859375
train loss:  0.3171650767326355
train gradient:  0.31947571646971434
iteration : 4043
train acc:  0.8125
train loss:  0.38543501496315
train gradient:  0.4809001604686753
iteration : 4044
train acc:  0.8203125
train loss:  0.3731357157230377
train gradient:  0.3675545199139411
iteration : 4045
train acc:  0.8359375
train loss:  0.34696993231773376
train gradient:  0.3616985383667248
iteration : 4046
train acc:  0.8359375
train loss:  0.38797062635421753
train gradient:  0.3354347476503826
iteration : 4047
train acc:  0.8203125
train loss:  0.3847016394138336
train gradient:  0.268755123879777
iteration : 4048
train acc:  0.7890625
train loss:  0.48149412870407104
train gradient:  0.4616152226032129
iteration : 4049
train acc:  0.8671875
train loss:  0.3486160933971405
train gradient:  0.37455488832723566
iteration : 4050
train acc:  0.8203125
train loss:  0.38470086455345154
train gradient:  0.33104062073200075
iteration : 4051
train acc:  0.796875
train loss:  0.42882055044174194
train gradient:  0.34200364444554493
iteration : 4052
train acc:  0.84375
train loss:  0.3616201877593994
train gradient:  0.3181430298625619
iteration : 4053
train acc:  0.828125
train loss:  0.3618772029876709
train gradient:  0.36578235505790185
iteration : 4054
train acc:  0.8359375
train loss:  0.378750205039978
train gradient:  0.29766929208185783
iteration : 4055
train acc:  0.8671875
train loss:  0.3165263235569
train gradient:  0.2746696043712303
iteration : 4056
train acc:  0.84375
train loss:  0.3414127230644226
train gradient:  0.24036408718831886
iteration : 4057
train acc:  0.890625
train loss:  0.30452239513397217
train gradient:  0.2989906582135649
iteration : 4058
train acc:  0.859375
train loss:  0.32556262612342834
train gradient:  0.2946548763145408
iteration : 4059
train acc:  0.8828125
train loss:  0.30834588408470154
train gradient:  0.25389245754328593
iteration : 4060
train acc:  0.8046875
train loss:  0.46475279331207275
train gradient:  0.6903437643197212
iteration : 4061
train acc:  0.8359375
train loss:  0.35785627365112305
train gradient:  0.4337602904260686
iteration : 4062
train acc:  0.8125
train loss:  0.38039499521255493
train gradient:  0.32734981670465785
iteration : 4063
train acc:  0.8359375
train loss:  0.398343563079834
train gradient:  0.4270608551555405
iteration : 4064
train acc:  0.8515625
train loss:  0.3442445397377014
train gradient:  0.2774318887864856
iteration : 4065
train acc:  0.8359375
train loss:  0.3280174136161804
train gradient:  0.29132829202687166
iteration : 4066
train acc:  0.84375
train loss:  0.30254876613616943
train gradient:  0.284438624002634
iteration : 4067
train acc:  0.8203125
train loss:  0.3919881284236908
train gradient:  0.35532067541043866
iteration : 4068
train acc:  0.8359375
train loss:  0.3845009505748749
train gradient:  0.3461865424199899
iteration : 4069
train acc:  0.8828125
train loss:  0.2793816030025482
train gradient:  0.17925897446887396
iteration : 4070
train acc:  0.921875
train loss:  0.29976820945739746
train gradient:  0.29506961653560493
iteration : 4071
train acc:  0.890625
train loss:  0.3287016749382019
train gradient:  0.20576937700916392
iteration : 4072
train acc:  0.8359375
train loss:  0.3793103098869324
train gradient:  0.3687427176224136
iteration : 4073
train acc:  0.8203125
train loss:  0.432683527469635
train gradient:  0.4365559805545277
iteration : 4074
train acc:  0.828125
train loss:  0.36107689142227173
train gradient:  0.25462135658238527
iteration : 4075
train acc:  0.875
train loss:  0.3178655207157135
train gradient:  0.3133825854801488
iteration : 4076
train acc:  0.859375
train loss:  0.3286868929862976
train gradient:  0.27722908272837044
iteration : 4077
train acc:  0.8125
train loss:  0.40526309609413147
train gradient:  0.35108286058879234
iteration : 4078
train acc:  0.84375
train loss:  0.3631700277328491
train gradient:  0.24719186607957827
iteration : 4079
train acc:  0.8515625
train loss:  0.3645443320274353
train gradient:  0.37458273551317056
iteration : 4080
train acc:  0.78125
train loss:  0.44772130250930786
train gradient:  0.3525803576084866
iteration : 4081
train acc:  0.8125
train loss:  0.3556016683578491
train gradient:  0.39993155727513874
iteration : 4082
train acc:  0.9296875
train loss:  0.24392765760421753
train gradient:  0.15421464866288165
iteration : 4083
train acc:  0.859375
train loss:  0.3202027678489685
train gradient:  0.25312903044699603
iteration : 4084
train acc:  0.890625
train loss:  0.29724377393722534
train gradient:  0.1920486511442838
iteration : 4085
train acc:  0.828125
train loss:  0.41747915744781494
train gradient:  0.34103524243550587
iteration : 4086
train acc:  0.8046875
train loss:  0.43760499358177185
train gradient:  0.6040847982569053
iteration : 4087
train acc:  0.859375
train loss:  0.3469719886779785
train gradient:  0.2646861749785161
iteration : 4088
train acc:  0.8671875
train loss:  0.31123456358909607
train gradient:  0.292373649379952
iteration : 4089
train acc:  0.84375
train loss:  0.3402021527290344
train gradient:  0.2313455482457477
iteration : 4090
train acc:  0.8984375
train loss:  0.2678413391113281
train gradient:  0.20685228582803833
iteration : 4091
train acc:  0.8828125
train loss:  0.31079384684562683
train gradient:  0.2600890173048057
iteration : 4092
train acc:  0.8125
train loss:  0.4484688341617584
train gradient:  0.3841598099508034
iteration : 4093
train acc:  0.84375
train loss:  0.43976008892059326
train gradient:  0.48003908045208743
iteration : 4094
train acc:  0.8203125
train loss:  0.3918788433074951
train gradient:  0.2831492236446859
iteration : 4095
train acc:  0.8359375
train loss:  0.2954431474208832
train gradient:  0.23839176074442117
iteration : 4096
train acc:  0.8984375
train loss:  0.3070377707481384
train gradient:  0.15388490013289058
iteration : 4097
train acc:  0.875
train loss:  0.293331116437912
train gradient:  0.2526618811069116
iteration : 4098
train acc:  0.859375
train loss:  0.3139987289905548
train gradient:  0.24938476843091756
iteration : 4099
train acc:  0.859375
train loss:  0.30993056297302246
train gradient:  0.19861511704453572
iteration : 4100
train acc:  0.84375
train loss:  0.35838884115219116
train gradient:  0.2601665825944225
iteration : 4101
train acc:  0.84375
train loss:  0.3618612289428711
train gradient:  0.2335309633540673
iteration : 4102
train acc:  0.8359375
train loss:  0.31328868865966797
train gradient:  0.2539131679962612
iteration : 4103
train acc:  0.7734375
train loss:  0.5153972506523132
train gradient:  0.44590149098409976
iteration : 4104
train acc:  0.8515625
train loss:  0.3545041084289551
train gradient:  0.2536593280885828
iteration : 4105
train acc:  0.8046875
train loss:  0.3936391770839691
train gradient:  0.35784365304382854
iteration : 4106
train acc:  0.859375
train loss:  0.3701709508895874
train gradient:  0.3561773837554421
iteration : 4107
train acc:  0.796875
train loss:  0.35550457239151
train gradient:  0.3538414139362846
iteration : 4108
train acc:  0.84375
train loss:  0.36076977849006653
train gradient:  0.4191780584952922
iteration : 4109
train acc:  0.828125
train loss:  0.39205294847488403
train gradient:  0.38435786486018836
iteration : 4110
train acc:  0.875
train loss:  0.35090821981430054
train gradient:  0.21270038079273312
iteration : 4111
train acc:  0.84375
train loss:  0.39028236269950867
train gradient:  0.3493406283172392
iteration : 4112
train acc:  0.8515625
train loss:  0.3514261245727539
train gradient:  0.23851677786244024
iteration : 4113
train acc:  0.8984375
train loss:  0.2884342670440674
train gradient:  0.18158470216830158
iteration : 4114
train acc:  0.828125
train loss:  0.36282598972320557
train gradient:  0.2576962032729779
iteration : 4115
train acc:  0.8359375
train loss:  0.37876975536346436
train gradient:  0.2704677151131179
iteration : 4116
train acc:  0.8515625
train loss:  0.4612944722175598
train gradient:  0.3741349088054202
iteration : 4117
train acc:  0.8359375
train loss:  0.3808361887931824
train gradient:  0.2913769726464938
iteration : 4118
train acc:  0.859375
train loss:  0.2579469680786133
train gradient:  0.1567160529654431
iteration : 4119
train acc:  0.8359375
train loss:  0.34764188528060913
train gradient:  0.23603877285445937
iteration : 4120
train acc:  0.84375
train loss:  0.4107320308685303
train gradient:  0.36046748178883437
iteration : 4121
train acc:  0.90625
train loss:  0.2559155523777008
train gradient:  0.18658895555877283
iteration : 4122
train acc:  0.875
train loss:  0.31097230315208435
train gradient:  0.22940497899861603
iteration : 4123
train acc:  0.828125
train loss:  0.3537817597389221
train gradient:  0.256711380735821
iteration : 4124
train acc:  0.859375
train loss:  0.35174429416656494
train gradient:  0.3238148019637659
iteration : 4125
train acc:  0.8828125
train loss:  0.2733333706855774
train gradient:  0.15196369847366667
iteration : 4126
train acc:  0.8046875
train loss:  0.40230613946914673
train gradient:  0.26300595653472225
iteration : 4127
train acc:  0.84375
train loss:  0.3559401035308838
train gradient:  0.23103269637449306
iteration : 4128
train acc:  0.84375
train loss:  0.3944459557533264
train gradient:  0.2619011926775765
iteration : 4129
train acc:  0.8203125
train loss:  0.3303612470626831
train gradient:  0.276484393905246
iteration : 4130
train acc:  0.7890625
train loss:  0.44783371686935425
train gradient:  0.3643586606711322
iteration : 4131
train acc:  0.890625
train loss:  0.28148549795150757
train gradient:  0.18047500875497693
iteration : 4132
train acc:  0.8203125
train loss:  0.42522358894348145
train gradient:  0.28816141597796296
iteration : 4133
train acc:  0.8515625
train loss:  0.36482149362564087
train gradient:  0.23088171022938891
iteration : 4134
train acc:  0.8125
train loss:  0.42564788460731506
train gradient:  0.39956635576468685
iteration : 4135
train acc:  0.8359375
train loss:  0.38358694314956665
train gradient:  0.26185489465576084
iteration : 4136
train acc:  0.8046875
train loss:  0.44165077805519104
train gradient:  0.3153302024235347
iteration : 4137
train acc:  0.8671875
train loss:  0.33697813749313354
train gradient:  0.26192492842503023
iteration : 4138
train acc:  0.8203125
train loss:  0.44978100061416626
train gradient:  0.3436391214346697
iteration : 4139
train acc:  0.8359375
train loss:  0.3968854546546936
train gradient:  0.2399739132926728
iteration : 4140
train acc:  0.8125
train loss:  0.4019050598144531
train gradient:  0.3526952016925932
iteration : 4141
train acc:  0.8515625
train loss:  0.33785390853881836
train gradient:  0.24664087693419068
iteration : 4142
train acc:  0.859375
train loss:  0.3200235962867737
train gradient:  0.20594364446181024
iteration : 4143
train acc:  0.8515625
train loss:  0.32379257678985596
train gradient:  0.2773051723568758
iteration : 4144
train acc:  0.8359375
train loss:  0.3516988158226013
train gradient:  0.21429443500736217
iteration : 4145
train acc:  0.890625
train loss:  0.2854252755641937
train gradient:  0.16666716236262308
iteration : 4146
train acc:  0.84375
train loss:  0.36451300978660583
train gradient:  0.2653418670535475
iteration : 4147
train acc:  0.8359375
train loss:  0.324768990278244
train gradient:  0.33090209335792903
iteration : 4148
train acc:  0.84375
train loss:  0.3471693694591522
train gradient:  0.2491933753830429
iteration : 4149
train acc:  0.8671875
train loss:  0.29535961151123047
train gradient:  0.19765961013503627
iteration : 4150
train acc:  0.8671875
train loss:  0.39116692543029785
train gradient:  0.41449431732403524
iteration : 4151
train acc:  0.90625
train loss:  0.2652067244052887
train gradient:  0.14341979732975157
iteration : 4152
train acc:  0.8046875
train loss:  0.4107016623020172
train gradient:  0.5291213463948081
iteration : 4153
train acc:  0.8359375
train loss:  0.41214197874069214
train gradient:  0.2577878220408364
iteration : 4154
train acc:  0.8671875
train loss:  0.37210139632225037
train gradient:  0.2537151286057886
iteration : 4155
train acc:  0.8984375
train loss:  0.27673810720443726
train gradient:  0.16417944823942227
iteration : 4156
train acc:  0.859375
train loss:  0.322309672832489
train gradient:  0.2308621247904409
iteration : 4157
train acc:  0.8671875
train loss:  0.29692432284355164
train gradient:  0.3929960656983579
iteration : 4158
train acc:  0.8203125
train loss:  0.38704922795295715
train gradient:  0.36601283428878956
iteration : 4159
train acc:  0.8515625
train loss:  0.42992717027664185
train gradient:  0.3397393301475573
iteration : 4160
train acc:  0.796875
train loss:  0.4302213191986084
train gradient:  0.373284403537617
iteration : 4161
train acc:  0.8359375
train loss:  0.41092780232429504
train gradient:  0.3533086315787639
iteration : 4162
train acc:  0.859375
train loss:  0.32399117946624756
train gradient:  0.3070603459905832
iteration : 4163
train acc:  0.8671875
train loss:  0.318751722574234
train gradient:  0.1889394978252016
iteration : 4164
train acc:  0.796875
train loss:  0.4676656723022461
train gradient:  0.46559834518006094
iteration : 4165
train acc:  0.8359375
train loss:  0.40800580382347107
train gradient:  0.3241378404058167
iteration : 4166
train acc:  0.84375
train loss:  0.3704058825969696
train gradient:  0.2706765110143448
iteration : 4167
train acc:  0.8046875
train loss:  0.40011101961135864
train gradient:  0.5631023165982387
iteration : 4168
train acc:  0.875
train loss:  0.3156023919582367
train gradient:  0.2296278322346345
iteration : 4169
train acc:  0.84375
train loss:  0.3441735506057739
train gradient:  0.2621142808223105
iteration : 4170
train acc:  0.921875
train loss:  0.2555896043777466
train gradient:  0.16559427218008402
iteration : 4171
train acc:  0.8125
train loss:  0.39294546842575073
train gradient:  0.34908820800826695
iteration : 4172
train acc:  0.8671875
train loss:  0.33317577838897705
train gradient:  0.25722966246458967
iteration : 4173
train acc:  0.84375
train loss:  0.3239833116531372
train gradient:  0.37030171039857246
iteration : 4174
train acc:  0.796875
train loss:  0.46442025899887085
train gradient:  0.47987859848806064
iteration : 4175
train acc:  0.84375
train loss:  0.33892083168029785
train gradient:  0.2716866767940478
iteration : 4176
train acc:  0.84375
train loss:  0.4251343905925751
train gradient:  0.3438258786793446
iteration : 4177
train acc:  0.8671875
train loss:  0.3066355586051941
train gradient:  0.31756192975159714
iteration : 4178
train acc:  0.84375
train loss:  0.37154215574264526
train gradient:  0.3181328955846863
iteration : 4179
train acc:  0.8515625
train loss:  0.34241098165512085
train gradient:  0.2683182602289783
iteration : 4180
train acc:  0.8046875
train loss:  0.3741786479949951
train gradient:  0.26621272226544457
iteration : 4181
train acc:  0.84375
train loss:  0.3404824137687683
train gradient:  0.2839709542755019
iteration : 4182
train acc:  0.8125
train loss:  0.38063615560531616
train gradient:  0.25898154281648844
iteration : 4183
train acc:  0.796875
train loss:  0.457536518573761
train gradient:  0.3821042159503652
iteration : 4184
train acc:  0.84375
train loss:  0.3495640158653259
train gradient:  0.3549572955078489
iteration : 4185
train acc:  0.859375
train loss:  0.3313242793083191
train gradient:  0.25597243226230687
iteration : 4186
train acc:  0.8125
train loss:  0.39476561546325684
train gradient:  0.3765729757101186
iteration : 4187
train acc:  0.875
train loss:  0.3254038989543915
train gradient:  0.2635785399969888
iteration : 4188
train acc:  0.828125
train loss:  0.43166112899780273
train gradient:  0.2830261009044951
iteration : 4189
train acc:  0.9375
train loss:  0.2575021982192993
train gradient:  0.14737600725667788
iteration : 4190
train acc:  0.828125
train loss:  0.37856513261795044
train gradient:  0.346875547484587
iteration : 4191
train acc:  0.8203125
train loss:  0.3506200313568115
train gradient:  0.25402038267436194
iteration : 4192
train acc:  0.890625
train loss:  0.292724609375
train gradient:  0.2244206789226684
iteration : 4193
train acc:  0.796875
train loss:  0.3765198290348053
train gradient:  0.2886966884041908
iteration : 4194
train acc:  0.8515625
train loss:  0.3278186321258545
train gradient:  0.2914343408921277
iteration : 4195
train acc:  0.7890625
train loss:  0.3698718547821045
train gradient:  0.3049283338483047
iteration : 4196
train acc:  0.84375
train loss:  0.3136219382286072
train gradient:  0.26515658624032423
iteration : 4197
train acc:  0.8125
train loss:  0.3797960877418518
train gradient:  0.3290638801140645
iteration : 4198
train acc:  0.8046875
train loss:  0.40493154525756836
train gradient:  0.40993888085617786
iteration : 4199
train acc:  0.8125
train loss:  0.38630980253219604
train gradient:  0.3458075822715671
iteration : 4200
train acc:  0.8359375
train loss:  0.3172823190689087
train gradient:  0.23047827144388403
iteration : 4201
train acc:  0.859375
train loss:  0.3398379981517792
train gradient:  0.276231322807307
iteration : 4202
train acc:  0.828125
train loss:  0.3999062180519104
train gradient:  0.36478206663651697
iteration : 4203
train acc:  0.8359375
train loss:  0.3713935911655426
train gradient:  0.2695983294900629
iteration : 4204
train acc:  0.8671875
train loss:  0.3179208040237427
train gradient:  0.2521327959411955
iteration : 4205
train acc:  0.7890625
train loss:  0.4407424330711365
train gradient:  0.4080264148332173
iteration : 4206
train acc:  0.8046875
train loss:  0.4023039937019348
train gradient:  0.3154691591335746
iteration : 4207
train acc:  0.8359375
train loss:  0.3552817702293396
train gradient:  0.22056478312887154
iteration : 4208
train acc:  0.8515625
train loss:  0.35569751262664795
train gradient:  0.6228260686917815
iteration : 4209
train acc:  0.8671875
train loss:  0.28838035464286804
train gradient:  0.20430909975099265
iteration : 4210
train acc:  0.8828125
train loss:  0.3504180908203125
train gradient:  0.3465460238493193
iteration : 4211
train acc:  0.7890625
train loss:  0.4590771496295929
train gradient:  0.4272586051618794
iteration : 4212
train acc:  0.8671875
train loss:  0.3779076933860779
train gradient:  0.3052722704997247
iteration : 4213
train acc:  0.828125
train loss:  0.3688126802444458
train gradient:  0.3335362630784323
iteration : 4214
train acc:  0.8515625
train loss:  0.37022465467453003
train gradient:  0.2378521272766057
iteration : 4215
train acc:  0.8515625
train loss:  0.36124858260154724
train gradient:  0.2453620372646354
iteration : 4216
train acc:  0.828125
train loss:  0.35846054553985596
train gradient:  0.2723962312166814
iteration : 4217
train acc:  0.7734375
train loss:  0.43057048320770264
train gradient:  0.36171928005400245
iteration : 4218
train acc:  0.859375
train loss:  0.32927295565605164
train gradient:  0.22151552271417077
iteration : 4219
train acc:  0.84375
train loss:  0.32556939125061035
train gradient:  0.27309463580029647
iteration : 4220
train acc:  0.8359375
train loss:  0.3957045376300812
train gradient:  0.26358734806626455
iteration : 4221
train acc:  0.8828125
train loss:  0.2713589668273926
train gradient:  0.16345138354422384
iteration : 4222
train acc:  0.859375
train loss:  0.31335020065307617
train gradient:  0.26050581246716203
iteration : 4223
train acc:  0.8515625
train loss:  0.35459887981414795
train gradient:  0.19206982082449964
iteration : 4224
train acc:  0.84375
train loss:  0.33403968811035156
train gradient:  0.24929606104716523
iteration : 4225
train acc:  0.8359375
train loss:  0.352953165769577
train gradient:  0.20730728588798253
iteration : 4226
train acc:  0.796875
train loss:  0.39420199394226074
train gradient:  0.32080578527503717
iteration : 4227
train acc:  0.828125
train loss:  0.3672333359718323
train gradient:  0.2426361333989361
iteration : 4228
train acc:  0.8203125
train loss:  0.3539382815361023
train gradient:  0.32981100630622684
iteration : 4229
train acc:  0.828125
train loss:  0.4121882915496826
train gradient:  0.3242896275194478
iteration : 4230
train acc:  0.84375
train loss:  0.3463191092014313
train gradient:  0.24287707669225733
iteration : 4231
train acc:  0.8671875
train loss:  0.30547893047332764
train gradient:  0.2149709741218468
iteration : 4232
train acc:  0.8359375
train loss:  0.40462726354599
train gradient:  0.4075774167577872
iteration : 4233
train acc:  0.828125
train loss:  0.39448386430740356
train gradient:  0.4063885986679057
iteration : 4234
train acc:  0.8359375
train loss:  0.36808499693870544
train gradient:  0.30117403521239733
iteration : 4235
train acc:  0.9296875
train loss:  0.2877655327320099
train gradient:  0.23552662214352454
iteration : 4236
train acc:  0.8671875
train loss:  0.31845682859420776
train gradient:  0.19477820386520206
iteration : 4237
train acc:  0.8359375
train loss:  0.4156774878501892
train gradient:  0.313713119420357
iteration : 4238
train acc:  0.859375
train loss:  0.32307443022727966
train gradient:  0.18755809585579747
iteration : 4239
train acc:  0.8828125
train loss:  0.3311917781829834
train gradient:  0.19367094410007185
iteration : 4240
train acc:  0.828125
train loss:  0.3657921254634857
train gradient:  0.24835847170510517
iteration : 4241
train acc:  0.8828125
train loss:  0.2737733721733093
train gradient:  0.1767254560912434
iteration : 4242
train acc:  0.8203125
train loss:  0.38775986433029175
train gradient:  0.32470522999837576
iteration : 4243
train acc:  0.828125
train loss:  0.3533672094345093
train gradient:  0.24700338536405503
iteration : 4244
train acc:  0.8359375
train loss:  0.3601814806461334
train gradient:  0.2985641157647532
iteration : 4245
train acc:  0.859375
train loss:  0.4027579426765442
train gradient:  0.4104444933952949
iteration : 4246
train acc:  0.8515625
train loss:  0.36406099796295166
train gradient:  0.29030325748341546
iteration : 4247
train acc:  0.8046875
train loss:  0.38603782653808594
train gradient:  0.30163666025758984
iteration : 4248
train acc:  0.8046875
train loss:  0.35821133852005005
train gradient:  0.21171456885481776
iteration : 4249
train acc:  0.84375
train loss:  0.4030594825744629
train gradient:  0.31961664808131823
iteration : 4250
train acc:  0.7890625
train loss:  0.4563595652580261
train gradient:  0.32710608221138193
iteration : 4251
train acc:  0.78125
train loss:  0.45246291160583496
train gradient:  0.3729181488175129
iteration : 4252
train acc:  0.8515625
train loss:  0.3459140956401825
train gradient:  0.27495164344324846
iteration : 4253
train acc:  0.828125
train loss:  0.3680553436279297
train gradient:  0.2159637760200863
iteration : 4254
train acc:  0.890625
train loss:  0.24826942384243011
train gradient:  0.14700541317849902
iteration : 4255
train acc:  0.84375
train loss:  0.38193920254707336
train gradient:  0.2593314277959082
iteration : 4256
train acc:  0.796875
train loss:  0.3904971182346344
train gradient:  0.312710463840828
iteration : 4257
train acc:  0.8203125
train loss:  0.39187806844711304
train gradient:  0.2785688892528482
iteration : 4258
train acc:  0.828125
train loss:  0.3491182029247284
train gradient:  0.3252826922618543
iteration : 4259
train acc:  0.8671875
train loss:  0.2993517220020294
train gradient:  0.32877406327002656
iteration : 4260
train acc:  0.8515625
train loss:  0.34532058238983154
train gradient:  0.26156043398592616
iteration : 4261
train acc:  0.875
train loss:  0.3156418800354004
train gradient:  0.41730549176268156
iteration : 4262
train acc:  0.8203125
train loss:  0.4655815064907074
train gradient:  0.4587241063072045
iteration : 4263
train acc:  0.8515625
train loss:  0.40962088108062744
train gradient:  0.535475791696505
iteration : 4264
train acc:  0.7890625
train loss:  0.45822852849960327
train gradient:  0.4727505810219443
iteration : 4265
train acc:  0.859375
train loss:  0.3293091058731079
train gradient:  0.25546225304824627
iteration : 4266
train acc:  0.8203125
train loss:  0.3798856735229492
train gradient:  0.4147251943098855
iteration : 4267
train acc:  0.828125
train loss:  0.36909836530685425
train gradient:  0.3060287895932921
iteration : 4268
train acc:  0.828125
train loss:  0.36659640073776245
train gradient:  0.29123217826384035
iteration : 4269
train acc:  0.8828125
train loss:  0.3184153437614441
train gradient:  0.30314101485365147
iteration : 4270
train acc:  0.8671875
train loss:  0.3359915614128113
train gradient:  0.23360842462214879
iteration : 4271
train acc:  0.828125
train loss:  0.3314937949180603
train gradient:  0.2354702308503035
iteration : 4272
train acc:  0.8359375
train loss:  0.40489768981933594
train gradient:  0.3551838540655607
iteration : 4273
train acc:  0.8359375
train loss:  0.35765528678894043
train gradient:  0.2169887999789415
iteration : 4274
train acc:  0.765625
train loss:  0.49175137281417847
train gradient:  0.45690405687701674
iteration : 4275
train acc:  0.8125
train loss:  0.3877454102039337
train gradient:  0.24917017482055698
iteration : 4276
train acc:  0.8515625
train loss:  0.3424082398414612
train gradient:  0.21157647433906973
iteration : 4277
train acc:  0.84375
train loss:  0.3541143834590912
train gradient:  0.1958155309079334
iteration : 4278
train acc:  0.828125
train loss:  0.3815087378025055
train gradient:  0.28800135576517016
iteration : 4279
train acc:  0.828125
train loss:  0.44803404808044434
train gradient:  0.32588465703186625
iteration : 4280
train acc:  0.8203125
train loss:  0.32575851678848267
train gradient:  0.2610584849743422
iteration : 4281
train acc:  0.8671875
train loss:  0.3609776198863983
train gradient:  0.32407012129195967
iteration : 4282
train acc:  0.859375
train loss:  0.3582219183444977
train gradient:  0.18826125710403852
iteration : 4283
train acc:  0.84375
train loss:  0.3549898564815521
train gradient:  0.35348309992564353
iteration : 4284
train acc:  0.8984375
train loss:  0.25620779395103455
train gradient:  0.1493930621981005
iteration : 4285
train acc:  0.875
train loss:  0.336424857378006
train gradient:  0.32796426845447685
iteration : 4286
train acc:  0.8359375
train loss:  0.36840009689331055
train gradient:  0.3902715069679599
iteration : 4287
train acc:  0.828125
train loss:  0.41353392601013184
train gradient:  0.3362779514628358
iteration : 4288
train acc:  0.796875
train loss:  0.3860267400741577
train gradient:  0.23511514274979983
iteration : 4289
train acc:  0.8359375
train loss:  0.3496345281600952
train gradient:  0.31393596334492696
iteration : 4290
train acc:  0.8671875
train loss:  0.3326769471168518
train gradient:  0.251335263103734
iteration : 4291
train acc:  0.8515625
train loss:  0.3210279941558838
train gradient:  0.1833036686186686
iteration : 4292
train acc:  0.8359375
train loss:  0.35333526134490967
train gradient:  0.2768442576262151
iteration : 4293
train acc:  0.8515625
train loss:  0.3324986696243286
train gradient:  0.23251335917818647
iteration : 4294
train acc:  0.875
train loss:  0.3434102535247803
train gradient:  0.2694967785301788
iteration : 4295
train acc:  0.8125
train loss:  0.3535609841346741
train gradient:  0.27714167032575215
iteration : 4296
train acc:  0.8828125
train loss:  0.33167457580566406
train gradient:  0.42246663460624934
iteration : 4297
train acc:  0.8203125
train loss:  0.36070185899734497
train gradient:  0.20012712570979624
iteration : 4298
train acc:  0.8515625
train loss:  0.39008742570877075
train gradient:  0.28667852478042594
iteration : 4299
train acc:  0.859375
train loss:  0.3444652557373047
train gradient:  0.18731554479795456
iteration : 4300
train acc:  0.890625
train loss:  0.2899704575538635
train gradient:  0.22650404722628098
iteration : 4301
train acc:  0.828125
train loss:  0.3840511739253998
train gradient:  0.2975235816658934
iteration : 4302
train acc:  0.84375
train loss:  0.36899733543395996
train gradient:  0.3238554507142051
iteration : 4303
train acc:  0.859375
train loss:  0.315428763628006
train gradient:  0.16083254201761993
iteration : 4304
train acc:  0.859375
train loss:  0.31824782490730286
train gradient:  0.20400520464053812
iteration : 4305
train acc:  0.8125
train loss:  0.3218911290168762
train gradient:  0.2523698906704091
iteration : 4306
train acc:  0.7890625
train loss:  0.4002957344055176
train gradient:  0.3448844808455728
iteration : 4307
train acc:  0.796875
train loss:  0.38761842250823975
train gradient:  0.25527314697340775
iteration : 4308
train acc:  0.8203125
train loss:  0.3886603116989136
train gradient:  0.28238364982018743
iteration : 4309
train acc:  0.8828125
train loss:  0.3121057450771332
train gradient:  0.22644186387046533
iteration : 4310
train acc:  0.8203125
train loss:  0.38395458459854126
train gradient:  0.3654530905949707
iteration : 4311
train acc:  0.7890625
train loss:  0.3860752582550049
train gradient:  0.3096285088114305
iteration : 4312
train acc:  0.8671875
train loss:  0.34720271825790405
train gradient:  0.263085901833411
iteration : 4313
train acc:  0.890625
train loss:  0.2983929514884949
train gradient:  0.21647793444681074
iteration : 4314
train acc:  0.890625
train loss:  0.28756511211395264
train gradient:  0.20065737570790163
iteration : 4315
train acc:  0.859375
train loss:  0.3094518184661865
train gradient:  0.23438558230575793
iteration : 4316
train acc:  0.8359375
train loss:  0.36023837327957153
train gradient:  0.31101683681305775
iteration : 4317
train acc:  0.84375
train loss:  0.4356725215911865
train gradient:  0.5392385907646514
iteration : 4318
train acc:  0.75
train loss:  0.501038134098053
train gradient:  0.4407020800069691
iteration : 4319
train acc:  0.8125
train loss:  0.45552074909210205
train gradient:  0.6649188899566779
iteration : 4320
train acc:  0.84375
train loss:  0.36110830307006836
train gradient:  0.5725665514657464
iteration : 4321
train acc:  0.78125
train loss:  0.44519490003585815
train gradient:  0.47279515153001
iteration : 4322
train acc:  0.84375
train loss:  0.34374621510505676
train gradient:  0.5386398989825442
iteration : 4323
train acc:  0.84375
train loss:  0.33892932534217834
train gradient:  0.3008737911014153
iteration : 4324
train acc:  0.875
train loss:  0.2893236577510834
train gradient:  0.14393584970566148
iteration : 4325
train acc:  0.84375
train loss:  0.3978455662727356
train gradient:  0.3713219877239329
iteration : 4326
train acc:  0.9140625
train loss:  0.277729332447052
train gradient:  0.16804400538422354
iteration : 4327
train acc:  0.78125
train loss:  0.4182376265525818
train gradient:  0.38184024874771627
iteration : 4328
train acc:  0.8125
train loss:  0.40363526344299316
train gradient:  0.40781064030661834
iteration : 4329
train acc:  0.859375
train loss:  0.35983192920684814
train gradient:  0.2774307899046167
iteration : 4330
train acc:  0.8046875
train loss:  0.4082639515399933
train gradient:  0.3713295817731832
iteration : 4331
train acc:  0.8359375
train loss:  0.4158453941345215
train gradient:  0.23657844726180588
iteration : 4332
train acc:  0.8203125
train loss:  0.47606879472732544
train gradient:  0.5219581691787484
iteration : 4333
train acc:  0.890625
train loss:  0.31685885787010193
train gradient:  0.6320207142770045
iteration : 4334
train acc:  0.78125
train loss:  0.41799625754356384
train gradient:  0.3507841091894904
iteration : 4335
train acc:  0.8125
train loss:  0.3884766101837158
train gradient:  0.3662295377682853
iteration : 4336
train acc:  0.8515625
train loss:  0.33945536613464355
train gradient:  0.20562267389193498
iteration : 4337
train acc:  0.8203125
train loss:  0.44333022832870483
train gradient:  0.43116151315988077
iteration : 4338
train acc:  0.8359375
train loss:  0.3292478919029236
train gradient:  0.24799341608997139
iteration : 4339
train acc:  0.890625
train loss:  0.3011821210384369
train gradient:  0.2387546738595922
iteration : 4340
train acc:  0.8125
train loss:  0.38900718092918396
train gradient:  0.24697003439087498
iteration : 4341
train acc:  0.875
train loss:  0.316982626914978
train gradient:  0.23328383722129437
iteration : 4342
train acc:  0.8046875
train loss:  0.42397719621658325
train gradient:  0.32450184629808354
iteration : 4343
train acc:  0.84375
train loss:  0.4201795160770416
train gradient:  0.4044955230431029
iteration : 4344
train acc:  0.921875
train loss:  0.27729517221450806
train gradient:  0.27689164768567326
iteration : 4345
train acc:  0.8203125
train loss:  0.33159253001213074
train gradient:  0.2711068855748025
iteration : 4346
train acc:  0.796875
train loss:  0.4382774829864502
train gradient:  0.3621087794809818
iteration : 4347
train acc:  0.8203125
train loss:  0.3432386517524719
train gradient:  0.21947771627919718
iteration : 4348
train acc:  0.84375
train loss:  0.36291182041168213
train gradient:  0.23284580961355061
iteration : 4349
train acc:  0.84375
train loss:  0.35483765602111816
train gradient:  0.2482130779682765
iteration : 4350
train acc:  0.8203125
train loss:  0.37812042236328125
train gradient:  0.25890673808488723
iteration : 4351
train acc:  0.8515625
train loss:  0.3401763141155243
train gradient:  0.23918642675808238
iteration : 4352
train acc:  0.8359375
train loss:  0.3379436433315277
train gradient:  0.45481383723290975
iteration : 4353
train acc:  0.828125
train loss:  0.35971418023109436
train gradient:  0.26083895843361543
iteration : 4354
train acc:  0.8515625
train loss:  0.3006313741207123
train gradient:  0.26857375085827145
iteration : 4355
train acc:  0.78125
train loss:  0.4554431736469269
train gradient:  0.34155674183832785
iteration : 4356
train acc:  0.875
train loss:  0.35303980112075806
train gradient:  0.2402679375370214
iteration : 4357
train acc:  0.890625
train loss:  0.2855013608932495
train gradient:  0.18764738693072744
iteration : 4358
train acc:  0.8828125
train loss:  0.28536733984947205
train gradient:  0.2171250016123925
iteration : 4359
train acc:  0.8671875
train loss:  0.3330073356628418
train gradient:  0.273431472939348
iteration : 4360
train acc:  0.8125
train loss:  0.40245190262794495
train gradient:  0.29111204273081626
iteration : 4361
train acc:  0.8125
train loss:  0.3524261713027954
train gradient:  0.2661122701210244
iteration : 4362
train acc:  0.8125
train loss:  0.38441628217697144
train gradient:  0.23287667185181882
iteration : 4363
train acc:  0.8828125
train loss:  0.31127816438674927
train gradient:  0.3323477297491241
iteration : 4364
train acc:  0.8515625
train loss:  0.3486015498638153
train gradient:  0.25584005974454826
iteration : 4365
train acc:  0.828125
train loss:  0.4547848105430603
train gradient:  0.40579252326836596
iteration : 4366
train acc:  0.828125
train loss:  0.41706153750419617
train gradient:  0.2825904203134827
iteration : 4367
train acc:  0.828125
train loss:  0.3855469226837158
train gradient:  0.35476349346136743
iteration : 4368
train acc:  0.84375
train loss:  0.3405943512916565
train gradient:  0.26560403276211253
iteration : 4369
train acc:  0.8671875
train loss:  0.3800888657569885
train gradient:  0.3667928528067922
iteration : 4370
train acc:  0.78125
train loss:  0.4406931400299072
train gradient:  0.3221319221668107
iteration : 4371
train acc:  0.84375
train loss:  0.34174835681915283
train gradient:  0.2544051258836988
iteration : 4372
train acc:  0.875
train loss:  0.31334584951400757
train gradient:  0.2305685411580626
iteration : 4373
train acc:  0.859375
train loss:  0.3620811700820923
train gradient:  0.23684451146970825
iteration : 4374
train acc:  0.84375
train loss:  0.29864412546157837
train gradient:  0.16827800550863722
iteration : 4375
train acc:  0.8671875
train loss:  0.3391399085521698
train gradient:  0.24021619780908998
iteration : 4376
train acc:  0.8515625
train loss:  0.35067129135131836
train gradient:  0.2293041688924752
iteration : 4377
train acc:  0.8671875
train loss:  0.3801311254501343
train gradient:  0.2594143378173711
iteration : 4378
train acc:  0.890625
train loss:  0.2781692147254944
train gradient:  0.15205984901915487
iteration : 4379
train acc:  0.84375
train loss:  0.3411014974117279
train gradient:  0.24869828425427348
iteration : 4380
train acc:  0.8671875
train loss:  0.3222285211086273
train gradient:  0.20692011491573353
iteration : 4381
train acc:  0.859375
train loss:  0.3539906144142151
train gradient:  0.1948594527393821
iteration : 4382
train acc:  0.828125
train loss:  0.33433887362480164
train gradient:  0.1954955713178749
iteration : 4383
train acc:  0.859375
train loss:  0.39085620641708374
train gradient:  0.20470972348428967
iteration : 4384
train acc:  0.8671875
train loss:  0.32801955938339233
train gradient:  0.19764851997211702
iteration : 4385
train acc:  0.8828125
train loss:  0.28715264797210693
train gradient:  0.25231495013136174
iteration : 4386
train acc:  0.8125
train loss:  0.42089760303497314
train gradient:  0.3343682174834425
iteration : 4387
train acc:  0.8359375
train loss:  0.38071390986442566
train gradient:  0.4626981870097073
iteration : 4388
train acc:  0.8359375
train loss:  0.37844032049179077
train gradient:  0.3230891769822311
iteration : 4389
train acc:  0.859375
train loss:  0.3449617326259613
train gradient:  0.23294004200081853
iteration : 4390
train acc:  0.8671875
train loss:  0.34313204884529114
train gradient:  0.2375999296590613
iteration : 4391
train acc:  0.8671875
train loss:  0.3312864899635315
train gradient:  0.18236286875159521
iteration : 4392
train acc:  0.8046875
train loss:  0.39344683289527893
train gradient:  0.39415645580276526
iteration : 4393
train acc:  0.8125
train loss:  0.3882575035095215
train gradient:  0.23383966293356218
iteration : 4394
train acc:  0.8203125
train loss:  0.3838665187358856
train gradient:  0.28552395428009325
iteration : 4395
train acc:  0.8359375
train loss:  0.3372582793235779
train gradient:  0.32486333846536475
iteration : 4396
train acc:  0.796875
train loss:  0.3883827328681946
train gradient:  0.3170170492935861
iteration : 4397
train acc:  0.8125
train loss:  0.35827118158340454
train gradient:  0.24847997292263052
iteration : 4398
train acc:  0.84375
train loss:  0.3627479672431946
train gradient:  0.27821595599693666
iteration : 4399
train acc:  0.7890625
train loss:  0.4014088809490204
train gradient:  0.32268544764560675
iteration : 4400
train acc:  0.875
train loss:  0.31114351749420166
train gradient:  0.22028559578984402
iteration : 4401
train acc:  0.84375
train loss:  0.3626454472541809
train gradient:  0.26564736182228327
iteration : 4402
train acc:  0.8515625
train loss:  0.345300555229187
train gradient:  0.22913231110139898
iteration : 4403
train acc:  0.859375
train loss:  0.31095296144485474
train gradient:  0.21978820511080988
iteration : 4404
train acc:  0.84375
train loss:  0.33048418164253235
train gradient:  0.17547055394033434
iteration : 4405
train acc:  0.8125
train loss:  0.36932921409606934
train gradient:  0.22999777307133418
iteration : 4406
train acc:  0.8203125
train loss:  0.3496952950954437
train gradient:  0.32399162221009137
iteration : 4407
train acc:  0.8515625
train loss:  0.32411086559295654
train gradient:  0.20531926321118538
iteration : 4408
train acc:  0.8359375
train loss:  0.3433327376842499
train gradient:  0.26291222300332523
iteration : 4409
train acc:  0.8125
train loss:  0.34131526947021484
train gradient:  0.28543446599326217
iteration : 4410
train acc:  0.84375
train loss:  0.4154835343360901
train gradient:  0.3885119957274631
iteration : 4411
train acc:  0.84375
train loss:  0.3891894817352295
train gradient:  0.4034764669357472
iteration : 4412
train acc:  0.828125
train loss:  0.3687009811401367
train gradient:  0.2562182397109718
iteration : 4413
train acc:  0.7890625
train loss:  0.4035226106643677
train gradient:  0.43773711330721543
iteration : 4414
train acc:  0.8515625
train loss:  0.33304983377456665
train gradient:  0.3014941833871532
iteration : 4415
train acc:  0.8515625
train loss:  0.3338264226913452
train gradient:  0.2224807657899719
iteration : 4416
train acc:  0.828125
train loss:  0.3306843042373657
train gradient:  0.22234759923246322
iteration : 4417
train acc:  0.8671875
train loss:  0.3069837689399719
train gradient:  0.19043664846927943
iteration : 4418
train acc:  0.8359375
train loss:  0.349519282579422
train gradient:  0.22136610892436215
iteration : 4419
train acc:  0.8203125
train loss:  0.380048930644989
train gradient:  0.2786610231255223
iteration : 4420
train acc:  0.90625
train loss:  0.3626829981803894
train gradient:  0.32281787729634603
iteration : 4421
train acc:  0.8671875
train loss:  0.3538333773612976
train gradient:  0.21938248232643232
iteration : 4422
train acc:  0.8359375
train loss:  0.368312269449234
train gradient:  0.21250743054204135
iteration : 4423
train acc:  0.8125
train loss:  0.3959636688232422
train gradient:  0.3217966488142343
iteration : 4424
train acc:  0.8046875
train loss:  0.43037861585617065
train gradient:  0.35213104398867223
iteration : 4425
train acc:  0.84375
train loss:  0.36337870359420776
train gradient:  0.2569512868738754
iteration : 4426
train acc:  0.890625
train loss:  0.26370301842689514
train gradient:  0.1999833738607673
iteration : 4427
train acc:  0.84375
train loss:  0.39609187841415405
train gradient:  0.5014645724435385
iteration : 4428
train acc:  0.859375
train loss:  0.3786628544330597
train gradient:  0.261422301145338
iteration : 4429
train acc:  0.859375
train loss:  0.3032021224498749
train gradient:  0.18450169516896614
iteration : 4430
train acc:  0.8046875
train loss:  0.3905196785926819
train gradient:  0.21390311709345555
iteration : 4431
train acc:  0.8046875
train loss:  0.48446229100227356
train gradient:  0.43203822883848764
iteration : 4432
train acc:  0.8046875
train loss:  0.42958343029022217
train gradient:  0.3848577205934575
iteration : 4433
train acc:  0.78125
train loss:  0.41708308458328247
train gradient:  0.4239730481013227
iteration : 4434
train acc:  0.84375
train loss:  0.3824835419654846
train gradient:  0.3710532639015011
iteration : 4435
train acc:  0.859375
train loss:  0.34801286458969116
train gradient:  0.2645231520011679
iteration : 4436
train acc:  0.8671875
train loss:  0.3605346977710724
train gradient:  0.29016463888730604
iteration : 4437
train acc:  0.859375
train loss:  0.2945120930671692
train gradient:  0.21102965845835248
iteration : 4438
train acc:  0.8828125
train loss:  0.3378625512123108
train gradient:  0.33801555436154196
iteration : 4439
train acc:  0.875
train loss:  0.289241224527359
train gradient:  0.17241425417814327
iteration : 4440
train acc:  0.875
train loss:  0.32075035572052
train gradient:  0.19156932198863913
iteration : 4441
train acc:  0.8046875
train loss:  0.3974834978580475
train gradient:  0.2684287727750917
iteration : 4442
train acc:  0.8125
train loss:  0.373818963766098
train gradient:  0.32361824704653863
iteration : 4443
train acc:  0.7890625
train loss:  0.40004345774650574
train gradient:  0.2985893897195835
iteration : 4444
train acc:  0.8359375
train loss:  0.35953760147094727
train gradient:  0.20803975226259644
iteration : 4445
train acc:  0.8671875
train loss:  0.2835991084575653
train gradient:  0.194037083704371
iteration : 4446
train acc:  0.8359375
train loss:  0.3630944490432739
train gradient:  0.4619446608139335
iteration : 4447
train acc:  0.8125
train loss:  0.42431357502937317
train gradient:  0.2749552548216979
iteration : 4448
train acc:  0.8125
train loss:  0.39757001399993896
train gradient:  0.3377239600867435
iteration : 4449
train acc:  0.8671875
train loss:  0.3598170876502991
train gradient:  0.29713805394265125
iteration : 4450
train acc:  0.828125
train loss:  0.4021545350551605
train gradient:  0.42363895401986074
iteration : 4451
train acc:  0.8359375
train loss:  0.33488088846206665
train gradient:  0.2534585982168788
iteration : 4452
train acc:  0.859375
train loss:  0.3183603882789612
train gradient:  0.27388287367503356
iteration : 4453
train acc:  0.8359375
train loss:  0.34547799825668335
train gradient:  0.2794124595423813
iteration : 4454
train acc:  0.8515625
train loss:  0.32865801453590393
train gradient:  0.2644111658922213
iteration : 4455
train acc:  0.8671875
train loss:  0.3309100270271301
train gradient:  0.22903681516521104
iteration : 4456
train acc:  0.8671875
train loss:  0.34639838337898254
train gradient:  0.22139266657581078
iteration : 4457
train acc:  0.8671875
train loss:  0.38379693031311035
train gradient:  0.24887683837205105
iteration : 4458
train acc:  0.7734375
train loss:  0.4126039147377014
train gradient:  0.32367614805073497
iteration : 4459
train acc:  0.8515625
train loss:  0.3518329858779907
train gradient:  0.20622830298989947
iteration : 4460
train acc:  0.7578125
train loss:  0.4262353479862213
train gradient:  0.3314033938746943
iteration : 4461
train acc:  0.8125
train loss:  0.3584880232810974
train gradient:  0.3118986636463418
iteration : 4462
train acc:  0.875
train loss:  0.33472657203674316
train gradient:  0.24636467842569315
iteration : 4463
train acc:  0.859375
train loss:  0.32024502754211426
train gradient:  0.23439344753174102
iteration : 4464
train acc:  0.8359375
train loss:  0.3718680143356323
train gradient:  0.2772228993024398
iteration : 4465
train acc:  0.796875
train loss:  0.38860711455345154
train gradient:  0.3043555121134869
iteration : 4466
train acc:  0.8125
train loss:  0.34466108679771423
train gradient:  0.2578482702158627
iteration : 4467
train acc:  0.828125
train loss:  0.36890512704849243
train gradient:  0.26671842618157715
iteration : 4468
train acc:  0.828125
train loss:  0.35109710693359375
train gradient:  0.18712282523497997
iteration : 4469
train acc:  0.7421875
train loss:  0.5076324939727783
train gradient:  0.49683821053616756
iteration : 4470
train acc:  0.8203125
train loss:  0.34267085790634155
train gradient:  0.27127577744901116
iteration : 4471
train acc:  0.859375
train loss:  0.3212708830833435
train gradient:  0.17679581002986045
iteration : 4472
train acc:  0.828125
train loss:  0.3703210651874542
train gradient:  0.24924969925044654
iteration : 4473
train acc:  0.84375
train loss:  0.32120323181152344
train gradient:  0.22892714693069222
iteration : 4474
train acc:  0.921875
train loss:  0.2430557906627655
train gradient:  0.11421704326481442
iteration : 4475
train acc:  0.8515625
train loss:  0.36298754811286926
train gradient:  0.2793453679891497
iteration : 4476
train acc:  0.84375
train loss:  0.37227168679237366
train gradient:  0.3739398818380287
iteration : 4477
train acc:  0.8125
train loss:  0.386993408203125
train gradient:  0.3631369487816181
iteration : 4478
train acc:  0.8671875
train loss:  0.35299989581108093
train gradient:  0.27876998674654746
iteration : 4479
train acc:  0.8359375
train loss:  0.3416132926940918
train gradient:  0.20828122270034954
iteration : 4480
train acc:  0.8671875
train loss:  0.3254246711730957
train gradient:  0.21096596775209925
iteration : 4481
train acc:  0.9140625
train loss:  0.31437432765960693
train gradient:  0.23405718942030335
iteration : 4482
train acc:  0.8359375
train loss:  0.35153698921203613
train gradient:  0.2276344744214344
iteration : 4483
train acc:  0.78125
train loss:  0.4045967161655426
train gradient:  0.3082380232970832
iteration : 4484
train acc:  0.8515625
train loss:  0.34638768434524536
train gradient:  0.44531225804966085
iteration : 4485
train acc:  0.8359375
train loss:  0.36722874641418457
train gradient:  0.2535016506330346
iteration : 4486
train acc:  0.8828125
train loss:  0.29176732897758484
train gradient:  0.18184657635385482
iteration : 4487
train acc:  0.8515625
train loss:  0.3524976968765259
train gradient:  0.2033712916582875
iteration : 4488
train acc:  0.828125
train loss:  0.38887423276901245
train gradient:  0.23747180329213077
iteration : 4489
train acc:  0.8515625
train loss:  0.37770187854766846
train gradient:  0.3203834745658756
iteration : 4490
train acc:  0.8203125
train loss:  0.35033681988716125
train gradient:  0.23881156343685514
iteration : 4491
train acc:  0.8671875
train loss:  0.290142297744751
train gradient:  0.2321623793831706
iteration : 4492
train acc:  0.8515625
train loss:  0.3198198080062866
train gradient:  0.33282378455996214
iteration : 4493
train acc:  0.84375
train loss:  0.3596615195274353
train gradient:  0.29213913691947135
iteration : 4494
train acc:  0.90625
train loss:  0.26065099239349365
train gradient:  0.20075470878172175
iteration : 4495
train acc:  0.8671875
train loss:  0.3485884964466095
train gradient:  0.2532836829705709
iteration : 4496
train acc:  0.828125
train loss:  0.4183964133262634
train gradient:  0.435976844765592
iteration : 4497
train acc:  0.8515625
train loss:  0.3465000092983246
train gradient:  0.3219364308119863
iteration : 4498
train acc:  0.796875
train loss:  0.4938250482082367
train gradient:  0.4796198880393614
iteration : 4499
train acc:  0.8359375
train loss:  0.38243791460990906
train gradient:  0.2645934325526974
iteration : 4500
train acc:  0.828125
train loss:  0.40839678049087524
train gradient:  0.49931303510205405
iteration : 4501
train acc:  0.8515625
train loss:  0.32619136571884155
train gradient:  0.20760577486160833
iteration : 4502
train acc:  0.8828125
train loss:  0.37353965640068054
train gradient:  0.2592294606293317
iteration : 4503
train acc:  0.84375
train loss:  0.39910125732421875
train gradient:  0.49093575769098113
iteration : 4504
train acc:  0.8515625
train loss:  0.3772249221801758
train gradient:  0.25569582825735104
iteration : 4505
train acc:  0.859375
train loss:  0.3229772448539734
train gradient:  0.2480512314374827
iteration : 4506
train acc:  0.7890625
train loss:  0.4264627695083618
train gradient:  0.410081859441536
iteration : 4507
train acc:  0.765625
train loss:  0.471919447183609
train gradient:  0.5431778455760344
iteration : 4508
train acc:  0.8671875
train loss:  0.32869958877563477
train gradient:  0.19043196413641195
iteration : 4509
train acc:  0.859375
train loss:  0.31096363067626953
train gradient:  0.18889524989465076
iteration : 4510
train acc:  0.8671875
train loss:  0.33718883991241455
train gradient:  0.28293848563753676
iteration : 4511
train acc:  0.890625
train loss:  0.2580989599227905
train gradient:  0.199982979351458
iteration : 4512
train acc:  0.8515625
train loss:  0.31086039543151855
train gradient:  0.14814838597432134
iteration : 4513
train acc:  0.84375
train loss:  0.33968549966812134
train gradient:  0.24475215820634066
iteration : 4514
train acc:  0.84375
train loss:  0.31349217891693115
train gradient:  0.25900069045647434
iteration : 4515
train acc:  0.8671875
train loss:  0.34780633449554443
train gradient:  0.21495619436114666
iteration : 4516
train acc:  0.8203125
train loss:  0.43288958072662354
train gradient:  0.2487056540477467
iteration : 4517
train acc:  0.796875
train loss:  0.47335097193717957
train gradient:  0.5899984326708169
iteration : 4518
train acc:  0.8125
train loss:  0.3476714491844177
train gradient:  0.2575544101357568
iteration : 4519
train acc:  0.84375
train loss:  0.33544278144836426
train gradient:  0.23920946826803763
iteration : 4520
train acc:  0.7890625
train loss:  0.4295049011707306
train gradient:  0.27980207416751696
iteration : 4521
train acc:  0.8671875
train loss:  0.3374635577201843
train gradient:  0.21155905981936132
iteration : 4522
train acc:  0.8359375
train loss:  0.3208215534687042
train gradient:  0.18171968366562827
iteration : 4523
train acc:  0.8046875
train loss:  0.37207967042922974
train gradient:  0.21972405163857645
iteration : 4524
train acc:  0.734375
train loss:  0.4853910803794861
train gradient:  0.3525762377126966
iteration : 4525
train acc:  0.8671875
train loss:  0.34929776191711426
train gradient:  0.2169809406732759
iteration : 4526
train acc:  0.7890625
train loss:  0.4626668691635132
train gradient:  0.3262276698036974
iteration : 4527
train acc:  0.90625
train loss:  0.3020186424255371
train gradient:  0.21995010379124658
iteration : 4528
train acc:  0.796875
train loss:  0.41336554288864136
train gradient:  0.30239624083180616
iteration : 4529
train acc:  0.8203125
train loss:  0.4092981815338135
train gradient:  0.2700335058870319
iteration : 4530
train acc:  0.8828125
train loss:  0.26038211584091187
train gradient:  0.13406022791140626
iteration : 4531
train acc:  0.828125
train loss:  0.36936748027801514
train gradient:  0.22866059485490928
iteration : 4532
train acc:  0.8046875
train loss:  0.42438891530036926
train gradient:  0.33896823073885074
iteration : 4533
train acc:  0.8359375
train loss:  0.3582959771156311
train gradient:  0.2694522212606539
iteration : 4534
train acc:  0.8125
train loss:  0.3587721884250641
train gradient:  0.22385242030332078
iteration : 4535
train acc:  0.828125
train loss:  0.3799411654472351
train gradient:  0.4066026298895279
iteration : 4536
train acc:  0.8359375
train loss:  0.35147660970687866
train gradient:  0.20403508998111583
iteration : 4537
train acc:  0.8671875
train loss:  0.2964898347854614
train gradient:  0.18671478405700234
iteration : 4538
train acc:  0.8359375
train loss:  0.40324464440345764
train gradient:  0.3660101712013799
iteration : 4539
train acc:  0.8671875
train loss:  0.33795610070228577
train gradient:  0.21661051500481962
iteration : 4540
train acc:  0.8203125
train loss:  0.39260977506637573
train gradient:  0.371472639330746
iteration : 4541
train acc:  0.796875
train loss:  0.3950909376144409
train gradient:  0.3050336296911306
iteration : 4542
train acc:  0.8046875
train loss:  0.4029649496078491
train gradient:  0.35462256256972335
iteration : 4543
train acc:  0.90625
train loss:  0.29004916548728943
train gradient:  0.2684714329926485
iteration : 4544
train acc:  0.8203125
train loss:  0.3664514720439911
train gradient:  0.23233532339751273
iteration : 4545
train acc:  0.765625
train loss:  0.4605932831764221
train gradient:  0.38660867505298546
iteration : 4546
train acc:  0.8125
train loss:  0.4142047166824341
train gradient:  0.30180493580999507
iteration : 4547
train acc:  0.8515625
train loss:  0.37853822112083435
train gradient:  0.26821523232706523
iteration : 4548
train acc:  0.8359375
train loss:  0.40738070011138916
train gradient:  0.44468766488811395
iteration : 4549
train acc:  0.796875
train loss:  0.4004055857658386
train gradient:  0.40055600661930224
iteration : 4550
train acc:  0.8125
train loss:  0.33691221475601196
train gradient:  0.360035509068415
iteration : 4551
train acc:  0.8828125
train loss:  0.29886502027511597
train gradient:  0.2123915616657503
iteration : 4552
train acc:  0.84375
train loss:  0.36953842639923096
train gradient:  0.19908798173222714
iteration : 4553
train acc:  0.8828125
train loss:  0.3218004107475281
train gradient:  0.191316510303591
iteration : 4554
train acc:  0.84375
train loss:  0.3767510652542114
train gradient:  0.3684785219881981
iteration : 4555
train acc:  0.875
train loss:  0.2745990753173828
train gradient:  0.23095532471798827
iteration : 4556
train acc:  0.8125
train loss:  0.4317978024482727
train gradient:  0.3768478939060651
iteration : 4557
train acc:  0.8515625
train loss:  0.4271180033683777
train gradient:  0.28763000678735723
iteration : 4558
train acc:  0.8671875
train loss:  0.36796489357948303
train gradient:  0.2640565473686594
iteration : 4559
train acc:  0.875
train loss:  0.283143013715744
train gradient:  0.18015033610096357
iteration : 4560
train acc:  0.8515625
train loss:  0.3369235694408417
train gradient:  0.25244142868809316
iteration : 4561
train acc:  0.8984375
train loss:  0.29680126905441284
train gradient:  0.23867547579981135
iteration : 4562
train acc:  0.8671875
train loss:  0.3243778944015503
train gradient:  0.22687820035203682
iteration : 4563
train acc:  0.8671875
train loss:  0.33658289909362793
train gradient:  0.21963057964549898
iteration : 4564
train acc:  0.8828125
train loss:  0.32187533378601074
train gradient:  0.15202858473049075
iteration : 4565
train acc:  0.8671875
train loss:  0.30305492877960205
train gradient:  0.21469603238227655
iteration : 4566
train acc:  0.859375
train loss:  0.34619686007499695
train gradient:  0.31116309414735105
iteration : 4567
train acc:  0.859375
train loss:  0.3516882359981537
train gradient:  0.3194577290461446
iteration : 4568
train acc:  0.875
train loss:  0.30223172903060913
train gradient:  0.1773494444221022
iteration : 4569
train acc:  0.828125
train loss:  0.43286842107772827
train gradient:  0.35971725840970953
iteration : 4570
train acc:  0.8125
train loss:  0.3441779613494873
train gradient:  0.28683565635243946
iteration : 4571
train acc:  0.8203125
train loss:  0.37647294998168945
train gradient:  0.23666656082230533
iteration : 4572
train acc:  0.796875
train loss:  0.4890143871307373
train gradient:  0.3426218860666346
iteration : 4573
train acc:  0.828125
train loss:  0.3764915466308594
train gradient:  0.2136093719600169
iteration : 4574
train acc:  0.8984375
train loss:  0.3027251958847046
train gradient:  0.2273348804540892
iteration : 4575
train acc:  0.8828125
train loss:  0.2862718999385834
train gradient:  0.1395714806610354
iteration : 4576
train acc:  0.859375
train loss:  0.35185879468917847
train gradient:  0.32137200565510016
iteration : 4577
train acc:  0.859375
train loss:  0.3400229215621948
train gradient:  0.2765590140267565
iteration : 4578
train acc:  0.8828125
train loss:  0.2820131778717041
train gradient:  0.21084763775174853
iteration : 4579
train acc:  0.8515625
train loss:  0.36014890670776367
train gradient:  0.26683576765890693
iteration : 4580
train acc:  0.8515625
train loss:  0.3472166061401367
train gradient:  0.22347113696149845
iteration : 4581
train acc:  0.8203125
train loss:  0.34721818566322327
train gradient:  0.27152590571039653
iteration : 4582
train acc:  0.8359375
train loss:  0.35261788964271545
train gradient:  0.2807677824129021
iteration : 4583
train acc:  0.859375
train loss:  0.3553793430328369
train gradient:  0.23093727014502394
iteration : 4584
train acc:  0.8515625
train loss:  0.2895858585834503
train gradient:  0.17237865520628587
iteration : 4585
train acc:  0.859375
train loss:  0.34531813859939575
train gradient:  0.23738646047647505
iteration : 4586
train acc:  0.859375
train loss:  0.3549068570137024
train gradient:  0.2584067332404117
iteration : 4587
train acc:  0.8515625
train loss:  0.30177760124206543
train gradient:  0.20179509423061862
iteration : 4588
train acc:  0.828125
train loss:  0.36517465114593506
train gradient:  0.32360897985995923
iteration : 4589
train acc:  0.828125
train loss:  0.3834056258201599
train gradient:  0.26487715425191777
iteration : 4590
train acc:  0.828125
train loss:  0.3271442651748657
train gradient:  0.2949287352866538
iteration : 4591
train acc:  0.890625
train loss:  0.29512277245521545
train gradient:  0.18212873990529568
iteration : 4592
train acc:  0.8203125
train loss:  0.3757404088973999
train gradient:  0.25727843973562714
iteration : 4593
train acc:  0.8203125
train loss:  0.4441956877708435
train gradient:  0.43949847627524447
iteration : 4594
train acc:  0.828125
train loss:  0.4024296998977661
train gradient:  0.3940565835722232
iteration : 4595
train acc:  0.8359375
train loss:  0.3163937032222748
train gradient:  0.33400953890548535
iteration : 4596
train acc:  0.8125
train loss:  0.37123870849609375
train gradient:  0.25191196986535264
iteration : 4597
train acc:  0.859375
train loss:  0.3362705409526825
train gradient:  0.2959047256318205
iteration : 4598
train acc:  0.828125
train loss:  0.3605392277240753
train gradient:  0.1968668480420488
iteration : 4599
train acc:  0.828125
train loss:  0.3613432049751282
train gradient:  0.2664406965199606
iteration : 4600
train acc:  0.90625
train loss:  0.27583032846450806
train gradient:  0.1763323100668212
iteration : 4601
train acc:  0.890625
train loss:  0.3113378882408142
train gradient:  0.24563990786267959
iteration : 4602
train acc:  0.8671875
train loss:  0.2955233156681061
train gradient:  0.17135571322470872
iteration : 4603
train acc:  0.8125
train loss:  0.36207565665245056
train gradient:  0.2674169348785939
iteration : 4604
train acc:  0.84375
train loss:  0.4330950677394867
train gradient:  0.38853049701109876
iteration : 4605
train acc:  0.8359375
train loss:  0.35721686482429504
train gradient:  0.363275867530546
iteration : 4606
train acc:  0.84375
train loss:  0.3825482130050659
train gradient:  0.34671300587696957
iteration : 4607
train acc:  0.8984375
train loss:  0.28441837430000305
train gradient:  0.2433663941191039
iteration : 4608
train acc:  0.796875
train loss:  0.390832781791687
train gradient:  0.28863755560136684
iteration : 4609
train acc:  0.859375
train loss:  0.3141981363296509
train gradient:  0.18107613538012413
iteration : 4610
train acc:  0.8203125
train loss:  0.4261923134326935
train gradient:  0.31707636966706454
iteration : 4611
train acc:  0.8359375
train loss:  0.33723050355911255
train gradient:  0.28249670779068947
iteration : 4612
train acc:  0.8203125
train loss:  0.4394252896308899
train gradient:  0.3141996527383859
iteration : 4613
train acc:  0.828125
train loss:  0.38926461338996887
train gradient:  0.26267229282635485
iteration : 4614
train acc:  0.8359375
train loss:  0.3921266198158264
train gradient:  0.462433310012654
iteration : 4615
train acc:  0.84375
train loss:  0.3336324393749237
train gradient:  0.27463295768994533
iteration : 4616
train acc:  0.8359375
train loss:  0.3536917567253113
train gradient:  0.4069671696288774
iteration : 4617
train acc:  0.796875
train loss:  0.4394115209579468
train gradient:  0.45643806096175693
iteration : 4618
train acc:  0.8203125
train loss:  0.38086357712745667
train gradient:  0.3340361770767601
iteration : 4619
train acc:  0.8671875
train loss:  0.2946215271949768
train gradient:  0.2179432395425029
iteration : 4620
train acc:  0.859375
train loss:  0.33365243673324585
train gradient:  0.22820649809255178
iteration : 4621
train acc:  0.8515625
train loss:  0.31868070363998413
train gradient:  0.17313264050133245
iteration : 4622
train acc:  0.8828125
train loss:  0.28552865982055664
train gradient:  0.17558071754673585
iteration : 4623
train acc:  0.8828125
train loss:  0.3002094030380249
train gradient:  0.22996690416829718
iteration : 4624
train acc:  0.8984375
train loss:  0.2714989483356476
train gradient:  0.19487756454972405
iteration : 4625
train acc:  0.875
train loss:  0.3105555474758148
train gradient:  0.16262619154913913
iteration : 4626
train acc:  0.859375
train loss:  0.38535889983177185
train gradient:  0.3456243937957526
iteration : 4627
train acc:  0.84375
train loss:  0.38873785734176636
train gradient:  0.23602639089984367
iteration : 4628
train acc:  0.8359375
train loss:  0.3471826910972595
train gradient:  0.2294938279729925
iteration : 4629
train acc:  0.828125
train loss:  0.39702507853507996
train gradient:  0.25799925993164746
iteration : 4630
train acc:  0.8203125
train loss:  0.4316212832927704
train gradient:  0.2741442245009695
iteration : 4631
train acc:  0.875
train loss:  0.36641910672187805
train gradient:  0.2508692853925409
iteration : 4632
train acc:  0.7890625
train loss:  0.4457482099533081
train gradient:  0.26738190527751277
iteration : 4633
train acc:  0.8984375
train loss:  0.27121803164482117
train gradient:  0.15570274000428694
iteration : 4634
train acc:  0.8203125
train loss:  0.3395474851131439
train gradient:  0.2133888332552098
iteration : 4635
train acc:  0.828125
train loss:  0.4190298914909363
train gradient:  0.30798993384149004
iteration : 4636
train acc:  0.8046875
train loss:  0.40167295932769775
train gradient:  0.41978842691386453
iteration : 4637
train acc:  0.7421875
train loss:  0.5231653451919556
train gradient:  0.6623206522883054
iteration : 4638
train acc:  0.859375
train loss:  0.2659125030040741
train gradient:  0.10816275819827725
iteration : 4639
train acc:  0.84375
train loss:  0.3287069797515869
train gradient:  0.19490904641900525
iteration : 4640
train acc:  0.828125
train loss:  0.42118412256240845
train gradient:  0.3338613297036969
iteration : 4641
train acc:  0.8203125
train loss:  0.34718209505081177
train gradient:  0.20132206516172357
iteration : 4642
train acc:  0.859375
train loss:  0.30632275342941284
train gradient:  0.20067446280058543
iteration : 4643
train acc:  0.8671875
train loss:  0.2752622961997986
train gradient:  0.18414795356397842
iteration : 4644
train acc:  0.8203125
train loss:  0.43864905834198
train gradient:  0.36987653564755707
iteration : 4645
train acc:  0.7890625
train loss:  0.3853702247142792
train gradient:  0.2629167994222144
iteration : 4646
train acc:  0.8359375
train loss:  0.3557957708835602
train gradient:  0.2457739363362666
iteration : 4647
train acc:  0.8671875
train loss:  0.34080275893211365
train gradient:  0.2032911716812802
iteration : 4648
train acc:  0.796875
train loss:  0.4190094470977783
train gradient:  0.36709868178541516
iteration : 4649
train acc:  0.875
train loss:  0.3536350727081299
train gradient:  0.21630918949955974
iteration : 4650
train acc:  0.828125
train loss:  0.3975479006767273
train gradient:  0.2413911209871572
iteration : 4651
train acc:  0.8046875
train loss:  0.36908262968063354
train gradient:  0.2576756834724549
iteration : 4652
train acc:  0.859375
train loss:  0.32239338755607605
train gradient:  0.18121092634709093
iteration : 4653
train acc:  0.875
train loss:  0.29368311166763306
train gradient:  0.19165430657123628
iteration : 4654
train acc:  0.828125
train loss:  0.38600870966911316
train gradient:  0.30325875794438717
iteration : 4655
train acc:  0.8671875
train loss:  0.31619739532470703
train gradient:  0.1871676940272809
iteration : 4656
train acc:  0.8671875
train loss:  0.3060723543167114
train gradient:  0.19506285650594277
iteration : 4657
train acc:  0.8515625
train loss:  0.34060055017471313
train gradient:  0.24725115025699432
iteration : 4658
train acc:  0.8203125
train loss:  0.38426223397254944
train gradient:  0.24314493004710966
iteration : 4659
train acc:  0.8828125
train loss:  0.31486088037490845
train gradient:  0.19300293979730163
iteration : 4660
train acc:  0.8828125
train loss:  0.2998851239681244
train gradient:  0.24580227809454608
iteration : 4661
train acc:  0.84375
train loss:  0.38559603691101074
train gradient:  0.2704009055825033
iteration : 4662
train acc:  0.8515625
train loss:  0.33503222465515137
train gradient:  0.24186315585886173
iteration : 4663
train acc:  0.8046875
train loss:  0.41627809405326843
train gradient:  0.3051165801326564
iteration : 4664
train acc:  0.859375
train loss:  0.3431244492530823
train gradient:  0.23216992971006858
iteration : 4665
train acc:  0.84375
train loss:  0.33374616503715515
train gradient:  0.22896326025816308
iteration : 4666
train acc:  0.8515625
train loss:  0.34238454699516296
train gradient:  0.23057670887916407
iteration : 4667
train acc:  0.875
train loss:  0.29171788692474365
train gradient:  0.18993402067161047
iteration : 4668
train acc:  0.8515625
train loss:  0.36371105909347534
train gradient:  0.23225192054982247
iteration : 4669
train acc:  0.875
train loss:  0.32641375064849854
train gradient:  0.19996613522283677
iteration : 4670
train acc:  0.8515625
train loss:  0.32947826385498047
train gradient:  0.22200169265132866
iteration : 4671
train acc:  0.9140625
train loss:  0.2650652527809143
train gradient:  0.11102364354645487
iteration : 4672
train acc:  0.8359375
train loss:  0.3160191476345062
train gradient:  0.2005257863728508
iteration : 4673
train acc:  0.859375
train loss:  0.29625841975212097
train gradient:  0.16719583311606684
iteration : 4674
train acc:  0.8515625
train loss:  0.3398645520210266
train gradient:  0.24754243871540377
iteration : 4675
train acc:  0.859375
train loss:  0.30244237184524536
train gradient:  0.208173617251441
iteration : 4676
train acc:  0.8203125
train loss:  0.3653794527053833
train gradient:  0.2476276378377645
iteration : 4677
train acc:  0.8203125
train loss:  0.38289669156074524
train gradient:  0.39273537786550977
iteration : 4678
train acc:  0.828125
train loss:  0.348970502614975
train gradient:  0.2790645787024776
iteration : 4679
train acc:  0.8359375
train loss:  0.3803331255912781
train gradient:  0.29651401166017105
iteration : 4680
train acc:  0.8359375
train loss:  0.3901848793029785
train gradient:  0.33158346525488713
iteration : 4681
train acc:  0.8203125
train loss:  0.4005017876625061
train gradient:  0.3740773801855795
iteration : 4682
train acc:  0.828125
train loss:  0.40913134813308716
train gradient:  0.3820491387116593
iteration : 4683
train acc:  0.8671875
train loss:  0.36740896105766296
train gradient:  0.32999525114884865
iteration : 4684
train acc:  0.8359375
train loss:  0.3907560408115387
train gradient:  0.3609791296998412
iteration : 4685
train acc:  0.859375
train loss:  0.29172539710998535
train gradient:  0.2709525354356486
iteration : 4686
train acc:  0.8359375
train loss:  0.35148513317108154
train gradient:  0.22101059845664373
iteration : 4687
train acc:  0.8125
train loss:  0.42834317684173584
train gradient:  0.3473494617141331
iteration : 4688
train acc:  0.828125
train loss:  0.4056098759174347
train gradient:  0.3838487344889248
iteration : 4689
train acc:  0.7734375
train loss:  0.4951927661895752
train gradient:  0.46055264943078894
iteration : 4690
train acc:  0.859375
train loss:  0.34332746267318726
train gradient:  0.2371514359609994
iteration : 4691
train acc:  0.796875
train loss:  0.3815561532974243
train gradient:  0.2586130126339831
iteration : 4692
train acc:  0.796875
train loss:  0.3717634677886963
train gradient:  0.35476177668552145
iteration : 4693
train acc:  0.84375
train loss:  0.3313189446926117
train gradient:  0.20906629428254542
iteration : 4694
train acc:  0.796875
train loss:  0.4118339419364929
train gradient:  0.4800622332853789
iteration : 4695
train acc:  0.875
train loss:  0.3223000764846802
train gradient:  0.21878830402344357
iteration : 4696
train acc:  0.890625
train loss:  0.2887918949127197
train gradient:  0.1617874042695927
iteration : 4697
train acc:  0.859375
train loss:  0.34061169624328613
train gradient:  0.1814892152359398
iteration : 4698
train acc:  0.8515625
train loss:  0.327140212059021
train gradient:  0.21676600038952268
iteration : 4699
train acc:  0.8515625
train loss:  0.3505941927433014
train gradient:  0.16941630099366414
iteration : 4700
train acc:  0.7890625
train loss:  0.4452330470085144
train gradient:  0.38918698678314856
iteration : 4701
train acc:  0.8203125
train loss:  0.3730556070804596
train gradient:  0.37485125801448493
iteration : 4702
train acc:  0.890625
train loss:  0.26287925243377686
train gradient:  0.17639721726710317
iteration : 4703
train acc:  0.8515625
train loss:  0.3038833439350128
train gradient:  0.1865332248301259
iteration : 4704
train acc:  0.8046875
train loss:  0.37953704595565796
train gradient:  0.22448934830620518
iteration : 4705
train acc:  0.8671875
train loss:  0.3169770836830139
train gradient:  0.5008698297237695
iteration : 4706
train acc:  0.8515625
train loss:  0.34014689922332764
train gradient:  0.365474104109486
iteration : 4707
train acc:  0.7890625
train loss:  0.3681502938270569
train gradient:  0.22820366463766192
iteration : 4708
train acc:  0.875
train loss:  0.4054635763168335
train gradient:  0.406039911122204
iteration : 4709
train acc:  0.8515625
train loss:  0.3521738052368164
train gradient:  0.2264221089209003
iteration : 4710
train acc:  0.8671875
train loss:  0.2997184693813324
train gradient:  0.17618842577355384
iteration : 4711
train acc:  0.828125
train loss:  0.35630422830581665
train gradient:  0.20777455979602705
iteration : 4712
train acc:  0.8203125
train loss:  0.3673705458641052
train gradient:  0.23302185787540886
iteration : 4713
train acc:  0.875
train loss:  0.2921122908592224
train gradient:  0.5336092384554195
iteration : 4714
train acc:  0.84375
train loss:  0.32568737864494324
train gradient:  0.2406924162749339
iteration : 4715
train acc:  0.859375
train loss:  0.32731422781944275
train gradient:  0.28786595067425874
iteration : 4716
train acc:  0.828125
train loss:  0.3561641573905945
train gradient:  0.2805619736302113
iteration : 4717
train acc:  0.890625
train loss:  0.33876675367355347
train gradient:  0.24129205396152104
iteration : 4718
train acc:  0.8125
train loss:  0.4101792871952057
train gradient:  0.34507721524017226
iteration : 4719
train acc:  0.8359375
train loss:  0.43388232588768005
train gradient:  0.42285324120797296
iteration : 4720
train acc:  0.8046875
train loss:  0.4053376615047455
train gradient:  0.24105967992322963
iteration : 4721
train acc:  0.8515625
train loss:  0.3159061074256897
train gradient:  0.15600423077837108
iteration : 4722
train acc:  0.875
train loss:  0.28344249725341797
train gradient:  0.17289688544434637
iteration : 4723
train acc:  0.84375
train loss:  0.34491047263145447
train gradient:  0.2333506227689522
iteration : 4724
train acc:  0.890625
train loss:  0.2956346571445465
train gradient:  0.21906801613873828
iteration : 4725
train acc:  0.8515625
train loss:  0.3092852234840393
train gradient:  0.18315794344624467
iteration : 4726
train acc:  0.8671875
train loss:  0.3123602271080017
train gradient:  0.2063109739141042
iteration : 4727
train acc:  0.8515625
train loss:  0.37939655780792236
train gradient:  0.27062840739360494
iteration : 4728
train acc:  0.8671875
train loss:  0.29401105642318726
train gradient:  0.20779618057341498
iteration : 4729
train acc:  0.78125
train loss:  0.47785913944244385
train gradient:  0.4355944125394161
iteration : 4730
train acc:  0.890625
train loss:  0.3301572799682617
train gradient:  0.32657043932396185
iteration : 4731
train acc:  0.8828125
train loss:  0.3108123540878296
train gradient:  0.21122209312735502
iteration : 4732
train acc:  0.890625
train loss:  0.3012111783027649
train gradient:  0.17231926123286262
iteration : 4733
train acc:  0.828125
train loss:  0.39665675163269043
train gradient:  0.3013223009744983
iteration : 4734
train acc:  0.8203125
train loss:  0.35852712392807007
train gradient:  0.2823434886853061
iteration : 4735
train acc:  0.84375
train loss:  0.3413037657737732
train gradient:  0.21958548545314366
iteration : 4736
train acc:  0.8828125
train loss:  0.29037314653396606
train gradient:  0.17593067877249358
iteration : 4737
train acc:  0.9140625
train loss:  0.2675395905971527
train gradient:  0.16840527015222906
iteration : 4738
train acc:  0.859375
train loss:  0.3386540412902832
train gradient:  0.19860580423259927
iteration : 4739
train acc:  0.8359375
train loss:  0.39772582054138184
train gradient:  0.23922722198658686
iteration : 4740
train acc:  0.875
train loss:  0.35275980830192566
train gradient:  0.22784072646507253
iteration : 4741
train acc:  0.8671875
train loss:  0.34995269775390625
train gradient:  0.32161752653372117
iteration : 4742
train acc:  0.8515625
train loss:  0.3456868529319763
train gradient:  0.2487220036057603
iteration : 4743
train acc:  0.84375
train loss:  0.36947447061538696
train gradient:  0.2337489486265783
iteration : 4744
train acc:  0.8125
train loss:  0.37215933203697205
train gradient:  0.3166561217680326
iteration : 4745
train acc:  0.8203125
train loss:  0.38352349400520325
train gradient:  0.2847680326486666
iteration : 4746
train acc:  0.8125
train loss:  0.456438273191452
train gradient:  0.39929630191621795
iteration : 4747
train acc:  0.828125
train loss:  0.4131929874420166
train gradient:  0.30603910421116787
iteration : 4748
train acc:  0.84375
train loss:  0.3615885376930237
train gradient:  0.2994517213187165
iteration : 4749
train acc:  0.796875
train loss:  0.38263052701950073
train gradient:  0.28209703093231087
iteration : 4750
train acc:  0.8046875
train loss:  0.4311292767524719
train gradient:  0.455698743904776
iteration : 4751
train acc:  0.84375
train loss:  0.32396960258483887
train gradient:  0.22683328863857527
iteration : 4752
train acc:  0.8203125
train loss:  0.4756408929824829
train gradient:  0.44420603906336276
iteration : 4753
train acc:  0.8671875
train loss:  0.3108159303665161
train gradient:  0.23153159268001716
iteration : 4754
train acc:  0.8046875
train loss:  0.3906542658805847
train gradient:  0.1823104214195332
iteration : 4755
train acc:  0.8046875
train loss:  0.3610684275627136
train gradient:  0.4117125690973761
iteration : 4756
train acc:  0.8125
train loss:  0.3741394877433777
train gradient:  0.2715238449119295
iteration : 4757
train acc:  0.8046875
train loss:  0.4200447201728821
train gradient:  0.3340903494030753
iteration : 4758
train acc:  0.8515625
train loss:  0.35045546293258667
train gradient:  0.4505447896289436
iteration : 4759
train acc:  0.8203125
train loss:  0.39180535078048706
train gradient:  0.2735231477974836
iteration : 4760
train acc:  0.8515625
train loss:  0.3541741371154785
train gradient:  0.39763971312232516
iteration : 4761
train acc:  0.8671875
train loss:  0.35607367753982544
train gradient:  0.3214267641410478
iteration : 4762
train acc:  0.859375
train loss:  0.3298584520816803
train gradient:  0.26018477903073767
iteration : 4763
train acc:  0.828125
train loss:  0.3307112455368042
train gradient:  0.2120020907332698
iteration : 4764
train acc:  0.8125
train loss:  0.372355580329895
train gradient:  0.274837560109317
iteration : 4765
train acc:  0.8984375
train loss:  0.30847084522247314
train gradient:  0.18015230476309363
iteration : 4766
train acc:  0.84375
train loss:  0.3994801640510559
train gradient:  0.28270045891062046
iteration : 4767
train acc:  0.890625
train loss:  0.26693105697631836
train gradient:  0.16308145953333875
iteration : 4768
train acc:  0.859375
train loss:  0.28848737478256226
train gradient:  0.148395998423274
iteration : 4769
train acc:  0.859375
train loss:  0.30782419443130493
train gradient:  0.23009219967947817
iteration : 4770
train acc:  0.875
train loss:  0.3056570589542389
train gradient:  0.17868812338796308
iteration : 4771
train acc:  0.8203125
train loss:  0.391727477312088
train gradient:  0.27593794204905814
iteration : 4772
train acc:  0.8515625
train loss:  0.33723926544189453
train gradient:  0.2698733504556942
iteration : 4773
train acc:  0.859375
train loss:  0.2877345085144043
train gradient:  0.2281463366247632
iteration : 4774
train acc:  0.8125
train loss:  0.35149478912353516
train gradient:  0.18242766449681094
iteration : 4775
train acc:  0.8515625
train loss:  0.3851032853126526
train gradient:  0.37158069330440563
iteration : 4776
train acc:  0.78125
train loss:  0.43220990896224976
train gradient:  0.31905884798317824
iteration : 4777
train acc:  0.8671875
train loss:  0.3019993305206299
train gradient:  0.15476522779260407
iteration : 4778
train acc:  0.90625
train loss:  0.3049299418926239
train gradient:  0.3748808193028618
iteration : 4779
train acc:  0.875
train loss:  0.3589853048324585
train gradient:  0.18959176669100825
iteration : 4780
train acc:  0.890625
train loss:  0.3218144178390503
train gradient:  0.20881218130670448
iteration : 4781
train acc:  0.859375
train loss:  0.30514082312583923
train gradient:  0.18851527114689506
iteration : 4782
train acc:  0.8984375
train loss:  0.2795553207397461
train gradient:  0.21278393351789904
iteration : 4783
train acc:  0.84375
train loss:  0.3342415690422058
train gradient:  0.21528588647572844
iteration : 4784
train acc:  0.8125
train loss:  0.42567551136016846
train gradient:  0.45485131077957636
iteration : 4785
train acc:  0.890625
train loss:  0.3262261748313904
train gradient:  0.23240772159217618
iteration : 4786
train acc:  0.890625
train loss:  0.2655996084213257
train gradient:  0.14740337781983837
iteration : 4787
train acc:  0.8671875
train loss:  0.3215804100036621
train gradient:  0.2382689944068848
iteration : 4788
train acc:  0.859375
train loss:  0.3267376720905304
train gradient:  0.16605897335648334
iteration : 4789
train acc:  0.859375
train loss:  0.3244854807853699
train gradient:  0.2272092610048701
iteration : 4790
train acc:  0.8046875
train loss:  0.4345239996910095
train gradient:  0.3784818929186895
iteration : 4791
train acc:  0.859375
train loss:  0.37556982040405273
train gradient:  0.36246485601755635
iteration : 4792
train acc:  0.78125
train loss:  0.4609803557395935
train gradient:  0.5970920993488984
iteration : 4793
train acc:  0.859375
train loss:  0.3482275605201721
train gradient:  0.23627261922759868
iteration : 4794
train acc:  0.8046875
train loss:  0.36850810050964355
train gradient:  0.2472862416979979
iteration : 4795
train acc:  0.859375
train loss:  0.3610648512840271
train gradient:  0.2579455841868333
iteration : 4796
train acc:  0.84375
train loss:  0.33816006779670715
train gradient:  0.18303297309582134
iteration : 4797
train acc:  0.90625
train loss:  0.2972630560398102
train gradient:  0.35148460445256896
iteration : 4798
train acc:  0.8359375
train loss:  0.345096617937088
train gradient:  0.28928518499710065
iteration : 4799
train acc:  0.890625
train loss:  0.3205183744430542
train gradient:  0.35575093546764797
iteration : 4800
train acc:  0.84375
train loss:  0.33748942613601685
train gradient:  0.23151425554431332
iteration : 4801
train acc:  0.8671875
train loss:  0.31183063983917236
train gradient:  0.28695930974263795
iteration : 4802
train acc:  0.8515625
train loss:  0.317648321390152
train gradient:  0.19613466468042823
iteration : 4803
train acc:  0.7890625
train loss:  0.4590468406677246
train gradient:  0.37117201273716066
iteration : 4804
train acc:  0.8046875
train loss:  0.38211309909820557
train gradient:  0.2927694417180448
iteration : 4805
train acc:  0.78125
train loss:  0.4282263517379761
train gradient:  0.4127555503680588
iteration : 4806
train acc:  0.8671875
train loss:  0.33690696954727173
train gradient:  0.24893754614372204
iteration : 4807
train acc:  0.84375
train loss:  0.36639416217803955
train gradient:  0.23889371764666717
iteration : 4808
train acc:  0.8515625
train loss:  0.36208534240722656
train gradient:  0.24444675380640418
iteration : 4809
train acc:  0.8984375
train loss:  0.29909324645996094
train gradient:  0.19552235007981544
iteration : 4810
train acc:  0.8671875
train loss:  0.29592493176460266
train gradient:  0.22249752496742126
iteration : 4811
train acc:  0.8125
train loss:  0.36561450362205505
train gradient:  0.3357173531523273
iteration : 4812
train acc:  0.8515625
train loss:  0.35660624504089355
train gradient:  0.21660049770284306
iteration : 4813
train acc:  0.7890625
train loss:  0.4249623119831085
train gradient:  0.2443348914756424
iteration : 4814
train acc:  0.8046875
train loss:  0.41934046149253845
train gradient:  0.28545037786202593
iteration : 4815
train acc:  0.8671875
train loss:  0.2988811433315277
train gradient:  0.18007670712213553
iteration : 4816
train acc:  0.8671875
train loss:  0.2958518862724304
train gradient:  0.23506175630087234
iteration : 4817
train acc:  0.8125
train loss:  0.40300634503364563
train gradient:  0.3426071443855543
iteration : 4818
train acc:  0.8671875
train loss:  0.2771218419075012
train gradient:  0.21714709158858497
iteration : 4819
train acc:  0.8515625
train loss:  0.36220306158065796
train gradient:  0.239368496061602
iteration : 4820
train acc:  0.890625
train loss:  0.31260138750076294
train gradient:  0.21330637231920485
iteration : 4821
train acc:  0.875
train loss:  0.2790790796279907
train gradient:  0.2034813655219465
iteration : 4822
train acc:  0.8671875
train loss:  0.32826218008995056
train gradient:  0.32644917359716913
iteration : 4823
train acc:  0.8828125
train loss:  0.3425425589084625
train gradient:  0.18801091072559795
iteration : 4824
train acc:  0.8046875
train loss:  0.44608673453330994
train gradient:  0.3312118002360092
iteration : 4825
train acc:  0.84375
train loss:  0.38830918073654175
train gradient:  0.3312094562625019
iteration : 4826
train acc:  0.8359375
train loss:  0.3762296438217163
train gradient:  0.2975777784510326
iteration : 4827
train acc:  0.90625
train loss:  0.2827138900756836
train gradient:  0.1903889034857635
iteration : 4828
train acc:  0.8671875
train loss:  0.33753612637519836
train gradient:  0.2668592224932949
iteration : 4829
train acc:  0.8515625
train loss:  0.37141117453575134
train gradient:  0.2892099581970608
iteration : 4830
train acc:  0.8359375
train loss:  0.39653539657592773
train gradient:  0.37133228611324004
iteration : 4831
train acc:  0.8046875
train loss:  0.38528475165367126
train gradient:  0.26676539122409926
iteration : 4832
train acc:  0.84375
train loss:  0.38887500762939453
train gradient:  0.449429724478476
iteration : 4833
train acc:  0.8515625
train loss:  0.3667178750038147
train gradient:  0.281521607436744
iteration : 4834
train acc:  0.796875
train loss:  0.38865363597869873
train gradient:  0.2534830694823318
iteration : 4835
train acc:  0.8984375
train loss:  0.29725152254104614
train gradient:  0.1801943466875363
iteration : 4836
train acc:  0.828125
train loss:  0.3545141816139221
train gradient:  0.28534420912675473
iteration : 4837
train acc:  0.8984375
train loss:  0.30010199546813965
train gradient:  0.2291830722161764
iteration : 4838
train acc:  0.8203125
train loss:  0.3693540096282959
train gradient:  0.245155377547719
iteration : 4839
train acc:  0.8125
train loss:  0.4428701400756836
train gradient:  0.4096825037374691
iteration : 4840
train acc:  0.90625
train loss:  0.24053940176963806
train gradient:  0.1411265045758373
iteration : 4841
train acc:  0.828125
train loss:  0.3329789638519287
train gradient:  0.2623846204724776
iteration : 4842
train acc:  0.828125
train loss:  0.38722601532936096
train gradient:  0.437451162612729
iteration : 4843
train acc:  0.84375
train loss:  0.3510434925556183
train gradient:  0.22430806608387288
iteration : 4844
train acc:  0.8515625
train loss:  0.3169529438018799
train gradient:  0.17436536130780894
iteration : 4845
train acc:  0.84375
train loss:  0.46769845485687256
train gradient:  0.26135923051164145
iteration : 4846
train acc:  0.8359375
train loss:  0.3240540027618408
train gradient:  0.2190325482856704
iteration : 4847
train acc:  0.8203125
train loss:  0.3680337965488434
train gradient:  0.25959226674912345
iteration : 4848
train acc:  0.796875
train loss:  0.39849063754081726
train gradient:  0.3476299994330076
iteration : 4849
train acc:  0.8203125
train loss:  0.3802875876426697
train gradient:  0.35904966327724974
iteration : 4850
train acc:  0.8046875
train loss:  0.3590027093887329
train gradient:  0.2991116399651881
iteration : 4851
train acc:  0.84375
train loss:  0.3235885500907898
train gradient:  0.16609518866898385
iteration : 4852
train acc:  0.78125
train loss:  0.4331168532371521
train gradient:  0.3078778875892498
iteration : 4853
train acc:  0.8671875
train loss:  0.32071930170059204
train gradient:  0.23595460102231935
iteration : 4854
train acc:  0.8515625
train loss:  0.33895033597946167
train gradient:  0.2301905627958623
iteration : 4855
train acc:  0.8671875
train loss:  0.3258190155029297
train gradient:  0.23288313500831312
iteration : 4856
train acc:  0.8203125
train loss:  0.39091014862060547
train gradient:  0.2523483224092751
iteration : 4857
train acc:  0.859375
train loss:  0.3392622768878937
train gradient:  0.2716736343206374
iteration : 4858
train acc:  0.8671875
train loss:  0.34262409806251526
train gradient:  0.26945263764746563
iteration : 4859
train acc:  0.8046875
train loss:  0.3969789147377014
train gradient:  0.29603805312494
iteration : 4860
train acc:  0.84375
train loss:  0.351988285779953
train gradient:  0.28846663888618124
iteration : 4861
train acc:  0.8359375
train loss:  0.3861342668533325
train gradient:  0.3090051724288496
iteration : 4862
train acc:  0.796875
train loss:  0.40582162141799927
train gradient:  0.40288362921839405
iteration : 4863
train acc:  0.8125
train loss:  0.4571666717529297
train gradient:  0.40511571557484993
iteration : 4864
train acc:  0.8515625
train loss:  0.34263336658477783
train gradient:  0.2992168988242341
iteration : 4865
train acc:  0.828125
train loss:  0.3933864235877991
train gradient:  0.35879391626810997
iteration : 4866
train acc:  0.890625
train loss:  0.2977793216705322
train gradient:  0.20382689020254258
iteration : 4867
train acc:  0.8515625
train loss:  0.3472743630409241
train gradient:  0.20415515437844625
iteration : 4868
train acc:  0.8359375
train loss:  0.36692094802856445
train gradient:  0.3753527486622367
iteration : 4869
train acc:  0.8515625
train loss:  0.38607722520828247
train gradient:  0.28819710805090065
iteration : 4870
train acc:  0.875
train loss:  0.2975751757621765
train gradient:  0.2081210042601414
iteration : 4871
train acc:  0.8828125
train loss:  0.27142056822776794
train gradient:  0.1903238107291918
iteration : 4872
train acc:  0.8671875
train loss:  0.33025065064430237
train gradient:  0.20778422124859203
iteration : 4873
train acc:  0.8359375
train loss:  0.3340262770652771
train gradient:  0.25983680733831666
iteration : 4874
train acc:  0.8671875
train loss:  0.325427383184433
train gradient:  0.2690362708284184
iteration : 4875
train acc:  0.8359375
train loss:  0.405529260635376
train gradient:  0.24446776681291899
iteration : 4876
train acc:  0.84375
train loss:  0.36949706077575684
train gradient:  0.2584750998081692
iteration : 4877
train acc:  0.8359375
train loss:  0.3266049027442932
train gradient:  0.21219610399029537
iteration : 4878
train acc:  0.8359375
train loss:  0.35782718658447266
train gradient:  0.2188004787104499
iteration : 4879
train acc:  0.84375
train loss:  0.3052397072315216
train gradient:  0.23715203803068713
iteration : 4880
train acc:  0.8203125
train loss:  0.39127954840660095
train gradient:  0.24109461633397547
iteration : 4881
train acc:  0.8125
train loss:  0.41392672061920166
train gradient:  0.3422178072075674
iteration : 4882
train acc:  0.859375
train loss:  0.350445955991745
train gradient:  0.2742548265990566
iteration : 4883
train acc:  0.859375
train loss:  0.39435821771621704
train gradient:  0.2847375118526112
iteration : 4884
train acc:  0.8828125
train loss:  0.26153111457824707
train gradient:  0.16156635064869623
iteration : 4885
train acc:  0.859375
train loss:  0.36478909850120544
train gradient:  0.33223997635974234
iteration : 4886
train acc:  0.8125
train loss:  0.37806981801986694
train gradient:  0.3489003860384763
iteration : 4887
train acc:  0.828125
train loss:  0.3861110508441925
train gradient:  0.2655917114891489
iteration : 4888
train acc:  0.8046875
train loss:  0.379058837890625
train gradient:  0.3808165384875152
iteration : 4889
train acc:  0.8515625
train loss:  0.3586282730102539
train gradient:  0.21908959941783984
iteration : 4890
train acc:  0.8828125
train loss:  0.3195686638355255
train gradient:  0.3206051066555842
iteration : 4891
train acc:  0.8671875
train loss:  0.3179047107696533
train gradient:  0.24489003179755187
iteration : 4892
train acc:  0.8671875
train loss:  0.33599042892456055
train gradient:  0.1977015576122802
iteration : 4893
train acc:  0.875
train loss:  0.29999953508377075
train gradient:  0.18574086708362722
iteration : 4894
train acc:  0.8359375
train loss:  0.40192681550979614
train gradient:  0.40156034434290033
iteration : 4895
train acc:  0.78125
train loss:  0.4030517339706421
train gradient:  0.2970774084868532
iteration : 4896
train acc:  0.8671875
train loss:  0.36876070499420166
train gradient:  0.2869255468328165
iteration : 4897
train acc:  0.9140625
train loss:  0.278753399848938
train gradient:  0.21167275876672056
iteration : 4898
train acc:  0.8359375
train loss:  0.323800265789032
train gradient:  0.2062610586209746
iteration : 4899
train acc:  0.84375
train loss:  0.3182867467403412
train gradient:  0.19588889986141725
iteration : 4900
train acc:  0.875
train loss:  0.2929568290710449
train gradient:  0.1554552694904987
iteration : 4901
train acc:  0.8671875
train loss:  0.2760002613067627
train gradient:  0.1951782790136696
iteration : 4902
train acc:  0.8671875
train loss:  0.34024789929389954
train gradient:  0.3145374446091653
iteration : 4903
train acc:  0.875
train loss:  0.3337298631668091
train gradient:  0.19150325642279656
iteration : 4904
train acc:  0.8046875
train loss:  0.41841816902160645
train gradient:  0.31508015466344186
iteration : 4905
train acc:  0.8828125
train loss:  0.3121218681335449
train gradient:  0.22542343012434304
iteration : 4906
train acc:  0.875
train loss:  0.3556177020072937
train gradient:  0.30451062558949055
iteration : 4907
train acc:  0.8046875
train loss:  0.365224689245224
train gradient:  0.22344264802665637
iteration : 4908
train acc:  0.8203125
train loss:  0.3689534664154053
train gradient:  0.2721640728299508
iteration : 4909
train acc:  0.84375
train loss:  0.3277367055416107
train gradient:  0.19852694465865836
iteration : 4910
train acc:  0.8515625
train loss:  0.3111017048358917
train gradient:  0.2171595356031441
iteration : 4911
train acc:  0.890625
train loss:  0.336462140083313
train gradient:  0.20642510296549244
iteration : 4912
train acc:  0.859375
train loss:  0.4089415669441223
train gradient:  0.2920390732622363
iteration : 4913
train acc:  0.8515625
train loss:  0.36090385913848877
train gradient:  0.2993680698051816
iteration : 4914
train acc:  0.7890625
train loss:  0.4109070301055908
train gradient:  0.2816288542087068
iteration : 4915
train acc:  0.8515625
train loss:  0.3221568465232849
train gradient:  0.22446081202478085
iteration : 4916
train acc:  0.859375
train loss:  0.3235588073730469
train gradient:  0.2383090983527461
iteration : 4917
train acc:  0.859375
train loss:  0.30093181133270264
train gradient:  0.23760699374183225
iteration : 4918
train acc:  0.8515625
train loss:  0.3784630298614502
train gradient:  0.3303788065815718
iteration : 4919
train acc:  0.8515625
train loss:  0.4074563980102539
train gradient:  0.3179271216349741
iteration : 4920
train acc:  0.875
train loss:  0.34115809202194214
train gradient:  0.3618008538839379
iteration : 4921
train acc:  0.875
train loss:  0.3689073920249939
train gradient:  0.28159240637352384
iteration : 4922
train acc:  0.8359375
train loss:  0.4026249647140503
train gradient:  0.3586628200523266
iteration : 4923
train acc:  0.859375
train loss:  0.33659273386001587
train gradient:  0.1872229673291968
iteration : 4924
train acc:  0.8359375
train loss:  0.35497337579727173
train gradient:  0.3490836163011228
iteration : 4925
train acc:  0.84375
train loss:  0.34654587507247925
train gradient:  0.2401942416096734
iteration : 4926
train acc:  0.8515625
train loss:  0.3983280062675476
train gradient:  0.2952958819810101
iteration : 4927
train acc:  0.875
train loss:  0.2651047706604004
train gradient:  0.2033047730816484
iteration : 4928
train acc:  0.765625
train loss:  0.4247761070728302
train gradient:  0.3585340112653033
iteration : 4929
train acc:  0.8359375
train loss:  0.3896978497505188
train gradient:  0.2850046003603487
iteration : 4930
train acc:  0.8828125
train loss:  0.39233875274658203
train gradient:  0.2591119351884146
iteration : 4931
train acc:  0.8828125
train loss:  0.3344976007938385
train gradient:  0.2913875836799813
iteration : 4932
train acc:  0.8203125
train loss:  0.4006539285182953
train gradient:  0.3665298603557847
iteration : 4933
train acc:  0.828125
train loss:  0.3798172175884247
train gradient:  0.26330235389908635
iteration : 4934
train acc:  0.859375
train loss:  0.3774453401565552
train gradient:  0.2782266548894327
iteration : 4935
train acc:  0.8125
train loss:  0.38998591899871826
train gradient:  0.3630962996460774
iteration : 4936
train acc:  0.7890625
train loss:  0.4819234311580658
train gradient:  0.3524581386542272
iteration : 4937
train acc:  0.8203125
train loss:  0.33574146032333374
train gradient:  0.23150868708705874
iteration : 4938
train acc:  0.859375
train loss:  0.29599446058273315
train gradient:  0.14187794216594501
iteration : 4939
train acc:  0.828125
train loss:  0.36631056666374207
train gradient:  0.34155698999514583
iteration : 4940
train acc:  0.859375
train loss:  0.32082903385162354
train gradient:  0.21701853147533545
iteration : 4941
train acc:  0.8359375
train loss:  0.37272512912750244
train gradient:  0.3946098859065055
iteration : 4942
train acc:  0.8515625
train loss:  0.37252020835876465
train gradient:  0.23345532292240231
iteration : 4943
train acc:  0.8046875
train loss:  0.41331747174263
train gradient:  0.359227648006515
iteration : 4944
train acc:  0.8828125
train loss:  0.2910047769546509
train gradient:  0.16744999019394785
iteration : 4945
train acc:  0.859375
train loss:  0.3400234580039978
train gradient:  0.32107593911463606
iteration : 4946
train acc:  0.8046875
train loss:  0.35014647245407104
train gradient:  0.2769830043591249
iteration : 4947
train acc:  0.84375
train loss:  0.3845422863960266
train gradient:  0.2246692955883109
iteration : 4948
train acc:  0.8671875
train loss:  0.2846643924713135
train gradient:  0.20816066501854547
iteration : 4949
train acc:  0.859375
train loss:  0.3463100492954254
train gradient:  0.3215443987038288
iteration : 4950
train acc:  0.8046875
train loss:  0.3820585608482361
train gradient:  0.21827670846382632
iteration : 4951
train acc:  0.8515625
train loss:  0.30359891057014465
train gradient:  0.16978910093774274
iteration : 4952
train acc:  0.859375
train loss:  0.3455507755279541
train gradient:  0.19764129496029192
iteration : 4953
train acc:  0.8671875
train loss:  0.29228344559669495
train gradient:  0.18778313554747517
iteration : 4954
train acc:  0.8125
train loss:  0.377916544675827
train gradient:  0.2913788086401534
iteration : 4955
train acc:  0.8359375
train loss:  0.39545708894729614
train gradient:  0.27507592662801233
iteration : 4956
train acc:  0.890625
train loss:  0.3173942565917969
train gradient:  0.3249683048522537
iteration : 4957
train acc:  0.890625
train loss:  0.2977273464202881
train gradient:  0.17571114133856658
iteration : 4958
train acc:  0.75
train loss:  0.499314546585083
train gradient:  0.6487731705529687
iteration : 4959
train acc:  0.8203125
train loss:  0.4195747971534729
train gradient:  0.28781970480390856
iteration : 4960
train acc:  0.7890625
train loss:  0.4121093153953552
train gradient:  0.352113662061928
iteration : 4961
train acc:  0.84375
train loss:  0.35882899165153503
train gradient:  0.30364314275176624
iteration : 4962
train acc:  0.828125
train loss:  0.4029773473739624
train gradient:  0.2573034440494345
iteration : 4963
train acc:  0.8359375
train loss:  0.3417297899723053
train gradient:  0.29558302587083857
iteration : 4964
train acc:  0.8671875
train loss:  0.2911999821662903
train gradient:  0.2543271145621062
iteration : 4965
train acc:  0.875
train loss:  0.3754432201385498
train gradient:  0.2177737193394808
iteration : 4966
train acc:  0.8125
train loss:  0.41289493441581726
train gradient:  0.2286823565587459
iteration : 4967
train acc:  0.859375
train loss:  0.33331355452537537
train gradient:  0.1986621484851922
iteration : 4968
train acc:  0.828125
train loss:  0.3493387699127197
train gradient:  0.21246558354313316
iteration : 4969
train acc:  0.796875
train loss:  0.3608386218547821
train gradient:  0.20989585388961773
iteration : 4970
train acc:  0.8359375
train loss:  0.3177502453327179
train gradient:  0.19154516144229067
iteration : 4971
train acc:  0.859375
train loss:  0.32369768619537354
train gradient:  0.21211694586465554
iteration : 4972
train acc:  0.8046875
train loss:  0.4334124028682709
train gradient:  0.28015633619897923
iteration : 4973
train acc:  0.8359375
train loss:  0.3115135729312897
train gradient:  0.14542322097268207
iteration : 4974
train acc:  0.796875
train loss:  0.39938920736312866
train gradient:  0.2665126724656337
iteration : 4975
train acc:  0.859375
train loss:  0.33764001727104187
train gradient:  0.2756489922568257
iteration : 4976
train acc:  0.8671875
train loss:  0.28638482093811035
train gradient:  0.2164120241876285
iteration : 4977
train acc:  0.8984375
train loss:  0.3195812404155731
train gradient:  0.7232296413287005
iteration : 4978
train acc:  0.7734375
train loss:  0.4261869788169861
train gradient:  0.4085199803833477
iteration : 4979
train acc:  0.8359375
train loss:  0.3630548119544983
train gradient:  0.17866823352371405
iteration : 4980
train acc:  0.8125
train loss:  0.35336026549339294
train gradient:  0.2607119445680143
iteration : 4981
train acc:  0.828125
train loss:  0.349432110786438
train gradient:  0.16164091104377754
iteration : 4982
train acc:  0.796875
train loss:  0.46355965733528137
train gradient:  0.4374147946689143
iteration : 4983
train acc:  0.828125
train loss:  0.3593965172767639
train gradient:  0.23794831207440167
iteration : 4984
train acc:  0.875
train loss:  0.2973778247833252
train gradient:  0.21546631814133854
iteration : 4985
train acc:  0.8984375
train loss:  0.26455438137054443
train gradient:  0.2139474239478524
iteration : 4986
train acc:  0.8515625
train loss:  0.3141815662384033
train gradient:  0.23112522230599752
iteration : 4987
train acc:  0.84375
train loss:  0.3108974099159241
train gradient:  0.18643412601631243
iteration : 4988
train acc:  0.84375
train loss:  0.3976620137691498
train gradient:  0.26180039679824174
iteration : 4989
train acc:  0.8125
train loss:  0.40771594643592834
train gradient:  0.2660964957312336
iteration : 4990
train acc:  0.8515625
train loss:  0.31168606877326965
train gradient:  0.1602741958204098
iteration : 4991
train acc:  0.875
train loss:  0.350170761346817
train gradient:  0.41942153518195247
iteration : 4992
train acc:  0.8671875
train loss:  0.3459879159927368
train gradient:  0.21120918539221714
iteration : 4993
train acc:  0.796875
train loss:  0.4630178213119507
train gradient:  0.3632331962969592
iteration : 4994
train acc:  0.875
train loss:  0.3278309404850006
train gradient:  0.2417548728056354
iteration : 4995
train acc:  0.8046875
train loss:  0.34238454699516296
train gradient:  0.21249364665885728
iteration : 4996
train acc:  0.859375
train loss:  0.3304113745689392
train gradient:  0.2005734074253271
iteration : 4997
train acc:  0.8203125
train loss:  0.3590732216835022
train gradient:  0.2217884445473759
iteration : 4998
train acc:  0.8671875
train loss:  0.31152093410491943
train gradient:  0.19299797644903124
iteration : 4999
train acc:  0.8125
train loss:  0.3728887736797333
train gradient:  0.1834648196715936
iteration : 5000
train acc:  0.8125
train loss:  0.4763062298297882
train gradient:  0.31052898510220084
iteration : 5001
train acc:  0.8515625
train loss:  0.4277966320514679
train gradient:  0.38657250932852355
iteration : 5002
train acc:  0.8203125
train loss:  0.45288628339767456
train gradient:  0.49001789530510437
iteration : 5003
train acc:  0.8515625
train loss:  0.31124669313430786
train gradient:  0.19816539360060478
iteration : 5004
train acc:  0.84375
train loss:  0.3754799962043762
train gradient:  0.3131618065492562
iteration : 5005
train acc:  0.8984375
train loss:  0.2841227054595947
train gradient:  0.20763756125030558
iteration : 5006
train acc:  0.84375
train loss:  0.36044204235076904
train gradient:  0.2786632830844135
iteration : 5007
train acc:  0.7890625
train loss:  0.44747236371040344
train gradient:  0.2960676200595109
iteration : 5008
train acc:  0.875
train loss:  0.34920012950897217
train gradient:  0.3370611155968336
iteration : 5009
train acc:  0.8671875
train loss:  0.4023170471191406
train gradient:  0.2949451739882742
iteration : 5010
train acc:  0.84375
train loss:  0.34657764434814453
train gradient:  0.17053788364723105
iteration : 5011
train acc:  0.8203125
train loss:  0.46588364243507385
train gradient:  0.35914499380938153
iteration : 5012
train acc:  0.8671875
train loss:  0.31982535123825073
train gradient:  0.16778129865571717
iteration : 5013
train acc:  0.875
train loss:  0.28346097469329834
train gradient:  0.167607671422486
iteration : 5014
train acc:  0.8671875
train loss:  0.3304334878921509
train gradient:  0.16270607963307535
iteration : 5015
train acc:  0.8359375
train loss:  0.38970285654067993
train gradient:  0.3037796890136763
iteration : 5016
train acc:  0.875
train loss:  0.3160630464553833
train gradient:  0.2323257505951548
iteration : 5017
train acc:  0.8515625
train loss:  0.3155060112476349
train gradient:  0.17646368641157273
iteration : 5018
train acc:  0.8828125
train loss:  0.30338263511657715
train gradient:  0.2088454462820895
iteration : 5019
train acc:  0.890625
train loss:  0.27681171894073486
train gradient:  0.18036902840827218
iteration : 5020
train acc:  0.8671875
train loss:  0.26667624711990356
train gradient:  0.21242660412421366
iteration : 5021
train acc:  0.8671875
train loss:  0.30566972494125366
train gradient:  0.23677339524899305
iteration : 5022
train acc:  0.8359375
train loss:  0.34254634380340576
train gradient:  0.22310472195995718
iteration : 5023
train acc:  0.8671875
train loss:  0.3126383125782013
train gradient:  0.18986556456991813
iteration : 5024
train acc:  0.828125
train loss:  0.3288695216178894
train gradient:  0.26411131108291463
iteration : 5025
train acc:  0.8203125
train loss:  0.4128296673297882
train gradient:  0.3770445510333924
iteration : 5026
train acc:  0.8359375
train loss:  0.42333757877349854
train gradient:  0.37671763465794383
iteration : 5027
train acc:  0.8203125
train loss:  0.37188881635665894
train gradient:  0.2510288619030018
iteration : 5028
train acc:  0.8203125
train loss:  0.4157881438732147
train gradient:  0.4061717528091218
iteration : 5029
train acc:  0.828125
train loss:  0.41043221950531006
train gradient:  0.415180113015622
iteration : 5030
train acc:  0.796875
train loss:  0.4265289306640625
train gradient:  0.41670164966880086
iteration : 5031
train acc:  0.8359375
train loss:  0.4067919850349426
train gradient:  0.32619162674588753
iteration : 5032
train acc:  0.859375
train loss:  0.3234224319458008
train gradient:  0.21812728114170676
iteration : 5033
train acc:  0.84375
train loss:  0.32372963428497314
train gradient:  0.23956340914833
iteration : 5034
train acc:  0.8203125
train loss:  0.3507992923259735
train gradient:  0.24775641661344655
iteration : 5035
train acc:  0.828125
train loss:  0.37465423345565796
train gradient:  0.28434516089828904
iteration : 5036
train acc:  0.78125
train loss:  0.4222041368484497
train gradient:  0.2444921396864527
iteration : 5037
train acc:  0.9140625
train loss:  0.2747587561607361
train gradient:  0.1512779290917486
iteration : 5038
train acc:  0.828125
train loss:  0.39554715156555176
train gradient:  0.24251754362202677
iteration : 5039
train acc:  0.859375
train loss:  0.3411576747894287
train gradient:  0.26676013226173356
iteration : 5040
train acc:  0.8359375
train loss:  0.3392758369445801
train gradient:  0.22548060821442123
iteration : 5041
train acc:  0.828125
train loss:  0.35484278202056885
train gradient:  0.29805337089587597
iteration : 5042
train acc:  0.8515625
train loss:  0.30115365982055664
train gradient:  0.16256269546889562
iteration : 5043
train acc:  0.8203125
train loss:  0.3770032525062561
train gradient:  0.23061958113737213
iteration : 5044
train acc:  0.8671875
train loss:  0.31745219230651855
train gradient:  0.2981339451444547
iteration : 5045
train acc:  0.875
train loss:  0.3142232894897461
train gradient:  0.23092183701294236
iteration : 5046
train acc:  0.8359375
train loss:  0.42780089378356934
train gradient:  0.31456050237527416
iteration : 5047
train acc:  0.828125
train loss:  0.3567716181278229
train gradient:  0.2249940462216174
iteration : 5048
train acc:  0.875
train loss:  0.34367603063583374
train gradient:  0.20147150412199788
iteration : 5049
train acc:  0.828125
train loss:  0.3710455894470215
train gradient:  0.28970920540207945
iteration : 5050
train acc:  0.828125
train loss:  0.3981926143169403
train gradient:  0.31556501309745033
iteration : 5051
train acc:  0.8359375
train loss:  0.34902864694595337
train gradient:  0.1800241379966631
iteration : 5052
train acc:  0.8515625
train loss:  0.3479321599006653
train gradient:  0.21911471375488173
iteration : 5053
train acc:  0.859375
train loss:  0.3469698131084442
train gradient:  0.42060311003242973
iteration : 5054
train acc:  0.84375
train loss:  0.33665788173675537
train gradient:  0.36061704327543226
iteration : 5055
train acc:  0.78125
train loss:  0.46893855929374695
train gradient:  0.3302620940115427
iteration : 5056
train acc:  0.765625
train loss:  0.4137328565120697
train gradient:  0.31282683762306024
iteration : 5057
train acc:  0.8671875
train loss:  0.35353153944015503
train gradient:  0.22121193999384792
iteration : 5058
train acc:  0.8515625
train loss:  0.34162667393684387
train gradient:  0.241938515023288
iteration : 5059
train acc:  0.8125
train loss:  0.37838590145111084
train gradient:  0.24025310467447955
iteration : 5060
train acc:  0.8359375
train loss:  0.41372841596603394
train gradient:  0.30297760827012793
iteration : 5061
train acc:  0.859375
train loss:  0.3559889495372772
train gradient:  0.23314689213051837
iteration : 5062
train acc:  0.8125
train loss:  0.36826738715171814
train gradient:  0.2303087414604689
iteration : 5063
train acc:  0.921875
train loss:  0.278637170791626
train gradient:  0.19356826552927647
iteration : 5064
train acc:  0.8359375
train loss:  0.3491695523262024
train gradient:  0.29839063340652683
iteration : 5065
train acc:  0.890625
train loss:  0.3340524435043335
train gradient:  0.2296865701073481
iteration : 5066
train acc:  0.875
train loss:  0.37338173389434814
train gradient:  0.28964931916885345
iteration : 5067
train acc:  0.8515625
train loss:  0.3828774094581604
train gradient:  0.432780054420566
iteration : 5068
train acc:  0.8984375
train loss:  0.30039140582084656
train gradient:  0.15143422661774125
iteration : 5069
train acc:  0.8515625
train loss:  0.35580530762672424
train gradient:  0.25378876677555584
iteration : 5070
train acc:  0.8515625
train loss:  0.3087916076183319
train gradient:  0.22355466432486104
iteration : 5071
train acc:  0.890625
train loss:  0.28438904881477356
train gradient:  0.19095669393072556
iteration : 5072
train acc:  0.78125
train loss:  0.45173466205596924
train gradient:  0.31046469446896474
iteration : 5073
train acc:  0.8828125
train loss:  0.2868751287460327
train gradient:  0.21955797482332418
iteration : 5074
train acc:  0.84375
train loss:  0.3819408416748047
train gradient:  0.18599502365557424
iteration : 5075
train acc:  0.8359375
train loss:  0.3817412257194519
train gradient:  0.41688773925661576
iteration : 5076
train acc:  0.84375
train loss:  0.3318602740764618
train gradient:  0.24893323951221497
iteration : 5077
train acc:  0.8828125
train loss:  0.2707067131996155
train gradient:  0.2611860866298018
iteration : 5078
train acc:  0.7890625
train loss:  0.42508465051651
train gradient:  0.49752544609333416
iteration : 5079
train acc:  0.828125
train loss:  0.31874629855155945
train gradient:  0.2052781039629894
iteration : 5080
train acc:  0.7890625
train loss:  0.38381603360176086
train gradient:  0.23460440747468161
iteration : 5081
train acc:  0.859375
train loss:  0.3109307289123535
train gradient:  0.30334455754405326
iteration : 5082
train acc:  0.8203125
train loss:  0.4119921922683716
train gradient:  0.3103859994173309
iteration : 5083
train acc:  0.828125
train loss:  0.39342841506004333
train gradient:  0.3245892522745882
iteration : 5084
train acc:  0.859375
train loss:  0.3535219430923462
train gradient:  0.2848403709010321
iteration : 5085
train acc:  0.8359375
train loss:  0.35217946767807007
train gradient:  0.34100751515864325
iteration : 5086
train acc:  0.828125
train loss:  0.43225568532943726
train gradient:  0.289932767083872
iteration : 5087
train acc:  0.796875
train loss:  0.4245380759239197
train gradient:  0.3601667054348733
iteration : 5088
train acc:  0.8046875
train loss:  0.38415759801864624
train gradient:  0.2357894906137603
iteration : 5089
train acc:  0.8359375
train loss:  0.3790236711502075
train gradient:  0.3190201529659264
iteration : 5090
train acc:  0.84375
train loss:  0.3414536118507385
train gradient:  0.19050438762141925
iteration : 5091
train acc:  0.8515625
train loss:  0.38645559549331665
train gradient:  0.269012630031641
iteration : 5092
train acc:  0.8125
train loss:  0.3803364932537079
train gradient:  0.2669804269417923
iteration : 5093
train acc:  0.8125
train loss:  0.3322764039039612
train gradient:  0.20752189819075803
iteration : 5094
train acc:  0.8203125
train loss:  0.4250434339046478
train gradient:  0.5271552527124655
iteration : 5095
train acc:  0.8671875
train loss:  0.3444058299064636
train gradient:  0.24531824769963492
iteration : 5096
train acc:  0.84375
train loss:  0.328050434589386
train gradient:  0.1678915736153755
iteration : 5097
train acc:  0.8515625
train loss:  0.333675742149353
train gradient:  0.2379080668052655
iteration : 5098
train acc:  0.859375
train loss:  0.35336995124816895
train gradient:  0.20424232895902197
iteration : 5099
train acc:  0.859375
train loss:  0.3411582112312317
train gradient:  0.23379600019965333
iteration : 5100
train acc:  0.8671875
train loss:  0.29327666759490967
train gradient:  0.22744043725364022
iteration : 5101
train acc:  0.859375
train loss:  0.38274216651916504
train gradient:  0.2265188105665908
iteration : 5102
train acc:  0.8515625
train loss:  0.3725004196166992
train gradient:  0.18439412661870952
iteration : 5103
train acc:  0.8828125
train loss:  0.28834104537963867
train gradient:  0.1559986064589878
iteration : 5104
train acc:  0.765625
train loss:  0.45175623893737793
train gradient:  0.36296004051840064
iteration : 5105
train acc:  0.8515625
train loss:  0.32759609818458557
train gradient:  0.22012555276585263
iteration : 5106
train acc:  0.8125
train loss:  0.33881473541259766
train gradient:  0.21775415730979664
iteration : 5107
train acc:  0.8046875
train loss:  0.3801978826522827
train gradient:  0.2899748487190669
iteration : 5108
train acc:  0.859375
train loss:  0.3242011070251465
train gradient:  0.16549479071078518
iteration : 5109
train acc:  0.828125
train loss:  0.3825571537017822
train gradient:  0.23332017291680454
iteration : 5110
train acc:  0.84375
train loss:  0.377524197101593
train gradient:  0.32204574880805026
iteration : 5111
train acc:  0.859375
train loss:  0.3708057999610901
train gradient:  0.2304216854379732
iteration : 5112
train acc:  0.8828125
train loss:  0.289964884519577
train gradient:  0.18497593511637606
iteration : 5113
train acc:  0.84375
train loss:  0.30565145611763
train gradient:  0.16427731069687004
iteration : 5114
train acc:  0.84375
train loss:  0.39757484197616577
train gradient:  0.26697218306640014
iteration : 5115
train acc:  0.8359375
train loss:  0.4013710618019104
train gradient:  0.3065198225430717
iteration : 5116
train acc:  0.8671875
train loss:  0.36293870210647583
train gradient:  0.2890422587034747
iteration : 5117
train acc:  0.8984375
train loss:  0.28527671098709106
train gradient:  0.2094113202669408
iteration : 5118
train acc:  0.8515625
train loss:  0.37803885340690613
train gradient:  0.30616552883212106
iteration : 5119
train acc:  0.828125
train loss:  0.37323087453842163
train gradient:  0.2370654712542733
iteration : 5120
train acc:  0.8359375
train loss:  0.40811261534690857
train gradient:  0.45980313572674714
iteration : 5121
train acc:  0.8359375
train loss:  0.3553329408168793
train gradient:  0.18691204184445442
iteration : 5122
train acc:  0.84375
train loss:  0.3396017551422119
train gradient:  0.21780264302827232
iteration : 5123
train acc:  0.921875
train loss:  0.27907299995422363
train gradient:  0.16354979393404734
iteration : 5124
train acc:  0.890625
train loss:  0.2898772656917572
train gradient:  0.26945631474382026
iteration : 5125
train acc:  0.8515625
train loss:  0.34221112728118896
train gradient:  0.2573635370236645
iteration : 5126
train acc:  0.8359375
train loss:  0.3150426745414734
train gradient:  0.3325240907517556
iteration : 5127
train acc:  0.8671875
train loss:  0.33252209424972534
train gradient:  0.26625543490967624
iteration : 5128
train acc:  0.8515625
train loss:  0.34886541962623596
train gradient:  0.17658378434428246
iteration : 5129
train acc:  0.8046875
train loss:  0.4177446961402893
train gradient:  0.40779883886850143
iteration : 5130
train acc:  0.8671875
train loss:  0.30205056071281433
train gradient:  0.1474830537281579
iteration : 5131
train acc:  0.8046875
train loss:  0.3802800178527832
train gradient:  0.25114252412836113
iteration : 5132
train acc:  0.765625
train loss:  0.42016083002090454
train gradient:  0.3375334487163491
iteration : 5133
train acc:  0.8125
train loss:  0.4016975164413452
train gradient:  0.41281740133868416
iteration : 5134
train acc:  0.828125
train loss:  0.3889428377151489
train gradient:  0.2959433491036386
iteration : 5135
train acc:  0.8203125
train loss:  0.4124913215637207
train gradient:  0.5285340478172647
iteration : 5136
train acc:  0.8046875
train loss:  0.3643656075000763
train gradient:  0.20993394863465126
iteration : 5137
train acc:  0.875
train loss:  0.35299891233444214
train gradient:  0.20171080456809798
iteration : 5138
train acc:  0.8671875
train loss:  0.42187970876693726
train gradient:  0.36562696891216634
iteration : 5139
train acc:  0.8125
train loss:  0.3965667486190796
train gradient:  0.2422540824403041
iteration : 5140
train acc:  0.859375
train loss:  0.2931128144264221
train gradient:  0.19291221712864792
iteration : 5141
train acc:  0.8671875
train loss:  0.30924320220947266
train gradient:  0.16245715588769472
iteration : 5142
train acc:  0.859375
train loss:  0.33219295740127563
train gradient:  0.2660177054532101
iteration : 5143
train acc:  0.8671875
train loss:  0.3462544083595276
train gradient:  0.30601848714311897
iteration : 5144
train acc:  0.90625
train loss:  0.2761703431606293
train gradient:  0.1647422590702924
iteration : 5145
train acc:  0.8203125
train loss:  0.38345199823379517
train gradient:  0.22480646001878743
iteration : 5146
train acc:  0.8125
train loss:  0.39549633860588074
train gradient:  0.24367958604691786
iteration : 5147
train acc:  0.8203125
train loss:  0.3562825918197632
train gradient:  0.3263636012059417
iteration : 5148
train acc:  0.90625
train loss:  0.2501276731491089
train gradient:  0.16468021079050216
iteration : 5149
train acc:  0.8359375
train loss:  0.3120978772640228
train gradient:  0.20067902618007932
iteration : 5150
train acc:  0.8046875
train loss:  0.4066149890422821
train gradient:  0.3416602573637486
iteration : 5151
train acc:  0.8515625
train loss:  0.30624502897262573
train gradient:  0.21288619012641186
iteration : 5152
train acc:  0.8671875
train loss:  0.343803346157074
train gradient:  0.22860914377421404
iteration : 5153
train acc:  0.7890625
train loss:  0.36106815934181213
train gradient:  0.271422916208368
iteration : 5154
train acc:  0.8203125
train loss:  0.40917864441871643
train gradient:  0.27406410155965194
iteration : 5155
train acc:  0.8515625
train loss:  0.35176217555999756
train gradient:  0.4125631519650765
iteration : 5156
train acc:  0.9140625
train loss:  0.25028538703918457
train gradient:  0.19569365042989645
iteration : 5157
train acc:  0.84375
train loss:  0.3765077590942383
train gradient:  0.28780830152148434
iteration : 5158
train acc:  0.8671875
train loss:  0.31182020902633667
train gradient:  0.22842888143531473
iteration : 5159
train acc:  0.90625
train loss:  0.2471054494380951
train gradient:  0.11802725377388193
iteration : 5160
train acc:  0.78125
train loss:  0.40629178285598755
train gradient:  0.3819828612826004
iteration : 5161
train acc:  0.828125
train loss:  0.34089305996894836
train gradient:  0.2698330767576289
iteration : 5162
train acc:  0.8203125
train loss:  0.399389386177063
train gradient:  0.25579045561297525
iteration : 5163
train acc:  0.875
train loss:  0.2844443619251251
train gradient:  0.1752182209138618
iteration : 5164
train acc:  0.765625
train loss:  0.5192461609840393
train gradient:  0.41810330032837156
iteration : 5165
train acc:  0.8203125
train loss:  0.38872814178466797
train gradient:  0.27601456834358223
iteration : 5166
train acc:  0.828125
train loss:  0.379041850566864
train gradient:  0.3469690564301969
iteration : 5167
train acc:  0.78125
train loss:  0.4245857298374176
train gradient:  0.38555165551484816
iteration : 5168
train acc:  0.8125
train loss:  0.45801419019699097
train gradient:  0.4457056604508625
iteration : 5169
train acc:  0.828125
train loss:  0.352145254611969
train gradient:  0.23522157370073543
iteration : 5170
train acc:  0.84375
train loss:  0.3371439576148987
train gradient:  0.2892002018594549
iteration : 5171
train acc:  0.8515625
train loss:  0.39044198393821716
train gradient:  0.27197260251160765
iteration : 5172
train acc:  0.8125
train loss:  0.4313555359840393
train gradient:  0.315172928320922
iteration : 5173
train acc:  0.75
train loss:  0.4464209973812103
train gradient:  0.4866835842355829
iteration : 5174
train acc:  0.859375
train loss:  0.3180901110172272
train gradient:  0.18111376383187927
iteration : 5175
train acc:  0.8671875
train loss:  0.31233611702919006
train gradient:  0.33253390470350946
iteration : 5176
train acc:  0.8203125
train loss:  0.41350120306015015
train gradient:  0.6724040279668895
iteration : 5177
train acc:  0.875
train loss:  0.29355382919311523
train gradient:  0.18233813138634797
iteration : 5178
train acc:  0.8046875
train loss:  0.44624215364456177
train gradient:  0.28838862433852264
iteration : 5179
train acc:  0.8671875
train loss:  0.38141870498657227
train gradient:  0.3828226719511017
iteration : 5180
train acc:  0.875
train loss:  0.31100577116012573
train gradient:  0.1859568678358336
iteration : 5181
train acc:  0.9140625
train loss:  0.28085362911224365
train gradient:  0.2096279728885302
iteration : 5182
train acc:  0.875
train loss:  0.2679332494735718
train gradient:  0.21701749297085166
iteration : 5183
train acc:  0.7734375
train loss:  0.4847925305366516
train gradient:  0.5021822361576542
iteration : 5184
train acc:  0.8359375
train loss:  0.3616622984409332
train gradient:  0.21323829548314444
iteration : 5185
train acc:  0.8671875
train loss:  0.2984284460544586
train gradient:  0.21145875827828386
iteration : 5186
train acc:  0.875
train loss:  0.29843685030937195
train gradient:  0.27469701277661573
iteration : 5187
train acc:  0.859375
train loss:  0.3558233380317688
train gradient:  0.2866926469229748
iteration : 5188
train acc:  0.796875
train loss:  0.44193950295448303
train gradient:  0.34628687022739335
iteration : 5189
train acc:  0.8671875
train loss:  0.32227495312690735
train gradient:  0.2171107372383887
iteration : 5190
train acc:  0.796875
train loss:  0.35428479313850403
train gradient:  0.29711554401715023
iteration : 5191
train acc:  0.8046875
train loss:  0.43572622537612915
train gradient:  0.27373163265197514
iteration : 5192
train acc:  0.859375
train loss:  0.34111592173576355
train gradient:  0.281188337733769
iteration : 5193
train acc:  0.859375
train loss:  0.31151318550109863
train gradient:  0.2208458993809354
iteration : 5194
train acc:  0.8046875
train loss:  0.41818156838417053
train gradient:  0.27601537098705065
iteration : 5195
train acc:  0.8359375
train loss:  0.39490997791290283
train gradient:  0.50997014306146
iteration : 5196
train acc:  0.8515625
train loss:  0.3909291625022888
train gradient:  0.191750489572342
iteration : 5197
train acc:  0.8359375
train loss:  0.35712605714797974
train gradient:  0.2626363482952233
iteration : 5198
train acc:  0.8515625
train loss:  0.35339638590812683
train gradient:  0.18770922271162965
iteration : 5199
train acc:  0.8828125
train loss:  0.3301435708999634
train gradient:  0.23927825362196434
iteration : 5200
train acc:  0.8203125
train loss:  0.38031959533691406
train gradient:  0.2608879001645932
iteration : 5201
train acc:  0.8828125
train loss:  0.3157232403755188
train gradient:  0.19733084602460285
iteration : 5202
train acc:  0.8359375
train loss:  0.332602858543396
train gradient:  0.16522203630628834
iteration : 5203
train acc:  0.8515625
train loss:  0.34677913784980774
train gradient:  0.20997961788766656
iteration : 5204
train acc:  0.828125
train loss:  0.43109142780303955
train gradient:  0.28742045405967465
iteration : 5205
train acc:  0.828125
train loss:  0.37739455699920654
train gradient:  0.1910525980766282
iteration : 5206
train acc:  0.8125
train loss:  0.34590038657188416
train gradient:  0.24645642137779905
iteration : 5207
train acc:  0.8671875
train loss:  0.32145410776138306
train gradient:  0.19121972279143012
iteration : 5208
train acc:  0.890625
train loss:  0.28699952363967896
train gradient:  0.16000676864781418
iteration : 5209
train acc:  0.8203125
train loss:  0.33345913887023926
train gradient:  0.1739868342927619
iteration : 5210
train acc:  0.8515625
train loss:  0.3423558473587036
train gradient:  0.21104116814296933
iteration : 5211
train acc:  0.8515625
train loss:  0.3326232433319092
train gradient:  0.15552709687614635
iteration : 5212
train acc:  0.84375
train loss:  0.3893229365348816
train gradient:  0.24072632815228892
iteration : 5213
train acc:  0.8515625
train loss:  0.3182472288608551
train gradient:  0.21254007138757236
iteration : 5214
train acc:  0.765625
train loss:  0.44136086106300354
train gradient:  0.36963971064772655
iteration : 5215
train acc:  0.8515625
train loss:  0.3006773889064789
train gradient:  0.2982742323006267
iteration : 5216
train acc:  0.8515625
train loss:  0.3659176826477051
train gradient:  0.2490466548075197
iteration : 5217
train acc:  0.84375
train loss:  0.33567583560943604
train gradient:  0.2582433205914601
iteration : 5218
train acc:  0.8671875
train loss:  0.30809009075164795
train gradient:  0.14286619500926506
iteration : 5219
train acc:  0.828125
train loss:  0.41163331270217896
train gradient:  0.2953322899742751
iteration : 5220
train acc:  0.84375
train loss:  0.35436439514160156
train gradient:  0.22677101520970463
iteration : 5221
train acc:  0.8203125
train loss:  0.3753175139427185
train gradient:  0.36200053489899325
iteration : 5222
train acc:  0.8359375
train loss:  0.36632996797561646
train gradient:  0.19857587442062363
iteration : 5223
train acc:  0.828125
train loss:  0.3908083438873291
train gradient:  0.29712804421492833
iteration : 5224
train acc:  0.796875
train loss:  0.37289416790008545
train gradient:  0.5266251236720488
iteration : 5225
train acc:  0.8203125
train loss:  0.44179201126098633
train gradient:  0.36588916575611546
iteration : 5226
train acc:  0.8515625
train loss:  0.4047737419605255
train gradient:  0.3334700880755053
iteration : 5227
train acc:  0.859375
train loss:  0.35153478384017944
train gradient:  0.258823685534783
iteration : 5228
train acc:  0.8515625
train loss:  0.3052222728729248
train gradient:  0.1724153356374304
iteration : 5229
train acc:  0.8671875
train loss:  0.3294159471988678
train gradient:  0.18499866271764617
iteration : 5230
train acc:  0.859375
train loss:  0.3414808511734009
train gradient:  0.2702435418571672
iteration : 5231
train acc:  0.828125
train loss:  0.39367571473121643
train gradient:  0.22632121720645448
iteration : 5232
train acc:  0.828125
train loss:  0.34129953384399414
train gradient:  0.24214771562505533
iteration : 5233
train acc:  0.84375
train loss:  0.3500784635543823
train gradient:  0.2745592543283626
iteration : 5234
train acc:  0.8359375
train loss:  0.44650745391845703
train gradient:  0.3308371934194947
iteration : 5235
train acc:  0.8828125
train loss:  0.30339282751083374
train gradient:  0.20245429538981397
iteration : 5236
train acc:  0.8359375
train loss:  0.3259294033050537
train gradient:  0.156798109445049
iteration : 5237
train acc:  0.8515625
train loss:  0.3464365005493164
train gradient:  0.20895612722408485
iteration : 5238
train acc:  0.7578125
train loss:  0.4846983850002289
train gradient:  0.3656845885781947
iteration : 5239
train acc:  0.8515625
train loss:  0.3247552812099457
train gradient:  0.1744426015359111
iteration : 5240
train acc:  0.8046875
train loss:  0.41766905784606934
train gradient:  0.43083447504791217
iteration : 5241
train acc:  0.8359375
train loss:  0.37448009848594666
train gradient:  0.2425755489252099
iteration : 5242
train acc:  0.8046875
train loss:  0.3421952426433563
train gradient:  0.20674842711442767
iteration : 5243
train acc:  0.90625
train loss:  0.3167991042137146
train gradient:  0.1924782262069377
iteration : 5244
train acc:  0.8125
train loss:  0.3570007383823395
train gradient:  0.2801829332077857
iteration : 5245
train acc:  0.84375
train loss:  0.3492125868797302
train gradient:  0.2081660249394559
iteration : 5246
train acc:  0.8046875
train loss:  0.3513212502002716
train gradient:  0.20359098230671274
iteration : 5247
train acc:  0.7734375
train loss:  0.4329269826412201
train gradient:  0.32211632716594574
iteration : 5248
train acc:  0.8515625
train loss:  0.32155513763427734
train gradient:  0.1968029920217057
iteration : 5249
train acc:  0.8984375
train loss:  0.2651383876800537
train gradient:  0.16758809196246327
iteration : 5250
train acc:  0.8203125
train loss:  0.4137376844882965
train gradient:  0.37589847414770333
iteration : 5251
train acc:  0.8671875
train loss:  0.28184184432029724
train gradient:  0.1321364883165323
iteration : 5252
train acc:  0.8203125
train loss:  0.3946439027786255
train gradient:  0.2101962546559914
iteration : 5253
train acc:  0.828125
train loss:  0.3616991639137268
train gradient:  0.3184732249981737
iteration : 5254
train acc:  0.859375
train loss:  0.33638352155685425
train gradient:  0.2511366087711294
iteration : 5255
train acc:  0.8125
train loss:  0.3818449378013611
train gradient:  0.2685285916191907
iteration : 5256
train acc:  0.84375
train loss:  0.3558496832847595
train gradient:  0.23764888483838045
iteration : 5257
train acc:  0.8984375
train loss:  0.28971266746520996
train gradient:  0.15831330303376123
iteration : 5258
train acc:  0.796875
train loss:  0.35983145236968994
train gradient:  0.23311240243543863
iteration : 5259
train acc:  0.84375
train loss:  0.3765156865119934
train gradient:  0.21681754059088854
iteration : 5260
train acc:  0.8046875
train loss:  0.3649865686893463
train gradient:  0.20645891157534754
iteration : 5261
train acc:  0.7734375
train loss:  0.4178234934806824
train gradient:  0.3704133656098558
iteration : 5262
train acc:  0.859375
train loss:  0.3176725208759308
train gradient:  0.14949911255727988
iteration : 5263
train acc:  0.8515625
train loss:  0.3370826542377472
train gradient:  0.2287755016096918
iteration : 5264
train acc:  0.84375
train loss:  0.3454364240169525
train gradient:  0.36755320916925666
iteration : 5265
train acc:  0.84375
train loss:  0.305341899394989
train gradient:  0.1770369394907318
iteration : 5266
train acc:  0.8359375
train loss:  0.35912802815437317
train gradient:  0.27887958903613547
iteration : 5267
train acc:  0.84375
train loss:  0.39426860213279724
train gradient:  0.3777980464960695
iteration : 5268
train acc:  0.828125
train loss:  0.4130079746246338
train gradient:  0.36577850153365804
iteration : 5269
train acc:  0.8671875
train loss:  0.3339231312274933
train gradient:  0.28034418976479797
iteration : 5270
train acc:  0.8046875
train loss:  0.36424148082733154
train gradient:  0.4834353227275101
iteration : 5271
train acc:  0.828125
train loss:  0.3834022879600525
train gradient:  0.41697549069338147
iteration : 5272
train acc:  0.9296875
train loss:  0.2234521508216858
train gradient:  0.11004985921877475
iteration : 5273
train acc:  0.8359375
train loss:  0.32422879338264465
train gradient:  0.2188615772097957
iteration : 5274
train acc:  0.8046875
train loss:  0.39158082008361816
train gradient:  0.24273618998087487
iteration : 5275
train acc:  0.859375
train loss:  0.33957749605178833
train gradient:  0.21664225131493595
iteration : 5276
train acc:  0.890625
train loss:  0.3057418763637543
train gradient:  0.25609450329480316
iteration : 5277
train acc:  0.890625
train loss:  0.27054864168167114
train gradient:  0.14195865764736457
iteration : 5278
train acc:  0.8125
train loss:  0.35907188057899475
train gradient:  0.33248050957009495
iteration : 5279
train acc:  0.890625
train loss:  0.30709412693977356
train gradient:  0.2351180602043983
iteration : 5280
train acc:  0.8515625
train loss:  0.31676843762397766
train gradient:  0.43339263895239255
iteration : 5281
train acc:  0.8203125
train loss:  0.37106719613075256
train gradient:  0.2550370083249255
iteration : 5282
train acc:  0.7890625
train loss:  0.4701572060585022
train gradient:  0.44171315909596637
iteration : 5283
train acc:  0.8515625
train loss:  0.37492454051971436
train gradient:  0.2350591648622804
iteration : 5284
train acc:  0.859375
train loss:  0.32514283061027527
train gradient:  0.2044632457507155
iteration : 5285
train acc:  0.8828125
train loss:  0.2793070077896118
train gradient:  0.15121776340968351
iteration : 5286
train acc:  0.828125
train loss:  0.3970767855644226
train gradient:  0.27738215005114303
iteration : 5287
train acc:  0.859375
train loss:  0.32621437311172485
train gradient:  0.15799389117118065
iteration : 5288
train acc:  0.8515625
train loss:  0.34761524200439453
train gradient:  0.22133763165217277
iteration : 5289
train acc:  0.8125
train loss:  0.43678784370422363
train gradient:  0.3143987326575787
iteration : 5290
train acc:  0.875
train loss:  0.276893675327301
train gradient:  0.1486425699385707
iteration : 5291
train acc:  0.890625
train loss:  0.2913152575492859
train gradient:  0.16935082738052848
iteration : 5292
train acc:  0.8203125
train loss:  0.40206801891326904
train gradient:  0.25548153204331103
iteration : 5293
train acc:  0.859375
train loss:  0.3170331120491028
train gradient:  0.33136426635403876
iteration : 5294
train acc:  0.90625
train loss:  0.2865448296070099
train gradient:  0.19328467113620446
iteration : 5295
train acc:  0.828125
train loss:  0.411234974861145
train gradient:  0.3955411972547446
iteration : 5296
train acc:  0.8046875
train loss:  0.38301774859428406
train gradient:  0.2484740899419719
iteration : 5297
train acc:  0.8671875
train loss:  0.32255077362060547
train gradient:  0.2546986567804841
iteration : 5298
train acc:  0.8671875
train loss:  0.3238102197647095
train gradient:  0.26020069723537925
iteration : 5299
train acc:  0.828125
train loss:  0.38121747970581055
train gradient:  0.27283533810218424
iteration : 5300
train acc:  0.8671875
train loss:  0.3505529761314392
train gradient:  0.17364534538167473
iteration : 5301
train acc:  0.90625
train loss:  0.23947983980178833
train gradient:  0.13623282829449282
iteration : 5302
train acc:  0.8046875
train loss:  0.3639179468154907
train gradient:  0.22267284504190976
iteration : 5303
train acc:  0.796875
train loss:  0.4343984127044678
train gradient:  0.4166020481847376
iteration : 5304
train acc:  0.8671875
train loss:  0.3451361358165741
train gradient:  0.208916816253571
iteration : 5305
train acc:  0.8359375
train loss:  0.3776289224624634
train gradient:  0.24576655911416462
iteration : 5306
train acc:  0.875
train loss:  0.30729371309280396
train gradient:  0.19908778370445993
iteration : 5307
train acc:  0.84375
train loss:  0.3709423840045929
train gradient:  0.36097679672613564
iteration : 5308
train acc:  0.8125
train loss:  0.42491593956947327
train gradient:  0.36190678854512937
iteration : 5309
train acc:  0.796875
train loss:  0.3569890558719635
train gradient:  0.23465174596218624
iteration : 5310
train acc:  0.8515625
train loss:  0.3428192138671875
train gradient:  0.31602228527178783
iteration : 5311
train acc:  0.7734375
train loss:  0.44989708065986633
train gradient:  0.399331928383607
iteration : 5312
train acc:  0.8671875
train loss:  0.32346460223197937
train gradient:  0.21596385590100595
iteration : 5313
train acc:  0.828125
train loss:  0.39817753434181213
train gradient:  0.28817062300818264
iteration : 5314
train acc:  0.828125
train loss:  0.3970235288143158
train gradient:  0.47791779141960744
iteration : 5315
train acc:  0.8203125
train loss:  0.3733852803707123
train gradient:  0.2334716109587977
iteration : 5316
train acc:  0.828125
train loss:  0.39895451068878174
train gradient:  0.28826180477536933
iteration : 5317
train acc:  0.7890625
train loss:  0.3984296917915344
train gradient:  0.266535490088159
iteration : 5318
train acc:  0.8046875
train loss:  0.38756263256073
train gradient:  0.28795039982437287
iteration : 5319
train acc:  0.8515625
train loss:  0.3542342483997345
train gradient:  0.2766843268465849
iteration : 5320
train acc:  0.84375
train loss:  0.3638547360897064
train gradient:  0.18613121400724514
iteration : 5321
train acc:  0.8515625
train loss:  0.3471216559410095
train gradient:  0.3164549661389305
iteration : 5322
train acc:  0.796875
train loss:  0.3816179037094116
train gradient:  0.26099000518898086
iteration : 5323
train acc:  0.8515625
train loss:  0.3655533492565155
train gradient:  0.25429476142531665
iteration : 5324
train acc:  0.8125
train loss:  0.38574302196502686
train gradient:  0.22249865032639474
iteration : 5325
train acc:  0.8359375
train loss:  0.41525912284851074
train gradient:  0.2795040601999156
iteration : 5326
train acc:  0.7734375
train loss:  0.4307091236114502
train gradient:  0.26653240867400835
iteration : 5327
train acc:  0.8828125
train loss:  0.29028958082199097
train gradient:  0.16126221653642203
iteration : 5328
train acc:  0.8203125
train loss:  0.38908421993255615
train gradient:  0.2814014977970071
iteration : 5329
train acc:  0.8515625
train loss:  0.3467979431152344
train gradient:  0.21207478160123794
iteration : 5330
train acc:  0.796875
train loss:  0.43497520685195923
train gradient:  0.36312717208558853
iteration : 5331
train acc:  0.8828125
train loss:  0.3333854079246521
train gradient:  0.19258990795243291
iteration : 5332
train acc:  0.890625
train loss:  0.29662540555000305
train gradient:  0.2353009417335481
iteration : 5333
train acc:  0.9140625
train loss:  0.281360000371933
train gradient:  0.16233442032062567
iteration : 5334
train acc:  0.8515625
train loss:  0.3464762568473816
train gradient:  0.22751786706722277
iteration : 5335
train acc:  0.9140625
train loss:  0.26974955201148987
train gradient:  0.16081621826185205
iteration : 5336
train acc:  0.8046875
train loss:  0.4521512985229492
train gradient:  0.2965953832198562
iteration : 5337
train acc:  0.8046875
train loss:  0.3841252624988556
train gradient:  0.2874670075659752
iteration : 5338
train acc:  0.8203125
train loss:  0.377374529838562
train gradient:  0.2280671749465744
iteration : 5339
train acc:  0.8515625
train loss:  0.3231457471847534
train gradient:  0.2343612083951158
iteration : 5340
train acc:  0.8046875
train loss:  0.40775585174560547
train gradient:  0.28661316200350306
iteration : 5341
train acc:  0.8515625
train loss:  0.3570300340652466
train gradient:  0.16051120787619236
iteration : 5342
train acc:  0.8203125
train loss:  0.38445547223091125
train gradient:  0.2847699100642802
iteration : 5343
train acc:  0.8828125
train loss:  0.2963021993637085
train gradient:  0.14198344655877088
iteration : 5344
train acc:  0.796875
train loss:  0.46235308051109314
train gradient:  0.3416542711106292
iteration : 5345
train acc:  0.8828125
train loss:  0.27694791555404663
train gradient:  0.13378008207467096
iteration : 5346
train acc:  0.84375
train loss:  0.35331422090530396
train gradient:  0.20683919362509245
iteration : 5347
train acc:  0.8203125
train loss:  0.3619915544986725
train gradient:  0.2939486088857216
iteration : 5348
train acc:  0.875
train loss:  0.2979608476161957
train gradient:  0.1918902445089529
iteration : 5349
train acc:  0.8515625
train loss:  0.3266063928604126
train gradient:  0.23909150387881076
iteration : 5350
train acc:  0.8125
train loss:  0.36358168721199036
train gradient:  0.20357701549649948
iteration : 5351
train acc:  0.796875
train loss:  0.4325099587440491
train gradient:  0.3359723901424341
iteration : 5352
train acc:  0.8671875
train loss:  0.30323582887649536
train gradient:  0.15067408127148435
iteration : 5353
train acc:  0.890625
train loss:  0.2833693325519562
train gradient:  0.14132186665560778
iteration : 5354
train acc:  0.8203125
train loss:  0.39505308866500854
train gradient:  0.2774242831930915
iteration : 5355
train acc:  0.78125
train loss:  0.4602849781513214
train gradient:  0.3068139515240093
iteration : 5356
train acc:  0.7734375
train loss:  0.49117594957351685
train gradient:  0.5689953671879779
iteration : 5357
train acc:  0.859375
train loss:  0.32337382435798645
train gradient:  0.3969054052808834
iteration : 5358
train acc:  0.890625
train loss:  0.28356271982192993
train gradient:  0.1403825209433307
iteration : 5359
train acc:  0.84375
train loss:  0.36655938625335693
train gradient:  0.2198331796005636
iteration : 5360
train acc:  0.8671875
train loss:  0.2654813230037689
train gradient:  0.1460378226487365
iteration : 5361
train acc:  0.8671875
train loss:  0.36433491110801697
train gradient:  0.21839525106692997
iteration : 5362
train acc:  0.875
train loss:  0.3595831096172333
train gradient:  0.25160715644212134
iteration : 5363
train acc:  0.859375
train loss:  0.2946624159812927
train gradient:  0.19138597608122743
iteration : 5364
train acc:  0.84375
train loss:  0.35077035427093506
train gradient:  0.2058522144472617
iteration : 5365
train acc:  0.8203125
train loss:  0.34592539072036743
train gradient:  0.30977121938627883
iteration : 5366
train acc:  0.890625
train loss:  0.266895055770874
train gradient:  0.15403851929789159
iteration : 5367
train acc:  0.8671875
train loss:  0.3497485816478729
train gradient:  0.18412084695617487
iteration : 5368
train acc:  0.84375
train loss:  0.3491494655609131
train gradient:  0.2478702919315809
iteration : 5369
train acc:  0.8984375
train loss:  0.2608817517757416
train gradient:  0.16748604187035082
iteration : 5370
train acc:  0.8515625
train loss:  0.3534920811653137
train gradient:  0.1732602397193724
iteration : 5371
train acc:  0.828125
train loss:  0.4121653437614441
train gradient:  0.32134270841333007
iteration : 5372
train acc:  0.859375
train loss:  0.2949776351451874
train gradient:  0.18339980269509665
iteration : 5373
train acc:  0.890625
train loss:  0.3051673173904419
train gradient:  0.13938100818779092
iteration : 5374
train acc:  0.84375
train loss:  0.3352421522140503
train gradient:  0.16066122343403408
iteration : 5375
train acc:  0.8828125
train loss:  0.32383692264556885
train gradient:  0.1893737763157099
iteration : 5376
train acc:  0.8515625
train loss:  0.3343369960784912
train gradient:  0.31629977899100853
iteration : 5377
train acc:  0.8046875
train loss:  0.4547845423221588
train gradient:  0.42287855762859716
iteration : 5378
train acc:  0.7890625
train loss:  0.3724696636199951
train gradient:  0.2793935507387413
iteration : 5379
train acc:  0.9140625
train loss:  0.26192188262939453
train gradient:  0.18318161483217976
iteration : 5380
train acc:  0.859375
train loss:  0.31099754571914673
train gradient:  0.1964398979189264
iteration : 5381
train acc:  0.890625
train loss:  0.2828313410282135
train gradient:  0.1434635920361318
iteration : 5382
train acc:  0.8828125
train loss:  0.29924389719963074
train gradient:  0.17266138851634222
iteration : 5383
train acc:  0.8671875
train loss:  0.2798842787742615
train gradient:  0.13441950735153074
iteration : 5384
train acc:  0.7890625
train loss:  0.44962188601493835
train gradient:  0.31306152568814316
iteration : 5385
train acc:  0.8828125
train loss:  0.31478649377822876
train gradient:  0.22841005512140966
iteration : 5386
train acc:  0.8515625
train loss:  0.3447302579879761
train gradient:  0.2629179671124658
iteration : 5387
train acc:  0.796875
train loss:  0.4280126094818115
train gradient:  0.28872060068176825
iteration : 5388
train acc:  0.875
train loss:  0.34702518582344055
train gradient:  0.2617013189311561
iteration : 5389
train acc:  0.8671875
train loss:  0.3427865505218506
train gradient:  0.3188722126555371
iteration : 5390
train acc:  0.8828125
train loss:  0.3209116458892822
train gradient:  0.20446114757222716
iteration : 5391
train acc:  0.859375
train loss:  0.3236003518104553
train gradient:  0.2269540331671478
iteration : 5392
train acc:  0.84375
train loss:  0.2836208939552307
train gradient:  0.1423619756499101
iteration : 5393
train acc:  0.859375
train loss:  0.3441946506500244
train gradient:  0.4538417339512663
iteration : 5394
train acc:  0.8671875
train loss:  0.3083224892616272
train gradient:  0.2272118478178924
iteration : 5395
train acc:  0.8359375
train loss:  0.3344692289829254
train gradient:  0.22534508693363547
iteration : 5396
train acc:  0.859375
train loss:  0.327217698097229
train gradient:  0.21770661748905507
iteration : 5397
train acc:  0.84375
train loss:  0.3676393926143646
train gradient:  0.2691273825016061
iteration : 5398
train acc:  0.7734375
train loss:  0.409510999917984
train gradient:  0.37105694187180016
iteration : 5399
train acc:  0.8984375
train loss:  0.2525853216648102
train gradient:  0.16522611595247652
iteration : 5400
train acc:  0.8359375
train loss:  0.34928613901138306
train gradient:  0.21829613095460987
iteration : 5401
train acc:  0.8671875
train loss:  0.31198716163635254
train gradient:  0.29265495613423376
iteration : 5402
train acc:  0.8671875
train loss:  0.3077247142791748
train gradient:  0.17903852708285
iteration : 5403
train acc:  0.859375
train loss:  0.35985463857650757
train gradient:  0.2351464813869623
iteration : 5404
train acc:  0.8671875
train loss:  0.3432232737541199
train gradient:  0.24570877972039634
iteration : 5405
train acc:  0.796875
train loss:  0.37660735845565796
train gradient:  0.2544244551636834
iteration : 5406
train acc:  0.828125
train loss:  0.40823641419410706
train gradient:  0.3107950795662944
iteration : 5407
train acc:  0.8203125
train loss:  0.34493914246559143
train gradient:  0.22140637981629305
iteration : 5408
train acc:  0.8125
train loss:  0.390328049659729
train gradient:  0.3157239339899097
iteration : 5409
train acc:  0.8515625
train loss:  0.30040377378463745
train gradient:  0.19523851761014033
iteration : 5410
train acc:  0.8515625
train loss:  0.31030744314193726
train gradient:  0.2513210318269849
iteration : 5411
train acc:  0.84375
train loss:  0.3579551577568054
train gradient:  0.31421782388563185
iteration : 5412
train acc:  0.8046875
train loss:  0.39692264795303345
train gradient:  0.32619838070849155
iteration : 5413
train acc:  0.8125
train loss:  0.4233824610710144
train gradient:  0.36178400796400817
iteration : 5414
train acc:  0.8125
train loss:  0.3723364770412445
train gradient:  0.23889796065542834
iteration : 5415
train acc:  0.828125
train loss:  0.4365639090538025
train gradient:  0.27129150438441385
iteration : 5416
train acc:  0.8515625
train loss:  0.3357941508293152
train gradient:  0.23125784789230555
iteration : 5417
train acc:  0.8984375
train loss:  0.24206054210662842
train gradient:  0.15116031663383533
iteration : 5418
train acc:  0.8359375
train loss:  0.3863525688648224
train gradient:  0.32669626608648517
iteration : 5419
train acc:  0.8671875
train loss:  0.3472570776939392
train gradient:  0.23819593795992072
iteration : 5420
train acc:  0.8203125
train loss:  0.4064546227455139
train gradient:  0.30864687067316243
iteration : 5421
train acc:  0.84375
train loss:  0.34249523282051086
train gradient:  0.2933049436340649
iteration : 5422
train acc:  0.8671875
train loss:  0.32106223702430725
train gradient:  0.20575318468066536
iteration : 5423
train acc:  0.84375
train loss:  0.38059380650520325
train gradient:  0.25323859273920296
iteration : 5424
train acc:  0.78125
train loss:  0.4907863736152649
train gradient:  0.4430826076199522
iteration : 5425
train acc:  0.8671875
train loss:  0.3602129817008972
train gradient:  0.1977937548859976
iteration : 5426
train acc:  0.8359375
train loss:  0.39951616525650024
train gradient:  0.3405111720617731
iteration : 5427
train acc:  0.8671875
train loss:  0.35083186626434326
train gradient:  0.2072296742323522
iteration : 5428
train acc:  0.859375
train loss:  0.30546051263809204
train gradient:  0.14588609561714444
iteration : 5429
train acc:  0.890625
train loss:  0.2756724953651428
train gradient:  0.22793322943174998
iteration : 5430
train acc:  0.890625
train loss:  0.25462132692337036
train gradient:  0.11359769159710953
iteration : 5431
train acc:  0.890625
train loss:  0.3285336494445801
train gradient:  0.3164060453735968
iteration : 5432
train acc:  0.8515625
train loss:  0.37406688928604126
train gradient:  0.3845779842492522
iteration : 5433
train acc:  0.859375
train loss:  0.3419091999530792
train gradient:  0.28875777512920126
iteration : 5434
train acc:  0.7890625
train loss:  0.40239417552948
train gradient:  0.277418973352162
iteration : 5435
train acc:  0.796875
train loss:  0.4322379529476166
train gradient:  0.25988991703045217
iteration : 5436
train acc:  0.8203125
train loss:  0.35550597310066223
train gradient:  0.21350585346631323
iteration : 5437
train acc:  0.828125
train loss:  0.3455507755279541
train gradient:  0.21712736806620156
iteration : 5438
train acc:  0.765625
train loss:  0.4813843369483948
train gradient:  0.3798623850507275
iteration : 5439
train acc:  0.859375
train loss:  0.32635697722435
train gradient:  0.27135369744830923
iteration : 5440
train acc:  0.796875
train loss:  0.3970959484577179
train gradient:  0.299205190548788
iteration : 5441
train acc:  0.8828125
train loss:  0.29903852939605713
train gradient:  0.16320125334874935
iteration : 5442
train acc:  0.8671875
train loss:  0.29241299629211426
train gradient:  0.12644274909483239
iteration : 5443
train acc:  0.8671875
train loss:  0.30789661407470703
train gradient:  0.21668762685032497
iteration : 5444
train acc:  0.8671875
train loss:  0.3488719165325165
train gradient:  0.15236218759048245
iteration : 5445
train acc:  0.84375
train loss:  0.3519408702850342
train gradient:  0.28672821250124714
iteration : 5446
train acc:  0.7890625
train loss:  0.4170219302177429
train gradient:  0.3326695070957658
iteration : 5447
train acc:  0.8359375
train loss:  0.37517213821411133
train gradient:  0.2790984770430625
iteration : 5448
train acc:  0.8671875
train loss:  0.32670706510543823
train gradient:  0.16948072759722743
iteration : 5449
train acc:  0.8671875
train loss:  0.3480931520462036
train gradient:  0.22575764505473983
iteration : 5450
train acc:  0.828125
train loss:  0.42187610268592834
train gradient:  0.28318756683291485
iteration : 5451
train acc:  0.796875
train loss:  0.3774099349975586
train gradient:  0.3006414640872557
iteration : 5452
train acc:  0.828125
train loss:  0.3234318196773529
train gradient:  0.21673007947704095
iteration : 5453
train acc:  0.8828125
train loss:  0.3256557583808899
train gradient:  0.1712318654146281
iteration : 5454
train acc:  0.8671875
train loss:  0.3735184669494629
train gradient:  0.3030340272320069
iteration : 5455
train acc:  0.8125
train loss:  0.35847392678260803
train gradient:  0.2103439070004778
iteration : 5456
train acc:  0.859375
train loss:  0.3669634759426117
train gradient:  0.27385613520958324
iteration : 5457
train acc:  0.7734375
train loss:  0.45917147397994995
train gradient:  0.6519257459995282
iteration : 5458
train acc:  0.859375
train loss:  0.40016108751296997
train gradient:  0.28234234970297933
iteration : 5459
train acc:  0.8125
train loss:  0.36249256134033203
train gradient:  0.27516860716795577
iteration : 5460
train acc:  0.9140625
train loss:  0.28202617168426514
train gradient:  0.1284047647472375
iteration : 5461
train acc:  0.8359375
train loss:  0.35135242342948914
train gradient:  0.21640866142417214
iteration : 5462
train acc:  0.859375
train loss:  0.36828887462615967
train gradient:  0.31779161138966255
iteration : 5463
train acc:  0.875
train loss:  0.30968278646469116
train gradient:  0.19665752559612285
iteration : 5464
train acc:  0.8125
train loss:  0.4440285265445709
train gradient:  0.3254790258965924
iteration : 5465
train acc:  0.875
train loss:  0.33982396125793457
train gradient:  0.2630136546082701
iteration : 5466
train acc:  0.859375
train loss:  0.30446314811706543
train gradient:  0.19124217380384365
iteration : 5467
train acc:  0.8359375
train loss:  0.31302371621131897
train gradient:  0.1919846071074561
iteration : 5468
train acc:  0.875
train loss:  0.2978496551513672
train gradient:  0.14925194750731308
iteration : 5469
train acc:  0.8359375
train loss:  0.3864361047744751
train gradient:  0.19142188210742406
iteration : 5470
train acc:  0.859375
train loss:  0.3026893734931946
train gradient:  0.19547959192835057
iteration : 5471
train acc:  0.90625
train loss:  0.3524453341960907
train gradient:  0.23732760992243057
iteration : 5472
train acc:  0.8515625
train loss:  0.36886778473854065
train gradient:  0.23826429604570598
iteration : 5473
train acc:  0.8359375
train loss:  0.36012423038482666
train gradient:  0.23616964904614512
iteration : 5474
train acc:  0.7890625
train loss:  0.4483166038990021
train gradient:  0.286897187683027
iteration : 5475
train acc:  0.8984375
train loss:  0.3467051386833191
train gradient:  0.300625421194051
iteration : 5476
train acc:  0.8515625
train loss:  0.35284149646759033
train gradient:  0.2959361102031467
iteration : 5477
train acc:  0.84375
train loss:  0.35555458068847656
train gradient:  0.25825619946356393
iteration : 5478
train acc:  0.859375
train loss:  0.35757893323898315
train gradient:  0.309018355700447
iteration : 5479
train acc:  0.8671875
train loss:  0.33094322681427
train gradient:  0.14099913207736564
iteration : 5480
train acc:  0.8984375
train loss:  0.3016372323036194
train gradient:  0.3003264297506669
iteration : 5481
train acc:  0.8671875
train loss:  0.3995651304721832
train gradient:  0.27045931066015416
iteration : 5482
train acc:  0.828125
train loss:  0.3495762348175049
train gradient:  0.15303042241756926
iteration : 5483
train acc:  0.8125
train loss:  0.37974750995635986
train gradient:  0.20103972406304133
iteration : 5484
train acc:  0.8125
train loss:  0.4247605800628662
train gradient:  0.2436979915482636
iteration : 5485
train acc:  0.796875
train loss:  0.4166213870048523
train gradient:  0.3596476444456682
iteration : 5486
train acc:  0.8828125
train loss:  0.2486482560634613
train gradient:  0.13950033616317536
iteration : 5487
train acc:  0.8671875
train loss:  0.29471659660339355
train gradient:  0.1535570566454526
iteration : 5488
train acc:  0.859375
train loss:  0.3698463439941406
train gradient:  0.35439890514243055
iteration : 5489
train acc:  0.828125
train loss:  0.3589709401130676
train gradient:  0.1953313170618438
iteration : 5490
train acc:  0.8203125
train loss:  0.34960848093032837
train gradient:  0.2233276527950833
iteration : 5491
train acc:  0.875
train loss:  0.37383145093917847
train gradient:  0.2226165784939156
iteration : 5492
train acc:  0.859375
train loss:  0.32651400566101074
train gradient:  0.14588722514053165
iteration : 5493
train acc:  0.8515625
train loss:  0.39569827914237976
train gradient:  0.2735394292808023
iteration : 5494
train acc:  0.8828125
train loss:  0.26205015182495117
train gradient:  0.3161328634850429
iteration : 5495
train acc:  0.84375
train loss:  0.3416528105735779
train gradient:  0.25863389063632114
iteration : 5496
train acc:  0.796875
train loss:  0.43836525082588196
train gradient:  0.28666964258288247
iteration : 5497
train acc:  0.78125
train loss:  0.512866735458374
train gradient:  0.3146781886768609
iteration : 5498
train acc:  0.8203125
train loss:  0.34616780281066895
train gradient:  0.26818751566995025
iteration : 5499
train acc:  0.8125
train loss:  0.3845406174659729
train gradient:  0.21691044284829325
iteration : 5500
train acc:  0.8203125
train loss:  0.37923920154571533
train gradient:  0.23532368173100854
iteration : 5501
train acc:  0.8671875
train loss:  0.28995755314826965
train gradient:  0.16971485587876312
iteration : 5502
train acc:  0.828125
train loss:  0.34139588475227356
train gradient:  0.2150070033011257
iteration : 5503
train acc:  0.859375
train loss:  0.292333722114563
train gradient:  0.12483956527845012
iteration : 5504
train acc:  0.828125
train loss:  0.37798213958740234
train gradient:  0.3415209625386245
iteration : 5505
train acc:  0.8515625
train loss:  0.3260959982872009
train gradient:  0.22354261699376435
iteration : 5506
train acc:  0.859375
train loss:  0.3405066728591919
train gradient:  0.31595747250208267
iteration : 5507
train acc:  0.84375
train loss:  0.3713811933994293
train gradient:  0.21639132609312242
iteration : 5508
train acc:  0.8671875
train loss:  0.33603179454803467
train gradient:  0.23666112936213374
iteration : 5509
train acc:  0.828125
train loss:  0.3467235267162323
train gradient:  0.20176028555059028
iteration : 5510
train acc:  0.8359375
train loss:  0.4312286972999573
train gradient:  0.2592820454415671
iteration : 5511
train acc:  0.890625
train loss:  0.27544137835502625
train gradient:  0.15270529671253932
iteration : 5512
train acc:  0.8984375
train loss:  0.2946843206882477
train gradient:  0.14372650756087138
iteration : 5513
train acc:  0.796875
train loss:  0.426044762134552
train gradient:  0.29095284475049604
iteration : 5514
train acc:  0.7734375
train loss:  0.3651644289493561
train gradient:  0.1917110351429775
iteration : 5515
train acc:  0.828125
train loss:  0.37787294387817383
train gradient:  0.2459894737547914
iteration : 5516
train acc:  0.8359375
train loss:  0.3554186224937439
train gradient:  0.19635231691207147
iteration : 5517
train acc:  0.828125
train loss:  0.338114857673645
train gradient:  0.18976890375873035
iteration : 5518
train acc:  0.7734375
train loss:  0.44544798135757446
train gradient:  0.37422479606733294
iteration : 5519
train acc:  0.8203125
train loss:  0.3851231336593628
train gradient:  0.2525983631409178
iteration : 5520
train acc:  0.84375
train loss:  0.330968976020813
train gradient:  0.17848582189135478
iteration : 5521
train acc:  0.8828125
train loss:  0.305874228477478
train gradient:  0.19831947741575015
iteration : 5522
train acc:  0.828125
train loss:  0.3919934630393982
train gradient:  0.31506483836840127
iteration : 5523
train acc:  0.78125
train loss:  0.4635773301124573
train gradient:  0.2568120934210716
iteration : 5524
train acc:  0.84375
train loss:  0.3428630828857422
train gradient:  0.19059110417343056
iteration : 5525
train acc:  0.8671875
train loss:  0.3360683023929596
train gradient:  0.19117669162017903
iteration : 5526
train acc:  0.8984375
train loss:  0.33224982023239136
train gradient:  0.15489643565013983
iteration : 5527
train acc:  0.875
train loss:  0.3557930588722229
train gradient:  0.22127559897651877
iteration : 5528
train acc:  0.859375
train loss:  0.2980901598930359
train gradient:  0.12668920967856684
iteration : 5529
train acc:  0.859375
train loss:  0.29466843605041504
train gradient:  0.16699486977483816
iteration : 5530
train acc:  0.875
train loss:  0.2905697822570801
train gradient:  0.11649845652657326
iteration : 5531
train acc:  0.8515625
train loss:  0.35707616806030273
train gradient:  0.22510292948526328
iteration : 5532
train acc:  0.8828125
train loss:  0.38458821177482605
train gradient:  0.2329226285601692
iteration : 5533
train acc:  0.828125
train loss:  0.391285240650177
train gradient:  0.27971520213979995
iteration : 5534
train acc:  0.8203125
train loss:  0.38570719957351685
train gradient:  0.34732300983048153
iteration : 5535
train acc:  0.8203125
train loss:  0.3494648337364197
train gradient:  0.19118704253649413
iteration : 5536
train acc:  0.859375
train loss:  0.2970050573348999
train gradient:  0.21186297662266637
iteration : 5537
train acc:  0.859375
train loss:  0.3422126770019531
train gradient:  0.20298961263870857
iteration : 5538
train acc:  0.828125
train loss:  0.3489473760128021
train gradient:  0.24182289556866046
iteration : 5539
train acc:  0.8515625
train loss:  0.36788398027420044
train gradient:  0.2740082379011173
iteration : 5540
train acc:  0.796875
train loss:  0.35573697090148926
train gradient:  0.21421308390116434
iteration : 5541
train acc:  0.8359375
train loss:  0.3689030706882477
train gradient:  0.2152257392699401
iteration : 5542
train acc:  0.859375
train loss:  0.3320770859718323
train gradient:  0.15212167540608357
iteration : 5543
train acc:  0.828125
train loss:  0.38046085834503174
train gradient:  0.20859097343878635
iteration : 5544
train acc:  0.8203125
train loss:  0.4284769296646118
train gradient:  0.27311024974642384
iteration : 5545
train acc:  0.90625
train loss:  0.2911888659000397
train gradient:  0.2517279404164012
iteration : 5546
train acc:  0.859375
train loss:  0.31547319889068604
train gradient:  0.15808651295030882
iteration : 5547
train acc:  0.8515625
train loss:  0.33654069900512695
train gradient:  0.2223627663187315
iteration : 5548
train acc:  0.828125
train loss:  0.3575092852115631
train gradient:  0.2160894972810568
iteration : 5549
train acc:  0.8828125
train loss:  0.2754128575325012
train gradient:  0.16213675570483252
iteration : 5550
train acc:  0.8359375
train loss:  0.3974182605743408
train gradient:  0.2308041916984978
iteration : 5551
train acc:  0.8671875
train loss:  0.3388575315475464
train gradient:  0.20182367358874403
iteration : 5552
train acc:  0.8671875
train loss:  0.3008730113506317
train gradient:  0.18137840495573687
iteration : 5553
train acc:  0.8671875
train loss:  0.3550959527492523
train gradient:  0.3476369020103502
iteration : 5554
train acc:  0.8046875
train loss:  0.3917873501777649
train gradient:  0.22362968554845808
iteration : 5555
train acc:  0.8046875
train loss:  0.3636784553527832
train gradient:  0.22195388560026919
iteration : 5556
train acc:  0.8203125
train loss:  0.4303090572357178
train gradient:  0.2991280786850325
iteration : 5557
train acc:  0.859375
train loss:  0.307802677154541
train gradient:  0.20444376146729412
iteration : 5558
train acc:  0.8203125
train loss:  0.39456290006637573
train gradient:  0.3415072189385209
iteration : 5559
train acc:  0.828125
train loss:  0.3855431079864502
train gradient:  0.22820529873809323
iteration : 5560
train acc:  0.84375
train loss:  0.34970271587371826
train gradient:  0.24968306110177457
iteration : 5561
train acc:  0.8359375
train loss:  0.34582430124282837
train gradient:  0.1999974913725855
iteration : 5562
train acc:  0.8984375
train loss:  0.30238252878189087
train gradient:  0.17883887186580297
iteration : 5563
train acc:  0.828125
train loss:  0.3604121506214142
train gradient:  0.29006602918593677
iteration : 5564
train acc:  0.8515625
train loss:  0.36992213129997253
train gradient:  0.2136714655102883
iteration : 5565
train acc:  0.84375
train loss:  0.3510654866695404
train gradient:  0.23947392243795118
iteration : 5566
train acc:  0.8984375
train loss:  0.2565259337425232
train gradient:  0.1822732861601214
iteration : 5567
train acc:  0.828125
train loss:  0.3377501964569092
train gradient:  0.20574548062377374
iteration : 5568
train acc:  0.8125
train loss:  0.4143747091293335
train gradient:  0.4357544050382184
iteration : 5569
train acc:  0.84375
train loss:  0.3523086905479431
train gradient:  0.21454066855893975
iteration : 5570
train acc:  0.875
train loss:  0.2967582941055298
train gradient:  0.16545760229951506
iteration : 5571
train acc:  0.875
train loss:  0.30573388934135437
train gradient:  0.17858820043691748
iteration : 5572
train acc:  0.8828125
train loss:  0.2950681447982788
train gradient:  0.16381177061440433
iteration : 5573
train acc:  0.875
train loss:  0.33985573053359985
train gradient:  0.39473581811082176
iteration : 5574
train acc:  0.859375
train loss:  0.30873775482177734
train gradient:  0.1751473357276381
iteration : 5575
train acc:  0.859375
train loss:  0.31031492352485657
train gradient:  0.25117003537435106
iteration : 5576
train acc:  0.8671875
train loss:  0.3228462040424347
train gradient:  0.18303833289385768
iteration : 5577
train acc:  0.8515625
train loss:  0.30716991424560547
train gradient:  0.14940623757906285
iteration : 5578
train acc:  0.8203125
train loss:  0.38573354482650757
train gradient:  0.4337464731752949
iteration : 5579
train acc:  0.859375
train loss:  0.34011518955230713
train gradient:  0.27944284650789436
iteration : 5580
train acc:  0.8359375
train loss:  0.3808092474937439
train gradient:  0.35509936622701394
iteration : 5581
train acc:  0.84375
train loss:  0.3738129138946533
train gradient:  0.23052793979509772
iteration : 5582
train acc:  0.828125
train loss:  0.3544093370437622
train gradient:  0.26160600908263293
iteration : 5583
train acc:  0.859375
train loss:  0.30788081884384155
train gradient:  0.17057875033230535
iteration : 5584
train acc:  0.8515625
train loss:  0.33173924684524536
train gradient:  0.5658027213656094
iteration : 5585
train acc:  0.890625
train loss:  0.36199986934661865
train gradient:  0.19354542007089137
iteration : 5586
train acc:  0.8515625
train loss:  0.35842448472976685
train gradient:  0.2765032281112427
iteration : 5587
train acc:  0.8125
train loss:  0.4087056517601013
train gradient:  0.284576808344181
iteration : 5588
train acc:  0.8046875
train loss:  0.42419958114624023
train gradient:  0.2873579006282734
iteration : 5589
train acc:  0.8515625
train loss:  0.3159744143486023
train gradient:  0.2205479496296637
iteration : 5590
train acc:  0.796875
train loss:  0.4356189966201782
train gradient:  0.32380353673783174
iteration : 5591
train acc:  0.8671875
train loss:  0.2883596420288086
train gradient:  0.16910393711036292
iteration : 5592
train acc:  0.84375
train loss:  0.3242935538291931
train gradient:  0.21986827035329234
iteration : 5593
train acc:  0.8203125
train loss:  0.38525936007499695
train gradient:  0.3442564366193
iteration : 5594
train acc:  0.8515625
train loss:  0.32672441005706787
train gradient:  0.2483675173580503
iteration : 5595
train acc:  0.8359375
train loss:  0.3645353615283966
train gradient:  0.36244443189726927
iteration : 5596
train acc:  0.90625
train loss:  0.280700147151947
train gradient:  0.1686957335672391
iteration : 5597
train acc:  0.8984375
train loss:  0.3224250078201294
train gradient:  0.2622710837735793
iteration : 5598
train acc:  0.875
train loss:  0.3184302747249603
train gradient:  0.21677193733956124
iteration : 5599
train acc:  0.859375
train loss:  0.3340367078781128
train gradient:  0.22034274604475873
iteration : 5600
train acc:  0.828125
train loss:  0.37949156761169434
train gradient:  0.22888441523201172
iteration : 5601
train acc:  0.8515625
train loss:  0.3592236042022705
train gradient:  0.3018331792319189
iteration : 5602
train acc:  0.8515625
train loss:  0.3124783933162689
train gradient:  0.2533249438458729
iteration : 5603
train acc:  0.828125
train loss:  0.3843575119972229
train gradient:  0.30716407440280996
iteration : 5604
train acc:  0.8046875
train loss:  0.4336112439632416
train gradient:  0.3451666656183884
iteration : 5605
train acc:  0.8125
train loss:  0.3927372097969055
train gradient:  0.2748550799911716
iteration : 5606
train acc:  0.875
train loss:  0.32856813073158264
train gradient:  0.2869521135080919
iteration : 5607
train acc:  0.7890625
train loss:  0.43717581033706665
train gradient:  0.3797317026418765
iteration : 5608
train acc:  0.8515625
train loss:  0.33619052171707153
train gradient:  0.2747385051216896
iteration : 5609
train acc:  0.8828125
train loss:  0.2858602702617645
train gradient:  0.19367054467242517
iteration : 5610
train acc:  0.890625
train loss:  0.31582435965538025
train gradient:  0.1686835916321786
iteration : 5611
train acc:  0.9140625
train loss:  0.3075525164604187
train gradient:  0.25405143140316244
iteration : 5612
train acc:  0.8515625
train loss:  0.3062351644039154
train gradient:  0.19511289287929084
iteration : 5613
train acc:  0.84375
train loss:  0.30766797065734863
train gradient:  0.23331253608527347
iteration : 5614
train acc:  0.8515625
train loss:  0.3284943699836731
train gradient:  0.2633955276398971
iteration : 5615
train acc:  0.796875
train loss:  0.42412281036376953
train gradient:  0.27225539286056505
iteration : 5616
train acc:  0.84375
train loss:  0.30850017070770264
train gradient:  0.23462840221629816
iteration : 5617
train acc:  0.8984375
train loss:  0.25838518142700195
train gradient:  0.15646710544018022
iteration : 5618
train acc:  0.859375
train loss:  0.3000982105731964
train gradient:  0.24770464506791118
iteration : 5619
train acc:  0.859375
train loss:  0.31915321946144104
train gradient:  0.2585230995814375
iteration : 5620
train acc:  0.796875
train loss:  0.3864481449127197
train gradient:  0.3306914061326243
iteration : 5621
train acc:  0.84375
train loss:  0.4415881931781769
train gradient:  0.3211698271345098
iteration : 5622
train acc:  0.875
train loss:  0.27882176637649536
train gradient:  0.16912144913722327
iteration : 5623
train acc:  0.8359375
train loss:  0.38009005784988403
train gradient:  0.26110247750296667
iteration : 5624
train acc:  0.84375
train loss:  0.37457844614982605
train gradient:  0.2008949389846573
iteration : 5625
train acc:  0.859375
train loss:  0.34389573335647583
train gradient:  0.23852656096336705
iteration : 5626
train acc:  0.7890625
train loss:  0.4084169864654541
train gradient:  0.33740931430250376
iteration : 5627
train acc:  0.875
train loss:  0.3124304413795471
train gradient:  0.22048172565190782
iteration : 5628
train acc:  0.8671875
train loss:  0.33159753680229187
train gradient:  0.20761254396368878
iteration : 5629
train acc:  0.8515625
train loss:  0.3421850800514221
train gradient:  0.23780517720263128
iteration : 5630
train acc:  0.796875
train loss:  0.47315895557403564
train gradient:  0.38891807952404106
iteration : 5631
train acc:  0.8125
train loss:  0.39558088779449463
train gradient:  0.3380229454499679
iteration : 5632
train acc:  0.859375
train loss:  0.29434311389923096
train gradient:  0.16216959490549754
iteration : 5633
train acc:  0.84375
train loss:  0.27778804302215576
train gradient:  0.1471565757199449
iteration : 5634
train acc:  0.859375
train loss:  0.3675985038280487
train gradient:  0.21343190176142599
iteration : 5635
train acc:  0.7890625
train loss:  0.4050784111022949
train gradient:  0.31698411184875086
iteration : 5636
train acc:  0.8359375
train loss:  0.41066157817840576
train gradient:  0.2677346688769845
iteration : 5637
train acc:  0.8515625
train loss:  0.3361605405807495
train gradient:  0.24901006856206126
iteration : 5638
train acc:  0.8359375
train loss:  0.3599311113357544
train gradient:  0.2711510410128843
iteration : 5639
train acc:  0.8515625
train loss:  0.3601513206958771
train gradient:  0.21039317985903247
iteration : 5640
train acc:  0.796875
train loss:  0.42599862813949585
train gradient:  0.32946873206569793
iteration : 5641
train acc:  0.8203125
train loss:  0.36235642433166504
train gradient:  0.2713320093039269
iteration : 5642
train acc:  0.8515625
train loss:  0.36226436495780945
train gradient:  0.24101468268849002
iteration : 5643
train acc:  0.8359375
train loss:  0.38646307587623596
train gradient:  0.3109649302062989
iteration : 5644
train acc:  0.8671875
train loss:  0.347395122051239
train gradient:  0.24993867170151543
iteration : 5645
train acc:  0.90625
train loss:  0.2608947157859802
train gradient:  0.20421148277766582
iteration : 5646
train acc:  0.8671875
train loss:  0.3403570055961609
train gradient:  0.21103298484541397
iteration : 5647
train acc:  0.7734375
train loss:  0.4432200491428375
train gradient:  0.26764077379771345
iteration : 5648
train acc:  0.8671875
train loss:  0.34377503395080566
train gradient:  0.23958006785773456
iteration : 5649
train acc:  0.8515625
train loss:  0.30504077672958374
train gradient:  0.14744360792832822
iteration : 5650
train acc:  0.84375
train loss:  0.37582454085350037
train gradient:  0.23338504887793401
iteration : 5651
train acc:  0.84375
train loss:  0.37206900119781494
train gradient:  0.19738690854381652
iteration : 5652
train acc:  0.8359375
train loss:  0.37892553210258484
train gradient:  0.1993936028450708
iteration : 5653
train acc:  0.84375
train loss:  0.3674662709236145
train gradient:  0.1839191997522165
iteration : 5654
train acc:  0.8046875
train loss:  0.39153143763542175
train gradient:  0.23319061067867136
iteration : 5655
train acc:  0.8203125
train loss:  0.36327850818634033
train gradient:  0.22563660054200857
iteration : 5656
train acc:  0.8671875
train loss:  0.335116982460022
train gradient:  0.2049674039670682
iteration : 5657
train acc:  0.9140625
train loss:  0.2968115210533142
train gradient:  0.2611472995425589
iteration : 5658
train acc:  0.8515625
train loss:  0.3219970166683197
train gradient:  0.25295178633784027
iteration : 5659
train acc:  0.8046875
train loss:  0.40115290880203247
train gradient:  0.27608705156409963
iteration : 5660
train acc:  0.84375
train loss:  0.3491550385951996
train gradient:  0.19447312461146815
iteration : 5661
train acc:  0.8515625
train loss:  0.33197203278541565
train gradient:  0.32530339000066283
iteration : 5662
train acc:  0.828125
train loss:  0.3608452379703522
train gradient:  0.28099984513046344
iteration : 5663
train acc:  0.859375
train loss:  0.3210808038711548
train gradient:  0.2156893750884533
iteration : 5664
train acc:  0.8203125
train loss:  0.3574208617210388
train gradient:  0.26552293204559696
iteration : 5665
train acc:  0.7734375
train loss:  0.433531790971756
train gradient:  0.30066100915593946
iteration : 5666
train acc:  0.8671875
train loss:  0.3477027416229248
train gradient:  0.28732390533682545
iteration : 5667
train acc:  0.796875
train loss:  0.5141228437423706
train gradient:  0.35284707520832403
iteration : 5668
train acc:  0.828125
train loss:  0.44540929794311523
train gradient:  0.4275347514399306
iteration : 5669
train acc:  0.8671875
train loss:  0.3282485902309418
train gradient:  0.16828283114861703
iteration : 5670
train acc:  0.8046875
train loss:  0.40382373332977295
train gradient:  0.25228149501659264
iteration : 5671
train acc:  0.828125
train loss:  0.37812793254852295
train gradient:  0.22426760311808985
iteration : 5672
train acc:  0.8671875
train loss:  0.31513911485671997
train gradient:  0.1209546714985009
iteration : 5673
train acc:  0.84375
train loss:  0.3630790114402771
train gradient:  0.34524575552300557
iteration : 5674
train acc:  0.890625
train loss:  0.2524838149547577
train gradient:  0.1778100072922432
iteration : 5675
train acc:  0.84375
train loss:  0.3631056249141693
train gradient:  0.20506654926492687
iteration : 5676
train acc:  0.8828125
train loss:  0.2953220009803772
train gradient:  0.1435533670059348
iteration : 5677
train acc:  0.8359375
train loss:  0.3903873562812805
train gradient:  0.2353063262801433
iteration : 5678
train acc:  0.875
train loss:  0.3413141369819641
train gradient:  0.17771811263014795
iteration : 5679
train acc:  0.8671875
train loss:  0.2913697063922882
train gradient:  0.141713367286867
iteration : 5680
train acc:  0.890625
train loss:  0.3486140966415405
train gradient:  0.2218628614388799
iteration : 5681
train acc:  0.875
train loss:  0.38968732953071594
train gradient:  0.24420290133075948
iteration : 5682
train acc:  0.8515625
train loss:  0.3231200575828552
train gradient:  0.1919705082987876
iteration : 5683
train acc:  0.8046875
train loss:  0.468631774187088
train gradient:  0.3130013231483745
iteration : 5684
train acc:  0.8515625
train loss:  0.34749430418014526
train gradient:  0.19730526125510953
iteration : 5685
train acc:  0.8671875
train loss:  0.36640506982803345
train gradient:  0.25621084827268553
iteration : 5686
train acc:  0.859375
train loss:  0.37032565474510193
train gradient:  0.2932418411975972
iteration : 5687
train acc:  0.8046875
train loss:  0.44786378741264343
train gradient:  0.3598411370265972
iteration : 5688
train acc:  0.8203125
train loss:  0.37052756547927856
train gradient:  0.27754044143200923
iteration : 5689
train acc:  0.828125
train loss:  0.401253342628479
train gradient:  0.2726548526902556
iteration : 5690
train acc:  0.8046875
train loss:  0.3858473300933838
train gradient:  0.35596290450867923
iteration : 5691
train acc:  0.8125
train loss:  0.3794020414352417
train gradient:  0.31016289370408695
iteration : 5692
train acc:  0.8515625
train loss:  0.37925028800964355
train gradient:  0.22201927593479065
iteration : 5693
train acc:  0.84375
train loss:  0.3941139578819275
train gradient:  0.2386119396374121
iteration : 5694
train acc:  0.8203125
train loss:  0.3535010814666748
train gradient:  0.206923788548886
iteration : 5695
train acc:  0.8359375
train loss:  0.3616170287132263
train gradient:  0.22416187590106773
iteration : 5696
train acc:  0.859375
train loss:  0.3159908950328827
train gradient:  0.24728145681636082
iteration : 5697
train acc:  0.796875
train loss:  0.38120824098587036
train gradient:  0.22169345239173882
iteration : 5698
train acc:  0.8828125
train loss:  0.30526238679885864
train gradient:  0.1860407846646052
iteration : 5699
train acc:  0.84375
train loss:  0.32476890087127686
train gradient:  0.1901526304071347
iteration : 5700
train acc:  0.78125
train loss:  0.4272657036781311
train gradient:  0.387245721303838
iteration : 5701
train acc:  0.8984375
train loss:  0.30160045623779297
train gradient:  0.1503671400188131
iteration : 5702
train acc:  0.796875
train loss:  0.3748839497566223
train gradient:  0.2707493845154412
iteration : 5703
train acc:  0.859375
train loss:  0.3082435131072998
train gradient:  0.18899988340227303
iteration : 5704
train acc:  0.8203125
train loss:  0.42319709062576294
train gradient:  0.2552123803115303
iteration : 5705
train acc:  0.796875
train loss:  0.43868276476860046
train gradient:  0.3771125603801054
iteration : 5706
train acc:  0.8046875
train loss:  0.4192221760749817
train gradient:  0.2587395574869369
iteration : 5707
train acc:  0.8671875
train loss:  0.3323710560798645
train gradient:  0.15881841129600197
iteration : 5708
train acc:  0.8671875
train loss:  0.32622236013412476
train gradient:  0.17640298521304368
iteration : 5709
train acc:  0.8046875
train loss:  0.4840931296348572
train gradient:  0.458355462618025
iteration : 5710
train acc:  0.90625
train loss:  0.2596035599708557
train gradient:  0.12758508693206935
iteration : 5711
train acc:  0.875
train loss:  0.31183406710624695
train gradient:  0.22199465114291125
iteration : 5712
train acc:  0.8203125
train loss:  0.38550490140914917
train gradient:  0.1733512796874261
iteration : 5713
train acc:  0.90625
train loss:  0.3585778474807739
train gradient:  0.14320025525397514
iteration : 5714
train acc:  0.875
train loss:  0.30370938777923584
train gradient:  0.18327197202204742
iteration : 5715
train acc:  0.8359375
train loss:  0.4242537021636963
train gradient:  0.29076591167255217
iteration : 5716
train acc:  0.84375
train loss:  0.3750547468662262
train gradient:  0.2949122358841327
iteration : 5717
train acc:  0.828125
train loss:  0.3846001625061035
train gradient:  0.2573719527237056
iteration : 5718
train acc:  0.921875
train loss:  0.2603912949562073
train gradient:  0.17060590318943303
iteration : 5719
train acc:  0.796875
train loss:  0.4296713173389435
train gradient:  0.20846451448568504
iteration : 5720
train acc:  0.8203125
train loss:  0.36222922801971436
train gradient:  0.29915980608954756
iteration : 5721
train acc:  0.8125
train loss:  0.33815455436706543
train gradient:  0.19741318026842472
iteration : 5722
train acc:  0.8359375
train loss:  0.3798162341117859
train gradient:  0.184418467514533
iteration : 5723
train acc:  0.890625
train loss:  0.28617119789123535
train gradient:  0.172742945273133
iteration : 5724
train acc:  0.8359375
train loss:  0.32581403851509094
train gradient:  0.1863083661360052
iteration : 5725
train acc:  0.84375
train loss:  0.3658826947212219
train gradient:  0.23432435883317557
iteration : 5726
train acc:  0.8671875
train loss:  0.2915900945663452
train gradient:  0.19309546591760424
iteration : 5727
train acc:  0.8203125
train loss:  0.37486034631729126
train gradient:  0.23176572123698147
iteration : 5728
train acc:  0.875
train loss:  0.2942330539226532
train gradient:  0.17554267375553245
iteration : 5729
train acc:  0.859375
train loss:  0.40142732858657837
train gradient:  0.3418019534289906
iteration : 5730
train acc:  0.8359375
train loss:  0.3335490822792053
train gradient:  0.17602232155564657
iteration : 5731
train acc:  0.84375
train loss:  0.36266952753067017
train gradient:  0.2566880035626341
iteration : 5732
train acc:  0.7890625
train loss:  0.3771420419216156
train gradient:  0.23078171210362752
iteration : 5733
train acc:  0.9375
train loss:  0.2693701982498169
train gradient:  0.17193125371115947
iteration : 5734
train acc:  0.8125
train loss:  0.32343313097953796
train gradient:  0.2171607536481926
iteration : 5735
train acc:  0.84375
train loss:  0.37328678369522095
train gradient:  0.24212714034029043
iteration : 5736
train acc:  0.8671875
train loss:  0.31520533561706543
train gradient:  0.20432822746595894
iteration : 5737
train acc:  0.796875
train loss:  0.3998912274837494
train gradient:  0.27991794501835665
iteration : 5738
train acc:  0.7890625
train loss:  0.39492014050483704
train gradient:  0.2314132180623492
iteration : 5739
train acc:  0.859375
train loss:  0.32018956542015076
train gradient:  0.2246135115602531
iteration : 5740
train acc:  0.859375
train loss:  0.3378560543060303
train gradient:  0.2730176577953337
iteration : 5741
train acc:  0.8984375
train loss:  0.30206966400146484
train gradient:  0.14274236945907404
iteration : 5742
train acc:  0.84375
train loss:  0.3592272996902466
train gradient:  0.20722511935735016
iteration : 5743
train acc:  0.9140625
train loss:  0.29503142833709717
train gradient:  0.30482045058852303
iteration : 5744
train acc:  0.8125
train loss:  0.4709807336330414
train gradient:  0.385561573067626
iteration : 5745
train acc:  0.78125
train loss:  0.4222918152809143
train gradient:  0.2915523226795964
iteration : 5746
train acc:  0.8046875
train loss:  0.39985227584838867
train gradient:  0.29473002262529563
iteration : 5747
train acc:  0.828125
train loss:  0.3383234739303589
train gradient:  0.20057969287057822
iteration : 5748
train acc:  0.8359375
train loss:  0.3225884437561035
train gradient:  0.3546724230816761
iteration : 5749
train acc:  0.859375
train loss:  0.3011111617088318
train gradient:  0.14843019013495531
iteration : 5750
train acc:  0.859375
train loss:  0.2901355028152466
train gradient:  0.16070286417817997
iteration : 5751
train acc:  0.890625
train loss:  0.2761242091655731
train gradient:  0.20266701795242942
iteration : 5752
train acc:  0.828125
train loss:  0.34597641229629517
train gradient:  0.21465992446511806
iteration : 5753
train acc:  0.8515625
train loss:  0.3364769220352173
train gradient:  0.17556915475587082
iteration : 5754
train acc:  0.78125
train loss:  0.4337746798992157
train gradient:  0.3971310837007199
iteration : 5755
train acc:  0.8828125
train loss:  0.3224197030067444
train gradient:  0.2559449375365196
iteration : 5756
train acc:  0.8125
train loss:  0.42176616191864014
train gradient:  0.28965688675753304
iteration : 5757
train acc:  0.796875
train loss:  0.4420769214630127
train gradient:  0.36214905514239965
iteration : 5758
train acc:  0.875
train loss:  0.34677982330322266
train gradient:  0.2303384803788048
iteration : 5759
train acc:  0.8671875
train loss:  0.3705304265022278
train gradient:  0.2012236888225809
iteration : 5760
train acc:  0.859375
train loss:  0.31583255529403687
train gradient:  0.2749681570616002
iteration : 5761
train acc:  0.8828125
train loss:  0.35712945461273193
train gradient:  0.2486261777621217
iteration : 5762
train acc:  0.8359375
train loss:  0.3668726086616516
train gradient:  0.1867093114827701
iteration : 5763
train acc:  0.8203125
train loss:  0.3542013168334961
train gradient:  0.18801698370832276
iteration : 5764
train acc:  0.828125
train loss:  0.40625137090682983
train gradient:  0.275787684868955
iteration : 5765
train acc:  0.8515625
train loss:  0.325685977935791
train gradient:  0.22385966525051074
iteration : 5766
train acc:  0.859375
train loss:  0.292917400598526
train gradient:  0.12069634589550798
iteration : 5767
train acc:  0.8671875
train loss:  0.30971795320510864
train gradient:  0.27512404736499046
iteration : 5768
train acc:  0.8359375
train loss:  0.33185094594955444
train gradient:  0.19475120084123151
iteration : 5769
train acc:  0.84375
train loss:  0.37009239196777344
train gradient:  0.1858825291947116
iteration : 5770
train acc:  0.859375
train loss:  0.36861008405685425
train gradient:  0.22929982389208942
iteration : 5771
train acc:  0.8828125
train loss:  0.34164318442344666
train gradient:  0.26394799536042673
iteration : 5772
train acc:  0.84375
train loss:  0.32112208008766174
train gradient:  0.18594964203671657
iteration : 5773
train acc:  0.8828125
train loss:  0.30334484577178955
train gradient:  0.21931481623537818
iteration : 5774
train acc:  0.8203125
train loss:  0.360441118478775
train gradient:  0.22301050670182615
iteration : 5775
train acc:  0.828125
train loss:  0.39131301641464233
train gradient:  0.17984568180605326
iteration : 5776
train acc:  0.8046875
train loss:  0.4227254390716553
train gradient:  0.2806882220841583
iteration : 5777
train acc:  0.90625
train loss:  0.2715388536453247
train gradient:  0.12552475845459898
iteration : 5778
train acc:  0.859375
train loss:  0.3125838041305542
train gradient:  0.17128217131184603
iteration : 5779
train acc:  0.859375
train loss:  0.40709343552589417
train gradient:  0.2922861730660304
iteration : 5780
train acc:  0.875
train loss:  0.2913578152656555
train gradient:  0.31407808606322896
iteration : 5781
train acc:  0.84375
train loss:  0.4085465669631958
train gradient:  0.29598187876783366
iteration : 5782
train acc:  0.8125
train loss:  0.42973101139068604
train gradient:  0.40778534319662774
iteration : 5783
train acc:  0.875
train loss:  0.32352936267852783
train gradient:  0.35965731933687217
iteration : 5784
train acc:  0.8828125
train loss:  0.2753968834877014
train gradient:  0.225416972337534
iteration : 5785
train acc:  0.859375
train loss:  0.2910785377025604
train gradient:  0.1936859759781
iteration : 5786
train acc:  0.8671875
train loss:  0.2751008868217468
train gradient:  0.16122208038659522
iteration : 5787
train acc:  0.8125
train loss:  0.3551914393901825
train gradient:  0.3403000546970059
iteration : 5788
train acc:  0.90625
train loss:  0.21938788890838623
train gradient:  0.12947064830313432
iteration : 5789
train acc:  0.765625
train loss:  0.41702696681022644
train gradient:  0.25335337948508485
iteration : 5790
train acc:  0.890625
train loss:  0.3629330098628998
train gradient:  0.21122129133686307
iteration : 5791
train acc:  0.8515625
train loss:  0.30719584226608276
train gradient:  0.13250779833462425
iteration : 5792
train acc:  0.8515625
train loss:  0.3286905884742737
train gradient:  0.1824364305743116
iteration : 5793
train acc:  0.8671875
train loss:  0.28770825266838074
train gradient:  0.14426872190396064
iteration : 5794
train acc:  0.84375
train loss:  0.33286088705062866
train gradient:  0.17733660174966626
iteration : 5795
train acc:  0.8828125
train loss:  0.27201372385025024
train gradient:  0.15700378472147608
iteration : 5796
train acc:  0.84375
train loss:  0.3190847635269165
train gradient:  0.25533071697769827
iteration : 5797
train acc:  0.8203125
train loss:  0.39327937364578247
train gradient:  0.21724735137292833
iteration : 5798
train acc:  0.8359375
train loss:  0.3494603931903839
train gradient:  0.2193235167481475
iteration : 5799
train acc:  0.859375
train loss:  0.34086546301841736
train gradient:  0.24153198874688303
iteration : 5800
train acc:  0.828125
train loss:  0.3532586693763733
train gradient:  0.2654943888295701
iteration : 5801
train acc:  0.828125
train loss:  0.40661266446113586
train gradient:  0.2994626379609292
iteration : 5802
train acc:  0.84375
train loss:  0.32866135239601135
train gradient:  0.2194568108824484
iteration : 5803
train acc:  0.8671875
train loss:  0.28789836168289185
train gradient:  0.20085619675975006
iteration : 5804
train acc:  0.84375
train loss:  0.34408530592918396
train gradient:  0.1786008397318471
iteration : 5805
train acc:  0.90625
train loss:  0.2884948253631592
train gradient:  0.16715479563361044
iteration : 5806
train acc:  0.8359375
train loss:  0.354445219039917
train gradient:  0.17224745605522615
iteration : 5807
train acc:  0.859375
train loss:  0.3320864737033844
train gradient:  0.49086401486812736
iteration : 5808
train acc:  0.875
train loss:  0.3069329857826233
train gradient:  0.1850599666629803
iteration : 5809
train acc:  0.8203125
train loss:  0.3250366747379303
train gradient:  0.21933696473694064
iteration : 5810
train acc:  0.8359375
train loss:  0.3320859372615814
train gradient:  0.19248735028560637
iteration : 5811
train acc:  0.84375
train loss:  0.3729442358016968
train gradient:  0.3013424352191028
iteration : 5812
train acc:  0.765625
train loss:  0.4532153010368347
train gradient:  0.4897610476245561
iteration : 5813
train acc:  0.859375
train loss:  0.30349865555763245
train gradient:  0.1368785420230646
iteration : 5814
train acc:  0.890625
train loss:  0.314836323261261
train gradient:  0.3010801158190464
iteration : 5815
train acc:  0.859375
train loss:  0.34799063205718994
train gradient:  0.21863738077686337
iteration : 5816
train acc:  0.8515625
train loss:  0.3064943850040436
train gradient:  0.2719137535665795
iteration : 5817
train acc:  0.84375
train loss:  0.3242526352405548
train gradient:  0.35681378475140174
iteration : 5818
train acc:  0.8359375
train loss:  0.33132612705230713
train gradient:  0.24342122360070728
iteration : 5819
train acc:  0.890625
train loss:  0.3120397925376892
train gradient:  0.15232388855107332
iteration : 5820
train acc:  0.875
train loss:  0.34998005628585815
train gradient:  0.2452991184298805
iteration : 5821
train acc:  0.875
train loss:  0.33308327198028564
train gradient:  0.1677300587368588
iteration : 5822
train acc:  0.9375
train loss:  0.21827715635299683
train gradient:  0.14489435768711922
iteration : 5823
train acc:  0.828125
train loss:  0.37700963020324707
train gradient:  0.2459904812445102
iteration : 5824
train acc:  0.8046875
train loss:  0.3765377998352051
train gradient:  0.28295657177379324
iteration : 5825
train acc:  0.859375
train loss:  0.31246083974838257
train gradient:  0.20013387838265712
iteration : 5826
train acc:  0.875
train loss:  0.35471194982528687
train gradient:  0.2004259865776138
iteration : 5827
train acc:  0.8046875
train loss:  0.45964276790618896
train gradient:  0.333893437504497
iteration : 5828
train acc:  0.796875
train loss:  0.41947466135025024
train gradient:  0.3194987091141787
iteration : 5829
train acc:  0.78125
train loss:  0.38791096210479736
train gradient:  0.32319175813729967
iteration : 5830
train acc:  0.8203125
train loss:  0.3814902901649475
train gradient:  0.28312213095123745
iteration : 5831
train acc:  0.8515625
train loss:  0.29495158791542053
train gradient:  0.24049407685246854
iteration : 5832
train acc:  0.8671875
train loss:  0.3654174208641052
train gradient:  0.2987147563311754
iteration : 5833
train acc:  0.8671875
train loss:  0.34850603342056274
train gradient:  0.339351824111513
iteration : 5834
train acc:  0.8125
train loss:  0.43292877078056335
train gradient:  0.342670756538566
iteration : 5835
train acc:  0.8671875
train loss:  0.29483410716056824
train gradient:  0.16285648140870596
iteration : 5836
train acc:  0.875
train loss:  0.34297651052474976
train gradient:  0.2560998323759447
iteration : 5837
train acc:  0.84375
train loss:  0.3963305950164795
train gradient:  0.29718485529933164
iteration : 5838
train acc:  0.8125
train loss:  0.36947840452194214
train gradient:  0.2633694134066275
iteration : 5839
train acc:  0.8828125
train loss:  0.3206140398979187
train gradient:  0.16493172299612113
iteration : 5840
train acc:  0.9140625
train loss:  0.22697807848453522
train gradient:  0.15103220678963394
iteration : 5841
train acc:  0.8828125
train loss:  0.2879581153392792
train gradient:  0.18064657009096238
iteration : 5842
train acc:  0.8203125
train loss:  0.4699038863182068
train gradient:  0.36384313436372373
iteration : 5843
train acc:  0.859375
train loss:  0.39644861221313477
train gradient:  0.23226193357897043
iteration : 5844
train acc:  0.84375
train loss:  0.4673842787742615
train gradient:  0.33113595295686193
iteration : 5845
train acc:  0.875
train loss:  0.3047596216201782
train gradient:  0.17390756327881637
iteration : 5846
train acc:  0.7890625
train loss:  0.4119179844856262
train gradient:  0.28407728159400597
iteration : 5847
train acc:  0.8125
train loss:  0.4321899116039276
train gradient:  0.3711733993931605
iteration : 5848
train acc:  0.875
train loss:  0.27163195610046387
train gradient:  0.19125108774646432
iteration : 5849
train acc:  0.8828125
train loss:  0.2806495428085327
train gradient:  0.14397031007804506
iteration : 5850
train acc:  0.859375
train loss:  0.34389758110046387
train gradient:  0.20427623118600124
iteration : 5851
train acc:  0.8671875
train loss:  0.28347834944725037
train gradient:  0.17449345635628488
iteration : 5852
train acc:  0.828125
train loss:  0.43430817127227783
train gradient:  0.23774793921760376
iteration : 5853
train acc:  0.859375
train loss:  0.36533838510513306
train gradient:  0.18152936901650396
iteration : 5854
train acc:  0.828125
train loss:  0.36324894428253174
train gradient:  0.2417317412762181
iteration : 5855
train acc:  0.8046875
train loss:  0.43213456869125366
train gradient:  0.3274621831374382
iteration : 5856
train acc:  0.8828125
train loss:  0.2785927951335907
train gradient:  0.12389865783210892
iteration : 5857
train acc:  0.8359375
train loss:  0.3584362864494324
train gradient:  0.18741079188484522
iteration : 5858
train acc:  0.859375
train loss:  0.3306879699230194
train gradient:  0.2102005868376639
iteration : 5859
train acc:  0.859375
train loss:  0.3115966320037842
train gradient:  0.14785425525806384
iteration : 5860
train acc:  0.8515625
train loss:  0.3362577557563782
train gradient:  0.22572514675978556
iteration : 5861
train acc:  0.828125
train loss:  0.4086838960647583
train gradient:  0.28703992462883826
iteration : 5862
train acc:  0.8828125
train loss:  0.34819233417510986
train gradient:  0.20640707904843017
iteration : 5863
train acc:  0.84375
train loss:  0.3371760845184326
train gradient:  0.23175252475886945
iteration : 5864
train acc:  0.8671875
train loss:  0.3781868815422058
train gradient:  0.3348951863048142
iteration : 5865
train acc:  0.859375
train loss:  0.32464832067489624
train gradient:  0.41964382930678634
iteration : 5866
train acc:  0.890625
train loss:  0.29601621627807617
train gradient:  0.14750200038767436
iteration : 5867
train acc:  0.875
train loss:  0.3298152685165405
train gradient:  0.18163020039257033
iteration : 5868
train acc:  0.859375
train loss:  0.3249053359031677
train gradient:  0.15705396035540745
iteration : 5869
train acc:  0.859375
train loss:  0.34762901067733765
train gradient:  0.20953960143984957
iteration : 5870
train acc:  0.84375
train loss:  0.3698796033859253
train gradient:  0.23020909709852255
iteration : 5871
train acc:  0.84375
train loss:  0.33980751037597656
train gradient:  0.21529572757908752
iteration : 5872
train acc:  0.8359375
train loss:  0.37315618991851807
train gradient:  0.22970052450928344
iteration : 5873
train acc:  0.828125
train loss:  0.3577386140823364
train gradient:  0.18483116610064654
iteration : 5874
train acc:  0.7890625
train loss:  0.3888976275920868
train gradient:  0.24588313223837666
iteration : 5875
train acc:  0.8125
train loss:  0.38664186000823975
train gradient:  0.2834405675367056
iteration : 5876
train acc:  0.8125
train loss:  0.4010595679283142
train gradient:  0.2980554579412662
iteration : 5877
train acc:  0.8515625
train loss:  0.389427125453949
train gradient:  0.2518231698733474
iteration : 5878
train acc:  0.8046875
train loss:  0.400961697101593
train gradient:  0.25434229089261184
iteration : 5879
train acc:  0.8359375
train loss:  0.3666558861732483
train gradient:  0.22595743454113248
iteration : 5880
train acc:  0.828125
train loss:  0.37990301847457886
train gradient:  0.2413110543178602
iteration : 5881
train acc:  0.8515625
train loss:  0.35330134630203247
train gradient:  0.22593612210815753
iteration : 5882
train acc:  0.8125
train loss:  0.3656435012817383
train gradient:  0.20674411876825555
iteration : 5883
train acc:  0.84375
train loss:  0.33948439359664917
train gradient:  0.25871308355534006
iteration : 5884
train acc:  0.90625
train loss:  0.2701433598995209
train gradient:  0.15785704092954955
iteration : 5885
train acc:  0.859375
train loss:  0.3317776918411255
train gradient:  0.18896279965916205
iteration : 5886
train acc:  0.828125
train loss:  0.32079747319221497
train gradient:  0.192401416386899
iteration : 5887
train acc:  0.8515625
train loss:  0.37189528346061707
train gradient:  0.22524927370090414
iteration : 5888
train acc:  0.8671875
train loss:  0.33786556124687195
train gradient:  0.21262903734447033
iteration : 5889
train acc:  0.8671875
train loss:  0.32190293073654175
train gradient:  0.17608516301390287
iteration : 5890
train acc:  0.8828125
train loss:  0.3014463484287262
train gradient:  0.22181624417070717
iteration : 5891
train acc:  0.7890625
train loss:  0.3577446937561035
train gradient:  0.2194443274789913
iteration : 5892
train acc:  0.828125
train loss:  0.3753792643547058
train gradient:  0.33224426842773847
iteration : 5893
train acc:  0.8984375
train loss:  0.34857431054115295
train gradient:  0.17034061752701493
iteration : 5894
train acc:  0.8671875
train loss:  0.32221558690071106
train gradient:  0.15132057738903287
iteration : 5895
train acc:  0.8828125
train loss:  0.33700019121170044
train gradient:  0.21369620426213226
iteration : 5896
train acc:  0.765625
train loss:  0.42584148049354553
train gradient:  0.3221951025533687
iteration : 5897
train acc:  0.8046875
train loss:  0.42274701595306396
train gradient:  0.247552186971068
iteration : 5898
train acc:  0.8203125
train loss:  0.3392605483531952
train gradient:  0.2175970461424622
iteration : 5899
train acc:  0.8359375
train loss:  0.34553202986717224
train gradient:  0.18823878706355568
iteration : 5900
train acc:  0.8203125
train loss:  0.4273991882801056
train gradient:  0.17634013008945065
iteration : 5901
train acc:  0.890625
train loss:  0.3279939889907837
train gradient:  0.1860667252568649
iteration : 5902
train acc:  0.84375
train loss:  0.3206935524940491
train gradient:  0.207251980783013
iteration : 5903
train acc:  0.8984375
train loss:  0.28110483288764954
train gradient:  0.16315615200077455
iteration : 5904
train acc:  0.828125
train loss:  0.34964048862457275
train gradient:  0.23347821647670863
iteration : 5905
train acc:  0.859375
train loss:  0.3332390785217285
train gradient:  0.18607253237087715
iteration : 5906
train acc:  0.8671875
train loss:  0.31599605083465576
train gradient:  0.16985882103184835
iteration : 5907
train acc:  0.84375
train loss:  0.3429218530654907
train gradient:  0.23314986770882498
iteration : 5908
train acc:  0.8515625
train loss:  0.3109091818332672
train gradient:  0.17871286564458028
iteration : 5909
train acc:  0.8828125
train loss:  0.3420328199863434
train gradient:  0.19372451736931467
iteration : 5910
train acc:  0.8125
train loss:  0.4235004186630249
train gradient:  0.30481172528842054
iteration : 5911
train acc:  0.734375
train loss:  0.4841861128807068
train gradient:  0.3095667843456877
iteration : 5912
train acc:  0.859375
train loss:  0.3641953468322754
train gradient:  0.283263875131967
iteration : 5913
train acc:  0.828125
train loss:  0.36203378438949585
train gradient:  0.23472378191782858
iteration : 5914
train acc:  0.859375
train loss:  0.3283866047859192
train gradient:  0.15837095785792485
iteration : 5915
train acc:  0.8671875
train loss:  0.32616567611694336
train gradient:  0.20980770172949104
iteration : 5916
train acc:  0.828125
train loss:  0.4075321555137634
train gradient:  0.24367567701828832
iteration : 5917
train acc:  0.8828125
train loss:  0.31742316484451294
train gradient:  0.14701423699351218
iteration : 5918
train acc:  0.859375
train loss:  0.30984488129615784
train gradient:  0.19830352808544177
iteration : 5919
train acc:  0.8828125
train loss:  0.307623028755188
train gradient:  0.14581067903792955
iteration : 5920
train acc:  0.890625
train loss:  0.33785250782966614
train gradient:  0.1455148307914937
iteration : 5921
train acc:  0.8828125
train loss:  0.3079158067703247
train gradient:  0.11765548414050085
iteration : 5922
train acc:  0.828125
train loss:  0.33544954657554626
train gradient:  0.19096969954772425
iteration : 5923
train acc:  0.8359375
train loss:  0.37134838104248047
train gradient:  0.24419348606557195
iteration : 5924
train acc:  0.8359375
train loss:  0.3202626705169678
train gradient:  0.21487460229389405
iteration : 5925
train acc:  0.8359375
train loss:  0.41322553157806396
train gradient:  0.33517377062523673
iteration : 5926
train acc:  0.859375
train loss:  0.3219326138496399
train gradient:  0.16775061103958572
iteration : 5927
train acc:  0.8671875
train loss:  0.3182838261127472
train gradient:  0.22393220219969065
iteration : 5928
train acc:  0.875
train loss:  0.31771254539489746
train gradient:  0.11001678303886268
iteration : 5929
train acc:  0.84375
train loss:  0.370145320892334
train gradient:  0.26307313288986583
iteration : 5930
train acc:  0.8515625
train loss:  0.33075693249702454
train gradient:  0.19020991077626057
iteration : 5931
train acc:  0.859375
train loss:  0.3263028562068939
train gradient:  0.14526037409683995
iteration : 5932
train acc:  0.8359375
train loss:  0.3714239001274109
train gradient:  0.24191980604304464
iteration : 5933
train acc:  0.796875
train loss:  0.3953307867050171
train gradient:  0.29217459700815895
iteration : 5934
train acc:  0.8046875
train loss:  0.3852742314338684
train gradient:  0.22451512927484613
iteration : 5935
train acc:  0.875
train loss:  0.26086175441741943
train gradient:  0.15521506039885863
iteration : 5936
train acc:  0.8515625
train loss:  0.3426423668861389
train gradient:  0.2019620658193764
iteration : 5937
train acc:  0.8828125
train loss:  0.27557453513145447
train gradient:  0.12662995972427393
iteration : 5938
train acc:  0.84375
train loss:  0.36515045166015625
train gradient:  0.2508603264883418
iteration : 5939
train acc:  0.8671875
train loss:  0.3003487288951874
train gradient:  0.15179324085545978
iteration : 5940
train acc:  0.8671875
train loss:  0.3207216262817383
train gradient:  0.14805881834857287
iteration : 5941
train acc:  0.8671875
train loss:  0.33207106590270996
train gradient:  0.24198623222702864
iteration : 5942
train acc:  0.828125
train loss:  0.36260029673576355
train gradient:  0.2410893752398025
iteration : 5943
train acc:  0.8203125
train loss:  0.3541252613067627
train gradient:  0.27339705947746756
iteration : 5944
train acc:  0.890625
train loss:  0.2773701846599579
train gradient:  0.15897958738872367
iteration : 5945
train acc:  0.859375
train loss:  0.36647137999534607
train gradient:  0.31799968209354434
iteration : 5946
train acc:  0.8125
train loss:  0.48822417855262756
train gradient:  0.3274938114234627
iteration : 5947
train acc:  0.8203125
train loss:  0.4099649488925934
train gradient:  0.3562615876458188
iteration : 5948
train acc:  0.8984375
train loss:  0.29839026927948
train gradient:  0.1635006483441182
iteration : 5949
train acc:  0.8203125
train loss:  0.42243069410324097
train gradient:  0.33663682307483495
iteration : 5950
train acc:  0.8359375
train loss:  0.328802227973938
train gradient:  0.19778929654774108
iteration : 5951
train acc:  0.859375
train loss:  0.3114236891269684
train gradient:  0.1663323950764783
iteration : 5952
train acc:  0.8828125
train loss:  0.292556494474411
train gradient:  0.15939825215699738
iteration : 5953
train acc:  0.859375
train loss:  0.34421518445014954
train gradient:  0.287910638726704
iteration : 5954
train acc:  0.8125
train loss:  0.3721470534801483
train gradient:  0.19001401244686805
iteration : 5955
train acc:  0.828125
train loss:  0.3741176724433899
train gradient:  0.2320554145541329
iteration : 5956
train acc:  0.84375
train loss:  0.3372712731361389
train gradient:  0.2625722062786422
iteration : 5957
train acc:  0.890625
train loss:  0.3184945583343506
train gradient:  0.13211313468228147
iteration : 5958
train acc:  0.8359375
train loss:  0.4190595746040344
train gradient:  0.2383592086890369
iteration : 5959
train acc:  0.8671875
train loss:  0.2879999279975891
train gradient:  0.17995321169892894
iteration : 5960
train acc:  0.84375
train loss:  0.32134222984313965
train gradient:  0.19030455886128161
iteration : 5961
train acc:  0.7734375
train loss:  0.42169389128685
train gradient:  0.2751183472774379
iteration : 5962
train acc:  0.84375
train loss:  0.3273351788520813
train gradient:  0.28521696300560295
iteration : 5963
train acc:  0.8359375
train loss:  0.38616180419921875
train gradient:  0.2596595803393386
iteration : 5964
train acc:  0.8203125
train loss:  0.42792946100234985
train gradient:  0.38956458511746517
iteration : 5965
train acc:  0.859375
train loss:  0.3124704360961914
train gradient:  0.21165636878157668
iteration : 5966
train acc:  0.8359375
train loss:  0.39927592873573303
train gradient:  0.2737806395144442
iteration : 5967
train acc:  0.8203125
train loss:  0.43065881729125977
train gradient:  0.4050496460143721
iteration : 5968
train acc:  0.8046875
train loss:  0.41444700956344604
train gradient:  0.40304704188895324
iteration : 5969
train acc:  0.859375
train loss:  0.3448554277420044
train gradient:  0.23063286654366064
iteration : 5970
train acc:  0.7734375
train loss:  0.44459229707717896
train gradient:  0.23148692066630078
iteration : 5971
train acc:  0.890625
train loss:  0.28555116057395935
train gradient:  0.16136381520751683
iteration : 5972
train acc:  0.8203125
train loss:  0.4202103316783905
train gradient:  0.34606131581097
iteration : 5973
train acc:  0.8359375
train loss:  0.3404613733291626
train gradient:  0.1682288037231135
iteration : 5974
train acc:  0.8359375
train loss:  0.34348180890083313
train gradient:  0.27487691675723497
iteration : 5975
train acc:  0.8828125
train loss:  0.32541829347610474
train gradient:  0.16101281901751793
iteration : 5976
train acc:  0.875
train loss:  0.30891281366348267
train gradient:  0.18835218306304558
iteration : 5977
train acc:  0.84375
train loss:  0.3757375478744507
train gradient:  0.19180393539922092
iteration : 5978
train acc:  0.859375
train loss:  0.34431135654449463
train gradient:  0.22035991406024152
iteration : 5979
train acc:  0.8984375
train loss:  0.26014700531959534
train gradient:  0.12787811203897398
iteration : 5980
train acc:  0.84375
train loss:  0.37699463963508606
train gradient:  0.23996667447766784
iteration : 5981
train acc:  0.84375
train loss:  0.31249839067459106
train gradient:  0.1299011785112819
iteration : 5982
train acc:  0.8046875
train loss:  0.4070107340812683
train gradient:  0.2582875689115571
iteration : 5983
train acc:  0.84375
train loss:  0.41953331232070923
train gradient:  0.2868250935529568
iteration : 5984
train acc:  0.8828125
train loss:  0.3064451217651367
train gradient:  0.15201420188410258
iteration : 5985
train acc:  0.8203125
train loss:  0.37937861680984497
train gradient:  0.2679832974902551
iteration : 5986
train acc:  0.875
train loss:  0.2982178032398224
train gradient:  0.15450769910691942
iteration : 5987
train acc:  0.8515625
train loss:  0.3011604845523834
train gradient:  0.14718096973805028
iteration : 5988
train acc:  0.8515625
train loss:  0.28709226846694946
train gradient:  0.17527871369304962
iteration : 5989
train acc:  0.8515625
train loss:  0.38633328676223755
train gradient:  0.3291599493981553
iteration : 5990
train acc:  0.8046875
train loss:  0.3890571892261505
train gradient:  0.20750007701604892
iteration : 5991
train acc:  0.84375
train loss:  0.35490480065345764
train gradient:  0.2092794313845313
iteration : 5992
train acc:  0.8515625
train loss:  0.3510677218437195
train gradient:  0.1892248394061683
iteration : 5993
train acc:  0.875
train loss:  0.31364893913269043
train gradient:  0.1708640327496964
iteration : 5994
train acc:  0.890625
train loss:  0.26818031072616577
train gradient:  0.13521222990372683
iteration : 5995
train acc:  0.8515625
train loss:  0.32507452368736267
train gradient:  0.19962981316327483
iteration : 5996
train acc:  0.8671875
train loss:  0.29532676935195923
train gradient:  0.154474872761593
iteration : 5997
train acc:  0.875
train loss:  0.27028483152389526
train gradient:  0.19125094115778463
iteration : 5998
train acc:  0.765625
train loss:  0.45101630687713623
train gradient:  0.2709108474270319
iteration : 5999
train acc:  0.8046875
train loss:  0.418979674577713
train gradient:  0.2647969502945029
iteration : 6000
train acc:  0.84375
train loss:  0.47107112407684326
train gradient:  0.46351962406577896
iteration : 6001
train acc:  0.78125
train loss:  0.4827467203140259
train gradient:  0.35460000218189497
iteration : 6002
train acc:  0.8671875
train loss:  0.33282148838043213
train gradient:  0.16977013191785856
iteration : 6003
train acc:  0.8984375
train loss:  0.2999434471130371
train gradient:  0.19717837775766556
iteration : 6004
train acc:  0.8671875
train loss:  0.30069491267204285
train gradient:  0.21347433976436217
iteration : 6005
train acc:  0.828125
train loss:  0.42292696237564087
train gradient:  0.2762204206433156
iteration : 6006
train acc:  0.8828125
train loss:  0.31884288787841797
train gradient:  0.16224362944009033
iteration : 6007
train acc:  0.8515625
train loss:  0.33272436261177063
train gradient:  0.16482045862507172
iteration : 6008
train acc:  0.8828125
train loss:  0.33692359924316406
train gradient:  0.16517094621625103
iteration : 6009
train acc:  0.8515625
train loss:  0.29403629899024963
train gradient:  0.14736512350387013
iteration : 6010
train acc:  0.8671875
train loss:  0.3385019302368164
train gradient:  0.1680481547192958
iteration : 6011
train acc:  0.8125
train loss:  0.30939990282058716
train gradient:  0.17247272788371687
iteration : 6012
train acc:  0.8515625
train loss:  0.356812983751297
train gradient:  0.2067124423331292
iteration : 6013
train acc:  0.8515625
train loss:  0.340675413608551
train gradient:  0.2916736427234921
iteration : 6014
train acc:  0.8046875
train loss:  0.3380964398384094
train gradient:  0.3048996639009981
iteration : 6015
train acc:  0.8203125
train loss:  0.36284521222114563
train gradient:  0.17973724357921259
iteration : 6016
train acc:  0.8359375
train loss:  0.31489479541778564
train gradient:  0.163527178195628
iteration : 6017
train acc:  0.859375
train loss:  0.32960519194602966
train gradient:  0.19168560046567665
iteration : 6018
train acc:  0.8828125
train loss:  0.2976086139678955
train gradient:  0.13069873421321504
iteration : 6019
train acc:  0.8984375
train loss:  0.3058999180793762
train gradient:  0.18423753255728448
iteration : 6020
train acc:  0.859375
train loss:  0.34302711486816406
train gradient:  0.2564684461160505
iteration : 6021
train acc:  0.84375
train loss:  0.35326552391052246
train gradient:  0.21606229584734155
iteration : 6022
train acc:  0.796875
train loss:  0.43891632556915283
train gradient:  0.3023434316857703
iteration : 6023
train acc:  0.84375
train loss:  0.3604666292667389
train gradient:  0.19107227771233226
iteration : 6024
train acc:  0.875
train loss:  0.3162333369255066
train gradient:  0.21847032127596083
iteration : 6025
train acc:  0.8515625
train loss:  0.3095099925994873
train gradient:  0.1476465936651925
iteration : 6026
train acc:  0.7734375
train loss:  0.414537250995636
train gradient:  0.3421756324677855
iteration : 6027
train acc:  0.90625
train loss:  0.28959763050079346
train gradient:  0.18249209045459452
iteration : 6028
train acc:  0.8046875
train loss:  0.36745333671569824
train gradient:  0.2889785586281694
iteration : 6029
train acc:  0.796875
train loss:  0.41827744245529175
train gradient:  0.3333817064435442
iteration : 6030
train acc:  0.90625
train loss:  0.2720375657081604
train gradient:  0.19080167866644243
iteration : 6031
train acc:  0.7734375
train loss:  0.38671427965164185
train gradient:  0.2336487112442251
iteration : 6032
train acc:  0.8515625
train loss:  0.4002711772918701
train gradient:  0.3515904437083101
iteration : 6033
train acc:  0.8984375
train loss:  0.2856823801994324
train gradient:  0.15509459428380332
iteration : 6034
train acc:  0.84375
train loss:  0.3336690664291382
train gradient:  0.14760069161575995
iteration : 6035
train acc:  0.8984375
train loss:  0.2917168438434601
train gradient:  0.1791415294515154
iteration : 6036
train acc:  0.8515625
train loss:  0.34301233291625977
train gradient:  0.27476710630623313
iteration : 6037
train acc:  0.7734375
train loss:  0.4349343776702881
train gradient:  0.27583688299736314
iteration : 6038
train acc:  0.875
train loss:  0.3230724632740021
train gradient:  0.21080415626559068
iteration : 6039
train acc:  0.8828125
train loss:  0.3397873640060425
train gradient:  0.1686575809407388
iteration : 6040
train acc:  0.859375
train loss:  0.3482045829296112
train gradient:  0.2511948001567242
iteration : 6041
train acc:  0.875
train loss:  0.36410561203956604
train gradient:  0.24372914854708522
iteration : 6042
train acc:  0.828125
train loss:  0.41878536343574524
train gradient:  0.28283830619829003
iteration : 6043
train acc:  0.84375
train loss:  0.313150554895401
train gradient:  0.17233061797981933
iteration : 6044
train acc:  0.84375
train loss:  0.30100828409194946
train gradient:  0.2348851824148762
iteration : 6045
train acc:  0.828125
train loss:  0.3374829888343811
train gradient:  0.23562421591483584
iteration : 6046
train acc:  0.8671875
train loss:  0.3106129765510559
train gradient:  0.15006949702887487
iteration : 6047
train acc:  0.8359375
train loss:  0.31110814213752747
train gradient:  0.2858118330589118
iteration : 6048
train acc:  0.8359375
train loss:  0.37077999114990234
train gradient:  0.1980811992355016
iteration : 6049
train acc:  0.8203125
train loss:  0.3861415982246399
train gradient:  0.3315188569445056
iteration : 6050
train acc:  0.8671875
train loss:  0.3130219578742981
train gradient:  0.16314469283106325
iteration : 6051
train acc:  0.8515625
train loss:  0.33870917558670044
train gradient:  0.22712536555181814
iteration : 6052
train acc:  0.7890625
train loss:  0.43781498074531555
train gradient:  0.3537082725036218
iteration : 6053
train acc:  0.8984375
train loss:  0.255094051361084
train gradient:  0.16259235599345584
iteration : 6054
train acc:  0.8125
train loss:  0.42524483799934387
train gradient:  0.33208451248975585
iteration : 6055
train acc:  0.8984375
train loss:  0.3468337655067444
train gradient:  0.26227869076253174
iteration : 6056
train acc:  0.859375
train loss:  0.3132665157318115
train gradient:  0.19167080140463855
iteration : 6057
train acc:  0.8984375
train loss:  0.3024921715259552
train gradient:  0.18133367980318488
iteration : 6058
train acc:  0.8671875
train loss:  0.3339259922504425
train gradient:  0.24434519043916525
iteration : 6059
train acc:  0.828125
train loss:  0.3409096896648407
train gradient:  0.23187141149551826
iteration : 6060
train acc:  0.8359375
train loss:  0.37918031215667725
train gradient:  0.3581350646071349
iteration : 6061
train acc:  0.8828125
train loss:  0.28311485052108765
train gradient:  0.24535876035656254
iteration : 6062
train acc:  0.84375
train loss:  0.3774912357330322
train gradient:  0.3133810311982458
iteration : 6063
train acc:  0.84375
train loss:  0.290254145860672
train gradient:  0.19274295587235293
iteration : 6064
train acc:  0.8515625
train loss:  0.347927987575531
train gradient:  0.2620004352523821
iteration : 6065
train acc:  0.84375
train loss:  0.3274478316307068
train gradient:  0.23351066377551333
iteration : 6066
train acc:  0.8828125
train loss:  0.2931048572063446
train gradient:  0.17332820990606906
iteration : 6067
train acc:  0.8359375
train loss:  0.36575061082839966
train gradient:  0.29193274176489226
iteration : 6068
train acc:  0.8671875
train loss:  0.2964615523815155
train gradient:  0.18635928683060327
iteration : 6069
train acc:  0.796875
train loss:  0.42732518911361694
train gradient:  0.35108368262969225
iteration : 6070
train acc:  0.875
train loss:  0.272263765335083
train gradient:  0.13118737377758327
iteration : 6071
train acc:  0.8671875
train loss:  0.359833300113678
train gradient:  0.19907984546643148
iteration : 6072
train acc:  0.8671875
train loss:  0.3026212155818939
train gradient:  0.23248881560413065
iteration : 6073
train acc:  0.8203125
train loss:  0.3470863699913025
train gradient:  0.29350207656237554
iteration : 6074
train acc:  0.796875
train loss:  0.43433940410614014
train gradient:  0.29446924785856765
iteration : 6075
train acc:  0.8671875
train loss:  0.31200987100601196
train gradient:  0.18874006050128683
iteration : 6076
train acc:  0.8203125
train loss:  0.36737632751464844
train gradient:  0.2033056454137538
iteration : 6077
train acc:  0.8515625
train loss:  0.40884929895401
train gradient:  0.2544254588667759
iteration : 6078
train acc:  0.8671875
train loss:  0.32055801153182983
train gradient:  0.24759796725398653
iteration : 6079
train acc:  0.84375
train loss:  0.38553386926651
train gradient:  0.2330040658576477
iteration : 6080
train acc:  0.8515625
train loss:  0.3501007854938507
train gradient:  0.28280843200643035
iteration : 6081
train acc:  0.84375
train loss:  0.3269217312335968
train gradient:  0.23885386399399017
iteration : 6082
train acc:  0.8671875
train loss:  0.37056681513786316
train gradient:  0.2933877823638723
iteration : 6083
train acc:  0.8203125
train loss:  0.3358778953552246
train gradient:  0.23197279316682007
iteration : 6084
train acc:  0.84375
train loss:  0.33965346217155457
train gradient:  0.2078643391774158
iteration : 6085
train acc:  0.8515625
train loss:  0.3119790554046631
train gradient:  0.2585504762792339
iteration : 6086
train acc:  0.8828125
train loss:  0.31198471784591675
train gradient:  0.16547193123075132
iteration : 6087
train acc:  0.8515625
train loss:  0.3913390636444092
train gradient:  0.26115342098667105
iteration : 6088
train acc:  0.8203125
train loss:  0.3684206008911133
train gradient:  0.2228621772638422
iteration : 6089
train acc:  0.8671875
train loss:  0.3502628803253174
train gradient:  0.17891668048687354
iteration : 6090
train acc:  0.9296875
train loss:  0.2600729763507843
train gradient:  0.25013529707704213
iteration : 6091
train acc:  0.8359375
train loss:  0.3611968159675598
train gradient:  0.29967083632838976
iteration : 6092
train acc:  0.875
train loss:  0.3092941343784332
train gradient:  0.17452730822481577
iteration : 6093
train acc:  0.8515625
train loss:  0.36483246088027954
train gradient:  0.27954939405565776
iteration : 6094
train acc:  0.8125
train loss:  0.39045292139053345
train gradient:  0.2578904138506175
iteration : 6095
train acc:  0.8828125
train loss:  0.3012509346008301
train gradient:  0.21799219250208207
iteration : 6096
train acc:  0.8515625
train loss:  0.3343828618526459
train gradient:  0.2234256756704583
iteration : 6097
train acc:  0.8359375
train loss:  0.3857964277267456
train gradient:  0.2498639850840696
iteration : 6098
train acc:  0.84375
train loss:  0.3394918441772461
train gradient:  0.2885909849395955
iteration : 6099
train acc:  0.828125
train loss:  0.3580896854400635
train gradient:  0.20440324905656615
iteration : 6100
train acc:  0.84375
train loss:  0.3652276396751404
train gradient:  0.2260902357212392
iteration : 6101
train acc:  0.859375
train loss:  0.3703168034553528
train gradient:  0.32244488138654526
iteration : 6102
train acc:  0.875
train loss:  0.3406292796134949
train gradient:  0.1718432758113436
iteration : 6103
train acc:  0.7890625
train loss:  0.4489758610725403
train gradient:  0.3562584254164794
iteration : 6104
train acc:  0.8515625
train loss:  0.3297359347343445
train gradient:  0.2017491535301604
iteration : 6105
train acc:  0.8125
train loss:  0.3980415463447571
train gradient:  0.31685559164333527
iteration : 6106
train acc:  0.8515625
train loss:  0.3701660633087158
train gradient:  0.27126726306054816
iteration : 6107
train acc:  0.890625
train loss:  0.33344197273254395
train gradient:  0.1892437430740672
iteration : 6108
train acc:  0.859375
train loss:  0.3989189565181732
train gradient:  0.232345576509717
iteration : 6109
train acc:  0.859375
train loss:  0.3885922431945801
train gradient:  0.21301463010900928
iteration : 6110
train acc:  0.9140625
train loss:  0.26484033465385437
train gradient:  0.14729874429616108
iteration : 6111
train acc:  0.796875
train loss:  0.3959231972694397
train gradient:  0.3022008240321492
iteration : 6112
train acc:  0.8515625
train loss:  0.3230088949203491
train gradient:  0.17155402602708614
iteration : 6113
train acc:  0.828125
train loss:  0.36629098653793335
train gradient:  0.1863677666589871
iteration : 6114
train acc:  0.828125
train loss:  0.3796789050102234
train gradient:  0.24412446916310668
iteration : 6115
train acc:  0.8671875
train loss:  0.30130723118782043
train gradient:  0.18809359452316526
iteration : 6116
train acc:  0.8125
train loss:  0.40306204557418823
train gradient:  0.3071132227432847
iteration : 6117
train acc:  0.9140625
train loss:  0.22691479325294495
train gradient:  0.1151278456872955
iteration : 6118
train acc:  0.890625
train loss:  0.23834912478923798
train gradient:  0.1912960498567144
iteration : 6119
train acc:  0.8984375
train loss:  0.27318066358566284
train gradient:  0.1337554396659831
iteration : 6120
train acc:  0.765625
train loss:  0.3936537206172943
train gradient:  0.30373617487077786
iteration : 6121
train acc:  0.828125
train loss:  0.3689519166946411
train gradient:  0.22308191714087797
iteration : 6122
train acc:  0.8359375
train loss:  0.3334621489048004
train gradient:  0.29268236028623645
iteration : 6123
train acc:  0.8984375
train loss:  0.2626599669456482
train gradient:  0.18569948624739363
iteration : 6124
train acc:  0.859375
train loss:  0.33900952339172363
train gradient:  0.2439983699726663
iteration : 6125
train acc:  0.8359375
train loss:  0.36381515860557556
train gradient:  0.2175115821951496
iteration : 6126
train acc:  0.8125
train loss:  0.40992671251296997
train gradient:  0.2591513553500677
iteration : 6127
train acc:  0.8828125
train loss:  0.28003495931625366
train gradient:  0.14456432901619606
iteration : 6128
train acc:  0.8046875
train loss:  0.42490682005882263
train gradient:  0.29811693339240275
iteration : 6129
train acc:  0.8046875
train loss:  0.4116531014442444
train gradient:  0.24853877114637676
iteration : 6130
train acc:  0.8515625
train loss:  0.3630308508872986
train gradient:  0.21843456170144232
iteration : 6131
train acc:  0.8984375
train loss:  0.2425890862941742
train gradient:  0.15058177070868647
iteration : 6132
train acc:  0.8359375
train loss:  0.36638444662094116
train gradient:  0.2372863206404896
iteration : 6133
train acc:  0.859375
train loss:  0.3323391079902649
train gradient:  0.15961214351574599
iteration : 6134
train acc:  0.8359375
train loss:  0.39301973581314087
train gradient:  0.22846698401442567
iteration : 6135
train acc:  0.8515625
train loss:  0.2904663383960724
train gradient:  0.1708289713060699
iteration : 6136
train acc:  0.8203125
train loss:  0.360312283039093
train gradient:  0.23694388837513045
iteration : 6137
train acc:  0.8984375
train loss:  0.2748523950576782
train gradient:  0.12699008349313068
iteration : 6138
train acc:  0.84375
train loss:  0.3474642038345337
train gradient:  0.30674597202914544
iteration : 6139
train acc:  0.875
train loss:  0.29989588260650635
train gradient:  0.1636574162773465
iteration : 6140
train acc:  0.7890625
train loss:  0.41545289754867554
train gradient:  0.3453080121545367
iteration : 6141
train acc:  0.8671875
train loss:  0.3057875335216522
train gradient:  0.16310197105968685
iteration : 6142
train acc:  0.8515625
train loss:  0.3295394778251648
train gradient:  0.21749342012051065
iteration : 6143
train acc:  0.8515625
train loss:  0.36554116010665894
train gradient:  0.24239316343753842
iteration : 6144
train acc:  0.8203125
train loss:  0.35623621940612793
train gradient:  0.222919397880469
iteration : 6145
train acc:  0.875
train loss:  0.3074706196784973
train gradient:  0.2740421758949946
iteration : 6146
train acc:  0.859375
train loss:  0.3554244041442871
train gradient:  0.18337666478507336
iteration : 6147
train acc:  0.8828125
train loss:  0.32934191823005676
train gradient:  0.15869265569509178
iteration : 6148
train acc:  0.8125
train loss:  0.3787469267845154
train gradient:  0.2710529739286569
iteration : 6149
train acc:  0.84375
train loss:  0.34502044320106506
train gradient:  0.24122393075298404
iteration : 6150
train acc:  0.8359375
train loss:  0.3588888645172119
train gradient:  0.23736054504230364
iteration : 6151
train acc:  0.859375
train loss:  0.33640432357788086
train gradient:  0.15358273898397085
iteration : 6152
train acc:  0.8671875
train loss:  0.3473457992076874
train gradient:  0.21461188421822353
iteration : 6153
train acc:  0.8671875
train loss:  0.32272621989250183
train gradient:  0.22846857589270952
iteration : 6154
train acc:  0.875
train loss:  0.34316179156303406
train gradient:  0.23198910513098525
iteration : 6155
train acc:  0.8203125
train loss:  0.4529944658279419
train gradient:  0.401808804682458
iteration : 6156
train acc:  0.8359375
train loss:  0.34076717495918274
train gradient:  0.2369303549535531
iteration : 6157
train acc:  0.8359375
train loss:  0.3608585596084595
train gradient:  0.17792850570912444
iteration : 6158
train acc:  0.8984375
train loss:  0.28383058309555054
train gradient:  0.14439064016357261
iteration : 6159
train acc:  0.8515625
train loss:  0.3089349865913391
train gradient:  0.2185776428424852
iteration : 6160
train acc:  0.859375
train loss:  0.3970220685005188
train gradient:  0.2766352809402682
iteration : 6161
train acc:  0.828125
train loss:  0.4166461229324341
train gradient:  0.3938993978811804
iteration : 6162
train acc:  0.859375
train loss:  0.3321484625339508
train gradient:  0.19338181985929095
iteration : 6163
train acc:  0.8515625
train loss:  0.3622779846191406
train gradient:  0.29093381747839886
iteration : 6164
train acc:  0.8828125
train loss:  0.35051393508911133
train gradient:  0.25778567289732773
iteration : 6165
train acc:  0.8203125
train loss:  0.34368282556533813
train gradient:  0.25228743768342576
iteration : 6166
train acc:  0.828125
train loss:  0.43072208762168884
train gradient:  0.34559285695864855
iteration : 6167
train acc:  0.90625
train loss:  0.2758241891860962
train gradient:  0.20748068598010377
iteration : 6168
train acc:  0.828125
train loss:  0.3758973479270935
train gradient:  0.2815365144408748
iteration : 6169
train acc:  0.84375
train loss:  0.4049185514450073
train gradient:  0.24710578560529614
iteration : 6170
train acc:  0.8828125
train loss:  0.309065580368042
train gradient:  0.1963163881288248
iteration : 6171
train acc:  0.8203125
train loss:  0.41109687089920044
train gradient:  0.24836453909319323
iteration : 6172
train acc:  0.8359375
train loss:  0.413878858089447
train gradient:  0.30961337418300755
iteration : 6173
train acc:  0.796875
train loss:  0.38712000846862793
train gradient:  0.29294440959964846
iteration : 6174
train acc:  0.859375
train loss:  0.34806180000305176
train gradient:  0.17858947814884818
iteration : 6175
train acc:  0.875
train loss:  0.36215895414352417
train gradient:  0.16662257415409676
iteration : 6176
train acc:  0.8203125
train loss:  0.4378018081188202
train gradient:  0.2829311249604846
iteration : 6177
train acc:  0.8515625
train loss:  0.31163927912712097
train gradient:  0.23145299841505734
iteration : 6178
train acc:  0.8671875
train loss:  0.32600343227386475
train gradient:  0.19141786138160097
iteration : 6179
train acc:  0.8359375
train loss:  0.36467698216438293
train gradient:  0.2885396547380529
iteration : 6180
train acc:  0.875
train loss:  0.2908409535884857
train gradient:  0.16566690213135485
iteration : 6181
train acc:  0.859375
train loss:  0.3875430226325989
train gradient:  0.3403833291491745
iteration : 6182
train acc:  0.875
train loss:  0.3204544186592102
train gradient:  0.19595468537983657
iteration : 6183
train acc:  0.84375
train loss:  0.33743083477020264
train gradient:  0.14630562194581376
iteration : 6184
train acc:  0.828125
train loss:  0.3392638564109802
train gradient:  0.204660053482848
iteration : 6185
train acc:  0.875
train loss:  0.31594574451446533
train gradient:  0.164020278780675
iteration : 6186
train acc:  0.859375
train loss:  0.35120877623558044
train gradient:  0.18353608019951942
iteration : 6187
train acc:  0.859375
train loss:  0.34768491983413696
train gradient:  0.26683599622748366
iteration : 6188
train acc:  0.8828125
train loss:  0.29559755325317383
train gradient:  0.15895253219235683
iteration : 6189
train acc:  0.84375
train loss:  0.37585878372192383
train gradient:  0.3437464616603566
iteration : 6190
train acc:  0.7734375
train loss:  0.39921361207962036
train gradient:  0.318036444789288
iteration : 6191
train acc:  0.8828125
train loss:  0.2630172073841095
train gradient:  0.1822978304579441
iteration : 6192
train acc:  0.890625
train loss:  0.3037167191505432
train gradient:  0.13948315910018894
iteration : 6193
train acc:  0.8515625
train loss:  0.2818574905395508
train gradient:  0.13250341216783834
iteration : 6194
train acc:  0.859375
train loss:  0.3378264009952545
train gradient:  0.20118331640121506
iteration : 6195
train acc:  0.828125
train loss:  0.36445555090904236
train gradient:  0.3384848630201881
iteration : 6196
train acc:  0.8203125
train loss:  0.337150901556015
train gradient:  0.18406748734761208
iteration : 6197
train acc:  0.828125
train loss:  0.4082045257091522
train gradient:  0.2704882615138404
iteration : 6198
train acc:  0.8203125
train loss:  0.3581201136112213
train gradient:  0.2342166008544531
iteration : 6199
train acc:  0.84375
train loss:  0.36017757654190063
train gradient:  0.17127874080751387
iteration : 6200
train acc:  0.84375
train loss:  0.3266373872756958
train gradient:  0.1554590572687633
iteration : 6201
train acc:  0.8359375
train loss:  0.3379003405570984
train gradient:  0.22247649011345488
iteration : 6202
train acc:  0.9140625
train loss:  0.2906891107559204
train gradient:  0.27768967049941784
iteration : 6203
train acc:  0.890625
train loss:  0.3158223032951355
train gradient:  0.19019926871220805
iteration : 6204
train acc:  0.8515625
train loss:  0.33562228083610535
train gradient:  0.2066416925673682
iteration : 6205
train acc:  0.84375
train loss:  0.328252375125885
train gradient:  0.22759554576933239
iteration : 6206
train acc:  0.828125
train loss:  0.3791363835334778
train gradient:  0.2807919699813824
iteration : 6207
train acc:  0.78125
train loss:  0.45101091265678406
train gradient:  0.3783558482971253
iteration : 6208
train acc:  0.8828125
train loss:  0.315684974193573
train gradient:  0.18139300502331118
iteration : 6209
train acc:  0.7421875
train loss:  0.5065203905105591
train gradient:  0.432440340985687
iteration : 6210
train acc:  0.8359375
train loss:  0.34192365407943726
train gradient:  0.20284229856419544
iteration : 6211
train acc:  0.8125
train loss:  0.37370067834854126
train gradient:  0.20181273413670492
iteration : 6212
train acc:  0.8359375
train loss:  0.2973649501800537
train gradient:  0.17723421385349547
iteration : 6213
train acc:  0.8203125
train loss:  0.3944310247898102
train gradient:  0.29818685083285934
iteration : 6214
train acc:  0.8359375
train loss:  0.3658580183982849
train gradient:  0.28226871460584063
iteration : 6215
train acc:  0.859375
train loss:  0.3001527190208435
train gradient:  0.2429337391191499
iteration : 6216
train acc:  0.8359375
train loss:  0.3618604242801666
train gradient:  0.20374509145145692
iteration : 6217
train acc:  0.828125
train loss:  0.35837048292160034
train gradient:  0.21920991308981908
iteration : 6218
train acc:  0.84375
train loss:  0.34063130617141724
train gradient:  0.13688991445807464
iteration : 6219
train acc:  0.7890625
train loss:  0.4052058458328247
train gradient:  0.2885671053280516
iteration : 6220
train acc:  0.875
train loss:  0.3095197081565857
train gradient:  0.34918156045996884
iteration : 6221
train acc:  0.8515625
train loss:  0.3498961925506592
train gradient:  0.19799147210502632
iteration : 6222
train acc:  0.8515625
train loss:  0.35363566875457764
train gradient:  0.2733659892696669
iteration : 6223
train acc:  0.8671875
train loss:  0.3212961256504059
train gradient:  0.1836604006848354
iteration : 6224
train acc:  0.859375
train loss:  0.354053795337677
train gradient:  0.3214163133179486
iteration : 6225
train acc:  0.890625
train loss:  0.3447442054748535
train gradient:  0.2192797068342844
iteration : 6226
train acc:  0.8046875
train loss:  0.3924444317817688
train gradient:  0.24494190799281973
iteration : 6227
train acc:  0.8671875
train loss:  0.29164549708366394
train gradient:  0.23495043431828377
iteration : 6228
train acc:  0.8984375
train loss:  0.2752693295478821
train gradient:  0.12258926690686262
iteration : 6229
train acc:  0.8203125
train loss:  0.34746497869491577
train gradient:  0.2906683731808437
iteration : 6230
train acc:  0.8671875
train loss:  0.2985076308250427
train gradient:  0.18126371990193824
iteration : 6231
train acc:  0.8359375
train loss:  0.31719011068344116
train gradient:  0.16684454128538045
iteration : 6232
train acc:  0.859375
train loss:  0.3226344585418701
train gradient:  0.18046959948972843
iteration : 6233
train acc:  0.8828125
train loss:  0.36010563373565674
train gradient:  0.32046568748001675
iteration : 6234
train acc:  0.828125
train loss:  0.3617919087409973
train gradient:  0.19324356270035076
iteration : 6235
train acc:  0.8203125
train loss:  0.3817174434661865
train gradient:  0.22390896079435973
iteration : 6236
train acc:  0.8359375
train loss:  0.34724968671798706
train gradient:  0.22496318735328308
iteration : 6237
train acc:  0.875
train loss:  0.32651200890541077
train gradient:  0.2653494718678458
iteration : 6238
train acc:  0.8203125
train loss:  0.413124144077301
train gradient:  0.29370709571738646
iteration : 6239
train acc:  0.84375
train loss:  0.348766565322876
train gradient:  0.2520953519650172
iteration : 6240
train acc:  0.875
train loss:  0.3075697720050812
train gradient:  0.1717548911324484
iteration : 6241
train acc:  0.8515625
train loss:  0.3663276433944702
train gradient:  0.3040352527396925
iteration : 6242
train acc:  0.859375
train loss:  0.3020608425140381
train gradient:  0.1874787107846379
iteration : 6243
train acc:  0.8515625
train loss:  0.34620147943496704
train gradient:  0.24875393640633156
iteration : 6244
train acc:  0.8515625
train loss:  0.3562834858894348
train gradient:  0.2653023153115747
iteration : 6245
train acc:  0.8515625
train loss:  0.366258442401886
train gradient:  0.18050633466602212
iteration : 6246
train acc:  0.8359375
train loss:  0.33992695808410645
train gradient:  0.245073973158336
iteration : 6247
train acc:  0.828125
train loss:  0.385474294424057
train gradient:  0.5689013227324369
iteration : 6248
train acc:  0.890625
train loss:  0.2915831208229065
train gradient:  0.23105645736228442
iteration : 6249
train acc:  0.8046875
train loss:  0.4682231843471527
train gradient:  0.37477128522258235
iteration : 6250
train acc:  0.84375
train loss:  0.3568652272224426
train gradient:  0.2179906631467329
iteration : 6251
train acc:  0.8828125
train loss:  0.2527443766593933
train gradient:  0.1429666397172376
iteration : 6252
train acc:  0.84375
train loss:  0.36279505491256714
train gradient:  0.29532486502555344
iteration : 6253
train acc:  0.8359375
train loss:  0.36193370819091797
train gradient:  0.20870466774603086
iteration : 6254
train acc:  0.859375
train loss:  0.328693687915802
train gradient:  0.3001508945105407
iteration : 6255
train acc:  0.8125
train loss:  0.3728201687335968
train gradient:  0.17038821131248302
iteration : 6256
train acc:  0.8671875
train loss:  0.2638702988624573
train gradient:  0.11598760349408441
iteration : 6257
train acc:  0.859375
train loss:  0.3364388644695282
train gradient:  0.2405701262759767
iteration : 6258
train acc:  0.859375
train loss:  0.3030904531478882
train gradient:  0.30947876315812295
iteration : 6259
train acc:  0.7890625
train loss:  0.41415858268737793
train gradient:  0.3372769781993031
iteration : 6260
train acc:  0.8359375
train loss:  0.3504241108894348
train gradient:  0.21923146142269245
iteration : 6261
train acc:  0.8984375
train loss:  0.28358229994773865
train gradient:  0.15668893188613603
iteration : 6262
train acc:  0.875
train loss:  0.2984224557876587
train gradient:  0.16201644377089863
iteration : 6263
train acc:  0.8671875
train loss:  0.3117567300796509
train gradient:  0.17035255396363447
iteration : 6264
train acc:  0.8515625
train loss:  0.35864585638046265
train gradient:  0.3226596935467159
iteration : 6265
train acc:  0.8125
train loss:  0.376595139503479
train gradient:  0.42000850274504814
iteration : 6266
train acc:  0.8203125
train loss:  0.34023839235305786
train gradient:  0.20252419163316454
iteration : 6267
train acc:  0.859375
train loss:  0.33652132749557495
train gradient:  0.16414088256769294
iteration : 6268
train acc:  0.78125
train loss:  0.4608183801174164
train gradient:  0.29821237207266427
iteration : 6269
train acc:  0.8515625
train loss:  0.33340102434158325
train gradient:  0.13866297347091622
iteration : 6270
train acc:  0.828125
train loss:  0.3471755087375641
train gradient:  0.15285324951197748
iteration : 6271
train acc:  0.8828125
train loss:  0.32771944999694824
train gradient:  0.29625149790753086
iteration : 6272
train acc:  0.8125
train loss:  0.3692934513092041
train gradient:  0.36718656282008055
iteration : 6273
train acc:  0.7734375
train loss:  0.43501022458076477
train gradient:  0.29031580094946846
iteration : 6274
train acc:  0.84375
train loss:  0.39129871129989624
train gradient:  0.2567331621296356
iteration : 6275
train acc:  0.8515625
train loss:  0.33574509620666504
train gradient:  0.20295486966360754
iteration : 6276
train acc:  0.875
train loss:  0.29785841703414917
train gradient:  0.21873071707761182
iteration : 6277
train acc:  0.921875
train loss:  0.22633546590805054
train gradient:  0.16500441100347502
iteration : 6278
train acc:  0.890625
train loss:  0.2619212865829468
train gradient:  0.12201520289765859
iteration : 6279
train acc:  0.7890625
train loss:  0.37484508752822876
train gradient:  0.28981096973641485
iteration : 6280
train acc:  0.859375
train loss:  0.30033552646636963
train gradient:  0.19098552121762918
iteration : 6281
train acc:  0.8203125
train loss:  0.39946722984313965
train gradient:  0.2408092408384449
iteration : 6282
train acc:  0.7890625
train loss:  0.4474678635597229
train gradient:  0.39593567606327984
iteration : 6283
train acc:  0.8125
train loss:  0.3953460454940796
train gradient:  0.2187793332755319
iteration : 6284
train acc:  0.828125
train loss:  0.3596200942993164
train gradient:  0.21965625313079096
iteration : 6285
train acc:  0.796875
train loss:  0.46243488788604736
train gradient:  0.3066737980812173
iteration : 6286
train acc:  0.8828125
train loss:  0.33622366189956665
train gradient:  0.18104749205484957
iteration : 6287
train acc:  0.875
train loss:  0.3137747347354889
train gradient:  0.18061697322285863
iteration : 6288
train acc:  0.8359375
train loss:  0.41041529178619385
train gradient:  0.2878082354048786
iteration : 6289
train acc:  0.828125
train loss:  0.3484959900379181
train gradient:  0.20616975748474148
iteration : 6290
train acc:  0.859375
train loss:  0.35374715924263
train gradient:  0.23144272667781865
iteration : 6291
train acc:  0.796875
train loss:  0.39778244495391846
train gradient:  0.2791181492786219
iteration : 6292
train acc:  0.890625
train loss:  0.26013022661209106
train gradient:  0.1619447160179015
iteration : 6293
train acc:  0.8515625
train loss:  0.38465940952301025
train gradient:  0.21663594979755516
iteration : 6294
train acc:  0.8515625
train loss:  0.38264918327331543
train gradient:  0.2242768195522828
iteration : 6295
train acc:  0.84375
train loss:  0.39553672075271606
train gradient:  0.2293713240574099
iteration : 6296
train acc:  0.8515625
train loss:  0.34474003314971924
train gradient:  0.25258846573958954
iteration : 6297
train acc:  0.8828125
train loss:  0.31255871057510376
train gradient:  0.2533702167739811
iteration : 6298
train acc:  0.8359375
train loss:  0.35114604234695435
train gradient:  0.23060057013331683
iteration : 6299
train acc:  0.8515625
train loss:  0.3258803188800812
train gradient:  0.211111991432412
iteration : 6300
train acc:  0.8203125
train loss:  0.34289833903312683
train gradient:  0.1902348811375974
iteration : 6301
train acc:  0.828125
train loss:  0.4387485086917877
train gradient:  0.40030452148830714
iteration : 6302
train acc:  0.8359375
train loss:  0.3352224826812744
train gradient:  0.21594108660324843
iteration : 6303
train acc:  0.890625
train loss:  0.29621562361717224
train gradient:  0.19040860171573076
iteration : 6304
train acc:  0.859375
train loss:  0.3303685188293457
train gradient:  0.1814533912119213
iteration : 6305
train acc:  0.8671875
train loss:  0.3580673635005951
train gradient:  0.25704572960314453
iteration : 6306
train acc:  0.8359375
train loss:  0.3863162398338318
train gradient:  0.2806417943902297
iteration : 6307
train acc:  0.8203125
train loss:  0.3814960718154907
train gradient:  0.2454709729087241
iteration : 6308
train acc:  0.8359375
train loss:  0.34297293424606323
train gradient:  0.19338964806780198
iteration : 6309
train acc:  0.859375
train loss:  0.367084801197052
train gradient:  0.2065872120402863
iteration : 6310
train acc:  0.84375
train loss:  0.3387976884841919
train gradient:  0.16366743880103435
iteration : 6311
train acc:  0.859375
train loss:  0.2930564880371094
train gradient:  0.13820090719060024
iteration : 6312
train acc:  0.8515625
train loss:  0.3408392667770386
train gradient:  0.2422383889103426
iteration : 6313
train acc:  0.84375
train loss:  0.3093392252922058
train gradient:  0.15673132330513756
iteration : 6314
train acc:  0.8125
train loss:  0.3850666880607605
train gradient:  0.21156192666242368
iteration : 6315
train acc:  0.796875
train loss:  0.4251379072666168
train gradient:  0.2563768897532367
iteration : 6316
train acc:  0.84375
train loss:  0.3681793808937073
train gradient:  0.21126982300477007
iteration : 6317
train acc:  0.8828125
train loss:  0.30089884996414185
train gradient:  0.17470551403849394
iteration : 6318
train acc:  0.8984375
train loss:  0.3192596435546875
train gradient:  0.1973512313849724
iteration : 6319
train acc:  0.8125
train loss:  0.4531846046447754
train gradient:  0.32469257440774
iteration : 6320
train acc:  0.875
train loss:  0.3026823401451111
train gradient:  0.1643573171632149
iteration : 6321
train acc:  0.84375
train loss:  0.3142176866531372
train gradient:  0.18328339039601813
iteration : 6322
train acc:  0.8203125
train loss:  0.3936593532562256
train gradient:  0.2600563597187353
iteration : 6323
train acc:  0.765625
train loss:  0.415513277053833
train gradient:  0.32442545725407507
iteration : 6324
train acc:  0.8984375
train loss:  0.23840543627738953
train gradient:  0.1110234250471128
iteration : 6325
train acc:  0.859375
train loss:  0.38198012113571167
train gradient:  0.28785599800386635
iteration : 6326
train acc:  0.875
train loss:  0.2933087646961212
train gradient:  0.1759457194605959
iteration : 6327
train acc:  0.8359375
train loss:  0.37080785632133484
train gradient:  0.4552405512002407
iteration : 6328
train acc:  0.8984375
train loss:  0.24743884801864624
train gradient:  0.1495977453861655
iteration : 6329
train acc:  0.84375
train loss:  0.34849637746810913
train gradient:  0.20590769499971395
iteration : 6330
train acc:  0.8671875
train loss:  0.3057508170604706
train gradient:  0.24552889114741372
iteration : 6331
train acc:  0.8828125
train loss:  0.2987804114818573
train gradient:  0.16725940810245576
iteration : 6332
train acc:  0.7890625
train loss:  0.40342167019844055
train gradient:  0.2878899232766243
iteration : 6333
train acc:  0.84375
train loss:  0.3359888195991516
train gradient:  0.23847703446016907
iteration : 6334
train acc:  0.84375
train loss:  0.3327513337135315
train gradient:  0.2053748376131468
iteration : 6335
train acc:  0.8828125
train loss:  0.3014996647834778
train gradient:  0.1838046405206184
iteration : 6336
train acc:  0.828125
train loss:  0.3451855480670929
train gradient:  0.1634315745770973
iteration : 6337
train acc:  0.875
train loss:  0.34998589754104614
train gradient:  0.20989786405510621
iteration : 6338
train acc:  0.8671875
train loss:  0.3373701870441437
train gradient:  0.30899999032601394
iteration : 6339
train acc:  0.8125
train loss:  0.36593079566955566
train gradient:  0.269182140014921
iteration : 6340
train acc:  0.890625
train loss:  0.26734742522239685
train gradient:  0.1688604013151949
iteration : 6341
train acc:  0.8515625
train loss:  0.2994689345359802
train gradient:  0.14864594616220567
iteration : 6342
train acc:  0.8046875
train loss:  0.3981421887874603
train gradient:  0.2531114516268849
iteration : 6343
train acc:  0.8515625
train loss:  0.3406323790550232
train gradient:  0.14303034563557956
iteration : 6344
train acc:  0.828125
train loss:  0.331620454788208
train gradient:  0.18662101070410705
iteration : 6345
train acc:  0.8203125
train loss:  0.31802767515182495
train gradient:  0.19419351069194296
iteration : 6346
train acc:  0.8359375
train loss:  0.37887042760849
train gradient:  0.2446547143247061
iteration : 6347
train acc:  0.8046875
train loss:  0.4244838058948517
train gradient:  0.31193329596135383
iteration : 6348
train acc:  0.875
train loss:  0.328321635723114
train gradient:  0.2354958402820013
iteration : 6349
train acc:  0.8515625
train loss:  0.383838415145874
train gradient:  0.22573720852534013
iteration : 6350
train acc:  0.875
train loss:  0.3133583664894104
train gradient:  0.1700572602969531
iteration : 6351
train acc:  0.8203125
train loss:  0.42788559198379517
train gradient:  0.2855960151155167
iteration : 6352
train acc:  0.8046875
train loss:  0.3802153766155243
train gradient:  0.31977647494644457
iteration : 6353
train acc:  0.8671875
train loss:  0.32849636673927307
train gradient:  0.2672691318184366
iteration : 6354
train acc:  0.8515625
train loss:  0.3433268070220947
train gradient:  0.2070122684331148
iteration : 6355
train acc:  0.8671875
train loss:  0.3309512138366699
train gradient:  0.22116020956938415
iteration : 6356
train acc:  0.828125
train loss:  0.39920860528945923
train gradient:  0.2306049376751107
iteration : 6357
train acc:  0.828125
train loss:  0.35975155234336853
train gradient:  0.2979612232583564
iteration : 6358
train acc:  0.8203125
train loss:  0.3639850616455078
train gradient:  0.24781026119603825
iteration : 6359
train acc:  0.8671875
train loss:  0.31584781408309937
train gradient:  0.1719915689306229
iteration : 6360
train acc:  0.84375
train loss:  0.31492525339126587
train gradient:  0.36138553523527134
iteration : 6361
train acc:  0.8125
train loss:  0.3934139609336853
train gradient:  0.2739368362866744
iteration : 6362
train acc:  0.875
train loss:  0.3120523989200592
train gradient:  0.14525840172599927
iteration : 6363
train acc:  0.8984375
train loss:  0.3154286742210388
train gradient:  0.1926509450180512
iteration : 6364
train acc:  0.921875
train loss:  0.23154491186141968
train gradient:  0.18760365811566468
iteration : 6365
train acc:  0.8828125
train loss:  0.29670852422714233
train gradient:  0.15846700845353362
iteration : 6366
train acc:  0.859375
train loss:  0.3239130973815918
train gradient:  0.16429831046606658
iteration : 6367
train acc:  0.859375
train loss:  0.3362081050872803
train gradient:  0.2503093890386724
iteration : 6368
train acc:  0.90625
train loss:  0.27429676055908203
train gradient:  0.15890300213809552
iteration : 6369
train acc:  0.8359375
train loss:  0.3616105914115906
train gradient:  0.33619452984545345
iteration : 6370
train acc:  0.828125
train loss:  0.39629167318344116
train gradient:  0.2898469837726924
iteration : 6371
train acc:  0.8203125
train loss:  0.3940419554710388
train gradient:  0.2953507431843546
iteration : 6372
train acc:  0.7890625
train loss:  0.372717022895813
train gradient:  0.26445866282821956
iteration : 6373
train acc:  0.890625
train loss:  0.29129159450531006
train gradient:  0.2146269471743726
iteration : 6374
train acc:  0.828125
train loss:  0.3801878094673157
train gradient:  0.2606921688452034
iteration : 6375
train acc:  0.875
train loss:  0.34641748666763306
train gradient:  0.17957856878552014
iteration : 6376
train acc:  0.859375
train loss:  0.34927618503570557
train gradient:  0.20480791839779586
iteration : 6377
train acc:  0.890625
train loss:  0.29270029067993164
train gradient:  0.16966651205630173
iteration : 6378
train acc:  0.796875
train loss:  0.3844834566116333
train gradient:  0.29329728768873703
iteration : 6379
train acc:  0.8203125
train loss:  0.3622227907180786
train gradient:  0.2588173042634736
iteration : 6380
train acc:  0.8125
train loss:  0.3590496778488159
train gradient:  0.2012161253244352
iteration : 6381
train acc:  0.890625
train loss:  0.267621248960495
train gradient:  0.204867173886993
iteration : 6382
train acc:  0.8046875
train loss:  0.3771120011806488
train gradient:  0.32132543486278714
iteration : 6383
train acc:  0.7890625
train loss:  0.3818960189819336
train gradient:  0.28555871335652655
iteration : 6384
train acc:  0.8671875
train loss:  0.30481553077697754
train gradient:  0.16534578124131766
iteration : 6385
train acc:  0.8984375
train loss:  0.2984369099140167
train gradient:  0.15485282106918957
iteration : 6386
train acc:  0.8671875
train loss:  0.30082494020462036
train gradient:  0.15619870216304443
iteration : 6387
train acc:  0.8671875
train loss:  0.3371095359325409
train gradient:  0.1476104053247258
iteration : 6388
train acc:  0.8515625
train loss:  0.3339109420776367
train gradient:  0.17475034888139906
iteration : 6389
train acc:  0.875
train loss:  0.32937538623809814
train gradient:  0.3109258724101639
iteration : 6390
train acc:  0.859375
train loss:  0.30240941047668457
train gradient:  0.16711531890244308
iteration : 6391
train acc:  0.84375
train loss:  0.35431718826293945
train gradient:  0.3339297213988471
iteration : 6392
train acc:  0.8125
train loss:  0.39083391427993774
train gradient:  0.30064524062915293
iteration : 6393
train acc:  0.8125
train loss:  0.4056398868560791
train gradient:  0.27358945389666506
iteration : 6394
train acc:  0.859375
train loss:  0.37758520245552063
train gradient:  0.2501276234180308
iteration : 6395
train acc:  0.84375
train loss:  0.35619056224823
train gradient:  0.23361493971696184
iteration : 6396
train acc:  0.796875
train loss:  0.417121946811676
train gradient:  0.6447096186082266
iteration : 6397
train acc:  0.84375
train loss:  0.33218955993652344
train gradient:  0.15754958513967504
iteration : 6398
train acc:  0.8125
train loss:  0.40356409549713135
train gradient:  0.32254638212803266
iteration : 6399
train acc:  0.8828125
train loss:  0.31629085540771484
train gradient:  0.1833548868120909
iteration : 6400
train acc:  0.8203125
train loss:  0.3920223116874695
train gradient:  0.2360774807138086
iteration : 6401
train acc:  0.875
train loss:  0.3247503936290741
train gradient:  0.17934224460226778
iteration : 6402
train acc:  0.8125
train loss:  0.3802349865436554
train gradient:  0.2418561025972918
iteration : 6403
train acc:  0.8125
train loss:  0.4807072877883911
train gradient:  0.5647348145614157
iteration : 6404
train acc:  0.8359375
train loss:  0.33449476957321167
train gradient:  0.135568641654948
iteration : 6405
train acc:  0.875
train loss:  0.2997051477432251
train gradient:  0.19963566223937074
iteration : 6406
train acc:  0.8359375
train loss:  0.3064562678337097
train gradient:  0.1574476698776393
iteration : 6407
train acc:  0.8125
train loss:  0.4305823743343353
train gradient:  0.37997412707770606
iteration : 6408
train acc:  0.859375
train loss:  0.366275817155838
train gradient:  0.20165735192705683
iteration : 6409
train acc:  0.8984375
train loss:  0.25411128997802734
train gradient:  0.15244603574209498
iteration : 6410
train acc:  0.84375
train loss:  0.33747223019599915
train gradient:  0.25306806675868276
iteration : 6411
train acc:  0.859375
train loss:  0.37932807207107544
train gradient:  0.22557481433167165
iteration : 6412
train acc:  0.875
train loss:  0.35727107524871826
train gradient:  0.2197841810754279
iteration : 6413
train acc:  0.828125
train loss:  0.359816312789917
train gradient:  0.23042440517155577
iteration : 6414
train acc:  0.859375
train loss:  0.3316341042518616
train gradient:  0.1968221132281991
iteration : 6415
train acc:  0.875
train loss:  0.29531288146972656
train gradient:  0.1514401352813793
iteration : 6416
train acc:  0.890625
train loss:  0.29360175132751465
train gradient:  0.168343353029968
iteration : 6417
train acc:  0.859375
train loss:  0.281525194644928
train gradient:  0.12307053063278872
iteration : 6418
train acc:  0.8828125
train loss:  0.27881529927253723
train gradient:  0.16889641118391596
iteration : 6419
train acc:  0.890625
train loss:  0.2997106909751892
train gradient:  0.19553259782978044
iteration : 6420
train acc:  0.828125
train loss:  0.34653139114379883
train gradient:  0.1879841919422443
iteration : 6421
train acc:  0.84375
train loss:  0.34123125672340393
train gradient:  0.15376102944985565
iteration : 6422
train acc:  0.875
train loss:  0.32564133405685425
train gradient:  0.22814043348387858
iteration : 6423
train acc:  0.84375
train loss:  0.3624038100242615
train gradient:  0.38154888122149233
iteration : 6424
train acc:  0.7890625
train loss:  0.40308770537376404
train gradient:  0.26863918856784835
iteration : 6425
train acc:  0.875
train loss:  0.3046044111251831
train gradient:  0.2603498065115295
iteration : 6426
train acc:  0.8046875
train loss:  0.40351030230522156
train gradient:  0.2550772431847335
iteration : 6427
train acc:  0.8515625
train loss:  0.30380815267562866
train gradient:  0.23821007296209334
iteration : 6428
train acc:  0.859375
train loss:  0.3845148980617523
train gradient:  0.22256550834088212
iteration : 6429
train acc:  0.828125
train loss:  0.36493852734565735
train gradient:  0.3116202688908475
iteration : 6430
train acc:  0.828125
train loss:  0.43129482865333557
train gradient:  0.2665766337460197
iteration : 6431
train acc:  0.8359375
train loss:  0.32982608675956726
train gradient:  0.26167271469094666
iteration : 6432
train acc:  0.828125
train loss:  0.37281572818756104
train gradient:  0.36063794764170815
iteration : 6433
train acc:  0.875
train loss:  0.2720523178577423
train gradient:  0.12826082727106955
iteration : 6434
train acc:  0.8203125
train loss:  0.3759656846523285
train gradient:  0.21483153082739959
iteration : 6435
train acc:  0.8671875
train loss:  0.2817757725715637
train gradient:  0.17114324949479387
iteration : 6436
train acc:  0.8359375
train loss:  0.3472406268119812
train gradient:  0.22733844997024516
iteration : 6437
train acc:  0.8359375
train loss:  0.37427759170532227
train gradient:  0.2550044570242149
iteration : 6438
train acc:  0.875
train loss:  0.308587908744812
train gradient:  0.25354946324174654
iteration : 6439
train acc:  0.8828125
train loss:  0.262935996055603
train gradient:  0.12331520816086182
iteration : 6440
train acc:  0.84375
train loss:  0.3933565020561218
train gradient:  0.2975244849969147
iteration : 6441
train acc:  0.8671875
train loss:  0.355953186750412
train gradient:  0.26531374685998566
iteration : 6442
train acc:  0.90625
train loss:  0.24049046635627747
train gradient:  0.15703535983591327
iteration : 6443
train acc:  0.8359375
train loss:  0.39141327142715454
train gradient:  0.3059104965042954
iteration : 6444
train acc:  0.890625
train loss:  0.33470863103866577
train gradient:  0.1228895998144032
iteration : 6445
train acc:  0.8359375
train loss:  0.3718775808811188
train gradient:  0.32561299563573787
iteration : 6446
train acc:  0.859375
train loss:  0.3306003510951996
train gradient:  0.17687302408415922
iteration : 6447
train acc:  0.828125
train loss:  0.34894001483917236
train gradient:  0.21805505450643595
iteration : 6448
train acc:  0.890625
train loss:  0.2456938624382019
train gradient:  0.11448049807633782
iteration : 6449
train acc:  0.84375
train loss:  0.3571643829345703
train gradient:  0.1987076930122041
iteration : 6450
train acc:  0.796875
train loss:  0.40647342801094055
train gradient:  0.28945435896555793
iteration : 6451
train acc:  0.8515625
train loss:  0.3406676948070526
train gradient:  0.18952517135762142
iteration : 6452
train acc:  0.8515625
train loss:  0.42664191126823425
train gradient:  0.3911104814899593
iteration : 6453
train acc:  0.8203125
train loss:  0.4695838689804077
train gradient:  0.2953202615024193
iteration : 6454
train acc:  0.859375
train loss:  0.30181658267974854
train gradient:  0.1615888199217368
iteration : 6455
train acc:  0.7890625
train loss:  0.38941606879234314
train gradient:  0.26814899527904845
iteration : 6456
train acc:  0.84375
train loss:  0.32036522030830383
train gradient:  0.14711277956558
iteration : 6457
train acc:  0.828125
train loss:  0.4184706211090088
train gradient:  0.257450354845107
iteration : 6458
train acc:  0.890625
train loss:  0.3276923894882202
train gradient:  0.2361932575200048
iteration : 6459
train acc:  0.8359375
train loss:  0.36389949917793274
train gradient:  0.1839273673897146
iteration : 6460
train acc:  0.84375
train loss:  0.3083757758140564
train gradient:  0.13774973997293466
iteration : 6461
train acc:  0.8359375
train loss:  0.3734273910522461
train gradient:  0.2989143728178044
iteration : 6462
train acc:  0.8828125
train loss:  0.28955909609794617
train gradient:  0.14829895240117635
iteration : 6463
train acc:  0.890625
train loss:  0.3274458944797516
train gradient:  0.22184088689650633
iteration : 6464
train acc:  0.8515625
train loss:  0.3160836100578308
train gradient:  0.18999167351186555
iteration : 6465
train acc:  0.8671875
train loss:  0.29335638880729675
train gradient:  0.1439986800453223
iteration : 6466
train acc:  0.8828125
train loss:  0.2559059262275696
train gradient:  0.15136630109327404
iteration : 6467
train acc:  0.796875
train loss:  0.3608039915561676
train gradient:  0.282564279025789
iteration : 6468
train acc:  0.8671875
train loss:  0.3168349862098694
train gradient:  0.17690041057824213
iteration : 6469
train acc:  0.8359375
train loss:  0.33472132682800293
train gradient:  0.23158415101625657
iteration : 6470
train acc:  0.8515625
train loss:  0.3475422263145447
train gradient:  0.2516292343520228
iteration : 6471
train acc:  0.84375
train loss:  0.369321346282959
train gradient:  0.3112211543151421
iteration : 6472
train acc:  0.875
train loss:  0.27450448274612427
train gradient:  0.1805750304698607
iteration : 6473
train acc:  0.8515625
train loss:  0.3360287845134735
train gradient:  0.26970446842008855
iteration : 6474
train acc:  0.84375
train loss:  0.31807106733322144
train gradient:  0.18595947097990345
iteration : 6475
train acc:  0.859375
train loss:  0.3360980749130249
train gradient:  0.22109559875852303
iteration : 6476
train acc:  0.8359375
train loss:  0.3492737412452698
train gradient:  0.23994487890707683
iteration : 6477
train acc:  0.8671875
train loss:  0.3442245125770569
train gradient:  0.16159815605096517
iteration : 6478
train acc:  0.8125
train loss:  0.39305514097213745
train gradient:  0.26744537612535896
iteration : 6479
train acc:  0.78125
train loss:  0.45999979972839355
train gradient:  0.3234467056613775
iteration : 6480
train acc:  0.8671875
train loss:  0.41192156076431274
train gradient:  0.35927787082601176
iteration : 6481
train acc:  0.890625
train loss:  0.2894859313964844
train gradient:  0.15082879880211936
iteration : 6482
train acc:  0.78125
train loss:  0.47991451621055603
train gradient:  0.3610055805691007
iteration : 6483
train acc:  0.84375
train loss:  0.3710809350013733
train gradient:  0.21370122997427138
iteration : 6484
train acc:  0.8203125
train loss:  0.39286741614341736
train gradient:  0.32707455298516774
iteration : 6485
train acc:  0.828125
train loss:  0.3448949158191681
train gradient:  0.20069049255771765
iteration : 6486
train acc:  0.84375
train loss:  0.35511404275894165
train gradient:  0.1631129005234812
iteration : 6487
train acc:  0.84375
train loss:  0.3291338384151459
train gradient:  0.1830154310956926
iteration : 6488
train acc:  0.8671875
train loss:  0.36137625575065613
train gradient:  0.1591536008853535
iteration : 6489
train acc:  0.8515625
train loss:  0.25700974464416504
train gradient:  0.14146660193934768
iteration : 6490
train acc:  0.890625
train loss:  0.34349608421325684
train gradient:  0.1925098636996342
iteration : 6491
train acc:  0.8125
train loss:  0.34523656964302063
train gradient:  0.21828427526211128
iteration : 6492
train acc:  0.84375
train loss:  0.35952281951904297
train gradient:  0.242359284714234
iteration : 6493
train acc:  0.859375
train loss:  0.34523630142211914
train gradient:  0.2584353057368773
iteration : 6494
train acc:  0.890625
train loss:  0.28082531690597534
train gradient:  0.1332795976748518
iteration : 6495
train acc:  0.8125
train loss:  0.397032767534256
train gradient:  0.25699678247760716
iteration : 6496
train acc:  0.890625
train loss:  0.26476359367370605
train gradient:  0.16223686480927157
iteration : 6497
train acc:  0.84375
train loss:  0.30727797746658325
train gradient:  0.15727775569755673
iteration : 6498
train acc:  0.8515625
train loss:  0.3159770965576172
train gradient:  0.13462529207390034
iteration : 6499
train acc:  0.8359375
train loss:  0.34546539187431335
train gradient:  0.19842264847077967
iteration : 6500
train acc:  0.8125
train loss:  0.37236642837524414
train gradient:  0.27289244267335716
iteration : 6501
train acc:  0.8671875
train loss:  0.31282010674476624
train gradient:  0.1689367283004997
iteration : 6502
train acc:  0.8671875
train loss:  0.30975407361984253
train gradient:  0.1461153901846742
iteration : 6503
train acc:  0.828125
train loss:  0.3395954966545105
train gradient:  0.20100296134982953
iteration : 6504
train acc:  0.8125
train loss:  0.4351602792739868
train gradient:  0.35037333977479374
iteration : 6505
train acc:  0.828125
train loss:  0.32452547550201416
train gradient:  0.1722168180432133
iteration : 6506
train acc:  0.8671875
train loss:  0.3405059576034546
train gradient:  0.19896263753344912
iteration : 6507
train acc:  0.828125
train loss:  0.3495241403579712
train gradient:  0.1757855782894669
iteration : 6508
train acc:  0.8984375
train loss:  0.2852468490600586
train gradient:  0.15304414433056052
iteration : 6509
train acc:  0.8125
train loss:  0.34671586751937866
train gradient:  0.29503088970901414
iteration : 6510
train acc:  0.7890625
train loss:  0.41007736325263977
train gradient:  0.25221304602220596
iteration : 6511
train acc:  0.875
train loss:  0.399127721786499
train gradient:  0.29293316204974496
iteration : 6512
train acc:  0.8828125
train loss:  0.3182956576347351
train gradient:  0.20634695441456802
iteration : 6513
train acc:  0.8515625
train loss:  0.3485972285270691
train gradient:  0.2768375588268444
iteration : 6514
train acc:  0.921875
train loss:  0.28149905800819397
train gradient:  0.1456479802792625
iteration : 6515
train acc:  0.84375
train loss:  0.3336153030395508
train gradient:  0.2286315517766998
iteration : 6516
train acc:  0.8515625
train loss:  0.3306742012500763
train gradient:  0.1506915444051704
iteration : 6517
train acc:  0.890625
train loss:  0.31173574924468994
train gradient:  0.16905799933691396
iteration : 6518
train acc:  0.8984375
train loss:  0.2872270941734314
train gradient:  0.20259571859875863
iteration : 6519
train acc:  0.859375
train loss:  0.31308603286743164
train gradient:  0.2623678140906286
iteration : 6520
train acc:  0.875
train loss:  0.3186298608779907
train gradient:  0.1445881537001391
iteration : 6521
train acc:  0.8828125
train loss:  0.2611175775527954
train gradient:  0.15360499534001631
iteration : 6522
train acc:  0.8359375
train loss:  0.32735416293144226
train gradient:  0.2497081028238321
iteration : 6523
train acc:  0.84375
train loss:  0.39375919103622437
train gradient:  0.27224915283361084
iteration : 6524
train acc:  0.8203125
train loss:  0.40786200761795044
train gradient:  0.35202129400622717
iteration : 6525
train acc:  0.859375
train loss:  0.2938053607940674
train gradient:  0.14876768404852886
iteration : 6526
train acc:  0.8359375
train loss:  0.3415021598339081
train gradient:  0.2142400281586555
iteration : 6527
train acc:  0.859375
train loss:  0.36678874492645264
train gradient:  0.2018183388495028
iteration : 6528
train acc:  0.84375
train loss:  0.35185617208480835
train gradient:  0.33074856675219977
iteration : 6529
train acc:  0.890625
train loss:  0.35757923126220703
train gradient:  0.29259042437786675
iteration : 6530
train acc:  0.8046875
train loss:  0.37852299213409424
train gradient:  0.26895911425368235
iteration : 6531
train acc:  0.890625
train loss:  0.2920644283294678
train gradient:  0.11462286084061585
iteration : 6532
train acc:  0.859375
train loss:  0.33832868933677673
train gradient:  0.2359890542757249
iteration : 6533
train acc:  0.8125
train loss:  0.37793219089508057
train gradient:  0.21257404253273215
iteration : 6534
train acc:  0.8671875
train loss:  0.29848894476890564
train gradient:  0.16274577847577082
iteration : 6535
train acc:  0.875
train loss:  0.3188825249671936
train gradient:  0.40844867973874893
iteration : 6536
train acc:  0.859375
train loss:  0.34005457162857056
train gradient:  0.17454589991910724
iteration : 6537
train acc:  0.84375
train loss:  0.3412969708442688
train gradient:  0.24241187985602614
iteration : 6538
train acc:  0.796875
train loss:  0.44391101598739624
train gradient:  0.29160372694399006
iteration : 6539
train acc:  0.8984375
train loss:  0.29064667224884033
train gradient:  0.1895619370373127
iteration : 6540
train acc:  0.8671875
train loss:  0.2962036430835724
train gradient:  0.20381710233115916
iteration : 6541
train acc:  0.8984375
train loss:  0.28912076354026794
train gradient:  0.1524115383778749
iteration : 6542
train acc:  0.8515625
train loss:  0.33112049102783203
train gradient:  0.1729482298387917
iteration : 6543
train acc:  0.9140625
train loss:  0.30732783675193787
train gradient:  0.16372389301970225
iteration : 6544
train acc:  0.828125
train loss:  0.33393508195877075
train gradient:  0.1758993159632645
iteration : 6545
train acc:  0.84375
train loss:  0.4222590923309326
train gradient:  0.2807974819655961
iteration : 6546
train acc:  0.875
train loss:  0.30099982023239136
train gradient:  0.1609009011504192
iteration : 6547
train acc:  0.8125
train loss:  0.36842429637908936
train gradient:  0.24311444941544502
iteration : 6548
train acc:  0.875
train loss:  0.33256667852401733
train gradient:  0.2162434526841359
iteration : 6549
train acc:  0.8203125
train loss:  0.36297476291656494
train gradient:  0.21388990977538952
iteration : 6550
train acc:  0.8671875
train loss:  0.3342626094818115
train gradient:  0.15277354805329035
iteration : 6551
train acc:  0.8515625
train loss:  0.3538792133331299
train gradient:  0.2748029225085043
iteration : 6552
train acc:  0.875
train loss:  0.35812127590179443
train gradient:  0.3060922099983317
iteration : 6553
train acc:  0.859375
train loss:  0.320361852645874
train gradient:  0.20379357441645646
iteration : 6554
train acc:  0.875
train loss:  0.2934179902076721
train gradient:  0.14049870135496778
iteration : 6555
train acc:  0.8515625
train loss:  0.3506457805633545
train gradient:  0.18983717756684015
iteration : 6556
train acc:  0.8671875
train loss:  0.3314136862754822
train gradient:  0.17289853955283135
iteration : 6557
train acc:  0.8515625
train loss:  0.31747180223464966
train gradient:  0.24235646767894342
iteration : 6558
train acc:  0.84375
train loss:  0.32823505997657776
train gradient:  0.3351858509439888
iteration : 6559
train acc:  0.78125
train loss:  0.40470629930496216
train gradient:  0.29876875552317833
iteration : 6560
train acc:  0.859375
train loss:  0.3552754521369934
train gradient:  0.28403417564694156
iteration : 6561
train acc:  0.8515625
train loss:  0.322683185338974
train gradient:  0.2223759560731305
iteration : 6562
train acc:  0.8515625
train loss:  0.3434603810310364
train gradient:  0.24179379100028198
iteration : 6563
train acc:  0.8671875
train loss:  0.28487956523895264
train gradient:  0.21435465464227027
iteration : 6564
train acc:  0.8359375
train loss:  0.3674126863479614
train gradient:  0.28834058929484396
iteration : 6565
train acc:  0.84375
train loss:  0.3443513810634613
train gradient:  0.21979123028580622
iteration : 6566
train acc:  0.8203125
train loss:  0.4223189949989319
train gradient:  0.2859548031025543
iteration : 6567
train acc:  0.890625
train loss:  0.3261796236038208
train gradient:  0.21259852692737902
iteration : 6568
train acc:  0.890625
train loss:  0.285127192735672
train gradient:  0.12698876286940491
iteration : 6569
train acc:  0.8828125
train loss:  0.32687902450561523
train gradient:  0.20233371304706838
iteration : 6570
train acc:  0.8671875
train loss:  0.32052817940711975
train gradient:  0.14412618916253844
iteration : 6571
train acc:  0.8828125
train loss:  0.2833544611930847
train gradient:  0.14000773150989848
iteration : 6572
train acc:  0.8515625
train loss:  0.38728371262550354
train gradient:  0.3510275106211864
iteration : 6573
train acc:  0.8046875
train loss:  0.38587766885757446
train gradient:  0.5799677664312008
iteration : 6574
train acc:  0.828125
train loss:  0.34447580575942993
train gradient:  0.19469655167281785
iteration : 6575
train acc:  0.875
train loss:  0.33062976598739624
train gradient:  0.19579035191802038
iteration : 6576
train acc:  0.90625
train loss:  0.2158099263906479
train gradient:  0.1150367096085098
iteration : 6577
train acc:  0.84375
train loss:  0.3408905863761902
train gradient:  0.24079724892493526
iteration : 6578
train acc:  0.8984375
train loss:  0.2383372038602829
train gradient:  0.10752651414341884
iteration : 6579
train acc:  0.8359375
train loss:  0.3499281406402588
train gradient:  0.18589410631353998
iteration : 6580
train acc:  0.8671875
train loss:  0.2885974645614624
train gradient:  0.23296474493018934
iteration : 6581
train acc:  0.8671875
train loss:  0.31696000695228577
train gradient:  0.3156538301464795
iteration : 6582
train acc:  0.8671875
train loss:  0.2810836732387543
train gradient:  0.20179574072572776
iteration : 6583
train acc:  0.828125
train loss:  0.37396126985549927
train gradient:  0.22484961230062522
iteration : 6584
train acc:  0.8984375
train loss:  0.2718212604522705
train gradient:  0.20003917071499783
iteration : 6585
train acc:  0.84375
train loss:  0.3580814599990845
train gradient:  0.22545337402220533
iteration : 6586
train acc:  0.8046875
train loss:  0.379705011844635
train gradient:  0.3474564548260821
iteration : 6587
train acc:  0.921875
train loss:  0.26685065031051636
train gradient:  0.13351930959457026
iteration : 6588
train acc:  0.8515625
train loss:  0.3341999650001526
train gradient:  0.2017098758831174
iteration : 6589
train acc:  0.8515625
train loss:  0.3356563150882721
train gradient:  0.2508295740796191
iteration : 6590
train acc:  0.84375
train loss:  0.3345385193824768
train gradient:  0.19602825204043706
iteration : 6591
train acc:  0.8828125
train loss:  0.35221633315086365
train gradient:  0.25758975570711046
iteration : 6592
train acc:  0.8671875
train loss:  0.29564160108566284
train gradient:  0.17810568727169881
iteration : 6593
train acc:  0.8515625
train loss:  0.30831170082092285
train gradient:  0.16492317671565937
iteration : 6594
train acc:  0.8359375
train loss:  0.4330333471298218
train gradient:  0.29939237176618394
iteration : 6595
train acc:  0.8984375
train loss:  0.2377978265285492
train gradient:  0.1237403440618732
iteration : 6596
train acc:  0.8046875
train loss:  0.40641236305236816
train gradient:  0.27453731307959345
iteration : 6597
train acc:  0.875
train loss:  0.3698309659957886
train gradient:  0.25345936763464105
iteration : 6598
train acc:  0.890625
train loss:  0.27823829650878906
train gradient:  0.13609066176356102
iteration : 6599
train acc:  0.7734375
train loss:  0.5228943824768066
train gradient:  0.518536037423613
iteration : 6600
train acc:  0.859375
train loss:  0.32546567916870117
train gradient:  0.18397676408337996
iteration : 6601
train acc:  0.84375
train loss:  0.4263227581977844
train gradient:  0.23470500057717195
iteration : 6602
train acc:  0.796875
train loss:  0.43025994300842285
train gradient:  0.32324934077311945
iteration : 6603
train acc:  0.84375
train loss:  0.3457742929458618
train gradient:  0.205922730551366
iteration : 6604
train acc:  0.859375
train loss:  0.342496395111084
train gradient:  0.38608240967597346
iteration : 6605
train acc:  0.8046875
train loss:  0.383383572101593
train gradient:  0.2791627339400961
iteration : 6606
train acc:  0.8359375
train loss:  0.39913061261177063
train gradient:  0.22419643323860983
iteration : 6607
train acc:  0.8671875
train loss:  0.32042038440704346
train gradient:  0.1583162495892096
iteration : 6608
train acc:  0.859375
train loss:  0.3581083416938782
train gradient:  0.32052359063895386
iteration : 6609
train acc:  0.8203125
train loss:  0.3587855398654938
train gradient:  0.1895784617835124
iteration : 6610
train acc:  0.75
train loss:  0.5063794255256653
train gradient:  0.4352367583684349
iteration : 6611
train acc:  0.8515625
train loss:  0.33192652463912964
train gradient:  0.16250836518241138
iteration : 6612
train acc:  0.8828125
train loss:  0.2895485758781433
train gradient:  0.2222095835924371
iteration : 6613
train acc:  0.8515625
train loss:  0.39815855026245117
train gradient:  0.31674860528250176
iteration : 6614
train acc:  0.90625
train loss:  0.24937641620635986
train gradient:  0.18993655552771443
iteration : 6615
train acc:  0.7890625
train loss:  0.3902776837348938
train gradient:  0.23734326281208262
iteration : 6616
train acc:  0.8671875
train loss:  0.2936092019081116
train gradient:  0.17490177545138869
iteration : 6617
train acc:  0.8203125
train loss:  0.3842275142669678
train gradient:  0.18736990608594023
iteration : 6618
train acc:  0.8828125
train loss:  0.29453712701797485
train gradient:  0.15817527777504015
iteration : 6619
train acc:  0.875
train loss:  0.33884289860725403
train gradient:  0.15068547436522928
iteration : 6620
train acc:  0.8125
train loss:  0.42200207710266113
train gradient:  0.23945535867342876
iteration : 6621
train acc:  0.8671875
train loss:  0.35596609115600586
train gradient:  0.2328663278633439
iteration : 6622
train acc:  0.859375
train loss:  0.3247317671775818
train gradient:  0.1856138527163172
iteration : 6623
train acc:  0.8359375
train loss:  0.3966255187988281
train gradient:  0.1941580485785354
iteration : 6624
train acc:  0.859375
train loss:  0.35366684198379517
train gradient:  0.20532138947957068
iteration : 6625
train acc:  0.875
train loss:  0.3048737347126007
train gradient:  0.16328494665680127
iteration : 6626
train acc:  0.84375
train loss:  0.37149718403816223
train gradient:  0.20289477089960106
iteration : 6627
train acc:  0.875
train loss:  0.32479435205459595
train gradient:  0.17146540995755166
iteration : 6628
train acc:  0.828125
train loss:  0.39657363295555115
train gradient:  0.33409401728792215
iteration : 6629
train acc:  0.859375
train loss:  0.32509171962738037
train gradient:  0.19311881048587698
iteration : 6630
train acc:  0.8125
train loss:  0.41707465052604675
train gradient:  0.1926990848107019
iteration : 6631
train acc:  0.8359375
train loss:  0.3781365752220154
train gradient:  0.28085791618885453
iteration : 6632
train acc:  0.859375
train loss:  0.3587287664413452
train gradient:  0.18925209731948744
iteration : 6633
train acc:  0.8359375
train loss:  0.3461894690990448
train gradient:  0.1631809732619229
iteration : 6634
train acc:  0.8671875
train loss:  0.3031117916107178
train gradient:  0.18520237224380393
iteration : 6635
train acc:  0.8203125
train loss:  0.34939470887184143
train gradient:  0.2259017834501617
iteration : 6636
train acc:  0.859375
train loss:  0.3198837637901306
train gradient:  0.19828494927098245
iteration : 6637
train acc:  0.875
train loss:  0.31607773900032043
train gradient:  0.144825336772676
iteration : 6638
train acc:  0.84375
train loss:  0.3291594386100769
train gradient:  0.19467650204403092
iteration : 6639
train acc:  0.859375
train loss:  0.347927451133728
train gradient:  0.1727526478145296
iteration : 6640
train acc:  0.8359375
train loss:  0.39852380752563477
train gradient:  0.2630986399920559
iteration : 6641
train acc:  0.8828125
train loss:  0.2725760042667389
train gradient:  0.21538516019955373
iteration : 6642
train acc:  0.875
train loss:  0.3349016308784485
train gradient:  0.18008619358274014
iteration : 6643
train acc:  0.8671875
train loss:  0.3556699752807617
train gradient:  0.2138366908708363
iteration : 6644
train acc:  0.84375
train loss:  0.3377639651298523
train gradient:  0.22395167648124986
iteration : 6645
train acc:  0.8125
train loss:  0.4099447429180145
train gradient:  0.23869271726028987
iteration : 6646
train acc:  0.84375
train loss:  0.3478517532348633
train gradient:  0.18260995697659688
iteration : 6647
train acc:  0.8515625
train loss:  0.29371023178100586
train gradient:  0.15048914172484074
iteration : 6648
train acc:  0.828125
train loss:  0.3915311098098755
train gradient:  0.2065227092662827
iteration : 6649
train acc:  0.859375
train loss:  0.41665056347846985
train gradient:  0.3004031140146398
iteration : 6650
train acc:  0.859375
train loss:  0.3169264495372772
train gradient:  0.2580092381591523
iteration : 6651
train acc:  0.796875
train loss:  0.4537671208381653
train gradient:  0.3352816218139163
iteration : 6652
train acc:  0.84375
train loss:  0.3136352598667145
train gradient:  0.15890057632588517
iteration : 6653
train acc:  0.828125
train loss:  0.3277493119239807
train gradient:  0.1160326256569325
iteration : 6654
train acc:  0.890625
train loss:  0.30663347244262695
train gradient:  0.17253321785837536
iteration : 6655
train acc:  0.8828125
train loss:  0.29649507999420166
train gradient:  0.1977426970955018
iteration : 6656
train acc:  0.8671875
train loss:  0.3675110340118408
train gradient:  0.24317363319477736
iteration : 6657
train acc:  0.859375
train loss:  0.31794893741607666
train gradient:  0.20114362631126592
iteration : 6658
train acc:  0.8359375
train loss:  0.3321259319782257
train gradient:  0.22335563166783284
iteration : 6659
train acc:  0.8828125
train loss:  0.30534589290618896
train gradient:  0.1149490741256427
iteration : 6660
train acc:  0.875
train loss:  0.3183688521385193
train gradient:  0.17190301865902669
iteration : 6661
train acc:  0.84375
train loss:  0.359994113445282
train gradient:  0.20621945827928828
iteration : 6662
train acc:  0.84375
train loss:  0.3652207851409912
train gradient:  0.24973780406113205
iteration : 6663
train acc:  0.828125
train loss:  0.32712823152542114
train gradient:  0.14884496693165766
iteration : 6664
train acc:  0.875
train loss:  0.3155837655067444
train gradient:  0.11719285155760077
iteration : 6665
train acc:  0.8046875
train loss:  0.4467966556549072
train gradient:  0.2612459887297468
iteration : 6666
train acc:  0.875
train loss:  0.317091703414917
train gradient:  0.23137240024713274
iteration : 6667
train acc:  0.90625
train loss:  0.27184629440307617
train gradient:  0.22654434336146695
iteration : 6668
train acc:  0.8828125
train loss:  0.34501704573631287
train gradient:  0.20002362985056255
iteration : 6669
train acc:  0.8671875
train loss:  0.2707440257072449
train gradient:  0.15303955032371205
iteration : 6670
train acc:  0.828125
train loss:  0.3869327902793884
train gradient:  0.2397675260225216
iteration : 6671
train acc:  0.7890625
train loss:  0.45118725299835205
train gradient:  0.3847501915767091
iteration : 6672
train acc:  0.8828125
train loss:  0.31691426038742065
train gradient:  0.2681765997048813
iteration : 6673
train acc:  0.8515625
train loss:  0.34443241357803345
train gradient:  0.2636952209090309
iteration : 6674
train acc:  0.859375
train loss:  0.3386523127555847
train gradient:  0.20514380247840744
iteration : 6675
train acc:  0.8125
train loss:  0.44764459133148193
train gradient:  0.3089489352858889
iteration : 6676
train acc:  0.8359375
train loss:  0.407986581325531
train gradient:  0.28311498992552336
iteration : 6677
train acc:  0.8515625
train loss:  0.3155258893966675
train gradient:  0.176717853872015
iteration : 6678
train acc:  0.875
train loss:  0.2935391068458557
train gradient:  0.247748797705017
iteration : 6679
train acc:  0.8671875
train loss:  0.3415411710739136
train gradient:  0.15167602011459114
iteration : 6680
train acc:  0.8359375
train loss:  0.33813703060150146
train gradient:  0.14561252324968327
iteration : 6681
train acc:  0.8203125
train loss:  0.35145190358161926
train gradient:  0.23443186452338233
iteration : 6682
train acc:  0.890625
train loss:  0.27879276871681213
train gradient:  0.149955965154786
iteration : 6683
train acc:  0.796875
train loss:  0.3808150589466095
train gradient:  0.3070857807551907
iteration : 6684
train acc:  0.8125
train loss:  0.3634331524372101
train gradient:  0.18083085899237422
iteration : 6685
train acc:  0.875
train loss:  0.2743723690509796
train gradient:  0.20572352537800664
iteration : 6686
train acc:  0.8359375
train loss:  0.4011848270893097
train gradient:  0.36360509614649383
iteration : 6687
train acc:  0.8125
train loss:  0.376625657081604
train gradient:  0.2312269758116407
iteration : 6688
train acc:  0.8359375
train loss:  0.33487921953201294
train gradient:  0.15657769296852203
iteration : 6689
train acc:  0.890625
train loss:  0.3512694239616394
train gradient:  0.27207848855649813
iteration : 6690
train acc:  0.890625
train loss:  0.28597769141197205
train gradient:  0.1307602897015744
iteration : 6691
train acc:  0.859375
train loss:  0.35679158568382263
train gradient:  0.2586569135601173
iteration : 6692
train acc:  0.8125
train loss:  0.3555212914943695
train gradient:  0.21832404273577483
iteration : 6693
train acc:  0.8125
train loss:  0.422573983669281
train gradient:  0.29819082879528863
iteration : 6694
train acc:  0.8359375
train loss:  0.3611387014389038
train gradient:  0.2197520778419124
iteration : 6695
train acc:  0.84375
train loss:  0.42059585452079773
train gradient:  0.4461121913250381
iteration : 6696
train acc:  0.796875
train loss:  0.4101174771785736
train gradient:  0.27169968454463533
iteration : 6697
train acc:  0.796875
train loss:  0.3798062205314636
train gradient:  0.27066522253076464
iteration : 6698
train acc:  0.859375
train loss:  0.3362957835197449
train gradient:  0.19093763663892266
iteration : 6699
train acc:  0.84375
train loss:  0.31385281682014465
train gradient:  0.1863479811173938
iteration : 6700
train acc:  0.9140625
train loss:  0.25648221373558044
train gradient:  0.16090456754861449
iteration : 6701
train acc:  0.84375
train loss:  0.308702290058136
train gradient:  0.16463513322994291
iteration : 6702
train acc:  0.84375
train loss:  0.35923194885253906
train gradient:  0.2749984948316398
iteration : 6703
train acc:  0.78125
train loss:  0.43887418508529663
train gradient:  0.2670630411316629
iteration : 6704
train acc:  0.7890625
train loss:  0.3856218159198761
train gradient:  0.180762815639044
iteration : 6705
train acc:  0.8671875
train loss:  0.3417397141456604
train gradient:  0.21075466627281353
iteration : 6706
train acc:  0.859375
train loss:  0.3096625804901123
train gradient:  0.16864883128303118
iteration : 6707
train acc:  0.8203125
train loss:  0.4013630151748657
train gradient:  0.30746808464089226
iteration : 6708
train acc:  0.8515625
train loss:  0.3438752293586731
train gradient:  0.21540112296069153
iteration : 6709
train acc:  0.8359375
train loss:  0.3724871575832367
train gradient:  0.21858199448600324
iteration : 6710
train acc:  0.8671875
train loss:  0.3153080940246582
train gradient:  0.16794023742056868
iteration : 6711
train acc:  0.8203125
train loss:  0.3852733373641968
train gradient:  0.23666849414643562
iteration : 6712
train acc:  0.8671875
train loss:  0.3247298002243042
train gradient:  0.15581800050988776
iteration : 6713
train acc:  0.8671875
train loss:  0.3363224267959595
train gradient:  0.18251506647446114
iteration : 6714
train acc:  0.8828125
train loss:  0.2953575849533081
train gradient:  0.14815252757010533
iteration : 6715
train acc:  0.8359375
train loss:  0.39194732904434204
train gradient:  0.26884352135655676
iteration : 6716
train acc:  0.8984375
train loss:  0.22529536485671997
train gradient:  0.10099978563576074
iteration : 6717
train acc:  0.8203125
train loss:  0.40262213349342346
train gradient:  0.2407782455499789
iteration : 6718
train acc:  0.8359375
train loss:  0.3781631290912628
train gradient:  0.246922538399692
iteration : 6719
train acc:  0.90625
train loss:  0.25007933378219604
train gradient:  0.1178055351607795
iteration : 6720
train acc:  0.9140625
train loss:  0.3185039460659027
train gradient:  0.15717595006862956
iteration : 6721
train acc:  0.8125
train loss:  0.3672717213630676
train gradient:  0.18258115284358975
iteration : 6722
train acc:  0.8359375
train loss:  0.38523033261299133
train gradient:  0.39085786320739685
iteration : 6723
train acc:  0.84375
train loss:  0.3061450719833374
train gradient:  0.19631524696776514
iteration : 6724
train acc:  0.890625
train loss:  0.28844574093818665
train gradient:  0.11530545863116456
iteration : 6725
train acc:  0.859375
train loss:  0.34224551916122437
train gradient:  0.23234342000673106
iteration : 6726
train acc:  0.859375
train loss:  0.3218011260032654
train gradient:  0.2172486775046628
iteration : 6727
train acc:  0.7890625
train loss:  0.41412049531936646
train gradient:  0.27214891859344087
iteration : 6728
train acc:  0.875
train loss:  0.3037612736225128
train gradient:  0.15407477975016204
iteration : 6729
train acc:  0.8125
train loss:  0.3867884576320648
train gradient:  0.3234113391579381
iteration : 6730
train acc:  0.890625
train loss:  0.3148861527442932
train gradient:  0.18595477871875762
iteration : 6731
train acc:  0.859375
train loss:  0.28390443325042725
train gradient:  0.11774563038166405
iteration : 6732
train acc:  0.8359375
train loss:  0.35148221254348755
train gradient:  0.20225634235277606
iteration : 6733
train acc:  0.796875
train loss:  0.35431158542633057
train gradient:  0.2745488245405139
iteration : 6734
train acc:  0.828125
train loss:  0.38213664293289185
train gradient:  0.23098869568907285
iteration : 6735
train acc:  0.8515625
train loss:  0.353249728679657
train gradient:  0.22447803561925567
iteration : 6736
train acc:  0.8671875
train loss:  0.32416170835494995
train gradient:  0.28498254376292276
iteration : 6737
train acc:  0.8203125
train loss:  0.41962960362434387
train gradient:  0.2918970722900484
iteration : 6738
train acc:  0.8984375
train loss:  0.25889962911605835
train gradient:  0.12593500289408596
iteration : 6739
train acc:  0.796875
train loss:  0.37095704674720764
train gradient:  0.24414658320316673
iteration : 6740
train acc:  0.78125
train loss:  0.5105388164520264
train gradient:  0.3381914668482281
iteration : 6741
train acc:  0.8359375
train loss:  0.39941996335983276
train gradient:  0.47683525742877136
iteration : 6742
train acc:  0.8671875
train loss:  0.33198118209838867
train gradient:  0.14580674293368318
iteration : 6743
train acc:  0.8671875
train loss:  0.35875529050827026
train gradient:  0.240355283729066
iteration : 6744
train acc:  0.7890625
train loss:  0.3475794196128845
train gradient:  0.1993009180695859
iteration : 6745
train acc:  0.8671875
train loss:  0.2676338851451874
train gradient:  0.2157885383599773
iteration : 6746
train acc:  0.84375
train loss:  0.3502160310745239
train gradient:  0.23156978912446652
iteration : 6747
train acc:  0.8359375
train loss:  0.3497648537158966
train gradient:  0.17809304501924048
iteration : 6748
train acc:  0.859375
train loss:  0.32817506790161133
train gradient:  0.16444013826245252
iteration : 6749
train acc:  0.8359375
train loss:  0.38897186517715454
train gradient:  0.2577676104505589
iteration : 6750
train acc:  0.8515625
train loss:  0.3050995469093323
train gradient:  0.1400346299591098
iteration : 6751
train acc:  0.875
train loss:  0.299915611743927
train gradient:  0.1336776466947749
iteration : 6752
train acc:  0.796875
train loss:  0.403283029794693
train gradient:  0.2669594423206371
iteration : 6753
train acc:  0.8671875
train loss:  0.3401047885417938
train gradient:  0.18357698426778418
iteration : 6754
train acc:  0.84375
train loss:  0.3674941062927246
train gradient:  0.22465778264939534
iteration : 6755
train acc:  0.8515625
train loss:  0.35162580013275146
train gradient:  0.26669075537234105
iteration : 6756
train acc:  0.8359375
train loss:  0.3601522445678711
train gradient:  0.1871391565534919
iteration : 6757
train acc:  0.875
train loss:  0.30639779567718506
train gradient:  0.15851242738943477
iteration : 6758
train acc:  0.875
train loss:  0.2838313579559326
train gradient:  0.196380000192158
iteration : 6759
train acc:  0.90625
train loss:  0.3154066503047943
train gradient:  0.1550096601505679
iteration : 6760
train acc:  0.84375
train loss:  0.3935796618461609
train gradient:  0.1893910651200824
iteration : 6761
train acc:  0.8671875
train loss:  0.28955078125
train gradient:  0.16476455390585415
iteration : 6762
train acc:  0.8046875
train loss:  0.4076065421104431
train gradient:  0.25305557700759174
iteration : 6763
train acc:  0.84375
train loss:  0.3163985311985016
train gradient:  0.26489267706366626
iteration : 6764
train acc:  0.8359375
train loss:  0.36700403690338135
train gradient:  0.3180710149139346
iteration : 6765
train acc:  0.84375
train loss:  0.3139262795448303
train gradient:  0.16549004280995389
iteration : 6766
train acc:  0.8828125
train loss:  0.3074982166290283
train gradient:  0.15076846359514043
iteration : 6767
train acc:  0.8203125
train loss:  0.3645991384983063
train gradient:  0.26916112138442627
iteration : 6768
train acc:  0.875
train loss:  0.3062721788883209
train gradient:  0.17227497208579395
iteration : 6769
train acc:  0.78125
train loss:  0.42289942502975464
train gradient:  0.24927681463389928
iteration : 6770
train acc:  0.8671875
train loss:  0.3051803708076477
train gradient:  0.16024872923513245
iteration : 6771
train acc:  0.875
train loss:  0.32993653416633606
train gradient:  0.1387420465607559
iteration : 6772
train acc:  0.8046875
train loss:  0.38512492179870605
train gradient:  0.3305111234405236
iteration : 6773
train acc:  0.90625
train loss:  0.2787824273109436
train gradient:  0.15936301320643032
iteration : 6774
train acc:  0.84375
train loss:  0.3207376003265381
train gradient:  0.4122687301485529
iteration : 6775
train acc:  0.84375
train loss:  0.30610790848731995
train gradient:  0.1687989065848567
iteration : 6776
train acc:  0.859375
train loss:  0.3284607231616974
train gradient:  0.2145495373560044
iteration : 6777
train acc:  0.8515625
train loss:  0.2834011912345886
train gradient:  0.15628432341071205
iteration : 6778
train acc:  0.84375
train loss:  0.3308951258659363
train gradient:  0.16608744036322354
iteration : 6779
train acc:  0.8671875
train loss:  0.30326521396636963
train gradient:  0.18536155498306087
iteration : 6780
train acc:  0.875
train loss:  0.29975244402885437
train gradient:  0.1406481235255586
iteration : 6781
train acc:  0.8984375
train loss:  0.24720901250839233
train gradient:  0.13164185813905346
iteration : 6782
train acc:  0.8671875
train loss:  0.3161683678627014
train gradient:  0.14373315756786603
iteration : 6783
train acc:  0.8125
train loss:  0.4436454772949219
train gradient:  0.36091711002751137
iteration : 6784
train acc:  0.8359375
train loss:  0.35351380705833435
train gradient:  0.20878010745742362
iteration : 6785
train acc:  0.828125
train loss:  0.3244369626045227
train gradient:  0.17584341730688374
iteration : 6786
train acc:  0.859375
train loss:  0.3793562650680542
train gradient:  0.27875837548078036
iteration : 6787
train acc:  0.84375
train loss:  0.289837121963501
train gradient:  0.171367729806573
iteration : 6788
train acc:  0.8125
train loss:  0.4003114700317383
train gradient:  0.393077426999655
iteration : 6789
train acc:  0.828125
train loss:  0.3418445587158203
train gradient:  0.20879642216262206
iteration : 6790
train acc:  0.84375
train loss:  0.350073903799057
train gradient:  0.2676710512944634
iteration : 6791
train acc:  0.8671875
train loss:  0.3503769636154175
train gradient:  0.3034181858686843
iteration : 6792
train acc:  0.7734375
train loss:  0.4299570322036743
train gradient:  0.28377607416318434
iteration : 6793
train acc:  0.875
train loss:  0.3071466386318207
train gradient:  0.19398652178834003
iteration : 6794
train acc:  0.875
train loss:  0.30247586965560913
train gradient:  0.18317910093502998
iteration : 6795
train acc:  0.9296875
train loss:  0.217710942029953
train gradient:  0.11166063766500949
iteration : 6796
train acc:  0.828125
train loss:  0.33553260564804077
train gradient:  0.16638052488073796
iteration : 6797
train acc:  0.8125
train loss:  0.3454078435897827
train gradient:  0.1733527197828632
iteration : 6798
train acc:  0.859375
train loss:  0.3023850917816162
train gradient:  0.15128348873103498
iteration : 6799
train acc:  0.8203125
train loss:  0.3348158597946167
train gradient:  0.18199381239979368
iteration : 6800
train acc:  0.84375
train loss:  0.3279340863227844
train gradient:  0.1518513419141848
iteration : 6801
train acc:  0.8671875
train loss:  0.3234837055206299
train gradient:  0.18418910028855803
iteration : 6802
train acc:  0.859375
train loss:  0.3761095404624939
train gradient:  0.24822554441855985
iteration : 6803
train acc:  0.8515625
train loss:  0.3545936942100525
train gradient:  0.2163897317332077
iteration : 6804
train acc:  0.890625
train loss:  0.2900206446647644
train gradient:  0.17870866725655976
iteration : 6805
train acc:  0.8359375
train loss:  0.39174145460128784
train gradient:  0.21027288337785083
iteration : 6806
train acc:  0.8359375
train loss:  0.3773956298828125
train gradient:  0.23539939208322913
iteration : 6807
train acc:  0.84375
train loss:  0.28150853514671326
train gradient:  0.19097435988309397
iteration : 6808
train acc:  0.875
train loss:  0.31310808658599854
train gradient:  0.17482717961550673
iteration : 6809
train acc:  0.796875
train loss:  0.38504642248153687
train gradient:  0.2578341803411773
iteration : 6810
train acc:  0.828125
train loss:  0.3879413604736328
train gradient:  0.23799017200604272
iteration : 6811
train acc:  0.8828125
train loss:  0.2925493121147156
train gradient:  0.15223194077788993
iteration : 6812
train acc:  0.8984375
train loss:  0.3151971399784088
train gradient:  0.15450103781133698
iteration : 6813
train acc:  0.8984375
train loss:  0.25923827290534973
train gradient:  0.1973786418098037
iteration : 6814
train acc:  0.8671875
train loss:  0.31600719690322876
train gradient:  0.20218841183256053
iteration : 6815
train acc:  0.8359375
train loss:  0.4090016484260559
train gradient:  0.302468957049069
iteration : 6816
train acc:  0.875
train loss:  0.28669679164886475
train gradient:  0.13732327766298835
iteration : 6817
train acc:  0.8046875
train loss:  0.4667082726955414
train gradient:  0.3871436868608458
iteration : 6818
train acc:  0.875
train loss:  0.3716789782047272
train gradient:  0.26827947991812123
iteration : 6819
train acc:  0.8671875
train loss:  0.311518132686615
train gradient:  0.2216767961282204
iteration : 6820
train acc:  0.828125
train loss:  0.3609459400177002
train gradient:  0.19149404911600582
iteration : 6821
train acc:  0.8359375
train loss:  0.3645772933959961
train gradient:  0.22030255394381532
iteration : 6822
train acc:  0.8125
train loss:  0.4039421081542969
train gradient:  0.23523665988266695
iteration : 6823
train acc:  0.828125
train loss:  0.3250270187854767
train gradient:  0.17326476976067567
iteration : 6824
train acc:  0.8515625
train loss:  0.3080761134624481
train gradient:  0.1424745735651054
iteration : 6825
train acc:  0.84375
train loss:  0.3107296824455261
train gradient:  0.12815541904903494
iteration : 6826
train acc:  0.8671875
train loss:  0.28492024540901184
train gradient:  0.11482462789171823
iteration : 6827
train acc:  0.8359375
train loss:  0.354026198387146
train gradient:  0.2000210952240904
iteration : 6828
train acc:  0.8828125
train loss:  0.32099977135658264
train gradient:  0.19990905753103957
iteration : 6829
train acc:  0.8515625
train loss:  0.35934674739837646
train gradient:  0.2985477969893878
iteration : 6830
train acc:  0.9140625
train loss:  0.2381395399570465
train gradient:  0.10921727073687473
iteration : 6831
train acc:  0.84375
train loss:  0.4045106768608093
train gradient:  0.2910739069070272
iteration : 6832
train acc:  0.828125
train loss:  0.3650824725627899
train gradient:  0.21424019968514857
iteration : 6833
train acc:  0.8515625
train loss:  0.3820649981498718
train gradient:  0.2819718304741259
iteration : 6834
train acc:  0.8515625
train loss:  0.3349437713623047
train gradient:  0.18112137112897325
iteration : 6835
train acc:  0.859375
train loss:  0.34546130895614624
train gradient:  0.18751942817709752
iteration : 6836
train acc:  0.828125
train loss:  0.3139815032482147
train gradient:  0.19176251061852112
iteration : 6837
train acc:  0.8125
train loss:  0.37476715445518494
train gradient:  0.2555946598360385
iteration : 6838
train acc:  0.84375
train loss:  0.44324132800102234
train gradient:  0.30485317487380575
iteration : 6839
train acc:  0.859375
train loss:  0.32002153992652893
train gradient:  0.1981694849443154
iteration : 6840
train acc:  0.8359375
train loss:  0.3423168659210205
train gradient:  0.33273585001976713
iteration : 6841
train acc:  0.8984375
train loss:  0.2743321657180786
train gradient:  0.14677837564689383
iteration : 6842
train acc:  0.8515625
train loss:  0.3665164113044739
train gradient:  0.24178837603642975
iteration : 6843
train acc:  0.875
train loss:  0.3517584800720215
train gradient:  0.18655313398979362
iteration : 6844
train acc:  0.84375
train loss:  0.3936813473701477
train gradient:  0.34025489496316624
iteration : 6845
train acc:  0.84375
train loss:  0.3678003251552582
train gradient:  0.18535076694231317
iteration : 6846
train acc:  0.90625
train loss:  0.27732351422309875
train gradient:  0.12429738664136611
iteration : 6847
train acc:  0.84375
train loss:  0.34245219826698303
train gradient:  0.24544298647456497
iteration : 6848
train acc:  0.875
train loss:  0.35938775539398193
train gradient:  0.30243112867695626
iteration : 6849
train acc:  0.828125
train loss:  0.34578293561935425
train gradient:  0.20652157403701282
iteration : 6850
train acc:  0.7734375
train loss:  0.43401187658309937
train gradient:  0.36802881090814593
iteration : 6851
train acc:  0.828125
train loss:  0.38230636715888977
train gradient:  0.25537440428017144
iteration : 6852
train acc:  0.875
train loss:  0.2958778142929077
train gradient:  0.1822076990267164
iteration : 6853
train acc:  0.734375
train loss:  0.4825727939605713
train gradient:  0.3379620132966959
iteration : 6854
train acc:  0.875
train loss:  0.32642510533332825
train gradient:  0.19771507013972836
iteration : 6855
train acc:  0.8359375
train loss:  0.417951762676239
train gradient:  0.26802286435508094
iteration : 6856
train acc:  0.9140625
train loss:  0.23605230450630188
train gradient:  0.14400537860091656
iteration : 6857
train acc:  0.84375
train loss:  0.31365224719047546
train gradient:  0.21416272086763127
iteration : 6858
train acc:  0.8125
train loss:  0.43136441707611084
train gradient:  0.27089230354153837
iteration : 6859
train acc:  0.8828125
train loss:  0.36143046617507935
train gradient:  0.22041849207323402
iteration : 6860
train acc:  0.8828125
train loss:  0.2741551399230957
train gradient:  0.1858624283092991
iteration : 6861
train acc:  0.8359375
train loss:  0.3169609308242798
train gradient:  0.28092870521835395
iteration : 6862
train acc:  0.8671875
train loss:  0.3207216262817383
train gradient:  0.1890482113283375
iteration : 6863
train acc:  0.7578125
train loss:  0.4719001352787018
train gradient:  0.3565462531227671
iteration : 6864
train acc:  0.828125
train loss:  0.4231486916542053
train gradient:  0.2901843308071771
iteration : 6865
train acc:  0.875
train loss:  0.31674253940582275
train gradient:  0.24721389266665025
iteration : 6866
train acc:  0.828125
train loss:  0.38666319847106934
train gradient:  0.27696698414459014
iteration : 6867
train acc:  0.859375
train loss:  0.3530535101890564
train gradient:  0.1866158571888722
iteration : 6868
train acc:  0.8046875
train loss:  0.36905068159103394
train gradient:  0.2194370360137221
iteration : 6869
train acc:  0.8359375
train loss:  0.39080148935317993
train gradient:  0.24684559177513296
iteration : 6870
train acc:  0.8828125
train loss:  0.31309211254119873
train gradient:  0.16712497482498634
iteration : 6871
train acc:  0.859375
train loss:  0.2920709252357483
train gradient:  0.13486330098261154
iteration : 6872
train acc:  0.8203125
train loss:  0.3522578775882721
train gradient:  0.23621350417494308
iteration : 6873
train acc:  0.828125
train loss:  0.41637566685676575
train gradient:  0.19381561730150476
iteration : 6874
train acc:  0.8359375
train loss:  0.35581809282302856
train gradient:  0.22563998631773435
iteration : 6875
train acc:  0.8203125
train loss:  0.4225669503211975
train gradient:  0.3281258848384087
iteration : 6876
train acc:  0.84375
train loss:  0.34845221042633057
train gradient:  0.13422011297443867
iteration : 6877
train acc:  0.84375
train loss:  0.36069321632385254
train gradient:  0.22626076981049803
iteration : 6878
train acc:  0.84375
train loss:  0.3258237838745117
train gradient:  0.18479169924422806
iteration : 6879
train acc:  0.859375
train loss:  0.3378123641014099
train gradient:  0.2122569786587873
iteration : 6880
train acc:  0.8359375
train loss:  0.3405090570449829
train gradient:  0.26369917419367694
iteration : 6881
train acc:  0.84375
train loss:  0.35096973180770874
train gradient:  0.15194744044851577
iteration : 6882
train acc:  0.8671875
train loss:  0.32114332914352417
train gradient:  0.22661305659856842
iteration : 6883
train acc:  0.8046875
train loss:  0.46585264801979065
train gradient:  0.40983000648847473
iteration : 6884
train acc:  0.8515625
train loss:  0.3445025086402893
train gradient:  0.22042488124747328
iteration : 6885
train acc:  0.8828125
train loss:  0.25727730989456177
train gradient:  0.12752231973055417
iteration : 6886
train acc:  0.8046875
train loss:  0.46498623490333557
train gradient:  0.4342126835460663
iteration : 6887
train acc:  0.859375
train loss:  0.3241666257381439
train gradient:  0.29180902581471674
iteration : 6888
train acc:  0.796875
train loss:  0.4509648084640503
train gradient:  0.26160082945167
iteration : 6889
train acc:  0.796875
train loss:  0.3707246780395508
train gradient:  0.26704749774822767
iteration : 6890
train acc:  0.8515625
train loss:  0.30595847964286804
train gradient:  0.11878224247885222
iteration : 6891
train acc:  0.8828125
train loss:  0.3352629542350769
train gradient:  0.16252469026047767
iteration : 6892
train acc:  0.8359375
train loss:  0.32063591480255127
train gradient:  0.16039721994306955
iteration : 6893
train acc:  0.8984375
train loss:  0.2583896219730377
train gradient:  0.1428240008787594
iteration : 6894
train acc:  0.875
train loss:  0.2804667353630066
train gradient:  0.1735870188005188
iteration : 6895
train acc:  0.8828125
train loss:  0.2969215214252472
train gradient:  0.27541455739097864
iteration : 6896
train acc:  0.8203125
train loss:  0.3680741786956787
train gradient:  0.21065048060902278
iteration : 6897
train acc:  0.859375
train loss:  0.31239089369773865
train gradient:  0.1871532162197741
iteration : 6898
train acc:  0.8515625
train loss:  0.3418125510215759
train gradient:  0.16269656142575928
iteration : 6899
train acc:  0.8984375
train loss:  0.24292856454849243
train gradient:  0.12778223453862
iteration : 6900
train acc:  0.890625
train loss:  0.2594532370567322
train gradient:  0.19082471269075912
iteration : 6901
train acc:  0.8671875
train loss:  0.32685187458992004
train gradient:  0.16961748874820576
iteration : 6902
train acc:  0.84375
train loss:  0.33570635318756104
train gradient:  0.17815180740690423
iteration : 6903
train acc:  0.7734375
train loss:  0.4927707016468048
train gradient:  0.46781408329314217
iteration : 6904
train acc:  0.875
train loss:  0.2707662880420685
train gradient:  0.11897837788116288
iteration : 6905
train acc:  0.7734375
train loss:  0.45311787724494934
train gradient:  0.2567314277196004
iteration : 6906
train acc:  0.890625
train loss:  0.2901032269001007
train gradient:  0.19724798882123504
iteration : 6907
train acc:  0.8671875
train loss:  0.4252871870994568
train gradient:  0.30220816715869103
iteration : 6908
train acc:  0.859375
train loss:  0.2762369215488434
train gradient:  0.1494801544703097
iteration : 6909
train acc:  0.828125
train loss:  0.3485080599784851
train gradient:  0.1694571774223223
iteration : 6910
train acc:  0.8671875
train loss:  0.31937968730926514
train gradient:  0.11618663916530263
iteration : 6911
train acc:  0.84375
train loss:  0.33228886127471924
train gradient:  0.22818578430381273
iteration : 6912
train acc:  0.8125
train loss:  0.3757157623767853
train gradient:  0.2844027543943298
iteration : 6913
train acc:  0.875
train loss:  0.27987033128738403
train gradient:  0.1315323426077255
iteration : 6914
train acc:  0.8515625
train loss:  0.35221192240715027
train gradient:  0.19580950873782305
iteration : 6915
train acc:  0.8515625
train loss:  0.3593485951423645
train gradient:  0.4135674892518768
iteration : 6916
train acc:  0.890625
train loss:  0.2664692997932434
train gradient:  0.17105933646039062
iteration : 6917
train acc:  0.8828125
train loss:  0.27002769708633423
train gradient:  0.13452517960893234
iteration : 6918
train acc:  0.8046875
train loss:  0.4090415835380554
train gradient:  0.20879475518165222
iteration : 6919
train acc:  0.8515625
train loss:  0.3603036403656006
train gradient:  0.5388118115427414
iteration : 6920
train acc:  0.7890625
train loss:  0.40237632393836975
train gradient:  0.29391304501895904
iteration : 6921
train acc:  0.8359375
train loss:  0.3452407717704773
train gradient:  0.18918518396727863
iteration : 6922
train acc:  0.8359375
train loss:  0.3706994354724884
train gradient:  0.23398790516872858
iteration : 6923
train acc:  0.8203125
train loss:  0.37084153294563293
train gradient:  0.1951594615388649
iteration : 6924
train acc:  0.8828125
train loss:  0.33987122774124146
train gradient:  0.23731261461435643
iteration : 6925
train acc:  0.875
train loss:  0.30429792404174805
train gradient:  0.15565095177428184
iteration : 6926
train acc:  0.796875
train loss:  0.4488202929496765
train gradient:  0.31760566688397784
iteration : 6927
train acc:  0.859375
train loss:  0.34659844636917114
train gradient:  0.1784185484631384
iteration : 6928
train acc:  0.8203125
train loss:  0.3865668773651123
train gradient:  0.20630490373169239
iteration : 6929
train acc:  0.859375
train loss:  0.3225025534629822
train gradient:  0.15382601859689035
iteration : 6930
train acc:  0.78125
train loss:  0.4965183138847351
train gradient:  0.32210391809987143
iteration : 6931
train acc:  0.875
train loss:  0.30175501108169556
train gradient:  0.1572707024459949
iteration : 6932
train acc:  0.8671875
train loss:  0.28392189741134644
train gradient:  0.2184311945157191
iteration : 6933
train acc:  0.828125
train loss:  0.4097558259963989
train gradient:  0.2908726221067761
iteration : 6934
train acc:  0.8515625
train loss:  0.382121741771698
train gradient:  0.21018410953515188
iteration : 6935
train acc:  0.8515625
train loss:  0.31439560651779175
train gradient:  0.12122258386494027
iteration : 6936
train acc:  0.8828125
train loss:  0.26943469047546387
train gradient:  0.11511890793663292
iteration : 6937
train acc:  0.84375
train loss:  0.41600295901298523
train gradient:  0.2781866975963243
iteration : 6938
train acc:  0.8671875
train loss:  0.3115633428096771
train gradient:  0.12522753620337135
iteration : 6939
train acc:  0.890625
train loss:  0.3331248164176941
train gradient:  0.1432522303359418
iteration : 6940
train acc:  0.9296875
train loss:  0.22103151679039001
train gradient:  0.1162785261348916
iteration : 6941
train acc:  0.8125
train loss:  0.36413612961769104
train gradient:  0.19471156475019524
iteration : 6942
train acc:  0.859375
train loss:  0.3377547860145569
train gradient:  0.22640083532393887
iteration : 6943
train acc:  0.8515625
train loss:  0.35826024413108826
train gradient:  0.16709922588965226
iteration : 6944
train acc:  0.828125
train loss:  0.3654511570930481
train gradient:  0.274259201838952
iteration : 6945
train acc:  0.859375
train loss:  0.3106399178504944
train gradient:  0.1280665513370085
iteration : 6946
train acc:  0.8203125
train loss:  0.3496052026748657
train gradient:  0.17094399545293698
iteration : 6947
train acc:  0.8984375
train loss:  0.29656803607940674
train gradient:  0.19500836665606372
iteration : 6948
train acc:  0.8359375
train loss:  0.39541369676589966
train gradient:  0.26610666527871357
iteration : 6949
train acc:  0.7890625
train loss:  0.4534950852394104
train gradient:  0.39370532211506687
iteration : 6950
train acc:  0.84375
train loss:  0.3284842073917389
train gradient:  0.22511833328559602
iteration : 6951
train acc:  0.8515625
train loss:  0.36169612407684326
train gradient:  0.16246080257313103
iteration : 6952
train acc:  0.78125
train loss:  0.38489389419555664
train gradient:  0.25067045357682716
iteration : 6953
train acc:  0.796875
train loss:  0.5027815699577332
train gradient:  0.5776106403079155
iteration : 6954
train acc:  0.8359375
train loss:  0.29528018832206726
train gradient:  0.14779898229953375
iteration : 6955
train acc:  0.8984375
train loss:  0.23757418990135193
train gradient:  0.1785172995973114
iteration : 6956
train acc:  0.8515625
train loss:  0.4200652241706848
train gradient:  0.2642147767373522
iteration : 6957
train acc:  0.8125
train loss:  0.38747313618659973
train gradient:  0.2301499524944699
iteration : 6958
train acc:  0.8046875
train loss:  0.3438115119934082
train gradient:  0.20848633560653945
iteration : 6959
train acc:  0.859375
train loss:  0.27974164485931396
train gradient:  0.1460071058406313
iteration : 6960
train acc:  0.84375
train loss:  0.30713146924972534
train gradient:  0.16814019839874816
iteration : 6961
train acc:  0.8359375
train loss:  0.42727744579315186
train gradient:  0.2531541556505802
iteration : 6962
train acc:  0.8828125
train loss:  0.36425745487213135
train gradient:  0.17697576663508685
iteration : 6963
train acc:  0.859375
train loss:  0.35339778661727905
train gradient:  0.21361662828604472
iteration : 6964
train acc:  0.8515625
train loss:  0.3606646955013275
train gradient:  0.19882981054119853
iteration : 6965
train acc:  0.8828125
train loss:  0.33016830682754517
train gradient:  0.13515739781060004
iteration : 6966
train acc:  0.8515625
train loss:  0.35098719596862793
train gradient:  0.27629455125790187
iteration : 6967
train acc:  0.8828125
train loss:  0.31779658794403076
train gradient:  0.19581737699283452
iteration : 6968
train acc:  0.8515625
train loss:  0.32741689682006836
train gradient:  0.21558297712071872
iteration : 6969
train acc:  0.8203125
train loss:  0.3629056513309479
train gradient:  0.2489191408517382
iteration : 6970
train acc:  0.8046875
train loss:  0.4075385630130768
train gradient:  0.20592175989879136
iteration : 6971
train acc:  0.9140625
train loss:  0.2556301951408386
train gradient:  0.13265160598687248
iteration : 6972
train acc:  0.859375
train loss:  0.31203389167785645
train gradient:  0.15067349239080713
iteration : 6973
train acc:  0.828125
train loss:  0.37982648611068726
train gradient:  0.18696605302460206
iteration : 6974
train acc:  0.8515625
train loss:  0.30500638484954834
train gradient:  0.18997128820780665
iteration : 6975
train acc:  0.8359375
train loss:  0.3470233678817749
train gradient:  0.2245540303602957
iteration : 6976
train acc:  0.8515625
train loss:  0.37834590673446655
train gradient:  0.22092131447144234
iteration : 6977
train acc:  0.8984375
train loss:  0.29950618743896484
train gradient:  0.125016201598535
iteration : 6978
train acc:  0.796875
train loss:  0.4173031151294708
train gradient:  0.21341211403075097
iteration : 6979
train acc:  0.8515625
train loss:  0.3211561441421509
train gradient:  0.1705405691897281
iteration : 6980
train acc:  0.859375
train loss:  0.27746689319610596
train gradient:  0.13339576591924523
iteration : 6981
train acc:  0.8046875
train loss:  0.4848930239677429
train gradient:  0.40378677936375323
iteration : 6982
train acc:  0.828125
train loss:  0.3734663128852844
train gradient:  0.2194333198437342
iteration : 6983
train acc:  0.84375
train loss:  0.35588544607162476
train gradient:  0.17134432082805987
iteration : 6984
train acc:  0.875
train loss:  0.3630378842353821
train gradient:  0.257566017868851
iteration : 6985
train acc:  0.8671875
train loss:  0.2926447093486786
train gradient:  0.17210113941717198
iteration : 6986
train acc:  0.8828125
train loss:  0.30001240968704224
train gradient:  0.16610386211346154
iteration : 6987
train acc:  0.890625
train loss:  0.23599325120449066
train gradient:  0.11255676458316224
iteration : 6988
train acc:  0.859375
train loss:  0.36164769530296326
train gradient:  0.21954859746318234
iteration : 6989
train acc:  0.84375
train loss:  0.3663908541202545
train gradient:  0.22385141424409633
iteration : 6990
train acc:  0.875
train loss:  0.30489253997802734
train gradient:  0.12938419258471828
iteration : 6991
train acc:  0.8671875
train loss:  0.320092111825943
train gradient:  0.16405286998846974
iteration : 6992
train acc:  0.8359375
train loss:  0.2987372875213623
train gradient:  0.16530269642280185
iteration : 6993
train acc:  0.859375
train loss:  0.34468209743499756
train gradient:  0.17977768165475355
iteration : 6994
train acc:  0.8515625
train loss:  0.3151891529560089
train gradient:  0.15534060202625824
iteration : 6995
train acc:  0.8984375
train loss:  0.2660977840423584
train gradient:  0.1529265467672611
iteration : 6996
train acc:  0.8359375
train loss:  0.34860724210739136
train gradient:  0.20396082657126818
iteration : 6997
train acc:  0.8359375
train loss:  0.38507080078125
train gradient:  0.19643490656486295
iteration : 6998
train acc:  0.828125
train loss:  0.38796690106391907
train gradient:  0.27519644357945733
iteration : 6999
train acc:  0.8046875
train loss:  0.36174464225769043
train gradient:  0.18445542559867284
iteration : 7000
train acc:  0.8671875
train loss:  0.3205992877483368
train gradient:  0.252895985915371
iteration : 7001
train acc:  0.8671875
train loss:  0.30495697259902954
train gradient:  0.14278365004673085
iteration : 7002
train acc:  0.8359375
train loss:  0.3899719715118408
train gradient:  0.2286645538165784
iteration : 7003
train acc:  0.8671875
train loss:  0.2910146117210388
train gradient:  0.15162853810574212
iteration : 7004
train acc:  0.890625
train loss:  0.2652589976787567
train gradient:  0.21322139719301134
iteration : 7005
train acc:  0.8828125
train loss:  0.27842047810554504
train gradient:  0.20857193012329991
iteration : 7006
train acc:  0.8671875
train loss:  0.37425029277801514
train gradient:  0.247686817047177
iteration : 7007
train acc:  0.8515625
train loss:  0.36948907375335693
train gradient:  0.1972201756232536
iteration : 7008
train acc:  0.8046875
train loss:  0.3514239192008972
train gradient:  0.2126755523628569
iteration : 7009
train acc:  0.765625
train loss:  0.5017790794372559
train gradient:  0.42256667191027886
iteration : 7010
train acc:  0.8671875
train loss:  0.2932521402835846
train gradient:  0.1445342695707265
iteration : 7011
train acc:  0.8515625
train loss:  0.27596989274024963
train gradient:  0.17319203891685878
iteration : 7012
train acc:  0.859375
train loss:  0.3156781792640686
train gradient:  0.19672657148950173
iteration : 7013
train acc:  0.828125
train loss:  0.3516601324081421
train gradient:  0.22477347954944926
iteration : 7014
train acc:  0.8203125
train loss:  0.3617840111255646
train gradient:  0.25993899169931345
iteration : 7015
train acc:  0.921875
train loss:  0.21999898552894592
train gradient:  0.10730353618524607
iteration : 7016
train acc:  0.8515625
train loss:  0.334780752658844
train gradient:  0.2091721185705258
iteration : 7017
train acc:  0.796875
train loss:  0.4172038435935974
train gradient:  0.364975245532483
iteration : 7018
train acc:  0.90625
train loss:  0.283838152885437
train gradient:  0.15121231602894958
iteration : 7019
train acc:  0.8359375
train loss:  0.3308391571044922
train gradient:  0.12952832645167284
iteration : 7020
train acc:  0.8671875
train loss:  0.2985570430755615
train gradient:  0.1606432855071268
iteration : 7021
train acc:  0.8359375
train loss:  0.27659130096435547
train gradient:  0.14589597895573128
iteration : 7022
train acc:  0.8125
train loss:  0.40152233839035034
train gradient:  0.27857807685858277
iteration : 7023
train acc:  0.8828125
train loss:  0.2921332120895386
train gradient:  0.21163500273104735
iteration : 7024
train acc:  0.890625
train loss:  0.33225661516189575
train gradient:  0.19167787580235524
iteration : 7025
train acc:  0.796875
train loss:  0.3370288014411926
train gradient:  0.1925029094164351
iteration : 7026
train acc:  0.875
train loss:  0.3013972043991089
train gradient:  0.1784714703645954
iteration : 7027
train acc:  0.9296875
train loss:  0.2328261435031891
train gradient:  0.1313828876789787
iteration : 7028
train acc:  0.828125
train loss:  0.39205992221832275
train gradient:  0.34163689461312075
iteration : 7029
train acc:  0.8125
train loss:  0.37760406732559204
train gradient:  0.2456242830196711
iteration : 7030
train acc:  0.875
train loss:  0.2898397743701935
train gradient:  0.2057059535417711
iteration : 7031
train acc:  0.84375
train loss:  0.34506550431251526
train gradient:  0.1822424432744419
iteration : 7032
train acc:  0.8828125
train loss:  0.27098989486694336
train gradient:  0.13163658818000032
iteration : 7033
train acc:  0.8671875
train loss:  0.3212486505508423
train gradient:  0.22182913433897178
iteration : 7034
train acc:  0.859375
train loss:  0.3084622323513031
train gradient:  0.3169277317477675
iteration : 7035
train acc:  0.84375
train loss:  0.39988720417022705
train gradient:  0.3278475465713564
iteration : 7036
train acc:  0.8828125
train loss:  0.31922000646591187
train gradient:  0.19998091013565072
iteration : 7037
train acc:  0.8515625
train loss:  0.37164774537086487
train gradient:  0.2204860882610916
iteration : 7038
train acc:  0.84375
train loss:  0.319497287273407
train gradient:  0.20036410956008094
iteration : 7039
train acc:  0.8828125
train loss:  0.32057294249534607
train gradient:  0.25431397983181153
iteration : 7040
train acc:  0.8828125
train loss:  0.29349586367607117
train gradient:  0.1617942776635264
iteration : 7041
train acc:  0.8828125
train loss:  0.2734101712703705
train gradient:  0.14418807320846783
iteration : 7042
train acc:  0.84375
train loss:  0.3822006285190582
train gradient:  0.18708694574859713
iteration : 7043
train acc:  0.90625
train loss:  0.2503858506679535
train gradient:  0.17008804063541752
iteration : 7044
train acc:  0.875
train loss:  0.3008296489715576
train gradient:  0.22997428605299114
iteration : 7045
train acc:  0.8046875
train loss:  0.3869069218635559
train gradient:  0.28153075192973914
iteration : 7046
train acc:  0.9140625
train loss:  0.2586851716041565
train gradient:  0.1167028980890175
iteration : 7047
train acc:  0.84375
train loss:  0.3520808517932892
train gradient:  0.22807499414692917
iteration : 7048
train acc:  0.859375
train loss:  0.3358427882194519
train gradient:  0.16962108133081497
iteration : 7049
train acc:  0.8203125
train loss:  0.380237340927124
train gradient:  0.21518592318497073
iteration : 7050
train acc:  0.828125
train loss:  0.4226120710372925
train gradient:  0.33633991205024205
iteration : 7051
train acc:  0.875
train loss:  0.30740243196487427
train gradient:  0.22914589566019278
iteration : 7052
train acc:  0.859375
train loss:  0.31476786732673645
train gradient:  0.19472564130792158
iteration : 7053
train acc:  0.8359375
train loss:  0.35556894540786743
train gradient:  0.21877309458259098
iteration : 7054
train acc:  0.84375
train loss:  0.3542901575565338
train gradient:  0.19342547746172678
iteration : 7055
train acc:  0.8671875
train loss:  0.31071388721466064
train gradient:  0.1455221334887341
iteration : 7056
train acc:  0.90625
train loss:  0.27172261476516724
train gradient:  0.1325339981367208
iteration : 7057
train acc:  0.875
train loss:  0.3480879068374634
train gradient:  0.23452415900793
iteration : 7058
train acc:  0.8203125
train loss:  0.3727824091911316
train gradient:  0.23248784636324776
iteration : 7059
train acc:  0.8125
train loss:  0.44050633907318115
train gradient:  0.40363393537214465
iteration : 7060
train acc:  0.859375
train loss:  0.34652799367904663
train gradient:  0.19716415912971888
iteration : 7061
train acc:  0.8828125
train loss:  0.29324764013290405
train gradient:  0.2448296211182185
iteration : 7062
train acc:  0.8671875
train loss:  0.3057641386985779
train gradient:  0.19021321009418302
iteration : 7063
train acc:  0.8671875
train loss:  0.3018663227558136
train gradient:  0.23431856824621677
iteration : 7064
train acc:  0.8671875
train loss:  0.2663992941379547
train gradient:  0.1313943155512042
iteration : 7065
train acc:  0.859375
train loss:  0.34991592168807983
train gradient:  0.23251538436408814
iteration : 7066
train acc:  0.890625
train loss:  0.248676598072052
train gradient:  0.14129718282720155
iteration : 7067
train acc:  0.84375
train loss:  0.33255907893180847
train gradient:  0.4305490404691473
iteration : 7068
train acc:  0.84375
train loss:  0.3111926317214966
train gradient:  0.16657494377510657
iteration : 7069
train acc:  0.8203125
train loss:  0.3561464548110962
train gradient:  0.19026075996314412
iteration : 7070
train acc:  0.84375
train loss:  0.36095237731933594
train gradient:  0.18115112027366823
iteration : 7071
train acc:  0.84375
train loss:  0.33166617155075073
train gradient:  0.16953905437522793
iteration : 7072
train acc:  0.8046875
train loss:  0.33535274863243103
train gradient:  0.17690021467316958
iteration : 7073
train acc:  0.8515625
train loss:  0.37903833389282227
train gradient:  0.22445190200371057
iteration : 7074
train acc:  0.8203125
train loss:  0.4158495366573334
train gradient:  0.3881879258961215
iteration : 7075
train acc:  0.8984375
train loss:  0.2620777189731598
train gradient:  0.1354113007598659
iteration : 7076
train acc:  0.8203125
train loss:  0.37934231758117676
train gradient:  0.32236325253078174
iteration : 7077
train acc:  0.875
train loss:  0.3286532759666443
train gradient:  0.14972795263669908
iteration : 7078
train acc:  0.90625
train loss:  0.26950061321258545
train gradient:  0.1514048069089559
iteration : 7079
train acc:  0.8828125
train loss:  0.29136401414871216
train gradient:  0.14603657467084755
iteration : 7080
train acc:  0.7890625
train loss:  0.41198137402534485
train gradient:  0.3816347741798462
iteration : 7081
train acc:  0.90625
train loss:  0.2559352517127991
train gradient:  0.17414952079489326
iteration : 7082
train acc:  0.84375
train loss:  0.32318875193595886
train gradient:  0.2515957462121509
iteration : 7083
train acc:  0.90625
train loss:  0.2676149308681488
train gradient:  0.14610441277620065
iteration : 7084
train acc:  0.8125
train loss:  0.4628358483314514
train gradient:  0.35281798969287825
iteration : 7085
train acc:  0.8984375
train loss:  0.30633485317230225
train gradient:  0.24466299467500113
iteration : 7086
train acc:  0.8359375
train loss:  0.3102695047855377
train gradient:  0.18730926375757292
iteration : 7087
train acc:  0.7734375
train loss:  0.4750794768333435
train gradient:  0.39127552970911156
iteration : 7088
train acc:  0.890625
train loss:  0.2661013603210449
train gradient:  0.12354315482794552
iteration : 7089
train acc:  0.9140625
train loss:  0.24534864723682404
train gradient:  0.14322060037037757
iteration : 7090
train acc:  0.859375
train loss:  0.30391424894332886
train gradient:  0.23128898675527226
iteration : 7091
train acc:  0.9140625
train loss:  0.256470263004303
train gradient:  0.1394385396897312
iteration : 7092
train acc:  0.8359375
train loss:  0.43323495984077454
train gradient:  0.3510709338487754
iteration : 7093
train acc:  0.8046875
train loss:  0.38792455196380615
train gradient:  0.2633491693834244
iteration : 7094
train acc:  0.859375
train loss:  0.30594006180763245
train gradient:  0.2630851134175324
iteration : 7095
train acc:  0.8515625
train loss:  0.3321419656276703
train gradient:  0.2591358433038996
iteration : 7096
train acc:  0.859375
train loss:  0.31439974904060364
train gradient:  0.2075962413196785
iteration : 7097
train acc:  0.7890625
train loss:  0.3977470397949219
train gradient:  0.25655651771897847
iteration : 7098
train acc:  0.796875
train loss:  0.3905152976512909
train gradient:  0.2352448805134411
iteration : 7099
train acc:  0.875
train loss:  0.2715247869491577
train gradient:  0.13993079595707503
iteration : 7100
train acc:  0.8125
train loss:  0.3631898760795593
train gradient:  0.19732197230927448
iteration : 7101
train acc:  0.8125
train loss:  0.3578638732433319
train gradient:  0.22383925863316328
iteration : 7102
train acc:  0.875
train loss:  0.336941659450531
train gradient:  0.18917031956652824
iteration : 7103
train acc:  0.875
train loss:  0.33866044878959656
train gradient:  0.20628811814213205
iteration : 7104
train acc:  0.8671875
train loss:  0.32778239250183105
train gradient:  0.21468521605842
iteration : 7105
train acc:  0.8203125
train loss:  0.4194942116737366
train gradient:  0.32082823085711437
iteration : 7106
train acc:  0.828125
train loss:  0.34594082832336426
train gradient:  0.20342212805262272
iteration : 7107
train acc:  0.84375
train loss:  0.32709258794784546
train gradient:  0.21735693549694468
iteration : 7108
train acc:  0.8125
train loss:  0.40499091148376465
train gradient:  0.38528387671891356
iteration : 7109
train acc:  0.8515625
train loss:  0.33505433797836304
train gradient:  0.14889733877281994
iteration : 7110
train acc:  0.890625
train loss:  0.28200775384902954
train gradient:  0.18008303844996104
iteration : 7111
train acc:  0.8359375
train loss:  0.34159594774246216
train gradient:  0.17769464631740178
iteration : 7112
train acc:  0.8125
train loss:  0.4566100537776947
train gradient:  0.32922401996800005
iteration : 7113
train acc:  0.8046875
train loss:  0.38948410749435425
train gradient:  0.3216368170518963
iteration : 7114
train acc:  0.875
train loss:  0.3340890109539032
train gradient:  0.1829174833077839
iteration : 7115
train acc:  0.921875
train loss:  0.24051271378993988
train gradient:  0.12465052305502357
iteration : 7116
train acc:  0.8828125
train loss:  0.2850436568260193
train gradient:  0.1749444877062511
iteration : 7117
train acc:  0.8203125
train loss:  0.3906896710395813
train gradient:  0.42210154810005557
iteration : 7118
train acc:  0.8828125
train loss:  0.31653088331222534
train gradient:  0.1912417063179825
iteration : 7119
train acc:  0.8671875
train loss:  0.27051031589508057
train gradient:  0.14595876191885326
iteration : 7120
train acc:  0.796875
train loss:  0.4541107714176178
train gradient:  0.2969119150975809
iteration : 7121
train acc:  0.859375
train loss:  0.3465137481689453
train gradient:  0.2439356342911681
iteration : 7122
train acc:  0.875
train loss:  0.3410349488258362
train gradient:  0.188204387263957
iteration : 7123
train acc:  0.84375
train loss:  0.3979339301586151
train gradient:  0.22819744277657422
iteration : 7124
train acc:  0.859375
train loss:  0.3458194434642792
train gradient:  0.25957542496393504
iteration : 7125
train acc:  0.8359375
train loss:  0.37364816665649414
train gradient:  0.2821664237088514
iteration : 7126
train acc:  0.9296875
train loss:  0.25852811336517334
train gradient:  0.18101587141453657
iteration : 7127
train acc:  0.8203125
train loss:  0.3701689839363098
train gradient:  0.2295844451783935
iteration : 7128
train acc:  0.796875
train loss:  0.39090996980667114
train gradient:  0.235247933967764
iteration : 7129
train acc:  0.90625
train loss:  0.2526044547557831
train gradient:  0.12076243886830226
iteration : 7130
train acc:  0.8984375
train loss:  0.28906333446502686
train gradient:  0.13324209641739215
iteration : 7131
train acc:  0.8046875
train loss:  0.4206448197364807
train gradient:  0.21688427233594523
iteration : 7132
train acc:  0.90625
train loss:  0.31089282035827637
train gradient:  0.1922429567988143
iteration : 7133
train acc:  0.8828125
train loss:  0.3149760961532593
train gradient:  0.18433990813384146
iteration : 7134
train acc:  0.8203125
train loss:  0.3749040961265564
train gradient:  0.2064112462880842
iteration : 7135
train acc:  0.859375
train loss:  0.3261863887310028
train gradient:  0.12978735453088036
iteration : 7136
train acc:  0.859375
train loss:  0.30908533930778503
train gradient:  0.2014169344211866
iteration : 7137
train acc:  0.828125
train loss:  0.3648949861526489
train gradient:  0.2909077255800329
iteration : 7138
train acc:  0.8671875
train loss:  0.2764883041381836
train gradient:  0.12710800500032027
iteration : 7139
train acc:  0.8984375
train loss:  0.3521469831466675
train gradient:  0.2941552774562142
iteration : 7140
train acc:  0.8125
train loss:  0.40974563360214233
train gradient:  0.23901648034043488
iteration : 7141
train acc:  0.8515625
train loss:  0.33655139803886414
train gradient:  0.21723205397044532
iteration : 7142
train acc:  0.8828125
train loss:  0.26111745834350586
train gradient:  0.13240058224474427
iteration : 7143
train acc:  0.8515625
train loss:  0.37605923414230347
train gradient:  0.2727659021392677
iteration : 7144
train acc:  0.8671875
train loss:  0.35146456956863403
train gradient:  0.18648504154168183
iteration : 7145
train acc:  0.859375
train loss:  0.3208625316619873
train gradient:  0.17583968765889607
iteration : 7146
train acc:  0.8828125
train loss:  0.2980079650878906
train gradient:  0.187234203260637
iteration : 7147
train acc:  0.9375
train loss:  0.22667686641216278
train gradient:  0.12432986551873113
iteration : 7148
train acc:  0.859375
train loss:  0.3330191373825073
train gradient:  0.1717359950311349
iteration : 7149
train acc:  0.828125
train loss:  0.38729292154312134
train gradient:  0.30668347649533123
iteration : 7150
train acc:  0.8671875
train loss:  0.29070842266082764
train gradient:  0.15190198680036723
iteration : 7151
train acc:  0.875
train loss:  0.32363027334213257
train gradient:  0.1412475851300442
iteration : 7152
train acc:  0.84375
train loss:  0.29429173469543457
train gradient:  0.2458993784719586
iteration : 7153
train acc:  0.828125
train loss:  0.3938871920108795
train gradient:  0.2798142499100867
iteration : 7154
train acc:  0.890625
train loss:  0.31108999252319336
train gradient:  0.16476349705070542
iteration : 7155
train acc:  0.8515625
train loss:  0.31482166051864624
train gradient:  0.1470886551989734
iteration : 7156
train acc:  0.8359375
train loss:  0.38336193561553955
train gradient:  0.2882302774707404
iteration : 7157
train acc:  0.8125
train loss:  0.3874605596065521
train gradient:  0.2503103158911056
iteration : 7158
train acc:  0.890625
train loss:  0.27916404604911804
train gradient:  0.16581348853205236
iteration : 7159
train acc:  0.78125
train loss:  0.481533944606781
train gradient:  0.47315844774422844
iteration : 7160
train acc:  0.8515625
train loss:  0.32312333583831787
train gradient:  0.1765124659659651
iteration : 7161
train acc:  0.8515625
train loss:  0.3563310503959656
train gradient:  0.29633006003528534
iteration : 7162
train acc:  0.890625
train loss:  0.29833054542541504
train gradient:  0.10286065107634752
iteration : 7163
train acc:  0.84375
train loss:  0.38991063833236694
train gradient:  0.2728811869777221
iteration : 7164
train acc:  0.90625
train loss:  0.3023608326911926
train gradient:  0.14765520660229028
iteration : 7165
train acc:  0.8515625
train loss:  0.31717219948768616
train gradient:  0.16690590231460753
iteration : 7166
train acc:  0.8671875
train loss:  0.348237007856369
train gradient:  0.20494980034443977
iteration : 7167
train acc:  0.8203125
train loss:  0.3140045404434204
train gradient:  0.2018804961100248
iteration : 7168
train acc:  0.8359375
train loss:  0.41911208629608154
train gradient:  0.2538967339396751
iteration : 7169
train acc:  0.859375
train loss:  0.2666642963886261
train gradient:  0.13282264949923805
iteration : 7170
train acc:  0.8359375
train loss:  0.3362455368041992
train gradient:  0.21713439325684983
iteration : 7171
train acc:  0.7578125
train loss:  0.5021066665649414
train gradient:  0.3171784120884559
iteration : 7172
train acc:  0.828125
train loss:  0.3600618541240692
train gradient:  0.23359997596464976
iteration : 7173
train acc:  0.84375
train loss:  0.3423789143562317
train gradient:  0.2053440242943277
iteration : 7174
train acc:  0.890625
train loss:  0.31953755021095276
train gradient:  0.18369974485173313
iteration : 7175
train acc:  0.828125
train loss:  0.33220282196998596
train gradient:  0.1768772779053317
iteration : 7176
train acc:  0.8515625
train loss:  0.368875116109848
train gradient:  0.23086196841680212
iteration : 7177
train acc:  0.859375
train loss:  0.28662019968032837
train gradient:  0.17076220944670129
iteration : 7178
train acc:  0.8125
train loss:  0.35712116956710815
train gradient:  0.16039747659918605
iteration : 7179
train acc:  0.796875
train loss:  0.46746695041656494
train gradient:  0.3869829654880986
iteration : 7180
train acc:  0.890625
train loss:  0.24462440609931946
train gradient:  0.10044679933298055
iteration : 7181
train acc:  0.8359375
train loss:  0.3398764729499817
train gradient:  0.22447716549762606
iteration : 7182
train acc:  0.8984375
train loss:  0.29649943113327026
train gradient:  0.11080365666590421
iteration : 7183
train acc:  0.90625
train loss:  0.254921555519104
train gradient:  0.13155583074599814
iteration : 7184
train acc:  0.8203125
train loss:  0.3927798867225647
train gradient:  0.31726399664115196
iteration : 7185
train acc:  0.8984375
train loss:  0.2631242573261261
train gradient:  0.09352709251337943
iteration : 7186
train acc:  0.796875
train loss:  0.35533082485198975
train gradient:  0.16935233231142965
iteration : 7187
train acc:  0.8359375
train loss:  0.3211665749549866
train gradient:  0.2004807028511762
iteration : 7188
train acc:  0.8984375
train loss:  0.29339638352394104
train gradient:  0.15696205903386595
iteration : 7189
train acc:  0.828125
train loss:  0.39926138520240784
train gradient:  0.2927767399046615
iteration : 7190
train acc:  0.8671875
train loss:  0.35027140378952026
train gradient:  0.26452871882933215
iteration : 7191
train acc:  0.8984375
train loss:  0.2512096166610718
train gradient:  0.19599875984523002
iteration : 7192
train acc:  0.859375
train loss:  0.2969726324081421
train gradient:  0.1547472106816945
iteration : 7193
train acc:  0.8671875
train loss:  0.3368818163871765
train gradient:  0.19982554467758712
iteration : 7194
train acc:  0.8984375
train loss:  0.29687029123306274
train gradient:  0.1650536648643682
iteration : 7195
train acc:  0.8828125
train loss:  0.33437401056289673
train gradient:  0.18723934661404612
iteration : 7196
train acc:  0.84375
train loss:  0.3301903009414673
train gradient:  0.21782584373998481
iteration : 7197
train acc:  0.8828125
train loss:  0.3286309242248535
train gradient:  0.15920357163076593
iteration : 7198
train acc:  0.8671875
train loss:  0.3363800644874573
train gradient:  0.20826684094248943
iteration : 7199
train acc:  0.828125
train loss:  0.38742148876190186
train gradient:  0.25328867600502897
iteration : 7200
train acc:  0.859375
train loss:  0.3604368567466736
train gradient:  0.23192584851428252
iteration : 7201
train acc:  0.796875
train loss:  0.4129640460014343
train gradient:  0.2886663307703654
iteration : 7202
train acc:  0.8984375
train loss:  0.2860012650489807
train gradient:  0.17886226719885523
iteration : 7203
train acc:  0.84375
train loss:  0.34222325682640076
train gradient:  0.18342749326183422
iteration : 7204
train acc:  0.875
train loss:  0.3326057493686676
train gradient:  0.2114740880653161
iteration : 7205
train acc:  0.8359375
train loss:  0.312120646238327
train gradient:  0.12361444364184539
iteration : 7206
train acc:  0.8046875
train loss:  0.4455677270889282
train gradient:  0.18982322511881006
iteration : 7207
train acc:  0.859375
train loss:  0.31382453441619873
train gradient:  0.15723810306770047
iteration : 7208
train acc:  0.8984375
train loss:  0.2780712842941284
train gradient:  0.10839094434141014
iteration : 7209
train acc:  0.8515625
train loss:  0.33853012323379517
train gradient:  0.16692186289344874
iteration : 7210
train acc:  0.875
train loss:  0.31139078736305237
train gradient:  0.2082960109987037
iteration : 7211
train acc:  0.8828125
train loss:  0.30643054842948914
train gradient:  0.19075770957248273
iteration : 7212
train acc:  0.8515625
train loss:  0.32564473152160645
train gradient:  0.1369467097953471
iteration : 7213
train acc:  0.8984375
train loss:  0.29032087326049805
train gradient:  0.14794518285056252
iteration : 7214
train acc:  0.8828125
train loss:  0.3117755055427551
train gradient:  0.17820447368283604
iteration : 7215
train acc:  0.8359375
train loss:  0.43633610010147095
train gradient:  0.31018697648754634
iteration : 7216
train acc:  0.859375
train loss:  0.2998904585838318
train gradient:  0.15827658653444354
iteration : 7217
train acc:  0.875
train loss:  0.31744104623794556
train gradient:  0.11557423811771185
iteration : 7218
train acc:  0.828125
train loss:  0.3692895770072937
train gradient:  0.2352527723647232
iteration : 7219
train acc:  0.859375
train loss:  0.3213690519332886
train gradient:  0.17901763735528142
iteration : 7220
train acc:  0.875
train loss:  0.3083122968673706
train gradient:  0.16552932816816343
iteration : 7221
train acc:  0.828125
train loss:  0.3293790817260742
train gradient:  0.16689275514584062
iteration : 7222
train acc:  0.875
train loss:  0.278033971786499
train gradient:  0.18649271228065106
iteration : 7223
train acc:  0.8125
train loss:  0.3465976417064667
train gradient:  0.1957766770797132
iteration : 7224
train acc:  0.8359375
train loss:  0.35994380712509155
train gradient:  0.1546679997871484
iteration : 7225
train acc:  0.84375
train loss:  0.3805505037307739
train gradient:  0.2071757755161668
iteration : 7226
train acc:  0.8515625
train loss:  0.38684898614883423
train gradient:  0.19734412602185636
iteration : 7227
train acc:  0.859375
train loss:  0.33807170391082764
train gradient:  0.1348396999645761
iteration : 7228
train acc:  0.8671875
train loss:  0.30400609970092773
train gradient:  0.11694060192720356
iteration : 7229
train acc:  0.8359375
train loss:  0.3242148160934448
train gradient:  0.1445936939431436
iteration : 7230
train acc:  0.8828125
train loss:  0.2939005494117737
train gradient:  0.2066218977922762
iteration : 7231
train acc:  0.8671875
train loss:  0.3586356043815613
train gradient:  0.1964726977891425
iteration : 7232
train acc:  0.90625
train loss:  0.2746104896068573
train gradient:  0.12826764446339237
iteration : 7233
train acc:  0.890625
train loss:  0.3182414174079895
train gradient:  0.16429743094440358
iteration : 7234
train acc:  0.875
train loss:  0.2880384027957916
train gradient:  0.14225578205829284
iteration : 7235
train acc:  0.828125
train loss:  0.34230515360832214
train gradient:  0.2420540982754948
iteration : 7236
train acc:  0.8046875
train loss:  0.37193241715431213
train gradient:  0.20011018997581448
iteration : 7237
train acc:  0.875
train loss:  0.29635438323020935
train gradient:  0.1464331408369055
iteration : 7238
train acc:  0.828125
train loss:  0.33697956800460815
train gradient:  0.1969383256097525
iteration : 7239
train acc:  0.8671875
train loss:  0.3153465986251831
train gradient:  0.16888610353428551
iteration : 7240
train acc:  0.859375
train loss:  0.32549697160720825
train gradient:  0.2504420092414981
iteration : 7241
train acc:  0.828125
train loss:  0.41697314381599426
train gradient:  0.2964301381779707
iteration : 7242
train acc:  0.8515625
train loss:  0.32977914810180664
train gradient:  0.2334717964845297
iteration : 7243
train acc:  0.8203125
train loss:  0.3814620077610016
train gradient:  0.28400953393837064
iteration : 7244
train acc:  0.921875
train loss:  0.22258834540843964
train gradient:  0.13857310943346762
iteration : 7245
train acc:  0.859375
train loss:  0.37619316577911377
train gradient:  0.29907558724640143
iteration : 7246
train acc:  0.859375
train loss:  0.3007640242576599
train gradient:  0.15790261236315312
iteration : 7247
train acc:  0.8515625
train loss:  0.33064478635787964
train gradient:  0.19472348559099117
iteration : 7248
train acc:  0.8359375
train loss:  0.35181349515914917
train gradient:  0.2460020481734083
iteration : 7249
train acc:  0.8125
train loss:  0.393101304769516
train gradient:  0.23339068158077408
iteration : 7250
train acc:  0.8671875
train loss:  0.2970975339412689
train gradient:  0.13146254756266357
iteration : 7251
train acc:  0.8828125
train loss:  0.27338671684265137
train gradient:  0.13103001223439037
iteration : 7252
train acc:  0.8671875
train loss:  0.2990378737449646
train gradient:  0.1201932796248481
iteration : 7253
train acc:  0.859375
train loss:  0.3511679768562317
train gradient:  0.2914999509533562
iteration : 7254
train acc:  0.8046875
train loss:  0.376337468624115
train gradient:  0.30674288380853476
iteration : 7255
train acc:  0.9375
train loss:  0.2830626666545868
train gradient:  0.1353524969734304
iteration : 7256
train acc:  0.8984375
train loss:  0.24283353984355927
train gradient:  0.1353989363232984
iteration : 7257
train acc:  0.8125
train loss:  0.3745843768119812
train gradient:  0.1995117711029952
iteration : 7258
train acc:  0.8203125
train loss:  0.3652341961860657
train gradient:  0.26444697096458525
iteration : 7259
train acc:  0.859375
train loss:  0.32113850116729736
train gradient:  0.19796964717434085
iteration : 7260
train acc:  0.8828125
train loss:  0.2908821403980255
train gradient:  0.14538723175279583
iteration : 7261
train acc:  0.8515625
train loss:  0.31758204102516174
train gradient:  0.25216787015097647
iteration : 7262
train acc:  0.875
train loss:  0.2619607448577881
train gradient:  0.16517382560366595
iteration : 7263
train acc:  0.8828125
train loss:  0.3038611114025116
train gradient:  0.1725454330395575
iteration : 7264
train acc:  0.8359375
train loss:  0.4190807342529297
train gradient:  0.2655949156577549
iteration : 7265
train acc:  0.8671875
train loss:  0.27998292446136475
train gradient:  0.16397752629104406
iteration : 7266
train acc:  0.828125
train loss:  0.3674846887588501
train gradient:  0.26060264904299774
iteration : 7267
train acc:  0.890625
train loss:  0.3436698913574219
train gradient:  0.23575309470859737
iteration : 7268
train acc:  0.8046875
train loss:  0.3823481798171997
train gradient:  0.3859735213167931
iteration : 7269
train acc:  0.875
train loss:  0.3056332468986511
train gradient:  0.2188059978823806
iteration : 7270
train acc:  0.8125
train loss:  0.39464646577835083
train gradient:  0.24899276205386536
iteration : 7271
train acc:  0.828125
train loss:  0.33405470848083496
train gradient:  0.2506963789841345
iteration : 7272
train acc:  0.8984375
train loss:  0.3035377860069275
train gradient:  0.14339943838915542
iteration : 7273
train acc:  0.859375
train loss:  0.33989840745925903
train gradient:  0.16221247342443554
iteration : 7274
train acc:  0.8671875
train loss:  0.29030656814575195
train gradient:  0.17502778191080418
iteration : 7275
train acc:  0.84375
train loss:  0.4023706316947937
train gradient:  0.25111931705097673
iteration : 7276
train acc:  0.875
train loss:  0.3624255657196045
train gradient:  0.24240647627111592
iteration : 7277
train acc:  0.8359375
train loss:  0.38703781366348267
train gradient:  0.24262563388421532
iteration : 7278
train acc:  0.859375
train loss:  0.3493005633354187
train gradient:  0.2180069728947297
iteration : 7279
train acc:  0.875
train loss:  0.26761022210121155
train gradient:  0.15031208369788762
iteration : 7280
train acc:  0.84375
train loss:  0.3853094279766083
train gradient:  0.3777680302630711
iteration : 7281
train acc:  0.8203125
train loss:  0.34385666251182556
train gradient:  0.21340377269117716
iteration : 7282
train acc:  0.8671875
train loss:  0.35270994901657104
train gradient:  0.2524041098382014
iteration : 7283
train acc:  0.890625
train loss:  0.2975411117076874
train gradient:  0.15937924056901936
iteration : 7284
train acc:  0.8515625
train loss:  0.3379821181297302
train gradient:  0.21943238475177163
iteration : 7285
train acc:  0.890625
train loss:  0.33099353313446045
train gradient:  0.1610805719796575
iteration : 7286
train acc:  0.875
train loss:  0.32536113262176514
train gradient:  0.2116401488762592
iteration : 7287
train acc:  0.8828125
train loss:  0.2798069715499878
train gradient:  0.1348701651551309
iteration : 7288
train acc:  0.875
train loss:  0.32397255301475525
train gradient:  0.21702720115047153
iteration : 7289
train acc:  0.859375
train loss:  0.32091444730758667
train gradient:  0.195312520362788
iteration : 7290
train acc:  0.890625
train loss:  0.27158570289611816
train gradient:  0.1493055955194031
iteration : 7291
train acc:  0.7734375
train loss:  0.48055151104927063
train gradient:  0.4540826657879062
iteration : 7292
train acc:  0.859375
train loss:  0.31818974018096924
train gradient:  0.18907437432482666
iteration : 7293
train acc:  0.8984375
train loss:  0.3059447407722473
train gradient:  0.1897368470478839
iteration : 7294
train acc:  0.8203125
train loss:  0.37631404399871826
train gradient:  0.2466572441084585
iteration : 7295
train acc:  0.8515625
train loss:  0.32051947712898254
train gradient:  0.2009402861203218
iteration : 7296
train acc:  0.84375
train loss:  0.32378584146499634
train gradient:  0.22776416601464644
iteration : 7297
train acc:  0.8671875
train loss:  0.3192073106765747
train gradient:  0.14350116359748208
iteration : 7298
train acc:  0.828125
train loss:  0.40529385209083557
train gradient:  0.38014578656586984
iteration : 7299
train acc:  0.8125
train loss:  0.38129034638404846
train gradient:  0.3910448802352543
iteration : 7300
train acc:  0.890625
train loss:  0.30384325981140137
train gradient:  0.15413877636998405
iteration : 7301
train acc:  0.859375
train loss:  0.351510226726532
train gradient:  0.3407670238135257
iteration : 7302
train acc:  0.8671875
train loss:  0.3600086569786072
train gradient:  0.28077995855001375
iteration : 7303
train acc:  0.875
train loss:  0.2856932282447815
train gradient:  0.14960916217332737
iteration : 7304
train acc:  0.859375
train loss:  0.31588298082351685
train gradient:  0.17424782827824478
iteration : 7305
train acc:  0.8671875
train loss:  0.31310975551605225
train gradient:  0.272102145029545
iteration : 7306
train acc:  0.8515625
train loss:  0.30724644660949707
train gradient:  0.23569100920239527
iteration : 7307
train acc:  0.78125
train loss:  0.4571561813354492
train gradient:  0.36537994963195236
iteration : 7308
train acc:  0.8671875
train loss:  0.29903271794319153
train gradient:  0.17874904942574796
iteration : 7309
train acc:  0.8828125
train loss:  0.26375705003738403
train gradient:  0.218335759928766
iteration : 7310
train acc:  0.8359375
train loss:  0.38069698214530945
train gradient:  0.2773722881276099
iteration : 7311
train acc:  0.859375
train loss:  0.2729755938053131
train gradient:  0.15067949584101692
iteration : 7312
train acc:  0.8046875
train loss:  0.39341652393341064
train gradient:  0.5524478860121167
iteration : 7313
train acc:  0.8203125
train loss:  0.377495676279068
train gradient:  0.2980208999262746
iteration : 7314
train acc:  0.8671875
train loss:  0.2950582206249237
train gradient:  0.14915447901448714
iteration : 7315
train acc:  0.84375
train loss:  0.41397082805633545
train gradient:  0.28095171525970386
iteration : 7316
train acc:  0.8359375
train loss:  0.31539809703826904
train gradient:  0.19711190809372253
iteration : 7317
train acc:  0.8828125
train loss:  0.2891289293766022
train gradient:  0.1742019310343288
iteration : 7318
train acc:  0.8671875
train loss:  0.30230337381362915
train gradient:  0.17815425888785127
iteration : 7319
train acc:  0.8203125
train loss:  0.32262662053108215
train gradient:  0.18271219213070683
iteration : 7320
train acc:  0.8515625
train loss:  0.31998080015182495
train gradient:  0.1668169653531525
iteration : 7321
train acc:  0.8671875
train loss:  0.30932843685150146
train gradient:  0.16323162024510007
iteration : 7322
train acc:  0.8515625
train loss:  0.34708577394485474
train gradient:  0.27484999637412333
iteration : 7323
train acc:  0.875
train loss:  0.3056914806365967
train gradient:  0.1703097833034488
iteration : 7324
train acc:  0.84375
train loss:  0.3803192675113678
train gradient:  0.21843238883885946
iteration : 7325
train acc:  0.84375
train loss:  0.4594039022922516
train gradient:  0.40702932898114463
iteration : 7326
train acc:  0.8359375
train loss:  0.36802029609680176
train gradient:  0.28981392891126206
iteration : 7327
train acc:  0.8671875
train loss:  0.2587147653102875
train gradient:  0.13745601476611302
iteration : 7328
train acc:  0.84375
train loss:  0.3419511914253235
train gradient:  0.24083992039376215
iteration : 7329
train acc:  0.8671875
train loss:  0.2916807532310486
train gradient:  0.21644500496440924
iteration : 7330
train acc:  0.84375
train loss:  0.41352933645248413
train gradient:  0.2593990425744942
iteration : 7331
train acc:  0.8671875
train loss:  0.3320350646972656
train gradient:  0.21635932428748544
iteration : 7332
train acc:  0.8515625
train loss:  0.3692033290863037
train gradient:  0.21466565315459368
iteration : 7333
train acc:  0.859375
train loss:  0.32081565260887146
train gradient:  0.19385334393374787
iteration : 7334
train acc:  0.84375
train loss:  0.43592554330825806
train gradient:  0.33064190764279655
iteration : 7335
train acc:  0.8671875
train loss:  0.3034277856349945
train gradient:  0.1765892159927065
iteration : 7336
train acc:  0.8828125
train loss:  0.36651045083999634
train gradient:  0.20077928242295395
iteration : 7337
train acc:  0.84375
train loss:  0.30742156505584717
train gradient:  0.19398656541439524
iteration : 7338
train acc:  0.8515625
train loss:  0.32059526443481445
train gradient:  0.26539897534169143
iteration : 7339
train acc:  0.828125
train loss:  0.39780277013778687
train gradient:  0.29111452964735024
iteration : 7340
train acc:  0.890625
train loss:  0.2591738998889923
train gradient:  0.13481669460949391
iteration : 7341
train acc:  0.8515625
train loss:  0.36009103059768677
train gradient:  0.33593422161308745
iteration : 7342
train acc:  0.84375
train loss:  0.36623817682266235
train gradient:  0.23029667216023303
iteration : 7343
train acc:  0.90625
train loss:  0.2873755097389221
train gradient:  0.14841351696417415
iteration : 7344
train acc:  0.8671875
train loss:  0.35471436381340027
train gradient:  0.1920541849524504
iteration : 7345
train acc:  0.8125
train loss:  0.4134693145751953
train gradient:  0.2738489936502412
iteration : 7346
train acc:  0.8203125
train loss:  0.3835275173187256
train gradient:  0.22115247225924567
iteration : 7347
train acc:  0.875
train loss:  0.27915507555007935
train gradient:  0.21778240834596743
iteration : 7348
train acc:  0.875
train loss:  0.28477394580841064
train gradient:  0.17887868365155357
iteration : 7349
train acc:  0.8828125
train loss:  0.31514501571655273
train gradient:  0.21233025253054263
iteration : 7350
train acc:  0.8515625
train loss:  0.29497605562210083
train gradient:  0.2684997202061975
iteration : 7351
train acc:  0.890625
train loss:  0.29809099435806274
train gradient:  0.14148616173567485
iteration : 7352
train acc:  0.8203125
train loss:  0.3461673855781555
train gradient:  0.2546595839729673
iteration : 7353
train acc:  0.859375
train loss:  0.3064114451408386
train gradient:  0.14017975902347427
iteration : 7354
train acc:  0.8515625
train loss:  0.36789435148239136
train gradient:  0.18895834248695215
iteration : 7355
train acc:  0.828125
train loss:  0.33022135496139526
train gradient:  0.20496848223783867
iteration : 7356
train acc:  0.859375
train loss:  0.3217346668243408
train gradient:  0.19005348267556324
iteration : 7357
train acc:  0.8515625
train loss:  0.3487369418144226
train gradient:  0.18273542137294654
iteration : 7358
train acc:  0.8828125
train loss:  0.31904804706573486
train gradient:  0.18094686625023698
iteration : 7359
train acc:  0.8984375
train loss:  0.2624870538711548
train gradient:  0.11968883864067147
iteration : 7360
train acc:  0.859375
train loss:  0.33813825249671936
train gradient:  0.25517481427115
iteration : 7361
train acc:  0.84375
train loss:  0.3984600305557251
train gradient:  0.27126068463021097
iteration : 7362
train acc:  0.8671875
train loss:  0.32441964745521545
train gradient:  0.20922653978027514
iteration : 7363
train acc:  0.84375
train loss:  0.37954312562942505
train gradient:  0.2542238462678373
iteration : 7364
train acc:  0.8515625
train loss:  0.30762091279029846
train gradient:  0.15493745389130217
iteration : 7365
train acc:  0.90625
train loss:  0.26736998558044434
train gradient:  0.14310670985698273
iteration : 7366
train acc:  0.859375
train loss:  0.33437472581863403
train gradient:  0.272468533529869
iteration : 7367
train acc:  0.8125
train loss:  0.4113883078098297
train gradient:  0.3107828412411651
iteration : 7368
train acc:  0.8671875
train loss:  0.32008975744247437
train gradient:  0.20242750918847818
iteration : 7369
train acc:  0.78125
train loss:  0.43384265899658203
train gradient:  0.36968178423956705
iteration : 7370
train acc:  0.875
train loss:  0.33191752433776855
train gradient:  0.34672536897770895
iteration : 7371
train acc:  0.859375
train loss:  0.39407944679260254
train gradient:  0.2527388408878321
iteration : 7372
train acc:  0.8515625
train loss:  0.3246409296989441
train gradient:  0.30091063083925507
iteration : 7373
train acc:  0.8671875
train loss:  0.3228909373283386
train gradient:  0.19650447877702737
iteration : 7374
train acc:  0.8359375
train loss:  0.32756203413009644
train gradient:  0.1678129345839397
iteration : 7375
train acc:  0.8515625
train loss:  0.33444058895111084
train gradient:  0.2192647365089922
iteration : 7376
train acc:  0.875
train loss:  0.2936320900917053
train gradient:  0.11684065151569252
iteration : 7377
train acc:  0.8671875
train loss:  0.28947770595550537
train gradient:  0.18222545251533054
iteration : 7378
train acc:  0.859375
train loss:  0.30586764216423035
train gradient:  0.14349005750174015
iteration : 7379
train acc:  0.8984375
train loss:  0.26623356342315674
train gradient:  0.15385727710686686
iteration : 7380
train acc:  0.8515625
train loss:  0.34934002161026
train gradient:  0.19158567380477431
iteration : 7381
train acc:  0.8359375
train loss:  0.3250436782836914
train gradient:  0.22108309240477786
iteration : 7382
train acc:  0.8828125
train loss:  0.28875869512557983
train gradient:  0.1326507063145715
iteration : 7383
train acc:  0.8828125
train loss:  0.29228562116622925
train gradient:  0.19635641112569194
iteration : 7384
train acc:  0.8359375
train loss:  0.32395702600479126
train gradient:  0.24773426639457075
iteration : 7385
train acc:  0.8671875
train loss:  0.3214302062988281
train gradient:  0.17985440832208271
iteration : 7386
train acc:  0.8671875
train loss:  0.31546059250831604
train gradient:  0.16317979717271847
iteration : 7387
train acc:  0.8515625
train loss:  0.2941006124019623
train gradient:  0.16137522077068095
iteration : 7388
train acc:  0.8671875
train loss:  0.29189634323120117
train gradient:  0.1796163538467988
iteration : 7389
train acc:  0.796875
train loss:  0.43718621134757996
train gradient:  0.2999175082093175
iteration : 7390
train acc:  0.8828125
train loss:  0.353165864944458
train gradient:  0.2009471984962183
iteration : 7391
train acc:  0.828125
train loss:  0.41033029556274414
train gradient:  0.2414621449415544
iteration : 7392
train acc:  0.875
train loss:  0.33185452222824097
train gradient:  0.17413794677288627
iteration : 7393
train acc:  0.828125
train loss:  0.44476085901260376
train gradient:  0.3200639436399084
iteration : 7394
train acc:  0.8515625
train loss:  0.316700279712677
train gradient:  0.15678049309665487
iteration : 7395
train acc:  0.8359375
train loss:  0.3554573059082031
train gradient:  0.19095292636612382
iteration : 7396
train acc:  0.875
train loss:  0.2946091890335083
train gradient:  0.16368988268032514
iteration : 7397
train acc:  0.8671875
train loss:  0.3717142641544342
train gradient:  0.2468252814909983
iteration : 7398
train acc:  0.78125
train loss:  0.40894854068756104
train gradient:  0.29782018969321694
iteration : 7399
train acc:  0.8359375
train loss:  0.39357370138168335
train gradient:  0.2670109830798779
iteration : 7400
train acc:  0.84375
train loss:  0.3304387032985687
train gradient:  0.17896395349200106
iteration : 7401
train acc:  0.8359375
train loss:  0.34370192885398865
train gradient:  0.22595466131522948
iteration : 7402
train acc:  0.859375
train loss:  0.35845544934272766
train gradient:  0.21597264990129555
iteration : 7403
train acc:  0.84375
train loss:  0.30976662039756775
train gradient:  0.21484227828242594
iteration : 7404
train acc:  0.859375
train loss:  0.30921489000320435
train gradient:  0.18730648750943685
iteration : 7405
train acc:  0.8515625
train loss:  0.31202206015586853
train gradient:  0.2194986598455309
iteration : 7406
train acc:  0.7734375
train loss:  0.40871351957321167
train gradient:  0.2538600831617897
iteration : 7407
train acc:  0.8359375
train loss:  0.3659358024597168
train gradient:  0.2664161800660586
iteration : 7408
train acc:  0.890625
train loss:  0.2871888279914856
train gradient:  0.17738434783483975
iteration : 7409
train acc:  0.859375
train loss:  0.35248732566833496
train gradient:  0.18338747349500595
iteration : 7410
train acc:  0.875
train loss:  0.35054340958595276
train gradient:  0.22711376193428684
iteration : 7411
train acc:  0.8125
train loss:  0.3880019187927246
train gradient:  0.18735346439286948
iteration : 7412
train acc:  0.84375
train loss:  0.3124297559261322
train gradient:  0.24712646132335553
iteration : 7413
train acc:  0.8671875
train loss:  0.29396742582321167
train gradient:  0.1435526088105796
iteration : 7414
train acc:  0.875
train loss:  0.33659517765045166
train gradient:  0.1507310986070146
iteration : 7415
train acc:  0.8984375
train loss:  0.30226707458496094
train gradient:  0.13270177567646024
iteration : 7416
train acc:  0.8359375
train loss:  0.4427812099456787
train gradient:  0.6222041470860676
iteration : 7417
train acc:  0.859375
train loss:  0.30849915742874146
train gradient:  0.18016210990637058
iteration : 7418
train acc:  0.84375
train loss:  0.362909734249115
train gradient:  0.2122098231976377
iteration : 7419
train acc:  0.8515625
train loss:  0.3717096447944641
train gradient:  0.2339460898773872
iteration : 7420
train acc:  0.796875
train loss:  0.43679776787757874
train gradient:  0.22904705835184327
iteration : 7421
train acc:  0.8515625
train loss:  0.34435784816741943
train gradient:  0.14573964286233262
iteration : 7422
train acc:  0.84375
train loss:  0.3512200713157654
train gradient:  0.17138668468523682
iteration : 7423
train acc:  0.84375
train loss:  0.3659970760345459
train gradient:  0.1567442224988043
iteration : 7424
train acc:  0.828125
train loss:  0.416190505027771
train gradient:  0.34915432917544603
iteration : 7425
train acc:  0.8359375
train loss:  0.3652828335762024
train gradient:  0.18931695186282932
iteration : 7426
train acc:  0.8984375
train loss:  0.24447107315063477
train gradient:  0.1259702398703941
iteration : 7427
train acc:  0.8984375
train loss:  0.2932407259941101
train gradient:  0.11680765820450446
iteration : 7428
train acc:  0.8828125
train loss:  0.3667212128639221
train gradient:  0.18607174667879478
iteration : 7429
train acc:  0.828125
train loss:  0.34921586513519287
train gradient:  0.2237743486294803
iteration : 7430
train acc:  0.8515625
train loss:  0.2729250192642212
train gradient:  0.11028383328556972
iteration : 7431
train acc:  0.890625
train loss:  0.28685057163238525
train gradient:  0.13657578573527634
iteration : 7432
train acc:  0.8515625
train loss:  0.36351537704467773
train gradient:  0.20795886498065924
iteration : 7433
train acc:  0.90625
train loss:  0.2612452805042267
train gradient:  0.20848360330818083
iteration : 7434
train acc:  0.890625
train loss:  0.2908974885940552
train gradient:  0.10802024194683857
iteration : 7435
train acc:  0.921875
train loss:  0.28072136640548706
train gradient:  0.1731279273126412
iteration : 7436
train acc:  0.8671875
train loss:  0.37935590744018555
train gradient:  0.2026223171499285
iteration : 7437
train acc:  0.8671875
train loss:  0.3280721604824066
train gradient:  0.15064989584689445
iteration : 7438
train acc:  0.7890625
train loss:  0.405090868473053
train gradient:  0.26259958789702714
iteration : 7439
train acc:  0.7890625
train loss:  0.47461891174316406
train gradient:  0.3730093978511114
iteration : 7440
train acc:  0.8359375
train loss:  0.34300127625465393
train gradient:  0.25802871991490045
iteration : 7441
train acc:  0.8125
train loss:  0.39322638511657715
train gradient:  0.1839361859419234
iteration : 7442
train acc:  0.78125
train loss:  0.42247897386550903
train gradient:  0.2923557032925014
iteration : 7443
train acc:  0.8515625
train loss:  0.40230464935302734
train gradient:  0.28478292214865114
iteration : 7444
train acc:  0.875
train loss:  0.2780708968639374
train gradient:  0.1301180694335462
iteration : 7445
train acc:  0.890625
train loss:  0.32171469926834106
train gradient:  0.23839228028223783
iteration : 7446
train acc:  0.859375
train loss:  0.36768031120300293
train gradient:  0.18419689877383838
iteration : 7447
train acc:  0.8046875
train loss:  0.44178852438926697
train gradient:  0.24931876787354879
iteration : 7448
train acc:  0.7890625
train loss:  0.3491891622543335
train gradient:  0.25247031602676095
iteration : 7449
train acc:  0.8984375
train loss:  0.2698543071746826
train gradient:  0.11611282793886032
iteration : 7450
train acc:  0.875
train loss:  0.34660953283309937
train gradient:  0.21835699208707615
iteration : 7451
train acc:  0.84375
train loss:  0.3264457583427429
train gradient:  0.23400399600955468
iteration : 7452
train acc:  0.84375
train loss:  0.3407756984233856
train gradient:  0.2665522788295019
iteration : 7453
train acc:  0.8828125
train loss:  0.29895463585853577
train gradient:  0.17309128270820281
iteration : 7454
train acc:  0.8125
train loss:  0.32378315925598145
train gradient:  0.20815371079062925
iteration : 7455
train acc:  0.84375
train loss:  0.3107187747955322
train gradient:  0.17274493696983628
iteration : 7456
train acc:  0.8515625
train loss:  0.3079758882522583
train gradient:  0.17786440268855697
iteration : 7457
train acc:  0.890625
train loss:  0.26746538281440735
train gradient:  0.13081520258878115
iteration : 7458
train acc:  0.8203125
train loss:  0.34216421842575073
train gradient:  0.16431909789277524
iteration : 7459
train acc:  0.8515625
train loss:  0.36217355728149414
train gradient:  0.1904619066634382
iteration : 7460
train acc:  0.8046875
train loss:  0.381793737411499
train gradient:  0.22769348965123415
iteration : 7461
train acc:  0.8203125
train loss:  0.347163587808609
train gradient:  0.21809416087140798
iteration : 7462
train acc:  0.8125
train loss:  0.38845008611679077
train gradient:  0.21864503891311576
iteration : 7463
train acc:  0.8515625
train loss:  0.32300490140914917
train gradient:  0.2050804957099075
iteration : 7464
train acc:  0.8671875
train loss:  0.33501821756362915
train gradient:  0.1472880923230297
iteration : 7465
train acc:  0.8359375
train loss:  0.31885355710983276
train gradient:  0.22302972522648612
iteration : 7466
train acc:  0.8203125
train loss:  0.3643040657043457
train gradient:  0.18733684183194887
iteration : 7467
train acc:  0.8203125
train loss:  0.4367397129535675
train gradient:  0.31848444447164836
iteration : 7468
train acc:  0.8984375
train loss:  0.25487881898880005
train gradient:  0.1630498096845843
iteration : 7469
train acc:  0.8046875
train loss:  0.4247893989086151
train gradient:  0.2601723212366638
iteration : 7470
train acc:  0.8515625
train loss:  0.3316391110420227
train gradient:  0.16959513468591084
iteration : 7471
train acc:  0.8203125
train loss:  0.3587864637374878
train gradient:  0.31576377734715455
iteration : 7472
train acc:  0.8515625
train loss:  0.3135676383972168
train gradient:  0.1540143248342073
iteration : 7473
train acc:  0.8046875
train loss:  0.4152325689792633
train gradient:  0.3182795371446075
iteration : 7474
train acc:  0.9296875
train loss:  0.22656717896461487
train gradient:  0.10548347083094924
iteration : 7475
train acc:  0.90625
train loss:  0.2704358696937561
train gradient:  0.11645864859794791
iteration : 7476
train acc:  0.875
train loss:  0.3584262728691101
train gradient:  0.1774082556761639
iteration : 7477
train acc:  0.8203125
train loss:  0.4324597120285034
train gradient:  0.20623239239834062
iteration : 7478
train acc:  0.859375
train loss:  0.28870081901550293
train gradient:  0.1833305745862908
iteration : 7479
train acc:  0.8359375
train loss:  0.32141199707984924
train gradient:  0.2928484508917385
iteration : 7480
train acc:  0.828125
train loss:  0.38173234462738037
train gradient:  0.18159862427984447
iteration : 7481
train acc:  0.890625
train loss:  0.2835501432418823
train gradient:  0.1948792719487033
iteration : 7482
train acc:  0.8046875
train loss:  0.33986783027648926
train gradient:  0.14513947299625563
iteration : 7483
train acc:  0.8671875
train loss:  0.3270537257194519
train gradient:  0.3436079297748004
iteration : 7484
train acc:  0.8203125
train loss:  0.4011235237121582
train gradient:  0.20467233397119078
iteration : 7485
train acc:  0.8515625
train loss:  0.37876617908477783
train gradient:  0.15155577098667544
iteration : 7486
train acc:  0.9296875
train loss:  0.2123042345046997
train gradient:  0.08829087769415836
iteration : 7487
train acc:  0.8984375
train loss:  0.2597222924232483
train gradient:  0.16628611983752517
iteration : 7488
train acc:  0.9140625
train loss:  0.23014983534812927
train gradient:  0.12628400128131723
iteration : 7489
train acc:  0.859375
train loss:  0.2995259165763855
train gradient:  0.2303443911415566
iteration : 7490
train acc:  0.921875
train loss:  0.2126145213842392
train gradient:  0.10276914468867661
iteration : 7491
train acc:  0.8984375
train loss:  0.25311458110809326
train gradient:  0.1285583670683395
iteration : 7492
train acc:  0.859375
train loss:  0.2970640957355499
train gradient:  0.14083747534069424
iteration : 7493
train acc:  0.8203125
train loss:  0.35426193475723267
train gradient:  0.25310915660935734
iteration : 7494
train acc:  0.7890625
train loss:  0.4460424780845642
train gradient:  0.2741444109319263
iteration : 7495
train acc:  0.8203125
train loss:  0.42160019278526306
train gradient:  0.3542450746780331
iteration : 7496
train acc:  0.875
train loss:  0.26299452781677246
train gradient:  0.12156519613041039
iteration : 7497
train acc:  0.8515625
train loss:  0.36223143339157104
train gradient:  0.202370985144197
iteration : 7498
train acc:  0.8515625
train loss:  0.35668492317199707
train gradient:  0.247700663982067
iteration : 7499
train acc:  0.8515625
train loss:  0.3479612469673157
train gradient:  0.16211124976904323
iteration : 7500
train acc:  0.890625
train loss:  0.2686586081981659
train gradient:  0.11577117194781598
iteration : 7501
train acc:  0.8671875
train loss:  0.2668898105621338
train gradient:  0.12996480650363684
iteration : 7502
train acc:  0.7890625
train loss:  0.47323232889175415
train gradient:  0.404223453237526
iteration : 7503
train acc:  0.8984375
train loss:  0.2505573630332947
train gradient:  0.11060085302542577
iteration : 7504
train acc:  0.8515625
train loss:  0.35786086320877075
train gradient:  0.1843244653318965
iteration : 7505
train acc:  0.875
train loss:  0.27721160650253296
train gradient:  0.18406786560347294
iteration : 7506
train acc:  0.8046875
train loss:  0.3667239844799042
train gradient:  0.21743641137019426
iteration : 7507
train acc:  0.8359375
train loss:  0.35208991169929504
train gradient:  0.2150588626025593
iteration : 7508
train acc:  0.875
train loss:  0.27819156646728516
train gradient:  0.15692256314498015
iteration : 7509
train acc:  0.8671875
train loss:  0.30585744976997375
train gradient:  0.18569911718018572
iteration : 7510
train acc:  0.84375
train loss:  0.4075062572956085
train gradient:  0.2958631504450406
iteration : 7511
train acc:  0.8203125
train loss:  0.36687952280044556
train gradient:  0.2463826415417824
iteration : 7512
train acc:  0.859375
train loss:  0.36960655450820923
train gradient:  0.24477026852131467
iteration : 7513
train acc:  0.84375
train loss:  0.3396896421909332
train gradient:  0.18309109699415962
iteration : 7514
train acc:  0.8984375
train loss:  0.3206990957260132
train gradient:  0.20117333474368015
iteration : 7515
train acc:  0.875
train loss:  0.3323668837547302
train gradient:  0.2061507842488362
iteration : 7516
train acc:  0.8515625
train loss:  0.34637805819511414
train gradient:  0.21405257027663543
iteration : 7517
train acc:  0.828125
train loss:  0.3298220634460449
train gradient:  0.18314969971324868
iteration : 7518
train acc:  0.8984375
train loss:  0.2994140386581421
train gradient:  0.12883113747898228
iteration : 7519
train acc:  0.875
train loss:  0.28581976890563965
train gradient:  0.1391308892843286
iteration : 7520
train acc:  0.875
train loss:  0.3367645740509033
train gradient:  0.18994607477113218
iteration : 7521
train acc:  0.859375
train loss:  0.29539257287979126
train gradient:  0.16839483475955713
iteration : 7522
train acc:  0.859375
train loss:  0.3266289234161377
train gradient:  0.1286693579362668
iteration : 7523
train acc:  0.84375
train loss:  0.32882124185562134
train gradient:  0.16828774118180673
iteration : 7524
train acc:  0.8515625
train loss:  0.38061538338661194
train gradient:  0.3824702915678747
iteration : 7525
train acc:  0.8515625
train loss:  0.2884872555732727
train gradient:  0.11824994125108097
iteration : 7526
train acc:  0.8984375
train loss:  0.24998901784420013
train gradient:  0.1038092525564301
iteration : 7527
train acc:  0.859375
train loss:  0.32617881894111633
train gradient:  0.14621359731576708
iteration : 7528
train acc:  0.8359375
train loss:  0.367459237575531
train gradient:  0.24073486116260157
iteration : 7529
train acc:  0.8125
train loss:  0.43035203218460083
train gradient:  0.30095502544712494
iteration : 7530
train acc:  0.8359375
train loss:  0.338049054145813
train gradient:  0.1617223110834031
iteration : 7531
train acc:  0.875
train loss:  0.31946688890457153
train gradient:  0.17596397172934092
iteration : 7532
train acc:  0.875
train loss:  0.3489980697631836
train gradient:  0.24460170620293858
iteration : 7533
train acc:  0.8359375
train loss:  0.3670668303966522
train gradient:  0.18376582303521596
iteration : 7534
train acc:  0.828125
train loss:  0.3466190993785858
train gradient:  0.23784262648815563
iteration : 7535
train acc:  0.8671875
train loss:  0.30823183059692383
train gradient:  0.16245919850689572
iteration : 7536
train acc:  0.875
train loss:  0.2687114477157593
train gradient:  0.20951653974009046
iteration : 7537
train acc:  0.890625
train loss:  0.2738649845123291
train gradient:  0.14098802079600412
iteration : 7538
train acc:  0.84375
train loss:  0.31275227665901184
train gradient:  0.246372703032451
iteration : 7539
train acc:  0.828125
train loss:  0.40449729561805725
train gradient:  0.3071689423942114
iteration : 7540
train acc:  0.859375
train loss:  0.363573282957077
train gradient:  0.24107100226029876
iteration : 7541
train acc:  0.75
train loss:  0.4514332413673401
train gradient:  0.2721112371746771
iteration : 7542
train acc:  0.90625
train loss:  0.3040989935398102
train gradient:  0.24780862164995468
iteration : 7543
train acc:  0.8828125
train loss:  0.3166850209236145
train gradient:  0.1453948568841662
iteration : 7544
train acc:  0.84375
train loss:  0.4012331962585449
train gradient:  0.2531097528799089
iteration : 7545
train acc:  0.890625
train loss:  0.27348560094833374
train gradient:  0.15602539406573057
iteration : 7546
train acc:  0.8671875
train loss:  0.32205110788345337
train gradient:  0.21596087471120756
iteration : 7547
train acc:  0.8515625
train loss:  0.34040090441703796
train gradient:  0.16533559034983328
iteration : 7548
train acc:  0.84375
train loss:  0.2986898422241211
train gradient:  0.15985260766691498
iteration : 7549
train acc:  0.84375
train loss:  0.3591684401035309
train gradient:  0.2756311488090699
iteration : 7550
train acc:  0.84375
train loss:  0.39246827363967896
train gradient:  0.18958515429005032
iteration : 7551
train acc:  0.8671875
train loss:  0.2947810888290405
train gradient:  0.18881173934707896
iteration : 7552
train acc:  0.8828125
train loss:  0.2945399880409241
train gradient:  0.20911613073577845
iteration : 7553
train acc:  0.8671875
train loss:  0.37624847888946533
train gradient:  0.3052963569773884
iteration : 7554
train acc:  0.875
train loss:  0.28782910108566284
train gradient:  0.1358792133647473
iteration : 7555
train acc:  0.90625
train loss:  0.25341400504112244
train gradient:  0.12590594865965876
iteration : 7556
train acc:  0.8828125
train loss:  0.34857919812202454
train gradient:  0.19369706659243455
iteration : 7557
train acc:  0.875
train loss:  0.3125064969062805
train gradient:  0.2854344753805651
iteration : 7558
train acc:  0.8203125
train loss:  0.3675782382488251
train gradient:  0.21961024635870924
iteration : 7559
train acc:  0.8515625
train loss:  0.33750808238983154
train gradient:  0.17380286487173432
iteration : 7560
train acc:  0.875
train loss:  0.2963927090167999
train gradient:  0.21106527469688396
iteration : 7561
train acc:  0.8359375
train loss:  0.37802067399024963
train gradient:  0.227381635175471
iteration : 7562
train acc:  0.828125
train loss:  0.40699151158332825
train gradient:  0.3401404501849997
iteration : 7563
train acc:  0.8125
train loss:  0.44836220145225525
train gradient:  0.44315863214082685
iteration : 7564
train acc:  0.90625
train loss:  0.25709906220436096
train gradient:  0.13966485269405904
iteration : 7565
train acc:  0.859375
train loss:  0.3072058856487274
train gradient:  0.28842246221326756
iteration : 7566
train acc:  0.8515625
train loss:  0.38166218996047974
train gradient:  0.33815502648882556
iteration : 7567
train acc:  0.796875
train loss:  0.45299726724624634
train gradient:  0.3776055721902959
iteration : 7568
train acc:  0.828125
train loss:  0.32378649711608887
train gradient:  0.19256623079061552
iteration : 7569
train acc:  0.8203125
train loss:  0.3656853437423706
train gradient:  0.2328538051195171
iteration : 7570
train acc:  0.8515625
train loss:  0.39534053206443787
train gradient:  0.2346030216134513
iteration : 7571
train acc:  0.90625
train loss:  0.31239384412765503
train gradient:  0.18384273563382597
iteration : 7572
train acc:  0.859375
train loss:  0.3128291964530945
train gradient:  0.21834290157003877
iteration : 7573
train acc:  0.78125
train loss:  0.4639520049095154
train gradient:  0.33457959457821584
iteration : 7574
train acc:  0.8515625
train loss:  0.3361073136329651
train gradient:  0.21158365032494467
iteration : 7575
train acc:  0.859375
train loss:  0.29762881994247437
train gradient:  0.15297658800828223
iteration : 7576
train acc:  0.8671875
train loss:  0.30252617597579956
train gradient:  0.13386504082651846
iteration : 7577
train acc:  0.875
train loss:  0.2734571397304535
train gradient:  0.15199424824318997
iteration : 7578
train acc:  0.8828125
train loss:  0.3153265416622162
train gradient:  0.15234034739738117
iteration : 7579
train acc:  0.8359375
train loss:  0.3571236729621887
train gradient:  0.2301496852273879
iteration : 7580
train acc:  0.8515625
train loss:  0.33594971895217896
train gradient:  0.19825199258971532
iteration : 7581
train acc:  0.9140625
train loss:  0.30379509925842285
train gradient:  0.14863050188963695
iteration : 7582
train acc:  0.8203125
train loss:  0.49723851680755615
train gradient:  0.3343400300755376
iteration : 7583
train acc:  0.859375
train loss:  0.32793569564819336
train gradient:  0.17094062183549033
iteration : 7584
train acc:  0.859375
train loss:  0.3223908841609955
train gradient:  0.1705845063991316
iteration : 7585
train acc:  0.875
train loss:  0.29762041568756104
train gradient:  0.19335142292359198
iteration : 7586
train acc:  0.8671875
train loss:  0.3115699887275696
train gradient:  0.12530380692681004
iteration : 7587
train acc:  0.90625
train loss:  0.3171464800834656
train gradient:  0.21551325735676494
iteration : 7588
train acc:  0.8046875
train loss:  0.4206610918045044
train gradient:  0.2920291412298941
iteration : 7589
train acc:  0.84375
train loss:  0.34113946557044983
train gradient:  0.19926607547648736
iteration : 7590
train acc:  0.8671875
train loss:  0.33325061202049255
train gradient:  0.17258222157062725
iteration : 7591
train acc:  0.859375
train loss:  0.3012051284313202
train gradient:  0.14220739136934096
iteration : 7592
train acc:  0.828125
train loss:  0.3967990279197693
train gradient:  0.30057092457813
iteration : 7593
train acc:  0.7890625
train loss:  0.46544331312179565
train gradient:  0.3157001732269103
iteration : 7594
train acc:  0.78125
train loss:  0.38360995054244995
train gradient:  0.2577034859484591
iteration : 7595
train acc:  0.875
train loss:  0.31701749563217163
train gradient:  0.156799444062878
iteration : 7596
train acc:  0.8828125
train loss:  0.3043719530105591
train gradient:  0.1295506076946521
iteration : 7597
train acc:  0.8359375
train loss:  0.37498125433921814
train gradient:  0.18129226308331148
iteration : 7598
train acc:  0.90625
train loss:  0.23928232491016388
train gradient:  0.10954340123820563
iteration : 7599
train acc:  0.796875
train loss:  0.3846988081932068
train gradient:  0.23998251779248225
iteration : 7600
train acc:  0.890625
train loss:  0.3471476137638092
train gradient:  0.17934121517855073
iteration : 7601
train acc:  0.8359375
train loss:  0.3418106436729431
train gradient:  0.20656046269173314
iteration : 7602
train acc:  0.828125
train loss:  0.364584743976593
train gradient:  0.2176156154581881
iteration : 7603
train acc:  0.828125
train loss:  0.37436074018478394
train gradient:  0.2800285144599078
iteration : 7604
train acc:  0.8515625
train loss:  0.3112228512763977
train gradient:  0.15327520840981007
iteration : 7605
train acc:  0.875
train loss:  0.2960053086280823
train gradient:  0.1263795853153316
iteration : 7606
train acc:  0.8125
train loss:  0.38260865211486816
train gradient:  0.23835496150757518
iteration : 7607
train acc:  0.8046875
train loss:  0.41945594549179077
train gradient:  0.22639563592794593
iteration : 7608
train acc:  0.890625
train loss:  0.2644202411174774
train gradient:  0.12719047589088522
iteration : 7609
train acc:  0.90625
train loss:  0.3231964111328125
train gradient:  0.1464710855098425
iteration : 7610
train acc:  0.84375
train loss:  0.36441823840141296
train gradient:  0.22788642001768178
iteration : 7611
train acc:  0.8984375
train loss:  0.2686540186405182
train gradient:  0.10668622069555736
iteration : 7612
train acc:  0.8203125
train loss:  0.3663609027862549
train gradient:  0.19753108842705594
iteration : 7613
train acc:  0.8984375
train loss:  0.27889886498451233
train gradient:  0.1784304795619711
iteration : 7614
train acc:  0.8359375
train loss:  0.3720542788505554
train gradient:  0.2395975230927278
iteration : 7615
train acc:  0.8359375
train loss:  0.3378782868385315
train gradient:  0.12677440816946178
iteration : 7616
train acc:  0.84375
train loss:  0.3565932810306549
train gradient:  0.2027922514482436
iteration : 7617
train acc:  0.8515625
train loss:  0.356375515460968
train gradient:  0.17552676563471067
iteration : 7618
train acc:  0.875
train loss:  0.29987555742263794
train gradient:  0.2427754536652064
iteration : 7619
train acc:  0.828125
train loss:  0.36514121294021606
train gradient:  0.211020220145976
iteration : 7620
train acc:  0.8515625
train loss:  0.31314313411712646
train gradient:  0.1653264869161582
iteration : 7621
train acc:  0.84375
train loss:  0.43141627311706543
train gradient:  0.35467053410592553
iteration : 7622
train acc:  0.84375
train loss:  0.3768268823623657
train gradient:  0.1955084848685634
iteration : 7623
train acc:  0.859375
train loss:  0.30579063296318054
train gradient:  0.14395952798178818
iteration : 7624
train acc:  0.84375
train loss:  0.3764715790748596
train gradient:  0.2098250634358334
iteration : 7625
train acc:  0.828125
train loss:  0.35095617175102234
train gradient:  0.27120463176534365
iteration : 7626
train acc:  0.875
train loss:  0.33341217041015625
train gradient:  0.18335764914074168
iteration : 7627
train acc:  0.84375
train loss:  0.40451860427856445
train gradient:  0.2957176629238877
iteration : 7628
train acc:  0.8359375
train loss:  0.3462713062763214
train gradient:  0.20781753577317397
iteration : 7629
train acc:  0.828125
train loss:  0.35585612058639526
train gradient:  0.23537742444606757
iteration : 7630
train acc:  0.90625
train loss:  0.322882741689682
train gradient:  0.17691207725984465
iteration : 7631
train acc:  0.84375
train loss:  0.4033238887786865
train gradient:  0.2375912077185554
iteration : 7632
train acc:  0.84375
train loss:  0.33519360423088074
train gradient:  0.3380241518907836
iteration : 7633
train acc:  0.8515625
train loss:  0.3048519492149353
train gradient:  0.1717716356120148
iteration : 7634
train acc:  0.875
train loss:  0.2685616612434387
train gradient:  0.1977800437092664
iteration : 7635
train acc:  0.84375
train loss:  0.36970388889312744
train gradient:  0.22676308708039067
iteration : 7636
train acc:  0.84375
train loss:  0.3803199827671051
train gradient:  0.2119919765555171
iteration : 7637
train acc:  0.875
train loss:  0.3024844527244568
train gradient:  0.15591463783328957
iteration : 7638
train acc:  0.84375
train loss:  0.37367168068885803
train gradient:  0.25318043706782206
iteration : 7639
train acc:  0.859375
train loss:  0.3347713053226471
train gradient:  0.2603648422094393
iteration : 7640
train acc:  0.90625
train loss:  0.26914045214653015
train gradient:  0.16733005984330634
iteration : 7641
train acc:  0.8984375
train loss:  0.2727848291397095
train gradient:  0.1747408227998403
iteration : 7642
train acc:  0.859375
train loss:  0.33209118247032166
train gradient:  0.20237058257247909
iteration : 7643
train acc:  0.8984375
train loss:  0.32697129249572754
train gradient:  0.30824089266921856
iteration : 7644
train acc:  0.8125
train loss:  0.33544692397117615
train gradient:  0.1709251979555707
iteration : 7645
train acc:  0.8125
train loss:  0.3952310085296631
train gradient:  0.17842740172264043
iteration : 7646
train acc:  0.8828125
train loss:  0.2727470397949219
train gradient:  0.1353095927452938
iteration : 7647
train acc:  0.859375
train loss:  0.42271900177001953
train gradient:  0.38744077471444555
iteration : 7648
train acc:  0.875
train loss:  0.26879557967185974
train gradient:  0.1578033096688791
iteration : 7649
train acc:  0.828125
train loss:  0.3937857747077942
train gradient:  0.25966025015587896
iteration : 7650
train acc:  0.8125
train loss:  0.3713839054107666
train gradient:  0.2425216113567676
iteration : 7651
train acc:  0.90625
train loss:  0.24889516830444336
train gradient:  0.10656859513956246
iteration : 7652
train acc:  0.890625
train loss:  0.2870687246322632
train gradient:  0.19373199245911496
iteration : 7653
train acc:  0.875
train loss:  0.3064020574092865
train gradient:  0.15842875114030958
iteration : 7654
train acc:  0.875
train loss:  0.3227422535419464
train gradient:  0.17009775753959377
iteration : 7655
train acc:  0.8359375
train loss:  0.32732635736465454
train gradient:  0.19679012766357404
iteration : 7656
train acc:  0.828125
train loss:  0.32150423526763916
train gradient:  0.15999628481532022
iteration : 7657
train acc:  0.8671875
train loss:  0.3131182789802551
train gradient:  0.14483354904447007
iteration : 7658
train acc:  0.8828125
train loss:  0.24370770156383514
train gradient:  0.10670172861725433
iteration : 7659
train acc:  0.8671875
train loss:  0.3108680844306946
train gradient:  0.17838373410746683
iteration : 7660
train acc:  0.8515625
train loss:  0.30299171805381775
train gradient:  0.13001253520423112
iteration : 7661
train acc:  0.890625
train loss:  0.2870928943157196
train gradient:  0.17360402824107968
iteration : 7662
train acc:  0.8125
train loss:  0.4574793875217438
train gradient:  0.31007461976235495
iteration : 7663
train acc:  0.859375
train loss:  0.3301466405391693
train gradient:  0.14504184690896563
iteration : 7664
train acc:  0.8671875
train loss:  0.3475649654865265
train gradient:  0.2740481467093895
iteration : 7665
train acc:  0.8828125
train loss:  0.26685643196105957
train gradient:  0.2511828184700047
iteration : 7666
train acc:  0.8671875
train loss:  0.3107624053955078
train gradient:  0.25328219129262325
iteration : 7667
train acc:  0.859375
train loss:  0.3449622392654419
train gradient:  0.23075566308513834
iteration : 7668
train acc:  0.8046875
train loss:  0.448360413312912
train gradient:  0.3318284914521128
iteration : 7669
train acc:  0.859375
train loss:  0.312274694442749
train gradient:  0.1660112860874504
iteration : 7670
train acc:  0.8203125
train loss:  0.4305182993412018
train gradient:  0.3702411558512038
iteration : 7671
train acc:  0.859375
train loss:  0.3389098048210144
train gradient:  0.22296408261161543
iteration : 7672
train acc:  0.8828125
train loss:  0.30610859394073486
train gradient:  0.1557945466305437
iteration : 7673
train acc:  0.8671875
train loss:  0.3217145502567291
train gradient:  0.1858871486246868
iteration : 7674
train acc:  0.859375
train loss:  0.2963619828224182
train gradient:  0.133901652009158
iteration : 7675
train acc:  0.8359375
train loss:  0.28627809882164
train gradient:  0.18590325863336454
iteration : 7676
train acc:  0.8984375
train loss:  0.2420443296432495
train gradient:  0.13501616908717537
iteration : 7677
train acc:  0.890625
train loss:  0.34644070267677307
train gradient:  0.20396248182765642
iteration : 7678
train acc:  0.8515625
train loss:  0.3008613586425781
train gradient:  0.15738683607605203
iteration : 7679
train acc:  0.8828125
train loss:  0.2914920449256897
train gradient:  0.19785540634785515
iteration : 7680
train acc:  0.8515625
train loss:  0.34580737352371216
train gradient:  0.20822803480587077
iteration : 7681
train acc:  0.8046875
train loss:  0.44110536575317383
train gradient:  0.412195781910274
iteration : 7682
train acc:  0.8671875
train loss:  0.3278964161872864
train gradient:  0.166238065396209
iteration : 7683
train acc:  0.84375
train loss:  0.3357696533203125
train gradient:  0.18741695956795856
iteration : 7684
train acc:  0.8671875
train loss:  0.35010698437690735
train gradient:  0.2023818797909075
iteration : 7685
train acc:  0.90625
train loss:  0.30096763372421265
train gradient:  0.1783724167369317
iteration : 7686
train acc:  0.859375
train loss:  0.33660435676574707
train gradient:  0.16221905991304428
iteration : 7687
train acc:  0.8671875
train loss:  0.37012216448783875
train gradient:  0.262933760275767
iteration : 7688
train acc:  0.8671875
train loss:  0.37324678897857666
train gradient:  0.29515488675898244
iteration : 7689
train acc:  0.8125
train loss:  0.3995438814163208
train gradient:  0.29690352782153256
iteration : 7690
train acc:  0.8515625
train loss:  0.3311161398887634
train gradient:  0.19905955856837673
iteration : 7691
train acc:  0.8359375
train loss:  0.3745341897010803
train gradient:  0.21530192304229234
iteration : 7692
train acc:  0.828125
train loss:  0.3626251518726349
train gradient:  0.20446077013837866
iteration : 7693
train acc:  0.921875
train loss:  0.2740524411201477
train gradient:  0.1907975029764073
iteration : 7694
train acc:  0.828125
train loss:  0.37697333097457886
train gradient:  0.3400159085896719
iteration : 7695
train acc:  0.859375
train loss:  0.2981681227684021
train gradient:  0.16691124381114672
iteration : 7696
train acc:  0.859375
train loss:  0.30673786997795105
train gradient:  0.14527559409650215
iteration : 7697
train acc:  0.84375
train loss:  0.31437885761260986
train gradient:  0.22037041511095512
iteration : 7698
train acc:  0.8515625
train loss:  0.3700768053531647
train gradient:  0.2623403159231853
iteration : 7699
train acc:  0.859375
train loss:  0.324659526348114
train gradient:  0.21129614355339277
iteration : 7700
train acc:  0.7890625
train loss:  0.4448091983795166
train gradient:  0.29023882299513476
iteration : 7701
train acc:  0.859375
train loss:  0.28510335087776184
train gradient:  0.18150399036831188
iteration : 7702
train acc:  0.8359375
train loss:  0.3503313660621643
train gradient:  0.1682641594076843
iteration : 7703
train acc:  0.890625
train loss:  0.27572885155677795
train gradient:  0.13797455105752154
iteration : 7704
train acc:  0.859375
train loss:  0.3176133632659912
train gradient:  0.161607175749871
iteration : 7705
train acc:  0.84375
train loss:  0.32718396186828613
train gradient:  0.2556472928606162
iteration : 7706
train acc:  0.8671875
train loss:  0.2878543436527252
train gradient:  0.15523906978352406
iteration : 7707
train acc:  0.8515625
train loss:  0.32746583223342896
train gradient:  0.2121079246478357
iteration : 7708
train acc:  0.8359375
train loss:  0.37687504291534424
train gradient:  0.2132891635198583
iteration : 7709
train acc:  0.84375
train loss:  0.3840901851654053
train gradient:  0.23142536290624666
iteration : 7710
train acc:  0.8515625
train loss:  0.31309542059898376
train gradient:  0.13430883285630962
iteration : 7711
train acc:  0.84375
train loss:  0.3937167525291443
train gradient:  0.20920985398840897
iteration : 7712
train acc:  0.8671875
train loss:  0.28969836235046387
train gradient:  0.2581195235026592
iteration : 7713
train acc:  0.8671875
train loss:  0.33011794090270996
train gradient:  0.1558341290549421
iteration : 7714
train acc:  0.890625
train loss:  0.2695145905017853
train gradient:  0.14093258909065293
iteration : 7715
train acc:  0.8671875
train loss:  0.31609946489334106
train gradient:  0.23832801289335034
iteration : 7716
train acc:  0.8828125
train loss:  0.32158035039901733
train gradient:  0.16242456812472783
iteration : 7717
train acc:  0.8828125
train loss:  0.29857099056243896
train gradient:  0.16906610148608953
iteration : 7718
train acc:  0.875
train loss:  0.2915235459804535
train gradient:  0.18300043099938618
iteration : 7719
train acc:  0.8515625
train loss:  0.3703392744064331
train gradient:  0.28475709654227455
iteration : 7720
train acc:  0.8984375
train loss:  0.26431459188461304
train gradient:  0.14543211257066135
iteration : 7721
train acc:  0.890625
train loss:  0.30974581837654114
train gradient:  0.23922315914218248
iteration : 7722
train acc:  0.8125
train loss:  0.4480968713760376
train gradient:  0.3692513600156704
iteration : 7723
train acc:  0.875
train loss:  0.3170124888420105
train gradient:  0.20140630886842764
iteration : 7724
train acc:  0.875
train loss:  0.3228418231010437
train gradient:  0.269749132930054
iteration : 7725
train acc:  0.9296875
train loss:  0.24323569238185883
train gradient:  0.13455719508624908
iteration : 7726
train acc:  0.8359375
train loss:  0.3995877802371979
train gradient:  0.19519624925037538
iteration : 7727
train acc:  0.84375
train loss:  0.35621902346611023
train gradient:  0.2919283618291461
iteration : 7728
train acc:  0.828125
train loss:  0.38105130195617676
train gradient:  0.23802686631756145
iteration : 7729
train acc:  0.8515625
train loss:  0.32512688636779785
train gradient:  0.2000326379551723
iteration : 7730
train acc:  0.859375
train loss:  0.32639825344085693
train gradient:  0.2284262998739494
iteration : 7731
train acc:  0.8671875
train loss:  0.33636772632598877
train gradient:  0.2184461379301723
iteration : 7732
train acc:  0.8203125
train loss:  0.3674542307853699
train gradient:  0.22810508938522037
iteration : 7733
train acc:  0.8984375
train loss:  0.2447301745414734
train gradient:  0.11883640199545045
iteration : 7734
train acc:  0.90625
train loss:  0.26189136505126953
train gradient:  0.12566900683564708
iteration : 7735
train acc:  0.8828125
train loss:  0.36383554339408875
train gradient:  0.301923636590714
iteration : 7736
train acc:  0.9140625
train loss:  0.2315506637096405
train gradient:  0.12271669373804307
iteration : 7737
train acc:  0.890625
train loss:  0.2930525541305542
train gradient:  0.19207080282110878
iteration : 7738
train acc:  0.8515625
train loss:  0.311728835105896
train gradient:  0.12631884307000457
iteration : 7739
train acc:  0.84375
train loss:  0.3481031060218811
train gradient:  0.2352845922970737
iteration : 7740
train acc:  0.84375
train loss:  0.3666161298751831
train gradient:  0.24838199210172918
iteration : 7741
train acc:  0.84375
train loss:  0.34325313568115234
train gradient:  0.22758651757058476
iteration : 7742
train acc:  0.8125
train loss:  0.3913692831993103
train gradient:  0.17030841223393484
iteration : 7743
train acc:  0.859375
train loss:  0.32753151655197144
train gradient:  0.1361513673619855
iteration : 7744
train acc:  0.8046875
train loss:  0.37468573451042175
train gradient:  0.2554162111445491
iteration : 7745
train acc:  0.8671875
train loss:  0.28958430886268616
train gradient:  0.2088271074132892
iteration : 7746
train acc:  0.796875
train loss:  0.3610837161540985
train gradient:  0.275924193326187
iteration : 7747
train acc:  0.8828125
train loss:  0.265153706073761
train gradient:  0.183251183348123
iteration : 7748
train acc:  0.84375
train loss:  0.34707555174827576
train gradient:  0.16551580873467525
iteration : 7749
train acc:  0.8203125
train loss:  0.38596421480178833
train gradient:  0.3023288945588531
iteration : 7750
train acc:  0.84375
train loss:  0.326908677816391
train gradient:  0.30477860856326044
iteration : 7751
train acc:  0.8203125
train loss:  0.34358102083206177
train gradient:  0.21397368249254373
iteration : 7752
train acc:  0.8203125
train loss:  0.3347819149494171
train gradient:  0.16703892488706015
iteration : 7753
train acc:  0.8515625
train loss:  0.3441213369369507
train gradient:  0.1967531439438337
iteration : 7754
train acc:  0.8125
train loss:  0.4078543186187744
train gradient:  0.2475806736422165
iteration : 7755
train acc:  0.8515625
train loss:  0.30341392755508423
train gradient:  0.17769279621261466
iteration : 7756
train acc:  0.8515625
train loss:  0.3289790153503418
train gradient:  0.22455082339325588
iteration : 7757
train acc:  0.8203125
train loss:  0.3639751672744751
train gradient:  0.1570901080474655
iteration : 7758
train acc:  0.84375
train loss:  0.4163459539413452
train gradient:  0.34086792547097355
iteration : 7759
train acc:  0.8515625
train loss:  0.29944753646850586
train gradient:  0.16298653835405486
iteration : 7760
train acc:  0.84375
train loss:  0.32288116216659546
train gradient:  0.15199612545224062
iteration : 7761
train acc:  0.8984375
train loss:  0.2614824175834656
train gradient:  0.12082356010198297
iteration : 7762
train acc:  0.8984375
train loss:  0.29669153690338135
train gradient:  0.2897176476800327
iteration : 7763
train acc:  0.8984375
train loss:  0.2624937891960144
train gradient:  0.13362041261223107
iteration : 7764
train acc:  0.84375
train loss:  0.3597712516784668
train gradient:  0.1940115666394497
iteration : 7765
train acc:  0.8203125
train loss:  0.34389621019363403
train gradient:  0.22183695253907731
iteration : 7766
train acc:  0.859375
train loss:  0.3310149013996124
train gradient:  0.1691119527055978
iteration : 7767
train acc:  0.8515625
train loss:  0.35250574350357056
train gradient:  0.2111369278522945
iteration : 7768
train acc:  0.8359375
train loss:  0.36579903960227966
train gradient:  0.23522781718752864
iteration : 7769
train acc:  0.8203125
train loss:  0.3690422475337982
train gradient:  0.2310446180329025
iteration : 7770
train acc:  0.890625
train loss:  0.2527581751346588
train gradient:  0.11703796221408633
iteration : 7771
train acc:  0.84375
train loss:  0.3971800208091736
train gradient:  0.2665175063522108
iteration : 7772
train acc:  0.8984375
train loss:  0.28303033113479614
train gradient:  0.11793756877848469
iteration : 7773
train acc:  0.828125
train loss:  0.3633020520210266
train gradient:  0.21948611276701774
iteration : 7774
train acc:  0.890625
train loss:  0.3016616702079773
train gradient:  0.1334521405765115
iteration : 7775
train acc:  0.84375
train loss:  0.36162668466567993
train gradient:  0.15112326773134444
iteration : 7776
train acc:  0.9140625
train loss:  0.2542839050292969
train gradient:  0.10616862275411883
iteration : 7777
train acc:  0.8515625
train loss:  0.41592755913734436
train gradient:  0.20587923281616854
iteration : 7778
train acc:  0.859375
train loss:  0.30130571126937866
train gradient:  0.14979322127861228
iteration : 7779
train acc:  0.859375
train loss:  0.314879834651947
train gradient:  0.13706983271094694
iteration : 7780
train acc:  0.9140625
train loss:  0.2690511643886566
train gradient:  0.11736044654769083
iteration : 7781
train acc:  0.8828125
train loss:  0.3789267838001251
train gradient:  0.20785685726449732
iteration : 7782
train acc:  0.8515625
train loss:  0.37648898363113403
train gradient:  0.37865185628674825
iteration : 7783
train acc:  0.8828125
train loss:  0.31591683626174927
train gradient:  0.19522794915103936
iteration : 7784
train acc:  0.8125
train loss:  0.4330958127975464
train gradient:  0.25165002765341077
iteration : 7785
train acc:  0.8671875
train loss:  0.33854442834854126
train gradient:  0.32403674815352435
iteration : 7786
train acc:  0.8203125
train loss:  0.3563916087150574
train gradient:  0.2656987168359537
iteration : 7787
train acc:  0.8359375
train loss:  0.36615169048309326
train gradient:  0.26365270392944173
iteration : 7788
train acc:  0.8671875
train loss:  0.28588783740997314
train gradient:  0.17300863753549828
iteration : 7789
train acc:  0.8515625
train loss:  0.3653371334075928
train gradient:  0.3101161942784779
iteration : 7790
train acc:  0.8125
train loss:  0.34899306297302246
train gradient:  0.2652131488452699
iteration : 7791
train acc:  0.859375
train loss:  0.31456589698791504
train gradient:  0.17855685443979252
iteration : 7792
train acc:  0.859375
train loss:  0.31898659467697144
train gradient:  0.19710109514283897
iteration : 7793
train acc:  0.8671875
train loss:  0.28421550989151
train gradient:  0.20022252990303685
iteration : 7794
train acc:  0.875
train loss:  0.32175785303115845
train gradient:  0.20448132343028994
iteration : 7795
train acc:  0.8359375
train loss:  0.3724477291107178
train gradient:  0.22088004362546554
iteration : 7796
train acc:  0.8515625
train loss:  0.36974990367889404
train gradient:  0.22927730108167152
iteration : 7797
train acc:  0.84375
train loss:  0.36533981561660767
train gradient:  0.21777595737661581
iteration : 7798
train acc:  0.8203125
train loss:  0.3158074617385864
train gradient:  0.1458221764045887
iteration : 7799
train acc:  0.875
train loss:  0.3804389238357544
train gradient:  0.21120747887154812
iteration : 7800
train acc:  0.8984375
train loss:  0.30959591269493103
train gradient:  0.10711029681540567
iteration : 7801
train acc:  0.890625
train loss:  0.24569837749004364
train gradient:  0.10428351167422899
iteration : 7802
train acc:  0.84375
train loss:  0.37185347080230713
train gradient:  0.20381123773218374
iteration : 7803
train acc:  0.8671875
train loss:  0.2962621748447418
train gradient:  0.17733434395677952
iteration : 7804
train acc:  0.890625
train loss:  0.2555120587348938
train gradient:  0.11413612226051131
iteration : 7805
train acc:  0.875
train loss:  0.3143010437488556
train gradient:  0.12666279421261348
iteration : 7806
train acc:  0.8671875
train loss:  0.3119259476661682
train gradient:  0.16633517353105354
iteration : 7807
train acc:  0.8515625
train loss:  0.31760790944099426
train gradient:  0.1726846430651921
iteration : 7808
train acc:  0.890625
train loss:  0.32109206914901733
train gradient:  0.18093982934803604
iteration : 7809
train acc:  0.8203125
train loss:  0.40751439332962036
train gradient:  0.3629439997616914
iteration : 7810
train acc:  0.859375
train loss:  0.34851306676864624
train gradient:  0.19655983352460582
iteration : 7811
train acc:  0.796875
train loss:  0.37800508737564087
train gradient:  0.24063678910915803
iteration : 7812
train acc:  0.875
train loss:  0.3473963737487793
train gradient:  0.25650339384481774
iteration : 7813
train acc:  0.890625
train loss:  0.3358888626098633
train gradient:  0.19583151631441642
iteration : 7814
train acc:  0.84375
train loss:  0.33762332797050476
train gradient:  0.16327964375172466
iteration : 7815
train acc:  0.859375
train loss:  0.30834266543388367
train gradient:  0.15721943244901654
iteration : 7816
train acc:  0.8515625
train loss:  0.28498268127441406
train gradient:  0.1497825171018574
iteration : 7817
train acc:  0.859375
train loss:  0.3607385754585266
train gradient:  0.14608518786168712
iteration : 7818
train acc:  0.8515625
train loss:  0.364591121673584
train gradient:  0.2095611375350713
iteration : 7819
train acc:  0.859375
train loss:  0.3294851779937744
train gradient:  0.30516688651177215
iteration : 7820
train acc:  0.875
train loss:  0.2827188968658447
train gradient:  0.15739051150478628
iteration : 7821
train acc:  0.859375
train loss:  0.3246479630470276
train gradient:  0.1922343400145478
iteration : 7822
train acc:  0.875
train loss:  0.27663424611091614
train gradient:  0.13725115928125253
iteration : 7823
train acc:  0.8046875
train loss:  0.4368017017841339
train gradient:  0.2941830659549013
iteration : 7824
train acc:  0.828125
train loss:  0.36018240451812744
train gradient:  0.20516865813093513
iteration : 7825
train acc:  0.828125
train loss:  0.3257456421852112
train gradient:  0.23225240839102368
iteration : 7826
train acc:  0.859375
train loss:  0.3622424006462097
train gradient:  0.17921202106784198
iteration : 7827
train acc:  0.7890625
train loss:  0.4690799117088318
train gradient:  0.2868657488487834
iteration : 7828
train acc:  0.8046875
train loss:  0.3554003834724426
train gradient:  0.25273040674428193
iteration : 7829
train acc:  0.84375
train loss:  0.37185704708099365
train gradient:  0.23048540060720762
iteration : 7830
train acc:  0.8671875
train loss:  0.35233962535858154
train gradient:  0.14618972596662388
iteration : 7831
train acc:  0.9140625
train loss:  0.23409563302993774
train gradient:  0.1180385688731133
iteration : 7832
train acc:  0.8515625
train loss:  0.3696324825286865
train gradient:  0.2133130271055026
iteration : 7833
train acc:  0.8046875
train loss:  0.4759238064289093
train gradient:  0.363671409492137
iteration : 7834
train acc:  0.8515625
train loss:  0.333997905254364
train gradient:  0.16555982123031748
iteration : 7835
train acc:  0.8203125
train loss:  0.40488237142562866
train gradient:  0.38688869676837373
iteration : 7836
train acc:  0.8671875
train loss:  0.34409016370773315
train gradient:  0.19095028332679148
iteration : 7837
train acc:  0.8671875
train loss:  0.3083939254283905
train gradient:  0.17204551658958317
iteration : 7838
train acc:  0.828125
train loss:  0.3307912051677704
train gradient:  0.1551699351082795
iteration : 7839
train acc:  0.84375
train loss:  0.34557613730430603
train gradient:  0.23904841542185584
iteration : 7840
train acc:  0.8515625
train loss:  0.3322780430316925
train gradient:  0.16515842517555232
iteration : 7841
train acc:  0.8828125
train loss:  0.30935001373291016
train gradient:  0.20821288527034204
iteration : 7842
train acc:  0.9140625
train loss:  0.30689412355422974
train gradient:  0.18907517108833533
iteration : 7843
train acc:  0.890625
train loss:  0.315515398979187
train gradient:  0.1357381505651396
iteration : 7844
train acc:  0.8671875
train loss:  0.30603116750717163
train gradient:  0.14772781305631566
iteration : 7845
train acc:  0.8359375
train loss:  0.35245025157928467
train gradient:  0.2479650644738779
iteration : 7846
train acc:  0.8359375
train loss:  0.36743658781051636
train gradient:  0.18330429998872377
iteration : 7847
train acc:  0.796875
train loss:  0.4055559039115906
train gradient:  0.31546357776013667
iteration : 7848
train acc:  0.765625
train loss:  0.5283017158508301
train gradient:  0.32560389624265834
iteration : 7849
train acc:  0.859375
train loss:  0.32679271697998047
train gradient:  0.24856488315614123
iteration : 7850
train acc:  0.828125
train loss:  0.3256535530090332
train gradient:  0.1623617571835994
iteration : 7851
train acc:  0.796875
train loss:  0.3862305283546448
train gradient:  0.2571091756492751
iteration : 7852
train acc:  0.8671875
train loss:  0.3869713544845581
train gradient:  0.21964496372869452
iteration : 7853
train acc:  0.8203125
train loss:  0.4180615544319153
train gradient:  0.24110891981012827
iteration : 7854
train acc:  0.7734375
train loss:  0.4677794575691223
train gradient:  0.3551335725907973
iteration : 7855
train acc:  0.84375
train loss:  0.32501548528671265
train gradient:  0.13436455492205324
iteration : 7856
train acc:  0.8671875
train loss:  0.28959155082702637
train gradient:  0.15264629842942135
iteration : 7857
train acc:  0.875
train loss:  0.29850631952285767
train gradient:  0.15189108146940705
iteration : 7858
train acc:  0.8984375
train loss:  0.3101760745048523
train gradient:  0.17087347920155355
iteration : 7859
train acc:  0.8671875
train loss:  0.3425237834453583
train gradient:  0.12280800051618
iteration : 7860
train acc:  0.859375
train loss:  0.29629239439964294
train gradient:  0.1443982797385054
iteration : 7861
train acc:  0.8828125
train loss:  0.25675514340400696
train gradient:  0.13394118845319103
iteration : 7862
train acc:  0.8359375
train loss:  0.3382601737976074
train gradient:  0.1365072283437856
iteration : 7863
train acc:  0.8515625
train loss:  0.3472897410392761
train gradient:  0.16741765312397683
iteration : 7864
train acc:  0.84375
train loss:  0.35624271631240845
train gradient:  0.16941297986983112
iteration : 7865
train acc:  0.875
train loss:  0.2878764867782593
train gradient:  0.12709427283731894
iteration : 7866
train acc:  0.859375
train loss:  0.3275314271450043
train gradient:  0.13849218899436283
iteration : 7867
train acc:  0.859375
train loss:  0.3523094356060028
train gradient:  0.16899450867369975
iteration : 7868
train acc:  0.8828125
train loss:  0.29968929290771484
train gradient:  0.11661488382275968
iteration : 7869
train acc:  0.828125
train loss:  0.37101203203201294
train gradient:  0.24863809475578671
iteration : 7870
train acc:  0.890625
train loss:  0.24732014536857605
train gradient:  0.08930919726212228
iteration : 7871
train acc:  0.828125
train loss:  0.3789769411087036
train gradient:  0.16469754090988664
iteration : 7872
train acc:  0.8125
train loss:  0.4137907028198242
train gradient:  0.22765890025034527
iteration : 7873
train acc:  0.84375
train loss:  0.3670732378959656
train gradient:  0.22061232334803565
iteration : 7874
train acc:  0.8828125
train loss:  0.3096582293510437
train gradient:  0.16401367011365656
iteration : 7875
train acc:  0.859375
train loss:  0.3166910409927368
train gradient:  0.15701964004069752
iteration : 7876
train acc:  0.8515625
train loss:  0.32618066668510437
train gradient:  0.24352676778298754
iteration : 7877
train acc:  0.8515625
train loss:  0.3418799638748169
train gradient:  0.14992785544132192
iteration : 7878
train acc:  0.90625
train loss:  0.23982685804367065
train gradient:  0.11517081371051145
iteration : 7879
train acc:  0.8125
train loss:  0.39952385425567627
train gradient:  0.13921892812064277
iteration : 7880
train acc:  0.8984375
train loss:  0.2846849858760834
train gradient:  0.1081123976346551
iteration : 7881
train acc:  0.828125
train loss:  0.36171260476112366
train gradient:  0.19111208605471547
iteration : 7882
train acc:  0.890625
train loss:  0.3150655925273895
train gradient:  0.1900360512744045
iteration : 7883
train acc:  0.90625
train loss:  0.2938465178012848
train gradient:  0.38335244716272804
iteration : 7884
train acc:  0.7890625
train loss:  0.4706076979637146
train gradient:  0.3321482757876104
iteration : 7885
train acc:  0.890625
train loss:  0.2766174376010895
train gradient:  0.1494530731431378
iteration : 7886
train acc:  0.8828125
train loss:  0.2837526798248291
train gradient:  0.133102631872678
iteration : 7887
train acc:  0.8984375
train loss:  0.28657740354537964
train gradient:  0.13209327337652854
iteration : 7888
train acc:  0.8359375
train loss:  0.31850823760032654
train gradient:  0.13062044473879145
iteration : 7889
train acc:  0.8515625
train loss:  0.4013921618461609
train gradient:  0.22366866253209597
iteration : 7890
train acc:  0.8359375
train loss:  0.3796173930168152
train gradient:  0.17157426279101856
iteration : 7891
train acc:  0.828125
train loss:  0.34123384952545166
train gradient:  0.1773393537164545
iteration : 7892
train acc:  0.875
train loss:  0.2983534336090088
train gradient:  0.18668038640349194
iteration : 7893
train acc:  0.828125
train loss:  0.321556031703949
train gradient:  0.16080843819295326
iteration : 7894
train acc:  0.8828125
train loss:  0.28724968433380127
train gradient:  0.13270268029182697
iteration : 7895
train acc:  0.8671875
train loss:  0.33321818709373474
train gradient:  0.2275431143312715
iteration : 7896
train acc:  0.859375
train loss:  0.3027411997318268
train gradient:  0.14689346429030659
iteration : 7897
train acc:  0.78125
train loss:  0.4641326665878296
train gradient:  0.5794601024153097
iteration : 7898
train acc:  0.890625
train loss:  0.30508771538734436
train gradient:  0.11976662874632678
iteration : 7899
train acc:  0.859375
train loss:  0.3244054317474365
train gradient:  0.18782172496813665
iteration : 7900
train acc:  0.90625
train loss:  0.2609972357749939
train gradient:  0.1955325042459174
iteration : 7901
train acc:  0.859375
train loss:  0.32347410917282104
train gradient:  0.26267644668801227
iteration : 7902
train acc:  0.8203125
train loss:  0.386127769947052
train gradient:  0.24580109485276555
iteration : 7903
train acc:  0.84375
train loss:  0.3502694368362427
train gradient:  0.16107147567021923
iteration : 7904
train acc:  0.8671875
train loss:  0.32595381140708923
train gradient:  0.17971649938183457
iteration : 7905
train acc:  0.8515625
train loss:  0.33298027515411377
train gradient:  0.20809016893783955
iteration : 7906
train acc:  0.8828125
train loss:  0.28274548053741455
train gradient:  0.1712237348262135
iteration : 7907
train acc:  0.828125
train loss:  0.3953939378261566
train gradient:  0.2535648695524873
iteration : 7908
train acc:  0.8359375
train loss:  0.382335364818573
train gradient:  0.22352211066014516
iteration : 7909
train acc:  0.890625
train loss:  0.25965186953544617
train gradient:  0.15129511505051707
iteration : 7910
train acc:  0.8515625
train loss:  0.33587637543678284
train gradient:  0.18276110361471104
iteration : 7911
train acc:  0.84375
train loss:  0.3751174509525299
train gradient:  0.17512579701387196
iteration : 7912
train acc:  0.875
train loss:  0.32909202575683594
train gradient:  0.20212871154894277
iteration : 7913
train acc:  0.8984375
train loss:  0.2986292839050293
train gradient:  0.1927010531337495
iteration : 7914
train acc:  0.8671875
train loss:  0.3930118680000305
train gradient:  0.28124228123972134
iteration : 7915
train acc:  0.875
train loss:  0.3409261703491211
train gradient:  0.21020681416771975
iteration : 7916
train acc:  0.8984375
train loss:  0.27124443650245667
train gradient:  0.1252005000008025
iteration : 7917
train acc:  0.8515625
train loss:  0.3884161710739136
train gradient:  0.23940308622292317
iteration : 7918
train acc:  0.828125
train loss:  0.43498528003692627
train gradient:  0.3121789612186793
iteration : 7919
train acc:  0.828125
train loss:  0.46145370602607727
train gradient:  0.2686749658105879
iteration : 7920
train acc:  0.8046875
train loss:  0.374285489320755
train gradient:  0.2794449767831555
iteration : 7921
train acc:  0.8828125
train loss:  0.2913093864917755
train gradient:  0.14418203048286143
iteration : 7922
train acc:  0.8828125
train loss:  0.27247434854507446
train gradient:  0.1267621550933552
iteration : 7923
train acc:  0.875
train loss:  0.2896575927734375
train gradient:  0.1389696715048961
iteration : 7924
train acc:  0.828125
train loss:  0.35596662759780884
train gradient:  0.21260586889475036
iteration : 7925
train acc:  0.8359375
train loss:  0.35882237553596497
train gradient:  0.20307576436918434
iteration : 7926
train acc:  0.8125
train loss:  0.3692343235015869
train gradient:  0.2536654707636203
iteration : 7927
train acc:  0.875
train loss:  0.2706405520439148
train gradient:  0.15405146429626185
iteration : 7928
train acc:  0.8671875
train loss:  0.32371214032173157
train gradient:  0.21694243574554384
iteration : 7929
train acc:  0.84375
train loss:  0.3511408567428589
train gradient:  0.20107292753981654
iteration : 7930
train acc:  0.8359375
train loss:  0.31712406873703003
train gradient:  0.21275419787960034
iteration : 7931
train acc:  0.875
train loss:  0.28780221939086914
train gradient:  0.1625987564856083
iteration : 7932
train acc:  0.8515625
train loss:  0.32122254371643066
train gradient:  0.22171923284287332
iteration : 7933
train acc:  0.8671875
train loss:  0.3274049162864685
train gradient:  0.15872467930043976
iteration : 7934
train acc:  0.8671875
train loss:  0.3733341693878174
train gradient:  0.1759910510270075
iteration : 7935
train acc:  0.859375
train loss:  0.3686293363571167
train gradient:  0.2696087067020707
iteration : 7936
train acc:  0.8828125
train loss:  0.29508358240127563
train gradient:  0.21104359535127576
iteration : 7937
train acc:  0.875
train loss:  0.2632715404033661
train gradient:  0.13503482497901353
iteration : 7938
train acc:  0.7890625
train loss:  0.4321294128894806
train gradient:  0.3225615098882004
iteration : 7939
train acc:  0.78125
train loss:  0.4820437431335449
train gradient:  0.37100033655044795
iteration : 7940
train acc:  0.8515625
train loss:  0.4005112051963806
train gradient:  0.22238932779459541
iteration : 7941
train acc:  0.90625
train loss:  0.2591165602207184
train gradient:  0.09158986335284763
iteration : 7942
train acc:  0.8828125
train loss:  0.2763962149620056
train gradient:  0.14769958229835795
iteration : 7943
train acc:  0.8828125
train loss:  0.27670419216156006
train gradient:  0.11688215031951185
iteration : 7944
train acc:  0.90625
train loss:  0.2506398558616638
train gradient:  0.12761063997582053
iteration : 7945
train acc:  0.8515625
train loss:  0.35490870475769043
train gradient:  0.1650503260244568
iteration : 7946
train acc:  0.7890625
train loss:  0.43276286125183105
train gradient:  0.2675236755122196
iteration : 7947
train acc:  0.8515625
train loss:  0.3989022374153137
train gradient:  0.22824739887753748
iteration : 7948
train acc:  0.8203125
train loss:  0.4042888581752777
train gradient:  0.24590851456190085
iteration : 7949
train acc:  0.8515625
train loss:  0.30638062953948975
train gradient:  0.1542172699527917
iteration : 7950
train acc:  0.7890625
train loss:  0.3986360430717468
train gradient:  0.2563586174491831
iteration : 7951
train acc:  0.8515625
train loss:  0.30910158157348633
train gradient:  0.22878818272501508
iteration : 7952
train acc:  0.8359375
train loss:  0.37072068452835083
train gradient:  0.31733863189307226
iteration : 7953
train acc:  0.84375
train loss:  0.3635881841182709
train gradient:  0.16574142457874966
iteration : 7954
train acc:  0.8515625
train loss:  0.3788180351257324
train gradient:  0.2181848282871508
iteration : 7955
train acc:  0.8515625
train loss:  0.38631072640419006
train gradient:  0.27988734571011537
iteration : 7956
train acc:  0.8671875
train loss:  0.3092818856239319
train gradient:  0.1272983385607821
iteration : 7957
train acc:  0.8515625
train loss:  0.47137993574142456
train gradient:  0.26507960872907416
iteration : 7958
train acc:  0.890625
train loss:  0.2922375202178955
train gradient:  0.13051857905115286
iteration : 7959
train acc:  0.8203125
train loss:  0.3290748596191406
train gradient:  0.15403041242734006
iteration : 7960
train acc:  0.9140625
train loss:  0.24908548593521118
train gradient:  0.13864253755521383
iteration : 7961
train acc:  0.8359375
train loss:  0.3290405571460724
train gradient:  0.1526682893691751
iteration : 7962
train acc:  0.796875
train loss:  0.4503380358219147
train gradient:  0.32182388967964426
iteration : 7963
train acc:  0.84375
train loss:  0.349281907081604
train gradient:  0.1718290159221898
iteration : 7964
train acc:  0.796875
train loss:  0.3660926818847656
train gradient:  0.16876940056255824
iteration : 7965
train acc:  0.8359375
train loss:  0.38575732707977295
train gradient:  0.19338925467861576
iteration : 7966
train acc:  0.8515625
train loss:  0.34888774156570435
train gradient:  0.17641387897517774
iteration : 7967
train acc:  0.8359375
train loss:  0.37504664063453674
train gradient:  0.20890847109843108
iteration : 7968
train acc:  0.8671875
train loss:  0.288386732339859
train gradient:  0.14083638837494797
iteration : 7969
train acc:  0.875
train loss:  0.3162534236907959
train gradient:  0.15881878860894144
iteration : 7970
train acc:  0.8671875
train loss:  0.27112752199172974
train gradient:  0.10854023265541186
iteration : 7971
train acc:  0.8828125
train loss:  0.3493101894855499
train gradient:  0.18986290771280456
iteration : 7972
train acc:  0.859375
train loss:  0.32339122891426086
train gradient:  0.18634558921980193
iteration : 7973
train acc:  0.8515625
train loss:  0.3359748125076294
train gradient:  0.14260862787288936
iteration : 7974
train acc:  0.8984375
train loss:  0.3090789020061493
train gradient:  0.28936659448021895
iteration : 7975
train acc:  0.8125
train loss:  0.3391806483268738
train gradient:  0.1692541299856187
iteration : 7976
train acc:  0.8671875
train loss:  0.3458153009414673
train gradient:  0.1421179010010973
iteration : 7977
train acc:  0.828125
train loss:  0.3541860580444336
train gradient:  0.3051129817160416
iteration : 7978
train acc:  0.84375
train loss:  0.3227355182170868
train gradient:  0.17986512542229743
iteration : 7979
train acc:  0.8046875
train loss:  0.3460904359817505
train gradient:  0.2251641541657075
iteration : 7980
train acc:  0.7890625
train loss:  0.3614500164985657
train gradient:  0.18111582752944605
iteration : 7981
train acc:  0.8359375
train loss:  0.3618965148925781
train gradient:  0.1682934007972015
iteration : 7982
train acc:  0.8984375
train loss:  0.24082401394844055
train gradient:  0.081260921992158
iteration : 7983
train acc:  0.8984375
train loss:  0.2832620441913605
train gradient:  0.16558291113505885
iteration : 7984
train acc:  0.859375
train loss:  0.3001570403575897
train gradient:  0.16246780497093138
iteration : 7985
train acc:  0.8984375
train loss:  0.2779456079006195
train gradient:  0.1024521176644932
iteration : 7986
train acc:  0.8828125
train loss:  0.30399057269096375
train gradient:  0.2776289865186913
iteration : 7987
train acc:  0.8359375
train loss:  0.37128379940986633
train gradient:  0.20771047119432412
iteration : 7988
train acc:  0.921875
train loss:  0.2491251528263092
train gradient:  0.09691904453102435
iteration : 7989
train acc:  0.859375
train loss:  0.3135198950767517
train gradient:  0.12686128367895988
iteration : 7990
train acc:  0.8359375
train loss:  0.38253527879714966
train gradient:  0.23680123019359867
iteration : 7991
train acc:  0.8515625
train loss:  0.3838267922401428
train gradient:  0.26546734672461886
iteration : 7992
train acc:  0.859375
train loss:  0.37687140703201294
train gradient:  0.16151004085804072
iteration : 7993
train acc:  0.84375
train loss:  0.362249493598938
train gradient:  0.20949138889547883
iteration : 7994
train acc:  0.875
train loss:  0.27763912081718445
train gradient:  0.12080673498379012
iteration : 7995
train acc:  0.8359375
train loss:  0.31745028495788574
train gradient:  0.16649006835661725
iteration : 7996
train acc:  0.859375
train loss:  0.30889925360679626
train gradient:  0.152057575115968
iteration : 7997
train acc:  0.8359375
train loss:  0.3583316206932068
train gradient:  0.2453749500209967
iteration : 7998
train acc:  0.84375
train loss:  0.36290502548217773
train gradient:  0.1711823452129555
iteration : 7999
train acc:  0.8359375
train loss:  0.3918132483959198
train gradient:  0.3050513876444019
iteration : 8000
train acc:  0.8046875
train loss:  0.4718456268310547
train gradient:  0.2854993113001551
iteration : 8001
train acc:  0.8671875
train loss:  0.3846972584724426
train gradient:  0.19300437382817198
iteration : 8002
train acc:  0.8515625
train loss:  0.30230405926704407
train gradient:  0.1593361251996594
iteration : 8003
train acc:  0.828125
train loss:  0.34539148211479187
train gradient:  0.23129145934055917
iteration : 8004
train acc:  0.7890625
train loss:  0.35078978538513184
train gradient:  0.23411126657915432
iteration : 8005
train acc:  0.7890625
train loss:  0.4385538101196289
train gradient:  0.23260123039246136
iteration : 8006
train acc:  0.875
train loss:  0.2851586937904358
train gradient:  0.10545587760226512
iteration : 8007
train acc:  0.875
train loss:  0.3003537058830261
train gradient:  0.14767379741439635
iteration : 8008
train acc:  0.8671875
train loss:  0.3099369406700134
train gradient:  0.1463952687487577
iteration : 8009
train acc:  0.765625
train loss:  0.43831562995910645
train gradient:  0.24565254411203996
iteration : 8010
train acc:  0.875
train loss:  0.334430992603302
train gradient:  0.14536844111469172
iteration : 8011
train acc:  0.8828125
train loss:  0.25866928696632385
train gradient:  0.12277481543045732
iteration : 8012
train acc:  0.8203125
train loss:  0.4400537312030792
train gradient:  0.29260828266209576
iteration : 8013
train acc:  0.84375
train loss:  0.3829672932624817
train gradient:  0.27646572548315856
iteration : 8014
train acc:  0.8125
train loss:  0.4161638617515564
train gradient:  0.1784480334937767
iteration : 8015
train acc:  0.890625
train loss:  0.27835047245025635
train gradient:  0.1559732169234517
iteration : 8016
train acc:  0.8828125
train loss:  0.3289353549480438
train gradient:  0.1517397902539857
iteration : 8017
train acc:  0.8515625
train loss:  0.3328424096107483
train gradient:  0.1618092772870721
iteration : 8018
train acc:  0.8515625
train loss:  0.346574068069458
train gradient:  0.18758491550759915
iteration : 8019
train acc:  0.8828125
train loss:  0.2882675528526306
train gradient:  0.13603943120344322
iteration : 8020
train acc:  0.875
train loss:  0.34379762411117554
train gradient:  0.19423090870417165
iteration : 8021
train acc:  0.7890625
train loss:  0.4296233057975769
train gradient:  0.23748006471700572
iteration : 8022
train acc:  0.8515625
train loss:  0.3450111448764801
train gradient:  0.22516626091258124
iteration : 8023
train acc:  0.8984375
train loss:  0.3052007555961609
train gradient:  0.18469422956045625
iteration : 8024
train acc:  0.828125
train loss:  0.4017406105995178
train gradient:  0.18700629286066964
iteration : 8025
train acc:  0.8984375
train loss:  0.2640199065208435
train gradient:  0.14241394026560406
iteration : 8026
train acc:  0.875
train loss:  0.3321303427219391
train gradient:  0.2292414054286756
iteration : 8027
train acc:  0.8671875
train loss:  0.304263174533844
train gradient:  0.15316849919997008
iteration : 8028
train acc:  0.8671875
train loss:  0.35182642936706543
train gradient:  0.1450809110332764
iteration : 8029
train acc:  0.8671875
train loss:  0.36835113167762756
train gradient:  0.15205024357236516
iteration : 8030
train acc:  0.859375
train loss:  0.297638475894928
train gradient:  0.16892137980135338
iteration : 8031
train acc:  0.8359375
train loss:  0.2961120307445526
train gradient:  0.14571719673386355
iteration : 8032
train acc:  0.84375
train loss:  0.37297147512435913
train gradient:  0.14388365697038605
iteration : 8033
train acc:  0.8125
train loss:  0.40695369243621826
train gradient:  0.41658764786708
iteration : 8034
train acc:  0.8359375
train loss:  0.37464556097984314
train gradient:  0.1987871762272344
iteration : 8035
train acc:  0.8203125
train loss:  0.40957653522491455
train gradient:  0.2032862442579492
iteration : 8036
train acc:  0.875
train loss:  0.3383881151676178
train gradient:  0.1516173502985765
iteration : 8037
train acc:  0.859375
train loss:  0.32717934250831604
train gradient:  0.184022879853963
iteration : 8038
train acc:  0.859375
train loss:  0.31295114755630493
train gradient:  0.1394620921058516
iteration : 8039
train acc:  0.84375
train loss:  0.36693668365478516
train gradient:  0.2614711797719442
iteration : 8040
train acc:  0.8515625
train loss:  0.30415597558021545
train gradient:  0.11349063271436362
iteration : 8041
train acc:  0.8828125
train loss:  0.3108353614807129
train gradient:  0.12009344065279345
iteration : 8042
train acc:  0.8828125
train loss:  0.2965037226676941
train gradient:  0.14284550783187774
iteration : 8043
train acc:  0.84375
train loss:  0.3340577781200409
train gradient:  0.15570610574886418
iteration : 8044
train acc:  0.8125
train loss:  0.3921565115451813
train gradient:  0.23068502613817587
iteration : 8045
train acc:  0.828125
train loss:  0.3860612213611603
train gradient:  0.28176056192448806
iteration : 8046
train acc:  0.890625
train loss:  0.2831394374370575
train gradient:  0.15499858584312923
iteration : 8047
train acc:  0.84375
train loss:  0.3161833882331848
train gradient:  0.17062047327034677
iteration : 8048
train acc:  0.8828125
train loss:  0.30055633187294006
train gradient:  0.1263484933347354
iteration : 8049
train acc:  0.8515625
train loss:  0.3351909816265106
train gradient:  0.2105193625292367
iteration : 8050
train acc:  0.9296875
train loss:  0.24418175220489502
train gradient:  0.11100083043707341
iteration : 8051
train acc:  0.8671875
train loss:  0.30692818760871887
train gradient:  0.13958289153798303
iteration : 8052
train acc:  0.8828125
train loss:  0.2897189259529114
train gradient:  0.17179444808485894
iteration : 8053
train acc:  0.8671875
train loss:  0.3712863624095917
train gradient:  0.23340348441881947
iteration : 8054
train acc:  0.8515625
train loss:  0.32477694749832153
train gradient:  0.213980799086563
iteration : 8055
train acc:  0.8125
train loss:  0.35207515954971313
train gradient:  0.1656513185159792
iteration : 8056
train acc:  0.8515625
train loss:  0.3717525601387024
train gradient:  0.177272721213815
iteration : 8057
train acc:  0.8515625
train loss:  0.32515203952789307
train gradient:  0.14778983174781404
iteration : 8058
train acc:  0.8515625
train loss:  0.3098493814468384
train gradient:  0.14765635524216741
iteration : 8059
train acc:  0.8046875
train loss:  0.3917831480503082
train gradient:  0.24510687050324997
iteration : 8060
train acc:  0.8515625
train loss:  0.3503296971321106
train gradient:  0.214716451727204
iteration : 8061
train acc:  0.8671875
train loss:  0.367942214012146
train gradient:  0.16329110161944022
iteration : 8062
train acc:  0.859375
train loss:  0.3166660964488983
train gradient:  0.19504314118090066
iteration : 8063
train acc:  0.859375
train loss:  0.3898716866970062
train gradient:  1.1071801656720368
iteration : 8064
train acc:  0.8515625
train loss:  0.3000236749649048
train gradient:  0.15341119943262627
iteration : 8065
train acc:  0.875
train loss:  0.2637297213077545
train gradient:  0.12124384157105461
iteration : 8066
train acc:  0.8203125
train loss:  0.406541109085083
train gradient:  0.27923976605419426
iteration : 8067
train acc:  0.8359375
train loss:  0.3606354594230652
train gradient:  0.21722864518648755
iteration : 8068
train acc:  0.875
train loss:  0.27496135234832764
train gradient:  0.1625447754006286
iteration : 8069
train acc:  0.8828125
train loss:  0.32487544417381287
train gradient:  0.16484146503197986
iteration : 8070
train acc:  0.8671875
train loss:  0.28556668758392334
train gradient:  0.15522447820840524
iteration : 8071
train acc:  0.859375
train loss:  0.3126394748687744
train gradient:  0.19005887680949776
iteration : 8072
train acc:  0.875
train loss:  0.37212884426116943
train gradient:  0.19481753680690778
iteration : 8073
train acc:  0.9140625
train loss:  0.3241010904312134
train gradient:  0.17416409988930254
iteration : 8074
train acc:  0.8671875
train loss:  0.2968212068080902
train gradient:  0.11367357530904751
iteration : 8075
train acc:  0.8125
train loss:  0.3298211991786957
train gradient:  0.20804462900929482
iteration : 8076
train acc:  0.859375
train loss:  0.30176180601119995
train gradient:  0.16424308489102313
iteration : 8077
train acc:  0.859375
train loss:  0.34922051429748535
train gradient:  0.17255703410007306
iteration : 8078
train acc:  0.8671875
train loss:  0.32925352454185486
train gradient:  0.23202771141611
iteration : 8079
train acc:  0.875
train loss:  0.3196599781513214
train gradient:  0.20678172964788688
iteration : 8080
train acc:  0.8828125
train loss:  0.3062577545642853
train gradient:  0.33141542548941755
iteration : 8081
train acc:  0.890625
train loss:  0.28436899185180664
train gradient:  0.18353806080929594
iteration : 8082
train acc:  0.8828125
train loss:  0.28896111249923706
train gradient:  0.21106625617889616
iteration : 8083
train acc:  0.84375
train loss:  0.35603800415992737
train gradient:  0.22296079186234713
iteration : 8084
train acc:  0.8671875
train loss:  0.30882465839385986
train gradient:  0.28296464064406945
iteration : 8085
train acc:  0.8359375
train loss:  0.34992218017578125
train gradient:  0.20435239336558803
iteration : 8086
train acc:  0.84375
train loss:  0.29423314332962036
train gradient:  0.1475652021020017
iteration : 8087
train acc:  0.8515625
train loss:  0.4030861556529999
train gradient:  0.23443280972856467
iteration : 8088
train acc:  0.828125
train loss:  0.3852217197418213
train gradient:  0.1731037069844438
iteration : 8089
train acc:  0.7890625
train loss:  0.39439958333969116
train gradient:  0.24845200970517034
iteration : 8090
train acc:  0.8984375
train loss:  0.3338380455970764
train gradient:  0.12101359099796079
iteration : 8091
train acc:  0.8984375
train loss:  0.2936438322067261
train gradient:  0.11980985771435473
iteration : 8092
train acc:  0.78125
train loss:  0.41540247201919556
train gradient:  0.2557174748443898
iteration : 8093
train acc:  0.78125
train loss:  0.4510771632194519
train gradient:  0.4072993060700461
iteration : 8094
train acc:  0.8046875
train loss:  0.41400596499443054
train gradient:  0.593464957640518
iteration : 8095
train acc:  0.84375
train loss:  0.354955792427063
train gradient:  0.15158375649256833
iteration : 8096
train acc:  0.8359375
train loss:  0.36940380930900574
train gradient:  0.27434542542557505
iteration : 8097
train acc:  0.8359375
train loss:  0.3153856098651886
train gradient:  0.2832344271367681
iteration : 8098
train acc:  0.8984375
train loss:  0.2994786500930786
train gradient:  0.15737778553672807
iteration : 8099
train acc:  0.8828125
train loss:  0.30007219314575195
train gradient:  0.10828725725795947
iteration : 8100
train acc:  0.8828125
train loss:  0.2875257730484009
train gradient:  0.15003453255297394
iteration : 8101
train acc:  0.9140625
train loss:  0.22325268387794495
train gradient:  0.15161775574857284
iteration : 8102
train acc:  0.875
train loss:  0.3952600061893463
train gradient:  0.22918329014017785
iteration : 8103
train acc:  0.828125
train loss:  0.3926994204521179
train gradient:  0.21145507817210965
iteration : 8104
train acc:  0.859375
train loss:  0.33000048995018005
train gradient:  0.1679938269215322
iteration : 8105
train acc:  0.828125
train loss:  0.3540850877761841
train gradient:  0.18784339842454667
iteration : 8106
train acc:  0.875
train loss:  0.3166249990463257
train gradient:  0.14702576947160667
iteration : 8107
train acc:  0.8359375
train loss:  0.34004566073417664
train gradient:  0.1589033707170706
iteration : 8108
train acc:  0.8671875
train loss:  0.3416881263256073
train gradient:  0.21409038440705144
iteration : 8109
train acc:  0.8203125
train loss:  0.35185402631759644
train gradient:  0.19491128478952635
iteration : 8110
train acc:  0.875
train loss:  0.352190226316452
train gradient:  0.22192114004760655
iteration : 8111
train acc:  0.8515625
train loss:  0.34197473526000977
train gradient:  0.213883338338494
iteration : 8112
train acc:  0.8203125
train loss:  0.3513050973415375
train gradient:  0.20507005222911215
iteration : 8113
train acc:  0.859375
train loss:  0.34168654680252075
train gradient:  0.15047321186214277
iteration : 8114
train acc:  0.8984375
train loss:  0.3143506348133087
train gradient:  0.1482700581769669
iteration : 8115
train acc:  0.890625
train loss:  0.3033079206943512
train gradient:  0.14694832402380598
iteration : 8116
train acc:  0.8671875
train loss:  0.3727169334888458
train gradient:  0.23508653513528033
iteration : 8117
train acc:  0.8828125
train loss:  0.3042641282081604
train gradient:  0.1784305199831311
iteration : 8118
train acc:  0.84375
train loss:  0.39897996187210083
train gradient:  0.2644553481356688
iteration : 8119
train acc:  0.828125
train loss:  0.3816159963607788
train gradient:  0.27952560771268986
iteration : 8120
train acc:  0.8671875
train loss:  0.32386231422424316
train gradient:  0.14228682755700447
iteration : 8121
train acc:  0.8046875
train loss:  0.4101877808570862
train gradient:  0.230561702835949
iteration : 8122
train acc:  0.8984375
train loss:  0.3161383867263794
train gradient:  0.14211178963903745
iteration : 8123
train acc:  0.8046875
train loss:  0.3861902356147766
train gradient:  0.19739993696047503
iteration : 8124
train acc:  0.8671875
train loss:  0.35415002703666687
train gradient:  0.21677792582776192
iteration : 8125
train acc:  0.8359375
train loss:  0.33698076009750366
train gradient:  0.20836148062162507
iteration : 8126
train acc:  0.8046875
train loss:  0.3677687644958496
train gradient:  0.20311145571669526
iteration : 8127
train acc:  0.875
train loss:  0.29821547865867615
train gradient:  0.13368748703864036
iteration : 8128
train acc:  0.8671875
train loss:  0.3259827494621277
train gradient:  0.14271460517618306
iteration : 8129
train acc:  0.8359375
train loss:  0.38331541419029236
train gradient:  0.18887396707267867
iteration : 8130
train acc:  0.8359375
train loss:  0.30094772577285767
train gradient:  0.13959304780154064
iteration : 8131
train acc:  0.828125
train loss:  0.4279398024082184
train gradient:  0.22057208797994116
iteration : 8132
train acc:  0.9140625
train loss:  0.22475029528141022
train gradient:  0.09829713563429705
iteration : 8133
train acc:  0.828125
train loss:  0.3293645977973938
train gradient:  0.19680506807306464
iteration : 8134
train acc:  0.890625
train loss:  0.2902930676937103
train gradient:  0.19312004820415235
iteration : 8135
train acc:  0.828125
train loss:  0.3582058846950531
train gradient:  0.2030209870053636
iteration : 8136
train acc:  0.875
train loss:  0.3175611197948456
train gradient:  0.19832585447417203
iteration : 8137
train acc:  0.828125
train loss:  0.3983524739742279
train gradient:  0.24962374282512245
iteration : 8138
train acc:  0.8046875
train loss:  0.42188799381256104
train gradient:  0.24467119274131643
iteration : 8139
train acc:  0.859375
train loss:  0.3364296555519104
train gradient:  0.21257118256555724
iteration : 8140
train acc:  0.8828125
train loss:  0.3202654719352722
train gradient:  0.1475553890320534
iteration : 8141
train acc:  0.875
train loss:  0.24019809067249298
train gradient:  0.09908265834563888
iteration : 8142
train acc:  0.84375
train loss:  0.3770180642604828
train gradient:  0.20544213798454114
iteration : 8143
train acc:  0.859375
train loss:  0.3103504776954651
train gradient:  0.27139743313242454
iteration : 8144
train acc:  0.8359375
train loss:  0.2969558835029602
train gradient:  0.15293321491946732
iteration : 8145
train acc:  0.8203125
train loss:  0.3751561939716339
train gradient:  0.16037450601815886
iteration : 8146
train acc:  0.8828125
train loss:  0.2905239462852478
train gradient:  0.12443558322935869
iteration : 8147
train acc:  0.921875
train loss:  0.22966702282428741
train gradient:  0.10334507516223757
iteration : 8148
train acc:  0.8515625
train loss:  0.38464510440826416
train gradient:  0.23947264210305905
iteration : 8149
train acc:  0.8046875
train loss:  0.3494325876235962
train gradient:  0.18007629332552552
iteration : 8150
train acc:  0.84375
train loss:  0.38563573360443115
train gradient:  0.1713543014331934
iteration : 8151
train acc:  0.875
train loss:  0.33749115467071533
train gradient:  0.20132193080910935
iteration : 8152
train acc:  0.84375
train loss:  0.3206937909126282
train gradient:  0.1107559977279928
iteration : 8153
train acc:  0.8671875
train loss:  0.301095575094223
train gradient:  0.14963090800313517
iteration : 8154
train acc:  0.8046875
train loss:  0.4064670205116272
train gradient:  0.29509503998578474
iteration : 8155
train acc:  0.828125
train loss:  0.39717674255371094
train gradient:  0.1815073233737795
iteration : 8156
train acc:  0.8984375
train loss:  0.2755541503429413
train gradient:  0.17014389412653547
iteration : 8157
train acc:  0.8125
train loss:  0.400603711605072
train gradient:  0.22286962659292633
iteration : 8158
train acc:  0.8671875
train loss:  0.4062914252281189
train gradient:  0.22536317881681941
iteration : 8159
train acc:  0.765625
train loss:  0.35876569151878357
train gradient:  0.15471604549430437
iteration : 8160
train acc:  0.84375
train loss:  0.3950232267379761
train gradient:  0.2454752338340475
iteration : 8161
train acc:  0.8828125
train loss:  0.3094434142112732
train gradient:  0.14099030980203403
iteration : 8162
train acc:  0.875
train loss:  0.33757999539375305
train gradient:  0.1377583794319701
iteration : 8163
train acc:  0.8515625
train loss:  0.34066081047058105
train gradient:  0.20145753461403904
iteration : 8164
train acc:  0.8671875
train loss:  0.33329278230667114
train gradient:  0.1812753487669476
iteration : 8165
train acc:  0.8359375
train loss:  0.3495754301548004
train gradient:  0.22366434973810612
iteration : 8166
train acc:  0.90625
train loss:  0.25192394852638245
train gradient:  0.19372944306767792
iteration : 8167
train acc:  0.8515625
train loss:  0.34943151473999023
train gradient:  0.21339590735784916
iteration : 8168
train acc:  0.890625
train loss:  0.3072317838668823
train gradient:  0.12869040495914585
iteration : 8169
train acc:  0.8828125
train loss:  0.27187126874923706
train gradient:  0.1406091446939854
iteration : 8170
train acc:  0.84375
train loss:  0.3582041263580322
train gradient:  0.18335739723672306
iteration : 8171
train acc:  0.8671875
train loss:  0.3258255422115326
train gradient:  0.1384754388869802
iteration : 8172
train acc:  0.84375
train loss:  0.34134429693222046
train gradient:  0.1960953943564428
iteration : 8173
train acc:  0.8671875
train loss:  0.3182007074356079
train gradient:  0.14524058391377198
iteration : 8174
train acc:  0.84375
train loss:  0.36246949434280396
train gradient:  0.17839348670291474
iteration : 8175
train acc:  0.8515625
train loss:  0.3418346643447876
train gradient:  0.16789977213823526
iteration : 8176
train acc:  0.8515625
train loss:  0.3348960280418396
train gradient:  0.20925122362714405
iteration : 8177
train acc:  0.84375
train loss:  0.3327314853668213
train gradient:  0.25491751956943465
iteration : 8178
train acc:  0.859375
train loss:  0.296596497297287
train gradient:  0.13013579197425196
iteration : 8179
train acc:  0.8671875
train loss:  0.3457305133342743
train gradient:  0.2071608718255064
iteration : 8180
train acc:  0.8046875
train loss:  0.41895514726638794
train gradient:  0.2769430891297516
iteration : 8181
train acc:  0.8515625
train loss:  0.33838218450546265
train gradient:  0.16282098546836954
iteration : 8182
train acc:  0.7890625
train loss:  0.40917837619781494
train gradient:  0.2696770265602416
iteration : 8183
train acc:  0.7890625
train loss:  0.40000441670417786
train gradient:  0.1443840166462676
iteration : 8184
train acc:  0.8828125
train loss:  0.30285191535949707
train gradient:  0.18899112636547438
iteration : 8185
train acc:  0.84375
train loss:  0.35877227783203125
train gradient:  0.2346435331028121
iteration : 8186
train acc:  0.84375
train loss:  0.2873391807079315
train gradient:  0.11582321472323029
iteration : 8187
train acc:  0.859375
train loss:  0.3276328444480896
train gradient:  0.16889490855577285
iteration : 8188
train acc:  0.859375
train loss:  0.37221503257751465
train gradient:  0.2539621379370076
iteration : 8189
train acc:  0.8671875
train loss:  0.35139602422714233
train gradient:  0.15296340211444265
iteration : 8190
train acc:  0.859375
train loss:  0.32022029161453247
train gradient:  0.2392264141237647
iteration : 8191
train acc:  0.859375
train loss:  0.3030073642730713
train gradient:  0.1378638666556049
iteration : 8192
train acc:  0.890625
train loss:  0.2513916492462158
train gradient:  0.1616374602455175
iteration : 8193
train acc:  0.90625
train loss:  0.29936274886131287
train gradient:  0.11485529322557302
iteration : 8194
train acc:  0.8671875
train loss:  0.3749198913574219
train gradient:  0.1667369236659618
iteration : 8195
train acc:  0.84375
train loss:  0.35810387134552
train gradient:  0.24541192188953356
iteration : 8196
train acc:  0.8203125
train loss:  0.32538414001464844
train gradient:  0.1657237969479078
iteration : 8197
train acc:  0.8828125
train loss:  0.30891239643096924
train gradient:  0.14284264023908153
iteration : 8198
train acc:  0.8359375
train loss:  0.31237393617630005
train gradient:  0.141417331146988
iteration : 8199
train acc:  0.859375
train loss:  0.392156720161438
train gradient:  0.18001168103404253
iteration : 8200
train acc:  0.796875
train loss:  0.41003596782684326
train gradient:  0.179536568158496
iteration : 8201
train acc:  0.8515625
train loss:  0.3689229488372803
train gradient:  0.2093771610518928
iteration : 8202
train acc:  0.8515625
train loss:  0.38724902272224426
train gradient:  0.20533324439818584
iteration : 8203
train acc:  0.8203125
train loss:  0.38739633560180664
train gradient:  0.20036851175446307
iteration : 8204
train acc:  0.84375
train loss:  0.34768593311309814
train gradient:  0.17250250454857388
iteration : 8205
train acc:  0.8203125
train loss:  0.3571128249168396
train gradient:  0.14998613638039932
iteration : 8206
train acc:  0.7734375
train loss:  0.4338220953941345
train gradient:  0.7036131141541049
iteration : 8207
train acc:  0.859375
train loss:  0.4069565236568451
train gradient:  0.2188940674949575
iteration : 8208
train acc:  0.8671875
train loss:  0.2952819764614105
train gradient:  0.1596287534897175
iteration : 8209
train acc:  0.8984375
train loss:  0.24947881698608398
train gradient:  0.12307290610602312
iteration : 8210
train acc:  0.828125
train loss:  0.3527336120605469
train gradient:  0.19804598202998613
iteration : 8211
train acc:  0.8515625
train loss:  0.2955799400806427
train gradient:  0.24722266786016406
iteration : 8212
train acc:  0.828125
train loss:  0.35888877511024475
train gradient:  0.23266408881646122
iteration : 8213
train acc:  0.8984375
train loss:  0.2789683938026428
train gradient:  0.11892407946812016
iteration : 8214
train acc:  0.859375
train loss:  0.3335571587085724
train gradient:  0.15073768208364496
iteration : 8215
train acc:  0.9140625
train loss:  0.2300613820552826
train gradient:  0.13621712724570878
iteration : 8216
train acc:  0.8671875
train loss:  0.3209661543369293
train gradient:  0.21271194687968314
iteration : 8217
train acc:  0.90625
train loss:  0.2836097478866577
train gradient:  0.17422620577072834
iteration : 8218
train acc:  0.8203125
train loss:  0.3985709846019745
train gradient:  0.27571865554743147
iteration : 8219
train acc:  0.8828125
train loss:  0.291991263628006
train gradient:  0.20285908426544227
iteration : 8220
train acc:  0.890625
train loss:  0.2767179012298584
train gradient:  0.12518798194342856
iteration : 8221
train acc:  0.859375
train loss:  0.312794953584671
train gradient:  0.2059046209440585
iteration : 8222
train acc:  0.8828125
train loss:  0.35128462314605713
train gradient:  0.13729149080045383
iteration : 8223
train acc:  0.8203125
train loss:  0.4868085980415344
train gradient:  0.3930567704897362
iteration : 8224
train acc:  0.8984375
train loss:  0.2945888638496399
train gradient:  0.20616328780930804
iteration : 8225
train acc:  0.8671875
train loss:  0.28804677724838257
train gradient:  0.10578075688584739
iteration : 8226
train acc:  0.859375
train loss:  0.3310547471046448
train gradient:  0.13087398669051892
iteration : 8227
train acc:  0.8828125
train loss:  0.32851743698120117
train gradient:  0.1568334344778022
iteration : 8228
train acc:  0.8515625
train loss:  0.3561980724334717
train gradient:  0.3251207831719933
iteration : 8229
train acc:  0.8203125
train loss:  0.3332427144050598
train gradient:  0.20318588482158273
iteration : 8230
train acc:  0.875
train loss:  0.33268511295318604
train gradient:  0.17505848338242497
iteration : 8231
train acc:  0.828125
train loss:  0.36844003200531006
train gradient:  0.2047539966145971
iteration : 8232
train acc:  0.8125
train loss:  0.39515766501426697
train gradient:  0.21723949153024721
iteration : 8233
train acc:  0.84375
train loss:  0.30664128065109253
train gradient:  0.17602120343680033
iteration : 8234
train acc:  0.9140625
train loss:  0.23975181579589844
train gradient:  0.13362186138863846
iteration : 8235
train acc:  0.859375
train loss:  0.3578944802284241
train gradient:  0.19548009047291898
iteration : 8236
train acc:  0.8828125
train loss:  0.2824546694755554
train gradient:  0.15086517388624304
iteration : 8237
train acc:  0.8671875
train loss:  0.35155344009399414
train gradient:  0.24940538773041412
iteration : 8238
train acc:  0.8203125
train loss:  0.37854164838790894
train gradient:  0.2315710486118821
iteration : 8239
train acc:  0.875
train loss:  0.3056378960609436
train gradient:  0.1556606907923011
iteration : 8240
train acc:  0.8984375
train loss:  0.24586595594882965
train gradient:  0.09793023848668168
iteration : 8241
train acc:  0.8046875
train loss:  0.3820415735244751
train gradient:  0.2868304593060033
iteration : 8242
train acc:  0.8359375
train loss:  0.3525562286376953
train gradient:  0.2674665873134561
iteration : 8243
train acc:  0.8125
train loss:  0.33414149284362793
train gradient:  0.14411701709342997
iteration : 8244
train acc:  0.8359375
train loss:  0.33270764350891113
train gradient:  0.2577982030578905
iteration : 8245
train acc:  0.84375
train loss:  0.3549939692020416
train gradient:  0.14432610055657077
iteration : 8246
train acc:  0.8828125
train loss:  0.3116534948348999
train gradient:  0.1468167725411024
iteration : 8247
train acc:  0.859375
train loss:  0.3317086398601532
train gradient:  0.20332820390963685
iteration : 8248
train acc:  0.875
train loss:  0.27067601680755615
train gradient:  0.14080653495176954
iteration : 8249
train acc:  0.859375
train loss:  0.309379518032074
train gradient:  0.18320282211296932
iteration : 8250
train acc:  0.84375
train loss:  0.37861505150794983
train gradient:  0.22283910012754168
iteration : 8251
train acc:  0.8125
train loss:  0.36438530683517456
train gradient:  0.2009687512501258
iteration : 8252
train acc:  0.890625
train loss:  0.27170330286026
train gradient:  0.17420498470479107
iteration : 8253
train acc:  0.8359375
train loss:  0.2966753840446472
train gradient:  0.108086631733617
iteration : 8254
train acc:  0.8359375
train loss:  0.3382338285446167
train gradient:  0.16645857281442128
iteration : 8255
train acc:  0.875
train loss:  0.30283063650131226
train gradient:  0.17705836428783228
iteration : 8256
train acc:  0.8203125
train loss:  0.40492919087409973
train gradient:  0.23655004983509714
iteration : 8257
train acc:  0.84375
train loss:  0.3421822786331177
train gradient:  0.2857929060491493
iteration : 8258
train acc:  0.8671875
train loss:  0.33480238914489746
train gradient:  0.1724549632478663
iteration : 8259
train acc:  0.84375
train loss:  0.35562944412231445
train gradient:  0.2191629571024576
iteration : 8260
train acc:  0.875
train loss:  0.34902292490005493
train gradient:  0.18666361186030928
iteration : 8261
train acc:  0.8828125
train loss:  0.2735626995563507
train gradient:  0.15029580111811663
iteration : 8262
train acc:  0.859375
train loss:  0.31394630670547485
train gradient:  0.14052729767722602
iteration : 8263
train acc:  0.84375
train loss:  0.3292730450630188
train gradient:  0.1943111164334492
iteration : 8264
train acc:  0.875
train loss:  0.3346218466758728
train gradient:  0.22087955976360002
iteration : 8265
train acc:  0.8828125
train loss:  0.26350224018096924
train gradient:  0.11117884542482918
iteration : 8266
train acc:  0.8359375
train loss:  0.3709069490432739
train gradient:  0.18217535164070986
iteration : 8267
train acc:  0.875
train loss:  0.3404143154621124
train gradient:  0.23304273039448437
iteration : 8268
train acc:  0.859375
train loss:  0.3539276123046875
train gradient:  0.2060192694682509
iteration : 8269
train acc:  0.84375
train loss:  0.36531177163124084
train gradient:  0.2139270000073057
iteration : 8270
train acc:  0.8203125
train loss:  0.3634922504425049
train gradient:  0.21841932485521132
iteration : 8271
train acc:  0.84375
train loss:  0.3076952397823334
train gradient:  0.18657274849435138
iteration : 8272
train acc:  0.8203125
train loss:  0.3541112244129181
train gradient:  0.15483338750180559
iteration : 8273
train acc:  0.9140625
train loss:  0.25891390442848206
train gradient:  0.11827381570290361
iteration : 8274
train acc:  0.8984375
train loss:  0.2873438596725464
train gradient:  0.16278186216993853
iteration : 8275
train acc:  0.8828125
train loss:  0.26049402356147766
train gradient:  0.13837174850473766
iteration : 8276
train acc:  0.875
train loss:  0.30874669551849365
train gradient:  0.18136365628051243
iteration : 8277
train acc:  0.8359375
train loss:  0.32856351137161255
train gradient:  0.16479949407609887
iteration : 8278
train acc:  0.8828125
train loss:  0.3184680938720703
train gradient:  0.2722911634659961
iteration : 8279
train acc:  0.890625
train loss:  0.2842539846897125
train gradient:  0.1438144879525688
iteration : 8280
train acc:  0.8359375
train loss:  0.4323747158050537
train gradient:  0.3010374503135142
iteration : 8281
train acc:  0.859375
train loss:  0.36161574721336365
train gradient:  0.16919225936146284
iteration : 8282
train acc:  0.875
train loss:  0.3002573251724243
train gradient:  0.12815449785764066
iteration : 8283
train acc:  0.890625
train loss:  0.2633286714553833
train gradient:  0.14356988689811953
iteration : 8284
train acc:  0.9140625
train loss:  0.24328389763832092
train gradient:  0.10604299891393082
iteration : 8285
train acc:  0.921875
train loss:  0.27180320024490356
train gradient:  0.13861527584724692
iteration : 8286
train acc:  0.8046875
train loss:  0.38108840584754944
train gradient:  0.213179292744077
iteration : 8287
train acc:  0.890625
train loss:  0.2652038335800171
train gradient:  0.14367913044648584
iteration : 8288
train acc:  0.890625
train loss:  0.2489621639251709
train gradient:  0.1218679041507578
iteration : 8289
train acc:  0.8515625
train loss:  0.33100438117980957
train gradient:  0.1871544673495984
iteration : 8290
train acc:  0.8828125
train loss:  0.30025458335876465
train gradient:  0.14014335683947476
iteration : 8291
train acc:  0.84375
train loss:  0.3621688187122345
train gradient:  0.21382464389666808
iteration : 8292
train acc:  0.8515625
train loss:  0.29678094387054443
train gradient:  0.15880608499936738
iteration : 8293
train acc:  0.828125
train loss:  0.38894033432006836
train gradient:  0.29609159419460074
iteration : 8294
train acc:  0.875
train loss:  0.2828609347343445
train gradient:  0.11461821417081881
iteration : 8295
train acc:  0.8515625
train loss:  0.29865163564682007
train gradient:  0.15874178220363144
iteration : 8296
train acc:  0.8359375
train loss:  0.3154078722000122
train gradient:  0.13573325094622873
iteration : 8297
train acc:  0.8828125
train loss:  0.28754812479019165
train gradient:  0.15433324724874464
iteration : 8298
train acc:  0.8671875
train loss:  0.31017225980758667
train gradient:  0.27762786615819074
iteration : 8299
train acc:  0.890625
train loss:  0.3066975474357605
train gradient:  0.13547736720537384
iteration : 8300
train acc:  0.7890625
train loss:  0.5039430260658264
train gradient:  0.3604130392892053
iteration : 8301
train acc:  0.8125
train loss:  0.4079967141151428
train gradient:  0.24957366396497155
iteration : 8302
train acc:  0.8515625
train loss:  0.34531649947166443
train gradient:  0.44839436413584594
iteration : 8303
train acc:  0.8515625
train loss:  0.33332616090774536
train gradient:  0.19372252856634029
iteration : 8304
train acc:  0.890625
train loss:  0.27517610788345337
train gradient:  0.12616445399480158
iteration : 8305
train acc:  0.8046875
train loss:  0.3605913519859314
train gradient:  0.26088446560056683
iteration : 8306
train acc:  0.890625
train loss:  0.2640065550804138
train gradient:  0.14360676654083482
iteration : 8307
train acc:  0.828125
train loss:  0.38193628191947937
train gradient:  0.20024003712394667
iteration : 8308
train acc:  0.8515625
train loss:  0.35307013988494873
train gradient:  0.19936547572893082
iteration : 8309
train acc:  0.890625
train loss:  0.3069393038749695
train gradient:  0.15998854682497832
iteration : 8310
train acc:  0.84375
train loss:  0.35657259821891785
train gradient:  0.24192321967677866
iteration : 8311
train acc:  0.828125
train loss:  0.46016907691955566
train gradient:  0.36120160874998886
iteration : 8312
train acc:  0.890625
train loss:  0.2280472069978714
train gradient:  0.12244941149063478
iteration : 8313
train acc:  0.8828125
train loss:  0.25231489539146423
train gradient:  0.13294856637084418
iteration : 8314
train acc:  0.8984375
train loss:  0.3085196912288666
train gradient:  0.13538518095669394
iteration : 8315
train acc:  0.8125
train loss:  0.38513490557670593
train gradient:  0.1837633741430678
iteration : 8316
train acc:  0.8359375
train loss:  0.313859224319458
train gradient:  0.15999724287692843
iteration : 8317
train acc:  0.859375
train loss:  0.33017539978027344
train gradient:  0.25697582089841586
iteration : 8318
train acc:  0.84375
train loss:  0.3718077838420868
train gradient:  0.17841167186214157
iteration : 8319
train acc:  0.828125
train loss:  0.4324303865432739
train gradient:  0.3684845304980577
iteration : 8320
train acc:  0.8515625
train loss:  0.38441625237464905
train gradient:  0.24633704886801366
iteration : 8321
train acc:  0.8984375
train loss:  0.3119856119155884
train gradient:  0.1594320770674376
iteration : 8322
train acc:  0.84375
train loss:  0.32933396100997925
train gradient:  0.18067782316356865
iteration : 8323
train acc:  0.859375
train loss:  0.34968629479408264
train gradient:  0.25627121991582597
iteration : 8324
train acc:  0.875
train loss:  0.26043999195098877
train gradient:  0.14670202718471007
iteration : 8325
train acc:  0.8515625
train loss:  0.2958574891090393
train gradient:  0.15034217281062817
iteration : 8326
train acc:  0.8515625
train loss:  0.34691524505615234
train gradient:  0.20747180254923986
iteration : 8327
train acc:  0.8828125
train loss:  0.31541892886161804
train gradient:  0.1755480354585082
iteration : 8328
train acc:  0.8515625
train loss:  0.382377952337265
train gradient:  0.2075434539698371
iteration : 8329
train acc:  0.8828125
train loss:  0.3140018582344055
train gradient:  0.16603289067553378
iteration : 8330
train acc:  0.8515625
train loss:  0.34202882647514343
train gradient:  0.21689872872731894
iteration : 8331
train acc:  0.8671875
train loss:  0.29885613918304443
train gradient:  0.2453426492635322
iteration : 8332
train acc:  0.8359375
train loss:  0.3464556336402893
train gradient:  0.1611418951162447
iteration : 8333
train acc:  0.8515625
train loss:  0.3108134865760803
train gradient:  0.2403085555673966
iteration : 8334
train acc:  0.7890625
train loss:  0.3935800790786743
train gradient:  0.20922034063591877
iteration : 8335
train acc:  0.8359375
train loss:  0.3721115291118622
train gradient:  0.2177757382704369
iteration : 8336
train acc:  0.8828125
train loss:  0.31385326385498047
train gradient:  0.27238820554687376
iteration : 8337
train acc:  0.828125
train loss:  0.3357405662536621
train gradient:  0.22242490084475788
iteration : 8338
train acc:  0.8203125
train loss:  0.379296213388443
train gradient:  0.29196640805202384
iteration : 8339
train acc:  0.828125
train loss:  0.41117119789123535
train gradient:  0.2661349034089967
iteration : 8340
train acc:  0.9140625
train loss:  0.22808679938316345
train gradient:  0.12513995605691158
iteration : 8341
train acc:  0.8515625
train loss:  0.3068390190601349
train gradient:  0.15744030653123567
iteration : 8342
train acc:  0.875
train loss:  0.26171016693115234
train gradient:  0.1483809738391509
iteration : 8343
train acc:  0.8515625
train loss:  0.29260414838790894
train gradient:  0.15113493906523212
iteration : 8344
train acc:  0.859375
train loss:  0.3169373869895935
train gradient:  0.1951099530067057
iteration : 8345
train acc:  0.8515625
train loss:  0.30029428005218506
train gradient:  0.17631960101550903
iteration : 8346
train acc:  0.8984375
train loss:  0.2888128161430359
train gradient:  0.19521454880606365
iteration : 8347
train acc:  0.859375
train loss:  0.3755720257759094
train gradient:  0.18802178883764503
iteration : 8348
train acc:  0.84375
train loss:  0.33066684007644653
train gradient:  0.22309769467316765
iteration : 8349
train acc:  0.8515625
train loss:  0.3180726170539856
train gradient:  0.1875900646453924
iteration : 8350
train acc:  0.8515625
train loss:  0.42828941345214844
train gradient:  0.2989318040941754
iteration : 8351
train acc:  0.8984375
train loss:  0.2494250386953354
train gradient:  0.11748445578220378
iteration : 8352
train acc:  0.859375
train loss:  0.385417640209198
train gradient:  0.2100845897469361
iteration : 8353
train acc:  0.9296875
train loss:  0.28619152307510376
train gradient:  0.10930705721119816
iteration : 8354
train acc:  0.84375
train loss:  0.33673638105392456
train gradient:  0.16278581279859802
iteration : 8355
train acc:  0.828125
train loss:  0.3582953214645386
train gradient:  0.2539308713159205
iteration : 8356
train acc:  0.8046875
train loss:  0.3962390422821045
train gradient:  0.1963485622415286
iteration : 8357
train acc:  0.8359375
train loss:  0.3321775496006012
train gradient:  0.21987768398070517
iteration : 8358
train acc:  0.84375
train loss:  0.36521977186203003
train gradient:  0.21563344911377258
iteration : 8359
train acc:  0.859375
train loss:  0.3023539185523987
train gradient:  0.21430559327567725
iteration : 8360
train acc:  0.890625
train loss:  0.2849067449569702
train gradient:  0.25466913457719637
iteration : 8361
train acc:  0.8515625
train loss:  0.3252294659614563
train gradient:  0.20288408344004608
iteration : 8362
train acc:  0.8515625
train loss:  0.3344828188419342
train gradient:  0.2023537486968174
iteration : 8363
train acc:  0.8359375
train loss:  0.34316664934158325
train gradient:  0.1405981037979138
iteration : 8364
train acc:  0.84375
train loss:  0.3329230546951294
train gradient:  0.16862570241378905
iteration : 8365
train acc:  0.8046875
train loss:  0.3812856078147888
train gradient:  0.2440294928760178
iteration : 8366
train acc:  0.8671875
train loss:  0.3780507445335388
train gradient:  0.23869194265944882
iteration : 8367
train acc:  0.9140625
train loss:  0.21243058145046234
train gradient:  0.11470929097907875
iteration : 8368
train acc:  0.875
train loss:  0.33243125677108765
train gradient:  0.22768431329843802
iteration : 8369
train acc:  0.8671875
train loss:  0.3732175827026367
train gradient:  0.19958806837745385
iteration : 8370
train acc:  0.8515625
train loss:  0.32027414441108704
train gradient:  0.22854290174027217
iteration : 8371
train acc:  0.8671875
train loss:  0.3533173203468323
train gradient:  0.21345872577525943
iteration : 8372
train acc:  0.8125
train loss:  0.43160614371299744
train gradient:  0.2517007938249591
iteration : 8373
train acc:  0.859375
train loss:  0.3068355917930603
train gradient:  0.18966633390063936
iteration : 8374
train acc:  0.8125
train loss:  0.3525869846343994
train gradient:  0.17550475873335336
iteration : 8375
train acc:  0.84375
train loss:  0.3047443628311157
train gradient:  0.2172715338922171
iteration : 8376
train acc:  0.8515625
train loss:  0.331315815448761
train gradient:  0.20688978857339826
iteration : 8377
train acc:  0.828125
train loss:  0.3588087260723114
train gradient:  0.19791337123295177
iteration : 8378
train acc:  0.8671875
train loss:  0.29201918840408325
train gradient:  0.1170445721703338
iteration : 8379
train acc:  0.8671875
train loss:  0.3028925657272339
train gradient:  0.1511846472488217
iteration : 8380
train acc:  0.8359375
train loss:  0.3301927447319031
train gradient:  0.1927239525749273
iteration : 8381
train acc:  0.828125
train loss:  0.3861319422721863
train gradient:  0.20790607327653576
iteration : 8382
train acc:  0.9140625
train loss:  0.2616535723209381
train gradient:  0.1459601591246773
iteration : 8383
train acc:  0.875
train loss:  0.3206545412540436
train gradient:  0.16744317277726797
iteration : 8384
train acc:  0.875
train loss:  0.3297616243362427
train gradient:  0.1447319829027019
iteration : 8385
train acc:  0.8515625
train loss:  0.3564247190952301
train gradient:  0.17298519761977105
iteration : 8386
train acc:  0.8359375
train loss:  0.3543253242969513
train gradient:  0.13759541139567225
iteration : 8387
train acc:  0.859375
train loss:  0.31887680292129517
train gradient:  0.20788575783098812
iteration : 8388
train acc:  0.8671875
train loss:  0.3551744520664215
train gradient:  0.1640451191507718
iteration : 8389
train acc:  0.828125
train loss:  0.3772951066493988
train gradient:  0.23144255432912597
iteration : 8390
train acc:  0.8125
train loss:  0.3673308789730072
train gradient:  0.19994087292478305
iteration : 8391
train acc:  0.765625
train loss:  0.41091251373291016
train gradient:  0.25559919789636576
iteration : 8392
train acc:  0.796875
train loss:  0.35513240098953247
train gradient:  0.16411326249423158
iteration : 8393
train acc:  0.8359375
train loss:  0.3743482828140259
train gradient:  0.3169086592756487
iteration : 8394
train acc:  0.8515625
train loss:  0.3759518265724182
train gradient:  0.18551254727700872
iteration : 8395
train acc:  0.859375
train loss:  0.2800613343715668
train gradient:  0.13591266954210346
iteration : 8396
train acc:  0.8671875
train loss:  0.29844745993614197
train gradient:  0.12444703241588666
iteration : 8397
train acc:  0.859375
train loss:  0.3284863233566284
train gradient:  0.1786396489615884
iteration : 8398
train acc:  0.875
train loss:  0.2799089550971985
train gradient:  0.15925254848222803
iteration : 8399
train acc:  0.8671875
train loss:  0.29559212923049927
train gradient:  0.1398939196592483
iteration : 8400
train acc:  0.84375
train loss:  0.36199751496315
train gradient:  0.16842560281907215
iteration : 8401
train acc:  0.78125
train loss:  0.3478611707687378
train gradient:  0.18223631560574827
iteration : 8402
train acc:  0.828125
train loss:  0.3743598461151123
train gradient:  0.20081555008794974
iteration : 8403
train acc:  0.828125
train loss:  0.3811171054840088
train gradient:  0.19004336245496598
iteration : 8404
train acc:  0.8515625
train loss:  0.31063467264175415
train gradient:  0.16673453849486192
iteration : 8405
train acc:  0.796875
train loss:  0.40229323506355286
train gradient:  0.23221649037476585
iteration : 8406
train acc:  0.8125
train loss:  0.38230782747268677
train gradient:  0.20084346431623792
iteration : 8407
train acc:  0.921875
train loss:  0.22121509909629822
train gradient:  0.11647362420452871
iteration : 8408
train acc:  0.8046875
train loss:  0.4358789324760437
train gradient:  0.3918027640644236
iteration : 8409
train acc:  0.8671875
train loss:  0.2697208523750305
train gradient:  0.19896032600830693
iteration : 8410
train acc:  0.875
train loss:  0.24983498454093933
train gradient:  0.14584474801941144
iteration : 8411
train acc:  0.8203125
train loss:  0.4132654070854187
train gradient:  0.2554085329658818
iteration : 8412
train acc:  0.8671875
train loss:  0.2854006588459015
train gradient:  0.1297042419296047
iteration : 8413
train acc:  0.7734375
train loss:  0.39290034770965576
train gradient:  0.23230065849301332
iteration : 8414
train acc:  0.8125
train loss:  0.4287474453449249
train gradient:  0.24956712928741104
iteration : 8415
train acc:  0.859375
train loss:  0.3153579831123352
train gradient:  0.13699418392456642
iteration : 8416
train acc:  0.84375
train loss:  0.33569273352622986
train gradient:  0.17002274164172157
iteration : 8417
train acc:  0.75
train loss:  0.4725828766822815
train gradient:  0.30445365023762233
iteration : 8418
train acc:  0.890625
train loss:  0.29475367069244385
train gradient:  0.12881105059838102
iteration : 8419
train acc:  0.8125
train loss:  0.38303524255752563
train gradient:  0.22121557343920006
iteration : 8420
train acc:  0.8515625
train loss:  0.3361097574234009
train gradient:  0.1763170824103818
iteration : 8421
train acc:  0.8046875
train loss:  0.4200482666492462
train gradient:  0.22306131802015
iteration : 8422
train acc:  0.78125
train loss:  0.40865975618362427
train gradient:  0.24784295658923436
iteration : 8423
train acc:  0.875
train loss:  0.28620782494544983
train gradient:  0.1610820062692736
iteration : 8424
train acc:  0.8671875
train loss:  0.30263254046440125
train gradient:  0.1294937461022779
iteration : 8425
train acc:  0.921875
train loss:  0.2856937050819397
train gradient:  0.11581073343793763
iteration : 8426
train acc:  0.8828125
train loss:  0.2795026898384094
train gradient:  0.12968482461846192
iteration : 8427
train acc:  0.84375
train loss:  0.35739248991012573
train gradient:  0.17486924277108473
iteration : 8428
train acc:  0.8671875
train loss:  0.3849090039730072
train gradient:  0.20507263548327687
iteration : 8429
train acc:  0.8984375
train loss:  0.2976365387439728
train gradient:  0.1105536802842614
iteration : 8430
train acc:  0.8359375
train loss:  0.3418949544429779
train gradient:  0.2352237786222321
iteration : 8431
train acc:  0.8828125
train loss:  0.31276074051856995
train gradient:  0.1742544639985493
iteration : 8432
train acc:  0.8984375
train loss:  0.2612665891647339
train gradient:  0.20315368665071626
iteration : 8433
train acc:  0.8359375
train loss:  0.32758161425590515
train gradient:  0.1420172777452769
iteration : 8434
train acc:  0.9140625
train loss:  0.2387235313653946
train gradient:  0.11663845014947025
iteration : 8435
train acc:  0.8359375
train loss:  0.3698155879974365
train gradient:  0.24802404215668278
iteration : 8436
train acc:  0.8125
train loss:  0.40036481618881226
train gradient:  0.2565401345474429
iteration : 8437
train acc:  0.8359375
train loss:  0.35425981879234314
train gradient:  0.18091190605145258
iteration : 8438
train acc:  0.84375
train loss:  0.32277458906173706
train gradient:  0.20185976092518276
iteration : 8439
train acc:  0.890625
train loss:  0.237566739320755
train gradient:  0.0994993672914347
iteration : 8440
train acc:  0.8125
train loss:  0.4378112554550171
train gradient:  0.22893273531037678
iteration : 8441
train acc:  0.8828125
train loss:  0.3035370707511902
train gradient:  0.12550797190901836
iteration : 8442
train acc:  0.890625
train loss:  0.27027302980422974
train gradient:  0.12917939429916347
iteration : 8443
train acc:  0.828125
train loss:  0.38373470306396484
train gradient:  0.1923299509955635
iteration : 8444
train acc:  0.8515625
train loss:  0.32624125480651855
train gradient:  0.14671701014323515
iteration : 8445
train acc:  0.8125
train loss:  0.34971675276756287
train gradient:  0.1624449408848636
iteration : 8446
train acc:  0.828125
train loss:  0.37461134791374207
train gradient:  0.2856598844955823
iteration : 8447
train acc:  0.8359375
train loss:  0.3234526813030243
train gradient:  0.15212016954208052
iteration : 8448
train acc:  0.90625
train loss:  0.3182264268398285
train gradient:  0.294476377725822
iteration : 8449
train acc:  0.7734375
train loss:  0.4153149724006653
train gradient:  0.3514049622043409
iteration : 8450
train acc:  0.84375
train loss:  0.3323172330856323
train gradient:  0.22309628271839238
iteration : 8451
train acc:  0.828125
train loss:  0.4011574387550354
train gradient:  0.3558311018900266
iteration : 8452
train acc:  0.8671875
train loss:  0.28800639510154724
train gradient:  0.13738407354024873
iteration : 8453
train acc:  0.8671875
train loss:  0.3172636926174164
train gradient:  0.15966696139837772
iteration : 8454
train acc:  0.890625
train loss:  0.2993031442165375
train gradient:  0.12769756228946783
iteration : 8455
train acc:  0.8671875
train loss:  0.2918912172317505
train gradient:  0.14856807780464074
iteration : 8456
train acc:  0.8671875
train loss:  0.316466748714447
train gradient:  0.3456271878290355
iteration : 8457
train acc:  0.8203125
train loss:  0.41159552335739136
train gradient:  0.2440637552677954
iteration : 8458
train acc:  0.8515625
train loss:  0.3507162630558014
train gradient:  0.22423783224455757
iteration : 8459
train acc:  0.875
train loss:  0.2684517502784729
train gradient:  0.12937917417286063
iteration : 8460
train acc:  0.8671875
train loss:  0.3300234079360962
train gradient:  0.2129698450784137
iteration : 8461
train acc:  0.875
train loss:  0.3350750207901001
train gradient:  0.12837548460297865
iteration : 8462
train acc:  0.8671875
train loss:  0.3413859009742737
train gradient:  0.22235189124165256
iteration : 8463
train acc:  0.8828125
train loss:  0.28455549478530884
train gradient:  0.12618205011553743
iteration : 8464
train acc:  0.8203125
train loss:  0.41655319929122925
train gradient:  0.2258429996817429
iteration : 8465
train acc:  0.859375
train loss:  0.3583233952522278
train gradient:  0.25639094119640726
iteration : 8466
train acc:  0.828125
train loss:  0.41614872217178345
train gradient:  0.30502705975673583
iteration : 8467
train acc:  0.8671875
train loss:  0.29411622881889343
train gradient:  0.2831169587669033
iteration : 8468
train acc:  0.84375
train loss:  0.3607456684112549
train gradient:  0.16180983171588453
iteration : 8469
train acc:  0.8359375
train loss:  0.32011350989341736
train gradient:  0.14793921467664845
iteration : 8470
train acc:  0.8125
train loss:  0.40517109632492065
train gradient:  0.21551377374739805
iteration : 8471
train acc:  0.8046875
train loss:  0.40580111742019653
train gradient:  0.25100128427852614
iteration : 8472
train acc:  0.8984375
train loss:  0.3097906708717346
train gradient:  0.20267824633889123
iteration : 8473
train acc:  0.8359375
train loss:  0.3795703053474426
train gradient:  0.20326235859272554
iteration : 8474
train acc:  0.875
train loss:  0.32433193922042847
train gradient:  0.17292938067824484
iteration : 8475
train acc:  0.8828125
train loss:  0.3197413682937622
train gradient:  0.1479801378697933
iteration : 8476
train acc:  0.8671875
train loss:  0.418965607881546
train gradient:  0.29724742392147757
iteration : 8477
train acc:  0.8125
train loss:  0.4479645788669586
train gradient:  0.3101829124500192
iteration : 8478
train acc:  0.765625
train loss:  0.46090227365493774
train gradient:  0.26744404252298426
iteration : 8479
train acc:  0.8984375
train loss:  0.28341296315193176
train gradient:  0.13878205318381634
iteration : 8480
train acc:  0.859375
train loss:  0.3088494539260864
train gradient:  0.17459380298083133
iteration : 8481
train acc:  0.8515625
train loss:  0.3860208988189697
train gradient:  0.1738042092987948
iteration : 8482
train acc:  0.8203125
train loss:  0.38394689559936523
train gradient:  0.1956440553617893
iteration : 8483
train acc:  0.859375
train loss:  0.3292488753795624
train gradient:  0.23640310754146526
iteration : 8484
train acc:  0.8359375
train loss:  0.3382325768470764
train gradient:  0.246010114338433
iteration : 8485
train acc:  0.8671875
train loss:  0.3136432468891144
train gradient:  0.10887840104197449
iteration : 8486
train acc:  0.90625
train loss:  0.26126089692115784
train gradient:  0.10245898361329872
iteration : 8487
train acc:  0.890625
train loss:  0.30991125106811523
train gradient:  0.18320187597272666
iteration : 8488
train acc:  0.84375
train loss:  0.33307942748069763
train gradient:  0.2221532263339438
iteration : 8489
train acc:  0.8515625
train loss:  0.3275242745876312
train gradient:  0.14035869338438817
iteration : 8490
train acc:  0.859375
train loss:  0.3590272068977356
train gradient:  0.20886226452377482
iteration : 8491
train acc:  0.8515625
train loss:  0.4014853239059448
train gradient:  0.2596571939735426
iteration : 8492
train acc:  0.7734375
train loss:  0.4458645284175873
train gradient:  0.3112150053367341
iteration : 8493
train acc:  0.84375
train loss:  0.35731297731399536
train gradient:  0.2173933869073352
iteration : 8494
train acc:  0.875
train loss:  0.3820513188838959
train gradient:  0.32055974424500594
iteration : 8495
train acc:  0.84375
train loss:  0.3665955662727356
train gradient:  0.14081719310260588
iteration : 8496
train acc:  0.8515625
train loss:  0.36140578985214233
train gradient:  0.15698481861830982
iteration : 8497
train acc:  0.796875
train loss:  0.45234593749046326
train gradient:  0.3297376214260101
iteration : 8498
train acc:  0.8359375
train loss:  0.3638790547847748
train gradient:  0.18255139589892486
iteration : 8499
train acc:  0.828125
train loss:  0.3248390555381775
train gradient:  0.1398761923020555
iteration : 8500
train acc:  0.84375
train loss:  0.3704841136932373
train gradient:  0.18024699521252405
iteration : 8501
train acc:  0.8984375
train loss:  0.2748088836669922
train gradient:  0.24028798114191344
iteration : 8502
train acc:  0.8671875
train loss:  0.3471022844314575
train gradient:  0.25374519995057165
iteration : 8503
train acc:  0.890625
train loss:  0.2889198660850525
train gradient:  0.13830532804779885
iteration : 8504
train acc:  0.890625
train loss:  0.2784988284111023
train gradient:  0.14541783879180037
iteration : 8505
train acc:  0.8515625
train loss:  0.36359381675720215
train gradient:  0.2289072606235261
iteration : 8506
train acc:  0.8203125
train loss:  0.40174704790115356
train gradient:  0.2524050551975254
iteration : 8507
train acc:  0.890625
train loss:  0.33147236704826355
train gradient:  0.15265792172585646
iteration : 8508
train acc:  0.8359375
train loss:  0.3319840431213379
train gradient:  0.1828389155674516
iteration : 8509
train acc:  0.875
train loss:  0.29929715394973755
train gradient:  0.12570241776779276
iteration : 8510
train acc:  0.8671875
train loss:  0.3263777196407318
train gradient:  0.17972496774472768
iteration : 8511
train acc:  0.9296875
train loss:  0.2546308934688568
train gradient:  0.13585940836638083
iteration : 8512
train acc:  0.890625
train loss:  0.25969064235687256
train gradient:  0.11194039999025795
iteration : 8513
train acc:  0.8671875
train loss:  0.32406342029571533
train gradient:  0.1510986697874409
iteration : 8514
train acc:  0.875
train loss:  0.29519784450531006
train gradient:  0.14330324695961802
iteration : 8515
train acc:  0.84375
train loss:  0.3812443017959595
train gradient:  0.24032854040290003
iteration : 8516
train acc:  0.859375
train loss:  0.2732577323913574
train gradient:  0.0956884697360579
iteration : 8517
train acc:  0.875
train loss:  0.3330537676811218
train gradient:  0.19459089293324458
iteration : 8518
train acc:  0.875
train loss:  0.3076730966567993
train gradient:  0.1395193311978686
iteration : 8519
train acc:  0.828125
train loss:  0.36363333463668823
train gradient:  0.19120195731169393
iteration : 8520
train acc:  0.875
train loss:  0.2807709276676178
train gradient:  0.12170990046806421
iteration : 8521
train acc:  0.84375
train loss:  0.37363165616989136
train gradient:  0.20728426490438634
iteration : 8522
train acc:  0.84375
train loss:  0.3725177049636841
train gradient:  0.2623448084497159
iteration : 8523
train acc:  0.859375
train loss:  0.32056981325149536
train gradient:  0.26201805375298065
iteration : 8524
train acc:  0.9140625
train loss:  0.2708183526992798
train gradient:  0.13522242110713223
iteration : 8525
train acc:  0.8046875
train loss:  0.4277459383010864
train gradient:  0.21062548712901435
iteration : 8526
train acc:  0.859375
train loss:  0.30411282181739807
train gradient:  0.2305018271512082
iteration : 8527
train acc:  0.8359375
train loss:  0.34337323904037476
train gradient:  0.28173714979446257
iteration : 8528
train acc:  0.875
train loss:  0.3022584319114685
train gradient:  0.11568952063266848
iteration : 8529
train acc:  0.859375
train loss:  0.3595098853111267
train gradient:  0.14670898729584772
iteration : 8530
train acc:  0.828125
train loss:  0.3747238516807556
train gradient:  0.2883165260291091
iteration : 8531
train acc:  0.8671875
train loss:  0.3022754192352295
train gradient:  0.14149231689547695
iteration : 8532
train acc:  0.8125
train loss:  0.4256424307823181
train gradient:  0.3016258395244012
iteration : 8533
train acc:  0.8203125
train loss:  0.33955544233322144
train gradient:  0.20785541372085123
iteration : 8534
train acc:  0.875
train loss:  0.2871401309967041
train gradient:  0.15322163161719954
iteration : 8535
train acc:  0.828125
train loss:  0.37954527139663696
train gradient:  0.2262070234064078
iteration : 8536
train acc:  0.8515625
train loss:  0.2971377968788147
train gradient:  0.1428492526850189
iteration : 8537
train acc:  0.8671875
train loss:  0.3356003761291504
train gradient:  0.1744103539710529
iteration : 8538
train acc:  0.890625
train loss:  0.29494690895080566
train gradient:  0.17666227027617598
iteration : 8539
train acc:  0.8359375
train loss:  0.3573678135871887
train gradient:  0.2095775246829979
iteration : 8540
train acc:  0.828125
train loss:  0.3479360044002533
train gradient:  0.2446290810779842
iteration : 8541
train acc:  0.875
train loss:  0.2576639950275421
train gradient:  0.12510989807639306
iteration : 8542
train acc:  0.8359375
train loss:  0.3446406126022339
train gradient:  0.33681125965106
iteration : 8543
train acc:  0.9140625
train loss:  0.27396905422210693
train gradient:  0.0933781123365157
iteration : 8544
train acc:  0.8515625
train loss:  0.3107646703720093
train gradient:  0.1902119065402839
iteration : 8545
train acc:  0.7890625
train loss:  0.35316067934036255
train gradient:  0.18364694161309636
iteration : 8546
train acc:  0.8828125
train loss:  0.37001487612724304
train gradient:  0.1987713369737216
iteration : 8547
train acc:  0.8671875
train loss:  0.25747573375701904
train gradient:  0.09655171574627734
iteration : 8548
train acc:  0.828125
train loss:  0.3220601975917816
train gradient:  0.19413512413457884
iteration : 8549
train acc:  0.84375
train loss:  0.3529493808746338
train gradient:  0.3127534249383365
iteration : 8550
train acc:  0.8515625
train loss:  0.3207542300224304
train gradient:  0.14783834609816437
iteration : 8551
train acc:  0.8671875
train loss:  0.3021584749221802
train gradient:  0.14022309733005728
iteration : 8552
train acc:  0.8515625
train loss:  0.29148513078689575
train gradient:  0.13537456947379176
iteration : 8553
train acc:  0.8671875
train loss:  0.3041920065879822
train gradient:  0.1381311453828739
iteration : 8554
train acc:  0.875
train loss:  0.26780906319618225
train gradient:  0.11263844680792617
iteration : 8555
train acc:  0.8203125
train loss:  0.3744421601295471
train gradient:  0.22899023073138114
iteration : 8556
train acc:  0.8984375
train loss:  0.24386285245418549
train gradient:  0.15274081616474744
iteration : 8557
train acc:  0.890625
train loss:  0.29674533009529114
train gradient:  0.17248917732736635
iteration : 8558
train acc:  0.8671875
train loss:  0.3659760355949402
train gradient:  0.16433527306644122
iteration : 8559
train acc:  0.8671875
train loss:  0.3292641043663025
train gradient:  0.15003379130001493
iteration : 8560
train acc:  0.890625
train loss:  0.28913170099258423
train gradient:  0.15192061048434408
iteration : 8561
train acc:  0.8828125
train loss:  0.2517056465148926
train gradient:  0.13900323065264297
iteration : 8562
train acc:  0.8671875
train loss:  0.3404662311077118
train gradient:  0.19562420856740054
iteration : 8563
train acc:  0.859375
train loss:  0.3389761447906494
train gradient:  0.1550792417701528
iteration : 8564
train acc:  0.875
train loss:  0.26184600591659546
train gradient:  0.16624121431706057
iteration : 8565
train acc:  0.8359375
train loss:  0.3240690231323242
train gradient:  0.1542700728436861
iteration : 8566
train acc:  0.8203125
train loss:  0.419170081615448
train gradient:  0.3202848319192735
iteration : 8567
train acc:  0.8671875
train loss:  0.34650230407714844
train gradient:  0.14680670067850793
iteration : 8568
train acc:  0.859375
train loss:  0.3519030511379242
train gradient:  0.24209616603615142
iteration : 8569
train acc:  0.8359375
train loss:  0.34101855754852295
train gradient:  0.2066023880805558
iteration : 8570
train acc:  0.90625
train loss:  0.28616130352020264
train gradient:  0.1274305299789128
iteration : 8571
train acc:  0.890625
train loss:  0.3519316613674164
train gradient:  0.15498631605555352
iteration : 8572
train acc:  0.8984375
train loss:  0.309851735830307
train gradient:  0.1739720706626119
iteration : 8573
train acc:  0.828125
train loss:  0.34474554657936096
train gradient:  0.2301347172193507
iteration : 8574
train acc:  0.828125
train loss:  0.3630080223083496
train gradient:  0.17839224447525573
iteration : 8575
train acc:  0.828125
train loss:  0.3907797038555145
train gradient:  0.2470868330804264
iteration : 8576
train acc:  0.8359375
train loss:  0.39357054233551025
train gradient:  0.30653858552811325
iteration : 8577
train acc:  0.8671875
train loss:  0.27518612146377563
train gradient:  0.13251486032245485
iteration : 8578
train acc:  0.8671875
train loss:  0.31499946117401123
train gradient:  0.18219048802734888
iteration : 8579
train acc:  0.84375
train loss:  0.3605087697505951
train gradient:  0.16813379782913584
iteration : 8580
train acc:  0.8046875
train loss:  0.35740339756011963
train gradient:  0.18520758113549277
iteration : 8581
train acc:  0.890625
train loss:  0.2965205907821655
train gradient:  0.17426359040714384
iteration : 8582
train acc:  0.7890625
train loss:  0.35161665081977844
train gradient:  0.1385963650265125
iteration : 8583
train acc:  0.875
train loss:  0.35543522238731384
train gradient:  0.24738244344996552
iteration : 8584
train acc:  0.8828125
train loss:  0.28064823150634766
train gradient:  0.13517062983847367
iteration : 8585
train acc:  0.8515625
train loss:  0.3549603819847107
train gradient:  0.1900958229776906
iteration : 8586
train acc:  0.8359375
train loss:  0.37150007486343384
train gradient:  0.28361600158913136
iteration : 8587
train acc:  0.8515625
train loss:  0.29832133650779724
train gradient:  0.13879802408924696
iteration : 8588
train acc:  0.9140625
train loss:  0.272077739238739
train gradient:  0.15283150711998894
iteration : 8589
train acc:  0.8359375
train loss:  0.35291731357574463
train gradient:  0.17262215014394036
iteration : 8590
train acc:  0.875
train loss:  0.31256169080734253
train gradient:  0.12435864272549625
iteration : 8591
train acc:  0.859375
train loss:  0.39415642619132996
train gradient:  0.3525523123980766
iteration : 8592
train acc:  0.828125
train loss:  0.35230469703674316
train gradient:  0.1667416883227496
iteration : 8593
train acc:  0.8828125
train loss:  0.325972318649292
train gradient:  0.20642716374287076
iteration : 8594
train acc:  0.8984375
train loss:  0.31875288486480713
train gradient:  0.2000219318368046
iteration : 8595
train acc:  0.875
train loss:  0.25185316801071167
train gradient:  0.11252190106288538
iteration : 8596
train acc:  0.7890625
train loss:  0.4188997745513916
train gradient:  0.23056213710969464
iteration : 8597
train acc:  0.8671875
train loss:  0.3327022194862366
train gradient:  0.1821670360056924
iteration : 8598
train acc:  0.8359375
train loss:  0.38112112879753113
train gradient:  0.19229961823349756
iteration : 8599
train acc:  0.890625
train loss:  0.33369654417037964
train gradient:  0.20438148497708256
iteration : 8600
train acc:  0.859375
train loss:  0.3470951020717621
train gradient:  0.19106220233882107
iteration : 8601
train acc:  0.9140625
train loss:  0.27081984281539917
train gradient:  0.10084779411104884
iteration : 8602
train acc:  0.859375
train loss:  0.32099372148513794
train gradient:  0.20600077854748544
iteration : 8603
train acc:  0.875
train loss:  0.3565709888935089
train gradient:  0.21264886279553608
iteration : 8604
train acc:  0.8828125
train loss:  0.30052122473716736
train gradient:  0.20115949527897498
iteration : 8605
train acc:  0.8359375
train loss:  0.4167928397655487
train gradient:  0.24431611078175047
iteration : 8606
train acc:  0.78125
train loss:  0.48211318254470825
train gradient:  0.2871392782484646
iteration : 8607
train acc:  0.8828125
train loss:  0.2895810604095459
train gradient:  0.19746541466027553
iteration : 8608
train acc:  0.8671875
train loss:  0.2952657639980316
train gradient:  0.13798190680287578
iteration : 8609
train acc:  0.8828125
train loss:  0.27657249569892883
train gradient:  0.14274714815436423
iteration : 8610
train acc:  0.890625
train loss:  0.2727093994617462
train gradient:  0.14750099378824008
iteration : 8611
train acc:  0.875
train loss:  0.3078739047050476
train gradient:  0.14606425784159957
iteration : 8612
train acc:  0.8671875
train loss:  0.31856536865234375
train gradient:  0.16098410527893908
iteration : 8613
train acc:  0.8828125
train loss:  0.315297394990921
train gradient:  0.1858208236591659
iteration : 8614
train acc:  0.890625
train loss:  0.3072282075881958
train gradient:  0.11146736290653887
iteration : 8615
train acc:  0.875
train loss:  0.32082223892211914
train gradient:  0.17081065276888047
iteration : 8616
train acc:  0.9140625
train loss:  0.25868794322013855
train gradient:  0.11804254863708998
iteration : 8617
train acc:  0.875
train loss:  0.29373255372047424
train gradient:  0.18448083751599773
iteration : 8618
train acc:  0.8671875
train loss:  0.2935410439968109
train gradient:  0.17266318270695089
iteration : 8619
train acc:  0.8515625
train loss:  0.39283043146133423
train gradient:  0.17505737580570815
iteration : 8620
train acc:  0.859375
train loss:  0.32950928807258606
train gradient:  0.26448931888574156
iteration : 8621
train acc:  0.921875
train loss:  0.2322569489479065
train gradient:  0.10249752304641538
iteration : 8622
train acc:  0.84375
train loss:  0.33797264099121094
train gradient:  0.17227511737349482
iteration : 8623
train acc:  0.8203125
train loss:  0.3721781373023987
train gradient:  0.1854736626717121
iteration : 8624
train acc:  0.8828125
train loss:  0.255351722240448
train gradient:  0.19852998481913448
iteration : 8625
train acc:  0.890625
train loss:  0.28319162130355835
train gradient:  0.1713159044670654
iteration : 8626
train acc:  0.8671875
train loss:  0.30042266845703125
train gradient:  0.18460391187171246
iteration : 8627
train acc:  0.828125
train loss:  0.37663212418556213
train gradient:  0.2096802286966566
iteration : 8628
train acc:  0.90625
train loss:  0.2844635844230652
train gradient:  0.14336650105309862
iteration : 8629
train acc:  0.8828125
train loss:  0.2866528630256653
train gradient:  0.13496776905643218
iteration : 8630
train acc:  0.84375
train loss:  0.29410600662231445
train gradient:  0.14343096437185587
iteration : 8631
train acc:  0.90625
train loss:  0.2719462215900421
train gradient:  0.1711104801363817
iteration : 8632
train acc:  0.8828125
train loss:  0.2917969822883606
train gradient:  0.12468591064990162
iteration : 8633
train acc:  0.8671875
train loss:  0.30592775344848633
train gradient:  0.16350671778774878
iteration : 8634
train acc:  0.875
train loss:  0.32465407252311707
train gradient:  0.18720519815780262
iteration : 8635
train acc:  0.84375
train loss:  0.3079993426799774
train gradient:  0.14930798346212204
iteration : 8636
train acc:  0.8515625
train loss:  0.33156105875968933
train gradient:  0.20393351567221363
iteration : 8637
train acc:  0.8515625
train loss:  0.32390934228897095
train gradient:  0.18712773766379284
iteration : 8638
train acc:  0.8125
train loss:  0.3785184323787689
train gradient:  0.3036331359652863
iteration : 8639
train acc:  0.7890625
train loss:  0.4513578414916992
train gradient:  0.2812086417499107
iteration : 8640
train acc:  0.859375
train loss:  0.36912354826927185
train gradient:  0.22308444458791882
iteration : 8641
train acc:  0.828125
train loss:  0.3471626043319702
train gradient:  0.24771307734609588
iteration : 8642
train acc:  0.875
train loss:  0.3462596535682678
train gradient:  0.18419841580609977
iteration : 8643
train acc:  0.859375
train loss:  0.2973957061767578
train gradient:  0.11155062591604571
iteration : 8644
train acc:  0.8828125
train loss:  0.33396777510643005
train gradient:  0.18951459987543623
iteration : 8645
train acc:  0.859375
train loss:  0.3325807452201843
train gradient:  0.19997067299012097
iteration : 8646
train acc:  0.859375
train loss:  0.2965909242630005
train gradient:  0.1516525575098225
iteration : 8647
train acc:  0.8359375
train loss:  0.37811633944511414
train gradient:  0.21500003291761188
iteration : 8648
train acc:  0.8828125
train loss:  0.3254448473453522
train gradient:  0.17375996347458825
iteration : 8649
train acc:  0.9296875
train loss:  0.22142454981803894
train gradient:  0.1175092242217599
iteration : 8650
train acc:  0.84375
train loss:  0.31612730026245117
train gradient:  0.14494590283629755
iteration : 8651
train acc:  0.90625
train loss:  0.2463846653699875
train gradient:  0.23355566449626863
iteration : 8652
train acc:  0.890625
train loss:  0.27079033851623535
train gradient:  0.14528232538049862
iteration : 8653
train acc:  0.9140625
train loss:  0.26044782996177673
train gradient:  0.18147559778726952
iteration : 8654
train acc:  0.8828125
train loss:  0.306355357170105
train gradient:  0.12225019754567706
iteration : 8655
train acc:  0.8828125
train loss:  0.2829986810684204
train gradient:  0.14338930975770584
iteration : 8656
train acc:  0.875
train loss:  0.3135558068752289
train gradient:  0.14726697273696027
iteration : 8657
train acc:  0.828125
train loss:  0.33039358258247375
train gradient:  0.22532795028505176
iteration : 8658
train acc:  0.828125
train loss:  0.32596322894096375
train gradient:  0.15871825120689215
iteration : 8659
train acc:  0.8359375
train loss:  0.36650460958480835
train gradient:  0.18613338432270526
iteration : 8660
train acc:  0.8984375
train loss:  0.2667241096496582
train gradient:  0.1764074664959141
iteration : 8661
train acc:  0.875
train loss:  0.3339926600456238
train gradient:  0.21135647028372956
iteration : 8662
train acc:  0.8515625
train loss:  0.29772740602493286
train gradient:  0.14857498177143874
iteration : 8663
train acc:  0.890625
train loss:  0.27938053011894226
train gradient:  0.10911662165839221
iteration : 8664
train acc:  0.8515625
train loss:  0.3087252974510193
train gradient:  0.16252034200061966
iteration : 8665
train acc:  0.84375
train loss:  0.3560113310813904
train gradient:  0.2263511377913115
iteration : 8666
train acc:  0.8828125
train loss:  0.2637292146682739
train gradient:  0.15142047491034033
iteration : 8667
train acc:  0.859375
train loss:  0.28610461950302124
train gradient:  0.12523439014995988
iteration : 8668
train acc:  0.859375
train loss:  0.33581751585006714
train gradient:  0.193915959892618
iteration : 8669
train acc:  0.8515625
train loss:  0.2830197215080261
train gradient:  0.11293948337082965
iteration : 8670
train acc:  0.84375
train loss:  0.3355930745601654
train gradient:  0.13156109306146135
iteration : 8671
train acc:  0.8828125
train loss:  0.2709375321865082
train gradient:  0.11268627220696537
iteration : 8672
train acc:  0.8671875
train loss:  0.27713584899902344
train gradient:  0.16452570586801973
iteration : 8673
train acc:  0.8515625
train loss:  0.3905524015426636
train gradient:  0.1949705565163739
iteration : 8674
train acc:  0.8359375
train loss:  0.3391570448875427
train gradient:  0.21477559088751058
iteration : 8675
train acc:  0.8515625
train loss:  0.3660854697227478
train gradient:  0.25347838267078376
iteration : 8676
train acc:  0.8203125
train loss:  0.31356292963027954
train gradient:  0.20917622771115135
iteration : 8677
train acc:  0.8203125
train loss:  0.3582995533943176
train gradient:  0.17120824816708058
iteration : 8678
train acc:  0.8046875
train loss:  0.4447728991508484
train gradient:  0.3384486379615713
iteration : 8679
train acc:  0.859375
train loss:  0.36871618032455444
train gradient:  0.2828787955733933
iteration : 8680
train acc:  0.8515625
train loss:  0.4128228425979614
train gradient:  0.20055917709475168
iteration : 8681
train acc:  0.8671875
train loss:  0.2601131200790405
train gradient:  0.14523891626026664
iteration : 8682
train acc:  0.8515625
train loss:  0.3507978916168213
train gradient:  0.19913279237822695
iteration : 8683
train acc:  0.875
train loss:  0.34147176146507263
train gradient:  0.22121123554489522
iteration : 8684
train acc:  0.84375
train loss:  0.31755414605140686
train gradient:  0.16008491088542048
iteration : 8685
train acc:  0.8359375
train loss:  0.33192068338394165
train gradient:  0.21488695332047344
iteration : 8686
train acc:  0.84375
train loss:  0.4075012505054474
train gradient:  0.3040586171177281
iteration : 8687
train acc:  0.9296875
train loss:  0.2732545733451843
train gradient:  0.16386341521373907
iteration : 8688
train acc:  0.859375
train loss:  0.37421801686286926
train gradient:  0.20569582441584328
iteration : 8689
train acc:  0.875
train loss:  0.3449065685272217
train gradient:  0.16420005151047382
iteration : 8690
train acc:  0.84375
train loss:  0.30275461077690125
train gradient:  0.1889674749469445
iteration : 8691
train acc:  0.859375
train loss:  0.2678963541984558
train gradient:  0.14475084239174768
iteration : 8692
train acc:  0.84375
train loss:  0.4074285626411438
train gradient:  0.2992604556968112
iteration : 8693
train acc:  0.828125
train loss:  0.32517367601394653
train gradient:  0.14079180481253534
iteration : 8694
train acc:  0.8671875
train loss:  0.339170902967453
train gradient:  0.21294332005098207
iteration : 8695
train acc:  0.90625
train loss:  0.24207592010498047
train gradient:  0.10500348588231845
iteration : 8696
train acc:  0.828125
train loss:  0.37747126817703247
train gradient:  0.21934116874508097
iteration : 8697
train acc:  0.875
train loss:  0.3770129084587097
train gradient:  0.2076008673531879
iteration : 8698
train acc:  0.8359375
train loss:  0.3238462507724762
train gradient:  0.15390205393828918
iteration : 8699
train acc:  0.8828125
train loss:  0.30056655406951904
train gradient:  0.15830332109527484
iteration : 8700
train acc:  0.875
train loss:  0.3301619589328766
train gradient:  0.14471246966918158
iteration : 8701
train acc:  0.859375
train loss:  0.31073057651519775
train gradient:  0.16906305906744595
iteration : 8702
train acc:  0.8125
train loss:  0.4405083656311035
train gradient:  0.28850659291958797
iteration : 8703
train acc:  0.8203125
train loss:  0.3351841866970062
train gradient:  0.1465805558295914
iteration : 8704
train acc:  0.8515625
train loss:  0.3434395492076874
train gradient:  0.18438173100363436
iteration : 8705
train acc:  0.875
train loss:  0.33666157722473145
train gradient:  0.17008984655226844
iteration : 8706
train acc:  0.8671875
train loss:  0.33615195751190186
train gradient:  0.16899369561431968
iteration : 8707
train acc:  0.8203125
train loss:  0.38063424825668335
train gradient:  0.2605114683023544
iteration : 8708
train acc:  0.8828125
train loss:  0.31613171100616455
train gradient:  0.1446532371635219
iteration : 8709
train acc:  0.8828125
train loss:  0.31921130418777466
train gradient:  0.2097271118710266
iteration : 8710
train acc:  0.8203125
train loss:  0.3075973391532898
train gradient:  0.13474338187103826
iteration : 8711
train acc:  0.859375
train loss:  0.313272625207901
train gradient:  0.1615535307123628
iteration : 8712
train acc:  0.8515625
train loss:  0.33893322944641113
train gradient:  0.23418779752678337
iteration : 8713
train acc:  0.8203125
train loss:  0.3935099244117737
train gradient:  0.4398122658029122
iteration : 8714
train acc:  0.84375
train loss:  0.3650939464569092
train gradient:  0.18645644113387885
iteration : 8715
train acc:  0.8046875
train loss:  0.44974812865257263
train gradient:  0.2395308112462001
iteration : 8716
train acc:  0.875
train loss:  0.2913113832473755
train gradient:  0.15207840788294583
iteration : 8717
train acc:  0.8984375
train loss:  0.2535412609577179
train gradient:  0.14972024820796864
iteration : 8718
train acc:  0.890625
train loss:  0.2651812434196472
train gradient:  0.11034261217814224
iteration : 8719
train acc:  0.8203125
train loss:  0.35472407937049866
train gradient:  0.22162821187459528
iteration : 8720
train acc:  0.890625
train loss:  0.27729636430740356
train gradient:  0.11306760875198316
iteration : 8721
train acc:  0.8828125
train loss:  0.2853357791900635
train gradient:  0.127991181476229
iteration : 8722
train acc:  0.828125
train loss:  0.35934028029441833
train gradient:  0.30388051181544634
iteration : 8723
train acc:  0.875
train loss:  0.23684623837471008
train gradient:  0.13275380826148078
iteration : 8724
train acc:  0.8515625
train loss:  0.355294406414032
train gradient:  0.1349087758331346
iteration : 8725
train acc:  0.859375
train loss:  0.31654995679855347
train gradient:  0.1660407631955204
iteration : 8726
train acc:  0.8125
train loss:  0.35154008865356445
train gradient:  0.19752197533450913
iteration : 8727
train acc:  0.859375
train loss:  0.3209097683429718
train gradient:  0.14191942698390492
iteration : 8728
train acc:  0.8671875
train loss:  0.31173425912857056
train gradient:  0.1911246013758838
iteration : 8729
train acc:  0.921875
train loss:  0.2671068608760834
train gradient:  0.13757935472078328
iteration : 8730
train acc:  0.8203125
train loss:  0.39389729499816895
train gradient:  0.3127832878135833
iteration : 8731
train acc:  0.90625
train loss:  0.3015742897987366
train gradient:  0.1691641131871556
iteration : 8732
train acc:  0.84375
train loss:  0.307735800743103
train gradient:  0.15773632302045745
iteration : 8733
train acc:  0.859375
train loss:  0.2999301552772522
train gradient:  0.14518696860192265
iteration : 8734
train acc:  0.890625
train loss:  0.29058825969696045
train gradient:  0.1790258147076551
iteration : 8735
train acc:  0.8125
train loss:  0.4337414503097534
train gradient:  0.2723067186247831
iteration : 8736
train acc:  0.8125
train loss:  0.3680560290813446
train gradient:  0.21976214935685
iteration : 8737
train acc:  0.890625
train loss:  0.3060281276702881
train gradient:  0.1245685938008225
iteration : 8738
train acc:  0.8671875
train loss:  0.27518945932388306
train gradient:  0.12841275490215237
iteration : 8739
train acc:  0.8515625
train loss:  0.3061998188495636
train gradient:  0.1773861627850994
iteration : 8740
train acc:  0.8828125
train loss:  0.28391873836517334
train gradient:  0.1287314002713706
iteration : 8741
train acc:  0.828125
train loss:  0.36971315741539
train gradient:  0.23343130758559058
iteration : 8742
train acc:  0.828125
train loss:  0.3793925642967224
train gradient:  0.21972385090465008
iteration : 8743
train acc:  0.8359375
train loss:  0.3965577781200409
train gradient:  0.20964731683523674
iteration : 8744
train acc:  0.8671875
train loss:  0.33459052443504333
train gradient:  0.18366904634189934
iteration : 8745
train acc:  0.8359375
train loss:  0.37302809953689575
train gradient:  0.19381249402720443
iteration : 8746
train acc:  0.875
train loss:  0.2969992756843567
train gradient:  0.11722609597232225
iteration : 8747
train acc:  0.8984375
train loss:  0.2664855122566223
train gradient:  0.1268813963898453
iteration : 8748
train acc:  0.8515625
train loss:  0.3130336403846741
train gradient:  0.2352542128068124
iteration : 8749
train acc:  0.8984375
train loss:  0.3135555386543274
train gradient:  0.16032020024335122
iteration : 8750
train acc:  0.8671875
train loss:  0.2514578104019165
train gradient:  0.1263694891681185
iteration : 8751
train acc:  0.84375
train loss:  0.31744837760925293
train gradient:  0.18997579226307418
iteration : 8752
train acc:  0.8515625
train loss:  0.32255813479423523
train gradient:  0.22430473930962608
iteration : 8753
train acc:  0.84375
train loss:  0.33352988958358765
train gradient:  0.19860739045603998
iteration : 8754
train acc:  0.8515625
train loss:  0.3387598395347595
train gradient:  0.1998683354070755
iteration : 8755
train acc:  0.9375
train loss:  0.22315827012062073
train gradient:  0.07536045457218087
iteration : 8756
train acc:  0.828125
train loss:  0.3477400541305542
train gradient:  0.18605428810129676
iteration : 8757
train acc:  0.84375
train loss:  0.2869519889354706
train gradient:  0.15641536231704248
iteration : 8758
train acc:  0.875
train loss:  0.27214542031288147
train gradient:  0.10717806360207603
iteration : 8759
train acc:  0.828125
train loss:  0.37678641080856323
train gradient:  0.21826782534463512
iteration : 8760
train acc:  0.84375
train loss:  0.2971062958240509
train gradient:  0.18375790079513335
iteration : 8761
train acc:  0.8046875
train loss:  0.39244863390922546
train gradient:  0.20287214521993052
iteration : 8762
train acc:  0.8828125
train loss:  0.26841217279434204
train gradient:  0.11631504001879088
iteration : 8763
train acc:  0.890625
train loss:  0.27221590280532837
train gradient:  0.16274096123135495
iteration : 8764
train acc:  0.8671875
train loss:  0.2898711860179901
train gradient:  0.11319629320290582
iteration : 8765
train acc:  0.921875
train loss:  0.21276170015335083
train gradient:  0.09441695576989795
iteration : 8766
train acc:  0.890625
train loss:  0.246093288064003
train gradient:  0.10035683108036271
iteration : 8767
train acc:  0.8671875
train loss:  0.3297726511955261
train gradient:  0.20688973172241004
iteration : 8768
train acc:  0.8984375
train loss:  0.3481549322605133
train gradient:  0.29394247888057545
iteration : 8769
train acc:  0.84375
train loss:  0.36372077465057373
train gradient:  0.20135022230828292
iteration : 8770
train acc:  0.8671875
train loss:  0.33643466234207153
train gradient:  0.1966091265734088
iteration : 8771
train acc:  0.8359375
train loss:  0.30345308780670166
train gradient:  0.14438766323838884
iteration : 8772
train acc:  0.859375
train loss:  0.3300568461418152
train gradient:  0.15490812133606996
iteration : 8773
train acc:  0.8125
train loss:  0.40342360734939575
train gradient:  0.2672134408602922
iteration : 8774
train acc:  0.875
train loss:  0.29030996561050415
train gradient:  0.13998192312625074
iteration : 8775
train acc:  0.875
train loss:  0.3464851379394531
train gradient:  0.19739084595882206
iteration : 8776
train acc:  0.8828125
train loss:  0.2909156084060669
train gradient:  0.1297741773882049
iteration : 8777
train acc:  0.84375
train loss:  0.3163261413574219
train gradient:  0.1797204422177471
iteration : 8778
train acc:  0.796875
train loss:  0.35770589113235474
train gradient:  0.23198768621693477
iteration : 8779
train acc:  0.8515625
train loss:  0.32064029574394226
train gradient:  0.13707576265853222
iteration : 8780
train acc:  0.9140625
train loss:  0.25424736738204956
train gradient:  0.10415725699249309
iteration : 8781
train acc:  0.8984375
train loss:  0.2872543931007385
train gradient:  0.1544926934610376
iteration : 8782
train acc:  0.8046875
train loss:  0.4849795401096344
train gradient:  0.4427113436786362
iteration : 8783
train acc:  0.8671875
train loss:  0.2866756021976471
train gradient:  0.20466567109784753
iteration : 8784
train acc:  0.8359375
train loss:  0.34825941920280457
train gradient:  0.20234399374235934
iteration : 8785
train acc:  0.8828125
train loss:  0.28327029943466187
train gradient:  0.15070272166636978
iteration : 8786
train acc:  0.8203125
train loss:  0.371704638004303
train gradient:  0.30379383906718915
iteration : 8787
train acc:  0.8828125
train loss:  0.2599545121192932
train gradient:  0.21952862847179913
iteration : 8788
train acc:  0.8828125
train loss:  0.2997729480266571
train gradient:  0.14674445926226787
iteration : 8789
train acc:  0.8359375
train loss:  0.3074895441532135
train gradient:  0.1725901917201182
iteration : 8790
train acc:  0.84375
train loss:  0.34848934412002563
train gradient:  0.21630004763802696
iteration : 8791
train acc:  0.8828125
train loss:  0.3063952326774597
train gradient:  0.13897700240073751
iteration : 8792
train acc:  0.875
train loss:  0.32312169671058655
train gradient:  0.27379399673888555
iteration : 8793
train acc:  0.859375
train loss:  0.35431843996047974
train gradient:  0.3435350021525807
iteration : 8794
train acc:  0.859375
train loss:  0.32499590516090393
train gradient:  0.19003880719611535
iteration : 8795
train acc:  0.8671875
train loss:  0.30966150760650635
train gradient:  0.13580054194914615
iteration : 8796
train acc:  0.8671875
train loss:  0.34765625
train gradient:  0.1305803598119401
iteration : 8797
train acc:  0.859375
train loss:  0.32261964678764343
train gradient:  0.2854243114867534
iteration : 8798
train acc:  0.875
train loss:  0.3368713855743408
train gradient:  0.16776605267846711
iteration : 8799
train acc:  0.8671875
train loss:  0.28092193603515625
train gradient:  0.10404431782109877
iteration : 8800
train acc:  0.84375
train loss:  0.38481631875038147
train gradient:  0.29060750734719004
iteration : 8801
train acc:  0.8359375
train loss:  0.3799513578414917
train gradient:  0.23232340837124246
iteration : 8802
train acc:  0.8359375
train loss:  0.3406943082809448
train gradient:  0.16702187782380135
iteration : 8803
train acc:  0.8515625
train loss:  0.39640283584594727
train gradient:  0.45777343129987247
iteration : 8804
train acc:  0.90625
train loss:  0.2548324465751648
train gradient:  0.11389878858523124
iteration : 8805
train acc:  0.9140625
train loss:  0.25330132246017456
train gradient:  0.09761331325394512
iteration : 8806
train acc:  0.7578125
train loss:  0.5373498797416687
train gradient:  0.5695656787400577
iteration : 8807
train acc:  0.890625
train loss:  0.2788042724132538
train gradient:  0.16488833714176737
iteration : 8808
train acc:  0.875
train loss:  0.3153837323188782
train gradient:  0.1277874610465335
iteration : 8809
train acc:  0.828125
train loss:  0.39901673793792725
train gradient:  0.2617096233609993
iteration : 8810
train acc:  0.8203125
train loss:  0.37717264890670776
train gradient:  0.21351778535813765
iteration : 8811
train acc:  0.875
train loss:  0.2965335249900818
train gradient:  0.12181168834646149
iteration : 8812
train acc:  0.875
train loss:  0.2980390191078186
train gradient:  0.15588811169539227
iteration : 8813
train acc:  0.796875
train loss:  0.45993995666503906
train gradient:  0.3406175847697055
iteration : 8814
train acc:  0.890625
train loss:  0.2889922857284546
train gradient:  0.1969249110286363
iteration : 8815
train acc:  0.859375
train loss:  0.2940812408924103
train gradient:  0.15472835916199565
iteration : 8816
train acc:  0.84375
train loss:  0.32265257835388184
train gradient:  0.14908740173601975
iteration : 8817
train acc:  0.875
train loss:  0.3387758135795593
train gradient:  0.23110939528498733
iteration : 8818
train acc:  0.8828125
train loss:  0.27438583970069885
train gradient:  0.10590313312982959
iteration : 8819
train acc:  0.859375
train loss:  0.3603443503379822
train gradient:  0.16754976690438755
iteration : 8820
train acc:  0.8515625
train loss:  0.28979307413101196
train gradient:  0.14462782528655535
iteration : 8821
train acc:  0.859375
train loss:  0.3265233039855957
train gradient:  0.15206429698286061
iteration : 8822
train acc:  0.8046875
train loss:  0.41211462020874023
train gradient:  0.2512208922413955
iteration : 8823
train acc:  0.8515625
train loss:  0.33167749643325806
train gradient:  0.2277143086667867
iteration : 8824
train acc:  0.8125
train loss:  0.38961073756217957
train gradient:  0.19904911935290304
iteration : 8825
train acc:  0.8984375
train loss:  0.2798102796077728
train gradient:  0.15701522634453702
iteration : 8826
train acc:  0.890625
train loss:  0.27320945262908936
train gradient:  0.15207985935267831
iteration : 8827
train acc:  0.84375
train loss:  0.31281888484954834
train gradient:  0.18755713858653544
iteration : 8828
train acc:  0.78125
train loss:  0.4642255902290344
train gradient:  0.4160011544254105
iteration : 8829
train acc:  0.828125
train loss:  0.4000776410102844
train gradient:  0.2488465128142485
iteration : 8830
train acc:  0.84375
train loss:  0.36755046248435974
train gradient:  0.1835511950948736
iteration : 8831
train acc:  0.8828125
train loss:  0.2700992226600647
train gradient:  0.13133295892987373
iteration : 8832
train acc:  0.8515625
train loss:  0.32873547077178955
train gradient:  0.20451111872927624
iteration : 8833
train acc:  0.8671875
train loss:  0.28846508264541626
train gradient:  0.1975444602118679
iteration : 8834
train acc:  0.859375
train loss:  0.3087844252586365
train gradient:  0.1459858237613998
iteration : 8835
train acc:  0.84375
train loss:  0.33038002252578735
train gradient:  0.1847868220819546
iteration : 8836
train acc:  0.859375
train loss:  0.34471315145492554
train gradient:  0.16180498990127404
iteration : 8837
train acc:  0.84375
train loss:  0.36568403244018555
train gradient:  0.17953434932862505
iteration : 8838
train acc:  0.875
train loss:  0.30745577812194824
train gradient:  0.13732158491025337
iteration : 8839
train acc:  0.8828125
train loss:  0.2918659448623657
train gradient:  0.1351410093437163
iteration : 8840
train acc:  0.8828125
train loss:  0.31526830792427063
train gradient:  0.13330918374230877
iteration : 8841
train acc:  0.8515625
train loss:  0.3575703799724579
train gradient:  0.12404064875613108
iteration : 8842
train acc:  0.8515625
train loss:  0.37339770793914795
train gradient:  0.14473942004882884
iteration : 8843
train acc:  0.8671875
train loss:  0.335785835981369
train gradient:  0.18112378358860715
iteration : 8844
train acc:  0.84375
train loss:  0.34857648611068726
train gradient:  0.1345089621792186
iteration : 8845
train acc:  0.8671875
train loss:  0.3153660297393799
train gradient:  0.17287299525380487
iteration : 8846
train acc:  0.828125
train loss:  0.3928042948246002
train gradient:  0.25538278241178003
iteration : 8847
train acc:  0.9140625
train loss:  0.24697694182395935
train gradient:  0.15357893267332384
iteration : 8848
train acc:  0.8515625
train loss:  0.3730064332485199
train gradient:  0.18783074074778175
iteration : 8849
train acc:  0.8359375
train loss:  0.32313811779022217
train gradient:  0.10436427671022712
iteration : 8850
train acc:  0.875
train loss:  0.29454201459884644
train gradient:  0.13475960620124222
iteration : 8851
train acc:  0.828125
train loss:  0.35339510440826416
train gradient:  0.18610235239000963
iteration : 8852
train acc:  0.8984375
train loss:  0.30682283639907837
train gradient:  0.15791080568824414
iteration : 8853
train acc:  0.8046875
train loss:  0.3475658893585205
train gradient:  0.2122544846943929
iteration : 8854
train acc:  0.890625
train loss:  0.3276214003562927
train gradient:  0.16065511080920442
iteration : 8855
train acc:  0.8046875
train loss:  0.3617003560066223
train gradient:  0.21385546083159385
iteration : 8856
train acc:  0.8203125
train loss:  0.3417491018772125
train gradient:  0.1769572312098376
iteration : 8857
train acc:  0.875
train loss:  0.3152150511741638
train gradient:  0.17659325907936102
iteration : 8858
train acc:  0.84375
train loss:  0.38404083251953125
train gradient:  0.19276110789730874
iteration : 8859
train acc:  0.890625
train loss:  0.27896010875701904
train gradient:  0.15358820171205068
iteration : 8860
train acc:  0.890625
train loss:  0.278022825717926
train gradient:  0.1582091325530143
iteration : 8861
train acc:  0.875
train loss:  0.3203487694263458
train gradient:  0.15979153678914154
iteration : 8862
train acc:  0.8125
train loss:  0.3945019841194153
train gradient:  0.24678695195113665
iteration : 8863
train acc:  0.8515625
train loss:  0.32636281847953796
train gradient:  0.18669171487012706
iteration : 8864
train acc:  0.859375
train loss:  0.3144664466381073
train gradient:  0.18135627327211518
iteration : 8865
train acc:  0.859375
train loss:  0.3656838536262512
train gradient:  0.20467571366940618
iteration : 8866
train acc:  0.8359375
train loss:  0.3042415976524353
train gradient:  0.16532370464284693
iteration : 8867
train acc:  0.859375
train loss:  0.34426671266555786
train gradient:  0.16834367584067847
iteration : 8868
train acc:  0.84375
train loss:  0.2939503788948059
train gradient:  0.1630702156846585
iteration : 8869
train acc:  0.859375
train loss:  0.3130592107772827
train gradient:  0.12386316837875029
iteration : 8870
train acc:  0.84375
train loss:  0.3448958694934845
train gradient:  0.22530418024966792
iteration : 8871
train acc:  0.875
train loss:  0.34086400270462036
train gradient:  0.2876254085971703
iteration : 8872
train acc:  0.875
train loss:  0.2592242658138275
train gradient:  0.10629800691693068
iteration : 8873
train acc:  0.8828125
train loss:  0.3351035714149475
train gradient:  0.17467247516597004
iteration : 8874
train acc:  0.875
train loss:  0.2718356251716614
train gradient:  0.18235476058741673
iteration : 8875
train acc:  0.765625
train loss:  0.40114444494247437
train gradient:  0.27321571495282565
iteration : 8876
train acc:  0.84375
train loss:  0.36047258973121643
train gradient:  0.18375335060643416
iteration : 8877
train acc:  0.8515625
train loss:  0.34936511516571045
train gradient:  0.4931684178305509
iteration : 8878
train acc:  0.8359375
train loss:  0.34161514043807983
train gradient:  0.1475301152647937
iteration : 8879
train acc:  0.859375
train loss:  0.34994977712631226
train gradient:  0.23121910435333473
iteration : 8880
train acc:  0.8671875
train loss:  0.32113122940063477
train gradient:  0.15027714613665794
iteration : 8881
train acc:  0.8671875
train loss:  0.3177974820137024
train gradient:  0.1527262391868212
iteration : 8882
train acc:  0.8359375
train loss:  0.35392555594444275
train gradient:  0.20402149936883662
iteration : 8883
train acc:  0.875
train loss:  0.3043469190597534
train gradient:  0.11046933864651287
iteration : 8884
train acc:  0.8671875
train loss:  0.37094271183013916
train gradient:  0.15001163168095813
iteration : 8885
train acc:  0.8828125
train loss:  0.2842734456062317
train gradient:  0.1430069837084092
iteration : 8886
train acc:  0.828125
train loss:  0.3491101861000061
train gradient:  0.18049294366416485
iteration : 8887
train acc:  0.875
train loss:  0.3112303912639618
train gradient:  0.16060249937063126
iteration : 8888
train acc:  0.921875
train loss:  0.227863609790802
train gradient:  0.09836437346766705
iteration : 8889
train acc:  0.9140625
train loss:  0.2912026047706604
train gradient:  0.18266076951199042
iteration : 8890
train acc:  0.859375
train loss:  0.32468274235725403
train gradient:  0.16925995005919697
iteration : 8891
train acc:  0.90625
train loss:  0.22947293519973755
train gradient:  0.08170860705798584
iteration : 8892
train acc:  0.8828125
train loss:  0.2846485674381256
train gradient:  0.15376796125867892
iteration : 8893
train acc:  0.8671875
train loss:  0.3593897521495819
train gradient:  0.16474960795685978
iteration : 8894
train acc:  0.9140625
train loss:  0.2711145877838135
train gradient:  0.2327156270098372
iteration : 8895
train acc:  0.8984375
train loss:  0.28428488969802856
train gradient:  0.11820471754594246
iteration : 8896
train acc:  0.8046875
train loss:  0.4107193052768707
train gradient:  0.20786541327828803
iteration : 8897
train acc:  0.8046875
train loss:  0.37916257977485657
train gradient:  0.2008055802154557
iteration : 8898
train acc:  0.8515625
train loss:  0.3513348698616028
train gradient:  0.1558420656273161
iteration : 8899
train acc:  0.84375
train loss:  0.30231165885925293
train gradient:  0.1360344675887505
iteration : 8900
train acc:  0.8515625
train loss:  0.3740420341491699
train gradient:  0.1890744371466942
iteration : 8901
train acc:  0.859375
train loss:  0.3036661446094513
train gradient:  0.13720942884905946
iteration : 8902
train acc:  0.8671875
train loss:  0.3106132447719574
train gradient:  0.18804195799187678
iteration : 8903
train acc:  0.8515625
train loss:  0.3534635305404663
train gradient:  0.18120984074595836
iteration : 8904
train acc:  0.8515625
train loss:  0.344871461391449
train gradient:  0.2334019841457289
iteration : 8905
train acc:  0.7890625
train loss:  0.4807906448841095
train gradient:  0.27080821259672483
iteration : 8906
train acc:  0.890625
train loss:  0.23604229092597961
train gradient:  0.1099051148778737
iteration : 8907
train acc:  0.8828125
train loss:  0.314212441444397
train gradient:  0.14738188721344272
iteration : 8908
train acc:  0.78125
train loss:  0.422016441822052
train gradient:  0.266583088211494
iteration : 8909
train acc:  0.859375
train loss:  0.41066232323646545
train gradient:  0.28493519308113685
iteration : 8910
train acc:  0.921875
train loss:  0.23641332983970642
train gradient:  0.09464169245913676
iteration : 8911
train acc:  0.890625
train loss:  0.2682706117630005
train gradient:  0.12267326157807891
iteration : 8912
train acc:  0.8984375
train loss:  0.290818452835083
train gradient:  0.18263847560085958
iteration : 8913
train acc:  0.828125
train loss:  0.3099311590194702
train gradient:  0.17624130027620988
iteration : 8914
train acc:  0.921875
train loss:  0.19821292161941528
train gradient:  0.07548529770887046
iteration : 8915
train acc:  0.8671875
train loss:  0.32397615909576416
train gradient:  0.14859128648110745
iteration : 8916
train acc:  0.84375
train loss:  0.3525065779685974
train gradient:  0.20540785418478769
iteration : 8917
train acc:  0.84375
train loss:  0.34807419776916504
train gradient:  0.34199363067506416
iteration : 8918
train acc:  0.84375
train loss:  0.387204647064209
train gradient:  0.2263040081419827
iteration : 8919
train acc:  0.875
train loss:  0.2723931074142456
train gradient:  0.15804569348175823
iteration : 8920
train acc:  0.8125
train loss:  0.3063400387763977
train gradient:  0.16643963192264097
iteration : 8921
train acc:  0.8203125
train loss:  0.4438502788543701
train gradient:  0.2490634716876223
iteration : 8922
train acc:  0.8515625
train loss:  0.3431130349636078
train gradient:  0.250887894526266
iteration : 8923
train acc:  0.8515625
train loss:  0.3385973572731018
train gradient:  0.19869930880986292
iteration : 8924
train acc:  0.8515625
train loss:  0.2909899353981018
train gradient:  0.13924048683480805
iteration : 8925
train acc:  0.828125
train loss:  0.423885315656662
train gradient:  0.2339103794989133
iteration : 8926
train acc:  0.8828125
train loss:  0.28164681792259216
train gradient:  0.14503427029694588
iteration : 8927
train acc:  0.828125
train loss:  0.37983256578445435
train gradient:  0.31965824208421556
iteration : 8928
train acc:  0.84375
train loss:  0.4559231400489807
train gradient:  0.2397401851696354
iteration : 8929
train acc:  0.8203125
train loss:  0.36412346363067627
train gradient:  0.31525099844632176
iteration : 8930
train acc:  0.8515625
train loss:  0.3637795150279999
train gradient:  0.1848458053832859
iteration : 8931
train acc:  0.859375
train loss:  0.33987802267074585
train gradient:  0.16184001089553654
iteration : 8932
train acc:  0.84375
train loss:  0.3530994653701782
train gradient:  0.14786619955659608
iteration : 8933
train acc:  0.8515625
train loss:  0.38423076272010803
train gradient:  0.23403509889801344
iteration : 8934
train acc:  0.875
train loss:  0.34743988513946533
train gradient:  0.2160686858247467
iteration : 8935
train acc:  0.875
train loss:  0.3036895990371704
train gradient:  0.1367389630072683
iteration : 8936
train acc:  0.859375
train loss:  0.29736220836639404
train gradient:  0.1352628527747714
iteration : 8937
train acc:  0.8203125
train loss:  0.36383748054504395
train gradient:  0.2806335269953449
iteration : 8938
train acc:  0.90625
train loss:  0.26176491379737854
train gradient:  0.15486420710210008
iteration : 8939
train acc:  0.8828125
train loss:  0.2989400625228882
train gradient:  0.13633273271626462
iteration : 8940
train acc:  0.8671875
train loss:  0.29331475496292114
train gradient:  0.16764701923865435
iteration : 8941
train acc:  0.859375
train loss:  0.2929242253303528
train gradient:  0.1673106534319801
iteration : 8942
train acc:  0.8046875
train loss:  0.368610680103302
train gradient:  0.1866430717748908
iteration : 8943
train acc:  0.8671875
train loss:  0.3414112329483032
train gradient:  0.20424587818244605
iteration : 8944
train acc:  0.8828125
train loss:  0.2989361584186554
train gradient:  0.14217543492615883
iteration : 8945
train acc:  0.8671875
train loss:  0.34121453762054443
train gradient:  0.1323349428340309
iteration : 8946
train acc:  0.859375
train loss:  0.2999388873577118
train gradient:  0.13106502971945697
iteration : 8947
train acc:  0.8984375
train loss:  0.2977220118045807
train gradient:  0.17276820588899083
iteration : 8948
train acc:  0.859375
train loss:  0.3452189266681671
train gradient:  0.2499334041576426
iteration : 8949
train acc:  0.875
train loss:  0.27723854780197144
train gradient:  0.10160572635391434
iteration : 8950
train acc:  0.8359375
train loss:  0.30351898074150085
train gradient:  0.12518476231150405
iteration : 8951
train acc:  0.8671875
train loss:  0.2734566330909729
train gradient:  0.12322775250093956
iteration : 8952
train acc:  0.890625
train loss:  0.24782128632068634
train gradient:  0.14732248237233547
iteration : 8953
train acc:  0.84375
train loss:  0.42896124720573425
train gradient:  0.18491429644440344
iteration : 8954
train acc:  0.9140625
train loss:  0.23090611398220062
train gradient:  0.13634968620090016
iteration : 8955
train acc:  0.84375
train loss:  0.4238954782485962
train gradient:  0.19254536376789247
iteration : 8956
train acc:  0.84375
train loss:  0.3221457898616791
train gradient:  0.1967900270056846
iteration : 8957
train acc:  0.9140625
train loss:  0.2656456530094147
train gradient:  0.12899391022315632
iteration : 8958
train acc:  0.8671875
train loss:  0.36907607316970825
train gradient:  0.24415737707785815
iteration : 8959
train acc:  0.8984375
train loss:  0.24220429360866547
train gradient:  0.09700545039160305
iteration : 8960
train acc:  0.875
train loss:  0.33284085988998413
train gradient:  0.16547417219475927
iteration : 8961
train acc:  0.890625
train loss:  0.2697142958641052
train gradient:  0.11006752453978291
iteration : 8962
train acc:  0.859375
train loss:  0.3148346543312073
train gradient:  0.16389236157913298
iteration : 8963
train acc:  0.8359375
train loss:  0.3791172504425049
train gradient:  0.25479491060248494
iteration : 8964
train acc:  0.890625
train loss:  0.3346294164657593
train gradient:  0.20949545515734913
iteration : 8965
train acc:  0.8515625
train loss:  0.3519117832183838
train gradient:  0.19383031648338134
iteration : 8966
train acc:  0.828125
train loss:  0.3954896330833435
train gradient:  0.20684795069816342
iteration : 8967
train acc:  0.8515625
train loss:  0.34125587344169617
train gradient:  0.230404745214205
iteration : 8968
train acc:  0.875
train loss:  0.3428274393081665
train gradient:  0.2644316076027156
iteration : 8969
train acc:  0.875
train loss:  0.298178493976593
train gradient:  0.12473216147460452
iteration : 8970
train acc:  0.828125
train loss:  0.4463949501514435
train gradient:  0.288648854551441
iteration : 8971
train acc:  0.8515625
train loss:  0.3048677444458008
train gradient:  0.13770464929988852
iteration : 8972
train acc:  0.8515625
train loss:  0.2971994876861572
train gradient:  0.16440937233029834
iteration : 8973
train acc:  0.8828125
train loss:  0.24871742725372314
train gradient:  0.12479869327583847
iteration : 8974
train acc:  0.8515625
train loss:  0.3411257266998291
train gradient:  0.2252762690998631
iteration : 8975
train acc:  0.890625
train loss:  0.2631807029247284
train gradient:  0.13188568117536592
iteration : 8976
train acc:  0.828125
train loss:  0.3382544219493866
train gradient:  0.1992753280125753
iteration : 8977
train acc:  0.859375
train loss:  0.31020882725715637
train gradient:  0.16472270131234995
iteration : 8978
train acc:  0.8828125
train loss:  0.26222553849220276
train gradient:  0.1332317108769295
iteration : 8979
train acc:  0.8984375
train loss:  0.27691930532455444
train gradient:  0.11499785727470678
iteration : 8980
train acc:  0.8359375
train loss:  0.4193437099456787
train gradient:  0.2946692273234635
iteration : 8981
train acc:  0.84375
train loss:  0.3251166343688965
train gradient:  0.2123398390105784
iteration : 8982
train acc:  0.9140625
train loss:  0.26938721537590027
train gradient:  0.14476781647975118
iteration : 8983
train acc:  0.8125
train loss:  0.4502500295639038
train gradient:  0.26778504493791705
iteration : 8984
train acc:  0.8125
train loss:  0.3930167555809021
train gradient:  0.1751430875967975
iteration : 8985
train acc:  0.8984375
train loss:  0.23859462141990662
train gradient:  0.08051694990549725
iteration : 8986
train acc:  0.8671875
train loss:  0.3153415322303772
train gradient:  0.17183885500552876
iteration : 8987
train acc:  0.84375
train loss:  0.3547157943248749
train gradient:  0.22259763291088036
iteration : 8988
train acc:  0.8515625
train loss:  0.29907238483428955
train gradient:  0.10398833858470076
iteration : 8989
train acc:  0.8984375
train loss:  0.27528995275497437
train gradient:  0.14566100836955256
iteration : 8990
train acc:  0.8046875
train loss:  0.35015517473220825
train gradient:  0.20344762827796675
iteration : 8991
train acc:  0.8828125
train loss:  0.3128419518470764
train gradient:  0.15479749501738505
iteration : 8992
train acc:  0.796875
train loss:  0.4230648875236511
train gradient:  0.23530917547691135
iteration : 8993
train acc:  0.859375
train loss:  0.29997414350509644
train gradient:  0.1153338603903796
iteration : 8994
train acc:  0.8515625
train loss:  0.3231176435947418
train gradient:  0.1609825168372705
iteration : 8995
train acc:  0.8984375
train loss:  0.2611599564552307
train gradient:  0.14408029007111206
iteration : 8996
train acc:  0.875
train loss:  0.3305790424346924
train gradient:  0.2166224218548138
iteration : 8997
train acc:  0.859375
train loss:  0.30374017357826233
train gradient:  0.1947690002570916
iteration : 8998
train acc:  0.890625
train loss:  0.30396005511283875
train gradient:  0.1430856479810923
iteration : 8999
train acc:  0.8828125
train loss:  0.2943000793457031
train gradient:  0.1894880050738041
iteration : 9000
train acc:  0.8203125
train loss:  0.33564695715904236
train gradient:  0.17618822255375988
iteration : 9001
train acc:  0.8359375
train loss:  0.31791773438453674
train gradient:  0.1780099250378919
iteration : 9002
train acc:  0.890625
train loss:  0.31264376640319824
train gradient:  0.18026997989663202
iteration : 9003
train acc:  0.84375
train loss:  0.4218892455101013
train gradient:  0.30506828217204435
iteration : 9004
train acc:  0.7890625
train loss:  0.40419524908065796
train gradient:  0.1886552820904538
iteration : 9005
train acc:  0.8515625
train loss:  0.2983930706977844
train gradient:  0.13576773034057318
iteration : 9006
train acc:  0.7890625
train loss:  0.4012889862060547
train gradient:  0.34218719474913595
iteration : 9007
train acc:  0.8515625
train loss:  0.32590174674987793
train gradient:  0.1873891232120675
iteration : 9008
train acc:  0.8515625
train loss:  0.39772331714630127
train gradient:  0.19368172235223788
iteration : 9009
train acc:  0.859375
train loss:  0.3426326513290405
train gradient:  0.22537394654781054
iteration : 9010
train acc:  0.8828125
train loss:  0.270752876996994
train gradient:  0.11077977336929279
iteration : 9011
train acc:  0.875
train loss:  0.31292781233787537
train gradient:  0.2000133479262559
iteration : 9012
train acc:  0.859375
train loss:  0.3580862283706665
train gradient:  0.22295871387069627
iteration : 9013
train acc:  0.8125
train loss:  0.37636494636535645
train gradient:  0.20621477837459418
iteration : 9014
train acc:  0.8671875
train loss:  0.3089418411254883
train gradient:  0.13269910058639617
iteration : 9015
train acc:  0.875
train loss:  0.28712689876556396
train gradient:  0.13059370494380887
iteration : 9016
train acc:  0.8515625
train loss:  0.3152428865432739
train gradient:  0.18379444294566621
iteration : 9017
train acc:  0.84375
train loss:  0.41435152292251587
train gradient:  0.2384096589706809
iteration : 9018
train acc:  0.8671875
train loss:  0.3693251311779022
train gradient:  0.21325442515932896
iteration : 9019
train acc:  0.90625
train loss:  0.3009478747844696
train gradient:  0.17856739093684493
iteration : 9020
train acc:  0.8671875
train loss:  0.31130170822143555
train gradient:  0.22239128873386804
iteration : 9021
train acc:  0.828125
train loss:  0.36807578802108765
train gradient:  0.1854469932898436
iteration : 9022
train acc:  0.8828125
train loss:  0.3584953546524048
train gradient:  0.15830335532469036
iteration : 9023
train acc:  0.8828125
train loss:  0.3156434893608093
train gradient:  0.12752640107529992
iteration : 9024
train acc:  0.890625
train loss:  0.3354968726634979
train gradient:  0.14546668411886104
iteration : 9025
train acc:  0.828125
train loss:  0.3420250415802002
train gradient:  0.24736715110141144
iteration : 9026
train acc:  0.84375
train loss:  0.35369518399238586
train gradient:  0.16150538927542846
iteration : 9027
train acc:  0.90625
train loss:  0.2518845200538635
train gradient:  0.10453103659084133
iteration : 9028
train acc:  0.890625
train loss:  0.27966976165771484
train gradient:  0.12429032029374718
iteration : 9029
train acc:  0.84375
train loss:  0.3319947123527527
train gradient:  0.12331147107810943
iteration : 9030
train acc:  0.7890625
train loss:  0.43125009536743164
train gradient:  0.2310399627470742
iteration : 9031
train acc:  0.8828125
train loss:  0.26945483684539795
train gradient:  0.1022468489433543
iteration : 9032
train acc:  0.8828125
train loss:  0.36364883184432983
train gradient:  0.16753562537014335
iteration : 9033
train acc:  0.8359375
train loss:  0.3667522072792053
train gradient:  0.15763853301847672
iteration : 9034
train acc:  0.84375
train loss:  0.39660021662712097
train gradient:  0.2411213019601835
iteration : 9035
train acc:  0.8671875
train loss:  0.29706883430480957
train gradient:  0.13864513042045032
iteration : 9036
train acc:  0.828125
train loss:  0.341391384601593
train gradient:  0.189835103915168
iteration : 9037
train acc:  0.8828125
train loss:  0.28025537729263306
train gradient:  0.11794372165524326
iteration : 9038
train acc:  0.8203125
train loss:  0.36999446153640747
train gradient:  0.18182740363296973
iteration : 9039
train acc:  0.8046875
train loss:  0.3879559636116028
train gradient:  0.2533870862644653
iteration : 9040
train acc:  0.8125
train loss:  0.3819258213043213
train gradient:  0.22655448850845514
iteration : 9041
train acc:  0.8515625
train loss:  0.3622584939002991
train gradient:  0.2721574723021119
iteration : 9042
train acc:  0.8359375
train loss:  0.3383917212486267
train gradient:  0.17147329385932128
iteration : 9043
train acc:  0.875
train loss:  0.29903745651245117
train gradient:  0.12397960617669045
iteration : 9044
train acc:  0.84375
train loss:  0.2976422607898712
train gradient:  0.13800645350194418
iteration : 9045
train acc:  0.859375
train loss:  0.32804837822914124
train gradient:  0.1513985476629922
iteration : 9046
train acc:  0.8203125
train loss:  0.33382904529571533
train gradient:  0.17131663239645484
iteration : 9047
train acc:  0.875
train loss:  0.32315993309020996
train gradient:  0.1429009653091634
iteration : 9048
train acc:  0.921875
train loss:  0.23788049817085266
train gradient:  0.09180028821896907
iteration : 9049
train acc:  0.8359375
train loss:  0.27785852551460266
train gradient:  0.14115382876345692
iteration : 9050
train acc:  0.828125
train loss:  0.3534053862094879
train gradient:  0.14110118207597921
iteration : 9051
train acc:  0.8828125
train loss:  0.2967611253261566
train gradient:  0.15841368022149235
iteration : 9052
train acc:  0.859375
train loss:  0.332908034324646
train gradient:  0.14800412187567388
iteration : 9053
train acc:  0.875
train loss:  0.3020022511482239
train gradient:  0.13591943594975564
iteration : 9054
train acc:  0.8203125
train loss:  0.3481613099575043
train gradient:  0.2590515258986845
iteration : 9055
train acc:  0.875
train loss:  0.351992666721344
train gradient:  0.24723727380016036
iteration : 9056
train acc:  0.8828125
train loss:  0.33602479100227356
train gradient:  0.14715481548015735
iteration : 9057
train acc:  0.84375
train loss:  0.3317776620388031
train gradient:  0.12745875096176557
iteration : 9058
train acc:  0.859375
train loss:  0.3080770969390869
train gradient:  0.22441052290345215
iteration : 9059
train acc:  0.9296875
train loss:  0.24772301316261292
train gradient:  0.15149145261296718
iteration : 9060
train acc:  0.8671875
train loss:  0.3774148225784302
train gradient:  0.31787568909189196
iteration : 9061
train acc:  0.9296875
train loss:  0.23450732231140137
train gradient:  0.09578628914263346
iteration : 9062
train acc:  0.8359375
train loss:  0.3865763545036316
train gradient:  0.25067298451746894
iteration : 9063
train acc:  0.90625
train loss:  0.24906763434410095
train gradient:  0.11101360485040128
iteration : 9064
train acc:  0.8515625
train loss:  0.28199321031570435
train gradient:  0.12557345939083725
iteration : 9065
train acc:  0.8203125
train loss:  0.4286806285381317
train gradient:  0.2448399783007424
iteration : 9066
train acc:  0.828125
train loss:  0.39768701791763306
train gradient:  0.3156639471835505
iteration : 9067
train acc:  0.890625
train loss:  0.27412426471710205
train gradient:  0.11589490277491037
iteration : 9068
train acc:  0.8359375
train loss:  0.3088199496269226
train gradient:  0.1476857013781553
iteration : 9069
train acc:  0.875
train loss:  0.31589072942733765
train gradient:  0.27018438147133056
iteration : 9070
train acc:  0.859375
train loss:  0.30044373869895935
train gradient:  0.13825788602067732
iteration : 9071
train acc:  0.890625
train loss:  0.2453589290380478
train gradient:  0.08376591096252631
iteration : 9072
train acc:  0.8515625
train loss:  0.3345309793949127
train gradient:  0.18557524823435073
iteration : 9073
train acc:  0.859375
train loss:  0.34667134284973145
train gradient:  0.1442393401505409
iteration : 9074
train acc:  0.859375
train loss:  0.34560835361480713
train gradient:  0.1711498551204927
iteration : 9075
train acc:  0.8203125
train loss:  0.316837877035141
train gradient:  0.15283851895802236
iteration : 9076
train acc:  0.828125
train loss:  0.3529994785785675
train gradient:  0.24699903855463445
iteration : 9077
train acc:  0.859375
train loss:  0.30154263973236084
train gradient:  0.20383307691136796
iteration : 9078
train acc:  0.875
train loss:  0.2943856716156006
train gradient:  0.11179752442680277
iteration : 9079
train acc:  0.8515625
train loss:  0.3296135365962982
train gradient:  0.18563465052276001
iteration : 9080
train acc:  0.921875
train loss:  0.2215549498796463
train gradient:  0.08992430994447419
iteration : 9081
train acc:  0.8671875
train loss:  0.28711700439453125
train gradient:  0.11249004110709995
iteration : 9082
train acc:  0.859375
train loss:  0.34478628635406494
train gradient:  0.21489216475193007
iteration : 9083
train acc:  0.828125
train loss:  0.3814215064048767
train gradient:  0.19751201801850526
iteration : 9084
train acc:  0.8671875
train loss:  0.2712720036506653
train gradient:  0.10121710336928932
iteration : 9085
train acc:  0.890625
train loss:  0.33192938566207886
train gradient:  0.18474093383607754
iteration : 9086
train acc:  0.8203125
train loss:  0.35462185740470886
train gradient:  0.19149422912578767
iteration : 9087
train acc:  0.8828125
train loss:  0.31057122349739075
train gradient:  0.13865573816090251
iteration : 9088
train acc:  0.84375
train loss:  0.46733757853507996
train gradient:  0.31121477526257096
iteration : 9089
train acc:  0.8203125
train loss:  0.35782843828201294
train gradient:  0.24802515584212956
iteration : 9090
train acc:  0.890625
train loss:  0.2611335217952728
train gradient:  0.15797627217003515
iteration : 9091
train acc:  0.8515625
train loss:  0.31934618949890137
train gradient:  0.18768929369645432
iteration : 9092
train acc:  0.828125
train loss:  0.32106924057006836
train gradient:  0.1362055322093552
iteration : 9093
train acc:  0.8671875
train loss:  0.2683602273464203
train gradient:  0.10354855293130327
iteration : 9094
train acc:  0.859375
train loss:  0.30668604373931885
train gradient:  0.18444310410714737
iteration : 9095
train acc:  0.8359375
train loss:  0.36551493406295776
train gradient:  0.19146796275888256
iteration : 9096
train acc:  0.875
train loss:  0.30198973417282104
train gradient:  0.13283122242787215
iteration : 9097
train acc:  0.90625
train loss:  0.23468753695487976
train gradient:  0.09711919959335237
iteration : 9098
train acc:  0.828125
train loss:  0.43667566776275635
train gradient:  0.29908648945646343
iteration : 9099
train acc:  0.90625
train loss:  0.31871044635772705
train gradient:  0.17853066459296418
iteration : 9100
train acc:  0.859375
train loss:  0.3435627818107605
train gradient:  0.15072583113135937
iteration : 9101
train acc:  0.8515625
train loss:  0.3164098560810089
train gradient:  0.15176402779412165
iteration : 9102
train acc:  0.890625
train loss:  0.2568795680999756
train gradient:  0.11015776972483311
iteration : 9103
train acc:  0.8125
train loss:  0.37191513180732727
train gradient:  0.21728197819183315
iteration : 9104
train acc:  0.890625
train loss:  0.26201993227005005
train gradient:  0.0959819833321506
iteration : 9105
train acc:  0.875
train loss:  0.30589842796325684
train gradient:  0.126083213420504
iteration : 9106
train acc:  0.875
train loss:  0.2832653820514679
train gradient:  0.12641733662668206
iteration : 9107
train acc:  0.9375
train loss:  0.20927612483501434
train gradient:  0.1054970465425195
iteration : 9108
train acc:  0.84375
train loss:  0.3006472587585449
train gradient:  0.18660161303841077
iteration : 9109
train acc:  0.828125
train loss:  0.33852481842041016
train gradient:  0.2047687758534234
iteration : 9110
train acc:  0.8203125
train loss:  0.3888726830482483
train gradient:  0.29710773345271063
iteration : 9111
train acc:  0.9140625
train loss:  0.2476964294910431
train gradient:  0.17746429356157192
iteration : 9112
train acc:  0.8515625
train loss:  0.3444412052631378
train gradient:  0.21421574621148923
iteration : 9113
train acc:  0.8671875
train loss:  0.27632638812065125
train gradient:  0.13152838113325194
iteration : 9114
train acc:  0.875
train loss:  0.33051300048828125
train gradient:  0.1944349914122636
iteration : 9115
train acc:  0.8203125
train loss:  0.395937979221344
train gradient:  0.29358501325706066
iteration : 9116
train acc:  0.8515625
train loss:  0.3206148147583008
train gradient:  0.21896017746982913
iteration : 9117
train acc:  0.8671875
train loss:  0.322543740272522
train gradient:  0.1522925798417591
iteration : 9118
train acc:  0.8515625
train loss:  0.29327183961868286
train gradient:  0.1158675082596534
iteration : 9119
train acc:  0.875
train loss:  0.3426550030708313
train gradient:  0.24063652084312281
iteration : 9120
train acc:  0.84375
train loss:  0.30916672945022583
train gradient:  0.18391771584065847
iteration : 9121
train acc:  0.78125
train loss:  0.43885886669158936
train gradient:  0.37398289368430254
iteration : 9122
train acc:  0.8203125
train loss:  0.37597882747650146
train gradient:  0.3295372936199846
iteration : 9123
train acc:  0.8359375
train loss:  0.34838610887527466
train gradient:  0.26647635020306965
iteration : 9124
train acc:  0.859375
train loss:  0.2977163791656494
train gradient:  0.16859006966052925
iteration : 9125
train acc:  0.828125
train loss:  0.38758522272109985
train gradient:  0.25303298921738326
iteration : 9126
train acc:  0.84375
train loss:  0.3593619465827942
train gradient:  0.2527842923904305
iteration : 9127
train acc:  0.828125
train loss:  0.3505755364894867
train gradient:  0.20787599595616235
iteration : 9128
train acc:  0.859375
train loss:  0.3310711681842804
train gradient:  0.14928448860490728
iteration : 9129
train acc:  0.8359375
train loss:  0.36356261372566223
train gradient:  0.22056526895492123
iteration : 9130
train acc:  0.8359375
train loss:  0.3225932717323303
train gradient:  0.17269249916472001
iteration : 9131
train acc:  0.9140625
train loss:  0.2335129678249359
train gradient:  0.11625565259602977
iteration : 9132
train acc:  0.8125
train loss:  0.45003196597099304
train gradient:  0.26710462345455643
iteration : 9133
train acc:  0.890625
train loss:  0.30560365319252014
train gradient:  0.15522523233054217
iteration : 9134
train acc:  0.8828125
train loss:  0.28717362880706787
train gradient:  0.19040861064435957
iteration : 9135
train acc:  0.8125
train loss:  0.4039232134819031
train gradient:  0.253474125672451
iteration : 9136
train acc:  0.828125
train loss:  0.36501869559288025
train gradient:  0.2809845337738614
iteration : 9137
train acc:  0.8515625
train loss:  0.3465299904346466
train gradient:  0.24434635569038982
iteration : 9138
train acc:  0.8359375
train loss:  0.4026898741722107
train gradient:  0.1926463175974371
iteration : 9139
train acc:  0.8515625
train loss:  0.3456529378890991
train gradient:  0.2232305959333717
iteration : 9140
train acc:  0.8671875
train loss:  0.2862289845943451
train gradient:  0.13344427966855066
iteration : 9141
train acc:  0.8515625
train loss:  0.35524672269821167
train gradient:  0.27786313339804597
iteration : 9142
train acc:  0.9140625
train loss:  0.2919642925262451
train gradient:  0.12125758567081138
iteration : 9143
train acc:  0.859375
train loss:  0.3160540461540222
train gradient:  0.15230685651035208
iteration : 9144
train acc:  0.859375
train loss:  0.3704296946525574
train gradient:  0.224747001213704
iteration : 9145
train acc:  0.8828125
train loss:  0.27132102847099304
train gradient:  0.1264919353886364
iteration : 9146
train acc:  0.828125
train loss:  0.3858518600463867
train gradient:  0.18984849791070363
iteration : 9147
train acc:  0.84375
train loss:  0.33684471249580383
train gradient:  0.1905528951396555
iteration : 9148
train acc:  0.859375
train loss:  0.365580677986145
train gradient:  0.23686741342920148
iteration : 9149
train acc:  0.828125
train loss:  0.33330509066581726
train gradient:  0.24071707819324384
iteration : 9150
train acc:  0.828125
train loss:  0.3313030004501343
train gradient:  0.20281677348252083
iteration : 9151
train acc:  0.75
train loss:  0.41187021136283875
train gradient:  0.26785922903468995
iteration : 9152
train acc:  0.8984375
train loss:  0.2722017168998718
train gradient:  0.11896380438973422
iteration : 9153
train acc:  0.8046875
train loss:  0.3686681389808655
train gradient:  0.17137602587271167
iteration : 9154
train acc:  0.828125
train loss:  0.4580147862434387
train gradient:  0.28352480714806316
iteration : 9155
train acc:  0.890625
train loss:  0.3025892376899719
train gradient:  0.1589923400770924
iteration : 9156
train acc:  0.8359375
train loss:  0.31462329626083374
train gradient:  0.17538165033362618
iteration : 9157
train acc:  0.8359375
train loss:  0.3667422831058502
train gradient:  0.19351467332441094
iteration : 9158
train acc:  0.84375
train loss:  0.3847857713699341
train gradient:  0.1670158301518137
iteration : 9159
train acc:  0.8828125
train loss:  0.28799790143966675
train gradient:  0.229794795842412
iteration : 9160
train acc:  0.8828125
train loss:  0.29287078976631165
train gradient:  0.13803952887434817
iteration : 9161
train acc:  0.8671875
train loss:  0.3433074355125427
train gradient:  0.1460652476229603
iteration : 9162
train acc:  0.859375
train loss:  0.37926918268203735
train gradient:  0.2190513774061541
iteration : 9163
train acc:  0.875
train loss:  0.30763155221939087
train gradient:  0.23189697515244068
iteration : 9164
train acc:  0.8671875
train loss:  0.3541155457496643
train gradient:  0.2225937079746877
iteration : 9165
train acc:  0.859375
train loss:  0.3404984474182129
train gradient:  0.18765679081627146
iteration : 9166
train acc:  0.84375
train loss:  0.3492036461830139
train gradient:  0.1616496336253589
iteration : 9167
train acc:  0.8125
train loss:  0.3308069407939911
train gradient:  0.11369244168428858
iteration : 9168
train acc:  0.8046875
train loss:  0.38200899958610535
train gradient:  0.2960382506208465
iteration : 9169
train acc:  0.9140625
train loss:  0.2817402482032776
train gradient:  0.10132130482937865
iteration : 9170
train acc:  0.8515625
train loss:  0.3124906122684479
train gradient:  0.18704052549414435
iteration : 9171
train acc:  0.8359375
train loss:  0.3219347894191742
train gradient:  0.14200469430844165
iteration : 9172
train acc:  0.8203125
train loss:  0.36520522832870483
train gradient:  0.16167124787513487
iteration : 9173
train acc:  0.84375
train loss:  0.41713303327560425
train gradient:  0.22157478441732634
iteration : 9174
train acc:  0.90625
train loss:  0.27879464626312256
train gradient:  0.13635707508211586
iteration : 9175
train acc:  0.8828125
train loss:  0.32286396622657776
train gradient:  0.15033918016804376
iteration : 9176
train acc:  0.828125
train loss:  0.38855209946632385
train gradient:  0.1760819367428701
iteration : 9177
train acc:  0.8203125
train loss:  0.3953809440135956
train gradient:  0.1791785287128453
iteration : 9178
train acc:  0.890625
train loss:  0.2642481327056885
train gradient:  0.111647871002784
iteration : 9179
train acc:  0.84375
train loss:  0.37193432450294495
train gradient:  0.21498940262069105
iteration : 9180
train acc:  0.859375
train loss:  0.33706432580947876
train gradient:  0.14609618012098813
iteration : 9181
train acc:  0.859375
train loss:  0.2861376702785492
train gradient:  0.15039586681258899
iteration : 9182
train acc:  0.8203125
train loss:  0.38713955879211426
train gradient:  0.18800194272172055
iteration : 9183
train acc:  0.8671875
train loss:  0.34021151065826416
train gradient:  0.1734770960512397
iteration : 9184
train acc:  0.859375
train loss:  0.3621921241283417
train gradient:  0.21447175281321917
iteration : 9185
train acc:  0.890625
train loss:  0.29110032320022583
train gradient:  0.10620481989601126
iteration : 9186
train acc:  0.8203125
train loss:  0.32986605167388916
train gradient:  0.14602962384121215
iteration : 9187
train acc:  0.8828125
train loss:  0.3391537666320801
train gradient:  0.20963446281341358
iteration : 9188
train acc:  0.875
train loss:  0.30011194944381714
train gradient:  0.10934562270437223
iteration : 9189
train acc:  0.875
train loss:  0.2733765244483948
train gradient:  0.13177573002757062
iteration : 9190
train acc:  0.859375
train loss:  0.4308374524116516
train gradient:  0.24882705261967497
iteration : 9191
train acc:  0.859375
train loss:  0.4274238049983978
train gradient:  0.40214716113157145
iteration : 9192
train acc:  0.890625
train loss:  0.3286822736263275
train gradient:  0.20306640038875776
iteration : 9193
train acc:  0.859375
train loss:  0.3335684537887573
train gradient:  0.14895319872951676
iteration : 9194
train acc:  0.8359375
train loss:  0.38474133610725403
train gradient:  0.19227363138142564
iteration : 9195
train acc:  0.828125
train loss:  0.33056509494781494
train gradient:  0.2828331309476358
iteration : 9196
train acc:  0.8515625
train loss:  0.3085220456123352
train gradient:  0.17290632052049965
iteration : 9197
train acc:  0.8359375
train loss:  0.32504451274871826
train gradient:  0.11796391965703684
iteration : 9198
train acc:  0.875
train loss:  0.26242437958717346
train gradient:  0.0862637143448114
iteration : 9199
train acc:  0.8828125
train loss:  0.2837117910385132
train gradient:  0.09805693201669771
iteration : 9200
train acc:  0.7890625
train loss:  0.34640270471572876
train gradient:  0.17944913761900427
iteration : 9201
train acc:  0.8828125
train loss:  0.2717587649822235
train gradient:  0.10632053104129965
iteration : 9202
train acc:  0.8515625
train loss:  0.3722758889198303
train gradient:  0.17557339450146361
iteration : 9203
train acc:  0.84375
train loss:  0.32562726736068726
train gradient:  0.18795004746041494
iteration : 9204
train acc:  0.875
train loss:  0.2560462951660156
train gradient:  0.1070974906112551
iteration : 9205
train acc:  0.828125
train loss:  0.3504585027694702
train gradient:  0.200012713984355
iteration : 9206
train acc:  0.921875
train loss:  0.25927096605300903
train gradient:  0.11418477057815293
iteration : 9207
train acc:  0.875
train loss:  0.32893988490104675
train gradient:  0.173422944699507
iteration : 9208
train acc:  0.8515625
train loss:  0.33398866653442383
train gradient:  0.1426521564115424
iteration : 9209
train acc:  0.921875
train loss:  0.2499859482049942
train gradient:  0.11150256200337524
iteration : 9210
train acc:  0.859375
train loss:  0.29900220036506653
train gradient:  0.19467211544117768
iteration : 9211
train acc:  0.921875
train loss:  0.23708954453468323
train gradient:  0.10969239681921067
iteration : 9212
train acc:  0.8046875
train loss:  0.4318786859512329
train gradient:  0.29321200999231545
iteration : 9213
train acc:  0.8515625
train loss:  0.34836968779563904
train gradient:  0.15700993556246098
iteration : 9214
train acc:  0.875
train loss:  0.2713257670402527
train gradient:  0.16110032559521065
iteration : 9215
train acc:  0.8359375
train loss:  0.347267210483551
train gradient:  0.21549474707277017
iteration : 9216
train acc:  0.8515625
train loss:  0.3380361795425415
train gradient:  0.20563569101944656
iteration : 9217
train acc:  0.8828125
train loss:  0.27018532156944275
train gradient:  0.16907786324378485
iteration : 9218
train acc:  0.859375
train loss:  0.33291468024253845
train gradient:  0.18141132130686258
iteration : 9219
train acc:  0.8515625
train loss:  0.40075722336769104
train gradient:  0.17854122525046248
iteration : 9220
train acc:  0.859375
train loss:  0.30762022733688354
train gradient:  0.12669943812519635
iteration : 9221
train acc:  0.859375
train loss:  0.38622766733169556
train gradient:  0.18721669664666687
iteration : 9222
train acc:  0.84375
train loss:  0.336361825466156
train gradient:  0.16755074577274104
iteration : 9223
train acc:  0.859375
train loss:  0.29289984703063965
train gradient:  0.13537467575102605
iteration : 9224
train acc:  0.8359375
train loss:  0.38590317964553833
train gradient:  0.4112458853219324
iteration : 9225
train acc:  0.8125
train loss:  0.38642990589141846
train gradient:  0.24366804202277625
iteration : 9226
train acc:  0.828125
train loss:  0.36785566806793213
train gradient:  0.18453140565709802
iteration : 9227
train acc:  0.859375
train loss:  0.384033203125
train gradient:  0.2118546006910469
iteration : 9228
train acc:  0.8671875
train loss:  0.31501108407974243
train gradient:  0.25404665345162236
iteration : 9229
train acc:  0.8828125
train loss:  0.3042580187320709
train gradient:  0.11229773481973274
iteration : 9230
train acc:  0.8125
train loss:  0.35004889965057373
train gradient:  0.18162154478086617
iteration : 9231
train acc:  0.8671875
train loss:  0.348868191242218
train gradient:  0.17437135568902898
iteration : 9232
train acc:  0.8671875
train loss:  0.3045761287212372
train gradient:  0.15521421035538596
iteration : 9233
train acc:  0.84375
train loss:  0.3071429431438446
train gradient:  0.18542358762438665
iteration : 9234
train acc:  0.8515625
train loss:  0.3500659465789795
train gradient:  0.19932311838745106
iteration : 9235
train acc:  0.8671875
train loss:  0.32011234760284424
train gradient:  0.14681047723495866
iteration : 9236
train acc:  0.8828125
train loss:  0.30369842052459717
train gradient:  0.13563386791192908
iteration : 9237
train acc:  0.859375
train loss:  0.3257213830947876
train gradient:  0.14850449045383102
iteration : 9238
train acc:  0.8828125
train loss:  0.3402487635612488
train gradient:  0.1515618579030214
iteration : 9239
train acc:  0.828125
train loss:  0.3469927906990051
train gradient:  0.18948998452967328
iteration : 9240
train acc:  0.8515625
train loss:  0.33051273226737976
train gradient:  0.21142329693078676
iteration : 9241
train acc:  0.921875
train loss:  0.2679276466369629
train gradient:  0.12709465931294542
iteration : 9242
train acc:  0.84375
train loss:  0.4130096435546875
train gradient:  0.3216457155366334
iteration : 9243
train acc:  0.890625
train loss:  0.27398282289505005
train gradient:  0.12701546069339376
iteration : 9244
train acc:  0.8125
train loss:  0.3614664673805237
train gradient:  0.18589738267100112
iteration : 9245
train acc:  0.8046875
train loss:  0.4046534299850464
train gradient:  0.2365713502571681
iteration : 9246
train acc:  0.875
train loss:  0.2993856966495514
train gradient:  0.1292154847236891
iteration : 9247
train acc:  0.8125
train loss:  0.3092424273490906
train gradient:  0.15123768385429398
iteration : 9248
train acc:  0.828125
train loss:  0.43297791481018066
train gradient:  0.2833192677051043
iteration : 9249
train acc:  0.921875
train loss:  0.25302574038505554
train gradient:  0.1247862902459878
iteration : 9250
train acc:  0.84375
train loss:  0.33424943685531616
train gradient:  0.17879107373620493
iteration : 9251
train acc:  0.8125
train loss:  0.360417902469635
train gradient:  0.20219346854746528
iteration : 9252
train acc:  0.8359375
train loss:  0.36005574464797974
train gradient:  0.18677650069329232
iteration : 9253
train acc:  0.7734375
train loss:  0.43717214465141296
train gradient:  0.3197856205420074
iteration : 9254
train acc:  0.8359375
train loss:  0.29567766189575195
train gradient:  0.12130249197351345
iteration : 9255
train acc:  0.84375
train loss:  0.3184118866920471
train gradient:  0.17437311877683806
iteration : 9256
train acc:  0.8984375
train loss:  0.3047369718551636
train gradient:  0.14087773331480063
iteration : 9257
train acc:  0.84375
train loss:  0.34088200330734253
train gradient:  0.19293604218950836
iteration : 9258
train acc:  0.8671875
train loss:  0.34985682368278503
train gradient:  0.1661504167437592
iteration : 9259
train acc:  0.921875
train loss:  0.2610906958580017
train gradient:  0.14541894661680874
iteration : 9260
train acc:  0.84375
train loss:  0.34594202041625977
train gradient:  0.17424001334470557
iteration : 9261
train acc:  0.859375
train loss:  0.3541642129421234
train gradient:  0.19122761873758942
iteration : 9262
train acc:  0.8984375
train loss:  0.3185334801673889
train gradient:  0.12218933557328873
iteration : 9263
train acc:  0.921875
train loss:  0.26978087425231934
train gradient:  0.1297595520747547
iteration : 9264
train acc:  0.828125
train loss:  0.4239159822463989
train gradient:  0.2119984979821536
iteration : 9265
train acc:  0.90625
train loss:  0.2939710319042206
train gradient:  0.13066204198651915
iteration : 9266
train acc:  0.875
train loss:  0.31664466857910156
train gradient:  0.16196650038147295
iteration : 9267
train acc:  0.875
train loss:  0.2658100724220276
train gradient:  0.1511052983672282
iteration : 9268
train acc:  0.8203125
train loss:  0.4018864333629608
train gradient:  0.18321227432655943
iteration : 9269
train acc:  0.8046875
train loss:  0.38289836049079895
train gradient:  0.17177377658207896
iteration : 9270
train acc:  0.8984375
train loss:  0.2629672884941101
train gradient:  0.10068253421666312
iteration : 9271
train acc:  0.859375
train loss:  0.33011916279792786
train gradient:  0.14577594021495238
iteration : 9272
train acc:  0.8671875
train loss:  0.2948572635650635
train gradient:  0.15970079114052843
iteration : 9273
train acc:  0.875
train loss:  0.29126179218292236
train gradient:  0.20600180772273707
iteration : 9274
train acc:  0.8671875
train loss:  0.3855653405189514
train gradient:  0.36231193289894453
iteration : 9275
train acc:  0.8828125
train loss:  0.29301130771636963
train gradient:  0.17936131413932727
iteration : 9276
train acc:  0.8515625
train loss:  0.3208695650100708
train gradient:  0.1877441826973146
iteration : 9277
train acc:  0.828125
train loss:  0.3535446524620056
train gradient:  0.17701280437264316
iteration : 9278
train acc:  0.84375
train loss:  0.31234055757522583
train gradient:  0.14337103895134853
iteration : 9279
train acc:  0.8671875
train loss:  0.327195405960083
train gradient:  0.1785522793038271
iteration : 9280
train acc:  0.84375
train loss:  0.3542581796646118
train gradient:  0.14274710510745656
iteration : 9281
train acc:  0.875
train loss:  0.3124893307685852
train gradient:  0.10369044485032566
iteration : 9282
train acc:  0.890625
train loss:  0.285234659910202
train gradient:  0.15675666840498975
iteration : 9283
train acc:  0.859375
train loss:  0.30155590176582336
train gradient:  0.12341458254513506
iteration : 9284
train acc:  0.875
train loss:  0.3038075566291809
train gradient:  0.15405137020547588
iteration : 9285
train acc:  0.828125
train loss:  0.39050477743148804
train gradient:  0.20166067363208373
iteration : 9286
train acc:  0.84375
train loss:  0.359092116355896
train gradient:  0.1836490960961128
iteration : 9287
train acc:  0.875
train loss:  0.2610912322998047
train gradient:  0.1205745811857592
iteration : 9288
train acc:  0.8359375
train loss:  0.3710741102695465
train gradient:  0.2001211691175391
iteration : 9289
train acc:  0.8671875
train loss:  0.2732738256454468
train gradient:  0.1270764352227547
iteration : 9290
train acc:  0.859375
train loss:  0.2919566333293915
train gradient:  0.17002454117397936
iteration : 9291
train acc:  0.8671875
train loss:  0.313705712556839
train gradient:  0.13922690974902954
iteration : 9292
train acc:  0.859375
train loss:  0.3024207353591919
train gradient:  0.19436102743452516
iteration : 9293
train acc:  0.875
train loss:  0.2697828412055969
train gradient:  0.17517822677252642
iteration : 9294
train acc:  0.9140625
train loss:  0.2585665285587311
train gradient:  0.2087261948152484
iteration : 9295
train acc:  0.8671875
train loss:  0.27708160877227783
train gradient:  0.19245367667703753
iteration : 9296
train acc:  0.8125
train loss:  0.4198130965232849
train gradient:  0.20948167351731634
iteration : 9297
train acc:  0.875
train loss:  0.324726402759552
train gradient:  0.15468544044917545
iteration : 9298
train acc:  0.828125
train loss:  0.3353360891342163
train gradient:  0.12749893447183286
iteration : 9299
train acc:  0.7890625
train loss:  0.4228202998638153
train gradient:  0.27966257533305977
iteration : 9300
train acc:  0.84375
train loss:  0.3459784984588623
train gradient:  0.176592330291547
iteration : 9301
train acc:  0.8125
train loss:  0.3574293255805969
train gradient:  0.20115239443378413
iteration : 9302
train acc:  0.828125
train loss:  0.35637545585632324
train gradient:  0.2351801790763956
iteration : 9303
train acc:  0.8046875
train loss:  0.37663233280181885
train gradient:  0.35761479315643735
iteration : 9304
train acc:  0.859375
train loss:  0.368677020072937
train gradient:  0.2209714808508353
iteration : 9305
train acc:  0.828125
train loss:  0.3852691948413849
train gradient:  0.29253715803206876
iteration : 9306
train acc:  0.8125
train loss:  0.45118600130081177
train gradient:  0.8709030402989588
iteration : 9307
train acc:  0.859375
train loss:  0.3588950037956238
train gradient:  0.20219145359577073
iteration : 9308
train acc:  0.7734375
train loss:  0.4179942011833191
train gradient:  0.21442319344009275
iteration : 9309
train acc:  0.859375
train loss:  0.305169016122818
train gradient:  0.17686018434224504
iteration : 9310
train acc:  0.8515625
train loss:  0.352286696434021
train gradient:  0.2111190411836521
iteration : 9311
train acc:  0.8359375
train loss:  0.29981160163879395
train gradient:  0.1184656008242567
iteration : 9312
train acc:  0.8203125
train loss:  0.34740686416625977
train gradient:  0.12893883022407077
iteration : 9313
train acc:  0.828125
train loss:  0.4059309959411621
train gradient:  0.18116321707906605
iteration : 9314
train acc:  0.828125
train loss:  0.3569261133670807
train gradient:  0.15705819753039435
iteration : 9315
train acc:  0.8671875
train loss:  0.3095090091228485
train gradient:  0.13607293273322882
iteration : 9316
train acc:  0.875
train loss:  0.2983401417732239
train gradient:  0.15836632096847372
iteration : 9317
train acc:  0.8671875
train loss:  0.3057408928871155
train gradient:  0.12995201097642917
iteration : 9318
train acc:  0.8828125
train loss:  0.317288339138031
train gradient:  0.14926358463610065
iteration : 9319
train acc:  0.7421875
train loss:  0.5167044401168823
train gradient:  0.5841693283565821
iteration : 9320
train acc:  0.796875
train loss:  0.3981964886188507
train gradient:  0.205017237604101
iteration : 9321
train acc:  0.90625
train loss:  0.3124639391899109
train gradient:  0.12418408898612013
iteration : 9322
train acc:  0.8125
train loss:  0.3958946466445923
train gradient:  0.26946648420328634
iteration : 9323
train acc:  0.8984375
train loss:  0.27847498655319214
train gradient:  0.11946139778111313
iteration : 9324
train acc:  0.8828125
train loss:  0.2666366398334503
train gradient:  0.13927632137838786
iteration : 9325
train acc:  0.8828125
train loss:  0.2548466622829437
train gradient:  0.1297252512885112
iteration : 9326
train acc:  0.875
train loss:  0.27933469414711
train gradient:  0.1742688855150669
iteration : 9327
train acc:  0.8203125
train loss:  0.3448179364204407
train gradient:  0.13674832140466356
iteration : 9328
train acc:  0.8359375
train loss:  0.296836256980896
train gradient:  0.14127351568146956
iteration : 9329
train acc:  0.8359375
train loss:  0.351088285446167
train gradient:  0.20693644999092958
iteration : 9330
train acc:  0.90625
train loss:  0.2926003038883209
train gradient:  0.16797185786555016
iteration : 9331
train acc:  0.8359375
train loss:  0.41470152139663696
train gradient:  0.23697212409250518
iteration : 9332
train acc:  0.84375
train loss:  0.33059778809547424
train gradient:  0.1397788896901233
iteration : 9333
train acc:  0.8515625
train loss:  0.34431418776512146
train gradient:  0.16394880896821556
iteration : 9334
train acc:  0.9140625
train loss:  0.2666473388671875
train gradient:  0.17584600304631734
iteration : 9335
train acc:  0.9296875
train loss:  0.2384883612394333
train gradient:  0.10942431185454876
iteration : 9336
train acc:  0.8828125
train loss:  0.3191039562225342
train gradient:  0.12614098877897204
iteration : 9337
train acc:  0.8203125
train loss:  0.4184682369232178
train gradient:  0.2675636413121348
iteration : 9338
train acc:  0.8359375
train loss:  0.3296312391757965
train gradient:  0.16726472033542744
iteration : 9339
train acc:  0.890625
train loss:  0.25610893964767456
train gradient:  0.15849852426235503
iteration : 9340
train acc:  0.8515625
train loss:  0.3537154495716095
train gradient:  0.21795961419714083
iteration : 9341
train acc:  0.84375
train loss:  0.40387994050979614
train gradient:  0.3786835987497945
iteration : 9342
train acc:  0.796875
train loss:  0.47752323746681213
train gradient:  0.4111657889803776
iteration : 9343
train acc:  0.9140625
train loss:  0.26027360558509827
train gradient:  0.11027309072073793
iteration : 9344
train acc:  0.8515625
train loss:  0.28110837936401367
train gradient:  0.0975188081398293
iteration : 9345
train acc:  0.8671875
train loss:  0.30213093757629395
train gradient:  0.1274987209418993
iteration : 9346
train acc:  0.8671875
train loss:  0.29469817876815796
train gradient:  0.1343916027958414
iteration : 9347
train acc:  0.8046875
train loss:  0.40104103088378906
train gradient:  0.19901645856058334
iteration : 9348
train acc:  0.875
train loss:  0.285161554813385
train gradient:  0.13855067604833005
iteration : 9349
train acc:  0.8359375
train loss:  0.34270304441452026
train gradient:  0.12861430058985163
iteration : 9350
train acc:  0.8671875
train loss:  0.2964244484901428
train gradient:  0.18553226742329948
iteration : 9351
train acc:  0.8828125
train loss:  0.2764563262462616
train gradient:  0.14336815641418485
iteration : 9352
train acc:  0.828125
train loss:  0.31870532035827637
train gradient:  0.1111451840566889
iteration : 9353
train acc:  0.8671875
train loss:  0.3412576913833618
train gradient:  0.16225984556028886
iteration : 9354
train acc:  0.875
train loss:  0.272317111492157
train gradient:  0.16823027347307143
iteration : 9355
train acc:  0.8359375
train loss:  0.34974923729896545
train gradient:  0.18288726725132834
iteration : 9356
train acc:  0.90625
train loss:  0.2528025805950165
train gradient:  0.12075901351199601
iteration : 9357
train acc:  0.8515625
train loss:  0.3671124279499054
train gradient:  0.2025778206191974
iteration : 9358
train acc:  0.890625
train loss:  0.28333795070648193
train gradient:  0.12989983000064498
iteration : 9359
train acc:  0.8671875
train loss:  0.32765993475914
train gradient:  0.1678404409638969
iteration : 9360
train acc:  0.8671875
train loss:  0.3007901608943939
train gradient:  0.16930738096465744
iteration : 9361
train acc:  0.8671875
train loss:  0.310372531414032
train gradient:  0.16348303762061195
iteration : 9362
train acc:  0.8203125
train loss:  0.4019010365009308
train gradient:  0.2335876060877854
iteration : 9363
train acc:  0.8125
train loss:  0.3876183331012726
train gradient:  0.19235742346588414
iteration : 9364
train acc:  0.8359375
train loss:  0.29216107726097107
train gradient:  0.1413984944120767
iteration : 9365
train acc:  0.8359375
train loss:  0.32653823494911194
train gradient:  0.1805965618577965
iteration : 9366
train acc:  0.8046875
train loss:  0.3875352144241333
train gradient:  0.220243235418019
iteration : 9367
train acc:  0.8984375
train loss:  0.3201136589050293
train gradient:  0.12177051266025558
iteration : 9368
train acc:  0.9140625
train loss:  0.2390756756067276
train gradient:  0.12570644343790896
iteration : 9369
train acc:  0.890625
train loss:  0.35540223121643066
train gradient:  0.19330875780014264
iteration : 9370
train acc:  0.875
train loss:  0.2689458131790161
train gradient:  0.13546188401290826
iteration : 9371
train acc:  0.8828125
train loss:  0.2892165780067444
train gradient:  0.21795117992197102
iteration : 9372
train acc:  0.8671875
train loss:  0.30434584617614746
train gradient:  0.155298692123363
iteration : 9373
train acc:  0.8203125
train loss:  0.36058148741722107
train gradient:  0.19287404955983728
iteration : 9374
train acc:  0.796875
train loss:  0.4085446000099182
train gradient:  0.26766839164306355
iteration : 9375
train acc:  0.8515625
train loss:  0.33286696672439575
train gradient:  0.17031053452731748
iteration : 9376
train acc:  0.8125
train loss:  0.3904939591884613
train gradient:  0.20979443785345386
iteration : 9377
train acc:  0.8203125
train loss:  0.45181846618652344
train gradient:  0.2598334377744061
iteration : 9378
train acc:  0.9296875
train loss:  0.25217440724372864
train gradient:  0.10750224306038261
iteration : 9379
train acc:  0.8515625
train loss:  0.30232489109039307
train gradient:  0.10815859265647926
iteration : 9380
train acc:  0.828125
train loss:  0.36746519804000854
train gradient:  0.2051429271590488
iteration : 9381
train acc:  0.859375
train loss:  0.3189670145511627
train gradient:  0.1426588240763131
iteration : 9382
train acc:  0.8828125
train loss:  0.2653331160545349
train gradient:  0.11885329446151145
iteration : 9383
train acc:  0.8828125
train loss:  0.2951492369174957
train gradient:  0.15169545565127496
iteration : 9384
train acc:  0.890625
train loss:  0.27189844846725464
train gradient:  0.12830807945588252
iteration : 9385
train acc:  0.859375
train loss:  0.30603155493736267
train gradient:  0.123854131035557
iteration : 9386
train acc:  0.8671875
train loss:  0.305237352848053
train gradient:  0.16487030094220181
iteration : 9387
train acc:  0.859375
train loss:  0.28140249848365784
train gradient:  0.13894999825770213
iteration : 9388
train acc:  0.828125
train loss:  0.3305951952934265
train gradient:  0.16588778844839713
iteration : 9389
train acc:  0.8203125
train loss:  0.35569632053375244
train gradient:  0.15630859103885708
iteration : 9390
train acc:  0.859375
train loss:  0.3848232626914978
train gradient:  0.23814898792859723
iteration : 9391
train acc:  0.875
train loss:  0.26156681776046753
train gradient:  0.181003532959715
iteration : 9392
train acc:  0.8359375
train loss:  0.37205713987350464
train gradient:  0.23318347823103247
iteration : 9393
train acc:  0.890625
train loss:  0.2923979163169861
train gradient:  0.16123481435197146
iteration : 9394
train acc:  0.8203125
train loss:  0.41486459970474243
train gradient:  0.25756205831738865
iteration : 9395
train acc:  0.90625
train loss:  0.2596715986728668
train gradient:  0.10333913841257895
iteration : 9396
train acc:  0.90625
train loss:  0.261107474565506
train gradient:  0.14436836052953037
iteration : 9397
train acc:  0.8359375
train loss:  0.44604602456092834
train gradient:  0.24693862827297355
iteration : 9398
train acc:  0.875
train loss:  0.35540422797203064
train gradient:  0.20747350215032995
iteration : 9399
train acc:  0.8515625
train loss:  0.3181297183036804
train gradient:  0.17653828229311502
iteration : 9400
train acc:  0.84375
train loss:  0.34676581621170044
train gradient:  0.19609598062938233
iteration : 9401
train acc:  0.84375
train loss:  0.3753392994403839
train gradient:  0.2305137346565903
iteration : 9402
train acc:  0.84375
train loss:  0.32606685161590576
train gradient:  0.22784913746561436
iteration : 9403
train acc:  0.859375
train loss:  0.3279898464679718
train gradient:  0.16769316004912876
iteration : 9404
train acc:  0.8984375
train loss:  0.29615822434425354
train gradient:  0.14272624319934565
iteration : 9405
train acc:  0.8671875
train loss:  0.3271881341934204
train gradient:  0.17165740749038805
iteration : 9406
train acc:  0.828125
train loss:  0.43026337027549744
train gradient:  0.256149870005967
iteration : 9407
train acc:  0.859375
train loss:  0.31555742025375366
train gradient:  0.13771612266185904
iteration : 9408
train acc:  0.8515625
train loss:  0.30157631635665894
train gradient:  0.1420174449797374
iteration : 9409
train acc:  0.8984375
train loss:  0.2462294101715088
train gradient:  0.12927076993510286
iteration : 9410
train acc:  0.859375
train loss:  0.29671457409858704
train gradient:  0.13484677101896142
iteration : 9411
train acc:  0.890625
train loss:  0.3053447902202606
train gradient:  0.15328382573687327
iteration : 9412
train acc:  0.8125
train loss:  0.43122026324272156
train gradient:  0.20663271338091732
iteration : 9413
train acc:  0.8515625
train loss:  0.31614765524864197
train gradient:  0.11559647728554964
iteration : 9414
train acc:  0.859375
train loss:  0.30404895544052124
train gradient:  0.15914357358332368
iteration : 9415
train acc:  0.859375
train loss:  0.35322126746177673
train gradient:  0.2208139796866444
iteration : 9416
train acc:  0.796875
train loss:  0.43919894099235535
train gradient:  0.30781655082174186
iteration : 9417
train acc:  0.8671875
train loss:  0.31530874967575073
train gradient:  0.156830941132074
iteration : 9418
train acc:  0.828125
train loss:  0.32983657717704773
train gradient:  0.1715043407244211
iteration : 9419
train acc:  0.8125
train loss:  0.38318586349487305
train gradient:  0.18930704244728203
iteration : 9420
train acc:  0.8671875
train loss:  0.2759156823158264
train gradient:  0.15566075678809121
iteration : 9421
train acc:  0.890625
train loss:  0.2896665036678314
train gradient:  0.14275277545302767
iteration : 9422
train acc:  0.90625
train loss:  0.27843576669692993
train gradient:  0.12257824805684528
iteration : 9423
train acc:  0.8203125
train loss:  0.3753293752670288
train gradient:  0.16448112781454433
iteration : 9424
train acc:  0.859375
train loss:  0.38326746225357056
train gradient:  0.2260353911203559
iteration : 9425
train acc:  0.875
train loss:  0.31247618794441223
train gradient:  0.1601167876955214
iteration : 9426
train acc:  0.859375
train loss:  0.3226909339427948
train gradient:  0.17895462369014126
iteration : 9427
train acc:  0.7890625
train loss:  0.42849960923194885
train gradient:  0.21670539978283299
iteration : 9428
train acc:  0.8671875
train loss:  0.29940009117126465
train gradient:  0.15289232255650553
iteration : 9429
train acc:  0.875
train loss:  0.3054085969924927
train gradient:  0.11542958501033482
iteration : 9430
train acc:  0.84375
train loss:  0.3882381319999695
train gradient:  0.16887640770506607
iteration : 9431
train acc:  0.8515625
train loss:  0.327525794506073
train gradient:  0.203800418221577
iteration : 9432
train acc:  0.890625
train loss:  0.26044920086860657
train gradient:  0.12087598176837801
iteration : 9433
train acc:  0.859375
train loss:  0.35970765352249146
train gradient:  0.24791116269876565
iteration : 9434
train acc:  0.875
train loss:  0.3002167344093323
train gradient:  0.18015729098516897
iteration : 9435
train acc:  0.8359375
train loss:  0.3456045389175415
train gradient:  0.1585152139636903
iteration : 9436
train acc:  0.8515625
train loss:  0.3423846364021301
train gradient:  0.17904668216219521
iteration : 9437
train acc:  0.8828125
train loss:  0.3713605999946594
train gradient:  0.22759917035552352
iteration : 9438
train acc:  0.859375
train loss:  0.3042164444923401
train gradient:  0.13314832842342322
iteration : 9439
train acc:  0.828125
train loss:  0.38514456152915955
train gradient:  0.21873632699763618
iteration : 9440
train acc:  0.890625
train loss:  0.25006455183029175
train gradient:  0.19012955476924312
iteration : 9441
train acc:  0.859375
train loss:  0.2939525246620178
train gradient:  0.11220340864646183
iteration : 9442
train acc:  0.8515625
train loss:  0.2934952974319458
train gradient:  0.15253086408753314
iteration : 9443
train acc:  0.8046875
train loss:  0.3704005181789398
train gradient:  0.2165830496907593
iteration : 9444
train acc:  0.828125
train loss:  0.3616480827331543
train gradient:  0.20802567477940911
iteration : 9445
train acc:  0.84375
train loss:  0.387027382850647
train gradient:  0.2582837008425659
iteration : 9446
train acc:  0.84375
train loss:  0.3886326253414154
train gradient:  0.22842348405694313
iteration : 9447
train acc:  0.84375
train loss:  0.36299413442611694
train gradient:  0.16983188739915206
iteration : 9448
train acc:  0.84375
train loss:  0.27031490206718445
train gradient:  0.11488985180581279
iteration : 9449
train acc:  0.8515625
train loss:  0.32886195182800293
train gradient:  0.14277443995803116
iteration : 9450
train acc:  0.8515625
train loss:  0.31512612104415894
train gradient:  0.14722058439419677
iteration : 9451
train acc:  0.859375
train loss:  0.29513096809387207
train gradient:  0.13561202184550436
iteration : 9452
train acc:  0.8359375
train loss:  0.3281385898590088
train gradient:  0.1388531298024099
iteration : 9453
train acc:  0.875
train loss:  0.30104976892471313
train gradient:  0.16595167097344404
iteration : 9454
train acc:  0.84375
train loss:  0.34859514236450195
train gradient:  0.1525196414834984
iteration : 9455
train acc:  0.84375
train loss:  0.31908172369003296
train gradient:  0.1198977354823658
iteration : 9456
train acc:  0.828125
train loss:  0.3535751402378082
train gradient:  0.1474095527962913
iteration : 9457
train acc:  0.8984375
train loss:  0.2771598696708679
train gradient:  0.11877903903026199
iteration : 9458
train acc:  0.8359375
train loss:  0.33648431301116943
train gradient:  0.14207750570795158
iteration : 9459
train acc:  0.8671875
train loss:  0.31199246644973755
train gradient:  0.1667790475907495
iteration : 9460
train acc:  0.8515625
train loss:  0.345392644405365
train gradient:  0.14538097891201152
iteration : 9461
train acc:  0.8671875
train loss:  0.3142439126968384
train gradient:  0.17237428703213148
iteration : 9462
train acc:  0.859375
train loss:  0.32054609060287476
train gradient:  0.13291643994283397
iteration : 9463
train acc:  0.90625
train loss:  0.2587292790412903
train gradient:  0.12509543586599842
iteration : 9464
train acc:  0.8828125
train loss:  0.24852091073989868
train gradient:  0.08672745080772752
iteration : 9465
train acc:  0.890625
train loss:  0.28041011095046997
train gradient:  0.14095041902264688
iteration : 9466
train acc:  0.875
train loss:  0.2806404232978821
train gradient:  0.0976883281914318
iteration : 9467
train acc:  0.875
train loss:  0.32826071977615356
train gradient:  0.19107670935812038
iteration : 9468
train acc:  0.859375
train loss:  0.34456774592399597
train gradient:  0.23550243363455597
iteration : 9469
train acc:  0.828125
train loss:  0.3423302173614502
train gradient:  0.21010000772234858
iteration : 9470
train acc:  0.8046875
train loss:  0.36657458543777466
train gradient:  0.1885012934340179
iteration : 9471
train acc:  0.8671875
train loss:  0.2823368310928345
train gradient:  0.18610238950017993
iteration : 9472
train acc:  0.8828125
train loss:  0.3141232132911682
train gradient:  0.17016268774050416
iteration : 9473
train acc:  0.875
train loss:  0.3627156913280487
train gradient:  0.1809902857481146
iteration : 9474
train acc:  0.8125
train loss:  0.4093592166900635
train gradient:  0.3032405079523132
iteration : 9475
train acc:  0.859375
train loss:  0.3321908116340637
train gradient:  0.3066805318855561
iteration : 9476
train acc:  0.90625
train loss:  0.2585919201374054
train gradient:  0.09268297161912517
iteration : 9477
train acc:  0.84375
train loss:  0.35363543033599854
train gradient:  0.17537595384790122
iteration : 9478
train acc:  0.84375
train loss:  0.35983985662460327
train gradient:  0.2278497673786526
iteration : 9479
train acc:  0.7890625
train loss:  0.4191177487373352
train gradient:  0.21075398200933013
iteration : 9480
train acc:  0.875
train loss:  0.2926657199859619
train gradient:  0.15549664260526536
iteration : 9481
train acc:  0.8359375
train loss:  0.3384690284729004
train gradient:  0.15890511785999184
iteration : 9482
train acc:  0.8671875
train loss:  0.3428179919719696
train gradient:  0.22965595401516392
iteration : 9483
train acc:  0.9140625
train loss:  0.24107274413108826
train gradient:  0.11451629866882096
iteration : 9484
train acc:  0.8828125
train loss:  0.2631939649581909
train gradient:  0.12314368182552238
iteration : 9485
train acc:  0.8515625
train loss:  0.35661035776138306
train gradient:  0.17981904962157075
iteration : 9486
train acc:  0.9296875
train loss:  0.2524254322052002
train gradient:  0.08629774663760827
iteration : 9487
train acc:  0.890625
train loss:  0.3200267553329468
train gradient:  0.2567964976566305
iteration : 9488
train acc:  0.875
train loss:  0.3617011308670044
train gradient:  0.20253517026818643
iteration : 9489
train acc:  0.8828125
train loss:  0.309542715549469
train gradient:  0.11944670815520449
iteration : 9490
train acc:  0.8671875
train loss:  0.41751736402511597
train gradient:  0.2355608520755193
iteration : 9491
train acc:  0.84375
train loss:  0.34152019023895264
train gradient:  0.16865700722598945
iteration : 9492
train acc:  0.8359375
train loss:  0.35539644956588745
train gradient:  0.19751545397614723
iteration : 9493
train acc:  0.8828125
train loss:  0.2723308205604553
train gradient:  0.17514498697021463
iteration : 9494
train acc:  0.890625
train loss:  0.27195799350738525
train gradient:  0.1638928561195705
iteration : 9495
train acc:  0.8125
train loss:  0.37126559019088745
train gradient:  0.16554162087260466
iteration : 9496
train acc:  0.890625
train loss:  0.2953120470046997
train gradient:  0.26314890088812404
iteration : 9497
train acc:  0.84375
train loss:  0.3633284568786621
train gradient:  0.2308601816045972
iteration : 9498
train acc:  0.8046875
train loss:  0.378131240606308
train gradient:  0.17609924411936545
iteration : 9499
train acc:  0.8359375
train loss:  0.3724495470523834
train gradient:  0.35651935332830587
iteration : 9500
train acc:  0.9296875
train loss:  0.21721196174621582
train gradient:  0.11839856277215385
iteration : 9501
train acc:  0.890625
train loss:  0.2501571476459503
train gradient:  0.12849591075618777
iteration : 9502
train acc:  0.8671875
train loss:  0.3012140393257141
train gradient:  0.1144636482374683
iteration : 9503
train acc:  0.875
train loss:  0.2738581895828247
train gradient:  0.13314691316210023
iteration : 9504
train acc:  0.8359375
train loss:  0.3837607204914093
train gradient:  0.23309398823033933
iteration : 9505
train acc:  0.7890625
train loss:  0.3909943699836731
train gradient:  0.23556850326424789
iteration : 9506
train acc:  0.84375
train loss:  0.3182268440723419
train gradient:  0.17645228811414054
iteration : 9507
train acc:  0.8515625
train loss:  0.32220378518104553
train gradient:  0.14358466388623622
iteration : 9508
train acc:  0.8671875
train loss:  0.2707004249095917
train gradient:  0.08796272178643588
iteration : 9509
train acc:  0.90625
train loss:  0.2627159357070923
train gradient:  0.16639753907563393
iteration : 9510
train acc:  0.8671875
train loss:  0.27667373418807983
train gradient:  0.13876616607360612
iteration : 9511
train acc:  0.859375
train loss:  0.3990199565887451
train gradient:  0.2313760065079799
iteration : 9512
train acc:  0.828125
train loss:  0.3546718955039978
train gradient:  0.24436820377659585
iteration : 9513
train acc:  0.8828125
train loss:  0.27471330761909485
train gradient:  0.11611357669157933
iteration : 9514
train acc:  0.8671875
train loss:  0.34202665090560913
train gradient:  0.3261301923689041
iteration : 9515
train acc:  0.8828125
train loss:  0.31270813941955566
train gradient:  0.19080418880402794
iteration : 9516
train acc:  0.8046875
train loss:  0.4155650734901428
train gradient:  0.24415021995130415
iteration : 9517
train acc:  0.8671875
train loss:  0.3220400810241699
train gradient:  0.19628114230234334
iteration : 9518
train acc:  0.90625
train loss:  0.2624293565750122
train gradient:  0.1270934743461356
iteration : 9519
train acc:  0.828125
train loss:  0.3455604314804077
train gradient:  0.12870362214988368
iteration : 9520
train acc:  0.8125
train loss:  0.34910061955451965
train gradient:  0.16852355679262712
iteration : 9521
train acc:  0.8515625
train loss:  0.31428858637809753
train gradient:  0.144691556628594
iteration : 9522
train acc:  0.890625
train loss:  0.2895853519439697
train gradient:  0.1644992753867389
iteration : 9523
train acc:  0.828125
train loss:  0.43244829773902893
train gradient:  0.32951909690760084
iteration : 9524
train acc:  0.8984375
train loss:  0.2649489939212799
train gradient:  0.09788088988021491
iteration : 9525
train acc:  0.828125
train loss:  0.34590572118759155
train gradient:  0.1786571966013823
iteration : 9526
train acc:  0.78125
train loss:  0.4286220967769623
train gradient:  0.27811080211146394
iteration : 9527
train acc:  0.90625
train loss:  0.31304770708084106
train gradient:  0.1530719671167876
iteration : 9528
train acc:  0.859375
train loss:  0.2944985032081604
train gradient:  0.11932130530664693
iteration : 9529
train acc:  0.859375
train loss:  0.2893452048301697
train gradient:  0.12615695153567827
iteration : 9530
train acc:  0.8515625
train loss:  0.32572513818740845
train gradient:  0.1446856789876585
iteration : 9531
train acc:  0.8359375
train loss:  0.38450586795806885
train gradient:  0.2552742560305771
iteration : 9532
train acc:  0.8671875
train loss:  0.3229748010635376
train gradient:  0.16540532157445703
iteration : 9533
train acc:  0.84375
train loss:  0.383558988571167
train gradient:  0.14679505355807448
iteration : 9534
train acc:  0.8359375
train loss:  0.39923107624053955
train gradient:  0.23972241621826157
iteration : 9535
train acc:  0.90625
train loss:  0.26200592517852783
train gradient:  0.10606685686548609
iteration : 9536
train acc:  0.890625
train loss:  0.2651210427284241
train gradient:  0.10360575517180193
iteration : 9537
train acc:  0.8359375
train loss:  0.32335808873176575
train gradient:  0.1675455283580773
iteration : 9538
train acc:  0.875
train loss:  0.27307865023612976
train gradient:  0.1263651810764524
iteration : 9539
train acc:  0.8828125
train loss:  0.34991055727005005
train gradient:  0.22789242232746193
iteration : 9540
train acc:  0.8359375
train loss:  0.35814929008483887
train gradient:  0.1638645421952183
iteration : 9541
train acc:  0.8515625
train loss:  0.33558017015457153
train gradient:  0.1732302470152564
iteration : 9542
train acc:  0.8359375
train loss:  0.379916250705719
train gradient:  0.21578707333825298
iteration : 9543
train acc:  0.8515625
train loss:  0.2954208552837372
train gradient:  0.1459693721288367
iteration : 9544
train acc:  0.8125
train loss:  0.4268195629119873
train gradient:  0.2906138060285933
iteration : 9545
train acc:  0.8359375
train loss:  0.36519622802734375
train gradient:  0.2570397915856701
iteration : 9546
train acc:  0.8515625
train loss:  0.2726428508758545
train gradient:  0.13767146634967903
iteration : 9547
train acc:  0.90625
train loss:  0.3245962858200073
train gradient:  0.17583065828069844
iteration : 9548
train acc:  0.90625
train loss:  0.2561599016189575
train gradient:  0.11000950233485818
iteration : 9549
train acc:  0.8828125
train loss:  0.33569401502609253
train gradient:  0.12725907255524743
iteration : 9550
train acc:  0.859375
train loss:  0.31989583373069763
train gradient:  0.17953440742234877
iteration : 9551
train acc:  0.8125
train loss:  0.3819156885147095
train gradient:  0.24303113810884946
iteration : 9552
train acc:  0.84375
train loss:  0.31583020091056824
train gradient:  0.13831724536415546
iteration : 9553
train acc:  0.890625
train loss:  0.2588755190372467
train gradient:  0.09466316141181404
iteration : 9554
train acc:  0.8671875
train loss:  0.3294981122016907
train gradient:  0.16280937492994568
iteration : 9555
train acc:  0.8515625
train loss:  0.3260149359703064
train gradient:  0.14699278396336374
iteration : 9556
train acc:  0.8515625
train loss:  0.3151808977127075
train gradient:  0.12284091134048027
iteration : 9557
train acc:  0.8828125
train loss:  0.2820942997932434
train gradient:  0.17764026694675128
iteration : 9558
train acc:  0.875
train loss:  0.3037976324558258
train gradient:  0.16991306345051788
iteration : 9559
train acc:  0.8359375
train loss:  0.3366365432739258
train gradient:  0.17478051492361657
iteration : 9560
train acc:  0.8828125
train loss:  0.2770559787750244
train gradient:  0.1520779429701027
iteration : 9561
train acc:  0.8046875
train loss:  0.4038854241371155
train gradient:  0.2243420184906028
iteration : 9562
train acc:  0.859375
train loss:  0.29439762234687805
train gradient:  0.1781226888699069
iteration : 9563
train acc:  0.8671875
train loss:  0.3477509319782257
train gradient:  0.12735447561734786
iteration : 9564
train acc:  0.9140625
train loss:  0.2699348032474518
train gradient:  0.12049482001361694
iteration : 9565
train acc:  0.8984375
train loss:  0.2769899368286133
train gradient:  0.16471204382548627
iteration : 9566
train acc:  0.8515625
train loss:  0.38496506214141846
train gradient:  0.23174376391507867
iteration : 9567
train acc:  0.84375
train loss:  0.3981800079345703
train gradient:  0.20233010304718094
iteration : 9568
train acc:  0.8046875
train loss:  0.4064391255378723
train gradient:  0.2870228175624653
iteration : 9569
train acc:  0.8515625
train loss:  0.30045780539512634
train gradient:  0.1550851016006815
iteration : 9570
train acc:  0.9296875
train loss:  0.2384464144706726
train gradient:  0.10071396067850844
iteration : 9571
train acc:  0.890625
train loss:  0.2985287606716156
train gradient:  0.129073466204307
iteration : 9572
train acc:  0.828125
train loss:  0.42366650700569153
train gradient:  0.2462770269680587
iteration : 9573
train acc:  0.859375
train loss:  0.2755952775478363
train gradient:  0.13377014260505
iteration : 9574
train acc:  0.8125
train loss:  0.32988983392715454
train gradient:  0.17225099315855158
iteration : 9575
train acc:  0.84375
train loss:  0.31803572177886963
train gradient:  0.1795423825668886
iteration : 9576
train acc:  0.859375
train loss:  0.3072432279586792
train gradient:  0.19268583912739398
iteration : 9577
train acc:  0.8671875
train loss:  0.2750568389892578
train gradient:  0.13224826798773007
iteration : 9578
train acc:  0.84375
train loss:  0.35873767733573914
train gradient:  0.19499068468474734
iteration : 9579
train acc:  0.8515625
train loss:  0.35709139704704285
train gradient:  0.2587131465338779
iteration : 9580
train acc:  0.8828125
train loss:  0.2875779867172241
train gradient:  0.17454799496693618
iteration : 9581
train acc:  0.8359375
train loss:  0.38220423460006714
train gradient:  0.21086727631951727
iteration : 9582
train acc:  0.84375
train loss:  0.4109472334384918
train gradient:  0.3578347375646152
iteration : 9583
train acc:  0.8125
train loss:  0.3717205822467804
train gradient:  0.19494031296157274
iteration : 9584
train acc:  0.8828125
train loss:  0.28620439767837524
train gradient:  0.2866099870714922
iteration : 9585
train acc:  0.796875
train loss:  0.4044066071510315
train gradient:  0.21525461687957814
iteration : 9586
train acc:  0.859375
train loss:  0.28827476501464844
train gradient:  0.1289354480905528
iteration : 9587
train acc:  0.828125
train loss:  0.34712517261505127
train gradient:  0.17867738854698856
iteration : 9588
train acc:  0.890625
train loss:  0.28120237588882446
train gradient:  0.23119223297416397
iteration : 9589
train acc:  0.859375
train loss:  0.279220849275589
train gradient:  0.16129862657327756
iteration : 9590
train acc:  0.890625
train loss:  0.2719363570213318
train gradient:  0.17320907192202237
iteration : 9591
train acc:  0.828125
train loss:  0.4498675465583801
train gradient:  0.2703141604453859
iteration : 9592
train acc:  0.890625
train loss:  0.2726527452468872
train gradient:  0.0984708816746696
iteration : 9593
train acc:  0.796875
train loss:  0.42838054895401
train gradient:  0.1966873612813706
iteration : 9594
train acc:  0.8828125
train loss:  0.2871473431587219
train gradient:  0.09369932895987663
iteration : 9595
train acc:  0.8203125
train loss:  0.37802234292030334
train gradient:  0.20064598366502234
iteration : 9596
train acc:  0.7734375
train loss:  0.4254443049430847
train gradient:  0.3508426897681433
iteration : 9597
train acc:  0.8828125
train loss:  0.28074911236763
train gradient:  0.14464743804653202
iteration : 9598
train acc:  0.84375
train loss:  0.3995366096496582
train gradient:  0.285059891896527
iteration : 9599
train acc:  0.8671875
train loss:  0.3227955400943756
train gradient:  0.1237040077724445
iteration : 9600
train acc:  0.859375
train loss:  0.3090343475341797
train gradient:  0.12793627338711835
iteration : 9601
train acc:  0.84375
train loss:  0.3118034601211548
train gradient:  0.1606900859805872
iteration : 9602
train acc:  0.875
train loss:  0.29754331707954407
train gradient:  0.15719119767757733
iteration : 9603
train acc:  0.8515625
train loss:  0.32483115792274475
train gradient:  0.1952444893801496
iteration : 9604
train acc:  0.859375
train loss:  0.34112441539764404
train gradient:  0.15036117752887226
iteration : 9605
train acc:  0.8515625
train loss:  0.3718883991241455
train gradient:  0.22192500809040389
iteration : 9606
train acc:  0.8359375
train loss:  0.3517456650733948
train gradient:  0.18643724991062868
iteration : 9607
train acc:  0.859375
train loss:  0.35108235478401184
train gradient:  0.22136713034085514
iteration : 9608
train acc:  0.8515625
train loss:  0.28800755739212036
train gradient:  0.10257506724977952
iteration : 9609
train acc:  0.8203125
train loss:  0.31674110889434814
train gradient:  0.19042021023158853
iteration : 9610
train acc:  0.8671875
train loss:  0.30093511939048767
train gradient:  0.15626398003650507
iteration : 9611
train acc:  0.8515625
train loss:  0.3413168489933014
train gradient:  0.14395218650329447
iteration : 9612
train acc:  0.7890625
train loss:  0.40046510100364685
train gradient:  0.23625901984050718
iteration : 9613
train acc:  0.8359375
train loss:  0.3500046133995056
train gradient:  0.16958479714820224
iteration : 9614
train acc:  0.8359375
train loss:  0.3246704936027527
train gradient:  0.17016330462658913
iteration : 9615
train acc:  0.84375
train loss:  0.33875519037246704
train gradient:  0.19056581206673934
iteration : 9616
train acc:  0.8359375
train loss:  0.3005555272102356
train gradient:  0.17518194623219124
iteration : 9617
train acc:  0.8828125
train loss:  0.29393839836120605
train gradient:  0.17692690993236415
iteration : 9618
train acc:  0.8984375
train loss:  0.3071264922618866
train gradient:  0.1210186212381115
iteration : 9619
train acc:  0.84375
train loss:  0.34177565574645996
train gradient:  0.1636959529688809
iteration : 9620
train acc:  0.8828125
train loss:  0.36417853832244873
train gradient:  0.19267953547376762
iteration : 9621
train acc:  0.859375
train loss:  0.334805428981781
train gradient:  0.11785347650970746
iteration : 9622
train acc:  0.8828125
train loss:  0.2592426836490631
train gradient:  0.11348534314451701
iteration : 9623
train acc:  0.7890625
train loss:  0.39361459016799927
train gradient:  0.2510565900659055
iteration : 9624
train acc:  0.8671875
train loss:  0.39647412300109863
train gradient:  0.24010329082941365
iteration : 9625
train acc:  0.890625
train loss:  0.280040442943573
train gradient:  0.13301191191226408
iteration : 9626
train acc:  0.875
train loss:  0.29916444420814514
train gradient:  0.12619887503873653
iteration : 9627
train acc:  0.84375
train loss:  0.35845834016799927
train gradient:  0.1641668556572304
iteration : 9628
train acc:  0.78125
train loss:  0.4194942116737366
train gradient:  0.25241744142280237
iteration : 9629
train acc:  0.8671875
train loss:  0.30504652857780457
train gradient:  0.15824981323160162
iteration : 9630
train acc:  0.875
train loss:  0.2785193622112274
train gradient:  0.14673247328481043
iteration : 9631
train acc:  0.8203125
train loss:  0.4271802306175232
train gradient:  0.34786640123742985
iteration : 9632
train acc:  0.890625
train loss:  0.2694297432899475
train gradient:  0.11115836290857793
iteration : 9633
train acc:  0.8984375
train loss:  0.2704176604747772
train gradient:  0.10587909646601687
iteration : 9634
train acc:  0.8828125
train loss:  0.3434862196445465
train gradient:  0.23408086458604205
iteration : 9635
train acc:  0.84375
train loss:  0.3787669837474823
train gradient:  0.22659222160897324
iteration : 9636
train acc:  0.8515625
train loss:  0.3121053874492645
train gradient:  0.1735990126795453
iteration : 9637
train acc:  0.859375
train loss:  0.29159653186798096
train gradient:  0.13351672491265063
iteration : 9638
train acc:  0.84375
train loss:  0.3550543785095215
train gradient:  0.1694616418619232
iteration : 9639
train acc:  0.828125
train loss:  0.2892378568649292
train gradient:  0.10121870316300174
iteration : 9640
train acc:  0.8984375
train loss:  0.28804507851600647
train gradient:  0.15188503552684424
iteration : 9641
train acc:  0.8671875
train loss:  0.3615281581878662
train gradient:  0.1842851673738462
iteration : 9642
train acc:  0.859375
train loss:  0.3321288526058197
train gradient:  0.24178309566495065
iteration : 9643
train acc:  0.875
train loss:  0.3295176327228546
train gradient:  0.15446799539312253
iteration : 9644
train acc:  0.875
train loss:  0.3450046479701996
train gradient:  0.16397010991947403
iteration : 9645
train acc:  0.8359375
train loss:  0.38166120648384094
train gradient:  0.23932516590228886
iteration : 9646
train acc:  0.8203125
train loss:  0.3316305875778198
train gradient:  0.1746870287082758
iteration : 9647
train acc:  0.7890625
train loss:  0.40906041860580444
train gradient:  0.23528867060531175
iteration : 9648
train acc:  0.8671875
train loss:  0.2550290822982788
train gradient:  0.10197892768043237
iteration : 9649
train acc:  0.796875
train loss:  0.39071881771087646
train gradient:  0.21437189811099772
iteration : 9650
train acc:  0.8359375
train loss:  0.35020941495895386
train gradient:  0.18594088880212392
iteration : 9651
train acc:  0.8828125
train loss:  0.2582632303237915
train gradient:  0.12356125503239405
iteration : 9652
train acc:  0.8671875
train loss:  0.34192466735839844
train gradient:  0.14372178385285317
iteration : 9653
train acc:  0.8125
train loss:  0.3994387090206146
train gradient:  0.20822602608383528
iteration : 9654
train acc:  0.875
train loss:  0.32938724756240845
train gradient:  0.14320076187419958
iteration : 9655
train acc:  0.8671875
train loss:  0.387941837310791
train gradient:  0.21527450432350936
iteration : 9656
train acc:  0.8359375
train loss:  0.3484971225261688
train gradient:  0.1971843915863251
iteration : 9657
train acc:  0.859375
train loss:  0.3361338973045349
train gradient:  0.1473456380464151
iteration : 9658
train acc:  0.8046875
train loss:  0.40849748253822327
train gradient:  0.2366602468852292
iteration : 9659
train acc:  0.84375
train loss:  0.3421136736869812
train gradient:  0.14905649852670655
iteration : 9660
train acc:  0.8125
train loss:  0.34821581840515137
train gradient:  0.2065276540036014
iteration : 9661
train acc:  0.8828125
train loss:  0.3247179388999939
train gradient:  0.1789773468224903
iteration : 9662
train acc:  0.8515625
train loss:  0.3447027802467346
train gradient:  0.23248635070710838
iteration : 9663
train acc:  0.84375
train loss:  0.41107502579689026
train gradient:  0.21434581532471272
iteration : 9664
train acc:  0.859375
train loss:  0.3234584331512451
train gradient:  0.14809392829939994
iteration : 9665
train acc:  0.9140625
train loss:  0.2508390545845032
train gradient:  0.1176698142813679
iteration : 9666
train acc:  0.8984375
train loss:  0.28952598571777344
train gradient:  0.12392423461580379
iteration : 9667
train acc:  0.8828125
train loss:  0.28139305114746094
train gradient:  0.11945645195307923
iteration : 9668
train acc:  0.90625
train loss:  0.25229305028915405
train gradient:  0.10169369286679544
iteration : 9669
train acc:  0.8359375
train loss:  0.3752080202102661
train gradient:  0.2626798195979767
iteration : 9670
train acc:  0.8515625
train loss:  0.31181779503822327
train gradient:  0.13282427400654367
iteration : 9671
train acc:  0.8828125
train loss:  0.28636741638183594
train gradient:  0.11088989976783989
iteration : 9672
train acc:  0.890625
train loss:  0.3121311068534851
train gradient:  0.11112950495160777
iteration : 9673
train acc:  0.875
train loss:  0.30273717641830444
train gradient:  0.1366766329747296
iteration : 9674
train acc:  0.8828125
train loss:  0.30399662256240845
train gradient:  0.1814559050201418
iteration : 9675
train acc:  0.8984375
train loss:  0.26571324467658997
train gradient:  0.07472230717970184
iteration : 9676
train acc:  0.859375
train loss:  0.3425140380859375
train gradient:  0.1280372950789227
iteration : 9677
train acc:  0.84375
train loss:  0.29061463475227356
train gradient:  0.16920226469907623
iteration : 9678
train acc:  0.8828125
train loss:  0.279416024684906
train gradient:  0.1463599991964906
iteration : 9679
train acc:  0.9296875
train loss:  0.2308540642261505
train gradient:  0.1269678401256328
iteration : 9680
train acc:  0.8359375
train loss:  0.3601331114768982
train gradient:  0.15811721039923066
iteration : 9681
train acc:  0.8359375
train loss:  0.3700346052646637
train gradient:  0.18813440575406962
iteration : 9682
train acc:  0.8828125
train loss:  0.3396354019641876
train gradient:  0.16454543564584573
iteration : 9683
train acc:  0.8828125
train loss:  0.29391294717788696
train gradient:  0.10147017070945906
iteration : 9684
train acc:  0.84375
train loss:  0.3284211754798889
train gradient:  0.20764600937235667
iteration : 9685
train acc:  0.8671875
train loss:  0.3102750778198242
train gradient:  0.18063456380644116
iteration : 9686
train acc:  0.8515625
train loss:  0.3148183226585388
train gradient:  0.22538270270844424
iteration : 9687
train acc:  0.90625
train loss:  0.2918623685836792
train gradient:  0.14830733501056248
iteration : 9688
train acc:  0.8515625
train loss:  0.35218971967697144
train gradient:  0.2175357457024865
iteration : 9689
train acc:  0.84375
train loss:  0.2935835123062134
train gradient:  0.16448121847444602
iteration : 9690
train acc:  0.84375
train loss:  0.34320080280303955
train gradient:  0.1504337412632229
iteration : 9691
train acc:  0.859375
train loss:  0.3342711925506592
train gradient:  0.22887444387551154
iteration : 9692
train acc:  0.875
train loss:  0.28455811738967896
train gradient:  0.11789094827123607
iteration : 9693
train acc:  0.875
train loss:  0.310100257396698
train gradient:  0.14165808253344858
iteration : 9694
train acc:  0.8359375
train loss:  0.35212069749832153
train gradient:  0.20517508134627732
iteration : 9695
train acc:  0.84375
train loss:  0.30956777930259705
train gradient:  0.1653898393479327
iteration : 9696
train acc:  0.875
train loss:  0.29578304290771484
train gradient:  0.1771063054126147
iteration : 9697
train acc:  0.859375
train loss:  0.3443256914615631
train gradient:  0.17064086728714037
iteration : 9698
train acc:  0.8046875
train loss:  0.3604426085948944
train gradient:  0.17474409426744986
iteration : 9699
train acc:  0.890625
train loss:  0.319873571395874
train gradient:  0.11554615409112204
iteration : 9700
train acc:  0.9140625
train loss:  0.302671879529953
train gradient:  0.15683706725075386
iteration : 9701
train acc:  0.8671875
train loss:  0.3045486807823181
train gradient:  0.11810104010247274
iteration : 9702
train acc:  0.90625
train loss:  0.27361249923706055
train gradient:  0.11225096033824726
iteration : 9703
train acc:  0.828125
train loss:  0.3863268196582794
train gradient:  0.25216095949927436
iteration : 9704
train acc:  0.8359375
train loss:  0.3123544454574585
train gradient:  0.16914065878890633
iteration : 9705
train acc:  0.8359375
train loss:  0.3794993758201599
train gradient:  0.22288407617380854
iteration : 9706
train acc:  0.8671875
train loss:  0.3495122790336609
train gradient:  0.16577221929512387
iteration : 9707
train acc:  0.8359375
train loss:  0.3743803799152374
train gradient:  0.28350232256313335
iteration : 9708
train acc:  0.8515625
train loss:  0.3402145802974701
train gradient:  0.17855742167028174
iteration : 9709
train acc:  0.8359375
train loss:  0.411215603351593
train gradient:  0.3580918809508001
iteration : 9710
train acc:  0.859375
train loss:  0.3971148729324341
train gradient:  0.18830669236748906
iteration : 9711
train acc:  0.8828125
train loss:  0.24916395545005798
train gradient:  0.09642882013401435
iteration : 9712
train acc:  0.859375
train loss:  0.28507453203201294
train gradient:  0.14582222581468607
iteration : 9713
train acc:  0.875
train loss:  0.3529391288757324
train gradient:  0.18327265984678834
iteration : 9714
train acc:  0.84375
train loss:  0.3393557071685791
train gradient:  0.16931267327632224
iteration : 9715
train acc:  0.8359375
train loss:  0.3200054466724396
train gradient:  0.18302407629980827
iteration : 9716
train acc:  0.84375
train loss:  0.38809269666671753
train gradient:  0.218401910319847
iteration : 9717
train acc:  0.8828125
train loss:  0.28499650955200195
train gradient:  0.14307201107099932
iteration : 9718
train acc:  0.8828125
train loss:  0.3022553324699402
train gradient:  0.14209458893546945
iteration : 9719
train acc:  0.8515625
train loss:  0.3419853448867798
train gradient:  0.24448031843757145
iteration : 9720
train acc:  0.859375
train loss:  0.38501065969467163
train gradient:  0.2809284946700053
iteration : 9721
train acc:  0.875
train loss:  0.3313176929950714
train gradient:  0.19823443214278957
iteration : 9722
train acc:  0.8828125
train loss:  0.39586424827575684
train gradient:  0.2891053446929156
iteration : 9723
train acc:  0.921875
train loss:  0.24193039536476135
train gradient:  0.11225051807262641
iteration : 9724
train acc:  0.8515625
train loss:  0.3119370937347412
train gradient:  0.1386403328534091
iteration : 9725
train acc:  0.8125
train loss:  0.42779380083084106
train gradient:  0.3379210736918025
iteration : 9726
train acc:  0.8828125
train loss:  0.2592778205871582
train gradient:  0.10940281632293937
iteration : 9727
train acc:  0.859375
train loss:  0.32824960350990295
train gradient:  0.20280406946785423
iteration : 9728
train acc:  0.90625
train loss:  0.23001937568187714
train gradient:  0.1587039778166566
iteration : 9729
train acc:  0.8125
train loss:  0.3566601276397705
train gradient:  0.2303616241686723
iteration : 9730
train acc:  0.8828125
train loss:  0.2850179672241211
train gradient:  0.1199765923991315
iteration : 9731
train acc:  0.90625
train loss:  0.2874215543270111
train gradient:  0.11574049856628794
iteration : 9732
train acc:  0.8515625
train loss:  0.32181382179260254
train gradient:  0.17050098621222626
iteration : 9733
train acc:  0.859375
train loss:  0.29367271065711975
train gradient:  0.11592269717563129
iteration : 9734
train acc:  0.84375
train loss:  0.3622477054595947
train gradient:  0.17918919331232808
iteration : 9735
train acc:  0.890625
train loss:  0.29492834210395813
train gradient:  0.13295818975063167
iteration : 9736
train acc:  0.8046875
train loss:  0.3860417604446411
train gradient:  0.21593855495617514
iteration : 9737
train acc:  0.84375
train loss:  0.3377879858016968
train gradient:  0.23327223297178137
iteration : 9738
train acc:  0.859375
train loss:  0.3418857753276825
train gradient:  0.17479891963789201
iteration : 9739
train acc:  0.84375
train loss:  0.36229395866394043
train gradient:  0.16779713070477237
iteration : 9740
train acc:  0.859375
train loss:  0.3097838759422302
train gradient:  0.20824214025716972
iteration : 9741
train acc:  0.84375
train loss:  0.3449905216693878
train gradient:  0.1744648850222777
iteration : 9742
train acc:  0.8359375
train loss:  0.33563005924224854
train gradient:  0.19293213207510373
iteration : 9743
train acc:  0.828125
train loss:  0.3581334352493286
train gradient:  0.16020161780416417
iteration : 9744
train acc:  0.8515625
train loss:  0.3911208212375641
train gradient:  0.22722187792917814
iteration : 9745
train acc:  0.890625
train loss:  0.2863914668560028
train gradient:  0.14150488864124317
iteration : 9746
train acc:  0.8984375
train loss:  0.2728227972984314
train gradient:  0.15050892419738265
iteration : 9747
train acc:  0.8046875
train loss:  0.4131406247615814
train gradient:  0.23984102161832363
iteration : 9748
train acc:  0.8359375
train loss:  0.323466420173645
train gradient:  0.13463392147704167
iteration : 9749
train acc:  0.8359375
train loss:  0.3457218408584595
train gradient:  0.15239368225214628
iteration : 9750
train acc:  0.828125
train loss:  0.3597186803817749
train gradient:  0.17691148022343395
iteration : 9751
train acc:  0.9140625
train loss:  0.251406192779541
train gradient:  0.08323339228579182
iteration : 9752
train acc:  0.9140625
train loss:  0.2303919792175293
train gradient:  0.11455177200037431
iteration : 9753
train acc:  0.8203125
train loss:  0.41470563411712646
train gradient:  0.2536603221205213
iteration : 9754
train acc:  0.8671875
train loss:  0.3969070315361023
train gradient:  0.21035050965785593
iteration : 9755
train acc:  0.828125
train loss:  0.3792777955532074
train gradient:  0.22199660839269697
iteration : 9756
train acc:  0.8046875
train loss:  0.37772947549819946
train gradient:  0.1988303087953341
iteration : 9757
train acc:  0.8515625
train loss:  0.3297940194606781
train gradient:  0.1480648840437901
iteration : 9758
train acc:  0.8984375
train loss:  0.29377222061157227
train gradient:  0.1896039103425312
iteration : 9759
train acc:  0.8125
train loss:  0.4030107855796814
train gradient:  0.17967648404081632
iteration : 9760
train acc:  0.8203125
train loss:  0.39117753505706787
train gradient:  0.22833091972266106
iteration : 9761
train acc:  0.8515625
train loss:  0.3367518484592438
train gradient:  0.14837442950667817
iteration : 9762
train acc:  0.8515625
train loss:  0.3133500814437866
train gradient:  0.14836820982575954
iteration : 9763
train acc:  0.8046875
train loss:  0.38171643018722534
train gradient:  0.14781623677499978
iteration : 9764
train acc:  0.8359375
train loss:  0.3813921809196472
train gradient:  0.24313747091293014
iteration : 9765
train acc:  0.8515625
train loss:  0.3548063635826111
train gradient:  0.31985526934908753
iteration : 9766
train acc:  0.796875
train loss:  0.45205530524253845
train gradient:  0.2059388649339934
iteration : 9767
train acc:  0.9140625
train loss:  0.30758389830589294
train gradient:  0.10105884651819542
iteration : 9768
train acc:  0.8671875
train loss:  0.30978965759277344
train gradient:  0.1595239332151689
iteration : 9769
train acc:  0.890625
train loss:  0.31520652770996094
train gradient:  0.13878927396751506
iteration : 9770
train acc:  0.84375
train loss:  0.3625358045101166
train gradient:  0.1587430144939224
iteration : 9771
train acc:  0.8671875
train loss:  0.32476043701171875
train gradient:  0.17099547638669724
iteration : 9772
train acc:  0.875
train loss:  0.31454765796661377
train gradient:  0.1626900836912715
iteration : 9773
train acc:  0.828125
train loss:  0.37618839740753174
train gradient:  0.19151949991884118
iteration : 9774
train acc:  0.8515625
train loss:  0.33730465173721313
train gradient:  0.24999433077206512
iteration : 9775
train acc:  0.8203125
train loss:  0.3692588806152344
train gradient:  0.1812806238675369
iteration : 9776
train acc:  0.921875
train loss:  0.26015520095825195
train gradient:  0.09118864089121363
iteration : 9777
train acc:  0.859375
train loss:  0.312486857175827
train gradient:  0.15261027085417905
iteration : 9778
train acc:  0.921875
train loss:  0.25025472044944763
train gradient:  0.15467914733890364
iteration : 9779
train acc:  0.84375
train loss:  0.32932472229003906
train gradient:  0.11872654899332115
iteration : 9780
train acc:  0.859375
train loss:  0.3136204779148102
train gradient:  0.10935406572564861
iteration : 9781
train acc:  0.875
train loss:  0.3011910319328308
train gradient:  0.12928803283573104
iteration : 9782
train acc:  0.84375
train loss:  0.34877854585647583
train gradient:  0.1407081318272538
iteration : 9783
train acc:  0.8984375
train loss:  0.24943123757839203
train gradient:  0.08612124611832642
iteration : 9784
train acc:  0.859375
train loss:  0.34926700592041016
train gradient:  0.20453299251567877
iteration : 9785
train acc:  0.8359375
train loss:  0.36774569749832153
train gradient:  0.16954147445958828
iteration : 9786
train acc:  0.859375
train loss:  0.29151785373687744
train gradient:  0.11609474612535833
iteration : 9787
train acc:  0.8984375
train loss:  0.2576132118701935
train gradient:  0.08769955415754367
iteration : 9788
train acc:  0.84375
train loss:  0.3803309500217438
train gradient:  0.15057168430924425
iteration : 9789
train acc:  0.875
train loss:  0.32475969195365906
train gradient:  0.15132635886441093
iteration : 9790
train acc:  0.8828125
train loss:  0.3176708221435547
train gradient:  0.12580983784352173
iteration : 9791
train acc:  0.90625
train loss:  0.29353731870651245
train gradient:  0.12305917666770405
iteration : 9792
train acc:  0.859375
train loss:  0.31728559732437134
train gradient:  0.22106636730675255
iteration : 9793
train acc:  0.8125
train loss:  0.4424198269844055
train gradient:  0.6024447362027647
iteration : 9794
train acc:  0.8125
train loss:  0.3817330598831177
train gradient:  0.22968296488230655
iteration : 9795
train acc:  0.859375
train loss:  0.3355836272239685
train gradient:  0.17607531834633777
iteration : 9796
train acc:  0.828125
train loss:  0.3280894160270691
train gradient:  0.1674961485138382
iteration : 9797
train acc:  0.84375
train loss:  0.3141143321990967
train gradient:  0.13965603265780782
iteration : 9798
train acc:  0.8125
train loss:  0.3864108920097351
train gradient:  0.20085407742521338
iteration : 9799
train acc:  0.921875
train loss:  0.2730128765106201
train gradient:  0.10789348566427934
iteration : 9800
train acc:  0.8828125
train loss:  0.2943721115589142
train gradient:  0.09445760265532216
iteration : 9801
train acc:  0.8515625
train loss:  0.35792309045791626
train gradient:  0.13832561822225833
iteration : 9802
train acc:  0.8359375
train loss:  0.37638866901397705
train gradient:  0.21943499696499238
iteration : 9803
train acc:  0.8046875
train loss:  0.3931681513786316
train gradient:  0.20255389549461594
iteration : 9804
train acc:  0.875
train loss:  0.31641945242881775
train gradient:  0.14001615792788857
iteration : 9805
train acc:  0.875
train loss:  0.27219271659851074
train gradient:  0.1722557570697725
iteration : 9806
train acc:  0.84375
train loss:  0.2904921770095825
train gradient:  0.12074181723830836
iteration : 9807
train acc:  0.8828125
train loss:  0.30096882581710815
train gradient:  0.10993545024222483
iteration : 9808
train acc:  0.859375
train loss:  0.3399304449558258
train gradient:  0.1953486386157438
iteration : 9809
train acc:  0.859375
train loss:  0.37050554156303406
train gradient:  0.3016592350435224
iteration : 9810
train acc:  0.875
train loss:  0.32780516147613525
train gradient:  0.16526848594717836
iteration : 9811
train acc:  0.8046875
train loss:  0.4251822829246521
train gradient:  0.2765158847970815
iteration : 9812
train acc:  0.859375
train loss:  0.3361147344112396
train gradient:  0.16265196238643098
iteration : 9813
train acc:  0.8359375
train loss:  0.38910672068595886
train gradient:  0.26606089784303394
iteration : 9814
train acc:  0.859375
train loss:  0.32270997762680054
train gradient:  0.12487969220393304
iteration : 9815
train acc:  0.875
train loss:  0.271851122379303
train gradient:  0.1322977912780578
iteration : 9816
train acc:  0.8203125
train loss:  0.36916929483413696
train gradient:  0.3735646043125428
iteration : 9817
train acc:  0.859375
train loss:  0.2849525213241577
train gradient:  0.17077121831558667
iteration : 9818
train acc:  0.8046875
train loss:  0.39490991830825806
train gradient:  0.3012292382043783
iteration : 9819
train acc:  0.890625
train loss:  0.3187749981880188
train gradient:  0.17565973804341375
iteration : 9820
train acc:  0.7890625
train loss:  0.43816882371902466
train gradient:  0.23424998980838085
iteration : 9821
train acc:  0.9140625
train loss:  0.2623530328273773
train gradient:  0.1096922749576222
iteration : 9822
train acc:  0.859375
train loss:  0.3078915774822235
train gradient:  0.11478241473585775
iteration : 9823
train acc:  0.9140625
train loss:  0.2800644338130951
train gradient:  0.1964353205069062
iteration : 9824
train acc:  0.75
train loss:  0.4467232823371887
train gradient:  0.2827508229147251
iteration : 9825
train acc:  0.875
train loss:  0.3239816427230835
train gradient:  0.16243375884271422
iteration : 9826
train acc:  0.84375
train loss:  0.32104963064193726
train gradient:  0.144215603707759
iteration : 9827
train acc:  0.859375
train loss:  0.3158118724822998
train gradient:  0.1657959155757993
iteration : 9828
train acc:  0.859375
train loss:  0.27436938881874084
train gradient:  0.17256849867162535
iteration : 9829
train acc:  0.84375
train loss:  0.3062122166156769
train gradient:  0.1349848641892577
iteration : 9830
train acc:  0.8828125
train loss:  0.30703672766685486
train gradient:  0.12655482933064005
iteration : 9831
train acc:  0.859375
train loss:  0.28545093536376953
train gradient:  0.1417359956406382
iteration : 9832
train acc:  0.875
train loss:  0.30241650342941284
train gradient:  0.18600849481287202
iteration : 9833
train acc:  0.8359375
train loss:  0.3488645851612091
train gradient:  0.18141781520081823
iteration : 9834
train acc:  0.8671875
train loss:  0.29543325304985046
train gradient:  0.12808542355595678
iteration : 9835
train acc:  0.84375
train loss:  0.3255702257156372
train gradient:  0.1569847583651079
iteration : 9836
train acc:  0.8359375
train loss:  0.3651992082595825
train gradient:  0.21044278555839974
iteration : 9837
train acc:  0.8359375
train loss:  0.3817523419857025
train gradient:  0.16381710572666414
iteration : 9838
train acc:  0.8671875
train loss:  0.3781055212020874
train gradient:  0.15515561339475803
iteration : 9839
train acc:  0.90625
train loss:  0.27581703662872314
train gradient:  0.20547929669360576
iteration : 9840
train acc:  0.8828125
train loss:  0.287738561630249
train gradient:  0.108529357552979
iteration : 9841
train acc:  0.8359375
train loss:  0.3364700973033905
train gradient:  0.14732477457829896
iteration : 9842
train acc:  0.7890625
train loss:  0.3901726305484772
train gradient:  0.2577919909826663
iteration : 9843
train acc:  0.8515625
train loss:  0.34685835242271423
train gradient:  0.1735750028474494
iteration : 9844
train acc:  0.78125
train loss:  0.3809027373790741
train gradient:  0.14444422713954455
iteration : 9845
train acc:  0.859375
train loss:  0.3594641387462616
train gradient:  0.20777539810090856
iteration : 9846
train acc:  0.9140625
train loss:  0.2582271099090576
train gradient:  0.11183444402893018
iteration : 9847
train acc:  0.828125
train loss:  0.3464425206184387
train gradient:  0.16106624610795908
iteration : 9848
train acc:  0.890625
train loss:  0.29512232542037964
train gradient:  0.14088683311033207
iteration : 9849
train acc:  0.90625
train loss:  0.25917375087738037
train gradient:  0.13122644361451627
iteration : 9850
train acc:  0.890625
train loss:  0.2637365460395813
train gradient:  0.12791755674764677
iteration : 9851
train acc:  0.8671875
train loss:  0.30985864996910095
train gradient:  0.12451603575761043
iteration : 9852
train acc:  0.796875
train loss:  0.40273886919021606
train gradient:  0.24442969705319784
iteration : 9853
train acc:  0.84375
train loss:  0.3658536374568939
train gradient:  0.23274445717461195
iteration : 9854
train acc:  0.8671875
train loss:  0.3470369577407837
train gradient:  0.2013986553809271
iteration : 9855
train acc:  0.8671875
train loss:  0.32824042439460754
train gradient:  0.1295798542211744
iteration : 9856
train acc:  0.859375
train loss:  0.3231849670410156
train gradient:  0.14399983713809344
iteration : 9857
train acc:  0.859375
train loss:  0.35162341594696045
train gradient:  0.17083410377667665
iteration : 9858
train acc:  0.8515625
train loss:  0.37405869364738464
train gradient:  0.15640118488653632
iteration : 9859
train acc:  0.8515625
train loss:  0.3006053566932678
train gradient:  0.19463758816703952
iteration : 9860
train acc:  0.8203125
train loss:  0.31022554636001587
train gradient:  0.17835975941449106
iteration : 9861
train acc:  0.921875
train loss:  0.24023091793060303
train gradient:  0.09463874942828453
iteration : 9862
train acc:  0.8515625
train loss:  0.33110272884368896
train gradient:  0.12915898044505908
iteration : 9863
train acc:  0.859375
train loss:  0.30642759799957275
train gradient:  0.15953401222654146
iteration : 9864
train acc:  0.7578125
train loss:  0.47280123829841614
train gradient:  0.4364362450640527
iteration : 9865
train acc:  0.7734375
train loss:  0.4333527088165283
train gradient:  0.2808182712352829
iteration : 9866
train acc:  0.8359375
train loss:  0.3252701461315155
train gradient:  0.20501737374978757
iteration : 9867
train acc:  0.859375
train loss:  0.2895182967185974
train gradient:  0.10625520359054481
iteration : 9868
train acc:  0.90625
train loss:  0.2604713439941406
train gradient:  0.11551888096257582
iteration : 9869
train acc:  0.859375
train loss:  0.34253379702568054
train gradient:  0.18183058788179743
iteration : 9870
train acc:  0.8203125
train loss:  0.4002351760864258
train gradient:  0.25867860540972926
iteration : 9871
train acc:  0.90625
train loss:  0.23111368715763092
train gradient:  0.09404705842260395
iteration : 9872
train acc:  0.890625
train loss:  0.25175270438194275
train gradient:  0.09654470894344125
iteration : 9873
train acc:  0.8671875
train loss:  0.32923030853271484
train gradient:  0.13945393138948553
iteration : 9874
train acc:  0.859375
train loss:  0.3293885588645935
train gradient:  0.1691856969644468
iteration : 9875
train acc:  0.8515625
train loss:  0.4005049765110016
train gradient:  0.16690793001228027
iteration : 9876
train acc:  0.8203125
train loss:  0.38812583684921265
train gradient:  0.19929858877465145
iteration : 9877
train acc:  0.8984375
train loss:  0.3219686448574066
train gradient:  0.19781503875811235
iteration : 9878
train acc:  0.828125
train loss:  0.2945879399776459
train gradient:  0.14285499395186252
iteration : 9879
train acc:  0.890625
train loss:  0.30576539039611816
train gradient:  0.1293059236689302
iteration : 9880
train acc:  0.8203125
train loss:  0.3454170823097229
train gradient:  0.1404655373569671
iteration : 9881
train acc:  0.828125
train loss:  0.30741173028945923
train gradient:  0.11777592525064985
iteration : 9882
train acc:  0.828125
train loss:  0.39399540424346924
train gradient:  0.22695653959275103
iteration : 9883
train acc:  0.8203125
train loss:  0.3885722756385803
train gradient:  0.16938785737555603
iteration : 9884
train acc:  0.84375
train loss:  0.391692578792572
train gradient:  0.22732016908364566
iteration : 9885
train acc:  0.8359375
train loss:  0.33052608370780945
train gradient:  0.1643480957579119
iteration : 9886
train acc:  0.8359375
train loss:  0.346242755651474
train gradient:  0.21352110291140525
iteration : 9887
train acc:  0.8828125
train loss:  0.25673139095306396
train gradient:  0.13269649636953562
iteration : 9888
train acc:  0.8515625
train loss:  0.31942301988601685
train gradient:  0.14461797050261646
iteration : 9889
train acc:  0.859375
train loss:  0.31436240673065186
train gradient:  0.14388063813942537
iteration : 9890
train acc:  0.828125
train loss:  0.39531072974205017
train gradient:  0.1922950244420513
iteration : 9891
train acc:  0.875
train loss:  0.28750988841056824
train gradient:  0.15675735095638907
iteration : 9892
train acc:  0.8203125
train loss:  0.33695071935653687
train gradient:  0.16209038872116507
iteration : 9893
train acc:  0.8359375
train loss:  0.3440498113632202
train gradient:  0.2513822642631881
iteration : 9894
train acc:  0.8515625
train loss:  0.30221250653266907
train gradient:  0.15059022041879253
iteration : 9895
train acc:  0.8359375
train loss:  0.3465324640274048
train gradient:  0.16303146302061428
iteration : 9896
train acc:  0.875
train loss:  0.3116089701652527
train gradient:  0.14337362691484815
iteration : 9897
train acc:  0.8203125
train loss:  0.3370942771434784
train gradient:  0.15572251676219045
iteration : 9898
train acc:  0.8203125
train loss:  0.3970959186553955
train gradient:  0.2000223417312884
iteration : 9899
train acc:  0.7890625
train loss:  0.39439135789871216
train gradient:  0.21973386947842427
iteration : 9900
train acc:  0.921875
train loss:  0.23568037152290344
train gradient:  0.10368773627586014
iteration : 9901
train acc:  0.8515625
train loss:  0.33150821924209595
train gradient:  0.14258143498092413
iteration : 9902
train acc:  0.859375
train loss:  0.3487784266471863
train gradient:  0.147616178554382
iteration : 9903
train acc:  0.8828125
train loss:  0.3278082013130188
train gradient:  0.16712218037757423
iteration : 9904
train acc:  0.8828125
train loss:  0.3394940495491028
train gradient:  0.15474488260327085
iteration : 9905
train acc:  0.8515625
train loss:  0.37122073769569397
train gradient:  0.18354593892590954
iteration : 9906
train acc:  0.8828125
train loss:  0.3894708752632141
train gradient:  0.1900137976650242
iteration : 9907
train acc:  0.9140625
train loss:  0.3023785948753357
train gradient:  0.09933284137899692
iteration : 9908
train acc:  0.84375
train loss:  0.3411102890968323
train gradient:  0.17413445999868926
iteration : 9909
train acc:  0.8515625
train loss:  0.36425673961639404
train gradient:  0.16071262826972113
iteration : 9910
train acc:  0.8984375
train loss:  0.2958485782146454
train gradient:  0.12827682456051492
iteration : 9911
train acc:  0.8125
train loss:  0.38102495670318604
train gradient:  0.17260348395588526
iteration : 9912
train acc:  0.859375
train loss:  0.29133498668670654
train gradient:  0.12186670642351419
iteration : 9913
train acc:  0.8671875
train loss:  0.34688103199005127
train gradient:  0.16678036971821097
iteration : 9914
train acc:  0.8203125
train loss:  0.4524535834789276
train gradient:  0.3309413848737066
iteration : 9915
train acc:  0.875
train loss:  0.33773666620254517
train gradient:  0.20469532838564164
iteration : 9916
train acc:  0.859375
train loss:  0.36231547594070435
train gradient:  0.2373791445081544
iteration : 9917
train acc:  0.7890625
train loss:  0.44335052371025085
train gradient:  0.1971208850926533
iteration : 9918
train acc:  0.8359375
train loss:  0.37694334983825684
train gradient:  0.13568853384427504
iteration : 9919
train acc:  0.859375
train loss:  0.3421824872493744
train gradient:  0.17145853136565203
iteration : 9920
train acc:  0.8515625
train loss:  0.2963472008705139
train gradient:  0.10967761767973944
iteration : 9921
train acc:  0.9140625
train loss:  0.31045976281166077
train gradient:  0.12263470384141775
iteration : 9922
train acc:  0.84375
train loss:  0.30766886472702026
train gradient:  0.24645851597097454
iteration : 9923
train acc:  0.84375
train loss:  0.34096890687942505
train gradient:  0.11675003772083567
iteration : 9924
train acc:  0.8671875
train loss:  0.3189381957054138
train gradient:  0.16325618115850682
iteration : 9925
train acc:  0.875
train loss:  0.30770742893218994
train gradient:  0.11038093832740588
iteration : 9926
train acc:  0.890625
train loss:  0.23146645724773407
train gradient:  0.08551311247030728
iteration : 9927
train acc:  0.859375
train loss:  0.2897745370864868
train gradient:  0.13009728264971637
iteration : 9928
train acc:  0.828125
train loss:  0.40314579010009766
train gradient:  0.1616302465835387
iteration : 9929
train acc:  0.890625
train loss:  0.2567331790924072
train gradient:  0.08966564190195592
iteration : 9930
train acc:  0.859375
train loss:  0.35561007261276245
train gradient:  0.15768481526218275
iteration : 9931
train acc:  0.8671875
train loss:  0.3712705373764038
train gradient:  0.14838961828631458
iteration : 9932
train acc:  0.890625
train loss:  0.2729181945323944
train gradient:  0.1130846991588705
iteration : 9933
train acc:  0.859375
train loss:  0.27548736333847046
train gradient:  0.09222338587927943
iteration : 9934
train acc:  0.828125
train loss:  0.38924044370651245
train gradient:  0.16318736590498162
iteration : 9935
train acc:  0.8671875
train loss:  0.2963276505470276
train gradient:  0.10799222264846237
iteration : 9936
train acc:  0.8828125
train loss:  0.37259361147880554
train gradient:  0.2073148262499781
iteration : 9937
train acc:  0.8515625
train loss:  0.35920602083206177
train gradient:  0.17731156829053432
iteration : 9938
train acc:  0.921875
train loss:  0.2638278901576996
train gradient:  0.10541139731455713
iteration : 9939
train acc:  0.8515625
train loss:  0.3590599596500397
train gradient:  0.15162741071448557
iteration : 9940
train acc:  0.765625
train loss:  0.44362232089042664
train gradient:  0.22902956655049153
iteration : 9941
train acc:  0.796875
train loss:  0.3873754143714905
train gradient:  0.17577272094953078
iteration : 9942
train acc:  0.84375
train loss:  0.3489995002746582
train gradient:  0.1253957476643557
iteration : 9943
train acc:  0.890625
train loss:  0.29014670848846436
train gradient:  0.15930687821997014
iteration : 9944
train acc:  0.84375
train loss:  0.35700419545173645
train gradient:  0.24921242618075615
iteration : 9945
train acc:  0.890625
train loss:  0.32267943024635315
train gradient:  0.12921352852873658
iteration : 9946
train acc:  0.8203125
train loss:  0.3600291907787323
train gradient:  0.1813510516583793
iteration : 9947
train acc:  0.875
train loss:  0.2889643907546997
train gradient:  0.11372946181684775
iteration : 9948
train acc:  0.875
train loss:  0.3174450993537903
train gradient:  0.13756714308058626
iteration : 9949
train acc:  0.8984375
train loss:  0.2686675488948822
train gradient:  0.10285304942218743
iteration : 9950
train acc:  0.8359375
train loss:  0.3749774098396301
train gradient:  0.2150348625805694
iteration : 9951
train acc:  0.859375
train loss:  0.2851932644844055
train gradient:  0.11437891634228005
iteration : 9952
train acc:  0.875
train loss:  0.2942996621131897
train gradient:  0.19437956010729301
iteration : 9953
train acc:  0.921875
train loss:  0.28422069549560547
train gradient:  0.543301570333496
iteration : 9954
train acc:  0.8671875
train loss:  0.29163533449172974
train gradient:  0.10067943421785663
iteration : 9955
train acc:  0.84375
train loss:  0.3846026062965393
train gradient:  0.2215839114972869
iteration : 9956
train acc:  0.8125
train loss:  0.44301852583885193
train gradient:  0.2773575276561987
iteration : 9957
train acc:  0.8671875
train loss:  0.326849102973938
train gradient:  0.19844120076146282
iteration : 9958
train acc:  0.921875
train loss:  0.27338021993637085
train gradient:  0.11066783190726732
iteration : 9959
train acc:  0.828125
train loss:  0.3525381088256836
train gradient:  0.15309471856455792
iteration : 9960
train acc:  0.8203125
train loss:  0.36348065733909607
train gradient:  0.23294643828582673
iteration : 9961
train acc:  0.84375
train loss:  0.3817674517631531
train gradient:  0.17486525532829086
iteration : 9962
train acc:  0.84375
train loss:  0.3581969738006592
train gradient:  0.15179820111082115
iteration : 9963
train acc:  0.8984375
train loss:  0.29436787962913513
train gradient:  0.1188017258597685
iteration : 9964
train acc:  0.8046875
train loss:  0.407835990190506
train gradient:  0.2399833986295481
iteration : 9965
train acc:  0.8515625
train loss:  0.3777170777320862
train gradient:  0.1674267579513872
iteration : 9966
train acc:  0.890625
train loss:  0.28945139050483704
train gradient:  0.1682287349748024
iteration : 9967
train acc:  0.8046875
train loss:  0.4130999743938446
train gradient:  0.19730981745426415
iteration : 9968
train acc:  0.859375
train loss:  0.3334316611289978
train gradient:  0.14221497001657324
iteration : 9969
train acc:  0.8828125
train loss:  0.30606794357299805
train gradient:  0.13132937881809376
iteration : 9970
train acc:  0.8046875
train loss:  0.41692817211151123
train gradient:  0.21406543124540672
iteration : 9971
train acc:  0.8671875
train loss:  0.3217979371547699
train gradient:  0.1005718435825164
iteration : 9972
train acc:  0.828125
train loss:  0.3658105134963989
train gradient:  0.15967921278153613
iteration : 9973
train acc:  0.890625
train loss:  0.2964945137500763
train gradient:  0.10800127877960268
iteration : 9974
train acc:  0.90625
train loss:  0.2578361928462982
train gradient:  0.09037138595258039
iteration : 9975
train acc:  0.8359375
train loss:  0.32619693875312805
train gradient:  0.13221645925672854
iteration : 9976
train acc:  0.9375
train loss:  0.21176600456237793
train gradient:  0.07941339522005808
iteration : 9977
train acc:  0.84375
train loss:  0.332046240568161
train gradient:  0.16314585369197704
iteration : 9978
train acc:  0.8203125
train loss:  0.41038018465042114
train gradient:  0.25119994592945366
iteration : 9979
train acc:  0.84375
train loss:  0.3298220634460449
train gradient:  0.15674193016904903
iteration : 9980
train acc:  0.828125
train loss:  0.40945199131965637
train gradient:  0.21482079044750596
iteration : 9981
train acc:  0.8515625
train loss:  0.3534564673900604
train gradient:  0.14458802587666542
iteration : 9982
train acc:  0.8203125
train loss:  0.3176458775997162
train gradient:  0.12723089652175268
iteration : 9983
train acc:  0.84375
train loss:  0.35621386766433716
train gradient:  0.19038834338593139
iteration : 9984
train acc:  0.828125
train loss:  0.40391695499420166
train gradient:  0.19948464929266235
iteration : 9985
train acc:  0.84375
train loss:  0.3511757552623749
train gradient:  0.18615159606298784
iteration : 9986
train acc:  0.875
train loss:  0.279145210981369
train gradient:  0.1360887712006214
iteration : 9987
train acc:  0.8671875
train loss:  0.34234344959259033
train gradient:  0.12904145530853295
iteration : 9988
train acc:  0.84375
train loss:  0.3558224141597748
train gradient:  0.21644038782028857
iteration : 9989
train acc:  0.8515625
train loss:  0.361177533864975
train gradient:  0.24761903962467424
iteration : 9990
train acc:  0.859375
train loss:  0.3434925675392151
train gradient:  0.11011639292766452
iteration : 9991
train acc:  0.8359375
train loss:  0.35246729850769043
train gradient:  0.1391289440993525
iteration : 9992
train acc:  0.8203125
train loss:  0.3612291216850281
train gradient:  0.14185075878891865
iteration : 9993
train acc:  0.859375
train loss:  0.2858140766620636
train gradient:  0.1475744012611977
iteration : 9994
train acc:  0.875
train loss:  0.28271645307540894
train gradient:  0.15830605472715784
iteration : 9995
train acc:  0.8359375
train loss:  0.35552874207496643
train gradient:  0.17659241135629564
iteration : 9996
train acc:  0.796875
train loss:  0.4373645484447479
train gradient:  0.2897857613060412
iteration : 9997
train acc:  0.8984375
train loss:  0.28517261147499084
train gradient:  0.11918495100450302
iteration : 9998
train acc:  0.875
train loss:  0.302312433719635
train gradient:  0.11771493574070298
iteration : 9999
train acc:  0.875
train loss:  0.30653059482574463
train gradient:  0.1462895383700456
iteration : 10000
train acc:  0.84375
train loss:  0.38565221428871155
train gradient:  0.18157133760092847
iteration : 10001
train acc:  0.8515625
train loss:  0.357370525598526
train gradient:  0.19825864386385483
iteration : 10002
train acc:  0.8671875
train loss:  0.36419811844825745
train gradient:  0.15318123649654797
iteration : 10003
train acc:  0.8515625
train loss:  0.3284450173377991
train gradient:  0.17770225493371175
iteration : 10004
train acc:  0.75
train loss:  0.46917665004730225
train gradient:  0.2679803266286226
iteration : 10005
train acc:  0.859375
train loss:  0.28944289684295654
train gradient:  0.11816650158581689
iteration : 10006
train acc:  0.8671875
train loss:  0.3513820171356201
train gradient:  0.1719765690277856
iteration : 10007
train acc:  0.8828125
train loss:  0.27713263034820557
train gradient:  0.11169075377674632
iteration : 10008
train acc:  0.8359375
train loss:  0.3543608784675598
train gradient:  0.1387870395790176
iteration : 10009
train acc:  0.890625
train loss:  0.24673324823379517
train gradient:  0.09714733886537703
iteration : 10010
train acc:  0.8671875
train loss:  0.278734415769577
train gradient:  0.10175958418789591
iteration : 10011
train acc:  0.8359375
train loss:  0.3148958086967468
train gradient:  0.1134471611862911
iteration : 10012
train acc:  0.8828125
train loss:  0.2658136785030365
train gradient:  0.10913867062711077
iteration : 10013
train acc:  0.890625
train loss:  0.2636762261390686
train gradient:  0.1808859043163629
iteration : 10014
train acc:  0.8671875
train loss:  0.2968679964542389
train gradient:  0.10456439808858493
iteration : 10015
train acc:  0.890625
train loss:  0.27752435207366943
train gradient:  0.1116877037715771
iteration : 10016
train acc:  0.84375
train loss:  0.32754191756248474
train gradient:  0.15285105351740216
iteration : 10017
train acc:  0.8984375
train loss:  0.2736949324607849
train gradient:  0.13518250480167793
iteration : 10018
train acc:  0.84375
train loss:  0.342867374420166
train gradient:  0.14230467715283202
iteration : 10019
train acc:  0.859375
train loss:  0.28997793793678284
train gradient:  0.11761814935614776
iteration : 10020
train acc:  0.8046875
train loss:  0.358419269323349
train gradient:  0.17508181711725002
iteration : 10021
train acc:  0.84375
train loss:  0.32914045453071594
train gradient:  0.1678905225772217
iteration : 10022
train acc:  0.859375
train loss:  0.3048013150691986
train gradient:  0.11937554621683046
iteration : 10023
train acc:  0.8984375
train loss:  0.25982925295829773
train gradient:  0.10997380257105689
iteration : 10024
train acc:  0.8671875
train loss:  0.2799302339553833
train gradient:  0.13848657913605264
iteration : 10025
train acc:  0.8203125
train loss:  0.34314462542533875
train gradient:  0.21016900911130748
iteration : 10026
train acc:  0.84375
train loss:  0.3324340581893921
train gradient:  0.15067049195440735
iteration : 10027
train acc:  0.8125
train loss:  0.4262411594390869
train gradient:  0.26122246296824203
iteration : 10028
train acc:  0.859375
train loss:  0.3072662949562073
train gradient:  0.10803645868006603
iteration : 10029
train acc:  0.828125
train loss:  0.33487504720687866
train gradient:  0.1532625131684648
iteration : 10030
train acc:  0.8515625
train loss:  0.2566557824611664
train gradient:  0.12229445802739096
iteration : 10031
train acc:  0.828125
train loss:  0.3992430567741394
train gradient:  0.2043789159940802
iteration : 10032
train acc:  0.84375
train loss:  0.3470020294189453
train gradient:  0.15043082150273185
iteration : 10033
train acc:  0.828125
train loss:  0.3811190128326416
train gradient:  0.19015660231636483
iteration : 10034
train acc:  0.8515625
train loss:  0.30081772804260254
train gradient:  0.1216036081523028
iteration : 10035
train acc:  0.8828125
train loss:  0.2775793671607971
train gradient:  0.09110090331737833
iteration : 10036
train acc:  0.8125
train loss:  0.3671730160713196
train gradient:  0.21628352251355792
iteration : 10037
train acc:  0.890625
train loss:  0.2652934491634369
train gradient:  0.11375625238929428
iteration : 10038
train acc:  0.8671875
train loss:  0.3390901982784271
train gradient:  0.147459974874863
iteration : 10039
train acc:  0.828125
train loss:  0.3435773551464081
train gradient:  0.14135902407136994
iteration : 10040
train acc:  0.859375
train loss:  0.3279191851615906
train gradient:  0.13871241155809025
iteration : 10041
train acc:  0.890625
train loss:  0.2853691577911377
train gradient:  0.1169692275895651
iteration : 10042
train acc:  0.8359375
train loss:  0.3599894046783447
train gradient:  0.19200612416945398
iteration : 10043
train acc:  0.765625
train loss:  0.48771220445632935
train gradient:  0.23893228933215754
iteration : 10044
train acc:  0.859375
train loss:  0.34589433670043945
train gradient:  0.3504238175731952
iteration : 10045
train acc:  0.890625
train loss:  0.2939226031303406
train gradient:  0.2055325868973561
iteration : 10046
train acc:  0.890625
train loss:  0.2699953317642212
train gradient:  0.09471754280381459
iteration : 10047
train acc:  0.90625
train loss:  0.26215916872024536
train gradient:  0.10053176418827359
iteration : 10048
train acc:  0.8671875
train loss:  0.3130418658256531
train gradient:  0.23378253171251198
iteration : 10049
train acc:  0.8515625
train loss:  0.2968716025352478
train gradient:  0.14954055156953508
iteration : 10050
train acc:  0.8359375
train loss:  0.3403211832046509
train gradient:  0.2151218420656233
iteration : 10051
train acc:  0.796875
train loss:  0.40731239318847656
train gradient:  0.278408766827438
iteration : 10052
train acc:  0.8671875
train loss:  0.35415831208229065
train gradient:  0.18016297788997582
iteration : 10053
train acc:  0.84375
train loss:  0.31114867329597473
train gradient:  0.11943607573825733
iteration : 10054
train acc:  0.84375
train loss:  0.3208586275577545
train gradient:  0.15220091738703195
iteration : 10055
train acc:  0.875
train loss:  0.36783337593078613
train gradient:  0.21359485246193205
iteration : 10056
train acc:  0.859375
train loss:  0.3057173490524292
train gradient:  0.1423892024560109
iteration : 10057
train acc:  0.828125
train loss:  0.3821024000644684
train gradient:  0.24181746174659263
iteration : 10058
train acc:  0.90625
train loss:  0.2551726698875427
train gradient:  0.12597227019915008
iteration : 10059
train acc:  0.875
train loss:  0.2790369987487793
train gradient:  0.1468341716216608
iteration : 10060
train acc:  0.8515625
train loss:  0.33201417326927185
train gradient:  0.17879234280087847
iteration : 10061
train acc:  0.8671875
train loss:  0.30178457498550415
train gradient:  0.10838024237470907
iteration : 10062
train acc:  0.875
train loss:  0.2836732268333435
train gradient:  0.12808274313602716
iteration : 10063
train acc:  0.859375
train loss:  0.2605137228965759
train gradient:  0.093308916501569
iteration : 10064
train acc:  0.890625
train loss:  0.2607342004776001
train gradient:  0.1272326612670513
iteration : 10065
train acc:  0.90625
train loss:  0.23753352463245392
train gradient:  0.12373520541880645
iteration : 10066
train acc:  0.8828125
train loss:  0.32078441977500916
train gradient:  0.1382157419179761
iteration : 10067
train acc:  0.859375
train loss:  0.3337445557117462
train gradient:  0.14296745298859542
iteration : 10068
train acc:  0.859375
train loss:  0.2608102560043335
train gradient:  0.1018711415284343
iteration : 10069
train acc:  0.90625
train loss:  0.2701854705810547
train gradient:  0.10411150808068707
iteration : 10070
train acc:  0.8671875
train loss:  0.30336129665374756
train gradient:  0.11288961188891213
iteration : 10071
train acc:  0.875
train loss:  0.31547436118125916
train gradient:  0.16634181111793778
iteration : 10072
train acc:  0.828125
train loss:  0.3131476044654846
train gradient:  0.2221865275992153
iteration : 10073
train acc:  0.859375
train loss:  0.3509211540222168
train gradient:  0.14300859933787222
iteration : 10074
train acc:  0.828125
train loss:  0.39636296033859253
train gradient:  0.3245713646088018
iteration : 10075
train acc:  0.8828125
train loss:  0.38172703981399536
train gradient:  0.15151903585919385
iteration : 10076
train acc:  0.90625
train loss:  0.26171445846557617
train gradient:  0.28084402670568004
iteration : 10077
train acc:  0.9140625
train loss:  0.22405150532722473
train gradient:  0.07966898684339985
iteration : 10078
train acc:  0.8125
train loss:  0.3384765088558197
train gradient:  0.2740483879903409
iteration : 10079
train acc:  0.90625
train loss:  0.23365147411823273
train gradient:  0.09025004713830535
iteration : 10080
train acc:  0.8828125
train loss:  0.301912784576416
train gradient:  0.10781415016235807
iteration : 10081
train acc:  0.9140625
train loss:  0.2335192859172821
train gradient:  0.14202538275016197
iteration : 10082
train acc:  0.8828125
train loss:  0.2772829234600067
train gradient:  0.1312684363576421
iteration : 10083
train acc:  0.8203125
train loss:  0.407571941614151
train gradient:  0.27935380837773327
iteration : 10084
train acc:  0.9296875
train loss:  0.2400541454553604
train gradient:  0.11070412253111678
iteration : 10085
train acc:  0.796875
train loss:  0.4007343053817749
train gradient:  0.17249928340869647
iteration : 10086
train acc:  0.859375
train loss:  0.3686531186103821
train gradient:  0.17524019047025163
iteration : 10087
train acc:  0.8203125
train loss:  0.35208308696746826
train gradient:  0.43867889813780186
iteration : 10088
train acc:  0.921875
train loss:  0.21824173629283905
train gradient:  0.1167382041188997
iteration : 10089
train acc:  0.8671875
train loss:  0.368475079536438
train gradient:  0.20285889349413747
iteration : 10090
train acc:  0.8515625
train loss:  0.36886489391326904
train gradient:  0.25984858297334257
iteration : 10091
train acc:  0.859375
train loss:  0.4004001021385193
train gradient:  0.26375750499339706
iteration : 10092
train acc:  0.7890625
train loss:  0.31528255343437195
train gradient:  0.1313196903735107
iteration : 10093
train acc:  0.8203125
train loss:  0.43494856357574463
train gradient:  0.2490230861143234
iteration : 10094
train acc:  0.8515625
train loss:  0.3027244210243225
train gradient:  0.1841032609685283
iteration : 10095
train acc:  0.8828125
train loss:  0.2692525386810303
train gradient:  0.12140105112736459
iteration : 10096
train acc:  0.8828125
train loss:  0.30771395564079285
train gradient:  0.1646397492209643
iteration : 10097
train acc:  0.90625
train loss:  0.2996772825717926
train gradient:  0.17588909565411542
iteration : 10098
train acc:  0.8671875
train loss:  0.3374822735786438
train gradient:  0.19790970428828136
iteration : 10099
train acc:  0.875
train loss:  0.262567937374115
train gradient:  0.12163483203433068
iteration : 10100
train acc:  0.8359375
train loss:  0.3615416884422302
train gradient:  0.25851915676534615
iteration : 10101
train acc:  0.84375
train loss:  0.3153470754623413
train gradient:  0.23740314420960446
iteration : 10102
train acc:  0.875
train loss:  0.3248293101787567
train gradient:  0.17489936186455676
iteration : 10103
train acc:  0.8671875
train loss:  0.27873629331588745
train gradient:  0.2074229476933256
iteration : 10104
train acc:  0.84375
train loss:  0.3355898857116699
train gradient:  0.14692163443875128
iteration : 10105
train acc:  0.828125
train loss:  0.38442474603652954
train gradient:  0.2510393624092044
iteration : 10106
train acc:  0.8515625
train loss:  0.3115101456642151
train gradient:  0.19174826770852577
iteration : 10107
train acc:  0.84375
train loss:  0.33380377292633057
train gradient:  0.16166704518368802
iteration : 10108
train acc:  0.8359375
train loss:  0.310321182012558
train gradient:  0.13975032810198823
iteration : 10109
train acc:  0.7734375
train loss:  0.4735261797904968
train gradient:  0.3131021525736752
iteration : 10110
train acc:  0.796875
train loss:  0.4475945234298706
train gradient:  0.2606872484223623
iteration : 10111
train acc:  0.890625
train loss:  0.2669909596443176
train gradient:  0.1458007017658765
iteration : 10112
train acc:  0.890625
train loss:  0.27007073163986206
train gradient:  0.14790493549571795
iteration : 10113
train acc:  0.875
train loss:  0.32446765899658203
train gradient:  0.15169438907037466
iteration : 10114
train acc:  0.8046875
train loss:  0.38615870475769043
train gradient:  0.2541781066775587
iteration : 10115
train acc:  0.8984375
train loss:  0.28082218766212463
train gradient:  0.15001128572433198
iteration : 10116
train acc:  0.84375
train loss:  0.3528854548931122
train gradient:  0.25377786283912535
iteration : 10117
train acc:  0.8515625
train loss:  0.3357611894607544
train gradient:  0.20322919566815295
iteration : 10118
train acc:  0.859375
train loss:  0.3228559195995331
train gradient:  0.11585220434514534
iteration : 10119
train acc:  0.8359375
train loss:  0.3634541928768158
train gradient:  0.17944215214628817
iteration : 10120
train acc:  0.859375
train loss:  0.4025173783302307
train gradient:  0.21630386126770718
iteration : 10121
train acc:  0.8359375
train loss:  0.3780958652496338
train gradient:  0.4305571596348791
iteration : 10122
train acc:  0.8125
train loss:  0.38776522874832153
train gradient:  0.1790928133315739
iteration : 10123
train acc:  0.8984375
train loss:  0.2527979016304016
train gradient:  0.14976993955821707
iteration : 10124
train acc:  0.84375
train loss:  0.3343263864517212
train gradient:  0.21445557841346127
iteration : 10125
train acc:  0.875
train loss:  0.2800542116165161
train gradient:  0.18108947434954367
iteration : 10126
train acc:  0.875
train loss:  0.3096800446510315
train gradient:  0.1781089819387016
iteration : 10127
train acc:  0.796875
train loss:  0.399074912071228
train gradient:  0.20526455873095878
iteration : 10128
train acc:  0.84375
train loss:  0.3418208360671997
train gradient:  0.19164101940360712
iteration : 10129
train acc:  0.8828125
train loss:  0.27687564492225647
train gradient:  0.08362837049037344
iteration : 10130
train acc:  0.84375
train loss:  0.39191481471061707
train gradient:  0.18926865538824267
iteration : 10131
train acc:  0.8203125
train loss:  0.4069051742553711
train gradient:  0.19527160183647488
iteration : 10132
train acc:  0.8515625
train loss:  0.2948531210422516
train gradient:  0.13745836234164496
iteration : 10133
train acc:  0.828125
train loss:  0.3418448567390442
train gradient:  0.11016551738891123
iteration : 10134
train acc:  0.8671875
train loss:  0.30376288294792175
train gradient:  0.1352608000324233
iteration : 10135
train acc:  0.890625
train loss:  0.2662278413772583
train gradient:  0.12568992431009646
iteration : 10136
train acc:  0.8828125
train loss:  0.30429181456565857
train gradient:  0.13014317412666077
iteration : 10137
train acc:  0.8359375
train loss:  0.3968632221221924
train gradient:  0.20870869371186518
iteration : 10138
train acc:  0.8828125
train loss:  0.32759714126586914
train gradient:  0.16647365454119925
iteration : 10139
train acc:  0.828125
train loss:  0.377722829580307
train gradient:  0.22925034871609257
iteration : 10140
train acc:  0.8203125
train loss:  0.3500155210494995
train gradient:  0.1850622941266502
iteration : 10141
train acc:  0.8046875
train loss:  0.482339471578598
train gradient:  0.3090103743308814
iteration : 10142
train acc:  0.8359375
train loss:  0.35657867789268494
train gradient:  0.17427230370039737
iteration : 10143
train acc:  0.8125
train loss:  0.4120974540710449
train gradient:  0.22882698316012162
iteration : 10144
train acc:  0.8203125
train loss:  0.35091084241867065
train gradient:  0.15153156441871918
iteration : 10145
train acc:  0.84375
train loss:  0.3330196142196655
train gradient:  0.16884097584895083
iteration : 10146
train acc:  0.875
train loss:  0.30077338218688965
train gradient:  0.1419745168507258
iteration : 10147
train acc:  0.8671875
train loss:  0.26706984639167786
train gradient:  0.10025719729607313
iteration : 10148
train acc:  0.8359375
train loss:  0.3439024090766907
train gradient:  0.24182513516717272
iteration : 10149
train acc:  0.890625
train loss:  0.33180123567581177
train gradient:  0.1704250356046367
iteration : 10150
train acc:  0.8984375
train loss:  0.26508277654647827
train gradient:  0.08344781840815058
iteration : 10151
train acc:  0.8828125
train loss:  0.2775507867336273
train gradient:  0.10824540137057535
iteration : 10152
train acc:  0.8828125
train loss:  0.34663838148117065
train gradient:  0.1520008963877027
iteration : 10153
train acc:  0.84375
train loss:  0.2835383415222168
train gradient:  0.1581586515059757
iteration : 10154
train acc:  0.90625
train loss:  0.3093525767326355
train gradient:  0.15291774874784528
iteration : 10155
train acc:  0.8984375
train loss:  0.2350362241268158
train gradient:  0.08547711341923535
iteration : 10156
train acc:  0.8203125
train loss:  0.40006041526794434
train gradient:  0.35215881111403935
iteration : 10157
train acc:  0.875
train loss:  0.28056979179382324
train gradient:  0.1360940386043598
iteration : 10158
train acc:  0.8828125
train loss:  0.3145485520362854
train gradient:  0.119369874145403
iteration : 10159
train acc:  0.859375
train loss:  0.35335561633110046
train gradient:  0.19414213094779248
iteration : 10160
train acc:  0.8515625
train loss:  0.30759283900260925
train gradient:  0.1436506783697238
iteration : 10161
train acc:  0.8828125
train loss:  0.2740771472454071
train gradient:  0.08611151774068784
iteration : 10162
train acc:  0.828125
train loss:  0.3671553134918213
train gradient:  0.18923531543244101
iteration : 10163
train acc:  0.875
train loss:  0.2837706208229065
train gradient:  0.12988660444178785
iteration : 10164
train acc:  0.8515625
train loss:  0.3359718322753906
train gradient:  0.16383169900204564
iteration : 10165
train acc:  0.7890625
train loss:  0.42852872610092163
train gradient:  0.21439360585647121
iteration : 10166
train acc:  0.8984375
train loss:  0.27876055240631104
train gradient:  0.18899374752299852
iteration : 10167
train acc:  0.828125
train loss:  0.4042396545410156
train gradient:  0.2320339709574482
iteration : 10168
train acc:  0.8515625
train loss:  0.37762483954429626
train gradient:  0.26175180168214784
iteration : 10169
train acc:  0.8125
train loss:  0.42593932151794434
train gradient:  0.41192965821586053
iteration : 10170
train acc:  0.890625
train loss:  0.26014912128448486
train gradient:  0.07997458258367733
iteration : 10171
train acc:  0.828125
train loss:  0.4122840464115143
train gradient:  0.20774927314715194
iteration : 10172
train acc:  0.84375
train loss:  0.3881252110004425
train gradient:  0.3001174936782437
iteration : 10173
train acc:  0.890625
train loss:  0.2981235086917877
train gradient:  0.18823923884831167
iteration : 10174
train acc:  0.828125
train loss:  0.36645299196243286
train gradient:  0.15971873793069277
iteration : 10175
train acc:  0.84375
train loss:  0.3697476089000702
train gradient:  0.1290375391223539
iteration : 10176
train acc:  0.8515625
train loss:  0.4026232957839966
train gradient:  0.22908113805454233
iteration : 10177
train acc:  0.84375
train loss:  0.31947892904281616
train gradient:  0.1856429007809988
iteration : 10178
train acc:  0.8125
train loss:  0.42830774188041687
train gradient:  0.17372361983631826
iteration : 10179
train acc:  0.8828125
train loss:  0.2824360132217407
train gradient:  0.1045131848593763
iteration : 10180
train acc:  0.90625
train loss:  0.2546507716178894
train gradient:  0.10426656201221286
iteration : 10181
train acc:  0.8046875
train loss:  0.433186411857605
train gradient:  0.22282217396763046
iteration : 10182
train acc:  0.828125
train loss:  0.3215794563293457
train gradient:  0.12167013131172319
iteration : 10183
train acc:  0.8515625
train loss:  0.35929009318351746
train gradient:  0.16663260071903221
iteration : 10184
train acc:  0.890625
train loss:  0.23363110423088074
train gradient:  0.09960311594319413
iteration : 10185
train acc:  0.8828125
train loss:  0.3114786148071289
train gradient:  0.11625744487600247
iteration : 10186
train acc:  0.8984375
train loss:  0.2714628577232361
train gradient:  0.10139129629690598
iteration : 10187
train acc:  0.8125
train loss:  0.4219500422477722
train gradient:  0.2082675518083027
iteration : 10188
train acc:  0.8046875
train loss:  0.36599600315093994
train gradient:  0.1761695554693274
iteration : 10189
train acc:  0.859375
train loss:  0.3279750347137451
train gradient:  0.16396290016918844
iteration : 10190
train acc:  0.8203125
train loss:  0.36273694038391113
train gradient:  0.1914753570882684
iteration : 10191
train acc:  0.8515625
train loss:  0.30754518508911133
train gradient:  0.1923750217928109
iteration : 10192
train acc:  0.8828125
train loss:  0.31132709980010986
train gradient:  0.1160760019390978
iteration : 10193
train acc:  0.8984375
train loss:  0.289956271648407
train gradient:  0.1437018188177056
iteration : 10194
train acc:  0.7890625
train loss:  0.4074997305870056
train gradient:  0.20124216816366453
iteration : 10195
train acc:  0.875
train loss:  0.3325479030609131
train gradient:  0.14554094013902724
iteration : 10196
train acc:  0.9140625
train loss:  0.25975728034973145
train gradient:  0.08413932584834581
iteration : 10197
train acc:  0.8359375
train loss:  0.35483700037002563
train gradient:  0.1635609928564117
iteration : 10198
train acc:  0.8046875
train loss:  0.40793576836586
train gradient:  0.1663467010637029
iteration : 10199
train acc:  0.875
train loss:  0.311917781829834
train gradient:  0.12122556980730748
iteration : 10200
train acc:  0.8671875
train loss:  0.32544219493865967
train gradient:  0.18865391498877696
iteration : 10201
train acc:  0.890625
train loss:  0.3448070287704468
train gradient:  0.24768807997848263
iteration : 10202
train acc:  0.8828125
train loss:  0.3112216591835022
train gradient:  0.1934697375865515
iteration : 10203
train acc:  0.859375
train loss:  0.3228301703929901
train gradient:  0.14964260090239082
iteration : 10204
train acc:  0.859375
train loss:  0.28573477268218994
train gradient:  0.13064642898342108
iteration : 10205
train acc:  0.8515625
train loss:  0.2976278066635132
train gradient:  0.197004479422209
iteration : 10206
train acc:  0.8515625
train loss:  0.36123716831207275
train gradient:  0.1491794616273372
iteration : 10207
train acc:  0.84375
train loss:  0.3813062906265259
train gradient:  0.15404389208174485
iteration : 10208
train acc:  0.875
train loss:  0.276157021522522
train gradient:  0.1109507190384798
iteration : 10209
train acc:  0.8125
train loss:  0.4145461916923523
train gradient:  0.18996187169634543
iteration : 10210
train acc:  0.890625
train loss:  0.2714431881904602
train gradient:  0.07192080889678582
iteration : 10211
train acc:  0.8515625
train loss:  0.29434633255004883
train gradient:  0.12718230898127925
iteration : 10212
train acc:  0.875
train loss:  0.33812761306762695
train gradient:  0.13678298153350416
iteration : 10213
train acc:  0.875
train loss:  0.2980208694934845
train gradient:  0.09227992885773245
iteration : 10214
train acc:  0.828125
train loss:  0.3724879324436188
train gradient:  0.1712385637112758
iteration : 10215
train acc:  0.875
train loss:  0.30559924244880676
train gradient:  0.11485828618407118
iteration : 10216
train acc:  0.8515625
train loss:  0.32379624247550964
train gradient:  0.15584074719298296
iteration : 10217
train acc:  0.8671875
train loss:  0.3504427671432495
train gradient:  0.16866625485346082
iteration : 10218
train acc:  0.8671875
train loss:  0.34834831953048706
train gradient:  0.1792469112234143
iteration : 10219
train acc:  0.7890625
train loss:  0.4208001494407654
train gradient:  0.30778441599458145
iteration : 10220
train acc:  0.859375
train loss:  0.3192356824874878
train gradient:  0.13903295392737058
iteration : 10221
train acc:  0.8671875
train loss:  0.3022199869155884
train gradient:  0.16960155649082215
iteration : 10222
train acc:  0.9375
train loss:  0.190697580575943
train gradient:  0.05900165031067089
iteration : 10223
train acc:  0.859375
train loss:  0.2827978730201721
train gradient:  0.16328649717465282
iteration : 10224
train acc:  0.859375
train loss:  0.30696338415145874
train gradient:  0.13371550547570996
iteration : 10225
train acc:  0.84375
train loss:  0.37310144305229187
train gradient:  0.14026978665339274
iteration : 10226
train acc:  0.8359375
train loss:  0.375852108001709
train gradient:  0.1875900312430503
iteration : 10227
train acc:  0.8515625
train loss:  0.3933873474597931
train gradient:  0.2802589589198581
iteration : 10228
train acc:  0.875
train loss:  0.2836579382419586
train gradient:  0.13972793254228882
iteration : 10229
train acc:  0.859375
train loss:  0.2939360737800598
train gradient:  0.1256852968713167
iteration : 10230
train acc:  0.796875
train loss:  0.3832266926765442
train gradient:  0.2425746294092052
iteration : 10231
train acc:  0.84375
train loss:  0.3125728666782379
train gradient:  0.11767461880497525
iteration : 10232
train acc:  0.8828125
train loss:  0.3234424591064453
train gradient:  0.17470280629384222
iteration : 10233
train acc:  0.8828125
train loss:  0.2939874529838562
train gradient:  0.15912822252249414
iteration : 10234
train acc:  0.890625
train loss:  0.25292205810546875
train gradient:  0.09179431824768643
iteration : 10235
train acc:  0.8515625
train loss:  0.3082866668701172
train gradient:  0.15784607047260676
iteration : 10236
train acc:  0.8046875
train loss:  0.3967873156070709
train gradient:  0.2983087402393473
iteration : 10237
train acc:  0.828125
train loss:  0.3905511796474457
train gradient:  0.23686165951904598
iteration : 10238
train acc:  0.8515625
train loss:  0.31830450892448425
train gradient:  0.16506999985265147
iteration : 10239
train acc:  0.8203125
train loss:  0.3655710816383362
train gradient:  0.17410182895289927
iteration : 10240
train acc:  0.8984375
train loss:  0.22638870775699615
train gradient:  0.0806419296593995
iteration : 10241
train acc:  0.8671875
train loss:  0.27023279666900635
train gradient:  0.13646202526882734
iteration : 10242
train acc:  0.8515625
train loss:  0.29492732882499695
train gradient:  0.1357889751310601
iteration : 10243
train acc:  0.8671875
train loss:  0.3417035639286041
train gradient:  0.22095656179196
iteration : 10244
train acc:  0.875
train loss:  0.33815646171569824
train gradient:  0.19208180980579803
iteration : 10245
train acc:  0.859375
train loss:  0.24112612009048462
train gradient:  0.06532251097256744
iteration : 10246
train acc:  0.8359375
train loss:  0.42254626750946045
train gradient:  0.2765473643521627
iteration : 10247
train acc:  0.8046875
train loss:  0.41186338663101196
train gradient:  0.2115808937289704
iteration : 10248
train acc:  0.921875
train loss:  0.22279982268810272
train gradient:  0.10165501090165345
iteration : 10249
train acc:  0.8515625
train loss:  0.3285551965236664
train gradient:  0.23023600832439
iteration : 10250
train acc:  0.8359375
train loss:  0.3476346731185913
train gradient:  0.1380098768922735
iteration : 10251
train acc:  0.8515625
train loss:  0.320592999458313
train gradient:  0.18076398528995596
iteration : 10252
train acc:  0.859375
train loss:  0.31355196237564087
train gradient:  0.17951145635410776
iteration : 10253
train acc:  0.8515625
train loss:  0.3064531683921814
train gradient:  0.1341511574295059
iteration : 10254
train acc:  0.8515625
train loss:  0.3491664230823517
train gradient:  0.22726047664032967
iteration : 10255
train acc:  0.84375
train loss:  0.41377145051956177
train gradient:  0.19157283958530164
iteration : 10256
train acc:  0.8671875
train loss:  0.30144694447517395
train gradient:  0.1227300389284028
iteration : 10257
train acc:  0.8984375
train loss:  0.3186878561973572
train gradient:  0.17244164653541566
iteration : 10258
train acc:  0.8515625
train loss:  0.3590330183506012
train gradient:  0.17585796070753507
iteration : 10259
train acc:  0.859375
train loss:  0.3485887944698334
train gradient:  0.1470918213085845
iteration : 10260
train acc:  0.8828125
train loss:  0.2774246335029602
train gradient:  0.157055425848045
iteration : 10261
train acc:  0.84375
train loss:  0.3275250792503357
train gradient:  0.15881532662434478
iteration : 10262
train acc:  0.8671875
train loss:  0.28621193766593933
train gradient:  0.18509804483422362
iteration : 10263
train acc:  0.796875
train loss:  0.42531949281692505
train gradient:  0.18330899620672458
iteration : 10264
train acc:  0.84375
train loss:  0.3339540660381317
train gradient:  0.14305007390855512
iteration : 10265
train acc:  0.8203125
train loss:  0.3212878108024597
train gradient:  0.14145658274179887
iteration : 10266
train acc:  0.8125
train loss:  0.4130554795265198
train gradient:  0.2998870710347228
iteration : 10267
train acc:  0.8828125
train loss:  0.34719693660736084
train gradient:  0.207602222682436
iteration : 10268
train acc:  0.8125
train loss:  0.3785511255264282
train gradient:  0.19426341720291135
iteration : 10269
train acc:  0.8671875
train loss:  0.28302979469299316
train gradient:  0.0992935243982338
iteration : 10270
train acc:  0.8671875
train loss:  0.3784579336643219
train gradient:  0.18424854655422687
iteration : 10271
train acc:  0.8203125
train loss:  0.3170105516910553
train gradient:  0.17086785864085846
iteration : 10272
train acc:  0.90625
train loss:  0.23810997605323792
train gradient:  0.09303078055140569
iteration : 10273
train acc:  0.890625
train loss:  0.3062278628349304
train gradient:  0.14723128789493845
iteration : 10274
train acc:  0.90625
train loss:  0.2737353444099426
train gradient:  0.11006373485041741
iteration : 10275
train acc:  0.78125
train loss:  0.4520907998085022
train gradient:  0.25424272705862044
iteration : 10276
train acc:  0.8671875
train loss:  0.2948320508003235
train gradient:  0.15635380658896747
iteration : 10277
train acc:  0.8671875
train loss:  0.3141040503978729
train gradient:  0.13348575614774866
iteration : 10278
train acc:  0.8828125
train loss:  0.3226207494735718
train gradient:  0.1637555318644705
iteration : 10279
train acc:  0.8828125
train loss:  0.32545581459999084
train gradient:  0.20400488372088316
iteration : 10280
train acc:  0.8359375
train loss:  0.3821727931499481
train gradient:  0.20857061196041604
iteration : 10281
train acc:  0.8828125
train loss:  0.28366485238075256
train gradient:  0.12353848423299288
iteration : 10282
train acc:  0.8828125
train loss:  0.29370731115341187
train gradient:  0.17040193597711523
iteration : 10283
train acc:  0.859375
train loss:  0.3502292037010193
train gradient:  0.2031120106511796
iteration : 10284
train acc:  0.8203125
train loss:  0.3873940706253052
train gradient:  0.19745689566353064
iteration : 10285
train acc:  0.8984375
train loss:  0.31312334537506104
train gradient:  0.23269445080613274
iteration : 10286
train acc:  0.8984375
train loss:  0.2715756893157959
train gradient:  0.15134062324534323
iteration : 10287
train acc:  0.875
train loss:  0.3483228087425232
train gradient:  0.19422602360046487
iteration : 10288
train acc:  0.84375
train loss:  0.32280540466308594
train gradient:  0.12655649057083967
iteration : 10289
train acc:  0.875
train loss:  0.3081930875778198
train gradient:  0.14248529047929454
iteration : 10290
train acc:  0.90625
train loss:  0.2757994532585144
train gradient:  0.2574311611419955
iteration : 10291
train acc:  0.8359375
train loss:  0.3311704993247986
train gradient:  0.1477357083220377
iteration : 10292
train acc:  0.875
train loss:  0.2829347252845764
train gradient:  0.12703726424140957
iteration : 10293
train acc:  0.890625
train loss:  0.2629948854446411
train gradient:  0.08973754373343765
iteration : 10294
train acc:  0.890625
train loss:  0.33865222334861755
train gradient:  0.2088303535899686
iteration : 10295
train acc:  0.859375
train loss:  0.2773662507534027
train gradient:  0.10443601550663814
iteration : 10296
train acc:  0.8203125
train loss:  0.406075656414032
train gradient:  0.25846114593241465
iteration : 10297
train acc:  0.8359375
train loss:  0.4003884792327881
train gradient:  0.1678863960756078
iteration : 10298
train acc:  0.890625
train loss:  0.28668761253356934
train gradient:  0.11243099029791663
iteration : 10299
train acc:  0.8359375
train loss:  0.39698317646980286
train gradient:  0.18854662315966078
iteration : 10300
train acc:  0.8515625
train loss:  0.3843315839767456
train gradient:  0.2554807071584278
iteration : 10301
train acc:  0.875
train loss:  0.26801639795303345
train gradient:  0.13494861683292847
iteration : 10302
train acc:  0.828125
train loss:  0.35545238852500916
train gradient:  0.13562199580206347
iteration : 10303
train acc:  0.8671875
train loss:  0.30935072898864746
train gradient:  0.15293890196211907
iteration : 10304
train acc:  0.8671875
train loss:  0.26246967911720276
train gradient:  0.13081223368998662
iteration : 10305
train acc:  0.8671875
train loss:  0.2950216233730316
train gradient:  0.13858838123822495
iteration : 10306
train acc:  0.8046875
train loss:  0.4194478392601013
train gradient:  0.1957232984662488
iteration : 10307
train acc:  0.875
train loss:  0.305931031703949
train gradient:  0.1605858386713993
iteration : 10308
train acc:  0.8125
train loss:  0.36018770933151245
train gradient:  0.23118946184009614
iteration : 10309
train acc:  0.890625
train loss:  0.29649239778518677
train gradient:  0.1657729871799309
iteration : 10310
train acc:  0.84375
train loss:  0.3385506272315979
train gradient:  0.18692852309487748
iteration : 10311
train acc:  0.8828125
train loss:  0.27292460203170776
train gradient:  0.1418475846534853
iteration : 10312
train acc:  0.8671875
train loss:  0.36993396282196045
train gradient:  0.1500235700356197
iteration : 10313
train acc:  0.859375
train loss:  0.30486398935317993
train gradient:  0.11232269895763276
iteration : 10314
train acc:  0.8671875
train loss:  0.31701189279556274
train gradient:  0.15049008713035855
iteration : 10315
train acc:  0.890625
train loss:  0.2768641710281372
train gradient:  0.14769667928809493
iteration : 10316
train acc:  0.8828125
train loss:  0.32959091663360596
train gradient:  0.19911765922732072
iteration : 10317
train acc:  0.875
train loss:  0.2995215058326721
train gradient:  0.141937436765938
iteration : 10318
train acc:  0.875
train loss:  0.3171968162059784
train gradient:  0.18242596519234167
iteration : 10319
train acc:  0.8359375
train loss:  0.4264790713787079
train gradient:  0.2919210879704184
iteration : 10320
train acc:  0.859375
train loss:  0.3700726628303528
train gradient:  0.169733434833441
iteration : 10321
train acc:  0.90625
train loss:  0.29029369354248047
train gradient:  0.10338737020916561
iteration : 10322
train acc:  0.859375
train loss:  0.29299020767211914
train gradient:  0.1641437852649518
iteration : 10323
train acc:  0.828125
train loss:  0.3375391364097595
train gradient:  0.18133531750220178
iteration : 10324
train acc:  0.8515625
train loss:  0.32110151648521423
train gradient:  0.1722754382946744
iteration : 10325
train acc:  0.828125
train loss:  0.37874841690063477
train gradient:  0.17482128401555108
iteration : 10326
train acc:  0.90625
train loss:  0.24640627205371857
train gradient:  0.12997148225692007
iteration : 10327
train acc:  0.8828125
train loss:  0.27873608469963074
train gradient:  0.1566048796976453
iteration : 10328
train acc:  0.828125
train loss:  0.4019981622695923
train gradient:  0.31179760897709574
iteration : 10329
train acc:  0.8984375
train loss:  0.32642805576324463
train gradient:  0.142897801472821
iteration : 10330
train acc:  0.8671875
train loss:  0.30053094029426575
train gradient:  0.1411960060673181
iteration : 10331
train acc:  0.78125
train loss:  0.39648836851119995
train gradient:  0.24476850483288728
iteration : 10332
train acc:  0.7890625
train loss:  0.39104658365249634
train gradient:  0.16947331905580137
iteration : 10333
train acc:  0.859375
train loss:  0.3914678692817688
train gradient:  0.19744127545622697
iteration : 10334
train acc:  0.875
train loss:  0.24467608332633972
train gradient:  0.09687897096370883
iteration : 10335
train acc:  0.890625
train loss:  0.2685554027557373
train gradient:  0.08343251384803961
iteration : 10336
train acc:  0.8671875
train loss:  0.3169315457344055
train gradient:  0.12489500394402475
iteration : 10337
train acc:  0.859375
train loss:  0.37661290168762207
train gradient:  0.20811918116698236
iteration : 10338
train acc:  0.890625
train loss:  0.2570715546607971
train gradient:  0.14508821407602912
iteration : 10339
train acc:  0.8046875
train loss:  0.4636088013648987
train gradient:  0.29645577733682416
iteration : 10340
train acc:  0.8984375
train loss:  0.2919434905052185
train gradient:  0.1272471441264262
iteration : 10341
train acc:  0.8046875
train loss:  0.3365456759929657
train gradient:  0.13289541730202564
iteration : 10342
train acc:  0.78125
train loss:  0.4648776054382324
train gradient:  0.27955038660144965
iteration : 10343
train acc:  0.9140625
train loss:  0.2625717520713806
train gradient:  0.11117429745882905
iteration : 10344
train acc:  0.8515625
train loss:  0.3366524875164032
train gradient:  0.15363290933311374
iteration : 10345
train acc:  0.890625
train loss:  0.29517677426338196
train gradient:  0.14598706458423166
iteration : 10346
train acc:  0.8671875
train loss:  0.38195574283599854
train gradient:  0.18519962601511344
iteration : 10347
train acc:  0.875
train loss:  0.27213340997695923
train gradient:  0.12580584698640748
iteration : 10348
train acc:  0.8515625
train loss:  0.4120418429374695
train gradient:  0.23019158902475417
iteration : 10349
train acc:  0.8671875
train loss:  0.30561745166778564
train gradient:  0.1364628258238818
iteration : 10350
train acc:  0.8828125
train loss:  0.3052765130996704
train gradient:  0.20931873353418867
iteration : 10351
train acc:  0.8984375
train loss:  0.28476738929748535
train gradient:  0.1182502514204902
iteration : 10352
train acc:  0.828125
train loss:  0.3586159348487854
train gradient:  0.19177397928335826
iteration : 10353
train acc:  0.9453125
train loss:  0.22946973145008087
train gradient:  0.11956243582249652
iteration : 10354
train acc:  0.8984375
train loss:  0.30747634172439575
train gradient:  0.15086622322226462
iteration : 10355
train acc:  0.921875
train loss:  0.282279372215271
train gradient:  0.10076074275496981
iteration : 10356
train acc:  0.8125
train loss:  0.4253383278846741
train gradient:  0.23360971814712372
iteration : 10357
train acc:  0.8515625
train loss:  0.3127481937408447
train gradient:  0.15944931661897993
iteration : 10358
train acc:  0.8515625
train loss:  0.31548836827278137
train gradient:  0.14735055665265104
iteration : 10359
train acc:  0.8515625
train loss:  0.32577818632125854
train gradient:  0.15783524324455125
iteration : 10360
train acc:  0.9140625
train loss:  0.281566321849823
train gradient:  0.12606618304825043
iteration : 10361
train acc:  0.84375
train loss:  0.3443714380264282
train gradient:  0.14569138569553225
iteration : 10362
train acc:  0.8984375
train loss:  0.2924804985523224
train gradient:  0.11933654638493027
iteration : 10363
train acc:  0.8828125
train loss:  0.2838766276836395
train gradient:  0.1264296412820565
iteration : 10364
train acc:  0.8359375
train loss:  0.38775524497032166
train gradient:  0.22440626293823315
iteration : 10365
train acc:  0.8828125
train loss:  0.28251686692237854
train gradient:  0.18062355644981906
iteration : 10366
train acc:  0.9140625
train loss:  0.2523350119590759
train gradient:  0.09666143592843264
iteration : 10367
train acc:  0.8203125
train loss:  0.49895375967025757
train gradient:  0.3140697549533399
iteration : 10368
train acc:  0.828125
train loss:  0.3714260458946228
train gradient:  0.17833131741992667
iteration : 10369
train acc:  0.859375
train loss:  0.31922221183776855
train gradient:  0.14292467978532875
iteration : 10370
train acc:  0.859375
train loss:  0.319805383682251
train gradient:  0.20018958377410156
iteration : 10371
train acc:  0.8359375
train loss:  0.4519366919994354
train gradient:  0.2599378521557321
iteration : 10372
train acc:  0.890625
train loss:  0.3231143057346344
train gradient:  0.14318751378345201
iteration : 10373
train acc:  0.875
train loss:  0.2772391438484192
train gradient:  0.13729867646813382
iteration : 10374
train acc:  0.8515625
train loss:  0.35464876890182495
train gradient:  0.15580676683165756
iteration : 10375
train acc:  0.8515625
train loss:  0.2920896112918854
train gradient:  0.125680378550725
iteration : 10376
train acc:  0.875
train loss:  0.3222181797027588
train gradient:  0.1364403391144747
iteration : 10377
train acc:  0.8671875
train loss:  0.32919421792030334
train gradient:  0.13005738307156275
iteration : 10378
train acc:  0.8046875
train loss:  0.42297473549842834
train gradient:  0.25636119568396293
iteration : 10379
train acc:  0.8125
train loss:  0.37874412536621094
train gradient:  0.2283921233917338
iteration : 10380
train acc:  0.8359375
train loss:  0.43955594301223755
train gradient:  0.28456262422678047
iteration : 10381
train acc:  0.8671875
train loss:  0.31089651584625244
train gradient:  0.14764381411356292
iteration : 10382
train acc:  0.890625
train loss:  0.23385091125965118
train gradient:  0.12394131238154639
iteration : 10383
train acc:  0.8515625
train loss:  0.31805720925331116
train gradient:  0.13347283701650364
iteration : 10384
train acc:  0.875
train loss:  0.34210947155952454
train gradient:  0.14329293305324287
iteration : 10385
train acc:  0.859375
train loss:  0.39581233263015747
train gradient:  0.19161779701056283
iteration : 10386
train acc:  0.8203125
train loss:  0.373124361038208
train gradient:  0.20360060195091487
iteration : 10387
train acc:  0.8515625
train loss:  0.32984182238578796
train gradient:  0.17800576315281894
iteration : 10388
train acc:  0.859375
train loss:  0.3086695969104767
train gradient:  0.17499895360110151
iteration : 10389
train acc:  0.8984375
train loss:  0.320721298456192
train gradient:  0.18486089519884608
iteration : 10390
train acc:  0.9140625
train loss:  0.2657821774482727
train gradient:  0.127363603554138
iteration : 10391
train acc:  0.859375
train loss:  0.2938708961009979
train gradient:  0.21215470223056337
iteration : 10392
train acc:  0.7734375
train loss:  0.3602433502674103
train gradient:  0.17519739662267997
iteration : 10393
train acc:  0.796875
train loss:  0.37433916330337524
train gradient:  0.2194396482723997
iteration : 10394
train acc:  0.796875
train loss:  0.338604211807251
train gradient:  0.15712775082359282
iteration : 10395
train acc:  0.8828125
train loss:  0.2861616015434265
train gradient:  0.10844875406340236
iteration : 10396
train acc:  0.9375
train loss:  0.22879216074943542
train gradient:  0.08126039538007587
iteration : 10397
train acc:  0.8515625
train loss:  0.3298516273498535
train gradient:  0.13448383405576553
iteration : 10398
train acc:  0.8828125
train loss:  0.29795369505882263
train gradient:  0.12117870969073705
iteration : 10399
train acc:  0.90625
train loss:  0.2532566785812378
train gradient:  0.07504897480713738
iteration : 10400
train acc:  0.8828125
train loss:  0.31109967827796936
train gradient:  0.0835891171951796
iteration : 10401
train acc:  0.828125
train loss:  0.3182705342769623
train gradient:  0.16948271703617385
iteration : 10402
train acc:  0.875
train loss:  0.3443671464920044
train gradient:  0.1500108442509163
iteration : 10403
train acc:  0.828125
train loss:  0.35360032320022583
train gradient:  0.20727000332233236
iteration : 10404
train acc:  0.8984375
train loss:  0.2756075859069824
train gradient:  0.10509968823926533
iteration : 10405
train acc:  0.828125
train loss:  0.36870715022087097
train gradient:  0.223496372357028
iteration : 10406
train acc:  0.84375
train loss:  0.31396758556365967
train gradient:  0.1625773446509985
iteration : 10407
train acc:  0.859375
train loss:  0.30566519498825073
train gradient:  0.13809883343778748
iteration : 10408
train acc:  0.9296875
train loss:  0.23684251308441162
train gradient:  0.08413792154001849
iteration : 10409
train acc:  0.8671875
train loss:  0.29851868748664856
train gradient:  0.15219192652401844
iteration : 10410
train acc:  0.90625
train loss:  0.22882261872291565
train gradient:  0.07947406503039615
iteration : 10411
train acc:  0.8671875
train loss:  0.3648003041744232
train gradient:  0.15815343692778888
iteration : 10412
train acc:  0.8046875
train loss:  0.38091498613357544
train gradient:  0.3061055102924927
iteration : 10413
train acc:  0.8671875
train loss:  0.30373069643974304
train gradient:  0.15051206662171118
iteration : 10414
train acc:  0.890625
train loss:  0.2698601186275482
train gradient:  0.11536674740575899
iteration : 10415
train acc:  0.8828125
train loss:  0.3040396273136139
train gradient:  0.1315397520952771
iteration : 10416
train acc:  0.8125
train loss:  0.43235546350479126
train gradient:  0.1997404447430559
iteration : 10417
train acc:  0.84375
train loss:  0.3277007043361664
train gradient:  0.2010146805723826
iteration : 10418
train acc:  0.84375
train loss:  0.35605746507644653
train gradient:  0.18299407931824788
iteration : 10419
train acc:  0.859375
train loss:  0.3071754574775696
train gradient:  0.12376434773647418
iteration : 10420
train acc:  0.875
train loss:  0.32032114267349243
train gradient:  0.17832543692802028
iteration : 10421
train acc:  0.765625
train loss:  0.4360641837120056
train gradient:  0.2584082808676392
iteration : 10422
train acc:  0.8828125
train loss:  0.25164949893951416
train gradient:  0.10410410702209823
iteration : 10423
train acc:  0.8671875
train loss:  0.3851294219493866
train gradient:  0.1809269187256976
iteration : 10424
train acc:  0.859375
train loss:  0.3223566710948944
train gradient:  0.18931429048146542
iteration : 10425
train acc:  0.8984375
train loss:  0.27163970470428467
train gradient:  0.09311450829656144
iteration : 10426
train acc:  0.859375
train loss:  0.29481327533721924
train gradient:  0.14855467908468106
iteration : 10427
train acc:  0.90625
train loss:  0.2301708161830902
train gradient:  0.07711648731114519
iteration : 10428
train acc:  0.84375
train loss:  0.36717498302459717
train gradient:  0.15822291191077303
iteration : 10429
train acc:  0.8515625
train loss:  0.3435918688774109
train gradient:  0.21259697622307341
iteration : 10430
train acc:  0.875
train loss:  0.29920050501823425
train gradient:  0.12343524209777759
iteration : 10431
train acc:  0.9140625
train loss:  0.23320621252059937
train gradient:  0.07953572850879426
iteration : 10432
train acc:  0.875
train loss:  0.26731929183006287
train gradient:  0.12884762780976688
iteration : 10433
train acc:  0.8125
train loss:  0.37483447790145874
train gradient:  0.1606665954314317
iteration : 10434
train acc:  0.859375
train loss:  0.3567555546760559
train gradient:  0.15017608479578737
iteration : 10435
train acc:  0.8359375
train loss:  0.34767019748687744
train gradient:  0.2278861712800063
iteration : 10436
train acc:  0.84375
train loss:  0.31640374660491943
train gradient:  0.14880713019535408
iteration : 10437
train acc:  0.8359375
train loss:  0.3920004963874817
train gradient:  0.2368544480525583
iteration : 10438
train acc:  0.8125
train loss:  0.42193150520324707
train gradient:  0.19123039485960064
iteration : 10439
train acc:  0.8984375
train loss:  0.3055360019207001
train gradient:  0.12717709859465126
iteration : 10440
train acc:  0.859375
train loss:  0.38951706886291504
train gradient:  0.2021521081713934
iteration : 10441
train acc:  0.875
train loss:  0.3171386122703552
train gradient:  0.11332864115551727
iteration : 10442
train acc:  0.859375
train loss:  0.34944283962249756
train gradient:  0.17798031600836955
iteration : 10443
train acc:  0.8359375
train loss:  0.36830562353134155
train gradient:  0.2183884952317211
iteration : 10444
train acc:  0.8828125
train loss:  0.28596025705337524
train gradient:  0.13265556482155816
iteration : 10445
train acc:  0.9140625
train loss:  0.26794424653053284
train gradient:  0.1342816112121748
iteration : 10446
train acc:  0.875
train loss:  0.3073977530002594
train gradient:  0.2006990681963575
iteration : 10447
train acc:  0.8984375
train loss:  0.28547585010528564
train gradient:  0.1286405994081518
iteration : 10448
train acc:  0.84375
train loss:  0.406347393989563
train gradient:  0.2160058659641858
iteration : 10449
train acc:  0.859375
train loss:  0.3425801396369934
train gradient:  0.1890185946759962
iteration : 10450
train acc:  0.875
train loss:  0.2962842881679535
train gradient:  0.15327508818387447
iteration : 10451
train acc:  0.875
train loss:  0.2848657965660095
train gradient:  0.18390453858273237
iteration : 10452
train acc:  0.921875
train loss:  0.25274258852005005
train gradient:  0.1636626175267235
iteration : 10453
train acc:  0.8984375
train loss:  0.25387781858444214
train gradient:  0.10278888010613829
iteration : 10454
train acc:  0.8203125
train loss:  0.3841727375984192
train gradient:  0.15219344531859408
iteration : 10455
train acc:  0.9140625
train loss:  0.24863705039024353
train gradient:  0.1041265189160225
iteration : 10456
train acc:  0.8671875
train loss:  0.30108964443206787
train gradient:  0.1178082517373917
iteration : 10457
train acc:  0.9296875
train loss:  0.2172381728887558
train gradient:  0.15579787139319196
iteration : 10458
train acc:  0.859375
train loss:  0.3293520212173462
train gradient:  0.18000210443838413
iteration : 10459
train acc:  0.8828125
train loss:  0.2736035883426666
train gradient:  0.13154780180722023
iteration : 10460
train acc:  0.890625
train loss:  0.2638479173183441
train gradient:  0.1337528392930103
iteration : 10461
train acc:  0.9375
train loss:  0.24632684886455536
train gradient:  0.12632691787795036
iteration : 10462
train acc:  0.890625
train loss:  0.308044970035553
train gradient:  0.2439764513794845
iteration : 10463
train acc:  0.859375
train loss:  0.30510810017585754
train gradient:  0.1692019287882372
iteration : 10464
train acc:  0.9140625
train loss:  0.280756413936615
train gradient:  0.1808096988604292
iteration : 10465
train acc:  0.875
train loss:  0.3242419958114624
train gradient:  0.18302190057409312
iteration : 10466
train acc:  0.8203125
train loss:  0.37481752038002014
train gradient:  0.19411456503976035
iteration : 10467
train acc:  0.8125
train loss:  0.3315684199333191
train gradient:  0.17268259631598948
iteration : 10468
train acc:  0.8671875
train loss:  0.31611061096191406
train gradient:  0.1401973121799468
iteration : 10469
train acc:  0.890625
train loss:  0.3184700608253479
train gradient:  0.12041320305280628
iteration : 10470
train acc:  0.8125
train loss:  0.3495372533798218
train gradient:  0.19280891034474626
iteration : 10471
train acc:  0.8828125
train loss:  0.2428484559059143
train gradient:  0.1253498160602758
iteration : 10472
train acc:  0.84375
train loss:  0.40189024806022644
train gradient:  0.19234683890661555
iteration : 10473
train acc:  0.859375
train loss:  0.34775832295417786
train gradient:  0.14981719918848194
iteration : 10474
train acc:  0.8203125
train loss:  0.41440606117248535
train gradient:  0.2862099743970504
iteration : 10475
train acc:  0.8125
train loss:  0.4026995897293091
train gradient:  0.2523016327085303
iteration : 10476
train acc:  0.875
train loss:  0.29658427834510803
train gradient:  0.12210257909239375
iteration : 10477
train acc:  0.859375
train loss:  0.3089916706085205
train gradient:  0.11626259636295726
iteration : 10478
train acc:  0.84375
train loss:  0.2618391513824463
train gradient:  0.20297817072897034
iteration : 10479
train acc:  0.84375
train loss:  0.3628693222999573
train gradient:  0.19142729387368068
iteration : 10480
train acc:  0.8359375
train loss:  0.4046928584575653
train gradient:  0.263770118138429
iteration : 10481
train acc:  0.8515625
train loss:  0.34727537631988525
train gradient:  0.19610291029712118
iteration : 10482
train acc:  0.828125
train loss:  0.3785072863101959
train gradient:  0.18973591647920673
iteration : 10483
train acc:  0.8515625
train loss:  0.34055978059768677
train gradient:  0.14683731435276307
iteration : 10484
train acc:  0.8984375
train loss:  0.2867324650287628
train gradient:  0.15273337976451198
iteration : 10485
train acc:  0.828125
train loss:  0.376136839389801
train gradient:  0.2709008060323237
iteration : 10486
train acc:  0.8671875
train loss:  0.3263520896434784
train gradient:  0.1524496488614991
iteration : 10487
train acc:  0.8125
train loss:  0.42064645886421204
train gradient:  0.17486142064084043
iteration : 10488
train acc:  0.8359375
train loss:  0.4049515128135681
train gradient:  0.25447894842891156
iteration : 10489
train acc:  0.890625
train loss:  0.32002127170562744
train gradient:  0.0964056786472725
iteration : 10490
train acc:  0.8359375
train loss:  0.3797081410884857
train gradient:  0.23880219999726465
iteration : 10491
train acc:  0.796875
train loss:  0.4513537585735321
train gradient:  0.2766298095069975
iteration : 10492
train acc:  0.890625
train loss:  0.24772386252880096
train gradient:  0.11058187763741494
iteration : 10493
train acc:  0.7734375
train loss:  0.5012771487236023
train gradient:  0.3733333895884611
iteration : 10494
train acc:  0.875
train loss:  0.2976298928260803
train gradient:  0.14515176508459668
iteration : 10495
train acc:  0.8671875
train loss:  0.3295499086380005
train gradient:  0.15633178775078455
iteration : 10496
train acc:  0.890625
train loss:  0.27005916833877563
train gradient:  0.08510285001100895
iteration : 10497
train acc:  0.8671875
train loss:  0.34369325637817383
train gradient:  0.15523497636145836
iteration : 10498
train acc:  0.8515625
train loss:  0.3842640519142151
train gradient:  0.17758667289548805
iteration : 10499
train acc:  0.8984375
train loss:  0.27023279666900635
train gradient:  0.10259137691694448
iteration : 10500
train acc:  0.8984375
train loss:  0.24675624072551727
train gradient:  0.10999096725054217
iteration : 10501
train acc:  0.8828125
train loss:  0.33485740423202515
train gradient:  0.15374106958454037
iteration : 10502
train acc:  0.8203125
train loss:  0.3498690724372864
train gradient:  0.2348285272779742
iteration : 10503
train acc:  0.78125
train loss:  0.4356924295425415
train gradient:  0.2625163125740974
iteration : 10504
train acc:  0.8671875
train loss:  0.27612102031707764
train gradient:  0.13870117152356587
iteration : 10505
train acc:  0.8359375
train loss:  0.41682833433151245
train gradient:  0.22767579527618834
iteration : 10506
train acc:  0.84375
train loss:  0.4166254699230194
train gradient:  0.2087923371288531
iteration : 10507
train acc:  0.8828125
train loss:  0.30181604623794556
train gradient:  0.12470732808151772
iteration : 10508
train acc:  0.8125
train loss:  0.3715747892856598
train gradient:  0.18470025380336014
iteration : 10509
train acc:  0.8515625
train loss:  0.36511412262916565
train gradient:  0.1905441607272091
iteration : 10510
train acc:  0.875
train loss:  0.3140243589878082
train gradient:  0.16866055907777278
iteration : 10511
train acc:  0.8203125
train loss:  0.394193172454834
train gradient:  0.18461540979145385
iteration : 10512
train acc:  0.796875
train loss:  0.3579467535018921
train gradient:  0.1352678744086257
iteration : 10513
train acc:  0.8671875
train loss:  0.33026090264320374
train gradient:  0.12683709856802022
iteration : 10514
train acc:  0.8359375
train loss:  0.34663474559783936
train gradient:  0.1441798179009876
iteration : 10515
train acc:  0.8671875
train loss:  0.28064998984336853
train gradient:  0.16982381134933866
iteration : 10516
train acc:  0.9296875
train loss:  0.27540841698646545
train gradient:  0.15034578734010315
iteration : 10517
train acc:  0.84375
train loss:  0.3681598901748657
train gradient:  0.1549416906646072
iteration : 10518
train acc:  0.8671875
train loss:  0.3277520537376404
train gradient:  0.22867654863717127
iteration : 10519
train acc:  0.8359375
train loss:  0.37709367275238037
train gradient:  0.2126713475745805
iteration : 10520
train acc:  0.859375
train loss:  0.3182741403579712
train gradient:  0.15413094513058695
iteration : 10521
train acc:  0.84375
train loss:  0.37874001264572144
train gradient:  0.1756448441025139
iteration : 10522
train acc:  0.859375
train loss:  0.37804746627807617
train gradient:  0.1571961920735258
iteration : 10523
train acc:  0.8359375
train loss:  0.32438570261001587
train gradient:  0.13730127704116185
iteration : 10524
train acc:  0.8671875
train loss:  0.2687869668006897
train gradient:  0.1071909259683995
iteration : 10525
train acc:  0.828125
train loss:  0.3770580291748047
train gradient:  0.16271022594343257
iteration : 10526
train acc:  0.8515625
train loss:  0.28298622369766235
train gradient:  0.09744464118926226
iteration : 10527
train acc:  0.8515625
train loss:  0.321738064289093
train gradient:  0.1528815180761695
iteration : 10528
train acc:  0.8828125
train loss:  0.25740480422973633
train gradient:  0.11845602449979228
iteration : 10529
train acc:  0.859375
train loss:  0.3239589035511017
train gradient:  0.12033325091916654
iteration : 10530
train acc:  0.8515625
train loss:  0.33191025257110596
train gradient:  0.20810331968003629
iteration : 10531
train acc:  0.890625
train loss:  0.319810688495636
train gradient:  0.16173072246274922
iteration : 10532
train acc:  0.8828125
train loss:  0.3052781820297241
train gradient:  0.17169267006590114
iteration : 10533
train acc:  0.8125
train loss:  0.45385557413101196
train gradient:  0.28397631339172924
iteration : 10534
train acc:  0.8125
train loss:  0.38614800572395325
train gradient:  0.1618266983856661
iteration : 10535
train acc:  0.796875
train loss:  0.4400196373462677
train gradient:  0.2343578803261632
iteration : 10536
train acc:  0.8828125
train loss:  0.30024123191833496
train gradient:  0.12128496371369298
iteration : 10537
train acc:  0.8125
train loss:  0.3733348250389099
train gradient:  0.19694345851939948
iteration : 10538
train acc:  0.859375
train loss:  0.37921497225761414
train gradient:  0.17256769266576943
iteration : 10539
train acc:  0.859375
train loss:  0.3652192950248718
train gradient:  0.175497585029745
iteration : 10540
train acc:  0.8515625
train loss:  0.3856499195098877
train gradient:  0.172219452508518
iteration : 10541
train acc:  0.875
train loss:  0.30966800451278687
train gradient:  0.12499865687809238
iteration : 10542
train acc:  0.8671875
train loss:  0.3234454393386841
train gradient:  0.13270376664710185
iteration : 10543
train acc:  0.8984375
train loss:  0.25936761498451233
train gradient:  0.085146065637602
iteration : 10544
train acc:  0.8671875
train loss:  0.3037129044532776
train gradient:  0.13300473908684468
iteration : 10545
train acc:  0.828125
train loss:  0.32080990076065063
train gradient:  0.13937222868409058
iteration : 10546
train acc:  0.8671875
train loss:  0.31193453073501587
train gradient:  0.17679395156712713
iteration : 10547
train acc:  0.8828125
train loss:  0.3059290051460266
train gradient:  0.30535878286007967
iteration : 10548
train acc:  0.859375
train loss:  0.36269307136535645
train gradient:  0.18991964073719106
iteration : 10549
train acc:  0.8515625
train loss:  0.38794171810150146
train gradient:  0.17864237429123284
iteration : 10550
train acc:  0.8125
train loss:  0.42292097210884094
train gradient:  0.24062871195172203
iteration : 10551
train acc:  0.828125
train loss:  0.36824601888656616
train gradient:  0.14177782535110778
iteration : 10552
train acc:  0.875
train loss:  0.2713007628917694
train gradient:  0.10869898905989571
iteration : 10553
train acc:  0.8515625
train loss:  0.3249776065349579
train gradient:  0.16031052740969842
iteration : 10554
train acc:  0.8671875
train loss:  0.3413206934928894
train gradient:  0.14318436097501117
iteration : 10555
train acc:  0.8515625
train loss:  0.336732417345047
train gradient:  0.15309947415745254
iteration : 10556
train acc:  0.8515625
train loss:  0.328607439994812
train gradient:  0.12294636904018517
iteration : 10557
train acc:  0.8671875
train loss:  0.324055016040802
train gradient:  0.18213913236210616
iteration : 10558
train acc:  0.84375
train loss:  0.38949209451675415
train gradient:  0.16075818709433384
iteration : 10559
train acc:  0.8984375
train loss:  0.27381670475006104
train gradient:  0.12067349540576225
iteration : 10560
train acc:  0.84375
train loss:  0.35703733563423157
train gradient:  0.17199813525215163
iteration : 10561
train acc:  0.8125
train loss:  0.33993783593177795
train gradient:  0.16133640855719777
iteration : 10562
train acc:  0.921875
train loss:  0.23105867207050323
train gradient:  0.13198645422867822
iteration : 10563
train acc:  0.8046875
train loss:  0.4374154806137085
train gradient:  0.30126287120901263
iteration : 10564
train acc:  0.921875
train loss:  0.26577138900756836
train gradient:  0.1149753478707793
iteration : 10565
train acc:  0.9140625
train loss:  0.26046493649482727
train gradient:  0.13011309803638574
iteration : 10566
train acc:  0.859375
train loss:  0.3900245130062103
train gradient:  0.2083773535475444
iteration : 10567
train acc:  0.890625
train loss:  0.28081363439559937
train gradient:  0.17965117659103197
iteration : 10568
train acc:  0.8828125
train loss:  0.32540959119796753
train gradient:  0.14291510972178878
iteration : 10569
train acc:  0.8828125
train loss:  0.27922725677490234
train gradient:  0.12242184897210102
iteration : 10570
train acc:  0.875
train loss:  0.2822932004928589
train gradient:  0.07873582802809724
iteration : 10571
train acc:  0.8828125
train loss:  0.3230421841144562
train gradient:  0.17170163297855112
iteration : 10572
train acc:  0.875
train loss:  0.33093687891960144
train gradient:  0.18716323574946025
iteration : 10573
train acc:  0.90625
train loss:  0.2864683270454407
train gradient:  0.10382464180635761
iteration : 10574
train acc:  0.859375
train loss:  0.3231160044670105
train gradient:  0.11654894246588707
iteration : 10575
train acc:  0.8828125
train loss:  0.30758190155029297
train gradient:  0.16212652592793136
iteration : 10576
train acc:  0.8359375
train loss:  0.38334405422210693
train gradient:  0.20307473949574456
iteration : 10577
train acc:  0.8828125
train loss:  0.2844028174877167
train gradient:  0.13154947815596796
iteration : 10578
train acc:  0.8671875
train loss:  0.356681764125824
train gradient:  0.2307986376968408
iteration : 10579
train acc:  0.84375
train loss:  0.3944905400276184
train gradient:  0.22307793421088828
iteration : 10580
train acc:  0.8515625
train loss:  0.3396559953689575
train gradient:  0.14001811598562577
iteration : 10581
train acc:  0.859375
train loss:  0.3102032542228699
train gradient:  0.1592613497736578
iteration : 10582
train acc:  0.859375
train loss:  0.3012751638889313
train gradient:  0.13256250746917808
iteration : 10583
train acc:  0.8671875
train loss:  0.28489720821380615
train gradient:  0.09115008196619992
iteration : 10584
train acc:  0.8671875
train loss:  0.3014361560344696
train gradient:  0.104315308560665
iteration : 10585
train acc:  0.8359375
train loss:  0.31112146377563477
train gradient:  0.13048603174842294
iteration : 10586
train acc:  0.84375
train loss:  0.35944002866744995
train gradient:  0.1805617668571584
iteration : 10587
train acc:  0.8828125
train loss:  0.28086644411087036
train gradient:  0.11084496471944122
iteration : 10588
train acc:  0.8515625
train loss:  0.3500382900238037
train gradient:  0.13850757943310416
iteration : 10589
train acc:  0.90625
train loss:  0.25224119424819946
train gradient:  0.1019663754053919
iteration : 10590
train acc:  0.8671875
train loss:  0.34427303075790405
train gradient:  0.1814945064925237
iteration : 10591
train acc:  0.8828125
train loss:  0.30467137694358826
train gradient:  0.12485389987016023
iteration : 10592
train acc:  0.890625
train loss:  0.2622455656528473
train gradient:  0.11587032187004692
iteration : 10593
train acc:  0.8359375
train loss:  0.3579029440879822
train gradient:  0.29253996064424065
iteration : 10594
train acc:  0.875
train loss:  0.35759687423706055
train gradient:  0.2237596668028853
iteration : 10595
train acc:  0.8515625
train loss:  0.32341527938842773
train gradient:  0.1673645253633716
iteration : 10596
train acc:  0.90625
train loss:  0.2612193822860718
train gradient:  0.11509271139220782
iteration : 10597
train acc:  0.8515625
train loss:  0.34841763973236084
train gradient:  0.14306243266644725
iteration : 10598
train acc:  0.9296875
train loss:  0.21805362403392792
train gradient:  0.1641057227327982
iteration : 10599
train acc:  0.875
train loss:  0.2931293249130249
train gradient:  0.13434820622743215
iteration : 10600
train acc:  0.8671875
train loss:  0.3460437059402466
train gradient:  0.153033097445531
iteration : 10601
train acc:  0.828125
train loss:  0.3859216570854187
train gradient:  0.17840861469179714
iteration : 10602
train acc:  0.84375
train loss:  0.39381879568099976
train gradient:  0.189787442878333
iteration : 10603
train acc:  0.84375
train loss:  0.3358096182346344
train gradient:  0.22965827823791496
iteration : 10604
train acc:  0.859375
train loss:  0.3155169188976288
train gradient:  0.15533063348701784
iteration : 10605
train acc:  0.8671875
train loss:  0.3882143497467041
train gradient:  0.17096765142980097
iteration : 10606
train acc:  0.859375
train loss:  0.3581019341945648
train gradient:  0.18819460231928895
iteration : 10607
train acc:  0.859375
train loss:  0.3039449453353882
train gradient:  0.12748456445558698
iteration : 10608
train acc:  0.8828125
train loss:  0.3163789212703705
train gradient:  0.10742079685253433
iteration : 10609
train acc:  0.84375
train loss:  0.3366215229034424
train gradient:  0.24096063777681337
iteration : 10610
train acc:  0.8671875
train loss:  0.32033777236938477
train gradient:  0.13458657144454184
iteration : 10611
train acc:  0.828125
train loss:  0.36479783058166504
train gradient:  0.22507646164903158
iteration : 10612
train acc:  0.8515625
train loss:  0.3776167631149292
train gradient:  0.1682925759253974
iteration : 10613
train acc:  0.84375
train loss:  0.45230191946029663
train gradient:  0.24585994712373388
iteration : 10614
train acc:  0.859375
train loss:  0.3164513111114502
train gradient:  0.14667938195539426
iteration : 10615
train acc:  0.859375
train loss:  0.2950060963630676
train gradient:  0.10366551772488772
iteration : 10616
train acc:  0.859375
train loss:  0.2914634346961975
train gradient:  0.12167382304800725
iteration : 10617
train acc:  0.8828125
train loss:  0.24818919599056244
train gradient:  0.10387879107614528
iteration : 10618
train acc:  0.8515625
train loss:  0.4440360367298126
train gradient:  0.25325101479570405
iteration : 10619
train acc:  0.8359375
train loss:  0.3283708989620209
train gradient:  0.13018553571871472
iteration : 10620
train acc:  0.84375
train loss:  0.3473224341869354
train gradient:  0.4819146013238738
iteration : 10621
train acc:  0.8515625
train loss:  0.3369411528110504
train gradient:  0.12230438025979203
iteration : 10622
train acc:  0.84375
train loss:  0.3715988099575043
train gradient:  0.1924911603833428
iteration : 10623
train acc:  0.7890625
train loss:  0.39883801341056824
train gradient:  0.22955982207968023
iteration : 10624
train acc:  0.796875
train loss:  0.4262382388114929
train gradient:  0.25253261966424356
iteration : 10625
train acc:  0.859375
train loss:  0.34012505412101746
train gradient:  0.1342485465208018
iteration : 10626
train acc:  0.859375
train loss:  0.31601977348327637
train gradient:  0.15601610132038032
iteration : 10627
train acc:  0.828125
train loss:  0.4872336685657501
train gradient:  0.2170389894415814
iteration : 10628
train acc:  0.84375
train loss:  0.339367151260376
train gradient:  0.1777789375795556
iteration : 10629
train acc:  0.8515625
train loss:  0.33544060587882996
train gradient:  0.12906761895951724
iteration : 10630
train acc:  0.890625
train loss:  0.28455448150634766
train gradient:  0.09302487915263659
iteration : 10631
train acc:  0.8203125
train loss:  0.3951414227485657
train gradient:  0.2113502629782229
iteration : 10632
train acc:  0.875
train loss:  0.281828910112381
train gradient:  0.1070374762740503
iteration : 10633
train acc:  0.8515625
train loss:  0.3036249279975891
train gradient:  0.09183367771248928
iteration : 10634
train acc:  0.8515625
train loss:  0.3489784002304077
train gradient:  0.12307986808015199
iteration : 10635
train acc:  0.8828125
train loss:  0.2748261094093323
train gradient:  0.1075698472995865
iteration : 10636
train acc:  0.828125
train loss:  0.38659948110580444
train gradient:  0.16517823503868215
iteration : 10637
train acc:  0.875
train loss:  0.301909863948822
train gradient:  0.13059358632650417
iteration : 10638
train acc:  0.78125
train loss:  0.42773887515068054
train gradient:  0.23481098331253084
iteration : 10639
train acc:  0.8203125
train loss:  0.3758068084716797
train gradient:  0.18746173853499137
iteration : 10640
train acc:  0.8515625
train loss:  0.33906662464141846
train gradient:  0.15041981827983525
iteration : 10641
train acc:  0.8515625
train loss:  0.3558858036994934
train gradient:  0.21880744764415638
iteration : 10642
train acc:  0.828125
train loss:  0.3992924690246582
train gradient:  0.1673524402915459
iteration : 10643
train acc:  0.8828125
train loss:  0.2958422899246216
train gradient:  0.16526476646918448
iteration : 10644
train acc:  0.84375
train loss:  0.34272265434265137
train gradient:  0.12304706296163385
iteration : 10645
train acc:  0.8359375
train loss:  0.33352386951446533
train gradient:  0.13497370946113652
iteration : 10646
train acc:  0.8984375
train loss:  0.29087087512016296
train gradient:  0.10497128316239134
iteration : 10647
train acc:  0.8359375
train loss:  0.35612359642982483
train gradient:  0.1497912037147049
iteration : 10648
train acc:  0.8359375
train loss:  0.3531532287597656
train gradient:  0.1700673117174144
iteration : 10649
train acc:  0.7890625
train loss:  0.38159245252609253
train gradient:  0.1920277244740046
iteration : 10650
train acc:  0.890625
train loss:  0.2741313576698303
train gradient:  0.1092247527716305
iteration : 10651
train acc:  0.875
train loss:  0.3072441816329956
train gradient:  0.1135594455773305
iteration : 10652
train acc:  0.7890625
train loss:  0.3739251494407654
train gradient:  0.13500315754447256
iteration : 10653
train acc:  0.8984375
train loss:  0.2718310058116913
train gradient:  0.12932671932478304
iteration : 10654
train acc:  0.84375
train loss:  0.3460877537727356
train gradient:  0.2245970716079977
iteration : 10655
train acc:  0.8828125
train loss:  0.28210586309432983
train gradient:  0.1321349068055892
iteration : 10656
train acc:  0.890625
train loss:  0.2858673334121704
train gradient:  0.13105984848344732
iteration : 10657
train acc:  0.90625
train loss:  0.21276026964187622
train gradient:  0.12442572093898167
iteration : 10658
train acc:  0.8515625
train loss:  0.29494142532348633
train gradient:  0.16493117655083955
iteration : 10659
train acc:  0.890625
train loss:  0.2765750288963318
train gradient:  0.15387432240471782
iteration : 10660
train acc:  0.796875
train loss:  0.38809895515441895
train gradient:  0.22843865085156934
iteration : 10661
train acc:  0.8671875
train loss:  0.2920409142971039
train gradient:  0.15860910420990776
iteration : 10662
train acc:  0.8984375
train loss:  0.3087763786315918
train gradient:  0.13826016251079276
iteration : 10663
train acc:  0.828125
train loss:  0.3490222096443176
train gradient:  0.18393941796145444
iteration : 10664
train acc:  0.8515625
train loss:  0.3558160662651062
train gradient:  0.19267246433327795
iteration : 10665
train acc:  0.84375
train loss:  0.37732890248298645
train gradient:  0.1756361824697379
iteration : 10666
train acc:  0.8203125
train loss:  0.38884931802749634
train gradient:  0.20964014784661023
iteration : 10667
train acc:  0.890625
train loss:  0.2891472578048706
train gradient:  0.09291435353410707
iteration : 10668
train acc:  0.859375
train loss:  0.3066164255142212
train gradient:  0.1597180575404209
iteration : 10669
train acc:  0.8046875
train loss:  0.386633962392807
train gradient:  0.1336935763192445
iteration : 10670
train acc:  0.828125
train loss:  0.385215699672699
train gradient:  0.1942313919543468
iteration : 10671
train acc:  0.8671875
train loss:  0.3082450032234192
train gradient:  0.23327765023604963
iteration : 10672
train acc:  0.8203125
train loss:  0.38591280579566956
train gradient:  0.23352058981265045
iteration : 10673
train acc:  0.8828125
train loss:  0.3331596553325653
train gradient:  0.1691889524057683
iteration : 10674
train acc:  0.953125
train loss:  0.21707482635974884
train gradient:  0.16612016638534238
iteration : 10675
train acc:  0.8671875
train loss:  0.30746006965637207
train gradient:  0.14345509347209992
iteration : 10676
train acc:  0.84375
train loss:  0.30124369263648987
train gradient:  0.11055795327555894
iteration : 10677
train acc:  0.828125
train loss:  0.4009717106819153
train gradient:  0.26760642081980324
iteration : 10678
train acc:  0.828125
train loss:  0.36713284254074097
train gradient:  0.1672890752331493
iteration : 10679
train acc:  0.8125
train loss:  0.4080544710159302
train gradient:  0.2052616768679976
iteration : 10680
train acc:  0.9140625
train loss:  0.25041139125823975
train gradient:  0.09727630111032627
iteration : 10681
train acc:  0.859375
train loss:  0.34092652797698975
train gradient:  0.14293042487228308
iteration : 10682
train acc:  0.8515625
train loss:  0.3130246102809906
train gradient:  0.16793731485003144
iteration : 10683
train acc:  0.8828125
train loss:  0.2902344763278961
train gradient:  0.14704172289282968
iteration : 10684
train acc:  0.8984375
train loss:  0.2733401656150818
train gradient:  0.2264520677471959
iteration : 10685
train acc:  0.8515625
train loss:  0.34462541341781616
train gradient:  0.19276149185058056
iteration : 10686
train acc:  0.8984375
train loss:  0.22823013365268707
train gradient:  0.10114548036130185
iteration : 10687
train acc:  0.84375
train loss:  0.31304818391799927
train gradient:  0.17808591546373453
iteration : 10688
train acc:  0.84375
train loss:  0.3400987982749939
train gradient:  0.14875905626588173
iteration : 10689
train acc:  0.890625
train loss:  0.2922258973121643
train gradient:  0.15450457622226743
iteration : 10690
train acc:  0.875
train loss:  0.28197455406188965
train gradient:  0.12073190978049082
iteration : 10691
train acc:  0.796875
train loss:  0.3870384693145752
train gradient:  0.2015170224317604
iteration : 10692
train acc:  0.8515625
train loss:  0.3524184823036194
train gradient:  0.20144695201261584
iteration : 10693
train acc:  0.859375
train loss:  0.29553988575935364
train gradient:  0.1660556511927445
iteration : 10694
train acc:  0.875
train loss:  0.2962738871574402
train gradient:  0.14002897341223852
iteration : 10695
train acc:  0.859375
train loss:  0.32443004846572876
train gradient:  0.11467278653229354
iteration : 10696
train acc:  0.8984375
train loss:  0.2724725306034088
train gradient:  0.12811923428595529
iteration : 10697
train acc:  0.828125
train loss:  0.38917556405067444
train gradient:  0.3208392765281593
iteration : 10698
train acc:  0.859375
train loss:  0.3264866769313812
train gradient:  0.15649388968197395
iteration : 10699
train acc:  0.8984375
train loss:  0.26432734727859497
train gradient:  0.1106152985417675
iteration : 10700
train acc:  0.8671875
train loss:  0.31874215602874756
train gradient:  0.18531375042136572
iteration : 10701
train acc:  0.8671875
train loss:  0.2710699439048767
train gradient:  0.10321251190461987
iteration : 10702
train acc:  0.875
train loss:  0.2942395806312561
train gradient:  0.15465081504036715
iteration : 10703
train acc:  0.828125
train loss:  0.37489813566207886
train gradient:  0.3502887922803935
iteration : 10704
train acc:  0.8203125
train loss:  0.32821112871170044
train gradient:  0.18172492214420713
iteration : 10705
train acc:  0.8515625
train loss:  0.2912924289703369
train gradient:  0.1284756626944012
iteration : 10706
train acc:  0.875
train loss:  0.3029065728187561
train gradient:  0.20892872871150187
iteration : 10707
train acc:  0.8515625
train loss:  0.35418501496315
train gradient:  0.19060360413772146
iteration : 10708
train acc:  0.8671875
train loss:  0.36416709423065186
train gradient:  0.2555520442409312
iteration : 10709
train acc:  0.8515625
train loss:  0.26305869221687317
train gradient:  0.12278267903915469
iteration : 10710
train acc:  0.8671875
train loss:  0.29444974660873413
train gradient:  0.1462750497209095
iteration : 10711
train acc:  0.84375
train loss:  0.33468741178512573
train gradient:  0.15325060201362797
iteration : 10712
train acc:  0.828125
train loss:  0.36682096123695374
train gradient:  0.24661316972044298
iteration : 10713
train acc:  0.8046875
train loss:  0.42507317662239075
train gradient:  0.2863232898692645
iteration : 10714
train acc:  0.8671875
train loss:  0.3183835744857788
train gradient:  0.11562504312645046
iteration : 10715
train acc:  0.875
train loss:  0.3151513934135437
train gradient:  0.12751346231648433
iteration : 10716
train acc:  0.8203125
train loss:  0.39075180888175964
train gradient:  0.30995474895299147
iteration : 10717
train acc:  0.828125
train loss:  0.3078536093235016
train gradient:  0.1327805976563102
iteration : 10718
train acc:  0.8515625
train loss:  0.3550194799900055
train gradient:  0.20193408594232015
iteration : 10719
train acc:  0.8515625
train loss:  0.3367864489555359
train gradient:  0.15232355874663706
iteration : 10720
train acc:  0.890625
train loss:  0.29581761360168457
train gradient:  0.10247852924385886
iteration : 10721
train acc:  0.890625
train loss:  0.2704693377017975
train gradient:  0.16376140687302254
iteration : 10722
train acc:  0.8671875
train loss:  0.29181191325187683
train gradient:  0.14053288011762793
iteration : 10723
train acc:  0.828125
train loss:  0.33874037861824036
train gradient:  0.1989626906926924
iteration : 10724
train acc:  0.8828125
train loss:  0.25689899921417236
train gradient:  0.0943117713190684
iteration : 10725
train acc:  0.84375
train loss:  0.3405052125453949
train gradient:  0.17508639062197792
iteration : 10726
train acc:  0.828125
train loss:  0.3822280168533325
train gradient:  0.14982803445741402
iteration : 10727
train acc:  0.796875
train loss:  0.3611587882041931
train gradient:  0.1921550203833391
iteration : 10728
train acc:  0.859375
train loss:  0.3128073513507843
train gradient:  0.14335873448038317
iteration : 10729
train acc:  0.8359375
train loss:  0.36701396107673645
train gradient:  0.2046064240934038
iteration : 10730
train acc:  0.890625
train loss:  0.2533468008041382
train gradient:  0.11467391696745521
iteration : 10731
train acc:  0.8359375
train loss:  0.32559776306152344
train gradient:  0.11818002335515722
iteration : 10732
train acc:  0.8671875
train loss:  0.3463021516799927
train gradient:  0.14404797835645583
iteration : 10733
train acc:  0.859375
train loss:  0.37012702226638794
train gradient:  0.19217823646262977
iteration : 10734
train acc:  0.90625
train loss:  0.32050833106040955
train gradient:  0.21159525431913062
iteration : 10735
train acc:  0.875
train loss:  0.3329761028289795
train gradient:  0.17174680115221003
iteration : 10736
train acc:  0.8359375
train loss:  0.27224862575531006
train gradient:  0.10931976440329226
iteration : 10737
train acc:  0.859375
train loss:  0.34576937556266785
train gradient:  0.17075735040653797
iteration : 10738
train acc:  0.8984375
train loss:  0.2856414318084717
train gradient:  0.10345271922972653
iteration : 10739
train acc:  0.84375
train loss:  0.28692466020584106
train gradient:  0.1265972102154168
iteration : 10740
train acc:  0.796875
train loss:  0.45005476474761963
train gradient:  0.28172727330363123
iteration : 10741
train acc:  0.9296875
train loss:  0.2232576608657837
train gradient:  0.10613620377569132
iteration : 10742
train acc:  0.8671875
train loss:  0.3594115972518921
train gradient:  0.18043565788939775
iteration : 10743
train acc:  0.875
train loss:  0.36784273386001587
train gradient:  0.1683811053392961
iteration : 10744
train acc:  0.8203125
train loss:  0.3639049232006073
train gradient:  0.18213004582607512
iteration : 10745
train acc:  0.7890625
train loss:  0.444492906332016
train gradient:  0.2635313653937918
iteration : 10746
train acc:  0.8125
train loss:  0.37646403908729553
train gradient:  0.15438814965084763
iteration : 10747
train acc:  0.8359375
train loss:  0.3244284987449646
train gradient:  0.17829246886264374
iteration : 10748
train acc:  0.890625
train loss:  0.2876009941101074
train gradient:  0.11815680162702338
iteration : 10749
train acc:  0.90625
train loss:  0.30821844935417175
train gradient:  0.13429671141464022
iteration : 10750
train acc:  0.890625
train loss:  0.2946821451187134
train gradient:  0.09699233415041054
iteration : 10751
train acc:  0.828125
train loss:  0.358786940574646
train gradient:  0.18302742180529943
iteration : 10752
train acc:  0.875
train loss:  0.28161993622779846
train gradient:  0.14626612717983448
iteration : 10753
train acc:  0.875
train loss:  0.2655218243598938
train gradient:  0.11794131013134797
iteration : 10754
train acc:  0.875
train loss:  0.3169134259223938
train gradient:  0.1411462407446038
iteration : 10755
train acc:  0.8203125
train loss:  0.3810144066810608
train gradient:  0.2678176119661686
iteration : 10756
train acc:  0.8203125
train loss:  0.3784756660461426
train gradient:  0.20928026651723353
iteration : 10757
train acc:  0.84375
train loss:  0.31157881021499634
train gradient:  0.188615705315602
iteration : 10758
train acc:  0.8515625
train loss:  0.31350216269493103
train gradient:  0.2318595770752952
iteration : 10759
train acc:  0.8515625
train loss:  0.2828062176704407
train gradient:  0.14251167227801648
iteration : 10760
train acc:  0.84375
train loss:  0.3212050795555115
train gradient:  0.10405426379488607
iteration : 10761
train acc:  0.90625
train loss:  0.23540741205215454
train gradient:  0.14365274234602984
iteration : 10762
train acc:  0.8984375
train loss:  0.3107236325740814
train gradient:  0.2660598726923364
iteration : 10763
train acc:  0.890625
train loss:  0.3312307894229889
train gradient:  0.21088874504387523
iteration : 10764
train acc:  0.8515625
train loss:  0.3439428210258484
train gradient:  0.1742688085457307
iteration : 10765
train acc:  0.828125
train loss:  0.3817444443702698
train gradient:  0.13582271710869243
iteration : 10766
train acc:  0.8359375
train loss:  0.34685707092285156
train gradient:  0.17718144928256216
iteration : 10767
train acc:  0.875
train loss:  0.3496837019920349
train gradient:  0.1392350190663163
iteration : 10768
train acc:  0.90625
train loss:  0.22518345713615417
train gradient:  0.06605866489430538
iteration : 10769
train acc:  0.8359375
train loss:  0.3731881380081177
train gradient:  0.17111987942182016
iteration : 10770
train acc:  0.8125
train loss:  0.3811103105545044
train gradient:  0.1524988357214513
iteration : 10771
train acc:  0.8359375
train loss:  0.39426034688949585
train gradient:  0.1988148518975924
iteration : 10772
train acc:  0.8828125
train loss:  0.33019018173217773
train gradient:  0.15287852159233156
iteration : 10773
train acc:  0.8984375
train loss:  0.2648546099662781
train gradient:  0.1810640388053996
iteration : 10774
train acc:  0.84375
train loss:  0.3630605936050415
train gradient:  0.13857598978367772
iteration : 10775
train acc:  0.875
train loss:  0.25671303272247314
train gradient:  0.10544617052235618
iteration : 10776
train acc:  0.859375
train loss:  0.3174072504043579
train gradient:  0.1868155482007252
iteration : 10777
train acc:  0.84375
train loss:  0.3210628032684326
train gradient:  0.1440352145894882
iteration : 10778
train acc:  0.828125
train loss:  0.39055487513542175
train gradient:  0.17210435109801642
iteration : 10779
train acc:  0.859375
train loss:  0.30281972885131836
train gradient:  0.1681797127072528
iteration : 10780
train acc:  0.8203125
train loss:  0.34446418285369873
train gradient:  0.15113225847062695
iteration : 10781
train acc:  0.8671875
train loss:  0.2757128179073334
train gradient:  0.14849157291244836
iteration : 10782
train acc:  0.8359375
train loss:  0.29795849323272705
train gradient:  0.1306724244824794
iteration : 10783
train acc:  0.890625
train loss:  0.2587147355079651
train gradient:  0.10708288567781415
iteration : 10784
train acc:  0.890625
train loss:  0.2383062094449997
train gradient:  0.0867801632144449
iteration : 10785
train acc:  0.8828125
train loss:  0.26508331298828125
train gradient:  0.1339151825563251
iteration : 10786
train acc:  0.890625
train loss:  0.2705773413181305
train gradient:  0.12282282438097734
iteration : 10787
train acc:  0.8203125
train loss:  0.35517430305480957
train gradient:  0.17264131948043873
iteration : 10788
train acc:  0.8984375
train loss:  0.2583011984825134
train gradient:  0.10534487482806804
iteration : 10789
train acc:  0.8359375
train loss:  0.3112422227859497
train gradient:  0.146987065535616
iteration : 10790
train acc:  0.8671875
train loss:  0.34553349018096924
train gradient:  0.1284168464437649
iteration : 10791
train acc:  0.890625
train loss:  0.3133845329284668
train gradient:  0.1474508145751268
iteration : 10792
train acc:  0.8515625
train loss:  0.3087998628616333
train gradient:  0.13279452365846878
iteration : 10793
train acc:  0.7734375
train loss:  0.527970552444458
train gradient:  0.3276013505271496
iteration : 10794
train acc:  0.8359375
train loss:  0.35936683416366577
train gradient:  0.20698165902479448
iteration : 10795
train acc:  0.828125
train loss:  0.32083287835121155
train gradient:  0.1433894050225663
iteration : 10796
train acc:  0.9140625
train loss:  0.2253701388835907
train gradient:  0.095042243661193
iteration : 10797
train acc:  0.875
train loss:  0.28913548588752747
train gradient:  0.1472514573229647
iteration : 10798
train acc:  0.90625
train loss:  0.2808908224105835
train gradient:  0.14198720247619945
iteration : 10799
train acc:  0.90625
train loss:  0.24130260944366455
train gradient:  0.08913428325307976
iteration : 10800
train acc:  0.8828125
train loss:  0.3100839853286743
train gradient:  0.11204306325330227
iteration : 10801
train acc:  0.8515625
train loss:  0.35010918974876404
train gradient:  0.16080736210385788
iteration : 10802
train acc:  0.90625
train loss:  0.2843984365463257
train gradient:  0.13864578767380914
iteration : 10803
train acc:  0.8671875
train loss:  0.3072493076324463
train gradient:  0.14344395602001492
iteration : 10804
train acc:  0.875
train loss:  0.24333029985427856
train gradient:  0.10648937797087518
iteration : 10805
train acc:  0.8359375
train loss:  0.36181673407554626
train gradient:  0.19375373683489197
iteration : 10806
train acc:  0.8671875
train loss:  0.3484949767589569
train gradient:  0.2347352249111322
iteration : 10807
train acc:  0.8203125
train loss:  0.35445210337638855
train gradient:  0.14760688161970514
iteration : 10808
train acc:  0.84375
train loss:  0.3498780429363251
train gradient:  0.35763628349177673
iteration : 10809
train acc:  0.90625
train loss:  0.24725916981697083
train gradient:  0.09452611938854864
iteration : 10810
train acc:  0.8828125
train loss:  0.3571964502334595
train gradient:  0.1731873814679017
iteration : 10811
train acc:  0.84375
train loss:  0.3371613323688507
train gradient:  0.17422265495498174
iteration : 10812
train acc:  0.875
train loss:  0.3087629973888397
train gradient:  0.12098161865732326
iteration : 10813
train acc:  0.8203125
train loss:  0.40488192439079285
train gradient:  0.21565062751244804
iteration : 10814
train acc:  0.9453125
train loss:  0.21884533762931824
train gradient:  0.11801294086852379
iteration : 10815
train acc:  0.8203125
train loss:  0.4022284746170044
train gradient:  0.20682590001814782
iteration : 10816
train acc:  0.8984375
train loss:  0.28979140520095825
train gradient:  0.14536316200370358
iteration : 10817
train acc:  0.8671875
train loss:  0.3313003182411194
train gradient:  0.15927820990139374
iteration : 10818
train acc:  0.890625
train loss:  0.2768566608428955
train gradient:  0.14918047751025554
iteration : 10819
train acc:  0.8984375
train loss:  0.23202252388000488
train gradient:  0.08262994253106087
iteration : 10820
train acc:  0.8515625
train loss:  0.4157344102859497
train gradient:  0.32045005447789454
iteration : 10821
train acc:  0.8671875
train loss:  0.29524558782577515
train gradient:  0.1422921894470613
iteration : 10822
train acc:  0.8515625
train loss:  0.3203257918357849
train gradient:  0.12834766377954962
iteration : 10823
train acc:  0.8125
train loss:  0.3277781903743744
train gradient:  0.15680746783266297
iteration : 10824
train acc:  0.8828125
train loss:  0.3107007145881653
train gradient:  0.10446470901133346
iteration : 10825
train acc:  0.859375
train loss:  0.3707461357116699
train gradient:  0.14339501034294194
iteration : 10826
train acc:  0.84375
train loss:  0.325347363948822
train gradient:  0.11362074041947234
iteration : 10827
train acc:  0.84375
train loss:  0.3196452260017395
train gradient:  0.23016796325496042
iteration : 10828
train acc:  0.859375
train loss:  0.3121104836463928
train gradient:  0.1544068467337849
iteration : 10829
train acc:  0.84375
train loss:  0.30598944425582886
train gradient:  0.138694294384385
iteration : 10830
train acc:  0.8203125
train loss:  0.35148659348487854
train gradient:  0.18560280000667884
iteration : 10831
train acc:  0.9296875
train loss:  0.22427161037921906
train gradient:  0.0700781399697373
iteration : 10832
train acc:  0.8203125
train loss:  0.353344589471817
train gradient:  0.22144763474352466
iteration : 10833
train acc:  0.8125
train loss:  0.40813541412353516
train gradient:  0.236599651710542
iteration : 10834
train acc:  0.8671875
train loss:  0.30214667320251465
train gradient:  0.12571105034569804
iteration : 10835
train acc:  0.8203125
train loss:  0.29734110832214355
train gradient:  0.12644189741652143
iteration : 10836
train acc:  0.890625
train loss:  0.26423102617263794
train gradient:  0.12137596205756113
iteration : 10837
train acc:  0.859375
train loss:  0.31684136390686035
train gradient:  0.11865786143112048
iteration : 10838
train acc:  0.8828125
train loss:  0.2953227460384369
train gradient:  0.12712158810079335
iteration : 10839
train acc:  0.875
train loss:  0.29214197397232056
train gradient:  0.10975933979143222
iteration : 10840
train acc:  0.8828125
train loss:  0.3177427649497986
train gradient:  0.14191532404920193
iteration : 10841
train acc:  0.90625
train loss:  0.2858697474002838
train gradient:  0.1354037884805546
iteration : 10842
train acc:  0.8828125
train loss:  0.2887797951698303
train gradient:  0.10267202060007995
iteration : 10843
train acc:  0.8671875
train loss:  0.29710620641708374
train gradient:  0.11990812098326986
iteration : 10844
train acc:  0.890625
train loss:  0.2590738534927368
train gradient:  0.11698434767721431
iteration : 10845
train acc:  0.9140625
train loss:  0.3241341710090637
train gradient:  0.13180228641694625
iteration : 10846
train acc:  0.859375
train loss:  0.3712702989578247
train gradient:  0.14792130323416885
iteration : 10847
train acc:  0.90625
train loss:  0.258381724357605
train gradient:  0.1467015195316595
iteration : 10848
train acc:  0.84375
train loss:  0.3476511836051941
train gradient:  0.28220932117828135
iteration : 10849
train acc:  0.890625
train loss:  0.29544204473495483
train gradient:  0.15228939376274797
iteration : 10850
train acc:  0.8203125
train loss:  0.3589734733104706
train gradient:  0.15532552449076809
iteration : 10851
train acc:  0.90625
train loss:  0.25538647174835205
train gradient:  0.11965542078752217
iteration : 10852
train acc:  0.890625
train loss:  0.2450595498085022
train gradient:  0.09490829699135586
iteration : 10853
train acc:  0.8515625
train loss:  0.28577160835266113
train gradient:  0.1227448990572384
iteration : 10854
train acc:  0.8125
train loss:  0.3933633863925934
train gradient:  0.2880480690654691
iteration : 10855
train acc:  0.8984375
train loss:  0.3391917049884796
train gradient:  0.16481856439381842
iteration : 10856
train acc:  0.875
train loss:  0.3272913992404938
train gradient:  0.1698753055973721
iteration : 10857
train acc:  0.828125
train loss:  0.3987656235694885
train gradient:  0.2009666379197848
iteration : 10858
train acc:  0.8359375
train loss:  0.3132808804512024
train gradient:  0.13453860960440844
iteration : 10859
train acc:  0.890625
train loss:  0.2453191727399826
train gradient:  0.1375769775514607
iteration : 10860
train acc:  0.8046875
train loss:  0.45848849415779114
train gradient:  0.24236383274084172
iteration : 10861
train acc:  0.890625
train loss:  0.2797200083732605
train gradient:  0.10896485401779729
iteration : 10862
train acc:  0.859375
train loss:  0.3339729905128479
train gradient:  0.2799669370523845
iteration : 10863
train acc:  0.890625
train loss:  0.2530486285686493
train gradient:  0.10042004694711934
iteration : 10864
train acc:  0.84375
train loss:  0.34271085262298584
train gradient:  0.15549638820886172
iteration : 10865
train acc:  0.859375
train loss:  0.3261914849281311
train gradient:  0.184447328840755
iteration : 10866
train acc:  0.8203125
train loss:  0.3740033507347107
train gradient:  0.18576297152770754
iteration : 10867
train acc:  0.890625
train loss:  0.27352604269981384
train gradient:  0.1223901549411458
iteration : 10868
train acc:  0.875
train loss:  0.36541709303855896
train gradient:  0.21158928786388775
iteration : 10869
train acc:  0.828125
train loss:  0.37208133935928345
train gradient:  0.17257285088731628
iteration : 10870
train acc:  0.8359375
train loss:  0.35510653257369995
train gradient:  0.2520414364148079
iteration : 10871
train acc:  0.8203125
train loss:  0.4426633417606354
train gradient:  0.29193892800329835
iteration : 10872
train acc:  0.828125
train loss:  0.34622299671173096
train gradient:  0.1520905774262822
iteration : 10873
train acc:  0.8515625
train loss:  0.3224984407424927
train gradient:  0.13004443245228167
iteration : 10874
train acc:  0.796875
train loss:  0.4169618487358093
train gradient:  0.2619064101254718
iteration : 10875
train acc:  0.875
train loss:  0.30005717277526855
train gradient:  0.150440726927816
iteration : 10876
train acc:  0.875
train loss:  0.30599260330200195
train gradient:  0.13805352798855414
iteration : 10877
train acc:  0.859375
train loss:  0.3508358597755432
train gradient:  0.13298363450911121
iteration : 10878
train acc:  0.84375
train loss:  0.3463144302368164
train gradient:  0.18755688451092267
iteration : 10879
train acc:  0.8359375
train loss:  0.35958731174468994
train gradient:  0.19623011987592998
iteration : 10880
train acc:  0.8125
train loss:  0.39839863777160645
train gradient:  0.19948398016191365
iteration : 10881
train acc:  0.875
train loss:  0.30207359790802
train gradient:  0.13515163490362542
iteration : 10882
train acc:  0.8203125
train loss:  0.34268617630004883
train gradient:  0.19697435538124022
iteration : 10883
train acc:  0.78125
train loss:  0.4139339327812195
train gradient:  0.16204182291978747
iteration : 10884
train acc:  0.875
train loss:  0.3278300166130066
train gradient:  0.17208812534492585
iteration : 10885
train acc:  0.859375
train loss:  0.2720945179462433
train gradient:  0.10917469046162165
iteration : 10886
train acc:  0.8046875
train loss:  0.3852669596672058
train gradient:  0.2048776603621813
iteration : 10887
train acc:  0.8359375
train loss:  0.32397714257240295
train gradient:  0.1285066378813608
iteration : 10888
train acc:  0.875
train loss:  0.28681159019470215
train gradient:  0.12285258896817282
iteration : 10889
train acc:  0.84375
train loss:  0.34856435656547546
train gradient:  0.15827549651884748
iteration : 10890
train acc:  0.8671875
train loss:  0.28862157464027405
train gradient:  0.14538009061405552
iteration : 10891
train acc:  0.84375
train loss:  0.353449285030365
train gradient:  0.1807771540407329
iteration : 10892
train acc:  0.84375
train loss:  0.36007899045944214
train gradient:  0.17097497526655153
iteration : 10893
train acc:  0.828125
train loss:  0.36461836099624634
train gradient:  0.20484065439418245
iteration : 10894
train acc:  0.8828125
train loss:  0.31617891788482666
train gradient:  0.1286562929110833
iteration : 10895
train acc:  0.8359375
train loss:  0.4020286500453949
train gradient:  0.2656689448428922
iteration : 10896
train acc:  0.8203125
train loss:  0.3545154333114624
train gradient:  0.16225917583654367
iteration : 10897
train acc:  0.8203125
train loss:  0.3931933641433716
train gradient:  0.2162112409574395
iteration : 10898
train acc:  0.8359375
train loss:  0.3639424443244934
train gradient:  0.15278969854167335
iteration : 10899
train acc:  0.875
train loss:  0.29903897643089294
train gradient:  0.10075935393945024
iteration : 10900
train acc:  0.8828125
train loss:  0.294075071811676
train gradient:  0.14153252791836474
iteration : 10901
train acc:  0.859375
train loss:  0.3359348177909851
train gradient:  0.2041291312978255
iteration : 10902
train acc:  0.8984375
train loss:  0.2596569359302521
train gradient:  0.09070012097923842
iteration : 10903
train acc:  0.8515625
train loss:  0.39929312467575073
train gradient:  0.2696003609081912
iteration : 10904
train acc:  0.8515625
train loss:  0.3276793658733368
train gradient:  0.17550984742123868
iteration : 10905
train acc:  0.84375
train loss:  0.33928024768829346
train gradient:  0.11370310144537356
iteration : 10906
train acc:  0.796875
train loss:  0.32667455077171326
train gradient:  0.16850483989409848
iteration : 10907
train acc:  0.828125
train loss:  0.37173470854759216
train gradient:  0.21309275588014887
iteration : 10908
train acc:  0.828125
train loss:  0.37824082374572754
train gradient:  0.22711479325154227
iteration : 10909
train acc:  0.875
train loss:  0.3224527835845947
train gradient:  0.14752279076372207
iteration : 10910
train acc:  0.890625
train loss:  0.27702584862709045
train gradient:  0.07994766883751316
iteration : 10911
train acc:  0.8828125
train loss:  0.275657594203949
train gradient:  0.1355378165637864
iteration : 10912
train acc:  0.8359375
train loss:  0.38992953300476074
train gradient:  0.20315496441300973
iteration : 10913
train acc:  0.859375
train loss:  0.3139943480491638
train gradient:  0.15421562471146394
iteration : 10914
train acc:  0.875
train loss:  0.23751163482666016
train gradient:  0.08425078233695321
iteration : 10915
train acc:  0.8984375
train loss:  0.3079800009727478
train gradient:  0.13045744660536746
iteration : 10916
train acc:  0.875
train loss:  0.2830347716808319
train gradient:  0.1431756150515928
iteration : 10917
train acc:  0.90625
train loss:  0.27985692024230957
train gradient:  0.14813412337298826
iteration : 10918
train acc:  0.875
train loss:  0.27366161346435547
train gradient:  0.17007701194808814
iteration : 10919
train acc:  0.8828125
train loss:  0.29460790753364563
train gradient:  0.11062222946274537
iteration : 10920
train acc:  0.921875
train loss:  0.25602325797080994
train gradient:  0.12753232197381295
iteration : 10921
train acc:  0.8671875
train loss:  0.3256406784057617
train gradient:  0.16255815175020488
iteration : 10922
train acc:  0.8125
train loss:  0.40953385829925537
train gradient:  0.24088131938379886
iteration : 10923
train acc:  0.8359375
train loss:  0.3347380757331848
train gradient:  0.31221989810342554
iteration : 10924
train acc:  0.8515625
train loss:  0.3612757623195648
train gradient:  0.22200918834305466
iteration : 10925
train acc:  0.84375
train loss:  0.33793556690216064
train gradient:  0.1837068502070285
iteration : 10926
train acc:  0.875
train loss:  0.32898834347724915
train gradient:  0.12226112170002722
iteration : 10927
train acc:  0.8671875
train loss:  0.3356519341468811
train gradient:  0.13934447494155588
iteration : 10928
train acc:  0.8984375
train loss:  0.22812005877494812
train gradient:  0.10541663341501477
iteration : 10929
train acc:  0.8515625
train loss:  0.28628042340278625
train gradient:  0.15579722543731597
iteration : 10930
train acc:  0.90625
train loss:  0.24910055100917816
train gradient:  0.11642660592725376
iteration : 10931
train acc:  0.8828125
train loss:  0.3022027015686035
train gradient:  0.12006605669849128
iteration : 10932
train acc:  0.796875
train loss:  0.419014036655426
train gradient:  0.19184071512055453
iteration : 10933
train acc:  0.8515625
train loss:  0.41783377528190613
train gradient:  0.1915119987890311
iteration : 10934
train acc:  0.8984375
train loss:  0.29749977588653564
train gradient:  0.10664374129037979
iteration : 10935
train acc:  0.8515625
train loss:  0.31938016414642334
train gradient:  0.21217771398472457
iteration : 10936
train acc:  0.84375
train loss:  0.3612450957298279
train gradient:  0.1665516243392736
iteration : 10937
train acc:  0.8828125
train loss:  0.3142006993293762
train gradient:  0.15271969712542635
iteration : 10938
train acc:  0.8984375
train loss:  0.3028165102005005
train gradient:  0.13328403721738163
iteration : 10939
train acc:  0.9140625
train loss:  0.22953173518180847
train gradient:  0.10099230874999893
iteration : 10940
train acc:  0.8671875
train loss:  0.2713308334350586
train gradient:  0.18073184456199057
iteration : 10941
train acc:  0.84375
train loss:  0.3440708518028259
train gradient:  0.13274096354043122
iteration : 10942
train acc:  0.890625
train loss:  0.25334596633911133
train gradient:  0.0969370027842537
iteration : 10943
train acc:  0.8515625
train loss:  0.37978723645210266
train gradient:  0.18009654781619344
iteration : 10944
train acc:  0.8671875
train loss:  0.31416064500808716
train gradient:  0.189168639531451
iteration : 10945
train acc:  0.8828125
train loss:  0.36463791131973267
train gradient:  0.15588478101125877
iteration : 10946
train acc:  0.8046875
train loss:  0.33818963170051575
train gradient:  0.16913948023831105
iteration : 10947
train acc:  0.8515625
train loss:  0.3383282423019409
train gradient:  0.15500226294600572
iteration : 10948
train acc:  0.890625
train loss:  0.3022156059741974
train gradient:  0.13597505936809923
iteration : 10949
train acc:  0.8828125
train loss:  0.30480071902275085
train gradient:  0.15419558796470612
iteration : 10950
train acc:  0.859375
train loss:  0.3392602205276489
train gradient:  0.14137814201201945
iteration : 10951
train acc:  0.875
train loss:  0.2644578814506531
train gradient:  0.12894266935346071
iteration : 10952
train acc:  0.8359375
train loss:  0.29248157143592834
train gradient:  0.11214102706397477
iteration : 10953
train acc:  0.8515625
train loss:  0.37283802032470703
train gradient:  0.1805270086609374
iteration : 10954
train acc:  0.875
train loss:  0.279003381729126
train gradient:  0.15924342787278578
iteration : 10955
train acc:  0.890625
train loss:  0.28502559661865234
train gradient:  0.11787099298747826
iteration : 10956
train acc:  0.84375
train loss:  0.3286643624305725
train gradient:  0.1551754735544289
iteration : 10957
train acc:  0.8984375
train loss:  0.2533530592918396
train gradient:  0.09475029892175331
iteration : 10958
train acc:  0.875
train loss:  0.30434566736221313
train gradient:  0.11411336018806867
iteration : 10959
train acc:  0.8828125
train loss:  0.3229157328605652
train gradient:  0.136131553998809
iteration : 10960
train acc:  0.8125
train loss:  0.4178045094013214
train gradient:  0.2591091212425563
iteration : 10961
train acc:  0.8359375
train loss:  0.35026174783706665
train gradient:  0.17359137038280198
iteration : 10962
train acc:  0.90625
train loss:  0.3302285671234131
train gradient:  0.15134595019951325
iteration : 10963
train acc:  0.890625
train loss:  0.2826875150203705
train gradient:  0.14149541735483534
iteration : 10964
train acc:  0.7890625
train loss:  0.40602171421051025
train gradient:  0.2347746704728887
iteration : 10965
train acc:  0.8359375
train loss:  0.30879923701286316
train gradient:  0.18292233133293556
iteration : 10966
train acc:  0.8359375
train loss:  0.37344181537628174
train gradient:  0.20824056340406802
iteration : 10967
train acc:  0.84375
train loss:  0.350421279668808
train gradient:  0.2487933551953329
iteration : 10968
train acc:  0.90625
train loss:  0.3022408187389374
train gradient:  0.1987911145215122
iteration : 10969
train acc:  0.875
train loss:  0.2911986708641052
train gradient:  0.15285693598011066
iteration : 10970
train acc:  0.8515625
train loss:  0.29933491349220276
train gradient:  0.1454523119364306
iteration : 10971
train acc:  0.8203125
train loss:  0.4370649456977844
train gradient:  0.30598017938996697
iteration : 10972
train acc:  0.8515625
train loss:  0.3882748782634735
train gradient:  0.23144092646755543
iteration : 10973
train acc:  0.8671875
train loss:  0.3310842514038086
train gradient:  0.12167006311196484
iteration : 10974
train acc:  0.8828125
train loss:  0.26723712682724
train gradient:  0.11060119053261534
iteration : 10975
train acc:  0.890625
train loss:  0.24993029236793518
train gradient:  0.09455492945166565
iteration : 10976
train acc:  0.890625
train loss:  0.24877426028251648
train gradient:  0.1536542222471559
iteration : 10977
train acc:  0.875
train loss:  0.3320233225822449
train gradient:  0.12060554209877548
iteration : 10978
train acc:  0.890625
train loss:  0.3001212179660797
train gradient:  0.19486975245351562
iteration : 10979
train acc:  0.828125
train loss:  0.33322757482528687
train gradient:  0.1399924979192791
iteration : 10980
train acc:  0.8203125
train loss:  0.3780285716056824
train gradient:  0.21222819737471182
iteration : 10981
train acc:  0.859375
train loss:  0.32625681161880493
train gradient:  0.16663124223577128
iteration : 10982
train acc:  0.828125
train loss:  0.3439467251300812
train gradient:  0.1808352158715993
iteration : 10983
train acc:  0.8203125
train loss:  0.4146071672439575
train gradient:  0.18333502025274878
iteration : 10984
train acc:  0.8515625
train loss:  0.34472107887268066
train gradient:  0.21109417591260998
iteration : 10985
train acc:  0.890625
train loss:  0.26947537064552307
train gradient:  0.11008002195052535
iteration : 10986
train acc:  0.8046875
train loss:  0.449863076210022
train gradient:  0.21739152148856924
iteration : 10987
train acc:  0.8671875
train loss:  0.372550368309021
train gradient:  0.18616597907613566
iteration : 10988
train acc:  0.8203125
train loss:  0.3384019136428833
train gradient:  0.19775192884598738
iteration : 10989
train acc:  0.84375
train loss:  0.32195937633514404
train gradient:  0.14948308976259267
iteration : 10990
train acc:  0.8984375
train loss:  0.32282620668411255
train gradient:  0.1689724153190402
iteration : 10991
train acc:  0.8515625
train loss:  0.31165188550949097
train gradient:  0.14050695501691418
iteration : 10992
train acc:  0.8515625
train loss:  0.31492871046066284
train gradient:  0.2069956830619898
iteration : 10993
train acc:  0.9140625
train loss:  0.25684189796447754
train gradient:  0.13664332253179096
iteration : 10994
train acc:  0.890625
train loss:  0.29165661334991455
train gradient:  0.17022392565080757
iteration : 10995
train acc:  0.875
train loss:  0.25890326499938965
train gradient:  0.10809235666233367
iteration : 10996
train acc:  0.84375
train loss:  0.3965246081352234
train gradient:  0.218316888934953
iteration : 10997
train acc:  0.859375
train loss:  0.3642624020576477
train gradient:  0.2072971868523581
iteration : 10998
train acc:  0.859375
train loss:  0.38538360595703125
train gradient:  0.2207518098531478
iteration : 10999
train acc:  0.8671875
train loss:  0.35852712392807007
train gradient:  0.16784701307808625
iteration : 11000
train acc:  0.8515625
train loss:  0.37822890281677246
train gradient:  0.16904779624767424
iteration : 11001
train acc:  0.875
train loss:  0.2818886637687683
train gradient:  0.152553040478974
iteration : 11002
train acc:  0.828125
train loss:  0.3073042631149292
train gradient:  0.1495088869768623
iteration : 11003
train acc:  0.828125
train loss:  0.3764951825141907
train gradient:  0.25558745871780586
iteration : 11004
train acc:  0.765625
train loss:  0.4362686574459076
train gradient:  0.22167076930309773
iteration : 11005
train acc:  0.84375
train loss:  0.2839246988296509
train gradient:  0.12088284562201797
iteration : 11006
train acc:  0.859375
train loss:  0.31739121675491333
train gradient:  0.13442996098887708
iteration : 11007
train acc:  0.9140625
train loss:  0.23100802302360535
train gradient:  0.09416339790685797
iteration : 11008
train acc:  0.8515625
train loss:  0.33179205656051636
train gradient:  0.16163022179102127
iteration : 11009
train acc:  0.8984375
train loss:  0.2917708158493042
train gradient:  0.1059392882239246
iteration : 11010
train acc:  0.890625
train loss:  0.2517980933189392
train gradient:  0.11548802710230709
iteration : 11011
train acc:  0.8515625
train loss:  0.3509039282798767
train gradient:  0.13895732843324365
iteration : 11012
train acc:  0.859375
train loss:  0.34232792258262634
train gradient:  0.2093406185608976
iteration : 11013
train acc:  0.890625
train loss:  0.274203896522522
train gradient:  0.11940753631118459
iteration : 11014
train acc:  0.8515625
train loss:  0.35416656732559204
train gradient:  0.20546268554779623
iteration : 11015
train acc:  0.8125
train loss:  0.37168824672698975
train gradient:  0.20047355135403022
iteration : 11016
train acc:  0.859375
train loss:  0.35775017738342285
train gradient:  0.43465640210303175
iteration : 11017
train acc:  0.8671875
train loss:  0.3425575792789459
train gradient:  0.15249109974980748
iteration : 11018
train acc:  0.90625
train loss:  0.322481244802475
train gradient:  0.17440713000793515
iteration : 11019
train acc:  0.890625
train loss:  0.24168278276920319
train gradient:  0.10587866112667284
iteration : 11020
train acc:  0.84375
train loss:  0.3205099105834961
train gradient:  0.15162105570439743
iteration : 11021
train acc:  0.875
train loss:  0.28795260190963745
train gradient:  0.22350316301292972
iteration : 11022
train acc:  0.90625
train loss:  0.28269195556640625
train gradient:  0.14752766970098347
iteration : 11023
train acc:  0.8203125
train loss:  0.3934559226036072
train gradient:  0.24808913902297008
iteration : 11024
train acc:  0.8671875
train loss:  0.3018762469291687
train gradient:  0.12403247216424629
iteration : 11025
train acc:  0.8359375
train loss:  0.33642494678497314
train gradient:  0.19026754673730542
iteration : 11026
train acc:  0.8515625
train loss:  0.329037606716156
train gradient:  0.14950523713939923
iteration : 11027
train acc:  0.828125
train loss:  0.29038065671920776
train gradient:  0.13078447735275198
iteration : 11028
train acc:  0.875
train loss:  0.2963007092475891
train gradient:  0.1428691023841523
iteration : 11029
train acc:  0.8359375
train loss:  0.4074333906173706
train gradient:  0.22229745562709968
iteration : 11030
train acc:  0.8515625
train loss:  0.3303010165691376
train gradient:  0.181098896739395
iteration : 11031
train acc:  0.875
train loss:  0.32874229550361633
train gradient:  0.1009654232086292
iteration : 11032
train acc:  0.8984375
train loss:  0.24794597923755646
train gradient:  0.09612635234372638
iteration : 11033
train acc:  0.8359375
train loss:  0.3175619840621948
train gradient:  0.14183019375802575
iteration : 11034
train acc:  0.90625
train loss:  0.2783181071281433
train gradient:  0.11058512240397364
iteration : 11035
train acc:  0.890625
train loss:  0.285785049200058
train gradient:  0.09476267436795917
iteration : 11036
train acc:  0.8828125
train loss:  0.2945501208305359
train gradient:  0.1890325325455383
iteration : 11037
train acc:  0.8515625
train loss:  0.3192558288574219
train gradient:  0.12977770479187337
iteration : 11038
train acc:  0.8203125
train loss:  0.3407621383666992
train gradient:  0.19759978452666818
iteration : 11039
train acc:  0.921875
train loss:  0.26359230279922485
train gradient:  0.0870990778588129
iteration : 11040
train acc:  0.875
train loss:  0.26633039116859436
train gradient:  0.10401399975484338
iteration : 11041
train acc:  0.8515625
train loss:  0.32140833139419556
train gradient:  0.14062488182371194
iteration : 11042
train acc:  0.84375
train loss:  0.3073248565196991
train gradient:  0.1908356468256386
iteration : 11043
train acc:  0.8828125
train loss:  0.25388914346694946
train gradient:  0.09462908523421568
iteration : 11044
train acc:  0.890625
train loss:  0.30111050605773926
train gradient:  0.14144342202632415
iteration : 11045
train acc:  0.875
train loss:  0.3214529752731323
train gradient:  0.14389951695970676
iteration : 11046
train acc:  0.8359375
train loss:  0.4426201283931732
train gradient:  0.29891927923224326
iteration : 11047
train acc:  0.8671875
train loss:  0.2846033573150635
train gradient:  0.17477948095283283
iteration : 11048
train acc:  0.8671875
train loss:  0.3050239682197571
train gradient:  0.13935084494057082
iteration : 11049
train acc:  0.875
train loss:  0.27143219113349915
train gradient:  0.11367336741333191
iteration : 11050
train acc:  0.8671875
train loss:  0.26056045293807983
train gradient:  0.10838008516444676
iteration : 11051
train acc:  0.8125
train loss:  0.4428749978542328
train gradient:  0.256741797033583
iteration : 11052
train acc:  0.9140625
train loss:  0.24174116551876068
train gradient:  0.10142581823954284
iteration : 11053
train acc:  0.9375
train loss:  0.20520545542240143
train gradient:  0.07524267873423142
iteration : 11054
train acc:  0.8515625
train loss:  0.33446449041366577
train gradient:  0.15984672981925754
iteration : 11055
train acc:  0.84375
train loss:  0.3319529592990875
train gradient:  0.18978007723009926
iteration : 11056
train acc:  0.8359375
train loss:  0.36253735423088074
train gradient:  0.20294686263440292
iteration : 11057
train acc:  0.875
train loss:  0.28587085008621216
train gradient:  0.12020407346801952
iteration : 11058
train acc:  0.8359375
train loss:  0.3700418174266815
train gradient:  0.2184277970589199
iteration : 11059
train acc:  0.8671875
train loss:  0.33238717913627625
train gradient:  0.22142859651749391
iteration : 11060
train acc:  0.828125
train loss:  0.343164324760437
train gradient:  0.1626299031927896
iteration : 11061
train acc:  0.84375
train loss:  0.40106073021888733
train gradient:  0.19220388419036072
iteration : 11062
train acc:  0.8828125
train loss:  0.3012005388736725
train gradient:  0.1482392263214942
iteration : 11063
train acc:  0.8671875
train loss:  0.3175352215766907
train gradient:  0.12739117081474752
iteration : 11064
train acc:  0.9140625
train loss:  0.261126309633255
train gradient:  0.10007156599986766
iteration : 11065
train acc:  0.84375
train loss:  0.34781980514526367
train gradient:  0.1534494972258364
iteration : 11066
train acc:  0.890625
train loss:  0.283804714679718
train gradient:  0.14719148682021663
iteration : 11067
train acc:  0.890625
train loss:  0.2728126049041748
train gradient:  0.1279981631699746
iteration : 11068
train acc:  0.875
train loss:  0.2826891541481018
train gradient:  0.12257015837480027
iteration : 11069
train acc:  0.875
train loss:  0.3023141026496887
train gradient:  0.09981328037654744
iteration : 11070
train acc:  0.8984375
train loss:  0.2712564170360565
train gradient:  0.12270060430932701
iteration : 11071
train acc:  0.796875
train loss:  0.42027032375335693
train gradient:  0.21157272949161016
iteration : 11072
train acc:  0.9296875
train loss:  0.21085631847381592
train gradient:  0.10101864484038087
iteration : 11073
train acc:  0.8515625
train loss:  0.3723728060722351
train gradient:  0.18966675100741517
iteration : 11074
train acc:  0.875
train loss:  0.3067932724952698
train gradient:  0.14634076725944256
iteration : 11075
train acc:  0.8671875
train loss:  0.34878039360046387
train gradient:  0.1628228449842414
iteration : 11076
train acc:  0.890625
train loss:  0.2942698895931244
train gradient:  0.10545135206093206
iteration : 11077
train acc:  0.8359375
train loss:  0.38857924938201904
train gradient:  0.47225674285479535
iteration : 11078
train acc:  0.828125
train loss:  0.3588530421257019
train gradient:  0.1785681344965948
iteration : 11079
train acc:  0.84375
train loss:  0.33351776003837585
train gradient:  0.17709978382555125
iteration : 11080
train acc:  0.859375
train loss:  0.32988297939300537
train gradient:  0.25192713359931895
iteration : 11081
train acc:  0.8828125
train loss:  0.27303409576416016
train gradient:  0.11302313065504135
iteration : 11082
train acc:  0.890625
train loss:  0.3211873769760132
train gradient:  0.13218472455821878
iteration : 11083
train acc:  0.8359375
train loss:  0.29890841245651245
train gradient:  0.1277071324476775
iteration : 11084
train acc:  0.875
train loss:  0.2968441843986511
train gradient:  0.16087313321840635
iteration : 11085
train acc:  0.8515625
train loss:  0.31618380546569824
train gradient:  0.1557604269438139
iteration : 11086
train acc:  0.8203125
train loss:  0.3163023293018341
train gradient:  0.12909493085298623
iteration : 11087
train acc:  0.875
train loss:  0.32214051485061646
train gradient:  0.22749410337656478
iteration : 11088
train acc:  0.8984375
train loss:  0.26278746128082275
train gradient:  0.11960236116919773
iteration : 11089
train acc:  0.8359375
train loss:  0.3727748394012451
train gradient:  0.15103269635843447
iteration : 11090
train acc:  0.8203125
train loss:  0.4329913258552551
train gradient:  0.32661115705304683
iteration : 11091
train acc:  0.890625
train loss:  0.2834736108779907
train gradient:  0.17229370598159974
iteration : 11092
train acc:  0.8359375
train loss:  0.33550527691841125
train gradient:  0.1569862502566795
iteration : 11093
train acc:  0.8515625
train loss:  0.3061472773551941
train gradient:  0.14819254130648468
iteration : 11094
train acc:  0.8046875
train loss:  0.3656091094017029
train gradient:  0.19986875003124882
iteration : 11095
train acc:  0.890625
train loss:  0.27775049209594727
train gradient:  0.15531216336270248
iteration : 11096
train acc:  0.9140625
train loss:  0.30939731001853943
train gradient:  0.13378931438376346
iteration : 11097
train acc:  0.8359375
train loss:  0.40329572558403015
train gradient:  0.20324869982745952
iteration : 11098
train acc:  0.8671875
train loss:  0.30890217423439026
train gradient:  0.15005271305797924
iteration : 11099
train acc:  0.828125
train loss:  0.357907235622406
train gradient:  0.18742359024443403
iteration : 11100
train acc:  0.8828125
train loss:  0.3194747567176819
train gradient:  0.18958715603078102
iteration : 11101
train acc:  0.84375
train loss:  0.38264966011047363
train gradient:  0.21531572216785905
iteration : 11102
train acc:  0.8359375
train loss:  0.3850459158420563
train gradient:  0.19533048264315486
iteration : 11103
train acc:  0.8828125
train loss:  0.3334183990955353
train gradient:  0.18597353932243224
iteration : 11104
train acc:  0.8828125
train loss:  0.31876057386398315
train gradient:  0.17242935320522107
iteration : 11105
train acc:  0.8671875
train loss:  0.39838528633117676
train gradient:  0.23865965334570902
iteration : 11106
train acc:  0.859375
train loss:  0.34489211440086365
train gradient:  0.19040257310409975
iteration : 11107
train acc:  0.84375
train loss:  0.3175496459007263
train gradient:  0.1496738091739917
iteration : 11108
train acc:  0.859375
train loss:  0.31954240798950195
train gradient:  0.13963657280188474
iteration : 11109
train acc:  0.921875
train loss:  0.22116996347904205
train gradient:  0.1031291995553994
iteration : 11110
train acc:  0.8046875
train loss:  0.3924141228199005
train gradient:  0.19070136863700687
iteration : 11111
train acc:  0.8828125
train loss:  0.30136245489120483
train gradient:  0.1465342680725136
iteration : 11112
train acc:  0.8984375
train loss:  0.30586227774620056
train gradient:  0.11249825422986955
iteration : 11113
train acc:  0.859375
train loss:  0.33873623609542847
train gradient:  0.19115628745603241
iteration : 11114
train acc:  0.8515625
train loss:  0.3109753131866455
train gradient:  0.12971714747177143
iteration : 11115
train acc:  0.859375
train loss:  0.36668747663497925
train gradient:  0.22982018627617068
iteration : 11116
train acc:  0.8828125
train loss:  0.3000049293041229
train gradient:  0.12041639625113756
iteration : 11117
train acc:  0.8125
train loss:  0.37709206342697144
train gradient:  0.2324453442762079
iteration : 11118
train acc:  0.859375
train loss:  0.36573612689971924
train gradient:  0.23743442955477326
iteration : 11119
train acc:  0.8828125
train loss:  0.2982654869556427
train gradient:  0.12036677323298149
iteration : 11120
train acc:  0.8203125
train loss:  0.3746834099292755
train gradient:  0.19605332923573984
iteration : 11121
train acc:  0.8203125
train loss:  0.29790839552879333
train gradient:  0.13257856768231369
iteration : 11122
train acc:  0.8515625
train loss:  0.3294573128223419
train gradient:  0.18530514280124846
iteration : 11123
train acc:  0.859375
train loss:  0.31146740913391113
train gradient:  0.142920555249138
iteration : 11124
train acc:  0.859375
train loss:  0.3111962080001831
train gradient:  0.16976496173016642
iteration : 11125
train acc:  0.8515625
train loss:  0.3469477891921997
train gradient:  0.15940651057464975
iteration : 11126
train acc:  0.8828125
train loss:  0.2733454704284668
train gradient:  0.12430048491220633
iteration : 11127
train acc:  0.828125
train loss:  0.35539495944976807
train gradient:  0.20970917865560534
iteration : 11128
train acc:  0.8828125
train loss:  0.26184704899787903
train gradient:  0.09925625834529592
iteration : 11129
train acc:  0.8984375
train loss:  0.30595093965530396
train gradient:  0.14826956416721754
iteration : 11130
train acc:  0.8359375
train loss:  0.3730246424674988
train gradient:  0.1462957854478623
iteration : 11131
train acc:  0.8984375
train loss:  0.27387022972106934
train gradient:  0.119927357877423
iteration : 11132
train acc:  0.8671875
train loss:  0.29247719049453735
train gradient:  0.10995476457199697
iteration : 11133
train acc:  0.8828125
train loss:  0.23391155898571014
train gradient:  0.08450442450161523
iteration : 11134
train acc:  0.890625
train loss:  0.2840382754802704
train gradient:  0.1284397971668485
iteration : 11135
train acc:  0.8828125
train loss:  0.30142053961753845
train gradient:  0.1368517859424187
iteration : 11136
train acc:  0.8984375
train loss:  0.2418491542339325
train gradient:  0.10587514554811096
iteration : 11137
train acc:  0.8671875
train loss:  0.34974080324172974
train gradient:  0.30850178766328834
iteration : 11138
train acc:  0.8515625
train loss:  0.3209521770477295
train gradient:  0.15291556876995124
iteration : 11139
train acc:  0.875
train loss:  0.28800082206726074
train gradient:  0.160852096692659
iteration : 11140
train acc:  0.8046875
train loss:  0.3703606128692627
train gradient:  0.15163657753707543
iteration : 11141
train acc:  0.7734375
train loss:  0.4147968888282776
train gradient:  0.3208530070848999
iteration : 11142
train acc:  0.828125
train loss:  0.29104286432266235
train gradient:  0.13911893207891235
iteration : 11143
train acc:  0.9296875
train loss:  0.2753753662109375
train gradient:  0.13063324409529653
iteration : 11144
train acc:  0.859375
train loss:  0.3011290431022644
train gradient:  0.13459514771109474
iteration : 11145
train acc:  0.84375
train loss:  0.36790016293525696
train gradient:  0.14786231380165482
iteration : 11146
train acc:  0.859375
train loss:  0.31859129667282104
train gradient:  0.20424240664801774
iteration : 11147
train acc:  0.796875
train loss:  0.3781611919403076
train gradient:  0.2046746954142131
iteration : 11148
train acc:  0.8828125
train loss:  0.32262659072875977
train gradient:  0.20690090835383984
iteration : 11149
train acc:  0.84375
train loss:  0.2834896445274353
train gradient:  0.08293573351854926
iteration : 11150
train acc:  0.8984375
train loss:  0.2707161605358124
train gradient:  0.10673677909688202
iteration : 11151
train acc:  0.8515625
train loss:  0.36889535188674927
train gradient:  0.1989864134949214
iteration : 11152
train acc:  0.859375
train loss:  0.36918210983276367
train gradient:  0.17298100561729923
iteration : 11153
train acc:  0.828125
train loss:  0.3372209966182709
train gradient:  0.19354840178884425
iteration : 11154
train acc:  0.8828125
train loss:  0.30915963649749756
train gradient:  0.15626327075190266
iteration : 11155
train acc:  0.84375
train loss:  0.3138769268989563
train gradient:  0.12857071658966115
iteration : 11156
train acc:  0.921875
train loss:  0.20780739188194275
train gradient:  0.10969871491080829
iteration : 11157
train acc:  0.90625
train loss:  0.2540733218193054
train gradient:  0.17420556806869972
iteration : 11158
train acc:  0.8203125
train loss:  0.41330215334892273
train gradient:  0.2888299712242682
iteration : 11159
train acc:  0.921875
train loss:  0.21187737584114075
train gradient:  0.10229046560498589
iteration : 11160
train acc:  0.859375
train loss:  0.39117980003356934
train gradient:  0.1700594364900736
iteration : 11161
train acc:  0.84375
train loss:  0.3547070622444153
train gradient:  0.16998527436960487
iteration : 11162
train acc:  0.8671875
train loss:  0.3100685477256775
train gradient:  0.12231463209339882
iteration : 11163
train acc:  0.8515625
train loss:  0.32579150795936584
train gradient:  0.18018334247507614
iteration : 11164
train acc:  0.8828125
train loss:  0.27534011006355286
train gradient:  0.1433301246880042
iteration : 11165
train acc:  0.8125
train loss:  0.34772196412086487
train gradient:  0.12603505728044148
iteration : 11166
train acc:  0.8515625
train loss:  0.3574201464653015
train gradient:  0.2429335565371819
iteration : 11167
train acc:  0.8359375
train loss:  0.35839909315109253
train gradient:  0.15259987234850822
iteration : 11168
train acc:  0.796875
train loss:  0.4581139385700226
train gradient:  0.284300673960961
iteration : 11169
train acc:  0.8359375
train loss:  0.423622190952301
train gradient:  0.31673655909352905
iteration : 11170
train acc:  0.8515625
train loss:  0.31537699699401855
train gradient:  0.14316033390860394
iteration : 11171
train acc:  0.875
train loss:  0.3166036605834961
train gradient:  0.17829843301206522
iteration : 11172
train acc:  0.8515625
train loss:  0.3739743232727051
train gradient:  0.20421101166126412
iteration : 11173
train acc:  0.8515625
train loss:  0.3292219638824463
train gradient:  0.18541145989906588
iteration : 11174
train acc:  0.8203125
train loss:  0.38751885294914246
train gradient:  0.16909076081665586
iteration : 11175
train acc:  0.90625
train loss:  0.2989744544029236
train gradient:  0.21741290788328188
iteration : 11176
train acc:  0.875
train loss:  0.34038734436035156
train gradient:  0.18527357942832526
iteration : 11177
train acc:  0.84375
train loss:  0.35761332511901855
train gradient:  0.1868662502841354
iteration : 11178
train acc:  0.8515625
train loss:  0.3413991928100586
train gradient:  0.1495387192586364
iteration : 11179
train acc:  0.8828125
train loss:  0.3173089921474457
train gradient:  0.1497481436480355
iteration : 11180
train acc:  0.8671875
train loss:  0.32312875986099243
train gradient:  0.16512179998497584
iteration : 11181
train acc:  0.84375
train loss:  0.3883790373802185
train gradient:  0.21872953028127942
iteration : 11182
train acc:  0.8828125
train loss:  0.30234771966934204
train gradient:  0.1106554569417343
iteration : 11183
train acc:  0.890625
train loss:  0.2740990221500397
train gradient:  0.1200686254740715
iteration : 11184
train acc:  0.828125
train loss:  0.32413846254348755
train gradient:  0.10217843168724909
iteration : 11185
train acc:  0.8515625
train loss:  0.36542969942092896
train gradient:  0.16772235851190742
iteration : 11186
train acc:  0.8046875
train loss:  0.33330440521240234
train gradient:  0.1280968970942725
iteration : 11187
train acc:  0.859375
train loss:  0.2778931260108948
train gradient:  0.14344487142788076
iteration : 11188
train acc:  0.9296875
train loss:  0.2280123233795166
train gradient:  0.09296351508482381
iteration : 11189
train acc:  0.859375
train loss:  0.3478239178657532
train gradient:  0.12302695451870517
iteration : 11190
train acc:  0.828125
train loss:  0.4239776134490967
train gradient:  0.23690384929556774
iteration : 11191
train acc:  0.8359375
train loss:  0.41615840792655945
train gradient:  0.22602745714917175
iteration : 11192
train acc:  0.8203125
train loss:  0.314940482378006
train gradient:  0.11185060371571766
iteration : 11193
train acc:  0.8515625
train loss:  0.33859893679618835
train gradient:  0.13210306239125258
iteration : 11194
train acc:  0.8515625
train loss:  0.3054407238960266
train gradient:  0.10728425414161626
iteration : 11195
train acc:  0.875
train loss:  0.3144795000553131
train gradient:  0.10585325921713724
iteration : 11196
train acc:  0.8203125
train loss:  0.3947751522064209
train gradient:  0.20505875385989952
iteration : 11197
train acc:  0.890625
train loss:  0.2605823278427124
train gradient:  0.09890659820838077
iteration : 11198
train acc:  0.890625
train loss:  0.28482547402381897
train gradient:  0.15299311577702127
iteration : 11199
train acc:  0.84375
train loss:  0.3463657796382904
train gradient:  0.14788196301187476
iteration : 11200
train acc:  0.84375
train loss:  0.3596315085887909
train gradient:  0.13240310083618378
iteration : 11201
train acc:  0.875
train loss:  0.2883411943912506
train gradient:  0.16517136565918544
iteration : 11202
train acc:  0.859375
train loss:  0.3605431914329529
train gradient:  0.2049214928081815
iteration : 11203
train acc:  0.8359375
train loss:  0.397693008184433
train gradient:  0.17606283846936888
iteration : 11204
train acc:  0.8671875
train loss:  0.3173665404319763
train gradient:  0.12268252246959507
iteration : 11205
train acc:  0.859375
train loss:  0.32455477118492126
train gradient:  0.1431637179927015
iteration : 11206
train acc:  0.890625
train loss:  0.24973353743553162
train gradient:  0.10235634770659377
iteration : 11207
train acc:  0.8671875
train loss:  0.3327242136001587
train gradient:  0.1411638872849409
iteration : 11208
train acc:  0.8125
train loss:  0.40197864174842834
train gradient:  0.2009591569270604
iteration : 11209
train acc:  0.890625
train loss:  0.2969295084476471
train gradient:  0.13374582121770584
iteration : 11210
train acc:  0.8515625
train loss:  0.31695908308029175
train gradient:  0.257468165972371
iteration : 11211
train acc:  0.8359375
train loss:  0.40742242336273193
train gradient:  0.20423169101169145
iteration : 11212
train acc:  0.8828125
train loss:  0.27729058265686035
train gradient:  0.1026892033848479
iteration : 11213
train acc:  0.8828125
train loss:  0.326494961977005
train gradient:  0.16068660029961412
iteration : 11214
train acc:  0.8828125
train loss:  0.3133887052536011
train gradient:  0.1050921285776076
iteration : 11215
train acc:  0.8671875
train loss:  0.32101011276245117
train gradient:  0.1194415731745061
iteration : 11216
train acc:  0.875
train loss:  0.33517637848854065
train gradient:  0.1270959647307662
iteration : 11217
train acc:  0.875
train loss:  0.34120577573776245
train gradient:  0.1416590015863588
iteration : 11218
train acc:  0.890625
train loss:  0.24972912669181824
train gradient:  0.09480892663545035
iteration : 11219
train acc:  0.9296875
train loss:  0.2148715853691101
train gradient:  0.11034619890847532
iteration : 11220
train acc:  0.8984375
train loss:  0.24633967876434326
train gradient:  0.09929744218906837
iteration : 11221
train acc:  0.875
train loss:  0.28344812989234924
train gradient:  0.11337445490085471
iteration : 11222
train acc:  0.8203125
train loss:  0.3677365183830261
train gradient:  0.2751517351817858
iteration : 11223
train acc:  0.8671875
train loss:  0.33535677194595337
train gradient:  0.13023973037693118
iteration : 11224
train acc:  0.84375
train loss:  0.38058385252952576
train gradient:  0.17184507571944513
iteration : 11225
train acc:  0.8515625
train loss:  0.33114510774612427
train gradient:  0.15740704909446618
iteration : 11226
train acc:  0.9296875
train loss:  0.23229840397834778
train gradient:  0.09031384571014166
iteration : 11227
train acc:  0.859375
train loss:  0.31166872382164
train gradient:  0.14669980090569107
iteration : 11228
train acc:  0.8515625
train loss:  0.3636656105518341
train gradient:  0.16197423083716866
iteration : 11229
train acc:  0.875
train loss:  0.306264191865921
train gradient:  0.16295073094779067
iteration : 11230
train acc:  0.828125
train loss:  0.3057008385658264
train gradient:  0.1300550196708949
iteration : 11231
train acc:  0.8359375
train loss:  0.3168182373046875
train gradient:  0.14851481884378256
iteration : 11232
train acc:  0.875
train loss:  0.29987215995788574
train gradient:  0.13636502015983348
iteration : 11233
train acc:  0.8671875
train loss:  0.35720503330230713
train gradient:  0.15630396860227871
iteration : 11234
train acc:  0.8515625
train loss:  0.3495679199695587
train gradient:  0.17739631924114685
iteration : 11235
train acc:  0.8046875
train loss:  0.37260788679122925
train gradient:  0.18844786744982436
iteration : 11236
train acc:  0.8203125
train loss:  0.4326362907886505
train gradient:  0.2204549882631577
iteration : 11237
train acc:  0.890625
train loss:  0.2821306586265564
train gradient:  0.1051152874327616
iteration : 11238
train acc:  0.8203125
train loss:  0.37274229526519775
train gradient:  0.16258978012780714
iteration : 11239
train acc:  0.84375
train loss:  0.337176114320755
train gradient:  0.17304113181189917
iteration : 11240
train acc:  0.859375
train loss:  0.34684717655181885
train gradient:  0.1647618629879473
iteration : 11241
train acc:  0.890625
train loss:  0.2869381904602051
train gradient:  0.10457324804879677
iteration : 11242
train acc:  0.8984375
train loss:  0.2747949957847595
train gradient:  0.14529186414732087
iteration : 11243
train acc:  0.859375
train loss:  0.34552696347236633
train gradient:  0.16662792900106232
iteration : 11244
train acc:  0.8828125
train loss:  0.33889836072921753
train gradient:  0.17409531086059082
iteration : 11245
train acc:  0.8515625
train loss:  0.31813520193099976
train gradient:  0.1273548502349482
iteration : 11246
train acc:  0.8125
train loss:  0.38769814372062683
train gradient:  0.2348357120205492
iteration : 11247
train acc:  0.9296875
train loss:  0.23985332250595093
train gradient:  0.08754388978277798
iteration : 11248
train acc:  0.875
train loss:  0.3086208701133728
train gradient:  0.13913651360990434
iteration : 11249
train acc:  0.84375
train loss:  0.3667878210544586
train gradient:  0.21635988488372276
iteration : 11250
train acc:  0.8671875
train loss:  0.29698193073272705
train gradient:  0.14215075831343615
iteration : 11251
train acc:  0.8671875
train loss:  0.3043293356895447
train gradient:  0.10397769096593977
iteration : 11252
train acc:  0.890625
train loss:  0.2729361653327942
train gradient:  0.12660886475726937
iteration : 11253
train acc:  0.8515625
train loss:  0.3061334788799286
train gradient:  0.11561481914203067
iteration : 11254
train acc:  0.890625
train loss:  0.27473685145378113
train gradient:  0.09317698558628751
iteration : 11255
train acc:  0.8828125
train loss:  0.26366668939590454
train gradient:  0.0830283219717082
iteration : 11256
train acc:  0.875
train loss:  0.2926098704338074
train gradient:  0.07897276819666743
iteration : 11257
train acc:  0.8359375
train loss:  0.3742000460624695
train gradient:  0.11815538628135963
iteration : 11258
train acc:  0.84375
train loss:  0.3191159963607788
train gradient:  0.10541358374050279
iteration : 11259
train acc:  0.8203125
train loss:  0.3571206331253052
train gradient:  0.1831952899828774
iteration : 11260
train acc:  0.890625
train loss:  0.2872235178947449
train gradient:  0.1244099053588934
iteration : 11261
train acc:  0.875
train loss:  0.28416872024536133
train gradient:  0.11220408031519966
iteration : 11262
train acc:  0.8515625
train loss:  0.27183446288108826
train gradient:  0.13257461106245522
iteration : 11263
train acc:  0.875
train loss:  0.34302806854248047
train gradient:  0.16797623058283584
iteration : 11264
train acc:  0.890625
train loss:  0.24929635226726532
train gradient:  0.11446026612581145
iteration : 11265
train acc:  0.8515625
train loss:  0.30274075269699097
train gradient:  0.15236162136542816
iteration : 11266
train acc:  0.8359375
train loss:  0.32125499844551086
train gradient:  0.13596252203568093
iteration : 11267
train acc:  0.8828125
train loss:  0.3434891700744629
train gradient:  0.16471175655142362
iteration : 11268
train acc:  0.84375
train loss:  0.4010796844959259
train gradient:  0.1794738780647877
iteration : 11269
train acc:  0.8984375
train loss:  0.2588777542114258
train gradient:  0.08779989150590621
iteration : 11270
train acc:  0.8203125
train loss:  0.30959847569465637
train gradient:  0.14925411310163667
iteration : 11271
train acc:  0.859375
train loss:  0.3229907155036926
train gradient:  0.17571408872277638
iteration : 11272
train acc:  0.84375
train loss:  0.34473395347595215
train gradient:  0.1523276632531373
iteration : 11273
train acc:  0.84375
train loss:  0.3463425636291504
train gradient:  0.2034498235199136
iteration : 11274
train acc:  0.8125
train loss:  0.3510948717594147
train gradient:  0.138566171585411
iteration : 11275
train acc:  0.84375
train loss:  0.33698105812072754
train gradient:  0.2260219193841333
iteration : 11276
train acc:  0.8671875
train loss:  0.35335081815719604
train gradient:  0.1352745944480528
iteration : 11277
train acc:  0.875
train loss:  0.3165660500526428
train gradient:  0.15190752923487333
iteration : 11278
train acc:  0.8671875
train loss:  0.3159840404987335
train gradient:  0.25871684152813107
iteration : 11279
train acc:  0.8046875
train loss:  0.37289905548095703
train gradient:  0.1498290766979476
iteration : 11280
train acc:  0.8515625
train loss:  0.32071030139923096
train gradient:  0.14348688871799015
iteration : 11281
train acc:  0.84375
train loss:  0.3132922649383545
train gradient:  0.15663006242497912
iteration : 11282
train acc:  0.8515625
train loss:  0.3483678102493286
train gradient:  0.22848093273415312
iteration : 11283
train acc:  0.828125
train loss:  0.3691979646682739
train gradient:  0.1391364564738679
iteration : 11284
train acc:  0.8671875
train loss:  0.3158218264579773
train gradient:  0.13250113268475153
iteration : 11285
train acc:  0.84375
train loss:  0.33714020252227783
train gradient:  0.13818229280147726
iteration : 11286
train acc:  0.859375
train loss:  0.3296986222267151
train gradient:  0.1371065782527751
iteration : 11287
train acc:  0.8671875
train loss:  0.30327552556991577
train gradient:  0.11392876122672743
iteration : 11288
train acc:  0.8671875
train loss:  0.30614668130874634
train gradient:  0.23350276156128746
iteration : 11289
train acc:  0.8515625
train loss:  0.31178557872772217
train gradient:  0.3043136998676722
iteration : 11290
train acc:  0.84375
train loss:  0.35698774456977844
train gradient:  0.15693723800854503
iteration : 11291
train acc:  0.875
train loss:  0.30692926049232483
train gradient:  0.10243598333962477
iteration : 11292
train acc:  0.8515625
train loss:  0.3682195544242859
train gradient:  0.12340018432022935
iteration : 11293
train acc:  0.84375
train loss:  0.3187309503555298
train gradient:  0.15059884095863327
iteration : 11294
train acc:  0.8828125
train loss:  0.29986631870269775
train gradient:  0.13076646895253788
iteration : 11295
train acc:  0.8359375
train loss:  0.30479538440704346
train gradient:  0.2633458115872771
iteration : 11296
train acc:  0.8515625
train loss:  0.39935311675071716
train gradient:  0.16875187185639595
iteration : 11297
train acc:  0.8515625
train loss:  0.29850858449935913
train gradient:  0.11064229167548105
iteration : 11298
train acc:  0.8515625
train loss:  0.31765058636665344
train gradient:  0.1892236837038383
iteration : 11299
train acc:  0.8828125
train loss:  0.25704020261764526
train gradient:  0.12523695151209221
iteration : 11300
train acc:  0.875
train loss:  0.2630031108856201
train gradient:  0.09792278269511777
iteration : 11301
train acc:  0.921875
train loss:  0.25406506657600403
train gradient:  0.09085240942254791
iteration : 11302
train acc:  0.7734375
train loss:  0.3950527310371399
train gradient:  0.19812252047125598
iteration : 11303
train acc:  0.796875
train loss:  0.45650967955589294
train gradient:  0.23959672215018363
iteration : 11304
train acc:  0.8984375
train loss:  0.27682167291641235
train gradient:  0.09761911714775665
iteration : 11305
train acc:  0.8984375
train loss:  0.32582953572273254
train gradient:  0.18791448273840633
iteration : 11306
train acc:  0.8125
train loss:  0.4185401499271393
train gradient:  0.5252522540245199
iteration : 11307
train acc:  0.859375
train loss:  0.26986485719680786
train gradient:  0.08957718928906602
iteration : 11308
train acc:  0.890625
train loss:  0.2716728746891022
train gradient:  0.12640824450885196
iteration : 11309
train acc:  0.8046875
train loss:  0.3334945738315582
train gradient:  0.113795627214464
iteration : 11310
train acc:  0.84375
train loss:  0.3316958546638489
train gradient:  0.15397811193455635
iteration : 11311
train acc:  0.875
train loss:  0.3025249242782593
train gradient:  0.1251292164639162
iteration : 11312
train acc:  0.8984375
train loss:  0.23699918389320374
train gradient:  0.11700344669824904
iteration : 11313
train acc:  0.8828125
train loss:  0.2759481966495514
train gradient:  0.1177958021980981
iteration : 11314
train acc:  0.8671875
train loss:  0.3128495216369629
train gradient:  0.12962709657156218
iteration : 11315
train acc:  0.875
train loss:  0.3296283781528473
train gradient:  0.1508512165047876
iteration : 11316
train acc:  0.875
train loss:  0.2826218605041504
train gradient:  0.1080515698141114
iteration : 11317
train acc:  0.859375
train loss:  0.312101811170578
train gradient:  0.14557922023589911
iteration : 11318
train acc:  0.7734375
train loss:  0.46202242374420166
train gradient:  0.3776134988940133
iteration : 11319
train acc:  0.8359375
train loss:  0.3122134208679199
train gradient:  0.13043076405791454
iteration : 11320
train acc:  0.8359375
train loss:  0.3552156686782837
train gradient:  0.26652695363901885
iteration : 11321
train acc:  0.890625
train loss:  0.27529871463775635
train gradient:  0.10988868446250752
iteration : 11322
train acc:  0.8359375
train loss:  0.38346219062805176
train gradient:  0.20992808794007456
iteration : 11323
train acc:  0.875
train loss:  0.30996012687683105
train gradient:  0.16196516527074206
iteration : 11324
train acc:  0.859375
train loss:  0.35973668098449707
train gradient:  0.17811611677370887
iteration : 11325
train acc:  0.8515625
train loss:  0.37308815121650696
train gradient:  0.1719439881956247
iteration : 11326
train acc:  0.890625
train loss:  0.25049564242362976
train gradient:  0.1254218255758794
iteration : 11327
train acc:  0.859375
train loss:  0.32344940304756165
train gradient:  0.16711776291853558
iteration : 11328
train acc:  0.875
train loss:  0.32837337255477905
train gradient:  0.15213271099108422
iteration : 11329
train acc:  0.8828125
train loss:  0.25669533014297485
train gradient:  0.10817028450484603
iteration : 11330
train acc:  0.8984375
train loss:  0.2759697437286377
train gradient:  0.13706258252639336
iteration : 11331
train acc:  0.8984375
train loss:  0.29687240719795227
train gradient:  0.10000242583978106
iteration : 11332
train acc:  0.8359375
train loss:  0.38019782304763794
train gradient:  0.16782098736468148
iteration : 11333
train acc:  0.9140625
train loss:  0.2633116543292999
train gradient:  0.14927423119196986
iteration : 11334
train acc:  0.8984375
train loss:  0.2713428735733032
train gradient:  0.08199785671560436
iteration : 11335
train acc:  0.8359375
train loss:  0.3300572335720062
train gradient:  0.1365544560129967
iteration : 11336
train acc:  0.8984375
train loss:  0.24362364411354065
train gradient:  0.0868607213476457
iteration : 11337
train acc:  0.8203125
train loss:  0.43929851055145264
train gradient:  0.19532856742291466
iteration : 11338
train acc:  0.8515625
train loss:  0.30155959725379944
train gradient:  0.11864667993335795
iteration : 11339
train acc:  0.8828125
train loss:  0.2891605496406555
train gradient:  0.1279831549378959
iteration : 11340
train acc:  0.84375
train loss:  0.35215067863464355
train gradient:  0.12817748656505218
iteration : 11341
train acc:  0.8125
train loss:  0.4489385485649109
train gradient:  0.28350463472777176
iteration : 11342
train acc:  0.8359375
train loss:  0.3661448061466217
train gradient:  0.30610821305267144
iteration : 11343
train acc:  0.828125
train loss:  0.32526731491088867
train gradient:  0.1162178904415384
iteration : 11344
train acc:  0.8671875
train loss:  0.2865827977657318
train gradient:  0.11213929476189788
iteration : 11345
train acc:  0.8359375
train loss:  0.33192694187164307
train gradient:  0.15340500155213802
iteration : 11346
train acc:  0.890625
train loss:  0.2858012914657593
train gradient:  0.14338330581346315
iteration : 11347
train acc:  0.875
train loss:  0.265310138463974
train gradient:  0.11036104841343353
iteration : 11348
train acc:  0.890625
train loss:  0.3461635708808899
train gradient:  0.1342801977193174
iteration : 11349
train acc:  0.8984375
train loss:  0.24063381552696228
train gradient:  0.15567083829414907
iteration : 11350
train acc:  0.8359375
train loss:  0.30169931054115295
train gradient:  0.14435473759433853
iteration : 11351
train acc:  0.8515625
train loss:  0.3413318395614624
train gradient:  0.2038366671263293
iteration : 11352
train acc:  0.84375
train loss:  0.36183592677116394
train gradient:  0.18452441607524445
iteration : 11353
train acc:  0.8125
train loss:  0.367552250623703
train gradient:  0.2546365548870228
iteration : 11354
train acc:  0.828125
train loss:  0.37193241715431213
train gradient:  0.20409708264785448
iteration : 11355
train acc:  0.8203125
train loss:  0.34175580739974976
train gradient:  0.13128343621312333
iteration : 11356
train acc:  0.890625
train loss:  0.29102084040641785
train gradient:  0.11833629229420121
iteration : 11357
train acc:  0.8671875
train loss:  0.2759925127029419
train gradient:  0.11956661181710433
iteration : 11358
train acc:  0.8828125
train loss:  0.3242780864238739
train gradient:  0.10972031398433846
iteration : 11359
train acc:  0.90625
train loss:  0.28009700775146484
train gradient:  0.07562604979902589
iteration : 11360
train acc:  0.8359375
train loss:  0.31357914209365845
train gradient:  0.1309989895118624
iteration : 11361
train acc:  0.84375
train loss:  0.3199146091938019
train gradient:  0.19257683933804892
iteration : 11362
train acc:  0.890625
train loss:  0.27626290917396545
train gradient:  0.09058314890456591
iteration : 11363
train acc:  0.8671875
train loss:  0.2921598255634308
train gradient:  0.13682529562985624
iteration : 11364
train acc:  0.875
train loss:  0.28706327080726624
train gradient:  0.14566888395836153
iteration : 11365
train acc:  0.8359375
train loss:  0.3648666739463806
train gradient:  0.2337067153070117
iteration : 11366
train acc:  0.859375
train loss:  0.3028291165828705
train gradient:  0.13162235207002457
iteration : 11367
train acc:  0.8515625
train loss:  0.3480014503002167
train gradient:  0.16301929587493547
iteration : 11368
train acc:  0.8125
train loss:  0.41508379578590393
train gradient:  0.22046090362229454
iteration : 11369
train acc:  0.8984375
train loss:  0.22537274658679962
train gradient:  0.11098484450792362
iteration : 11370
train acc:  0.90625
train loss:  0.2689877152442932
train gradient:  0.08911877283790626
iteration : 11371
train acc:  0.859375
train loss:  0.2989133596420288
train gradient:  0.1478304163326997
iteration : 11372
train acc:  0.828125
train loss:  0.38850730657577515
train gradient:  0.1734794901501913
iteration : 11373
train acc:  0.8203125
train loss:  0.34820061922073364
train gradient:  0.16760842341042942
iteration : 11374
train acc:  0.8359375
train loss:  0.39796364307403564
train gradient:  0.1746541597295874
iteration : 11375
train acc:  0.8828125
train loss:  0.22471457719802856
train gradient:  0.14559672950845196
iteration : 11376
train acc:  0.8359375
train loss:  0.37445640563964844
train gradient:  0.22175988884736803
iteration : 11377
train acc:  0.84375
train loss:  0.3306710422039032
train gradient:  0.13130153567078662
iteration : 11378
train acc:  0.8515625
train loss:  0.3616592288017273
train gradient:  0.17613009412188763
iteration : 11379
train acc:  0.8203125
train loss:  0.39142340421676636
train gradient:  0.1822000288034072
iteration : 11380
train acc:  0.8203125
train loss:  0.3807598948478699
train gradient:  0.15255220439205036
iteration : 11381
train acc:  0.875
train loss:  0.26802611351013184
train gradient:  0.07980648944402789
iteration : 11382
train acc:  0.8828125
train loss:  0.29984647035598755
train gradient:  0.16801733577826095
iteration : 11383
train acc:  0.875
train loss:  0.2861458957195282
train gradient:  0.11534180553729184
iteration : 11384
train acc:  0.9296875
train loss:  0.22488674521446228
train gradient:  0.07964571266278397
iteration : 11385
train acc:  0.8984375
train loss:  0.26619499921798706
train gradient:  0.14512920766506465
iteration : 11386
train acc:  0.8828125
train loss:  0.2992769479751587
train gradient:  0.10307203265900867
iteration : 11387
train acc:  0.90625
train loss:  0.2942393124103546
train gradient:  0.09972403842327375
iteration : 11388
train acc:  0.8359375
train loss:  0.3438670039176941
train gradient:  0.21147253623394674
iteration : 11389
train acc:  0.78125
train loss:  0.44905316829681396
train gradient:  0.20690695643003854
iteration : 11390
train acc:  0.8203125
train loss:  0.31701958179473877
train gradient:  0.17909390111823126
iteration : 11391
train acc:  0.8671875
train loss:  0.31068357825279236
train gradient:  0.13450219680423509
iteration : 11392
train acc:  0.875
train loss:  0.3038817346096039
train gradient:  0.15948213743621714
iteration : 11393
train acc:  0.875
train loss:  0.33297663927078247
train gradient:  0.2207244604071964
iteration : 11394
train acc:  0.859375
train loss:  0.2821800112724304
train gradient:  0.12353262837848103
iteration : 11395
train acc:  0.859375
train loss:  0.35257646441459656
train gradient:  0.25158438869491695
iteration : 11396
train acc:  0.875
train loss:  0.2988777458667755
train gradient:  0.12220512431914106
iteration : 11397
train acc:  0.8515625
train loss:  0.3257230222225189
train gradient:  0.14735734098914477
iteration : 11398
train acc:  0.8125
train loss:  0.42665690183639526
train gradient:  0.17651414975614182
iteration : 11399
train acc:  0.859375
train loss:  0.3201618790626526
train gradient:  0.21376430247587364
iteration : 11400
train acc:  0.8515625
train loss:  0.32780712842941284
train gradient:  0.12212017975092618
iteration : 11401
train acc:  0.828125
train loss:  0.3719465732574463
train gradient:  0.19393212348270866
iteration : 11402
train acc:  0.8515625
train loss:  0.3499598503112793
train gradient:  0.16812437508982278
iteration : 11403
train acc:  0.828125
train loss:  0.3843379616737366
train gradient:  0.18973363395146647
iteration : 11404
train acc:  0.875
train loss:  0.35605770349502563
train gradient:  0.15550972418527348
iteration : 11405
train acc:  0.8671875
train loss:  0.3274649381637573
train gradient:  0.1454041216232986
iteration : 11406
train acc:  0.8515625
train loss:  0.31793421506881714
train gradient:  0.12354172270417034
iteration : 11407
train acc:  0.8359375
train loss:  0.32090842723846436
train gradient:  0.17629511299143946
iteration : 11408
train acc:  0.8671875
train loss:  0.29983463883399963
train gradient:  0.14629411547968574
iteration : 11409
train acc:  0.890625
train loss:  0.2678666114807129
train gradient:  0.11496960434026893
iteration : 11410
train acc:  0.90625
train loss:  0.3018551170825958
train gradient:  0.09889519548590128
iteration : 11411
train acc:  0.8359375
train loss:  0.32760101556777954
train gradient:  0.16317072763370577
iteration : 11412
train acc:  0.8828125
train loss:  0.35987240076065063
train gradient:  0.20859142918970836
iteration : 11413
train acc:  0.84375
train loss:  0.36172860860824585
train gradient:  0.14751604154020853
iteration : 11414
train acc:  0.8984375
train loss:  0.30595001578330994
train gradient:  0.13091610329063286
iteration : 11415
train acc:  0.8671875
train loss:  0.2761584222316742
train gradient:  0.07839914126179931
iteration : 11416
train acc:  0.8984375
train loss:  0.3011215329170227
train gradient:  0.204492783981822
iteration : 11417
train acc:  0.8671875
train loss:  0.27781155705451965
train gradient:  0.13035384212901802
iteration : 11418
train acc:  0.90625
train loss:  0.25259649753570557
train gradient:  0.08227571314085577
iteration : 11419
train acc:  0.8671875
train loss:  0.37274810671806335
train gradient:  0.18601322351997837
iteration : 11420
train acc:  0.828125
train loss:  0.36450284719467163
train gradient:  0.1259871599461637
iteration : 11421
train acc:  0.921875
train loss:  0.256000816822052
train gradient:  0.1440589120324562
iteration : 11422
train acc:  0.8671875
train loss:  0.26299798488616943
train gradient:  0.10099148672122239
iteration : 11423
train acc:  0.875
train loss:  0.291687935590744
train gradient:  0.13815829930995796
iteration : 11424
train acc:  0.8671875
train loss:  0.3633139729499817
train gradient:  0.180042936054603
iteration : 11425
train acc:  0.875
train loss:  0.2951226532459259
train gradient:  0.1438393482793805
iteration : 11426
train acc:  0.9140625
train loss:  0.2279944121837616
train gradient:  0.11446285740248921
iteration : 11427
train acc:  0.875
train loss:  0.29263484477996826
train gradient:  0.17689521487811272
iteration : 11428
train acc:  0.890625
train loss:  0.3003910183906555
train gradient:  0.08876017539102632
iteration : 11429
train acc:  0.890625
train loss:  0.31250908970832825
train gradient:  0.14811121387239773
iteration : 11430
train acc:  0.8515625
train loss:  0.40490686893463135
train gradient:  0.24530889705053835
iteration : 11431
train acc:  0.859375
train loss:  0.3286325931549072
train gradient:  0.19787623627889828
iteration : 11432
train acc:  0.859375
train loss:  0.29944294691085815
train gradient:  0.15352545236863116
iteration : 11433
train acc:  0.8671875
train loss:  0.2793946862220764
train gradient:  0.08863119713516113
iteration : 11434
train acc:  0.875
train loss:  0.3266754150390625
train gradient:  0.13054847306993547
iteration : 11435
train acc:  0.890625
train loss:  0.2818087935447693
train gradient:  0.16650989547490497
iteration : 11436
train acc:  0.8359375
train loss:  0.3283412456512451
train gradient:  0.15933813440439928
iteration : 11437
train acc:  0.84375
train loss:  0.34324267506599426
train gradient:  0.15954594846890505
iteration : 11438
train acc:  0.8359375
train loss:  0.4304594397544861
train gradient:  0.19555658776881388
iteration : 11439
train acc:  0.8359375
train loss:  0.35723814368247986
train gradient:  0.11177968069135144
iteration : 11440
train acc:  0.8046875
train loss:  0.4189602732658386
train gradient:  0.20915311335112913
iteration : 11441
train acc:  0.8515625
train loss:  0.28270384669303894
train gradient:  0.10886015098963062
iteration : 11442
train acc:  0.8125
train loss:  0.3879423439502716
train gradient:  0.17999342764921153
iteration : 11443
train acc:  0.796875
train loss:  0.4201914668083191
train gradient:  0.1996302631712793
iteration : 11444
train acc:  0.8359375
train loss:  0.3467640280723572
train gradient:  0.15045847026454326
iteration : 11445
train acc:  0.875
train loss:  0.2796480357646942
train gradient:  0.1699569148018114
iteration : 11446
train acc:  0.8359375
train loss:  0.3647044897079468
train gradient:  0.17151549524126397
iteration : 11447
train acc:  0.8359375
train loss:  0.302925169467926
train gradient:  0.12594616442864776
iteration : 11448
train acc:  0.8203125
train loss:  0.3340266942977905
train gradient:  0.13290143310959757
iteration : 11449
train acc:  0.8359375
train loss:  0.3217242360115051
train gradient:  0.10756798114192515
iteration : 11450
train acc:  0.8359375
train loss:  0.34884047508239746
train gradient:  0.14870594726345784
iteration : 11451
train acc:  0.859375
train loss:  0.31335127353668213
train gradient:  0.13709465369010349
iteration : 11452
train acc:  0.8984375
train loss:  0.2389724999666214
train gradient:  0.11654537869940217
iteration : 11453
train acc:  0.8984375
train loss:  0.28185975551605225
train gradient:  0.10697116943621561
iteration : 11454
train acc:  0.921875
train loss:  0.25910311937332153
train gradient:  0.10336020875140133
iteration : 11455
train acc:  0.8828125
train loss:  0.2939551770687103
train gradient:  0.14805545522336636
iteration : 11456
train acc:  0.90625
train loss:  0.23346024751663208
train gradient:  0.07727757126095142
iteration : 11457
train acc:  0.8671875
train loss:  0.29562002420425415
train gradient:  0.10444863789300896
iteration : 11458
train acc:  0.8671875
train loss:  0.2814292311668396
train gradient:  0.0883361145289433
iteration : 11459
train acc:  0.84375
train loss:  0.3370714783668518
train gradient:  0.1343575946536687
iteration : 11460
train acc:  0.8515625
train loss:  0.30827897787094116
train gradient:  0.17627252710260827
iteration : 11461
train acc:  0.8828125
train loss:  0.26550495624542236
train gradient:  0.1707854675822184
iteration : 11462
train acc:  0.8125
train loss:  0.3540689945220947
train gradient:  0.17107229306479801
iteration : 11463
train acc:  0.875
train loss:  0.3115025460720062
train gradient:  0.15683356713226204
iteration : 11464
train acc:  0.84375
train loss:  0.353333055973053
train gradient:  0.15363998228547343
iteration : 11465
train acc:  0.8671875
train loss:  0.28571414947509766
train gradient:  0.11524288215201735
iteration : 11466
train acc:  0.8984375
train loss:  0.3074932098388672
train gradient:  0.17768715517492356
iteration : 11467
train acc:  0.8515625
train loss:  0.33291876316070557
train gradient:  0.13168033815937688
iteration : 11468
train acc:  0.8515625
train loss:  0.32718560099601746
train gradient:  0.1880234195535255
iteration : 11469
train acc:  0.875
train loss:  0.28345543146133423
train gradient:  0.15633991185678314
iteration : 11470
train acc:  0.875
train loss:  0.3180954158306122
train gradient:  0.10911837988452303
iteration : 11471
train acc:  0.890625
train loss:  0.26207756996154785
train gradient:  0.25800948271178764
iteration : 11472
train acc:  0.8515625
train loss:  0.30385929346084595
train gradient:  0.14943685996209927
iteration : 11473
train acc:  0.859375
train loss:  0.3454723358154297
train gradient:  0.1589253598480124
iteration : 11474
train acc:  0.9140625
train loss:  0.2180446982383728
train gradient:  0.1052801077433505
iteration : 11475
train acc:  0.8125
train loss:  0.3691020905971527
train gradient:  0.17100981143134653
iteration : 11476
train acc:  0.859375
train loss:  0.329889178276062
train gradient:  0.1391136526865431
iteration : 11477
train acc:  0.875
train loss:  0.3232726454734802
train gradient:  0.22756052186347764
iteration : 11478
train acc:  0.8125
train loss:  0.36187729239463806
train gradient:  0.17199628087422109
iteration : 11479
train acc:  0.78125
train loss:  0.5006301403045654
train gradient:  0.3286117051243729
iteration : 11480
train acc:  0.859375
train loss:  0.370150625705719
train gradient:  0.17876847546918162
iteration : 11481
train acc:  0.859375
train loss:  0.41188734769821167
train gradient:  0.17642161297530917
iteration : 11482
train acc:  0.8671875
train loss:  0.32852548360824585
train gradient:  0.16372405301162726
iteration : 11483
train acc:  0.859375
train loss:  0.3528977930545807
train gradient:  0.13535283511775204
iteration : 11484
train acc:  0.9296875
train loss:  0.21692906320095062
train gradient:  0.07271353861531114
iteration : 11485
train acc:  0.828125
train loss:  0.33257219195365906
train gradient:  0.1957692612581401
iteration : 11486
train acc:  0.8671875
train loss:  0.27546119689941406
train gradient:  0.13026648903848365
iteration : 11487
train acc:  0.859375
train loss:  0.33258986473083496
train gradient:  0.11423407972252099
iteration : 11488
train acc:  0.9140625
train loss:  0.23051391541957855
train gradient:  0.10966593184363488
iteration : 11489
train acc:  0.8359375
train loss:  0.37935930490493774
train gradient:  0.19095423552956864
iteration : 11490
train acc:  0.8359375
train loss:  0.3029592037200928
train gradient:  0.1393076939981411
iteration : 11491
train acc:  0.8984375
train loss:  0.2703495919704437
train gradient:  0.10266247358997932
iteration : 11492
train acc:  0.8671875
train loss:  0.300849974155426
train gradient:  0.12629039162803646
iteration : 11493
train acc:  0.8671875
train loss:  0.31987449526786804
train gradient:  0.11633512908383062
iteration : 11494
train acc:  0.8828125
train loss:  0.2718345522880554
train gradient:  0.0919985755319666
iteration : 11495
train acc:  0.8359375
train loss:  0.3338562250137329
train gradient:  0.12198787245459985
iteration : 11496
train acc:  0.875
train loss:  0.29024070501327515
train gradient:  0.1106430268582787
iteration : 11497
train acc:  0.8671875
train loss:  0.3294013738632202
train gradient:  0.14387998337139324
iteration : 11498
train acc:  0.8984375
train loss:  0.2591862082481384
train gradient:  0.14963851632601832
iteration : 11499
train acc:  0.8515625
train loss:  0.30544525384902954
train gradient:  0.1161549469622914
iteration : 11500
train acc:  0.8125
train loss:  0.3578290343284607
train gradient:  0.16404228997134246
iteration : 11501
train acc:  0.8515625
train loss:  0.31823524832725525
train gradient:  0.11077567183696173
iteration : 11502
train acc:  0.921875
train loss:  0.2618681788444519
train gradient:  0.1284755305584167
iteration : 11503
train acc:  0.84375
train loss:  0.3932402431964874
train gradient:  0.27440323478352796
iteration : 11504
train acc:  0.8203125
train loss:  0.32959794998168945
train gradient:  0.2548762745890482
iteration : 11505
train acc:  0.9296875
train loss:  0.23104314506053925
train gradient:  0.07468277413396883
iteration : 11506
train acc:  0.8671875
train loss:  0.3275858163833618
train gradient:  0.13504710571194795
iteration : 11507
train acc:  0.8828125
train loss:  0.38295334577560425
train gradient:  0.14752113582663318
iteration : 11508
train acc:  0.8984375
train loss:  0.2814021110534668
train gradient:  0.12364566451141437
iteration : 11509
train acc:  0.8359375
train loss:  0.3152823746204376
train gradient:  0.1388499291616257
iteration : 11510
train acc:  0.7734375
train loss:  0.44501692056655884
train gradient:  0.2588196723525207
iteration : 11511
train acc:  0.875
train loss:  0.25683313608169556
train gradient:  0.0968772136691198
iteration : 11512
train acc:  0.84375
train loss:  0.35577255487442017
train gradient:  0.28710989694636535
iteration : 11513
train acc:  0.8828125
train loss:  0.254092276096344
train gradient:  0.1286868243887332
iteration : 11514
train acc:  0.84375
train loss:  0.29982319474220276
train gradient:  0.19585930624193185
iteration : 11515
train acc:  0.84375
train loss:  0.3383726179599762
train gradient:  0.18950506211652018
iteration : 11516
train acc:  0.921875
train loss:  0.24608738720417023
train gradient:  0.13926397418084324
iteration : 11517
train acc:  0.8203125
train loss:  0.36022430658340454
train gradient:  0.1867300438472126
iteration : 11518
train acc:  0.8984375
train loss:  0.23859083652496338
train gradient:  0.11917604751884886
iteration : 11519
train acc:  0.875
train loss:  0.35026705265045166
train gradient:  0.15423634116832347
iteration : 11520
train acc:  0.8515625
train loss:  0.33171170949935913
train gradient:  0.18251068894863404
iteration : 11521
train acc:  0.859375
train loss:  0.33436959981918335
train gradient:  0.1544962885019966
iteration : 11522
train acc:  0.921875
train loss:  0.2556450366973877
train gradient:  0.10852139614016681
iteration : 11523
train acc:  0.8515625
train loss:  0.33228930830955505
train gradient:  0.15383298736411946
iteration : 11524
train acc:  0.875
train loss:  0.29184040427207947
train gradient:  0.14744449756546169
iteration : 11525
train acc:  0.8359375
train loss:  0.4140673577785492
train gradient:  0.2538693931036443
iteration : 11526
train acc:  0.828125
train loss:  0.3872373402118683
train gradient:  0.16468810272413364
iteration : 11527
train acc:  0.921875
train loss:  0.2667280435562134
train gradient:  0.19882483960848077
iteration : 11528
train acc:  0.84375
train loss:  0.3425712585449219
train gradient:  0.19849381135541244
iteration : 11529
train acc:  0.8515625
train loss:  0.3289869725704193
train gradient:  0.13152355676828006
iteration : 11530
train acc:  0.859375
train loss:  0.27916979789733887
train gradient:  0.07871382660035622
iteration : 11531
train acc:  0.8515625
train loss:  0.293692409992218
train gradient:  0.13505320848016394
iteration : 11532
train acc:  0.8359375
train loss:  0.34760889410972595
train gradient:  0.13209529416691929
iteration : 11533
train acc:  0.84375
train loss:  0.326791912317276
train gradient:  0.17208553987591835
iteration : 11534
train acc:  0.8515625
train loss:  0.35097718238830566
train gradient:  0.14741883125303806
iteration : 11535
train acc:  0.8515625
train loss:  0.29775291681289673
train gradient:  0.19990849631599794
iteration : 11536
train acc:  0.84375
train loss:  0.34259194135665894
train gradient:  0.22880807493755417
iteration : 11537
train acc:  0.859375
train loss:  0.29535558819770813
train gradient:  0.11328668073064344
iteration : 11538
train acc:  0.875
train loss:  0.3076510429382324
train gradient:  0.13572708142058687
iteration : 11539
train acc:  0.8984375
train loss:  0.2975669205188751
train gradient:  0.12370432468533264
iteration : 11540
train acc:  0.875
train loss:  0.2943466305732727
train gradient:  0.11391675983465502
iteration : 11541
train acc:  0.875
train loss:  0.28306594491004944
train gradient:  0.12929017350264826
iteration : 11542
train acc:  0.921875
train loss:  0.23978202044963837
train gradient:  0.09106876803065041
iteration : 11543
train acc:  0.828125
train loss:  0.3364356756210327
train gradient:  0.15606452076246646
iteration : 11544
train acc:  0.859375
train loss:  0.2993662655353546
train gradient:  0.13425433464266026
iteration : 11545
train acc:  0.8671875
train loss:  0.3441830277442932
train gradient:  0.10903702908289048
iteration : 11546
train acc:  0.8984375
train loss:  0.2992243468761444
train gradient:  0.1094675724694576
iteration : 11547
train acc:  0.84375
train loss:  0.35752564668655396
train gradient:  0.62033144465964
iteration : 11548
train acc:  0.8359375
train loss:  0.39980095624923706
train gradient:  0.2876639386938474
iteration : 11549
train acc:  0.8515625
train loss:  0.3342510759830475
train gradient:  0.13656725012449947
iteration : 11550
train acc:  0.890625
train loss:  0.28758591413497925
train gradient:  0.10224665000807268
iteration : 11551
train acc:  0.8671875
train loss:  0.3048824667930603
train gradient:  0.14023069754767642
iteration : 11552
train acc:  0.875
train loss:  0.347148060798645
train gradient:  0.16673035360463265
iteration : 11553
train acc:  0.8671875
train loss:  0.30860719084739685
train gradient:  0.16481079221572173
iteration : 11554
train acc:  0.828125
train loss:  0.34659522771835327
train gradient:  0.15217634701584676
iteration : 11555
train acc:  0.859375
train loss:  0.3441307842731476
train gradient:  0.14943162400534615
iteration : 11556
train acc:  0.8671875
train loss:  0.296326220035553
train gradient:  0.12281940172409582
iteration : 11557
train acc:  0.890625
train loss:  0.24209634959697723
train gradient:  0.09438051117669755
iteration : 11558
train acc:  0.8828125
train loss:  0.3438160717487335
train gradient:  0.2043843055864101
iteration : 11559
train acc:  0.8828125
train loss:  0.3393491506576538
train gradient:  0.1696854101438799
iteration : 11560
train acc:  0.828125
train loss:  0.37130922079086304
train gradient:  0.167750362893974
iteration : 11561
train acc:  0.84375
train loss:  0.3359518349170685
train gradient:  0.1946630269772623
iteration : 11562
train acc:  0.8671875
train loss:  0.3074467182159424
train gradient:  0.12922430170062355
iteration : 11563
train acc:  0.8671875
train loss:  0.2568753659725189
train gradient:  0.07732800265348663
iteration : 11564
train acc:  0.8515625
train loss:  0.3345329165458679
train gradient:  0.15603128168996366
iteration : 11565
train acc:  0.859375
train loss:  0.3238121271133423
train gradient:  0.1573102465411425
iteration : 11566
train acc:  0.8359375
train loss:  0.407338947057724
train gradient:  0.2167107517020423
iteration : 11567
train acc:  0.8515625
train loss:  0.3130265176296234
train gradient:  0.15127379286538636
iteration : 11568
train acc:  0.859375
train loss:  0.2986133098602295
train gradient:  0.17622644878739208
iteration : 11569
train acc:  0.8828125
train loss:  0.29305028915405273
train gradient:  0.12110552287879485
iteration : 11570
train acc:  0.890625
train loss:  0.31292277574539185
train gradient:  0.13845752330498168
iteration : 11571
train acc:  0.8828125
train loss:  0.3352145552635193
train gradient:  0.17520840025400902
iteration : 11572
train acc:  0.875
train loss:  0.2678323984146118
train gradient:  0.1531378709033114
iteration : 11573
train acc:  0.7890625
train loss:  0.3912128210067749
train gradient:  0.27144607773416685
iteration : 11574
train acc:  0.890625
train loss:  0.36205530166625977
train gradient:  0.1491978149941287
iteration : 11575
train acc:  0.84375
train loss:  0.3724173307418823
train gradient:  0.1765857068831561
iteration : 11576
train acc:  0.8984375
train loss:  0.3214167058467865
train gradient:  0.15850193110621963
iteration : 11577
train acc:  0.859375
train loss:  0.34006306529045105
train gradient:  0.23738397046862975
iteration : 11578
train acc:  0.875
train loss:  0.2963257431983948
train gradient:  0.1911213187104827
iteration : 11579
train acc:  0.8671875
train loss:  0.34154653549194336
train gradient:  0.09880283046568393
iteration : 11580
train acc:  0.828125
train loss:  0.3275240361690521
train gradient:  0.1322462331190798
iteration : 11581
train acc:  0.8671875
train loss:  0.285715252161026
train gradient:  0.1310093128180181
iteration : 11582
train acc:  0.8203125
train loss:  0.4038825035095215
train gradient:  0.27557279816196206
iteration : 11583
train acc:  0.859375
train loss:  0.30277925729751587
train gradient:  0.11117351870378638
iteration : 11584
train acc:  0.875
train loss:  0.3076487183570862
train gradient:  0.16074193847307475
iteration : 11585
train acc:  0.8359375
train loss:  0.3413228988647461
train gradient:  0.12548686491112448
iteration : 11586
train acc:  0.859375
train loss:  0.28292810916900635
train gradient:  0.1543863602334043
iteration : 11587
train acc:  0.8671875
train loss:  0.31054407358169556
train gradient:  0.1715718421763648
iteration : 11588
train acc:  0.8671875
train loss:  0.3241395950317383
train gradient:  0.2763165766733033
iteration : 11589
train acc:  0.90625
train loss:  0.28643399477005005
train gradient:  0.12981835912902007
iteration : 11590
train acc:  0.859375
train loss:  0.27680280804634094
train gradient:  0.10451044151423589
iteration : 11591
train acc:  0.8125
train loss:  0.3898842930793762
train gradient:  0.1251172584908286
iteration : 11592
train acc:  0.890625
train loss:  0.28158774971961975
train gradient:  0.0984350818230242
iteration : 11593
train acc:  0.890625
train loss:  0.28483259677886963
train gradient:  0.13957045980199395
iteration : 11594
train acc:  0.8515625
train loss:  0.2651250958442688
train gradient:  0.09204345039985914
iteration : 11595
train acc:  0.8359375
train loss:  0.3394879698753357
train gradient:  0.15421185715257482
iteration : 11596
train acc:  0.8125
train loss:  0.3956703841686249
train gradient:  0.20581804815077184
iteration : 11597
train acc:  0.796875
train loss:  0.36176708340644836
train gradient:  0.2001845499928349
iteration : 11598
train acc:  0.8828125
train loss:  0.3554500937461853
train gradient:  0.1767635836142803
iteration : 11599
train acc:  0.8828125
train loss:  0.2925626039505005
train gradient:  0.11868366285124866
iteration : 11600
train acc:  0.7890625
train loss:  0.3798373341560364
train gradient:  0.21256464042657203
iteration : 11601
train acc:  0.8515625
train loss:  0.33531737327575684
train gradient:  0.12279991803842313
iteration : 11602
train acc:  0.84375
train loss:  0.32882750034332275
train gradient:  0.148791345521532
iteration : 11603
train acc:  0.828125
train loss:  0.35707640647888184
train gradient:  0.22312929773747725
iteration : 11604
train acc:  0.8671875
train loss:  0.36814290285110474
train gradient:  0.2052673487656289
iteration : 11605
train acc:  0.84375
train loss:  0.35455387830734253
train gradient:  0.1523191297512547
iteration : 11606
train acc:  0.90625
train loss:  0.25179585814476013
train gradient:  0.10280532000130879
iteration : 11607
train acc:  0.8828125
train loss:  0.28397417068481445
train gradient:  0.13318458829237528
iteration : 11608
train acc:  0.875
train loss:  0.33258360624313354
train gradient:  0.20392508080893917
iteration : 11609
train acc:  0.8671875
train loss:  0.30790477991104126
train gradient:  0.13030271651843917
iteration : 11610
train acc:  0.875
train loss:  0.30274444818496704
train gradient:  0.12967581119273325
iteration : 11611
train acc:  0.84375
train loss:  0.3610464930534363
train gradient:  0.1644286327060492
iteration : 11612
train acc:  0.8671875
train loss:  0.29535698890686035
train gradient:  0.14080242793638215
iteration : 11613
train acc:  0.9140625
train loss:  0.28253844380378723
train gradient:  0.22052175034451327
iteration : 11614
train acc:  0.875
train loss:  0.32157641649246216
train gradient:  0.16658932403809734
iteration : 11615
train acc:  0.8671875
train loss:  0.3052413761615753
train gradient:  0.11383026663815168
iteration : 11616
train acc:  0.875
train loss:  0.33230701088905334
train gradient:  0.16217883935790206
iteration : 11617
train acc:  0.8984375
train loss:  0.25864431262016296
train gradient:  0.09358846438561827
iteration : 11618
train acc:  0.8515625
train loss:  0.31815028190612793
train gradient:  0.08866311548047501
iteration : 11619
train acc:  0.859375
train loss:  0.3268416225910187
train gradient:  0.13824435279168318
iteration : 11620
train acc:  0.8671875
train loss:  0.366239070892334
train gradient:  0.17507709731750354
iteration : 11621
train acc:  0.8359375
train loss:  0.3284911811351776
train gradient:  0.2064933134790965
iteration : 11622
train acc:  0.890625
train loss:  0.30721205472946167
train gradient:  0.1252340508028006
iteration : 11623
train acc:  0.8125
train loss:  0.3571488857269287
train gradient:  0.17684037079835097
iteration : 11624
train acc:  0.8515625
train loss:  0.34198397397994995
train gradient:  0.14440955006207853
iteration : 11625
train acc:  0.890625
train loss:  0.3133412003517151
train gradient:  0.11806421745124442
iteration : 11626
train acc:  0.8671875
train loss:  0.2792712450027466
train gradient:  0.11947382785929422
iteration : 11627
train acc:  0.8046875
train loss:  0.41125044226646423
train gradient:  0.22525246730611254
iteration : 11628
train acc:  0.9140625
train loss:  0.24330276250839233
train gradient:  0.07990480287820734
iteration : 11629
train acc:  0.875
train loss:  0.31753307580947876
train gradient:  0.1063106381965721
iteration : 11630
train acc:  0.890625
train loss:  0.2568437457084656
train gradient:  0.07880217274499302
iteration : 11631
train acc:  0.890625
train loss:  0.24325892329216003
train gradient:  0.09778088013797337
iteration : 11632
train acc:  0.828125
train loss:  0.3711709976196289
train gradient:  0.15197712331498225
iteration : 11633
train acc:  0.8125
train loss:  0.28892940282821655
train gradient:  0.1448999896748717
iteration : 11634
train acc:  0.828125
train loss:  0.4095100164413452
train gradient:  0.23139784762402751
iteration : 11635
train acc:  0.8828125
train loss:  0.30945247411727905
train gradient:  0.15705425881683743
iteration : 11636
train acc:  0.859375
train loss:  0.34144484996795654
train gradient:  0.14083688338122496
iteration : 11637
train acc:  0.8359375
train loss:  0.33625558018684387
train gradient:  0.1259715031581795
iteration : 11638
train acc:  0.859375
train loss:  0.2630273699760437
train gradient:  0.09401765286705782
iteration : 11639
train acc:  0.8984375
train loss:  0.26330140233039856
train gradient:  0.13134166703690275
iteration : 11640
train acc:  0.8828125
train loss:  0.2999759912490845
train gradient:  0.11159854960932755
iteration : 11641
train acc:  0.7890625
train loss:  0.39744919538497925
train gradient:  0.1800829789649111
iteration : 11642
train acc:  0.8125
train loss:  0.3551485538482666
train gradient:  0.15872567526548975
iteration : 11643
train acc:  0.890625
train loss:  0.26751095056533813
train gradient:  0.10200526787923624
iteration : 11644
train acc:  0.8515625
train loss:  0.3637036085128784
train gradient:  0.263505239162719
iteration : 11645
train acc:  0.8828125
train loss:  0.32657140493392944
train gradient:  0.12748776773312764
iteration : 11646
train acc:  0.8828125
train loss:  0.28783851861953735
train gradient:  0.13219856502170607
iteration : 11647
train acc:  0.859375
train loss:  0.3850911259651184
train gradient:  0.17516404162341767
iteration : 11648
train acc:  0.8046875
train loss:  0.4115057587623596
train gradient:  0.2537860756978736
iteration : 11649
train acc:  0.8515625
train loss:  0.32194194197654724
train gradient:  0.20691283649178352
iteration : 11650
train acc:  0.875
train loss:  0.25970354676246643
train gradient:  0.0790366749738315
iteration : 11651
train acc:  0.796875
train loss:  0.3637867569923401
train gradient:  0.11847211560041891
iteration : 11652
train acc:  0.8515625
train loss:  0.323779821395874
train gradient:  0.13400866942611783
iteration : 11653
train acc:  0.859375
train loss:  0.27821460366249084
train gradient:  0.10865248416206358
iteration : 11654
train acc:  0.8046875
train loss:  0.37604671716690063
train gradient:  0.17493518243044812
iteration : 11655
train acc:  0.8828125
train loss:  0.2895110845565796
train gradient:  0.14109884079078241
iteration : 11656
train acc:  0.8671875
train loss:  0.2644847333431244
train gradient:  0.10978123043009858
iteration : 11657
train acc:  0.8671875
train loss:  0.35027626156806946
train gradient:  0.1712516811257831
iteration : 11658
train acc:  0.8125
train loss:  0.32506948709487915
train gradient:  0.14327404099045055
iteration : 11659
train acc:  0.8046875
train loss:  0.366594135761261
train gradient:  0.16665765730736476
iteration : 11660
train acc:  0.84375
train loss:  0.3046014904975891
train gradient:  0.15576987227833694
iteration : 11661
train acc:  0.8515625
train loss:  0.2969825863838196
train gradient:  0.10646050813508694
iteration : 11662
train acc:  0.84375
train loss:  0.3593651056289673
train gradient:  0.24614450259342774
iteration : 11663
train acc:  0.9140625
train loss:  0.25306275486946106
train gradient:  0.10819925374913382
iteration : 11664
train acc:  0.8515625
train loss:  0.3020629286766052
train gradient:  0.16239513806525663
iteration : 11665
train acc:  0.859375
train loss:  0.303244411945343
train gradient:  0.13956016998976545
iteration : 11666
train acc:  0.90625
train loss:  0.28457164764404297
train gradient:  0.1015264108703926
iteration : 11667
train acc:  0.796875
train loss:  0.39858299493789673
train gradient:  0.16815594550946247
iteration : 11668
train acc:  0.890625
train loss:  0.2774297595024109
train gradient:  0.10331672321660255
iteration : 11669
train acc:  0.8359375
train loss:  0.33964258432388306
train gradient:  0.13014020031409426
iteration : 11670
train acc:  0.84375
train loss:  0.37195783853530884
train gradient:  0.16365685199296873
iteration : 11671
train acc:  0.828125
train loss:  0.3529462516307831
train gradient:  0.1518345677846662
iteration : 11672
train acc:  0.84375
train loss:  0.36522525548934937
train gradient:  0.22494929356740556
iteration : 11673
train acc:  0.8515625
train loss:  0.3779340386390686
train gradient:  0.253049638635697
iteration : 11674
train acc:  0.875
train loss:  0.30839166045188904
train gradient:  0.1175587454692057
iteration : 11675
train acc:  0.8828125
train loss:  0.25219184160232544
train gradient:  0.0822669765452596
iteration : 11676
train acc:  0.890625
train loss:  0.3035310208797455
train gradient:  0.10581734330450902
iteration : 11677
train acc:  0.8984375
train loss:  0.25628483295440674
train gradient:  0.12534922319711947
iteration : 11678
train acc:  0.8984375
train loss:  0.26987770199775696
train gradient:  0.0972113147645207
iteration : 11679
train acc:  0.8203125
train loss:  0.39432287216186523
train gradient:  0.2028437005189329
iteration : 11680
train acc:  0.8828125
train loss:  0.23829469084739685
train gradient:  0.08083302391825668
iteration : 11681
train acc:  0.890625
train loss:  0.2685675621032715
train gradient:  0.09560071766360892
iteration : 11682
train acc:  0.859375
train loss:  0.34762316942214966
train gradient:  0.21531271779257993
iteration : 11683
train acc:  0.890625
train loss:  0.27992841601371765
train gradient:  0.10690476905674895
iteration : 11684
train acc:  0.859375
train loss:  0.32287317514419556
train gradient:  0.1355368707991162
iteration : 11685
train acc:  0.8359375
train loss:  0.3755151629447937
train gradient:  0.12755208801313134
iteration : 11686
train acc:  0.84375
train loss:  0.3182581961154938
train gradient:  0.21769151909387283
iteration : 11687
train acc:  0.890625
train loss:  0.273059219121933
train gradient:  0.11575900677806801
iteration : 11688
train acc:  0.8515625
train loss:  0.39174753427505493
train gradient:  0.17389372387292804
iteration : 11689
train acc:  0.8515625
train loss:  0.3538472056388855
train gradient:  0.18480287521639127
iteration : 11690
train acc:  0.8359375
train loss:  0.3112119436264038
train gradient:  0.12224955508951717
iteration : 11691
train acc:  0.8203125
train loss:  0.3242284655570984
train gradient:  0.12579572681007906
iteration : 11692
train acc:  0.859375
train loss:  0.3226863145828247
train gradient:  0.17711009984465914
iteration : 11693
train acc:  0.890625
train loss:  0.3048572838306427
train gradient:  0.17182203664850892
iteration : 11694
train acc:  0.875
train loss:  0.3002817630767822
train gradient:  0.1852507785447812
iteration : 11695
train acc:  0.8359375
train loss:  0.38011452555656433
train gradient:  0.2799457736061566
iteration : 11696
train acc:  0.8671875
train loss:  0.2963070869445801
train gradient:  0.11744412935083648
iteration : 11697
train acc:  0.90625
train loss:  0.28043627738952637
train gradient:  0.16224957253119562
iteration : 11698
train acc:  0.8203125
train loss:  0.41878455877304077
train gradient:  0.1649789659570194
iteration : 11699
train acc:  0.8515625
train loss:  0.34757673740386963
train gradient:  0.10632217540295158
iteration : 11700
train acc:  0.859375
train loss:  0.3681052327156067
train gradient:  0.18569305392342572
iteration : 11701
train acc:  0.828125
train loss:  0.3440347909927368
train gradient:  0.14880681720630376
iteration : 11702
train acc:  0.8515625
train loss:  0.28357696533203125
train gradient:  0.07903500372153442
iteration : 11703
train acc:  0.890625
train loss:  0.29332640767097473
train gradient:  0.1020118481169818
iteration : 11704
train acc:  0.8671875
train loss:  0.3836864233016968
train gradient:  0.23805148434821843
iteration : 11705
train acc:  0.890625
train loss:  0.27649539709091187
train gradient:  0.10501114137908667
iteration : 11706
train acc:  0.8828125
train loss:  0.3597407042980194
train gradient:  0.12045236086765547
iteration : 11707
train acc:  0.90625
train loss:  0.2641592025756836
train gradient:  0.11949743473397809
iteration : 11708
train acc:  0.8359375
train loss:  0.397624671459198
train gradient:  0.2224598093511873
iteration : 11709
train acc:  0.8515625
train loss:  0.31459468603134155
train gradient:  0.10423570359871641
iteration : 11710
train acc:  0.84375
train loss:  0.3482457995414734
train gradient:  0.10826925827280391
iteration : 11711
train acc:  0.9296875
train loss:  0.23755395412445068
train gradient:  0.06349383442475472
iteration : 11712
train acc:  0.875
train loss:  0.34450310468673706
train gradient:  0.13267573747912614
iteration : 11713
train acc:  0.8203125
train loss:  0.3750308156013489
train gradient:  0.18783725537967355
iteration : 11714
train acc:  0.8515625
train loss:  0.34465694427490234
train gradient:  0.14656598801892656
iteration : 11715
train acc:  0.859375
train loss:  0.3022751212120056
train gradient:  0.11053023110466001
iteration : 11716
train acc:  0.8359375
train loss:  0.32927387952804565
train gradient:  0.14357145189775702
iteration : 11717
train acc:  0.875
train loss:  0.2825232744216919
train gradient:  0.14177688469646357
iteration : 11718
train acc:  0.8359375
train loss:  0.36200058460235596
train gradient:  0.2326319333657605
iteration : 11719
train acc:  0.8828125
train loss:  0.2648161053657532
train gradient:  0.1060713147008191
iteration : 11720
train acc:  0.859375
train loss:  0.30342987179756165
train gradient:  0.13497959013438485
iteration : 11721
train acc:  0.8203125
train loss:  0.39505332708358765
train gradient:  0.18053093060861447
iteration : 11722
train acc:  0.8125
train loss:  0.4117070436477661
train gradient:  0.2026502355828244
iteration : 11723
train acc:  0.859375
train loss:  0.3477379083633423
train gradient:  0.16840837928761146
iteration : 11724
train acc:  0.875
train loss:  0.3462681770324707
train gradient:  0.1558164710561144
iteration : 11725
train acc:  0.8203125
train loss:  0.4025990068912506
train gradient:  0.21793549440672294
iteration : 11726
train acc:  0.8359375
train loss:  0.35031604766845703
train gradient:  0.17404564630098435
iteration : 11727
train acc:  0.8515625
train loss:  0.3010740280151367
train gradient:  0.13389050254947585
iteration : 11728
train acc:  0.875
train loss:  0.3146181106567383
train gradient:  0.17169205355991662
iteration : 11729
train acc:  0.84375
train loss:  0.2966056168079376
train gradient:  0.149544573819198
iteration : 11730
train acc:  0.828125
train loss:  0.366357684135437
train gradient:  0.20891183609462266
iteration : 11731
train acc:  0.8515625
train loss:  0.3615614175796509
train gradient:  0.19606927956734543
iteration : 11732
train acc:  0.84375
train loss:  0.3637014627456665
train gradient:  0.1296266662424439
iteration : 11733
train acc:  0.8671875
train loss:  0.32013240456581116
train gradient:  0.14766723537299273
iteration : 11734
train acc:  0.9375
train loss:  0.22061379253864288
train gradient:  0.13668389880183351
iteration : 11735
train acc:  0.84375
train loss:  0.40625372529029846
train gradient:  0.3798991766171187
iteration : 11736
train acc:  0.8125
train loss:  0.31947433948516846
train gradient:  0.11341948735363928
iteration : 11737
train acc:  0.8359375
train loss:  0.3523913323879242
train gradient:  0.15658641127697562
iteration : 11738
train acc:  0.875
train loss:  0.28590232133865356
train gradient:  0.08304089800852077
iteration : 11739
train acc:  0.8828125
train loss:  0.2957441210746765
train gradient:  0.12603659007641177
iteration : 11740
train acc:  0.8671875
train loss:  0.3096041977405548
train gradient:  0.12758414841567806
iteration : 11741
train acc:  0.8671875
train loss:  0.35501161217689514
train gradient:  0.18104914903669558
iteration : 11742
train acc:  0.859375
train loss:  0.3354586064815521
train gradient:  0.11940422802941769
iteration : 11743
train acc:  0.8984375
train loss:  0.2734019160270691
train gradient:  0.08876561506587614
iteration : 11744
train acc:  0.875
train loss:  0.28559574484825134
train gradient:  0.10533951268843873
iteration : 11745
train acc:  0.8203125
train loss:  0.37599635124206543
train gradient:  0.1887920059180111
iteration : 11746
train acc:  0.8671875
train loss:  0.31133896112442017
train gradient:  0.12778943563747638
iteration : 11747
train acc:  0.84375
train loss:  0.3104296922683716
train gradient:  0.11659988051110755
iteration : 11748
train acc:  0.890625
train loss:  0.2840791940689087
train gradient:  0.11938059063674956
iteration : 11749
train acc:  0.921875
train loss:  0.23315370082855225
train gradient:  0.07930756344188304
iteration : 11750
train acc:  0.8515625
train loss:  0.310693621635437
train gradient:  0.1264229249994731
iteration : 11751
train acc:  0.8515625
train loss:  0.3165314197540283
train gradient:  0.1158884903982253
iteration : 11752
train acc:  0.8984375
train loss:  0.2542790174484253
train gradient:  0.09780058944224466
iteration : 11753
train acc:  0.828125
train loss:  0.35032081604003906
train gradient:  0.18579975656527814
iteration : 11754
train acc:  0.875
train loss:  0.3505377769470215
train gradient:  0.13966572923847484
iteration : 11755
train acc:  0.8359375
train loss:  0.35441792011260986
train gradient:  0.1420636881324076
iteration : 11756
train acc:  0.8515625
train loss:  0.38572150468826294
train gradient:  0.18723870047023444
iteration : 11757
train acc:  0.8125
train loss:  0.3709220290184021
train gradient:  0.19281523977502352
iteration : 11758
train acc:  0.8515625
train loss:  0.3893449902534485
train gradient:  0.18385018808327586
iteration : 11759
train acc:  0.859375
train loss:  0.32531416416168213
train gradient:  0.11385030654644436
iteration : 11760
train acc:  0.8359375
train loss:  0.3380909860134125
train gradient:  0.16712048431960075
iteration : 11761
train acc:  0.859375
train loss:  0.33885347843170166
train gradient:  0.12455230719905139
iteration : 11762
train acc:  0.8515625
train loss:  0.3377184271812439
train gradient:  0.166123946095122
iteration : 11763
train acc:  0.9140625
train loss:  0.27959856390953064
train gradient:  0.14832308721664006
iteration : 11764
train acc:  0.8828125
train loss:  0.2643298804759979
train gradient:  0.123828854417014
iteration : 11765
train acc:  0.8515625
train loss:  0.3613503575325012
train gradient:  0.14557953082592592
iteration : 11766
train acc:  0.890625
train loss:  0.2854691743850708
train gradient:  0.12701234785043317
iteration : 11767
train acc:  0.828125
train loss:  0.33096909523010254
train gradient:  0.1217798237262424
iteration : 11768
train acc:  0.8203125
train loss:  0.36421626806259155
train gradient:  0.1826760241985001
iteration : 11769
train acc:  0.8671875
train loss:  0.33339089155197144
train gradient:  0.1284713195868445
iteration : 11770
train acc:  0.90625
train loss:  0.258281409740448
train gradient:  0.113308318723969
iteration : 11771
train acc:  0.921875
train loss:  0.22378253936767578
train gradient:  0.07338691209124262
iteration : 11772
train acc:  0.8984375
train loss:  0.2522374093532562
train gradient:  0.09283953046337026
iteration : 11773
train acc:  0.859375
train loss:  0.3207881450653076
train gradient:  0.1272849377300197
iteration : 11774
train acc:  0.8359375
train loss:  0.4394817352294922
train gradient:  0.20909739794957566
iteration : 11775
train acc:  0.84375
train loss:  0.3538997769355774
train gradient:  0.19062232485128222
iteration : 11776
train acc:  0.7890625
train loss:  0.3555052876472473
train gradient:  0.15249345721803018
iteration : 11777
train acc:  0.921875
train loss:  0.27958154678344727
train gradient:  0.1134430617362886
iteration : 11778
train acc:  0.8359375
train loss:  0.3097962439060211
train gradient:  0.12848062001610266
iteration : 11779
train acc:  0.8359375
train loss:  0.319633811712265
train gradient:  0.11432149102015039
iteration : 11780
train acc:  0.921875
train loss:  0.2607894837856293
train gradient:  0.10043916299274024
iteration : 11781
train acc:  0.921875
train loss:  0.23633575439453125
train gradient:  0.08490266323537608
iteration : 11782
train acc:  0.8203125
train loss:  0.41866257786750793
train gradient:  0.21223059536352112
iteration : 11783
train acc:  0.8671875
train loss:  0.2864765524864197
train gradient:  0.11951223798896969
iteration : 11784
train acc:  0.8359375
train loss:  0.3929300308227539
train gradient:  0.140652155575212
iteration : 11785
train acc:  0.8359375
train loss:  0.28376060724258423
train gradient:  0.16645151582251377
iteration : 11786
train acc:  0.8046875
train loss:  0.36813366413116455
train gradient:  0.21386000377485848
iteration : 11787
train acc:  0.8984375
train loss:  0.28465089201927185
train gradient:  0.11698304096802033
iteration : 11788
train acc:  0.859375
train loss:  0.27624616026878357
train gradient:  0.1331545828254679
iteration : 11789
train acc:  0.8671875
train loss:  0.2973274290561676
train gradient:  0.16987212678026342
iteration : 11790
train acc:  0.921875
train loss:  0.2794126272201538
train gradient:  0.1812337137646683
iteration : 11791
train acc:  0.9140625
train loss:  0.2410392463207245
train gradient:  0.08469104212003026
iteration : 11792
train acc:  0.859375
train loss:  0.35542935132980347
train gradient:  0.17091382652864162
iteration : 11793
train acc:  0.890625
train loss:  0.26952633261680603
train gradient:  0.09869308450501357
iteration : 11794
train acc:  0.8828125
train loss:  0.2691677212715149
train gradient:  0.1723185793714921
iteration : 11795
train acc:  0.8515625
train loss:  0.31793683767318726
train gradient:  0.15617683752051706
iteration : 11796
train acc:  0.84375
train loss:  0.3540055751800537
train gradient:  0.15173996068490003
iteration : 11797
train acc:  0.8828125
train loss:  0.27050063014030457
train gradient:  0.10500475380231573
iteration : 11798
train acc:  0.875
train loss:  0.27699729800224304
train gradient:  0.14068596615942375
iteration : 11799
train acc:  0.8515625
train loss:  0.3263150453567505
train gradient:  0.0952522551099564
iteration : 11800
train acc:  0.8828125
train loss:  0.29241007566452026
train gradient:  0.13657006910076613
iteration : 11801
train acc:  0.859375
train loss:  0.291920930147171
train gradient:  0.12155655774253595
iteration : 11802
train acc:  0.8671875
train loss:  0.36223939061164856
train gradient:  0.23428344048913494
iteration : 11803
train acc:  0.90625
train loss:  0.2599663734436035
train gradient:  0.12489798978666894
iteration : 11804
train acc:  0.921875
train loss:  0.2559483051300049
train gradient:  0.1452666099277531
iteration : 11805
train acc:  0.875
train loss:  0.29963207244873047
train gradient:  0.13695662108608858
iteration : 11806
train acc:  0.8828125
train loss:  0.2682475745677948
train gradient:  0.11400591651824446
iteration : 11807
train acc:  0.8828125
train loss:  0.28002429008483887
train gradient:  0.13885760498648825
iteration : 11808
train acc:  0.875
train loss:  0.2944908142089844
train gradient:  0.13995438818258107
iteration : 11809
train acc:  0.8984375
train loss:  0.2806796729564667
train gradient:  0.09266305577407503
iteration : 11810
train acc:  0.8828125
train loss:  0.2727198600769043
train gradient:  0.11069437814015065
iteration : 11811
train acc:  0.8203125
train loss:  0.40548914670944214
train gradient:  0.19514304162555346
iteration : 11812
train acc:  0.9296875
train loss:  0.21984589099884033
train gradient:  0.10002500093465727
iteration : 11813
train acc:  0.828125
train loss:  0.3769562244415283
train gradient:  0.1698104740614504
iteration : 11814
train acc:  0.84375
train loss:  0.36426955461502075
train gradient:  0.15845393416142112
iteration : 11815
train acc:  0.8828125
train loss:  0.26746562123298645
train gradient:  0.10671196412017417
iteration : 11816
train acc:  0.828125
train loss:  0.4001673460006714
train gradient:  0.23638702082662644
iteration : 11817
train acc:  0.859375
train loss:  0.35524463653564453
train gradient:  0.1941249305632886
iteration : 11818
train acc:  0.8046875
train loss:  0.41740626096725464
train gradient:  0.2807205824346014
iteration : 11819
train acc:  0.828125
train loss:  0.362819641828537
train gradient:  0.23743519433434207
iteration : 11820
train acc:  0.84375
train loss:  0.31091615557670593
train gradient:  0.16820372051920318
iteration : 11821
train acc:  0.9296875
train loss:  0.22068725526332855
train gradient:  0.10752667514843353
iteration : 11822
train acc:  0.890625
train loss:  0.27697378396987915
train gradient:  0.10183471307329418
iteration : 11823
train acc:  0.8671875
train loss:  0.2880626916885376
train gradient:  0.10231734891752436
iteration : 11824
train acc:  0.890625
train loss:  0.30140766501426697
train gradient:  0.12264260603259468
iteration : 11825
train acc:  0.8359375
train loss:  0.3627014756202698
train gradient:  0.15820453878773635
iteration : 11826
train acc:  0.90625
train loss:  0.25074005126953125
train gradient:  0.09413936342128984
iteration : 11827
train acc:  0.8203125
train loss:  0.3917248249053955
train gradient:  0.2251788215548094
iteration : 11828
train acc:  0.875
train loss:  0.28805112838745117
train gradient:  0.1477771695369767
iteration : 11829
train acc:  0.8515625
train loss:  0.3024711608886719
train gradient:  0.11378347443936901
iteration : 11830
train acc:  0.8984375
train loss:  0.29071956872940063
train gradient:  0.15396785884872544
iteration : 11831
train acc:  0.8203125
train loss:  0.34740346670150757
train gradient:  0.15483473983128604
iteration : 11832
train acc:  0.828125
train loss:  0.3113894760608673
train gradient:  0.14175417081827663
iteration : 11833
train acc:  0.875
train loss:  0.255312979221344
train gradient:  0.13010305271441167
iteration : 11834
train acc:  0.8671875
train loss:  0.356060653924942
train gradient:  0.1783428647792208
iteration : 11835
train acc:  0.8125
train loss:  0.39895734190940857
train gradient:  0.2533712496827334
iteration : 11836
train acc:  0.8984375
train loss:  0.29547855257987976
train gradient:  0.16718979270249618
iteration : 11837
train acc:  0.828125
train loss:  0.4056640565395355
train gradient:  0.18479097052298177
iteration : 11838
train acc:  0.875
train loss:  0.31358960270881653
train gradient:  0.18506456581802527
iteration : 11839
train acc:  0.8203125
train loss:  0.3543897867202759
train gradient:  0.1847854159344437
iteration : 11840
train acc:  0.8203125
train loss:  0.3978942632675171
train gradient:  0.2319094233590194
iteration : 11841
train acc:  0.875
train loss:  0.2829655408859253
train gradient:  0.13261765382683638
iteration : 11842
train acc:  0.890625
train loss:  0.24218696355819702
train gradient:  0.080496617752185
iteration : 11843
train acc:  0.8671875
train loss:  0.26833951473236084
train gradient:  0.12167217991826691
iteration : 11844
train acc:  0.890625
train loss:  0.2339501976966858
train gradient:  0.10577935651069716
iteration : 11845
train acc:  0.8984375
train loss:  0.24600741267204285
train gradient:  0.12290973077173074
iteration : 11846
train acc:  0.859375
train loss:  0.37253639101982117
train gradient:  0.1898401551831406
iteration : 11847
train acc:  0.84375
train loss:  0.3485097289085388
train gradient:  0.12961995559346942
iteration : 11848
train acc:  0.8515625
train loss:  0.311201810836792
train gradient:  0.21781602884548618
iteration : 11849
train acc:  0.828125
train loss:  0.33418336510658264
train gradient:  0.14557879572402443
iteration : 11850
train acc:  0.8046875
train loss:  0.36232054233551025
train gradient:  0.15173995623379705
iteration : 11851
train acc:  0.7890625
train loss:  0.44169187545776367
train gradient:  0.2732457960246579
iteration : 11852
train acc:  0.8125
train loss:  0.46566835045814514
train gradient:  0.321107533397744
iteration : 11853
train acc:  0.8515625
train loss:  0.3239797353744507
train gradient:  0.18376298790731171
iteration : 11854
train acc:  0.8671875
train loss:  0.432573139667511
train gradient:  0.22437432885381822
iteration : 11855
train acc:  0.859375
train loss:  0.30886754393577576
train gradient:  0.15867105674012763
iteration : 11856
train acc:  0.828125
train loss:  0.29800695180892944
train gradient:  0.15535046240900546
iteration : 11857
train acc:  0.84375
train loss:  0.387350857257843
train gradient:  0.20983146266415292
iteration : 11858
train acc:  0.9296875
train loss:  0.24485106766223907
train gradient:  0.09389741603332473
iteration : 11859
train acc:  0.8984375
train loss:  0.2640564739704132
train gradient:  0.09783757977168471
iteration : 11860
train acc:  0.8515625
train loss:  0.36465317010879517
train gradient:  0.19039825637206362
iteration : 11861
train acc:  0.8515625
train loss:  0.3208051025867462
train gradient:  0.12491485535891651
iteration : 11862
train acc:  0.859375
train loss:  0.2756863236427307
train gradient:  0.08858205346832347
iteration : 11863
train acc:  0.8984375
train loss:  0.2851155996322632
train gradient:  0.12893843136950509
iteration : 11864
train acc:  0.828125
train loss:  0.38159453868865967
train gradient:  0.17149370606005226
iteration : 11865
train acc:  0.890625
train loss:  0.24210652709007263
train gradient:  0.09930174623052367
iteration : 11866
train acc:  0.890625
train loss:  0.2830343246459961
train gradient:  0.08425553351461135
iteration : 11867
train acc:  0.875
train loss:  0.2967931032180786
train gradient:  0.09833377305526032
iteration : 11868
train acc:  0.9140625
train loss:  0.28363266587257385
train gradient:  0.1593825838819377
iteration : 11869
train acc:  0.8828125
train loss:  0.26422518491744995
train gradient:  0.09457488231541746
iteration : 11870
train acc:  0.875
train loss:  0.32435065507888794
train gradient:  0.17529608393436874
iteration : 11871
train acc:  0.8671875
train loss:  0.31850185990333557
train gradient:  0.1267812142935462
iteration : 11872
train acc:  0.890625
train loss:  0.2732478380203247
train gradient:  0.10765337843001363
iteration : 11873
train acc:  0.8046875
train loss:  0.38305264711380005
train gradient:  0.18957509319714488
iteration : 11874
train acc:  0.84375
train loss:  0.36055612564086914
train gradient:  0.231986243498173
iteration : 11875
train acc:  0.828125
train loss:  0.3606826663017273
train gradient:  0.19619840966624402
iteration : 11876
train acc:  0.8515625
train loss:  0.35044246912002563
train gradient:  0.2232498538371547
iteration : 11877
train acc:  0.8828125
train loss:  0.2586083710193634
train gradient:  0.12088283708482873
iteration : 11878
train acc:  0.8046875
train loss:  0.3145749568939209
train gradient:  0.1499658036347811
iteration : 11879
train acc:  0.890625
train loss:  0.31104493141174316
train gradient:  0.12686148081712803
iteration : 11880
train acc:  0.828125
train loss:  0.38735947012901306
train gradient:  0.15393079307504454
iteration : 11881
train acc:  0.8359375
train loss:  0.40762510895729065
train gradient:  0.22834794723325152
iteration : 11882
train acc:  0.859375
train loss:  0.30633559823036194
train gradient:  0.3201242523902363
iteration : 11883
train acc:  0.8671875
train loss:  0.2826906442642212
train gradient:  0.15040969850445662
iteration : 11884
train acc:  0.8046875
train loss:  0.3728216886520386
train gradient:  0.2877784721253844
iteration : 11885
train acc:  0.890625
train loss:  0.24777862429618835
train gradient:  0.09807002360718417
iteration : 11886
train acc:  0.828125
train loss:  0.4317566454410553
train gradient:  0.5095667592854627
iteration : 11887
train acc:  0.8828125
train loss:  0.3188081979751587
train gradient:  0.12702194081988527
iteration : 11888
train acc:  0.859375
train loss:  0.3543621301651001
train gradient:  0.12518240881788328
iteration : 11889
train acc:  0.8671875
train loss:  0.35196131467819214
train gradient:  0.15376307034211126
iteration : 11890
train acc:  0.8671875
train loss:  0.30711400508880615
train gradient:  0.11924271965093171
iteration : 11891
train acc:  0.90625
train loss:  0.2721995711326599
train gradient:  0.1025878147436518
iteration : 11892
train acc:  0.875
train loss:  0.26238885521888733
train gradient:  0.11495723873593876
iteration : 11893
train acc:  0.890625
train loss:  0.3131457567214966
train gradient:  0.20921605918108005
iteration : 11894
train acc:  0.8671875
train loss:  0.33493557572364807
train gradient:  0.23017730019467725
iteration : 11895
train acc:  0.8515625
train loss:  0.3629554212093353
train gradient:  0.2039038664959655
iteration : 11896
train acc:  0.9296875
train loss:  0.2526995539665222
train gradient:  0.14805374051656192
iteration : 11897
train acc:  0.859375
train loss:  0.3441736102104187
train gradient:  0.16851730056725667
iteration : 11898
train acc:  0.8671875
train loss:  0.2784692049026489
train gradient:  0.13409129120768887
iteration : 11899
train acc:  0.8046875
train loss:  0.3801864981651306
train gradient:  0.22795706786764453
iteration : 11900
train acc:  0.8828125
train loss:  0.3339783549308777
train gradient:  0.17946221214594363
iteration : 11901
train acc:  0.828125
train loss:  0.37097859382629395
train gradient:  0.21172633075272362
iteration : 11902
train acc:  0.84375
train loss:  0.32753288745880127
train gradient:  0.1552433952877221
iteration : 11903
train acc:  0.78125
train loss:  0.4385848641395569
train gradient:  0.18733269456830465
iteration : 11904
train acc:  0.8515625
train loss:  0.3034355044364929
train gradient:  0.10596935492669983
iteration : 11905
train acc:  0.8984375
train loss:  0.2683854103088379
train gradient:  0.12391191142588598
iteration : 11906
train acc:  0.8125
train loss:  0.4037781357765198
train gradient:  0.23400320103821193
iteration : 11907
train acc:  0.8671875
train loss:  0.33264273405075073
train gradient:  0.17325287013781648
iteration : 11908
train acc:  0.8671875
train loss:  0.3058337867259979
train gradient:  0.18183381296032983
iteration : 11909
train acc:  0.8359375
train loss:  0.35820460319519043
train gradient:  0.24600122807277575
iteration : 11910
train acc:  0.7890625
train loss:  0.3948395848274231
train gradient:  0.18338466874716708
iteration : 11911
train acc:  0.8203125
train loss:  0.39822834730148315
train gradient:  0.18229522266185327
iteration : 11912
train acc:  0.90625
train loss:  0.2642836570739746
train gradient:  0.1852692530699424
iteration : 11913
train acc:  0.84375
train loss:  0.3744931221008301
train gradient:  0.23086622498073733
iteration : 11914
train acc:  0.8359375
train loss:  0.36243298649787903
train gradient:  0.23492161538670336
iteration : 11915
train acc:  0.8671875
train loss:  0.2977164089679718
train gradient:  0.13636604498602756
iteration : 11916
train acc:  0.8515625
train loss:  0.28949496150016785
train gradient:  0.12481187302588109
iteration : 11917
train acc:  0.90625
train loss:  0.2920582592487335
train gradient:  0.09725464159386107
iteration : 11918
train acc:  0.84375
train loss:  0.3567529320716858
train gradient:  0.18255019614984364
iteration : 11919
train acc:  0.875
train loss:  0.3426777720451355
train gradient:  0.1852437536464726
iteration : 11920
train acc:  0.8671875
train loss:  0.28055161237716675
train gradient:  0.1186109340099282
iteration : 11921
train acc:  0.828125
train loss:  0.38770079612731934
train gradient:  0.23984229150202355
iteration : 11922
train acc:  0.875
train loss:  0.28962981700897217
train gradient:  0.11486023041597164
iteration : 11923
train acc:  0.8359375
train loss:  0.33779871463775635
train gradient:  0.13222071631016247
iteration : 11924
train acc:  0.9140625
train loss:  0.34990358352661133
train gradient:  0.16204454934802376
iteration : 11925
train acc:  0.8671875
train loss:  0.35451948642730713
train gradient:  0.17565971385134782
iteration : 11926
train acc:  0.859375
train loss:  0.3208043873310089
train gradient:  0.15725254112635034
iteration : 11927
train acc:  0.8671875
train loss:  0.2615557312965393
train gradient:  0.11461601420434732
iteration : 11928
train acc:  0.84375
train loss:  0.3199837803840637
train gradient:  0.1609284627592781
iteration : 11929
train acc:  0.828125
train loss:  0.36648958921432495
train gradient:  0.19857182843084184
iteration : 11930
train acc:  0.8671875
train loss:  0.27900680899620056
train gradient:  0.12136813190831508
iteration : 11931
train acc:  0.8671875
train loss:  0.26011353731155396
train gradient:  0.1028905244504311
iteration : 11932
train acc:  0.8671875
train loss:  0.3060576021671295
train gradient:  0.1398612942913888
iteration : 11933
train acc:  0.8828125
train loss:  0.2680961787700653
train gradient:  0.13157751979687576
iteration : 11934
train acc:  0.8828125
train loss:  0.24010080099105835
train gradient:  0.08824564237603494
iteration : 11935
train acc:  0.90625
train loss:  0.22231923043727875
train gradient:  0.10027456973569952
iteration : 11936
train acc:  0.8515625
train loss:  0.3927440047264099
train gradient:  0.15791878352404506
iteration : 11937
train acc:  0.890625
train loss:  0.2544584572315216
train gradient:  0.09690214792636187
iteration : 11938
train acc:  0.875
train loss:  0.29272159934043884
train gradient:  0.1763520321189495
iteration : 11939
train acc:  0.8515625
train loss:  0.4225940406322479
train gradient:  0.20922579474560704
iteration : 11940
train acc:  0.8515625
train loss:  0.3287116587162018
train gradient:  0.18325817379467046
iteration : 11941
train acc:  0.84375
train loss:  0.31716376543045044
train gradient:  0.17783149190478348
iteration : 11942
train acc:  0.8671875
train loss:  0.29417410492897034
train gradient:  0.1353892307206775
iteration : 11943
train acc:  0.8671875
train loss:  0.36122530698776245
train gradient:  0.19789021780947433
iteration : 11944
train acc:  0.796875
train loss:  0.3668564260005951
train gradient:  0.17815379062565195
iteration : 11945
train acc:  0.8515625
train loss:  0.3032904863357544
train gradient:  0.11371318972285059
iteration : 11946
train acc:  0.84375
train loss:  0.3759715259075165
train gradient:  0.1979597830735666
iteration : 11947
train acc:  0.84375
train loss:  0.2815015912055969
train gradient:  0.1794651844019525
iteration : 11948
train acc:  0.8203125
train loss:  0.358306348323822
train gradient:  0.17374334835488814
iteration : 11949
train acc:  0.8515625
train loss:  0.3123829960823059
train gradient:  0.1544983536036113
iteration : 11950
train acc:  0.8515625
train loss:  0.3626878559589386
train gradient:  0.21158296456591835
iteration : 11951
train acc:  0.828125
train loss:  0.31289011240005493
train gradient:  0.16092531070675578
iteration : 11952
train acc:  0.859375
train loss:  0.33898353576660156
train gradient:  0.23882340247891454
iteration : 11953
train acc:  0.8828125
train loss:  0.321053683757782
train gradient:  0.17716355292635919
iteration : 11954
train acc:  0.8828125
train loss:  0.3104372024536133
train gradient:  0.1455964171697455
iteration : 11955
train acc:  0.9140625
train loss:  0.24384449422359467
train gradient:  0.10477995508210507
iteration : 11956
train acc:  0.8203125
train loss:  0.3674914836883545
train gradient:  0.1621237425330129
iteration : 11957
train acc:  0.859375
train loss:  0.25892382860183716
train gradient:  0.12021652624144322
iteration : 11958
train acc:  0.8359375
train loss:  0.35857337713241577
train gradient:  0.1641404383083115
iteration : 11959
train acc:  0.875
train loss:  0.316180944442749
train gradient:  0.15671810233992986
iteration : 11960
train acc:  0.859375
train loss:  0.3693290054798126
train gradient:  0.20523107009819186
iteration : 11961
train acc:  0.890625
train loss:  0.3441777229309082
train gradient:  0.18642100489661853
iteration : 11962
train acc:  0.875
train loss:  0.28960472345352173
train gradient:  0.1611195615682359
iteration : 11963
train acc:  0.8828125
train loss:  0.26946377754211426
train gradient:  0.0848229619113736
iteration : 11964
train acc:  0.84375
train loss:  0.3972335457801819
train gradient:  0.23128361744671544
iteration : 11965
train acc:  0.8828125
train loss:  0.2930404245853424
train gradient:  0.11983266302903921
iteration : 11966
train acc:  0.8671875
train loss:  0.2815794050693512
train gradient:  0.10305285850167774
iteration : 11967
train acc:  0.859375
train loss:  0.35378167033195496
train gradient:  0.1426748218481643
iteration : 11968
train acc:  0.859375
train loss:  0.3430962860584259
train gradient:  0.2519274018710134
iteration : 11969
train acc:  0.84375
train loss:  0.3247378468513489
train gradient:  0.18381472920471564
iteration : 11970
train acc:  0.8671875
train loss:  0.2606048583984375
train gradient:  0.11156647756252588
iteration : 11971
train acc:  0.8359375
train loss:  0.37125712633132935
train gradient:  0.16301424034364942
iteration : 11972
train acc:  0.875
train loss:  0.31220030784606934
train gradient:  0.1558789847519661
iteration : 11973
train acc:  0.90625
train loss:  0.20372557640075684
train gradient:  0.07301212639725638
iteration : 11974
train acc:  0.921875
train loss:  0.2296094000339508
train gradient:  0.0859456099565002
iteration : 11975
train acc:  0.8125
train loss:  0.35822826623916626
train gradient:  0.14970236997034217
iteration : 11976
train acc:  0.84375
train loss:  0.33379364013671875
train gradient:  0.20643492950136322
iteration : 11977
train acc:  0.9296875
train loss:  0.19543692469596863
train gradient:  0.07232216612217282
iteration : 11978
train acc:  0.8984375
train loss:  0.25595664978027344
train gradient:  0.09676358626280426
iteration : 11979
train acc:  0.8515625
train loss:  0.3301805853843689
train gradient:  0.21233024517594218
iteration : 11980
train acc:  0.8046875
train loss:  0.3750545382499695
train gradient:  0.21662192356754273
iteration : 11981
train acc:  0.84375
train loss:  0.3087632656097412
train gradient:  0.1261819207818855
iteration : 11982
train acc:  0.8203125
train loss:  0.37540996074676514
train gradient:  0.229003018210986
iteration : 11983
train acc:  0.875
train loss:  0.2755241394042969
train gradient:  0.0834345569455809
iteration : 11984
train acc:  0.90625
train loss:  0.2551124095916748
train gradient:  0.1433341954574791
iteration : 11985
train acc:  0.828125
train loss:  0.351276695728302
train gradient:  0.16366125413978286
iteration : 11986
train acc:  0.828125
train loss:  0.377755731344223
train gradient:  0.2173594338838699
iteration : 11987
train acc:  0.8046875
train loss:  0.3912450671195984
train gradient:  0.2061503510650658
iteration : 11988
train acc:  0.8359375
train loss:  0.33535870909690857
train gradient:  0.18244430468729203
iteration : 11989
train acc:  0.8046875
train loss:  0.45946004986763
train gradient:  0.23296807082514567
iteration : 11990
train acc:  0.875
train loss:  0.3469504714012146
train gradient:  0.18054135796260998
iteration : 11991
train acc:  0.8984375
train loss:  0.2951388955116272
train gradient:  0.16230546077945807
iteration : 11992
train acc:  0.8515625
train loss:  0.3487301468849182
train gradient:  0.1319389292352681
iteration : 11993
train acc:  0.8359375
train loss:  0.35139262676239014
train gradient:  0.1659321841932737
iteration : 11994
train acc:  0.859375
train loss:  0.267802357673645
train gradient:  0.12746057726775945
iteration : 11995
train acc:  0.8515625
train loss:  0.33043330907821655
train gradient:  0.13732134413214986
iteration : 11996
train acc:  0.890625
train loss:  0.26436129212379456
train gradient:  0.09044651158717867
iteration : 11997
train acc:  0.9375
train loss:  0.18517008423805237
train gradient:  0.09721952768034454
iteration : 11998
train acc:  0.8671875
train loss:  0.36033010482788086
train gradient:  0.16049358705047745
iteration : 11999
train acc:  0.84375
train loss:  0.3159922957420349
train gradient:  0.18928605965546394
iteration : 12000
train acc:  0.8125
train loss:  0.4181821942329407
train gradient:  0.17610482564260624
iteration : 12001
train acc:  0.875
train loss:  0.2999873161315918
train gradient:  0.11282097052760177
iteration : 12002
train acc:  0.875
train loss:  0.29168373346328735
train gradient:  0.1678691417975025
iteration : 12003
train acc:  0.859375
train loss:  0.3216286301612854
train gradient:  0.14738852891587642
iteration : 12004
train acc:  0.90625
train loss:  0.2872508466243744
train gradient:  0.09993176192534113
iteration : 12005
train acc:  0.8828125
train loss:  0.3004704713821411
train gradient:  0.12117960633467686
iteration : 12006
train acc:  0.8828125
train loss:  0.28089725971221924
train gradient:  0.14155567820565582
iteration : 12007
train acc:  0.859375
train loss:  0.33570367097854614
train gradient:  0.18969220476322382
iteration : 12008
train acc:  0.84375
train loss:  0.3141390085220337
train gradient:  0.17064042314202216
iteration : 12009
train acc:  0.8671875
train loss:  0.3303981423377991
train gradient:  0.17748226133501352
iteration : 12010
train acc:  0.8203125
train loss:  0.38309943675994873
train gradient:  0.1991079293017639
iteration : 12011
train acc:  0.875
train loss:  0.2749496400356293
train gradient:  0.14336420449555448
iteration : 12012
train acc:  0.8515625
train loss:  0.3227579593658447
train gradient:  0.15236916972484638
iteration : 12013
train acc:  0.8984375
train loss:  0.3097369074821472
train gradient:  0.14109638987582257
iteration : 12014
train acc:  0.9140625
train loss:  0.25328439474105835
train gradient:  0.1168215815379611
iteration : 12015
train acc:  0.84375
train loss:  0.31865575909614563
train gradient:  0.14855132683695266
iteration : 12016
train acc:  0.8828125
train loss:  0.35247567296028137
train gradient:  0.1846785124292355
iteration : 12017
train acc:  0.8828125
train loss:  0.21447589993476868
train gradient:  0.07854002794307068
iteration : 12018
train acc:  0.8359375
train loss:  0.36765187978744507
train gradient:  0.1989242486624843
iteration : 12019
train acc:  0.8203125
train loss:  0.40119966864585876
train gradient:  0.20760103528205892
iteration : 12020
train acc:  0.8125
train loss:  0.35311180353164673
train gradient:  0.25924621886401034
iteration : 12021
train acc:  0.8046875
train loss:  0.3973601758480072
train gradient:  0.23869654650674238
iteration : 12022
train acc:  0.796875
train loss:  0.3993363678455353
train gradient:  0.19763755847733153
iteration : 12023
train acc:  0.8828125
train loss:  0.2669537663459778
train gradient:  0.13053521510088406
iteration : 12024
train acc:  0.859375
train loss:  0.35052552819252014
train gradient:  0.11756516580185265
iteration : 12025
train acc:  0.84375
train loss:  0.33183109760284424
train gradient:  0.240058096366131
iteration : 12026
train acc:  0.9140625
train loss:  0.24861761927604675
train gradient:  0.20634558103872952
iteration : 12027
train acc:  0.8359375
train loss:  0.3182680606842041
train gradient:  0.17927376393759098
iteration : 12028
train acc:  0.828125
train loss:  0.3346720337867737
train gradient:  0.12275664165214029
iteration : 12029
train acc:  0.859375
train loss:  0.33504873514175415
train gradient:  0.28654077894946356
iteration : 12030
train acc:  0.8359375
train loss:  0.33055663108825684
train gradient:  0.14601821593562608
iteration : 12031
train acc:  0.8671875
train loss:  0.293544739484787
train gradient:  0.11157648444917995
iteration : 12032
train acc:  0.875
train loss:  0.2817053198814392
train gradient:  0.16707717737599215
iteration : 12033
train acc:  0.8671875
train loss:  0.38374102115631104
train gradient:  0.2397617515194612
iteration : 12034
train acc:  0.859375
train loss:  0.31354767084121704
train gradient:  0.10680786359173675
iteration : 12035
train acc:  0.8515625
train loss:  0.31996816396713257
train gradient:  0.15249705331033087
iteration : 12036
train acc:  0.84375
train loss:  0.3284008502960205
train gradient:  0.1268422807511114
iteration : 12037
train acc:  0.859375
train loss:  0.3673310875892639
train gradient:  0.14443343083695184
iteration : 12038
train acc:  0.875
train loss:  0.2819635272026062
train gradient:  0.14180612018108424
iteration : 12039
train acc:  0.875
train loss:  0.29970529675483704
train gradient:  0.10051417924469296
iteration : 12040
train acc:  0.8359375
train loss:  0.33558911085128784
train gradient:  0.14541198837182118
iteration : 12041
train acc:  0.8515625
train loss:  0.3008471429347992
train gradient:  0.16051807415273592
iteration : 12042
train acc:  0.859375
train loss:  0.31328991055488586
train gradient:  0.12637459613467106
iteration : 12043
train acc:  0.875
train loss:  0.3328295648097992
train gradient:  0.16855840951313072
iteration : 12044
train acc:  0.828125
train loss:  0.32385939359664917
train gradient:  0.16695170053564135
iteration : 12045
train acc:  0.890625
train loss:  0.266478955745697
train gradient:  0.14688670059996262
iteration : 12046
train acc:  0.8515625
train loss:  0.35773128271102905
train gradient:  0.1707724781331833
iteration : 12047
train acc:  0.8515625
train loss:  0.32052236795425415
train gradient:  0.12985268351583767
iteration : 12048
train acc:  0.890625
train loss:  0.26875239610671997
train gradient:  0.11374515744879218
iteration : 12049
train acc:  0.859375
train loss:  0.3349389433860779
train gradient:  0.1212216613793714
iteration : 12050
train acc:  0.8125
train loss:  0.4061790704727173
train gradient:  0.17459829890761433
iteration : 12051
train acc:  0.859375
train loss:  0.3751745820045471
train gradient:  0.18702061338123804
iteration : 12052
train acc:  0.8359375
train loss:  0.3225981891155243
train gradient:  0.13603753662882534
iteration : 12053
train acc:  0.8203125
train loss:  0.37065234780311584
train gradient:  0.1693070229999763
iteration : 12054
train acc:  0.8515625
train loss:  0.39790505170822144
train gradient:  0.18920562176404476
iteration : 12055
train acc:  0.875
train loss:  0.3763640820980072
train gradient:  0.1624907497046914
iteration : 12056
train acc:  0.84375
train loss:  0.35215798020362854
train gradient:  0.23012375985891045
iteration : 12057
train acc:  0.875
train loss:  0.3025030195713043
train gradient:  0.1287526411632428
iteration : 12058
train acc:  0.8984375
train loss:  0.26512935757637024
train gradient:  0.10914592448778199
iteration : 12059
train acc:  0.8828125
train loss:  0.3066474199295044
train gradient:  0.18377919056513353
iteration : 12060
train acc:  0.8515625
train loss:  0.3382461667060852
train gradient:  0.1402815025485793
iteration : 12061
train acc:  0.8828125
train loss:  0.31024670600891113
train gradient:  0.09204087802482294
iteration : 12062
train acc:  0.8828125
train loss:  0.3099004030227661
train gradient:  0.23998501040054337
iteration : 12063
train acc:  0.8125
train loss:  0.3201389014720917
train gradient:  0.11718258457228538
iteration : 12064
train acc:  0.875
train loss:  0.3160408139228821
train gradient:  0.13364820409313322
iteration : 12065
train acc:  0.8515625
train loss:  0.38767850399017334
train gradient:  0.13323093915369466
iteration : 12066
train acc:  0.84375
train loss:  0.34088313579559326
train gradient:  0.1560901735954634
iteration : 12067
train acc:  0.8046875
train loss:  0.4086992144584656
train gradient:  0.232698534533197
iteration : 12068
train acc:  0.8515625
train loss:  0.35087573528289795
train gradient:  0.16624243070115607
iteration : 12069
train acc:  0.8515625
train loss:  0.3212342858314514
train gradient:  0.13386327017598393
iteration : 12070
train acc:  0.9140625
train loss:  0.2563078701496124
train gradient:  0.08708433448542799
iteration : 12071
train acc:  0.859375
train loss:  0.3639255166053772
train gradient:  0.17880490853013026
iteration : 12072
train acc:  0.875
train loss:  0.3059757351875305
train gradient:  0.13689111409052376
iteration : 12073
train acc:  0.84375
train loss:  0.32947224378585815
train gradient:  0.16003809568779892
iteration : 12074
train acc:  0.828125
train loss:  0.3687855899333954
train gradient:  0.16509695685242237
iteration : 12075
train acc:  0.859375
train loss:  0.32849934697151184
train gradient:  0.19516360097419194
iteration : 12076
train acc:  0.90625
train loss:  0.24103930592536926
train gradient:  0.13302577933288928
iteration : 12077
train acc:  0.875
train loss:  0.30585452914237976
train gradient:  0.11687204835783677
iteration : 12078
train acc:  0.859375
train loss:  0.3344971537590027
train gradient:  0.14938644097876855
iteration : 12079
train acc:  0.890625
train loss:  0.25642168521881104
train gradient:  0.1081063325240868
iteration : 12080
train acc:  0.875
train loss:  0.32382819056510925
train gradient:  0.1456869750643942
iteration : 12081
train acc:  0.875
train loss:  0.3577839136123657
train gradient:  0.2041109355400884
iteration : 12082
train acc:  0.84375
train loss:  0.3249613046646118
train gradient:  0.1081643853470505
iteration : 12083
train acc:  0.90625
train loss:  0.25298774242401123
train gradient:  0.08460518190629704
iteration : 12084
train acc:  0.8828125
train loss:  0.31854671239852905
train gradient:  0.11048854867679131
iteration : 12085
train acc:  0.8984375
train loss:  0.24306398630142212
train gradient:  0.0924469916781027
iteration : 12086
train acc:  0.8359375
train loss:  0.3577600121498108
train gradient:  0.2452584394110311
iteration : 12087
train acc:  0.8671875
train loss:  0.28932347893714905
train gradient:  0.11741506033281388
iteration : 12088
train acc:  0.875
train loss:  0.2680298089981079
train gradient:  0.11847783033516793
iteration : 12089
train acc:  0.8671875
train loss:  0.31332898139953613
train gradient:  0.16515468812424683
iteration : 12090
train acc:  0.8671875
train loss:  0.3256317377090454
train gradient:  0.1326235962563433
iteration : 12091
train acc:  0.875
train loss:  0.2994873523712158
train gradient:  0.10448454909618074
iteration : 12092
train acc:  0.8203125
train loss:  0.34413886070251465
train gradient:  0.17784135641234422
iteration : 12093
train acc:  0.8515625
train loss:  0.3757256269454956
train gradient:  0.14416055781391518
iteration : 12094
train acc:  0.859375
train loss:  0.30585843324661255
train gradient:  0.123739305903367
iteration : 12095
train acc:  0.8515625
train loss:  0.3486514389514923
train gradient:  0.1615296298874716
iteration : 12096
train acc:  0.8515625
train loss:  0.3997293710708618
train gradient:  0.21849833996825252
iteration : 12097
train acc:  0.90625
train loss:  0.24326187372207642
train gradient:  0.11777398749037925
iteration : 12098
train acc:  0.875
train loss:  0.3138771057128906
train gradient:  0.13361736550556283
iteration : 12099
train acc:  0.859375
train loss:  0.2943228483200073
train gradient:  0.12407362878025201
iteration : 12100
train acc:  0.8828125
train loss:  0.2953779101371765
train gradient:  0.12063675407176583
iteration : 12101
train acc:  0.859375
train loss:  0.36457979679107666
train gradient:  0.2666296405245698
iteration : 12102
train acc:  0.8671875
train loss:  0.300193190574646
train gradient:  0.1353760649372288
iteration : 12103
train acc:  0.8671875
train loss:  0.32828691601753235
train gradient:  0.11072760413733455
iteration : 12104
train acc:  0.8671875
train loss:  0.2972278594970703
train gradient:  0.1307946276237743
iteration : 12105
train acc:  0.9140625
train loss:  0.2096400260925293
train gradient:  0.09101688544105482
iteration : 12106
train acc:  0.8125
train loss:  0.364841103553772
train gradient:  0.1398291004933616
iteration : 12107
train acc:  0.8046875
train loss:  0.39124852418899536
train gradient:  0.19137155191031607
iteration : 12108
train acc:  0.78125
train loss:  0.4874226152896881
train gradient:  0.2507884759838825
iteration : 12109
train acc:  0.828125
train loss:  0.4435667395591736
train gradient:  0.2905809017544366
iteration : 12110
train acc:  0.8359375
train loss:  0.3617611527442932
train gradient:  0.18746665488895292
iteration : 12111
train acc:  0.8125
train loss:  0.35676687955856323
train gradient:  0.1295587191686477
iteration : 12112
train acc:  0.875
train loss:  0.3169142007827759
train gradient:  0.1302009486465805
iteration : 12113
train acc:  0.828125
train loss:  0.38129881024360657
train gradient:  0.14895996212424706
iteration : 12114
train acc:  0.8671875
train loss:  0.31432366371154785
train gradient:  0.1974699634502808
iteration : 12115
train acc:  0.875
train loss:  0.27980947494506836
train gradient:  0.08457938511801039
iteration : 12116
train acc:  0.859375
train loss:  0.30072999000549316
train gradient:  0.1118154282037674
iteration : 12117
train acc:  0.9140625
train loss:  0.23605060577392578
train gradient:  0.1322832046010191
iteration : 12118
train acc:  0.875
train loss:  0.32749590277671814
train gradient:  0.2790113622534691
iteration : 12119
train acc:  0.875
train loss:  0.3005604147911072
train gradient:  0.14029086342320657
iteration : 12120
train acc:  0.84375
train loss:  0.32540255784988403
train gradient:  0.15433863674023263
iteration : 12121
train acc:  0.8046875
train loss:  0.430644690990448
train gradient:  0.2181538332899738
iteration : 12122
train acc:  0.84375
train loss:  0.32623979449272156
train gradient:  0.1308072080734332
iteration : 12123
train acc:  0.8828125
train loss:  0.27307677268981934
train gradient:  0.08373917813072544
iteration : 12124
train acc:  0.890625
train loss:  0.26341512799263
train gradient:  0.1087000148961943
iteration : 12125
train acc:  0.8515625
train loss:  0.2913944125175476
train gradient:  0.1051055126819317
iteration : 12126
train acc:  0.8359375
train loss:  0.33494025468826294
train gradient:  0.21087742393805742
iteration : 12127
train acc:  0.84375
train loss:  0.34019845724105835
train gradient:  0.12095864244774718
iteration : 12128
train acc:  0.84375
train loss:  0.38425424695014954
train gradient:  0.1922254683680416
iteration : 12129
train acc:  0.859375
train loss:  0.3570655286312103
train gradient:  0.15168535619865509
iteration : 12130
train acc:  0.8671875
train loss:  0.30013924837112427
train gradient:  0.11138659563629727
iteration : 12131
train acc:  0.828125
train loss:  0.34552472829818726
train gradient:  0.18221010723389225
iteration : 12132
train acc:  0.8515625
train loss:  0.3631632924079895
train gradient:  0.24159351163307888
iteration : 12133
train acc:  0.859375
train loss:  0.2873011827468872
train gradient:  0.11584741488275749
iteration : 12134
train acc:  0.8828125
train loss:  0.3071448802947998
train gradient:  0.11343926434570989
iteration : 12135
train acc:  0.890625
train loss:  0.29216456413269043
train gradient:  0.11328600185166489
iteration : 12136
train acc:  0.890625
train loss:  0.26912379264831543
train gradient:  0.14531992263310317
iteration : 12137
train acc:  0.8046875
train loss:  0.3908917009830475
train gradient:  0.22822347237431745
iteration : 12138
train acc:  0.8203125
train loss:  0.38892999291419983
train gradient:  0.15224751516634383
iteration : 12139
train acc:  0.8671875
train loss:  0.29499924182891846
train gradient:  0.13598711543383762
iteration : 12140
train acc:  0.859375
train loss:  0.27930518984794617
train gradient:  0.09317176699830547
iteration : 12141
train acc:  0.8515625
train loss:  0.33979368209838867
train gradient:  0.1396954398484081
iteration : 12142
train acc:  0.921875
train loss:  0.2440066635608673
train gradient:  0.0998910810773106
iteration : 12143
train acc:  0.78125
train loss:  0.4526798129081726
train gradient:  0.30785808039298784
iteration : 12144
train acc:  0.84375
train loss:  0.3118053674697876
train gradient:  0.12281947987146646
iteration : 12145
train acc:  0.890625
train loss:  0.2706802189350128
train gradient:  0.09091178380114183
iteration : 12146
train acc:  0.7890625
train loss:  0.39753302931785583
train gradient:  0.19538521520764562
iteration : 12147
train acc:  0.8359375
train loss:  0.3421452045440674
train gradient:  0.10296941747391816
iteration : 12148
train acc:  0.921875
train loss:  0.22008253633975983
train gradient:  0.0855756080799679
iteration : 12149
train acc:  0.8828125
train loss:  0.3067260682582855
train gradient:  0.1342738148699164
iteration : 12150
train acc:  0.921875
train loss:  0.23764653503894806
train gradient:  0.08922016418211008
iteration : 12151
train acc:  0.8984375
train loss:  0.28617456555366516
train gradient:  0.0913513229743814
iteration : 12152
train acc:  0.84375
train loss:  0.34804201126098633
train gradient:  0.135856287126706
iteration : 12153
train acc:  0.828125
train loss:  0.3359840512275696
train gradient:  0.13447774705145335
iteration : 12154
train acc:  0.8046875
train loss:  0.3980487585067749
train gradient:  0.17075429639389755
iteration : 12155
train acc:  0.8671875
train loss:  0.32284581661224365
train gradient:  0.1313481631137889
iteration : 12156
train acc:  0.8828125
train loss:  0.3083537220954895
train gradient:  0.1276414669556614
iteration : 12157
train acc:  0.90625
train loss:  0.3171182870864868
train gradient:  0.23470805043375587
iteration : 12158
train acc:  0.8671875
train loss:  0.23856773972511292
train gradient:  0.09229268459334551
iteration : 12159
train acc:  0.9140625
train loss:  0.22001734375953674
train gradient:  0.12188890046605645
iteration : 12160
train acc:  0.875
train loss:  0.3708546459674835
train gradient:  0.13006019907474692
iteration : 12161
train acc:  0.8359375
train loss:  0.293135404586792
train gradient:  0.06576839110628929
iteration : 12162
train acc:  0.8515625
train loss:  0.32466521859169006
train gradient:  0.13744460418968754
iteration : 12163
train acc:  0.8359375
train loss:  0.3408045768737793
train gradient:  0.13376209624953694
iteration : 12164
train acc:  0.828125
train loss:  0.3311987519264221
train gradient:  0.16327312556925902
iteration : 12165
train acc:  0.8125
train loss:  0.35827547311782837
train gradient:  0.14724164795955635
iteration : 12166
train acc:  0.875
train loss:  0.304232656955719
train gradient:  0.12027438396689186
iteration : 12167
train acc:  0.875
train loss:  0.24443548917770386
train gradient:  0.08855490933396802
iteration : 12168
train acc:  0.8203125
train loss:  0.3929154872894287
train gradient:  0.25129084238482213
iteration : 12169
train acc:  0.875
train loss:  0.32077664136886597
train gradient:  0.17569101911329577
iteration : 12170
train acc:  0.859375
train loss:  0.30786198377609253
train gradient:  0.11612299759305969
iteration : 12171
train acc:  0.875
train loss:  0.2851347029209137
train gradient:  0.09959641820348482
iteration : 12172
train acc:  0.8046875
train loss:  0.37027430534362793
train gradient:  0.18371007697204328
iteration : 12173
train acc:  0.8671875
train loss:  0.34233415126800537
train gradient:  0.18329331361355572
iteration : 12174
train acc:  0.890625
train loss:  0.28002530336380005
train gradient:  0.08764469683185887
iteration : 12175
train acc:  0.8515625
train loss:  0.3052298426628113
train gradient:  0.10528874341682842
iteration : 12176
train acc:  0.875
train loss:  0.31020259857177734
train gradient:  0.1502659556381671
iteration : 12177
train acc:  0.921875
train loss:  0.2473660558462143
train gradient:  0.10034880145972735
iteration : 12178
train acc:  0.90625
train loss:  0.2515307664871216
train gradient:  0.08843341865510328
iteration : 12179
train acc:  0.8671875
train loss:  0.32355791330337524
train gradient:  0.14236359910123006
iteration : 12180
train acc:  0.859375
train loss:  0.3131180703639984
train gradient:  0.09321375820378326
iteration : 12181
train acc:  0.8984375
train loss:  0.29495227336883545
train gradient:  0.14743954374280493
iteration : 12182
train acc:  0.84375
train loss:  0.3537760376930237
train gradient:  0.2474872644932528
iteration : 12183
train acc:  0.8515625
train loss:  0.35449278354644775
train gradient:  0.15749482159750466
iteration : 12184
train acc:  0.8515625
train loss:  0.30850255489349365
train gradient:  0.11605145387150986
iteration : 12185
train acc:  0.8828125
train loss:  0.24693959951400757
train gradient:  0.08534022367117104
iteration : 12186
train acc:  0.8828125
train loss:  0.2740499973297119
train gradient:  0.12305487712423074
iteration : 12187
train acc:  0.8515625
train loss:  0.3540107011795044
train gradient:  0.14277577316887313
iteration : 12188
train acc:  0.8359375
train loss:  0.35442548990249634
train gradient:  0.17045462163695718
iteration : 12189
train acc:  0.8984375
train loss:  0.2633804678916931
train gradient:  0.1174480327374121
iteration : 12190
train acc:  0.8671875
train loss:  0.30468758940696716
train gradient:  0.12820371779193313
iteration : 12191
train acc:  0.9140625
train loss:  0.2992593050003052
train gradient:  0.1712586199124915
iteration : 12192
train acc:  0.890625
train loss:  0.28329557180404663
train gradient:  0.18490201748289378
iteration : 12193
train acc:  0.8671875
train loss:  0.25686395168304443
train gradient:  0.1069163494601118
iteration : 12194
train acc:  0.875
train loss:  0.2903556227684021
train gradient:  0.16465008572059492
iteration : 12195
train acc:  0.8515625
train loss:  0.35333964228630066
train gradient:  0.17052774969623924
iteration : 12196
train acc:  0.875
train loss:  0.2801688015460968
train gradient:  0.13189781381601068
iteration : 12197
train acc:  0.8359375
train loss:  0.35439416766166687
train gradient:  0.14419771406708212
iteration : 12198
train acc:  0.890625
train loss:  0.2885461449623108
train gradient:  0.1425543063073702
iteration : 12199
train acc:  0.8671875
train loss:  0.31456267833709717
train gradient:  0.10853346128921539
iteration : 12200
train acc:  0.859375
train loss:  0.2644805908203125
train gradient:  0.1002786116398456
iteration : 12201
train acc:  0.8515625
train loss:  0.3357398509979248
train gradient:  0.11935612949036727
iteration : 12202
train acc:  0.875
train loss:  0.2687495946884155
train gradient:  0.09477712291139265
iteration : 12203
train acc:  0.921875
train loss:  0.2266993224620819
train gradient:  0.09148941807012412
iteration : 12204
train acc:  0.8671875
train loss:  0.36123815178871155
train gradient:  0.2203128431248945
iteration : 12205
train acc:  0.9375
train loss:  0.2231890857219696
train gradient:  0.06581383219896635
iteration : 12206
train acc:  0.828125
train loss:  0.3484615087509155
train gradient:  0.19031419196593757
iteration : 12207
train acc:  0.8984375
train loss:  0.2781122028827667
train gradient:  0.13816961998945138
iteration : 12208
train acc:  0.8671875
train loss:  0.3436698317527771
train gradient:  0.15144981569251287
iteration : 12209
train acc:  0.875
train loss:  0.3576950430870056
train gradient:  0.18515808804392292
iteration : 12210
train acc:  0.90625
train loss:  0.2897303104400635
train gradient:  0.13152806344976248
iteration : 12211
train acc:  0.8828125
train loss:  0.3259546756744385
train gradient:  0.182695967923381
iteration : 12212
train acc:  0.8515625
train loss:  0.32060691714286804
train gradient:  0.13043015747443643
iteration : 12213
train acc:  0.84375
train loss:  0.37473753094673157
train gradient:  0.18836508194333867
iteration : 12214
train acc:  0.90625
train loss:  0.26605135202407837
train gradient:  0.30150740354522254
iteration : 12215
train acc:  0.8203125
train loss:  0.3226596713066101
train gradient:  0.16018908506437943
iteration : 12216
train acc:  0.8359375
train loss:  0.3504212498664856
train gradient:  0.18897671744688657
iteration : 12217
train acc:  0.8515625
train loss:  0.29087162017822266
train gradient:  0.12001216024190109
iteration : 12218
train acc:  0.9453125
train loss:  0.22796258330345154
train gradient:  0.10167487246277784
iteration : 12219
train acc:  0.890625
train loss:  0.27794820070266724
train gradient:  0.12284567381459835
iteration : 12220
train acc:  0.859375
train loss:  0.29795533418655396
train gradient:  0.166094300227057
iteration : 12221
train acc:  0.921875
train loss:  0.2626804709434509
train gradient:  0.2146220639524713
iteration : 12222
train acc:  0.8515625
train loss:  0.3358100354671478
train gradient:  0.21784179464730266
iteration : 12223
train acc:  0.828125
train loss:  0.3426891565322876
train gradient:  0.24968886571130341
iteration : 12224
train acc:  0.890625
train loss:  0.2836378216743469
train gradient:  0.12452831102499026
iteration : 12225
train acc:  0.90625
train loss:  0.2736518681049347
train gradient:  0.15292610121118402
iteration : 12226
train acc:  0.875
train loss:  0.3132936358451843
train gradient:  0.17735097430770275
iteration : 12227
train acc:  0.859375
train loss:  0.3692604899406433
train gradient:  0.19121512223994963
iteration : 12228
train acc:  0.8828125
train loss:  0.2929612398147583
train gradient:  0.1511718227363605
iteration : 12229
train acc:  0.875
train loss:  0.287585973739624
train gradient:  0.1386557274791575
iteration : 12230
train acc:  0.8359375
train loss:  0.33716338872909546
train gradient:  0.18279416548121816
iteration : 12231
train acc:  0.8515625
train loss:  0.3810275197029114
train gradient:  0.23905428804544265
iteration : 12232
train acc:  0.8359375
train loss:  0.4348984956741333
train gradient:  0.28258344001241265
iteration : 12233
train acc:  0.8046875
train loss:  0.3378366231918335
train gradient:  0.18383967754947778
iteration : 12234
train acc:  0.859375
train loss:  0.34904783964157104
train gradient:  0.14002719310720946
iteration : 12235
train acc:  0.84375
train loss:  0.30525046586990356
train gradient:  0.15496685977254482
iteration : 12236
train acc:  0.8515625
train loss:  0.34460708498954773
train gradient:  0.16282469763156254
iteration : 12237
train acc:  0.8828125
train loss:  0.2791029214859009
train gradient:  0.13709635369988835
iteration : 12238
train acc:  0.8515625
train loss:  0.3590458035469055
train gradient:  0.2953840415791271
iteration : 12239
train acc:  0.8515625
train loss:  0.3565382659435272
train gradient:  0.2112227181175234
iteration : 12240
train acc:  0.859375
train loss:  0.336892694234848
train gradient:  0.18080139030846215
iteration : 12241
train acc:  0.8515625
train loss:  0.3584805727005005
train gradient:  0.14282627009956778
iteration : 12242
train acc:  0.765625
train loss:  0.4785829484462738
train gradient:  0.2574679009601373
iteration : 12243
train acc:  0.8984375
train loss:  0.2594080865383148
train gradient:  0.12114182890517869
iteration : 12244
train acc:  0.90625
train loss:  0.22961464524269104
train gradient:  0.09380054844959418
iteration : 12245
train acc:  0.84375
train loss:  0.40820783376693726
train gradient:  0.1788123338716749
iteration : 12246
train acc:  0.90625
train loss:  0.2976164221763611
train gradient:  0.18077429306342513
iteration : 12247
train acc:  0.875
train loss:  0.3772459030151367
train gradient:  0.14996541379123812
iteration : 12248
train acc:  0.8828125
train loss:  0.30283576250076294
train gradient:  0.17484485012147538
iteration : 12249
train acc:  0.8671875
train loss:  0.28640255331993103
train gradient:  0.42198581907078203
iteration : 12250
train acc:  0.8125
train loss:  0.3621169924736023
train gradient:  0.18845743444002178
iteration : 12251
train acc:  0.890625
train loss:  0.2693207263946533
train gradient:  0.10802115584386571
iteration : 12252
train acc:  0.8515625
train loss:  0.3754100799560547
train gradient:  0.19870729928190028
iteration : 12253
train acc:  0.8046875
train loss:  0.3690316677093506
train gradient:  0.18195661515218442
iteration : 12254
train acc:  0.8828125
train loss:  0.2871488928794861
train gradient:  0.12471962807939584
iteration : 12255
train acc:  0.8828125
train loss:  0.28682053089141846
train gradient:  0.11120281231805468
iteration : 12256
train acc:  0.859375
train loss:  0.28919559717178345
train gradient:  0.09657485038028643
iteration : 12257
train acc:  0.875
train loss:  0.3064694404602051
train gradient:  0.11877734880005665
iteration : 12258
train acc:  0.8828125
train loss:  0.26317739486694336
train gradient:  0.10448766351285992
iteration : 12259
train acc:  0.8359375
train loss:  0.38226035237312317
train gradient:  0.21742524862275714
iteration : 12260
train acc:  0.8515625
train loss:  0.30274707078933716
train gradient:  0.11951680272259117
iteration : 12261
train acc:  0.8671875
train loss:  0.2961111068725586
train gradient:  0.10856297225162369
iteration : 12262
train acc:  0.8046875
train loss:  0.35194724798202515
train gradient:  0.1762566448237975
iteration : 12263
train acc:  0.8203125
train loss:  0.3225958049297333
train gradient:  0.16725692605761022
iteration : 12264
train acc:  0.9140625
train loss:  0.2580304741859436
train gradient:  0.1126887546438604
iteration : 12265
train acc:  0.9296875
train loss:  0.23130404949188232
train gradient:  0.10728721070604856
iteration : 12266
train acc:  0.8515625
train loss:  0.294441819190979
train gradient:  0.11309516465799417
iteration : 12267
train acc:  0.8671875
train loss:  0.34362417459487915
train gradient:  0.1882863295003527
iteration : 12268
train acc:  0.8828125
train loss:  0.2995326519012451
train gradient:  0.12848327344572844
iteration : 12269
train acc:  0.859375
train loss:  0.36828672885894775
train gradient:  0.23997038881314825
iteration : 12270
train acc:  0.8515625
train loss:  0.3308015465736389
train gradient:  0.11627210831267418
iteration : 12271
train acc:  0.890625
train loss:  0.27602022886276245
train gradient:  0.13951084357233728
iteration : 12272
train acc:  0.84375
train loss:  0.3098866939544678
train gradient:  0.13756182186646643
iteration : 12273
train acc:  0.8125
train loss:  0.3361037075519562
train gradient:  0.13478017501205444
iteration : 12274
train acc:  0.8828125
train loss:  0.3162587881088257
train gradient:  0.13318456126173214
iteration : 12275
train acc:  0.828125
train loss:  0.4341713488101959
train gradient:  0.28437475028308773
iteration : 12276
train acc:  0.8359375
train loss:  0.3866273760795593
train gradient:  0.17678264868776633
iteration : 12277
train acc:  0.828125
train loss:  0.3962950110435486
train gradient:  0.2165176340407599
iteration : 12278
train acc:  0.78125
train loss:  0.44623762369155884
train gradient:  0.16415099848192727
iteration : 12279
train acc:  0.8203125
train loss:  0.34762656688690186
train gradient:  0.21641616904753058
iteration : 12280
train acc:  0.8828125
train loss:  0.2679293155670166
train gradient:  0.14489807750750522
iteration : 12281
train acc:  0.8515625
train loss:  0.32729601860046387
train gradient:  0.27508731916039864
iteration : 12282
train acc:  0.8671875
train loss:  0.29550546407699585
train gradient:  0.170768624344727
iteration : 12283
train acc:  0.84375
train loss:  0.396943062543869
train gradient:  0.17799778048516784
iteration : 12284
train acc:  0.8125
train loss:  0.41117632389068604
train gradient:  0.17916336961468807
iteration : 12285
train acc:  0.8203125
train loss:  0.310832142829895
train gradient:  0.13750340349599943
iteration : 12286
train acc:  0.8984375
train loss:  0.258389413356781
train gradient:  0.13231055990868792
iteration : 12287
train acc:  0.8671875
train loss:  0.3178003430366516
train gradient:  0.16169815944470367
iteration : 12288
train acc:  0.890625
train loss:  0.27073758840560913
train gradient:  0.13160131204208636
iteration : 12289
train acc:  0.8515625
train loss:  0.3690348267555237
train gradient:  0.1419380583453972
iteration : 12290
train acc:  0.921875
train loss:  0.2534574270248413
train gradient:  0.0889003147293495
iteration : 12291
train acc:  0.84375
train loss:  0.3153828978538513
train gradient:  0.21886771086532647
iteration : 12292
train acc:  0.8515625
train loss:  0.29898378252983093
train gradient:  0.12495040541574855
iteration : 12293
train acc:  0.8515625
train loss:  0.3491515815258026
train gradient:  0.1381394661712862
iteration : 12294
train acc:  0.890625
train loss:  0.2964106798171997
train gradient:  0.11338775964768436
iteration : 12295
train acc:  0.8984375
train loss:  0.2744826078414917
train gradient:  0.19160779303066422
iteration : 12296
train acc:  0.859375
train loss:  0.3494691848754883
train gradient:  0.18931585508332555
iteration : 12297
train acc:  0.8515625
train loss:  0.312267005443573
train gradient:  0.122270370217216
iteration : 12298
train acc:  0.8515625
train loss:  0.3114360570907593
train gradient:  0.17753798963186412
iteration : 12299
train acc:  0.90625
train loss:  0.3071114122867584
train gradient:  0.1529033393014199
iteration : 12300
train acc:  0.921875
train loss:  0.2616357207298279
train gradient:  0.07547445347472014
iteration : 12301
train acc:  0.875
train loss:  0.30489838123321533
train gradient:  0.14289485484872258
iteration : 12302
train acc:  0.84375
train loss:  0.3281572461128235
train gradient:  0.13208717376452128
iteration : 12303
train acc:  0.84375
train loss:  0.4316590428352356
train gradient:  0.3413150543862559
iteration : 12304
train acc:  0.828125
train loss:  0.37881234288215637
train gradient:  0.25520720954864007
iteration : 12305
train acc:  0.8515625
train loss:  0.3713374137878418
train gradient:  0.12732421844063077
iteration : 12306
train acc:  0.9140625
train loss:  0.23209500312805176
train gradient:  0.09957998640028301
iteration : 12307
train acc:  0.8984375
train loss:  0.21853721141815186
train gradient:  0.07551539710014327
iteration : 12308
train acc:  0.875
train loss:  0.25768381357192993
train gradient:  0.14314091540988316
iteration : 12309
train acc:  0.7890625
train loss:  0.43747180700302124
train gradient:  0.21605845086693876
iteration : 12310
train acc:  0.875
train loss:  0.3357353210449219
train gradient:  0.13697918622210933
iteration : 12311
train acc:  0.859375
train loss:  0.3021247684955597
train gradient:  0.10275026440146288
iteration : 12312
train acc:  0.828125
train loss:  0.349565714597702
train gradient:  0.19712335093056882
iteration : 12313
train acc:  0.8671875
train loss:  0.332988440990448
train gradient:  0.1667307608422227
iteration : 12314
train acc:  0.8046875
train loss:  0.3261229991912842
train gradient:  0.14322967307893952
iteration : 12315
train acc:  0.84375
train loss:  0.3584991693496704
train gradient:  0.18524986825751094
iteration : 12316
train acc:  0.859375
train loss:  0.3305266201496124
train gradient:  0.15998594700621668
iteration : 12317
train acc:  0.84375
train loss:  0.2878760099411011
train gradient:  0.10893965133400402
iteration : 12318
train acc:  0.890625
train loss:  0.3154379427433014
train gradient:  0.11456515953075948
iteration : 12319
train acc:  0.859375
train loss:  0.31476515531539917
train gradient:  0.13558776582570461
iteration : 12320
train acc:  0.890625
train loss:  0.25106772780418396
train gradient:  0.11353990804182638
iteration : 12321
train acc:  0.875
train loss:  0.3577963709831238
train gradient:  0.19034065361192454
iteration : 12322
train acc:  0.84375
train loss:  0.30125901103019714
train gradient:  0.11803808608060312
iteration : 12323
train acc:  0.84375
train loss:  0.3640004098415375
train gradient:  0.1947333150680946
iteration : 12324
train acc:  0.8203125
train loss:  0.4219386577606201
train gradient:  0.19891131396483663
iteration : 12325
train acc:  0.7734375
train loss:  0.41398510336875916
train gradient:  0.19566071311847627
iteration : 12326
train acc:  0.890625
train loss:  0.2936680316925049
train gradient:  0.11220843172736952
iteration : 12327
train acc:  0.9140625
train loss:  0.28928154706954956
train gradient:  0.10473114188971569
iteration : 12328
train acc:  0.8671875
train loss:  0.3048873543739319
train gradient:  0.15479829923288627
iteration : 12329
train acc:  0.875
train loss:  0.3432087004184723
train gradient:  0.13639120964844542
iteration : 12330
train acc:  0.890625
train loss:  0.24460817873477936
train gradient:  0.15778082663945253
iteration : 12331
train acc:  0.8515625
train loss:  0.31372755765914917
train gradient:  0.12751094871749663
iteration : 12332
train acc:  0.859375
train loss:  0.2846139967441559
train gradient:  0.11574603891249251
iteration : 12333
train acc:  0.8984375
train loss:  0.29397428035736084
train gradient:  0.14818403364932586
iteration : 12334
train acc:  0.9453125
train loss:  0.20687836408615112
train gradient:  0.07198645340940653
iteration : 12335
train acc:  0.8515625
train loss:  0.332609087228775
train gradient:  0.1233158881368814
iteration : 12336
train acc:  0.859375
train loss:  0.2975009083747864
train gradient:  0.10610864917109745
iteration : 12337
train acc:  0.859375
train loss:  0.3779894709587097
train gradient:  0.14731527174575207
iteration : 12338
train acc:  0.890625
train loss:  0.27970629930496216
train gradient:  0.11541636760115859
iteration : 12339
train acc:  0.8515625
train loss:  0.2962089776992798
train gradient:  0.11401750430658544
iteration : 12340
train acc:  0.9140625
train loss:  0.26483404636383057
train gradient:  0.1280178642000675
iteration : 12341
train acc:  0.8671875
train loss:  0.3293684720993042
train gradient:  0.11445630425841559
iteration : 12342
train acc:  0.9296875
train loss:  0.2545064389705658
train gradient:  0.10773354133027743
iteration : 12343
train acc:  0.8515625
train loss:  0.27243098616600037
train gradient:  0.09061689639593062
iteration : 12344
train acc:  0.8125
train loss:  0.36355483531951904
train gradient:  0.1867607208652493
iteration : 12345
train acc:  0.8984375
train loss:  0.2639537453651428
train gradient:  0.10035507342148721
iteration : 12346
train acc:  0.7890625
train loss:  0.45845282077789307
train gradient:  0.25380042118334684
iteration : 12347
train acc:  0.7890625
train loss:  0.3741368055343628
train gradient:  0.1780842505435061
iteration : 12348
train acc:  0.828125
train loss:  0.37913864850997925
train gradient:  0.16649938500807143
iteration : 12349
train acc:  0.890625
train loss:  0.278639018535614
train gradient:  0.14009515803371259
iteration : 12350
train acc:  0.78125
train loss:  0.40892189741134644
train gradient:  0.18409655054352175
iteration : 12351
train acc:  0.796875
train loss:  0.3960656225681305
train gradient:  0.18887684582047537
iteration : 12352
train acc:  0.8671875
train loss:  0.29147663712501526
train gradient:  0.12695062804533003
iteration : 12353
train acc:  0.890625
train loss:  0.23866133391857147
train gradient:  0.12005024123396207
iteration : 12354
train acc:  0.8671875
train loss:  0.32388997077941895
train gradient:  0.129025268658211
iteration : 12355
train acc:  0.8671875
train loss:  0.30724382400512695
train gradient:  0.13853401690136397
iteration : 12356
train acc:  0.921875
train loss:  0.2518870234489441
train gradient:  0.11016747776031151
iteration : 12357
train acc:  0.875
train loss:  0.2735176682472229
train gradient:  0.1313408692984438
iteration : 12358
train acc:  0.8125
train loss:  0.43830227851867676
train gradient:  0.29327787210627226
iteration : 12359
train acc:  0.875
train loss:  0.301317423582077
train gradient:  0.1405247663695047
iteration : 12360
train acc:  0.859375
train loss:  0.3007909059524536
train gradient:  0.12824187716018742
iteration : 12361
train acc:  0.828125
train loss:  0.38374513387680054
train gradient:  0.21370026027518044
iteration : 12362
train acc:  0.8671875
train loss:  0.2986338436603546
train gradient:  0.20661208238961204
iteration : 12363
train acc:  0.84375
train loss:  0.33626389503479004
train gradient:  0.1458913454827293
iteration : 12364
train acc:  0.875
train loss:  0.29144805669784546
train gradient:  0.1327769164102376
iteration : 12365
train acc:  0.890625
train loss:  0.3219776153564453
train gradient:  0.12313543357355598
iteration : 12366
train acc:  0.859375
train loss:  0.3323652148246765
train gradient:  0.12672817199361988
iteration : 12367
train acc:  0.8671875
train loss:  0.307872474193573
train gradient:  0.11248777154591093
iteration : 12368
train acc:  0.84375
train loss:  0.3139362931251526
train gradient:  0.15928390524669817
iteration : 12369
train acc:  0.8359375
train loss:  0.381158709526062
train gradient:  0.1913398290368981
iteration : 12370
train acc:  0.875
train loss:  0.2977886497974396
train gradient:  0.145781711613936
iteration : 12371
train acc:  0.90625
train loss:  0.27609720826148987
train gradient:  0.10227741668826792
iteration : 12372
train acc:  0.921875
train loss:  0.23808634281158447
train gradient:  0.07725290826922353
iteration : 12373
train acc:  0.8359375
train loss:  0.36222410202026367
train gradient:  0.16103547089649622
iteration : 12374
train acc:  0.9375
train loss:  0.2356448918581009
train gradient:  0.10499701471007743
iteration : 12375
train acc:  0.8828125
train loss:  0.3037366271018982
train gradient:  0.09794416210535997
iteration : 12376
train acc:  0.828125
train loss:  0.33791863918304443
train gradient:  0.12850699778769936
iteration : 12377
train acc:  0.8515625
train loss:  0.3378515839576721
train gradient:  0.21334233105019718
iteration : 12378
train acc:  0.8125
train loss:  0.3632344603538513
train gradient:  0.2111053025395713
iteration : 12379
train acc:  0.8359375
train loss:  0.342625230550766
train gradient:  0.1396590830265398
iteration : 12380
train acc:  0.8828125
train loss:  0.3257063329219818
train gradient:  0.13015542446962025
iteration : 12381
train acc:  0.828125
train loss:  0.37655898928642273
train gradient:  0.1990293394880941
iteration : 12382
train acc:  0.84375
train loss:  0.33937370777130127
train gradient:  0.1262042773511927
iteration : 12383
train acc:  0.84375
train loss:  0.33926478028297424
train gradient:  0.18563256289477248
iteration : 12384
train acc:  0.890625
train loss:  0.25477245450019836
train gradient:  0.11592928473645515
iteration : 12385
train acc:  0.90625
train loss:  0.2717030942440033
train gradient:  0.12171070650472549
iteration : 12386
train acc:  0.8515625
train loss:  0.3694872260093689
train gradient:  0.1242354587827894
iteration : 12387
train acc:  0.84375
train loss:  0.3495529592037201
train gradient:  0.19860495889382623
iteration : 12388
train acc:  0.9140625
train loss:  0.2671443223953247
train gradient:  0.14467903996942652
iteration : 12389
train acc:  0.828125
train loss:  0.32846304774284363
train gradient:  0.14249626204977342
iteration : 12390
train acc:  0.8671875
train loss:  0.30774474143981934
train gradient:  0.13525481048928328
iteration : 12391
train acc:  0.875
train loss:  0.30714792013168335
train gradient:  0.10919140643332928
iteration : 12392
train acc:  0.890625
train loss:  0.295737087726593
train gradient:  0.14973684544215643
iteration : 12393
train acc:  0.8203125
train loss:  0.3156205415725708
train gradient:  0.17072712408667723
iteration : 12394
train acc:  0.8359375
train loss:  0.3147519528865814
train gradient:  0.17495510928128377
iteration : 12395
train acc:  0.828125
train loss:  0.40157437324523926
train gradient:  0.15535145888901566
iteration : 12396
train acc:  0.796875
train loss:  0.41712474822998047
train gradient:  0.26058094431225076
iteration : 12397
train acc:  0.84375
train loss:  0.3508031368255615
train gradient:  0.16361082773020447
iteration : 12398
train acc:  0.875
train loss:  0.27705544233322144
train gradient:  0.12484374567949091
iteration : 12399
train acc:  0.8671875
train loss:  0.35365965962409973
train gradient:  0.1512377556753337
iteration : 12400
train acc:  0.890625
train loss:  0.2784777283668518
train gradient:  0.11011409856712973
iteration : 12401
train acc:  0.875
train loss:  0.3143012821674347
train gradient:  0.1496232681784017
iteration : 12402
train acc:  0.84375
train loss:  0.4054252505302429
train gradient:  0.2316344555380813
iteration : 12403
train acc:  0.8046875
train loss:  0.39255157113075256
train gradient:  0.2366711076502795
iteration : 12404
train acc:  0.8125
train loss:  0.3630172610282898
train gradient:  0.2056207117323714
iteration : 12405
train acc:  0.90625
train loss:  0.2820844054222107
train gradient:  0.1744632537926975
iteration : 12406
train acc:  0.90625
train loss:  0.26421332359313965
train gradient:  0.08943258523707003
iteration : 12407
train acc:  0.8125
train loss:  0.4182867407798767
train gradient:  0.20025134696879288
iteration : 12408
train acc:  0.890625
train loss:  0.3022722601890564
train gradient:  0.1551411548106619
iteration : 12409
train acc:  0.828125
train loss:  0.36773356795310974
train gradient:  0.16143479748334
iteration : 12410
train acc:  0.8125
train loss:  0.4096720218658447
train gradient:  0.23227770687294136
iteration : 12411
train acc:  0.84375
train loss:  0.3265179693698883
train gradient:  0.15150789686318605
iteration : 12412
train acc:  0.8671875
train loss:  0.3032001256942749
train gradient:  0.10824502489387228
iteration : 12413
train acc:  0.8359375
train loss:  0.35423487424850464
train gradient:  0.16690089429511512
iteration : 12414
train acc:  0.796875
train loss:  0.4064132571220398
train gradient:  0.3163999194407266
iteration : 12415
train acc:  0.84375
train loss:  0.30675429105758667
train gradient:  0.14698843755047278
iteration : 12416
train acc:  0.84375
train loss:  0.33622539043426514
train gradient:  0.1367176205367025
iteration : 12417
train acc:  0.828125
train loss:  0.3519885540008545
train gradient:  0.1330616072272215
iteration : 12418
train acc:  0.8984375
train loss:  0.2743509113788605
train gradient:  0.1263017756451275
iteration : 12419
train acc:  0.7578125
train loss:  0.4466055631637573
train gradient:  0.2140392845422387
iteration : 12420
train acc:  0.7890625
train loss:  0.39443379640579224
train gradient:  0.16536654501331166
iteration : 12421
train acc:  0.875
train loss:  0.2744339108467102
train gradient:  0.11904993060289518
iteration : 12422
train acc:  0.828125
train loss:  0.3344099819660187
train gradient:  0.16473459938137863
iteration : 12423
train acc:  0.8984375
train loss:  0.2676061987876892
train gradient:  0.16213256115532512
iteration : 12424
train acc:  0.8828125
train loss:  0.2691459655761719
train gradient:  0.11189811239714162
iteration : 12425
train acc:  0.8828125
train loss:  0.2685890197753906
train gradient:  0.12594575650542056
iteration : 12426
train acc:  0.84375
train loss:  0.3531133830547333
train gradient:  0.1464009650591113
iteration : 12427
train acc:  0.828125
train loss:  0.3680982291698456
train gradient:  0.12156438807680164
iteration : 12428
train acc:  0.8671875
train loss:  0.29366201162338257
train gradient:  0.15639835494232218
iteration : 12429
train acc:  0.859375
train loss:  0.3035338819026947
train gradient:  0.10623275072379147
iteration : 12430
train acc:  0.890625
train loss:  0.3314962685108185
train gradient:  0.29184778477424583
iteration : 12431
train acc:  0.8828125
train loss:  0.26348966360092163
train gradient:  0.12746673446513143
iteration : 12432
train acc:  0.859375
train loss:  0.34133800864219666
train gradient:  0.3415679272556332
iteration : 12433
train acc:  0.8828125
train loss:  0.29604536294937134
train gradient:  0.10885544050979643
iteration : 12434
train acc:  0.8203125
train loss:  0.37071993947029114
train gradient:  0.20609267375083862
iteration : 12435
train acc:  0.9140625
train loss:  0.2777320444583893
train gradient:  0.10480570527078081
iteration : 12436
train acc:  0.8984375
train loss:  0.26985588669776917
train gradient:  0.08906456497675239
iteration : 12437
train acc:  0.8671875
train loss:  0.2715384364128113
train gradient:  0.126358420583023
iteration : 12438
train acc:  0.8203125
train loss:  0.4402170777320862
train gradient:  0.2055935036378612
iteration : 12439
train acc:  0.890625
train loss:  0.2986864447593689
train gradient:  0.1385603789069919
iteration : 12440
train acc:  0.78125
train loss:  0.3610168397426605
train gradient:  0.20066133752631282
iteration : 12441
train acc:  0.8828125
train loss:  0.31145593523979187
train gradient:  0.15512456905073502
iteration : 12442
train acc:  0.890625
train loss:  0.26915860176086426
train gradient:  0.12803273950744432
iteration : 12443
train acc:  0.828125
train loss:  0.3765118718147278
train gradient:  0.15800998553033432
iteration : 12444
train acc:  0.8828125
train loss:  0.35351285338401794
train gradient:  0.12952816497274672
iteration : 12445
train acc:  0.8828125
train loss:  0.3363952040672302
train gradient:  0.1378004112920473
iteration : 12446
train acc:  0.890625
train loss:  0.2526106536388397
train gradient:  0.10119135827961037
iteration : 12447
train acc:  0.859375
train loss:  0.33934980630874634
train gradient:  0.16673258481619263
iteration : 12448
train acc:  0.9140625
train loss:  0.2663727402687073
train gradient:  0.1075886399422476
iteration : 12449
train acc:  0.890625
train loss:  0.3036333918571472
train gradient:  0.10113696460818263
iteration : 12450
train acc:  0.8515625
train loss:  0.3964521288871765
train gradient:  0.1938866280273864
iteration : 12451
train acc:  0.8671875
train loss:  0.32187435030937195
train gradient:  0.16594450503009578
iteration : 12452
train acc:  0.8828125
train loss:  0.2883179485797882
train gradient:  0.1454724773307286
iteration : 12453
train acc:  0.9140625
train loss:  0.24437230825424194
train gradient:  0.10311143584246193
iteration : 12454
train acc:  0.890625
train loss:  0.2599351108074188
train gradient:  0.0981045764151695
iteration : 12455
train acc:  0.9140625
train loss:  0.23945803940296173
train gradient:  0.0857552634087614
iteration : 12456
train acc:  0.8828125
train loss:  0.2845577001571655
train gradient:  0.08102251237372483
iteration : 12457
train acc:  0.8828125
train loss:  0.27355578541755676
train gradient:  0.11137798591701951
iteration : 12458
train acc:  0.859375
train loss:  0.30091750621795654
train gradient:  0.14062720942691634
iteration : 12459
train acc:  0.8984375
train loss:  0.25927385687828064
train gradient:  0.09809169195224064
iteration : 12460
train acc:  0.84375
train loss:  0.37533658742904663
train gradient:  0.16860600488459132
iteration : 12461
train acc:  0.828125
train loss:  0.3443930149078369
train gradient:  0.17640009330553644
iteration : 12462
train acc:  0.8828125
train loss:  0.27170002460479736
train gradient:  0.09517011355694449
iteration : 12463
train acc:  0.890625
train loss:  0.2837291359901428
train gradient:  0.11730267847837526
iteration : 12464
train acc:  0.84375
train loss:  0.32803744077682495
train gradient:  0.13867334906892856
iteration : 12465
train acc:  0.8359375
train loss:  0.3299976587295532
train gradient:  0.18907696476348915
iteration : 12466
train acc:  0.8828125
train loss:  0.28454726934432983
train gradient:  0.099298622001028
iteration : 12467
train acc:  0.8984375
train loss:  0.3016377091407776
train gradient:  0.15910422867797672
iteration : 12468
train acc:  0.8828125
train loss:  0.28141871094703674
train gradient:  0.14738178670084695
iteration : 12469
train acc:  0.8828125
train loss:  0.2935442626476288
train gradient:  0.14262918872619193
iteration : 12470
train acc:  0.859375
train loss:  0.2944768965244293
train gradient:  0.14183482844748047
iteration : 12471
train acc:  0.8671875
train loss:  0.28019610047340393
train gradient:  0.08799964358395229
iteration : 12472
train acc:  0.828125
train loss:  0.37712162733078003
train gradient:  0.1982208203560079
iteration : 12473
train acc:  0.8984375
train loss:  0.2551889419555664
train gradient:  0.11119597243461499
iteration : 12474
train acc:  0.8671875
train loss:  0.286918580532074
train gradient:  0.10944783217978203
iteration : 12475
train acc:  0.859375
train loss:  0.3412628173828125
train gradient:  0.13602485707358325
iteration : 12476
train acc:  0.8828125
train loss:  0.3092114329338074
train gradient:  0.13033545072393493
iteration : 12477
train acc:  0.875
train loss:  0.26030611991882324
train gradient:  0.09890890058516598
iteration : 12478
train acc:  0.8515625
train loss:  0.3160874843597412
train gradient:  0.14687276723570597
iteration : 12479
train acc:  0.859375
train loss:  0.27977824211120605
train gradient:  0.13541344598212957
iteration : 12480
train acc:  0.8671875
train loss:  0.2861379086971283
train gradient:  0.11189380902719177
iteration : 12481
train acc:  0.84375
train loss:  0.3118058443069458
train gradient:  0.13599768885989283
iteration : 12482
train acc:  0.8203125
train loss:  0.3129197955131531
train gradient:  0.18367116987135823
iteration : 12483
train acc:  0.9140625
train loss:  0.25485238432884216
train gradient:  0.09434626711392757
iteration : 12484
train acc:  0.875
train loss:  0.2794952690601349
train gradient:  0.12224515084574458
iteration : 12485
train acc:  0.890625
train loss:  0.29427778720855713
train gradient:  0.13683849231422462
iteration : 12486
train acc:  0.84375
train loss:  0.3608362674713135
train gradient:  0.18493626259914236
iteration : 12487
train acc:  0.7890625
train loss:  0.410204142332077
train gradient:  0.269650431598368
iteration : 12488
train acc:  0.859375
train loss:  0.34869062900543213
train gradient:  0.20436750605592746
iteration : 12489
train acc:  0.84375
train loss:  0.3331490755081177
train gradient:  0.16853268979703023
iteration : 12490
train acc:  0.8359375
train loss:  0.3212531805038452
train gradient:  0.16026557021019988
iteration : 12491
train acc:  0.890625
train loss:  0.29544270038604736
train gradient:  0.135073112223075
iteration : 12492
train acc:  0.8125
train loss:  0.4157422184944153
train gradient:  0.22123324408146194
iteration : 12493
train acc:  0.8203125
train loss:  0.39982450008392334
train gradient:  0.1953981929218935
iteration : 12494
train acc:  0.8828125
train loss:  0.25344282388687134
train gradient:  0.11842270683967865
iteration : 12495
train acc:  0.875
train loss:  0.3004641532897949
train gradient:  0.21190918064044134
iteration : 12496
train acc:  0.8359375
train loss:  0.3484509587287903
train gradient:  0.22568031199434282
iteration : 12497
train acc:  0.859375
train loss:  0.2599702477455139
train gradient:  0.1284527136826043
iteration : 12498
train acc:  0.890625
train loss:  0.26031234860420227
train gradient:  0.11065072592268517
iteration : 12499
train acc:  0.828125
train loss:  0.4254588782787323
train gradient:  0.23472350496094457
iteration : 12500
train acc:  0.859375
train loss:  0.39118677377700806
train gradient:  0.19896623069010666
iteration : 12501
train acc:  0.7890625
train loss:  0.42319396138191223
train gradient:  0.3403396953390626
iteration : 12502
train acc:  0.84375
train loss:  0.32623881101608276
train gradient:  0.11761666220524584
iteration : 12503
train acc:  0.9140625
train loss:  0.2647930383682251
train gradient:  0.11079227087665301
iteration : 12504
train acc:  0.859375
train loss:  0.2735981345176697
train gradient:  0.09597630306645044
iteration : 12505
train acc:  0.84375
train loss:  0.37233084440231323
train gradient:  0.18198806752295474
iteration : 12506
train acc:  0.875
train loss:  0.24828766286373138
train gradient:  0.09519948953148354
iteration : 12507
train acc:  0.8828125
train loss:  0.26398614048957825
train gradient:  0.1293092605874367
iteration : 12508
train acc:  0.859375
train loss:  0.30832597613334656
train gradient:  0.11082831043068764
iteration : 12509
train acc:  0.8359375
train loss:  0.3301093578338623
train gradient:  0.20695368952964155
iteration : 12510
train acc:  0.890625
train loss:  0.23736503720283508
train gradient:  0.08486516339028242
iteration : 12511
train acc:  0.8984375
train loss:  0.2709849774837494
train gradient:  0.13937157237440667
iteration : 12512
train acc:  0.8515625
train loss:  0.395483136177063
train gradient:  0.2358147292132864
iteration : 12513
train acc:  0.90625
train loss:  0.23933979868888855
train gradient:  0.11725949438966607
iteration : 12514
train acc:  0.84375
train loss:  0.3181088864803314
train gradient:  0.19951568844660597
iteration : 12515
train acc:  0.90625
train loss:  0.27498990297317505
train gradient:  0.14831127781411713
iteration : 12516
train acc:  0.8828125
train loss:  0.2954115569591522
train gradient:  0.13741925182067932
iteration : 12517
train acc:  0.8515625
train loss:  0.3258136510848999
train gradient:  0.17148585590670207
iteration : 12518
train acc:  0.8046875
train loss:  0.34131920337677
train gradient:  0.19605502280360904
iteration : 12519
train acc:  0.921875
train loss:  0.25946080684661865
train gradient:  0.09802792882652617
iteration : 12520
train acc:  0.921875
train loss:  0.23241332173347473
train gradient:  0.08553932820777695
iteration : 12521
train acc:  0.828125
train loss:  0.3065100908279419
train gradient:  0.13591269944036657
iteration : 12522
train acc:  0.828125
train loss:  0.34941649436950684
train gradient:  0.18707794450017068
iteration : 12523
train acc:  0.8359375
train loss:  0.36752384901046753
train gradient:  0.15295418629069255
iteration : 12524
train acc:  0.8359375
train loss:  0.29436084628105164
train gradient:  0.15885421369241035
iteration : 12525
train acc:  0.8828125
train loss:  0.28795188665390015
train gradient:  0.18251958629004658
iteration : 12526
train acc:  0.8671875
train loss:  0.34811532497406006
train gradient:  0.12891805056717748
iteration : 12527
train acc:  0.8828125
train loss:  0.2768867313861847
train gradient:  0.10206820354995363
iteration : 12528
train acc:  0.8359375
train loss:  0.380085825920105
train gradient:  0.21178103120534214
iteration : 12529
train acc:  0.8671875
train loss:  0.35389721393585205
train gradient:  0.18555028359362996
iteration : 12530
train acc:  0.875
train loss:  0.2935636639595032
train gradient:  0.11908781299184258
iteration : 12531
train acc:  0.859375
train loss:  0.38888734579086304
train gradient:  0.33173474775479667
iteration : 12532
train acc:  0.875
train loss:  0.2505114674568176
train gradient:  0.10580362948431561
iteration : 12533
train acc:  0.8359375
train loss:  0.3536105751991272
train gradient:  0.1482274066314538
iteration : 12534
train acc:  0.875
train loss:  0.3008715510368347
train gradient:  0.16502649670979552
iteration : 12535
train acc:  0.8828125
train loss:  0.24761426448822021
train gradient:  0.082824477663206
iteration : 12536
train acc:  0.8203125
train loss:  0.36794590950012207
train gradient:  0.20696634055125157
iteration : 12537
train acc:  0.9140625
train loss:  0.2606689929962158
train gradient:  0.14897966528929923
iteration : 12538
train acc:  0.875
train loss:  0.22711917757987976
train gradient:  0.12563044843512
iteration : 12539
train acc:  0.875
train loss:  0.28363263607025146
train gradient:  0.10578596160182942
iteration : 12540
train acc:  0.8359375
train loss:  0.3175048232078552
train gradient:  0.13052667533368853
iteration : 12541
train acc:  0.8828125
train loss:  0.30830490589141846
train gradient:  0.14555889289185178
iteration : 12542
train acc:  0.8828125
train loss:  0.3590257167816162
train gradient:  0.14938813285284863
iteration : 12543
train acc:  0.8515625
train loss:  0.39563310146331787
train gradient:  0.23653804284992402
iteration : 12544
train acc:  0.859375
train loss:  0.2747984826564789
train gradient:  0.12319627610623106
iteration : 12545
train acc:  0.8125
train loss:  0.4077718257904053
train gradient:  0.20472646762036967
iteration : 12546
train acc:  0.84375
train loss:  0.3305606544017792
train gradient:  0.11456796913389844
iteration : 12547
train acc:  0.8359375
train loss:  0.42686617374420166
train gradient:  0.2177593592216244
iteration : 12548
train acc:  0.859375
train loss:  0.2892320156097412
train gradient:  0.11116273285000275
iteration : 12549
train acc:  0.8671875
train loss:  0.31223011016845703
train gradient:  0.15702192607217014
iteration : 12550
train acc:  0.8828125
train loss:  0.27251136302948
train gradient:  0.5867241299353481
iteration : 12551
train acc:  0.8515625
train loss:  0.33926671743392944
train gradient:  0.13874406381470572
iteration : 12552
train acc:  0.890625
train loss:  0.2430858016014099
train gradient:  0.08849712166715956
iteration : 12553
train acc:  0.8125
train loss:  0.41967010498046875
train gradient:  0.20130948065660081
iteration : 12554
train acc:  0.875
train loss:  0.3570060431957245
train gradient:  0.15547276396144222
iteration : 12555
train acc:  0.8515625
train loss:  0.35606229305267334
train gradient:  0.15415126565222545
iteration : 12556
train acc:  0.8828125
train loss:  0.30713436007499695
train gradient:  0.16924992701971814
iteration : 12557
train acc:  0.8203125
train loss:  0.3239993751049042
train gradient:  0.17004106842801833
iteration : 12558
train acc:  0.890625
train loss:  0.2525382936000824
train gradient:  0.16440983745413915
iteration : 12559
train acc:  0.875
train loss:  0.2676662504673004
train gradient:  0.09104909878681433
iteration : 12560
train acc:  0.859375
train loss:  0.35308438539505005
train gradient:  0.12969119414139052
iteration : 12561
train acc:  0.8671875
train loss:  0.29086530208587646
train gradient:  0.13617532328928494
iteration : 12562
train acc:  0.828125
train loss:  0.2982300817966461
train gradient:  0.15842921849092795
iteration : 12563
train acc:  0.8046875
train loss:  0.3730353116989136
train gradient:  0.18377754479459507
iteration : 12564
train acc:  0.828125
train loss:  0.3764931261539459
train gradient:  0.19266080970766192
iteration : 12565
train acc:  0.8671875
train loss:  0.353024959564209
train gradient:  0.1785922809066816
iteration : 12566
train acc:  0.8515625
train loss:  0.30210646986961365
train gradient:  0.1591872537654998
iteration : 12567
train acc:  0.890625
train loss:  0.28710755705833435
train gradient:  0.16674658747682478
iteration : 12568
train acc:  0.875
train loss:  0.25455212593078613
train gradient:  0.09687518179758693
iteration : 12569
train acc:  0.875
train loss:  0.31926077604293823
train gradient:  0.13296748791925367
iteration : 12570
train acc:  0.7890625
train loss:  0.4172879755496979
train gradient:  0.1626019627346184
iteration : 12571
train acc:  0.921875
train loss:  0.26880982518196106
train gradient:  0.1010102254905191
iteration : 12572
train acc:  0.84375
train loss:  0.42796266078948975
train gradient:  0.20603518895171075
iteration : 12573
train acc:  0.9296875
train loss:  0.23106297850608826
train gradient:  0.10986827216615937
iteration : 12574
train acc:  0.84375
train loss:  0.37867963314056396
train gradient:  0.15312586885531593
iteration : 12575
train acc:  0.84375
train loss:  0.31723666191101074
train gradient:  0.16804884300393125
iteration : 12576
train acc:  0.8125
train loss:  0.48866111040115356
train gradient:  0.2960463869365931
iteration : 12577
train acc:  0.84375
train loss:  0.3074246048927307
train gradient:  0.1231022245282207
iteration : 12578
train acc:  0.8359375
train loss:  0.35056614875793457
train gradient:  0.24495736741081808
iteration : 12579
train acc:  0.8828125
train loss:  0.32173362374305725
train gradient:  0.11500216774475451
iteration : 12580
train acc:  0.8828125
train loss:  0.23037894070148468
train gradient:  0.09369185301381025
iteration : 12581
train acc:  0.875
train loss:  0.26014602184295654
train gradient:  0.18167292940560176
iteration : 12582
train acc:  0.875
train loss:  0.32746976613998413
train gradient:  0.18028880531558472
iteration : 12583
train acc:  0.828125
train loss:  0.40484559535980225
train gradient:  0.181316093129486
iteration : 12584
train acc:  0.8671875
train loss:  0.28954628109931946
train gradient:  0.16026227289607606
iteration : 12585
train acc:  0.8359375
train loss:  0.3057728707790375
train gradient:  0.13992257449949985
iteration : 12586
train acc:  0.8984375
train loss:  0.2972041964530945
train gradient:  0.16816245063456126
iteration : 12587
train acc:  0.8671875
train loss:  0.2686137557029724
train gradient:  0.10111529412135475
iteration : 12588
train acc:  0.90625
train loss:  0.22934602200984955
train gradient:  0.11558136279824743
iteration : 12589
train acc:  0.875
train loss:  0.3169686496257782
train gradient:  0.17704565856358667
iteration : 12590
train acc:  0.84375
train loss:  0.33598560094833374
train gradient:  0.12136082115547603
iteration : 12591
train acc:  0.859375
train loss:  0.3137098550796509
train gradient:  0.1250645987706398
iteration : 12592
train acc:  0.7890625
train loss:  0.4214248061180115
train gradient:  0.2920834350226566
iteration : 12593
train acc:  0.875
train loss:  0.30246931314468384
train gradient:  0.17864049304191598
iteration : 12594
train acc:  0.8671875
train loss:  0.32299643754959106
train gradient:  0.16008001341787936
iteration : 12595
train acc:  0.8828125
train loss:  0.3331984579563141
train gradient:  0.12775357102387613
iteration : 12596
train acc:  0.8203125
train loss:  0.31499969959259033
train gradient:  0.10794326902765332
iteration : 12597
train acc:  0.875
train loss:  0.2920350432395935
train gradient:  0.22373123927559074
iteration : 12598
train acc:  0.8828125
train loss:  0.29226425290107727
train gradient:  0.08662710278873725
iteration : 12599
train acc:  0.859375
train loss:  0.3514987826347351
train gradient:  0.13691199252043218
iteration : 12600
train acc:  0.84375
train loss:  0.3507973253726959
train gradient:  0.1768582983523736
iteration : 12601
train acc:  0.8984375
train loss:  0.26701775193214417
train gradient:  0.11586114094249482
iteration : 12602
train acc:  0.8359375
train loss:  0.36080431938171387
train gradient:  0.14131610008450585
iteration : 12603
train acc:  0.8125
train loss:  0.3620522618293762
train gradient:  0.13069475116090978
iteration : 12604
train acc:  0.859375
train loss:  0.3014202117919922
train gradient:  0.11821921556082818
iteration : 12605
train acc:  0.8203125
train loss:  0.4066448211669922
train gradient:  0.2342379280995534
iteration : 12606
train acc:  0.8203125
train loss:  0.35352352261543274
train gradient:  0.149939200163844
iteration : 12607
train acc:  0.8828125
train loss:  0.30159181356430054
train gradient:  0.14600658815300388
iteration : 12608
train acc:  0.8125
train loss:  0.37699535489082336
train gradient:  0.18628089198136336
iteration : 12609
train acc:  0.8671875
train loss:  0.2934023439884186
train gradient:  0.1351120878152416
iteration : 12610
train acc:  0.8984375
train loss:  0.23009268939495087
train gradient:  0.13258666500472094
iteration : 12611
train acc:  0.796875
train loss:  0.38574814796447754
train gradient:  0.19519729256046583
iteration : 12612
train acc:  0.90625
train loss:  0.26554328203201294
train gradient:  0.125440009317379
iteration : 12613
train acc:  0.8515625
train loss:  0.3109414577484131
train gradient:  0.11930414348590071
iteration : 12614
train acc:  0.84375
train loss:  0.32284995913505554
train gradient:  0.14699541581919542
iteration : 12615
train acc:  0.859375
train loss:  0.40616893768310547
train gradient:  0.20003510368745564
iteration : 12616
train acc:  0.84375
train loss:  0.3297377824783325
train gradient:  0.19385108218102964
iteration : 12617
train acc:  0.8046875
train loss:  0.3833596706390381
train gradient:  0.205839432027597
iteration : 12618
train acc:  0.859375
train loss:  0.3413039445877075
train gradient:  0.17774738135714502
iteration : 12619
train acc:  0.828125
train loss:  0.3184854984283447
train gradient:  0.12478329050096884
iteration : 12620
train acc:  0.9140625
train loss:  0.2651922106742859
train gradient:  0.11529155824262452
iteration : 12621
train acc:  0.875
train loss:  0.2873876690864563
train gradient:  0.09443417511009912
iteration : 12622
train acc:  0.8671875
train loss:  0.37795788049697876
train gradient:  0.24454467082245623
iteration : 12623
train acc:  0.8671875
train loss:  0.30930203199386597
train gradient:  0.12800599030697413
iteration : 12624
train acc:  0.8359375
train loss:  0.33318498730659485
train gradient:  0.16276473776218062
iteration : 12625
train acc:  0.8515625
train loss:  0.3653634190559387
train gradient:  0.1793589865553069
iteration : 12626
train acc:  0.84375
train loss:  0.37050431966781616
train gradient:  0.1471502630547714
iteration : 12627
train acc:  0.8671875
train loss:  0.29972177743911743
train gradient:  0.11270603779232395
iteration : 12628
train acc:  0.8515625
train loss:  0.312430739402771
train gradient:  0.14316533573250745
iteration : 12629
train acc:  0.8515625
train loss:  0.3031231164932251
train gradient:  0.1076670553192821
iteration : 12630
train acc:  0.859375
train loss:  0.3108828365802765
train gradient:  0.19309986258113165
iteration : 12631
train acc:  0.8671875
train loss:  0.33082205057144165
train gradient:  0.2169965335267138
iteration : 12632
train acc:  0.859375
train loss:  0.3430761992931366
train gradient:  0.1991076338122738
iteration : 12633
train acc:  0.890625
train loss:  0.26750653982162476
train gradient:  0.1075076033919984
iteration : 12634
train acc:  0.8671875
train loss:  0.273780882358551
train gradient:  0.13560954474568426
iteration : 12635
train acc:  0.84375
train loss:  0.3560642600059509
train gradient:  0.18607018932797176
iteration : 12636
train acc:  0.8828125
train loss:  0.29329800605773926
train gradient:  0.11847520672039137
iteration : 12637
train acc:  0.8828125
train loss:  0.25174593925476074
train gradient:  0.10925285024337196
iteration : 12638
train acc:  0.8671875
train loss:  0.2842963933944702
train gradient:  0.1173671812407267
iteration : 12639
train acc:  0.8203125
train loss:  0.31875091791152954
train gradient:  0.13923220682346216
iteration : 12640
train acc:  0.8359375
train loss:  0.4254796802997589
train gradient:  0.2376140883821567
iteration : 12641
train acc:  0.8671875
train loss:  0.29319852590560913
train gradient:  0.124420619226464
iteration : 12642
train acc:  0.8671875
train loss:  0.2702639698982239
train gradient:  0.13124991182160745
iteration : 12643
train acc:  0.8203125
train loss:  0.35527297854423523
train gradient:  0.14667018155795225
iteration : 12644
train acc:  0.8515625
train loss:  0.2846095860004425
train gradient:  0.09622585042858503
iteration : 12645
train acc:  0.84375
train loss:  0.3063785433769226
train gradient:  0.12319114747950063
iteration : 12646
train acc:  0.8359375
train loss:  0.42119914293289185
train gradient:  0.2263162034555501
iteration : 12647
train acc:  0.8203125
train loss:  0.36802762746810913
train gradient:  0.173572696719334
iteration : 12648
train acc:  0.8984375
train loss:  0.26139360666275024
train gradient:  0.09638872234716732
iteration : 12649
train acc:  0.875
train loss:  0.3146207630634308
train gradient:  0.13859158993244983
iteration : 12650
train acc:  0.84375
train loss:  0.33486735820770264
train gradient:  0.15327560306839091
iteration : 12651
train acc:  0.8515625
train loss:  0.3764108419418335
train gradient:  0.13336573919965225
iteration : 12652
train acc:  0.859375
train loss:  0.3740546703338623
train gradient:  0.2504595307461097
iteration : 12653
train acc:  0.8984375
train loss:  0.2951822280883789
train gradient:  0.11187145303893
iteration : 12654
train acc:  0.828125
train loss:  0.4409274458885193
train gradient:  0.25964289353004205
iteration : 12655
train acc:  0.84375
train loss:  0.3271029591560364
train gradient:  0.11804696545554584
iteration : 12656
train acc:  0.890625
train loss:  0.25170087814331055
train gradient:  0.12091403332299347
iteration : 12657
train acc:  0.890625
train loss:  0.30273959040641785
train gradient:  0.1116157722316296
iteration : 12658
train acc:  0.828125
train loss:  0.335330992937088
train gradient:  0.14599179983600902
iteration : 12659
train acc:  0.8671875
train loss:  0.2477702796459198
train gradient:  0.09523476495322034
iteration : 12660
train acc:  0.859375
train loss:  0.32578301429748535
train gradient:  0.11823503464607123
iteration : 12661
train acc:  0.8515625
train loss:  0.3591820299625397
train gradient:  0.11252136083323185
iteration : 12662
train acc:  0.921875
train loss:  0.26604554057121277
train gradient:  0.10123239899013911
iteration : 12663
train acc:  0.8828125
train loss:  0.27243924140930176
train gradient:  0.1321992400773106
iteration : 12664
train acc:  0.8359375
train loss:  0.3190391957759857
train gradient:  0.15358306099041513
iteration : 12665
train acc:  0.8828125
train loss:  0.25841623544692993
train gradient:  0.0769828236249135
iteration : 12666
train acc:  0.890625
train loss:  0.28084689378738403
train gradient:  0.12350963209239019
iteration : 12667
train acc:  0.8828125
train loss:  0.3381970226764679
train gradient:  0.15857361459036476
iteration : 12668
train acc:  0.890625
train loss:  0.2678449749946594
train gradient:  0.07581849015893062
iteration : 12669
train acc:  0.8359375
train loss:  0.39914238452911377
train gradient:  0.22102111339759373
iteration : 12670
train acc:  0.8125
train loss:  0.3863864839076996
train gradient:  0.13761479807527072
iteration : 12671
train acc:  0.8671875
train loss:  0.266323983669281
train gradient:  0.09844179762153343
iteration : 12672
train acc:  0.875
train loss:  0.2923225164413452
train gradient:  0.14270829334239393
iteration : 12673
train acc:  0.828125
train loss:  0.3506966829299927
train gradient:  0.22961932435487803
iteration : 12674
train acc:  0.875
train loss:  0.29770949482917786
train gradient:  0.10305362504332298
iteration : 12675
train acc:  0.8671875
train loss:  0.3206334114074707
train gradient:  0.14738411336995977
iteration : 12676
train acc:  0.8125
train loss:  0.338398277759552
train gradient:  0.16801262880793125
iteration : 12677
train acc:  0.875
train loss:  0.2946365475654602
train gradient:  0.18780403511431454
iteration : 12678
train acc:  0.921875
train loss:  0.25582069158554077
train gradient:  0.12290052268574359
iteration : 12679
train acc:  0.828125
train loss:  0.38795730471611023
train gradient:  0.1723136447763322
iteration : 12680
train acc:  0.8203125
train loss:  0.42746680974960327
train gradient:  0.156002769386273
iteration : 12681
train acc:  0.796875
train loss:  0.4147123694419861
train gradient:  0.1699500272804356
iteration : 12682
train acc:  0.8203125
train loss:  0.34152722358703613
train gradient:  0.16322870657475771
iteration : 12683
train acc:  0.890625
train loss:  0.2764620780944824
train gradient:  0.13102134425741452
iteration : 12684
train acc:  0.8359375
train loss:  0.4108174443244934
train gradient:  0.22502807506065875
iteration : 12685
train acc:  0.8515625
train loss:  0.3759543299674988
train gradient:  0.19807789556786018
iteration : 12686
train acc:  0.859375
train loss:  0.32843852043151855
train gradient:  0.23543875360243502
iteration : 12687
train acc:  0.875
train loss:  0.27595189213752747
train gradient:  0.10087374833667813
iteration : 12688
train acc:  0.8984375
train loss:  0.24884288012981415
train gradient:  0.1061389078030942
iteration : 12689
train acc:  0.828125
train loss:  0.35017552971839905
train gradient:  0.11774061571591336
iteration : 12690
train acc:  0.859375
train loss:  0.33042311668395996
train gradient:  0.17530937664128438
iteration : 12691
train acc:  0.875
train loss:  0.2898097634315491
train gradient:  0.08510037213245912
iteration : 12692
train acc:  0.90625
train loss:  0.28353649377822876
train gradient:  0.12503278742425472
iteration : 12693
train acc:  0.90625
train loss:  0.2426077425479889
train gradient:  0.11134001757994393
iteration : 12694
train acc:  0.875
train loss:  0.29001909494400024
train gradient:  0.19347524134017402
iteration : 12695
train acc:  0.859375
train loss:  0.3348909318447113
train gradient:  0.1723079857591871
iteration : 12696
train acc:  0.875
train loss:  0.2822805345058441
train gradient:  0.11101629286467045
iteration : 12697
train acc:  0.890625
train loss:  0.32733073830604553
train gradient:  0.12944439070447236
iteration : 12698
train acc:  0.8671875
train loss:  0.3619682192802429
train gradient:  0.23437192750760633
iteration : 12699
train acc:  0.8984375
train loss:  0.2600439190864563
train gradient:  0.07666755969066102
iteration : 12700
train acc:  0.9140625
train loss:  0.21641023457050323
train gradient:  0.08011589145840078
iteration : 12701
train acc:  0.8203125
train loss:  0.46206530928611755
train gradient:  0.27694493228029654
iteration : 12702
train acc:  0.8828125
train loss:  0.3052843511104584
train gradient:  0.10737751493214466
iteration : 12703
train acc:  0.9140625
train loss:  0.252119779586792
train gradient:  0.11103048749065987
iteration : 12704
train acc:  0.890625
train loss:  0.24705246090888977
train gradient:  0.17206089246700962
iteration : 12705
train acc:  0.875
train loss:  0.2965792715549469
train gradient:  0.10987563093438195
iteration : 12706
train acc:  0.8671875
train loss:  0.32599055767059326
train gradient:  0.15670002879290984
iteration : 12707
train acc:  0.84375
train loss:  0.30637386441230774
train gradient:  0.19410124468079903
iteration : 12708
train acc:  0.828125
train loss:  0.41234588623046875
train gradient:  0.228258084097333
iteration : 12709
train acc:  0.8984375
train loss:  0.33289334177970886
train gradient:  0.16363604887630157
iteration : 12710
train acc:  0.8671875
train loss:  0.32516592741012573
train gradient:  0.1317998533954962
iteration : 12711
train acc:  0.875
train loss:  0.3222047984600067
train gradient:  0.2240660258464397
iteration : 12712
train acc:  0.890625
train loss:  0.2771393656730652
train gradient:  0.10705767446042037
iteration : 12713
train acc:  0.875
train loss:  0.31415867805480957
train gradient:  0.18136726245381068
iteration : 12714
train acc:  0.890625
train loss:  0.2719710171222687
train gradient:  0.10023562675216495
iteration : 12715
train acc:  0.9140625
train loss:  0.19513748586177826
train gradient:  0.08618874722176074
iteration : 12716
train acc:  0.8671875
train loss:  0.3587152361869812
train gradient:  0.19798514503250556
iteration : 12717
train acc:  0.859375
train loss:  0.36693406105041504
train gradient:  0.21603933376778095
iteration : 12718
train acc:  0.8828125
train loss:  0.31061363220214844
train gradient:  0.3137323268043841
iteration : 12719
train acc:  0.8828125
train loss:  0.3436180353164673
train gradient:  0.15425281862399606
iteration : 12720
train acc:  0.8359375
train loss:  0.3332514464855194
train gradient:  0.1377237006818847
iteration : 12721
train acc:  0.828125
train loss:  0.35746270418167114
train gradient:  0.14863403274460382
iteration : 12722
train acc:  0.890625
train loss:  0.254792720079422
train gradient:  0.11494157522559931
iteration : 12723
train acc:  0.875
train loss:  0.26123046875
train gradient:  0.10122975898653291
iteration : 12724
train acc:  0.8984375
train loss:  0.28333598375320435
train gradient:  0.11023748832610755
iteration : 12725
train acc:  0.859375
train loss:  0.3476652503013611
train gradient:  0.15578707155513105
iteration : 12726
train acc:  0.84375
train loss:  0.34309864044189453
train gradient:  0.1414651519447171
iteration : 12727
train acc:  0.8046875
train loss:  0.3730899691581726
train gradient:  0.1646557831297988
iteration : 12728
train acc:  0.875
train loss:  0.324404239654541
train gradient:  0.13947507202524984
iteration : 12729
train acc:  0.875
train loss:  0.27165189385414124
train gradient:  0.10938155350726143
iteration : 12730
train acc:  0.8203125
train loss:  0.35291194915771484
train gradient:  0.15483127605153738
iteration : 12731
train acc:  0.8359375
train loss:  0.34059661626815796
train gradient:  0.2262979368755918
iteration : 12732
train acc:  0.8125
train loss:  0.39186084270477295
train gradient:  0.1738520036882337
iteration : 12733
train acc:  0.828125
train loss:  0.3483847379684448
train gradient:  0.15959642941768876
iteration : 12734
train acc:  0.921875
train loss:  0.2741400897502899
train gradient:  0.11582863194779654
iteration : 12735
train acc:  0.8671875
train loss:  0.2768665850162506
train gradient:  0.09035641468504861
iteration : 12736
train acc:  0.90625
train loss:  0.26547765731811523
train gradient:  0.11628623394144291
iteration : 12737
train acc:  0.84375
train loss:  0.37765300273895264
train gradient:  0.1713110306264941
iteration : 12738
train acc:  0.9140625
train loss:  0.25530827045440674
train gradient:  0.09250988429228893
iteration : 12739
train acc:  0.8828125
train loss:  0.28122568130493164
train gradient:  0.12455441399596272
iteration : 12740
train acc:  0.8828125
train loss:  0.23991386592388153
train gradient:  0.10599738994490648
iteration : 12741
train acc:  0.8671875
train loss:  0.29152029752731323
train gradient:  0.14680441971506297
iteration : 12742
train acc:  0.84375
train loss:  0.34186720848083496
train gradient:  0.209142581534639
iteration : 12743
train acc:  0.8671875
train loss:  0.31148561835289
train gradient:  0.16995236571423517
iteration : 12744
train acc:  0.890625
train loss:  0.2507953345775604
train gradient:  0.16894240185851958
iteration : 12745
train acc:  0.8984375
train loss:  0.2797795534133911
train gradient:  0.11208979185124307
iteration : 12746
train acc:  0.8671875
train loss:  0.2821120023727417
train gradient:  0.1208110390073086
iteration : 12747
train acc:  0.8203125
train loss:  0.402633398771286
train gradient:  0.22653464504922577
iteration : 12748
train acc:  0.78125
train loss:  0.42984044551849365
train gradient:  0.2670295929199844
iteration : 12749
train acc:  0.875
train loss:  0.3094598352909088
train gradient:  0.12834351003879726
iteration : 12750
train acc:  0.828125
train loss:  0.36076462268829346
train gradient:  0.17727225670158514
iteration : 12751
train acc:  0.890625
train loss:  0.26218998432159424
train gradient:  0.09668683262713512
iteration : 12752
train acc:  0.8828125
train loss:  0.30777299404144287
train gradient:  0.12751315542122466
iteration : 12753
train acc:  0.8125
train loss:  0.4369288980960846
train gradient:  0.2674067022265264
iteration : 12754
train acc:  0.875
train loss:  0.30222246050834656
train gradient:  0.11918167122246294
iteration : 12755
train acc:  0.8828125
train loss:  0.25567856431007385
train gradient:  0.1094900238430949
iteration : 12756
train acc:  0.8828125
train loss:  0.33897554874420166
train gradient:  0.13683254194217406
iteration : 12757
train acc:  0.859375
train loss:  0.3186146318912506
train gradient:  0.21211151541800743
iteration : 12758
train acc:  0.90625
train loss:  0.28696781396865845
train gradient:  0.1434253021410346
iteration : 12759
train acc:  0.84375
train loss:  0.3212534785270691
train gradient:  0.1265061559346356
iteration : 12760
train acc:  0.8125
train loss:  0.34400808811187744
train gradient:  0.14257473071766558
iteration : 12761
train acc:  0.8984375
train loss:  0.28558096289634705
train gradient:  0.15702034020957495
iteration : 12762
train acc:  0.859375
train loss:  0.3073819577693939
train gradient:  0.10651472571226206
iteration : 12763
train acc:  0.8359375
train loss:  0.3174123466014862
train gradient:  0.16971018336931115
iteration : 12764
train acc:  0.8515625
train loss:  0.2710380256175995
train gradient:  0.09494496203184262
iteration : 12765
train acc:  0.8515625
train loss:  0.3396029472351074
train gradient:  0.1489748776787081
iteration : 12766
train acc:  0.8984375
train loss:  0.2744385004043579
train gradient:  0.12994807388726967
iteration : 12767
train acc:  0.8671875
train loss:  0.2786499261856079
train gradient:  0.1099026535611827
iteration : 12768
train acc:  0.8828125
train loss:  0.25120070576667786
train gradient:  0.07612988877991028
iteration : 12769
train acc:  0.8515625
train loss:  0.34330394864082336
train gradient:  0.1378464976655315
iteration : 12770
train acc:  0.8125
train loss:  0.40546661615371704
train gradient:  0.17387594019975944
iteration : 12771
train acc:  0.8984375
train loss:  0.27217379212379456
train gradient:  0.10030024294844
iteration : 12772
train acc:  0.8515625
train loss:  0.32482001185417175
train gradient:  0.11268981916407743
iteration : 12773
train acc:  0.890625
train loss:  0.24853554368019104
train gradient:  0.0888094983091723
iteration : 12774
train acc:  0.8671875
train loss:  0.31471681594848633
train gradient:  0.18811541388655462
iteration : 12775
train acc:  0.9140625
train loss:  0.2430715560913086
train gradient:  0.09165505508315037
iteration : 12776
train acc:  0.8984375
train loss:  0.2980688810348511
train gradient:  0.11601089442642706
iteration : 12777
train acc:  0.8984375
train loss:  0.25275886058807373
train gradient:  0.11844672815090351
iteration : 12778
train acc:  0.9140625
train loss:  0.23354989290237427
train gradient:  0.15926642559722576
iteration : 12779
train acc:  0.8203125
train loss:  0.4382927417755127
train gradient:  0.2884706608073307
iteration : 12780
train acc:  0.84375
train loss:  0.38334792852401733
train gradient:  0.21165748823707253
iteration : 12781
train acc:  0.8828125
train loss:  0.25215160846710205
train gradient:  0.08484974738049791
iteration : 12782
train acc:  0.9140625
train loss:  0.23757247626781464
train gradient:  0.09491542033479271
iteration : 12783
train acc:  0.875
train loss:  0.2881286144256592
train gradient:  0.12586794403233004
iteration : 12784
train acc:  0.8359375
train loss:  0.38358524441719055
train gradient:  0.16365243456379294
iteration : 12785
train acc:  0.8828125
train loss:  0.3341931998729706
train gradient:  0.1608597382108406
iteration : 12786
train acc:  0.8359375
train loss:  0.3517344892024994
train gradient:  0.18408942324976568
iteration : 12787
train acc:  0.84375
train loss:  0.39010104537010193
train gradient:  0.1915101705774095
iteration : 12788
train acc:  0.859375
train loss:  0.27112242579460144
train gradient:  0.10172542058526585
iteration : 12789
train acc:  0.8828125
train loss:  0.31429827213287354
train gradient:  0.12740555695134914
iteration : 12790
train acc:  0.8671875
train loss:  0.3353467881679535
train gradient:  0.2146001533140453
iteration : 12791
train acc:  0.8203125
train loss:  0.43841850757598877
train gradient:  0.26049867744958555
iteration : 12792
train acc:  0.8828125
train loss:  0.3351183533668518
train gradient:  0.15851350300628514
iteration : 12793
train acc:  0.8515625
train loss:  0.3092823028564453
train gradient:  0.1284505245617127
iteration : 12794
train acc:  0.828125
train loss:  0.40886205434799194
train gradient:  0.19142694243944985
iteration : 12795
train acc:  0.875
train loss:  0.2876153290271759
train gradient:  0.10746744037095476
iteration : 12796
train acc:  0.8125
train loss:  0.4544946551322937
train gradient:  0.36229542440159407
iteration : 12797
train acc:  0.8515625
train loss:  0.3063310980796814
train gradient:  0.16624398538396856
iteration : 12798
train acc:  0.8203125
train loss:  0.38645869493484497
train gradient:  0.18878360597431404
iteration : 12799
train acc:  0.8671875
train loss:  0.28645753860473633
train gradient:  0.12059954926153246
iteration : 12800
train acc:  0.8203125
train loss:  0.35185471177101135
train gradient:  0.2866370349919293
iteration : 12801
train acc:  0.828125
train loss:  0.39365410804748535
train gradient:  0.19254414745118692
iteration : 12802
train acc:  0.8515625
train loss:  0.3301451802253723
train gradient:  0.10928711589234044
iteration : 12803
train acc:  0.828125
train loss:  0.35787010192871094
train gradient:  0.15624827692570697
iteration : 12804
train acc:  0.828125
train loss:  0.4021734595298767
train gradient:  0.25402364423810186
iteration : 12805
train acc:  0.84375
train loss:  0.3465954065322876
train gradient:  0.14585537671565196
iteration : 12806
train acc:  0.8125
train loss:  0.4374551773071289
train gradient:  0.2323700086628012
iteration : 12807
train acc:  0.90625
train loss:  0.2735903561115265
train gradient:  0.11767003649016092
iteration : 12808
train acc:  0.90625
train loss:  0.22350162267684937
train gradient:  0.08117750118443541
iteration : 12809
train acc:  0.875
train loss:  0.27922046184539795
train gradient:  0.11788623982690084
iteration : 12810
train acc:  0.875
train loss:  0.33254602551460266
train gradient:  0.15830693878814572
iteration : 12811
train acc:  0.84375
train loss:  0.3338964581489563
train gradient:  0.13493530555382272
iteration : 12812
train acc:  0.8359375
train loss:  0.3462802469730377
train gradient:  0.20214350418399188
iteration : 12813
train acc:  0.890625
train loss:  0.2834724485874176
train gradient:  0.12092078909443003
iteration : 12814
train acc:  0.8828125
train loss:  0.2869758605957031
train gradient:  0.11346270629553001
iteration : 12815
train acc:  0.90625
train loss:  0.25344717502593994
train gradient:  0.11611056958402169
iteration : 12816
train acc:  0.8828125
train loss:  0.307940274477005
train gradient:  0.1646420367494696
iteration : 12817
train acc:  0.875
train loss:  0.240626260638237
train gradient:  0.07028603832260184
iteration : 12818
train acc:  0.875
train loss:  0.2864118814468384
train gradient:  0.0796770064534089
iteration : 12819
train acc:  0.8671875
train loss:  0.30390626192092896
train gradient:  0.13327216092763944
iteration : 12820
train acc:  0.828125
train loss:  0.3643369972705841
train gradient:  0.15103238664237464
iteration : 12821
train acc:  0.8984375
train loss:  0.2638241946697235
train gradient:  0.09008702188474495
iteration : 12822
train acc:  0.921875
train loss:  0.2269001007080078
train gradient:  0.08602721931258493
iteration : 12823
train acc:  0.84375
train loss:  0.32655009627342224
train gradient:  0.11022520868424439
iteration : 12824
train acc:  0.9375
train loss:  0.2262495458126068
train gradient:  0.07354291759213501
iteration : 12825
train acc:  0.8203125
train loss:  0.31830695271492004
train gradient:  0.1083268951262439
iteration : 12826
train acc:  0.84375
train loss:  0.3152726888656616
train gradient:  0.17150317558731043
iteration : 12827
train acc:  0.84375
train loss:  0.33469516038894653
train gradient:  0.14925103678259366
iteration : 12828
train acc:  0.8671875
train loss:  0.31348615884780884
train gradient:  0.16473850130451517
iteration : 12829
train acc:  0.8984375
train loss:  0.26310521364212036
train gradient:  0.09655175035719253
iteration : 12830
train acc:  0.875
train loss:  0.31962746381759644
train gradient:  0.2876496177413927
iteration : 12831
train acc:  0.890625
train loss:  0.2594805061817169
train gradient:  0.07199877773864427
iteration : 12832
train acc:  0.84375
train loss:  0.31077349185943604
train gradient:  0.1353491753343142
iteration : 12833
train acc:  0.8359375
train loss:  0.41624605655670166
train gradient:  0.20707916590054182
iteration : 12834
train acc:  0.875
train loss:  0.28081214427948
train gradient:  0.10118514635519711
iteration : 12835
train acc:  0.8828125
train loss:  0.3478319048881531
train gradient:  0.1473062116751408
iteration : 12836
train acc:  0.8203125
train loss:  0.3881339430809021
train gradient:  0.1950837351071828
iteration : 12837
train acc:  0.828125
train loss:  0.3555939793586731
train gradient:  0.24150787170936572
iteration : 12838
train acc:  0.890625
train loss:  0.2743125557899475
train gradient:  0.12024466916124221
iteration : 12839
train acc:  0.8359375
train loss:  0.34163177013397217
train gradient:  0.1399626154860094
iteration : 12840
train acc:  0.8203125
train loss:  0.3588106036186218
train gradient:  0.1731901042245087
iteration : 12841
train acc:  0.890625
train loss:  0.3515491783618927
train gradient:  0.12916750345628616
iteration : 12842
train acc:  0.875
train loss:  0.3097230792045593
train gradient:  0.14090188782279905
iteration : 12843
train acc:  0.8515625
train loss:  0.40152937173843384
train gradient:  0.1719239420565885
iteration : 12844
train acc:  0.84375
train loss:  0.3389497399330139
train gradient:  0.21352129157018687
iteration : 12845
train acc:  0.8203125
train loss:  0.39947080612182617
train gradient:  0.19028684614905272
iteration : 12846
train acc:  0.8359375
train loss:  0.3674667179584503
train gradient:  0.13553725220328305
iteration : 12847
train acc:  0.859375
train loss:  0.32505691051483154
train gradient:  0.1469215507339527
iteration : 12848
train acc:  0.875
train loss:  0.3200867772102356
train gradient:  0.1527076266116943
iteration : 12849
train acc:  0.875
train loss:  0.2906835079193115
train gradient:  0.08290957327712135
iteration : 12850
train acc:  0.90625
train loss:  0.26096564531326294
train gradient:  0.09158229552423111
iteration : 12851
train acc:  0.921875
train loss:  0.22027753293514252
train gradient:  0.08650331044713155
iteration : 12852
train acc:  0.8671875
train loss:  0.3268885612487793
train gradient:  0.13485539263245827
iteration : 12853
train acc:  0.8671875
train loss:  0.30655133724212646
train gradient:  0.16710614356990405
iteration : 12854
train acc:  0.90625
train loss:  0.2802448868751526
train gradient:  0.09708238769920796
iteration : 12855
train acc:  0.9140625
train loss:  0.22238463163375854
train gradient:  0.12232457247854417
iteration : 12856
train acc:  0.8046875
train loss:  0.419882595539093
train gradient:  0.27519996368150834
iteration : 12857
train acc:  0.890625
train loss:  0.23937396705150604
train gradient:  0.126265908394433
iteration : 12858
train acc:  0.8359375
train loss:  0.3458673357963562
train gradient:  0.1782105990175364
iteration : 12859
train acc:  0.859375
train loss:  0.36095282435417175
train gradient:  0.13654639441022173
iteration : 12860
train acc:  0.8984375
train loss:  0.23548990488052368
train gradient:  0.09085402235758143
iteration : 12861
train acc:  0.8828125
train loss:  0.28297895193099976
train gradient:  0.0794859071936991
iteration : 12862
train acc:  0.8203125
train loss:  0.42773759365081787
train gradient:  0.2887382382833486
iteration : 12863
train acc:  0.875
train loss:  0.3491870164871216
train gradient:  0.16944946540082723
iteration : 12864
train acc:  0.875
train loss:  0.2811054587364197
train gradient:  0.09174117872901792
iteration : 12865
train acc:  0.890625
train loss:  0.315401554107666
train gradient:  0.14243706929828082
iteration : 12866
train acc:  0.8359375
train loss:  0.33980420231819153
train gradient:  0.16307840220615177
iteration : 12867
train acc:  0.90625
train loss:  0.2389848530292511
train gradient:  0.09239185686418787
iteration : 12868
train acc:  0.8828125
train loss:  0.27137309312820435
train gradient:  0.13529756417103847
iteration : 12869
train acc:  0.8203125
train loss:  0.4094640910625458
train gradient:  0.28659691943896004
iteration : 12870
train acc:  0.8359375
train loss:  0.3625972270965576
train gradient:  0.1493958099565768
iteration : 12871
train acc:  0.8828125
train loss:  0.25425684452056885
train gradient:  0.11551869133586806
iteration : 12872
train acc:  0.8671875
train loss:  0.3706858158111572
train gradient:  0.18984650927247018
iteration : 12873
train acc:  0.8671875
train loss:  0.3135349750518799
train gradient:  0.2057401083906097
iteration : 12874
train acc:  0.90625
train loss:  0.2619544267654419
train gradient:  0.08240525357511586
iteration : 12875
train acc:  0.8359375
train loss:  0.2846042513847351
train gradient:  0.10872555335853132
iteration : 12876
train acc:  0.8359375
train loss:  0.38190019130706787
train gradient:  0.16132034476440404
iteration : 12877
train acc:  0.828125
train loss:  0.3700559735298157
train gradient:  0.15780209502913095
iteration : 12878
train acc:  0.78125
train loss:  0.4837896227836609
train gradient:  0.3159655998199769
iteration : 12879
train acc:  0.875
train loss:  0.2652393579483032
train gradient:  0.15496890123705637
iteration : 12880
train acc:  0.8359375
train loss:  0.33137062191963196
train gradient:  0.12974013960334752
iteration : 12881
train acc:  0.890625
train loss:  0.2536274194717407
train gradient:  0.13607046154720354
iteration : 12882
train acc:  0.875
train loss:  0.30745720863342285
train gradient:  0.1631261393545479
iteration : 12883
train acc:  0.8828125
train loss:  0.350236177444458
train gradient:  0.10480156609588195
iteration : 12884
train acc:  0.8359375
train loss:  0.41655898094177246
train gradient:  0.24309483098108506
iteration : 12885
train acc:  0.8515625
train loss:  0.3448183536529541
train gradient:  0.1653896575641541
iteration : 12886
train acc:  0.90625
train loss:  0.25485557317733765
train gradient:  0.07871745737091139
iteration : 12887
train acc:  0.8671875
train loss:  0.3043627142906189
train gradient:  0.11952112526131412
iteration : 12888
train acc:  0.90625
train loss:  0.27178722620010376
train gradient:  0.17299327507031714
iteration : 12889
train acc:  0.875
train loss:  0.3182418942451477
train gradient:  0.23973553288126262
iteration : 12890
train acc:  0.8984375
train loss:  0.2490071803331375
train gradient:  0.10027771010088903
iteration : 12891
train acc:  0.8984375
train loss:  0.25370949506759644
train gradient:  0.09854102639322357
iteration : 12892
train acc:  0.796875
train loss:  0.40256762504577637
train gradient:  0.2666241301810933
iteration : 12893
train acc:  0.90625
train loss:  0.21912804245948792
train gradient:  0.09801739866664752
iteration : 12894
train acc:  0.8125
train loss:  0.4490032494068146
train gradient:  0.26173662984096846
iteration : 12895
train acc:  0.828125
train loss:  0.3533186912536621
train gradient:  0.15797992539909939
iteration : 12896
train acc:  0.9296875
train loss:  0.23291604220867157
train gradient:  0.095046869632562
iteration : 12897
train acc:  0.875
train loss:  0.2820571959018707
train gradient:  0.10626785845682385
iteration : 12898
train acc:  0.9140625
train loss:  0.2502506375312805
train gradient:  0.113519046765016
iteration : 12899
train acc:  0.875
train loss:  0.3252890110015869
train gradient:  0.15382546005326947
iteration : 12900
train acc:  0.7890625
train loss:  0.4033665060997009
train gradient:  0.26952985939646007
iteration : 12901
train acc:  0.859375
train loss:  0.372904896736145
train gradient:  0.18606611118257327
iteration : 12902
train acc:  0.859375
train loss:  0.35886454582214355
train gradient:  0.21086257732003977
iteration : 12903
train acc:  0.796875
train loss:  0.42892006039619446
train gradient:  0.16926905140266607
iteration : 12904
train acc:  0.859375
train loss:  0.3075011074542999
train gradient:  0.15847938379990156
iteration : 12905
train acc:  0.828125
train loss:  0.3691663146018982
train gradient:  0.19622482723658746
iteration : 12906
train acc:  0.8515625
train loss:  0.3217872381210327
train gradient:  0.13773303976302742
iteration : 12907
train acc:  0.8984375
train loss:  0.25991857051849365
train gradient:  0.10546337987734389
iteration : 12908
train acc:  0.8046875
train loss:  0.3630087375640869
train gradient:  0.2037239366270063
iteration : 12909
train acc:  0.890625
train loss:  0.289229154586792
train gradient:  0.08631759951139006
iteration : 12910
train acc:  0.8515625
train loss:  0.27279341220855713
train gradient:  0.09402880794735734
iteration : 12911
train acc:  0.8828125
train loss:  0.31610262393951416
train gradient:  0.12017132245520513
iteration : 12912
train acc:  0.890625
train loss:  0.3213915228843689
train gradient:  0.17729862692804438
iteration : 12913
train acc:  0.8984375
train loss:  0.28283441066741943
train gradient:  0.0865701726431492
iteration : 12914
train acc:  0.8828125
train loss:  0.2762073278427124
train gradient:  0.11313491568477944
iteration : 12915
train acc:  0.875
train loss:  0.29129183292388916
train gradient:  0.1411662338309453
iteration : 12916
train acc:  0.84375
train loss:  0.28090915083885193
train gradient:  0.1295104320688109
iteration : 12917
train acc:  0.8359375
train loss:  0.3174137473106384
train gradient:  0.24022064272942795
iteration : 12918
train acc:  0.8828125
train loss:  0.3080665171146393
train gradient:  0.11272234595402994
iteration : 12919
train acc:  0.8046875
train loss:  0.4221051335334778
train gradient:  0.21063281891231972
iteration : 12920
train acc:  0.8203125
train loss:  0.32574814558029175
train gradient:  0.17679006835644395
iteration : 12921
train acc:  0.8984375
train loss:  0.3035462498664856
train gradient:  0.1480093395280774
iteration : 12922
train acc:  0.875
train loss:  0.32525360584259033
train gradient:  0.14499830311612127
iteration : 12923
train acc:  0.875
train loss:  0.2503751516342163
train gradient:  0.11438672373370631
iteration : 12924
train acc:  0.859375
train loss:  0.3243965804576874
train gradient:  0.1378885239784374
iteration : 12925
train acc:  0.8203125
train loss:  0.374714732170105
train gradient:  0.28454311490891043
iteration : 12926
train acc:  0.875
train loss:  0.27046501636505127
train gradient:  0.10490939877312945
iteration : 12927
train acc:  0.859375
train loss:  0.3309065103530884
train gradient:  0.2717873023921727
iteration : 12928
train acc:  0.8828125
train loss:  0.31960538029670715
train gradient:  0.1143349173752227
iteration : 12929
train acc:  0.90625
train loss:  0.24907147884368896
train gradient:  0.08386375306912136
iteration : 12930
train acc:  0.8671875
train loss:  0.2779253423213959
train gradient:  0.11766814076973175
iteration : 12931
train acc:  0.8046875
train loss:  0.4054903984069824
train gradient:  0.2057876693476552
iteration : 12932
train acc:  0.8203125
train loss:  0.3637901246547699
train gradient:  0.20209804032513443
iteration : 12933
train acc:  0.8828125
train loss:  0.24445898830890656
train gradient:  0.09430449339104685
iteration : 12934
train acc:  0.8203125
train loss:  0.4864709973335266
train gradient:  0.296411753482756
iteration : 12935
train acc:  0.9140625
train loss:  0.2639143764972687
train gradient:  0.08295030920491332
iteration : 12936
train acc:  0.9140625
train loss:  0.2861520051956177
train gradient:  0.11267280815694086
iteration : 12937
train acc:  0.8203125
train loss:  0.3627853989601135
train gradient:  0.19963127826650037
iteration : 12938
train acc:  0.8671875
train loss:  0.3522688150405884
train gradient:  0.2596709826018628
iteration : 12939
train acc:  0.8828125
train loss:  0.268256813287735
train gradient:  0.10924336281453112
iteration : 12940
train acc:  0.875
train loss:  0.25197115540504456
train gradient:  0.09498247201656122
iteration : 12941
train acc:  0.8671875
train loss:  0.2671089172363281
train gradient:  0.09788973725041425
iteration : 12942
train acc:  0.890625
train loss:  0.2792366147041321
train gradient:  0.11122193303989003
iteration : 12943
train acc:  0.8515625
train loss:  0.3653085231781006
train gradient:  0.1870419224564962
iteration : 12944
train acc:  0.859375
train loss:  0.35207292437553406
train gradient:  0.20697997786395145
iteration : 12945
train acc:  0.8515625
train loss:  0.28200045228004456
train gradient:  0.091809945474741
iteration : 12946
train acc:  0.859375
train loss:  0.2928130030632019
train gradient:  0.13739996101469187
iteration : 12947
train acc:  0.875
train loss:  0.24110829830169678
train gradient:  0.06807654469399371
iteration : 12948
train acc:  0.8515625
train loss:  0.3581615686416626
train gradient:  0.17520965293793583
iteration : 12949
train acc:  0.890625
train loss:  0.2760493755340576
train gradient:  0.1716863761880213
iteration : 12950
train acc:  0.7890625
train loss:  0.4559164047241211
train gradient:  0.2496999094831293
iteration : 12951
train acc:  0.859375
train loss:  0.28152135014533997
train gradient:  0.17994101828014558
iteration : 12952
train acc:  0.8828125
train loss:  0.3530612587928772
train gradient:  0.1540897056081721
iteration : 12953
train acc:  0.78125
train loss:  0.39455848932266235
train gradient:  0.14601721438837623
iteration : 12954
train acc:  0.8359375
train loss:  0.3622265160083771
train gradient:  0.17835730434840202
iteration : 12955
train acc:  0.890625
train loss:  0.26166921854019165
train gradient:  0.10846474204414935
iteration : 12956
train acc:  0.890625
train loss:  0.33530792593955994
train gradient:  0.15681242503999798
iteration : 12957
train acc:  0.8671875
train loss:  0.32130008935928345
train gradient:  0.1765763450402705
iteration : 12958
train acc:  0.9296875
train loss:  0.22626860439777374
train gradient:  0.14399914923784546
iteration : 12959
train acc:  0.890625
train loss:  0.2714850902557373
train gradient:  0.1099797584108524
iteration : 12960
train acc:  0.90625
train loss:  0.2286718785762787
train gradient:  0.0763157120378189
iteration : 12961
train acc:  0.875
train loss:  0.2965506613254547
train gradient:  0.11073425841477698
iteration : 12962
train acc:  0.84375
train loss:  0.4177910089492798
train gradient:  0.396653293297783
iteration : 12963
train acc:  0.859375
train loss:  0.3289819061756134
train gradient:  0.12483145660971494
iteration : 12964
train acc:  0.859375
train loss:  0.37852317094802856
train gradient:  0.18305795557011773
iteration : 12965
train acc:  0.8828125
train loss:  0.2854306995868683
train gradient:  0.20936400485219986
iteration : 12966
train acc:  0.859375
train loss:  0.290166974067688
train gradient:  0.10089902995838944
iteration : 12967
train acc:  0.84375
train loss:  0.3774968683719635
train gradient:  0.15621182748381168
iteration : 12968
train acc:  0.875
train loss:  0.27066075801849365
train gradient:  0.0989535543741876
iteration : 12969
train acc:  0.828125
train loss:  0.34446588158607483
train gradient:  0.1337009131203603
iteration : 12970
train acc:  0.8515625
train loss:  0.3250524699687958
train gradient:  0.1957281082958292
iteration : 12971
train acc:  0.828125
train loss:  0.3581664562225342
train gradient:  0.2594619222472362
iteration : 12972
train acc:  0.8671875
train loss:  0.2744293510913849
train gradient:  0.11382524317586208
iteration : 12973
train acc:  0.8671875
train loss:  0.34245234727859497
train gradient:  0.16853684733879704
iteration : 12974
train acc:  0.8828125
train loss:  0.2920456826686859
train gradient:  0.177328063415928
iteration : 12975
train acc:  0.828125
train loss:  0.4223770201206207
train gradient:  0.2612030767275041
iteration : 12976
train acc:  0.8515625
train loss:  0.2999744415283203
train gradient:  0.17619844312862504
iteration : 12977
train acc:  0.828125
train loss:  0.3854735493659973
train gradient:  0.2360792693208264
iteration : 12978
train acc:  0.8671875
train loss:  0.31357795000076294
train gradient:  0.14011864185228146
iteration : 12979
train acc:  0.875
train loss:  0.291435182094574
train gradient:  0.17646378873139787
iteration : 12980
train acc:  0.8828125
train loss:  0.306774765253067
train gradient:  0.2491519264570568
iteration : 12981
train acc:  0.890625
train loss:  0.29038727283477783
train gradient:  0.14099713333438132
iteration : 12982
train acc:  0.8359375
train loss:  0.34905382990837097
train gradient:  0.17692941214083902
iteration : 12983
train acc:  0.9296875
train loss:  0.23455321788787842
train gradient:  0.12552397664872755
iteration : 12984
train acc:  0.8828125
train loss:  0.30118969082832336
train gradient:  0.10384029426815436
iteration : 12985
train acc:  0.8671875
train loss:  0.2889665961265564
train gradient:  0.09284728269717565
iteration : 12986
train acc:  0.828125
train loss:  0.33999478816986084
train gradient:  0.13894275888610294
iteration : 12987
train acc:  0.859375
train loss:  0.32208478450775146
train gradient:  0.17552330792938595
iteration : 12988
train acc:  0.8203125
train loss:  0.3069956302642822
train gradient:  0.12021079296614626
iteration : 12989
train acc:  0.8515625
train loss:  0.2884395122528076
train gradient:  0.12253897972166711
iteration : 12990
train acc:  0.875
train loss:  0.3409736454486847
train gradient:  0.18510559589167536
iteration : 12991
train acc:  0.875
train loss:  0.3039214611053467
train gradient:  0.11201673417050804
iteration : 12992
train acc:  0.8828125
train loss:  0.2634804844856262
train gradient:  0.3118967399605157
iteration : 12993
train acc:  0.8671875
train loss:  0.3009280562400818
train gradient:  0.12635809679900542
iteration : 12994
train acc:  0.953125
train loss:  0.18050694465637207
train gradient:  0.06056929211908016
iteration : 12995
train acc:  0.84375
train loss:  0.3180789351463318
train gradient:  0.1689015472018483
iteration : 12996
train acc:  0.8046875
train loss:  0.4473946690559387
train gradient:  0.24947098654335156
iteration : 12997
train acc:  0.828125
train loss:  0.3735678493976593
train gradient:  0.20976108290575363
iteration : 12998
train acc:  0.8671875
train loss:  0.36101076006889343
train gradient:  0.20719282989814908
iteration : 12999
train acc:  0.859375
train loss:  0.30030134320259094
train gradient:  0.11048790530667722
iteration : 13000
train acc:  0.8359375
train loss:  0.33475780487060547
train gradient:  0.12143133200456714
iteration : 13001
train acc:  0.875
train loss:  0.31392034888267517
train gradient:  0.1164399492762173
iteration : 13002
train acc:  0.8671875
train loss:  0.25716614723205566
train gradient:  0.09996961857507161
iteration : 13003
train acc:  0.890625
train loss:  0.31606924533843994
train gradient:  0.10306492117089017
iteration : 13004
train acc:  0.921875
train loss:  0.1977621614933014
train gradient:  0.08314607971309225
iteration : 13005
train acc:  0.90625
train loss:  0.2531338334083557
train gradient:  0.12530647829502545
iteration : 13006
train acc:  0.8828125
train loss:  0.27993500232696533
train gradient:  0.12454419905836923
iteration : 13007
train acc:  0.8828125
train loss:  0.2653997242450714
train gradient:  0.1035668973241241
iteration : 13008
train acc:  0.875
train loss:  0.25705358386039734
train gradient:  0.1350707126944416
iteration : 13009
train acc:  0.78125
train loss:  0.4356263279914856
train gradient:  0.22680601056763114
iteration : 13010
train acc:  0.8984375
train loss:  0.2626074552536011
train gradient:  0.07375644389426626
iteration : 13011
train acc:  0.875
train loss:  0.3230212330818176
train gradient:  0.13443097916214064
iteration : 13012
train acc:  0.875
train loss:  0.33245787024497986
train gradient:  0.10544245283980559
iteration : 13013
train acc:  0.859375
train loss:  0.36501139402389526
train gradient:  0.11747848977662696
iteration : 13014
train acc:  0.875
train loss:  0.2948618531227112
train gradient:  0.15612360437172063
iteration : 13015
train acc:  0.90625
train loss:  0.281430184841156
train gradient:  0.15116955339793098
iteration : 13016
train acc:  0.84375
train loss:  0.3347841501235962
train gradient:  0.14693442956005437
iteration : 13017
train acc:  0.90625
train loss:  0.26988834142684937
train gradient:  0.10057037550764135
iteration : 13018
train acc:  0.890625
train loss:  0.2605099678039551
train gradient:  0.1052862618361591
iteration : 13019
train acc:  0.8984375
train loss:  0.27006295323371887
train gradient:  0.12193958641152962
iteration : 13020
train acc:  0.8359375
train loss:  0.39818212389945984
train gradient:  0.190631743665666
iteration : 13021
train acc:  0.8515625
train loss:  0.3920944929122925
train gradient:  0.18641220232538608
iteration : 13022
train acc:  0.84375
train loss:  0.37367743253707886
train gradient:  0.2282316992931746
iteration : 13023
train acc:  0.8671875
train loss:  0.29738837480545044
train gradient:  0.10689982626085485
iteration : 13024
train acc:  0.84375
train loss:  0.3286902606487274
train gradient:  0.1596703632379396
iteration : 13025
train acc:  0.8359375
train loss:  0.3674665689468384
train gradient:  0.18286276624714087
iteration : 13026
train acc:  0.9296875
train loss:  0.25002461671829224
train gradient:  0.11549955525656405
iteration : 13027
train acc:  0.8046875
train loss:  0.4547872543334961
train gradient:  0.280129545298091
iteration : 13028
train acc:  0.8046875
train loss:  0.37702471017837524
train gradient:  0.14148446622122274
iteration : 13029
train acc:  0.84375
train loss:  0.3533051311969757
train gradient:  0.17237409501724338
iteration : 13030
train acc:  0.84375
train loss:  0.3752942681312561
train gradient:  0.23620923107290642
iteration : 13031
train acc:  0.875
train loss:  0.29256248474121094
train gradient:  0.18256686549398635
iteration : 13032
train acc:  0.875
train loss:  0.29426300525665283
train gradient:  0.15747459044188128
iteration : 13033
train acc:  0.8671875
train loss:  0.3087824881076813
train gradient:  0.11641302031108619
iteration : 13034
train acc:  0.8359375
train loss:  0.28589141368865967
train gradient:  0.10221619738762663
iteration : 13035
train acc:  0.859375
train loss:  0.3361538052558899
train gradient:  0.19520227881745983
iteration : 13036
train acc:  0.875
train loss:  0.32504409551620483
train gradient:  0.13244283674915525
iteration : 13037
train acc:  0.8671875
train loss:  0.29014134407043457
train gradient:  0.1267765236935119
iteration : 13038
train acc:  0.84375
train loss:  0.3435284197330475
train gradient:  0.11222963970304595
iteration : 13039
train acc:  0.9296875
train loss:  0.2406071126461029
train gradient:  0.09852508203729293
iteration : 13040
train acc:  0.90625
train loss:  0.29118409752845764
train gradient:  0.2000902075341095
iteration : 13041
train acc:  0.8671875
train loss:  0.30136579275131226
train gradient:  0.17231875454937629
iteration : 13042
train acc:  0.8359375
train loss:  0.32106661796569824
train gradient:  0.1106903366308141
iteration : 13043
train acc:  0.8515625
train loss:  0.32566919922828674
train gradient:  0.18204348203665757
iteration : 13044
train acc:  0.84375
train loss:  0.3887159824371338
train gradient:  0.21232347434177545
iteration : 13045
train acc:  0.8828125
train loss:  0.269238144159317
train gradient:  0.08575685464915385
iteration : 13046
train acc:  0.8828125
train loss:  0.3017956614494324
train gradient:  0.13082546924312274
iteration : 13047
train acc:  0.8046875
train loss:  0.4030167758464813
train gradient:  0.16660889992121208
iteration : 13048
train acc:  0.875
train loss:  0.29367688298225403
train gradient:  0.08395366463724317
iteration : 13049
train acc:  0.8671875
train loss:  0.3105042278766632
train gradient:  0.14500368937862895
iteration : 13050
train acc:  0.859375
train loss:  0.3388292193412781
train gradient:  0.13663115224824762
iteration : 13051
train acc:  0.859375
train loss:  0.30933845043182373
train gradient:  0.10935024671618526
iteration : 13052
train acc:  0.859375
train loss:  0.3214207887649536
train gradient:  0.10453095128618815
iteration : 13053
train acc:  0.890625
train loss:  0.28838247060775757
train gradient:  0.1429702897454474
iteration : 13054
train acc:  0.90625
train loss:  0.26993733644485474
train gradient:  0.0809576961101456
iteration : 13055
train acc:  0.828125
train loss:  0.3140644431114197
train gradient:  0.14741141491118917
iteration : 13056
train acc:  0.890625
train loss:  0.24082109332084656
train gradient:  0.07857732458645056
iteration : 13057
train acc:  0.8828125
train loss:  0.3232131004333496
train gradient:  0.12005475253770258
iteration : 13058
train acc:  0.8125
train loss:  0.4051707983016968
train gradient:  0.1698664170459715
iteration : 13059
train acc:  0.8515625
train loss:  0.32220226526260376
train gradient:  0.13643043343726957
iteration : 13060
train acc:  0.90625
train loss:  0.3661808967590332
train gradient:  0.15595656731081953
iteration : 13061
train acc:  0.8359375
train loss:  0.34322232007980347
train gradient:  0.11758515713814006
iteration : 13062
train acc:  0.8125
train loss:  0.42927467823028564
train gradient:  0.22438257897577937
iteration : 13063
train acc:  0.8046875
train loss:  0.4897489845752716
train gradient:  0.27366633534519913
iteration : 13064
train acc:  0.8359375
train loss:  0.3388819694519043
train gradient:  0.1314482983152408
iteration : 13065
train acc:  0.8203125
train loss:  0.4118390679359436
train gradient:  0.17470477770018988
iteration : 13066
train acc:  0.8125
train loss:  0.40926164388656616
train gradient:  0.2497889875985578
iteration : 13067
train acc:  0.828125
train loss:  0.34909525513648987
train gradient:  0.13340258406715283
iteration : 13068
train acc:  0.875
train loss:  0.27979207038879395
train gradient:  0.09526099861934458
iteration : 13069
train acc:  0.9140625
train loss:  0.24876141548156738
train gradient:  0.09125043008538326
iteration : 13070
train acc:  0.8359375
train loss:  0.3527200520038605
train gradient:  0.14824701820799874
iteration : 13071
train acc:  0.921875
train loss:  0.2707405686378479
train gradient:  0.10627043549967585
iteration : 13072
train acc:  0.875
train loss:  0.3056160509586334
train gradient:  0.09949368570485652
iteration : 13073
train acc:  0.890625
train loss:  0.2442091703414917
train gradient:  0.07252567486496189
iteration : 13074
train acc:  0.8515625
train loss:  0.3419743776321411
train gradient:  0.15908071288028797
iteration : 13075
train acc:  0.8125
train loss:  0.3216947019100189
train gradient:  0.10359478042844787
iteration : 13076
train acc:  0.828125
train loss:  0.4229016900062561
train gradient:  0.23236704680609788
iteration : 13077
train acc:  0.90625
train loss:  0.23408879339694977
train gradient:  0.09092493539335618
iteration : 13078
train acc:  0.828125
train loss:  0.37886568903923035
train gradient:  0.15201993208080983
iteration : 13079
train acc:  0.875
train loss:  0.30449551343917847
train gradient:  0.11258665356566332
iteration : 13080
train acc:  0.8359375
train loss:  0.41746026277542114
train gradient:  0.16330394728161188
iteration : 13081
train acc:  0.8203125
train loss:  0.373521625995636
train gradient:  0.1583114458483624
iteration : 13082
train acc:  0.8515625
train loss:  0.3155897259712219
train gradient:  0.09454021652575059
iteration : 13083
train acc:  0.8828125
train loss:  0.2980663776397705
train gradient:  0.12461886795803134
iteration : 13084
train acc:  0.828125
train loss:  0.406924307346344
train gradient:  0.13717517289206702
iteration : 13085
train acc:  0.828125
train loss:  0.373268187046051
train gradient:  0.16497840060272992
iteration : 13086
train acc:  0.8515625
train loss:  0.3212234377861023
train gradient:  0.15819808503049404
iteration : 13087
train acc:  0.8984375
train loss:  0.27552473545074463
train gradient:  0.12886181817896547
iteration : 13088
train acc:  0.8359375
train loss:  0.3151877224445343
train gradient:  0.13619290567519898
iteration : 13089
train acc:  0.8828125
train loss:  0.27432554960250854
train gradient:  0.11027071863829063
iteration : 13090
train acc:  0.84375
train loss:  0.3630440831184387
train gradient:  0.19932735388819453
iteration : 13091
train acc:  0.84375
train loss:  0.3636220693588257
train gradient:  0.15163026631475213
iteration : 13092
train acc:  0.8828125
train loss:  0.3076231777667999
train gradient:  0.13119220058773268
iteration : 13093
train acc:  0.84375
train loss:  0.30579492449760437
train gradient:  0.1105498281756741
iteration : 13094
train acc:  0.8671875
train loss:  0.30110111832618713
train gradient:  0.11517864199215914
iteration : 13095
train acc:  0.8515625
train loss:  0.4298669695854187
train gradient:  0.21086778540194023
iteration : 13096
train acc:  0.859375
train loss:  0.2965013086795807
train gradient:  0.11526165138078692
iteration : 13097
train acc:  0.9140625
train loss:  0.27198606729507446
train gradient:  0.13788002241779487
iteration : 13098
train acc:  0.90625
train loss:  0.26965269446372986
train gradient:  0.11703238526719487
iteration : 13099
train acc:  0.8671875
train loss:  0.33274346590042114
train gradient:  0.11138873682425061
iteration : 13100
train acc:  0.921875
train loss:  0.2609865367412567
train gradient:  0.08553273223295614
iteration : 13101
train acc:  0.8984375
train loss:  0.2288895845413208
train gradient:  0.09943919713325004
iteration : 13102
train acc:  0.890625
train loss:  0.2512195110321045
train gradient:  0.08575881875009993
iteration : 13103
train acc:  0.859375
train loss:  0.3015422224998474
train gradient:  0.07728922910851832
iteration : 13104
train acc:  0.828125
train loss:  0.39158961176872253
train gradient:  0.21910665349240122
iteration : 13105
train acc:  0.859375
train loss:  0.3471556305885315
train gradient:  0.19535830289945744
iteration : 13106
train acc:  0.8828125
train loss:  0.2564207911491394
train gradient:  0.10663229952649927
iteration : 13107
train acc:  0.8125
train loss:  0.3219950199127197
train gradient:  0.12290356799988686
iteration : 13108
train acc:  0.8359375
train loss:  0.363893985748291
train gradient:  0.1504186020600786
iteration : 13109
train acc:  0.8984375
train loss:  0.2602517604827881
train gradient:  0.10268909367260058
iteration : 13110
train acc:  0.9296875
train loss:  0.23532786965370178
train gradient:  0.0976932221090115
iteration : 13111
train acc:  0.890625
train loss:  0.253060907125473
train gradient:  0.08170731762727207
iteration : 13112
train acc:  0.8125
train loss:  0.3944164514541626
train gradient:  0.21665405574240615
iteration : 13113
train acc:  0.875
train loss:  0.308979868888855
train gradient:  0.12645336606798824
iteration : 13114
train acc:  0.84375
train loss:  0.3317022919654846
train gradient:  0.1250134628677476
iteration : 13115
train acc:  0.8828125
train loss:  0.3389376699924469
train gradient:  0.24447399205477083
iteration : 13116
train acc:  0.875
train loss:  0.2845131754875183
train gradient:  0.1042737648354249
iteration : 13117
train acc:  0.859375
train loss:  0.32349246740341187
train gradient:  0.15027955390220596
iteration : 13118
train acc:  0.8671875
train loss:  0.33282774686813354
train gradient:  0.14204367184829933
iteration : 13119
train acc:  0.890625
train loss:  0.30183303356170654
train gradient:  0.12670394594072598
iteration : 13120
train acc:  0.8359375
train loss:  0.3642588257789612
train gradient:  0.22107498429304495
iteration : 13121
train acc:  0.8203125
train loss:  0.42103397846221924
train gradient:  0.17077410373944574
iteration : 13122
train acc:  0.8671875
train loss:  0.33368194103240967
train gradient:  0.1780270164112054
iteration : 13123
train acc:  0.859375
train loss:  0.28405147790908813
train gradient:  0.06449674638292359
iteration : 13124
train acc:  0.828125
train loss:  0.36836978793144226
train gradient:  0.1728207290598155
iteration : 13125
train acc:  0.828125
train loss:  0.3580371141433716
train gradient:  0.12559522575851412
iteration : 13126
train acc:  0.8359375
train loss:  0.3475165367126465
train gradient:  0.14768180991536983
iteration : 13127
train acc:  0.8515625
train loss:  0.35660499334335327
train gradient:  0.10415374019200488
iteration : 13128
train acc:  0.8203125
train loss:  0.4174194931983948
train gradient:  0.23235590580868787
iteration : 13129
train acc:  0.9296875
train loss:  0.23087391257286072
train gradient:  0.09032984778043895
iteration : 13130
train acc:  0.859375
train loss:  0.31542086601257324
train gradient:  0.12119624520976023
iteration : 13131
train acc:  0.875
train loss:  0.2776661217212677
train gradient:  0.11316956511752409
iteration : 13132
train acc:  0.90625
train loss:  0.2358647733926773
train gradient:  0.09381385846357354
iteration : 13133
train acc:  0.859375
train loss:  0.3622913956642151
train gradient:  0.1351244462340781
iteration : 13134
train acc:  0.875
train loss:  0.28838950395584106
train gradient:  0.18360485843873714
iteration : 13135
train acc:  0.875
train loss:  0.30463486909866333
train gradient:  0.0998912330032187
iteration : 13136
train acc:  0.8359375
train loss:  0.35269176959991455
train gradient:  0.15991402968331747
iteration : 13137
train acc:  0.9140625
train loss:  0.22679048776626587
train gradient:  0.09783400564325807
iteration : 13138
train acc:  0.8515625
train loss:  0.2902938723564148
train gradient:  0.12165969060581776
iteration : 13139
train acc:  0.8515625
train loss:  0.32438355684280396
train gradient:  0.10741041482356377
iteration : 13140
train acc:  0.921875
train loss:  0.2545303702354431
train gradient:  0.10233352096384758
iteration : 13141
train acc:  0.8671875
train loss:  0.28983795642852783
train gradient:  0.11984526573032636
iteration : 13142
train acc:  0.84375
train loss:  0.31951457262039185
train gradient:  0.18125954776640185
iteration : 13143
train acc:  0.828125
train loss:  0.3001171350479126
train gradient:  0.09848332414242919
iteration : 13144
train acc:  0.890625
train loss:  0.2760704755783081
train gradient:  0.10960538293332267
iteration : 13145
train acc:  0.875
train loss:  0.29665523767471313
train gradient:  0.13355769812701923
iteration : 13146
train acc:  0.84375
train loss:  0.3342054486274719
train gradient:  0.1390759805823843
iteration : 13147
train acc:  0.8359375
train loss:  0.3881351351737976
train gradient:  0.21493227302104068
iteration : 13148
train acc:  0.8671875
train loss:  0.2876555919647217
train gradient:  0.11391776860430221
iteration : 13149
train acc:  0.8671875
train loss:  0.3270847201347351
train gradient:  0.0868055287900872
iteration : 13150
train acc:  0.8671875
train loss:  0.3315708339214325
train gradient:  0.13264874534396875
iteration : 13151
train acc:  0.875
train loss:  0.26952454447746277
train gradient:  0.12298510409673254
iteration : 13152
train acc:  0.875
train loss:  0.27603909373283386
train gradient:  0.1272916397934078
iteration : 13153
train acc:  0.8671875
train loss:  0.3195405602455139
train gradient:  0.136101180373996
iteration : 13154
train acc:  0.8671875
train loss:  0.36153244972229004
train gradient:  0.11818963588512664
iteration : 13155
train acc:  0.8671875
train loss:  0.33175837993621826
train gradient:  0.15704242743499117
iteration : 13156
train acc:  0.8515625
train loss:  0.39839619398117065
train gradient:  0.14762678937285212
iteration : 13157
train acc:  0.890625
train loss:  0.26332294940948486
train gradient:  0.19900262052829865
iteration : 13158
train acc:  0.8125
train loss:  0.43000978231430054
train gradient:  0.28855544363369223
iteration : 13159
train acc:  0.890625
train loss:  0.31749019026756287
train gradient:  0.1272258613787343
iteration : 13160
train acc:  0.8671875
train loss:  0.4485071897506714
train gradient:  0.35830061846474653
iteration : 13161
train acc:  0.8515625
train loss:  0.27622830867767334
train gradient:  0.17963384027091284
iteration : 13162
train acc:  0.9140625
train loss:  0.25298887491226196
train gradient:  0.10902311813713404
iteration : 13163
train acc:  0.8671875
train loss:  0.35579776763916016
train gradient:  0.1376075115839233
iteration : 13164
train acc:  0.9140625
train loss:  0.2610396146774292
train gradient:  0.08824282908420118
iteration : 13165
train acc:  0.9140625
train loss:  0.24996712803840637
train gradient:  0.11047549831779939
iteration : 13166
train acc:  0.890625
train loss:  0.28443846106529236
train gradient:  0.10561243010777832
iteration : 13167
train acc:  0.828125
train loss:  0.3184063136577606
train gradient:  0.1826315625265169
iteration : 13168
train acc:  0.8515625
train loss:  0.35083016753196716
train gradient:  0.1454444410678579
iteration : 13169
train acc:  0.828125
train loss:  0.3282623887062073
train gradient:  0.13456107763785047
iteration : 13170
train acc:  0.859375
train loss:  0.3034401535987854
train gradient:  0.12480279209073343
iteration : 13171
train acc:  0.8671875
train loss:  0.33739280700683594
train gradient:  0.15690068824772954
iteration : 13172
train acc:  0.9140625
train loss:  0.2513763904571533
train gradient:  0.08261507445115006
iteration : 13173
train acc:  0.8515625
train loss:  0.3172195851802826
train gradient:  0.14084717289354803
iteration : 13174
train acc:  0.84375
train loss:  0.3686010539531708
train gradient:  0.20365688739367688
iteration : 13175
train acc:  0.875
train loss:  0.2705325782299042
train gradient:  0.09143595219941535
iteration : 13176
train acc:  0.859375
train loss:  0.3318784236907959
train gradient:  0.13208786678615206
iteration : 13177
train acc:  0.8515625
train loss:  0.3883627653121948
train gradient:  0.1551222953758733
iteration : 13178
train acc:  0.84375
train loss:  0.3622830808162689
train gradient:  0.13091599183427316
iteration : 13179
train acc:  0.921875
train loss:  0.2751627564430237
train gradient:  0.08677987532551883
iteration : 13180
train acc:  0.8984375
train loss:  0.24875293672084808
train gradient:  0.0926744239013888
iteration : 13181
train acc:  0.875
train loss:  0.2998029887676239
train gradient:  0.18235099149460382
iteration : 13182
train acc:  0.8046875
train loss:  0.44534212350845337
train gradient:  0.2485823964459657
iteration : 13183
train acc:  0.8515625
train loss:  0.3643624484539032
train gradient:  0.14592452554547827
iteration : 13184
train acc:  0.8671875
train loss:  0.3070008158683777
train gradient:  0.11407843042034961
iteration : 13185
train acc:  0.828125
train loss:  0.3283013701438904
train gradient:  0.12094745616556288
iteration : 13186
train acc:  0.84375
train loss:  0.3340036869049072
train gradient:  0.1609475004437246
iteration : 13187
train acc:  0.8515625
train loss:  0.30186232924461365
train gradient:  0.09631858461599004
iteration : 13188
train acc:  0.8671875
train loss:  0.3152095675468445
train gradient:  0.13312038018563008
iteration : 13189
train acc:  0.921875
train loss:  0.20127713680267334
train gradient:  0.06171112491077863
iteration : 13190
train acc:  0.84375
train loss:  0.30501294136047363
train gradient:  0.20388320424975914
iteration : 13191
train acc:  0.8515625
train loss:  0.31803905963897705
train gradient:  0.116812135769373
iteration : 13192
train acc:  0.8125
train loss:  0.3538675308227539
train gradient:  0.16687961606464619
iteration : 13193
train acc:  0.84375
train loss:  0.3335459232330322
train gradient:  0.17483855800188403
iteration : 13194
train acc:  0.8359375
train loss:  0.34251803159713745
train gradient:  0.11500803955997259
iteration : 13195
train acc:  0.9296875
train loss:  0.27119091153144836
train gradient:  0.08241845854225877
iteration : 13196
train acc:  0.875
train loss:  0.27681100368499756
train gradient:  0.12233181634644787
iteration : 13197
train acc:  0.84375
train loss:  0.40036022663116455
train gradient:  0.1643387840913838
iteration : 13198
train acc:  0.859375
train loss:  0.2722981870174408
train gradient:  0.08497074467061506
iteration : 13199
train acc:  0.8515625
train loss:  0.33500128984451294
train gradient:  0.1569683927843309
iteration : 13200
train acc:  0.9296875
train loss:  0.19659090042114258
train gradient:  0.06028955278956297
iteration : 13201
train acc:  0.84375
train loss:  0.2946648597717285
train gradient:  0.12835688082583585
iteration : 13202
train acc:  0.828125
train loss:  0.34417402744293213
train gradient:  0.18827059733144214
iteration : 13203
train acc:  0.8515625
train loss:  0.3503369987010956
train gradient:  0.18936480741254558
iteration : 13204
train acc:  0.8515625
train loss:  0.35507845878601074
train gradient:  0.1582877842546852
iteration : 13205
train acc:  0.8046875
train loss:  0.43556326627731323
train gradient:  0.22274801713497128
iteration : 13206
train acc:  0.8828125
train loss:  0.26997891068458557
train gradient:  0.11179536915634009
iteration : 13207
train acc:  0.859375
train loss:  0.36619579792022705
train gradient:  0.15529870658453193
iteration : 13208
train acc:  0.8203125
train loss:  0.3837849199771881
train gradient:  0.2420352747932067
iteration : 13209
train acc:  0.8671875
train loss:  0.29435527324676514
train gradient:  0.14590742102226983
iteration : 13210
train acc:  0.84375
train loss:  0.32800042629241943
train gradient:  0.12658785350801477
iteration : 13211
train acc:  0.8828125
train loss:  0.295560747385025
train gradient:  0.10941713648359978
iteration : 13212
train acc:  0.859375
train loss:  0.31475216150283813
train gradient:  0.09066578159707912
iteration : 13213
train acc:  0.8671875
train loss:  0.33848822116851807
train gradient:  0.12352720476039766
iteration : 13214
train acc:  0.890625
train loss:  0.30816954374313354
train gradient:  0.09487842071746437
iteration : 13215
train acc:  0.890625
train loss:  0.28133058547973633
train gradient:  0.13582775222416107
iteration : 13216
train acc:  0.875
train loss:  0.2920547425746918
train gradient:  0.10384810734723271
iteration : 13217
train acc:  0.8359375
train loss:  0.36530935764312744
train gradient:  0.19151970169236632
iteration : 13218
train acc:  0.890625
train loss:  0.3177313804626465
train gradient:  0.1294887185455046
iteration : 13219
train acc:  0.84375
train loss:  0.34919700026512146
train gradient:  0.13719976978473292
iteration : 13220
train acc:  0.8359375
train loss:  0.30701684951782227
train gradient:  0.11990777588263205
iteration : 13221
train acc:  0.828125
train loss:  0.3848128318786621
train gradient:  0.13370644089636408
iteration : 13222
train acc:  0.9140625
train loss:  0.24150416254997253
train gradient:  0.09202939304544543
iteration : 13223
train acc:  0.84375
train loss:  0.318173885345459
train gradient:  0.1106654440938066
iteration : 13224
train acc:  0.875
train loss:  0.33201760053634644
train gradient:  0.1081011624751997
iteration : 13225
train acc:  0.8125
train loss:  0.4203110337257385
train gradient:  0.19780872693849164
iteration : 13226
train acc:  0.7734375
train loss:  0.5462393164634705
train gradient:  0.40286300443177386
iteration : 13227
train acc:  0.8515625
train loss:  0.3727404475212097
train gradient:  0.244144697860947
iteration : 13228
train acc:  0.8359375
train loss:  0.35534194111824036
train gradient:  0.18311565627438006
iteration : 13229
train acc:  0.8828125
train loss:  0.29615622758865356
train gradient:  0.12236568379376628
iteration : 13230
train acc:  0.8203125
train loss:  0.33336055278778076
train gradient:  0.11655075747099419
iteration : 13231
train acc:  0.890625
train loss:  0.2821628153324127
train gradient:  0.09487416677241028
iteration : 13232
train acc:  0.859375
train loss:  0.3521236479282379
train gradient:  0.14779344432183394
iteration : 13233
train acc:  0.921875
train loss:  0.21875101327896118
train gradient:  0.10183017998675574
iteration : 13234
train acc:  0.8515625
train loss:  0.358722984790802
train gradient:  0.1566590936367616
iteration : 13235
train acc:  0.796875
train loss:  0.38801997900009155
train gradient:  0.19604192202782
iteration : 13236
train acc:  0.8515625
train loss:  0.310504287481308
train gradient:  0.1463356579292331
iteration : 13237
train acc:  0.890625
train loss:  0.2695586085319519
train gradient:  0.07515454184142724
iteration : 13238
train acc:  0.8671875
train loss:  0.28873974084854126
train gradient:  0.10487349009204074
iteration : 13239
train acc:  0.90625
train loss:  0.26014992594718933
train gradient:  0.10062261465518879
iteration : 13240
train acc:  0.8125
train loss:  0.3544132709503174
train gradient:  0.13143562705715167
iteration : 13241
train acc:  0.8203125
train loss:  0.3899818956851959
train gradient:  0.13123968136884107
iteration : 13242
train acc:  0.890625
train loss:  0.27214083075523376
train gradient:  0.11215706656920951
iteration : 13243
train acc:  0.9296875
train loss:  0.24109993875026703
train gradient:  0.0771975201720679
iteration : 13244
train acc:  0.84375
train loss:  0.2486356496810913
train gradient:  0.09036816003520819
iteration : 13245
train acc:  0.8515625
train loss:  0.4298013746738434
train gradient:  0.1523113498960939
iteration : 13246
train acc:  0.84375
train loss:  0.4509793519973755
train gradient:  0.25679033699777076
iteration : 13247
train acc:  0.859375
train loss:  0.3492504954338074
train gradient:  0.14481791317212722
iteration : 13248
train acc:  0.828125
train loss:  0.34685730934143066
train gradient:  0.11370125968561129
iteration : 13249
train acc:  0.8515625
train loss:  0.32373446226119995
train gradient:  0.09475923156600993
iteration : 13250
train acc:  0.890625
train loss:  0.26079893112182617
train gradient:  0.11186314754932651
iteration : 13251
train acc:  0.859375
train loss:  0.35908985137939453
train gradient:  0.1267964963338858
iteration : 13252
train acc:  0.8984375
train loss:  0.2932843863964081
train gradient:  0.16890896863119798
iteration : 13253
train acc:  0.8671875
train loss:  0.3412776589393616
train gradient:  0.1315829993614354
iteration : 13254
train acc:  0.890625
train loss:  0.27803874015808105
train gradient:  0.10115602696940411
iteration : 13255
train acc:  0.8515625
train loss:  0.3560170531272888
train gradient:  0.15598650694910124
iteration : 13256
train acc:  0.859375
train loss:  0.3349236845970154
train gradient:  0.14953753488780147
iteration : 13257
train acc:  0.796875
train loss:  0.44703203439712524
train gradient:  0.2109084604840959
iteration : 13258
train acc:  0.8203125
train loss:  0.34617167711257935
train gradient:  0.15092267361461575
iteration : 13259
train acc:  0.859375
train loss:  0.3448791801929474
train gradient:  0.15842393077532851
iteration : 13260
train acc:  0.8984375
train loss:  0.29220980405807495
train gradient:  0.09224577332723358
iteration : 13261
train acc:  0.90625
train loss:  0.260073721408844
train gradient:  0.09693806170017256
iteration : 13262
train acc:  0.84375
train loss:  0.32177305221557617
train gradient:  0.13453168706915153
iteration : 13263
train acc:  0.859375
train loss:  0.3169851303100586
train gradient:  0.1007595847652244
iteration : 13264
train acc:  0.8359375
train loss:  0.32672059535980225
train gradient:  0.1496385870172909
iteration : 13265
train acc:  0.8671875
train loss:  0.3550383448600769
train gradient:  0.15360875429056126
iteration : 13266
train acc:  0.796875
train loss:  0.3609846234321594
train gradient:  0.13705041796196565
iteration : 13267
train acc:  0.875
train loss:  0.31007227301597595
train gradient:  0.09917305404770961
iteration : 13268
train acc:  0.875
train loss:  0.3195075988769531
train gradient:  0.10870541291351742
iteration : 13269
train acc:  0.8359375
train loss:  0.3212820291519165
train gradient:  0.11491178103324881
iteration : 13270
train acc:  0.859375
train loss:  0.3293452858924866
train gradient:  0.10357488038855997
iteration : 13271
train acc:  0.8125
train loss:  0.42946183681488037
train gradient:  0.2441217515467456
iteration : 13272
train acc:  0.90625
train loss:  0.25435546040534973
train gradient:  0.10208530701718051
iteration : 13273
train acc:  0.84375
train loss:  0.30931293964385986
train gradient:  0.14405594291668866
iteration : 13274
train acc:  0.8359375
train loss:  0.33609527349472046
train gradient:  0.13165802376889638
iteration : 13275
train acc:  0.859375
train loss:  0.32071977853775024
train gradient:  0.11433121180402991
iteration : 13276
train acc:  0.90625
train loss:  0.27679628133773804
train gradient:  0.09341887114873018
iteration : 13277
train acc:  0.875
train loss:  0.33232319355010986
train gradient:  0.1843461624565938
iteration : 13278
train acc:  0.8984375
train loss:  0.237584188580513
train gradient:  0.08126544876425978
iteration : 13279
train acc:  0.8515625
train loss:  0.36134588718414307
train gradient:  0.1447325568290532
iteration : 13280
train acc:  0.8671875
train loss:  0.2964266538619995
train gradient:  0.16886853603774465
iteration : 13281
train acc:  0.890625
train loss:  0.3027620315551758
train gradient:  0.1759325952973279
iteration : 13282
train acc:  0.84375
train loss:  0.2961476743221283
train gradient:  0.11265450915530104
iteration : 13283
train acc:  0.8984375
train loss:  0.2582090497016907
train gradient:  0.08704744128105148
iteration : 13284
train acc:  0.8671875
train loss:  0.28895890712738037
train gradient:  0.09702056464661639
iteration : 13285
train acc:  0.8984375
train loss:  0.2832634449005127
train gradient:  0.14198517867748087
iteration : 13286
train acc:  0.890625
train loss:  0.26613688468933105
train gradient:  0.0926998393118455
iteration : 13287
train acc:  0.84375
train loss:  0.3596438765525818
train gradient:  0.12537290072195434
iteration : 13288
train acc:  0.8359375
train loss:  0.3623858690261841
train gradient:  0.17819151498276564
iteration : 13289
train acc:  0.9296875
train loss:  0.2774425148963928
train gradient:  0.10603814147944336
iteration : 13290
train acc:  0.84375
train loss:  0.32697784900665283
train gradient:  0.16636946898640642
iteration : 13291
train acc:  0.8828125
train loss:  0.26696836948394775
train gradient:  0.10836644906130292
iteration : 13292
train acc:  0.859375
train loss:  0.3819226622581482
train gradient:  0.2451340644429451
iteration : 13293
train acc:  0.8515625
train loss:  0.3511907756328583
train gradient:  0.14418991683050353
iteration : 13294
train acc:  0.8671875
train loss:  0.2812631130218506
train gradient:  0.10260181143067462
iteration : 13295
train acc:  0.875
train loss:  0.3678605556488037
train gradient:  0.1638126550688927
iteration : 13296
train acc:  0.8359375
train loss:  0.38356709480285645
train gradient:  0.1598085069156463
iteration : 13297
train acc:  0.828125
train loss:  0.3685619831085205
train gradient:  0.14995155298503574
iteration : 13298
train acc:  0.8828125
train loss:  0.3375282287597656
train gradient:  0.13462653142980113
iteration : 13299
train acc:  0.8828125
train loss:  0.25751760601997375
train gradient:  0.12139614255033683
iteration : 13300
train acc:  0.8671875
train loss:  0.29178091883659363
train gradient:  0.10944936532259952
iteration : 13301
train acc:  0.84375
train loss:  0.34159472584724426
train gradient:  0.1138914602415071
iteration : 13302
train acc:  0.8359375
train loss:  0.3418177664279938
train gradient:  0.17714836081783805
iteration : 13303
train acc:  0.8984375
train loss:  0.28509172797203064
train gradient:  0.10624092667982045
iteration : 13304
train acc:  0.859375
train loss:  0.3074057996273041
train gradient:  0.08540194243771965
iteration : 13305
train acc:  0.84375
train loss:  0.3891088366508484
train gradient:  0.2772966926510655
iteration : 13306
train acc:  0.859375
train loss:  0.3403168320655823
train gradient:  0.1689694813861392
iteration : 13307
train acc:  0.859375
train loss:  0.32133156061172485
train gradient:  0.16535868562980016
iteration : 13308
train acc:  0.8671875
train loss:  0.31860002875328064
train gradient:  0.16551187190397343
iteration : 13309
train acc:  0.7890625
train loss:  0.4088820815086365
train gradient:  0.19001538131007495
iteration : 13310
train acc:  0.875
train loss:  0.26150208711624146
train gradient:  0.13670629716801463
iteration : 13311
train acc:  0.8828125
train loss:  0.31292739510536194
train gradient:  0.10935755182408487
iteration : 13312
train acc:  0.875
train loss:  0.38956165313720703
train gradient:  0.18284927473794016
iteration : 13313
train acc:  0.8515625
train loss:  0.2915630042552948
train gradient:  0.11884111725372382
iteration : 13314
train acc:  0.84375
train loss:  0.3449086546897888
train gradient:  0.2630264470625023
iteration : 13315
train acc:  0.8671875
train loss:  0.3363775610923767
train gradient:  0.1706560468217116
iteration : 13316
train acc:  0.8203125
train loss:  0.3988136947154999
train gradient:  0.19283283896436718
iteration : 13317
train acc:  0.8828125
train loss:  0.3052917718887329
train gradient:  0.11663725919409687
iteration : 13318
train acc:  0.890625
train loss:  0.33498895168304443
train gradient:  0.18710923355257847
iteration : 13319
train acc:  0.859375
train loss:  0.25730201601982117
train gradient:  0.1027816471571321
iteration : 13320
train acc:  0.8828125
train loss:  0.33500468730926514
train gradient:  0.1503571243252181
iteration : 13321
train acc:  0.8984375
train loss:  0.26635026931762695
train gradient:  0.10566374107930197
iteration : 13322
train acc:  0.8203125
train loss:  0.33784469962120056
train gradient:  0.16272041510913332
iteration : 13323
train acc:  0.8984375
train loss:  0.31957751512527466
train gradient:  0.11738491711345986
iteration : 13324
train acc:  0.828125
train loss:  0.3420262038707733
train gradient:  0.11116259614697548
iteration : 13325
train acc:  0.8671875
train loss:  0.3069947361946106
train gradient:  0.11895833607459724
iteration : 13326
train acc:  0.8359375
train loss:  0.38845276832580566
train gradient:  0.15108205618388948
iteration : 13327
train acc:  0.828125
train loss:  0.38832947611808777
train gradient:  0.2708297692928382
iteration : 13328
train acc:  0.859375
train loss:  0.29727041721343994
train gradient:  0.07585956120010709
iteration : 13329
train acc:  0.8515625
train loss:  0.3573535680770874
train gradient:  0.17262886701993274
iteration : 13330
train acc:  0.8671875
train loss:  0.2706783413887024
train gradient:  0.1008862543204595
iteration : 13331
train acc:  0.921875
train loss:  0.26514363288879395
train gradient:  0.10800321903183631
iteration : 13332
train acc:  0.9296875
train loss:  0.2256837785243988
train gradient:  0.09614550506529555
iteration : 13333
train acc:  0.890625
train loss:  0.26684504747390747
train gradient:  0.1181612954489912
iteration : 13334
train acc:  0.8359375
train loss:  0.35006797313690186
train gradient:  0.14282635945395633
iteration : 13335
train acc:  0.859375
train loss:  0.29714512825012207
train gradient:  0.1378348436959462
iteration : 13336
train acc:  0.8359375
train loss:  0.3740631341934204
train gradient:  0.16955758847996594
iteration : 13337
train acc:  0.8828125
train loss:  0.3005536198616028
train gradient:  0.12777092219348848
iteration : 13338
train acc:  0.84375
train loss:  0.31836432218551636
train gradient:  0.13731302728219086
iteration : 13339
train acc:  0.8671875
train loss:  0.33888521790504456
train gradient:  0.17119226488924943
iteration : 13340
train acc:  0.875
train loss:  0.38755619525909424
train gradient:  0.25413932951309476
iteration : 13341
train acc:  0.8828125
train loss:  0.2958201766014099
train gradient:  0.09220803230097717
iteration : 13342
train acc:  0.859375
train loss:  0.3314664661884308
train gradient:  0.19236654581846654
iteration : 13343
train acc:  0.890625
train loss:  0.3067147731781006
train gradient:  0.3103314253321715
iteration : 13344
train acc:  0.8671875
train loss:  0.28610366582870483
train gradient:  0.10084343589683377
iteration : 13345
train acc:  0.921875
train loss:  0.22476571798324585
train gradient:  0.10183207234792507
iteration : 13346
train acc:  0.875
train loss:  0.33939048647880554
train gradient:  0.1368126502336347
iteration : 13347
train acc:  0.8125
train loss:  0.3594479262828827
train gradient:  0.14318555042518627
iteration : 13348
train acc:  0.859375
train loss:  0.3237350285053253
train gradient:  0.10438644418428711
iteration : 13349
train acc:  0.8671875
train loss:  0.33463549613952637
train gradient:  0.19447179830474967
iteration : 13350
train acc:  0.859375
train loss:  0.2902083098888397
train gradient:  0.1449195357777765
iteration : 13351
train acc:  0.8671875
train loss:  0.3063259720802307
train gradient:  0.1579956609626481
iteration : 13352
train acc:  0.90625
train loss:  0.2148243635892868
train gradient:  0.07042330615581391
iteration : 13353
train acc:  0.890625
train loss:  0.2663438320159912
train gradient:  0.09205787493787963
iteration : 13354
train acc:  0.921875
train loss:  0.2067914605140686
train gradient:  0.07232574580767878
iteration : 13355
train acc:  0.859375
train loss:  0.27557530999183655
train gradient:  0.09989741034985411
iteration : 13356
train acc:  0.890625
train loss:  0.2859446704387665
train gradient:  0.09651997701804624
iteration : 13357
train acc:  0.921875
train loss:  0.22150927782058716
train gradient:  0.11199446143948807
iteration : 13358
train acc:  0.875
train loss:  0.3468382954597473
train gradient:  0.13861680881916832
iteration : 13359
train acc:  0.8984375
train loss:  0.260879784822464
train gradient:  0.1064809494753583
iteration : 13360
train acc:  0.84375
train loss:  0.33062833547592163
train gradient:  0.1289994881597195
iteration : 13361
train acc:  0.8828125
train loss:  0.35610976815223694
train gradient:  0.14746371661347124
iteration : 13362
train acc:  0.8359375
train loss:  0.3404192328453064
train gradient:  0.13871572069055144
iteration : 13363
train acc:  0.8671875
train loss:  0.3167344927787781
train gradient:  0.13783739534823936
iteration : 13364
train acc:  0.84375
train loss:  0.35459399223327637
train gradient:  0.16011230990805314
iteration : 13365
train acc:  0.8515625
train loss:  0.28485313057899475
train gradient:  0.16820248028137558
iteration : 13366
train acc:  0.8828125
train loss:  0.3072153627872467
train gradient:  0.135717572177598
iteration : 13367
train acc:  0.859375
train loss:  0.29425597190856934
train gradient:  0.12858997592751206
iteration : 13368
train acc:  0.8359375
train loss:  0.3409661650657654
train gradient:  0.1583086605303437
iteration : 13369
train acc:  0.78125
train loss:  0.4388614296913147
train gradient:  0.18244157811918324
iteration : 13370
train acc:  0.8359375
train loss:  0.40943458676338196
train gradient:  0.19582712491427196
iteration : 13371
train acc:  0.84375
train loss:  0.32284778356552124
train gradient:  0.1431992236188463
iteration : 13372
train acc:  0.8203125
train loss:  0.38468676805496216
train gradient:  0.3626105728077901
iteration : 13373
train acc:  0.8984375
train loss:  0.2384134978055954
train gradient:  0.08434110583292193
iteration : 13374
train acc:  0.921875
train loss:  0.23945936560630798
train gradient:  0.09673338355372467
iteration : 13375
train acc:  0.8671875
train loss:  0.26753759384155273
train gradient:  0.10903897876304876
iteration : 13376
train acc:  0.890625
train loss:  0.2939559817314148
train gradient:  0.10233022352548284
iteration : 13377
train acc:  0.8828125
train loss:  0.2976301908493042
train gradient:  0.13386758543320762
iteration : 13378
train acc:  0.8828125
train loss:  0.2958643138408661
train gradient:  0.09461094555876375
iteration : 13379
train acc:  0.8671875
train loss:  0.2744626998901367
train gradient:  0.09176795246998215
iteration : 13380
train acc:  0.8828125
train loss:  0.2998053729534149
train gradient:  0.10790443537222202
iteration : 13381
train acc:  0.859375
train loss:  0.3036951422691345
train gradient:  0.23510843273510046
iteration : 13382
train acc:  0.890625
train loss:  0.24154523015022278
train gradient:  0.07424762871061498
iteration : 13383
train acc:  0.84375
train loss:  0.38363829255104065
train gradient:  0.17342371792694494
iteration : 13384
train acc:  0.8515625
train loss:  0.37118932604789734
train gradient:  0.16209267477466094
iteration : 13385
train acc:  0.84375
train loss:  0.34212955832481384
train gradient:  0.17954824114432277
iteration : 13386
train acc:  0.875
train loss:  0.29096925258636475
train gradient:  0.09349241874388822
iteration : 13387
train acc:  0.921875
train loss:  0.23556995391845703
train gradient:  0.10845030752060744
iteration : 13388
train acc:  0.875
train loss:  0.2982165217399597
train gradient:  0.1370591131520914
iteration : 13389
train acc:  0.8359375
train loss:  0.32411953806877136
train gradient:  0.1207679675504531
iteration : 13390
train acc:  0.84375
train loss:  0.34582725167274475
train gradient:  0.17034467625954153
iteration : 13391
train acc:  0.8828125
train loss:  0.24267345666885376
train gradient:  0.0741295199548899
iteration : 13392
train acc:  0.84375
train loss:  0.30889880657196045
train gradient:  0.1265170558930423
iteration : 13393
train acc:  0.8359375
train loss:  0.37377509474754333
train gradient:  0.16376360059365497
iteration : 13394
train acc:  0.890625
train loss:  0.28890806436538696
train gradient:  0.09958908452079976
iteration : 13395
train acc:  0.890625
train loss:  0.24548405408859253
train gradient:  0.08486909890934655
iteration : 13396
train acc:  0.8203125
train loss:  0.3612990975379944
train gradient:  0.1706564231390847
iteration : 13397
train acc:  0.890625
train loss:  0.2562296986579895
train gradient:  0.12366132358451161
iteration : 13398
train acc:  0.90625
train loss:  0.3125711977481842
train gradient:  0.23302435848201206
iteration : 13399
train acc:  0.875
train loss:  0.33502092957496643
train gradient:  0.13938831317675288
iteration : 13400
train acc:  0.84375
train loss:  0.2762688994407654
train gradient:  0.121320930034069
iteration : 13401
train acc:  0.8203125
train loss:  0.3960561752319336
train gradient:  0.23175793851153445
iteration : 13402
train acc:  0.875
train loss:  0.3349800109863281
train gradient:  0.13969824938052305
iteration : 13403
train acc:  0.8359375
train loss:  0.3845791518688202
train gradient:  0.14907009233036625
iteration : 13404
train acc:  0.8671875
train loss:  0.3380364775657654
train gradient:  0.15268630507488085
iteration : 13405
train acc:  0.8203125
train loss:  0.38478899002075195
train gradient:  0.16637098438053818
iteration : 13406
train acc:  0.859375
train loss:  0.30388009548187256
train gradient:  0.09544641050738985
iteration : 13407
train acc:  0.8671875
train loss:  0.2863922119140625
train gradient:  0.10596296634683586
iteration : 13408
train acc:  0.8671875
train loss:  0.30342400074005127
train gradient:  0.08381614574331372
iteration : 13409
train acc:  0.8984375
train loss:  0.2358217090368271
train gradient:  0.09838640242988117
iteration : 13410
train acc:  0.8515625
train loss:  0.37679988145828247
train gradient:  0.1672426490846754
iteration : 13411
train acc:  0.9375
train loss:  0.21432942152023315
train gradient:  0.08492324745261391
iteration : 13412
train acc:  0.8046875
train loss:  0.38766857981681824
train gradient:  0.2183992874284903
iteration : 13413
train acc:  0.8671875
train loss:  0.2649124264717102
train gradient:  0.1181542983871424
iteration : 13414
train acc:  0.9140625
train loss:  0.2695451080799103
train gradient:  0.08349962162309262
iteration : 13415
train acc:  0.890625
train loss:  0.31411218643188477
train gradient:  0.11340977779955796
iteration : 13416
train acc:  0.875
train loss:  0.2887135148048401
train gradient:  0.12723747453526296
iteration : 13417
train acc:  0.8046875
train loss:  0.4326441287994385
train gradient:  0.21228758463168257
iteration : 13418
train acc:  0.875
train loss:  0.3370550870895386
train gradient:  0.16587428179907193
iteration : 13419
train acc:  0.8671875
train loss:  0.2551623582839966
train gradient:  0.09926806849230188
iteration : 13420
train acc:  0.890625
train loss:  0.25306516885757446
train gradient:  0.10767254350768539
iteration : 13421
train acc:  0.8359375
train loss:  0.3668403923511505
train gradient:  0.1751866637431556
iteration : 13422
train acc:  0.84375
train loss:  0.4224339425563812
train gradient:  0.1957332406841822
iteration : 13423
train acc:  0.8125
train loss:  0.34175899624824524
train gradient:  0.16084725905648578
iteration : 13424
train acc:  0.84375
train loss:  0.2816854417324066
train gradient:  0.11808501650995622
iteration : 13425
train acc:  0.84375
train loss:  0.3057706952095032
train gradient:  0.11778438686841425
iteration : 13426
train acc:  0.8359375
train loss:  0.34772002696990967
train gradient:  0.16001377840619085
iteration : 13427
train acc:  0.90625
train loss:  0.2893258333206177
train gradient:  0.11451639530082804
iteration : 13428
train acc:  0.90625
train loss:  0.29612278938293457
train gradient:  0.11699876843439032
iteration : 13429
train acc:  0.875
train loss:  0.2763644754886627
train gradient:  0.10546086078976781
iteration : 13430
train acc:  0.8359375
train loss:  0.33234527707099915
train gradient:  0.16531471439604972
iteration : 13431
train acc:  0.8984375
train loss:  0.2496211677789688
train gradient:  0.06889879057942869
iteration : 13432
train acc:  0.8515625
train loss:  0.36473149061203003
train gradient:  0.20737570460887034
iteration : 13433
train acc:  0.8515625
train loss:  0.28362518548965454
train gradient:  0.11431291306042939
iteration : 13434
train acc:  0.84375
train loss:  0.35672980546951294
train gradient:  0.22467705358634
iteration : 13435
train acc:  0.890625
train loss:  0.26667192578315735
train gradient:  0.07677563239215884
iteration : 13436
train acc:  0.8125
train loss:  0.4234103262424469
train gradient:  0.22800214476738456
iteration : 13437
train acc:  0.90625
train loss:  0.2686081826686859
train gradient:  0.10673644465233645
iteration : 13438
train acc:  0.84375
train loss:  0.3280245363712311
train gradient:  0.1815236102435972
iteration : 13439
train acc:  0.9140625
train loss:  0.2626999020576477
train gradient:  0.1165344226856829
iteration : 13440
train acc:  0.8671875
train loss:  0.274843692779541
train gradient:  0.106273965428759
iteration : 13441
train acc:  0.84375
train loss:  0.43810686469078064
train gradient:  0.22599678842847662
iteration : 13442
train acc:  0.8671875
train loss:  0.27751094102859497
train gradient:  0.1203536629095756
iteration : 13443
train acc:  0.90625
train loss:  0.2575905919075012
train gradient:  0.09807316727932848
iteration : 13444
train acc:  0.8671875
train loss:  0.27748122811317444
train gradient:  0.13081398040047698
iteration : 13445
train acc:  0.8359375
train loss:  0.39141130447387695
train gradient:  0.18935150514639437
iteration : 13446
train acc:  0.9296875
train loss:  0.22798210382461548
train gradient:  0.09514875888804797
iteration : 13447
train acc:  0.875
train loss:  0.29544317722320557
train gradient:  0.13883717152324287
iteration : 13448
train acc:  0.890625
train loss:  0.21840792894363403
train gradient:  0.08588736934726894
iteration : 13449
train acc:  0.8671875
train loss:  0.3185356557369232
train gradient:  0.15277908776905377
iteration : 13450
train acc:  0.859375
train loss:  0.3109939396381378
train gradient:  0.10574207024672345
iteration : 13451
train acc:  0.8671875
train loss:  0.3215828537940979
train gradient:  0.14897500142850414
iteration : 13452
train acc:  0.84375
train loss:  0.33843767642974854
train gradient:  0.13658305892970146
iteration : 13453
train acc:  0.84375
train loss:  0.36426591873168945
train gradient:  0.14811540718170457
iteration : 13454
train acc:  0.8828125
train loss:  0.2544800937175751
train gradient:  0.10526012826262299
iteration : 13455
train acc:  0.875
train loss:  0.2728334069252014
train gradient:  0.1555990995401126
iteration : 13456
train acc:  0.84375
train loss:  0.3902834951877594
train gradient:  0.23963711566535706
iteration : 13457
train acc:  0.8984375
train loss:  0.2645224928855896
train gradient:  0.11958190195397414
iteration : 13458
train acc:  0.8515625
train loss:  0.34248799085617065
train gradient:  0.17914319916143834
iteration : 13459
train acc:  0.890625
train loss:  0.2640199661254883
train gradient:  0.12479353045440598
iteration : 13460
train acc:  0.828125
train loss:  0.3504835367202759
train gradient:  0.14994740961121447
iteration : 13461
train acc:  0.828125
train loss:  0.37505030632019043
train gradient:  0.16549964333690992
iteration : 13462
train acc:  0.84375
train loss:  0.35247310996055603
train gradient:  0.15944882169625657
iteration : 13463
train acc:  0.890625
train loss:  0.260708212852478
train gradient:  0.11440440668002706
iteration : 13464
train acc:  0.8125
train loss:  0.36281588673591614
train gradient:  0.1449635104652839
iteration : 13465
train acc:  0.8359375
train loss:  0.3210980296134949
train gradient:  0.14766893718037016
iteration : 13466
train acc:  0.84375
train loss:  0.35532018542289734
train gradient:  0.13306695752218972
iteration : 13467
train acc:  0.8359375
train loss:  0.3398682475090027
train gradient:  0.1451353260577859
iteration : 13468
train acc:  0.8828125
train loss:  0.2922539710998535
train gradient:  0.11216830646166602
iteration : 13469
train acc:  0.8671875
train loss:  0.29806458950042725
train gradient:  0.13892787506069737
iteration : 13470
train acc:  0.8515625
train loss:  0.3041929602622986
train gradient:  0.13366162748296906
iteration : 13471
train acc:  0.828125
train loss:  0.4261826276779175
train gradient:  0.22215034122107857
iteration : 13472
train acc:  0.8671875
train loss:  0.26434993743896484
train gradient:  0.10652836200046341
iteration : 13473
train acc:  0.8203125
train loss:  0.38641244173049927
train gradient:  0.20832209611536168
iteration : 13474
train acc:  0.8125
train loss:  0.2893337607383728
train gradient:  0.14281725528024025
iteration : 13475
train acc:  0.8828125
train loss:  0.2819991111755371
train gradient:  0.10454719142321188
iteration : 13476
train acc:  0.8359375
train loss:  0.3214421272277832
train gradient:  0.1310945986259034
iteration : 13477
train acc:  0.8359375
train loss:  0.39796167612075806
train gradient:  0.22136645908913347
iteration : 13478
train acc:  0.8515625
train loss:  0.3263881206512451
train gradient:  0.1446583119081043
iteration : 13479
train acc:  0.8671875
train loss:  0.2964296340942383
train gradient:  0.13033277469249735
iteration : 13480
train acc:  0.8984375
train loss:  0.32477182149887085
train gradient:  0.14682447526874842
iteration : 13481
train acc:  0.90625
train loss:  0.3051881790161133
train gradient:  0.13736497116448954
iteration : 13482
train acc:  0.796875
train loss:  0.4534169137477875
train gradient:  0.2396661237796925
iteration : 13483
train acc:  0.8359375
train loss:  0.37040215730667114
train gradient:  0.2446826076275393
iteration : 13484
train acc:  0.828125
train loss:  0.29783010482788086
train gradient:  0.1736695442866516
iteration : 13485
train acc:  0.890625
train loss:  0.2839633524417877
train gradient:  0.11445599075869789
iteration : 13486
train acc:  0.90625
train loss:  0.2774060368537903
train gradient:  0.11567169649679696
iteration : 13487
train acc:  0.828125
train loss:  0.3800841271877289
train gradient:  0.1989123617471793
iteration : 13488
train acc:  0.8515625
train loss:  0.34346866607666016
train gradient:  0.11626600060975201
iteration : 13489
train acc:  0.8828125
train loss:  0.2765668034553528
train gradient:  0.0869898462287159
iteration : 13490
train acc:  0.8125
train loss:  0.38596728444099426
train gradient:  0.22635982921322478
iteration : 13491
train acc:  0.9140625
train loss:  0.218936949968338
train gradient:  0.08281658139378745
iteration : 13492
train acc:  0.875
train loss:  0.3018435835838318
train gradient:  0.12800819950768186
iteration : 13493
train acc:  0.8515625
train loss:  0.33317261934280396
train gradient:  0.15776371499934286
iteration : 13494
train acc:  0.8671875
train loss:  0.2953741252422333
train gradient:  0.11419195183402822
iteration : 13495
train acc:  0.828125
train loss:  0.36352843046188354
train gradient:  0.15416652749963358
iteration : 13496
train acc:  0.8671875
train loss:  0.26135390996932983
train gradient:  0.12462635988682988
iteration : 13497
train acc:  0.828125
train loss:  0.318725049495697
train gradient:  0.136791471714575
iteration : 13498
train acc:  0.8984375
train loss:  0.257455438375473
train gradient:  0.08100945643457905
iteration : 13499
train acc:  0.8125
train loss:  0.4130558967590332
train gradient:  0.2037990942544451
iteration : 13500
train acc:  0.8828125
train loss:  0.2743526101112366
train gradient:  0.07379829467973736
iteration : 13501
train acc:  0.8203125
train loss:  0.3589669466018677
train gradient:  0.15881045798173268
iteration : 13502
train acc:  0.84375
train loss:  0.34208202362060547
train gradient:  0.125551805018959
iteration : 13503
train acc:  0.8984375
train loss:  0.25444212555885315
train gradient:  0.11639284288434101
iteration : 13504
train acc:  0.828125
train loss:  0.37798669934272766
train gradient:  0.1859844237196499
iteration : 13505
train acc:  0.84375
train loss:  0.3310493230819702
train gradient:  0.13342228048311103
iteration : 13506
train acc:  0.8203125
train loss:  0.4385797679424286
train gradient:  0.20937632043373117
iteration : 13507
train acc:  0.796875
train loss:  0.36073213815689087
train gradient:  0.17887277999481885
iteration : 13508
train acc:  0.8046875
train loss:  0.4092206358909607
train gradient:  0.18825361838196653
iteration : 13509
train acc:  0.90625
train loss:  0.28061559796333313
train gradient:  0.11320838195091312
iteration : 13510
train acc:  0.875
train loss:  0.2483593076467514
train gradient:  0.09632923084606602
iteration : 13511
train acc:  0.8203125
train loss:  0.3475121557712555
train gradient:  0.1178843084843531
iteration : 13512
train acc:  0.828125
train loss:  0.4105355143547058
train gradient:  0.20607653393220174
iteration : 13513
train acc:  0.8828125
train loss:  0.3348362445831299
train gradient:  0.10507835034405305
iteration : 13514
train acc:  0.8515625
train loss:  0.3306390941143036
train gradient:  0.10816009047635086
iteration : 13515
train acc:  0.8828125
train loss:  0.30562880635261536
train gradient:  0.12780364069061445
iteration : 13516
train acc:  0.84375
train loss:  0.32364940643310547
train gradient:  0.12242582898191413
iteration : 13517
train acc:  0.8515625
train loss:  0.29218679666519165
train gradient:  0.1662926259449713
iteration : 13518
train acc:  0.8125
train loss:  0.3954225182533264
train gradient:  0.24469113149660002
iteration : 13519
train acc:  0.84375
train loss:  0.33524054288864136
train gradient:  0.13682441605149628
iteration : 13520
train acc:  0.8515625
train loss:  0.33628374338150024
train gradient:  0.12096924193671285
iteration : 13521
train acc:  0.8828125
train loss:  0.30242589116096497
train gradient:  0.1431608450360608
iteration : 13522
train acc:  0.8515625
train loss:  0.33727139234542847
train gradient:  0.13391769124855074
iteration : 13523
train acc:  0.859375
train loss:  0.2802814543247223
train gradient:  0.156753473115092
iteration : 13524
train acc:  0.875
train loss:  0.3237127363681793
train gradient:  0.10649183852534178
iteration : 13525
train acc:  0.8125
train loss:  0.39547693729400635
train gradient:  0.18163011728811432
iteration : 13526
train acc:  0.8359375
train loss:  0.3488474488258362
train gradient:  0.13285448073364894
iteration : 13527
train acc:  0.859375
train loss:  0.3028727173805237
train gradient:  0.13210342945779302
iteration : 13528
train acc:  0.9375
train loss:  0.21838225424289703
train gradient:  0.07587364219784784
iteration : 13529
train acc:  0.859375
train loss:  0.3688400983810425
train gradient:  0.20182477589842038
iteration : 13530
train acc:  0.828125
train loss:  0.33854126930236816
train gradient:  0.14152195758307495
iteration : 13531
train acc:  0.8515625
train loss:  0.3313007652759552
train gradient:  0.1923748589103772
iteration : 13532
train acc:  0.8203125
train loss:  0.3940170705318451
train gradient:  0.12425096740231702
iteration : 13533
train acc:  0.8671875
train loss:  0.29846400022506714
train gradient:  0.0972627098509302
iteration : 13534
train acc:  0.8671875
train loss:  0.31279388070106506
train gradient:  0.14582317049698895
iteration : 13535
train acc:  0.8515625
train loss:  0.3274303078651428
train gradient:  0.0999422716416401
iteration : 13536
train acc:  0.875
train loss:  0.2916991710662842
train gradient:  0.12912937649210166
iteration : 13537
train acc:  0.921875
train loss:  0.24590273201465607
train gradient:  0.07294148209901585
iteration : 13538
train acc:  0.8046875
train loss:  0.3698861598968506
train gradient:  0.1159497080249495
iteration : 13539
train acc:  0.8828125
train loss:  0.26755908131599426
train gradient:  0.08317277239847129
iteration : 13540
train acc:  0.8046875
train loss:  0.3769543766975403
train gradient:  0.10398241112762117
iteration : 13541
train acc:  0.90625
train loss:  0.2656117081642151
train gradient:  0.11682796720567247
iteration : 13542
train acc:  0.859375
train loss:  0.30875250697135925
train gradient:  0.09174360258844956
iteration : 13543
train acc:  0.8125
train loss:  0.4641609787940979
train gradient:  0.1966291191355171
iteration : 13544
train acc:  0.8125
train loss:  0.40566566586494446
train gradient:  0.19615911641715483
iteration : 13545
train acc:  0.828125
train loss:  0.31603357195854187
train gradient:  0.10135360621272872
iteration : 13546
train acc:  0.8671875
train loss:  0.3293718695640564
train gradient:  0.11570899045812932
iteration : 13547
train acc:  0.875
train loss:  0.3178534507751465
train gradient:  0.12536896489417057
iteration : 13548
train acc:  0.8515625
train loss:  0.34742701053619385
train gradient:  0.12422727772267526
iteration : 13549
train acc:  0.8359375
train loss:  0.4122422933578491
train gradient:  0.1859501862578039
iteration : 13550
train acc:  0.921875
train loss:  0.21979650855064392
train gradient:  0.07425250063055781
iteration : 13551
train acc:  0.84375
train loss:  0.32985663414001465
train gradient:  0.15112677580353368
iteration : 13552
train acc:  0.9296875
train loss:  0.2675086259841919
train gradient:  0.10389365893860883
iteration : 13553
train acc:  0.890625
train loss:  0.3052836060523987
train gradient:  0.2716438282767188
iteration : 13554
train acc:  0.875
train loss:  0.3240249752998352
train gradient:  0.0939189937275121
iteration : 13555
train acc:  0.890625
train loss:  0.2799363136291504
train gradient:  0.08771081610236396
iteration : 13556
train acc:  0.7734375
train loss:  0.43567347526550293
train gradient:  0.24979776324832675
iteration : 13557
train acc:  0.8515625
train loss:  0.28773778676986694
train gradient:  0.11627075471241424
iteration : 13558
train acc:  0.8125
train loss:  0.416967511177063
train gradient:  0.27093824148299933
iteration : 13559
train acc:  0.859375
train loss:  0.29284602403640747
train gradient:  0.14765331220516398
iteration : 13560
train acc:  0.8515625
train loss:  0.3374120593070984
train gradient:  0.1521203825562749
iteration : 13561
train acc:  0.875
train loss:  0.31402653455734253
train gradient:  0.1376291206578253
iteration : 13562
train acc:  0.90625
train loss:  0.2704094648361206
train gradient:  0.09632966495989265
iteration : 13563
train acc:  0.84375
train loss:  0.3158709406852722
train gradient:  0.19114066828100135
iteration : 13564
train acc:  0.90625
train loss:  0.309770792722702
train gradient:  0.12482966765065398
iteration : 13565
train acc:  0.90625
train loss:  0.284358948469162
train gradient:  0.1096296342059758
iteration : 13566
train acc:  0.8828125
train loss:  0.2725643813610077
train gradient:  0.09088069205898709
iteration : 13567
train acc:  0.8671875
train loss:  0.3196660578250885
train gradient:  0.12649821385716942
iteration : 13568
train acc:  0.78125
train loss:  0.42354410886764526
train gradient:  0.2570199350842723
iteration : 13569
train acc:  0.9296875
train loss:  0.26141536235809326
train gradient:  0.11186351698388036
iteration : 13570
train acc:  0.8671875
train loss:  0.3283995985984802
train gradient:  0.11077566128924765
iteration : 13571
train acc:  0.8515625
train loss:  0.3170417547225952
train gradient:  0.10155077489561826
iteration : 13572
train acc:  0.8828125
train loss:  0.32806074619293213
train gradient:  0.17696483665803683
iteration : 13573
train acc:  0.9140625
train loss:  0.27648067474365234
train gradient:  0.0902702873412469
iteration : 13574
train acc:  0.859375
train loss:  0.3582775890827179
train gradient:  0.19872858356370357
iteration : 13575
train acc:  0.875
train loss:  0.3003326654434204
train gradient:  0.11496044130183455
iteration : 13576
train acc:  0.8359375
train loss:  0.3414466977119446
train gradient:  0.12463507060116495
iteration : 13577
train acc:  0.859375
train loss:  0.2919013202190399
train gradient:  0.11244641766257421
iteration : 13578
train acc:  0.8671875
train loss:  0.2951996922492981
train gradient:  0.10128787907004375
iteration : 13579
train acc:  0.9296875
train loss:  0.2758826017379761
train gradient:  0.12704047111578798
iteration : 13580
train acc:  0.890625
train loss:  0.277805894613266
train gradient:  0.15259310961988035
iteration : 13581
train acc:  0.8828125
train loss:  0.30316975712776184
train gradient:  0.09951310012731467
iteration : 13582
train acc:  0.84375
train loss:  0.3336966633796692
train gradient:  0.12597476925727458
iteration : 13583
train acc:  0.8515625
train loss:  0.30345651507377625
train gradient:  0.13095525188075172
iteration : 13584
train acc:  0.875
train loss:  0.3322768211364746
train gradient:  0.22775844599493134
iteration : 13585
train acc:  0.84375
train loss:  0.3140482008457184
train gradient:  0.16579010249961112
iteration : 13586
train acc:  0.84375
train loss:  0.2770548462867737
train gradient:  0.09537293592823654
iteration : 13587
train acc:  0.9140625
train loss:  0.2554323673248291
train gradient:  0.15715353685478578
iteration : 13588
train acc:  0.8671875
train loss:  0.2830084562301636
train gradient:  0.12619781957342693
iteration : 13589
train acc:  0.859375
train loss:  0.33438366651535034
train gradient:  0.1842581175115795
iteration : 13590
train acc:  0.875
train loss:  0.30103349685668945
train gradient:  0.14330296787052416
iteration : 13591
train acc:  0.8515625
train loss:  0.3597395420074463
train gradient:  0.14093672524592157
iteration : 13592
train acc:  0.875
train loss:  0.3049062490463257
train gradient:  0.15260518546211793
iteration : 13593
train acc:  0.859375
train loss:  0.3169911503791809
train gradient:  0.09859682589246928
iteration : 13594
train acc:  0.90625
train loss:  0.2752091884613037
train gradient:  0.1329878199069685
iteration : 13595
train acc:  0.8671875
train loss:  0.3008609712123871
train gradient:  0.10530230224453078
iteration : 13596
train acc:  0.8984375
train loss:  0.26161158084869385
train gradient:  0.08660105033053887
iteration : 13597
train acc:  0.8515625
train loss:  0.3423529267311096
train gradient:  0.1843298519448932
iteration : 13598
train acc:  0.8671875
train loss:  0.3092682957649231
train gradient:  0.11059172896668581
iteration : 13599
train acc:  0.859375
train loss:  0.3054637908935547
train gradient:  0.09162662216359578
iteration : 13600
train acc:  0.8515625
train loss:  0.38653600215911865
train gradient:  0.1560364904024973
iteration : 13601
train acc:  0.8984375
train loss:  0.226190447807312
train gradient:  0.09393696573770317
iteration : 13602
train acc:  0.875
train loss:  0.29291412234306335
train gradient:  0.10637566894858204
iteration : 13603
train acc:  0.859375
train loss:  0.33292293548583984
train gradient:  0.15326281993474689
iteration : 13604
train acc:  0.90625
train loss:  0.27246424555778503
train gradient:  0.08332318838762966
iteration : 13605
train acc:  0.890625
train loss:  0.27989277243614197
train gradient:  0.09274664428344649
iteration : 13606
train acc:  0.8984375
train loss:  0.2545018792152405
train gradient:  0.06616268206346007
iteration : 13607
train acc:  0.8828125
train loss:  0.28890174627304077
train gradient:  0.12095236522634402
iteration : 13608
train acc:  0.859375
train loss:  0.2814928889274597
train gradient:  0.13321768196316636
iteration : 13609
train acc:  0.84375
train loss:  0.3558565378189087
train gradient:  0.18708527086192253
iteration : 13610
train acc:  0.84375
train loss:  0.3260895609855652
train gradient:  0.13395016724409659
iteration : 13611
train acc:  0.875
train loss:  0.31091466546058655
train gradient:  0.15256690950111326
iteration : 13612
train acc:  0.8359375
train loss:  0.33580702543258667
train gradient:  0.12898130357975351
iteration : 13613
train acc:  0.796875
train loss:  0.349789023399353
train gradient:  0.20574160002535133
iteration : 13614
train acc:  0.90625
train loss:  0.2425232231616974
train gradient:  0.08371936643825319
iteration : 13615
train acc:  0.90625
train loss:  0.23955568671226501
train gradient:  0.10404844318321332
iteration : 13616
train acc:  0.8828125
train loss:  0.29032546281814575
train gradient:  0.14346942573892574
iteration : 13617
train acc:  0.84375
train loss:  0.32851025462150574
train gradient:  0.19045825683957013
iteration : 13618
train acc:  0.8515625
train loss:  0.3530198037624359
train gradient:  0.18253364411212725
iteration : 13619
train acc:  0.84375
train loss:  0.34223830699920654
train gradient:  0.19104365859047884
iteration : 13620
train acc:  0.8359375
train loss:  0.3606143295764923
train gradient:  0.14637047377584256
iteration : 13621
train acc:  0.84375
train loss:  0.3119696378707886
train gradient:  0.12295659281053731
iteration : 13622
train acc:  0.8828125
train loss:  0.2595962584018707
train gradient:  0.11134539610276649
iteration : 13623
train acc:  0.875
train loss:  0.3038727045059204
train gradient:  0.14168016557339863
iteration : 13624
train acc:  0.8671875
train loss:  0.33232831954956055
train gradient:  0.167216072366915
iteration : 13625
train acc:  0.8828125
train loss:  0.2681429982185364
train gradient:  0.13030208308320235
iteration : 13626
train acc:  0.8515625
train loss:  0.2886578440666199
train gradient:  0.11104950362356907
iteration : 13627
train acc:  0.8203125
train loss:  0.31215453147888184
train gradient:  0.14175264857423153
iteration : 13628
train acc:  0.8125
train loss:  0.347526490688324
train gradient:  0.14187174396407198
iteration : 13629
train acc:  0.8359375
train loss:  0.3714732229709625
train gradient:  0.18525402909120142
iteration : 13630
train acc:  0.828125
train loss:  0.3648855984210968
train gradient:  0.14593137706702736
iteration : 13631
train acc:  0.890625
train loss:  0.28211507201194763
train gradient:  0.16562900024906255
iteration : 13632
train acc:  0.8671875
train loss:  0.32911747694015503
train gradient:  0.12787734394540107
iteration : 13633
train acc:  0.875
train loss:  0.33304375410079956
train gradient:  0.12503170764653182
iteration : 13634
train acc:  0.8359375
train loss:  0.4023129940032959
train gradient:  0.22919203091423795
iteration : 13635
train acc:  0.8671875
train loss:  0.328561395406723
train gradient:  0.1708845510680642
iteration : 13636
train acc:  0.875
train loss:  0.31739264726638794
train gradient:  0.14231861454781025
iteration : 13637
train acc:  0.8515625
train loss:  0.34238868951797485
train gradient:  0.13512788864553346
iteration : 13638
train acc:  0.8203125
train loss:  0.3738933205604553
train gradient:  0.21183156206805348
iteration : 13639
train acc:  0.828125
train loss:  0.38291698694229126
train gradient:  0.2573351256263721
iteration : 13640
train acc:  0.859375
train loss:  0.26618126034736633
train gradient:  0.14403395822501336
iteration : 13641
train acc:  0.828125
train loss:  0.36141300201416016
train gradient:  0.15402810522254037
iteration : 13642
train acc:  0.859375
train loss:  0.33514416217803955
train gradient:  0.15869928244822235
iteration : 13643
train acc:  0.8515625
train loss:  0.30986422300338745
train gradient:  0.11232225527749168
iteration : 13644
train acc:  0.8046875
train loss:  0.36795276403427124
train gradient:  0.19956825757163316
iteration : 13645
train acc:  0.8828125
train loss:  0.2942250967025757
train gradient:  0.1042820748288781
iteration : 13646
train acc:  0.8359375
train loss:  0.36253783106803894
train gradient:  0.23176244253266143
iteration : 13647
train acc:  0.875
train loss:  0.340481162071228
train gradient:  0.13650044684720503
iteration : 13648
train acc:  0.859375
train loss:  0.3641994297504425
train gradient:  0.1611074350178493
iteration : 13649
train acc:  0.8828125
train loss:  0.37880706787109375
train gradient:  0.1634791202037607
iteration : 13650
train acc:  0.8671875
train loss:  0.2711060345172882
train gradient:  0.11606336296311487
iteration : 13651
train acc:  0.921875
train loss:  0.2160359025001526
train gradient:  0.08299385516383838
iteration : 13652
train acc:  0.8671875
train loss:  0.2692032754421234
train gradient:  0.10528127116610349
iteration : 13653
train acc:  0.9140625
train loss:  0.26059672236442566
train gradient:  0.11989519896032348
iteration : 13654
train acc:  0.890625
train loss:  0.2484823763370514
train gradient:  0.07942978789428018
iteration : 13655
train acc:  0.890625
train loss:  0.2529434263706207
train gradient:  0.08473947398902519
iteration : 13656
train acc:  0.90625
train loss:  0.2796010375022888
train gradient:  0.13041684391018998
iteration : 13657
train acc:  0.8515625
train loss:  0.35746920108795166
train gradient:  0.13434195650709618
iteration : 13658
train acc:  0.90625
train loss:  0.28017956018447876
train gradient:  0.178238387351841
iteration : 13659
train acc:  0.8359375
train loss:  0.42812222242355347
train gradient:  0.2655204909067036
iteration : 13660
train acc:  0.8828125
train loss:  0.26550525426864624
train gradient:  0.0954641438557942
iteration : 13661
train acc:  0.8359375
train loss:  0.290496289730072
train gradient:  0.1445719585709953
iteration : 13662
train acc:  0.890625
train loss:  0.2649520933628082
train gradient:  0.14106727514350142
iteration : 13663
train acc:  0.859375
train loss:  0.3625735640525818
train gradient:  0.24879074867958975
iteration : 13664
train acc:  0.8984375
train loss:  0.2877494692802429
train gradient:  0.11300627504915438
iteration : 13665
train acc:  0.8359375
train loss:  0.3503100275993347
train gradient:  0.17699928136541984
iteration : 13666
train acc:  0.8359375
train loss:  0.3511999547481537
train gradient:  0.18339681970778168
iteration : 13667
train acc:  0.8828125
train loss:  0.3132690191268921
train gradient:  0.12478945704626418
iteration : 13668
train acc:  0.828125
train loss:  0.38145506381988525
train gradient:  0.20376843364587224
iteration : 13669
train acc:  0.875
train loss:  0.2923849821090698
train gradient:  0.15808315197883213
iteration : 13670
train acc:  0.7890625
train loss:  0.3549845218658447
train gradient:  0.31825547229048395
iteration : 13671
train acc:  0.890625
train loss:  0.24353712797164917
train gradient:  0.08159750429806797
iteration : 13672
train acc:  0.859375
train loss:  0.3424645662307739
train gradient:  0.17995061271893764
iteration : 13673
train acc:  0.8828125
train loss:  0.29187917709350586
train gradient:  0.19299508923636916
iteration : 13674
train acc:  0.8984375
train loss:  0.2552986741065979
train gradient:  0.10459662259612774
iteration : 13675
train acc:  0.859375
train loss:  0.2767959237098694
train gradient:  0.1333772789559515
iteration : 13676
train acc:  0.859375
train loss:  0.27229851484298706
train gradient:  0.12297514489731998
iteration : 13677
train acc:  0.8828125
train loss:  0.32920122146606445
train gradient:  0.12073676512627488
iteration : 13678
train acc:  0.8828125
train loss:  0.29724693298339844
train gradient:  0.11422691526240937
iteration : 13679
train acc:  0.890625
train loss:  0.29648035764694214
train gradient:  0.14188424068604916
iteration : 13680
train acc:  0.8359375
train loss:  0.32586365938186646
train gradient:  0.1403182385421753
iteration : 13681
train acc:  0.8515625
train loss:  0.31997305154800415
train gradient:  0.110314656516049
iteration : 13682
train acc:  0.890625
train loss:  0.32875537872314453
train gradient:  0.14993532700148673
iteration : 13683
train acc:  0.8828125
train loss:  0.30077630281448364
train gradient:  0.12307721932114185
iteration : 13684
train acc:  0.875
train loss:  0.31507277488708496
train gradient:  0.16930352424216366
iteration : 13685
train acc:  0.890625
train loss:  0.2673304080963135
train gradient:  0.08982254228051255
iteration : 13686
train acc:  0.7890625
train loss:  0.41033637523651123
train gradient:  0.17858535401888853
iteration : 13687
train acc:  0.828125
train loss:  0.33016619086265564
train gradient:  0.18591138087457765
iteration : 13688
train acc:  0.8984375
train loss:  0.25400981307029724
train gradient:  0.08539212111302165
iteration : 13689
train acc:  0.8671875
train loss:  0.33430999517440796
train gradient:  0.13297178417643196
iteration : 13690
train acc:  0.8671875
train loss:  0.3257347345352173
train gradient:  0.13927787521479565
iteration : 13691
train acc:  0.8984375
train loss:  0.2319907397031784
train gradient:  0.08290521620483114
iteration : 13692
train acc:  0.921875
train loss:  0.2194213569164276
train gradient:  0.1094309512481877
iteration : 13693
train acc:  0.921875
train loss:  0.25628137588500977
train gradient:  0.06629164701475564
iteration : 13694
train acc:  0.8359375
train loss:  0.30843299627304077
train gradient:  0.11518331684141808
iteration : 13695
train acc:  0.8203125
train loss:  0.3440278470516205
train gradient:  0.14859654139575634
iteration : 13696
train acc:  0.8671875
train loss:  0.3292023539543152
train gradient:  0.15504610979834044
iteration : 13697
train acc:  0.8125
train loss:  0.3911358714103699
train gradient:  0.2360244703273392
iteration : 13698
train acc:  0.90625
train loss:  0.26229429244995117
train gradient:  0.07677255741303574
iteration : 13699
train acc:  0.859375
train loss:  0.2983315587043762
train gradient:  0.10968424631140082
iteration : 13700
train acc:  0.84375
train loss:  0.3920324444770813
train gradient:  0.14576689394072373
iteration : 13701
train acc:  0.921875
train loss:  0.24698054790496826
train gradient:  0.09080973706356511
iteration : 13702
train acc:  0.8359375
train loss:  0.31968116760253906
train gradient:  0.10347349519009685
iteration : 13703
train acc:  0.890625
train loss:  0.3082900047302246
train gradient:  0.13009342843539345
iteration : 13704
train acc:  0.890625
train loss:  0.25142765045166016
train gradient:  0.08900124894049923
iteration : 13705
train acc:  0.875
train loss:  0.29301032423973083
train gradient:  0.10030229060731034
iteration : 13706
train acc:  0.859375
train loss:  0.34179916977882385
train gradient:  0.22048144452759405
iteration : 13707
train acc:  0.828125
train loss:  0.32827961444854736
train gradient:  0.1278011845434463
iteration : 13708
train acc:  0.8671875
train loss:  0.2695411145687103
train gradient:  0.14871764590682876
iteration : 13709
train acc:  0.8515625
train loss:  0.35490983724594116
train gradient:  0.19568245105884013
iteration : 13710
train acc:  0.8671875
train loss:  0.26287946105003357
train gradient:  0.09113431140727951
iteration : 13711
train acc:  0.8828125
train loss:  0.29356664419174194
train gradient:  0.09572636090201131
iteration : 13712
train acc:  0.8828125
train loss:  0.26479196548461914
train gradient:  0.10733142448548952
iteration : 13713
train acc:  0.8515625
train loss:  0.32362914085388184
train gradient:  0.1383407105119544
iteration : 13714
train acc:  0.8828125
train loss:  0.35378995537757874
train gradient:  0.1546268915450059
iteration : 13715
train acc:  0.875
train loss:  0.27731141448020935
train gradient:  0.11300945861737395
iteration : 13716
train acc:  0.84375
train loss:  0.3273916244506836
train gradient:  0.12235282578936918
iteration : 13717
train acc:  0.8359375
train loss:  0.3008368909358978
train gradient:  0.14626331906462575
iteration : 13718
train acc:  0.8828125
train loss:  0.2785913944244385
train gradient:  0.14836506844091568
iteration : 13719
train acc:  0.8203125
train loss:  0.355069637298584
train gradient:  0.1723570848344433
iteration : 13720
train acc:  0.890625
train loss:  0.253303587436676
train gradient:  0.10137629782975774
iteration : 13721
train acc:  0.8984375
train loss:  0.2861507534980774
train gradient:  0.0960926046135032
iteration : 13722
train acc:  0.8125
train loss:  0.3826701045036316
train gradient:  0.13250678625584136
iteration : 13723
train acc:  0.859375
train loss:  0.4294010102748871
train gradient:  0.2519971354786255
iteration : 13724
train acc:  0.875
train loss:  0.27497321367263794
train gradient:  0.08661127762779774
iteration : 13725
train acc:  0.890625
train loss:  0.238372802734375
train gradient:  0.07866621557024339
iteration : 13726
train acc:  0.8984375
train loss:  0.24852214753627777
train gradient:  0.10048936421766866
iteration : 13727
train acc:  0.90625
train loss:  0.28086477518081665
train gradient:  0.08203476828920647
iteration : 13728
train acc:  0.8671875
train loss:  0.29316049814224243
train gradient:  0.19420194306384342
iteration : 13729
train acc:  0.8359375
train loss:  0.3311319649219513
train gradient:  0.2807602255951285
iteration : 13730
train acc:  0.8671875
train loss:  0.27325865626335144
train gradient:  0.10112770830359609
iteration : 13731
train acc:  0.8359375
train loss:  0.45001158118247986
train gradient:  0.2099180858670006
iteration : 13732
train acc:  0.890625
train loss:  0.28952640295028687
train gradient:  0.11612418653145977
iteration : 13733
train acc:  0.859375
train loss:  0.3404032289981842
train gradient:  0.13572310761959866
iteration : 13734
train acc:  0.8828125
train loss:  0.28001806139945984
train gradient:  0.1002792187376368
iteration : 13735
train acc:  0.890625
train loss:  0.33661046624183655
train gradient:  0.14835775546722016
iteration : 13736
train acc:  0.875
train loss:  0.3139813244342804
train gradient:  0.186463185740735
iteration : 13737
train acc:  0.8828125
train loss:  0.2741747796535492
train gradient:  0.11013341820135929
iteration : 13738
train acc:  0.8671875
train loss:  0.3293282389640808
train gradient:  0.11673894172027591
iteration : 13739
train acc:  0.890625
train loss:  0.28667372465133667
train gradient:  0.10826018899900437
iteration : 13740
train acc:  0.8125
train loss:  0.40302351117134094
train gradient:  0.17223821834543718
iteration : 13741
train acc:  0.8515625
train loss:  0.34368765354156494
train gradient:  0.2050326379191862
iteration : 13742
train acc:  0.9140625
train loss:  0.24704360961914062
train gradient:  0.09852519641298113
iteration : 13743
train acc:  0.859375
train loss:  0.28453630208969116
train gradient:  0.11316089424005395
iteration : 13744
train acc:  0.8515625
train loss:  0.37797605991363525
train gradient:  0.14778135947986082
iteration : 13745
train acc:  0.8984375
train loss:  0.2328518182039261
train gradient:  0.10389476369544784
iteration : 13746
train acc:  0.859375
train loss:  0.3110378682613373
train gradient:  0.16378900492635545
iteration : 13747
train acc:  0.8515625
train loss:  0.2766229212284088
train gradient:  0.09677209155226457
iteration : 13748
train acc:  0.875
train loss:  0.30913788080215454
train gradient:  0.24496535789854146
iteration : 13749
train acc:  0.8515625
train loss:  0.40375202894210815
train gradient:  0.19970949094214804
iteration : 13750
train acc:  0.8359375
train loss:  0.38586777448654175
train gradient:  0.20269330166001343
iteration : 13751
train acc:  0.859375
train loss:  0.3655620813369751
train gradient:  0.19661829509397044
iteration : 13752
train acc:  0.8515625
train loss:  0.31015947461128235
train gradient:  0.10898691630335915
iteration : 13753
train acc:  0.8046875
train loss:  0.3811008334159851
train gradient:  0.18226944486883673
iteration : 13754
train acc:  0.859375
train loss:  0.31791216135025024
train gradient:  0.13369135068004445
iteration : 13755
train acc:  0.875
train loss:  0.33573585748672485
train gradient:  0.17550039026977157
iteration : 13756
train acc:  0.8828125
train loss:  0.30889660120010376
train gradient:  0.16194505333037698
iteration : 13757
train acc:  0.8125
train loss:  0.37762853503227234
train gradient:  0.25684084528850765
iteration : 13758
train acc:  0.875
train loss:  0.2902924120426178
train gradient:  0.11865481476358263
iteration : 13759
train acc:  0.859375
train loss:  0.3620222508907318
train gradient:  0.20086838867099455
iteration : 13760
train acc:  0.84375
train loss:  0.32744094729423523
train gradient:  0.1701671899524016
iteration : 13761
train acc:  0.8828125
train loss:  0.2574506402015686
train gradient:  0.10725462522953148
iteration : 13762
train acc:  0.8671875
train loss:  0.3581688404083252
train gradient:  0.169354201753871
iteration : 13763
train acc:  0.90625
train loss:  0.2713279724121094
train gradient:  0.13048872223816263
iteration : 13764
train acc:  0.8359375
train loss:  0.31635794043540955
train gradient:  0.14427053111085758
iteration : 13765
train acc:  0.8828125
train loss:  0.3440503478050232
train gradient:  0.1464631577205024
iteration : 13766
train acc:  0.8828125
train loss:  0.25198376178741455
train gradient:  0.07959652812776281
iteration : 13767
train acc:  0.8515625
train loss:  0.3154129385948181
train gradient:  0.10859279693095021
iteration : 13768
train acc:  0.8828125
train loss:  0.2789921462535858
train gradient:  0.1186346092767909
iteration : 13769
train acc:  0.84375
train loss:  0.33531254529953003
train gradient:  0.2131517520453196
iteration : 13770
train acc:  0.84375
train loss:  0.3200575113296509
train gradient:  0.12810883571635306
iteration : 13771
train acc:  0.9140625
train loss:  0.23085442185401917
train gradient:  0.13793705982401766
iteration : 13772
train acc:  0.8671875
train loss:  0.2549261152744293
train gradient:  0.08482379658704932
iteration : 13773
train acc:  0.84375
train loss:  0.33612391352653503
train gradient:  0.15443169857583078
iteration : 13774
train acc:  0.8828125
train loss:  0.288875937461853
train gradient:  0.10811964272617734
iteration : 13775
train acc:  0.875
train loss:  0.28869500756263733
train gradient:  0.10694359579942646
iteration : 13776
train acc:  0.828125
train loss:  0.3933364748954773
train gradient:  0.28788376539987476
iteration : 13777
train acc:  0.8359375
train loss:  0.3294926881790161
train gradient:  0.20711298682091442
iteration : 13778
train acc:  0.921875
train loss:  0.2278362512588501
train gradient:  0.07452350406918257
iteration : 13779
train acc:  0.8203125
train loss:  0.4073062539100647
train gradient:  0.2515337867755922
iteration : 13780
train acc:  0.828125
train loss:  0.3615441620349884
train gradient:  0.22043352432102173
iteration : 13781
train acc:  0.8359375
train loss:  0.3786313533782959
train gradient:  0.17280081719743506
iteration : 13782
train acc:  0.8671875
train loss:  0.29224663972854614
train gradient:  0.14386818742405838
iteration : 13783
train acc:  0.8671875
train loss:  0.30219727754592896
train gradient:  0.15981889993995418
iteration : 13784
train acc:  0.8046875
train loss:  0.41596299409866333
train gradient:  0.22918117792406195
iteration : 13785
train acc:  0.8828125
train loss:  0.28819432854652405
train gradient:  0.12925338217539076
iteration : 13786
train acc:  0.8671875
train loss:  0.3451673984527588
train gradient:  0.1543536097052259
iteration : 13787
train acc:  0.890625
train loss:  0.22645336389541626
train gradient:  0.07172249081589398
iteration : 13788
train acc:  0.875
train loss:  0.2929549217224121
train gradient:  0.1512116642384222
iteration : 13789
train acc:  0.890625
train loss:  0.2708905339241028
train gradient:  0.10062053456869681
iteration : 13790
train acc:  0.8984375
train loss:  0.2479175329208374
train gradient:  0.1357309517022524
iteration : 13791
train acc:  0.8984375
train loss:  0.2750956118106842
train gradient:  0.1469370743489795
iteration : 13792
train acc:  0.8828125
train loss:  0.2677914798259735
train gradient:  0.1041847507177262
iteration : 13793
train acc:  0.90625
train loss:  0.2735644280910492
train gradient:  0.126779424083987
iteration : 13794
train acc:  0.8984375
train loss:  0.2970510423183441
train gradient:  0.18085038149771088
iteration : 13795
train acc:  0.8828125
train loss:  0.27953922748565674
train gradient:  0.12999006965718693
iteration : 13796
train acc:  0.8359375
train loss:  0.35317474603652954
train gradient:  0.13210671991919176
iteration : 13797
train acc:  0.84375
train loss:  0.35650500655174255
train gradient:  0.2077733169867939
iteration : 13798
train acc:  0.8828125
train loss:  0.27998316287994385
train gradient:  0.11388171689961296
iteration : 13799
train acc:  0.875
train loss:  0.26793521642684937
train gradient:  0.14603622027079569
iteration : 13800
train acc:  0.8515625
train loss:  0.35130178928375244
train gradient:  0.13181117682891513
iteration : 13801
train acc:  0.8515625
train loss:  0.29656022787094116
train gradient:  0.12110488177695218
iteration : 13802
train acc:  0.8671875
train loss:  0.25902074575424194
train gradient:  0.11810923008717508
iteration : 13803
train acc:  0.859375
train loss:  0.3102579116821289
train gradient:  0.14202106542059834
iteration : 13804
train acc:  0.8515625
train loss:  0.30275586247444153
train gradient:  0.155281493033457
iteration : 13805
train acc:  0.875
train loss:  0.2838776707649231
train gradient:  0.09977619674748799
iteration : 13806
train acc:  0.8359375
train loss:  0.35791367292404175
train gradient:  0.21162619390103038
iteration : 13807
train acc:  0.8984375
train loss:  0.2516925632953644
train gradient:  0.08314764661556644
iteration : 13808
train acc:  0.78125
train loss:  0.39035695791244507
train gradient:  0.15027751692181568
iteration : 13809
train acc:  0.84375
train loss:  0.3533063530921936
train gradient:  0.21908043580720712
iteration : 13810
train acc:  0.8515625
train loss:  0.3761310279369354
train gradient:  0.1615606276617032
iteration : 13811
train acc:  0.8984375
train loss:  0.2669000029563904
train gradient:  0.11141117650572314
iteration : 13812
train acc:  0.84375
train loss:  0.3708709478378296
train gradient:  0.24318509944684813
iteration : 13813
train acc:  0.9375
train loss:  0.2284597009420395
train gradient:  0.0784871070027247
iteration : 13814
train acc:  0.8515625
train loss:  0.3036515712738037
train gradient:  0.15084243551133986
iteration : 13815
train acc:  0.84375
train loss:  0.3860679864883423
train gradient:  0.19659674290060564
iteration : 13816
train acc:  0.875
train loss:  0.2899756133556366
train gradient:  0.12219606015122804
iteration : 13817
train acc:  0.890625
train loss:  0.295172780752182
train gradient:  0.10723365202180077
iteration : 13818
train acc:  0.828125
train loss:  0.33806222677230835
train gradient:  0.1404479970911981
iteration : 13819
train acc:  0.8671875
train loss:  0.3264394998550415
train gradient:  0.12749185509080405
iteration : 13820
train acc:  0.859375
train loss:  0.3245598375797272
train gradient:  0.13550727677211868
iteration : 13821
train acc:  0.90625
train loss:  0.2530372142791748
train gradient:  0.09515566059901703
iteration : 13822
train acc:  0.859375
train loss:  0.34261804819107056
train gradient:  0.21388034757405533
iteration : 13823
train acc:  0.8203125
train loss:  0.352685809135437
train gradient:  0.13986593490263097
iteration : 13824
train acc:  0.890625
train loss:  0.3001132011413574
train gradient:  0.1409503163265274
iteration : 13825
train acc:  0.8828125
train loss:  0.28991541266441345
train gradient:  0.0956721824382673
iteration : 13826
train acc:  0.921875
train loss:  0.27023398876190186
train gradient:  0.11674849206825468
iteration : 13827
train acc:  0.8046875
train loss:  0.4034627377986908
train gradient:  0.23065957939261728
iteration : 13828
train acc:  0.875
train loss:  0.2779618501663208
train gradient:  0.11403490241057077
iteration : 13829
train acc:  0.859375
train loss:  0.34295591711997986
train gradient:  0.1482718970308825
iteration : 13830
train acc:  0.84375
train loss:  0.38982877135276794
train gradient:  0.17305038043436277
iteration : 13831
train acc:  0.859375
train loss:  0.3412594795227051
train gradient:  0.14904003849729236
iteration : 13832
train acc:  0.8359375
train loss:  0.3277098834514618
train gradient:  0.12237952866903345
iteration : 13833
train acc:  0.8515625
train loss:  0.3596304655075073
train gradient:  0.17967815161825385
iteration : 13834
train acc:  0.8984375
train loss:  0.2801630198955536
train gradient:  0.17003385076179822
iteration : 13835
train acc:  0.8515625
train loss:  0.2911524474620819
train gradient:  0.11311945463596759
iteration : 13836
train acc:  0.890625
train loss:  0.29985368251800537
train gradient:  0.09012786423359802
iteration : 13837
train acc:  0.9140625
train loss:  0.22688257694244385
train gradient:  0.10795167079924213
iteration : 13838
train acc:  0.84375
train loss:  0.3787757158279419
train gradient:  0.1636400754186015
iteration : 13839
train acc:  0.8671875
train loss:  0.3028114140033722
train gradient:  0.15988510751351598
iteration : 13840
train acc:  0.8828125
train loss:  0.2938174605369568
train gradient:  0.1070021863689771
iteration : 13841
train acc:  0.8828125
train loss:  0.26246362924575806
train gradient:  0.12441123398969942
iteration : 13842
train acc:  0.8828125
train loss:  0.29115551710128784
train gradient:  0.1255821444327269
iteration : 13843
train acc:  0.78125
train loss:  0.36283230781555176
train gradient:  0.14643978981991945
iteration : 13844
train acc:  0.8515625
train loss:  0.3257503807544708
train gradient:  0.1083734900164833
iteration : 13845
train acc:  0.8984375
train loss:  0.2594805061817169
train gradient:  0.13540243270459162
iteration : 13846
train acc:  0.84375
train loss:  0.37033793330192566
train gradient:  0.20968042594461933
iteration : 13847
train acc:  0.8984375
train loss:  0.2860513925552368
train gradient:  0.10014421071053195
iteration : 13848
train acc:  0.8359375
train loss:  0.3263859748840332
train gradient:  0.13745666514719737
iteration : 13849
train acc:  0.796875
train loss:  0.39707237482070923
train gradient:  0.2516259916603536
iteration : 13850
train acc:  0.859375
train loss:  0.3448781371116638
train gradient:  0.16317831490299717
iteration : 13851
train acc:  0.8515625
train loss:  0.4165392518043518
train gradient:  0.19753512518116406
iteration : 13852
train acc:  0.8203125
train loss:  0.38995882868766785
train gradient:  0.18761835004747793
iteration : 13853
train acc:  0.8359375
train loss:  0.39260557293891907
train gradient:  0.1862594462860065
iteration : 13854
train acc:  0.8515625
train loss:  0.3053444027900696
train gradient:  0.14121721705523124
iteration : 13855
train acc:  0.875
train loss:  0.254261314868927
train gradient:  0.09226058032565246
iteration : 13856
train acc:  0.859375
train loss:  0.3269812762737274
train gradient:  0.13475095234028628
iteration : 13857
train acc:  0.828125
train loss:  0.40880194306373596
train gradient:  0.22072216192155603
iteration : 13858
train acc:  0.8984375
train loss:  0.2575637698173523
train gradient:  0.08470811348806691
iteration : 13859
train acc:  0.890625
train loss:  0.36083394289016724
train gradient:  0.32320382548016297
iteration : 13860
train acc:  0.875
train loss:  0.2729451656341553
train gradient:  0.10835808203072872
iteration : 13861
train acc:  0.84375
train loss:  0.38791725039482117
train gradient:  0.15314060008243902
iteration : 13862
train acc:  0.8515625
train loss:  0.30293673276901245
train gradient:  0.1319028167567381
iteration : 13863
train acc:  0.8828125
train loss:  0.2975561022758484
train gradient:  0.10161130168197953
iteration : 13864
train acc:  0.9140625
train loss:  0.20787537097930908
train gradient:  0.08426254160533608
iteration : 13865
train acc:  0.8515625
train loss:  0.340497225522995
train gradient:  0.18903368683935873
iteration : 13866
train acc:  0.875
train loss:  0.2937975525856018
train gradient:  0.1273591061315893
iteration : 13867
train acc:  0.8828125
train loss:  0.2771938741207123
train gradient:  0.07686638930589368
iteration : 13868
train acc:  0.890625
train loss:  0.2754318118095398
train gradient:  0.14652464809505883
iteration : 13869
train acc:  0.8828125
train loss:  0.3327532410621643
train gradient:  0.11378104246510976
iteration : 13870
train acc:  0.8984375
train loss:  0.2578645348548889
train gradient:  0.10032254374454411
iteration : 13871
train acc:  0.8515625
train loss:  0.2799553871154785
train gradient:  0.12891594487474678
iteration : 13872
train acc:  0.8515625
train loss:  0.3993415832519531
train gradient:  0.16515705995078356
iteration : 13873
train acc:  0.8671875
train loss:  0.3317396640777588
train gradient:  0.13734174943261301
iteration : 13874
train acc:  0.8671875
train loss:  0.32402753829956055
train gradient:  0.17060010255373748
iteration : 13875
train acc:  0.8671875
train loss:  0.3338588774204254
train gradient:  0.15785766096337478
iteration : 13876
train acc:  0.8828125
train loss:  0.31998157501220703
train gradient:  0.17703882390860615
iteration : 13877
train acc:  0.84375
train loss:  0.33966195583343506
train gradient:  0.16962144187338213
iteration : 13878
train acc:  0.859375
train loss:  0.30724868178367615
train gradient:  0.1371531977054168
iteration : 13879
train acc:  0.8984375
train loss:  0.2773756980895996
train gradient:  0.1072033054994872
iteration : 13880
train acc:  0.90625
train loss:  0.2509157657623291
train gradient:  0.07938626890043875
iteration : 13881
train acc:  0.8515625
train loss:  0.3385327458381653
train gradient:  0.2037273979740614
iteration : 13882
train acc:  0.8515625
train loss:  0.3250967562198639
train gradient:  0.21511043148017167
iteration : 13883
train acc:  0.796875
train loss:  0.42573148012161255
train gradient:  0.15542749193689293
iteration : 13884
train acc:  0.875
train loss:  0.2958012819290161
train gradient:  0.11057041544616997
iteration : 13885
train acc:  0.875
train loss:  0.2964123785495758
train gradient:  0.07519840815905882
iteration : 13886
train acc:  0.8671875
train loss:  0.3480375409126282
train gradient:  0.13405378750361155
iteration : 13887
train acc:  0.84375
train loss:  0.29691997170448303
train gradient:  0.15199260266847708
iteration : 13888
train acc:  0.8671875
train loss:  0.2804033160209656
train gradient:  0.11966935012156152
iteration : 13889
train acc:  0.859375
train loss:  0.3217669725418091
train gradient:  0.1456789429597662
iteration : 13890
train acc:  0.8046875
train loss:  0.3896557092666626
train gradient:  0.20506090678398942
iteration : 13891
train acc:  0.8671875
train loss:  0.3363528251647949
train gradient:  0.13868073252026053
iteration : 13892
train acc:  0.8125
train loss:  0.36602580547332764
train gradient:  0.1628073197197937
iteration : 13893
train acc:  0.8984375
train loss:  0.2721303701400757
train gradient:  0.09133579574371473
iteration : 13894
train acc:  0.8671875
train loss:  0.2835009694099426
train gradient:  0.08324242525591659
iteration : 13895
train acc:  0.859375
train loss:  0.28737902641296387
train gradient:  0.1068200208530913
iteration : 13896
train acc:  0.90625
train loss:  0.262103796005249
train gradient:  0.11272477810339321
iteration : 13897
train acc:  0.90625
train loss:  0.2443426549434662
train gradient:  0.10168060474432185
iteration : 13898
train acc:  0.8515625
train loss:  0.3436443507671356
train gradient:  0.08592674875902363
iteration : 13899
train acc:  0.859375
train loss:  0.26710689067840576
train gradient:  0.126405274799322
iteration : 13900
train acc:  0.859375
train loss:  0.34698086977005005
train gradient:  0.1269607641923046
iteration : 13901
train acc:  0.875
train loss:  0.29613059759140015
train gradient:  0.09193001503808289
iteration : 13902
train acc:  0.90625
train loss:  0.2901410162448883
train gradient:  0.07326331601841389
iteration : 13903
train acc:  0.8828125
train loss:  0.2884368896484375
train gradient:  0.09936507298835102
iteration : 13904
train acc:  0.859375
train loss:  0.35963553190231323
train gradient:  0.16598549781603766
iteration : 13905
train acc:  0.84375
train loss:  0.3675713539123535
train gradient:  0.16448667635117425
iteration : 13906
train acc:  0.8828125
train loss:  0.3629578948020935
train gradient:  0.13183546523485795
iteration : 13907
train acc:  0.8828125
train loss:  0.2737978398799896
train gradient:  0.10413306790276533
iteration : 13908
train acc:  0.875
train loss:  0.2424047589302063
train gradient:  0.09614436988303167
iteration : 13909
train acc:  0.8203125
train loss:  0.34392160177230835
train gradient:  0.18303259949026818
iteration : 13910
train acc:  0.890625
train loss:  0.3188104033470154
train gradient:  0.13230798425018103
iteration : 13911
train acc:  0.8359375
train loss:  0.3249959647655487
train gradient:  0.12906410805442523
iteration : 13912
train acc:  0.921875
train loss:  0.2517671585083008
train gradient:  0.0943849396865363
iteration : 13913
train acc:  0.8671875
train loss:  0.2740277647972107
train gradient:  0.11415650855756433
iteration : 13914
train acc:  0.8125
train loss:  0.3538218140602112
train gradient:  0.1578662921471227
iteration : 13915
train acc:  0.8515625
train loss:  0.3124412000179291
train gradient:  0.13975009716786352
iteration : 13916
train acc:  0.890625
train loss:  0.34451889991760254
train gradient:  0.20369607956410984
iteration : 13917
train acc:  0.8671875
train loss:  0.29640400409698486
train gradient:  0.12340336749968635
iteration : 13918
train acc:  0.921875
train loss:  0.2696686387062073
train gradient:  0.08669542266550911
iteration : 13919
train acc:  0.8984375
train loss:  0.2532847821712494
train gradient:  0.0832145890559128
iteration : 13920
train acc:  0.8671875
train loss:  0.2824854552745819
train gradient:  0.126182408557941
iteration : 13921
train acc:  0.8984375
train loss:  0.29248616099357605
train gradient:  0.13927431479510646
iteration : 13922
train acc:  0.859375
train loss:  0.3122904300689697
train gradient:  0.09546127058260198
iteration : 13923
train acc:  0.8828125
train loss:  0.2812700867652893
train gradient:  0.08924613442840731
iteration : 13924
train acc:  0.90625
train loss:  0.2616606652736664
train gradient:  0.08918531205135968
iteration : 13925
train acc:  0.8203125
train loss:  0.3952749967575073
train gradient:  0.20310528943895162
iteration : 13926
train acc:  0.828125
train loss:  0.3674662113189697
train gradient:  0.12515270851359328
iteration : 13927
train acc:  0.8203125
train loss:  0.36684587597846985
train gradient:  0.1640220059495449
iteration : 13928
train acc:  0.875
train loss:  0.28223666548728943
train gradient:  0.14112503942295535
iteration : 13929
train acc:  0.875
train loss:  0.3152700662612915
train gradient:  0.17359405414752738
iteration : 13930
train acc:  0.8203125
train loss:  0.37799155712127686
train gradient:  0.1942206197073628
iteration : 13931
train acc:  0.8828125
train loss:  0.3196118175983429
train gradient:  0.11800261153961539
iteration : 13932
train acc:  0.890625
train loss:  0.2606442868709564
train gradient:  0.11554440001226375
iteration : 13933
train acc:  0.8828125
train loss:  0.30153098702430725
train gradient:  0.13986523950492813
iteration : 13934
train acc:  0.8671875
train loss:  0.26586928963661194
train gradient:  0.08898923780124085
iteration : 13935
train acc:  0.875
train loss:  0.30160701274871826
train gradient:  0.1334517717440976
iteration : 13936
train acc:  0.8671875
train loss:  0.33102715015411377
train gradient:  0.17809696816033538
iteration : 13937
train acc:  0.8984375
train loss:  0.23738788068294525
train gradient:  0.09949542928199273
iteration : 13938
train acc:  0.8515625
train loss:  0.3376620411872864
train gradient:  0.17393937908584303
iteration : 13939
train acc:  0.8515625
train loss:  0.38130176067352295
train gradient:  0.2207096470116576
iteration : 13940
train acc:  0.8125
train loss:  0.37831661105155945
train gradient:  0.21678766511712658
iteration : 13941
train acc:  0.890625
train loss:  0.2991025447845459
train gradient:  0.11902068779957893
iteration : 13942
train acc:  0.8359375
train loss:  0.3888150453567505
train gradient:  0.1887231894252379
iteration : 13943
train acc:  0.875
train loss:  0.2921575903892517
train gradient:  0.1465698038162004
iteration : 13944
train acc:  0.8828125
train loss:  0.2886981964111328
train gradient:  0.17944487357796987
iteration : 13945
train acc:  0.890625
train loss:  0.25803425908088684
train gradient:  0.11151043734231236
iteration : 13946
train acc:  0.8828125
train loss:  0.25386178493499756
train gradient:  0.0853646255385099
iteration : 13947
train acc:  0.8671875
train loss:  0.3593716621398926
train gradient:  0.17091653924845437
iteration : 13948
train acc:  0.8671875
train loss:  0.2738155126571655
train gradient:  0.09414097034872441
iteration : 13949
train acc:  0.8046875
train loss:  0.3979748487472534
train gradient:  0.2201165808964993
iteration : 13950
train acc:  0.8671875
train loss:  0.3166270852088928
train gradient:  0.09246765822344631
iteration : 13951
train acc:  0.8671875
train loss:  0.29833129048347473
train gradient:  0.10374374038587195
iteration : 13952
train acc:  0.875
train loss:  0.27295660972595215
train gradient:  0.06721070809512986
iteration : 13953
train acc:  0.875
train loss:  0.34024906158447266
train gradient:  0.15314539186192397
iteration : 13954
train acc:  0.8828125
train loss:  0.28199589252471924
train gradient:  0.0947404902327774
iteration : 13955
train acc:  0.890625
train loss:  0.2664012908935547
train gradient:  0.07649033819907673
iteration : 13956
train acc:  0.8828125
train loss:  0.2894468903541565
train gradient:  0.13376457040157053
iteration : 13957
train acc:  0.875
train loss:  0.36154410243034363
train gradient:  0.17161291308039966
iteration : 13958
train acc:  0.8359375
train loss:  0.3849698603153229
train gradient:  0.1386958873614717
iteration : 13959
train acc:  0.875
train loss:  0.3298090696334839
train gradient:  0.13916427068417186
iteration : 13960
train acc:  0.8828125
train loss:  0.28727149963378906
train gradient:  0.11569719680449425
iteration : 13961
train acc:  0.8984375
train loss:  0.23998001217842102
train gradient:  0.10387762666689083
iteration : 13962
train acc:  0.890625
train loss:  0.28682267665863037
train gradient:  0.11669542618769754
iteration : 13963
train acc:  0.8984375
train loss:  0.26429465413093567
train gradient:  0.11528556625264233
iteration : 13964
train acc:  0.796875
train loss:  0.43286144733428955
train gradient:  0.20720702224648418
iteration : 13965
train acc:  0.8515625
train loss:  0.3215808868408203
train gradient:  0.1467742616151135
iteration : 13966
train acc:  0.8828125
train loss:  0.3336108326911926
train gradient:  0.1601625812861945
iteration : 13967
train acc:  0.8359375
train loss:  0.34504956007003784
train gradient:  0.13757447222908412
iteration : 13968
train acc:  0.8828125
train loss:  0.292045533657074
train gradient:  0.08864211808029235
iteration : 13969
train acc:  0.828125
train loss:  0.35289645195007324
train gradient:  0.11875140242200366
iteration : 13970
train acc:  0.8671875
train loss:  0.3217158913612366
train gradient:  0.1345493064839325
iteration : 13971
train acc:  0.875
train loss:  0.36332711577415466
train gradient:  0.19774485819456133
iteration : 13972
train acc:  0.828125
train loss:  0.3489101231098175
train gradient:  0.13215985255919147
iteration : 13973
train acc:  0.875
train loss:  0.28959739208221436
train gradient:  0.11733504627661691
iteration : 13974
train acc:  0.875
train loss:  0.33197686076164246
train gradient:  0.16176963828768082
iteration : 13975
train acc:  0.859375
train loss:  0.3111727237701416
train gradient:  0.1487346102937136
iteration : 13976
train acc:  0.8125
train loss:  0.398740291595459
train gradient:  0.20663023359427
iteration : 13977
train acc:  0.8828125
train loss:  0.2943986654281616
train gradient:  0.09240761782370295
iteration : 13978
train acc:  0.84375
train loss:  0.33578482270240784
train gradient:  0.10021515018423006
iteration : 13979
train acc:  0.9140625
train loss:  0.2605159282684326
train gradient:  0.20991258917706418
iteration : 13980
train acc:  0.8984375
train loss:  0.29186394810676575
train gradient:  0.07747641854301826
iteration : 13981
train acc:  0.859375
train loss:  0.32608211040496826
train gradient:  0.1057528300252479
iteration : 13982
train acc:  0.875
train loss:  0.3241502046585083
train gradient:  0.13542300041576258
iteration : 13983
train acc:  0.90625
train loss:  0.2663920819759369
train gradient:  0.0848942644179496
iteration : 13984
train acc:  0.84375
train loss:  0.33190035820007324
train gradient:  0.13851943825882201
iteration : 13985
train acc:  0.8828125
train loss:  0.28750962018966675
train gradient:  0.1258409058912211
iteration : 13986
train acc:  0.8515625
train loss:  0.317883163690567
train gradient:  0.13384282988464494
iteration : 13987
train acc:  0.8671875
train loss:  0.34010881185531616
train gradient:  0.12527893282695785
iteration : 13988
train acc:  0.875
train loss:  0.3095231056213379
train gradient:  0.1168119755862756
iteration : 13989
train acc:  0.8515625
train loss:  0.3242102861404419
train gradient:  0.13957294983521662
iteration : 13990
train acc:  0.875
train loss:  0.2645759582519531
train gradient:  0.07970490230465822
iteration : 13991
train acc:  0.90625
train loss:  0.21898266673088074
train gradient:  0.07179269700855578
iteration : 13992
train acc:  0.8515625
train loss:  0.32632917165756226
train gradient:  0.10658088597250621
iteration : 13993
train acc:  0.8671875
train loss:  0.2978501319885254
train gradient:  0.0937854390494492
iteration : 13994
train acc:  0.921875
train loss:  0.3054989278316498
train gradient:  0.08147951837640874
iteration : 13995
train acc:  0.890625
train loss:  0.3486208915710449
train gradient:  0.14171995434791784
iteration : 13996
train acc:  0.875
train loss:  0.2572246491909027
train gradient:  0.08831714113058685
iteration : 13997
train acc:  0.90625
train loss:  0.23336732387542725
train gradient:  0.05949114308623775
iteration : 13998
train acc:  0.8828125
train loss:  0.37335747480392456
train gradient:  0.1875339525900132
iteration : 13999
train acc:  0.8828125
train loss:  0.28943854570388794
train gradient:  0.1159331774207127
iteration : 14000
train acc:  0.8671875
train loss:  0.27393752336502075
train gradient:  0.10809047106519108
iteration : 14001
train acc:  0.8828125
train loss:  0.3213155269622803
train gradient:  0.0843981431943753
iteration : 14002
train acc:  0.921875
train loss:  0.21994777023792267
train gradient:  0.08675487185421223
iteration : 14003
train acc:  0.8515625
train loss:  0.33552438020706177
train gradient:  0.12252775930639344
iteration : 14004
train acc:  0.8125
train loss:  0.39826956391334534
train gradient:  0.18940373217283218
iteration : 14005
train acc:  0.8671875
train loss:  0.31423717737197876
train gradient:  0.11405128189247403
iteration : 14006
train acc:  0.875
train loss:  0.29893094301223755
train gradient:  0.11312400395753534
iteration : 14007
train acc:  0.90625
train loss:  0.26448309421539307
train gradient:  0.11706134661480612
iteration : 14008
train acc:  0.859375
train loss:  0.2922723591327667
train gradient:  0.1021353144299517
iteration : 14009
train acc:  0.890625
train loss:  0.26845061779022217
train gradient:  0.10350193046524879
iteration : 14010
train acc:  0.8984375
train loss:  0.23503908514976501
train gradient:  0.09610214185991371
iteration : 14011
train acc:  0.90625
train loss:  0.2311752438545227
train gradient:  0.09900813947043796
iteration : 14012
train acc:  0.890625
train loss:  0.2713273763656616
train gradient:  0.09440060978743353
iteration : 14013
train acc:  0.90625
train loss:  0.27329957485198975
train gradient:  0.08268332537047406
iteration : 14014
train acc:  0.8203125
train loss:  0.33323806524276733
train gradient:  0.1641258627739537
iteration : 14015
train acc:  0.765625
train loss:  0.4962642788887024
train gradient:  0.3538699034142793
iteration : 14016
train acc:  0.828125
train loss:  0.47323155403137207
train gradient:  0.23984267683175886
iteration : 14017
train acc:  0.859375
train loss:  0.3530512750148773
train gradient:  0.16207793624665645
iteration : 14018
train acc:  0.84375
train loss:  0.3521645963191986
train gradient:  0.1798683312226862
iteration : 14019
train acc:  0.8828125
train loss:  0.2869020104408264
train gradient:  0.13654241248910493
iteration : 14020
train acc:  0.875
train loss:  0.33474841713905334
train gradient:  0.13870726654074644
iteration : 14021
train acc:  0.90625
train loss:  0.2591569721698761
train gradient:  0.09877098700199564
iteration : 14022
train acc:  0.828125
train loss:  0.3134837746620178
train gradient:  0.18559871068768505
iteration : 14023
train acc:  0.8515625
train loss:  0.3513745665550232
train gradient:  0.1501290422849345
iteration : 14024
train acc:  0.890625
train loss:  0.23259633779525757
train gradient:  0.07483385157926643
iteration : 14025
train acc:  0.8828125
train loss:  0.28435638546943665
train gradient:  0.10116520082065254
iteration : 14026
train acc:  0.890625
train loss:  0.30053114891052246
train gradient:  0.14727667404413156
iteration : 14027
train acc:  0.9140625
train loss:  0.26956385374069214
train gradient:  0.0916885362906274
iteration : 14028
train acc:  0.828125
train loss:  0.3506421744823456
train gradient:  0.12875228416101725
iteration : 14029
train acc:  0.875
train loss:  0.2794145941734314
train gradient:  0.08360708060582359
iteration : 14030
train acc:  0.859375
train loss:  0.37557196617126465
train gradient:  0.1459202317798416
iteration : 14031
train acc:  0.8359375
train loss:  0.4321494400501251
train gradient:  0.18471808776757556
iteration : 14032
train acc:  0.8984375
train loss:  0.29256829619407654
train gradient:  0.0891530723766708
iteration : 14033
train acc:  0.8359375
train loss:  0.36235979199409485
train gradient:  0.11659143973297303
iteration : 14034
train acc:  0.8671875
train loss:  0.2590346336364746
train gradient:  0.16306954419429143
iteration : 14035
train acc:  0.828125
train loss:  0.37499624490737915
train gradient:  0.16602674494607877
iteration : 14036
train acc:  0.875
train loss:  0.2964514493942261
train gradient:  0.08829957986968781
iteration : 14037
train acc:  0.890625
train loss:  0.27626585960388184
train gradient:  0.09368288043562797
iteration : 14038
train acc:  0.875
train loss:  0.28246456384658813
train gradient:  0.10932166726073476
iteration : 14039
train acc:  0.859375
train loss:  0.3465043902397156
train gradient:  0.12852979740648843
iteration : 14040
train acc:  0.875
train loss:  0.2966259717941284
train gradient:  0.09657604490574706
iteration : 14041
train acc:  0.8671875
train loss:  0.31460434198379517
train gradient:  0.14442903880887226
iteration : 14042
train acc:  0.8671875
train loss:  0.2962099015712738
train gradient:  0.16574302647394712
iteration : 14043
train acc:  0.8203125
train loss:  0.33278709650039673
train gradient:  0.09222879275293529
iteration : 14044
train acc:  0.8984375
train loss:  0.29894739389419556
train gradient:  0.12233181465176042
iteration : 14045
train acc:  0.8515625
train loss:  0.2963446080684662
train gradient:  0.10396300821287797
iteration : 14046
train acc:  0.828125
train loss:  0.42159801721572876
train gradient:  0.21220623049357645
iteration : 14047
train acc:  0.8671875
train loss:  0.3696427643299103
train gradient:  0.17435872157804405
iteration : 14048
train acc:  0.859375
train loss:  0.33789363503456116
train gradient:  0.1449977338136525
iteration : 14049
train acc:  0.9140625
train loss:  0.2667543292045593
train gradient:  0.1676791580820289
iteration : 14050
train acc:  0.796875
train loss:  0.3768153488636017
train gradient:  0.18115902863202232
iteration : 14051
train acc:  0.8359375
train loss:  0.37138068675994873
train gradient:  0.17088364658936833
iteration : 14052
train acc:  0.8671875
train loss:  0.3279893100261688
train gradient:  0.12419916586367072
iteration : 14053
train acc:  0.84375
train loss:  0.3793291449546814
train gradient:  0.17201894461963255
iteration : 14054
train acc:  0.875
train loss:  0.2878614068031311
train gradient:  0.07778832924510891
iteration : 14055
train acc:  0.890625
train loss:  0.21612685918807983
train gradient:  0.11849724898989185
iteration : 14056
train acc:  0.8359375
train loss:  0.3569962978363037
train gradient:  0.18456013279424033
iteration : 14057
train acc:  0.8359375
train loss:  0.36046546697616577
train gradient:  0.1749683216742558
iteration : 14058
train acc:  0.890625
train loss:  0.2691118121147156
train gradient:  0.09235407205634463
iteration : 14059
train acc:  0.890625
train loss:  0.26954948902130127
train gradient:  0.10469634635943342
iteration : 14060
train acc:  0.8515625
train loss:  0.32083699107170105
train gradient:  0.16234234447925625
iteration : 14061
train acc:  0.8359375
train loss:  0.3405354619026184
train gradient:  0.19428014210712108
iteration : 14062
train acc:  0.8671875
train loss:  0.3237043619155884
train gradient:  0.12715614166502792
iteration : 14063
train acc:  0.8203125
train loss:  0.399300754070282
train gradient:  0.21750051416103727
iteration : 14064
train acc:  0.90625
train loss:  0.26960861682891846
train gradient:  0.0860336035098058
iteration : 14065
train acc:  0.8671875
train loss:  0.3342198431491852
train gradient:  0.08875441670688583
iteration : 14066
train acc:  0.8828125
train loss:  0.2926065921783447
train gradient:  0.10446004778735991
iteration : 14067
train acc:  0.8671875
train loss:  0.32093197107315063
train gradient:  0.13706202194267247
iteration : 14068
train acc:  0.8671875
train loss:  0.33381152153015137
train gradient:  0.13616106879925988
iteration : 14069
train acc:  0.828125
train loss:  0.34189146757125854
train gradient:  0.10967107840030049
iteration : 14070
train acc:  0.859375
train loss:  0.35987043380737305
train gradient:  0.14193742806427223
iteration : 14071
train acc:  0.875
train loss:  0.28026849031448364
train gradient:  0.13589502283604524
iteration : 14072
train acc:  0.8671875
train loss:  0.33413246273994446
train gradient:  0.19873208976451565
iteration : 14073
train acc:  0.90625
train loss:  0.23919013142585754
train gradient:  0.10548798584752327
iteration : 14074
train acc:  0.859375
train loss:  0.35149645805358887
train gradient:  0.1397764591274708
iteration : 14075
train acc:  0.8984375
train loss:  0.2684909403324127
train gradient:  0.11786961780775582
iteration : 14076
train acc:  0.921875
train loss:  0.235336035490036
train gradient:  0.08761334561267498
iteration : 14077
train acc:  0.8359375
train loss:  0.37746086716651917
train gradient:  0.16863215744546117
iteration : 14078
train acc:  0.8203125
train loss:  0.36915522813796997
train gradient:  0.16024696492539006
iteration : 14079
train acc:  0.859375
train loss:  0.3440166115760803
train gradient:  0.14459506855895804
iteration : 14080
train acc:  0.90625
train loss:  0.26451975107192993
train gradient:  0.11344494938757627
iteration : 14081
train acc:  0.8515625
train loss:  0.371148943901062
train gradient:  0.15730187924611796
iteration : 14082
train acc:  0.9140625
train loss:  0.2599208354949951
train gradient:  0.10356808338941041
iteration : 14083
train acc:  0.875
train loss:  0.35502755641937256
train gradient:  0.1274358336814244
iteration : 14084
train acc:  0.875
train loss:  0.24564054608345032
train gradient:  0.08146183562375608
iteration : 14085
train acc:  0.8984375
train loss:  0.21485696732997894
train gradient:  0.06637854342924634
iteration : 14086
train acc:  0.859375
train loss:  0.3138788938522339
train gradient:  0.11723920897348569
iteration : 14087
train acc:  0.9140625
train loss:  0.2415885031223297
train gradient:  0.07982239755344067
iteration : 14088
train acc:  0.8828125
train loss:  0.2612760066986084
train gradient:  0.11306499473474164
iteration : 14089
train acc:  0.8984375
train loss:  0.2866140604019165
train gradient:  0.1470049325827789
iteration : 14090
train acc:  0.8828125
train loss:  0.2890770435333252
train gradient:  0.11635230182204107
iteration : 14091
train acc:  0.78125
train loss:  0.49535566568374634
train gradient:  0.3874406220620562
iteration : 14092
train acc:  0.8671875
train loss:  0.3247595429420471
train gradient:  0.12075384920011614
iteration : 14093
train acc:  0.8515625
train loss:  0.3241785764694214
train gradient:  0.17793306817478893
iteration : 14094
train acc:  0.84375
train loss:  0.3106920123100281
train gradient:  0.14905225461536808
iteration : 14095
train acc:  0.8671875
train loss:  0.3108556270599365
train gradient:  0.10926261818624146
iteration : 14096
train acc:  0.90625
train loss:  0.23412078619003296
train gradient:  0.06385919894904797
iteration : 14097
train acc:  0.859375
train loss:  0.32038360834121704
train gradient:  0.1715833313070418
iteration : 14098
train acc:  0.8828125
train loss:  0.3113127648830414
train gradient:  0.11484333980423607
iteration : 14099
train acc:  0.8046875
train loss:  0.4539756774902344
train gradient:  0.2951137241864077
iteration : 14100
train acc:  0.875
train loss:  0.2904796004295349
train gradient:  0.12717378447140743
iteration : 14101
train acc:  0.8671875
train loss:  0.299333393573761
train gradient:  0.13948682824897987
iteration : 14102
train acc:  0.9296875
train loss:  0.22763682901859283
train gradient:  0.11753046132857721
iteration : 14103
train acc:  0.828125
train loss:  0.4225289523601532
train gradient:  0.1894209605158127
iteration : 14104
train acc:  0.8515625
train loss:  0.38460227847099304
train gradient:  0.15466856057384215
iteration : 14105
train acc:  0.8828125
train loss:  0.3438383638858795
train gradient:  0.16521387648562258
iteration : 14106
train acc:  0.890625
train loss:  0.3144039511680603
train gradient:  0.10377822743405143
iteration : 14107
train acc:  0.8671875
train loss:  0.29027801752090454
train gradient:  0.13137818536849757
iteration : 14108
train acc:  0.8984375
train loss:  0.30640909075737
train gradient:  0.11413638247116677
iteration : 14109
train acc:  0.84375
train loss:  0.39857017993927
train gradient:  0.18658918438150107
iteration : 14110
train acc:  0.8984375
train loss:  0.25658366084098816
train gradient:  0.12558790966799793
iteration : 14111
train acc:  0.890625
train loss:  0.2959614098072052
train gradient:  0.11409904180223815
iteration : 14112
train acc:  0.8984375
train loss:  0.2579909563064575
train gradient:  0.07930796597667636
iteration : 14113
train acc:  0.9375
train loss:  0.17859990894794464
train gradient:  0.07888929413215964
iteration : 14114
train acc:  0.8984375
train loss:  0.2635158598423004
train gradient:  0.09943005165726959
iteration : 14115
train acc:  0.8984375
train loss:  0.2909144163131714
train gradient:  0.13013122972256877
iteration : 14116
train acc:  0.8203125
train loss:  0.3722372055053711
train gradient:  0.20404718141875794
iteration : 14117
train acc:  0.8671875
train loss:  0.30079442262649536
train gradient:  0.1275635018699986
iteration : 14118
train acc:  0.8984375
train loss:  0.27360954880714417
train gradient:  0.07556084883716999
iteration : 14119
train acc:  0.8203125
train loss:  0.31940874457359314
train gradient:  0.13479700480686885
iteration : 14120
train acc:  0.8671875
train loss:  0.2860810160636902
train gradient:  0.13828388013484097
iteration : 14121
train acc:  0.8359375
train loss:  0.35144782066345215
train gradient:  0.23346031576832513
iteration : 14122
train acc:  0.84375
train loss:  0.4570111036300659
train gradient:  0.20767326261060481
iteration : 14123
train acc:  0.875
train loss:  0.3073623776435852
train gradient:  0.15689920581312583
iteration : 14124
train acc:  0.9140625
train loss:  0.2696821093559265
train gradient:  0.086481252127387
iteration : 14125
train acc:  0.8515625
train loss:  0.32778677344322205
train gradient:  0.17784067183206007
iteration : 14126
train acc:  0.8671875
train loss:  0.28624802827835083
train gradient:  0.1090275412438047
iteration : 14127
train acc:  0.8671875
train loss:  0.2681885361671448
train gradient:  0.1171080172412226
iteration : 14128
train acc:  0.8828125
train loss:  0.3319302797317505
train gradient:  0.2189152396002359
iteration : 14129
train acc:  0.8515625
train loss:  0.29121142625808716
train gradient:  0.12193536274312677
iteration : 14130
train acc:  0.7890625
train loss:  0.3777715265750885
train gradient:  0.22917183216899434
iteration : 14131
train acc:  0.8515625
train loss:  0.3432164192199707
train gradient:  0.19234552134050298
iteration : 14132
train acc:  0.9140625
train loss:  0.2392648607492447
train gradient:  0.08493250252800608
iteration : 14133
train acc:  0.890625
train loss:  0.29718661308288574
train gradient:  0.11240506617904733
iteration : 14134
train acc:  0.8359375
train loss:  0.30815622210502625
train gradient:  0.12145093830369504
iteration : 14135
train acc:  0.8671875
train loss:  0.29030129313468933
train gradient:  0.10421595562590835
iteration : 14136
train acc:  0.875
train loss:  0.2943955063819885
train gradient:  0.1167707483774929
iteration : 14137
train acc:  0.84375
train loss:  0.3330100476741791
train gradient:  0.20986312083681677
iteration : 14138
train acc:  0.875
train loss:  0.30384162068367004
train gradient:  0.12052975628963886
iteration : 14139
train acc:  0.8359375
train loss:  0.3198506832122803
train gradient:  0.12747603662163645
iteration : 14140
train acc:  0.8359375
train loss:  0.33383285999298096
train gradient:  0.10681787397035891
iteration : 14141
train acc:  0.8203125
train loss:  0.35623642802238464
train gradient:  0.15069403693814187
iteration : 14142
train acc:  0.828125
train loss:  0.3994792401790619
train gradient:  0.20189659556142425
iteration : 14143
train acc:  0.84375
train loss:  0.35908079147338867
train gradient:  0.23673017156158527
iteration : 14144
train acc:  0.8203125
train loss:  0.34418895840644836
train gradient:  0.13804832929159835
iteration : 14145
train acc:  0.890625
train loss:  0.3021923303604126
train gradient:  0.1937675846545777
iteration : 14146
train acc:  0.8984375
train loss:  0.30476370453834534
train gradient:  0.12205851794364102
iteration : 14147
train acc:  0.875
train loss:  0.2934107184410095
train gradient:  0.1684182352090216
iteration : 14148
train acc:  0.8046875
train loss:  0.33086442947387695
train gradient:  0.16052667917358954
iteration : 14149
train acc:  0.8671875
train loss:  0.3138604164123535
train gradient:  0.17025769092992937
iteration : 14150
train acc:  0.828125
train loss:  0.312660276889801
train gradient:  0.1170636602164176
iteration : 14151
train acc:  0.8515625
train loss:  0.26162993907928467
train gradient:  0.12488128424481569
iteration : 14152
train acc:  0.8359375
train loss:  0.33176761865615845
train gradient:  0.14513182449765497
iteration : 14153
train acc:  0.84375
train loss:  0.3429937958717346
train gradient:  0.16599903552036988
iteration : 14154
train acc:  0.8359375
train loss:  0.36263322830200195
train gradient:  0.19426521685783704
iteration : 14155
train acc:  0.828125
train loss:  0.3592117428779602
train gradient:  0.18751409786327947
iteration : 14156
train acc:  0.8125
train loss:  0.33015498518943787
train gradient:  0.13214251848194103
iteration : 14157
train acc:  0.828125
train loss:  0.3243778944015503
train gradient:  0.13098297731437278
iteration : 14158
train acc:  0.90625
train loss:  0.2696724832057953
train gradient:  0.12905530200661286
iteration : 14159
train acc:  0.8515625
train loss:  0.3335058093070984
train gradient:  0.11947315226540904
iteration : 14160
train acc:  0.8515625
train loss:  0.30288344621658325
train gradient:  0.1153876068255202
iteration : 14161
train acc:  0.8203125
train loss:  0.39446383714675903
train gradient:  0.213320791813785
iteration : 14162
train acc:  0.859375
train loss:  0.31832242012023926
train gradient:  0.17861882725998368
iteration : 14163
train acc:  0.9296875
train loss:  0.2075764536857605
train gradient:  0.0912630539946681
iteration : 14164
train acc:  0.828125
train loss:  0.35743454098701477
train gradient:  0.13855875885721997
iteration : 14165
train acc:  0.8515625
train loss:  0.3600369691848755
train gradient:  0.20671314744135924
iteration : 14166
train acc:  0.875
train loss:  0.26929864287376404
train gradient:  0.08631186806106951
iteration : 14167
train acc:  0.8671875
train loss:  0.3338393270969391
train gradient:  0.14252420416357753
iteration : 14168
train acc:  0.8671875
train loss:  0.32661837339401245
train gradient:  0.14495133525945514
iteration : 14169
train acc:  0.8984375
train loss:  0.29472777247428894
train gradient:  0.13691381012566983
iteration : 14170
train acc:  0.90625
train loss:  0.300577312707901
train gradient:  0.17459921081173144
iteration : 14171
train acc:  0.859375
train loss:  0.32474976778030396
train gradient:  0.11517140468219157
iteration : 14172
train acc:  0.8828125
train loss:  0.27808526158332825
train gradient:  0.08003294427432736
iteration : 14173
train acc:  0.8828125
train loss:  0.28874072432518005
train gradient:  0.13723076173124304
iteration : 14174
train acc:  0.8203125
train loss:  0.34852999448776245
train gradient:  0.1943601658174629
iteration : 14175
train acc:  0.875
train loss:  0.2695423662662506
train gradient:  0.10338399698885109
iteration : 14176
train acc:  0.8984375
train loss:  0.29640993475914
train gradient:  0.08673329011022153
iteration : 14177
train acc:  0.8515625
train loss:  0.3046511709690094
train gradient:  0.09348899409610704
iteration : 14178
train acc:  0.875
train loss:  0.33144938945770264
train gradient:  0.11483709843574252
iteration : 14179
train acc:  0.859375
train loss:  0.3120417892932892
train gradient:  0.10027648580024305
iteration : 14180
train acc:  0.9140625
train loss:  0.23896938562393188
train gradient:  0.0912596513430568
iteration : 14181
train acc:  0.9296875
train loss:  0.23907384276390076
train gradient:  0.09420021105393019
iteration : 14182
train acc:  0.8828125
train loss:  0.28975558280944824
train gradient:  0.1441963048196589
iteration : 14183
train acc:  0.859375
train loss:  0.27745193243026733
train gradient:  0.09812716313521982
iteration : 14184
train acc:  0.90625
train loss:  0.30905643105506897
train gradient:  0.10825100465094463
iteration : 14185
train acc:  0.7890625
train loss:  0.45613712072372437
train gradient:  0.34466393914659
iteration : 14186
train acc:  0.7777777777777778
train loss:  0.4180574417114258
train gradient:  0.675018645940717
val acc:  0.8687607747692931
val f1:  0.8678515084523365
val confusion matrix:  [[86347 12263]
 [13620 84990]]

----------------------------------------new_epoch--------------------------------------

epoch:  1
iteration : 0
train acc:  0.84375
train loss:  0.34916409850120544
train gradient:  0.14178100218655837
iteration : 1
train acc:  0.84375
train loss:  0.28085535764694214
train gradient:  0.1697372959135757
iteration : 2
train acc:  0.8984375
train loss:  0.2710641920566559
train gradient:  0.08061350766469148
iteration : 3
train acc:  0.8515625
train loss:  0.39247819781303406
train gradient:  0.16069077069440602
iteration : 4
train acc:  0.9140625
train loss:  0.22829589247703552
train gradient:  0.07757782758720509
iteration : 5
train acc:  0.8359375
train loss:  0.3481317460536957
train gradient:  0.15512567030791974
iteration : 6
train acc:  0.8359375
train loss:  0.3827688694000244
train gradient:  0.2625215831971874
iteration : 7
train acc:  0.84375
train loss:  0.29967978596687317
train gradient:  0.10671811905521834
iteration : 8
train acc:  0.7890625
train loss:  0.4393545985221863
train gradient:  0.17101419627704778
iteration : 9
train acc:  0.8828125
train loss:  0.25916218757629395
train gradient:  0.08646421699112575
iteration : 10
train acc:  0.859375
train loss:  0.32239410281181335
train gradient:  0.18707795569757316
iteration : 11
train acc:  0.8828125
train loss:  0.3036532402038574
train gradient:  0.1352421928575661
iteration : 12
train acc:  0.875
train loss:  0.33815035223960876
train gradient:  0.10393122308908215
iteration : 13
train acc:  0.90625
train loss:  0.26032814383506775
train gradient:  0.08646727705486301
iteration : 14
train acc:  0.78125
train loss:  0.39050960540771484
train gradient:  0.24902224225586844
iteration : 15
train acc:  0.8359375
train loss:  0.36226147413253784
train gradient:  0.16349663894636846
iteration : 16
train acc:  0.828125
train loss:  0.4229593873023987
train gradient:  0.2489327688045305
iteration : 17
train acc:  0.8671875
train loss:  0.2925587296485901
train gradient:  0.10918181504034592
iteration : 18
train acc:  0.84375
train loss:  0.3427729606628418
train gradient:  0.22091514801863266
iteration : 19
train acc:  0.8203125
train loss:  0.34204888343811035
train gradient:  0.1363938877775745
iteration : 20
train acc:  0.890625
train loss:  0.2609792947769165
train gradient:  0.09935343246067788
iteration : 21
train acc:  0.875
train loss:  0.3409537076950073
train gradient:  0.1793221583182404
iteration : 22
train acc:  0.828125
train loss:  0.32768142223358154
train gradient:  0.13475017064648293
iteration : 23
train acc:  0.875
train loss:  0.294364333152771
train gradient:  0.10095202943853254
iteration : 24
train acc:  0.875
train loss:  0.2754133343696594
train gradient:  0.1055776004229342
iteration : 25
train acc:  0.890625
train loss:  0.23463761806488037
train gradient:  0.08492732276016113
iteration : 26
train acc:  0.8671875
train loss:  0.2848466634750366
train gradient:  0.15459431450293856
iteration : 27
train acc:  0.8828125
train loss:  0.287092387676239
train gradient:  0.10138751267896019
iteration : 28
train acc:  0.84375
train loss:  0.3319548964500427
train gradient:  0.12527415219421353
iteration : 29
train acc:  0.875
train loss:  0.2564157247543335
train gradient:  0.07474589836440684
iteration : 30
train acc:  0.875
train loss:  0.32077497243881226
train gradient:  0.1128775966747656
iteration : 31
train acc:  0.8671875
train loss:  0.3179033398628235
train gradient:  0.11489599598111822
iteration : 32
train acc:  0.8125
train loss:  0.39930400252342224
train gradient:  0.1814693459606158
iteration : 33
train acc:  0.875
train loss:  0.27150124311447144
train gradient:  0.11052222995154982
iteration : 34
train acc:  0.859375
train loss:  0.365254670381546
train gradient:  0.17029546256034728
iteration : 35
train acc:  0.90625
train loss:  0.3424728512763977
train gradient:  0.2412499561035827
iteration : 36
train acc:  0.8515625
train loss:  0.33063966035842896
train gradient:  0.25335793945715557
iteration : 37
train acc:  0.859375
train loss:  0.3164038360118866
train gradient:  0.13329171847530524
iteration : 38
train acc:  0.7890625
train loss:  0.4203259348869324
train gradient:  0.22460083751125398
iteration : 39
train acc:  0.859375
train loss:  0.31022053956985474
train gradient:  0.1098522126293954
iteration : 40
train acc:  0.9140625
train loss:  0.28793346881866455
train gradient:  0.12699786848652217
iteration : 41
train acc:  0.8125
train loss:  0.37052589654922485
train gradient:  0.157292787650437
iteration : 42
train acc:  0.8671875
train loss:  0.28302258253097534
train gradient:  0.10520921924704163
iteration : 43
train acc:  0.859375
train loss:  0.32148438692092896
train gradient:  0.13700226596776227
iteration : 44
train acc:  0.84375
train loss:  0.3128488063812256
train gradient:  0.1006458296863219
iteration : 45
train acc:  0.84375
train loss:  0.31811976432800293
train gradient:  0.11426885713622677
iteration : 46
train acc:  0.8515625
train loss:  0.3519318103790283
train gradient:  0.13525880292308715
iteration : 47
train acc:  0.84375
train loss:  0.3485136926174164
train gradient:  0.12387097904474012
iteration : 48
train acc:  0.859375
train loss:  0.32906293869018555
train gradient:  0.12637805115121037
iteration : 49
train acc:  0.9296875
train loss:  0.22395353019237518
train gradient:  0.08014442210215315
iteration : 50
train acc:  0.9140625
train loss:  0.2550070881843567
train gradient:  0.08788170622942279
iteration : 51
train acc:  0.875
train loss:  0.23299270868301392
train gradient:  0.08003621678011111
iteration : 52
train acc:  0.8203125
train loss:  0.4427791237831116
train gradient:  0.2702249599895292
iteration : 53
train acc:  0.8515625
train loss:  0.34150075912475586
train gradient:  0.1259925800186324
iteration : 54
train acc:  0.890625
train loss:  0.30037373304367065
train gradient:  0.07810328405469207
iteration : 55
train acc:  0.859375
train loss:  0.348411500453949
train gradient:  0.15083019007214724
iteration : 56
train acc:  0.90625
train loss:  0.2620677947998047
train gradient:  0.09016683361522183
iteration : 57
train acc:  0.875
train loss:  0.31019702553749084
train gradient:  0.14717855844148756
iteration : 58
train acc:  0.8359375
train loss:  0.4272507131099701
train gradient:  0.18865721357573983
iteration : 59
train acc:  0.84375
train loss:  0.31455349922180176
train gradient:  0.14788308495000296
iteration : 60
train acc:  0.7890625
train loss:  0.4283398985862732
train gradient:  0.24766910538757497
iteration : 61
train acc:  0.8515625
train loss:  0.3745449185371399
train gradient:  0.2455575088661074
iteration : 62
train acc:  0.859375
train loss:  0.3449523448944092
train gradient:  0.12660987129452506
iteration : 63
train acc:  0.8671875
train loss:  0.3109275996685028
train gradient:  0.11148369713553678
iteration : 64
train acc:  0.8828125
train loss:  0.29506388306617737
train gradient:  0.14508268707736877
iteration : 65
train acc:  0.8828125
train loss:  0.23875078558921814
train gradient:  0.07507125335378964
iteration : 66
train acc:  0.828125
train loss:  0.4094438850879669
train gradient:  0.24606191855286966
iteration : 67
train acc:  0.75
train loss:  0.4264810383319855
train gradient:  0.19187090643714713
iteration : 68
train acc:  0.8984375
train loss:  0.25338107347488403
train gradient:  0.10386653576988723
iteration : 69
train acc:  0.8359375
train loss:  0.3862459063529968
train gradient:  0.21813696871495925
iteration : 70
train acc:  0.8125
train loss:  0.3906838297843933
train gradient:  0.1748319424863855
iteration : 71
train acc:  0.828125
train loss:  0.33003097772598267
train gradient:  0.1116427059347005
iteration : 72
train acc:  0.8828125
train loss:  0.28679120540618896
train gradient:  0.10491813859824455
iteration : 73
train acc:  0.875
train loss:  0.319141685962677
train gradient:  0.1020405191559
iteration : 74
train acc:  0.8515625
train loss:  0.3282689154148102
train gradient:  0.10194739545474431
iteration : 75
train acc:  0.84375
train loss:  0.3628697991371155
train gradient:  0.1279355416702453
iteration : 76
train acc:  0.8984375
train loss:  0.28627604246139526
train gradient:  0.08107352619353668
iteration : 77
train acc:  0.84375
train loss:  0.3383433222770691
train gradient:  0.14651455020224166
iteration : 78
train acc:  0.8671875
train loss:  0.31075674295425415
train gradient:  0.10488660310515188
iteration : 79
train acc:  0.9140625
train loss:  0.25502270460128784
train gradient:  0.13179373322281246
iteration : 80
train acc:  0.84375
train loss:  0.3280590772628784
train gradient:  0.10033476520838101
iteration : 81
train acc:  0.84375
train loss:  0.3646811246871948
train gradient:  0.13221303654654032
iteration : 82
train acc:  0.8984375
train loss:  0.3014826774597168
train gradient:  0.10948559583033377
iteration : 83
train acc:  0.828125
train loss:  0.36572587490081787
train gradient:  0.1526906096315889
iteration : 84
train acc:  0.875
train loss:  0.3136001229286194
train gradient:  0.10481107092739644
iteration : 85
train acc:  0.890625
train loss:  0.29433855414390564
train gradient:  0.1584869808540705
iteration : 86
train acc:  0.8046875
train loss:  0.3806261420249939
train gradient:  0.1274656636876133
iteration : 87
train acc:  0.859375
train loss:  0.3051985502243042
train gradient:  0.09917304125732089
iteration : 88
train acc:  0.828125
train loss:  0.4036295711994171
train gradient:  0.19772611386789396
iteration : 89
train acc:  0.921875
train loss:  0.2412121742963791
train gradient:  0.1457429631029295
iteration : 90
train acc:  0.84375
train loss:  0.27956628799438477
train gradient:  0.09455606373329985
iteration : 91
train acc:  0.84375
train loss:  0.3153517544269562
train gradient:  0.10243673811486667
iteration : 92
train acc:  0.859375
train loss:  0.2993662655353546
train gradient:  0.1099003700752314
iteration : 93
train acc:  0.90625
train loss:  0.23769304156303406
train gradient:  0.09491321834570553
iteration : 94
train acc:  0.8828125
train loss:  0.28422874212265015
train gradient:  0.1179478570136816
iteration : 95
train acc:  0.8828125
train loss:  0.3075789511203766
train gradient:  0.11895766608382712
iteration : 96
train acc:  0.8671875
train loss:  0.2863349914550781
train gradient:  0.10037884074080668
iteration : 97
train acc:  0.890625
train loss:  0.3081966042518616
train gradient:  0.19527607650601458
iteration : 98
train acc:  0.84375
train loss:  0.33582526445388794
train gradient:  0.11985365212127982
iteration : 99
train acc:  0.8671875
train loss:  0.31002965569496155
train gradient:  0.10368832751352933
iteration : 100
train acc:  0.9140625
train loss:  0.2696969509124756
train gradient:  0.09178705771242682
iteration : 101
train acc:  0.859375
train loss:  0.29259634017944336
train gradient:  0.09656279905636075
iteration : 102
train acc:  0.8515625
train loss:  0.2987029552459717
train gradient:  0.11631526384065816
iteration : 103
train acc:  0.8515625
train loss:  0.26441410183906555
train gradient:  0.08479119049743716
iteration : 104
train acc:  0.84375
train loss:  0.32030361890792847
train gradient:  0.12310288832467114
iteration : 105
train acc:  0.8359375
train loss:  0.3318256437778473
train gradient:  0.1266549351661551
iteration : 106
train acc:  0.890625
train loss:  0.2908346652984619
train gradient:  0.08888764391496226
iteration : 107
train acc:  0.8671875
train loss:  0.33042681217193604
train gradient:  0.11732007339203479
iteration : 108
train acc:  0.8359375
train loss:  0.3238169550895691
train gradient:  0.13290830729446795
iteration : 109
train acc:  0.890625
train loss:  0.258246511220932
train gradient:  0.09811644286380208
iteration : 110
train acc:  0.84375
train loss:  0.3382038474082947
train gradient:  0.15822861994223042
iteration : 111
train acc:  0.8828125
train loss:  0.29901379346847534
train gradient:  0.12778676317639562
iteration : 112
train acc:  0.8671875
train loss:  0.29561132192611694
train gradient:  0.10397037359867362
iteration : 113
train acc:  0.8828125
train loss:  0.251669704914093
train gradient:  0.09770481057318367
iteration : 114
train acc:  0.8515625
train loss:  0.27542027831077576
train gradient:  0.08078063767937262
iteration : 115
train acc:  0.875
train loss:  0.2901204228401184
train gradient:  0.11567385579909453
iteration : 116
train acc:  0.90625
train loss:  0.255871057510376
train gradient:  0.07843212484424163
iteration : 117
train acc:  0.875
train loss:  0.261614590883255
train gradient:  0.11234217386204602
iteration : 118
train acc:  0.9140625
train loss:  0.2189681977033615
train gradient:  0.08116566884250302
iteration : 119
train acc:  0.8515625
train loss:  0.33062446117401123
train gradient:  0.14924612515018315
iteration : 120
train acc:  0.8984375
train loss:  0.2849048376083374
train gradient:  0.10418847115099303
iteration : 121
train acc:  0.8828125
train loss:  0.2539803683757782
train gradient:  0.11601821034507948
iteration : 122
train acc:  0.796875
train loss:  0.39866214990615845
train gradient:  0.17956800371304887
iteration : 123
train acc:  0.90625
train loss:  0.2707020044326782
train gradient:  0.13408625789986
iteration : 124
train acc:  0.90625
train loss:  0.29393893480300903
train gradient:  0.10681477909819795
iteration : 125
train acc:  0.8515625
train loss:  0.3099489212036133
train gradient:  0.09475744371551784
iteration : 126
train acc:  0.8828125
train loss:  0.32093918323516846
train gradient:  0.16202594302495435
iteration : 127
train acc:  0.9140625
train loss:  0.2349337488412857
train gradient:  0.08498067031139922
iteration : 128
train acc:  0.8671875
train loss:  0.3081400394439697
train gradient:  0.10150128980143733
iteration : 129
train acc:  0.859375
train loss:  0.28645002841949463
train gradient:  0.11266059774705989
iteration : 130
train acc:  0.8359375
train loss:  0.30073732137680054
train gradient:  0.1182804737992958
iteration : 131
train acc:  0.875
train loss:  0.30357271432876587
train gradient:  0.11177057645215889
iteration : 132
train acc:  0.8359375
train loss:  0.3262362480163574
train gradient:  0.09554530653177586
iteration : 133
train acc:  0.859375
train loss:  0.3457304835319519
train gradient:  0.1879357997428739
iteration : 134
train acc:  0.9140625
train loss:  0.203843355178833
train gradient:  0.07623991668497537
iteration : 135
train acc:  0.8046875
train loss:  0.40991610288619995
train gradient:  0.16980091037411899
iteration : 136
train acc:  0.890625
train loss:  0.22434484958648682
train gradient:  0.1446666608775492
iteration : 137
train acc:  0.84375
train loss:  0.37576931715011597
train gradient:  0.11497031546764237
iteration : 138
train acc:  0.8203125
train loss:  0.42695367336273193
train gradient:  0.205429553529963
iteration : 139
train acc:  0.8828125
train loss:  0.24029654264450073
train gradient:  0.07942088670153212
iteration : 140
train acc:  0.796875
train loss:  0.4290427565574646
train gradient:  0.22360055211509205
iteration : 141
train acc:  0.8515625
train loss:  0.3658163845539093
train gradient:  0.13709044331571488
iteration : 142
train acc:  0.8515625
train loss:  0.2930554747581482
train gradient:  0.13392755901783276
iteration : 143
train acc:  0.8671875
train loss:  0.28954896330833435
train gradient:  0.07563760461505718
iteration : 144
train acc:  0.890625
train loss:  0.28816062211990356
train gradient:  0.08995701074631969
iteration : 145
train acc:  0.84375
train loss:  0.3316792845726013
train gradient:  0.1597495979618604
iteration : 146
train acc:  0.84375
train loss:  0.3573944568634033
train gradient:  0.14966284105601532
iteration : 147
train acc:  0.828125
train loss:  0.39984768629074097
train gradient:  0.2919012034665966
iteration : 148
train acc:  0.84375
train loss:  0.32610923051834106
train gradient:  0.25704071376358334
iteration : 149
train acc:  0.8984375
train loss:  0.24616214632987976
train gradient:  0.06850425209832203
iteration : 150
train acc:  0.921875
train loss:  0.2697827219963074
train gradient:  0.13058237942129813
iteration : 151
train acc:  0.8671875
train loss:  0.32375049591064453
train gradient:  0.1320891867603401
iteration : 152
train acc:  0.875
train loss:  0.27365124225616455
train gradient:  0.11617711004019356
iteration : 153
train acc:  0.90625
train loss:  0.3065136671066284
train gradient:  0.09543057431143223
iteration : 154
train acc:  0.875
train loss:  0.3257860839366913
train gradient:  0.10996753692629774
iteration : 155
train acc:  0.796875
train loss:  0.40385329723358154
train gradient:  0.21135843930637466
iteration : 156
train acc:  0.8203125
train loss:  0.3635767102241516
train gradient:  0.16146679793786217
iteration : 157
train acc:  0.9140625
train loss:  0.2618156671524048
train gradient:  0.10763144455683445
iteration : 158
train acc:  0.921875
train loss:  0.2156427800655365
train gradient:  0.08345976487733336
iteration : 159
train acc:  0.8671875
train loss:  0.29740390181541443
train gradient:  0.12609341693143872
iteration : 160
train acc:  0.921875
train loss:  0.3109856843948364
train gradient:  0.13571169306252012
iteration : 161
train acc:  0.8984375
train loss:  0.27826008200645447
train gradient:  0.10212447377739325
iteration : 162
train acc:  0.8203125
train loss:  0.36144477128982544
train gradient:  0.17982335630642948
iteration : 163
train acc:  0.875
train loss:  0.3058168888092041
train gradient:  0.13091210673254164
iteration : 164
train acc:  0.8828125
train loss:  0.3107069134712219
train gradient:  0.12938947868743908
iteration : 165
train acc:  0.890625
train loss:  0.29874151945114136
train gradient:  0.14572577141253018
iteration : 166
train acc:  0.8984375
train loss:  0.2611582577228546
train gradient:  0.11779889462860257
iteration : 167
train acc:  0.8515625
train loss:  0.3517810106277466
train gradient:  0.12004090023059422
iteration : 168
train acc:  0.875
train loss:  0.28623518347740173
train gradient:  0.13156584088161058
iteration : 169
train acc:  0.859375
train loss:  0.29697713255882263
train gradient:  0.12558743033059172
iteration : 170
train acc:  0.859375
train loss:  0.3297671973705292
train gradient:  0.16241793326875786
iteration : 171
train acc:  0.84375
train loss:  0.2870374917984009
train gradient:  0.10851375454615522
iteration : 172
train acc:  0.8515625
train loss:  0.44489604234695435
train gradient:  0.2589782629578909
iteration : 173
train acc:  0.875
train loss:  0.30867162346839905
train gradient:  0.14260734238956924
iteration : 174
train acc:  0.8359375
train loss:  0.318267822265625
train gradient:  0.10519660487654311
iteration : 175
train acc:  0.828125
train loss:  0.3621819317340851
train gradient:  0.15578214676304206
iteration : 176
train acc:  0.90625
train loss:  0.2534162998199463
train gradient:  0.12084488236978592
iteration : 177
train acc:  0.8359375
train loss:  0.4210004210472107
train gradient:  0.1949743305768076
iteration : 178
train acc:  0.859375
train loss:  0.33416324853897095
train gradient:  0.1485305000743403
iteration : 179
train acc:  0.8125
train loss:  0.32958972454071045
train gradient:  0.09876412130261501
iteration : 180
train acc:  0.8671875
train loss:  0.304148405790329
train gradient:  0.09309028812774602
iteration : 181
train acc:  0.890625
train loss:  0.2934132218360901
train gradient:  0.09381342452667851
iteration : 182
train acc:  0.9140625
train loss:  0.298028826713562
train gradient:  0.09833001360830712
iteration : 183
train acc:  0.875
train loss:  0.307600200176239
train gradient:  0.12302892598732716
iteration : 184
train acc:  0.8515625
train loss:  0.40614908933639526
train gradient:  0.22653894486826104
iteration : 185
train acc:  0.8359375
train loss:  0.3610641062259674
train gradient:  0.1583838876648137
iteration : 186
train acc:  0.84375
train loss:  0.26452434062957764
train gradient:  0.08118327579804292
iteration : 187
train acc:  0.8203125
train loss:  0.40976548194885254
train gradient:  0.17721927089424722
iteration : 188
train acc:  0.8828125
train loss:  0.3421318531036377
train gradient:  0.1255764123929738
iteration : 189
train acc:  0.8046875
train loss:  0.38887590169906616
train gradient:  0.3216213288909736
iteration : 190
train acc:  0.8828125
train loss:  0.36632806062698364
train gradient:  0.27059468809816745
iteration : 191
train acc:  0.890625
train loss:  0.2993699014186859
train gradient:  0.10319437622982827
iteration : 192
train acc:  0.890625
train loss:  0.2872176468372345
train gradient:  0.1097833892118923
iteration : 193
train acc:  0.8984375
train loss:  0.2664836347103119
train gradient:  0.10726991674517457
iteration : 194
train acc:  0.84375
train loss:  0.3771059215068817
train gradient:  0.13362219767025768
iteration : 195
train acc:  0.859375
train loss:  0.33379077911376953
train gradient:  0.1564204746868036
iteration : 196
train acc:  0.8046875
train loss:  0.43358245491981506
train gradient:  0.15989240629762588
iteration : 197
train acc:  0.9140625
train loss:  0.2428259551525116
train gradient:  0.1618544665001213
iteration : 198
train acc:  0.859375
train loss:  0.29541122913360596
train gradient:  0.14009159359397252
iteration : 199
train acc:  0.875
train loss:  0.24968817830085754
train gradient:  0.10804723279907781
iteration : 200
train acc:  0.8671875
train loss:  0.30950695276260376
train gradient:  0.10647691739346993
iteration : 201
train acc:  0.8984375
train loss:  0.25779685378074646
train gradient:  0.07229194376493484
iteration : 202
train acc:  0.890625
train loss:  0.31333792209625244
train gradient:  0.27649629807976084
iteration : 203
train acc:  0.8359375
train loss:  0.4123471975326538
train gradient:  0.13313388900787995
iteration : 204
train acc:  0.875
train loss:  0.2751839756965637
train gradient:  0.1268748299683402
iteration : 205
train acc:  0.8515625
train loss:  0.3375474810600281
train gradient:  0.16152497261336576
iteration : 206
train acc:  0.8359375
train loss:  0.35151785612106323
train gradient:  0.16379638780113515
iteration : 207
train acc:  0.8046875
train loss:  0.37260475754737854
train gradient:  0.141066546744263
iteration : 208
train acc:  0.890625
train loss:  0.2665363848209381
train gradient:  0.08354001108823636
iteration : 209
train acc:  0.859375
train loss:  0.3140932321548462
train gradient:  0.11213360889668493
iteration : 210
train acc:  0.9140625
train loss:  0.2745283246040344
train gradient:  0.1093065521921036
iteration : 211
train acc:  0.859375
train loss:  0.2979952096939087
train gradient:  0.10150606833129401
iteration : 212
train acc:  0.875
train loss:  0.32210320234298706
train gradient:  0.12085686653677087
iteration : 213
train acc:  0.875
train loss:  0.2855692505836487
train gradient:  0.08568646811749914
iteration : 214
train acc:  0.7890625
train loss:  0.39442136883735657
train gradient:  0.16235566814097047
iteration : 215
train acc:  0.859375
train loss:  0.2921730875968933
train gradient:  0.07859319585768147
iteration : 216
train acc:  0.828125
train loss:  0.34747517108917236
train gradient:  0.1619809153688659
iteration : 217
train acc:  0.8515625
train loss:  0.2954355478286743
train gradient:  0.09913483455544993
iteration : 218
train acc:  0.8203125
train loss:  0.3553725481033325
train gradient:  0.15265412761673922
iteration : 219
train acc:  0.8515625
train loss:  0.27804261445999146
train gradient:  0.12019519821178615
iteration : 220
train acc:  0.859375
train loss:  0.2847767472267151
train gradient:  0.12921725850476662
iteration : 221
train acc:  0.921875
train loss:  0.25854140520095825
train gradient:  0.08637603857266613
iteration : 222
train acc:  0.84375
train loss:  0.32812532782554626
train gradient:  0.1273853326470144
iteration : 223
train acc:  0.8359375
train loss:  0.3939223885536194
train gradient:  0.19208280370011183
iteration : 224
train acc:  0.8515625
train loss:  0.32815021276474
train gradient:  0.18759069193572836
iteration : 225
train acc:  0.828125
train loss:  0.33248165249824524
train gradient:  0.12912239007913712
iteration : 226
train acc:  0.859375
train loss:  0.303574800491333
train gradient:  0.1602118145491926
iteration : 227
train acc:  0.8359375
train loss:  0.3532136380672455
train gradient:  0.14086754011154462
iteration : 228
train acc:  0.8359375
train loss:  0.36190587282180786
train gradient:  0.18789680380569243
iteration : 229
train acc:  0.890625
train loss:  0.2974342703819275
train gradient:  0.1452597668674479
iteration : 230
train acc:  0.84375
train loss:  0.32714471220970154
train gradient:  0.10548071671487097
iteration : 231
train acc:  0.859375
train loss:  0.28228437900543213
train gradient:  0.10846362207343879
iteration : 232
train acc:  0.890625
train loss:  0.2594040036201477
train gradient:  0.09878133554299066
iteration : 233
train acc:  0.859375
train loss:  0.36079561710357666
train gradient:  0.14458234521149982
iteration : 234
train acc:  0.84375
train loss:  0.3524046540260315
train gradient:  0.18097587871578724
iteration : 235
train acc:  0.859375
train loss:  0.2564832270145416
train gradient:  0.0971478343892903
iteration : 236
train acc:  0.9140625
train loss:  0.23125571012496948
train gradient:  0.09644477238426566
iteration : 237
train acc:  0.90625
train loss:  0.29410529136657715
train gradient:  0.11888217382764582
iteration : 238
train acc:  0.84375
train loss:  0.27971115708351135
train gradient:  0.11017686183147911
iteration : 239
train acc:  0.8828125
train loss:  0.2829154133796692
train gradient:  0.17686000359328058
iteration : 240
train acc:  0.796875
train loss:  0.36996257305145264
train gradient:  0.13245499938735256
iteration : 241
train acc:  0.8359375
train loss:  0.38558125495910645
train gradient:  0.16155143608946215
iteration : 242
train acc:  0.875
train loss:  0.32815778255462646
train gradient:  0.1233532835304731
iteration : 243
train acc:  0.8984375
train loss:  0.2594451308250427
train gradient:  0.06605550206150622
iteration : 244
train acc:  0.8828125
train loss:  0.2891937792301178
train gradient:  0.10745923494801449
iteration : 245
train acc:  0.859375
train loss:  0.24560974538326263
train gradient:  0.09950361816092958
iteration : 246
train acc:  0.8515625
train loss:  0.3120875656604767
train gradient:  0.11328395635915056
iteration : 247
train acc:  0.8671875
train loss:  0.3064577579498291
train gradient:  0.12285531858494426
iteration : 248
train acc:  0.8828125
train loss:  0.3301359713077545
train gradient:  0.14198711443439366
iteration : 249
train acc:  0.84375
train loss:  0.30023419857025146
train gradient:  0.1473538101387192
iteration : 250
train acc:  0.859375
train loss:  0.3053228557109833
train gradient:  0.10326200183568977
iteration : 251
train acc:  0.8203125
train loss:  0.3537960648536682
train gradient:  0.13622159793516614
iteration : 252
train acc:  0.8828125
train loss:  0.27954766154289246
train gradient:  0.09685193273872618
iteration : 253
train acc:  0.8671875
train loss:  0.307827889919281
train gradient:  0.0991915468092508
iteration : 254
train acc:  0.890625
train loss:  0.3018651604652405
train gradient:  0.1731389934104163
iteration : 255
train acc:  0.890625
train loss:  0.2492673546075821
train gradient:  0.09426819883344223
iteration : 256
train acc:  0.859375
train loss:  0.2878735065460205
train gradient:  0.08849116363761478
iteration : 257
train acc:  0.890625
train loss:  0.2610882520675659
train gradient:  0.08457355538157017
iteration : 258
train acc:  0.84375
train loss:  0.3298434019088745
train gradient:  0.10608031532222509
iteration : 259
train acc:  0.8671875
train loss:  0.30905771255493164
train gradient:  0.11505575083695245
iteration : 260
train acc:  0.8671875
train loss:  0.3080545961856842
train gradient:  0.07163393473268137
iteration : 261
train acc:  0.84375
train loss:  0.3911880850791931
train gradient:  0.17401867671597093
iteration : 262
train acc:  0.8828125
train loss:  0.2533957064151764
train gradient:  0.08381353361411109
iteration : 263
train acc:  0.859375
train loss:  0.2671603262424469
train gradient:  0.09803244171956522
iteration : 264
train acc:  0.8203125
train loss:  0.38581955432891846
train gradient:  0.1493153098214049
iteration : 265
train acc:  0.859375
train loss:  0.2963181734085083
train gradient:  0.10310355110096417
iteration : 266
train acc:  0.8671875
train loss:  0.2924552857875824
train gradient:  0.08402089641105755
iteration : 267
train acc:  0.8984375
train loss:  0.23856815695762634
train gradient:  0.08140229234579756
iteration : 268
train acc:  0.84375
train loss:  0.295585572719574
train gradient:  0.12779512513334207
iteration : 269
train acc:  0.84375
train loss:  0.3381965160369873
train gradient:  0.14414303639891946
iteration : 270
train acc:  0.875
train loss:  0.3169277012348175
train gradient:  0.13791809751506334
iteration : 271
train acc:  0.78125
train loss:  0.5156577825546265
train gradient:  0.21361437593387528
iteration : 272
train acc:  0.84375
train loss:  0.31190603971481323
train gradient:  0.11272587574514367
iteration : 273
train acc:  0.796875
train loss:  0.4745994210243225
train gradient:  0.21445345307907923
iteration : 274
train acc:  0.796875
train loss:  0.4339926838874817
train gradient:  0.23631392170369506
iteration : 275
train acc:  0.8359375
train loss:  0.3234435021877289
train gradient:  0.14983901685353718
iteration : 276
train acc:  0.8671875
train loss:  0.34737539291381836
train gradient:  0.15908002599925622
iteration : 277
train acc:  0.921875
train loss:  0.23640231788158417
train gradient:  0.10262638459648407
iteration : 278
train acc:  0.890625
train loss:  0.24173054099082947
train gradient:  0.10150330223696744
iteration : 279
train acc:  0.8671875
train loss:  0.3204774260520935
train gradient:  0.14633483919418588
iteration : 280
train acc:  0.859375
train loss:  0.32818710803985596
train gradient:  0.1469738369156096
iteration : 281
train acc:  0.859375
train loss:  0.3189160227775574
train gradient:  0.10608406891462029
iteration : 282
train acc:  0.875
train loss:  0.3515191078186035
train gradient:  0.11751516729215923
iteration : 283
train acc:  0.875
train loss:  0.3389878571033478
train gradient:  0.16825578697409696
iteration : 284
train acc:  0.890625
train loss:  0.27454808354377747
train gradient:  0.1164081199947271
iteration : 285
train acc:  0.8828125
train loss:  0.31998446583747864
train gradient:  0.10968502749501222
iteration : 286
train acc:  0.8515625
train loss:  0.2912253141403198
train gradient:  0.1121835911504656
iteration : 287
train acc:  0.8984375
train loss:  0.22019128501415253
train gradient:  0.07670358658818992
iteration : 288
train acc:  0.8515625
train loss:  0.2964170277118683
train gradient:  0.09095303562976917
iteration : 289
train acc:  0.875
train loss:  0.33380311727523804
train gradient:  0.18779431719165135
iteration : 290
train acc:  0.859375
train loss:  0.28440988063812256
train gradient:  0.07662387585070299
iteration : 291
train acc:  0.8359375
train loss:  0.3728801906108856
train gradient:  0.18479400755678826
iteration : 292
train acc:  0.890625
train loss:  0.2667815685272217
train gradient:  0.094068953389698
iteration : 293
train acc:  0.84375
train loss:  0.3168787956237793
train gradient:  0.11514109242007181
iteration : 294
train acc:  0.796875
train loss:  0.3477940559387207
train gradient:  0.148425720680112
iteration : 295
train acc:  0.84375
train loss:  0.3040241003036499
train gradient:  0.1553269459414413
iteration : 296
train acc:  0.8359375
train loss:  0.40390580892562866
train gradient:  0.17350861654725475
iteration : 297
train acc:  0.8515625
train loss:  0.27059000730514526
train gradient:  0.08169428618582768
iteration : 298
train acc:  0.859375
train loss:  0.28860175609588623
train gradient:  0.11112694992743062
iteration : 299
train acc:  0.828125
train loss:  0.40971165895462036
train gradient:  0.17836180342501518
iteration : 300
train acc:  0.890625
train loss:  0.2594861090183258
train gradient:  0.09681603194990614
iteration : 301
train acc:  0.828125
train loss:  0.32855045795440674
train gradient:  0.1500821109773401
iteration : 302
train acc:  0.8515625
train loss:  0.3634584844112396
train gradient:  0.24766663382749815
iteration : 303
train acc:  0.84375
train loss:  0.2868313193321228
train gradient:  0.09638596128088835
iteration : 304
train acc:  0.8359375
train loss:  0.3906927704811096
train gradient:  0.22337268621072892
iteration : 305
train acc:  0.8984375
train loss:  0.28319472074508667
train gradient:  0.17178624459442082
iteration : 306
train acc:  0.875
train loss:  0.2940833568572998
train gradient:  0.0902586106306372
iteration : 307
train acc:  0.8125
train loss:  0.3835073709487915
train gradient:  0.11817462794329013
iteration : 308
train acc:  0.9140625
train loss:  0.2715147137641907
train gradient:  0.08827032930862884
iteration : 309
train acc:  0.796875
train loss:  0.37904828786849976
train gradient:  0.15942045978198272
iteration : 310
train acc:  0.890625
train loss:  0.30809158086776733
train gradient:  0.1341169539932422
iteration : 311
train acc:  0.8828125
train loss:  0.27987828850746155
train gradient:  0.08509527313874501
iteration : 312
train acc:  0.859375
train loss:  0.3104811906814575
train gradient:  0.09437358684500649
iteration : 313
train acc:  0.8359375
train loss:  0.305171400308609
train gradient:  0.10986959804322512
iteration : 314
train acc:  0.9140625
train loss:  0.2712528109550476
train gradient:  0.21604465723149258
iteration : 315
train acc:  0.8671875
train loss:  0.34916770458221436
train gradient:  0.1598579360376874
iteration : 316
train acc:  0.875
train loss:  0.27877533435821533
train gradient:  0.10519864159752608
iteration : 317
train acc:  0.828125
train loss:  0.36778151988983154
train gradient:  0.23537325456625496
iteration : 318
train acc:  0.8359375
train loss:  0.36260876059532166
train gradient:  0.19696092703773788
iteration : 319
train acc:  0.8671875
train loss:  0.30553868412971497
train gradient:  0.11377363381058311
iteration : 320
train acc:  0.9140625
train loss:  0.23782481253147125
train gradient:  0.08652848906711212
iteration : 321
train acc:  0.90625
train loss:  0.24264740943908691
train gradient:  0.08137040968740347
iteration : 322
train acc:  0.8515625
train loss:  0.3146480917930603
train gradient:  0.1215532575321029
iteration : 323
train acc:  0.890625
train loss:  0.3209385275840759
train gradient:  0.10618828217943985
iteration : 324
train acc:  0.8828125
train loss:  0.3326341509819031
train gradient:  0.09566348777496486
iteration : 325
train acc:  0.890625
train loss:  0.26293766498565674
train gradient:  0.10594355551404432
iteration : 326
train acc:  0.84375
train loss:  0.3730223476886749
train gradient:  0.27807089998662277
iteration : 327
train acc:  0.8671875
train loss:  0.3184635639190674
train gradient:  0.1490568716160965
iteration : 328
train acc:  0.84375
train loss:  0.33101212978363037
train gradient:  0.10367305293608546
iteration : 329
train acc:  0.84375
train loss:  0.3154063820838928
train gradient:  0.1536377557505402
iteration : 330
train acc:  0.8671875
train loss:  0.34200942516326904
train gradient:  0.14728038331889473
iteration : 331
train acc:  0.8984375
train loss:  0.2726163864135742
train gradient:  0.11736359403090031
iteration : 332
train acc:  0.8046875
train loss:  0.3788134455680847
train gradient:  0.17927294979513325
iteration : 333
train acc:  0.8203125
train loss:  0.41427576541900635
train gradient:  0.1496200775392278
iteration : 334
train acc:  0.859375
train loss:  0.2821791470050812
train gradient:  0.10690522190568137
iteration : 335
train acc:  0.875
train loss:  0.3056371212005615
train gradient:  0.1278854808528384
iteration : 336
train acc:  0.8671875
train loss:  0.34585684537887573
train gradient:  0.13108120457995667
iteration : 337
train acc:  0.8203125
train loss:  0.39325734972953796
train gradient:  0.18727219605026368
iteration : 338
train acc:  0.8203125
train loss:  0.4225594997406006
train gradient:  0.22673065305945767
iteration : 339
train acc:  0.78125
train loss:  0.41054844856262207
train gradient:  0.23806007214755326
iteration : 340
train acc:  0.859375
train loss:  0.4585069417953491
train gradient:  0.278187219568843
iteration : 341
train acc:  0.8671875
train loss:  0.37238338589668274
train gradient:  0.14631678388943478
iteration : 342
train acc:  0.890625
train loss:  0.2761411666870117
train gradient:  0.11155059945528195
iteration : 343
train acc:  0.859375
train loss:  0.30283185839653015
train gradient:  0.12241355979063599
iteration : 344
train acc:  0.8203125
train loss:  0.3495538532733917
train gradient:  0.12111809092673674
iteration : 345
train acc:  0.9140625
train loss:  0.2572910785675049
train gradient:  0.11675833347985652
iteration : 346
train acc:  0.875
train loss:  0.2767981290817261
train gradient:  0.07629216986057422
iteration : 347
train acc:  0.890625
train loss:  0.2606477737426758
train gradient:  0.06424240866576024
iteration : 348
train acc:  0.8828125
train loss:  0.3290475308895111
train gradient:  0.10226637566241652
iteration : 349
train acc:  0.859375
train loss:  0.37237298488616943
train gradient:  0.18174330937080205
iteration : 350
train acc:  0.859375
train loss:  0.27168190479278564
train gradient:  0.07868151530719442
iteration : 351
train acc:  0.8671875
train loss:  0.32015326619148254
train gradient:  0.13078838965755557
iteration : 352
train acc:  0.84375
train loss:  0.3525089919567108
train gradient:  0.1324157988219422
iteration : 353
train acc:  0.859375
train loss:  0.3330942690372467
train gradient:  0.11328134529136921
iteration : 354
train acc:  0.859375
train loss:  0.37071382999420166
train gradient:  0.14676995573890111
iteration : 355
train acc:  0.8203125
train loss:  0.3473491966724396
train gradient:  0.1503410042632446
iteration : 356
train acc:  0.8359375
train loss:  0.3418469727039337
train gradient:  0.20061221356229586
iteration : 357
train acc:  0.90625
train loss:  0.28615638613700867
train gradient:  0.23266163183314523
iteration : 358
train acc:  0.875
train loss:  0.3126479983329773
train gradient:  0.11026342409531942
iteration : 359
train acc:  0.8515625
train loss:  0.3291684687137604
train gradient:  0.12150750607007195
iteration : 360
train acc:  0.875
train loss:  0.3008641004562378
train gradient:  0.09516514410918639
iteration : 361
train acc:  0.8203125
train loss:  0.4110211431980133
train gradient:  0.22326110583748782
iteration : 362
train acc:  0.828125
train loss:  0.3643462061882019
train gradient:  0.275975823247252
iteration : 363
train acc:  0.828125
train loss:  0.4092273712158203
train gradient:  0.24265595163290354
iteration : 364
train acc:  0.796875
train loss:  0.42129120230674744
train gradient:  0.19511385498258232
iteration : 365
train acc:  0.8828125
train loss:  0.2751396894454956
train gradient:  0.0884138381866526
iteration : 366
train acc:  0.921875
train loss:  0.289419949054718
train gradient:  0.09085588370861317
iteration : 367
train acc:  0.9296875
train loss:  0.25050243735313416
train gradient:  0.07832201110740533
iteration : 368
train acc:  0.8671875
train loss:  0.2813115119934082
train gradient:  0.10373954941414583
iteration : 369
train acc:  0.8671875
train loss:  0.3317115604877472
train gradient:  0.12653782372120956
iteration : 370
train acc:  0.875
train loss:  0.29719167947769165
train gradient:  0.14651042147420645
iteration : 371
train acc:  0.8828125
train loss:  0.25780630111694336
train gradient:  0.07944305204206167
iteration : 372
train acc:  0.8515625
train loss:  0.3755563795566559
train gradient:  0.1537155328726464
iteration : 373
train acc:  0.875
train loss:  0.27298155426979065
train gradient:  0.08222122823564
iteration : 374
train acc:  0.875
train loss:  0.3064707815647125
train gradient:  0.09710682932646493
iteration : 375
train acc:  0.8671875
train loss:  0.3250548839569092
train gradient:  0.09180619653017956
iteration : 376
train acc:  0.8984375
train loss:  0.23304055631160736
train gradient:  0.1383446969752336
iteration : 377
train acc:  0.8515625
train loss:  0.3305424451828003
train gradient:  0.12960988993921368
iteration : 378
train acc:  0.875
train loss:  0.29174795746803284
train gradient:  0.09121388569583297
iteration : 379
train acc:  0.84375
train loss:  0.3369220793247223
train gradient:  0.1482151801165818
iteration : 380
train acc:  0.890625
train loss:  0.2825632095336914
train gradient:  0.08586876454746147
iteration : 381
train acc:  0.875
train loss:  0.3576706349849701
train gradient:  0.12502867333224854
iteration : 382
train acc:  0.875
train loss:  0.3142099976539612
train gradient:  0.07981746776614415
iteration : 383
train acc:  0.8828125
train loss:  0.2558574974536896
train gradient:  0.09534330951786284
iteration : 384
train acc:  0.859375
train loss:  0.3222723603248596
train gradient:  0.09443419644604426
iteration : 385
train acc:  0.84375
train loss:  0.35408610105514526
train gradient:  0.20120407757836084
iteration : 386
train acc:  0.90625
train loss:  0.27044668793678284
train gradient:  0.07858903306753304
iteration : 387
train acc:  0.90625
train loss:  0.25011056661605835
train gradient:  0.07303732415675522
iteration : 388
train acc:  0.9140625
train loss:  0.264337956905365
train gradient:  0.08233063814575102
iteration : 389
train acc:  0.796875
train loss:  0.39527156949043274
train gradient:  0.16116080617403916
iteration : 390
train acc:  0.84375
train loss:  0.34014105796813965
train gradient:  0.16203178468253474
iteration : 391
train acc:  0.890625
train loss:  0.2954343557357788
train gradient:  0.11912472031940842
iteration : 392
train acc:  0.78125
train loss:  0.37460237741470337
train gradient:  0.2161441185341556
iteration : 393
train acc:  0.8359375
train loss:  0.33693230152130127
train gradient:  0.1107208579672715
iteration : 394
train acc:  0.890625
train loss:  0.26871687173843384
train gradient:  0.09174114292289304
iteration : 395
train acc:  0.84375
train loss:  0.3164927363395691
train gradient:  0.10377972496318308
iteration : 396
train acc:  0.875
train loss:  0.3352298140525818
train gradient:  0.1381609651802312
iteration : 397
train acc:  0.8515625
train loss:  0.3996187448501587
train gradient:  0.14927167444108888
iteration : 398
train acc:  0.875
train loss:  0.27988946437835693
train gradient:  0.10542425771331973
iteration : 399
train acc:  0.8984375
train loss:  0.26107943058013916
train gradient:  0.1166611169690311
iteration : 400
train acc:  0.875
train loss:  0.30986228585243225
train gradient:  0.1195015530201201
iteration : 401
train acc:  0.8203125
train loss:  0.3968906104564667
train gradient:  0.1489291995644939
iteration : 402
train acc:  0.8828125
train loss:  0.29599088430404663
train gradient:  0.13140451736362274
iteration : 403
train acc:  0.8828125
train loss:  0.3052886426448822
train gradient:  0.24323947258420192
iteration : 404
train acc:  0.8515625
train loss:  0.3151361644268036
train gradient:  0.144089014159923
iteration : 405
train acc:  0.890625
train loss:  0.29110532999038696
train gradient:  0.09792292202849409
iteration : 406
train acc:  0.828125
train loss:  0.4064508080482483
train gradient:  0.16860927472810086
iteration : 407
train acc:  0.8984375
train loss:  0.28591543436050415
train gradient:  0.10175734445105211
iteration : 408
train acc:  0.796875
train loss:  0.4035273492336273
train gradient:  0.16252235987237285
iteration : 409
train acc:  0.8671875
train loss:  0.3188997507095337
train gradient:  0.13075131249427363
iteration : 410
train acc:  0.8046875
train loss:  0.32175213098526
train gradient:  0.09256034500878028
iteration : 411
train acc:  0.8828125
train loss:  0.30988162755966187
train gradient:  0.12424215952906652
iteration : 412
train acc:  0.953125
train loss:  0.19426041841506958
train gradient:  0.05833198387460313
iteration : 413
train acc:  0.875
train loss:  0.29420459270477295
train gradient:  0.08336019231697732
iteration : 414
train acc:  0.9140625
train loss:  0.26027923822402954
train gradient:  0.11475148911019396
iteration : 415
train acc:  0.859375
train loss:  0.2949758470058441
train gradient:  0.1345830468435217
iteration : 416
train acc:  0.8671875
train loss:  0.34081757068634033
train gradient:  0.14765757462375917
iteration : 417
train acc:  0.859375
train loss:  0.31108686327934265
train gradient:  0.13904438173349853
iteration : 418
train acc:  0.8671875
train loss:  0.31072890758514404
train gradient:  0.11598607208487824
iteration : 419
train acc:  0.8828125
train loss:  0.2818862199783325
train gradient:  0.18932827637722915
iteration : 420
train acc:  0.796875
train loss:  0.40886783599853516
train gradient:  0.2392196313485485
iteration : 421
train acc:  0.8515625
train loss:  0.31679341197013855
train gradient:  0.09984627068351244
iteration : 422
train acc:  0.8984375
train loss:  0.21506047248840332
train gradient:  0.1044752359158913
iteration : 423
train acc:  0.875
train loss:  0.26213937997817993
train gradient:  0.09805428693979644
iteration : 424
train acc:  0.890625
train loss:  0.31052011251449585
train gradient:  0.11379442149237402
iteration : 425
train acc:  0.875
train loss:  0.32032832503318787
train gradient:  0.14558611256247955
iteration : 426
train acc:  0.84375
train loss:  0.3120358884334564
train gradient:  0.1472593902853353
iteration : 427
train acc:  0.859375
train loss:  0.37313905358314514
train gradient:  0.13391164768621017
iteration : 428
train acc:  0.859375
train loss:  0.378345787525177
train gradient:  0.1522468096803583
iteration : 429
train acc:  0.90625
train loss:  0.24353072047233582
train gradient:  0.07958979534201584
iteration : 430
train acc:  0.828125
train loss:  0.36440908908843994
train gradient:  0.16483321751364466
iteration : 431
train acc:  0.875
train loss:  0.2722119092941284
train gradient:  0.12559151094450055
iteration : 432
train acc:  0.921875
train loss:  0.20819422602653503
train gradient:  0.09586901632522893
iteration : 433
train acc:  0.90625
train loss:  0.25481048226356506
train gradient:  0.0651410978815983
iteration : 434
train acc:  0.875
train loss:  0.28740113973617554
train gradient:  0.09911682003688796
iteration : 435
train acc:  0.828125
train loss:  0.3899894058704376
train gradient:  0.17076081140302476
iteration : 436
train acc:  0.8359375
train loss:  0.3440594971179962
train gradient:  0.14014617437969618
iteration : 437
train acc:  0.8359375
train loss:  0.3596421182155609
train gradient:  0.14718509799183194
iteration : 438
train acc:  0.84375
train loss:  0.2746899127960205
train gradient:  0.11988110383659828
iteration : 439
train acc:  0.84375
train loss:  0.2963056266307831
train gradient:  0.10636482465419132
iteration : 440
train acc:  0.8984375
train loss:  0.28663337230682373
train gradient:  0.07860967999662684
iteration : 441
train acc:  0.8515625
train loss:  0.293277770280838
train gradient:  0.11347651548988898
iteration : 442
train acc:  0.84375
train loss:  0.40459775924682617
train gradient:  0.15990711019688314
iteration : 443
train acc:  0.921875
train loss:  0.2512509226799011
train gradient:  0.11308138263402286
iteration : 444
train acc:  0.859375
train loss:  0.36868157982826233
train gradient:  0.1640950892743689
iteration : 445
train acc:  0.8359375
train loss:  0.3156837224960327
train gradient:  0.14785853106985716
iteration : 446
train acc:  0.859375
train loss:  0.3024633526802063
train gradient:  0.12268699980438252
iteration : 447
train acc:  0.8359375
train loss:  0.3920435607433319
train gradient:  0.15184787549787068
iteration : 448
train acc:  0.875
train loss:  0.31334370374679565
train gradient:  0.1477871174317589
iteration : 449
train acc:  0.890625
train loss:  0.27623409032821655
train gradient:  0.1304889822359116
iteration : 450
train acc:  0.84375
train loss:  0.31656575202941895
train gradient:  0.18690538738979176
iteration : 451
train acc:  0.8828125
train loss:  0.2741450369358063
train gradient:  0.093005853740809
iteration : 452
train acc:  0.84375
train loss:  0.27423208951950073
train gradient:  0.10701907893046568
iteration : 453
train acc:  0.8359375
train loss:  0.32524144649505615
train gradient:  0.10859560178595208
iteration : 454
train acc:  0.8828125
train loss:  0.23586520552635193
train gradient:  0.09248320807430269
iteration : 455
train acc:  0.8515625
train loss:  0.2997714877128601
train gradient:  0.13357079645312156
iteration : 456
train acc:  0.8984375
train loss:  0.28019243478775024
train gradient:  0.12648774733549695
iteration : 457
train acc:  0.8828125
train loss:  0.28059759736061096
train gradient:  0.09274986338741951
iteration : 458
train acc:  0.8828125
train loss:  0.2978362441062927
train gradient:  0.09322074982666675
iteration : 459
train acc:  0.8671875
train loss:  0.32163816690444946
train gradient:  0.14256196024403806
iteration : 460
train acc:  0.8671875
train loss:  0.26081904768943787
train gradient:  0.09690631396479935
iteration : 461
train acc:  0.9140625
train loss:  0.230394184589386
train gradient:  0.0984330974606327
iteration : 462
train acc:  0.90625
train loss:  0.26328831911087036
train gradient:  0.1266675289736418
iteration : 463
train acc:  0.84375
train loss:  0.3464193046092987
train gradient:  0.14378500950063694
iteration : 464
train acc:  0.828125
train loss:  0.3638024628162384
train gradient:  0.12103833272898647
iteration : 465
train acc:  0.859375
train loss:  0.2828352451324463
train gradient:  0.14985188762210533
iteration : 466
train acc:  0.859375
train loss:  0.3002856969833374
train gradient:  0.10404870619499955
iteration : 467
train acc:  0.859375
train loss:  0.3278161287307739
train gradient:  0.1530869540848439
iteration : 468
train acc:  0.84375
train loss:  0.3465416431427002
train gradient:  0.15366320755436338
iteration : 469
train acc:  0.890625
train loss:  0.2793181836605072
train gradient:  0.13740199441845177
iteration : 470
train acc:  0.9140625
train loss:  0.25311538577079773
train gradient:  0.09470829174638049
iteration : 471
train acc:  0.8359375
train loss:  0.3514355421066284
train gradient:  0.13354974616354445
iteration : 472
train acc:  0.90625
train loss:  0.2801019549369812
train gradient:  0.09996962644538766
iteration : 473
train acc:  0.8828125
train loss:  0.28904134035110474
train gradient:  0.1064291905118279
iteration : 474
train acc:  0.796875
train loss:  0.4501642882823944
train gradient:  0.20328482707779366
iteration : 475
train acc:  0.8359375
train loss:  0.3421662747859955
train gradient:  0.15843530393849747
iteration : 476
train acc:  0.8515625
train loss:  0.39640772342681885
train gradient:  0.1699231180381446
iteration : 477
train acc:  0.8203125
train loss:  0.3294341266155243
train gradient:  0.21445371705413596
iteration : 478
train acc:  0.84375
train loss:  0.3323656916618347
train gradient:  0.1697868481410018
iteration : 479
train acc:  0.90625
train loss:  0.26902908086776733
train gradient:  0.1616928808602246
iteration : 480
train acc:  0.859375
train loss:  0.3169611692428589
train gradient:  0.1346944478940467
iteration : 481
train acc:  0.859375
train loss:  0.3052061200141907
train gradient:  0.10579727108992841
iteration : 482
train acc:  0.9140625
train loss:  0.24364402890205383
train gradient:  0.11250133780209816
iteration : 483
train acc:  0.859375
train loss:  0.3104173541069031
train gradient:  0.1319296669518275
iteration : 484
train acc:  0.890625
train loss:  0.26111263036727905
train gradient:  0.11099827132510788
iteration : 485
train acc:  0.8671875
train loss:  0.30321747064590454
train gradient:  0.12386389801434708
iteration : 486
train acc:  0.875
train loss:  0.311393141746521
train gradient:  0.11326813123201737
iteration : 487
train acc:  0.90625
train loss:  0.27201414108276367
train gradient:  0.09357246735755835
iteration : 488
train acc:  0.875
train loss:  0.2840547561645508
train gradient:  0.12795030910482183
iteration : 489
train acc:  0.8515625
train loss:  0.32587379217147827
train gradient:  0.19058424123153705
iteration : 490
train acc:  0.84375
train loss:  0.33457452058792114
train gradient:  0.15745584226860293
iteration : 491
train acc:  0.8203125
train loss:  0.37647631764411926
train gradient:  0.1759610378699884
iteration : 492
train acc:  0.8828125
train loss:  0.298603355884552
train gradient:  0.16456680924987044
iteration : 493
train acc:  0.9140625
train loss:  0.2335573136806488
train gradient:  0.06276276825781153
iteration : 494
train acc:  0.8515625
train loss:  0.2989424169063568
train gradient:  0.110570719505492
iteration : 495
train acc:  0.875
train loss:  0.3083210587501526
train gradient:  0.14482814133056926
iteration : 496
train acc:  0.8671875
train loss:  0.27416911721229553
train gradient:  0.0996129352939868
iteration : 497
train acc:  0.796875
train loss:  0.5267882943153381
train gradient:  0.2765965539825388
iteration : 498
train acc:  0.875
train loss:  0.3042963445186615
train gradient:  0.11368158778744193
iteration : 499
train acc:  0.8671875
train loss:  0.3575601577758789
train gradient:  0.15302821584997647
iteration : 500
train acc:  0.8828125
train loss:  0.2705690562725067
train gradient:  0.13300504030195667
iteration : 501
train acc:  0.921875
train loss:  0.2244199812412262
train gradient:  0.17589711827317706
iteration : 502
train acc:  0.890625
train loss:  0.25886446237564087
train gradient:  0.09835372067404312
iteration : 503
train acc:  0.828125
train loss:  0.3197316527366638
train gradient:  0.1373621502035448
iteration : 504
train acc:  0.8046875
train loss:  0.3945881724357605
train gradient:  0.19303526952966066
iteration : 505
train acc:  0.8359375
train loss:  0.3365463316440582
train gradient:  0.2158142076658666
iteration : 506
train acc:  0.8515625
train loss:  0.2884906530380249
train gradient:  0.07692985830106412
iteration : 507
train acc:  0.8984375
train loss:  0.26599133014678955
train gradient:  0.09659602564283669
iteration : 508
train acc:  0.890625
train loss:  0.27596646547317505
train gradient:  0.11418959605864266
iteration : 509
train acc:  0.8828125
train loss:  0.26743483543395996
train gradient:  0.09947071180865902
iteration : 510
train acc:  0.8359375
train loss:  0.364551842212677
train gradient:  0.3303860393016713
iteration : 511
train acc:  0.8984375
train loss:  0.327311635017395
train gradient:  0.14635370237903972
iteration : 512
train acc:  0.90625
train loss:  0.25960248708724976
train gradient:  0.09839328766991756
iteration : 513
train acc:  0.90625
train loss:  0.2666165828704834
train gradient:  0.10546292969227758
iteration : 514
train acc:  0.8671875
train loss:  0.2964898645877838
train gradient:  0.13402556068690863
iteration : 515
train acc:  0.875
train loss:  0.2673391103744507
train gradient:  0.10416255873867689
iteration : 516
train acc:  0.859375
train loss:  0.34255653619766235
train gradient:  0.13328974425038634
iteration : 517
train acc:  0.859375
train loss:  0.3343107998371124
train gradient:  0.13993162491174616
iteration : 518
train acc:  0.8828125
train loss:  0.27539604902267456
train gradient:  0.13134326424847487
iteration : 519
train acc:  0.84375
train loss:  0.3400810956954956
train gradient:  0.1719512984152175
iteration : 520
train acc:  0.8515625
train loss:  0.3452584743499756
train gradient:  0.1771597668528688
iteration : 521
train acc:  0.9140625
train loss:  0.2520112991333008
train gradient:  0.08806118582471618
iteration : 522
train acc:  0.859375
train loss:  0.32084089517593384
train gradient:  0.12389694077258977
iteration : 523
train acc:  0.8828125
train loss:  0.26608097553253174
train gradient:  0.09321017948888893
iteration : 524
train acc:  0.875
train loss:  0.3135682940483093
train gradient:  0.13343721237512454
iteration : 525
train acc:  0.875
train loss:  0.24143145978450775
train gradient:  0.13562764301301394
iteration : 526
train acc:  0.8828125
train loss:  0.3094187378883362
train gradient:  0.10930699692022708
iteration : 527
train acc:  0.8359375
train loss:  0.3247769773006439
train gradient:  0.12454429302777974
iteration : 528
train acc:  0.8828125
train loss:  0.28063786029815674
train gradient:  0.11448818286400006
iteration : 529
train acc:  0.859375
train loss:  0.29042255878448486
train gradient:  0.09556999286063107
iteration : 530
train acc:  0.8828125
train loss:  0.34852808713912964
train gradient:  0.14399162189630338
iteration : 531
train acc:  0.8125
train loss:  0.3981499671936035
train gradient:  0.2509639635800599
iteration : 532
train acc:  0.890625
train loss:  0.3002902865409851
train gradient:  0.110884570907464
iteration : 533
train acc:  0.8671875
train loss:  0.3566375970840454
train gradient:  0.22989670315660807
iteration : 534
train acc:  0.875
train loss:  0.30979278683662415
train gradient:  0.1218469832026594
iteration : 535
train acc:  0.890625
train loss:  0.2418321818113327
train gradient:  0.09339154202517576
iteration : 536
train acc:  0.8359375
train loss:  0.36245226860046387
train gradient:  0.16636641191835594
iteration : 537
train acc:  0.828125
train loss:  0.378746896982193
train gradient:  0.18383144606005763
iteration : 538
train acc:  0.8828125
train loss:  0.27743881940841675
train gradient:  0.11653466143760996
iteration : 539
train acc:  0.8515625
train loss:  0.3685548007488251
train gradient:  0.19649448990617613
iteration : 540
train acc:  0.9140625
train loss:  0.2647896707057953
train gradient:  0.09920547011991268
iteration : 541
train acc:  0.8828125
train loss:  0.28083932399749756
train gradient:  0.11907571143837431
iteration : 542
train acc:  0.875
train loss:  0.2921130955219269
train gradient:  0.11115009257341324
iteration : 543
train acc:  0.890625
train loss:  0.2819395661354065
train gradient:  0.1246845268426482
iteration : 544
train acc:  0.8359375
train loss:  0.37090063095092773
train gradient:  0.15467491930470278
iteration : 545
train acc:  0.8046875
train loss:  0.4025912284851074
train gradient:  0.19788600981133703
iteration : 546
train acc:  0.9140625
train loss:  0.2511608898639679
train gradient:  0.07891621948982869
iteration : 547
train acc:  0.8515625
train loss:  0.34270182251930237
train gradient:  0.13860766073936492
iteration : 548
train acc:  0.859375
train loss:  0.3030409812927246
train gradient:  0.11689332703851606
iteration : 549
train acc:  0.859375
train loss:  0.2913717031478882
train gradient:  0.13591350492212256
iteration : 550
train acc:  0.8828125
train loss:  0.2524414360523224
train gradient:  0.08480072696237505
iteration : 551
train acc:  0.8515625
train loss:  0.3523120880126953
train gradient:  0.12422545008381658
iteration : 552
train acc:  0.8515625
train loss:  0.3374110460281372
train gradient:  0.12573950798025937
iteration : 553
train acc:  0.84375
train loss:  0.4180390238761902
train gradient:  0.18804233873982865
iteration : 554
train acc:  0.890625
train loss:  0.26075422763824463
train gradient:  0.09497706136527882
iteration : 555
train acc:  0.8203125
train loss:  0.44541051983833313
train gradient:  0.2009199791736088
iteration : 556
train acc:  0.875
train loss:  0.3222421705722809
train gradient:  0.1403442397341351
iteration : 557
train acc:  0.8828125
train loss:  0.2570258378982544
train gradient:  0.07265351452242767
iteration : 558
train acc:  0.8828125
train loss:  0.25321274995803833
train gradient:  0.07351967622520755
iteration : 559
train acc:  0.8984375
train loss:  0.2583793103694916
train gradient:  0.1186049030013902
iteration : 560
train acc:  0.8984375
train loss:  0.29768550395965576
train gradient:  0.09132037997951017
iteration : 561
train acc:  0.84375
train loss:  0.3300846517086029
train gradient:  0.14267388800921033
iteration : 562
train acc:  0.890625
train loss:  0.2913218140602112
train gradient:  0.16249153535251776
iteration : 563
train acc:  0.8515625
train loss:  0.34791100025177
train gradient:  0.17369670998781991
iteration : 564
train acc:  0.859375
train loss:  0.32458069920539856
train gradient:  0.11554695956934298
iteration : 565
train acc:  0.828125
train loss:  0.33618831634521484
train gradient:  0.14606180756698606
iteration : 566
train acc:  0.90625
train loss:  0.2749454379081726
train gradient:  0.07820853503680002
iteration : 567
train acc:  0.890625
train loss:  0.2664094567298889
train gradient:  0.11660854039582373
iteration : 568
train acc:  0.859375
train loss:  0.36487042903900146
train gradient:  0.2208512377167503
iteration : 569
train acc:  0.828125
train loss:  0.35665854811668396
train gradient:  0.1775461576322831
iteration : 570
train acc:  0.90625
train loss:  0.2742586135864258
train gradient:  0.12480553786716463
iteration : 571
train acc:  0.8125
train loss:  0.4518483877182007
train gradient:  0.2158687754084595
iteration : 572
train acc:  0.8515625
train loss:  0.3649480938911438
train gradient:  0.14922302628030876
iteration : 573
train acc:  0.78125
train loss:  0.48862600326538086
train gradient:  0.2759252402003466
iteration : 574
train acc:  0.8828125
train loss:  0.30787432193756104
train gradient:  0.1266682418783098
iteration : 575
train acc:  0.859375
train loss:  0.29895928502082825
train gradient:  0.17705972730614156
iteration : 576
train acc:  0.84375
train loss:  0.364342600107193
train gradient:  0.1485654900767946
iteration : 577
train acc:  0.8515625
train loss:  0.2890092134475708
train gradient:  0.23738816936188586
iteration : 578
train acc:  0.9140625
train loss:  0.22523590922355652
train gradient:  0.07992936813436452
iteration : 579
train acc:  0.84375
train loss:  0.36148321628570557
train gradient:  0.14868281787350124
iteration : 580
train acc:  0.84375
train loss:  0.32954710721969604
train gradient:  0.11112871885981135
iteration : 581
train acc:  0.84375
train loss:  0.39054566621780396
train gradient:  0.14747410770751046
iteration : 582
train acc:  0.859375
train loss:  0.30785369873046875
train gradient:  0.17174340444095942
iteration : 583
train acc:  0.8359375
train loss:  0.34785574674606323
train gradient:  0.1598123432355399
iteration : 584
train acc:  0.90625
train loss:  0.2630707621574402
train gradient:  0.14674061738449604
iteration : 585
train acc:  0.8828125
train loss:  0.2947443127632141
train gradient:  0.105451914115158
iteration : 586
train acc:  0.859375
train loss:  0.3370869755744934
train gradient:  0.11824688970083094
iteration : 587
train acc:  0.8515625
train loss:  0.3575730323791504
train gradient:  0.1300506734609529
iteration : 588
train acc:  0.875
train loss:  0.30556708574295044
train gradient:  0.10654776213524936
iteration : 589
train acc:  0.8984375
train loss:  0.3321530520915985
train gradient:  0.1217128965914646
iteration : 590
train acc:  0.84375
train loss:  0.35318776965141296
train gradient:  0.14575703945450746
iteration : 591
train acc:  0.8671875
train loss:  0.3751641809940338
train gradient:  0.1671859423949909
iteration : 592
train acc:  0.859375
train loss:  0.3237459659576416
train gradient:  0.1696523752510554
iteration : 593
train acc:  0.859375
train loss:  0.32637935876846313
train gradient:  0.11445881262036904
iteration : 594
train acc:  0.828125
train loss:  0.37948161363601685
train gradient:  0.15768761064110953
iteration : 595
train acc:  0.84375
train loss:  0.37232375144958496
train gradient:  0.150163116212153
iteration : 596
train acc:  0.8125
train loss:  0.3519931435585022
train gradient:  0.12614016072451328
iteration : 597
train acc:  0.8203125
train loss:  0.3947179913520813
train gradient:  0.14561207052761693
iteration : 598
train acc:  0.890625
train loss:  0.28851640224456787
train gradient:  0.10198185146866699
iteration : 599
train acc:  0.8671875
train loss:  0.28400880098342896
train gradient:  0.07146768296587583
iteration : 600
train acc:  0.8828125
train loss:  0.24473363161087036
train gradient:  0.08296844165240524
iteration : 601
train acc:  0.9609375
train loss:  0.20451462268829346
train gradient:  0.07323828231617766
iteration : 602
train acc:  0.8828125
train loss:  0.3175809979438782
train gradient:  0.11682524057068415
iteration : 603
train acc:  0.8671875
train loss:  0.30455783009529114
train gradient:  0.10888983707915031
iteration : 604
train acc:  0.859375
train loss:  0.2974122166633606
train gradient:  0.13096650353831013
iteration : 605
train acc:  0.90625
train loss:  0.263387531042099
train gradient:  0.09565102104940391
iteration : 606
train acc:  0.859375
train loss:  0.3415255546569824
train gradient:  0.15688967986579325
iteration : 607
train acc:  0.90625
train loss:  0.27634209394454956
train gradient:  0.40420035153939265
iteration : 608
train acc:  0.8359375
train loss:  0.3518386781215668
train gradient:  0.14639414833182657
iteration : 609
train acc:  0.8515625
train loss:  0.3444410264492035
train gradient:  0.1416934710906001
iteration : 610
train acc:  0.8984375
train loss:  0.24528515338897705
train gradient:  0.07033892638250298
iteration : 611
train acc:  0.8203125
train loss:  0.3172212839126587
train gradient:  0.0927521875521484
iteration : 612
train acc:  0.9296875
train loss:  0.23494872450828552
train gradient:  0.10222949245451583
iteration : 613
train acc:  0.8203125
train loss:  0.3611745238304138
train gradient:  0.13419675815851828
iteration : 614
train acc:  0.8984375
train loss:  0.24818864464759827
train gradient:  0.079744360458681
iteration : 615
train acc:  0.8515625
train loss:  0.35556760430336
train gradient:  0.19166223001339092
iteration : 616
train acc:  0.8984375
train loss:  0.30499565601348877
train gradient:  0.13884890178397358
iteration : 617
train acc:  0.8828125
train loss:  0.37519222497940063
train gradient:  0.16453234813871592
iteration : 618
train acc:  0.90625
train loss:  0.24383777379989624
train gradient:  0.0976924512094701
iteration : 619
train acc:  0.8671875
train loss:  0.3032035231590271
train gradient:  0.12198879435274841
iteration : 620
train acc:  0.90625
train loss:  0.23869571089744568
train gradient:  0.08286611134648988
iteration : 621
train acc:  0.8203125
train loss:  0.43480271100997925
train gradient:  0.2525006161602629
iteration : 622
train acc:  0.859375
train loss:  0.3633033037185669
train gradient:  0.18311763699298855
iteration : 623
train acc:  0.90625
train loss:  0.2540404200553894
train gradient:  0.08142755589505783
iteration : 624
train acc:  0.84375
train loss:  0.30586162209510803
train gradient:  0.1337547410694052
iteration : 625
train acc:  0.875
train loss:  0.3338167071342468
train gradient:  0.1574395626064845
iteration : 626
train acc:  0.875
train loss:  0.32699835300445557
train gradient:  0.15817557745443095
iteration : 627
train acc:  0.8359375
train loss:  0.3280022442340851
train gradient:  0.5868777771206163
iteration : 628
train acc:  0.84375
train loss:  0.3922206461429596
train gradient:  0.1613760015210874
iteration : 629
train acc:  0.8828125
train loss:  0.3140755295753479
train gradient:  0.13710665414573533
iteration : 630
train acc:  0.8359375
train loss:  0.2942497432231903
train gradient:  0.10675060891277585
iteration : 631
train acc:  0.875
train loss:  0.3131411373615265
train gradient:  0.09858960689535297
iteration : 632
train acc:  0.859375
train loss:  0.3196864426136017
train gradient:  0.12126258993472423
iteration : 633
train acc:  0.8828125
train loss:  0.2747308611869812
train gradient:  0.14853800585522065
iteration : 634
train acc:  0.8671875
train loss:  0.2693154215812683
train gradient:  0.12567965050312685
iteration : 635
train acc:  0.828125
train loss:  0.37697920203208923
train gradient:  0.16148166734311065
iteration : 636
train acc:  0.9375
train loss:  0.2119835615158081
train gradient:  0.06649989018545491
iteration : 637
train acc:  0.875
train loss:  0.2928498685359955
train gradient:  0.166060496033262
iteration : 638
train acc:  0.8046875
train loss:  0.3698554039001465
train gradient:  0.14631532828753208
iteration : 639
train acc:  0.84375
train loss:  0.35104092955589294
train gradient:  0.15714978049829323
iteration : 640
train acc:  0.8984375
train loss:  0.24354709684848785
train gradient:  0.067761960434468
iteration : 641
train acc:  0.84375
train loss:  0.3739379942417145
train gradient:  0.15263604643495443
iteration : 642
train acc:  0.8203125
train loss:  0.3627597689628601
train gradient:  0.1395669480007555
iteration : 643
train acc:  0.8515625
train loss:  0.3565721809864044
train gradient:  0.13805150263528876
iteration : 644
train acc:  0.859375
train loss:  0.3282320499420166
train gradient:  0.14930736501556724
iteration : 645
train acc:  0.859375
train loss:  0.2764694094657898
train gradient:  0.10754571750028477
iteration : 646
train acc:  0.8671875
train loss:  0.29153650999069214
train gradient:  0.1066146018115627
iteration : 647
train acc:  0.8828125
train loss:  0.36888858675956726
train gradient:  0.14376930760882164
iteration : 648
train acc:  0.875
train loss:  0.3534937798976898
train gradient:  0.18986719232475585
iteration : 649
train acc:  0.890625
train loss:  0.25010114908218384
train gradient:  0.07188680368618972
iteration : 650
train acc:  0.8515625
train loss:  0.3088203966617584
train gradient:  0.172400159848117
iteration : 651
train acc:  0.90625
train loss:  0.20348379015922546
train gradient:  0.07622773289598192
iteration : 652
train acc:  0.8515625
train loss:  0.3010154962539673
train gradient:  0.11176368669600206
iteration : 653
train acc:  0.8515625
train loss:  0.2896099388599396
train gradient:  0.09905571800828333
iteration : 654
train acc:  0.9140625
train loss:  0.237517848610878
train gradient:  0.06336440686178083
iteration : 655
train acc:  0.828125
train loss:  0.34393519163131714
train gradient:  0.17115051321463037
iteration : 656
train acc:  0.90625
train loss:  0.2895778715610504
train gradient:  0.11755159430981127
iteration : 657
train acc:  0.9140625
train loss:  0.2743704319000244
train gradient:  0.10121116640190335
iteration : 658
train acc:  0.8515625
train loss:  0.3158816695213318
train gradient:  0.14534332648369352
iteration : 659
train acc:  0.8828125
train loss:  0.2911597192287445
train gradient:  0.11975371196434811
iteration : 660
train acc:  0.875
train loss:  0.31643199920654297
train gradient:  0.15736505970531872
iteration : 661
train acc:  0.875
train loss:  0.2864643931388855
train gradient:  0.12171549807811442
iteration : 662
train acc:  0.84375
train loss:  0.34662026166915894
train gradient:  0.13302090438632164
iteration : 663
train acc:  0.8984375
train loss:  0.2369476854801178
train gradient:  0.07835265490418555
iteration : 664
train acc:  0.828125
train loss:  0.3986280560493469
train gradient:  0.16117063135156157
iteration : 665
train acc:  0.8671875
train loss:  0.2602636218070984
train gradient:  0.10280604596899544
iteration : 666
train acc:  0.8984375
train loss:  0.29171526432037354
train gradient:  0.11100228124392468
iteration : 667
train acc:  0.859375
train loss:  0.2507673501968384
train gradient:  0.13038969926768795
iteration : 668
train acc:  0.9375
train loss:  0.20787155628204346
train gradient:  0.08067143984338614
iteration : 669
train acc:  0.875
train loss:  0.2569388151168823
train gradient:  0.12811729801531177
iteration : 670
train acc:  0.8828125
train loss:  0.30682533979415894
train gradient:  0.27486287907575385
iteration : 671
train acc:  0.875
train loss:  0.29300493001937866
train gradient:  0.11369255348227736
iteration : 672
train acc:  0.8984375
train loss:  0.25336647033691406
train gradient:  0.09422009497556963
iteration : 673
train acc:  0.8359375
train loss:  0.30774033069610596
train gradient:  0.13175292300117342
iteration : 674
train acc:  0.8515625
train loss:  0.3781116008758545
train gradient:  0.20501447634321174
iteration : 675
train acc:  0.8125
train loss:  0.3490023910999298
train gradient:  0.1528743017774196
iteration : 676
train acc:  0.8671875
train loss:  0.36671945452690125
train gradient:  0.17083771642669288
iteration : 677
train acc:  0.859375
train loss:  0.3341468274593353
train gradient:  0.11069211844626325
iteration : 678
train acc:  0.8671875
train loss:  0.29822397232055664
train gradient:  0.1104508867858807
iteration : 679
train acc:  0.8984375
train loss:  0.25753527879714966
train gradient:  0.10683977830874135
iteration : 680
train acc:  0.875
train loss:  0.3284905254840851
train gradient:  0.1494097561810818
iteration : 681
train acc:  0.90625
train loss:  0.27222001552581787
train gradient:  0.07672170475595676
iteration : 682
train acc:  0.8671875
train loss:  0.32073312997817993
train gradient:  0.12451890476132646
iteration : 683
train acc:  0.9296875
train loss:  0.19307786226272583
train gradient:  0.0634261994544865
iteration : 684
train acc:  0.9140625
train loss:  0.28301531076431274
train gradient:  0.12329829424108082
iteration : 685
train acc:  0.828125
train loss:  0.38868045806884766
train gradient:  0.21150101757422085
iteration : 686
train acc:  0.84375
train loss:  0.3346561789512634
train gradient:  0.16377986429490032
iteration : 687
train acc:  0.8515625
train loss:  0.30119743943214417
train gradient:  0.08189581435938598
iteration : 688
train acc:  0.921875
train loss:  0.21931812167167664
train gradient:  0.11218907461449656
iteration : 689
train acc:  0.9140625
train loss:  0.2309962660074234
train gradient:  0.07083057928366383
iteration : 690
train acc:  0.859375
train loss:  0.3555859923362732
train gradient:  0.16780997193512856
iteration : 691
train acc:  0.8515625
train loss:  0.2943621277809143
train gradient:  0.12420474044013041
iteration : 692
train acc:  0.8515625
train loss:  0.3838089108467102
train gradient:  0.16939527803498305
iteration : 693
train acc:  0.875
train loss:  0.2734273076057434
train gradient:  0.22113795296344668
iteration : 694
train acc:  0.84375
train loss:  0.3516075611114502
train gradient:  0.157304186677221
iteration : 695
train acc:  0.859375
train loss:  0.30024394392967224
train gradient:  0.11576692268243885
iteration : 696
train acc:  0.84375
train loss:  0.3269612789154053
train gradient:  0.14525440729325828
iteration : 697
train acc:  0.8203125
train loss:  0.3607756495475769
train gradient:  0.18582163772995391
iteration : 698
train acc:  0.875
train loss:  0.3042347729206085
train gradient:  0.10207898722608229
iteration : 699
train acc:  0.8671875
train loss:  0.32845360040664673
train gradient:  0.13676341802213335
iteration : 700
train acc:  0.8828125
train loss:  0.29810208082199097
train gradient:  0.18096534361198757
iteration : 701
train acc:  0.8515625
train loss:  0.3949015438556671
train gradient:  0.20277339232090683
iteration : 702
train acc:  0.890625
train loss:  0.2482713758945465
train gradient:  0.11798059790544728
iteration : 703
train acc:  0.828125
train loss:  0.36832907795906067
train gradient:  0.22251760346204957
iteration : 704
train acc:  0.78125
train loss:  0.4299992024898529
train gradient:  0.23440978461292516
iteration : 705
train acc:  0.8828125
train loss:  0.3128253221511841
train gradient:  0.13769931281461542
iteration : 706
train acc:  0.9140625
train loss:  0.22714176774024963
train gradient:  0.08619740667341935
iteration : 707
train acc:  0.8828125
train loss:  0.25489819049835205
train gradient:  0.07991375310044944
iteration : 708
train acc:  0.828125
train loss:  0.3688795566558838
train gradient:  0.1602489400037442
iteration : 709
train acc:  0.8125
train loss:  0.3994341194629669
train gradient:  0.17543419860542253
iteration : 710
train acc:  0.8203125
train loss:  0.4261873662471771
train gradient:  0.234443016213237
iteration : 711
train acc:  0.828125
train loss:  0.39768660068511963
train gradient:  0.189026028610481
iteration : 712
train acc:  0.8671875
train loss:  0.35645031929016113
train gradient:  0.21514037732811753
iteration : 713
train acc:  0.8984375
train loss:  0.24381062388420105
train gradient:  0.07569180813478997
iteration : 714
train acc:  0.8359375
train loss:  0.3469564616680145
train gradient:  0.15416804945579526
iteration : 715
train acc:  0.9140625
train loss:  0.22891183197498322
train gradient:  0.08290583350263264
iteration : 716
train acc:  0.8671875
train loss:  0.3215412199497223
train gradient:  0.10521909730052904
iteration : 717
train acc:  0.8203125
train loss:  0.41945070028305054
train gradient:  0.24658735814240285
iteration : 718
train acc:  0.84375
train loss:  0.34957122802734375
train gradient:  0.14915908721793775
iteration : 719
train acc:  0.8359375
train loss:  0.31556421518325806
train gradient:  0.11015338453125519
iteration : 720
train acc:  0.8828125
train loss:  0.35057932138442993
train gradient:  0.13452765908357586
iteration : 721
train acc:  0.875
train loss:  0.2946576774120331
train gradient:  0.1029579432220654
iteration : 722
train acc:  0.8671875
train loss:  0.29255542159080505
train gradient:  0.17116840346801943
iteration : 723
train acc:  0.875
train loss:  0.3093116283416748
train gradient:  0.13792348401167698
iteration : 724
train acc:  0.8515625
train loss:  0.3484782576560974
train gradient:  0.19083488190544687
iteration : 725
train acc:  0.8515625
train loss:  0.31864267587661743
train gradient:  0.10135113130020476
iteration : 726
train acc:  0.875
train loss:  0.36472034454345703
train gradient:  0.13802081603868543
iteration : 727
train acc:  0.875
train loss:  0.3150515556335449
train gradient:  0.11140196974734651
iteration : 728
train acc:  0.84375
train loss:  0.2684156894683838
train gradient:  0.08142075833272791
iteration : 729
train acc:  0.875
train loss:  0.2937513589859009
train gradient:  0.1059283245125141
iteration : 730
train acc:  0.859375
train loss:  0.30561745166778564
train gradient:  0.12033915122104699
iteration : 731
train acc:  0.828125
train loss:  0.338773638010025
train gradient:  0.10454764851590388
iteration : 732
train acc:  0.875
train loss:  0.29311829805374146
train gradient:  0.19874938978968726
iteration : 733
train acc:  0.7890625
train loss:  0.402713418006897
train gradient:  0.1430709984979725
iteration : 734
train acc:  0.8984375
train loss:  0.28993234038352966
train gradient:  0.1883267192947339
iteration : 735
train acc:  0.875
train loss:  0.32634344696998596
train gradient:  0.14958322195648355
iteration : 736
train acc:  0.8203125
train loss:  0.35634294152259827
train gradient:  0.17638235988076514
iteration : 737
train acc:  0.921875
train loss:  0.2277754545211792
train gradient:  0.07167614197922535
iteration : 738
train acc:  0.890625
train loss:  0.2766295075416565
train gradient:  0.11443481364062646
iteration : 739
train acc:  0.8984375
train loss:  0.20835751295089722
train gradient:  0.07237629090707107
iteration : 740
train acc:  0.859375
train loss:  0.31001946330070496
train gradient:  0.11661547415947343
iteration : 741
train acc:  0.859375
train loss:  0.32952871918678284
train gradient:  0.14148970237989617
iteration : 742
train acc:  0.875
train loss:  0.2530825734138489
train gradient:  0.06810248485739959
iteration : 743
train acc:  0.890625
train loss:  0.27213796973228455
train gradient:  0.1332377111484362
iteration : 744
train acc:  0.890625
train loss:  0.28145891427993774
train gradient:  0.09759115333829546
iteration : 745
train acc:  0.84375
train loss:  0.32691603899002075
train gradient:  0.10354935603967237
iteration : 746
train acc:  0.828125
train loss:  0.34788960218429565
train gradient:  0.1324849204762792
iteration : 747
train acc:  0.8203125
train loss:  0.35561519861221313
train gradient:  0.12065247940922409
iteration : 748
train acc:  0.8671875
train loss:  0.2785051465034485
train gradient:  0.0924168156274926
iteration : 749
train acc:  0.859375
train loss:  0.2744262218475342
train gradient:  0.12058985562613368
iteration : 750
train acc:  0.890625
train loss:  0.27574461698532104
train gradient:  0.09436157394511117
iteration : 751
train acc:  0.8203125
train loss:  0.43058404326438904
train gradient:  0.22143683578379478
iteration : 752
train acc:  0.8515625
train loss:  0.3357427716255188
train gradient:  0.13625823130783696
iteration : 753
train acc:  0.8359375
train loss:  0.3817838430404663
train gradient:  0.21163358906556484
iteration : 754
train acc:  0.9140625
train loss:  0.2564419209957123
train gradient:  0.09982135963797048
iteration : 755
train acc:  0.8671875
train loss:  0.2842429578304291
train gradient:  0.08996452382959697
iteration : 756
train acc:  0.859375
train loss:  0.3568199574947357
train gradient:  0.22505528651466197
iteration : 757
train acc:  0.8828125
train loss:  0.2517542243003845
train gradient:  0.09962976429528385
iteration : 758
train acc:  0.8046875
train loss:  0.4246900677680969
train gradient:  0.25665512604112395
iteration : 759
train acc:  0.8828125
train loss:  0.26244762539863586
train gradient:  0.07576918704540714
iteration : 760
train acc:  0.875
train loss:  0.28708595037460327
train gradient:  0.11796486888466612
iteration : 761
train acc:  0.8671875
train loss:  0.3031828999519348
train gradient:  0.11308882259493457
iteration : 762
train acc:  0.859375
train loss:  0.2994069457054138
train gradient:  0.10316914079231883
iteration : 763
train acc:  0.8515625
train loss:  0.32910311222076416
train gradient:  0.1268737072465262
iteration : 764
train acc:  0.90625
train loss:  0.3053795099258423
train gradient:  0.11910907005627128
iteration : 765
train acc:  0.8203125
train loss:  0.3285801410675049
train gradient:  0.11056554403187036
iteration : 766
train acc:  0.8359375
train loss:  0.3502822518348694
train gradient:  0.17802834145525415
iteration : 767
train acc:  0.8984375
train loss:  0.2429390847682953
train gradient:  0.09785595086564343
iteration : 768
train acc:  0.8125
train loss:  0.3733341097831726
train gradient:  0.262734912333324
iteration : 769
train acc:  0.8203125
train loss:  0.37095439434051514
train gradient:  0.17963005934331844
iteration : 770
train acc:  0.8828125
train loss:  0.290436327457428
train gradient:  0.13724393275394037
iteration : 771
train acc:  0.8828125
train loss:  0.29266825318336487
train gradient:  0.11057454917041941
iteration : 772
train acc:  0.8671875
train loss:  0.3219643831253052
train gradient:  0.12749288713227142
iteration : 773
train acc:  0.8515625
train loss:  0.2936994135379791
train gradient:  0.11201735371598776
iteration : 774
train acc:  0.796875
train loss:  0.39302194118499756
train gradient:  0.1632003105371665
iteration : 775
train acc:  0.859375
train loss:  0.32344889640808105
train gradient:  0.12033931006953219
iteration : 776
train acc:  0.9140625
train loss:  0.26868942379951477
train gradient:  0.10597999015338284
iteration : 777
train acc:  0.859375
train loss:  0.3043581247329712
train gradient:  0.13377442475648496
iteration : 778
train acc:  0.8828125
train loss:  0.2707265019416809
train gradient:  0.09070230936753089
iteration : 779
train acc:  0.8828125
train loss:  0.27964797616004944
train gradient:  0.07702672240543297
iteration : 780
train acc:  0.8828125
train loss:  0.2778570055961609
train gradient:  0.14318003776186145
iteration : 781
train acc:  0.875
train loss:  0.23764201998710632
train gradient:  0.11256487724958375
iteration : 782
train acc:  0.8515625
train loss:  0.3058266043663025
train gradient:  0.10600930798207774
iteration : 783
train acc:  0.84375
train loss:  0.3090057075023651
train gradient:  0.10542163331292774
iteration : 784
train acc:  0.859375
train loss:  0.3465428352355957
train gradient:  0.2082312656180602
iteration : 785
train acc:  0.7890625
train loss:  0.47089260816574097
train gradient:  0.2286489416369658
iteration : 786
train acc:  0.8828125
train loss:  0.32546424865722656
train gradient:  0.19959678398953584
iteration : 787
train acc:  0.8359375
train loss:  0.3075176477432251
train gradient:  0.3391839123461759
iteration : 788
train acc:  0.828125
train loss:  0.34980106353759766
train gradient:  0.12526546837849917
iteration : 789
train acc:  0.890625
train loss:  0.30871373414993286
train gradient:  0.0988831992420108
iteration : 790
train acc:  0.875
train loss:  0.2549550533294678
train gradient:  0.1128158166798418
iteration : 791
train acc:  0.875
train loss:  0.3049367070198059
train gradient:  0.09977454567418716
iteration : 792
train acc:  0.890625
train loss:  0.24320057034492493
train gradient:  0.07997266199824596
iteration : 793
train acc:  0.890625
train loss:  0.260959655046463
train gradient:  0.07222925550932696
iteration : 794
train acc:  0.8828125
train loss:  0.3304938077926636
train gradient:  0.1974615077847775
iteration : 795
train acc:  0.859375
train loss:  0.2870568037033081
train gradient:  0.1363749492017845
iteration : 796
train acc:  0.8984375
train loss:  0.262953519821167
train gradient:  0.09069576561424926
iteration : 797
train acc:  0.8359375
train loss:  0.3074171841144562
train gradient:  0.13843295124510985
iteration : 798
train acc:  0.859375
train loss:  0.35038435459136963
train gradient:  0.12862917399616824
iteration : 799
train acc:  0.84375
train loss:  0.39619025588035583
train gradient:  0.18507088498121993
iteration : 800
train acc:  0.8046875
train loss:  0.3971456289291382
train gradient:  0.1701193321551041
iteration : 801
train acc:  0.875
train loss:  0.33856475353240967
train gradient:  0.15839331428092257
iteration : 802
train acc:  0.859375
train loss:  0.35513433814048767
train gradient:  0.14787268232489548
iteration : 803
train acc:  0.8828125
train loss:  0.2865288257598877
train gradient:  0.10610944167522775
iteration : 804
train acc:  0.859375
train loss:  0.2909969091415405
train gradient:  0.11140318721020125
iteration : 805
train acc:  0.84375
train loss:  0.351828396320343
train gradient:  0.16036645229156649
iteration : 806
train acc:  0.9375
train loss:  0.20243117213249207
train gradient:  0.08541487004211255
iteration : 807
train acc:  0.8671875
train loss:  0.3266725540161133
train gradient:  0.194521983262052
iteration : 808
train acc:  0.890625
train loss:  0.32761138677597046
train gradient:  0.11031751844589816
iteration : 809
train acc:  0.8125
train loss:  0.3908727467060089
train gradient:  0.1532418945483383
iteration : 810
train acc:  0.890625
train loss:  0.2721216082572937
train gradient:  0.11009903299044518
iteration : 811
train acc:  0.8359375
train loss:  0.409503698348999
train gradient:  0.2584141373279963
iteration : 812
train acc:  0.875
train loss:  0.3380523920059204
train gradient:  0.1971250763251494
iteration : 813
train acc:  0.890625
train loss:  0.2784709930419922
train gradient:  0.10824812149011445
iteration : 814
train acc:  0.8828125
train loss:  0.28349995613098145
train gradient:  0.20313913466336656
iteration : 815
train acc:  0.8359375
train loss:  0.38483160734176636
train gradient:  0.14836764224770338
iteration : 816
train acc:  0.90625
train loss:  0.2892897129058838
train gradient:  0.07383630952390957
iteration : 817
train acc:  0.84375
train loss:  0.39752453565597534
train gradient:  0.20252628615301282
iteration : 818
train acc:  0.84375
train loss:  0.32448700070381165
train gradient:  0.1398495030107949
iteration : 819
train acc:  0.859375
train loss:  0.3226962685585022
train gradient:  0.12598980705563528
iteration : 820
train acc:  0.8671875
train loss:  0.34764158725738525
train gradient:  0.13736289328659634
iteration : 821
train acc:  0.875
train loss:  0.2883443534374237
train gradient:  0.10183494566893055
iteration : 822
train acc:  0.859375
train loss:  0.3683686852455139
train gradient:  0.20509976418739034
iteration : 823
train acc:  0.8359375
train loss:  0.34244680404663086
train gradient:  0.1709649975611594
iteration : 824
train acc:  0.8671875
train loss:  0.3248995542526245
train gradient:  0.12864021327938396
iteration : 825
train acc:  0.84375
train loss:  0.3326953649520874
train gradient:  0.16364831370464739
iteration : 826
train acc:  0.890625
train loss:  0.2919883728027344
train gradient:  0.11517261188533004
iteration : 827
train acc:  0.828125
train loss:  0.3668057322502136
train gradient:  0.1420715518364291
iteration : 828
train acc:  0.8828125
train loss:  0.34040725231170654
train gradient:  0.11176112307653507
iteration : 829
train acc:  0.9140625
train loss:  0.2745606303215027
train gradient:  0.07892958109207565
iteration : 830
train acc:  0.8359375
train loss:  0.3319759964942932
train gradient:  0.12137952424944035
iteration : 831
train acc:  0.875
train loss:  0.2814593017101288
train gradient:  0.12812972767037317
iteration : 832
train acc:  0.8984375
train loss:  0.2450459897518158
train gradient:  0.13851354176564035
iteration : 833
train acc:  0.8515625
train loss:  0.35879701375961304
train gradient:  0.13614033252160082
iteration : 834
train acc:  0.8203125
train loss:  0.39742445945739746
train gradient:  0.20056152981468073
iteration : 835
train acc:  0.8828125
train loss:  0.3329172134399414
train gradient:  0.18619657591042177
iteration : 836
train acc:  0.8671875
train loss:  0.2911277413368225
train gradient:  0.14537773384171193
iteration : 837
train acc:  0.859375
train loss:  0.30751466751098633
train gradient:  0.11713268918978449
iteration : 838
train acc:  0.890625
train loss:  0.253908634185791
train gradient:  0.09838312369961663
iteration : 839
train acc:  0.9296875
train loss:  0.2723119854927063
train gradient:  0.0865853987070163
iteration : 840
train acc:  0.8984375
train loss:  0.27154773473739624
train gradient:  0.08602620364000321
iteration : 841
train acc:  0.875
train loss:  0.2899324893951416
train gradient:  0.0946847162790836
iteration : 842
train acc:  0.8203125
train loss:  0.3793727457523346
train gradient:  0.17105378882156064
iteration : 843
train acc:  0.8828125
train loss:  0.24254752695560455
train gradient:  0.07518283787507594
iteration : 844
train acc:  0.828125
train loss:  0.3809567093849182
train gradient:  0.2197462899152622
iteration : 845
train acc:  0.8828125
train loss:  0.3303276300430298
train gradient:  0.1157317388483192
iteration : 846
train acc:  0.8515625
train loss:  0.31382572650909424
train gradient:  0.09810746577382198
iteration : 847
train acc:  0.8671875
train loss:  0.34415239095687866
train gradient:  0.15236753910276518
iteration : 848
train acc:  0.8515625
train loss:  0.3510172367095947
train gradient:  0.14575830482884886
iteration : 849
train acc:  0.90625
train loss:  0.2697128355503082
train gradient:  0.08009650481097919
iteration : 850
train acc:  0.859375
train loss:  0.295958012342453
train gradient:  0.0810050790843156
iteration : 851
train acc:  0.8125
train loss:  0.3545062839984894
train gradient:  0.1482230521318481
iteration : 852
train acc:  0.9296875
train loss:  0.23303624987602234
train gradient:  0.06445389622538765
iteration : 853
train acc:  0.8203125
train loss:  0.44344431161880493
train gradient:  0.2043010620080064
iteration : 854
train acc:  0.8828125
train loss:  0.27042222023010254
train gradient:  0.09486400712494178
iteration : 855
train acc:  0.875
train loss:  0.3060761094093323
train gradient:  0.11568669913048779
iteration : 856
train acc:  0.8515625
train loss:  0.2667556703090668
train gradient:  0.09805103956376886
iteration : 857
train acc:  0.8515625
train loss:  0.33848345279693604
train gradient:  0.17177798761045648
iteration : 858
train acc:  0.875
train loss:  0.2614652216434479
train gradient:  0.07984502993580311
iteration : 859
train acc:  0.8515625
train loss:  0.3227366805076599
train gradient:  0.14928460709331864
iteration : 860
train acc:  0.859375
train loss:  0.3220093548297882
train gradient:  0.21256067874920684
iteration : 861
train acc:  0.859375
train loss:  0.3580007553100586
train gradient:  0.1509048618744068
iteration : 862
train acc:  0.8359375
train loss:  0.3874910771846771
train gradient:  0.16171392351228825
iteration : 863
train acc:  0.8515625
train loss:  0.3116300106048584
train gradient:  0.13087773531708274
iteration : 864
train acc:  0.90625
train loss:  0.2717856764793396
train gradient:  0.11236527033312496
iteration : 865
train acc:  0.8359375
train loss:  0.2944256365299225
train gradient:  0.09289738545323423
iteration : 866
train acc:  0.859375
train loss:  0.28244680166244507
train gradient:  0.1345344891824587
iteration : 867
train acc:  0.921875
train loss:  0.25984013080596924
train gradient:  0.08798220360753094
iteration : 868
train acc:  0.8515625
train loss:  0.2877139151096344
train gradient:  0.11284140136387434
iteration : 869
train acc:  0.921875
train loss:  0.22427788376808167
train gradient:  0.07124036650858293
iteration : 870
train acc:  0.8515625
train loss:  0.3224709630012512
train gradient:  0.12589692365048735
iteration : 871
train acc:  0.8828125
train loss:  0.2928793728351593
train gradient:  0.10476071889449062
iteration : 872
train acc:  0.8046875
train loss:  0.41438186168670654
train gradient:  0.1505357244713107
iteration : 873
train acc:  0.859375
train loss:  0.32702645659446716
train gradient:  0.11103531920399862
iteration : 874
train acc:  0.8671875
train loss:  0.26514163613319397
train gradient:  0.11444600326005167
iteration : 875
train acc:  0.90625
train loss:  0.23504026234149933
train gradient:  0.07449099070445553
iteration : 876
train acc:  0.84375
train loss:  0.30668753385543823
train gradient:  0.13051236604934996
iteration : 877
train acc:  0.8828125
train loss:  0.2823094129562378
train gradient:  0.10366797209109099
iteration : 878
train acc:  0.8515625
train loss:  0.4007682204246521
train gradient:  0.25890731006344975
iteration : 879
train acc:  0.890625
train loss:  0.2543901205062866
train gradient:  0.08168243756537552
iteration : 880
train acc:  0.875
train loss:  0.3179418742656708
train gradient:  0.19610245473731985
iteration : 881
train acc:  0.8359375
train loss:  0.34957289695739746
train gradient:  0.21808949416809847
iteration : 882
train acc:  0.828125
train loss:  0.36416488885879517
train gradient:  0.13772919224909597
iteration : 883
train acc:  0.9140625
train loss:  0.2990662455558777
train gradient:  0.16659947585138546
iteration : 884
train acc:  0.859375
train loss:  0.3121698796749115
train gradient:  0.10399198965192682
iteration : 885
train acc:  0.8828125
train loss:  0.2768118977546692
train gradient:  0.10316199658151858
iteration : 886
train acc:  0.890625
train loss:  0.28711438179016113
train gradient:  0.1429803666270686
iteration : 887
train acc:  0.8984375
train loss:  0.2502385079860687
train gradient:  0.45611166517689483
iteration : 888
train acc:  0.8515625
train loss:  0.316295862197876
train gradient:  0.1193948506578865
iteration : 889
train acc:  0.8828125
train loss:  0.302789568901062
train gradient:  0.10144232052541956
iteration : 890
train acc:  0.8671875
train loss:  0.32927894592285156
train gradient:  0.13188276341723454
iteration : 891
train acc:  0.921875
train loss:  0.24405670166015625
train gradient:  0.0988624986925908
iteration : 892
train acc:  0.828125
train loss:  0.3518974781036377
train gradient:  0.14977030157135363
iteration : 893
train acc:  0.8984375
train loss:  0.28319594264030457
train gradient:  0.11921707582974811
iteration : 894
train acc:  0.8828125
train loss:  0.31596267223358154
train gradient:  0.16362311698479054
iteration : 895
train acc:  0.84375
train loss:  0.37210309505462646
train gradient:  0.2381549501677685
iteration : 896
train acc:  0.84375
train loss:  0.39066654443740845
train gradient:  0.17713514717285977
iteration : 897
train acc:  0.8359375
train loss:  0.34610673785209656
train gradient:  0.14416893880335938
iteration : 898
train acc:  0.875
train loss:  0.3358333706855774
train gradient:  0.12032750569485984
iteration : 899
train acc:  0.8984375
train loss:  0.24394458532333374
train gradient:  0.11351046980212419
iteration : 900
train acc:  0.8359375
train loss:  0.29917436838150024
train gradient:  0.17058521246470057
iteration : 901
train acc:  0.875
train loss:  0.3292735815048218
train gradient:  0.10378468832260142
iteration : 902
train acc:  0.9140625
train loss:  0.2613182067871094
train gradient:  0.12387588601585814
iteration : 903
train acc:  0.8984375
train loss:  0.29632294178009033
train gradient:  0.12468777788474207
iteration : 904
train acc:  0.8359375
train loss:  0.30584824085235596
train gradient:  0.10424871738909922
iteration : 905
train acc:  0.8984375
train loss:  0.24227769672870636
train gradient:  0.10374411680681031
iteration : 906
train acc:  0.8359375
train loss:  0.2971723675727844
train gradient:  0.11742736853517696
iteration : 907
train acc:  0.84375
train loss:  0.3329915404319763
train gradient:  0.17672826018714993
iteration : 908
train acc:  0.8984375
train loss:  0.26142334938049316
train gradient:  0.08227590751554113
iteration : 909
train acc:  0.8984375
train loss:  0.24008867144584656
train gradient:  0.08977746303916663
iteration : 910
train acc:  0.90625
train loss:  0.2507633566856384
train gradient:  0.10501393038545771
iteration : 911
train acc:  0.84375
train loss:  0.3524867296218872
train gradient:  0.14523215870458825
iteration : 912
train acc:  0.875
train loss:  0.3003917932510376
train gradient:  0.10865128153324949
iteration : 913
train acc:  0.8828125
train loss:  0.3110126852989197
train gradient:  0.11905580210389122
iteration : 914
train acc:  0.890625
train loss:  0.23892799019813538
train gradient:  0.0705711961535143
iteration : 915
train acc:  0.8671875
train loss:  0.3442190885543823
train gradient:  0.14422359624442127
iteration : 916
train acc:  0.921875
train loss:  0.2183258831501007
train gradient:  0.09475016065366515
iteration : 917
train acc:  0.859375
train loss:  0.281856894493103
train gradient:  0.11936396876484766
iteration : 918
train acc:  0.8671875
train loss:  0.2934294044971466
train gradient:  0.14930145196465322
iteration : 919
train acc:  0.8671875
train loss:  0.3509209156036377
train gradient:  0.17433250659773997
iteration : 920
train acc:  0.84375
train loss:  0.3276822566986084
train gradient:  0.1528408531989095
iteration : 921
train acc:  0.8828125
train loss:  0.3270098865032196
train gradient:  0.12850617064699815
iteration : 922
train acc:  0.8984375
train loss:  0.2755590081214905
train gradient:  0.08121737858376575
iteration : 923
train acc:  0.9375
train loss:  0.1921440213918686
train gradient:  0.06529747224841174
iteration : 924
train acc:  0.8515625
train loss:  0.3730252683162689
train gradient:  0.1936435784890576
iteration : 925
train acc:  0.8671875
train loss:  0.2550528049468994
train gradient:  0.11136635958744251
iteration : 926
train acc:  0.859375
train loss:  0.2796444296836853
train gradient:  0.1265957577000928
iteration : 927
train acc:  0.890625
train loss:  0.2705487012863159
train gradient:  0.10039981330446493
iteration : 928
train acc:  0.8125
train loss:  0.3507699966430664
train gradient:  0.17587256163713055
iteration : 929
train acc:  0.8984375
train loss:  0.2628183960914612
train gradient:  0.08690867693450673
iteration : 930
train acc:  0.890625
train loss:  0.2663382887840271
train gradient:  0.08252785108847283
iteration : 931
train acc:  0.859375
train loss:  0.33547553420066833
train gradient:  0.12198279670766446
iteration : 932
train acc:  0.8515625
train loss:  0.30527639389038086
train gradient:  0.17112307288660847
iteration : 933
train acc:  0.8828125
train loss:  0.2917712330818176
train gradient:  0.1274134724575955
iteration : 934
train acc:  0.8828125
train loss:  0.28909796476364136
train gradient:  0.13829240815810556
iteration : 935
train acc:  0.890625
train loss:  0.24244041740894318
train gradient:  0.09835225847316043
iteration : 936
train acc:  0.890625
train loss:  0.26431483030319214
train gradient:  0.11403369215178669
iteration : 937
train acc:  0.8828125
train loss:  0.31309235095977783
train gradient:  0.2060547481920313
iteration : 938
train acc:  0.875
train loss:  0.30565524101257324
train gradient:  0.10674471381104851
iteration : 939
train acc:  0.859375
train loss:  0.3058430552482605
train gradient:  0.16440326956129847
iteration : 940
train acc:  0.9296875
train loss:  0.21333205699920654
train gradient:  0.09002321189376707
iteration : 941
train acc:  0.84375
train loss:  0.3146468997001648
train gradient:  0.21623172989592881
iteration : 942
train acc:  0.859375
train loss:  0.32750001549720764
train gradient:  0.14791248325001538
iteration : 943
train acc:  0.84375
train loss:  0.31589192152023315
train gradient:  0.164047839230075
iteration : 944
train acc:  0.921875
train loss:  0.26065322756767273
train gradient:  0.07455855059847907
iteration : 945
train acc:  0.8203125
train loss:  0.39207902550697327
train gradient:  0.17662882078099967
iteration : 946
train acc:  0.8671875
train loss:  0.292802631855011
train gradient:  0.12524700857546944
iteration : 947
train acc:  0.8671875
train loss:  0.305202454328537
train gradient:  0.14429233759852889
iteration : 948
train acc:  0.859375
train loss:  0.31950849294662476
train gradient:  0.12474919688762659
iteration : 949
train acc:  0.828125
train loss:  0.4115194082260132
train gradient:  0.16315162704340402
iteration : 950
train acc:  0.8125
train loss:  0.40408778190612793
train gradient:  0.19508088536294807
iteration : 951
train acc:  0.890625
train loss:  0.27782076597213745
train gradient:  0.07514260094371435
iteration : 952
train acc:  0.84375
train loss:  0.32899200916290283
train gradient:  0.16276849279954358
iteration : 953
train acc:  0.8984375
train loss:  0.29727795720100403
train gradient:  0.12527495052708812
iteration : 954
train acc:  0.8828125
train loss:  0.26876431703567505
train gradient:  0.20755887026768566
iteration : 955
train acc:  0.8359375
train loss:  0.3564646244049072
train gradient:  0.20901214749801605
iteration : 956
train acc:  0.890625
train loss:  0.30407947301864624
train gradient:  0.13606642200170158
iteration : 957
train acc:  0.90625
train loss:  0.22972963750362396
train gradient:  0.08502156174042187
iteration : 958
train acc:  0.8515625
train loss:  0.2814481854438782
train gradient:  0.08776302073787681
iteration : 959
train acc:  0.875
train loss:  0.38203951716423035
train gradient:  0.2168151481095463
iteration : 960
train acc:  0.9140625
train loss:  0.24284744262695312
train gradient:  0.10203846163771677
iteration : 961
train acc:  0.859375
train loss:  0.27043119072914124
train gradient:  0.1395743612960411
iteration : 962
train acc:  0.890625
train loss:  0.29349780082702637
train gradient:  0.10418784876840619
iteration : 963
train acc:  0.8671875
train loss:  0.34604978561401367
train gradient:  0.14480204218248816
iteration : 964
train acc:  0.8359375
train loss:  0.36464595794677734
train gradient:  0.1742529079088163
iteration : 965
train acc:  0.90625
train loss:  0.251180499792099
train gradient:  0.08222580068605233
iteration : 966
train acc:  0.9296875
train loss:  0.23858138918876648
train gradient:  0.09163726805715193
iteration : 967
train acc:  0.8359375
train loss:  0.40407222509384155
train gradient:  0.23357800737481815
iteration : 968
train acc:  0.90625
train loss:  0.26865431666374207
train gradient:  0.09737230318620989
iteration : 969
train acc:  0.890625
train loss:  0.2925410866737366
train gradient:  0.11355377551547143
iteration : 970
train acc:  0.8984375
train loss:  0.33488917350769043
train gradient:  0.12137839149635485
iteration : 971
train acc:  0.8515625
train loss:  0.30334770679473877
train gradient:  0.10567284822779154
iteration : 972
train acc:  0.828125
train loss:  0.35259944200515747
train gradient:  0.21888045507446582
iteration : 973
train acc:  0.8125
train loss:  0.39543581008911133
train gradient:  0.19288562895978423
iteration : 974
train acc:  0.8671875
train loss:  0.3617144227027893
train gradient:  0.1477912237191828
iteration : 975
train acc:  0.890625
train loss:  0.2644796371459961
train gradient:  0.13130936625661194
iteration : 976
train acc:  0.875
train loss:  0.2752661108970642
train gradient:  0.11051110200843862
iteration : 977
train acc:  0.8359375
train loss:  0.37820035219192505
train gradient:  0.20815053675484652
iteration : 978
train acc:  0.921875
train loss:  0.27517038583755493
train gradient:  0.09554591971192404
iteration : 979
train acc:  0.9140625
train loss:  0.25383615493774414
train gradient:  0.12000028494988249
iteration : 980
train acc:  0.890625
train loss:  0.2349587380886078
train gradient:  0.16280174640421385
iteration : 981
train acc:  0.875
train loss:  0.3594582676887512
train gradient:  0.17132206974249076
iteration : 982
train acc:  0.8203125
train loss:  0.383939266204834
train gradient:  0.2509808281989175
iteration : 983
train acc:  0.8359375
train loss:  0.37449318170547485
train gradient:  0.2165524216352614
iteration : 984
train acc:  0.890625
train loss:  0.2432710826396942
train gradient:  0.10810829095693
iteration : 985
train acc:  0.875
train loss:  0.2872593104839325
train gradient:  0.08311905092408654
iteration : 986
train acc:  0.8203125
train loss:  0.2732618451118469
train gradient:  0.11901086798282158
iteration : 987
train acc:  0.859375
train loss:  0.2815489172935486
train gradient:  0.10524438038154817
iteration : 988
train acc:  0.8828125
train loss:  0.2552115321159363
train gradient:  0.1271656009578917
iteration : 989
train acc:  0.8515625
train loss:  0.33292651176452637
train gradient:  0.16978587181763954
iteration : 990
train acc:  0.8671875
train loss:  0.3316241204738617
train gradient:  0.11800204356684381
iteration : 991
train acc:  0.875
train loss:  0.30648088455200195
train gradient:  0.141723388105008
iteration : 992
train acc:  0.875
train loss:  0.31182578206062317
train gradient:  0.13785202799100757
iteration : 993
train acc:  0.8125
train loss:  0.3427295684814453
train gradient:  0.10892203330395062
iteration : 994
train acc:  0.8828125
train loss:  0.2937755584716797
train gradient:  0.13634788341374815
iteration : 995
train acc:  0.875
train loss:  0.32095715403556824
train gradient:  0.10409636473279973
iteration : 996
train acc:  0.8359375
train loss:  0.37924036383628845
train gradient:  0.18493985177191963
iteration : 997
train acc:  0.8828125
train loss:  0.23410242795944214
train gradient:  0.07817590761522518
iteration : 998
train acc:  0.921875
train loss:  0.25854822993278503
train gradient:  0.14028404470356182
iteration : 999
train acc:  0.8125
train loss:  0.34984469413757324
train gradient:  0.13131002010264456
iteration : 1000
train acc:  0.875
train loss:  0.2898188829421997
train gradient:  0.10430767860776133
iteration : 1001
train acc:  0.8828125
train loss:  0.27847397327423096
train gradient:  0.12341666232831286
iteration : 1002
train acc:  0.8828125
train loss:  0.30660948157310486
train gradient:  0.09934149459826067
iteration : 1003
train acc:  0.890625
train loss:  0.2592533230781555
train gradient:  0.09817242743572617
iteration : 1004
train acc:  0.828125
train loss:  0.4054480791091919
train gradient:  0.26426518654686065
iteration : 1005
train acc:  0.84375
train loss:  0.3861047029495239
train gradient:  0.19648478710526918
iteration : 1006
train acc:  0.8828125
train loss:  0.2889899015426636
train gradient:  0.1903930524096177
iteration : 1007
train acc:  0.84375
train loss:  0.30952024459838867
train gradient:  0.1542287397377548
iteration : 1008
train acc:  0.8515625
train loss:  0.32280194759368896
train gradient:  0.08617174184016571
iteration : 1009
train acc:  0.8671875
train loss:  0.2641527056694031
train gradient:  0.09969712889009788
iteration : 1010
train acc:  0.90625
train loss:  0.27918481826782227
train gradient:  0.07634074122212818
iteration : 1011
train acc:  0.8359375
train loss:  0.36766502261161804
train gradient:  0.21085384327399276
iteration : 1012
train acc:  0.8515625
train loss:  0.3435935974121094
train gradient:  0.16859558033966796
iteration : 1013
train acc:  0.890625
train loss:  0.2548874020576477
train gradient:  0.12361674316565097
iteration : 1014
train acc:  0.8515625
train loss:  0.28448259830474854
train gradient:  0.08638010264410921
iteration : 1015
train acc:  0.8671875
train loss:  0.29704204201698303
train gradient:  0.09794199518113135
iteration : 1016
train acc:  0.8984375
train loss:  0.23586270213127136
train gradient:  0.08657551806121444
iteration : 1017
train acc:  0.84375
train loss:  0.35274142026901245
train gradient:  0.20614898166080586
iteration : 1018
train acc:  0.78125
train loss:  0.3459954559803009
train gradient:  0.12885118272928628
iteration : 1019
train acc:  0.875
train loss:  0.3248603940010071
train gradient:  0.16973550267794954
iteration : 1020
train acc:  0.8515625
train loss:  0.33214443922042847
train gradient:  0.1381761592342602
iteration : 1021
train acc:  0.859375
train loss:  0.29556936025619507
train gradient:  0.11130522847778188
iteration : 1022
train acc:  0.8203125
train loss:  0.42815643548965454
train gradient:  0.2326827924180096
iteration : 1023
train acc:  0.8515625
train loss:  0.32197317481040955
train gradient:  0.15998213906453046
iteration : 1024
train acc:  0.9140625
train loss:  0.26102274656295776
train gradient:  0.10109355146581282
iteration : 1025
train acc:  0.875
train loss:  0.33655261993408203
train gradient:  0.1425876580522039
iteration : 1026
train acc:  0.90625
train loss:  0.2740987539291382
train gradient:  0.11946392484258485
iteration : 1027
train acc:  0.8515625
train loss:  0.34401941299438477
train gradient:  0.17682338169260556
iteration : 1028
train acc:  0.84375
train loss:  0.3478389084339142
train gradient:  0.1179795768654056
iteration : 1029
train acc:  0.8671875
train loss:  0.3597918748855591
train gradient:  0.15579290685951588
iteration : 1030
train acc:  0.8984375
train loss:  0.26919108629226685
train gradient:  0.11664697256844339
iteration : 1031
train acc:  0.8828125
train loss:  0.24736039340496063
train gradient:  0.09416552756846687
iteration : 1032
train acc:  0.859375
train loss:  0.37144380807876587
train gradient:  0.17611681631896792
iteration : 1033
train acc:  0.9140625
train loss:  0.26103901863098145
train gradient:  0.09794418457808896
iteration : 1034
train acc:  0.8671875
train loss:  0.4041239023208618
train gradient:  0.18630764259454452
iteration : 1035
train acc:  0.8515625
train loss:  0.3733189105987549
train gradient:  0.16665113511997934
iteration : 1036
train acc:  0.8671875
train loss:  0.2888612151145935
train gradient:  0.07886196931276895
iteration : 1037
train acc:  0.8203125
train loss:  0.3333580493927002
train gradient:  0.143546949572462
iteration : 1038
train acc:  0.828125
train loss:  0.3682735860347748
train gradient:  0.1581899345810469
iteration : 1039
train acc:  0.796875
train loss:  0.44038450717926025
train gradient:  0.17356585238422753
iteration : 1040
train acc:  0.90625
train loss:  0.26237785816192627
train gradient:  0.10042901953265285
iteration : 1041
train acc:  0.9140625
train loss:  0.24155741930007935
train gradient:  0.14107771110452083
iteration : 1042
train acc:  0.875
train loss:  0.3401504158973694
train gradient:  0.15435308237772094
iteration : 1043
train acc:  0.8828125
train loss:  0.31072282791137695
train gradient:  0.10854839097805749
iteration : 1044
train acc:  0.84375
train loss:  0.3669019937515259
train gradient:  0.18343888337959618
iteration : 1045
train acc:  0.921875
train loss:  0.23054656386375427
train gradient:  0.08124586565307128
iteration : 1046
train acc:  0.859375
train loss:  0.28547534346580505
train gradient:  0.11367638995990988
iteration : 1047
train acc:  0.8671875
train loss:  0.3259473443031311
train gradient:  0.1319471328244447
iteration : 1048
train acc:  0.8828125
train loss:  0.3286285996437073
train gradient:  0.1407725587388255
iteration : 1049
train acc:  0.8203125
train loss:  0.3443906605243683
train gradient:  0.12952398179887548
iteration : 1050
train acc:  0.84375
train loss:  0.33767908811569214
train gradient:  0.2030006333798035
iteration : 1051
train acc:  0.859375
train loss:  0.30833882093429565
train gradient:  0.1614241354246766
iteration : 1052
train acc:  0.84375
train loss:  0.3688473701477051
train gradient:  0.10660519785048907
iteration : 1053
train acc:  0.8125
train loss:  0.37166738510131836
train gradient:  0.14949365189892483
iteration : 1054
train acc:  0.890625
train loss:  0.26729631423950195
train gradient:  0.0719674194987377
iteration : 1055
train acc:  0.84375
train loss:  0.3392939269542694
train gradient:  0.1253062873240271
iteration : 1056
train acc:  0.8828125
train loss:  0.294491708278656
train gradient:  0.11223652281123458
iteration : 1057
train acc:  0.8671875
train loss:  0.31961122155189514
train gradient:  0.14375706976221905
iteration : 1058
train acc:  0.90625
train loss:  0.25910118222236633
train gradient:  0.08731047889540763
iteration : 1059
train acc:  0.8515625
train loss:  0.36862510442733765
train gradient:  0.19300338875965678
iteration : 1060
train acc:  0.890625
train loss:  0.27979040145874023
train gradient:  0.13230301869395314
iteration : 1061
train acc:  0.8515625
train loss:  0.3625463843345642
train gradient:  0.17492531332435424
iteration : 1062
train acc:  0.890625
train loss:  0.32111069560050964
train gradient:  0.09810444051837663
iteration : 1063
train acc:  0.8984375
train loss:  0.2870320975780487
train gradient:  0.13872062438619265
iteration : 1064
train acc:  0.8515625
train loss:  0.33947235345840454
train gradient:  0.11071645430239065
iteration : 1065
train acc:  0.921875
train loss:  0.2477683126926422
train gradient:  0.12132286553682066
iteration : 1066
train acc:  0.8828125
train loss:  0.2594957947731018
train gradient:  0.08690579744612241
iteration : 1067
train acc:  0.8828125
train loss:  0.2821041941642761
train gradient:  0.11815682635022388
iteration : 1068
train acc:  0.8515625
train loss:  0.3613881468772888
train gradient:  0.22459038824657213
iteration : 1069
train acc:  0.9140625
train loss:  0.2489030659198761
train gradient:  0.10143431793075172
iteration : 1070
train acc:  0.859375
train loss:  0.2947687804698944
train gradient:  0.11066854312675277
iteration : 1071
train acc:  0.90625
train loss:  0.2689652144908905
train gradient:  0.10130400405133332
iteration : 1072
train acc:  0.828125
train loss:  0.35107070207595825
train gradient:  0.12505863542907725
iteration : 1073
train acc:  0.8515625
train loss:  0.30293774604797363
train gradient:  0.11257400033611493
iteration : 1074
train acc:  0.8203125
train loss:  0.31440767645835876
train gradient:  0.13181518023312538
iteration : 1075
train acc:  0.828125
train loss:  0.3616591691970825
train gradient:  0.15083098752267124
iteration : 1076
train acc:  0.8828125
train loss:  0.25234878063201904
train gradient:  0.07917090638716731
iteration : 1077
train acc:  0.8984375
train loss:  0.28278249502182007
train gradient:  0.09684222583377892
iteration : 1078
train acc:  0.8828125
train loss:  0.2779342532157898
train gradient:  0.1438144697645036
iteration : 1079
train acc:  0.90625
train loss:  0.24009950459003448
train gradient:  0.08807665937562005
iteration : 1080
train acc:  0.8671875
train loss:  0.29161375761032104
train gradient:  0.10748581907101805
iteration : 1081
train acc:  0.8984375
train loss:  0.23128217458724976
train gradient:  0.07311453451041065
iteration : 1082
train acc:  0.875
train loss:  0.28164637088775635
train gradient:  0.11388440630984625
iteration : 1083
train acc:  0.8671875
train loss:  0.31777727603912354
train gradient:  0.11000756397815314
iteration : 1084
train acc:  0.8671875
train loss:  0.31065797805786133
train gradient:  0.12415322515056873
iteration : 1085
train acc:  0.8359375
train loss:  0.3654269874095917
train gradient:  0.3034753116055332
iteration : 1086
train acc:  0.8671875
train loss:  0.2916472852230072
train gradient:  0.11917372595917783
iteration : 1087
train acc:  0.84375
train loss:  0.36546480655670166
train gradient:  0.14828515236974932
iteration : 1088
train acc:  0.84375
train loss:  0.3153434991836548
train gradient:  0.13398628213918495
iteration : 1089
train acc:  0.8828125
train loss:  0.35317152738571167
train gradient:  0.15278017500548585
iteration : 1090
train acc:  0.90625
train loss:  0.2643832564353943
train gradient:  0.14410234215407686
iteration : 1091
train acc:  0.875
train loss:  0.28936371207237244
train gradient:  0.1321791937189813
iteration : 1092
train acc:  0.828125
train loss:  0.3131955564022064
train gradient:  0.11890078309173423
iteration : 1093
train acc:  0.875
train loss:  0.29457834362983704
train gradient:  0.12168846660981719
iteration : 1094
train acc:  0.8046875
train loss:  0.5079320073127747
train gradient:  0.328273273457718
iteration : 1095
train acc:  0.859375
train loss:  0.3149265646934509
train gradient:  0.10489052778013082
iteration : 1096
train acc:  0.8515625
train loss:  0.3131406307220459
train gradient:  0.14050102227904956
iteration : 1097
train acc:  0.90625
train loss:  0.2033776342868805
train gradient:  0.05992936885387079
iteration : 1098
train acc:  0.890625
train loss:  0.2564659118652344
train gradient:  0.10136159227462516
iteration : 1099
train acc:  0.875
train loss:  0.2718784213066101
train gradient:  0.09519400170900583
iteration : 1100
train acc:  0.8359375
train loss:  0.3349500596523285
train gradient:  0.14532335696861504
iteration : 1101
train acc:  0.8984375
train loss:  0.258899450302124
train gradient:  0.10964683943071524
iteration : 1102
train acc:  0.90625
train loss:  0.2302601933479309
train gradient:  0.08932370712452628
iteration : 1103
train acc:  0.9140625
train loss:  0.2439291775226593
train gradient:  0.0726749346484756
iteration : 1104
train acc:  0.8828125
train loss:  0.31945741176605225
train gradient:  0.13171153930545446
iteration : 1105
train acc:  0.859375
train loss:  0.30020010471343994
train gradient:  0.13354056601017858
iteration : 1106
train acc:  0.859375
train loss:  0.3738768994808197
train gradient:  0.38188979870166007
iteration : 1107
train acc:  0.8671875
train loss:  0.3535156846046448
train gradient:  0.23836907906791321
iteration : 1108
train acc:  0.859375
train loss:  0.33166736364364624
train gradient:  0.22932844480622372
iteration : 1109
train acc:  0.8828125
train loss:  0.27946680784225464
train gradient:  0.09457902532036122
iteration : 1110
train acc:  0.921875
train loss:  0.22931233048439026
train gradient:  0.09435822962379282
iteration : 1111
train acc:  0.9140625
train loss:  0.27565205097198486
train gradient:  0.08619861589717406
iteration : 1112
train acc:  0.8671875
train loss:  0.3029026985168457
train gradient:  0.10212026078347242
iteration : 1113
train acc:  0.8515625
train loss:  0.2889421284198761
train gradient:  0.10797590538601042
iteration : 1114
train acc:  0.8515625
train loss:  0.35707831382751465
train gradient:  0.16453044331684857
iteration : 1115
train acc:  0.859375
train loss:  0.28738176822662354
train gradient:  0.10335122892832266
iteration : 1116
train acc:  0.8828125
train loss:  0.31385260820388794
train gradient:  0.19421235312381271
iteration : 1117
train acc:  0.859375
train loss:  0.38543999195098877
train gradient:  0.16889567122621227
iteration : 1118
train acc:  0.8984375
train loss:  0.29109400510787964
train gradient:  0.11506471254634092
iteration : 1119
train acc:  0.8828125
train loss:  0.28797274827957153
train gradient:  0.12260208272929281
iteration : 1120
train acc:  0.8515625
train loss:  0.3708767592906952
train gradient:  0.1403396686735174
iteration : 1121
train acc:  0.8515625
train loss:  0.34257733821868896
train gradient:  0.17684800197836087
iteration : 1122
train acc:  0.9140625
train loss:  0.27317342162132263
train gradient:  0.09006419017912669
iteration : 1123
train acc:  0.890625
train loss:  0.29318857192993164
train gradient:  0.08899170780063696
iteration : 1124
train acc:  0.8671875
train loss:  0.31426018476486206
train gradient:  0.1057708619240997
iteration : 1125
train acc:  0.859375
train loss:  0.29151076078414917
train gradient:  0.15208338156146664
iteration : 1126
train acc:  0.8828125
train loss:  0.28278160095214844
train gradient:  0.06253115952519812
iteration : 1127
train acc:  0.8828125
train loss:  0.2793652415275574
train gradient:  0.10975500194462369
iteration : 1128
train acc:  0.8203125
train loss:  0.410739928483963
train gradient:  0.15992689142238742
iteration : 1129
train acc:  0.8984375
train loss:  0.2155594378709793
train gradient:  0.08447300968210739
iteration : 1130
train acc:  0.890625
train loss:  0.2957323491573334
train gradient:  0.0981321082173764
iteration : 1131
train acc:  0.9140625
train loss:  0.2812347412109375
train gradient:  0.11029958772923111
iteration : 1132
train acc:  0.8359375
train loss:  0.36337810754776
train gradient:  0.12511588654482342
iteration : 1133
train acc:  0.890625
train loss:  0.24997593462467194
train gradient:  0.10735376329144158
iteration : 1134
train acc:  0.9140625
train loss:  0.26785868406295776
train gradient:  0.10218457754827953
iteration : 1135
train acc:  0.8359375
train loss:  0.4123953580856323
train gradient:  0.27849682505274326
iteration : 1136
train acc:  0.8984375
train loss:  0.25931423902511597
train gradient:  0.10922282028212653
iteration : 1137
train acc:  0.84375
train loss:  0.33784475922584534
train gradient:  0.19450191994524035
iteration : 1138
train acc:  0.921875
train loss:  0.23850128054618835
train gradient:  0.0812504021463572
iteration : 1139
train acc:  0.8828125
train loss:  0.2365076243877411
train gradient:  0.11531043932275079
iteration : 1140
train acc:  0.921875
train loss:  0.21454717218875885
train gradient:  0.07293550890439061
iteration : 1141
train acc:  0.890625
train loss:  0.30488789081573486
train gradient:  0.12648514432404406
iteration : 1142
train acc:  0.84375
train loss:  0.35336628556251526
train gradient:  0.19857120099319497
iteration : 1143
train acc:  0.859375
train loss:  0.3179103434085846
train gradient:  0.0962193889627976
iteration : 1144
train acc:  0.859375
train loss:  0.3532765209674835
train gradient:  0.1800311774496563
iteration : 1145
train acc:  0.8828125
train loss:  0.24146874248981476
train gradient:  0.09490538204225836
iteration : 1146
train acc:  0.828125
train loss:  0.3088434934616089
train gradient:  0.16107504839783482
iteration : 1147
train acc:  0.8671875
train loss:  0.35222142934799194
train gradient:  0.12369962834588599
iteration : 1148
train acc:  0.8671875
train loss:  0.32544803619384766
train gradient:  0.1264841773148157
iteration : 1149
train acc:  0.8828125
train loss:  0.347540020942688
train gradient:  0.2313658078683623
iteration : 1150
train acc:  0.8515625
train loss:  0.29226863384246826
train gradient:  0.10328269591553599
iteration : 1151
train acc:  0.84375
train loss:  0.3295544981956482
train gradient:  0.16469898592538462
iteration : 1152
train acc:  0.8828125
train loss:  0.2738715410232544
train gradient:  0.09979118870876394
iteration : 1153
train acc:  0.8046875
train loss:  0.41990095376968384
train gradient:  0.19581773210316514
iteration : 1154
train acc:  0.921875
train loss:  0.2182474285364151
train gradient:  0.0760871513803948
iteration : 1155
train acc:  0.796875
train loss:  0.4198121428489685
train gradient:  0.22007485703417817
iteration : 1156
train acc:  0.8671875
train loss:  0.2988751530647278
train gradient:  0.12198780994115843
iteration : 1157
train acc:  0.8828125
train loss:  0.3091908097267151
train gradient:  0.10508693573706952
iteration : 1158
train acc:  0.8984375
train loss:  0.31238996982574463
train gradient:  0.10054549646146736
iteration : 1159
train acc:  0.828125
train loss:  0.32164525985717773
train gradient:  0.14038476182382065
iteration : 1160
train acc:  0.8828125
train loss:  0.29600319266319275
train gradient:  0.2509616714795117
iteration : 1161
train acc:  0.828125
train loss:  0.3296664357185364
train gradient:  0.12363801554199083
iteration : 1162
train acc:  0.90625
train loss:  0.23514610528945923
train gradient:  0.08644670762910556
iteration : 1163
train acc:  0.828125
train loss:  0.3593406081199646
train gradient:  0.12615692608049062
iteration : 1164
train acc:  0.859375
train loss:  0.2725714445114136
train gradient:  0.11377111892835139
iteration : 1165
train acc:  0.8984375
train loss:  0.2693764567375183
train gradient:  0.10770109810964733
iteration : 1166
train acc:  0.8671875
train loss:  0.3355010151863098
train gradient:  0.11864476118165702
iteration : 1167
train acc:  0.8828125
train loss:  0.30304229259490967
train gradient:  0.10980816922298198
iteration : 1168
train acc:  0.8515625
train loss:  0.34876108169555664
train gradient:  0.16303698118322021
iteration : 1169
train acc:  0.9140625
train loss:  0.24460774660110474
train gradient:  0.1308845134019111
iteration : 1170
train acc:  0.8515625
train loss:  0.31778478622436523
train gradient:  0.11582453000946923
iteration : 1171
train acc:  0.8828125
train loss:  0.27552974224090576
train gradient:  0.10988401768909932
iteration : 1172
train acc:  0.84375
train loss:  0.30850130319595337
train gradient:  0.13841464369363016
iteration : 1173
train acc:  0.9296875
train loss:  0.2341068983078003
train gradient:  0.10366365634350524
iteration : 1174
train acc:  0.890625
train loss:  0.2796581983566284
train gradient:  0.13881946455813543
iteration : 1175
train acc:  0.8671875
train loss:  0.3267148733139038
train gradient:  0.1026811087078259
iteration : 1176
train acc:  0.8828125
train loss:  0.2845032513141632
train gradient:  0.09851425382318942
iteration : 1177
train acc:  0.828125
train loss:  0.3436970114707947
train gradient:  0.18546032105859359
iteration : 1178
train acc:  0.859375
train loss:  0.31659743189811707
train gradient:  0.16880209425202092
iteration : 1179
train acc:  0.8203125
train loss:  0.3558727204799652
train gradient:  0.12967147571841145
iteration : 1180
train acc:  0.828125
train loss:  0.39302194118499756
train gradient:  0.14751903524483173
iteration : 1181
train acc:  0.859375
train loss:  0.2831494212150574
train gradient:  0.12620623498349
iteration : 1182
train acc:  0.875
train loss:  0.2895140051841736
train gradient:  0.1337738526208309
iteration : 1183
train acc:  0.8984375
train loss:  0.2985665798187256
train gradient:  0.1374957005151337
iteration : 1184
train acc:  0.859375
train loss:  0.30701541900634766
train gradient:  0.14760859921014696
iteration : 1185
train acc:  0.8515625
train loss:  0.2924489378929138
train gradient:  0.13640020527013202
iteration : 1186
train acc:  0.8671875
train loss:  0.2952723503112793
train gradient:  0.13335100212880452
iteration : 1187
train acc:  0.8359375
train loss:  0.34500864148139954
train gradient:  0.1838655419787229
iteration : 1188
train acc:  0.8125
train loss:  0.40008363127708435
train gradient:  0.1566411450119127
iteration : 1189
train acc:  0.8671875
train loss:  0.31692588329315186
train gradient:  0.1385375657440292
iteration : 1190
train acc:  0.8359375
train loss:  0.37500709295272827
train gradient:  0.16703239748669857
iteration : 1191
train acc:  0.90625
train loss:  0.20603254437446594
train gradient:  0.07409946204243335
iteration : 1192
train acc:  0.8046875
train loss:  0.41687247157096863
train gradient:  0.17873545700046073
iteration : 1193
train acc:  0.8515625
train loss:  0.31395214796066284
train gradient:  0.16808715601991076
iteration : 1194
train acc:  0.8125
train loss:  0.36658576130867004
train gradient:  0.14012819069946889
iteration : 1195
train acc:  0.84375
train loss:  0.3707881271839142
train gradient:  0.1538169229444729
iteration : 1196
train acc:  0.859375
train loss:  0.34536686539649963
train gradient:  0.1823943821701669
iteration : 1197
train acc:  0.875
train loss:  0.28821030259132385
train gradient:  0.15195647427425685
iteration : 1198
train acc:  0.8828125
train loss:  0.3357611298561096
train gradient:  0.13136583350886744
iteration : 1199
train acc:  0.8203125
train loss:  0.401183545589447
train gradient:  0.2282001945405436
iteration : 1200
train acc:  0.8515625
train loss:  0.306174099445343
train gradient:  0.11007345947368699
iteration : 1201
train acc:  0.8515625
train loss:  0.2826615273952484
train gradient:  0.08275244385670413
iteration : 1202
train acc:  0.8515625
train loss:  0.37642061710357666
train gradient:  0.19336801967035794
iteration : 1203
train acc:  0.8671875
train loss:  0.28182584047317505
train gradient:  0.09901783441504831
iteration : 1204
train acc:  0.875
train loss:  0.28362494707107544
train gradient:  0.0909931072777388
iteration : 1205
train acc:  0.90625
train loss:  0.24877209961414337
train gradient:  0.07328610346701184
iteration : 1206
train acc:  0.8671875
train loss:  0.309346079826355
train gradient:  0.11556523191823626
iteration : 1207
train acc:  0.84375
train loss:  0.3087051510810852
train gradient:  0.11341429419005386
iteration : 1208
train acc:  0.875
train loss:  0.280617892742157
train gradient:  0.08039091547781653
iteration : 1209
train acc:  0.8671875
train loss:  0.36081165075302124
train gradient:  0.16490236317715157
iteration : 1210
train acc:  0.8515625
train loss:  0.3221842646598816
train gradient:  0.13724441932022474
iteration : 1211
train acc:  0.890625
train loss:  0.2968382239341736
train gradient:  0.1586505713170504
iteration : 1212
train acc:  0.8515625
train loss:  0.31085407733917236
train gradient:  0.10271611944355175
iteration : 1213
train acc:  0.8671875
train loss:  0.30307117104530334
train gradient:  0.12072806676447284
iteration : 1214
train acc:  0.875
train loss:  0.35316306352615356
train gradient:  0.140081928362548
iteration : 1215
train acc:  0.8828125
train loss:  0.32999664545059204
train gradient:  0.135564544485413
iteration : 1216
train acc:  0.8515625
train loss:  0.33195969462394714
train gradient:  0.10747382657497292
iteration : 1217
train acc:  0.875
train loss:  0.3196510672569275
train gradient:  0.14685258696658038
iteration : 1218
train acc:  0.8984375
train loss:  0.2758580148220062
train gradient:  0.09144162667185181
iteration : 1219
train acc:  0.8671875
train loss:  0.3270075023174286
train gradient:  0.12791812160314103
iteration : 1220
train acc:  0.8515625
train loss:  0.33277371525764465
train gradient:  0.156456799491965
iteration : 1221
train acc:  0.921875
train loss:  0.22290897369384766
train gradient:  0.08782136765550791
iteration : 1222
train acc:  0.875
train loss:  0.30668556690216064
train gradient:  0.11531248502733457
iteration : 1223
train acc:  0.9140625
train loss:  0.249149352312088
train gradient:  0.12707020865877897
iteration : 1224
train acc:  0.8515625
train loss:  0.288788378238678
train gradient:  0.08625541906456449
iteration : 1225
train acc:  0.84375
train loss:  0.36603620648384094
train gradient:  0.2068470179848158
iteration : 1226
train acc:  0.9296875
train loss:  0.19960319995880127
train gradient:  0.061891529826772575
iteration : 1227
train acc:  0.8984375
train loss:  0.241655170917511
train gradient:  0.0944081046164258
iteration : 1228
train acc:  0.828125
train loss:  0.38495075702667236
train gradient:  0.1791034343931347
iteration : 1229
train acc:  0.875
train loss:  0.2734009623527527
train gradient:  0.15885791515900474
iteration : 1230
train acc:  0.8671875
train loss:  0.26154136657714844
train gradient:  0.10612000655716076
iteration : 1231
train acc:  0.875
train loss:  0.29588767886161804
train gradient:  0.18502641324137115
iteration : 1232
train acc:  0.84375
train loss:  0.3186669945716858
train gradient:  0.1402306774534452
iteration : 1233
train acc:  0.90625
train loss:  0.22422197461128235
train gradient:  0.06610873263872691
iteration : 1234
train acc:  0.8984375
train loss:  0.3411143124103546
train gradient:  0.15369702141060304
iteration : 1235
train acc:  0.859375
train loss:  0.28157922625541687
train gradient:  0.11715603494836363
iteration : 1236
train acc:  0.859375
train loss:  0.2700497508049011
train gradient:  0.11489915126991491
iteration : 1237
train acc:  0.8828125
train loss:  0.2494782656431198
train gradient:  0.07721497296022226
iteration : 1238
train acc:  0.8359375
train loss:  0.3340893089771271
train gradient:  0.22106031582382418
iteration : 1239
train acc:  0.875
train loss:  0.32415300607681274
train gradient:  0.11638091882226571
iteration : 1240
train acc:  0.890625
train loss:  0.2757410407066345
train gradient:  0.1028645871569969
iteration : 1241
train acc:  0.8984375
train loss:  0.28391745686531067
train gradient:  0.10263071337329382
iteration : 1242
train acc:  0.84375
train loss:  0.3144837021827698
train gradient:  0.1823880724354508
iteration : 1243
train acc:  0.859375
train loss:  0.2824307084083557
train gradient:  0.11083443023833303
iteration : 1244
train acc:  0.8203125
train loss:  0.3790832757949829
train gradient:  0.16401579662282156
iteration : 1245
train acc:  0.9140625
train loss:  0.23643693327903748
train gradient:  0.09179937306206243
iteration : 1246
train acc:  0.828125
train loss:  0.36267930269241333
train gradient:  0.12486364339112357
iteration : 1247
train acc:  0.8515625
train loss:  0.31737855076789856
train gradient:  0.12233260648112153
iteration : 1248
train acc:  0.875
train loss:  0.30861467123031616
train gradient:  0.1463423438634513
iteration : 1249
train acc:  0.875
train loss:  0.2859373688697815
train gradient:  0.10900928869490457
iteration : 1250
train acc:  0.9140625
train loss:  0.25601455569267273
train gradient:  0.16562118459274106
iteration : 1251
train acc:  0.890625
train loss:  0.29175442457199097
train gradient:  0.10436266903839793
iteration : 1252
train acc:  0.9140625
train loss:  0.26007747650146484
train gradient:  0.09948901547060146
iteration : 1253
train acc:  0.8359375
train loss:  0.3242087662220001
train gradient:  0.12991930008799135
iteration : 1254
train acc:  0.828125
train loss:  0.3487096428871155
train gradient:  0.15592493044723882
iteration : 1255
train acc:  0.9375
train loss:  0.22366906702518463
train gradient:  0.0861897883088794
iteration : 1256
train acc:  0.875
train loss:  0.31540316343307495
train gradient:  0.13450907059177558
iteration : 1257
train acc:  0.796875
train loss:  0.36835822463035583
train gradient:  0.1606580769521539
iteration : 1258
train acc:  0.875
train loss:  0.2654489576816559
train gradient:  0.10920958706095375
iteration : 1259
train acc:  0.859375
train loss:  0.28220251202583313
train gradient:  0.10047838003158466
iteration : 1260
train acc:  0.8515625
train loss:  0.3486989140510559
train gradient:  0.16578635880186338
iteration : 1261
train acc:  0.8828125
train loss:  0.3123362958431244
train gradient:  0.20197565808239218
iteration : 1262
train acc:  0.8984375
train loss:  0.25584447383880615
train gradient:  0.10791731511543293
iteration : 1263
train acc:  0.875
train loss:  0.27360641956329346
train gradient:  0.10785782968488264
iteration : 1264
train acc:  0.8984375
train loss:  0.22218018770217896
train gradient:  0.07667426057444839
iteration : 1265
train acc:  0.90625
train loss:  0.2545984387397766
train gradient:  0.08654949420412231
iteration : 1266
train acc:  0.859375
train loss:  0.2887384295463562
train gradient:  0.18215228612342638
iteration : 1267
train acc:  0.796875
train loss:  0.46954861283302307
train gradient:  0.2582951466761462
iteration : 1268
train acc:  0.8515625
train loss:  0.346038281917572
train gradient:  0.31708138419018733
iteration : 1269
train acc:  0.8671875
train loss:  0.2855900526046753
train gradient:  0.11315429357332249
iteration : 1270
train acc:  0.90625
train loss:  0.2538478970527649
train gradient:  0.11762050369497613
iteration : 1271
train acc:  0.8828125
train loss:  0.2843934893608093
train gradient:  0.09455151202110086
iteration : 1272
train acc:  0.8203125
train loss:  0.35302144289016724
train gradient:  0.18170424403850333
iteration : 1273
train acc:  0.8671875
train loss:  0.275227427482605
train gradient:  0.10869680499618418
iteration : 1274
train acc:  0.8359375
train loss:  0.42363715171813965
train gradient:  0.2708747340978206
iteration : 1275
train acc:  0.921875
train loss:  0.23910769820213318
train gradient:  0.10217226498273714
iteration : 1276
train acc:  0.875
train loss:  0.29394257068634033
train gradient:  0.11685571966251014
iteration : 1277
train acc:  0.8515625
train loss:  0.35258179903030396
train gradient:  0.1121916067241556
iteration : 1278
train acc:  0.8671875
train loss:  0.3956318199634552
train gradient:  0.20784899712577806
iteration : 1279
train acc:  0.90625
train loss:  0.21136324107646942
train gradient:  0.07009948734170336
iteration : 1280
train acc:  0.8671875
train loss:  0.30006012320518494
train gradient:  0.10842385986207528
iteration : 1281
train acc:  0.8515625
train loss:  0.3235229551792145
train gradient:  0.11802033298968621
iteration : 1282
train acc:  0.8515625
train loss:  0.35945600271224976
train gradient:  0.24579006069355364
iteration : 1283
train acc:  0.875
train loss:  0.30135467648506165
train gradient:  0.09331414806489759
iteration : 1284
train acc:  0.796875
train loss:  0.4133436679840088
train gradient:  0.1933826955716862
iteration : 1285
train acc:  0.8828125
train loss:  0.2793474793434143
train gradient:  0.12215962409221671
iteration : 1286
train acc:  0.890625
train loss:  0.25879961252212524
train gradient:  0.1060935683333718
iteration : 1287
train acc:  0.90625
train loss:  0.2864091098308563
train gradient:  0.08935500230228405
iteration : 1288
train acc:  0.8671875
train loss:  0.32933804392814636
train gradient:  0.14191627936431395
iteration : 1289
train acc:  0.875
train loss:  0.33156728744506836
train gradient:  0.18213819846142626
iteration : 1290
train acc:  0.890625
train loss:  0.27362096309661865
train gradient:  0.0899434705971939
iteration : 1291
train acc:  0.875
train loss:  0.3234931230545044
train gradient:  0.12032778716355479
iteration : 1292
train acc:  0.8828125
train loss:  0.3297971189022064
train gradient:  0.1375298115289777
iteration : 1293
train acc:  0.8203125
train loss:  0.4160066545009613
train gradient:  0.2693918961066326
iteration : 1294
train acc:  0.8671875
train loss:  0.3600947856903076
train gradient:  0.21009802545057543
iteration : 1295
train acc:  0.890625
train loss:  0.27547407150268555
train gradient:  0.104056741433017
iteration : 1296
train acc:  0.8046875
train loss:  0.4615541696548462
train gradient:  0.23259642843923684
iteration : 1297
train acc:  0.828125
train loss:  0.33696186542510986
train gradient:  0.17759460512381758
iteration : 1298
train acc:  0.875
train loss:  0.30003252625465393
train gradient:  0.1200412744426067
iteration : 1299
train acc:  0.8515625
train loss:  0.2827383279800415
train gradient:  0.22310934722258596
iteration : 1300
train acc:  0.8671875
train loss:  0.31639665365219116
train gradient:  0.24587686798066385
iteration : 1301
train acc:  0.875
train loss:  0.3654671907424927
train gradient:  0.1796769142245725
iteration : 1302
train acc:  0.8359375
train loss:  0.382344126701355
train gradient:  0.14650813884212882
iteration : 1303
train acc:  0.890625
train loss:  0.33518731594085693
train gradient:  0.16586784358442969
iteration : 1304
train acc:  0.8515625
train loss:  0.3618905544281006
train gradient:  0.16736554040358714
iteration : 1305
train acc:  0.8671875
train loss:  0.3280937671661377
train gradient:  0.14893973398862834
iteration : 1306
train acc:  0.890625
train loss:  0.2647348642349243
train gradient:  0.10994055138983408
iteration : 1307
train acc:  0.8671875
train loss:  0.33794456720352173
train gradient:  0.1508317994916376
iteration : 1308
train acc:  0.8828125
train loss:  0.30888527631759644
train gradient:  0.12809588570907804
iteration : 1309
train acc:  0.84375
train loss:  0.35380175709724426
train gradient:  0.18651034314170656
iteration : 1310
train acc:  0.890625
train loss:  0.23209300637245178
train gradient:  0.11835194603799329
iteration : 1311
train acc:  0.890625
train loss:  0.28276851773262024
train gradient:  0.08666648538405616
iteration : 1312
train acc:  0.890625
train loss:  0.2368350625038147
train gradient:  0.07057805448248974
iteration : 1313
train acc:  0.8671875
train loss:  0.2733863592147827
train gradient:  0.11310098875631072
iteration : 1314
train acc:  0.890625
train loss:  0.2768886685371399
train gradient:  0.09081814647784323
iteration : 1315
train acc:  0.8515625
train loss:  0.30603107810020447
train gradient:  0.10082683258266112
iteration : 1316
train acc:  0.8671875
train loss:  0.3184548616409302
train gradient:  0.11426545262686902
iteration : 1317
train acc:  0.9375
train loss:  0.20097492635250092
train gradient:  0.05246910584728487
iteration : 1318
train acc:  0.8359375
train loss:  0.3365638256072998
train gradient:  0.15556581820843884
iteration : 1319
train acc:  0.9140625
train loss:  0.22594395279884338
train gradient:  0.07747474924142558
iteration : 1320
train acc:  0.8984375
train loss:  0.2810511589050293
train gradient:  0.10613422505554868
iteration : 1321
train acc:  0.8828125
train loss:  0.23882117867469788
train gradient:  0.07194998963499705
iteration : 1322
train acc:  0.8203125
train loss:  0.4017198085784912
train gradient:  0.2414207389833941
iteration : 1323
train acc:  0.8984375
train loss:  0.23507292568683624
train gradient:  0.08494972013321082
iteration : 1324
train acc:  0.8671875
train loss:  0.3422974646091461
train gradient:  0.1726079721280817
iteration : 1325
train acc:  0.828125
train loss:  0.37544387578964233
train gradient:  0.17359114694889607
iteration : 1326
train acc:  0.8125
train loss:  0.3547203242778778
train gradient:  0.14926926325586315
iteration : 1327
train acc:  0.8515625
train loss:  0.3292144536972046
train gradient:  0.14202577994625717
iteration : 1328
train acc:  0.8828125
train loss:  0.25234729051589966
train gradient:  0.10635489580004875
iteration : 1329
train acc:  0.8828125
train loss:  0.24975071847438812
train gradient:  0.11095919032229687
iteration : 1330
train acc:  0.859375
train loss:  0.27668967843055725
train gradient:  0.11730849325676819
iteration : 1331
train acc:  0.8828125
train loss:  0.30293580889701843
train gradient:  0.08526815974240086
iteration : 1332
train acc:  0.8984375
train loss:  0.24600067734718323
train gradient:  0.08527539077030868
iteration : 1333
train acc:  0.8828125
train loss:  0.3011036813259125
train gradient:  0.15161274274476705
iteration : 1334
train acc:  0.8359375
train loss:  0.3515614867210388
train gradient:  0.1575076690166084
iteration : 1335
train acc:  0.9296875
train loss:  0.22644159197807312
train gradient:  0.08588288020906064
iteration : 1336
train acc:  0.8671875
train loss:  0.34948229789733887
train gradient:  0.12181962019641243
iteration : 1337
train acc:  0.8046875
train loss:  0.39172399044036865
train gradient:  0.1460151617057378
iteration : 1338
train acc:  0.78125
train loss:  0.38965797424316406
train gradient:  0.16408266217115014
iteration : 1339
train acc:  0.828125
train loss:  0.3986908197402954
train gradient:  0.17033981033036646
iteration : 1340
train acc:  0.859375
train loss:  0.283005028963089
train gradient:  0.11289693490259523
iteration : 1341
train acc:  0.8671875
train loss:  0.30492788553237915
train gradient:  0.15012963935078644
iteration : 1342
train acc:  0.8828125
train loss:  0.27326613664627075
train gradient:  0.11354879980993968
iteration : 1343
train acc:  0.8984375
train loss:  0.2691144049167633
train gradient:  0.07087733657006065
iteration : 1344
train acc:  0.875
train loss:  0.34583181142807007
train gradient:  0.14879911687086494
iteration : 1345
train acc:  0.9140625
train loss:  0.2669435441493988
train gradient:  0.10264774905781117
iteration : 1346
train acc:  0.8671875
train loss:  0.358248233795166
train gradient:  0.19756988070793247
iteration : 1347
train acc:  0.8984375
train loss:  0.297370582818985
train gradient:  0.1369994336133455
iteration : 1348
train acc:  0.7890625
train loss:  0.45799535512924194
train gradient:  0.24497922253149684
iteration : 1349
train acc:  0.875
train loss:  0.279432475566864
train gradient:  0.12479972370782941
iteration : 1350
train acc:  0.8515625
train loss:  0.29733380675315857
train gradient:  0.09554867161318158
iteration : 1351
train acc:  0.90625
train loss:  0.2306414246559143
train gradient:  0.07006946229969109
iteration : 1352
train acc:  0.8671875
train loss:  0.3235226273536682
train gradient:  0.16634797295743334
iteration : 1353
train acc:  0.8125
train loss:  0.36482754349708557
train gradient:  0.1287782552929262
iteration : 1354
train acc:  0.8828125
train loss:  0.3337084949016571
train gradient:  0.14138622766174985
iteration : 1355
train acc:  0.90625
train loss:  0.22361280024051666
train gradient:  0.27038358803880247
iteration : 1356
train acc:  0.890625
train loss:  0.3076704144477844
train gradient:  0.11840225264067805
iteration : 1357
train acc:  0.921875
train loss:  0.279437780380249
train gradient:  0.10595342301893303
iteration : 1358
train acc:  0.8828125
train loss:  0.3128856420516968
train gradient:  0.10585282043194011
iteration : 1359
train acc:  0.8359375
train loss:  0.3548845648765564
train gradient:  0.1354875053146007
iteration : 1360
train acc:  0.9140625
train loss:  0.20378023386001587
train gradient:  0.07029954920258706
iteration : 1361
train acc:  0.8984375
train loss:  0.2597976624965668
train gradient:  0.11084143606104417
iteration : 1362
train acc:  0.859375
train loss:  0.3424367904663086
train gradient:  0.1955449912622501
iteration : 1363
train acc:  0.859375
train loss:  0.2996824383735657
train gradient:  0.09316723571830107
iteration : 1364
train acc:  0.8984375
train loss:  0.2626456022262573
train gradient:  0.09399503713084668
iteration : 1365
train acc:  0.8125
train loss:  0.407473087310791
train gradient:  0.20702036622506306
iteration : 1366
train acc:  0.828125
train loss:  0.35083162784576416
train gradient:  0.15761627384807303
iteration : 1367
train acc:  0.8828125
train loss:  0.24774135649204254
train gradient:  0.10660258276434112
iteration : 1368
train acc:  0.8203125
train loss:  0.2894139587879181
train gradient:  0.07925912884089034
iteration : 1369
train acc:  0.8828125
train loss:  0.29059991240501404
train gradient:  0.12049378206602512
iteration : 1370
train acc:  0.84375
train loss:  0.3935661315917969
train gradient:  0.17815142410188706
iteration : 1371
train acc:  0.8828125
train loss:  0.2604025602340698
train gradient:  0.09518788661949741
iteration : 1372
train acc:  0.8359375
train loss:  0.3371841609477997
train gradient:  0.15421019916273024
iteration : 1373
train acc:  0.8828125
train loss:  0.27812400460243225
train gradient:  0.09378831275011712
iteration : 1374
train acc:  0.8203125
train loss:  0.42155948281288147
train gradient:  0.24028377248307825
iteration : 1375
train acc:  0.859375
train loss:  0.35253652930259705
train gradient:  0.1543782745604508
iteration : 1376
train acc:  0.8984375
train loss:  0.2885205149650574
train gradient:  0.07612152013892012
iteration : 1377
train acc:  0.8828125
train loss:  0.2840130925178528
train gradient:  0.13173418073003273
iteration : 1378
train acc:  0.8515625
train loss:  0.3168151378631592
train gradient:  0.11872498888995756
iteration : 1379
train acc:  0.828125
train loss:  0.3361348807811737
train gradient:  0.12842511482747324
iteration : 1380
train acc:  0.859375
train loss:  0.3447370231151581
train gradient:  0.14313916848792785
iteration : 1381
train acc:  0.8359375
train loss:  0.36937081813812256
train gradient:  0.15400517527803395
iteration : 1382
train acc:  0.875
train loss:  0.28773200511932373
train gradient:  0.1290422901373065
iteration : 1383
train acc:  0.890625
train loss:  0.24552536010742188
train gradient:  0.09187310413149942
iteration : 1384
train acc:  0.8515625
train loss:  0.2972433269023895
train gradient:  0.11224396004507951
iteration : 1385
train acc:  0.890625
train loss:  0.3055737018585205
train gradient:  0.13123426619100553
iteration : 1386
train acc:  0.8828125
train loss:  0.2726231813430786
train gradient:  0.10571153717476109
iteration : 1387
train acc:  0.9296875
train loss:  0.22320646047592163
train gradient:  0.08192146827896776
iteration : 1388
train acc:  0.8359375
train loss:  0.3312675356864929
train gradient:  0.19404206223178783
iteration : 1389
train acc:  0.875
train loss:  0.27358102798461914
train gradient:  0.09332812247400168
iteration : 1390
train acc:  0.8984375
train loss:  0.23463760316371918
train gradient:  0.06398670093909098
iteration : 1391
train acc:  0.8984375
train loss:  0.31872546672821045
train gradient:  0.10185993804255213
iteration : 1392
train acc:  0.859375
train loss:  0.34400731325149536
train gradient:  0.10698459643901044
iteration : 1393
train acc:  0.8671875
train loss:  0.3329702615737915
train gradient:  0.11310841858860682
iteration : 1394
train acc:  0.875
train loss:  0.2637099623680115
train gradient:  0.10253086776760387
iteration : 1395
train acc:  0.8515625
train loss:  0.3057362735271454
train gradient:  0.13014208719898349
iteration : 1396
train acc:  0.8203125
train loss:  0.3492090106010437
train gradient:  0.12397270314089866
iteration : 1397
train acc:  0.8046875
train loss:  0.37308573722839355
train gradient:  0.30092101539138594
iteration : 1398
train acc:  0.8515625
train loss:  0.3498137593269348
train gradient:  0.1669787533139926
iteration : 1399
train acc:  0.875
train loss:  0.3157864511013031
train gradient:  0.13416085888549975
iteration : 1400
train acc:  0.8828125
train loss:  0.25812381505966187
train gradient:  0.08229244352020021
iteration : 1401
train acc:  0.796875
train loss:  0.3774884343147278
train gradient:  0.1469083840861436
iteration : 1402
train acc:  0.859375
train loss:  0.33442357182502747
train gradient:  0.11831300225053533
iteration : 1403
train acc:  0.890625
train loss:  0.3025914430618286
train gradient:  0.14066007158193966
iteration : 1404
train acc:  0.8515625
train loss:  0.33131158351898193
train gradient:  0.1292830554926897
iteration : 1405
train acc:  0.875
train loss:  0.3210000693798065
train gradient:  0.0992163022984756
iteration : 1406
train acc:  0.890625
train loss:  0.2596386671066284
train gradient:  0.11330452788974103
iteration : 1407
train acc:  0.890625
train loss:  0.24216923117637634
train gradient:  0.09010718901693622
iteration : 1408
train acc:  0.84375
train loss:  0.26659682393074036
train gradient:  0.06971227243493772
iteration : 1409
train acc:  0.875
train loss:  0.28764447569847107
train gradient:  0.09879174613627227
iteration : 1410
train acc:  0.9453125
train loss:  0.2112956941127777
train gradient:  0.07628952713048497
iteration : 1411
train acc:  0.875
train loss:  0.2659430503845215
train gradient:  0.1199950064234196
iteration : 1412
train acc:  0.8515625
train loss:  0.30709511041641235
train gradient:  0.2177684902600053
iteration : 1413
train acc:  0.890625
train loss:  0.3267589807510376
train gradient:  0.12395615054947823
iteration : 1414
train acc:  0.8203125
train loss:  0.35303398966789246
train gradient:  0.11559541158965686
iteration : 1415
train acc:  0.8828125
train loss:  0.2765484154224396
train gradient:  0.10761383522899841
iteration : 1416
train acc:  0.8984375
train loss:  0.2834547758102417
train gradient:  0.11565039701577277
iteration : 1417
train acc:  0.8125
train loss:  0.3998919725418091
train gradient:  0.13704074993939724
iteration : 1418
train acc:  0.890625
train loss:  0.24104198813438416
train gradient:  0.10026092924936861
iteration : 1419
train acc:  0.8828125
train loss:  0.2782275080680847
train gradient:  0.11871013845169243
iteration : 1420
train acc:  0.890625
train loss:  0.26028531789779663
train gradient:  0.07961613862972824
iteration : 1421
train acc:  0.890625
train loss:  0.30181118845939636
train gradient:  0.14271201069154074
iteration : 1422
train acc:  0.90625
train loss:  0.24019935727119446
train gradient:  0.06901299675967609
iteration : 1423
train acc:  0.890625
train loss:  0.22505418956279755
train gradient:  0.09799200955970207
iteration : 1424
train acc:  0.8671875
train loss:  0.2926940321922302
train gradient:  0.13202312692666962
iteration : 1425
train acc:  0.8671875
train loss:  0.34000712633132935
train gradient:  0.13622513487966179
iteration : 1426
train acc:  0.828125
train loss:  0.38427451252937317
train gradient:  0.1914747344151368
iteration : 1427
train acc:  0.8515625
train loss:  0.33572256565093994
train gradient:  0.2528504666917127
iteration : 1428
train acc:  0.8359375
train loss:  0.3935982584953308
train gradient:  0.23015620090188635
iteration : 1429
train acc:  0.796875
train loss:  0.4632548987865448
train gradient:  0.21242818464932223
iteration : 1430
train acc:  0.8828125
train loss:  0.28361696004867554
train gradient:  0.09135679840453033
iteration : 1431
train acc:  0.8046875
train loss:  0.39291709661483765
train gradient:  0.19710772540092064
iteration : 1432
train acc:  0.8515625
train loss:  0.3570334017276764
train gradient:  0.14048595071319092
iteration : 1433
train acc:  0.890625
train loss:  0.2579399347305298
train gradient:  0.10028982186352323
iteration : 1434
train acc:  0.796875
train loss:  0.41862258315086365
train gradient:  0.18445918953371016
iteration : 1435
train acc:  0.9375
train loss:  0.17594358325004578
train gradient:  0.049480541443783055
iteration : 1436
train acc:  0.8671875
train loss:  0.3055921792984009
train gradient:  0.12766749409006384
iteration : 1437
train acc:  0.8828125
train loss:  0.29355770349502563
train gradient:  0.12249813585387712
iteration : 1438
train acc:  0.8671875
train loss:  0.3197989761829376
train gradient:  0.1312135580944828
iteration : 1439
train acc:  0.90625
train loss:  0.27060216665267944
train gradient:  0.10332672245720739
iteration : 1440
train acc:  0.875
train loss:  0.36599817872047424
train gradient:  0.1842514662019896
iteration : 1441
train acc:  0.84375
train loss:  0.3322940766811371
train gradient:  0.12696846021932745
iteration : 1442
train acc:  0.796875
train loss:  0.3965378999710083
train gradient:  0.18171911829467613
iteration : 1443
train acc:  0.8671875
train loss:  0.306749165058136
train gradient:  0.14290579060969538
iteration : 1444
train acc:  0.796875
train loss:  0.3840521574020386
train gradient:  0.13551792311281027
iteration : 1445
train acc:  0.84375
train loss:  0.341318815946579
train gradient:  0.24382766790793195
iteration : 1446
train acc:  0.8828125
train loss:  0.2788357436656952
train gradient:  0.1157707799853255
iteration : 1447
train acc:  0.8515625
train loss:  0.30729007720947266
train gradient:  0.10423631220322753
iteration : 1448
train acc:  0.8515625
train loss:  0.30261480808258057
train gradient:  0.1548281898330331
iteration : 1449
train acc:  0.859375
train loss:  0.28500571846961975
train gradient:  0.12048391322523673
iteration : 1450
train acc:  0.8828125
train loss:  0.27525705099105835
train gradient:  0.1073723757044422
iteration : 1451
train acc:  0.8125
train loss:  0.3386611342430115
train gradient:  0.2346145154492476
iteration : 1452
train acc:  0.8515625
train loss:  0.327261745929718
train gradient:  0.22102809221511027
iteration : 1453
train acc:  0.859375
train loss:  0.3310982882976532
train gradient:  0.11664263582835944
iteration : 1454
train acc:  0.859375
train loss:  0.3392576575279236
train gradient:  0.10641794544841274
iteration : 1455
train acc:  0.875
train loss:  0.27581435441970825
train gradient:  0.10026709715836508
iteration : 1456
train acc:  0.796875
train loss:  0.47657081484794617
train gradient:  0.24036964732994542
iteration : 1457
train acc:  0.8515625
train loss:  0.3379051089286804
train gradient:  0.20194177900313515
iteration : 1458
train acc:  0.828125
train loss:  0.3512114882469177
train gradient:  0.13530002888441817
iteration : 1459
train acc:  0.90625
train loss:  0.2670076787471771
train gradient:  0.09518782408868422
iteration : 1460
train acc:  0.84375
train loss:  0.33267444372177124
train gradient:  0.1288384029387279
iteration : 1461
train acc:  0.9140625
train loss:  0.23360797762870789
train gradient:  0.07385970873292885
iteration : 1462
train acc:  0.90625
train loss:  0.2540115714073181
train gradient:  0.17414320442714587
iteration : 1463
train acc:  0.875
train loss:  0.3078831434249878
train gradient:  0.1081408785569827
iteration : 1464
train acc:  0.90625
train loss:  0.2402614951133728
train gradient:  0.07273401115262217
iteration : 1465
train acc:  0.890625
train loss:  0.22184762358665466
train gradient:  0.09684222307811545
iteration : 1466
train acc:  0.8203125
train loss:  0.3849530518054962
train gradient:  0.16914520705669445
iteration : 1467
train acc:  0.890625
train loss:  0.2743527293205261
train gradient:  0.10605333230466586
iteration : 1468
train acc:  0.8671875
train loss:  0.3723906874656677
train gradient:  0.17881963818837968
iteration : 1469
train acc:  0.890625
train loss:  0.29055482149124146
train gradient:  0.11678760077845486
iteration : 1470
train acc:  0.8671875
train loss:  0.3482673764228821
train gradient:  0.14408429715761206
iteration : 1471
train acc:  0.859375
train loss:  0.32358518242836
train gradient:  0.14517239214246902
iteration : 1472
train acc:  0.9140625
train loss:  0.2456035315990448
train gradient:  0.08773041139738792
iteration : 1473
train acc:  0.8984375
train loss:  0.27428510785102844
train gradient:  0.10819310546405123
iteration : 1474
train acc:  0.8828125
train loss:  0.3516260087490082
train gradient:  0.1435553962957229
iteration : 1475
train acc:  0.8671875
train loss:  0.3030804991722107
train gradient:  0.11775864015776488
iteration : 1476
train acc:  0.8984375
train loss:  0.24916167557239532
train gradient:  0.08516577502083814
iteration : 1477
train acc:  0.8828125
train loss:  0.29902106523513794
train gradient:  0.17424660483060966
iteration : 1478
train acc:  0.9375
train loss:  0.2453305423259735
train gradient:  0.09631501823507631
iteration : 1479
train acc:  0.8671875
train loss:  0.29492101073265076
train gradient:  0.11437610530501323
iteration : 1480
train acc:  0.8984375
train loss:  0.26971930265426636
train gradient:  0.08977758933754526
iteration : 1481
train acc:  0.875
train loss:  0.2922944724559784
train gradient:  0.11089862810724406
iteration : 1482
train acc:  0.8515625
train loss:  0.325326532125473
train gradient:  0.10451104165763007
iteration : 1483
train acc:  0.875
train loss:  0.2940489649772644
train gradient:  0.09034651475830927
iteration : 1484
train acc:  0.859375
train loss:  0.26747599244117737
train gradient:  0.10319731530942146
iteration : 1485
train acc:  0.7890625
train loss:  0.40703338384628296
train gradient:  0.1763798060101236
iteration : 1486
train acc:  0.859375
train loss:  0.2869409918785095
train gradient:  0.08982490382485506
iteration : 1487
train acc:  0.796875
train loss:  0.4083467125892639
train gradient:  0.19863789894540715
iteration : 1488
train acc:  0.9140625
train loss:  0.2715124487876892
train gradient:  0.07573522740943651
iteration : 1489
train acc:  0.8671875
train loss:  0.3049662113189697
train gradient:  0.09284133050818727
iteration : 1490
train acc:  0.8515625
train loss:  0.31746524572372437
train gradient:  0.1061325143795808
iteration : 1491
train acc:  0.90625
train loss:  0.30884939432144165
train gradient:  0.20313697172738604
iteration : 1492
train acc:  0.8828125
train loss:  0.28508472442626953
train gradient:  0.10330798366089293
iteration : 1493
train acc:  0.921875
train loss:  0.23346693813800812
train gradient:  0.07659247052572224
iteration : 1494
train acc:  0.8984375
train loss:  0.2802284359931946
train gradient:  0.07856102745297715
iteration : 1495
train acc:  0.8984375
train loss:  0.24208803474903107
train gradient:  0.12142507661254459
iteration : 1496
train acc:  0.8359375
train loss:  0.358084499835968
train gradient:  0.13064166548237793
iteration : 1497
train acc:  0.8515625
train loss:  0.2906016707420349
train gradient:  0.1023937618102965
iteration : 1498
train acc:  0.890625
train loss:  0.2617321014404297
train gradient:  0.07954020968606744
iteration : 1499
train acc:  0.828125
train loss:  0.35018807649612427
train gradient:  0.09729661431641415
iteration : 1500
train acc:  0.90625
train loss:  0.22397765517234802
train gradient:  0.12771074791024958
iteration : 1501
train acc:  0.890625
train loss:  0.263150155544281
train gradient:  0.10971100292393705
iteration : 1502
train acc:  0.84375
train loss:  0.3567520081996918
train gradient:  0.13962453120709856
iteration : 1503
train acc:  0.9375
train loss:  0.2208089381456375
train gradient:  0.12575900228251097
iteration : 1504
train acc:  0.8828125
train loss:  0.32738152146339417
train gradient:  0.13661884761131016
iteration : 1505
train acc:  0.8984375
train loss:  0.232482448220253
train gradient:  0.10313916402342557
iteration : 1506
train acc:  0.84375
train loss:  0.36667633056640625
train gradient:  0.17629648979911988
iteration : 1507
train acc:  0.859375
train loss:  0.33504700660705566
train gradient:  0.12259568767770394
iteration : 1508
train acc:  0.859375
train loss:  0.29191261529922485
train gradient:  0.17358261766138333
iteration : 1509
train acc:  0.8671875
train loss:  0.28929996490478516
train gradient:  0.13762517507295932
iteration : 1510
train acc:  0.921875
train loss:  0.24082119762897491
train gradient:  0.14371916960792047
iteration : 1511
train acc:  0.875
train loss:  0.27860796451568604
train gradient:  0.14090467252444883
iteration : 1512
train acc:  0.90625
train loss:  0.293717622756958
train gradient:  0.11824417208845552
iteration : 1513
train acc:  0.9296875
train loss:  0.22139877080917358
train gradient:  0.0637833491000777
iteration : 1514
train acc:  0.828125
train loss:  0.3668622076511383
train gradient:  0.15154346000370023
iteration : 1515
train acc:  0.890625
train loss:  0.28265202045440674
train gradient:  0.146858682538726
iteration : 1516
train acc:  0.8671875
train loss:  0.27751901745796204
train gradient:  0.10416656193552187
iteration : 1517
train acc:  0.8359375
train loss:  0.3026113510131836
train gradient:  0.10504313848864905
iteration : 1518
train acc:  0.890625
train loss:  0.2711317837238312
train gradient:  0.11864200808996553
iteration : 1519
train acc:  0.875
train loss:  0.3425569534301758
train gradient:  0.1151484425767624
iteration : 1520
train acc:  0.8359375
train loss:  0.35924792289733887
train gradient:  0.13726618040346483
iteration : 1521
train acc:  0.9296875
train loss:  0.2211504876613617
train gradient:  0.09821823499712189
iteration : 1522
train acc:  0.875
train loss:  0.35537073016166687
train gradient:  0.2123934807172308
iteration : 1523
train acc:  0.90625
train loss:  0.2706362009048462
train gradient:  0.08102798949667311
iteration : 1524
train acc:  0.890625
train loss:  0.3026718497276306
train gradient:  0.12402150102962343
iteration : 1525
train acc:  0.859375
train loss:  0.2990669012069702
train gradient:  0.09498850693527826
iteration : 1526
train acc:  0.84375
train loss:  0.31501853466033936
train gradient:  0.1169285713653731
iteration : 1527
train acc:  0.890625
train loss:  0.2428625226020813
train gradient:  0.08655447998530716
iteration : 1528
train acc:  0.828125
train loss:  0.32298874855041504
train gradient:  0.10074029703865334
iteration : 1529
train acc:  0.8671875
train loss:  0.3417332172393799
train gradient:  0.11135072919791247
iteration : 1530
train acc:  0.8828125
train loss:  0.2828032076358795
train gradient:  0.16402201691891577
iteration : 1531
train acc:  0.8828125
train loss:  0.3349158763885498
train gradient:  0.12601926637981947
iteration : 1532
train acc:  0.78125
train loss:  0.4316360056400299
train gradient:  0.24212299900637355
iteration : 1533
train acc:  0.828125
train loss:  0.35884904861450195
train gradient:  0.15291086339227256
iteration : 1534
train acc:  0.875
train loss:  0.23974309861660004
train gradient:  0.0759102358144252
iteration : 1535
train acc:  0.828125
train loss:  0.32565760612487793
train gradient:  0.14009257918829748
iteration : 1536
train acc:  0.90625
train loss:  0.234421044588089
train gradient:  0.07248426406312172
iteration : 1537
train acc:  0.890625
train loss:  0.2527662515640259
train gradient:  0.10425961263318113
iteration : 1538
train acc:  0.7890625
train loss:  0.43994632363319397
train gradient:  0.169491987914691
iteration : 1539
train acc:  0.90625
train loss:  0.29320627450942993
train gradient:  0.1826690717996669
iteration : 1540
train acc:  0.875
train loss:  0.2697628438472748
train gradient:  0.1414843675913273
iteration : 1541
train acc:  0.8828125
train loss:  0.3120318055152893
train gradient:  0.10905374024891336
iteration : 1542
train acc:  0.8515625
train loss:  0.3264469802379608
train gradient:  0.13082250439563597
iteration : 1543
train acc:  0.875
train loss:  0.24380187690258026
train gradient:  0.1236248080450679
iteration : 1544
train acc:  0.8671875
train loss:  0.27986088395118713
train gradient:  0.1160102699974219
iteration : 1545
train acc:  0.84375
train loss:  0.32571661472320557
train gradient:  0.16814030892619863
iteration : 1546
train acc:  0.8125
train loss:  0.44343286752700806
train gradient:  0.23287406602093946
iteration : 1547
train acc:  0.859375
train loss:  0.30677977204322815
train gradient:  0.09786043968638623
iteration : 1548
train acc:  0.7890625
train loss:  0.42242369055747986
train gradient:  0.18124389698794954
iteration : 1549
train acc:  0.9140625
train loss:  0.2535814046859741
train gradient:  0.10719523791466715
iteration : 1550
train acc:  0.84375
train loss:  0.35975489020347595
train gradient:  0.12027362925990173
iteration : 1551
train acc:  0.8984375
train loss:  0.25553470849990845
train gradient:  0.0856619667184939
iteration : 1552
train acc:  0.9375
train loss:  0.20447054505348206
train gradient:  0.07432218131866096
iteration : 1553
train acc:  0.8671875
train loss:  0.3000414967536926
train gradient:  0.10300807921676979
iteration : 1554
train acc:  0.8515625
train loss:  0.305745929479599
train gradient:  0.16369405667186648
iteration : 1555
train acc:  0.8359375
train loss:  0.3626854419708252
train gradient:  0.12421152104149528
iteration : 1556
train acc:  0.8515625
train loss:  0.4342026114463806
train gradient:  0.26621348693801633
iteration : 1557
train acc:  0.8984375
train loss:  0.31576114892959595
train gradient:  0.16369405490085992
iteration : 1558
train acc:  0.796875
train loss:  0.39069509506225586
train gradient:  0.16628085115738828
iteration : 1559
train acc:  0.90625
train loss:  0.25564754009246826
train gradient:  0.08637736550323517
iteration : 1560
train acc:  0.84375
train loss:  0.34931445121765137
train gradient:  0.14262884277833154
iteration : 1561
train acc:  0.859375
train loss:  0.309657484292984
train gradient:  0.13298604704136713
iteration : 1562
train acc:  0.8828125
train loss:  0.3011362850666046
train gradient:  0.09672113010857482
iteration : 1563
train acc:  0.9140625
train loss:  0.22930628061294556
train gradient:  0.09187685627451324
iteration : 1564
train acc:  0.890625
train loss:  0.2778179943561554
train gradient:  0.12021401930491969
iteration : 1565
train acc:  0.8828125
train loss:  0.27350708842277527
train gradient:  0.11331399761106878
iteration : 1566
train acc:  0.859375
train loss:  0.2953435778617859
train gradient:  0.09392215220109534
iteration : 1567
train acc:  0.8203125
train loss:  0.4139683246612549
train gradient:  0.1856517889596908
iteration : 1568
train acc:  0.890625
train loss:  0.2873368561267853
train gradient:  0.12562057164750726
iteration : 1569
train acc:  0.875
train loss:  0.27285677194595337
train gradient:  0.11301625666770627
iteration : 1570
train acc:  0.8515625
train loss:  0.39238786697387695
train gradient:  0.18579155986213075
iteration : 1571
train acc:  0.8984375
train loss:  0.2662714421749115
train gradient:  0.09237447838843704
iteration : 1572
train acc:  0.890625
train loss:  0.25013813376426697
train gradient:  0.11535755744462449
iteration : 1573
train acc:  0.828125
train loss:  0.3909754753112793
train gradient:  0.16781621362587457
iteration : 1574
train acc:  0.8046875
train loss:  0.44161492586135864
train gradient:  0.17399652699808782
iteration : 1575
train acc:  0.8828125
train loss:  0.2646831274032593
train gradient:  0.09006262442515389
iteration : 1576
train acc:  0.8515625
train loss:  0.29581767320632935
train gradient:  0.10529943492973946
iteration : 1577
train acc:  0.890625
train loss:  0.3025703430175781
train gradient:  0.10693793748049855
iteration : 1578
train acc:  0.8828125
train loss:  0.25919926166534424
train gradient:  0.11349218159946611
iteration : 1579
train acc:  0.859375
train loss:  0.3152328133583069
train gradient:  0.093628258353132
iteration : 1580
train acc:  0.90625
train loss:  0.2516617774963379
train gradient:  0.07977246054154098
iteration : 1581
train acc:  0.859375
train loss:  0.3011874258518219
train gradient:  0.09613704680547695
iteration : 1582
train acc:  0.8359375
train loss:  0.31120550632476807
train gradient:  0.13545180785389102
iteration : 1583
train acc:  0.953125
train loss:  0.19791512191295624
train gradient:  0.07567543795088565
iteration : 1584
train acc:  0.890625
train loss:  0.26970967650413513
train gradient:  0.1279027181202517
iteration : 1585
train acc:  0.84375
train loss:  0.31613436341285706
train gradient:  0.10950388888731435
iteration : 1586
train acc:  0.8671875
train loss:  0.34738683700561523
train gradient:  0.10022627097765031
iteration : 1587
train acc:  0.8984375
train loss:  0.3038696050643921
train gradient:  0.09379792606830215
iteration : 1588
train acc:  0.8828125
train loss:  0.2993932366371155
train gradient:  0.10859426044781348
iteration : 1589
train acc:  0.921875
train loss:  0.2447022944688797
train gradient:  0.08530859168960099
iteration : 1590
train acc:  0.90625
train loss:  0.2496243119239807
train gradient:  0.09274844536139067
iteration : 1591
train acc:  0.8828125
train loss:  0.25812214612960815
train gradient:  0.1264697743260923
iteration : 1592
train acc:  0.859375
train loss:  0.38203102350234985
train gradient:  0.11971599736214744
iteration : 1593
train acc:  0.859375
train loss:  0.29239049553871155
train gradient:  0.10540852347291516
iteration : 1594
train acc:  0.8515625
train loss:  0.28638309240341187
train gradient:  0.12792881544516524
iteration : 1595
train acc:  0.8671875
train loss:  0.38229823112487793
train gradient:  0.13103268919420097
iteration : 1596
train acc:  0.9140625
train loss:  0.2553834319114685
train gradient:  0.08894660684110688
iteration : 1597
train acc:  0.84375
train loss:  0.3720748722553253
train gradient:  0.14649253172836824
iteration : 1598
train acc:  0.8515625
train loss:  0.32833871245384216
train gradient:  0.14194813490106634
iteration : 1599
train acc:  0.859375
train loss:  0.309162437915802
train gradient:  0.08275930119382716
iteration : 1600
train acc:  0.8984375
train loss:  0.25473523139953613
train gradient:  0.09312751689687405
iteration : 1601
train acc:  0.90625
train loss:  0.24319595098495483
train gradient:  0.0742481643630595
iteration : 1602
train acc:  0.875
train loss:  0.30774539709091187
train gradient:  0.07727480443231842
iteration : 1603
train acc:  0.890625
train loss:  0.2556479573249817
train gradient:  0.11302282841024018
iteration : 1604
train acc:  0.875
train loss:  0.2738777697086334
train gradient:  0.0959964963383677
iteration : 1605
train acc:  0.859375
train loss:  0.3157486021518707
train gradient:  0.10185078166998893
iteration : 1606
train acc:  0.90625
train loss:  0.2411414235830307
train gradient:  0.08577725837211227
iteration : 1607
train acc:  0.875
train loss:  0.2869584858417511
train gradient:  0.23541895580659078
iteration : 1608
train acc:  0.890625
train loss:  0.3031863272190094
train gradient:  0.11741119784673514
iteration : 1609
train acc:  0.90625
train loss:  0.25828611850738525
train gradient:  0.0866456345436056
iteration : 1610
train acc:  0.875
train loss:  0.2697952389717102
train gradient:  0.08720469896690063
iteration : 1611
train acc:  0.84375
train loss:  0.3273113965988159
train gradient:  0.12245785384713401
iteration : 1612
train acc:  0.8515625
train loss:  0.35616642236709595
train gradient:  0.09815143095185686
iteration : 1613
train acc:  0.8203125
train loss:  0.45209380984306335
train gradient:  0.21965909173163603
iteration : 1614
train acc:  0.8828125
train loss:  0.2800391912460327
train gradient:  0.13985008458564036
iteration : 1615
train acc:  0.859375
train loss:  0.35016024112701416
train gradient:  0.09864777493118437
iteration : 1616
train acc:  0.8515625
train loss:  0.26133739948272705
train gradient:  0.06773359124016788
iteration : 1617
train acc:  0.8671875
train loss:  0.3068341016769409
train gradient:  0.10752912051212263
iteration : 1618
train acc:  0.828125
train loss:  0.34041494131088257
train gradient:  0.12950549763196717
iteration : 1619
train acc:  0.890625
train loss:  0.32424306869506836
train gradient:  0.11544130637246369
iteration : 1620
train acc:  0.8515625
train loss:  0.2767673432826996
train gradient:  0.19817376253341581
iteration : 1621
train acc:  0.890625
train loss:  0.2540091872215271
train gradient:  0.08248591201718294
iteration : 1622
train acc:  0.84375
train loss:  0.3022063970565796
train gradient:  0.13504188216712482
iteration : 1623
train acc:  0.8828125
train loss:  0.3406367301940918
train gradient:  0.12901440683752785
iteration : 1624
train acc:  0.8828125
train loss:  0.2612079977989197
train gradient:  0.08455229442046552
iteration : 1625
train acc:  0.8984375
train loss:  0.2621348798274994
train gradient:  0.10051413354235474
iteration : 1626
train acc:  0.84375
train loss:  0.3826337158679962
train gradient:  0.21095052472400028
iteration : 1627
train acc:  0.890625
train loss:  0.26641857624053955
train gradient:  0.0857781466405991
iteration : 1628
train acc:  0.875
train loss:  0.32112839818000793
train gradient:  0.12000855298832802
iteration : 1629
train acc:  0.84375
train loss:  0.32757771015167236
train gradient:  0.12367841936045129
iteration : 1630
train acc:  0.8515625
train loss:  0.36725521087646484
train gradient:  0.1362368293468757
iteration : 1631
train acc:  0.8515625
train loss:  0.2975012958049774
train gradient:  0.12382485366043233
iteration : 1632
train acc:  0.8515625
train loss:  0.336699903011322
train gradient:  0.1518566030908161
iteration : 1633
train acc:  0.828125
train loss:  0.40197497606277466
train gradient:  0.16748013995413769
iteration : 1634
train acc:  0.890625
train loss:  0.26736798882484436
train gradient:  0.08752529454837465
iteration : 1635
train acc:  0.8671875
train loss:  0.34790825843811035
train gradient:  0.14087055236748647
iteration : 1636
train acc:  0.8671875
train loss:  0.284305602312088
train gradient:  0.14259128587749825
iteration : 1637
train acc:  0.859375
train loss:  0.3082759976387024
train gradient:  0.08371768994321124
iteration : 1638
train acc:  0.8359375
train loss:  0.3384804129600525
train gradient:  0.14775167210447016
iteration : 1639
train acc:  0.875
train loss:  0.304307758808136
train gradient:  0.1038449679734557
iteration : 1640
train acc:  0.8359375
train loss:  0.3934403359889984
train gradient:  0.15459229767747176
iteration : 1641
train acc:  0.890625
train loss:  0.25929635763168335
train gradient:  0.09205673289395228
iteration : 1642
train acc:  0.8828125
train loss:  0.31229686737060547
train gradient:  0.12705061250705735
iteration : 1643
train acc:  0.84375
train loss:  0.3420468270778656
train gradient:  0.14199754705931578
iteration : 1644
train acc:  0.9140625
train loss:  0.2715829014778137
train gradient:  0.15208055109513258
iteration : 1645
train acc:  0.8125
train loss:  0.326244592666626
train gradient:  0.13774052880193507
iteration : 1646
train acc:  0.8671875
train loss:  0.27766039967536926
train gradient:  0.10629545575507583
iteration : 1647
train acc:  0.84375
train loss:  0.3615149259567261
train gradient:  0.15842921844975033
iteration : 1648
train acc:  0.890625
train loss:  0.285305917263031
train gradient:  0.12276178640749145
iteration : 1649
train acc:  0.84375
train loss:  0.30256563425064087
train gradient:  0.11762335453083103
iteration : 1650
train acc:  0.8828125
train loss:  0.3118300437927246
train gradient:  0.14313553980579094
iteration : 1651
train acc:  0.8515625
train loss:  0.3385190963745117
train gradient:  0.14181497372449742
iteration : 1652
train acc:  0.859375
train loss:  0.31954723596572876
train gradient:  0.10696309713523768
iteration : 1653
train acc:  0.859375
train loss:  0.3571988642215729
train gradient:  0.12388319022477515
iteration : 1654
train acc:  0.8203125
train loss:  0.35494986176490784
train gradient:  0.1658617367534782
iteration : 1655
train acc:  0.8125
train loss:  0.37471771240234375
train gradient:  0.21947917560497862
iteration : 1656
train acc:  0.875
train loss:  0.27606120705604553
train gradient:  0.09132445412040852
iteration : 1657
train acc:  0.859375
train loss:  0.31429433822631836
train gradient:  0.09620812627017408
iteration : 1658
train acc:  0.8828125
train loss:  0.27604982256889343
train gradient:  0.09732979735749017
iteration : 1659
train acc:  0.8828125
train loss:  0.3272721767425537
train gradient:  0.17228179444224745
iteration : 1660
train acc:  0.9140625
train loss:  0.2445666790008545
train gradient:  0.11391562657964607
iteration : 1661
train acc:  0.8671875
train loss:  0.29563936591148376
train gradient:  0.13569243444166024
iteration : 1662
train acc:  0.875
train loss:  0.29644083976745605
train gradient:  0.11425647146062859
iteration : 1663
train acc:  0.8828125
train loss:  0.27923935651779175
train gradient:  0.09113880541544944
iteration : 1664
train acc:  0.875
train loss:  0.32382339239120483
train gradient:  0.16431096489476837
iteration : 1665
train acc:  0.8515625
train loss:  0.3356163501739502
train gradient:  0.11466470222639981
iteration : 1666
train acc:  0.859375
train loss:  0.3479284346103668
train gradient:  0.1689800784676975
iteration : 1667
train acc:  0.8984375
train loss:  0.24835939705371857
train gradient:  0.0851641772778735
iteration : 1668
train acc:  0.859375
train loss:  0.2934108376502991
train gradient:  0.12164865852286691
iteration : 1669
train acc:  0.8359375
train loss:  0.3512531816959381
train gradient:  0.13232447600904623
iteration : 1670
train acc:  0.8203125
train loss:  0.3885560631752014
train gradient:  0.17216335174349326
iteration : 1671
train acc:  0.828125
train loss:  0.328797310590744
train gradient:  0.11039385858951001
iteration : 1672
train acc:  0.8671875
train loss:  0.32559365034103394
train gradient:  0.13171773984705865
iteration : 1673
train acc:  0.875
train loss:  0.2751106917858124
train gradient:  0.09599165925649356
iteration : 1674
train acc:  0.8046875
train loss:  0.3612448275089264
train gradient:  0.18348946203976063
iteration : 1675
train acc:  0.8515625
train loss:  0.38484427332878113
train gradient:  0.15139760292089147
iteration : 1676
train acc:  0.8203125
train loss:  0.32812607288360596
train gradient:  0.11295532522716709
iteration : 1677
train acc:  0.8515625
train loss:  0.318528950214386
train gradient:  0.11645283057933471
iteration : 1678
train acc:  0.8984375
train loss:  0.2627359926700592
train gradient:  0.13187045318872356
iteration : 1679
train acc:  0.875
train loss:  0.28970983624458313
train gradient:  0.08543826904420668
iteration : 1680
train acc:  0.8671875
train loss:  0.32193058729171753
train gradient:  0.22240124856022275
iteration : 1681
train acc:  0.90625
train loss:  0.22890764474868774
train gradient:  0.12464500576399636
iteration : 1682
train acc:  0.84375
train loss:  0.3784521818161011
train gradient:  0.14443781936983918
iteration : 1683
train acc:  0.890625
train loss:  0.2369682490825653
train gradient:  0.08884768794807658
iteration : 1684
train acc:  0.890625
train loss:  0.2734927535057068
train gradient:  0.07913388535948628
iteration : 1685
train acc:  0.875
train loss:  0.3282015919685364
train gradient:  0.11053176725088512
iteration : 1686
train acc:  0.875
train loss:  0.2979736626148224
train gradient:  0.13430981965476807
iteration : 1687
train acc:  0.8515625
train loss:  0.3024638295173645
train gradient:  0.1275145737642739
iteration : 1688
train acc:  0.90625
train loss:  0.257740318775177
train gradient:  0.10222304935205323
iteration : 1689
train acc:  0.859375
train loss:  0.28638702630996704
train gradient:  0.09159540223611518
iteration : 1690
train acc:  0.9140625
train loss:  0.21646854281425476
train gradient:  0.10178056088314279
iteration : 1691
train acc:  0.8515625
train loss:  0.338620662689209
train gradient:  0.1133440135757596
iteration : 1692
train acc:  0.8046875
train loss:  0.4471941888332367
train gradient:  0.2630304689759541
iteration : 1693
train acc:  0.875
train loss:  0.32390666007995605
train gradient:  0.09829199978889794
iteration : 1694
train acc:  0.8828125
train loss:  0.31243258714675903
train gradient:  0.11837029417307489
iteration : 1695
train acc:  0.859375
train loss:  0.35160860419273376
train gradient:  0.17918976115361998
iteration : 1696
train acc:  0.8671875
train loss:  0.30275410413742065
train gradient:  0.09937116819771344
iteration : 1697
train acc:  0.8671875
train loss:  0.31764328479766846
train gradient:  0.09916820521878035
iteration : 1698
train acc:  0.796875
train loss:  0.45143309235572815
train gradient:  0.25375757593745035
iteration : 1699
train acc:  0.875
train loss:  0.32314229011535645
train gradient:  0.0981363054646794
iteration : 1700
train acc:  0.8984375
train loss:  0.2155250608921051
train gradient:  0.09175695604939209
iteration : 1701
train acc:  0.9140625
train loss:  0.2313562035560608
train gradient:  0.06445009934432445
iteration : 1702
train acc:  0.875
train loss:  0.27136293053627014
train gradient:  0.08237194968734403
iteration : 1703
train acc:  0.8671875
train loss:  0.30862948298454285
train gradient:  0.1057951833061676
iteration : 1704
train acc:  0.828125
train loss:  0.32652878761291504
train gradient:  0.09597776785164867
iteration : 1705
train acc:  0.890625
train loss:  0.2770044207572937
train gradient:  0.12367162741237096
iteration : 1706
train acc:  0.8671875
train loss:  0.290743350982666
train gradient:  0.08466497790157683
iteration : 1707
train acc:  0.875
train loss:  0.3223138451576233
train gradient:  0.15200689784863636
iteration : 1708
train acc:  0.8046875
train loss:  0.4787881374359131
train gradient:  0.20830153798007345
iteration : 1709
train acc:  0.8671875
train loss:  0.32906055450439453
train gradient:  0.17323742514021107
iteration : 1710
train acc:  0.84375
train loss:  0.3611332178115845
train gradient:  0.11880598260991299
iteration : 1711
train acc:  0.890625
train loss:  0.29748380184173584
train gradient:  0.10407786134169596
iteration : 1712
train acc:  0.8515625
train loss:  0.37258726358413696
train gradient:  0.1115722018494917
iteration : 1713
train acc:  0.875
train loss:  0.28647381067276
train gradient:  0.10866970608377165
iteration : 1714
train acc:  0.8359375
train loss:  0.3429300785064697
train gradient:  0.172992292885039
iteration : 1715
train acc:  0.859375
train loss:  0.4166959822177887
train gradient:  0.25871960716601183
iteration : 1716
train acc:  0.875
train loss:  0.30327752232551575
train gradient:  0.13333333665740865
iteration : 1717
train acc:  0.8828125
train loss:  0.3540780246257782
train gradient:  0.14616559927213724
iteration : 1718
train acc:  0.859375
train loss:  0.34481656551361084
train gradient:  0.12754525125053123
iteration : 1719
train acc:  0.8515625
train loss:  0.30073869228363037
train gradient:  0.10575039003596831
it